{"buggy_code": ["import atexit\nimport codecs\nimport errno\nimport fnmatch\nimport gzip\nimport json\nimport logging\nimport math\nimport os\nimport pathlib\nimport posixpath\nimport shutil\nimport stat\nimport subprocess\nimport sys\nimport tarfile\nimport tempfile\nimport time\nimport urllib.parse\nimport urllib.request\nimport uuid\nfrom concurrent.futures import as_completed\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom subprocess import CalledProcessError, TimeoutExpired\nfrom typing import Optional, Union\nfrom urllib.parse import unquote\nfrom urllib.request import pathname2url\n\nimport yaml\n\nfrom mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE\n\ntry:\n    from yaml import CSafeDumper as YamlSafeDumper\n    from yaml import CSafeLoader as YamlSafeLoader\nexcept ImportError:\n    from yaml import SafeDumper as YamlSafeDumper\n    from yaml import SafeLoader as YamlSafeLoader\n\nfrom mlflow.entities import FileInfo\nfrom mlflow.environment_variables import (\n    _MLFLOW_MPD_NUM_RETRIES,\n    _MLFLOW_MPD_RETRY_INTERVAL_SECONDS,\n    MLFLOW_DOWNLOAD_CHUNK_TIMEOUT,\n    MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR,\n)\nfrom mlflow.exceptions import MissingConfigException, MlflowException\nfrom mlflow.protos.databricks_artifacts_pb2 import ArtifactCredentialType\nfrom mlflow.utils import download_cloud_file_chunk, merge_dicts\nfrom mlflow.utils.databricks_utils import _get_dbutils\nfrom mlflow.utils.os import is_windows\nfrom mlflow.utils.process import cache_return_value_per_process\nfrom mlflow.utils.request_utils import cloud_storage_http_request, download_chunk\nfrom mlflow.utils.rest_utils import augmented_raise_for_status\n\nENCODING = \"utf-8\"\nMAX_PARALLEL_DOWNLOAD_WORKERS = os.cpu_count() * 2\n_PROGRESS_BAR_DISPLAY_THRESHOLD = 500_000_000  # 500 MB\n\n_logger = logging.getLogger(__name__)\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass ArtifactProgressBar:\n    def __init__(self, desc, total, step, **kwargs) -> None:\n        self.desc = desc\n        self.total = total\n        self.step = step\n        self.pbar = None\n        self.progress = 0\n        self.kwargs = kwargs\n\n    def set_pbar(self):\n        if MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR.get():\n            try:\n                from tqdm.auto import tqdm\n\n                self.pbar = tqdm(total=self.total, desc=self.desc, **self.kwargs)\n            except ImportError:\n                pass\n\n    @classmethod\n    def chunks(cls, file_size, desc, chunk_size):\n        bar = cls(\n            desc,\n            total=file_size,\n            step=chunk_size,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n            miniters=1,\n        )\n        if file_size >= _PROGRESS_BAR_DISPLAY_THRESHOLD:\n            bar.set_pbar()\n        return bar\n\n    @classmethod\n    def files(cls, desc, total):\n        bar = cls(desc, total=total, step=1)\n        bar.set_pbar()\n        return bar\n\n    def update(self):\n        if self.pbar:\n            update_step = min(self.total - self.progress, self.step)\n            self.pbar.update(update_step)\n            self.pbar.refresh()\n            self.progress += update_step\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        if self.pbar:\n            self.pbar.close()\n\n\ndef is_directory(name):\n    return os.path.isdir(name)\n\n\ndef is_file(name):\n    return os.path.isfile(name)\n\n\ndef exists(name):\n    return os.path.exists(name)\n\n\ndef list_all(root, filter_func=lambda x: True, full_path=False):\n    \"\"\"\n    List all entities directly under 'dir_name' that satisfy 'filter_func'\n\n    :param root: Name of directory to start search\n    :param filter_func: function or lambda that takes path\n    :param full_path: If True will return results as full path including `root`\n\n    :return: list of all files or directories that satisfy the criteria.\n    \"\"\"\n    if not is_directory(root):\n        raise Exception(f\"Invalid parent directory '{root}'\")\n    matches = [x for x in os.listdir(root) if filter_func(os.path.join(root, x))]\n    return [os.path.join(root, m) for m in matches] if full_path else matches\n\n\ndef list_subdirs(dir_name, full_path=False):\n    \"\"\"\n    Equivalent to UNIX command:\n      ``find $dir_name -depth 1 -type d``\n\n    :param dir_name: Name of directory to start search\n    :param full_path: If True will return results as full path including `root`\n\n    :return: list of all directories directly under 'dir_name'\n    \"\"\"\n    return list_all(dir_name, os.path.isdir, full_path)\n\n\ndef list_files(dir_name, full_path=False):\n    \"\"\"\n    Equivalent to UNIX command:\n      ``find $dir_name -depth 1 -type f``\n\n    :param dir_name: Name of directory to start search\n    :param full_path: If True will return results as full path including `root`\n\n    :return: list of all files directly under 'dir_name'\n    \"\"\"\n    return list_all(dir_name, os.path.isfile, full_path)\n\n\ndef find(root, name, full_path=False):\n    \"\"\"\n    Search for a file in a root directory. Equivalent to:\n      ``find $root -name \"$name\" -depth 1``\n\n    :param root: Name of root directory for find\n    :param name: Name of file or directory to find directly under root directory\n    :param full_path: If True will return results as full path including `root`\n\n    :return: list of matching files or directories\n    \"\"\"\n    path_name = os.path.join(root, name)\n    return list_all(root, lambda x: x == path_name, full_path)\n\n\ndef mkdir(root, name=None):\n    \"\"\"\n    Make directory with name \"root/name\", or just \"root\" if name is None.\n\n    :param root: Name of parent directory\n    :param name: Optional name of leaf directory\n\n    :return: Path to created directory\n    \"\"\"\n    target = os.path.join(root, name) if name is not None else root\n    try:\n        os.makedirs(target)\n    except OSError as e:\n        if e.errno != errno.EEXIST or not os.path.isdir(target):\n            raise e\n    return target\n\n\ndef make_containing_dirs(path):\n    \"\"\"\n    Create the base directory for a given file path if it does not exist; also creates parent\n    directories.\n    \"\"\"\n    dir_name = os.path.dirname(path)\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n\n\ndef write_yaml(root, file_name, data, overwrite=False, sort_keys=True, ensure_yaml_extension=True):\n    \"\"\"\n    Write dictionary data in yaml format.\n\n    :param root: Directory name.\n    :param file_name: Desired file name.\n    :param data: data to be dumped as yaml format\n    :param overwrite: If True, will overwrite existing files\n    :param ensure_yaml_extension: If True, will automatically add .yaml extension if not given\n    \"\"\"\n    if not exists(root):\n        raise MissingConfigException(f\"Parent directory '{root}' does not exist.\")\n\n    file_path = os.path.join(root, file_name)\n    yaml_file_name = file_path\n    if ensure_yaml_extension and not file_path.endswith(\".yaml\"):\n        yaml_file_name = file_path + \".yaml\"\n\n    if exists(yaml_file_name) and not overwrite:\n        raise Exception(f\"Yaml file '{file_path}' exists as '{yaml_file_name}\")\n\n    try:\n        with codecs.open(yaml_file_name, mode=\"w\", encoding=ENCODING) as yaml_file:\n            yaml.dump(\n                data,\n                yaml_file,\n                default_flow_style=False,\n                allow_unicode=True,\n                sort_keys=sort_keys,\n                Dumper=YamlSafeDumper,\n            )\n    except Exception as e:\n        raise e\n\n\ndef overwrite_yaml(root, file_name, data, ensure_yaml_extension=True):\n    \"\"\"\n    Safely overwrites a preexisting yaml file, ensuring that file contents are not deleted or\n    corrupted if the write fails. This is achieved by writing contents to a temporary file\n    and moving the temporary file to replace the preexisting file, rather than opening the\n    preexisting file for a direct write.\n\n    :param root: Directory name.\n    :param file_name: File name.\n    :param data: The data to write, represented as a dictionary.\n    :param ensure_yaml_extension: If True, Will automatically add .yaml extension if not given\n    \"\"\"\n    tmp_file_path = None\n    original_file_path = os.path.join(root, file_name)\n    original_file_mode = os.stat(original_file_path).st_mode\n    try:\n        tmp_file_fd, tmp_file_path = tempfile.mkstemp(suffix=\"file.yaml\")\n        os.close(tmp_file_fd)\n        write_yaml(\n            root=get_parent_dir(tmp_file_path),\n            file_name=os.path.basename(tmp_file_path),\n            data=data,\n            overwrite=True,\n            sort_keys=True,\n            ensure_yaml_extension=ensure_yaml_extension,\n        )\n        shutil.move(tmp_file_path, original_file_path)\n        # restores original file permissions, see https://docs.python.org/3/library/tempfile.html#tempfile.mkstemp\n        os.chmod(original_file_path, original_file_mode)\n    finally:\n        if tmp_file_path is not None and os.path.exists(tmp_file_path):\n            os.remove(tmp_file_path)\n\n\ndef read_yaml(root, file_name):\n    \"\"\"\n    Read data from yaml file and return as dictionary\n\n    :param root: Directory name\n    :param file_name: File name. Expects to have '.yaml' extension\n\n    :return: Data in yaml file as dictionary\n    \"\"\"\n    if not exists(root):\n        raise MissingConfigException(\n            f\"Cannot read '{file_name}'. Parent dir '{root}' does not exist.\"\n        )\n\n    file_path = os.path.join(root, file_name)\n    if not exists(file_path):\n        raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n    try:\n        with codecs.open(file_path, mode=\"r\", encoding=ENCODING) as yaml_file:\n            return yaml.load(yaml_file, Loader=YamlSafeLoader)\n    except Exception as e:\n        raise e\n\n\nclass UniqueKeyLoader(YamlSafeLoader):\n    def construct_mapping(self, node, deep=False):\n        mapping = set()\n        for key_node, _ in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            if key in mapping:\n                raise ValueError(f\"Duplicate '{key}' key found in YAML.\")\n            mapping.add(key)\n        return super().construct_mapping(node, deep)\n\n\ndef render_and_merge_yaml(root, template_name, context_name):\n    \"\"\"\n    Renders a Jinja2-templated YAML file based on a YAML context file, merge them, and return\n    result as a dictionary.\n\n    :param root: Root directory of the YAML files\n    :param template_name: Name of the template file\n    :param context_name: Name of the context file\n    :return: Data in yaml file as dictionary\n    \"\"\"\n    import jinja2\n\n    template_path = os.path.join(root, template_name)\n    context_path = os.path.join(root, context_name)\n\n    for path in (template_path, context_path):\n        if not pathlib.Path(path).is_file():\n            raise MissingConfigException(f\"Yaml file '{path}' does not exist.\")\n\n    j2_env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(root, encoding=ENCODING),\n        undefined=jinja2.StrictUndefined,\n        line_comment_prefix=\"#\",\n    )\n\n    def from_json(input_var):\n        with open(input_var, encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    j2_env.filters[\"from_json\"] = from_json\n    # Compute final source of context file (e.g. my-profile.yml), applying Jinja filters\n    # like from_json as needed to load context information from files, then load into a dict\n    context_source = j2_env.get_template(context_name).render({})\n    context_dict = yaml.load(context_source, Loader=UniqueKeyLoader) or {}\n\n    # Substitute parameters from context dict into template\n    source = j2_env.get_template(template_name).render(context_dict)\n    rendered_template_dict = yaml.load(source, Loader=UniqueKeyLoader)\n    return merge_dicts(rendered_template_dict, context_dict)\n\n\ndef read_parquet_as_pandas_df(data_parquet_path: str):\n    \"\"\"\n    Deserialize and load the specified parquet file as a Pandas DataFrame.\n\n    :param data_parquet_path: String, path object (implementing os.PathLike[str]),\n    or file-like object implementing a binary read() function. The string\n    could be a URL. Valid URL schemes include http, ftp, s3, gs, and file.\n    For file URLs, a host is expected. A local file could\n    be: file://localhost/path/to/table.parquet. A file URL can also be a path to a\n    directory that contains multiple partitioned parquet files. Pyarrow\n    support paths to directories as well as file URLs. A directory\n    path could be: file://localhost/path/to/tables or s3://bucket/partition_dir.\n    :return: pandas dataframe\n    \"\"\"\n    import pandas as pd\n\n    return pd.read_parquet(data_parquet_path, engine=\"pyarrow\")\n\n\ndef write_pandas_df_as_parquet(df, data_parquet_path: str):\n    \"\"\"\n    Write a DataFrame to the binary parquet format.\n\n    :param df: pandas data frame.\n    :param data_parquet_path: String, path object (implementing os.PathLike[str]),\n    or file-like object implementing a binary write() function.\n    \"\"\"\n    df.to_parquet(data_parquet_path, engine=\"pyarrow\")\n\n\nclass TempDir:\n    def __init__(self, chdr=False, remove_on_exit=True):\n        self._dir = None\n        self._path = None\n        self._chdr = chdr\n        self._remove = remove_on_exit\n\n    def __enter__(self):\n        self._path = os.path.abspath(create_tmp_dir())\n        assert os.path.exists(self._path)\n        if self._chdr:\n            self._dir = os.path.abspath(os.getcwd())\n            os.chdir(self._path)\n        return self\n\n    def __exit__(self, tp, val, traceback):\n        if self._chdr and self._dir:\n            os.chdir(self._dir)\n            self._dir = None\n        if self._remove and os.path.exists(self._path):\n            shutil.rmtree(self._path)\n\n        assert not self._remove or not os.path.exists(self._path)\n        assert os.path.exists(os.getcwd())\n\n    def path(self, *path):\n        return os.path.join(\"./\", *path) if self._chdr else os.path.join(self._path, *path)\n\n\ndef read_file_lines(parent_path, file_name):\n    \"\"\"\n    Return the contents of the file as an array where each element is a separate line.\n\n    :param parent_path: Full path to the directory that contains the file.\n    :param file_name: Leaf file name.\n\n    :return: All lines in the file as an array.\n    \"\"\"\n    file_path = os.path.join(parent_path, file_name)\n    with codecs.open(file_path, mode=\"r\", encoding=ENCODING) as f:\n        return f.readlines()\n\n\ndef read_file(parent_path, file_name):\n    \"\"\"\n    Return the contents of the file.\n\n    :param parent_path: Full path to the directory that contains the file.\n    :param file_name: Leaf file name.\n\n    :return: The contents of the file.\n    \"\"\"\n    file_path = os.path.join(parent_path, file_name)\n    with codecs.open(file_path, mode=\"r\", encoding=ENCODING) as f:\n        return f.read()\n\n\ndef get_file_info(path, rel_path):\n    \"\"\"\n    Returns file meta data : location, size, ... etc\n\n    :param path: Path to artifact\n\n    :return: `FileInfo` object\n    \"\"\"\n    if is_directory(path):\n        return FileInfo(rel_path, True, None)\n    else:\n        return FileInfo(rel_path, False, os.path.getsize(path))\n\n\ndef get_relative_path(root_path, target_path):\n    \"\"\"\n    Remove root path common prefix and return part of `path` relative to `root_path`.\n\n    :param root_path: Root path\n    :param target_path: Desired path for common prefix removal\n\n    :return: Path relative to root_path\n    \"\"\"\n    if len(root_path) > len(target_path):\n        raise Exception(f\"Root path '{root_path}' longer than target path '{target_path}'\")\n    common_prefix = os.path.commonprefix([root_path, target_path])\n    return os.path.relpath(target_path, common_prefix)\n\n\ndef mv(target, new_parent):\n    shutil.move(target, new_parent)\n\n\ndef write_to(filename, data):\n    with codecs.open(filename, mode=\"w\", encoding=ENCODING) as handle:\n        handle.write(data)\n\n\ndef append_to(filename, data):\n    with open(filename, \"a\") as handle:\n        handle.write(data)\n\n\ndef make_tarfile(output_filename, source_dir, archive_name, custom_filter=None):\n    # Helper for filtering out modification timestamps\n    def _filter_timestamps(tar_info):\n        tar_info.mtime = 0\n        return tar_info if custom_filter is None else custom_filter(tar_info)\n\n    unzipped_file_handle, unzipped_filename = tempfile.mkstemp()\n    try:\n        with tarfile.open(unzipped_filename, \"w\") as tar:\n            tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n        # When gzipping the tar, don't include the tar's filename or modification time in the\n        # zipped archive (see https://docs.python.org/3/library/gzip.html#gzip.GzipFile)\n        with gzip.GzipFile(\n            filename=\"\", fileobj=open(output_filename, \"wb\"), mode=\"wb\", mtime=0\n        ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar:\n            gzipped_tar.write(tar.read())\n    finally:\n        os.close(unzipped_file_handle)\n\n\ndef _copy_project(src_path, dst_path=\"\"):\n    \"\"\"\n    Internal function used to copy MLflow project during development.\n\n    Copies the content of the whole directory tree except patterns defined in .dockerignore.\n    The MLflow is assumed to be accessible as a local directory in this case.\n\n\n    :param dst_path: MLflow will be copied here\n    :return: name of the MLflow project directory\n    \"\"\"\n\n    def _docker_ignore(mlflow_root):\n        docker_ignore = os.path.join(mlflow_root, \".dockerignore\")\n        patterns = []\n        if os.path.exists(docker_ignore):\n            with open(docker_ignore) as f:\n                patterns = [x.strip() for x in f.readlines()]\n\n        def ignore(_, names):\n            res = set()\n            for p in patterns:\n                res.update(set(fnmatch.filter(names, p)))\n            return list(res)\n\n        return ignore if patterns else None\n\n    mlflow_dir = \"mlflow-project\"\n    # check if we have project root\n    assert os.path.isfile(os.path.join(src_path, \"setup.py\")), \"file not found \" + str(\n        os.path.abspath(os.path.join(src_path, \"setup.py\"))\n    )\n    shutil.copytree(src_path, os.path.join(dst_path, mlflow_dir), ignore=_docker_ignore(src_path))\n    return mlflow_dir\n\n\ndef _copy_file_or_tree(src, dst, dst_dir=None):\n    \"\"\"\n    :return: The path to the copied artifacts, relative to `dst`\n    \"\"\"\n    dst_subpath = os.path.basename(os.path.abspath(src))\n    if dst_dir is not None:\n        dst_subpath = os.path.join(dst_dir, dst_subpath)\n    dst_path = os.path.join(dst, dst_subpath)\n    if os.path.isfile(src):\n        dst_dirpath = os.path.dirname(dst_path)\n        if not os.path.exists(dst_dirpath):\n            os.makedirs(dst_dirpath)\n        shutil.copy(src=src, dst=dst_path)\n    else:\n        shutil.copytree(src=src, dst=dst_path, ignore=shutil.ignore_patterns(\"__pycache__\"))\n    return dst_subpath\n\n\ndef _get_local_project_dir_size(project_path):\n    \"\"\"\n    Internal function for reporting the size of a local project directory before copying to\n    destination for cli logging reporting to stdout.\n    :param project_path: local path of the project directory\n    :return: directory file sizes in KB, rounded to single decimal point for legibility\n    \"\"\"\n\n    total_size = 0\n    for root, _, files in os.walk(project_path):\n        for f in files:\n            path = os.path.join(root, f)\n            total_size += os.path.getsize(path)\n    return round(total_size / 1024.0, 1)\n\n\ndef _get_local_file_size(file):\n    \"\"\"\n    Get the size of a local file in KB\n    \"\"\"\n    return round(os.path.getsize(file) / 1024.0, 1)\n\n\ndef get_parent_dir(path):\n    return os.path.abspath(os.path.join(path, os.pardir))\n\n\ndef relative_path_to_artifact_path(path):\n    if os.path == posixpath:\n        return path\n    if os.path.abspath(path) == path:\n        raise Exception(\"This method only works with relative paths.\")\n    return unquote(pathname2url(path))\n\n\ndef path_to_local_file_uri(path):\n    \"\"\"\n    Convert local filesystem path to local file uri.\n    \"\"\"\n    return pathlib.Path(os.path.abspath(path)).as_uri()\n\n\ndef path_to_local_sqlite_uri(path):\n    \"\"\"\n    Convert local filesystem path to sqlite uri.\n    \"\"\"\n    path = posixpath.abspath(pathname2url(os.path.abspath(path)))\n    prefix = \"sqlite://\" if sys.platform == \"win32\" else \"sqlite:///\"\n    return prefix + path\n\n\ndef local_file_uri_to_path(uri):\n    \"\"\"\n    Convert URI to local filesystem path.\n    No-op if the uri does not have the expected scheme.\n    \"\"\"\n    path = uri\n    if uri.startswith(\"file:\"):\n        parsed_path = urllib.parse.urlparse(uri)\n        path = parsed_path.path\n        # Fix for retaining server name in UNC path.\n        if is_windows() and parsed_path.netloc:\n            return urllib.request.url2pathname(rf\"\\\\{parsed_path.netloc}{path}\")\n    return urllib.request.url2pathname(path)\n\n\ndef get_local_path_or_none(path_or_uri):\n    \"\"\"Check if the argument is a local path (no scheme or file:///) and return local path if true,\n    None otherwise.\n    \"\"\"\n    parsed_uri = urllib.parse.urlparse(path_or_uri)\n    if len(parsed_uri.scheme) == 0 or parsed_uri.scheme == \"file\" and len(parsed_uri.netloc) == 0:\n        return local_file_uri_to_path(path_or_uri)\n    else:\n        return None\n\n\ndef yield_file_in_chunks(file, chunk_size=100000000):\n    \"\"\"\n    Generator to chunk-ify the inputted file based on the chunk-size.\n    \"\"\"\n    with open(file, \"rb\") as f:\n        while True:\n            chunk = f.read(chunk_size)\n            if chunk:\n                yield chunk\n            else:\n                break\n\n\ndef download_file_using_http_uri(http_uri, download_path, chunk_size=100000000, headers=None):\n    \"\"\"\n    Downloads a file specified using the `http_uri` to a local `download_path`. This function\n    uses a `chunk_size` to ensure an OOM error is not raised a large file is downloaded.\n\n    Note : This function is meant to download files using presigned urls from various cloud\n            providers.\n    \"\"\"\n    if headers is None:\n        headers = {}\n    with cloud_storage_http_request(\"get\", http_uri, stream=True, headers=headers) as response:\n        augmented_raise_for_status(response)\n        with open(download_path, \"wb\") as output_file:\n            for chunk in response.iter_content(chunk_size=chunk_size):\n                if not chunk:\n                    break\n                output_file.write(chunk)\n\n\n@dataclass(frozen=True)\nclass _Chunk:\n    index: int\n    start: int\n    end: int\n    path: str\n\n\ndef _yield_chunks(path, file_size, chunk_size):\n    num_requests = int(math.ceil(file_size / float(chunk_size)))\n    for i in range(num_requests):\n        range_start = i * chunk_size\n        range_end = min(range_start + chunk_size - 1, file_size - 1)\n        yield _Chunk(i, range_start, range_end, path)\n\n\ndef parallelized_download_file_using_http_uri(\n    thread_pool_executor,\n    http_uri,\n    download_path,\n    remote_file_path,\n    file_size,\n    uri_type,\n    chunk_size,\n    env,\n    headers=None,\n):\n    \"\"\"\n    Downloads a file specified using the `http_uri` to a local `download_path`. This function\n    sends multiple requests in parallel each specifying its own desired byte range as a header,\n    then reconstructs the file from the downloaded chunks. This allows for downloads of large files\n    without OOM risk.\n\n    Note : This function is meant to download files using presigned urls from various cloud\n            providers.\n    Returns a dict of chunk index : exception, if one was thrown for that index.\n    \"\"\"\n\n    def run_download(chunk: _Chunk):\n        try:\n            subprocess.run(\n                [\n                    sys.executable,\n                    download_cloud_file_chunk.__file__,\n                    \"--range-start\",\n                    str(chunk.start),\n                    \"--range-end\",\n                    str(chunk.end),\n                    \"--headers\",\n                    json.dumps(headers or {}),\n                    \"--download-path\",\n                    download_path,\n                    \"--http-uri\",\n                    http_uri,\n                ],\n                text=True,\n                check=True,\n                capture_output=True,\n                timeout=MLFLOW_DOWNLOAD_CHUNK_TIMEOUT.get(),\n                env=env,\n            )\n        except (TimeoutExpired, CalledProcessError) as e:\n            raise MlflowException(\n                f\"\"\"\n----- stdout -----\n{e.stdout.strip()}\n\n----- stderr -----\n{e.stderr.strip()}\n\"\"\"\n            ) from e\n\n    chunks = _yield_chunks(remote_file_path, file_size, chunk_size)\n    # Create file if it doesn't exist or erase the contents if it does. We should do this here\n    # before sending to the workers so they can each individually seek to their respective positions\n    # and write chunks without overwriting.\n    with open(download_path, \"w\"):\n        pass\n    if uri_type == ArtifactCredentialType.GCP_SIGNED_URL or uri_type is None:\n        chunk = next(chunks)\n        # GCP files could be transcoded, in which case the range header is ignored.\n        # Test if this is the case by downloading one chunk and seeing if it's larger than the\n        # requested size. If yes, let that be the file; if not, continue downloading more chunks.\n        download_chunk(\n            range_start=chunk.start,\n            range_end=chunk.end,\n            headers=headers,\n            download_path=download_path,\n            http_uri=http_uri,\n        )\n        downloaded_size = os.path.getsize(download_path)\n        # If downloaded size was equal to the chunk size it would have been downloaded serially,\n        # so we don't need to consider this here\n        if downloaded_size > chunk_size:\n            return {}\n\n    futures = {thread_pool_executor.submit(run_download, chunk): chunk for chunk in chunks}\n    failed_downloads = {}\n    with ArtifactProgressBar.chunks(file_size, f\"Downloading {download_path}\", chunk_size) as pbar:\n        for future in as_completed(futures):\n            chunk = futures[future]\n            try:\n                future.result()\n            except Exception as e:\n                _logger.debug(\n                    f\"Failed to download chunk {chunk.index} for {chunk.path}: {e}. \"\n                    f\"The download of this chunk will be retried later.\"\n                )\n                failed_downloads[chunk] = future.exception()\n            else:\n                pbar.update()\n\n    return failed_downloads\n\n\ndef download_chunk_retries(*, chunks, http_uri, headers, download_path):\n    num_retries = _MLFLOW_MPD_NUM_RETRIES.get()\n    interval = _MLFLOW_MPD_RETRY_INTERVAL_SECONDS.get()\n    for chunk in chunks:\n        _logger.info(f\"Retrying download of chunk {chunk.index} for {chunk.path}\")\n        for retry in range(num_retries):\n            try:\n                download_chunk(\n                    range_start=chunk.start,\n                    range_end=chunk.end,\n                    headers=headers,\n                    download_path=download_path,\n                    http_uri=http_uri,\n                )\n                _logger.info(f\"Successfully downloaded chunk {chunk.index} for {chunk.path}\")\n                break\n            except Exception:\n                if retry == num_retries - 1:\n                    raise\n            time.sleep(interval)\n\n\ndef _handle_readonly_on_windows(func, path, exc_info):\n    \"\"\"\n    This function should not be called directly but should be passed to `onerror` of\n    `shutil.rmtree` in order to reattempt the removal of a read-only file after making\n    it writable on Windows.\n\n    References:\n    - https://bugs.python.org/issue19643\n    - https://bugs.python.org/issue43657\n    \"\"\"\n    exc_type, exc_value = exc_info[:2]\n    should_reattempt = (\n        os.name == \"nt\"\n        and func in (os.unlink, os.rmdir)\n        and issubclass(exc_type, PermissionError)\n        and exc_value.winerror == 5\n    )\n    if not should_reattempt:\n        raise exc_value\n    os.chmod(path, stat.S_IWRITE)\n    func(path)\n\n\ndef _get_tmp_dir():\n    from mlflow.utils.databricks_utils import get_repl_id, is_in_databricks_runtime\n\n    if is_in_databricks_runtime():\n        try:\n            return _get_dbutils().entry_point.getReplLocalTempDir()\n        except Exception:\n            pass\n\n        if repl_id := get_repl_id():\n            return os.path.join(\"/tmp\", \"repl_tmp_data\", repl_id)\n\n    return None\n\n\ndef create_tmp_dir():\n    if directory := _get_tmp_dir():\n        os.makedirs(directory, exist_ok=True)\n        return tempfile.mkdtemp(dir=directory)\n\n    return tempfile.mkdtemp()\n\n\n@cache_return_value_per_process\ndef get_or_create_tmp_dir():\n    \"\"\"\n    Get or create a temporary directory which will be removed once python process exit.\n    \"\"\"\n    from mlflow.utils.databricks_utils import get_repl_id, is_in_databricks_runtime\n\n    if is_in_databricks_runtime() and get_repl_id() is not None:\n        # Note: For python process attached to databricks notebook, atexit does not work.\n        # The directory returned by `dbutils.entry_point.getReplLocalTempDir()`\n        # will be removed once databricks notebook detaches.\n        # The temp directory is designed to be used by all kinds of applications,\n        # so create a child directory \"mlflow\" for storing mlflow temp data.\n        try:\n            repl_local_tmp_dir = _get_dbutils().entry_point.getReplLocalTempDir()\n        except Exception:\n            repl_local_tmp_dir = os.path.join(\"/tmp\", \"repl_tmp_data\", get_repl_id())\n\n        tmp_dir = os.path.join(repl_local_tmp_dir, \"mlflow\")\n        os.makedirs(tmp_dir, exist_ok=True)\n    else:\n        tmp_dir = tempfile.mkdtemp()\n        # mkdtemp creates a directory with permission 0o700\n        # change it to be 0o777 to ensure it can be seen in spark UDF\n        os.chmod(tmp_dir, 0o777)\n        atexit.register(shutil.rmtree, tmp_dir, ignore_errors=True)\n\n    return tmp_dir\n\n\n@cache_return_value_per_process\ndef get_or_create_nfs_tmp_dir():\n    \"\"\"\n    Get or create a temporary NFS directory which will be removed once python process exit.\n    \"\"\"\n    from mlflow.utils.databricks_utils import get_repl_id, is_in_databricks_runtime\n    from mlflow.utils.nfs_on_spark import get_nfs_cache_root_dir\n\n    nfs_root_dir = get_nfs_cache_root_dir()\n\n    if is_in_databricks_runtime() and get_repl_id() is not None:\n        # Note: In databricks, atexit hook does not work.\n        # The directory returned by `dbutils.entry_point.getReplNFSTempDir()`\n        # will be removed once databricks notebook detaches.\n        # The temp directory is designed to be used by all kinds of applications,\n        # so create a child directory \"mlflow\" for storing mlflow temp data.\n        try:\n            repl_nfs_tmp_dir = _get_dbutils().entry_point.getReplNFSTempDir()\n        except Exception:\n            repl_nfs_tmp_dir = os.path.join(nfs_root_dir, \"repl_tmp_data\", get_repl_id())\n\n        tmp_nfs_dir = os.path.join(repl_nfs_tmp_dir, \"mlflow\")\n        os.makedirs(tmp_nfs_dir, exist_ok=True)\n    else:\n        tmp_nfs_dir = tempfile.mkdtemp(dir=nfs_root_dir)\n        # mkdtemp creates a directory with permission 0o700\n        # change it to be 0o777 to ensure it can be seen in spark UDF\n        os.chmod(tmp_nfs_dir, 0o777)\n        atexit.register(shutil.rmtree, tmp_nfs_dir, ignore_errors=True)\n\n    return tmp_nfs_dir\n\n\ndef write_spark_dataframe_to_parquet_on_local_disk(spark_df, output_path):\n    \"\"\"\n    Write spark dataframe in parquet format to local disk.\n\n    :param spark_df: Spark dataframe\n    :param output_path: path to write the data to\n    \"\"\"\n    from mlflow.utils.databricks_utils import is_in_databricks_runtime\n\n    if is_in_databricks_runtime():\n        dbfs_path = os.path.join(\".mlflow\", \"cache\", str(uuid.uuid4()))\n        spark_df.coalesce(1).write.format(\"parquet\").save(dbfs_path)\n        shutil.copytree(\"/dbfs/\" + dbfs_path, output_path)\n        shutil.rmtree(\"/dbfs/\" + dbfs_path)\n    else:\n        spark_df.coalesce(1).write.format(\"parquet\").save(output_path)\n\n\ndef shutil_copytree_without_file_permissions(src_dir, dst_dir):\n    \"\"\"\n    Copies the directory src_dir into dst_dir, without preserving filesystem permissions\n    \"\"\"\n    for dirpath, dirnames, filenames in os.walk(src_dir):\n        for dirname in dirnames:\n            relative_dir_path = os.path.relpath(os.path.join(dirpath, dirname), src_dir)\n            # For each directory <dirname> immediately under <dirpath>, create an equivalently-named\n            # directory under the destination directory\n            abs_dir_path = os.path.join(dst_dir, relative_dir_path)\n            os.mkdir(abs_dir_path)\n        for filename in filenames:\n            # For each file with name <filename> immediately under <dirpath>, copy that file to\n            # the appropriate location in the destination directory\n            file_path = os.path.join(dirpath, filename)\n            relative_file_path = os.path.relpath(file_path, src_dir)\n            abs_file_path = os.path.join(dst_dir, relative_file_path)\n            shutil.copy2(file_path, abs_file_path)\n\n\ndef contains_path_separator(path):\n    \"\"\"\n    Returns True if a path contains a path separator, False otherwise.\n    \"\"\"\n    return any((sep in path) for sep in (os.path.sep, os.path.altsep) if sep is not None)\n\n\ndef read_chunk(path: os.PathLike, size: int, start_byte: int = 0) -> bytes:\n    \"\"\"\n    Read a chunk of bytes from a file.\n\n    :param path: Path to the file.\n    :param size: The size of the chunk.\n    :param start_byte: The start byte of the chunk.\n    :return: The chunk of bytes.\n    \"\"\"\n    with open(path, \"rb\") as f:\n        if start_byte > 0:\n            f.seek(start_byte)\n        return f.read(size)\n\n\n@contextmanager\ndef remove_on_error(path: os.PathLike, onerror=None):\n    \"\"\"\n    A context manager that removes a file or directory if an exception is raised during execution.\n\n    :param path: Path to the file or directory.\n    :param onerror: A callback function that will be called with the captured exception before\n                    the file or directory is removed. For example, you can use this callback to\n                    log the exception.\n    \"\"\"\n    try:\n        yield\n    except Exception as e:\n        if onerror:\n            onerror(e)\n        if os.path.exists(path):\n            if os.path.isfile(path):\n                os.remove(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        raise\n\n\n@contextmanager\ndef chdir(path: str) -> None:\n    \"\"\"\n    Temporarily change the current working directory to the specified path.\n\n    :param path: The path to use as the temporary working directory.\n    \"\"\"\n    cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(cwd)\n\n\ndef get_total_file_size(path: Union[str, pathlib.Path]) -> Optional[int]:\n    \"\"\"\n    Return the size of all files under given path, including files in subdirectories.\n\n    :param path: The absolute path of a local directory.\n    :return: size in bytes.\n    \"\"\"\n    try:\n        if isinstance(path, pathlib.Path):\n            path = str(path)\n        if not os.path.exists(path):\n            raise MlflowException(\n                message=f\"The given {path} does not exist.\", error_code=INVALID_PARAMETER_VALUE\n            )\n        if not os.path.isdir(path):\n            raise MlflowException(\n                message=f\"The given {path} is not a directory.\", error_code=INVALID_PARAMETER_VALUE\n            )\n\n        total_size = 0\n        for cur_path, dirs, files in os.walk(path):\n            full_paths = [os.path.join(cur_path, file) for file in files]\n            total_size += sum([os.path.getsize(file) for file in full_paths])\n        return total_size\n    except Exception as e:\n        _logger.info(f\"Failed to get the total size of {path} because of error :{e}\")\n        return None\n"], "fixing_code": ["import atexit\nimport codecs\nimport errno\nimport fnmatch\nimport gzip\nimport json\nimport logging\nimport math\nimport os\nimport pathlib\nimport posixpath\nimport shutil\nimport stat\nimport subprocess\nimport sys\nimport tarfile\nimport tempfile\nimport time\nimport urllib.parse\nimport urllib.request\nimport uuid\nfrom concurrent.futures import as_completed\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom subprocess import CalledProcessError, TimeoutExpired\nfrom typing import Optional, Union\nfrom urllib.parse import unquote\nfrom urllib.request import pathname2url\n\nimport yaml\n\nfrom mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE\n\ntry:\n    from yaml import CSafeDumper as YamlSafeDumper\n    from yaml import CSafeLoader as YamlSafeLoader\nexcept ImportError:\n    from yaml import SafeDumper as YamlSafeDumper\n    from yaml import SafeLoader as YamlSafeLoader\n\nfrom mlflow.entities import FileInfo\nfrom mlflow.environment_variables import (\n    _MLFLOW_MPD_NUM_RETRIES,\n    _MLFLOW_MPD_RETRY_INTERVAL_SECONDS,\n    MLFLOW_DOWNLOAD_CHUNK_TIMEOUT,\n    MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR,\n)\nfrom mlflow.exceptions import MissingConfigException, MlflowException\nfrom mlflow.protos.databricks_artifacts_pb2 import ArtifactCredentialType\nfrom mlflow.utils import download_cloud_file_chunk, merge_dicts\nfrom mlflow.utils.databricks_utils import _get_dbutils\nfrom mlflow.utils.os import is_windows\nfrom mlflow.utils.process import cache_return_value_per_process\nfrom mlflow.utils.request_utils import cloud_storage_http_request, download_chunk\nfrom mlflow.utils.rest_utils import augmented_raise_for_status\n\nENCODING = \"utf-8\"\nMAX_PARALLEL_DOWNLOAD_WORKERS = os.cpu_count() * 2\n_PROGRESS_BAR_DISPLAY_THRESHOLD = 500_000_000  # 500 MB\n\n_logger = logging.getLogger(__name__)\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass ArtifactProgressBar:\n    def __init__(self, desc, total, step, **kwargs) -> None:\n        self.desc = desc\n        self.total = total\n        self.step = step\n        self.pbar = None\n        self.progress = 0\n        self.kwargs = kwargs\n\n    def set_pbar(self):\n        if MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR.get():\n            try:\n                from tqdm.auto import tqdm\n\n                self.pbar = tqdm(total=self.total, desc=self.desc, **self.kwargs)\n            except ImportError:\n                pass\n\n    @classmethod\n    def chunks(cls, file_size, desc, chunk_size):\n        bar = cls(\n            desc,\n            total=file_size,\n            step=chunk_size,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n            miniters=1,\n        )\n        if file_size >= _PROGRESS_BAR_DISPLAY_THRESHOLD:\n            bar.set_pbar()\n        return bar\n\n    @classmethod\n    def files(cls, desc, total):\n        bar = cls(desc, total=total, step=1)\n        bar.set_pbar()\n        return bar\n\n    def update(self):\n        if self.pbar:\n            update_step = min(self.total - self.progress, self.step)\n            self.pbar.update(update_step)\n            self.pbar.refresh()\n            self.progress += update_step\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        if self.pbar:\n            self.pbar.close()\n\n\ndef is_directory(name):\n    return os.path.isdir(name)\n\n\ndef is_file(name):\n    return os.path.isfile(name)\n\n\ndef exists(name):\n    return os.path.exists(name)\n\n\ndef list_all(root, filter_func=lambda x: True, full_path=False):\n    \"\"\"\n    List all entities directly under 'dir_name' that satisfy 'filter_func'\n\n    :param root: Name of directory to start search\n    :param filter_func: function or lambda that takes path\n    :param full_path: If True will return results as full path including `root`\n\n    :return: list of all files or directories that satisfy the criteria.\n    \"\"\"\n    if not is_directory(root):\n        raise Exception(f\"Invalid parent directory '{root}'\")\n    matches = [x for x in os.listdir(root) if filter_func(os.path.join(root, x))]\n    return [os.path.join(root, m) for m in matches] if full_path else matches\n\n\ndef list_subdirs(dir_name, full_path=False):\n    \"\"\"\n    Equivalent to UNIX command:\n      ``find $dir_name -depth 1 -type d``\n\n    :param dir_name: Name of directory to start search\n    :param full_path: If True will return results as full path including `root`\n\n    :return: list of all directories directly under 'dir_name'\n    \"\"\"\n    return list_all(dir_name, os.path.isdir, full_path)\n\n\ndef list_files(dir_name, full_path=False):\n    \"\"\"\n    Equivalent to UNIX command:\n      ``find $dir_name -depth 1 -type f``\n\n    :param dir_name: Name of directory to start search\n    :param full_path: If True will return results as full path including `root`\n\n    :return: list of all files directly under 'dir_name'\n    \"\"\"\n    return list_all(dir_name, os.path.isfile, full_path)\n\n\ndef find(root, name, full_path=False):\n    \"\"\"\n    Search for a file in a root directory. Equivalent to:\n      ``find $root -name \"$name\" -depth 1``\n\n    :param root: Name of root directory for find\n    :param name: Name of file or directory to find directly under root directory\n    :param full_path: If True will return results as full path including `root`\n\n    :return: list of matching files or directories\n    \"\"\"\n    path_name = os.path.join(root, name)\n    return list_all(root, lambda x: x == path_name, full_path)\n\n\ndef mkdir(root, name=None):\n    \"\"\"\n    Make directory with name \"root/name\", or just \"root\" if name is None.\n\n    :param root: Name of parent directory\n    :param name: Optional name of leaf directory\n\n    :return: Path to created directory\n    \"\"\"\n    target = os.path.join(root, name) if name is not None else root\n    try:\n        os.makedirs(target)\n    except OSError as e:\n        if e.errno != errno.EEXIST or not os.path.isdir(target):\n            raise e\n    return target\n\n\ndef make_containing_dirs(path):\n    \"\"\"\n    Create the base directory for a given file path if it does not exist; also creates parent\n    directories.\n    \"\"\"\n    dir_name = os.path.dirname(path)\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n\n\ndef write_yaml(root, file_name, data, overwrite=False, sort_keys=True, ensure_yaml_extension=True):\n    \"\"\"\n    Write dictionary data in yaml format.\n\n    :param root: Directory name.\n    :param file_name: Desired file name.\n    :param data: data to be dumped as yaml format\n    :param overwrite: If True, will overwrite existing files\n    :param ensure_yaml_extension: If True, will automatically add .yaml extension if not given\n    \"\"\"\n    if not exists(root):\n        raise MissingConfigException(f\"Parent directory '{root}' does not exist.\")\n\n    file_path = os.path.join(root, file_name)\n    yaml_file_name = file_path\n    if ensure_yaml_extension and not file_path.endswith(\".yaml\"):\n        yaml_file_name = file_path + \".yaml\"\n\n    if exists(yaml_file_name) and not overwrite:\n        raise Exception(f\"Yaml file '{file_path}' exists as '{yaml_file_name}\")\n\n    try:\n        with codecs.open(yaml_file_name, mode=\"w\", encoding=ENCODING) as yaml_file:\n            yaml.dump(\n                data,\n                yaml_file,\n                default_flow_style=False,\n                allow_unicode=True,\n                sort_keys=sort_keys,\n                Dumper=YamlSafeDumper,\n            )\n    except Exception as e:\n        raise e\n\n\ndef overwrite_yaml(root, file_name, data, ensure_yaml_extension=True):\n    \"\"\"\n    Safely overwrites a preexisting yaml file, ensuring that file contents are not deleted or\n    corrupted if the write fails. This is achieved by writing contents to a temporary file\n    and moving the temporary file to replace the preexisting file, rather than opening the\n    preexisting file for a direct write.\n\n    :param root: Directory name.\n    :param file_name: File name.\n    :param data: The data to write, represented as a dictionary.\n    :param ensure_yaml_extension: If True, Will automatically add .yaml extension if not given\n    \"\"\"\n    tmp_file_path = None\n    original_file_path = os.path.join(root, file_name)\n    original_file_mode = os.stat(original_file_path).st_mode\n    try:\n        tmp_file_fd, tmp_file_path = tempfile.mkstemp(suffix=\"file.yaml\")\n        os.close(tmp_file_fd)\n        write_yaml(\n            root=get_parent_dir(tmp_file_path),\n            file_name=os.path.basename(tmp_file_path),\n            data=data,\n            overwrite=True,\n            sort_keys=True,\n            ensure_yaml_extension=ensure_yaml_extension,\n        )\n        shutil.move(tmp_file_path, original_file_path)\n        # restores original file permissions, see https://docs.python.org/3/library/tempfile.html#tempfile.mkstemp\n        os.chmod(original_file_path, original_file_mode)\n    finally:\n        if tmp_file_path is not None and os.path.exists(tmp_file_path):\n            os.remove(tmp_file_path)\n\n\ndef read_yaml(root, file_name):\n    \"\"\"\n    Read data from yaml file and return as dictionary\n\n    :param root: Directory name\n    :param file_name: File name. Expects to have '.yaml' extension\n\n    :return: Data in yaml file as dictionary\n    \"\"\"\n    if not exists(root):\n        raise MissingConfigException(\n            f\"Cannot read '{file_name}'. Parent dir '{root}' does not exist.\"\n        )\n\n    file_path = os.path.join(root, file_name)\n    if not exists(file_path):\n        raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n    try:\n        with codecs.open(file_path, mode=\"r\", encoding=ENCODING) as yaml_file:\n            return yaml.load(yaml_file, Loader=YamlSafeLoader)\n    except Exception as e:\n        raise e\n\n\nclass UniqueKeyLoader(YamlSafeLoader):\n    def construct_mapping(self, node, deep=False):\n        mapping = set()\n        for key_node, _ in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            if key in mapping:\n                raise ValueError(f\"Duplicate '{key}' key found in YAML.\")\n            mapping.add(key)\n        return super().construct_mapping(node, deep)\n\n\ndef render_and_merge_yaml(root, template_name, context_name):\n    \"\"\"\n    Renders a Jinja2-templated YAML file based on a YAML context file, merge them, and return\n    result as a dictionary.\n\n    :param root: Root directory of the YAML files\n    :param template_name: Name of the template file\n    :param context_name: Name of the context file\n    :return: Data in yaml file as dictionary\n    \"\"\"\n    from jinja2 import FileSystemLoader, StrictUndefined\n    from jinja2.sandbox import SandboxedEnvironment\n\n    template_path = os.path.join(root, template_name)\n    context_path = os.path.join(root, context_name)\n\n    for path in (template_path, context_path):\n        if not pathlib.Path(path).is_file():\n            raise MissingConfigException(f\"Yaml file '{path}' does not exist.\")\n\n    j2_env = SandboxedEnvironment(\n        loader=FileSystemLoader(root, encoding=ENCODING),\n        undefined=StrictUndefined,\n        line_comment_prefix=\"#\",\n    )\n\n    def from_json(input_var):\n        with open(input_var, encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    j2_env.filters[\"from_json\"] = from_json\n    # Compute final source of context file (e.g. my-profile.yml), applying Jinja filters\n    # like from_json as needed to load context information from files, then load into a dict\n    context_source = j2_env.get_template(context_name).render({})\n    context_dict = yaml.load(context_source, Loader=UniqueKeyLoader) or {}\n\n    # Substitute parameters from context dict into template\n    source = j2_env.get_template(template_name).render(context_dict)\n    rendered_template_dict = yaml.load(source, Loader=UniqueKeyLoader)\n    return merge_dicts(rendered_template_dict, context_dict)\n\n\ndef read_parquet_as_pandas_df(data_parquet_path: str):\n    \"\"\"\n    Deserialize and load the specified parquet file as a Pandas DataFrame.\n\n    :param data_parquet_path: String, path object (implementing os.PathLike[str]),\n    or file-like object implementing a binary read() function. The string\n    could be a URL. Valid URL schemes include http, ftp, s3, gs, and file.\n    For file URLs, a host is expected. A local file could\n    be: file://localhost/path/to/table.parquet. A file URL can also be a path to a\n    directory that contains multiple partitioned parquet files. Pyarrow\n    support paths to directories as well as file URLs. A directory\n    path could be: file://localhost/path/to/tables or s3://bucket/partition_dir.\n    :return: pandas dataframe\n    \"\"\"\n    import pandas as pd\n\n    return pd.read_parquet(data_parquet_path, engine=\"pyarrow\")\n\n\ndef write_pandas_df_as_parquet(df, data_parquet_path: str):\n    \"\"\"\n    Write a DataFrame to the binary parquet format.\n\n    :param df: pandas data frame.\n    :param data_parquet_path: String, path object (implementing os.PathLike[str]),\n    or file-like object implementing a binary write() function.\n    \"\"\"\n    df.to_parquet(data_parquet_path, engine=\"pyarrow\")\n\n\nclass TempDir:\n    def __init__(self, chdr=False, remove_on_exit=True):\n        self._dir = None\n        self._path = None\n        self._chdr = chdr\n        self._remove = remove_on_exit\n\n    def __enter__(self):\n        self._path = os.path.abspath(create_tmp_dir())\n        assert os.path.exists(self._path)\n        if self._chdr:\n            self._dir = os.path.abspath(os.getcwd())\n            os.chdir(self._path)\n        return self\n\n    def __exit__(self, tp, val, traceback):\n        if self._chdr and self._dir:\n            os.chdir(self._dir)\n            self._dir = None\n        if self._remove and os.path.exists(self._path):\n            shutil.rmtree(self._path)\n\n        assert not self._remove or not os.path.exists(self._path)\n        assert os.path.exists(os.getcwd())\n\n    def path(self, *path):\n        return os.path.join(\"./\", *path) if self._chdr else os.path.join(self._path, *path)\n\n\ndef read_file_lines(parent_path, file_name):\n    \"\"\"\n    Return the contents of the file as an array where each element is a separate line.\n\n    :param parent_path: Full path to the directory that contains the file.\n    :param file_name: Leaf file name.\n\n    :return: All lines in the file as an array.\n    \"\"\"\n    file_path = os.path.join(parent_path, file_name)\n    with codecs.open(file_path, mode=\"r\", encoding=ENCODING) as f:\n        return f.readlines()\n\n\ndef read_file(parent_path, file_name):\n    \"\"\"\n    Return the contents of the file.\n\n    :param parent_path: Full path to the directory that contains the file.\n    :param file_name: Leaf file name.\n\n    :return: The contents of the file.\n    \"\"\"\n    file_path = os.path.join(parent_path, file_name)\n    with codecs.open(file_path, mode=\"r\", encoding=ENCODING) as f:\n        return f.read()\n\n\ndef get_file_info(path, rel_path):\n    \"\"\"\n    Returns file meta data : location, size, ... etc\n\n    :param path: Path to artifact\n\n    :return: `FileInfo` object\n    \"\"\"\n    if is_directory(path):\n        return FileInfo(rel_path, True, None)\n    else:\n        return FileInfo(rel_path, False, os.path.getsize(path))\n\n\ndef get_relative_path(root_path, target_path):\n    \"\"\"\n    Remove root path common prefix and return part of `path` relative to `root_path`.\n\n    :param root_path: Root path\n    :param target_path: Desired path for common prefix removal\n\n    :return: Path relative to root_path\n    \"\"\"\n    if len(root_path) > len(target_path):\n        raise Exception(f\"Root path '{root_path}' longer than target path '{target_path}'\")\n    common_prefix = os.path.commonprefix([root_path, target_path])\n    return os.path.relpath(target_path, common_prefix)\n\n\ndef mv(target, new_parent):\n    shutil.move(target, new_parent)\n\n\ndef write_to(filename, data):\n    with codecs.open(filename, mode=\"w\", encoding=ENCODING) as handle:\n        handle.write(data)\n\n\ndef append_to(filename, data):\n    with open(filename, \"a\") as handle:\n        handle.write(data)\n\n\ndef make_tarfile(output_filename, source_dir, archive_name, custom_filter=None):\n    # Helper for filtering out modification timestamps\n    def _filter_timestamps(tar_info):\n        tar_info.mtime = 0\n        return tar_info if custom_filter is None else custom_filter(tar_info)\n\n    unzipped_file_handle, unzipped_filename = tempfile.mkstemp()\n    try:\n        with tarfile.open(unzipped_filename, \"w\") as tar:\n            tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n        # When gzipping the tar, don't include the tar's filename or modification time in the\n        # zipped archive (see https://docs.python.org/3/library/gzip.html#gzip.GzipFile)\n        with gzip.GzipFile(\n            filename=\"\", fileobj=open(output_filename, \"wb\"), mode=\"wb\", mtime=0\n        ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar:\n            gzipped_tar.write(tar.read())\n    finally:\n        os.close(unzipped_file_handle)\n\n\ndef _copy_project(src_path, dst_path=\"\"):\n    \"\"\"\n    Internal function used to copy MLflow project during development.\n\n    Copies the content of the whole directory tree except patterns defined in .dockerignore.\n    The MLflow is assumed to be accessible as a local directory in this case.\n\n\n    :param dst_path: MLflow will be copied here\n    :return: name of the MLflow project directory\n    \"\"\"\n\n    def _docker_ignore(mlflow_root):\n        docker_ignore = os.path.join(mlflow_root, \".dockerignore\")\n        patterns = []\n        if os.path.exists(docker_ignore):\n            with open(docker_ignore) as f:\n                patterns = [x.strip() for x in f.readlines()]\n\n        def ignore(_, names):\n            res = set()\n            for p in patterns:\n                res.update(set(fnmatch.filter(names, p)))\n            return list(res)\n\n        return ignore if patterns else None\n\n    mlflow_dir = \"mlflow-project\"\n    # check if we have project root\n    assert os.path.isfile(os.path.join(src_path, \"setup.py\")), \"file not found \" + str(\n        os.path.abspath(os.path.join(src_path, \"setup.py\"))\n    )\n    shutil.copytree(src_path, os.path.join(dst_path, mlflow_dir), ignore=_docker_ignore(src_path))\n    return mlflow_dir\n\n\ndef _copy_file_or_tree(src, dst, dst_dir=None):\n    \"\"\"\n    :return: The path to the copied artifacts, relative to `dst`\n    \"\"\"\n    dst_subpath = os.path.basename(os.path.abspath(src))\n    if dst_dir is not None:\n        dst_subpath = os.path.join(dst_dir, dst_subpath)\n    dst_path = os.path.join(dst, dst_subpath)\n    if os.path.isfile(src):\n        dst_dirpath = os.path.dirname(dst_path)\n        if not os.path.exists(dst_dirpath):\n            os.makedirs(dst_dirpath)\n        shutil.copy(src=src, dst=dst_path)\n    else:\n        shutil.copytree(src=src, dst=dst_path, ignore=shutil.ignore_patterns(\"__pycache__\"))\n    return dst_subpath\n\n\ndef _get_local_project_dir_size(project_path):\n    \"\"\"\n    Internal function for reporting the size of a local project directory before copying to\n    destination for cli logging reporting to stdout.\n    :param project_path: local path of the project directory\n    :return: directory file sizes in KB, rounded to single decimal point for legibility\n    \"\"\"\n\n    total_size = 0\n    for root, _, files in os.walk(project_path):\n        for f in files:\n            path = os.path.join(root, f)\n            total_size += os.path.getsize(path)\n    return round(total_size / 1024.0, 1)\n\n\ndef _get_local_file_size(file):\n    \"\"\"\n    Get the size of a local file in KB\n    \"\"\"\n    return round(os.path.getsize(file) / 1024.0, 1)\n\n\ndef get_parent_dir(path):\n    return os.path.abspath(os.path.join(path, os.pardir))\n\n\ndef relative_path_to_artifact_path(path):\n    if os.path == posixpath:\n        return path\n    if os.path.abspath(path) == path:\n        raise Exception(\"This method only works with relative paths.\")\n    return unquote(pathname2url(path))\n\n\ndef path_to_local_file_uri(path):\n    \"\"\"\n    Convert local filesystem path to local file uri.\n    \"\"\"\n    return pathlib.Path(os.path.abspath(path)).as_uri()\n\n\ndef path_to_local_sqlite_uri(path):\n    \"\"\"\n    Convert local filesystem path to sqlite uri.\n    \"\"\"\n    path = posixpath.abspath(pathname2url(os.path.abspath(path)))\n    prefix = \"sqlite://\" if sys.platform == \"win32\" else \"sqlite:///\"\n    return prefix + path\n\n\ndef local_file_uri_to_path(uri):\n    \"\"\"\n    Convert URI to local filesystem path.\n    No-op if the uri does not have the expected scheme.\n    \"\"\"\n    path = uri\n    if uri.startswith(\"file:\"):\n        parsed_path = urllib.parse.urlparse(uri)\n        path = parsed_path.path\n        # Fix for retaining server name in UNC path.\n        if is_windows() and parsed_path.netloc:\n            return urllib.request.url2pathname(rf\"\\\\{parsed_path.netloc}{path}\")\n    return urllib.request.url2pathname(path)\n\n\ndef get_local_path_or_none(path_or_uri):\n    \"\"\"Check if the argument is a local path (no scheme or file:///) and return local path if true,\n    None otherwise.\n    \"\"\"\n    parsed_uri = urllib.parse.urlparse(path_or_uri)\n    if len(parsed_uri.scheme) == 0 or parsed_uri.scheme == \"file\" and len(parsed_uri.netloc) == 0:\n        return local_file_uri_to_path(path_or_uri)\n    else:\n        return None\n\n\ndef yield_file_in_chunks(file, chunk_size=100000000):\n    \"\"\"\n    Generator to chunk-ify the inputted file based on the chunk-size.\n    \"\"\"\n    with open(file, \"rb\") as f:\n        while True:\n            chunk = f.read(chunk_size)\n            if chunk:\n                yield chunk\n            else:\n                break\n\n\ndef download_file_using_http_uri(http_uri, download_path, chunk_size=100000000, headers=None):\n    \"\"\"\n    Downloads a file specified using the `http_uri` to a local `download_path`. This function\n    uses a `chunk_size` to ensure an OOM error is not raised a large file is downloaded.\n\n    Note : This function is meant to download files using presigned urls from various cloud\n            providers.\n    \"\"\"\n    if headers is None:\n        headers = {}\n    with cloud_storage_http_request(\"get\", http_uri, stream=True, headers=headers) as response:\n        augmented_raise_for_status(response)\n        with open(download_path, \"wb\") as output_file:\n            for chunk in response.iter_content(chunk_size=chunk_size):\n                if not chunk:\n                    break\n                output_file.write(chunk)\n\n\n@dataclass(frozen=True)\nclass _Chunk:\n    index: int\n    start: int\n    end: int\n    path: str\n\n\ndef _yield_chunks(path, file_size, chunk_size):\n    num_requests = int(math.ceil(file_size / float(chunk_size)))\n    for i in range(num_requests):\n        range_start = i * chunk_size\n        range_end = min(range_start + chunk_size - 1, file_size - 1)\n        yield _Chunk(i, range_start, range_end, path)\n\n\ndef parallelized_download_file_using_http_uri(\n    thread_pool_executor,\n    http_uri,\n    download_path,\n    remote_file_path,\n    file_size,\n    uri_type,\n    chunk_size,\n    env,\n    headers=None,\n):\n    \"\"\"\n    Downloads a file specified using the `http_uri` to a local `download_path`. This function\n    sends multiple requests in parallel each specifying its own desired byte range as a header,\n    then reconstructs the file from the downloaded chunks. This allows for downloads of large files\n    without OOM risk.\n\n    Note : This function is meant to download files using presigned urls from various cloud\n            providers.\n    Returns a dict of chunk index : exception, if one was thrown for that index.\n    \"\"\"\n\n    def run_download(chunk: _Chunk):\n        try:\n            subprocess.run(\n                [\n                    sys.executable,\n                    download_cloud_file_chunk.__file__,\n                    \"--range-start\",\n                    str(chunk.start),\n                    \"--range-end\",\n                    str(chunk.end),\n                    \"--headers\",\n                    json.dumps(headers or {}),\n                    \"--download-path\",\n                    download_path,\n                    \"--http-uri\",\n                    http_uri,\n                ],\n                text=True,\n                check=True,\n                capture_output=True,\n                timeout=MLFLOW_DOWNLOAD_CHUNK_TIMEOUT.get(),\n                env=env,\n            )\n        except (TimeoutExpired, CalledProcessError) as e:\n            raise MlflowException(\n                f\"\"\"\n----- stdout -----\n{e.stdout.strip()}\n\n----- stderr -----\n{e.stderr.strip()}\n\"\"\"\n            ) from e\n\n    chunks = _yield_chunks(remote_file_path, file_size, chunk_size)\n    # Create file if it doesn't exist or erase the contents if it does. We should do this here\n    # before sending to the workers so they can each individually seek to their respective positions\n    # and write chunks without overwriting.\n    with open(download_path, \"w\"):\n        pass\n    if uri_type == ArtifactCredentialType.GCP_SIGNED_URL or uri_type is None:\n        chunk = next(chunks)\n        # GCP files could be transcoded, in which case the range header is ignored.\n        # Test if this is the case by downloading one chunk and seeing if it's larger than the\n        # requested size. If yes, let that be the file; if not, continue downloading more chunks.\n        download_chunk(\n            range_start=chunk.start,\n            range_end=chunk.end,\n            headers=headers,\n            download_path=download_path,\n            http_uri=http_uri,\n        )\n        downloaded_size = os.path.getsize(download_path)\n        # If downloaded size was equal to the chunk size it would have been downloaded serially,\n        # so we don't need to consider this here\n        if downloaded_size > chunk_size:\n            return {}\n\n    futures = {thread_pool_executor.submit(run_download, chunk): chunk for chunk in chunks}\n    failed_downloads = {}\n    with ArtifactProgressBar.chunks(file_size, f\"Downloading {download_path}\", chunk_size) as pbar:\n        for future in as_completed(futures):\n            chunk = futures[future]\n            try:\n                future.result()\n            except Exception as e:\n                _logger.debug(\n                    f\"Failed to download chunk {chunk.index} for {chunk.path}: {e}. \"\n                    f\"The download of this chunk will be retried later.\"\n                )\n                failed_downloads[chunk] = future.exception()\n            else:\n                pbar.update()\n\n    return failed_downloads\n\n\ndef download_chunk_retries(*, chunks, http_uri, headers, download_path):\n    num_retries = _MLFLOW_MPD_NUM_RETRIES.get()\n    interval = _MLFLOW_MPD_RETRY_INTERVAL_SECONDS.get()\n    for chunk in chunks:\n        _logger.info(f\"Retrying download of chunk {chunk.index} for {chunk.path}\")\n        for retry in range(num_retries):\n            try:\n                download_chunk(\n                    range_start=chunk.start,\n                    range_end=chunk.end,\n                    headers=headers,\n                    download_path=download_path,\n                    http_uri=http_uri,\n                )\n                _logger.info(f\"Successfully downloaded chunk {chunk.index} for {chunk.path}\")\n                break\n            except Exception:\n                if retry == num_retries - 1:\n                    raise\n            time.sleep(interval)\n\n\ndef _handle_readonly_on_windows(func, path, exc_info):\n    \"\"\"\n    This function should not be called directly but should be passed to `onerror` of\n    `shutil.rmtree` in order to reattempt the removal of a read-only file after making\n    it writable on Windows.\n\n    References:\n    - https://bugs.python.org/issue19643\n    - https://bugs.python.org/issue43657\n    \"\"\"\n    exc_type, exc_value = exc_info[:2]\n    should_reattempt = (\n        os.name == \"nt\"\n        and func in (os.unlink, os.rmdir)\n        and issubclass(exc_type, PermissionError)\n        and exc_value.winerror == 5\n    )\n    if not should_reattempt:\n        raise exc_value\n    os.chmod(path, stat.S_IWRITE)\n    func(path)\n\n\ndef _get_tmp_dir():\n    from mlflow.utils.databricks_utils import get_repl_id, is_in_databricks_runtime\n\n    if is_in_databricks_runtime():\n        try:\n            return _get_dbutils().entry_point.getReplLocalTempDir()\n        except Exception:\n            pass\n\n        if repl_id := get_repl_id():\n            return os.path.join(\"/tmp\", \"repl_tmp_data\", repl_id)\n\n    return None\n\n\ndef create_tmp_dir():\n    if directory := _get_tmp_dir():\n        os.makedirs(directory, exist_ok=True)\n        return tempfile.mkdtemp(dir=directory)\n\n    return tempfile.mkdtemp()\n\n\n@cache_return_value_per_process\ndef get_or_create_tmp_dir():\n    \"\"\"\n    Get or create a temporary directory which will be removed once python process exit.\n    \"\"\"\n    from mlflow.utils.databricks_utils import get_repl_id, is_in_databricks_runtime\n\n    if is_in_databricks_runtime() and get_repl_id() is not None:\n        # Note: For python process attached to databricks notebook, atexit does not work.\n        # The directory returned by `dbutils.entry_point.getReplLocalTempDir()`\n        # will be removed once databricks notebook detaches.\n        # The temp directory is designed to be used by all kinds of applications,\n        # so create a child directory \"mlflow\" for storing mlflow temp data.\n        try:\n            repl_local_tmp_dir = _get_dbutils().entry_point.getReplLocalTempDir()\n        except Exception:\n            repl_local_tmp_dir = os.path.join(\"/tmp\", \"repl_tmp_data\", get_repl_id())\n\n        tmp_dir = os.path.join(repl_local_tmp_dir, \"mlflow\")\n        os.makedirs(tmp_dir, exist_ok=True)\n    else:\n        tmp_dir = tempfile.mkdtemp()\n        # mkdtemp creates a directory with permission 0o700\n        # change it to be 0o777 to ensure it can be seen in spark UDF\n        os.chmod(tmp_dir, 0o777)\n        atexit.register(shutil.rmtree, tmp_dir, ignore_errors=True)\n\n    return tmp_dir\n\n\n@cache_return_value_per_process\ndef get_or_create_nfs_tmp_dir():\n    \"\"\"\n    Get or create a temporary NFS directory which will be removed once python process exit.\n    \"\"\"\n    from mlflow.utils.databricks_utils import get_repl_id, is_in_databricks_runtime\n    from mlflow.utils.nfs_on_spark import get_nfs_cache_root_dir\n\n    nfs_root_dir = get_nfs_cache_root_dir()\n\n    if is_in_databricks_runtime() and get_repl_id() is not None:\n        # Note: In databricks, atexit hook does not work.\n        # The directory returned by `dbutils.entry_point.getReplNFSTempDir()`\n        # will be removed once databricks notebook detaches.\n        # The temp directory is designed to be used by all kinds of applications,\n        # so create a child directory \"mlflow\" for storing mlflow temp data.\n        try:\n            repl_nfs_tmp_dir = _get_dbutils().entry_point.getReplNFSTempDir()\n        except Exception:\n            repl_nfs_tmp_dir = os.path.join(nfs_root_dir, \"repl_tmp_data\", get_repl_id())\n\n        tmp_nfs_dir = os.path.join(repl_nfs_tmp_dir, \"mlflow\")\n        os.makedirs(tmp_nfs_dir, exist_ok=True)\n    else:\n        tmp_nfs_dir = tempfile.mkdtemp(dir=nfs_root_dir)\n        # mkdtemp creates a directory with permission 0o700\n        # change it to be 0o777 to ensure it can be seen in spark UDF\n        os.chmod(tmp_nfs_dir, 0o777)\n        atexit.register(shutil.rmtree, tmp_nfs_dir, ignore_errors=True)\n\n    return tmp_nfs_dir\n\n\ndef write_spark_dataframe_to_parquet_on_local_disk(spark_df, output_path):\n    \"\"\"\n    Write spark dataframe in parquet format to local disk.\n\n    :param spark_df: Spark dataframe\n    :param output_path: path to write the data to\n    \"\"\"\n    from mlflow.utils.databricks_utils import is_in_databricks_runtime\n\n    if is_in_databricks_runtime():\n        dbfs_path = os.path.join(\".mlflow\", \"cache\", str(uuid.uuid4()))\n        spark_df.coalesce(1).write.format(\"parquet\").save(dbfs_path)\n        shutil.copytree(\"/dbfs/\" + dbfs_path, output_path)\n        shutil.rmtree(\"/dbfs/\" + dbfs_path)\n    else:\n        spark_df.coalesce(1).write.format(\"parquet\").save(output_path)\n\n\ndef shutil_copytree_without_file_permissions(src_dir, dst_dir):\n    \"\"\"\n    Copies the directory src_dir into dst_dir, without preserving filesystem permissions\n    \"\"\"\n    for dirpath, dirnames, filenames in os.walk(src_dir):\n        for dirname in dirnames:\n            relative_dir_path = os.path.relpath(os.path.join(dirpath, dirname), src_dir)\n            # For each directory <dirname> immediately under <dirpath>, create an equivalently-named\n            # directory under the destination directory\n            abs_dir_path = os.path.join(dst_dir, relative_dir_path)\n            os.mkdir(abs_dir_path)\n        for filename in filenames:\n            # For each file with name <filename> immediately under <dirpath>, copy that file to\n            # the appropriate location in the destination directory\n            file_path = os.path.join(dirpath, filename)\n            relative_file_path = os.path.relpath(file_path, src_dir)\n            abs_file_path = os.path.join(dst_dir, relative_file_path)\n            shutil.copy2(file_path, abs_file_path)\n\n\ndef contains_path_separator(path):\n    \"\"\"\n    Returns True if a path contains a path separator, False otherwise.\n    \"\"\"\n    return any((sep in path) for sep in (os.path.sep, os.path.altsep) if sep is not None)\n\n\ndef read_chunk(path: os.PathLike, size: int, start_byte: int = 0) -> bytes:\n    \"\"\"\n    Read a chunk of bytes from a file.\n\n    :param path: Path to the file.\n    :param size: The size of the chunk.\n    :param start_byte: The start byte of the chunk.\n    :return: The chunk of bytes.\n    \"\"\"\n    with open(path, \"rb\") as f:\n        if start_byte > 0:\n            f.seek(start_byte)\n        return f.read(size)\n\n\n@contextmanager\ndef remove_on_error(path: os.PathLike, onerror=None):\n    \"\"\"\n    A context manager that removes a file or directory if an exception is raised during execution.\n\n    :param path: Path to the file or directory.\n    :param onerror: A callback function that will be called with the captured exception before\n                    the file or directory is removed. For example, you can use this callback to\n                    log the exception.\n    \"\"\"\n    try:\n        yield\n    except Exception as e:\n        if onerror:\n            onerror(e)\n        if os.path.exists(path):\n            if os.path.isfile(path):\n                os.remove(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        raise\n\n\n@contextmanager\ndef chdir(path: str) -> None:\n    \"\"\"\n    Temporarily change the current working directory to the specified path.\n\n    :param path: The path to use as the temporary working directory.\n    \"\"\"\n    cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(cwd)\n\n\ndef get_total_file_size(path: Union[str, pathlib.Path]) -> Optional[int]:\n    \"\"\"\n    Return the size of all files under given path, including files in subdirectories.\n\n    :param path: The absolute path of a local directory.\n    :return: size in bytes.\n    \"\"\"\n    try:\n        if isinstance(path, pathlib.Path):\n            path = str(path)\n        if not os.path.exists(path):\n            raise MlflowException(\n                message=f\"The given {path} does not exist.\", error_code=INVALID_PARAMETER_VALUE\n            )\n        if not os.path.isdir(path):\n            raise MlflowException(\n                message=f\"The given {path} is not a directory.\", error_code=INVALID_PARAMETER_VALUE\n            )\n\n        total_size = 0\n        for cur_path, dirs, files in os.walk(path):\n            full_paths = [os.path.join(cur_path, file) for file in files]\n            total_size += sum([os.path.getsize(file) for file in full_paths])\n        return total_size\n    except Exception as e:\n        _logger.info(f\"Failed to get the total size of {path} because of error :{e}\")\n        return None\n"], "filenames": ["mlflow/utils/file_utils.py"], "buggy_code_start_loc": [332], "buggy_code_end_loc": [344], "fixing_code_start_loc": [332], "fixing_code_end_loc": [345], "type": "CWE-77", "message": "with only one user interaction(download a malicious config), attackers can gain full command execution on the victim system.", "other": {"cve": {"id": "CVE-2023-6940", "sourceIdentifier": "security@huntr.dev", "published": "2023-12-19T02:15:45.050", "lastModified": "2023-12-29T16:58:04.290", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "with only one user interaction(download a malicious config), attackers can gain full command execution on the victim system."}, {"lang": "es", "value": "Con solo una interacci\u00f3n del usuario (descargar una configuraci\u00f3n maliciosa), los atacantes pueden obtener la ejecuci\u00f3n completa del comando en el sistema v\u00edctima."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}], "cvssMetricV30": [{"source": "security@huntr.dev", "type": "Secondary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:L/UI:R/S:C/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.0, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 2.3, "impactScore": 6.0}]}, "weaknesses": [{"source": "security@huntr.dev", "type": "Primary", "description": [{"lang": "en", "value": "CWE-77"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:lfprojects:mlflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.9.2", "matchCriteriaId": "6B5585E2-CC70-4BED-AA89-B791F081ACFC"}]}]}], "references": [{"url": "https://github.com/mlflow/mlflow/commit/5139b1087d686fa52e2b087e09da66aff86297b1", "source": "security@huntr.dev", "tags": ["Patch"]}, {"url": "https://huntr.com/bounties/c6f59480-ce47-4f78-a3dc-4bd8ca15029c", "source": "security@huntr.dev", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/mlflow/mlflow/commit/5139b1087d686fa52e2b087e09da66aff86297b1"}}