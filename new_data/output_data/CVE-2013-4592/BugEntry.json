{"buggy_code": ["/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * This module enables machines with Intel VT-x extensions to run virtual\n * machines without emulation or binary translation.\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include \"iodev.h\"\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/percpu.h>\n#include <linux/mm.h>\n#include <linux/miscdevice.h>\n#include <linux/vmalloc.h>\n#include <linux/reboot.h>\n#include <linux/debugfs.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/syscore_ops.h>\n#include <linux/cpu.h>\n#include <linux/sched.h>\n#include <linux/cpumask.h>\n#include <linux/smp.h>\n#include <linux/anon_inodes.h>\n#include <linux/profile.h>\n#include <linux/kvm_para.h>\n#include <linux/pagemap.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/bitops.h>\n#include <linux/spinlock.h>\n#include <linux/compat.h>\n#include <linux/srcu.h>\n#include <linux/hugetlb.h>\n#include <linux/slab.h>\n#include <linux/sort.h>\n#include <linux/bsearch.h>\n\n#include <asm/processor.h>\n#include <asm/io.h>\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n\n#include \"coalesced_mmio.h\"\n#include \"async_pf.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/kvm.h>\n\nMODULE_AUTHOR(\"Qumranet\");\nMODULE_LICENSE(\"GPL\");\n\n/*\n * Ordering of locks:\n *\n * \t\tkvm->lock --> kvm->slots_lock --> kvm->irq_lock\n */\n\nDEFINE_RAW_SPINLOCK(kvm_lock);\nLIST_HEAD(vm_list);\n\nstatic cpumask_var_t cpus_hardware_enabled;\nstatic int kvm_usage_count = 0;\nstatic atomic_t hardware_enable_failed;\n\nstruct kmem_cache *kvm_vcpu_cache;\nEXPORT_SYMBOL_GPL(kvm_vcpu_cache);\n\nstatic __read_mostly struct preempt_ops kvm_preempt_ops;\n\nstruct dentry *kvm_debugfs_dir;\n\nstatic long kvm_vcpu_ioctl(struct file *file, unsigned int ioctl,\n\t\t\t   unsigned long arg);\n#ifdef CONFIG_COMPAT\nstatic long kvm_vcpu_compat_ioctl(struct file *file, unsigned int ioctl,\n\t\t\t\t  unsigned long arg);\n#endif\nstatic int hardware_enable_all(void);\nstatic void hardware_disable_all(void);\n\nstatic void kvm_io_bus_destroy(struct kvm_io_bus *bus);\n\nbool kvm_rebooting;\nEXPORT_SYMBOL_GPL(kvm_rebooting);\n\nstatic bool largepages_enabled = true;\n\nbool kvm_is_mmio_pfn(pfn_t pfn)\n{\n\tif (pfn_valid(pfn)) {\n\t\tint reserved;\n\t\tstruct page *tail = pfn_to_page(pfn);\n\t\tstruct page *head = compound_trans_head(tail);\n\t\treserved = PageReserved(head);\n\t\tif (head != tail) {\n\t\t\t/*\n\t\t\t * \"head\" is not a dangling pointer\n\t\t\t * (compound_trans_head takes care of that)\n\t\t\t * but the hugepage may have been splitted\n\t\t\t * from under us (and we may not hold a\n\t\t\t * reference count on the head page so it can\n\t\t\t * be reused before we run PageReferenced), so\n\t\t\t * we've to check PageTail before returning\n\t\t\t * what we just read.\n\t\t\t */\n\t\t\tsmp_rmb();\n\t\t\tif (PageTail(tail))\n\t\t\t\treturn reserved;\n\t\t}\n\t\treturn PageReserved(tail);\n\t}\n\n\treturn true;\n}\n\n/*\n * Switches to specified vcpu, until a matching vcpu_put()\n */\nvoid vcpu_load(struct kvm_vcpu *vcpu)\n{\n\tint cpu;\n\n\tmutex_lock(&vcpu->mutex);\n\tif (unlikely(vcpu->pid != current->pids[PIDTYPE_PID].pid)) {\n\t\t/* The thread running this VCPU changed. */\n\t\tstruct pid *oldpid = vcpu->pid;\n\t\tstruct pid *newpid = get_task_pid(current, PIDTYPE_PID);\n\t\trcu_assign_pointer(vcpu->pid, newpid);\n\t\tsynchronize_rcu();\n\t\tput_pid(oldpid);\n\t}\n\tcpu = get_cpu();\n\tpreempt_notifier_register(&vcpu->preempt_notifier);\n\tkvm_arch_vcpu_load(vcpu, cpu);\n\tput_cpu();\n}\n\nvoid vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tpreempt_disable();\n\tkvm_arch_vcpu_put(vcpu);\n\tpreempt_notifier_unregister(&vcpu->preempt_notifier);\n\tpreempt_enable();\n\tmutex_unlock(&vcpu->mutex);\n}\n\nstatic void ack_flush(void *_completed)\n{\n}\n\nstatic bool make_all_cpus_request(struct kvm *kvm, unsigned int req)\n{\n\tint i, cpu, me;\n\tcpumask_var_t cpus;\n\tbool called = true;\n\tstruct kvm_vcpu *vcpu;\n\n\tzalloc_cpumask_var(&cpus, GFP_ATOMIC);\n\n\tme = get_cpu();\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_make_request(req, vcpu);\n\t\tcpu = vcpu->cpu;\n\n\t\t/* Set ->requests bit before we read ->mode */\n\t\tsmp_mb();\n\n\t\tif (cpus != NULL && cpu != -1 && cpu != me &&\n\t\t      kvm_vcpu_exiting_guest_mode(vcpu) != OUTSIDE_GUEST_MODE)\n\t\t\tcpumask_set_cpu(cpu, cpus);\n\t}\n\tif (unlikely(cpus == NULL))\n\t\tsmp_call_function_many(cpu_online_mask, ack_flush, NULL, 1);\n\telse if (!cpumask_empty(cpus))\n\t\tsmp_call_function_many(cpus, ack_flush, NULL, 1);\n\telse\n\t\tcalled = false;\n\tput_cpu();\n\tfree_cpumask_var(cpus);\n\treturn called;\n}\n\nvoid kvm_flush_remote_tlbs(struct kvm *kvm)\n{\n\tlong dirty_count = kvm->tlbs_dirty;\n\n\tsmp_mb();\n\tif (make_all_cpus_request(kvm, KVM_REQ_TLB_FLUSH))\n\t\t++kvm->stat.remote_tlb_flush;\n\tcmpxchg(&kvm->tlbs_dirty, dirty_count, 0);\n}\n\nvoid kvm_reload_remote_mmus(struct kvm *kvm)\n{\n\tmake_all_cpus_request(kvm, KVM_REQ_MMU_RELOAD);\n}\n\nint kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)\n{\n\tstruct page *page;\n\tint r;\n\n\tmutex_init(&vcpu->mutex);\n\tvcpu->cpu = -1;\n\tvcpu->kvm = kvm;\n\tvcpu->vcpu_id = id;\n\tvcpu->pid = NULL;\n\tinit_waitqueue_head(&vcpu->wq);\n\tkvm_async_pf_vcpu_init(vcpu);\n\n\tpage = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\tif (!page) {\n\t\tr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tvcpu->run = page_address(page);\n\n\tkvm_vcpu_set_in_spin_loop(vcpu, false);\n\tkvm_vcpu_set_dy_eligible(vcpu, false);\n\n\tr = kvm_arch_vcpu_init(vcpu);\n\tif (r < 0)\n\t\tgoto fail_free_run;\n\treturn 0;\n\nfail_free_run:\n\tfree_page((unsigned long)vcpu->run);\nfail:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_init);\n\nvoid kvm_vcpu_uninit(struct kvm_vcpu *vcpu)\n{\n\tput_pid(vcpu->pid);\n\tkvm_arch_vcpu_uninit(vcpu);\n\tfree_page((unsigned long)vcpu->run);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_uninit);\n\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\nstatic inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)\n{\n\treturn container_of(mn, struct kvm, mmu_notifier);\n}\n\nstatic void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,\n\t\t\t\t\t     struct mm_struct *mm,\n\t\t\t\t\t     unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint need_tlb_flush, idx;\n\n\t/*\n\t * When ->invalidate_page runs, the linux pte has been zapped\n\t * already but the page is still allocated until\n\t * ->invalidate_page returns. So if we increase the sequence\n\t * here the kvm page fault will notice if the spte can't be\n\t * established because the page is going to be freed. If\n\t * instead the kvm page fault establishes the spte before\n\t * ->invalidate_page runs, kvm_unmap_hva will release it\n\t * before returning.\n\t *\n\t * The sequence increase only need to be seen at spin_unlock\n\t * time, and not at spin_lock time.\n\t *\n\t * Increasing the sequence after the spin_unlock would be\n\t * unsafe because the kvm page fault could then establish the\n\t * pte after kvm_unmap_hva returned, without noticing the page\n\t * is going to be freed.\n\t */\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\n\tkvm->mmu_notifier_seq++;\n\tneed_tlb_flush = kvm_unmap_hva(kvm, address) | kvm->tlbs_dirty;\n\t/* we've to flush the tlb before the pages can be freed */\n\tif (need_tlb_flush)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,\n\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpte_t pte)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tkvm->mmu_notifier_seq++;\n\tkvm_set_spte_hva(kvm, address, pte);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,\n\t\t\t\t\t\t    struct mm_struct *mm,\n\t\t\t\t\t\t    unsigned long start,\n\t\t\t\t\t\t    unsigned long end)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint need_tlb_flush = 0, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\t/*\n\t * The count increase must become visible at unlock time as no\n\t * spte can be established without taking the mmu_lock and\n\t * count is also read inside the mmu_lock critical section.\n\t */\n\tkvm->mmu_notifier_count++;\n\tneed_tlb_flush = kvm_unmap_hva_range(kvm, start, end);\n\tneed_tlb_flush |= kvm->tlbs_dirty;\n\t/* we've to flush the tlb before the pages can be freed */\n\tif (need_tlb_flush)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,\n\t\t\t\t\t\t  struct mm_struct *mm,\n\t\t\t\t\t\t  unsigned long start,\n\t\t\t\t\t\t  unsigned long end)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\n\tspin_lock(&kvm->mmu_lock);\n\t/*\n\t * This sequence increase will notify the kvm page fault that\n\t * the page that is going to be mapped in the spte could have\n\t * been freed.\n\t */\n\tkvm->mmu_notifier_seq++;\n\tsmp_wmb();\n\t/*\n\t * The above sequence increase must be visible before the\n\t * below count decrease, which is ensured by the smp_wmb above\n\t * in conjunction with the smp_rmb in mmu_notifier_retry().\n\t */\n\tkvm->mmu_notifier_count--;\n\tspin_unlock(&kvm->mmu_lock);\n\n\tBUG_ON(kvm->mmu_notifier_count < 0);\n}\n\nstatic int kvm_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,\n\t\t\t\t\t      struct mm_struct *mm,\n\t\t\t\t\t      unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint young, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\n\tyoung = kvm_age_hva(kvm, address);\n\tif (young)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn young;\n}\n\nstatic int kvm_mmu_notifier_test_young(struct mmu_notifier *mn,\n\t\t\t\t       struct mm_struct *mm,\n\t\t\t\t       unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint young, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tyoung = kvm_test_age_hva(kvm, address);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn young;\n}\n\nstatic void kvm_mmu_notifier_release(struct mmu_notifier *mn,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tkvm_arch_flush_shadow_all(kvm);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic const struct mmu_notifier_ops kvm_mmu_notifier_ops = {\n\t.invalidate_page\t= kvm_mmu_notifier_invalidate_page,\n\t.invalidate_range_start\t= kvm_mmu_notifier_invalidate_range_start,\n\t.invalidate_range_end\t= kvm_mmu_notifier_invalidate_range_end,\n\t.clear_flush_young\t= kvm_mmu_notifier_clear_flush_young,\n\t.test_young\t\t= kvm_mmu_notifier_test_young,\n\t.change_pte\t\t= kvm_mmu_notifier_change_pte,\n\t.release\t\t= kvm_mmu_notifier_release,\n};\n\nstatic int kvm_init_mmu_notifier(struct kvm *kvm)\n{\n\tkvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;\n\treturn mmu_notifier_register(&kvm->mmu_notifier, current->mm);\n}\n\n#else  /* !(CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER) */\n\nstatic int kvm_init_mmu_notifier(struct kvm *kvm)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER */\n\nstatic void kvm_init_memslots_id(struct kvm *kvm)\n{\n\tint i;\n\tstruct kvm_memslots *slots = kvm->memslots;\n\n\tfor (i = 0; i < KVM_MEM_SLOTS_NUM; i++)\n\t\tslots->id_to_index[i] = slots->memslots[i].id = i;\n}\n\nstatic struct kvm *kvm_create_vm(unsigned long type)\n{\n\tint r, i;\n\tstruct kvm *kvm = kvm_arch_alloc_vm();\n\n\tif (!kvm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tr = kvm_arch_init_vm(kvm, type);\n\tif (r)\n\t\tgoto out_err_nodisable;\n\n\tr = hardware_enable_all();\n\tif (r)\n\t\tgoto out_err_nodisable;\n\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n\tINIT_HLIST_HEAD(&kvm->mask_notifier_list);\n\tINIT_HLIST_HEAD(&kvm->irq_ack_notifier_list);\n#endif\n\n\tr = -ENOMEM;\n\tkvm->memslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\tif (!kvm->memslots)\n\t\tgoto out_err_nosrcu;\n\tkvm_init_memslots_id(kvm);\n\tif (init_srcu_struct(&kvm->srcu))\n\t\tgoto out_err_nosrcu;\n\tfor (i = 0; i < KVM_NR_BUSES; i++) {\n\t\tkvm->buses[i] = kzalloc(sizeof(struct kvm_io_bus),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!kvm->buses[i])\n\t\t\tgoto out_err;\n\t}\n\n\tspin_lock_init(&kvm->mmu_lock);\n\tkvm->mm = current->mm;\n\tatomic_inc(&kvm->mm->mm_count);\n\tkvm_eventfd_init(kvm);\n\tmutex_init(&kvm->lock);\n\tmutex_init(&kvm->irq_lock);\n\tmutex_init(&kvm->slots_lock);\n\tatomic_set(&kvm->users_count, 1);\n\n\tr = kvm_init_mmu_notifier(kvm);\n\tif (r)\n\t\tgoto out_err;\n\n\traw_spin_lock(&kvm_lock);\n\tlist_add(&kvm->vm_list, &vm_list);\n\traw_spin_unlock(&kvm_lock);\n\n\treturn kvm;\n\nout_err:\n\tcleanup_srcu_struct(&kvm->srcu);\nout_err_nosrcu:\n\thardware_disable_all();\nout_err_nodisable:\n\tfor (i = 0; i < KVM_NR_BUSES; i++)\n\t\tkfree(kvm->buses[i]);\n\tkfree(kvm->memslots);\n\tkvm_arch_free_vm(kvm);\n\treturn ERR_PTR(r);\n}\n\n/*\n * Avoid using vmalloc for a small buffer.\n * Should not be used when the size is statically known.\n */\nvoid *kvm_kvzalloc(unsigned long size)\n{\n\tif (size > PAGE_SIZE)\n\t\treturn vzalloc(size);\n\telse\n\t\treturn kzalloc(size, GFP_KERNEL);\n}\n\nvoid kvm_kvfree(const void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\tvfree(addr);\n\telse\n\t\tkfree(addr);\n}\n\nstatic void kvm_destroy_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n\tif (!memslot->dirty_bitmap)\n\t\treturn;\n\n\tkvm_kvfree(memslot->dirty_bitmap);\n\tmemslot->dirty_bitmap = NULL;\n}\n\n/*\n * Free any memory in @free but not in @dont.\n */\nstatic void kvm_free_physmem_slot(struct kvm_memory_slot *free,\n\t\t\t\t  struct kvm_memory_slot *dont)\n{\n\tif (!dont || free->dirty_bitmap != dont->dirty_bitmap)\n\t\tkvm_destroy_dirty_bitmap(free);\n\n\tkvm_arch_free_memslot(free, dont);\n\n\tfree->npages = 0;\n}\n\nvoid kvm_free_physmem(struct kvm *kvm)\n{\n\tstruct kvm_memslots *slots = kvm->memslots;\n\tstruct kvm_memory_slot *memslot;\n\n\tkvm_for_each_memslot(memslot, slots)\n\t\tkvm_free_physmem_slot(memslot, NULL);\n\n\tkfree(kvm->memslots);\n}\n\nstatic void kvm_destroy_vm(struct kvm *kvm)\n{\n\tint i;\n\tstruct mm_struct *mm = kvm->mm;\n\n\tkvm_arch_sync_events(kvm);\n\traw_spin_lock(&kvm_lock);\n\tlist_del(&kvm->vm_list);\n\traw_spin_unlock(&kvm_lock);\n\tkvm_free_irq_routing(kvm);\n\tfor (i = 0; i < KVM_NR_BUSES; i++)\n\t\tkvm_io_bus_destroy(kvm->buses[i]);\n\tkvm_coalesced_mmio_free(kvm);\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\tmmu_notifier_unregister(&kvm->mmu_notifier, kvm->mm);\n#else\n\tkvm_arch_flush_shadow_all(kvm);\n#endif\n\tkvm_arch_destroy_vm(kvm);\n\tkvm_free_physmem(kvm);\n\tcleanup_srcu_struct(&kvm->srcu);\n\tkvm_arch_free_vm(kvm);\n\thardware_disable_all();\n\tmmdrop(mm);\n}\n\nvoid kvm_get_kvm(struct kvm *kvm)\n{\n\tatomic_inc(&kvm->users_count);\n}\nEXPORT_SYMBOL_GPL(kvm_get_kvm);\n\nvoid kvm_put_kvm(struct kvm *kvm)\n{\n\tif (atomic_dec_and_test(&kvm->users_count))\n\t\tkvm_destroy_vm(kvm);\n}\nEXPORT_SYMBOL_GPL(kvm_put_kvm);\n\n\nstatic int kvm_vm_release(struct inode *inode, struct file *filp)\n{\n\tstruct kvm *kvm = filp->private_data;\n\n\tkvm_irqfd_release(kvm);\n\n\tkvm_put_kvm(kvm);\n\treturn 0;\n}\n\n/*\n * Allocation size is twice as large as the actual dirty bitmap size.\n * See x86's kvm_vm_ioctl_get_dirty_log() why this is needed.\n */\nstatic int kvm_create_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n#ifndef CONFIG_S390\n\tunsigned long dirty_bytes = 2 * kvm_dirty_bitmap_bytes(memslot);\n\n\tmemslot->dirty_bitmap = kvm_kvzalloc(dirty_bytes);\n\tif (!memslot->dirty_bitmap)\n\t\treturn -ENOMEM;\n\n#endif /* !CONFIG_S390 */\n\treturn 0;\n}\n\nstatic int cmp_memslot(const void *slot1, const void *slot2)\n{\n\tstruct kvm_memory_slot *s1, *s2;\n\n\ts1 = (struct kvm_memory_slot *)slot1;\n\ts2 = (struct kvm_memory_slot *)slot2;\n\n\tif (s1->npages < s2->npages)\n\t\treturn 1;\n\tif (s1->npages > s2->npages)\n\t\treturn -1;\n\n\treturn 0;\n}\n\n/*\n * Sort the memslots base on its size, so the larger slots\n * will get better fit.\n */\nstatic void sort_memslots(struct kvm_memslots *slots)\n{\n\tint i;\n\n\tsort(slots->memslots, KVM_MEM_SLOTS_NUM,\n\t      sizeof(struct kvm_memory_slot), cmp_memslot, NULL);\n\n\tfor (i = 0; i < KVM_MEM_SLOTS_NUM; i++)\n\t\tslots->id_to_index[slots->memslots[i].id] = i;\n}\n\nvoid update_memslots(struct kvm_memslots *slots, struct kvm_memory_slot *new)\n{\n\tif (new) {\n\t\tint id = new->id;\n\t\tstruct kvm_memory_slot *old = id_to_memslot(slots, id);\n\t\tunsigned long npages = old->npages;\n\n\t\t*old = *new;\n\t\tif (new->npages != npages)\n\t\t\tsort_memslots(slots);\n\t}\n\n\tslots->generation++;\n}\n\nstatic int check_memory_region_flags(struct kvm_userspace_memory_region *mem)\n{\n\tu32 valid_flags = KVM_MEM_LOG_DIRTY_PAGES;\n\n#ifdef KVM_CAP_READONLY_MEM\n\tvalid_flags |= KVM_MEM_READONLY;\n#endif\n\n\tif (mem->flags & ~valid_flags)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n/*\n * Allocate some memory and give it an address in the guest physical address\n * space.\n *\n * Discontiguous memory is allowed, mostly for framebuffers.\n *\n * Must be called holding mmap_sem for write.\n */\nint __kvm_set_memory_region(struct kvm *kvm,\n\t\t\t    struct kvm_userspace_memory_region *mem,\n\t\t\t    int user_alloc)\n{\n\tint r;\n\tgfn_t base_gfn;\n\tunsigned long npages;\n\tunsigned long i;\n\tstruct kvm_memory_slot *memslot;\n\tstruct kvm_memory_slot old, new;\n\tstruct kvm_memslots *slots, *old_memslots;\n\n\tr = check_memory_region_flags(mem);\n\tif (r)\n\t\tgoto out;\n\n\tr = -EINVAL;\n\t/* General sanity checks */\n\tif (mem->memory_size & (PAGE_SIZE - 1))\n\t\tgoto out;\n\tif (mem->guest_phys_addr & (PAGE_SIZE - 1))\n\t\tgoto out;\n\t/* We can read the guest memory with __xxx_user() later on. */\n\tif (user_alloc &&\n\t    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||\n\t     !access_ok(VERIFY_WRITE,\n\t\t\t(void __user *)(unsigned long)mem->userspace_addr,\n\t\t\tmem->memory_size)))\n\t\tgoto out;\n\tif (mem->slot >= KVM_MEM_SLOTS_NUM)\n\t\tgoto out;\n\tif (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)\n\t\tgoto out;\n\n\tmemslot = id_to_memslot(kvm->memslots, mem->slot);\n\tbase_gfn = mem->guest_phys_addr >> PAGE_SHIFT;\n\tnpages = mem->memory_size >> PAGE_SHIFT;\n\n\tr = -EINVAL;\n\tif (npages > KVM_MEM_MAX_NR_PAGES)\n\t\tgoto out;\n\n\tif (!npages)\n\t\tmem->flags &= ~KVM_MEM_LOG_DIRTY_PAGES;\n\n\tnew = old = *memslot;\n\n\tnew.id = mem->slot;\n\tnew.base_gfn = base_gfn;\n\tnew.npages = npages;\n\tnew.flags = mem->flags;\n\n\t/* Disallow changing a memory slot's size. */\n\tr = -EINVAL;\n\tif (npages && old.npages && npages != old.npages)\n\t\tgoto out_free;\n\n\t/* Check for overlaps */\n\tr = -EEXIST;\n\tfor (i = 0; i < KVM_MEMORY_SLOTS; ++i) {\n\t\tstruct kvm_memory_slot *s = &kvm->memslots->memslots[i];\n\n\t\tif (s == memslot || !s->npages)\n\t\t\tcontinue;\n\t\tif (!((base_gfn + npages <= s->base_gfn) ||\n\t\t      (base_gfn >= s->base_gfn + s->npages)))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Free page dirty bitmap if unneeded */\n\tif (!(new.flags & KVM_MEM_LOG_DIRTY_PAGES))\n\t\tnew.dirty_bitmap = NULL;\n\n\tr = -ENOMEM;\n\n\t/* Allocate if a slot is being created */\n\tif (npages && !old.npages) {\n\t\tnew.user_alloc = user_alloc;\n\t\tnew.userspace_addr = mem->userspace_addr;\n\n\t\tif (kvm_arch_create_memslot(&new, npages))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Allocate page dirty bitmap if needed */\n\tif ((new.flags & KVM_MEM_LOG_DIRTY_PAGES) && !new.dirty_bitmap) {\n\t\tif (kvm_create_dirty_bitmap(&new) < 0)\n\t\t\tgoto out_free;\n\t\t/* destroy any largepage mappings for dirty tracking */\n\t}\n\n\tif (!npages) {\n\t\tstruct kvm_memory_slot *slot;\n\n\t\tr = -ENOMEM;\n\t\tslots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!slots)\n\t\t\tgoto out_free;\n\t\tslot = id_to_memslot(slots, mem->slot);\n\t\tslot->flags |= KVM_MEMSLOT_INVALID;\n\n\t\tupdate_memslots(slots, NULL);\n\n\t\told_memslots = kvm->memslots;\n\t\trcu_assign_pointer(kvm->memslots, slots);\n\t\tsynchronize_srcu_expedited(&kvm->srcu);\n\t\t/* From this point no new shadow pages pointing to a deleted\n\t\t * memslot will be created.\n\t\t *\n\t\t * validation of sp->gfn happens in:\n\t\t * \t- gfn_to_hva (kvm_read_guest, gfn_to_pfn)\n\t\t * \t- kvm_is_visible_gfn (mmu_check_roots)\n\t\t */\n\t\tkvm_arch_flush_shadow_memslot(kvm, slot);\n\t\tkfree(old_memslots);\n\t}\n\n\tr = kvm_arch_prepare_memory_region(kvm, &new, old, mem, user_alloc);\n\tif (r)\n\t\tgoto out_free;\n\n\t/* map/unmap the pages in iommu page table */\n\tif (npages) {\n\t\tr = kvm_iommu_map_pages(kvm, &new);\n\t\tif (r)\n\t\t\tgoto out_free;\n\t} else\n\t\tkvm_iommu_unmap_pages(kvm, &old);\n\n\tr = -ENOMEM;\n\tslots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),\n\t\t\tGFP_KERNEL);\n\tif (!slots)\n\t\tgoto out_free;\n\n\t/* actual memory is freed via old in kvm_free_physmem_slot below */\n\tif (!npages) {\n\t\tnew.dirty_bitmap = NULL;\n\t\tmemset(&new.arch, 0, sizeof(new.arch));\n\t}\n\n\tupdate_memslots(slots, &new);\n\told_memslots = kvm->memslots;\n\trcu_assign_pointer(kvm->memslots, slots);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\n\tkvm_arch_commit_memory_region(kvm, mem, old, user_alloc);\n\n\t/*\n\t * If the new memory slot is created, we need to clear all\n\t * mmio sptes.\n\t */\n\tif (npages && old.base_gfn != mem->guest_phys_addr >> PAGE_SHIFT)\n\t\tkvm_arch_flush_shadow_all(kvm);\n\n\tkvm_free_physmem_slot(&old, &new);\n\tkfree(old_memslots);\n\n\treturn 0;\n\nout_free:\n\tkvm_free_physmem_slot(&new, &old);\nout:\n\treturn r;\n\n}\nEXPORT_SYMBOL_GPL(__kvm_set_memory_region);\n\nint kvm_set_memory_region(struct kvm *kvm,\n\t\t\t  struct kvm_userspace_memory_region *mem,\n\t\t\t  int user_alloc)\n{\n\tint r;\n\n\tmutex_lock(&kvm->slots_lock);\n\tr = __kvm_set_memory_region(kvm, mem, user_alloc);\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_set_memory_region);\n\nint kvm_vm_ioctl_set_memory_region(struct kvm *kvm,\n\t\t\t\t   struct\n\t\t\t\t   kvm_userspace_memory_region *mem,\n\t\t\t\t   int user_alloc)\n{\n\tif (mem->slot >= KVM_MEMORY_SLOTS)\n\t\treturn -EINVAL;\n\treturn kvm_set_memory_region(kvm, mem, user_alloc);\n}\n\nint kvm_get_dirty_log(struct kvm *kvm,\n\t\t\tstruct kvm_dirty_log *log, int *is_dirty)\n{\n\tstruct kvm_memory_slot *memslot;\n\tint r, i;\n\tunsigned long n;\n\tunsigned long any = 0;\n\n\tr = -EINVAL;\n\tif (log->slot >= KVM_MEMORY_SLOTS)\n\t\tgoto out;\n\n\tmemslot = id_to_memslot(kvm->memslots, log->slot);\n\tr = -ENOENT;\n\tif (!memslot->dirty_bitmap)\n\t\tgoto out;\n\n\tn = kvm_dirty_bitmap_bytes(memslot);\n\n\tfor (i = 0; !any && i < n/sizeof(long); ++i)\n\t\tany = memslot->dirty_bitmap[i];\n\n\tr = -EFAULT;\n\tif (copy_to_user(log->dirty_bitmap, memslot->dirty_bitmap, n))\n\t\tgoto out;\n\n\tif (any)\n\t\t*is_dirty = 1;\n\n\tr = 0;\nout:\n\treturn r;\n}\n\nbool kvm_largepages_enabled(void)\n{\n\treturn largepages_enabled;\n}\n\nvoid kvm_disable_largepages(void)\n{\n\tlargepages_enabled = false;\n}\nEXPORT_SYMBOL_GPL(kvm_disable_largepages);\n\nstruct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_memslot(kvm_memslots(kvm), gfn);\n}\nEXPORT_SYMBOL_GPL(gfn_to_memslot);\n\nint kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct kvm_memory_slot *memslot = gfn_to_memslot(kvm, gfn);\n\n\tif (!memslot || memslot->id >= KVM_MEMORY_SLOTS ||\n\t      memslot->flags & KVM_MEMSLOT_INVALID)\n\t\treturn 0;\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_is_visible_gfn);\n\nunsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long addr, size;\n\n\tsize = PAGE_SIZE;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn PAGE_SIZE;\n\n\tdown_read(&current->mm->mmap_sem);\n\tvma = find_vma(current->mm, addr);\n\tif (!vma)\n\t\tgoto out;\n\n\tsize = vma_kernel_pagesize(vma);\n\nout:\n\tup_read(&current->mm->mmap_sem);\n\n\treturn size;\n}\n\nstatic bool memslot_is_readonly(struct kvm_memory_slot *slot)\n{\n\treturn slot->flags & KVM_MEM_READONLY;\n}\n\nstatic unsigned long __gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t       gfn_t *nr_pages, bool write)\n{\n\tif (!slot || slot->flags & KVM_MEMSLOT_INVALID)\n\t\treturn KVM_HVA_ERR_BAD;\n\n\tif (memslot_is_readonly(slot) && write)\n\t\treturn KVM_HVA_ERR_RO_BAD;\n\n\tif (nr_pages)\n\t\t*nr_pages = slot->npages - (gfn - slot->base_gfn);\n\n\treturn __gfn_to_hva_memslot(slot, gfn);\n}\n\nstatic unsigned long gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t     gfn_t *nr_pages)\n{\n\treturn __gfn_to_hva_many(slot, gfn, nr_pages, true);\n}\n\nunsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot,\n\t\t\t\t gfn_t gfn)\n{\n\treturn gfn_to_hva_many(slot, gfn, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_hva_memslot);\n\nunsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn)\n{\n\treturn gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_hva);\n\n/*\n * The hva returned by this function is only allowed to be read.\n * It should pair with kvm_read_hva() or kvm_read_hva_atomic().\n */\nstatic unsigned long gfn_to_hva_read(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL, false);\n}\n\nstatic int kvm_read_hva(void *data, void __user *hva, int len)\n{\n\treturn __copy_from_user(data, hva, len);\n}\n\nstatic int kvm_read_hva_atomic(void *data, void __user *hva, int len)\n{\n\treturn __copy_from_user_inatomic(data, hva, len);\n}\n\nint get_user_page_nowait(struct task_struct *tsk, struct mm_struct *mm,\n\tunsigned long start, int write, struct page **page)\n{\n\tint flags = FOLL_TOUCH | FOLL_NOWAIT | FOLL_HWPOISON | FOLL_GET;\n\n\tif (write)\n\t\tflags |= FOLL_WRITE;\n\n\treturn __get_user_pages(tsk, mm, start, 1, flags, page, NULL, NULL);\n}\n\nstatic inline int check_user_page_hwpoison(unsigned long addr)\n{\n\tint rc, flags = FOLL_TOUCH | FOLL_HWPOISON | FOLL_WRITE;\n\n\trc = __get_user_pages(current, current->mm, addr, 1,\n\t\t\t      flags, NULL, NULL, NULL);\n\treturn rc == -EHWPOISON;\n}\n\n/*\n * The atomic path to get the writable pfn which will be stored in @pfn,\n * true indicates success, otherwise false is returned.\n */\nstatic bool hva_to_pfn_fast(unsigned long addr, bool atomic, bool *async,\n\t\t\t    bool write_fault, bool *writable, pfn_t *pfn)\n{\n\tstruct page *page[1];\n\tint npages;\n\n\tif (!(async || atomic))\n\t\treturn false;\n\n\t/*\n\t * Fast pin a writable pfn only if it is a write fault request\n\t * or the caller allows to map a writable pfn for a read fault\n\t * request.\n\t */\n\tif (!(write_fault || writable))\n\t\treturn false;\n\n\tnpages = __get_user_pages_fast(addr, 1, 1, page);\n\tif (npages == 1) {\n\t\t*pfn = page_to_pfn(page[0]);\n\n\t\tif (writable)\n\t\t\t*writable = true;\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * The slow path to get the pfn of the specified host virtual address,\n * 1 indicates success, -errno is returned if error is detected.\n */\nstatic int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,\n\t\t\t   bool *writable, pfn_t *pfn)\n{\n\tstruct page *page[1];\n\tint npages = 0;\n\n\tmight_sleep();\n\n\tif (writable)\n\t\t*writable = write_fault;\n\n\tif (async) {\n\t\tdown_read(&current->mm->mmap_sem);\n\t\tnpages = get_user_page_nowait(current, current->mm,\n\t\t\t\t\t      addr, write_fault, page);\n\t\tup_read(&current->mm->mmap_sem);\n\t} else\n\t\tnpages = get_user_pages_fast(addr, 1, write_fault,\n\t\t\t\t\t     page);\n\tif (npages != 1)\n\t\treturn npages;\n\n\t/* map read fault as writable if possible */\n\tif (unlikely(!write_fault) && writable) {\n\t\tstruct page *wpage[1];\n\n\t\tnpages = __get_user_pages_fast(addr, 1, 1, wpage);\n\t\tif (npages == 1) {\n\t\t\t*writable = true;\n\t\t\tput_page(page[0]);\n\t\t\tpage[0] = wpage[0];\n\t\t}\n\n\t\tnpages = 1;\n\t}\n\t*pfn = page_to_pfn(page[0]);\n\treturn npages;\n}\n\nstatic bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)\n{\n\tif (unlikely(!(vma->vm_flags & VM_READ)))\n\t\treturn false;\n\n\tif (write_fault && (unlikely(!(vma->vm_flags & VM_WRITE))))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Pin guest page in memory and return its pfn.\n * @addr: host virtual address which maps memory to the guest\n * @atomic: whether this function can sleep\n * @async: whether this function need to wait IO complete if the\n *         host page is not in the memory\n * @write_fault: whether we should get a writable host page\n * @writable: whether it allows to map a writable host page for !@write_fault\n *\n * The function will map a writable host page for these two cases:\n * 1): @write_fault = true\n * 2): @write_fault = false && @writable, @writable will tell the caller\n *     whether the mapping is writable.\n */\nstatic pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,\n\t\t\tbool write_fault, bool *writable)\n{\n\tstruct vm_area_struct *vma;\n\tpfn_t pfn = 0;\n\tint npages;\n\n\t/* we can do it either atomically or asynchronously, not both */\n\tBUG_ON(atomic && async);\n\n\tif (hva_to_pfn_fast(addr, atomic, async, write_fault, writable, &pfn))\n\t\treturn pfn;\n\n\tif (atomic)\n\t\treturn KVM_PFN_ERR_FAULT;\n\n\tnpages = hva_to_pfn_slow(addr, async, write_fault, writable, &pfn);\n\tif (npages == 1)\n\t\treturn pfn;\n\n\tdown_read(&current->mm->mmap_sem);\n\tif (npages == -EHWPOISON ||\n\t      (!async && check_user_page_hwpoison(addr))) {\n\t\tpfn = KVM_PFN_ERR_HWPOISON;\n\t\tgoto exit;\n\t}\n\n\tvma = find_vma_intersection(current->mm, addr, addr + 1);\n\n\tif (vma == NULL)\n\t\tpfn = KVM_PFN_ERR_FAULT;\n\telse if ((vma->vm_flags & VM_PFNMAP)) {\n\t\tpfn = ((addr - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\t\tBUG_ON(!kvm_is_mmio_pfn(pfn));\n\t} else {\n\t\tif (async && vma_is_valid(vma, write_fault))\n\t\t\t*async = true;\n\t\tpfn = KVM_PFN_ERR_FAULT;\n\t}\nexit:\n\tup_read(&current->mm->mmap_sem);\n\treturn pfn;\n}\n\nstatic pfn_t\n__gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,\n\t\t     bool *async, bool write_fault, bool *writable)\n{\n\tunsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);\n\n\tif (addr == KVM_HVA_ERR_RO_BAD)\n\t\treturn KVM_PFN_ERR_RO_FAULT;\n\n\tif (kvm_is_error_hva(addr))\n\t\treturn KVM_PFN_ERR_BAD;\n\n\t/* Do not map writable pfn in the readonly memslot. */\n\tif (writable && memslot_is_readonly(slot)) {\n\t\t*writable = false;\n\t\twritable = NULL;\n\t}\n\n\treturn hva_to_pfn(addr, atomic, async, write_fault,\n\t\t\t  writable);\n}\n\nstatic pfn_t __gfn_to_pfn(struct kvm *kvm, gfn_t gfn, bool atomic, bool *async,\n\t\t\t  bool write_fault, bool *writable)\n{\n\tstruct kvm_memory_slot *slot;\n\n\tif (async)\n\t\t*async = false;\n\n\tslot = gfn_to_memslot(kvm, gfn);\n\n\treturn __gfn_to_pfn_memslot(slot, gfn, atomic, async, write_fault,\n\t\t\t\t    writable);\n}\n\npfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_pfn(kvm, gfn, true, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_atomic);\n\npfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async,\n\t\t       bool write_fault, bool *writable)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, async, write_fault, writable);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_async);\n\npfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn);\n\npfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,\n\t\t      bool *writable)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, NULL, write_fault, writable);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_prot);\n\npfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\treturn __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL);\n}\n\npfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\treturn __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_memslot_atomic);\n\nint gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,\n\t\t\t\t\t\t\t\t  int nr_pages)\n{\n\tunsigned long addr;\n\tgfn_t entry;\n\n\taddr = gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, &entry);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -1;\n\n\tif (entry < nr_pages)\n\t\treturn 0;\n\n\treturn __get_user_pages_fast(addr, nr_pages, 1, pages);\n}\nEXPORT_SYMBOL_GPL(gfn_to_page_many_atomic);\n\nstatic struct page *kvm_pfn_to_page(pfn_t pfn)\n{\n\tif (is_error_pfn(pfn))\n\t\treturn KVM_ERR_PTR_BAD_PAGE;\n\n\tif (kvm_is_mmio_pfn(pfn)) {\n\t\tWARN_ON(1);\n\t\treturn KVM_ERR_PTR_BAD_PAGE;\n\t}\n\n\treturn pfn_to_page(pfn);\n}\n\nstruct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)\n{\n\tpfn_t pfn;\n\n\tpfn = gfn_to_pfn(kvm, gfn);\n\n\treturn kvm_pfn_to_page(pfn);\n}\n\nEXPORT_SYMBOL_GPL(gfn_to_page);\n\nvoid kvm_release_page_clean(struct page *page)\n{\n\tWARN_ON(is_error_page(page));\n\n\tkvm_release_pfn_clean(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_release_page_clean);\n\nvoid kvm_release_pfn_clean(pfn_t pfn)\n{\n\tWARN_ON(is_error_pfn(pfn));\n\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tput_page(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_release_pfn_clean);\n\nvoid kvm_release_page_dirty(struct page *page)\n{\n\tWARN_ON(is_error_page(page));\n\n\tkvm_release_pfn_dirty(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_release_page_dirty);\n\nvoid kvm_release_pfn_dirty(pfn_t pfn)\n{\n\tkvm_set_pfn_dirty(pfn);\n\tkvm_release_pfn_clean(pfn);\n}\nEXPORT_SYMBOL_GPL(kvm_release_pfn_dirty);\n\nvoid kvm_set_page_dirty(struct page *page)\n{\n\tkvm_set_pfn_dirty(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_set_page_dirty);\n\nvoid kvm_set_pfn_dirty(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn)) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\t\tif (!PageReserved(page))\n\t\t\tSetPageDirty(page);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);\n\nvoid kvm_set_pfn_accessed(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tmark_page_accessed(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_set_pfn_accessed);\n\nvoid kvm_get_pfn(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tget_page(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_get_pfn);\n\nstatic int next_segment(unsigned long len, int offset)\n{\n\tif (len > PAGE_SIZE - offset)\n\t\treturn PAGE_SIZE - offset;\n\telse\n\t\treturn len;\n}\n\nint kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,\n\t\t\tint len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva_read(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = kvm_read_hva(data, (void __user *)addr + offset, len);\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_page);\n\nint kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n\twhile ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_read_guest_page(kvm, gfn, data, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\tdata += seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest);\n\nint kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,\n\t\t\t  unsigned long len)\n{\n\tint r;\n\tunsigned long addr;\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint offset = offset_in_page(gpa);\n\n\taddr = gfn_to_hva_read(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tpagefault_disable();\n\tr = kvm_read_hva_atomic(data, (void __user *)addr + offset, len);\n\tpagefault_enable();\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(kvm_read_guest_atomic);\n\nint kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,\n\t\t\t int offset, int len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = __copy_to_user((void __user *)addr + offset, data, len);\n\tif (r)\n\t\treturn -EFAULT;\n\tmark_page_dirty(kvm, gfn);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_page);\n\nint kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,\n\t\t    unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n\twhile ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_write_guest_page(kvm, gfn, data, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\tdata += seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\n\nint kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t      gpa_t gpa)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint offset = offset_in_page(gpa);\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\n\tghc->gpa = gpa;\n\tghc->generation = slots->generation;\n\tghc->memslot = gfn_to_memslot(kvm, gfn);\n\tghc->hva = gfn_to_hva_many(ghc->memslot, gfn, NULL);\n\tif (!kvm_is_error_hva(ghc->hva))\n\t\tghc->hva += offset;\n\telse\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_gfn_to_hva_cache_init);\n\nint kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t   void *data, unsigned long len)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint r;\n\n\tif (slots->generation != ghc->generation)\n\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa);\n\n\tif (kvm_is_error_hva(ghc->hva))\n\t\treturn -EFAULT;\n\n\tr = __copy_to_user((void __user *)ghc->hva, data, len);\n\tif (r)\n\t\treturn -EFAULT;\n\tmark_page_dirty_in_slot(kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_cached);\n\nint kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t   void *data, unsigned long len)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint r;\n\n\tif (slots->generation != ghc->generation)\n\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa);\n\n\tif (kvm_is_error_hva(ghc->hva))\n\t\treturn -EFAULT;\n\n\tr = __copy_from_user(data, (void __user *)ghc->hva, len);\n\tif (r)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_cached);\n\nint kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len)\n{\n\treturn kvm_write_guest_page(kvm, gfn, (const void *) empty_zero_page,\n\t\t\t\t    offset, len);\n}\nEXPORT_SYMBOL_GPL(kvm_clear_guest_page);\n\nint kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n        while ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_clear_guest_page(kvm, gfn, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_clear_guest);\n\nvoid mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,\n\t\t\t     gfn_t gfn)\n{\n\tif (memslot && memslot->dirty_bitmap) {\n\t\tunsigned long rel_gfn = gfn - memslot->base_gfn;\n\n\t\t/* TODO: introduce set_bit_le() and use it */\n\t\ttest_and_set_bit_le(rel_gfn, memslot->dirty_bitmap);\n\t}\n}\n\nvoid mark_page_dirty(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct kvm_memory_slot *memslot;\n\n\tmemslot = gfn_to_memslot(kvm, gfn);\n\tmark_page_dirty_in_slot(kvm, memslot, gfn);\n}\n\n/*\n * The vCPU has executed a HLT instruction with in-kernel mode enabled.\n */\nvoid kvm_vcpu_block(struct kvm_vcpu *vcpu)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (kvm_arch_vcpu_runnable(vcpu)) {\n\t\t\tkvm_make_request(KVM_REQ_UNHALT, vcpu);\n\t\t\tbreak;\n\t\t}\n\t\tif (kvm_cpu_has_pending_timer(vcpu))\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\n\tfinish_wait(&vcpu->wq, &wait);\n}\n\n#ifndef CONFIG_S390\n/*\n * Kick a sleeping VCPU, or a guest VCPU in guest mode, into host kernel mode.\n */\nvoid kvm_vcpu_kick(struct kvm_vcpu *vcpu)\n{\n\tint me;\n\tint cpu = vcpu->cpu;\n\twait_queue_head_t *wqp;\n\n\twqp = kvm_arch_vcpu_wq(vcpu);\n\tif (waitqueue_active(wqp)) {\n\t\twake_up_interruptible(wqp);\n\t\t++vcpu->stat.halt_wakeup;\n\t}\n\n\tme = get_cpu();\n\tif (cpu != me && (unsigned)cpu < nr_cpu_ids && cpu_online(cpu))\n\t\tif (kvm_arch_vcpu_should_kick(vcpu))\n\t\t\tsmp_send_reschedule(cpu);\n\tput_cpu();\n}\n#endif /* !CONFIG_S390 */\n\nvoid kvm_resched(struct kvm_vcpu *vcpu)\n{\n\tif (!need_resched())\n\t\treturn;\n\tcond_resched();\n}\nEXPORT_SYMBOL_GPL(kvm_resched);\n\nbool kvm_vcpu_yield_to(struct kvm_vcpu *target)\n{\n\tstruct pid *pid;\n\tstruct task_struct *task = NULL;\n\n\trcu_read_lock();\n\tpid = rcu_dereference(target->pid);\n\tif (pid)\n\t\ttask = get_pid_task(target->pid, PIDTYPE_PID);\n\trcu_read_unlock();\n\tif (!task)\n\t\treturn false;\n\tif (task->flags & PF_VCPU) {\n\t\tput_task_struct(task);\n\t\treturn false;\n\t}\n\tif (yield_to(task, 1)) {\n\t\tput_task_struct(task);\n\t\treturn true;\n\t}\n\tput_task_struct(task);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_yield_to);\n\n#ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT\n/*\n * Helper that checks whether a VCPU is eligible for directed yield.\n * Most eligible candidate to yield is decided by following heuristics:\n *\n *  (a) VCPU which has not done pl-exit or cpu relax intercepted recently\n *  (preempted lock holder), indicated by @in_spin_loop.\n *  Set at the beiginning and cleared at the end of interception/PLE handler.\n *\n *  (b) VCPU which has done pl-exit/ cpu relax intercepted but did not get\n *  chance last time (mostly it has become eligible now since we have probably\n *  yielded to lockholder in last iteration. This is done by toggling\n *  @dy_eligible each time a VCPU checked for eligibility.)\n *\n *  Yielding to a recently pl-exited/cpu relax intercepted VCPU before yielding\n *  to preempted lock-holder could result in wrong VCPU selection and CPU\n *  burning. Giving priority for a potential lock-holder increases lock\n *  progress.\n *\n *  Since algorithm is based on heuristics, accessing another VCPU data without\n *  locking does not harm. It may result in trying to yield to  same VCPU, fail\n *  and continue with next VCPU and so on.\n */\nbool kvm_vcpu_eligible_for_directed_yield(struct kvm_vcpu *vcpu)\n{\n\tbool eligible;\n\n\teligible = !vcpu->spin_loop.in_spin_loop ||\n\t\t\t(vcpu->spin_loop.in_spin_loop &&\n\t\t\t vcpu->spin_loop.dy_eligible);\n\n\tif (vcpu->spin_loop.in_spin_loop)\n\t\tkvm_vcpu_set_dy_eligible(vcpu, !vcpu->spin_loop.dy_eligible);\n\n\treturn eligible;\n}\n#endif\nvoid kvm_vcpu_on_spin(struct kvm_vcpu *me)\n{\n\tstruct kvm *kvm = me->kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint last_boosted_vcpu = me->kvm->last_boosted_vcpu;\n\tint yielded = 0;\n\tint pass;\n\tint i;\n\n\tkvm_vcpu_set_in_spin_loop(me, true);\n\t/*\n\t * We boost the priority of a VCPU that is runnable but not\n\t * currently running, because it got preempted by something\n\t * else and called schedule in __vcpu_run.  Hopefully that\n\t * VCPU is holding the lock that we need and will release it.\n\t * We approximate round-robin by starting at the last boosted VCPU.\n\t */\n\tfor (pass = 0; pass < 2 && !yielded; pass++) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (!pass && i <= last_boosted_vcpu) {\n\t\t\t\ti = last_boosted_vcpu;\n\t\t\t\tcontinue;\n\t\t\t} else if (pass && i > last_boosted_vcpu)\n\t\t\t\tbreak;\n\t\t\tif (vcpu == me)\n\t\t\t\tcontinue;\n\t\t\tif (waitqueue_active(&vcpu->wq))\n\t\t\t\tcontinue;\n\t\t\tif (!kvm_vcpu_eligible_for_directed_yield(vcpu))\n\t\t\t\tcontinue;\n\t\t\tif (kvm_vcpu_yield_to(vcpu)) {\n\t\t\t\tkvm->last_boosted_vcpu = i;\n\t\t\t\tyielded = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tkvm_vcpu_set_in_spin_loop(me, false);\n\n\t/* Ensure vcpu is not eligible during next spinloop */\n\tkvm_vcpu_set_dy_eligible(me, false);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_on_spin);\n\nstatic int kvm_vcpu_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct kvm_vcpu *vcpu = vma->vm_file->private_data;\n\tstruct page *page;\n\n\tif (vmf->pgoff == 0)\n\t\tpage = virt_to_page(vcpu->run);\n#ifdef CONFIG_X86\n\telse if (vmf->pgoff == KVM_PIO_PAGE_OFFSET)\n\t\tpage = virt_to_page(vcpu->arch.pio_data);\n#endif\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\telse if (vmf->pgoff == KVM_COALESCED_MMIO_PAGE_OFFSET)\n\t\tpage = virt_to_page(vcpu->kvm->coalesced_mmio_ring);\n#endif\n\telse\n\t\treturn kvm_arch_vcpu_fault(vcpu, vmf);\n\tget_page(page);\n\tvmf->page = page;\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct kvm_vcpu_vm_ops = {\n\t.fault = kvm_vcpu_fault,\n};\n\nstatic int kvm_vcpu_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_ops = &kvm_vcpu_vm_ops;\n\treturn 0;\n}\n\nstatic int kvm_vcpu_release(struct inode *inode, struct file *filp)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\n\tkvm_put_kvm(vcpu->kvm);\n\treturn 0;\n}\n\nstatic struct file_operations kvm_vcpu_fops = {\n\t.release        = kvm_vcpu_release,\n\t.unlocked_ioctl = kvm_vcpu_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl   = kvm_vcpu_compat_ioctl,\n#endif\n\t.mmap           = kvm_vcpu_mmap,\n\t.llseek\t\t= noop_llseek,\n};\n\n/*\n * Allocates an inode for the vcpu.\n */\nstatic int create_vcpu_fd(struct kvm_vcpu *vcpu)\n{\n\treturn anon_inode_getfd(\"kvm-vcpu\", &kvm_vcpu_fops, vcpu, O_RDWR);\n}\n\n/*\n * Creates some virtual cpus.  Good luck creating more than one.\n */\nstatic int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)\n{\n\tint r;\n\tstruct kvm_vcpu *vcpu, *v;\n\n\tvcpu = kvm_arch_vcpu_create(kvm, id);\n\tif (IS_ERR(vcpu))\n\t\treturn PTR_ERR(vcpu);\n\n\tpreempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);\n\n\tr = kvm_arch_vcpu_setup(vcpu);\n\tif (r)\n\t\tgoto vcpu_destroy;\n\n\tmutex_lock(&kvm->lock);\n\tif (!kvm_vcpu_compatible(vcpu)) {\n\t\tr = -EINVAL;\n\t\tgoto unlock_vcpu_destroy;\n\t}\n\tif (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {\n\t\tr = -EINVAL;\n\t\tgoto unlock_vcpu_destroy;\n\t}\n\n\tkvm_for_each_vcpu(r, v, kvm)\n\t\tif (v->vcpu_id == id) {\n\t\t\tr = -EEXIST;\n\t\t\tgoto unlock_vcpu_destroy;\n\t\t}\n\n\tBUG_ON(kvm->vcpus[atomic_read(&kvm->online_vcpus)]);\n\n\t/* Now it's all set up, let userspace reach it */\n\tkvm_get_kvm(kvm);\n\tr = create_vcpu_fd(vcpu);\n\tif (r < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\tgoto unlock_vcpu_destroy;\n\t}\n\n\tkvm->vcpus[atomic_read(&kvm->online_vcpus)] = vcpu;\n\tsmp_wmb();\n\tatomic_inc(&kvm->online_vcpus);\n\n\tmutex_unlock(&kvm->lock);\n\treturn r;\n\nunlock_vcpu_destroy:\n\tmutex_unlock(&kvm->lock);\nvcpu_destroy:\n\tkvm_arch_vcpu_destroy(vcpu);\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)\n{\n\tif (sigset) {\n\t\tsigdelsetmask(sigset, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\t\tvcpu->sigset_active = 1;\n\t\tvcpu->sigset = *sigset;\n\t} else\n\t\tvcpu->sigset_active = 0;\n\treturn 0;\n}\n\nstatic long kvm_vcpu_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\tstruct kvm_fpu *fpu = NULL;\n\tstruct kvm_sregs *kvm_sregs = NULL;\n\n\tif (vcpu->kvm->mm != current->mm)\n\t\treturn -EIO;\n\n#if defined(CONFIG_S390) || defined(CONFIG_PPC)\n\t/*\n\t * Special cases: vcpu ioctls that are asynchronous to vcpu execution,\n\t * so vcpu_load() would break it.\n\t */\n\tif (ioctl == KVM_S390_INTERRUPT || ioctl == KVM_INTERRUPT)\n\t\treturn kvm_arch_vcpu_ioctl(filp, ioctl, arg);\n#endif\n\n\n\tvcpu_load(vcpu);\n\tswitch (ioctl) {\n\tcase KVM_RUN:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);\n\t\ttrace_kvm_userspace_exit(vcpu->run->exit_reason, r);\n\t\tbreak;\n\tcase KVM_GET_REGS: {\n\t\tstruct kvm_regs *kvm_regs;\n\n\t\tr = -ENOMEM;\n\t\tkvm_regs = kzalloc(sizeof(struct kvm_regs), GFP_KERNEL);\n\t\tif (!kvm_regs)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_regs(vcpu, kvm_regs);\n\t\tif (r)\n\t\t\tgoto out_free1;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, kvm_regs, sizeof(struct kvm_regs)))\n\t\t\tgoto out_free1;\n\t\tr = 0;\nout_free1:\n\t\tkfree(kvm_regs);\n\t\tbreak;\n\t}\n\tcase KVM_SET_REGS: {\n\t\tstruct kvm_regs *kvm_regs;\n\n\t\tr = -ENOMEM;\n\t\tkvm_regs = memdup_user(argp, sizeof(*kvm_regs));\n\t\tif (IS_ERR(kvm_regs)) {\n\t\t\tr = PTR_ERR(kvm_regs);\n\t\t\tgoto out;\n\t\t}\n\t\tr = kvm_arch_vcpu_ioctl_set_regs(vcpu, kvm_regs);\n\t\tif (r)\n\t\t\tgoto out_free2;\n\t\tr = 0;\nout_free2:\n\t\tkfree(kvm_regs);\n\t\tbreak;\n\t}\n\tcase KVM_GET_SREGS: {\n\t\tkvm_sregs = kzalloc(sizeof(struct kvm_sregs), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!kvm_sregs)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_sregs(vcpu, kvm_sregs);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, kvm_sregs, sizeof(struct kvm_sregs)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SREGS: {\n\t\tkvm_sregs = memdup_user(argp, sizeof(*kvm_sregs));\n\t\tif (IS_ERR(kvm_sregs)) {\n\t\t\tr = PTR_ERR(kvm_sregs);\n\t\t\tgoto out;\n\t\t}\n\t\tr = kvm_arch_vcpu_ioctl_set_sregs(vcpu, kvm_sregs);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MP_STATE: {\n\t\tstruct kvm_mp_state mp_state;\n\n\t\tr = kvm_arch_vcpu_ioctl_get_mpstate(vcpu, &mp_state);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &mp_state, sizeof mp_state))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_MP_STATE: {\n\t\tstruct kvm_mp_state mp_state;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mp_state, argp, sizeof mp_state))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_mpstate(vcpu, &mp_state);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_TRANSLATE: {\n\t\tstruct kvm_translation tr;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&tr, argp, sizeof tr))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_translate(vcpu, &tr);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &tr, sizeof tr))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_GUEST_DEBUG: {\n\t\tstruct kvm_guest_debug dbg;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dbg, argp, sizeof dbg))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_guest_debug(vcpu, &dbg);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SIGNAL_MASK: {\n\t\tstruct kvm_signal_mask __user *sigmask_arg = argp;\n\t\tstruct kvm_signal_mask kvm_sigmask;\n\t\tsigset_t sigset, *p;\n\n\t\tp = NULL;\n\t\tif (argp) {\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&kvm_sigmask, argp,\n\t\t\t\t\t   sizeof kvm_sigmask))\n\t\t\t\tgoto out;\n\t\t\tr = -EINVAL;\n\t\t\tif (kvm_sigmask.len != sizeof sigset)\n\t\t\t\tgoto out;\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&sigset, sigmask_arg->sigset,\n\t\t\t\t\t   sizeof sigset))\n\t\t\t\tgoto out;\n\t\t\tp = &sigset;\n\t\t}\n\t\tr = kvm_vcpu_ioctl_set_sigmask(vcpu, p);\n\t\tbreak;\n\t}\n\tcase KVM_GET_FPU: {\n\t\tfpu = kzalloc(sizeof(struct kvm_fpu), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!fpu)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_fpu(vcpu, fpu);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, fpu, sizeof(struct kvm_fpu)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_FPU: {\n\t\tfpu = memdup_user(argp, sizeof(*fpu));\n\t\tif (IS_ERR(fpu)) {\n\t\t\tr = PTR_ERR(fpu);\n\t\t\tgoto out;\n\t\t}\n\t\tr = kvm_arch_vcpu_ioctl_set_fpu(vcpu, fpu);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_arch_vcpu_ioctl(filp, ioctl, arg);\n\t}\nout:\n\tvcpu_put(vcpu);\n\tkfree(fpu);\n\tkfree(kvm_sregs);\n\treturn r;\n}\n\n#ifdef CONFIG_COMPAT\nstatic long kvm_vcpu_compat_ioctl(struct file *filp,\n\t\t\t\t  unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = compat_ptr(arg);\n\tint r;\n\n\tif (vcpu->kvm->mm != current->mm)\n\t\treturn -EIO;\n\n\tswitch (ioctl) {\n\tcase KVM_SET_SIGNAL_MASK: {\n\t\tstruct kvm_signal_mask __user *sigmask_arg = argp;\n\t\tstruct kvm_signal_mask kvm_sigmask;\n\t\tcompat_sigset_t csigset;\n\t\tsigset_t sigset;\n\n\t\tif (argp) {\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&kvm_sigmask, argp,\n\t\t\t\t\t   sizeof kvm_sigmask))\n\t\t\t\tgoto out;\n\t\t\tr = -EINVAL;\n\t\t\tif (kvm_sigmask.len != sizeof csigset)\n\t\t\t\tgoto out;\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&csigset, sigmask_arg->sigset,\n\t\t\t\t\t   sizeof csigset))\n\t\t\t\tgoto out;\n\t\t}\n\t\tsigset_from_compat(&sigset, &csigset);\n\t\tr = kvm_vcpu_ioctl_set_sigmask(vcpu, &sigset);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_vcpu_ioctl(filp, ioctl, arg);\n\t}\n\nout:\n\treturn r;\n}\n#endif\n\nstatic long kvm_vm_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\n\tif (kvm->mm != current->mm)\n\t\treturn -EIO;\n\tswitch (ioctl) {\n\tcase KVM_CREATE_VCPU:\n\t\tr = kvm_vm_ioctl_create_vcpu(kvm, arg);\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase KVM_SET_USER_MEMORY_REGION: {\n\t\tstruct kvm_userspace_memory_region kvm_userspace_mem;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&kvm_userspace_mem, argp,\n\t\t\t\t\t\tsizeof kvm_userspace_mem))\n\t\t\tgoto out;\n\n\t\tr = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem, 1);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\tcase KVM_GET_DIRTY_LOG: {\n\t\tstruct kvm_dirty_log log;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&log, argp, sizeof log))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_dirty_log(kvm, &log);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\tcase KVM_REGISTER_COALESCED_MMIO: {\n\t\tstruct kvm_coalesced_mmio_zone zone;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&zone, argp, sizeof zone))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_register_coalesced_mmio(kvm, &zone);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_UNREGISTER_COALESCED_MMIO: {\n\t\tstruct kvm_coalesced_mmio_zone zone;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&zone, argp, sizeof zone))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_unregister_coalesced_mmio(kvm, &zone);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_IRQFD: {\n\t\tstruct kvm_irqfd data;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&data, argp, sizeof data))\n\t\t\tgoto out;\n\t\tr = kvm_irqfd(kvm, &data);\n\t\tbreak;\n\t}\n\tcase KVM_IOEVENTFD: {\n\t\tstruct kvm_ioeventfd data;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&data, argp, sizeof data))\n\t\t\tgoto out;\n\t\tr = kvm_ioeventfd(kvm, &data);\n\t\tbreak;\n\t}\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tcase KVM_SET_BOOT_CPU_ID:\n\t\tr = 0;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (atomic_read(&kvm->online_vcpus) != 0)\n\t\t\tr = -EBUSY;\n\t\telse\n\t\t\tkvm->bsp_vcpu_id = arg;\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n#endif\n#ifdef CONFIG_HAVE_KVM_MSI\n\tcase KVM_SIGNAL_MSI: {\n\t\tstruct kvm_msi msi;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&msi, argp, sizeof msi))\n\t\t\tgoto out;\n\t\tr = kvm_send_userspace_msi(kvm, &msi);\n\t\tbreak;\n\t}\n#endif\n#ifdef __KVM_HAVE_IRQ_LINE\n\tcase KVM_IRQ_LINE_STATUS:\n\tcase KVM_IRQ_LINE: {\n\t\tstruct kvm_irq_level irq_event;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&irq_event, argp, sizeof irq_event))\n\t\t\tgoto out;\n\n\t\tr = kvm_vm_ioctl_irq_line(kvm, &irq_event);\n\t\tif (r)\n\t\t\tgoto out;\n\n\t\tr = -EFAULT;\n\t\tif (ioctl == KVM_IRQ_LINE_STATUS) {\n\t\t\tif (copy_to_user(argp, &irq_event, sizeof irq_event))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tr = kvm_arch_vm_ioctl(filp, ioctl, arg);\n\t\tif (r == -ENOTTY)\n\t\t\tr = kvm_vm_ioctl_assigned_device(kvm, ioctl, arg);\n\t}\nout:\n\treturn r;\n}\n\n#ifdef CONFIG_COMPAT\nstruct compat_kvm_dirty_log {\n\t__u32 slot;\n\t__u32 padding1;\n\tunion {\n\t\tcompat_uptr_t dirty_bitmap; /* one bit per page */\n\t\t__u64 padding2;\n\t};\n};\n\nstatic long kvm_vm_compat_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tint r;\n\n\tif (kvm->mm != current->mm)\n\t\treturn -EIO;\n\tswitch (ioctl) {\n\tcase KVM_GET_DIRTY_LOG: {\n\t\tstruct compat_kvm_dirty_log compat_log;\n\t\tstruct kvm_dirty_log log;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&compat_log, (void __user *)arg,\n\t\t\t\t   sizeof(compat_log)))\n\t\t\tgoto out;\n\t\tlog.slot\t = compat_log.slot;\n\t\tlog.padding1\t = compat_log.padding1;\n\t\tlog.padding2\t = compat_log.padding2;\n\t\tlog.dirty_bitmap = compat_ptr(compat_log.dirty_bitmap);\n\n\t\tr = kvm_vm_ioctl_get_dirty_log(kvm, &log);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_vm_ioctl(filp, ioctl, arg);\n\t}\n\nout:\n\treturn r;\n}\n#endif\n\nstatic int kvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page[1];\n\tunsigned long addr;\n\tint npages;\n\tgfn_t gfn = vmf->pgoff;\n\tstruct kvm *kvm = vma->vm_file->private_data;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tnpages = get_user_pages(current, current->mm, addr, 1, 1, 0, page,\n\t\t\t\tNULL);\n\tif (unlikely(npages != 1))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tvmf->page = page[0];\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct kvm_vm_vm_ops = {\n\t.fault = kvm_vm_fault,\n};\n\nstatic int kvm_vm_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_ops = &kvm_vm_vm_ops;\n\treturn 0;\n}\n\nstatic struct file_operations kvm_vm_fops = {\n\t.release        = kvm_vm_release,\n\t.unlocked_ioctl = kvm_vm_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl   = kvm_vm_compat_ioctl,\n#endif\n\t.mmap           = kvm_vm_mmap,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic int kvm_dev_ioctl_create_vm(unsigned long type)\n{\n\tint r;\n\tstruct kvm *kvm;\n\n\tkvm = kvm_create_vm(type);\n\tif (IS_ERR(kvm))\n\t\treturn PTR_ERR(kvm);\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\tr = kvm_coalesced_mmio_init(kvm);\n\tif (r < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\treturn r;\n\t}\n#endif\n\tr = anon_inode_getfd(\"kvm-vm\", &kvm_vm_fops, kvm, O_RDWR);\n\tif (r < 0)\n\t\tkvm_put_kvm(kvm);\n\n\treturn r;\n}\n\nstatic long kvm_dev_ioctl_check_extension_generic(long arg)\n{\n\tswitch (arg) {\n\tcase KVM_CAP_USER_MEMORY:\n\tcase KVM_CAP_DESTROY_MEMORY_REGION_WORKS:\n\tcase KVM_CAP_JOIN_MEMORY_REGIONS_WORKS:\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tcase KVM_CAP_SET_BOOT_CPU_ID:\n#endif\n\tcase KVM_CAP_INTERNAL_ERROR_DATA:\n#ifdef CONFIG_HAVE_KVM_MSI\n\tcase KVM_CAP_SIGNAL_MSI:\n#endif\n\t\treturn 1;\n#ifdef KVM_CAP_IRQ_ROUTING\n\tcase KVM_CAP_IRQ_ROUTING:\n\t\treturn KVM_MAX_IRQ_ROUTES;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\treturn kvm_dev_ioctl_check_extension(arg);\n}\n\nstatic long kvm_dev_ioctl(struct file *filp,\n\t\t\t  unsigned int ioctl, unsigned long arg)\n{\n\tlong r = -EINVAL;\n\n\tswitch (ioctl) {\n\tcase KVM_GET_API_VERSION:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = KVM_API_VERSION;\n\t\tbreak;\n\tcase KVM_CREATE_VM:\n\t\tr = kvm_dev_ioctl_create_vm(arg);\n\t\tbreak;\n\tcase KVM_CHECK_EXTENSION:\n\t\tr = kvm_dev_ioctl_check_extension_generic(arg);\n\t\tbreak;\n\tcase KVM_GET_VCPU_MMAP_SIZE:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = PAGE_SIZE;     /* struct kvm_run */\n#ifdef CONFIG_X86\n\t\tr += PAGE_SIZE;    /* pio data page */\n#endif\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\t\tr += PAGE_SIZE;    /* coalesced mmio ring page */\n#endif\n\t\tbreak;\n\tcase KVM_TRACE_ENABLE:\n\tcase KVM_TRACE_PAUSE:\n\tcase KVM_TRACE_DISABLE:\n\t\tr = -EOPNOTSUPP;\n\t\tbreak;\n\tdefault:\n\t\treturn kvm_arch_dev_ioctl(filp, ioctl, arg);\n\t}\nout:\n\treturn r;\n}\n\nstatic struct file_operations kvm_chardev_ops = {\n\t.unlocked_ioctl = kvm_dev_ioctl,\n\t.compat_ioctl   = kvm_dev_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice kvm_dev = {\n\tKVM_MINOR,\n\t\"kvm\",\n\t&kvm_chardev_ops,\n};\n\nstatic void hardware_enable_nolock(void *junk)\n{\n\tint cpu = raw_smp_processor_id();\n\tint r;\n\n\tif (cpumask_test_cpu(cpu, cpus_hardware_enabled))\n\t\treturn;\n\n\tcpumask_set_cpu(cpu, cpus_hardware_enabled);\n\n\tr = kvm_arch_hardware_enable(NULL);\n\n\tif (r) {\n\t\tcpumask_clear_cpu(cpu, cpus_hardware_enabled);\n\t\tatomic_inc(&hardware_enable_failed);\n\t\tprintk(KERN_INFO \"kvm: enabling virtualization on \"\n\t\t\t\t \"CPU%d failed\\n\", cpu);\n\t}\n}\n\nstatic void hardware_enable(void *junk)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_enable_nolock(junk);\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic void hardware_disable_nolock(void *junk)\n{\n\tint cpu = raw_smp_processor_id();\n\n\tif (!cpumask_test_cpu(cpu, cpus_hardware_enabled))\n\t\treturn;\n\tcpumask_clear_cpu(cpu, cpus_hardware_enabled);\n\tkvm_arch_hardware_disable(NULL);\n}\n\nstatic void hardware_disable(void *junk)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_disable_nolock(junk);\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic void hardware_disable_all_nolock(void)\n{\n\tBUG_ON(!kvm_usage_count);\n\n\tkvm_usage_count--;\n\tif (!kvm_usage_count)\n\t\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n}\n\nstatic void hardware_disable_all(void)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_disable_all_nolock();\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic int hardware_enable_all(void)\n{\n\tint r = 0;\n\n\traw_spin_lock(&kvm_lock);\n\n\tkvm_usage_count++;\n\tif (kvm_usage_count == 1) {\n\t\tatomic_set(&hardware_enable_failed, 0);\n\t\ton_each_cpu(hardware_enable_nolock, NULL, 1);\n\n\t\tif (atomic_read(&hardware_enable_failed)) {\n\t\t\thardware_disable_all_nolock();\n\t\t\tr = -EBUSY;\n\t\t}\n\t}\n\n\traw_spin_unlock(&kvm_lock);\n\n\treturn r;\n}\n\nstatic int kvm_cpu_hotplug(struct notifier_block *notifier, unsigned long val,\n\t\t\t   void *v)\n{\n\tint cpu = (long)v;\n\n\tif (!kvm_usage_count)\n\t\treturn NOTIFY_OK;\n\n\tval &= ~CPU_TASKS_FROZEN;\n\tswitch (val) {\n\tcase CPU_DYING:\n\t\tprintk(KERN_INFO \"kvm: disabling virtualization on CPU%d\\n\",\n\t\t       cpu);\n\t\thardware_disable(NULL);\n\t\tbreak;\n\tcase CPU_STARTING:\n\t\tprintk(KERN_INFO \"kvm: enabling virtualization on CPU%d\\n\",\n\t\t       cpu);\n\t\thardware_enable(NULL);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\n\nasmlinkage void kvm_spurious_fault(void)\n{\n\t/* Fault while not rebooting.  We want the trace. */\n\tBUG();\n}\nEXPORT_SYMBOL_GPL(kvm_spurious_fault);\n\nstatic int kvm_reboot(struct notifier_block *notifier, unsigned long val,\n\t\t      void *v)\n{\n\t/*\n\t * Some (well, at least mine) BIOSes hang on reboot if\n\t * in vmx root mode.\n\t *\n\t * And Intel TXT required VMX off for all cpu when system shutdown.\n\t */\n\tprintk(KERN_INFO \"kvm: exiting hardware virtualization\\n\");\n\tkvm_rebooting = true;\n\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block kvm_reboot_notifier = {\n\t.notifier_call = kvm_reboot,\n\t.priority = 0,\n};\n\nstatic void kvm_io_bus_destroy(struct kvm_io_bus *bus)\n{\n\tint i;\n\n\tfor (i = 0; i < bus->dev_count; i++) {\n\t\tstruct kvm_io_device *pos = bus->range[i].dev;\n\n\t\tkvm_iodevice_destructor(pos);\n\t}\n\tkfree(bus);\n}\n\nint kvm_io_bus_sort_cmp(const void *p1, const void *p2)\n{\n\tconst struct kvm_io_range *r1 = p1;\n\tconst struct kvm_io_range *r2 = p2;\n\n\tif (r1->addr < r2->addr)\n\t\treturn -1;\n\tif (r1->addr + r1->len > r2->addr + r2->len)\n\t\treturn 1;\n\treturn 0;\n}\n\nint kvm_io_bus_insert_dev(struct kvm_io_bus *bus, struct kvm_io_device *dev,\n\t\t\t  gpa_t addr, int len)\n{\n\tbus->range[bus->dev_count++] = (struct kvm_io_range) {\n\t\t.addr = addr,\n\t\t.len = len,\n\t\t.dev = dev,\n\t};\n\n\tsort(bus->range, bus->dev_count, sizeof(struct kvm_io_range),\n\t\tkvm_io_bus_sort_cmp, NULL);\n\n\treturn 0;\n}\n\nint kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,\n\t\t\t     gpa_t addr, int len)\n{\n\tstruct kvm_io_range *range, key;\n\tint off;\n\n\tkey = (struct kvm_io_range) {\n\t\t.addr = addr,\n\t\t.len = len,\n\t};\n\n\trange = bsearch(&key, bus->range, bus->dev_count,\n\t\t\tsizeof(struct kvm_io_range), kvm_io_bus_sort_cmp);\n\tif (range == NULL)\n\t\treturn -ENOENT;\n\n\toff = range - bus->range;\n\n\twhile (off > 0 && kvm_io_bus_sort_cmp(&key, &bus->range[off-1]) == 0)\n\t\toff--;\n\n\treturn off;\n}\n\n/* kvm_io_bus_write - called under kvm->slots_lock */\nint kvm_io_bus_write(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t     int len, const void *val)\n{\n\tint idx;\n\tstruct kvm_io_bus *bus;\n\tstruct kvm_io_range range;\n\n\trange = (struct kvm_io_range) {\n\t\t.addr = addr,\n\t\t.len = len,\n\t};\n\n\tbus = srcu_dereference(kvm->buses[bus_idx], &kvm->srcu);\n\tidx = kvm_io_bus_get_first_dev(bus, addr, len);\n\tif (idx < 0)\n\t\treturn -EOPNOTSUPP;\n\n\twhile (idx < bus->dev_count &&\n\t\tkvm_io_bus_sort_cmp(&range, &bus->range[idx]) == 0) {\n\t\tif (!kvm_iodevice_write(bus->range[idx].dev, addr, len, val))\n\t\t\treturn 0;\n\t\tidx++;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\n/* kvm_io_bus_read - called under kvm->slots_lock */\nint kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t    int len, void *val)\n{\n\tint idx;\n\tstruct kvm_io_bus *bus;\n\tstruct kvm_io_range range;\n\n\trange = (struct kvm_io_range) {\n\t\t.addr = addr,\n\t\t.len = len,\n\t};\n\n\tbus = srcu_dereference(kvm->buses[bus_idx], &kvm->srcu);\n\tidx = kvm_io_bus_get_first_dev(bus, addr, len);\n\tif (idx < 0)\n\t\treturn -EOPNOTSUPP;\n\n\twhile (idx < bus->dev_count &&\n\t\tkvm_io_bus_sort_cmp(&range, &bus->range[idx]) == 0) {\n\t\tif (!kvm_iodevice_read(bus->range[idx].dev, addr, len, val))\n\t\t\treturn 0;\n\t\tidx++;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\n/* Caller must hold slots_lock. */\nint kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t\t    int len, struct kvm_io_device *dev)\n{\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tbus = kvm->buses[bus_idx];\n\tif (bus->dev_count > NR_IOBUS_DEVS - 1)\n\t\treturn -ENOSPC;\n\n\tnew_bus = kzalloc(sizeof(*bus) + ((bus->dev_count + 1) *\n\t\t\t  sizeof(struct kvm_io_range)), GFP_KERNEL);\n\tif (!new_bus)\n\t\treturn -ENOMEM;\n\tmemcpy(new_bus, bus, sizeof(*bus) + (bus->dev_count *\n\t       sizeof(struct kvm_io_range)));\n\tkvm_io_bus_insert_dev(new_bus, dev, addr, len);\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\n\treturn 0;\n}\n\n/* Caller must hold slots_lock. */\nint kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t      struct kvm_io_device *dev)\n{\n\tint i, r;\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tbus = kvm->buses[bus_idx];\n\tr = -ENOENT;\n\tfor (i = 0; i < bus->dev_count; i++)\n\t\tif (bus->range[i].dev == dev) {\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\n\tif (r)\n\t\treturn r;\n\n\tnew_bus = kzalloc(sizeof(*bus) + ((bus->dev_count - 1) *\n\t\t\t  sizeof(struct kvm_io_range)), GFP_KERNEL);\n\tif (!new_bus)\n\t\treturn -ENOMEM;\n\n\tmemcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));\n\tnew_bus->dev_count--;\n\tmemcpy(new_bus->range + i, bus->range + i + 1,\n\t       (new_bus->dev_count - i) * sizeof(struct kvm_io_range));\n\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\treturn r;\n}\n\nstatic struct notifier_block kvm_cpu_notifier = {\n\t.notifier_call = kvm_cpu_hotplug,\n};\n\nstatic int vm_stat_get(void *_offset, u64 *val)\n{\n\tunsigned offset = (long)_offset;\n\tstruct kvm *kvm;\n\n\t*val = 0;\n\traw_spin_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\t*val += *(u32 *)((void *)kvm + offset);\n\traw_spin_unlock(&kvm_lock);\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(vm_stat_fops, vm_stat_get, NULL, \"%llu\\n\");\n\nstatic int vcpu_stat_get(void *_offset, u64 *val)\n{\n\tunsigned offset = (long)_offset;\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\t*val = 0;\n\traw_spin_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\t\t*val += *(u32 *)((void *)vcpu + offset);\n\n\traw_spin_unlock(&kvm_lock);\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, NULL, \"%llu\\n\");\n\nstatic const struct file_operations *stat_fops[] = {\n\t[KVM_STAT_VCPU] = &vcpu_stat_fops,\n\t[KVM_STAT_VM]   = &vm_stat_fops,\n};\n\nstatic int kvm_init_debug(void)\n{\n\tint r = -EFAULT;\n\tstruct kvm_stats_debugfs_item *p;\n\n\tkvm_debugfs_dir = debugfs_create_dir(\"kvm\", NULL);\n\tif (kvm_debugfs_dir == NULL)\n\t\tgoto out;\n\n\tfor (p = debugfs_entries; p->name; ++p) {\n\t\tp->dentry = debugfs_create_file(p->name, 0444, kvm_debugfs_dir,\n\t\t\t\t\t\t(void *)(long)p->offset,\n\t\t\t\t\t\tstat_fops[p->kind]);\n\t\tif (p->dentry == NULL)\n\t\t\tgoto out_dir;\n\t}\n\n\treturn 0;\n\nout_dir:\n\tdebugfs_remove_recursive(kvm_debugfs_dir);\nout:\n\treturn r;\n}\n\nstatic void kvm_exit_debug(void)\n{\n\tstruct kvm_stats_debugfs_item *p;\n\n\tfor (p = debugfs_entries; p->name; ++p)\n\t\tdebugfs_remove(p->dentry);\n\tdebugfs_remove(kvm_debugfs_dir);\n}\n\nstatic int kvm_suspend(void)\n{\n\tif (kvm_usage_count)\n\t\thardware_disable_nolock(NULL);\n\treturn 0;\n}\n\nstatic void kvm_resume(void)\n{\n\tif (kvm_usage_count) {\n\t\tWARN_ON(raw_spin_is_locked(&kvm_lock));\n\t\thardware_enable_nolock(NULL);\n\t}\n}\n\nstatic struct syscore_ops kvm_syscore_ops = {\n\t.suspend = kvm_suspend,\n\t.resume = kvm_resume,\n};\n\nstatic inline\nstruct kvm_vcpu *preempt_notifier_to_vcpu(struct preempt_notifier *pn)\n{\n\treturn container_of(pn, struct kvm_vcpu, preempt_notifier);\n}\n\nstatic void kvm_sched_in(struct preempt_notifier *pn, int cpu)\n{\n\tstruct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);\n\n\tkvm_arch_vcpu_load(vcpu, cpu);\n}\n\nstatic void kvm_sched_out(struct preempt_notifier *pn,\n\t\t\t  struct task_struct *next)\n{\n\tstruct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);\n\n\tkvm_arch_vcpu_put(vcpu);\n}\n\nint kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,\n\t\t  struct module *module)\n{\n\tint r;\n\tint cpu;\n\n\tr = kvm_arch_init(opaque);\n\tif (r)\n\t\tgoto out_fail;\n\n\tif (!zalloc_cpumask_var(&cpus_hardware_enabled, GFP_KERNEL)) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_0;\n\t}\n\n\tr = kvm_arch_hardware_setup();\n\tif (r < 0)\n\t\tgoto out_free_0a;\n\n\tfor_each_online_cpu(cpu) {\n\t\tsmp_call_function_single(cpu,\n\t\t\t\tkvm_arch_check_processor_compat,\n\t\t\t\t&r, 1);\n\t\tif (r < 0)\n\t\t\tgoto out_free_1;\n\t}\n\n\tr = register_cpu_notifier(&kvm_cpu_notifier);\n\tif (r)\n\t\tgoto out_free_2;\n\tregister_reboot_notifier(&kvm_reboot_notifier);\n\n\t/* A kmem cache lets us meet the alignment requirements of fx_save. */\n\tif (!vcpu_align)\n\t\tvcpu_align = __alignof__(struct kvm_vcpu);\n\tkvm_vcpu_cache = kmem_cache_create(\"kvm_vcpu\", vcpu_size, vcpu_align,\n\t\t\t\t\t   0, NULL);\n\tif (!kvm_vcpu_cache) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_3;\n\t}\n\n\tr = kvm_async_pf_init();\n\tif (r)\n\t\tgoto out_free;\n\n\tkvm_chardev_ops.owner = module;\n\tkvm_vm_fops.owner = module;\n\tkvm_vcpu_fops.owner = module;\n\n\tr = misc_register(&kvm_dev);\n\tif (r) {\n\t\tprintk(KERN_ERR \"kvm: misc device register failed\\n\");\n\t\tgoto out_unreg;\n\t}\n\n\tregister_syscore_ops(&kvm_syscore_ops);\n\n\tkvm_preempt_ops.sched_in = kvm_sched_in;\n\tkvm_preempt_ops.sched_out = kvm_sched_out;\n\n\tr = kvm_init_debug();\n\tif (r) {\n\t\tprintk(KERN_ERR \"kvm: create debugfs files failed\\n\");\n\t\tgoto out_undebugfs;\n\t}\n\n\treturn 0;\n\nout_undebugfs:\n\tunregister_syscore_ops(&kvm_syscore_ops);\nout_unreg:\n\tkvm_async_pf_deinit();\nout_free:\n\tkmem_cache_destroy(kvm_vcpu_cache);\nout_free_3:\n\tunregister_reboot_notifier(&kvm_reboot_notifier);\n\tunregister_cpu_notifier(&kvm_cpu_notifier);\nout_free_2:\nout_free_1:\n\tkvm_arch_hardware_unsetup();\nout_free_0a:\n\tfree_cpumask_var(cpus_hardware_enabled);\nout_free_0:\n\tkvm_arch_exit();\nout_fail:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_init);\n\nvoid kvm_exit(void)\n{\n\tkvm_exit_debug();\n\tmisc_deregister(&kvm_dev);\n\tkmem_cache_destroy(kvm_vcpu_cache);\n\tkvm_async_pf_deinit();\n\tunregister_syscore_ops(&kvm_syscore_ops);\n\tunregister_reboot_notifier(&kvm_reboot_notifier);\n\tunregister_cpu_notifier(&kvm_cpu_notifier);\n\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n\tkvm_arch_hardware_unsetup();\n\tkvm_arch_exit();\n\tfree_cpumask_var(cpus_hardware_enabled);\n}\nEXPORT_SYMBOL_GPL(kvm_exit);\n"], "fixing_code": ["/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * This module enables machines with Intel VT-x extensions to run virtual\n * machines without emulation or binary translation.\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include \"iodev.h\"\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/percpu.h>\n#include <linux/mm.h>\n#include <linux/miscdevice.h>\n#include <linux/vmalloc.h>\n#include <linux/reboot.h>\n#include <linux/debugfs.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/syscore_ops.h>\n#include <linux/cpu.h>\n#include <linux/sched.h>\n#include <linux/cpumask.h>\n#include <linux/smp.h>\n#include <linux/anon_inodes.h>\n#include <linux/profile.h>\n#include <linux/kvm_para.h>\n#include <linux/pagemap.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/bitops.h>\n#include <linux/spinlock.h>\n#include <linux/compat.h>\n#include <linux/srcu.h>\n#include <linux/hugetlb.h>\n#include <linux/slab.h>\n#include <linux/sort.h>\n#include <linux/bsearch.h>\n\n#include <asm/processor.h>\n#include <asm/io.h>\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n\n#include \"coalesced_mmio.h\"\n#include \"async_pf.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/kvm.h>\n\nMODULE_AUTHOR(\"Qumranet\");\nMODULE_LICENSE(\"GPL\");\n\n/*\n * Ordering of locks:\n *\n * \t\tkvm->lock --> kvm->slots_lock --> kvm->irq_lock\n */\n\nDEFINE_RAW_SPINLOCK(kvm_lock);\nLIST_HEAD(vm_list);\n\nstatic cpumask_var_t cpus_hardware_enabled;\nstatic int kvm_usage_count = 0;\nstatic atomic_t hardware_enable_failed;\n\nstruct kmem_cache *kvm_vcpu_cache;\nEXPORT_SYMBOL_GPL(kvm_vcpu_cache);\n\nstatic __read_mostly struct preempt_ops kvm_preempt_ops;\n\nstruct dentry *kvm_debugfs_dir;\n\nstatic long kvm_vcpu_ioctl(struct file *file, unsigned int ioctl,\n\t\t\t   unsigned long arg);\n#ifdef CONFIG_COMPAT\nstatic long kvm_vcpu_compat_ioctl(struct file *file, unsigned int ioctl,\n\t\t\t\t  unsigned long arg);\n#endif\nstatic int hardware_enable_all(void);\nstatic void hardware_disable_all(void);\n\nstatic void kvm_io_bus_destroy(struct kvm_io_bus *bus);\n\nbool kvm_rebooting;\nEXPORT_SYMBOL_GPL(kvm_rebooting);\n\nstatic bool largepages_enabled = true;\n\nbool kvm_is_mmio_pfn(pfn_t pfn)\n{\n\tif (pfn_valid(pfn)) {\n\t\tint reserved;\n\t\tstruct page *tail = pfn_to_page(pfn);\n\t\tstruct page *head = compound_trans_head(tail);\n\t\treserved = PageReserved(head);\n\t\tif (head != tail) {\n\t\t\t/*\n\t\t\t * \"head\" is not a dangling pointer\n\t\t\t * (compound_trans_head takes care of that)\n\t\t\t * but the hugepage may have been splitted\n\t\t\t * from under us (and we may not hold a\n\t\t\t * reference count on the head page so it can\n\t\t\t * be reused before we run PageReferenced), so\n\t\t\t * we've to check PageTail before returning\n\t\t\t * what we just read.\n\t\t\t */\n\t\t\tsmp_rmb();\n\t\t\tif (PageTail(tail))\n\t\t\t\treturn reserved;\n\t\t}\n\t\treturn PageReserved(tail);\n\t}\n\n\treturn true;\n}\n\n/*\n * Switches to specified vcpu, until a matching vcpu_put()\n */\nvoid vcpu_load(struct kvm_vcpu *vcpu)\n{\n\tint cpu;\n\n\tmutex_lock(&vcpu->mutex);\n\tif (unlikely(vcpu->pid != current->pids[PIDTYPE_PID].pid)) {\n\t\t/* The thread running this VCPU changed. */\n\t\tstruct pid *oldpid = vcpu->pid;\n\t\tstruct pid *newpid = get_task_pid(current, PIDTYPE_PID);\n\t\trcu_assign_pointer(vcpu->pid, newpid);\n\t\tsynchronize_rcu();\n\t\tput_pid(oldpid);\n\t}\n\tcpu = get_cpu();\n\tpreempt_notifier_register(&vcpu->preempt_notifier);\n\tkvm_arch_vcpu_load(vcpu, cpu);\n\tput_cpu();\n}\n\nvoid vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tpreempt_disable();\n\tkvm_arch_vcpu_put(vcpu);\n\tpreempt_notifier_unregister(&vcpu->preempt_notifier);\n\tpreempt_enable();\n\tmutex_unlock(&vcpu->mutex);\n}\n\nstatic void ack_flush(void *_completed)\n{\n}\n\nstatic bool make_all_cpus_request(struct kvm *kvm, unsigned int req)\n{\n\tint i, cpu, me;\n\tcpumask_var_t cpus;\n\tbool called = true;\n\tstruct kvm_vcpu *vcpu;\n\n\tzalloc_cpumask_var(&cpus, GFP_ATOMIC);\n\n\tme = get_cpu();\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_make_request(req, vcpu);\n\t\tcpu = vcpu->cpu;\n\n\t\t/* Set ->requests bit before we read ->mode */\n\t\tsmp_mb();\n\n\t\tif (cpus != NULL && cpu != -1 && cpu != me &&\n\t\t      kvm_vcpu_exiting_guest_mode(vcpu) != OUTSIDE_GUEST_MODE)\n\t\t\tcpumask_set_cpu(cpu, cpus);\n\t}\n\tif (unlikely(cpus == NULL))\n\t\tsmp_call_function_many(cpu_online_mask, ack_flush, NULL, 1);\n\telse if (!cpumask_empty(cpus))\n\t\tsmp_call_function_many(cpus, ack_flush, NULL, 1);\n\telse\n\t\tcalled = false;\n\tput_cpu();\n\tfree_cpumask_var(cpus);\n\treturn called;\n}\n\nvoid kvm_flush_remote_tlbs(struct kvm *kvm)\n{\n\tlong dirty_count = kvm->tlbs_dirty;\n\n\tsmp_mb();\n\tif (make_all_cpus_request(kvm, KVM_REQ_TLB_FLUSH))\n\t\t++kvm->stat.remote_tlb_flush;\n\tcmpxchg(&kvm->tlbs_dirty, dirty_count, 0);\n}\n\nvoid kvm_reload_remote_mmus(struct kvm *kvm)\n{\n\tmake_all_cpus_request(kvm, KVM_REQ_MMU_RELOAD);\n}\n\nint kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)\n{\n\tstruct page *page;\n\tint r;\n\n\tmutex_init(&vcpu->mutex);\n\tvcpu->cpu = -1;\n\tvcpu->kvm = kvm;\n\tvcpu->vcpu_id = id;\n\tvcpu->pid = NULL;\n\tinit_waitqueue_head(&vcpu->wq);\n\tkvm_async_pf_vcpu_init(vcpu);\n\n\tpage = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\tif (!page) {\n\t\tr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tvcpu->run = page_address(page);\n\n\tkvm_vcpu_set_in_spin_loop(vcpu, false);\n\tkvm_vcpu_set_dy_eligible(vcpu, false);\n\n\tr = kvm_arch_vcpu_init(vcpu);\n\tif (r < 0)\n\t\tgoto fail_free_run;\n\treturn 0;\n\nfail_free_run:\n\tfree_page((unsigned long)vcpu->run);\nfail:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_init);\n\nvoid kvm_vcpu_uninit(struct kvm_vcpu *vcpu)\n{\n\tput_pid(vcpu->pid);\n\tkvm_arch_vcpu_uninit(vcpu);\n\tfree_page((unsigned long)vcpu->run);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_uninit);\n\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\nstatic inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)\n{\n\treturn container_of(mn, struct kvm, mmu_notifier);\n}\n\nstatic void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,\n\t\t\t\t\t     struct mm_struct *mm,\n\t\t\t\t\t     unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint need_tlb_flush, idx;\n\n\t/*\n\t * When ->invalidate_page runs, the linux pte has been zapped\n\t * already but the page is still allocated until\n\t * ->invalidate_page returns. So if we increase the sequence\n\t * here the kvm page fault will notice if the spte can't be\n\t * established because the page is going to be freed. If\n\t * instead the kvm page fault establishes the spte before\n\t * ->invalidate_page runs, kvm_unmap_hva will release it\n\t * before returning.\n\t *\n\t * The sequence increase only need to be seen at spin_unlock\n\t * time, and not at spin_lock time.\n\t *\n\t * Increasing the sequence after the spin_unlock would be\n\t * unsafe because the kvm page fault could then establish the\n\t * pte after kvm_unmap_hva returned, without noticing the page\n\t * is going to be freed.\n\t */\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\n\tkvm->mmu_notifier_seq++;\n\tneed_tlb_flush = kvm_unmap_hva(kvm, address) | kvm->tlbs_dirty;\n\t/* we've to flush the tlb before the pages can be freed */\n\tif (need_tlb_flush)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,\n\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpte_t pte)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tkvm->mmu_notifier_seq++;\n\tkvm_set_spte_hva(kvm, address, pte);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,\n\t\t\t\t\t\t    struct mm_struct *mm,\n\t\t\t\t\t\t    unsigned long start,\n\t\t\t\t\t\t    unsigned long end)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint need_tlb_flush = 0, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\t/*\n\t * The count increase must become visible at unlock time as no\n\t * spte can be established without taking the mmu_lock and\n\t * count is also read inside the mmu_lock critical section.\n\t */\n\tkvm->mmu_notifier_count++;\n\tneed_tlb_flush = kvm_unmap_hva_range(kvm, start, end);\n\tneed_tlb_flush |= kvm->tlbs_dirty;\n\t/* we've to flush the tlb before the pages can be freed */\n\tif (need_tlb_flush)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,\n\t\t\t\t\t\t  struct mm_struct *mm,\n\t\t\t\t\t\t  unsigned long start,\n\t\t\t\t\t\t  unsigned long end)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\n\tspin_lock(&kvm->mmu_lock);\n\t/*\n\t * This sequence increase will notify the kvm page fault that\n\t * the page that is going to be mapped in the spte could have\n\t * been freed.\n\t */\n\tkvm->mmu_notifier_seq++;\n\tsmp_wmb();\n\t/*\n\t * The above sequence increase must be visible before the\n\t * below count decrease, which is ensured by the smp_wmb above\n\t * in conjunction with the smp_rmb in mmu_notifier_retry().\n\t */\n\tkvm->mmu_notifier_count--;\n\tspin_unlock(&kvm->mmu_lock);\n\n\tBUG_ON(kvm->mmu_notifier_count < 0);\n}\n\nstatic int kvm_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,\n\t\t\t\t\t      struct mm_struct *mm,\n\t\t\t\t\t      unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint young, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\n\tyoung = kvm_age_hva(kvm, address);\n\tif (young)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn young;\n}\n\nstatic int kvm_mmu_notifier_test_young(struct mmu_notifier *mn,\n\t\t\t\t       struct mm_struct *mm,\n\t\t\t\t       unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint young, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tyoung = kvm_test_age_hva(kvm, address);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn young;\n}\n\nstatic void kvm_mmu_notifier_release(struct mmu_notifier *mn,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tkvm_arch_flush_shadow_all(kvm);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic const struct mmu_notifier_ops kvm_mmu_notifier_ops = {\n\t.invalidate_page\t= kvm_mmu_notifier_invalidate_page,\n\t.invalidate_range_start\t= kvm_mmu_notifier_invalidate_range_start,\n\t.invalidate_range_end\t= kvm_mmu_notifier_invalidate_range_end,\n\t.clear_flush_young\t= kvm_mmu_notifier_clear_flush_young,\n\t.test_young\t\t= kvm_mmu_notifier_test_young,\n\t.change_pte\t\t= kvm_mmu_notifier_change_pte,\n\t.release\t\t= kvm_mmu_notifier_release,\n};\n\nstatic int kvm_init_mmu_notifier(struct kvm *kvm)\n{\n\tkvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;\n\treturn mmu_notifier_register(&kvm->mmu_notifier, current->mm);\n}\n\n#else  /* !(CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER) */\n\nstatic int kvm_init_mmu_notifier(struct kvm *kvm)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER */\n\nstatic void kvm_init_memslots_id(struct kvm *kvm)\n{\n\tint i;\n\tstruct kvm_memslots *slots = kvm->memslots;\n\n\tfor (i = 0; i < KVM_MEM_SLOTS_NUM; i++)\n\t\tslots->id_to_index[i] = slots->memslots[i].id = i;\n}\n\nstatic struct kvm *kvm_create_vm(unsigned long type)\n{\n\tint r, i;\n\tstruct kvm *kvm = kvm_arch_alloc_vm();\n\n\tif (!kvm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tr = kvm_arch_init_vm(kvm, type);\n\tif (r)\n\t\tgoto out_err_nodisable;\n\n\tr = hardware_enable_all();\n\tif (r)\n\t\tgoto out_err_nodisable;\n\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n\tINIT_HLIST_HEAD(&kvm->mask_notifier_list);\n\tINIT_HLIST_HEAD(&kvm->irq_ack_notifier_list);\n#endif\n\n\tr = -ENOMEM;\n\tkvm->memslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\tif (!kvm->memslots)\n\t\tgoto out_err_nosrcu;\n\tkvm_init_memslots_id(kvm);\n\tif (init_srcu_struct(&kvm->srcu))\n\t\tgoto out_err_nosrcu;\n\tfor (i = 0; i < KVM_NR_BUSES; i++) {\n\t\tkvm->buses[i] = kzalloc(sizeof(struct kvm_io_bus),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!kvm->buses[i])\n\t\t\tgoto out_err;\n\t}\n\n\tspin_lock_init(&kvm->mmu_lock);\n\tkvm->mm = current->mm;\n\tatomic_inc(&kvm->mm->mm_count);\n\tkvm_eventfd_init(kvm);\n\tmutex_init(&kvm->lock);\n\tmutex_init(&kvm->irq_lock);\n\tmutex_init(&kvm->slots_lock);\n\tatomic_set(&kvm->users_count, 1);\n\n\tr = kvm_init_mmu_notifier(kvm);\n\tif (r)\n\t\tgoto out_err;\n\n\traw_spin_lock(&kvm_lock);\n\tlist_add(&kvm->vm_list, &vm_list);\n\traw_spin_unlock(&kvm_lock);\n\n\treturn kvm;\n\nout_err:\n\tcleanup_srcu_struct(&kvm->srcu);\nout_err_nosrcu:\n\thardware_disable_all();\nout_err_nodisable:\n\tfor (i = 0; i < KVM_NR_BUSES; i++)\n\t\tkfree(kvm->buses[i]);\n\tkfree(kvm->memslots);\n\tkvm_arch_free_vm(kvm);\n\treturn ERR_PTR(r);\n}\n\n/*\n * Avoid using vmalloc for a small buffer.\n * Should not be used when the size is statically known.\n */\nvoid *kvm_kvzalloc(unsigned long size)\n{\n\tif (size > PAGE_SIZE)\n\t\treturn vzalloc(size);\n\telse\n\t\treturn kzalloc(size, GFP_KERNEL);\n}\n\nvoid kvm_kvfree(const void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\tvfree(addr);\n\telse\n\t\tkfree(addr);\n}\n\nstatic void kvm_destroy_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n\tif (!memslot->dirty_bitmap)\n\t\treturn;\n\n\tkvm_kvfree(memslot->dirty_bitmap);\n\tmemslot->dirty_bitmap = NULL;\n}\n\n/*\n * Free any memory in @free but not in @dont.\n */\nstatic void kvm_free_physmem_slot(struct kvm_memory_slot *free,\n\t\t\t\t  struct kvm_memory_slot *dont)\n{\n\tif (!dont || free->dirty_bitmap != dont->dirty_bitmap)\n\t\tkvm_destroy_dirty_bitmap(free);\n\n\tkvm_arch_free_memslot(free, dont);\n\n\tfree->npages = 0;\n}\n\nvoid kvm_free_physmem(struct kvm *kvm)\n{\n\tstruct kvm_memslots *slots = kvm->memslots;\n\tstruct kvm_memory_slot *memslot;\n\n\tkvm_for_each_memslot(memslot, slots)\n\t\tkvm_free_physmem_slot(memslot, NULL);\n\n\tkfree(kvm->memslots);\n}\n\nstatic void kvm_destroy_vm(struct kvm *kvm)\n{\n\tint i;\n\tstruct mm_struct *mm = kvm->mm;\n\n\tkvm_arch_sync_events(kvm);\n\traw_spin_lock(&kvm_lock);\n\tlist_del(&kvm->vm_list);\n\traw_spin_unlock(&kvm_lock);\n\tkvm_free_irq_routing(kvm);\n\tfor (i = 0; i < KVM_NR_BUSES; i++)\n\t\tkvm_io_bus_destroy(kvm->buses[i]);\n\tkvm_coalesced_mmio_free(kvm);\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\tmmu_notifier_unregister(&kvm->mmu_notifier, kvm->mm);\n#else\n\tkvm_arch_flush_shadow_all(kvm);\n#endif\n\tkvm_arch_destroy_vm(kvm);\n\tkvm_free_physmem(kvm);\n\tcleanup_srcu_struct(&kvm->srcu);\n\tkvm_arch_free_vm(kvm);\n\thardware_disable_all();\n\tmmdrop(mm);\n}\n\nvoid kvm_get_kvm(struct kvm *kvm)\n{\n\tatomic_inc(&kvm->users_count);\n}\nEXPORT_SYMBOL_GPL(kvm_get_kvm);\n\nvoid kvm_put_kvm(struct kvm *kvm)\n{\n\tif (atomic_dec_and_test(&kvm->users_count))\n\t\tkvm_destroy_vm(kvm);\n}\nEXPORT_SYMBOL_GPL(kvm_put_kvm);\n\n\nstatic int kvm_vm_release(struct inode *inode, struct file *filp)\n{\n\tstruct kvm *kvm = filp->private_data;\n\n\tkvm_irqfd_release(kvm);\n\n\tkvm_put_kvm(kvm);\n\treturn 0;\n}\n\n/*\n * Allocation size is twice as large as the actual dirty bitmap size.\n * See x86's kvm_vm_ioctl_get_dirty_log() why this is needed.\n */\nstatic int kvm_create_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n#ifndef CONFIG_S390\n\tunsigned long dirty_bytes = 2 * kvm_dirty_bitmap_bytes(memslot);\n\n\tmemslot->dirty_bitmap = kvm_kvzalloc(dirty_bytes);\n\tif (!memslot->dirty_bitmap)\n\t\treturn -ENOMEM;\n\n#endif /* !CONFIG_S390 */\n\treturn 0;\n}\n\nstatic int cmp_memslot(const void *slot1, const void *slot2)\n{\n\tstruct kvm_memory_slot *s1, *s2;\n\n\ts1 = (struct kvm_memory_slot *)slot1;\n\ts2 = (struct kvm_memory_slot *)slot2;\n\n\tif (s1->npages < s2->npages)\n\t\treturn 1;\n\tif (s1->npages > s2->npages)\n\t\treturn -1;\n\n\treturn 0;\n}\n\n/*\n * Sort the memslots base on its size, so the larger slots\n * will get better fit.\n */\nstatic void sort_memslots(struct kvm_memslots *slots)\n{\n\tint i;\n\n\tsort(slots->memslots, KVM_MEM_SLOTS_NUM,\n\t      sizeof(struct kvm_memory_slot), cmp_memslot, NULL);\n\n\tfor (i = 0; i < KVM_MEM_SLOTS_NUM; i++)\n\t\tslots->id_to_index[slots->memslots[i].id] = i;\n}\n\nvoid update_memslots(struct kvm_memslots *slots, struct kvm_memory_slot *new)\n{\n\tif (new) {\n\t\tint id = new->id;\n\t\tstruct kvm_memory_slot *old = id_to_memslot(slots, id);\n\t\tunsigned long npages = old->npages;\n\n\t\t*old = *new;\n\t\tif (new->npages != npages)\n\t\t\tsort_memslots(slots);\n\t}\n\n\tslots->generation++;\n}\n\nstatic int check_memory_region_flags(struct kvm_userspace_memory_region *mem)\n{\n\tu32 valid_flags = KVM_MEM_LOG_DIRTY_PAGES;\n\n#ifdef KVM_CAP_READONLY_MEM\n\tvalid_flags |= KVM_MEM_READONLY;\n#endif\n\n\tif (mem->flags & ~valid_flags)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n/*\n * Allocate some memory and give it an address in the guest physical address\n * space.\n *\n * Discontiguous memory is allowed, mostly for framebuffers.\n *\n * Must be called holding mmap_sem for write.\n */\nint __kvm_set_memory_region(struct kvm *kvm,\n\t\t\t    struct kvm_userspace_memory_region *mem,\n\t\t\t    int user_alloc)\n{\n\tint r;\n\tgfn_t base_gfn;\n\tunsigned long npages;\n\tunsigned long i;\n\tstruct kvm_memory_slot *memslot;\n\tstruct kvm_memory_slot old, new;\n\tstruct kvm_memslots *slots, *old_memslots;\n\n\tr = check_memory_region_flags(mem);\n\tif (r)\n\t\tgoto out;\n\n\tr = -EINVAL;\n\t/* General sanity checks */\n\tif (mem->memory_size & (PAGE_SIZE - 1))\n\t\tgoto out;\n\tif (mem->guest_phys_addr & (PAGE_SIZE - 1))\n\t\tgoto out;\n\t/* We can read the guest memory with __xxx_user() later on. */\n\tif (user_alloc &&\n\t    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||\n\t     !access_ok(VERIFY_WRITE,\n\t\t\t(void __user *)(unsigned long)mem->userspace_addr,\n\t\t\tmem->memory_size)))\n\t\tgoto out;\n\tif (mem->slot >= KVM_MEM_SLOTS_NUM)\n\t\tgoto out;\n\tif (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)\n\t\tgoto out;\n\n\tmemslot = id_to_memslot(kvm->memslots, mem->slot);\n\tbase_gfn = mem->guest_phys_addr >> PAGE_SHIFT;\n\tnpages = mem->memory_size >> PAGE_SHIFT;\n\n\tr = -EINVAL;\n\tif (npages > KVM_MEM_MAX_NR_PAGES)\n\t\tgoto out;\n\n\tif (!npages)\n\t\tmem->flags &= ~KVM_MEM_LOG_DIRTY_PAGES;\n\n\tnew = old = *memslot;\n\n\tnew.id = mem->slot;\n\tnew.base_gfn = base_gfn;\n\tnew.npages = npages;\n\tnew.flags = mem->flags;\n\n\t/* Disallow changing a memory slot's size. */\n\tr = -EINVAL;\n\tif (npages && old.npages && npages != old.npages)\n\t\tgoto out_free;\n\n\t/* Check for overlaps */\n\tr = -EEXIST;\n\tfor (i = 0; i < KVM_MEMORY_SLOTS; ++i) {\n\t\tstruct kvm_memory_slot *s = &kvm->memslots->memslots[i];\n\n\t\tif (s == memslot || !s->npages)\n\t\t\tcontinue;\n\t\tif (!((base_gfn + npages <= s->base_gfn) ||\n\t\t      (base_gfn >= s->base_gfn + s->npages)))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Free page dirty bitmap if unneeded */\n\tif (!(new.flags & KVM_MEM_LOG_DIRTY_PAGES))\n\t\tnew.dirty_bitmap = NULL;\n\n\tr = -ENOMEM;\n\n\t/* Allocate if a slot is being created */\n\tif (npages && !old.npages) {\n\t\tnew.user_alloc = user_alloc;\n\t\tnew.userspace_addr = mem->userspace_addr;\n\n\t\tif (kvm_arch_create_memslot(&new, npages))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Allocate page dirty bitmap if needed */\n\tif ((new.flags & KVM_MEM_LOG_DIRTY_PAGES) && !new.dirty_bitmap) {\n\t\tif (kvm_create_dirty_bitmap(&new) < 0)\n\t\t\tgoto out_free;\n\t\t/* destroy any largepage mappings for dirty tracking */\n\t}\n\n\tif (!npages || base_gfn != old.base_gfn) {\n\t\tstruct kvm_memory_slot *slot;\n\n\t\tr = -ENOMEM;\n\t\tslots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!slots)\n\t\t\tgoto out_free;\n\t\tslot = id_to_memslot(slots, mem->slot);\n\t\tslot->flags |= KVM_MEMSLOT_INVALID;\n\n\t\tupdate_memslots(slots, NULL);\n\n\t\told_memslots = kvm->memslots;\n\t\trcu_assign_pointer(kvm->memslots, slots);\n\t\tsynchronize_srcu_expedited(&kvm->srcu);\n\t\t/* From this point no new shadow pages pointing to a deleted,\n\t\t * or moved, memslot will be created.\n\t\t *\n\t\t * validation of sp->gfn happens in:\n\t\t * \t- gfn_to_hva (kvm_read_guest, gfn_to_pfn)\n\t\t * \t- kvm_is_visible_gfn (mmu_check_roots)\n\t\t */\n\t\tkvm_arch_flush_shadow_memslot(kvm, slot);\n\t\tkfree(old_memslots);\n\t}\n\n\tr = kvm_arch_prepare_memory_region(kvm, &new, old, mem, user_alloc);\n\tif (r)\n\t\tgoto out_free;\n\n\t/* map/unmap the pages in iommu page table */\n\tif (npages) {\n\t\tr = kvm_iommu_map_pages(kvm, &new);\n\t\tif (r)\n\t\t\tgoto out_free;\n\t} else\n\t\tkvm_iommu_unmap_pages(kvm, &old);\n\n\tr = -ENOMEM;\n\tslots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),\n\t\t\tGFP_KERNEL);\n\tif (!slots)\n\t\tgoto out_free;\n\n\t/* actual memory is freed via old in kvm_free_physmem_slot below */\n\tif (!npages) {\n\t\tnew.dirty_bitmap = NULL;\n\t\tmemset(&new.arch, 0, sizeof(new.arch));\n\t}\n\n\tupdate_memslots(slots, &new);\n\told_memslots = kvm->memslots;\n\trcu_assign_pointer(kvm->memslots, slots);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\n\tkvm_arch_commit_memory_region(kvm, mem, old, user_alloc);\n\n\t/*\n\t * If the new memory slot is created, we need to clear all\n\t * mmio sptes.\n\t */\n\tif (npages && old.base_gfn != mem->guest_phys_addr >> PAGE_SHIFT)\n\t\tkvm_arch_flush_shadow_all(kvm);\n\n\tkvm_free_physmem_slot(&old, &new);\n\tkfree(old_memslots);\n\n\treturn 0;\n\nout_free:\n\tkvm_free_physmem_slot(&new, &old);\nout:\n\treturn r;\n\n}\nEXPORT_SYMBOL_GPL(__kvm_set_memory_region);\n\nint kvm_set_memory_region(struct kvm *kvm,\n\t\t\t  struct kvm_userspace_memory_region *mem,\n\t\t\t  int user_alloc)\n{\n\tint r;\n\n\tmutex_lock(&kvm->slots_lock);\n\tr = __kvm_set_memory_region(kvm, mem, user_alloc);\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_set_memory_region);\n\nint kvm_vm_ioctl_set_memory_region(struct kvm *kvm,\n\t\t\t\t   struct\n\t\t\t\t   kvm_userspace_memory_region *mem,\n\t\t\t\t   int user_alloc)\n{\n\tif (mem->slot >= KVM_MEMORY_SLOTS)\n\t\treturn -EINVAL;\n\treturn kvm_set_memory_region(kvm, mem, user_alloc);\n}\n\nint kvm_get_dirty_log(struct kvm *kvm,\n\t\t\tstruct kvm_dirty_log *log, int *is_dirty)\n{\n\tstruct kvm_memory_slot *memslot;\n\tint r, i;\n\tunsigned long n;\n\tunsigned long any = 0;\n\n\tr = -EINVAL;\n\tif (log->slot >= KVM_MEMORY_SLOTS)\n\t\tgoto out;\n\n\tmemslot = id_to_memslot(kvm->memslots, log->slot);\n\tr = -ENOENT;\n\tif (!memslot->dirty_bitmap)\n\t\tgoto out;\n\n\tn = kvm_dirty_bitmap_bytes(memslot);\n\n\tfor (i = 0; !any && i < n/sizeof(long); ++i)\n\t\tany = memslot->dirty_bitmap[i];\n\n\tr = -EFAULT;\n\tif (copy_to_user(log->dirty_bitmap, memslot->dirty_bitmap, n))\n\t\tgoto out;\n\n\tif (any)\n\t\t*is_dirty = 1;\n\n\tr = 0;\nout:\n\treturn r;\n}\n\nbool kvm_largepages_enabled(void)\n{\n\treturn largepages_enabled;\n}\n\nvoid kvm_disable_largepages(void)\n{\n\tlargepages_enabled = false;\n}\nEXPORT_SYMBOL_GPL(kvm_disable_largepages);\n\nstruct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_memslot(kvm_memslots(kvm), gfn);\n}\nEXPORT_SYMBOL_GPL(gfn_to_memslot);\n\nint kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct kvm_memory_slot *memslot = gfn_to_memslot(kvm, gfn);\n\n\tif (!memslot || memslot->id >= KVM_MEMORY_SLOTS ||\n\t      memslot->flags & KVM_MEMSLOT_INVALID)\n\t\treturn 0;\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_is_visible_gfn);\n\nunsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long addr, size;\n\n\tsize = PAGE_SIZE;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn PAGE_SIZE;\n\n\tdown_read(&current->mm->mmap_sem);\n\tvma = find_vma(current->mm, addr);\n\tif (!vma)\n\t\tgoto out;\n\n\tsize = vma_kernel_pagesize(vma);\n\nout:\n\tup_read(&current->mm->mmap_sem);\n\n\treturn size;\n}\n\nstatic bool memslot_is_readonly(struct kvm_memory_slot *slot)\n{\n\treturn slot->flags & KVM_MEM_READONLY;\n}\n\nstatic unsigned long __gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t       gfn_t *nr_pages, bool write)\n{\n\tif (!slot || slot->flags & KVM_MEMSLOT_INVALID)\n\t\treturn KVM_HVA_ERR_BAD;\n\n\tif (memslot_is_readonly(slot) && write)\n\t\treturn KVM_HVA_ERR_RO_BAD;\n\n\tif (nr_pages)\n\t\t*nr_pages = slot->npages - (gfn - slot->base_gfn);\n\n\treturn __gfn_to_hva_memslot(slot, gfn);\n}\n\nstatic unsigned long gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t     gfn_t *nr_pages)\n{\n\treturn __gfn_to_hva_many(slot, gfn, nr_pages, true);\n}\n\nunsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot,\n\t\t\t\t gfn_t gfn)\n{\n\treturn gfn_to_hva_many(slot, gfn, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_hva_memslot);\n\nunsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn)\n{\n\treturn gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_hva);\n\n/*\n * The hva returned by this function is only allowed to be read.\n * It should pair with kvm_read_hva() or kvm_read_hva_atomic().\n */\nstatic unsigned long gfn_to_hva_read(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL, false);\n}\n\nstatic int kvm_read_hva(void *data, void __user *hva, int len)\n{\n\treturn __copy_from_user(data, hva, len);\n}\n\nstatic int kvm_read_hva_atomic(void *data, void __user *hva, int len)\n{\n\treturn __copy_from_user_inatomic(data, hva, len);\n}\n\nint get_user_page_nowait(struct task_struct *tsk, struct mm_struct *mm,\n\tunsigned long start, int write, struct page **page)\n{\n\tint flags = FOLL_TOUCH | FOLL_NOWAIT | FOLL_HWPOISON | FOLL_GET;\n\n\tif (write)\n\t\tflags |= FOLL_WRITE;\n\n\treturn __get_user_pages(tsk, mm, start, 1, flags, page, NULL, NULL);\n}\n\nstatic inline int check_user_page_hwpoison(unsigned long addr)\n{\n\tint rc, flags = FOLL_TOUCH | FOLL_HWPOISON | FOLL_WRITE;\n\n\trc = __get_user_pages(current, current->mm, addr, 1,\n\t\t\t      flags, NULL, NULL, NULL);\n\treturn rc == -EHWPOISON;\n}\n\n/*\n * The atomic path to get the writable pfn which will be stored in @pfn,\n * true indicates success, otherwise false is returned.\n */\nstatic bool hva_to_pfn_fast(unsigned long addr, bool atomic, bool *async,\n\t\t\t    bool write_fault, bool *writable, pfn_t *pfn)\n{\n\tstruct page *page[1];\n\tint npages;\n\n\tif (!(async || atomic))\n\t\treturn false;\n\n\t/*\n\t * Fast pin a writable pfn only if it is a write fault request\n\t * or the caller allows to map a writable pfn for a read fault\n\t * request.\n\t */\n\tif (!(write_fault || writable))\n\t\treturn false;\n\n\tnpages = __get_user_pages_fast(addr, 1, 1, page);\n\tif (npages == 1) {\n\t\t*pfn = page_to_pfn(page[0]);\n\n\t\tif (writable)\n\t\t\t*writable = true;\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * The slow path to get the pfn of the specified host virtual address,\n * 1 indicates success, -errno is returned if error is detected.\n */\nstatic int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,\n\t\t\t   bool *writable, pfn_t *pfn)\n{\n\tstruct page *page[1];\n\tint npages = 0;\n\n\tmight_sleep();\n\n\tif (writable)\n\t\t*writable = write_fault;\n\n\tif (async) {\n\t\tdown_read(&current->mm->mmap_sem);\n\t\tnpages = get_user_page_nowait(current, current->mm,\n\t\t\t\t\t      addr, write_fault, page);\n\t\tup_read(&current->mm->mmap_sem);\n\t} else\n\t\tnpages = get_user_pages_fast(addr, 1, write_fault,\n\t\t\t\t\t     page);\n\tif (npages != 1)\n\t\treturn npages;\n\n\t/* map read fault as writable if possible */\n\tif (unlikely(!write_fault) && writable) {\n\t\tstruct page *wpage[1];\n\n\t\tnpages = __get_user_pages_fast(addr, 1, 1, wpage);\n\t\tif (npages == 1) {\n\t\t\t*writable = true;\n\t\t\tput_page(page[0]);\n\t\t\tpage[0] = wpage[0];\n\t\t}\n\n\t\tnpages = 1;\n\t}\n\t*pfn = page_to_pfn(page[0]);\n\treturn npages;\n}\n\nstatic bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)\n{\n\tif (unlikely(!(vma->vm_flags & VM_READ)))\n\t\treturn false;\n\n\tif (write_fault && (unlikely(!(vma->vm_flags & VM_WRITE))))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Pin guest page in memory and return its pfn.\n * @addr: host virtual address which maps memory to the guest\n * @atomic: whether this function can sleep\n * @async: whether this function need to wait IO complete if the\n *         host page is not in the memory\n * @write_fault: whether we should get a writable host page\n * @writable: whether it allows to map a writable host page for !@write_fault\n *\n * The function will map a writable host page for these two cases:\n * 1): @write_fault = true\n * 2): @write_fault = false && @writable, @writable will tell the caller\n *     whether the mapping is writable.\n */\nstatic pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,\n\t\t\tbool write_fault, bool *writable)\n{\n\tstruct vm_area_struct *vma;\n\tpfn_t pfn = 0;\n\tint npages;\n\n\t/* we can do it either atomically or asynchronously, not both */\n\tBUG_ON(atomic && async);\n\n\tif (hva_to_pfn_fast(addr, atomic, async, write_fault, writable, &pfn))\n\t\treturn pfn;\n\n\tif (atomic)\n\t\treturn KVM_PFN_ERR_FAULT;\n\n\tnpages = hva_to_pfn_slow(addr, async, write_fault, writable, &pfn);\n\tif (npages == 1)\n\t\treturn pfn;\n\n\tdown_read(&current->mm->mmap_sem);\n\tif (npages == -EHWPOISON ||\n\t      (!async && check_user_page_hwpoison(addr))) {\n\t\tpfn = KVM_PFN_ERR_HWPOISON;\n\t\tgoto exit;\n\t}\n\n\tvma = find_vma_intersection(current->mm, addr, addr + 1);\n\n\tif (vma == NULL)\n\t\tpfn = KVM_PFN_ERR_FAULT;\n\telse if ((vma->vm_flags & VM_PFNMAP)) {\n\t\tpfn = ((addr - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\t\tBUG_ON(!kvm_is_mmio_pfn(pfn));\n\t} else {\n\t\tif (async && vma_is_valid(vma, write_fault))\n\t\t\t*async = true;\n\t\tpfn = KVM_PFN_ERR_FAULT;\n\t}\nexit:\n\tup_read(&current->mm->mmap_sem);\n\treturn pfn;\n}\n\nstatic pfn_t\n__gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,\n\t\t     bool *async, bool write_fault, bool *writable)\n{\n\tunsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);\n\n\tif (addr == KVM_HVA_ERR_RO_BAD)\n\t\treturn KVM_PFN_ERR_RO_FAULT;\n\n\tif (kvm_is_error_hva(addr))\n\t\treturn KVM_PFN_ERR_BAD;\n\n\t/* Do not map writable pfn in the readonly memslot. */\n\tif (writable && memslot_is_readonly(slot)) {\n\t\t*writable = false;\n\t\twritable = NULL;\n\t}\n\n\treturn hva_to_pfn(addr, atomic, async, write_fault,\n\t\t\t  writable);\n}\n\nstatic pfn_t __gfn_to_pfn(struct kvm *kvm, gfn_t gfn, bool atomic, bool *async,\n\t\t\t  bool write_fault, bool *writable)\n{\n\tstruct kvm_memory_slot *slot;\n\n\tif (async)\n\t\t*async = false;\n\n\tslot = gfn_to_memslot(kvm, gfn);\n\n\treturn __gfn_to_pfn_memslot(slot, gfn, atomic, async, write_fault,\n\t\t\t\t    writable);\n}\n\npfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_pfn(kvm, gfn, true, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_atomic);\n\npfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async,\n\t\t       bool write_fault, bool *writable)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, async, write_fault, writable);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_async);\n\npfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn);\n\npfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,\n\t\t      bool *writable)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, NULL, write_fault, writable);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_prot);\n\npfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\treturn __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL);\n}\n\npfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\treturn __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_memslot_atomic);\n\nint gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,\n\t\t\t\t\t\t\t\t  int nr_pages)\n{\n\tunsigned long addr;\n\tgfn_t entry;\n\n\taddr = gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, &entry);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -1;\n\n\tif (entry < nr_pages)\n\t\treturn 0;\n\n\treturn __get_user_pages_fast(addr, nr_pages, 1, pages);\n}\nEXPORT_SYMBOL_GPL(gfn_to_page_many_atomic);\n\nstatic struct page *kvm_pfn_to_page(pfn_t pfn)\n{\n\tif (is_error_pfn(pfn))\n\t\treturn KVM_ERR_PTR_BAD_PAGE;\n\n\tif (kvm_is_mmio_pfn(pfn)) {\n\t\tWARN_ON(1);\n\t\treturn KVM_ERR_PTR_BAD_PAGE;\n\t}\n\n\treturn pfn_to_page(pfn);\n}\n\nstruct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)\n{\n\tpfn_t pfn;\n\n\tpfn = gfn_to_pfn(kvm, gfn);\n\n\treturn kvm_pfn_to_page(pfn);\n}\n\nEXPORT_SYMBOL_GPL(gfn_to_page);\n\nvoid kvm_release_page_clean(struct page *page)\n{\n\tWARN_ON(is_error_page(page));\n\n\tkvm_release_pfn_clean(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_release_page_clean);\n\nvoid kvm_release_pfn_clean(pfn_t pfn)\n{\n\tWARN_ON(is_error_pfn(pfn));\n\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tput_page(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_release_pfn_clean);\n\nvoid kvm_release_page_dirty(struct page *page)\n{\n\tWARN_ON(is_error_page(page));\n\n\tkvm_release_pfn_dirty(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_release_page_dirty);\n\nvoid kvm_release_pfn_dirty(pfn_t pfn)\n{\n\tkvm_set_pfn_dirty(pfn);\n\tkvm_release_pfn_clean(pfn);\n}\nEXPORT_SYMBOL_GPL(kvm_release_pfn_dirty);\n\nvoid kvm_set_page_dirty(struct page *page)\n{\n\tkvm_set_pfn_dirty(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_set_page_dirty);\n\nvoid kvm_set_pfn_dirty(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn)) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\t\tif (!PageReserved(page))\n\t\t\tSetPageDirty(page);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);\n\nvoid kvm_set_pfn_accessed(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tmark_page_accessed(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_set_pfn_accessed);\n\nvoid kvm_get_pfn(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tget_page(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_get_pfn);\n\nstatic int next_segment(unsigned long len, int offset)\n{\n\tif (len > PAGE_SIZE - offset)\n\t\treturn PAGE_SIZE - offset;\n\telse\n\t\treturn len;\n}\n\nint kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,\n\t\t\tint len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva_read(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = kvm_read_hva(data, (void __user *)addr + offset, len);\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_page);\n\nint kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n\twhile ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_read_guest_page(kvm, gfn, data, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\tdata += seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest);\n\nint kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,\n\t\t\t  unsigned long len)\n{\n\tint r;\n\tunsigned long addr;\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint offset = offset_in_page(gpa);\n\n\taddr = gfn_to_hva_read(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tpagefault_disable();\n\tr = kvm_read_hva_atomic(data, (void __user *)addr + offset, len);\n\tpagefault_enable();\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(kvm_read_guest_atomic);\n\nint kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,\n\t\t\t int offset, int len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = __copy_to_user((void __user *)addr + offset, data, len);\n\tif (r)\n\t\treturn -EFAULT;\n\tmark_page_dirty(kvm, gfn);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_page);\n\nint kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,\n\t\t    unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n\twhile ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_write_guest_page(kvm, gfn, data, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\tdata += seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\n\nint kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t      gpa_t gpa)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint offset = offset_in_page(gpa);\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\n\tghc->gpa = gpa;\n\tghc->generation = slots->generation;\n\tghc->memslot = gfn_to_memslot(kvm, gfn);\n\tghc->hva = gfn_to_hva_many(ghc->memslot, gfn, NULL);\n\tif (!kvm_is_error_hva(ghc->hva))\n\t\tghc->hva += offset;\n\telse\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_gfn_to_hva_cache_init);\n\nint kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t   void *data, unsigned long len)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint r;\n\n\tif (slots->generation != ghc->generation)\n\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa);\n\n\tif (kvm_is_error_hva(ghc->hva))\n\t\treturn -EFAULT;\n\n\tr = __copy_to_user((void __user *)ghc->hva, data, len);\n\tif (r)\n\t\treturn -EFAULT;\n\tmark_page_dirty_in_slot(kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_cached);\n\nint kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t   void *data, unsigned long len)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint r;\n\n\tif (slots->generation != ghc->generation)\n\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa);\n\n\tif (kvm_is_error_hva(ghc->hva))\n\t\treturn -EFAULT;\n\n\tr = __copy_from_user(data, (void __user *)ghc->hva, len);\n\tif (r)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_cached);\n\nint kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len)\n{\n\treturn kvm_write_guest_page(kvm, gfn, (const void *) empty_zero_page,\n\t\t\t\t    offset, len);\n}\nEXPORT_SYMBOL_GPL(kvm_clear_guest_page);\n\nint kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n        while ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_clear_guest_page(kvm, gfn, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_clear_guest);\n\nvoid mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,\n\t\t\t     gfn_t gfn)\n{\n\tif (memslot && memslot->dirty_bitmap) {\n\t\tunsigned long rel_gfn = gfn - memslot->base_gfn;\n\n\t\t/* TODO: introduce set_bit_le() and use it */\n\t\ttest_and_set_bit_le(rel_gfn, memslot->dirty_bitmap);\n\t}\n}\n\nvoid mark_page_dirty(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct kvm_memory_slot *memslot;\n\n\tmemslot = gfn_to_memslot(kvm, gfn);\n\tmark_page_dirty_in_slot(kvm, memslot, gfn);\n}\n\n/*\n * The vCPU has executed a HLT instruction with in-kernel mode enabled.\n */\nvoid kvm_vcpu_block(struct kvm_vcpu *vcpu)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (kvm_arch_vcpu_runnable(vcpu)) {\n\t\t\tkvm_make_request(KVM_REQ_UNHALT, vcpu);\n\t\t\tbreak;\n\t\t}\n\t\tif (kvm_cpu_has_pending_timer(vcpu))\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\n\tfinish_wait(&vcpu->wq, &wait);\n}\n\n#ifndef CONFIG_S390\n/*\n * Kick a sleeping VCPU, or a guest VCPU in guest mode, into host kernel mode.\n */\nvoid kvm_vcpu_kick(struct kvm_vcpu *vcpu)\n{\n\tint me;\n\tint cpu = vcpu->cpu;\n\twait_queue_head_t *wqp;\n\n\twqp = kvm_arch_vcpu_wq(vcpu);\n\tif (waitqueue_active(wqp)) {\n\t\twake_up_interruptible(wqp);\n\t\t++vcpu->stat.halt_wakeup;\n\t}\n\n\tme = get_cpu();\n\tif (cpu != me && (unsigned)cpu < nr_cpu_ids && cpu_online(cpu))\n\t\tif (kvm_arch_vcpu_should_kick(vcpu))\n\t\t\tsmp_send_reschedule(cpu);\n\tput_cpu();\n}\n#endif /* !CONFIG_S390 */\n\nvoid kvm_resched(struct kvm_vcpu *vcpu)\n{\n\tif (!need_resched())\n\t\treturn;\n\tcond_resched();\n}\nEXPORT_SYMBOL_GPL(kvm_resched);\n\nbool kvm_vcpu_yield_to(struct kvm_vcpu *target)\n{\n\tstruct pid *pid;\n\tstruct task_struct *task = NULL;\n\n\trcu_read_lock();\n\tpid = rcu_dereference(target->pid);\n\tif (pid)\n\t\ttask = get_pid_task(target->pid, PIDTYPE_PID);\n\trcu_read_unlock();\n\tif (!task)\n\t\treturn false;\n\tif (task->flags & PF_VCPU) {\n\t\tput_task_struct(task);\n\t\treturn false;\n\t}\n\tif (yield_to(task, 1)) {\n\t\tput_task_struct(task);\n\t\treturn true;\n\t}\n\tput_task_struct(task);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_yield_to);\n\n#ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT\n/*\n * Helper that checks whether a VCPU is eligible for directed yield.\n * Most eligible candidate to yield is decided by following heuristics:\n *\n *  (a) VCPU which has not done pl-exit or cpu relax intercepted recently\n *  (preempted lock holder), indicated by @in_spin_loop.\n *  Set at the beiginning and cleared at the end of interception/PLE handler.\n *\n *  (b) VCPU which has done pl-exit/ cpu relax intercepted but did not get\n *  chance last time (mostly it has become eligible now since we have probably\n *  yielded to lockholder in last iteration. This is done by toggling\n *  @dy_eligible each time a VCPU checked for eligibility.)\n *\n *  Yielding to a recently pl-exited/cpu relax intercepted VCPU before yielding\n *  to preempted lock-holder could result in wrong VCPU selection and CPU\n *  burning. Giving priority for a potential lock-holder increases lock\n *  progress.\n *\n *  Since algorithm is based on heuristics, accessing another VCPU data without\n *  locking does not harm. It may result in trying to yield to  same VCPU, fail\n *  and continue with next VCPU and so on.\n */\nbool kvm_vcpu_eligible_for_directed_yield(struct kvm_vcpu *vcpu)\n{\n\tbool eligible;\n\n\teligible = !vcpu->spin_loop.in_spin_loop ||\n\t\t\t(vcpu->spin_loop.in_spin_loop &&\n\t\t\t vcpu->spin_loop.dy_eligible);\n\n\tif (vcpu->spin_loop.in_spin_loop)\n\t\tkvm_vcpu_set_dy_eligible(vcpu, !vcpu->spin_loop.dy_eligible);\n\n\treturn eligible;\n}\n#endif\nvoid kvm_vcpu_on_spin(struct kvm_vcpu *me)\n{\n\tstruct kvm *kvm = me->kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint last_boosted_vcpu = me->kvm->last_boosted_vcpu;\n\tint yielded = 0;\n\tint pass;\n\tint i;\n\n\tkvm_vcpu_set_in_spin_loop(me, true);\n\t/*\n\t * We boost the priority of a VCPU that is runnable but not\n\t * currently running, because it got preempted by something\n\t * else and called schedule in __vcpu_run.  Hopefully that\n\t * VCPU is holding the lock that we need and will release it.\n\t * We approximate round-robin by starting at the last boosted VCPU.\n\t */\n\tfor (pass = 0; pass < 2 && !yielded; pass++) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (!pass && i <= last_boosted_vcpu) {\n\t\t\t\ti = last_boosted_vcpu;\n\t\t\t\tcontinue;\n\t\t\t} else if (pass && i > last_boosted_vcpu)\n\t\t\t\tbreak;\n\t\t\tif (vcpu == me)\n\t\t\t\tcontinue;\n\t\t\tif (waitqueue_active(&vcpu->wq))\n\t\t\t\tcontinue;\n\t\t\tif (!kvm_vcpu_eligible_for_directed_yield(vcpu))\n\t\t\t\tcontinue;\n\t\t\tif (kvm_vcpu_yield_to(vcpu)) {\n\t\t\t\tkvm->last_boosted_vcpu = i;\n\t\t\t\tyielded = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tkvm_vcpu_set_in_spin_loop(me, false);\n\n\t/* Ensure vcpu is not eligible during next spinloop */\n\tkvm_vcpu_set_dy_eligible(me, false);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_on_spin);\n\nstatic int kvm_vcpu_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct kvm_vcpu *vcpu = vma->vm_file->private_data;\n\tstruct page *page;\n\n\tif (vmf->pgoff == 0)\n\t\tpage = virt_to_page(vcpu->run);\n#ifdef CONFIG_X86\n\telse if (vmf->pgoff == KVM_PIO_PAGE_OFFSET)\n\t\tpage = virt_to_page(vcpu->arch.pio_data);\n#endif\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\telse if (vmf->pgoff == KVM_COALESCED_MMIO_PAGE_OFFSET)\n\t\tpage = virt_to_page(vcpu->kvm->coalesced_mmio_ring);\n#endif\n\telse\n\t\treturn kvm_arch_vcpu_fault(vcpu, vmf);\n\tget_page(page);\n\tvmf->page = page;\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct kvm_vcpu_vm_ops = {\n\t.fault = kvm_vcpu_fault,\n};\n\nstatic int kvm_vcpu_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_ops = &kvm_vcpu_vm_ops;\n\treturn 0;\n}\n\nstatic int kvm_vcpu_release(struct inode *inode, struct file *filp)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\n\tkvm_put_kvm(vcpu->kvm);\n\treturn 0;\n}\n\nstatic struct file_operations kvm_vcpu_fops = {\n\t.release        = kvm_vcpu_release,\n\t.unlocked_ioctl = kvm_vcpu_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl   = kvm_vcpu_compat_ioctl,\n#endif\n\t.mmap           = kvm_vcpu_mmap,\n\t.llseek\t\t= noop_llseek,\n};\n\n/*\n * Allocates an inode for the vcpu.\n */\nstatic int create_vcpu_fd(struct kvm_vcpu *vcpu)\n{\n\treturn anon_inode_getfd(\"kvm-vcpu\", &kvm_vcpu_fops, vcpu, O_RDWR);\n}\n\n/*\n * Creates some virtual cpus.  Good luck creating more than one.\n */\nstatic int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)\n{\n\tint r;\n\tstruct kvm_vcpu *vcpu, *v;\n\n\tvcpu = kvm_arch_vcpu_create(kvm, id);\n\tif (IS_ERR(vcpu))\n\t\treturn PTR_ERR(vcpu);\n\n\tpreempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);\n\n\tr = kvm_arch_vcpu_setup(vcpu);\n\tif (r)\n\t\tgoto vcpu_destroy;\n\n\tmutex_lock(&kvm->lock);\n\tif (!kvm_vcpu_compatible(vcpu)) {\n\t\tr = -EINVAL;\n\t\tgoto unlock_vcpu_destroy;\n\t}\n\tif (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {\n\t\tr = -EINVAL;\n\t\tgoto unlock_vcpu_destroy;\n\t}\n\n\tkvm_for_each_vcpu(r, v, kvm)\n\t\tif (v->vcpu_id == id) {\n\t\t\tr = -EEXIST;\n\t\t\tgoto unlock_vcpu_destroy;\n\t\t}\n\n\tBUG_ON(kvm->vcpus[atomic_read(&kvm->online_vcpus)]);\n\n\t/* Now it's all set up, let userspace reach it */\n\tkvm_get_kvm(kvm);\n\tr = create_vcpu_fd(vcpu);\n\tif (r < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\tgoto unlock_vcpu_destroy;\n\t}\n\n\tkvm->vcpus[atomic_read(&kvm->online_vcpus)] = vcpu;\n\tsmp_wmb();\n\tatomic_inc(&kvm->online_vcpus);\n\n\tmutex_unlock(&kvm->lock);\n\treturn r;\n\nunlock_vcpu_destroy:\n\tmutex_unlock(&kvm->lock);\nvcpu_destroy:\n\tkvm_arch_vcpu_destroy(vcpu);\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)\n{\n\tif (sigset) {\n\t\tsigdelsetmask(sigset, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\t\tvcpu->sigset_active = 1;\n\t\tvcpu->sigset = *sigset;\n\t} else\n\t\tvcpu->sigset_active = 0;\n\treturn 0;\n}\n\nstatic long kvm_vcpu_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\tstruct kvm_fpu *fpu = NULL;\n\tstruct kvm_sregs *kvm_sregs = NULL;\n\n\tif (vcpu->kvm->mm != current->mm)\n\t\treturn -EIO;\n\n#if defined(CONFIG_S390) || defined(CONFIG_PPC)\n\t/*\n\t * Special cases: vcpu ioctls that are asynchronous to vcpu execution,\n\t * so vcpu_load() would break it.\n\t */\n\tif (ioctl == KVM_S390_INTERRUPT || ioctl == KVM_INTERRUPT)\n\t\treturn kvm_arch_vcpu_ioctl(filp, ioctl, arg);\n#endif\n\n\n\tvcpu_load(vcpu);\n\tswitch (ioctl) {\n\tcase KVM_RUN:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);\n\t\ttrace_kvm_userspace_exit(vcpu->run->exit_reason, r);\n\t\tbreak;\n\tcase KVM_GET_REGS: {\n\t\tstruct kvm_regs *kvm_regs;\n\n\t\tr = -ENOMEM;\n\t\tkvm_regs = kzalloc(sizeof(struct kvm_regs), GFP_KERNEL);\n\t\tif (!kvm_regs)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_regs(vcpu, kvm_regs);\n\t\tif (r)\n\t\t\tgoto out_free1;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, kvm_regs, sizeof(struct kvm_regs)))\n\t\t\tgoto out_free1;\n\t\tr = 0;\nout_free1:\n\t\tkfree(kvm_regs);\n\t\tbreak;\n\t}\n\tcase KVM_SET_REGS: {\n\t\tstruct kvm_regs *kvm_regs;\n\n\t\tr = -ENOMEM;\n\t\tkvm_regs = memdup_user(argp, sizeof(*kvm_regs));\n\t\tif (IS_ERR(kvm_regs)) {\n\t\t\tr = PTR_ERR(kvm_regs);\n\t\t\tgoto out;\n\t\t}\n\t\tr = kvm_arch_vcpu_ioctl_set_regs(vcpu, kvm_regs);\n\t\tif (r)\n\t\t\tgoto out_free2;\n\t\tr = 0;\nout_free2:\n\t\tkfree(kvm_regs);\n\t\tbreak;\n\t}\n\tcase KVM_GET_SREGS: {\n\t\tkvm_sregs = kzalloc(sizeof(struct kvm_sregs), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!kvm_sregs)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_sregs(vcpu, kvm_sregs);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, kvm_sregs, sizeof(struct kvm_sregs)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SREGS: {\n\t\tkvm_sregs = memdup_user(argp, sizeof(*kvm_sregs));\n\t\tif (IS_ERR(kvm_sregs)) {\n\t\t\tr = PTR_ERR(kvm_sregs);\n\t\t\tgoto out;\n\t\t}\n\t\tr = kvm_arch_vcpu_ioctl_set_sregs(vcpu, kvm_sregs);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MP_STATE: {\n\t\tstruct kvm_mp_state mp_state;\n\n\t\tr = kvm_arch_vcpu_ioctl_get_mpstate(vcpu, &mp_state);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &mp_state, sizeof mp_state))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_MP_STATE: {\n\t\tstruct kvm_mp_state mp_state;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mp_state, argp, sizeof mp_state))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_mpstate(vcpu, &mp_state);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_TRANSLATE: {\n\t\tstruct kvm_translation tr;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&tr, argp, sizeof tr))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_translate(vcpu, &tr);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &tr, sizeof tr))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_GUEST_DEBUG: {\n\t\tstruct kvm_guest_debug dbg;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dbg, argp, sizeof dbg))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_guest_debug(vcpu, &dbg);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SIGNAL_MASK: {\n\t\tstruct kvm_signal_mask __user *sigmask_arg = argp;\n\t\tstruct kvm_signal_mask kvm_sigmask;\n\t\tsigset_t sigset, *p;\n\n\t\tp = NULL;\n\t\tif (argp) {\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&kvm_sigmask, argp,\n\t\t\t\t\t   sizeof kvm_sigmask))\n\t\t\t\tgoto out;\n\t\t\tr = -EINVAL;\n\t\t\tif (kvm_sigmask.len != sizeof sigset)\n\t\t\t\tgoto out;\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&sigset, sigmask_arg->sigset,\n\t\t\t\t\t   sizeof sigset))\n\t\t\t\tgoto out;\n\t\t\tp = &sigset;\n\t\t}\n\t\tr = kvm_vcpu_ioctl_set_sigmask(vcpu, p);\n\t\tbreak;\n\t}\n\tcase KVM_GET_FPU: {\n\t\tfpu = kzalloc(sizeof(struct kvm_fpu), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!fpu)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_fpu(vcpu, fpu);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, fpu, sizeof(struct kvm_fpu)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_FPU: {\n\t\tfpu = memdup_user(argp, sizeof(*fpu));\n\t\tif (IS_ERR(fpu)) {\n\t\t\tr = PTR_ERR(fpu);\n\t\t\tgoto out;\n\t\t}\n\t\tr = kvm_arch_vcpu_ioctl_set_fpu(vcpu, fpu);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_arch_vcpu_ioctl(filp, ioctl, arg);\n\t}\nout:\n\tvcpu_put(vcpu);\n\tkfree(fpu);\n\tkfree(kvm_sregs);\n\treturn r;\n}\n\n#ifdef CONFIG_COMPAT\nstatic long kvm_vcpu_compat_ioctl(struct file *filp,\n\t\t\t\t  unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = compat_ptr(arg);\n\tint r;\n\n\tif (vcpu->kvm->mm != current->mm)\n\t\treturn -EIO;\n\n\tswitch (ioctl) {\n\tcase KVM_SET_SIGNAL_MASK: {\n\t\tstruct kvm_signal_mask __user *sigmask_arg = argp;\n\t\tstruct kvm_signal_mask kvm_sigmask;\n\t\tcompat_sigset_t csigset;\n\t\tsigset_t sigset;\n\n\t\tif (argp) {\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&kvm_sigmask, argp,\n\t\t\t\t\t   sizeof kvm_sigmask))\n\t\t\t\tgoto out;\n\t\t\tr = -EINVAL;\n\t\t\tif (kvm_sigmask.len != sizeof csigset)\n\t\t\t\tgoto out;\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&csigset, sigmask_arg->sigset,\n\t\t\t\t\t   sizeof csigset))\n\t\t\t\tgoto out;\n\t\t}\n\t\tsigset_from_compat(&sigset, &csigset);\n\t\tr = kvm_vcpu_ioctl_set_sigmask(vcpu, &sigset);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_vcpu_ioctl(filp, ioctl, arg);\n\t}\n\nout:\n\treturn r;\n}\n#endif\n\nstatic long kvm_vm_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\n\tif (kvm->mm != current->mm)\n\t\treturn -EIO;\n\tswitch (ioctl) {\n\tcase KVM_CREATE_VCPU:\n\t\tr = kvm_vm_ioctl_create_vcpu(kvm, arg);\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase KVM_SET_USER_MEMORY_REGION: {\n\t\tstruct kvm_userspace_memory_region kvm_userspace_mem;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&kvm_userspace_mem, argp,\n\t\t\t\t\t\tsizeof kvm_userspace_mem))\n\t\t\tgoto out;\n\n\t\tr = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem, 1);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\tcase KVM_GET_DIRTY_LOG: {\n\t\tstruct kvm_dirty_log log;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&log, argp, sizeof log))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_dirty_log(kvm, &log);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\tcase KVM_REGISTER_COALESCED_MMIO: {\n\t\tstruct kvm_coalesced_mmio_zone zone;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&zone, argp, sizeof zone))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_register_coalesced_mmio(kvm, &zone);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_UNREGISTER_COALESCED_MMIO: {\n\t\tstruct kvm_coalesced_mmio_zone zone;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&zone, argp, sizeof zone))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_unregister_coalesced_mmio(kvm, &zone);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_IRQFD: {\n\t\tstruct kvm_irqfd data;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&data, argp, sizeof data))\n\t\t\tgoto out;\n\t\tr = kvm_irqfd(kvm, &data);\n\t\tbreak;\n\t}\n\tcase KVM_IOEVENTFD: {\n\t\tstruct kvm_ioeventfd data;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&data, argp, sizeof data))\n\t\t\tgoto out;\n\t\tr = kvm_ioeventfd(kvm, &data);\n\t\tbreak;\n\t}\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tcase KVM_SET_BOOT_CPU_ID:\n\t\tr = 0;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (atomic_read(&kvm->online_vcpus) != 0)\n\t\t\tr = -EBUSY;\n\t\telse\n\t\t\tkvm->bsp_vcpu_id = arg;\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n#endif\n#ifdef CONFIG_HAVE_KVM_MSI\n\tcase KVM_SIGNAL_MSI: {\n\t\tstruct kvm_msi msi;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&msi, argp, sizeof msi))\n\t\t\tgoto out;\n\t\tr = kvm_send_userspace_msi(kvm, &msi);\n\t\tbreak;\n\t}\n#endif\n#ifdef __KVM_HAVE_IRQ_LINE\n\tcase KVM_IRQ_LINE_STATUS:\n\tcase KVM_IRQ_LINE: {\n\t\tstruct kvm_irq_level irq_event;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&irq_event, argp, sizeof irq_event))\n\t\t\tgoto out;\n\n\t\tr = kvm_vm_ioctl_irq_line(kvm, &irq_event);\n\t\tif (r)\n\t\t\tgoto out;\n\n\t\tr = -EFAULT;\n\t\tif (ioctl == KVM_IRQ_LINE_STATUS) {\n\t\t\tif (copy_to_user(argp, &irq_event, sizeof irq_event))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tr = kvm_arch_vm_ioctl(filp, ioctl, arg);\n\t\tif (r == -ENOTTY)\n\t\t\tr = kvm_vm_ioctl_assigned_device(kvm, ioctl, arg);\n\t}\nout:\n\treturn r;\n}\n\n#ifdef CONFIG_COMPAT\nstruct compat_kvm_dirty_log {\n\t__u32 slot;\n\t__u32 padding1;\n\tunion {\n\t\tcompat_uptr_t dirty_bitmap; /* one bit per page */\n\t\t__u64 padding2;\n\t};\n};\n\nstatic long kvm_vm_compat_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tint r;\n\n\tif (kvm->mm != current->mm)\n\t\treturn -EIO;\n\tswitch (ioctl) {\n\tcase KVM_GET_DIRTY_LOG: {\n\t\tstruct compat_kvm_dirty_log compat_log;\n\t\tstruct kvm_dirty_log log;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&compat_log, (void __user *)arg,\n\t\t\t\t   sizeof(compat_log)))\n\t\t\tgoto out;\n\t\tlog.slot\t = compat_log.slot;\n\t\tlog.padding1\t = compat_log.padding1;\n\t\tlog.padding2\t = compat_log.padding2;\n\t\tlog.dirty_bitmap = compat_ptr(compat_log.dirty_bitmap);\n\n\t\tr = kvm_vm_ioctl_get_dirty_log(kvm, &log);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_vm_ioctl(filp, ioctl, arg);\n\t}\n\nout:\n\treturn r;\n}\n#endif\n\nstatic int kvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page[1];\n\tunsigned long addr;\n\tint npages;\n\tgfn_t gfn = vmf->pgoff;\n\tstruct kvm *kvm = vma->vm_file->private_data;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tnpages = get_user_pages(current, current->mm, addr, 1, 1, 0, page,\n\t\t\t\tNULL);\n\tif (unlikely(npages != 1))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tvmf->page = page[0];\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct kvm_vm_vm_ops = {\n\t.fault = kvm_vm_fault,\n};\n\nstatic int kvm_vm_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_ops = &kvm_vm_vm_ops;\n\treturn 0;\n}\n\nstatic struct file_operations kvm_vm_fops = {\n\t.release        = kvm_vm_release,\n\t.unlocked_ioctl = kvm_vm_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl   = kvm_vm_compat_ioctl,\n#endif\n\t.mmap           = kvm_vm_mmap,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic int kvm_dev_ioctl_create_vm(unsigned long type)\n{\n\tint r;\n\tstruct kvm *kvm;\n\n\tkvm = kvm_create_vm(type);\n\tif (IS_ERR(kvm))\n\t\treturn PTR_ERR(kvm);\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\tr = kvm_coalesced_mmio_init(kvm);\n\tif (r < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\treturn r;\n\t}\n#endif\n\tr = anon_inode_getfd(\"kvm-vm\", &kvm_vm_fops, kvm, O_RDWR);\n\tif (r < 0)\n\t\tkvm_put_kvm(kvm);\n\n\treturn r;\n}\n\nstatic long kvm_dev_ioctl_check_extension_generic(long arg)\n{\n\tswitch (arg) {\n\tcase KVM_CAP_USER_MEMORY:\n\tcase KVM_CAP_DESTROY_MEMORY_REGION_WORKS:\n\tcase KVM_CAP_JOIN_MEMORY_REGIONS_WORKS:\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tcase KVM_CAP_SET_BOOT_CPU_ID:\n#endif\n\tcase KVM_CAP_INTERNAL_ERROR_DATA:\n#ifdef CONFIG_HAVE_KVM_MSI\n\tcase KVM_CAP_SIGNAL_MSI:\n#endif\n\t\treturn 1;\n#ifdef KVM_CAP_IRQ_ROUTING\n\tcase KVM_CAP_IRQ_ROUTING:\n\t\treturn KVM_MAX_IRQ_ROUTES;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\treturn kvm_dev_ioctl_check_extension(arg);\n}\n\nstatic long kvm_dev_ioctl(struct file *filp,\n\t\t\t  unsigned int ioctl, unsigned long arg)\n{\n\tlong r = -EINVAL;\n\n\tswitch (ioctl) {\n\tcase KVM_GET_API_VERSION:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = KVM_API_VERSION;\n\t\tbreak;\n\tcase KVM_CREATE_VM:\n\t\tr = kvm_dev_ioctl_create_vm(arg);\n\t\tbreak;\n\tcase KVM_CHECK_EXTENSION:\n\t\tr = kvm_dev_ioctl_check_extension_generic(arg);\n\t\tbreak;\n\tcase KVM_GET_VCPU_MMAP_SIZE:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = PAGE_SIZE;     /* struct kvm_run */\n#ifdef CONFIG_X86\n\t\tr += PAGE_SIZE;    /* pio data page */\n#endif\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\t\tr += PAGE_SIZE;    /* coalesced mmio ring page */\n#endif\n\t\tbreak;\n\tcase KVM_TRACE_ENABLE:\n\tcase KVM_TRACE_PAUSE:\n\tcase KVM_TRACE_DISABLE:\n\t\tr = -EOPNOTSUPP;\n\t\tbreak;\n\tdefault:\n\t\treturn kvm_arch_dev_ioctl(filp, ioctl, arg);\n\t}\nout:\n\treturn r;\n}\n\nstatic struct file_operations kvm_chardev_ops = {\n\t.unlocked_ioctl = kvm_dev_ioctl,\n\t.compat_ioctl   = kvm_dev_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice kvm_dev = {\n\tKVM_MINOR,\n\t\"kvm\",\n\t&kvm_chardev_ops,\n};\n\nstatic void hardware_enable_nolock(void *junk)\n{\n\tint cpu = raw_smp_processor_id();\n\tint r;\n\n\tif (cpumask_test_cpu(cpu, cpus_hardware_enabled))\n\t\treturn;\n\n\tcpumask_set_cpu(cpu, cpus_hardware_enabled);\n\n\tr = kvm_arch_hardware_enable(NULL);\n\n\tif (r) {\n\t\tcpumask_clear_cpu(cpu, cpus_hardware_enabled);\n\t\tatomic_inc(&hardware_enable_failed);\n\t\tprintk(KERN_INFO \"kvm: enabling virtualization on \"\n\t\t\t\t \"CPU%d failed\\n\", cpu);\n\t}\n}\n\nstatic void hardware_enable(void *junk)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_enable_nolock(junk);\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic void hardware_disable_nolock(void *junk)\n{\n\tint cpu = raw_smp_processor_id();\n\n\tif (!cpumask_test_cpu(cpu, cpus_hardware_enabled))\n\t\treturn;\n\tcpumask_clear_cpu(cpu, cpus_hardware_enabled);\n\tkvm_arch_hardware_disable(NULL);\n}\n\nstatic void hardware_disable(void *junk)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_disable_nolock(junk);\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic void hardware_disable_all_nolock(void)\n{\n\tBUG_ON(!kvm_usage_count);\n\n\tkvm_usage_count--;\n\tif (!kvm_usage_count)\n\t\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n}\n\nstatic void hardware_disable_all(void)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_disable_all_nolock();\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic int hardware_enable_all(void)\n{\n\tint r = 0;\n\n\traw_spin_lock(&kvm_lock);\n\n\tkvm_usage_count++;\n\tif (kvm_usage_count == 1) {\n\t\tatomic_set(&hardware_enable_failed, 0);\n\t\ton_each_cpu(hardware_enable_nolock, NULL, 1);\n\n\t\tif (atomic_read(&hardware_enable_failed)) {\n\t\t\thardware_disable_all_nolock();\n\t\t\tr = -EBUSY;\n\t\t}\n\t}\n\n\traw_spin_unlock(&kvm_lock);\n\n\treturn r;\n}\n\nstatic int kvm_cpu_hotplug(struct notifier_block *notifier, unsigned long val,\n\t\t\t   void *v)\n{\n\tint cpu = (long)v;\n\n\tif (!kvm_usage_count)\n\t\treturn NOTIFY_OK;\n\n\tval &= ~CPU_TASKS_FROZEN;\n\tswitch (val) {\n\tcase CPU_DYING:\n\t\tprintk(KERN_INFO \"kvm: disabling virtualization on CPU%d\\n\",\n\t\t       cpu);\n\t\thardware_disable(NULL);\n\t\tbreak;\n\tcase CPU_STARTING:\n\t\tprintk(KERN_INFO \"kvm: enabling virtualization on CPU%d\\n\",\n\t\t       cpu);\n\t\thardware_enable(NULL);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\n\nasmlinkage void kvm_spurious_fault(void)\n{\n\t/* Fault while not rebooting.  We want the trace. */\n\tBUG();\n}\nEXPORT_SYMBOL_GPL(kvm_spurious_fault);\n\nstatic int kvm_reboot(struct notifier_block *notifier, unsigned long val,\n\t\t      void *v)\n{\n\t/*\n\t * Some (well, at least mine) BIOSes hang on reboot if\n\t * in vmx root mode.\n\t *\n\t * And Intel TXT required VMX off for all cpu when system shutdown.\n\t */\n\tprintk(KERN_INFO \"kvm: exiting hardware virtualization\\n\");\n\tkvm_rebooting = true;\n\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block kvm_reboot_notifier = {\n\t.notifier_call = kvm_reboot,\n\t.priority = 0,\n};\n\nstatic void kvm_io_bus_destroy(struct kvm_io_bus *bus)\n{\n\tint i;\n\n\tfor (i = 0; i < bus->dev_count; i++) {\n\t\tstruct kvm_io_device *pos = bus->range[i].dev;\n\n\t\tkvm_iodevice_destructor(pos);\n\t}\n\tkfree(bus);\n}\n\nint kvm_io_bus_sort_cmp(const void *p1, const void *p2)\n{\n\tconst struct kvm_io_range *r1 = p1;\n\tconst struct kvm_io_range *r2 = p2;\n\n\tif (r1->addr < r2->addr)\n\t\treturn -1;\n\tif (r1->addr + r1->len > r2->addr + r2->len)\n\t\treturn 1;\n\treturn 0;\n}\n\nint kvm_io_bus_insert_dev(struct kvm_io_bus *bus, struct kvm_io_device *dev,\n\t\t\t  gpa_t addr, int len)\n{\n\tbus->range[bus->dev_count++] = (struct kvm_io_range) {\n\t\t.addr = addr,\n\t\t.len = len,\n\t\t.dev = dev,\n\t};\n\n\tsort(bus->range, bus->dev_count, sizeof(struct kvm_io_range),\n\t\tkvm_io_bus_sort_cmp, NULL);\n\n\treturn 0;\n}\n\nint kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,\n\t\t\t     gpa_t addr, int len)\n{\n\tstruct kvm_io_range *range, key;\n\tint off;\n\n\tkey = (struct kvm_io_range) {\n\t\t.addr = addr,\n\t\t.len = len,\n\t};\n\n\trange = bsearch(&key, bus->range, bus->dev_count,\n\t\t\tsizeof(struct kvm_io_range), kvm_io_bus_sort_cmp);\n\tif (range == NULL)\n\t\treturn -ENOENT;\n\n\toff = range - bus->range;\n\n\twhile (off > 0 && kvm_io_bus_sort_cmp(&key, &bus->range[off-1]) == 0)\n\t\toff--;\n\n\treturn off;\n}\n\n/* kvm_io_bus_write - called under kvm->slots_lock */\nint kvm_io_bus_write(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t     int len, const void *val)\n{\n\tint idx;\n\tstruct kvm_io_bus *bus;\n\tstruct kvm_io_range range;\n\n\trange = (struct kvm_io_range) {\n\t\t.addr = addr,\n\t\t.len = len,\n\t};\n\n\tbus = srcu_dereference(kvm->buses[bus_idx], &kvm->srcu);\n\tidx = kvm_io_bus_get_first_dev(bus, addr, len);\n\tif (idx < 0)\n\t\treturn -EOPNOTSUPP;\n\n\twhile (idx < bus->dev_count &&\n\t\tkvm_io_bus_sort_cmp(&range, &bus->range[idx]) == 0) {\n\t\tif (!kvm_iodevice_write(bus->range[idx].dev, addr, len, val))\n\t\t\treturn 0;\n\t\tidx++;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\n/* kvm_io_bus_read - called under kvm->slots_lock */\nint kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t    int len, void *val)\n{\n\tint idx;\n\tstruct kvm_io_bus *bus;\n\tstruct kvm_io_range range;\n\n\trange = (struct kvm_io_range) {\n\t\t.addr = addr,\n\t\t.len = len,\n\t};\n\n\tbus = srcu_dereference(kvm->buses[bus_idx], &kvm->srcu);\n\tidx = kvm_io_bus_get_first_dev(bus, addr, len);\n\tif (idx < 0)\n\t\treturn -EOPNOTSUPP;\n\n\twhile (idx < bus->dev_count &&\n\t\tkvm_io_bus_sort_cmp(&range, &bus->range[idx]) == 0) {\n\t\tif (!kvm_iodevice_read(bus->range[idx].dev, addr, len, val))\n\t\t\treturn 0;\n\t\tidx++;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\n/* Caller must hold slots_lock. */\nint kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t\t    int len, struct kvm_io_device *dev)\n{\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tbus = kvm->buses[bus_idx];\n\tif (bus->dev_count > NR_IOBUS_DEVS - 1)\n\t\treturn -ENOSPC;\n\n\tnew_bus = kzalloc(sizeof(*bus) + ((bus->dev_count + 1) *\n\t\t\t  sizeof(struct kvm_io_range)), GFP_KERNEL);\n\tif (!new_bus)\n\t\treturn -ENOMEM;\n\tmemcpy(new_bus, bus, sizeof(*bus) + (bus->dev_count *\n\t       sizeof(struct kvm_io_range)));\n\tkvm_io_bus_insert_dev(new_bus, dev, addr, len);\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\n\treturn 0;\n}\n\n/* Caller must hold slots_lock. */\nint kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t      struct kvm_io_device *dev)\n{\n\tint i, r;\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tbus = kvm->buses[bus_idx];\n\tr = -ENOENT;\n\tfor (i = 0; i < bus->dev_count; i++)\n\t\tif (bus->range[i].dev == dev) {\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\n\tif (r)\n\t\treturn r;\n\n\tnew_bus = kzalloc(sizeof(*bus) + ((bus->dev_count - 1) *\n\t\t\t  sizeof(struct kvm_io_range)), GFP_KERNEL);\n\tif (!new_bus)\n\t\treturn -ENOMEM;\n\n\tmemcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));\n\tnew_bus->dev_count--;\n\tmemcpy(new_bus->range + i, bus->range + i + 1,\n\t       (new_bus->dev_count - i) * sizeof(struct kvm_io_range));\n\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\treturn r;\n}\n\nstatic struct notifier_block kvm_cpu_notifier = {\n\t.notifier_call = kvm_cpu_hotplug,\n};\n\nstatic int vm_stat_get(void *_offset, u64 *val)\n{\n\tunsigned offset = (long)_offset;\n\tstruct kvm *kvm;\n\n\t*val = 0;\n\traw_spin_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\t*val += *(u32 *)((void *)kvm + offset);\n\traw_spin_unlock(&kvm_lock);\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(vm_stat_fops, vm_stat_get, NULL, \"%llu\\n\");\n\nstatic int vcpu_stat_get(void *_offset, u64 *val)\n{\n\tunsigned offset = (long)_offset;\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\t*val = 0;\n\traw_spin_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\t\t*val += *(u32 *)((void *)vcpu + offset);\n\n\traw_spin_unlock(&kvm_lock);\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, NULL, \"%llu\\n\");\n\nstatic const struct file_operations *stat_fops[] = {\n\t[KVM_STAT_VCPU] = &vcpu_stat_fops,\n\t[KVM_STAT_VM]   = &vm_stat_fops,\n};\n\nstatic int kvm_init_debug(void)\n{\n\tint r = -EFAULT;\n\tstruct kvm_stats_debugfs_item *p;\n\n\tkvm_debugfs_dir = debugfs_create_dir(\"kvm\", NULL);\n\tif (kvm_debugfs_dir == NULL)\n\t\tgoto out;\n\n\tfor (p = debugfs_entries; p->name; ++p) {\n\t\tp->dentry = debugfs_create_file(p->name, 0444, kvm_debugfs_dir,\n\t\t\t\t\t\t(void *)(long)p->offset,\n\t\t\t\t\t\tstat_fops[p->kind]);\n\t\tif (p->dentry == NULL)\n\t\t\tgoto out_dir;\n\t}\n\n\treturn 0;\n\nout_dir:\n\tdebugfs_remove_recursive(kvm_debugfs_dir);\nout:\n\treturn r;\n}\n\nstatic void kvm_exit_debug(void)\n{\n\tstruct kvm_stats_debugfs_item *p;\n\n\tfor (p = debugfs_entries; p->name; ++p)\n\t\tdebugfs_remove(p->dentry);\n\tdebugfs_remove(kvm_debugfs_dir);\n}\n\nstatic int kvm_suspend(void)\n{\n\tif (kvm_usage_count)\n\t\thardware_disable_nolock(NULL);\n\treturn 0;\n}\n\nstatic void kvm_resume(void)\n{\n\tif (kvm_usage_count) {\n\t\tWARN_ON(raw_spin_is_locked(&kvm_lock));\n\t\thardware_enable_nolock(NULL);\n\t}\n}\n\nstatic struct syscore_ops kvm_syscore_ops = {\n\t.suspend = kvm_suspend,\n\t.resume = kvm_resume,\n};\n\nstatic inline\nstruct kvm_vcpu *preempt_notifier_to_vcpu(struct preempt_notifier *pn)\n{\n\treturn container_of(pn, struct kvm_vcpu, preempt_notifier);\n}\n\nstatic void kvm_sched_in(struct preempt_notifier *pn, int cpu)\n{\n\tstruct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);\n\n\tkvm_arch_vcpu_load(vcpu, cpu);\n}\n\nstatic void kvm_sched_out(struct preempt_notifier *pn,\n\t\t\t  struct task_struct *next)\n{\n\tstruct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);\n\n\tkvm_arch_vcpu_put(vcpu);\n}\n\nint kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,\n\t\t  struct module *module)\n{\n\tint r;\n\tint cpu;\n\n\tr = kvm_arch_init(opaque);\n\tif (r)\n\t\tgoto out_fail;\n\n\tif (!zalloc_cpumask_var(&cpus_hardware_enabled, GFP_KERNEL)) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_0;\n\t}\n\n\tr = kvm_arch_hardware_setup();\n\tif (r < 0)\n\t\tgoto out_free_0a;\n\n\tfor_each_online_cpu(cpu) {\n\t\tsmp_call_function_single(cpu,\n\t\t\t\tkvm_arch_check_processor_compat,\n\t\t\t\t&r, 1);\n\t\tif (r < 0)\n\t\t\tgoto out_free_1;\n\t}\n\n\tr = register_cpu_notifier(&kvm_cpu_notifier);\n\tif (r)\n\t\tgoto out_free_2;\n\tregister_reboot_notifier(&kvm_reboot_notifier);\n\n\t/* A kmem cache lets us meet the alignment requirements of fx_save. */\n\tif (!vcpu_align)\n\t\tvcpu_align = __alignof__(struct kvm_vcpu);\n\tkvm_vcpu_cache = kmem_cache_create(\"kvm_vcpu\", vcpu_size, vcpu_align,\n\t\t\t\t\t   0, NULL);\n\tif (!kvm_vcpu_cache) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_3;\n\t}\n\n\tr = kvm_async_pf_init();\n\tif (r)\n\t\tgoto out_free;\n\n\tkvm_chardev_ops.owner = module;\n\tkvm_vm_fops.owner = module;\n\tkvm_vcpu_fops.owner = module;\n\n\tr = misc_register(&kvm_dev);\n\tif (r) {\n\t\tprintk(KERN_ERR \"kvm: misc device register failed\\n\");\n\t\tgoto out_unreg;\n\t}\n\n\tregister_syscore_ops(&kvm_syscore_ops);\n\n\tkvm_preempt_ops.sched_in = kvm_sched_in;\n\tkvm_preempt_ops.sched_out = kvm_sched_out;\n\n\tr = kvm_init_debug();\n\tif (r) {\n\t\tprintk(KERN_ERR \"kvm: create debugfs files failed\\n\");\n\t\tgoto out_undebugfs;\n\t}\n\n\treturn 0;\n\nout_undebugfs:\n\tunregister_syscore_ops(&kvm_syscore_ops);\nout_unreg:\n\tkvm_async_pf_deinit();\nout_free:\n\tkmem_cache_destroy(kvm_vcpu_cache);\nout_free_3:\n\tunregister_reboot_notifier(&kvm_reboot_notifier);\n\tunregister_cpu_notifier(&kvm_cpu_notifier);\nout_free_2:\nout_free_1:\n\tkvm_arch_hardware_unsetup();\nout_free_0a:\n\tfree_cpumask_var(cpus_hardware_enabled);\nout_free_0:\n\tkvm_arch_exit();\nout_fail:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_init);\n\nvoid kvm_exit(void)\n{\n\tkvm_exit_debug();\n\tmisc_deregister(&kvm_dev);\n\tkmem_cache_destroy(kvm_vcpu_cache);\n\tkvm_async_pf_deinit();\n\tunregister_syscore_ops(&kvm_syscore_ops);\n\tunregister_reboot_notifier(&kvm_reboot_notifier);\n\tunregister_cpu_notifier(&kvm_cpu_notifier);\n\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n\tkvm_arch_hardware_unsetup();\n\tkvm_arch_exit();\n\tfree_cpumask_var(cpus_hardware_enabled);\n}\nEXPORT_SYMBOL_GPL(kvm_exit);\n"], "filenames": ["virt/kvm/kvm_main.c"], "buggy_code_start_loc": [794], "buggy_code_end_loc": [812], "fixing_code_start_loc": [794], "fixing_code_end_loc": [812], "type": "CWE-399", "message": "Memory leak in the __kvm_set_memory_region function in virt/kvm/kvm_main.c in the Linux kernel before 3.9 allows local users to cause a denial of service (memory consumption) by leveraging certain device access to trigger movement of memory slots.", "other": {"cve": {"id": "CVE-2013-4592", "sourceIdentifier": "secalert@redhat.com", "published": "2013-11-20T13:19:42.243", "lastModified": "2023-02-13T04:49:03.570", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Memory leak in the __kvm_set_memory_region function in virt/kvm/kvm_main.c in the Linux kernel before 3.9 allows local users to cause a denial of service (memory consumption) by leveraging certain device access to trigger movement of memory slots."}, {"lang": "es", "value": "Filtraci\u00f3n de memoria en la funci\u00f3n __kvm_set_memory_region de virt/kvm/kvm_main.c en el kernel de Linux anterior a la versi\u00f3n 3.9 permite a usuarios locales provocar una denegaci\u00f3n de servicio (consumo de memoria) mediante el aprovechamiento de cierto acceso al dispositivo para desencadenar el movimiento de slots de memoria."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:H/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "HIGH", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 1.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-399"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:rc7:*:*:*:*:*:*", "versionEndIncluding": "3.9", "matchCriteriaId": "10A36153-A19D-4D19-A186-4B181C6304F7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.35:*:*:*:*:*:*:*", "matchCriteriaId": "F72412E3-8DA9-4CC9-A426-B534202ADBA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.36:*:*:*:*:*:*:*", "matchCriteriaId": "FCAA9D7A-3C3E-4C0B-9D38-EA80E68C2E46"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.37:*:*:*:*:*:*:*", "matchCriteriaId": "4A9E3AE5-3FCF-4CBB-A30B-082BCFBFB0CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.38:*:*:*:*:*:*:*", "matchCriteriaId": "CF715657-4C3A-4392-B85D-1BBF4DE45D89"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.39:*:*:*:*:*:*:*", "matchCriteriaId": "4B63C618-AC3D-4EF7-AFDF-27B9BF482B78"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.40:*:*:*:*:*:*:*", "matchCriteriaId": "C33DA5A9-5E40-4365-9602-82FB4DCD15B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.41:*:*:*:*:*:*:*", "matchCriteriaId": "EFAFDB74-40BD-46FA-89AC-617EB2C7160B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.42:*:*:*:*:*:*:*", "matchCriteriaId": "CF5F17DA-30A7-40CF-BD7C-CEDF06D64617"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.43:*:*:*:*:*:*:*", "matchCriteriaId": "71A276F5-BD9D-4C1B-90DF-9B0C15B6F7DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.44:*:*:*:*:*:*:*", "matchCriteriaId": "F8F6EBEC-3C29-444B-BB85-6EF239B59EC1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.45:*:*:*:*:*:*:*", "matchCriteriaId": "FDB91302-FD18-44CF-A8A8-B31483328539"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.46:*:*:*:*:*:*:*", "matchCriteriaId": "9B81DC2B-46FA-4640-AD6C-2A404D94BA0B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.47:*:*:*:*:*:*:*", "matchCriteriaId": "BA6A1663-BC4C-4FC9-B5EB-A52EDED17B26"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.48:*:*:*:*:*:*:*", "matchCriteriaId": "69C33D6C-6B9F-49F4-B505-E7B589CDEC50"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.49:*:*:*:*:*:*:*", "matchCriteriaId": "C464796B-2F31-4159-A132-82A0C74137B7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.50:*:*:*:*:*:*:*", "matchCriteriaId": "1D6C6E46-FE29-4D2D-A0EC-43DA5112BCC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.51:*:*:*:*:*:*:*", "matchCriteriaId": "1A370E91-73A1-4D62-8E7B-696B920203F8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.52:*:*:*:*:*:*:*", "matchCriteriaId": "340197CD-9645-4B7E-B976-F3F5A7D4C5BE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.53:*:*:*:*:*:*:*", "matchCriteriaId": "96030636-0C4A-4A10-B768-525D6A0E18CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.54:*:*:*:*:*:*:*", "matchCriteriaId": "A42D8419-914F-4AD6-B0E9-C1290D514FF1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.55:*:*:*:*:*:*:*", "matchCriteriaId": "F4E2C88B-42EA-4F4F-B1F6-A9332EC6888B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.56:*:*:*:*:*:*:*", "matchCriteriaId": "2449D13B-3314-4182-832F-03F6B11AA31F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.57:*:*:*:*:*:*:*", "matchCriteriaId": "9A35B66C-F050-4462-A58E-FEE061B5582E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.58:*:*:*:*:*:*:*", "matchCriteriaId": "1B551164-0167-49BB-A3AE-4034BDA3DCB4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.59:*:*:*:*:*:*:*", "matchCriteriaId": "7244278E-49B6-4405-A14C-F3540C8F5AF8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.60:*:*:*:*:*:*:*", "matchCriteriaId": "B4C3E4B8-7274-4ABB-B7CE-6A39C183CE18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.61:*:*:*:*:*:*:*", "matchCriteriaId": "6501EDB9-4847-47F8-90EE-B295626E4CDC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.62:*:*:*:*:*:*:*", "matchCriteriaId": "2D676D48-7521-45E2-8563-6B966FF86A35"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.63:*:*:*:*:*:*:*", "matchCriteriaId": "3B69FA17-0AB9-4986-A5A7-2A4C1DD24222"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.64:*:*:*:*:*:*:*", "matchCriteriaId": "7BC35593-96C7-41F0-B738-1568F8129121"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.65:*:*:*:*:*:*:*", "matchCriteriaId": "38D23794-0E7C-4FA5-A7A8-CF940E3FA962"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.66:*:*:*:*:*:*:*", "matchCriteriaId": "008E1E7D-4C20-4560-9288-EF532ADB0029"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.67:*:*:*:*:*:*:*", "matchCriteriaId": "3B3A7044-A92E-47A9-A7BD-35E5B575F5FD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.68:*:*:*:*:*:*:*", "matchCriteriaId": "783E2980-B6AB-489E-B157-B6A2E10A32CA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.21:*:*:*:*:*:*:*", "matchCriteriaId": "1FF841F3-48A7-41D7-9C45-A8170435A5EB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.22:*:*:*:*:*:*:*", "matchCriteriaId": "EF506916-A6DC-4B1E-90E5-959492AF55F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.23:*:*:*:*:*:*:*", "matchCriteriaId": "B3CDAD1F-2C6A-48C0-8FAB-C2659373FA25"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.24:*:*:*:*:*:*:*", "matchCriteriaId": "4FFE4B22-C96A-43D0-B993-F51EDD9C5E0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.25:*:*:*:*:*:*:*", "matchCriteriaId": "F571CC8B-B212-4553-B463-1DB01D616E8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.26:*:*:*:*:*:*:*", "matchCriteriaId": "84E3E151-D437-48ED-A529-731EEFF88567"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.27:*:*:*:*:*:*:*", "matchCriteriaId": "E9E3EA3C-CCA5-4433-86E0-3D02C4757A0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.28:*:*:*:*:*:*:*", "matchCriteriaId": "F7AC4F7D-9FA6-4CF1-B2E9-70BF7D4D177C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.29:*:*:*:*:*:*:*", "matchCriteriaId": "3CE3A80D-9648-43CC-8F99-D741ED6552BF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.30:*:*:*:*:*:*:*", "matchCriteriaId": "C8A98C03-A465-41B4-A551-A26FEC7FFD94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "916E97D4-1FAB-42F5-826B-653B1C0909A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.5:*:*:*:*:*:*:*", "matchCriteriaId": "33FD2217-C5D0-48C1-AD74-3527127FEF9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.6:*:*:*:*:*:*:*", "matchCriteriaId": "2E92971F-B629-4E0A-9A50-8B235F9704B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.7:*:*:*:*:*:*:*", "matchCriteriaId": "EDD3A069-3829-4EE2-9D5A-29459F29D4C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.8:*:*:*:*:*:*:*", "matchCriteriaId": "A4A0964C-CEB2-41D7-A69C-1599B05B6171"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "A0F36FAC-141D-476D-84C5-A558C199F904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "51D64824-25F6-4761-BD6A-29038A143744"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "E284C8A1-740F-454D-A774-99CD3A21B594"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "C70D72AE-0CBF-4324-9935-57E28EC6279C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F674B06B-7E86-4E41-9126-8152D0DDABAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.6:*:*:*:*:*:*:*", "matchCriteriaId": "7039B3EC-8B22-413E-B582-B4BEC6181241"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.7:*:*:*:*:*:*:*", "matchCriteriaId": "35CF1DD2-80B9-4476-8963-5C3EF52B33F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.8:*:*:*:*:*:*:*", "matchCriteriaId": "BFB0B05B-A5CE-4B9C-AE7F-83062868D35B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.9:*:*:*:*:*:*:*", "matchCriteriaId": "D166A66E-7454-47EC-BB56-861A9AFEAFE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7DA94F50-2A62-4300-BF4D-A342AAE35629"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.11:*:*:*:*:*:*:*", "matchCriteriaId": "252D937B-50DC-444F-AE73-5FCF6203DF27"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.12:*:*:*:*:*:*:*", "matchCriteriaId": "F6D8EE51-02C1-47BC-A92C-0A8ABEFD28FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.13:*:*:*:*:*:*:*", "matchCriteriaId": "7F20A5D7-3B38-4911-861A-04C8310D5916"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.14:*:*:*:*:*:*:*", "matchCriteriaId": "D472DE3A-71D8-4F40-9DDE-85929A2B047D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.15:*:*:*:*:*:*:*", "matchCriteriaId": "B2AED943-65A8-4FDB-BBD0-CCEF8682A48C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.16:*:*:*:*:*:*:*", "matchCriteriaId": "D4640185-F3D8-4575-A71D-4C889A93DE2C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.17:*:*:*:*:*:*:*", "matchCriteriaId": "144CCF7C-025E-4879-B2E7-ABB8E4390BE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.18:*:*:*:*:*:*:*", "matchCriteriaId": "B6FAA052-0B2B-40CE-8C98-919B8D08A5ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.19:*:*:*:*:*:*:*", "matchCriteriaId": "4B5A53DE-9C83-4A6B-96F3-23C03BF445D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.20:*:*:*:*:*:*:*", "matchCriteriaId": "063EB879-CB05-4E33-AA90-9E43516839B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.21:*:*:*:*:*:*:*", "matchCriteriaId": "2D25764F-4B02-4C65-954E-8C7D6632DE00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.22:*:*:*:*:*:*:*", "matchCriteriaId": "F31F5BF3-CD0A-465C-857F-273841BCD28A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.23:*:*:*:*:*:*:*", "matchCriteriaId": "FF302C8A-079B-42B9-B455-CD9083BFA067"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.24:*:*:*:*:*:*:*", "matchCriteriaId": "744999C0-33D3-4363-B3DB-E0D02CDD3918"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.25:*:*:*:*:*:*:*", "matchCriteriaId": "C2E77A76-2A60-45D8-9337-867BC22C5110"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.26:*:*:*:*:*:*:*", "matchCriteriaId": "C9F4AAE7-C870-46B7-B559-2949737BE777"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.27:*:*:*:*:*:*:*", "matchCriteriaId": "20FA2824-20B0-48B8-BB0A-4904C1D3E8AA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.28:*:*:*:*:*:*:*", "matchCriteriaId": "9F9B347E-61AC-419F-9701-B862BBFA46F2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.29:*:*:*:*:*:*:*", "matchCriteriaId": "989F351C-8B7C-4C1B-AFA2-AE9431576368"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.30:*:*:*:*:*:*:*", "matchCriteriaId": "8D22172A-9FA7-42E0-8451-165D8E47A573"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.31:*:*:*:*:*:*:*", "matchCriteriaId": "CE31624C-94F9-45D8-9B4A-D0028F10602F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.32:*:*:*:*:*:*:*", "matchCriteriaId": "70967A83-28F6-4568-9ADA-6EF232E5BBC2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.1:*:*:*:*:*:*:*", "matchCriteriaId": "962B0C45-AB29-4383-AC16-C6E8245D0FF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.2:*:*:*:*:*:*:*", "matchCriteriaId": "A0EE126B-74B2-4F79-BFE1-3DC169F3F9B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.3:*:*:*:*:*:*:*", "matchCriteriaId": "392075E0-A9C7-4B4A-90F9-7F1ADFF5EFA7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.4:*:*:*:*:*:*:*", "matchCriteriaId": "ECC66968-06F0-4874-A95A-A292C36E45C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.5:*:*:*:*:*:*:*", "matchCriteriaId": "5FE986E6-1068-4E1B-8EAB-DF1EAF32B4E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.6:*:*:*:*:*:*:*", "matchCriteriaId": "543E8536-1A8E-4E76-B89F-1B1F9F26FAB8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.7:*:*:*:*:*:*:*", "matchCriteriaId": "EC2B45E3-31E1-4B46-85FA-3A84E75B8F84"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6:*:*:*:*:*:*:*", "matchCriteriaId": "DDB8CC75-D3EE-417C-A83D-CB6D666FE595"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.1:*:*:*:*:*:*:*", "matchCriteriaId": "09A072F1-7BEE-4236-ACBB-55DB8FEF4A03"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.2:*:*:*:*:*:*:*", "matchCriteriaId": "E19D5A58-17D6-4502-A57A-70B2F84817A4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.3:*:*:*:*:*:*:*", "matchCriteriaId": "D58BA035-1204-4DFA-98A1-12111FB6222E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.4:*:*:*:*:*:*:*", "matchCriteriaId": "A17F2E87-8EB8-476A-B5B5-9AE5CF53D9FE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.5:*:*:*:*:*:*:*", "matchCriteriaId": "A8CCC101-5852-4299-9B67-EA1B149D58C0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.6:*:*:*:*:*:*:*", "matchCriteriaId": "B8074D32-C252-4AD3-A579-1C5EDDD7014B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.7:*:*:*:*:*:*:*", "matchCriteriaId": "962AA802-8179-4606-AAC0-9363BAEABC9F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.8:*:*:*:*:*:*:*", "matchCriteriaId": "1286C858-D5A2-45F3-86D1-E50FE53FB23C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.9:*:*:*:*:*:*:*", "matchCriteriaId": "5AC4A13E-F560-4D01-98A3-E2A2B82EB25B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.10:*:*:*:*:*:*:*", "matchCriteriaId": "942C462A-5398-4BB9-A792-598682E1FEF2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.11:*:*:*:*:*:*:*", "matchCriteriaId": "B852F7E0-0282-483D-BB4D-18CB7A4F1392"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7:*:*:*:*:*:*:*", "matchCriteriaId": "53ED9A31-99CC-41C8-8B72-5B2A9B49AA6C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.1:*:*:*:*:*:*:*", "matchCriteriaId": "EFD646BC-62F7-47CF-B0BE-768F701F7D9A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.2:*:*:*:*:*:*:*", "matchCriteriaId": "F43D418E-87C1-4C83-9FF1-4F45B4F452DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.3:*:*:*:*:*:*:*", "matchCriteriaId": "680D0E00-F29A-487C-8770-8E7EAC672B7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.4:*:*:*:*:*:*:*", "matchCriteriaId": "2DCA96A4-A836-4E94-A39C-3AD3EA1D9611"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.5:*:*:*:*:*:*:*", "matchCriteriaId": "753C05E3-B603-4E36-B9BA-FAEDCBF62A7D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.6:*:*:*:*:*:*:*", "matchCriteriaId": "E385C2E0-B9F1-4564-8E6D-56FD9E762405"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.7:*:*:*:*:*:*:*", "matchCriteriaId": "041335D4-05E1-4004-9381-28AAD5994B47"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.8:*:*:*:*:*:*:*", "matchCriteriaId": "370F2AE5-3DBC-46B9-AC70-F052C9229C00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.9:*:*:*:*:*:*:*", "matchCriteriaId": "7A971BE3-259D-4494-BBC5-12793D92DB57"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.10:*:*:*:*:*:*:*", "matchCriteriaId": "8E4719A6-FDEA-4714-A830-E23A52AE90BC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.0:*:*:*:*:*:*:*", "matchCriteriaId": "1A6E41FB-38CE-49F2-B796-9A5AA648E73F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.1:*:*:*:*:*:*:*", "matchCriteriaId": "93523FE1-5993-46CB-9299-7C8C1A04E873"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.2:*:*:*:*:*:*:*", "matchCriteriaId": "27ADC356-6BE9-43A3-9E0B-393DC4B1559A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.3:*:*:*:*:*:*:*", "matchCriteriaId": "4F543D23-1774-4D14-A7D1-AD49EDEA94DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.4:*:*:*:*:*:*:*", "matchCriteriaId": "FC323F58-CA00-4C3C-BA4D-CC2C0A6E5F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.5:*:*:*:*:*:*:*", "matchCriteriaId": "FEA0B2E3-668D-40ED-9D3D-709EB6449F8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.6:*:*:*:*:*:*:*", "matchCriteriaId": "3431B258-4EC8-4E7F-87BB-4D934880601E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.7:*:*:*:*:*:*:*", "matchCriteriaId": "1B09FA1E-8B28-4F2A-BA7E-8E1C40365970"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.8:*:*:*:*:*:*:*", "matchCriteriaId": "91917120-9D68-41C0-8B5D-85C256BC6200"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.9:*:*:*:*:*:*:*", "matchCriteriaId": "AAD268A0-096C-4C31-BEC5-D47F5149D462"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.10:*:*:*:*:*:*:*", "matchCriteriaId": "32BD2427-C47F-4660-A1D9-448E500EF5B9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.11:*:*:*:*:*:*:*", "matchCriteriaId": "02048CE5-81C7-4DFB-BC40-CE4C86B7E022"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.12:*:*:*:*:*:*:*", "matchCriteriaId": "934D2B37-0575-4A75-B00B-0028316D6DF0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.13:*:*:*:*:*:*:*", "matchCriteriaId": "06754C21-995C-4850-A4DC-F21826C0F8C5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc1:*:*:*:*:*:*", "matchCriteriaId": "42633FF9-FB0C-4095-B4A1-8D623A98683B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc2:*:*:*:*:*:*", "matchCriteriaId": "08C04619-89A2-4B15-82A2-48BCC662C1F1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc3:*:*:*:*:*:*", "matchCriteriaId": "5B039196-7159-476C-876A-C61242CC41DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc4:*:*:*:*:*:*", "matchCriteriaId": "3A9E0457-53C9-44DD-ACFB-31EE1D1E060E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc5:*:*:*:*:*:*", "matchCriteriaId": "BEE406E7-87BA-44BA-BF61-673E6CC44A2F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9:rc6:*:*:*:*:*:*", "matchCriteriaId": "29FBA173-658F-45DC-8205-934CACD67166"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=12d6e7538e2d418c08f082b1b44ffa5fb7270ed8", "source": "secalert@redhat.com"}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=e40f193f5bb022e927a57a4f5d5194e4f12ddb74", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-updates/2014-02/msg00045.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-1645.html", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/11/18/3", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-2066-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-2067-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-2111-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-2112-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-2114-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-2115-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-2116-1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1031702", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/12d6e7538e2d418c08f082b1b44ffa5fb7270ed8", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/e40f193f5bb022e927a57a4f5d5194e4f12ddb74", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/patch-3.9.bz2", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/12d6e7538e2d418c08f082b1b44ffa5fb7270ed8"}}