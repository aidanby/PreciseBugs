{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <string>\n#include <unordered_map>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/gtl/map_util.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/util/tensor_bundle/tensor_bundle.h\"\n\nnamespace tensorflow {\n\nnamespace {\n// Returning a Status instead of using OP_REQUIRES directly since that doesn't\n// seem to work outside the main OpKernel functions.\nStatus RemapVectorToMap(\n    const TTypes<const int64_t>::Vec& remapping, std::vector<bool>* id_present,\n    std::unordered_map<int64_t, int64_t>* old_id_to_new_id) {\n  id_present->clear();\n  id_present->resize(remapping.size(), false);\n  for (int i = 0; i < remapping.size(); ++i) {\n    const int64_t old_id = remapping(i);\n    if (old_id < 0) continue;\n    (*id_present)[i] = true;\n    if (!gtl::InsertIfNotPresent(old_id_to_new_id, old_id, i)) {\n      return errors::Unimplemented(\n          strings::StrCat(\"Old ID \", old_id, \" is mapped to both new ID \",\n                          old_id_to_new_id->at(old_id), \" and \", i,\n                          \", which is not supported.\"));\n    }\n  }\n  return Status::OK();\n}\n}  // anonymous namespace\n\n// This op loads a rank-2 Tensor (matrix) from a TensorFlow checkpoint (V2) and\n// swaps around the rows/columns according to row_remapping/col_remapping.\n// \"Missing\" cells are initialized with values from initializing_values.\nclass LoadAndRemapMatrixOp : public OpKernel {\n public:\n  explicit LoadAndRemapMatrixOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_rows\", &num_rows_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_cols\", &num_cols_));\n    OP_REQUIRES_OK(\n        context, context->GetAttr(\"max_rows_in_memory\", &max_rows_in_memory_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Checks what we're remapping and inverts the relevant remapping Tensors to\n    // be maps with key = old ID, value = new ID.\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n\n    // Calculates the min/max old row ID that we need to read, to save us from\n    // reading some unnecessary slices of the old tensor.\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n\n    // Processes the remapping for columns.\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    // Note that we always \"remap rows\", even when the row vocabulary does\n    // not change, because partitioning requires a mapping from partitioned\n    // Variables to the full checkpoints we load.\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n\n    // Processes the checkpoint source and the provided Tensor name.\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    // This op is limited to loading Tensors of rank 2 (matrices).\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n\n    if (!remap_cols) {\n      // TODO(weiho): Consider relaxing this restriction to allow partial column\n      // loading (even when no column remapping is specified) if there turns out\n      // to be a use case for it.\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n\n    // Uses TensorSlice to potentially load the old tensor in chunks in case\n    // memory usage is a concern.\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      // TODO(weiho): Given the list of old row IDs of interest (the keys of\n      // old_row_to_new_row_map), we could also try something smarter to\n      // find some minimal set of covering ranges for the list of old row IDs\n      // such that the size of each range is less than max_rows_in_memory_.\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                // If max_rows_in_memory_ <= 0, we just load the entire chunk.\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n\n    // Allocates the output matrix.\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n\n    // Iterates through tensor slices and copies over values from the old tensor\n    // to the output matrix.\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      // Potentially re-allocates the tensor buffer since the last slice may\n      // have fewer rows than the other slices.\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n\n      // Iterates through the old loaded tensor slice row-by-row.\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n\n        // If the old row ID is not found in old_row_to_new_row_map, continue\n        // to the next row; otherwise, copy it to the output matrix.\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n\n        // Copies over the row element-by-element, in case remapping is needed\n        // along the column axis.\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              // Column remapping is specified, but this column is not found in\n              // old_col_to_new_col_map, so we leave it uninitialized, to be\n              // filled in with initializing_values later.\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n\n    // At this point, there are potentially whole rows/columns uninitialized\n    // (corresponding to the indices where row_id_present/col_id_present are\n    // false). We fill this in cell-by-cell using row_id_present and\n    // col_id_present while dequeuing from the initializing_values vector.\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n\n    // Checks that we used all the given initializing values.\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }\n\n private:\n  int64_t num_rows_;\n  int64_t num_cols_;\n  int64_t max_rows_in_memory_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"LoadAndRemapMatrix\").Device(DEVICE_CPU),\n                        LoadAndRemapMatrixOp);\n\n}  // namespace tensorflow\n", "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for the ops to generate and execute vocab remapping.\"\"\"\nimport os\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gen_checkpoint_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training import saver\n\nFLAGS = flags.FLAGS\n\n\nclass GenerateVocabRemappingTest(test.TestCase):\n  \"\"\"Tests for the generate_vocab_remapping() method.\"\"\"\n\n  def setUp(self):\n    self.new_vocab_file = os.path.join(self.get_temp_dir(),\n                                       'keyword_shifted.txt')\n    with open(self.new_vocab_file, 'w') as f:\n      f.write('\\n'.join(['MISSING', 'knitting', 'eminem']) + '\\n')\n    self.old_vocab_file = os.path.join(self.get_temp_dir(),\n                                       'keyword.txt')\n    with open(self.old_vocab_file, 'w') as f:\n      f.write('\\n'.join(['knitting', 'eminem', 'MISSING']) + '\\n')\n\n  @test_util.run_deprecated_v1\n  def test_generate_remapping_with_no_vocab_changes(self):\n    \"\"\"Tests where vocab does not change at all.\"\"\"\n    remapping, num_present = gen_checkpoint_ops.generate_vocab_remapping(\n        new_vocab_file=self.old_vocab_file,\n        old_vocab_file=self.old_vocab_file,\n        num_new_vocab=3,\n        new_vocab_offset=0)\n    expected_remapping = range(0, 3)\n    expected_num_present = 3\n    with self.cached_session():\n      self.assertAllEqual(expected_remapping, self.evaluate(remapping))\n      self.assertAllEqual(expected_num_present, self.evaluate(num_present))\n\n  def test_generate_remapping_with_shifted_vocab(self):\n    \"\"\"Tests where vocab is the same, but shifted / ordered differently.\"\"\"\n    remapping, num_present = gen_checkpoint_ops.generate_vocab_remapping(\n        new_vocab_file=self.new_vocab_file,\n        old_vocab_file=self.old_vocab_file,\n        num_new_vocab=3,\n        new_vocab_offset=0)\n    expected_remapping = [2, 0, 1]\n    expected_num_present = 3\n    with self.cached_session():\n      self.assertAllEqual(expected_remapping, self.evaluate(remapping))\n      self.assertAllEqual(expected_num_present, self.evaluate(num_present))\n\n  def test_generate_remapping_with_offset(self):\n    \"\"\"Tests offset and num_new_vocab logic.\"\"\"\n    remapping, num_present = gen_checkpoint_ops.generate_vocab_remapping(\n        new_vocab_file=self.new_vocab_file,\n        old_vocab_file=self.old_vocab_file,\n        num_new_vocab=1,\n        new_vocab_offset=1)\n    expected_remapping = [0]\n    expected_num_present = 1\n    with self.cached_session():\n      self.assertAllEqual(expected_remapping, self.evaluate(remapping))\n      self.assertAllEqual(expected_num_present, self.evaluate(num_present))\n\n  def test_generate_remapping_with_old_vocab_size(self):\n    \"\"\"Tests where old_vocab_size is specified.\"\"\"\n    remapping, num_present = gen_checkpoint_ops.generate_vocab_remapping(\n        new_vocab_file=self.new_vocab_file,\n        old_vocab_file=self.old_vocab_file,\n        num_new_vocab=3,\n        new_vocab_offset=0,\n        # Old vocabulary becomes ['knitting', 'eminem'].\n        old_vocab_size=2)\n    expected_remapping = [-1, 0, 1]\n    expected_num_present = 2\n    with self.cached_session():\n      self.assertAllEqual(expected_remapping, self.evaluate(remapping))\n      self.assertAllEqual(expected_num_present, self.evaluate(num_present))\n\n\nclass LoadAndRemapMatrixTest(test.TestCase):\n  \"\"\"Tests for the load_and_remap_matrix() op.\"\"\"\n\n  def setUp(self):\n    ops.reset_default_graph()\n    self.old_num_rows = 5\n    self.old_num_cols = 16\n    self.matrix_value = np.reshape(\n        range(0, self.old_num_rows * self.old_num_cols), (self.old_num_rows,\n                                                          self.old_num_cols))\n    with variable_scope.variable_scope('some_scope'):\n      matrix = variable_scope.get_variable(\n          'matrix',\n          dtype=dtypes.float32,\n          initializer=constant_op.constant(\n              self.matrix_value, dtype=dtypes.float32))\n      self.old_tensor_name = 'some_scope/matrix'\n\n    save = saver.Saver([matrix])\n    with self.cached_session() as sess:\n      self.evaluate(variables.global_variables_initializer())\n      self.bundle_file = os.path.join(test.get_temp_dir(), 'bundle_checkpoint')\n      save.save(sess, self.bundle_file)\n\n  def test_load_and_remap_no_missing(self):\n    \"\"\"Tests the op's load and remap where there are no missing entries.\"\"\"\n\n    # No column remapping, new weight matrix has second row, then first row.\n    row_remapping = [1, 0]\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=row_remapping,\n        col_remapping=[],\n        initializing_values=[],\n        num_rows=2,\n        num_cols=self.old_num_cols)\n    with self.cached_session():\n      self.assertAllClose(self.matrix_value[row_remapping],\n                          self.evaluate(remapped_matrix))\n\n    # No row remapping, new weight matrix has third col, then first col.\n    row_remapping = list(range(self.old_num_rows))\n    col_remapping = [2, 0]\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=row_remapping,\n        col_remapping=col_remapping,\n        initializing_values=[],\n        num_rows=len(row_remapping),\n        num_cols=len(col_remapping))\n    with self.cached_session():\n      self.assertAllClose(self.matrix_value[row_remapping][:, col_remapping],\n                          self.evaluate(remapped_matrix))\n\n    # Both row and column remappings.\n    row_remapping = [1, 0, 4]\n    col_remapping = [1, 15]\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=row_remapping,\n        col_remapping=col_remapping,\n        initializing_values=[],\n        num_rows=len(row_remapping),\n        num_cols=len(col_remapping))\n    with self.cached_session():\n      self.assertAllClose(self.matrix_value[row_remapping][:, col_remapping],\n                          self.evaluate(remapped_matrix))\n\n  def test_load_and_remap_with_init(self):\n    \"\"\"Tests the op's load and remap where there are missing entries.\"\"\"\n    init_val = 42\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[2, -1, 0],\n        col_remapping=[1, -1],\n        initializing_values=[init_val] * 4,\n        num_rows=3,\n        num_cols=2)\n\n    expected_remapped_matrix = np.reshape(\n        [33, init_val, init_val, init_val, 1, init_val], [3, 2])\n\n    with self.cached_session():\n      self.assertAllClose(expected_remapped_matrix,\n                          self.evaluate(remapped_matrix))\n\n  def test_load_and_remap_all_missing_rows(self):\n    \"\"\"Tests when all the rows are missing and need to be initialized.\"\"\"\n    num_rows = 7\n    initializing_values = [42] * num_rows * self.old_num_cols\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[-1] * num_rows,\n        col_remapping=[],\n        initializing_values=initializing_values,\n        num_rows=num_rows,\n        num_cols=self.old_num_cols)\n    with self.cached_session():\n      self.assertAllClose(\n          np.reshape(initializing_values, (num_rows, self.old_num_cols)),\n          self.evaluate(remapped_matrix))\n\n  def test_load_and_remap_all_missing_rows_and_cols(self):\n    \"\"\"Tests when all the rows & cols are missing and need to be initialized.\"\"\"\n    num_rows = 7\n    num_cols = 4\n    initializing_values = [42] * num_rows * num_cols\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[-1] * num_rows,\n        col_remapping=[-1] * num_cols,\n        initializing_values=initializing_values,\n        num_rows=num_rows,\n        num_cols=num_cols)\n    with self.cached_session():\n      self.assertAllClose(\n          np.reshape(initializing_values, (num_rows, num_cols)),\n          self.evaluate(remapped_matrix))\n\n  @test_util.run_deprecated_v1\n  def test_load_and_remap_invalid_remapping(self):\n    \"\"\"Tests that errors are raised when an ID maps to multiple new IDs.\n\n    (This should usually not happen when using public APIs).\n    \"\"\"\n    invalid_remapping = [1, 0, 0, 0, 1, 2]\n\n    # Invalid row remapping.\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=invalid_remapping,\n        col_remapping=[],\n        initializing_values=[],\n        num_rows=len(invalid_remapping),\n        num_cols=self.old_num_cols)\n    with self.cached_session(), self.assertRaises(errors.UnimplementedError):\n      self.evaluate(remapped_matrix)\n\n    # Invalid column remapping.\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=list(range(self.old_num_rows)),\n        col_remapping=invalid_remapping,\n        initializing_values=[],\n        num_rows=self.old_num_rows,\n        num_cols=len(invalid_remapping))\n    with self.cached_session(), self.assertRaises(errors.UnimplementedError):\n      self.evaluate(remapped_matrix)\n\n  @test_util.run_deprecated_v1\n  def test_load_and_remap_incorrect_initializing_values(self):\n    \"\"\"Tests that errors are raised with incorrect number of init values.\"\"\"\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[2, -1, 0],\n        col_remapping=[1, -1],\n        # Too few initializing values - there should be 4. For some reason,\n        # initializing_values must contain no element (instead of 3 or fewer) to\n        # ensure that a seg fault would reliably occur if the check raising the\n        # InvalidArgumentError were not present.\n        initializing_values=[],\n        num_rows=3,\n        num_cols=2)\n    with self.cached_session(), self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(remapped_matrix)\n\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[2, -1, 0],\n        col_remapping=[1, -1],\n        # Too many initializing values - there should be 4.\n        initializing_values=[0] * 5,\n        num_rows=3,\n        num_cols=2)\n    with self.cached_session(), self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(remapped_matrix)\n\n\nclass LoadAndRemapMatrixWithMaxRowsTest(test.TestCase):\n  \"\"\"Tests for the load_and_remap_matrix() op.\n\n  (Specifically focused on the max_rows_in_memory arg and its effects on\n  TensorBundle's BundleReader and TensorSlice logic).\n  \"\"\"\n\n  def _test_loading_variable_with_max_rows(self, np_value, partitioner,\n                                           max_rows_in_memory):\n    \"\"\"Helper function for various tests using max_rows_in_memory.\"\"\"\n    ops.reset_default_graph()\n    old_tensor_name = 'matrix_to_load_and_remap'\n    matrix = variable_scope.get_variable(\n        old_tensor_name,\n        dtype=dtypes.float32,\n        initializer=constant_op.constant(np_value, dtype=dtypes.float32),\n        partitioner=partitioner)\n\n    with self.cached_session() as sess:\n      ckpt_path = os.path.join(test.get_temp_dir(), 'temp_ckpt')\n      save = saver.Saver([matrix])\n      self.evaluate(variables.global_variables_initializer())\n      save.save(sess, ckpt_path)\n      num_rows, num_cols = np_value.shape\n\n      # Tests loading the entire tensor (except reversed).\n      remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n          ckpt_path=ckpt_path,\n          old_tensor_name=old_tensor_name,\n          # Simply reverses the rows of the matrix.\n          row_remapping=list(range(num_rows - 1, -1, -1)),\n          col_remapping=[],\n          initializing_values=[],\n          num_rows=num_rows,\n          num_cols=num_cols,\n          max_rows_in_memory=max_rows_in_memory)\n      self.assertAllClose(np_value[::-1], self.evaluate(remapped_matrix))\n\n      # Tests loading the tensor (except for the first and last rows), with\n      # uninitialized values. Requires num_rows to be at least 3 since we're\n      # skipping the first and last rows.\n      self.assertGreater(num_rows, 2)\n      prefix_rows = 2\n      suffix_rows = 3\n      remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n          ckpt_path=ckpt_path,\n          old_tensor_name=old_tensor_name,\n          # Reverses the rows of the matrix, then prepends and appends\n          # uninitialized rows.\n          row_remapping=([-1] * prefix_rows + list(range(1, num_rows - 1)) +\n                         [-1] * suffix_rows),\n          col_remapping=[],\n          initializing_values=[42] * (prefix_rows + suffix_rows) * num_cols,\n          num_rows=num_rows - 2 + prefix_rows + suffix_rows,\n          num_cols=num_cols,\n          max_rows_in_memory=max_rows_in_memory)\n      self.assertAllClose(\n          np.vstack([\n              np.tile(42, [prefix_rows, num_cols]), np_value[1:-1],\n              np.tile(42, [suffix_rows, num_cols])\n          ]), self.evaluate(remapped_matrix))\n\n      # Tests when everything is taken from initializing_values.\n      new_rows = 7\n      initializing_values = [42] * new_rows * num_cols\n      remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n          ckpt_path=ckpt_path,\n          old_tensor_name=old_tensor_name,\n          # Nothing is loaded from the old tensor.\n          row_remapping=[-1] * new_rows,\n          col_remapping=[],\n          initializing_values=initializing_values,\n          num_rows=new_rows,\n          num_cols=num_cols,\n          max_rows_in_memory=max_rows_in_memory)\n      self.assertAllClose(\n          np.reshape(initializing_values, (new_rows, num_cols)),\n          self.evaluate(remapped_matrix))\n\n  @test_util.run_deprecated_v1\n  def test_loading_rows_divisible_by_max_rows(self):\n    \"\"\"Tests loading normal var when rows are evenly divisible by max_rows.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=None,\n        # 9 is evenly divisible by 3.\n        max_rows_in_memory=3)\n\n  @test_util.run_deprecated_v1\n  def test_loading_rows_not_divisible_by_max_rows(self):\n    \"\"\"Tests loading normal var when rows aren't divisible by max_rows.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=None,\n        # 9 is not evenly divisible by 4.\n        max_rows_in_memory=4)\n\n  @test_util.run_deprecated_v1\n  def test_loading_rows_less_than_max_rows(self):\n    \"\"\"Tests loading normal var as a single slice.\n\n    (When the specified max_rows_in_memory is larger than the number of rows)\n    \"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=None,\n        # 10 > 9.\n        max_rows_in_memory=10)\n\n  @test_util.run_deprecated_v1\n  def test_loading_no_max_rows(self):\n    \"\"\"Tests loading normal var as a single slice with no valid max_rows.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 18)), (6, 3)),\n        partitioner=None,\n        max_rows_in_memory=-1)\n\n  @test_util.run_deprecated_v1\n  def test_loading_partitions_equals_max_rows(self):\n    \"\"\"Tests loading partitioned var sliced on partition boundary.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=partitioned_variables.fixed_size_partitioner(3),\n        # With a tensor of shape [9, 3] and 3 partitions, each partition has\n        # exactly 3 rows.\n        max_rows_in_memory=3)\n\n  @test_util.run_deprecated_v1\n  def test_loading_partitions_greater_than_max_rows(self):\n    \"\"\"Tests loading partitioned var with more slices than partitions.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=partitioned_variables.fixed_size_partitioner(3),\n        # Even though each partition has 3 rows, we'll only load the tensor one\n        # row at a time.\n        max_rows_in_memory=1)\n\n  @test_util.run_deprecated_v1\n  def test_loading_partitions_less_than_max_rows(self):\n    \"\"\"Tests loading partitioned var as a single slice.\n\n    (When the specified max_rows_in_memory is larger than the number of rows)\n    \"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=partitioned_variables.fixed_size_partitioner(3),\n        max_rows_in_memory=10)\n\n  @test_util.run_deprecated_v1\n  def test_loading_partitions_no_max_rows(self):\n    \"\"\"Tests loading partitioned var as single slice with no valid max_rows.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=partitioned_variables.fixed_size_partitioner(3),\n        max_rows_in_memory=-1)\n\n\nif __name__ == '__main__':\n  test.main()\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <string>\n#include <unordered_map>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/gtl/map_util.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/util/tensor_bundle/tensor_bundle.h\"\n\nnamespace tensorflow {\n\nnamespace {\n// Returning a Status instead of using OP_REQUIRES directly since that doesn't\n// seem to work outside the main OpKernel functions.\nStatus RemapVectorToMap(\n    const TTypes<const int64_t>::Vec& remapping, std::vector<bool>* id_present,\n    std::unordered_map<int64_t, int64_t>* old_id_to_new_id) {\n  id_present->clear();\n  id_present->resize(remapping.size(), false);\n  for (int i = 0; i < remapping.size(); ++i) {\n    const int64_t old_id = remapping(i);\n    if (old_id < 0) continue;\n    (*id_present)[i] = true;\n    if (!gtl::InsertIfNotPresent(old_id_to_new_id, old_id, i)) {\n      return errors::Unimplemented(\n          strings::StrCat(\"Old ID \", old_id, \" is mapped to both new ID \",\n                          old_id_to_new_id->at(old_id), \" and \", i,\n                          \", which is not supported.\"));\n    }\n  }\n  return Status::OK();\n}\n}  // anonymous namespace\n\n// This op loads a rank-2 Tensor (matrix) from a TensorFlow checkpoint (V2) and\n// swaps around the rows/columns according to row_remapping/col_remapping.\n// \"Missing\" cells are initialized with values from initializing_values.\nclass LoadAndRemapMatrixOp : public OpKernel {\n public:\n  explicit LoadAndRemapMatrixOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_rows\", &num_rows_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_cols\", &num_cols_));\n    OP_REQUIRES_OK(\n        context, context->GetAttr(\"max_rows_in_memory\", &max_rows_in_memory_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Checks what we're remapping and inverts the relevant remapping Tensors to\n    // be maps with key = old ID, value = new ID.\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    OP_REQUIRES(\n        context, row_remapping_t->dims() == 1,\n        errors::InvalidArgument(\"The `row_remapping` tensor must be 1-D, got \"\n                                \"a tensor of shape \",\n                                row_remapping_t->shape().DebugString()));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n\n    // Calculates the min/max old row ID that we need to read, to save us from\n    // reading some unnecessary slices of the old tensor.\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n\n    // Processes the remapping for columns.\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    // Note that we always \"remap rows\", even when the row vocabulary does\n    // not change, because partitioning requires a mapping from partitioned\n    // Variables to the full checkpoints we load.\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n\n    // Processes the checkpoint source and the provided Tensor name.\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    // This op is limited to loading Tensors of rank 2 (matrices).\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n\n    if (!remap_cols) {\n      // TODO(weiho): Consider relaxing this restriction to allow partial column\n      // loading (even when no column remapping is specified) if there turns out\n      // to be a use case for it.\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n\n    // Uses TensorSlice to potentially load the old tensor in chunks in case\n    // memory usage is a concern.\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      // TODO(weiho): Given the list of old row IDs of interest (the keys of\n      // old_row_to_new_row_map), we could also try something smarter to\n      // find some minimal set of covering ranges for the list of old row IDs\n      // such that the size of each range is less than max_rows_in_memory_.\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                // If max_rows_in_memory_ <= 0, we just load the entire chunk.\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n\n    // Allocates the output matrix.\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n\n    // Iterates through tensor slices and copies over values from the old tensor\n    // to the output matrix.\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      // Potentially re-allocates the tensor buffer since the last slice may\n      // have fewer rows than the other slices.\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n\n      // Iterates through the old loaded tensor slice row-by-row.\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n\n        // If the old row ID is not found in old_row_to_new_row_map, continue\n        // to the next row; otherwise, copy it to the output matrix.\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n\n        // Copies over the row element-by-element, in case remapping is needed\n        // along the column axis.\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              // Column remapping is specified, but this column is not found in\n              // old_col_to_new_col_map, so we leave it uninitialized, to be\n              // filled in with initializing_values later.\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n\n    // At this point, there are potentially whole rows/columns uninitialized\n    // (corresponding to the indices where row_id_present/col_id_present are\n    // false). We fill this in cell-by-cell using row_id_present and\n    // col_id_present while dequeuing from the initializing_values vector.\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n\n    // Checks that we used all the given initializing values.\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }\n\n private:\n  int64_t num_rows_;\n  int64_t num_cols_;\n  int64_t max_rows_in_memory_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"LoadAndRemapMatrix\").Device(DEVICE_CPU),\n                        LoadAndRemapMatrixOp);\n\n}  // namespace tensorflow\n", "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for the ops to generate and execute vocab remapping.\"\"\"\nimport os\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gen_checkpoint_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training import saver\n\nFLAGS = flags.FLAGS\n\n\nclass GenerateVocabRemappingTest(test.TestCase):\n  \"\"\"Tests for the generate_vocab_remapping() method.\"\"\"\n\n  def setUp(self):\n    self.new_vocab_file = os.path.join(self.get_temp_dir(),\n                                       'keyword_shifted.txt')\n    with open(self.new_vocab_file, 'w') as f:\n      f.write('\\n'.join(['MISSING', 'knitting', 'eminem']) + '\\n')\n    self.old_vocab_file = os.path.join(self.get_temp_dir(),\n                                       'keyword.txt')\n    with open(self.old_vocab_file, 'w') as f:\n      f.write('\\n'.join(['knitting', 'eminem', 'MISSING']) + '\\n')\n\n  @test_util.run_deprecated_v1\n  def test_generate_remapping_with_no_vocab_changes(self):\n    \"\"\"Tests where vocab does not change at all.\"\"\"\n    remapping, num_present = gen_checkpoint_ops.generate_vocab_remapping(\n        new_vocab_file=self.old_vocab_file,\n        old_vocab_file=self.old_vocab_file,\n        num_new_vocab=3,\n        new_vocab_offset=0)\n    expected_remapping = range(0, 3)\n    expected_num_present = 3\n    with self.cached_session():\n      self.assertAllEqual(expected_remapping, self.evaluate(remapping))\n      self.assertAllEqual(expected_num_present, self.evaluate(num_present))\n\n  def test_generate_remapping_with_shifted_vocab(self):\n    \"\"\"Tests where vocab is the same, but shifted / ordered differently.\"\"\"\n    remapping, num_present = gen_checkpoint_ops.generate_vocab_remapping(\n        new_vocab_file=self.new_vocab_file,\n        old_vocab_file=self.old_vocab_file,\n        num_new_vocab=3,\n        new_vocab_offset=0)\n    expected_remapping = [2, 0, 1]\n    expected_num_present = 3\n    with self.cached_session():\n      self.assertAllEqual(expected_remapping, self.evaluate(remapping))\n      self.assertAllEqual(expected_num_present, self.evaluate(num_present))\n\n  def test_generate_remapping_with_offset(self):\n    \"\"\"Tests offset and num_new_vocab logic.\"\"\"\n    remapping, num_present = gen_checkpoint_ops.generate_vocab_remapping(\n        new_vocab_file=self.new_vocab_file,\n        old_vocab_file=self.old_vocab_file,\n        num_new_vocab=1,\n        new_vocab_offset=1)\n    expected_remapping = [0]\n    expected_num_present = 1\n    with self.cached_session():\n      self.assertAllEqual(expected_remapping, self.evaluate(remapping))\n      self.assertAllEqual(expected_num_present, self.evaluate(num_present))\n\n  def test_generate_remapping_with_old_vocab_size(self):\n    \"\"\"Tests where old_vocab_size is specified.\"\"\"\n    remapping, num_present = gen_checkpoint_ops.generate_vocab_remapping(\n        new_vocab_file=self.new_vocab_file,\n        old_vocab_file=self.old_vocab_file,\n        num_new_vocab=3,\n        new_vocab_offset=0,\n        # Old vocabulary becomes ['knitting', 'eminem'].\n        old_vocab_size=2)\n    expected_remapping = [-1, 0, 1]\n    expected_num_present = 2\n    with self.cached_session():\n      self.assertAllEqual(expected_remapping, self.evaluate(remapping))\n      self.assertAllEqual(expected_num_present, self.evaluate(num_present))\n\n\nclass LoadAndRemapMatrixTest(test.TestCase):\n  \"\"\"Tests for the load_and_remap_matrix() op.\"\"\"\n\n  def setUp(self):\n    ops.reset_default_graph()\n    self.old_num_rows = 5\n    self.old_num_cols = 16\n    self.matrix_value = np.reshape(\n        range(0, self.old_num_rows * self.old_num_cols), (self.old_num_rows,\n                                                          self.old_num_cols))\n    with variable_scope.variable_scope('some_scope'):\n      matrix = variable_scope.get_variable(\n          'matrix',\n          dtype=dtypes.float32,\n          initializer=constant_op.constant(\n              self.matrix_value, dtype=dtypes.float32))\n      self.old_tensor_name = 'some_scope/matrix'\n\n    save = saver.Saver([matrix])\n    with self.cached_session() as sess:\n      self.evaluate(variables.global_variables_initializer())\n      self.bundle_file = os.path.join(test.get_temp_dir(), 'bundle_checkpoint')\n      save.save(sess, self.bundle_file)\n\n  def test_load_and_remap_no_missing(self):\n    \"\"\"Tests the op's load and remap where there are no missing entries.\"\"\"\n\n    # No column remapping, new weight matrix has second row, then first row.\n    row_remapping = [1, 0]\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=row_remapping,\n        col_remapping=[],\n        initializing_values=[],\n        num_rows=2,\n        num_cols=self.old_num_cols)\n    with self.cached_session():\n      self.assertAllClose(self.matrix_value[row_remapping],\n                          self.evaluate(remapped_matrix))\n\n    # No row remapping, new weight matrix has third col, then first col.\n    row_remapping = list(range(self.old_num_rows))\n    col_remapping = [2, 0]\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=row_remapping,\n        col_remapping=col_remapping,\n        initializing_values=[],\n        num_rows=len(row_remapping),\n        num_cols=len(col_remapping))\n    with self.cached_session():\n      self.assertAllClose(self.matrix_value[row_remapping][:, col_remapping],\n                          self.evaluate(remapped_matrix))\n\n    # Both row and column remappings.\n    row_remapping = [1, 0, 4]\n    col_remapping = [1, 15]\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=row_remapping,\n        col_remapping=col_remapping,\n        initializing_values=[],\n        num_rows=len(row_remapping),\n        num_cols=len(col_remapping))\n    with self.cached_session():\n      self.assertAllClose(self.matrix_value[row_remapping][:, col_remapping],\n                          self.evaluate(remapped_matrix))\n\n  def test_load_and_remap_with_init(self):\n    \"\"\"Tests the op's load and remap where there are missing entries.\"\"\"\n    init_val = 42\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[2, -1, 0],\n        col_remapping=[1, -1],\n        initializing_values=[init_val] * 4,\n        num_rows=3,\n        num_cols=2)\n\n    expected_remapped_matrix = np.reshape(\n        [33, init_val, init_val, init_val, 1, init_val], [3, 2])\n\n    with self.cached_session():\n      self.assertAllClose(expected_remapped_matrix,\n                          self.evaluate(remapped_matrix))\n\n  def test_load_and_remap_all_missing_rows(self):\n    \"\"\"Tests when all the rows are missing and need to be initialized.\"\"\"\n    num_rows = 7\n    initializing_values = [42] * num_rows * self.old_num_cols\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[-1] * num_rows,\n        col_remapping=[],\n        initializing_values=initializing_values,\n        num_rows=num_rows,\n        num_cols=self.old_num_cols)\n    with self.cached_session():\n      self.assertAllClose(\n          np.reshape(initializing_values, (num_rows, self.old_num_cols)),\n          self.evaluate(remapped_matrix))\n\n  def test_load_and_remap_all_missing_rows_and_cols(self):\n    \"\"\"Tests when all the rows & cols are missing and need to be initialized.\"\"\"\n    num_rows = 7\n    num_cols = 4\n    initializing_values = [42] * num_rows * num_cols\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[-1] * num_rows,\n        col_remapping=[-1] * num_cols,\n        initializing_values=initializing_values,\n        num_rows=num_rows,\n        num_cols=num_cols)\n    with self.cached_session():\n      self.assertAllClose(\n          np.reshape(initializing_values, (num_rows, num_cols)),\n          self.evaluate(remapped_matrix))\n\n  def test_load_and_remap_invalid_dims(self):\n    ckpt_path = constant_op.constant(\n        '/tmp/warm_starting_util_test5kl2a3pc/tmpph76tep2/model-0',\n        shape=[],\n        dtype=dtypes.string)\n    old_tensor_name = constant_op.constant(\n        '/tmp/warm_starting_util_test5kl2a3pc/tmpph76tep2/model-0',\n        shape=[],\n        dtype=dtypes.string)\n    row_remapping = constant_op.constant(0, shape=[], dtype=dtypes.int64)\n    col_remapping = constant_op.constant(3, shape=[3], dtype=dtypes.int64)\n    initializing_values = constant_op.constant([],\n                                               shape=[0, 1],\n                                               dtype=dtypes.float32)\n    with self.cached_session(), self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError), 'tensor must be 1-D'):\n      self.evaluate(\n          gen_checkpoint_ops.load_and_remap_matrix(\n              ckpt_path=ckpt_path,\n              old_tensor_name=old_tensor_name,\n              row_remapping=row_remapping,\n              col_remapping=col_remapping,\n              initializing_values=initializing_values,\n              num_rows=1,\n              num_cols=1))\n\n  @test_util.run_deprecated_v1\n  def test_load_and_remap_invalid_remapping(self):\n    \"\"\"Tests that errors are raised when an ID maps to multiple new IDs.\n\n    (This should usually not happen when using public APIs).\n    \"\"\"\n    invalid_remapping = [1, 0, 0, 0, 1, 2]\n\n    # Invalid row remapping.\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=invalid_remapping,\n        col_remapping=[],\n        initializing_values=[],\n        num_rows=len(invalid_remapping),\n        num_cols=self.old_num_cols)\n    with self.cached_session(), self.assertRaises(errors.UnimplementedError):\n      self.evaluate(remapped_matrix)\n\n    # Invalid column remapping.\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=list(range(self.old_num_rows)),\n        col_remapping=invalid_remapping,\n        initializing_values=[],\n        num_rows=self.old_num_rows,\n        num_cols=len(invalid_remapping))\n    with self.cached_session(), self.assertRaises(errors.UnimplementedError):\n      self.evaluate(remapped_matrix)\n\n  @test_util.run_deprecated_v1\n  def test_load_and_remap_incorrect_initializing_values(self):\n    \"\"\"Tests that errors are raised with incorrect number of init values.\"\"\"\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[2, -1, 0],\n        col_remapping=[1, -1],\n        # Too few initializing values - there should be 4. For some reason,\n        # initializing_values must contain no element (instead of 3 or fewer) to\n        # ensure that a seg fault would reliably occur if the check raising the\n        # InvalidArgumentError were not present.\n        initializing_values=[],\n        num_rows=3,\n        num_cols=2)\n    with self.cached_session(), self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(remapped_matrix)\n\n    remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n        ckpt_path=[self.bundle_file],\n        old_tensor_name=self.old_tensor_name,\n        row_remapping=[2, -1, 0],\n        col_remapping=[1, -1],\n        # Too many initializing values - there should be 4.\n        initializing_values=[0] * 5,\n        num_rows=3,\n        num_cols=2)\n    with self.cached_session(), self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(remapped_matrix)\n\n\nclass LoadAndRemapMatrixWithMaxRowsTest(test.TestCase):\n  \"\"\"Tests for the load_and_remap_matrix() op.\n\n  (Specifically focused on the max_rows_in_memory arg and its effects on\n  TensorBundle's BundleReader and TensorSlice logic).\n  \"\"\"\n\n  def _test_loading_variable_with_max_rows(self, np_value, partitioner,\n                                           max_rows_in_memory):\n    \"\"\"Helper function for various tests using max_rows_in_memory.\"\"\"\n    ops.reset_default_graph()\n    old_tensor_name = 'matrix_to_load_and_remap'\n    matrix = variable_scope.get_variable(\n        old_tensor_name,\n        dtype=dtypes.float32,\n        initializer=constant_op.constant(np_value, dtype=dtypes.float32),\n        partitioner=partitioner)\n\n    with self.cached_session() as sess:\n      ckpt_path = os.path.join(test.get_temp_dir(), 'temp_ckpt')\n      save = saver.Saver([matrix])\n      self.evaluate(variables.global_variables_initializer())\n      save.save(sess, ckpt_path)\n      num_rows, num_cols = np_value.shape\n\n      # Tests loading the entire tensor (except reversed).\n      remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n          ckpt_path=ckpt_path,\n          old_tensor_name=old_tensor_name,\n          # Simply reverses the rows of the matrix.\n          row_remapping=list(range(num_rows - 1, -1, -1)),\n          col_remapping=[],\n          initializing_values=[],\n          num_rows=num_rows,\n          num_cols=num_cols,\n          max_rows_in_memory=max_rows_in_memory)\n      self.assertAllClose(np_value[::-1], self.evaluate(remapped_matrix))\n\n      # Tests loading the tensor (except for the first and last rows), with\n      # uninitialized values. Requires num_rows to be at least 3 since we're\n      # skipping the first and last rows.\n      self.assertGreater(num_rows, 2)\n      prefix_rows = 2\n      suffix_rows = 3\n      remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n          ckpt_path=ckpt_path,\n          old_tensor_name=old_tensor_name,\n          # Reverses the rows of the matrix, then prepends and appends\n          # uninitialized rows.\n          row_remapping=([-1] * prefix_rows + list(range(1, num_rows - 1)) +\n                         [-1] * suffix_rows),\n          col_remapping=[],\n          initializing_values=[42] * (prefix_rows + suffix_rows) * num_cols,\n          num_rows=num_rows - 2 + prefix_rows + suffix_rows,\n          num_cols=num_cols,\n          max_rows_in_memory=max_rows_in_memory)\n      self.assertAllClose(\n          np.vstack([\n              np.tile(42, [prefix_rows, num_cols]), np_value[1:-1],\n              np.tile(42, [suffix_rows, num_cols])\n          ]), self.evaluate(remapped_matrix))\n\n      # Tests when everything is taken from initializing_values.\n      new_rows = 7\n      initializing_values = [42] * new_rows * num_cols\n      remapped_matrix = gen_checkpoint_ops.load_and_remap_matrix(\n          ckpt_path=ckpt_path,\n          old_tensor_name=old_tensor_name,\n          # Nothing is loaded from the old tensor.\n          row_remapping=[-1] * new_rows,\n          col_remapping=[],\n          initializing_values=initializing_values,\n          num_rows=new_rows,\n          num_cols=num_cols,\n          max_rows_in_memory=max_rows_in_memory)\n      self.assertAllClose(\n          np.reshape(initializing_values, (new_rows, num_cols)),\n          self.evaluate(remapped_matrix))\n\n  @test_util.run_deprecated_v1\n  def test_loading_rows_divisible_by_max_rows(self):\n    \"\"\"Tests loading normal var when rows are evenly divisible by max_rows.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=None,\n        # 9 is evenly divisible by 3.\n        max_rows_in_memory=3)\n\n  @test_util.run_deprecated_v1\n  def test_loading_rows_not_divisible_by_max_rows(self):\n    \"\"\"Tests loading normal var when rows aren't divisible by max_rows.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=None,\n        # 9 is not evenly divisible by 4.\n        max_rows_in_memory=4)\n\n  @test_util.run_deprecated_v1\n  def test_loading_rows_less_than_max_rows(self):\n    \"\"\"Tests loading normal var as a single slice.\n\n    (When the specified max_rows_in_memory is larger than the number of rows)\n    \"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=None,\n        # 10 > 9.\n        max_rows_in_memory=10)\n\n  @test_util.run_deprecated_v1\n  def test_loading_no_max_rows(self):\n    \"\"\"Tests loading normal var as a single slice with no valid max_rows.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 18)), (6, 3)),\n        partitioner=None,\n        max_rows_in_memory=-1)\n\n  @test_util.run_deprecated_v1\n  def test_loading_partitions_equals_max_rows(self):\n    \"\"\"Tests loading partitioned var sliced on partition boundary.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=partitioned_variables.fixed_size_partitioner(3),\n        # With a tensor of shape [9, 3] and 3 partitions, each partition has\n        # exactly 3 rows.\n        max_rows_in_memory=3)\n\n  @test_util.run_deprecated_v1\n  def test_loading_partitions_greater_than_max_rows(self):\n    \"\"\"Tests loading partitioned var with more slices than partitions.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=partitioned_variables.fixed_size_partitioner(3),\n        # Even though each partition has 3 rows, we'll only load the tensor one\n        # row at a time.\n        max_rows_in_memory=1)\n\n  @test_util.run_deprecated_v1\n  def test_loading_partitions_less_than_max_rows(self):\n    \"\"\"Tests loading partitioned var as a single slice.\n\n    (When the specified max_rows_in_memory is larger than the number of rows)\n    \"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=partitioned_variables.fixed_size_partitioner(3),\n        max_rows_in_memory=10)\n\n  @test_util.run_deprecated_v1\n  def test_loading_partitions_no_max_rows(self):\n    \"\"\"Tests loading partitioned var as single slice with no valid max_rows.\"\"\"\n    self._test_loading_variable_with_max_rows(\n        np_value=np.reshape(list(range(0, 36)), (9, 4)),\n        partitioner=partitioned_variables.fixed_size_partitioner(3),\n        max_rows_in_memory=-1)\n\n\nif __name__ == '__main__':\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/load_and_remap_matrix_op.cc", "tensorflow/python/kernel_tests/io_ops/checkpoint_ops_test.py"], "buggy_code_start_loc": [76, 228], "buggy_code_end_loc": [76, 228], "fixing_code_start_loc": [77, 229], "fixing_code_end_loc": [82, 255], "type": "CWE-20", "message": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.LoadAndRemapMatrix does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. The code assumes `initializing_values` is a vector but there is no validation for this before accessing its value. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "other": {"cve": {"id": "CVE-2022-29199", "sourceIdentifier": "security-advisories@github.com", "published": "2022-05-20T22:16:40.870", "lastModified": "2022-05-27T00:09:24.320", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.LoadAndRemapMatrix does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. The code assumes `initializing_values` is a vector but there is no validation for this before accessing its value. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En versiones anteriores a 2.9.0, 2.8.1, 2.7.2 y 2.6.4, la implementaci\u00f3n de \"tf.raw_ops.LoadAndRemapMatrix\" no comprueba completamente los argumentos de entrada. Esto resulta en un fallo de \"CHECK\" que puede ser usado para desencadenar un ataque de denegaci\u00f3n de servicio. El c\u00f3digo asume que \"initializing_values\" es un vector pero no es comprobado antes de acceder a su valor. Las versiones 2.9.0, 2.8.1, 2.7.2 y 2.6.4 contienen un parche para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.4", "matchCriteriaId": "D9359D32-D090-44CF-AC43-2046084A28BB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:-:*:*:*:*:*:*", "matchCriteriaId": "E9EA1898-ACAA-4699-8BAE-54D62C1819FB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "130DE3C9-6842-456F-A259-BF8FF8457217"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "BBF2FCEF-989C-409D-9F4C-81418C65B972"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "9CFB1CFC-579D-4647-A472-6DE8BE1951DE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "F3F3F37E-D27F-4060-830C-0AFF16150777"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/load_and_remap_matrix_op.cc#L70-L98", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/3150642acbbe254e3c3c5d2232143fa591855ac9", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.6.4", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.7.2", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.8.1", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.9.0", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-p9rc-rmr5-529j", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/3150642acbbe254e3c3c5d2232143fa591855ac9"}}