{"buggy_code": ["# Copyright 2021 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport codecs\nimport itertools\nimport logging\nimport re\nfrom typing import TYPE_CHECKING, Dict, Generator, Iterable, Optional, Set, Union\n\nif TYPE_CHECKING:\n    from lxml import etree\n\nlogger = logging.getLogger(__name__)\n\n_charset_match = re.compile(\n    rb'<\\s*meta[^>]*charset\\s*=\\s*\"?([a-z0-9_-]+)\"?', flags=re.I\n)\n_xml_encoding_match = re.compile(\n    rb'\\s*<\\s*\\?\\s*xml[^>]*encoding=\"([a-z0-9_-]+)\"', flags=re.I\n)\n_content_type_match = re.compile(r'.*; *charset=\"?(.*?)\"?(;|$)', flags=re.I)\n\n# Certain elements aren't meant for display.\nARIA_ROLES_TO_IGNORE = {\"directory\", \"menu\", \"menubar\", \"toolbar\"}\n\n\ndef _normalise_encoding(encoding: str) -> Optional[str]:\n    \"\"\"Use the Python codec's name as the normalised entry.\"\"\"\n    try:\n        return codecs.lookup(encoding).name\n    except LookupError:\n        return None\n\n\ndef _get_html_media_encodings(\n    body: bytes, content_type: Optional[str]\n) -> Iterable[str]:\n    \"\"\"\n    Get potential encoding of the body based on the (presumably) HTML body or the content-type header.\n\n    The precedence used for finding a character encoding is:\n\n    1. <meta> tag with a charset declared.\n    2. The XML document's character encoding attribute.\n    3. The Content-Type header.\n    4. Fallback to utf-8.\n    5. Fallback to windows-1252.\n\n    This roughly follows the algorithm used by BeautifulSoup's bs4.dammit.EncodingDetector.\n\n    Args:\n        body: The HTML document, as bytes.\n        content_type: The Content-Type header.\n\n    Returns:\n        The character encoding of the body, as a string.\n    \"\"\"\n    # There's no point in returning an encoding more than once.\n    attempted_encodings: Set[str] = set()\n\n    # Limit searches to the first 1kb, since it ought to be at the top.\n    body_start = body[:1024]\n\n    # Check if it has an encoding set in a meta tag.\n    match = _charset_match.search(body_start)\n    if match:\n        encoding = _normalise_encoding(match.group(1).decode(\"ascii\"))\n        if encoding:\n            attempted_encodings.add(encoding)\n            yield encoding\n\n    # TODO Support <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\n\n    # Check if it has an XML document with an encoding.\n    match = _xml_encoding_match.match(body_start)\n    if match:\n        encoding = _normalise_encoding(match.group(1).decode(\"ascii\"))\n        if encoding and encoding not in attempted_encodings:\n            attempted_encodings.add(encoding)\n            yield encoding\n\n    # Check the HTTP Content-Type header for a character set.\n    if content_type:\n        content_match = _content_type_match.match(content_type)\n        if content_match:\n            encoding = _normalise_encoding(content_match.group(1))\n            if encoding and encoding not in attempted_encodings:\n                attempted_encodings.add(encoding)\n                yield encoding\n\n    # Finally, fallback to UTF-8, then windows-1252.\n    for fallback in (\"utf-8\", \"cp1252\"):\n        if fallback not in attempted_encodings:\n            yield fallback\n\n\ndef decode_body(\n    body: bytes, uri: str, content_type: Optional[str] = None\n) -> Optional[\"etree.Element\"]:\n    \"\"\"\n    This uses lxml to parse the HTML document.\n\n    Args:\n        body: The HTML document, as bytes.\n        uri: The URI used to download the body.\n        content_type: The Content-Type header.\n\n    Returns:\n        The parsed HTML body, or None if an error occurred during processed.\n    \"\"\"\n    # If there's no body, nothing useful is going to be found.\n    if not body:\n        return None\n\n    # The idea here is that multiple encodings are tried until one works.\n    # Unfortunately the result is never used and then LXML will decode the string\n    # again with the found encoding.\n    for encoding in _get_html_media_encodings(body, content_type):\n        try:\n            body.decode(encoding)\n        except Exception:\n            pass\n        else:\n            break\n    else:\n        logger.warning(\"Unable to decode HTML body for %s\", uri)\n        return None\n\n    from lxml import etree\n\n    # Create an HTML parser.\n    parser = etree.HTMLParser(recover=True, encoding=encoding)\n\n    # Attempt to parse the body. Returns None if the body was successfully\n    # parsed, but no tree was found.\n    return etree.fromstring(body, parser)\n\n\ndef parse_html_to_open_graph(tree: \"etree.Element\") -> Dict[str, Optional[str]]:\n    \"\"\"\n    Parse the HTML document into an Open Graph response.\n\n    This uses lxml to search the HTML document for Open Graph data (or\n    synthesizes it from the document).\n\n    Args:\n        tree: The parsed HTML document.\n\n    Returns:\n        The Open Graph response as a dictionary.\n    \"\"\"\n\n    # if we see any image URLs in the OG response, then spider them\n    # (although the client could choose to do this by asking for previews of those\n    # URLs to avoid DoSing the server)\n\n    # \"og:type\"         : \"video\",\n    # \"og:url\"          : \"https://www.youtube.com/watch?v=LXDBoHyjmtw\",\n    # \"og:site_name\"    : \"YouTube\",\n    # \"og:video:type\"   : \"application/x-shockwave-flash\",\n    # \"og:description\"  : \"Fun stuff happening here\",\n    # \"og:title\"        : \"RemoteJam - Matrix team hack for Disrupt Europe Hackathon\",\n    # \"og:image\"        : \"https://i.ytimg.com/vi/LXDBoHyjmtw/maxresdefault.jpg\",\n    # \"og:video:url\"    : \"http://www.youtube.com/v/LXDBoHyjmtw?version=3&autohide=1\",\n    # \"og:video:width\"  : \"1280\"\n    # \"og:video:height\" : \"720\",\n    # \"og:video:secure_url\": \"https://www.youtube.com/v/LXDBoHyjmtw?version=3\",\n\n    og: Dict[str, Optional[str]] = {}\n    for tag in tree.xpath(\n        \"//*/meta[starts-with(@property, 'og:')][@content][not(@content='')]\"\n    ):\n        # if we've got more than 50 tags, someone is taking the piss\n        if len(og) >= 50:\n            logger.warning(\"Skipping OG for page with too many 'og:' tags\")\n            return {}\n\n        og[tag.attrib[\"property\"]] = tag.attrib[\"content\"]\n\n    # TODO: grab article: meta tags too, e.g.:\n\n    # \"article:publisher\" : \"https://www.facebook.com/thethudonline\" />\n    # \"article:author\" content=\"https://www.facebook.com/thethudonline\" />\n    # \"article:tag\" content=\"baby\" />\n    # \"article:section\" content=\"Breaking News\" />\n    # \"article:published_time\" content=\"2016-03-31T19:58:24+00:00\" />\n    # \"article:modified_time\" content=\"2016-04-01T18:31:53+00:00\" />\n\n    if \"og:title\" not in og:\n        # Attempt to find a title from the title tag, or the biggest header on the page.\n        title = tree.xpath(\"((//title)[1] | (//h1)[1] | (//h2)[1] | (//h3)[1])/text()\")\n        if title:\n            og[\"og:title\"] = title[0].strip()\n        else:\n            og[\"og:title\"] = None\n\n    if \"og:image\" not in og:\n        meta_image = tree.xpath(\n            \"//*/meta[translate(@itemprop, 'IMAGE', 'image')='image'][not(@content='')]/@content[1]\"\n        )\n        # If a meta image is found, use it.\n        if meta_image:\n            og[\"og:image\"] = meta_image[0]\n        else:\n            # Try to find images which are larger than 10px by 10px.\n            #\n            # TODO: consider inlined CSS styles as well as width & height attribs\n            images = tree.xpath(\"//img[@src][number(@width)>10][number(@height)>10]\")\n            images = sorted(\n                images,\n                key=lambda i: (\n                    -1 * float(i.attrib[\"width\"]) * float(i.attrib[\"height\"])\n                ),\n            )\n            # If no images were found, try to find *any* images.\n            if not images:\n                images = tree.xpath(\"//img[@src][1]\")\n            if images:\n                og[\"og:image\"] = images[0].attrib[\"src\"]\n\n            # Finally, fallback to the favicon if nothing else.\n            else:\n                favicons = tree.xpath(\"//link[@href][contains(@rel, 'icon')]/@href[1]\")\n                if favicons:\n                    og[\"og:image\"] = favicons[0]\n\n    if \"og:description\" not in og:\n        # Check the first meta description tag for content.\n        meta_description = tree.xpath(\n            \"//*/meta[translate(@name, 'DESCRIPTION', 'description')='description'][not(@content='')]/@content[1]\"\n        )\n        # If a meta description is found with content, use it.\n        if meta_description:\n            og[\"og:description\"] = meta_description[0]\n        else:\n            og[\"og:description\"] = parse_html_description(tree)\n    elif og[\"og:description\"]:\n        # This must be a non-empty string at this point.\n        assert isinstance(og[\"og:description\"], str)\n        og[\"og:description\"] = summarize_paragraphs([og[\"og:description\"]])\n\n    # TODO: delete the url downloads to stop diskfilling,\n    # as we only ever cared about its OG\n    return og\n\n\ndef parse_html_description(tree: \"etree.Element\") -> Optional[str]:\n    \"\"\"\n    Calculate a text description based on an HTML document.\n\n    Grabs any text nodes which are inside the <body/> tag, unless they are within\n    an HTML5 semantic markup tag (<header/>, <nav/>, <aside/>, <footer/>), or\n    if they are within a <script/>, <svg/> or <style/> tag, or if they are within\n    a tag whose content is usually only shown to old browsers\n    (<iframe/>, <video/>, <canvas/>, <picture/>).\n\n    This is a very very very coarse approximation to a plain text render of the page.\n\n    Args:\n        tree: The parsed HTML document.\n\n    Returns:\n        The plain text description, or None if one cannot be generated.\n    \"\"\"\n    # We don't just use XPATH here as that is slow on some machines.\n\n    from lxml import etree\n\n    TAGS_TO_REMOVE = (\n        \"header\",\n        \"nav\",\n        \"aside\",\n        \"footer\",\n        \"script\",\n        \"noscript\",\n        \"style\",\n        \"svg\",\n        \"iframe\",\n        \"video\",\n        \"canvas\",\n        \"img\",\n        \"picture\",\n        etree.Comment,\n    )\n\n    # Split all the text nodes into paragraphs (by splitting on new\n    # lines)\n    text_nodes = (\n        re.sub(r\"\\s+\", \"\\n\", el).strip()\n        for el in _iterate_over_text(tree.find(\"body\"), *TAGS_TO_REMOVE)\n    )\n    return summarize_paragraphs(text_nodes)\n\n\ndef _iterate_over_text(\n    tree: \"etree.Element\", *tags_to_ignore: Union[str, \"etree.Comment\"]\n) -> Generator[str, None, None]:\n    \"\"\"Iterate over the tree returning text nodes in a depth first fashion,\n    skipping text nodes inside certain tags.\n    \"\"\"\n    # This is basically a stack that we extend using itertools.chain.\n    # This will either consist of an element to iterate over *or* a string\n    # to be returned.\n    elements = iter([tree])\n    while True:\n        el = next(elements, None)\n        if el is None:\n            return\n\n        if isinstance(el, str):\n            yield el\n        elif el.tag not in tags_to_ignore:\n            # If the element isn't meant for display, ignore it.\n            if el.get(\"role\") in ARIA_ROLES_TO_IGNORE:\n                continue\n\n            # el.text is the text before the first child, so we can immediately\n            # return it if the text exists.\n            if el.text:\n                yield el.text\n\n            # We add to the stack all the elements children, interspersed with\n            # each child's tail text (if it exists). The tail text of a node\n            # is text that comes *after* the node, so we always include it even\n            # if we ignore the child node.\n            elements = itertools.chain(\n                itertools.chain.from_iterable(  # Basically a flatmap\n                    [child, child.tail] if child.tail else [child]\n                    for child in el.iterchildren()\n                ),\n                elements,\n            )\n\n\ndef summarize_paragraphs(\n    text_nodes: Iterable[str], min_size: int = 200, max_size: int = 500\n) -> Optional[str]:\n    \"\"\"\n    Try to get a summary respecting first paragraph and then word boundaries.\n\n    Args:\n        text_nodes: The paragraphs to summarize.\n        min_size: The minimum number of words to include.\n        max_size: The maximum number of words to include.\n\n    Returns:\n        A summary of the text nodes, or None if that was not possible.\n    \"\"\"\n\n    # TODO: Respect sentences?\n\n    description = \"\"\n\n    # Keep adding paragraphs until we get to the MIN_SIZE.\n    for text_node in text_nodes:\n        if len(description) < min_size:\n            text_node = re.sub(r\"[\\t \\r\\n]+\", \" \", text_node)\n            description += text_node + \"\\n\\n\"\n        else:\n            break\n\n    description = description.strip()\n    description = re.sub(r\"[\\t ]+\", \" \", description)\n    description = re.sub(r\"[\\t \\r\\n]*[\\r\\n]+\", \"\\n\\n\", description)\n\n    # If the concatenation of paragraphs to get above MIN_SIZE\n    # took us over MAX_SIZE, then we need to truncate mid paragraph\n    if len(description) > max_size:\n        new_desc = \"\"\n\n        # This splits the paragraph into words, but keeping the\n        # (preceding) whitespace intact so we can easily concat\n        # words back together.\n        for match in re.finditer(r\"\\s*\\S+\", description):\n            word = match.group()\n\n            # Keep adding words while the total length is less than\n            # MAX_SIZE.\n            if len(word) + len(new_desc) < max_size:\n                new_desc += word\n            else:\n                # At this point the next word *will* take us over\n                # MAX_SIZE, but we also want to ensure that its not\n                # a huge word. If it is add it anyway and we'll\n                # truncate later.\n                if len(new_desc) < min_size:\n                    new_desc += word\n                break\n\n        # Double check that we're not over the limit\n        if len(new_desc) > max_size:\n            new_desc = new_desc[:max_size]\n\n        # We always add an ellipsis because at the very least\n        # we chopped mid paragraph.\n        description = new_desc.strip() + \"\u2026\"\n    return description if description else None\n", "# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom synapse.rest.media.v1.preview_html import (\n    _get_html_media_encodings,\n    decode_body,\n    parse_html_to_open_graph,\n    summarize_paragraphs,\n)\n\nfrom tests import unittest\n\ntry:\n    import lxml\nexcept ImportError:\n    lxml = None\n\n\nclass SummarizeTestCase(unittest.TestCase):\n    if not lxml:\n        skip = \"url preview feature requires lxml\"\n\n    def test_long_summarize(self) -> None:\n        example_paras = [\n            \"\"\"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\n            Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\n            Troms county, Norway. The administrative centre of the municipality is\n            the city of Troms\u00f8. Outside of Norway, Tromso and Troms\u00f6 are\n            alternative spellings of the city.Troms\u00f8 is considered the northernmost\n            city in the world with a population above 50,000. The most populous town\n            north of it is Alta, Norway, with a population of 14,272 (2013).\"\"\",\n            \"\"\"Troms\u00f8 lies in Northern Norway. The municipality has a population of\n            (2015) 72,066, but with an annual influx of students it has over 75,000\n            most of the year. It is the largest urban area in Northern Norway and the\n            third largest north of the Arctic Circle (following Murmansk and Norilsk).\n            Most of Troms\u00f8, including the city centre, is located on the island of\n            Troms\u00f8ya, 350 kilometres (217 mi) north of the Arctic Circle. In 2012,\n            Troms\u00f8ya had a population of 36,088. Substantial parts of the urban area\n            are also situated on the mainland to the east, and on parts of Kval\u00f8ya\u2014a\n            large island to the west. Troms\u00f8ya is connected to the mainland by the Troms\u00f8\n            Bridge and the Troms\u00f8ysund Tunnel, and to the island of Kval\u00f8ya by the\n            Sandnessund Bridge. Troms\u00f8 Airport connects the city to many destinations\n            in Europe. The city is warmer than most other places located on the same\n            latitude, due to the warming effect of the Gulf Stream.\"\"\",\n            \"\"\"The city centre of Troms\u00f8 contains the highest number of old wooden\n            houses in Northern Norway, the oldest house dating from 1789. The Arctic\n            Cathedral, a modern church from 1965, is probably the most famous landmark\n            in Troms\u00f8. The city is a cultural centre for its region, with several\n            festivals taking place in the summer. Some of Norway's best-known\n             musicians, Torbj\u00f8rn Brundtland and Svein Berge of the electronica duo\n             R\u00f6yksopp and Lene Marlin grew up and started their careers in Troms\u00f8.\n             Noted electronic musician Geir Jenssen also hails from Troms\u00f8.\"\"\",\n        ]\n\n        desc = summarize_paragraphs(example_paras, min_size=200, max_size=500)\n\n        self.assertEqual(\n            desc,\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway. The administrative centre of the municipality is\"\n            \" the city of Troms\u00f8. Outside of Norway, Tromso and Troms\u00f6 are\"\n            \" alternative spellings of the city.Troms\u00f8 is considered the northernmost\"\n            \" city in the world with a population above 50,000. The most populous town\"\n            \" north of it is Alta, Norway, with a population of 14,272 (2013).\",\n        )\n\n        desc = summarize_paragraphs(example_paras[1:], min_size=200, max_size=500)\n\n        self.assertEqual(\n            desc,\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year. It is the largest urban area in Northern Norway and the\"\n            \" third largest north of the Arctic Circle (following Murmansk and Norilsk).\"\n            \" Most of Troms\u00f8, including the city centre, is located on the island of\"\n            \" Troms\u00f8ya, 350 kilometres (217 mi) north of the Arctic Circle. In 2012,\"\n            \" Troms\u00f8ya had a population of 36,088. Substantial parts of the urban\u2026\",\n        )\n\n    def test_short_summarize(self) -> None:\n        example_paras = [\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway.\",\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year.\",\n            \"The city centre of Troms\u00f8 contains the highest number of old wooden\"\n            \" houses in Northern Norway, the oldest house dating from 1789. The Arctic\"\n            \" Cathedral, a modern church from 1965, is probably the most famous landmark\"\n            \" in Troms\u00f8.\",\n        ]\n\n        desc = summarize_paragraphs(example_paras, min_size=200, max_size=500)\n\n        self.assertEqual(\n            desc,\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway.\\n\"\n            \"\\n\"\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year.\",\n        )\n\n    def test_small_then_large_summarize(self) -> None:\n        example_paras = [\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway.\",\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year.\"\n            \" The city centre of Troms\u00f8 contains the highest number of old wooden\"\n            \" houses in Northern Norway, the oldest house dating from 1789. The Arctic\"\n            \" Cathedral, a modern church from 1965, is probably the most famous landmark\"\n            \" in Troms\u00f8.\",\n        ]\n\n        desc = summarize_paragraphs(example_paras, min_size=200, max_size=500)\n        self.assertEqual(\n            desc,\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway.\\n\"\n            \"\\n\"\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year. The city centre of Troms\u00f8 contains the highest number\"\n            \" of old wooden houses in Northern Norway, the oldest house dating from\"\n            \" 1789. The Arctic Cathedral, a modern church from\u2026\",\n        )\n\n\nclass OpenGraphFromHtmlTestCase(unittest.TestCase):\n    if not lxml:\n        skip = \"url preview feature requires lxml\"\n\n    def test_simple(self) -> None:\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_comment(self) -> None:\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        <!-- HTML comment -->\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_comment2(self) -> None:\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        Some text.\n        <!-- HTML comment -->\n        Some more text.\n        <p>Text</p>\n        More text\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(\n            og,\n            {\n                \"og:title\": \"Foo\",\n                \"og:description\": \"Some text.\\n\\nSome more text.\\n\\nText\\n\\nMore text\",\n            },\n        )\n\n    def test_script(self) -> None:\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        <script> (function() {})() </script>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_missing_title(self) -> None:\n        html = b\"\"\"\n        <html>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": None, \"og:description\": \"Some text.\"})\n\n        # Another variant is a title with no content.\n        html = b\"\"\"\n        <html>\n        <head><title></title></head>\n        <body>\n        <h1>Title</h1>\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Title\", \"og:description\": \"Title\"})\n\n    def test_h1_as_title(self) -> None:\n        html = b\"\"\"\n        <html>\n        <meta property=\"og:description\" content=\"Some text.\"/>\n        <body>\n        <h1>Title</h1>\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Title\", \"og:description\": \"Some text.\"})\n\n    def test_empty_description(self) -> None:\n        \"\"\"Description tags with empty content should be ignored.\"\"\"\n        html = b\"\"\"\n        <html>\n        <meta property=\"og:description\" content=\"\"/>\n        <meta property=\"og:description\"/>\n        <meta name=\"description\" content=\"\"/>\n        <meta name=\"description\"/>\n        <meta name=\"description\" content=\"Finally!\"/>\n        <body>\n        <h1>Title</h1>\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Title\", \"og:description\": \"Finally!\"})\n\n    def test_missing_title_and_broken_h1(self) -> None:\n        html = b\"\"\"\n        <html>\n        <body>\n        <h1><a href=\"foo\"/></h1>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": None, \"og:description\": \"Some text.\"})\n\n    def test_empty(self) -> None:\n        \"\"\"Test a body with no data in it.\"\"\"\n        html = b\"\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        self.assertIsNone(tree)\n\n    def test_no_tree(self) -> None:\n        \"\"\"A valid body with no tree in it.\"\"\"\n        html = b\"\\x00\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        self.assertIsNone(tree)\n\n    def test_xml(self) -> None:\n        \"\"\"Test decoding XML and ensure it works properly.\"\"\"\n        # Note that the strip() call is important to ensure the xml tag starts\n        # at the initial byte.\n        html = b\"\"\"\n        <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n        <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n        <html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n        <head><title>Foo</title></head><body>Some text.</body></html>\n        \"\"\".strip()\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_invalid_encoding(self) -> None:\n        \"\"\"An invalid character encoding should be ignored and treated as UTF-8, if possible.\"\"\"\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n        tree = decode_body(html, \"http://example.com/test.html\", \"invalid-encoding\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_invalid_encoding2(self) -> None:\n        \"\"\"A body which doesn't match the sent character encoding.\"\"\"\n        # Note that this contains an invalid UTF-8 sequence in the title.\n        html = b\"\"\"\n        <html>\n        <head><title>\\xff\\xff Foo</title></head>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(og, {\"og:title\": \"\u00ff\u00ff Foo\", \"og:description\": \"Some text.\"})\n\n    def test_windows_1252(self) -> None:\n        \"\"\"A body which uses cp1252, but doesn't declare that.\"\"\"\n        html = b\"\"\"\n        <html>\n        <head><title>\\xf3</title></head>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(og, {\"og:title\": \"\u00f3\", \"og:description\": \"Some text.\"})\n\n\nclass MediaEncodingTestCase(unittest.TestCase):\n    def test_meta_charset(self) -> None:\n        \"\"\"A character encoding is found via the meta tag.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <html>\n        <head><meta charset=\"ascii\">\n        </head>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"ascii\", \"utf-8\", \"cp1252\"])\n\n        # A less well-formed version.\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <html>\n        <head>< meta charset = ascii>\n        </head>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"ascii\", \"utf-8\", \"cp1252\"])\n\n    def test_meta_charset_underscores(self) -> None:\n        \"\"\"A character encoding contains underscore.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <html>\n        <head><meta charset=\"Shift_JIS\">\n        </head>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"shift_jis\", \"utf-8\", \"cp1252\"])\n\n    def test_xml_encoding(self) -> None:\n        \"\"\"A character encoding is found via the meta tag.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <?xml version=\"1.0\" encoding=\"ascii\"?>\n        <html>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"ascii\", \"utf-8\", \"cp1252\"])\n\n    def test_meta_xml_encoding(self) -> None:\n        \"\"\"Meta tags take precedence over XML encoding.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <?xml version=\"1.0\" encoding=\"ascii\"?>\n        <html>\n        <head><meta charset=\"UTF-16\">\n        </head>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"utf-16\", \"ascii\", \"utf-8\", \"cp1252\"])\n\n    def test_content_type(self) -> None:\n        \"\"\"A character encoding is found via the Content-Type header.\"\"\"\n        # Test a few variations of the header.\n        headers = (\n            'text/html; charset=\"ascii\";',\n            \"text/html;charset=ascii;\",\n            'text/html;  charset=\"ascii\"',\n            \"text/html; charset=ascii\",\n            'text/html; charset=\"ascii;',\n            'text/html; charset=ascii\";',\n        )\n        for header in headers:\n            encodings = _get_html_media_encodings(b\"\", header)\n            self.assertEqual(list(encodings), [\"ascii\", \"utf-8\", \"cp1252\"])\n\n    def test_fallback(self) -> None:\n        \"\"\"A character encoding cannot be found in the body or header.\"\"\"\n        encodings = _get_html_media_encodings(b\"\", \"text/html\")\n        self.assertEqual(list(encodings), [\"utf-8\", \"cp1252\"])\n\n    def test_duplicates(self) -> None:\n        \"\"\"Ensure each encoding is only attempted once.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <?xml version=\"1.0\" encoding=\"utf8\"?>\n        <html>\n        <head><meta charset=\"UTF-8\">\n        </head>\n        </html>\n        \"\"\",\n            'text/html; charset=\"UTF_8\"',\n        )\n        self.assertEqual(list(encodings), [\"utf-8\", \"cp1252\"])\n\n    def test_unknown_invalid(self) -> None:\n        \"\"\"A character encoding should be ignored if it is unknown or invalid.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <html>\n        <head><meta charset=\"invalid\">\n        </head>\n        </html>\n        \"\"\",\n            'text/html; charset=\"invalid\"',\n        )\n        self.assertEqual(list(encodings), [\"utf-8\", \"cp1252\"])\n"], "fixing_code": ["# Copyright 2021 The Matrix.org Foundation C.I.C.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport codecs\nimport logging\nimport re\nfrom typing import TYPE_CHECKING, Dict, Generator, Iterable, List, Optional, Set, Union\n\nif TYPE_CHECKING:\n    from lxml import etree\n\nlogger = logging.getLogger(__name__)\n\n_charset_match = re.compile(\n    rb'<\\s*meta[^>]*charset\\s*=\\s*\"?([a-z0-9_-]+)\"?', flags=re.I\n)\n_xml_encoding_match = re.compile(\n    rb'\\s*<\\s*\\?\\s*xml[^>]*encoding=\"([a-z0-9_-]+)\"', flags=re.I\n)\n_content_type_match = re.compile(r'.*; *charset=\"?(.*?)\"?(;|$)', flags=re.I)\n\n# Certain elements aren't meant for display.\nARIA_ROLES_TO_IGNORE = {\"directory\", \"menu\", \"menubar\", \"toolbar\"}\n\n\ndef _normalise_encoding(encoding: str) -> Optional[str]:\n    \"\"\"Use the Python codec's name as the normalised entry.\"\"\"\n    try:\n        return codecs.lookup(encoding).name\n    except LookupError:\n        return None\n\n\ndef _get_html_media_encodings(\n    body: bytes, content_type: Optional[str]\n) -> Iterable[str]:\n    \"\"\"\n    Get potential encoding of the body based on the (presumably) HTML body or the content-type header.\n\n    The precedence used for finding a character encoding is:\n\n    1. <meta> tag with a charset declared.\n    2. The XML document's character encoding attribute.\n    3. The Content-Type header.\n    4. Fallback to utf-8.\n    5. Fallback to windows-1252.\n\n    This roughly follows the algorithm used by BeautifulSoup's bs4.dammit.EncodingDetector.\n\n    Args:\n        body: The HTML document, as bytes.\n        content_type: The Content-Type header.\n\n    Returns:\n        The character encoding of the body, as a string.\n    \"\"\"\n    # There's no point in returning an encoding more than once.\n    attempted_encodings: Set[str] = set()\n\n    # Limit searches to the first 1kb, since it ought to be at the top.\n    body_start = body[:1024]\n\n    # Check if it has an encoding set in a meta tag.\n    match = _charset_match.search(body_start)\n    if match:\n        encoding = _normalise_encoding(match.group(1).decode(\"ascii\"))\n        if encoding:\n            attempted_encodings.add(encoding)\n            yield encoding\n\n    # TODO Support <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\n\n    # Check if it has an XML document with an encoding.\n    match = _xml_encoding_match.match(body_start)\n    if match:\n        encoding = _normalise_encoding(match.group(1).decode(\"ascii\"))\n        if encoding and encoding not in attempted_encodings:\n            attempted_encodings.add(encoding)\n            yield encoding\n\n    # Check the HTTP Content-Type header for a character set.\n    if content_type:\n        content_match = _content_type_match.match(content_type)\n        if content_match:\n            encoding = _normalise_encoding(content_match.group(1))\n            if encoding and encoding not in attempted_encodings:\n                attempted_encodings.add(encoding)\n                yield encoding\n\n    # Finally, fallback to UTF-8, then windows-1252.\n    for fallback in (\"utf-8\", \"cp1252\"):\n        if fallback not in attempted_encodings:\n            yield fallback\n\n\ndef decode_body(\n    body: bytes, uri: str, content_type: Optional[str] = None\n) -> Optional[\"etree.Element\"]:\n    \"\"\"\n    This uses lxml to parse the HTML document.\n\n    Args:\n        body: The HTML document, as bytes.\n        uri: The URI used to download the body.\n        content_type: The Content-Type header.\n\n    Returns:\n        The parsed HTML body, or None if an error occurred during processed.\n    \"\"\"\n    # If there's no body, nothing useful is going to be found.\n    if not body:\n        return None\n\n    # The idea here is that multiple encodings are tried until one works.\n    # Unfortunately the result is never used and then LXML will decode the string\n    # again with the found encoding.\n    for encoding in _get_html_media_encodings(body, content_type):\n        try:\n            body.decode(encoding)\n        except Exception:\n            pass\n        else:\n            break\n    else:\n        logger.warning(\"Unable to decode HTML body for %s\", uri)\n        return None\n\n    from lxml import etree\n\n    # Create an HTML parser.\n    parser = etree.HTMLParser(recover=True, encoding=encoding)\n\n    # Attempt to parse the body. Returns None if the body was successfully\n    # parsed, but no tree was found.\n    return etree.fromstring(body, parser)\n\n\ndef parse_html_to_open_graph(tree: \"etree.Element\") -> Dict[str, Optional[str]]:\n    \"\"\"\n    Parse the HTML document into an Open Graph response.\n\n    This uses lxml to search the HTML document for Open Graph data (or\n    synthesizes it from the document).\n\n    Args:\n        tree: The parsed HTML document.\n\n    Returns:\n        The Open Graph response as a dictionary.\n    \"\"\"\n\n    # if we see any image URLs in the OG response, then spider them\n    # (although the client could choose to do this by asking for previews of those\n    # URLs to avoid DoSing the server)\n\n    # \"og:type\"         : \"video\",\n    # \"og:url\"          : \"https://www.youtube.com/watch?v=LXDBoHyjmtw\",\n    # \"og:site_name\"    : \"YouTube\",\n    # \"og:video:type\"   : \"application/x-shockwave-flash\",\n    # \"og:description\"  : \"Fun stuff happening here\",\n    # \"og:title\"        : \"RemoteJam - Matrix team hack for Disrupt Europe Hackathon\",\n    # \"og:image\"        : \"https://i.ytimg.com/vi/LXDBoHyjmtw/maxresdefault.jpg\",\n    # \"og:video:url\"    : \"http://www.youtube.com/v/LXDBoHyjmtw?version=3&autohide=1\",\n    # \"og:video:width\"  : \"1280\"\n    # \"og:video:height\" : \"720\",\n    # \"og:video:secure_url\": \"https://www.youtube.com/v/LXDBoHyjmtw?version=3\",\n\n    og: Dict[str, Optional[str]] = {}\n    for tag in tree.xpath(\n        \"//*/meta[starts-with(@property, 'og:')][@content][not(@content='')]\"\n    ):\n        # if we've got more than 50 tags, someone is taking the piss\n        if len(og) >= 50:\n            logger.warning(\"Skipping OG for page with too many 'og:' tags\")\n            return {}\n\n        og[tag.attrib[\"property\"]] = tag.attrib[\"content\"]\n\n    # TODO: grab article: meta tags too, e.g.:\n\n    # \"article:publisher\" : \"https://www.facebook.com/thethudonline\" />\n    # \"article:author\" content=\"https://www.facebook.com/thethudonline\" />\n    # \"article:tag\" content=\"baby\" />\n    # \"article:section\" content=\"Breaking News\" />\n    # \"article:published_time\" content=\"2016-03-31T19:58:24+00:00\" />\n    # \"article:modified_time\" content=\"2016-04-01T18:31:53+00:00\" />\n\n    if \"og:title\" not in og:\n        # Attempt to find a title from the title tag, or the biggest header on the page.\n        title = tree.xpath(\"((//title)[1] | (//h1)[1] | (//h2)[1] | (//h3)[1])/text()\")\n        if title:\n            og[\"og:title\"] = title[0].strip()\n        else:\n            og[\"og:title\"] = None\n\n    if \"og:image\" not in og:\n        meta_image = tree.xpath(\n            \"//*/meta[translate(@itemprop, 'IMAGE', 'image')='image'][not(@content='')]/@content[1]\"\n        )\n        # If a meta image is found, use it.\n        if meta_image:\n            og[\"og:image\"] = meta_image[0]\n        else:\n            # Try to find images which are larger than 10px by 10px.\n            #\n            # TODO: consider inlined CSS styles as well as width & height attribs\n            images = tree.xpath(\"//img[@src][number(@width)>10][number(@height)>10]\")\n            images = sorted(\n                images,\n                key=lambda i: (\n                    -1 * float(i.attrib[\"width\"]) * float(i.attrib[\"height\"])\n                ),\n            )\n            # If no images were found, try to find *any* images.\n            if not images:\n                images = tree.xpath(\"//img[@src][1]\")\n            if images:\n                og[\"og:image\"] = images[0].attrib[\"src\"]\n\n            # Finally, fallback to the favicon if nothing else.\n            else:\n                favicons = tree.xpath(\"//link[@href][contains(@rel, 'icon')]/@href[1]\")\n                if favicons:\n                    og[\"og:image\"] = favicons[0]\n\n    if \"og:description\" not in og:\n        # Check the first meta description tag for content.\n        meta_description = tree.xpath(\n            \"//*/meta[translate(@name, 'DESCRIPTION', 'description')='description'][not(@content='')]/@content[1]\"\n        )\n        # If a meta description is found with content, use it.\n        if meta_description:\n            og[\"og:description\"] = meta_description[0]\n        else:\n            og[\"og:description\"] = parse_html_description(tree)\n    elif og[\"og:description\"]:\n        # This must be a non-empty string at this point.\n        assert isinstance(og[\"og:description\"], str)\n        og[\"og:description\"] = summarize_paragraphs([og[\"og:description\"]])\n\n    # TODO: delete the url downloads to stop diskfilling,\n    # as we only ever cared about its OG\n    return og\n\n\ndef parse_html_description(tree: \"etree.Element\") -> Optional[str]:\n    \"\"\"\n    Calculate a text description based on an HTML document.\n\n    Grabs any text nodes which are inside the <body/> tag, unless they are within\n    an HTML5 semantic markup tag (<header/>, <nav/>, <aside/>, <footer/>), or\n    if they are within a <script/>, <svg/> or <style/> tag, or if they are within\n    a tag whose content is usually only shown to old browsers\n    (<iframe/>, <video/>, <canvas/>, <picture/>).\n\n    This is a very very very coarse approximation to a plain text render of the page.\n\n    Args:\n        tree: The parsed HTML document.\n\n    Returns:\n        The plain text description, or None if one cannot be generated.\n    \"\"\"\n    # We don't just use XPATH here as that is slow on some machines.\n\n    from lxml import etree\n\n    TAGS_TO_REMOVE = {\n        \"header\",\n        \"nav\",\n        \"aside\",\n        \"footer\",\n        \"script\",\n        \"noscript\",\n        \"style\",\n        \"svg\",\n        \"iframe\",\n        \"video\",\n        \"canvas\",\n        \"img\",\n        \"picture\",\n        etree.Comment,\n    }\n\n    # Split all the text nodes into paragraphs (by splitting on new\n    # lines)\n    text_nodes = (\n        re.sub(r\"\\s+\", \"\\n\", el).strip()\n        for el in _iterate_over_text(tree.find(\"body\"), TAGS_TO_REMOVE)\n    )\n    return summarize_paragraphs(text_nodes)\n\n\ndef _iterate_over_text(\n    tree: Optional[\"etree.Element\"],\n    tags_to_ignore: Set[Union[str, \"etree.Comment\"]],\n    stack_limit: int = 1024,\n) -> Generator[str, None, None]:\n    \"\"\"Iterate over the tree returning text nodes in a depth first fashion,\n    skipping text nodes inside certain tags.\n\n    Args:\n        tree: The parent element to iterate. Can be None if there isn't one.\n        tags_to_ignore: Set of tags to ignore\n        stack_limit: Maximum stack size limit for depth-first traversal.\n            Nodes will be dropped if this limit is hit, which may truncate the\n            textual result.\n            Intended to limit the maximum working memory when generating a preview.\n    \"\"\"\n\n    if tree is None:\n        return\n\n    # This is a stack whose items are elements to iterate over *or* strings\n    # to be returned.\n    elements: List[Union[str, \"etree.Element\"]] = [tree]\n    while elements:\n        el = elements.pop()\n\n        if isinstance(el, str):\n            yield el\n        elif el.tag not in tags_to_ignore:\n            # If the element isn't meant for display, ignore it.\n            if el.get(\"role\") in ARIA_ROLES_TO_IGNORE:\n                continue\n\n            # el.text is the text before the first child, so we can immediately\n            # return it if the text exists.\n            if el.text:\n                yield el.text\n\n            # We add to the stack all the element's children, interspersed with\n            # each child's tail text (if it exists).\n            #\n            # We iterate in reverse order so that earlier pieces of text appear\n            # closer to the top of the stack.\n            for child in el.iterchildren(reversed=True):\n                if len(elements) > stack_limit:\n                    # We've hit our limit for working memory\n                    break\n\n                if child.tail:\n                    # The tail text of a node is text that comes *after* the node,\n                    # so we always include it even if we ignore the child node.\n                    elements.append(child.tail)\n\n                elements.append(child)\n\n\ndef summarize_paragraphs(\n    text_nodes: Iterable[str], min_size: int = 200, max_size: int = 500\n) -> Optional[str]:\n    \"\"\"\n    Try to get a summary respecting first paragraph and then word boundaries.\n\n    Args:\n        text_nodes: The paragraphs to summarize.\n        min_size: The minimum number of words to include.\n        max_size: The maximum number of words to include.\n\n    Returns:\n        A summary of the text nodes, or None if that was not possible.\n    \"\"\"\n\n    # TODO: Respect sentences?\n\n    description = \"\"\n\n    # Keep adding paragraphs until we get to the MIN_SIZE.\n    for text_node in text_nodes:\n        if len(description) < min_size:\n            text_node = re.sub(r\"[\\t \\r\\n]+\", \" \", text_node)\n            description += text_node + \"\\n\\n\"\n        else:\n            break\n\n    description = description.strip()\n    description = re.sub(r\"[\\t ]+\", \" \", description)\n    description = re.sub(r\"[\\t \\r\\n]*[\\r\\n]+\", \"\\n\\n\", description)\n\n    # If the concatenation of paragraphs to get above MIN_SIZE\n    # took us over MAX_SIZE, then we need to truncate mid paragraph\n    if len(description) > max_size:\n        new_desc = \"\"\n\n        # This splits the paragraph into words, but keeping the\n        # (preceding) whitespace intact so we can easily concat\n        # words back together.\n        for match in re.finditer(r\"\\s*\\S+\", description):\n            word = match.group()\n\n            # Keep adding words while the total length is less than\n            # MAX_SIZE.\n            if len(word) + len(new_desc) < max_size:\n                new_desc += word\n            else:\n                # At this point the next word *will* take us over\n                # MAX_SIZE, but we also want to ensure that its not\n                # a huge word. If it is add it anyway and we'll\n                # truncate later.\n                if len(new_desc) < min_size:\n                    new_desc += word\n                break\n\n        # Double check that we're not over the limit\n        if len(new_desc) > max_size:\n            new_desc = new_desc[:max_size]\n\n        # We always add an ellipsis because at the very least\n        # we chopped mid paragraph.\n        description = new_desc.strip() + \"\u2026\"\n    return description if description else None\n", "# Copyright 2014-2016 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom synapse.rest.media.v1.preview_html import (\n    _get_html_media_encodings,\n    decode_body,\n    parse_html_to_open_graph,\n    summarize_paragraphs,\n)\n\nfrom tests import unittest\n\ntry:\n    import lxml\nexcept ImportError:\n    lxml = None\n\n\nclass SummarizeTestCase(unittest.TestCase):\n    if not lxml:\n        skip = \"url preview feature requires lxml\"\n\n    def test_long_summarize(self) -> None:\n        example_paras = [\n            \"\"\"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\n            Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\n            Troms county, Norway. The administrative centre of the municipality is\n            the city of Troms\u00f8. Outside of Norway, Tromso and Troms\u00f6 are\n            alternative spellings of the city.Troms\u00f8 is considered the northernmost\n            city in the world with a population above 50,000. The most populous town\n            north of it is Alta, Norway, with a population of 14,272 (2013).\"\"\",\n            \"\"\"Troms\u00f8 lies in Northern Norway. The municipality has a population of\n            (2015) 72,066, but with an annual influx of students it has over 75,000\n            most of the year. It is the largest urban area in Northern Norway and the\n            third largest north of the Arctic Circle (following Murmansk and Norilsk).\n            Most of Troms\u00f8, including the city centre, is located on the island of\n            Troms\u00f8ya, 350 kilometres (217 mi) north of the Arctic Circle. In 2012,\n            Troms\u00f8ya had a population of 36,088. Substantial parts of the urban area\n            are also situated on the mainland to the east, and on parts of Kval\u00f8ya\u2014a\n            large island to the west. Troms\u00f8ya is connected to the mainland by the Troms\u00f8\n            Bridge and the Troms\u00f8ysund Tunnel, and to the island of Kval\u00f8ya by the\n            Sandnessund Bridge. Troms\u00f8 Airport connects the city to many destinations\n            in Europe. The city is warmer than most other places located on the same\n            latitude, due to the warming effect of the Gulf Stream.\"\"\",\n            \"\"\"The city centre of Troms\u00f8 contains the highest number of old wooden\n            houses in Northern Norway, the oldest house dating from 1789. The Arctic\n            Cathedral, a modern church from 1965, is probably the most famous landmark\n            in Troms\u00f8. The city is a cultural centre for its region, with several\n            festivals taking place in the summer. Some of Norway's best-known\n             musicians, Torbj\u00f8rn Brundtland and Svein Berge of the electronica duo\n             R\u00f6yksopp and Lene Marlin grew up and started their careers in Troms\u00f8.\n             Noted electronic musician Geir Jenssen also hails from Troms\u00f8.\"\"\",\n        ]\n\n        desc = summarize_paragraphs(example_paras, min_size=200, max_size=500)\n\n        self.assertEqual(\n            desc,\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway. The administrative centre of the municipality is\"\n            \" the city of Troms\u00f8. Outside of Norway, Tromso and Troms\u00f6 are\"\n            \" alternative spellings of the city.Troms\u00f8 is considered the northernmost\"\n            \" city in the world with a population above 50,000. The most populous town\"\n            \" north of it is Alta, Norway, with a population of 14,272 (2013).\",\n        )\n\n        desc = summarize_paragraphs(example_paras[1:], min_size=200, max_size=500)\n\n        self.assertEqual(\n            desc,\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year. It is the largest urban area in Northern Norway and the\"\n            \" third largest north of the Arctic Circle (following Murmansk and Norilsk).\"\n            \" Most of Troms\u00f8, including the city centre, is located on the island of\"\n            \" Troms\u00f8ya, 350 kilometres (217 mi) north of the Arctic Circle. In 2012,\"\n            \" Troms\u00f8ya had a population of 36,088. Substantial parts of the urban\u2026\",\n        )\n\n    def test_short_summarize(self) -> None:\n        example_paras = [\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway.\",\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year.\",\n            \"The city centre of Troms\u00f8 contains the highest number of old wooden\"\n            \" houses in Northern Norway, the oldest house dating from 1789. The Arctic\"\n            \" Cathedral, a modern church from 1965, is probably the most famous landmark\"\n            \" in Troms\u00f8.\",\n        ]\n\n        desc = summarize_paragraphs(example_paras, min_size=200, max_size=500)\n\n        self.assertEqual(\n            desc,\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway.\\n\"\n            \"\\n\"\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year.\",\n        )\n\n    def test_small_then_large_summarize(self) -> None:\n        example_paras = [\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway.\",\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year.\"\n            \" The city centre of Troms\u00f8 contains the highest number of old wooden\"\n            \" houses in Northern Norway, the oldest house dating from 1789. The Arctic\"\n            \" Cathedral, a modern church from 1965, is probably the most famous landmark\"\n            \" in Troms\u00f8.\",\n        ]\n\n        desc = summarize_paragraphs(example_paras, min_size=200, max_size=500)\n        self.assertEqual(\n            desc,\n            \"Troms\u00f8 (Norwegian pronunciation: [\u02c8tr\u028ams\u0153] ( listen); Northern Sami:\"\n            \" Romsa; Finnish: Tromssa[2] Kven: Tromssa) is a city and municipality in\"\n            \" Troms county, Norway.\\n\"\n            \"\\n\"\n            \"Troms\u00f8 lies in Northern Norway. The municipality has a population of\"\n            \" (2015) 72,066, but with an annual influx of students it has over 75,000\"\n            \" most of the year. The city centre of Troms\u00f8 contains the highest number\"\n            \" of old wooden houses in Northern Norway, the oldest house dating from\"\n            \" 1789. The Arctic Cathedral, a modern church from\u2026\",\n        )\n\n\nclass OpenGraphFromHtmlTestCase(unittest.TestCase):\n    if not lxml:\n        skip = \"url preview feature requires lxml\"\n\n    def test_simple(self) -> None:\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_comment(self) -> None:\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        <!-- HTML comment -->\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_comment2(self) -> None:\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        Some text.\n        <!-- HTML comment -->\n        Some more text.\n        <p>Text</p>\n        More text\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(\n            og,\n            {\n                \"og:title\": \"Foo\",\n                \"og:description\": \"Some text.\\n\\nSome more text.\\n\\nText\\n\\nMore text\",\n            },\n        )\n\n    def test_script(self) -> None:\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        <script> (function() {})() </script>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_missing_title(self) -> None:\n        html = b\"\"\"\n        <html>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": None, \"og:description\": \"Some text.\"})\n\n        # Another variant is a title with no content.\n        html = b\"\"\"\n        <html>\n        <head><title></title></head>\n        <body>\n        <h1>Title</h1>\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Title\", \"og:description\": \"Title\"})\n\n    def test_h1_as_title(self) -> None:\n        html = b\"\"\"\n        <html>\n        <meta property=\"og:description\" content=\"Some text.\"/>\n        <body>\n        <h1>Title</h1>\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Title\", \"og:description\": \"Some text.\"})\n\n    def test_empty_description(self) -> None:\n        \"\"\"Description tags with empty content should be ignored.\"\"\"\n        html = b\"\"\"\n        <html>\n        <meta property=\"og:description\" content=\"\"/>\n        <meta property=\"og:description\"/>\n        <meta name=\"description\" content=\"\"/>\n        <meta name=\"description\"/>\n        <meta name=\"description\" content=\"Finally!\"/>\n        <body>\n        <h1>Title</h1>\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": \"Title\", \"og:description\": \"Finally!\"})\n\n    def test_missing_title_and_broken_h1(self) -> None:\n        html = b\"\"\"\n        <html>\n        <body>\n        <h1><a href=\"foo\"/></h1>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n\n        self.assertEqual(og, {\"og:title\": None, \"og:description\": \"Some text.\"})\n\n    def test_empty(self) -> None:\n        \"\"\"Test a body with no data in it.\"\"\"\n        html = b\"\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        self.assertIsNone(tree)\n\n    def test_no_tree(self) -> None:\n        \"\"\"A valid body with no tree in it.\"\"\"\n        html = b\"\\x00\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        self.assertIsNone(tree)\n\n    def test_xml(self) -> None:\n        \"\"\"Test decoding XML and ensure it works properly.\"\"\"\n        # Note that the strip() call is important to ensure the xml tag starts\n        # at the initial byte.\n        html = b\"\"\"\n        <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n        <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n        <html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n        <head><title>Foo</title></head><body>Some text.</body></html>\n        \"\"\".strip()\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_invalid_encoding(self) -> None:\n        \"\"\"An invalid character encoding should be ignored and treated as UTF-8, if possible.\"\"\"\n        html = b\"\"\"\n        <html>\n        <head><title>Foo</title></head>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n        tree = decode_body(html, \"http://example.com/test.html\", \"invalid-encoding\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(og, {\"og:title\": \"Foo\", \"og:description\": \"Some text.\"})\n\n    def test_invalid_encoding2(self) -> None:\n        \"\"\"A body which doesn't match the sent character encoding.\"\"\"\n        # Note that this contains an invalid UTF-8 sequence in the title.\n        html = b\"\"\"\n        <html>\n        <head><title>\\xff\\xff Foo</title></head>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(og, {\"og:title\": \"\u00ff\u00ff Foo\", \"og:description\": \"Some text.\"})\n\n    def test_windows_1252(self) -> None:\n        \"\"\"A body which uses cp1252, but doesn't declare that.\"\"\"\n        html = b\"\"\"\n        <html>\n        <head><title>\\xf3</title></head>\n        <body>\n        Some text.\n        </body>\n        </html>\n        \"\"\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(og, {\"og:title\": \"\u00f3\", \"og:description\": \"Some text.\"})\n\n    def test_nested_nodes(self) -> None:\n        \"\"\"A body with some nested nodes. Tests that we iterate over children\n        in the right order (and don't reverse the order of the text).\"\"\"\n        html = b\"\"\"\n        <a href=\"somewhere\">Welcome <b>the bold <u>and underlined text <svg>\n        with a cheeky SVG</svg></u> and <strong>some</strong> tail text</b></a>\n        \"\"\"\n        tree = decode_body(html, \"http://example.com/test.html\")\n        og = parse_html_to_open_graph(tree)\n        self.assertEqual(\n            og,\n            {\n                \"og:title\": None,\n                \"og:description\": \"Welcome\\n\\nthe bold\\n\\nand underlined text\\n\\nand\\n\\nsome\\n\\ntail text\",\n            },\n        )\n\n\nclass MediaEncodingTestCase(unittest.TestCase):\n    def test_meta_charset(self) -> None:\n        \"\"\"A character encoding is found via the meta tag.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <html>\n        <head><meta charset=\"ascii\">\n        </head>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"ascii\", \"utf-8\", \"cp1252\"])\n\n        # A less well-formed version.\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <html>\n        <head>< meta charset = ascii>\n        </head>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"ascii\", \"utf-8\", \"cp1252\"])\n\n    def test_meta_charset_underscores(self) -> None:\n        \"\"\"A character encoding contains underscore.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <html>\n        <head><meta charset=\"Shift_JIS\">\n        </head>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"shift_jis\", \"utf-8\", \"cp1252\"])\n\n    def test_xml_encoding(self) -> None:\n        \"\"\"A character encoding is found via the meta tag.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <?xml version=\"1.0\" encoding=\"ascii\"?>\n        <html>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"ascii\", \"utf-8\", \"cp1252\"])\n\n    def test_meta_xml_encoding(self) -> None:\n        \"\"\"Meta tags take precedence over XML encoding.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <?xml version=\"1.0\" encoding=\"ascii\"?>\n        <html>\n        <head><meta charset=\"UTF-16\">\n        </head>\n        </html>\n        \"\"\",\n            \"text/html\",\n        )\n        self.assertEqual(list(encodings), [\"utf-16\", \"ascii\", \"utf-8\", \"cp1252\"])\n\n    def test_content_type(self) -> None:\n        \"\"\"A character encoding is found via the Content-Type header.\"\"\"\n        # Test a few variations of the header.\n        headers = (\n            'text/html; charset=\"ascii\";',\n            \"text/html;charset=ascii;\",\n            'text/html;  charset=\"ascii\"',\n            \"text/html; charset=ascii\",\n            'text/html; charset=\"ascii;',\n            'text/html; charset=ascii\";',\n        )\n        for header in headers:\n            encodings = _get_html_media_encodings(b\"\", header)\n            self.assertEqual(list(encodings), [\"ascii\", \"utf-8\", \"cp1252\"])\n\n    def test_fallback(self) -> None:\n        \"\"\"A character encoding cannot be found in the body or header.\"\"\"\n        encodings = _get_html_media_encodings(b\"\", \"text/html\")\n        self.assertEqual(list(encodings), [\"utf-8\", \"cp1252\"])\n\n    def test_duplicates(self) -> None:\n        \"\"\"Ensure each encoding is only attempted once.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <?xml version=\"1.0\" encoding=\"utf8\"?>\n        <html>\n        <head><meta charset=\"UTF-8\">\n        </head>\n        </html>\n        \"\"\",\n            'text/html; charset=\"UTF_8\"',\n        )\n        self.assertEqual(list(encodings), [\"utf-8\", \"cp1252\"])\n\n    def test_unknown_invalid(self) -> None:\n        \"\"\"A character encoding should be ignored if it is unknown or invalid.\"\"\"\n        encodings = _get_html_media_encodings(\n            b\"\"\"\n        <html>\n        <head><meta charset=\"invalid\">\n        </head>\n        </html>\n        \"\"\",\n            'text/html; charset=\"invalid\"',\n        )\n        self.assertEqual(list(encodings), [\"utf-8\", \"cp1252\"])\n"], "filenames": ["synapse/rest/media/v1/preview_html.py", "tests/rest/media/v1/test_html_preview.py"], "buggy_code_start_loc": [15, 372], "buggy_code_end_loc": [343, 372], "fixing_code_start_loc": [14, 373], "fixing_code_end_loc": [358, 390], "type": "CWE-674", "message": "Synapse is an open source home server implementation for the Matrix chat network. In versions prior to 1.61.1 URL previews of some web pages can exhaust the available stack space for the Synapse process due to unbounded recursion. This is sometimes recoverable and leads to an error for the request causing the problem, but in other cases the Synapse process may crash altogether. It is possible to exploit this maliciously, either by malicious users on the homeserver, or by remote users sending URLs that a local user's client may automatically request a URL preview for. Remote users are not able to exploit this directly, because the URL preview endpoint is authenticated. Deployments with `url_preview_enabled: false` set in configuration are not affected. Deployments with `url_preview_enabled: true` set in configuration **are** affected. Deployments with no configuration value set for `url_preview_enabled` are not affected, because the default is `false`. Administrators of homeservers with URL previews enabled are advised to upgrade to v1.61.1 or higher. Users unable to upgrade should set `url_preview_enabled` to false.", "other": {"cve": {"id": "CVE-2022-31052", "sourceIdentifier": "security-advisories@github.com", "published": "2022-06-28T17:15:08.217", "lastModified": "2022-07-09T00:15:29.740", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Synapse is an open source home server implementation for the Matrix chat network. In versions prior to 1.61.1 URL previews of some web pages can exhaust the available stack space for the Synapse process due to unbounded recursion. This is sometimes recoverable and leads to an error for the request causing the problem, but in other cases the Synapse process may crash altogether. It is possible to exploit this maliciously, either by malicious users on the homeserver, or by remote users sending URLs that a local user's client may automatically request a URL preview for. Remote users are not able to exploit this directly, because the URL preview endpoint is authenticated. Deployments with `url_preview_enabled: false` set in configuration are not affected. Deployments with `url_preview_enabled: true` set in configuration **are** affected. Deployments with no configuration value set for `url_preview_enabled` are not affected, because the default is `false`. Administrators of homeservers with URL previews enabled are advised to upgrade to v1.61.1 or higher. Users unable to upgrade should set `url_preview_enabled` to false."}, {"lang": "es", "value": "Synapse es una implementaci\u00f3n de servidor dom\u00e9stico de c\u00f3digo abierto para la red de chat Matrix. En versiones anteriores a 1.61.1, las previsualizaciones de URL de algunas p\u00e1ginas web pueden agotar el espacio de pila disponible para el proceso de Synapse debido a una recursi\u00f3n sin l\u00edmites. Esto a veces es recuperable y conlleva a un error para la petici\u00f3n que causa el problema, pero en otros casos el proceso de Synapse puede bloquearse por completo. Es posible explotar esto de forma maliciosa, ya sea por usuarios maliciosos en el servidor dom\u00e9stico, o por usuarios remotos que env\u00edan URLs para las que el cliente de un usuario local puede solicitar autom\u00e1ticamente una visualizaci\u00f3n previa de la URL. Los usuarios remotos no son capaces de explotar esto directamente, porque el  endpoint de la visualizaci\u00f3n previa de la URL est\u00e1 autenticado. Los despliegues con \"url_preview_enabled: false\" establecidos en la configuraci\u00f3n no est\u00e1n afectados. Los despliegues con \"url_preview_enabled: true\" establecido en la configuraci\u00f3n **est\u00e1n** afectados. Los despliegues sin valor de configuraci\u00f3n para \"url_preview_enabled\" no est\u00e1n afectados, porque el valor por defecto es \"false\". Es recomendado a administradores de servidores dom\u00e9sticos con visualizaciones previas de URL habilitadas que actualicen a versi\u00f3n v1.61.1 o superior. Los usuarios que no puedan actualizarse deben establecer \"url_preview_enabled\" en false"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:S/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 3.5}, "baseSeverity": "LOW", "exploitabilityScore": 6.8, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-674"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:matrix:synapse:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.61.1", "matchCriteriaId": "52787569-D73B-46A3-99E8-EEB0D5971C69"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:35:*:*:*:*:*:*:*", "matchCriteriaId": "80E516C0-98A4-4ADE-B69F-66A772E2BAAA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:36:*:*:*:*:*:*:*", "matchCriteriaId": "5C675112-476C-4D7C-BCB9-A2FB2D0BC9FD"}]}]}], "references": [{"url": "https://github.com/matrix-org/synapse/commit/fa1308061802ac7b7d20e954ba7372c5ac292333", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/matrix-org/synapse/security/advisories/GHSA-22p3-qrh9-cx32", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/7EARKKJZ2W7WUITFDT4EG4NVATFYJQHF/", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/QGSDQ4YAITCUACAB7SXQZDJIU3IQ4CJD/", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://spec.matrix.org/v1.2/client-server-api/#get_matrixmediav3preview_url", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/matrix-org/synapse/commit/fa1308061802ac7b7d20e954ba7372c5ac292333"}}