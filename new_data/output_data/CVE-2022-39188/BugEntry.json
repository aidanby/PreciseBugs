{"buggy_code": ["/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* include/asm-generic/tlb.h\n *\n *\tGeneric TLB shootdown code\n *\n * Copyright 2001 Red Hat, Inc.\n * Based on code from mm/memory.c Copyright Linus Torvalds and others.\n *\n * Copyright 2011 Red Hat, Inc., Peter Zijlstra\n */\n#ifndef _ASM_GENERIC__TLB_H\n#define _ASM_GENERIC__TLB_H\n\n#include <linux/mmu_notifier.h>\n#include <linux/swap.h>\n#include <linux/hugetlb_inline.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n\n/*\n * Blindly accessing user memory from NMI context can be dangerous\n * if we're in the middle of switching the current user task or switching\n * the loaded mm.\n */\n#ifndef nmi_uaccess_okay\n# define nmi_uaccess_okay() true\n#endif\n\n#ifdef CONFIG_MMU\n\n/*\n * Generic MMU-gather implementation.\n *\n * The mmu_gather data structure is used by the mm code to implement the\n * correct and efficient ordering of freeing pages and TLB invalidations.\n *\n * This correct ordering is:\n *\n *  1) unhook page\n *  2) TLB invalidate page\n *  3) free page\n *\n * That is, we must never free a page before we have ensured there are no live\n * translations left to it. Otherwise it might be possible to observe (or\n * worse, change) the page content after it has been reused.\n *\n * The mmu_gather API consists of:\n *\n *  - tlb_gather_mmu() / tlb_gather_mmu_fullmm() / tlb_finish_mmu()\n *\n *    start and finish a mmu_gather\n *\n *    Finish in particular will issue a (final) TLB invalidate and free\n *    all (remaining) queued pages.\n *\n *  - tlb_start_vma() / tlb_end_vma(); marks the start / end of a VMA\n *\n *    Defaults to flushing at tlb_end_vma() to reset the range; helps when\n *    there's large holes between the VMAs.\n *\n *  - tlb_remove_table()\n *\n *    tlb_remove_table() is the basic primitive to free page-table directories\n *    (__p*_free_tlb()).  In it's most primitive form it is an alias for\n *    tlb_remove_page() below, for when page directories are pages and have no\n *    additional constraints.\n *\n *    See also MMU_GATHER_TABLE_FREE and MMU_GATHER_RCU_TABLE_FREE.\n *\n *  - tlb_remove_page() / __tlb_remove_page()\n *  - tlb_remove_page_size() / __tlb_remove_page_size()\n *\n *    __tlb_remove_page_size() is the basic primitive that queues a page for\n *    freeing. __tlb_remove_page() assumes PAGE_SIZE. Both will return a\n *    boolean indicating if the queue is (now) full and a call to\n *    tlb_flush_mmu() is required.\n *\n *    tlb_remove_page() and tlb_remove_page_size() imply the call to\n *    tlb_flush_mmu() when required and has no return value.\n *\n *  - tlb_change_page_size()\n *\n *    call before __tlb_remove_page*() to set the current page-size; implies a\n *    possible tlb_flush_mmu() call.\n *\n *  - tlb_flush_mmu() / tlb_flush_mmu_tlbonly()\n *\n *    tlb_flush_mmu_tlbonly() - does the TLB invalidate (and resets\n *                              related state, like the range)\n *\n *    tlb_flush_mmu() - in addition to the above TLB invalidate, also frees\n *\t\t\twhatever pages are still batched.\n *\n *  - mmu_gather::fullmm\n *\n *    A flag set by tlb_gather_mmu_fullmm() to indicate we're going to free\n *    the entire mm; this allows a number of optimizations.\n *\n *    - We can ignore tlb_{start,end}_vma(); because we don't\n *      care about ranges. Everything will be shot down.\n *\n *    - (RISC) architectures that use ASIDs can cycle to a new ASID\n *      and delay the invalidation until ASID space runs out.\n *\n *  - mmu_gather::need_flush_all\n *\n *    A flag that can be set by the arch code if it wants to force\n *    flush the entire TLB irrespective of the range. For instance\n *    x86-PAE needs this when changing top-level entries.\n *\n * And allows the architecture to provide and implement tlb_flush():\n *\n * tlb_flush() may, in addition to the above mentioned mmu_gather fields, make\n * use of:\n *\n *  - mmu_gather::start / mmu_gather::end\n *\n *    which provides the range that needs to be flushed to cover the pages to\n *    be freed.\n *\n *  - mmu_gather::freed_tables\n *\n *    set when we freed page table pages\n *\n *  - tlb_get_unmap_shift() / tlb_get_unmap_size()\n *\n *    returns the smallest TLB entry size unmapped in this range.\n *\n * If an architecture does not provide tlb_flush() a default implementation\n * based on flush_tlb_range() will be used, unless MMU_GATHER_NO_RANGE is\n * specified, in which case we'll default to flush_tlb_mm().\n *\n * Additionally there are a few opt-in features:\n *\n *  MMU_GATHER_PAGE_SIZE\n *\n *  This ensures we call tlb_flush() every time tlb_change_page_size() actually\n *  changes the size and provides mmu_gather::page_size to tlb_flush().\n *\n *  This might be useful if your architecture has size specific TLB\n *  invalidation instructions.\n *\n *  MMU_GATHER_TABLE_FREE\n *\n *  This provides tlb_remove_table(), to be used instead of tlb_remove_page()\n *  for page directores (__p*_free_tlb()).\n *\n *  Useful if your architecture has non-page page directories.\n *\n *  When used, an architecture is expected to provide __tlb_remove_table()\n *  which does the actual freeing of these pages.\n *\n *  MMU_GATHER_RCU_TABLE_FREE\n *\n *  Like MMU_GATHER_TABLE_FREE, and adds semi-RCU semantics to the free (see\n *  comment below).\n *\n *  Useful if your architecture doesn't use IPIs for remote TLB invalidates\n *  and therefore doesn't naturally serialize with software page-table walkers.\n *\n *  MMU_GATHER_NO_FLUSH_CACHE\n *\n *  Indicates the architecture has flush_cache_range() but it needs *NOT* be called\n *  before unmapping a VMA.\n *\n *  NOTE: strictly speaking we shouldn't have this knob and instead rely on\n *\t  flush_cache_range() being a NOP, except Sparc64 seems to be\n *\t  different here.\n *\n *  MMU_GATHER_MERGE_VMAS\n *\n *  Indicates the architecture wants to merge ranges over VMAs; typical when\n *  multiple range invalidates are more expensive than a full invalidate.\n *\n *  MMU_GATHER_NO_RANGE\n *\n *  Use this if your architecture lacks an efficient flush_tlb_range(). This\n *  option implies MMU_GATHER_MERGE_VMAS above.\n *\n *  MMU_GATHER_NO_GATHER\n *\n *  If the option is set the mmu_gather will not track individual pages for\n *  delayed page free anymore. A platform that enables the option needs to\n *  provide its own implementation of the __tlb_remove_page_size() function to\n *  free pages.\n *\n *  This is useful if your architecture already flushes TLB entries in the\n *  various ptep_get_and_clear() functions.\n */\n\n#ifdef CONFIG_MMU_GATHER_TABLE_FREE\n\nstruct mmu_table_batch {\n#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE\n\tstruct rcu_head\t\trcu;\n#endif\n\tunsigned int\t\tnr;\n\tvoid\t\t\t*tables[];\n};\n\n#define MAX_TABLE_BATCH\t\t\\\n\t((PAGE_SIZE - sizeof(struct mmu_table_batch)) / sizeof(void *))\n\nextern void tlb_remove_table(struct mmu_gather *tlb, void *table);\n\n#else /* !CONFIG_MMU_GATHER_HAVE_TABLE_FREE */\n\n/*\n * Without MMU_GATHER_TABLE_FREE the architecture is assumed to have page based\n * page directories and we can use the normal page batching to free them.\n */\n#define tlb_remove_table(tlb, page) tlb_remove_page((tlb), (page))\n\n#endif /* CONFIG_MMU_GATHER_TABLE_FREE */\n\n#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE\n/*\n * This allows an architecture that does not use the linux page-tables for\n * hardware to skip the TLBI when freeing page tables.\n */\n#ifndef tlb_needs_table_invalidate\n#define tlb_needs_table_invalidate() (true)\n#endif\n\n#else\n\n#ifdef tlb_needs_table_invalidate\n#error tlb_needs_table_invalidate() requires MMU_GATHER_RCU_TABLE_FREE\n#endif\n\n#endif /* CONFIG_MMU_GATHER_RCU_TABLE_FREE */\n\n\n#ifndef CONFIG_MMU_GATHER_NO_GATHER\n/*\n * If we can't allocate a page to make a big batch of page pointers\n * to work on, then just handle a few from the on-stack structure.\n */\n#define MMU_GATHER_BUNDLE\t8\n\nstruct mmu_gather_batch {\n\tstruct mmu_gather_batch\t*next;\n\tunsigned int\t\tnr;\n\tunsigned int\t\tmax;\n\tstruct page\t\t*pages[];\n};\n\n#define MAX_GATHER_BATCH\t\\\n\t((PAGE_SIZE - sizeof(struct mmu_gather_batch)) / sizeof(void *))\n\n/*\n * Limit the maximum number of mmu_gather batches to reduce a risk of soft\n * lockups for non-preemptible kernels on huge machines when a lot of memory\n * is zapped during unmapping.\n * 10K pages freed at once should be safe even without a preemption point.\n */\n#define MAX_GATHER_BATCH_COUNT\t(10000UL/MAX_GATHER_BATCH)\n\nextern bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,\n\t\t\t\t   int page_size);\n#endif\n\n/*\n * struct mmu_gather is an opaque type used by the mm code for passing around\n * any data needed by arch specific code for tlb_remove_page.\n */\nstruct mmu_gather {\n\tstruct mm_struct\t*mm;\n\n#ifdef CONFIG_MMU_GATHER_TABLE_FREE\n\tstruct mmu_table_batch\t*batch;\n#endif\n\n\tunsigned long\t\tstart;\n\tunsigned long\t\tend;\n\t/*\n\t * we are in the middle of an operation to clear\n\t * a full mm and can make some optimizations\n\t */\n\tunsigned int\t\tfullmm : 1;\n\n\t/*\n\t * we have performed an operation which\n\t * requires a complete flush of the tlb\n\t */\n\tunsigned int\t\tneed_flush_all : 1;\n\n\t/*\n\t * we have removed page directories\n\t */\n\tunsigned int\t\tfreed_tables : 1;\n\n\t/*\n\t * at which levels have we cleared entries?\n\t */\n\tunsigned int\t\tcleared_ptes : 1;\n\tunsigned int\t\tcleared_pmds : 1;\n\tunsigned int\t\tcleared_puds : 1;\n\tunsigned int\t\tcleared_p4ds : 1;\n\n\t/*\n\t * tracks VM_EXEC | VM_HUGETLB in tlb_start_vma\n\t */\n\tunsigned int\t\tvma_exec : 1;\n\tunsigned int\t\tvma_huge : 1;\n\n\tunsigned int\t\tbatch_count;\n\n#ifndef CONFIG_MMU_GATHER_NO_GATHER\n\tstruct mmu_gather_batch *active;\n\tstruct mmu_gather_batch\tlocal;\n\tstruct page\t\t*__pages[MMU_GATHER_BUNDLE];\n\n#ifdef CONFIG_MMU_GATHER_PAGE_SIZE\n\tunsigned int page_size;\n#endif\n#endif\n};\n\nvoid tlb_flush_mmu(struct mmu_gather *tlb);\n\nstatic inline void __tlb_adjust_range(struct mmu_gather *tlb,\n\t\t\t\t      unsigned long address,\n\t\t\t\t      unsigned int range_size)\n{\n\ttlb->start = min(tlb->start, address);\n\ttlb->end = max(tlb->end, address + range_size);\n}\n\nstatic inline void __tlb_reset_range(struct mmu_gather *tlb)\n{\n\tif (tlb->fullmm) {\n\t\ttlb->start = tlb->end = ~0;\n\t} else {\n\t\ttlb->start = TASK_SIZE;\n\t\ttlb->end = 0;\n\t}\n\ttlb->freed_tables = 0;\n\ttlb->cleared_ptes = 0;\n\ttlb->cleared_pmds = 0;\n\ttlb->cleared_puds = 0;\n\ttlb->cleared_p4ds = 0;\n\t/*\n\t * Do not reset mmu_gather::vma_* fields here, we do not\n\t * call into tlb_start_vma() again to set them if there is an\n\t * intermediate flush.\n\t */\n}\n\n#ifdef CONFIG_MMU_GATHER_NO_RANGE\n\n#if defined(tlb_flush)\n#error MMU_GATHER_NO_RANGE relies on default tlb_flush()\n#endif\n\n/*\n * When an architecture does not have efficient means of range flushing TLBs\n * there is no point in doing intermediate flushes on tlb_end_vma() to keep the\n * range small. We equally don't have to worry about page granularity or other\n * things.\n *\n * All we need to do is issue a full flush for any !0 range.\n */\nstatic inline void tlb_flush(struct mmu_gather *tlb)\n{\n\tif (tlb->end)\n\t\tflush_tlb_mm(tlb->mm);\n}\n\nstatic inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma) { }\n\n#else /* CONFIG_MMU_GATHER_NO_RANGE */\n\n#ifndef tlb_flush\n\n/*\n * When an architecture does not provide its own tlb_flush() implementation\n * but does have a reasonably efficient flush_vma_range() implementation\n * use that.\n */\nstatic inline void tlb_flush(struct mmu_gather *tlb)\n{\n\tif (tlb->fullmm || tlb->need_flush_all) {\n\t\tflush_tlb_mm(tlb->mm);\n\t} else if (tlb->end) {\n\t\tstruct vm_area_struct vma = {\n\t\t\t.vm_mm = tlb->mm,\n\t\t\t.vm_flags = (tlb->vma_exec ? VM_EXEC    : 0) |\n\t\t\t\t    (tlb->vma_huge ? VM_HUGETLB : 0),\n\t\t};\n\n\t\tflush_tlb_range(&vma, tlb->start, tlb->end);\n\t}\n}\n\nstatic inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\t/*\n\t * flush_tlb_range() implementations that look at VM_HUGETLB (tile,\n\t * mips-4k) flush only large pages.\n\t *\n\t * flush_tlb_range() implementations that flush I-TLB also flush D-TLB\n\t * (tile, xtensa, arm), so it's ok to just add VM_EXEC to an existing\n\t * range.\n\t *\n\t * We rely on tlb_end_vma() to issue a flush, such that when we reset\n\t * these values the batch is empty.\n\t */\n\ttlb->vma_huge = is_vm_hugetlb_page(vma);\n\ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);\n}\n\n#else\n\nstatic inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma) { }\n\n#endif\n\n#endif /* CONFIG_MMU_GATHER_NO_RANGE */\n\nstatic inline void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)\n{\n\t/*\n\t * Anything calling __tlb_adjust_range() also sets at least one of\n\t * these bits.\n\t */\n\tif (!(tlb->freed_tables || tlb->cleared_ptes || tlb->cleared_pmds ||\n\t      tlb->cleared_puds || tlb->cleared_p4ds))\n\t\treturn;\n\n\ttlb_flush(tlb);\n\tmmu_notifier_invalidate_range(tlb->mm, tlb->start, tlb->end);\n\t__tlb_reset_range(tlb);\n}\n\nstatic inline void tlb_remove_page_size(struct mmu_gather *tlb,\n\t\t\t\t\tstruct page *page, int page_size)\n{\n\tif (__tlb_remove_page_size(tlb, page, page_size))\n\t\ttlb_flush_mmu(tlb);\n}\n\nstatic inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)\n{\n\treturn __tlb_remove_page_size(tlb, page, PAGE_SIZE);\n}\n\n/* tlb_remove_page\n *\tSimilar to __tlb_remove_page but will call tlb_flush_mmu() itself when\n *\trequired.\n */\nstatic inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)\n{\n\treturn tlb_remove_page_size(tlb, page, PAGE_SIZE);\n}\n\nstatic inline void tlb_change_page_size(struct mmu_gather *tlb,\n\t\t\t\t\t\t     unsigned int page_size)\n{\n#ifdef CONFIG_MMU_GATHER_PAGE_SIZE\n\tif (tlb->page_size && tlb->page_size != page_size) {\n\t\tif (!tlb->fullmm && !tlb->need_flush_all)\n\t\t\ttlb_flush_mmu(tlb);\n\t}\n\n\ttlb->page_size = page_size;\n#endif\n}\n\nstatic inline unsigned long tlb_get_unmap_shift(struct mmu_gather *tlb)\n{\n\tif (tlb->cleared_ptes)\n\t\treturn PAGE_SHIFT;\n\tif (tlb->cleared_pmds)\n\t\treturn PMD_SHIFT;\n\tif (tlb->cleared_puds)\n\t\treturn PUD_SHIFT;\n\tif (tlb->cleared_p4ds)\n\t\treturn P4D_SHIFT;\n\n\treturn PAGE_SHIFT;\n}\n\nstatic inline unsigned long tlb_get_unmap_size(struct mmu_gather *tlb)\n{\n\treturn 1UL << tlb_get_unmap_shift(tlb);\n}\n\n/*\n * In the case of tlb vma handling, we can optimise these away in the\n * case where we're doing a full MM flush.  When we're doing a munmap,\n * the vmas are adjusted to only cover the region to be torn down.\n */\nstatic inline void tlb_start_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (tlb->fullmm)\n\t\treturn;\n\n\ttlb_update_vma_flags(tlb, vma);\n#ifndef CONFIG_MMU_GATHER_NO_FLUSH_CACHE\n\tflush_cache_range(vma, vma->vm_start, vma->vm_end);\n#endif\n}\n\nstatic inline void tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (tlb->fullmm || IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS))\n\t\treturn;\n\n\t/*\n\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n\t * the ranges growing with the unused space between consecutive VMAs,\n\t * but also the mmu_gather::vma_* flags from tlb_start_vma() rely on\n\t * this.\n\t */\n\ttlb_flush_mmu_tlbonly(tlb);\n}\n\n/*\n * tlb_flush_{pte|pmd|pud|p4d}_range() adjust the tlb->start and tlb->end,\n * and set corresponding cleared_*.\n */\nstatic inline void tlb_flush_pte_range(struct mmu_gather *tlb,\n\t\t\t\t     unsigned long address, unsigned long size)\n{\n\t__tlb_adjust_range(tlb, address, size);\n\ttlb->cleared_ptes = 1;\n}\n\nstatic inline void tlb_flush_pmd_range(struct mmu_gather *tlb,\n\t\t\t\t     unsigned long address, unsigned long size)\n{\n\t__tlb_adjust_range(tlb, address, size);\n\ttlb->cleared_pmds = 1;\n}\n\nstatic inline void tlb_flush_pud_range(struct mmu_gather *tlb,\n\t\t\t\t     unsigned long address, unsigned long size)\n{\n\t__tlb_adjust_range(tlb, address, size);\n\ttlb->cleared_puds = 1;\n}\n\nstatic inline void tlb_flush_p4d_range(struct mmu_gather *tlb,\n\t\t\t\t     unsigned long address, unsigned long size)\n{\n\t__tlb_adjust_range(tlb, address, size);\n\ttlb->cleared_p4ds = 1;\n}\n\n#ifndef __tlb_remove_tlb_entry\n#define __tlb_remove_tlb_entry(tlb, ptep, address) do { } while (0)\n#endif\n\n/**\n * tlb_remove_tlb_entry - remember a pte unmapping for later tlb invalidation.\n *\n * Record the fact that pte's were really unmapped by updating the range,\n * so we can later optimise away the tlb invalidate.   This helps when\n * userspace is unmapping already-unmapped pages, which happens quite a lot.\n */\n#define tlb_remove_tlb_entry(tlb, ptep, address)\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pte_range(tlb, address, PAGE_SIZE);\t\\\n\t\t__tlb_remove_tlb_entry(tlb, ptep, address);\t\\\n\t} while (0)\n\n#define tlb_remove_huge_tlb_entry(h, tlb, ptep, address)\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tunsigned long _sz = huge_page_size(h);\t\t\\\n\t\tif (_sz >= P4D_SIZE)\t\t\t\t\\\n\t\t\ttlb_flush_p4d_range(tlb, address, _sz);\t\\\n\t\telse if (_sz >= PUD_SIZE)\t\t\t\\\n\t\t\ttlb_flush_pud_range(tlb, address, _sz);\t\\\n\t\telse if (_sz >= PMD_SIZE)\t\t\t\\\n\t\t\ttlb_flush_pmd_range(tlb, address, _sz);\t\\\n\t\telse\t\t\t\t\t\t\\\n\t\t\ttlb_flush_pte_range(tlb, address, _sz);\t\\\n\t\t__tlb_remove_tlb_entry(tlb, ptep, address);\t\\\n\t} while (0)\n\n/**\n * tlb_remove_pmd_tlb_entry - remember a pmd mapping for later tlb invalidation\n * This is a nop so far, because only x86 needs it.\n */\n#ifndef __tlb_remove_pmd_tlb_entry\n#define __tlb_remove_pmd_tlb_entry(tlb, pmdp, address) do {} while (0)\n#endif\n\n#define tlb_remove_pmd_tlb_entry(tlb, pmdp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pmd_range(tlb, address, HPAGE_PMD_SIZE);\t\\\n\t\t__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);\t\t\\\n\t} while (0)\n\n/**\n * tlb_remove_pud_tlb_entry - remember a pud mapping for later tlb\n * invalidation. This is a nop so far, because only x86 needs it.\n */\n#ifndef __tlb_remove_pud_tlb_entry\n#define __tlb_remove_pud_tlb_entry(tlb, pudp, address) do {} while (0)\n#endif\n\n#define tlb_remove_pud_tlb_entry(tlb, pudp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pud_range(tlb, address, HPAGE_PUD_SIZE);\t\\\n\t\t__tlb_remove_pud_tlb_entry(tlb, pudp, address);\t\t\\\n\t} while (0)\n\n/*\n * For things like page tables caches (ie caching addresses \"inside\" the\n * page tables, like x86 does), for legacy reasons, flushing an\n * individual page had better flush the page table caches behind it. This\n * is definitely how x86 works, for example. And if you have an\n * architected non-legacy page table cache (which I'm not aware of\n * anybody actually doing), you're going to have some architecturally\n * explicit flushing for that, likely *separate* from a regular TLB entry\n * flush, and thus you'd need more than just some range expansion..\n *\n * So if we ever find an architecture\n * that would want something that odd, I think it is up to that\n * architecture to do its own odd thing, not cause pain for others\n * http://lkml.kernel.org/r/CA+55aFzBggoXtNXQeng5d_mRoDnaMBE5Y+URs+PHR67nUpMtaw@mail.gmail.com\n *\n * For now w.r.t page table cache, mark the range_size as PAGE_SIZE\n */\n\n#ifndef pte_free_tlb\n#define pte_free_tlb(tlb, ptep, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pmd_range(tlb, address, PAGE_SIZE);\t\\\n\t\ttlb->freed_tables = 1;\t\t\t\t\\\n\t\t__pte_free_tlb(tlb, ptep, address);\t\t\\\n\t} while (0)\n#endif\n\n#ifndef pmd_free_tlb\n#define pmd_free_tlb(tlb, pmdp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pud_range(tlb, address, PAGE_SIZE);\t\\\n\t\ttlb->freed_tables = 1;\t\t\t\t\\\n\t\t__pmd_free_tlb(tlb, pmdp, address);\t\t\\\n\t} while (0)\n#endif\n\n#ifndef pud_free_tlb\n#define pud_free_tlb(tlb, pudp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttlb_flush_p4d_range(tlb, address, PAGE_SIZE);\t\\\n\t\ttlb->freed_tables = 1;\t\t\t\t\\\n\t\t__pud_free_tlb(tlb, pudp, address);\t\t\\\n\t} while (0)\n#endif\n\n#ifndef p4d_free_tlb\n#define p4d_free_tlb(tlb, pudp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\t__tlb_adjust_range(tlb, address, PAGE_SIZE);\t\\\n\t\ttlb->freed_tables = 1;\t\t\t\t\\\n\t\t__p4d_free_tlb(tlb, pudp, address);\t\t\\\n\t} while (0)\n#endif\n\n#ifndef pte_needs_flush\nstatic inline bool pte_needs_flush(pte_t oldpte, pte_t newpte)\n{\n\treturn true;\n}\n#endif\n\n#ifndef huge_pmd_needs_flush\nstatic inline bool huge_pmd_needs_flush(pmd_t oldpmd, pmd_t newpmd)\n{\n\treturn true;\n}\n#endif\n\n#endif /* CONFIG_MMU */\n\n#endif /* _ASM_GENERIC__TLB_H */\n"], "fixing_code": ["/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* include/asm-generic/tlb.h\n *\n *\tGeneric TLB shootdown code\n *\n * Copyright 2001 Red Hat, Inc.\n * Based on code from mm/memory.c Copyright Linus Torvalds and others.\n *\n * Copyright 2011 Red Hat, Inc., Peter Zijlstra\n */\n#ifndef _ASM_GENERIC__TLB_H\n#define _ASM_GENERIC__TLB_H\n\n#include <linux/mmu_notifier.h>\n#include <linux/swap.h>\n#include <linux/hugetlb_inline.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n\n/*\n * Blindly accessing user memory from NMI context can be dangerous\n * if we're in the middle of switching the current user task or switching\n * the loaded mm.\n */\n#ifndef nmi_uaccess_okay\n# define nmi_uaccess_okay() true\n#endif\n\n#ifdef CONFIG_MMU\n\n/*\n * Generic MMU-gather implementation.\n *\n * The mmu_gather data structure is used by the mm code to implement the\n * correct and efficient ordering of freeing pages and TLB invalidations.\n *\n * This correct ordering is:\n *\n *  1) unhook page\n *  2) TLB invalidate page\n *  3) free page\n *\n * That is, we must never free a page before we have ensured there are no live\n * translations left to it. Otherwise it might be possible to observe (or\n * worse, change) the page content after it has been reused.\n *\n * The mmu_gather API consists of:\n *\n *  - tlb_gather_mmu() / tlb_gather_mmu_fullmm() / tlb_finish_mmu()\n *\n *    start and finish a mmu_gather\n *\n *    Finish in particular will issue a (final) TLB invalidate and free\n *    all (remaining) queued pages.\n *\n *  - tlb_start_vma() / tlb_end_vma(); marks the start / end of a VMA\n *\n *    Defaults to flushing at tlb_end_vma() to reset the range; helps when\n *    there's large holes between the VMAs.\n *\n *  - tlb_remove_table()\n *\n *    tlb_remove_table() is the basic primitive to free page-table directories\n *    (__p*_free_tlb()).  In it's most primitive form it is an alias for\n *    tlb_remove_page() below, for when page directories are pages and have no\n *    additional constraints.\n *\n *    See also MMU_GATHER_TABLE_FREE and MMU_GATHER_RCU_TABLE_FREE.\n *\n *  - tlb_remove_page() / __tlb_remove_page()\n *  - tlb_remove_page_size() / __tlb_remove_page_size()\n *\n *    __tlb_remove_page_size() is the basic primitive that queues a page for\n *    freeing. __tlb_remove_page() assumes PAGE_SIZE. Both will return a\n *    boolean indicating if the queue is (now) full and a call to\n *    tlb_flush_mmu() is required.\n *\n *    tlb_remove_page() and tlb_remove_page_size() imply the call to\n *    tlb_flush_mmu() when required and has no return value.\n *\n *  - tlb_change_page_size()\n *\n *    call before __tlb_remove_page*() to set the current page-size; implies a\n *    possible tlb_flush_mmu() call.\n *\n *  - tlb_flush_mmu() / tlb_flush_mmu_tlbonly()\n *\n *    tlb_flush_mmu_tlbonly() - does the TLB invalidate (and resets\n *                              related state, like the range)\n *\n *    tlb_flush_mmu() - in addition to the above TLB invalidate, also frees\n *\t\t\twhatever pages are still batched.\n *\n *  - mmu_gather::fullmm\n *\n *    A flag set by tlb_gather_mmu_fullmm() to indicate we're going to free\n *    the entire mm; this allows a number of optimizations.\n *\n *    - We can ignore tlb_{start,end}_vma(); because we don't\n *      care about ranges. Everything will be shot down.\n *\n *    - (RISC) architectures that use ASIDs can cycle to a new ASID\n *      and delay the invalidation until ASID space runs out.\n *\n *  - mmu_gather::need_flush_all\n *\n *    A flag that can be set by the arch code if it wants to force\n *    flush the entire TLB irrespective of the range. For instance\n *    x86-PAE needs this when changing top-level entries.\n *\n * And allows the architecture to provide and implement tlb_flush():\n *\n * tlb_flush() may, in addition to the above mentioned mmu_gather fields, make\n * use of:\n *\n *  - mmu_gather::start / mmu_gather::end\n *\n *    which provides the range that needs to be flushed to cover the pages to\n *    be freed.\n *\n *  - mmu_gather::freed_tables\n *\n *    set when we freed page table pages\n *\n *  - tlb_get_unmap_shift() / tlb_get_unmap_size()\n *\n *    returns the smallest TLB entry size unmapped in this range.\n *\n * If an architecture does not provide tlb_flush() a default implementation\n * based on flush_tlb_range() will be used, unless MMU_GATHER_NO_RANGE is\n * specified, in which case we'll default to flush_tlb_mm().\n *\n * Additionally there are a few opt-in features:\n *\n *  MMU_GATHER_PAGE_SIZE\n *\n *  This ensures we call tlb_flush() every time tlb_change_page_size() actually\n *  changes the size and provides mmu_gather::page_size to tlb_flush().\n *\n *  This might be useful if your architecture has size specific TLB\n *  invalidation instructions.\n *\n *  MMU_GATHER_TABLE_FREE\n *\n *  This provides tlb_remove_table(), to be used instead of tlb_remove_page()\n *  for page directores (__p*_free_tlb()).\n *\n *  Useful if your architecture has non-page page directories.\n *\n *  When used, an architecture is expected to provide __tlb_remove_table()\n *  which does the actual freeing of these pages.\n *\n *  MMU_GATHER_RCU_TABLE_FREE\n *\n *  Like MMU_GATHER_TABLE_FREE, and adds semi-RCU semantics to the free (see\n *  comment below).\n *\n *  Useful if your architecture doesn't use IPIs for remote TLB invalidates\n *  and therefore doesn't naturally serialize with software page-table walkers.\n *\n *  MMU_GATHER_NO_FLUSH_CACHE\n *\n *  Indicates the architecture has flush_cache_range() but it needs *NOT* be called\n *  before unmapping a VMA.\n *\n *  NOTE: strictly speaking we shouldn't have this knob and instead rely on\n *\t  flush_cache_range() being a NOP, except Sparc64 seems to be\n *\t  different here.\n *\n *  MMU_GATHER_MERGE_VMAS\n *\n *  Indicates the architecture wants to merge ranges over VMAs; typical when\n *  multiple range invalidates are more expensive than a full invalidate.\n *\n *  MMU_GATHER_NO_RANGE\n *\n *  Use this if your architecture lacks an efficient flush_tlb_range(). This\n *  option implies MMU_GATHER_MERGE_VMAS above.\n *\n *  MMU_GATHER_NO_GATHER\n *\n *  If the option is set the mmu_gather will not track individual pages for\n *  delayed page free anymore. A platform that enables the option needs to\n *  provide its own implementation of the __tlb_remove_page_size() function to\n *  free pages.\n *\n *  This is useful if your architecture already flushes TLB entries in the\n *  various ptep_get_and_clear() functions.\n */\n\n#ifdef CONFIG_MMU_GATHER_TABLE_FREE\n\nstruct mmu_table_batch {\n#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE\n\tstruct rcu_head\t\trcu;\n#endif\n\tunsigned int\t\tnr;\n\tvoid\t\t\t*tables[];\n};\n\n#define MAX_TABLE_BATCH\t\t\\\n\t((PAGE_SIZE - sizeof(struct mmu_table_batch)) / sizeof(void *))\n\nextern void tlb_remove_table(struct mmu_gather *tlb, void *table);\n\n#else /* !CONFIG_MMU_GATHER_HAVE_TABLE_FREE */\n\n/*\n * Without MMU_GATHER_TABLE_FREE the architecture is assumed to have page based\n * page directories and we can use the normal page batching to free them.\n */\n#define tlb_remove_table(tlb, page) tlb_remove_page((tlb), (page))\n\n#endif /* CONFIG_MMU_GATHER_TABLE_FREE */\n\n#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE\n/*\n * This allows an architecture that does not use the linux page-tables for\n * hardware to skip the TLBI when freeing page tables.\n */\n#ifndef tlb_needs_table_invalidate\n#define tlb_needs_table_invalidate() (true)\n#endif\n\n#else\n\n#ifdef tlb_needs_table_invalidate\n#error tlb_needs_table_invalidate() requires MMU_GATHER_RCU_TABLE_FREE\n#endif\n\n#endif /* CONFIG_MMU_GATHER_RCU_TABLE_FREE */\n\n\n#ifndef CONFIG_MMU_GATHER_NO_GATHER\n/*\n * If we can't allocate a page to make a big batch of page pointers\n * to work on, then just handle a few from the on-stack structure.\n */\n#define MMU_GATHER_BUNDLE\t8\n\nstruct mmu_gather_batch {\n\tstruct mmu_gather_batch\t*next;\n\tunsigned int\t\tnr;\n\tunsigned int\t\tmax;\n\tstruct page\t\t*pages[];\n};\n\n#define MAX_GATHER_BATCH\t\\\n\t((PAGE_SIZE - sizeof(struct mmu_gather_batch)) / sizeof(void *))\n\n/*\n * Limit the maximum number of mmu_gather batches to reduce a risk of soft\n * lockups for non-preemptible kernels on huge machines when a lot of memory\n * is zapped during unmapping.\n * 10K pages freed at once should be safe even without a preemption point.\n */\n#define MAX_GATHER_BATCH_COUNT\t(10000UL/MAX_GATHER_BATCH)\n\nextern bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,\n\t\t\t\t   int page_size);\n#endif\n\n/*\n * struct mmu_gather is an opaque type used by the mm code for passing around\n * any data needed by arch specific code for tlb_remove_page.\n */\nstruct mmu_gather {\n\tstruct mm_struct\t*mm;\n\n#ifdef CONFIG_MMU_GATHER_TABLE_FREE\n\tstruct mmu_table_batch\t*batch;\n#endif\n\n\tunsigned long\t\tstart;\n\tunsigned long\t\tend;\n\t/*\n\t * we are in the middle of an operation to clear\n\t * a full mm and can make some optimizations\n\t */\n\tunsigned int\t\tfullmm : 1;\n\n\t/*\n\t * we have performed an operation which\n\t * requires a complete flush of the tlb\n\t */\n\tunsigned int\t\tneed_flush_all : 1;\n\n\t/*\n\t * we have removed page directories\n\t */\n\tunsigned int\t\tfreed_tables : 1;\n\n\t/*\n\t * at which levels have we cleared entries?\n\t */\n\tunsigned int\t\tcleared_ptes : 1;\n\tunsigned int\t\tcleared_pmds : 1;\n\tunsigned int\t\tcleared_puds : 1;\n\tunsigned int\t\tcleared_p4ds : 1;\n\n\t/*\n\t * tracks VM_EXEC | VM_HUGETLB in tlb_start_vma\n\t */\n\tunsigned int\t\tvma_exec : 1;\n\tunsigned int\t\tvma_huge : 1;\n\tunsigned int\t\tvma_pfn  : 1;\n\n\tunsigned int\t\tbatch_count;\n\n#ifndef CONFIG_MMU_GATHER_NO_GATHER\n\tstruct mmu_gather_batch *active;\n\tstruct mmu_gather_batch\tlocal;\n\tstruct page\t\t*__pages[MMU_GATHER_BUNDLE];\n\n#ifdef CONFIG_MMU_GATHER_PAGE_SIZE\n\tunsigned int page_size;\n#endif\n#endif\n};\n\nvoid tlb_flush_mmu(struct mmu_gather *tlb);\n\nstatic inline void __tlb_adjust_range(struct mmu_gather *tlb,\n\t\t\t\t      unsigned long address,\n\t\t\t\t      unsigned int range_size)\n{\n\ttlb->start = min(tlb->start, address);\n\ttlb->end = max(tlb->end, address + range_size);\n}\n\nstatic inline void __tlb_reset_range(struct mmu_gather *tlb)\n{\n\tif (tlb->fullmm) {\n\t\ttlb->start = tlb->end = ~0;\n\t} else {\n\t\ttlb->start = TASK_SIZE;\n\t\ttlb->end = 0;\n\t}\n\ttlb->freed_tables = 0;\n\ttlb->cleared_ptes = 0;\n\ttlb->cleared_pmds = 0;\n\ttlb->cleared_puds = 0;\n\ttlb->cleared_p4ds = 0;\n\t/*\n\t * Do not reset mmu_gather::vma_* fields here, we do not\n\t * call into tlb_start_vma() again to set them if there is an\n\t * intermediate flush.\n\t */\n}\n\n#ifdef CONFIG_MMU_GATHER_NO_RANGE\n\n#if defined(tlb_flush)\n#error MMU_GATHER_NO_RANGE relies on default tlb_flush()\n#endif\n\n/*\n * When an architecture does not have efficient means of range flushing TLBs\n * there is no point in doing intermediate flushes on tlb_end_vma() to keep the\n * range small. We equally don't have to worry about page granularity or other\n * things.\n *\n * All we need to do is issue a full flush for any !0 range.\n */\nstatic inline void tlb_flush(struct mmu_gather *tlb)\n{\n\tif (tlb->end)\n\t\tflush_tlb_mm(tlb->mm);\n}\n\nstatic inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma) { }\n\n#else /* CONFIG_MMU_GATHER_NO_RANGE */\n\n#ifndef tlb_flush\n/*\n * When an architecture does not provide its own tlb_flush() implementation\n * but does have a reasonably efficient flush_vma_range() implementation\n * use that.\n */\nstatic inline void tlb_flush(struct mmu_gather *tlb)\n{\n\tif (tlb->fullmm || tlb->need_flush_all) {\n\t\tflush_tlb_mm(tlb->mm);\n\t} else if (tlb->end) {\n\t\tstruct vm_area_struct vma = {\n\t\t\t.vm_mm = tlb->mm,\n\t\t\t.vm_flags = (tlb->vma_exec ? VM_EXEC    : 0) |\n\t\t\t\t    (tlb->vma_huge ? VM_HUGETLB : 0),\n\t\t};\n\n\t\tflush_tlb_range(&vma, tlb->start, tlb->end);\n\t}\n}\n#endif\n\n#endif /* CONFIG_MMU_GATHER_NO_RANGE */\n\nstatic inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\t/*\n\t * flush_tlb_range() implementations that look at VM_HUGETLB (tile,\n\t * mips-4k) flush only large pages.\n\t *\n\t * flush_tlb_range() implementations that flush I-TLB also flush D-TLB\n\t * (tile, xtensa, arm), so it's ok to just add VM_EXEC to an existing\n\t * range.\n\t *\n\t * We rely on tlb_end_vma() to issue a flush, such that when we reset\n\t * these values the batch is empty.\n\t */\n\ttlb->vma_huge = is_vm_hugetlb_page(vma);\n\ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);\n\ttlb->vma_pfn  = !!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP));\n}\n\nstatic inline void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)\n{\n\t/*\n\t * Anything calling __tlb_adjust_range() also sets at least one of\n\t * these bits.\n\t */\n\tif (!(tlb->freed_tables || tlb->cleared_ptes || tlb->cleared_pmds ||\n\t      tlb->cleared_puds || tlb->cleared_p4ds))\n\t\treturn;\n\n\ttlb_flush(tlb);\n\tmmu_notifier_invalidate_range(tlb->mm, tlb->start, tlb->end);\n\t__tlb_reset_range(tlb);\n}\n\nstatic inline void tlb_remove_page_size(struct mmu_gather *tlb,\n\t\t\t\t\tstruct page *page, int page_size)\n{\n\tif (__tlb_remove_page_size(tlb, page, page_size))\n\t\ttlb_flush_mmu(tlb);\n}\n\nstatic inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)\n{\n\treturn __tlb_remove_page_size(tlb, page, PAGE_SIZE);\n}\n\n/* tlb_remove_page\n *\tSimilar to __tlb_remove_page but will call tlb_flush_mmu() itself when\n *\trequired.\n */\nstatic inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)\n{\n\treturn tlb_remove_page_size(tlb, page, PAGE_SIZE);\n}\n\nstatic inline void tlb_change_page_size(struct mmu_gather *tlb,\n\t\t\t\t\t\t     unsigned int page_size)\n{\n#ifdef CONFIG_MMU_GATHER_PAGE_SIZE\n\tif (tlb->page_size && tlb->page_size != page_size) {\n\t\tif (!tlb->fullmm && !tlb->need_flush_all)\n\t\t\ttlb_flush_mmu(tlb);\n\t}\n\n\ttlb->page_size = page_size;\n#endif\n}\n\nstatic inline unsigned long tlb_get_unmap_shift(struct mmu_gather *tlb)\n{\n\tif (tlb->cleared_ptes)\n\t\treturn PAGE_SHIFT;\n\tif (tlb->cleared_pmds)\n\t\treturn PMD_SHIFT;\n\tif (tlb->cleared_puds)\n\t\treturn PUD_SHIFT;\n\tif (tlb->cleared_p4ds)\n\t\treturn P4D_SHIFT;\n\n\treturn PAGE_SHIFT;\n}\n\nstatic inline unsigned long tlb_get_unmap_size(struct mmu_gather *tlb)\n{\n\treturn 1UL << tlb_get_unmap_shift(tlb);\n}\n\n/*\n * In the case of tlb vma handling, we can optimise these away in the\n * case where we're doing a full MM flush.  When we're doing a munmap,\n * the vmas are adjusted to only cover the region to be torn down.\n */\nstatic inline void tlb_start_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (tlb->fullmm)\n\t\treturn;\n\n\ttlb_update_vma_flags(tlb, vma);\n#ifndef CONFIG_MMU_GATHER_NO_FLUSH_CACHE\n\tflush_cache_range(vma, vma->vm_start, vma->vm_end);\n#endif\n}\n\nstatic inline void tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (tlb->fullmm)\n\t\treturn;\n\n\t/*\n\t * VM_PFNMAP is more fragile because the core mm will not track the\n\t * page mapcount -- there might not be page-frames for these PFNs after\n\t * all. Force flush TLBs for such ranges to avoid munmap() vs\n\t * unmap_mapping_range() races.\n\t */\n\tif (tlb->vma_pfn || !IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS)) {\n\t\t/*\n\t\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n\t\t * the ranges growing with the unused space between consecutive VMAs.\n\t\t */\n\t\ttlb_flush_mmu_tlbonly(tlb);\n\t}\n}\n\n/*\n * tlb_flush_{pte|pmd|pud|p4d}_range() adjust the tlb->start and tlb->end,\n * and set corresponding cleared_*.\n */\nstatic inline void tlb_flush_pte_range(struct mmu_gather *tlb,\n\t\t\t\t     unsigned long address, unsigned long size)\n{\n\t__tlb_adjust_range(tlb, address, size);\n\ttlb->cleared_ptes = 1;\n}\n\nstatic inline void tlb_flush_pmd_range(struct mmu_gather *tlb,\n\t\t\t\t     unsigned long address, unsigned long size)\n{\n\t__tlb_adjust_range(tlb, address, size);\n\ttlb->cleared_pmds = 1;\n}\n\nstatic inline void tlb_flush_pud_range(struct mmu_gather *tlb,\n\t\t\t\t     unsigned long address, unsigned long size)\n{\n\t__tlb_adjust_range(tlb, address, size);\n\ttlb->cleared_puds = 1;\n}\n\nstatic inline void tlb_flush_p4d_range(struct mmu_gather *tlb,\n\t\t\t\t     unsigned long address, unsigned long size)\n{\n\t__tlb_adjust_range(tlb, address, size);\n\ttlb->cleared_p4ds = 1;\n}\n\n#ifndef __tlb_remove_tlb_entry\n#define __tlb_remove_tlb_entry(tlb, ptep, address) do { } while (0)\n#endif\n\n/**\n * tlb_remove_tlb_entry - remember a pte unmapping for later tlb invalidation.\n *\n * Record the fact that pte's were really unmapped by updating the range,\n * so we can later optimise away the tlb invalidate.   This helps when\n * userspace is unmapping already-unmapped pages, which happens quite a lot.\n */\n#define tlb_remove_tlb_entry(tlb, ptep, address)\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pte_range(tlb, address, PAGE_SIZE);\t\\\n\t\t__tlb_remove_tlb_entry(tlb, ptep, address);\t\\\n\t} while (0)\n\n#define tlb_remove_huge_tlb_entry(h, tlb, ptep, address)\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tunsigned long _sz = huge_page_size(h);\t\t\\\n\t\tif (_sz >= P4D_SIZE)\t\t\t\t\\\n\t\t\ttlb_flush_p4d_range(tlb, address, _sz);\t\\\n\t\telse if (_sz >= PUD_SIZE)\t\t\t\\\n\t\t\ttlb_flush_pud_range(tlb, address, _sz);\t\\\n\t\telse if (_sz >= PMD_SIZE)\t\t\t\\\n\t\t\ttlb_flush_pmd_range(tlb, address, _sz);\t\\\n\t\telse\t\t\t\t\t\t\\\n\t\t\ttlb_flush_pte_range(tlb, address, _sz);\t\\\n\t\t__tlb_remove_tlb_entry(tlb, ptep, address);\t\\\n\t} while (0)\n\n/**\n * tlb_remove_pmd_tlb_entry - remember a pmd mapping for later tlb invalidation\n * This is a nop so far, because only x86 needs it.\n */\n#ifndef __tlb_remove_pmd_tlb_entry\n#define __tlb_remove_pmd_tlb_entry(tlb, pmdp, address) do {} while (0)\n#endif\n\n#define tlb_remove_pmd_tlb_entry(tlb, pmdp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pmd_range(tlb, address, HPAGE_PMD_SIZE);\t\\\n\t\t__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);\t\t\\\n\t} while (0)\n\n/**\n * tlb_remove_pud_tlb_entry - remember a pud mapping for later tlb\n * invalidation. This is a nop so far, because only x86 needs it.\n */\n#ifndef __tlb_remove_pud_tlb_entry\n#define __tlb_remove_pud_tlb_entry(tlb, pudp, address) do {} while (0)\n#endif\n\n#define tlb_remove_pud_tlb_entry(tlb, pudp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pud_range(tlb, address, HPAGE_PUD_SIZE);\t\\\n\t\t__tlb_remove_pud_tlb_entry(tlb, pudp, address);\t\t\\\n\t} while (0)\n\n/*\n * For things like page tables caches (ie caching addresses \"inside\" the\n * page tables, like x86 does), for legacy reasons, flushing an\n * individual page had better flush the page table caches behind it. This\n * is definitely how x86 works, for example. And if you have an\n * architected non-legacy page table cache (which I'm not aware of\n * anybody actually doing), you're going to have some architecturally\n * explicit flushing for that, likely *separate* from a regular TLB entry\n * flush, and thus you'd need more than just some range expansion..\n *\n * So if we ever find an architecture\n * that would want something that odd, I think it is up to that\n * architecture to do its own odd thing, not cause pain for others\n * http://lkml.kernel.org/r/CA+55aFzBggoXtNXQeng5d_mRoDnaMBE5Y+URs+PHR67nUpMtaw@mail.gmail.com\n *\n * For now w.r.t page table cache, mark the range_size as PAGE_SIZE\n */\n\n#ifndef pte_free_tlb\n#define pte_free_tlb(tlb, ptep, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pmd_range(tlb, address, PAGE_SIZE);\t\\\n\t\ttlb->freed_tables = 1;\t\t\t\t\\\n\t\t__pte_free_tlb(tlb, ptep, address);\t\t\\\n\t} while (0)\n#endif\n\n#ifndef pmd_free_tlb\n#define pmd_free_tlb(tlb, pmdp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttlb_flush_pud_range(tlb, address, PAGE_SIZE);\t\\\n\t\ttlb->freed_tables = 1;\t\t\t\t\\\n\t\t__pmd_free_tlb(tlb, pmdp, address);\t\t\\\n\t} while (0)\n#endif\n\n#ifndef pud_free_tlb\n#define pud_free_tlb(tlb, pudp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttlb_flush_p4d_range(tlb, address, PAGE_SIZE);\t\\\n\t\ttlb->freed_tables = 1;\t\t\t\t\\\n\t\t__pud_free_tlb(tlb, pudp, address);\t\t\\\n\t} while (0)\n#endif\n\n#ifndef p4d_free_tlb\n#define p4d_free_tlb(tlb, pudp, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\t__tlb_adjust_range(tlb, address, PAGE_SIZE);\t\\\n\t\ttlb->freed_tables = 1;\t\t\t\t\\\n\t\t__p4d_free_tlb(tlb, pudp, address);\t\t\\\n\t} while (0)\n#endif\n\n#ifndef pte_needs_flush\nstatic inline bool pte_needs_flush(pte_t oldpte, pte_t newpte)\n{\n\treturn true;\n}\n#endif\n\n#ifndef huge_pmd_needs_flush\nstatic inline bool huge_pmd_needs_flush(pmd_t oldpmd, pmd_t newpmd)\n{\n\treturn true;\n}\n#endif\n\n#endif /* CONFIG_MMU */\n\n#endif /* _ASM_GENERIC__TLB_H */\n"], "filenames": ["include/asm-generic/tlb.h"], "buggy_code_start_loc": [305], "buggy_code_end_loc": [520], "fixing_code_start_loc": [306], "fixing_code_end_loc": [521], "type": "CWE-362", "message": "An issue was discovered in include/asm-generic/tlb.h in the Linux kernel before 5.19. Because of a race condition (unmap_mapping_range versus munmap), a device driver can free a page while it still has stale TLB entries. This only occurs in situations with VM_PFNMAP VMAs.", "other": {"cve": {"id": "CVE-2022-39188", "sourceIdentifier": "cve@mitre.org", "published": "2022-09-02T05:15:07.650", "lastModified": "2022-11-21T19:45:11.747", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in include/asm-generic/tlb.h in the Linux kernel before 5.19. Because of a race condition (unmap_mapping_range versus munmap), a device driver can free a page while it still has stale TLB entries. This only occurs in situations with VM_PFNMAP VMAs."}, {"lang": "es", "value": "Se ha detectado un problema en el archivo include/asm-generic/tlb.h en el kernel de Linux versiones anteriores a 5.19. Debido a una condici\u00f3n de carrera (unmap_mapping_range frente a munmap), un controlador de dispositivo puede liberar una p\u00e1gina mientras todav\u00eda presenta entradas de TLB antiguas. Esto s\u00f3lo ocurre en situaciones con VM_PFNMAP VMAs"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.19", "matchCriteriaId": "E74E9AF8-BDF5-4917-A9CA-0AAD8E13149B"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:11.0:*:*:*:*:*:*:*", "matchCriteriaId": "FA6FEEC2-9F11-4643-8827-749718254FED"}]}]}], "references": [{"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=2329", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.19", "source": "cve@mitre.org", "tags": ["Patch", "Release Notes", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b67fbebd4cf980aecbcc750e1462128bffe8ae15", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/b67fbebd4cf980aecbcc750e1462128bffe8ae15", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/10/msg00000.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/11/msg00001.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lore.kernel.org/stable/CAG48ez3SEqOPcPCYGHVZv4iqEApujD5VtM3Re-tCKLDEFdEdbg@mail.gmail.com/", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://www.debian.org/security/2022/dsa-5257", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b67fbebd4cf980aecbcc750e1462128bffe8ae15"}}