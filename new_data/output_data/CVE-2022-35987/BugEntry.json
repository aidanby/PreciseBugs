{"buggy_code": ["/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <memory>\n#include <vector>\n\n#include \"tensorflow/compiler/tf2xla/type_util.h\"\n#include \"tensorflow/compiler/tf2xla/xla_helpers.h\"\n#include \"tensorflow/compiler/tf2xla/xla_op_kernel.h\"\n#include \"tensorflow/compiler/tf2xla/xla_op_registry.h\"\n#include \"tensorflow/compiler/xla/client/lib/arithmetic.h\"\n#include \"tensorflow/compiler/xla/client/lib/comparators.h\"\n#include \"tensorflow/compiler/xla/client/lib/constants.h\"\n#include \"tensorflow/compiler/xla/client/xla_computation.h\"\n#include \"tensorflow/compiler/xla/shape_util.h\"\n\nnamespace tensorflow {\nnamespace {\n\nclass DenseBincountOp : public XlaOpKernel {\n public:\n  explicit DenseBincountOp(OpKernelConstruction* ctx) : XlaOpKernel(ctx) {\n    // It is optional for Bincount and required for DenseBincount\n    (void)ctx->GetAttr(\"binary_output\", &binary_output_);\n  }\n\n private:\n  bool binary_output_ = false;\n  void Compile(XlaOpKernelContext* ctx) override {\n    int64_t output_size;\n    xla::XlaOp output_size_param = ctx->Input(\"size\");\n    StatusOr<xla::Shape> output_shape_or =\n        ctx->builder()->GetShape(output_size_param);\n    OP_REQUIRES_OK(ctx, output_shape_or.status());\n    auto output_shape_param = output_shape_or.ValueOrDie();\n    auto output_rank = output_shape_param.rank();\n    OP_REQUIRES(ctx, output_rank == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        output_rank));\n    OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntScalar(\"size\", &output_size));\n    OP_REQUIRES(ctx, output_size >= 0,\n                errors::InvalidArgument(\"size (\", output_size,\n                                        \") must be non-negative\"));\n    xla::XlaOp idx, updates, output;\n    xla::XlaOp input = ctx->Input(0);\n    auto input_xla_type = ctx->input_xla_type(0);\n    xla::PrimitiveType dtype = ctx->InputXlaType(\"weights\");\n    auto zero = xla::Zero(ctx->builder(), dtype);\n    auto one = xla::One(ctx->builder(), dtype);\n    StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n    OP_REQUIRES_OK(ctx, input_shape_or.status());\n    auto input_shape = input_shape_or.ValueOrDie();\n    auto size = input_shape.dimensions(0);\n\n    if (!size) {\n      output = xla::Broadcast(zero, {output_size});\n      ctx->SetOutput(0, output);\n      return;\n    }\n    auto rank = input_shape.rank();\n\n    OP_REQUIRES(ctx, rank <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", rank));\n\n    xla::XlaOp weights = ctx->Input(2);\n    StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n    OP_REQUIRES_OK(ctx, weights_shape_or.status());\n\n    auto weights_shape = weights_shape_or.ValueOrDie();\n    auto weights_size = weights_shape.dimensions(0);\n    bool has_weights = false;\n    if (weights_size) {\n      has_weights = true;\n    }\n    xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n    xla::ScatterDimensionNumbers scatter_dnums;\n    scatter_dnums.set_index_vector_dim(1);\n    scatter_dnums.add_inserted_window_dims(0);\n    scatter_dnums.add_scatter_dims_to_operand_dims(0);\n\n    if (rank == 2) {\n      output_shape = xla::ShapeUtil::MakeShape(dtype, {size, output_size});\n      scatter_dnums.add_inserted_window_dims(1);\n      scatter_dnums.add_scatter_dims_to_operand_dims(1);\n      auto i_shape =\n          xla::ShapeUtil::MakeShape(input_xla_type, {input_shape.dimensions()});\n      auto i = xla::Iota(ctx->builder(), i_shape, 0);\n      i = xla::Reshape(\n          i, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      auto j = xla::Reshape(\n          input, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      std::vector<xla::XlaOp> iotas_to_concat;\n      iotas_to_concat.push_back(i);\n      iotas_to_concat.push_back(j);\n      idx = xla::ConcatInDim(ctx->builder(), iotas_to_concat, 1);\n      updates = xla::Broadcast(\n          one, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n      output = xla::Broadcast(\n          zero, {output_shape.dimensions(0), output_shape.dimensions(1)});\n      if (has_weights && !binary_output_) {\n        weights = xla::Reshape(\n            weights, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n        updates = weights;\n      }\n    } else {\n      input = xla::Reshape(input, {size, 1});\n      idx = xla::Reshape(input, {size, 1});\n      updates = xla::Broadcast(one, {size});\n      output = xla::Broadcast(zero, {output_size});\n      if (has_weights && !binary_output_) {\n        updates = weights;\n      }\n    }\n\n    xla::XlaComputation assn_computation = [&] {\n      std::unique_ptr<xla::XlaBuilder> subb =\n          ctx->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape param_shape = xla::ShapeUtil::MakeShape(dtype, {});\n      auto p0 = xla::Parameter(subb.get(), 0, param_shape, \"p0\");\n      auto p1 = xla::Parameter(subb.get(), 1, param_shape, \"p1\");\n      if (!binary_output_) {\n        xla::Add(p0, p1);\n      }\n      return subb->BuildAndNoteError();\n    }();\n    output = xla::Scatter(output, idx, updates, assn_computation, scatter_dnums,\n                          false, false);\n    ctx->SetOutput(0, output);\n  }\n};\n\nREGISTER_XLA_OP(Name(\"DenseBincount\").CompileTimeConstantInput(\"size\"),\n                DenseBincountOp);\nREGISTER_XLA_OP(Name(\"Bincount\").CompileTimeConstantInput(\"size\"),\n                DenseBincountOp);\n\n}  // namespace\n}  // namespace tensorflow\n", "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#include \"tensorflow/core/platform/errors.h\"\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/bincount_op.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/sparse_utils.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/determinism.h\"\n\nnamespace tensorflow {\n\nusing thread::ThreadPool;\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename Tidx, typename T>\nstruct BincountFunctor<CPUDevice, Tidx, T, true> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n                        const typename TTypes<T, 1>::ConstTensor& weights,\n                        typename TTypes<T, 1>::Tensor& output,\n                        const Tidx num_bins) {\n    Tensor all_nonneg_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({}), &all_nonneg_t, AllocatorAttributes()));\n    all_nonneg_t.scalar<bool>().device(context->eigen_cpu_device()) =\n        (arr >= Tidx(0)).all();\n    if (!all_nonneg_t.scalar<bool>()()) {\n      return errors::InvalidArgument(\"Input arr must be non-negative!\");\n    }\n\n    // Allocate partial output bin sums for each worker thread. Worker ids in\n    // ParallelForWithWorkerId range from 0 to NumThreads() inclusive.\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    const int64_t num_threads = thread_pool->NumThreads() + 1;\n    Tensor partial_bins_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({num_threads, num_bins}), &partial_bins_t));\n    auto partial_bins = partial_bins_t.matrix<bool>();\n    partial_bins.setZero();\n    thread_pool->ParallelForWithWorkerId(\n        arr.size(), 8 /* cost */,\n        [&](int64_t start_ind, int64_t limit_ind, int64_t worker_id) {\n          for (int64_t i = start_ind; i < limit_ind; i++) {\n            Tidx value = arr(i);\n            if (value < num_bins) {\n              partial_bins(worker_id, value) = true;\n            }\n          }\n        });\n\n    // Sum the partial bins along the 0th axis.\n    Eigen::array<int, 1> reduce_dim({0});\n    output.device(context->eigen_cpu_device()) =\n        partial_bins.any(reduce_dim).cast<T>();\n    return OkStatus();\n  }\n};\n\ntemplate <typename Tidx, typename T>\nstruct BincountFunctor<CPUDevice, Tidx, T, false> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n                        const typename TTypes<T, 1>::ConstTensor& weights,\n                        typename TTypes<T, 1>::Tensor& output,\n                        const Tidx num_bins) {\n    Tensor all_nonneg_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({}), &all_nonneg_t, AllocatorAttributes()));\n    all_nonneg_t.scalar<bool>().device(context->eigen_cpu_device()) =\n        (arr >= Tidx(0)).all();\n    if (!all_nonneg_t.scalar<bool>()()) {\n      return errors::InvalidArgument(\"Input arr must be non-negative!\");\n    }\n\n    // Allocate partial output bin sums for each worker thread. Worker ids in\n    // ParallelForWithWorkerId range from 0 to NumThreads() inclusive.\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    const int64_t num_threads = thread_pool->NumThreads() + 1;\n    const Tidx* arr_data = arr.data();\n    const std::ptrdiff_t arr_size = arr.size();\n    const T* weight_data = weights.data();\n    if (weights.size() && weights.size() != arr_size) {\n      return errors::InvalidArgument(\n          \"Input indices and weights must have the same size.\");\n    }\n    if (num_threads == 1) {\n      output.setZero();\n      T* output_data = output.data();\n      if (weights.size()) {\n        for (int64_t i = 0; i < arr_size; i++) {\n          const Tidx value = arr_data[i];\n          if (value < num_bins) {\n            output_data[value] += weight_data[i];\n          }\n        }\n      } else {\n        for (int64_t i = 0; i < arr_size; i++) {\n          const Tidx value = arr_data[i];\n          if (value < num_bins) {\n            // Complex numbers don't support \"++\".\n            output_data[value] += T(1);\n          }\n        }\n      }\n    } else {\n      Tensor partial_bins_t;\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DataTypeToEnum<T>::value, TensorShape({num_threads, num_bins}),\n          &partial_bins_t));\n      auto partial_bins = partial_bins_t.matrix<T>();\n      partial_bins.setZero();\n      thread_pool->ParallelForWithWorkerId(\n          arr_size, 8 /* cost */,\n          [&](int64_t start_ind, int64_t limit_ind, int64_t worker_id) {\n            if (weights.size()) {\n              for (int64_t i = start_ind; i < limit_ind; i++) {\n                Tidx value = arr_data[i];\n                if (value < num_bins) {\n                  partial_bins(worker_id, value) += weight_data[i];\n                }\n              }\n            } else {\n              for (int64_t i = start_ind; i < limit_ind; i++) {\n                Tidx value = arr_data[i];\n                if (value < num_bins) {\n                  // Complex numbers don't support \"++\".\n                  partial_bins(worker_id, value) += T(1);\n                }\n              }\n            }\n          });\n\n      // Sum the partial bins along the 0th axis.\n      Eigen::array<int, 1> reduce_dim({0});\n      output.device(context->eigen_cpu_device()) = partial_bins.sum(reduce_dim);\n    }\n    return OkStatus();\n  }\n};\n\ntemplate <typename Tidx, typename T, bool binary_output>\nstruct BincountReduceFunctor<CPUDevice, Tidx, T, binary_output> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 2>::ConstTensor& in,\n                        const typename TTypes<T, 2>::ConstTensor& weights,\n                        typename TTypes<T, 2>::Tensor& out,\n                        const Tidx num_bins) {\n    const int num_rows = out.dimension(0);\n    const int num_cols = in.dimension(1);\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    thread_pool->ParallelForWithWorkerId(\n        num_rows, 8 /* cost */,\n        [&](int64_t start_row, int64_t end_row, int64_t worker_id) {\n          for (int64_t i = start_row; i < end_row; ++i) {\n            for (int64_t j = 0; j < num_cols; ++j) {\n              Tidx value = in(i, j);\n              if (value < num_bins) {\n                if (binary_output) {\n                  out(i, value) = T(1);\n                } else {\n                  if (weights.size()) {\n                    out(i, value) += weights(i, j);\n                  } else {\n                    out(i, value) += T(1);\n                  }\n                }\n              }\n            }\n          }\n        });\n    return OkStatus();\n  }\n};\n\n}  // namespace functor\n\ntemplate <typename Device, typename T>\nclass BincountOp : public OpKernel {\n public:\n  explicit BincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& arr_t = ctx->input(0);\n    const Tensor& size_tensor = ctx->input(1);\n    OP_REQUIRES(ctx, size_tensor.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_tensor.dims()));\n    int32_t size = size_tensor.scalar<int32_t>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    const Tensor& weights_t = ctx->input(2);\n    const auto arr = arr_t.flat<int32_t>();\n    const auto weights = weights_t.flat<T>();\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({size}), &output_t));\n    auto output = output_t->flat<T>();\n    OP_REQUIRES_OK(ctx,\n                   functor::BincountFunctor<Device, int32_t, T, false>::Compute(\n                       ctx, arr, weights, output, size));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(\"Bincount\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      BincountOp<CPUDevice, type>)\n\nTF_CALL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"Bincount\")                \\\n                              .Device(DEVICE_GPU)         \\\n                              .HostMemory(\"size\")         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          BincountOp<GPUDevice, type>)\n\nTF_CALL_int32(REGISTER_KERNELS);\nTF_CALL_float(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename Tidx, typename T>\nclass DenseBincountOp : public OpKernel {\n public:\n  explicit DenseBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n    if (std::is_same<Device, GPUDevice>::value) {\n      OP_REQUIRES(\n          ctx, !OpDeterminismRequired(),\n          errors::Unimplemented(\n              \"Determinism is not yet supported in GPU implementation of \"\n              \"DenseBincount.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& data = ctx->input(0);\n    OP_REQUIRES(ctx, data.dims() <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", data.dims()));\n\n    const Tensor& size_t = ctx->input(1);\n    const Tensor& weights = ctx->input(2);\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (data.dims() == 1) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      }\n    } else if (data.dims() == 2) {\n      const int64_t num_rows = data.dim_size(0);\n      auto weight_matrix =\n          (weights.NumElements() == 0)\n              ? weights.shaped<T, 2>(gtl::InlinedVector<int64_t, 2>(2, 0))\n              : weights.matrix<T>();\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountReduceFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx,\n            functor::BincountReduceFunctor<Device, Tidx, T, false>::Compute(\n                ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"DenseBincount\")              \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          DenseBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"DenseBincount\")              \\\n                              .Device(DEVICE_GPU)            \\\n                              .HostMemory(\"size\")            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          DenseBincountOp<GPUDevice, Tidx, T>);\n#define REGISTER_GPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_int32(REGISTER_GPU_KERNELS);\nTF_CALL_float(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename Tidx, typename T>\nclass SparseBincountOp : public OpKernel {\n public:\n  explicit SparseBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const Tensor& values = ctx->input(1);\n    const auto values_flat = values.flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n    OP_REQUIRES_OK(\n        ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n                 indices, values, dense_shape, /*validate_indices=*/true));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values_flat, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values_flat, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values_flat(i);\n        OP_REQUIRES(\n            ctx, batch < out.dimension(0),\n            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n                                    \") must be less than the dimension size (\",\n                                    out.dimension(0), \").\"));\n        OP_REQUIRES(\n            ctx, bin < out.dimension(1),\n            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n                                    \") must be less then the dimension size (\",\n                                    out.dimension(1), \").\"));\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseBincount\")             \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          SparseBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\ntemplate <typename Device, typename Tidx, typename T>\nclass RaggedBincountOp : public OpKernel {\n public:\n  explicit RaggedBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const auto splits = ctx->input(0).flat<int64_t>();\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& size_t = ctx->input(2);\n    const auto weights = ctx->input(3).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    OP_REQUIRES(ctx, splits(0) == 0,\n                errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                        splits(0)));\n\n    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n                errors::InvalidArgument(\n                    \"Splits must end with the number of values, got \",\n                    splits(num_rows), \" instead of \", num_values));\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;\n    fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n    const auto out = out_t->matrix<T>();\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      while (idx >= splits(batch_idx)) {\n        batch_idx++;\n      }\n      Tidx bin = values(idx);\n      OP_REQUIRES(ctx, bin >= 0,\n                  errors::InvalidArgument(\"Input must be non-negative\"));\n      if (bin < size) {\n        if (binary_output_) {\n          out(batch_idx - 1, bin) = T(1);\n        } else {\n          T value = (weights_size > 0) ? weights(idx) : T(1);\n          out(batch_idx - 1, bin) += value;\n        }\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedBincount\")             \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          RaggedBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\n}  // end namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for bincount_ops.bincount.\"\"\"\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import bincount_ops\nfrom tensorflow.python.ops import gen_math_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.platform import googletest\n\n\nclass BincountTest(test_util.TensorFlowTestCase):\n\n  def test_empty(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=5)),\n          [0, 0, 0, 0, 0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=1)), [0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=0)), [])\n      self.assertEqual(\n          self.evaluate(\n              bincount_ops.bincount([], minlength=0, dtype=np.float32)).dtype,\n          np.float32)\n      self.assertEqual(\n          self.evaluate(\n              bincount_ops.bincount([], minlength=3, dtype=np.float64)).dtype,\n          np.float64)\n\n  def test_values(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([1, 1, 1, 2, 2, 3])),\n          [0, 3, 2, 1])\n      arr = [1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5]\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(arr)), [0, 5, 4, 3, 2, 1])\n      arr += [0, 0, 0, 0, 0, 0]\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(arr)), [6, 5, 4, 3, 2, 1])\n\n      self.assertAllEqual(self.evaluate(bincount_ops.bincount([])), [])\n      self.assertAllEqual(self.evaluate(bincount_ops.bincount([0, 0, 0])), [3])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([5])), [0, 0, 0, 0, 0, 1])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(np.arange(10000))),\n          np.ones(10000))\n\n  def test_maxlength(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([5], maxlength=3)), [0, 0, 0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([1], maxlength=3)), [0, 1])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], maxlength=3)), [])\n\n  def test_random_with_weights(self):\n    num_samples = 10000\n    with self.session():\n      np.random.seed(42)\n      for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n        arr = np.random.randint(0, 1000, num_samples)\n        if dtype == dtypes.int32 or dtype == dtypes.int64:\n          weights = np.random.randint(-100, 100, num_samples)\n        else:\n          weights = np.random.random(num_samples)\n        self.assertAllClose(\n            self.evaluate(bincount_ops.bincount(arr, weights)),\n            np.bincount(arr, weights))\n\n  def test_random_without_weights(self):\n    num_samples = 10000\n    with self.session():\n      np.random.seed(42)\n      for dtype in [np.int32, np.float32]:\n        arr = np.random.randint(0, 1000, num_samples)\n        weights = np.ones(num_samples).astype(dtype)\n        self.assertAllClose(\n            self.evaluate(bincount_ops.bincount(arr, None)),\n            np.bincount(arr, weights))\n\n  @test_util.run_gpu_only\n  @test_util.disable_xla(\"XLA uses scatter and could be non-deterministic\")\n  def test_bincount_determinism_error(self):\n    arr = np.random.randint(0, 1000, size=1000)\n    with test_util.deterministic_ops(), self.assertRaisesRegex(\n        errors_impl.UnimplementedError,\n        \"Determinism is not yet supported in GPU implementation of Bincount.\"):\n      self.evaluate(bincount_ops.bincount(arr, None, axis=None))\n    arr = np.random.randint(0, 1000, size=(100, 100))\n    with test_util.deterministic_ops(), self.assertRaisesRegex(\n        errors_impl.UnimplementedError,\n        \"Determinism is not yet supported in GPU implementation of \"\n        \"DenseBincount.\"):\n      self.evaluate(bincount_ops.bincount(arr, None, axis=-1))\n\n  def test_zero_weights(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(np.arange(1000), np.zeros(1000))),\n          np.zeros(1000))\n\n  @test_util.disable_xla(\"This is not raised on XLA CPU\")\n  def test_negative(self):\n    # unsorted_segment_sum will only report InvalidArgumentError on CPU\n    with self.cached_session(), ops.device(\"/CPU:0\"):\n      with self.assertRaises(errors.InvalidArgumentError):\n        self.evaluate(bincount_ops.bincount([1, 2, 3, -1, 6, 8]))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_shape_function(self):\n    # size must be scalar.\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"Shape must be rank 0 but is rank 1(?s).*Bincount\"):\n      gen_math_ops.bincount([1, 2, 3, 1, 6, 8], [1], [])\n    # size must be positive.\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be non-negative\"):\n      gen_math_ops.bincount([1, 2, 3, 1, 6, 8], -5, [])\n    # if size is a constant then the shape is known.\n    v1 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], 5, [])\n    self.assertAllEqual(v1.get_shape().as_list(), [5])\n    # if size is a placeholder then the shape is unknown.\n    with ops.Graph().as_default():\n      s = array_ops.placeholder(dtype=dtypes.int32)\n      v2 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], s, [])\n      self.assertAllEqual(v2.get_shape().as_list(), [None])\n\n\nclass BincountOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    size = 1000\n    inp = np.random.randint(0, size, (4096), dtype=dtype)\n    np_out = np.bincount(inp, minlength=size)\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(input=inp, weights=[], size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    size = 1000\n    inp = np.random.randint(0, size, (4096,), dtype=dtype)\n    np_weight = np.random.random((4096,))\n    np_out = np.bincount(inp, minlength=size, weights=np_weight)\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    size = 10\n    inp = np.random.randint(0, size, (4096), dtype=dtype)\n    np_out = np.ones((size,))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=[], size=size, binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_binary_with_weights(self, dtype):\n    np.random.seed(42)\n    size = 10\n    inp = np.random.randint(0, size, (4096,), dtype=dtype)\n    np_weight = np.random.random((4096,))\n    np_out = np.ones((size,))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size, binary_output=True)))\n\n  def _test_bincount_col_count(self, num_rows, num_cols, size, dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(input=inp, weights=[], size=size)))\n\n  def _test_bincount_col_binary(self, num_rows, num_cols, size, dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=[], size=size, binary_output=True)))\n\n  def _test_bincount_col_count_with_weights(self, num_rows, num_cols, size,\n                                            dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_weight = np.random.random((num_rows, num_cols))\n    np_out = np.reshape(\n        np.concatenate([\n            np.bincount(inp[j, :], weights=np_weight[j, :], minlength=size)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      evaluated = self.evaluate(\n          gen_math_ops.dense_bincount(input=inp, weights=np_weight, size=size))\n      if np_out.dtype in (np.float32, np.float64):\n        self.assertAllClose(np_out, evaluated)\n      else:\n        self.assertAllEqual(np_out, evaluated)\n\n  def test_col_reduce_basic(self):\n    with test_util.use_gpu():\n      v = self.evaluate(\n          gen_math_ops.dense_bincount(\n              input=[[1, 2, 3], [0, 3, 2]], weights=[], size=4))\n    expected_out = [[0., 1., 1., 1.], [1., 0., 1., 1.]]\n    self.assertAllEqual(expected_out, v)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_shared_memory(self, dtype):\n    # num_rows * num_bins less than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 10\n    self._test_bincount_col_count(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_global_memory(self, dtype):\n    # num_rows * num_bins more than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 1024\n    self._test_bincount_col_count(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_shared_memory_with_weights(self, dtype):\n    # num_rows * num_bins less than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    self._test_bincount_col_count_with_weights(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_global_memory_with_weights(self, dtype):\n    # num_rows * num_bins more than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 1024\n    self._test_bincount_col_count_with_weights(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_binary(self, dtype):\n    num_rows = 128\n    num_cols = 7\n    size = 10\n    self._test_bincount_col_binary(num_rows, num_cols, size, dtype)\n\n  def test_invalid_rank(self):\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"at most rank 2\"):\n      with test_util.use_gpu():\n        self.evaluate(\n            gen_math_ops.dense_bincount(\n                input=[[[1, 2, 3], [0, 3, 2]]], weights=[], size=10))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.dense_bincount(\n              input=[0], size=[1, 1], weights=[3], binary_output=False))\n\n\nclass SparseBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n\n    np_out = np.bincount(inp_vals, minlength=size)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[])))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n\n    np_out = np.bincount(inp_vals, minlength=size, weights=inp_weight)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[],\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_binary_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight,\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_col_reduce_count(self, dtype):\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    # from_dense will filter out 0s.\n    inp = inp + 1\n    # from_dense will cause OOM in GPU.\n    with ops.device(\"/CPU:0\"):\n      inp_sparse = sparse_ops.from_dense(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_sparse.indices,\n                values=inp_sparse.values - 1,\n                dense_shape=inp_sparse.dense_shape,\n                size=size,\n                weights=[])))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_col_reduce_binary(self, dtype):\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    # from_dense will filter out 0s.\n    inp = inp + 1\n    # from_dense will cause OOM in GPU.\n    with ops.device(\"/CPU:0\"):\n      inp_sparse = sparse_ops.from_dense(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_sparse.indices,\n                values=inp_sparse.values - 1,\n                dense_shape=inp_sparse.dense_shape,\n                size=size,\n                weights=[],\n                binary_output=True)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.sparse_bincount(\n              indices=[[0], [1]],\n              values=[0, 0],\n              dense_shape=[1, 1],\n              size=[1, 1],\n              weights=[0, 0],\n              binary_output=False))\n\n  def test_sparse_bincount_input_validation(self):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,))\n\n    # Insert negative index.\n    inp_indices[10, 0] = -2\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"out of bounds\"):\n      self.evaluate(\n          gen_math_ops.sparse_bincount(\n              indices=inp_indices,\n              values=inp_vals,\n              dense_shape=[num_rows],\n              size=size,\n              weights=[]))\n\n\nclass RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 2, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=6)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_binary(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=[],\n                size=6,\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_with_weights(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    weights = ragged_factory_ops.constant([[], [], [.1, .2, .3], [],\n                                           [.2, .5, .6, .3]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0],\n                       [.2, .3, 0, .1, 0, 0], [0, 0, 0, 0, 0, 0],\n                       [.5, 0, 0, 0, .9, .2]]\n    self.assertAllClose(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=weights.values,\n                size=6)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_np(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_np_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_weight = np.random.random((num_rows, num_cols))\n    np_out = np.reshape(\n        np.concatenate([\n            np.bincount(inp[j, :], weights=np_weight[j, :], minlength=size)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=np_weight,\n                size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_binary_np_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=[],\n                size=size,\n                binary_output=True)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.ragged_bincount(\n              splits=[0, 0, 1],\n              values=[1],\n              size=[1, 1],\n              weights=[0, 0, 0],\n              binary_output=False,\n              name=None))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <memory>\n#include <vector>\n\n#include \"tensorflow/compiler/tf2xla/type_util.h\"\n#include \"tensorflow/compiler/tf2xla/xla_helpers.h\"\n#include \"tensorflow/compiler/tf2xla/xla_op_kernel.h\"\n#include \"tensorflow/compiler/tf2xla/xla_op_registry.h\"\n#include \"tensorflow/compiler/xla/client/lib/arithmetic.h\"\n#include \"tensorflow/compiler/xla/client/lib/comparators.h\"\n#include \"tensorflow/compiler/xla/client/lib/constants.h\"\n#include \"tensorflow/compiler/xla/client/xla_computation.h\"\n#include \"tensorflow/compiler/xla/shape_util.h\"\n\nnamespace tensorflow {\nnamespace {\n\nclass DenseBincountOp : public XlaOpKernel {\n public:\n  explicit DenseBincountOp(OpKernelConstruction* ctx) : XlaOpKernel(ctx) {\n    // It is optional for Bincount and required for DenseBincount\n    (void)ctx->GetAttr(\"binary_output\", &binary_output_);\n  }\n\n private:\n  bool binary_output_ = false;\n  void Compile(XlaOpKernelContext* ctx) override {\n    int64_t output_size;\n    xla::XlaOp output_size_param = ctx->Input(\"size\");\n    StatusOr<xla::Shape> output_shape_or =\n        ctx->builder()->GetShape(output_size_param);\n    OP_REQUIRES_OK(ctx, output_shape_or.status());\n    auto output_shape_param = output_shape_or.ValueOrDie();\n    auto output_rank = output_shape_param.rank();\n    OP_REQUIRES(ctx, output_rank == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        output_rank));\n    OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntScalar(\"size\", &output_size));\n    OP_REQUIRES(ctx, output_size >= 0,\n                errors::InvalidArgument(\"size (\", output_size,\n                                        \") must be non-negative\"));\n    xla::XlaOp idx, updates, output;\n    xla::XlaOp input = ctx->Input(0);\n    auto input_xla_type = ctx->input_xla_type(0);\n    xla::PrimitiveType dtype = ctx->InputXlaType(\"weights\");\n    auto zero = xla::Zero(ctx->builder(), dtype);\n    auto one = xla::One(ctx->builder(), dtype);\n    StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n    OP_REQUIRES_OK(ctx, input_shape_or.status());\n    auto input_shape = input_shape_or.ValueOrDie();\n    auto size = input_shape.dimensions(0);\n\n    if (!size) {\n      output = xla::Broadcast(zero, {output_size});\n      ctx->SetOutput(0, output);\n      return;\n    }\n    auto rank = input_shape.rank();\n\n    OP_REQUIRES(ctx, rank <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", rank));\n\n    xla::XlaOp weights = ctx->Input(2);\n    StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n    OP_REQUIRES_OK(ctx, weights_shape_or.status());\n\n    auto weights_shape = weights_shape_or.ValueOrDie();\n    OP_REQUIRES(ctx,\n                xla::ShapeUtil::CompatibleIgnoringElementType(weights_shape,\n                                                              input_shape) ||\n                    (weights_shape.dimensions_size() > 0 &&\n                     weights_shape.dimensions(0) == 0),\n                errors::InvalidArgument(\n                    \"`weights` must be the same shape as `arr` or a length-0 \"\n                    \"`Tensor`, in which case it acts as all weights equal to \"\n                    \"1. Received \",\n                    weights_shape.DebugString()));\n\n    auto weights_size = weights_shape.dimensions(0);\n    bool has_weights = false;\n    if (weights_size) {\n      has_weights = true;\n    }\n    xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n    xla::ScatterDimensionNumbers scatter_dnums;\n    scatter_dnums.set_index_vector_dim(1);\n    scatter_dnums.add_inserted_window_dims(0);\n    scatter_dnums.add_scatter_dims_to_operand_dims(0);\n\n    if (rank == 2) {\n      output_shape = xla::ShapeUtil::MakeShape(dtype, {size, output_size});\n      scatter_dnums.add_inserted_window_dims(1);\n      scatter_dnums.add_scatter_dims_to_operand_dims(1);\n      auto i_shape =\n          xla::ShapeUtil::MakeShape(input_xla_type, {input_shape.dimensions()});\n      auto i = xla::Iota(ctx->builder(), i_shape, 0);\n      i = xla::Reshape(\n          i, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      auto j = xla::Reshape(\n          input, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      std::vector<xla::XlaOp> iotas_to_concat;\n      iotas_to_concat.push_back(i);\n      iotas_to_concat.push_back(j);\n      idx = xla::ConcatInDim(ctx->builder(), iotas_to_concat, 1);\n      updates = xla::Broadcast(\n          one, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n      output = xla::Broadcast(\n          zero, {output_shape.dimensions(0), output_shape.dimensions(1)});\n      if (has_weights && !binary_output_) {\n        weights = xla::Reshape(\n            weights, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n        updates = weights;\n      }\n    } else {\n      input = xla::Reshape(input, {size, 1});\n      idx = xla::Reshape(input, {size, 1});\n      updates = xla::Broadcast(one, {size});\n      output = xla::Broadcast(zero, {output_size});\n      if (has_weights && !binary_output_) {\n        updates = weights;\n      }\n    }\n\n    xla::XlaComputation assn_computation = [&] {\n      std::unique_ptr<xla::XlaBuilder> subb =\n          ctx->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape param_shape = xla::ShapeUtil::MakeShape(dtype, {});\n      auto p0 = xla::Parameter(subb.get(), 0, param_shape, \"p0\");\n      auto p1 = xla::Parameter(subb.get(), 1, param_shape, \"p1\");\n      if (!binary_output_) {\n        xla::Add(p0, p1);\n      }\n      return subb->BuildAndNoteError();\n    }();\n    output = xla::Scatter(output, idx, updates, assn_computation, scatter_dnums,\n                          false, false);\n    ctx->SetOutput(0, output);\n  }\n};\n\nREGISTER_XLA_OP(Name(\"DenseBincount\").CompileTimeConstantInput(\"size\"),\n                DenseBincountOp);\nREGISTER_XLA_OP(Name(\"Bincount\").CompileTimeConstantInput(\"size\"),\n                DenseBincountOp);\n\n}  // namespace\n}  // namespace tensorflow\n", "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#include \"tensorflow/core/platform/errors.h\"\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/bincount_op.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/sparse_utils.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/determinism.h\"\n\nnamespace tensorflow {\n\nusing thread::ThreadPool;\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename Tidx, typename T>\nstruct BincountFunctor<CPUDevice, Tidx, T, true> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n                        const typename TTypes<T, 1>::ConstTensor& weights,\n                        typename TTypes<T, 1>::Tensor& output,\n                        const Tidx num_bins) {\n    Tensor all_nonneg_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({}), &all_nonneg_t, AllocatorAttributes()));\n    all_nonneg_t.scalar<bool>().device(context->eigen_cpu_device()) =\n        (arr >= Tidx(0)).all();\n    if (!all_nonneg_t.scalar<bool>()()) {\n      return errors::InvalidArgument(\"Input arr must be non-negative!\");\n    }\n\n    // Allocate partial output bin sums for each worker thread. Worker ids in\n    // ParallelForWithWorkerId range from 0 to NumThreads() inclusive.\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    const int64_t num_threads = thread_pool->NumThreads() + 1;\n    Tensor partial_bins_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({num_threads, num_bins}), &partial_bins_t));\n    auto partial_bins = partial_bins_t.matrix<bool>();\n    partial_bins.setZero();\n    thread_pool->ParallelForWithWorkerId(\n        arr.size(), 8 /* cost */,\n        [&](int64_t start_ind, int64_t limit_ind, int64_t worker_id) {\n          for (int64_t i = start_ind; i < limit_ind; i++) {\n            Tidx value = arr(i);\n            if (value < num_bins) {\n              partial_bins(worker_id, value) = true;\n            }\n          }\n        });\n\n    // Sum the partial bins along the 0th axis.\n    Eigen::array<int, 1> reduce_dim({0});\n    output.device(context->eigen_cpu_device()) =\n        partial_bins.any(reduce_dim).cast<T>();\n    return OkStatus();\n  }\n};\n\ntemplate <typename Tidx, typename T>\nstruct BincountFunctor<CPUDevice, Tidx, T, false> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n                        const typename TTypes<T, 1>::ConstTensor& weights,\n                        typename TTypes<T, 1>::Tensor& output,\n                        const Tidx num_bins) {\n    Tensor all_nonneg_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({}), &all_nonneg_t, AllocatorAttributes()));\n    all_nonneg_t.scalar<bool>().device(context->eigen_cpu_device()) =\n        (arr >= Tidx(0)).all();\n    if (!all_nonneg_t.scalar<bool>()()) {\n      return errors::InvalidArgument(\"Input arr must be non-negative!\");\n    }\n\n    // Allocate partial output bin sums for each worker thread. Worker ids in\n    // ParallelForWithWorkerId range from 0 to NumThreads() inclusive.\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    const int64_t num_threads = thread_pool->NumThreads() + 1;\n    const Tidx* arr_data = arr.data();\n    const std::ptrdiff_t arr_size = arr.size();\n    const T* weight_data = weights.data();\n    if (weights.size() && weights.size() != arr_size) {\n      return errors::InvalidArgument(\n          \"Input indices and weights must have the same size.\");\n    }\n    if (num_threads == 1) {\n      output.setZero();\n      T* output_data = output.data();\n      if (weights.size()) {\n        for (int64_t i = 0; i < arr_size; i++) {\n          const Tidx value = arr_data[i];\n          if (value < num_bins) {\n            output_data[value] += weight_data[i];\n          }\n        }\n      } else {\n        for (int64_t i = 0; i < arr_size; i++) {\n          const Tidx value = arr_data[i];\n          if (value < num_bins) {\n            // Complex numbers don't support \"++\".\n            output_data[value] += T(1);\n          }\n        }\n      }\n    } else {\n      Tensor partial_bins_t;\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DataTypeToEnum<T>::value, TensorShape({num_threads, num_bins}),\n          &partial_bins_t));\n      auto partial_bins = partial_bins_t.matrix<T>();\n      partial_bins.setZero();\n      thread_pool->ParallelForWithWorkerId(\n          arr_size, 8 /* cost */,\n          [&](int64_t start_ind, int64_t limit_ind, int64_t worker_id) {\n            if (weights.size()) {\n              for (int64_t i = start_ind; i < limit_ind; i++) {\n                Tidx value = arr_data[i];\n                if (value < num_bins) {\n                  partial_bins(worker_id, value) += weight_data[i];\n                }\n              }\n            } else {\n              for (int64_t i = start_ind; i < limit_ind; i++) {\n                Tidx value = arr_data[i];\n                if (value < num_bins) {\n                  // Complex numbers don't support \"++\".\n                  partial_bins(worker_id, value) += T(1);\n                }\n              }\n            }\n          });\n\n      // Sum the partial bins along the 0th axis.\n      Eigen::array<int, 1> reduce_dim({0});\n      output.device(context->eigen_cpu_device()) = partial_bins.sum(reduce_dim);\n    }\n    return OkStatus();\n  }\n};\n\ntemplate <typename Tidx, typename T, bool binary_output>\nstruct BincountReduceFunctor<CPUDevice, Tidx, T, binary_output> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 2>::ConstTensor& in,\n                        const typename TTypes<T, 2>::ConstTensor& weights,\n                        typename TTypes<T, 2>::Tensor& out,\n                        const Tidx num_bins) {\n    const int num_rows = out.dimension(0);\n    const int num_cols = in.dimension(1);\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    thread_pool->ParallelForWithWorkerId(\n        num_rows, 8 /* cost */,\n        [&](int64_t start_row, int64_t end_row, int64_t worker_id) {\n          for (int64_t i = start_row; i < end_row; ++i) {\n            for (int64_t j = 0; j < num_cols; ++j) {\n              Tidx value = in(i, j);\n              if (value < num_bins) {\n                if (binary_output) {\n                  out(i, value) = T(1);\n                } else {\n                  if (weights.size()) {\n                    out(i, value) += weights(i, j);\n                  } else {\n                    out(i, value) += T(1);\n                  }\n                }\n              }\n            }\n          }\n        });\n    return OkStatus();\n  }\n};\n\n}  // namespace functor\n\ntemplate <typename Device, typename T>\nclass BincountOp : public OpKernel {\n public:\n  explicit BincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& arr_t = ctx->input(0);\n    const Tensor& size_tensor = ctx->input(1);\n    OP_REQUIRES(ctx, size_tensor.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_tensor.dims()));\n    int32_t size = size_tensor.scalar<int32_t>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    const Tensor& weights_t = ctx->input(2);\n    const auto arr = arr_t.flat<int32_t>();\n    const auto weights = weights_t.flat<T>();\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({size}), &output_t));\n    auto output = output_t->flat<T>();\n    OP_REQUIRES_OK(ctx,\n                   functor::BincountFunctor<Device, int32_t, T, false>::Compute(\n                       ctx, arr, weights, output, size));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(\"Bincount\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      BincountOp<CPUDevice, type>)\n\nTF_CALL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"Bincount\")                \\\n                              .Device(DEVICE_GPU)         \\\n                              .HostMemory(\"size\")         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          BincountOp<GPUDevice, type>)\n\nTF_CALL_int32(REGISTER_KERNELS);\nTF_CALL_float(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename Tidx, typename T>\nclass DenseBincountOp : public OpKernel {\n public:\n  explicit DenseBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n    if (std::is_same<Device, GPUDevice>::value) {\n      OP_REQUIRES(\n          ctx, !OpDeterminismRequired(),\n          errors::Unimplemented(\n              \"Determinism is not yet supported in GPU implementation of \"\n              \"DenseBincount.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& data = ctx->input(0);\n    OP_REQUIRES(ctx, data.dims() <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", data.dims()));\n\n    const Tensor& size_t = ctx->input(1);\n    const Tensor& weights = ctx->input(2);\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    OP_REQUIRES(ctx,\n                weights.shape() == data.shape() || weights.NumElements() == 0,\n                errors::InvalidArgument(\n                    \"`weights` must be the same shape as `arr` or a length-0 \"\n                    \"`Tensor`, in which case it acts as all weights equal to \"\n                    \"1. Received \",\n                    weights.shape().DebugString()));\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (data.dims() == 1) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      }\n    } else if (data.dims() == 2) {\n      const int64_t num_rows = data.dim_size(0);\n      auto weight_matrix =\n          (weights.NumElements() == 0)\n              ? weights.shaped<T, 2>(gtl::InlinedVector<int64_t, 2>(2, 0))\n              : weights.matrix<T>();\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountReduceFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx,\n            functor::BincountReduceFunctor<Device, Tidx, T, false>::Compute(\n                ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"DenseBincount\")              \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          DenseBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"DenseBincount\")              \\\n                              .Device(DEVICE_GPU)            \\\n                              .HostMemory(\"size\")            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          DenseBincountOp<GPUDevice, Tidx, T>);\n#define REGISTER_GPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_int32(REGISTER_GPU_KERNELS);\nTF_CALL_float(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename Tidx, typename T>\nclass SparseBincountOp : public OpKernel {\n public:\n  explicit SparseBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const Tensor& values = ctx->input(1);\n    const auto values_flat = values.flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n    OP_REQUIRES_OK(\n        ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n                 indices, values, dense_shape, /*validate_indices=*/true));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values_flat, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values_flat, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values_flat(i);\n        OP_REQUIRES(\n            ctx, batch < out.dimension(0),\n            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n                                    \") must be less than the dimension size (\",\n                                    out.dimension(0), \").\"));\n        OP_REQUIRES(\n            ctx, bin < out.dimension(1),\n            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n                                    \") must be less then the dimension size (\",\n                                    out.dimension(1), \").\"));\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseBincount\")             \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          SparseBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\ntemplate <typename Device, typename Tidx, typename T>\nclass RaggedBincountOp : public OpKernel {\n public:\n  explicit RaggedBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const auto splits = ctx->input(0).flat<int64_t>();\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& size_t = ctx->input(2);\n    const auto weights = ctx->input(3).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    OP_REQUIRES(ctx, splits(0) == 0,\n                errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                        splits(0)));\n\n    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n                errors::InvalidArgument(\n                    \"Splits must end with the number of values, got \",\n                    splits(num_rows), \" instead of \", num_values));\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;\n    fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n    const auto out = out_t->matrix<T>();\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      while (idx >= splits(batch_idx)) {\n        batch_idx++;\n      }\n      Tidx bin = values(idx);\n      OP_REQUIRES(ctx, bin >= 0,\n                  errors::InvalidArgument(\"Input must be non-negative\"));\n      if (bin < size) {\n        if (binary_output_) {\n          out(batch_idx - 1, bin) = T(1);\n        } else {\n          T value = (weights_size > 0) ? weights(idx) : T(1);\n          out(batch_idx - 1, bin) += value;\n        }\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedBincount\")             \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          RaggedBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\n}  // end namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for bincount_ops.bincount.\"\"\"\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import bincount_ops\nfrom tensorflow.python.ops import gen_math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.platform import googletest\n\n\nclass BincountTest(test_util.TensorFlowTestCase):\n\n  def test_empty(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=5)),\n          [0, 0, 0, 0, 0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=1)), [0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=0)), [])\n      self.assertEqual(\n          self.evaluate(\n              bincount_ops.bincount([], minlength=0, dtype=np.float32)).dtype,\n          np.float32)\n      self.assertEqual(\n          self.evaluate(\n              bincount_ops.bincount([], minlength=3, dtype=np.float64)).dtype,\n          np.float64)\n\n  def test_values(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([1, 1, 1, 2, 2, 3])),\n          [0, 3, 2, 1])\n      arr = [1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5]\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(arr)), [0, 5, 4, 3, 2, 1])\n      arr += [0, 0, 0, 0, 0, 0]\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(arr)), [6, 5, 4, 3, 2, 1])\n\n      self.assertAllEqual(self.evaluate(bincount_ops.bincount([])), [])\n      self.assertAllEqual(self.evaluate(bincount_ops.bincount([0, 0, 0])), [3])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([5])), [0, 0, 0, 0, 0, 1])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(np.arange(10000))),\n          np.ones(10000))\n\n  def test_maxlength(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([5], maxlength=3)), [0, 0, 0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([1], maxlength=3)), [0, 1])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], maxlength=3)), [])\n\n  def test_random_with_weights(self):\n    num_samples = 10000\n    with self.session():\n      np.random.seed(42)\n      for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n        arr = np.random.randint(0, 1000, num_samples)\n        if dtype == dtypes.int32 or dtype == dtypes.int64:\n          weights = np.random.randint(-100, 100, num_samples)\n        else:\n          weights = np.random.random(num_samples)\n        self.assertAllClose(\n            self.evaluate(bincount_ops.bincount(arr, weights)),\n            np.bincount(arr, weights))\n\n  def test_random_without_weights(self):\n    num_samples = 10000\n    with self.session():\n      np.random.seed(42)\n      for dtype in [np.int32, np.float32]:\n        arr = np.random.randint(0, 1000, num_samples)\n        weights = np.ones(num_samples).astype(dtype)\n        self.assertAllClose(\n            self.evaluate(bincount_ops.bincount(arr, None)),\n            np.bincount(arr, weights))\n\n  @test_util.run_gpu_only\n  @test_util.disable_xla(\"XLA uses scatter and could be non-deterministic\")\n  def test_bincount_determinism_error(self):\n    arr = np.random.randint(0, 1000, size=1000)\n    with test_util.deterministic_ops(), self.assertRaisesRegex(\n        errors_impl.UnimplementedError,\n        \"Determinism is not yet supported in GPU implementation of Bincount.\"):\n      self.evaluate(bincount_ops.bincount(arr, None, axis=None))\n    arr = np.random.randint(0, 1000, size=(100, 100))\n    with test_util.deterministic_ops(), self.assertRaisesRegex(\n        errors_impl.UnimplementedError,\n        \"Determinism is not yet supported in GPU implementation of \"\n        \"DenseBincount.\"):\n      self.evaluate(bincount_ops.bincount(arr, None, axis=-1))\n\n  def test_zero_weights(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(np.arange(1000), np.zeros(1000))),\n          np.zeros(1000))\n\n  @test_util.disable_xla(\"This is not raised on XLA CPU\")\n  def test_negative(self):\n    # unsorted_segment_sum will only report InvalidArgumentError on CPU\n    with self.cached_session(), ops.device(\"/CPU:0\"):\n      with self.assertRaises(errors.InvalidArgumentError):\n        self.evaluate(bincount_ops.bincount([1, 2, 3, -1, 6, 8]))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_shape_function(self):\n    # size must be scalar.\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"Shape must be rank 0 but is rank 1(?s).*Bincount\"):\n      gen_math_ops.bincount([1, 2, 3, 1, 6, 8], [1], [])\n    # size must be positive.\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be non-negative\"):\n      gen_math_ops.bincount([1, 2, 3, 1, 6, 8], -5, [])\n    # if size is a constant then the shape is known.\n    v1 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], 5, [])\n    self.assertAllEqual(v1.get_shape().as_list(), [5])\n    # if size is a placeholder then the shape is unknown.\n    with ops.Graph().as_default():\n      s = array_ops.placeholder(dtype=dtypes.int32)\n      v2 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], s, [])\n      self.assertAllEqual(v2.get_shape().as_list(), [None])\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    binary_output = True\n    inp = random_ops.random_uniform(\n        shape=[10, 10],\n        minval=-10000,\n        maxval=10000,\n        dtype=dtypes.int32,\n        seed=-2460)\n    size = random_ops.random_uniform(\n        shape=[], minval=-10000, maxval=10000, dtype=dtypes.int32, seed=-10000)\n    weights = random_ops.random_uniform(\n        shape=[],\n        minval=-10000,\n        maxval=10000,\n        dtype=dtypes.float32,\n        seed=-10000)\n    with self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(\n          gen_math_ops.dense_bincount(\n              input=inp,\n              size=size,\n              weights=weights,\n              binary_output=binary_output))\n\n\nclass BincountOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    size = 1000\n    inp = np.random.randint(0, size, (4096), dtype=dtype)\n    np_out = np.bincount(inp, minlength=size)\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(input=inp, weights=[], size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    size = 1000\n    inp = np.random.randint(0, size, (4096,), dtype=dtype)\n    np_weight = np.random.random((4096,))\n    np_out = np.bincount(inp, minlength=size, weights=np_weight)\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    size = 10\n    inp = np.random.randint(0, size, (4096), dtype=dtype)\n    np_out = np.ones((size,))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=[], size=size, binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_binary_with_weights(self, dtype):\n    np.random.seed(42)\n    size = 10\n    inp = np.random.randint(0, size, (4096,), dtype=dtype)\n    np_weight = np.random.random((4096,))\n    np_out = np.ones((size,))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size, binary_output=True)))\n\n  def _test_bincount_col_count(self, num_rows, num_cols, size, dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(input=inp, weights=[], size=size)))\n\n  def _test_bincount_col_binary(self, num_rows, num_cols, size, dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=[], size=size, binary_output=True)))\n\n  def _test_bincount_col_count_with_weights(self, num_rows, num_cols, size,\n                                            dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_weight = np.random.random((num_rows, num_cols))\n    np_out = np.reshape(\n        np.concatenate([\n            np.bincount(inp[j, :], weights=np_weight[j, :], minlength=size)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      evaluated = self.evaluate(\n          gen_math_ops.dense_bincount(input=inp, weights=np_weight, size=size))\n      if np_out.dtype in (np.float32, np.float64):\n        self.assertAllClose(np_out, evaluated)\n      else:\n        self.assertAllEqual(np_out, evaluated)\n\n  def test_col_reduce_basic(self):\n    with test_util.use_gpu():\n      v = self.evaluate(\n          gen_math_ops.dense_bincount(\n              input=[[1, 2, 3], [0, 3, 2]], weights=[], size=4))\n    expected_out = [[0., 1., 1., 1.], [1., 0., 1., 1.]]\n    self.assertAllEqual(expected_out, v)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_shared_memory(self, dtype):\n    # num_rows * num_bins less than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 10\n    self._test_bincount_col_count(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_global_memory(self, dtype):\n    # num_rows * num_bins more than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 1024\n    self._test_bincount_col_count(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_shared_memory_with_weights(self, dtype):\n    # num_rows * num_bins less than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    self._test_bincount_col_count_with_weights(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_global_memory_with_weights(self, dtype):\n    # num_rows * num_bins more than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 1024\n    self._test_bincount_col_count_with_weights(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_binary(self, dtype):\n    num_rows = 128\n    num_cols = 7\n    size = 10\n    self._test_bincount_col_binary(num_rows, num_cols, size, dtype)\n\n  def test_invalid_rank(self):\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"at most rank 2\"):\n      with test_util.use_gpu():\n        self.evaluate(\n            gen_math_ops.dense_bincount(\n                input=[[[1, 2, 3], [0, 3, 2]]], weights=[], size=10))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.dense_bincount(\n              input=[0], size=[1, 1], weights=[3], binary_output=False))\n\n\nclass SparseBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n\n    np_out = np.bincount(inp_vals, minlength=size)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[])))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n\n    np_out = np.bincount(inp_vals, minlength=size, weights=inp_weight)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[],\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_binary_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight,\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_col_reduce_count(self, dtype):\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    # from_dense will filter out 0s.\n    inp = inp + 1\n    # from_dense will cause OOM in GPU.\n    with ops.device(\"/CPU:0\"):\n      inp_sparse = sparse_ops.from_dense(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_sparse.indices,\n                values=inp_sparse.values - 1,\n                dense_shape=inp_sparse.dense_shape,\n                size=size,\n                weights=[])))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_col_reduce_binary(self, dtype):\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    # from_dense will filter out 0s.\n    inp = inp + 1\n    # from_dense will cause OOM in GPU.\n    with ops.device(\"/CPU:0\"):\n      inp_sparse = sparse_ops.from_dense(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_sparse.indices,\n                values=inp_sparse.values - 1,\n                dense_shape=inp_sparse.dense_shape,\n                size=size,\n                weights=[],\n                binary_output=True)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.sparse_bincount(\n              indices=[[0], [1]],\n              values=[0, 0],\n              dense_shape=[1, 1],\n              size=[1, 1],\n              weights=[0, 0],\n              binary_output=False))\n\n  def test_sparse_bincount_input_validation(self):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n    inp_vals = np.random.randint(0, size, (n_elems,))\n\n    # Insert negative index.\n    inp_indices[10, 0] = -2\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"out of bounds\"):\n      self.evaluate(\n          gen_math_ops.sparse_bincount(\n              indices=inp_indices,\n              values=inp_vals,\n              dense_shape=[num_rows],\n              size=size,\n              weights=[]))\n\n\nclass RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 2, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=6)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_binary(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=[],\n                size=6,\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_with_weights(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    weights = ragged_factory_ops.constant([[], [], [.1, .2, .3], [],\n                                           [.2, .5, .6, .3]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0],\n                       [.2, .3, 0, .1, 0, 0], [0, 0, 0, 0, 0, 0],\n                       [.5, 0, 0, 0, .9, .2]]\n    self.assertAllClose(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=weights.values,\n                size=6)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_np(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_np_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_weight = np.random.random((num_rows, num_cols))\n    np_out = np.reshape(\n        np.concatenate([\n            np.bincount(inp[j, :], weights=np_weight[j, :], minlength=size)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=np_weight,\n                size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_binary_np_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=[],\n                size=size,\n                binary_output=True)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.ragged_bincount(\n              splits=[0, 0, 1],\n              values=[1],\n              size=[1, 1],\n              weights=[0, 0, 0],\n              binary_output=False,\n              name=None))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "filenames": ["tensorflow/compiler/tf2xla/kernels/bincount_op.cc", "tensorflow/core/kernels/bincount_op.cc", "tensorflow/python/kernel_tests/math_ops/bincount_op_test.py"], "buggy_code_start_loc": [82, 282, 26], "buggy_code_end_loc": [82, 282, 154], "fixing_code_start_loc": [83, 283, 27], "fixing_code_end_loc": [94, 291, 181], "type": "CWE-617", "message": "TensorFlow is an open source platform for machine learning. `DenseBincount` assumes its input tensor `weights` to either have the same shape as its input tensor `input` or to be length-0. A different `weights` shape will trigger a `CHECK` fail that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit bf4c14353c2328636a18bfad1e151052c81d5f43. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35987", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T22:15:11.547", "lastModified": "2022-09-20T14:54:26.550", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. `DenseBincount` assumes its input tensor `weights` to either have the same shape as its input tensor `input` or to be length-0. A different `weights` shape will trigger a `CHECK` fail that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit bf4c14353c2328636a18bfad1e151052c81d5f43. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. \"DenseBincount\" asume que su tensor de entrada \"pesos\" presenta la misma forma que su tensor de entrada \"input\" o es de longitud 0. Una forma diferente de \"weights\" desencadenar\u00e1 un fallo de \"CHECK\" que puede ser usado para desencadenar un ataque de denegaci\u00f3n de servicio. Hemos parcheado el problema en el commit bf4c14353c2328636a18bfad1e151052c81d5f43 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C6622D95-1C86-45C5-AB55-E6EEEA0996DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/bf4c14353c2328636a18bfad1e151052c81d5f43", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-w62h-8xjm-fv49", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/bf4c14353c2328636a18bfad1e151052c81d5f43"}}