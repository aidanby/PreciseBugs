{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <cmath>\n#include <random>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/util/guarded_philox_random.h\"\n#include \"tensorflow/core/util/overflow.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nclass FractionalAvgPoolOp : public OpKernel {\n public:\n  explicit FractionalAvgPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\n                    \"pooling_ratio field must specify 4 dimensions\"));\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional average pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n      OP_REQUIRES(\n          context, pooling_ratio_[i] <= input_size[i],\n          errors::InvalidArgument(\n              \"Pooling ratio cannot be bigger than input tensor dim size.\"));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> row_cum_seq;\n    std::vector<int64_t> col_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    row_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                          &generator, pseudo_random_);\n    col_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                          &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_row_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     1, TensorShape({static_cast<int64_t>(row_cum_seq.size())}),\n                     &output_row_seq_tensor));\n    Tensor* output_col_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     2, TensorShape({static_cast<int64_t>(col_cum_seq.size())}),\n                     &output_col_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    // out_count corresponds to number of elements in each pooling cell.\n    Eigen::Matrix<T, Eigen::Dynamic, 1> out_count(out_mat.cols());\n\n    // Initializes the output tensor and out_count with 0.\n    out_mat.setZero();\n    out_count.setZero();\n\n    auto output_row_seq_flat = output_row_seq_tensor->flat<int64_t>();\n    auto output_col_seq_flat = output_col_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < row_cum_seq.size(); ++i) {\n      output_row_seq_flat(i) = row_cum_seq[i];\n    }\n\n    for (int i = 0; i < col_cum_seq.size(); ++i) {\n      output_col_seq_flat(i) = col_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: row / row\n    // 2: col / col\n    // 3: depth / channel\n    const int64_t row_max = input_size[1] - 1;\n    const int64_t col_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // row sequence.\n      for (int64_t hs = 0; hs < row_cum_seq.size() - 1; ++hs) {\n        // row start and end.\n        const int64_t row_start = row_cum_seq[hs];\n        int64_t row_end =\n            overlapping_ ? row_cum_seq[hs + 1] : row_cum_seq[hs + 1] - 1;\n        row_end = std::min(row_end, row_max);\n\n        // col sequence.\n        for (int64_t ws = 0; ws < col_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // col start and end.\n          const int64_t col_start = col_cum_seq[ws];\n          int64_t col_end =\n              overlapping_ ? col_cum_seq[ws + 1] : col_cum_seq[ws + 1] - 1;\n          col_end = std::min(col_end, col_max);\n          for (int64_t h = row_start; h <= row_end; ++h) {\n            for (int64_t w = col_start; w <= col_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) += in_mat.col(in_offset);\n              out_count(out_offset)++;\n            }\n          }\n        }\n      }\n    }\n    DCHECK_GT(out_count.minCoeff(), 0);\n    out_mat.array().rowwise() /= out_count.transpose().array();\n  }\n\n private:\n  bool deterministic_;\n  int64_t seed_;\n  int64_t seed2_;\n  std::vector<float> pooling_ratio_;\n  bool pseudo_random_;\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALAVGPOOL(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"FractionalAvgPool\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      FractionalAvgPoolOp<type>)\n\nREGISTER_FRACTIONALAVGPOOL(int32);\nREGISTER_FRACTIONALAVGPOOL(int64_t);\nREGISTER_FRACTIONALAVGPOOL(float);\nREGISTER_FRACTIONALAVGPOOL(double);\n\n#undef REGISTER_FRACTIONALAVGPOOL\n\ntemplate <class T>\nclass FractionalAvgPoolGradOp : public OpKernel {\n public:\n  explicit FractionalAvgPoolGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Here's the basic idea:\n    // Batch and depth dimension are independent from row and col dimension. And\n    // because FractionalAvgPool currently only support pooling along row and\n    // col, we can basically think of this 4D tensor backpropagation as\n    // operation of a series of 2D planes.\n    //\n    // For each element of a 'slice' (2D plane) of output_backprop, we need to\n    // figure out its contributors when doing FractionalAvgPool operation. This\n    // can be done based on row_pooling_sequence, col_pooling_seq and\n    // overlapping.\n    // Once we figure out the original contributors, we just need to evenly\n    // divide the value of this element among these contributors.\n    //\n    // Internally, we divide the out_backprop tensor and store it in a temporary\n    // tensor of double type. And cast it to the corresponding type.\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenDoubleMatrixMap;\n\n    // Grab the inputs.\n    const Tensor& orig_input_tensor_shape = context->input(0);\n    OP_REQUIRES(context,\n                orig_input_tensor_shape.dims() == 1 &&\n                    orig_input_tensor_shape.NumElements() == 4,\n                errors::InvalidArgument(\"original input tensor shape must be\"\n                                        \"1-dimensional and 4 elements\"));\n    int64_t num_elements = 1;\n    for (int i = 0; i < orig_input_tensor_shape.dims(); i++) {\n      OP_REQUIRES(context, orig_input_tensor_shape.dim_size(i) > 0,\n                  errors::InvalidArgument(\n                      \"orig_input_tensor_shape must be positive, got: \",\n                      orig_input_tensor_shape.dim_size(i)));\n      num_elements = MultiplyWithoutOverflow(\n          num_elements, orig_input_tensor_shape.dim_size(i));\n      OP_REQUIRES(\n          context, num_elements > 0,\n          errors::InvalidArgument(\n              \"The total elements specified by orig_input_tensor_shape\",\n              \" is too large. Encountered overflow after multiplying \",\n              orig_input_tensor_shape.dim_size(i), \", result: \", num_elements));\n    }\n\n    const Tensor& out_backprop = context->input(1);\n    OP_REQUIRES(context, out_backprop.dims() == 4,\n                errors::InvalidArgument(\"out_backprop must be 4-dimensional\"));\n    for (int i = 0; i < out_backprop.dims(); i++) {\n      OP_REQUIRES(context, out_backprop.dim_size(i) > 0,\n                  errors::InvalidArgument(\n                      \"out_backprop must be positive for all dimension, got:\",\n                      out_backprop.dim_size(i)));\n    }\n\n    const Tensor& row_seq_tensor = context->input(2);\n    const Tensor& col_seq_tensor = context->input(3);\n\n    const int64_t out_batch = out_backprop.dim_size(0);\n    const int64_t out_rows = out_backprop.dim_size(1);\n    const int64_t out_cols = out_backprop.dim_size(2);\n    const int64_t out_depth = out_backprop.dim_size(3);\n\n    OP_REQUIRES(context, row_seq_tensor.NumElements() > out_rows,\n                errors::InvalidArgument(\"Given out_backprop shape \",\n                                        out_backprop.shape().DebugString(),\n                                        \", row_seq_tensor must have at least \",\n                                        out_rows + 1, \" elements, but got \",\n                                        row_seq_tensor.NumElements()));\n    OP_REQUIRES(context, col_seq_tensor.NumElements() > out_cols,\n                errors::InvalidArgument(\"Given out_backprop shape \",\n                                        out_backprop.shape().DebugString(),\n                                        \", col_seq_tensor must have at least \",\n                                        out_cols + 1, \" elements, but got \",\n                                        col_seq_tensor.NumElements()));\n\n    auto row_seq_tensor_flat = row_seq_tensor.flat<int64_t>();\n    auto col_seq_tensor_flat = col_seq_tensor.flat<int64_t>();\n    auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64_t>();\n\n    const int64_t in_batch = orig_input_tensor_shape_flat(0);\n    const int64_t in_rows = orig_input_tensor_shape_flat(1);\n    const int64_t in_cols = orig_input_tensor_shape_flat(2);\n    const int64_t in_depth = orig_input_tensor_shape_flat(3);\n    OP_REQUIRES(\n        context, in_batch != 0,\n        errors::InvalidArgument(\"Batch dimension of input must not be 0\"));\n    OP_REQUIRES(\n        context, in_rows != 0,\n        errors::InvalidArgument(\"Rows dimension of input must not be 0\"));\n    OP_REQUIRES(\n        context, in_cols != 0,\n        errors::InvalidArgument(\"Columns dimension of input must not be 0\"));\n    OP_REQUIRES(\n        context, in_depth != 0,\n        errors::InvalidArgument(\"Depth dimension of input must not be 0\"));\n\n    constexpr int tensor_in_and_out_dims = 4;\n    // Transform orig_input_tensor_shape into TensorShape\n    TensorShape in_shape;\n    for (auto i = 0; i < tensor_in_and_out_dims; ++i) {\n      in_shape.AddDim(orig_input_tensor_shape_flat(i));\n    }\n\n    // Create intermediate in_backprop.\n    Tensor in_backprop_tensor_temp;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_temp(\n                                {0}, DataTypeToEnum<double>::v(), in_shape,\n                                &in_backprop_tensor_temp));\n    in_backprop_tensor_temp.flat<double>().setZero();\n    // Transform 4D tensor to 2D matrix.\n    EigenDoubleMatrixMap in_backprop_tensor_temp_mat(\n        in_backprop_tensor_temp.flat<double>().data(), in_depth,\n        in_cols * in_rows * in_batch);\n    ConstEigenMatrixMap out_backprop_mat(out_backprop.flat<T>().data(),\n                                         out_depth,\n                                         out_cols * out_rows * out_batch);\n    // Loop through each element of out_backprop and evenly distribute the\n    // element to the corresponding pooling cell.\n    const int64_t in_max_row_index = in_rows - 1;\n    const int64_t in_max_col_index = in_cols - 1;\n    for (int64_t b = 0; b < out_batch; ++b) {\n      for (int64_t r = 0; r < out_rows; ++r) {\n        const int64_t in_row_start = row_seq_tensor_flat(r);\n\n        int64_t in_row_end = overlapping_ ? row_seq_tensor_flat(r + 1)\n                                          : row_seq_tensor_flat(r + 1) - 1;\n        in_row_end = std::min(in_row_end, in_max_row_index);\n        OP_REQUIRES(context, in_row_start >= 0 && in_row_end >= 0,\n                    errors::InvalidArgument(\n                        \"Row sequence tensor values must not be negative, got \",\n                        row_seq_tensor_flat));\n\n        for (int64_t c = 0; c < out_cols; ++c) {\n          const int64_t in_col_start = col_seq_tensor_flat(c);\n          int64_t in_col_end = overlapping_ ? col_seq_tensor_flat(c + 1)\n                                            : col_seq_tensor_flat(c + 1) - 1;\n          in_col_end = std::min(in_col_end, in_max_col_index);\n\n          OP_REQUIRES(\n              context, in_col_start >= 0 && in_col_end >= 0,\n              errors::InvalidArgument(\n                  \"Column sequence tensor values must not be negative, got \",\n                  col_seq_tensor_flat));\n          const int64_t num_elements_in_pooling_cell =\n              (in_row_end - in_row_start + 1) * (in_col_end - in_col_start + 1);\n          const int64_t out_index = (b * out_rows + r) * out_cols + c;\n          // Now we can evenly distribute out_backprop(b, h, w, *) to\n          // in_backprop(b, hs:he, ws:we, *).\n          for (int64_t in_r = in_row_start; in_r <= in_row_end; ++in_r) {\n            for (int64_t in_c = in_col_start; in_c <= in_col_end; ++in_c) {\n              const int64_t in_index = (b * in_rows + in_r) * in_cols + in_c;\n              // Walk through each channel (depth).\n              for (int64_t d = 0; d < out_depth; ++d) {\n                const double out_backprop_element = static_cast<double>(\n                    out_backprop_mat.coeffRef(d, out_index));\n                double& in_backprop_ref =\n                    in_backprop_tensor_temp_mat.coeffRef(d, in_index);\n                in_backprop_ref +=\n                    out_backprop_element / num_elements_in_pooling_cell;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // Depending on the type, cast double to type T.\n    Tensor* in_backprop_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, in_shape, &in_backprop_tensor));\n    auto in_backprop_tensor_flat = in_backprop_tensor->flat<T>();\n    auto in_backprop_tensor_temp_flat = in_backprop_tensor_temp.flat<double>();\n    for (int64_t i = 0; i < in_backprop_tensor_flat.size(); ++i) {\n      in_backprop_tensor_flat(i) =\n          static_cast<T>(in_backprop_tensor_temp_flat(i));\n    }\n  }\n\n private:\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALAVGPOOLGRAD(type)              \\\n  REGISTER_KERNEL_BUILDER(Name(\"FractionalAvgPoolGrad\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          FractionalAvgPoolGradOp<type>)\n\nREGISTER_FRACTIONALAVGPOOLGRAD(int32);\nREGISTER_FRACTIONALAVGPOOLGRAD(int64_t);\nREGISTER_FRACTIONALAVGPOOLGRAD(float);\nREGISTER_FRACTIONALAVGPOOLGRAD(double);\n\n#undef REGISTER_FRACTIONALAVGPOOLGRAD\n}  // namespace tensorflow\n", "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <cmath>\n#include <random>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/util/guarded_philox_random.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nclass FractionalMaxPoolOp : public OpKernel {\n public:\n  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n\n      OP_REQUIRES(\n          context, input_size[i] >= pooling_ratio_[i],\n          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n                                  \"dimension size for dimension \",\n                                  i, \". Input dim size: \", input_size[i],\n                                  \" pooling ratio: \", pooling_ratio_[i]));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      // This must match the same logic in the shape function in\n      // core/ops/nn_ops.cc.\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n\n    // Initializes the output tensor with MIN<T>.\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: height / row\n    // 2: width / col\n    // 3: depth / channel\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // height sequence.\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        // height start and end.\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n\n        // width sequence.\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // width start and end.\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }\n\n private:\n  bool deterministic_;\n  int64_t seed_;\n  int64_t seed2_;\n  std::vector<float> pooling_ratio_;\n  bool pseudo_random_;\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALMAXPOOL(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"FractionalMaxPool\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      FractionalMaxPoolOp<type>)\n\nREGISTER_FRACTIONALMAXPOOL(int32);\nREGISTER_FRACTIONALMAXPOOL(int64_t);\nREGISTER_FRACTIONALMAXPOOL(float);\nREGISTER_FRACTIONALMAXPOOL(double);\n\n#undef REGISTER_FRACTIONALMAXPOOL\n\nstatic const int kInvalidMaxPoolingIndex = -1;\n\ntemplate <class T>\nclass FractionalMaxPoolGradOp : public OpKernel {\n public:\n  explicit FractionalMaxPoolGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // There are two steps when calculating gradient for FractionalMaxPool.\n    // 1) Walk through the process of calculating fractional pooling given\n    //    pooling region; however, in the process, keep track of where the max\n    //    element comes from. (arg_max)\n    // 2) Populate the value of out_backprop to where arg_max indicates. If\n    //    we support overlapping, it is likely to have multiple out_backprop[i]\n    //    propagates back to the same arg_max value.\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<int64, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenIndexMatrixMap;\n\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    const Tensor& height_seq_tensor = context->input(3);\n    const Tensor& width_seq_tensor = context->input(4);\n\n    // Just to make it similar to FractionalMaxPoolOp.\n    constexpr int tensor_in_and_out_dims = 4;\n    OP_REQUIRES(\n        context, tensor_in.dims() == tensor_in_and_out_dims,\n        errors::InvalidArgument(\"orig_input should be a tensor of rank 4, got \",\n                                tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"orig_input must not be empty, got \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\n                    \"orig_output should be a tensor of rank 4, got \",\n                    tensor_out.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"orig_output must not be empty, got \",\n                                        tensor_out.DebugString()));\n    OP_REQUIRES(\n        context,\n        height_seq_tensor.NumElements() * width_seq_tensor.NumElements() <=\n            tensor_in.NumElements(),\n        errors::InvalidArgument(\n            \"Pooling region has more elements than the input tensor. \"\n            \"row_pooling_sequence: \",\n            height_seq_tensor.DebugString(),\n            \"col_pooling_sequence: \", width_seq_tensor.DebugString(),\n            \"orig_input: \", tensor_in.DebugString()));\n\n    //\n    std::vector<int64_t> input_size(tensor_in_and_out_dims);\n    std::vector<int64_t> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] = tensor_out.dim_size(i);\n    }\n\n    // ---------\n    // Step 1\n    // ---------\n    Tensor tensor_out_dup;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_temp(\n                                {1}, DataTypeToEnum<T>::v(), tensor_out.shape(),\n                                &tensor_out_dup));\n    Tensor tensor_out_arg_max;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<int64_t>::v(),\n                                                   tensor_out.shape(),\n                                                   &tensor_out_arg_max));\n    // Find arg_max for each tensor_out\n    ConstEigenMatrixMap tensor_in_mat(\n        tensor_in.flat<T>().data(), input_size[3],\n        input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap tensor_out_dup_mat(\n        tensor_out_dup.flat<T>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n    EigenIndexMatrixMap tensor_out_arg_max_mat(\n        tensor_out_arg_max.flat<int64_t>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n\n    tensor_out_arg_max.flat<int64_t>().setConstant(kInvalidMaxPoolingIndex);\n    // Initializes the duplicate output tensor with MIN<T>.\n    tensor_out_dup.flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n\n    auto height_seq_tensor_flat = height_seq_tensor.flat<int64_t>();\n    auto width_seq_tensor_flat = width_seq_tensor.flat<int64_t>();\n\n    // Now walk through the process of fractional max pooling again.\n    // For both input and output,\n    // 0: batch\n    // 1: height / row\n    // 2: width / col\n    // 3: depth / channel\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // height sequence.\n      for (int64_t hs = 0; hs < height_seq_tensor.dim_size(0) - 1; ++hs) {\n        // height start and end.\n        const int64_t height_start = height_seq_tensor_flat(hs);\n        int64_t height_end = overlapping_ ? height_seq_tensor_flat(hs + 1)\n                                          : height_seq_tensor_flat(hs + 1) - 1;\n        height_end = std::min(height_end, height_max);\n\n        // width sequence.\n        for (int64_t ws = 0; ws < width_seq_tensor.dim_size(0) - 1; ++ws) {\n          const int64_t out_index =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // width start and end.\n          const int64_t width_start = width_seq_tensor_flat(ws);\n          int64_t width_end = overlapping_ ? width_seq_tensor_flat(ws + 1)\n                                           : width_seq_tensor_flat(ws + 1) - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_index =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              // Walk through each channel (depth).\n              for (int64_t d = 0; d < input_size[3]; ++d) {\n                const T& input_ref = tensor_in_mat.coeffRef(d, in_index);\n                T& output_ref = tensor_out_dup_mat.coeffRef(d, out_index);\n                int64_t& out_arg_max_ref =\n                    tensor_out_arg_max_mat.coeffRef(d, out_index);\n                if (output_ref < input_ref ||\n                    out_arg_max_ref == kInvalidMaxPoolingIndex) {\n                  output_ref = input_ref;\n                  int input_offset = in_index * input_size[3] + d;\n                  out_arg_max_ref = input_offset;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // Check tensor_out_dup is the same as tensor_out.\n    ConstEigenMatrixMap tensor_out_mat(\n        tensor_out.flat<T>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n    const int64_t num_reshaped_cols =\n        output_size[2] * output_size[1] * output_size[0];\n    for (int64_t i = 0; i < num_reshaped_cols; ++i) {\n      for (int64_t j = 0; j < output_size[3]; ++j) {\n        OP_REQUIRES(context, tensor_out_dup_mat(j, i) == tensor_out_mat(j, i),\n                    errors::InvalidArgument(\n                        \"tensor_out_dup is not the same as tensor_out\"));\n      }\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, tensor_in.shape(), &output));\n    output->flat<T>().setZero();\n\n    auto out_backprop_flat = out_backprop.flat<T>();\n    auto input_backprop_flat = output->flat<T>();\n    auto out_arg_max_flat = tensor_out_arg_max.flat<int64_t>();\n    int num_total_outputs = out_backprop_flat.size();\n    int num_total_inputs = input_backprop_flat.size();\n\n    for (int index = 0; index < num_total_outputs; ++index) {\n      int input_backprop_index = out_arg_max_flat(index);\n      OP_REQUIRES(\n          context,\n          input_backprop_index >= 0 && input_backprop_index < num_total_inputs,\n          errors::InvalidArgument(\n              \"Invalid input backprop index: \", input_backprop_index, \", \",\n              num_total_inputs));\n      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n    }\n  }\n\n private:\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALMAXPOOLGRAD(type)              \\\n  REGISTER_KERNEL_BUILDER(Name(\"FractionalMaxPoolGrad\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          FractionalMaxPoolGradOp<type>)\n\nREGISTER_FRACTIONALMAXPOOLGRAD(int32);\nREGISTER_FRACTIONALMAXPOOLGRAD(int64_t);\nREGISTER_FRACTIONALMAXPOOLGRAD(float);\nREGISTER_FRACTIONALMAXPOOLGRAD(double);\n\n#undef REGISTER_FRACTIONALMAXPOOLGRAD\n}  // namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <cmath>\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/lib/math/math_util.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nnamespace {\n\nStatus FractionalPoolShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  std::vector<float> pooling_ratio;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"pooling_ratio\", &pooling_ratio));\n  if (pooling_ratio.size() != 4) {\n    return errors::InvalidArgument(\n        \"pooling_ratio field must specify 4 dimensions\");\n  }\n  std::vector<DimensionHandle> output_dims;\n  for (int i = 0; i < 4; ++i) {\n    DimensionHandle d = c->Dim(input, i);\n    if (c->ValueKnown(d)) {\n      // This must match the same logic in the kernel function in\n      // core/kernels/fractional_max_pool_op.cc.\n      auto val =\n          static_cast<int64_t>(std::floor(c->Value(d) / pooling_ratio[i]));\n      if (val < 0) {\n        return errors::InvalidArgument(\"Size computed for dim \", i,\n                                       \" is negative: \", val);\n      }\n      output_dims.push_back(c->MakeDim(val));\n    } else {\n      output_dims.push_back(c->UnknownDim());\n    }\n  }\n\n  c->set_output(0, c->MakeShape(output_dims));\n  c->set_output(1, c->Vector(output_dims[1]));\n  c->set_output(2, c->Vector(output_dims[2]));\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolShape);\n\nREGISTER_OP(\"AvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BatchNormWithGlobalNormalization\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"beta: T\")\n    .Input(\"gamma: T\")\n    .Output(\"result: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"BatchNormWithGlobalNormalizationGrad\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"gamma: T\")\n    .Input(\"backprop: T\")\n    .Output(\"dx: T\")\n    .Output(\"dm: T\")\n    .Output(\"dv: T\")\n    .Output(\"db: T\")\n    .Output(\"dg: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n      TF_RETURN_IF_ERROR(\n          c->Merge(input, c->input(4), &input));  // with backprop\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 4; ++i) {  // covers m, v, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle dx;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &dx));\n      c->set_output(0, dx);\n\n      ShapeHandle vector_shape = c->Vector(last_dim);\n      c->set_output(1, vector_shape);\n      c->set_output(2, vector_shape);\n      c->set_output(3, vector_shape);\n      c->set_output(4, vector_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV3\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {bfloat16, float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormV3Shape);\n\nREGISTER_OP(\"_FusedBatchNormEx\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"side_input: num_side_inputs * T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, float, bfloat16}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNorm operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n\nREGISTER_OP(\"FusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV3\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_FusedBatchNormGradEx\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Input(\"offset: float\")\n    .Input(\"y: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Output(\"side_input_backprop: num_side_inputs * T\")\n    .Attr(\"T: {half, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNormGrad operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAdd\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddGrad\")\n    .Attr(\"T: numbertype\")\n    .Input(\"out_backprop: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddGradShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddV1\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding);\n\nREGISTER_OP(\"Conv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropInputShape);\n\n// TODO(jeff): Instead of 'use_cudnn_for_gpu', maybe we should have a\n// more general string attribute ('kernel_impl'?) that can be used to\n// select among several possible implementations.\nREGISTER_OP(\"Conv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: TArgs\")\n    .Input(\"host_args : num_host_args * float\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, int8, qint8}\")\n    .Attr(\"TArgs: list(type)\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"num_host_args: int >= 0 =0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: { 'NHWC', 'NCHW', 'NCHW_VECT_C' } = 'NHWC'\")\n    .Attr(\"filter_format: {'HWIO', 'OIHW', 'OIHW_VECT_I'} = 'HWIO'\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nPerforms a convolution followed by a specified series of operations.\n\nThe inputs to the convolution are `input` and `filter`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [X] and [X,A], where X is one of\n{\"BiasAdd\",\"FusedBatchNorm\"} and A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to op X is the Conv2D result, and the additional input(s) to X\nare specified by `args`.\n* If there is an op A specified, the output of op X is the input to op A, and op\nA produces the _FusedConv2D output. Otherwise, op X produces the _FusedConv2D\noutput.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is expected to\ncreate these operators.\n)doc\");\n\nnamespace {\n\nStatus CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  ShapeHandle resized = input;\n  int paddings_index = 1;\n  int filter_index = 2;\n  if (has_resize) {\n    paddings_index = 2;\n    filter_index = 3;\n\n    ShapeHandle unused_size;\n    TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->Vector(2), &unused_size));\n\n    const Tensor* size = c->input_tensor(1);\n    DimensionHandle new_height = c->UnknownDim();\n    DimensionHandle new_width = c->UnknownDim();\n    if (size != nullptr) {\n      new_height = c->MakeDim(size->flat<int32>()(0));\n      new_width = c->MakeDim(size->flat<int32>()(1));\n    }\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 1, new_height, &resized));\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 2, new_width, &resized));\n  }\n\n  ShapeHandle paddings;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(paddings_index), 2, &paddings));\n  TF_RETURN_IF_ERROR(\n      c->WithRank(resized, c->Value(c->Dim(paddings, 0)), &resized));\n  TF_RETURN_IF_ERROR(\n      c->Merge(paddings, c->Matrix(c->Rank(resized), 2), &paddings));\n\n  const Tensor* paddings_t = c->input_tensor(paddings_index);\n  ShapeHandle padded;\n  if (paddings_t != nullptr) {\n    std::vector<DimensionHandle> output_dims;\n    for (int i = 0; i < 4; ++i) {\n      DimensionHandle dim = c->Dim(resized, i);\n      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 0));\n      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 1));\n      if (p0 < 0 || p1 < 0) {\n        return errors::InvalidArgument(\"Paddings must be non-negative\");\n      }\n\n      TF_RETURN_IF_ERROR(c->Add(dim, p0 + p1, &dim));\n      output_dims.push_back(dim);\n    }\n    padded = c->MakeShape(output_dims);\n  } else {\n    padded = c->UnknownShapeOfRank(4);\n  }\n\n  // Work out the convolution's effect with 'padded' as the input.\n  ShapeHandle filter;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(filter_index), 4, &filter));\n  std::vector<int32> strides;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n  if (strides.size() != 4) {\n    return errors::InvalidArgument(\n        \"Operation requires the stride attribute to contain 4 values, but \",\n        \"got: \", strides.size());\n  }\n\n  int32_t stride_rows = strides[1];\n  int32_t stride_cols = strides[2];\n\n  DimensionHandle batch_size_dim = c->Dim(padded, 0);\n  DimensionHandle in_rows_dim = c->Dim(padded, 1);\n  DimensionHandle in_cols_dim = c->Dim(padded, 2);\n  DimensionHandle filter_rows_dim = c->Dim(filter, 0);\n  DimensionHandle filter_cols_dim = c->Dim(filter, 1);\n  DimensionHandle output_depth_dim = c->Dim(filter, 3);\n\n  DimensionHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(c->Dim(padded, 3), c->Dim(filter, 2), &unused));\n\n  Padding padding;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n  DimensionHandle output_rows, output_cols;\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_rows_dim, filter_rows_dim, stride_rows, padding, &output_rows));\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_cols_dim, filter_cols_dim, stride_cols, padding, &output_cols));\n\n  ShapeHandle output_shape = c->MakeShape(\n      {batch_size_dim, output_rows, output_cols, output_depth_dim});\n  c->set_output(0, output_shape);\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"DataFormatDimMap\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"DataFormatVecPermute\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FusedResizeAndPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"size: int32\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"resize_align_corners: bool = false\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, /*has_resize=*/true);\n    });\n\nREGISTER_OP(\"FusedPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, /*has_resize=*/false);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"DepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: num_args * T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape);\n\nREGISTER_OP(\"Conv3DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropInputV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 5);\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropFilterV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 5, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"AvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"MaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape);\n\nREGISTER_OP(\"MaxPool3DGradGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5 \")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Pool3DShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"L2Loss\")\n    .Input(\"t: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LRN\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"LRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::MaxPoolShapeWithExplicitPadding);\n\nREGISTER_OP(\"MaxPoolV2\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 3));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\nREGISTER_OP(\"MaxPoolGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\n// TODO(b/150813181): Implement explicit padding.\nREGISTER_OP(\"MaxPoolGradGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 5));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"Targmax: {int32, int64} = DT_INT64\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Output(\"argmax: Targmax\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      c->set_output(1, c->output(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"MaxPoolGradGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &unused));\n      // Validate 'argmax' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Dilation2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n      ShapeHandle filter_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &filter_shape));\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the stride attribute to contain 4 values, but \"\n            \"got: \",\n            strides.size());\n      }\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the rates attribute to contain 4 values, but \"\n            \"got: \",\n            rates.size());\n      }\n\n      int32_t stride_rows = strides[1];\n      int32_t stride_cols = strides[2];\n\n      int32_t rate_rows = rates[1];\n      int32_t rate_cols = rates[2];\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 2);\n      DimensionHandle filter_rows_dim = c->Dim(filter_shape, 0);\n      DimensionHandle filter_cols_dim = c->Dim(filter_shape, 1);\n      DimensionHandle output_depth_dim = c->Dim(filter_shape, 2);\n\n      if (!c->ValueKnown(in_rows_dim) || !c->ValueKnown(in_cols_dim) ||\n          !c->ValueKnown(filter_rows_dim) || !c->ValueKnown(filter_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(input_shape, 3), output_depth_dim, &unused));\n\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n      auto filter_rows = c->Value(filter_rows_dim);\n      auto filter_cols = c->Value(filter_cols_dim);\n      auto filter_rows_eff = filter_rows + (filter_rows - 1) * (rate_rows - 1);\n      auto filter_cols_eff = filter_cols + (filter_cols - 1) * (rate_cols - 1);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, filter_rows_eff, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, filter_cols_eff, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n\n      ShapeHandle output_shape = c->MakeShape(\n          {batch_size_dim, output_rows, output_cols, output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Dilation2DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"in_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Dilation2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"filter_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Relu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {realnumbertype, qint8}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Relu6\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Relu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"LeakyRelu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"LeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Elu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"EluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Selu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SeluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softplus\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftplusGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softsign\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftsignGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Softmax\")\n    .Input(\"logits: T\")\n    .Output(\"softmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LogSoftmax\")\n    .Input(\"logits: T\")\n    .Output(\"logsoftmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"SoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: T\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      if (c->WithRank(c->input(0), 2, &input) == OkStatus() &&\n          c->Merge(input, c->input(1), &input) == OkStatus()) {\n        DimensionHandle batch_size = c->Dim(input, 0);\n        c->set_output(0, c->Vector(batch_size));\n        c->set_output(1, input);\n        return OkStatus();\n      }\n      TF_RETURN_IF_ERROR(BroadcastBinaryOpOutputShapeFn(c, 1));\n\n      if (!c->RankKnown(c->output(1))) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank is unknown.\");\n      }\n\n      if (c->Rank(c->output(1)) != 2) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank \",\n            c->Rank(c->output(1)));\n      }\n      DimensionHandle batch_size = c->Dim(c->output(1), 0);\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"SparseSoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: Tlabels\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"Tlabels: {int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle features;\n      ShapeHandle labels;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &features));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &labels));\n\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(features, 0), c->Dim(labels, 0), &batch_size));\n      TF_RETURN_IF_ERROR(c->ReplaceDim(features, 0, batch_size, &features));\n\n      c->set_output(0, c->Vector(batch_size));\n      c->set_output(1, features);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"InTopK\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"k: int\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\n// This is the same as `InTopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"InTopKV2\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Input(\"k: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus TopKShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n  // Get the k value, either from input tensor or attribute.\n  DimensionHandle k_dim;\n  if (c->num_inputs() >= 2) {\n    TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &k_dim));\n  } else {\n    int32_t k;\n    TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n    if (k < 0) {\n      return errors::InvalidArgument(\"Need k >= 0, got \", k);\n    }\n    k_dim = c->MakeDim(k);\n  }\n\n  DimensionHandle last_dim = c->Dim(input, -1);\n  if (c->ValueKnown(last_dim) && c->ValueKnown(k_dim) &&\n      c->Value(last_dim) < c->Value(k_dim)) {\n    return errors::InvalidArgument(\n        \"input must have last dimension >= k = \", c->Value(k_dim), \" but is \",\n        c->Value(last_dim));\n  }\n\n  // Replace last_dim with k_dim.\n  ShapeHandle s;\n  TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n  TF_RETURN_IF_ERROR(c->Concatenate(s, c->Vector(k_dim), &s));\n  c->set_output(0, s);\n  c->set_output(1, s);\n  return OkStatus();\n}\n\n// Utility functions for ApproxTopKShape.\n// It is not easy to link xla/client/lib into the tensorflow core lib, so we\n// have to replicate the logic.\n// LINT.IfChange\ninline uint32_t log2_floor(uint64_t value) {\n  return value == 0 ? 0 : Log2Floor(value);\n}\n\ninline uint32_t log2_ceil(uint64_t value) {\n  return value == 0 ? 0 : Log2Ceiling(value);\n}\n\nStatus ApproxTopKShape(shape_inference::InferenceContext* c) {\n  int64_t k;\n  int64_t reduction_dimension;\n  float recall_target;\n  int64_t reduction_input_size_override;\n  bool aggregate_to_topk;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_dimension\", &reduction_dimension));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"recall_target\", &recall_target));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_input_size_override\",\n                                &reduction_input_size_override));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"aggregate_to_topk\", &aggregate_to_topk));\n  ShapeHandle input_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input_shape));\n  if (reduction_dimension < 0) {\n    // Reverse index\n    reduction_dimension += c->Rank(input_shape);\n  }\n  int64_t reduction_dim_value =\n      c->Value(c->Dim(input_shape, reduction_dimension));\n\n  if (reduction_dim_value < k) {\n    return errors::InvalidArgument(\"input must have last dimension >= k = \", k,\n                                   \" but was \", reduction_dim_value);\n  }\n\n  int64_t output_dim_value = [&] {\n    if (aggregate_to_topk) {\n      return k;\n    }\n    int64_t tpu_tiling = c->Rank(input_shape) == 1 ? 1024 : 128;\n    if (reduction_dim_value <= tpu_tiling || recall_target == 1.0) {\n      return reduction_dim_value;\n    }\n    if (k == 1) {\n      return tpu_tiling;\n    }\n    uint64_t logical_input_size = reduction_input_size_override >= 0\n                                      ? reduction_input_size_override\n                                      : reduction_dim_value;\n    uint64_t m = std::min<uint64_t>(\n        std::max<uint64_t>(\n            static_cast<uint64_t>((1.0 - k) /\n                                  std::log(static_cast<double>(recall_target))),\n            tpu_tiling),\n        reduction_dim_value);\n    uint32_t log2_reduction = log2_floor(logical_input_size / m);\n    if (log2_reduction == 0) {\n      return reduction_dim_value;\n    }\n    log2_reduction = std::min<uint32_t>(\n        log2_reduction, log2_ceil(reduction_dim_value / tpu_tiling));\n    return tensorflow::MathUtil::CeilOfRatio<int64_t>(\n               tensorflow::MathUtil::CeilOfRatio<int64_t>(reduction_dim_value,\n                                                          tpu_tiling),\n               (1 << log2_reduction)) *\n           tpu_tiling;\n  }();\n\n  auto output_dim = c->MakeDim(output_dim_value);\n\n  ShapeHandle output_shape;\n  TF_RETURN_IF_ERROR(c->ReplaceDim(input_shape, reduction_dimension, output_dim,\n                                   &output_shape));\n  c->set_output(0, output_shape);\n  c->set_output(1, output_shape);\n  return OkStatus();\n}\n// LINT.ThenChange(//tensorflow/compiler/xla/client/lib/approx_topk_shape.cc)\n\n}  // namespace\n\nREGISTER_OP(\"TopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .Deprecated(7, \"Use TopKV2 instead\")\n    .SetShapeFn(TopKShapeFn);\n\n// This is the same as `TopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"TopKV2\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(TopKShapeFn);\n\nREGISTER_OP(\"ApproxTopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"reduction_dimension: int = -1\")\n    .Attr(\"recall_target: float = 0.95\")\n    .Attr(\"is_max_k: bool = true\")\n    .Attr(\"reduction_input_size_override: int = -1\")\n    .Attr(\"aggregate_to_topk: bool = true\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(ApproxTopKShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"NthElement\")\n    .Input(\"input: T\")\n    .Input(\"n: int32\")\n    .Output(\"values: T\")\n    .Attr(\"reverse: bool = false\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n      // Get the n value from input tensor, and make sure which is a scalar.\n      DimensionHandle n_dim;\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &n_dim));\n\n      // The last dimension of input tensor must be greater than N.\n      DimensionHandle last_dim = c->Dim(input, -1);\n      if (c->ValueKnown(last_dim) && c->ValueKnown(n_dim) &&\n          c->Value(last_dim) <= c->Value(n_dim)) {\n        return errors::InvalidArgument(\n            \"Input must have last dimension > n = \", c->Value(n_dim),\n            \" but is \", c->Value(last_dim));\n      }\n\n      // Reduce last_dim for output tensor\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalMaxPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalMaxPoolGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRank(c, 4);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalAvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalAvgPoolGrad\")\n    .Input(\"orig_input_tensor_shape: int64\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      if (c->input_tensor(0) != nullptr) {\n        ShapeHandle out;\n        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n        c->set_output(0, out);\n      } else {\n        c->set_output(0, c->UnknownShapeOfRank(4));\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedAvgPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::QuantizedAvgPoolShape);\n\nREGISTER_OP(\"QuantizedBiasAdd\")\n    .Input(\"input: T1\")\n    .Input(\"bias: T2\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_bias: float\")\n    .Input(\"max_bias: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BiasAddShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::QuantizedConv2DShape);\n\nREGISTER_OP(\"QuantizedMaxPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu6\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedReluX\")\n    .Input(\"features: Tinput\")\n    .Input(\"max_value: float\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedBatchNormWithGlobalNormalization\")\n    .Input(\"t: Tinput\")\n    .Input(\"t_min: float\")\n    .Input(\"t_max: float\")\n    .Input(\"m: Tinput\")\n    .Input(\"m_min: float\")\n    .Input(\"m_max: float\")\n    .Input(\"v: Tinput\")\n    .Input(\"v_min: float\")\n    .Input(\"v_max: float\")\n    .Input(\"beta: Tinput\")\n    .Input(\"beta_min: float\")\n    .Input(\"beta_max: float\")\n    .Input(\"gamma: Tinput\")\n    .Input(\"gamma_min: float\")\n    .Input(\"gamma_max: float\")\n    .Output(\"result: out_type\")\n    .Output(\"result_min: float\")\n    .Output(\"result_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i * 3), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n\n      return OkStatus();\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"_MklConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D operator. Uses MKL DNN APIs to perform 2D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\n    MKL version of Conv2D operator for Eager mode. Uses MKL DNN APIs to perform 2D convolution.\n\n    NOTE Do not invoke this operator directly in Python. Eager Op rewrite is\n    expected to invoke these operators.\n    )doc\");\n\nREGISTER_OP(\"__MklDummyConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2D and BiasAdd operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Conv2D and BiasAdd.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_bias: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D and BiasAdd operator. Uses MKL DNN APIs to perform\n2D convolution and add Bias to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nDummy node that enables fusing Pad and Conv2D operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Pad and Conv2D.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_paddings: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nMKL version of Pad and Conv2D operator. Uses MKL DNN APIs to perform\nPad and 2D convolution to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Eager Op rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      // Fetch the data_format attribute, which may not exist.\n      string data_format;\n      Status s = c->GetAttr(\"data_format\", &data_format);\n\n      if (s.ok() && data_format == \"NCHW\") {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -3)));\n      } else {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -1)));\n      }\n      ShapeHandle sh;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &sh));\n      TF_RETURN_IF_ERROR(c->WithRank(sh, 4, &sh));\n      c->set_output(0, sh);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2DBackpropFilter and BiasAddGrad operator\nfor MKL. This node does not perform anything. It is just created as an\nintermediate output of merging Conv2DBackpropFilter and BiasAddGrad.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_bias_grad: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropFilterWithBiasShape)\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilterWithBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#ifdef INTEL_MKL_ML_ONLY\nREGISTER_OP(\"_MklConv2DWithBiasBackpropBias\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the bias.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\nREGISTER_OP(\"_MklConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Eager op rewrite is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape)\n    .Doc(R\"doc(\nMKL version of Conv3D operator. Uses MKL DNN APIs to perform 3D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution3D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv3DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu operator. Uses MKL DNN APIs to implement Relu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu6 operator. Uses MKL DNN APIs to implement Relu6 operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of Relu6Grad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu6 operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of LeakyRelu operator. Uses MKL DNN APIs to implement\nLeakyRelu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of LeakyReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for LeakyReluGrad operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklElu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Elu operator. Uses MKL DNN APIs to implement Elu operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklEluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of EluGrad operator. Uses MKL DNN APIs to compute Elu\ngradients for Elu operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklSoftmax\")\n    .Input(\"logits: T\")\n    .Input(\"mkl_logits: uint8\")\n    .Output(\"softmax: T\")\n    .Output(\"mkl_softmax: uint8\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    })\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n)doc\");\n\nREGISTER_OP(\"_MklTanh\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Tanh operator. Uses MKL DNN APIs to implement Tanh operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklTanhGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of TanhGrad operator. Uses MKL DNN APIs to compute tanh\ngradients for Tanh operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"workspace_enabled: bool = false\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Output(\"workspace: T\")\n#else\n    .Output(\"workspace: uint8\")\n#endif\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolShape)\n    .Doc(R\"doc(\nMKL version of MaxPool operator. Uses MKL DNN APIs to perform max pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPoolGrad\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Input(\"workspace: T\")\n#else\n    .Input(\"workspace: uint8\")\n#endif\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPoolGrad. Uses oneDNN APIs to compute gradients of\nMaxPool operator.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolShape)\n    .Doc(R\"doc(\nMKL version of AvgPool operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPoolGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3D\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of AvgPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3D\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of MaxPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof MaxPool3D function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRN\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    })\n    .Doc(R\"doc(\nMKL version of LRN operator. Uses MKL DNN APIs to perform local response\nnormalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_input_grads: uint8\")\n    .Input(\"mkl_input_image: uint8\")\n    .Input(\"mkl_output_image: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of LRNGrad operator. Uses MKL DNN APIs to compute gradient for\nlocal response normalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNorm operator. Uses oneDNN APIs to perform fused\nbatch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNormGrad operator. Uses oneDNN APIs to compute\ngradients for fused batch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"_MklFusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_MklToTf\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, bfloat16, qint8, quint8, qint32}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to convert a tensor from MKL layout to TensorFlow layout.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklInputConversion\")\n    .Input(\"input_0: T\")\n    .Input(\"input_1: T\")\n    .Input(\"mkl_input_0: uint8\")\n    .Input(\"mkl_input_1: uint8\")\n    .Output(\"output_0: T\")\n    .Output(\"output_1: T\")\n    .Output(\"mkl_output_0: uint8\")\n    .Output(\"mkl_output_1: uint8\")\n    // All datatypes supported by element-wise ops\n    .Attr(\n        \"T: {half, float, bfloat16, double, uint8, int8, uint16, int16, int32, \"\n        \"int64, complex64, complex128}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to process the inputs to an elementwise MKL op. Both inputs\nneed to be either in TF or in MKL format. This op is added before every\nelement-wise MKL op.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#endif  // INTEL_MKL\nREGISTER_OP(\"QuantizedConv2DAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and BiasAdd.\nREGISTER_OP(\"QuantizedConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and Relu.\nREGISTER_OP(\"QuantizedConv2DAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Relu, and Requantize.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Sum, and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSignedSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      // Since activations are not requantized per channel, `min_output`\n      // and `max_output` are scalars.\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized MatMul and BiasAdd.\nREGISTER_OP(\"QuantizedMatMulWithBias\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRelu\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: float\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndReluAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndDequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: {float}\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DPerChannel\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedDepthwiseConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"IsotonicRegression\")\n    .Input(\"input: T\")\n    .Output(\"output: output_dtype\")\n    .Output(\"segments: int32\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"output_dtype: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* context) {\n      context->set_output(0, context->input(0));\n      context->set_output(1, context->input(0));\n      return OkStatus();\n    });\n\n}  // namespace tensorflow\n", "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/fake_input.h\"\n#include \"tensorflow/core/framework/node_def_builder.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference_testutil.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\n\nTEST(NNOpsTest, TopK_ShapeFn) {\n  ShapeInferenceTestOp op(\"TopK\");\n  auto set_k = [&op](int k) {\n    TF_ASSERT_OK(NodeDefBuilder(\"test\", \"Pack\")\n                     .Input({{\"a\", 0, DT_FLOAT}})\n                     .Attr(\"k\", k)\n                     .Finalize(&op.node_def));\n  };\n\n  set_k(20);\n  // With known input, each output is an unknown shape.\n  INFER_OK(op, \"?\", \"?;?\");\n  // With vector input, each output is [k].\n  INFER_OK(op, \"[20]\", \"[20];[20]\");\n  INFER_OK(op, \"[21]\", \"[20];[20]\");\n\n  // With input rank 3, each output is the two first 2 dims of input, plus k.\n  INFER_OK(op, \"[1,?,21]\", \"[d0_0,d0_1,20];[d0_0,d0_1,20]\");\n  // With input rank 4, each output is the two first 3 dims of input, plus k.\n  INFER_OK(op, \"[1,?,21,?]\", \"[d0_0,d0_1,d0_2,20];[d0_0,d0_1,d0_2,20]\");\n\n  INFER_ERROR(\"Shape must be at least rank 1 but is rank 0\", op, \"[]\");\n  INFER_ERROR(\"input must have last dimension >= k = 20 but is 1\", op, \"[1]\");\n  INFER_ERROR(\"input must have last dimension >= k = 20 but is 4\", op,\n              \"[1,2,3,4]\");\n  set_k(-1);\n  INFER_ERROR(\"Need k >= 0, got -1\", op, \"[1,2,3,4]\");\n}\n\nTEST(NNOpsTest, TopKV2_ShapeFn) {\n  ShapeInferenceTestOp op(\"TopKV2\");\n  op.input_tensors.resize(2);\n\n  Tensor k_t;\n  op.input_tensors[1] = &k_t;\n\n  k_t = test::AsScalar<int32>(20);\n  // With known input, each output is an unknown shape.\n  INFER_OK(op, \"?;[]\", \"?;?\");\n  // With vector input, each output is [k].\n  INFER_OK(op, \"[20];[]\", \"[20];[20]\");\n\n  // With input rank 3, each output is the two first 2 dims of input, plus k.\n  INFER_OK(op, \"[1,?,21];[]\", \"[d0_0,d0_1,20];[d0_0,d0_1,20]\");\n  // With input rank 4, each output is the two first 3 dims of input, plus k.\n  INFER_OK(op, \"[1,?,21,?];[]\", \"[d0_0,d0_1,d0_2,20];[d0_0,d0_1,d0_2,20]\");\n\n  INFER_ERROR(\"Shape must be at least rank 1 but is rank 0\", op, \"[];[]\");\n  INFER_ERROR(\"input must have last dimension >= k = 20 but is 1\", op,\n              \"[1];[]\");\n  INFER_ERROR(\"input must have last dimension >= k = 20 but is 4\", op,\n              \"[1,2,3,4];[]\");\n  k_t = test::AsScalar<int32>(-1);\n  INFER_ERROR(\n      \"Dimension size, given by scalar input 1, must be non-negative but is -1\",\n      op, \"[1,2,3,4];[]\");\n}\n\nTEST(NNOpsTest, NthElement_ShapeFn) {\n  ShapeInferenceTestOp op(\"NthElement\");\n  op.input_tensors.resize(2);\n\n  Tensor n_t;\n  op.input_tensors[1] = &n_t;\n  n_t = test::AsScalar<int32>(20);\n\n  INFER_OK(op, \"?;[]\", \"?\");\n  INFER_OK(op, \"[21];[]\", \"[]\");\n  INFER_OK(op, \"[2,?,?];[]\", \"[d0_0,d0_1]\");\n  INFER_OK(op, \"[?,3,?,21];[]\", \"[d0_0,d0_1,d0_2]\");\n\n  INFER_ERROR(\"Shape must be at least rank 1 but is rank 0\", op, \"[];[]\");\n  INFER_ERROR(\"Input must have last dimension > n = 20 but is 1\", op, \"[1];[]\");\n  INFER_ERROR(\"Input must have last dimension > n = 20 but is 20\", op,\n              \"[1,2,3,20];[]\");\n  n_t = test::AsScalar<int32>(-1);\n  INFER_ERROR(\n      \"Dimension size, given by scalar input 1, must be non-negative but is -1\",\n      op, \"[1,2,3,4];[]\");\n}\n\nTEST(NNOpsTest, BatchNormWithGlobalNormalization_ShapeFn) {\n  ShapeInferenceTestOp op(\"BatchNormWithGlobalNormalization\");\n\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n\n  // last dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,?,?,d3_0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,?,?,d4_0]\");\n  INFER_OK(op, \"[1,2,3,4];[4];[4];[4];[4]\",\n           \"[d0_0,d0_1,d0_2,d0_3|d1_0|d2_0|d3_0|d4_0]\");\n}\n\nTEST(NNOpsTest, QuantizedBatchNormWithGlobalNormalization_ShapeFn) {\n  // These are the same tests as BatchNormWithGlobalNormalization tests, but\n  // with extra scalar inputs and outputs for the mins and maxes.\n\n  ShapeInferenceTestOp op(\"QuantizedBatchNormWithGlobalNormalization\");\n\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op,\n              \"[1,2,3];?;?;?;?;?;?;?;?;?;?;?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op,\n              \"?;?;?;[1,2,3];?;?;?;?;?;?;?;?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op,\n              \"?;?;?;?;?;?;[1,2,3];?;?;?;?;?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op,\n              \"?;?;?;?;?;?;?;?;?;[1,2,3];?;?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op,\n              \"?;?;?;?;?;?;?;?;?;?;?;?;[1,2,3];?;?\");\n\n  // last dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;[];[];?;[];[];?;[];[];?;[];[];?;[];[]\", \"[?,?,?,?];[];[]\");\n  INFER_OK(op, \"?;[];[];[1];[];[];?;[];[];?;[];[];?;[];[]\",\n           \"[?,?,?,d3_0];[];[]\");\n  INFER_OK(op, \"?;[];[];?;[];[];[1];[];[];?;[];[];?;[];[]\",\n           \"[?,?,?,d6_0];[];[]\");\n  INFER_OK(op, \"?;[];[];?;[];[];?;[];[];[1];[];[];?;[];[]\",\n           \"[?,?,?,d9_0];[];[]\");\n  INFER_OK(op, \"?;[];[];?;[];[];?;[];[];?;[];[];[1];[];[]\",\n           \"[?,?,?,d12_0];[];[]\");\n  INFER_OK(op, \"[1,2,3,4];[];[];[4];[];[];[4];[];[];[4];[];[];[4];[];[]\",\n           \"[d0_0,d0_1,d0_2,d0_3|d3_0|d6_0|d9_0|d12_0];[];[]\");\n}\n\nTEST(NNOpsTest, BatchNormWithGlobalNormalizationGrad_ShapeFn) {\n  ShapeInferenceTestOp op(\"BatchNormWithGlobalNormalizationGrad\");\n\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shapes must be equal rank, but are 4 and 3\", op,\n              \"?;?;?;?;[1,2,3]\");\n\n  // The first output comes from the first and last inputs merged together.\n  // Other inputs are merged with the last dim of that merge result, and that\n  // merged vector dim is the last 4 outputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,?,?,d3_0];[d3_0];[d3_0];[d3_0];[d3_0]\");\n  INFER_OK(op, \"[1,?,3,?];[?];[?];[?];[?,2,?,4]\",\n           \"[d0_0,d4_1,d0_2,d4_3];[d4_3];[d4_3];[d4_3];[d4_3]\");\n}\n\nTEST(NNOpsTest, FusedBatchNorm_ShapeFn) {\n  ShapeInferenceTestOp op(\"FusedBatchNorm\");\n\n  auto set_op = [&op](bool is_training, float exponential_avg_factor,\n                      string data_format) {\n    TF_ASSERT_OK(NodeDefBuilder(\"test\", \"FusedBatchNorm\")\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Attr(\"data_format\", data_format)\n                     .Attr(\"is_training\", is_training)\n                     .Attr(\"exponential_avg_factor\", exponential_avg_factor)\n                     .Finalize(&op.node_def));\n  };\n\n  set_op(true, 1.0, \"NHWC\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"[1,2,3,4];[4];[4];?;?\",\n           \"[d0_0,d0_1,d0_2,d0_3|d1_0|d2_0];\"\n           \"[d0_3|d1_0|d2_0];[d0_3|d1_0|d2_0];\"\n           \"[d0_3|d1_0|d2_0];[d0_3|d1_0|d2_0]\");\n\n  set_op(true, 0.5, \"NHWC\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"[1,2,3,4];[4];[4];?;?\",\n           \"[d0_0,d0_1,d0_2,d0_3|d1_0|d2_0];\"\n           \"[d0_3|d1_0|d2_0];[d0_3|d1_0|d2_0];\"\n           \"[d0_3|d1_0|d2_0];[d0_3|d1_0|d2_0]\");\n\n  set_op(true, 1.0, \"NCHW\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,d1_0,?,?];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,d2_0,?,?];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"[1,4,2,3];[4];[4];?;?\",\n           \"[d0_0,d0_1|d1_0|d2_0,d0_2,d0_3];\"\n           \"[d0_1|d1_0|d2_0];[d0_1|d1_0|d2_0];\"\n           \"[d0_1|d1_0|d2_0];[d0_1|d1_0|d2_0]\");\n\n  set_op(false, 1.0, \"NHWC\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,?,?,d3_0];[d3_0];[d3_0];[d3_0];[d3_0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,?,?,d4_0];[d4_0];[d4_0];[d4_0];[d4_0]\");\n  INFER_OK(op, \"[1,2,3,4];[4];[4];[4];[4]\",\n           \"[d0_0,d0_1,d0_2,d0_3|d1_0|d2_0|d3_0|d4_0];\"\n           \"[d0_3|d1_0|d2_0|d3_0|d4_0];[d0_3|d1_0|d2_0|d3_0|d4_0];\"\n           \"[d0_3|d1_0|d2_0|d3_0|d4_0];[d0_3|d1_0|d2_0|d3_0|d4_0]\");\n\n  set_op(false, 1.0, \"NCHW\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,d1_0,?,?];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,d2_0,?,?];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,d3_0,?,?];[d3_0];[d3_0];[d3_0];[d3_0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,d4_0,?,?];[d4_0];[d4_0];[d4_0];[d4_0]\");\n  INFER_OK(op, \"[1,4,2,3];[4];[4];[4];[4]\",\n           \"[d0_0,d0_1|d1_0|d2_0|d3_0|d4_0,d0_2,d0_3];\"\n           \"[d0_1|d1_0|d2_0|d3_0|d4_0];[d0_1|d1_0|d2_0|d3_0|d4_0];\"\n           \"[d0_1|d1_0|d2_0|d3_0|d4_0];[d0_1|d1_0|d2_0|d3_0|d4_0]\");\n}\n\nTEST(NNOpsTest, FusedBatchNormGrad_ShapeFn) {\n  ShapeInferenceTestOp op(\"FusedBatchNormGrad\");\n  auto set_op = [&op](string data_format) {\n    TF_ASSERT_OK(NodeDefBuilder(\"test\", \"FusedBatchNormGrad\")\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Attr(\"data_format\", data_format)\n                     .Finalize(&op.node_def));\n  };\n\n  set_op(\"NCHW\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[0];[0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,d2_0,?,?];[d2_0];[d2_0];[0];[0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,d3_0,?,?];[d3_0];[d3_0];[0];[0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,d4_0,?,?];[d4_0];[d4_0];[0];[0]\");\n  INFER_OK(op, \"[1,4,2,3];[1,4,2,3];[4];[4];[4]\",\n           \"[d0_0,d0_1|d2_0|d3_0|d4_0,d0_2,d0_3];\"\n           \"[d0_1|d2_0|d3_0|d4_0];[d0_1|d2_0|d3_0|d4_0];[0];[0]\");\n\n  set_op(\"NHWC\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[0];[0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[0];[0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,?,?,d3_0];[d3_0];[d3_0];[0];[0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,?,?,d4_0];[d4_0];[d4_0];[0];[0]\");\n  INFER_OK(op, \"[1,2,3,4];[1,2,3,4];[4];[4];[4]\",\n           \"[d0_0,d0_1,d0_2,d0_3|d2_0|d3_0|d4_0];\"\n           \"[d0_3|d2_0|d3_0|d4_0];[d0_3|d2_0|d3_0|d4_0];[0];[0]\");\n}\n\nTEST(NNOpsTest, Conv2DBackpropInput_ShapeFn) {\n  ShapeInferenceTestOp op(\"Conv2DBackpropInput\");\n\n  // Test rank error.\n  INFER_ERROR(\"input_sizes to contain 4 values or 2 values\", op,\n              \"[3];[?,?,?,?];[?,?,?,?]\");\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op,\n              \"[4];[?,?,?,?];[?,?,?]\");\n\n  // When input_sizes is a 4D shape and the convolution is grouped, the channel\n  // size of the input grad doesn't always equal the input channel size of the\n  // filter. So, when input_sizes is a 4D shape, the channel size of the input\n  // grad is determined by the content of input_sizes.\n  INFER_OK(op, \"[4];[?,?,2,?];[1,?,?,?]\", \"[d2_0,?,?,?]\");\n  // When input_sizes is a 2D shape, the channel size of the input grad always\n  // matches the filter shape.\n  INFER_OK(op, \"[2];[?,?,2,?];[1,?,?,?]\", \"[d2_0,?,?,d1_2]\");\n}\n\nTEST(NNOpsTest, Conv3DBackpropInput_ShapeFn) {\n  ShapeInferenceTestOp op(\"Conv3DBackpropInput\");\n\n  // Test rank error.\n  INFER_ERROR(\"Shape must be rank 5 but is rank 3\", op, \"[1,2,3];?;?\");\n\n  // input[1] is transferred to output after asserting its rank.\n  INFER_OK(op, \"?;?;?\", \"[?,?,?,?,?]\");\n  INFER_OK(op, \"[?,?,?,?,?];?;?\", \"in0\");\n  INFER_OK(op, \"[?,2,?,4,?];?;?\", \"in0\");\n}\n\nTEST(NNOpsTest, Conv3DBackpropFilter_ShapeFn) {\n  ShapeInferenceTestOp op(\"Conv3DBackpropFilter\");\n\n  // Test rank error.\n  INFER_ERROR(\"Shape must be rank 5 but is rank 3\", op, \"?;[1,2,3];?\");\n\n  // input[1] is transferred to output after asserting its rank.\n  INFER_OK(op, \"?;?;?\", \"[?,?,?,?,?]\");\n  INFER_OK(op, \"?;[?,?,?,?,?];?\", \"in1\");\n  INFER_OK(op, \"?;[?,2,?,4,?];?\", \"in1\");\n}\n\nTEST(NNOpsTest, MaxPool3DGrad_ShapeFn) {\n  ShapeInferenceTestOp op(\"MaxPool3DGrad\");\n\n  // Test rank error.\n  INFER_ERROR(\"Shape must be rank 5 but is rank 3\", op, \"[1,2,3];?;?\");\n\n  // input[0] is transferred to output after asserting its rank.\n  INFER_OK(op, \"?;?;?\", \"[?,?,?,?,?]\");\n  INFER_OK(op, \"[?,?,?,?,?];?;?\", \"in0\");\n  INFER_OK(op, \"[?,2,?,4,?];?;?\", \"in0\");\n}\n\nTEST(NNOpsTest, LRNGrad_ShapeFn) {\n  ShapeInferenceTestOp op(\"LRNGrad\");\n\n  // LRN Grad is a merge of all three inputs, of rank 4.\n  INFER_OK(op, \"[1,?,?,4];[?,2,?,?];[?,?,3,?]\", \"[d0_0,d1_1,d2_2,d0_3]\");\n\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?\");\n  INFER_ERROR(\"Shapes must be equal rank, but are 4 and 3\", op, \"?;[1,2,3];?\");\n  INFER_ERROR(\"Shapes must be equal rank, but are 4 and 3\", op, \"?;?;[1,2,3]\");\n}\n\nTEST(NNOpsTest, MaxPoolGrad_ShapeFn) {\n  for (const char* op_name : {\"MaxPoolGrad\", \"MaxPoolGradWithArgmax\"}) {\n    ShapeInferenceTestOp op(op_name);\n\n    // Test rank error.\n    INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?\");\n\n    // input[0] is transferred to output after asserting its rank.\n    INFER_OK(op, \"?;?;?\", \"[?,?,?,?]\");\n    INFER_OK(op, \"[?,?,?,?];?;?\", \"in0\");\n    INFER_OK(op, \"[?,2,?,4];?;?\", \"in0\");\n  }\n}\n\nTEST(NNOpsTest, Dilation2DBackpropInput_ShapeFn) {\n  ShapeInferenceTestOp op(\"Dilation2DBackpropInput\");\n\n  // input[0] is transferred to output.\n  INFER_OK(op, \"?;?;?\", \"in0\");\n  INFER_OK(op, \"?;[?,?,?,?,?];?\", \"in0\");\n  INFER_OK(op, \"?;[?,2,?,4,?];?\", \"in0\");\n}\n\nTEST(NNOpsTest, Dilation2DBackpropFilter_ShapeFn) {\n  ShapeInferenceTestOp op(\"Dilation2DBackpropFilter\");\n\n  // input[1] is transferred to output.\n  INFER_OK(op, \"?;?;?\", \"in1\");\n  INFER_OK(op, \"?;[?,?,?,?,?];?\", \"in1\");\n  INFER_OK(op, \"?;[?,2,?,4,?];?\", \"in1\");\n}\n\nTEST(NNOpsTest, MergeBothInputs_ShapeFn) {\n  for (const char* op_name : {\"ReluGrad\", \"Relu6Grad\", \"EluGrad\", \"SeluGrad\",\n                              \"SoftplusGrad\", \"SoftsignGrad\"}) {\n    ShapeInferenceTestOp op(op_name);\n\n    INFER_OK(op, \"?;?\", \"in0|in1\");\n    INFER_OK(op, \"?;[1,?,3]\", \"in1\");\n    INFER_OK(op, \"[1,?,3];?\", \"in0\");\n    INFER_OK(op, \"[1,?];[?,2]\", \"[d0_0,d1_1]\");\n    INFER_ERROR(\"Dimension 1 in both shapes must be equal, but are 3 and 2\", op,\n                \"[1,3];[?,2]\");\n  }\n}\n\nTEST(NNOpsTest, SoftmaxCrossEntropyWithLogits_ShapeFn) {\n  ShapeInferenceTestOp op(\"SoftmaxCrossEntropyWithLogits\");\n\n  // Inputs are [batch_size,N] and [batch_size,N], and outputs are [batch_size]\n  // and\n  // [batch_size,N].\n  INFER_OK(op, \"?;?\", \"[?];[?,?]\");\n  INFER_OK(op, \"[?,?];[?,?]\", \"[d0_0|d1_0];in0|in1\");\n  INFER_OK(op, \"[1,2];[?,2]\", \"[d0_0];in0\");\n  INFER_OK(op, \"[1,?];[?,2]\", \"[d0_0];[d0_0,d0_1|d1_1]\");\n  INFER_OK(op, \"[?,2];[1,2]\", \"[d1_0];in1\");\n\n  INFER_ERROR(\"Shape must be broadcasted with rank 2\", op, \"[1,2,3];?\");\n  INFER_ERROR(\"Shape must be broadcasted with rank 2\", op, \"?;[1,2,3]\");\n\n  // Broadcast example\n  // [1,4] and [2,4] are broadcasted to [2,4]\n  INFER_OK(op, \"[1,4];[2,4]\", \"[d1_0];[d1_0,d0_1|d1_1]\");\n  // [2,4] and [2,1] are broadcasted to [2,4]\n  INFER_OK(op, \"[2,4];[2,1]\", \"[d0_0];[d0_0|d1_0,d0_1]\");\n  // [1,?] and [2,4] are broadcasted to [2,4]\n  INFER_OK(op, \"[1,?];[2,4]\", \"[d1_0];[d1_0,d0_1|d1_1]\");\n  // [2,4] and [?,1] are broadcasted to [2,4]\n  INFER_OK(op, \"[2,4];[?,1]\", \"[d0_0];[d0_0|d1_0,d0_1]\");\n}\n\nTEST(NNOpsTest, SparseSoftmaxCrossEntropyWithLogits_ShapeFn) {\n  ShapeInferenceTestOp op(\"SparseSoftmaxCrossEntropyWithLogits\");\n\n  // Inputs are [batch_size,N] and [batch_size], and outputs are [batch_size]\n  // and [batch_size,N].\n  INFER_OK(op, \"?;?\", \"[?];[?,?]\");\n  INFER_OK(op, \"[?,?];[?]\", \"[d0_0|d1_0];[d0_0|d1_0,d0_1]\");\n  INFER_OK(op, \"[1,2];[1]\", \"[d0_0|d1_0];[d0_0|d1_0,d0_1]\");\n  INFER_OK(op, \"[?,2];[1]\", \"[d1_0];[d1_0,d0_1]\");\n\n  INFER_ERROR(\"Dimensions must be equal, but are 1 and 2\", op, \"[1,?];[2]\");\n  INFER_ERROR(\"Shape must be rank 2 but is rank 3\", op, \"[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 2\", op, \"?;[1,2]\");\n}\n\nTEST(NNOpsTest, InTopK_ShapeFn) {\n  ShapeInferenceTestOp op(\"InTopK\");\n\n  // Inputs are [batch_size,N] and [batch_size], and output is [batch_size].\n  INFER_OK(op, \"?;?\", \"[?]\");\n  INFER_OK(op, \"[?,?];[?]\", \"[d0_0|d1_0]\");\n  INFER_OK(op, \"[1,2];[1]\", \"[d0_0|d1_0]\");\n  INFER_OK(op, \"[?,2];[1]\", \"[d1_0]\");\n\n  INFER_ERROR(\"Dimensions must be equal, but are 1 and 2\", op, \"[1,?];[2]\");\n  INFER_ERROR(\"Shape must be rank 2 but is rank 3\", op, \"[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 2\", op, \"?;[1,2]\");\n}\n\nTEST(NNOpsTest, Dilation2DShapeTest) {\n  ShapeInferenceTestOp op(\"Dilation2D\");\n  auto set_op = [&op](const std::vector<int32>& strides,\n                      const std::vector<int32>& rates, const string& padding) {\n    TF_ASSERT_OK(NodeDefBuilder(\"test\", \"Dilation2D\")\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Input(\"filter\", 0, DT_FLOAT)\n                     .Attr(\"strides\", strides)\n                     .Attr(\"rates\", rates)\n                     .Attr(\"padding\", padding)\n                     .Finalize(&op.node_def));\n  };\n\n  // rate rows and cols is 1, so filter_rows and cols are unchanged.\n  // We have a 1x1 filter so the output is still 2x2.\n  set_op({1, 1, 1, 1}, {1, 1, 1, 1}, \"VALID\");\n  INFER_OK(op, \"[1,2,2,2];[1,1,2]\", \"[d0_0,2,2,d1_2]\");\n\n  // rate rows and cols is 2, so filter_rows and cols are changed to\n  // be 2 + (2 - 1) = 3.  7x7 input with 3x3 filter and 1x1 stride\n  // gives a 5x5 output.\n  set_op({1, 1, 1, 1}, {1, 2, 2, 1}, \"VALID\");\n  INFER_OK(op, \"[1,7,7,2];[2,2,2]\", \"[d0_0,5,5,d1_2]\");\n}\n\nTEST(NNOpsTest, FractionalPool_ShapeFn) {\n  for (const char* op_name : {\"FractionalAvgPool\", \"FractionalMaxPool\"}) {\n    ShapeInferenceTestOp op(op_name);\n    auto set_op = [&op, op_name](const std::vector<float>& pooling_ratio) {\n      TF_ASSERT_OK(NodeDefBuilder(\"test\", op_name)\n                       .Input(\"input\", 0, DT_FLOAT)\n                       .Attr(\"pooling_ratio\", pooling_ratio)\n                       .Finalize(&op.node_def));\n    };\n\n    set_op(std::vector<float>{2.0f, 1, 1 / 1.5f, 1 / 2.0f});\n\n    // Rank check.\n    INFER_ERROR(\"must be rank 4\", op, \"[?,?,?]\");\n\n    // Unknown inputs.\n    INFER_OK(op, \"?\", \"[?,?,?,?];[?];[?]\");\n    INFER_OK(op, \"[?,?,?,?]\", \"[?,?,?,?];[?];[?]\");\n\n    INFER_OK(op, \"[10,20,30,40]\", \"[5,20,45,80];[20];[45]\");\n    INFER_OK(op, \"[?,20,30,40]\", \"[?,20,45,80];[20];[45]\");\n    INFER_OK(op, \"[10,?,30,40]\", \"[5,?,45,80];[?];[45]\");\n    INFER_OK(op, \"[10,20,?,40]\", \"[5,20,?,80];[20];[?]\");\n    INFER_OK(op, \"[10,20,30,?]\", \"[5,20,45,?];[20];[45]\");\n\n    // Wrong number of values for pooling_ratio.\n    set_op(std::vector<float>{.5, 1.0, 1.5});\n    INFER_ERROR(\"pooling_ratio field\", op, \"?\");\n    set_op(std::vector<float>{1, 2, 3, 4, 5});\n    INFER_ERROR(\"pooling_ratio field\", op, \"?\");\n\n    // Check dim size >= 0.\n    set_op(std::vector<float>{-1, 2, 3, 4});\n    INFER_ERROR(\"is negative\", op, \"[1,2,3,4]\");\n  }\n}\n\nTEST(NNOpsTest, FractionalMaxPoolGrad) {\n  ShapeInferenceTestOp op(\"FractionalMaxPoolGrad\");\n\n  // Note that the shape fn only uses input[0] for computation.\n  INFER_ERROR(\"must be rank 4\", op, \"[?,?,?];?;?;?;?\");\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?]\");\n  INFER_OK(op, \"[?,?,3,4];?;?;?;?\", \"in0\");\n}\n\nTEST(NNOpsTest, FractionalAvgPoolGrad) {\n  ShapeInferenceTestOp op(\"FractionalAvgPoolGrad\");\n  op.input_tensors.resize(1);\n\n  // With no input shape tensor, returns unknown of rank 4.\n  INFER_OK(op, \"?;?;?;?\", \"[?,?,?,?]\");\n\n  // When input tensor is known, its values determine output shape.\n  std::vector<int32> shape{1, 2, 3, 4};\n  Tensor shape_t = test::AsTensor<int32>(shape);\n  op.input_tensors[0] = &shape_t;\n  INFER_OK(op, \"[5];?;?;?\", \"[1,2,3,4]\");\n}\n\n}  // end namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fractional average pool operation.\"\"\"\n\nimport math\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass FractionalAvgTest(test.TestCase):\n\n  # Random number generate with seed.\n  _PRNG = np.random.RandomState(341261000)\n  _SEED = 341261001\n\n  def _AvgPoolAlongRows(self, input_matrix, row_seq, overlapping):\n    \"\"\"Perform average pool along row of a 2-D matrix based on row_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      row_seq: Cumulative pooling sequence along row.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = len(row_seq)-1\n        * num_cols = input_matrix.num_cols.\n    \"\"\"\n    output_image = np.zeros(input_matrix.shape[1])\n    row_max = row_seq[-1]\n    for i in range(row_seq.shape[0] - 1):\n      row_start = row_seq[i]\n      row_end = row_seq[i + 1] + 1 if overlapping else row_seq[i + 1]\n      row_end = min(row_end, row_max)\n      output_image = np.vstack((output_image, np.mean(\n          input_matrix[row_start:row_end, :], axis=0)))  # axis 0 is along row\n    # remove the sentinel row\n    return output_image[1:, :]\n\n  def _AvgPoolAlongCols(self, input_matrix, col_seq, overlapping):\n    \"\"\"Perform average pool along column of a 2-D matrix based on col_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = input_matrix.num_rows\n        * num_cols = len(col_seq)-1.\n    \"\"\"\n    input_matrix = input_matrix.transpose()\n    output_matrix = self._AvgPoolAlongRows(input_matrix, col_seq, overlapping)\n    return output_matrix.transpose()\n\n  def _GetExpectedFractionalAvgPoolResult(self, input_tensor, row_seq, col_seq,\n                                          overlapping):\n    \"\"\"Get expected fractional average pooling result.\n\n    row_seq and col_seq together defines the fractional pooling region.\n\n    Args:\n      input_tensor: Original input tensor, assuming it is a 4-D tensor, with\n        dimension as [batch, height/row, width/column, channels/depth].\n      row_seq: Cumulative pooling sequence along row.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Use overlapping when doing pooling.\n\n    Returns:\n      A 4-D tensor that is the result of average pooling on input_tensor based\n        on pooling region defined by row_seq and col_seq, conditioned on whether\n        or not overlapping is used.\n    \"\"\"\n    input_shape = input_tensor.shape\n    output_shape = (input_shape[0], len(row_seq) - 1, len(col_seq) - 1,\n                    input_shape[3])\n    output_tensor = np.zeros(shape=output_shape, dtype=input_tensor.dtype)\n    for batch in range(input_shape[0]):\n      for channel in range(input_shape[3]):\n        two_dim_slice = input_tensor[batch, :, :, channel]\n        tmp = self._AvgPoolAlongRows(two_dim_slice, row_seq, overlapping)\n        output_tensor[batch, :, :, channel] = self._AvgPoolAlongCols(\n            tmp, col_seq, overlapping)\n\n    return output_tensor\n\n  def _ValidateFractionalAvgPoolResult(self, input_tensor, pooling_ratio,\n                                       pseudo_random, overlapping):\n    \"\"\"Validate FractionalAvgPool's result against expected.\n\n    Expected result is computed given input_tensor, and pooling region defined\n    by row_seq and col_seq.\n\n    Args:\n      input_tensor: A tensor or numpy ndarray.\n      pooling_ratio: A list or tuple of length 4, first and last element be 1.\n      pseudo_random: Use pseudo random method to generate pooling sequence.\n      overlapping: Use overlapping when pooling.\n\n    Returns:\n      None\n    \"\"\"\n    with self.cached_session() as sess:\n      p, r, c = nn_ops.fractional_avg_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      actual, row_seq, col_seq = self.evaluate([p, r, c])\n      expected = self._GetExpectedFractionalAvgPoolResult(input_tensor, row_seq,\n                                                          col_seq, overlapping)\n      self.assertShapeEqual(expected, p)\n      self.assertAllClose(expected, actual)\n\n  def _testVisually(self):\n    \"\"\"Manual test by printing out intermediate result of a small random tensor.\n\n    Since _GetExpectedFractionalAvgPoolResult is 'automated', it feels safer to\n    have a test case that you can see what's happening.\n    This test will generate a small, random, int 2D matrix, and feed it to\n    FractionalAvgPool and _GetExpectedFractionalAvgPoolResult.\n    \"\"\"\n    num_rows = 6\n    num_cols = 6\n    tensor_shape = (1, num_rows, num_cols, 1)\n    pseudo_random = False\n    for overlapping in True, False:\n      print(\"-\" * 70)\n      print(\"Testing FractionalAvgPool with overlapping = {}\".format(\n          overlapping))\n      rand_mat = self._PRNG.randint(10, size=tensor_shape)\n      pooling_ratio = [1, math.sqrt(2), math.sqrt(2), 1]\n      with self.cached_session() as sess:\n        p, r, c = nn_ops.fractional_avg_pool_v2(\n            rand_mat.astype(np.float32),\n            pooling_ratio,\n            pseudo_random,\n            overlapping,\n            seed=self._SEED)\n        tensor_output, row_seq, col_seq = self.evaluate([p, r, c])\n        expected_result = self._GetExpectedFractionalAvgPoolResult(\n            rand_mat.astype(np.float32), row_seq, col_seq, overlapping)\n        print(\"row sequence:\")\n        print(row_seq)\n        print(\"column sequence:\")\n        print(col_seq)\n\n        print(\"Input:\")\n        # Print input with pooling region marked.\n        for i in range(num_rows):\n          row_to_print = []\n          for j in range(num_cols):\n            if j in col_seq:\n              row_to_print.append(\"|\")\n            row_to_print.append(str(rand_mat[0, i, j, 0]))\n          row_to_print.append(\"|\")\n          if i in row_seq:\n            print(\"-\" * 2 * len(row_to_print))\n          print(\" \".join(row_to_print))\n        print(\"-\" * 2 * len(row_to_print))\n\n        print(\"Output from FractionalAvgPool:\")\n        print(tensor_output[0, :, :, 0])\n        print(\"Expected result:\")\n        print(expected_result[0, :, :, 0])\n\n  def testAllInputOptions(self):\n    \"\"\"Try all possible input options for fractional_avg_pool.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalAvgPoolResult(\n            rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n            overlapping)\n\n  def testIntegerTensorInput(self):\n    \"\"\"Test FractionalAvgPool works fine when input tensor is integer type.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    tensor_shape = (1, 6, 6, 1)\n    # pyformat: disable\n    mat = np.array([\n        [2, 6, 4, 1, 3, 6],\n        [8, 9, 1, 6, 6, 8],\n        [3, 9, 8, 2, 5, 6],\n        [2, 7, 9, 5, 4, 5],\n        [8, 5, 0, 5, 7, 4],\n        [4, 4, 5, 9, 7, 2]\n    ])\n    # pyformat: enable\n    self._ValidateFractionalAvgPoolResult(mat.reshape(tensor_shape),\n                                          [1, math.sqrt(3), math.sqrt(2), 1],\n                                          pseudo_random, overlapping)\n\n  def testDifferentTensorShapes(self):\n    \"\"\"Test different shapes of input tensor.\n\n    Mainly test different combinations of num_rows and num_cols.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    for num_batches in [1, 3]:\n      for num_channels in [1, 3]:\n        for num_rows in [10, 20, 50]:\n          for num_cols in [10, 20, 50]:\n            tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n            # random tensor with value in [-500.0, 500.0)\n            rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n            self._ValidateFractionalAvgPoolResult(\n                rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n                overlapping)\n\n  def testLargePoolingRatio(self):\n    \"\"\"Test when pooling ratio is not within [1, 2).\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    for row_ratio in [math.sqrt(11), math.sqrt(37)]:\n      for col_ratio in [math.sqrt(11), math.sqrt(27)]:\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalAvgPoolResult(rand_mat,\n                                              [1, row_ratio, col_ratio, 1],\n                                              pseudo_random, overlapping)\n\n  def testDivisiblePoolingRatio(self):\n    \"\"\"Test when num of rows/cols can evenly divide pooling ratio.\n\n    This is a case regular average pooling can handle. Should be handled by\n    fractional pooling as well.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    # random tensor with value in [-500.0, 500.0)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    self._ValidateFractionalAvgPoolResult(rand_mat, [1, 2, 2, 1], pseudo_random,\n                                          overlapping)\n\n  @test_util.run_deprecated_v1\n  def testDifferentInputTensorShape(self):\n    \"\"\"Runs the operation in one session with different input tensor shapes.\"\"\"\n    with self.cached_session() as sess:\n      input_holder = array_ops.placeholder(dtypes.float32,\n                                           [None, None, None, 3])\n      pooling_ratio = [1, 1.5, 1.5, 1]\n      pseudo_random = False\n      overlapping = False\n      p, r, c = nn_ops.fractional_avg_pool_v2(\n          input_holder,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      # First run.\n      input_a = np.zeros([3, 32, 32, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_a})\n      expected = self._GetExpectedFractionalAvgPoolResult(\n          input_a, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n      # Second run.\n      input_b = np.zeros([4, 60, 60, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_b})\n      expected = self._GetExpectedFractionalAvgPoolResult(\n          input_b, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n\n  def testNegativeSeqValuesForGradOp(self):\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"Row sequence tensor values must not be negative.*\"):\n      y = nn_ops.gen_nn_ops.fractional_avg_pool_grad(\n          orig_input_tensor_shape=[2, 2, 2, 2],\n          out_backprop=[[[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11,\n                                                                      12]]]],\n          row_pooling_sequence=[-10, 1, 2, 3],\n          col_pooling_sequence=[1, 2, 3, 4],\n          overlapping=True)\n\n      self.evaluate(y)\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Column sequence tensor values must not be negative.*\"):\n        z = nn_ops.gen_nn_ops.fractional_avg_pool_grad(\n            orig_input_tensor_shape=[2, 2, 2, 2],\n            out_backprop=[[[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11,\n                                                                        12]]]],\n            row_pooling_sequence=[10, 1, 2, 3],\n            col_pooling_sequence=[1, 2, -3, 4],\n            overlapping=True)\n\n        self.evaluate(z)\n\n\nclass FractionalAvgPoolGradTest(test.TestCase):\n  \"\"\"Tests for FractionalAvgPoolGrad.\n\n  Two types of tests for FractionalAvgPoolGrad.\n  1) Test fractional_avg_pool_grad() directly.\n    This type of test relies on gen_nn_ops.avg_pool_grad() returns the\n  correct result. For example:\n    * input_tensor_shape = (1, 10, 10, 1)\n    * window_size = (1, 2, 2, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not really important, since 10/2 is divisible\n  avg pooling should generate the same result as fractional avg pooling with:\n    * row_sequence = [0, 2, 4, 6, 8, 10]\n    * col_sequence = [0, 2, 4, 6, 8, 10]\n    * overlapping = False\n  This also means their gradients in such case will be the same.\n\n  Similarly, when\n    * input_tensor_shape = (1, 7, 7, 1)\n    * window_size = (1, 3, 3, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not important\n  avg pooling should generate the same result as fractional avg pooling with:\n    * row_sequence = [0, 2, 4, 7]\n    * col_sequence = [0, 2, 4, 7]\n    * overlapping = True\n  2) Test through compute_gradient_error()\n  \"\"\"\n  _PRNG = np.random.RandomState(341261004)\n  _SEED = 341261005\n\n  def _GenerateRandomInputTensor(self, shape):\n    num_elements = 1\n    for dim_size in shape:\n      num_elements *= dim_size\n    x = self._PRNG.rand(num_elements) * 1000\n    return x.reshape(shape)\n\n  def testDirectNotUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = row_window_size * 5\n          num_cols = col_window_size * 7\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateRandomInputTensor(input_shape).astype(\n                      np.float32))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size, col_window_size, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.avg_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              num_elements = 1\n              for dim_size in output_data.shape:\n                num_elements *= dim_size\n              output_backprop = (self._PRNG.rand(num_elements) *\n                                 1000).reshape(output_data.shape)\n              input_backprop_tensor = gen_nn_ops.avg_pool_grad(\n                  input_tensor.get_shape(), output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows + 1, row_window_size))\n              col_seq = list(range(0, num_cols + 1, col_window_size))\n              fap_input_backprop_tensor = gen_nn_ops.fractional_avg_pool_grad(\n                  input_tensor.get_shape(),\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=False)\n              fap_input_backprop = self.evaluate(fap_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fap_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fap_input_backprop)\n\n  def testDirectUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = (row_window_size - 1) * 5 + 1\n          num_cols = (col_window_size - 1) * 7 + 1\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateRandomInputTensor(input_shape).astype(\n                      np.float32))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size - 1, col_window_size - 1, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.avg_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              num_elements = 1\n              for dim_size in output_data.shape:\n                num_elements *= dim_size\n              output_backprop = (self._PRNG.rand(num_elements) *\n                                 1000).reshape(output_data.shape)\n              input_backprop_tensor = gen_nn_ops.avg_pool_grad(\n                  input_tensor.get_shape(), output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows, row_window_size - 1))\n              col_seq = list(range(0, num_cols, col_window_size - 1))\n              row_seq[-1] += 1\n              col_seq[-1] += 1\n              fap_input_backprop_tensor = gen_nn_ops.fractional_avg_pool_grad(\n                  input_tensor.get_shape(),\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=True)\n              fap_input_backprop = self.evaluate(fap_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fap_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fap_input_backprop)\n\n  @test_util.run_deprecated_v1\n  def testAllInputOptionsThroughGradientError(self):\n    input_shape = (1, 7, 13, 1)\n    input_data = self._GenerateRandomInputTensor(input_shape)\n    pooling_ratio = [1, math.sqrt(2), math.sqrt(3), 1]\n\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        with self.cached_session() as _:\n          input_tensor = constant_op.constant(input_data, shape=input_shape)\n          output_tensor, unused_a, unused_b = nn_ops.fractional_avg_pool_v2(\n              input_tensor,\n              pooling_ratio,\n              pseudo_random=pseudo_random,\n              overlapping=overlapping,\n              seed=self._SEED)\n          output_data = self.evaluate(output_tensor)\n          output_shape = output_data.shape\n          # error_margin and delta setting is similar to avg_pool_grad.\n          error_margin = 1e-4\n          gradient_error = gradient_checker.compute_gradient_error(\n              input_tensor,\n              input_shape,\n              output_tensor,\n              output_shape,\n              x_init_value=input_data.reshape(input_shape),\n              delta=1e-2)\n          self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testDifferentTensorShapesThroughGradientError(self):\n    pseudo_random = True\n    overlapping = True\n    pooling_ratio = [1, math.sqrt(3), math.sqrt(2), 1]\n    for num_batches in [1, 2]:\n      for num_rows in [5, 13]:\n        for num_cols in [5, 11]:\n          for num_channels in [1, 3]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            input_data = self._GenerateRandomInputTensor(input_shape)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(input_data, shape=input_shape)\n              output_tensor, unused_a, unused_b = nn_ops.fractional_avg_pool_v2(\n                  input_tensor,\n                  pooling_ratio,\n                  pseudo_random=pseudo_random,\n                  overlapping=overlapping,\n                  seed=self._SEED)\n              output_data = self.evaluate(output_tensor)\n              output_shape = output_data.shape\n              # error_margin and delta setting is similar to avg_pool_grad.\n              error_margin = 1e-4\n              gradient_error = gradient_checker.compute_gradient_error(\n                  input_tensor,\n                  input_shape,\n                  output_tensor,\n                  output_shape,\n                  x_init_value=input_data.reshape(input_shape),\n                  delta=1e-2)\n              self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testLargePoolingRatioThroughGradientError(self):\n    input_shape = (1, 17, 23, 1)\n    input_data = self._GenerateRandomInputTensor(input_shape)\n    pooling_ratio = (1, math.sqrt(13), math.sqrt(7), 1)\n    output_shape = [int(a / b) for a, b in zip(input_shape, pooling_ratio)]\n    overlapping = True\n    pseudo_random = False\n\n    with self.cached_session() as _:\n      input_tensor = constant_op.constant(input_data, shape=input_shape)\n      output_tensor, unused_a, unused_b = nn_ops.fractional_avg_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random=pseudo_random,\n          overlapping=overlapping,\n          seed=self._SEED)\n      # error_margin and delta setting is similar to avg_pool_grad.\n      error_margin = 1e-4\n      gradient_error = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_shape,\n          output_tensor,\n          output_shape,\n          x_init_value=input_data.reshape(input_shape),\n          delta=1e-2)\n      self.assertLess(gradient_error, error_margin)\n\n  def testInvalidSeqRaiseErrorForFractionalAvgPoolGrad(self):\n    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n      with self.cached_session() as _:\n        overlapping = True\n        orig_input_tensor_shape = constant_op.constant(\n            -1879048192, shape=[4], dtype=dtypes.int64)\n        out_backprop = constant_op.constant([],\n                                            shape=[0, 0, 0, 0],\n                                            dtype=dtypes.float64)\n        row_pooling_sequence = constant_op.constant(\n            1, shape=[4], dtype=dtypes.int64)\n        col_pooling_sequence = constant_op.constant(\n            1, shape=[4], dtype=dtypes.int64)\n        t = gen_nn_ops.fractional_avg_pool_grad(\n            orig_input_tensor_shape=orig_input_tensor_shape,\n            out_backprop=out_backprop,\n            row_pooling_sequence=row_pooling_sequence,\n            col_pooling_sequence=col_pooling_sequence,\n            overlapping=overlapping)\n        self.evaluate(t)\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fractional max pool operation.\"\"\"\n\nimport math\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass FractionalMaxPoolTest(test.TestCase):\n\n  # Random number generate with seed.\n  _PRNG = np.random.RandomState(341261)\n  _SEED = 123456\n\n  def _MaxPoolAlongRows(self, input_matrix, row_seq, overlapping):\n    \"\"\"Perform max pool along row of a 2-D matrix based on row_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      row_seq: Cumulative pooling sequence along row.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = len(row_seq)-1\n        * num_cols = input_matrix.num_cols.\n    \"\"\"\n    output_image = np.zeros(input_matrix.shape[1])\n    row_max = row_seq[-1]\n    for i in range(row_seq.shape[0] - 1):\n      row_start = row_seq[i]\n      row_end = row_seq[i + 1] + 1 if overlapping else row_seq[i + 1]\n      row_end = min(row_end, row_max)\n      output_image = np.vstack((output_image, np.amax(\n          input_matrix[row_start:row_end, :], axis=0)))  # axis 0 is along row\n    # remove the sentinel row\n    return output_image[1:, :]\n\n  def _MaxPoolAlongCols(self, input_matrix, col_seq, overlapping):\n    \"\"\"Perform max pool along column of a 2-D matrix based on col_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = input_matrix.num_rows\n        * num_cols = len(col_seq)-1.\n    \"\"\"\n    input_matrix = input_matrix.transpose()\n    output_matrix = self._MaxPoolAlongRows(input_matrix, col_seq, overlapping)\n    return output_matrix.transpose()\n\n  def _GetExpectedFractionalMaxPoolResult(self, input_tensor, row_seq, col_seq,\n                                          overlapping):\n    \"\"\"Get expected fractional max pool result.\n\n    row_seq and col_seq together defines the fractional pooling region.\n\n    Args:\n      input_tensor: Original input tensor, assuming it is a 4-D tensor, with\n        dimension as [batch, height/row, width/column, channels/depth].\n      row_seq: Cumulative pooling sequence along row.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Use overlapping when doing pooling.\n\n    Returns:\n      A 4-D tensor that is the result of max pooling on input_tensor based on\n        pooling region defined by row_seq and col_seq, conditioned on whether or\n        not overlapping is used.\n    \"\"\"\n    input_shape = input_tensor.shape\n    output_shape = (input_shape[0], len(row_seq) - 1, len(col_seq) - 1,\n                    input_shape[3])\n    output_tensor = np.zeros(shape=output_shape, dtype=input_tensor.dtype)\n    for batch in range(input_shape[0]):\n      for channel in range(input_shape[3]):\n        two_dim_slice = input_tensor[batch, :, :, channel]\n        tmp = self._MaxPoolAlongRows(two_dim_slice, row_seq, overlapping)\n        output_tensor[batch, :, :, channel] = self._MaxPoolAlongCols(\n            tmp, col_seq, overlapping)\n\n    return output_tensor\n\n  def _ValidateFractionalMaxPoolResult(self, input_tensor, pooling_ratio,\n                                       pseudo_random, overlapping):\n    \"\"\"Validate FractionalMaxPool's result against expected.\n\n    Expected result is computed given input_tensor, and pooling region defined\n    by row_seq and col_seq.\n\n    Args:\n      input_tensor: A tensor or numpy ndarray.\n      pooling_ratio: A list or tuple of length 4, first and last element be 1.\n      pseudo_random: Use pseudo random method to generate pooling sequence.\n      overlapping: Use overlapping when pooling.\n\n    Returns:\n      None\n    \"\"\"\n    with self.cached_session():\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      actual, row_seq, col_seq = self.evaluate([p, r, c])\n      expected = self._GetExpectedFractionalMaxPoolResult(input_tensor, row_seq,\n                                                          col_seq, overlapping)\n      self.assertShapeEqual(expected, p)\n      self.assertAllClose(expected, actual)\n\n  def _testVisually(self):\n    \"\"\"Manual test by printing out intermediate result of a small random tensor.\n\n    Since _GetExpectedFractionalMaxPoolResult is 'automated', it feel safer to\n    have a test case that you can see what's happening.\n    This test will generate a small, random, int 2D matrix, and feed it to\n    FractionalMaxPool and _GetExpectedFractionalMaxPoolResult.\n    \"\"\"\n    num_rows = 6\n    num_cols = 6\n    tensor_shape = (1, num_rows, num_cols, 1)\n    pseudo_random = False\n    for overlapping in True, False:\n      print(\"-\" * 70)\n      print(\"Testing FractionalMaxPool with overlapping = {}\".format(\n          overlapping))\n      rand_mat = self._PRNG.randint(10, size=tensor_shape)\n      pooling_ratio = [1, math.sqrt(2), math.sqrt(2), 1]\n      with self.cached_session():\n        p, r, c = nn_ops.fractional_max_pool_v2(\n            rand_mat,\n            pooling_ratio,\n            pseudo_random,\n            overlapping,\n            seed=self._SEED)\n        tensor_output, row_seq, col_seq = self.evaluate([p, r, c])\n        expected_result = self._GetExpectedFractionalMaxPoolResult(rand_mat,\n                                                                   row_seq,\n                                                                   col_seq,\n                                                                   overlapping)\n        print(\"row sequence:\")\n        print(row_seq)\n        print(\"column sequence:\")\n        print(col_seq)\n\n        print(\"Input:\")\n        # Print input with pooling region marked.\n        for i in range(num_rows):\n          row_to_print = []\n          for j in range(num_cols):\n            if j in col_seq:\n              row_to_print.append(\"|\")\n            row_to_print.append(str(rand_mat[0, i, j, 0]))\n          row_to_print.append(\"|\")\n          if i in row_seq:\n            print(\"-\" * 2 * len(row_to_print))\n          print(\" \".join(row_to_print))\n        print(\"-\" * 2 * len(row_to_print))\n\n        print(\"Output from FractionalMaxPool:\")\n        print(tensor_output[0, :, :, 0])\n        print(\"Expected result:\")\n        print(expected_result[0, :, :, 0])\n\n  def testAllInputOptions(self):\n    \"\"\"Try all possible input options for fractional_max_pool.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalMaxPoolResult(\n            rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n            overlapping)\n\n  def testIntegerTensorInput(self):\n    \"\"\"Test it works fine when input tensor is integer type.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    pseudo_random = True\n    overlapping = True\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    rand_mat = self._PRNG.randint(1000, size=tensor_shape)\n    self._ValidateFractionalMaxPoolResult(rand_mat,\n                                          [1, math.sqrt(3), math.sqrt(2), 1],\n                                          pseudo_random, overlapping)\n\n  def testDifferentTensorShapes(self):\n    \"\"\"Test different shapes of input tensor.\n\n    Mainly test different combinations of num_rows and num_cols.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    for num_batches in [1, 3]:\n      for num_channels in [1, 3]:\n        for num_rows in [10, 20, 50]:\n          for num_cols in [10, 20, 50]:\n            tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n            # random tensor with value in [-500.0, 500.0)\n            rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n            self._ValidateFractionalMaxPoolResult(\n                rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n                overlapping)\n\n  def testLargePoolingRatio(self):\n    \"\"\"Test when pooling ratio is not within [1, 2).\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    for row_ratio in [math.sqrt(11), math.sqrt(37)]:\n      for col_ratio in [math.sqrt(11), math.sqrt(27)]:\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalMaxPoolResult(rand_mat,\n                                              [1, row_ratio, col_ratio, 1],\n                                              pseudo_random, overlapping)\n\n  def testDivisiblePoolingRatio(self):\n    \"\"\"Test when num of rows/cols can evenly divide pooling ratio.\n\n    This is a case regular max pooling can handle. Should be handled by\n    fractional pooling as well.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    # random tensor with value in [-500.0, 500.0)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    self._ValidateFractionalMaxPoolResult(rand_mat, [1, 2, 2, 1], pseudo_random,\n                                          overlapping)\n\n  @test_util.run_deprecated_v1\n  def testDifferentInputTensorShape(self):\n    \"\"\"Runs the operation in one session with different input tensor shapes.\"\"\"\n    with self.cached_session() as sess:\n      input_holder = array_ops.placeholder(dtypes.float32,\n                                           [None, None, None, 3])\n      pooling_ratio = [1, 1.5, 1.5, 1]\n      pseudo_random = False\n      overlapping = False\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_holder,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      # First run.\n      input_a = np.zeros([3, 32, 32, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_a})\n      expected = self._GetExpectedFractionalMaxPoolResult(\n          input_a, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n      # Second run.\n      input_b = np.zeros([4, 45, 45, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_b})\n      expected = self._GetExpectedFractionalMaxPoolResult(\n          input_b, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n\n  def testDeterminismExceptionThrowing(self):\n    tensor_shape = (5, 20, 20, 3)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    with test_util.deterministic_ops():\n      with self.assertRaisesRegex(\n          ValueError, \"requires a non-zero seed to be passed in when \"\n          \"determinism is enabled\"):\n        nn_ops.fractional_max_pool_v2(rand_mat, [1, 1.5, 1.5, 1])\n      nn_ops.fractional_max_pool_v2(rand_mat, [1, 1.5, 1.5, 1], seed=1)\n\n      with self.assertRaisesRegex(ValueError,\n                                  'requires \"seed\" and \"seed2\" to be non-zero'):\n        nn_ops.fractional_max_pool(rand_mat, [1, 1.5, 1.5, 1])\n      nn_ops.fractional_max_pool(\n          rand_mat, [1, 1.5, 1.5, 1], seed=1, seed2=1, deterministic=True)\n\n  def testPoolingRatio(self):\n    with self.cached_session() as _:\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Pooling ratio is higher than input dimension size for dimension 1.*\"\n      ):\n        result = nn_ops.gen_nn_ops.fractional_max_pool(\n            value=constant_op.constant(\n                value=[[[[1, 4, 2, 3]]]], dtype=dtypes.int64),\n            pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n            pseudo_random=False,\n            overlapping=False,\n            deterministic=False,\n            seed=0,\n            seed2=0,\n            name=None)\n        self.evaluate(result)\n\n\nclass FractionalMaxPoolGradTest(test.TestCase):\n  \"\"\"Tests for FractionalMaxPoolGrad.\n\n  Two types of tests for FractionalMaxPoolGrad.\n  1) Test fractional_max_pool_grad() directly.\n    This type of test relies on gen_nn_ops.max_pool_grad() returns the correct\n  result. For example:\n    * input_tensor_shape = (1, 10, 10, 1)\n    * window_size = (1, 2, 2, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not really import, since 10/2 is divisible\n  max pooling should generate the same result as fractional max pooling with:\n    * row_sequence = [0, 2, 4, 6, 8, 10]\n    * col_sequence = [0, 2, 4, 6, 8, 10]\n    * overlapping = False\n  This also means their gradients in such case will be the same.\n\n    Similarly, when\n    * input_tensor_shape = (1, 7, 7, 1)\n    * window_size = (1, 3, 3, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not important\n  max pooling should generate the same result as fractional max pooling with:\n    * row_sequence = [0, 2, 4, 7]\n    * col_sequence = [0, 2, 4, 7]\n    * overlapping = True\n  2) Test through compute_gradient_error()\n  \"\"\"\n\n  _PRNG = np.random.RandomState(341261)\n  _SEED = 123456\n\n  def _GenerateUniqueRandomInputTensor(self, shape):\n    \"\"\"Generate 'unique' random input tensor.\n\n    'Unique' means there's no collision values in the tensor, all elements are\n    different. This is done by generating sequence of integers with step of 1\n    and then randomly shuffle these integers.\n\n    Args:\n      shape: Shape of the tensor desired.\n\n    Returns:\n      A numpy ndarray with size = shape and dtype = numpy.float32.\n    \"\"\"\n    num_elements = 1\n    for size in shape:\n      num_elements *= size\n    x = np.arange(num_elements, dtype=np.float32)\n    self._PRNG.shuffle(x)\n    return x.reshape(shape)\n\n  def testDirectNotUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = row_window_size * 5\n          num_cols = col_window_size * 7\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateUniqueRandomInputTensor(input_shape))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size, col_window_size, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.max_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              output_backprop = self._PRNG.randint(100, size=output_data.shape)\n              input_backprop_tensor = gen_nn_ops.max_pool_grad(\n                  input_tensor, output_tensor, output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows + 1, row_window_size))\n              col_seq = list(range(0, num_cols + 1, col_window_size))\n              fmp_input_backprop_tensor = gen_nn_ops.fractional_max_pool_grad(\n                  input_tensor,\n                  output_tensor,\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=False)\n              fmp_input_backprop = self.evaluate(fmp_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fmp_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fmp_input_backprop)\n\n  def testDirectUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = (row_window_size - 1) * 5 + 1\n          num_cols = (col_window_size - 1) * 7 + 1\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateUniqueRandomInputTensor(input_shape))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size - 1, col_window_size - 1, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.max_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              output_backprop = self._PRNG.randint(100, size=output_data.shape)\n              input_backprop_tensor = gen_nn_ops.max_pool_grad(\n                  input_tensor, output_tensor, output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows, row_window_size - 1))\n              col_seq = list(range(0, num_cols, col_window_size - 1))\n              row_seq[-1] += 1\n              col_seq[-1] += 1\n              fmp_input_backprop_tensor = gen_nn_ops.fractional_max_pool_grad(\n                  input_tensor,\n                  output_tensor,\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=True)\n              fmp_input_backprop = self.evaluate(fmp_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fmp_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fmp_input_backprop)\n\n  @test_util.run_deprecated_v1\n  def testAllInputOptionsThroughGradientError(self):\n    input_shape = (1, 7, 13, 1)\n    input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n    # Add some randomness to make input_data not so 'integer'\n    input_data += self._PRNG.random_sample(input_shape)\n    pooling_ratio = [1, math.sqrt(2), math.sqrt(3), 1]\n\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        with self.cached_session() as _:\n          input_tensor = constant_op.constant(input_data, shape=input_shape)\n          output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n              input_tensor,\n              pooling_ratio,\n              pseudo_random=pseudo_random,\n              overlapping=overlapping,\n              seed=self._SEED)\n          output_data = self.evaluate(output_tensor)\n          output_shape = output_data.shape\n          # error_margin and delta setting is similar to max_pool_grad.\n          error_margin = 1e-3\n          gradient_error = gradient_checker.compute_gradient_error(\n              input_tensor,\n              input_shape,\n              output_tensor,\n              output_shape,\n              x_init_value=input_data.reshape(input_shape),\n              delta=1e-2)\n          self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testDifferentTensorShapesThroughGradientError(self):\n    pseudo_random = True\n    overlapping = True\n    pooling_ratio = [1, math.sqrt(3), math.sqrt(2), 1]\n    for num_batches in [1, 2]:\n      for num_rows in [5, 13]:\n        for num_cols in [5, 11]:\n          for num_channels in [1, 3]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n            # Add some randomness to make input_data not so 'integer'\n            input_data += self._PRNG.random_sample(input_shape)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(input_data, shape=input_shape)\n              output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n                  input_tensor,\n                  pooling_ratio,\n                  pseudo_random=pseudo_random,\n                  overlapping=overlapping,\n                  seed=self._SEED)\n              output_data = self.evaluate(output_tensor)\n              output_shape = output_data.shape\n              # error_margin and delta setting is similar to max_pool_grad.\n              error_margin = 1e-3\n              gradient_error = gradient_checker.compute_gradient_error(\n                  input_tensor,\n                  input_shape,\n                  output_tensor,\n                  output_shape,\n                  x_init_value=input_data.reshape(input_shape),\n                  delta=1e-2)\n              self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testLargePoolingRatioThroughGradientError(self):\n    input_shape = (1, 17, 23, 1)\n    input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n    # Add some randomness to make input_data not so 'integer'\n    input_data += self._PRNG.random_sample(input_shape)\n    pooling_ratio = (1, math.sqrt(13), math.sqrt(7), 1)\n    output_shape = [int(a / b) for a, b in zip(input_shape, pooling_ratio)]\n    overlapping = True\n    pseudo_random = False\n\n    with self.cached_session() as _:\n      input_tensor = constant_op.constant(input_data, shape=input_shape)\n      output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random=pseudo_random,\n          overlapping=overlapping,\n          seed=self._SEED)\n      # error_margin and delta setting is similar to max_pool_grad.\n      error_margin = 1e-3\n      gradient_error = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_shape,\n          output_tensor,\n          output_shape,\n          x_init_value=input_data.reshape(input_shape),\n          delta=1e-2)\n      self.assertLess(gradient_error, error_margin)\n\n  def testWhenRepeatedMaxValueInPoolingRegion(self):\n    \"\"\"Test when there's repeating value in pooling region.\n\n    There's no formal definition for what the gradient should be when there're\n    multiple max value within a pooling cell. Such as\n        | 1 5 |\n        | 5 3 |\n    The expected result depends heavily on implementation, if someone swap the\n    order of a nested for loop when walking through the tensor, result would be\n    very different.\n\n    The goal of this test is to alert when someone else change the\n    implementation. Current implementation scans row-by-row.\n    \"\"\"\n    input_data = [5.0, 4.0, 6.0, 7.0,\n                  3.0, 5.0, 9.0, 6.0,\n                  8.0, 8.0, 9.0, 5.0,\n                  7.0, 4.0, 0.0, 0.0]  # pyformat: disable\n    input_size = [1, 4, 4, 1]\n    output_backprop = [12.0, 15.0,\n                       17.0, -5.0,\n                       6.0, 21.0]  # pyformat: disable\n    row_seq = [0, 1, 3, 4]\n    col_seq = [0, 2, 4]\n    output_data_not_overlapping = [5.0, 7.0,\n                                   8.0, 9.0,\n                                   7.0, 0.0]  # pyformat: disable\n    output_data_overlapping = [9.0, 9.0,\n                               9.0, 9.0,\n                               7.0, 0.0]  # pyformat: disable\n    output_size = [1, 3, 2, 1]\n    expected_input_backprop_not_overlapping = np.reshape(\n        [12.0, 0.0, 0.0, 15.0,\n         0.0, 0.0, -5.0, 0.0,\n         17.0, 0.0, 0.0, 0.0,\n         6.0, 0.0, 21.0, 0.0],\n        input_size)  # pyformat: disable\n    expected_input_backprop_overlapping = np.reshape(\n        [0.0, 0.0, 0.0, 0.0,\n         0.0, 0.0, 39.0, 0.0,\n         0.0, 0.0, 0.0, 0.0,\n         6.0, 0.0, 21.0, 0.0],\n        input_size)  # pyformat: disable\n    with self.cached_session() as _:\n      # Test when overlapping is False\n      input_tensor = constant_op.constant(input_data, shape=input_size)\n      output_tensor = constant_op.constant(\n          output_data_not_overlapping, shape=output_size)\n      grad = constant_op.constant(output_backprop, shape=output_size)\n      r = gen_nn_ops.fractional_max_pool_grad(\n          input_tensor,\n          output_tensor,\n          grad,\n          row_seq,\n          col_seq,\n          overlapping=False)\n      input_backprop_not_overlapping = self.evaluate(r)\n      self.assertShapeEqual(\n          np.reshape(expected_input_backprop_not_overlapping, input_size), r)\n      self.assertAllClose(expected_input_backprop_not_overlapping,\n                          input_backprop_not_overlapping)\n      # Test when overlapping is True\n      output_tensor = constant_op.constant(\n          output_data_overlapping, shape=output_size)\n      r = gen_nn_ops.fractional_max_pool_grad(\n          input_tensor, output_tensor, grad, row_seq, col_seq, overlapping=True)\n      input_backprop_overlapping = self.evaluate(r)\n      self.assertShapeEqual(\n          np.reshape(expected_input_backprop_overlapping, input_size), r)\n      self.assertAllClose(expected_input_backprop_overlapping,\n                          input_backprop_overlapping)\n\n  def testInvalidSeqRaiseErrorForFractionalMaxPoolGrad(self):\n    with self.assertRaises(errors.InvalidArgumentError):\n      with self.cached_session():\n        overlapping = True\n        orig_input = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        orig_output = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        out_backprop = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        row_pooling_sequence = constant_op.constant(\n            0, shape=[5], dtype=dtypes.int64)\n        col_pooling_sequence = constant_op.constant(\n            0, shape=[5], dtype=dtypes.int64)\n        t = gen_nn_ops.FractionalMaxPoolGrad(\n            orig_input=orig_input,\n            orig_output=orig_output,\n            out_backprop=out_backprop,\n            row_pooling_sequence=row_pooling_sequence,\n            col_pooling_sequence=col_pooling_sequence,\n            overlapping=overlapping)\n        self.evaluate(t)\n\n  def testOverLargeSeqRaiseErrorForFractionalMaxPoolGrad(self):\n    with self.assertRaises(errors.InvalidArgumentError):\n      with self.cached_session():\n        overlapping = False\n        orig_input = [[[[1, 1, 1, 1, 1]]]]\n        orig_output = [[[[1, 1, 1]]]]\n        out_backprop = [[[[3], [3], [6]]]]\n        row_pooling_sequence = [-0x4000000, 1, 1]\n        col_pooling_sequence = [-0x4000000, 1, 1]\n        t = gen_nn_ops.FractionalMaxPoolGrad(\n            orig_input=orig_input,\n            orig_output=orig_output,\n            out_backprop=out_backprop,\n            row_pooling_sequence=row_pooling_sequence,\n            col_pooling_sequence=col_pooling_sequence,\n            overlapping=overlapping)\n        self.evaluate(t)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <cmath>\n#include <random>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/util/guarded_philox_random.h\"\n#include \"tensorflow/core/util/overflow.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nclass FractionalAvgPoolOp : public OpKernel {\n public:\n  explicit FractionalAvgPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\n                    \"pooling_ratio field must specify 4 dimensions\"));\n    for (std::size_t i = 0; i < pooling_ratio_.size(); ++i) {\n      OP_REQUIRES(context, pooling_ratio_[i] >= 1,\n                  errors::InvalidArgument(\n                      \"pooling_ratio cannot be smaller than 1, got: \",\n                      pooling_ratio_[i]));\n    }\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional average pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n      OP_REQUIRES(\n          context, input_size[i] >= pooling_ratio_[i],\n          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n                                  \"dimension size for dimension \",\n                                  i, \". Input dim size: \", input_size[i],\n                                  \" pooling ratio: \", pooling_ratio_[i]));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> row_cum_seq;\n    std::vector<int64_t> col_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    row_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                          &generator, pseudo_random_);\n    col_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                          &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_row_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     1, TensorShape({static_cast<int64_t>(row_cum_seq.size())}),\n                     &output_row_seq_tensor));\n    Tensor* output_col_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     2, TensorShape({static_cast<int64_t>(col_cum_seq.size())}),\n                     &output_col_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    // out_count corresponds to number of elements in each pooling cell.\n    Eigen::Matrix<T, Eigen::Dynamic, 1> out_count(out_mat.cols());\n\n    // Initializes the output tensor and out_count with 0.\n    out_mat.setZero();\n    out_count.setZero();\n\n    auto output_row_seq_flat = output_row_seq_tensor->flat<int64_t>();\n    auto output_col_seq_flat = output_col_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < row_cum_seq.size(); ++i) {\n      output_row_seq_flat(i) = row_cum_seq[i];\n    }\n\n    for (int i = 0; i < col_cum_seq.size(); ++i) {\n      output_col_seq_flat(i) = col_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: row / row\n    // 2: col / col\n    // 3: depth / channel\n    const int64_t row_max = input_size[1] - 1;\n    const int64_t col_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // row sequence.\n      for (int64_t hs = 0; hs < row_cum_seq.size() - 1; ++hs) {\n        // row start and end.\n        const int64_t row_start = row_cum_seq[hs];\n        int64_t row_end =\n            overlapping_ ? row_cum_seq[hs + 1] : row_cum_seq[hs + 1] - 1;\n        row_end = std::min(row_end, row_max);\n\n        // col sequence.\n        for (int64_t ws = 0; ws < col_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // col start and end.\n          const int64_t col_start = col_cum_seq[ws];\n          int64_t col_end =\n              overlapping_ ? col_cum_seq[ws + 1] : col_cum_seq[ws + 1] - 1;\n          col_end = std::min(col_end, col_max);\n          for (int64_t h = row_start; h <= row_end; ++h) {\n            for (int64_t w = col_start; w <= col_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) += in_mat.col(in_offset);\n              out_count(out_offset)++;\n            }\n          }\n        }\n      }\n    }\n    DCHECK_GT(out_count.minCoeff(), 0);\n    out_mat.array().rowwise() /= out_count.transpose().array();\n  }\n\n private:\n  bool deterministic_;\n  int64_t seed_;\n  int64_t seed2_;\n  std::vector<float> pooling_ratio_;\n  bool pseudo_random_;\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALAVGPOOL(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"FractionalAvgPool\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      FractionalAvgPoolOp<type>)\n\nREGISTER_FRACTIONALAVGPOOL(int32);\nREGISTER_FRACTIONALAVGPOOL(int64_t);\nREGISTER_FRACTIONALAVGPOOL(float);\nREGISTER_FRACTIONALAVGPOOL(double);\n\n#undef REGISTER_FRACTIONALAVGPOOL\n\ntemplate <class T>\nclass FractionalAvgPoolGradOp : public OpKernel {\n public:\n  explicit FractionalAvgPoolGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Here's the basic idea:\n    // Batch and depth dimension are independent from row and col dimension. And\n    // because FractionalAvgPool currently only support pooling along row and\n    // col, we can basically think of this 4D tensor backpropagation as\n    // operation of a series of 2D planes.\n    //\n    // For each element of a 'slice' (2D plane) of output_backprop, we need to\n    // figure out its contributors when doing FractionalAvgPool operation. This\n    // can be done based on row_pooling_sequence, col_pooling_seq and\n    // overlapping.\n    // Once we figure out the original contributors, we just need to evenly\n    // divide the value of this element among these contributors.\n    //\n    // Internally, we divide the out_backprop tensor and store it in a temporary\n    // tensor of double type. And cast it to the corresponding type.\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenDoubleMatrixMap;\n\n    // Grab the inputs.\n    const Tensor& orig_input_tensor_shape = context->input(0);\n    OP_REQUIRES(context,\n                orig_input_tensor_shape.dims() == 1 &&\n                    orig_input_tensor_shape.NumElements() == 4,\n                errors::InvalidArgument(\"original input tensor shape must be\"\n                                        \"1-dimensional and 4 elements\"));\n    int64_t num_elements = 1;\n    for (int i = 0; i < orig_input_tensor_shape.dims(); i++) {\n      OP_REQUIRES(context, orig_input_tensor_shape.dim_size(i) > 0,\n                  errors::InvalidArgument(\n                      \"orig_input_tensor_shape must be positive, got: \",\n                      orig_input_tensor_shape.dim_size(i)));\n      num_elements = MultiplyWithoutOverflow(\n          num_elements, orig_input_tensor_shape.dim_size(i));\n      OP_REQUIRES(\n          context, num_elements > 0,\n          errors::InvalidArgument(\n              \"The total elements specified by orig_input_tensor_shape\",\n              \" is too large. Encountered overflow after multiplying \",\n              orig_input_tensor_shape.dim_size(i), \", result: \", num_elements));\n    }\n\n    const Tensor& out_backprop = context->input(1);\n    OP_REQUIRES(context, out_backprop.dims() == 4,\n                errors::InvalidArgument(\"out_backprop must be 4-dimensional\"));\n    for (int i = 0; i < out_backprop.dims(); i++) {\n      OP_REQUIRES(context, out_backprop.dim_size(i) > 0,\n                  errors::InvalidArgument(\n                      \"out_backprop must be positive for all dimension, got:\",\n                      out_backprop.dim_size(i)));\n    }\n\n    const Tensor& row_seq_tensor = context->input(2);\n    const Tensor& col_seq_tensor = context->input(3);\n\n    const int64_t out_batch = out_backprop.dim_size(0);\n    const int64_t out_rows = out_backprop.dim_size(1);\n    const int64_t out_cols = out_backprop.dim_size(2);\n    const int64_t out_depth = out_backprop.dim_size(3);\n\n    OP_REQUIRES(context, row_seq_tensor.NumElements() > out_rows,\n                errors::InvalidArgument(\"Given out_backprop shape \",\n                                        out_backprop.shape().DebugString(),\n                                        \", row_seq_tensor must have at least \",\n                                        out_rows + 1, \" elements, but got \",\n                                        row_seq_tensor.NumElements()));\n    OP_REQUIRES(context, col_seq_tensor.NumElements() > out_cols,\n                errors::InvalidArgument(\"Given out_backprop shape \",\n                                        out_backprop.shape().DebugString(),\n                                        \", col_seq_tensor must have at least \",\n                                        out_cols + 1, \" elements, but got \",\n                                        col_seq_tensor.NumElements()));\n\n    auto row_seq_tensor_flat = row_seq_tensor.flat<int64_t>();\n    auto col_seq_tensor_flat = col_seq_tensor.flat<int64_t>();\n    auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64_t>();\n\n    const int64_t in_batch = orig_input_tensor_shape_flat(0);\n    const int64_t in_rows = orig_input_tensor_shape_flat(1);\n    const int64_t in_cols = orig_input_tensor_shape_flat(2);\n    const int64_t in_depth = orig_input_tensor_shape_flat(3);\n    OP_REQUIRES(\n        context, in_batch != 0,\n        errors::InvalidArgument(\"Batch dimension of input must not be 0\"));\n    OP_REQUIRES(\n        context, in_rows != 0,\n        errors::InvalidArgument(\"Rows dimension of input must not be 0\"));\n    OP_REQUIRES(\n        context, in_cols != 0,\n        errors::InvalidArgument(\"Columns dimension of input must not be 0\"));\n    OP_REQUIRES(\n        context, in_depth != 0,\n        errors::InvalidArgument(\"Depth dimension of input must not be 0\"));\n\n    constexpr int tensor_in_and_out_dims = 4;\n    // Transform orig_input_tensor_shape into TensorShape\n    TensorShape in_shape;\n    for (auto i = 0; i < tensor_in_and_out_dims; ++i) {\n      in_shape.AddDim(orig_input_tensor_shape_flat(i));\n    }\n\n    // Create intermediate in_backprop.\n    Tensor in_backprop_tensor_temp;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_temp(\n                                {0}, DataTypeToEnum<double>::v(), in_shape,\n                                &in_backprop_tensor_temp));\n    in_backprop_tensor_temp.flat<double>().setZero();\n    // Transform 4D tensor to 2D matrix.\n    EigenDoubleMatrixMap in_backprop_tensor_temp_mat(\n        in_backprop_tensor_temp.flat<double>().data(), in_depth,\n        in_cols * in_rows * in_batch);\n    ConstEigenMatrixMap out_backprop_mat(out_backprop.flat<T>().data(),\n                                         out_depth,\n                                         out_cols * out_rows * out_batch);\n    // Loop through each element of out_backprop and evenly distribute the\n    // element to the corresponding pooling cell.\n    const int64_t in_max_row_index = in_rows - 1;\n    const int64_t in_max_col_index = in_cols - 1;\n    for (int64_t b = 0; b < out_batch; ++b) {\n      for (int64_t r = 0; r < out_rows; ++r) {\n        const int64_t in_row_start = row_seq_tensor_flat(r);\n\n        int64_t in_row_end = overlapping_ ? row_seq_tensor_flat(r + 1)\n                                          : row_seq_tensor_flat(r + 1) - 1;\n        in_row_end = std::min(in_row_end, in_max_row_index);\n        OP_REQUIRES(context, in_row_start >= 0 && in_row_end >= 0,\n                    errors::InvalidArgument(\n                        \"Row sequence tensor values must not be negative, got \",\n                        row_seq_tensor_flat));\n\n        for (int64_t c = 0; c < out_cols; ++c) {\n          const int64_t in_col_start = col_seq_tensor_flat(c);\n          int64_t in_col_end = overlapping_ ? col_seq_tensor_flat(c + 1)\n                                            : col_seq_tensor_flat(c + 1) - 1;\n          in_col_end = std::min(in_col_end, in_max_col_index);\n\n          OP_REQUIRES(\n              context, in_col_start >= 0 && in_col_end >= 0,\n              errors::InvalidArgument(\n                  \"Column sequence tensor values must not be negative, got \",\n                  col_seq_tensor_flat));\n          const int64_t num_elements_in_pooling_cell =\n              (in_row_end - in_row_start + 1) * (in_col_end - in_col_start + 1);\n          const int64_t out_index = (b * out_rows + r) * out_cols + c;\n          // Now we can evenly distribute out_backprop(b, h, w, *) to\n          // in_backprop(b, hs:he, ws:we, *).\n          for (int64_t in_r = in_row_start; in_r <= in_row_end; ++in_r) {\n            for (int64_t in_c = in_col_start; in_c <= in_col_end; ++in_c) {\n              const int64_t in_index = (b * in_rows + in_r) * in_cols + in_c;\n              // Walk through each channel (depth).\n              for (int64_t d = 0; d < out_depth; ++d) {\n                const double out_backprop_element = static_cast<double>(\n                    out_backprop_mat.coeffRef(d, out_index));\n                double& in_backprop_ref =\n                    in_backprop_tensor_temp_mat.coeffRef(d, in_index);\n                in_backprop_ref +=\n                    out_backprop_element / num_elements_in_pooling_cell;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // Depending on the type, cast double to type T.\n    Tensor* in_backprop_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, in_shape, &in_backprop_tensor));\n    auto in_backprop_tensor_flat = in_backprop_tensor->flat<T>();\n    auto in_backprop_tensor_temp_flat = in_backprop_tensor_temp.flat<double>();\n    for (int64_t i = 0; i < in_backprop_tensor_flat.size(); ++i) {\n      in_backprop_tensor_flat(i) =\n          static_cast<T>(in_backprop_tensor_temp_flat(i));\n    }\n  }\n\n private:\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALAVGPOOLGRAD(type)              \\\n  REGISTER_KERNEL_BUILDER(Name(\"FractionalAvgPoolGrad\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          FractionalAvgPoolGradOp<type>)\n\nREGISTER_FRACTIONALAVGPOOLGRAD(int32);\nREGISTER_FRACTIONALAVGPOOLGRAD(int64_t);\nREGISTER_FRACTIONALAVGPOOLGRAD(float);\nREGISTER_FRACTIONALAVGPOOLGRAD(double);\n\n#undef REGISTER_FRACTIONALAVGPOOLGRAD\n}  // namespace tensorflow\n", "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <cmath>\n#include <random>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/util/guarded_philox_random.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nclass FractionalMaxPoolOp : public OpKernel {\n public:\n  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n    for (std::size_t i = 0; i < pooling_ratio_.size(); ++i) {\n      OP_REQUIRES(context, pooling_ratio_[i] >= 1,\n                  errors::InvalidArgument(\n                      \"pooling_ratio cannot be smaller than 1, got: \",\n                      pooling_ratio_[i]));\n    }\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n\n      OP_REQUIRES(\n          context, input_size[i] >= pooling_ratio_[i],\n          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n                                  \"dimension size for dimension \",\n                                  i, \". Input dim size: \", input_size[i],\n                                  \" pooling ratio: \", pooling_ratio_[i]));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      // This must match the same logic in the shape function in\n      // core/ops/nn_ops.cc.\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n\n    // Initializes the output tensor with MIN<T>.\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: height / row\n    // 2: width / col\n    // 3: depth / channel\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // height sequence.\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        // height start and end.\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n\n        // width sequence.\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // width start and end.\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }\n\n private:\n  bool deterministic_;\n  int64_t seed_;\n  int64_t seed2_;\n  std::vector<float> pooling_ratio_;\n  bool pseudo_random_;\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALMAXPOOL(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"FractionalMaxPool\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      FractionalMaxPoolOp<type>)\n\nREGISTER_FRACTIONALMAXPOOL(int32);\nREGISTER_FRACTIONALMAXPOOL(int64_t);\nREGISTER_FRACTIONALMAXPOOL(float);\nREGISTER_FRACTIONALMAXPOOL(double);\n\n#undef REGISTER_FRACTIONALMAXPOOL\n\nstatic const int kInvalidMaxPoolingIndex = -1;\n\ntemplate <class T>\nclass FractionalMaxPoolGradOp : public OpKernel {\n public:\n  explicit FractionalMaxPoolGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // There are two steps when calculating gradient for FractionalMaxPool.\n    // 1) Walk through the process of calculating fractional pooling given\n    //    pooling region; however, in the process, keep track of where the max\n    //    element comes from. (arg_max)\n    // 2) Populate the value of out_backprop to where arg_max indicates. If\n    //    we support overlapping, it is likely to have multiple out_backprop[i]\n    //    propagates back to the same arg_max value.\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<int64, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenIndexMatrixMap;\n\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    const Tensor& height_seq_tensor = context->input(3);\n    const Tensor& width_seq_tensor = context->input(4);\n\n    // Just to make it similar to FractionalMaxPoolOp.\n    constexpr int tensor_in_and_out_dims = 4;\n    OP_REQUIRES(\n        context, tensor_in.dims() == tensor_in_and_out_dims,\n        errors::InvalidArgument(\"orig_input should be a tensor of rank 4, got \",\n                                tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"orig_input must not be empty, got \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\n                    \"orig_output should be a tensor of rank 4, got \",\n                    tensor_out.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"orig_output must not be empty, got \",\n                                        tensor_out.DebugString()));\n    OP_REQUIRES(\n        context,\n        height_seq_tensor.NumElements() * width_seq_tensor.NumElements() <=\n            tensor_in.NumElements(),\n        errors::InvalidArgument(\n            \"Pooling region has more elements than the input tensor. \"\n            \"row_pooling_sequence: \",\n            height_seq_tensor.DebugString(),\n            \"col_pooling_sequence: \", width_seq_tensor.DebugString(),\n            \"orig_input: \", tensor_in.DebugString()));\n\n    //\n    std::vector<int64_t> input_size(tensor_in_and_out_dims);\n    std::vector<int64_t> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] = tensor_out.dim_size(i);\n    }\n\n    // ---------\n    // Step 1\n    // ---------\n    Tensor tensor_out_dup;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_temp(\n                                {1}, DataTypeToEnum<T>::v(), tensor_out.shape(),\n                                &tensor_out_dup));\n    Tensor tensor_out_arg_max;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<int64_t>::v(),\n                                                   tensor_out.shape(),\n                                                   &tensor_out_arg_max));\n    // Find arg_max for each tensor_out\n    ConstEigenMatrixMap tensor_in_mat(\n        tensor_in.flat<T>().data(), input_size[3],\n        input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap tensor_out_dup_mat(\n        tensor_out_dup.flat<T>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n    EigenIndexMatrixMap tensor_out_arg_max_mat(\n        tensor_out_arg_max.flat<int64_t>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n\n    tensor_out_arg_max.flat<int64_t>().setConstant(kInvalidMaxPoolingIndex);\n    // Initializes the duplicate output tensor with MIN<T>.\n    tensor_out_dup.flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n\n    auto height_seq_tensor_flat = height_seq_tensor.flat<int64_t>();\n    auto width_seq_tensor_flat = width_seq_tensor.flat<int64_t>();\n\n    // Now walk through the process of fractional max pooling again.\n    // For both input and output,\n    // 0: batch\n    // 1: height / row\n    // 2: width / col\n    // 3: depth / channel\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // height sequence.\n      for (int64_t hs = 0; hs < height_seq_tensor.dim_size(0) - 1; ++hs) {\n        // height start and end.\n        const int64_t height_start = height_seq_tensor_flat(hs);\n        int64_t height_end = overlapping_ ? height_seq_tensor_flat(hs + 1)\n                                          : height_seq_tensor_flat(hs + 1) - 1;\n        height_end = std::min(height_end, height_max);\n\n        // width sequence.\n        for (int64_t ws = 0; ws < width_seq_tensor.dim_size(0) - 1; ++ws) {\n          const int64_t out_index =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // width start and end.\n          const int64_t width_start = width_seq_tensor_flat(ws);\n          int64_t width_end = overlapping_ ? width_seq_tensor_flat(ws + 1)\n                                           : width_seq_tensor_flat(ws + 1) - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_index =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              // Walk through each channel (depth).\n              for (int64_t d = 0; d < input_size[3]; ++d) {\n                const T& input_ref = tensor_in_mat.coeffRef(d, in_index);\n                T& output_ref = tensor_out_dup_mat.coeffRef(d, out_index);\n                int64_t& out_arg_max_ref =\n                    tensor_out_arg_max_mat.coeffRef(d, out_index);\n                if (output_ref < input_ref ||\n                    out_arg_max_ref == kInvalidMaxPoolingIndex) {\n                  output_ref = input_ref;\n                  int input_offset = in_index * input_size[3] + d;\n                  out_arg_max_ref = input_offset;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // Check tensor_out_dup is the same as tensor_out.\n    ConstEigenMatrixMap tensor_out_mat(\n        tensor_out.flat<T>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n    const int64_t num_reshaped_cols =\n        output_size[2] * output_size[1] * output_size[0];\n    for (int64_t i = 0; i < num_reshaped_cols; ++i) {\n      for (int64_t j = 0; j < output_size[3]; ++j) {\n        OP_REQUIRES(context, tensor_out_dup_mat(j, i) == tensor_out_mat(j, i),\n                    errors::InvalidArgument(\n                        \"tensor_out_dup is not the same as tensor_out\"));\n      }\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, tensor_in.shape(), &output));\n    output->flat<T>().setZero();\n\n    auto out_backprop_flat = out_backprop.flat<T>();\n    auto input_backprop_flat = output->flat<T>();\n    auto out_arg_max_flat = tensor_out_arg_max.flat<int64_t>();\n    int num_total_outputs = out_backprop_flat.size();\n    int num_total_inputs = input_backprop_flat.size();\n\n    for (int index = 0; index < num_total_outputs; ++index) {\n      int input_backprop_index = out_arg_max_flat(index);\n      OP_REQUIRES(\n          context,\n          input_backprop_index >= 0 && input_backprop_index < num_total_inputs,\n          errors::InvalidArgument(\n              \"Invalid input backprop index: \", input_backprop_index, \", \",\n              num_total_inputs));\n      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n    }\n  }\n\n private:\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALMAXPOOLGRAD(type)              \\\n  REGISTER_KERNEL_BUILDER(Name(\"FractionalMaxPoolGrad\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          FractionalMaxPoolGradOp<type>)\n\nREGISTER_FRACTIONALMAXPOOLGRAD(int32);\nREGISTER_FRACTIONALMAXPOOLGRAD(int64_t);\nREGISTER_FRACTIONALMAXPOOLGRAD(float);\nREGISTER_FRACTIONALMAXPOOLGRAD(double);\n\n#undef REGISTER_FRACTIONALMAXPOOLGRAD\n}  // namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <cmath>\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/lib/math/math_util.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nnamespace {\n\nStatus FractionalPoolShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  std::vector<float> pooling_ratio;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"pooling_ratio\", &pooling_ratio));\n  if (pooling_ratio.size() != 4) {\n    return errors::InvalidArgument(\n        \"pooling_ratio field must specify 4 dimensions\");\n  }\n  std::vector<DimensionHandle> output_dims;\n  for (int i = 0; i < 4; ++i) {\n    DimensionHandle d = c->Dim(input, i);\n    if (c->ValueKnown(d)) {\n      // This must match the same logic in the kernel function in\n      // core/kernels/fractional_max_pool_op.cc.\n      auto val =\n          static_cast<int64_t>(std::floor(c->Value(d) / pooling_ratio[i]));\n      if (val < 0) {\n        return errors::InvalidArgument(\"Size computed for dim \", i,\n                                       \" is negative: \", val);\n      }\n      output_dims.push_back(c->MakeDim(val));\n    } else {\n      output_dims.push_back(c->UnknownDim());\n    }\n  }\n\n  for (std::size_t i = 0; i < pooling_ratio.size(); ++i) {\n    if (pooling_ratio[i] < 1) {\n      return errors::InvalidArgument(\n          \"pooling_ratio cannot be smaller than 1, got: \", pooling_ratio[i]);\n    }\n  }\n\n  c->set_output(0, c->MakeShape(output_dims));\n  c->set_output(1, c->Vector(output_dims[1]));\n  c->set_output(2, c->Vector(output_dims[2]));\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolShape);\n\nREGISTER_OP(\"AvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BatchNormWithGlobalNormalization\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"beta: T\")\n    .Input(\"gamma: T\")\n    .Output(\"result: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"BatchNormWithGlobalNormalizationGrad\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"gamma: T\")\n    .Input(\"backprop: T\")\n    .Output(\"dx: T\")\n    .Output(\"dm: T\")\n    .Output(\"dv: T\")\n    .Output(\"db: T\")\n    .Output(\"dg: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n      TF_RETURN_IF_ERROR(\n          c->Merge(input, c->input(4), &input));  // with backprop\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 4; ++i) {  // covers m, v, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle dx;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &dx));\n      c->set_output(0, dx);\n\n      ShapeHandle vector_shape = c->Vector(last_dim);\n      c->set_output(1, vector_shape);\n      c->set_output(2, vector_shape);\n      c->set_output(3, vector_shape);\n      c->set_output(4, vector_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV3\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {bfloat16, float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormV3Shape);\n\nREGISTER_OP(\"_FusedBatchNormEx\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"side_input: num_side_inputs * T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, float, bfloat16}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNorm operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n\nREGISTER_OP(\"FusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV3\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_FusedBatchNormGradEx\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Input(\"offset: float\")\n    .Input(\"y: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Output(\"side_input_backprop: num_side_inputs * T\")\n    .Attr(\"T: {half, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNormGrad operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAdd\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddGrad\")\n    .Attr(\"T: numbertype\")\n    .Input(\"out_backprop: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddGradShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddV1\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding);\n\nREGISTER_OP(\"Conv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropInputShape);\n\n// TODO(jeff): Instead of 'use_cudnn_for_gpu', maybe we should have a\n// more general string attribute ('kernel_impl'?) that can be used to\n// select among several possible implementations.\nREGISTER_OP(\"Conv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: TArgs\")\n    .Input(\"host_args : num_host_args * float\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, int8, qint8}\")\n    .Attr(\"TArgs: list(type)\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"num_host_args: int >= 0 =0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: { 'NHWC', 'NCHW', 'NCHW_VECT_C' } = 'NHWC'\")\n    .Attr(\"filter_format: {'HWIO', 'OIHW', 'OIHW_VECT_I'} = 'HWIO'\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nPerforms a convolution followed by a specified series of operations.\n\nThe inputs to the convolution are `input` and `filter`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [X] and [X,A], where X is one of\n{\"BiasAdd\",\"FusedBatchNorm\"} and A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to op X is the Conv2D result, and the additional input(s) to X\nare specified by `args`.\n* If there is an op A specified, the output of op X is the input to op A, and op\nA produces the _FusedConv2D output. Otherwise, op X produces the _FusedConv2D\noutput.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is expected to\ncreate these operators.\n)doc\");\n\nnamespace {\n\nStatus CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  ShapeHandle resized = input;\n  int paddings_index = 1;\n  int filter_index = 2;\n  if (has_resize) {\n    paddings_index = 2;\n    filter_index = 3;\n\n    ShapeHandle unused_size;\n    TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->Vector(2), &unused_size));\n\n    const Tensor* size = c->input_tensor(1);\n    DimensionHandle new_height = c->UnknownDim();\n    DimensionHandle new_width = c->UnknownDim();\n    if (size != nullptr) {\n      new_height = c->MakeDim(size->flat<int32>()(0));\n      new_width = c->MakeDim(size->flat<int32>()(1));\n    }\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 1, new_height, &resized));\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 2, new_width, &resized));\n  }\n\n  ShapeHandle paddings;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(paddings_index), 2, &paddings));\n  TF_RETURN_IF_ERROR(\n      c->WithRank(resized, c->Value(c->Dim(paddings, 0)), &resized));\n  TF_RETURN_IF_ERROR(\n      c->Merge(paddings, c->Matrix(c->Rank(resized), 2), &paddings));\n\n  const Tensor* paddings_t = c->input_tensor(paddings_index);\n  ShapeHandle padded;\n  if (paddings_t != nullptr) {\n    std::vector<DimensionHandle> output_dims;\n    for (int i = 0; i < 4; ++i) {\n      DimensionHandle dim = c->Dim(resized, i);\n      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 0));\n      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 1));\n      if (p0 < 0 || p1 < 0) {\n        return errors::InvalidArgument(\"Paddings must be non-negative\");\n      }\n\n      TF_RETURN_IF_ERROR(c->Add(dim, p0 + p1, &dim));\n      output_dims.push_back(dim);\n    }\n    padded = c->MakeShape(output_dims);\n  } else {\n    padded = c->UnknownShapeOfRank(4);\n  }\n\n  // Work out the convolution's effect with 'padded' as the input.\n  ShapeHandle filter;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(filter_index), 4, &filter));\n  std::vector<int32> strides;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n  if (strides.size() != 4) {\n    return errors::InvalidArgument(\n        \"Operation requires the stride attribute to contain 4 values, but \",\n        \"got: \", strides.size());\n  }\n\n  int32_t stride_rows = strides[1];\n  int32_t stride_cols = strides[2];\n\n  DimensionHandle batch_size_dim = c->Dim(padded, 0);\n  DimensionHandle in_rows_dim = c->Dim(padded, 1);\n  DimensionHandle in_cols_dim = c->Dim(padded, 2);\n  DimensionHandle filter_rows_dim = c->Dim(filter, 0);\n  DimensionHandle filter_cols_dim = c->Dim(filter, 1);\n  DimensionHandle output_depth_dim = c->Dim(filter, 3);\n\n  DimensionHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(c->Dim(padded, 3), c->Dim(filter, 2), &unused));\n\n  Padding padding;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n  DimensionHandle output_rows, output_cols;\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_rows_dim, filter_rows_dim, stride_rows, padding, &output_rows));\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_cols_dim, filter_cols_dim, stride_cols, padding, &output_cols));\n\n  ShapeHandle output_shape = c->MakeShape(\n      {batch_size_dim, output_rows, output_cols, output_depth_dim});\n  c->set_output(0, output_shape);\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"DataFormatDimMap\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"DataFormatVecPermute\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FusedResizeAndPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"size: int32\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"resize_align_corners: bool = false\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, /*has_resize=*/true);\n    });\n\nREGISTER_OP(\"FusedPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, /*has_resize=*/false);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"DepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: num_args * T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape);\n\nREGISTER_OP(\"Conv3DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropInputV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 5);\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropFilterV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 5, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"AvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"MaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape);\n\nREGISTER_OP(\"MaxPool3DGradGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5 \")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Pool3DShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"L2Loss\")\n    .Input(\"t: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LRN\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"LRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::MaxPoolShapeWithExplicitPadding);\n\nREGISTER_OP(\"MaxPoolV2\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 3));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\nREGISTER_OP(\"MaxPoolGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\n// TODO(b/150813181): Implement explicit padding.\nREGISTER_OP(\"MaxPoolGradGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 5));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"Targmax: {int32, int64} = DT_INT64\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Output(\"argmax: Targmax\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      c->set_output(1, c->output(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"MaxPoolGradGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &unused));\n      // Validate 'argmax' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Dilation2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n      ShapeHandle filter_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &filter_shape));\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the stride attribute to contain 4 values, but \"\n            \"got: \",\n            strides.size());\n      }\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the rates attribute to contain 4 values, but \"\n            \"got: \",\n            rates.size());\n      }\n\n      int32_t stride_rows = strides[1];\n      int32_t stride_cols = strides[2];\n\n      int32_t rate_rows = rates[1];\n      int32_t rate_cols = rates[2];\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 2);\n      DimensionHandle filter_rows_dim = c->Dim(filter_shape, 0);\n      DimensionHandle filter_cols_dim = c->Dim(filter_shape, 1);\n      DimensionHandle output_depth_dim = c->Dim(filter_shape, 2);\n\n      if (!c->ValueKnown(in_rows_dim) || !c->ValueKnown(in_cols_dim) ||\n          !c->ValueKnown(filter_rows_dim) || !c->ValueKnown(filter_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(input_shape, 3), output_depth_dim, &unused));\n\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n      auto filter_rows = c->Value(filter_rows_dim);\n      auto filter_cols = c->Value(filter_cols_dim);\n      auto filter_rows_eff = filter_rows + (filter_rows - 1) * (rate_rows - 1);\n      auto filter_cols_eff = filter_cols + (filter_cols - 1) * (rate_cols - 1);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, filter_rows_eff, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, filter_cols_eff, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n\n      ShapeHandle output_shape = c->MakeShape(\n          {batch_size_dim, output_rows, output_cols, output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Dilation2DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"in_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Dilation2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"filter_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Relu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {realnumbertype, qint8}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Relu6\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Relu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"LeakyRelu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"LeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Elu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"EluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Selu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SeluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softplus\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftplusGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softsign\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftsignGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Softmax\")\n    .Input(\"logits: T\")\n    .Output(\"softmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LogSoftmax\")\n    .Input(\"logits: T\")\n    .Output(\"logsoftmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"SoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: T\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      if (c->WithRank(c->input(0), 2, &input) == OkStatus() &&\n          c->Merge(input, c->input(1), &input) == OkStatus()) {\n        DimensionHandle batch_size = c->Dim(input, 0);\n        c->set_output(0, c->Vector(batch_size));\n        c->set_output(1, input);\n        return OkStatus();\n      }\n      TF_RETURN_IF_ERROR(BroadcastBinaryOpOutputShapeFn(c, 1));\n\n      if (!c->RankKnown(c->output(1))) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank is unknown.\");\n      }\n\n      if (c->Rank(c->output(1)) != 2) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank \",\n            c->Rank(c->output(1)));\n      }\n      DimensionHandle batch_size = c->Dim(c->output(1), 0);\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"SparseSoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: Tlabels\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"Tlabels: {int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle features;\n      ShapeHandle labels;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &features));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &labels));\n\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(features, 0), c->Dim(labels, 0), &batch_size));\n      TF_RETURN_IF_ERROR(c->ReplaceDim(features, 0, batch_size, &features));\n\n      c->set_output(0, c->Vector(batch_size));\n      c->set_output(1, features);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"InTopK\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"k: int\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\n// This is the same as `InTopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"InTopKV2\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Input(\"k: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus TopKShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n  // Get the k value, either from input tensor or attribute.\n  DimensionHandle k_dim;\n  if (c->num_inputs() >= 2) {\n    TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &k_dim));\n  } else {\n    int32_t k;\n    TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n    if (k < 0) {\n      return errors::InvalidArgument(\"Need k >= 0, got \", k);\n    }\n    k_dim = c->MakeDim(k);\n  }\n\n  DimensionHandle last_dim = c->Dim(input, -1);\n  if (c->ValueKnown(last_dim) && c->ValueKnown(k_dim) &&\n      c->Value(last_dim) < c->Value(k_dim)) {\n    return errors::InvalidArgument(\n        \"input must have last dimension >= k = \", c->Value(k_dim), \" but is \",\n        c->Value(last_dim));\n  }\n\n  // Replace last_dim with k_dim.\n  ShapeHandle s;\n  TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n  TF_RETURN_IF_ERROR(c->Concatenate(s, c->Vector(k_dim), &s));\n  c->set_output(0, s);\n  c->set_output(1, s);\n  return OkStatus();\n}\n\n// Utility functions for ApproxTopKShape.\n// It is not easy to link xla/client/lib into the tensorflow core lib, so we\n// have to replicate the logic.\n// LINT.IfChange\ninline uint32_t log2_floor(uint64_t value) {\n  return value == 0 ? 0 : Log2Floor(value);\n}\n\ninline uint32_t log2_ceil(uint64_t value) {\n  return value == 0 ? 0 : Log2Ceiling(value);\n}\n\nStatus ApproxTopKShape(shape_inference::InferenceContext* c) {\n  int64_t k;\n  int64_t reduction_dimension;\n  float recall_target;\n  int64_t reduction_input_size_override;\n  bool aggregate_to_topk;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_dimension\", &reduction_dimension));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"recall_target\", &recall_target));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_input_size_override\",\n                                &reduction_input_size_override));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"aggregate_to_topk\", &aggregate_to_topk));\n  ShapeHandle input_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input_shape));\n  if (reduction_dimension < 0) {\n    // Reverse index\n    reduction_dimension += c->Rank(input_shape);\n  }\n  int64_t reduction_dim_value =\n      c->Value(c->Dim(input_shape, reduction_dimension));\n\n  if (reduction_dim_value < k) {\n    return errors::InvalidArgument(\"input must have last dimension >= k = \", k,\n                                   \" but was \", reduction_dim_value);\n  }\n\n  int64_t output_dim_value = [&] {\n    if (aggregate_to_topk) {\n      return k;\n    }\n    int64_t tpu_tiling = c->Rank(input_shape) == 1 ? 1024 : 128;\n    if (reduction_dim_value <= tpu_tiling || recall_target == 1.0) {\n      return reduction_dim_value;\n    }\n    if (k == 1) {\n      return tpu_tiling;\n    }\n    uint64_t logical_input_size = reduction_input_size_override >= 0\n                                      ? reduction_input_size_override\n                                      : reduction_dim_value;\n    uint64_t m = std::min<uint64_t>(\n        std::max<uint64_t>(\n            static_cast<uint64_t>((1.0 - k) /\n                                  std::log(static_cast<double>(recall_target))),\n            tpu_tiling),\n        reduction_dim_value);\n    uint32_t log2_reduction = log2_floor(logical_input_size / m);\n    if (log2_reduction == 0) {\n      return reduction_dim_value;\n    }\n    log2_reduction = std::min<uint32_t>(\n        log2_reduction, log2_ceil(reduction_dim_value / tpu_tiling));\n    return tensorflow::MathUtil::CeilOfRatio<int64_t>(\n               tensorflow::MathUtil::CeilOfRatio<int64_t>(reduction_dim_value,\n                                                          tpu_tiling),\n               (1 << log2_reduction)) *\n           tpu_tiling;\n  }();\n\n  auto output_dim = c->MakeDim(output_dim_value);\n\n  ShapeHandle output_shape;\n  TF_RETURN_IF_ERROR(c->ReplaceDim(input_shape, reduction_dimension, output_dim,\n                                   &output_shape));\n  c->set_output(0, output_shape);\n  c->set_output(1, output_shape);\n  return OkStatus();\n}\n// LINT.ThenChange(//tensorflow/compiler/xla/client/lib/approx_topk_shape.cc)\n\n}  // namespace\n\nREGISTER_OP(\"TopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .Deprecated(7, \"Use TopKV2 instead\")\n    .SetShapeFn(TopKShapeFn);\n\n// This is the same as `TopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"TopKV2\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(TopKShapeFn);\n\nREGISTER_OP(\"ApproxTopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"reduction_dimension: int = -1\")\n    .Attr(\"recall_target: float = 0.95\")\n    .Attr(\"is_max_k: bool = true\")\n    .Attr(\"reduction_input_size_override: int = -1\")\n    .Attr(\"aggregate_to_topk: bool = true\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(ApproxTopKShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"NthElement\")\n    .Input(\"input: T\")\n    .Input(\"n: int32\")\n    .Output(\"values: T\")\n    .Attr(\"reverse: bool = false\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n      // Get the n value from input tensor, and make sure which is a scalar.\n      DimensionHandle n_dim;\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &n_dim));\n\n      // The last dimension of input tensor must be greater than N.\n      DimensionHandle last_dim = c->Dim(input, -1);\n      if (c->ValueKnown(last_dim) && c->ValueKnown(n_dim) &&\n          c->Value(last_dim) <= c->Value(n_dim)) {\n        return errors::InvalidArgument(\n            \"Input must have last dimension > n = \", c->Value(n_dim),\n            \" but is \", c->Value(last_dim));\n      }\n\n      // Reduce last_dim for output tensor\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalMaxPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalMaxPoolGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRank(c, 4);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalAvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalAvgPoolGrad\")\n    .Input(\"orig_input_tensor_shape: int64\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      if (c->input_tensor(0) != nullptr) {\n        ShapeHandle out;\n        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n        c->set_output(0, out);\n      } else {\n        c->set_output(0, c->UnknownShapeOfRank(4));\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedAvgPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::QuantizedAvgPoolShape);\n\nREGISTER_OP(\"QuantizedBiasAdd\")\n    .Input(\"input: T1\")\n    .Input(\"bias: T2\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_bias: float\")\n    .Input(\"max_bias: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BiasAddShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::QuantizedConv2DShape);\n\nREGISTER_OP(\"QuantizedMaxPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu6\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedReluX\")\n    .Input(\"features: Tinput\")\n    .Input(\"max_value: float\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedBatchNormWithGlobalNormalization\")\n    .Input(\"t: Tinput\")\n    .Input(\"t_min: float\")\n    .Input(\"t_max: float\")\n    .Input(\"m: Tinput\")\n    .Input(\"m_min: float\")\n    .Input(\"m_max: float\")\n    .Input(\"v: Tinput\")\n    .Input(\"v_min: float\")\n    .Input(\"v_max: float\")\n    .Input(\"beta: Tinput\")\n    .Input(\"beta_min: float\")\n    .Input(\"beta_max: float\")\n    .Input(\"gamma: Tinput\")\n    .Input(\"gamma_min: float\")\n    .Input(\"gamma_max: float\")\n    .Output(\"result: out_type\")\n    .Output(\"result_min: float\")\n    .Output(\"result_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i * 3), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n\n      return OkStatus();\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"_MklConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D operator. Uses MKL DNN APIs to perform 2D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\n    MKL version of Conv2D operator for Eager mode. Uses MKL DNN APIs to perform 2D convolution.\n\n    NOTE Do not invoke this operator directly in Python. Eager Op rewrite is\n    expected to invoke these operators.\n    )doc\");\n\nREGISTER_OP(\"__MklDummyConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2D and BiasAdd operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Conv2D and BiasAdd.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_bias: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D and BiasAdd operator. Uses MKL DNN APIs to perform\n2D convolution and add Bias to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nDummy node that enables fusing Pad and Conv2D operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Pad and Conv2D.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_paddings: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nMKL version of Pad and Conv2D operator. Uses MKL DNN APIs to perform\nPad and 2D convolution to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Eager Op rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      // Fetch the data_format attribute, which may not exist.\n      string data_format;\n      Status s = c->GetAttr(\"data_format\", &data_format);\n\n      if (s.ok() && data_format == \"NCHW\") {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -3)));\n      } else {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -1)));\n      }\n      ShapeHandle sh;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &sh));\n      TF_RETURN_IF_ERROR(c->WithRank(sh, 4, &sh));\n      c->set_output(0, sh);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2DBackpropFilter and BiasAddGrad operator\nfor MKL. This node does not perform anything. It is just created as an\nintermediate output of merging Conv2DBackpropFilter and BiasAddGrad.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_bias_grad: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropFilterWithBiasShape)\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilterWithBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#ifdef INTEL_MKL_ML_ONLY\nREGISTER_OP(\"_MklConv2DWithBiasBackpropBias\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the bias.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\nREGISTER_OP(\"_MklConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Eager op rewrite is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape)\n    .Doc(R\"doc(\nMKL version of Conv3D operator. Uses MKL DNN APIs to perform 3D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution3D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv3DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu operator. Uses MKL DNN APIs to implement Relu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu6 operator. Uses MKL DNN APIs to implement Relu6 operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of Relu6Grad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu6 operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of LeakyRelu operator. Uses MKL DNN APIs to implement\nLeakyRelu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of LeakyReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for LeakyReluGrad operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklElu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Elu operator. Uses MKL DNN APIs to implement Elu operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklEluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of EluGrad operator. Uses MKL DNN APIs to compute Elu\ngradients for Elu operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklSoftmax\")\n    .Input(\"logits: T\")\n    .Input(\"mkl_logits: uint8\")\n    .Output(\"softmax: T\")\n    .Output(\"mkl_softmax: uint8\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    })\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n)doc\");\n\nREGISTER_OP(\"_MklTanh\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Tanh operator. Uses MKL DNN APIs to implement Tanh operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklTanhGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of TanhGrad operator. Uses MKL DNN APIs to compute tanh\ngradients for Tanh operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"workspace_enabled: bool = false\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Output(\"workspace: T\")\n#else\n    .Output(\"workspace: uint8\")\n#endif\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolShape)\n    .Doc(R\"doc(\nMKL version of MaxPool operator. Uses MKL DNN APIs to perform max pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPoolGrad\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Input(\"workspace: T\")\n#else\n    .Input(\"workspace: uint8\")\n#endif\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPoolGrad. Uses oneDNN APIs to compute gradients of\nMaxPool operator.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolShape)\n    .Doc(R\"doc(\nMKL version of AvgPool operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPoolGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3D\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of AvgPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3D\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of MaxPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof MaxPool3D function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRN\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    })\n    .Doc(R\"doc(\nMKL version of LRN operator. Uses MKL DNN APIs to perform local response\nnormalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_input_grads: uint8\")\n    .Input(\"mkl_input_image: uint8\")\n    .Input(\"mkl_output_image: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of LRNGrad operator. Uses MKL DNN APIs to compute gradient for\nlocal response normalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNorm operator. Uses oneDNN APIs to perform fused\nbatch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNormGrad operator. Uses oneDNN APIs to compute\ngradients for fused batch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"_MklFusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_MklToTf\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, bfloat16, qint8, quint8, qint32}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to convert a tensor from MKL layout to TensorFlow layout.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklInputConversion\")\n    .Input(\"input_0: T\")\n    .Input(\"input_1: T\")\n    .Input(\"mkl_input_0: uint8\")\n    .Input(\"mkl_input_1: uint8\")\n    .Output(\"output_0: T\")\n    .Output(\"output_1: T\")\n    .Output(\"mkl_output_0: uint8\")\n    .Output(\"mkl_output_1: uint8\")\n    // All datatypes supported by element-wise ops\n    .Attr(\n        \"T: {half, float, bfloat16, double, uint8, int8, uint16, int16, int32, \"\n        \"int64, complex64, complex128}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to process the inputs to an elementwise MKL op. Both inputs\nneed to be either in TF or in MKL format. This op is added before every\nelement-wise MKL op.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#endif  // INTEL_MKL\nREGISTER_OP(\"QuantizedConv2DAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and BiasAdd.\nREGISTER_OP(\"QuantizedConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and Relu.\nREGISTER_OP(\"QuantizedConv2DAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Relu, and Requantize.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Sum, and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSignedSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      // Since activations are not requantized per channel, `min_output`\n      // and `max_output` are scalars.\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized MatMul and BiasAdd.\nREGISTER_OP(\"QuantizedMatMulWithBias\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRelu\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: float\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndReluAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndDequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: {float}\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DPerChannel\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedDepthwiseConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"IsotonicRegression\")\n    .Input(\"input: T\")\n    .Output(\"output: output_dtype\")\n    .Output(\"segments: int32\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"output_dtype: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* context) {\n      context->set_output(0, context->input(0));\n      context->set_output(1, context->input(0));\n      return OkStatus();\n    });\n\n}  // namespace tensorflow\n", "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/fake_input.h\"\n#include \"tensorflow/core/framework/node_def_builder.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference_testutil.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\n\nTEST(NNOpsTest, TopK_ShapeFn) {\n  ShapeInferenceTestOp op(\"TopK\");\n  auto set_k = [&op](int k) {\n    TF_ASSERT_OK(NodeDefBuilder(\"test\", \"Pack\")\n                     .Input({{\"a\", 0, DT_FLOAT}})\n                     .Attr(\"k\", k)\n                     .Finalize(&op.node_def));\n  };\n\n  set_k(20);\n  // With known input, each output is an unknown shape.\n  INFER_OK(op, \"?\", \"?;?\");\n  // With vector input, each output is [k].\n  INFER_OK(op, \"[20]\", \"[20];[20]\");\n  INFER_OK(op, \"[21]\", \"[20];[20]\");\n\n  // With input rank 3, each output is the two first 2 dims of input, plus k.\n  INFER_OK(op, \"[1,?,21]\", \"[d0_0,d0_1,20];[d0_0,d0_1,20]\");\n  // With input rank 4, each output is the two first 3 dims of input, plus k.\n  INFER_OK(op, \"[1,?,21,?]\", \"[d0_0,d0_1,d0_2,20];[d0_0,d0_1,d0_2,20]\");\n\n  INFER_ERROR(\"Shape must be at least rank 1 but is rank 0\", op, \"[]\");\n  INFER_ERROR(\"input must have last dimension >= k = 20 but is 1\", op, \"[1]\");\n  INFER_ERROR(\"input must have last dimension >= k = 20 but is 4\", op,\n              \"[1,2,3,4]\");\n  set_k(-1);\n  INFER_ERROR(\"Need k >= 0, got -1\", op, \"[1,2,3,4]\");\n}\n\nTEST(NNOpsTest, TopKV2_ShapeFn) {\n  ShapeInferenceTestOp op(\"TopKV2\");\n  op.input_tensors.resize(2);\n\n  Tensor k_t;\n  op.input_tensors[1] = &k_t;\n\n  k_t = test::AsScalar<int32>(20);\n  // With known input, each output is an unknown shape.\n  INFER_OK(op, \"?;[]\", \"?;?\");\n  // With vector input, each output is [k].\n  INFER_OK(op, \"[20];[]\", \"[20];[20]\");\n\n  // With input rank 3, each output is the two first 2 dims of input, plus k.\n  INFER_OK(op, \"[1,?,21];[]\", \"[d0_0,d0_1,20];[d0_0,d0_1,20]\");\n  // With input rank 4, each output is the two first 3 dims of input, plus k.\n  INFER_OK(op, \"[1,?,21,?];[]\", \"[d0_0,d0_1,d0_2,20];[d0_0,d0_1,d0_2,20]\");\n\n  INFER_ERROR(\"Shape must be at least rank 1 but is rank 0\", op, \"[];[]\");\n  INFER_ERROR(\"input must have last dimension >= k = 20 but is 1\", op,\n              \"[1];[]\");\n  INFER_ERROR(\"input must have last dimension >= k = 20 but is 4\", op,\n              \"[1,2,3,4];[]\");\n  k_t = test::AsScalar<int32>(-1);\n  INFER_ERROR(\n      \"Dimension size, given by scalar input 1, must be non-negative but is -1\",\n      op, \"[1,2,3,4];[]\");\n}\n\nTEST(NNOpsTest, NthElement_ShapeFn) {\n  ShapeInferenceTestOp op(\"NthElement\");\n  op.input_tensors.resize(2);\n\n  Tensor n_t;\n  op.input_tensors[1] = &n_t;\n  n_t = test::AsScalar<int32>(20);\n\n  INFER_OK(op, \"?;[]\", \"?\");\n  INFER_OK(op, \"[21];[]\", \"[]\");\n  INFER_OK(op, \"[2,?,?];[]\", \"[d0_0,d0_1]\");\n  INFER_OK(op, \"[?,3,?,21];[]\", \"[d0_0,d0_1,d0_2]\");\n\n  INFER_ERROR(\"Shape must be at least rank 1 but is rank 0\", op, \"[];[]\");\n  INFER_ERROR(\"Input must have last dimension > n = 20 but is 1\", op, \"[1];[]\");\n  INFER_ERROR(\"Input must have last dimension > n = 20 but is 20\", op,\n              \"[1,2,3,20];[]\");\n  n_t = test::AsScalar<int32>(-1);\n  INFER_ERROR(\n      \"Dimension size, given by scalar input 1, must be non-negative but is -1\",\n      op, \"[1,2,3,4];[]\");\n}\n\nTEST(NNOpsTest, BatchNormWithGlobalNormalization_ShapeFn) {\n  ShapeInferenceTestOp op(\"BatchNormWithGlobalNormalization\");\n\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n\n  // last dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,?,?,d3_0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,?,?,d4_0]\");\n  INFER_OK(op, \"[1,2,3,4];[4];[4];[4];[4]\",\n           \"[d0_0,d0_1,d0_2,d0_3|d1_0|d2_0|d3_0|d4_0]\");\n}\n\nTEST(NNOpsTest, QuantizedBatchNormWithGlobalNormalization_ShapeFn) {\n  // These are the same tests as BatchNormWithGlobalNormalization tests, but\n  // with extra scalar inputs and outputs for the mins and maxes.\n\n  ShapeInferenceTestOp op(\"QuantizedBatchNormWithGlobalNormalization\");\n\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op,\n              \"[1,2,3];?;?;?;?;?;?;?;?;?;?;?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op,\n              \"?;?;?;[1,2,3];?;?;?;?;?;?;?;?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op,\n              \"?;?;?;?;?;?;[1,2,3];?;?;?;?;?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op,\n              \"?;?;?;?;?;?;?;?;?;[1,2,3];?;?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op,\n              \"?;?;?;?;?;?;?;?;?;?;?;?;[1,2,3];?;?\");\n\n  // last dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;[];[];?;[];[];?;[];[];?;[];[];?;[];[]\", \"[?,?,?,?];[];[]\");\n  INFER_OK(op, \"?;[];[];[1];[];[];?;[];[];?;[];[];?;[];[]\",\n           \"[?,?,?,d3_0];[];[]\");\n  INFER_OK(op, \"?;[];[];?;[];[];[1];[];[];?;[];[];?;[];[]\",\n           \"[?,?,?,d6_0];[];[]\");\n  INFER_OK(op, \"?;[];[];?;[];[];?;[];[];[1];[];[];?;[];[]\",\n           \"[?,?,?,d9_0];[];[]\");\n  INFER_OK(op, \"?;[];[];?;[];[];?;[];[];?;[];[];[1];[];[]\",\n           \"[?,?,?,d12_0];[];[]\");\n  INFER_OK(op, \"[1,2,3,4];[];[];[4];[];[];[4];[];[];[4];[];[];[4];[];[]\",\n           \"[d0_0,d0_1,d0_2,d0_3|d3_0|d6_0|d9_0|d12_0];[];[]\");\n}\n\nTEST(NNOpsTest, BatchNormWithGlobalNormalizationGrad_ShapeFn) {\n  ShapeInferenceTestOp op(\"BatchNormWithGlobalNormalizationGrad\");\n\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shapes must be equal rank, but are 4 and 3\", op,\n              \"?;?;?;?;[1,2,3]\");\n\n  // The first output comes from the first and last inputs merged together.\n  // Other inputs are merged with the last dim of that merge result, and that\n  // merged vector dim is the last 4 outputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,?,?,d3_0];[d3_0];[d3_0];[d3_0];[d3_0]\");\n  INFER_OK(op, \"[1,?,3,?];[?];[?];[?];[?,2,?,4]\",\n           \"[d0_0,d4_1,d0_2,d4_3];[d4_3];[d4_3];[d4_3];[d4_3]\");\n}\n\nTEST(NNOpsTest, FusedBatchNorm_ShapeFn) {\n  ShapeInferenceTestOp op(\"FusedBatchNorm\");\n\n  auto set_op = [&op](bool is_training, float exponential_avg_factor,\n                      string data_format) {\n    TF_ASSERT_OK(NodeDefBuilder(\"test\", \"FusedBatchNorm\")\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Attr(\"data_format\", data_format)\n                     .Attr(\"is_training\", is_training)\n                     .Attr(\"exponential_avg_factor\", exponential_avg_factor)\n                     .Finalize(&op.node_def));\n  };\n\n  set_op(true, 1.0, \"NHWC\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"[1,2,3,4];[4];[4];?;?\",\n           \"[d0_0,d0_1,d0_2,d0_3|d1_0|d2_0];\"\n           \"[d0_3|d1_0|d2_0];[d0_3|d1_0|d2_0];\"\n           \"[d0_3|d1_0|d2_0];[d0_3|d1_0|d2_0]\");\n\n  set_op(true, 0.5, \"NHWC\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"[1,2,3,4];[4];[4];?;?\",\n           \"[d0_0,d0_1,d0_2,d0_3|d1_0|d2_0];\"\n           \"[d0_3|d1_0|d2_0];[d0_3|d1_0|d2_0];\"\n           \"[d0_3|d1_0|d2_0];[d0_3|d1_0|d2_0]\");\n\n  set_op(true, 1.0, \"NCHW\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,d1_0,?,?];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,d2_0,?,?];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"[1,4,2,3];[4];[4];?;?\",\n           \"[d0_0,d0_1|d1_0|d2_0,d0_2,d0_3];\"\n           \"[d0_1|d1_0|d2_0];[d0_1|d1_0|d2_0];\"\n           \"[d0_1|d1_0|d2_0];[d0_1|d1_0|d2_0]\");\n\n  set_op(false, 1.0, \"NHWC\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,?,?,d1_0];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,?,?,d3_0];[d3_0];[d3_0];[d3_0];[d3_0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,?,?,d4_0];[d4_0];[d4_0];[d4_0];[d4_0]\");\n  INFER_OK(op, \"[1,2,3,4];[4];[4];[4];[4]\",\n           \"[d0_0,d0_1,d0_2,d0_3|d1_0|d2_0|d3_0|d4_0];\"\n           \"[d0_3|d1_0|d2_0|d3_0|d4_0];[d0_3|d1_0|d2_0|d3_0|d4_0];\"\n           \"[d0_3|d1_0|d2_0|d3_0|d4_0];[d0_3|d1_0|d2_0|d3_0|d4_0]\");\n\n  set_op(false, 1.0, \"NCHW\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[?];[?]\");\n  INFER_OK(op, \"?;[1];?;?;?\", \"[?,d1_0,?,?];[d1_0];[d1_0];[d1_0];[d1_0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,d2_0,?,?];[d2_0];[d2_0];[d2_0];[d2_0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,d3_0,?,?];[d3_0];[d3_0];[d3_0];[d3_0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,d4_0,?,?];[d4_0];[d4_0];[d4_0];[d4_0]\");\n  INFER_OK(op, \"[1,4,2,3];[4];[4];[4];[4]\",\n           \"[d0_0,d0_1|d1_0|d2_0|d3_0|d4_0,d0_2,d0_3];\"\n           \"[d0_1|d1_0|d2_0|d3_0|d4_0];[d0_1|d1_0|d2_0|d3_0|d4_0];\"\n           \"[d0_1|d1_0|d2_0|d3_0|d4_0];[d0_1|d1_0|d2_0|d3_0|d4_0]\");\n}\n\nTEST(NNOpsTest, FusedBatchNormGrad_ShapeFn) {\n  ShapeInferenceTestOp op(\"FusedBatchNormGrad\");\n  auto set_op = [&op](string data_format) {\n    TF_ASSERT_OK(NodeDefBuilder(\"test\", \"FusedBatchNormGrad\")\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Input(FakeInput(DT_FLOAT))\n                     .Attr(\"data_format\", data_format)\n                     .Finalize(&op.node_def));\n  };\n\n  set_op(\"NCHW\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[0];[0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,d2_0,?,?];[d2_0];[d2_0];[0];[0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,d3_0,?,?];[d3_0];[d3_0];[0];[0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,d4_0,?,?];[d4_0];[d4_0];[0];[0]\");\n  INFER_OK(op, \"[1,4,2,3];[1,4,2,3];[4];[4];[4]\",\n           \"[d0_0,d0_1|d2_0|d3_0|d4_0,d0_2,d0_3];\"\n           \"[d0_1|d2_0|d3_0|d4_0];[d0_1|d2_0|d3_0|d4_0];[0];[0]\");\n\n  set_op(\"NHWC\");\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?;?;?\");\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"?;[1,2,3];?;?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;[1,2,3];?;?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 3\", op, \"?;?;?;?;[1,2,3]\");\n  // Channel dim of first input is merged with the single dim in other 4 inputs.\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?];[?];[?];[0];[0]\");\n  INFER_OK(op, \"?;?;[1];?;?\", \"[?,?,?,d2_0];[d2_0];[d2_0];[0];[0]\");\n  INFER_OK(op, \"?;?;?;[1];?\", \"[?,?,?,d3_0];[d3_0];[d3_0];[0];[0]\");\n  INFER_OK(op, \"?;?;?;?;[1]\", \"[?,?,?,d4_0];[d4_0];[d4_0];[0];[0]\");\n  INFER_OK(op, \"[1,2,3,4];[1,2,3,4];[4];[4];[4]\",\n           \"[d0_0,d0_1,d0_2,d0_3|d2_0|d3_0|d4_0];\"\n           \"[d0_3|d2_0|d3_0|d4_0];[d0_3|d2_0|d3_0|d4_0];[0];[0]\");\n}\n\nTEST(NNOpsTest, Conv2DBackpropInput_ShapeFn) {\n  ShapeInferenceTestOp op(\"Conv2DBackpropInput\");\n\n  // Test rank error.\n  INFER_ERROR(\"input_sizes to contain 4 values or 2 values\", op,\n              \"[3];[?,?,?,?];[?,?,?,?]\");\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op,\n              \"[4];[?,?,?,?];[?,?,?]\");\n\n  // When input_sizes is a 4D shape and the convolution is grouped, the channel\n  // size of the input grad doesn't always equal the input channel size of the\n  // filter. So, when input_sizes is a 4D shape, the channel size of the input\n  // grad is determined by the content of input_sizes.\n  INFER_OK(op, \"[4];[?,?,2,?];[1,?,?,?]\", \"[d2_0,?,?,?]\");\n  // When input_sizes is a 2D shape, the channel size of the input grad always\n  // matches the filter shape.\n  INFER_OK(op, \"[2];[?,?,2,?];[1,?,?,?]\", \"[d2_0,?,?,d1_2]\");\n}\n\nTEST(NNOpsTest, Conv3DBackpropInput_ShapeFn) {\n  ShapeInferenceTestOp op(\"Conv3DBackpropInput\");\n\n  // Test rank error.\n  INFER_ERROR(\"Shape must be rank 5 but is rank 3\", op, \"[1,2,3];?;?\");\n\n  // input[1] is transferred to output after asserting its rank.\n  INFER_OK(op, \"?;?;?\", \"[?,?,?,?,?]\");\n  INFER_OK(op, \"[?,?,?,?,?];?;?\", \"in0\");\n  INFER_OK(op, \"[?,2,?,4,?];?;?\", \"in0\");\n}\n\nTEST(NNOpsTest, Conv3DBackpropFilter_ShapeFn) {\n  ShapeInferenceTestOp op(\"Conv3DBackpropFilter\");\n\n  // Test rank error.\n  INFER_ERROR(\"Shape must be rank 5 but is rank 3\", op, \"?;[1,2,3];?\");\n\n  // input[1] is transferred to output after asserting its rank.\n  INFER_OK(op, \"?;?;?\", \"[?,?,?,?,?]\");\n  INFER_OK(op, \"?;[?,?,?,?,?];?\", \"in1\");\n  INFER_OK(op, \"?;[?,2,?,4,?];?\", \"in1\");\n}\n\nTEST(NNOpsTest, MaxPool3DGrad_ShapeFn) {\n  ShapeInferenceTestOp op(\"MaxPool3DGrad\");\n\n  // Test rank error.\n  INFER_ERROR(\"Shape must be rank 5 but is rank 3\", op, \"[1,2,3];?;?\");\n\n  // input[0] is transferred to output after asserting its rank.\n  INFER_OK(op, \"?;?;?\", \"[?,?,?,?,?]\");\n  INFER_OK(op, \"[?,?,?,?,?];?;?\", \"in0\");\n  INFER_OK(op, \"[?,2,?,4,?];?;?\", \"in0\");\n}\n\nTEST(NNOpsTest, LRNGrad_ShapeFn) {\n  ShapeInferenceTestOp op(\"LRNGrad\");\n\n  // LRN Grad is a merge of all three inputs, of rank 4.\n  INFER_OK(op, \"[1,?,?,4];[?,2,?,?];[?,?,3,?]\", \"[d0_0,d1_1,d2_2,d0_3]\");\n\n  // Test rank errors.\n  INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?\");\n  INFER_ERROR(\"Shapes must be equal rank, but are 4 and 3\", op, \"?;[1,2,3];?\");\n  INFER_ERROR(\"Shapes must be equal rank, but are 4 and 3\", op, \"?;?;[1,2,3]\");\n}\n\nTEST(NNOpsTest, MaxPoolGrad_ShapeFn) {\n  for (const char* op_name : {\"MaxPoolGrad\", \"MaxPoolGradWithArgmax\"}) {\n    ShapeInferenceTestOp op(op_name);\n\n    // Test rank error.\n    INFER_ERROR(\"Shape must be rank 4 but is rank 3\", op, \"[1,2,3];?;?\");\n\n    // input[0] is transferred to output after asserting its rank.\n    INFER_OK(op, \"?;?;?\", \"[?,?,?,?]\");\n    INFER_OK(op, \"[?,?,?,?];?;?\", \"in0\");\n    INFER_OK(op, \"[?,2,?,4];?;?\", \"in0\");\n  }\n}\n\nTEST(NNOpsTest, Dilation2DBackpropInput_ShapeFn) {\n  ShapeInferenceTestOp op(\"Dilation2DBackpropInput\");\n\n  // input[0] is transferred to output.\n  INFER_OK(op, \"?;?;?\", \"in0\");\n  INFER_OK(op, \"?;[?,?,?,?,?];?\", \"in0\");\n  INFER_OK(op, \"?;[?,2,?,4,?];?\", \"in0\");\n}\n\nTEST(NNOpsTest, Dilation2DBackpropFilter_ShapeFn) {\n  ShapeInferenceTestOp op(\"Dilation2DBackpropFilter\");\n\n  // input[1] is transferred to output.\n  INFER_OK(op, \"?;?;?\", \"in1\");\n  INFER_OK(op, \"?;[?,?,?,?,?];?\", \"in1\");\n  INFER_OK(op, \"?;[?,2,?,4,?];?\", \"in1\");\n}\n\nTEST(NNOpsTest, MergeBothInputs_ShapeFn) {\n  for (const char* op_name : {\"ReluGrad\", \"Relu6Grad\", \"EluGrad\", \"SeluGrad\",\n                              \"SoftplusGrad\", \"SoftsignGrad\"}) {\n    ShapeInferenceTestOp op(op_name);\n\n    INFER_OK(op, \"?;?\", \"in0|in1\");\n    INFER_OK(op, \"?;[1,?,3]\", \"in1\");\n    INFER_OK(op, \"[1,?,3];?\", \"in0\");\n    INFER_OK(op, \"[1,?];[?,2]\", \"[d0_0,d1_1]\");\n    INFER_ERROR(\"Dimension 1 in both shapes must be equal, but are 3 and 2\", op,\n                \"[1,3];[?,2]\");\n  }\n}\n\nTEST(NNOpsTest, SoftmaxCrossEntropyWithLogits_ShapeFn) {\n  ShapeInferenceTestOp op(\"SoftmaxCrossEntropyWithLogits\");\n\n  // Inputs are [batch_size,N] and [batch_size,N], and outputs are [batch_size]\n  // and\n  // [batch_size,N].\n  INFER_OK(op, \"?;?\", \"[?];[?,?]\");\n  INFER_OK(op, \"[?,?];[?,?]\", \"[d0_0|d1_0];in0|in1\");\n  INFER_OK(op, \"[1,2];[?,2]\", \"[d0_0];in0\");\n  INFER_OK(op, \"[1,?];[?,2]\", \"[d0_0];[d0_0,d0_1|d1_1]\");\n  INFER_OK(op, \"[?,2];[1,2]\", \"[d1_0];in1\");\n\n  INFER_ERROR(\"Shape must be broadcasted with rank 2\", op, \"[1,2,3];?\");\n  INFER_ERROR(\"Shape must be broadcasted with rank 2\", op, \"?;[1,2,3]\");\n\n  // Broadcast example\n  // [1,4] and [2,4] are broadcasted to [2,4]\n  INFER_OK(op, \"[1,4];[2,4]\", \"[d1_0];[d1_0,d0_1|d1_1]\");\n  // [2,4] and [2,1] are broadcasted to [2,4]\n  INFER_OK(op, \"[2,4];[2,1]\", \"[d0_0];[d0_0|d1_0,d0_1]\");\n  // [1,?] and [2,4] are broadcasted to [2,4]\n  INFER_OK(op, \"[1,?];[2,4]\", \"[d1_0];[d1_0,d0_1|d1_1]\");\n  // [2,4] and [?,1] are broadcasted to [2,4]\n  INFER_OK(op, \"[2,4];[?,1]\", \"[d0_0];[d0_0|d1_0,d0_1]\");\n}\n\nTEST(NNOpsTest, SparseSoftmaxCrossEntropyWithLogits_ShapeFn) {\n  ShapeInferenceTestOp op(\"SparseSoftmaxCrossEntropyWithLogits\");\n\n  // Inputs are [batch_size,N] and [batch_size], and outputs are [batch_size]\n  // and [batch_size,N].\n  INFER_OK(op, \"?;?\", \"[?];[?,?]\");\n  INFER_OK(op, \"[?,?];[?]\", \"[d0_0|d1_0];[d0_0|d1_0,d0_1]\");\n  INFER_OK(op, \"[1,2];[1]\", \"[d0_0|d1_0];[d0_0|d1_0,d0_1]\");\n  INFER_OK(op, \"[?,2];[1]\", \"[d1_0];[d1_0,d0_1]\");\n\n  INFER_ERROR(\"Dimensions must be equal, but are 1 and 2\", op, \"[1,?];[2]\");\n  INFER_ERROR(\"Shape must be rank 2 but is rank 3\", op, \"[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 2\", op, \"?;[1,2]\");\n}\n\nTEST(NNOpsTest, InTopK_ShapeFn) {\n  ShapeInferenceTestOp op(\"InTopK\");\n\n  // Inputs are [batch_size,N] and [batch_size], and output is [batch_size].\n  INFER_OK(op, \"?;?\", \"[?]\");\n  INFER_OK(op, \"[?,?];[?]\", \"[d0_0|d1_0]\");\n  INFER_OK(op, \"[1,2];[1]\", \"[d0_0|d1_0]\");\n  INFER_OK(op, \"[?,2];[1]\", \"[d1_0]\");\n\n  INFER_ERROR(\"Dimensions must be equal, but are 1 and 2\", op, \"[1,?];[2]\");\n  INFER_ERROR(\"Shape must be rank 2 but is rank 3\", op, \"[1,2,3];?\");\n  INFER_ERROR(\"Shape must be rank 1 but is rank 2\", op, \"?;[1,2]\");\n}\n\nTEST(NNOpsTest, Dilation2DShapeTest) {\n  ShapeInferenceTestOp op(\"Dilation2D\");\n  auto set_op = [&op](const std::vector<int32>& strides,\n                      const std::vector<int32>& rates, const string& padding) {\n    TF_ASSERT_OK(NodeDefBuilder(\"test\", \"Dilation2D\")\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Input(\"filter\", 0, DT_FLOAT)\n                     .Attr(\"strides\", strides)\n                     .Attr(\"rates\", rates)\n                     .Attr(\"padding\", padding)\n                     .Finalize(&op.node_def));\n  };\n\n  // rate rows and cols is 1, so filter_rows and cols are unchanged.\n  // We have a 1x1 filter so the output is still 2x2.\n  set_op({1, 1, 1, 1}, {1, 1, 1, 1}, \"VALID\");\n  INFER_OK(op, \"[1,2,2,2];[1,1,2]\", \"[d0_0,2,2,d1_2]\");\n\n  // rate rows and cols is 2, so filter_rows and cols are changed to\n  // be 2 + (2 - 1) = 3.  7x7 input with 3x3 filter and 1x1 stride\n  // gives a 5x5 output.\n  set_op({1, 1, 1, 1}, {1, 2, 2, 1}, \"VALID\");\n  INFER_OK(op, \"[1,7,7,2];[2,2,2]\", \"[d0_0,5,5,d1_2]\");\n}\n\nTEST(NNOpsTest, FractionalPool_ShapeFn) {\n  for (const char* op_name : {\"FractionalAvgPool\", \"FractionalMaxPool\"}) {\n    ShapeInferenceTestOp op(op_name);\n    auto set_op = [&op, op_name](const std::vector<float>& pooling_ratio) {\n      TF_ASSERT_OK(NodeDefBuilder(\"test\", op_name)\n                       .Input(\"input\", 0, DT_FLOAT)\n                       .Attr(\"pooling_ratio\", pooling_ratio)\n                       .Finalize(&op.node_def));\n    };\n\n    // pooling_ratio must >= 1.0\n    set_op(std::vector<float>{2.0f, 1, 1.5f, 4.0f});\n\n    // Rank check.\n    INFER_ERROR(\"must be rank 4\", op, \"[?,?,?]\");\n\n    // Unknown inputs.\n    INFER_OK(op, \"?\", \"[?,?,?,?];[?];[?]\");\n    INFER_OK(op, \"[?,?,?,?]\", \"[?,?,?,?];[?];[?]\");\n\n    INFER_OK(op, \"[10,20,30,40]\", \"[5,20,20,10];[20];[20]\");\n    INFER_OK(op, \"[?,20,30,40]\", \"[?,20,20,10];[20];[20]\");\n    INFER_OK(op, \"[10,?,30,40]\", \"[5,?,20,10];[?];[20]\");\n    INFER_OK(op, \"[10,20,?,40]\", \"[5,20,?,10];[20];[?]\");\n    INFER_OK(op, \"[10,20,30,?]\", \"[5,20,20,?];[20];[20]\");\n\n    // Wrong number of values for pooling_ratio.\n    set_op(std::vector<float>{.5, 1.0, 1.5});\n    INFER_ERROR(\"pooling_ratio field\", op, \"?\");\n    set_op(std::vector<float>{1, 2, 3, 4, 5});\n    INFER_ERROR(\"pooling_ratio field\", op, \"?\");\n\n    // Check dim size >= 0.\n    set_op(std::vector<float>{-1, 2, 3, 4});\n    INFER_ERROR(\"is negative\", op, \"[1,2,3,4]\");\n  }\n}\n\nTEST(NNOpsTest, FractionalMaxPoolGrad) {\n  ShapeInferenceTestOp op(\"FractionalMaxPoolGrad\");\n\n  // Note that the shape fn only uses input[0] for computation.\n  INFER_ERROR(\"must be rank 4\", op, \"[?,?,?];?;?;?;?\");\n  INFER_OK(op, \"?;?;?;?;?\", \"[?,?,?,?]\");\n  INFER_OK(op, \"[?,?,3,4];?;?;?;?\", \"in0\");\n}\n\nTEST(NNOpsTest, FractionalAvgPoolGrad) {\n  ShapeInferenceTestOp op(\"FractionalAvgPoolGrad\");\n  op.input_tensors.resize(1);\n\n  // With no input shape tensor, returns unknown of rank 4.\n  INFER_OK(op, \"?;?;?;?\", \"[?,?,?,?]\");\n\n  // When input tensor is known, its values determine output shape.\n  std::vector<int32> shape{1, 2, 3, 4};\n  Tensor shape_t = test::AsTensor<int32>(shape);\n  op.input_tensors[0] = &shape_t;\n  INFER_OK(op, \"[5];?;?;?\", \"[1,2,3,4]\");\n}\n\n}  // end namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fractional average pool operation.\"\"\"\n\nimport math\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass FractionalAvgTest(test.TestCase):\n\n  # Random number generate with seed.\n  _PRNG = np.random.RandomState(341261000)\n  _SEED = 341261001\n\n  def _AvgPoolAlongRows(self, input_matrix, row_seq, overlapping):\n    \"\"\"Perform average pool along row of a 2-D matrix based on row_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      row_seq: Cumulative pooling sequence along row.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = len(row_seq)-1\n        * num_cols = input_matrix.num_cols.\n    \"\"\"\n    output_image = np.zeros(input_matrix.shape[1])\n    row_max = row_seq[-1]\n    for i in range(row_seq.shape[0] - 1):\n      row_start = row_seq[i]\n      row_end = row_seq[i + 1] + 1 if overlapping else row_seq[i + 1]\n      row_end = min(row_end, row_max)\n      output_image = np.vstack((output_image, np.mean(\n          input_matrix[row_start:row_end, :], axis=0)))  # axis 0 is along row\n    # remove the sentinel row\n    return output_image[1:, :]\n\n  def _AvgPoolAlongCols(self, input_matrix, col_seq, overlapping):\n    \"\"\"Perform average pool along column of a 2-D matrix based on col_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = input_matrix.num_rows\n        * num_cols = len(col_seq)-1.\n    \"\"\"\n    input_matrix = input_matrix.transpose()\n    output_matrix = self._AvgPoolAlongRows(input_matrix, col_seq, overlapping)\n    return output_matrix.transpose()\n\n  def _GetExpectedFractionalAvgPoolResult(self, input_tensor, row_seq, col_seq,\n                                          overlapping):\n    \"\"\"Get expected fractional average pooling result.\n\n    row_seq and col_seq together defines the fractional pooling region.\n\n    Args:\n      input_tensor: Original input tensor, assuming it is a 4-D tensor, with\n        dimension as [batch, height/row, width/column, channels/depth].\n      row_seq: Cumulative pooling sequence along row.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Use overlapping when doing pooling.\n\n    Returns:\n      A 4-D tensor that is the result of average pooling on input_tensor based\n        on pooling region defined by row_seq and col_seq, conditioned on whether\n        or not overlapping is used.\n    \"\"\"\n    input_shape = input_tensor.shape\n    output_shape = (input_shape[0], len(row_seq) - 1, len(col_seq) - 1,\n                    input_shape[3])\n    output_tensor = np.zeros(shape=output_shape, dtype=input_tensor.dtype)\n    for batch in range(input_shape[0]):\n      for channel in range(input_shape[3]):\n        two_dim_slice = input_tensor[batch, :, :, channel]\n        tmp = self._AvgPoolAlongRows(two_dim_slice, row_seq, overlapping)\n        output_tensor[batch, :, :, channel] = self._AvgPoolAlongCols(\n            tmp, col_seq, overlapping)\n\n    return output_tensor\n\n  def _ValidateFractionalAvgPoolResult(self, input_tensor, pooling_ratio,\n                                       pseudo_random, overlapping):\n    \"\"\"Validate FractionalAvgPool's result against expected.\n\n    Expected result is computed given input_tensor, and pooling region defined\n    by row_seq and col_seq.\n\n    Args:\n      input_tensor: A tensor or numpy ndarray.\n      pooling_ratio: A list or tuple of length 4, first and last element be 1.\n      pseudo_random: Use pseudo random method to generate pooling sequence.\n      overlapping: Use overlapping when pooling.\n\n    Returns:\n      None\n    \"\"\"\n    with self.cached_session() as sess:\n      p, r, c = nn_ops.fractional_avg_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      actual, row_seq, col_seq = self.evaluate([p, r, c])\n      expected = self._GetExpectedFractionalAvgPoolResult(input_tensor, row_seq,\n                                                          col_seq, overlapping)\n      self.assertShapeEqual(expected, p)\n      self.assertAllClose(expected, actual)\n\n  def _testVisually(self):\n    \"\"\"Manual test by printing out intermediate result of a small random tensor.\n\n    Since _GetExpectedFractionalAvgPoolResult is 'automated', it feels safer to\n    have a test case that you can see what's happening.\n    This test will generate a small, random, int 2D matrix, and feed it to\n    FractionalAvgPool and _GetExpectedFractionalAvgPoolResult.\n    \"\"\"\n    num_rows = 6\n    num_cols = 6\n    tensor_shape = (1, num_rows, num_cols, 1)\n    pseudo_random = False\n    for overlapping in True, False:\n      print(\"-\" * 70)\n      print(\"Testing FractionalAvgPool with overlapping = {}\".format(\n          overlapping))\n      rand_mat = self._PRNG.randint(10, size=tensor_shape)\n      pooling_ratio = [1, math.sqrt(2), math.sqrt(2), 1]\n      with self.cached_session() as sess:\n        p, r, c = nn_ops.fractional_avg_pool_v2(\n            rand_mat.astype(np.float32),\n            pooling_ratio,\n            pseudo_random,\n            overlapping,\n            seed=self._SEED)\n        tensor_output, row_seq, col_seq = self.evaluate([p, r, c])\n        expected_result = self._GetExpectedFractionalAvgPoolResult(\n            rand_mat.astype(np.float32), row_seq, col_seq, overlapping)\n        print(\"row sequence:\")\n        print(row_seq)\n        print(\"column sequence:\")\n        print(col_seq)\n\n        print(\"Input:\")\n        # Print input with pooling region marked.\n        for i in range(num_rows):\n          row_to_print = []\n          for j in range(num_cols):\n            if j in col_seq:\n              row_to_print.append(\"|\")\n            row_to_print.append(str(rand_mat[0, i, j, 0]))\n          row_to_print.append(\"|\")\n          if i in row_seq:\n            print(\"-\" * 2 * len(row_to_print))\n          print(\" \".join(row_to_print))\n        print(\"-\" * 2 * len(row_to_print))\n\n        print(\"Output from FractionalAvgPool:\")\n        print(tensor_output[0, :, :, 0])\n        print(\"Expected result:\")\n        print(expected_result[0, :, :, 0])\n\n  def testAllInputOptions(self):\n    \"\"\"Try all possible input options for fractional_avg_pool.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalAvgPoolResult(\n            rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n            overlapping)\n\n  def testIntegerTensorInput(self):\n    \"\"\"Test FractionalAvgPool works fine when input tensor is integer type.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    tensor_shape = (1, 6, 6, 1)\n    # pyformat: disable\n    mat = np.array([\n        [2, 6, 4, 1, 3, 6],\n        [8, 9, 1, 6, 6, 8],\n        [3, 9, 8, 2, 5, 6],\n        [2, 7, 9, 5, 4, 5],\n        [8, 5, 0, 5, 7, 4],\n        [4, 4, 5, 9, 7, 2]\n    ])\n    # pyformat: enable\n    self._ValidateFractionalAvgPoolResult(mat.reshape(tensor_shape),\n                                          [1, math.sqrt(3), math.sqrt(2), 1],\n                                          pseudo_random, overlapping)\n\n  def testDifferentTensorShapes(self):\n    \"\"\"Test different shapes of input tensor.\n\n    Mainly test different combinations of num_rows and num_cols.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    for num_batches in [1, 3]:\n      for num_channels in [1, 3]:\n        for num_rows in [10, 20, 50]:\n          for num_cols in [10, 20, 50]:\n            tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n            # random tensor with value in [-500.0, 500.0)\n            rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n            self._ValidateFractionalAvgPoolResult(\n                rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n                overlapping)\n\n  def testLargePoolingRatio(self):\n    \"\"\"Test when pooling ratio is not within [1, 2).\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    for row_ratio in [math.sqrt(11), math.sqrt(37)]:\n      for col_ratio in [math.sqrt(11), math.sqrt(27)]:\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalAvgPoolResult(rand_mat,\n                                              [1, row_ratio, col_ratio, 1],\n                                              pseudo_random, overlapping)\n\n  def testDivisiblePoolingRatio(self):\n    \"\"\"Test when num of rows/cols can evenly divide pooling ratio.\n\n    This is a case regular average pooling can handle. Should be handled by\n    fractional pooling as well.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    # random tensor with value in [-500.0, 500.0)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    self._ValidateFractionalAvgPoolResult(rand_mat, [1, 2, 2, 1], pseudo_random,\n                                          overlapping)\n\n  @test_util.run_deprecated_v1\n  def testDifferentInputTensorShape(self):\n    \"\"\"Runs the operation in one session with different input tensor shapes.\"\"\"\n    with self.cached_session() as sess:\n      input_holder = array_ops.placeholder(dtypes.float32,\n                                           [None, None, None, 3])\n      pooling_ratio = [1, 1.5, 1.5, 1]\n      pseudo_random = False\n      overlapping = False\n      p, r, c = nn_ops.fractional_avg_pool_v2(\n          input_holder,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      # First run.\n      input_a = np.zeros([3, 32, 32, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_a})\n      expected = self._GetExpectedFractionalAvgPoolResult(\n          input_a, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n      # Second run.\n      input_b = np.zeros([4, 60, 60, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_b})\n      expected = self._GetExpectedFractionalAvgPoolResult(\n          input_b, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n\n  def testNegativeSeqValuesForGradOp(self):\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"Row sequence tensor values must not be negative.*\"):\n      y = nn_ops.gen_nn_ops.fractional_avg_pool_grad(\n          orig_input_tensor_shape=[2, 2, 2, 2],\n          out_backprop=[[[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11,\n                                                                      12]]]],\n          row_pooling_sequence=[-10, 1, 2, 3],\n          col_pooling_sequence=[1, 2, 3, 4],\n          overlapping=True)\n\n      self.evaluate(y)\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Column sequence tensor values must not be negative.*\"):\n        z = nn_ops.gen_nn_ops.fractional_avg_pool_grad(\n            orig_input_tensor_shape=[2, 2, 2, 2],\n            out_backprop=[[[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11,\n                                                                        12]]]],\n            row_pooling_sequence=[10, 1, 2, 3],\n            col_pooling_sequence=[1, 2, -3, 4],\n            overlapping=True)\n\n        self.evaluate(z)\n\n  def testPoolingRatioHasMoreDimThanInput(self):\n    with self.cached_session() as _:\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Pooling ratio is higher than input dimension size for dimension 1.*\"\n      ):\n        result = nn_ops.gen_nn_ops.fractional_avg_pool(\n            value=constant_op.constant(\n                value=[[[[1, 4, 2, 3]]]], dtype=dtypes.int64),\n            pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n            pseudo_random=False,\n            overlapping=False,\n            deterministic=False,\n            seed=0,\n            seed2=0,\n            name=None)\n        self.evaluate(result)\n\n  def testPoolingRatioValueOutOfRange(self):\n    with self.cached_session() as _:\n      # Whether turn on `TF2_BEHAVIOR` generates different error messages\n      with self.assertRaisesRegex(\n          (errors.InvalidArgumentError, ValueError),\n          r\"(pooling_ratio cannot be smaller than 1, got: .*)|(is negative)\"):\n        result = nn_ops.gen_nn_ops.fractional_avg_pool(\n            value=np.zeros([3, 30, 30, 3]),\n            pooling_ratio=[1, -1, 3, 1],\n            pseudo_random=False,\n            overlapping=False,\n            deterministic=False,\n            seed=0,\n            seed2=0,\n        )\n        self.evaluate(result)\n\n\nclass FractionalAvgPoolGradTest(test.TestCase):\n  \"\"\"Tests for FractionalAvgPoolGrad.\n\n  Two types of tests for FractionalAvgPoolGrad.\n  1) Test fractional_avg_pool_grad() directly.\n    This type of test relies on gen_nn_ops.avg_pool_grad() returns the\n  correct result. For example:\n    * input_tensor_shape = (1, 10, 10, 1)\n    * window_size = (1, 2, 2, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not really important, since 10/2 is divisible\n  avg pooling should generate the same result as fractional avg pooling with:\n    * row_sequence = [0, 2, 4, 6, 8, 10]\n    * col_sequence = [0, 2, 4, 6, 8, 10]\n    * overlapping = False\n  This also means their gradients in such case will be the same.\n\n  Similarly, when\n    * input_tensor_shape = (1, 7, 7, 1)\n    * window_size = (1, 3, 3, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not important\n  avg pooling should generate the same result as fractional avg pooling with:\n    * row_sequence = [0, 2, 4, 7]\n    * col_sequence = [0, 2, 4, 7]\n    * overlapping = True\n  2) Test through compute_gradient_error()\n  \"\"\"\n  _PRNG = np.random.RandomState(341261004)\n  _SEED = 341261005\n\n  def _GenerateRandomInputTensor(self, shape):\n    num_elements = 1\n    for dim_size in shape:\n      num_elements *= dim_size\n    x = self._PRNG.rand(num_elements) * 1000\n    return x.reshape(shape)\n\n  def testDirectNotUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = row_window_size * 5\n          num_cols = col_window_size * 7\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateRandomInputTensor(input_shape).astype(\n                      np.float32))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size, col_window_size, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.avg_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              num_elements = 1\n              for dim_size in output_data.shape:\n                num_elements *= dim_size\n              output_backprop = (self._PRNG.rand(num_elements) *\n                                 1000).reshape(output_data.shape)\n              input_backprop_tensor = gen_nn_ops.avg_pool_grad(\n                  input_tensor.get_shape(), output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows + 1, row_window_size))\n              col_seq = list(range(0, num_cols + 1, col_window_size))\n              fap_input_backprop_tensor = gen_nn_ops.fractional_avg_pool_grad(\n                  input_tensor.get_shape(),\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=False)\n              fap_input_backprop = self.evaluate(fap_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fap_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fap_input_backprop)\n\n  def testDirectUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = (row_window_size - 1) * 5 + 1\n          num_cols = (col_window_size - 1) * 7 + 1\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateRandomInputTensor(input_shape).astype(\n                      np.float32))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size - 1, col_window_size - 1, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.avg_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              num_elements = 1\n              for dim_size in output_data.shape:\n                num_elements *= dim_size\n              output_backprop = (self._PRNG.rand(num_elements) *\n                                 1000).reshape(output_data.shape)\n              input_backprop_tensor = gen_nn_ops.avg_pool_grad(\n                  input_tensor.get_shape(), output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows, row_window_size - 1))\n              col_seq = list(range(0, num_cols, col_window_size - 1))\n              row_seq[-1] += 1\n              col_seq[-1] += 1\n              fap_input_backprop_tensor = gen_nn_ops.fractional_avg_pool_grad(\n                  input_tensor.get_shape(),\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=True)\n              fap_input_backprop = self.evaluate(fap_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fap_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fap_input_backprop)\n\n  @test_util.run_deprecated_v1\n  def testAllInputOptionsThroughGradientError(self):\n    input_shape = (1, 7, 13, 1)\n    input_data = self._GenerateRandomInputTensor(input_shape)\n    pooling_ratio = [1, math.sqrt(2), math.sqrt(3), 1]\n\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        with self.cached_session() as _:\n          input_tensor = constant_op.constant(input_data, shape=input_shape)\n          output_tensor, unused_a, unused_b = nn_ops.fractional_avg_pool_v2(\n              input_tensor,\n              pooling_ratio,\n              pseudo_random=pseudo_random,\n              overlapping=overlapping,\n              seed=self._SEED)\n          output_data = self.evaluate(output_tensor)\n          output_shape = output_data.shape\n          # error_margin and delta setting is similar to avg_pool_grad.\n          error_margin = 1e-4\n          gradient_error = gradient_checker.compute_gradient_error(\n              input_tensor,\n              input_shape,\n              output_tensor,\n              output_shape,\n              x_init_value=input_data.reshape(input_shape),\n              delta=1e-2)\n          self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testDifferentTensorShapesThroughGradientError(self):\n    pseudo_random = True\n    overlapping = True\n    pooling_ratio = [1, math.sqrt(3), math.sqrt(2), 1]\n    for num_batches in [1, 2]:\n      for num_rows in [5, 13]:\n        for num_cols in [5, 11]:\n          for num_channels in [1, 3]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            input_data = self._GenerateRandomInputTensor(input_shape)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(input_data, shape=input_shape)\n              output_tensor, unused_a, unused_b = nn_ops.fractional_avg_pool_v2(\n                  input_tensor,\n                  pooling_ratio,\n                  pseudo_random=pseudo_random,\n                  overlapping=overlapping,\n                  seed=self._SEED)\n              output_data = self.evaluate(output_tensor)\n              output_shape = output_data.shape\n              # error_margin and delta setting is similar to avg_pool_grad.\n              error_margin = 1e-4\n              gradient_error = gradient_checker.compute_gradient_error(\n                  input_tensor,\n                  input_shape,\n                  output_tensor,\n                  output_shape,\n                  x_init_value=input_data.reshape(input_shape),\n                  delta=1e-2)\n              self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testLargePoolingRatioThroughGradientError(self):\n    input_shape = (1, 17, 23, 1)\n    input_data = self._GenerateRandomInputTensor(input_shape)\n    pooling_ratio = (1, math.sqrt(13), math.sqrt(7), 1)\n    output_shape = [int(a / b) for a, b in zip(input_shape, pooling_ratio)]\n    overlapping = True\n    pseudo_random = False\n\n    with self.cached_session() as _:\n      input_tensor = constant_op.constant(input_data, shape=input_shape)\n      output_tensor, unused_a, unused_b = nn_ops.fractional_avg_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random=pseudo_random,\n          overlapping=overlapping,\n          seed=self._SEED)\n      # error_margin and delta setting is similar to avg_pool_grad.\n      error_margin = 1e-4\n      gradient_error = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_shape,\n          output_tensor,\n          output_shape,\n          x_init_value=input_data.reshape(input_shape),\n          delta=1e-2)\n      self.assertLess(gradient_error, error_margin)\n\n  def testInvalidSeqRaiseErrorForFractionalAvgPoolGrad(self):\n    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n      with self.cached_session() as _:\n        overlapping = True\n        orig_input_tensor_shape = constant_op.constant(\n            -1879048192, shape=[4], dtype=dtypes.int64)\n        out_backprop = constant_op.constant([],\n                                            shape=[0, 0, 0, 0],\n                                            dtype=dtypes.float64)\n        row_pooling_sequence = constant_op.constant(\n            1, shape=[4], dtype=dtypes.int64)\n        col_pooling_sequence = constant_op.constant(\n            1, shape=[4], dtype=dtypes.int64)\n        t = gen_nn_ops.fractional_avg_pool_grad(\n            orig_input_tensor_shape=orig_input_tensor_shape,\n            out_backprop=out_backprop,\n            row_pooling_sequence=row_pooling_sequence,\n            col_pooling_sequence=col_pooling_sequence,\n            overlapping=overlapping)\n        self.evaluate(t)\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fractional max pool operation.\"\"\"\n\nimport math\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass FractionalMaxPoolTest(test.TestCase):\n\n  # Random number generate with seed.\n  _PRNG = np.random.RandomState(341261)\n  _SEED = 123456\n\n  def _MaxPoolAlongRows(self, input_matrix, row_seq, overlapping):\n    \"\"\"Perform max pool along row of a 2-D matrix based on row_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      row_seq: Cumulative pooling sequence along row.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = len(row_seq)-1\n        * num_cols = input_matrix.num_cols.\n    \"\"\"\n    output_image = np.zeros(input_matrix.shape[1])\n    row_max = row_seq[-1]\n    for i in range(row_seq.shape[0] - 1):\n      row_start = row_seq[i]\n      row_end = row_seq[i + 1] + 1 if overlapping else row_seq[i + 1]\n      row_end = min(row_end, row_max)\n      output_image = np.vstack((output_image, np.amax(\n          input_matrix[row_start:row_end, :], axis=0)))  # axis 0 is along row\n    # remove the sentinel row\n    return output_image[1:, :]\n\n  def _MaxPoolAlongCols(self, input_matrix, col_seq, overlapping):\n    \"\"\"Perform max pool along column of a 2-D matrix based on col_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = input_matrix.num_rows\n        * num_cols = len(col_seq)-1.\n    \"\"\"\n    input_matrix = input_matrix.transpose()\n    output_matrix = self._MaxPoolAlongRows(input_matrix, col_seq, overlapping)\n    return output_matrix.transpose()\n\n  def _GetExpectedFractionalMaxPoolResult(self, input_tensor, row_seq, col_seq,\n                                          overlapping):\n    \"\"\"Get expected fractional max pool result.\n\n    row_seq and col_seq together defines the fractional pooling region.\n\n    Args:\n      input_tensor: Original input tensor, assuming it is a 4-D tensor, with\n        dimension as [batch, height/row, width/column, channels/depth].\n      row_seq: Cumulative pooling sequence along row.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Use overlapping when doing pooling.\n\n    Returns:\n      A 4-D tensor that is the result of max pooling on input_tensor based on\n        pooling region defined by row_seq and col_seq, conditioned on whether or\n        not overlapping is used.\n    \"\"\"\n    input_shape = input_tensor.shape\n    output_shape = (input_shape[0], len(row_seq) - 1, len(col_seq) - 1,\n                    input_shape[3])\n    output_tensor = np.zeros(shape=output_shape, dtype=input_tensor.dtype)\n    for batch in range(input_shape[0]):\n      for channel in range(input_shape[3]):\n        two_dim_slice = input_tensor[batch, :, :, channel]\n        tmp = self._MaxPoolAlongRows(two_dim_slice, row_seq, overlapping)\n        output_tensor[batch, :, :, channel] = self._MaxPoolAlongCols(\n            tmp, col_seq, overlapping)\n\n    return output_tensor\n\n  def _ValidateFractionalMaxPoolResult(self, input_tensor, pooling_ratio,\n                                       pseudo_random, overlapping):\n    \"\"\"Validate FractionalMaxPool's result against expected.\n\n    Expected result is computed given input_tensor, and pooling region defined\n    by row_seq and col_seq.\n\n    Args:\n      input_tensor: A tensor or numpy ndarray.\n      pooling_ratio: A list or tuple of length 4, first and last element be 1.\n      pseudo_random: Use pseudo random method to generate pooling sequence.\n      overlapping: Use overlapping when pooling.\n\n    Returns:\n      None\n    \"\"\"\n    with self.cached_session():\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      actual, row_seq, col_seq = self.evaluate([p, r, c])\n      expected = self._GetExpectedFractionalMaxPoolResult(input_tensor, row_seq,\n                                                          col_seq, overlapping)\n      self.assertShapeEqual(expected, p)\n      self.assertAllClose(expected, actual)\n\n  def _testVisually(self):\n    \"\"\"Manual test by printing out intermediate result of a small random tensor.\n\n    Since _GetExpectedFractionalMaxPoolResult is 'automated', it feel safer to\n    have a test case that you can see what's happening.\n    This test will generate a small, random, int 2D matrix, and feed it to\n    FractionalMaxPool and _GetExpectedFractionalMaxPoolResult.\n    \"\"\"\n    num_rows = 6\n    num_cols = 6\n    tensor_shape = (1, num_rows, num_cols, 1)\n    pseudo_random = False\n    for overlapping in True, False:\n      print(\"-\" * 70)\n      print(\"Testing FractionalMaxPool with overlapping = {}\".format(\n          overlapping))\n      rand_mat = self._PRNG.randint(10, size=tensor_shape)\n      pooling_ratio = [1, math.sqrt(2), math.sqrt(2), 1]\n      with self.cached_session():\n        p, r, c = nn_ops.fractional_max_pool_v2(\n            rand_mat,\n            pooling_ratio,\n            pseudo_random,\n            overlapping,\n            seed=self._SEED)\n        tensor_output, row_seq, col_seq = self.evaluate([p, r, c])\n        expected_result = self._GetExpectedFractionalMaxPoolResult(rand_mat,\n                                                                   row_seq,\n                                                                   col_seq,\n                                                                   overlapping)\n        print(\"row sequence:\")\n        print(row_seq)\n        print(\"column sequence:\")\n        print(col_seq)\n\n        print(\"Input:\")\n        # Print input with pooling region marked.\n        for i in range(num_rows):\n          row_to_print = []\n          for j in range(num_cols):\n            if j in col_seq:\n              row_to_print.append(\"|\")\n            row_to_print.append(str(rand_mat[0, i, j, 0]))\n          row_to_print.append(\"|\")\n          if i in row_seq:\n            print(\"-\" * 2 * len(row_to_print))\n          print(\" \".join(row_to_print))\n        print(\"-\" * 2 * len(row_to_print))\n\n        print(\"Output from FractionalMaxPool:\")\n        print(tensor_output[0, :, :, 0])\n        print(\"Expected result:\")\n        print(expected_result[0, :, :, 0])\n\n  def testAllInputOptions(self):\n    \"\"\"Try all possible input options for fractional_max_pool.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalMaxPoolResult(\n            rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n            overlapping)\n\n  def testIntegerTensorInput(self):\n    \"\"\"Test it works fine when input tensor is integer type.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    pseudo_random = True\n    overlapping = True\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    rand_mat = self._PRNG.randint(1000, size=tensor_shape)\n    self._ValidateFractionalMaxPoolResult(rand_mat,\n                                          [1, math.sqrt(3), math.sqrt(2), 1],\n                                          pseudo_random, overlapping)\n\n  def testDifferentTensorShapes(self):\n    \"\"\"Test different shapes of input tensor.\n\n    Mainly test different combinations of num_rows and num_cols.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    for num_batches in [1, 3]:\n      for num_channels in [1, 3]:\n        for num_rows in [10, 20, 50]:\n          for num_cols in [10, 20, 50]:\n            tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n            # random tensor with value in [-500.0, 500.0)\n            rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n            self._ValidateFractionalMaxPoolResult(\n                rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n                overlapping)\n\n  def testLargePoolingRatio(self):\n    \"\"\"Test when pooling ratio is not within [1, 2).\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    for row_ratio in [math.sqrt(11), math.sqrt(37)]:\n      for col_ratio in [math.sqrt(11), math.sqrt(27)]:\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalMaxPoolResult(rand_mat,\n                                              [1, row_ratio, col_ratio, 1],\n                                              pseudo_random, overlapping)\n\n  def testDivisiblePoolingRatio(self):\n    \"\"\"Test when num of rows/cols can evenly divide pooling ratio.\n\n    This is a case regular max pooling can handle. Should be handled by\n    fractional pooling as well.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    # random tensor with value in [-500.0, 500.0)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    self._ValidateFractionalMaxPoolResult(rand_mat, [1, 2, 2, 1], pseudo_random,\n                                          overlapping)\n\n  @test_util.run_deprecated_v1\n  def testDifferentInputTensorShape(self):\n    \"\"\"Runs the operation in one session with different input tensor shapes.\"\"\"\n    with self.cached_session() as sess:\n      input_holder = array_ops.placeholder(dtypes.float32,\n                                           [None, None, None, 3])\n      pooling_ratio = [1, 1.5, 1.5, 1]\n      pseudo_random = False\n      overlapping = False\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_holder,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      # First run.\n      input_a = np.zeros([3, 32, 32, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_a})\n      expected = self._GetExpectedFractionalMaxPoolResult(\n          input_a, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n      # Second run.\n      input_b = np.zeros([4, 45, 45, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_b})\n      expected = self._GetExpectedFractionalMaxPoolResult(\n          input_b, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n\n  def testDeterminismExceptionThrowing(self):\n    tensor_shape = (5, 20, 20, 3)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    with test_util.deterministic_ops():\n      with self.assertRaisesRegex(\n          ValueError, \"requires a non-zero seed to be passed in when \"\n          \"determinism is enabled\"):\n        nn_ops.fractional_max_pool_v2(rand_mat, [1, 1.5, 1.5, 1])\n      nn_ops.fractional_max_pool_v2(rand_mat, [1, 1.5, 1.5, 1], seed=1)\n\n      with self.assertRaisesRegex(ValueError,\n                                  'requires \"seed\" and \"seed2\" to be non-zero'):\n        nn_ops.fractional_max_pool(rand_mat, [1, 1.5, 1.5, 1])\n      nn_ops.fractional_max_pool(\n          rand_mat, [1, 1.5, 1.5, 1], seed=1, seed2=1, deterministic=True)\n\n  def testPoolingRatioHasMoreDimThanInput(self):\n    with self.cached_session() as _:\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Pooling ratio is higher than input dimension size for dimension 1.*\"\n      ):\n        result = nn_ops.gen_nn_ops.fractional_max_pool(\n            value=constant_op.constant(\n                value=[[[[1, 4, 2, 3]]]], dtype=dtypes.int64),\n            pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n            pseudo_random=False,\n            overlapping=False,\n            deterministic=False,\n            seed=0,\n            seed2=0,\n            name=None)\n        self.evaluate(result)\n\n  def testPoolingRatioValueOutOfRange(self):\n    with self.cached_session() as _:\n      # Whether turn on `TF2_BEHAVIOR` generates different error messages\n      with self.assertRaisesRegex(\n          (errors.InvalidArgumentError, ValueError),\n          r\"(pooling_ratio cannot be smaller than 1, got: .*)|(is negative)\"):\n        result = nn_ops.gen_nn_ops.fractional_max_pool(\n            value=np.zeros([3, 30, 30, 3]),\n            pooling_ratio=[1, -1, 3, 1],\n            pseudo_random=False,\n            overlapping=False,\n            deterministic=False,\n            seed=0,\n            seed2=0,\n        )\n        self.evaluate(result)\n\n\nclass FractionalMaxPoolGradTest(test.TestCase):\n  \"\"\"Tests for FractionalMaxPoolGrad.\n\n  Two types of tests for FractionalMaxPoolGrad.\n  1) Test fractional_max_pool_grad() directly.\n    This type of test relies on gen_nn_ops.max_pool_grad() returns the correct\n  result. For example:\n    * input_tensor_shape = (1, 10, 10, 1)\n    * window_size = (1, 2, 2, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not really import, since 10/2 is divisible\n  max pooling should generate the same result as fractional max pooling with:\n    * row_sequence = [0, 2, 4, 6, 8, 10]\n    * col_sequence = [0, 2, 4, 6, 8, 10]\n    * overlapping = False\n  This also means their gradients in such case will be the same.\n\n    Similarly, when\n    * input_tensor_shape = (1, 7, 7, 1)\n    * window_size = (1, 3, 3, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not important\n  max pooling should generate the same result as fractional max pooling with:\n    * row_sequence = [0, 2, 4, 7]\n    * col_sequence = [0, 2, 4, 7]\n    * overlapping = True\n  2) Test through compute_gradient_error()\n  \"\"\"\n\n  _PRNG = np.random.RandomState(341261)\n  _SEED = 123456\n\n  def _GenerateUniqueRandomInputTensor(self, shape):\n    \"\"\"Generate 'unique' random input tensor.\n\n    'Unique' means there's no collision values in the tensor, all elements are\n    different. This is done by generating sequence of integers with step of 1\n    and then randomly shuffle these integers.\n\n    Args:\n      shape: Shape of the tensor desired.\n\n    Returns:\n      A numpy ndarray with size = shape and dtype = numpy.float32.\n    \"\"\"\n    num_elements = 1\n    for size in shape:\n      num_elements *= size\n    x = np.arange(num_elements, dtype=np.float32)\n    self._PRNG.shuffle(x)\n    return x.reshape(shape)\n\n  def testDirectNotUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = row_window_size * 5\n          num_cols = col_window_size * 7\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateUniqueRandomInputTensor(input_shape))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size, col_window_size, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.max_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              output_backprop = self._PRNG.randint(100, size=output_data.shape)\n              input_backprop_tensor = gen_nn_ops.max_pool_grad(\n                  input_tensor, output_tensor, output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows + 1, row_window_size))\n              col_seq = list(range(0, num_cols + 1, col_window_size))\n              fmp_input_backprop_tensor = gen_nn_ops.fractional_max_pool_grad(\n                  input_tensor,\n                  output_tensor,\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=False)\n              fmp_input_backprop = self.evaluate(fmp_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fmp_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fmp_input_backprop)\n\n  def testDirectUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = (row_window_size - 1) * 5 + 1\n          num_cols = (col_window_size - 1) * 7 + 1\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateUniqueRandomInputTensor(input_shape))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size - 1, col_window_size - 1, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.max_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              output_backprop = self._PRNG.randint(100, size=output_data.shape)\n              input_backprop_tensor = gen_nn_ops.max_pool_grad(\n                  input_tensor, output_tensor, output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows, row_window_size - 1))\n              col_seq = list(range(0, num_cols, col_window_size - 1))\n              row_seq[-1] += 1\n              col_seq[-1] += 1\n              fmp_input_backprop_tensor = gen_nn_ops.fractional_max_pool_grad(\n                  input_tensor,\n                  output_tensor,\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=True)\n              fmp_input_backprop = self.evaluate(fmp_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fmp_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fmp_input_backprop)\n\n  @test_util.run_deprecated_v1\n  def testAllInputOptionsThroughGradientError(self):\n    input_shape = (1, 7, 13, 1)\n    input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n    # Add some randomness to make input_data not so 'integer'\n    input_data += self._PRNG.random_sample(input_shape)\n    pooling_ratio = [1, math.sqrt(2), math.sqrt(3), 1]\n\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        with self.cached_session() as _:\n          input_tensor = constant_op.constant(input_data, shape=input_shape)\n          output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n              input_tensor,\n              pooling_ratio,\n              pseudo_random=pseudo_random,\n              overlapping=overlapping,\n              seed=self._SEED)\n          output_data = self.evaluate(output_tensor)\n          output_shape = output_data.shape\n          # error_margin and delta setting is similar to max_pool_grad.\n          error_margin = 1e-3\n          gradient_error = gradient_checker.compute_gradient_error(\n              input_tensor,\n              input_shape,\n              output_tensor,\n              output_shape,\n              x_init_value=input_data.reshape(input_shape),\n              delta=1e-2)\n          self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testDifferentTensorShapesThroughGradientError(self):\n    pseudo_random = True\n    overlapping = True\n    pooling_ratio = [1, math.sqrt(3), math.sqrt(2), 1]\n    for num_batches in [1, 2]:\n      for num_rows in [5, 13]:\n        for num_cols in [5, 11]:\n          for num_channels in [1, 3]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n            # Add some randomness to make input_data not so 'integer'\n            input_data += self._PRNG.random_sample(input_shape)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(input_data, shape=input_shape)\n              output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n                  input_tensor,\n                  pooling_ratio,\n                  pseudo_random=pseudo_random,\n                  overlapping=overlapping,\n                  seed=self._SEED)\n              output_data = self.evaluate(output_tensor)\n              output_shape = output_data.shape\n              # error_margin and delta setting is similar to max_pool_grad.\n              error_margin = 1e-3\n              gradient_error = gradient_checker.compute_gradient_error(\n                  input_tensor,\n                  input_shape,\n                  output_tensor,\n                  output_shape,\n                  x_init_value=input_data.reshape(input_shape),\n                  delta=1e-2)\n              self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testLargePoolingRatioThroughGradientError(self):\n    input_shape = (1, 17, 23, 1)\n    input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n    # Add some randomness to make input_data not so 'integer'\n    input_data += self._PRNG.random_sample(input_shape)\n    pooling_ratio = (1, math.sqrt(13), math.sqrt(7), 1)\n    output_shape = [int(a / b) for a, b in zip(input_shape, pooling_ratio)]\n    overlapping = True\n    pseudo_random = False\n\n    with self.cached_session() as _:\n      input_tensor = constant_op.constant(input_data, shape=input_shape)\n      output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random=pseudo_random,\n          overlapping=overlapping,\n          seed=self._SEED)\n      # error_margin and delta setting is similar to max_pool_grad.\n      error_margin = 1e-3\n      gradient_error = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_shape,\n          output_tensor,\n          output_shape,\n          x_init_value=input_data.reshape(input_shape),\n          delta=1e-2)\n      self.assertLess(gradient_error, error_margin)\n\n  def testWhenRepeatedMaxValueInPoolingRegion(self):\n    \"\"\"Test when there's repeating value in pooling region.\n\n    There's no formal definition for what the gradient should be when there're\n    multiple max value within a pooling cell. Such as\n        | 1 5 |\n        | 5 3 |\n    The expected result depends heavily on implementation, if someone swap the\n    order of a nested for loop when walking through the tensor, result would be\n    very different.\n\n    The goal of this test is to alert when someone else change the\n    implementation. Current implementation scans row-by-row.\n    \"\"\"\n    input_data = [5.0, 4.0, 6.0, 7.0,\n                  3.0, 5.0, 9.0, 6.0,\n                  8.0, 8.0, 9.0, 5.0,\n                  7.0, 4.0, 0.0, 0.0]  # pyformat: disable\n    input_size = [1, 4, 4, 1]\n    output_backprop = [12.0, 15.0,\n                       17.0, -5.0,\n                       6.0, 21.0]  # pyformat: disable\n    row_seq = [0, 1, 3, 4]\n    col_seq = [0, 2, 4]\n    output_data_not_overlapping = [5.0, 7.0,\n                                   8.0, 9.0,\n                                   7.0, 0.0]  # pyformat: disable\n    output_data_overlapping = [9.0, 9.0,\n                               9.0, 9.0,\n                               7.0, 0.0]  # pyformat: disable\n    output_size = [1, 3, 2, 1]\n    expected_input_backprop_not_overlapping = np.reshape(\n        [12.0, 0.0, 0.0, 15.0,\n         0.0, 0.0, -5.0, 0.0,\n         17.0, 0.0, 0.0, 0.0,\n         6.0, 0.0, 21.0, 0.0],\n        input_size)  # pyformat: disable\n    expected_input_backprop_overlapping = np.reshape(\n        [0.0, 0.0, 0.0, 0.0,\n         0.0, 0.0, 39.0, 0.0,\n         0.0, 0.0, 0.0, 0.0,\n         6.0, 0.0, 21.0, 0.0],\n        input_size)  # pyformat: disable\n    with self.cached_session() as _:\n      # Test when overlapping is False\n      input_tensor = constant_op.constant(input_data, shape=input_size)\n      output_tensor = constant_op.constant(\n          output_data_not_overlapping, shape=output_size)\n      grad = constant_op.constant(output_backprop, shape=output_size)\n      r = gen_nn_ops.fractional_max_pool_grad(\n          input_tensor,\n          output_tensor,\n          grad,\n          row_seq,\n          col_seq,\n          overlapping=False)\n      input_backprop_not_overlapping = self.evaluate(r)\n      self.assertShapeEqual(\n          np.reshape(expected_input_backprop_not_overlapping, input_size), r)\n      self.assertAllClose(expected_input_backprop_not_overlapping,\n                          input_backprop_not_overlapping)\n      # Test when overlapping is True\n      output_tensor = constant_op.constant(\n          output_data_overlapping, shape=output_size)\n      r = gen_nn_ops.fractional_max_pool_grad(\n          input_tensor, output_tensor, grad, row_seq, col_seq, overlapping=True)\n      input_backprop_overlapping = self.evaluate(r)\n      self.assertShapeEqual(\n          np.reshape(expected_input_backprop_overlapping, input_size), r)\n      self.assertAllClose(expected_input_backprop_overlapping,\n                          input_backprop_overlapping)\n\n  def testInvalidSeqRaiseErrorForFractionalMaxPoolGrad(self):\n    with self.assertRaises(errors.InvalidArgumentError):\n      with self.cached_session():\n        overlapping = True\n        orig_input = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        orig_output = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        out_backprop = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        row_pooling_sequence = constant_op.constant(\n            0, shape=[5], dtype=dtypes.int64)\n        col_pooling_sequence = constant_op.constant(\n            0, shape=[5], dtype=dtypes.int64)\n        t = gen_nn_ops.FractionalMaxPoolGrad(\n            orig_input=orig_input,\n            orig_output=orig_output,\n            out_backprop=out_backprop,\n            row_pooling_sequence=row_pooling_sequence,\n            col_pooling_sequence=col_pooling_sequence,\n            overlapping=overlapping)\n        self.evaluate(t)\n\n  def testOverLargeSeqRaiseErrorForFractionalMaxPoolGrad(self):\n    with self.assertRaises(errors.InvalidArgumentError):\n      with self.cached_session():\n        overlapping = False\n        orig_input = [[[[1, 1, 1, 1, 1]]]]\n        orig_output = [[[[1, 1, 1]]]]\n        out_backprop = [[[[3], [3], [6]]]]\n        row_pooling_sequence = [-0x4000000, 1, 1]\n        col_pooling_sequence = [-0x4000000, 1, 1]\n        t = gen_nn_ops.FractionalMaxPoolGrad(\n            orig_input=orig_input,\n            orig_output=orig_output,\n            out_backprop=out_backprop,\n            row_pooling_sequence=row_pooling_sequence,\n            col_pooling_sequence=col_pooling_sequence,\n            overlapping=overlapping)\n        self.evaluate(t)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/fractional_avg_pool_op.cc", "tensorflow/core/kernels/fractional_max_pool_op.cc", "tensorflow/core/ops/nn_ops.cc", "tensorflow/core/ops/nn_ops_test.cc", "tensorflow/python/kernel_tests/nn_ops/fractional_avg_pool_op_test.py", "tensorflow/python/kernel_tests/nn_ops/fractional_max_pool_op_test.py"], "buggy_code_start_loc": [46, 47, 62, 526, 334, 323], "buggy_code_end_loc": [88, 47, 62, 540, 334, 338], "fixing_code_start_loc": [47, 48, 63, 526, 335, 323], "fixing_code_end_loc": [96, 54, 70, 541, 370, 356], "type": "CWE-125", "message": "TensorFlow is an open source platform for machine learning. The security vulnerability results in FractionalMax(AVG)Pool with illegal pooling_ratio. Attackers using Tensorflow can exploit the vulnerability. They can access heap memory which is not in the control of user, leading to a crash or remote code execution. We have patched the issue in GitHub commit 216525144ee7c910296f5b05d214ca1327c9ce48. The fix will be included in TensorFlow 2.11.0. We will also cherry pick this commit on TensorFlow 2.10.1.", "other": {"cve": {"id": "CVE-2022-41900", "sourceIdentifier": "security-advisories@github.com", "published": "2022-11-18T22:15:20.273", "lastModified": "2022-11-23T13:35:44.303", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. The security vulnerability results in FractionalMax(AVG)Pool with illegal pooling_ratio. Attackers using Tensorflow can exploit the vulnerability. They can access heap memory which is not in the control of user, leading to a crash or remote code execution. We have patched the issue in GitHub commit 216525144ee7c910296f5b05d214ca1327c9ce48. The fix will be included in TensorFlow 2.11.0. We will also cherry pick this commit on TensorFlow 2.10.1."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:R/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.2, "impactScore": 5.9}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}, {"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.8.4", "matchCriteriaId": "A694EEE1-BFB9-4E6C-B275-02DC2731961C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.3", "matchCriteriaId": "9057B403-719C-4F10-BAB6-67F84786A89E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10.0:*:*:*:*:*:*:*", "matchCriteriaId": "6AE6CFC4-0232-4E1C-960D-268C87788735"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/216525144ee7c910296f5b05d214ca1327c9ce48", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-xvwp-h6jv-7472", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/216525144ee7c910296f5b05d214ca1327c9ce48"}}