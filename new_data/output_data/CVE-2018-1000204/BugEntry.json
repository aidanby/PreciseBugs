{"buggy_code": ["/*\n *  History:\n *  Started: Aug 9 by Lawrence Foard (entropy@world.std.com),\n *           to allow user process control of SCSI devices.\n *  Development Sponsored by Killy Corp. NY NY\n *\n * Original driver (sg.c):\n *        Copyright (C) 1992 Lawrence Foard\n * Version 2 and 3 extensions to driver:\n *        Copyright (C) 1998 - 2014 Douglas Gilbert\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2, or (at your option)\n * any later version.\n *\n */\n\nstatic int sg_version_num = 30536;\t/* 2 digits for each component */\n#define SG_VERSION_STR \"3.5.36\"\n\n/*\n *  D. P. Gilbert (dgilbert@interlog.com), notes:\n *      - scsi logging is available via SCSI_LOG_TIMEOUT macros. First\n *        the kernel/module needs to be built with CONFIG_SCSI_LOGGING\n *        (otherwise the macros compile to empty statements).\n *\n */\n#include <linux/module.h>\n\n#include <linux/fs.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/errno.h>\n#include <linux/mtio.h>\n#include <linux/ioctl.h>\n#include <linux/slab.h>\n#include <linux/fcntl.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/moduleparam.h>\n#include <linux/cdev.h>\n#include <linux/idr.h>\n#include <linux/seq_file.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/blktrace_api.h>\n#include <linux/mutex.h>\n#include <linux/atomic.h>\n#include <linux/ratelimit.h>\n#include <linux/uio.h>\n\n#include \"scsi.h\"\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/sg.h>\n\n#include \"scsi_logging.h\"\n\n#ifdef CONFIG_SCSI_PROC_FS\n#include <linux/proc_fs.h>\nstatic char *sg_version_date = \"20140603\";\n\nstatic int sg_proc_init(void);\nstatic void sg_proc_cleanup(void);\n#endif\n\n#define SG_ALLOW_DIO_DEF 0\n\n#define SG_MAX_DEVS 32768\n\n/* SG_MAX_CDB_SIZE should be 260 (spc4r37 section 3.1.30) however the type\n * of sg_io_hdr::cmd_len can only represent 255. All SCSI commands greater\n * than 16 bytes are \"variable length\" whose length is a multiple of 4\n */\n#define SG_MAX_CDB_SIZE 252\n\n#define SG_DEFAULT_TIMEOUT mult_frac(SG_DEFAULT_TIMEOUT_USER, HZ, USER_HZ)\n\nint sg_big_buff = SG_DEF_RESERVED_SIZE;\n/* N.B. This variable is readable and writeable via\n   /proc/scsi/sg/def_reserved_size . Each time sg_open() is called a buffer\n   of this size (or less if there is not enough memory) will be reserved\n   for use by this file descriptor. [Deprecated usage: this variable is also\n   readable via /proc/sys/kernel/sg-big-buff if the sg driver is built into\n   the kernel (i.e. it is not a module).] */\nstatic int def_reserved_size = -1;\t/* picks up init parameter */\nstatic int sg_allow_dio = SG_ALLOW_DIO_DEF;\n\nstatic int scatter_elem_sz = SG_SCATTER_SZ;\nstatic int scatter_elem_sz_prev = SG_SCATTER_SZ;\n\n#define SG_SECTOR_SZ 512\n\nstatic int sg_add_device(struct device *, struct class_interface *);\nstatic void sg_remove_device(struct device *, struct class_interface *);\n\nstatic DEFINE_IDR(sg_index_idr);\nstatic DEFINE_RWLOCK(sg_index_lock);\t/* Also used to lock\n\t\t\t\t\t\t\t   file descriptor list for device */\n\nstatic struct class_interface sg_interface = {\n\t.add_dev        = sg_add_device,\n\t.remove_dev     = sg_remove_device,\n};\n\ntypedef struct sg_scatter_hold { /* holding area for scsi scatter gather info */\n\tunsigned short k_use_sg; /* Count of kernel scatter-gather pieces */\n\tunsigned sglist_len; /* size of malloc'd scatter-gather list ++ */\n\tunsigned bufflen;\t/* Size of (aggregate) data buffer */\n\tstruct page **pages;\n\tint page_order;\n\tchar dio_in_use;\t/* 0->indirect IO (or mmap), 1->dio */\n\tunsigned char cmd_opcode; /* first byte of command */\n} Sg_scatter_hold;\n\nstruct sg_device;\t\t/* forward declarations */\nstruct sg_fd;\n\ntypedef struct sg_request {\t/* SG_MAX_QUEUE requests outstanding per file */\n\tstruct list_head entry;\t/* list entry */\n\tstruct sg_fd *parentfp;\t/* NULL -> not in use */\n\tSg_scatter_hold data;\t/* hold buffer, perhaps scatter list */\n\tsg_io_hdr_t header;\t/* scsi command+info, see <scsi/sg.h> */\n\tunsigned char sense_b[SCSI_SENSE_BUFFERSIZE];\n\tchar res_used;\t\t/* 1 -> using reserve buffer, 0 -> not ... */\n\tchar orphan;\t\t/* 1 -> drop on sight, 0 -> normal */\n\tchar sg_io_owned;\t/* 1 -> packet belongs to SG_IO */\n\t/* done protected by rq_list_lock */\n\tchar done;\t\t/* 0->before bh, 1->before read, 2->read */\n\tstruct request *rq;\n\tstruct bio *bio;\n\tstruct execute_work ew;\n} Sg_request;\n\ntypedef struct sg_fd {\t\t/* holds the state of a file descriptor */\n\tstruct list_head sfd_siblings;  /* protected by device's sfd_lock */\n\tstruct sg_device *parentdp;\t/* owning device */\n\twait_queue_head_t read_wait;\t/* queue read until command done */\n\trwlock_t rq_list_lock;\t/* protect access to list in req_arr */\n\tstruct mutex f_mutex;\t/* protect against changes in this fd */\n\tint timeout;\t\t/* defaults to SG_DEFAULT_TIMEOUT      */\n\tint timeout_user;\t/* defaults to SG_DEFAULT_TIMEOUT_USER */\n\tSg_scatter_hold reserve;\t/* buffer held for this file descriptor */\n\tstruct list_head rq_list; /* head of request list */\n\tstruct fasync_struct *async_qp;\t/* used by asynchronous notification */\n\tSg_request req_arr[SG_MAX_QUEUE];\t/* used as singly-linked list */\n\tchar force_packid;\t/* 1 -> pack_id input to read(), 0 -> ignored */\n\tchar cmd_q;\t\t/* 1 -> allow command queuing, 0 -> don't */\n\tunsigned char next_cmd_len; /* 0: automatic, >0: use on next write() */\n\tchar keep_orphan;\t/* 0 -> drop orphan (def), 1 -> keep for read() */\n\tchar mmap_called;\t/* 0 -> mmap() never called on this fd */\n\tchar res_in_use;\t/* 1 -> 'reserve' array in use */\n\tstruct kref f_ref;\n\tstruct execute_work ew;\n} Sg_fd;\n\ntypedef struct sg_device { /* holds the state of each scsi generic device */\n\tstruct scsi_device *device;\n\twait_queue_head_t open_wait;    /* queue open() when O_EXCL present */\n\tstruct mutex open_rel_lock;     /* held when in open() or release() */\n\tint sg_tablesize;\t/* adapter's max scatter-gather table size */\n\tu32 index;\t\t/* device index number */\n\tstruct list_head sfds;\n\trwlock_t sfd_lock;      /* protect access to sfd list */\n\tatomic_t detaching;     /* 0->device usable, 1->device detaching */\n\tbool exclude;\t\t/* 1->open(O_EXCL) succeeded and is active */\n\tint open_cnt;\t\t/* count of opens (perhaps < num(sfds) ) */\n\tchar sgdebug;\t\t/* 0->off, 1->sense, 9->dump dev, 10-> all devs */\n\tstruct gendisk *disk;\n\tstruct cdev * cdev;\t/* char_dev [sysfs: /sys/cdev/major/sg<n>] */\n\tstruct kref d_ref;\n} Sg_device;\n\n/* tasklet or soft irq callback */\nstatic void sg_rq_end_io(struct request *rq, blk_status_t status);\nstatic int sg_start_req(Sg_request *srp, unsigned char *cmd);\nstatic int sg_finish_rem_req(Sg_request * srp);\nstatic int sg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size);\nstatic ssize_t sg_new_read(Sg_fd * sfp, char __user *buf, size_t count,\n\t\t\t   Sg_request * srp);\nstatic ssize_t sg_new_write(Sg_fd *sfp, struct file *file,\n\t\t\tconst char __user *buf, size_t count, int blocking,\n\t\t\tint read_only, int sg_io_owned, Sg_request **o_srp);\nstatic int sg_common_write(Sg_fd * sfp, Sg_request * srp,\n\t\t\t   unsigned char *cmnd, int timeout, int blocking);\nstatic int sg_read_oxfer(Sg_request * srp, char __user *outp, int num_read_xfer);\nstatic void sg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp);\nstatic void sg_build_reserve(Sg_fd * sfp, int req_size);\nstatic void sg_link_reserve(Sg_fd * sfp, Sg_request * srp, int size);\nstatic void sg_unlink_reserve(Sg_fd * sfp, Sg_request * srp);\nstatic Sg_fd *sg_add_sfp(Sg_device * sdp);\nstatic void sg_remove_sfp(struct kref *);\nstatic Sg_request *sg_get_rq_mark(Sg_fd * sfp, int pack_id);\nstatic Sg_request *sg_add_request(Sg_fd * sfp);\nstatic int sg_remove_request(Sg_fd * sfp, Sg_request * srp);\nstatic Sg_device *sg_get_dev(int dev);\nstatic void sg_device_destroy(struct kref *kref);\n\n#define SZ_SG_HEADER sizeof(struct sg_header)\n#define SZ_SG_IO_HDR sizeof(sg_io_hdr_t)\n#define SZ_SG_IOVEC sizeof(sg_iovec_t)\n#define SZ_SG_REQ_INFO sizeof(sg_req_info_t)\n\n#define sg_printk(prefix, sdp, fmt, a...) \\\n\tsdev_prefix_printk(prefix, (sdp)->device,\t\t\\\n\t\t\t   (sdp)->disk->disk_name, fmt, ##a)\n\nstatic int sg_allow_access(struct file *filp, unsigned char *cmd)\n{\n\tstruct sg_fd *sfp = filp->private_data;\n\n\tif (sfp->parentdp->device->type == TYPE_SCANNER)\n\t\treturn 0;\n\n\treturn blk_verify_command(cmd, filp->f_mode);\n}\n\nstatic int\nopen_wait(Sg_device *sdp, int flags)\n{\n\tint retval = 0;\n\n\tif (flags & O_EXCL) {\n\t\twhile (sdp->open_cnt > 0) {\n\t\t\tmutex_unlock(&sdp->open_rel_lock);\n\t\t\tretval = wait_event_interruptible(sdp->open_wait,\n\t\t\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t\t\t !sdp->open_cnt));\n\t\t\tmutex_lock(&sdp->open_rel_lock);\n\n\t\t\tif (retval) /* -ERESTARTSYS */\n\t\t\t\treturn retval;\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t}\n\t} else {\n\t\twhile (sdp->exclude) {\n\t\t\tmutex_unlock(&sdp->open_rel_lock);\n\t\t\tretval = wait_event_interruptible(sdp->open_wait,\n\t\t\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t\t\t !sdp->exclude));\n\t\t\tmutex_lock(&sdp->open_rel_lock);\n\n\t\t\tif (retval) /* -ERESTARTSYS */\n\t\t\t\treturn retval;\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t}\n\t}\n\n\treturn retval;\n}\n\n/* Returns 0 on success, else a negated errno value */\nstatic int\nsg_open(struct inode *inode, struct file *filp)\n{\n\tint dev = iminor(inode);\n\tint flags = filp->f_flags;\n\tstruct request_queue *q;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tint retval;\n\n\tnonseekable_open(inode, filp);\n\tif ((flags & O_EXCL) && (O_RDONLY == (flags & O_ACCMODE)))\n\t\treturn -EPERM; /* Can't lock it with read only access */\n\tsdp = sg_get_dev(dev);\n\tif (IS_ERR(sdp))\n\t\treturn PTR_ERR(sdp);\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_open: flags=0x%x\\n\", flags));\n\n\t/* This driver's module count bumped by fops_get in <linux/fs.h> */\n\t/* Prevent the device driver from vanishing while we sleep */\n\tretval = scsi_device_get(sdp->device);\n\tif (retval)\n\t\tgoto sg_put;\n\n\tretval = scsi_autopm_get_device(sdp->device);\n\tif (retval)\n\t\tgoto sdp_put;\n\n\t/* scsi_block_when_processing_errors() may block so bypass\n\t * check if O_NONBLOCK. Permits SCSI commands to be issued\n\t * during error recovery. Tread carefully. */\n\tif (!((flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device))) {\n\t\tretval = -ENXIO;\n\t\t/* we are in error recovery for this device */\n\t\tgoto error_out;\n\t}\n\n\tmutex_lock(&sdp->open_rel_lock);\n\tif (flags & O_NONBLOCK) {\n\t\tif (flags & O_EXCL) {\n\t\t\tif (sdp->open_cnt > 0) {\n\t\t\t\tretval = -EBUSY;\n\t\t\t\tgoto error_mutex_locked;\n\t\t\t}\n\t\t} else {\n\t\t\tif (sdp->exclude) {\n\t\t\t\tretval = -EBUSY;\n\t\t\t\tgoto error_mutex_locked;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tretval = open_wait(sdp, flags);\n\t\tif (retval) /* -ERESTARTSYS or -ENODEV */\n\t\t\tgoto error_mutex_locked;\n\t}\n\n\t/* N.B. at this point we are holding the open_rel_lock */\n\tif (flags & O_EXCL)\n\t\tsdp->exclude = true;\n\n\tif (sdp->open_cnt < 1) {  /* no existing opens */\n\t\tsdp->sgdebug = 0;\n\t\tq = sdp->device->request_queue;\n\t\tsdp->sg_tablesize = queue_max_segments(q);\n\t}\n\tsfp = sg_add_sfp(sdp);\n\tif (IS_ERR(sfp)) {\n\t\tretval = PTR_ERR(sfp);\n\t\tgoto out_undo;\n\t}\n\n\tfilp->private_data = sfp;\n\tsdp->open_cnt++;\n\tmutex_unlock(&sdp->open_rel_lock);\n\n\tretval = 0;\nsg_put:\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n\treturn retval;\n\nout_undo:\n\tif (flags & O_EXCL) {\n\t\tsdp->exclude = false;   /* undo if error */\n\t\twake_up_interruptible(&sdp->open_wait);\n\t}\nerror_mutex_locked:\n\tmutex_unlock(&sdp->open_rel_lock);\nerror_out:\n\tscsi_autopm_put_device(sdp->device);\nsdp_put:\n\tscsi_device_put(sdp->device);\n\tgoto sg_put;\n}\n\n/* Release resources associated with a successful sg_open()\n * Returns 0 on success, else a negated errno value */\nstatic int\nsg_release(struct inode *inode, struct file *filp)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp, \"sg_release\\n\"));\n\n\tmutex_lock(&sdp->open_rel_lock);\n\tscsi_autopm_put_device(sdp->device);\n\tkref_put(&sfp->f_ref, sg_remove_sfp);\n\tsdp->open_cnt--;\n\n\t/* possibly many open()s waiting on exlude clearing, start many;\n\t * only open(O_EXCL)s wait on 0==open_cnt so only start one */\n\tif (sdp->exclude) {\n\t\tsdp->exclude = false;\n\t\twake_up_interruptible_all(&sdp->open_wait);\n\t} else if (0 == sdp->open_cnt) {\n\t\twake_up_interruptible(&sdp->open_wait);\n\t}\n\tmutex_unlock(&sdp->open_rel_lock);\n\treturn 0;\n}\n\nstatic ssize_t\nsg_read(struct file *filp, char __user *buf, size_t count, loff_t * ppos)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tint req_pack_id = -1;\n\tsg_io_hdr_t *hp;\n\tstruct sg_header *old_hdr = NULL;\n\tint retval = 0;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_read: count=%d\\n\", (int) count));\n\n\tif (!access_ok(VERIFY_WRITE, buf, count))\n\t\treturn -EFAULT;\n\tif (sfp->force_packid && (count >= SZ_SG_HEADER)) {\n\t\told_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);\n\t\tif (!old_hdr)\n\t\t\treturn -ENOMEM;\n\t\tif (__copy_from_user(old_hdr, buf, SZ_SG_HEADER)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (old_hdr->reply_len < 0) {\n\t\t\tif (count >= SZ_SG_IO_HDR) {\n\t\t\t\tsg_io_hdr_t *new_hdr;\n\t\t\t\tnew_hdr = kmalloc(SZ_SG_IO_HDR, GFP_KERNEL);\n\t\t\t\tif (!new_hdr) {\n\t\t\t\t\tretval = -ENOMEM;\n\t\t\t\t\tgoto free_old_hdr;\n\t\t\t\t}\n\t\t\t\tretval =__copy_from_user\n\t\t\t\t    (new_hdr, buf, SZ_SG_IO_HDR);\n\t\t\t\treq_pack_id = new_hdr->pack_id;\n\t\t\t\tkfree(new_hdr);\n\t\t\t\tif (retval) {\n\t\t\t\t\tretval = -EFAULT;\n\t\t\t\t\tgoto free_old_hdr;\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\treq_pack_id = old_hdr->pack_id;\n\t}\n\tsrp = sg_get_rq_mark(sfp, req_pack_id);\n\tif (!srp) {\t\t/* now wait on packet to arrive */\n\t\tif (atomic_read(&sdp->detaching)) {\n\t\t\tretval = -ENODEV;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tretval = wait_event_interruptible(sfp->read_wait,\n\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t(srp = sg_get_rq_mark(sfp, req_pack_id))));\n\t\tif (atomic_read(&sdp->detaching)) {\n\t\t\tretval = -ENODEV;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (retval) {\n\t\t\t/* -ERESTARTSYS as signal hit process */\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t}\n\tif (srp->header.interface_id != '\\0') {\n\t\tretval = sg_new_read(sfp, buf, count, srp);\n\t\tgoto free_old_hdr;\n\t}\n\n\thp = &srp->header;\n\tif (old_hdr == NULL) {\n\t\told_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);\n\t\tif (! old_hdr) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t}\n\tmemset(old_hdr, 0, SZ_SG_HEADER);\n\told_hdr->reply_len = (int) hp->timeout;\n\told_hdr->pack_len = old_hdr->reply_len; /* old, strange behaviour */\n\told_hdr->pack_id = hp->pack_id;\n\told_hdr->twelve_byte =\n\t    ((srp->data.cmd_opcode >= 0xc0) && (12 == hp->cmd_len)) ? 1 : 0;\n\told_hdr->target_status = hp->masked_status;\n\told_hdr->host_status = hp->host_status;\n\told_hdr->driver_status = hp->driver_status;\n\tif ((CHECK_CONDITION & hp->masked_status) ||\n\t    (DRIVER_SENSE & hp->driver_status))\n\t\tmemcpy(old_hdr->sense_buffer, srp->sense_b,\n\t\t       sizeof (old_hdr->sense_buffer));\n\tswitch (hp->host_status) {\n\t/* This setup of 'result' is for backward compatibility and is best\n\t   ignored by the user who should use target, host + driver status */\n\tcase DID_OK:\n\tcase DID_PASSTHROUGH:\n\tcase DID_SOFT_ERROR:\n\t\told_hdr->result = 0;\n\t\tbreak;\n\tcase DID_NO_CONNECT:\n\tcase DID_BUS_BUSY:\n\tcase DID_TIME_OUT:\n\t\told_hdr->result = EBUSY;\n\t\tbreak;\n\tcase DID_BAD_TARGET:\n\tcase DID_ABORT:\n\tcase DID_PARITY:\n\tcase DID_RESET:\n\tcase DID_BAD_INTR:\n\t\told_hdr->result = EIO;\n\t\tbreak;\n\tcase DID_ERROR:\n\t\told_hdr->result = (srp->sense_b[0] == 0 && \n\t\t\t\t  hp->masked_status == GOOD) ? 0 : EIO;\n\t\tbreak;\n\tdefault:\n\t\told_hdr->result = EIO;\n\t\tbreak;\n\t}\n\n\t/* Now copy the result back to the user buffer.  */\n\tif (count >= SZ_SG_HEADER) {\n\t\tif (__copy_to_user(buf, old_hdr, SZ_SG_HEADER)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tbuf += SZ_SG_HEADER;\n\t\tif (count > old_hdr->reply_len)\n\t\t\tcount = old_hdr->reply_len;\n\t\tif (count > SZ_SG_HEADER) {\n\t\t\tif (sg_read_oxfer(srp, buf, count - SZ_SG_HEADER)) {\n\t\t\t\tretval = -EFAULT;\n\t\t\t\tgoto free_old_hdr;\n\t\t\t}\n\t\t}\n\t} else\n\t\tcount = (old_hdr->result == 0) ? 0 : -EIO;\n\tsg_finish_rem_req(srp);\n\tsg_remove_request(sfp, srp);\n\tretval = count;\nfree_old_hdr:\n\tkfree(old_hdr);\n\treturn retval;\n}\n\nstatic ssize_t\nsg_new_read(Sg_fd * sfp, char __user *buf, size_t count, Sg_request * srp)\n{\n\tsg_io_hdr_t *hp = &srp->header;\n\tint err = 0, err2;\n\tint len;\n\n\tif (count < SZ_SG_IO_HDR) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\thp->sb_len_wr = 0;\n\tif ((hp->mx_sb_len > 0) && hp->sbp) {\n\t\tif ((CHECK_CONDITION & hp->masked_status) ||\n\t\t    (DRIVER_SENSE & hp->driver_status)) {\n\t\t\tint sb_len = SCSI_SENSE_BUFFERSIZE;\n\t\t\tsb_len = (hp->mx_sb_len > sb_len) ? sb_len : hp->mx_sb_len;\n\t\t\tlen = 8 + (int) srp->sense_b[7];\t/* Additional sense length field */\n\t\t\tlen = (len > sb_len) ? sb_len : len;\n\t\t\tif (copy_to_user(hp->sbp, srp->sense_b, len)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\thp->sb_len_wr = len;\n\t\t}\n\t}\n\tif (hp->masked_status || hp->host_status || hp->driver_status)\n\t\thp->info |= SG_INFO_CHECK;\n\tif (copy_to_user(buf, hp, SZ_SG_IO_HDR)) {\n\t\terr = -EFAULT;\n\t\tgoto err_out;\n\t}\nerr_out:\n\terr2 = sg_finish_rem_req(srp);\n\tsg_remove_request(sfp, srp);\n\treturn err ? : err2 ? : count;\n}\n\nstatic ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif (unlikely(uaccess_kernel()))\n\t\treturn -EINVAL;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tmutex_lock(&sfp->f_mutex);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tmutex_unlock(&sfp->f_mutex);\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t   current->comm);\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}\n\nstatic ssize_t\nsg_new_write(Sg_fd *sfp, struct file *file, const char __user *buf,\n\t\t size_t count, int blocking, int read_only, int sg_io_owned,\n\t\t Sg_request **o_srp)\n{\n\tint k;\n\tSg_request *srp;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\tint timeout;\n\tunsigned long ul_timeout;\n\n\tif (count < SZ_SG_IO_HDR)\n\t\treturn -EINVAL;\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT; /* protects following copy_from_user()s + get_user()s */\n\n\tsfp->cmd_q = 1;\t/* when sg_io_hdr seen, set command queuing on */\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t\t      \"sg_new_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tsrp->sg_io_owned = sg_io_owned;\n\thp = &srp->header;\n\tif (__copy_from_user(hp, buf, SZ_SG_IO_HDR)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\n\t}\n\tif (hp->interface_id != 'S') {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -ENOSYS;\n\t}\n\tif (hp->flags & SG_FLAG_MMAP_IO) {\n\t\tif (hp->dxfer_len > sfp->reserve.bufflen) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -ENOMEM;\t/* MMAP_IO size must fit in reserve buffer */\n\t\t}\n\t\tif (hp->flags & SG_FLAG_DIRECT_IO) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -EINVAL;\t/* either MMAP_IO or DIRECT_IO (not both) */\n\t\t}\n\t\tif (sfp->res_in_use) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -EBUSY;\t/* reserve buffer already being used */\n\t\t}\n\t}\n\tul_timeout = msecs_to_jiffies(srp->header.timeout);\n\ttimeout = (ul_timeout < INT_MAX) ? ul_timeout : INT_MAX;\n\tif ((!hp->cmdp) || (hp->cmd_len < 6) || (hp->cmd_len > sizeof (cmnd))) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (!access_ok(VERIFY_READ, hp->cmdp, hp->cmd_len)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\t}\n\tif (__copy_from_user(cmnd, hp->cmdp, hp->cmd_len)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\n\t}\n\tif (read_only && sg_allow_access(file, cmnd)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EPERM;\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, timeout, blocking);\n\tif (k < 0)\n\t\treturn k;\n\tif (o_srp)\n\t\t*o_srp = srp;\n\treturn count;\n}\n\nstatic int\nsg_common_write(Sg_fd * sfp, Sg_request * srp,\n\t\tunsigned char *cmnd, int timeout, int blocking)\n{\n\tint k, at_head;\n\tSg_device *sdp = sfp->parentdp;\n\tsg_io_hdr_t *hp = &srp->header;\n\n\tsrp->data.cmd_opcode = cmnd[0];\t/* hold opcode of command */\n\thp->status = 0;\n\thp->masked_status = 0;\n\thp->msg_status = 0;\n\thp->info = 0;\n\thp->host_status = 0;\n\thp->driver_status = 0;\n\thp->resid = 0;\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\"sg_common_write:  scsi opcode=0x%02x, cmd_size=%d\\n\",\n\t\t\t(int) cmnd[0], (int) hp->cmd_len));\n\n\tif (hp->dxfer_len >= SZ_256M)\n\t\treturn -EINVAL;\n\n\tk = sg_start_req(srp, cmnd);\n\tif (k) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\"sg_common_write: start_req err=%d\\n\", k));\n\t\tsg_finish_rem_req(srp);\n\t\tsg_remove_request(sfp, srp);\n\t\treturn k;\t/* probably out of space --> ENOMEM */\n\t}\n\tif (atomic_read(&sdp->detaching)) {\n\t\tif (srp->bio) {\n\t\t\tscsi_req_free_cmd(scsi_req(srp->rq));\n\t\t\tblk_end_request_all(srp->rq, BLK_STS_IOERR);\n\t\t\tsrp->rq = NULL;\n\t\t}\n\n\t\tsg_finish_rem_req(srp);\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -ENODEV;\n\t}\n\n\thp->duration = jiffies_to_msecs(jiffies);\n\tif (hp->interface_id != '\\0' &&\t/* v3 (or later) interface */\n\t    (SG_FLAG_Q_AT_TAIL & hp->flags))\n\t\tat_head = 0;\n\telse\n\t\tat_head = 1;\n\n\tsrp->rq->timeout = timeout;\n\tkref_get(&sfp->f_ref); /* sg_rq_end_io() does kref_put(). */\n\tblk_execute_rq_nowait(sdp->device->request_queue, sdp->disk,\n\t\t\t      srp->rq, at_head, sg_rq_end_io);\n\treturn 0;\n}\n\nstatic int srp_done(Sg_fd *sfp, Sg_request *srp)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tread_lock_irqsave(&sfp->rq_list_lock, flags);\n\tret = srp->done;\n\tread_unlock_irqrestore(&sfp->rq_list_lock, flags);\n\treturn ret;\n}\n\nstatic int max_sectors_bytes(struct request_queue *q)\n{\n\tunsigned int max_sectors = queue_max_sectors(q);\n\n\tmax_sectors = min_t(unsigned int, max_sectors, INT_MAX >> 9);\n\n\treturn max_sectors << 9;\n}\n\nstatic void\nsg_fill_request_table(Sg_fd *sfp, sg_req_info_t *rinfo)\n{\n\tSg_request *srp;\n\tint val;\n\tunsigned int ms;\n\n\tval = 0;\n\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\tif (val >= SG_MAX_QUEUE)\n\t\t\tbreak;\n\t\trinfo[val].req_state = srp->done + 1;\n\t\trinfo[val].problem =\n\t\t\tsrp->header.masked_status &\n\t\t\tsrp->header.host_status &\n\t\t\tsrp->header.driver_status;\n\t\tif (srp->done)\n\t\t\trinfo[val].duration =\n\t\t\t\tsrp->header.duration;\n\t\telse {\n\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\trinfo[val].duration =\n\t\t\t\t(ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\t\t}\n\t\trinfo[val].orphan = srp->orphan;\n\t\trinfo[val].sg_io_owned = srp->sg_io_owned;\n\t\trinfo[val].pack_id = srp->header.pack_id;\n\t\trinfo[val].usr_ptr = srp->header.usr_ptr;\n\t\tval++;\n\t}\n}\n\nstatic long\nsg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tvoid __user *p = (void __user *)arg;\n\tint __user *ip = p;\n\tint result, val, read_only;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tunsigned long iflags;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t   \"sg_ioctl: cmd=0x%x\\n\", (int) cmd_in));\n\tread_only = (O_RDWR != (filp->f_flags & O_ACCMODE));\n\n\tswitch (cmd_in) {\n\tcase SG_IO:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (!scsi_block_when_processing_errors(sdp->device))\n\t\t\treturn -ENXIO;\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_IO_HDR))\n\t\t\treturn -EFAULT;\n\t\tresult = sg_new_write(sfp, filp, p, SZ_SG_IO_HDR,\n\t\t\t\t 1, read_only, 1, &srp);\n\t\tif (result < 0)\n\t\t\treturn result;\n\t\tresult = wait_event_interruptible(sfp->read_wait,\n\t\t\t(srp_done(sfp, srp) || atomic_read(&sdp->detaching)));\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\twrite_lock_irq(&sfp->rq_list_lock);\n\t\tif (srp->done) {\n\t\t\tsrp->done = 2;\n\t\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\t\tresult = sg_new_read(sfp, p, SZ_SG_IO_HDR, srp);\n\t\t\treturn (result < 0) ? result : 0;\n\t\t}\n\t\tsrp->orphan = 1;\n\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\treturn result;\t/* -ERESTARTSYS because signal hit process */\n\tcase SG_SET_TIMEOUT:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val < 0)\n\t\t\treturn -EIO;\n\t\tif (val >= mult_frac((s64)INT_MAX, USER_HZ, HZ))\n\t\t\tval = min_t(s64, mult_frac((s64)INT_MAX, USER_HZ, HZ),\n\t\t\t\t    INT_MAX);\n\t\tsfp->timeout_user = val;\n\t\tsfp->timeout = mult_frac(val, HZ, USER_HZ);\n\n\t\treturn 0;\n\tcase SG_GET_TIMEOUT:\t/* N.B. User receives timeout as return value */\n\t\t\t\t/* strange ..., for backward compatibility */\n\t\treturn sfp->timeout_user;\n\tcase SG_SET_FORCE_LOW_DMA:\n\t\t/*\n\t\t * N.B. This ioctl never worked properly, but failed to\n\t\t * return an error value. So returning '0' to keep compability\n\t\t * with legacy applications.\n\t\t */\n\t\treturn 0;\n\tcase SG_GET_LOW_DMA:\n\t\treturn put_user((int) sdp->device->host->unchecked_isa_dma, ip);\n\tcase SG_GET_SCSI_ID:\n\t\tif (!access_ok(VERIFY_WRITE, p, sizeof (sg_scsi_id_t)))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_scsi_id_t __user *sg_idp = p;\n\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t\t__put_user((int) sdp->device->host->host_no,\n\t\t\t\t   &sg_idp->host_no);\n\t\t\t__put_user((int) sdp->device->channel,\n\t\t\t\t   &sg_idp->channel);\n\t\t\t__put_user((int) sdp->device->id, &sg_idp->scsi_id);\n\t\t\t__put_user((int) sdp->device->lun, &sg_idp->lun);\n\t\t\t__put_user((int) sdp->device->type, &sg_idp->scsi_type);\n\t\t\t__put_user((short) sdp->device->host->cmd_per_lun,\n\t\t\t\t   &sg_idp->h_cmd_per_lun);\n\t\t\t__put_user((short) sdp->device->queue_depth,\n\t\t\t\t   &sg_idp->d_queue_depth);\n\t\t\t__put_user(0, &sg_idp->unused[0]);\n\t\t\t__put_user(0, &sg_idp->unused[1]);\n\t\t\treturn 0;\n\t\t}\n\tcase SG_SET_FORCE_PACK_ID:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->force_packid = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_PACK_ID:\n\t\tif (!access_ok(VERIFY_WRITE, ip, sizeof (int)))\n\t\t\treturn -EFAULT;\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned)) {\n\t\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock,\n\t\t\t\t\t\t       iflags);\n\t\t\t\t__put_user(srp->header.pack_id, ip);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t__put_user(-1, ip);\n\t\treturn 0;\n\tcase SG_GET_NUM_WAITING:\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tval = 0;\n\t\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned))\n\t\t\t\t++val;\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_SG_TABLESIZE:\n\t\treturn put_user(sdp->sg_tablesize, ip);\n\tcase SG_SET_RESERVED_SIZE:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n                if (val < 0)\n                        return -EINVAL;\n\t\tval = min_t(int, val,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\tmutex_lock(&sfp->f_mutex);\n\t\tif (val != sfp->reserve.bufflen) {\n\t\t\tif (sfp->mmap_called ||\n\t\t\t    sfp->res_in_use) {\n\t\t\t\tmutex_unlock(&sfp->f_mutex);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t\t\tsg_build_reserve(sfp, val);\n\t\t}\n\t\tmutex_unlock(&sfp->f_mutex);\n\t\treturn 0;\n\tcase SG_GET_RESERVED_SIZE:\n\t\tval = min_t(int, sfp->reserve.bufflen,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\treturn put_user(val, ip);\n\tcase SG_SET_COMMAND_Q:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->cmd_q = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_COMMAND_Q:\n\t\treturn put_user((int) sfp->cmd_q, ip);\n\tcase SG_SET_KEEP_ORPHAN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->keep_orphan = val;\n\t\treturn 0;\n\tcase SG_GET_KEEP_ORPHAN:\n\t\treturn put_user((int) sfp->keep_orphan, ip);\n\tcase SG_NEXT_CMD_LEN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val > SG_MAX_CDB_SIZE)\n\t\t\treturn -ENOMEM;\n\t\tsfp->next_cmd_len = (val > 0) ? val : 0;\n\t\treturn 0;\n\tcase SG_GET_VERSION_NUM:\n\t\treturn put_user(sg_version_num, ip);\n\tcase SG_GET_ACCESS_COUNT:\n\t\t/* faked - we don't have a real access count anymore */\n\t\tval = (sdp->device ? 1 : 0);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_REQUEST_TABLE:\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_REQ_INFO * SG_MAX_QUEUE))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_req_info_t *rinfo;\n\n\t\t\trinfo = kzalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!rinfo)\n\t\t\t\treturn -ENOMEM;\n\t\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\t\tsg_fill_request_table(sfp, rinfo);\n\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t\tresult = __copy_to_user(p, rinfo,\n\t\t\t\t\t\tSZ_SG_REQ_INFO * SG_MAX_QUEUE);\n\t\t\tresult = result ? -EFAULT : 0;\n\t\t\tkfree(rinfo);\n\t\t\treturn result;\n\t\t}\n\tcase SG_EMULATED_HOST:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\treturn put_user(sdp->device->host->hostt->emulated, ip);\n\tcase SCSI_IOCTL_SEND_COMMAND:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (read_only) {\n\t\t\tunsigned char opcode = WRITE_6;\n\t\t\tScsi_Ioctl_Command __user *siocp = p;\n\n\t\t\tif (copy_from_user(&opcode, siocp->data, 1))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (sg_allow_access(filp, &opcode))\n\t\t\t\treturn -EPERM;\n\t\t}\n\t\treturn sg_scsi_ioctl(sdp->device->request_queue, NULL, filp->f_mode, p);\n\tcase SG_SET_DEBUG:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsdp->sgdebug = (char) val;\n\t\treturn 0;\n\tcase BLKSECTGET:\n\t\treturn put_user(max_sectors_bytes(sdp->device->request_queue),\n\t\t\t\tip);\n\tcase BLKTRACESETUP:\n\t\treturn blk_trace_setup(sdp->device->request_queue,\n\t\t\t\t       sdp->disk->disk_name,\n\t\t\t\t       MKDEV(SCSI_GENERIC_MAJOR, sdp->index),\n\t\t\t\t       NULL, p);\n\tcase BLKTRACESTART:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 1);\n\tcase BLKTRACESTOP:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 0);\n\tcase BLKTRACETEARDOWN:\n\t\treturn blk_trace_remove(sdp->device->request_queue);\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SCSI_IOCTL_PROBE_HOST:\n\tcase SG_GET_TRANSFORM:\n\tcase SG_SCSI_RESET:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tbreak;\n\tdefault:\n\t\tif (read_only)\n\t\t\treturn -EPERM;\t/* don't know so take safe approach */\n\t\tbreak;\n\t}\n\n\tresult = scsi_ioctl_block_when_processing_errors(sdp->device,\n\t\t\tcmd_in, filp->f_flags & O_NDELAY);\n\tif (result)\n\t\treturn result;\n\treturn scsi_ioctl(sdp->device, cmd_in, p);\n}\n\n#ifdef CONFIG_COMPAT\nstatic long sg_compat_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tstruct scsi_device *sdev;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tsdev = sdp->device;\n\tif (sdev->host->hostt->compat_ioctl) { \n\t\tint ret;\n\n\t\tret = sdev->host->hostt->compat_ioctl(sdev, cmd_in, (void __user *)arg);\n\n\t\treturn ret;\n\t}\n\t\n\treturn -ENOIOCTLCMD;\n}\n#endif\n\nstatic __poll_t\nsg_poll(struct file *filp, poll_table * wait)\n{\n\t__poll_t res = 0;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tint count = 0;\n\tunsigned long iflags;\n\n\tsfp = filp->private_data;\n\tif (!sfp)\n\t\treturn EPOLLERR;\n\tsdp = sfp->parentdp;\n\tif (!sdp)\n\t\treturn EPOLLERR;\n\tpoll_wait(filp, &sfp->read_wait, wait);\n\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t/* if any read waiting, flag it */\n\t\tif ((0 == res) && (1 == srp->done) && (!srp->sg_io_owned))\n\t\t\tres = EPOLLIN | EPOLLRDNORM;\n\t\t++count;\n\t}\n\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (atomic_read(&sdp->detaching))\n\t\tres |= EPOLLHUP;\n\telse if (!sfp->cmd_q) {\n\t\tif (0 == count)\n\t\t\tres |= EPOLLOUT | EPOLLWRNORM;\n\t} else if (count < SG_MAX_QUEUE)\n\t\tres |= EPOLLOUT | EPOLLWRNORM;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_poll: res=0x%x\\n\", (__force u32) res));\n\treturn res;\n}\n\nstatic int\nsg_fasync(int fd, struct file *filp, int mode)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_fasync: mode=%d\\n\", mode));\n\n\treturn fasync_helper(fd, filp, mode, &sfp->async_qp);\n}\n\nstatic int\nsg_vma_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tSg_fd *sfp;\n\tunsigned long offset, len, sa;\n\tSg_scatter_hold *rsv_schp;\n\tint k, length;\n\n\tif ((NULL == vma) || (!(sfp = (Sg_fd *) vma->vm_private_data)))\n\t\treturn VM_FAULT_SIGBUS;\n\trsv_schp = &sfp->reserve;\n\toffset = vmf->pgoff << PAGE_SHIFT;\n\tif (offset >= rsv_schp->bufflen)\n\t\treturn VM_FAULT_SIGBUS;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_vma_fault: offset=%lu, scatg=%d\\n\",\n\t\t\t\t      offset, rsv_schp->k_use_sg));\n\tsa = vma->vm_start;\n\tlength = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg && sa < vma->vm_end; k++) {\n\t\tlen = vma->vm_end - sa;\n\t\tlen = (len < length) ? len : length;\n\t\tif (offset < len) {\n\t\t\tstruct page *page = nth_page(rsv_schp->pages[k],\n\t\t\t\t\t\t     offset >> PAGE_SHIFT);\n\t\t\tget_page(page);\t/* increment page count */\n\t\t\tvmf->page = page;\n\t\t\treturn 0; /* success */\n\t\t}\n\t\tsa += len;\n\t\toffset -= len;\n\t}\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic const struct vm_operations_struct sg_mmap_vm_ops = {\n\t.fault = sg_vma_fault,\n};\n\nstatic int\nsg_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tSg_fd *sfp;\n\tunsigned long req_sz, len, sa;\n\tSg_scatter_hold *rsv_schp;\n\tint k, length;\n\tint ret = 0;\n\n\tif ((!filp) || (!vma) || (!(sfp = (Sg_fd *) filp->private_data)))\n\t\treturn -ENXIO;\n\treq_sz = vma->vm_end - vma->vm_start;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_mmap starting, vm_start=%p, len=%d\\n\",\n\t\t\t\t      (void *) vma->vm_start, (int) req_sz));\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\t/* want no offset */\n\trsv_schp = &sfp->reserve;\n\tmutex_lock(&sfp->f_mutex);\n\tif (req_sz > rsv_schp->bufflen) {\n\t\tret = -ENOMEM;\t/* cannot map more than reserved buffer */\n\t\tgoto out;\n\t}\n\n\tsa = vma->vm_start;\n\tlength = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg && sa < vma->vm_end; k++) {\n\t\tlen = vma->vm_end - sa;\n\t\tlen = (len < length) ? len : length;\n\t\tsa += len;\n\t}\n\n\tsfp->mmap_called = 1;\n\tvma->vm_flags |= VM_IO | VM_DONTEXPAND | VM_DONTDUMP;\n\tvma->vm_private_data = sfp;\n\tvma->vm_ops = &sg_mmap_vm_ops;\nout:\n\tmutex_unlock(&sfp->f_mutex);\n\treturn ret;\n}\n\nstatic void\nsg_rq_end_io_usercontext(struct work_struct *work)\n{\n\tstruct sg_request *srp = container_of(work, struct sg_request, ew.work);\n\tstruct sg_fd *sfp = srp->parentfp;\n\n\tsg_finish_rem_req(srp);\n\tsg_remove_request(sfp, srp);\n\tkref_put(&sfp->f_ref, sg_remove_sfp);\n}\n\n/*\n * This function is a \"bottom half\" handler that is called by the mid\n * level when a command is completed (or has failed).\n */\nstatic void\nsg_rq_end_io(struct request *rq, blk_status_t status)\n{\n\tstruct sg_request *srp = rq->end_io_data;\n\tstruct scsi_request *req = scsi_req(rq);\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tunsigned long iflags;\n\tunsigned int ms;\n\tchar *sense;\n\tint result, resid, done = 1;\n\n\tif (WARN_ON(srp->done != 0))\n\t\treturn;\n\n\tsfp = srp->parentfp;\n\tif (WARN_ON(sfp == NULL))\n\t\treturn;\n\n\tsdp = sfp->parentdp;\n\tif (unlikely(atomic_read(&sdp->detaching)))\n\t\tpr_info(\"%s: device detaching\\n\", __func__);\n\n\tsense = req->sense;\n\tresult = req->result;\n\tresid = req->resid_len;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_cmd_done: pack_id=%d, res=0x%x\\n\",\n\t\t\t\t      srp->header.pack_id, result));\n\tsrp->header.resid = resid;\n\tms = jiffies_to_msecs(jiffies);\n\tsrp->header.duration = (ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\tif (0 != result) {\n\t\tstruct scsi_sense_hdr sshdr;\n\n\t\tsrp->header.status = 0xff & result;\n\t\tsrp->header.masked_status = status_byte(result);\n\t\tsrp->header.msg_status = msg_byte(result);\n\t\tsrp->header.host_status = host_byte(result);\n\t\tsrp->header.driver_status = driver_byte(result);\n\t\tif ((sdp->sgdebug > 0) &&\n\t\t    ((CHECK_CONDITION == srp->header.masked_status) ||\n\t\t     (COMMAND_TERMINATED == srp->header.masked_status)))\n\t\t\t__scsi_print_sense(sdp->device, __func__, sense,\n\t\t\t\t\t   SCSI_SENSE_BUFFERSIZE);\n\n\t\t/* Following if statement is a patch supplied by Eric Youngdale */\n\t\tif (driver_byte(result) != 0\n\t\t    && scsi_normalize_sense(sense, SCSI_SENSE_BUFFERSIZE, &sshdr)\n\t\t    && !scsi_sense_is_deferred(&sshdr)\n\t\t    && sshdr.sense_key == UNIT_ATTENTION\n\t\t    && sdp->device->removable) {\n\t\t\t/* Detected possible disc change. Set the bit - this */\n\t\t\t/* may be used if there are filesystems using this device */\n\t\t\tsdp->device->changed = 1;\n\t\t}\n\t}\n\n\tif (req->sense_len)\n\t\tmemcpy(srp->sense_b, req->sense, SCSI_SENSE_BUFFERSIZE);\n\n\t/* Rely on write phase to clean out srp status values, so no \"else\" */\n\n\t/*\n\t * Free the request as soon as it is complete so that its resources\n\t * can be reused without waiting for userspace to read() the\n\t * result.  But keep the associated bio (if any) around until\n\t * blk_rq_unmap_user() can be called from user context.\n\t */\n\tsrp->rq = NULL;\n\tscsi_req_free_cmd(scsi_req(rq));\n\t__blk_put_request(rq->q, rq);\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tif (unlikely(srp->orphan)) {\n\t\tif (sfp->keep_orphan)\n\t\t\tsrp->sg_io_owned = 0;\n\t\telse\n\t\t\tdone = 0;\n\t}\n\tsrp->done = done;\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (likely(done)) {\n\t\t/* Now wake up any sg_read() that is waiting for this\n\t\t * packet.\n\t\t */\n\t\twake_up_interruptible(&sfp->read_wait);\n\t\tkill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);\n\t\tkref_put(&sfp->f_ref, sg_remove_sfp);\n\t} else {\n\t\tINIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);\n\t\tschedule_work(&srp->ew.work);\n\t}\n}\n\nstatic const struct file_operations sg_fops = {\n\t.owner = THIS_MODULE,\n\t.read = sg_read,\n\t.write = sg_write,\n\t.poll = sg_poll,\n\t.unlocked_ioctl = sg_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = sg_compat_ioctl,\n#endif\n\t.open = sg_open,\n\t.mmap = sg_mmap,\n\t.release = sg_release,\n\t.fasync = sg_fasync,\n\t.llseek = no_llseek,\n};\n\nstatic struct class *sg_sysfs_class;\n\nstatic int sg_sysfs_valid = 0;\n\nstatic Sg_device *\nsg_alloc(struct gendisk *disk, struct scsi_device *scsidp)\n{\n\tstruct request_queue *q = scsidp->request_queue;\n\tSg_device *sdp;\n\tunsigned long iflags;\n\tint error;\n\tu32 k;\n\n\tsdp = kzalloc(sizeof(Sg_device), GFP_KERNEL);\n\tif (!sdp) {\n\t\tsdev_printk(KERN_WARNING, scsidp, \"%s: kmalloc Sg_device \"\n\t\t\t    \"failure\\n\", __func__);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tidr_preload(GFP_KERNEL);\n\twrite_lock_irqsave(&sg_index_lock, iflags);\n\n\terror = idr_alloc(&sg_index_idr, sdp, 0, SG_MAX_DEVS, GFP_NOWAIT);\n\tif (error < 0) {\n\t\tif (error == -ENOSPC) {\n\t\t\tsdev_printk(KERN_WARNING, scsidp,\n\t\t\t\t    \"Unable to attach sg device type=%d, minor number exceeds %d\\n\",\n\t\t\t\t    scsidp->type, SG_MAX_DEVS - 1);\n\t\t\terror = -ENODEV;\n\t\t} else {\n\t\t\tsdev_printk(KERN_WARNING, scsidp, \"%s: idr \"\n\t\t\t\t    \"allocation Sg_device failure: %d\\n\",\n\t\t\t\t    __func__, error);\n\t\t}\n\t\tgoto out_unlock;\n\t}\n\tk = error;\n\n\tSCSI_LOG_TIMEOUT(3, sdev_printk(KERN_INFO, scsidp,\n\t\t\t\t\t\"sg_alloc: dev=%d \\n\", k));\n\tsprintf(disk->disk_name, \"sg%d\", k);\n\tdisk->first_minor = k;\n\tsdp->disk = disk;\n\tsdp->device = scsidp;\n\tmutex_init(&sdp->open_rel_lock);\n\tINIT_LIST_HEAD(&sdp->sfds);\n\tinit_waitqueue_head(&sdp->open_wait);\n\tatomic_set(&sdp->detaching, 0);\n\trwlock_init(&sdp->sfd_lock);\n\tsdp->sg_tablesize = queue_max_segments(q);\n\tsdp->index = k;\n\tkref_init(&sdp->d_ref);\n\terror = 0;\n\nout_unlock:\n\twrite_unlock_irqrestore(&sg_index_lock, iflags);\n\tidr_preload_end();\n\n\tif (error) {\n\t\tkfree(sdp);\n\t\treturn ERR_PTR(error);\n\t}\n\treturn sdp;\n}\n\nstatic int\nsg_add_device(struct device *cl_dev, struct class_interface *cl_intf)\n{\n\tstruct scsi_device *scsidp = to_scsi_device(cl_dev->parent);\n\tstruct gendisk *disk;\n\tSg_device *sdp = NULL;\n\tstruct cdev * cdev = NULL;\n\tint error;\n\tunsigned long iflags;\n\n\tdisk = alloc_disk(1);\n\tif (!disk) {\n\t\tpr_warn(\"%s: alloc_disk failed\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tdisk->major = SCSI_GENERIC_MAJOR;\n\n\terror = -ENOMEM;\n\tcdev = cdev_alloc();\n\tif (!cdev) {\n\t\tpr_warn(\"%s: cdev_alloc failed\\n\", __func__);\n\t\tgoto out;\n\t}\n\tcdev->owner = THIS_MODULE;\n\tcdev->ops = &sg_fops;\n\n\tsdp = sg_alloc(disk, scsidp);\n\tif (IS_ERR(sdp)) {\n\t\tpr_warn(\"%s: sg_alloc failed\\n\", __func__);\n\t\terror = PTR_ERR(sdp);\n\t\tgoto out;\n\t}\n\n\terror = cdev_add(cdev, MKDEV(SCSI_GENERIC_MAJOR, sdp->index), 1);\n\tif (error)\n\t\tgoto cdev_add_err;\n\n\tsdp->cdev = cdev;\n\tif (sg_sysfs_valid) {\n\t\tstruct device *sg_class_member;\n\n\t\tsg_class_member = device_create(sg_sysfs_class, cl_dev->parent,\n\t\t\t\t\t\tMKDEV(SCSI_GENERIC_MAJOR,\n\t\t\t\t\t\t      sdp->index),\n\t\t\t\t\t\tsdp, \"%s\", disk->disk_name);\n\t\tif (IS_ERR(sg_class_member)) {\n\t\t\tpr_err(\"%s: device_create failed\\n\", __func__);\n\t\t\terror = PTR_ERR(sg_class_member);\n\t\t\tgoto cdev_add_err;\n\t\t}\n\t\terror = sysfs_create_link(&scsidp->sdev_gendev.kobj,\n\t\t\t\t\t  &sg_class_member->kobj, \"generic\");\n\t\tif (error)\n\t\t\tpr_err(\"%s: unable to make symlink 'generic' back \"\n\t\t\t       \"to sg%d\\n\", __func__, sdp->index);\n\t} else\n\t\tpr_warn(\"%s: sg_sys Invalid\\n\", __func__);\n\n\tsdev_printk(KERN_NOTICE, scsidp, \"Attached scsi generic sg%d \"\n\t\t    \"type %d\\n\", sdp->index, scsidp->type);\n\n\tdev_set_drvdata(cl_dev, sdp);\n\n\treturn 0;\n\ncdev_add_err:\n\twrite_lock_irqsave(&sg_index_lock, iflags);\n\tidr_remove(&sg_index_idr, sdp->index);\n\twrite_unlock_irqrestore(&sg_index_lock, iflags);\n\tkfree(sdp);\n\nout:\n\tput_disk(disk);\n\tif (cdev)\n\t\tcdev_del(cdev);\n\treturn error;\n}\n\nstatic void\nsg_device_destroy(struct kref *kref)\n{\n\tstruct sg_device *sdp = container_of(kref, struct sg_device, d_ref);\n\tunsigned long flags;\n\n\t/* CAUTION!  Note that the device can still be found via idr_find()\n\t * even though the refcount is 0.  Therefore, do idr_remove() BEFORE\n\t * any other cleanup.\n\t */\n\n\twrite_lock_irqsave(&sg_index_lock, flags);\n\tidr_remove(&sg_index_idr, sdp->index);\n\twrite_unlock_irqrestore(&sg_index_lock, flags);\n\n\tSCSI_LOG_TIMEOUT(3,\n\t\tsg_printk(KERN_INFO, sdp, \"sg_device_destroy\\n\"));\n\n\tput_disk(sdp->disk);\n\tkfree(sdp);\n}\n\nstatic void\nsg_remove_device(struct device *cl_dev, struct class_interface *cl_intf)\n{\n\tstruct scsi_device *scsidp = to_scsi_device(cl_dev->parent);\n\tSg_device *sdp = dev_get_drvdata(cl_dev);\n\tunsigned long iflags;\n\tSg_fd *sfp;\n\tint val;\n\n\tif (!sdp)\n\t\treturn;\n\t/* want sdp->detaching non-zero as soon as possible */\n\tval = atomic_inc_return(&sdp->detaching);\n\tif (val > 1)\n\t\treturn; /* only want to do following once per device */\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"%s\\n\", __func__));\n\n\tread_lock_irqsave(&sdp->sfd_lock, iflags);\n\tlist_for_each_entry(sfp, &sdp->sfds, sfd_siblings) {\n\t\twake_up_interruptible_all(&sfp->read_wait);\n\t\tkill_fasync(&sfp->async_qp, SIGPOLL, POLL_HUP);\n\t}\n\twake_up_interruptible_all(&sdp->open_wait);\n\tread_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\n\tsysfs_remove_link(&scsidp->sdev_gendev.kobj, \"generic\");\n\tdevice_destroy(sg_sysfs_class, MKDEV(SCSI_GENERIC_MAJOR, sdp->index));\n\tcdev_del(sdp->cdev);\n\tsdp->cdev = NULL;\n\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n}\n\nmodule_param_named(scatter_elem_sz, scatter_elem_sz, int, S_IRUGO | S_IWUSR);\nmodule_param_named(def_reserved_size, def_reserved_size, int,\n\t\t   S_IRUGO | S_IWUSR);\nmodule_param_named(allow_dio, sg_allow_dio, int, S_IRUGO | S_IWUSR);\n\nMODULE_AUTHOR(\"Douglas Gilbert\");\nMODULE_DESCRIPTION(\"SCSI generic (sg) driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(SG_VERSION_STR);\nMODULE_ALIAS_CHARDEV_MAJOR(SCSI_GENERIC_MAJOR);\n\nMODULE_PARM_DESC(scatter_elem_sz, \"scatter gather element \"\n                \"size (default: max(SG_SCATTER_SZ, PAGE_SIZE))\");\nMODULE_PARM_DESC(def_reserved_size, \"size of buffer reserved for each fd\");\nMODULE_PARM_DESC(allow_dio, \"allow direct I/O (default: 0 (disallow))\");\n\nstatic int __init\ninit_sg(void)\n{\n\tint rc;\n\n\tif (scatter_elem_sz < PAGE_SIZE) {\n\t\tscatter_elem_sz = PAGE_SIZE;\n\t\tscatter_elem_sz_prev = scatter_elem_sz;\n\t}\n\tif (def_reserved_size >= 0)\n\t\tsg_big_buff = def_reserved_size;\n\telse\n\t\tdef_reserved_size = sg_big_buff;\n\n\trc = register_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0), \n\t\t\t\t    SG_MAX_DEVS, \"sg\");\n\tif (rc)\n\t\treturn rc;\n        sg_sysfs_class = class_create(THIS_MODULE, \"scsi_generic\");\n        if ( IS_ERR(sg_sysfs_class) ) {\n\t\trc = PTR_ERR(sg_sysfs_class);\n\t\tgoto err_out;\n        }\n\tsg_sysfs_valid = 1;\n\trc = scsi_register_interface(&sg_interface);\n\tif (0 == rc) {\n#ifdef CONFIG_SCSI_PROC_FS\n\t\tsg_proc_init();\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\t\treturn 0;\n\t}\n\tclass_destroy(sg_sysfs_class);\nerr_out:\n\tunregister_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0), SG_MAX_DEVS);\n\treturn rc;\n}\n\nstatic void __exit\nexit_sg(void)\n{\n#ifdef CONFIG_SCSI_PROC_FS\n\tsg_proc_cleanup();\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\tscsi_unregister_interface(&sg_interface);\n\tclass_destroy(sg_sysfs_class);\n\tsg_sysfs_valid = 0;\n\tunregister_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0),\n\t\t\t\t SG_MAX_DEVS);\n\tidr_destroy(&sg_index_idr);\n}\n\nstatic int\nsg_start_req(Sg_request *srp, unsigned char *cmd)\n{\n\tint res;\n\tstruct request *rq;\n\tstruct scsi_request *req;\n\tSg_fd *sfp = srp->parentfp;\n\tsg_io_hdr_t *hp = &srp->header;\n\tint dxfer_len = (int) hp->dxfer_len;\n\tint dxfer_dir = hp->dxfer_direction;\n\tunsigned int iov_count = hp->iovec_count;\n\tSg_scatter_hold *req_schp = &srp->data;\n\tSg_scatter_hold *rsv_schp = &sfp->reserve;\n\tstruct request_queue *q = sfp->parentdp->device->request_queue;\n\tstruct rq_map_data *md, map_data;\n\tint rw = hp->dxfer_direction == SG_DXFER_TO_DEV ? WRITE : READ;\n\tunsigned char *long_cmdp = NULL;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_start_req: dxfer_len=%d\\n\",\n\t\t\t\t      dxfer_len));\n\n\tif (hp->cmd_len > BLK_MAX_CDB) {\n\t\tlong_cmdp = kzalloc(hp->cmd_len, GFP_KERNEL);\n\t\tif (!long_cmdp)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * NOTE\n\t *\n\t * With scsi-mq enabled, there are a fixed number of preallocated\n\t * requests equal in number to shost->can_queue.  If all of the\n\t * preallocated requests are already in use, then using GFP_ATOMIC with\n\t * blk_get_request() will return -EWOULDBLOCK, whereas using GFP_KERNEL\n\t * will cause blk_get_request() to sleep until an active command\n\t * completes, freeing up a request.  Neither option is ideal, but\n\t * GFP_KERNEL is the better choice to prevent userspace from getting an\n\t * unexpected EWOULDBLOCK.\n\t *\n\t * With scsi-mq disabled, blk_get_request() with GFP_KERNEL usually\n\t * does not sleep except under memory pressure.\n\t */\n\trq = blk_get_request(q, hp->dxfer_direction == SG_DXFER_TO_DEV ?\n\t\t\tREQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, GFP_KERNEL);\n\tif (IS_ERR(rq)) {\n\t\tkfree(long_cmdp);\n\t\treturn PTR_ERR(rq);\n\t}\n\treq = scsi_req(rq);\n\n\tif (hp->cmd_len > BLK_MAX_CDB)\n\t\treq->cmd = long_cmdp;\n\tmemcpy(req->cmd, cmd, hp->cmd_len);\n\treq->cmd_len = hp->cmd_len;\n\n\tsrp->rq = rq;\n\trq->end_io_data = srp;\n\treq->retries = SG_DEFAULT_RETRIES;\n\n\tif ((dxfer_len <= 0) || (dxfer_dir == SG_DXFER_NONE))\n\t\treturn 0;\n\n\tif (sg_allow_dio && hp->flags & SG_FLAG_DIRECT_IO &&\n\t    dxfer_dir != SG_DXFER_UNKNOWN && !iov_count &&\n\t    !sfp->parentdp->device->host->unchecked_isa_dma &&\n\t    blk_rq_aligned(q, (unsigned long)hp->dxferp, dxfer_len))\n\t\tmd = NULL;\n\telse\n\t\tmd = &map_data;\n\n\tif (md) {\n\t\tmutex_lock(&sfp->f_mutex);\n\t\tif (dxfer_len <= rsv_schp->bufflen &&\n\t\t    !sfp->res_in_use) {\n\t\t\tsfp->res_in_use = 1;\n\t\t\tsg_link_reserve(sfp, srp, dxfer_len);\n\t\t} else if (hp->flags & SG_FLAG_MMAP_IO) {\n\t\t\tres = -EBUSY; /* sfp->res_in_use == 1 */\n\t\t\tif (dxfer_len > rsv_schp->bufflen)\n\t\t\t\tres = -ENOMEM;\n\t\t\tmutex_unlock(&sfp->f_mutex);\n\t\t\treturn res;\n\t\t} else {\n\t\t\tres = sg_build_indirect(req_schp, sfp, dxfer_len);\n\t\t\tif (res) {\n\t\t\t\tmutex_unlock(&sfp->f_mutex);\n\t\t\t\treturn res;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&sfp->f_mutex);\n\n\t\tmd->pages = req_schp->pages;\n\t\tmd->page_order = req_schp->page_order;\n\t\tmd->nr_entries = req_schp->k_use_sg;\n\t\tmd->offset = 0;\n\t\tmd->null_mapped = hp->dxferp ? 0 : 1;\n\t\tif (dxfer_dir == SG_DXFER_TO_FROM_DEV)\n\t\t\tmd->from_user = 1;\n\t\telse\n\t\t\tmd->from_user = 0;\n\t}\n\n\tif (iov_count) {\n\t\tstruct iovec *iov = NULL;\n\t\tstruct iov_iter i;\n\n\t\tres = import_iovec(rw, hp->dxferp, iov_count, 0, &iov, &i);\n\t\tif (res < 0)\n\t\t\treturn res;\n\n\t\tiov_iter_truncate(&i, hp->dxfer_len);\n\t\tif (!iov_iter_count(&i)) {\n\t\t\tkfree(iov);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tres = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);\n\t\tkfree(iov);\n\t} else\n\t\tres = blk_rq_map_user(q, rq, md, hp->dxferp,\n\t\t\t\t      hp->dxfer_len, GFP_ATOMIC);\n\n\tif (!res) {\n\t\tsrp->bio = rq->bio;\n\n\t\tif (!md) {\n\t\t\treq_schp->dio_in_use = 1;\n\t\t\thp->info |= SG_INFO_DIRECT_IO;\n\t\t}\n\t}\n\treturn res;\n}\n\nstatic int\nsg_finish_rem_req(Sg_request *srp)\n{\n\tint ret = 0;\n\n\tSg_fd *sfp = srp->parentfp;\n\tSg_scatter_hold *req_schp = &srp->data;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_finish_rem_req: res_used=%d\\n\",\n\t\t\t\t      (int) srp->res_used));\n\tif (srp->bio)\n\t\tret = blk_rq_unmap_user(srp->bio);\n\n\tif (srp->rq) {\n\t\tscsi_req_free_cmd(scsi_req(srp->rq));\n\t\tblk_put_request(srp->rq);\n\t}\n\n\tif (srp->res_used)\n\t\tsg_unlink_reserve(sfp, srp);\n\telse\n\t\tsg_remove_scat(sfp, req_schp);\n\n\treturn ret;\n}\n\nstatic int\nsg_build_sgat(Sg_scatter_hold * schp, const Sg_fd * sfp, int tablesize)\n{\n\tint sg_bufflen = tablesize * sizeof(struct page *);\n\tgfp_t gfp_flags = GFP_ATOMIC | __GFP_NOWARN;\n\n\tschp->pages = kzalloc(sg_bufflen, gfp_flags);\n\tif (!schp->pages)\n\t\treturn -ENOMEM;\n\tschp->sglist_len = sg_bufflen;\n\treturn tablesize;\t/* number of scat_gath elements allocated */\n}\n\nstatic int\nsg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size)\n{\n\tint ret_sz = 0, i, k, rem_sz, num, mx_sc_elems;\n\tint sg_tablesize = sfp->parentdp->sg_tablesize;\n\tint blk_size = buff_size, order;\n\tgfp_t gfp_mask = GFP_ATOMIC | __GFP_COMP | __GFP_NOWARN;\n\tstruct sg_device *sdp = sfp->parentdp;\n\n\tif (blk_size < 0)\n\t\treturn -EFAULT;\n\tif (0 == blk_size)\n\t\t++blk_size;\t/* don't know why */\n\t/* round request up to next highest SG_SECTOR_SZ byte boundary */\n\tblk_size = ALIGN(blk_size, SG_SECTOR_SZ);\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\"sg_build_indirect: buff_size=%d, blk_size=%d\\n\",\n\t\tbuff_size, blk_size));\n\n\t/* N.B. ret_sz carried into this block ... */\n\tmx_sc_elems = sg_build_sgat(schp, sfp, sg_tablesize);\n\tif (mx_sc_elems < 0)\n\t\treturn mx_sc_elems;\t/* most likely -ENOMEM */\n\n\tnum = scatter_elem_sz;\n\tif (unlikely(num != scatter_elem_sz_prev)) {\n\t\tif (num < PAGE_SIZE) {\n\t\t\tscatter_elem_sz = PAGE_SIZE;\n\t\t\tscatter_elem_sz_prev = PAGE_SIZE;\n\t\t} else\n\t\t\tscatter_elem_sz_prev = num;\n\t}\n\n\tif (sdp->device->host->unchecked_isa_dma)\n\t\tgfp_mask |= GFP_DMA;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\tgfp_mask |= __GFP_ZERO;\n\n\torder = get_order(num);\nretry:\n\tret_sz = 1 << (PAGE_SHIFT + order);\n\n\tfor (k = 0, rem_sz = blk_size; rem_sz > 0 && k < mx_sc_elems;\n\t     k++, rem_sz -= ret_sz) {\n\n\t\tnum = (rem_sz > scatter_elem_sz_prev) ?\n\t\t\tscatter_elem_sz_prev : rem_sz;\n\n\t\tschp->pages[k] = alloc_pages(gfp_mask, order);\n\t\tif (!schp->pages[k])\n\t\t\tgoto out;\n\n\t\tif (num == scatter_elem_sz_prev) {\n\t\t\tif (unlikely(ret_sz > scatter_elem_sz_prev)) {\n\t\t\t\tscatter_elem_sz = ret_sz;\n\t\t\t\tscatter_elem_sz_prev = ret_sz;\n\t\t\t}\n\t\t}\n\n\t\tSCSI_LOG_TIMEOUT(5, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t \"sg_build_indirect: k=%d, num=%d, ret_sz=%d\\n\",\n\t\t\t\t k, num, ret_sz));\n\t}\t\t/* end of for loop */\n\n\tschp->page_order = order;\n\tschp->k_use_sg = k;\n\tSCSI_LOG_TIMEOUT(5, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_build_indirect: k_use_sg=%d, rem_sz=%d\\n\",\n\t\t\t k, rem_sz));\n\n\tschp->bufflen = blk_size;\n\tif (rem_sz > 0)\t/* must have failed */\n\t\treturn -ENOMEM;\n\treturn 0;\nout:\n\tfor (i = 0; i < k; i++)\n\t\t__free_pages(schp->pages[i], order);\n\n\tif (--order >= 0)\n\t\tgoto retry;\n\n\treturn -ENOMEM;\n}\n\nstatic void\nsg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp)\n{\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_remove_scat: k_use_sg=%d\\n\", schp->k_use_sg));\n\tif (schp->pages && schp->sglist_len > 0) {\n\t\tif (!schp->dio_in_use) {\n\t\t\tint k;\n\n\t\t\tfor (k = 0; k < schp->k_use_sg && schp->pages[k]; k++) {\n\t\t\t\tSCSI_LOG_TIMEOUT(5,\n\t\t\t\t\tsg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t\t\"sg_remove_scat: k=%d, pg=0x%p\\n\",\n\t\t\t\t\tk, schp->pages[k]));\n\t\t\t\t__free_pages(schp->pages[k], schp->page_order);\n\t\t\t}\n\n\t\t\tkfree(schp->pages);\n\t\t}\n\t}\n\tmemset(schp, 0, sizeof (*schp));\n}\n\nstatic int\nsg_read_oxfer(Sg_request * srp, char __user *outp, int num_read_xfer)\n{\n\tSg_scatter_hold *schp = &srp->data;\n\tint k, num;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, srp->parentfp->parentdp,\n\t\t\t \"sg_read_oxfer: num_read_xfer=%d\\n\",\n\t\t\t num_read_xfer));\n\tif ((!outp) || (num_read_xfer <= 0))\n\t\treturn 0;\n\n\tnum = 1 << (PAGE_SHIFT + schp->page_order);\n\tfor (k = 0; k < schp->k_use_sg && schp->pages[k]; k++) {\n\t\tif (num > num_read_xfer) {\n\t\t\tif (__copy_to_user(outp, page_address(schp->pages[k]),\n\t\t\t\t\t   num_read_xfer))\n\t\t\t\treturn -EFAULT;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tif (__copy_to_user(outp, page_address(schp->pages[k]),\n\t\t\t\t\t   num))\n\t\t\t\treturn -EFAULT;\n\t\t\tnum_read_xfer -= num;\n\t\t\tif (num_read_xfer <= 0)\n\t\t\t\tbreak;\n\t\t\toutp += num;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nsg_build_reserve(Sg_fd * sfp, int req_size)\n{\n\tSg_scatter_hold *schp = &sfp->reserve;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_build_reserve: req_size=%d\\n\", req_size));\n\tdo {\n\t\tif (req_size < PAGE_SIZE)\n\t\t\treq_size = PAGE_SIZE;\n\t\tif (0 == sg_build_indirect(schp, sfp, req_size))\n\t\t\treturn;\n\t\telse\n\t\t\tsg_remove_scat(sfp, schp);\n\t\treq_size >>= 1;\t/* divide by 2 */\n\t} while (req_size > (PAGE_SIZE / 2));\n}\n\nstatic void\nsg_link_reserve(Sg_fd * sfp, Sg_request * srp, int size)\n{\n\tSg_scatter_hold *req_schp = &srp->data;\n\tSg_scatter_hold *rsv_schp = &sfp->reserve;\n\tint k, num, rem;\n\n\tsrp->res_used = 1;\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_link_reserve: size=%d\\n\", size));\n\trem = size;\n\n\tnum = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg; k++) {\n\t\tif (rem <= num) {\n\t\t\treq_schp->k_use_sg = k + 1;\n\t\t\treq_schp->sglist_len = rsv_schp->sglist_len;\n\t\t\treq_schp->pages = rsv_schp->pages;\n\n\t\t\treq_schp->bufflen = size;\n\t\t\treq_schp->page_order = rsv_schp->page_order;\n\t\t\tbreak;\n\t\t} else\n\t\t\trem -= num;\n\t}\n\n\tif (k >= rsv_schp->k_use_sg)\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t \"sg_link_reserve: BAD size\\n\"));\n}\n\nstatic void\nsg_unlink_reserve(Sg_fd * sfp, Sg_request * srp)\n{\n\tSg_scatter_hold *req_schp = &srp->data;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, srp->parentfp->parentdp,\n\t\t\t\t      \"sg_unlink_reserve: req->k_use_sg=%d\\n\",\n\t\t\t\t      (int) req_schp->k_use_sg));\n\treq_schp->k_use_sg = 0;\n\treq_schp->bufflen = 0;\n\treq_schp->pages = NULL;\n\treq_schp->page_order = 0;\n\treq_schp->sglist_len = 0;\n\tsrp->res_used = 0;\n\t/* Called without mutex lock to avoid deadlock */\n\tsfp->res_in_use = 0;\n}\n\nstatic Sg_request *\nsg_get_rq_mark(Sg_fd * sfp, int pack_id)\n{\n\tSg_request *resp;\n\tunsigned long iflags;\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tlist_for_each_entry(resp, &sfp->rq_list, entry) {\n\t\t/* look for requests that are ready + not SG_IO owned */\n\t\tif ((1 == resp->done) && (!resp->sg_io_owned) &&\n\t\t    ((-1 == pack_id) || (resp->header.pack_id == pack_id))) {\n\t\t\tresp->done = 2;\t/* guard against other readers */\n\t\t\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t\treturn resp;\n\t\t}\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn NULL;\n}\n\n/* always adds to end of list */\nstatic Sg_request *\nsg_add_request(Sg_fd * sfp)\n{\n\tint k;\n\tunsigned long iflags;\n\tSg_request *rp = sfp->req_arr;\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tif (!list_empty(&sfp->rq_list)) {\n\t\tif (!sfp->cmd_q)\n\t\t\tgoto out_unlock;\n\n\t\tfor (k = 0; k < SG_MAX_QUEUE; ++k, ++rp) {\n\t\t\tif (!rp->parentfp)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (k >= SG_MAX_QUEUE)\n\t\t\tgoto out_unlock;\n\t}\n\tmemset(rp, 0, sizeof (Sg_request));\n\trp->parentfp = sfp;\n\trp->header.duration = jiffies_to_msecs(jiffies);\n\tlist_add_tail(&rp->entry, &sfp->rq_list);\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn rp;\nout_unlock:\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn NULL;\n}\n\n/* Return of 1 for found; 0 for not found */\nstatic int\nsg_remove_request(Sg_fd * sfp, Sg_request * srp)\n{\n\tunsigned long iflags;\n\tint res = 0;\n\n\tif (!sfp || !srp || list_empty(&sfp->rq_list))\n\t\treturn res;\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tif (!list_empty(&srp->entry)) {\n\t\tlist_del(&srp->entry);\n\t\tsrp->parentfp = NULL;\n\t\tres = 1;\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn res;\n}\n\nstatic Sg_fd *\nsg_add_sfp(Sg_device * sdp)\n{\n\tSg_fd *sfp;\n\tunsigned long iflags;\n\tint bufflen;\n\n\tsfp = kzalloc(sizeof(*sfp), GFP_ATOMIC | __GFP_NOWARN);\n\tif (!sfp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinit_waitqueue_head(&sfp->read_wait);\n\trwlock_init(&sfp->rq_list_lock);\n\tINIT_LIST_HEAD(&sfp->rq_list);\n\tkref_init(&sfp->f_ref);\n\tmutex_init(&sfp->f_mutex);\n\tsfp->timeout = SG_DEFAULT_TIMEOUT;\n\tsfp->timeout_user = SG_DEFAULT_TIMEOUT_USER;\n\tsfp->force_packid = SG_DEF_FORCE_PACK_ID;\n\tsfp->cmd_q = SG_DEF_COMMAND_Q;\n\tsfp->keep_orphan = SG_DEF_KEEP_ORPHAN;\n\tsfp->parentdp = sdp;\n\twrite_lock_irqsave(&sdp->sfd_lock, iflags);\n\tif (atomic_read(&sdp->detaching)) {\n\t\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\tlist_add_tail(&sfp->sfd_siblings, &sdp->sfds);\n\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_add_sfp: sfp=0x%p\\n\", sfp));\n\tif (unlikely(sg_big_buff != def_reserved_size))\n\t\tsg_big_buff = def_reserved_size;\n\n\tbufflen = min_t(int, sg_big_buff,\n\t\t\tmax_sectors_bytes(sdp->device->request_queue));\n\tsg_build_reserve(sfp, bufflen);\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_add_sfp: bufflen=%d, k_use_sg=%d\\n\",\n\t\t\t\t      sfp->reserve.bufflen,\n\t\t\t\t      sfp->reserve.k_use_sg));\n\n\tkref_get(&sdp->d_ref);\n\t__module_get(THIS_MODULE);\n\treturn sfp;\n}\n\nstatic void\nsg_remove_sfp_usercontext(struct work_struct *work)\n{\n\tstruct sg_fd *sfp = container_of(work, struct sg_fd, ew.work);\n\tstruct sg_device *sdp = sfp->parentdp;\n\tSg_request *srp;\n\tunsigned long iflags;\n\n\t/* Cleanup any responses which were never read(). */\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\twhile (!list_empty(&sfp->rq_list)) {\n\t\tsrp = list_first_entry(&sfp->rq_list, Sg_request, entry);\n\t\tsg_finish_rem_req(srp);\n\t\tlist_del(&srp->entry);\n\t\tsrp->parentfp = NULL;\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (sfp->reserve.bufflen > 0) {\n\t\tSCSI_LOG_TIMEOUT(6, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\"sg_remove_sfp:    bufflen=%d, k_use_sg=%d\\n\",\n\t\t\t\t(int) sfp->reserve.bufflen,\n\t\t\t\t(int) sfp->reserve.k_use_sg));\n\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t}\n\n\tSCSI_LOG_TIMEOUT(6, sg_printk(KERN_INFO, sdp,\n\t\t\t\"sg_remove_sfp: sfp=0x%p\\n\", sfp));\n\tkfree(sfp);\n\n\tscsi_device_put(sdp->device);\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n\tmodule_put(THIS_MODULE);\n}\n\nstatic void\nsg_remove_sfp(struct kref *kref)\n{\n\tstruct sg_fd *sfp = container_of(kref, struct sg_fd, f_ref);\n\tstruct sg_device *sdp = sfp->parentdp;\n\tunsigned long iflags;\n\n\twrite_lock_irqsave(&sdp->sfd_lock, iflags);\n\tlist_del(&sfp->sfd_siblings);\n\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\n\tINIT_WORK(&sfp->ew.work, sg_remove_sfp_usercontext);\n\tschedule_work(&sfp->ew.work);\n}\n\n#ifdef CONFIG_SCSI_PROC_FS\nstatic int\nsg_idr_max_id(int id, void *p, void *data)\n{\n\tint *k = data;\n\n\tif (*k < id)\n\t\t*k = id;\n\n\treturn 0;\n}\n\nstatic int\nsg_last_dev(void)\n{\n\tint k = -1;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tidr_for_each(&sg_index_idr, sg_idr_max_id, &k);\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn k + 1;\t\t/* origin 1 */\n}\n#endif\n\n/* must be called with sg_index_lock held */\nstatic Sg_device *sg_lookup_dev(int dev)\n{\n\treturn idr_find(&sg_index_idr, dev);\n}\n\nstatic Sg_device *\nsg_get_dev(int dev)\n{\n\tstruct sg_device *sdp;\n\tunsigned long flags;\n\n\tread_lock_irqsave(&sg_index_lock, flags);\n\tsdp = sg_lookup_dev(dev);\n\tif (!sdp)\n\t\tsdp = ERR_PTR(-ENXIO);\n\telse if (atomic_read(&sdp->detaching)) {\n\t\t/* If sdp->detaching, then the refcount may already be 0, in\n\t\t * which case it would be a bug to do kref_get().\n\t\t */\n\t\tsdp = ERR_PTR(-ENODEV);\n\t} else\n\t\tkref_get(&sdp->d_ref);\n\tread_unlock_irqrestore(&sg_index_lock, flags);\n\n\treturn sdp;\n}\n\n#ifdef CONFIG_SCSI_PROC_FS\n\nstatic struct proc_dir_entry *sg_proc_sgp = NULL;\n\nstatic char sg_proc_sg_dirname[] = \"scsi/sg\";\n\nstatic int sg_proc_seq_show_int(struct seq_file *s, void *v);\n\nstatic int sg_proc_single_open_adio(struct inode *inode, struct file *file);\nstatic ssize_t sg_proc_write_adio(struct file *filp, const char __user *buffer,\n\t\t\t          size_t count, loff_t *off);\nstatic const struct file_operations adio_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_adio,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.write = sg_proc_write_adio,\n\t.release = single_release,\n};\n\nstatic int sg_proc_single_open_dressz(struct inode *inode, struct file *file);\nstatic ssize_t sg_proc_write_dressz(struct file *filp, \n\t\tconst char __user *buffer, size_t count, loff_t *off);\nstatic const struct file_operations dressz_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_dressz,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.write = sg_proc_write_dressz,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_version(struct seq_file *s, void *v);\nstatic int sg_proc_single_open_version(struct inode *inode, struct file *file);\nstatic const struct file_operations version_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_version,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_devhdr(struct seq_file *s, void *v);\nstatic int sg_proc_single_open_devhdr(struct inode *inode, struct file *file);\nstatic const struct file_operations devhdr_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_devhdr,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_dev(struct seq_file *s, void *v);\nstatic int sg_proc_open_dev(struct inode *inode, struct file *file);\nstatic void * dev_seq_start(struct seq_file *s, loff_t *pos);\nstatic void * dev_seq_next(struct seq_file *s, void *v, loff_t *pos);\nstatic void dev_seq_stop(struct seq_file *s, void *v);\nstatic const struct file_operations dev_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_dev,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations dev_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_dev,\n};\n\nstatic int sg_proc_seq_show_devstrs(struct seq_file *s, void *v);\nstatic int sg_proc_open_devstrs(struct inode *inode, struct file *file);\nstatic const struct file_operations devstrs_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_devstrs,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations devstrs_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_devstrs,\n};\n\nstatic int sg_proc_seq_show_debug(struct seq_file *s, void *v);\nstatic int sg_proc_open_debug(struct inode *inode, struct file *file);\nstatic const struct file_operations debug_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_debug,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations debug_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_debug,\n};\n\n\nstruct sg_proc_leaf {\n\tconst char * name;\n\tconst struct file_operations * fops;\n};\n\nstatic const struct sg_proc_leaf sg_proc_leaf_arr[] = {\n\t{\"allow_dio\", &adio_fops},\n\t{\"debug\", &debug_fops},\n\t{\"def_reserved_size\", &dressz_fops},\n\t{\"device_hdr\", &devhdr_fops},\n\t{\"devices\", &dev_fops},\n\t{\"device_strs\", &devstrs_fops},\n\t{\"version\", &version_fops}\n};\n\nstatic int\nsg_proc_init(void)\n{\n\tint num_leaves = ARRAY_SIZE(sg_proc_leaf_arr);\n\tint k;\n\n\tsg_proc_sgp = proc_mkdir(sg_proc_sg_dirname, NULL);\n\tif (!sg_proc_sgp)\n\t\treturn 1;\n\tfor (k = 0; k < num_leaves; ++k) {\n\t\tconst struct sg_proc_leaf *leaf = &sg_proc_leaf_arr[k];\n\t\tumode_t mask = leaf->fops->write ? S_IRUGO | S_IWUSR : S_IRUGO;\n\t\tproc_create(leaf->name, mask, sg_proc_sgp, leaf->fops);\n\t}\n\treturn 0;\n}\n\nstatic void\nsg_proc_cleanup(void)\n{\n\tint k;\n\tint num_leaves = ARRAY_SIZE(sg_proc_leaf_arr);\n\n\tif (!sg_proc_sgp)\n\t\treturn;\n\tfor (k = 0; k < num_leaves; ++k)\n\t\tremove_proc_entry(sg_proc_leaf_arr[k].name, sg_proc_sgp);\n\tremove_proc_entry(sg_proc_sg_dirname, NULL);\n}\n\n\nstatic int sg_proc_seq_show_int(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", *((int *)s->private));\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_adio(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_int, &sg_allow_dio);\n}\n\nstatic ssize_t \nsg_proc_write_adio(struct file *filp, const char __user *buffer,\n\t\t   size_t count, loff_t *off)\n{\n\tint err;\n\tunsigned long num;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\terr = kstrtoul_from_user(buffer, count, 0, &num);\n\tif (err)\n\t\treturn err;\n\tsg_allow_dio = num ? 1 : 0;\n\treturn count;\n}\n\nstatic int sg_proc_single_open_dressz(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_int, &sg_big_buff);\n}\n\nstatic ssize_t \nsg_proc_write_dressz(struct file *filp, const char __user *buffer,\n\t\t     size_t count, loff_t *off)\n{\n\tint err;\n\tunsigned long k = ULONG_MAX;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\n\terr = kstrtoul_from_user(buffer, count, 0, &k);\n\tif (err)\n\t\treturn err;\n\tif (k <= 1048576) {\t/* limit \"big buff\" to 1 MB */\n\t\tsg_big_buff = k;\n\t\treturn count;\n\t}\n\treturn -ERANGE;\n}\n\nstatic int sg_proc_seq_show_version(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\t%s [%s]\\n\", sg_version_num, SG_VERSION_STR,\n\t\t   sg_version_date);\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_version(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_version, NULL);\n}\n\nstatic int sg_proc_seq_show_devhdr(struct seq_file *s, void *v)\n{\n\tseq_puts(s, \"host\\tchan\\tid\\tlun\\ttype\\topens\\tqdepth\\tbusy\\tonline\\n\");\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_devhdr(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_devhdr, NULL);\n}\n\nstruct sg_proc_deviter {\n\tloff_t\tindex;\n\tsize_t\tmax;\n};\n\nstatic void * dev_seq_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct sg_proc_deviter * it = kmalloc(sizeof(*it), GFP_KERNEL);\n\n\ts->private = it;\n\tif (! it)\n\t\treturn NULL;\n\n\tit->index = *pos;\n\tit->max = sg_last_dev();\n\tif (it->index >= it->max)\n\t\treturn NULL;\n\treturn it;\n}\n\nstatic void * dev_seq_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct sg_proc_deviter * it = s->private;\n\n\t*pos = ++it->index;\n\treturn (it->index < it->max) ? it : NULL;\n}\n\nstatic void dev_seq_stop(struct seq_file *s, void *v)\n{\n\tkfree(s->private);\n}\n\nstatic int sg_proc_open_dev(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &dev_seq_ops);\n}\n\nstatic int sg_proc_seq_show_dev(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tstruct scsi_device *scsidp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tif ((NULL == sdp) || (NULL == sdp->device) ||\n\t    (atomic_read(&sdp->detaching)))\n\t\tseq_puts(s, \"-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\n\");\n\telse {\n\t\tscsidp = sdp->device;\n\t\tseq_printf(s, \"%d\\t%d\\t%d\\t%llu\\t%d\\t%d\\t%d\\t%d\\t%d\\n\",\n\t\t\t      scsidp->host->host_no, scsidp->channel,\n\t\t\t      scsidp->id, scsidp->lun, (int) scsidp->type,\n\t\t\t      1,\n\t\t\t      (int) scsidp->queue_depth,\n\t\t\t      (int) atomic_read(&scsidp->device_busy),\n\t\t\t      (int) scsi_device_online(scsidp));\n\t}\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\nstatic int sg_proc_open_devstrs(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &devstrs_seq_ops);\n}\n\nstatic int sg_proc_seq_show_devstrs(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tstruct scsi_device *scsidp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tscsidp = sdp ? sdp->device : NULL;\n\tif (sdp && scsidp && (!atomic_read(&sdp->detaching)))\n\t\tseq_printf(s, \"%8.8s\\t%16.16s\\t%4.4s\\n\",\n\t\t\t   scsidp->vendor, scsidp->model, scsidp->rev);\n\telse\n\t\tseq_puts(s, \"<no active device>\\n\");\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\n/* must be called while holding sg_index_lock */\nstatic void sg_proc_debug_helper(struct seq_file *s, Sg_device * sdp)\n{\n\tint k, new_interface, blen, usg;\n\tSg_request *srp;\n\tSg_fd *fp;\n\tconst sg_io_hdr_t *hp;\n\tconst char * cp;\n\tunsigned int ms;\n\n\tk = 0;\n\tlist_for_each_entry(fp, &sdp->sfds, sfd_siblings) {\n\t\tk++;\n\t\tread_lock(&fp->rq_list_lock); /* irqs already disabled */\n\t\tseq_printf(s, \"   FD(%d): timeout=%dms bufflen=%d \"\n\t\t\t   \"(res)sgat=%d low_dma=%d\\n\", k,\n\t\t\t   jiffies_to_msecs(fp->timeout),\n\t\t\t   fp->reserve.bufflen,\n\t\t\t   (int) fp->reserve.k_use_sg,\n\t\t\t   (int) sdp->device->host->unchecked_isa_dma);\n\t\tseq_printf(s, \"   cmd_q=%d f_packid=%d k_orphan=%d closed=0\\n\",\n\t\t\t   (int) fp->cmd_q, (int) fp->force_packid,\n\t\t\t   (int) fp->keep_orphan);\n\t\tlist_for_each_entry(srp, &fp->rq_list, entry) {\n\t\t\thp = &srp->header;\n\t\t\tnew_interface = (hp->interface_id == '\\0') ? 0 : 1;\n\t\t\tif (srp->res_used) {\n\t\t\t\tif (new_interface &&\n\t\t\t\t    (SG_FLAG_MMAP_IO & hp->flags))\n\t\t\t\t\tcp = \"     mmap>> \";\n\t\t\t\telse\n\t\t\t\t\tcp = \"     rb>> \";\n\t\t\t} else {\n\t\t\t\tif (SG_INFO_DIRECT_IO_MASK & hp->info)\n\t\t\t\t\tcp = \"     dio>> \";\n\t\t\t\telse\n\t\t\t\t\tcp = \"     \";\n\t\t\t}\n\t\t\tseq_puts(s, cp);\n\t\t\tblen = srp->data.bufflen;\n\t\t\tusg = srp->data.k_use_sg;\n\t\t\tseq_puts(s, srp->done ?\n\t\t\t\t ((1 == srp->done) ?  \"rcv:\" : \"fin:\")\n\t\t\t\t  : \"act:\");\n\t\t\tseq_printf(s, \" id=%d blen=%d\",\n\t\t\t\t   srp->header.pack_id, blen);\n\t\t\tif (srp->done)\n\t\t\t\tseq_printf(s, \" dur=%d\", hp->duration);\n\t\t\telse {\n\t\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\t\tseq_printf(s, \" t_o/elap=%d/%d\",\n\t\t\t\t\t(new_interface ? hp->timeout :\n\t\t\t\t\t\t  jiffies_to_msecs(fp->timeout)),\n\t\t\t\t\t(ms > hp->duration ? ms - hp->duration : 0));\n\t\t\t}\n\t\t\tseq_printf(s, \"ms sgat=%d op=0x%02x\\n\", usg,\n\t\t\t\t   (int) srp->data.cmd_opcode);\n\t\t}\n\t\tif (list_empty(&fp->rq_list))\n\t\t\tseq_puts(s, \"     No requests active\\n\");\n\t\tread_unlock(&fp->rq_list_lock);\n\t}\n}\n\nstatic int sg_proc_open_debug(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &debug_seq_ops);\n}\n\nstatic int sg_proc_seq_show_debug(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tunsigned long iflags;\n\n\tif (it && (0 == it->index))\n\t\tseq_printf(s, \"max_active_device=%d  def_reserved_size=%d\\n\",\n\t\t\t   (int)it->max, sg_big_buff);\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tif (NULL == sdp)\n\t\tgoto skip;\n\tread_lock(&sdp->sfd_lock);\n\tif (!list_empty(&sdp->sfds)) {\n\t\tseq_printf(s, \" >>> device=%s \", sdp->disk->disk_name);\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\tseq_puts(s, \"detaching pending close \");\n\t\telse if (sdp->device) {\n\t\t\tstruct scsi_device *scsidp = sdp->device;\n\n\t\t\tseq_printf(s, \"%d:%d:%d:%llu   em=%d\",\n\t\t\t\t   scsidp->host->host_no,\n\t\t\t\t   scsidp->channel, scsidp->id,\n\t\t\t\t   scsidp->lun,\n\t\t\t\t   scsidp->host->hostt->emulated);\n\t\t}\n\t\tseq_printf(s, \" sg_tablesize=%d excl=%d open_cnt=%d\\n\",\n\t\t\t   sdp->sg_tablesize, sdp->exclude, sdp->open_cnt);\n\t\tsg_proc_debug_helper(s, sdp);\n\t}\n\tread_unlock(&sdp->sfd_lock);\nskip:\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\nmodule_init(init_sg);\nmodule_exit(exit_sg);\n"], "fixing_code": ["/*\n *  History:\n *  Started: Aug 9 by Lawrence Foard (entropy@world.std.com),\n *           to allow user process control of SCSI devices.\n *  Development Sponsored by Killy Corp. NY NY\n *\n * Original driver (sg.c):\n *        Copyright (C) 1992 Lawrence Foard\n * Version 2 and 3 extensions to driver:\n *        Copyright (C) 1998 - 2014 Douglas Gilbert\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2, or (at your option)\n * any later version.\n *\n */\n\nstatic int sg_version_num = 30536;\t/* 2 digits for each component */\n#define SG_VERSION_STR \"3.5.36\"\n\n/*\n *  D. P. Gilbert (dgilbert@interlog.com), notes:\n *      - scsi logging is available via SCSI_LOG_TIMEOUT macros. First\n *        the kernel/module needs to be built with CONFIG_SCSI_LOGGING\n *        (otherwise the macros compile to empty statements).\n *\n */\n#include <linux/module.h>\n\n#include <linux/fs.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/errno.h>\n#include <linux/mtio.h>\n#include <linux/ioctl.h>\n#include <linux/slab.h>\n#include <linux/fcntl.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/moduleparam.h>\n#include <linux/cdev.h>\n#include <linux/idr.h>\n#include <linux/seq_file.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/blktrace_api.h>\n#include <linux/mutex.h>\n#include <linux/atomic.h>\n#include <linux/ratelimit.h>\n#include <linux/uio.h>\n\n#include \"scsi.h\"\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/sg.h>\n\n#include \"scsi_logging.h\"\n\n#ifdef CONFIG_SCSI_PROC_FS\n#include <linux/proc_fs.h>\nstatic char *sg_version_date = \"20140603\";\n\nstatic int sg_proc_init(void);\nstatic void sg_proc_cleanup(void);\n#endif\n\n#define SG_ALLOW_DIO_DEF 0\n\n#define SG_MAX_DEVS 32768\n\n/* SG_MAX_CDB_SIZE should be 260 (spc4r37 section 3.1.30) however the type\n * of sg_io_hdr::cmd_len can only represent 255. All SCSI commands greater\n * than 16 bytes are \"variable length\" whose length is a multiple of 4\n */\n#define SG_MAX_CDB_SIZE 252\n\n#define SG_DEFAULT_TIMEOUT mult_frac(SG_DEFAULT_TIMEOUT_USER, HZ, USER_HZ)\n\nint sg_big_buff = SG_DEF_RESERVED_SIZE;\n/* N.B. This variable is readable and writeable via\n   /proc/scsi/sg/def_reserved_size . Each time sg_open() is called a buffer\n   of this size (or less if there is not enough memory) will be reserved\n   for use by this file descriptor. [Deprecated usage: this variable is also\n   readable via /proc/sys/kernel/sg-big-buff if the sg driver is built into\n   the kernel (i.e. it is not a module).] */\nstatic int def_reserved_size = -1;\t/* picks up init parameter */\nstatic int sg_allow_dio = SG_ALLOW_DIO_DEF;\n\nstatic int scatter_elem_sz = SG_SCATTER_SZ;\nstatic int scatter_elem_sz_prev = SG_SCATTER_SZ;\n\n#define SG_SECTOR_SZ 512\n\nstatic int sg_add_device(struct device *, struct class_interface *);\nstatic void sg_remove_device(struct device *, struct class_interface *);\n\nstatic DEFINE_IDR(sg_index_idr);\nstatic DEFINE_RWLOCK(sg_index_lock);\t/* Also used to lock\n\t\t\t\t\t\t\t   file descriptor list for device */\n\nstatic struct class_interface sg_interface = {\n\t.add_dev        = sg_add_device,\n\t.remove_dev     = sg_remove_device,\n};\n\ntypedef struct sg_scatter_hold { /* holding area for scsi scatter gather info */\n\tunsigned short k_use_sg; /* Count of kernel scatter-gather pieces */\n\tunsigned sglist_len; /* size of malloc'd scatter-gather list ++ */\n\tunsigned bufflen;\t/* Size of (aggregate) data buffer */\n\tstruct page **pages;\n\tint page_order;\n\tchar dio_in_use;\t/* 0->indirect IO (or mmap), 1->dio */\n\tunsigned char cmd_opcode; /* first byte of command */\n} Sg_scatter_hold;\n\nstruct sg_device;\t\t/* forward declarations */\nstruct sg_fd;\n\ntypedef struct sg_request {\t/* SG_MAX_QUEUE requests outstanding per file */\n\tstruct list_head entry;\t/* list entry */\n\tstruct sg_fd *parentfp;\t/* NULL -> not in use */\n\tSg_scatter_hold data;\t/* hold buffer, perhaps scatter list */\n\tsg_io_hdr_t header;\t/* scsi command+info, see <scsi/sg.h> */\n\tunsigned char sense_b[SCSI_SENSE_BUFFERSIZE];\n\tchar res_used;\t\t/* 1 -> using reserve buffer, 0 -> not ... */\n\tchar orphan;\t\t/* 1 -> drop on sight, 0 -> normal */\n\tchar sg_io_owned;\t/* 1 -> packet belongs to SG_IO */\n\t/* done protected by rq_list_lock */\n\tchar done;\t\t/* 0->before bh, 1->before read, 2->read */\n\tstruct request *rq;\n\tstruct bio *bio;\n\tstruct execute_work ew;\n} Sg_request;\n\ntypedef struct sg_fd {\t\t/* holds the state of a file descriptor */\n\tstruct list_head sfd_siblings;  /* protected by device's sfd_lock */\n\tstruct sg_device *parentdp;\t/* owning device */\n\twait_queue_head_t read_wait;\t/* queue read until command done */\n\trwlock_t rq_list_lock;\t/* protect access to list in req_arr */\n\tstruct mutex f_mutex;\t/* protect against changes in this fd */\n\tint timeout;\t\t/* defaults to SG_DEFAULT_TIMEOUT      */\n\tint timeout_user;\t/* defaults to SG_DEFAULT_TIMEOUT_USER */\n\tSg_scatter_hold reserve;\t/* buffer held for this file descriptor */\n\tstruct list_head rq_list; /* head of request list */\n\tstruct fasync_struct *async_qp;\t/* used by asynchronous notification */\n\tSg_request req_arr[SG_MAX_QUEUE];\t/* used as singly-linked list */\n\tchar force_packid;\t/* 1 -> pack_id input to read(), 0 -> ignored */\n\tchar cmd_q;\t\t/* 1 -> allow command queuing, 0 -> don't */\n\tunsigned char next_cmd_len; /* 0: automatic, >0: use on next write() */\n\tchar keep_orphan;\t/* 0 -> drop orphan (def), 1 -> keep for read() */\n\tchar mmap_called;\t/* 0 -> mmap() never called on this fd */\n\tchar res_in_use;\t/* 1 -> 'reserve' array in use */\n\tstruct kref f_ref;\n\tstruct execute_work ew;\n} Sg_fd;\n\ntypedef struct sg_device { /* holds the state of each scsi generic device */\n\tstruct scsi_device *device;\n\twait_queue_head_t open_wait;    /* queue open() when O_EXCL present */\n\tstruct mutex open_rel_lock;     /* held when in open() or release() */\n\tint sg_tablesize;\t/* adapter's max scatter-gather table size */\n\tu32 index;\t\t/* device index number */\n\tstruct list_head sfds;\n\trwlock_t sfd_lock;      /* protect access to sfd list */\n\tatomic_t detaching;     /* 0->device usable, 1->device detaching */\n\tbool exclude;\t\t/* 1->open(O_EXCL) succeeded and is active */\n\tint open_cnt;\t\t/* count of opens (perhaps < num(sfds) ) */\n\tchar sgdebug;\t\t/* 0->off, 1->sense, 9->dump dev, 10-> all devs */\n\tstruct gendisk *disk;\n\tstruct cdev * cdev;\t/* char_dev [sysfs: /sys/cdev/major/sg<n>] */\n\tstruct kref d_ref;\n} Sg_device;\n\n/* tasklet or soft irq callback */\nstatic void sg_rq_end_io(struct request *rq, blk_status_t status);\nstatic int sg_start_req(Sg_request *srp, unsigned char *cmd);\nstatic int sg_finish_rem_req(Sg_request * srp);\nstatic int sg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size);\nstatic ssize_t sg_new_read(Sg_fd * sfp, char __user *buf, size_t count,\n\t\t\t   Sg_request * srp);\nstatic ssize_t sg_new_write(Sg_fd *sfp, struct file *file,\n\t\t\tconst char __user *buf, size_t count, int blocking,\n\t\t\tint read_only, int sg_io_owned, Sg_request **o_srp);\nstatic int sg_common_write(Sg_fd * sfp, Sg_request * srp,\n\t\t\t   unsigned char *cmnd, int timeout, int blocking);\nstatic int sg_read_oxfer(Sg_request * srp, char __user *outp, int num_read_xfer);\nstatic void sg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp);\nstatic void sg_build_reserve(Sg_fd * sfp, int req_size);\nstatic void sg_link_reserve(Sg_fd * sfp, Sg_request * srp, int size);\nstatic void sg_unlink_reserve(Sg_fd * sfp, Sg_request * srp);\nstatic Sg_fd *sg_add_sfp(Sg_device * sdp);\nstatic void sg_remove_sfp(struct kref *);\nstatic Sg_request *sg_get_rq_mark(Sg_fd * sfp, int pack_id);\nstatic Sg_request *sg_add_request(Sg_fd * sfp);\nstatic int sg_remove_request(Sg_fd * sfp, Sg_request * srp);\nstatic Sg_device *sg_get_dev(int dev);\nstatic void sg_device_destroy(struct kref *kref);\n\n#define SZ_SG_HEADER sizeof(struct sg_header)\n#define SZ_SG_IO_HDR sizeof(sg_io_hdr_t)\n#define SZ_SG_IOVEC sizeof(sg_iovec_t)\n#define SZ_SG_REQ_INFO sizeof(sg_req_info_t)\n\n#define sg_printk(prefix, sdp, fmt, a...) \\\n\tsdev_prefix_printk(prefix, (sdp)->device,\t\t\\\n\t\t\t   (sdp)->disk->disk_name, fmt, ##a)\n\nstatic int sg_allow_access(struct file *filp, unsigned char *cmd)\n{\n\tstruct sg_fd *sfp = filp->private_data;\n\n\tif (sfp->parentdp->device->type == TYPE_SCANNER)\n\t\treturn 0;\n\n\treturn blk_verify_command(cmd, filp->f_mode);\n}\n\nstatic int\nopen_wait(Sg_device *sdp, int flags)\n{\n\tint retval = 0;\n\n\tif (flags & O_EXCL) {\n\t\twhile (sdp->open_cnt > 0) {\n\t\t\tmutex_unlock(&sdp->open_rel_lock);\n\t\t\tretval = wait_event_interruptible(sdp->open_wait,\n\t\t\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t\t\t !sdp->open_cnt));\n\t\t\tmutex_lock(&sdp->open_rel_lock);\n\n\t\t\tif (retval) /* -ERESTARTSYS */\n\t\t\t\treturn retval;\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t}\n\t} else {\n\t\twhile (sdp->exclude) {\n\t\t\tmutex_unlock(&sdp->open_rel_lock);\n\t\t\tretval = wait_event_interruptible(sdp->open_wait,\n\t\t\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t\t\t !sdp->exclude));\n\t\t\tmutex_lock(&sdp->open_rel_lock);\n\n\t\t\tif (retval) /* -ERESTARTSYS */\n\t\t\t\treturn retval;\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t}\n\t}\n\n\treturn retval;\n}\n\n/* Returns 0 on success, else a negated errno value */\nstatic int\nsg_open(struct inode *inode, struct file *filp)\n{\n\tint dev = iminor(inode);\n\tint flags = filp->f_flags;\n\tstruct request_queue *q;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tint retval;\n\n\tnonseekable_open(inode, filp);\n\tif ((flags & O_EXCL) && (O_RDONLY == (flags & O_ACCMODE)))\n\t\treturn -EPERM; /* Can't lock it with read only access */\n\tsdp = sg_get_dev(dev);\n\tif (IS_ERR(sdp))\n\t\treturn PTR_ERR(sdp);\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_open: flags=0x%x\\n\", flags));\n\n\t/* This driver's module count bumped by fops_get in <linux/fs.h> */\n\t/* Prevent the device driver from vanishing while we sleep */\n\tretval = scsi_device_get(sdp->device);\n\tif (retval)\n\t\tgoto sg_put;\n\n\tretval = scsi_autopm_get_device(sdp->device);\n\tif (retval)\n\t\tgoto sdp_put;\n\n\t/* scsi_block_when_processing_errors() may block so bypass\n\t * check if O_NONBLOCK. Permits SCSI commands to be issued\n\t * during error recovery. Tread carefully. */\n\tif (!((flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device))) {\n\t\tretval = -ENXIO;\n\t\t/* we are in error recovery for this device */\n\t\tgoto error_out;\n\t}\n\n\tmutex_lock(&sdp->open_rel_lock);\n\tif (flags & O_NONBLOCK) {\n\t\tif (flags & O_EXCL) {\n\t\t\tif (sdp->open_cnt > 0) {\n\t\t\t\tretval = -EBUSY;\n\t\t\t\tgoto error_mutex_locked;\n\t\t\t}\n\t\t} else {\n\t\t\tif (sdp->exclude) {\n\t\t\t\tretval = -EBUSY;\n\t\t\t\tgoto error_mutex_locked;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tretval = open_wait(sdp, flags);\n\t\tif (retval) /* -ERESTARTSYS or -ENODEV */\n\t\t\tgoto error_mutex_locked;\n\t}\n\n\t/* N.B. at this point we are holding the open_rel_lock */\n\tif (flags & O_EXCL)\n\t\tsdp->exclude = true;\n\n\tif (sdp->open_cnt < 1) {  /* no existing opens */\n\t\tsdp->sgdebug = 0;\n\t\tq = sdp->device->request_queue;\n\t\tsdp->sg_tablesize = queue_max_segments(q);\n\t}\n\tsfp = sg_add_sfp(sdp);\n\tif (IS_ERR(sfp)) {\n\t\tretval = PTR_ERR(sfp);\n\t\tgoto out_undo;\n\t}\n\n\tfilp->private_data = sfp;\n\tsdp->open_cnt++;\n\tmutex_unlock(&sdp->open_rel_lock);\n\n\tretval = 0;\nsg_put:\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n\treturn retval;\n\nout_undo:\n\tif (flags & O_EXCL) {\n\t\tsdp->exclude = false;   /* undo if error */\n\t\twake_up_interruptible(&sdp->open_wait);\n\t}\nerror_mutex_locked:\n\tmutex_unlock(&sdp->open_rel_lock);\nerror_out:\n\tscsi_autopm_put_device(sdp->device);\nsdp_put:\n\tscsi_device_put(sdp->device);\n\tgoto sg_put;\n}\n\n/* Release resources associated with a successful sg_open()\n * Returns 0 on success, else a negated errno value */\nstatic int\nsg_release(struct inode *inode, struct file *filp)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp, \"sg_release\\n\"));\n\n\tmutex_lock(&sdp->open_rel_lock);\n\tscsi_autopm_put_device(sdp->device);\n\tkref_put(&sfp->f_ref, sg_remove_sfp);\n\tsdp->open_cnt--;\n\n\t/* possibly many open()s waiting on exlude clearing, start many;\n\t * only open(O_EXCL)s wait on 0==open_cnt so only start one */\n\tif (sdp->exclude) {\n\t\tsdp->exclude = false;\n\t\twake_up_interruptible_all(&sdp->open_wait);\n\t} else if (0 == sdp->open_cnt) {\n\t\twake_up_interruptible(&sdp->open_wait);\n\t}\n\tmutex_unlock(&sdp->open_rel_lock);\n\treturn 0;\n}\n\nstatic ssize_t\nsg_read(struct file *filp, char __user *buf, size_t count, loff_t * ppos)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tint req_pack_id = -1;\n\tsg_io_hdr_t *hp;\n\tstruct sg_header *old_hdr = NULL;\n\tint retval = 0;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_read: count=%d\\n\", (int) count));\n\n\tif (!access_ok(VERIFY_WRITE, buf, count))\n\t\treturn -EFAULT;\n\tif (sfp->force_packid && (count >= SZ_SG_HEADER)) {\n\t\told_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);\n\t\tif (!old_hdr)\n\t\t\treturn -ENOMEM;\n\t\tif (__copy_from_user(old_hdr, buf, SZ_SG_HEADER)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (old_hdr->reply_len < 0) {\n\t\t\tif (count >= SZ_SG_IO_HDR) {\n\t\t\t\tsg_io_hdr_t *new_hdr;\n\t\t\t\tnew_hdr = kmalloc(SZ_SG_IO_HDR, GFP_KERNEL);\n\t\t\t\tif (!new_hdr) {\n\t\t\t\t\tretval = -ENOMEM;\n\t\t\t\t\tgoto free_old_hdr;\n\t\t\t\t}\n\t\t\t\tretval =__copy_from_user\n\t\t\t\t    (new_hdr, buf, SZ_SG_IO_HDR);\n\t\t\t\treq_pack_id = new_hdr->pack_id;\n\t\t\t\tkfree(new_hdr);\n\t\t\t\tif (retval) {\n\t\t\t\t\tretval = -EFAULT;\n\t\t\t\t\tgoto free_old_hdr;\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\treq_pack_id = old_hdr->pack_id;\n\t}\n\tsrp = sg_get_rq_mark(sfp, req_pack_id);\n\tif (!srp) {\t\t/* now wait on packet to arrive */\n\t\tif (atomic_read(&sdp->detaching)) {\n\t\t\tretval = -ENODEV;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tretval = wait_event_interruptible(sfp->read_wait,\n\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t(srp = sg_get_rq_mark(sfp, req_pack_id))));\n\t\tif (atomic_read(&sdp->detaching)) {\n\t\t\tretval = -ENODEV;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (retval) {\n\t\t\t/* -ERESTARTSYS as signal hit process */\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t}\n\tif (srp->header.interface_id != '\\0') {\n\t\tretval = sg_new_read(sfp, buf, count, srp);\n\t\tgoto free_old_hdr;\n\t}\n\n\thp = &srp->header;\n\tif (old_hdr == NULL) {\n\t\told_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);\n\t\tif (! old_hdr) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t}\n\tmemset(old_hdr, 0, SZ_SG_HEADER);\n\told_hdr->reply_len = (int) hp->timeout;\n\told_hdr->pack_len = old_hdr->reply_len; /* old, strange behaviour */\n\told_hdr->pack_id = hp->pack_id;\n\told_hdr->twelve_byte =\n\t    ((srp->data.cmd_opcode >= 0xc0) && (12 == hp->cmd_len)) ? 1 : 0;\n\told_hdr->target_status = hp->masked_status;\n\told_hdr->host_status = hp->host_status;\n\told_hdr->driver_status = hp->driver_status;\n\tif ((CHECK_CONDITION & hp->masked_status) ||\n\t    (DRIVER_SENSE & hp->driver_status))\n\t\tmemcpy(old_hdr->sense_buffer, srp->sense_b,\n\t\t       sizeof (old_hdr->sense_buffer));\n\tswitch (hp->host_status) {\n\t/* This setup of 'result' is for backward compatibility and is best\n\t   ignored by the user who should use target, host + driver status */\n\tcase DID_OK:\n\tcase DID_PASSTHROUGH:\n\tcase DID_SOFT_ERROR:\n\t\told_hdr->result = 0;\n\t\tbreak;\n\tcase DID_NO_CONNECT:\n\tcase DID_BUS_BUSY:\n\tcase DID_TIME_OUT:\n\t\told_hdr->result = EBUSY;\n\t\tbreak;\n\tcase DID_BAD_TARGET:\n\tcase DID_ABORT:\n\tcase DID_PARITY:\n\tcase DID_RESET:\n\tcase DID_BAD_INTR:\n\t\told_hdr->result = EIO;\n\t\tbreak;\n\tcase DID_ERROR:\n\t\told_hdr->result = (srp->sense_b[0] == 0 && \n\t\t\t\t  hp->masked_status == GOOD) ? 0 : EIO;\n\t\tbreak;\n\tdefault:\n\t\told_hdr->result = EIO;\n\t\tbreak;\n\t}\n\n\t/* Now copy the result back to the user buffer.  */\n\tif (count >= SZ_SG_HEADER) {\n\t\tif (__copy_to_user(buf, old_hdr, SZ_SG_HEADER)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tbuf += SZ_SG_HEADER;\n\t\tif (count > old_hdr->reply_len)\n\t\t\tcount = old_hdr->reply_len;\n\t\tif (count > SZ_SG_HEADER) {\n\t\t\tif (sg_read_oxfer(srp, buf, count - SZ_SG_HEADER)) {\n\t\t\t\tretval = -EFAULT;\n\t\t\t\tgoto free_old_hdr;\n\t\t\t}\n\t\t}\n\t} else\n\t\tcount = (old_hdr->result == 0) ? 0 : -EIO;\n\tsg_finish_rem_req(srp);\n\tsg_remove_request(sfp, srp);\n\tretval = count;\nfree_old_hdr:\n\tkfree(old_hdr);\n\treturn retval;\n}\n\nstatic ssize_t\nsg_new_read(Sg_fd * sfp, char __user *buf, size_t count, Sg_request * srp)\n{\n\tsg_io_hdr_t *hp = &srp->header;\n\tint err = 0, err2;\n\tint len;\n\n\tif (count < SZ_SG_IO_HDR) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\thp->sb_len_wr = 0;\n\tif ((hp->mx_sb_len > 0) && hp->sbp) {\n\t\tif ((CHECK_CONDITION & hp->masked_status) ||\n\t\t    (DRIVER_SENSE & hp->driver_status)) {\n\t\t\tint sb_len = SCSI_SENSE_BUFFERSIZE;\n\t\t\tsb_len = (hp->mx_sb_len > sb_len) ? sb_len : hp->mx_sb_len;\n\t\t\tlen = 8 + (int) srp->sense_b[7];\t/* Additional sense length field */\n\t\t\tlen = (len > sb_len) ? sb_len : len;\n\t\t\tif (copy_to_user(hp->sbp, srp->sense_b, len)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\thp->sb_len_wr = len;\n\t\t}\n\t}\n\tif (hp->masked_status || hp->host_status || hp->driver_status)\n\t\thp->info |= SG_INFO_CHECK;\n\tif (copy_to_user(buf, hp, SZ_SG_IO_HDR)) {\n\t\terr = -EFAULT;\n\t\tgoto err_out;\n\t}\nerr_out:\n\terr2 = sg_finish_rem_req(srp);\n\tsg_remove_request(sfp, srp);\n\treturn err ? : err2 ? : count;\n}\n\nstatic ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif (unlikely(uaccess_kernel()))\n\t\treturn -EINVAL;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tmutex_lock(&sfp->f_mutex);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tmutex_unlock(&sfp->f_mutex);\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t   current->comm);\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}\n\nstatic ssize_t\nsg_new_write(Sg_fd *sfp, struct file *file, const char __user *buf,\n\t\t size_t count, int blocking, int read_only, int sg_io_owned,\n\t\t Sg_request **o_srp)\n{\n\tint k;\n\tSg_request *srp;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\tint timeout;\n\tunsigned long ul_timeout;\n\n\tif (count < SZ_SG_IO_HDR)\n\t\treturn -EINVAL;\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT; /* protects following copy_from_user()s + get_user()s */\n\n\tsfp->cmd_q = 1;\t/* when sg_io_hdr seen, set command queuing on */\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t\t      \"sg_new_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tsrp->sg_io_owned = sg_io_owned;\n\thp = &srp->header;\n\tif (__copy_from_user(hp, buf, SZ_SG_IO_HDR)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\n\t}\n\tif (hp->interface_id != 'S') {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -ENOSYS;\n\t}\n\tif (hp->flags & SG_FLAG_MMAP_IO) {\n\t\tif (hp->dxfer_len > sfp->reserve.bufflen) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -ENOMEM;\t/* MMAP_IO size must fit in reserve buffer */\n\t\t}\n\t\tif (hp->flags & SG_FLAG_DIRECT_IO) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -EINVAL;\t/* either MMAP_IO or DIRECT_IO (not both) */\n\t\t}\n\t\tif (sfp->res_in_use) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -EBUSY;\t/* reserve buffer already being used */\n\t\t}\n\t}\n\tul_timeout = msecs_to_jiffies(srp->header.timeout);\n\ttimeout = (ul_timeout < INT_MAX) ? ul_timeout : INT_MAX;\n\tif ((!hp->cmdp) || (hp->cmd_len < 6) || (hp->cmd_len > sizeof (cmnd))) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (!access_ok(VERIFY_READ, hp->cmdp, hp->cmd_len)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\t}\n\tif (__copy_from_user(cmnd, hp->cmdp, hp->cmd_len)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\n\t}\n\tif (read_only && sg_allow_access(file, cmnd)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EPERM;\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, timeout, blocking);\n\tif (k < 0)\n\t\treturn k;\n\tif (o_srp)\n\t\t*o_srp = srp;\n\treturn count;\n}\n\nstatic int\nsg_common_write(Sg_fd * sfp, Sg_request * srp,\n\t\tunsigned char *cmnd, int timeout, int blocking)\n{\n\tint k, at_head;\n\tSg_device *sdp = sfp->parentdp;\n\tsg_io_hdr_t *hp = &srp->header;\n\n\tsrp->data.cmd_opcode = cmnd[0];\t/* hold opcode of command */\n\thp->status = 0;\n\thp->masked_status = 0;\n\thp->msg_status = 0;\n\thp->info = 0;\n\thp->host_status = 0;\n\thp->driver_status = 0;\n\thp->resid = 0;\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\"sg_common_write:  scsi opcode=0x%02x, cmd_size=%d\\n\",\n\t\t\t(int) cmnd[0], (int) hp->cmd_len));\n\n\tif (hp->dxfer_len >= SZ_256M)\n\t\treturn -EINVAL;\n\n\tk = sg_start_req(srp, cmnd);\n\tif (k) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\"sg_common_write: start_req err=%d\\n\", k));\n\t\tsg_finish_rem_req(srp);\n\t\tsg_remove_request(sfp, srp);\n\t\treturn k;\t/* probably out of space --> ENOMEM */\n\t}\n\tif (atomic_read(&sdp->detaching)) {\n\t\tif (srp->bio) {\n\t\t\tscsi_req_free_cmd(scsi_req(srp->rq));\n\t\t\tblk_end_request_all(srp->rq, BLK_STS_IOERR);\n\t\t\tsrp->rq = NULL;\n\t\t}\n\n\t\tsg_finish_rem_req(srp);\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -ENODEV;\n\t}\n\n\thp->duration = jiffies_to_msecs(jiffies);\n\tif (hp->interface_id != '\\0' &&\t/* v3 (or later) interface */\n\t    (SG_FLAG_Q_AT_TAIL & hp->flags))\n\t\tat_head = 0;\n\telse\n\t\tat_head = 1;\n\n\tsrp->rq->timeout = timeout;\n\tkref_get(&sfp->f_ref); /* sg_rq_end_io() does kref_put(). */\n\tblk_execute_rq_nowait(sdp->device->request_queue, sdp->disk,\n\t\t\t      srp->rq, at_head, sg_rq_end_io);\n\treturn 0;\n}\n\nstatic int srp_done(Sg_fd *sfp, Sg_request *srp)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tread_lock_irqsave(&sfp->rq_list_lock, flags);\n\tret = srp->done;\n\tread_unlock_irqrestore(&sfp->rq_list_lock, flags);\n\treturn ret;\n}\n\nstatic int max_sectors_bytes(struct request_queue *q)\n{\n\tunsigned int max_sectors = queue_max_sectors(q);\n\n\tmax_sectors = min_t(unsigned int, max_sectors, INT_MAX >> 9);\n\n\treturn max_sectors << 9;\n}\n\nstatic void\nsg_fill_request_table(Sg_fd *sfp, sg_req_info_t *rinfo)\n{\n\tSg_request *srp;\n\tint val;\n\tunsigned int ms;\n\n\tval = 0;\n\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\tif (val >= SG_MAX_QUEUE)\n\t\t\tbreak;\n\t\trinfo[val].req_state = srp->done + 1;\n\t\trinfo[val].problem =\n\t\t\tsrp->header.masked_status &\n\t\t\tsrp->header.host_status &\n\t\t\tsrp->header.driver_status;\n\t\tif (srp->done)\n\t\t\trinfo[val].duration =\n\t\t\t\tsrp->header.duration;\n\t\telse {\n\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\trinfo[val].duration =\n\t\t\t\t(ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\t\t}\n\t\trinfo[val].orphan = srp->orphan;\n\t\trinfo[val].sg_io_owned = srp->sg_io_owned;\n\t\trinfo[val].pack_id = srp->header.pack_id;\n\t\trinfo[val].usr_ptr = srp->header.usr_ptr;\n\t\tval++;\n\t}\n}\n\nstatic long\nsg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tvoid __user *p = (void __user *)arg;\n\tint __user *ip = p;\n\tint result, val, read_only;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tunsigned long iflags;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t   \"sg_ioctl: cmd=0x%x\\n\", (int) cmd_in));\n\tread_only = (O_RDWR != (filp->f_flags & O_ACCMODE));\n\n\tswitch (cmd_in) {\n\tcase SG_IO:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (!scsi_block_when_processing_errors(sdp->device))\n\t\t\treturn -ENXIO;\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_IO_HDR))\n\t\t\treturn -EFAULT;\n\t\tresult = sg_new_write(sfp, filp, p, SZ_SG_IO_HDR,\n\t\t\t\t 1, read_only, 1, &srp);\n\t\tif (result < 0)\n\t\t\treturn result;\n\t\tresult = wait_event_interruptible(sfp->read_wait,\n\t\t\t(srp_done(sfp, srp) || atomic_read(&sdp->detaching)));\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\twrite_lock_irq(&sfp->rq_list_lock);\n\t\tif (srp->done) {\n\t\t\tsrp->done = 2;\n\t\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\t\tresult = sg_new_read(sfp, p, SZ_SG_IO_HDR, srp);\n\t\t\treturn (result < 0) ? result : 0;\n\t\t}\n\t\tsrp->orphan = 1;\n\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\treturn result;\t/* -ERESTARTSYS because signal hit process */\n\tcase SG_SET_TIMEOUT:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val < 0)\n\t\t\treturn -EIO;\n\t\tif (val >= mult_frac((s64)INT_MAX, USER_HZ, HZ))\n\t\t\tval = min_t(s64, mult_frac((s64)INT_MAX, USER_HZ, HZ),\n\t\t\t\t    INT_MAX);\n\t\tsfp->timeout_user = val;\n\t\tsfp->timeout = mult_frac(val, HZ, USER_HZ);\n\n\t\treturn 0;\n\tcase SG_GET_TIMEOUT:\t/* N.B. User receives timeout as return value */\n\t\t\t\t/* strange ..., for backward compatibility */\n\t\treturn sfp->timeout_user;\n\tcase SG_SET_FORCE_LOW_DMA:\n\t\t/*\n\t\t * N.B. This ioctl never worked properly, but failed to\n\t\t * return an error value. So returning '0' to keep compability\n\t\t * with legacy applications.\n\t\t */\n\t\treturn 0;\n\tcase SG_GET_LOW_DMA:\n\t\treturn put_user((int) sdp->device->host->unchecked_isa_dma, ip);\n\tcase SG_GET_SCSI_ID:\n\t\tif (!access_ok(VERIFY_WRITE, p, sizeof (sg_scsi_id_t)))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_scsi_id_t __user *sg_idp = p;\n\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t\t__put_user((int) sdp->device->host->host_no,\n\t\t\t\t   &sg_idp->host_no);\n\t\t\t__put_user((int) sdp->device->channel,\n\t\t\t\t   &sg_idp->channel);\n\t\t\t__put_user((int) sdp->device->id, &sg_idp->scsi_id);\n\t\t\t__put_user((int) sdp->device->lun, &sg_idp->lun);\n\t\t\t__put_user((int) sdp->device->type, &sg_idp->scsi_type);\n\t\t\t__put_user((short) sdp->device->host->cmd_per_lun,\n\t\t\t\t   &sg_idp->h_cmd_per_lun);\n\t\t\t__put_user((short) sdp->device->queue_depth,\n\t\t\t\t   &sg_idp->d_queue_depth);\n\t\t\t__put_user(0, &sg_idp->unused[0]);\n\t\t\t__put_user(0, &sg_idp->unused[1]);\n\t\t\treturn 0;\n\t\t}\n\tcase SG_SET_FORCE_PACK_ID:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->force_packid = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_PACK_ID:\n\t\tif (!access_ok(VERIFY_WRITE, ip, sizeof (int)))\n\t\t\treturn -EFAULT;\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned)) {\n\t\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock,\n\t\t\t\t\t\t       iflags);\n\t\t\t\t__put_user(srp->header.pack_id, ip);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t__put_user(-1, ip);\n\t\treturn 0;\n\tcase SG_GET_NUM_WAITING:\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tval = 0;\n\t\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned))\n\t\t\t\t++val;\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_SG_TABLESIZE:\n\t\treturn put_user(sdp->sg_tablesize, ip);\n\tcase SG_SET_RESERVED_SIZE:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n                if (val < 0)\n                        return -EINVAL;\n\t\tval = min_t(int, val,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\tmutex_lock(&sfp->f_mutex);\n\t\tif (val != sfp->reserve.bufflen) {\n\t\t\tif (sfp->mmap_called ||\n\t\t\t    sfp->res_in_use) {\n\t\t\t\tmutex_unlock(&sfp->f_mutex);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t\t\tsg_build_reserve(sfp, val);\n\t\t}\n\t\tmutex_unlock(&sfp->f_mutex);\n\t\treturn 0;\n\tcase SG_GET_RESERVED_SIZE:\n\t\tval = min_t(int, sfp->reserve.bufflen,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\treturn put_user(val, ip);\n\tcase SG_SET_COMMAND_Q:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->cmd_q = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_COMMAND_Q:\n\t\treturn put_user((int) sfp->cmd_q, ip);\n\tcase SG_SET_KEEP_ORPHAN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->keep_orphan = val;\n\t\treturn 0;\n\tcase SG_GET_KEEP_ORPHAN:\n\t\treturn put_user((int) sfp->keep_orphan, ip);\n\tcase SG_NEXT_CMD_LEN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val > SG_MAX_CDB_SIZE)\n\t\t\treturn -ENOMEM;\n\t\tsfp->next_cmd_len = (val > 0) ? val : 0;\n\t\treturn 0;\n\tcase SG_GET_VERSION_NUM:\n\t\treturn put_user(sg_version_num, ip);\n\tcase SG_GET_ACCESS_COUNT:\n\t\t/* faked - we don't have a real access count anymore */\n\t\tval = (sdp->device ? 1 : 0);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_REQUEST_TABLE:\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_REQ_INFO * SG_MAX_QUEUE))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_req_info_t *rinfo;\n\n\t\t\trinfo = kzalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!rinfo)\n\t\t\t\treturn -ENOMEM;\n\t\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\t\tsg_fill_request_table(sfp, rinfo);\n\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t\tresult = __copy_to_user(p, rinfo,\n\t\t\t\t\t\tSZ_SG_REQ_INFO * SG_MAX_QUEUE);\n\t\t\tresult = result ? -EFAULT : 0;\n\t\t\tkfree(rinfo);\n\t\t\treturn result;\n\t\t}\n\tcase SG_EMULATED_HOST:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\treturn put_user(sdp->device->host->hostt->emulated, ip);\n\tcase SCSI_IOCTL_SEND_COMMAND:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (read_only) {\n\t\t\tunsigned char opcode = WRITE_6;\n\t\t\tScsi_Ioctl_Command __user *siocp = p;\n\n\t\t\tif (copy_from_user(&opcode, siocp->data, 1))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (sg_allow_access(filp, &opcode))\n\t\t\t\treturn -EPERM;\n\t\t}\n\t\treturn sg_scsi_ioctl(sdp->device->request_queue, NULL, filp->f_mode, p);\n\tcase SG_SET_DEBUG:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsdp->sgdebug = (char) val;\n\t\treturn 0;\n\tcase BLKSECTGET:\n\t\treturn put_user(max_sectors_bytes(sdp->device->request_queue),\n\t\t\t\tip);\n\tcase BLKTRACESETUP:\n\t\treturn blk_trace_setup(sdp->device->request_queue,\n\t\t\t\t       sdp->disk->disk_name,\n\t\t\t\t       MKDEV(SCSI_GENERIC_MAJOR, sdp->index),\n\t\t\t\t       NULL, p);\n\tcase BLKTRACESTART:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 1);\n\tcase BLKTRACESTOP:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 0);\n\tcase BLKTRACETEARDOWN:\n\t\treturn blk_trace_remove(sdp->device->request_queue);\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SCSI_IOCTL_PROBE_HOST:\n\tcase SG_GET_TRANSFORM:\n\tcase SG_SCSI_RESET:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tbreak;\n\tdefault:\n\t\tif (read_only)\n\t\t\treturn -EPERM;\t/* don't know so take safe approach */\n\t\tbreak;\n\t}\n\n\tresult = scsi_ioctl_block_when_processing_errors(sdp->device,\n\t\t\tcmd_in, filp->f_flags & O_NDELAY);\n\tif (result)\n\t\treturn result;\n\treturn scsi_ioctl(sdp->device, cmd_in, p);\n}\n\n#ifdef CONFIG_COMPAT\nstatic long sg_compat_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tstruct scsi_device *sdev;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tsdev = sdp->device;\n\tif (sdev->host->hostt->compat_ioctl) { \n\t\tint ret;\n\n\t\tret = sdev->host->hostt->compat_ioctl(sdev, cmd_in, (void __user *)arg);\n\n\t\treturn ret;\n\t}\n\t\n\treturn -ENOIOCTLCMD;\n}\n#endif\n\nstatic __poll_t\nsg_poll(struct file *filp, poll_table * wait)\n{\n\t__poll_t res = 0;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tint count = 0;\n\tunsigned long iflags;\n\n\tsfp = filp->private_data;\n\tif (!sfp)\n\t\treturn EPOLLERR;\n\tsdp = sfp->parentdp;\n\tif (!sdp)\n\t\treturn EPOLLERR;\n\tpoll_wait(filp, &sfp->read_wait, wait);\n\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t/* if any read waiting, flag it */\n\t\tif ((0 == res) && (1 == srp->done) && (!srp->sg_io_owned))\n\t\t\tres = EPOLLIN | EPOLLRDNORM;\n\t\t++count;\n\t}\n\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (atomic_read(&sdp->detaching))\n\t\tres |= EPOLLHUP;\n\telse if (!sfp->cmd_q) {\n\t\tif (0 == count)\n\t\t\tres |= EPOLLOUT | EPOLLWRNORM;\n\t} else if (count < SG_MAX_QUEUE)\n\t\tres |= EPOLLOUT | EPOLLWRNORM;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_poll: res=0x%x\\n\", (__force u32) res));\n\treturn res;\n}\n\nstatic int\nsg_fasync(int fd, struct file *filp, int mode)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_fasync: mode=%d\\n\", mode));\n\n\treturn fasync_helper(fd, filp, mode, &sfp->async_qp);\n}\n\nstatic int\nsg_vma_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tSg_fd *sfp;\n\tunsigned long offset, len, sa;\n\tSg_scatter_hold *rsv_schp;\n\tint k, length;\n\n\tif ((NULL == vma) || (!(sfp = (Sg_fd *) vma->vm_private_data)))\n\t\treturn VM_FAULT_SIGBUS;\n\trsv_schp = &sfp->reserve;\n\toffset = vmf->pgoff << PAGE_SHIFT;\n\tif (offset >= rsv_schp->bufflen)\n\t\treturn VM_FAULT_SIGBUS;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_vma_fault: offset=%lu, scatg=%d\\n\",\n\t\t\t\t      offset, rsv_schp->k_use_sg));\n\tsa = vma->vm_start;\n\tlength = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg && sa < vma->vm_end; k++) {\n\t\tlen = vma->vm_end - sa;\n\t\tlen = (len < length) ? len : length;\n\t\tif (offset < len) {\n\t\t\tstruct page *page = nth_page(rsv_schp->pages[k],\n\t\t\t\t\t\t     offset >> PAGE_SHIFT);\n\t\t\tget_page(page);\t/* increment page count */\n\t\t\tvmf->page = page;\n\t\t\treturn 0; /* success */\n\t\t}\n\t\tsa += len;\n\t\toffset -= len;\n\t}\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic const struct vm_operations_struct sg_mmap_vm_ops = {\n\t.fault = sg_vma_fault,\n};\n\nstatic int\nsg_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tSg_fd *sfp;\n\tunsigned long req_sz, len, sa;\n\tSg_scatter_hold *rsv_schp;\n\tint k, length;\n\tint ret = 0;\n\n\tif ((!filp) || (!vma) || (!(sfp = (Sg_fd *) filp->private_data)))\n\t\treturn -ENXIO;\n\treq_sz = vma->vm_end - vma->vm_start;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_mmap starting, vm_start=%p, len=%d\\n\",\n\t\t\t\t      (void *) vma->vm_start, (int) req_sz));\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\t/* want no offset */\n\trsv_schp = &sfp->reserve;\n\tmutex_lock(&sfp->f_mutex);\n\tif (req_sz > rsv_schp->bufflen) {\n\t\tret = -ENOMEM;\t/* cannot map more than reserved buffer */\n\t\tgoto out;\n\t}\n\n\tsa = vma->vm_start;\n\tlength = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg && sa < vma->vm_end; k++) {\n\t\tlen = vma->vm_end - sa;\n\t\tlen = (len < length) ? len : length;\n\t\tsa += len;\n\t}\n\n\tsfp->mmap_called = 1;\n\tvma->vm_flags |= VM_IO | VM_DONTEXPAND | VM_DONTDUMP;\n\tvma->vm_private_data = sfp;\n\tvma->vm_ops = &sg_mmap_vm_ops;\nout:\n\tmutex_unlock(&sfp->f_mutex);\n\treturn ret;\n}\n\nstatic void\nsg_rq_end_io_usercontext(struct work_struct *work)\n{\n\tstruct sg_request *srp = container_of(work, struct sg_request, ew.work);\n\tstruct sg_fd *sfp = srp->parentfp;\n\n\tsg_finish_rem_req(srp);\n\tsg_remove_request(sfp, srp);\n\tkref_put(&sfp->f_ref, sg_remove_sfp);\n}\n\n/*\n * This function is a \"bottom half\" handler that is called by the mid\n * level when a command is completed (or has failed).\n */\nstatic void\nsg_rq_end_io(struct request *rq, blk_status_t status)\n{\n\tstruct sg_request *srp = rq->end_io_data;\n\tstruct scsi_request *req = scsi_req(rq);\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tunsigned long iflags;\n\tunsigned int ms;\n\tchar *sense;\n\tint result, resid, done = 1;\n\n\tif (WARN_ON(srp->done != 0))\n\t\treturn;\n\n\tsfp = srp->parentfp;\n\tif (WARN_ON(sfp == NULL))\n\t\treturn;\n\n\tsdp = sfp->parentdp;\n\tif (unlikely(atomic_read(&sdp->detaching)))\n\t\tpr_info(\"%s: device detaching\\n\", __func__);\n\n\tsense = req->sense;\n\tresult = req->result;\n\tresid = req->resid_len;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_cmd_done: pack_id=%d, res=0x%x\\n\",\n\t\t\t\t      srp->header.pack_id, result));\n\tsrp->header.resid = resid;\n\tms = jiffies_to_msecs(jiffies);\n\tsrp->header.duration = (ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\tif (0 != result) {\n\t\tstruct scsi_sense_hdr sshdr;\n\n\t\tsrp->header.status = 0xff & result;\n\t\tsrp->header.masked_status = status_byte(result);\n\t\tsrp->header.msg_status = msg_byte(result);\n\t\tsrp->header.host_status = host_byte(result);\n\t\tsrp->header.driver_status = driver_byte(result);\n\t\tif ((sdp->sgdebug > 0) &&\n\t\t    ((CHECK_CONDITION == srp->header.masked_status) ||\n\t\t     (COMMAND_TERMINATED == srp->header.masked_status)))\n\t\t\t__scsi_print_sense(sdp->device, __func__, sense,\n\t\t\t\t\t   SCSI_SENSE_BUFFERSIZE);\n\n\t\t/* Following if statement is a patch supplied by Eric Youngdale */\n\t\tif (driver_byte(result) != 0\n\t\t    && scsi_normalize_sense(sense, SCSI_SENSE_BUFFERSIZE, &sshdr)\n\t\t    && !scsi_sense_is_deferred(&sshdr)\n\t\t    && sshdr.sense_key == UNIT_ATTENTION\n\t\t    && sdp->device->removable) {\n\t\t\t/* Detected possible disc change. Set the bit - this */\n\t\t\t/* may be used if there are filesystems using this device */\n\t\t\tsdp->device->changed = 1;\n\t\t}\n\t}\n\n\tif (req->sense_len)\n\t\tmemcpy(srp->sense_b, req->sense, SCSI_SENSE_BUFFERSIZE);\n\n\t/* Rely on write phase to clean out srp status values, so no \"else\" */\n\n\t/*\n\t * Free the request as soon as it is complete so that its resources\n\t * can be reused without waiting for userspace to read() the\n\t * result.  But keep the associated bio (if any) around until\n\t * blk_rq_unmap_user() can be called from user context.\n\t */\n\tsrp->rq = NULL;\n\tscsi_req_free_cmd(scsi_req(rq));\n\t__blk_put_request(rq->q, rq);\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tif (unlikely(srp->orphan)) {\n\t\tif (sfp->keep_orphan)\n\t\t\tsrp->sg_io_owned = 0;\n\t\telse\n\t\t\tdone = 0;\n\t}\n\tsrp->done = done;\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (likely(done)) {\n\t\t/* Now wake up any sg_read() that is waiting for this\n\t\t * packet.\n\t\t */\n\t\twake_up_interruptible(&sfp->read_wait);\n\t\tkill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);\n\t\tkref_put(&sfp->f_ref, sg_remove_sfp);\n\t} else {\n\t\tINIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);\n\t\tschedule_work(&srp->ew.work);\n\t}\n}\n\nstatic const struct file_operations sg_fops = {\n\t.owner = THIS_MODULE,\n\t.read = sg_read,\n\t.write = sg_write,\n\t.poll = sg_poll,\n\t.unlocked_ioctl = sg_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = sg_compat_ioctl,\n#endif\n\t.open = sg_open,\n\t.mmap = sg_mmap,\n\t.release = sg_release,\n\t.fasync = sg_fasync,\n\t.llseek = no_llseek,\n};\n\nstatic struct class *sg_sysfs_class;\n\nstatic int sg_sysfs_valid = 0;\n\nstatic Sg_device *\nsg_alloc(struct gendisk *disk, struct scsi_device *scsidp)\n{\n\tstruct request_queue *q = scsidp->request_queue;\n\tSg_device *sdp;\n\tunsigned long iflags;\n\tint error;\n\tu32 k;\n\n\tsdp = kzalloc(sizeof(Sg_device), GFP_KERNEL);\n\tif (!sdp) {\n\t\tsdev_printk(KERN_WARNING, scsidp, \"%s: kmalloc Sg_device \"\n\t\t\t    \"failure\\n\", __func__);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tidr_preload(GFP_KERNEL);\n\twrite_lock_irqsave(&sg_index_lock, iflags);\n\n\terror = idr_alloc(&sg_index_idr, sdp, 0, SG_MAX_DEVS, GFP_NOWAIT);\n\tif (error < 0) {\n\t\tif (error == -ENOSPC) {\n\t\t\tsdev_printk(KERN_WARNING, scsidp,\n\t\t\t\t    \"Unable to attach sg device type=%d, minor number exceeds %d\\n\",\n\t\t\t\t    scsidp->type, SG_MAX_DEVS - 1);\n\t\t\terror = -ENODEV;\n\t\t} else {\n\t\t\tsdev_printk(KERN_WARNING, scsidp, \"%s: idr \"\n\t\t\t\t    \"allocation Sg_device failure: %d\\n\",\n\t\t\t\t    __func__, error);\n\t\t}\n\t\tgoto out_unlock;\n\t}\n\tk = error;\n\n\tSCSI_LOG_TIMEOUT(3, sdev_printk(KERN_INFO, scsidp,\n\t\t\t\t\t\"sg_alloc: dev=%d \\n\", k));\n\tsprintf(disk->disk_name, \"sg%d\", k);\n\tdisk->first_minor = k;\n\tsdp->disk = disk;\n\tsdp->device = scsidp;\n\tmutex_init(&sdp->open_rel_lock);\n\tINIT_LIST_HEAD(&sdp->sfds);\n\tinit_waitqueue_head(&sdp->open_wait);\n\tatomic_set(&sdp->detaching, 0);\n\trwlock_init(&sdp->sfd_lock);\n\tsdp->sg_tablesize = queue_max_segments(q);\n\tsdp->index = k;\n\tkref_init(&sdp->d_ref);\n\terror = 0;\n\nout_unlock:\n\twrite_unlock_irqrestore(&sg_index_lock, iflags);\n\tidr_preload_end();\n\n\tif (error) {\n\t\tkfree(sdp);\n\t\treturn ERR_PTR(error);\n\t}\n\treturn sdp;\n}\n\nstatic int\nsg_add_device(struct device *cl_dev, struct class_interface *cl_intf)\n{\n\tstruct scsi_device *scsidp = to_scsi_device(cl_dev->parent);\n\tstruct gendisk *disk;\n\tSg_device *sdp = NULL;\n\tstruct cdev * cdev = NULL;\n\tint error;\n\tunsigned long iflags;\n\n\tdisk = alloc_disk(1);\n\tif (!disk) {\n\t\tpr_warn(\"%s: alloc_disk failed\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tdisk->major = SCSI_GENERIC_MAJOR;\n\n\terror = -ENOMEM;\n\tcdev = cdev_alloc();\n\tif (!cdev) {\n\t\tpr_warn(\"%s: cdev_alloc failed\\n\", __func__);\n\t\tgoto out;\n\t}\n\tcdev->owner = THIS_MODULE;\n\tcdev->ops = &sg_fops;\n\n\tsdp = sg_alloc(disk, scsidp);\n\tif (IS_ERR(sdp)) {\n\t\tpr_warn(\"%s: sg_alloc failed\\n\", __func__);\n\t\terror = PTR_ERR(sdp);\n\t\tgoto out;\n\t}\n\n\terror = cdev_add(cdev, MKDEV(SCSI_GENERIC_MAJOR, sdp->index), 1);\n\tif (error)\n\t\tgoto cdev_add_err;\n\n\tsdp->cdev = cdev;\n\tif (sg_sysfs_valid) {\n\t\tstruct device *sg_class_member;\n\n\t\tsg_class_member = device_create(sg_sysfs_class, cl_dev->parent,\n\t\t\t\t\t\tMKDEV(SCSI_GENERIC_MAJOR,\n\t\t\t\t\t\t      sdp->index),\n\t\t\t\t\t\tsdp, \"%s\", disk->disk_name);\n\t\tif (IS_ERR(sg_class_member)) {\n\t\t\tpr_err(\"%s: device_create failed\\n\", __func__);\n\t\t\terror = PTR_ERR(sg_class_member);\n\t\t\tgoto cdev_add_err;\n\t\t}\n\t\terror = sysfs_create_link(&scsidp->sdev_gendev.kobj,\n\t\t\t\t\t  &sg_class_member->kobj, \"generic\");\n\t\tif (error)\n\t\t\tpr_err(\"%s: unable to make symlink 'generic' back \"\n\t\t\t       \"to sg%d\\n\", __func__, sdp->index);\n\t} else\n\t\tpr_warn(\"%s: sg_sys Invalid\\n\", __func__);\n\n\tsdev_printk(KERN_NOTICE, scsidp, \"Attached scsi generic sg%d \"\n\t\t    \"type %d\\n\", sdp->index, scsidp->type);\n\n\tdev_set_drvdata(cl_dev, sdp);\n\n\treturn 0;\n\ncdev_add_err:\n\twrite_lock_irqsave(&sg_index_lock, iflags);\n\tidr_remove(&sg_index_idr, sdp->index);\n\twrite_unlock_irqrestore(&sg_index_lock, iflags);\n\tkfree(sdp);\n\nout:\n\tput_disk(disk);\n\tif (cdev)\n\t\tcdev_del(cdev);\n\treturn error;\n}\n\nstatic void\nsg_device_destroy(struct kref *kref)\n{\n\tstruct sg_device *sdp = container_of(kref, struct sg_device, d_ref);\n\tunsigned long flags;\n\n\t/* CAUTION!  Note that the device can still be found via idr_find()\n\t * even though the refcount is 0.  Therefore, do idr_remove() BEFORE\n\t * any other cleanup.\n\t */\n\n\twrite_lock_irqsave(&sg_index_lock, flags);\n\tidr_remove(&sg_index_idr, sdp->index);\n\twrite_unlock_irqrestore(&sg_index_lock, flags);\n\n\tSCSI_LOG_TIMEOUT(3,\n\t\tsg_printk(KERN_INFO, sdp, \"sg_device_destroy\\n\"));\n\n\tput_disk(sdp->disk);\n\tkfree(sdp);\n}\n\nstatic void\nsg_remove_device(struct device *cl_dev, struct class_interface *cl_intf)\n{\n\tstruct scsi_device *scsidp = to_scsi_device(cl_dev->parent);\n\tSg_device *sdp = dev_get_drvdata(cl_dev);\n\tunsigned long iflags;\n\tSg_fd *sfp;\n\tint val;\n\n\tif (!sdp)\n\t\treturn;\n\t/* want sdp->detaching non-zero as soon as possible */\n\tval = atomic_inc_return(&sdp->detaching);\n\tif (val > 1)\n\t\treturn; /* only want to do following once per device */\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"%s\\n\", __func__));\n\n\tread_lock_irqsave(&sdp->sfd_lock, iflags);\n\tlist_for_each_entry(sfp, &sdp->sfds, sfd_siblings) {\n\t\twake_up_interruptible_all(&sfp->read_wait);\n\t\tkill_fasync(&sfp->async_qp, SIGPOLL, POLL_HUP);\n\t}\n\twake_up_interruptible_all(&sdp->open_wait);\n\tread_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\n\tsysfs_remove_link(&scsidp->sdev_gendev.kobj, \"generic\");\n\tdevice_destroy(sg_sysfs_class, MKDEV(SCSI_GENERIC_MAJOR, sdp->index));\n\tcdev_del(sdp->cdev);\n\tsdp->cdev = NULL;\n\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n}\n\nmodule_param_named(scatter_elem_sz, scatter_elem_sz, int, S_IRUGO | S_IWUSR);\nmodule_param_named(def_reserved_size, def_reserved_size, int,\n\t\t   S_IRUGO | S_IWUSR);\nmodule_param_named(allow_dio, sg_allow_dio, int, S_IRUGO | S_IWUSR);\n\nMODULE_AUTHOR(\"Douglas Gilbert\");\nMODULE_DESCRIPTION(\"SCSI generic (sg) driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(SG_VERSION_STR);\nMODULE_ALIAS_CHARDEV_MAJOR(SCSI_GENERIC_MAJOR);\n\nMODULE_PARM_DESC(scatter_elem_sz, \"scatter gather element \"\n                \"size (default: max(SG_SCATTER_SZ, PAGE_SIZE))\");\nMODULE_PARM_DESC(def_reserved_size, \"size of buffer reserved for each fd\");\nMODULE_PARM_DESC(allow_dio, \"allow direct I/O (default: 0 (disallow))\");\n\nstatic int __init\ninit_sg(void)\n{\n\tint rc;\n\n\tif (scatter_elem_sz < PAGE_SIZE) {\n\t\tscatter_elem_sz = PAGE_SIZE;\n\t\tscatter_elem_sz_prev = scatter_elem_sz;\n\t}\n\tif (def_reserved_size >= 0)\n\t\tsg_big_buff = def_reserved_size;\n\telse\n\t\tdef_reserved_size = sg_big_buff;\n\n\trc = register_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0), \n\t\t\t\t    SG_MAX_DEVS, \"sg\");\n\tif (rc)\n\t\treturn rc;\n        sg_sysfs_class = class_create(THIS_MODULE, \"scsi_generic\");\n        if ( IS_ERR(sg_sysfs_class) ) {\n\t\trc = PTR_ERR(sg_sysfs_class);\n\t\tgoto err_out;\n        }\n\tsg_sysfs_valid = 1;\n\trc = scsi_register_interface(&sg_interface);\n\tif (0 == rc) {\n#ifdef CONFIG_SCSI_PROC_FS\n\t\tsg_proc_init();\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\t\treturn 0;\n\t}\n\tclass_destroy(sg_sysfs_class);\nerr_out:\n\tunregister_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0), SG_MAX_DEVS);\n\treturn rc;\n}\n\nstatic void __exit\nexit_sg(void)\n{\n#ifdef CONFIG_SCSI_PROC_FS\n\tsg_proc_cleanup();\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\tscsi_unregister_interface(&sg_interface);\n\tclass_destroy(sg_sysfs_class);\n\tsg_sysfs_valid = 0;\n\tunregister_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0),\n\t\t\t\t SG_MAX_DEVS);\n\tidr_destroy(&sg_index_idr);\n}\n\nstatic int\nsg_start_req(Sg_request *srp, unsigned char *cmd)\n{\n\tint res;\n\tstruct request *rq;\n\tstruct scsi_request *req;\n\tSg_fd *sfp = srp->parentfp;\n\tsg_io_hdr_t *hp = &srp->header;\n\tint dxfer_len = (int) hp->dxfer_len;\n\tint dxfer_dir = hp->dxfer_direction;\n\tunsigned int iov_count = hp->iovec_count;\n\tSg_scatter_hold *req_schp = &srp->data;\n\tSg_scatter_hold *rsv_schp = &sfp->reserve;\n\tstruct request_queue *q = sfp->parentdp->device->request_queue;\n\tstruct rq_map_data *md, map_data;\n\tint rw = hp->dxfer_direction == SG_DXFER_TO_DEV ? WRITE : READ;\n\tunsigned char *long_cmdp = NULL;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_start_req: dxfer_len=%d\\n\",\n\t\t\t\t      dxfer_len));\n\n\tif (hp->cmd_len > BLK_MAX_CDB) {\n\t\tlong_cmdp = kzalloc(hp->cmd_len, GFP_KERNEL);\n\t\tif (!long_cmdp)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * NOTE\n\t *\n\t * With scsi-mq enabled, there are a fixed number of preallocated\n\t * requests equal in number to shost->can_queue.  If all of the\n\t * preallocated requests are already in use, then using GFP_ATOMIC with\n\t * blk_get_request() will return -EWOULDBLOCK, whereas using GFP_KERNEL\n\t * will cause blk_get_request() to sleep until an active command\n\t * completes, freeing up a request.  Neither option is ideal, but\n\t * GFP_KERNEL is the better choice to prevent userspace from getting an\n\t * unexpected EWOULDBLOCK.\n\t *\n\t * With scsi-mq disabled, blk_get_request() with GFP_KERNEL usually\n\t * does not sleep except under memory pressure.\n\t */\n\trq = blk_get_request(q, hp->dxfer_direction == SG_DXFER_TO_DEV ?\n\t\t\tREQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, GFP_KERNEL);\n\tif (IS_ERR(rq)) {\n\t\tkfree(long_cmdp);\n\t\treturn PTR_ERR(rq);\n\t}\n\treq = scsi_req(rq);\n\n\tif (hp->cmd_len > BLK_MAX_CDB)\n\t\treq->cmd = long_cmdp;\n\tmemcpy(req->cmd, cmd, hp->cmd_len);\n\treq->cmd_len = hp->cmd_len;\n\n\tsrp->rq = rq;\n\trq->end_io_data = srp;\n\treq->retries = SG_DEFAULT_RETRIES;\n\n\tif ((dxfer_len <= 0) || (dxfer_dir == SG_DXFER_NONE))\n\t\treturn 0;\n\n\tif (sg_allow_dio && hp->flags & SG_FLAG_DIRECT_IO &&\n\t    dxfer_dir != SG_DXFER_UNKNOWN && !iov_count &&\n\t    !sfp->parentdp->device->host->unchecked_isa_dma &&\n\t    blk_rq_aligned(q, (unsigned long)hp->dxferp, dxfer_len))\n\t\tmd = NULL;\n\telse\n\t\tmd = &map_data;\n\n\tif (md) {\n\t\tmutex_lock(&sfp->f_mutex);\n\t\tif (dxfer_len <= rsv_schp->bufflen &&\n\t\t    !sfp->res_in_use) {\n\t\t\tsfp->res_in_use = 1;\n\t\t\tsg_link_reserve(sfp, srp, dxfer_len);\n\t\t} else if (hp->flags & SG_FLAG_MMAP_IO) {\n\t\t\tres = -EBUSY; /* sfp->res_in_use == 1 */\n\t\t\tif (dxfer_len > rsv_schp->bufflen)\n\t\t\t\tres = -ENOMEM;\n\t\t\tmutex_unlock(&sfp->f_mutex);\n\t\t\treturn res;\n\t\t} else {\n\t\t\tres = sg_build_indirect(req_schp, sfp, dxfer_len);\n\t\t\tif (res) {\n\t\t\t\tmutex_unlock(&sfp->f_mutex);\n\t\t\t\treturn res;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&sfp->f_mutex);\n\n\t\tmd->pages = req_schp->pages;\n\t\tmd->page_order = req_schp->page_order;\n\t\tmd->nr_entries = req_schp->k_use_sg;\n\t\tmd->offset = 0;\n\t\tmd->null_mapped = hp->dxferp ? 0 : 1;\n\t\tif (dxfer_dir == SG_DXFER_TO_FROM_DEV)\n\t\t\tmd->from_user = 1;\n\t\telse\n\t\t\tmd->from_user = 0;\n\t}\n\n\tif (iov_count) {\n\t\tstruct iovec *iov = NULL;\n\t\tstruct iov_iter i;\n\n\t\tres = import_iovec(rw, hp->dxferp, iov_count, 0, &iov, &i);\n\t\tif (res < 0)\n\t\t\treturn res;\n\n\t\tiov_iter_truncate(&i, hp->dxfer_len);\n\t\tif (!iov_iter_count(&i)) {\n\t\t\tkfree(iov);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tres = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);\n\t\tkfree(iov);\n\t} else\n\t\tres = blk_rq_map_user(q, rq, md, hp->dxferp,\n\t\t\t\t      hp->dxfer_len, GFP_ATOMIC);\n\n\tif (!res) {\n\t\tsrp->bio = rq->bio;\n\n\t\tif (!md) {\n\t\t\treq_schp->dio_in_use = 1;\n\t\t\thp->info |= SG_INFO_DIRECT_IO;\n\t\t}\n\t}\n\treturn res;\n}\n\nstatic int\nsg_finish_rem_req(Sg_request *srp)\n{\n\tint ret = 0;\n\n\tSg_fd *sfp = srp->parentfp;\n\tSg_scatter_hold *req_schp = &srp->data;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_finish_rem_req: res_used=%d\\n\",\n\t\t\t\t      (int) srp->res_used));\n\tif (srp->bio)\n\t\tret = blk_rq_unmap_user(srp->bio);\n\n\tif (srp->rq) {\n\t\tscsi_req_free_cmd(scsi_req(srp->rq));\n\t\tblk_put_request(srp->rq);\n\t}\n\n\tif (srp->res_used)\n\t\tsg_unlink_reserve(sfp, srp);\n\telse\n\t\tsg_remove_scat(sfp, req_schp);\n\n\treturn ret;\n}\n\nstatic int\nsg_build_sgat(Sg_scatter_hold * schp, const Sg_fd * sfp, int tablesize)\n{\n\tint sg_bufflen = tablesize * sizeof(struct page *);\n\tgfp_t gfp_flags = GFP_ATOMIC | __GFP_NOWARN;\n\n\tschp->pages = kzalloc(sg_bufflen, gfp_flags);\n\tif (!schp->pages)\n\t\treturn -ENOMEM;\n\tschp->sglist_len = sg_bufflen;\n\treturn tablesize;\t/* number of scat_gath elements allocated */\n}\n\nstatic int\nsg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size)\n{\n\tint ret_sz = 0, i, k, rem_sz, num, mx_sc_elems;\n\tint sg_tablesize = sfp->parentdp->sg_tablesize;\n\tint blk_size = buff_size, order;\n\tgfp_t gfp_mask = GFP_ATOMIC | __GFP_COMP | __GFP_NOWARN;\n\tstruct sg_device *sdp = sfp->parentdp;\n\n\tif (blk_size < 0)\n\t\treturn -EFAULT;\n\tif (0 == blk_size)\n\t\t++blk_size;\t/* don't know why */\n\t/* round request up to next highest SG_SECTOR_SZ byte boundary */\n\tblk_size = ALIGN(blk_size, SG_SECTOR_SZ);\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\"sg_build_indirect: buff_size=%d, blk_size=%d\\n\",\n\t\tbuff_size, blk_size));\n\n\t/* N.B. ret_sz carried into this block ... */\n\tmx_sc_elems = sg_build_sgat(schp, sfp, sg_tablesize);\n\tif (mx_sc_elems < 0)\n\t\treturn mx_sc_elems;\t/* most likely -ENOMEM */\n\n\tnum = scatter_elem_sz;\n\tif (unlikely(num != scatter_elem_sz_prev)) {\n\t\tif (num < PAGE_SIZE) {\n\t\t\tscatter_elem_sz = PAGE_SIZE;\n\t\t\tscatter_elem_sz_prev = PAGE_SIZE;\n\t\t} else\n\t\t\tscatter_elem_sz_prev = num;\n\t}\n\n\tif (sdp->device->host->unchecked_isa_dma)\n\t\tgfp_mask |= GFP_DMA;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\tgfp_mask |= __GFP_ZERO;\n\n\torder = get_order(num);\nretry:\n\tret_sz = 1 << (PAGE_SHIFT + order);\n\n\tfor (k = 0, rem_sz = blk_size; rem_sz > 0 && k < mx_sc_elems;\n\t     k++, rem_sz -= ret_sz) {\n\n\t\tnum = (rem_sz > scatter_elem_sz_prev) ?\n\t\t\tscatter_elem_sz_prev : rem_sz;\n\n\t\tschp->pages[k] = alloc_pages(gfp_mask | __GFP_ZERO, order);\n\t\tif (!schp->pages[k])\n\t\t\tgoto out;\n\n\t\tif (num == scatter_elem_sz_prev) {\n\t\t\tif (unlikely(ret_sz > scatter_elem_sz_prev)) {\n\t\t\t\tscatter_elem_sz = ret_sz;\n\t\t\t\tscatter_elem_sz_prev = ret_sz;\n\t\t\t}\n\t\t}\n\n\t\tSCSI_LOG_TIMEOUT(5, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t \"sg_build_indirect: k=%d, num=%d, ret_sz=%d\\n\",\n\t\t\t\t k, num, ret_sz));\n\t}\t\t/* end of for loop */\n\n\tschp->page_order = order;\n\tschp->k_use_sg = k;\n\tSCSI_LOG_TIMEOUT(5, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_build_indirect: k_use_sg=%d, rem_sz=%d\\n\",\n\t\t\t k, rem_sz));\n\n\tschp->bufflen = blk_size;\n\tif (rem_sz > 0)\t/* must have failed */\n\t\treturn -ENOMEM;\n\treturn 0;\nout:\n\tfor (i = 0; i < k; i++)\n\t\t__free_pages(schp->pages[i], order);\n\n\tif (--order >= 0)\n\t\tgoto retry;\n\n\treturn -ENOMEM;\n}\n\nstatic void\nsg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp)\n{\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_remove_scat: k_use_sg=%d\\n\", schp->k_use_sg));\n\tif (schp->pages && schp->sglist_len > 0) {\n\t\tif (!schp->dio_in_use) {\n\t\t\tint k;\n\n\t\t\tfor (k = 0; k < schp->k_use_sg && schp->pages[k]; k++) {\n\t\t\t\tSCSI_LOG_TIMEOUT(5,\n\t\t\t\t\tsg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t\t\"sg_remove_scat: k=%d, pg=0x%p\\n\",\n\t\t\t\t\tk, schp->pages[k]));\n\t\t\t\t__free_pages(schp->pages[k], schp->page_order);\n\t\t\t}\n\n\t\t\tkfree(schp->pages);\n\t\t}\n\t}\n\tmemset(schp, 0, sizeof (*schp));\n}\n\nstatic int\nsg_read_oxfer(Sg_request * srp, char __user *outp, int num_read_xfer)\n{\n\tSg_scatter_hold *schp = &srp->data;\n\tint k, num;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, srp->parentfp->parentdp,\n\t\t\t \"sg_read_oxfer: num_read_xfer=%d\\n\",\n\t\t\t num_read_xfer));\n\tif ((!outp) || (num_read_xfer <= 0))\n\t\treturn 0;\n\n\tnum = 1 << (PAGE_SHIFT + schp->page_order);\n\tfor (k = 0; k < schp->k_use_sg && schp->pages[k]; k++) {\n\t\tif (num > num_read_xfer) {\n\t\t\tif (__copy_to_user(outp, page_address(schp->pages[k]),\n\t\t\t\t\t   num_read_xfer))\n\t\t\t\treturn -EFAULT;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tif (__copy_to_user(outp, page_address(schp->pages[k]),\n\t\t\t\t\t   num))\n\t\t\t\treturn -EFAULT;\n\t\t\tnum_read_xfer -= num;\n\t\t\tif (num_read_xfer <= 0)\n\t\t\t\tbreak;\n\t\t\toutp += num;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nsg_build_reserve(Sg_fd * sfp, int req_size)\n{\n\tSg_scatter_hold *schp = &sfp->reserve;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_build_reserve: req_size=%d\\n\", req_size));\n\tdo {\n\t\tif (req_size < PAGE_SIZE)\n\t\t\treq_size = PAGE_SIZE;\n\t\tif (0 == sg_build_indirect(schp, sfp, req_size))\n\t\t\treturn;\n\t\telse\n\t\t\tsg_remove_scat(sfp, schp);\n\t\treq_size >>= 1;\t/* divide by 2 */\n\t} while (req_size > (PAGE_SIZE / 2));\n}\n\nstatic void\nsg_link_reserve(Sg_fd * sfp, Sg_request * srp, int size)\n{\n\tSg_scatter_hold *req_schp = &srp->data;\n\tSg_scatter_hold *rsv_schp = &sfp->reserve;\n\tint k, num, rem;\n\n\tsrp->res_used = 1;\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_link_reserve: size=%d\\n\", size));\n\trem = size;\n\n\tnum = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg; k++) {\n\t\tif (rem <= num) {\n\t\t\treq_schp->k_use_sg = k + 1;\n\t\t\treq_schp->sglist_len = rsv_schp->sglist_len;\n\t\t\treq_schp->pages = rsv_schp->pages;\n\n\t\t\treq_schp->bufflen = size;\n\t\t\treq_schp->page_order = rsv_schp->page_order;\n\t\t\tbreak;\n\t\t} else\n\t\t\trem -= num;\n\t}\n\n\tif (k >= rsv_schp->k_use_sg)\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t \"sg_link_reserve: BAD size\\n\"));\n}\n\nstatic void\nsg_unlink_reserve(Sg_fd * sfp, Sg_request * srp)\n{\n\tSg_scatter_hold *req_schp = &srp->data;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, srp->parentfp->parentdp,\n\t\t\t\t      \"sg_unlink_reserve: req->k_use_sg=%d\\n\",\n\t\t\t\t      (int) req_schp->k_use_sg));\n\treq_schp->k_use_sg = 0;\n\treq_schp->bufflen = 0;\n\treq_schp->pages = NULL;\n\treq_schp->page_order = 0;\n\treq_schp->sglist_len = 0;\n\tsrp->res_used = 0;\n\t/* Called without mutex lock to avoid deadlock */\n\tsfp->res_in_use = 0;\n}\n\nstatic Sg_request *\nsg_get_rq_mark(Sg_fd * sfp, int pack_id)\n{\n\tSg_request *resp;\n\tunsigned long iflags;\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tlist_for_each_entry(resp, &sfp->rq_list, entry) {\n\t\t/* look for requests that are ready + not SG_IO owned */\n\t\tif ((1 == resp->done) && (!resp->sg_io_owned) &&\n\t\t    ((-1 == pack_id) || (resp->header.pack_id == pack_id))) {\n\t\t\tresp->done = 2;\t/* guard against other readers */\n\t\t\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t\treturn resp;\n\t\t}\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn NULL;\n}\n\n/* always adds to end of list */\nstatic Sg_request *\nsg_add_request(Sg_fd * sfp)\n{\n\tint k;\n\tunsigned long iflags;\n\tSg_request *rp = sfp->req_arr;\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tif (!list_empty(&sfp->rq_list)) {\n\t\tif (!sfp->cmd_q)\n\t\t\tgoto out_unlock;\n\n\t\tfor (k = 0; k < SG_MAX_QUEUE; ++k, ++rp) {\n\t\t\tif (!rp->parentfp)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (k >= SG_MAX_QUEUE)\n\t\t\tgoto out_unlock;\n\t}\n\tmemset(rp, 0, sizeof (Sg_request));\n\trp->parentfp = sfp;\n\trp->header.duration = jiffies_to_msecs(jiffies);\n\tlist_add_tail(&rp->entry, &sfp->rq_list);\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn rp;\nout_unlock:\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn NULL;\n}\n\n/* Return of 1 for found; 0 for not found */\nstatic int\nsg_remove_request(Sg_fd * sfp, Sg_request * srp)\n{\n\tunsigned long iflags;\n\tint res = 0;\n\n\tif (!sfp || !srp || list_empty(&sfp->rq_list))\n\t\treturn res;\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tif (!list_empty(&srp->entry)) {\n\t\tlist_del(&srp->entry);\n\t\tsrp->parentfp = NULL;\n\t\tres = 1;\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn res;\n}\n\nstatic Sg_fd *\nsg_add_sfp(Sg_device * sdp)\n{\n\tSg_fd *sfp;\n\tunsigned long iflags;\n\tint bufflen;\n\n\tsfp = kzalloc(sizeof(*sfp), GFP_ATOMIC | __GFP_NOWARN);\n\tif (!sfp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinit_waitqueue_head(&sfp->read_wait);\n\trwlock_init(&sfp->rq_list_lock);\n\tINIT_LIST_HEAD(&sfp->rq_list);\n\tkref_init(&sfp->f_ref);\n\tmutex_init(&sfp->f_mutex);\n\tsfp->timeout = SG_DEFAULT_TIMEOUT;\n\tsfp->timeout_user = SG_DEFAULT_TIMEOUT_USER;\n\tsfp->force_packid = SG_DEF_FORCE_PACK_ID;\n\tsfp->cmd_q = SG_DEF_COMMAND_Q;\n\tsfp->keep_orphan = SG_DEF_KEEP_ORPHAN;\n\tsfp->parentdp = sdp;\n\twrite_lock_irqsave(&sdp->sfd_lock, iflags);\n\tif (atomic_read(&sdp->detaching)) {\n\t\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\tlist_add_tail(&sfp->sfd_siblings, &sdp->sfds);\n\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_add_sfp: sfp=0x%p\\n\", sfp));\n\tif (unlikely(sg_big_buff != def_reserved_size))\n\t\tsg_big_buff = def_reserved_size;\n\n\tbufflen = min_t(int, sg_big_buff,\n\t\t\tmax_sectors_bytes(sdp->device->request_queue));\n\tsg_build_reserve(sfp, bufflen);\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_add_sfp: bufflen=%d, k_use_sg=%d\\n\",\n\t\t\t\t      sfp->reserve.bufflen,\n\t\t\t\t      sfp->reserve.k_use_sg));\n\n\tkref_get(&sdp->d_ref);\n\t__module_get(THIS_MODULE);\n\treturn sfp;\n}\n\nstatic void\nsg_remove_sfp_usercontext(struct work_struct *work)\n{\n\tstruct sg_fd *sfp = container_of(work, struct sg_fd, ew.work);\n\tstruct sg_device *sdp = sfp->parentdp;\n\tSg_request *srp;\n\tunsigned long iflags;\n\n\t/* Cleanup any responses which were never read(). */\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\twhile (!list_empty(&sfp->rq_list)) {\n\t\tsrp = list_first_entry(&sfp->rq_list, Sg_request, entry);\n\t\tsg_finish_rem_req(srp);\n\t\tlist_del(&srp->entry);\n\t\tsrp->parentfp = NULL;\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (sfp->reserve.bufflen > 0) {\n\t\tSCSI_LOG_TIMEOUT(6, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\"sg_remove_sfp:    bufflen=%d, k_use_sg=%d\\n\",\n\t\t\t\t(int) sfp->reserve.bufflen,\n\t\t\t\t(int) sfp->reserve.k_use_sg));\n\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t}\n\n\tSCSI_LOG_TIMEOUT(6, sg_printk(KERN_INFO, sdp,\n\t\t\t\"sg_remove_sfp: sfp=0x%p\\n\", sfp));\n\tkfree(sfp);\n\n\tscsi_device_put(sdp->device);\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n\tmodule_put(THIS_MODULE);\n}\n\nstatic void\nsg_remove_sfp(struct kref *kref)\n{\n\tstruct sg_fd *sfp = container_of(kref, struct sg_fd, f_ref);\n\tstruct sg_device *sdp = sfp->parentdp;\n\tunsigned long iflags;\n\n\twrite_lock_irqsave(&sdp->sfd_lock, iflags);\n\tlist_del(&sfp->sfd_siblings);\n\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\n\tINIT_WORK(&sfp->ew.work, sg_remove_sfp_usercontext);\n\tschedule_work(&sfp->ew.work);\n}\n\n#ifdef CONFIG_SCSI_PROC_FS\nstatic int\nsg_idr_max_id(int id, void *p, void *data)\n{\n\tint *k = data;\n\n\tif (*k < id)\n\t\t*k = id;\n\n\treturn 0;\n}\n\nstatic int\nsg_last_dev(void)\n{\n\tint k = -1;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tidr_for_each(&sg_index_idr, sg_idr_max_id, &k);\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn k + 1;\t\t/* origin 1 */\n}\n#endif\n\n/* must be called with sg_index_lock held */\nstatic Sg_device *sg_lookup_dev(int dev)\n{\n\treturn idr_find(&sg_index_idr, dev);\n}\n\nstatic Sg_device *\nsg_get_dev(int dev)\n{\n\tstruct sg_device *sdp;\n\tunsigned long flags;\n\n\tread_lock_irqsave(&sg_index_lock, flags);\n\tsdp = sg_lookup_dev(dev);\n\tif (!sdp)\n\t\tsdp = ERR_PTR(-ENXIO);\n\telse if (atomic_read(&sdp->detaching)) {\n\t\t/* If sdp->detaching, then the refcount may already be 0, in\n\t\t * which case it would be a bug to do kref_get().\n\t\t */\n\t\tsdp = ERR_PTR(-ENODEV);\n\t} else\n\t\tkref_get(&sdp->d_ref);\n\tread_unlock_irqrestore(&sg_index_lock, flags);\n\n\treturn sdp;\n}\n\n#ifdef CONFIG_SCSI_PROC_FS\n\nstatic struct proc_dir_entry *sg_proc_sgp = NULL;\n\nstatic char sg_proc_sg_dirname[] = \"scsi/sg\";\n\nstatic int sg_proc_seq_show_int(struct seq_file *s, void *v);\n\nstatic int sg_proc_single_open_adio(struct inode *inode, struct file *file);\nstatic ssize_t sg_proc_write_adio(struct file *filp, const char __user *buffer,\n\t\t\t          size_t count, loff_t *off);\nstatic const struct file_operations adio_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_adio,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.write = sg_proc_write_adio,\n\t.release = single_release,\n};\n\nstatic int sg_proc_single_open_dressz(struct inode *inode, struct file *file);\nstatic ssize_t sg_proc_write_dressz(struct file *filp, \n\t\tconst char __user *buffer, size_t count, loff_t *off);\nstatic const struct file_operations dressz_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_dressz,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.write = sg_proc_write_dressz,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_version(struct seq_file *s, void *v);\nstatic int sg_proc_single_open_version(struct inode *inode, struct file *file);\nstatic const struct file_operations version_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_version,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_devhdr(struct seq_file *s, void *v);\nstatic int sg_proc_single_open_devhdr(struct inode *inode, struct file *file);\nstatic const struct file_operations devhdr_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_devhdr,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_dev(struct seq_file *s, void *v);\nstatic int sg_proc_open_dev(struct inode *inode, struct file *file);\nstatic void * dev_seq_start(struct seq_file *s, loff_t *pos);\nstatic void * dev_seq_next(struct seq_file *s, void *v, loff_t *pos);\nstatic void dev_seq_stop(struct seq_file *s, void *v);\nstatic const struct file_operations dev_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_dev,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations dev_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_dev,\n};\n\nstatic int sg_proc_seq_show_devstrs(struct seq_file *s, void *v);\nstatic int sg_proc_open_devstrs(struct inode *inode, struct file *file);\nstatic const struct file_operations devstrs_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_devstrs,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations devstrs_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_devstrs,\n};\n\nstatic int sg_proc_seq_show_debug(struct seq_file *s, void *v);\nstatic int sg_proc_open_debug(struct inode *inode, struct file *file);\nstatic const struct file_operations debug_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_debug,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations debug_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_debug,\n};\n\n\nstruct sg_proc_leaf {\n\tconst char * name;\n\tconst struct file_operations * fops;\n};\n\nstatic const struct sg_proc_leaf sg_proc_leaf_arr[] = {\n\t{\"allow_dio\", &adio_fops},\n\t{\"debug\", &debug_fops},\n\t{\"def_reserved_size\", &dressz_fops},\n\t{\"device_hdr\", &devhdr_fops},\n\t{\"devices\", &dev_fops},\n\t{\"device_strs\", &devstrs_fops},\n\t{\"version\", &version_fops}\n};\n\nstatic int\nsg_proc_init(void)\n{\n\tint num_leaves = ARRAY_SIZE(sg_proc_leaf_arr);\n\tint k;\n\n\tsg_proc_sgp = proc_mkdir(sg_proc_sg_dirname, NULL);\n\tif (!sg_proc_sgp)\n\t\treturn 1;\n\tfor (k = 0; k < num_leaves; ++k) {\n\t\tconst struct sg_proc_leaf *leaf = &sg_proc_leaf_arr[k];\n\t\tumode_t mask = leaf->fops->write ? S_IRUGO | S_IWUSR : S_IRUGO;\n\t\tproc_create(leaf->name, mask, sg_proc_sgp, leaf->fops);\n\t}\n\treturn 0;\n}\n\nstatic void\nsg_proc_cleanup(void)\n{\n\tint k;\n\tint num_leaves = ARRAY_SIZE(sg_proc_leaf_arr);\n\n\tif (!sg_proc_sgp)\n\t\treturn;\n\tfor (k = 0; k < num_leaves; ++k)\n\t\tremove_proc_entry(sg_proc_leaf_arr[k].name, sg_proc_sgp);\n\tremove_proc_entry(sg_proc_sg_dirname, NULL);\n}\n\n\nstatic int sg_proc_seq_show_int(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", *((int *)s->private));\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_adio(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_int, &sg_allow_dio);\n}\n\nstatic ssize_t \nsg_proc_write_adio(struct file *filp, const char __user *buffer,\n\t\t   size_t count, loff_t *off)\n{\n\tint err;\n\tunsigned long num;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\terr = kstrtoul_from_user(buffer, count, 0, &num);\n\tif (err)\n\t\treturn err;\n\tsg_allow_dio = num ? 1 : 0;\n\treturn count;\n}\n\nstatic int sg_proc_single_open_dressz(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_int, &sg_big_buff);\n}\n\nstatic ssize_t \nsg_proc_write_dressz(struct file *filp, const char __user *buffer,\n\t\t     size_t count, loff_t *off)\n{\n\tint err;\n\tunsigned long k = ULONG_MAX;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\n\terr = kstrtoul_from_user(buffer, count, 0, &k);\n\tif (err)\n\t\treturn err;\n\tif (k <= 1048576) {\t/* limit \"big buff\" to 1 MB */\n\t\tsg_big_buff = k;\n\t\treturn count;\n\t}\n\treturn -ERANGE;\n}\n\nstatic int sg_proc_seq_show_version(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\t%s [%s]\\n\", sg_version_num, SG_VERSION_STR,\n\t\t   sg_version_date);\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_version(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_version, NULL);\n}\n\nstatic int sg_proc_seq_show_devhdr(struct seq_file *s, void *v)\n{\n\tseq_puts(s, \"host\\tchan\\tid\\tlun\\ttype\\topens\\tqdepth\\tbusy\\tonline\\n\");\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_devhdr(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_devhdr, NULL);\n}\n\nstruct sg_proc_deviter {\n\tloff_t\tindex;\n\tsize_t\tmax;\n};\n\nstatic void * dev_seq_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct sg_proc_deviter * it = kmalloc(sizeof(*it), GFP_KERNEL);\n\n\ts->private = it;\n\tif (! it)\n\t\treturn NULL;\n\n\tit->index = *pos;\n\tit->max = sg_last_dev();\n\tif (it->index >= it->max)\n\t\treturn NULL;\n\treturn it;\n}\n\nstatic void * dev_seq_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct sg_proc_deviter * it = s->private;\n\n\t*pos = ++it->index;\n\treturn (it->index < it->max) ? it : NULL;\n}\n\nstatic void dev_seq_stop(struct seq_file *s, void *v)\n{\n\tkfree(s->private);\n}\n\nstatic int sg_proc_open_dev(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &dev_seq_ops);\n}\n\nstatic int sg_proc_seq_show_dev(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tstruct scsi_device *scsidp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tif ((NULL == sdp) || (NULL == sdp->device) ||\n\t    (atomic_read(&sdp->detaching)))\n\t\tseq_puts(s, \"-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\n\");\n\telse {\n\t\tscsidp = sdp->device;\n\t\tseq_printf(s, \"%d\\t%d\\t%d\\t%llu\\t%d\\t%d\\t%d\\t%d\\t%d\\n\",\n\t\t\t      scsidp->host->host_no, scsidp->channel,\n\t\t\t      scsidp->id, scsidp->lun, (int) scsidp->type,\n\t\t\t      1,\n\t\t\t      (int) scsidp->queue_depth,\n\t\t\t      (int) atomic_read(&scsidp->device_busy),\n\t\t\t      (int) scsi_device_online(scsidp));\n\t}\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\nstatic int sg_proc_open_devstrs(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &devstrs_seq_ops);\n}\n\nstatic int sg_proc_seq_show_devstrs(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tstruct scsi_device *scsidp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tscsidp = sdp ? sdp->device : NULL;\n\tif (sdp && scsidp && (!atomic_read(&sdp->detaching)))\n\t\tseq_printf(s, \"%8.8s\\t%16.16s\\t%4.4s\\n\",\n\t\t\t   scsidp->vendor, scsidp->model, scsidp->rev);\n\telse\n\t\tseq_puts(s, \"<no active device>\\n\");\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\n/* must be called while holding sg_index_lock */\nstatic void sg_proc_debug_helper(struct seq_file *s, Sg_device * sdp)\n{\n\tint k, new_interface, blen, usg;\n\tSg_request *srp;\n\tSg_fd *fp;\n\tconst sg_io_hdr_t *hp;\n\tconst char * cp;\n\tunsigned int ms;\n\n\tk = 0;\n\tlist_for_each_entry(fp, &sdp->sfds, sfd_siblings) {\n\t\tk++;\n\t\tread_lock(&fp->rq_list_lock); /* irqs already disabled */\n\t\tseq_printf(s, \"   FD(%d): timeout=%dms bufflen=%d \"\n\t\t\t   \"(res)sgat=%d low_dma=%d\\n\", k,\n\t\t\t   jiffies_to_msecs(fp->timeout),\n\t\t\t   fp->reserve.bufflen,\n\t\t\t   (int) fp->reserve.k_use_sg,\n\t\t\t   (int) sdp->device->host->unchecked_isa_dma);\n\t\tseq_printf(s, \"   cmd_q=%d f_packid=%d k_orphan=%d closed=0\\n\",\n\t\t\t   (int) fp->cmd_q, (int) fp->force_packid,\n\t\t\t   (int) fp->keep_orphan);\n\t\tlist_for_each_entry(srp, &fp->rq_list, entry) {\n\t\t\thp = &srp->header;\n\t\t\tnew_interface = (hp->interface_id == '\\0') ? 0 : 1;\n\t\t\tif (srp->res_used) {\n\t\t\t\tif (new_interface &&\n\t\t\t\t    (SG_FLAG_MMAP_IO & hp->flags))\n\t\t\t\t\tcp = \"     mmap>> \";\n\t\t\t\telse\n\t\t\t\t\tcp = \"     rb>> \";\n\t\t\t} else {\n\t\t\t\tif (SG_INFO_DIRECT_IO_MASK & hp->info)\n\t\t\t\t\tcp = \"     dio>> \";\n\t\t\t\telse\n\t\t\t\t\tcp = \"     \";\n\t\t\t}\n\t\t\tseq_puts(s, cp);\n\t\t\tblen = srp->data.bufflen;\n\t\t\tusg = srp->data.k_use_sg;\n\t\t\tseq_puts(s, srp->done ?\n\t\t\t\t ((1 == srp->done) ?  \"rcv:\" : \"fin:\")\n\t\t\t\t  : \"act:\");\n\t\t\tseq_printf(s, \" id=%d blen=%d\",\n\t\t\t\t   srp->header.pack_id, blen);\n\t\t\tif (srp->done)\n\t\t\t\tseq_printf(s, \" dur=%d\", hp->duration);\n\t\t\telse {\n\t\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\t\tseq_printf(s, \" t_o/elap=%d/%d\",\n\t\t\t\t\t(new_interface ? hp->timeout :\n\t\t\t\t\t\t  jiffies_to_msecs(fp->timeout)),\n\t\t\t\t\t(ms > hp->duration ? ms - hp->duration : 0));\n\t\t\t}\n\t\t\tseq_printf(s, \"ms sgat=%d op=0x%02x\\n\", usg,\n\t\t\t\t   (int) srp->data.cmd_opcode);\n\t\t}\n\t\tif (list_empty(&fp->rq_list))\n\t\t\tseq_puts(s, \"     No requests active\\n\");\n\t\tread_unlock(&fp->rq_list_lock);\n\t}\n}\n\nstatic int sg_proc_open_debug(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &debug_seq_ops);\n}\n\nstatic int sg_proc_seq_show_debug(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tunsigned long iflags;\n\n\tif (it && (0 == it->index))\n\t\tseq_printf(s, \"max_active_device=%d  def_reserved_size=%d\\n\",\n\t\t\t   (int)it->max, sg_big_buff);\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tif (NULL == sdp)\n\t\tgoto skip;\n\tread_lock(&sdp->sfd_lock);\n\tif (!list_empty(&sdp->sfds)) {\n\t\tseq_printf(s, \" >>> device=%s \", sdp->disk->disk_name);\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\tseq_puts(s, \"detaching pending close \");\n\t\telse if (sdp->device) {\n\t\t\tstruct scsi_device *scsidp = sdp->device;\n\n\t\t\tseq_printf(s, \"%d:%d:%d:%llu   em=%d\",\n\t\t\t\t   scsidp->host->host_no,\n\t\t\t\t   scsidp->channel, scsidp->id,\n\t\t\t\t   scsidp->lun,\n\t\t\t\t   scsidp->host->hostt->emulated);\n\t\t}\n\t\tseq_printf(s, \" sg_tablesize=%d excl=%d open_cnt=%d\\n\",\n\t\t\t   sdp->sg_tablesize, sdp->exclude, sdp->open_cnt);\n\t\tsg_proc_debug_helper(s, sdp);\n\t}\n\tread_unlock(&sdp->sfd_lock);\nskip:\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\nmodule_init(init_sg);\nmodule_exit(exit_sg);\n"], "filenames": ["drivers/scsi/sg.c"], "buggy_code_start_loc": [1897], "buggy_code_end_loc": [1898], "fixing_code_start_loc": [1897], "fixing_code_end_loc": [1898], "type": "NVD-CWE-noinfo", "message": "** DISPUTED ** Linux Kernel version 3.18 to 4.16 incorrectly handles an SG_IO ioctl on /dev/sg0 with dxfer_direction=SG_DXFER_FROM_DEV and an empty 6-byte cmdp. This may lead to copying up to 1000 kernel heap pages to the userspace. This has been fixed upstream in https://github.com/torvalds/linux/commit/a45b599ad808c3c982fdcdc12b0b8611c2f92824 already. The problem has limited scope, as users don't usually have permissions to access SCSI devices. On the other hand, e.g. the Nero user manual suggests doing `chmod o+r+w /dev/sg*` to make the devices accessible. NOTE: third parties dispute the relevance of this report, noting that the requirement for an attacker to have both the CAP_SYS_ADMIN and CAP_SYS_RAWIO capabilities makes it \"virtually impossible to exploit.\"", "other": {"cve": {"id": "CVE-2018-1000204", "sourceIdentifier": "cve@mitre.org", "published": "2018-06-26T14:29:02.160", "lastModified": "2019-10-03T00:03:26.223", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "** DISPUTED ** Linux Kernel version 3.18 to 4.16 incorrectly handles an SG_IO ioctl on /dev/sg0 with dxfer_direction=SG_DXFER_FROM_DEV and an empty 6-byte cmdp. This may lead to copying up to 1000 kernel heap pages to the userspace. This has been fixed upstream in https://github.com/torvalds/linux/commit/a45b599ad808c3c982fdcdc12b0b8611c2f92824 already. The problem has limited scope, as users don't usually have permissions to access SCSI devices. On the other hand, e.g. the Nero user manual suggests doing `chmod o+r+w /dev/sg*` to make the devices accessible. NOTE: third parties dispute the relevance of this report, noting that the requirement for an attacker to have both the CAP_SYS_ADMIN and CAP_SYS_RAWIO capabilities makes it \"virtually impossible to exploit.\""}, {"lang": "es", "value": "** EN DISPUTA ** El kernel de Linux desde la versi\u00f3n 3.18 hasta la 4.16 manipula incorrectamente una llamada IOCTL SG_IO en /dev/sg0 con dxfer_direction=SG_DXFER_FROM_DEV y un cmdp de 6 bytes vac\u00edo. Esto puede permitir que se copien hasta 1000 p\u00e1ginas de la memoria din\u00e1mica (heap) del kernel al espacio de usuario. Esto ya se ha resuelto en versiones upstream en https://github.com/torvalds/linux/commit/a45b599ad808c3c982fdcdc12b0b8611c2f92824. El problema tiene un alcance limitado, ya que los usuarios no suelen tener permisos para acceder a los dispositivos SCSI. Por otro lado, por ejemplo, el manual de usuario de Nero sugiere hacer \"chmod o+r+w /dev/sg*\" para que los dispositivos sean accesibles. NOTA: terceros cuestionan la relevancia de este informe, se\u00f1alando que el requisito de que un atacante tenga las capacidades CAP_SYS_ADMIN y CAP_SYS_RAWIO lo hace \"virtualmente imposible de explotar\"."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.6, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:S/C:C/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "SINGLE", "confidentialityImpact": "COMPLETE", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 6.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 6.8, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.18", "versionEndIncluding": "4.16", "matchCriteriaId": "1C9DBE4B-803A-403C-87F1-C05E73CBC60F"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-05/msg00043.html", "source": "cve@mitre.org"}, {"url": "http://www.openwall.com/lists/oss-security/2018/06/26/3", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2948", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/a45b599ad808c3c982fdcdc12b0b8611c2f92824", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2018/07/msg00015.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2018/07/msg00016.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2018/07/msg00020.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3696-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3696-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3752-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3752-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3752-3/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3754-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/a45b599ad808c3c982fdcdc12b0b8611c2f92824"}}