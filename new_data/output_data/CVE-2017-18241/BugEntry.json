{"buggy_code": ["/*\n * fs/f2fs/segment.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/prefetch.h>\n#include <linux/kthread.h>\n#include <linux/swap.h>\n#include <linux/timer.h>\n#include <linux/freezer.h>\n\n#include \"f2fs.h\"\n#include \"segment.h\"\n#include \"node.h\"\n#include \"trace.h\"\n#include <trace/events/f2fs.h>\n\n#define __reverse_ffz(x) __reverse_ffs(~(x))\n\nstatic struct kmem_cache *discard_entry_slab;\nstatic struct kmem_cache *discard_cmd_slab;\nstatic struct kmem_cache *sit_entry_set_slab;\nstatic struct kmem_cache *inmem_entry_slab;\n\nstatic unsigned long __reverse_ulong(unsigned char *str)\n{\n\tunsigned long tmp = 0;\n\tint shift = 24, idx = 0;\n\n#if BITS_PER_LONG == 64\n\tshift = 56;\n#endif\n\twhile (shift >= 0) {\n\t\ttmp |= (unsigned long)str[idx++] << shift;\n\t\tshift -= BITS_PER_BYTE;\n\t}\n\treturn tmp;\n}\n\n/*\n * __reverse_ffs is copied from include/asm-generic/bitops/__ffs.h since\n * MSB and LSB are reversed in a byte by f2fs_set_bit.\n */\nstatic inline unsigned long __reverse_ffs(unsigned long word)\n{\n\tint num = 0;\n\n#if BITS_PER_LONG == 64\n\tif ((word & 0xffffffff00000000UL) == 0)\n\t\tnum += 32;\n\telse\n\t\tword >>= 32;\n#endif\n\tif ((word & 0xffff0000) == 0)\n\t\tnum += 16;\n\telse\n\t\tword >>= 16;\n\n\tif ((word & 0xff00) == 0)\n\t\tnum += 8;\n\telse\n\t\tword >>= 8;\n\n\tif ((word & 0xf0) == 0)\n\t\tnum += 4;\n\telse\n\t\tword >>= 4;\n\n\tif ((word & 0xc) == 0)\n\t\tnum += 2;\n\telse\n\t\tword >>= 2;\n\n\tif ((word & 0x2) == 0)\n\t\tnum += 1;\n\treturn num;\n}\n\n/*\n * __find_rev_next(_zero)_bit is copied from lib/find_next_bit.c because\n * f2fs_set_bit makes MSB and LSB reversed in a byte.\n * @size must be integral times of unsigned long.\n * Example:\n *                             MSB <--> LSB\n *   f2fs_set_bit(0, bitmap) => 1000 0000\n *   f2fs_set_bit(7, bitmap) => 0000 0001\n */\nstatic unsigned long __find_rev_next_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == 0)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\ttmp &= ~0UL >> offset;\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp &= (~0UL << (BITS_PER_LONG - size));\n\t\tif (tmp)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffs(tmp);\n}\n\nstatic unsigned long __find_rev_next_zero_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == ~0UL)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\tif (offset)\n\t\t\ttmp |= ~0UL << (BITS_PER_LONG - offset);\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp |= ~0UL >> size;\n\t\tif (tmp != ~0UL)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffz(tmp);\n}\n\nvoid register_inmem_page(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inmem_pages *new;\n\n\tf2fs_trace_pid(page);\n\n\tset_page_private(page, (unsigned long)ATOMIC_WRITTEN_PAGE);\n\tSetPagePrivate(page);\n\n\tnew = f2fs_kmem_cache_alloc(inmem_entry_slab, GFP_NOFS);\n\n\t/* add atomic page indices to the list */\n\tnew->page = page;\n\tINIT_LIST_HEAD(&new->list);\n\n\t/* increase reference count with clean state */\n\tmutex_lock(&fi->inmem_lock);\n\tget_page(page);\n\tlist_add_tail(&new->list, &fi->inmem_pages);\n\tinc_page_count(F2FS_I_SB(inode), F2FS_INMEM_PAGES);\n\tmutex_unlock(&fi->inmem_lock);\n\n\ttrace_f2fs_register_inmem_page(page, INMEM);\n}\n\nstatic int __revoke_inmem_pages(struct inode *inode,\n\t\t\t\tstruct list_head *head, bool drop, bool recover)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct inmem_pages *cur, *tmp;\n\tint err = 0;\n\n\tlist_for_each_entry_safe(cur, tmp, head, list) {\n\t\tstruct page *page = cur->page;\n\n\t\tif (drop)\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM_DROP);\n\n\t\tlock_page(page);\n\n\t\tif (recover) {\n\t\t\tstruct dnode_of_data dn;\n\t\t\tstruct node_info ni;\n\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM_REVOKE);\n\n\t\t\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\t\t\tif (get_dnode_of_data(&dn, page->index, LOOKUP_NODE)) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tget_node_info(sbi, dn.nid, &ni);\n\t\t\tf2fs_replace_block(sbi, &dn, dn.data_blkaddr,\n\t\t\t\t\tcur->old_addr, ni.version, true, true);\n\t\t\tf2fs_put_dnode(&dn);\n\t\t}\nnext:\n\t\t/* we don't need to invalidate this in the sccessful status */\n\t\tif (drop || recover)\n\t\t\tClearPageUptodate(page);\n\t\tset_page_private(page, 0);\n\t\tClearPagePrivate(page);\n\t\tf2fs_put_page(page, 1);\n\n\t\tlist_del(&cur->list);\n\t\tkmem_cache_free(inmem_entry_slab, cur);\n\t\tdec_page_count(F2FS_I_SB(inode), F2FS_INMEM_PAGES);\n\t}\n\treturn err;\n}\n\nvoid drop_inmem_pages(struct inode *inode)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tmutex_lock(&fi->inmem_lock);\n\t__revoke_inmem_pages(inode, &fi->inmem_pages, true, false);\n\tmutex_unlock(&fi->inmem_lock);\n\n\tclear_inode_flag(inode, FI_ATOMIC_FILE);\n\tstat_dec_atomic_write(inode);\n}\n\nvoid drop_inmem_page(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct list_head *head = &fi->inmem_pages;\n\tstruct inmem_pages *cur = NULL;\n\n\tf2fs_bug_on(sbi, !IS_ATOMIC_WRITTEN_PAGE(page));\n\n\tmutex_lock(&fi->inmem_lock);\n\tlist_for_each_entry(cur, head, list) {\n\t\tif (cur->page == page)\n\t\t\tbreak;\n\t}\n\n\tf2fs_bug_on(sbi, !cur || cur->page != page);\n\tlist_del(&cur->list);\n\tmutex_unlock(&fi->inmem_lock);\n\n\tdec_page_count(sbi, F2FS_INMEM_PAGES);\n\tkmem_cache_free(inmem_entry_slab, cur);\n\n\tClearPageUptodate(page);\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n\tf2fs_put_page(page, 0);\n\n\ttrace_f2fs_commit_inmem_page(page, INMEM_INVALIDATE);\n}\n\nstatic int __commit_inmem_pages(struct inode *inode,\n\t\t\t\t\tstruct list_head *revoke_list)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inmem_pages *cur, *tmp;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_PRIO,\n\t};\n\tpgoff_t last_idx = ULONG_MAX;\n\tint err = 0;\n\n\tlist_for_each_entry_safe(cur, tmp, &fi->inmem_pages, list) {\n\t\tstruct page *page = cur->page;\n\n\t\tlock_page(page);\n\t\tif (page->mapping == inode->i_mapping) {\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM);\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_wait_on_page_writeback(page, DATA, true);\n\t\t\tif (clear_page_dirty_for_io(page)) {\n\t\t\t\tinode_dec_dirty_pages(inode);\n\t\t\t\tremove_dirty_inode(inode);\n\t\t\t}\n\n\t\t\tfio.page = page;\n\t\t\tfio.old_blkaddr = NULL_ADDR;\n\t\t\tfio.encrypted_page = NULL;\n\t\t\tfio.need_lock = LOCK_DONE;\n\t\t\terr = do_write_data_page(&fio);\n\t\t\tif (err) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* record old blkaddr for revoking */\n\t\t\tcur->old_addr = fio.old_blkaddr;\n\t\t\tlast_idx = page->index;\n\t\t}\n\t\tunlock_page(page);\n\t\tlist_move_tail(&cur->list, revoke_list);\n\t}\n\n\tif (last_idx != ULONG_MAX)\n\t\tf2fs_submit_merged_write_cond(sbi, inode, 0, last_idx, DATA);\n\n\tif (!err)\n\t\t__revoke_inmem_pages(inode, revoke_list, false, false);\n\n\treturn err;\n}\n\nint commit_inmem_pages(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct list_head revoke_list;\n\tint err;\n\n\tINIT_LIST_HEAD(&revoke_list);\n\tf2fs_balance_fs(sbi, true);\n\tf2fs_lock_op(sbi);\n\n\tset_inode_flag(inode, FI_ATOMIC_COMMIT);\n\n\tmutex_lock(&fi->inmem_lock);\n\terr = __commit_inmem_pages(inode, &revoke_list);\n\tif (err) {\n\t\tint ret;\n\t\t/*\n\t\t * try to revoke all committed pages, but still we could fail\n\t\t * due to no memory or other reason, if that happened, EAGAIN\n\t\t * will be returned, which means in such case, transaction is\n\t\t * already not integrity, caller should use journal to do the\n\t\t * recovery or rewrite & commit last transaction. For other\n\t\t * error number, revoking was done by filesystem itself.\n\t\t */\n\t\tret = __revoke_inmem_pages(inode, &revoke_list, false, true);\n\t\tif (ret)\n\t\t\terr = ret;\n\n\t\t/* drop all uncommitted pages */\n\t\t__revoke_inmem_pages(inode, &fi->inmem_pages, true, false);\n\t}\n\tmutex_unlock(&fi->inmem_lock);\n\n\tclear_inode_flag(inode, FI_ATOMIC_COMMIT);\n\n\tf2fs_unlock_op(sbi);\n\treturn err;\n}\n\n/*\n * This function balances dirty node and dentry pages.\n * In addition, it controls garbage collection.\n */\nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need)\n{\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(sbi, FAULT_CHECKPOINT)) {\n\t\tf2fs_show_injection_info(FAULT_CHECKPOINT);\n\t\tf2fs_stop_checkpoint(sbi, false);\n\t}\n#endif\n\n\t/* balance_fs_bg is able to be pending */\n\tif (need && excess_cached_nats(sbi))\n\t\tf2fs_balance_fs_bg(sbi);\n\n\t/*\n\t * We should do GC or end up with checkpoint, if there are so many dirty\n\t * dir/node pages without enough free segments.\n\t */\n\tif (has_not_enough_free_secs(sbi, 0, 0)) {\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\tf2fs_gc(sbi, false, false, NULL_SEGNO);\n\t}\n}\n\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi)\n{\n\t/* try to shrink extent cache when there is no enough memory */\n\tif (!available_free_memory(sbi, EXTENT_CACHE))\n\t\tf2fs_shrink_extent_tree(sbi, EXTENT_CACHE_SHRINK_NUMBER);\n\n\t/* check the # of cached NAT entries */\n\tif (!available_free_memory(sbi, NAT_ENTRIES))\n\t\ttry_to_free_nats(sbi, NAT_ENTRY_PER_BLOCK);\n\n\tif (!available_free_memory(sbi, FREE_NIDS))\n\t\ttry_to_free_nids(sbi, MAX_FREE_NIDS);\n\telse\n\t\tbuild_free_nids(sbi, false, false);\n\n\tif (!is_idle(sbi) && !excess_dirty_nats(sbi))\n\t\treturn;\n\n\t/* checkpoint is the only way to shrink partial cached entries */\n\tif (!available_free_memory(sbi, NAT_ENTRIES) ||\n\t\t\t!available_free_memory(sbi, INO_ENTRIES) ||\n\t\t\texcess_prefree_segs(sbi) ||\n\t\t\texcess_dirty_nats(sbi) ||\n\t\t\tf2fs_time_over(sbi, CP_TIME)) {\n\t\tif (test_opt(sbi, DATA_FLUSH)) {\n\t\t\tstruct blk_plug plug;\n\n\t\t\tblk_start_plug(&plug);\n\t\t\tsync_dirty_inodes(sbi, FILE_INODE);\n\t\t\tblk_finish_plug(&plug);\n\t\t}\n\t\tf2fs_sync_fs(sbi->sb, true);\n\t\tstat_inc_bg_cp_count(sbi->stat_info);\n\t}\n}\n\nstatic int __submit_flush_wait(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev)\n{\n\tstruct bio *bio = f2fs_bio_alloc(0);\n\tint ret;\n\n\tbio->bi_opf = REQ_OP_WRITE | REQ_SYNC | REQ_PREFLUSH;\n\tbio->bi_bdev = bdev;\n\tret = submit_bio_wait(bio);\n\tbio_put(bio);\n\n\ttrace_f2fs_issue_flush(bdev, test_opt(sbi, NOBARRIER),\n\t\t\t\ttest_opt(sbi, FLUSH_MERGE), ret);\n\treturn ret;\n}\n\nstatic int submit_flush_wait(struct f2fs_sb_info *sbi)\n{\n\tint ret = __submit_flush_wait(sbi, sbi->sb->s_bdev);\n\tint i;\n\n\tif (!sbi->s_ndevs || ret)\n\t\treturn ret;\n\n\tfor (i = 1; i < sbi->s_ndevs; i++) {\n\t\tret = __submit_flush_wait(sbi, FDEV(i).bdev);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int issue_flush_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\twait_queue_head_t *q = &fcc->flush_wait_queue;\nrepeat:\n\tif (kthread_should_stop())\n\t\treturn 0;\n\n\tif (!llist_empty(&fcc->issue_list)) {\n\t\tstruct flush_cmd *cmd, *next;\n\t\tint ret;\n\n\t\tfcc->dispatch_list = llist_del_all(&fcc->issue_list);\n\t\tfcc->dispatch_list = llist_reverse_order(fcc->dispatch_list);\n\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_inc(&fcc->issued_flush);\n\n\t\tllist_for_each_entry_safe(cmd, next,\n\t\t\t\t\t  fcc->dispatch_list, llnode) {\n\t\t\tcmd->ret = ret;\n\t\t\tcomplete(&cmd->wait);\n\t\t}\n\t\tfcc->dispatch_list = NULL;\n\t}\n\n\twait_event_interruptible(*q,\n\t\tkthread_should_stop() || !llist_empty(&fcc->issue_list));\n\tgoto repeat;\n}\n\nint f2fs_issue_flush(struct f2fs_sb_info *sbi)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\tstruct flush_cmd cmd;\n\tint ret;\n\n\tif (test_opt(sbi, NOBARRIER))\n\t\treturn 0;\n\n\tif (!test_opt(sbi, FLUSH_MERGE)) {\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tif (!atomic_read(&fcc->issing_flush)) {\n\t\tatomic_inc(&fcc->issing_flush);\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_dec(&fcc->issing_flush);\n\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tinit_completion(&cmd.wait);\n\n\tatomic_inc(&fcc->issing_flush);\n\tllist_add(&cmd.llnode, &fcc->issue_list);\n\n\tif (!fcc->dispatch_list)\n\t\twake_up(&fcc->flush_wait_queue);\n\n\tif (fcc->f2fs_issue_flush) {\n\t\twait_for_completion(&cmd.wait);\n\t\tatomic_dec(&fcc->issing_flush);\n\t} else {\n\t\tllist_del_all(&fcc->issue_list);\n\t\tatomic_set(&fcc->issing_flush, 0);\n\t}\n\n\treturn cmd.ret;\n}\n\nint create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\tint err = 0;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = kzalloc(sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->issing_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\terr = PTR_ERR(fcc->f2fs_issue_flush);\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nvoid destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\n\tif (fcc && fcc->f2fs_issue_flush) {\n\t\tstruct task_struct *flush_thread = fcc->f2fs_issue_flush;\n\n\t\tfcc->f2fs_issue_flush = NULL;\n\t\tkthread_stop(flush_thread);\n\t}\n\tif (free) {\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t}\n}\n\nstatic void __locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\t/* need not be added */\n\tif (IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]++;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (unlikely(t >= DIRTY)) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\treturn;\n\t\t}\n\t\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]++;\n\t}\n}\n\nstatic void __remove_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]--;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]--;\n\n\t\tif (get_valid_blocks(sbi, segno, true) == 0)\n\t\t\tclear_bit(GET_SEC_FROM_SEG(sbi, segno),\n\t\t\t\t\t\tdirty_i->victim_secmap);\n\t}\n}\n\n/*\n * Should not occur error such as -ENOMEM.\n * Adding dirty entry into seglist is not critical operation.\n * If a given segment is one of current working segments, it won't be added.\n */\nstatic void locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned short valid_blocks;\n\n\tif (segno == NULL_SEGNO || IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\n\tif (valid_blocks == 0) {\n\t\t__locate_dirty_segment(sbi, segno, PRE);\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t} else if (valid_blocks < sbi->blocks_per_seg) {\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t} else {\n\t\t/* Recovery routine with SSR needs this */\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t}\n\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic struct discard_cmd *__create_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t lstart,\n\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc;\n\n\tf2fs_bug_on(sbi, !len);\n\n\tpend_list = &dcc->pend_list[plist_idx(len)];\n\n\tdc = f2fs_kmem_cache_alloc(discard_cmd_slab, GFP_NOFS);\n\tINIT_LIST_HEAD(&dc->list);\n\tdc->bdev = bdev;\n\tdc->lstart = lstart;\n\tdc->start = start;\n\tdc->len = len;\n\tdc->ref = 0;\n\tdc->state = D_PREP;\n\tdc->error = 0;\n\tinit_completion(&dc->wait);\n\tlist_add_tail(&dc->list, pend_list);\n\tatomic_inc(&dcc->discard_cmd_cnt);\n\tdcc->undiscard_blks += len;\n\n\treturn dc;\n}\n\nstatic struct discard_cmd *__attach_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len,\n\t\t\t\tstruct rb_node *parent, struct rb_node **p)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\n\tdc = __create_discard_cmd(sbi, bdev, lstart, start, len);\n\n\trb_link_node(&dc->rb_node, parent, p);\n\trb_insert_color(&dc->rb_node, &dcc->root);\n\n\treturn dc;\n}\n\nstatic void __detach_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tif (dc->state == D_DONE)\n\t\tatomic_dec(&dcc->issing_discard);\n\n\tlist_del(&dc->list);\n\trb_erase(&dc->rb_node, &dcc->root);\n\tdcc->undiscard_blks -= dc->len;\n\n\tkmem_cache_free(discard_cmd_slab, dc);\n\n\tatomic_dec(&dcc->discard_cmd_cnt);\n}\n\nstatic void __remove_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (dc->error == -EOPNOTSUPP)\n\t\tdc->error = 0;\n\n\tif (dc->error)\n\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"Issue discard(%u, %u, %u) failed, ret: %d\",\n\t\t\tdc->lstart, dc->start, dc->len, dc->error);\n\t__detach_discard_cmd(dcc, dc);\n}\n\nstatic void f2fs_submit_discard_endio(struct bio *bio)\n{\n\tstruct discard_cmd *dc = (struct discard_cmd *)bio->bi_private;\n\n\tdc->error = bio->bi_error;\n\tdc->state = D_DONE;\n\tcomplete_all(&dc->wait);\n\tbio_put(bio);\n}\n\n/* this function is copied from blkdev_issue_discard from block/blk-lib.c */\nstatic void __submit_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct bio *bio = NULL;\n\n\tif (dc->state != D_PREP)\n\t\treturn;\n\n\ttrace_f2fs_issue_discard(dc->bdev, dc->start, dc->len);\n\n\tdc->error = __blkdev_issue_discard(dc->bdev,\n\t\t\t\tSECTOR_FROM_BLOCK(dc->start),\n\t\t\t\tSECTOR_FROM_BLOCK(dc->len),\n\t\t\t\tGFP_NOFS, 0, &bio);\n\tif (!dc->error) {\n\t\t/* should keep before submission to avoid D_DONE right away */\n\t\tdc->state = D_SUBMIT;\n\t\tatomic_inc(&dcc->issued_discard);\n\t\tatomic_inc(&dcc->issing_discard);\n\t\tif (bio) {\n\t\t\tbio->bi_private = dc;\n\t\t\tbio->bi_end_io = f2fs_submit_discard_endio;\n\t\t\tbio->bi_opf |= REQ_SYNC;\n\t\t\tsubmit_bio(bio);\n\t\t\tlist_move_tail(&dc->list, &dcc->wait_list);\n\t\t}\n\t} else {\n\t\t__remove_discard_cmd(sbi, dc);\n\t}\n}\n\nstatic struct discard_cmd *__insert_discard_tree(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len,\n\t\t\t\tstruct rb_node **insert_p,\n\t\t\t\tstruct rb_node *insert_parent)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct rb_node **p = &dcc->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct discard_cmd *dc = NULL;\n\n\tif (insert_p && insert_parent) {\n\t\tparent = insert_parent;\n\t\tp = insert_p;\n\t\tgoto do_insert;\n\t}\n\n\tp = __lookup_rb_tree_for_insert(sbi, &dcc->root, &parent, lstart);\ndo_insert:\n\tdc = __attach_discard_cmd(sbi, bdev, lstart, start, len, parent, p);\n\tif (!dc)\n\t\treturn NULL;\n\n\treturn dc;\n}\n\nstatic void __relocate_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tlist_move_tail(&dc->list, &dcc->pend_list[plist_idx(dc->len)]);\n}\n\nstatic void __punch_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_info di = dc->di;\n\tbool modified = false;\n\n\tif (dc->state == D_DONE || dc->len == 1) {\n\t\t__remove_discard_cmd(sbi, dc);\n\t\treturn;\n\t}\n\n\tdcc->undiscard_blks -= di.len;\n\n\tif (blkaddr > di.lstart) {\n\t\tdc->len = blkaddr - dc->lstart;\n\t\tdcc->undiscard_blks += dc->len;\n\t\t__relocate_discard_cmd(dcc, dc);\n\t\tmodified = true;\n\t}\n\n\tif (blkaddr < di.lstart + di.len - 1) {\n\t\tif (modified) {\n\t\t\t__insert_discard_tree(sbi, dc->bdev, blkaddr + 1,\n\t\t\t\t\tdi.start + blkaddr + 1 - di.lstart,\n\t\t\t\t\tdi.lstart + di.len - 1 - blkaddr,\n\t\t\t\t\tNULL, NULL);\n\t\t} else {\n\t\t\tdc->lstart++;\n\t\t\tdc->len--;\n\t\t\tdc->start++;\n\t\t\tdcc->undiscard_blks += dc->len;\n\t\t\t__relocate_discard_cmd(dcc, dc);\n\t\t}\n\t}\n}\n\nstatic void __update_discard_tree_range(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *prev_dc = NULL, *next_dc = NULL;\n\tstruct discard_cmd *dc;\n\tstruct discard_info di = {0};\n\tstruct rb_node **insert_p = NULL, *insert_parent = NULL;\n\tblock_t end = lstart + len;\n\n\tmutex_lock(&dcc->cmd_lock);\n\n\tdc = (struct discard_cmd *)__lookup_rb_tree_ret(&dcc->root,\n\t\t\t\t\tNULL, lstart,\n\t\t\t\t\t(struct rb_entry **)&prev_dc,\n\t\t\t\t\t(struct rb_entry **)&next_dc,\n\t\t\t\t\t&insert_p, &insert_parent, true);\n\tif (dc)\n\t\tprev_dc = dc;\n\n\tif (!prev_dc) {\n\t\tdi.lstart = lstart;\n\t\tdi.len = next_dc ? next_dc->lstart - lstart : len;\n\t\tdi.len = min(di.len, len);\n\t\tdi.start = start;\n\t}\n\n\twhile (1) {\n\t\tstruct rb_node *node;\n\t\tbool merged = false;\n\t\tstruct discard_cmd *tdc = NULL;\n\n\t\tif (prev_dc) {\n\t\t\tdi.lstart = prev_dc->lstart + prev_dc->len;\n\t\t\tif (di.lstart < lstart)\n\t\t\t\tdi.lstart = lstart;\n\t\t\tif (di.lstart >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (!next_dc || next_dc->lstart > end)\n\t\t\t\tdi.len = end - di.lstart;\n\t\t\telse\n\t\t\t\tdi.len = next_dc->lstart - di.lstart;\n\t\t\tdi.start = start + di.lstart - lstart;\n\t\t}\n\n\t\tif (!di.len)\n\t\t\tgoto next;\n\n\t\tif (prev_dc && prev_dc->state == D_PREP &&\n\t\t\tprev_dc->bdev == bdev &&\n\t\t\t__is_discard_back_mergeable(&di, &prev_dc->di)) {\n\t\t\tprev_dc->di.len += di.len;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, prev_dc);\n\t\t\tdi = prev_dc->di;\n\t\t\ttdc = prev_dc;\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (next_dc && next_dc->state == D_PREP &&\n\t\t\tnext_dc->bdev == bdev &&\n\t\t\t__is_discard_front_mergeable(&di, &next_dc->di)) {\n\t\t\tnext_dc->di.lstart = di.lstart;\n\t\t\tnext_dc->di.len += di.len;\n\t\t\tnext_dc->di.start = di.start;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, next_dc);\n\t\t\tif (tdc)\n\t\t\t\t__remove_discard_cmd(sbi, tdc);\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (!merged) {\n\t\t\t__insert_discard_tree(sbi, bdev, di.lstart, di.start,\n\t\t\t\t\t\t\tdi.len, NULL, NULL);\n\t\t}\n next:\n\t\tprev_dc = next_dc;\n\t\tif (!prev_dc)\n\t\t\tbreak;\n\n\t\tnode = rb_next(&prev_dc->rb_node);\n\t\tnext_dc = rb_entry_safe(node, struct discard_cmd, rb_node);\n\t}\n\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic int __queue_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tblock_t lblkstart = blkstart;\n\n\ttrace_f2fs_queue_discard(bdev, blkstart, blklen);\n\n\tif (sbi->s_ndevs) {\n\t\tint devi = f2fs_target_device_index(sbi, blkstart);\n\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\t__update_discard_tree_range(sbi, bdev, lblkstart, blkstart, blklen);\n\treturn 0;\n}\n\nstatic void __issue_discard_cmd(struct f2fs_sb_info *sbi, bool issue_cond)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc, *tmp;\n\tstruct blk_plug plug;\n\tint i, iter = 0;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tf2fs_bug_on(sbi,\n\t\t!__check_rb_tree_consistence(sbi, &dcc->root));\n\tblk_start_plug(&plug);\n\tfor (i = MAX_PLIST_NUM - 1; i >= 0; i--) {\n\t\tpend_list = &dcc->pend_list[i];\n\t\tlist_for_each_entry_safe(dc, tmp, pend_list, list) {\n\t\t\tf2fs_bug_on(sbi, dc->state != D_PREP);\n\n\t\t\tif (!issue_cond || is_idle(sbi))\n\t\t\t\t__submit_discard_cmd(sbi, dc);\n\t\t\tif (issue_cond && iter++ > DISCARD_ISSUE_RATE)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tblk_finish_plug(&plug);\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic void __wait_discard_cmd(struct f2fs_sb_info *sbi, bool wait_cond)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *wait_list = &(dcc->wait_list);\n\tstruct discard_cmd *dc, *tmp;\n\tbool need_wait;\n\nnext:\n\tneed_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tlist_for_each_entry_safe(dc, tmp, wait_list, list) {\n\t\tif (!wait_cond || (dc->state == D_DONE && !dc->ref)) {\n\t\t\twait_for_completion_io(&dc->wait);\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait) {\n\t\twait_for_completion_io(&dc->wait);\n\t\tmutex_lock(&dcc->cmd_lock);\n\t\tf2fs_bug_on(sbi, dc->state != D_DONE);\n\t\tdc->ref--;\n\t\tif (!dc->ref)\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\tmutex_unlock(&dcc->cmd_lock);\n\t\tgoto next;\n\t}\n}\n\n/* This should be covered by global mutex, &sit_i->sentry_lock */\nvoid f2fs_wait_discard_bio(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\tbool need_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tdc = (struct discard_cmd *)__lookup_rb_tree(&dcc->root, NULL, blkaddr);\n\tif (dc) {\n\t\tif (dc->state == D_PREP) {\n\t\t\t__punch_discard_cmd(sbi, dc, blkaddr);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait) {\n\t\twait_for_completion_io(&dc->wait);\n\t\tmutex_lock(&dcc->cmd_lock);\n\t\tf2fs_bug_on(sbi, dc->state != D_DONE);\n\t\tdc->ref--;\n\t\tif (!dc->ref)\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\tmutex_unlock(&dcc->cmd_lock);\n\t}\n}\n\n/* This comes from f2fs_put_super */\nvoid f2fs_wait_discard_bios(struct f2fs_sb_info *sbi)\n{\n\t__issue_discard_cmd(sbi, false);\n\t__wait_discard_cmd(sbi, false);\n}\n\nstatic int issue_discard_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\twait_queue_head_t *q = &dcc->discard_wait_queue;\n\n\tset_freezable();\n\n\tdo {\n\t\twait_event_interruptible(*q, kthread_should_stop() ||\n\t\t\t\t\tfreezing(current) ||\n\t\t\t\t\tatomic_read(&dcc->discard_cmd_cnt));\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\t\tif (kthread_should_stop())\n\t\t\treturn 0;\n\n\t\t__issue_discard_cmd(sbi, true);\n\t\t__wait_discard_cmd(sbi, true);\n\n\t\tcongestion_wait(BLK_RW_SYNC, HZ/50);\n\t} while (!kthread_should_stop());\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int __f2fs_issue_discard_zone(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tsector_t sector, nr_sects;\n\tblock_t lblkstart = blkstart;\n\tint devi = 0;\n\n\tif (sbi->s_ndevs) {\n\t\tdevi = f2fs_target_device_index(sbi, blkstart);\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\n\t/*\n\t * We need to know the type of the zone: for conventional zones,\n\t * use regular discard if the drive supports it. For sequential\n\t * zones, reset the zone write pointer.\n\t */\n\tswitch (get_blkz_type(sbi, bdev, blkstart)) {\n\n\tcase BLK_ZONE_TYPE_CONVENTIONAL:\n\t\tif (!blk_queue_discard(bdev_get_queue(bdev)))\n\t\t\treturn 0;\n\t\treturn __queue_discard_cmd(sbi, bdev, lblkstart, blklen);\n\tcase BLK_ZONE_TYPE_SEQWRITE_REQ:\n\tcase BLK_ZONE_TYPE_SEQWRITE_PREF:\n\t\tsector = SECTOR_FROM_BLOCK(blkstart);\n\t\tnr_sects = SECTOR_FROM_BLOCK(blklen);\n\n\t\tif (sector & (bdev_zone_sectors(bdev) - 1) ||\n\t\t\t\tnr_sects != bdev_zone_sectors(bdev)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\t\"(%d) %s: Unaligned discard attempted (block %x + %x)\",\n\t\t\t\tdevi, sbi->s_ndevs ? FDEV(devi).path: \"\",\n\t\t\t\tblkstart, blklen);\n\t\t\treturn -EIO;\n\t\t}\n\t\ttrace_f2fs_issue_reset_zone(bdev, blkstart);\n\t\treturn blkdev_reset_zones(bdev, sector,\n\t\t\t\t\t  nr_sects, GFP_NOFS);\n\tdefault:\n\t\t/* Unknown zone type: broken device ? */\n\t\treturn -EIO;\n\t}\n}\n#endif\n\nstatic int __issue_discard_async(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_mounted_blkzoned(sbi->sb) &&\n\t\t\t\tbdev_zoned_model(bdev) != BLK_ZONED_NONE)\n\t\treturn __f2fs_issue_discard_zone(sbi, bdev, blkstart, blklen);\n#endif\n\treturn __queue_discard_cmd(sbi, bdev, blkstart, blklen);\n}\n\nstatic int f2fs_issue_discard(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blkstart, block_t blklen)\n{\n\tsector_t start = blkstart, len = 0;\n\tstruct block_device *bdev;\n\tstruct seg_entry *se;\n\tunsigned int offset;\n\tblock_t i;\n\tint err = 0;\n\n\tbdev = f2fs_target_device(sbi, blkstart, NULL);\n\n\tfor (i = blkstart; i < blkstart + blklen; i++, len++) {\n\t\tif (i != start) {\n\t\t\tstruct block_device *bdev2 =\n\t\t\t\tf2fs_target_device(sbi, i, NULL);\n\n\t\t\tif (bdev2 != bdev) {\n\t\t\t\terr = __issue_discard_async(sbi, bdev,\n\t\t\t\t\t\tstart, len);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tbdev = bdev2;\n\t\t\t\tstart = i;\n\t\t\t\tlen = 0;\n\t\t\t}\n\t\t}\n\n\t\tse = get_seg_entry(sbi, GET_SEGNO(sbi, i));\n\t\toffset = GET_BLKOFF_FROM_SEG0(sbi, i);\n\n\t\tif (!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\t}\n\n\tif (len)\n\t\terr = __issue_discard_async(sbi, bdev, start, len);\n\treturn err;\n}\n\nstatic bool add_discard_addrs(struct f2fs_sb_info *sbi, struct cp_control *cpc,\n\t\t\t\t\t\t\tbool check_only)\n{\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tint max_blocks = sbi->blocks_per_seg;\n\tstruct seg_entry *se = get_seg_entry(sbi, cpc->trim_start);\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *discard_map = (unsigned long *)se->discard_map;\n\tunsigned long *dmap = SIT_I(sbi)->tmp_map;\n\tunsigned int start = 0, end = -1;\n\tbool force = (cpc->reason & CP_DISCARD);\n\tstruct discard_entry *de = NULL;\n\tstruct list_head *head = &SM_I(sbi)->dcc_info->entry_list;\n\tint i;\n\n\tif (se->valid_blocks == max_blocks || !f2fs_discard_en(sbi))\n\t\treturn false;\n\n\tif (!force) {\n\t\tif (!test_opt(sbi, DISCARD) || !se->valid_blocks ||\n\t\t\tSM_I(sbi)->dcc_info->nr_discards >=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards)\n\t\t\treturn false;\n\t}\n\n\t/* SIT_VBLOCK_MAP_SIZE should be multiple of sizeof(unsigned long) */\n\tfor (i = 0; i < entries; i++)\n\t\tdmap[i] = force ? ~ckpt_map[i] & ~discard_map[i] :\n\t\t\t\t(cur_map[i] ^ ckpt_map[i]) & ckpt_map[i];\n\n\twhile (force || SM_I(sbi)->dcc_info->nr_discards <=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards) {\n\t\tstart = __find_rev_next_bit(dmap, max_blocks, end + 1);\n\t\tif (start >= max_blocks)\n\t\t\tbreak;\n\n\t\tend = __find_rev_next_zero_bit(dmap, max_blocks, start + 1);\n\t\tif (force && start && end != max_blocks\n\t\t\t\t\t&& (end - start) < cpc->trim_minlen)\n\t\t\tcontinue;\n\n\t\tif (check_only)\n\t\t\treturn true;\n\n\t\tif (!de) {\n\t\t\tde = f2fs_kmem_cache_alloc(discard_entry_slab,\n\t\t\t\t\t\t\t\tGFP_F2FS_ZERO);\n\t\t\tde->start_blkaddr = START_BLOCK(sbi, cpc->trim_start);\n\t\t\tlist_add_tail(&de->list, head);\n\t\t}\n\n\t\tfor (i = start; i < end; i++)\n\t\t\t__set_bit_le(i, (void *)de->discard_map);\n\n\t\tSM_I(sbi)->dcc_info->nr_discards += end - start;\n\t}\n\treturn false;\n}\n\nvoid release_discard_addrs(struct f2fs_sb_info *sbi)\n{\n\tstruct list_head *head = &(SM_I(sbi)->dcc_info->entry_list);\n\tstruct discard_entry *entry, *this;\n\n\t/* drop caches */\n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tlist_del(&entry->list);\n\t\tkmem_cache_free(discard_entry_slab, entry);\n\t}\n}\n\n/*\n * Should call clear_prefree_segments after checkpoint is done.\n */\nstatic void set_prefree_as_free_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor_each_set_bit(segno, dirty_i->dirty_segmap[PRE], MAIN_SEGS(sbi))\n\t\t__set_test_and_free(sbi, segno);\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nvoid clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct list_head *head = &(SM_I(sbi)->dcc_info->entry_list);\n\tstruct discard_entry *entry, *this;\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned long *prefree_map = dirty_i->dirty_segmap[PRE];\n\tunsigned int start = 0, end = -1;\n\tunsigned int secno, start_segno;\n\tbool force = (cpc->reason & CP_DISCARD);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\twhile (1) {\n\t\tint i;\n\t\tstart = find_next_bit(prefree_map, MAIN_SEGS(sbi), end + 1);\n\t\tif (start >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\tend = find_next_zero_bit(prefree_map, MAIN_SEGS(sbi),\n\t\t\t\t\t\t\t\tstart + 1);\n\n\t\tfor (i = start; i < end; i++)\n\t\t\tclear_bit(i, prefree_map);\n\n\t\tdirty_i->nr_dirty[PRE] -= end - start;\n\n\t\tif (!test_opt(sbi, DISCARD))\n\t\t\tcontinue;\n\n\t\tif (force && start >= cpc->trim_start &&\n\t\t\t\t\t(end - 1) <= cpc->trim_end)\n\t\t\t\tcontinue;\n\n\t\tif (!test_opt(sbi, LFS) || sbi->segs_per_sec == 1) {\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start),\n\t\t\t\t(end - start) << sbi->log_blocks_per_seg);\n\t\t\tcontinue;\n\t\t}\nnext:\n\t\tsecno = GET_SEC_FROM_SEG(sbi, start);\n\t\tstart_segno = GET_SEG_FROM_SEC(sbi, secno);\n\t\tif (!IS_CURSEC(sbi, secno) &&\n\t\t\t!get_valid_blocks(sbi, start, true))\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start_segno),\n\t\t\t\tsbi->segs_per_sec << sbi->log_blocks_per_seg);\n\n\t\tstart = start_segno + sbi->segs_per_sec;\n\t\tif (start < end)\n\t\t\tgoto next;\n\t\telse\n\t\t\tend = start - 1;\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\t/* send small discards */\n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tunsigned int cur_pos = 0, next_pos, len, total_len = 0;\n\t\tbool is_valid = test_bit_le(0, entry->discard_map);\n\nfind_next:\n\t\tif (is_valid) {\n\t\t\tnext_pos = find_next_zero_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t\tlen = next_pos - cur_pos;\n\n\t\t\tif (f2fs_sb_mounted_blkzoned(sbi->sb) ||\n\t\t\t    (force && len < cpc->trim_minlen))\n\t\t\t\tgoto skip;\n\n\t\t\tf2fs_issue_discard(sbi, entry->start_blkaddr + cur_pos,\n\t\t\t\t\t\t\t\t\tlen);\n\t\t\tcpc->trimmed += len;\n\t\t\ttotal_len += len;\n\t\t} else {\n\t\t\tnext_pos = find_next_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t}\nskip:\n\t\tcur_pos = next_pos;\n\t\tis_valid = !is_valid;\n\n\t\tif (cur_pos < sbi->blocks_per_seg)\n\t\t\tgoto find_next;\n\n\t\tlist_del(&entry->list);\n\t\tSM_I(sbi)->dcc_info->nr_discards -= total_len;\n\t\tkmem_cache_free(discard_entry_slab, entry);\n\t}\n\n\twake_up(&SM_I(sbi)->dcc_info->discard_wait_queue);\n}\n\nstatic int create_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct discard_cmd_control *dcc;\n\tint err = 0, i;\n\n\tif (SM_I(sbi)->dcc_info) {\n\t\tdcc = SM_I(sbi)->dcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tdcc = kzalloc(sizeof(struct discard_cmd_control), GFP_KERNEL);\n\tif (!dcc)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&dcc->entry_list);\n\tfor (i = 0; i < MAX_PLIST_NUM; i++)\n\t\tINIT_LIST_HEAD(&dcc->pend_list[i]);\n\tINIT_LIST_HEAD(&dcc->wait_list);\n\tmutex_init(&dcc->cmd_lock);\n\tatomic_set(&dcc->issued_discard, 0);\n\tatomic_set(&dcc->issing_discard, 0);\n\tatomic_set(&dcc->discard_cmd_cnt, 0);\n\tdcc->nr_discards = 0;\n\tdcc->max_discards = MAIN_SEGS(sbi) << sbi->log_blocks_per_seg;\n\tdcc->undiscard_blks = 0;\n\tdcc->root = RB_ROOT;\n\n\tinit_waitqueue_head(&dcc->discard_wait_queue);\n\tSM_I(sbi)->dcc_info = dcc;\ninit_thread:\n\tdcc->f2fs_issue_discard = kthread_run(issue_discard_thread, sbi,\n\t\t\t\t\"f2fs_discard-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(dcc->f2fs_issue_discard)) {\n\t\terr = PTR_ERR(dcc->f2fs_issue_discard);\n\t\tkfree(dcc);\n\t\tSM_I(sbi)->dcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nstatic void destroy_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (!dcc)\n\t\treturn;\n\n\tif (dcc->f2fs_issue_discard) {\n\t\tstruct task_struct *discard_thread = dcc->f2fs_issue_discard;\n\n\t\tdcc->f2fs_issue_discard = NULL;\n\t\tkthread_stop(discard_thread);\n\t}\n\n\tkfree(dcc);\n\tSM_I(sbi)->dcc_info = NULL;\n}\n\nstatic bool __mark_sit_entry_dirty(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tif (!__test_and_set_bit(segno, sit_i->dirty_sentries_bitmap)) {\n\t\tsit_i->dirty_sentries++;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void __set_sit_entry_type(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tunsigned int segno, int modified)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, segno);\n\tse->type = type;\n\tif (modified)\n\t\t__mark_sit_entry_dirty(sbi, segno);\n}\n\nstatic void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)\n{\n\tstruct seg_entry *se;\n\tunsigned int segno, offset;\n\tlong int new_vblocks;\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\n\tse = get_seg_entry(sbi, segno);\n\tnew_vblocks = se->valid_blocks + del;\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tf2fs_bug_on(sbi, (new_vblocks >> (sizeof(unsigned short) << 3) ||\n\t\t\t\t(new_vblocks > sbi->blocks_per_seg)));\n\n\tse->valid_blocks = new_vblocks;\n\tse->mtime = get_mtime(sbi);\n\tSIT_I(sbi)->max_mtime = se->mtime;\n\n\t/* Update valid block bitmap */\n\tif (del > 0) {\n\t\tif (f2fs_test_and_set_bit(offset, se->cur_valid_map)) {\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tif (f2fs_test_and_set_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir))\n\t\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\telse\n\t\t\t\tWARN_ON(1);\n#else\n\t\t\tf2fs_bug_on(sbi, 1);\n#endif\n\t\t}\n\t\tif (f2fs_discard_en(sbi) &&\n\t\t\t!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\n\t\t/* don't overwrite by SSR to keep node chain */\n\t\tif (se->type == CURSEG_WARM_NODE) {\n\t\t\tif (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))\n\t\t\t\tse->ckpt_valid_blocks++;\n\t\t}\n\t} else {\n\t\tif (!f2fs_test_and_clear_bit(offset, se->cur_valid_map)) {\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tif (!f2fs_test_and_clear_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir))\n\t\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\telse\n\t\t\t\tWARN_ON(1);\n#else\n\t\t\tf2fs_bug_on(sbi, 1);\n#endif\n\t\t}\n\t\tif (f2fs_discard_en(sbi) &&\n\t\t\tf2fs_test_and_clear_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks++;\n\t}\n\tif (!f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tse->ckpt_valid_blocks += del;\n\n\t__mark_sit_entry_dirty(sbi, segno);\n\n\t/* update total number of valid blocks to be written in ckpt area */\n\tSIT_I(sbi)->written_valid_blocks += del;\n\n\tif (sbi->segs_per_sec > 1)\n\t\tget_sec_entry(sbi, segno)->valid_blocks += del;\n}\n\nvoid refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new)\n{\n\tupdate_sit_entry(sbi, new, 1);\n\tif (GET_SEGNO(sbi, old) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old, -1);\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new));\n}\n\nvoid invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr)\n{\n\tunsigned int segno = GET_SEGNO(sbi, addr);\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tf2fs_bug_on(sbi, addr == NULL_ADDR);\n\tif (addr == NEW_ADDR)\n\t\treturn;\n\n\t/* add it into sit main buffer */\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tupdate_sit_entry(sbi, addr, -1);\n\n\t/* add it into dirty seglist */\n\tlocate_dirty_segment(sbi, segno);\n\n\tmutex_unlock(&sit_i->sentry_lock);\n}\n\nbool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno, offset;\n\tstruct seg_entry *se;\n\tbool is_cp = false;\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn true;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tif (f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tis_cp = true;\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\treturn is_cp;\n}\n\n/*\n * This function should be resided under the curseg_mutex lock\n */\nstatic void __add_sum_entry(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tstruct f2fs_summary *sum)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tvoid *addr = curseg->sum_blk;\n\taddr += curseg->next_blkoff * sizeof(struct f2fs_summary);\n\tmemcpy(addr, sum, sizeof(struct f2fs_summary));\n}\n\n/*\n * Calculate the number of current summary pages for writing\n */\nint npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra)\n{\n\tint valid_sum_count = 0;\n\tint i, sum_in_page;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tif (sbi->ckpt->alloc_type[i] == SSR)\n\t\t\tvalid_sum_count += sbi->blocks_per_seg;\n\t\telse {\n\t\t\tif (for_ra)\n\t\t\t\tvalid_sum_count += le16_to_cpu(\n\t\t\t\t\tF2FS_CKPT(sbi)->cur_data_blkoff[i]);\n\t\t\telse\n\t\t\t\tvalid_sum_count += curseg_blkoff(sbi, i);\n\t\t}\n\t}\n\n\tsum_in_page = (PAGE_SIZE - 2 * SUM_JOURNAL_SIZE -\n\t\t\tSUM_FOOTER_SIZE) / SUMMARY_SIZE;\n\tif (valid_sum_count <= sum_in_page)\n\t\treturn 1;\n\telse if ((valid_sum_count - sum_in_page) <=\n\t\t(PAGE_SIZE - SUM_FOOTER_SIZE) / SUMMARY_SIZE)\n\t\treturn 2;\n\treturn 3;\n}\n\n/*\n * Caller should put this summary page\n */\nstruct page *get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\treturn get_meta_page(sbi, GET_SUM_BLOCK(sbi, segno));\n}\n\nvoid update_meta_page(struct f2fs_sb_info *sbi, void *src, block_t blk_addr)\n{\n\tstruct page *page = grab_meta_page(sbi, blk_addr);\n\tvoid *dst = page_address(page);\n\n\tif (src)\n\t\tmemcpy(dst, src, PAGE_SIZE);\n\telse\n\t\tmemset(dst, 0, PAGE_SIZE);\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic void write_sum_page(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_summary_block *sum_blk, block_t blk_addr)\n{\n\tupdate_meta_page(sbi, (void *)sum_blk, blk_addr);\n}\n\nstatic void write_current_sum_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, block_t blk_addr)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct page *page = grab_meta_page(sbi, blk_addr);\n\tstruct f2fs_summary_block *src = curseg->sum_blk;\n\tstruct f2fs_summary_block *dst;\n\n\tdst = (struct f2fs_summary_block *)page_address(page);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\n\tdown_read(&curseg->journal_rwsem);\n\tmemcpy(&dst->journal, curseg->journal, SUM_JOURNAL_SIZE);\n\tup_read(&curseg->journal_rwsem);\n\n\tmemcpy(dst->entries, src->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&dst->footer, &src->footer, SUM_FOOTER_SIZE);\n\n\tmutex_unlock(&curseg->curseg_mutex);\n\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic int is_next_segment_free(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno = curseg->segno + 1;\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\n\tif (segno < MAIN_SEGS(sbi) && segno % sbi->segs_per_sec)\n\t\treturn !test_bit(segno, free_i->free_segmap);\n\treturn 0;\n}\n\n/*\n * Find a new segment from the free segments bitmap to right order\n * This function should be returned with success, otherwise BUG\n */\nstatic void get_new_segment(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int *newseg, bool new_sec, int dir)\n{\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno, secno, zoneno;\n\tunsigned int total_zones = MAIN_SECS(sbi) / sbi->secs_per_zone;\n\tunsigned int hint = GET_SEC_FROM_SEG(sbi, *newseg);\n\tunsigned int old_zoneno = GET_ZONE_FROM_SEG(sbi, *newseg);\n\tunsigned int left_start = hint;\n\tbool init = true;\n\tint go_left = 0;\n\tint i;\n\n\tspin_lock(&free_i->segmap_lock);\n\n\tif (!new_sec && ((*newseg + 1) % sbi->segs_per_sec)) {\n\t\tsegno = find_next_zero_bit(free_i->free_segmap,\n\t\t\tGET_SEG_FROM_SEC(sbi, hint + 1), *newseg + 1);\n\t\tif (segno < GET_SEG_FROM_SEC(sbi, hint + 1))\n\t\t\tgoto got_it;\n\t}\nfind_other_zone:\n\tsecno = find_next_zero_bit(free_i->free_secmap, MAIN_SECS(sbi), hint);\n\tif (secno >= MAIN_SECS(sbi)) {\n\t\tif (dir == ALLOC_RIGHT) {\n\t\t\tsecno = find_next_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi), 0);\n\t\t\tf2fs_bug_on(sbi, secno >= MAIN_SECS(sbi));\n\t\t} else {\n\t\t\tgo_left = 1;\n\t\t\tleft_start = hint - 1;\n\t\t}\n\t}\n\tif (go_left == 0)\n\t\tgoto skip_left;\n\n\twhile (test_bit(left_start, free_i->free_secmap)) {\n\t\tif (left_start > 0) {\n\t\t\tleft_start--;\n\t\t\tcontinue;\n\t\t}\n\t\tleft_start = find_next_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi), 0);\n\t\tf2fs_bug_on(sbi, left_start >= MAIN_SECS(sbi));\n\t\tbreak;\n\t}\n\tsecno = left_start;\nskip_left:\n\thint = secno;\n\tsegno = GET_SEG_FROM_SEC(sbi, secno);\n\tzoneno = GET_ZONE_FROM_SEC(sbi, secno);\n\n\t/* give up on finding another zone */\n\tif (!init)\n\t\tgoto got_it;\n\tif (sbi->secs_per_zone == 1)\n\t\tgoto got_it;\n\tif (zoneno == old_zoneno)\n\t\tgoto got_it;\n\tif (dir == ALLOC_LEFT) {\n\t\tif (!go_left && zoneno + 1 >= total_zones)\n\t\t\tgoto got_it;\n\t\tif (go_left && zoneno == 0)\n\t\t\tgoto got_it;\n\t}\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++)\n\t\tif (CURSEG_I(sbi, i)->zone == zoneno)\n\t\t\tbreak;\n\n\tif (i < NR_CURSEG_TYPE) {\n\t\t/* zone is in user, try another */\n\t\tif (go_left)\n\t\t\thint = zoneno * sbi->secs_per_zone - 1;\n\t\telse if (zoneno + 1 >= total_zones)\n\t\t\thint = 0;\n\t\telse\n\t\t\thint = (zoneno + 1) * sbi->secs_per_zone;\n\t\tinit = false;\n\t\tgoto find_other_zone;\n\t}\ngot_it:\n\t/* set it as dirty segment in free segmap */\n\tf2fs_bug_on(sbi, test_bit(segno, free_i->free_segmap));\n\t__set_inuse(sbi, segno);\n\t*newseg = segno;\n\tspin_unlock(&free_i->segmap_lock);\n}\n\nstatic void reset_curseg(struct f2fs_sb_info *sbi, int type, int modified)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct summary_footer *sum_footer;\n\n\tcurseg->segno = curseg->next_segno;\n\tcurseg->zone = GET_ZONE_FROM_SEG(sbi, curseg->segno);\n\tcurseg->next_blkoff = 0;\n\tcurseg->next_segno = NULL_SEGNO;\n\n\tsum_footer = &(curseg->sum_blk->footer);\n\tmemset(sum_footer, 0, sizeof(struct summary_footer));\n\tif (IS_DATASEG(type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_DATA);\n\tif (IS_NODESEG(type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_NODE);\n\t__set_sit_entry_type(sbi, type, curseg->segno, modified);\n}\n\nstatic unsigned int __get_next_segno(struct f2fs_sb_info *sbi, int type)\n{\n\t/* if segs_per_sec is large than 1, we need to keep original policy. */\n\tif (sbi->segs_per_sec != 1)\n\t\treturn CURSEG_I(sbi, type)->segno;\n\n\tif (type == CURSEG_HOT_DATA || IS_NODESEG(type))\n\t\treturn 0;\n\n\tif (SIT_I(sbi)->last_victim[ALLOC_NEXT])\n\t\treturn SIT_I(sbi)->last_victim[ALLOC_NEXT];\n\treturn CURSEG_I(sbi, type)->segno;\n}\n\n/*\n * Allocate a current working segment.\n * This function always allocates a free segment in LFS manner.\n */\nstatic void new_curseg(struct f2fs_sb_info *sbi, int type, bool new_sec)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno = curseg->segno;\n\tint dir = ALLOC_LEFT;\n\n\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, segno));\n\tif (type == CURSEG_WARM_DATA || type == CURSEG_COLD_DATA)\n\t\tdir = ALLOC_RIGHT;\n\n\tif (test_opt(sbi, NOHEAP))\n\t\tdir = ALLOC_RIGHT;\n\n\tsegno = __get_next_segno(sbi, type);\n\tget_new_segment(sbi, &segno, new_sec, dir);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = LFS;\n}\n\nstatic void __next_free_blkoff(struct f2fs_sb_info *sbi,\n\t\t\tstruct curseg_info *seg, block_t start)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, seg->segno);\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tunsigned long *target_map = SIT_I(sbi)->tmp_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tint i, pos;\n\n\tfor (i = 0; i < entries; i++)\n\t\ttarget_map[i] = ckpt_map[i] | cur_map[i];\n\n\tpos = __find_rev_next_zero_bit(target_map, sbi->blocks_per_seg, start);\n\n\tseg->next_blkoff = pos;\n}\n\n/*\n * If a segment is written by LFS manner, next block offset is just obtained\n * by increasing the current block offset. However, if a segment is written by\n * SSR manner, next block offset obtained by calling __next_free_blkoff\n */\nstatic void __refresh_next_blkoff(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct curseg_info *seg)\n{\n\tif (seg->alloc_type == SSR)\n\t\t__next_free_blkoff(sbi, seg, seg->next_blkoff + 1);\n\telse\n\t\tseg->next_blkoff++;\n}\n\n/*\n * This function always allocates a used segment(from dirty seglist) by SSR\n * manner, so it should recover the existing segment information of valid blocks\n */\nstatic void change_curseg(struct f2fs_sb_info *sbi, int type, bool reuse)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int new_segno = curseg->next_segno;\n\tstruct f2fs_summary_block *sum_node;\n\tstruct page *sum_page;\n\n\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, curseg->segno));\n\t__set_test_and_inuse(sbi, new_segno);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\t__remove_dirty_segment(sbi, new_segno, PRE);\n\t__remove_dirty_segment(sbi, new_segno, DIRTY);\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = SSR;\n\t__next_free_blkoff(sbi, curseg, 0);\n\n\tif (reuse) {\n\t\tsum_page = get_sum_page(sbi, new_segno);\n\t\tsum_node = (struct f2fs_summary_block *)page_address(sum_page);\n\t\tmemcpy(curseg->sum_blk, sum_node, SUM_ENTRY_SIZE);\n\t\tf2fs_put_page(sum_page, 1);\n\t}\n}\n\nstatic int get_ssr_segment(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tconst struct victim_selection *v_ops = DIRTY_I(sbi)->v_ops;\n\tunsigned segno = NULL_SEGNO;\n\tint i, cnt;\n\tbool reversed = false;\n\n\t/* need_SSR() already forces to do this */\n\tif (v_ops->get_victim(sbi, &segno, BG_GC, type, SSR)) {\n\t\tcurseg->next_segno = segno;\n\t\treturn 1;\n\t}\n\n\t/* For node segments, let's do SSR more intensively */\n\tif (IS_NODESEG(type)) {\n\t\tif (type >= CURSEG_WARM_NODE) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_NODE;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_NODE;\n\t\t}\n\t\tcnt = NR_CURSEG_NODE_TYPE;\n\t} else {\n\t\tif (type >= CURSEG_WARM_DATA) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_DATA;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_DATA;\n\t\t}\n\t\tcnt = NR_CURSEG_DATA_TYPE;\n\t}\n\n\tfor (; cnt-- > 0; reversed ? i-- : i++) {\n\t\tif (i == type)\n\t\t\tcontinue;\n\t\tif (v_ops->get_victim(sbi, &segno, BG_GC, i, SSR)) {\n\t\t\tcurseg->next_segno = segno;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * flush out current segment and replace it with new segment\n * This function should be returned with success, otherwise BUG\n */\nstatic void allocate_segment_by_default(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, bool force)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tif (force)\n\t\tnew_curseg(sbi, type, true);\n\telse if (!is_set_ckpt_flags(sbi, CP_CRC_RECOVERY_FLAG) &&\n\t\t\t\t\ttype == CURSEG_WARM_NODE)\n\t\tnew_curseg(sbi, type, false);\n\telse if (curseg->alloc_type == LFS && is_next_segment_free(sbi, type))\n\t\tnew_curseg(sbi, type, false);\n\telse if (need_SSR(sbi) && get_ssr_segment(sbi, type))\n\t\tchange_curseg(sbi, type, true);\n\telse\n\t\tnew_curseg(sbi, type, false);\n\n\tstat_inc_seg_type(sbi, curseg);\n}\n\nvoid allocate_new_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg;\n\tunsigned int old_segno;\n\tint i;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tcurseg = CURSEG_I(sbi, i);\n\t\told_segno = curseg->segno;\n\t\tSIT_I(sbi)->s_ops->allocate_segment(sbi, i, true);\n\t\tlocate_dirty_segment(sbi, old_segno);\n\t}\n}\n\nstatic const struct segment_allocation default_salloc_ops = {\n\t.allocate_segment = allocate_segment_by_default,\n};\n\nbool exist_trim_candidates(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\t__u64 trim_start = cpc->trim_start;\n\tbool has_candidate = false;\n\n\tmutex_lock(&SIT_I(sbi)->sentry_lock);\n\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++) {\n\t\tif (add_discard_addrs(sbi, cpc, true)) {\n\t\t\thas_candidate = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&SIT_I(sbi)->sentry_lock);\n\n\tcpc->trim_start = trim_start;\n\treturn has_candidate;\n}\n\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range)\n{\n\t__u64 start = F2FS_BYTES_TO_BLK(range->start);\n\t__u64 end = start + F2FS_BYTES_TO_BLK(range->len) - 1;\n\tunsigned int start_segno, end_segno;\n\tstruct cp_control cpc;\n\tint err = 0;\n\n\tif (start >= MAX_BLKADDR(sbi) || range->len < sbi->blocksize)\n\t\treturn -EINVAL;\n\n\tcpc.trimmed = 0;\n\tif (end <= MAIN_BLKADDR(sbi))\n\t\tgoto out;\n\n\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\"Found FS corruption, run fsck to fix.\");\n\t\tgoto out;\n\t}\n\n\t/* start/end segment number in main_area */\n\tstart_segno = (start <= MAIN_BLKADDR(sbi)) ? 0 : GET_SEGNO(sbi, start);\n\tend_segno = (end >= MAX_BLKADDR(sbi)) ? MAIN_SEGS(sbi) - 1 :\n\t\t\t\t\t\tGET_SEGNO(sbi, end);\n\tcpc.reason = CP_DISCARD;\n\tcpc.trim_minlen = max_t(__u64, 1, F2FS_BYTES_TO_BLK(range->minlen));\n\n\t/* do checkpoint to issue discard commands safely */\n\tfor (; start_segno <= end_segno; start_segno = cpc.trim_end + 1) {\n\t\tcpc.trim_start = start_segno;\n\n\t\tif (sbi->discard_blks == 0)\n\t\t\tbreak;\n\t\telse if (sbi->discard_blks < BATCHED_TRIM_BLOCKS(sbi))\n\t\t\tcpc.trim_end = end_segno;\n\t\telse\n\t\t\tcpc.trim_end = min_t(unsigned int,\n\t\t\t\trounddown(start_segno +\n\t\t\t\tBATCHED_TRIM_SEGMENTS(sbi),\n\t\t\t\tsbi->segs_per_sec) - 1, end_segno);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\nout:\n\trange->len = F2FS_BLK_TO_BYTES(cpc.trimmed);\n\treturn err;\n}\n\nstatic bool __has_curseg_space(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tif (curseg->next_blkoff < sbi->blocks_per_seg)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int __get_segment_type_2(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA)\n\t\treturn CURSEG_HOT_DATA;\n\telse\n\t\treturn CURSEG_HOT_NODE;\n}\n\nstatic int __get_segment_type_4(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (S_ISDIR(inode->i_mode))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\telse\n\t\t\treturn CURSEG_COLD_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page) && is_cold_node(fio->page))\n\t\t\treturn CURSEG_WARM_NODE;\n\t\telse\n\t\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type_6(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (is_cold_data(fio->page) || file_is_cold(inode))\n\t\t\treturn CURSEG_COLD_DATA;\n\t\tif (is_inode_flag_set(inode, FI_HOT_DATA))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\treturn CURSEG_WARM_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page))\n\t\t\treturn is_cold_node(fio->page) ? CURSEG_WARM_NODE :\n\t\t\t\t\t\tCURSEG_HOT_NODE;\n\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type(struct f2fs_io_info *fio)\n{\n\tint type = 0;\n\n\tswitch (fio->sbi->active_logs) {\n\tcase 2:\n\t\ttype = __get_segment_type_2(fio);\n\t\tbreak;\n\tcase 4:\n\t\ttype = __get_segment_type_4(fio);\n\t\tbreak;\n\tcase 6:\n\t\ttype = __get_segment_type_6(fio);\n\t\tbreak;\n\tdefault:\n\t\tf2fs_bug_on(fio->sbi, true);\n\t}\n\n\tif (IS_HOT(type))\n\t\tfio->temp = HOT;\n\telse if (IS_WARM(type))\n\t\tfio->temp = WARM;\n\telse\n\t\tfio->temp = COLD;\n\treturn type;\n}\n\nvoid allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\tstruct f2fs_summary *sum, int type,\n\t\tstruct f2fs_io_info *fio, bool add_list)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tmutex_lock(&sit_i->sentry_lock);\n\n\t*new_blkaddr = NEXT_FREE_BLKADDR(sbi, curseg);\n\n\tf2fs_wait_discard_bio(sbi, *new_blkaddr);\n\n\t/*\n\t * __add_sum_entry should be resided under the curseg_mutex\n\t * because, this function updates a summary entry in the\n\t * current summary block.\n\t */\n\t__add_sum_entry(sbi, type, sum);\n\n\t__refresh_next_blkoff(sbi, curseg);\n\n\tstat_inc_block_count(sbi, curseg);\n\n\tif (!__has_curseg_space(sbi, type))\n\t\tsit_i->s_ops->allocate_segment(sbi, type, false);\n\t/*\n\t * SIT information should be updated after segment allocation,\n\t * since we need to keep dirty segments precisely under SSR.\n\t */\n\trefresh_sit_entry(sbi, old_blkaddr, *new_blkaddr);\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\tif (page && IS_NODESEG(type))\n\t\tfill_node_footer_blkaddr(page, NEXT_FREE_BLKADDR(sbi, curseg));\n\n\tif (add_list) {\n\t\tstruct f2fs_bio_info *io;\n\n\t\tINIT_LIST_HEAD(&fio->list);\n\t\tfio->in_list = true;\n\t\tio = sbi->write_io[fio->type] + fio->temp;\n\t\tspin_lock(&io->io_lock);\n\t\tlist_add_tail(&fio->list, &io->io_list);\n\t\tspin_unlock(&io->io_lock);\n\t}\n\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nstatic void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)\n{\n\tint type = __get_segment_type(fio);\n\tint err;\n\nreallocate:\n\tallocate_data_block(fio->sbi, fio->page, fio->old_blkaddr,\n\t\t\t&fio->new_blkaddr, sum, type, fio, true);\n\n\t/* writeout dirty page into bdev */\n\terr = f2fs_submit_page_write(fio);\n\tif (err == -EAGAIN) {\n\t\tfio->old_blkaddr = fio->new_blkaddr;\n\t\tgoto reallocate;\n\t}\n}\n\nvoid write_meta_page(struct f2fs_sb_info *sbi, struct page *page)\n{\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = META,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_META | REQ_PRIO,\n\t\t.old_blkaddr = page->index,\n\t\t.new_blkaddr = page->index,\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t\t.in_list = false,\n\t};\n\n\tif (unlikely(page->index >= MAIN_BLKADDR(sbi)))\n\t\tfio.op_flags &= ~REQ_META;\n\n\tset_page_writeback(page);\n\tf2fs_submit_page_write(&fio);\n}\n\nvoid write_node_page(unsigned int nid, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, nid, 0, 0);\n\tdo_write_page(&sum, fio);\n}\n\nvoid write_data_page(struct dnode_of_data *dn, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\n\tf2fs_bug_on(sbi, dn->data_blkaddr == NULL_ADDR);\n\tget_node_info(sbi, dn->nid, &ni);\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\tdo_write_page(&sum, fio);\n\tf2fs_update_data_blkaddr(dn, fio->new_blkaddr);\n}\n\nint rewrite_data_page(struct f2fs_io_info *fio)\n{\n\tfio->new_blkaddr = fio->old_blkaddr;\n\tstat_inc_inplace_blocks(fio->sbi);\n\treturn f2fs_submit_page_bio(fio);\n}\n\nvoid __f2fs_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\t\tbool recover_curseg, bool recover_newaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg;\n\tunsigned int segno, old_cursegno;\n\tstruct seg_entry *se;\n\tint type;\n\tunsigned short old_blkoff;\n\n\tsegno = GET_SEGNO(sbi, new_blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\ttype = se->type;\n\n\tif (!recover_curseg) {\n\t\t/* for recovery flow */\n\t\tif (se->valid_blocks == 0 && !IS_CURSEG(sbi, segno)) {\n\t\t\tif (old_blkaddr == NULL_ADDR)\n\t\t\t\ttype = CURSEG_COLD_DATA;\n\t\t\telse\n\t\t\t\ttype = CURSEG_WARM_DATA;\n\t\t}\n\t} else {\n\t\tif (!IS_CURSEG(sbi, segno))\n\t\t\ttype = CURSEG_WARM_DATA;\n\t}\n\n\tcurseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tmutex_lock(&sit_i->sentry_lock);\n\n\told_cursegno = curseg->segno;\n\told_blkoff = curseg->next_blkoff;\n\n\t/* change the current segment */\n\tif (segno != curseg->segno) {\n\t\tcurseg->next_segno = segno;\n\t\tchange_curseg(sbi, type, true);\n\t}\n\n\tcurseg->next_blkoff = GET_BLKOFF_FROM_SEG0(sbi, new_blkaddr);\n\t__add_sum_entry(sbi, type, sum);\n\n\tif (!recover_curseg || recover_newaddr)\n\t\tupdate_sit_entry(sbi, new_blkaddr, 1);\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old_blkaddr, -1);\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old_blkaddr));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new_blkaddr));\n\n\tlocate_dirty_segment(sbi, old_cursegno);\n\n\tif (recover_curseg) {\n\t\tif (old_cursegno != curseg->segno) {\n\t\t\tcurseg->next_segno = old_cursegno;\n\t\t\tchange_curseg(sbi, type, true);\n\t\t}\n\t\tcurseg->next_blkoff = old_blkoff;\n\t}\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\t\tunsigned char version, bool recover_curseg,\n\t\t\t\tbool recover_newaddr)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, version);\n\n\t__f2fs_replace_block(sbi, &sum, old_addr, new_addr,\n\t\t\t\t\trecover_curseg, recover_newaddr);\n\n\tf2fs_update_data_blkaddr(dn, new_addr);\n}\n\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\t\tenum page_type type, bool ordered)\n{\n\tif (PageWriteback(page)) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_P_SB(page);\n\n\t\tf2fs_submit_merged_write_cond(sbi, page->mapping->host,\n\t\t\t\t\t\t0, page->index, type);\n\t\tif (ordered)\n\t\t\twait_on_page_writeback(page);\n\t\telse\n\t\t\twait_for_stable_page(page);\n\t}\n}\n\nvoid f2fs_wait_on_encrypted_page_writeback(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tblock_t blkaddr)\n{\n\tstruct page *cpage;\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn;\n\n\tcpage = find_lock_page(META_MAPPING(sbi), blkaddr);\n\tif (cpage) {\n\t\tf2fs_wait_on_page_writeback(cpage, DATA, true);\n\t\tf2fs_put_page(cpage, 1);\n\t}\n}\n\nstatic int read_compacted_summaries(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct curseg_info *seg_i;\n\tunsigned char *kaddr;\n\tstruct page *page;\n\tblock_t start;\n\tint i, j, offset;\n\n\tstart = start_sum_block(sbi);\n\n\tpage = get_meta_page(sbi, start++);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t/* Step 1: restore nat cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(seg_i->journal, kaddr, SUM_JOURNAL_SIZE);\n\n\t/* Step 2: restore sit cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(seg_i->journal, kaddr + SUM_JOURNAL_SIZE, SUM_JOURNAL_SIZE);\n\toffset = 2 * SUM_JOURNAL_SIZE;\n\n\t/* Step 3: restore summary entries */\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blk_off;\n\t\tunsigned int segno;\n\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[i]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[i]);\n\t\tseg_i->next_segno = segno;\n\t\treset_curseg(sbi, i, 0);\n\t\tseg_i->alloc_type = ckpt->alloc_type[i];\n\t\tseg_i->next_blkoff = blk_off;\n\n\t\tif (seg_i->alloc_type == SSR)\n\t\t\tblk_off = sbi->blocks_per_seg;\n\n\t\tfor (j = 0; j < blk_off; j++) {\n\t\t\tstruct f2fs_summary *s;\n\t\t\ts = (struct f2fs_summary *)(kaddr + offset);\n\t\t\tseg_i->sum_blk->entries[j] = *s;\n\t\t\toffset += SUMMARY_SIZE;\n\t\t\tif (offset + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\n\t\t\tpage = get_meta_page(sbi, start++);\n\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\toffset = 0;\n\t\t}\n\t}\n\tf2fs_put_page(page, 1);\n\treturn 0;\n}\n\nstatic int read_normal_summaries(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_summary_block *sum;\n\tstruct curseg_info *curseg;\n\tstruct page *new;\n\tunsigned short blk_off;\n\tunsigned int segno = 0;\n\tblock_t blk_addr = 0;\n\n\t/* get segment number and block addr */\n\tif (IS_DATASEG(type)) {\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[type]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_DATA]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_TYPE, type);\n\t\telse\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_DATA_TYPE, type);\n\t} else {\n\t\tsegno = le32_to_cpu(ckpt->cur_node_segno[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_node_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_NODE_TYPE,\n\t\t\t\t\t\t\ttype - CURSEG_HOT_NODE);\n\t\telse\n\t\t\tblk_addr = GET_SUM_BLOCK(sbi, segno);\n\t}\n\n\tnew = get_meta_page(sbi, blk_addr);\n\tsum = (struct f2fs_summary_block *)page_address(new);\n\n\tif (IS_NODESEG(type)) {\n\t\tif (__exist_node_summaries(sbi)) {\n\t\t\tstruct f2fs_summary *ns = &sum->entries[0];\n\t\t\tint i;\n\t\t\tfor (i = 0; i < sbi->blocks_per_seg; i++, ns++) {\n\t\t\t\tns->version = 0;\n\t\t\t\tns->ofs_in_node = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tint err;\n\n\t\t\terr = restore_node_summary(sbi, segno, sum);\n\t\t\tif (err) {\n\t\t\t\tf2fs_put_page(new, 1);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* set uncompleted segment to curseg */\n\tcurseg = CURSEG_I(sbi, type);\n\tmutex_lock(&curseg->curseg_mutex);\n\n\t/* update journal info */\n\tdown_write(&curseg->journal_rwsem);\n\tmemcpy(curseg->journal, &sum->journal, SUM_JOURNAL_SIZE);\n\tup_write(&curseg->journal_rwsem);\n\n\tmemcpy(curseg->sum_blk->entries, sum->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&curseg->sum_blk->footer, &sum->footer, SUM_FOOTER_SIZE);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 0);\n\tcurseg->alloc_type = ckpt->alloc_type[type];\n\tcurseg->next_blkoff = blk_off;\n\tmutex_unlock(&curseg->curseg_mutex);\n\tf2fs_put_page(new, 1);\n\treturn 0;\n}\n\nstatic int restore_curseg_summaries(struct f2fs_sb_info *sbi)\n{\n\tint type = CURSEG_HOT_DATA;\n\tint err;\n\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG)) {\n\t\tint npages = npages_for_summary_flush(sbi, true);\n\n\t\tif (npages >= 2)\n\t\t\tra_meta_pages(sbi, start_sum_block(sbi), npages,\n\t\t\t\t\t\t\tMETA_CP, true);\n\n\t\t/* restore for compacted data summary */\n\t\tif (read_compacted_summaries(sbi))\n\t\t\treturn -EINVAL;\n\t\ttype = CURSEG_HOT_NODE;\n\t}\n\n\tif (__exist_node_summaries(sbi))\n\t\tra_meta_pages(sbi, sum_blk_addr(sbi, NR_CURSEG_TYPE, type),\n\t\t\t\t\tNR_CURSEG_TYPE - type, META_CP, true);\n\n\tfor (; type <= CURSEG_COLD_NODE; type++) {\n\t\terr = read_normal_summaries(sbi, type);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void write_compacted_summaries(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct page *page;\n\tunsigned char *kaddr;\n\tstruct f2fs_summary *summary;\n\tstruct curseg_info *seg_i;\n\tint written_size = 0;\n\tint i, j;\n\n\tpage = grab_meta_page(sbi, blkaddr++);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t/* Step 1: write nat cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(kaddr, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t/* Step 2: write sit cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(kaddr + written_size, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t/* Step 3: write summary entries */\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blkoff;\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tif (sbi->ckpt->alloc_type[i] == SSR)\n\t\t\tblkoff = sbi->blocks_per_seg;\n\t\telse\n\t\t\tblkoff = curseg_blkoff(sbi, i);\n\n\t\tfor (j = 0; j < blkoff; j++) {\n\t\t\tif (!page) {\n\t\t\t\tpage = grab_meta_page(sbi, blkaddr++);\n\t\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\t\twritten_size = 0;\n\t\t\t}\n\t\t\tsummary = (struct f2fs_summary *)(kaddr + written_size);\n\t\t\t*summary = seg_i->sum_blk->entries[j];\n\t\t\twritten_size += SUMMARY_SIZE;\n\n\t\t\tif (written_size + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\t\t}\n\t}\n\tif (page) {\n\t\tset_page_dirty(page);\n\t\tf2fs_put_page(page, 1);\n\t}\n}\n\nstatic void write_normal_summaries(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type)\n{\n\tint i, end;\n\tif (IS_DATASEG(type))\n\t\tend = type + NR_CURSEG_DATA_TYPE;\n\telse\n\t\tend = type + NR_CURSEG_NODE_TYPE;\n\n\tfor (i = type; i < end; i++)\n\t\twrite_current_sum_page(sbi, i, blkaddr + (i - type));\n}\n\nvoid write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG))\n\t\twrite_compacted_summaries(sbi, start_blk);\n\telse\n\t\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_DATA);\n}\n\nvoid write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_NODE);\n}\n\nint lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\t\t\tunsigned int val, int alloc)\n{\n\tint i;\n\n\tif (type == NAT_JOURNAL) {\n\t\tfor (i = 0; i < nats_in_cursum(journal); i++) {\n\t\t\tif (le32_to_cpu(nid_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\t}\n\t\tif (alloc && __has_cursum_space(journal, 1, NAT_JOURNAL))\n\t\t\treturn update_nats_in_cursum(journal, 1);\n\t} else if (type == SIT_JOURNAL) {\n\t\tfor (i = 0; i < sits_in_cursum(journal); i++)\n\t\t\tif (le32_to_cpu(segno_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\tif (alloc && __has_cursum_space(journal, 1, SIT_JOURNAL))\n\t\t\treturn update_sits_in_cursum(journal, 1);\n\t}\n\treturn -1;\n}\n\nstatic struct page *get_current_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int segno)\n{\n\treturn get_meta_page(sbi, current_sit_addr(sbi, segno));\n}\n\nstatic struct page *get_next_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int start)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct page *src_page, *dst_page;\n\tpgoff_t src_off, dst_off;\n\tvoid *src_addr, *dst_addr;\n\n\tsrc_off = current_sit_addr(sbi, start);\n\tdst_off = next_sit_addr(sbi, src_off);\n\n\t/* get current sit block page without lock */\n\tsrc_page = get_meta_page(sbi, src_off);\n\tdst_page = grab_meta_page(sbi, dst_off);\n\tf2fs_bug_on(sbi, PageDirty(src_page));\n\n\tsrc_addr = page_address(src_page);\n\tdst_addr = page_address(dst_page);\n\tmemcpy(dst_addr, src_addr, PAGE_SIZE);\n\n\tset_page_dirty(dst_page);\n\tf2fs_put_page(src_page, 1);\n\n\tset_to_next_sit(sit_i, start);\n\n\treturn dst_page;\n}\n\nstatic struct sit_entry_set *grab_sit_entry_set(void)\n{\n\tstruct sit_entry_set *ses =\n\t\t\tf2fs_kmem_cache_alloc(sit_entry_set_slab, GFP_NOFS);\n\n\tses->entry_cnt = 0;\n\tINIT_LIST_HEAD(&ses->set_list);\n\treturn ses;\n}\n\nstatic void release_sit_entry_set(struct sit_entry_set *ses)\n{\n\tlist_del(&ses->set_list);\n\tkmem_cache_free(sit_entry_set_slab, ses);\n}\n\nstatic void adjust_sit_entry_set(struct sit_entry_set *ses,\n\t\t\t\t\t\tstruct list_head *head)\n{\n\tstruct sit_entry_set *next = ses;\n\n\tif (list_is_last(&ses->set_list, head))\n\t\treturn;\n\n\tlist_for_each_entry_continue(next, head, set_list)\n\t\tif (ses->entry_cnt <= next->entry_cnt)\n\t\t\tbreak;\n\n\tlist_move_tail(&ses->set_list, &next->set_list);\n}\n\nstatic void add_sit_entry(unsigned int segno, struct list_head *head)\n{\n\tstruct sit_entry_set *ses;\n\tunsigned int start_segno = START_SEGNO(segno);\n\n\tlist_for_each_entry(ses, head, set_list) {\n\t\tif (ses->start_segno == start_segno) {\n\t\t\tses->entry_cnt++;\n\t\t\tadjust_sit_entry_set(ses, head);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tses = grab_sit_entry_set();\n\n\tses->start_segno = start_segno;\n\tses->entry_cnt++;\n\tlist_add(&ses->set_list, head);\n}\n\nstatic void add_sits_in_set(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\tstruct list_head *set_list = &sm_info->sit_entry_set;\n\tunsigned long *bitmap = SIT_I(sbi)->dirty_sentries_bitmap;\n\tunsigned int segno;\n\n\tfor_each_set_bit(segno, bitmap, MAIN_SEGS(sbi))\n\t\tadd_sit_entry(segno, set_list);\n}\n\nstatic void remove_sits_in_journal(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tint i;\n\n\tdown_write(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int segno;\n\t\tbool dirtied;\n\n\t\tsegno = le32_to_cpu(segno_in_journal(journal, i));\n\t\tdirtied = __mark_sit_entry_dirty(sbi, segno);\n\n\t\tif (!dirtied)\n\t\t\tadd_sit_entry(segno, &SM_I(sbi)->sit_entry_set);\n\t}\n\tupdate_sits_in_cursum(journal, -i);\n\tup_write(&curseg->journal_rwsem);\n}\n\n/*\n * CP calls this function, which flushes SIT entries including sit_journal,\n * and moves prefree segs to free segs.\n */\nvoid flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned long *bitmap = sit_i->dirty_sentries_bitmap;\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct sit_entry_set *ses, *tmp;\n\tstruct list_head *head = &SM_I(sbi)->sit_entry_set;\n\tbool to_journal = true;\n\tstruct seg_entry *se;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tif (!sit_i->dirty_sentries)\n\t\tgoto out;\n\n\t/*\n\t * add and account sit entries of dirty bitmap in sit entry\n\t * set temporarily\n\t */\n\tadd_sits_in_set(sbi);\n\n\t/*\n\t * if there are no enough space in journal to store dirty sit\n\t * entries, remove all entries from journal and add and account\n\t * them in sit entry set.\n\t */\n\tif (!__has_cursum_space(journal, sit_i->dirty_sentries, SIT_JOURNAL))\n\t\tremove_sits_in_journal(sbi);\n\n\t/*\n\t * there are two steps to flush sit entries:\n\t * #1, flush sit entries to journal in current cold data summary block.\n\t * #2, flush sit entries to sit page.\n\t */\n\tlist_for_each_entry_safe(ses, tmp, head, set_list) {\n\t\tstruct page *page = NULL;\n\t\tstruct f2fs_sit_block *raw_sit = NULL;\n\t\tunsigned int start_segno = ses->start_segno;\n\t\tunsigned int end = min(start_segno + SIT_ENTRY_PER_BLOCK,\n\t\t\t\t\t\t(unsigned long)MAIN_SEGS(sbi));\n\t\tunsigned int segno = start_segno;\n\n\t\tif (to_journal &&\n\t\t\t!__has_cursum_space(journal, ses->entry_cnt, SIT_JOURNAL))\n\t\t\tto_journal = false;\n\n\t\tif (to_journal) {\n\t\t\tdown_write(&curseg->journal_rwsem);\n\t\t} else {\n\t\t\tpage = get_next_sit_page(sbi, start_segno);\n\t\t\traw_sit = page_address(page);\n\t\t}\n\n\t\t/* flush dirty sit entries in region of current sit set */\n\t\tfor_each_set_bit_from(segno, bitmap, end) {\n\t\t\tint offset, sit_offset;\n\n\t\t\tse = get_seg_entry(sbi, segno);\n\n\t\t\t/* add discard candidates */\n\t\t\tif (!(cpc->reason & CP_DISCARD)) {\n\t\t\t\tcpc->trim_start = segno;\n\t\t\t\tadd_discard_addrs(sbi, cpc, false);\n\t\t\t}\n\n\t\t\tif (to_journal) {\n\t\t\t\toffset = lookup_journal_in_cursum(journal,\n\t\t\t\t\t\t\tSIT_JOURNAL, segno, 1);\n\t\t\t\tf2fs_bug_on(sbi, offset < 0);\n\t\t\t\tsegno_in_journal(journal, offset) =\n\t\t\t\t\t\t\tcpu_to_le32(segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t&sit_in_journal(journal, offset));\n\t\t\t} else {\n\t\t\t\tsit_offset = SIT_ENTRY_OFFSET(sit_i, segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t\t&raw_sit->entries[sit_offset]);\n\t\t\t}\n\n\t\t\t__clear_bit(segno, bitmap);\n\t\t\tsit_i->dirty_sentries--;\n\t\t\tses->entry_cnt--;\n\t\t}\n\n\t\tif (to_journal)\n\t\t\tup_write(&curseg->journal_rwsem);\n\t\telse\n\t\t\tf2fs_put_page(page, 1);\n\n\t\tf2fs_bug_on(sbi, ses->entry_cnt);\n\t\trelease_sit_entry_set(ses);\n\t}\n\n\tf2fs_bug_on(sbi, !list_empty(head));\n\tf2fs_bug_on(sbi, sit_i->dirty_sentries);\nout:\n\tif (cpc->reason & CP_DISCARD) {\n\t\t__u64 trim_start = cpc->trim_start;\n\n\t\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++)\n\t\t\tadd_discard_addrs(sbi, cpc, false);\n\n\t\tcpc->trim_start = trim_start;\n\t}\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\tset_prefree_as_free_segments(sbi);\n}\n\nstatic int build_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct sit_info *sit_i;\n\tunsigned int sit_segs, start;\n\tchar *src_bitmap;\n\tunsigned int bitmap_size;\n\n\t/* allocate memory for SIT information */\n\tsit_i = kzalloc(sizeof(struct sit_info), GFP_KERNEL);\n\tif (!sit_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->sit_info = sit_i;\n\n\tsit_i->sentries = f2fs_kvzalloc(MAIN_SEGS(sbi) *\n\t\t\t\t\tsizeof(struct seg_entry), GFP_KERNEL);\n\tif (!sit_i->sentries)\n\t\treturn -ENOMEM;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tsit_i->dirty_sentries_bitmap = f2fs_kvzalloc(bitmap_size, GFP_KERNEL);\n\tif (!sit_i->dirty_sentries_bitmap)\n\t\treturn -ENOMEM;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tsit_i->sentries[start].cur_valid_map\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tsit_i->sentries[start].ckpt_valid_map\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tif (!sit_i->sentries[start].cur_valid_map ||\n\t\t\t\t!sit_i->sentries[start].ckpt_valid_map)\n\t\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tsit_i->sentries[start].cur_valid_map_mir\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tif (!sit_i->sentries[start].cur_valid_map_mir)\n\t\t\treturn -ENOMEM;\n#endif\n\n\t\tif (f2fs_discard_en(sbi)) {\n\t\t\tsit_i->sentries[start].discard_map\n\t\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\t\tif (!sit_i->sentries[start].discard_map)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tsit_i->tmp_map = kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\tif (!sit_i->tmp_map)\n\t\treturn -ENOMEM;\n\n\tif (sbi->segs_per_sec > 1) {\n\t\tsit_i->sec_entries = f2fs_kvzalloc(MAIN_SECS(sbi) *\n\t\t\t\t\tsizeof(struct sec_entry), GFP_KERNEL);\n\t\tif (!sit_i->sec_entries)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* get information related with SIT */\n\tsit_segs = le32_to_cpu(raw_super->segment_count_sit) >> 1;\n\n\t/* setup SIT bitmap from ckeckpoint pack */\n\tbitmap_size = __bitmap_size(sbi, SIT_BITMAP);\n\tsrc_bitmap = __bitmap_ptr(sbi, SIT_BITMAP);\n\n\tsit_i->sit_bitmap = kmemdup(src_bitmap, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap)\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\tsit_i->sit_bitmap_mir = kmemdup(src_bitmap, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap_mir)\n\t\treturn -ENOMEM;\n#endif\n\n\t/* init SIT information */\n\tsit_i->s_ops = &default_salloc_ops;\n\n\tsit_i->sit_base_addr = le32_to_cpu(raw_super->sit_blkaddr);\n\tsit_i->sit_blocks = sit_segs << sbi->log_blocks_per_seg;\n\tsit_i->written_valid_blocks = 0;\n\tsit_i->bitmap_size = bitmap_size;\n\tsit_i->dirty_sentries = 0;\n\tsit_i->sents_per_block = SIT_ENTRY_PER_BLOCK;\n\tsit_i->elapsed_time = le64_to_cpu(sbi->ckpt->elapsed_time);\n\tsit_i->mounted_time = CURRENT_TIME_SEC.tv_sec;\n\tmutex_init(&sit_i->sentry_lock);\n\treturn 0;\n}\n\nstatic int build_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i;\n\tunsigned int bitmap_size, sec_bitmap_size;\n\n\t/* allocate memory for free segmap information */\n\tfree_i = kzalloc(sizeof(struct free_segmap_info), GFP_KERNEL);\n\tif (!free_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->free_info = free_i;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tfree_i->free_segmap = f2fs_kvmalloc(bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_segmap)\n\t\treturn -ENOMEM;\n\n\tsec_bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\tfree_i->free_secmap = f2fs_kvmalloc(sec_bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_secmap)\n\t\treturn -ENOMEM;\n\n\t/* set all segments as dirty temporarily */\n\tmemset(free_i->free_segmap, 0xff, bitmap_size);\n\tmemset(free_i->free_secmap, 0xff, sec_bitmap_size);\n\n\t/* init free segmap information */\n\tfree_i->start_segno = GET_SEGNO_FROM_SEG0(sbi, MAIN_BLKADDR(sbi));\n\tfree_i->free_segments = 0;\n\tfree_i->free_sections = 0;\n\tspin_lock_init(&free_i->segmap_lock);\n\treturn 0;\n}\n\nstatic int build_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array;\n\tint i;\n\n\tarray = kcalloc(NR_CURSEG_TYPE, sizeof(*array), GFP_KERNEL);\n\tif (!array)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->curseg_array = array;\n\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tmutex_init(&array[i].curseg_mutex);\n\t\tarray[i].sum_blk = kzalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (!array[i].sum_blk)\n\t\t\treturn -ENOMEM;\n\t\tinit_rwsem(&array[i].journal_rwsem);\n\t\tarray[i].journal = kzalloc(sizeof(struct f2fs_journal),\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!array[i].journal)\n\t\t\treturn -ENOMEM;\n\t\tarray[i].segno = NULL_SEGNO;\n\t\tarray[i].next_blkoff = 0;\n\t}\n\treturn restore_curseg_summaries(sbi);\n}\n\nstatic void build_sit_entries(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct seg_entry *se;\n\tstruct f2fs_sit_entry sit;\n\tint sit_blk_cnt = SIT_BLK_CNT(sbi);\n\tunsigned int i, start, end;\n\tunsigned int readed, start_blk = 0;\n\n\tdo {\n\t\treaded = ra_meta_pages(sbi, start_blk, BIO_MAX_PAGES,\n\t\t\t\t\t\t\tMETA_SIT, true);\n\n\t\tstart = start_blk * sit_i->sents_per_block;\n\t\tend = (start_blk + readed) * sit_i->sents_per_block;\n\n\t\tfor (; start < end && start < MAIN_SEGS(sbi); start++) {\n\t\t\tstruct f2fs_sit_block *sit_blk;\n\t\t\tstruct page *page;\n\n\t\t\tse = &sit_i->sentries[start];\n\t\t\tpage = get_current_sit_page(sbi, start);\n\t\t\tsit_blk = (struct f2fs_sit_block *)page_address(page);\n\t\t\tsit = sit_blk->entries[SIT_ENTRY_OFFSET(sit_i, start)];\n\t\t\tf2fs_put_page(page, 1);\n\n\t\t\tcheck_block_count(sbi, start, &sit);\n\t\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\t\t/* build discard map only one time */\n\t\t\tif (f2fs_discard_en(sbi)) {\n\t\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t} else {\n\t\t\t\t\tmemcpy(se->discard_map,\n\t\t\t\t\t\tse->cur_valid_map,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t\tsbi->discard_blks +=\n\t\t\t\t\t\tsbi->blocks_per_seg -\n\t\t\t\t\t\tse->valid_blocks;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sbi->segs_per_sec > 1)\n\t\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t}\n\t\tstart_blk += readed;\n\t} while (start_blk < sit_blk_cnt);\n\n\tdown_read(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int old_valid_blocks;\n\n\t\tstart = le32_to_cpu(segno_in_journal(journal, i));\n\t\tse = &sit_i->sentries[start];\n\t\tsit = sit_in_journal(journal, i);\n\n\t\told_valid_blocks = se->valid_blocks;\n\n\t\tcheck_block_count(sbi, start, &sit);\n\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\tif (f2fs_discard_en(sbi)) {\n\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t} else {\n\t\t\t\tmemcpy(se->discard_map, se->cur_valid_map,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\tsbi->discard_blks += old_valid_blocks -\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t\t}\n\t\t}\n\n\t\tif (sbi->segs_per_sec > 1)\n\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\tse->valid_blocks - old_valid_blocks;\n\t}\n\tup_read(&curseg->journal_rwsem);\n}\n\nstatic void init_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tunsigned int start;\n\tint type;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, start);\n\t\tif (!sentry->valid_blocks)\n\t\t\t__set_free(sbi, start);\n\t\telse\n\t\t\tSIT_I(sbi)->written_valid_blocks +=\n\t\t\t\t\t\tsentry->valid_blocks;\n\t}\n\n\t/* set use the current segments */\n\tfor (type = CURSEG_HOT_DATA; type <= CURSEG_COLD_NODE; type++) {\n\t\tstruct curseg_info *curseg_t = CURSEG_I(sbi, type);\n\t\t__set_test_and_inuse(sbi, curseg_t->segno);\n\t}\n}\n\nstatic void init_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno = 0, offset = 0;\n\tunsigned short valid_blocks;\n\n\twhile (1) {\n\t\t/* find dirty segment based on free segmap */\n\t\tsegno = find_next_inuse(free_i, MAIN_SEGS(sbi), offset);\n\t\tif (segno >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\toffset = segno + 1;\n\t\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\t\tif (valid_blocks == sbi->blocks_per_seg || !valid_blocks)\n\t\t\tcontinue;\n\t\tif (valid_blocks > sbi->blocks_per_seg) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tcontinue;\n\t\t}\n\t\tmutex_lock(&dirty_i->seglist_lock);\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t\tmutex_unlock(&dirty_i->seglist_lock);\n\t}\n}\n\nstatic int init_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\n\tdirty_i->victim_secmap = f2fs_kvzalloc(bitmap_size, GFP_KERNEL);\n\tif (!dirty_i->victim_secmap)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic int build_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i;\n\tunsigned int bitmap_size, i;\n\n\t/* allocate memory for dirty segments list information */\n\tdirty_i = kzalloc(sizeof(struct dirty_seglist_info), GFP_KERNEL);\n\tif (!dirty_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->dirty_info = dirty_i;\n\tmutex_init(&dirty_i->seglist_lock);\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++) {\n\t\tdirty_i->dirty_segmap[i] = f2fs_kvzalloc(bitmap_size, GFP_KERNEL);\n\t\tif (!dirty_i->dirty_segmap[i])\n\t\t\treturn -ENOMEM;\n\t}\n\n\tinit_dirty_segmap(sbi);\n\treturn init_victim_secmap(sbi);\n}\n\n/*\n * Update min, max modified time for cost-benefit GC algorithm\n */\nstatic void init_min_max_mtime(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tsit_i->min_mtime = LLONG_MAX;\n\n\tfor (segno = 0; segno < MAIN_SEGS(sbi); segno += sbi->segs_per_sec) {\n\t\tunsigned int i;\n\t\tunsigned long long mtime = 0;\n\n\t\tfor (i = 0; i < sbi->segs_per_sec; i++)\n\t\t\tmtime += get_seg_entry(sbi, segno + i)->mtime;\n\n\t\tmtime = div_u64(mtime, sbi->segs_per_sec);\n\n\t\tif (sit_i->min_mtime > mtime)\n\t\t\tsit_i->min_mtime = mtime;\n\t}\n\tsit_i->max_mtime = get_mtime(sbi);\n\tmutex_unlock(&sit_i->sentry_lock);\n}\n\nint build_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_sm_info *sm_info;\n\tint err;\n\n\tsm_info = kzalloc(sizeof(struct f2fs_sm_info), GFP_KERNEL);\n\tif (!sm_info)\n\t\treturn -ENOMEM;\n\n\t/* init sm info */\n\tsbi->sm_info = sm_info;\n\tsm_info->seg0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tsm_info->main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tsm_info->segment_count = le32_to_cpu(raw_super->segment_count);\n\tsm_info->reserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\tsm_info->ovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\tsm_info->main_segments = le32_to_cpu(raw_super->segment_count_main);\n\tsm_info->ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tsm_info->rec_prefree_segments = sm_info->main_segments *\n\t\t\t\t\tDEF_RECLAIM_PREFREE_SEGMENTS / 100;\n\tif (sm_info->rec_prefree_segments > DEF_MAX_RECLAIM_PREFREE_SEGMENTS)\n\t\tsm_info->rec_prefree_segments = DEF_MAX_RECLAIM_PREFREE_SEGMENTS;\n\n\tif (!test_opt(sbi, LFS))\n\t\tsm_info->ipu_policy = 1 << F2FS_IPU_FSYNC;\n\tsm_info->min_ipu_util = DEF_MIN_IPU_UTIL;\n\tsm_info->min_fsync_blocks = DEF_MIN_FSYNC_BLOCKS;\n\tsm_info->min_hot_blocks = DEF_MIN_HOT_BLOCKS;\n\n\tsm_info->trim_sections = DEF_BATCHED_TRIM_SECTIONS;\n\n\tINIT_LIST_HEAD(&sm_info->sit_entry_set);\n\n\tif (test_opt(sbi, FLUSH_MERGE) && !f2fs_readonly(sbi->sb)) {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = create_discard_cmd_control(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = build_sit_info(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_free_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_curseg(sbi);\n\tif (err)\n\t\treturn err;\n\n\t/* reinit free segmap based on SIT */\n\tbuild_sit_entries(sbi);\n\n\tinit_free_segmap(sbi);\n\terr = build_dirty_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\n\tinit_min_max_mtime(sbi);\n\treturn 0;\n}\n\nstatic void discard_dirty_segmap(struct f2fs_sb_info *sbi,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tkvfree(dirty_i->dirty_segmap[dirty_type]);\n\tdirty_i->nr_dirty[dirty_type] = 0;\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic void destroy_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tkvfree(dirty_i->victim_secmap);\n}\n\nstatic void destroy_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tint i;\n\n\tif (!dirty_i)\n\t\treturn;\n\n\t/* discard pre-free/dirty segments list */\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++)\n\t\tdiscard_dirty_segmap(sbi, i);\n\n\tdestroy_victim_secmap(sbi);\n\tSM_I(sbi)->dirty_info = NULL;\n\tkfree(dirty_i);\n}\n\nstatic void destroy_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array = SM_I(sbi)->curseg_array;\n\tint i;\n\n\tif (!array)\n\t\treturn;\n\tSM_I(sbi)->curseg_array = NULL;\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tkfree(array[i].sum_blk);\n\t\tkfree(array[i].journal);\n\t}\n\tkfree(array);\n}\n\nstatic void destroy_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i = SM_I(sbi)->free_info;\n\tif (!free_i)\n\t\treturn;\n\tSM_I(sbi)->free_info = NULL;\n\tkvfree(free_i->free_segmap);\n\tkvfree(free_i->free_secmap);\n\tkfree(free_i);\n}\n\nstatic void destroy_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int start;\n\n\tif (!sit_i)\n\t\treturn;\n\n\tif (sit_i->sentries) {\n\t\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\t\tkfree(sit_i->sentries[start].cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tkfree(sit_i->sentries[start].cur_valid_map_mir);\n#endif\n\t\t\tkfree(sit_i->sentries[start].ckpt_valid_map);\n\t\t\tkfree(sit_i->sentries[start].discard_map);\n\t\t}\n\t}\n\tkfree(sit_i->tmp_map);\n\n\tkvfree(sit_i->sentries);\n\tkvfree(sit_i->sec_entries);\n\tkvfree(sit_i->dirty_sentries_bitmap);\n\n\tSM_I(sbi)->sit_info = NULL;\n\tkfree(sit_i->sit_bitmap);\n#ifdef CONFIG_F2FS_CHECK_FS\n\tkfree(sit_i->sit_bitmap_mir);\n#endif\n\tkfree(sit_i);\n}\n\nvoid destroy_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\n\tif (!sm_info)\n\t\treturn;\n\tdestroy_flush_cmd_control(sbi, true);\n\tdestroy_discard_cmd_control(sbi);\n\tdestroy_dirty_segmap(sbi);\n\tdestroy_curseg(sbi);\n\tdestroy_free_segmap(sbi);\n\tdestroy_sit_info(sbi);\n\tsbi->sm_info = NULL;\n\tkfree(sm_info);\n}\n\nint __init create_segment_manager_caches(void)\n{\n\tdiscard_entry_slab = f2fs_kmem_cache_create(\"discard_entry\",\n\t\t\tsizeof(struct discard_entry));\n\tif (!discard_entry_slab)\n\t\tgoto fail;\n\n\tdiscard_cmd_slab = f2fs_kmem_cache_create(\"discard_cmd\",\n\t\t\tsizeof(struct discard_cmd));\n\tif (!discard_cmd_slab)\n\t\tgoto destroy_discard_entry;\n\n\tsit_entry_set_slab = f2fs_kmem_cache_create(\"sit_entry_set\",\n\t\t\tsizeof(struct sit_entry_set));\n\tif (!sit_entry_set_slab)\n\t\tgoto destroy_discard_cmd;\n\n\tinmem_entry_slab = f2fs_kmem_cache_create(\"inmem_page_entry\",\n\t\t\tsizeof(struct inmem_pages));\n\tif (!inmem_entry_slab)\n\t\tgoto destroy_sit_entry_set;\n\treturn 0;\n\ndestroy_sit_entry_set:\n\tkmem_cache_destroy(sit_entry_set_slab);\ndestroy_discard_cmd:\n\tkmem_cache_destroy(discard_cmd_slab);\ndestroy_discard_entry:\n\tkmem_cache_destroy(discard_entry_slab);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid destroy_segment_manager_caches(void)\n{\n\tkmem_cache_destroy(sit_entry_set_slab);\n\tkmem_cache_destroy(discard_cmd_slab);\n\tkmem_cache_destroy(discard_entry_slab);\n\tkmem_cache_destroy(inmem_entry_slab);\n}\n"], "fixing_code": ["/*\n * fs/f2fs/segment.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/prefetch.h>\n#include <linux/kthread.h>\n#include <linux/swap.h>\n#include <linux/timer.h>\n#include <linux/freezer.h>\n\n#include \"f2fs.h\"\n#include \"segment.h\"\n#include \"node.h\"\n#include \"trace.h\"\n#include <trace/events/f2fs.h>\n\n#define __reverse_ffz(x) __reverse_ffs(~(x))\n\nstatic struct kmem_cache *discard_entry_slab;\nstatic struct kmem_cache *discard_cmd_slab;\nstatic struct kmem_cache *sit_entry_set_slab;\nstatic struct kmem_cache *inmem_entry_slab;\n\nstatic unsigned long __reverse_ulong(unsigned char *str)\n{\n\tunsigned long tmp = 0;\n\tint shift = 24, idx = 0;\n\n#if BITS_PER_LONG == 64\n\tshift = 56;\n#endif\n\twhile (shift >= 0) {\n\t\ttmp |= (unsigned long)str[idx++] << shift;\n\t\tshift -= BITS_PER_BYTE;\n\t}\n\treturn tmp;\n}\n\n/*\n * __reverse_ffs is copied from include/asm-generic/bitops/__ffs.h since\n * MSB and LSB are reversed in a byte by f2fs_set_bit.\n */\nstatic inline unsigned long __reverse_ffs(unsigned long word)\n{\n\tint num = 0;\n\n#if BITS_PER_LONG == 64\n\tif ((word & 0xffffffff00000000UL) == 0)\n\t\tnum += 32;\n\telse\n\t\tword >>= 32;\n#endif\n\tif ((word & 0xffff0000) == 0)\n\t\tnum += 16;\n\telse\n\t\tword >>= 16;\n\n\tif ((word & 0xff00) == 0)\n\t\tnum += 8;\n\telse\n\t\tword >>= 8;\n\n\tif ((word & 0xf0) == 0)\n\t\tnum += 4;\n\telse\n\t\tword >>= 4;\n\n\tif ((word & 0xc) == 0)\n\t\tnum += 2;\n\telse\n\t\tword >>= 2;\n\n\tif ((word & 0x2) == 0)\n\t\tnum += 1;\n\treturn num;\n}\n\n/*\n * __find_rev_next(_zero)_bit is copied from lib/find_next_bit.c because\n * f2fs_set_bit makes MSB and LSB reversed in a byte.\n * @size must be integral times of unsigned long.\n * Example:\n *                             MSB <--> LSB\n *   f2fs_set_bit(0, bitmap) => 1000 0000\n *   f2fs_set_bit(7, bitmap) => 0000 0001\n */\nstatic unsigned long __find_rev_next_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == 0)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\ttmp &= ~0UL >> offset;\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp &= (~0UL << (BITS_PER_LONG - size));\n\t\tif (tmp)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffs(tmp);\n}\n\nstatic unsigned long __find_rev_next_zero_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == ~0UL)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\tif (offset)\n\t\t\ttmp |= ~0UL << (BITS_PER_LONG - offset);\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp |= ~0UL >> size;\n\t\tif (tmp != ~0UL)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffz(tmp);\n}\n\nvoid register_inmem_page(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inmem_pages *new;\n\n\tf2fs_trace_pid(page);\n\n\tset_page_private(page, (unsigned long)ATOMIC_WRITTEN_PAGE);\n\tSetPagePrivate(page);\n\n\tnew = f2fs_kmem_cache_alloc(inmem_entry_slab, GFP_NOFS);\n\n\t/* add atomic page indices to the list */\n\tnew->page = page;\n\tINIT_LIST_HEAD(&new->list);\n\n\t/* increase reference count with clean state */\n\tmutex_lock(&fi->inmem_lock);\n\tget_page(page);\n\tlist_add_tail(&new->list, &fi->inmem_pages);\n\tinc_page_count(F2FS_I_SB(inode), F2FS_INMEM_PAGES);\n\tmutex_unlock(&fi->inmem_lock);\n\n\ttrace_f2fs_register_inmem_page(page, INMEM);\n}\n\nstatic int __revoke_inmem_pages(struct inode *inode,\n\t\t\t\tstruct list_head *head, bool drop, bool recover)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct inmem_pages *cur, *tmp;\n\tint err = 0;\n\n\tlist_for_each_entry_safe(cur, tmp, head, list) {\n\t\tstruct page *page = cur->page;\n\n\t\tif (drop)\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM_DROP);\n\n\t\tlock_page(page);\n\n\t\tif (recover) {\n\t\t\tstruct dnode_of_data dn;\n\t\t\tstruct node_info ni;\n\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM_REVOKE);\n\n\t\t\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\t\t\tif (get_dnode_of_data(&dn, page->index, LOOKUP_NODE)) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tget_node_info(sbi, dn.nid, &ni);\n\t\t\tf2fs_replace_block(sbi, &dn, dn.data_blkaddr,\n\t\t\t\t\tcur->old_addr, ni.version, true, true);\n\t\t\tf2fs_put_dnode(&dn);\n\t\t}\nnext:\n\t\t/* we don't need to invalidate this in the sccessful status */\n\t\tif (drop || recover)\n\t\t\tClearPageUptodate(page);\n\t\tset_page_private(page, 0);\n\t\tClearPagePrivate(page);\n\t\tf2fs_put_page(page, 1);\n\n\t\tlist_del(&cur->list);\n\t\tkmem_cache_free(inmem_entry_slab, cur);\n\t\tdec_page_count(F2FS_I_SB(inode), F2FS_INMEM_PAGES);\n\t}\n\treturn err;\n}\n\nvoid drop_inmem_pages(struct inode *inode)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tmutex_lock(&fi->inmem_lock);\n\t__revoke_inmem_pages(inode, &fi->inmem_pages, true, false);\n\tmutex_unlock(&fi->inmem_lock);\n\n\tclear_inode_flag(inode, FI_ATOMIC_FILE);\n\tstat_dec_atomic_write(inode);\n}\n\nvoid drop_inmem_page(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct list_head *head = &fi->inmem_pages;\n\tstruct inmem_pages *cur = NULL;\n\n\tf2fs_bug_on(sbi, !IS_ATOMIC_WRITTEN_PAGE(page));\n\n\tmutex_lock(&fi->inmem_lock);\n\tlist_for_each_entry(cur, head, list) {\n\t\tif (cur->page == page)\n\t\t\tbreak;\n\t}\n\n\tf2fs_bug_on(sbi, !cur || cur->page != page);\n\tlist_del(&cur->list);\n\tmutex_unlock(&fi->inmem_lock);\n\n\tdec_page_count(sbi, F2FS_INMEM_PAGES);\n\tkmem_cache_free(inmem_entry_slab, cur);\n\n\tClearPageUptodate(page);\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n\tf2fs_put_page(page, 0);\n\n\ttrace_f2fs_commit_inmem_page(page, INMEM_INVALIDATE);\n}\n\nstatic int __commit_inmem_pages(struct inode *inode,\n\t\t\t\t\tstruct list_head *revoke_list)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inmem_pages *cur, *tmp;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_PRIO,\n\t};\n\tpgoff_t last_idx = ULONG_MAX;\n\tint err = 0;\n\n\tlist_for_each_entry_safe(cur, tmp, &fi->inmem_pages, list) {\n\t\tstruct page *page = cur->page;\n\n\t\tlock_page(page);\n\t\tif (page->mapping == inode->i_mapping) {\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM);\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_wait_on_page_writeback(page, DATA, true);\n\t\t\tif (clear_page_dirty_for_io(page)) {\n\t\t\t\tinode_dec_dirty_pages(inode);\n\t\t\t\tremove_dirty_inode(inode);\n\t\t\t}\n\n\t\t\tfio.page = page;\n\t\t\tfio.old_blkaddr = NULL_ADDR;\n\t\t\tfio.encrypted_page = NULL;\n\t\t\tfio.need_lock = LOCK_DONE;\n\t\t\terr = do_write_data_page(&fio);\n\t\t\tif (err) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* record old blkaddr for revoking */\n\t\t\tcur->old_addr = fio.old_blkaddr;\n\t\t\tlast_idx = page->index;\n\t\t}\n\t\tunlock_page(page);\n\t\tlist_move_tail(&cur->list, revoke_list);\n\t}\n\n\tif (last_idx != ULONG_MAX)\n\t\tf2fs_submit_merged_write_cond(sbi, inode, 0, last_idx, DATA);\n\n\tif (!err)\n\t\t__revoke_inmem_pages(inode, revoke_list, false, false);\n\n\treturn err;\n}\n\nint commit_inmem_pages(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct list_head revoke_list;\n\tint err;\n\n\tINIT_LIST_HEAD(&revoke_list);\n\tf2fs_balance_fs(sbi, true);\n\tf2fs_lock_op(sbi);\n\n\tset_inode_flag(inode, FI_ATOMIC_COMMIT);\n\n\tmutex_lock(&fi->inmem_lock);\n\terr = __commit_inmem_pages(inode, &revoke_list);\n\tif (err) {\n\t\tint ret;\n\t\t/*\n\t\t * try to revoke all committed pages, but still we could fail\n\t\t * due to no memory or other reason, if that happened, EAGAIN\n\t\t * will be returned, which means in such case, transaction is\n\t\t * already not integrity, caller should use journal to do the\n\t\t * recovery or rewrite & commit last transaction. For other\n\t\t * error number, revoking was done by filesystem itself.\n\t\t */\n\t\tret = __revoke_inmem_pages(inode, &revoke_list, false, true);\n\t\tif (ret)\n\t\t\terr = ret;\n\n\t\t/* drop all uncommitted pages */\n\t\t__revoke_inmem_pages(inode, &fi->inmem_pages, true, false);\n\t}\n\tmutex_unlock(&fi->inmem_lock);\n\n\tclear_inode_flag(inode, FI_ATOMIC_COMMIT);\n\n\tf2fs_unlock_op(sbi);\n\treturn err;\n}\n\n/*\n * This function balances dirty node and dentry pages.\n * In addition, it controls garbage collection.\n */\nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need)\n{\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(sbi, FAULT_CHECKPOINT)) {\n\t\tf2fs_show_injection_info(FAULT_CHECKPOINT);\n\t\tf2fs_stop_checkpoint(sbi, false);\n\t}\n#endif\n\n\t/* balance_fs_bg is able to be pending */\n\tif (need && excess_cached_nats(sbi))\n\t\tf2fs_balance_fs_bg(sbi);\n\n\t/*\n\t * We should do GC or end up with checkpoint, if there are so many dirty\n\t * dir/node pages without enough free segments.\n\t */\n\tif (has_not_enough_free_secs(sbi, 0, 0)) {\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\tf2fs_gc(sbi, false, false, NULL_SEGNO);\n\t}\n}\n\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi)\n{\n\t/* try to shrink extent cache when there is no enough memory */\n\tif (!available_free_memory(sbi, EXTENT_CACHE))\n\t\tf2fs_shrink_extent_tree(sbi, EXTENT_CACHE_SHRINK_NUMBER);\n\n\t/* check the # of cached NAT entries */\n\tif (!available_free_memory(sbi, NAT_ENTRIES))\n\t\ttry_to_free_nats(sbi, NAT_ENTRY_PER_BLOCK);\n\n\tif (!available_free_memory(sbi, FREE_NIDS))\n\t\ttry_to_free_nids(sbi, MAX_FREE_NIDS);\n\telse\n\t\tbuild_free_nids(sbi, false, false);\n\n\tif (!is_idle(sbi) && !excess_dirty_nats(sbi))\n\t\treturn;\n\n\t/* checkpoint is the only way to shrink partial cached entries */\n\tif (!available_free_memory(sbi, NAT_ENTRIES) ||\n\t\t\t!available_free_memory(sbi, INO_ENTRIES) ||\n\t\t\texcess_prefree_segs(sbi) ||\n\t\t\texcess_dirty_nats(sbi) ||\n\t\t\tf2fs_time_over(sbi, CP_TIME)) {\n\t\tif (test_opt(sbi, DATA_FLUSH)) {\n\t\t\tstruct blk_plug plug;\n\n\t\t\tblk_start_plug(&plug);\n\t\t\tsync_dirty_inodes(sbi, FILE_INODE);\n\t\t\tblk_finish_plug(&plug);\n\t\t}\n\t\tf2fs_sync_fs(sbi->sb, true);\n\t\tstat_inc_bg_cp_count(sbi->stat_info);\n\t}\n}\n\nstatic int __submit_flush_wait(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev)\n{\n\tstruct bio *bio = f2fs_bio_alloc(0);\n\tint ret;\n\n\tbio->bi_opf = REQ_OP_WRITE | REQ_SYNC | REQ_PREFLUSH;\n\tbio->bi_bdev = bdev;\n\tret = submit_bio_wait(bio);\n\tbio_put(bio);\n\n\ttrace_f2fs_issue_flush(bdev, test_opt(sbi, NOBARRIER),\n\t\t\t\ttest_opt(sbi, FLUSH_MERGE), ret);\n\treturn ret;\n}\n\nstatic int submit_flush_wait(struct f2fs_sb_info *sbi)\n{\n\tint ret = __submit_flush_wait(sbi, sbi->sb->s_bdev);\n\tint i;\n\n\tif (!sbi->s_ndevs || ret)\n\t\treturn ret;\n\n\tfor (i = 1; i < sbi->s_ndevs; i++) {\n\t\tret = __submit_flush_wait(sbi, FDEV(i).bdev);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int issue_flush_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\twait_queue_head_t *q = &fcc->flush_wait_queue;\nrepeat:\n\tif (kthread_should_stop())\n\t\treturn 0;\n\n\tif (!llist_empty(&fcc->issue_list)) {\n\t\tstruct flush_cmd *cmd, *next;\n\t\tint ret;\n\n\t\tfcc->dispatch_list = llist_del_all(&fcc->issue_list);\n\t\tfcc->dispatch_list = llist_reverse_order(fcc->dispatch_list);\n\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_inc(&fcc->issued_flush);\n\n\t\tllist_for_each_entry_safe(cmd, next,\n\t\t\t\t\t  fcc->dispatch_list, llnode) {\n\t\t\tcmd->ret = ret;\n\t\t\tcomplete(&cmd->wait);\n\t\t}\n\t\tfcc->dispatch_list = NULL;\n\t}\n\n\twait_event_interruptible(*q,\n\t\tkthread_should_stop() || !llist_empty(&fcc->issue_list));\n\tgoto repeat;\n}\n\nint f2fs_issue_flush(struct f2fs_sb_info *sbi)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\tstruct flush_cmd cmd;\n\tint ret;\n\n\tif (test_opt(sbi, NOBARRIER))\n\t\treturn 0;\n\n\tif (!test_opt(sbi, FLUSH_MERGE)) {\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tif (!atomic_read(&fcc->issing_flush)) {\n\t\tatomic_inc(&fcc->issing_flush);\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_dec(&fcc->issing_flush);\n\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tinit_completion(&cmd.wait);\n\n\tatomic_inc(&fcc->issing_flush);\n\tllist_add(&cmd.llnode, &fcc->issue_list);\n\n\tif (!fcc->dispatch_list)\n\t\twake_up(&fcc->flush_wait_queue);\n\n\tif (fcc->f2fs_issue_flush) {\n\t\twait_for_completion(&cmd.wait);\n\t\tatomic_dec(&fcc->issing_flush);\n\t} else {\n\t\tllist_del_all(&fcc->issue_list);\n\t\tatomic_set(&fcc->issing_flush, 0);\n\t}\n\n\treturn cmd.ret;\n}\n\nint create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\tint err = 0;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = kzalloc(sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->issing_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\n\tif (!test_opt(sbi, FLUSH_MERGE))\n\t\treturn err;\n\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\terr = PTR_ERR(fcc->f2fs_issue_flush);\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nvoid destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\n\tif (fcc && fcc->f2fs_issue_flush) {\n\t\tstruct task_struct *flush_thread = fcc->f2fs_issue_flush;\n\n\t\tfcc->f2fs_issue_flush = NULL;\n\t\tkthread_stop(flush_thread);\n\t}\n\tif (free) {\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t}\n}\n\nstatic void __locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\t/* need not be added */\n\tif (IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]++;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (unlikely(t >= DIRTY)) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\treturn;\n\t\t}\n\t\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]++;\n\t}\n}\n\nstatic void __remove_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]--;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]--;\n\n\t\tif (get_valid_blocks(sbi, segno, true) == 0)\n\t\t\tclear_bit(GET_SEC_FROM_SEG(sbi, segno),\n\t\t\t\t\t\tdirty_i->victim_secmap);\n\t}\n}\n\n/*\n * Should not occur error such as -ENOMEM.\n * Adding dirty entry into seglist is not critical operation.\n * If a given segment is one of current working segments, it won't be added.\n */\nstatic void locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned short valid_blocks;\n\n\tif (segno == NULL_SEGNO || IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\n\tif (valid_blocks == 0) {\n\t\t__locate_dirty_segment(sbi, segno, PRE);\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t} else if (valid_blocks < sbi->blocks_per_seg) {\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t} else {\n\t\t/* Recovery routine with SSR needs this */\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t}\n\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic struct discard_cmd *__create_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t lstart,\n\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc;\n\n\tf2fs_bug_on(sbi, !len);\n\n\tpend_list = &dcc->pend_list[plist_idx(len)];\n\n\tdc = f2fs_kmem_cache_alloc(discard_cmd_slab, GFP_NOFS);\n\tINIT_LIST_HEAD(&dc->list);\n\tdc->bdev = bdev;\n\tdc->lstart = lstart;\n\tdc->start = start;\n\tdc->len = len;\n\tdc->ref = 0;\n\tdc->state = D_PREP;\n\tdc->error = 0;\n\tinit_completion(&dc->wait);\n\tlist_add_tail(&dc->list, pend_list);\n\tatomic_inc(&dcc->discard_cmd_cnt);\n\tdcc->undiscard_blks += len;\n\n\treturn dc;\n}\n\nstatic struct discard_cmd *__attach_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len,\n\t\t\t\tstruct rb_node *parent, struct rb_node **p)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\n\tdc = __create_discard_cmd(sbi, bdev, lstart, start, len);\n\n\trb_link_node(&dc->rb_node, parent, p);\n\trb_insert_color(&dc->rb_node, &dcc->root);\n\n\treturn dc;\n}\n\nstatic void __detach_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tif (dc->state == D_DONE)\n\t\tatomic_dec(&dcc->issing_discard);\n\n\tlist_del(&dc->list);\n\trb_erase(&dc->rb_node, &dcc->root);\n\tdcc->undiscard_blks -= dc->len;\n\n\tkmem_cache_free(discard_cmd_slab, dc);\n\n\tatomic_dec(&dcc->discard_cmd_cnt);\n}\n\nstatic void __remove_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (dc->error == -EOPNOTSUPP)\n\t\tdc->error = 0;\n\n\tif (dc->error)\n\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"Issue discard(%u, %u, %u) failed, ret: %d\",\n\t\t\tdc->lstart, dc->start, dc->len, dc->error);\n\t__detach_discard_cmd(dcc, dc);\n}\n\nstatic void f2fs_submit_discard_endio(struct bio *bio)\n{\n\tstruct discard_cmd *dc = (struct discard_cmd *)bio->bi_private;\n\n\tdc->error = bio->bi_error;\n\tdc->state = D_DONE;\n\tcomplete_all(&dc->wait);\n\tbio_put(bio);\n}\n\n/* this function is copied from blkdev_issue_discard from block/blk-lib.c */\nstatic void __submit_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct bio *bio = NULL;\n\n\tif (dc->state != D_PREP)\n\t\treturn;\n\n\ttrace_f2fs_issue_discard(dc->bdev, dc->start, dc->len);\n\n\tdc->error = __blkdev_issue_discard(dc->bdev,\n\t\t\t\tSECTOR_FROM_BLOCK(dc->start),\n\t\t\t\tSECTOR_FROM_BLOCK(dc->len),\n\t\t\t\tGFP_NOFS, 0, &bio);\n\tif (!dc->error) {\n\t\t/* should keep before submission to avoid D_DONE right away */\n\t\tdc->state = D_SUBMIT;\n\t\tatomic_inc(&dcc->issued_discard);\n\t\tatomic_inc(&dcc->issing_discard);\n\t\tif (bio) {\n\t\t\tbio->bi_private = dc;\n\t\t\tbio->bi_end_io = f2fs_submit_discard_endio;\n\t\t\tbio->bi_opf |= REQ_SYNC;\n\t\t\tsubmit_bio(bio);\n\t\t\tlist_move_tail(&dc->list, &dcc->wait_list);\n\t\t}\n\t} else {\n\t\t__remove_discard_cmd(sbi, dc);\n\t}\n}\n\nstatic struct discard_cmd *__insert_discard_tree(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len,\n\t\t\t\tstruct rb_node **insert_p,\n\t\t\t\tstruct rb_node *insert_parent)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct rb_node **p = &dcc->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct discard_cmd *dc = NULL;\n\n\tif (insert_p && insert_parent) {\n\t\tparent = insert_parent;\n\t\tp = insert_p;\n\t\tgoto do_insert;\n\t}\n\n\tp = __lookup_rb_tree_for_insert(sbi, &dcc->root, &parent, lstart);\ndo_insert:\n\tdc = __attach_discard_cmd(sbi, bdev, lstart, start, len, parent, p);\n\tif (!dc)\n\t\treturn NULL;\n\n\treturn dc;\n}\n\nstatic void __relocate_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tlist_move_tail(&dc->list, &dcc->pend_list[plist_idx(dc->len)]);\n}\n\nstatic void __punch_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_info di = dc->di;\n\tbool modified = false;\n\n\tif (dc->state == D_DONE || dc->len == 1) {\n\t\t__remove_discard_cmd(sbi, dc);\n\t\treturn;\n\t}\n\n\tdcc->undiscard_blks -= di.len;\n\n\tif (blkaddr > di.lstart) {\n\t\tdc->len = blkaddr - dc->lstart;\n\t\tdcc->undiscard_blks += dc->len;\n\t\t__relocate_discard_cmd(dcc, dc);\n\t\tmodified = true;\n\t}\n\n\tif (blkaddr < di.lstart + di.len - 1) {\n\t\tif (modified) {\n\t\t\t__insert_discard_tree(sbi, dc->bdev, blkaddr + 1,\n\t\t\t\t\tdi.start + blkaddr + 1 - di.lstart,\n\t\t\t\t\tdi.lstart + di.len - 1 - blkaddr,\n\t\t\t\t\tNULL, NULL);\n\t\t} else {\n\t\t\tdc->lstart++;\n\t\t\tdc->len--;\n\t\t\tdc->start++;\n\t\t\tdcc->undiscard_blks += dc->len;\n\t\t\t__relocate_discard_cmd(dcc, dc);\n\t\t}\n\t}\n}\n\nstatic void __update_discard_tree_range(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *prev_dc = NULL, *next_dc = NULL;\n\tstruct discard_cmd *dc;\n\tstruct discard_info di = {0};\n\tstruct rb_node **insert_p = NULL, *insert_parent = NULL;\n\tblock_t end = lstart + len;\n\n\tmutex_lock(&dcc->cmd_lock);\n\n\tdc = (struct discard_cmd *)__lookup_rb_tree_ret(&dcc->root,\n\t\t\t\t\tNULL, lstart,\n\t\t\t\t\t(struct rb_entry **)&prev_dc,\n\t\t\t\t\t(struct rb_entry **)&next_dc,\n\t\t\t\t\t&insert_p, &insert_parent, true);\n\tif (dc)\n\t\tprev_dc = dc;\n\n\tif (!prev_dc) {\n\t\tdi.lstart = lstart;\n\t\tdi.len = next_dc ? next_dc->lstart - lstart : len;\n\t\tdi.len = min(di.len, len);\n\t\tdi.start = start;\n\t}\n\n\twhile (1) {\n\t\tstruct rb_node *node;\n\t\tbool merged = false;\n\t\tstruct discard_cmd *tdc = NULL;\n\n\t\tif (prev_dc) {\n\t\t\tdi.lstart = prev_dc->lstart + prev_dc->len;\n\t\t\tif (di.lstart < lstart)\n\t\t\t\tdi.lstart = lstart;\n\t\t\tif (di.lstart >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (!next_dc || next_dc->lstart > end)\n\t\t\t\tdi.len = end - di.lstart;\n\t\t\telse\n\t\t\t\tdi.len = next_dc->lstart - di.lstart;\n\t\t\tdi.start = start + di.lstart - lstart;\n\t\t}\n\n\t\tif (!di.len)\n\t\t\tgoto next;\n\n\t\tif (prev_dc && prev_dc->state == D_PREP &&\n\t\t\tprev_dc->bdev == bdev &&\n\t\t\t__is_discard_back_mergeable(&di, &prev_dc->di)) {\n\t\t\tprev_dc->di.len += di.len;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, prev_dc);\n\t\t\tdi = prev_dc->di;\n\t\t\ttdc = prev_dc;\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (next_dc && next_dc->state == D_PREP &&\n\t\t\tnext_dc->bdev == bdev &&\n\t\t\t__is_discard_front_mergeable(&di, &next_dc->di)) {\n\t\t\tnext_dc->di.lstart = di.lstart;\n\t\t\tnext_dc->di.len += di.len;\n\t\t\tnext_dc->di.start = di.start;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, next_dc);\n\t\t\tif (tdc)\n\t\t\t\t__remove_discard_cmd(sbi, tdc);\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (!merged) {\n\t\t\t__insert_discard_tree(sbi, bdev, di.lstart, di.start,\n\t\t\t\t\t\t\tdi.len, NULL, NULL);\n\t\t}\n next:\n\t\tprev_dc = next_dc;\n\t\tif (!prev_dc)\n\t\t\tbreak;\n\n\t\tnode = rb_next(&prev_dc->rb_node);\n\t\tnext_dc = rb_entry_safe(node, struct discard_cmd, rb_node);\n\t}\n\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic int __queue_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tblock_t lblkstart = blkstart;\n\n\ttrace_f2fs_queue_discard(bdev, blkstart, blklen);\n\n\tif (sbi->s_ndevs) {\n\t\tint devi = f2fs_target_device_index(sbi, blkstart);\n\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\t__update_discard_tree_range(sbi, bdev, lblkstart, blkstart, blklen);\n\treturn 0;\n}\n\nstatic void __issue_discard_cmd(struct f2fs_sb_info *sbi, bool issue_cond)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc, *tmp;\n\tstruct blk_plug plug;\n\tint i, iter = 0;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tf2fs_bug_on(sbi,\n\t\t!__check_rb_tree_consistence(sbi, &dcc->root));\n\tblk_start_plug(&plug);\n\tfor (i = MAX_PLIST_NUM - 1; i >= 0; i--) {\n\t\tpend_list = &dcc->pend_list[i];\n\t\tlist_for_each_entry_safe(dc, tmp, pend_list, list) {\n\t\t\tf2fs_bug_on(sbi, dc->state != D_PREP);\n\n\t\t\tif (!issue_cond || is_idle(sbi))\n\t\t\t\t__submit_discard_cmd(sbi, dc);\n\t\t\tif (issue_cond && iter++ > DISCARD_ISSUE_RATE)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tblk_finish_plug(&plug);\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic void __wait_discard_cmd(struct f2fs_sb_info *sbi, bool wait_cond)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *wait_list = &(dcc->wait_list);\n\tstruct discard_cmd *dc, *tmp;\n\tbool need_wait;\n\nnext:\n\tneed_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tlist_for_each_entry_safe(dc, tmp, wait_list, list) {\n\t\tif (!wait_cond || (dc->state == D_DONE && !dc->ref)) {\n\t\t\twait_for_completion_io(&dc->wait);\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait) {\n\t\twait_for_completion_io(&dc->wait);\n\t\tmutex_lock(&dcc->cmd_lock);\n\t\tf2fs_bug_on(sbi, dc->state != D_DONE);\n\t\tdc->ref--;\n\t\tif (!dc->ref)\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\tmutex_unlock(&dcc->cmd_lock);\n\t\tgoto next;\n\t}\n}\n\n/* This should be covered by global mutex, &sit_i->sentry_lock */\nvoid f2fs_wait_discard_bio(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\tbool need_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tdc = (struct discard_cmd *)__lookup_rb_tree(&dcc->root, NULL, blkaddr);\n\tif (dc) {\n\t\tif (dc->state == D_PREP) {\n\t\t\t__punch_discard_cmd(sbi, dc, blkaddr);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait) {\n\t\twait_for_completion_io(&dc->wait);\n\t\tmutex_lock(&dcc->cmd_lock);\n\t\tf2fs_bug_on(sbi, dc->state != D_DONE);\n\t\tdc->ref--;\n\t\tif (!dc->ref)\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\tmutex_unlock(&dcc->cmd_lock);\n\t}\n}\n\n/* This comes from f2fs_put_super */\nvoid f2fs_wait_discard_bios(struct f2fs_sb_info *sbi)\n{\n\t__issue_discard_cmd(sbi, false);\n\t__wait_discard_cmd(sbi, false);\n}\n\nstatic int issue_discard_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\twait_queue_head_t *q = &dcc->discard_wait_queue;\n\n\tset_freezable();\n\n\tdo {\n\t\twait_event_interruptible(*q, kthread_should_stop() ||\n\t\t\t\t\tfreezing(current) ||\n\t\t\t\t\tatomic_read(&dcc->discard_cmd_cnt));\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\t\tif (kthread_should_stop())\n\t\t\treturn 0;\n\n\t\t__issue_discard_cmd(sbi, true);\n\t\t__wait_discard_cmd(sbi, true);\n\n\t\tcongestion_wait(BLK_RW_SYNC, HZ/50);\n\t} while (!kthread_should_stop());\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int __f2fs_issue_discard_zone(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tsector_t sector, nr_sects;\n\tblock_t lblkstart = blkstart;\n\tint devi = 0;\n\n\tif (sbi->s_ndevs) {\n\t\tdevi = f2fs_target_device_index(sbi, blkstart);\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\n\t/*\n\t * We need to know the type of the zone: for conventional zones,\n\t * use regular discard if the drive supports it. For sequential\n\t * zones, reset the zone write pointer.\n\t */\n\tswitch (get_blkz_type(sbi, bdev, blkstart)) {\n\n\tcase BLK_ZONE_TYPE_CONVENTIONAL:\n\t\tif (!blk_queue_discard(bdev_get_queue(bdev)))\n\t\t\treturn 0;\n\t\treturn __queue_discard_cmd(sbi, bdev, lblkstart, blklen);\n\tcase BLK_ZONE_TYPE_SEQWRITE_REQ:\n\tcase BLK_ZONE_TYPE_SEQWRITE_PREF:\n\t\tsector = SECTOR_FROM_BLOCK(blkstart);\n\t\tnr_sects = SECTOR_FROM_BLOCK(blklen);\n\n\t\tif (sector & (bdev_zone_sectors(bdev) - 1) ||\n\t\t\t\tnr_sects != bdev_zone_sectors(bdev)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\t\"(%d) %s: Unaligned discard attempted (block %x + %x)\",\n\t\t\t\tdevi, sbi->s_ndevs ? FDEV(devi).path: \"\",\n\t\t\t\tblkstart, blklen);\n\t\t\treturn -EIO;\n\t\t}\n\t\ttrace_f2fs_issue_reset_zone(bdev, blkstart);\n\t\treturn blkdev_reset_zones(bdev, sector,\n\t\t\t\t\t  nr_sects, GFP_NOFS);\n\tdefault:\n\t\t/* Unknown zone type: broken device ? */\n\t\treturn -EIO;\n\t}\n}\n#endif\n\nstatic int __issue_discard_async(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_mounted_blkzoned(sbi->sb) &&\n\t\t\t\tbdev_zoned_model(bdev) != BLK_ZONED_NONE)\n\t\treturn __f2fs_issue_discard_zone(sbi, bdev, blkstart, blklen);\n#endif\n\treturn __queue_discard_cmd(sbi, bdev, blkstart, blklen);\n}\n\nstatic int f2fs_issue_discard(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blkstart, block_t blklen)\n{\n\tsector_t start = blkstart, len = 0;\n\tstruct block_device *bdev;\n\tstruct seg_entry *se;\n\tunsigned int offset;\n\tblock_t i;\n\tint err = 0;\n\n\tbdev = f2fs_target_device(sbi, blkstart, NULL);\n\n\tfor (i = blkstart; i < blkstart + blklen; i++, len++) {\n\t\tif (i != start) {\n\t\t\tstruct block_device *bdev2 =\n\t\t\t\tf2fs_target_device(sbi, i, NULL);\n\n\t\t\tif (bdev2 != bdev) {\n\t\t\t\terr = __issue_discard_async(sbi, bdev,\n\t\t\t\t\t\tstart, len);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tbdev = bdev2;\n\t\t\t\tstart = i;\n\t\t\t\tlen = 0;\n\t\t\t}\n\t\t}\n\n\t\tse = get_seg_entry(sbi, GET_SEGNO(sbi, i));\n\t\toffset = GET_BLKOFF_FROM_SEG0(sbi, i);\n\n\t\tif (!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\t}\n\n\tif (len)\n\t\terr = __issue_discard_async(sbi, bdev, start, len);\n\treturn err;\n}\n\nstatic bool add_discard_addrs(struct f2fs_sb_info *sbi, struct cp_control *cpc,\n\t\t\t\t\t\t\tbool check_only)\n{\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tint max_blocks = sbi->blocks_per_seg;\n\tstruct seg_entry *se = get_seg_entry(sbi, cpc->trim_start);\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *discard_map = (unsigned long *)se->discard_map;\n\tunsigned long *dmap = SIT_I(sbi)->tmp_map;\n\tunsigned int start = 0, end = -1;\n\tbool force = (cpc->reason & CP_DISCARD);\n\tstruct discard_entry *de = NULL;\n\tstruct list_head *head = &SM_I(sbi)->dcc_info->entry_list;\n\tint i;\n\n\tif (se->valid_blocks == max_blocks || !f2fs_discard_en(sbi))\n\t\treturn false;\n\n\tif (!force) {\n\t\tif (!test_opt(sbi, DISCARD) || !se->valid_blocks ||\n\t\t\tSM_I(sbi)->dcc_info->nr_discards >=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards)\n\t\t\treturn false;\n\t}\n\n\t/* SIT_VBLOCK_MAP_SIZE should be multiple of sizeof(unsigned long) */\n\tfor (i = 0; i < entries; i++)\n\t\tdmap[i] = force ? ~ckpt_map[i] & ~discard_map[i] :\n\t\t\t\t(cur_map[i] ^ ckpt_map[i]) & ckpt_map[i];\n\n\twhile (force || SM_I(sbi)->dcc_info->nr_discards <=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards) {\n\t\tstart = __find_rev_next_bit(dmap, max_blocks, end + 1);\n\t\tif (start >= max_blocks)\n\t\t\tbreak;\n\n\t\tend = __find_rev_next_zero_bit(dmap, max_blocks, start + 1);\n\t\tif (force && start && end != max_blocks\n\t\t\t\t\t&& (end - start) < cpc->trim_minlen)\n\t\t\tcontinue;\n\n\t\tif (check_only)\n\t\t\treturn true;\n\n\t\tif (!de) {\n\t\t\tde = f2fs_kmem_cache_alloc(discard_entry_slab,\n\t\t\t\t\t\t\t\tGFP_F2FS_ZERO);\n\t\t\tde->start_blkaddr = START_BLOCK(sbi, cpc->trim_start);\n\t\t\tlist_add_tail(&de->list, head);\n\t\t}\n\n\t\tfor (i = start; i < end; i++)\n\t\t\t__set_bit_le(i, (void *)de->discard_map);\n\n\t\tSM_I(sbi)->dcc_info->nr_discards += end - start;\n\t}\n\treturn false;\n}\n\nvoid release_discard_addrs(struct f2fs_sb_info *sbi)\n{\n\tstruct list_head *head = &(SM_I(sbi)->dcc_info->entry_list);\n\tstruct discard_entry *entry, *this;\n\n\t/* drop caches */\n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tlist_del(&entry->list);\n\t\tkmem_cache_free(discard_entry_slab, entry);\n\t}\n}\n\n/*\n * Should call clear_prefree_segments after checkpoint is done.\n */\nstatic void set_prefree_as_free_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor_each_set_bit(segno, dirty_i->dirty_segmap[PRE], MAIN_SEGS(sbi))\n\t\t__set_test_and_free(sbi, segno);\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nvoid clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct list_head *head = &(SM_I(sbi)->dcc_info->entry_list);\n\tstruct discard_entry *entry, *this;\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned long *prefree_map = dirty_i->dirty_segmap[PRE];\n\tunsigned int start = 0, end = -1;\n\tunsigned int secno, start_segno;\n\tbool force = (cpc->reason & CP_DISCARD);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\twhile (1) {\n\t\tint i;\n\t\tstart = find_next_bit(prefree_map, MAIN_SEGS(sbi), end + 1);\n\t\tif (start >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\tend = find_next_zero_bit(prefree_map, MAIN_SEGS(sbi),\n\t\t\t\t\t\t\t\tstart + 1);\n\n\t\tfor (i = start; i < end; i++)\n\t\t\tclear_bit(i, prefree_map);\n\n\t\tdirty_i->nr_dirty[PRE] -= end - start;\n\n\t\tif (!test_opt(sbi, DISCARD))\n\t\t\tcontinue;\n\n\t\tif (force && start >= cpc->trim_start &&\n\t\t\t\t\t(end - 1) <= cpc->trim_end)\n\t\t\t\tcontinue;\n\n\t\tif (!test_opt(sbi, LFS) || sbi->segs_per_sec == 1) {\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start),\n\t\t\t\t(end - start) << sbi->log_blocks_per_seg);\n\t\t\tcontinue;\n\t\t}\nnext:\n\t\tsecno = GET_SEC_FROM_SEG(sbi, start);\n\t\tstart_segno = GET_SEG_FROM_SEC(sbi, secno);\n\t\tif (!IS_CURSEC(sbi, secno) &&\n\t\t\t!get_valid_blocks(sbi, start, true))\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start_segno),\n\t\t\t\tsbi->segs_per_sec << sbi->log_blocks_per_seg);\n\n\t\tstart = start_segno + sbi->segs_per_sec;\n\t\tif (start < end)\n\t\t\tgoto next;\n\t\telse\n\t\t\tend = start - 1;\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\t/* send small discards */\n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tunsigned int cur_pos = 0, next_pos, len, total_len = 0;\n\t\tbool is_valid = test_bit_le(0, entry->discard_map);\n\nfind_next:\n\t\tif (is_valid) {\n\t\t\tnext_pos = find_next_zero_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t\tlen = next_pos - cur_pos;\n\n\t\t\tif (f2fs_sb_mounted_blkzoned(sbi->sb) ||\n\t\t\t    (force && len < cpc->trim_minlen))\n\t\t\t\tgoto skip;\n\n\t\t\tf2fs_issue_discard(sbi, entry->start_blkaddr + cur_pos,\n\t\t\t\t\t\t\t\t\tlen);\n\t\t\tcpc->trimmed += len;\n\t\t\ttotal_len += len;\n\t\t} else {\n\t\t\tnext_pos = find_next_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t}\nskip:\n\t\tcur_pos = next_pos;\n\t\tis_valid = !is_valid;\n\n\t\tif (cur_pos < sbi->blocks_per_seg)\n\t\t\tgoto find_next;\n\n\t\tlist_del(&entry->list);\n\t\tSM_I(sbi)->dcc_info->nr_discards -= total_len;\n\t\tkmem_cache_free(discard_entry_slab, entry);\n\t}\n\n\twake_up(&SM_I(sbi)->dcc_info->discard_wait_queue);\n}\n\nstatic int create_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct discard_cmd_control *dcc;\n\tint err = 0, i;\n\n\tif (SM_I(sbi)->dcc_info) {\n\t\tdcc = SM_I(sbi)->dcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tdcc = kzalloc(sizeof(struct discard_cmd_control), GFP_KERNEL);\n\tif (!dcc)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&dcc->entry_list);\n\tfor (i = 0; i < MAX_PLIST_NUM; i++)\n\t\tINIT_LIST_HEAD(&dcc->pend_list[i]);\n\tINIT_LIST_HEAD(&dcc->wait_list);\n\tmutex_init(&dcc->cmd_lock);\n\tatomic_set(&dcc->issued_discard, 0);\n\tatomic_set(&dcc->issing_discard, 0);\n\tatomic_set(&dcc->discard_cmd_cnt, 0);\n\tdcc->nr_discards = 0;\n\tdcc->max_discards = MAIN_SEGS(sbi) << sbi->log_blocks_per_seg;\n\tdcc->undiscard_blks = 0;\n\tdcc->root = RB_ROOT;\n\n\tinit_waitqueue_head(&dcc->discard_wait_queue);\n\tSM_I(sbi)->dcc_info = dcc;\ninit_thread:\n\tdcc->f2fs_issue_discard = kthread_run(issue_discard_thread, sbi,\n\t\t\t\t\"f2fs_discard-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(dcc->f2fs_issue_discard)) {\n\t\terr = PTR_ERR(dcc->f2fs_issue_discard);\n\t\tkfree(dcc);\n\t\tSM_I(sbi)->dcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nstatic void destroy_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (!dcc)\n\t\treturn;\n\n\tif (dcc->f2fs_issue_discard) {\n\t\tstruct task_struct *discard_thread = dcc->f2fs_issue_discard;\n\n\t\tdcc->f2fs_issue_discard = NULL;\n\t\tkthread_stop(discard_thread);\n\t}\n\n\tkfree(dcc);\n\tSM_I(sbi)->dcc_info = NULL;\n}\n\nstatic bool __mark_sit_entry_dirty(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tif (!__test_and_set_bit(segno, sit_i->dirty_sentries_bitmap)) {\n\t\tsit_i->dirty_sentries++;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void __set_sit_entry_type(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tunsigned int segno, int modified)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, segno);\n\tse->type = type;\n\tif (modified)\n\t\t__mark_sit_entry_dirty(sbi, segno);\n}\n\nstatic void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)\n{\n\tstruct seg_entry *se;\n\tunsigned int segno, offset;\n\tlong int new_vblocks;\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\n\tse = get_seg_entry(sbi, segno);\n\tnew_vblocks = se->valid_blocks + del;\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tf2fs_bug_on(sbi, (new_vblocks >> (sizeof(unsigned short) << 3) ||\n\t\t\t\t(new_vblocks > sbi->blocks_per_seg)));\n\n\tse->valid_blocks = new_vblocks;\n\tse->mtime = get_mtime(sbi);\n\tSIT_I(sbi)->max_mtime = se->mtime;\n\n\t/* Update valid block bitmap */\n\tif (del > 0) {\n\t\tif (f2fs_test_and_set_bit(offset, se->cur_valid_map)) {\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tif (f2fs_test_and_set_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir))\n\t\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\telse\n\t\t\t\tWARN_ON(1);\n#else\n\t\t\tf2fs_bug_on(sbi, 1);\n#endif\n\t\t}\n\t\tif (f2fs_discard_en(sbi) &&\n\t\t\t!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\n\t\t/* don't overwrite by SSR to keep node chain */\n\t\tif (se->type == CURSEG_WARM_NODE) {\n\t\t\tif (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))\n\t\t\t\tse->ckpt_valid_blocks++;\n\t\t}\n\t} else {\n\t\tif (!f2fs_test_and_clear_bit(offset, se->cur_valid_map)) {\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tif (!f2fs_test_and_clear_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir))\n\t\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\telse\n\t\t\t\tWARN_ON(1);\n#else\n\t\t\tf2fs_bug_on(sbi, 1);\n#endif\n\t\t}\n\t\tif (f2fs_discard_en(sbi) &&\n\t\t\tf2fs_test_and_clear_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks++;\n\t}\n\tif (!f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tse->ckpt_valid_blocks += del;\n\n\t__mark_sit_entry_dirty(sbi, segno);\n\n\t/* update total number of valid blocks to be written in ckpt area */\n\tSIT_I(sbi)->written_valid_blocks += del;\n\n\tif (sbi->segs_per_sec > 1)\n\t\tget_sec_entry(sbi, segno)->valid_blocks += del;\n}\n\nvoid refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new)\n{\n\tupdate_sit_entry(sbi, new, 1);\n\tif (GET_SEGNO(sbi, old) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old, -1);\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new));\n}\n\nvoid invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr)\n{\n\tunsigned int segno = GET_SEGNO(sbi, addr);\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tf2fs_bug_on(sbi, addr == NULL_ADDR);\n\tif (addr == NEW_ADDR)\n\t\treturn;\n\n\t/* add it into sit main buffer */\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tupdate_sit_entry(sbi, addr, -1);\n\n\t/* add it into dirty seglist */\n\tlocate_dirty_segment(sbi, segno);\n\n\tmutex_unlock(&sit_i->sentry_lock);\n}\n\nbool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno, offset;\n\tstruct seg_entry *se;\n\tbool is_cp = false;\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn true;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tif (f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tis_cp = true;\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\treturn is_cp;\n}\n\n/*\n * This function should be resided under the curseg_mutex lock\n */\nstatic void __add_sum_entry(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tstruct f2fs_summary *sum)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tvoid *addr = curseg->sum_blk;\n\taddr += curseg->next_blkoff * sizeof(struct f2fs_summary);\n\tmemcpy(addr, sum, sizeof(struct f2fs_summary));\n}\n\n/*\n * Calculate the number of current summary pages for writing\n */\nint npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra)\n{\n\tint valid_sum_count = 0;\n\tint i, sum_in_page;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tif (sbi->ckpt->alloc_type[i] == SSR)\n\t\t\tvalid_sum_count += sbi->blocks_per_seg;\n\t\telse {\n\t\t\tif (for_ra)\n\t\t\t\tvalid_sum_count += le16_to_cpu(\n\t\t\t\t\tF2FS_CKPT(sbi)->cur_data_blkoff[i]);\n\t\t\telse\n\t\t\t\tvalid_sum_count += curseg_blkoff(sbi, i);\n\t\t}\n\t}\n\n\tsum_in_page = (PAGE_SIZE - 2 * SUM_JOURNAL_SIZE -\n\t\t\tSUM_FOOTER_SIZE) / SUMMARY_SIZE;\n\tif (valid_sum_count <= sum_in_page)\n\t\treturn 1;\n\telse if ((valid_sum_count - sum_in_page) <=\n\t\t(PAGE_SIZE - SUM_FOOTER_SIZE) / SUMMARY_SIZE)\n\t\treturn 2;\n\treturn 3;\n}\n\n/*\n * Caller should put this summary page\n */\nstruct page *get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\treturn get_meta_page(sbi, GET_SUM_BLOCK(sbi, segno));\n}\n\nvoid update_meta_page(struct f2fs_sb_info *sbi, void *src, block_t blk_addr)\n{\n\tstruct page *page = grab_meta_page(sbi, blk_addr);\n\tvoid *dst = page_address(page);\n\n\tif (src)\n\t\tmemcpy(dst, src, PAGE_SIZE);\n\telse\n\t\tmemset(dst, 0, PAGE_SIZE);\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic void write_sum_page(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_summary_block *sum_blk, block_t blk_addr)\n{\n\tupdate_meta_page(sbi, (void *)sum_blk, blk_addr);\n}\n\nstatic void write_current_sum_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, block_t blk_addr)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct page *page = grab_meta_page(sbi, blk_addr);\n\tstruct f2fs_summary_block *src = curseg->sum_blk;\n\tstruct f2fs_summary_block *dst;\n\n\tdst = (struct f2fs_summary_block *)page_address(page);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\n\tdown_read(&curseg->journal_rwsem);\n\tmemcpy(&dst->journal, curseg->journal, SUM_JOURNAL_SIZE);\n\tup_read(&curseg->journal_rwsem);\n\n\tmemcpy(dst->entries, src->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&dst->footer, &src->footer, SUM_FOOTER_SIZE);\n\n\tmutex_unlock(&curseg->curseg_mutex);\n\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic int is_next_segment_free(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno = curseg->segno + 1;\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\n\tif (segno < MAIN_SEGS(sbi) && segno % sbi->segs_per_sec)\n\t\treturn !test_bit(segno, free_i->free_segmap);\n\treturn 0;\n}\n\n/*\n * Find a new segment from the free segments bitmap to right order\n * This function should be returned with success, otherwise BUG\n */\nstatic void get_new_segment(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int *newseg, bool new_sec, int dir)\n{\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno, secno, zoneno;\n\tunsigned int total_zones = MAIN_SECS(sbi) / sbi->secs_per_zone;\n\tunsigned int hint = GET_SEC_FROM_SEG(sbi, *newseg);\n\tunsigned int old_zoneno = GET_ZONE_FROM_SEG(sbi, *newseg);\n\tunsigned int left_start = hint;\n\tbool init = true;\n\tint go_left = 0;\n\tint i;\n\n\tspin_lock(&free_i->segmap_lock);\n\n\tif (!new_sec && ((*newseg + 1) % sbi->segs_per_sec)) {\n\t\tsegno = find_next_zero_bit(free_i->free_segmap,\n\t\t\tGET_SEG_FROM_SEC(sbi, hint + 1), *newseg + 1);\n\t\tif (segno < GET_SEG_FROM_SEC(sbi, hint + 1))\n\t\t\tgoto got_it;\n\t}\nfind_other_zone:\n\tsecno = find_next_zero_bit(free_i->free_secmap, MAIN_SECS(sbi), hint);\n\tif (secno >= MAIN_SECS(sbi)) {\n\t\tif (dir == ALLOC_RIGHT) {\n\t\t\tsecno = find_next_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi), 0);\n\t\t\tf2fs_bug_on(sbi, secno >= MAIN_SECS(sbi));\n\t\t} else {\n\t\t\tgo_left = 1;\n\t\t\tleft_start = hint - 1;\n\t\t}\n\t}\n\tif (go_left == 0)\n\t\tgoto skip_left;\n\n\twhile (test_bit(left_start, free_i->free_secmap)) {\n\t\tif (left_start > 0) {\n\t\t\tleft_start--;\n\t\t\tcontinue;\n\t\t}\n\t\tleft_start = find_next_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi), 0);\n\t\tf2fs_bug_on(sbi, left_start >= MAIN_SECS(sbi));\n\t\tbreak;\n\t}\n\tsecno = left_start;\nskip_left:\n\thint = secno;\n\tsegno = GET_SEG_FROM_SEC(sbi, secno);\n\tzoneno = GET_ZONE_FROM_SEC(sbi, secno);\n\n\t/* give up on finding another zone */\n\tif (!init)\n\t\tgoto got_it;\n\tif (sbi->secs_per_zone == 1)\n\t\tgoto got_it;\n\tif (zoneno == old_zoneno)\n\t\tgoto got_it;\n\tif (dir == ALLOC_LEFT) {\n\t\tif (!go_left && zoneno + 1 >= total_zones)\n\t\t\tgoto got_it;\n\t\tif (go_left && zoneno == 0)\n\t\t\tgoto got_it;\n\t}\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++)\n\t\tif (CURSEG_I(sbi, i)->zone == zoneno)\n\t\t\tbreak;\n\n\tif (i < NR_CURSEG_TYPE) {\n\t\t/* zone is in user, try another */\n\t\tif (go_left)\n\t\t\thint = zoneno * sbi->secs_per_zone - 1;\n\t\telse if (zoneno + 1 >= total_zones)\n\t\t\thint = 0;\n\t\telse\n\t\t\thint = (zoneno + 1) * sbi->secs_per_zone;\n\t\tinit = false;\n\t\tgoto find_other_zone;\n\t}\ngot_it:\n\t/* set it as dirty segment in free segmap */\n\tf2fs_bug_on(sbi, test_bit(segno, free_i->free_segmap));\n\t__set_inuse(sbi, segno);\n\t*newseg = segno;\n\tspin_unlock(&free_i->segmap_lock);\n}\n\nstatic void reset_curseg(struct f2fs_sb_info *sbi, int type, int modified)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct summary_footer *sum_footer;\n\n\tcurseg->segno = curseg->next_segno;\n\tcurseg->zone = GET_ZONE_FROM_SEG(sbi, curseg->segno);\n\tcurseg->next_blkoff = 0;\n\tcurseg->next_segno = NULL_SEGNO;\n\n\tsum_footer = &(curseg->sum_blk->footer);\n\tmemset(sum_footer, 0, sizeof(struct summary_footer));\n\tif (IS_DATASEG(type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_DATA);\n\tif (IS_NODESEG(type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_NODE);\n\t__set_sit_entry_type(sbi, type, curseg->segno, modified);\n}\n\nstatic unsigned int __get_next_segno(struct f2fs_sb_info *sbi, int type)\n{\n\t/* if segs_per_sec is large than 1, we need to keep original policy. */\n\tif (sbi->segs_per_sec != 1)\n\t\treturn CURSEG_I(sbi, type)->segno;\n\n\tif (type == CURSEG_HOT_DATA || IS_NODESEG(type))\n\t\treturn 0;\n\n\tif (SIT_I(sbi)->last_victim[ALLOC_NEXT])\n\t\treturn SIT_I(sbi)->last_victim[ALLOC_NEXT];\n\treturn CURSEG_I(sbi, type)->segno;\n}\n\n/*\n * Allocate a current working segment.\n * This function always allocates a free segment in LFS manner.\n */\nstatic void new_curseg(struct f2fs_sb_info *sbi, int type, bool new_sec)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno = curseg->segno;\n\tint dir = ALLOC_LEFT;\n\n\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, segno));\n\tif (type == CURSEG_WARM_DATA || type == CURSEG_COLD_DATA)\n\t\tdir = ALLOC_RIGHT;\n\n\tif (test_opt(sbi, NOHEAP))\n\t\tdir = ALLOC_RIGHT;\n\n\tsegno = __get_next_segno(sbi, type);\n\tget_new_segment(sbi, &segno, new_sec, dir);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = LFS;\n}\n\nstatic void __next_free_blkoff(struct f2fs_sb_info *sbi,\n\t\t\tstruct curseg_info *seg, block_t start)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, seg->segno);\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tunsigned long *target_map = SIT_I(sbi)->tmp_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tint i, pos;\n\n\tfor (i = 0; i < entries; i++)\n\t\ttarget_map[i] = ckpt_map[i] | cur_map[i];\n\n\tpos = __find_rev_next_zero_bit(target_map, sbi->blocks_per_seg, start);\n\n\tseg->next_blkoff = pos;\n}\n\n/*\n * If a segment is written by LFS manner, next block offset is just obtained\n * by increasing the current block offset. However, if a segment is written by\n * SSR manner, next block offset obtained by calling __next_free_blkoff\n */\nstatic void __refresh_next_blkoff(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct curseg_info *seg)\n{\n\tif (seg->alloc_type == SSR)\n\t\t__next_free_blkoff(sbi, seg, seg->next_blkoff + 1);\n\telse\n\t\tseg->next_blkoff++;\n}\n\n/*\n * This function always allocates a used segment(from dirty seglist) by SSR\n * manner, so it should recover the existing segment information of valid blocks\n */\nstatic void change_curseg(struct f2fs_sb_info *sbi, int type, bool reuse)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int new_segno = curseg->next_segno;\n\tstruct f2fs_summary_block *sum_node;\n\tstruct page *sum_page;\n\n\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, curseg->segno));\n\t__set_test_and_inuse(sbi, new_segno);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\t__remove_dirty_segment(sbi, new_segno, PRE);\n\t__remove_dirty_segment(sbi, new_segno, DIRTY);\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = SSR;\n\t__next_free_blkoff(sbi, curseg, 0);\n\n\tif (reuse) {\n\t\tsum_page = get_sum_page(sbi, new_segno);\n\t\tsum_node = (struct f2fs_summary_block *)page_address(sum_page);\n\t\tmemcpy(curseg->sum_blk, sum_node, SUM_ENTRY_SIZE);\n\t\tf2fs_put_page(sum_page, 1);\n\t}\n}\n\nstatic int get_ssr_segment(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tconst struct victim_selection *v_ops = DIRTY_I(sbi)->v_ops;\n\tunsigned segno = NULL_SEGNO;\n\tint i, cnt;\n\tbool reversed = false;\n\n\t/* need_SSR() already forces to do this */\n\tif (v_ops->get_victim(sbi, &segno, BG_GC, type, SSR)) {\n\t\tcurseg->next_segno = segno;\n\t\treturn 1;\n\t}\n\n\t/* For node segments, let's do SSR more intensively */\n\tif (IS_NODESEG(type)) {\n\t\tif (type >= CURSEG_WARM_NODE) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_NODE;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_NODE;\n\t\t}\n\t\tcnt = NR_CURSEG_NODE_TYPE;\n\t} else {\n\t\tif (type >= CURSEG_WARM_DATA) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_DATA;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_DATA;\n\t\t}\n\t\tcnt = NR_CURSEG_DATA_TYPE;\n\t}\n\n\tfor (; cnt-- > 0; reversed ? i-- : i++) {\n\t\tif (i == type)\n\t\t\tcontinue;\n\t\tif (v_ops->get_victim(sbi, &segno, BG_GC, i, SSR)) {\n\t\t\tcurseg->next_segno = segno;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * flush out current segment and replace it with new segment\n * This function should be returned with success, otherwise BUG\n */\nstatic void allocate_segment_by_default(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, bool force)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tif (force)\n\t\tnew_curseg(sbi, type, true);\n\telse if (!is_set_ckpt_flags(sbi, CP_CRC_RECOVERY_FLAG) &&\n\t\t\t\t\ttype == CURSEG_WARM_NODE)\n\t\tnew_curseg(sbi, type, false);\n\telse if (curseg->alloc_type == LFS && is_next_segment_free(sbi, type))\n\t\tnew_curseg(sbi, type, false);\n\telse if (need_SSR(sbi) && get_ssr_segment(sbi, type))\n\t\tchange_curseg(sbi, type, true);\n\telse\n\t\tnew_curseg(sbi, type, false);\n\n\tstat_inc_seg_type(sbi, curseg);\n}\n\nvoid allocate_new_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg;\n\tunsigned int old_segno;\n\tint i;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tcurseg = CURSEG_I(sbi, i);\n\t\told_segno = curseg->segno;\n\t\tSIT_I(sbi)->s_ops->allocate_segment(sbi, i, true);\n\t\tlocate_dirty_segment(sbi, old_segno);\n\t}\n}\n\nstatic const struct segment_allocation default_salloc_ops = {\n\t.allocate_segment = allocate_segment_by_default,\n};\n\nbool exist_trim_candidates(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\t__u64 trim_start = cpc->trim_start;\n\tbool has_candidate = false;\n\n\tmutex_lock(&SIT_I(sbi)->sentry_lock);\n\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++) {\n\t\tif (add_discard_addrs(sbi, cpc, true)) {\n\t\t\thas_candidate = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&SIT_I(sbi)->sentry_lock);\n\n\tcpc->trim_start = trim_start;\n\treturn has_candidate;\n}\n\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range)\n{\n\t__u64 start = F2FS_BYTES_TO_BLK(range->start);\n\t__u64 end = start + F2FS_BYTES_TO_BLK(range->len) - 1;\n\tunsigned int start_segno, end_segno;\n\tstruct cp_control cpc;\n\tint err = 0;\n\n\tif (start >= MAX_BLKADDR(sbi) || range->len < sbi->blocksize)\n\t\treturn -EINVAL;\n\n\tcpc.trimmed = 0;\n\tif (end <= MAIN_BLKADDR(sbi))\n\t\tgoto out;\n\n\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\"Found FS corruption, run fsck to fix.\");\n\t\tgoto out;\n\t}\n\n\t/* start/end segment number in main_area */\n\tstart_segno = (start <= MAIN_BLKADDR(sbi)) ? 0 : GET_SEGNO(sbi, start);\n\tend_segno = (end >= MAX_BLKADDR(sbi)) ? MAIN_SEGS(sbi) - 1 :\n\t\t\t\t\t\tGET_SEGNO(sbi, end);\n\tcpc.reason = CP_DISCARD;\n\tcpc.trim_minlen = max_t(__u64, 1, F2FS_BYTES_TO_BLK(range->minlen));\n\n\t/* do checkpoint to issue discard commands safely */\n\tfor (; start_segno <= end_segno; start_segno = cpc.trim_end + 1) {\n\t\tcpc.trim_start = start_segno;\n\n\t\tif (sbi->discard_blks == 0)\n\t\t\tbreak;\n\t\telse if (sbi->discard_blks < BATCHED_TRIM_BLOCKS(sbi))\n\t\t\tcpc.trim_end = end_segno;\n\t\telse\n\t\t\tcpc.trim_end = min_t(unsigned int,\n\t\t\t\trounddown(start_segno +\n\t\t\t\tBATCHED_TRIM_SEGMENTS(sbi),\n\t\t\t\tsbi->segs_per_sec) - 1, end_segno);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\nout:\n\trange->len = F2FS_BLK_TO_BYTES(cpc.trimmed);\n\treturn err;\n}\n\nstatic bool __has_curseg_space(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tif (curseg->next_blkoff < sbi->blocks_per_seg)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int __get_segment_type_2(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA)\n\t\treturn CURSEG_HOT_DATA;\n\telse\n\t\treturn CURSEG_HOT_NODE;\n}\n\nstatic int __get_segment_type_4(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (S_ISDIR(inode->i_mode))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\telse\n\t\t\treturn CURSEG_COLD_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page) && is_cold_node(fio->page))\n\t\t\treturn CURSEG_WARM_NODE;\n\t\telse\n\t\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type_6(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (is_cold_data(fio->page) || file_is_cold(inode))\n\t\t\treturn CURSEG_COLD_DATA;\n\t\tif (is_inode_flag_set(inode, FI_HOT_DATA))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\treturn CURSEG_WARM_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page))\n\t\t\treturn is_cold_node(fio->page) ? CURSEG_WARM_NODE :\n\t\t\t\t\t\tCURSEG_HOT_NODE;\n\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type(struct f2fs_io_info *fio)\n{\n\tint type = 0;\n\n\tswitch (fio->sbi->active_logs) {\n\tcase 2:\n\t\ttype = __get_segment_type_2(fio);\n\t\tbreak;\n\tcase 4:\n\t\ttype = __get_segment_type_4(fio);\n\t\tbreak;\n\tcase 6:\n\t\ttype = __get_segment_type_6(fio);\n\t\tbreak;\n\tdefault:\n\t\tf2fs_bug_on(fio->sbi, true);\n\t}\n\n\tif (IS_HOT(type))\n\t\tfio->temp = HOT;\n\telse if (IS_WARM(type))\n\t\tfio->temp = WARM;\n\telse\n\t\tfio->temp = COLD;\n\treturn type;\n}\n\nvoid allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\tstruct f2fs_summary *sum, int type,\n\t\tstruct f2fs_io_info *fio, bool add_list)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tmutex_lock(&sit_i->sentry_lock);\n\n\t*new_blkaddr = NEXT_FREE_BLKADDR(sbi, curseg);\n\n\tf2fs_wait_discard_bio(sbi, *new_blkaddr);\n\n\t/*\n\t * __add_sum_entry should be resided under the curseg_mutex\n\t * because, this function updates a summary entry in the\n\t * current summary block.\n\t */\n\t__add_sum_entry(sbi, type, sum);\n\n\t__refresh_next_blkoff(sbi, curseg);\n\n\tstat_inc_block_count(sbi, curseg);\n\n\tif (!__has_curseg_space(sbi, type))\n\t\tsit_i->s_ops->allocate_segment(sbi, type, false);\n\t/*\n\t * SIT information should be updated after segment allocation,\n\t * since we need to keep dirty segments precisely under SSR.\n\t */\n\trefresh_sit_entry(sbi, old_blkaddr, *new_blkaddr);\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\tif (page && IS_NODESEG(type))\n\t\tfill_node_footer_blkaddr(page, NEXT_FREE_BLKADDR(sbi, curseg));\n\n\tif (add_list) {\n\t\tstruct f2fs_bio_info *io;\n\n\t\tINIT_LIST_HEAD(&fio->list);\n\t\tfio->in_list = true;\n\t\tio = sbi->write_io[fio->type] + fio->temp;\n\t\tspin_lock(&io->io_lock);\n\t\tlist_add_tail(&fio->list, &io->io_list);\n\t\tspin_unlock(&io->io_lock);\n\t}\n\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nstatic void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)\n{\n\tint type = __get_segment_type(fio);\n\tint err;\n\nreallocate:\n\tallocate_data_block(fio->sbi, fio->page, fio->old_blkaddr,\n\t\t\t&fio->new_blkaddr, sum, type, fio, true);\n\n\t/* writeout dirty page into bdev */\n\terr = f2fs_submit_page_write(fio);\n\tif (err == -EAGAIN) {\n\t\tfio->old_blkaddr = fio->new_blkaddr;\n\t\tgoto reallocate;\n\t}\n}\n\nvoid write_meta_page(struct f2fs_sb_info *sbi, struct page *page)\n{\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = META,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_META | REQ_PRIO,\n\t\t.old_blkaddr = page->index,\n\t\t.new_blkaddr = page->index,\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t\t.in_list = false,\n\t};\n\n\tif (unlikely(page->index >= MAIN_BLKADDR(sbi)))\n\t\tfio.op_flags &= ~REQ_META;\n\n\tset_page_writeback(page);\n\tf2fs_submit_page_write(&fio);\n}\n\nvoid write_node_page(unsigned int nid, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, nid, 0, 0);\n\tdo_write_page(&sum, fio);\n}\n\nvoid write_data_page(struct dnode_of_data *dn, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\n\tf2fs_bug_on(sbi, dn->data_blkaddr == NULL_ADDR);\n\tget_node_info(sbi, dn->nid, &ni);\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\tdo_write_page(&sum, fio);\n\tf2fs_update_data_blkaddr(dn, fio->new_blkaddr);\n}\n\nint rewrite_data_page(struct f2fs_io_info *fio)\n{\n\tfio->new_blkaddr = fio->old_blkaddr;\n\tstat_inc_inplace_blocks(fio->sbi);\n\treturn f2fs_submit_page_bio(fio);\n}\n\nvoid __f2fs_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\t\tbool recover_curseg, bool recover_newaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg;\n\tunsigned int segno, old_cursegno;\n\tstruct seg_entry *se;\n\tint type;\n\tunsigned short old_blkoff;\n\n\tsegno = GET_SEGNO(sbi, new_blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\ttype = se->type;\n\n\tif (!recover_curseg) {\n\t\t/* for recovery flow */\n\t\tif (se->valid_blocks == 0 && !IS_CURSEG(sbi, segno)) {\n\t\t\tif (old_blkaddr == NULL_ADDR)\n\t\t\t\ttype = CURSEG_COLD_DATA;\n\t\t\telse\n\t\t\t\ttype = CURSEG_WARM_DATA;\n\t\t}\n\t} else {\n\t\tif (!IS_CURSEG(sbi, segno))\n\t\t\ttype = CURSEG_WARM_DATA;\n\t}\n\n\tcurseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tmutex_lock(&sit_i->sentry_lock);\n\n\told_cursegno = curseg->segno;\n\told_blkoff = curseg->next_blkoff;\n\n\t/* change the current segment */\n\tif (segno != curseg->segno) {\n\t\tcurseg->next_segno = segno;\n\t\tchange_curseg(sbi, type, true);\n\t}\n\n\tcurseg->next_blkoff = GET_BLKOFF_FROM_SEG0(sbi, new_blkaddr);\n\t__add_sum_entry(sbi, type, sum);\n\n\tif (!recover_curseg || recover_newaddr)\n\t\tupdate_sit_entry(sbi, new_blkaddr, 1);\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old_blkaddr, -1);\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old_blkaddr));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new_blkaddr));\n\n\tlocate_dirty_segment(sbi, old_cursegno);\n\n\tif (recover_curseg) {\n\t\tif (old_cursegno != curseg->segno) {\n\t\t\tcurseg->next_segno = old_cursegno;\n\t\t\tchange_curseg(sbi, type, true);\n\t\t}\n\t\tcurseg->next_blkoff = old_blkoff;\n\t}\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\t\tunsigned char version, bool recover_curseg,\n\t\t\t\tbool recover_newaddr)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, version);\n\n\t__f2fs_replace_block(sbi, &sum, old_addr, new_addr,\n\t\t\t\t\trecover_curseg, recover_newaddr);\n\n\tf2fs_update_data_blkaddr(dn, new_addr);\n}\n\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\t\tenum page_type type, bool ordered)\n{\n\tif (PageWriteback(page)) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_P_SB(page);\n\n\t\tf2fs_submit_merged_write_cond(sbi, page->mapping->host,\n\t\t\t\t\t\t0, page->index, type);\n\t\tif (ordered)\n\t\t\twait_on_page_writeback(page);\n\t\telse\n\t\t\twait_for_stable_page(page);\n\t}\n}\n\nvoid f2fs_wait_on_encrypted_page_writeback(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tblock_t blkaddr)\n{\n\tstruct page *cpage;\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn;\n\n\tcpage = find_lock_page(META_MAPPING(sbi), blkaddr);\n\tif (cpage) {\n\t\tf2fs_wait_on_page_writeback(cpage, DATA, true);\n\t\tf2fs_put_page(cpage, 1);\n\t}\n}\n\nstatic int read_compacted_summaries(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct curseg_info *seg_i;\n\tunsigned char *kaddr;\n\tstruct page *page;\n\tblock_t start;\n\tint i, j, offset;\n\n\tstart = start_sum_block(sbi);\n\n\tpage = get_meta_page(sbi, start++);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t/* Step 1: restore nat cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(seg_i->journal, kaddr, SUM_JOURNAL_SIZE);\n\n\t/* Step 2: restore sit cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(seg_i->journal, kaddr + SUM_JOURNAL_SIZE, SUM_JOURNAL_SIZE);\n\toffset = 2 * SUM_JOURNAL_SIZE;\n\n\t/* Step 3: restore summary entries */\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blk_off;\n\t\tunsigned int segno;\n\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[i]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[i]);\n\t\tseg_i->next_segno = segno;\n\t\treset_curseg(sbi, i, 0);\n\t\tseg_i->alloc_type = ckpt->alloc_type[i];\n\t\tseg_i->next_blkoff = blk_off;\n\n\t\tif (seg_i->alloc_type == SSR)\n\t\t\tblk_off = sbi->blocks_per_seg;\n\n\t\tfor (j = 0; j < blk_off; j++) {\n\t\t\tstruct f2fs_summary *s;\n\t\t\ts = (struct f2fs_summary *)(kaddr + offset);\n\t\t\tseg_i->sum_blk->entries[j] = *s;\n\t\t\toffset += SUMMARY_SIZE;\n\t\t\tif (offset + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\n\t\t\tpage = get_meta_page(sbi, start++);\n\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\toffset = 0;\n\t\t}\n\t}\n\tf2fs_put_page(page, 1);\n\treturn 0;\n}\n\nstatic int read_normal_summaries(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_summary_block *sum;\n\tstruct curseg_info *curseg;\n\tstruct page *new;\n\tunsigned short blk_off;\n\tunsigned int segno = 0;\n\tblock_t blk_addr = 0;\n\n\t/* get segment number and block addr */\n\tif (IS_DATASEG(type)) {\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[type]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_DATA]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_TYPE, type);\n\t\telse\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_DATA_TYPE, type);\n\t} else {\n\t\tsegno = le32_to_cpu(ckpt->cur_node_segno[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_node_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_NODE_TYPE,\n\t\t\t\t\t\t\ttype - CURSEG_HOT_NODE);\n\t\telse\n\t\t\tblk_addr = GET_SUM_BLOCK(sbi, segno);\n\t}\n\n\tnew = get_meta_page(sbi, blk_addr);\n\tsum = (struct f2fs_summary_block *)page_address(new);\n\n\tif (IS_NODESEG(type)) {\n\t\tif (__exist_node_summaries(sbi)) {\n\t\t\tstruct f2fs_summary *ns = &sum->entries[0];\n\t\t\tint i;\n\t\t\tfor (i = 0; i < sbi->blocks_per_seg; i++, ns++) {\n\t\t\t\tns->version = 0;\n\t\t\t\tns->ofs_in_node = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tint err;\n\n\t\t\terr = restore_node_summary(sbi, segno, sum);\n\t\t\tif (err) {\n\t\t\t\tf2fs_put_page(new, 1);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* set uncompleted segment to curseg */\n\tcurseg = CURSEG_I(sbi, type);\n\tmutex_lock(&curseg->curseg_mutex);\n\n\t/* update journal info */\n\tdown_write(&curseg->journal_rwsem);\n\tmemcpy(curseg->journal, &sum->journal, SUM_JOURNAL_SIZE);\n\tup_write(&curseg->journal_rwsem);\n\n\tmemcpy(curseg->sum_blk->entries, sum->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&curseg->sum_blk->footer, &sum->footer, SUM_FOOTER_SIZE);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 0);\n\tcurseg->alloc_type = ckpt->alloc_type[type];\n\tcurseg->next_blkoff = blk_off;\n\tmutex_unlock(&curseg->curseg_mutex);\n\tf2fs_put_page(new, 1);\n\treturn 0;\n}\n\nstatic int restore_curseg_summaries(struct f2fs_sb_info *sbi)\n{\n\tint type = CURSEG_HOT_DATA;\n\tint err;\n\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG)) {\n\t\tint npages = npages_for_summary_flush(sbi, true);\n\n\t\tif (npages >= 2)\n\t\t\tra_meta_pages(sbi, start_sum_block(sbi), npages,\n\t\t\t\t\t\t\tMETA_CP, true);\n\n\t\t/* restore for compacted data summary */\n\t\tif (read_compacted_summaries(sbi))\n\t\t\treturn -EINVAL;\n\t\ttype = CURSEG_HOT_NODE;\n\t}\n\n\tif (__exist_node_summaries(sbi))\n\t\tra_meta_pages(sbi, sum_blk_addr(sbi, NR_CURSEG_TYPE, type),\n\t\t\t\t\tNR_CURSEG_TYPE - type, META_CP, true);\n\n\tfor (; type <= CURSEG_COLD_NODE; type++) {\n\t\terr = read_normal_summaries(sbi, type);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void write_compacted_summaries(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct page *page;\n\tunsigned char *kaddr;\n\tstruct f2fs_summary *summary;\n\tstruct curseg_info *seg_i;\n\tint written_size = 0;\n\tint i, j;\n\n\tpage = grab_meta_page(sbi, blkaddr++);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t/* Step 1: write nat cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(kaddr, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t/* Step 2: write sit cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(kaddr + written_size, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t/* Step 3: write summary entries */\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blkoff;\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tif (sbi->ckpt->alloc_type[i] == SSR)\n\t\t\tblkoff = sbi->blocks_per_seg;\n\t\telse\n\t\t\tblkoff = curseg_blkoff(sbi, i);\n\n\t\tfor (j = 0; j < blkoff; j++) {\n\t\t\tif (!page) {\n\t\t\t\tpage = grab_meta_page(sbi, blkaddr++);\n\t\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\t\twritten_size = 0;\n\t\t\t}\n\t\t\tsummary = (struct f2fs_summary *)(kaddr + written_size);\n\t\t\t*summary = seg_i->sum_blk->entries[j];\n\t\t\twritten_size += SUMMARY_SIZE;\n\n\t\t\tif (written_size + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\t\t}\n\t}\n\tif (page) {\n\t\tset_page_dirty(page);\n\t\tf2fs_put_page(page, 1);\n\t}\n}\n\nstatic void write_normal_summaries(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type)\n{\n\tint i, end;\n\tif (IS_DATASEG(type))\n\t\tend = type + NR_CURSEG_DATA_TYPE;\n\telse\n\t\tend = type + NR_CURSEG_NODE_TYPE;\n\n\tfor (i = type; i < end; i++)\n\t\twrite_current_sum_page(sbi, i, blkaddr + (i - type));\n}\n\nvoid write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG))\n\t\twrite_compacted_summaries(sbi, start_blk);\n\telse\n\t\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_DATA);\n}\n\nvoid write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_NODE);\n}\n\nint lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\t\t\tunsigned int val, int alloc)\n{\n\tint i;\n\n\tif (type == NAT_JOURNAL) {\n\t\tfor (i = 0; i < nats_in_cursum(journal); i++) {\n\t\t\tif (le32_to_cpu(nid_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\t}\n\t\tif (alloc && __has_cursum_space(journal, 1, NAT_JOURNAL))\n\t\t\treturn update_nats_in_cursum(journal, 1);\n\t} else if (type == SIT_JOURNAL) {\n\t\tfor (i = 0; i < sits_in_cursum(journal); i++)\n\t\t\tif (le32_to_cpu(segno_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\tif (alloc && __has_cursum_space(journal, 1, SIT_JOURNAL))\n\t\t\treturn update_sits_in_cursum(journal, 1);\n\t}\n\treturn -1;\n}\n\nstatic struct page *get_current_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int segno)\n{\n\treturn get_meta_page(sbi, current_sit_addr(sbi, segno));\n}\n\nstatic struct page *get_next_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int start)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct page *src_page, *dst_page;\n\tpgoff_t src_off, dst_off;\n\tvoid *src_addr, *dst_addr;\n\n\tsrc_off = current_sit_addr(sbi, start);\n\tdst_off = next_sit_addr(sbi, src_off);\n\n\t/* get current sit block page without lock */\n\tsrc_page = get_meta_page(sbi, src_off);\n\tdst_page = grab_meta_page(sbi, dst_off);\n\tf2fs_bug_on(sbi, PageDirty(src_page));\n\n\tsrc_addr = page_address(src_page);\n\tdst_addr = page_address(dst_page);\n\tmemcpy(dst_addr, src_addr, PAGE_SIZE);\n\n\tset_page_dirty(dst_page);\n\tf2fs_put_page(src_page, 1);\n\n\tset_to_next_sit(sit_i, start);\n\n\treturn dst_page;\n}\n\nstatic struct sit_entry_set *grab_sit_entry_set(void)\n{\n\tstruct sit_entry_set *ses =\n\t\t\tf2fs_kmem_cache_alloc(sit_entry_set_slab, GFP_NOFS);\n\n\tses->entry_cnt = 0;\n\tINIT_LIST_HEAD(&ses->set_list);\n\treturn ses;\n}\n\nstatic void release_sit_entry_set(struct sit_entry_set *ses)\n{\n\tlist_del(&ses->set_list);\n\tkmem_cache_free(sit_entry_set_slab, ses);\n}\n\nstatic void adjust_sit_entry_set(struct sit_entry_set *ses,\n\t\t\t\t\t\tstruct list_head *head)\n{\n\tstruct sit_entry_set *next = ses;\n\n\tif (list_is_last(&ses->set_list, head))\n\t\treturn;\n\n\tlist_for_each_entry_continue(next, head, set_list)\n\t\tif (ses->entry_cnt <= next->entry_cnt)\n\t\t\tbreak;\n\n\tlist_move_tail(&ses->set_list, &next->set_list);\n}\n\nstatic void add_sit_entry(unsigned int segno, struct list_head *head)\n{\n\tstruct sit_entry_set *ses;\n\tunsigned int start_segno = START_SEGNO(segno);\n\n\tlist_for_each_entry(ses, head, set_list) {\n\t\tif (ses->start_segno == start_segno) {\n\t\t\tses->entry_cnt++;\n\t\t\tadjust_sit_entry_set(ses, head);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tses = grab_sit_entry_set();\n\n\tses->start_segno = start_segno;\n\tses->entry_cnt++;\n\tlist_add(&ses->set_list, head);\n}\n\nstatic void add_sits_in_set(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\tstruct list_head *set_list = &sm_info->sit_entry_set;\n\tunsigned long *bitmap = SIT_I(sbi)->dirty_sentries_bitmap;\n\tunsigned int segno;\n\n\tfor_each_set_bit(segno, bitmap, MAIN_SEGS(sbi))\n\t\tadd_sit_entry(segno, set_list);\n}\n\nstatic void remove_sits_in_journal(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tint i;\n\n\tdown_write(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int segno;\n\t\tbool dirtied;\n\n\t\tsegno = le32_to_cpu(segno_in_journal(journal, i));\n\t\tdirtied = __mark_sit_entry_dirty(sbi, segno);\n\n\t\tif (!dirtied)\n\t\t\tadd_sit_entry(segno, &SM_I(sbi)->sit_entry_set);\n\t}\n\tupdate_sits_in_cursum(journal, -i);\n\tup_write(&curseg->journal_rwsem);\n}\n\n/*\n * CP calls this function, which flushes SIT entries including sit_journal,\n * and moves prefree segs to free segs.\n */\nvoid flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned long *bitmap = sit_i->dirty_sentries_bitmap;\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct sit_entry_set *ses, *tmp;\n\tstruct list_head *head = &SM_I(sbi)->sit_entry_set;\n\tbool to_journal = true;\n\tstruct seg_entry *se;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tif (!sit_i->dirty_sentries)\n\t\tgoto out;\n\n\t/*\n\t * add and account sit entries of dirty bitmap in sit entry\n\t * set temporarily\n\t */\n\tadd_sits_in_set(sbi);\n\n\t/*\n\t * if there are no enough space in journal to store dirty sit\n\t * entries, remove all entries from journal and add and account\n\t * them in sit entry set.\n\t */\n\tif (!__has_cursum_space(journal, sit_i->dirty_sentries, SIT_JOURNAL))\n\t\tremove_sits_in_journal(sbi);\n\n\t/*\n\t * there are two steps to flush sit entries:\n\t * #1, flush sit entries to journal in current cold data summary block.\n\t * #2, flush sit entries to sit page.\n\t */\n\tlist_for_each_entry_safe(ses, tmp, head, set_list) {\n\t\tstruct page *page = NULL;\n\t\tstruct f2fs_sit_block *raw_sit = NULL;\n\t\tunsigned int start_segno = ses->start_segno;\n\t\tunsigned int end = min(start_segno + SIT_ENTRY_PER_BLOCK,\n\t\t\t\t\t\t(unsigned long)MAIN_SEGS(sbi));\n\t\tunsigned int segno = start_segno;\n\n\t\tif (to_journal &&\n\t\t\t!__has_cursum_space(journal, ses->entry_cnt, SIT_JOURNAL))\n\t\t\tto_journal = false;\n\n\t\tif (to_journal) {\n\t\t\tdown_write(&curseg->journal_rwsem);\n\t\t} else {\n\t\t\tpage = get_next_sit_page(sbi, start_segno);\n\t\t\traw_sit = page_address(page);\n\t\t}\n\n\t\t/* flush dirty sit entries in region of current sit set */\n\t\tfor_each_set_bit_from(segno, bitmap, end) {\n\t\t\tint offset, sit_offset;\n\n\t\t\tse = get_seg_entry(sbi, segno);\n\n\t\t\t/* add discard candidates */\n\t\t\tif (!(cpc->reason & CP_DISCARD)) {\n\t\t\t\tcpc->trim_start = segno;\n\t\t\t\tadd_discard_addrs(sbi, cpc, false);\n\t\t\t}\n\n\t\t\tif (to_journal) {\n\t\t\t\toffset = lookup_journal_in_cursum(journal,\n\t\t\t\t\t\t\tSIT_JOURNAL, segno, 1);\n\t\t\t\tf2fs_bug_on(sbi, offset < 0);\n\t\t\t\tsegno_in_journal(journal, offset) =\n\t\t\t\t\t\t\tcpu_to_le32(segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t&sit_in_journal(journal, offset));\n\t\t\t} else {\n\t\t\t\tsit_offset = SIT_ENTRY_OFFSET(sit_i, segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t\t&raw_sit->entries[sit_offset]);\n\t\t\t}\n\n\t\t\t__clear_bit(segno, bitmap);\n\t\t\tsit_i->dirty_sentries--;\n\t\t\tses->entry_cnt--;\n\t\t}\n\n\t\tif (to_journal)\n\t\t\tup_write(&curseg->journal_rwsem);\n\t\telse\n\t\t\tf2fs_put_page(page, 1);\n\n\t\tf2fs_bug_on(sbi, ses->entry_cnt);\n\t\trelease_sit_entry_set(ses);\n\t}\n\n\tf2fs_bug_on(sbi, !list_empty(head));\n\tf2fs_bug_on(sbi, sit_i->dirty_sentries);\nout:\n\tif (cpc->reason & CP_DISCARD) {\n\t\t__u64 trim_start = cpc->trim_start;\n\n\t\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++)\n\t\t\tadd_discard_addrs(sbi, cpc, false);\n\n\t\tcpc->trim_start = trim_start;\n\t}\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\tset_prefree_as_free_segments(sbi);\n}\n\nstatic int build_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct sit_info *sit_i;\n\tunsigned int sit_segs, start;\n\tchar *src_bitmap;\n\tunsigned int bitmap_size;\n\n\t/* allocate memory for SIT information */\n\tsit_i = kzalloc(sizeof(struct sit_info), GFP_KERNEL);\n\tif (!sit_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->sit_info = sit_i;\n\n\tsit_i->sentries = f2fs_kvzalloc(MAIN_SEGS(sbi) *\n\t\t\t\t\tsizeof(struct seg_entry), GFP_KERNEL);\n\tif (!sit_i->sentries)\n\t\treturn -ENOMEM;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tsit_i->dirty_sentries_bitmap = f2fs_kvzalloc(bitmap_size, GFP_KERNEL);\n\tif (!sit_i->dirty_sentries_bitmap)\n\t\treturn -ENOMEM;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tsit_i->sentries[start].cur_valid_map\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tsit_i->sentries[start].ckpt_valid_map\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tif (!sit_i->sentries[start].cur_valid_map ||\n\t\t\t\t!sit_i->sentries[start].ckpt_valid_map)\n\t\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tsit_i->sentries[start].cur_valid_map_mir\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tif (!sit_i->sentries[start].cur_valid_map_mir)\n\t\t\treturn -ENOMEM;\n#endif\n\n\t\tif (f2fs_discard_en(sbi)) {\n\t\t\tsit_i->sentries[start].discard_map\n\t\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\t\tif (!sit_i->sentries[start].discard_map)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tsit_i->tmp_map = kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\tif (!sit_i->tmp_map)\n\t\treturn -ENOMEM;\n\n\tif (sbi->segs_per_sec > 1) {\n\t\tsit_i->sec_entries = f2fs_kvzalloc(MAIN_SECS(sbi) *\n\t\t\t\t\tsizeof(struct sec_entry), GFP_KERNEL);\n\t\tif (!sit_i->sec_entries)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* get information related with SIT */\n\tsit_segs = le32_to_cpu(raw_super->segment_count_sit) >> 1;\n\n\t/* setup SIT bitmap from ckeckpoint pack */\n\tbitmap_size = __bitmap_size(sbi, SIT_BITMAP);\n\tsrc_bitmap = __bitmap_ptr(sbi, SIT_BITMAP);\n\n\tsit_i->sit_bitmap = kmemdup(src_bitmap, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap)\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\tsit_i->sit_bitmap_mir = kmemdup(src_bitmap, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap_mir)\n\t\treturn -ENOMEM;\n#endif\n\n\t/* init SIT information */\n\tsit_i->s_ops = &default_salloc_ops;\n\n\tsit_i->sit_base_addr = le32_to_cpu(raw_super->sit_blkaddr);\n\tsit_i->sit_blocks = sit_segs << sbi->log_blocks_per_seg;\n\tsit_i->written_valid_blocks = 0;\n\tsit_i->bitmap_size = bitmap_size;\n\tsit_i->dirty_sentries = 0;\n\tsit_i->sents_per_block = SIT_ENTRY_PER_BLOCK;\n\tsit_i->elapsed_time = le64_to_cpu(sbi->ckpt->elapsed_time);\n\tsit_i->mounted_time = CURRENT_TIME_SEC.tv_sec;\n\tmutex_init(&sit_i->sentry_lock);\n\treturn 0;\n}\n\nstatic int build_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i;\n\tunsigned int bitmap_size, sec_bitmap_size;\n\n\t/* allocate memory for free segmap information */\n\tfree_i = kzalloc(sizeof(struct free_segmap_info), GFP_KERNEL);\n\tif (!free_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->free_info = free_i;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tfree_i->free_segmap = f2fs_kvmalloc(bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_segmap)\n\t\treturn -ENOMEM;\n\n\tsec_bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\tfree_i->free_secmap = f2fs_kvmalloc(sec_bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_secmap)\n\t\treturn -ENOMEM;\n\n\t/* set all segments as dirty temporarily */\n\tmemset(free_i->free_segmap, 0xff, bitmap_size);\n\tmemset(free_i->free_secmap, 0xff, sec_bitmap_size);\n\n\t/* init free segmap information */\n\tfree_i->start_segno = GET_SEGNO_FROM_SEG0(sbi, MAIN_BLKADDR(sbi));\n\tfree_i->free_segments = 0;\n\tfree_i->free_sections = 0;\n\tspin_lock_init(&free_i->segmap_lock);\n\treturn 0;\n}\n\nstatic int build_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array;\n\tint i;\n\n\tarray = kcalloc(NR_CURSEG_TYPE, sizeof(*array), GFP_KERNEL);\n\tif (!array)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->curseg_array = array;\n\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tmutex_init(&array[i].curseg_mutex);\n\t\tarray[i].sum_blk = kzalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (!array[i].sum_blk)\n\t\t\treturn -ENOMEM;\n\t\tinit_rwsem(&array[i].journal_rwsem);\n\t\tarray[i].journal = kzalloc(sizeof(struct f2fs_journal),\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!array[i].journal)\n\t\t\treturn -ENOMEM;\n\t\tarray[i].segno = NULL_SEGNO;\n\t\tarray[i].next_blkoff = 0;\n\t}\n\treturn restore_curseg_summaries(sbi);\n}\n\nstatic void build_sit_entries(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct seg_entry *se;\n\tstruct f2fs_sit_entry sit;\n\tint sit_blk_cnt = SIT_BLK_CNT(sbi);\n\tunsigned int i, start, end;\n\tunsigned int readed, start_blk = 0;\n\n\tdo {\n\t\treaded = ra_meta_pages(sbi, start_blk, BIO_MAX_PAGES,\n\t\t\t\t\t\t\tMETA_SIT, true);\n\n\t\tstart = start_blk * sit_i->sents_per_block;\n\t\tend = (start_blk + readed) * sit_i->sents_per_block;\n\n\t\tfor (; start < end && start < MAIN_SEGS(sbi); start++) {\n\t\t\tstruct f2fs_sit_block *sit_blk;\n\t\t\tstruct page *page;\n\n\t\t\tse = &sit_i->sentries[start];\n\t\t\tpage = get_current_sit_page(sbi, start);\n\t\t\tsit_blk = (struct f2fs_sit_block *)page_address(page);\n\t\t\tsit = sit_blk->entries[SIT_ENTRY_OFFSET(sit_i, start)];\n\t\t\tf2fs_put_page(page, 1);\n\n\t\t\tcheck_block_count(sbi, start, &sit);\n\t\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\t\t/* build discard map only one time */\n\t\t\tif (f2fs_discard_en(sbi)) {\n\t\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t} else {\n\t\t\t\t\tmemcpy(se->discard_map,\n\t\t\t\t\t\tse->cur_valid_map,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t\tsbi->discard_blks +=\n\t\t\t\t\t\tsbi->blocks_per_seg -\n\t\t\t\t\t\tse->valid_blocks;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sbi->segs_per_sec > 1)\n\t\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t}\n\t\tstart_blk += readed;\n\t} while (start_blk < sit_blk_cnt);\n\n\tdown_read(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int old_valid_blocks;\n\n\t\tstart = le32_to_cpu(segno_in_journal(journal, i));\n\t\tse = &sit_i->sentries[start];\n\t\tsit = sit_in_journal(journal, i);\n\n\t\told_valid_blocks = se->valid_blocks;\n\n\t\tcheck_block_count(sbi, start, &sit);\n\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\tif (f2fs_discard_en(sbi)) {\n\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t} else {\n\t\t\t\tmemcpy(se->discard_map, se->cur_valid_map,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\tsbi->discard_blks += old_valid_blocks -\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t\t}\n\t\t}\n\n\t\tif (sbi->segs_per_sec > 1)\n\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\tse->valid_blocks - old_valid_blocks;\n\t}\n\tup_read(&curseg->journal_rwsem);\n}\n\nstatic void init_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tunsigned int start;\n\tint type;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, start);\n\t\tif (!sentry->valid_blocks)\n\t\t\t__set_free(sbi, start);\n\t\telse\n\t\t\tSIT_I(sbi)->written_valid_blocks +=\n\t\t\t\t\t\tsentry->valid_blocks;\n\t}\n\n\t/* set use the current segments */\n\tfor (type = CURSEG_HOT_DATA; type <= CURSEG_COLD_NODE; type++) {\n\t\tstruct curseg_info *curseg_t = CURSEG_I(sbi, type);\n\t\t__set_test_and_inuse(sbi, curseg_t->segno);\n\t}\n}\n\nstatic void init_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno = 0, offset = 0;\n\tunsigned short valid_blocks;\n\n\twhile (1) {\n\t\t/* find dirty segment based on free segmap */\n\t\tsegno = find_next_inuse(free_i, MAIN_SEGS(sbi), offset);\n\t\tif (segno >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\toffset = segno + 1;\n\t\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\t\tif (valid_blocks == sbi->blocks_per_seg || !valid_blocks)\n\t\t\tcontinue;\n\t\tif (valid_blocks > sbi->blocks_per_seg) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tcontinue;\n\t\t}\n\t\tmutex_lock(&dirty_i->seglist_lock);\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t\tmutex_unlock(&dirty_i->seglist_lock);\n\t}\n}\n\nstatic int init_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\n\tdirty_i->victim_secmap = f2fs_kvzalloc(bitmap_size, GFP_KERNEL);\n\tif (!dirty_i->victim_secmap)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic int build_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i;\n\tunsigned int bitmap_size, i;\n\n\t/* allocate memory for dirty segments list information */\n\tdirty_i = kzalloc(sizeof(struct dirty_seglist_info), GFP_KERNEL);\n\tif (!dirty_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->dirty_info = dirty_i;\n\tmutex_init(&dirty_i->seglist_lock);\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++) {\n\t\tdirty_i->dirty_segmap[i] = f2fs_kvzalloc(bitmap_size, GFP_KERNEL);\n\t\tif (!dirty_i->dirty_segmap[i])\n\t\t\treturn -ENOMEM;\n\t}\n\n\tinit_dirty_segmap(sbi);\n\treturn init_victim_secmap(sbi);\n}\n\n/*\n * Update min, max modified time for cost-benefit GC algorithm\n */\nstatic void init_min_max_mtime(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tsit_i->min_mtime = LLONG_MAX;\n\n\tfor (segno = 0; segno < MAIN_SEGS(sbi); segno += sbi->segs_per_sec) {\n\t\tunsigned int i;\n\t\tunsigned long long mtime = 0;\n\n\t\tfor (i = 0; i < sbi->segs_per_sec; i++)\n\t\t\tmtime += get_seg_entry(sbi, segno + i)->mtime;\n\n\t\tmtime = div_u64(mtime, sbi->segs_per_sec);\n\n\t\tif (sit_i->min_mtime > mtime)\n\t\t\tsit_i->min_mtime = mtime;\n\t}\n\tsit_i->max_mtime = get_mtime(sbi);\n\tmutex_unlock(&sit_i->sentry_lock);\n}\n\nint build_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_sm_info *sm_info;\n\tint err;\n\n\tsm_info = kzalloc(sizeof(struct f2fs_sm_info), GFP_KERNEL);\n\tif (!sm_info)\n\t\treturn -ENOMEM;\n\n\t/* init sm info */\n\tsbi->sm_info = sm_info;\n\tsm_info->seg0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tsm_info->main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tsm_info->segment_count = le32_to_cpu(raw_super->segment_count);\n\tsm_info->reserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\tsm_info->ovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\tsm_info->main_segments = le32_to_cpu(raw_super->segment_count_main);\n\tsm_info->ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tsm_info->rec_prefree_segments = sm_info->main_segments *\n\t\t\t\t\tDEF_RECLAIM_PREFREE_SEGMENTS / 100;\n\tif (sm_info->rec_prefree_segments > DEF_MAX_RECLAIM_PREFREE_SEGMENTS)\n\t\tsm_info->rec_prefree_segments = DEF_MAX_RECLAIM_PREFREE_SEGMENTS;\n\n\tif (!test_opt(sbi, LFS))\n\t\tsm_info->ipu_policy = 1 << F2FS_IPU_FSYNC;\n\tsm_info->min_ipu_util = DEF_MIN_IPU_UTIL;\n\tsm_info->min_fsync_blocks = DEF_MIN_FSYNC_BLOCKS;\n\tsm_info->min_hot_blocks = DEF_MIN_HOT_BLOCKS;\n\n\tsm_info->trim_sections = DEF_BATCHED_TRIM_SECTIONS;\n\n\tINIT_LIST_HEAD(&sm_info->sit_entry_set);\n\n\tif (!f2fs_readonly(sbi->sb)) {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = create_discard_cmd_control(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = build_sit_info(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_free_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_curseg(sbi);\n\tif (err)\n\t\treturn err;\n\n\t/* reinit free segmap based on SIT */\n\tbuild_sit_entries(sbi);\n\n\tinit_free_segmap(sbi);\n\terr = build_dirty_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\n\tinit_min_max_mtime(sbi);\n\treturn 0;\n}\n\nstatic void discard_dirty_segmap(struct f2fs_sb_info *sbi,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tkvfree(dirty_i->dirty_segmap[dirty_type]);\n\tdirty_i->nr_dirty[dirty_type] = 0;\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic void destroy_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tkvfree(dirty_i->victim_secmap);\n}\n\nstatic void destroy_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tint i;\n\n\tif (!dirty_i)\n\t\treturn;\n\n\t/* discard pre-free/dirty segments list */\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++)\n\t\tdiscard_dirty_segmap(sbi, i);\n\n\tdestroy_victim_secmap(sbi);\n\tSM_I(sbi)->dirty_info = NULL;\n\tkfree(dirty_i);\n}\n\nstatic void destroy_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array = SM_I(sbi)->curseg_array;\n\tint i;\n\n\tif (!array)\n\t\treturn;\n\tSM_I(sbi)->curseg_array = NULL;\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tkfree(array[i].sum_blk);\n\t\tkfree(array[i].journal);\n\t}\n\tkfree(array);\n}\n\nstatic void destroy_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i = SM_I(sbi)->free_info;\n\tif (!free_i)\n\t\treturn;\n\tSM_I(sbi)->free_info = NULL;\n\tkvfree(free_i->free_segmap);\n\tkvfree(free_i->free_secmap);\n\tkfree(free_i);\n}\n\nstatic void destroy_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int start;\n\n\tif (!sit_i)\n\t\treturn;\n\n\tif (sit_i->sentries) {\n\t\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\t\tkfree(sit_i->sentries[start].cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tkfree(sit_i->sentries[start].cur_valid_map_mir);\n#endif\n\t\t\tkfree(sit_i->sentries[start].ckpt_valid_map);\n\t\t\tkfree(sit_i->sentries[start].discard_map);\n\t\t}\n\t}\n\tkfree(sit_i->tmp_map);\n\n\tkvfree(sit_i->sentries);\n\tkvfree(sit_i->sec_entries);\n\tkvfree(sit_i->dirty_sentries_bitmap);\n\n\tSM_I(sbi)->sit_info = NULL;\n\tkfree(sit_i->sit_bitmap);\n#ifdef CONFIG_F2FS_CHECK_FS\n\tkfree(sit_i->sit_bitmap_mir);\n#endif\n\tkfree(sit_i);\n}\n\nvoid destroy_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\n\tif (!sm_info)\n\t\treturn;\n\tdestroy_flush_cmd_control(sbi, true);\n\tdestroy_discard_cmd_control(sbi);\n\tdestroy_dirty_segmap(sbi);\n\tdestroy_curseg(sbi);\n\tdestroy_free_segmap(sbi);\n\tdestroy_sit_info(sbi);\n\tsbi->sm_info = NULL;\n\tkfree(sm_info);\n}\n\nint __init create_segment_manager_caches(void)\n{\n\tdiscard_entry_slab = f2fs_kmem_cache_create(\"discard_entry\",\n\t\t\tsizeof(struct discard_entry));\n\tif (!discard_entry_slab)\n\t\tgoto fail;\n\n\tdiscard_cmd_slab = f2fs_kmem_cache_create(\"discard_cmd\",\n\t\t\tsizeof(struct discard_cmd));\n\tif (!discard_cmd_slab)\n\t\tgoto destroy_discard_entry;\n\n\tsit_entry_set_slab = f2fs_kmem_cache_create(\"sit_entry_set\",\n\t\t\tsizeof(struct sit_entry_set));\n\tif (!sit_entry_set_slab)\n\t\tgoto destroy_discard_cmd;\n\n\tinmem_entry_slab = f2fs_kmem_cache_create(\"inmem_page_entry\",\n\t\t\tsizeof(struct inmem_pages));\n\tif (!inmem_entry_slab)\n\t\tgoto destroy_sit_entry_set;\n\treturn 0;\n\ndestroy_sit_entry_set:\n\tkmem_cache_destroy(sit_entry_set_slab);\ndestroy_discard_cmd:\n\tkmem_cache_destroy(discard_cmd_slab);\ndestroy_discard_entry:\n\tkmem_cache_destroy(discard_entry_slab);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid destroy_segment_manager_caches(void)\n{\n\tkmem_cache_destroy(sit_entry_set_slab);\n\tkmem_cache_destroy(discard_cmd_slab);\n\tkmem_cache_destroy(discard_entry_slab);\n\tkmem_cache_destroy(inmem_entry_slab);\n}\n"], "filenames": ["fs/f2fs/segment.c"], "buggy_code_start_loc": [568], "buggy_code_end_loc": [3244], "fixing_code_start_loc": [569], "fixing_code_end_loc": [3247], "type": "CWE-476", "message": "fs/f2fs/segment.c in the Linux kernel before 4.13 allows local users to cause a denial of service (NULL pointer dereference and panic) by using a noflush_merge option that triggers a NULL value for a flush_cmd_control data structure.", "other": {"cve": {"id": "CVE-2017-18241", "sourceIdentifier": "cve@mitre.org", "published": "2018-03-21T16:29:00.200", "lastModified": "2019-03-18T15:20:53.747", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "fs/f2fs/segment.c in the Linux kernel before 4.13 allows local users to cause a denial of service (NULL pointer dereference and panic) by using a noflush_merge option that triggers a NULL value for a flush_cmd_control data structure."}, {"lang": "es", "value": "fs/f2fs/segment.c en el kernel de Linux, en versiones anteriores a la 4.13, permite que usuarios locales provoquen una denegaci\u00f3n de servicio (desreferencia de puntero NULL y p\u00e1nico) mediante el uso de una opci\u00f3n noflush_merge que desencadena un valor NULL para una estructura de datos flush_cmd_control."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.13", "matchCriteriaId": "D2894F22-5448-4402-AE27-6E43BD08E14E"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=d4fdf8ba0e5808ba9ad6b44337783bd9935e0982", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/d4fdf8ba0e5808ba9ad6b44337783bd9935e0982", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3910-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3910-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4187", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4188", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/d4fdf8ba0e5808ba9ad6b44337783bd9935e0982"}}