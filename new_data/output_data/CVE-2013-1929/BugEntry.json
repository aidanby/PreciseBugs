{"buggy_code": ["/*\n * tg3.c: Broadcom Tigon3 ethernet driver.\n *\n * Copyright (C) 2001, 2002, 2003, 2004 David S. Miller (davem@redhat.com)\n * Copyright (C) 2001, 2002, 2003 Jeff Garzik (jgarzik@pobox.com)\n * Copyright (C) 2004 Sun Microsystems Inc.\n * Copyright (C) 2005-2013 Broadcom Corporation.\n *\n * Firmware is:\n *\tDerived from proprietary unpublished source code,\n *\tCopyright (C) 2000-2003 Broadcom Corporation.\n *\n *\tPermission is hereby granted for the distribution of this firmware\n *\tdata in hexadecimal or equivalent format, provided this copyright\n *\tnotice is accompanying it.\n */\n\n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/stringify.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/in.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/ioport.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/ethtool.h>\n#include <linux/mdio.h>\n#include <linux/mii.h>\n#include <linux/phy.h>\n#include <linux/brcmphy.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/workqueue.h>\n#include <linux/prefetch.h>\n#include <linux/dma-mapping.h>\n#include <linux/firmware.h>\n#include <linux/ssb/ssb_driver_gige.h>\n#include <linux/hwmon.h>\n#include <linux/hwmon-sysfs.h>\n\n#include <net/checksum.h>\n#include <net/ip.h>\n\n#include <linux/io.h>\n#include <asm/byteorder.h>\n#include <linux/uaccess.h>\n\n#include <uapi/linux/net_tstamp.h>\n#include <linux/ptp_clock_kernel.h>\n\n#ifdef CONFIG_SPARC\n#include <asm/idprom.h>\n#include <asm/prom.h>\n#endif\n\n#define BAR_0\t0\n#define BAR_2\t2\n\n#include \"tg3.h\"\n\n/* Functions & macros to verify TG3_FLAGS types */\n\nstatic inline int _tg3_flag(enum TG3_FLAGS flag, unsigned long *bits)\n{\n\treturn test_bit(flag, bits);\n}\n\nstatic inline void _tg3_flag_set(enum TG3_FLAGS flag, unsigned long *bits)\n{\n\tset_bit(flag, bits);\n}\n\nstatic inline void _tg3_flag_clear(enum TG3_FLAGS flag, unsigned long *bits)\n{\n\tclear_bit(flag, bits);\n}\n\n#define tg3_flag(tp, flag)\t\t\t\t\\\n\t_tg3_flag(TG3_FLAG_##flag, (tp)->tg3_flags)\n#define tg3_flag_set(tp, flag)\t\t\t\t\\\n\t_tg3_flag_set(TG3_FLAG_##flag, (tp)->tg3_flags)\n#define tg3_flag_clear(tp, flag)\t\t\t\\\n\t_tg3_flag_clear(TG3_FLAG_##flag, (tp)->tg3_flags)\n\n#define DRV_MODULE_NAME\t\t\"tg3\"\n#define TG3_MAJ_NUM\t\t\t3\n#define TG3_MIN_NUM\t\t\t130\n#define DRV_MODULE_VERSION\t\\\n\t__stringify(TG3_MAJ_NUM) \".\" __stringify(TG3_MIN_NUM)\n#define DRV_MODULE_RELDATE\t\"February 14, 2013\"\n\n#define RESET_KIND_SHUTDOWN\t0\n#define RESET_KIND_INIT\t\t1\n#define RESET_KIND_SUSPEND\t2\n\n#define TG3_DEF_RX_MODE\t\t0\n#define TG3_DEF_TX_MODE\t\t0\n#define TG3_DEF_MSG_ENABLE\t  \\\n\t(NETIF_MSG_DRV\t\t| \\\n\t NETIF_MSG_PROBE\t| \\\n\t NETIF_MSG_LINK\t\t| \\\n\t NETIF_MSG_TIMER\t| \\\n\t NETIF_MSG_IFDOWN\t| \\\n\t NETIF_MSG_IFUP\t\t| \\\n\t NETIF_MSG_RX_ERR\t| \\\n\t NETIF_MSG_TX_ERR)\n\n#define TG3_GRC_LCLCTL_PWRSW_DELAY\t100\n\n/* length of time before we decide the hardware is borked,\n * and dev->tx_timeout() should be called to fix the problem\n */\n\n#define TG3_TX_TIMEOUT\t\t\t(5 * HZ)\n\n/* hardware minimum and maximum for a single frame's data payload */\n#define TG3_MIN_MTU\t\t\t60\n#define TG3_MAX_MTU(tp)\t\\\n\t(tg3_flag(tp, JUMBO_CAPABLE) ? 9000 : 1500)\n\n/* These numbers seem to be hard coded in the NIC firmware somehow.\n * You can't change the ring sizes, but you can change where you place\n * them in the NIC onboard memory.\n */\n#define TG3_RX_STD_RING_SIZE(tp) \\\n\t(tg3_flag(tp, LRG_PROD_RING_CAP) ? \\\n\t TG3_RX_STD_MAX_SIZE_5717 : TG3_RX_STD_MAX_SIZE_5700)\n#define TG3_DEF_RX_RING_PENDING\t\t200\n#define TG3_RX_JMB_RING_SIZE(tp) \\\n\t(tg3_flag(tp, LRG_PROD_RING_CAP) ? \\\n\t TG3_RX_JMB_MAX_SIZE_5717 : TG3_RX_JMB_MAX_SIZE_5700)\n#define TG3_DEF_RX_JUMBO_RING_PENDING\t100\n\n/* Do not place this n-ring entries value into the tp struct itself,\n * we really want to expose these constants to GCC so that modulo et\n * al.  operations are done with shifts and masks instead of with\n * hw multiply/modulo instructions.  Another solution would be to\n * replace things like '% foo' with '& (foo - 1)'.\n */\n\n#define TG3_TX_RING_SIZE\t\t512\n#define TG3_DEF_TX_RING_PENDING\t\t(TG3_TX_RING_SIZE - 1)\n\n#define TG3_RX_STD_RING_BYTES(tp) \\\n\t(sizeof(struct tg3_rx_buffer_desc) * TG3_RX_STD_RING_SIZE(tp))\n#define TG3_RX_JMB_RING_BYTES(tp) \\\n\t(sizeof(struct tg3_ext_rx_buffer_desc) * TG3_RX_JMB_RING_SIZE(tp))\n#define TG3_RX_RCB_RING_BYTES(tp) \\\n\t(sizeof(struct tg3_rx_buffer_desc) * (tp->rx_ret_ring_mask + 1))\n#define TG3_TX_RING_BYTES\t(sizeof(struct tg3_tx_buffer_desc) * \\\n\t\t\t\t TG3_TX_RING_SIZE)\n#define NEXT_TX(N)\t\t(((N) + 1) & (TG3_TX_RING_SIZE - 1))\n\n#define TG3_DMA_BYTE_ENAB\t\t64\n\n#define TG3_RX_STD_DMA_SZ\t\t1536\n#define TG3_RX_JMB_DMA_SZ\t\t9046\n\n#define TG3_RX_DMA_TO_MAP_SZ(x)\t\t((x) + TG3_DMA_BYTE_ENAB)\n\n#define TG3_RX_STD_MAP_SZ\t\tTG3_RX_DMA_TO_MAP_SZ(TG3_RX_STD_DMA_SZ)\n#define TG3_RX_JMB_MAP_SZ\t\tTG3_RX_DMA_TO_MAP_SZ(TG3_RX_JMB_DMA_SZ)\n\n#define TG3_RX_STD_BUFF_RING_SIZE(tp) \\\n\t(sizeof(struct ring_info) * TG3_RX_STD_RING_SIZE(tp))\n\n#define TG3_RX_JMB_BUFF_RING_SIZE(tp) \\\n\t(sizeof(struct ring_info) * TG3_RX_JMB_RING_SIZE(tp))\n\n/* Due to a hardware bug, the 5701 can only DMA to memory addresses\n * that are at least dword aligned when used in PCIX mode.  The driver\n * works around this bug by double copying the packet.  This workaround\n * is built into the normal double copy length check for efficiency.\n *\n * However, the double copy is only necessary on those architectures\n * where unaligned memory accesses are inefficient.  For those architectures\n * where unaligned memory accesses incur little penalty, we can reintegrate\n * the 5701 in the normal rx path.  Doing so saves a device structure\n * dereference by hardcoding the double copy threshold in place.\n */\n#define TG3_RX_COPY_THRESHOLD\t\t256\n#if NET_IP_ALIGN == 0 || defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\t#define TG3_RX_COPY_THRESH(tp)\tTG3_RX_COPY_THRESHOLD\n#else\n\t#define TG3_RX_COPY_THRESH(tp)\t((tp)->rx_copy_thresh)\n#endif\n\n#if (NET_IP_ALIGN != 0)\n#define TG3_RX_OFFSET(tp)\t((tp)->rx_offset)\n#else\n#define TG3_RX_OFFSET(tp)\t(NET_SKB_PAD)\n#endif\n\n/* minimum number of free TX descriptors required to wake up TX process */\n#define TG3_TX_WAKEUP_THRESH(tnapi)\t\t((tnapi)->tx_pending / 4)\n#define TG3_TX_BD_DMA_MAX_2K\t\t2048\n#define TG3_TX_BD_DMA_MAX_4K\t\t4096\n\n#define TG3_RAW_IP_ALIGN 2\n\n#define TG3_FW_UPDATE_TIMEOUT_SEC\t5\n#define TG3_FW_UPDATE_FREQ_SEC\t\t(TG3_FW_UPDATE_TIMEOUT_SEC / 2)\n\n#define FIRMWARE_TG3\t\t\"tigon/tg3.bin\"\n#define FIRMWARE_TG3TSO\t\t\"tigon/tg3_tso.bin\"\n#define FIRMWARE_TG3TSO5\t\"tigon/tg3_tso5.bin\"\n\nstatic char version[] =\n\tDRV_MODULE_NAME \".c:v\" DRV_MODULE_VERSION \" (\" DRV_MODULE_RELDATE \")\";\n\nMODULE_AUTHOR(\"David S. Miller (davem@redhat.com) and Jeff Garzik (jgarzik@pobox.com)\");\nMODULE_DESCRIPTION(\"Broadcom Tigon3 ethernet driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(DRV_MODULE_VERSION);\nMODULE_FIRMWARE(FIRMWARE_TG3);\nMODULE_FIRMWARE(FIRMWARE_TG3TSO);\nMODULE_FIRMWARE(FIRMWARE_TG3TSO5);\n\nstatic int tg3_debug = -1;\t/* -1 == use TG3_DEF_MSG_ENABLE as value */\nmodule_param(tg3_debug, int, 0);\nMODULE_PARM_DESC(tg3_debug, \"Tigon3 bitmapped debugging message enable value\");\n\n#define TG3_DRV_DATA_FLAG_10_100_ONLY\t0x0001\n#define TG3_DRV_DATA_FLAG_5705_10_100\t0x0002\n\nstatic DEFINE_PCI_DEVICE_TABLE(tg3_pci_tbl) = {\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5700)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5701)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5702)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5703)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5704)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5702FE)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705_2)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705M_2)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5702X)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5703X)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5704S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5702A3)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5703A3)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5782)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5788)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5789)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5901),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY |\n\t\t\tTG3_DRV_DATA_FLAG_5705_10_100},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5901_2),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY |\n\t\t\tTG3_DRV_DATA_FLAG_5705_10_100},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5704S_2)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705F),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY |\n\t\t\tTG3_DRV_DATA_FLAG_5705_10_100},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5721)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5722)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5750)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5751)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5751M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5751F),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5752)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5752M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5753)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5753M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5753F),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5754)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5754M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5755)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5755M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5756)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5786)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5787)},\n\t{PCI_DEVICE_SUB(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5787M,\n\t\t\tPCI_VENDOR_ID_LENOVO,\n\t\t\tTG3PCI_SUBDEVICE_ID_LENOVO_5787M),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5787M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5787F),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5714)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5714S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5715)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5715S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5780)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5780S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5781)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5906)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5906M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5784)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5764)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5723)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5761)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5761E)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5761S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5761SE)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5785_G)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5785_F)},\n\t{PCI_DEVICE_SUB(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57780,\n\t\t\tPCI_VENDOR_ID_AI, TG3PCI_SUBDEVICE_ID_ACER_57780_A),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE_SUB(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57780,\n\t\t\tPCI_VENDOR_ID_AI, TG3PCI_SUBDEVICE_ID_ACER_57780_B),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57780)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57760)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57790),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57788)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5717)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5717_C)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5718)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57781)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57785)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57761)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57765)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57791),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57795),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5719)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5720)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57762)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57766)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5762)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5725)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5727)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_SYSKONNECT, PCI_DEVICE_ID_SYSKONNECT_9DXX)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_SYSKONNECT, PCI_DEVICE_ID_SYSKONNECT_9MXX)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_ALTIMA, PCI_DEVICE_ID_ALTIMA_AC1000)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_ALTIMA, PCI_DEVICE_ID_ALTIMA_AC1001)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_ALTIMA, PCI_DEVICE_ID_ALTIMA_AC1003)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_ALTIMA, PCI_DEVICE_ID_ALTIMA_AC9100)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_TIGON3)},\n\t{PCI_DEVICE(0x10cf, 0x11a2)}, /* Fujitsu 1000base-SX with BCM5703SKHB */\n\t{}\n};\n\nMODULE_DEVICE_TABLE(pci, tg3_pci_tbl);\n\nstatic const struct {\n\tconst char string[ETH_GSTRING_LEN];\n} ethtool_stats_keys[] = {\n\t{ \"rx_octets\" },\n\t{ \"rx_fragments\" },\n\t{ \"rx_ucast_packets\" },\n\t{ \"rx_mcast_packets\" },\n\t{ \"rx_bcast_packets\" },\n\t{ \"rx_fcs_errors\" },\n\t{ \"rx_align_errors\" },\n\t{ \"rx_xon_pause_rcvd\" },\n\t{ \"rx_xoff_pause_rcvd\" },\n\t{ \"rx_mac_ctrl_rcvd\" },\n\t{ \"rx_xoff_entered\" },\n\t{ \"rx_frame_too_long_errors\" },\n\t{ \"rx_jabbers\" },\n\t{ \"rx_undersize_packets\" },\n\t{ \"rx_in_length_errors\" },\n\t{ \"rx_out_length_errors\" },\n\t{ \"rx_64_or_less_octet_packets\" },\n\t{ \"rx_65_to_127_octet_packets\" },\n\t{ \"rx_128_to_255_octet_packets\" },\n\t{ \"rx_256_to_511_octet_packets\" },\n\t{ \"rx_512_to_1023_octet_packets\" },\n\t{ \"rx_1024_to_1522_octet_packets\" },\n\t{ \"rx_1523_to_2047_octet_packets\" },\n\t{ \"rx_2048_to_4095_octet_packets\" },\n\t{ \"rx_4096_to_8191_octet_packets\" },\n\t{ \"rx_8192_to_9022_octet_packets\" },\n\n\t{ \"tx_octets\" },\n\t{ \"tx_collisions\" },\n\n\t{ \"tx_xon_sent\" },\n\t{ \"tx_xoff_sent\" },\n\t{ \"tx_flow_control\" },\n\t{ \"tx_mac_errors\" },\n\t{ \"tx_single_collisions\" },\n\t{ \"tx_mult_collisions\" },\n\t{ \"tx_deferred\" },\n\t{ \"tx_excessive_collisions\" },\n\t{ \"tx_late_collisions\" },\n\t{ \"tx_collide_2times\" },\n\t{ \"tx_collide_3times\" },\n\t{ \"tx_collide_4times\" },\n\t{ \"tx_collide_5times\" },\n\t{ \"tx_collide_6times\" },\n\t{ \"tx_collide_7times\" },\n\t{ \"tx_collide_8times\" },\n\t{ \"tx_collide_9times\" },\n\t{ \"tx_collide_10times\" },\n\t{ \"tx_collide_11times\" },\n\t{ \"tx_collide_12times\" },\n\t{ \"tx_collide_13times\" },\n\t{ \"tx_collide_14times\" },\n\t{ \"tx_collide_15times\" },\n\t{ \"tx_ucast_packets\" },\n\t{ \"tx_mcast_packets\" },\n\t{ \"tx_bcast_packets\" },\n\t{ \"tx_carrier_sense_errors\" },\n\t{ \"tx_discards\" },\n\t{ \"tx_errors\" },\n\n\t{ \"dma_writeq_full\" },\n\t{ \"dma_write_prioq_full\" },\n\t{ \"rxbds_empty\" },\n\t{ \"rx_discards\" },\n\t{ \"rx_errors\" },\n\t{ \"rx_threshold_hit\" },\n\n\t{ \"dma_readq_full\" },\n\t{ \"dma_read_prioq_full\" },\n\t{ \"tx_comp_queue_full\" },\n\n\t{ \"ring_set_send_prod_index\" },\n\t{ \"ring_status_update\" },\n\t{ \"nic_irqs\" },\n\t{ \"nic_avoided_irqs\" },\n\t{ \"nic_tx_threshold_hit\" },\n\n\t{ \"mbuf_lwm_thresh_hit\" },\n};\n\n#define TG3_NUM_STATS\tARRAY_SIZE(ethtool_stats_keys)\n#define TG3_NVRAM_TEST\t\t0\n#define TG3_LINK_TEST\t\t1\n#define TG3_REGISTER_TEST\t2\n#define TG3_MEMORY_TEST\t\t3\n#define TG3_MAC_LOOPB_TEST\t4\n#define TG3_PHY_LOOPB_TEST\t5\n#define TG3_EXT_LOOPB_TEST\t6\n#define TG3_INTERRUPT_TEST\t7\n\n\nstatic const struct {\n\tconst char string[ETH_GSTRING_LEN];\n} ethtool_test_keys[] = {\n\t[TG3_NVRAM_TEST]\t= { \"nvram test        (online) \" },\n\t[TG3_LINK_TEST]\t\t= { \"link test         (online) \" },\n\t[TG3_REGISTER_TEST]\t= { \"register test     (offline)\" },\n\t[TG3_MEMORY_TEST]\t= { \"memory test       (offline)\" },\n\t[TG3_MAC_LOOPB_TEST]\t= { \"mac loopback test (offline)\" },\n\t[TG3_PHY_LOOPB_TEST]\t= { \"phy loopback test (offline)\" },\n\t[TG3_EXT_LOOPB_TEST]\t= { \"ext loopback test (offline)\" },\n\t[TG3_INTERRUPT_TEST]\t= { \"interrupt test    (offline)\" },\n};\n\n#define TG3_NUM_TEST\tARRAY_SIZE(ethtool_test_keys)\n\n\nstatic void tg3_write32(struct tg3 *tp, u32 off, u32 val)\n{\n\twritel(val, tp->regs + off);\n}\n\nstatic u32 tg3_read32(struct tg3 *tp, u32 off)\n{\n\treturn readl(tp->regs + off);\n}\n\nstatic void tg3_ape_write32(struct tg3 *tp, u32 off, u32 val)\n{\n\twritel(val, tp->aperegs + off);\n}\n\nstatic u32 tg3_ape_read32(struct tg3 *tp, u32 off)\n{\n\treturn readl(tp->aperegs + off);\n}\n\nstatic void tg3_write_indirect_reg32(struct tg3 *tp, u32 off, u32 val)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_BASE_ADDR, off);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_DATA, val);\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n}\n\nstatic void tg3_write_flush_reg32(struct tg3 *tp, u32 off, u32 val)\n{\n\twritel(val, tp->regs + off);\n\treadl(tp->regs + off);\n}\n\nstatic u32 tg3_read_indirect_reg32(struct tg3 *tp, u32 off)\n{\n\tunsigned long flags;\n\tu32 val;\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_BASE_ADDR, off);\n\tpci_read_config_dword(tp->pdev, TG3PCI_REG_DATA, &val);\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n\treturn val;\n}\n\nstatic void tg3_write_indirect_mbox(struct tg3 *tp, u32 off, u32 val)\n{\n\tunsigned long flags;\n\n\tif (off == (MAILBOX_RCVRET_CON_IDX_0 + TG3_64BIT_REG_LOW)) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_RCV_RET_RING_CON_IDX +\n\t\t\t\t       TG3_64BIT_REG_LOW, val);\n\t\treturn;\n\t}\n\tif (off == TG3_RX_STD_PROD_IDX_REG) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_STD_RING_PROD_IDX +\n\t\t\t\t       TG3_64BIT_REG_LOW, val);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_BASE_ADDR, off + 0x5600);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_DATA, val);\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n\n\t/* In indirect mode when disabling interrupts, we also need\n\t * to clear the interrupt bit in the GRC local ctrl register.\n\t */\n\tif ((off == (MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW)) &&\n\t    (val == 0x1)) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MISC_LOCAL_CTRL,\n\t\t\t\t       tp->grc_local_ctrl|GRC_LCLCTRL_CLEARINT);\n\t}\n}\n\nstatic u32 tg3_read_indirect_mbox(struct tg3 *tp, u32 off)\n{\n\tunsigned long flags;\n\tu32 val;\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_BASE_ADDR, off + 0x5600);\n\tpci_read_config_dword(tp->pdev, TG3PCI_REG_DATA, &val);\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n\treturn val;\n}\n\n/* usec_wait specifies the wait time in usec when writing to certain registers\n * where it is unsafe to read back the register without some delay.\n * GRC_LOCAL_CTRL is one example if the GPIOs are toggled to switch power.\n * TG3PCI_CLOCK_CTRL is another example if the clock frequencies are changed.\n */\nstatic void _tw32_flush(struct tg3 *tp, u32 off, u32 val, u32 usec_wait)\n{\n\tif (tg3_flag(tp, PCIX_TARGET_HWBUG) || tg3_flag(tp, ICH_WORKAROUND))\n\t\t/* Non-posted methods */\n\t\ttp->write32(tp, off, val);\n\telse {\n\t\t/* Posted method */\n\t\ttg3_write32(tp, off, val);\n\t\tif (usec_wait)\n\t\t\tudelay(usec_wait);\n\t\ttp->read32(tp, off);\n\t}\n\t/* Wait again after the read for the posted method to guarantee that\n\t * the wait time is met.\n\t */\n\tif (usec_wait)\n\t\tudelay(usec_wait);\n}\n\nstatic inline void tw32_mailbox_flush(struct tg3 *tp, u32 off, u32 val)\n{\n\ttp->write32_mbox(tp, off, val);\n\tif (tg3_flag(tp, FLUSH_POSTED_WRITES) ||\n\t    (!tg3_flag(tp, MBOX_WRITE_REORDER) &&\n\t     !tg3_flag(tp, ICH_WORKAROUND)))\n\t\ttp->read32_mbox(tp, off);\n}\n\nstatic void tg3_write32_tx_mbox(struct tg3 *tp, u32 off, u32 val)\n{\n\tvoid __iomem *mbox = tp->regs + off;\n\twritel(val, mbox);\n\tif (tg3_flag(tp, TXD_MBOX_HWBUG))\n\t\twritel(val, mbox);\n\tif (tg3_flag(tp, MBOX_WRITE_REORDER) ||\n\t    tg3_flag(tp, FLUSH_POSTED_WRITES))\n\t\treadl(mbox);\n}\n\nstatic u32 tg3_read32_mbox_5906(struct tg3 *tp, u32 off)\n{\n\treturn readl(tp->regs + off + GRCMBOX_BASE);\n}\n\nstatic void tg3_write32_mbox_5906(struct tg3 *tp, u32 off, u32 val)\n{\n\twritel(val, tp->regs + off + GRCMBOX_BASE);\n}\n\n#define tw32_mailbox(reg, val)\t\ttp->write32_mbox(tp, reg, val)\n#define tw32_mailbox_f(reg, val)\ttw32_mailbox_flush(tp, (reg), (val))\n#define tw32_rx_mbox(reg, val)\t\ttp->write32_rx_mbox(tp, reg, val)\n#define tw32_tx_mbox(reg, val)\t\ttp->write32_tx_mbox(tp, reg, val)\n#define tr32_mailbox(reg)\t\ttp->read32_mbox(tp, reg)\n\n#define tw32(reg, val)\t\t\ttp->write32(tp, reg, val)\n#define tw32_f(reg, val)\t\t_tw32_flush(tp, (reg), (val), 0)\n#define tw32_wait_f(reg, val, us)\t_tw32_flush(tp, (reg), (val), (us))\n#define tr32(reg)\t\t\ttp->read32(tp, reg)\n\nstatic void tg3_write_mem(struct tg3 *tp, u32 off, u32 val)\n{\n\tunsigned long flags;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906 &&\n\t    (off >= NIC_SRAM_STATS_BLK) && (off < NIC_SRAM_TX_BUFFER_DESC))\n\t\treturn;\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tif (tg3_flag(tp, SRAM_USE_CONFIG)) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, off);\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_DATA, val);\n\n\t\t/* Always leave this as zero. */\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\t} else {\n\t\ttw32_f(TG3PCI_MEM_WIN_BASE_ADDR, off);\n\t\ttw32_f(TG3PCI_MEM_WIN_DATA, val);\n\n\t\t/* Always leave this as zero. */\n\t\ttw32_f(TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\t}\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n}\n\nstatic void tg3_read_mem(struct tg3 *tp, u32 off, u32 *val)\n{\n\tunsigned long flags;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906 &&\n\t    (off >= NIC_SRAM_STATS_BLK) && (off < NIC_SRAM_TX_BUFFER_DESC)) {\n\t\t*val = 0;\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tif (tg3_flag(tp, SRAM_USE_CONFIG)) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, off);\n\t\tpci_read_config_dword(tp->pdev, TG3PCI_MEM_WIN_DATA, val);\n\n\t\t/* Always leave this as zero. */\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\t} else {\n\t\ttw32_f(TG3PCI_MEM_WIN_BASE_ADDR, off);\n\t\t*val = tr32(TG3PCI_MEM_WIN_DATA);\n\n\t\t/* Always leave this as zero. */\n\t\ttw32_f(TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\t}\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n}\n\nstatic void tg3_ape_lock_init(struct tg3 *tp)\n{\n\tint i;\n\tu32 regbase, bit;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\tregbase = TG3_APE_LOCK_GRANT;\n\telse\n\t\tregbase = TG3_APE_PER_LOCK_GRANT;\n\n\t/* Make sure the driver hasn't any stale locks. */\n\tfor (i = TG3_APE_LOCK_PHY0; i <= TG3_APE_LOCK_GPIO; i++) {\n\t\tswitch (i) {\n\t\tcase TG3_APE_LOCK_PHY0:\n\t\tcase TG3_APE_LOCK_PHY1:\n\t\tcase TG3_APE_LOCK_PHY2:\n\t\tcase TG3_APE_LOCK_PHY3:\n\t\t\tbit = APE_LOCK_GRANT_DRIVER;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (!tp->pci_fn)\n\t\t\t\tbit = APE_LOCK_GRANT_DRIVER;\n\t\t\telse\n\t\t\t\tbit = 1 << tp->pci_fn;\n\t\t}\n\t\ttg3_ape_write32(tp, regbase + 4 * i, bit);\n\t}\n\n}\n\nstatic int tg3_ape_lock(struct tg3 *tp, int locknum)\n{\n\tint i, off;\n\tint ret = 0;\n\tu32 status, req, gnt, bit;\n\n\tif (!tg3_flag(tp, ENABLE_APE))\n\t\treturn 0;\n\n\tswitch (locknum) {\n\tcase TG3_APE_LOCK_GPIO:\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\t\treturn 0;\n\tcase TG3_APE_LOCK_GRC:\n\tcase TG3_APE_LOCK_MEM:\n\t\tif (!tp->pci_fn)\n\t\t\tbit = APE_LOCK_REQ_DRIVER;\n\t\telse\n\t\t\tbit = 1 << tp->pci_fn;\n\t\tbreak;\n\tcase TG3_APE_LOCK_PHY0:\n\tcase TG3_APE_LOCK_PHY1:\n\tcase TG3_APE_LOCK_PHY2:\n\tcase TG3_APE_LOCK_PHY3:\n\t\tbit = APE_LOCK_REQ_DRIVER;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761) {\n\t\treq = TG3_APE_LOCK_REQ;\n\t\tgnt = TG3_APE_LOCK_GRANT;\n\t} else {\n\t\treq = TG3_APE_PER_LOCK_REQ;\n\t\tgnt = TG3_APE_PER_LOCK_GRANT;\n\t}\n\n\toff = 4 * locknum;\n\n\ttg3_ape_write32(tp, req + off, bit);\n\n\t/* Wait for up to 1 millisecond to acquire lock. */\n\tfor (i = 0; i < 100; i++) {\n\t\tstatus = tg3_ape_read32(tp, gnt + off);\n\t\tif (status == bit)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\tif (status != bit) {\n\t\t/* Revoke the lock request. */\n\t\ttg3_ape_write32(tp, gnt + off, bit);\n\t\tret = -EBUSY;\n\t}\n\n\treturn ret;\n}\n\nstatic void tg3_ape_unlock(struct tg3 *tp, int locknum)\n{\n\tu32 gnt, bit;\n\n\tif (!tg3_flag(tp, ENABLE_APE))\n\t\treturn;\n\n\tswitch (locknum) {\n\tcase TG3_APE_LOCK_GPIO:\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\t\treturn;\n\tcase TG3_APE_LOCK_GRC:\n\tcase TG3_APE_LOCK_MEM:\n\t\tif (!tp->pci_fn)\n\t\t\tbit = APE_LOCK_GRANT_DRIVER;\n\t\telse\n\t\t\tbit = 1 << tp->pci_fn;\n\t\tbreak;\n\tcase TG3_APE_LOCK_PHY0:\n\tcase TG3_APE_LOCK_PHY1:\n\tcase TG3_APE_LOCK_PHY2:\n\tcase TG3_APE_LOCK_PHY3:\n\t\tbit = APE_LOCK_GRANT_DRIVER;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\tgnt = TG3_APE_LOCK_GRANT;\n\telse\n\t\tgnt = TG3_APE_PER_LOCK_GRANT;\n\n\ttg3_ape_write32(tp, gnt + 4 * locknum, bit);\n}\n\nstatic int tg3_ape_event_lock(struct tg3 *tp, u32 timeout_us)\n{\n\tu32 apedata;\n\n\twhile (timeout_us) {\n\t\tif (tg3_ape_lock(tp, TG3_APE_LOCK_MEM))\n\t\t\treturn -EBUSY;\n\n\t\tapedata = tg3_ape_read32(tp, TG3_APE_EVENT_STATUS);\n\t\tif (!(apedata & APE_EVENT_STATUS_EVENT_PENDING))\n\t\t\tbreak;\n\n\t\ttg3_ape_unlock(tp, TG3_APE_LOCK_MEM);\n\n\t\tudelay(10);\n\t\ttimeout_us -= (timeout_us > 10) ? 10 : timeout_us;\n\t}\n\n\treturn timeout_us ? 0 : -EBUSY;\n}\n\nstatic int tg3_ape_wait_for_event(struct tg3 *tp, u32 timeout_us)\n{\n\tu32 i, apedata;\n\n\tfor (i = 0; i < timeout_us / 10; i++) {\n\t\tapedata = tg3_ape_read32(tp, TG3_APE_EVENT_STATUS);\n\n\t\tif (!(apedata & APE_EVENT_STATUS_EVENT_PENDING))\n\t\t\tbreak;\n\n\t\tudelay(10);\n\t}\n\n\treturn i == timeout_us / 10;\n}\n\nstatic int tg3_ape_scratchpad_read(struct tg3 *tp, u32 *data, u32 base_off,\n\t\t\t\t   u32 len)\n{\n\tint err;\n\tu32 i, bufoff, msgoff, maxlen, apedata;\n\n\tif (!tg3_flag(tp, APE_HAS_NCSI))\n\t\treturn 0;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_SEG_SIG);\n\tif (apedata != APE_SEG_SIG_MAGIC)\n\t\treturn -ENODEV;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_FW_STATUS);\n\tif (!(apedata & APE_FW_STATUS_READY))\n\t\treturn -EAGAIN;\n\n\tbufoff = tg3_ape_read32(tp, TG3_APE_SEG_MSG_BUF_OFF) +\n\t\t TG3_APE_SHMEM_BASE;\n\tmsgoff = bufoff + 2 * sizeof(u32);\n\tmaxlen = tg3_ape_read32(tp, TG3_APE_SEG_MSG_BUF_LEN);\n\n\twhile (len) {\n\t\tu32 length;\n\n\t\t/* Cap xfer sizes to scratchpad limits. */\n\t\tlength = (len > maxlen) ? maxlen : len;\n\t\tlen -= length;\n\n\t\tapedata = tg3_ape_read32(tp, TG3_APE_FW_STATUS);\n\t\tif (!(apedata & APE_FW_STATUS_READY))\n\t\t\treturn -EAGAIN;\n\n\t\t/* Wait for up to 1 msec for APE to service previous event. */\n\t\terr = tg3_ape_event_lock(tp, 1000);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tapedata = APE_EVENT_STATUS_DRIVER_EVNT |\n\t\t\t  APE_EVENT_STATUS_SCRTCHPD_READ |\n\t\t\t  APE_EVENT_STATUS_EVENT_PENDING;\n\t\ttg3_ape_write32(tp, TG3_APE_EVENT_STATUS, apedata);\n\n\t\ttg3_ape_write32(tp, bufoff, base_off);\n\t\ttg3_ape_write32(tp, bufoff + sizeof(u32), length);\n\n\t\ttg3_ape_unlock(tp, TG3_APE_LOCK_MEM);\n\t\ttg3_ape_write32(tp, TG3_APE_EVENT, APE_EVENT_1);\n\n\t\tbase_off += length;\n\n\t\tif (tg3_ape_wait_for_event(tp, 30000))\n\t\t\treturn -EAGAIN;\n\n\t\tfor (i = 0; length; i += 4, length -= 4) {\n\t\t\tu32 val = tg3_ape_read32(tp, msgoff + i);\n\t\t\tmemcpy(data, &val, sizeof(u32));\n\t\t\tdata++;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int tg3_ape_send_event(struct tg3 *tp, u32 event)\n{\n\tint err;\n\tu32 apedata;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_SEG_SIG);\n\tif (apedata != APE_SEG_SIG_MAGIC)\n\t\treturn -EAGAIN;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_FW_STATUS);\n\tif (!(apedata & APE_FW_STATUS_READY))\n\t\treturn -EAGAIN;\n\n\t/* Wait for up to 1 millisecond for APE to service previous event. */\n\terr = tg3_ape_event_lock(tp, 1000);\n\tif (err)\n\t\treturn err;\n\n\ttg3_ape_write32(tp, TG3_APE_EVENT_STATUS,\n\t\t\tevent | APE_EVENT_STATUS_EVENT_PENDING);\n\n\ttg3_ape_unlock(tp, TG3_APE_LOCK_MEM);\n\ttg3_ape_write32(tp, TG3_APE_EVENT, APE_EVENT_1);\n\n\treturn 0;\n}\n\nstatic void tg3_ape_driver_state_change(struct tg3 *tp, int kind)\n{\n\tu32 event;\n\tu32 apedata;\n\n\tif (!tg3_flag(tp, ENABLE_APE))\n\t\treturn;\n\n\tswitch (kind) {\n\tcase RESET_KIND_INIT:\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_SEG_SIG,\n\t\t\t\tAPE_HOST_SEG_SIG_MAGIC);\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_SEG_LEN,\n\t\t\t\tAPE_HOST_SEG_LEN_MAGIC);\n\t\tapedata = tg3_ape_read32(tp, TG3_APE_HOST_INIT_COUNT);\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_INIT_COUNT, ++apedata);\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_DRIVER_ID,\n\t\t\tAPE_HOST_DRIVER_ID_MAGIC(TG3_MAJ_NUM, TG3_MIN_NUM));\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_BEHAVIOR,\n\t\t\t\tAPE_HOST_BEHAV_NO_PHYLOCK);\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_DRVR_STATE,\n\t\t\t\t    TG3_APE_HOST_DRVR_STATE_START);\n\n\t\tevent = APE_EVENT_STATUS_STATE_START;\n\t\tbreak;\n\tcase RESET_KIND_SHUTDOWN:\n\t\t/* With the interface we are currently using,\n\t\t * APE does not track driver state.  Wiping\n\t\t * out the HOST SEGMENT SIGNATURE forces\n\t\t * the APE to assume OS absent status.\n\t\t */\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_SEG_SIG, 0x0);\n\n\t\tif (device_may_wakeup(&tp->pdev->dev) &&\n\t\t    tg3_flag(tp, WOL_ENABLE)) {\n\t\t\ttg3_ape_write32(tp, TG3_APE_HOST_WOL_SPEED,\n\t\t\t\t\t    TG3_APE_HOST_WOL_SPEED_AUTO);\n\t\t\tapedata = TG3_APE_HOST_DRVR_STATE_WOL;\n\t\t} else\n\t\t\tapedata = TG3_APE_HOST_DRVR_STATE_UNLOAD;\n\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_DRVR_STATE, apedata);\n\n\t\tevent = APE_EVENT_STATUS_STATE_UNLOAD;\n\t\tbreak;\n\tcase RESET_KIND_SUSPEND:\n\t\tevent = APE_EVENT_STATUS_STATE_SUSPEND;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tevent |= APE_EVENT_STATUS_DRIVER_EVNT | APE_EVENT_STATUS_STATE_CHNGE;\n\n\ttg3_ape_send_event(tp, event);\n}\n\nstatic void tg3_disable_ints(struct tg3 *tp)\n{\n\tint i;\n\n\ttw32(TG3PCI_MISC_HOST_CTRL,\n\t     (tp->misc_host_ctrl | MISC_HOST_CTRL_MASK_PCI_INT));\n\tfor (i = 0; i < tp->irq_max; i++)\n\t\ttw32_mailbox_f(tp->napi[i].int_mbox, 0x00000001);\n}\n\nstatic void tg3_enable_ints(struct tg3 *tp)\n{\n\tint i;\n\n\ttp->irq_sync = 0;\n\twmb();\n\n\ttw32(TG3PCI_MISC_HOST_CTRL,\n\t     (tp->misc_host_ctrl & ~MISC_HOST_CTRL_MASK_PCI_INT));\n\n\ttp->coal_now = tp->coalesce_mode | HOSTCC_MODE_ENABLE;\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\ttw32_mailbox_f(tnapi->int_mbox, tnapi->last_tag << 24);\n\t\tif (tg3_flag(tp, 1SHOT_MSI))\n\t\t\ttw32_mailbox_f(tnapi->int_mbox, tnapi->last_tag << 24);\n\n\t\ttp->coal_now |= tnapi->coal_now;\n\t}\n\n\t/* Force an initial interrupt */\n\tif (!tg3_flag(tp, TAGGED_STATUS) &&\n\t    (tp->napi[0].hw_status->status & SD_STATUS_UPDATED))\n\t\ttw32(GRC_LOCAL_CTRL, tp->grc_local_ctrl | GRC_LCLCTRL_SETINT);\n\telse\n\t\ttw32(HOSTCC_MODE, tp->coal_now);\n\n\ttp->coal_now &= ~(tp->napi[0].coal_now | tp->napi[1].coal_now);\n}\n\nstatic inline unsigned int tg3_has_work(struct tg3_napi *tnapi)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\tunsigned int work_exists = 0;\n\n\t/* check for phy events */\n\tif (!(tg3_flag(tp, USE_LINKCHG_REG) || tg3_flag(tp, POLL_SERDES))) {\n\t\tif (sblk->status & SD_STATUS_LINK_CHG)\n\t\t\twork_exists = 1;\n\t}\n\n\t/* check for TX work to do */\n\tif (sblk->idx[0].tx_consumer != tnapi->tx_cons)\n\t\twork_exists = 1;\n\n\t/* check for RX work to do */\n\tif (tnapi->rx_rcb_prod_idx &&\n\t    *(tnapi->rx_rcb_prod_idx) != tnapi->rx_rcb_ptr)\n\t\twork_exists = 1;\n\n\treturn work_exists;\n}\n\n/* tg3_int_reenable\n *  similar to tg3_enable_ints, but it accurately determines whether there\n *  is new work pending and can return without flushing the PIO write\n *  which reenables interrupts\n */\nstatic void tg3_int_reenable(struct tg3_napi *tnapi)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\n\ttw32_mailbox(tnapi->int_mbox, tnapi->last_tag << 24);\n\tmmiowb();\n\n\t/* When doing tagged status, this work check is unnecessary.\n\t * The last_tag we write above tells the chip which piece of\n\t * work we've completed.\n\t */\n\tif (!tg3_flag(tp, TAGGED_STATUS) && tg3_has_work(tnapi))\n\t\ttw32(HOSTCC_MODE, tp->coalesce_mode |\n\t\t     HOSTCC_MODE_ENABLE | tnapi->coal_now);\n}\n\nstatic void tg3_switch_clocks(struct tg3 *tp)\n{\n\tu32 clock_ctrl;\n\tu32 orig_clock_ctrl;\n\n\tif (tg3_flag(tp, CPMU_PRESENT) || tg3_flag(tp, 5780_CLASS))\n\t\treturn;\n\n\tclock_ctrl = tr32(TG3PCI_CLOCK_CTRL);\n\n\torig_clock_ctrl = clock_ctrl;\n\tclock_ctrl &= (CLOCK_CTRL_FORCE_CLKRUN |\n\t\t       CLOCK_CTRL_CLKRUN_OENABLE |\n\t\t       0x1f);\n\ttp->pci_clock_ctrl = clock_ctrl;\n\n\tif (tg3_flag(tp, 5705_PLUS)) {\n\t\tif (orig_clock_ctrl & CLOCK_CTRL_625_CORE) {\n\t\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL,\n\t\t\t\t    clock_ctrl | CLOCK_CTRL_625_CORE, 40);\n\t\t}\n\t} else if ((orig_clock_ctrl & CLOCK_CTRL_44MHZ_CORE) != 0) {\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL,\n\t\t\t    clock_ctrl |\n\t\t\t    (CLOCK_CTRL_44MHZ_CORE | CLOCK_CTRL_ALTCLK),\n\t\t\t    40);\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL,\n\t\t\t    clock_ctrl | (CLOCK_CTRL_ALTCLK),\n\t\t\t    40);\n\t}\n\ttw32_wait_f(TG3PCI_CLOCK_CTRL, clock_ctrl, 40);\n}\n\n#define PHY_BUSY_LOOPS\t5000\n\nstatic int __tg3_readphy(struct tg3 *tp, unsigned int phy_addr, int reg,\n\t\t\t u32 *val)\n{\n\tu32 frame_val;\n\tunsigned int loops;\n\tint ret;\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE,\n\t\t     (tp->mi_mode & ~MAC_MI_MODE_AUTO_POLL));\n\t\tudelay(80);\n\t}\n\n\ttg3_ape_lock(tp, tp->phy_ape_lock);\n\n\t*val = 0x0;\n\n\tframe_val  = ((phy_addr << MI_COM_PHY_ADDR_SHIFT) &\n\t\t      MI_COM_PHY_ADDR_MASK);\n\tframe_val |= ((reg << MI_COM_REG_ADDR_SHIFT) &\n\t\t      MI_COM_REG_ADDR_MASK);\n\tframe_val |= (MI_COM_CMD_READ | MI_COM_START);\n\n\ttw32_f(MAC_MI_COM, frame_val);\n\n\tloops = PHY_BUSY_LOOPS;\n\twhile (loops != 0) {\n\t\tudelay(10);\n\t\tframe_val = tr32(MAC_MI_COM);\n\n\t\tif ((frame_val & MI_COM_BUSY) == 0) {\n\t\t\tudelay(5);\n\t\t\tframe_val = tr32(MAC_MI_COM);\n\t\t\tbreak;\n\t\t}\n\t\tloops -= 1;\n\t}\n\n\tret = -EBUSY;\n\tif (loops != 0) {\n\t\t*val = frame_val & MI_COM_DATA_MASK;\n\t\tret = 0;\n\t}\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE, tp->mi_mode);\n\t\tudelay(80);\n\t}\n\n\ttg3_ape_unlock(tp, tp->phy_ape_lock);\n\n\treturn ret;\n}\n\nstatic int tg3_readphy(struct tg3 *tp, int reg, u32 *val)\n{\n\treturn __tg3_readphy(tp, tp->phy_addr, reg, val);\n}\n\nstatic int __tg3_writephy(struct tg3 *tp, unsigned int phy_addr, int reg,\n\t\t\t  u32 val)\n{\n\tu32 frame_val;\n\tunsigned int loops;\n\tint ret;\n\n\tif ((tp->phy_flags & TG3_PHYFLG_IS_FET) &&\n\t    (reg == MII_CTRL1000 || reg == MII_TG3_AUX_CTRL))\n\t\treturn 0;\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE,\n\t\t     (tp->mi_mode & ~MAC_MI_MODE_AUTO_POLL));\n\t\tudelay(80);\n\t}\n\n\ttg3_ape_lock(tp, tp->phy_ape_lock);\n\n\tframe_val  = ((phy_addr << MI_COM_PHY_ADDR_SHIFT) &\n\t\t      MI_COM_PHY_ADDR_MASK);\n\tframe_val |= ((reg << MI_COM_REG_ADDR_SHIFT) &\n\t\t      MI_COM_REG_ADDR_MASK);\n\tframe_val |= (val & MI_COM_DATA_MASK);\n\tframe_val |= (MI_COM_CMD_WRITE | MI_COM_START);\n\n\ttw32_f(MAC_MI_COM, frame_val);\n\n\tloops = PHY_BUSY_LOOPS;\n\twhile (loops != 0) {\n\t\tudelay(10);\n\t\tframe_val = tr32(MAC_MI_COM);\n\t\tif ((frame_val & MI_COM_BUSY) == 0) {\n\t\t\tudelay(5);\n\t\t\tframe_val = tr32(MAC_MI_COM);\n\t\t\tbreak;\n\t\t}\n\t\tloops -= 1;\n\t}\n\n\tret = -EBUSY;\n\tif (loops != 0)\n\t\tret = 0;\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE, tp->mi_mode);\n\t\tudelay(80);\n\t}\n\n\ttg3_ape_unlock(tp, tp->phy_ape_lock);\n\n\treturn ret;\n}\n\nstatic int tg3_writephy(struct tg3 *tp, int reg, u32 val)\n{\n\treturn __tg3_writephy(tp, tp->phy_addr, reg, val);\n}\n\nstatic int tg3_phy_cl45_write(struct tg3 *tp, u32 devad, u32 addr, u32 val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_CTRL, devad);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_ADDRESS, addr);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_CTRL,\n\t\t\t   MII_TG3_MMD_CTRL_DATA_NOINC | devad);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_ADDRESS, val);\n\ndone:\n\treturn err;\n}\n\nstatic int tg3_phy_cl45_read(struct tg3 *tp, u32 devad, u32 addr, u32 *val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_CTRL, devad);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_ADDRESS, addr);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_CTRL,\n\t\t\t   MII_TG3_MMD_CTRL_DATA_NOINC | devad);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_readphy(tp, MII_TG3_MMD_ADDRESS, val);\n\ndone:\n\treturn err;\n}\n\nstatic int tg3_phydsp_read(struct tg3 *tp, u32 reg, u32 *val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_DSP_ADDRESS, reg);\n\tif (!err)\n\t\terr = tg3_readphy(tp, MII_TG3_DSP_RW_PORT, val);\n\n\treturn err;\n}\n\nstatic int tg3_phydsp_write(struct tg3 *tp, u32 reg, u32 val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_DSP_ADDRESS, reg);\n\tif (!err)\n\t\terr = tg3_writephy(tp, MII_TG3_DSP_RW_PORT, val);\n\n\treturn err;\n}\n\nstatic int tg3_phy_auxctl_read(struct tg3 *tp, int reg, u32 *val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_AUX_CTRL,\n\t\t\t   (reg << MII_TG3_AUXCTL_MISC_RDSEL_SHIFT) |\n\t\t\t   MII_TG3_AUXCTL_SHDWSEL_MISC);\n\tif (!err)\n\t\terr = tg3_readphy(tp, MII_TG3_AUX_CTRL, val);\n\n\treturn err;\n}\n\nstatic int tg3_phy_auxctl_write(struct tg3 *tp, int reg, u32 set)\n{\n\tif (reg == MII_TG3_AUXCTL_SHDWSEL_MISC)\n\t\tset |= MII_TG3_AUXCTL_MISC_WREN;\n\n\treturn tg3_writephy(tp, MII_TG3_AUX_CTRL, set | reg);\n}\n\nstatic int tg3_phy_toggle_auxctl_smdsp(struct tg3 *tp, bool enable)\n{\n\tu32 val;\n\tint err;\n\n\terr = tg3_phy_auxctl_read(tp, MII_TG3_AUXCTL_SHDWSEL_AUXCTL, &val);\n\n\tif (err)\n\t\treturn err;\n\tif (enable)\n\n\t\tval |= MII_TG3_AUXCTL_ACTL_SMDSP_ENA;\n\telse\n\t\tval &= ~MII_TG3_AUXCTL_ACTL_SMDSP_ENA;\n\n\terr = tg3_phy_auxctl_write((tp), MII_TG3_AUXCTL_SHDWSEL_AUXCTL,\n\t\t\t\t   val | MII_TG3_AUXCTL_ACTL_TX_6DB);\n\n\treturn err;\n}\n\nstatic int tg3_bmcr_reset(struct tg3 *tp)\n{\n\tu32 phy_control;\n\tint limit, err;\n\n\t/* OK, reset it, and poll the BMCR_RESET bit until it\n\t * clears or we time out.\n\t */\n\tphy_control = BMCR_RESET;\n\terr = tg3_writephy(tp, MII_BMCR, phy_control);\n\tif (err != 0)\n\t\treturn -EBUSY;\n\n\tlimit = 5000;\n\twhile (limit--) {\n\t\terr = tg3_readphy(tp, MII_BMCR, &phy_control);\n\t\tif (err != 0)\n\t\t\treturn -EBUSY;\n\n\t\tif ((phy_control & BMCR_RESET) == 0) {\n\t\t\tudelay(40);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(10);\n\t}\n\tif (limit < 0)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic int tg3_mdio_read(struct mii_bus *bp, int mii_id, int reg)\n{\n\tstruct tg3 *tp = bp->priv;\n\tu32 val;\n\n\tspin_lock_bh(&tp->lock);\n\n\tif (tg3_readphy(tp, reg, &val))\n\t\tval = -EIO;\n\n\tspin_unlock_bh(&tp->lock);\n\n\treturn val;\n}\n\nstatic int tg3_mdio_write(struct mii_bus *bp, int mii_id, int reg, u16 val)\n{\n\tstruct tg3 *tp = bp->priv;\n\tu32 ret = 0;\n\n\tspin_lock_bh(&tp->lock);\n\n\tif (tg3_writephy(tp, reg, val))\n\t\tret = -EIO;\n\n\tspin_unlock_bh(&tp->lock);\n\n\treturn ret;\n}\n\nstatic int tg3_mdio_reset(struct mii_bus *bp)\n{\n\treturn 0;\n}\n\nstatic void tg3_mdio_config_5785(struct tg3 *tp)\n{\n\tu32 val;\n\tstruct phy_device *phydev;\n\n\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\tswitch (phydev->drv->phy_id & phydev->drv->phy_id_mask) {\n\tcase PHY_ID_BCM50610:\n\tcase PHY_ID_BCM50610M:\n\t\tval = MAC_PHYCFG2_50610_LED_MODES;\n\t\tbreak;\n\tcase PHY_ID_BCMAC131:\n\t\tval = MAC_PHYCFG2_AC131_LED_MODES;\n\t\tbreak;\n\tcase PHY_ID_RTL8211C:\n\t\tval = MAC_PHYCFG2_RTL8211C_LED_MODES;\n\t\tbreak;\n\tcase PHY_ID_RTL8201E:\n\t\tval = MAC_PHYCFG2_RTL8201E_LED_MODES;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (phydev->interface != PHY_INTERFACE_MODE_RGMII) {\n\t\ttw32(MAC_PHYCFG2, val);\n\n\t\tval = tr32(MAC_PHYCFG1);\n\t\tval &= ~(MAC_PHYCFG1_RGMII_INT |\n\t\t\t MAC_PHYCFG1_RXCLK_TO_MASK | MAC_PHYCFG1_TXCLK_TO_MASK);\n\t\tval |= MAC_PHYCFG1_RXCLK_TIMEOUT | MAC_PHYCFG1_TXCLK_TIMEOUT;\n\t\ttw32(MAC_PHYCFG1, val);\n\n\t\treturn;\n\t}\n\n\tif (!tg3_flag(tp, RGMII_INBAND_DISABLE))\n\t\tval |= MAC_PHYCFG2_EMODE_MASK_MASK |\n\t\t       MAC_PHYCFG2_FMODE_MASK_MASK |\n\t\t       MAC_PHYCFG2_GMODE_MASK_MASK |\n\t\t       MAC_PHYCFG2_ACT_MASK_MASK   |\n\t\t       MAC_PHYCFG2_QUAL_MASK_MASK |\n\t\t       MAC_PHYCFG2_INBAND_ENABLE;\n\n\ttw32(MAC_PHYCFG2, val);\n\n\tval = tr32(MAC_PHYCFG1);\n\tval &= ~(MAC_PHYCFG1_RXCLK_TO_MASK | MAC_PHYCFG1_TXCLK_TO_MASK |\n\t\t MAC_PHYCFG1_RGMII_EXT_RX_DEC | MAC_PHYCFG1_RGMII_SND_STAT_EN);\n\tif (!tg3_flag(tp, RGMII_INBAND_DISABLE)) {\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_RX_EN))\n\t\t\tval |= MAC_PHYCFG1_RGMII_EXT_RX_DEC;\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_TX_EN))\n\t\t\tval |= MAC_PHYCFG1_RGMII_SND_STAT_EN;\n\t}\n\tval |= MAC_PHYCFG1_RXCLK_TIMEOUT | MAC_PHYCFG1_TXCLK_TIMEOUT |\n\t       MAC_PHYCFG1_RGMII_INT | MAC_PHYCFG1_TXC_DRV;\n\ttw32(MAC_PHYCFG1, val);\n\n\tval = tr32(MAC_EXT_RGMII_MODE);\n\tval &= ~(MAC_RGMII_MODE_RX_INT_B |\n\t\t MAC_RGMII_MODE_RX_QUALITY |\n\t\t MAC_RGMII_MODE_RX_ACTIVITY |\n\t\t MAC_RGMII_MODE_RX_ENG_DET |\n\t\t MAC_RGMII_MODE_TX_ENABLE |\n\t\t MAC_RGMII_MODE_TX_LOWPWR |\n\t\t MAC_RGMII_MODE_TX_RESET);\n\tif (!tg3_flag(tp, RGMII_INBAND_DISABLE)) {\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_RX_EN))\n\t\t\tval |= MAC_RGMII_MODE_RX_INT_B |\n\t\t\t       MAC_RGMII_MODE_RX_QUALITY |\n\t\t\t       MAC_RGMII_MODE_RX_ACTIVITY |\n\t\t\t       MAC_RGMII_MODE_RX_ENG_DET;\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_TX_EN))\n\t\t\tval |= MAC_RGMII_MODE_TX_ENABLE |\n\t\t\t       MAC_RGMII_MODE_TX_LOWPWR |\n\t\t\t       MAC_RGMII_MODE_TX_RESET;\n\t}\n\ttw32(MAC_EXT_RGMII_MODE, val);\n}\n\nstatic void tg3_mdio_start(struct tg3 *tp)\n{\n\ttp->mi_mode &= ~MAC_MI_MODE_AUTO_POLL;\n\ttw32_f(MAC_MI_MODE, tp->mi_mode);\n\tudelay(80);\n\n\tif (tg3_flag(tp, MDIOBUS_INITED) &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\ttg3_mdio_config_5785(tp);\n}\n\nstatic int tg3_mdio_init(struct tg3 *tp)\n{\n\tint i;\n\tu32 reg;\n\tstruct phy_device *phydev;\n\n\tif (tg3_flag(tp, 5717_PLUS)) {\n\t\tu32 is_serdes;\n\n\t\ttp->phy_addr = tp->pci_fn + 1;\n\n\t\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5717_A0)\n\t\t\tis_serdes = tr32(SG_DIG_STATUS) & SG_DIG_IS_SERDES;\n\t\telse\n\t\t\tis_serdes = tr32(TG3_CPMU_PHY_STRAP) &\n\t\t\t\t    TG3_CPMU_PHY_STRAP_IS_SERDES;\n\t\tif (is_serdes)\n\t\t\ttp->phy_addr += 7;\n\t} else\n\t\ttp->phy_addr = TG3_PHY_MII_ADDR;\n\n\ttg3_mdio_start(tp);\n\n\tif (!tg3_flag(tp, USE_PHYLIB) || tg3_flag(tp, MDIOBUS_INITED))\n\t\treturn 0;\n\n\ttp->mdio_bus = mdiobus_alloc();\n\tif (tp->mdio_bus == NULL)\n\t\treturn -ENOMEM;\n\n\ttp->mdio_bus->name     = \"tg3 mdio bus\";\n\tsnprintf(tp->mdio_bus->id, MII_BUS_ID_SIZE, \"%x\",\n\t\t (tp->pdev->bus->number << 8) | tp->pdev->devfn);\n\ttp->mdio_bus->priv     = tp;\n\ttp->mdio_bus->parent   = &tp->pdev->dev;\n\ttp->mdio_bus->read     = &tg3_mdio_read;\n\ttp->mdio_bus->write    = &tg3_mdio_write;\n\ttp->mdio_bus->reset    = &tg3_mdio_reset;\n\ttp->mdio_bus->phy_mask = ~(1 << TG3_PHY_MII_ADDR);\n\ttp->mdio_bus->irq      = &tp->mdio_irq[0];\n\n\tfor (i = 0; i < PHY_MAX_ADDR; i++)\n\t\ttp->mdio_bus->irq[i] = PHY_POLL;\n\n\t/* The bus registration will look for all the PHYs on the mdio bus.\n\t * Unfortunately, it does not ensure the PHY is powered up before\n\t * accessing the PHY ID registers.  A chip reset is the\n\t * quickest way to bring the device back to an operational state..\n\t */\n\tif (tg3_readphy(tp, MII_BMCR, &reg) || (reg & BMCR_PDOWN))\n\t\ttg3_bmcr_reset(tp);\n\n\ti = mdiobus_register(tp->mdio_bus);\n\tif (i) {\n\t\tdev_warn(&tp->pdev->dev, \"mdiobus_reg failed (0x%x)\\n\", i);\n\t\tmdiobus_free(tp->mdio_bus);\n\t\treturn i;\n\t}\n\n\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\tif (!phydev || !phydev->drv) {\n\t\tdev_warn(&tp->pdev->dev, \"No PHY devices\\n\");\n\t\tmdiobus_unregister(tp->mdio_bus);\n\t\tmdiobus_free(tp->mdio_bus);\n\t\treturn -ENODEV;\n\t}\n\n\tswitch (phydev->drv->phy_id & phydev->drv->phy_id_mask) {\n\tcase PHY_ID_BCM57780:\n\t\tphydev->interface = PHY_INTERFACE_MODE_GMII;\n\t\tphydev->dev_flags |= PHY_BRCM_AUTO_PWRDWN_ENABLE;\n\t\tbreak;\n\tcase PHY_ID_BCM50610:\n\tcase PHY_ID_BCM50610M:\n\t\tphydev->dev_flags |= PHY_BRCM_CLEAR_RGMII_MODE |\n\t\t\t\t     PHY_BRCM_RX_REFCLK_UNUSED |\n\t\t\t\t     PHY_BRCM_DIS_TXCRXC_NOENRGY |\n\t\t\t\t     PHY_BRCM_AUTO_PWRDWN_ENABLE;\n\t\tif (tg3_flag(tp, RGMII_INBAND_DISABLE))\n\t\t\tphydev->dev_flags |= PHY_BRCM_STD_IBND_DISABLE;\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_RX_EN))\n\t\t\tphydev->dev_flags |= PHY_BRCM_EXT_IBND_RX_ENABLE;\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_TX_EN))\n\t\t\tphydev->dev_flags |= PHY_BRCM_EXT_IBND_TX_ENABLE;\n\t\t/* fallthru */\n\tcase PHY_ID_RTL8211C:\n\t\tphydev->interface = PHY_INTERFACE_MODE_RGMII;\n\t\tbreak;\n\tcase PHY_ID_RTL8201E:\n\tcase PHY_ID_BCMAC131:\n\t\tphydev->interface = PHY_INTERFACE_MODE_MII;\n\t\tphydev->dev_flags |= PHY_BRCM_AUTO_PWRDWN_ENABLE;\n\t\ttp->phy_flags |= TG3_PHYFLG_IS_FET;\n\t\tbreak;\n\t}\n\n\ttg3_flag_set(tp, MDIOBUS_INITED);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\ttg3_mdio_config_5785(tp);\n\n\treturn 0;\n}\n\nstatic void tg3_mdio_fini(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, MDIOBUS_INITED)) {\n\t\ttg3_flag_clear(tp, MDIOBUS_INITED);\n\t\tmdiobus_unregister(tp->mdio_bus);\n\t\tmdiobus_free(tp->mdio_bus);\n\t}\n}\n\n/* tp->lock is held. */\nstatic inline void tg3_generate_fw_event(struct tg3 *tp)\n{\n\tu32 val;\n\n\tval = tr32(GRC_RX_CPU_EVENT);\n\tval |= GRC_RX_CPU_DRIVER_EVENT;\n\ttw32_f(GRC_RX_CPU_EVENT, val);\n\n\ttp->last_event_jiffies = jiffies;\n}\n\n#define TG3_FW_EVENT_TIMEOUT_USEC 2500\n\n/* tp->lock is held. */\nstatic void tg3_wait_for_event_ack(struct tg3 *tp)\n{\n\tint i;\n\tunsigned int delay_cnt;\n\tlong time_remain;\n\n\t/* If enough time has passed, no wait is necessary. */\n\ttime_remain = (long)(tp->last_event_jiffies + 1 +\n\t\t      usecs_to_jiffies(TG3_FW_EVENT_TIMEOUT_USEC)) -\n\t\t      (long)jiffies;\n\tif (time_remain < 0)\n\t\treturn;\n\n\t/* Check if we can shorten the wait time. */\n\tdelay_cnt = jiffies_to_usecs(time_remain);\n\tif (delay_cnt > TG3_FW_EVENT_TIMEOUT_USEC)\n\t\tdelay_cnt = TG3_FW_EVENT_TIMEOUT_USEC;\n\tdelay_cnt = (delay_cnt >> 3) + 1;\n\n\tfor (i = 0; i < delay_cnt; i++) {\n\t\tif (!(tr32(GRC_RX_CPU_EVENT) & GRC_RX_CPU_DRIVER_EVENT))\n\t\t\tbreak;\n\t\tudelay(8);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_phy_gather_ump_data(struct tg3 *tp, u32 *data)\n{\n\tu32 reg, val;\n\n\tval = 0;\n\tif (!tg3_readphy(tp, MII_BMCR, &reg))\n\t\tval = reg << 16;\n\tif (!tg3_readphy(tp, MII_BMSR, &reg))\n\t\tval |= (reg & 0xffff);\n\t*data++ = val;\n\n\tval = 0;\n\tif (!tg3_readphy(tp, MII_ADVERTISE, &reg))\n\t\tval = reg << 16;\n\tif (!tg3_readphy(tp, MII_LPA, &reg))\n\t\tval |= (reg & 0xffff);\n\t*data++ = val;\n\n\tval = 0;\n\tif (!(tp->phy_flags & TG3_PHYFLG_MII_SERDES)) {\n\t\tif (!tg3_readphy(tp, MII_CTRL1000, &reg))\n\t\t\tval = reg << 16;\n\t\tif (!tg3_readphy(tp, MII_STAT1000, &reg))\n\t\t\tval |= (reg & 0xffff);\n\t}\n\t*data++ = val;\n\n\tif (!tg3_readphy(tp, MII_PHYADDR, &reg))\n\t\tval = reg << 16;\n\telse\n\t\tval = 0;\n\t*data++ = val;\n}\n\n/* tp->lock is held. */\nstatic void tg3_ump_link_report(struct tg3 *tp)\n{\n\tu32 data[4];\n\n\tif (!tg3_flag(tp, 5780_CLASS) || !tg3_flag(tp, ENABLE_ASF))\n\t\treturn;\n\n\ttg3_phy_gather_ump_data(tp, data);\n\n\ttg3_wait_for_event_ack(tp);\n\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_MBOX, FWCMD_NICDRV_LINK_UPDATE);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_LEN_MBOX, 14);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX + 0x0, data[0]);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX + 0x4, data[1]);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX + 0x8, data[2]);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX + 0xc, data[3]);\n\n\ttg3_generate_fw_event(tp);\n}\n\n/* tp->lock is held. */\nstatic void tg3_stop_fw(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, ENABLE_ASF) && !tg3_flag(tp, ENABLE_APE)) {\n\t\t/* Wait for RX cpu to ACK the previous event. */\n\t\ttg3_wait_for_event_ack(tp);\n\n\t\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_MBOX, FWCMD_NICDRV_PAUSE_FW);\n\n\t\ttg3_generate_fw_event(tp);\n\n\t\t/* Wait for RX cpu to ACK this event. */\n\t\ttg3_wait_for_event_ack(tp);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_write_sig_pre_reset(struct tg3 *tp, int kind)\n{\n\ttg3_write_mem(tp, NIC_SRAM_FIRMWARE_MBOX,\n\t\t      NIC_SRAM_FIRMWARE_MBOX_MAGIC1);\n\n\tif (tg3_flag(tp, ASF_NEW_HANDSHAKE)) {\n\t\tswitch (kind) {\n\t\tcase RESET_KIND_INIT:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_START);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SHUTDOWN:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_UNLOAD);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SUSPEND:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_SUSPEND);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (kind == RESET_KIND_INIT ||\n\t    kind == RESET_KIND_SUSPEND)\n\t\ttg3_ape_driver_state_change(tp, kind);\n}\n\n/* tp->lock is held. */\nstatic void tg3_write_sig_post_reset(struct tg3 *tp, int kind)\n{\n\tif (tg3_flag(tp, ASF_NEW_HANDSHAKE)) {\n\t\tswitch (kind) {\n\t\tcase RESET_KIND_INIT:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_START_DONE);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SHUTDOWN:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_UNLOAD_DONE);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (kind == RESET_KIND_SHUTDOWN)\n\t\ttg3_ape_driver_state_change(tp, kind);\n}\n\n/* tp->lock is held. */\nstatic void tg3_write_sig_legacy(struct tg3 *tp, int kind)\n{\n\tif (tg3_flag(tp, ENABLE_ASF)) {\n\t\tswitch (kind) {\n\t\tcase RESET_KIND_INIT:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_START);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SHUTDOWN:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_UNLOAD);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SUSPEND:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_SUSPEND);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int tg3_poll_fw(struct tg3 *tp)\n{\n\tint i;\n\tu32 val;\n\n\tif (tg3_flag(tp, IS_SSB_CORE)) {\n\t\t/* We don't use firmware. */\n\t\treturn 0;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t/* Wait up to 20ms for init done. */\n\t\tfor (i = 0; i < 200; i++) {\n\t\t\tif (tr32(VCPU_STATUS) & VCPU_STATUS_INIT_DONE)\n\t\t\t\treturn 0;\n\t\t\tudelay(100);\n\t\t}\n\t\treturn -ENODEV;\n\t}\n\n\t/* Wait for firmware initialization to complete. */\n\tfor (i = 0; i < 100000; i++) {\n\t\ttg3_read_mem(tp, NIC_SRAM_FIRMWARE_MBOX, &val);\n\t\tif (val == ~NIC_SRAM_FIRMWARE_MBOX_MAGIC1)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\t/* Chip might not be fitted with firmware.  Some Sun onboard\n\t * parts are configured like that.  So don't signal the timeout\n\t * of the above loop as an error, but do report the lack of\n\t * running firmware once.\n\t */\n\tif (i >= 100000 && !tg3_flag(tp, NO_FWARE_REPORTED)) {\n\t\ttg3_flag_set(tp, NO_FWARE_REPORTED);\n\n\t\tnetdev_info(tp->dev, \"No firmware running\\n\");\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_57765_A0) {\n\t\t/* The 57765 A0 needs a little more\n\t\t * time to do some important work.\n\t\t */\n\t\tmdelay(10);\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_link_report(struct tg3 *tp)\n{\n\tif (!netif_carrier_ok(tp->dev)) {\n\t\tnetif_info(tp, link, tp->dev, \"Link is down\\n\");\n\t\ttg3_ump_link_report(tp);\n\t} else if (netif_msg_link(tp)) {\n\t\tnetdev_info(tp->dev, \"Link is up at %d Mbps, %s duplex\\n\",\n\t\t\t    (tp->link_config.active_speed == SPEED_1000 ?\n\t\t\t     1000 :\n\t\t\t     (tp->link_config.active_speed == SPEED_100 ?\n\t\t\t      100 : 10)),\n\t\t\t    (tp->link_config.active_duplex == DUPLEX_FULL ?\n\t\t\t     \"full\" : \"half\"));\n\n\t\tnetdev_info(tp->dev, \"Flow control is %s for TX and %s for RX\\n\",\n\t\t\t    (tp->link_config.active_flowctrl & FLOW_CTRL_TX) ?\n\t\t\t    \"on\" : \"off\",\n\t\t\t    (tp->link_config.active_flowctrl & FLOW_CTRL_RX) ?\n\t\t\t    \"on\" : \"off\");\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_EEE_CAP)\n\t\t\tnetdev_info(tp->dev, \"EEE is %s\\n\",\n\t\t\t\t    tp->setlpicnt ? \"enabled\" : \"disabled\");\n\n\t\ttg3_ump_link_report(tp);\n\t}\n\n\ttp->link_up = netif_carrier_ok(tp->dev);\n}\n\nstatic u16 tg3_advert_flowctrl_1000X(u8 flow_ctrl)\n{\n\tu16 miireg;\n\n\tif ((flow_ctrl & FLOW_CTRL_TX) && (flow_ctrl & FLOW_CTRL_RX))\n\t\tmiireg = ADVERTISE_1000XPAUSE;\n\telse if (flow_ctrl & FLOW_CTRL_TX)\n\t\tmiireg = ADVERTISE_1000XPSE_ASYM;\n\telse if (flow_ctrl & FLOW_CTRL_RX)\n\t\tmiireg = ADVERTISE_1000XPAUSE | ADVERTISE_1000XPSE_ASYM;\n\telse\n\t\tmiireg = 0;\n\n\treturn miireg;\n}\n\nstatic u8 tg3_resolve_flowctrl_1000X(u16 lcladv, u16 rmtadv)\n{\n\tu8 cap = 0;\n\n\tif (lcladv & rmtadv & ADVERTISE_1000XPAUSE) {\n\t\tcap = FLOW_CTRL_TX | FLOW_CTRL_RX;\n\t} else if (lcladv & rmtadv & ADVERTISE_1000XPSE_ASYM) {\n\t\tif (lcladv & ADVERTISE_1000XPAUSE)\n\t\t\tcap = FLOW_CTRL_RX;\n\t\tif (rmtadv & ADVERTISE_1000XPAUSE)\n\t\t\tcap = FLOW_CTRL_TX;\n\t}\n\n\treturn cap;\n}\n\nstatic void tg3_setup_flow_control(struct tg3 *tp, u32 lcladv, u32 rmtadv)\n{\n\tu8 autoneg;\n\tu8 flowctrl = 0;\n\tu32 old_rx_mode = tp->rx_mode;\n\tu32 old_tx_mode = tp->tx_mode;\n\n\tif (tg3_flag(tp, USE_PHYLIB))\n\t\tautoneg = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]->autoneg;\n\telse\n\t\tautoneg = tp->link_config.autoneg;\n\n\tif (autoneg == AUTONEG_ENABLE && tg3_flag(tp, PAUSE_AUTONEG)) {\n\t\tif (tp->phy_flags & TG3_PHYFLG_ANY_SERDES)\n\t\t\tflowctrl = tg3_resolve_flowctrl_1000X(lcladv, rmtadv);\n\t\telse\n\t\t\tflowctrl = mii_resolve_flowctrl_fdx(lcladv, rmtadv);\n\t} else\n\t\tflowctrl = tp->link_config.flowctrl;\n\n\ttp->link_config.active_flowctrl = flowctrl;\n\n\tif (flowctrl & FLOW_CTRL_RX)\n\t\ttp->rx_mode |= RX_MODE_FLOW_CTRL_ENABLE;\n\telse\n\t\ttp->rx_mode &= ~RX_MODE_FLOW_CTRL_ENABLE;\n\n\tif (old_rx_mode != tp->rx_mode)\n\t\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\n\tif (flowctrl & FLOW_CTRL_TX)\n\t\ttp->tx_mode |= TX_MODE_FLOW_CTRL_ENABLE;\n\telse\n\t\ttp->tx_mode &= ~TX_MODE_FLOW_CTRL_ENABLE;\n\n\tif (old_tx_mode != tp->tx_mode)\n\t\ttw32_f(MAC_TX_MODE, tp->tx_mode);\n}\n\nstatic void tg3_adjust_link(struct net_device *dev)\n{\n\tu8 oldflowctrl, linkmesg = 0;\n\tu32 mac_mode, lcl_adv, rmt_adv;\n\tstruct tg3 *tp = netdev_priv(dev);\n\tstruct phy_device *phydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\tspin_lock_bh(&tp->lock);\n\n\tmac_mode = tp->mac_mode & ~(MAC_MODE_PORT_MODE_MASK |\n\t\t\t\t    MAC_MODE_HALF_DUPLEX);\n\n\toldflowctrl = tp->link_config.active_flowctrl;\n\n\tif (phydev->link) {\n\t\tlcl_adv = 0;\n\t\trmt_adv = 0;\n\n\t\tif (phydev->speed == SPEED_100 || phydev->speed == SPEED_10)\n\t\t\tmac_mode |= MAC_MODE_PORT_MODE_MII;\n\t\telse if (phydev->speed == SPEED_1000 ||\n\t\t\t tg3_asic_rev(tp) != ASIC_REV_5785)\n\t\t\tmac_mode |= MAC_MODE_PORT_MODE_GMII;\n\t\telse\n\t\t\tmac_mode |= MAC_MODE_PORT_MODE_MII;\n\n\t\tif (phydev->duplex == DUPLEX_HALF)\n\t\t\tmac_mode |= MAC_MODE_HALF_DUPLEX;\n\t\telse {\n\t\t\tlcl_adv = mii_advertise_flowctrl(\n\t\t\t\t  tp->link_config.flowctrl);\n\n\t\t\tif (phydev->pause)\n\t\t\t\trmt_adv = LPA_PAUSE_CAP;\n\t\t\tif (phydev->asym_pause)\n\t\t\t\trmt_adv |= LPA_PAUSE_ASYM;\n\t\t}\n\n\t\ttg3_setup_flow_control(tp, lcl_adv, rmt_adv);\n\t} else\n\t\tmac_mode |= MAC_MODE_PORT_MODE_GMII;\n\n\tif (mac_mode != tp->mac_mode) {\n\t\ttp->mac_mode = mac_mode;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5785) {\n\t\tif (phydev->speed == SPEED_10)\n\t\t\ttw32(MAC_MI_STAT,\n\t\t\t     MAC_MI_STAT_10MBPS_MODE |\n\t\t\t     MAC_MI_STAT_LNKSTAT_ATTN_ENAB);\n\t\telse\n\t\t\ttw32(MAC_MI_STAT, MAC_MI_STAT_LNKSTAT_ATTN_ENAB);\n\t}\n\n\tif (phydev->speed == SPEED_1000 && phydev->duplex == DUPLEX_HALF)\n\t\ttw32(MAC_TX_LENGTHS,\n\t\t     ((2 << TX_LENGTHS_IPG_CRS_SHIFT) |\n\t\t      (6 << TX_LENGTHS_IPG_SHIFT) |\n\t\t      (0xff << TX_LENGTHS_SLOT_TIME_SHIFT)));\n\telse\n\t\ttw32(MAC_TX_LENGTHS,\n\t\t     ((2 << TX_LENGTHS_IPG_CRS_SHIFT) |\n\t\t      (6 << TX_LENGTHS_IPG_SHIFT) |\n\t\t      (32 << TX_LENGTHS_SLOT_TIME_SHIFT)));\n\n\tif (phydev->link != tp->old_link ||\n\t    phydev->speed != tp->link_config.active_speed ||\n\t    phydev->duplex != tp->link_config.active_duplex ||\n\t    oldflowctrl != tp->link_config.active_flowctrl)\n\t\tlinkmesg = 1;\n\n\ttp->old_link = phydev->link;\n\ttp->link_config.active_speed = phydev->speed;\n\ttp->link_config.active_duplex = phydev->duplex;\n\n\tspin_unlock_bh(&tp->lock);\n\n\tif (linkmesg)\n\t\ttg3_link_report(tp);\n}\n\nstatic int tg3_phy_init(struct tg3 *tp)\n{\n\tstruct phy_device *phydev;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_CONNECTED)\n\t\treturn 0;\n\n\t/* Bring the PHY back to a known state. */\n\ttg3_bmcr_reset(tp);\n\n\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\t/* Attach the MAC to the PHY. */\n\tphydev = phy_connect(tp->dev, dev_name(&phydev->dev),\n\t\t\t     tg3_adjust_link, phydev->interface);\n\tif (IS_ERR(phydev)) {\n\t\tdev_err(&tp->pdev->dev, \"Could not attach to PHY\\n\");\n\t\treturn PTR_ERR(phydev);\n\t}\n\n\t/* Mask with MAC supported features. */\n\tswitch (phydev->interface) {\n\tcase PHY_INTERFACE_MODE_GMII:\n\tcase PHY_INTERFACE_MODE_RGMII:\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY)) {\n\t\t\tphydev->supported &= (PHY_GBIT_FEATURES |\n\t\t\t\t\t      SUPPORTED_Pause |\n\t\t\t\t\t      SUPPORTED_Asym_Pause);\n\t\t\tbreak;\n\t\t}\n\t\t/* fallthru */\n\tcase PHY_INTERFACE_MODE_MII:\n\t\tphydev->supported &= (PHY_BASIC_FEATURES |\n\t\t\t\t      SUPPORTED_Pause |\n\t\t\t\t      SUPPORTED_Asym_Pause);\n\t\tbreak;\n\tdefault:\n\t\tphy_disconnect(tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]);\n\t\treturn -EINVAL;\n\t}\n\n\ttp->phy_flags |= TG3_PHYFLG_IS_CONNECTED;\n\n\tphydev->advertising = phydev->supported;\n\n\treturn 0;\n}\n\nstatic void tg3_phy_start(struct tg3 *tp)\n{\n\tstruct phy_device *phydev;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\treturn;\n\n\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER) {\n\t\ttp->phy_flags &= ~TG3_PHYFLG_IS_LOW_POWER;\n\t\tphydev->speed = tp->link_config.speed;\n\t\tphydev->duplex = tp->link_config.duplex;\n\t\tphydev->autoneg = tp->link_config.autoneg;\n\t\tphydev->advertising = tp->link_config.advertising;\n\t}\n\n\tphy_start(phydev);\n\n\tphy_start_aneg(phydev);\n}\n\nstatic void tg3_phy_stop(struct tg3 *tp)\n{\n\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\treturn;\n\n\tphy_stop(tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]);\n}\n\nstatic void tg3_phy_fini(struct tg3 *tp)\n{\n\tif (tp->phy_flags & TG3_PHYFLG_IS_CONNECTED) {\n\t\tphy_disconnect(tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]);\n\t\ttp->phy_flags &= ~TG3_PHYFLG_IS_CONNECTED;\n\t}\n}\n\nstatic int tg3_phy_set_extloopbk(struct tg3 *tp)\n{\n\tint err;\n\tu32 val;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_FET)\n\t\treturn 0;\n\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5401) {\n\t\t/* Cannot do read-modify-write on 5401 */\n\t\terr = tg3_phy_auxctl_write(tp,\n\t\t\t\t\t   MII_TG3_AUXCTL_SHDWSEL_AUXCTL,\n\t\t\t\t\t   MII_TG3_AUXCTL_ACTL_EXTLOOPBK |\n\t\t\t\t\t   0x4c20);\n\t\tgoto done;\n\t}\n\n\terr = tg3_phy_auxctl_read(tp,\n\t\t\t\t  MII_TG3_AUXCTL_SHDWSEL_AUXCTL, &val);\n\tif (err)\n\t\treturn err;\n\n\tval |= MII_TG3_AUXCTL_ACTL_EXTLOOPBK;\n\terr = tg3_phy_auxctl_write(tp,\n\t\t\t\t   MII_TG3_AUXCTL_SHDWSEL_AUXCTL, val);\n\ndone:\n\treturn err;\n}\n\nstatic void tg3_phy_fet_toggle_apd(struct tg3 *tp, bool enable)\n{\n\tu32 phytest;\n\n\tif (!tg3_readphy(tp, MII_TG3_FET_TEST, &phytest)) {\n\t\tu32 phy;\n\n\t\ttg3_writephy(tp, MII_TG3_FET_TEST,\n\t\t\t     phytest | MII_TG3_FET_SHADOW_EN);\n\t\tif (!tg3_readphy(tp, MII_TG3_FET_SHDW_AUXSTAT2, &phy)) {\n\t\t\tif (enable)\n\t\t\t\tphy |= MII_TG3_FET_SHDW_AUXSTAT2_APD;\n\t\t\telse\n\t\t\t\tphy &= ~MII_TG3_FET_SHDW_AUXSTAT2_APD;\n\t\t\ttg3_writephy(tp, MII_TG3_FET_SHDW_AUXSTAT2, phy);\n\t\t}\n\t\ttg3_writephy(tp, MII_TG3_FET_TEST, phytest);\n\t}\n}\n\nstatic void tg3_phy_toggle_apd(struct tg3 *tp, bool enable)\n{\n\tu32 reg;\n\n\tif (!tg3_flag(tp, 5705_PLUS) ||\n\t    (tg3_flag(tp, 5717_PLUS) &&\n\t     (tp->phy_flags & TG3_PHYFLG_MII_SERDES)))\n\t\treturn;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\ttg3_phy_fet_toggle_apd(tp, enable);\n\t\treturn;\n\t}\n\n\treg = MII_TG3_MISC_SHDW_WREN |\n\t      MII_TG3_MISC_SHDW_SCR5_SEL |\n\t      MII_TG3_MISC_SHDW_SCR5_LPED |\n\t      MII_TG3_MISC_SHDW_SCR5_DLPTLM |\n\t      MII_TG3_MISC_SHDW_SCR5_SDTL |\n\t      MII_TG3_MISC_SHDW_SCR5_C125OE;\n\tif (tg3_asic_rev(tp) != ASIC_REV_5784 || !enable)\n\t\treg |= MII_TG3_MISC_SHDW_SCR5_DLLAPD;\n\n\ttg3_writephy(tp, MII_TG3_MISC_SHDW, reg);\n\n\n\treg = MII_TG3_MISC_SHDW_WREN |\n\t      MII_TG3_MISC_SHDW_APD_SEL |\n\t      MII_TG3_MISC_SHDW_APD_WKTM_84MS;\n\tif (enable)\n\t\treg |= MII_TG3_MISC_SHDW_APD_ENABLE;\n\n\ttg3_writephy(tp, MII_TG3_MISC_SHDW, reg);\n}\n\nstatic void tg3_phy_toggle_automdix(struct tg3 *tp, int enable)\n{\n\tu32 phy;\n\n\tif (!tg3_flag(tp, 5705_PLUS) ||\n\t    (tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\treturn;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\tu32 ephy;\n\n\t\tif (!tg3_readphy(tp, MII_TG3_FET_TEST, &ephy)) {\n\t\t\tu32 reg = MII_TG3_FET_SHDW_MISCCTRL;\n\n\t\t\ttg3_writephy(tp, MII_TG3_FET_TEST,\n\t\t\t\t     ephy | MII_TG3_FET_SHADOW_EN);\n\t\t\tif (!tg3_readphy(tp, reg, &phy)) {\n\t\t\t\tif (enable)\n\t\t\t\t\tphy |= MII_TG3_FET_SHDW_MISCCTRL_MDIX;\n\t\t\t\telse\n\t\t\t\t\tphy &= ~MII_TG3_FET_SHDW_MISCCTRL_MDIX;\n\t\t\t\ttg3_writephy(tp, reg, phy);\n\t\t\t}\n\t\t\ttg3_writephy(tp, MII_TG3_FET_TEST, ephy);\n\t\t}\n\t} else {\n\t\tint ret;\n\n\t\tret = tg3_phy_auxctl_read(tp,\n\t\t\t\t\t  MII_TG3_AUXCTL_SHDWSEL_MISC, &phy);\n\t\tif (!ret) {\n\t\t\tif (enable)\n\t\t\t\tphy |= MII_TG3_AUXCTL_MISC_FORCE_AMDIX;\n\t\t\telse\n\t\t\t\tphy &= ~MII_TG3_AUXCTL_MISC_FORCE_AMDIX;\n\t\t\ttg3_phy_auxctl_write(tp,\n\t\t\t\t\t     MII_TG3_AUXCTL_SHDWSEL_MISC, phy);\n\t\t}\n\t}\n}\n\nstatic void tg3_phy_set_wirespeed(struct tg3 *tp)\n{\n\tint ret;\n\tu32 val;\n\n\tif (tp->phy_flags & TG3_PHYFLG_NO_ETH_WIRE_SPEED)\n\t\treturn;\n\n\tret = tg3_phy_auxctl_read(tp, MII_TG3_AUXCTL_SHDWSEL_MISC, &val);\n\tif (!ret)\n\t\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_MISC,\n\t\t\t\t     val | MII_TG3_AUXCTL_MISC_WIRESPD_EN);\n}\n\nstatic void tg3_phy_apply_otp(struct tg3 *tp)\n{\n\tu32 otp, phy;\n\n\tif (!tp->phy_otp)\n\t\treturn;\n\n\totp = tp->phy_otp;\n\n\tif (tg3_phy_toggle_auxctl_smdsp(tp, true))\n\t\treturn;\n\n\tphy = ((otp & TG3_OTP_AGCTGT_MASK) >> TG3_OTP_AGCTGT_SHIFT);\n\tphy |= MII_TG3_DSP_TAP1_AGCTGT_DFLT;\n\ttg3_phydsp_write(tp, MII_TG3_DSP_TAP1, phy);\n\n\tphy = ((otp & TG3_OTP_HPFFLTR_MASK) >> TG3_OTP_HPFFLTR_SHIFT) |\n\t      ((otp & TG3_OTP_HPFOVER_MASK) >> TG3_OTP_HPFOVER_SHIFT);\n\ttg3_phydsp_write(tp, MII_TG3_DSP_AADJ1CH0, phy);\n\n\tphy = ((otp & TG3_OTP_LPFDIS_MASK) >> TG3_OTP_LPFDIS_SHIFT);\n\tphy |= MII_TG3_DSP_AADJ1CH3_ADCCKADJ;\n\ttg3_phydsp_write(tp, MII_TG3_DSP_AADJ1CH3, phy);\n\n\tphy = ((otp & TG3_OTP_VDAC_MASK) >> TG3_OTP_VDAC_SHIFT);\n\ttg3_phydsp_write(tp, MII_TG3_DSP_EXP75, phy);\n\n\tphy = ((otp & TG3_OTP_10BTAMP_MASK) >> TG3_OTP_10BTAMP_SHIFT);\n\ttg3_phydsp_write(tp, MII_TG3_DSP_EXP96, phy);\n\n\tphy = ((otp & TG3_OTP_ROFF_MASK) >> TG3_OTP_ROFF_SHIFT) |\n\t      ((otp & TG3_OTP_RCOFF_MASK) >> TG3_OTP_RCOFF_SHIFT);\n\ttg3_phydsp_write(tp, MII_TG3_DSP_EXP97, phy);\n\n\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n}\n\nstatic void tg3_phy_eee_adjust(struct tg3 *tp, u32 current_link_up)\n{\n\tu32 val;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_EEE_CAP))\n\t\treturn;\n\n\ttp->setlpicnt = 0;\n\n\tif (tp->link_config.autoneg == AUTONEG_ENABLE &&\n\t    current_link_up == 1 &&\n\t    tp->link_config.active_duplex == DUPLEX_FULL &&\n\t    (tp->link_config.active_speed == SPEED_100 ||\n\t     tp->link_config.active_speed == SPEED_1000)) {\n\t\tu32 eeectl;\n\n\t\tif (tp->link_config.active_speed == SPEED_1000)\n\t\t\teeectl = TG3_CPMU_EEE_CTRL_EXIT_16_5_US;\n\t\telse\n\t\t\teeectl = TG3_CPMU_EEE_CTRL_EXIT_36_US;\n\n\t\ttw32(TG3_CPMU_EEE_CTRL, eeectl);\n\n\t\ttg3_phy_cl45_read(tp, MDIO_MMD_AN,\n\t\t\t\t  TG3_CL45_D7_EEERES_STAT, &val);\n\n\t\tif (val == TG3_CL45_D7_EEERES_STAT_LP_1000T ||\n\t\t    val == TG3_CL45_D7_EEERES_STAT_LP_100TX)\n\t\t\ttp->setlpicnt = 2;\n\t}\n\n\tif (!tp->setlpicnt) {\n\t\tif (current_link_up == 1 &&\n\t\t   !tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\t\ttg3_phydsp_write(tp, MII_TG3_DSP_TAP26, 0x0000);\n\t\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t\t}\n\n\t\tval = tr32(TG3_CPMU_EEE_MODE);\n\t\ttw32(TG3_CPMU_EEE_MODE, val & ~TG3_CPMU_EEEMD_LPI_ENABLE);\n\t}\n}\n\nstatic void tg3_phy_eee_enable(struct tg3 *tp)\n{\n\tu32 val;\n\n\tif (tp->link_config.active_speed == SPEED_1000 &&\n\t    (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t     tg3_flag(tp, 57765_CLASS)) &&\n\t    !tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\tval = MII_TG3_DSP_TAP26_ALNOKO |\n\t\t      MII_TG3_DSP_TAP26_RMRXSTO;\n\t\ttg3_phydsp_write(tp, MII_TG3_DSP_TAP26, val);\n\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t}\n\n\tval = tr32(TG3_CPMU_EEE_MODE);\n\ttw32(TG3_CPMU_EEE_MODE, val | TG3_CPMU_EEEMD_LPI_ENABLE);\n}\n\nstatic int tg3_wait_macro_done(struct tg3 *tp)\n{\n\tint limit = 100;\n\n\twhile (limit--) {\n\t\tu32 tmp32;\n\n\t\tif (!tg3_readphy(tp, MII_TG3_DSP_CONTROL, &tmp32)) {\n\t\t\tif ((tmp32 & 0x1000) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (limit < 0)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic int tg3_phy_write_and_check_testpat(struct tg3 *tp, int *resetp)\n{\n\tstatic const u32 test_pat[4][6] = {\n\t{ 0x00005555, 0x00000005, 0x00002aaa, 0x0000000a, 0x00003456, 0x00000003 },\n\t{ 0x00002aaa, 0x0000000a, 0x00003333, 0x00000003, 0x0000789a, 0x00000005 },\n\t{ 0x00005a5a, 0x00000005, 0x00002a6a, 0x0000000a, 0x00001bcd, 0x00000003 },\n\t{ 0x00002a5a, 0x0000000a, 0x000033c3, 0x00000003, 0x00002ef1, 0x00000005 }\n\t};\n\tint chan;\n\n\tfor (chan = 0; chan < 4; chan++) {\n\t\tint i;\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t     (chan * 0x2000) | 0x0200);\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0002);\n\n\t\tfor (i = 0; i < 6; i++)\n\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT,\n\t\t\t\t     test_pat[chan][i]);\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0202);\n\t\tif (tg3_wait_macro_done(tp)) {\n\t\t\t*resetp = 1;\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t     (chan * 0x2000) | 0x0200);\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0082);\n\t\tif (tg3_wait_macro_done(tp)) {\n\t\t\t*resetp = 1;\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0802);\n\t\tif (tg3_wait_macro_done(tp)) {\n\t\t\t*resetp = 1;\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\tfor (i = 0; i < 6; i += 2) {\n\t\t\tu32 low, high;\n\n\t\t\tif (tg3_readphy(tp, MII_TG3_DSP_RW_PORT, &low) ||\n\t\t\t    tg3_readphy(tp, MII_TG3_DSP_RW_PORT, &high) ||\n\t\t\t    tg3_wait_macro_done(tp)) {\n\t\t\t\t*resetp = 1;\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t\tlow &= 0x7fff;\n\t\t\thigh &= 0x000f;\n\t\t\tif (low != test_pat[chan][i] ||\n\t\t\t    high != test_pat[chan][i+1]) {\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS, 0x000b);\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x4001);\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x4005);\n\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int tg3_phy_reset_chanpat(struct tg3 *tp)\n{\n\tint chan;\n\n\tfor (chan = 0; chan < 4; chan++) {\n\t\tint i;\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t     (chan * 0x2000) | 0x0200);\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0002);\n\t\tfor (i = 0; i < 6; i++)\n\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x000);\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0202);\n\t\tif (tg3_wait_macro_done(tp))\n\t\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic int tg3_phy_reset_5703_4_5(struct tg3 *tp)\n{\n\tu32 reg32, phy9_orig;\n\tint retries, do_phy_reset, err;\n\n\tretries = 10;\n\tdo_phy_reset = 1;\n\tdo {\n\t\tif (do_phy_reset) {\n\t\t\terr = tg3_bmcr_reset(tp);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tdo_phy_reset = 0;\n\t\t}\n\n\t\t/* Disable transmitter and interrupt.  */\n\t\tif (tg3_readphy(tp, MII_TG3_EXT_CTRL, &reg32))\n\t\t\tcontinue;\n\n\t\treg32 |= 0x3000;\n\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL, reg32);\n\n\t\t/* Set full-duplex, 1000 mbps.  */\n\t\ttg3_writephy(tp, MII_BMCR,\n\t\t\t     BMCR_FULLDPLX | BMCR_SPEED1000);\n\n\t\t/* Set to master mode.  */\n\t\tif (tg3_readphy(tp, MII_CTRL1000, &phy9_orig))\n\t\t\tcontinue;\n\n\t\ttg3_writephy(tp, MII_CTRL1000,\n\t\t\t     CTL1000_AS_MASTER | CTL1000_ENABLE_MASTER);\n\n\t\terr = tg3_phy_toggle_auxctl_smdsp(tp, true);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/* Block the PHY control access.  */\n\t\ttg3_phydsp_write(tp, 0x8005, 0x0800);\n\n\t\terr = tg3_phy_write_and_check_testpat(tp, &do_phy_reset);\n\t\tif (!err)\n\t\t\tbreak;\n\t} while (--retries);\n\n\terr = tg3_phy_reset_chanpat(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_phydsp_write(tp, 0x8005, 0x0000);\n\n\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS, 0x8200);\n\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0000);\n\n\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\n\ttg3_writephy(tp, MII_CTRL1000, phy9_orig);\n\n\tif (!tg3_readphy(tp, MII_TG3_EXT_CTRL, &reg32)) {\n\t\treg32 &= ~0x3000;\n\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL, reg32);\n\t} else if (!err)\n\t\terr = -EBUSY;\n\n\treturn err;\n}\n\nstatic void tg3_carrier_off(struct tg3 *tp)\n{\n\tnetif_carrier_off(tp->dev);\n\ttp->link_up = false;\n}\n\n/* This will reset the tigon3 PHY if there is no valid\n * link unless the FORCE argument is non-zero.\n */\nstatic int tg3_phy_reset(struct tg3 *tp)\n{\n\tu32 val, cpmuctrl;\n\tint err;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tval = tr32(GRC_MISC_CFG);\n\t\ttw32_f(GRC_MISC_CFG, val & ~GRC_MISC_CFG_EPHY_IDDQ);\n\t\tudelay(40);\n\t}\n\terr  = tg3_readphy(tp, MII_BMSR, &val);\n\terr |= tg3_readphy(tp, MII_BMSR, &val);\n\tif (err != 0)\n\t\treturn -EBUSY;\n\n\tif (netif_running(tp->dev) && tp->link_up) {\n\t\tnetif_carrier_off(tp->dev);\n\t\ttg3_link_report(tp);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\terr = tg3_phy_reset_5703_4_5(tp);\n\t\tif (err)\n\t\t\treturn err;\n\t\tgoto out;\n\t}\n\n\tcpmuctrl = 0;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5784 &&\n\t    tg3_chip_rev(tp) != CHIPREV_5784_AX) {\n\t\tcpmuctrl = tr32(TG3_CPMU_CTRL);\n\t\tif (cpmuctrl & CPMU_CTRL_GPHY_10MB_RXONLY)\n\t\t\ttw32(TG3_CPMU_CTRL,\n\t\t\t     cpmuctrl & ~CPMU_CTRL_GPHY_10MB_RXONLY);\n\t}\n\n\terr = tg3_bmcr_reset(tp);\n\tif (err)\n\t\treturn err;\n\n\tif (cpmuctrl & CPMU_CTRL_GPHY_10MB_RXONLY) {\n\t\tval = MII_TG3_DSP_EXP8_AEDW | MII_TG3_DSP_EXP8_REJ2MHz;\n\t\ttg3_phydsp_write(tp, MII_TG3_DSP_EXP8, val);\n\n\t\ttw32(TG3_CPMU_CTRL, cpmuctrl);\n\t}\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX ||\n\t    tg3_chip_rev(tp) == CHIPREV_5761_AX) {\n\t\tval = tr32(TG3_CPMU_LSPD_1000MB_CLK);\n\t\tif ((val & CPMU_LSPD_1000MB_MACCLK_MASK) ==\n\t\t    CPMU_LSPD_1000MB_MACCLK_12_5) {\n\t\t\tval &= ~CPMU_LSPD_1000MB_MACCLK_MASK;\n\t\t\tudelay(40);\n\t\t\ttw32_f(TG3_CPMU_LSPD_1000MB_CLK, val);\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, 5717_PLUS) &&\n\t    (tp->phy_flags & TG3_PHYFLG_MII_SERDES))\n\t\treturn 0;\n\n\ttg3_phy_apply_otp(tp);\n\n\tif (tp->phy_flags & TG3_PHYFLG_ENABLE_APD)\n\t\ttg3_phy_toggle_apd(tp, true);\n\telse\n\t\ttg3_phy_toggle_apd(tp, false);\n\nout:\n\tif ((tp->phy_flags & TG3_PHYFLG_ADC_BUG) &&\n\t    !tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\ttg3_phydsp_write(tp, 0x201f, 0x2aaa);\n\t\ttg3_phydsp_write(tp, 0x000a, 0x0323);\n\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t}\n\n\tif (tp->phy_flags & TG3_PHYFLG_5704_A0_BUG) {\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8d68);\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8d68);\n\t}\n\n\tif (tp->phy_flags & TG3_PHYFLG_BER_BUG) {\n\t\tif (!tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\t\ttg3_phydsp_write(tp, 0x000a, 0x310b);\n\t\t\ttg3_phydsp_write(tp, 0x201f, 0x9506);\n\t\t\ttg3_phydsp_write(tp, 0x401f, 0x14e2);\n\t\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t\t}\n\t} else if (tp->phy_flags & TG3_PHYFLG_JITTER_BUG) {\n\t\tif (!tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS, 0x000a);\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_ADJUST_TRIM) {\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x110b);\n\t\t\t\ttg3_writephy(tp, MII_TG3_TEST1,\n\t\t\t\t\t     MII_TG3_TEST1_TRIM_EN | 0x4);\n\t\t\t} else\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x010b);\n\n\t\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t\t}\n\t}\n\n\t/* Set Extended packet length bit (bit 14) on all chips that */\n\t/* support jumbo frames */\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5401) {\n\t\t/* Cannot do read-modify-write on 5401 */\n\t\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_AUXCTL, 0x4c20);\n\t} else if (tg3_flag(tp, JUMBO_CAPABLE)) {\n\t\t/* Set bit 14 with read-modify-write to preserve other bits */\n\t\terr = tg3_phy_auxctl_read(tp,\n\t\t\t\t\t  MII_TG3_AUXCTL_SHDWSEL_AUXCTL, &val);\n\t\tif (!err)\n\t\t\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_AUXCTL,\n\t\t\t\t\t   val | MII_TG3_AUXCTL_ACTL_EXTPKTLEN);\n\t}\n\n\t/* Set phy register 0x10 bit 0 to high fifo elasticity to support\n\t * jumbo frames transmission.\n\t */\n\tif (tg3_flag(tp, JUMBO_CAPABLE)) {\n\t\tif (!tg3_readphy(tp, MII_TG3_EXT_CTRL, &val))\n\t\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL,\n\t\t\t\t     val | MII_TG3_EXT_CTRL_FIFO_ELASTIC);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t/* adjust output voltage */\n\t\ttg3_writephy(tp, MII_TG3_FET_PTEST, 0x12);\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5762_A0)\n\t\ttg3_phydsp_write(tp, 0xffb, 0x4000);\n\n\ttg3_phy_toggle_automdix(tp, 1);\n\ttg3_phy_set_wirespeed(tp);\n\treturn 0;\n}\n\n#define TG3_GPIO_MSG_DRVR_PRES\t\t 0x00000001\n#define TG3_GPIO_MSG_NEED_VAUX\t\t 0x00000002\n#define TG3_GPIO_MSG_MASK\t\t (TG3_GPIO_MSG_DRVR_PRES | \\\n\t\t\t\t\t  TG3_GPIO_MSG_NEED_VAUX)\n#define TG3_GPIO_MSG_ALL_DRVR_PRES_MASK \\\n\t((TG3_GPIO_MSG_DRVR_PRES << 0) | \\\n\t (TG3_GPIO_MSG_DRVR_PRES << 4) | \\\n\t (TG3_GPIO_MSG_DRVR_PRES << 8) | \\\n\t (TG3_GPIO_MSG_DRVR_PRES << 12))\n\n#define TG3_GPIO_MSG_ALL_NEED_VAUX_MASK \\\n\t((TG3_GPIO_MSG_NEED_VAUX << 0) | \\\n\t (TG3_GPIO_MSG_NEED_VAUX << 4) | \\\n\t (TG3_GPIO_MSG_NEED_VAUX << 8) | \\\n\t (TG3_GPIO_MSG_NEED_VAUX << 12))\n\nstatic inline u32 tg3_set_function_status(struct tg3 *tp, u32 newstat)\n{\n\tu32 status, shift;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\tstatus = tg3_ape_read32(tp, TG3_APE_GPIO_MSG);\n\telse\n\t\tstatus = tr32(TG3_CPMU_DRV_STATUS);\n\n\tshift = TG3_APE_GPIO_MSG_SHIFT + 4 * tp->pci_fn;\n\tstatus &= ~(TG3_GPIO_MSG_MASK << shift);\n\tstatus |= (newstat << shift);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\ttg3_ape_write32(tp, TG3_APE_GPIO_MSG, status);\n\telse\n\t\ttw32(TG3_CPMU_DRV_STATUS, status);\n\n\treturn status >> TG3_APE_GPIO_MSG_SHIFT;\n}\n\nstatic inline int tg3_pwrsrc_switch_to_vmain(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, IS_NIC))\n\t\treturn 0;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720) {\n\t\tif (tg3_ape_lock(tp, TG3_APE_LOCK_GPIO))\n\t\t\treturn -EIO;\n\n\t\ttg3_set_function_status(tp, TG3_GPIO_MSG_DRVR_PRES);\n\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\ttg3_ape_unlock(tp, TG3_APE_LOCK_GPIO);\n\t} else {\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_pwrsrc_die_with_vmain(struct tg3 *tp)\n{\n\tu32 grc_local_ctrl;\n\n\tif (!tg3_flag(tp, IS_NIC) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5701)\n\t\treturn;\n\n\tgrc_local_ctrl = tp->grc_local_ctrl | GRC_LCLCTRL_GPIO_OE1;\n\n\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t    grc_local_ctrl | GRC_LCLCTRL_GPIO_OUTPUT1,\n\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t    grc_local_ctrl,\n\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t    grc_local_ctrl | GRC_LCLCTRL_GPIO_OUTPUT1,\n\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n}\n\nstatic void tg3_pwrsrc_switch_to_vaux(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, IS_NIC))\n\t\treturn;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl |\n\t\t\t    (GRC_LCLCTRL_GPIO_OE0 |\n\t\t\t     GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t     GRC_LCLCTRL_GPIO_OE2 |\n\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT0 |\n\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT1),\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t} else if (tp->pdev->device == PCI_DEVICE_ID_TIGON3_5761 ||\n\t\t   tp->pdev->device == TG3PCI_DEVICE_TIGON3_5761S) {\n\t\t/* The 5761 non-e device swaps GPIO 0 and GPIO 2. */\n\t\tu32 grc_local_ctrl = GRC_LCLCTRL_GPIO_OE0 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OE2 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT0 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT1 |\n\t\t\t\t     tp->grc_local_ctrl;\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\tgrc_local_ctrl |= GRC_LCLCTRL_GPIO_OUTPUT2;\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\tgrc_local_ctrl &= ~GRC_LCLCTRL_GPIO_OUTPUT0;\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t} else {\n\t\tu32 no_gpio2;\n\t\tu32 grc_local_ctrl = 0;\n\n\t\t/* Workaround to prevent overdrawing Amps. */\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\t\tgrc_local_ctrl |= GRC_LCLCTRL_GPIO_OE3;\n\t\t\ttw32_wait_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl |\n\t\t\t\t    grc_local_ctrl,\n\t\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t\t}\n\n\t\t/* On 5753 and variants, GPIO2 cannot be used. */\n\t\tno_gpio2 = tp->nic_sram_data_cfg &\n\t\t\t   NIC_SRAM_DATA_CFG_NO_GPIO2;\n\n\t\tgrc_local_ctrl |= GRC_LCLCTRL_GPIO_OE0 |\n\t\t\t\t  GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t\t  GRC_LCLCTRL_GPIO_OE2 |\n\t\t\t\t  GRC_LCLCTRL_GPIO_OUTPUT1 |\n\t\t\t\t  GRC_LCLCTRL_GPIO_OUTPUT2;\n\t\tif (no_gpio2) {\n\t\t\tgrc_local_ctrl &= ~(GRC_LCLCTRL_GPIO_OE2 |\n\t\t\t\t\t    GRC_LCLCTRL_GPIO_OUTPUT2);\n\t\t}\n\t\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t\t    tp->grc_local_ctrl | grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\tgrc_local_ctrl |= GRC_LCLCTRL_GPIO_OUTPUT0;\n\n\t\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t\t    tp->grc_local_ctrl | grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\tif (!no_gpio2) {\n\t\t\tgrc_local_ctrl &= ~GRC_LCLCTRL_GPIO_OUTPUT2;\n\t\t\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t\t\t    tp->grc_local_ctrl | grc_local_ctrl,\n\t\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t\t}\n\t}\n}\n\nstatic void tg3_frob_aux_power_5717(struct tg3 *tp, bool wol_enable)\n{\n\tu32 msg = 0;\n\n\t/* Serialize power state transitions */\n\tif (tg3_ape_lock(tp, TG3_APE_LOCK_GPIO))\n\t\treturn;\n\n\tif (tg3_flag(tp, ENABLE_ASF) || tg3_flag(tp, ENABLE_APE) || wol_enable)\n\t\tmsg = TG3_GPIO_MSG_NEED_VAUX;\n\n\tmsg = tg3_set_function_status(tp, msg);\n\n\tif (msg & TG3_GPIO_MSG_ALL_DRVR_PRES_MASK)\n\t\tgoto done;\n\n\tif (msg & TG3_GPIO_MSG_ALL_NEED_VAUX_MASK)\n\t\ttg3_pwrsrc_switch_to_vaux(tp);\n\telse\n\t\ttg3_pwrsrc_die_with_vmain(tp);\n\ndone:\n\ttg3_ape_unlock(tp, TG3_APE_LOCK_GPIO);\n}\n\nstatic void tg3_frob_aux_power(struct tg3 *tp, bool include_wol)\n{\n\tbool need_vaux = false;\n\n\t/* The GPIOs do something completely different on 57765. */\n\tif (!tg3_flag(tp, IS_NIC) || tg3_flag(tp, 57765_CLASS))\n\t\treturn;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720) {\n\t\ttg3_frob_aux_power_5717(tp, include_wol ?\n\t\t\t\t\ttg3_flag(tp, WOL_ENABLE) != 0 : 0);\n\t\treturn;\n\t}\n\n\tif (tp->pdev_peer && tp->pdev_peer != tp->pdev) {\n\t\tstruct net_device *dev_peer;\n\n\t\tdev_peer = pci_get_drvdata(tp->pdev_peer);\n\n\t\t/* remove_one() may have been run on the peer. */\n\t\tif (dev_peer) {\n\t\t\tstruct tg3 *tp_peer = netdev_priv(dev_peer);\n\n\t\t\tif (tg3_flag(tp_peer, INIT_COMPLETE))\n\t\t\t\treturn;\n\n\t\t\tif ((include_wol && tg3_flag(tp_peer, WOL_ENABLE)) ||\n\t\t\t    tg3_flag(tp_peer, ENABLE_ASF))\n\t\t\t\tneed_vaux = true;\n\t\t}\n\t}\n\n\tif ((include_wol && tg3_flag(tp, WOL_ENABLE)) ||\n\t    tg3_flag(tp, ENABLE_ASF))\n\t\tneed_vaux = true;\n\n\tif (need_vaux)\n\t\ttg3_pwrsrc_switch_to_vaux(tp);\n\telse\n\t\ttg3_pwrsrc_die_with_vmain(tp);\n}\n\nstatic int tg3_5700_link_polarity(struct tg3 *tp, u32 speed)\n{\n\tif (tp->led_ctrl == LED_CTRL_MODE_PHY_2)\n\t\treturn 1;\n\telse if ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5411) {\n\t\tif (speed != SPEED_10)\n\t\t\treturn 1;\n\t} else if (speed == SPEED_10)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void tg3_power_down_phy(struct tg3 *tp, bool do_low_power)\n{\n\tu32 val;\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5704) {\n\t\t\tu32 sg_dig_ctrl = tr32(SG_DIG_CTRL);\n\t\t\tu32 serdes_cfg = tr32(MAC_SERDES_CFG);\n\n\t\t\tsg_dig_ctrl |=\n\t\t\t\tSG_DIG_USING_HW_AUTONEG | SG_DIG_SOFT_RESET;\n\t\t\ttw32(SG_DIG_CTRL, sg_dig_ctrl);\n\t\t\ttw32(MAC_SERDES_CFG, serdes_cfg | (1 << 15));\n\t\t}\n\t\treturn;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\ttg3_bmcr_reset(tp);\n\t\tval = tr32(GRC_MISC_CFG);\n\t\ttw32_f(GRC_MISC_CFG, val | GRC_MISC_CFG_EPHY_IDDQ);\n\t\tudelay(40);\n\t\treturn;\n\t} else if (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\tu32 phytest;\n\t\tif (!tg3_readphy(tp, MII_TG3_FET_TEST, &phytest)) {\n\t\t\tu32 phy;\n\n\t\t\ttg3_writephy(tp, MII_ADVERTISE, 0);\n\t\t\ttg3_writephy(tp, MII_BMCR,\n\t\t\t\t     BMCR_ANENABLE | BMCR_ANRESTART);\n\n\t\t\ttg3_writephy(tp, MII_TG3_FET_TEST,\n\t\t\t\t     phytest | MII_TG3_FET_SHADOW_EN);\n\t\t\tif (!tg3_readphy(tp, MII_TG3_FET_SHDW_AUXMODE4, &phy)) {\n\t\t\t\tphy |= MII_TG3_FET_SHDW_AUXMODE4_SBPD;\n\t\t\t\ttg3_writephy(tp,\n\t\t\t\t\t     MII_TG3_FET_SHDW_AUXMODE4,\n\t\t\t\t\t     phy);\n\t\t\t}\n\t\t\ttg3_writephy(tp, MII_TG3_FET_TEST, phytest);\n\t\t}\n\t\treturn;\n\t} else if (do_low_power) {\n\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL,\n\t\t\t     MII_TG3_EXT_CTRL_FORCE_LED_OFF);\n\n\t\tval = MII_TG3_AUXCTL_PCTL_100TX_LPWR |\n\t\t      MII_TG3_AUXCTL_PCTL_SPR_ISOLATE |\n\t\t      MII_TG3_AUXCTL_PCTL_VREG_11V;\n\t\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_PWRCTL, val);\n\t}\n\n\t/* The PHY should not be powered down on some chips because\n\t * of bugs.\n\t */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    (tg3_asic_rev(tp) == ASIC_REV_5780 &&\n\t     (tp->phy_flags & TG3_PHYFLG_MII_SERDES)) ||\n\t    (tg3_asic_rev(tp) == ASIC_REV_5717 &&\n\t     !tp->pci_fn))\n\t\treturn;\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX ||\n\t    tg3_chip_rev(tp) == CHIPREV_5761_AX) {\n\t\tval = tr32(TG3_CPMU_LSPD_1000MB_CLK);\n\t\tval &= ~CPMU_LSPD_1000MB_MACCLK_MASK;\n\t\tval |= CPMU_LSPD_1000MB_MACCLK_12_5;\n\t\ttw32_f(TG3_CPMU_LSPD_1000MB_CLK, val);\n\t}\n\n\ttg3_writephy(tp, MII_BMCR, BMCR_PDOWN);\n}\n\n/* tp->lock is held. */\nstatic int tg3_nvram_lock(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, NVRAM)) {\n\t\tint i;\n\n\t\tif (tp->nvram_lock_cnt == 0) {\n\t\t\ttw32(NVRAM_SWARB, SWARB_REQ_SET1);\n\t\t\tfor (i = 0; i < 8000; i++) {\n\t\t\t\tif (tr32(NVRAM_SWARB) & SWARB_GNT1)\n\t\t\t\t\tbreak;\n\t\t\t\tudelay(20);\n\t\t\t}\n\t\t\tif (i == 8000) {\n\t\t\t\ttw32(NVRAM_SWARB, SWARB_REQ_CLR1);\n\t\t\t\treturn -ENODEV;\n\t\t\t}\n\t\t}\n\t\ttp->nvram_lock_cnt++;\n\t}\n\treturn 0;\n}\n\n/* tp->lock is held. */\nstatic void tg3_nvram_unlock(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, NVRAM)) {\n\t\tif (tp->nvram_lock_cnt > 0)\n\t\t\ttp->nvram_lock_cnt--;\n\t\tif (tp->nvram_lock_cnt == 0)\n\t\t\ttw32_f(NVRAM_SWARB, SWARB_REQ_CLR1);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_enable_nvram_access(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, 5750_PLUS) && !tg3_flag(tp, PROTECTED_NVRAM)) {\n\t\tu32 nvaccess = tr32(NVRAM_ACCESS);\n\n\t\ttw32(NVRAM_ACCESS, nvaccess | ACCESS_ENABLE);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_disable_nvram_access(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, 5750_PLUS) && !tg3_flag(tp, PROTECTED_NVRAM)) {\n\t\tu32 nvaccess = tr32(NVRAM_ACCESS);\n\n\t\ttw32(NVRAM_ACCESS, nvaccess & ~ACCESS_ENABLE);\n\t}\n}\n\nstatic int tg3_nvram_read_using_eeprom(struct tg3 *tp,\n\t\t\t\t\tu32 offset, u32 *val)\n{\n\tu32 tmp;\n\tint i;\n\n\tif (offset > EEPROM_ADDR_ADDR_MASK || (offset % 4) != 0)\n\t\treturn -EINVAL;\n\n\ttmp = tr32(GRC_EEPROM_ADDR) & ~(EEPROM_ADDR_ADDR_MASK |\n\t\t\t\t\tEEPROM_ADDR_DEVID_MASK |\n\t\t\t\t\tEEPROM_ADDR_READ);\n\ttw32(GRC_EEPROM_ADDR,\n\t     tmp |\n\t     (0 << EEPROM_ADDR_DEVID_SHIFT) |\n\t     ((offset << EEPROM_ADDR_ADDR_SHIFT) &\n\t      EEPROM_ADDR_ADDR_MASK) |\n\t     EEPROM_ADDR_READ | EEPROM_ADDR_START);\n\n\tfor (i = 0; i < 1000; i++) {\n\t\ttmp = tr32(GRC_EEPROM_ADDR);\n\n\t\tif (tmp & EEPROM_ADDR_COMPLETE)\n\t\t\tbreak;\n\t\tmsleep(1);\n\t}\n\tif (!(tmp & EEPROM_ADDR_COMPLETE))\n\t\treturn -EBUSY;\n\n\ttmp = tr32(GRC_EEPROM_DATA);\n\n\t/*\n\t * The data will always be opposite the native endian\n\t * format.  Perform a blind byteswap to compensate.\n\t */\n\t*val = swab32(tmp);\n\n\treturn 0;\n}\n\n#define NVRAM_CMD_TIMEOUT 10000\n\nstatic int tg3_nvram_exec_cmd(struct tg3 *tp, u32 nvram_cmd)\n{\n\tint i;\n\n\ttw32(NVRAM_CMD, nvram_cmd);\n\tfor (i = 0; i < NVRAM_CMD_TIMEOUT; i++) {\n\t\tudelay(10);\n\t\tif (tr32(NVRAM_CMD) & NVRAM_CMD_DONE) {\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == NVRAM_CMD_TIMEOUT)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic u32 tg3_nvram_phys_addr(struct tg3 *tp, u32 addr)\n{\n\tif (tg3_flag(tp, NVRAM) &&\n\t    tg3_flag(tp, NVRAM_BUFFERED) &&\n\t    tg3_flag(tp, FLASH) &&\n\t    !tg3_flag(tp, NO_NVRAM_ADDR_TRANS) &&\n\t    (tp->nvram_jedecnum == JEDEC_ATMEL))\n\n\t\taddr = ((addr / tp->nvram_pagesize) <<\n\t\t\tATMEL_AT45DB0X1B_PAGE_POS) +\n\t\t       (addr % tp->nvram_pagesize);\n\n\treturn addr;\n}\n\nstatic u32 tg3_nvram_logical_addr(struct tg3 *tp, u32 addr)\n{\n\tif (tg3_flag(tp, NVRAM) &&\n\t    tg3_flag(tp, NVRAM_BUFFERED) &&\n\t    tg3_flag(tp, FLASH) &&\n\t    !tg3_flag(tp, NO_NVRAM_ADDR_TRANS) &&\n\t    (tp->nvram_jedecnum == JEDEC_ATMEL))\n\n\t\taddr = ((addr >> ATMEL_AT45DB0X1B_PAGE_POS) *\n\t\t\ttp->nvram_pagesize) +\n\t\t       (addr & ((1 << ATMEL_AT45DB0X1B_PAGE_POS) - 1));\n\n\treturn addr;\n}\n\n/* NOTE: Data read in from NVRAM is byteswapped according to\n * the byteswapping settings for all other register accesses.\n * tg3 devices are BE devices, so on a BE machine, the data\n * returned will be exactly as it is seen in NVRAM.  On a LE\n * machine, the 32-bit value will be byteswapped.\n */\nstatic int tg3_nvram_read(struct tg3 *tp, u32 offset, u32 *val)\n{\n\tint ret;\n\n\tif (!tg3_flag(tp, NVRAM))\n\t\treturn tg3_nvram_read_using_eeprom(tp, offset, val);\n\n\toffset = tg3_nvram_phys_addr(tp, offset);\n\n\tif (offset > NVRAM_ADDR_MSK)\n\t\treturn -EINVAL;\n\n\tret = tg3_nvram_lock(tp);\n\tif (ret)\n\t\treturn ret;\n\n\ttg3_enable_nvram_access(tp);\n\n\ttw32(NVRAM_ADDR, offset);\n\tret = tg3_nvram_exec_cmd(tp, NVRAM_CMD_RD | NVRAM_CMD_GO |\n\t\tNVRAM_CMD_FIRST | NVRAM_CMD_LAST | NVRAM_CMD_DONE);\n\n\tif (ret == 0)\n\t\t*val = tr32(NVRAM_RDDATA);\n\n\ttg3_disable_nvram_access(tp);\n\n\ttg3_nvram_unlock(tp);\n\n\treturn ret;\n}\n\n/* Ensures NVRAM data is in bytestream format. */\nstatic int tg3_nvram_read_be32(struct tg3 *tp, u32 offset, __be32 *val)\n{\n\tu32 v;\n\tint res = tg3_nvram_read(tp, offset, &v);\n\tif (!res)\n\t\t*val = cpu_to_be32(v);\n\treturn res;\n}\n\nstatic int tg3_nvram_write_block_using_eeprom(struct tg3 *tp,\n\t\t\t\t    u32 offset, u32 len, u8 *buf)\n{\n\tint i, j, rc = 0;\n\tu32 val;\n\n\tfor (i = 0; i < len; i += 4) {\n\t\tu32 addr;\n\t\t__be32 data;\n\n\t\taddr = offset + i;\n\n\t\tmemcpy(&data, buf + i, 4);\n\n\t\t/*\n\t\t * The SEEPROM interface expects the data to always be opposite\n\t\t * the native endian format.  We accomplish this by reversing\n\t\t * all the operations that would have been performed on the\n\t\t * data from a call to tg3_nvram_read_be32().\n\t\t */\n\t\ttw32(GRC_EEPROM_DATA, swab32(be32_to_cpu(data)));\n\n\t\tval = tr32(GRC_EEPROM_ADDR);\n\t\ttw32(GRC_EEPROM_ADDR, val | EEPROM_ADDR_COMPLETE);\n\n\t\tval &= ~(EEPROM_ADDR_ADDR_MASK | EEPROM_ADDR_DEVID_MASK |\n\t\t\tEEPROM_ADDR_READ);\n\t\ttw32(GRC_EEPROM_ADDR, val |\n\t\t\t(0 << EEPROM_ADDR_DEVID_SHIFT) |\n\t\t\t(addr & EEPROM_ADDR_ADDR_MASK) |\n\t\t\tEEPROM_ADDR_START |\n\t\t\tEEPROM_ADDR_WRITE);\n\n\t\tfor (j = 0; j < 1000; j++) {\n\t\t\tval = tr32(GRC_EEPROM_ADDR);\n\n\t\t\tif (val & EEPROM_ADDR_COMPLETE)\n\t\t\t\tbreak;\n\t\t\tmsleep(1);\n\t\t}\n\t\tif (!(val & EEPROM_ADDR_COMPLETE)) {\n\t\t\trc = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\n/* offset and length are dword aligned */\nstatic int tg3_nvram_write_block_unbuffered(struct tg3 *tp, u32 offset, u32 len,\n\t\tu8 *buf)\n{\n\tint ret = 0;\n\tu32 pagesize = tp->nvram_pagesize;\n\tu32 pagemask = pagesize - 1;\n\tu32 nvram_cmd;\n\tu8 *tmp;\n\n\ttmp = kmalloc(pagesize, GFP_KERNEL);\n\tif (tmp == NULL)\n\t\treturn -ENOMEM;\n\n\twhile (len) {\n\t\tint j;\n\t\tu32 phy_addr, page_off, size;\n\n\t\tphy_addr = offset & ~pagemask;\n\n\t\tfor (j = 0; j < pagesize; j += 4) {\n\t\t\tret = tg3_nvram_read_be32(tp, phy_addr + j,\n\t\t\t\t\t\t  (__be32 *) (tmp + j));\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tpage_off = offset & pagemask;\n\t\tsize = pagesize;\n\t\tif (len < size)\n\t\t\tsize = len;\n\n\t\tlen -= size;\n\n\t\tmemcpy(tmp + page_off, buf, size);\n\n\t\toffset = offset + (pagesize - page_off);\n\n\t\ttg3_enable_nvram_access(tp);\n\n\t\t/*\n\t\t * Before we can erase the flash page, we need\n\t\t * to issue a special \"write enable\" command.\n\t\t */\n\t\tnvram_cmd = NVRAM_CMD_WREN | NVRAM_CMD_GO | NVRAM_CMD_DONE;\n\n\t\tif (tg3_nvram_exec_cmd(tp, nvram_cmd))\n\t\t\tbreak;\n\n\t\t/* Erase the target page */\n\t\ttw32(NVRAM_ADDR, phy_addr);\n\n\t\tnvram_cmd = NVRAM_CMD_GO | NVRAM_CMD_DONE | NVRAM_CMD_WR |\n\t\t\tNVRAM_CMD_FIRST | NVRAM_CMD_LAST | NVRAM_CMD_ERASE;\n\n\t\tif (tg3_nvram_exec_cmd(tp, nvram_cmd))\n\t\t\tbreak;\n\n\t\t/* Issue another write enable to start the write. */\n\t\tnvram_cmd = NVRAM_CMD_WREN | NVRAM_CMD_GO | NVRAM_CMD_DONE;\n\n\t\tif (tg3_nvram_exec_cmd(tp, nvram_cmd))\n\t\t\tbreak;\n\n\t\tfor (j = 0; j < pagesize; j += 4) {\n\t\t\t__be32 data;\n\n\t\t\tdata = *((__be32 *) (tmp + j));\n\n\t\t\ttw32(NVRAM_WRDATA, be32_to_cpu(data));\n\n\t\t\ttw32(NVRAM_ADDR, phy_addr + j);\n\n\t\t\tnvram_cmd = NVRAM_CMD_GO | NVRAM_CMD_DONE |\n\t\t\t\tNVRAM_CMD_WR;\n\n\t\t\tif (j == 0)\n\t\t\t\tnvram_cmd |= NVRAM_CMD_FIRST;\n\t\t\telse if (j == (pagesize - 4))\n\t\t\t\tnvram_cmd |= NVRAM_CMD_LAST;\n\n\t\t\tret = tg3_nvram_exec_cmd(tp, nvram_cmd);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tnvram_cmd = NVRAM_CMD_WRDI | NVRAM_CMD_GO | NVRAM_CMD_DONE;\n\ttg3_nvram_exec_cmd(tp, nvram_cmd);\n\n\tkfree(tmp);\n\n\treturn ret;\n}\n\n/* offset and length are dword aligned */\nstatic int tg3_nvram_write_block_buffered(struct tg3 *tp, u32 offset, u32 len,\n\t\tu8 *buf)\n{\n\tint i, ret = 0;\n\n\tfor (i = 0; i < len; i += 4, offset += 4) {\n\t\tu32 page_off, phy_addr, nvram_cmd;\n\t\t__be32 data;\n\n\t\tmemcpy(&data, buf + i, 4);\n\t\ttw32(NVRAM_WRDATA, be32_to_cpu(data));\n\n\t\tpage_off = offset % tp->nvram_pagesize;\n\n\t\tphy_addr = tg3_nvram_phys_addr(tp, offset);\n\n\t\tnvram_cmd = NVRAM_CMD_GO | NVRAM_CMD_DONE | NVRAM_CMD_WR;\n\n\t\tif (page_off == 0 || i == 0)\n\t\t\tnvram_cmd |= NVRAM_CMD_FIRST;\n\t\tif (page_off == (tp->nvram_pagesize - 4))\n\t\t\tnvram_cmd |= NVRAM_CMD_LAST;\n\n\t\tif (i == (len - 4))\n\t\t\tnvram_cmd |= NVRAM_CMD_LAST;\n\n\t\tif ((nvram_cmd & NVRAM_CMD_FIRST) ||\n\t\t    !tg3_flag(tp, FLASH) ||\n\t\t    !tg3_flag(tp, 57765_PLUS))\n\t\t\ttw32(NVRAM_ADDR, phy_addr);\n\n\t\tif (tg3_asic_rev(tp) != ASIC_REV_5752 &&\n\t\t    !tg3_flag(tp, 5755_PLUS) &&\n\t\t    (tp->nvram_jedecnum == JEDEC_ST) &&\n\t\t    (nvram_cmd & NVRAM_CMD_FIRST)) {\n\t\t\tu32 cmd;\n\n\t\t\tcmd = NVRAM_CMD_WREN | NVRAM_CMD_GO | NVRAM_CMD_DONE;\n\t\t\tret = tg3_nvram_exec_cmd(tp, cmd);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!tg3_flag(tp, FLASH)) {\n\t\t\t/* We always do complete word writes to eeprom. */\n\t\t\tnvram_cmd |= (NVRAM_CMD_FIRST | NVRAM_CMD_LAST);\n\t\t}\n\n\t\tret = tg3_nvram_exec_cmd(tp, nvram_cmd);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/* offset and length are dword aligned */\nstatic int tg3_nvram_write_block(struct tg3 *tp, u32 offset, u32 len, u8 *buf)\n{\n\tint ret;\n\n\tif (tg3_flag(tp, EEPROM_WRITE_PROT)) {\n\t\ttw32_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl &\n\t\t       ~GRC_LCLCTRL_GPIO_OUTPUT1);\n\t\tudelay(40);\n\t}\n\n\tif (!tg3_flag(tp, NVRAM)) {\n\t\tret = tg3_nvram_write_block_using_eeprom(tp, offset, len, buf);\n\t} else {\n\t\tu32 grc_mode;\n\n\t\tret = tg3_nvram_lock(tp);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\ttg3_enable_nvram_access(tp);\n\t\tif (tg3_flag(tp, 5750_PLUS) && !tg3_flag(tp, PROTECTED_NVRAM))\n\t\t\ttw32(NVRAM_WRITE1, 0x406);\n\n\t\tgrc_mode = tr32(GRC_MODE);\n\t\ttw32(GRC_MODE, grc_mode | GRC_MODE_NVRAM_WR_ENABLE);\n\n\t\tif (tg3_flag(tp, NVRAM_BUFFERED) || !tg3_flag(tp, FLASH)) {\n\t\t\tret = tg3_nvram_write_block_buffered(tp, offset, len,\n\t\t\t\tbuf);\n\t\t} else {\n\t\t\tret = tg3_nvram_write_block_unbuffered(tp, offset, len,\n\t\t\t\tbuf);\n\t\t}\n\n\t\tgrc_mode = tr32(GRC_MODE);\n\t\ttw32(GRC_MODE, grc_mode & ~GRC_MODE_NVRAM_WR_ENABLE);\n\n\t\ttg3_disable_nvram_access(tp);\n\t\ttg3_nvram_unlock(tp);\n\t}\n\n\tif (tg3_flag(tp, EEPROM_WRITE_PROT)) {\n\t\ttw32_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl);\n\t\tudelay(40);\n\t}\n\n\treturn ret;\n}\n\n#define RX_CPU_SCRATCH_BASE\t0x30000\n#define RX_CPU_SCRATCH_SIZE\t0x04000\n#define TX_CPU_SCRATCH_BASE\t0x34000\n#define TX_CPU_SCRATCH_SIZE\t0x04000\n\n/* tp->lock is held. */\nstatic int tg3_halt_cpu(struct tg3 *tp, u32 offset)\n{\n\tint i;\n\n\tBUG_ON(offset == TX_CPU_BASE && tg3_flag(tp, 5705_PLUS));\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tu32 val = tr32(GRC_VCPU_EXT_CTRL);\n\n\t\ttw32(GRC_VCPU_EXT_CTRL, val | GRC_VCPU_EXT_CTRL_HALT_CPU);\n\t\treturn 0;\n\t}\n\tif (offset == RX_CPU_BASE) {\n\t\tfor (i = 0; i < 10000; i++) {\n\t\t\ttw32(offset + CPU_STATE, 0xffffffff);\n\t\t\ttw32(offset + CPU_MODE,  CPU_MODE_HALT);\n\t\t\tif (tr32(offset + CPU_MODE) & CPU_MODE_HALT)\n\t\t\t\tbreak;\n\t\t}\n\n\t\ttw32(offset + CPU_STATE, 0xffffffff);\n\t\ttw32_f(offset + CPU_MODE,  CPU_MODE_HALT);\n\t\tudelay(10);\n\t} else {\n\t\t/*\n\t\t * There is only an Rx CPU for the 5750 derivative in the\n\t\t * BCM4785.\n\t\t */\n\t\tif (tg3_flag(tp, IS_SSB_CORE))\n\t\t\treturn 0;\n\n\t\tfor (i = 0; i < 10000; i++) {\n\t\t\ttw32(offset + CPU_STATE, 0xffffffff);\n\t\t\ttw32(offset + CPU_MODE,  CPU_MODE_HALT);\n\t\t\tif (tr32(offset + CPU_MODE) & CPU_MODE_HALT)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i >= 10000) {\n\t\tnetdev_err(tp->dev, \"%s timed out, %s CPU\\n\",\n\t\t\t   __func__, offset == RX_CPU_BASE ? \"RX\" : \"TX\");\n\t\treturn -ENODEV;\n\t}\n\n\t/* Clear firmware's nvram arbitration. */\n\tif (tg3_flag(tp, NVRAM))\n\t\ttw32(NVRAM_SWARB, SWARB_REQ_CLR0);\n\treturn 0;\n}\n\nstruct fw_info {\n\tunsigned int fw_base;\n\tunsigned int fw_len;\n\tconst __be32 *fw_data;\n};\n\n/* tp->lock is held. */\nstatic int tg3_load_firmware_cpu(struct tg3 *tp, u32 cpu_base,\n\t\t\t\t u32 cpu_scratch_base, int cpu_scratch_size,\n\t\t\t\t struct fw_info *info)\n{\n\tint err, lock_err, i;\n\tvoid (*write_op)(struct tg3 *, u32, u32);\n\n\tif (cpu_base == TX_CPU_BASE && tg3_flag(tp, 5705_PLUS)) {\n\t\tnetdev_err(tp->dev,\n\t\t\t   \"%s: Trying to load TX cpu firmware which is 5705\\n\",\n\t\t\t   __func__);\n\t\treturn -EINVAL;\n\t}\n\n\tif (tg3_flag(tp, 5705_PLUS))\n\t\twrite_op = tg3_write_mem;\n\telse\n\t\twrite_op = tg3_write_indirect_reg32;\n\n\t/* It is possible that bootcode is still loading at this point.\n\t * Get the nvram lock first before halting the cpu.\n\t */\n\tlock_err = tg3_nvram_lock(tp);\n\terr = tg3_halt_cpu(tp, cpu_base);\n\tif (!lock_err)\n\t\ttg3_nvram_unlock(tp);\n\tif (err)\n\t\tgoto out;\n\n\tfor (i = 0; i < cpu_scratch_size; i += sizeof(u32))\n\t\twrite_op(tp, cpu_scratch_base + i, 0);\n\ttw32(cpu_base + CPU_STATE, 0xffffffff);\n\ttw32(cpu_base + CPU_MODE, tr32(cpu_base+CPU_MODE)|CPU_MODE_HALT);\n\tfor (i = 0; i < (info->fw_len / sizeof(u32)); i++)\n\t\twrite_op(tp, (cpu_scratch_base +\n\t\t\t      (info->fw_base & 0xffff) +\n\t\t\t      (i * sizeof(u32))),\n\t\t\t      be32_to_cpu(info->fw_data[i]));\n\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/* tp->lock is held. */\nstatic int tg3_load_5701_a0_firmware_fix(struct tg3 *tp)\n{\n\tstruct fw_info info;\n\tconst __be32 *fw_data;\n\tint err, i;\n\n\tfw_data = (void *)tp->fw->data;\n\n\t/* Firmware blob starts with version numbers, followed by\n\t   start address and length. We are setting complete length.\n\t   length = end_address_of_bss - start_address_of_text.\n\t   Remainder is the blob to be loaded contiguously\n\t   from start address. */\n\n\tinfo.fw_base = be32_to_cpu(fw_data[1]);\n\tinfo.fw_len = tp->fw->size - 12;\n\tinfo.fw_data = &fw_data[3];\n\n\terr = tg3_load_firmware_cpu(tp, RX_CPU_BASE,\n\t\t\t\t    RX_CPU_SCRATCH_BASE, RX_CPU_SCRATCH_SIZE,\n\t\t\t\t    &info);\n\tif (err)\n\t\treturn err;\n\n\terr = tg3_load_firmware_cpu(tp, TX_CPU_BASE,\n\t\t\t\t    TX_CPU_SCRATCH_BASE, TX_CPU_SCRATCH_SIZE,\n\t\t\t\t    &info);\n\tif (err)\n\t\treturn err;\n\n\t/* Now startup only the RX cpu. */\n\ttw32(RX_CPU_BASE + CPU_STATE, 0xffffffff);\n\ttw32_f(RX_CPU_BASE + CPU_PC, info.fw_base);\n\n\tfor (i = 0; i < 5; i++) {\n\t\tif (tr32(RX_CPU_BASE + CPU_PC) == info.fw_base)\n\t\t\tbreak;\n\t\ttw32(RX_CPU_BASE + CPU_STATE, 0xffffffff);\n\t\ttw32(RX_CPU_BASE + CPU_MODE,  CPU_MODE_HALT);\n\t\ttw32_f(RX_CPU_BASE + CPU_PC, info.fw_base);\n\t\tudelay(1000);\n\t}\n\tif (i >= 5) {\n\t\tnetdev_err(tp->dev, \"%s fails to set RX CPU PC, is %08x \"\n\t\t\t   \"should be %08x\\n\", __func__,\n\t\t\t   tr32(RX_CPU_BASE + CPU_PC), info.fw_base);\n\t\treturn -ENODEV;\n\t}\n\ttw32(RX_CPU_BASE + CPU_STATE, 0xffffffff);\n\ttw32_f(RX_CPU_BASE + CPU_MODE,  0x00000000);\n\n\treturn 0;\n}\n\n/* tp->lock is held. */\nstatic int tg3_load_tso_firmware(struct tg3 *tp)\n{\n\tstruct fw_info info;\n\tconst __be32 *fw_data;\n\tunsigned long cpu_base, cpu_scratch_base, cpu_scratch_size;\n\tint err, i;\n\n\tif (tg3_flag(tp, HW_TSO_1) ||\n\t    tg3_flag(tp, HW_TSO_2) ||\n\t    tg3_flag(tp, HW_TSO_3))\n\t\treturn 0;\n\n\tfw_data = (void *)tp->fw->data;\n\n\t/* Firmware blob starts with version numbers, followed by\n\t   start address and length. We are setting complete length.\n\t   length = end_address_of_bss - start_address_of_text.\n\t   Remainder is the blob to be loaded contiguously\n\t   from start address. */\n\n\tinfo.fw_base = be32_to_cpu(fw_data[1]);\n\tcpu_scratch_size = tp->fw_len;\n\tinfo.fw_len = tp->fw->size - 12;\n\tinfo.fw_data = &fw_data[3];\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\tcpu_base = RX_CPU_BASE;\n\t\tcpu_scratch_base = NIC_SRAM_MBUF_POOL_BASE5705;\n\t} else {\n\t\tcpu_base = TX_CPU_BASE;\n\t\tcpu_scratch_base = TX_CPU_SCRATCH_BASE;\n\t\tcpu_scratch_size = TX_CPU_SCRATCH_SIZE;\n\t}\n\n\terr = tg3_load_firmware_cpu(tp, cpu_base,\n\t\t\t\t    cpu_scratch_base, cpu_scratch_size,\n\t\t\t\t    &info);\n\tif (err)\n\t\treturn err;\n\n\t/* Now startup the cpu. */\n\ttw32(cpu_base + CPU_STATE, 0xffffffff);\n\ttw32_f(cpu_base + CPU_PC, info.fw_base);\n\n\tfor (i = 0; i < 5; i++) {\n\t\tif (tr32(cpu_base + CPU_PC) == info.fw_base)\n\t\t\tbreak;\n\t\ttw32(cpu_base + CPU_STATE, 0xffffffff);\n\t\ttw32(cpu_base + CPU_MODE,  CPU_MODE_HALT);\n\t\ttw32_f(cpu_base + CPU_PC, info.fw_base);\n\t\tudelay(1000);\n\t}\n\tif (i >= 5) {\n\t\tnetdev_err(tp->dev,\n\t\t\t   \"%s fails to set CPU PC, is %08x should be %08x\\n\",\n\t\t\t   __func__, tr32(cpu_base + CPU_PC), info.fw_base);\n\t\treturn -ENODEV;\n\t}\n\ttw32(cpu_base + CPU_STATE, 0xffffffff);\n\ttw32_f(cpu_base + CPU_MODE,  0x00000000);\n\treturn 0;\n}\n\n\n/* tp->lock is held. */\nstatic void __tg3_set_mac_addr(struct tg3 *tp, int skip_mac_1)\n{\n\tu32 addr_high, addr_low;\n\tint i;\n\n\taddr_high = ((tp->dev->dev_addr[0] << 8) |\n\t\t     tp->dev->dev_addr[1]);\n\taddr_low = ((tp->dev->dev_addr[2] << 24) |\n\t\t    (tp->dev->dev_addr[3] << 16) |\n\t\t    (tp->dev->dev_addr[4] <<  8) |\n\t\t    (tp->dev->dev_addr[5] <<  0));\n\tfor (i = 0; i < 4; i++) {\n\t\tif (i == 1 && skip_mac_1)\n\t\t\tcontinue;\n\t\ttw32(MAC_ADDR_0_HIGH + (i * 8), addr_high);\n\t\ttw32(MAC_ADDR_0_LOW + (i * 8), addr_low);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5704) {\n\t\tfor (i = 0; i < 12; i++) {\n\t\t\ttw32(MAC_EXTADDR_0_HIGH + (i * 8), addr_high);\n\t\t\ttw32(MAC_EXTADDR_0_LOW + (i * 8), addr_low);\n\t\t}\n\t}\n\n\taddr_high = (tp->dev->dev_addr[0] +\n\t\t     tp->dev->dev_addr[1] +\n\t\t     tp->dev->dev_addr[2] +\n\t\t     tp->dev->dev_addr[3] +\n\t\t     tp->dev->dev_addr[4] +\n\t\t     tp->dev->dev_addr[5]) &\n\t\tTX_BACKOFF_SEED_MASK;\n\ttw32(MAC_TX_BACKOFF_SEED, addr_high);\n}\n\nstatic void tg3_enable_register_access(struct tg3 *tp)\n{\n\t/*\n\t * Make sure register accesses (indirect or otherwise) will function\n\t * correctly.\n\t */\n\tpci_write_config_dword(tp->pdev,\n\t\t\t       TG3PCI_MISC_HOST_CTRL, tp->misc_host_ctrl);\n}\n\nstatic int tg3_power_up(struct tg3 *tp)\n{\n\tint err;\n\n\ttg3_enable_register_access(tp);\n\n\terr = pci_set_power_state(tp->pdev, PCI_D0);\n\tif (!err) {\n\t\t/* Switch out of Vaux if it is a NIC */\n\t\ttg3_pwrsrc_switch_to_vmain(tp);\n\t} else {\n\t\tnetdev_err(tp->dev, \"Transition to D0 failed\\n\");\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_setup_phy(struct tg3 *, int);\n\nstatic int tg3_power_down_prepare(struct tg3 *tp)\n{\n\tu32 misc_host_ctrl;\n\tbool device_should_wake, do_low_power;\n\n\ttg3_enable_register_access(tp);\n\n\t/* Restore the CLKREQ setting. */\n\tif (tg3_flag(tp, CLKREQ_BUG))\n\t\tpcie_capability_set_word(tp->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t PCI_EXP_LNKCTL_CLKREQ_EN);\n\n\tmisc_host_ctrl = tr32(TG3PCI_MISC_HOST_CTRL);\n\ttw32(TG3PCI_MISC_HOST_CTRL,\n\t     misc_host_ctrl | MISC_HOST_CTRL_MASK_PCI_INT);\n\n\tdevice_should_wake = device_may_wakeup(&tp->pdev->dev) &&\n\t\t\t     tg3_flag(tp, WOL_ENABLE);\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tdo_low_power = false;\n\t\tif ((tp->phy_flags & TG3_PHYFLG_IS_CONNECTED) &&\n\t\t    !(tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)) {\n\t\t\tstruct phy_device *phydev;\n\t\t\tu32 phyid, advertising;\n\n\t\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\t\t\ttp->phy_flags |= TG3_PHYFLG_IS_LOW_POWER;\n\n\t\t\ttp->link_config.speed = phydev->speed;\n\t\t\ttp->link_config.duplex = phydev->duplex;\n\t\t\ttp->link_config.autoneg = phydev->autoneg;\n\t\t\ttp->link_config.advertising = phydev->advertising;\n\n\t\t\tadvertising = ADVERTISED_TP |\n\t\t\t\t      ADVERTISED_Pause |\n\t\t\t\t      ADVERTISED_Autoneg |\n\t\t\t\t      ADVERTISED_10baseT_Half;\n\n\t\t\tif (tg3_flag(tp, ENABLE_ASF) || device_should_wake) {\n\t\t\t\tif (tg3_flag(tp, WOL_SPEED_100MB))\n\t\t\t\t\tadvertising |=\n\t\t\t\t\t\tADVERTISED_100baseT_Half |\n\t\t\t\t\t\tADVERTISED_100baseT_Full |\n\t\t\t\t\t\tADVERTISED_10baseT_Full;\n\t\t\t\telse\n\t\t\t\t\tadvertising |= ADVERTISED_10baseT_Full;\n\t\t\t}\n\n\t\t\tphydev->advertising = advertising;\n\n\t\t\tphy_start_aneg(phydev);\n\n\t\t\tphyid = phydev->drv->phy_id & phydev->drv->phy_id_mask;\n\t\t\tif (phyid != PHY_ID_BCMAC131) {\n\t\t\t\tphyid &= PHY_BCM_OUI_MASK;\n\t\t\t\tif (phyid == PHY_BCM_OUI_1 ||\n\t\t\t\t    phyid == PHY_BCM_OUI_2 ||\n\t\t\t\t    phyid == PHY_BCM_OUI_3)\n\t\t\t\t\tdo_low_power = true;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tdo_low_power = true;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER))\n\t\t\ttp->phy_flags |= TG3_PHYFLG_IS_LOW_POWER;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\t\ttg3_setup_phy(tp, 0);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tu32 val;\n\n\t\tval = tr32(GRC_VCPU_EXT_CTRL);\n\t\ttw32(GRC_VCPU_EXT_CTRL, val | GRC_VCPU_EXT_CTRL_DISABLE_WOL);\n\t} else if (!tg3_flag(tp, ENABLE_ASF)) {\n\t\tint i;\n\t\tu32 val;\n\n\t\tfor (i = 0; i < 200; i++) {\n\t\t\ttg3_read_mem(tp, NIC_SRAM_FW_ASF_STATUS_MBOX, &val);\n\t\t\tif (val == ~NIC_SRAM_FIRMWARE_MBOX_MAGIC1)\n\t\t\t\tbreak;\n\t\t\tmsleep(1);\n\t\t}\n\t}\n\tif (tg3_flag(tp, WOL_CAP))\n\t\ttg3_write_mem(tp, NIC_SRAM_WOL_MBOX, WOL_SIGNATURE |\n\t\t\t\t\t\t     WOL_DRV_STATE_SHUTDOWN |\n\t\t\t\t\t\t     WOL_DRV_WOL |\n\t\t\t\t\t\t     WOL_SET_MAGIC_PKT);\n\n\tif (device_should_wake) {\n\t\tu32 mac_mode;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_PHY_SERDES)) {\n\t\t\tif (do_low_power &&\n\t\t\t    !(tp->phy_flags & TG3_PHYFLG_IS_FET)) {\n\t\t\t\ttg3_phy_auxctl_write(tp,\n\t\t\t\t\t       MII_TG3_AUXCTL_SHDWSEL_PWRCTL,\n\t\t\t\t\t       MII_TG3_AUXCTL_PCTL_WOL_EN |\n\t\t\t\t\t       MII_TG3_AUXCTL_PCTL_100TX_LPWR |\n\t\t\t\t\t       MII_TG3_AUXCTL_PCTL_CL_AB_TXDAC);\n\t\t\t\tudelay(40);\n\t\t\t}\n\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_MII_SERDES)\n\t\t\t\tmac_mode = MAC_MODE_PORT_MODE_GMII;\n\t\t\telse\n\t\t\t\tmac_mode = MAC_MODE_PORT_MODE_MII;\n\n\t\t\tmac_mode |= tp->mac_mode & MAC_MODE_LINK_POLARITY;\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700) {\n\t\t\t\tu32 speed = tg3_flag(tp, WOL_SPEED_100MB) ?\n\t\t\t\t\t     SPEED_100 : SPEED_10;\n\t\t\t\tif (tg3_5700_link_polarity(tp, speed))\n\t\t\t\t\tmac_mode |= MAC_MODE_LINK_POLARITY;\n\t\t\t\telse\n\t\t\t\t\tmac_mode &= ~MAC_MODE_LINK_POLARITY;\n\t\t\t}\n\t\t} else {\n\t\t\tmac_mode = MAC_MODE_PORT_MODE_TBI;\n\t\t}\n\n\t\tif (!tg3_flag(tp, 5750_PLUS))\n\t\t\ttw32(MAC_LED_CTRL, tp->led_ctrl);\n\n\t\tmac_mode |= MAC_MODE_MAGIC_PKT_ENABLE;\n\t\tif ((tg3_flag(tp, 5705_PLUS) && !tg3_flag(tp, 5780_CLASS)) &&\n\t\t    (tg3_flag(tp, ENABLE_ASF) || tg3_flag(tp, ENABLE_APE)))\n\t\t\tmac_mode |= MAC_MODE_KEEP_FRAME_IN_WOL;\n\n\t\tif (tg3_flag(tp, ENABLE_APE))\n\t\t\tmac_mode |= MAC_MODE_APE_TX_EN |\n\t\t\t\t    MAC_MODE_APE_RX_EN |\n\t\t\t\t    MAC_MODE_TDE_ENABLE;\n\n\t\ttw32_f(MAC_MODE, mac_mode);\n\t\tudelay(100);\n\n\t\ttw32_f(MAC_RX_MODE, RX_MODE_ENABLE);\n\t\tudelay(10);\n\t}\n\n\tif (!tg3_flag(tp, WOL_SPEED_100MB) &&\n\t    (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5701)) {\n\t\tu32 base_val;\n\n\t\tbase_val = tp->pci_clock_ctrl;\n\t\tbase_val |= (CLOCK_CTRL_RXCLK_DISABLE |\n\t\t\t     CLOCK_CTRL_TXCLK_DISABLE);\n\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL, base_val | CLOCK_CTRL_ALTCLK |\n\t\t\t    CLOCK_CTRL_PWRDOWN_PLL133, 40);\n\t} else if (tg3_flag(tp, 5780_CLASS) ||\n\t\t   tg3_flag(tp, CPMU_PRESENT) ||\n\t\t   tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t/* do nothing */\n\t} else if (!(tg3_flag(tp, 5750_PLUS) && tg3_flag(tp, ENABLE_ASF))) {\n\t\tu32 newbits1, newbits2;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\t\tnewbits1 = (CLOCK_CTRL_RXCLK_DISABLE |\n\t\t\t\t    CLOCK_CTRL_TXCLK_DISABLE |\n\t\t\t\t    CLOCK_CTRL_ALTCLK);\n\t\t\tnewbits2 = newbits1 | CLOCK_CTRL_44MHZ_CORE;\n\t\t} else if (tg3_flag(tp, 5705_PLUS)) {\n\t\t\tnewbits1 = CLOCK_CTRL_625_CORE;\n\t\t\tnewbits2 = newbits1 | CLOCK_CTRL_ALTCLK;\n\t\t} else {\n\t\t\tnewbits1 = CLOCK_CTRL_ALTCLK;\n\t\t\tnewbits2 = newbits1 | CLOCK_CTRL_44MHZ_CORE;\n\t\t}\n\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL, tp->pci_clock_ctrl | newbits1,\n\t\t\t    40);\n\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL, tp->pci_clock_ctrl | newbits2,\n\t\t\t    40);\n\n\t\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\t\tu32 newbits3;\n\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\t\t\tnewbits3 = (CLOCK_CTRL_RXCLK_DISABLE |\n\t\t\t\t\t    CLOCK_CTRL_TXCLK_DISABLE |\n\t\t\t\t\t    CLOCK_CTRL_44MHZ_CORE);\n\t\t\t} else {\n\t\t\t\tnewbits3 = CLOCK_CTRL_44MHZ_CORE;\n\t\t\t}\n\n\t\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL,\n\t\t\t\t    tp->pci_clock_ctrl | newbits3, 40);\n\t\t}\n\t}\n\n\tif (!(device_should_wake) && !tg3_flag(tp, ENABLE_ASF))\n\t\ttg3_power_down_phy(tp, do_low_power);\n\n\ttg3_frob_aux_power(tp, true);\n\n\t/* Workaround for unstable PLL clock */\n\tif ((!tg3_flag(tp, IS_SSB_CORE)) &&\n\t    ((tg3_chip_rev(tp) == CHIPREV_5750_AX) ||\n\t     (tg3_chip_rev(tp) == CHIPREV_5750_BX))) {\n\t\tu32 val = tr32(0x7d00);\n\n\t\tval &= ~((1 << 16) | (1 << 4) | (1 << 2) | (1 << 1) | 1);\n\t\ttw32(0x7d00, val);\n\t\tif (!tg3_flag(tp, ENABLE_ASF)) {\n\t\t\tint err;\n\n\t\t\terr = tg3_nvram_lock(tp);\n\t\t\ttg3_halt_cpu(tp, RX_CPU_BASE);\n\t\t\tif (!err)\n\t\t\t\ttg3_nvram_unlock(tp);\n\t\t}\n\t}\n\n\ttg3_write_sig_post_reset(tp, RESET_KIND_SHUTDOWN);\n\n\treturn 0;\n}\n\nstatic void tg3_power_down(struct tg3 *tp)\n{\n\ttg3_power_down_prepare(tp);\n\n\tpci_wake_from_d3(tp->pdev, tg3_flag(tp, WOL_ENABLE));\n\tpci_set_power_state(tp->pdev, PCI_D3hot);\n}\n\nstatic void tg3_aux_stat_to_speed_duplex(struct tg3 *tp, u32 val, u16 *speed, u8 *duplex)\n{\n\tswitch (val & MII_TG3_AUX_STAT_SPDMASK) {\n\tcase MII_TG3_AUX_STAT_10HALF:\n\t\t*speed = SPEED_10;\n\t\t*duplex = DUPLEX_HALF;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_10FULL:\n\t\t*speed = SPEED_10;\n\t\t*duplex = DUPLEX_FULL;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_100HALF:\n\t\t*speed = SPEED_100;\n\t\t*duplex = DUPLEX_HALF;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_100FULL:\n\t\t*speed = SPEED_100;\n\t\t*duplex = DUPLEX_FULL;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_1000HALF:\n\t\t*speed = SPEED_1000;\n\t\t*duplex = DUPLEX_HALF;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_1000FULL:\n\t\t*speed = SPEED_1000;\n\t\t*duplex = DUPLEX_FULL;\n\t\tbreak;\n\n\tdefault:\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\t\t*speed = (val & MII_TG3_AUX_STAT_100) ? SPEED_100 :\n\t\t\t\t SPEED_10;\n\t\t\t*duplex = (val & MII_TG3_AUX_STAT_FULL) ? DUPLEX_FULL :\n\t\t\t\t  DUPLEX_HALF;\n\t\t\tbreak;\n\t\t}\n\t\t*speed = SPEED_UNKNOWN;\n\t\t*duplex = DUPLEX_UNKNOWN;\n\t\tbreak;\n\t}\n}\n\nstatic int tg3_phy_autoneg_cfg(struct tg3 *tp, u32 advertise, u32 flowctrl)\n{\n\tint err = 0;\n\tu32 val, new_adv;\n\n\tnew_adv = ADVERTISE_CSMA;\n\tnew_adv |= ethtool_adv_to_mii_adv_t(advertise) & ADVERTISE_ALL;\n\tnew_adv |= mii_advertise_flowctrl(flowctrl);\n\n\terr = tg3_writephy(tp, MII_ADVERTISE, new_adv);\n\tif (err)\n\t\tgoto done;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY)) {\n\t\tnew_adv = ethtool_adv_to_mii_ctrl1000_t(advertise);\n\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0)\n\t\t\tnew_adv |= CTL1000_AS_MASTER | CTL1000_ENABLE_MASTER;\n\n\t\terr = tg3_writephy(tp, MII_CTRL1000, new_adv);\n\t\tif (err)\n\t\t\tgoto done;\n\t}\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_EEE_CAP))\n\t\tgoto done;\n\n\ttw32(TG3_CPMU_EEE_MODE,\n\t     tr32(TG3_CPMU_EEE_MODE) & ~TG3_CPMU_EEEMD_LPI_ENABLE);\n\n\terr = tg3_phy_toggle_auxctl_smdsp(tp, true);\n\tif (!err) {\n\t\tu32 err2;\n\n\t\tval = 0;\n\t\t/* Advertise 100-BaseTX EEE ability */\n\t\tif (advertise & ADVERTISED_100baseT_Full)\n\t\t\tval |= MDIO_AN_EEE_ADV_100TX;\n\t\t/* Advertise 1000-BaseT EEE ability */\n\t\tif (advertise & ADVERTISED_1000baseT_Full)\n\t\t\tval |= MDIO_AN_EEE_ADV_1000T;\n\t\terr = tg3_phy_cl45_write(tp, MDIO_MMD_AN, MDIO_AN_EEE_ADV, val);\n\t\tif (err)\n\t\t\tval = 0;\n\n\t\tswitch (tg3_asic_rev(tp)) {\n\t\tcase ASIC_REV_5717:\n\t\tcase ASIC_REV_57765:\n\t\tcase ASIC_REV_57766:\n\t\tcase ASIC_REV_5719:\n\t\t\t/* If we advertised any eee advertisements above... */\n\t\t\tif (val)\n\t\t\t\tval = MII_TG3_DSP_TAP26_ALNOKO |\n\t\t\t\t      MII_TG3_DSP_TAP26_RMRXSTO |\n\t\t\t\t      MII_TG3_DSP_TAP26_OPCSINPT;\n\t\t\ttg3_phydsp_write(tp, MII_TG3_DSP_TAP26, val);\n\t\t\t/* Fall through */\n\t\tcase ASIC_REV_5720:\n\t\tcase ASIC_REV_5762:\n\t\t\tif (!tg3_phydsp_read(tp, MII_TG3_DSP_CH34TP2, &val))\n\t\t\t\ttg3_phydsp_write(tp, MII_TG3_DSP_CH34TP2, val |\n\t\t\t\t\t\t MII_TG3_DSP_CH34TP2_HIBW01);\n\t\t}\n\n\t\terr2 = tg3_phy_toggle_auxctl_smdsp(tp, false);\n\t\tif (!err)\n\t\t\terr = err2;\n\t}\n\ndone:\n\treturn err;\n}\n\nstatic void tg3_phy_copper_begin(struct tg3 *tp)\n{\n\tif (tp->link_config.autoneg == AUTONEG_ENABLE ||\n\t    (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)) {\n\t\tu32 adv, fc;\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER) {\n\t\t\tadv = ADVERTISED_10baseT_Half |\n\t\t\t      ADVERTISED_10baseT_Full;\n\t\t\tif (tg3_flag(tp, WOL_SPEED_100MB))\n\t\t\t\tadv |= ADVERTISED_100baseT_Half |\n\t\t\t\t       ADVERTISED_100baseT_Full;\n\n\t\t\tfc = FLOW_CTRL_TX | FLOW_CTRL_RX;\n\t\t} else {\n\t\t\tadv = tp->link_config.advertising;\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_10_100_ONLY)\n\t\t\t\tadv &= ~(ADVERTISED_1000baseT_Half |\n\t\t\t\t\t ADVERTISED_1000baseT_Full);\n\n\t\t\tfc = tp->link_config.flowctrl;\n\t\t}\n\n\t\ttg3_phy_autoneg_cfg(tp, adv, fc);\n\n\t\ttg3_writephy(tp, MII_BMCR,\n\t\t\t     BMCR_ANENABLE | BMCR_ANRESTART);\n\t} else {\n\t\tint i;\n\t\tu32 bmcr, orig_bmcr;\n\n\t\ttp->link_config.active_speed = tp->link_config.speed;\n\t\ttp->link_config.active_duplex = tp->link_config.duplex;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\t\t/* With autoneg disabled, 5715 only links up when the\n\t\t\t * advertisement register has the configured speed\n\t\t\t * enabled.\n\t\t\t */\n\t\t\ttg3_writephy(tp, MII_ADVERTISE, ADVERTISE_ALL);\n\t\t}\n\n\t\tbmcr = 0;\n\t\tswitch (tp->link_config.speed) {\n\t\tdefault:\n\t\tcase SPEED_10:\n\t\t\tbreak;\n\n\t\tcase SPEED_100:\n\t\t\tbmcr |= BMCR_SPEED100;\n\t\t\tbreak;\n\n\t\tcase SPEED_1000:\n\t\t\tbmcr |= BMCR_SPEED1000;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tp->link_config.duplex == DUPLEX_FULL)\n\t\t\tbmcr |= BMCR_FULLDPLX;\n\n\t\tif (!tg3_readphy(tp, MII_BMCR, &orig_bmcr) &&\n\t\t    (bmcr != orig_bmcr)) {\n\t\t\ttg3_writephy(tp, MII_BMCR, BMCR_LOOPBACK);\n\t\t\tfor (i = 0; i < 1500; i++) {\n\t\t\t\tu32 tmp;\n\n\t\t\t\tudelay(10);\n\t\t\t\tif (tg3_readphy(tp, MII_BMSR, &tmp) ||\n\t\t\t\t    tg3_readphy(tp, MII_BMSR, &tmp))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!(tmp & BMSR_LSTATUS)) {\n\t\t\t\t\tudelay(40);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\ttg3_writephy(tp, MII_BMCR, bmcr);\n\t\t\tudelay(40);\n\t\t}\n\t}\n}\n\nstatic int tg3_init_5401phy_dsp(struct tg3 *tp)\n{\n\tint err;\n\n\t/* Turn off tap power management. */\n\t/* Set Extended packet length bit */\n\terr = tg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_AUXCTL, 0x4c20);\n\n\terr |= tg3_phydsp_write(tp, 0x0012, 0x1804);\n\terr |= tg3_phydsp_write(tp, 0x0013, 0x1204);\n\terr |= tg3_phydsp_write(tp, 0x8006, 0x0132);\n\terr |= tg3_phydsp_write(tp, 0x8006, 0x0232);\n\terr |= tg3_phydsp_write(tp, 0x201f, 0x0a20);\n\n\tudelay(40);\n\n\treturn err;\n}\n\nstatic bool tg3_phy_copper_an_config_ok(struct tg3 *tp, u32 *lcladv)\n{\n\tu32 advmsk, tgtadv, advertising;\n\n\tadvertising = tp->link_config.advertising;\n\ttgtadv = ethtool_adv_to_mii_adv_t(advertising) & ADVERTISE_ALL;\n\n\tadvmsk = ADVERTISE_ALL;\n\tif (tp->link_config.active_duplex == DUPLEX_FULL) {\n\t\ttgtadv |= mii_advertise_flowctrl(tp->link_config.flowctrl);\n\t\tadvmsk |= ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\n\t}\n\n\tif (tg3_readphy(tp, MII_ADVERTISE, lcladv))\n\t\treturn false;\n\n\tif ((*lcladv & advmsk) != tgtadv)\n\t\treturn false;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY)) {\n\t\tu32 tg3_ctrl;\n\n\t\ttgtadv = ethtool_adv_to_mii_ctrl1000_t(advertising);\n\n\t\tif (tg3_readphy(tp, MII_CTRL1000, &tg3_ctrl))\n\t\t\treturn false;\n\n\t\tif (tgtadv &&\n\t\t    (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t\t     tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0)) {\n\t\t\ttgtadv |= CTL1000_AS_MASTER | CTL1000_ENABLE_MASTER;\n\t\t\ttg3_ctrl &= (ADVERTISE_1000HALF | ADVERTISE_1000FULL |\n\t\t\t\t     CTL1000_AS_MASTER | CTL1000_ENABLE_MASTER);\n\t\t} else {\n\t\t\ttg3_ctrl &= (ADVERTISE_1000HALF | ADVERTISE_1000FULL);\n\t\t}\n\n\t\tif (tg3_ctrl != tgtadv)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool tg3_phy_copper_fetch_rmtadv(struct tg3 *tp, u32 *rmtadv)\n{\n\tu32 lpeth = 0;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY)) {\n\t\tu32 val;\n\n\t\tif (tg3_readphy(tp, MII_STAT1000, &val))\n\t\t\treturn false;\n\n\t\tlpeth = mii_stat1000_to_ethtool_lpa_t(val);\n\t}\n\n\tif (tg3_readphy(tp, MII_LPA, rmtadv))\n\t\treturn false;\n\n\tlpeth |= mii_lpa_to_ethtool_lpa_t(*rmtadv);\n\ttp->link_config.rmt_adv = lpeth;\n\n\treturn true;\n}\n\nstatic bool tg3_test_and_report_link_chg(struct tg3 *tp, int curr_link_up)\n{\n\tif (curr_link_up != tp->link_up) {\n\t\tif (curr_link_up) {\n\t\t\tnetif_carrier_on(tp->dev);\n\t\t} else {\n\t\t\tnetif_carrier_off(tp->dev);\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_MII_SERDES)\n\t\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t\t}\n\n\t\ttg3_link_report(tp);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int tg3_setup_copper_phy(struct tg3 *tp, int force_reset)\n{\n\tint current_link_up;\n\tu32 bmsr, val;\n\tu32 lcl_adv, rmt_adv;\n\tu16 current_speed;\n\tu8 current_duplex;\n\tint i, err;\n\n\ttw32(MAC_EVENT, 0);\n\n\ttw32_f(MAC_STATUS,\n\t     (MAC_STATUS_SYNC_CHANGED |\n\t      MAC_STATUS_CFG_CHANGED |\n\t      MAC_STATUS_MI_COMPLETION |\n\t      MAC_STATUS_LNKSTATE_CHANGED));\n\tudelay(40);\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE,\n\t\t     (tp->mi_mode & ~MAC_MI_MODE_AUTO_POLL));\n\t\tudelay(80);\n\t}\n\n\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_PWRCTL, 0);\n\n\t/* Some third-party PHYs need to be reset on link going\n\t * down.\n\t */\n\tif ((tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5705) &&\n\t    tp->link_up) {\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif (!tg3_readphy(tp, MII_BMSR, &bmsr) &&\n\t\t    !(bmsr & BMSR_LSTATUS))\n\t\t\tforce_reset = 1;\n\t}\n\tif (force_reset)\n\t\ttg3_phy_reset(tp);\n\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5401) {\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif (tg3_readphy(tp, MII_BMSR, &bmsr) ||\n\t\t    !tg3_flag(tp, INIT_COMPLETE))\n\t\t\tbmsr = 0;\n\n\t\tif (!(bmsr & BMSR_LSTATUS)) {\n\t\t\terr = tg3_init_5401phy_dsp(tp);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\t\tfor (i = 0; i < 1000; i++) {\n\t\t\t\tudelay(10);\n\t\t\t\tif (!tg3_readphy(tp, MII_BMSR, &bmsr) &&\n\t\t\t\t    (bmsr & BMSR_LSTATUS)) {\n\t\t\t\t\tudelay(40);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif ((tp->phy_id & TG3_PHY_ID_REV_MASK) ==\n\t\t\t    TG3_PHY_REV_BCM5401_B0 &&\n\t\t\t    !(bmsr & BMSR_LSTATUS) &&\n\t\t\t    tp->link_config.active_speed == SPEED_1000) {\n\t\t\t\terr = tg3_phy_reset(tp);\n\t\t\t\tif (!err)\n\t\t\t\t\terr = tg3_init_5401phy_dsp(tp);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t} else if (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t\t   tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0) {\n\t\t/* 5701 {A0,B0} CRC bug workaround */\n\t\ttg3_writephy(tp, 0x15, 0x0a75);\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8c68);\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8d68);\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8c68);\n\t}\n\n\t/* Clear pending interrupts... */\n\ttg3_readphy(tp, MII_TG3_ISTAT, &val);\n\ttg3_readphy(tp, MII_TG3_ISTAT, &val);\n\n\tif (tp->phy_flags & TG3_PHYFLG_USE_MI_INTERRUPT)\n\t\ttg3_writephy(tp, MII_TG3_IMASK, ~MII_TG3_INT_LINKCHG);\n\telse if (!(tp->phy_flags & TG3_PHYFLG_IS_FET))\n\t\ttg3_writephy(tp, MII_TG3_IMASK, ~0);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\tif (tp->led_ctrl == LED_CTRL_MODE_PHY_1)\n\t\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL,\n\t\t\t\t     MII_TG3_EXT_CTRL_LNK3_LED_MODE);\n\t\telse\n\t\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL, 0);\n\t}\n\n\tcurrent_link_up = 0;\n\tcurrent_speed = SPEED_UNKNOWN;\n\tcurrent_duplex = DUPLEX_UNKNOWN;\n\ttp->phy_flags &= ~TG3_PHYFLG_MDIX_STATE;\n\ttp->link_config.rmt_adv = 0;\n\n\tif (tp->phy_flags & TG3_PHYFLG_CAPACITIVE_COUPLING) {\n\t\terr = tg3_phy_auxctl_read(tp,\n\t\t\t\t\t  MII_TG3_AUXCTL_SHDWSEL_MISCTEST,\n\t\t\t\t\t  &val);\n\t\tif (!err && !(val & (1 << 10))) {\n\t\t\ttg3_phy_auxctl_write(tp,\n\t\t\t\t\t     MII_TG3_AUXCTL_SHDWSEL_MISCTEST,\n\t\t\t\t\t     val | (1 << 10));\n\t\t\tgoto relink;\n\t\t}\n\t}\n\n\tbmsr = 0;\n\tfor (i = 0; i < 100; i++) {\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif (!tg3_readphy(tp, MII_BMSR, &bmsr) &&\n\t\t    (bmsr & BMSR_LSTATUS))\n\t\t\tbreak;\n\t\tudelay(40);\n\t}\n\n\tif (bmsr & BMSR_LSTATUS) {\n\t\tu32 aux_stat, bmcr;\n\n\t\ttg3_readphy(tp, MII_TG3_AUX_STAT, &aux_stat);\n\t\tfor (i = 0; i < 2000; i++) {\n\t\t\tudelay(10);\n\t\t\tif (!tg3_readphy(tp, MII_TG3_AUX_STAT, &aux_stat) &&\n\t\t\t    aux_stat)\n\t\t\t\tbreak;\n\t\t}\n\n\t\ttg3_aux_stat_to_speed_duplex(tp, aux_stat,\n\t\t\t\t\t     &current_speed,\n\t\t\t\t\t     &current_duplex);\n\n\t\tbmcr = 0;\n\t\tfor (i = 0; i < 200; i++) {\n\t\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\t\t\tif (tg3_readphy(tp, MII_BMCR, &bmcr))\n\t\t\t\tcontinue;\n\t\t\tif (bmcr && bmcr != 0x7fff)\n\t\t\t\tbreak;\n\t\t\tudelay(10);\n\t\t}\n\n\t\tlcl_adv = 0;\n\t\trmt_adv = 0;\n\n\t\ttp->link_config.active_speed = current_speed;\n\t\ttp->link_config.active_duplex = current_duplex;\n\n\t\tif (tp->link_config.autoneg == AUTONEG_ENABLE) {\n\t\t\tif ((bmcr & BMCR_ANENABLE) &&\n\t\t\t    tg3_phy_copper_an_config_ok(tp, &lcl_adv) &&\n\t\t\t    tg3_phy_copper_fetch_rmtadv(tp, &rmt_adv))\n\t\t\t\tcurrent_link_up = 1;\n\t\t} else {\n\t\t\tif (!(bmcr & BMCR_ANENABLE) &&\n\t\t\t    tp->link_config.speed == current_speed &&\n\t\t\t    tp->link_config.duplex == current_duplex &&\n\t\t\t    tp->link_config.flowctrl ==\n\t\t\t    tp->link_config.active_flowctrl) {\n\t\t\t\tcurrent_link_up = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (current_link_up == 1 &&\n\t\t    tp->link_config.active_duplex == DUPLEX_FULL) {\n\t\t\tu32 reg, bit;\n\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\t\t\treg = MII_TG3_FET_GEN_STAT;\n\t\t\t\tbit = MII_TG3_FET_GEN_STAT_MDIXSTAT;\n\t\t\t} else {\n\t\t\t\treg = MII_TG3_EXT_STAT;\n\t\t\t\tbit = MII_TG3_EXT_STAT_MDIX;\n\t\t\t}\n\n\t\t\tif (!tg3_readphy(tp, reg, &val) && (val & bit))\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_MDIX_STATE;\n\n\t\t\ttg3_setup_flow_control(tp, lcl_adv, rmt_adv);\n\t\t}\n\t}\n\nrelink:\n\tif (current_link_up == 0 || (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)) {\n\t\ttg3_phy_copper_begin(tp);\n\n\t\tif (tg3_flag(tp, ROBOSWITCH)) {\n\t\t\tcurrent_link_up = 1;\n\t\t\t/* FIXME: when BCM5325 switch is used use 100 MBit/s */\n\t\t\tcurrent_speed = SPEED_1000;\n\t\t\tcurrent_duplex = DUPLEX_FULL;\n\t\t\ttp->link_config.active_speed = current_speed;\n\t\t\ttp->link_config.active_duplex = current_duplex;\n\t\t}\n\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif ((!tg3_readphy(tp, MII_BMSR, &bmsr) && (bmsr & BMSR_LSTATUS)) ||\n\t\t    (tp->mac_mode & MAC_MODE_PORT_INT_LPBACK))\n\t\t\tcurrent_link_up = 1;\n\t}\n\n\ttp->mac_mode &= ~MAC_MODE_PORT_MODE_MASK;\n\tif (current_link_up == 1) {\n\t\tif (tp->link_config.active_speed == SPEED_100 ||\n\t\t    tp->link_config.active_speed == SPEED_10)\n\t\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_MII;\n\t\telse\n\t\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_GMII;\n\t} else if (tp->phy_flags & TG3_PHYFLG_IS_FET)\n\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_MII;\n\telse\n\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_GMII;\n\n\t/* In order for the 5750 core in BCM4785 chip to work properly\n\t * in RGMII mode, the Led Control Register must be set up.\n\t */\n\tif (tg3_flag(tp, RGMII_MODE)) {\n\t\tu32 led_ctrl = tr32(MAC_LED_CTRL);\n\t\tled_ctrl &= ~(LED_CTRL_1000MBPS_ON | LED_CTRL_100MBPS_ON);\n\n\t\tif (tp->link_config.active_speed == SPEED_10)\n\t\t\tled_ctrl |= LED_CTRL_LNKLED_OVERRIDE;\n\t\telse if (tp->link_config.active_speed == SPEED_100)\n\t\t\tled_ctrl |= (LED_CTRL_LNKLED_OVERRIDE |\n\t\t\t\t     LED_CTRL_100MBPS_ON);\n\t\telse if (tp->link_config.active_speed == SPEED_1000)\n\t\t\tled_ctrl |= (LED_CTRL_LNKLED_OVERRIDE |\n\t\t\t\t     LED_CTRL_1000MBPS_ON);\n\n\t\ttw32(MAC_LED_CTRL, led_ctrl);\n\t\tudelay(40);\n\t}\n\n\ttp->mac_mode &= ~MAC_MODE_HALF_DUPLEX;\n\tif (tp->link_config.active_duplex == DUPLEX_HALF)\n\t\ttp->mac_mode |= MAC_MODE_HALF_DUPLEX;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700) {\n\t\tif (current_link_up == 1 &&\n\t\t    tg3_5700_link_polarity(tp, tp->link_config.active_speed))\n\t\t\ttp->mac_mode |= MAC_MODE_LINK_POLARITY;\n\t\telse\n\t\t\ttp->mac_mode &= ~MAC_MODE_LINK_POLARITY;\n\t}\n\n\t/* ??? Without this setting Netgear GA302T PHY does not\n\t * ??? send/receive packets...\n\t */\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5411 &&\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5700_ALTIMA) {\n\t\ttp->mi_mode |= MAC_MI_MODE_AUTO_POLL;\n\t\ttw32_f(MAC_MI_MODE, tp->mi_mode);\n\t\tudelay(80);\n\t}\n\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\ttg3_phy_eee_adjust(tp, current_link_up);\n\n\tif (tg3_flag(tp, USE_LINKCHG_REG)) {\n\t\t/* Polled via timer. */\n\t\ttw32_f(MAC_EVENT, 0);\n\t} else {\n\t\ttw32_f(MAC_EVENT, MAC_EVENT_LNKSTATE_CHANGED);\n\t}\n\tudelay(40);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 &&\n\t    current_link_up == 1 &&\n\t    tp->link_config.active_speed == SPEED_1000 &&\n\t    (tg3_flag(tp, PCIX_MODE) || tg3_flag(tp, PCI_HIGH_SPEED))) {\n\t\tudelay(120);\n\t\ttw32_f(MAC_STATUS,\n\t\t     (MAC_STATUS_SYNC_CHANGED |\n\t\t      MAC_STATUS_CFG_CHANGED));\n\t\tudelay(40);\n\t\ttg3_write_mem(tp,\n\t\t\t      NIC_SRAM_FIRMWARE_MBOX,\n\t\t\t      NIC_SRAM_FIRMWARE_MBOX_MAGIC2);\n\t}\n\n\t/* Prevent send BD corruption. */\n\tif (tg3_flag(tp, CLKREQ_BUG)) {\n\t\tif (tp->link_config.active_speed == SPEED_100 ||\n\t\t    tp->link_config.active_speed == SPEED_10)\n\t\t\tpcie_capability_clear_word(tp->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t   PCI_EXP_LNKCTL_CLKREQ_EN);\n\t\telse\n\t\t\tpcie_capability_set_word(tp->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t PCI_EXP_LNKCTL_CLKREQ_EN);\n\t}\n\n\ttg3_test_and_report_link_chg(tp, current_link_up);\n\n\treturn 0;\n}\n\nstruct tg3_fiber_aneginfo {\n\tint state;\n#define ANEG_STATE_UNKNOWN\t\t0\n#define ANEG_STATE_AN_ENABLE\t\t1\n#define ANEG_STATE_RESTART_INIT\t\t2\n#define ANEG_STATE_RESTART\t\t3\n#define ANEG_STATE_DISABLE_LINK_OK\t4\n#define ANEG_STATE_ABILITY_DETECT_INIT\t5\n#define ANEG_STATE_ABILITY_DETECT\t6\n#define ANEG_STATE_ACK_DETECT_INIT\t7\n#define ANEG_STATE_ACK_DETECT\t\t8\n#define ANEG_STATE_COMPLETE_ACK_INIT\t9\n#define ANEG_STATE_COMPLETE_ACK\t\t10\n#define ANEG_STATE_IDLE_DETECT_INIT\t11\n#define ANEG_STATE_IDLE_DETECT\t\t12\n#define ANEG_STATE_LINK_OK\t\t13\n#define ANEG_STATE_NEXT_PAGE_WAIT_INIT\t14\n#define ANEG_STATE_NEXT_PAGE_WAIT\t15\n\n\tu32 flags;\n#define MR_AN_ENABLE\t\t0x00000001\n#define MR_RESTART_AN\t\t0x00000002\n#define MR_AN_COMPLETE\t\t0x00000004\n#define MR_PAGE_RX\t\t0x00000008\n#define MR_NP_LOADED\t\t0x00000010\n#define MR_TOGGLE_TX\t\t0x00000020\n#define MR_LP_ADV_FULL_DUPLEX\t0x00000040\n#define MR_LP_ADV_HALF_DUPLEX\t0x00000080\n#define MR_LP_ADV_SYM_PAUSE\t0x00000100\n#define MR_LP_ADV_ASYM_PAUSE\t0x00000200\n#define MR_LP_ADV_REMOTE_FAULT1\t0x00000400\n#define MR_LP_ADV_REMOTE_FAULT2\t0x00000800\n#define MR_LP_ADV_NEXT_PAGE\t0x00001000\n#define MR_TOGGLE_RX\t\t0x00002000\n#define MR_NP_RX\t\t0x00004000\n\n#define MR_LINK_OK\t\t0x80000000\n\n\tunsigned long link_time, cur_time;\n\n\tu32 ability_match_cfg;\n\tint ability_match_count;\n\n\tchar ability_match, idle_match, ack_match;\n\n\tu32 txconfig, rxconfig;\n#define ANEG_CFG_NP\t\t0x00000080\n#define ANEG_CFG_ACK\t\t0x00000040\n#define ANEG_CFG_RF2\t\t0x00000020\n#define ANEG_CFG_RF1\t\t0x00000010\n#define ANEG_CFG_PS2\t\t0x00000001\n#define ANEG_CFG_PS1\t\t0x00008000\n#define ANEG_CFG_HD\t\t0x00004000\n#define ANEG_CFG_FD\t\t0x00002000\n#define ANEG_CFG_INVAL\t\t0x00001f06\n\n};\n#define ANEG_OK\t\t0\n#define ANEG_DONE\t1\n#define ANEG_TIMER_ENAB\t2\n#define ANEG_FAILED\t-1\n\n#define ANEG_STATE_SETTLE_TIME\t10000\n\nstatic int tg3_fiber_aneg_smachine(struct tg3 *tp,\n\t\t\t\t   struct tg3_fiber_aneginfo *ap)\n{\n\tu16 flowctrl;\n\tunsigned long delta;\n\tu32 rx_cfg_reg;\n\tint ret;\n\n\tif (ap->state == ANEG_STATE_UNKNOWN) {\n\t\tap->rxconfig = 0;\n\t\tap->link_time = 0;\n\t\tap->cur_time = 0;\n\t\tap->ability_match_cfg = 0;\n\t\tap->ability_match_count = 0;\n\t\tap->ability_match = 0;\n\t\tap->idle_match = 0;\n\t\tap->ack_match = 0;\n\t}\n\tap->cur_time++;\n\n\tif (tr32(MAC_STATUS) & MAC_STATUS_RCVD_CFG) {\n\t\trx_cfg_reg = tr32(MAC_RX_AUTO_NEG);\n\n\t\tif (rx_cfg_reg != ap->ability_match_cfg) {\n\t\t\tap->ability_match_cfg = rx_cfg_reg;\n\t\t\tap->ability_match = 0;\n\t\t\tap->ability_match_count = 0;\n\t\t} else {\n\t\t\tif (++ap->ability_match_count > 1) {\n\t\t\t\tap->ability_match = 1;\n\t\t\t\tap->ability_match_cfg = rx_cfg_reg;\n\t\t\t}\n\t\t}\n\t\tif (rx_cfg_reg & ANEG_CFG_ACK)\n\t\t\tap->ack_match = 1;\n\t\telse\n\t\t\tap->ack_match = 0;\n\n\t\tap->idle_match = 0;\n\t} else {\n\t\tap->idle_match = 1;\n\t\tap->ability_match_cfg = 0;\n\t\tap->ability_match_count = 0;\n\t\tap->ability_match = 0;\n\t\tap->ack_match = 0;\n\n\t\trx_cfg_reg = 0;\n\t}\n\n\tap->rxconfig = rx_cfg_reg;\n\tret = ANEG_OK;\n\n\tswitch (ap->state) {\n\tcase ANEG_STATE_UNKNOWN:\n\t\tif (ap->flags & (MR_AN_ENABLE | MR_RESTART_AN))\n\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\n\t\t/* fallthru */\n\tcase ANEG_STATE_AN_ENABLE:\n\t\tap->flags &= ~(MR_AN_COMPLETE | MR_PAGE_RX);\n\t\tif (ap->flags & MR_AN_ENABLE) {\n\t\t\tap->link_time = 0;\n\t\t\tap->cur_time = 0;\n\t\t\tap->ability_match_cfg = 0;\n\t\t\tap->ability_match_count = 0;\n\t\t\tap->ability_match = 0;\n\t\t\tap->idle_match = 0;\n\t\t\tap->ack_match = 0;\n\n\t\t\tap->state = ANEG_STATE_RESTART_INIT;\n\t\t} else {\n\t\t\tap->state = ANEG_STATE_DISABLE_LINK_OK;\n\t\t}\n\t\tbreak;\n\n\tcase ANEG_STATE_RESTART_INIT:\n\t\tap->link_time = ap->cur_time;\n\t\tap->flags &= ~(MR_NP_LOADED);\n\t\tap->txconfig = 0;\n\t\ttw32(MAC_TX_AUTO_NEG, 0);\n\t\ttp->mac_mode |= MAC_MODE_SEND_CONFIGS;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\n\t\tret = ANEG_TIMER_ENAB;\n\t\tap->state = ANEG_STATE_RESTART;\n\n\t\t/* fallthru */\n\tcase ANEG_STATE_RESTART:\n\t\tdelta = ap->cur_time - ap->link_time;\n\t\tif (delta > ANEG_STATE_SETTLE_TIME)\n\t\t\tap->state = ANEG_STATE_ABILITY_DETECT_INIT;\n\t\telse\n\t\t\tret = ANEG_TIMER_ENAB;\n\t\tbreak;\n\n\tcase ANEG_STATE_DISABLE_LINK_OK:\n\t\tret = ANEG_DONE;\n\t\tbreak;\n\n\tcase ANEG_STATE_ABILITY_DETECT_INIT:\n\t\tap->flags &= ~(MR_TOGGLE_TX);\n\t\tap->txconfig = ANEG_CFG_FD;\n\t\tflowctrl = tg3_advert_flowctrl_1000X(tp->link_config.flowctrl);\n\t\tif (flowctrl & ADVERTISE_1000XPAUSE)\n\t\t\tap->txconfig |= ANEG_CFG_PS1;\n\t\tif (flowctrl & ADVERTISE_1000XPSE_ASYM)\n\t\t\tap->txconfig |= ANEG_CFG_PS2;\n\t\ttw32(MAC_TX_AUTO_NEG, ap->txconfig);\n\t\ttp->mac_mode |= MAC_MODE_SEND_CONFIGS;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\n\t\tap->state = ANEG_STATE_ABILITY_DETECT;\n\t\tbreak;\n\n\tcase ANEG_STATE_ABILITY_DETECT:\n\t\tif (ap->ability_match != 0 && ap->rxconfig != 0)\n\t\t\tap->state = ANEG_STATE_ACK_DETECT_INIT;\n\t\tbreak;\n\n\tcase ANEG_STATE_ACK_DETECT_INIT:\n\t\tap->txconfig |= ANEG_CFG_ACK;\n\t\ttw32(MAC_TX_AUTO_NEG, ap->txconfig);\n\t\ttp->mac_mode |= MAC_MODE_SEND_CONFIGS;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\n\t\tap->state = ANEG_STATE_ACK_DETECT;\n\n\t\t/* fallthru */\n\tcase ANEG_STATE_ACK_DETECT:\n\t\tif (ap->ack_match != 0) {\n\t\t\tif ((ap->rxconfig & ~ANEG_CFG_ACK) ==\n\t\t\t    (ap->ability_match_cfg & ~ANEG_CFG_ACK)) {\n\t\t\t\tap->state = ANEG_STATE_COMPLETE_ACK_INIT;\n\t\t\t} else {\n\t\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\t\t\t}\n\t\t} else if (ap->ability_match != 0 &&\n\t\t\t   ap->rxconfig == 0) {\n\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\t\t}\n\t\tbreak;\n\n\tcase ANEG_STATE_COMPLETE_ACK_INIT:\n\t\tif (ap->rxconfig & ANEG_CFG_INVAL) {\n\t\t\tret = ANEG_FAILED;\n\t\t\tbreak;\n\t\t}\n\t\tap->flags &= ~(MR_LP_ADV_FULL_DUPLEX |\n\t\t\t       MR_LP_ADV_HALF_DUPLEX |\n\t\t\t       MR_LP_ADV_SYM_PAUSE |\n\t\t\t       MR_LP_ADV_ASYM_PAUSE |\n\t\t\t       MR_LP_ADV_REMOTE_FAULT1 |\n\t\t\t       MR_LP_ADV_REMOTE_FAULT2 |\n\t\t\t       MR_LP_ADV_NEXT_PAGE |\n\t\t\t       MR_TOGGLE_RX |\n\t\t\t       MR_NP_RX);\n\t\tif (ap->rxconfig & ANEG_CFG_FD)\n\t\t\tap->flags |= MR_LP_ADV_FULL_DUPLEX;\n\t\tif (ap->rxconfig & ANEG_CFG_HD)\n\t\t\tap->flags |= MR_LP_ADV_HALF_DUPLEX;\n\t\tif (ap->rxconfig & ANEG_CFG_PS1)\n\t\t\tap->flags |= MR_LP_ADV_SYM_PAUSE;\n\t\tif (ap->rxconfig & ANEG_CFG_PS2)\n\t\t\tap->flags |= MR_LP_ADV_ASYM_PAUSE;\n\t\tif (ap->rxconfig & ANEG_CFG_RF1)\n\t\t\tap->flags |= MR_LP_ADV_REMOTE_FAULT1;\n\t\tif (ap->rxconfig & ANEG_CFG_RF2)\n\t\t\tap->flags |= MR_LP_ADV_REMOTE_FAULT2;\n\t\tif (ap->rxconfig & ANEG_CFG_NP)\n\t\t\tap->flags |= MR_LP_ADV_NEXT_PAGE;\n\n\t\tap->link_time = ap->cur_time;\n\n\t\tap->flags ^= (MR_TOGGLE_TX);\n\t\tif (ap->rxconfig & 0x0008)\n\t\t\tap->flags |= MR_TOGGLE_RX;\n\t\tif (ap->rxconfig & ANEG_CFG_NP)\n\t\t\tap->flags |= MR_NP_RX;\n\t\tap->flags |= MR_PAGE_RX;\n\n\t\tap->state = ANEG_STATE_COMPLETE_ACK;\n\t\tret = ANEG_TIMER_ENAB;\n\t\tbreak;\n\n\tcase ANEG_STATE_COMPLETE_ACK:\n\t\tif (ap->ability_match != 0 &&\n\t\t    ap->rxconfig == 0) {\n\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\t\t\tbreak;\n\t\t}\n\t\tdelta = ap->cur_time - ap->link_time;\n\t\tif (delta > ANEG_STATE_SETTLE_TIME) {\n\t\t\tif (!(ap->flags & (MR_LP_ADV_NEXT_PAGE))) {\n\t\t\t\tap->state = ANEG_STATE_IDLE_DETECT_INIT;\n\t\t\t} else {\n\t\t\t\tif ((ap->txconfig & ANEG_CFG_NP) == 0 &&\n\t\t\t\t    !(ap->flags & MR_NP_RX)) {\n\t\t\t\t\tap->state = ANEG_STATE_IDLE_DETECT_INIT;\n\t\t\t\t} else {\n\t\t\t\t\tret = ANEG_FAILED;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase ANEG_STATE_IDLE_DETECT_INIT:\n\t\tap->link_time = ap->cur_time;\n\t\ttp->mac_mode &= ~MAC_MODE_SEND_CONFIGS;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\n\t\tap->state = ANEG_STATE_IDLE_DETECT;\n\t\tret = ANEG_TIMER_ENAB;\n\t\tbreak;\n\n\tcase ANEG_STATE_IDLE_DETECT:\n\t\tif (ap->ability_match != 0 &&\n\t\t    ap->rxconfig == 0) {\n\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\t\t\tbreak;\n\t\t}\n\t\tdelta = ap->cur_time - ap->link_time;\n\t\tif (delta > ANEG_STATE_SETTLE_TIME) {\n\t\t\t/* XXX another gem from the Broadcom driver :( */\n\t\t\tap->state = ANEG_STATE_LINK_OK;\n\t\t}\n\t\tbreak;\n\n\tcase ANEG_STATE_LINK_OK:\n\t\tap->flags |= (MR_AN_COMPLETE | MR_LINK_OK);\n\t\tret = ANEG_DONE;\n\t\tbreak;\n\n\tcase ANEG_STATE_NEXT_PAGE_WAIT_INIT:\n\t\t/* ??? unimplemented */\n\t\tbreak;\n\n\tcase ANEG_STATE_NEXT_PAGE_WAIT:\n\t\t/* ??? unimplemented */\n\t\tbreak;\n\n\tdefault:\n\t\tret = ANEG_FAILED;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int fiber_autoneg(struct tg3 *tp, u32 *txflags, u32 *rxflags)\n{\n\tint res = 0;\n\tstruct tg3_fiber_aneginfo aninfo;\n\tint status = ANEG_FAILED;\n\tunsigned int tick;\n\tu32 tmp;\n\n\ttw32_f(MAC_TX_AUTO_NEG, 0);\n\n\ttmp = tp->mac_mode & ~MAC_MODE_PORT_MODE_MASK;\n\ttw32_f(MAC_MODE, tmp | MAC_MODE_PORT_MODE_GMII);\n\tudelay(40);\n\n\ttw32_f(MAC_MODE, tp->mac_mode | MAC_MODE_SEND_CONFIGS);\n\tudelay(40);\n\n\tmemset(&aninfo, 0, sizeof(aninfo));\n\taninfo.flags |= MR_AN_ENABLE;\n\taninfo.state = ANEG_STATE_UNKNOWN;\n\taninfo.cur_time = 0;\n\ttick = 0;\n\twhile (++tick < 195000) {\n\t\tstatus = tg3_fiber_aneg_smachine(tp, &aninfo);\n\t\tif (status == ANEG_DONE || status == ANEG_FAILED)\n\t\t\tbreak;\n\n\t\tudelay(1);\n\t}\n\n\ttp->mac_mode &= ~MAC_MODE_SEND_CONFIGS;\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\t*txflags = aninfo.txconfig;\n\t*rxflags = aninfo.flags;\n\n\tif (status == ANEG_DONE &&\n\t    (aninfo.flags & (MR_AN_COMPLETE | MR_LINK_OK |\n\t\t\t     MR_LP_ADV_FULL_DUPLEX)))\n\t\tres = 1;\n\n\treturn res;\n}\n\nstatic void tg3_init_bcm8002(struct tg3 *tp)\n{\n\tu32 mac_status = tr32(MAC_STATUS);\n\tint i;\n\n\t/* Reset when initting first time or we have a link. */\n\tif (tg3_flag(tp, INIT_COMPLETE) &&\n\t    !(mac_status & MAC_STATUS_PCS_SYNCED))\n\t\treturn;\n\n\t/* Set PLL lock range. */\n\ttg3_writephy(tp, 0x16, 0x8007);\n\n\t/* SW reset */\n\ttg3_writephy(tp, MII_BMCR, BMCR_RESET);\n\n\t/* Wait for reset to complete. */\n\t/* XXX schedule_timeout() ... */\n\tfor (i = 0; i < 500; i++)\n\t\tudelay(10);\n\n\t/* Config mode; select PMA/Ch 1 regs. */\n\ttg3_writephy(tp, 0x10, 0x8411);\n\n\t/* Enable auto-lock and comdet, select txclk for tx. */\n\ttg3_writephy(tp, 0x11, 0x0a10);\n\n\ttg3_writephy(tp, 0x18, 0x00a0);\n\ttg3_writephy(tp, 0x16, 0x41ff);\n\n\t/* Assert and deassert POR. */\n\ttg3_writephy(tp, 0x13, 0x0400);\n\tudelay(40);\n\ttg3_writephy(tp, 0x13, 0x0000);\n\n\ttg3_writephy(tp, 0x11, 0x0a50);\n\tudelay(40);\n\ttg3_writephy(tp, 0x11, 0x0a10);\n\n\t/* Wait for signal to stabilize */\n\t/* XXX schedule_timeout() ... */\n\tfor (i = 0; i < 15000; i++)\n\t\tudelay(10);\n\n\t/* Deselect the channel register so we can read the PHYID\n\t * later.\n\t */\n\ttg3_writephy(tp, 0x10, 0x8011);\n}\n\nstatic int tg3_setup_fiber_hw_autoneg(struct tg3 *tp, u32 mac_status)\n{\n\tu16 flowctrl;\n\tu32 sg_dig_ctrl, sg_dig_status;\n\tu32 serdes_cfg, expected_sg_dig_ctrl;\n\tint workaround, port_a;\n\tint current_link_up;\n\n\tserdes_cfg = 0;\n\texpected_sg_dig_ctrl = 0;\n\tworkaround = 0;\n\tport_a = 1;\n\tcurrent_link_up = 0;\n\n\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5704_A0 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5704_A1) {\n\t\tworkaround = 1;\n\t\tif (tr32(TG3PCI_DUAL_MAC_CTRL) & DUAL_MAC_CTRL_ID)\n\t\t\tport_a = 0;\n\n\t\t/* preserve bits 0-11,13,14 for signal pre-emphasis */\n\t\t/* preserve bits 20-23 for voltage regulator */\n\t\tserdes_cfg = tr32(MAC_SERDES_CFG) & 0x00f06fff;\n\t}\n\n\tsg_dig_ctrl = tr32(SG_DIG_CTRL);\n\n\tif (tp->link_config.autoneg != AUTONEG_ENABLE) {\n\t\tif (sg_dig_ctrl & SG_DIG_USING_HW_AUTONEG) {\n\t\t\tif (workaround) {\n\t\t\t\tu32 val = serdes_cfg;\n\n\t\t\t\tif (port_a)\n\t\t\t\t\tval |= 0xc010000;\n\t\t\t\telse\n\t\t\t\t\tval |= 0x4010000;\n\t\t\t\ttw32_f(MAC_SERDES_CFG, val);\n\t\t\t}\n\n\t\t\ttw32_f(SG_DIG_CTRL, SG_DIG_COMMON_SETUP);\n\t\t}\n\t\tif (mac_status & MAC_STATUS_PCS_SYNCED) {\n\t\t\ttg3_setup_flow_control(tp, 0, 0);\n\t\t\tcurrent_link_up = 1;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Want auto-negotiation.  */\n\texpected_sg_dig_ctrl = SG_DIG_USING_HW_AUTONEG | SG_DIG_COMMON_SETUP;\n\n\tflowctrl = tg3_advert_flowctrl_1000X(tp->link_config.flowctrl);\n\tif (flowctrl & ADVERTISE_1000XPAUSE)\n\t\texpected_sg_dig_ctrl |= SG_DIG_PAUSE_CAP;\n\tif (flowctrl & ADVERTISE_1000XPSE_ASYM)\n\t\texpected_sg_dig_ctrl |= SG_DIG_ASYM_PAUSE;\n\n\tif (sg_dig_ctrl != expected_sg_dig_ctrl) {\n\t\tif ((tp->phy_flags & TG3_PHYFLG_PARALLEL_DETECT) &&\n\t\t    tp->serdes_counter &&\n\t\t    ((mac_status & (MAC_STATUS_PCS_SYNCED |\n\t\t\t\t    MAC_STATUS_RCVD_CFG)) ==\n\t\t     MAC_STATUS_PCS_SYNCED)) {\n\t\t\ttp->serdes_counter--;\n\t\t\tcurrent_link_up = 1;\n\t\t\tgoto out;\n\t\t}\nrestart_autoneg:\n\t\tif (workaround)\n\t\t\ttw32_f(MAC_SERDES_CFG, serdes_cfg | 0xc011000);\n\t\ttw32_f(SG_DIG_CTRL, expected_sg_dig_ctrl | SG_DIG_SOFT_RESET);\n\t\tudelay(5);\n\t\ttw32_f(SG_DIG_CTRL, expected_sg_dig_ctrl);\n\n\t\ttp->serdes_counter = SERDES_AN_TIMEOUT_5704S;\n\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t} else if (mac_status & (MAC_STATUS_PCS_SYNCED |\n\t\t\t\t MAC_STATUS_SIGNAL_DET)) {\n\t\tsg_dig_status = tr32(SG_DIG_STATUS);\n\t\tmac_status = tr32(MAC_STATUS);\n\n\t\tif ((sg_dig_status & SG_DIG_AUTONEG_COMPLETE) &&\n\t\t    (mac_status & MAC_STATUS_PCS_SYNCED)) {\n\t\t\tu32 local_adv = 0, remote_adv = 0;\n\n\t\t\tif (sg_dig_ctrl & SG_DIG_PAUSE_CAP)\n\t\t\t\tlocal_adv |= ADVERTISE_1000XPAUSE;\n\t\t\tif (sg_dig_ctrl & SG_DIG_ASYM_PAUSE)\n\t\t\t\tlocal_adv |= ADVERTISE_1000XPSE_ASYM;\n\n\t\t\tif (sg_dig_status & SG_DIG_PARTNER_PAUSE_CAPABLE)\n\t\t\t\tremote_adv |= LPA_1000XPAUSE;\n\t\t\tif (sg_dig_status & SG_DIG_PARTNER_ASYM_PAUSE)\n\t\t\t\tremote_adv |= LPA_1000XPAUSE_ASYM;\n\n\t\t\ttp->link_config.rmt_adv =\n\t\t\t\t\t   mii_adv_to_ethtool_adv_x(remote_adv);\n\n\t\t\ttg3_setup_flow_control(tp, local_adv, remote_adv);\n\t\t\tcurrent_link_up = 1;\n\t\t\ttp->serdes_counter = 0;\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t\t} else if (!(sg_dig_status & SG_DIG_AUTONEG_COMPLETE)) {\n\t\t\tif (tp->serdes_counter)\n\t\t\t\ttp->serdes_counter--;\n\t\t\telse {\n\t\t\t\tif (workaround) {\n\t\t\t\t\tu32 val = serdes_cfg;\n\n\t\t\t\t\tif (port_a)\n\t\t\t\t\t\tval |= 0xc010000;\n\t\t\t\t\telse\n\t\t\t\t\t\tval |= 0x4010000;\n\n\t\t\t\t\ttw32_f(MAC_SERDES_CFG, val);\n\t\t\t\t}\n\n\t\t\t\ttw32_f(SG_DIG_CTRL, SG_DIG_COMMON_SETUP);\n\t\t\t\tudelay(40);\n\n\t\t\t\t/* Link parallel detection - link is up */\n\t\t\t\t/* only if we have PCS_SYNC and not */\n\t\t\t\t/* receiving config code words */\n\t\t\t\tmac_status = tr32(MAC_STATUS);\n\t\t\t\tif ((mac_status & MAC_STATUS_PCS_SYNCED) &&\n\t\t\t\t    !(mac_status & MAC_STATUS_RCVD_CFG)) {\n\t\t\t\t\ttg3_setup_flow_control(tp, 0, 0);\n\t\t\t\t\tcurrent_link_up = 1;\n\t\t\t\t\ttp->phy_flags |=\n\t\t\t\t\t\tTG3_PHYFLG_PARALLEL_DETECT;\n\t\t\t\t\ttp->serdes_counter =\n\t\t\t\t\t\tSERDES_PARALLEL_DET_TIMEOUT;\n\t\t\t\t} else\n\t\t\t\t\tgoto restart_autoneg;\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttp->serdes_counter = SERDES_AN_TIMEOUT_5704S;\n\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t}\n\nout:\n\treturn current_link_up;\n}\n\nstatic int tg3_setup_fiber_by_hand(struct tg3 *tp, u32 mac_status)\n{\n\tint current_link_up = 0;\n\n\tif (!(mac_status & MAC_STATUS_PCS_SYNCED))\n\t\tgoto out;\n\n\tif (tp->link_config.autoneg == AUTONEG_ENABLE) {\n\t\tu32 txflags, rxflags;\n\t\tint i;\n\n\t\tif (fiber_autoneg(tp, &txflags, &rxflags)) {\n\t\t\tu32 local_adv = 0, remote_adv = 0;\n\n\t\t\tif (txflags & ANEG_CFG_PS1)\n\t\t\t\tlocal_adv |= ADVERTISE_1000XPAUSE;\n\t\t\tif (txflags & ANEG_CFG_PS2)\n\t\t\t\tlocal_adv |= ADVERTISE_1000XPSE_ASYM;\n\n\t\t\tif (rxflags & MR_LP_ADV_SYM_PAUSE)\n\t\t\t\tremote_adv |= LPA_1000XPAUSE;\n\t\t\tif (rxflags & MR_LP_ADV_ASYM_PAUSE)\n\t\t\t\tremote_adv |= LPA_1000XPAUSE_ASYM;\n\n\t\t\ttp->link_config.rmt_adv =\n\t\t\t\t\t   mii_adv_to_ethtool_adv_x(remote_adv);\n\n\t\t\ttg3_setup_flow_control(tp, local_adv, remote_adv);\n\n\t\t\tcurrent_link_up = 1;\n\t\t}\n\t\tfor (i = 0; i < 30; i++) {\n\t\t\tudelay(20);\n\t\t\ttw32_f(MAC_STATUS,\n\t\t\t       (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\tMAC_STATUS_CFG_CHANGED));\n\t\t\tudelay(40);\n\t\t\tif ((tr32(MAC_STATUS) &\n\t\t\t     (MAC_STATUS_SYNC_CHANGED |\n\t\t\t      MAC_STATUS_CFG_CHANGED)) == 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tmac_status = tr32(MAC_STATUS);\n\t\tif (current_link_up == 0 &&\n\t\t    (mac_status & MAC_STATUS_PCS_SYNCED) &&\n\t\t    !(mac_status & MAC_STATUS_RCVD_CFG))\n\t\t\tcurrent_link_up = 1;\n\t} else {\n\t\ttg3_setup_flow_control(tp, 0, 0);\n\n\t\t/* Forcing 1000FD link up. */\n\t\tcurrent_link_up = 1;\n\n\t\ttw32_f(MAC_MODE, (tp->mac_mode | MAC_MODE_SEND_CONFIGS));\n\t\tudelay(40);\n\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\t}\n\nout:\n\treturn current_link_up;\n}\n\nstatic int tg3_setup_fiber_phy(struct tg3 *tp, int force_reset)\n{\n\tu32 orig_pause_cfg;\n\tu16 orig_active_speed;\n\tu8 orig_active_duplex;\n\tu32 mac_status;\n\tint current_link_up;\n\tint i;\n\n\torig_pause_cfg = tp->link_config.active_flowctrl;\n\torig_active_speed = tp->link_config.active_speed;\n\torig_active_duplex = tp->link_config.active_duplex;\n\n\tif (!tg3_flag(tp, HW_AUTONEG) &&\n\t    tp->link_up &&\n\t    tg3_flag(tp, INIT_COMPLETE)) {\n\t\tmac_status = tr32(MAC_STATUS);\n\t\tmac_status &= (MAC_STATUS_PCS_SYNCED |\n\t\t\t       MAC_STATUS_SIGNAL_DET |\n\t\t\t       MAC_STATUS_CFG_CHANGED |\n\t\t\t       MAC_STATUS_RCVD_CFG);\n\t\tif (mac_status == (MAC_STATUS_PCS_SYNCED |\n\t\t\t\t   MAC_STATUS_SIGNAL_DET)) {\n\t\t\ttw32_f(MAC_STATUS, (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\t\t    MAC_STATUS_CFG_CHANGED));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\ttw32_f(MAC_TX_AUTO_NEG, 0);\n\n\ttp->mac_mode &= ~(MAC_MODE_PORT_MODE_MASK | MAC_MODE_HALF_DUPLEX);\n\ttp->mac_mode |= MAC_MODE_PORT_MODE_TBI;\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\tif (tp->phy_id == TG3_PHY_ID_BCM8002)\n\t\ttg3_init_bcm8002(tp);\n\n\t/* Enable link change event even when serdes polling.  */\n\ttw32_f(MAC_EVENT, MAC_EVENT_LNKSTATE_CHANGED);\n\tudelay(40);\n\n\tcurrent_link_up = 0;\n\ttp->link_config.rmt_adv = 0;\n\tmac_status = tr32(MAC_STATUS);\n\n\tif (tg3_flag(tp, HW_AUTONEG))\n\t\tcurrent_link_up = tg3_setup_fiber_hw_autoneg(tp, mac_status);\n\telse\n\t\tcurrent_link_up = tg3_setup_fiber_by_hand(tp, mac_status);\n\n\ttp->napi[0].hw_status->status =\n\t\t(SD_STATUS_UPDATED |\n\t\t (tp->napi[0].hw_status->status & ~SD_STATUS_LINK_CHG));\n\n\tfor (i = 0; i < 100; i++) {\n\t\ttw32_f(MAC_STATUS, (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\t    MAC_STATUS_CFG_CHANGED));\n\t\tudelay(5);\n\t\tif ((tr32(MAC_STATUS) & (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\t\t MAC_STATUS_CFG_CHANGED |\n\t\t\t\t\t MAC_STATUS_LNKSTATE_CHANGED)) == 0)\n\t\t\tbreak;\n\t}\n\n\tmac_status = tr32(MAC_STATUS);\n\tif ((mac_status & MAC_STATUS_PCS_SYNCED) == 0) {\n\t\tcurrent_link_up = 0;\n\t\tif (tp->link_config.autoneg == AUTONEG_ENABLE &&\n\t\t    tp->serdes_counter == 0) {\n\t\t\ttw32_f(MAC_MODE, (tp->mac_mode |\n\t\t\t\t\t  MAC_MODE_SEND_CONFIGS));\n\t\t\tudelay(1);\n\t\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\t}\n\t}\n\n\tif (current_link_up == 1) {\n\t\ttp->link_config.active_speed = SPEED_1000;\n\t\ttp->link_config.active_duplex = DUPLEX_FULL;\n\t\ttw32(MAC_LED_CTRL, (tp->led_ctrl |\n\t\t\t\t    LED_CTRL_LNKLED_OVERRIDE |\n\t\t\t\t    LED_CTRL_1000MBPS_ON));\n\t} else {\n\t\ttp->link_config.active_speed = SPEED_UNKNOWN;\n\t\ttp->link_config.active_duplex = DUPLEX_UNKNOWN;\n\t\ttw32(MAC_LED_CTRL, (tp->led_ctrl |\n\t\t\t\t    LED_CTRL_LNKLED_OVERRIDE |\n\t\t\t\t    LED_CTRL_TRAFFIC_OVERRIDE));\n\t}\n\n\tif (!tg3_test_and_report_link_chg(tp, current_link_up)) {\n\t\tu32 now_pause_cfg = tp->link_config.active_flowctrl;\n\t\tif (orig_pause_cfg != now_pause_cfg ||\n\t\t    orig_active_speed != tp->link_config.active_speed ||\n\t\t    orig_active_duplex != tp->link_config.active_duplex)\n\t\t\ttg3_link_report(tp);\n\t}\n\n\treturn 0;\n}\n\nstatic int tg3_setup_fiber_mii_phy(struct tg3 *tp, int force_reset)\n{\n\tint current_link_up, err = 0;\n\tu32 bmsr, bmcr;\n\tu16 current_speed;\n\tu8 current_duplex;\n\tu32 local_adv, remote_adv;\n\n\ttp->mac_mode |= MAC_MODE_PORT_MODE_GMII;\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\ttw32(MAC_EVENT, 0);\n\n\ttw32_f(MAC_STATUS,\n\t     (MAC_STATUS_SYNC_CHANGED |\n\t      MAC_STATUS_CFG_CHANGED |\n\t      MAC_STATUS_MI_COMPLETION |\n\t      MAC_STATUS_LNKSTATE_CHANGED));\n\tudelay(40);\n\n\tif (force_reset)\n\t\ttg3_phy_reset(tp);\n\n\tcurrent_link_up = 0;\n\tcurrent_speed = SPEED_UNKNOWN;\n\tcurrent_duplex = DUPLEX_UNKNOWN;\n\ttp->link_config.rmt_adv = 0;\n\n\terr |= tg3_readphy(tp, MII_BMSR, &bmsr);\n\terr |= tg3_readphy(tp, MII_BMSR, &bmsr);\n\tif (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\tif (tr32(MAC_TX_STATUS) & TX_STATUS_LINK_UP)\n\t\t\tbmsr |= BMSR_LSTATUS;\n\t\telse\n\t\t\tbmsr &= ~BMSR_LSTATUS;\n\t}\n\n\terr |= tg3_readphy(tp, MII_BMCR, &bmcr);\n\n\tif ((tp->link_config.autoneg == AUTONEG_ENABLE) && !force_reset &&\n\t    (tp->phy_flags & TG3_PHYFLG_PARALLEL_DETECT)) {\n\t\t/* do nothing, just check for link up at the end */\n\t} else if (tp->link_config.autoneg == AUTONEG_ENABLE) {\n\t\tu32 adv, newadv;\n\n\t\terr |= tg3_readphy(tp, MII_ADVERTISE, &adv);\n\t\tnewadv = adv & ~(ADVERTISE_1000XFULL | ADVERTISE_1000XHALF |\n\t\t\t\t ADVERTISE_1000XPAUSE |\n\t\t\t\t ADVERTISE_1000XPSE_ASYM |\n\t\t\t\t ADVERTISE_SLCT);\n\n\t\tnewadv |= tg3_advert_flowctrl_1000X(tp->link_config.flowctrl);\n\t\tnewadv |= ethtool_adv_to_mii_adv_x(tp->link_config.advertising);\n\n\t\tif ((newadv != adv) || !(bmcr & BMCR_ANENABLE)) {\n\t\t\ttg3_writephy(tp, MII_ADVERTISE, newadv);\n\t\t\tbmcr |= BMCR_ANENABLE | BMCR_ANRESTART;\n\t\t\ttg3_writephy(tp, MII_BMCR, bmcr);\n\n\t\t\ttw32_f(MAC_EVENT, MAC_EVENT_LNKSTATE_CHANGED);\n\t\t\ttp->serdes_counter = SERDES_AN_TIMEOUT_5714S;\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\n\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tu32 new_bmcr;\n\n\t\tbmcr &= ~BMCR_SPEED1000;\n\t\tnew_bmcr = bmcr & ~(BMCR_ANENABLE | BMCR_FULLDPLX);\n\n\t\tif (tp->link_config.duplex == DUPLEX_FULL)\n\t\t\tnew_bmcr |= BMCR_FULLDPLX;\n\n\t\tif (new_bmcr != bmcr) {\n\t\t\t/* BMCR_SPEED1000 is a reserved bit that needs\n\t\t\t * to be set on write.\n\t\t\t */\n\t\t\tnew_bmcr |= BMCR_SPEED1000;\n\n\t\t\t/* Force a linkdown */\n\t\t\tif (tp->link_up) {\n\t\t\t\tu32 adv;\n\n\t\t\t\terr |= tg3_readphy(tp, MII_ADVERTISE, &adv);\n\t\t\t\tadv &= ~(ADVERTISE_1000XFULL |\n\t\t\t\t\t ADVERTISE_1000XHALF |\n\t\t\t\t\t ADVERTISE_SLCT);\n\t\t\t\ttg3_writephy(tp, MII_ADVERTISE, adv);\n\t\t\t\ttg3_writephy(tp, MII_BMCR, bmcr |\n\t\t\t\t\t\t\t   BMCR_ANRESTART |\n\t\t\t\t\t\t\t   BMCR_ANENABLE);\n\t\t\t\tudelay(10);\n\t\t\t\ttg3_carrier_off(tp);\n\t\t\t}\n\t\t\ttg3_writephy(tp, MII_BMCR, new_bmcr);\n\t\t\tbmcr = new_bmcr;\n\t\t\terr |= tg3_readphy(tp, MII_BMSR, &bmsr);\n\t\t\terr |= tg3_readphy(tp, MII_BMSR, &bmsr);\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\t\t\tif (tr32(MAC_TX_STATUS) & TX_STATUS_LINK_UP)\n\t\t\t\t\tbmsr |= BMSR_LSTATUS;\n\t\t\t\telse\n\t\t\t\t\tbmsr &= ~BMSR_LSTATUS;\n\t\t\t}\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t\t}\n\t}\n\n\tif (bmsr & BMSR_LSTATUS) {\n\t\tcurrent_speed = SPEED_1000;\n\t\tcurrent_link_up = 1;\n\t\tif (bmcr & BMCR_FULLDPLX)\n\t\t\tcurrent_duplex = DUPLEX_FULL;\n\t\telse\n\t\t\tcurrent_duplex = DUPLEX_HALF;\n\n\t\tlocal_adv = 0;\n\t\tremote_adv = 0;\n\n\t\tif (bmcr & BMCR_ANENABLE) {\n\t\t\tu32 common;\n\n\t\t\terr |= tg3_readphy(tp, MII_ADVERTISE, &local_adv);\n\t\t\terr |= tg3_readphy(tp, MII_LPA, &remote_adv);\n\t\t\tcommon = local_adv & remote_adv;\n\t\t\tif (common & (ADVERTISE_1000XHALF |\n\t\t\t\t      ADVERTISE_1000XFULL)) {\n\t\t\t\tif (common & ADVERTISE_1000XFULL)\n\t\t\t\t\tcurrent_duplex = DUPLEX_FULL;\n\t\t\t\telse\n\t\t\t\t\tcurrent_duplex = DUPLEX_HALF;\n\n\t\t\t\ttp->link_config.rmt_adv =\n\t\t\t\t\t   mii_adv_to_ethtool_adv_x(remote_adv);\n\t\t\t} else if (!tg3_flag(tp, 5780_CLASS)) {\n\t\t\t\t/* Link is up via parallel detect */\n\t\t\t} else {\n\t\t\t\tcurrent_link_up = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (current_link_up == 1 && current_duplex == DUPLEX_FULL)\n\t\ttg3_setup_flow_control(tp, local_adv, remote_adv);\n\n\ttp->mac_mode &= ~MAC_MODE_HALF_DUPLEX;\n\tif (tp->link_config.active_duplex == DUPLEX_HALF)\n\t\ttp->mac_mode |= MAC_MODE_HALF_DUPLEX;\n\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\ttw32_f(MAC_EVENT, MAC_EVENT_LNKSTATE_CHANGED);\n\n\ttp->link_config.active_speed = current_speed;\n\ttp->link_config.active_duplex = current_duplex;\n\n\ttg3_test_and_report_link_chg(tp, current_link_up);\n\treturn err;\n}\n\nstatic void tg3_serdes_parallel_detect(struct tg3 *tp)\n{\n\tif (tp->serdes_counter) {\n\t\t/* Give autoneg time to complete. */\n\t\ttp->serdes_counter--;\n\t\treturn;\n\t}\n\n\tif (!tp->link_up &&\n\t    (tp->link_config.autoneg == AUTONEG_ENABLE)) {\n\t\tu32 bmcr;\n\n\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\t\tif (bmcr & BMCR_ANENABLE) {\n\t\t\tu32 phy1, phy2;\n\n\t\t\t/* Select shadow register 0x1f */\n\t\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x7c00);\n\t\t\ttg3_readphy(tp, MII_TG3_MISC_SHDW, &phy1);\n\n\t\t\t/* Select expansion interrupt status register */\n\t\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t\t\t MII_TG3_DSP_EXP1_INT_STAT);\n\t\t\ttg3_readphy(tp, MII_TG3_DSP_RW_PORT, &phy2);\n\t\t\ttg3_readphy(tp, MII_TG3_DSP_RW_PORT, &phy2);\n\n\t\t\tif ((phy1 & 0x10) && !(phy2 & 0x20)) {\n\t\t\t\t/* We have signal detect and not receiving\n\t\t\t\t * config code words, link is up by parallel\n\t\t\t\t * detection.\n\t\t\t\t */\n\n\t\t\t\tbmcr &= ~BMCR_ANENABLE;\n\t\t\t\tbmcr |= BMCR_SPEED1000 | BMCR_FULLDPLX;\n\t\t\t\ttg3_writephy(tp, MII_BMCR, bmcr);\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_PARALLEL_DETECT;\n\t\t\t}\n\t\t}\n\t} else if (tp->link_up &&\n\t\t   (tp->link_config.autoneg == AUTONEG_ENABLE) &&\n\t\t   (tp->phy_flags & TG3_PHYFLG_PARALLEL_DETECT)) {\n\t\tu32 phy2;\n\n\t\t/* Select expansion interrupt status register */\n\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t\t MII_TG3_DSP_EXP1_INT_STAT);\n\t\ttg3_readphy(tp, MII_TG3_DSP_RW_PORT, &phy2);\n\t\tif (phy2 & 0x20) {\n\t\t\tu32 bmcr;\n\n\t\t\t/* Config code words received, turn on autoneg. */\n\t\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\t\t\ttg3_writephy(tp, MII_BMCR, bmcr | BMCR_ANENABLE);\n\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\n\t\t}\n\t}\n}\n\nstatic int tg3_setup_phy(struct tg3 *tp, int force_reset)\n{\n\tu32 val;\n\tint err;\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\terr = tg3_setup_fiber_phy(tp, force_reset);\n\telse if (tp->phy_flags & TG3_PHYFLG_MII_SERDES)\n\t\terr = tg3_setup_fiber_mii_phy(tp, force_reset);\n\telse\n\t\terr = tg3_setup_copper_phy(tp, force_reset);\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX) {\n\t\tu32 scale;\n\n\t\tval = tr32(TG3_CPMU_CLCK_STAT) & CPMU_CLCK_STAT_MAC_CLCK_MASK;\n\t\tif (val == CPMU_CLCK_STAT_MAC_CLCK_62_5)\n\t\t\tscale = 65;\n\t\telse if (val == CPMU_CLCK_STAT_MAC_CLCK_6_25)\n\t\t\tscale = 6;\n\t\telse\n\t\t\tscale = 12;\n\n\t\tval = tr32(GRC_MISC_CFG) & ~GRC_MISC_CFG_PRESCALAR_MASK;\n\t\tval |= (scale << GRC_MISC_CFG_PRESCALAR_SHIFT);\n\t\ttw32(GRC_MISC_CFG, val);\n\t}\n\n\tval = (2 << TX_LENGTHS_IPG_CRS_SHIFT) |\n\t      (6 << TX_LENGTHS_IPG_SHIFT);\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tval |= tr32(MAC_TX_LENGTHS) &\n\t\t       (TX_LENGTHS_JMB_FRM_LEN_MSK |\n\t\t\tTX_LENGTHS_CNT_DWN_VAL_MSK);\n\n\tif (tp->link_config.active_speed == SPEED_1000 &&\n\t    tp->link_config.active_duplex == DUPLEX_HALF)\n\t\ttw32(MAC_TX_LENGTHS, val |\n\t\t     (0xff << TX_LENGTHS_SLOT_TIME_SHIFT));\n\telse\n\t\ttw32(MAC_TX_LENGTHS, val |\n\t\t     (32 << TX_LENGTHS_SLOT_TIME_SHIFT));\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\tif (tp->link_up) {\n\t\t\ttw32(HOSTCC_STAT_COAL_TICKS,\n\t\t\t     tp->coal.stats_block_coalesce_usecs);\n\t\t} else {\n\t\t\ttw32(HOSTCC_STAT_COAL_TICKS, 0);\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, ASPM_WORKAROUND)) {\n\t\tval = tr32(PCIE_PWR_MGMT_THRESH);\n\t\tif (!tp->link_up)\n\t\t\tval = (val & ~PCIE_PWR_MGMT_L1_THRESH_MSK) |\n\t\t\t      tp->pwrmgmt_thresh;\n\t\telse\n\t\t\tval |= PCIE_PWR_MGMT_L1_THRESH_MSK;\n\t\ttw32(PCIE_PWR_MGMT_THRESH, val);\n\t}\n\n\treturn err;\n}\n\n/* tp->lock must be held */\nstatic u64 tg3_refclk_read(struct tg3 *tp)\n{\n\tu64 stamp = tr32(TG3_EAV_REF_CLCK_LSB);\n\treturn stamp | (u64)tr32(TG3_EAV_REF_CLCK_MSB) << 32;\n}\n\n/* tp->lock must be held */\nstatic void tg3_refclk_write(struct tg3 *tp, u64 newval)\n{\n\ttw32(TG3_EAV_REF_CLCK_CTL, TG3_EAV_REF_CLCK_CTL_STOP);\n\ttw32(TG3_EAV_REF_CLCK_LSB, newval & 0xffffffff);\n\ttw32(TG3_EAV_REF_CLCK_MSB, newval >> 32);\n\ttw32_f(TG3_EAV_REF_CLCK_CTL, TG3_EAV_REF_CLCK_CTL_RESUME);\n}\n\nstatic inline void tg3_full_lock(struct tg3 *tp, int irq_sync);\nstatic inline void tg3_full_unlock(struct tg3 *tp);\nstatic int tg3_get_ts_info(struct net_device *dev, struct ethtool_ts_info *info)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tinfo->so_timestamping = SOF_TIMESTAMPING_TX_SOFTWARE |\n\t\t\t\tSOF_TIMESTAMPING_RX_SOFTWARE |\n\t\t\t\tSOF_TIMESTAMPING_SOFTWARE    |\n\t\t\t\tSOF_TIMESTAMPING_TX_HARDWARE |\n\t\t\t\tSOF_TIMESTAMPING_RX_HARDWARE |\n\t\t\t\tSOF_TIMESTAMPING_RAW_HARDWARE;\n\n\tif (tp->ptp_clock)\n\t\tinfo->phc_index = ptp_clock_index(tp->ptp_clock);\n\telse\n\t\tinfo->phc_index = -1;\n\n\tinfo->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON);\n\n\tinfo->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |\n\t\t\t   (1 << HWTSTAMP_FILTER_PTP_V1_L4_EVENT) |\n\t\t\t   (1 << HWTSTAMP_FILTER_PTP_V2_L2_EVENT) |\n\t\t\t   (1 << HWTSTAMP_FILTER_PTP_V2_L4_EVENT);\n\treturn 0;\n}\n\nstatic int tg3_ptp_adjfreq(struct ptp_clock_info *ptp, s32 ppb)\n{\n\tstruct tg3 *tp = container_of(ptp, struct tg3, ptp_info);\n\tbool neg_adj = false;\n\tu32 correction = 0;\n\n\tif (ppb < 0) {\n\t\tneg_adj = true;\n\t\tppb = -ppb;\n\t}\n\n\t/* Frequency adjustment is performed using hardware with a 24 bit\n\t * accumulator and a programmable correction value. On each clk, the\n\t * correction value gets added to the accumulator and when it\n\t * overflows, the time counter is incremented/decremented.\n\t *\n\t * So conversion from ppb to correction value is\n\t *\t\tppb * (1 << 24) / 1000000000\n\t */\n\tcorrection = div_u64((u64)ppb * (1 << 24), 1000000000ULL) &\n\t\t     TG3_EAV_REF_CLK_CORRECT_MASK;\n\n\ttg3_full_lock(tp, 0);\n\n\tif (correction)\n\t\ttw32(TG3_EAV_REF_CLK_CORRECT_CTL,\n\t\t     TG3_EAV_REF_CLK_CORRECT_EN |\n\t\t     (neg_adj ? TG3_EAV_REF_CLK_CORRECT_NEG : 0) | correction);\n\telse\n\t\ttw32(TG3_EAV_REF_CLK_CORRECT_CTL, 0);\n\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic int tg3_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)\n{\n\tstruct tg3 *tp = container_of(ptp, struct tg3, ptp_info);\n\n\ttg3_full_lock(tp, 0);\n\ttp->ptp_adjust += delta;\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic int tg3_ptp_gettime(struct ptp_clock_info *ptp, struct timespec *ts)\n{\n\tu64 ns;\n\tu32 remainder;\n\tstruct tg3 *tp = container_of(ptp, struct tg3, ptp_info);\n\n\ttg3_full_lock(tp, 0);\n\tns = tg3_refclk_read(tp);\n\tns += tp->ptp_adjust;\n\ttg3_full_unlock(tp);\n\n\tts->tv_sec = div_u64_rem(ns, 1000000000, &remainder);\n\tts->tv_nsec = remainder;\n\n\treturn 0;\n}\n\nstatic int tg3_ptp_settime(struct ptp_clock_info *ptp,\n\t\t\t   const struct timespec *ts)\n{\n\tu64 ns;\n\tstruct tg3 *tp = container_of(ptp, struct tg3, ptp_info);\n\n\tns = timespec_to_ns(ts);\n\n\ttg3_full_lock(tp, 0);\n\ttg3_refclk_write(tp, ns);\n\ttp->ptp_adjust = 0;\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic int tg3_ptp_enable(struct ptp_clock_info *ptp,\n\t\t\t  struct ptp_clock_request *rq, int on)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic const struct ptp_clock_info tg3_ptp_caps = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"tg3 clock\",\n\t.max_adj\t= 250000000,\n\t.n_alarm\t= 0,\n\t.n_ext_ts\t= 0,\n\t.n_per_out\t= 0,\n\t.pps\t\t= 0,\n\t.adjfreq\t= tg3_ptp_adjfreq,\n\t.adjtime\t= tg3_ptp_adjtime,\n\t.gettime\t= tg3_ptp_gettime,\n\t.settime\t= tg3_ptp_settime,\n\t.enable\t\t= tg3_ptp_enable,\n};\n\nstatic void tg3_hwclock_to_timestamp(struct tg3 *tp, u64 hwclock,\n\t\t\t\t     struct skb_shared_hwtstamps *timestamp)\n{\n\tmemset(timestamp, 0, sizeof(struct skb_shared_hwtstamps));\n\ttimestamp->hwtstamp  = ns_to_ktime((hwclock & TG3_TSTAMP_MASK) +\n\t\t\t\t\t   tp->ptp_adjust);\n}\n\n/* tp->lock must be held */\nstatic void tg3_ptp_init(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, PTP_CAPABLE))\n\t\treturn;\n\n\t/* Initialize the hardware clock to the system time. */\n\ttg3_refclk_write(tp, ktime_to_ns(ktime_get_real()));\n\ttp->ptp_adjust = 0;\n\ttp->ptp_info = tg3_ptp_caps;\n}\n\n/* tp->lock must be held */\nstatic void tg3_ptp_resume(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, PTP_CAPABLE))\n\t\treturn;\n\n\ttg3_refclk_write(tp, ktime_to_ns(ktime_get_real()) + tp->ptp_adjust);\n\ttp->ptp_adjust = 0;\n}\n\nstatic void tg3_ptp_fini(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, PTP_CAPABLE) || !tp->ptp_clock)\n\t\treturn;\n\n\tptp_clock_unregister(tp->ptp_clock);\n\ttp->ptp_clock = NULL;\n\ttp->ptp_adjust = 0;\n}\n\nstatic inline int tg3_irq_sync(struct tg3 *tp)\n{\n\treturn tp->irq_sync;\n}\n\nstatic inline void tg3_rd32_loop(struct tg3 *tp, u32 *dst, u32 off, u32 len)\n{\n\tint i;\n\n\tdst = (u32 *)((u8 *)dst + off);\n\tfor (i = 0; i < len; i += sizeof(u32))\n\t\t*dst++ = tr32(off + i);\n}\n\nstatic void tg3_dump_legacy_regs(struct tg3 *tp, u32 *regs)\n{\n\ttg3_rd32_loop(tp, regs, TG3PCI_VENDOR, 0xb0);\n\ttg3_rd32_loop(tp, regs, MAILBOX_INTERRUPT_0, 0x200);\n\ttg3_rd32_loop(tp, regs, MAC_MODE, 0x4f0);\n\ttg3_rd32_loop(tp, regs, SNDDATAI_MODE, 0xe0);\n\ttg3_rd32_loop(tp, regs, SNDDATAC_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, SNDBDS_MODE, 0x80);\n\ttg3_rd32_loop(tp, regs, SNDBDI_MODE, 0x48);\n\ttg3_rd32_loop(tp, regs, SNDBDC_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, RCVLPC_MODE, 0x20);\n\ttg3_rd32_loop(tp, regs, RCVLPC_SELLST_BASE, 0x15c);\n\ttg3_rd32_loop(tp, regs, RCVDBDI_MODE, 0x0c);\n\ttg3_rd32_loop(tp, regs, RCVDBDI_JUMBO_BD, 0x3c);\n\ttg3_rd32_loop(tp, regs, RCVDBDI_BD_PROD_IDX_0, 0x44);\n\ttg3_rd32_loop(tp, regs, RCVDCC_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, RCVBDI_MODE, 0x20);\n\ttg3_rd32_loop(tp, regs, RCVCC_MODE, 0x14);\n\ttg3_rd32_loop(tp, regs, RCVLSC_MODE, 0x08);\n\ttg3_rd32_loop(tp, regs, MBFREE_MODE, 0x08);\n\ttg3_rd32_loop(tp, regs, HOSTCC_MODE, 0x100);\n\n\tif (tg3_flag(tp, SUPPORT_MSIX))\n\t\ttg3_rd32_loop(tp, regs, HOSTCC_RXCOL_TICKS_VEC1, 0x180);\n\n\ttg3_rd32_loop(tp, regs, MEMARB_MODE, 0x10);\n\ttg3_rd32_loop(tp, regs, BUFMGR_MODE, 0x58);\n\ttg3_rd32_loop(tp, regs, RDMAC_MODE, 0x08);\n\ttg3_rd32_loop(tp, regs, WDMAC_MODE, 0x08);\n\ttg3_rd32_loop(tp, regs, RX_CPU_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, RX_CPU_STATE, 0x04);\n\ttg3_rd32_loop(tp, regs, RX_CPU_PGMCTR, 0x04);\n\ttg3_rd32_loop(tp, regs, RX_CPU_HWBKPT, 0x04);\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\ttg3_rd32_loop(tp, regs, TX_CPU_MODE, 0x04);\n\t\ttg3_rd32_loop(tp, regs, TX_CPU_STATE, 0x04);\n\t\ttg3_rd32_loop(tp, regs, TX_CPU_PGMCTR, 0x04);\n\t}\n\n\ttg3_rd32_loop(tp, regs, GRCMBOX_INTERRUPT_0, 0x110);\n\ttg3_rd32_loop(tp, regs, FTQ_RESET, 0x120);\n\ttg3_rd32_loop(tp, regs, MSGINT_MODE, 0x0c);\n\ttg3_rd32_loop(tp, regs, DMAC_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, GRC_MODE, 0x4c);\n\n\tif (tg3_flag(tp, NVRAM))\n\t\ttg3_rd32_loop(tp, regs, NVRAM_CMD, 0x24);\n}\n\nstatic void tg3_dump_state(struct tg3 *tp)\n{\n\tint i;\n\tu32 *regs;\n\n\tregs = kzalloc(TG3_REG_BLK_SIZE, GFP_ATOMIC);\n\tif (!regs)\n\t\treturn;\n\n\tif (tg3_flag(tp, PCI_EXPRESS)) {\n\t\t/* Read up to but not including private PCI registers */\n\t\tfor (i = 0; i < TG3_PCIE_TLDLPL_PORT; i += sizeof(u32))\n\t\t\tregs[i / sizeof(u32)] = tr32(i);\n\t} else\n\t\ttg3_dump_legacy_regs(tp, regs);\n\n\tfor (i = 0; i < TG3_REG_BLK_SIZE / sizeof(u32); i += 4) {\n\t\tif (!regs[i + 0] && !regs[i + 1] &&\n\t\t    !regs[i + 2] && !regs[i + 3])\n\t\t\tcontinue;\n\n\t\tnetdev_err(tp->dev, \"0x%08x: 0x%08x, 0x%08x, 0x%08x, 0x%08x\\n\",\n\t\t\t   i * 4,\n\t\t\t   regs[i + 0], regs[i + 1], regs[i + 2], regs[i + 3]);\n\t}\n\n\tkfree(regs);\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\t/* SW status block */\n\t\tnetdev_err(tp->dev,\n\t\t\t \"%d: Host status block [%08x:%08x:(%04x:%04x:%04x):(%04x:%04x)]\\n\",\n\t\t\t   i,\n\t\t\t   tnapi->hw_status->status,\n\t\t\t   tnapi->hw_status->status_tag,\n\t\t\t   tnapi->hw_status->rx_jumbo_consumer,\n\t\t\t   tnapi->hw_status->rx_consumer,\n\t\t\t   tnapi->hw_status->rx_mini_consumer,\n\t\t\t   tnapi->hw_status->idx[0].rx_producer,\n\t\t\t   tnapi->hw_status->idx[0].tx_consumer);\n\n\t\tnetdev_err(tp->dev,\n\t\t\"%d: NAPI info [%08x:%08x:(%04x:%04x:%04x):%04x:(%04x:%04x:%04x:%04x)]\\n\",\n\t\t\t   i,\n\t\t\t   tnapi->last_tag, tnapi->last_irq_tag,\n\t\t\t   tnapi->tx_prod, tnapi->tx_cons, tnapi->tx_pending,\n\t\t\t   tnapi->rx_rcb_ptr,\n\t\t\t   tnapi->prodring.rx_std_prod_idx,\n\t\t\t   tnapi->prodring.rx_std_cons_idx,\n\t\t\t   tnapi->prodring.rx_jmb_prod_idx,\n\t\t\t   tnapi->prodring.rx_jmb_cons_idx);\n\t}\n}\n\n/* This is called whenever we suspect that the system chipset is re-\n * ordering the sequence of MMIO to the tx send mailbox. The symptom\n * is bogus tx completions. We try to recover by setting the\n * TG3_FLAG_MBOX_WRITE_REORDER flag and resetting the chip later\n * in the workqueue.\n */\nstatic void tg3_tx_recover(struct tg3 *tp)\n{\n\tBUG_ON(tg3_flag(tp, MBOX_WRITE_REORDER) ||\n\t       tp->write32_tx_mbox == tg3_write_indirect_mbox);\n\n\tnetdev_warn(tp->dev,\n\t\t    \"The system may be re-ordering memory-mapped I/O \"\n\t\t    \"cycles to the network device, attempting to recover. \"\n\t\t    \"Please report the problem to the driver maintainer \"\n\t\t    \"and include system chipset information.\\n\");\n\n\tspin_lock(&tp->lock);\n\ttg3_flag_set(tp, TX_RECOVERY_PENDING);\n\tspin_unlock(&tp->lock);\n}\n\nstatic inline u32 tg3_tx_avail(struct tg3_napi *tnapi)\n{\n\t/* Tell compiler to fetch tx indices from memory. */\n\tbarrier();\n\treturn tnapi->tx_pending -\n\t       ((tnapi->tx_prod - tnapi->tx_cons) & (TG3_TX_RING_SIZE - 1));\n}\n\n/* Tigon3 never reports partial packet sends.  So we do not\n * need special logic to handle SKBs that have not had all\n * of their frags sent yet, like SunGEM does.\n */\nstatic void tg3_tx(struct tg3_napi *tnapi)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tu32 hw_idx = tnapi->hw_status->idx[0].tx_consumer;\n\tu32 sw_idx = tnapi->tx_cons;\n\tstruct netdev_queue *txq;\n\tint index = tnapi - tp->napi;\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\n\tif (tg3_flag(tp, ENABLE_TSS))\n\t\tindex--;\n\n\ttxq = netdev_get_tx_queue(tp->dev, index);\n\n\twhile (sw_idx != hw_idx) {\n\t\tstruct tg3_tx_ring_info *ri = &tnapi->tx_buffers[sw_idx];\n\t\tstruct sk_buff *skb = ri->skb;\n\t\tint i, tx_bug = 0;\n\n\t\tif (unlikely(skb == NULL)) {\n\t\t\ttg3_tx_recover(tp);\n\t\t\treturn;\n\t\t}\n\n\t\tif (tnapi->tx_ring[sw_idx].len_flags & TXD_FLAG_HWTSTAMP) {\n\t\t\tstruct skb_shared_hwtstamps timestamp;\n\t\t\tu64 hwclock = tr32(TG3_TX_TSTAMP_LSB);\n\t\t\thwclock |= (u64)tr32(TG3_TX_TSTAMP_MSB) << 32;\n\n\t\t\ttg3_hwclock_to_timestamp(tp, hwclock, &timestamp);\n\n\t\t\tskb_tstamp_tx(skb, &timestamp);\n\t\t}\n\n\t\tpci_unmap_single(tp->pdev,\n\t\t\t\t dma_unmap_addr(ri, mapping),\n\t\t\t\t skb_headlen(skb),\n\t\t\t\t PCI_DMA_TODEVICE);\n\n\t\tri->skb = NULL;\n\n\t\twhile (ri->fragmented) {\n\t\t\tri->fragmented = false;\n\t\t\tsw_idx = NEXT_TX(sw_idx);\n\t\t\tri = &tnapi->tx_buffers[sw_idx];\n\t\t}\n\n\t\tsw_idx = NEXT_TX(sw_idx);\n\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tri = &tnapi->tx_buffers[sw_idx];\n\t\t\tif (unlikely(ri->skb != NULL || sw_idx == hw_idx))\n\t\t\t\ttx_bug = 1;\n\n\t\t\tpci_unmap_page(tp->pdev,\n\t\t\t\t       dma_unmap_addr(ri, mapping),\n\t\t\t\t       skb_frag_size(&skb_shinfo(skb)->frags[i]),\n\t\t\t\t       PCI_DMA_TODEVICE);\n\n\t\t\twhile (ri->fragmented) {\n\t\t\t\tri->fragmented = false;\n\t\t\t\tsw_idx = NEXT_TX(sw_idx);\n\t\t\t\tri = &tnapi->tx_buffers[sw_idx];\n\t\t\t}\n\n\t\t\tsw_idx = NEXT_TX(sw_idx);\n\t\t}\n\n\t\tpkts_compl++;\n\t\tbytes_compl += skb->len;\n\n\t\tdev_kfree_skb(skb);\n\n\t\tif (unlikely(tx_bug)) {\n\t\t\ttg3_tx_recover(tp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tnetdev_tx_completed_queue(txq, pkts_compl, bytes_compl);\n\n\ttnapi->tx_cons = sw_idx;\n\n\t/* Need to make the tx_cons update visible to tg3_start_xmit()\n\t * before checking for netif_queue_stopped().  Without the\n\t * memory barrier, there is a small possibility that tg3_start_xmit()\n\t * will miss it and cause the queue to be stopped forever.\n\t */\n\tsmp_mb();\n\n\tif (unlikely(netif_tx_queue_stopped(txq) &&\n\t\t     (tg3_tx_avail(tnapi) > TG3_TX_WAKEUP_THRESH(tnapi)))) {\n\t\t__netif_tx_lock(txq, smp_processor_id());\n\t\tif (netif_tx_queue_stopped(txq) &&\n\t\t    (tg3_tx_avail(tnapi) > TG3_TX_WAKEUP_THRESH(tnapi)))\n\t\t\tnetif_tx_wake_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic void tg3_frag_free(bool is_frag, void *data)\n{\n\tif (is_frag)\n\t\tput_page(virt_to_head_page(data));\n\telse\n\t\tkfree(data);\n}\n\nstatic void tg3_rx_data_free(struct tg3 *tp, struct ring_info *ri, u32 map_sz)\n{\n\tunsigned int skb_size = SKB_DATA_ALIGN(map_sz + TG3_RX_OFFSET(tp)) +\n\t\t   SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tif (!ri->data)\n\t\treturn;\n\n\tpci_unmap_single(tp->pdev, dma_unmap_addr(ri, mapping),\n\t\t\t map_sz, PCI_DMA_FROMDEVICE);\n\ttg3_frag_free(skb_size <= PAGE_SIZE, ri->data);\n\tri->data = NULL;\n}\n\n\n/* Returns size of skb allocated or < 0 on error.\n *\n * We only need to fill in the address because the other members\n * of the RX descriptor are invariant, see tg3_init_rings.\n *\n * Note the purposeful assymetry of cpu vs. chip accesses.  For\n * posting buffers we only dirty the first cache line of the RX\n * descriptor (containing the address).  Whereas for the RX status\n * buffers the cpu only reads the last cacheline of the RX descriptor\n * (to fetch the error flags, vlan tag, checksum, and opaque cookie).\n */\nstatic int tg3_alloc_rx_data(struct tg3 *tp, struct tg3_rx_prodring_set *tpr,\n\t\t\t     u32 opaque_key, u32 dest_idx_unmasked,\n\t\t\t     unsigned int *frag_size)\n{\n\tstruct tg3_rx_buffer_desc *desc;\n\tstruct ring_info *map;\n\tu8 *data;\n\tdma_addr_t mapping;\n\tint skb_size, data_size, dest_idx;\n\n\tswitch (opaque_key) {\n\tcase RXD_OPAQUE_RING_STD:\n\t\tdest_idx = dest_idx_unmasked & tp->rx_std_ring_mask;\n\t\tdesc = &tpr->rx_std[dest_idx];\n\t\tmap = &tpr->rx_std_buffers[dest_idx];\n\t\tdata_size = tp->rx_pkt_map_sz;\n\t\tbreak;\n\n\tcase RXD_OPAQUE_RING_JUMBO:\n\t\tdest_idx = dest_idx_unmasked & tp->rx_jmb_ring_mask;\n\t\tdesc = &tpr->rx_jmb[dest_idx].std;\n\t\tmap = &tpr->rx_jmb_buffers[dest_idx];\n\t\tdata_size = TG3_RX_JMB_MAP_SZ;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* Do not overwrite any of the map or rp information\n\t * until we are sure we can commit to a new buffer.\n\t *\n\t * Callers depend upon this behavior and assume that\n\t * we leave everything unchanged if we fail.\n\t */\n\tskb_size = SKB_DATA_ALIGN(data_size + TG3_RX_OFFSET(tp)) +\n\t\t   SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tif (skb_size <= PAGE_SIZE) {\n\t\tdata = netdev_alloc_frag(skb_size);\n\t\t*frag_size = skb_size;\n\t} else {\n\t\tdata = kmalloc(skb_size, GFP_ATOMIC);\n\t\t*frag_size = 0;\n\t}\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tmapping = pci_map_single(tp->pdev,\n\t\t\t\t data + TG3_RX_OFFSET(tp),\n\t\t\t\t data_size,\n\t\t\t\t PCI_DMA_FROMDEVICE);\n\tif (unlikely(pci_dma_mapping_error(tp->pdev, mapping))) {\n\t\ttg3_frag_free(skb_size <= PAGE_SIZE, data);\n\t\treturn -EIO;\n\t}\n\n\tmap->data = data;\n\tdma_unmap_addr_set(map, mapping, mapping);\n\n\tdesc->addr_hi = ((u64)mapping >> 32);\n\tdesc->addr_lo = ((u64)mapping & 0xffffffff);\n\n\treturn data_size;\n}\n\n/* We only need to move over in the address because the other\n * members of the RX descriptor are invariant.  See notes above\n * tg3_alloc_rx_data for full details.\n */\nstatic void tg3_recycle_rx(struct tg3_napi *tnapi,\n\t\t\t   struct tg3_rx_prodring_set *dpr,\n\t\t\t   u32 opaque_key, int src_idx,\n\t\t\t   u32 dest_idx_unmasked)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_rx_buffer_desc *src_desc, *dest_desc;\n\tstruct ring_info *src_map, *dest_map;\n\tstruct tg3_rx_prodring_set *spr = &tp->napi[0].prodring;\n\tint dest_idx;\n\n\tswitch (opaque_key) {\n\tcase RXD_OPAQUE_RING_STD:\n\t\tdest_idx = dest_idx_unmasked & tp->rx_std_ring_mask;\n\t\tdest_desc = &dpr->rx_std[dest_idx];\n\t\tdest_map = &dpr->rx_std_buffers[dest_idx];\n\t\tsrc_desc = &spr->rx_std[src_idx];\n\t\tsrc_map = &spr->rx_std_buffers[src_idx];\n\t\tbreak;\n\n\tcase RXD_OPAQUE_RING_JUMBO:\n\t\tdest_idx = dest_idx_unmasked & tp->rx_jmb_ring_mask;\n\t\tdest_desc = &dpr->rx_jmb[dest_idx].std;\n\t\tdest_map = &dpr->rx_jmb_buffers[dest_idx];\n\t\tsrc_desc = &spr->rx_jmb[src_idx].std;\n\t\tsrc_map = &spr->rx_jmb_buffers[src_idx];\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n\n\tdest_map->data = src_map->data;\n\tdma_unmap_addr_set(dest_map, mapping,\n\t\t\t   dma_unmap_addr(src_map, mapping));\n\tdest_desc->addr_hi = src_desc->addr_hi;\n\tdest_desc->addr_lo = src_desc->addr_lo;\n\n\t/* Ensure that the update to the skb happens after the physical\n\t * addresses have been transferred to the new BD location.\n\t */\n\tsmp_wmb();\n\n\tsrc_map->data = NULL;\n}\n\n/* The RX ring scheme is composed of multiple rings which post fresh\n * buffers to the chip, and one special ring the chip uses to report\n * status back to the host.\n *\n * The special ring reports the status of received packets to the\n * host.  The chip does not write into the original descriptor the\n * RX buffer was obtained from.  The chip simply takes the original\n * descriptor as provided by the host, updates the status and length\n * field, then writes this into the next status ring entry.\n *\n * Each ring the host uses to post buffers to the chip is described\n * by a TG3_BDINFO entry in the chips SRAM area.  When a packet arrives,\n * it is first placed into the on-chip ram.  When the packet's length\n * is known, it walks down the TG3_BDINFO entries to select the ring.\n * Each TG3_BDINFO specifies a MAXLEN field and the first TG3_BDINFO\n * which is within the range of the new packet's length is chosen.\n *\n * The \"separate ring for rx status\" scheme may sound queer, but it makes\n * sense from a cache coherency perspective.  If only the host writes\n * to the buffer post rings, and only the chip writes to the rx status\n * rings, then cache lines never move beyond shared-modified state.\n * If both the host and chip were to write into the same ring, cache line\n * eviction could occur since both entities want it in an exclusive state.\n */\nstatic int tg3_rx(struct tg3_napi *tnapi, int budget)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tu32 work_mask, rx_std_posted = 0;\n\tu32 std_prod_idx, jmb_prod_idx;\n\tu32 sw_idx = tnapi->rx_rcb_ptr;\n\tu16 hw_idx;\n\tint received;\n\tstruct tg3_rx_prodring_set *tpr = &tnapi->prodring;\n\n\thw_idx = *(tnapi->rx_rcb_prod_idx);\n\t/*\n\t * We need to order the read of hw_idx and the read of\n\t * the opaque cookie.\n\t */\n\trmb();\n\twork_mask = 0;\n\treceived = 0;\n\tstd_prod_idx = tpr->rx_std_prod_idx;\n\tjmb_prod_idx = tpr->rx_jmb_prod_idx;\n\twhile (sw_idx != hw_idx && budget > 0) {\n\t\tstruct ring_info *ri;\n\t\tstruct tg3_rx_buffer_desc *desc = &tnapi->rx_rcb[sw_idx];\n\t\tunsigned int len;\n\t\tstruct sk_buff *skb;\n\t\tdma_addr_t dma_addr;\n\t\tu32 opaque_key, desc_idx, *post_ptr;\n\t\tu8 *data;\n\t\tu64 tstamp = 0;\n\n\t\tdesc_idx = desc->opaque & RXD_OPAQUE_INDEX_MASK;\n\t\topaque_key = desc->opaque & RXD_OPAQUE_RING_MASK;\n\t\tif (opaque_key == RXD_OPAQUE_RING_STD) {\n\t\t\tri = &tp->napi[0].prodring.rx_std_buffers[desc_idx];\n\t\t\tdma_addr = dma_unmap_addr(ri, mapping);\n\t\t\tdata = ri->data;\n\t\t\tpost_ptr = &std_prod_idx;\n\t\t\trx_std_posted++;\n\t\t} else if (opaque_key == RXD_OPAQUE_RING_JUMBO) {\n\t\t\tri = &tp->napi[0].prodring.rx_jmb_buffers[desc_idx];\n\t\t\tdma_addr = dma_unmap_addr(ri, mapping);\n\t\t\tdata = ri->data;\n\t\t\tpost_ptr = &jmb_prod_idx;\n\t\t} else\n\t\t\tgoto next_pkt_nopost;\n\n\t\twork_mask |= opaque_key;\n\n\t\tif ((desc->err_vlan & RXD_ERR_MASK) != 0 &&\n\t\t    (desc->err_vlan != RXD_ERR_ODD_NIBBLE_RCVD_MII)) {\n\t\tdrop_it:\n\t\t\ttg3_recycle_rx(tnapi, tpr, opaque_key,\n\t\t\t\t       desc_idx, *post_ptr);\n\t\tdrop_it_no_recycle:\n\t\t\t/* Other statistics kept track of by card. */\n\t\t\ttp->rx_dropped++;\n\t\t\tgoto next_pkt;\n\t\t}\n\n\t\tprefetch(data + TG3_RX_OFFSET(tp));\n\t\tlen = ((desc->idx_len & RXD_LEN_MASK) >> RXD_LEN_SHIFT) -\n\t\t      ETH_FCS_LEN;\n\n\t\tif ((desc->type_flags & RXD_FLAG_PTPSTAT_MASK) ==\n\t\t     RXD_FLAG_PTPSTAT_PTPV1 ||\n\t\t    (desc->type_flags & RXD_FLAG_PTPSTAT_MASK) ==\n\t\t     RXD_FLAG_PTPSTAT_PTPV2) {\n\t\t\ttstamp = tr32(TG3_RX_TSTAMP_LSB);\n\t\t\ttstamp |= (u64)tr32(TG3_RX_TSTAMP_MSB) << 32;\n\t\t}\n\n\t\tif (len > TG3_RX_COPY_THRESH(tp)) {\n\t\t\tint skb_size;\n\t\t\tunsigned int frag_size;\n\n\t\t\tskb_size = tg3_alloc_rx_data(tp, tpr, opaque_key,\n\t\t\t\t\t\t    *post_ptr, &frag_size);\n\t\t\tif (skb_size < 0)\n\t\t\t\tgoto drop_it;\n\n\t\t\tpci_unmap_single(tp->pdev, dma_addr, skb_size,\n\t\t\t\t\t PCI_DMA_FROMDEVICE);\n\n\t\t\tskb = build_skb(data, frag_size);\n\t\t\tif (!skb) {\n\t\t\t\ttg3_frag_free(frag_size != 0, data);\n\t\t\t\tgoto drop_it_no_recycle;\n\t\t\t}\n\t\t\tskb_reserve(skb, TG3_RX_OFFSET(tp));\n\t\t\t/* Ensure that the update to the data happens\n\t\t\t * after the usage of the old DMA mapping.\n\t\t\t */\n\t\t\tsmp_wmb();\n\n\t\t\tri->data = NULL;\n\n\t\t} else {\n\t\t\ttg3_recycle_rx(tnapi, tpr, opaque_key,\n\t\t\t\t       desc_idx, *post_ptr);\n\n\t\t\tskb = netdev_alloc_skb(tp->dev,\n\t\t\t\t\t       len + TG3_RAW_IP_ALIGN);\n\t\t\tif (skb == NULL)\n\t\t\t\tgoto drop_it_no_recycle;\n\n\t\t\tskb_reserve(skb, TG3_RAW_IP_ALIGN);\n\t\t\tpci_dma_sync_single_for_cpu(tp->pdev, dma_addr, len, PCI_DMA_FROMDEVICE);\n\t\t\tmemcpy(skb->data,\n\t\t\t       data + TG3_RX_OFFSET(tp),\n\t\t\t       len);\n\t\t\tpci_dma_sync_single_for_device(tp->pdev, dma_addr, len, PCI_DMA_FROMDEVICE);\n\t\t}\n\n\t\tskb_put(skb, len);\n\t\tif (tstamp)\n\t\t\ttg3_hwclock_to_timestamp(tp, tstamp,\n\t\t\t\t\t\t skb_hwtstamps(skb));\n\n\t\tif ((tp->dev->features & NETIF_F_RXCSUM) &&\n\t\t    (desc->type_flags & RXD_FLAG_TCPUDP_CSUM) &&\n\t\t    (((desc->ip_tcp_csum & RXD_TCPCSUM_MASK)\n\t\t      >> RXD_TCPCSUM_SHIFT) == 0xffff))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\telse\n\t\t\tskb_checksum_none_assert(skb);\n\n\t\tskb->protocol = eth_type_trans(skb, tp->dev);\n\n\t\tif (len > (tp->dev->mtu + ETH_HLEN) &&\n\t\t    skb->protocol != htons(ETH_P_8021Q)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\tgoto drop_it_no_recycle;\n\t\t}\n\n\t\tif (desc->type_flags & RXD_FLAG_VLAN &&\n\t\t    !(tp->rx_mode & RX_MODE_KEEP_VLAN_TAG))\n\t\t\t__vlan_hwaccel_put_tag(skb,\n\t\t\t\t\t       desc->err_vlan & RXD_VLAN_MASK);\n\n\t\tnapi_gro_receive(&tnapi->napi, skb);\n\n\t\treceived++;\n\t\tbudget--;\n\nnext_pkt:\n\t\t(*post_ptr)++;\n\n\t\tif (unlikely(rx_std_posted >= tp->rx_std_max_post)) {\n\t\t\ttpr->rx_std_prod_idx = std_prod_idx &\n\t\t\t\t\t       tp->rx_std_ring_mask;\n\t\t\ttw32_rx_mbox(TG3_RX_STD_PROD_IDX_REG,\n\t\t\t\t     tpr->rx_std_prod_idx);\n\t\t\twork_mask &= ~RXD_OPAQUE_RING_STD;\n\t\t\trx_std_posted = 0;\n\t\t}\nnext_pkt_nopost:\n\t\tsw_idx++;\n\t\tsw_idx &= tp->rx_ret_ring_mask;\n\n\t\t/* Refresh hw_idx to see if there is new work */\n\t\tif (sw_idx == hw_idx) {\n\t\t\thw_idx = *(tnapi->rx_rcb_prod_idx);\n\t\t\trmb();\n\t\t}\n\t}\n\n\t/* ACK the status ring. */\n\ttnapi->rx_rcb_ptr = sw_idx;\n\ttw32_rx_mbox(tnapi->consmbox, sw_idx);\n\n\t/* Refill RX ring(s). */\n\tif (!tg3_flag(tp, ENABLE_RSS)) {\n\t\t/* Sync BD data before updating mailbox */\n\t\twmb();\n\n\t\tif (work_mask & RXD_OPAQUE_RING_STD) {\n\t\t\ttpr->rx_std_prod_idx = std_prod_idx &\n\t\t\t\t\t       tp->rx_std_ring_mask;\n\t\t\ttw32_rx_mbox(TG3_RX_STD_PROD_IDX_REG,\n\t\t\t\t     tpr->rx_std_prod_idx);\n\t\t}\n\t\tif (work_mask & RXD_OPAQUE_RING_JUMBO) {\n\t\t\ttpr->rx_jmb_prod_idx = jmb_prod_idx &\n\t\t\t\t\t       tp->rx_jmb_ring_mask;\n\t\t\ttw32_rx_mbox(TG3_RX_JMB_PROD_IDX_REG,\n\t\t\t\t     tpr->rx_jmb_prod_idx);\n\t\t}\n\t\tmmiowb();\n\t} else if (work_mask) {\n\t\t/* rx_std_buffers[] and rx_jmb_buffers[] entries must be\n\t\t * updated before the producer indices can be updated.\n\t\t */\n\t\tsmp_wmb();\n\n\t\ttpr->rx_std_prod_idx = std_prod_idx & tp->rx_std_ring_mask;\n\t\ttpr->rx_jmb_prod_idx = jmb_prod_idx & tp->rx_jmb_ring_mask;\n\n\t\tif (tnapi != &tp->napi[1]) {\n\t\t\ttp->rx_refill = true;\n\t\t\tnapi_schedule(&tp->napi[1].napi);\n\t\t}\n\t}\n\n\treturn received;\n}\n\nstatic void tg3_poll_link(struct tg3 *tp)\n{\n\t/* handle link change and other phy events */\n\tif (!(tg3_flag(tp, USE_LINKCHG_REG) || tg3_flag(tp, POLL_SERDES))) {\n\t\tstruct tg3_hw_status *sblk = tp->napi[0].hw_status;\n\n\t\tif (sblk->status & SD_STATUS_LINK_CHG) {\n\t\t\tsblk->status = SD_STATUS_UPDATED |\n\t\t\t\t       (sblk->status & ~SD_STATUS_LINK_CHG);\n\t\t\tspin_lock(&tp->lock);\n\t\t\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\t\t\ttw32_f(MAC_STATUS,\n\t\t\t\t     (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\t      MAC_STATUS_CFG_CHANGED |\n\t\t\t\t      MAC_STATUS_MI_COMPLETION |\n\t\t\t\t      MAC_STATUS_LNKSTATE_CHANGED));\n\t\t\t\tudelay(40);\n\t\t\t} else\n\t\t\t\ttg3_setup_phy(tp, 0);\n\t\t\tspin_unlock(&tp->lock);\n\t\t}\n\t}\n}\n\nstatic int tg3_rx_prodring_xfer(struct tg3 *tp,\n\t\t\t\tstruct tg3_rx_prodring_set *dpr,\n\t\t\t\tstruct tg3_rx_prodring_set *spr)\n{\n\tu32 si, di, cpycnt, src_prod_idx;\n\tint i, err = 0;\n\n\twhile (1) {\n\t\tsrc_prod_idx = spr->rx_std_prod_idx;\n\n\t\t/* Make sure updates to the rx_std_buffers[] entries and the\n\t\t * standard producer index are seen in the correct order.\n\t\t */\n\t\tsmp_rmb();\n\n\t\tif (spr->rx_std_cons_idx == src_prod_idx)\n\t\t\tbreak;\n\n\t\tif (spr->rx_std_cons_idx < src_prod_idx)\n\t\t\tcpycnt = src_prod_idx - spr->rx_std_cons_idx;\n\t\telse\n\t\t\tcpycnt = tp->rx_std_ring_mask + 1 -\n\t\t\t\t spr->rx_std_cons_idx;\n\n\t\tcpycnt = min(cpycnt,\n\t\t\t     tp->rx_std_ring_mask + 1 - dpr->rx_std_prod_idx);\n\n\t\tsi = spr->rx_std_cons_idx;\n\t\tdi = dpr->rx_std_prod_idx;\n\n\t\tfor (i = di; i < di + cpycnt; i++) {\n\t\t\tif (dpr->rx_std_buffers[i].data) {\n\t\t\t\tcpycnt = i - di;\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!cpycnt)\n\t\t\tbreak;\n\n\t\t/* Ensure that updates to the rx_std_buffers ring and the\n\t\t * shadowed hardware producer ring from tg3_recycle_skb() are\n\t\t * ordered correctly WRT the skb check above.\n\t\t */\n\t\tsmp_rmb();\n\n\t\tmemcpy(&dpr->rx_std_buffers[di],\n\t\t       &spr->rx_std_buffers[si],\n\t\t       cpycnt * sizeof(struct ring_info));\n\n\t\tfor (i = 0; i < cpycnt; i++, di++, si++) {\n\t\t\tstruct tg3_rx_buffer_desc *sbd, *dbd;\n\t\t\tsbd = &spr->rx_std[si];\n\t\t\tdbd = &dpr->rx_std[di];\n\t\t\tdbd->addr_hi = sbd->addr_hi;\n\t\t\tdbd->addr_lo = sbd->addr_lo;\n\t\t}\n\n\t\tspr->rx_std_cons_idx = (spr->rx_std_cons_idx + cpycnt) &\n\t\t\t\t       tp->rx_std_ring_mask;\n\t\tdpr->rx_std_prod_idx = (dpr->rx_std_prod_idx + cpycnt) &\n\t\t\t\t       tp->rx_std_ring_mask;\n\t}\n\n\twhile (1) {\n\t\tsrc_prod_idx = spr->rx_jmb_prod_idx;\n\n\t\t/* Make sure updates to the rx_jmb_buffers[] entries and\n\t\t * the jumbo producer index are seen in the correct order.\n\t\t */\n\t\tsmp_rmb();\n\n\t\tif (spr->rx_jmb_cons_idx == src_prod_idx)\n\t\t\tbreak;\n\n\t\tif (spr->rx_jmb_cons_idx < src_prod_idx)\n\t\t\tcpycnt = src_prod_idx - spr->rx_jmb_cons_idx;\n\t\telse\n\t\t\tcpycnt = tp->rx_jmb_ring_mask + 1 -\n\t\t\t\t spr->rx_jmb_cons_idx;\n\n\t\tcpycnt = min(cpycnt,\n\t\t\t     tp->rx_jmb_ring_mask + 1 - dpr->rx_jmb_prod_idx);\n\n\t\tsi = spr->rx_jmb_cons_idx;\n\t\tdi = dpr->rx_jmb_prod_idx;\n\n\t\tfor (i = di; i < di + cpycnt; i++) {\n\t\t\tif (dpr->rx_jmb_buffers[i].data) {\n\t\t\t\tcpycnt = i - di;\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!cpycnt)\n\t\t\tbreak;\n\n\t\t/* Ensure that updates to the rx_jmb_buffers ring and the\n\t\t * shadowed hardware producer ring from tg3_recycle_skb() are\n\t\t * ordered correctly WRT the skb check above.\n\t\t */\n\t\tsmp_rmb();\n\n\t\tmemcpy(&dpr->rx_jmb_buffers[di],\n\t\t       &spr->rx_jmb_buffers[si],\n\t\t       cpycnt * sizeof(struct ring_info));\n\n\t\tfor (i = 0; i < cpycnt; i++, di++, si++) {\n\t\t\tstruct tg3_rx_buffer_desc *sbd, *dbd;\n\t\t\tsbd = &spr->rx_jmb[si].std;\n\t\t\tdbd = &dpr->rx_jmb[di].std;\n\t\t\tdbd->addr_hi = sbd->addr_hi;\n\t\t\tdbd->addr_lo = sbd->addr_lo;\n\t\t}\n\n\t\tspr->rx_jmb_cons_idx = (spr->rx_jmb_cons_idx + cpycnt) &\n\t\t\t\t       tp->rx_jmb_ring_mask;\n\t\tdpr->rx_jmb_prod_idx = (dpr->rx_jmb_prod_idx + cpycnt) &\n\t\t\t\t       tp->rx_jmb_ring_mask;\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_poll_work(struct tg3_napi *tnapi, int work_done, int budget)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\n\t/* run TX completion thread */\n\tif (tnapi->hw_status->idx[0].tx_consumer != tnapi->tx_cons) {\n\t\ttg3_tx(tnapi);\n\t\tif (unlikely(tg3_flag(tp, TX_RECOVERY_PENDING)))\n\t\t\treturn work_done;\n\t}\n\n\tif (!tnapi->rx_rcb_prod_idx)\n\t\treturn work_done;\n\n\t/* run RX thread, within the bounds set by NAPI.\n\t * All RX \"locking\" is done by ensuring outside\n\t * code synchronizes with tg3->napi.poll()\n\t */\n\tif (*(tnapi->rx_rcb_prod_idx) != tnapi->rx_rcb_ptr)\n\t\twork_done += tg3_rx(tnapi, budget - work_done);\n\n\tif (tg3_flag(tp, ENABLE_RSS) && tnapi == &tp->napi[1]) {\n\t\tstruct tg3_rx_prodring_set *dpr = &tp->napi[0].prodring;\n\t\tint i, err = 0;\n\t\tu32 std_prod_idx = dpr->rx_std_prod_idx;\n\t\tu32 jmb_prod_idx = dpr->rx_jmb_prod_idx;\n\n\t\ttp->rx_refill = false;\n\t\tfor (i = 1; i <= tp->rxq_cnt; i++)\n\t\t\terr |= tg3_rx_prodring_xfer(tp, dpr,\n\t\t\t\t\t\t    &tp->napi[i].prodring);\n\n\t\twmb();\n\n\t\tif (std_prod_idx != dpr->rx_std_prod_idx)\n\t\t\ttw32_rx_mbox(TG3_RX_STD_PROD_IDX_REG,\n\t\t\t\t     dpr->rx_std_prod_idx);\n\n\t\tif (jmb_prod_idx != dpr->rx_jmb_prod_idx)\n\t\t\ttw32_rx_mbox(TG3_RX_JMB_PROD_IDX_REG,\n\t\t\t\t     dpr->rx_jmb_prod_idx);\n\n\t\tmmiowb();\n\n\t\tif (err)\n\t\t\ttw32_f(HOSTCC_MODE, tp->coal_now);\n\t}\n\n\treturn work_done;\n}\n\nstatic inline void tg3_reset_task_schedule(struct tg3 *tp)\n{\n\tif (!test_and_set_bit(TG3_FLAG_RESET_TASK_PENDING, tp->tg3_flags))\n\t\tschedule_work(&tp->reset_task);\n}\n\nstatic inline void tg3_reset_task_cancel(struct tg3 *tp)\n{\n\tcancel_work_sync(&tp->reset_task);\n\ttg3_flag_clear(tp, RESET_TASK_PENDING);\n\ttg3_flag_clear(tp, TX_RECOVERY_PENDING);\n}\n\nstatic int tg3_poll_msix(struct napi_struct *napi, int budget)\n{\n\tstruct tg3_napi *tnapi = container_of(napi, struct tg3_napi, napi);\n\tstruct tg3 *tp = tnapi->tp;\n\tint work_done = 0;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\n\twhile (1) {\n\t\twork_done = tg3_poll_work(tnapi, work_done, budget);\n\n\t\tif (unlikely(tg3_flag(tp, TX_RECOVERY_PENDING)))\n\t\t\tgoto tx_recovery;\n\n\t\tif (unlikely(work_done >= budget))\n\t\t\tbreak;\n\n\t\t/* tp->last_tag is used in tg3_int_reenable() below\n\t\t * to tell the hw how much work has been processed,\n\t\t * so we must read it before checking for more work.\n\t\t */\n\t\ttnapi->last_tag = sblk->status_tag;\n\t\ttnapi->last_irq_tag = tnapi->last_tag;\n\t\trmb();\n\n\t\t/* check for RX/TX work to do */\n\t\tif (likely(sblk->idx[0].tx_consumer == tnapi->tx_cons &&\n\t\t\t   *(tnapi->rx_rcb_prod_idx) == tnapi->rx_rcb_ptr)) {\n\n\t\t\t/* This test here is not race free, but will reduce\n\t\t\t * the number of interrupts by looping again.\n\t\t\t */\n\t\t\tif (tnapi == &tp->napi[1] && tp->rx_refill)\n\t\t\t\tcontinue;\n\n\t\t\tnapi_complete(napi);\n\t\t\t/* Reenable interrupts. */\n\t\t\ttw32_mailbox(tnapi->int_mbox, tnapi->last_tag << 24);\n\n\t\t\t/* This test here is synchronized by napi_schedule()\n\t\t\t * and napi_complete() to close the race condition.\n\t\t\t */\n\t\t\tif (unlikely(tnapi == &tp->napi[1] && tp->rx_refill)) {\n\t\t\t\ttw32(HOSTCC_MODE, tp->coalesce_mode |\n\t\t\t\t\t\t  HOSTCC_MODE_ENABLE |\n\t\t\t\t\t\t  tnapi->coal_now);\n\t\t\t}\n\t\t\tmmiowb();\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn work_done;\n\ntx_recovery:\n\t/* work_done is guaranteed to be less than budget. */\n\tnapi_complete(napi);\n\ttg3_reset_task_schedule(tp);\n\treturn work_done;\n}\n\nstatic void tg3_process_error(struct tg3 *tp)\n{\n\tu32 val;\n\tbool real_error = false;\n\n\tif (tg3_flag(tp, ERROR_PROCESSED))\n\t\treturn;\n\n\t/* Check Flow Attention register */\n\tval = tr32(HOSTCC_FLOW_ATTN);\n\tif (val & ~HOSTCC_FLOW_ATTN_MBUF_LWM) {\n\t\tnetdev_err(tp->dev, \"FLOW Attention error.  Resetting chip.\\n\");\n\t\treal_error = true;\n\t}\n\n\tif (tr32(MSGINT_STATUS) & ~MSGINT_STATUS_MSI_REQ) {\n\t\tnetdev_err(tp->dev, \"MSI Status error.  Resetting chip.\\n\");\n\t\treal_error = true;\n\t}\n\n\tif (tr32(RDMAC_STATUS) || tr32(WDMAC_STATUS)) {\n\t\tnetdev_err(tp->dev, \"DMA Status error.  Resetting chip.\\n\");\n\t\treal_error = true;\n\t}\n\n\tif (!real_error)\n\t\treturn;\n\n\ttg3_dump_state(tp);\n\n\ttg3_flag_set(tp, ERROR_PROCESSED);\n\ttg3_reset_task_schedule(tp);\n}\n\nstatic int tg3_poll(struct napi_struct *napi, int budget)\n{\n\tstruct tg3_napi *tnapi = container_of(napi, struct tg3_napi, napi);\n\tstruct tg3 *tp = tnapi->tp;\n\tint work_done = 0;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\n\twhile (1) {\n\t\tif (sblk->status & SD_STATUS_ERROR)\n\t\t\ttg3_process_error(tp);\n\n\t\ttg3_poll_link(tp);\n\n\t\twork_done = tg3_poll_work(tnapi, work_done, budget);\n\n\t\tif (unlikely(tg3_flag(tp, TX_RECOVERY_PENDING)))\n\t\t\tgoto tx_recovery;\n\n\t\tif (unlikely(work_done >= budget))\n\t\t\tbreak;\n\n\t\tif (tg3_flag(tp, TAGGED_STATUS)) {\n\t\t\t/* tp->last_tag is used in tg3_int_reenable() below\n\t\t\t * to tell the hw how much work has been processed,\n\t\t\t * so we must read it before checking for more work.\n\t\t\t */\n\t\t\ttnapi->last_tag = sblk->status_tag;\n\t\t\ttnapi->last_irq_tag = tnapi->last_tag;\n\t\t\trmb();\n\t\t} else\n\t\t\tsblk->status &= ~SD_STATUS_UPDATED;\n\n\t\tif (likely(!tg3_has_work(tnapi))) {\n\t\t\tnapi_complete(napi);\n\t\t\ttg3_int_reenable(tnapi);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn work_done;\n\ntx_recovery:\n\t/* work_done is guaranteed to be less than budget. */\n\tnapi_complete(napi);\n\ttg3_reset_task_schedule(tp);\n\treturn work_done;\n}\n\nstatic void tg3_napi_disable(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = tp->irq_cnt - 1; i >= 0; i--)\n\t\tnapi_disable(&tp->napi[i].napi);\n}\n\nstatic void tg3_napi_enable(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\tnapi_enable(&tp->napi[i].napi);\n}\n\nstatic void tg3_napi_init(struct tg3 *tp)\n{\n\tint i;\n\n\tnetif_napi_add(tp->dev, &tp->napi[0].napi, tg3_poll, 64);\n\tfor (i = 1; i < tp->irq_cnt; i++)\n\t\tnetif_napi_add(tp->dev, &tp->napi[i].napi, tg3_poll_msix, 64);\n}\n\nstatic void tg3_napi_fini(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\tnetif_napi_del(&tp->napi[i].napi);\n}\n\nstatic inline void tg3_netif_stop(struct tg3 *tp)\n{\n\ttp->dev->trans_start = jiffies;\t/* prevent tx timeout */\n\ttg3_napi_disable(tp);\n\tnetif_carrier_off(tp->dev);\n\tnetif_tx_disable(tp->dev);\n}\n\n/* tp->lock must be held */\nstatic inline void tg3_netif_start(struct tg3 *tp)\n{\n\ttg3_ptp_resume(tp);\n\n\t/* NOTE: unconditional netif_tx_wake_all_queues is only\n\t * appropriate so long as all callers are assured to\n\t * have free tx slots (such as after tg3_init_hw)\n\t */\n\tnetif_tx_wake_all_queues(tp->dev);\n\n\tif (tp->link_up)\n\t\tnetif_carrier_on(tp->dev);\n\n\ttg3_napi_enable(tp);\n\ttp->napi[0].hw_status->status |= SD_STATUS_UPDATED;\n\ttg3_enable_ints(tp);\n}\n\nstatic void tg3_irq_quiesce(struct tg3 *tp)\n{\n\tint i;\n\n\tBUG_ON(tp->irq_sync);\n\n\ttp->irq_sync = 1;\n\tsmp_mb();\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\tsynchronize_irq(tp->napi[i].irq_vec);\n}\n\n/* Fully shutdown all tg3 driver activity elsewhere in the system.\n * If irq_sync is non-zero, then the IRQ handler must be synchronized\n * with as well.  Most of the time, this is not necessary except when\n * shutting down the device.\n */\nstatic inline void tg3_full_lock(struct tg3 *tp, int irq_sync)\n{\n\tspin_lock_bh(&tp->lock);\n\tif (irq_sync)\n\t\ttg3_irq_quiesce(tp);\n}\n\nstatic inline void tg3_full_unlock(struct tg3 *tp)\n{\n\tspin_unlock_bh(&tp->lock);\n}\n\n/* One-shot MSI handler - Chip automatically disables interrupt\n * after sending MSI so driver doesn't have to do it.\n */\nstatic irqreturn_t tg3_msi_1shot(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\n\tprefetch(tnapi->hw_status);\n\tif (tnapi->rx_rcb)\n\t\tprefetch(&tnapi->rx_rcb[tnapi->rx_rcb_ptr]);\n\n\tif (likely(!tg3_irq_sync(tp)))\n\t\tnapi_schedule(&tnapi->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n/* MSI ISR - No need to check for interrupt sharing and no need to\n * flush status block and interrupt mailbox. PCI ordering rules\n * guarantee that MSI will arrive after the status block.\n */\nstatic irqreturn_t tg3_msi(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\n\tprefetch(tnapi->hw_status);\n\tif (tnapi->rx_rcb)\n\t\tprefetch(&tnapi->rx_rcb[tnapi->rx_rcb_ptr]);\n\t/*\n\t * Writing any value to intr-mbox-0 clears PCI INTA# and\n\t * chip-internal interrupt pending events.\n\t * Writing non-zero to intr-mbox-0 additional tells the\n\t * NIC to stop sending us irqs, engaging \"in-intr-handler\"\n\t * event coalescing.\n\t */\n\ttw32_mailbox(tnapi->int_mbox, 0x00000001);\n\tif (likely(!tg3_irq_sync(tp)))\n\t\tnapi_schedule(&tnapi->napi);\n\n\treturn IRQ_RETVAL(1);\n}\n\nstatic irqreturn_t tg3_interrupt(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\tunsigned int handled = 1;\n\n\t/* In INTx mode, it is possible for the interrupt to arrive at\n\t * the CPU before the status block posted prior to the interrupt.\n\t * Reading the PCI State register will confirm whether the\n\t * interrupt is ours and will flush the status block.\n\t */\n\tif (unlikely(!(sblk->status & SD_STATUS_UPDATED))) {\n\t\tif (tg3_flag(tp, CHIP_RESETTING) ||\n\t\t    (tr32(TG3PCI_PCISTATE) & PCISTATE_INT_NOT_ACTIVE)) {\n\t\t\thandled = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * Writing any value to intr-mbox-0 clears PCI INTA# and\n\t * chip-internal interrupt pending events.\n\t * Writing non-zero to intr-mbox-0 additional tells the\n\t * NIC to stop sending us irqs, engaging \"in-intr-handler\"\n\t * event coalescing.\n\t *\n\t * Flush the mailbox to de-assert the IRQ immediately to prevent\n\t * spurious interrupts.  The flush impacts performance but\n\t * excessive spurious interrupts can be worse in some cases.\n\t */\n\ttw32_mailbox_f(MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW, 0x00000001);\n\tif (tg3_irq_sync(tp))\n\t\tgoto out;\n\tsblk->status &= ~SD_STATUS_UPDATED;\n\tif (likely(tg3_has_work(tnapi))) {\n\t\tprefetch(&tnapi->rx_rcb[tnapi->rx_rcb_ptr]);\n\t\tnapi_schedule(&tnapi->napi);\n\t} else {\n\t\t/* No work, shared interrupt perhaps?  re-enable\n\t\t * interrupts, and flush that PCI write\n\t\t */\n\t\ttw32_mailbox_f(MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW,\n\t\t\t       0x00000000);\n\t}\nout:\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic irqreturn_t tg3_interrupt_tagged(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\tunsigned int handled = 1;\n\n\t/* In INTx mode, it is possible for the interrupt to arrive at\n\t * the CPU before the status block posted prior to the interrupt.\n\t * Reading the PCI State register will confirm whether the\n\t * interrupt is ours and will flush the status block.\n\t */\n\tif (unlikely(sblk->status_tag == tnapi->last_irq_tag)) {\n\t\tif (tg3_flag(tp, CHIP_RESETTING) ||\n\t\t    (tr32(TG3PCI_PCISTATE) & PCISTATE_INT_NOT_ACTIVE)) {\n\t\t\thandled = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * writing any value to intr-mbox-0 clears PCI INTA# and\n\t * chip-internal interrupt pending events.\n\t * writing non-zero to intr-mbox-0 additional tells the\n\t * NIC to stop sending us irqs, engaging \"in-intr-handler\"\n\t * event coalescing.\n\t *\n\t * Flush the mailbox to de-assert the IRQ immediately to prevent\n\t * spurious interrupts.  The flush impacts performance but\n\t * excessive spurious interrupts can be worse in some cases.\n\t */\n\ttw32_mailbox_f(MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW, 0x00000001);\n\n\t/*\n\t * In a shared interrupt configuration, sometimes other devices'\n\t * interrupts will scream.  We record the current status tag here\n\t * so that the above check can report that the screaming interrupts\n\t * are unhandled.  Eventually they will be silenced.\n\t */\n\ttnapi->last_irq_tag = sblk->status_tag;\n\n\tif (tg3_irq_sync(tp))\n\t\tgoto out;\n\n\tprefetch(&tnapi->rx_rcb[tnapi->rx_rcb_ptr]);\n\n\tnapi_schedule(&tnapi->napi);\n\nout:\n\treturn IRQ_RETVAL(handled);\n}\n\n/* ISR for interrupt test */\nstatic irqreturn_t tg3_test_isr(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\n\tif ((sblk->status & SD_STATUS_UPDATED) ||\n\t    !(tr32(TG3PCI_PCISTATE) & PCISTATE_INT_NOT_ACTIVE)) {\n\t\ttg3_disable_ints(tp);\n\t\treturn IRQ_RETVAL(1);\n\t}\n\treturn IRQ_RETVAL(0);\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void tg3_poll_controller(struct net_device *dev)\n{\n\tint i;\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tg3_irq_sync(tp))\n\t\treturn;\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\ttg3_interrupt(tp->napi[i].irq_vec, &tp->napi[i]);\n}\n#endif\n\nstatic void tg3_tx_timeout(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (netif_msg_tx_err(tp)) {\n\t\tnetdev_err(dev, \"transmit timed out, resetting\\n\");\n\t\ttg3_dump_state(tp);\n\t}\n\n\ttg3_reset_task_schedule(tp);\n}\n\n/* Test for DMA buffers crossing any 4GB boundaries: 4G, 8G, etc */\nstatic inline int tg3_4g_overflow_test(dma_addr_t mapping, int len)\n{\n\tu32 base = (u32) mapping & 0xffffffff;\n\n\treturn (base > 0xffffdcc0) && (base + len + 8 < base);\n}\n\n/* Test for DMA addresses > 40-bit */\nstatic inline int tg3_40bit_overflow_test(struct tg3 *tp, dma_addr_t mapping,\n\t\t\t\t\t  int len)\n{\n#if defined(CONFIG_HIGHMEM) && (BITS_PER_LONG == 64)\n\tif (tg3_flag(tp, 40BIT_DMA_BUG))\n\t\treturn ((u64) mapping + len) > DMA_BIT_MASK(40);\n\treturn 0;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline void tg3_tx_set_bd(struct tg3_tx_buffer_desc *txbd,\n\t\t\t\t dma_addr_t mapping, u32 len, u32 flags,\n\t\t\t\t u32 mss, u32 vlan)\n{\n\ttxbd->addr_hi = ((u64) mapping >> 32);\n\ttxbd->addr_lo = ((u64) mapping & 0xffffffff);\n\ttxbd->len_flags = (len << TXD_LEN_SHIFT) | (flags & 0x0000ffff);\n\ttxbd->vlan_tag = (mss << TXD_MSS_SHIFT) | (vlan << TXD_VLAN_TAG_SHIFT);\n}\n\nstatic bool tg3_tx_frag_set(struct tg3_napi *tnapi, u32 *entry, u32 *budget,\n\t\t\t    dma_addr_t map, u32 len, u32 flags,\n\t\t\t    u32 mss, u32 vlan)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tbool hwbug = false;\n\n\tif (tg3_flag(tp, SHORT_DMA_BUG) && len <= 8)\n\t\thwbug = true;\n\n\tif (tg3_4g_overflow_test(map, len))\n\t\thwbug = true;\n\n\tif (tg3_40bit_overflow_test(tp, map, len))\n\t\thwbug = true;\n\n\tif (tp->dma_limit) {\n\t\tu32 prvidx = *entry;\n\t\tu32 tmp_flag = flags & ~TXD_FLAG_END;\n\t\twhile (len > tp->dma_limit && *budget) {\n\t\t\tu32 frag_len = tp->dma_limit;\n\t\t\tlen -= tp->dma_limit;\n\n\t\t\t/* Avoid the 8byte DMA problem */\n\t\t\tif (len <= 8) {\n\t\t\t\tlen += tp->dma_limit / 2;\n\t\t\t\tfrag_len = tp->dma_limit / 2;\n\t\t\t}\n\n\t\t\ttnapi->tx_buffers[*entry].fragmented = true;\n\n\t\t\ttg3_tx_set_bd(&tnapi->tx_ring[*entry], map,\n\t\t\t\t      frag_len, tmp_flag, mss, vlan);\n\t\t\t*budget -= 1;\n\t\t\tprvidx = *entry;\n\t\t\t*entry = NEXT_TX(*entry);\n\n\t\t\tmap += frag_len;\n\t\t}\n\n\t\tif (len) {\n\t\t\tif (*budget) {\n\t\t\t\ttg3_tx_set_bd(&tnapi->tx_ring[*entry], map,\n\t\t\t\t\t      len, flags, mss, vlan);\n\t\t\t\t*budget -= 1;\n\t\t\t\t*entry = NEXT_TX(*entry);\n\t\t\t} else {\n\t\t\t\thwbug = true;\n\t\t\t\ttnapi->tx_buffers[prvidx].fragmented = false;\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttg3_tx_set_bd(&tnapi->tx_ring[*entry], map,\n\t\t\t      len, flags, mss, vlan);\n\t\t*entry = NEXT_TX(*entry);\n\t}\n\n\treturn hwbug;\n}\n\nstatic void tg3_tx_skb_unmap(struct tg3_napi *tnapi, u32 entry, int last)\n{\n\tint i;\n\tstruct sk_buff *skb;\n\tstruct tg3_tx_ring_info *txb = &tnapi->tx_buffers[entry];\n\n\tskb = txb->skb;\n\ttxb->skb = NULL;\n\n\tpci_unmap_single(tnapi->tp->pdev,\n\t\t\t dma_unmap_addr(txb, mapping),\n\t\t\t skb_headlen(skb),\n\t\t\t PCI_DMA_TODEVICE);\n\n\twhile (txb->fragmented) {\n\t\ttxb->fragmented = false;\n\t\tentry = NEXT_TX(entry);\n\t\ttxb = &tnapi->tx_buffers[entry];\n\t}\n\n\tfor (i = 0; i <= last; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tentry = NEXT_TX(entry);\n\t\ttxb = &tnapi->tx_buffers[entry];\n\n\t\tpci_unmap_page(tnapi->tp->pdev,\n\t\t\t       dma_unmap_addr(txb, mapping),\n\t\t\t       skb_frag_size(frag), PCI_DMA_TODEVICE);\n\n\t\twhile (txb->fragmented) {\n\t\t\ttxb->fragmented = false;\n\t\t\tentry = NEXT_TX(entry);\n\t\t\ttxb = &tnapi->tx_buffers[entry];\n\t\t}\n\t}\n}\n\n/* Workaround 4GB and 40-bit hardware DMA bugs. */\nstatic int tigon3_dma_hwbug_workaround(struct tg3_napi *tnapi,\n\t\t\t\t       struct sk_buff **pskb,\n\t\t\t\t       u32 *entry, u32 *budget,\n\t\t\t\t       u32 base_flags, u32 mss, u32 vlan)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct sk_buff *new_skb, *skb = *pskb;\n\tdma_addr_t new_addr = 0;\n\tint ret = 0;\n\n\tif (tg3_asic_rev(tp) != ASIC_REV_5701)\n\t\tnew_skb = skb_copy(skb, GFP_ATOMIC);\n\telse {\n\t\tint more_headroom = 4 - ((unsigned long)skb->data & 3);\n\n\t\tnew_skb = skb_copy_expand(skb,\n\t\t\t\t\t  skb_headroom(skb) + more_headroom,\n\t\t\t\t\t  skb_tailroom(skb), GFP_ATOMIC);\n\t}\n\n\tif (!new_skb) {\n\t\tret = -1;\n\t} else {\n\t\t/* New SKB is guaranteed to be linear. */\n\t\tnew_addr = pci_map_single(tp->pdev, new_skb->data, new_skb->len,\n\t\t\t\t\t  PCI_DMA_TODEVICE);\n\t\t/* Make sure the mapping succeeded */\n\t\tif (pci_dma_mapping_error(tp->pdev, new_addr)) {\n\t\t\tdev_kfree_skb(new_skb);\n\t\t\tret = -1;\n\t\t} else {\n\t\t\tu32 save_entry = *entry;\n\n\t\t\tbase_flags |= TXD_FLAG_END;\n\n\t\t\ttnapi->tx_buffers[*entry].skb = new_skb;\n\t\t\tdma_unmap_addr_set(&tnapi->tx_buffers[*entry],\n\t\t\t\t\t   mapping, new_addr);\n\n\t\t\tif (tg3_tx_frag_set(tnapi, entry, budget, new_addr,\n\t\t\t\t\t    new_skb->len, base_flags,\n\t\t\t\t\t    mss, vlan)) {\n\t\t\t\ttg3_tx_skb_unmap(tnapi, save_entry, -1);\n\t\t\t\tdev_kfree_skb(new_skb);\n\t\t\t\tret = -1;\n\t\t\t}\n\t\t}\n\t}\n\n\tdev_kfree_skb(skb);\n\t*pskb = new_skb;\n\treturn ret;\n}\n\nstatic netdev_tx_t tg3_start_xmit(struct sk_buff *, struct net_device *);\n\n/* Use GSO to workaround a rare TSO bug that may be triggered when the\n * TSO header is greater than 80 bytes.\n */\nstatic int tg3_tso_bug(struct tg3 *tp, struct sk_buff *skb)\n{\n\tstruct sk_buff *segs, *nskb;\n\tu32 frag_cnt_est = skb_shinfo(skb)->gso_segs * 3;\n\n\t/* Estimate the number of fragments in the worst case */\n\tif (unlikely(tg3_tx_avail(&tp->napi[0]) <= frag_cnt_est)) {\n\t\tnetif_stop_queue(tp->dev);\n\n\t\t/* netif_tx_stop_queue() must be done before checking\n\t\t * checking tx index in tg3_tx_avail() below, because in\n\t\t * tg3_tx(), we update tx index before checking for\n\t\t * netif_tx_queue_stopped().\n\t\t */\n\t\tsmp_mb();\n\t\tif (tg3_tx_avail(&tp->napi[0]) <= frag_cnt_est)\n\t\t\treturn NETDEV_TX_BUSY;\n\n\t\tnetif_wake_queue(tp->dev);\n\t}\n\n\tsegs = skb_gso_segment(skb, tp->dev->features & ~NETIF_F_TSO);\n\tif (IS_ERR(segs))\n\t\tgoto tg3_tso_bug_end;\n\n\tdo {\n\t\tnskb = segs;\n\t\tsegs = segs->next;\n\t\tnskb->next = NULL;\n\t\ttg3_start_xmit(nskb, tp->dev);\n\t} while (segs);\n\ntg3_tso_bug_end:\n\tdev_kfree_skb(skb);\n\n\treturn NETDEV_TX_OK;\n}\n\n/* hard_start_xmit for devices that have the 4G bug and/or 40-bit bug and\n * support TG3_FLAG_HW_TSO_1 or firmware TSO only.\n */\nstatic netdev_tx_t tg3_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 len, entry, base_flags, mss, vlan = 0;\n\tu32 budget;\n\tint i = -1, would_hit_hwbug;\n\tdma_addr_t mapping;\n\tstruct tg3_napi *tnapi;\n\tstruct netdev_queue *txq;\n\tunsigned int last;\n\n\ttxq = netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\n\ttnapi = &tp->napi[skb_get_queue_mapping(skb)];\n\tif (tg3_flag(tp, ENABLE_TSS))\n\t\ttnapi++;\n\n\tbudget = tg3_tx_avail(tnapi);\n\n\t/* We are running in BH disabled context with netif_tx_lock\n\t * and TX reclaim runs via tp->napi.poll inside of a software\n\t * interrupt.  Furthermore, IRQ processing runs lockless so we have\n\t * no IRQ context deadlocks to worry about either.  Rejoice!\n\t */\n\tif (unlikely(budget <= (skb_shinfo(skb)->nr_frags + 1))) {\n\t\tif (!netif_tx_queue_stopped(txq)) {\n\t\t\tnetif_tx_stop_queue(txq);\n\n\t\t\t/* This is a hard error, log it. */\n\t\t\tnetdev_err(dev,\n\t\t\t\t   \"BUG! Tx Ring full when queue awake!\\n\");\n\t\t}\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tentry = tnapi->tx_prod;\n\tbase_flags = 0;\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tbase_flags |= TXD_FLAG_TCPUDP_CSUM;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (mss) {\n\t\tstruct iphdr *iph;\n\t\tu32 tcp_opt_len, hdr_len;\n\n\t\tif (skb_header_cloned(skb) &&\n\t\t    pskb_expand_head(skb, 0, 0, GFP_ATOMIC))\n\t\t\tgoto drop;\n\n\t\tiph = ip_hdr(skb);\n\t\ttcp_opt_len = tcp_optlen(skb);\n\n\t\thdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb) - ETH_HLEN;\n\n\t\tif (!skb_is_gso_v6(skb)) {\n\t\t\tiph->check = 0;\n\t\t\tiph->tot_len = htons(mss + hdr_len);\n\t\t}\n\n\t\tif (unlikely((ETH_HLEN + hdr_len) > 80) &&\n\t\t    tg3_flag(tp, TSO_BUG))\n\t\t\treturn tg3_tso_bug(tp, skb);\n\n\t\tbase_flags |= (TXD_FLAG_CPU_PRE_DMA |\n\t\t\t       TXD_FLAG_CPU_POST_DMA);\n\n\t\tif (tg3_flag(tp, HW_TSO_1) ||\n\t\t    tg3_flag(tp, HW_TSO_2) ||\n\t\t    tg3_flag(tp, HW_TSO_3)) {\n\t\t\ttcp_hdr(skb)->check = 0;\n\t\t\tbase_flags &= ~TXD_FLAG_TCPUDP_CSUM;\n\t\t} else\n\t\t\ttcp_hdr(skb)->check = ~csum_tcpudp_magic(iph->saddr,\n\t\t\t\t\t\t\t\t iph->daddr, 0,\n\t\t\t\t\t\t\t\t IPPROTO_TCP,\n\t\t\t\t\t\t\t\t 0);\n\n\t\tif (tg3_flag(tp, HW_TSO_3)) {\n\t\t\tmss |= (hdr_len & 0xc) << 12;\n\t\t\tif (hdr_len & 0x10)\n\t\t\t\tbase_flags |= 0x00000010;\n\t\t\tbase_flags |= (hdr_len & 0x3e0) << 5;\n\t\t} else if (tg3_flag(tp, HW_TSO_2))\n\t\t\tmss |= hdr_len << 9;\n\t\telse if (tg3_flag(tp, HW_TSO_1) ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\t\tif (tcp_opt_len || iph->ihl > 5) {\n\t\t\t\tint tsflags;\n\n\t\t\t\ttsflags = (iph->ihl - 5) + (tcp_opt_len >> 2);\n\t\t\t\tmss |= (tsflags << 11);\n\t\t\t}\n\t\t} else {\n\t\t\tif (tcp_opt_len || iph->ihl > 5) {\n\t\t\t\tint tsflags;\n\n\t\t\t\ttsflags = (iph->ihl - 5) + (tcp_opt_len >> 2);\n\t\t\t\tbase_flags |= tsflags << 12;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, USE_JUMBO_BDFLAG) &&\n\t    !mss && skb->len > VLAN_ETH_FRAME_LEN)\n\t\tbase_flags |= TXD_FLAG_JMB_PKT;\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tbase_flags |= TXD_FLAG_VLAN;\n\t\tvlan = vlan_tx_tag_get(skb);\n\t}\n\n\tif ((unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) &&\n\t    tg3_flag(tp, TX_TSTAMP_EN)) {\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\tbase_flags |= TXD_FLAG_HWTSTAMP;\n\t}\n\n\tlen = skb_headlen(skb);\n\n\tmapping = pci_map_single(tp->pdev, skb->data, len, PCI_DMA_TODEVICE);\n\tif (pci_dma_mapping_error(tp->pdev, mapping))\n\t\tgoto drop;\n\n\n\ttnapi->tx_buffers[entry].skb = skb;\n\tdma_unmap_addr_set(&tnapi->tx_buffers[entry], mapping, mapping);\n\n\twould_hit_hwbug = 0;\n\n\tif (tg3_flag(tp, 5701_DMA_BUG))\n\t\twould_hit_hwbug = 1;\n\n\tif (tg3_tx_frag_set(tnapi, &entry, &budget, mapping, len, base_flags |\n\t\t\t  ((skb_shinfo(skb)->nr_frags == 0) ? TXD_FLAG_END : 0),\n\t\t\t    mss, vlan)) {\n\t\twould_hit_hwbug = 1;\n\t} else if (skb_shinfo(skb)->nr_frags > 0) {\n\t\tu32 tmp_mss = mss;\n\n\t\tif (!tg3_flag(tp, HW_TSO_1) &&\n\t\t    !tg3_flag(tp, HW_TSO_2) &&\n\t\t    !tg3_flag(tp, HW_TSO_3))\n\t\t\ttmp_mss = 0;\n\n\t\t/* Now loop through additional data\n\t\t * fragments, and queue them.\n\t\t */\n\t\tlast = skb_shinfo(skb)->nr_frags - 1;\n\t\tfor (i = 0; i <= last; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tlen = skb_frag_size(frag);\n\t\t\tmapping = skb_frag_dma_map(&tp->pdev->dev, frag, 0,\n\t\t\t\t\t\t   len, DMA_TO_DEVICE);\n\n\t\t\ttnapi->tx_buffers[entry].skb = NULL;\n\t\t\tdma_unmap_addr_set(&tnapi->tx_buffers[entry], mapping,\n\t\t\t\t\t   mapping);\n\t\t\tif (dma_mapping_error(&tp->pdev->dev, mapping))\n\t\t\t\tgoto dma_error;\n\n\t\t\tif (!budget ||\n\t\t\t    tg3_tx_frag_set(tnapi, &entry, &budget, mapping,\n\t\t\t\t\t    len, base_flags |\n\t\t\t\t\t    ((i == last) ? TXD_FLAG_END : 0),\n\t\t\t\t\t    tmp_mss, vlan)) {\n\t\t\t\twould_hit_hwbug = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (would_hit_hwbug) {\n\t\ttg3_tx_skb_unmap(tnapi, tnapi->tx_prod, i);\n\n\t\t/* If the workaround fails due to memory/mapping\n\t\t * failure, silently drop this packet.\n\t\t */\n\t\tentry = tnapi->tx_prod;\n\t\tbudget = tg3_tx_avail(tnapi);\n\t\tif (tigon3_dma_hwbug_workaround(tnapi, &skb, &entry, &budget,\n\t\t\t\t\t\tbase_flags, mss, vlan))\n\t\t\tgoto drop_nofree;\n\t}\n\n\tskb_tx_timestamp(skb);\n\tnetdev_tx_sent_queue(txq, skb->len);\n\n\t/* Sync BD data before updating mailbox */\n\twmb();\n\n\t/* Packets are ready, update Tx producer idx local and on card. */\n\ttw32_tx_mbox(tnapi->prodmbox, entry);\n\n\ttnapi->tx_prod = entry;\n\tif (unlikely(tg3_tx_avail(tnapi) <= (MAX_SKB_FRAGS + 1))) {\n\t\tnetif_tx_stop_queue(txq);\n\n\t\t/* netif_tx_stop_queue() must be done before checking\n\t\t * checking tx index in tg3_tx_avail() below, because in\n\t\t * tg3_tx(), we update tx index before checking for\n\t\t * netif_tx_queue_stopped().\n\t\t */\n\t\tsmp_mb();\n\t\tif (tg3_tx_avail(tnapi) > TG3_TX_WAKEUP_THRESH(tnapi))\n\t\t\tnetif_tx_wake_queue(txq);\n\t}\n\n\tmmiowb();\n\treturn NETDEV_TX_OK;\n\ndma_error:\n\ttg3_tx_skb_unmap(tnapi, tnapi->tx_prod, --i);\n\ttnapi->tx_buffers[tnapi->tx_prod].skb = NULL;\ndrop:\n\tdev_kfree_skb(skb);\ndrop_nofree:\n\ttp->tx_dropped++;\n\treturn NETDEV_TX_OK;\n}\n\nstatic void tg3_mac_loopback(struct tg3 *tp, bool enable)\n{\n\tif (enable) {\n\t\ttp->mac_mode &= ~(MAC_MODE_HALF_DUPLEX |\n\t\t\t\t  MAC_MODE_PORT_MODE_MASK);\n\n\t\ttp->mac_mode |= MAC_MODE_PORT_INT_LPBACK;\n\n\t\tif (!tg3_flag(tp, 5705_PLUS))\n\t\t\ttp->mac_mode |= MAC_MODE_LINK_POLARITY;\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_10_100_ONLY)\n\t\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_MII;\n\t\telse\n\t\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_GMII;\n\t} else {\n\t\ttp->mac_mode &= ~MAC_MODE_PORT_INT_LPBACK;\n\n\t\tif (tg3_flag(tp, 5705_PLUS) ||\n\t\t    (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5700)\n\t\t\ttp->mac_mode &= ~MAC_MODE_LINK_POLARITY;\n\t}\n\n\ttw32(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n}\n\nstatic int tg3_phy_lpbk_set(struct tg3 *tp, u32 speed, bool extlpbk)\n{\n\tu32 val, bmcr, mac_mode, ptest = 0;\n\n\ttg3_phy_toggle_apd(tp, false);\n\ttg3_phy_toggle_automdix(tp, 0);\n\n\tif (extlpbk && tg3_phy_set_extloopbk(tp))\n\t\treturn -EIO;\n\n\tbmcr = BMCR_FULLDPLX;\n\tswitch (speed) {\n\tcase SPEED_10:\n\t\tbreak;\n\tcase SPEED_100:\n\t\tbmcr |= BMCR_SPEED100;\n\t\tbreak;\n\tcase SPEED_1000:\n\tdefault:\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\t\tspeed = SPEED_100;\n\t\t\tbmcr |= BMCR_SPEED100;\n\t\t} else {\n\t\t\tspeed = SPEED_1000;\n\t\t\tbmcr |= BMCR_SPEED1000;\n\t\t}\n\t}\n\n\tif (extlpbk) {\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_FET)) {\n\t\t\ttg3_readphy(tp, MII_CTRL1000, &val);\n\t\t\tval |= CTL1000_AS_MASTER |\n\t\t\t       CTL1000_ENABLE_MASTER;\n\t\t\ttg3_writephy(tp, MII_CTRL1000, val);\n\t\t} else {\n\t\t\tptest = MII_TG3_FET_PTEST_TRIM_SEL |\n\t\t\t\tMII_TG3_FET_PTEST_TRIM_2;\n\t\t\ttg3_writephy(tp, MII_TG3_FET_PTEST, ptest);\n\t\t}\n\t} else\n\t\tbmcr |= BMCR_LOOPBACK;\n\n\ttg3_writephy(tp, MII_BMCR, bmcr);\n\n\t/* The write needs to be flushed for the FETs */\n\tif (tp->phy_flags & TG3_PHYFLG_IS_FET)\n\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\n\tudelay(40);\n\n\tif ((tp->phy_flags & TG3_PHYFLG_IS_FET) &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5785) {\n\t\ttg3_writephy(tp, MII_TG3_FET_PTEST, ptest |\n\t\t\t     MII_TG3_FET_PTEST_FRC_TX_LINK |\n\t\t\t     MII_TG3_FET_PTEST_FRC_TX_LOCK);\n\n\t\t/* The write needs to be flushed for the AC131 */\n\t\ttg3_readphy(tp, MII_TG3_FET_PTEST, &val);\n\t}\n\n\t/* Reset to prevent losing 1st rx packet intermittently */\n\tif ((tp->phy_flags & TG3_PHYFLG_MII_SERDES) &&\n\t    tg3_flag(tp, 5780_CLASS)) {\n\t\ttw32_f(MAC_RX_MODE, RX_MODE_RESET);\n\t\tudelay(10);\n\t\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\t}\n\n\tmac_mode = tp->mac_mode &\n\t\t   ~(MAC_MODE_PORT_MODE_MASK | MAC_MODE_HALF_DUPLEX);\n\tif (speed == SPEED_1000)\n\t\tmac_mode |= MAC_MODE_PORT_MODE_GMII;\n\telse\n\t\tmac_mode |= MAC_MODE_PORT_MODE_MII;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700) {\n\t\tu32 masked_phy_id = tp->phy_id & TG3_PHY_ID_MASK;\n\n\t\tif (masked_phy_id == TG3_PHY_ID_BCM5401)\n\t\t\tmac_mode &= ~MAC_MODE_LINK_POLARITY;\n\t\telse if (masked_phy_id == TG3_PHY_ID_BCM5411)\n\t\t\tmac_mode |= MAC_MODE_LINK_POLARITY;\n\n\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL,\n\t\t\t     MII_TG3_EXT_CTRL_LNK3_LED_MODE);\n\t}\n\n\ttw32(MAC_MODE, mac_mode);\n\tudelay(40);\n\n\treturn 0;\n}\n\nstatic void tg3_set_loopback(struct net_device *dev, netdev_features_t features)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (features & NETIF_F_LOOPBACK) {\n\t\tif (tp->mac_mode & MAC_MODE_PORT_INT_LPBACK)\n\t\t\treturn;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\ttg3_mac_loopback(tp, true);\n\t\tnetif_carrier_on(tp->dev);\n\t\tspin_unlock_bh(&tp->lock);\n\t\tnetdev_info(dev, \"Internal MAC loopback mode enabled.\\n\");\n\t} else {\n\t\tif (!(tp->mac_mode & MAC_MODE_PORT_INT_LPBACK))\n\t\t\treturn;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\ttg3_mac_loopback(tp, false);\n\t\t/* Force link status check */\n\t\ttg3_setup_phy(tp, 1);\n\t\tspin_unlock_bh(&tp->lock);\n\t\tnetdev_info(dev, \"Internal MAC loopback mode disabled.\\n\");\n\t}\n}\n\nstatic netdev_features_t tg3_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (dev->mtu > ETH_DATA_LEN && tg3_flag(tp, 5780_CLASS))\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\n\treturn features;\n}\n\nstatic int tg3_set_features(struct net_device *dev, netdev_features_t features)\n{\n\tnetdev_features_t changed = dev->features ^ features;\n\n\tif ((changed & NETIF_F_LOOPBACK) && netif_running(dev))\n\t\ttg3_set_loopback(dev, features);\n\n\treturn 0;\n}\n\nstatic void tg3_rx_prodring_free(struct tg3 *tp,\n\t\t\t\t struct tg3_rx_prodring_set *tpr)\n{\n\tint i;\n\n\tif (tpr != &tp->napi[0].prodring) {\n\t\tfor (i = tpr->rx_std_cons_idx; i != tpr->rx_std_prod_idx;\n\t\t     i = (i + 1) & tp->rx_std_ring_mask)\n\t\t\ttg3_rx_data_free(tp, &tpr->rx_std_buffers[i],\n\t\t\t\t\ttp->rx_pkt_map_sz);\n\n\t\tif (tg3_flag(tp, JUMBO_CAPABLE)) {\n\t\t\tfor (i = tpr->rx_jmb_cons_idx;\n\t\t\t     i != tpr->rx_jmb_prod_idx;\n\t\t\t     i = (i + 1) & tp->rx_jmb_ring_mask) {\n\t\t\t\ttg3_rx_data_free(tp, &tpr->rx_jmb_buffers[i],\n\t\t\t\t\t\tTG3_RX_JMB_MAP_SZ);\n\t\t\t}\n\t\t}\n\n\t\treturn;\n\t}\n\n\tfor (i = 0; i <= tp->rx_std_ring_mask; i++)\n\t\ttg3_rx_data_free(tp, &tpr->rx_std_buffers[i],\n\t\t\t\ttp->rx_pkt_map_sz);\n\n\tif (tg3_flag(tp, JUMBO_CAPABLE) && !tg3_flag(tp, 5780_CLASS)) {\n\t\tfor (i = 0; i <= tp->rx_jmb_ring_mask; i++)\n\t\t\ttg3_rx_data_free(tp, &tpr->rx_jmb_buffers[i],\n\t\t\t\t\tTG3_RX_JMB_MAP_SZ);\n\t}\n}\n\n/* Initialize rx rings for packet processing.\n *\n * The chip has been shut down and the driver detached from\n * the networking, so no interrupts or new tx packets will\n * end up in the driver.  tp->{tx,}lock are held and thus\n * we may not sleep.\n */\nstatic int tg3_rx_prodring_alloc(struct tg3 *tp,\n\t\t\t\t struct tg3_rx_prodring_set *tpr)\n{\n\tu32 i, rx_pkt_dma_sz;\n\n\ttpr->rx_std_cons_idx = 0;\n\ttpr->rx_std_prod_idx = 0;\n\ttpr->rx_jmb_cons_idx = 0;\n\ttpr->rx_jmb_prod_idx = 0;\n\n\tif (tpr != &tp->napi[0].prodring) {\n\t\tmemset(&tpr->rx_std_buffers[0], 0,\n\t\t       TG3_RX_STD_BUFF_RING_SIZE(tp));\n\t\tif (tpr->rx_jmb_buffers)\n\t\t\tmemset(&tpr->rx_jmb_buffers[0], 0,\n\t\t\t       TG3_RX_JMB_BUFF_RING_SIZE(tp));\n\t\tgoto done;\n\t}\n\n\t/* Zero out all descriptors. */\n\tmemset(tpr->rx_std, 0, TG3_RX_STD_RING_BYTES(tp));\n\n\trx_pkt_dma_sz = TG3_RX_STD_DMA_SZ;\n\tif (tg3_flag(tp, 5780_CLASS) &&\n\t    tp->dev->mtu > ETH_DATA_LEN)\n\t\trx_pkt_dma_sz = TG3_RX_JMB_DMA_SZ;\n\ttp->rx_pkt_map_sz = TG3_RX_DMA_TO_MAP_SZ(rx_pkt_dma_sz);\n\n\t/* Initialize invariants of the rings, we only set this\n\t * stuff once.  This works because the card does not\n\t * write into the rx buffer posting rings.\n\t */\n\tfor (i = 0; i <= tp->rx_std_ring_mask; i++) {\n\t\tstruct tg3_rx_buffer_desc *rxd;\n\n\t\trxd = &tpr->rx_std[i];\n\t\trxd->idx_len = rx_pkt_dma_sz << RXD_LEN_SHIFT;\n\t\trxd->type_flags = (RXD_FLAG_END << RXD_FLAGS_SHIFT);\n\t\trxd->opaque = (RXD_OPAQUE_RING_STD |\n\t\t\t       (i << RXD_OPAQUE_INDEX_SHIFT));\n\t}\n\n\t/* Now allocate fresh SKBs for each rx ring. */\n\tfor (i = 0; i < tp->rx_pending; i++) {\n\t\tunsigned int frag_size;\n\n\t\tif (tg3_alloc_rx_data(tp, tpr, RXD_OPAQUE_RING_STD, i,\n\t\t\t\t      &frag_size) < 0) {\n\t\t\tnetdev_warn(tp->dev,\n\t\t\t\t    \"Using a smaller RX standard ring. Only \"\n\t\t\t\t    \"%d out of %d buffers were allocated \"\n\t\t\t\t    \"successfully\\n\", i, tp->rx_pending);\n\t\t\tif (i == 0)\n\t\t\t\tgoto initfail;\n\t\t\ttp->rx_pending = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!tg3_flag(tp, JUMBO_CAPABLE) || tg3_flag(tp, 5780_CLASS))\n\t\tgoto done;\n\n\tmemset(tpr->rx_jmb, 0, TG3_RX_JMB_RING_BYTES(tp));\n\n\tif (!tg3_flag(tp, JUMBO_RING_ENABLE))\n\t\tgoto done;\n\n\tfor (i = 0; i <= tp->rx_jmb_ring_mask; i++) {\n\t\tstruct tg3_rx_buffer_desc *rxd;\n\n\t\trxd = &tpr->rx_jmb[i].std;\n\t\trxd->idx_len = TG3_RX_JMB_DMA_SZ << RXD_LEN_SHIFT;\n\t\trxd->type_flags = (RXD_FLAG_END << RXD_FLAGS_SHIFT) |\n\t\t\t\t  RXD_FLAG_JUMBO;\n\t\trxd->opaque = (RXD_OPAQUE_RING_JUMBO |\n\t\t       (i << RXD_OPAQUE_INDEX_SHIFT));\n\t}\n\n\tfor (i = 0; i < tp->rx_jumbo_pending; i++) {\n\t\tunsigned int frag_size;\n\n\t\tif (tg3_alloc_rx_data(tp, tpr, RXD_OPAQUE_RING_JUMBO, i,\n\t\t\t\t      &frag_size) < 0) {\n\t\t\tnetdev_warn(tp->dev,\n\t\t\t\t    \"Using a smaller RX jumbo ring. Only %d \"\n\t\t\t\t    \"out of %d buffers were allocated \"\n\t\t\t\t    \"successfully\\n\", i, tp->rx_jumbo_pending);\n\t\t\tif (i == 0)\n\t\t\t\tgoto initfail;\n\t\t\ttp->rx_jumbo_pending = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\ndone:\n\treturn 0;\n\ninitfail:\n\ttg3_rx_prodring_free(tp, tpr);\n\treturn -ENOMEM;\n}\n\nstatic void tg3_rx_prodring_fini(struct tg3 *tp,\n\t\t\t\t struct tg3_rx_prodring_set *tpr)\n{\n\tkfree(tpr->rx_std_buffers);\n\ttpr->rx_std_buffers = NULL;\n\tkfree(tpr->rx_jmb_buffers);\n\ttpr->rx_jmb_buffers = NULL;\n\tif (tpr->rx_std) {\n\t\tdma_free_coherent(&tp->pdev->dev, TG3_RX_STD_RING_BYTES(tp),\n\t\t\t\t  tpr->rx_std, tpr->rx_std_mapping);\n\t\ttpr->rx_std = NULL;\n\t}\n\tif (tpr->rx_jmb) {\n\t\tdma_free_coherent(&tp->pdev->dev, TG3_RX_JMB_RING_BYTES(tp),\n\t\t\t\t  tpr->rx_jmb, tpr->rx_jmb_mapping);\n\t\ttpr->rx_jmb = NULL;\n\t}\n}\n\nstatic int tg3_rx_prodring_init(struct tg3 *tp,\n\t\t\t\tstruct tg3_rx_prodring_set *tpr)\n{\n\ttpr->rx_std_buffers = kzalloc(TG3_RX_STD_BUFF_RING_SIZE(tp),\n\t\t\t\t      GFP_KERNEL);\n\tif (!tpr->rx_std_buffers)\n\t\treturn -ENOMEM;\n\n\ttpr->rx_std = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t TG3_RX_STD_RING_BYTES(tp),\n\t\t\t\t\t &tpr->rx_std_mapping,\n\t\t\t\t\t GFP_KERNEL);\n\tif (!tpr->rx_std)\n\t\tgoto err_out;\n\n\tif (tg3_flag(tp, JUMBO_CAPABLE) && !tg3_flag(tp, 5780_CLASS)) {\n\t\ttpr->rx_jmb_buffers = kzalloc(TG3_RX_JMB_BUFF_RING_SIZE(tp),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!tpr->rx_jmb_buffers)\n\t\t\tgoto err_out;\n\n\t\ttpr->rx_jmb = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t\t TG3_RX_JMB_RING_BYTES(tp),\n\t\t\t\t\t\t &tpr->rx_jmb_mapping,\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (!tpr->rx_jmb)\n\t\t\tgoto err_out;\n\t}\n\n\treturn 0;\n\nerr_out:\n\ttg3_rx_prodring_fini(tp, tpr);\n\treturn -ENOMEM;\n}\n\n/* Free up pending packets in all rx/tx rings.\n *\n * The chip has been shut down and the driver detached from\n * the networking, so no interrupts or new tx packets will\n * end up in the driver.  tp->{tx,}lock is not held and we are not\n * in an interrupt context and thus may sleep.\n */\nstatic void tg3_free_rings(struct tg3 *tp)\n{\n\tint i, j;\n\n\tfor (j = 0; j < tp->irq_cnt; j++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[j];\n\n\t\ttg3_rx_prodring_free(tp, &tnapi->prodring);\n\n\t\tif (!tnapi->tx_buffers)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < TG3_TX_RING_SIZE; i++) {\n\t\t\tstruct sk_buff *skb = tnapi->tx_buffers[i].skb;\n\n\t\t\tif (!skb)\n\t\t\t\tcontinue;\n\n\t\t\ttg3_tx_skb_unmap(tnapi, i,\n\t\t\t\t\t skb_shinfo(skb)->nr_frags - 1);\n\n\t\t\tdev_kfree_skb_any(skb);\n\t\t}\n\t\tnetdev_tx_reset_queue(netdev_get_tx_queue(tp->dev, j));\n\t}\n}\n\n/* Initialize tx/rx rings for packet processing.\n *\n * The chip has been shut down and the driver detached from\n * the networking, so no interrupts or new tx packets will\n * end up in the driver.  tp->{tx,}lock are held and thus\n * we may not sleep.\n */\nstatic int tg3_init_rings(struct tg3 *tp)\n{\n\tint i;\n\n\t/* Free up all the SKBs. */\n\ttg3_free_rings(tp);\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\ttnapi->last_tag = 0;\n\t\ttnapi->last_irq_tag = 0;\n\t\ttnapi->hw_status->status = 0;\n\t\ttnapi->hw_status->status_tag = 0;\n\t\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\n\t\ttnapi->tx_prod = 0;\n\t\ttnapi->tx_cons = 0;\n\t\tif (tnapi->tx_ring)\n\t\t\tmemset(tnapi->tx_ring, 0, TG3_TX_RING_BYTES);\n\n\t\ttnapi->rx_rcb_ptr = 0;\n\t\tif (tnapi->rx_rcb)\n\t\t\tmemset(tnapi->rx_rcb, 0, TG3_RX_RCB_RING_BYTES(tp));\n\n\t\tif (tg3_rx_prodring_alloc(tp, &tnapi->prodring)) {\n\t\t\ttg3_free_rings(tp);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_mem_tx_release(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_max; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\tif (tnapi->tx_ring) {\n\t\t\tdma_free_coherent(&tp->pdev->dev, TG3_TX_RING_BYTES,\n\t\t\t\ttnapi->tx_ring, tnapi->tx_desc_mapping);\n\t\t\ttnapi->tx_ring = NULL;\n\t\t}\n\n\t\tkfree(tnapi->tx_buffers);\n\t\ttnapi->tx_buffers = NULL;\n\t}\n}\n\nstatic int tg3_mem_tx_acquire(struct tg3 *tp)\n{\n\tint i;\n\tstruct tg3_napi *tnapi = &tp->napi[0];\n\n\t/* If multivector TSS is enabled, vector 0 does not handle\n\t * tx interrupts.  Don't allocate any resources for it.\n\t */\n\tif (tg3_flag(tp, ENABLE_TSS))\n\t\ttnapi++;\n\n\tfor (i = 0; i < tp->txq_cnt; i++, tnapi++) {\n\t\ttnapi->tx_buffers = kzalloc(sizeof(struct tg3_tx_ring_info) *\n\t\t\t\t\t    TG3_TX_RING_SIZE, GFP_KERNEL);\n\t\tif (!tnapi->tx_buffers)\n\t\t\tgoto err_out;\n\n\t\ttnapi->tx_ring = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t\t    TG3_TX_RING_BYTES,\n\t\t\t\t\t\t    &tnapi->tx_desc_mapping,\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!tnapi->tx_ring)\n\t\t\tgoto err_out;\n\t}\n\n\treturn 0;\n\nerr_out:\n\ttg3_mem_tx_release(tp);\n\treturn -ENOMEM;\n}\n\nstatic void tg3_mem_rx_release(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_max; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\ttg3_rx_prodring_fini(tp, &tnapi->prodring);\n\n\t\tif (!tnapi->rx_rcb)\n\t\t\tcontinue;\n\n\t\tdma_free_coherent(&tp->pdev->dev,\n\t\t\t\t  TG3_RX_RCB_RING_BYTES(tp),\n\t\t\t\t  tnapi->rx_rcb,\n\t\t\t\t  tnapi->rx_rcb_mapping);\n\t\ttnapi->rx_rcb = NULL;\n\t}\n}\n\nstatic int tg3_mem_rx_acquire(struct tg3 *tp)\n{\n\tunsigned int i, limit;\n\n\tlimit = tp->rxq_cnt;\n\n\t/* If RSS is enabled, we need a (dummy) producer ring\n\t * set on vector zero.  This is the true hw prodring.\n\t */\n\tif (tg3_flag(tp, ENABLE_RSS))\n\t\tlimit++;\n\n\tfor (i = 0; i < limit; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\tif (tg3_rx_prodring_init(tp, &tnapi->prodring))\n\t\t\tgoto err_out;\n\n\t\t/* If multivector RSS is enabled, vector 0\n\t\t * does not handle rx or tx interrupts.\n\t\t * Don't allocate any resources for it.\n\t\t */\n\t\tif (!i && tg3_flag(tp, ENABLE_RSS))\n\t\t\tcontinue;\n\n\t\ttnapi->rx_rcb = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t\t   TG3_RX_RCB_RING_BYTES(tp),\n\t\t\t\t\t\t   &tnapi->rx_rcb_mapping,\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!tnapi->rx_rcb)\n\t\t\tgoto err_out;\n\n\t\tmemset(tnapi->rx_rcb, 0, TG3_RX_RCB_RING_BYTES(tp));\n\t}\n\n\treturn 0;\n\nerr_out:\n\ttg3_mem_rx_release(tp);\n\treturn -ENOMEM;\n}\n\n/*\n * Must not be invoked with interrupt sources disabled and\n * the hardware shutdown down.\n */\nstatic void tg3_free_consistent(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\tif (tnapi->hw_status) {\n\t\t\tdma_free_coherent(&tp->pdev->dev, TG3_HW_STATUS_SIZE,\n\t\t\t\t\t  tnapi->hw_status,\n\t\t\t\t\t  tnapi->status_mapping);\n\t\t\ttnapi->hw_status = NULL;\n\t\t}\n\t}\n\n\ttg3_mem_rx_release(tp);\n\ttg3_mem_tx_release(tp);\n\n\tif (tp->hw_stats) {\n\t\tdma_free_coherent(&tp->pdev->dev, sizeof(struct tg3_hw_stats),\n\t\t\t\t  tp->hw_stats, tp->stats_mapping);\n\t\ttp->hw_stats = NULL;\n\t}\n}\n\n/*\n * Must not be invoked with interrupt sources disabled and\n * the hardware shutdown down.  Can sleep.\n */\nstatic int tg3_alloc_consistent(struct tg3 *tp)\n{\n\tint i;\n\n\ttp->hw_stats = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t  sizeof(struct tg3_hw_stats),\n\t\t\t\t\t  &tp->stats_mapping,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!tp->hw_stats)\n\t\tgoto err_out;\n\n\tmemset(tp->hw_stats, 0, sizeof(struct tg3_hw_stats));\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tstruct tg3_hw_status *sblk;\n\n\t\ttnapi->hw_status = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t\t      TG3_HW_STATUS_SIZE,\n\t\t\t\t\t\t      &tnapi->status_mapping,\n\t\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!tnapi->hw_status)\n\t\t\tgoto err_out;\n\n\t\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\t\tsblk = tnapi->hw_status;\n\n\t\tif (tg3_flag(tp, ENABLE_RSS)) {\n\t\t\tu16 *prodptr = NULL;\n\n\t\t\t/*\n\t\t\t * When RSS is enabled, the status block format changes\n\t\t\t * slightly.  The \"rx_jumbo_consumer\", \"reserved\",\n\t\t\t * and \"rx_mini_consumer\" members get mapped to the\n\t\t\t * other three rx return ring producer indexes.\n\t\t\t */\n\t\t\tswitch (i) {\n\t\t\tcase 1:\n\t\t\t\tprodptr = &sblk->idx[0].rx_producer;\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tprodptr = &sblk->rx_jumbo_consumer;\n\t\t\t\tbreak;\n\t\t\tcase 3:\n\t\t\t\tprodptr = &sblk->reserved;\n\t\t\t\tbreak;\n\t\t\tcase 4:\n\t\t\t\tprodptr = &sblk->rx_mini_consumer;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttnapi->rx_rcb_prod_idx = prodptr;\n\t\t} else {\n\t\t\ttnapi->rx_rcb_prod_idx = &sblk->idx[0].rx_producer;\n\t\t}\n\t}\n\n\tif (tg3_mem_tx_acquire(tp) || tg3_mem_rx_acquire(tp))\n\t\tgoto err_out;\n\n\treturn 0;\n\nerr_out:\n\ttg3_free_consistent(tp);\n\treturn -ENOMEM;\n}\n\n#define MAX_WAIT_CNT 1000\n\n/* To stop a block, clear the enable bit and poll till it\n * clears.  tp->lock is held.\n */\nstatic int tg3_stop_block(struct tg3 *tp, unsigned long ofs, u32 enable_bit, int silent)\n{\n\tunsigned int i;\n\tu32 val;\n\n\tif (tg3_flag(tp, 5705_PLUS)) {\n\t\tswitch (ofs) {\n\t\tcase RCVLSC_MODE:\n\t\tcase DMAC_MODE:\n\t\tcase MBFREE_MODE:\n\t\tcase BUFMGR_MODE:\n\t\tcase MEMARB_MODE:\n\t\t\t/* We can't enable/disable these bits of the\n\t\t\t * 5705/5750, just say success.\n\t\t\t */\n\t\t\treturn 0;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tval = tr32(ofs);\n\tval &= ~enable_bit;\n\ttw32_f(ofs, val);\n\n\tfor (i = 0; i < MAX_WAIT_CNT; i++) {\n\t\tudelay(100);\n\t\tval = tr32(ofs);\n\t\tif ((val & enable_bit) == 0)\n\t\t\tbreak;\n\t}\n\n\tif (i == MAX_WAIT_CNT && !silent) {\n\t\tdev_err(&tp->pdev->dev,\n\t\t\t\"tg3_stop_block timed out, ofs=%lx enable_bit=%x\\n\",\n\t\t\tofs, enable_bit);\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\n/* tp->lock is held. */\nstatic int tg3_abort_hw(struct tg3 *tp, int silent)\n{\n\tint i, err;\n\n\ttg3_disable_ints(tp);\n\n\ttp->rx_mode &= ~RX_MODE_ENABLE;\n\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\tudelay(10);\n\n\terr  = tg3_stop_block(tp, RCVBDI_MODE, RCVBDI_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVLPC_MODE, RCVLPC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVLSC_MODE, RCVLSC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVDBDI_MODE, RCVDBDI_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVDCC_MODE, RCVDCC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVCC_MODE, RCVCC_MODE_ENABLE, silent);\n\n\terr |= tg3_stop_block(tp, SNDBDS_MODE, SNDBDS_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, SNDBDI_MODE, SNDBDI_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, SNDDATAI_MODE, SNDDATAI_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RDMAC_MODE, RDMAC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, SNDDATAC_MODE, SNDDATAC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, DMAC_MODE, DMAC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, SNDBDC_MODE, SNDBDC_MODE_ENABLE, silent);\n\n\ttp->mac_mode &= ~MAC_MODE_TDE_ENABLE;\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\ttp->tx_mode &= ~TX_MODE_ENABLE;\n\ttw32_f(MAC_TX_MODE, tp->tx_mode);\n\n\tfor (i = 0; i < MAX_WAIT_CNT; i++) {\n\t\tudelay(100);\n\t\tif (!(tr32(MAC_TX_MODE) & TX_MODE_ENABLE))\n\t\t\tbreak;\n\t}\n\tif (i >= MAX_WAIT_CNT) {\n\t\tdev_err(&tp->pdev->dev,\n\t\t\t\"%s timed out, TX_MODE_ENABLE will not clear \"\n\t\t\t\"MAC_TX_MODE=%08x\\n\", __func__, tr32(MAC_TX_MODE));\n\t\terr |= -ENODEV;\n\t}\n\n\terr |= tg3_stop_block(tp, HOSTCC_MODE, HOSTCC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, WDMAC_MODE, WDMAC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, MBFREE_MODE, MBFREE_MODE_ENABLE, silent);\n\n\ttw32(FTQ_RESET, 0xffffffff);\n\ttw32(FTQ_RESET, 0x00000000);\n\n\terr |= tg3_stop_block(tp, BUFMGR_MODE, BUFMGR_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, MEMARB_MODE, MEMARB_MODE_ENABLE, silent);\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tif (tnapi->hw_status)\n\t\t\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\t}\n\n\treturn err;\n}\n\n/* Save PCI command register before chip reset */\nstatic void tg3_save_pci_state(struct tg3 *tp)\n{\n\tpci_read_config_word(tp->pdev, PCI_COMMAND, &tp->pci_cmd);\n}\n\n/* Restore PCI state after chip reset */\nstatic void tg3_restore_pci_state(struct tg3 *tp)\n{\n\tu32 val;\n\n\t/* Re-enable indirect register accesses. */\n\tpci_write_config_dword(tp->pdev, TG3PCI_MISC_HOST_CTRL,\n\t\t\t       tp->misc_host_ctrl);\n\n\t/* Set MAX PCI retry to zero. */\n\tval = (PCISTATE_ROM_ENABLE | PCISTATE_ROM_RETRY_ENABLE);\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5704_A0 &&\n\t    tg3_flag(tp, PCIX_MODE))\n\t\tval |= PCISTATE_RETRY_SAME_DMA;\n\t/* Allow reads and writes to the APE register and memory space. */\n\tif (tg3_flag(tp, ENABLE_APE))\n\t\tval |= PCISTATE_ALLOW_APE_CTLSPC_WR |\n\t\t       PCISTATE_ALLOW_APE_SHMEM_WR |\n\t\t       PCISTATE_ALLOW_APE_PSPACE_WR;\n\tpci_write_config_dword(tp->pdev, TG3PCI_PCISTATE, val);\n\n\tpci_write_config_word(tp->pdev, PCI_COMMAND, tp->pci_cmd);\n\n\tif (!tg3_flag(tp, PCI_EXPRESS)) {\n\t\tpci_write_config_byte(tp->pdev, PCI_CACHE_LINE_SIZE,\n\t\t\t\t      tp->pci_cacheline_sz);\n\t\tpci_write_config_byte(tp->pdev, PCI_LATENCY_TIMER,\n\t\t\t\t      tp->pci_lat_timer);\n\t}\n\n\t/* Make sure PCI-X relaxed ordering bit is clear. */\n\tif (tg3_flag(tp, PCIX_MODE)) {\n\t\tu16 pcix_cmd;\n\n\t\tpci_read_config_word(tp->pdev, tp->pcix_cap + PCI_X_CMD,\n\t\t\t\t     &pcix_cmd);\n\t\tpcix_cmd &= ~PCI_X_CMD_ERO;\n\t\tpci_write_config_word(tp->pdev, tp->pcix_cap + PCI_X_CMD,\n\t\t\t\t      pcix_cmd);\n\t}\n\n\tif (tg3_flag(tp, 5780_CLASS)) {\n\n\t\t/* Chip reset on 5780 will reset MSI enable bit,\n\t\t * so need to restore it.\n\t\t */\n\t\tif (tg3_flag(tp, USING_MSI)) {\n\t\t\tu16 ctrl;\n\n\t\t\tpci_read_config_word(tp->pdev,\n\t\t\t\t\t     tp->msi_cap + PCI_MSI_FLAGS,\n\t\t\t\t\t     &ctrl);\n\t\t\tpci_write_config_word(tp->pdev,\n\t\t\t\t\t      tp->msi_cap + PCI_MSI_FLAGS,\n\t\t\t\t\t      ctrl | PCI_MSI_FLAGS_ENABLE);\n\t\t\tval = tr32(MSGINT_MODE);\n\t\t\ttw32(MSGINT_MODE, val | MSGINT_MODE_ENABLE);\n\t\t}\n\t}\n}\n\n/* tp->lock is held. */\nstatic int tg3_chip_reset(struct tg3 *tp)\n{\n\tu32 val;\n\tvoid (*write_op)(struct tg3 *, u32, u32);\n\tint i, err;\n\n\ttg3_nvram_lock(tp);\n\n\ttg3_ape_lock(tp, TG3_APE_LOCK_GRC);\n\n\t/* No matching tg3_nvram_unlock() after this because\n\t * chip reset below will undo the nvram lock.\n\t */\n\ttp->nvram_lock_cnt = 0;\n\n\t/* GRC_MISC_CFG core clock reset will clear the memory\n\t * enable bit in PCI register 4 and the MSI enable bit\n\t * on some chips, so we save relevant registers here.\n\t */\n\ttg3_save_pci_state(tp);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5752 ||\n\t    tg3_flag(tp, 5755_PLUS))\n\t\ttw32(GRC_FASTBOOT_PC, 0);\n\n\t/*\n\t * We must avoid the readl() that normally takes place.\n\t * It locks machines, causes machine checks, and other\n\t * fun things.  So, temporarily disable the 5701\n\t * hardware workaround, while we do the reset.\n\t */\n\twrite_op = tp->write32;\n\tif (write_op == tg3_write_flush_reg32)\n\t\ttp->write32 = tg3_write32;\n\n\t/* Prevent the irq handler from reading or writing PCI registers\n\t * during chip reset when the memory enable bit in the PCI command\n\t * register may be cleared.  The chip does not generate interrupt\n\t * at this time, but the irq handler may still be called due to irq\n\t * sharing or irqpoll.\n\t */\n\ttg3_flag_set(tp, CHIP_RESETTING);\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tif (tnapi->hw_status) {\n\t\t\ttnapi->hw_status->status = 0;\n\t\t\ttnapi->hw_status->status_tag = 0;\n\t\t}\n\t\ttnapi->last_tag = 0;\n\t\ttnapi->last_irq_tag = 0;\n\t}\n\tsmp_mb();\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\tsynchronize_irq(tp->napi[i].irq_vec);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_57780) {\n\t\tval = tr32(TG3_PCIE_LNKCTL) & ~TG3_PCIE_LNKCTL_L1_PLL_PD_EN;\n\t\ttw32(TG3_PCIE_LNKCTL, val | TG3_PCIE_LNKCTL_L1_PLL_PD_DIS);\n\t}\n\n\t/* do the reset */\n\tval = GRC_MISC_CFG_CORECLK_RESET;\n\n\tif (tg3_flag(tp, PCI_EXPRESS)) {\n\t\t/* Force PCIe 1.0a mode */\n\t\tif (tg3_asic_rev(tp) != ASIC_REV_5785 &&\n\t\t    !tg3_flag(tp, 57765_PLUS) &&\n\t\t    tr32(TG3_PCIE_PHY_TSTCTL) ==\n\t\t    (TG3_PCIE_PHY_TSTCTL_PCIE10 | TG3_PCIE_PHY_TSTCTL_PSCRAM))\n\t\t\ttw32(TG3_PCIE_PHY_TSTCTL, TG3_PCIE_PHY_TSTCTL_PSCRAM);\n\n\t\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A0) {\n\t\t\ttw32(GRC_MISC_CFG, (1 << 29));\n\t\t\tval |= (1 << 29);\n\t\t}\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\ttw32(VCPU_STATUS, tr32(VCPU_STATUS) | VCPU_STATUS_DRV_RESET);\n\t\ttw32(GRC_VCPU_EXT_CTRL,\n\t\t     tr32(GRC_VCPU_EXT_CTRL) & ~GRC_VCPU_EXT_CTRL_HALT_CPU);\n\t}\n\n\t/* Manage gphy power for all CPMU absent PCIe devices. */\n\tif (tg3_flag(tp, 5705_PLUS) && !tg3_flag(tp, CPMU_PRESENT))\n\t\tval |= GRC_MISC_CFG_KEEP_GPHY_POWER;\n\n\ttw32(GRC_MISC_CFG, val);\n\n\t/* restore 5701 hardware bug workaround write method */\n\ttp->write32 = write_op;\n\n\t/* Unfortunately, we have to delay before the PCI read back.\n\t * Some 575X chips even will not respond to a PCI cfg access\n\t * when the reset command is given to the chip.\n\t *\n\t * How do these hardware designers expect things to work\n\t * properly if the PCI write is posted for a long period\n\t * of time?  It is always necessary to have some method by\n\t * which a register read back can occur to push the write\n\t * out which does the reset.\n\t *\n\t * For most tg3 variants the trick below was working.\n\t * Ho hum...\n\t */\n\tudelay(120);\n\n\t/* Flush PCI posted writes.  The normal MMIO registers\n\t * are inaccessible at this time so this is the only\n\t * way to make this reliably (actually, this is no longer\n\t * the case, see above).  I tried to use indirect\n\t * register read/write but this upset some 5701 variants.\n\t */\n\tpci_read_config_dword(tp->pdev, PCI_COMMAND, &val);\n\n\tudelay(120);\n\n\tif (tg3_flag(tp, PCI_EXPRESS) && pci_is_pcie(tp->pdev)) {\n\t\tu16 val16;\n\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5750_A0) {\n\t\t\tint j;\n\t\t\tu32 cfg_val;\n\n\t\t\t/* Wait for link training to complete.  */\n\t\t\tfor (j = 0; j < 5000; j++)\n\t\t\t\tudelay(100);\n\n\t\t\tpci_read_config_dword(tp->pdev, 0xc4, &cfg_val);\n\t\t\tpci_write_config_dword(tp->pdev, 0xc4,\n\t\t\t\t\t       cfg_val | (1 << 15));\n\t\t}\n\n\t\t/* Clear the \"no snoop\" and \"relaxed ordering\" bits. */\n\t\tval16 = PCI_EXP_DEVCTL_RELAX_EN | PCI_EXP_DEVCTL_NOSNOOP_EN;\n\t\t/*\n\t\t * Older PCIe devices only support the 128 byte\n\t\t * MPS setting.  Enforce the restriction.\n\t\t */\n\t\tif (!tg3_flag(tp, CPMU_PRESENT))\n\t\t\tval16 |= PCI_EXP_DEVCTL_PAYLOAD;\n\t\tpcie_capability_clear_word(tp->pdev, PCI_EXP_DEVCTL, val16);\n\n\t\t/* Clear error status */\n\t\tpcie_capability_write_word(tp->pdev, PCI_EXP_DEVSTA,\n\t\t\t\t      PCI_EXP_DEVSTA_CED |\n\t\t\t\t      PCI_EXP_DEVSTA_NFED |\n\t\t\t\t      PCI_EXP_DEVSTA_FED |\n\t\t\t\t      PCI_EXP_DEVSTA_URD);\n\t}\n\n\ttg3_restore_pci_state(tp);\n\n\ttg3_flag_clear(tp, CHIP_RESETTING);\n\ttg3_flag_clear(tp, ERROR_PROCESSED);\n\n\tval = 0;\n\tif (tg3_flag(tp, 5780_CLASS))\n\t\tval = tr32(MEMARB_MODE);\n\ttw32(MEMARB_MODE, val | MEMARB_MODE_ENABLE);\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5750_A3) {\n\t\ttg3_stop_fw(tp);\n\t\ttw32(0x5000, 0x400);\n\t}\n\n\tif (tg3_flag(tp, IS_SSB_CORE)) {\n\t\t/*\n\t\t * BCM4785: In order to avoid repercussions from using\n\t\t * potentially defective internal ROM, stop the Rx RISC CPU,\n\t\t * which is not required.\n\t\t */\n\t\ttg3_stop_fw(tp);\n\t\ttg3_halt_cpu(tp, RX_CPU_BASE);\n\t}\n\n\ttw32(GRC_MODE, tp->grc_mode);\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A0) {\n\t\tval = tr32(0xc4);\n\n\t\ttw32(0xc4, val | (1 << 15));\n\t}\n\n\tif ((tp->nic_sram_data_cfg & NIC_SRAM_DATA_CFG_MINI_PCI) != 0 &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\ttp->pci_clock_ctrl |= CLOCK_CTRL_CLKRUN_OENABLE;\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A0)\n\t\t\ttp->pci_clock_ctrl |= CLOCK_CTRL_FORCE_CLKRUN;\n\t\ttw32(TG3PCI_CLOCK_CTRL, tp->pci_clock_ctrl);\n\t}\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\ttp->mac_mode = MAC_MODE_PORT_MODE_TBI;\n\t\tval = tp->mac_mode;\n\t} else if (tp->phy_flags & TG3_PHYFLG_MII_SERDES) {\n\t\ttp->mac_mode = MAC_MODE_PORT_MODE_GMII;\n\t\tval = tp->mac_mode;\n\t} else\n\t\tval = 0;\n\n\ttw32_f(MAC_MODE, val);\n\tudelay(40);\n\n\ttg3_ape_unlock(tp, TG3_APE_LOCK_GRC);\n\n\terr = tg3_poll_fw(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_mdio_start(tp);\n\n\tif (tg3_flag(tp, PCI_EXPRESS) &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A0 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5785 &&\n\t    !tg3_flag(tp, 57765_PLUS)) {\n\t\tval = tr32(0x7c00);\n\n\t\ttw32(0x7c00, val | (1 << 25));\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720) {\n\t\tval = tr32(TG3_CPMU_CLCK_ORIDE);\n\t\ttw32(TG3_CPMU_CLCK_ORIDE, val & ~CPMU_CLCK_ORIDE_MAC_ORIDE_EN);\n\t}\n\n\t/* Reprobe ASF enable state.  */\n\ttg3_flag_clear(tp, ENABLE_ASF);\n\ttg3_flag_clear(tp, ASF_NEW_HANDSHAKE);\n\ttg3_read_mem(tp, NIC_SRAM_DATA_SIG, &val);\n\tif (val == NIC_SRAM_DATA_SIG_MAGIC) {\n\t\tu32 nic_cfg;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG, &nic_cfg);\n\t\tif (nic_cfg & NIC_SRAM_DATA_CFG_ASF_ENABLE) {\n\t\t\ttg3_flag_set(tp, ENABLE_ASF);\n\t\t\ttp->last_event_jiffies = jiffies;\n\t\t\tif (tg3_flag(tp, 5750_PLUS))\n\t\t\t\ttg3_flag_set(tp, ASF_NEW_HANDSHAKE);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_get_nstats(struct tg3 *, struct rtnl_link_stats64 *);\nstatic void tg3_get_estats(struct tg3 *, struct tg3_ethtool_stats *);\n\n/* tp->lock is held. */\nstatic int tg3_halt(struct tg3 *tp, int kind, int silent)\n{\n\tint err;\n\n\ttg3_stop_fw(tp);\n\n\ttg3_write_sig_pre_reset(tp, kind);\n\n\ttg3_abort_hw(tp, silent);\n\terr = tg3_chip_reset(tp);\n\n\t__tg3_set_mac_addr(tp, 0);\n\n\ttg3_write_sig_legacy(tp, kind);\n\ttg3_write_sig_post_reset(tp, kind);\n\n\tif (tp->hw_stats) {\n\t\t/* Save the stats across chip resets... */\n\t\ttg3_get_nstats(tp, &tp->net_stats_prev);\n\t\ttg3_get_estats(tp, &tp->estats_prev);\n\n\t\t/* And make sure the next sample is new data */\n\t\tmemset(tp->hw_stats, 0, sizeof(struct tg3_hw_stats));\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int tg3_set_mac_addr(struct net_device *dev, void *p)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tstruct sockaddr *addr = p;\n\tint err = 0, skip_mac_1 = 0;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tif (tg3_flag(tp, ENABLE_ASF)) {\n\t\tu32 addr0_high, addr0_low, addr1_high, addr1_low;\n\n\t\taddr0_high = tr32(MAC_ADDR_0_HIGH);\n\t\taddr0_low = tr32(MAC_ADDR_0_LOW);\n\t\taddr1_high = tr32(MAC_ADDR_1_HIGH);\n\t\taddr1_low = tr32(MAC_ADDR_1_LOW);\n\n\t\t/* Skip MAC addr 1 if ASF is using it. */\n\t\tif ((addr0_high != addr1_high || addr0_low != addr1_low) &&\n\t\t    !(addr1_high == 0 && addr1_low == 0))\n\t\t\tskip_mac_1 = 1;\n\t}\n\tspin_lock_bh(&tp->lock);\n\t__tg3_set_mac_addr(tp, skip_mac_1);\n\tspin_unlock_bh(&tp->lock);\n\n\treturn err;\n}\n\n/* tp->lock is held. */\nstatic void tg3_set_bdinfo(struct tg3 *tp, u32 bdinfo_addr,\n\t\t\t   dma_addr_t mapping, u32 maxlen_flags,\n\t\t\t   u32 nic_addr)\n{\n\ttg3_write_mem(tp,\n\t\t      (bdinfo_addr + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_HIGH),\n\t\t      ((u64) mapping >> 32));\n\ttg3_write_mem(tp,\n\t\t      (bdinfo_addr + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_LOW),\n\t\t      ((u64) mapping & 0xffffffff));\n\ttg3_write_mem(tp,\n\t\t      (bdinfo_addr + TG3_BDINFO_MAXLEN_FLAGS),\n\t\t       maxlen_flags);\n\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\ttg3_write_mem(tp,\n\t\t\t      (bdinfo_addr + TG3_BDINFO_NIC_ADDR),\n\t\t\t      nic_addr);\n}\n\n\nstatic void tg3_coal_tx_init(struct tg3 *tp, struct ethtool_coalesce *ec)\n{\n\tint i = 0;\n\n\tif (!tg3_flag(tp, ENABLE_TSS)) {\n\t\ttw32(HOSTCC_TXCOL_TICKS, ec->tx_coalesce_usecs);\n\t\ttw32(HOSTCC_TXMAX_FRAMES, ec->tx_max_coalesced_frames);\n\t\ttw32(HOSTCC_TXCOAL_MAXF_INT, ec->tx_max_coalesced_frames_irq);\n\t} else {\n\t\ttw32(HOSTCC_TXCOL_TICKS, 0);\n\t\ttw32(HOSTCC_TXMAX_FRAMES, 0);\n\t\ttw32(HOSTCC_TXCOAL_MAXF_INT, 0);\n\n\t\tfor (; i < tp->txq_cnt; i++) {\n\t\t\tu32 reg;\n\n\t\t\treg = HOSTCC_TXCOL_TICKS_VEC1 + i * 0x18;\n\t\t\ttw32(reg, ec->tx_coalesce_usecs);\n\t\t\treg = HOSTCC_TXMAX_FRAMES_VEC1 + i * 0x18;\n\t\t\ttw32(reg, ec->tx_max_coalesced_frames);\n\t\t\treg = HOSTCC_TXCOAL_MAXF_INT_VEC1 + i * 0x18;\n\t\t\ttw32(reg, ec->tx_max_coalesced_frames_irq);\n\t\t}\n\t}\n\n\tfor (; i < tp->irq_max - 1; i++) {\n\t\ttw32(HOSTCC_TXCOL_TICKS_VEC1 + i * 0x18, 0);\n\t\ttw32(HOSTCC_TXMAX_FRAMES_VEC1 + i * 0x18, 0);\n\t\ttw32(HOSTCC_TXCOAL_MAXF_INT_VEC1 + i * 0x18, 0);\n\t}\n}\n\nstatic void tg3_coal_rx_init(struct tg3 *tp, struct ethtool_coalesce *ec)\n{\n\tint i = 0;\n\tu32 limit = tp->rxq_cnt;\n\n\tif (!tg3_flag(tp, ENABLE_RSS)) {\n\t\ttw32(HOSTCC_RXCOL_TICKS, ec->rx_coalesce_usecs);\n\t\ttw32(HOSTCC_RXMAX_FRAMES, ec->rx_max_coalesced_frames);\n\t\ttw32(HOSTCC_RXCOAL_MAXF_INT, ec->rx_max_coalesced_frames_irq);\n\t\tlimit--;\n\t} else {\n\t\ttw32(HOSTCC_RXCOL_TICKS, 0);\n\t\ttw32(HOSTCC_RXMAX_FRAMES, 0);\n\t\ttw32(HOSTCC_RXCOAL_MAXF_INT, 0);\n\t}\n\n\tfor (; i < limit; i++) {\n\t\tu32 reg;\n\n\t\treg = HOSTCC_RXCOL_TICKS_VEC1 + i * 0x18;\n\t\ttw32(reg, ec->rx_coalesce_usecs);\n\t\treg = HOSTCC_RXMAX_FRAMES_VEC1 + i * 0x18;\n\t\ttw32(reg, ec->rx_max_coalesced_frames);\n\t\treg = HOSTCC_RXCOAL_MAXF_INT_VEC1 + i * 0x18;\n\t\ttw32(reg, ec->rx_max_coalesced_frames_irq);\n\t}\n\n\tfor (; i < tp->irq_max - 1; i++) {\n\t\ttw32(HOSTCC_RXCOL_TICKS_VEC1 + i * 0x18, 0);\n\t\ttw32(HOSTCC_RXMAX_FRAMES_VEC1 + i * 0x18, 0);\n\t\ttw32(HOSTCC_RXCOAL_MAXF_INT_VEC1 + i * 0x18, 0);\n\t}\n}\n\nstatic void __tg3_set_coalesce(struct tg3 *tp, struct ethtool_coalesce *ec)\n{\n\ttg3_coal_tx_init(tp, ec);\n\ttg3_coal_rx_init(tp, ec);\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\tu32 val = ec->stats_block_coalesce_usecs;\n\n\t\ttw32(HOSTCC_RXCOAL_TICK_INT, ec->rx_coalesce_usecs_irq);\n\t\ttw32(HOSTCC_TXCOAL_TICK_INT, ec->tx_coalesce_usecs_irq);\n\n\t\tif (!tp->link_up)\n\t\t\tval = 0;\n\n\t\ttw32(HOSTCC_STAT_COAL_TICKS, val);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_rings_reset(struct tg3 *tp)\n{\n\tint i;\n\tu32 stblk, txrcb, rxrcb, limit;\n\tstruct tg3_napi *tnapi = &tp->napi[0];\n\n\t/* Disable all transmit rings but the first. */\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\tlimit = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE * 16;\n\telse if (tg3_flag(tp, 5717_PLUS))\n\t\tlimit = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE * 4;\n\telse if (tg3_flag(tp, 57765_CLASS) ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tlimit = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE * 2;\n\telse\n\t\tlimit = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE;\n\n\tfor (txrcb = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE;\n\t     txrcb < limit; txrcb += TG3_BDINFO_SIZE)\n\t\ttg3_write_mem(tp, txrcb + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t\t      BDINFO_FLAGS_DISABLED);\n\n\n\t/* Disable all receive return rings but the first. */\n\tif (tg3_flag(tp, 5717_PLUS))\n\t\tlimit = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE * 17;\n\telse if (!tg3_flag(tp, 5705_PLUS))\n\t\tlimit = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE * 16;\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5762 ||\n\t\t tg3_flag(tp, 57765_CLASS))\n\t\tlimit = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE * 4;\n\telse\n\t\tlimit = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE;\n\n\tfor (rxrcb = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE;\n\t     rxrcb < limit; rxrcb += TG3_BDINFO_SIZE)\n\t\ttg3_write_mem(tp, rxrcb + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t\t      BDINFO_FLAGS_DISABLED);\n\n\t/* Disable interrupts */\n\ttw32_mailbox_f(tp->napi[0].int_mbox, 1);\n\ttp->napi[0].chk_msi_cnt = 0;\n\ttp->napi[0].last_rx_cons = 0;\n\ttp->napi[0].last_tx_cons = 0;\n\n\t/* Zero mailbox registers. */\n\tif (tg3_flag(tp, SUPPORT_MSIX)) {\n\t\tfor (i = 1; i < tp->irq_max; i++) {\n\t\t\ttp->napi[i].tx_prod = 0;\n\t\t\ttp->napi[i].tx_cons = 0;\n\t\t\tif (tg3_flag(tp, ENABLE_TSS))\n\t\t\t\ttw32_mailbox(tp->napi[i].prodmbox, 0);\n\t\t\ttw32_rx_mbox(tp->napi[i].consmbox, 0);\n\t\t\ttw32_mailbox_f(tp->napi[i].int_mbox, 1);\n\t\t\ttp->napi[i].chk_msi_cnt = 0;\n\t\t\ttp->napi[i].last_rx_cons = 0;\n\t\t\ttp->napi[i].last_tx_cons = 0;\n\t\t}\n\t\tif (!tg3_flag(tp, ENABLE_TSS))\n\t\t\ttw32_mailbox(tp->napi[0].prodmbox, 0);\n\t} else {\n\t\ttp->napi[0].tx_prod = 0;\n\t\ttp->napi[0].tx_cons = 0;\n\t\ttw32_mailbox(tp->napi[0].prodmbox, 0);\n\t\ttw32_rx_mbox(tp->napi[0].consmbox, 0);\n\t}\n\n\t/* Make sure the NIC-based send BD rings are disabled. */\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\tu32 mbox = MAILBOX_SNDNIC_PROD_IDX_0 + TG3_64BIT_REG_LOW;\n\t\tfor (i = 0; i < 16; i++)\n\t\t\ttw32_tx_mbox(mbox + i * 8, 0);\n\t}\n\n\ttxrcb = NIC_SRAM_SEND_RCB;\n\trxrcb = NIC_SRAM_RCV_RET_RCB;\n\n\t/* Clear status block in ram. */\n\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\n\t/* Set status block DMA address */\n\ttw32(HOSTCC_STATUS_BLK_HOST_ADDR + TG3_64BIT_REG_HIGH,\n\t     ((u64) tnapi->status_mapping >> 32));\n\ttw32(HOSTCC_STATUS_BLK_HOST_ADDR + TG3_64BIT_REG_LOW,\n\t     ((u64) tnapi->status_mapping & 0xffffffff));\n\n\tif (tnapi->tx_ring) {\n\t\ttg3_set_bdinfo(tp, txrcb, tnapi->tx_desc_mapping,\n\t\t\t       (TG3_TX_RING_SIZE <<\n\t\t\t\tBDINFO_FLAGS_MAXLEN_SHIFT),\n\t\t\t       NIC_SRAM_TX_BUFFER_DESC);\n\t\ttxrcb += TG3_BDINFO_SIZE;\n\t}\n\n\tif (tnapi->rx_rcb) {\n\t\ttg3_set_bdinfo(tp, rxrcb, tnapi->rx_rcb_mapping,\n\t\t\t       (tp->rx_ret_ring_mask + 1) <<\n\t\t\t\tBDINFO_FLAGS_MAXLEN_SHIFT, 0);\n\t\trxrcb += TG3_BDINFO_SIZE;\n\t}\n\n\tstblk = HOSTCC_STATBLCK_RING1;\n\n\tfor (i = 1, tnapi++; i < tp->irq_cnt; i++, tnapi++) {\n\t\tu64 mapping = (u64)tnapi->status_mapping;\n\t\ttw32(stblk + TG3_64BIT_REG_HIGH, mapping >> 32);\n\t\ttw32(stblk + TG3_64BIT_REG_LOW, mapping & 0xffffffff);\n\n\t\t/* Clear status block in ram. */\n\t\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\n\t\tif (tnapi->tx_ring) {\n\t\t\ttg3_set_bdinfo(tp, txrcb, tnapi->tx_desc_mapping,\n\t\t\t\t       (TG3_TX_RING_SIZE <<\n\t\t\t\t\tBDINFO_FLAGS_MAXLEN_SHIFT),\n\t\t\t\t       NIC_SRAM_TX_BUFFER_DESC);\n\t\t\ttxrcb += TG3_BDINFO_SIZE;\n\t\t}\n\n\t\ttg3_set_bdinfo(tp, rxrcb, tnapi->rx_rcb_mapping,\n\t\t\t       ((tp->rx_ret_ring_mask + 1) <<\n\t\t\t\tBDINFO_FLAGS_MAXLEN_SHIFT), 0);\n\n\t\tstblk += 8;\n\t\trxrcb += TG3_BDINFO_SIZE;\n\t}\n}\n\nstatic void tg3_setup_rxbd_thresholds(struct tg3 *tp)\n{\n\tu32 val, bdcache_maxcnt, host_rep_thresh, nic_rep_thresh;\n\n\tif (!tg3_flag(tp, 5750_PLUS) ||\n\t    tg3_flag(tp, 5780_CLASS) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5750 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5752 ||\n\t    tg3_flag(tp, 57765_PLUS))\n\t\tbdcache_maxcnt = TG3_SRAM_RX_STD_BDCACHE_SIZE_5700;\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5787)\n\t\tbdcache_maxcnt = TG3_SRAM_RX_STD_BDCACHE_SIZE_5755;\n\telse\n\t\tbdcache_maxcnt = TG3_SRAM_RX_STD_BDCACHE_SIZE_5906;\n\n\tnic_rep_thresh = min(bdcache_maxcnt / 2, tp->rx_std_max_post);\n\thost_rep_thresh = max_t(u32, tp->rx_pending / 8, 1);\n\n\tval = min(nic_rep_thresh, host_rep_thresh);\n\ttw32(RCVBDI_STD_THRESH, val);\n\n\tif (tg3_flag(tp, 57765_PLUS))\n\t\ttw32(STD_REPLENISH_LWM, bdcache_maxcnt);\n\n\tif (!tg3_flag(tp, JUMBO_CAPABLE) || tg3_flag(tp, 5780_CLASS))\n\t\treturn;\n\n\tbdcache_maxcnt = TG3_SRAM_RX_JMB_BDCACHE_SIZE_5700;\n\n\thost_rep_thresh = max_t(u32, tp->rx_jumbo_pending / 8, 1);\n\n\tval = min(bdcache_maxcnt / 2, host_rep_thresh);\n\ttw32(RCVBDI_JUMBO_THRESH, val);\n\n\tif (tg3_flag(tp, 57765_PLUS))\n\t\ttw32(JMB_REPLENISH_LWM, bdcache_maxcnt);\n}\n\nstatic inline u32 calc_crc(unsigned char *buf, int len)\n{\n\tu32 reg;\n\tu32 tmp;\n\tint j, k;\n\n\treg = 0xffffffff;\n\n\tfor (j = 0; j < len; j++) {\n\t\treg ^= buf[j];\n\n\t\tfor (k = 0; k < 8; k++) {\n\t\t\ttmp = reg & 0x01;\n\n\t\t\treg >>= 1;\n\n\t\t\tif (tmp)\n\t\t\t\treg ^= 0xedb88320;\n\t\t}\n\t}\n\n\treturn ~reg;\n}\n\nstatic void tg3_set_multi(struct tg3 *tp, unsigned int accept_all)\n{\n\t/* accept or reject all multicast frames */\n\ttw32(MAC_HASH_REG_0, accept_all ? 0xffffffff : 0);\n\ttw32(MAC_HASH_REG_1, accept_all ? 0xffffffff : 0);\n\ttw32(MAC_HASH_REG_2, accept_all ? 0xffffffff : 0);\n\ttw32(MAC_HASH_REG_3, accept_all ? 0xffffffff : 0);\n}\n\nstatic void __tg3_set_rx_mode(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 rx_mode;\n\n\trx_mode = tp->rx_mode & ~(RX_MODE_PROMISC |\n\t\t\t\t  RX_MODE_KEEP_VLAN_TAG);\n\n#if !defined(CONFIG_VLAN_8021Q) && !defined(CONFIG_VLAN_8021Q_MODULE)\n\t/* When ASF is in use, we always keep the RX_MODE_KEEP_VLAN_TAG\n\t * flag clear.\n\t */\n\tif (!tg3_flag(tp, ENABLE_ASF))\n\t\trx_mode |= RX_MODE_KEEP_VLAN_TAG;\n#endif\n\n\tif (dev->flags & IFF_PROMISC) {\n\t\t/* Promiscuous mode. */\n\t\trx_mode |= RX_MODE_PROMISC;\n\t} else if (dev->flags & IFF_ALLMULTI) {\n\t\t/* Accept all multicast. */\n\t\ttg3_set_multi(tp, 1);\n\t} else if (netdev_mc_empty(dev)) {\n\t\t/* Reject all multicast. */\n\t\ttg3_set_multi(tp, 0);\n\t} else {\n\t\t/* Accept one or more multicast(s). */\n\t\tstruct netdev_hw_addr *ha;\n\t\tu32 mc_filter[4] = { 0, };\n\t\tu32 regidx;\n\t\tu32 bit;\n\t\tu32 crc;\n\n\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\tcrc = calc_crc(ha->addr, ETH_ALEN);\n\t\t\tbit = ~crc & 0x7f;\n\t\t\tregidx = (bit & 0x60) >> 5;\n\t\t\tbit &= 0x1f;\n\t\t\tmc_filter[regidx] |= (1 << bit);\n\t\t}\n\n\t\ttw32(MAC_HASH_REG_0, mc_filter[0]);\n\t\ttw32(MAC_HASH_REG_1, mc_filter[1]);\n\t\ttw32(MAC_HASH_REG_2, mc_filter[2]);\n\t\ttw32(MAC_HASH_REG_3, mc_filter[3]);\n\t}\n\n\tif (rx_mode != tp->rx_mode) {\n\t\ttp->rx_mode = rx_mode;\n\t\ttw32_f(MAC_RX_MODE, rx_mode);\n\t\tudelay(10);\n\t}\n}\n\nstatic void tg3_rss_init_dflt_indir_tbl(struct tg3 *tp, u32 qcnt)\n{\n\tint i;\n\n\tfor (i = 0; i < TG3_RSS_INDIR_TBL_SIZE; i++)\n\t\ttp->rss_ind_tbl[i] = ethtool_rxfh_indir_default(i, qcnt);\n}\n\nstatic void tg3_rss_check_indir_tbl(struct tg3 *tp)\n{\n\tint i;\n\n\tif (!tg3_flag(tp, SUPPORT_MSIX))\n\t\treturn;\n\n\tif (tp->rxq_cnt == 1) {\n\t\tmemset(&tp->rss_ind_tbl[0], 0, sizeof(tp->rss_ind_tbl));\n\t\treturn;\n\t}\n\n\t/* Validate table against current IRQ count */\n\tfor (i = 0; i < TG3_RSS_INDIR_TBL_SIZE; i++) {\n\t\tif (tp->rss_ind_tbl[i] >= tp->rxq_cnt)\n\t\t\tbreak;\n\t}\n\n\tif (i != TG3_RSS_INDIR_TBL_SIZE)\n\t\ttg3_rss_init_dflt_indir_tbl(tp, tp->rxq_cnt);\n}\n\nstatic void tg3_rss_write_indir_tbl(struct tg3 *tp)\n{\n\tint i = 0;\n\tu32 reg = MAC_RSS_INDIR_TBL_0;\n\n\twhile (i < TG3_RSS_INDIR_TBL_SIZE) {\n\t\tu32 val = tp->rss_ind_tbl[i];\n\t\ti++;\n\t\tfor (; i % 8; i++) {\n\t\t\tval <<= 4;\n\t\t\tval |= tp->rss_ind_tbl[i];\n\t\t}\n\t\ttw32(reg, val);\n\t\treg += 4;\n\t}\n}\n\n/* tp->lock is held. */\nstatic int tg3_reset_hw(struct tg3 *tp, int reset_phy)\n{\n\tu32 val, rdmac_mode;\n\tint i, err, limit;\n\tstruct tg3_rx_prodring_set *tpr = &tp->napi[0].prodring;\n\n\ttg3_disable_ints(tp);\n\n\ttg3_stop_fw(tp);\n\n\ttg3_write_sig_pre_reset(tp, RESET_KIND_INIT);\n\n\tif (tg3_flag(tp, INIT_COMPLETE))\n\t\ttg3_abort_hw(tp, 1);\n\n\t/* Enable MAC control of LPI */\n\tif (tp->phy_flags & TG3_PHYFLG_EEE_CAP) {\n\t\tval = TG3_CPMU_EEE_LNKIDL_PCIE_NL0 |\n\t\t      TG3_CPMU_EEE_LNKIDL_UART_IDL;\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_57765_A0)\n\t\t\tval |= TG3_CPMU_EEE_LNKIDL_APE_TX_MT;\n\n\t\ttw32_f(TG3_CPMU_EEE_LNKIDL_CTRL, val);\n\n\t\ttw32_f(TG3_CPMU_EEE_CTRL,\n\t\t       TG3_CPMU_EEE_CTRL_EXIT_20_1_US);\n\n\t\tval = TG3_CPMU_EEEMD_ERLY_L1_XIT_DET |\n\t\t      TG3_CPMU_EEEMD_LPI_IN_TX |\n\t\t      TG3_CPMU_EEEMD_LPI_IN_RX |\n\t\t      TG3_CPMU_EEEMD_EEE_ENABLE;\n\n\t\tif (tg3_asic_rev(tp) != ASIC_REV_5717)\n\t\t\tval |= TG3_CPMU_EEEMD_SND_IDX_DET_EN;\n\n\t\tif (tg3_flag(tp, ENABLE_APE))\n\t\t\tval |= TG3_CPMU_EEEMD_APE_TX_DET_EN;\n\n\t\ttw32_f(TG3_CPMU_EEE_MODE, val);\n\n\t\ttw32_f(TG3_CPMU_EEE_DBTMR1,\n\t\t       TG3_CPMU_DBTMR1_PCIEXIT_2047US |\n\t\t       TG3_CPMU_DBTMR1_LNKIDLE_2047US);\n\n\t\ttw32_f(TG3_CPMU_EEE_DBTMR2,\n\t\t       TG3_CPMU_DBTMR2_APE_TX_2047US |\n\t\t       TG3_CPMU_DBTMR2_TXIDXEQ_2047US);\n\t}\n\n\tif (reset_phy)\n\t\ttg3_phy_reset(tp);\n\n\terr = tg3_chip_reset(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_write_sig_legacy(tp, RESET_KIND_INIT);\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX) {\n\t\tval = tr32(TG3_CPMU_CTRL);\n\t\tval &= ~(CPMU_CTRL_LINK_AWARE_MODE | CPMU_CTRL_LINK_IDLE_MODE);\n\t\ttw32(TG3_CPMU_CTRL, val);\n\n\t\tval = tr32(TG3_CPMU_LSPD_10MB_CLK);\n\t\tval &= ~CPMU_LSPD_10MB_MACCLK_MASK;\n\t\tval |= CPMU_LSPD_10MB_MACCLK_6_25;\n\t\ttw32(TG3_CPMU_LSPD_10MB_CLK, val);\n\n\t\tval = tr32(TG3_CPMU_LNK_AWARE_PWRMD);\n\t\tval &= ~CPMU_LNK_AWARE_MACCLK_MASK;\n\t\tval |= CPMU_LNK_AWARE_MACCLK_6_25;\n\t\ttw32(TG3_CPMU_LNK_AWARE_PWRMD, val);\n\n\t\tval = tr32(TG3_CPMU_HST_ACC);\n\t\tval &= ~CPMU_HST_ACC_MACCLK_MASK;\n\t\tval |= CPMU_HST_ACC_MACCLK_6_25;\n\t\ttw32(TG3_CPMU_HST_ACC, val);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_57780) {\n\t\tval = tr32(PCIE_PWR_MGMT_THRESH) & ~PCIE_PWR_MGMT_L1_THRESH_MSK;\n\t\tval |= PCIE_PWR_MGMT_EXT_ASPM_TMR_EN |\n\t\t       PCIE_PWR_MGMT_L1_THRESH_4MS;\n\t\ttw32(PCIE_PWR_MGMT_THRESH, val);\n\n\t\tval = tr32(TG3_PCIE_EIDLE_DELAY) & ~TG3_PCIE_EIDLE_DELAY_MASK;\n\t\ttw32(TG3_PCIE_EIDLE_DELAY, val | TG3_PCIE_EIDLE_DELAY_13_CLKS);\n\n\t\ttw32(TG3_CORR_ERR_STAT, TG3_CORR_ERR_STAT_CLEAR);\n\n\t\tval = tr32(TG3_PCIE_LNKCTL) & ~TG3_PCIE_LNKCTL_L1_PLL_PD_EN;\n\t\ttw32(TG3_PCIE_LNKCTL, val | TG3_PCIE_LNKCTL_L1_PLL_PD_DIS);\n\t}\n\n\tif (tg3_flag(tp, L1PLLPD_EN)) {\n\t\tu32 grc_mode = tr32(GRC_MODE);\n\n\t\t/* Access the lower 1K of PL PCIE block registers. */\n\t\tval = grc_mode & ~GRC_MODE_PCIE_PORT_MASK;\n\t\ttw32(GRC_MODE, val | GRC_MODE_PCIE_PL_SEL);\n\n\t\tval = tr32(TG3_PCIE_TLDLPL_PORT + TG3_PCIE_PL_LO_PHYCTL1);\n\t\ttw32(TG3_PCIE_TLDLPL_PORT + TG3_PCIE_PL_LO_PHYCTL1,\n\t\t     val | TG3_PCIE_PL_LO_PHYCTL1_L1PLLPD_EN);\n\n\t\ttw32(GRC_MODE, grc_mode);\n\t}\n\n\tif (tg3_flag(tp, 57765_CLASS)) {\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_57765_A0) {\n\t\t\tu32 grc_mode = tr32(GRC_MODE);\n\n\t\t\t/* Access the lower 1K of PL PCIE block registers. */\n\t\t\tval = grc_mode & ~GRC_MODE_PCIE_PORT_MASK;\n\t\t\ttw32(GRC_MODE, val | GRC_MODE_PCIE_PL_SEL);\n\n\t\t\tval = tr32(TG3_PCIE_TLDLPL_PORT +\n\t\t\t\t   TG3_PCIE_PL_LO_PHYCTL5);\n\t\t\ttw32(TG3_PCIE_TLDLPL_PORT + TG3_PCIE_PL_LO_PHYCTL5,\n\t\t\t     val | TG3_PCIE_PL_LO_PHYCTL5_DIS_L2CLKREQ);\n\n\t\t\ttw32(GRC_MODE, grc_mode);\n\t\t}\n\n\t\tif (tg3_chip_rev(tp) != CHIPREV_57765_AX) {\n\t\t\tu32 grc_mode;\n\n\t\t\t/* Fix transmit hangs */\n\t\t\tval = tr32(TG3_CPMU_PADRNG_CTL);\n\t\t\tval |= TG3_CPMU_PADRNG_CTL_RDIV2;\n\t\t\ttw32(TG3_CPMU_PADRNG_CTL, val);\n\n\t\t\tgrc_mode = tr32(GRC_MODE);\n\n\t\t\t/* Access the lower 1K of DL PCIE block registers. */\n\t\t\tval = grc_mode & ~GRC_MODE_PCIE_PORT_MASK;\n\t\t\ttw32(GRC_MODE, val | GRC_MODE_PCIE_DL_SEL);\n\n\t\t\tval = tr32(TG3_PCIE_TLDLPL_PORT +\n\t\t\t\t   TG3_PCIE_DL_LO_FTSMAX);\n\t\t\tval &= ~TG3_PCIE_DL_LO_FTSMAX_MSK;\n\t\t\ttw32(TG3_PCIE_TLDLPL_PORT + TG3_PCIE_DL_LO_FTSMAX,\n\t\t\t     val | TG3_PCIE_DL_LO_FTSMAX_VAL);\n\n\t\t\ttw32(GRC_MODE, grc_mode);\n\t\t}\n\n\t\tval = tr32(TG3_CPMU_LSPD_10MB_CLK);\n\t\tval &= ~CPMU_LSPD_10MB_MACCLK_MASK;\n\t\tval |= CPMU_LSPD_10MB_MACCLK_6_25;\n\t\ttw32(TG3_CPMU_LSPD_10MB_CLK, val);\n\t}\n\n\t/* This works around an issue with Athlon chipsets on\n\t * B3 tigon3 silicon.  This bit has no effect on any\n\t * other revision.  But do not set this on PCI Express\n\t * chips and don't even touch the clocks if the CPMU is present.\n\t */\n\tif (!tg3_flag(tp, CPMU_PRESENT)) {\n\t\tif (!tg3_flag(tp, PCI_EXPRESS))\n\t\t\ttp->pci_clock_ctrl |= CLOCK_CTRL_DELAY_PCI_GRANT;\n\t\ttw32_f(TG3PCI_CLOCK_CTRL, tp->pci_clock_ctrl);\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5704_A0 &&\n\t    tg3_flag(tp, PCIX_MODE)) {\n\t\tval = tr32(TG3PCI_PCISTATE);\n\t\tval |= PCISTATE_RETRY_SAME_DMA;\n\t\ttw32(TG3PCI_PCISTATE, val);\n\t}\n\n\tif (tg3_flag(tp, ENABLE_APE)) {\n\t\t/* Allow reads and writes to the\n\t\t * APE register and memory space.\n\t\t */\n\t\tval = tr32(TG3PCI_PCISTATE);\n\t\tval |= PCISTATE_ALLOW_APE_CTLSPC_WR |\n\t\t       PCISTATE_ALLOW_APE_SHMEM_WR |\n\t\t       PCISTATE_ALLOW_APE_PSPACE_WR;\n\t\ttw32(TG3PCI_PCISTATE, val);\n\t}\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5704_BX) {\n\t\t/* Enable some hw fixes.  */\n\t\tval = tr32(TG3PCI_MSI_DATA);\n\t\tval |= (1 << 26) | (1 << 28) | (1 << 29);\n\t\ttw32(TG3PCI_MSI_DATA, val);\n\t}\n\n\t/* Descriptor ring init may make accesses to the\n\t * NIC SRAM area to setup the TX descriptors, so we\n\t * can only do this after the hardware has been\n\t * successfully reset.\n\t */\n\terr = tg3_init_rings(tp);\n\tif (err)\n\t\treturn err;\n\n\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\tval = tr32(TG3PCI_DMA_RW_CTRL) &\n\t\t      ~DMA_RWCTRL_DIS_CACHE_ALIGNMENT;\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_57765_A0)\n\t\t\tval &= ~DMA_RWCTRL_CRDRDR_RDMA_MRRS_MSK;\n\t\tif (!tg3_flag(tp, 57765_CLASS) &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5717 &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5762)\n\t\t\tval |= DMA_RWCTRL_TAGGED_STAT_WA;\n\t\ttw32(TG3PCI_DMA_RW_CTRL, val | tp->dma_rwctrl);\n\t} else if (tg3_asic_rev(tp) != ASIC_REV_5784 &&\n\t\t   tg3_asic_rev(tp) != ASIC_REV_5761) {\n\t\t/* This value is determined during the probe time DMA\n\t\t * engine test, tg3_test_dma.\n\t\t */\n\t\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\t}\n\n\ttp->grc_mode &= ~(GRC_MODE_HOST_SENDBDS |\n\t\t\t  GRC_MODE_4X_NIC_SEND_RINGS |\n\t\t\t  GRC_MODE_NO_TX_PHDR_CSUM |\n\t\t\t  GRC_MODE_NO_RX_PHDR_CSUM);\n\ttp->grc_mode |= GRC_MODE_HOST_SENDBDS;\n\n\t/* Pseudo-header checksum is done by hardware logic and not\n\t * the offload processers, so make the chip do the pseudo-\n\t * header checksums on receive.  For transmit it is more\n\t * convenient to do the pseudo-header checksum in software\n\t * as Linux does that on transmit for us in all cases.\n\t */\n\ttp->grc_mode |= GRC_MODE_NO_TX_PHDR_CSUM;\n\n\tval = GRC_MODE_IRQ_ON_MAC_ATTN | GRC_MODE_HOST_STACKUP;\n\tif (tp->rxptpctl)\n\t\ttw32(TG3_RX_PTP_CTL,\n\t\t     tp->rxptpctl | TG3_RX_PTP_CTL_HWTS_INTERLOCK);\n\n\tif (tg3_flag(tp, PTP_CAPABLE))\n\t\tval |= GRC_MODE_TIME_SYNC_ENABLE;\n\n\ttw32(GRC_MODE, tp->grc_mode | val);\n\n\t/* Setup the timer prescalar register.  Clock is always 66Mhz. */\n\tval = tr32(GRC_MISC_CFG);\n\tval &= ~0xff;\n\tval |= (65 << GRC_MISC_CFG_PRESCALAR_SHIFT);\n\ttw32(GRC_MISC_CFG, val);\n\n\t/* Initialize MBUF/DESC pool. */\n\tif (tg3_flag(tp, 5750_PLUS)) {\n\t\t/* Do nothing.  */\n\t} else if (tg3_asic_rev(tp) != ASIC_REV_5705) {\n\t\ttw32(BUFMGR_MB_POOL_ADDR, NIC_SRAM_MBUF_POOL_BASE);\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5704)\n\t\t\ttw32(BUFMGR_MB_POOL_SIZE, NIC_SRAM_MBUF_POOL_SIZE64);\n\t\telse\n\t\t\ttw32(BUFMGR_MB_POOL_SIZE, NIC_SRAM_MBUF_POOL_SIZE96);\n\t\ttw32(BUFMGR_DMA_DESC_POOL_ADDR, NIC_SRAM_DMA_DESC_POOL_BASE);\n\t\ttw32(BUFMGR_DMA_DESC_POOL_SIZE, NIC_SRAM_DMA_DESC_POOL_SIZE);\n\t} else if (tg3_flag(tp, TSO_CAPABLE)) {\n\t\tint fw_len;\n\n\t\tfw_len = tp->fw_len;\n\t\tfw_len = (fw_len + (0x80 - 1)) & ~(0x80 - 1);\n\t\ttw32(BUFMGR_MB_POOL_ADDR,\n\t\t     NIC_SRAM_MBUF_POOL_BASE5705 + fw_len);\n\t\ttw32(BUFMGR_MB_POOL_SIZE,\n\t\t     NIC_SRAM_MBUF_POOL_SIZE5705 - fw_len - 0xa00);\n\t}\n\n\tif (tp->dev->mtu <= ETH_DATA_LEN) {\n\t\ttw32(BUFMGR_MB_RDMA_LOW_WATER,\n\t\t     tp->bufmgr_config.mbuf_read_dma_low_water);\n\t\ttw32(BUFMGR_MB_MACRX_LOW_WATER,\n\t\t     tp->bufmgr_config.mbuf_mac_rx_low_water);\n\t\ttw32(BUFMGR_MB_HIGH_WATER,\n\t\t     tp->bufmgr_config.mbuf_high_water);\n\t} else {\n\t\ttw32(BUFMGR_MB_RDMA_LOW_WATER,\n\t\t     tp->bufmgr_config.mbuf_read_dma_low_water_jumbo);\n\t\ttw32(BUFMGR_MB_MACRX_LOW_WATER,\n\t\t     tp->bufmgr_config.mbuf_mac_rx_low_water_jumbo);\n\t\ttw32(BUFMGR_MB_HIGH_WATER,\n\t\t     tp->bufmgr_config.mbuf_high_water_jumbo);\n\t}\n\ttw32(BUFMGR_DMA_LOW_WATER,\n\t     tp->bufmgr_config.dma_low_water);\n\ttw32(BUFMGR_DMA_HIGH_WATER,\n\t     tp->bufmgr_config.dma_high_water);\n\n\tval = BUFMGR_MODE_ENABLE | BUFMGR_MODE_ATTN_ENABLE;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\tval |= BUFMGR_MODE_NO_TX_UNDERRUN;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5720_A0)\n\t\tval |= BUFMGR_MODE_MBLOW_ATTN_ENAB;\n\ttw32(BUFMGR_MODE, val);\n\tfor (i = 0; i < 2000; i++) {\n\t\tif (tr32(BUFMGR_MODE) & BUFMGR_MODE_ENABLE)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\tif (i >= 2000) {\n\t\tnetdev_err(tp->dev, \"%s cannot enable BUFMGR\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5906_A1)\n\t\ttw32(ISO_PKT_TX, (tr32(ISO_PKT_TX) & ~0x3) | 0x2);\n\n\ttg3_setup_rxbd_thresholds(tp);\n\n\t/* Initialize TG3_BDINFO's at:\n\t *  RCVDBDI_STD_BD:\tstandard eth size rx ring\n\t *  RCVDBDI_JUMBO_BD:\tjumbo frame rx ring\n\t *  RCVDBDI_MINI_BD:\tsmall frame rx ring (??? does not work)\n\t *\n\t * like so:\n\t *  TG3_BDINFO_HOST_ADDR:\thigh/low parts of DMA address of ring\n\t *  TG3_BDINFO_MAXLEN_FLAGS:\t(rx max buffer size << 16) |\n\t *                              ring attribute flags\n\t *  TG3_BDINFO_NIC_ADDR:\tlocation of descriptors in nic SRAM\n\t *\n\t * Standard receive ring @ NIC_SRAM_RX_BUFFER_DESC, 512 entries.\n\t * Jumbo receive ring @ NIC_SRAM_RX_JUMBO_BUFFER_DESC, 256 entries.\n\t *\n\t * The size of each ring is fixed in the firmware, but the location is\n\t * configurable.\n\t */\n\ttw32(RCVDBDI_STD_BD + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_HIGH,\n\t     ((u64) tpr->rx_std_mapping >> 32));\n\ttw32(RCVDBDI_STD_BD + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_LOW,\n\t     ((u64) tpr->rx_std_mapping & 0xffffffff));\n\tif (!tg3_flag(tp, 5717_PLUS))\n\t\ttw32(RCVDBDI_STD_BD + TG3_BDINFO_NIC_ADDR,\n\t\t     NIC_SRAM_RX_BUFFER_DESC);\n\n\t/* Disable the mini ring */\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\ttw32(RCVDBDI_MINI_BD + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t     BDINFO_FLAGS_DISABLED);\n\n\t/* Program the jumbo buffer descriptor ring control\n\t * blocks on those devices that have them.\n\t */\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0 ||\n\t    (tg3_flag(tp, JUMBO_CAPABLE) && !tg3_flag(tp, 5780_CLASS))) {\n\n\t\tif (tg3_flag(tp, JUMBO_RING_ENABLE)) {\n\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_HIGH,\n\t\t\t     ((u64) tpr->rx_jmb_mapping >> 32));\n\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_LOW,\n\t\t\t     ((u64) tpr->rx_jmb_mapping & 0xffffffff));\n\t\t\tval = TG3_RX_JMB_RING_SIZE(tp) <<\n\t\t\t      BDINFO_FLAGS_MAXLEN_SHIFT;\n\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t\t     val | BDINFO_FLAGS_USE_EXT_RECV);\n\t\t\tif (!tg3_flag(tp, USE_JUMBO_BDFLAG) ||\n\t\t\t    tg3_flag(tp, 57765_CLASS) ||\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_NIC_ADDR,\n\t\t\t\t     NIC_SRAM_RX_JUMBO_BUFFER_DESC);\n\t\t} else {\n\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t\t     BDINFO_FLAGS_DISABLED);\n\t\t}\n\n\t\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\t\tval = TG3_RX_STD_RING_SIZE(tp);\n\t\t\tval <<= BDINFO_FLAGS_MAXLEN_SHIFT;\n\t\t\tval |= (TG3_RX_STD_DMA_SZ << 2);\n\t\t} else\n\t\t\tval = TG3_RX_STD_DMA_SZ << BDINFO_FLAGS_MAXLEN_SHIFT;\n\t} else\n\t\tval = TG3_RX_STD_MAX_SIZE_5700 << BDINFO_FLAGS_MAXLEN_SHIFT;\n\n\ttw32(RCVDBDI_STD_BD + TG3_BDINFO_MAXLEN_FLAGS, val);\n\n\ttpr->rx_std_prod_idx = tp->rx_pending;\n\ttw32_rx_mbox(TG3_RX_STD_PROD_IDX_REG, tpr->rx_std_prod_idx);\n\n\ttpr->rx_jmb_prod_idx =\n\t\ttg3_flag(tp, JUMBO_RING_ENABLE) ? tp->rx_jumbo_pending : 0;\n\ttw32_rx_mbox(TG3_RX_JMB_PROD_IDX_REG, tpr->rx_jmb_prod_idx);\n\n\ttg3_rings_reset(tp);\n\n\t/* Initialize MAC address and backoff seed. */\n\t__tg3_set_mac_addr(tp, 0);\n\n\t/* MTU + ethernet header + FCS + optional VLAN tag */\n\ttw32(MAC_RX_MTU_SIZE,\n\t     tp->dev->mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);\n\n\t/* The slot time is changed by tg3_setup_phy if we\n\t * run at gigabit with half duplex.\n\t */\n\tval = (2 << TX_LENGTHS_IPG_CRS_SHIFT) |\n\t      (6 << TX_LENGTHS_IPG_SHIFT) |\n\t      (32 << TX_LENGTHS_SLOT_TIME_SHIFT);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tval |= tr32(MAC_TX_LENGTHS) &\n\t\t       (TX_LENGTHS_JMB_FRM_LEN_MSK |\n\t\t\tTX_LENGTHS_CNT_DWN_VAL_MSK);\n\n\ttw32(MAC_TX_LENGTHS, val);\n\n\t/* Receive rules. */\n\ttw32(MAC_RCV_RULE_CFG, RCV_RULE_CFG_DEFAULT_CLASS);\n\ttw32(RCVLPC_CONFIG, 0x0181);\n\n\t/* Calculate RDMAC_MODE setting early, we need it to determine\n\t * the RCVLPC_STATE_ENABLE mask.\n\t */\n\trdmac_mode = (RDMAC_MODE_ENABLE | RDMAC_MODE_TGTABORT_ENAB |\n\t\t      RDMAC_MODE_MSTABORT_ENAB | RDMAC_MODE_PARITYERR_ENAB |\n\t\t      RDMAC_MODE_ADDROFLOW_ENAB | RDMAC_MODE_FIFOOFLOW_ENAB |\n\t\t      RDMAC_MODE_FIFOURUN_ENAB | RDMAC_MODE_FIFOOREAD_ENAB |\n\t\t      RDMAC_MODE_LNGREAD_ENAB);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717)\n\t\trdmac_mode |= RDMAC_MODE_MULT_DMA_RD_DIS;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780)\n\t\trdmac_mode |= RDMAC_MODE_BD_SBD_CRPT_ENAB |\n\t\t\t      RDMAC_MODE_MBUF_RBD_CRPT_ENAB |\n\t\t\t      RDMAC_MODE_MBUF_SBD_CRPT_ENAB;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A0) {\n\t\tif (tg3_flag(tp, TSO_CAPABLE) &&\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\t\trdmac_mode |= RDMAC_MODE_FIFO_SIZE_128;\n\t\t} else if (!(tr32(TG3PCI_PCISTATE) & PCISTATE_BUS_SPEED_HIGH) &&\n\t\t\t   !tg3_flag(tp, IS_5788)) {\n\t\t\trdmac_mode |= RDMAC_MODE_FIFO_LONG_BURST;\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, PCI_EXPRESS))\n\t\trdmac_mode |= RDMAC_MODE_FIFO_LONG_BURST;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_57766) {\n\t\ttp->dma_limit = 0;\n\t\tif (tp->dev->mtu <= ETH_DATA_LEN) {\n\t\t\trdmac_mode |= RDMAC_MODE_JMB_2K_MMRR;\n\t\t\ttp->dma_limit = TG3_TX_BD_DMA_MAX_2K;\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, HW_TSO_1) ||\n\t    tg3_flag(tp, HW_TSO_2) ||\n\t    tg3_flag(tp, HW_TSO_3))\n\t\trdmac_mode |= RDMAC_MODE_IPV4_LSO_EN;\n\n\tif (tg3_flag(tp, 57765_PLUS) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780)\n\t\trdmac_mode |= RDMAC_MODE_IPV6_LSO_EN;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\trdmac_mode |= tr32(RDMAC_MODE) & RDMAC_MODE_H2BNC_VLAN_DET;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780 ||\n\t    tg3_flag(tp, 57765_PLUS)) {\n\t\tu32 tgtreg;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\t\ttgtreg = TG3_RDMA_RSRVCTRL_REG2;\n\t\telse\n\t\t\ttgtreg = TG3_RDMA_RSRVCTRL_REG;\n\n\t\tval = tr32(tgtreg);\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\t\tval &= ~(TG3_RDMA_RSRVCTRL_TXMRGN_MASK |\n\t\t\t\t TG3_RDMA_RSRVCTRL_FIFO_LWM_MASK |\n\t\t\t\t TG3_RDMA_RSRVCTRL_FIFO_HWM_MASK);\n\t\t\tval |= TG3_RDMA_RSRVCTRL_TXMRGN_320B |\n\t\t\t       TG3_RDMA_RSRVCTRL_FIFO_LWM_1_5K |\n\t\t\t       TG3_RDMA_RSRVCTRL_FIFO_HWM_1_5K;\n\t\t}\n\t\ttw32(tgtreg, val | TG3_RDMA_RSRVCTRL_FIFO_OFLW_FIX);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\tu32 tgtreg;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\t\ttgtreg = TG3_LSO_RD_DMA_CRPTEN_CTRL2;\n\t\telse\n\t\t\ttgtreg = TG3_LSO_RD_DMA_CRPTEN_CTRL;\n\n\t\tval = tr32(tgtreg);\n\t\ttw32(tgtreg, val |\n\t\t     TG3_LSO_RD_DMA_CRPTEN_CTRL_BLEN_BD_4K |\n\t\t     TG3_LSO_RD_DMA_CRPTEN_CTRL_BLEN_LSO_4K);\n\t}\n\n\t/* Receive/send statistics. */\n\tif (tg3_flag(tp, 5750_PLUS)) {\n\t\tval = tr32(RCVLPC_STATS_ENABLE);\n\t\tval &= ~RCVLPC_STATSENAB_DACK_FIX;\n\t\ttw32(RCVLPC_STATS_ENABLE, val);\n\t} else if ((rdmac_mode & RDMAC_MODE_FIFO_SIZE_128) &&\n\t\t   tg3_flag(tp, TSO_CAPABLE)) {\n\t\tval = tr32(RCVLPC_STATS_ENABLE);\n\t\tval &= ~RCVLPC_STATSENAB_LNGBRST_RFIX;\n\t\ttw32(RCVLPC_STATS_ENABLE, val);\n\t} else {\n\t\ttw32(RCVLPC_STATS_ENABLE, 0xffffff);\n\t}\n\ttw32(RCVLPC_STATSCTRL, RCVLPC_STATSCTRL_ENABLE);\n\ttw32(SNDDATAI_STATSENAB, 0xffffff);\n\ttw32(SNDDATAI_STATSCTRL,\n\t     (SNDDATAI_SCTRL_ENABLE |\n\t      SNDDATAI_SCTRL_FASTUPD));\n\n\t/* Setup host coalescing engine. */\n\ttw32(HOSTCC_MODE, 0);\n\tfor (i = 0; i < 2000; i++) {\n\t\tif (!(tr32(HOSTCC_MODE) & HOSTCC_MODE_ENABLE))\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\t__tg3_set_coalesce(tp, &tp->coal);\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\t/* Status/statistics block address.  See tg3_timer,\n\t\t * the tg3_periodic_fetch_stats call there, and\n\t\t * tg3_get_stats to see how this works for 5705/5750 chips.\n\t\t */\n\t\ttw32(HOSTCC_STATS_BLK_HOST_ADDR + TG3_64BIT_REG_HIGH,\n\t\t     ((u64) tp->stats_mapping >> 32));\n\t\ttw32(HOSTCC_STATS_BLK_HOST_ADDR + TG3_64BIT_REG_LOW,\n\t\t     ((u64) tp->stats_mapping & 0xffffffff));\n\t\ttw32(HOSTCC_STATS_BLK_NIC_ADDR, NIC_SRAM_STATS_BLK);\n\n\t\ttw32(HOSTCC_STATUS_BLK_NIC_ADDR, NIC_SRAM_STATUS_BLK);\n\n\t\t/* Clear statistics and status block memory areas */\n\t\tfor (i = NIC_SRAM_STATS_BLK;\n\t\t     i < NIC_SRAM_STATUS_BLK + TG3_HW_STATUS_SIZE;\n\t\t     i += sizeof(u32)) {\n\t\t\ttg3_write_mem(tp, i, 0);\n\t\t\tudelay(40);\n\t\t}\n\t}\n\n\ttw32(HOSTCC_MODE, HOSTCC_MODE_ENABLE | tp->coalesce_mode);\n\n\ttw32(RCVCC_MODE, RCVCC_MODE_ENABLE | RCVCC_MODE_ATTN_ENABLE);\n\ttw32(RCVLPC_MODE, RCVLPC_MODE_ENABLE);\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\ttw32(RCVLSC_MODE, RCVLSC_MODE_ENABLE | RCVLSC_MODE_ATTN_ENABLE);\n\n\tif (tp->phy_flags & TG3_PHYFLG_MII_SERDES) {\n\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t\t/* reset to prevent losing 1st rx packet intermittently */\n\t\ttw32_f(MAC_RX_MODE, RX_MODE_RESET);\n\t\tudelay(10);\n\t}\n\n\ttp->mac_mode |= MAC_MODE_TXSTAT_ENABLE | MAC_MODE_RXSTAT_ENABLE |\n\t\t\tMAC_MODE_TDE_ENABLE | MAC_MODE_RDE_ENABLE |\n\t\t\tMAC_MODE_FHDE_ENABLE;\n\tif (tg3_flag(tp, ENABLE_APE))\n\t\ttp->mac_mode |= MAC_MODE_APE_TX_EN | MAC_MODE_APE_RX_EN;\n\tif (!tg3_flag(tp, 5705_PLUS) &&\n\t    !(tp->phy_flags & TG3_PHYFLG_PHY_SERDES) &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5700)\n\t\ttp->mac_mode |= MAC_MODE_LINK_POLARITY;\n\ttw32_f(MAC_MODE, tp->mac_mode | MAC_MODE_RXSTAT_CLEAR | MAC_MODE_TXSTAT_CLEAR);\n\tudelay(40);\n\n\t/* tp->grc_local_ctrl is partially set up during tg3_get_invariants().\n\t * If TG3_FLAG_IS_NIC is zero, we should read the\n\t * register to preserve the GPIO settings for LOMs. The GPIOs,\n\t * whether used as inputs or outputs, are set by boot code after\n\t * reset.\n\t */\n\tif (!tg3_flag(tp, IS_NIC)) {\n\t\tu32 gpio_mask;\n\n\t\tgpio_mask = GRC_LCLCTRL_GPIO_OE0 | GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t    GRC_LCLCTRL_GPIO_OE2 | GRC_LCLCTRL_GPIO_OUTPUT0 |\n\t\t\t    GRC_LCLCTRL_GPIO_OUTPUT1 | GRC_LCLCTRL_GPIO_OUTPUT2;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5752)\n\t\t\tgpio_mask |= GRC_LCLCTRL_GPIO_OE3 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT3;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5755)\n\t\t\tgpio_mask |= GRC_LCLCTRL_GPIO_UART_SEL;\n\n\t\ttp->grc_local_ctrl &= ~gpio_mask;\n\t\ttp->grc_local_ctrl |= tr32(GRC_LOCAL_CTRL) & gpio_mask;\n\n\t\t/* GPIO1 must be driven high for eeprom write protect */\n\t\tif (tg3_flag(tp, EEPROM_WRITE_PROT))\n\t\t\ttp->grc_local_ctrl |= (GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t\t\t       GRC_LCLCTRL_GPIO_OUTPUT1);\n\t}\n\ttw32_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl);\n\tudelay(100);\n\n\tif (tg3_flag(tp, USING_MSIX)) {\n\t\tval = tr32(MSGINT_MODE);\n\t\tval |= MSGINT_MODE_ENABLE;\n\t\tif (tp->irq_cnt > 1)\n\t\t\tval |= MSGINT_MODE_MULTIVEC_EN;\n\t\tif (!tg3_flag(tp, 1SHOT_MSI))\n\t\t\tval |= MSGINT_MODE_ONE_SHOT_DISABLE;\n\t\ttw32(MSGINT_MODE, val);\n\t}\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\ttw32_f(DMAC_MODE, DMAC_MODE_ENABLE);\n\t\tudelay(40);\n\t}\n\n\tval = (WDMAC_MODE_ENABLE | WDMAC_MODE_TGTABORT_ENAB |\n\t       WDMAC_MODE_MSTABORT_ENAB | WDMAC_MODE_PARITYERR_ENAB |\n\t       WDMAC_MODE_ADDROFLOW_ENAB | WDMAC_MODE_FIFOOFLOW_ENAB |\n\t       WDMAC_MODE_FIFOURUN_ENAB | WDMAC_MODE_FIFOOREAD_ENAB |\n\t       WDMAC_MODE_LNGREAD_ENAB);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A0) {\n\t\tif (tg3_flag(tp, TSO_CAPABLE) &&\n\t\t    (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A1 ||\n\t\t     tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A2)) {\n\t\t\t/* nothing */\n\t\t} else if (!(tr32(TG3PCI_PCISTATE) & PCISTATE_BUS_SPEED_HIGH) &&\n\t\t\t   !tg3_flag(tp, IS_5788)) {\n\t\t\tval |= WDMAC_MODE_RX_ACCEL;\n\t\t}\n\t}\n\n\t/* Enable host coalescing bug fix */\n\tif (tg3_flag(tp, 5755_PLUS))\n\t\tval |= WDMAC_MODE_STATUS_TAG_FIX;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\tval |= WDMAC_MODE_BURST_ALL_DATA;\n\n\ttw32_f(WDMAC_MODE, val);\n\tudelay(40);\n\n\tif (tg3_flag(tp, PCIX_MODE)) {\n\t\tu16 pcix_cmd;\n\n\t\tpci_read_config_word(tp->pdev, tp->pcix_cap + PCI_X_CMD,\n\t\t\t\t     &pcix_cmd);\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5703) {\n\t\t\tpcix_cmd &= ~PCI_X_CMD_MAX_READ;\n\t\t\tpcix_cmd |= PCI_X_CMD_READ_2K;\n\t\t} else if (tg3_asic_rev(tp) == ASIC_REV_5704) {\n\t\t\tpcix_cmd &= ~(PCI_X_CMD_MAX_SPLIT | PCI_X_CMD_MAX_READ);\n\t\t\tpcix_cmd |= PCI_X_CMD_READ_2K;\n\t\t}\n\t\tpci_write_config_word(tp->pdev, tp->pcix_cap + PCI_X_CMD,\n\t\t\t\t      pcix_cmd);\n\t}\n\n\ttw32_f(RDMAC_MODE, rdmac_mode);\n\tudelay(40);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719) {\n\t\tfor (i = 0; i < TG3_NUM_RDMA_CHANNELS; i++) {\n\t\t\tif (tr32(TG3_RDMA_LENGTH + (i << 2)) > TG3_MAX_MTU(tp))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i < TG3_NUM_RDMA_CHANNELS) {\n\t\t\tval = tr32(TG3_LSO_RD_DMA_CRPTEN_CTRL);\n\t\t\tval |= TG3_LSO_RD_DMA_TX_LENGTH_WA;\n\t\t\ttw32(TG3_LSO_RD_DMA_CRPTEN_CTRL, val);\n\t\t\ttg3_flag_set(tp, 5719_RDMA_BUG);\n\t\t}\n\t}\n\n\ttw32(RCVDCC_MODE, RCVDCC_MODE_ENABLE | RCVDCC_MODE_ATTN_ENABLE);\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\ttw32(MBFREE_MODE, MBFREE_MODE_ENABLE);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\ttw32(SNDDATAC_MODE,\n\t\t     SNDDATAC_MODE_ENABLE | SNDDATAC_MODE_CDELAY);\n\telse\n\t\ttw32(SNDDATAC_MODE, SNDDATAC_MODE_ENABLE);\n\n\ttw32(SNDBDC_MODE, SNDBDC_MODE_ENABLE | SNDBDC_MODE_ATTN_ENABLE);\n\ttw32(RCVBDI_MODE, RCVBDI_MODE_ENABLE | RCVBDI_MODE_RCB_ATTN_ENAB);\n\tval = RCVDBDI_MODE_ENABLE | RCVDBDI_MODE_INV_RING_SZ;\n\tif (tg3_flag(tp, LRG_PROD_RING_CAP))\n\t\tval |= RCVDBDI_MODE_LRG_RING_SZ;\n\ttw32(RCVDBDI_MODE, val);\n\ttw32(SNDDATAI_MODE, SNDDATAI_MODE_ENABLE);\n\tif (tg3_flag(tp, HW_TSO_1) ||\n\t    tg3_flag(tp, HW_TSO_2) ||\n\t    tg3_flag(tp, HW_TSO_3))\n\t\ttw32(SNDDATAI_MODE, SNDDATAI_MODE_ENABLE | 0x8);\n\tval = SNDBDI_MODE_ENABLE | SNDBDI_MODE_ATTN_ENABLE;\n\tif (tg3_flag(tp, ENABLE_TSS))\n\t\tval |= SNDBDI_MODE_MULTI_TXQ_EN;\n\ttw32(SNDBDI_MODE, val);\n\ttw32(SNDBDS_MODE, SNDBDS_MODE_ENABLE | SNDBDS_MODE_ATTN_ENABLE);\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0) {\n\t\terr = tg3_load_5701_a0_firmware_fix(tp);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (tg3_flag(tp, TSO_CAPABLE)) {\n\t\terr = tg3_load_tso_firmware(tp);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\ttp->tx_mode = TX_MODE_ENABLE;\n\n\tif (tg3_flag(tp, 5755_PLUS) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\ttp->tx_mode |= TX_MODE_MBUF_LOCKUP_FIX;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\tval = TX_MODE_JMB_FRM_LEN | TX_MODE_CNT_DN_MODE;\n\t\ttp->tx_mode &= ~val;\n\t\ttp->tx_mode |= tr32(MAC_TX_MODE) & val;\n\t}\n\n\ttw32_f(MAC_TX_MODE, tp->tx_mode);\n\tudelay(100);\n\n\tif (tg3_flag(tp, ENABLE_RSS)) {\n\t\ttg3_rss_write_indir_tbl(tp);\n\n\t\t/* Setup the \"secret\" hash key. */\n\t\ttw32(MAC_RSS_HASH_KEY_0, 0x5f865437);\n\t\ttw32(MAC_RSS_HASH_KEY_1, 0xe4ac62cc);\n\t\ttw32(MAC_RSS_HASH_KEY_2, 0x50103a45);\n\t\ttw32(MAC_RSS_HASH_KEY_3, 0x36621985);\n\t\ttw32(MAC_RSS_HASH_KEY_4, 0xbf14c0e8);\n\t\ttw32(MAC_RSS_HASH_KEY_5, 0x1bc27a1e);\n\t\ttw32(MAC_RSS_HASH_KEY_6, 0x84f4b556);\n\t\ttw32(MAC_RSS_HASH_KEY_7, 0x094ea6fe);\n\t\ttw32(MAC_RSS_HASH_KEY_8, 0x7dda01e7);\n\t\ttw32(MAC_RSS_HASH_KEY_9, 0xc04d7481);\n\t}\n\n\ttp->rx_mode = RX_MODE_ENABLE;\n\tif (tg3_flag(tp, 5755_PLUS))\n\t\ttp->rx_mode |= RX_MODE_IPV6_CSUM_ENABLE;\n\n\tif (tg3_flag(tp, ENABLE_RSS))\n\t\ttp->rx_mode |= RX_MODE_RSS_ENABLE |\n\t\t\t       RX_MODE_RSS_ITBL_HASH_BITS_7 |\n\t\t\t       RX_MODE_RSS_IPV6_HASH_EN |\n\t\t\t       RX_MODE_RSS_TCP_IPV6_HASH_EN |\n\t\t\t       RX_MODE_RSS_IPV4_HASH_EN |\n\t\t\t       RX_MODE_RSS_TCP_IPV4_HASH_EN;\n\n\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\tudelay(10);\n\n\ttw32(MAC_LED_CTRL, tp->led_ctrl);\n\n\ttw32(MAC_MI_STAT, MAC_MI_STAT_LNKSTAT_ATTN_ENAB);\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\ttw32_f(MAC_RX_MODE, RX_MODE_RESET);\n\t\tudelay(10);\n\t}\n\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\tudelay(10);\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\tif ((tg3_asic_rev(tp) == ASIC_REV_5704) &&\n\t\t    !(tp->phy_flags & TG3_PHYFLG_SERDES_PREEMPHASIS)) {\n\t\t\t/* Set drive transmission level to 1.2V  */\n\t\t\t/* only if the signal pre-emphasis bit is not set  */\n\t\t\tval = tr32(MAC_SERDES_CFG);\n\t\t\tval &= 0xfffff000;\n\t\t\tval |= 0x880;\n\t\t\ttw32(MAC_SERDES_CFG, val);\n\t\t}\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5703_A1)\n\t\t\ttw32(MAC_SERDES_CFG, 0x616000);\n\t}\n\n\t/* Prevent chip from dropping frames when flow control\n\t * is enabled.\n\t */\n\tif (tg3_flag(tp, 57765_CLASS))\n\t\tval = 1;\n\telse\n\t\tval = 2;\n\ttw32_f(MAC_LOW_WMARK_MAX_RX_FRAME, val);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5704 &&\n\t    (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)) {\n\t\t/* Use hardware link auto-negotiation */\n\t\ttg3_flag_set(tp, HW_AUTONEG);\n\t}\n\n\tif ((tp->phy_flags & TG3_PHYFLG_MII_SERDES) &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\tu32 tmp;\n\n\t\ttmp = tr32(SERDES_RX_CTRL);\n\t\ttw32(SERDES_RX_CTRL, tmp | SERDES_RX_SIG_DETECT);\n\t\ttp->grc_local_ctrl &= ~GRC_LCLCTRL_USE_EXT_SIG_DETECT;\n\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_USE_SIG_DETECT;\n\t\ttw32(GRC_LOCAL_CTRL, tp->grc_local_ctrl);\n\t}\n\n\tif (!tg3_flag(tp, USE_PHYLIB)) {\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_IS_LOW_POWER;\n\n\t\terr = tg3_setup_phy(tp, 0);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_PHY_SERDES) &&\n\t\t    !(tp->phy_flags & TG3_PHYFLG_IS_FET)) {\n\t\t\tu32 tmp;\n\n\t\t\t/* Clear CRC stats. */\n\t\t\tif (!tg3_readphy(tp, MII_TG3_TEST1, &tmp)) {\n\t\t\t\ttg3_writephy(tp, MII_TG3_TEST1,\n\t\t\t\t\t     tmp | MII_TG3_TEST1_CRC_EN);\n\t\t\t\ttg3_readphy(tp, MII_TG3_RXR_COUNTERS, &tmp);\n\t\t\t}\n\t\t}\n\t}\n\n\t__tg3_set_rx_mode(tp->dev);\n\n\t/* Initialize receive rules. */\n\ttw32(MAC_RCV_RULE_0,  0xc2000000 & RCV_RULE_DISABLE_MASK);\n\ttw32(MAC_RCV_VALUE_0, 0xffffffff & RCV_RULE_DISABLE_MASK);\n\ttw32(MAC_RCV_RULE_1,  0x86000004 & RCV_RULE_DISABLE_MASK);\n\ttw32(MAC_RCV_VALUE_1, 0xffffffff & RCV_RULE_DISABLE_MASK);\n\n\tif (tg3_flag(tp, 5705_PLUS) && !tg3_flag(tp, 5780_CLASS))\n\t\tlimit = 8;\n\telse\n\t\tlimit = 16;\n\tif (tg3_flag(tp, ENABLE_ASF))\n\t\tlimit -= 4;\n\tswitch (limit) {\n\tcase 16:\n\t\ttw32(MAC_RCV_RULE_15,  0); tw32(MAC_RCV_VALUE_15,  0);\n\tcase 15:\n\t\ttw32(MAC_RCV_RULE_14,  0); tw32(MAC_RCV_VALUE_14,  0);\n\tcase 14:\n\t\ttw32(MAC_RCV_RULE_13,  0); tw32(MAC_RCV_VALUE_13,  0);\n\tcase 13:\n\t\ttw32(MAC_RCV_RULE_12,  0); tw32(MAC_RCV_VALUE_12,  0);\n\tcase 12:\n\t\ttw32(MAC_RCV_RULE_11,  0); tw32(MAC_RCV_VALUE_11,  0);\n\tcase 11:\n\t\ttw32(MAC_RCV_RULE_10,  0); tw32(MAC_RCV_VALUE_10,  0);\n\tcase 10:\n\t\ttw32(MAC_RCV_RULE_9,  0); tw32(MAC_RCV_VALUE_9,  0);\n\tcase 9:\n\t\ttw32(MAC_RCV_RULE_8,  0); tw32(MAC_RCV_VALUE_8,  0);\n\tcase 8:\n\t\ttw32(MAC_RCV_RULE_7,  0); tw32(MAC_RCV_VALUE_7,  0);\n\tcase 7:\n\t\ttw32(MAC_RCV_RULE_6,  0); tw32(MAC_RCV_VALUE_6,  0);\n\tcase 6:\n\t\ttw32(MAC_RCV_RULE_5,  0); tw32(MAC_RCV_VALUE_5,  0);\n\tcase 5:\n\t\ttw32(MAC_RCV_RULE_4,  0); tw32(MAC_RCV_VALUE_4,  0);\n\tcase 4:\n\t\t/* tw32(MAC_RCV_RULE_3,  0); tw32(MAC_RCV_VALUE_3,  0); */\n\tcase 3:\n\t\t/* tw32(MAC_RCV_RULE_2,  0); tw32(MAC_RCV_VALUE_2,  0); */\n\tcase 2:\n\tcase 1:\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (tg3_flag(tp, ENABLE_APE))\n\t\t/* Write our heartbeat update interval to APE. */\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_HEARTBEAT_INT_MS,\n\t\t\t\tAPE_HOST_HEARTBEAT_INT_DISABLE);\n\n\ttg3_write_sig_post_reset(tp, RESET_KIND_INIT);\n\n\treturn 0;\n}\n\n/* Called at device open time to get the chip ready for\n * packet processing.  Invoked with tp->lock held.\n */\nstatic int tg3_init_hw(struct tg3 *tp, int reset_phy)\n{\n\ttg3_switch_clocks(tp);\n\n\ttw32(TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\n\treturn tg3_reset_hw(tp, reset_phy);\n}\n\nstatic void tg3_sd_scan_scratchpad(struct tg3 *tp, struct tg3_ocir *ocir)\n{\n\tint i;\n\n\tfor (i = 0; i < TG3_SD_NUM_RECS; i++, ocir++) {\n\t\tu32 off = i * TG3_OCIR_LEN, len = TG3_OCIR_LEN;\n\n\t\ttg3_ape_scratchpad_read(tp, (u32 *) ocir, off, len);\n\t\toff += len;\n\n\t\tif (ocir->signature != TG3_OCIR_SIG_MAGIC ||\n\t\t    !(ocir->version_flags & TG3_OCIR_FLAG_ACTIVE))\n\t\t\tmemset(ocir, 0, TG3_OCIR_LEN);\n\t}\n}\n\n/* sysfs attributes for hwmon */\nstatic ssize_t tg3_show_temp(struct device *dev,\n\t\t\t     struct device_attribute *devattr, char *buf)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(netdev);\n\tstruct sensor_device_attribute *attr = to_sensor_dev_attr(devattr);\n\tu32 temperature;\n\n\tspin_lock_bh(&tp->lock);\n\ttg3_ape_scratchpad_read(tp, &temperature, attr->index,\n\t\t\t\tsizeof(temperature));\n\tspin_unlock_bh(&tp->lock);\n\treturn sprintf(buf, \"%u\\n\", temperature);\n}\n\n\nstatic SENSOR_DEVICE_ATTR(temp1_input, S_IRUGO, tg3_show_temp, NULL,\n\t\t\t  TG3_TEMP_SENSOR_OFFSET);\nstatic SENSOR_DEVICE_ATTR(temp1_crit, S_IRUGO, tg3_show_temp, NULL,\n\t\t\t  TG3_TEMP_CAUTION_OFFSET);\nstatic SENSOR_DEVICE_ATTR(temp1_max, S_IRUGO, tg3_show_temp, NULL,\n\t\t\t  TG3_TEMP_MAX_OFFSET);\n\nstatic struct attribute *tg3_attributes[] = {\n\t&sensor_dev_attr_temp1_input.dev_attr.attr,\n\t&sensor_dev_attr_temp1_crit.dev_attr.attr,\n\t&sensor_dev_attr_temp1_max.dev_attr.attr,\n\tNULL\n};\n\nstatic const struct attribute_group tg3_group = {\n\t.attrs = tg3_attributes,\n};\n\nstatic void tg3_hwmon_close(struct tg3 *tp)\n{\n\tif (tp->hwmon_dev) {\n\t\thwmon_device_unregister(tp->hwmon_dev);\n\t\ttp->hwmon_dev = NULL;\n\t\tsysfs_remove_group(&tp->pdev->dev.kobj, &tg3_group);\n\t}\n}\n\nstatic void tg3_hwmon_open(struct tg3 *tp)\n{\n\tint i, err;\n\tu32 size = 0;\n\tstruct pci_dev *pdev = tp->pdev;\n\tstruct tg3_ocir ocirs[TG3_SD_NUM_RECS];\n\n\ttg3_sd_scan_scratchpad(tp, ocirs);\n\n\tfor (i = 0; i < TG3_SD_NUM_RECS; i++) {\n\t\tif (!ocirs[i].src_data_length)\n\t\t\tcontinue;\n\n\t\tsize += ocirs[i].src_hdr_length;\n\t\tsize += ocirs[i].src_data_length;\n\t}\n\n\tif (!size)\n\t\treturn;\n\n\t/* Register hwmon sysfs hooks */\n\terr = sysfs_create_group(&pdev->dev.kobj, &tg3_group);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot create sysfs group, aborting\\n\");\n\t\treturn;\n\t}\n\n\ttp->hwmon_dev = hwmon_device_register(&pdev->dev);\n\tif (IS_ERR(tp->hwmon_dev)) {\n\t\ttp->hwmon_dev = NULL;\n\t\tdev_err(&pdev->dev, \"Cannot register hwmon device, aborting\\n\");\n\t\tsysfs_remove_group(&pdev->dev.kobj, &tg3_group);\n\t}\n}\n\n\n#define TG3_STAT_ADD32(PSTAT, REG) \\\ndo {\tu32 __val = tr32(REG); \\\n\t(PSTAT)->low += __val; \\\n\tif ((PSTAT)->low < __val) \\\n\t\t(PSTAT)->high += 1; \\\n} while (0)\n\nstatic void tg3_periodic_fetch_stats(struct tg3 *tp)\n{\n\tstruct tg3_hw_stats *sp = tp->hw_stats;\n\n\tif (!tp->link_up)\n\t\treturn;\n\n\tTG3_STAT_ADD32(&sp->tx_octets, MAC_TX_STATS_OCTETS);\n\tTG3_STAT_ADD32(&sp->tx_collisions, MAC_TX_STATS_COLLISIONS);\n\tTG3_STAT_ADD32(&sp->tx_xon_sent, MAC_TX_STATS_XON_SENT);\n\tTG3_STAT_ADD32(&sp->tx_xoff_sent, MAC_TX_STATS_XOFF_SENT);\n\tTG3_STAT_ADD32(&sp->tx_mac_errors, MAC_TX_STATS_MAC_ERRORS);\n\tTG3_STAT_ADD32(&sp->tx_single_collisions, MAC_TX_STATS_SINGLE_COLLISIONS);\n\tTG3_STAT_ADD32(&sp->tx_mult_collisions, MAC_TX_STATS_MULT_COLLISIONS);\n\tTG3_STAT_ADD32(&sp->tx_deferred, MAC_TX_STATS_DEFERRED);\n\tTG3_STAT_ADD32(&sp->tx_excessive_collisions, MAC_TX_STATS_EXCESSIVE_COL);\n\tTG3_STAT_ADD32(&sp->tx_late_collisions, MAC_TX_STATS_LATE_COL);\n\tTG3_STAT_ADD32(&sp->tx_ucast_packets, MAC_TX_STATS_UCAST);\n\tTG3_STAT_ADD32(&sp->tx_mcast_packets, MAC_TX_STATS_MCAST);\n\tTG3_STAT_ADD32(&sp->tx_bcast_packets, MAC_TX_STATS_BCAST);\n\tif (unlikely(tg3_flag(tp, 5719_RDMA_BUG) &&\n\t\t     (sp->tx_ucast_packets.low + sp->tx_mcast_packets.low +\n\t\t      sp->tx_bcast_packets.low) > TG3_NUM_RDMA_CHANNELS)) {\n\t\tu32 val;\n\n\t\tval = tr32(TG3_LSO_RD_DMA_CRPTEN_CTRL);\n\t\tval &= ~TG3_LSO_RD_DMA_TX_LENGTH_WA;\n\t\ttw32(TG3_LSO_RD_DMA_CRPTEN_CTRL, val);\n\t\ttg3_flag_clear(tp, 5719_RDMA_BUG);\n\t}\n\n\tTG3_STAT_ADD32(&sp->rx_octets, MAC_RX_STATS_OCTETS);\n\tTG3_STAT_ADD32(&sp->rx_fragments, MAC_RX_STATS_FRAGMENTS);\n\tTG3_STAT_ADD32(&sp->rx_ucast_packets, MAC_RX_STATS_UCAST);\n\tTG3_STAT_ADD32(&sp->rx_mcast_packets, MAC_RX_STATS_MCAST);\n\tTG3_STAT_ADD32(&sp->rx_bcast_packets, MAC_RX_STATS_BCAST);\n\tTG3_STAT_ADD32(&sp->rx_fcs_errors, MAC_RX_STATS_FCS_ERRORS);\n\tTG3_STAT_ADD32(&sp->rx_align_errors, MAC_RX_STATS_ALIGN_ERRORS);\n\tTG3_STAT_ADD32(&sp->rx_xon_pause_rcvd, MAC_RX_STATS_XON_PAUSE_RECVD);\n\tTG3_STAT_ADD32(&sp->rx_xoff_pause_rcvd, MAC_RX_STATS_XOFF_PAUSE_RECVD);\n\tTG3_STAT_ADD32(&sp->rx_mac_ctrl_rcvd, MAC_RX_STATS_MAC_CTRL_RECVD);\n\tTG3_STAT_ADD32(&sp->rx_xoff_entered, MAC_RX_STATS_XOFF_ENTERED);\n\tTG3_STAT_ADD32(&sp->rx_frame_too_long_errors, MAC_RX_STATS_FRAME_TOO_LONG);\n\tTG3_STAT_ADD32(&sp->rx_jabbers, MAC_RX_STATS_JABBERS);\n\tTG3_STAT_ADD32(&sp->rx_undersize_packets, MAC_RX_STATS_UNDERSIZE);\n\n\tTG3_STAT_ADD32(&sp->rxbds_empty, RCVLPC_NO_RCV_BD_CNT);\n\tif (tg3_asic_rev(tp) != ASIC_REV_5717 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5719_A0 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5720_A0) {\n\t\tTG3_STAT_ADD32(&sp->rx_discards, RCVLPC_IN_DISCARDS_CNT);\n\t} else {\n\t\tu32 val = tr32(HOSTCC_FLOW_ATTN);\n\t\tval = (val & HOSTCC_FLOW_ATTN_MBUF_LWM) ? 1 : 0;\n\t\tif (val) {\n\t\t\ttw32(HOSTCC_FLOW_ATTN, HOSTCC_FLOW_ATTN_MBUF_LWM);\n\t\t\tsp->rx_discards.low += val;\n\t\t\tif (sp->rx_discards.low < val)\n\t\t\t\tsp->rx_discards.high += 1;\n\t\t}\n\t\tsp->mbuf_lwm_thresh_hit = sp->rx_discards;\n\t}\n\tTG3_STAT_ADD32(&sp->rx_errors, RCVLPC_IN_ERRORS_CNT);\n}\n\nstatic void tg3_chk_missed_msi(struct tg3 *tp)\n{\n\tu32 i;\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\tif (tg3_has_work(tnapi)) {\n\t\t\tif (tnapi->last_rx_cons == tnapi->rx_rcb_ptr &&\n\t\t\t    tnapi->last_tx_cons == tnapi->tx_cons) {\n\t\t\t\tif (tnapi->chk_msi_cnt < 1) {\n\t\t\t\t\ttnapi->chk_msi_cnt++;\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\ttg3_msi(0, tnapi);\n\t\t\t}\n\t\t}\n\t\ttnapi->chk_msi_cnt = 0;\n\t\ttnapi->last_rx_cons = tnapi->rx_rcb_ptr;\n\t\ttnapi->last_tx_cons = tnapi->tx_cons;\n\t}\n}\n\nstatic void tg3_timer(unsigned long __opaque)\n{\n\tstruct tg3 *tp = (struct tg3 *) __opaque;\n\n\tif (tp->irq_sync || tg3_flag(tp, RESET_TASK_PENDING))\n\t\tgoto restart_timer;\n\n\tspin_lock(&tp->lock);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_flag(tp, 57765_CLASS))\n\t\ttg3_chk_missed_msi(tp);\n\n\tif (tg3_flag(tp, FLUSH_POSTED_WRITES)) {\n\t\t/* BCM4785: Flush posted writes from GbE to host memory. */\n\t\ttr32(HOSTCC_MODE);\n\t}\n\n\tif (!tg3_flag(tp, TAGGED_STATUS)) {\n\t\t/* All of this garbage is because when using non-tagged\n\t\t * IRQ status the mailbox/status_block protocol the chip\n\t\t * uses with the cpu is race prone.\n\t\t */\n\t\tif (tp->napi[0].hw_status->status & SD_STATUS_UPDATED) {\n\t\t\ttw32(GRC_LOCAL_CTRL,\n\t\t\t     tp->grc_local_ctrl | GRC_LCLCTRL_SETINT);\n\t\t} else {\n\t\t\ttw32(HOSTCC_MODE, tp->coalesce_mode |\n\t\t\t     HOSTCC_MODE_ENABLE | HOSTCC_MODE_NOW);\n\t\t}\n\n\t\tif (!(tr32(WDMAC_MODE) & WDMAC_MODE_ENABLE)) {\n\t\t\tspin_unlock(&tp->lock);\n\t\t\ttg3_reset_task_schedule(tp);\n\t\t\tgoto restart_timer;\n\t\t}\n\t}\n\n\t/* This part only runs once per second. */\n\tif (!--tp->timer_counter) {\n\t\tif (tg3_flag(tp, 5705_PLUS))\n\t\t\ttg3_periodic_fetch_stats(tp);\n\n\t\tif (tp->setlpicnt && !--tp->setlpicnt)\n\t\t\ttg3_phy_eee_enable(tp);\n\n\t\tif (tg3_flag(tp, USE_LINKCHG_REG)) {\n\t\t\tu32 mac_stat;\n\t\t\tint phy_event;\n\n\t\t\tmac_stat = tr32(MAC_STATUS);\n\n\t\t\tphy_event = 0;\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_USE_MI_INTERRUPT) {\n\t\t\t\tif (mac_stat & MAC_STATUS_MI_INTERRUPT)\n\t\t\t\t\tphy_event = 1;\n\t\t\t} else if (mac_stat & MAC_STATUS_LNKSTATE_CHANGED)\n\t\t\t\tphy_event = 1;\n\n\t\t\tif (phy_event)\n\t\t\t\ttg3_setup_phy(tp, 0);\n\t\t} else if (tg3_flag(tp, POLL_SERDES)) {\n\t\t\tu32 mac_stat = tr32(MAC_STATUS);\n\t\t\tint need_setup = 0;\n\n\t\t\tif (tp->link_up &&\n\t\t\t    (mac_stat & MAC_STATUS_LNKSTATE_CHANGED)) {\n\t\t\t\tneed_setup = 1;\n\t\t\t}\n\t\t\tif (!tp->link_up &&\n\t\t\t    (mac_stat & (MAC_STATUS_PCS_SYNCED |\n\t\t\t\t\t MAC_STATUS_SIGNAL_DET))) {\n\t\t\t\tneed_setup = 1;\n\t\t\t}\n\t\t\tif (need_setup) {\n\t\t\t\tif (!tp->serdes_counter) {\n\t\t\t\t\ttw32_f(MAC_MODE,\n\t\t\t\t\t     (tp->mac_mode &\n\t\t\t\t\t      ~MAC_MODE_PORT_MODE_MASK));\n\t\t\t\t\tudelay(40);\n\t\t\t\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\t\t\t\tudelay(40);\n\t\t\t\t}\n\t\t\t\ttg3_setup_phy(tp, 0);\n\t\t\t}\n\t\t} else if ((tp->phy_flags & TG3_PHYFLG_MII_SERDES) &&\n\t\t\t   tg3_flag(tp, 5780_CLASS)) {\n\t\t\ttg3_serdes_parallel_detect(tp);\n\t\t}\n\n\t\ttp->timer_counter = tp->timer_multiplier;\n\t}\n\n\t/* Heartbeat is only sent once every 2 seconds.\n\t *\n\t * The heartbeat is to tell the ASF firmware that the host\n\t * driver is still alive.  In the event that the OS crashes,\n\t * ASF needs to reset the hardware to free up the FIFO space\n\t * that may be filled with rx packets destined for the host.\n\t * If the FIFO is full, ASF will no longer function properly.\n\t *\n\t * Unintended resets have been reported on real time kernels\n\t * where the timer doesn't run on time.  Netpoll will also have\n\t * same problem.\n\t *\n\t * The new FWCMD_NICDRV_ALIVE3 command tells the ASF firmware\n\t * to check the ring condition when the heartbeat is expiring\n\t * before doing the reset.  This will prevent most unintended\n\t * resets.\n\t */\n\tif (!--tp->asf_counter) {\n\t\tif (tg3_flag(tp, ENABLE_ASF) && !tg3_flag(tp, ENABLE_APE)) {\n\t\t\ttg3_wait_for_event_ack(tp);\n\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_MBOX,\n\t\t\t\t      FWCMD_NICDRV_ALIVE3);\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_LEN_MBOX, 4);\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX,\n\t\t\t\t      TG3_FW_UPDATE_TIMEOUT_SEC);\n\n\t\t\ttg3_generate_fw_event(tp);\n\t\t}\n\t\ttp->asf_counter = tp->asf_multiplier;\n\t}\n\n\tspin_unlock(&tp->lock);\n\nrestart_timer:\n\ttp->timer.expires = jiffies + tp->timer_offset;\n\tadd_timer(&tp->timer);\n}\n\nstatic void tg3_timer_init(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, TAGGED_STATUS) &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5717 &&\n\t    !tg3_flag(tp, 57765_CLASS))\n\t\ttp->timer_offset = HZ;\n\telse\n\t\ttp->timer_offset = HZ / 10;\n\n\tBUG_ON(tp->timer_offset > HZ);\n\n\ttp->timer_multiplier = (HZ / tp->timer_offset);\n\ttp->asf_multiplier = (HZ / tp->timer_offset) *\n\t\t\t     TG3_FW_UPDATE_FREQ_SEC;\n\n\tinit_timer(&tp->timer);\n\ttp->timer.data = (unsigned long) tp;\n\ttp->timer.function = tg3_timer;\n}\n\nstatic void tg3_timer_start(struct tg3 *tp)\n{\n\ttp->asf_counter   = tp->asf_multiplier;\n\ttp->timer_counter = tp->timer_multiplier;\n\n\ttp->timer.expires = jiffies + tp->timer_offset;\n\tadd_timer(&tp->timer);\n}\n\nstatic void tg3_timer_stop(struct tg3 *tp)\n{\n\tdel_timer_sync(&tp->timer);\n}\n\n/* Restart hardware after configuration changes, self-test, etc.\n * Invoked with tp->lock held.\n */\nstatic int tg3_restart_hw(struct tg3 *tp, int reset_phy)\n\t__releases(tp->lock)\n\t__acquires(tp->lock)\n{\n\tint err;\n\n\terr = tg3_init_hw(tp, reset_phy);\n\tif (err) {\n\t\tnetdev_err(tp->dev,\n\t\t\t   \"Failed to re-initialize device, aborting\\n\");\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\ttg3_full_unlock(tp);\n\t\ttg3_timer_stop(tp);\n\t\ttp->irq_sync = 0;\n\t\ttg3_napi_enable(tp);\n\t\tdev_close(tp->dev);\n\t\ttg3_full_lock(tp, 0);\n\t}\n\treturn err;\n}\n\nstatic void tg3_reset_task(struct work_struct *work)\n{\n\tstruct tg3 *tp = container_of(work, struct tg3, reset_task);\n\tint err;\n\n\ttg3_full_lock(tp, 0);\n\n\tif (!netif_running(tp->dev)) {\n\t\ttg3_flag_clear(tp, RESET_TASK_PENDING);\n\t\ttg3_full_unlock(tp);\n\t\treturn;\n\t}\n\n\ttg3_full_unlock(tp);\n\n\ttg3_phy_stop(tp);\n\n\ttg3_netif_stop(tp);\n\n\ttg3_full_lock(tp, 1);\n\n\tif (tg3_flag(tp, TX_RECOVERY_PENDING)) {\n\t\ttp->write32_tx_mbox = tg3_write32_tx_mbox;\n\t\ttp->write32_rx_mbox = tg3_write_flush_reg32;\n\t\ttg3_flag_set(tp, MBOX_WRITE_REORDER);\n\t\ttg3_flag_clear(tp, TX_RECOVERY_PENDING);\n\t}\n\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 0);\n\terr = tg3_init_hw(tp, 1);\n\tif (err)\n\t\tgoto out;\n\n\ttg3_netif_start(tp);\n\nout:\n\ttg3_full_unlock(tp);\n\n\tif (!err)\n\t\ttg3_phy_start(tp);\n\n\ttg3_flag_clear(tp, RESET_TASK_PENDING);\n}\n\nstatic int tg3_request_irq(struct tg3 *tp, int irq_num)\n{\n\tirq_handler_t fn;\n\tunsigned long flags;\n\tchar *name;\n\tstruct tg3_napi *tnapi = &tp->napi[irq_num];\n\n\tif (tp->irq_cnt == 1)\n\t\tname = tp->dev->name;\n\telse {\n\t\tname = &tnapi->irq_lbl[0];\n\t\tsnprintf(name, IFNAMSIZ, \"%s-%d\", tp->dev->name, irq_num);\n\t\tname[IFNAMSIZ-1] = 0;\n\t}\n\n\tif (tg3_flag(tp, USING_MSI) || tg3_flag(tp, USING_MSIX)) {\n\t\tfn = tg3_msi;\n\t\tif (tg3_flag(tp, 1SHOT_MSI))\n\t\t\tfn = tg3_msi_1shot;\n\t\tflags = 0;\n\t} else {\n\t\tfn = tg3_interrupt;\n\t\tif (tg3_flag(tp, TAGGED_STATUS))\n\t\t\tfn = tg3_interrupt_tagged;\n\t\tflags = IRQF_SHARED;\n\t}\n\n\treturn request_irq(tnapi->irq_vec, fn, flags, name, tnapi);\n}\n\nstatic int tg3_test_interrupt(struct tg3 *tp)\n{\n\tstruct tg3_napi *tnapi = &tp->napi[0];\n\tstruct net_device *dev = tp->dev;\n\tint err, i, intr_ok = 0;\n\tu32 val;\n\n\tif (!netif_running(dev))\n\t\treturn -ENODEV;\n\n\ttg3_disable_ints(tp);\n\n\tfree_irq(tnapi->irq_vec, tnapi);\n\n\t/*\n\t * Turn off MSI one shot mode.  Otherwise this test has no\n\t * observable way to know whether the interrupt was delivered.\n\t */\n\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\tval = tr32(MSGINT_MODE) | MSGINT_MODE_ONE_SHOT_DISABLE;\n\t\ttw32(MSGINT_MODE, val);\n\t}\n\n\terr = request_irq(tnapi->irq_vec, tg3_test_isr,\n\t\t\t  IRQF_SHARED, dev->name, tnapi);\n\tif (err)\n\t\treturn err;\n\n\ttnapi->hw_status->status &= ~SD_STATUS_UPDATED;\n\ttg3_enable_ints(tp);\n\n\ttw32_f(HOSTCC_MODE, tp->coalesce_mode | HOSTCC_MODE_ENABLE |\n\t       tnapi->coal_now);\n\n\tfor (i = 0; i < 5; i++) {\n\t\tu32 int_mbox, misc_host_ctrl;\n\n\t\tint_mbox = tr32_mailbox(tnapi->int_mbox);\n\t\tmisc_host_ctrl = tr32(TG3PCI_MISC_HOST_CTRL);\n\n\t\tif ((int_mbox != 0) ||\n\t\t    (misc_host_ctrl & MISC_HOST_CTRL_MASK_PCI_INT)) {\n\t\t\tintr_ok = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tg3_flag(tp, 57765_PLUS) &&\n\t\t    tnapi->hw_status->status_tag != tnapi->last_tag)\n\t\t\ttw32_mailbox_f(tnapi->int_mbox, tnapi->last_tag << 24);\n\n\t\tmsleep(10);\n\t}\n\n\ttg3_disable_ints(tp);\n\n\tfree_irq(tnapi->irq_vec, tnapi);\n\n\terr = tg3_request_irq(tp, 0);\n\n\tif (err)\n\t\treturn err;\n\n\tif (intr_ok) {\n\t\t/* Reenable MSI one shot mode. */\n\t\tif (tg3_flag(tp, 57765_PLUS) && tg3_flag(tp, 1SHOT_MSI)) {\n\t\t\tval = tr32(MSGINT_MODE) & ~MSGINT_MODE_ONE_SHOT_DISABLE;\n\t\t\ttw32(MSGINT_MODE, val);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -EIO;\n}\n\n/* Returns 0 if MSI test succeeds or MSI test fails and INTx mode is\n * successfully restored\n */\nstatic int tg3_test_msi(struct tg3 *tp)\n{\n\tint err;\n\tu16 pci_cmd;\n\n\tif (!tg3_flag(tp, USING_MSI))\n\t\treturn 0;\n\n\t/* Turn off SERR reporting in case MSI terminates with Master\n\t * Abort.\n\t */\n\tpci_read_config_word(tp->pdev, PCI_COMMAND, &pci_cmd);\n\tpci_write_config_word(tp->pdev, PCI_COMMAND,\n\t\t\t      pci_cmd & ~PCI_COMMAND_SERR);\n\n\terr = tg3_test_interrupt(tp);\n\n\tpci_write_config_word(tp->pdev, PCI_COMMAND, pci_cmd);\n\n\tif (!err)\n\t\treturn 0;\n\n\t/* other failures */\n\tif (err != -EIO)\n\t\treturn err;\n\n\t/* MSI test failed, go back to INTx mode */\n\tnetdev_warn(tp->dev, \"No interrupt was generated using MSI. Switching \"\n\t\t    \"to INTx mode. Please report this failure to the PCI \"\n\t\t    \"maintainer and include system chipset information\\n\");\n\n\tfree_irq(tp->napi[0].irq_vec, &tp->napi[0]);\n\n\tpci_disable_msi(tp->pdev);\n\n\ttg3_flag_clear(tp, USING_MSI);\n\ttp->napi[0].irq_vec = tp->pdev->irq;\n\n\terr = tg3_request_irq(tp, 0);\n\tif (err)\n\t\treturn err;\n\n\t/* Need to reset the chip because the MSI cycle may have terminated\n\t * with Master Abort.\n\t */\n\ttg3_full_lock(tp, 1);\n\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\terr = tg3_init_hw(tp, 1);\n\n\ttg3_full_unlock(tp);\n\n\tif (err)\n\t\tfree_irq(tp->napi[0].irq_vec, &tp->napi[0]);\n\n\treturn err;\n}\n\nstatic int tg3_request_firmware(struct tg3 *tp)\n{\n\tconst __be32 *fw_data;\n\n\tif (request_firmware(&tp->fw, tp->fw_needed, &tp->pdev->dev)) {\n\t\tnetdev_err(tp->dev, \"Failed to load firmware \\\"%s\\\"\\n\",\n\t\t\t   tp->fw_needed);\n\t\treturn -ENOENT;\n\t}\n\n\tfw_data = (void *)tp->fw->data;\n\n\t/* Firmware blob starts with version numbers, followed by\n\t * start address and _full_ length including BSS sections\n\t * (which must be longer than the actual data, of course\n\t */\n\n\ttp->fw_len = be32_to_cpu(fw_data[2]);\t/* includes bss */\n\tif (tp->fw_len < (tp->fw->size - 12)) {\n\t\tnetdev_err(tp->dev, \"bogus length %d in \\\"%s\\\"\\n\",\n\t\t\t   tp->fw_len, tp->fw_needed);\n\t\trelease_firmware(tp->fw);\n\t\ttp->fw = NULL;\n\t\treturn -EINVAL;\n\t}\n\n\t/* We no longer need firmware; we have it. */\n\ttp->fw_needed = NULL;\n\treturn 0;\n}\n\nstatic u32 tg3_irq_count(struct tg3 *tp)\n{\n\tu32 irq_cnt = max(tp->rxq_cnt, tp->txq_cnt);\n\n\tif (irq_cnt > 1) {\n\t\t/* We want as many rx rings enabled as there are cpus.\n\t\t * In multiqueue MSI-X mode, the first MSI-X vector\n\t\t * only deals with link interrupts, etc, so we add\n\t\t * one to the number of vectors we are requesting.\n\t\t */\n\t\tirq_cnt = min_t(unsigned, irq_cnt + 1, tp->irq_max);\n\t}\n\n\treturn irq_cnt;\n}\n\nstatic bool tg3_enable_msix(struct tg3 *tp)\n{\n\tint i, rc;\n\tstruct msix_entry msix_ent[TG3_IRQ_MAX_VECS];\n\n\ttp->txq_cnt = tp->txq_req;\n\ttp->rxq_cnt = tp->rxq_req;\n\tif (!tp->rxq_cnt)\n\t\ttp->rxq_cnt = netif_get_num_default_rss_queues();\n\tif (tp->rxq_cnt > tp->rxq_max)\n\t\ttp->rxq_cnt = tp->rxq_max;\n\n\t/* Disable multiple TX rings by default.  Simple round-robin hardware\n\t * scheduling of the TX rings can cause starvation of rings with\n\t * small packets when other rings have TSO or jumbo packets.\n\t */\n\tif (!tp->txq_req)\n\t\ttp->txq_cnt = 1;\n\n\ttp->irq_cnt = tg3_irq_count(tp);\n\n\tfor (i = 0; i < tp->irq_max; i++) {\n\t\tmsix_ent[i].entry  = i;\n\t\tmsix_ent[i].vector = 0;\n\t}\n\n\trc = pci_enable_msix(tp->pdev, msix_ent, tp->irq_cnt);\n\tif (rc < 0) {\n\t\treturn false;\n\t} else if (rc != 0) {\n\t\tif (pci_enable_msix(tp->pdev, msix_ent, rc))\n\t\t\treturn false;\n\t\tnetdev_notice(tp->dev, \"Requested %d MSI-X vectors, received %d\\n\",\n\t\t\t      tp->irq_cnt, rc);\n\t\ttp->irq_cnt = rc;\n\t\ttp->rxq_cnt = max(rc - 1, 1);\n\t\tif (tp->txq_cnt)\n\t\t\ttp->txq_cnt = min(tp->rxq_cnt, tp->txq_max);\n\t}\n\n\tfor (i = 0; i < tp->irq_max; i++)\n\t\ttp->napi[i].irq_vec = msix_ent[i].vector;\n\n\tif (netif_set_real_num_rx_queues(tp->dev, tp->rxq_cnt)) {\n\t\tpci_disable_msix(tp->pdev);\n\t\treturn false;\n\t}\n\n\tif (tp->irq_cnt == 1)\n\t\treturn true;\n\n\ttg3_flag_set(tp, ENABLE_RSS);\n\n\tif (tp->txq_cnt > 1)\n\t\ttg3_flag_set(tp, ENABLE_TSS);\n\n\tnetif_set_real_num_tx_queues(tp->dev, tp->txq_cnt);\n\n\treturn true;\n}\n\nstatic void tg3_ints_init(struct tg3 *tp)\n{\n\tif ((tg3_flag(tp, SUPPORT_MSI) || tg3_flag(tp, SUPPORT_MSIX)) &&\n\t    !tg3_flag(tp, TAGGED_STATUS)) {\n\t\t/* All MSI supporting chips should support tagged\n\t\t * status.  Assert that this is the case.\n\t\t */\n\t\tnetdev_warn(tp->dev,\n\t\t\t    \"MSI without TAGGED_STATUS? Not using MSI\\n\");\n\t\tgoto defcfg;\n\t}\n\n\tif (tg3_flag(tp, SUPPORT_MSIX) && tg3_enable_msix(tp))\n\t\ttg3_flag_set(tp, USING_MSIX);\n\telse if (tg3_flag(tp, SUPPORT_MSI) && pci_enable_msi(tp->pdev) == 0)\n\t\ttg3_flag_set(tp, USING_MSI);\n\n\tif (tg3_flag(tp, USING_MSI) || tg3_flag(tp, USING_MSIX)) {\n\t\tu32 msi_mode = tr32(MSGINT_MODE);\n\t\tif (tg3_flag(tp, USING_MSIX) && tp->irq_cnt > 1)\n\t\t\tmsi_mode |= MSGINT_MODE_MULTIVEC_EN;\n\t\tif (!tg3_flag(tp, 1SHOT_MSI))\n\t\t\tmsi_mode |= MSGINT_MODE_ONE_SHOT_DISABLE;\n\t\ttw32(MSGINT_MODE, msi_mode | MSGINT_MODE_ENABLE);\n\t}\ndefcfg:\n\tif (!tg3_flag(tp, USING_MSIX)) {\n\t\ttp->irq_cnt = 1;\n\t\ttp->napi[0].irq_vec = tp->pdev->irq;\n\t}\n\n\tif (tp->irq_cnt == 1) {\n\t\ttp->txq_cnt = 1;\n\t\ttp->rxq_cnt = 1;\n\t\tnetif_set_real_num_tx_queues(tp->dev, 1);\n\t\tnetif_set_real_num_rx_queues(tp->dev, 1);\n\t}\n}\n\nstatic void tg3_ints_fini(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, USING_MSIX))\n\t\tpci_disable_msix(tp->pdev);\n\telse if (tg3_flag(tp, USING_MSI))\n\t\tpci_disable_msi(tp->pdev);\n\ttg3_flag_clear(tp, USING_MSI);\n\ttg3_flag_clear(tp, USING_MSIX);\n\ttg3_flag_clear(tp, ENABLE_RSS);\n\ttg3_flag_clear(tp, ENABLE_TSS);\n}\n\nstatic int tg3_start(struct tg3 *tp, bool reset_phy, bool test_irq,\n\t\t     bool init)\n{\n\tstruct net_device *dev = tp->dev;\n\tint i, err;\n\n\t/*\n\t * Setup interrupts first so we know how\n\t * many NAPI resources to allocate\n\t */\n\ttg3_ints_init(tp);\n\n\ttg3_rss_check_indir_tbl(tp);\n\n\t/* The placement of this call is tied\n\t * to the setup and use of Host TX descriptors.\n\t */\n\terr = tg3_alloc_consistent(tp);\n\tif (err)\n\t\tgoto err_out1;\n\n\ttg3_napi_init(tp);\n\n\ttg3_napi_enable(tp);\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\terr = tg3_request_irq(tp, i);\n\t\tif (err) {\n\t\t\tfor (i--; i >= 0; i--) {\n\t\t\t\ttnapi = &tp->napi[i];\n\t\t\t\tfree_irq(tnapi->irq_vec, tnapi);\n\t\t\t}\n\t\t\tgoto err_out2;\n\t\t}\n\t}\n\n\ttg3_full_lock(tp, 0);\n\n\terr = tg3_init_hw(tp, reset_phy);\n\tif (err) {\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\ttg3_free_rings(tp);\n\t}\n\n\ttg3_full_unlock(tp);\n\n\tif (err)\n\t\tgoto err_out3;\n\n\tif (test_irq && tg3_flag(tp, USING_MSI)) {\n\t\terr = tg3_test_msi(tp);\n\n\t\tif (err) {\n\t\t\ttg3_full_lock(tp, 0);\n\t\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\t\ttg3_free_rings(tp);\n\t\t\ttg3_full_unlock(tp);\n\n\t\t\tgoto err_out2;\n\t\t}\n\n\t\tif (!tg3_flag(tp, 57765_PLUS) && tg3_flag(tp, USING_MSI)) {\n\t\t\tu32 val = tr32(PCIE_TRANSACTION_CFG);\n\n\t\t\ttw32(PCIE_TRANSACTION_CFG,\n\t\t\t     val | PCIE_TRANS_CFG_1SHOT_MSI);\n\t\t}\n\t}\n\n\ttg3_phy_start(tp);\n\n\ttg3_hwmon_open(tp);\n\n\ttg3_full_lock(tp, 0);\n\n\ttg3_timer_start(tp);\n\ttg3_flag_set(tp, INIT_COMPLETE);\n\ttg3_enable_ints(tp);\n\n\tif (init)\n\t\ttg3_ptp_init(tp);\n\telse\n\t\ttg3_ptp_resume(tp);\n\n\n\ttg3_full_unlock(tp);\n\n\tnetif_tx_start_all_queues(dev);\n\n\t/*\n\t * Reset loopback feature if it was turned on while the device was down\n\t * make sure that it's installed properly now.\n\t */\n\tif (dev->features & NETIF_F_LOOPBACK)\n\t\ttg3_set_loopback(dev, dev->features);\n\n\treturn 0;\n\nerr_out3:\n\tfor (i = tp->irq_cnt - 1; i >= 0; i--) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tfree_irq(tnapi->irq_vec, tnapi);\n\t}\n\nerr_out2:\n\ttg3_napi_disable(tp);\n\ttg3_napi_fini(tp);\n\ttg3_free_consistent(tp);\n\nerr_out1:\n\ttg3_ints_fini(tp);\n\n\treturn err;\n}\n\nstatic void tg3_stop(struct tg3 *tp)\n{\n\tint i;\n\n\ttg3_reset_task_cancel(tp);\n\ttg3_netif_stop(tp);\n\n\ttg3_timer_stop(tp);\n\n\ttg3_hwmon_close(tp);\n\n\ttg3_phy_stop(tp);\n\n\ttg3_full_lock(tp, 1);\n\n\ttg3_disable_ints(tp);\n\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\ttg3_free_rings(tp);\n\ttg3_flag_clear(tp, INIT_COMPLETE);\n\n\ttg3_full_unlock(tp);\n\n\tfor (i = tp->irq_cnt - 1; i >= 0; i--) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tfree_irq(tnapi->irq_vec, tnapi);\n\t}\n\n\ttg3_ints_fini(tp);\n\n\ttg3_napi_fini(tp);\n\n\ttg3_free_consistent(tp);\n}\n\nstatic int tg3_open(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err;\n\n\tif (tp->fw_needed) {\n\t\terr = tg3_request_firmware(tp);\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0) {\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else if (err) {\n\t\t\tnetdev_warn(tp->dev, \"TSO capability disabled\\n\");\n\t\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\t} else if (!tg3_flag(tp, TSO_CAPABLE)) {\n\t\t\tnetdev_notice(tp->dev, \"TSO capability restored\\n\");\n\t\t\ttg3_flag_set(tp, TSO_CAPABLE);\n\t\t}\n\t}\n\n\ttg3_carrier_off(tp);\n\n\terr = tg3_power_up(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_full_lock(tp, 0);\n\n\ttg3_disable_ints(tp);\n\ttg3_flag_clear(tp, INIT_COMPLETE);\n\n\ttg3_full_unlock(tp);\n\n\terr = tg3_start(tp, true, true, true);\n\tif (err) {\n\t\ttg3_frob_aux_power(tp, false);\n\t\tpci_set_power_state(tp->pdev, PCI_D3hot);\n\t}\n\n\tif (tg3_flag(tp, PTP_CAPABLE)) {\n\t\ttp->ptp_clock = ptp_clock_register(&tp->ptp_info,\n\t\t\t\t\t\t   &tp->pdev->dev);\n\t\tif (IS_ERR(tp->ptp_clock))\n\t\t\ttp->ptp_clock = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_close(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\ttg3_ptp_fini(tp);\n\n\ttg3_stop(tp);\n\n\t/* Clear stats across close / open calls */\n\tmemset(&tp->net_stats_prev, 0, sizeof(tp->net_stats_prev));\n\tmemset(&tp->estats_prev, 0, sizeof(tp->estats_prev));\n\n\ttg3_power_down(tp);\n\n\ttg3_carrier_off(tp);\n\n\treturn 0;\n}\n\nstatic inline u64 get_stat64(tg3_stat64_t *val)\n{\n       return ((u64)val->high << 32) | ((u64)val->low);\n}\n\nstatic u64 tg3_calc_crc_errors(struct tg3 *tp)\n{\n\tstruct tg3_hw_stats *hw_stats = tp->hw_stats;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_PHY_SERDES) &&\n\t    (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5701)) {\n\t\tu32 val;\n\n\t\tif (!tg3_readphy(tp, MII_TG3_TEST1, &val)) {\n\t\t\ttg3_writephy(tp, MII_TG3_TEST1,\n\t\t\t\t     val | MII_TG3_TEST1_CRC_EN);\n\t\t\ttg3_readphy(tp, MII_TG3_RXR_COUNTERS, &val);\n\t\t} else\n\t\t\tval = 0;\n\n\t\ttp->phy_crc_errors += val;\n\n\t\treturn tp->phy_crc_errors;\n\t}\n\n\treturn get_stat64(&hw_stats->rx_fcs_errors);\n}\n\n#define ESTAT_ADD(member) \\\n\testats->member =\told_estats->member + \\\n\t\t\t\tget_stat64(&hw_stats->member)\n\nstatic void tg3_get_estats(struct tg3 *tp, struct tg3_ethtool_stats *estats)\n{\n\tstruct tg3_ethtool_stats *old_estats = &tp->estats_prev;\n\tstruct tg3_hw_stats *hw_stats = tp->hw_stats;\n\n\tESTAT_ADD(rx_octets);\n\tESTAT_ADD(rx_fragments);\n\tESTAT_ADD(rx_ucast_packets);\n\tESTAT_ADD(rx_mcast_packets);\n\tESTAT_ADD(rx_bcast_packets);\n\tESTAT_ADD(rx_fcs_errors);\n\tESTAT_ADD(rx_align_errors);\n\tESTAT_ADD(rx_xon_pause_rcvd);\n\tESTAT_ADD(rx_xoff_pause_rcvd);\n\tESTAT_ADD(rx_mac_ctrl_rcvd);\n\tESTAT_ADD(rx_xoff_entered);\n\tESTAT_ADD(rx_frame_too_long_errors);\n\tESTAT_ADD(rx_jabbers);\n\tESTAT_ADD(rx_undersize_packets);\n\tESTAT_ADD(rx_in_length_errors);\n\tESTAT_ADD(rx_out_length_errors);\n\tESTAT_ADD(rx_64_or_less_octet_packets);\n\tESTAT_ADD(rx_65_to_127_octet_packets);\n\tESTAT_ADD(rx_128_to_255_octet_packets);\n\tESTAT_ADD(rx_256_to_511_octet_packets);\n\tESTAT_ADD(rx_512_to_1023_octet_packets);\n\tESTAT_ADD(rx_1024_to_1522_octet_packets);\n\tESTAT_ADD(rx_1523_to_2047_octet_packets);\n\tESTAT_ADD(rx_2048_to_4095_octet_packets);\n\tESTAT_ADD(rx_4096_to_8191_octet_packets);\n\tESTAT_ADD(rx_8192_to_9022_octet_packets);\n\n\tESTAT_ADD(tx_octets);\n\tESTAT_ADD(tx_collisions);\n\tESTAT_ADD(tx_xon_sent);\n\tESTAT_ADD(tx_xoff_sent);\n\tESTAT_ADD(tx_flow_control);\n\tESTAT_ADD(tx_mac_errors);\n\tESTAT_ADD(tx_single_collisions);\n\tESTAT_ADD(tx_mult_collisions);\n\tESTAT_ADD(tx_deferred);\n\tESTAT_ADD(tx_excessive_collisions);\n\tESTAT_ADD(tx_late_collisions);\n\tESTAT_ADD(tx_collide_2times);\n\tESTAT_ADD(tx_collide_3times);\n\tESTAT_ADD(tx_collide_4times);\n\tESTAT_ADD(tx_collide_5times);\n\tESTAT_ADD(tx_collide_6times);\n\tESTAT_ADD(tx_collide_7times);\n\tESTAT_ADD(tx_collide_8times);\n\tESTAT_ADD(tx_collide_9times);\n\tESTAT_ADD(tx_collide_10times);\n\tESTAT_ADD(tx_collide_11times);\n\tESTAT_ADD(tx_collide_12times);\n\tESTAT_ADD(tx_collide_13times);\n\tESTAT_ADD(tx_collide_14times);\n\tESTAT_ADD(tx_collide_15times);\n\tESTAT_ADD(tx_ucast_packets);\n\tESTAT_ADD(tx_mcast_packets);\n\tESTAT_ADD(tx_bcast_packets);\n\tESTAT_ADD(tx_carrier_sense_errors);\n\tESTAT_ADD(tx_discards);\n\tESTAT_ADD(tx_errors);\n\n\tESTAT_ADD(dma_writeq_full);\n\tESTAT_ADD(dma_write_prioq_full);\n\tESTAT_ADD(rxbds_empty);\n\tESTAT_ADD(rx_discards);\n\tESTAT_ADD(rx_errors);\n\tESTAT_ADD(rx_threshold_hit);\n\n\tESTAT_ADD(dma_readq_full);\n\tESTAT_ADD(dma_read_prioq_full);\n\tESTAT_ADD(tx_comp_queue_full);\n\n\tESTAT_ADD(ring_set_send_prod_index);\n\tESTAT_ADD(ring_status_update);\n\tESTAT_ADD(nic_irqs);\n\tESTAT_ADD(nic_avoided_irqs);\n\tESTAT_ADD(nic_tx_threshold_hit);\n\n\tESTAT_ADD(mbuf_lwm_thresh_hit);\n}\n\nstatic void tg3_get_nstats(struct tg3 *tp, struct rtnl_link_stats64 *stats)\n{\n\tstruct rtnl_link_stats64 *old_stats = &tp->net_stats_prev;\n\tstruct tg3_hw_stats *hw_stats = tp->hw_stats;\n\n\tstats->rx_packets = old_stats->rx_packets +\n\t\tget_stat64(&hw_stats->rx_ucast_packets) +\n\t\tget_stat64(&hw_stats->rx_mcast_packets) +\n\t\tget_stat64(&hw_stats->rx_bcast_packets);\n\n\tstats->tx_packets = old_stats->tx_packets +\n\t\tget_stat64(&hw_stats->tx_ucast_packets) +\n\t\tget_stat64(&hw_stats->tx_mcast_packets) +\n\t\tget_stat64(&hw_stats->tx_bcast_packets);\n\n\tstats->rx_bytes = old_stats->rx_bytes +\n\t\tget_stat64(&hw_stats->rx_octets);\n\tstats->tx_bytes = old_stats->tx_bytes +\n\t\tget_stat64(&hw_stats->tx_octets);\n\n\tstats->rx_errors = old_stats->rx_errors +\n\t\tget_stat64(&hw_stats->rx_errors);\n\tstats->tx_errors = old_stats->tx_errors +\n\t\tget_stat64(&hw_stats->tx_errors) +\n\t\tget_stat64(&hw_stats->tx_mac_errors) +\n\t\tget_stat64(&hw_stats->tx_carrier_sense_errors) +\n\t\tget_stat64(&hw_stats->tx_discards);\n\n\tstats->multicast = old_stats->multicast +\n\t\tget_stat64(&hw_stats->rx_mcast_packets);\n\tstats->collisions = old_stats->collisions +\n\t\tget_stat64(&hw_stats->tx_collisions);\n\n\tstats->rx_length_errors = old_stats->rx_length_errors +\n\t\tget_stat64(&hw_stats->rx_frame_too_long_errors) +\n\t\tget_stat64(&hw_stats->rx_undersize_packets);\n\n\tstats->rx_over_errors = old_stats->rx_over_errors +\n\t\tget_stat64(&hw_stats->rxbds_empty);\n\tstats->rx_frame_errors = old_stats->rx_frame_errors +\n\t\tget_stat64(&hw_stats->rx_align_errors);\n\tstats->tx_aborted_errors = old_stats->tx_aborted_errors +\n\t\tget_stat64(&hw_stats->tx_discards);\n\tstats->tx_carrier_errors = old_stats->tx_carrier_errors +\n\t\tget_stat64(&hw_stats->tx_carrier_sense_errors);\n\n\tstats->rx_crc_errors = old_stats->rx_crc_errors +\n\t\ttg3_calc_crc_errors(tp);\n\n\tstats->rx_missed_errors = old_stats->rx_missed_errors +\n\t\tget_stat64(&hw_stats->rx_discards);\n\n\tstats->rx_dropped = tp->rx_dropped;\n\tstats->tx_dropped = tp->tx_dropped;\n}\n\nstatic int tg3_get_regs_len(struct net_device *dev)\n{\n\treturn TG3_REG_BLK_SIZE;\n}\n\nstatic void tg3_get_regs(struct net_device *dev,\n\t\tstruct ethtool_regs *regs, void *_p)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tregs->version = 0;\n\n\tmemset(_p, 0, TG3_REG_BLK_SIZE);\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\treturn;\n\n\ttg3_full_lock(tp, 0);\n\n\ttg3_dump_legacy_regs(tp, (u32 *)_p);\n\n\ttg3_full_unlock(tp);\n}\n\nstatic int tg3_get_eeprom_len(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\treturn tp->nvram_size;\n}\n\nstatic int tg3_get_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom, u8 *data)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint ret;\n\tu8  *pd;\n\tu32 i, offset, len, b_offset, b_count;\n\t__be32 val;\n\n\tif (tg3_flag(tp, NO_NVRAM))\n\t\treturn -EINVAL;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\treturn -EAGAIN;\n\n\toffset = eeprom->offset;\n\tlen = eeprom->len;\n\teeprom->len = 0;\n\n\teeprom->magic = TG3_EEPROM_MAGIC;\n\n\tif (offset & 3) {\n\t\t/* adjustments to start on required 4 byte boundary */\n\t\tb_offset = offset & 3;\n\t\tb_count = 4 - b_offset;\n\t\tif (b_count > len) {\n\t\t\t/* i.e. offset=1 len=2 */\n\t\t\tb_count = len;\n\t\t}\n\t\tret = tg3_nvram_read_be32(tp, offset-b_offset, &val);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmemcpy(data, ((char *)&val) + b_offset, b_count);\n\t\tlen -= b_count;\n\t\toffset += b_count;\n\t\teeprom->len += b_count;\n\t}\n\n\t/* read bytes up to the last 4 byte boundary */\n\tpd = &data[eeprom->len];\n\tfor (i = 0; i < (len - (len & 3)); i += 4) {\n\t\tret = tg3_nvram_read_be32(tp, offset + i, &val);\n\t\tif (ret) {\n\t\t\teeprom->len += i;\n\t\t\treturn ret;\n\t\t}\n\t\tmemcpy(pd + i, &val, 4);\n\t}\n\teeprom->len += i;\n\n\tif (len & 3) {\n\t\t/* read last bytes not ending on 4 byte boundary */\n\t\tpd = &data[eeprom->len];\n\t\tb_count = len & 3;\n\t\tb_offset = offset + len - b_count;\n\t\tret = tg3_nvram_read_be32(tp, b_offset, &val);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmemcpy(pd, &val, b_count);\n\t\teeprom->len += b_count;\n\t}\n\treturn 0;\n}\n\nstatic int tg3_set_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom, u8 *data)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint ret;\n\tu32 offset, len, b_offset, odd_len;\n\tu8 *buf;\n\t__be32 start, end;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\treturn -EAGAIN;\n\n\tif (tg3_flag(tp, NO_NVRAM) ||\n\t    eeprom->magic != TG3_EEPROM_MAGIC)\n\t\treturn -EINVAL;\n\n\toffset = eeprom->offset;\n\tlen = eeprom->len;\n\n\tif ((b_offset = (offset & 3))) {\n\t\t/* adjustments to start on required 4 byte boundary */\n\t\tret = tg3_nvram_read_be32(tp, offset-b_offset, &start);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tlen += b_offset;\n\t\toffset &= ~3;\n\t\tif (len < 4)\n\t\t\tlen = 4;\n\t}\n\n\todd_len = 0;\n\tif (len & 3) {\n\t\t/* adjustments to end on required 4 byte boundary */\n\t\todd_len = 1;\n\t\tlen = (len + 3) & ~3;\n\t\tret = tg3_nvram_read_be32(tp, offset+len-4, &end);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbuf = data;\n\tif (b_offset || odd_len) {\n\t\tbuf = kmalloc(len, GFP_KERNEL);\n\t\tif (!buf)\n\t\t\treturn -ENOMEM;\n\t\tif (b_offset)\n\t\t\tmemcpy(buf, &start, 4);\n\t\tif (odd_len)\n\t\t\tmemcpy(buf+len-4, &end, 4);\n\t\tmemcpy(buf + b_offset, data, eeprom->len);\n\t}\n\n\tret = tg3_nvram_write_block(tp, offset, len, buf);\n\n\tif (buf != data)\n\t\tkfree(buf);\n\n\treturn ret;\n}\n\nstatic int tg3_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tstruct phy_device *phydev;\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\t\treturn -EAGAIN;\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\t\treturn phy_ethtool_gset(phydev, cmd);\n\t}\n\n\tcmd->supported = (SUPPORTED_Autoneg);\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY))\n\t\tcmd->supported |= (SUPPORTED_1000baseT_Half |\n\t\t\t\t   SUPPORTED_1000baseT_Full);\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES)) {\n\t\tcmd->supported |= (SUPPORTED_100baseT_Half |\n\t\t\t\t  SUPPORTED_100baseT_Full |\n\t\t\t\t  SUPPORTED_10baseT_Half |\n\t\t\t\t  SUPPORTED_10baseT_Full |\n\t\t\t\t  SUPPORTED_TP);\n\t\tcmd->port = PORT_TP;\n\t} else {\n\t\tcmd->supported |= SUPPORTED_FIBRE;\n\t\tcmd->port = PORT_FIBRE;\n\t}\n\n\tcmd->advertising = tp->link_config.advertising;\n\tif (tg3_flag(tp, PAUSE_AUTONEG)) {\n\t\tif (tp->link_config.flowctrl & FLOW_CTRL_RX) {\n\t\t\tif (tp->link_config.flowctrl & FLOW_CTRL_TX) {\n\t\t\t\tcmd->advertising |= ADVERTISED_Pause;\n\t\t\t} else {\n\t\t\t\tcmd->advertising |= ADVERTISED_Pause |\n\t\t\t\t\t\t    ADVERTISED_Asym_Pause;\n\t\t\t}\n\t\t} else if (tp->link_config.flowctrl & FLOW_CTRL_TX) {\n\t\t\tcmd->advertising |= ADVERTISED_Asym_Pause;\n\t\t}\n\t}\n\tif (netif_running(dev) && tp->link_up) {\n\t\tethtool_cmd_speed_set(cmd, tp->link_config.active_speed);\n\t\tcmd->duplex = tp->link_config.active_duplex;\n\t\tcmd->lp_advertising = tp->link_config.rmt_adv;\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES)) {\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_MDIX_STATE)\n\t\t\t\tcmd->eth_tp_mdix = ETH_TP_MDI_X;\n\t\t\telse\n\t\t\t\tcmd->eth_tp_mdix = ETH_TP_MDI;\n\t\t}\n\t} else {\n\t\tethtool_cmd_speed_set(cmd, SPEED_UNKNOWN);\n\t\tcmd->duplex = DUPLEX_UNKNOWN;\n\t\tcmd->eth_tp_mdix = ETH_TP_MDI_INVALID;\n\t}\n\tcmd->phy_address = tp->phy_addr;\n\tcmd->transceiver = XCVR_INTERNAL;\n\tcmd->autoneg = tp->link_config.autoneg;\n\tcmd->maxtxpkt = 0;\n\tcmd->maxrxpkt = 0;\n\treturn 0;\n}\n\nstatic int tg3_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 speed = ethtool_cmd_speed(cmd);\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tstruct phy_device *phydev;\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\t\treturn -EAGAIN;\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\t\treturn phy_ethtool_sset(phydev, cmd);\n\t}\n\n\tif (cmd->autoneg != AUTONEG_ENABLE &&\n\t    cmd->autoneg != AUTONEG_DISABLE)\n\t\treturn -EINVAL;\n\n\tif (cmd->autoneg == AUTONEG_DISABLE &&\n\t    cmd->duplex != DUPLEX_FULL &&\n\t    cmd->duplex != DUPLEX_HALF)\n\t\treturn -EINVAL;\n\n\tif (cmd->autoneg == AUTONEG_ENABLE) {\n\t\tu32 mask = ADVERTISED_Autoneg |\n\t\t\t   ADVERTISED_Pause |\n\t\t\t   ADVERTISED_Asym_Pause;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY))\n\t\t\tmask |= ADVERTISED_1000baseT_Half |\n\t\t\t\tADVERTISED_1000baseT_Full;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\t\tmask |= ADVERTISED_100baseT_Half |\n\t\t\t\tADVERTISED_100baseT_Full |\n\t\t\t\tADVERTISED_10baseT_Half |\n\t\t\t\tADVERTISED_10baseT_Full |\n\t\t\t\tADVERTISED_TP;\n\t\telse\n\t\t\tmask |= ADVERTISED_FIBRE;\n\n\t\tif (cmd->advertising & ~mask)\n\t\t\treturn -EINVAL;\n\n\t\tmask &= (ADVERTISED_1000baseT_Half |\n\t\t\t ADVERTISED_1000baseT_Full |\n\t\t\t ADVERTISED_100baseT_Half |\n\t\t\t ADVERTISED_100baseT_Full |\n\t\t\t ADVERTISED_10baseT_Half |\n\t\t\t ADVERTISED_10baseT_Full);\n\n\t\tcmd->advertising &= mask;\n\t} else {\n\t\tif (tp->phy_flags & TG3_PHYFLG_ANY_SERDES) {\n\t\t\tif (speed != SPEED_1000)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (cmd->duplex != DUPLEX_FULL)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\tif (speed != SPEED_100 &&\n\t\t\t    speed != SPEED_10)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\ttg3_full_lock(tp, 0);\n\n\ttp->link_config.autoneg = cmd->autoneg;\n\tif (cmd->autoneg == AUTONEG_ENABLE) {\n\t\ttp->link_config.advertising = (cmd->advertising |\n\t\t\t\t\t      ADVERTISED_Autoneg);\n\t\ttp->link_config.speed = SPEED_UNKNOWN;\n\t\ttp->link_config.duplex = DUPLEX_UNKNOWN;\n\t} else {\n\t\ttp->link_config.advertising = 0;\n\t\ttp->link_config.speed = speed;\n\t\ttp->link_config.duplex = cmd->duplex;\n\t}\n\n\tif (netif_running(dev))\n\t\ttg3_setup_phy(tp, 1);\n\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic void tg3_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tstrlcpy(info->driver, DRV_MODULE_NAME, sizeof(info->driver));\n\tstrlcpy(info->version, DRV_MODULE_VERSION, sizeof(info->version));\n\tstrlcpy(info->fw_version, tp->fw_ver, sizeof(info->fw_version));\n\tstrlcpy(info->bus_info, pci_name(tp->pdev), sizeof(info->bus_info));\n}\n\nstatic void tg3_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tg3_flag(tp, WOL_CAP) && device_can_wakeup(&tp->pdev->dev))\n\t\twol->supported = WAKE_MAGIC;\n\telse\n\t\twol->supported = 0;\n\twol->wolopts = 0;\n\tif (tg3_flag(tp, WOL_ENABLE) && device_can_wakeup(&tp->pdev->dev))\n\t\twol->wolopts = WAKE_MAGIC;\n\tmemset(&wol->sopass, 0, sizeof(wol->sopass));\n}\n\nstatic int tg3_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tstruct device *dp = &tp->pdev->dev;\n\n\tif (wol->wolopts & ~WAKE_MAGIC)\n\t\treturn -EINVAL;\n\tif ((wol->wolopts & WAKE_MAGIC) &&\n\t    !(tg3_flag(tp, WOL_CAP) && device_can_wakeup(dp)))\n\t\treturn -EINVAL;\n\n\tdevice_set_wakeup_enable(dp, wol->wolopts & WAKE_MAGIC);\n\n\tspin_lock_bh(&tp->lock);\n\tif (device_may_wakeup(dp))\n\t\ttg3_flag_set(tp, WOL_ENABLE);\n\telse\n\t\ttg3_flag_clear(tp, WOL_ENABLE);\n\tspin_unlock_bh(&tp->lock);\n\n\treturn 0;\n}\n\nstatic u32 tg3_get_msglevel(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\treturn tp->msg_enable;\n}\n\nstatic void tg3_set_msglevel(struct net_device *dev, u32 value)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\ttp->msg_enable = value;\n}\n\nstatic int tg3_nway_reset(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint r;\n\n\tif (!netif_running(dev))\n\t\treturn -EAGAIN;\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\treturn -EINVAL;\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\t\treturn -EAGAIN;\n\t\tr = phy_start_aneg(tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]);\n\t} else {\n\t\tu32 bmcr;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\tr = -EINVAL;\n\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\t\tif (!tg3_readphy(tp, MII_BMCR, &bmcr) &&\n\t\t    ((bmcr & BMCR_ANENABLE) ||\n\t\t     (tp->phy_flags & TG3_PHYFLG_PARALLEL_DETECT))) {\n\t\t\ttg3_writephy(tp, MII_BMCR, bmcr | BMCR_ANRESTART |\n\t\t\t\t\t\t   BMCR_ANENABLE);\n\t\t\tr = 0;\n\t\t}\n\t\tspin_unlock_bh(&tp->lock);\n\t}\n\n\treturn r;\n}\n\nstatic void tg3_get_ringparam(struct net_device *dev, struct ethtool_ringparam *ering)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tering->rx_max_pending = tp->rx_std_ring_mask;\n\tif (tg3_flag(tp, JUMBO_RING_ENABLE))\n\t\tering->rx_jumbo_max_pending = tp->rx_jmb_ring_mask;\n\telse\n\t\tering->rx_jumbo_max_pending = 0;\n\n\tering->tx_max_pending = TG3_TX_RING_SIZE - 1;\n\n\tering->rx_pending = tp->rx_pending;\n\tif (tg3_flag(tp, JUMBO_RING_ENABLE))\n\t\tering->rx_jumbo_pending = tp->rx_jumbo_pending;\n\telse\n\t\tering->rx_jumbo_pending = 0;\n\n\tering->tx_pending = tp->napi[0].tx_pending;\n}\n\nstatic int tg3_set_ringparam(struct net_device *dev, struct ethtool_ringparam *ering)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint i, irq_sync = 0, err = 0;\n\n\tif ((ering->rx_pending > tp->rx_std_ring_mask) ||\n\t    (ering->rx_jumbo_pending > tp->rx_jmb_ring_mask) ||\n\t    (ering->tx_pending > TG3_TX_RING_SIZE - 1) ||\n\t    (ering->tx_pending <= MAX_SKB_FRAGS) ||\n\t    (tg3_flag(tp, TSO_BUG) &&\n\t     (ering->tx_pending <= (MAX_SKB_FRAGS * 3))))\n\t\treturn -EINVAL;\n\n\tif (netif_running(dev)) {\n\t\ttg3_phy_stop(tp);\n\t\ttg3_netif_stop(tp);\n\t\tirq_sync = 1;\n\t}\n\n\ttg3_full_lock(tp, irq_sync);\n\n\ttp->rx_pending = ering->rx_pending;\n\n\tif (tg3_flag(tp, MAX_RXPEND_64) &&\n\t    tp->rx_pending > 63)\n\t\ttp->rx_pending = 63;\n\ttp->rx_jumbo_pending = ering->rx_jumbo_pending;\n\n\tfor (i = 0; i < tp->irq_max; i++)\n\t\ttp->napi[i].tx_pending = ering->tx_pending;\n\n\tif (netif_running(dev)) {\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\terr = tg3_restart_hw(tp, 1);\n\t\tif (!err)\n\t\t\ttg3_netif_start(tp);\n\t}\n\n\ttg3_full_unlock(tp);\n\n\tif (irq_sync && !err)\n\t\ttg3_phy_start(tp);\n\n\treturn err;\n}\n\nstatic void tg3_get_pauseparam(struct net_device *dev, struct ethtool_pauseparam *epause)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tepause->autoneg = !!tg3_flag(tp, PAUSE_AUTONEG);\n\n\tif (tp->link_config.flowctrl & FLOW_CTRL_RX)\n\t\tepause->rx_pause = 1;\n\telse\n\t\tepause->rx_pause = 0;\n\n\tif (tp->link_config.flowctrl & FLOW_CTRL_TX)\n\t\tepause->tx_pause = 1;\n\telse\n\t\tepause->tx_pause = 0;\n}\n\nstatic int tg3_set_pauseparam(struct net_device *dev, struct ethtool_pauseparam *epause)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err = 0;\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tu32 newadv;\n\t\tstruct phy_device *phydev;\n\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\t\tif (!(phydev->supported & SUPPORTED_Pause) ||\n\t\t    (!(phydev->supported & SUPPORTED_Asym_Pause) &&\n\t\t     (epause->rx_pause != epause->tx_pause)))\n\t\t\treturn -EINVAL;\n\n\t\ttp->link_config.flowctrl = 0;\n\t\tif (epause->rx_pause) {\n\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_RX;\n\n\t\t\tif (epause->tx_pause) {\n\t\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_TX;\n\t\t\t\tnewadv = ADVERTISED_Pause;\n\t\t\t} else\n\t\t\t\tnewadv = ADVERTISED_Pause |\n\t\t\t\t\t ADVERTISED_Asym_Pause;\n\t\t} else if (epause->tx_pause) {\n\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_TX;\n\t\t\tnewadv = ADVERTISED_Asym_Pause;\n\t\t} else\n\t\t\tnewadv = 0;\n\n\t\tif (epause->autoneg)\n\t\t\ttg3_flag_set(tp, PAUSE_AUTONEG);\n\t\telse\n\t\t\ttg3_flag_clear(tp, PAUSE_AUTONEG);\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_CONNECTED) {\n\t\t\tu32 oldadv = phydev->advertising &\n\t\t\t\t     (ADVERTISED_Pause | ADVERTISED_Asym_Pause);\n\t\t\tif (oldadv != newadv) {\n\t\t\t\tphydev->advertising &=\n\t\t\t\t\t~(ADVERTISED_Pause |\n\t\t\t\t\t  ADVERTISED_Asym_Pause);\n\t\t\t\tphydev->advertising |= newadv;\n\t\t\t\tif (phydev->autoneg) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Always renegotiate the link to\n\t\t\t\t\t * inform our link partner of our\n\t\t\t\t\t * flow control settings, even if the\n\t\t\t\t\t * flow control is forced.  Let\n\t\t\t\t\t * tg3_adjust_link() do the final\n\t\t\t\t\t * flow control setup.\n\t\t\t\t\t */\n\t\t\t\t\treturn phy_start_aneg(phydev);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!epause->autoneg)\n\t\t\t\ttg3_setup_flow_control(tp, 0, 0);\n\t\t} else {\n\t\t\ttp->link_config.advertising &=\n\t\t\t\t\t~(ADVERTISED_Pause |\n\t\t\t\t\t  ADVERTISED_Asym_Pause);\n\t\t\ttp->link_config.advertising |= newadv;\n\t\t}\n\t} else {\n\t\tint irq_sync = 0;\n\n\t\tif (netif_running(dev)) {\n\t\t\ttg3_netif_stop(tp);\n\t\t\tirq_sync = 1;\n\t\t}\n\n\t\ttg3_full_lock(tp, irq_sync);\n\n\t\tif (epause->autoneg)\n\t\t\ttg3_flag_set(tp, PAUSE_AUTONEG);\n\t\telse\n\t\t\ttg3_flag_clear(tp, PAUSE_AUTONEG);\n\t\tif (epause->rx_pause)\n\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_RX;\n\t\telse\n\t\t\ttp->link_config.flowctrl &= ~FLOW_CTRL_RX;\n\t\tif (epause->tx_pause)\n\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_TX;\n\t\telse\n\t\t\ttp->link_config.flowctrl &= ~FLOW_CTRL_TX;\n\n\t\tif (netif_running(dev)) {\n\t\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\t\terr = tg3_restart_hw(tp, 1);\n\t\t\tif (!err)\n\t\t\t\ttg3_netif_start(tp);\n\t\t}\n\n\t\ttg3_full_unlock(tp);\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_get_sset_count(struct net_device *dev, int sset)\n{\n\tswitch (sset) {\n\tcase ETH_SS_TEST:\n\t\treturn TG3_NUM_TEST;\n\tcase ETH_SS_STATS:\n\t\treturn TG3_NUM_STATS;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int tg3_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info,\n\t\t\t u32 *rules __always_unused)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (!tg3_flag(tp, SUPPORT_MSIX))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (info->cmd) {\n\tcase ETHTOOL_GRXRINGS:\n\t\tif (netif_running(tp->dev))\n\t\t\tinfo->data = tp->rxq_cnt;\n\t\telse {\n\t\t\tinfo->data = num_online_cpus();\n\t\t\tif (info->data > TG3_RSS_MAX_NUM_QS)\n\t\t\t\tinfo->data = TG3_RSS_MAX_NUM_QS;\n\t\t}\n\n\t\t/* The first interrupt vector only\n\t\t * handles link interrupts.\n\t\t */\n\t\tinfo->data -= 1;\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic u32 tg3_get_rxfh_indir_size(struct net_device *dev)\n{\n\tu32 size = 0;\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tg3_flag(tp, SUPPORT_MSIX))\n\t\tsize = TG3_RSS_INDIR_TBL_SIZE;\n\n\treturn size;\n}\n\nstatic int tg3_get_rxfh_indir(struct net_device *dev, u32 *indir)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < TG3_RSS_INDIR_TBL_SIZE; i++)\n\t\tindir[i] = tp->rss_ind_tbl[i];\n\n\treturn 0;\n}\n\nstatic int tg3_set_rxfh_indir(struct net_device *dev, const u32 *indir)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tsize_t i;\n\n\tfor (i = 0; i < TG3_RSS_INDIR_TBL_SIZE; i++)\n\t\ttp->rss_ind_tbl[i] = indir[i];\n\n\tif (!netif_running(dev) || !tg3_flag(tp, ENABLE_RSS))\n\t\treturn 0;\n\n\t/* It is legal to write the indirection\n\t * table while the device is running.\n\t */\n\ttg3_full_lock(tp, 0);\n\ttg3_rss_write_indir_tbl(tp);\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic void tg3_get_channels(struct net_device *dev,\n\t\t\t     struct ethtool_channels *channel)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 deflt_qs = netif_get_num_default_rss_queues();\n\n\tchannel->max_rx = tp->rxq_max;\n\tchannel->max_tx = tp->txq_max;\n\n\tif (netif_running(dev)) {\n\t\tchannel->rx_count = tp->rxq_cnt;\n\t\tchannel->tx_count = tp->txq_cnt;\n\t} else {\n\t\tif (tp->rxq_req)\n\t\t\tchannel->rx_count = tp->rxq_req;\n\t\telse\n\t\t\tchannel->rx_count = min(deflt_qs, tp->rxq_max);\n\n\t\tif (tp->txq_req)\n\t\t\tchannel->tx_count = tp->txq_req;\n\t\telse\n\t\t\tchannel->tx_count = min(deflt_qs, tp->txq_max);\n\t}\n}\n\nstatic int tg3_set_channels(struct net_device *dev,\n\t\t\t    struct ethtool_channels *channel)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (!tg3_flag(tp, SUPPORT_MSIX))\n\t\treturn -EOPNOTSUPP;\n\n\tif (channel->rx_count > tp->rxq_max ||\n\t    channel->tx_count > tp->txq_max)\n\t\treturn -EINVAL;\n\n\ttp->rxq_req = channel->rx_count;\n\ttp->txq_req = channel->tx_count;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\ttg3_stop(tp);\n\n\ttg3_carrier_off(tp);\n\n\ttg3_start(tp, true, false, false);\n\n\treturn 0;\n}\n\nstatic void tg3_get_strings(struct net_device *dev, u32 stringset, u8 *buf)\n{\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tmemcpy(buf, &ethtool_stats_keys, sizeof(ethtool_stats_keys));\n\t\tbreak;\n\tcase ETH_SS_TEST:\n\t\tmemcpy(buf, &ethtool_test_keys, sizeof(ethtool_test_keys));\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\t/* we need a WARN() */\n\t\tbreak;\n\t}\n}\n\nstatic int tg3_set_phys_id(struct net_device *dev,\n\t\t\t    enum ethtool_phys_id_state state)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (!netif_running(tp->dev))\n\t\treturn -EAGAIN;\n\n\tswitch (state) {\n\tcase ETHTOOL_ID_ACTIVE:\n\t\treturn 1;\t/* cycle on/off once per second */\n\n\tcase ETHTOOL_ID_ON:\n\t\ttw32(MAC_LED_CTRL, LED_CTRL_LNKLED_OVERRIDE |\n\t\t     LED_CTRL_1000MBPS_ON |\n\t\t     LED_CTRL_100MBPS_ON |\n\t\t     LED_CTRL_10MBPS_ON |\n\t\t     LED_CTRL_TRAFFIC_OVERRIDE |\n\t\t     LED_CTRL_TRAFFIC_BLINK |\n\t\t     LED_CTRL_TRAFFIC_LED);\n\t\tbreak;\n\n\tcase ETHTOOL_ID_OFF:\n\t\ttw32(MAC_LED_CTRL, LED_CTRL_LNKLED_OVERRIDE |\n\t\t     LED_CTRL_TRAFFIC_OVERRIDE);\n\t\tbreak;\n\n\tcase ETHTOOL_ID_INACTIVE:\n\t\ttw32(MAC_LED_CTRL, tp->led_ctrl);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t   struct ethtool_stats *estats, u64 *tmp_stats)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tp->hw_stats)\n\t\ttg3_get_estats(tp, (struct tg3_ethtool_stats *)tmp_stats);\n\telse\n\t\tmemset(tmp_stats, 0, sizeof(struct tg3_ethtool_stats));\n}\n\nstatic __be32 *tg3_vpd_readblock(struct tg3 *tp, u32 *vpdlen)\n{\n\tint i;\n\t__be32 *buf;\n\tu32 offset = 0, len = 0;\n\tu32 magic, val;\n\n\tif (tg3_flag(tp, NO_NVRAM) || tg3_nvram_read(tp, 0, &magic))\n\t\treturn NULL;\n\n\tif (magic == TG3_EEPROM_MAGIC) {\n\t\tfor (offset = TG3_NVM_DIR_START;\n\t\t     offset < TG3_NVM_DIR_END;\n\t\t     offset += TG3_NVM_DIRENT_SIZE) {\n\t\t\tif (tg3_nvram_read(tp, offset, &val))\n\t\t\t\treturn NULL;\n\n\t\t\tif ((val >> TG3_NVM_DIRTYPE_SHIFT) ==\n\t\t\t    TG3_NVM_DIRTYPE_EXTVPD)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (offset != TG3_NVM_DIR_END) {\n\t\t\tlen = (val & TG3_NVM_DIRTYPE_LENMSK) * 4;\n\t\t\tif (tg3_nvram_read(tp, offset + 4, &offset))\n\t\t\t\treturn NULL;\n\n\t\t\toffset = tg3_nvram_logical_addr(tp, offset);\n\t\t}\n\t}\n\n\tif (!offset || !len) {\n\t\toffset = TG3_NVM_VPD_OFF;\n\t\tlen = TG3_NVM_VPD_LEN;\n\t}\n\n\tbuf = kmalloc(len, GFP_KERNEL);\n\tif (buf == NULL)\n\t\treturn NULL;\n\n\tif (magic == TG3_EEPROM_MAGIC) {\n\t\tfor (i = 0; i < len; i += 4) {\n\t\t\t/* The data is in little-endian format in NVRAM.\n\t\t\t * Use the big-endian read routines to preserve\n\t\t\t * the byte order as it exists in NVRAM.\n\t\t\t */\n\t\t\tif (tg3_nvram_read_be32(tp, offset + i, &buf[i/4]))\n\t\t\t\tgoto error;\n\t\t}\n\t} else {\n\t\tu8 *ptr;\n\t\tssize_t cnt;\n\t\tunsigned int pos = 0;\n\n\t\tptr = (u8 *)&buf[0];\n\t\tfor (i = 0; pos < len && i < 3; i++, pos += cnt, ptr += cnt) {\n\t\t\tcnt = pci_read_vpd(tp->pdev, pos,\n\t\t\t\t\t   len - pos, ptr);\n\t\t\tif (cnt == -ETIMEDOUT || cnt == -EINTR)\n\t\t\t\tcnt = 0;\n\t\t\telse if (cnt < 0)\n\t\t\t\tgoto error;\n\t\t}\n\t\tif (pos != len)\n\t\t\tgoto error;\n\t}\n\n\t*vpdlen = len;\n\n\treturn buf;\n\nerror:\n\tkfree(buf);\n\treturn NULL;\n}\n\n#define NVRAM_TEST_SIZE 0x100\n#define NVRAM_SELFBOOT_FORMAT1_0_SIZE\t0x14\n#define NVRAM_SELFBOOT_FORMAT1_2_SIZE\t0x18\n#define NVRAM_SELFBOOT_FORMAT1_3_SIZE\t0x1c\n#define NVRAM_SELFBOOT_FORMAT1_4_SIZE\t0x20\n#define NVRAM_SELFBOOT_FORMAT1_5_SIZE\t0x24\n#define NVRAM_SELFBOOT_FORMAT1_6_SIZE\t0x50\n#define NVRAM_SELFBOOT_HW_SIZE 0x20\n#define NVRAM_SELFBOOT_DATA_SIZE 0x1c\n\nstatic int tg3_test_nvram(struct tg3 *tp)\n{\n\tu32 csum, magic, len;\n\t__be32 *buf;\n\tint i, j, k, err = 0, size;\n\n\tif (tg3_flag(tp, NO_NVRAM))\n\t\treturn 0;\n\n\tif (tg3_nvram_read(tp, 0, &magic) != 0)\n\t\treturn -EIO;\n\n\tif (magic == TG3_EEPROM_MAGIC)\n\t\tsize = NVRAM_TEST_SIZE;\n\telse if ((magic & TG3_EEPROM_MAGIC_FW_MSK) == TG3_EEPROM_MAGIC_FW) {\n\t\tif ((magic & TG3_EEPROM_SB_FORMAT_MASK) ==\n\t\t    TG3_EEPROM_SB_FORMAT_1) {\n\t\t\tswitch (magic & TG3_EEPROM_SB_REVISION_MASK) {\n\t\t\tcase TG3_EEPROM_SB_REVISION_0:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_0_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_2:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_2_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_3:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_3_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_4:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_4_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_5:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_5_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_6:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_6_SIZE;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t} else\n\t\t\treturn 0;\n\t} else if ((magic & TG3_EEPROM_MAGIC_HW_MSK) == TG3_EEPROM_MAGIC_HW)\n\t\tsize = NVRAM_SELFBOOT_HW_SIZE;\n\telse\n\t\treturn -EIO;\n\n\tbuf = kmalloc(size, GFP_KERNEL);\n\tif (buf == NULL)\n\t\treturn -ENOMEM;\n\n\terr = -EIO;\n\tfor (i = 0, j = 0; i < size; i += 4, j++) {\n\t\terr = tg3_nvram_read_be32(tp, i, &buf[j]);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tif (i < size)\n\t\tgoto out;\n\n\t/* Selfboot format */\n\tmagic = be32_to_cpu(buf[0]);\n\tif ((magic & TG3_EEPROM_MAGIC_FW_MSK) ==\n\t    TG3_EEPROM_MAGIC_FW) {\n\t\tu8 *buf8 = (u8 *) buf, csum8 = 0;\n\n\t\tif ((magic & TG3_EEPROM_SB_REVISION_MASK) ==\n\t\t    TG3_EEPROM_SB_REVISION_2) {\n\t\t\t/* For rev 2, the csum doesn't include the MBA. */\n\t\t\tfor (i = 0; i < TG3_EEPROM_SB_F1R2_MBA_OFF; i++)\n\t\t\t\tcsum8 += buf8[i];\n\t\t\tfor (i = TG3_EEPROM_SB_F1R2_MBA_OFF + 4; i < size; i++)\n\t\t\t\tcsum8 += buf8[i];\n\t\t} else {\n\t\t\tfor (i = 0; i < size; i++)\n\t\t\t\tcsum8 += buf8[i];\n\t\t}\n\n\t\tif (csum8 == 0) {\n\t\t\terr = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tif ((magic & TG3_EEPROM_MAGIC_HW_MSK) ==\n\t    TG3_EEPROM_MAGIC_HW) {\n\t\tu8 data[NVRAM_SELFBOOT_DATA_SIZE];\n\t\tu8 parity[NVRAM_SELFBOOT_DATA_SIZE];\n\t\tu8 *buf8 = (u8 *) buf;\n\n\t\t/* Separate the parity bits and the data bytes.  */\n\t\tfor (i = 0, j = 0, k = 0; i < NVRAM_SELFBOOT_HW_SIZE; i++) {\n\t\t\tif ((i == 0) || (i == 8)) {\n\t\t\t\tint l;\n\t\t\t\tu8 msk;\n\n\t\t\t\tfor (l = 0, msk = 0x80; l < 7; l++, msk >>= 1)\n\t\t\t\t\tparity[k++] = buf8[i] & msk;\n\t\t\t\ti++;\n\t\t\t} else if (i == 16) {\n\t\t\t\tint l;\n\t\t\t\tu8 msk;\n\n\t\t\t\tfor (l = 0, msk = 0x20; l < 6; l++, msk >>= 1)\n\t\t\t\t\tparity[k++] = buf8[i] & msk;\n\t\t\t\ti++;\n\n\t\t\t\tfor (l = 0, msk = 0x80; l < 8; l++, msk >>= 1)\n\t\t\t\t\tparity[k++] = buf8[i] & msk;\n\t\t\t\ti++;\n\t\t\t}\n\t\t\tdata[j++] = buf8[i];\n\t\t}\n\n\t\terr = -EIO;\n\t\tfor (i = 0; i < NVRAM_SELFBOOT_DATA_SIZE; i++) {\n\t\t\tu8 hw8 = hweight8(data[i]);\n\n\t\t\tif ((hw8 & 0x1) && parity[i])\n\t\t\t\tgoto out;\n\t\t\telse if (!(hw8 & 0x1) && !parity[i])\n\t\t\t\tgoto out;\n\t\t}\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\terr = -EIO;\n\n\t/* Bootstrap checksum at offset 0x10 */\n\tcsum = calc_crc((unsigned char *) buf, 0x10);\n\tif (csum != le32_to_cpu(buf[0x10/4]))\n\t\tgoto out;\n\n\t/* Manufacturing block starts at offset 0x74, checksum at 0xfc */\n\tcsum = calc_crc((unsigned char *) &buf[0x74/4], 0x88);\n\tif (csum != le32_to_cpu(buf[0xfc/4]))\n\t\tgoto out;\n\n\tkfree(buf);\n\n\tbuf = tg3_vpd_readblock(tp, &len);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\ti = pci_vpd_find_tag((u8 *)buf, 0, len, PCI_VPD_LRDT_RO_DATA);\n\tif (i > 0) {\n\t\tj = pci_vpd_lrdt_size(&((u8 *)buf)[i]);\n\t\tif (j < 0)\n\t\t\tgoto out;\n\n\t\tif (i + PCI_VPD_LRDT_TAG_SIZE + j > len)\n\t\t\tgoto out;\n\n\t\ti += PCI_VPD_LRDT_TAG_SIZE;\n\t\tj = pci_vpd_find_info_keyword((u8 *)buf, i, j,\n\t\t\t\t\t      PCI_VPD_RO_KEYWORD_CHKSUM);\n\t\tif (j > 0) {\n\t\t\tu8 csum8 = 0;\n\n\t\t\tj += PCI_VPD_INFO_FLD_HDR_SIZE;\n\n\t\t\tfor (i = 0; i <= j; i++)\n\t\t\t\tcsum8 += ((u8 *)buf)[i];\n\n\t\t\tif (csum8)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\terr = 0;\n\nout:\n\tkfree(buf);\n\treturn err;\n}\n\n#define TG3_SERDES_TIMEOUT_SEC\t2\n#define TG3_COPPER_TIMEOUT_SEC\t6\n\nstatic int tg3_test_link(struct tg3 *tp)\n{\n\tint i, max;\n\n\tif (!netif_running(tp->dev))\n\t\treturn -ENODEV;\n\n\tif (tp->phy_flags & TG3_PHYFLG_ANY_SERDES)\n\t\tmax = TG3_SERDES_TIMEOUT_SEC;\n\telse\n\t\tmax = TG3_COPPER_TIMEOUT_SEC;\n\n\tfor (i = 0; i < max; i++) {\n\t\tif (tp->link_up)\n\t\t\treturn 0;\n\n\t\tif (msleep_interruptible(1000))\n\t\t\tbreak;\n\t}\n\n\treturn -EIO;\n}\n\n/* Only test the commonly used registers */\nstatic int tg3_test_registers(struct tg3 *tp)\n{\n\tint i, is_5705, is_5750;\n\tu32 offset, read_mask, write_mask, val, save_val, read_val;\n\tstatic struct {\n\t\tu16 offset;\n\t\tu16 flags;\n#define TG3_FL_5705\t0x1\n#define TG3_FL_NOT_5705\t0x2\n#define TG3_FL_NOT_5788\t0x4\n#define TG3_FL_NOT_5750\t0x8\n\t\tu32 read_mask;\n\t\tu32 write_mask;\n\t} reg_tbl[] = {\n\t\t/* MAC Control Registers */\n\t\t{ MAC_MODE, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x00ef6f8c },\n\t\t{ MAC_MODE, TG3_FL_5705,\n\t\t\t0x00000000, 0x01ef6b8c },\n\t\t{ MAC_STATUS, TG3_FL_NOT_5705,\n\t\t\t0x03800107, 0x00000000 },\n\t\t{ MAC_STATUS, TG3_FL_5705,\n\t\t\t0x03800100, 0x00000000 },\n\t\t{ MAC_ADDR_0_HIGH, 0x0000,\n\t\t\t0x00000000, 0x0000ffff },\n\t\t{ MAC_ADDR_0_LOW, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ MAC_RX_MTU_SIZE, 0x0000,\n\t\t\t0x00000000, 0x0000ffff },\n\t\t{ MAC_TX_MODE, 0x0000,\n\t\t\t0x00000000, 0x00000070 },\n\t\t{ MAC_TX_LENGTHS, 0x0000,\n\t\t\t0x00000000, 0x00003fff },\n\t\t{ MAC_RX_MODE, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x000007fc },\n\t\t{ MAC_RX_MODE, TG3_FL_5705,\n\t\t\t0x00000000, 0x000007dc },\n\t\t{ MAC_HASH_REG_0, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ MAC_HASH_REG_1, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ MAC_HASH_REG_2, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ MAC_HASH_REG_3, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\n\t\t/* Receive Data and Receive BD Initiator Control Registers. */\n\t\t{ RCVDBDI_JUMBO_BD+0, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_JUMBO_BD+4, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_JUMBO_BD+8, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x00000003 },\n\t\t{ RCVDBDI_JUMBO_BD+0xc, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_STD_BD+0, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_STD_BD+4, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_STD_BD+8, 0x0000,\n\t\t\t0x00000000, 0xffff0002 },\n\t\t{ RCVDBDI_STD_BD+0xc, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\n\t\t/* Receive BD Initiator Control Registers. */\n\t\t{ RCVBDI_STD_THRESH, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVBDI_STD_THRESH, TG3_FL_5705,\n\t\t\t0x00000000, 0x000003ff },\n\t\t{ RCVBDI_JUMBO_THRESH, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\n\t\t/* Host Coalescing Control Registers. */\n\t\t{ HOSTCC_MODE, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x00000004 },\n\t\t{ HOSTCC_MODE, TG3_FL_5705,\n\t\t\t0x00000000, 0x000000f6 },\n\t\t{ HOSTCC_RXCOL_TICKS, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_RXCOL_TICKS, TG3_FL_5705,\n\t\t\t0x00000000, 0x000003ff },\n\t\t{ HOSTCC_TXCOL_TICKS, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_TXCOL_TICKS, TG3_FL_5705,\n\t\t\t0x00000000, 0x000003ff },\n\t\t{ HOSTCC_RXMAX_FRAMES, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_RXMAX_FRAMES, TG3_FL_5705 | TG3_FL_NOT_5788,\n\t\t\t0x00000000, 0x000000ff },\n\t\t{ HOSTCC_TXMAX_FRAMES, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_TXMAX_FRAMES, TG3_FL_5705 | TG3_FL_NOT_5788,\n\t\t\t0x00000000, 0x000000ff },\n\t\t{ HOSTCC_RXCOAL_TICK_INT, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_TXCOAL_TICK_INT, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_RXCOAL_MAXF_INT, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_RXCOAL_MAXF_INT, TG3_FL_5705 | TG3_FL_NOT_5788,\n\t\t\t0x00000000, 0x000000ff },\n\t\t{ HOSTCC_TXCOAL_MAXF_INT, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_TXCOAL_MAXF_INT, TG3_FL_5705 | TG3_FL_NOT_5788,\n\t\t\t0x00000000, 0x000000ff },\n\t\t{ HOSTCC_STAT_COAL_TICKS, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATS_BLK_HOST_ADDR, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATS_BLK_HOST_ADDR+4, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATUS_BLK_HOST_ADDR, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATUS_BLK_HOST_ADDR+4, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATS_BLK_NIC_ADDR, 0x0000,\n\t\t\t0xffffffff, 0x00000000 },\n\t\t{ HOSTCC_STATUS_BLK_NIC_ADDR, 0x0000,\n\t\t\t0xffffffff, 0x00000000 },\n\n\t\t/* Buffer Manager Control Registers. */\n\t\t{ BUFMGR_MB_POOL_ADDR, TG3_FL_NOT_5750,\n\t\t\t0x00000000, 0x007fff80 },\n\t\t{ BUFMGR_MB_POOL_SIZE, TG3_FL_NOT_5750,\n\t\t\t0x00000000, 0x007fffff },\n\t\t{ BUFMGR_MB_RDMA_LOW_WATER, 0x0000,\n\t\t\t0x00000000, 0x0000003f },\n\t\t{ BUFMGR_MB_MACRX_LOW_WATER, 0x0000,\n\t\t\t0x00000000, 0x000001ff },\n\t\t{ BUFMGR_MB_HIGH_WATER, 0x0000,\n\t\t\t0x00000000, 0x000001ff },\n\t\t{ BUFMGR_DMA_DESC_POOL_ADDR, TG3_FL_NOT_5705,\n\t\t\t0xffffffff, 0x00000000 },\n\t\t{ BUFMGR_DMA_DESC_POOL_SIZE, TG3_FL_NOT_5705,\n\t\t\t0xffffffff, 0x00000000 },\n\n\t\t/* Mailbox Registers */\n\t\t{ GRCMBOX_RCVSTD_PROD_IDX+4, 0x0000,\n\t\t\t0x00000000, 0x000001ff },\n\t\t{ GRCMBOX_RCVJUMBO_PROD_IDX+4, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x000001ff },\n\t\t{ GRCMBOX_RCVRET_CON_IDX_0+4, 0x0000,\n\t\t\t0x00000000, 0x000007ff },\n\t\t{ GRCMBOX_SNDHOST_PROD_IDX_0+4, 0x0000,\n\t\t\t0x00000000, 0x000001ff },\n\n\t\t{ 0xffff, 0x0000, 0x00000000, 0x00000000 },\n\t};\n\n\tis_5705 = is_5750 = 0;\n\tif (tg3_flag(tp, 5705_PLUS)) {\n\t\tis_5705 = 1;\n\t\tif (tg3_flag(tp, 5750_PLUS))\n\t\t\tis_5750 = 1;\n\t}\n\n\tfor (i = 0; reg_tbl[i].offset != 0xffff; i++) {\n\t\tif (is_5705 && (reg_tbl[i].flags & TG3_FL_NOT_5705))\n\t\t\tcontinue;\n\n\t\tif (!is_5705 && (reg_tbl[i].flags & TG3_FL_5705))\n\t\t\tcontinue;\n\n\t\tif (tg3_flag(tp, IS_5788) &&\n\t\t    (reg_tbl[i].flags & TG3_FL_NOT_5788))\n\t\t\tcontinue;\n\n\t\tif (is_5750 && (reg_tbl[i].flags & TG3_FL_NOT_5750))\n\t\t\tcontinue;\n\n\t\toffset = (u32) reg_tbl[i].offset;\n\t\tread_mask = reg_tbl[i].read_mask;\n\t\twrite_mask = reg_tbl[i].write_mask;\n\n\t\t/* Save the original register content */\n\t\tsave_val = tr32(offset);\n\n\t\t/* Determine the read-only value. */\n\t\tread_val = save_val & read_mask;\n\n\t\t/* Write zero to the register, then make sure the read-only bits\n\t\t * are not changed and the read/write bits are all zeros.\n\t\t */\n\t\ttw32(offset, 0);\n\n\t\tval = tr32(offset);\n\n\t\t/* Test the read-only and read/write bits. */\n\t\tif (((val & read_mask) != read_val) || (val & write_mask))\n\t\t\tgoto out;\n\n\t\t/* Write ones to all the bits defined by RdMask and WrMask, then\n\t\t * make sure the read-only bits are not changed and the\n\t\t * read/write bits are all ones.\n\t\t */\n\t\ttw32(offset, read_mask | write_mask);\n\n\t\tval = tr32(offset);\n\n\t\t/* Test the read-only bits. */\n\t\tif ((val & read_mask) != read_val)\n\t\t\tgoto out;\n\n\t\t/* Test the read/write bits. */\n\t\tif ((val & write_mask) != write_mask)\n\t\t\tgoto out;\n\n\t\ttw32(offset, save_val);\n\t}\n\n\treturn 0;\n\nout:\n\tif (netif_msg_hw(tp))\n\t\tnetdev_err(tp->dev,\n\t\t\t   \"Register test failed at offset %x\\n\", offset);\n\ttw32(offset, save_val);\n\treturn -EIO;\n}\n\nstatic int tg3_do_mem_test(struct tg3 *tp, u32 offset, u32 len)\n{\n\tstatic const u32 test_pattern[] = { 0x00000000, 0xffffffff, 0xaa55a55a };\n\tint i;\n\tu32 j;\n\n\tfor (i = 0; i < ARRAY_SIZE(test_pattern); i++) {\n\t\tfor (j = 0; j < len; j += 4) {\n\t\t\tu32 val;\n\n\t\t\ttg3_write_mem(tp, offset + j, test_pattern[i]);\n\t\t\ttg3_read_mem(tp, offset + j, &val);\n\t\t\tif (val != test_pattern[i])\n\t\t\t\treturn -EIO;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int tg3_test_memory(struct tg3 *tp)\n{\n\tstatic struct mem_entry {\n\t\tu32 offset;\n\t\tu32 len;\n\t} mem_tbl_570x[] = {\n\t\t{ 0x00000000, 0x00b50},\n\t\t{ 0x00002000, 0x1c000},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_5705[] = {\n\t\t{ 0x00000100, 0x0000c},\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00004000, 0x00800},\n\t\t{ 0x00006000, 0x01000},\n\t\t{ 0x00008000, 0x02000},\n\t\t{ 0x00010000, 0x0e000},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_5755[] = {\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00004000, 0x00800},\n\t\t{ 0x00006000, 0x00800},\n\t\t{ 0x00008000, 0x02000},\n\t\t{ 0x00010000, 0x0c000},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_5906[] = {\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00004000, 0x00400},\n\t\t{ 0x00006000, 0x00400},\n\t\t{ 0x00008000, 0x01000},\n\t\t{ 0x00010000, 0x01000},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_5717[] = {\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00010000, 0x0a000},\n\t\t{ 0x00020000, 0x13c00},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_57765[] = {\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00004000, 0x00800},\n\t\t{ 0x00006000, 0x09800},\n\t\t{ 0x00010000, 0x0a000},\n\t\t{ 0xffffffff, 0x00000}\n\t};\n\tstruct mem_entry *mem_tbl;\n\tint err = 0;\n\tint i;\n\n\tif (tg3_flag(tp, 5717_PLUS))\n\t\tmem_tbl = mem_tbl_5717;\n\telse if (tg3_flag(tp, 57765_CLASS) ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tmem_tbl = mem_tbl_57765;\n\telse if (tg3_flag(tp, 5755_PLUS))\n\t\tmem_tbl = mem_tbl_5755;\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\tmem_tbl = mem_tbl_5906;\n\telse if (tg3_flag(tp, 5705_PLUS))\n\t\tmem_tbl = mem_tbl_5705;\n\telse\n\t\tmem_tbl = mem_tbl_570x;\n\n\tfor (i = 0; mem_tbl[i].offset != 0xffffffff; i++) {\n\t\terr = tg3_do_mem_test(tp, mem_tbl[i].offset, mem_tbl[i].len);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n#define TG3_TSO_MSS\t\t500\n\n#define TG3_TSO_IP_HDR_LEN\t20\n#define TG3_TSO_TCP_HDR_LEN\t20\n#define TG3_TSO_TCP_OPT_LEN\t12\n\nstatic const u8 tg3_tso_header[] = {\n0x08, 0x00,\n0x45, 0x00, 0x00, 0x00,\n0x00, 0x00, 0x40, 0x00,\n0x40, 0x06, 0x00, 0x00,\n0x0a, 0x00, 0x00, 0x01,\n0x0a, 0x00, 0x00, 0x02,\n0x0d, 0x00, 0xe0, 0x00,\n0x00, 0x00, 0x01, 0x00,\n0x00, 0x00, 0x02, 0x00,\n0x80, 0x10, 0x10, 0x00,\n0x14, 0x09, 0x00, 0x00,\n0x01, 0x01, 0x08, 0x0a,\n0x11, 0x11, 0x11, 0x11,\n0x11, 0x11, 0x11, 0x11,\n};\n\nstatic int tg3_run_loopback(struct tg3 *tp, u32 pktsz, bool tso_loopback)\n{\n\tu32 rx_start_idx, rx_idx, tx_idx, opaque_key;\n\tu32 base_flags = 0, mss = 0, desc_idx, coal_now, data_off, val;\n\tu32 budget;\n\tstruct sk_buff *skb;\n\tu8 *tx_data, *rx_data;\n\tdma_addr_t map;\n\tint num_pkts, tx_len, rx_len, i, err;\n\tstruct tg3_rx_buffer_desc *desc;\n\tstruct tg3_napi *tnapi, *rnapi;\n\tstruct tg3_rx_prodring_set *tpr = &tp->napi[0].prodring;\n\n\ttnapi = &tp->napi[0];\n\trnapi = &tp->napi[0];\n\tif (tp->irq_cnt > 1) {\n\t\tif (tg3_flag(tp, ENABLE_RSS))\n\t\t\trnapi = &tp->napi[1];\n\t\tif (tg3_flag(tp, ENABLE_TSS))\n\t\t\ttnapi = &tp->napi[1];\n\t}\n\tcoal_now = tnapi->coal_now | rnapi->coal_now;\n\n\terr = -EIO;\n\n\ttx_len = pktsz;\n\tskb = netdev_alloc_skb(tp->dev, tx_len);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\ttx_data = skb_put(skb, tx_len);\n\tmemcpy(tx_data, tp->dev->dev_addr, 6);\n\tmemset(tx_data + 6, 0x0, 8);\n\n\ttw32(MAC_RX_MTU_SIZE, tx_len + ETH_FCS_LEN);\n\n\tif (tso_loopback) {\n\t\tstruct iphdr *iph = (struct iphdr *)&tx_data[ETH_HLEN];\n\n\t\tu32 hdr_len = TG3_TSO_IP_HDR_LEN + TG3_TSO_TCP_HDR_LEN +\n\t\t\t      TG3_TSO_TCP_OPT_LEN;\n\n\t\tmemcpy(tx_data + ETH_ALEN * 2, tg3_tso_header,\n\t\t       sizeof(tg3_tso_header));\n\t\tmss = TG3_TSO_MSS;\n\n\t\tval = tx_len - ETH_ALEN * 2 - sizeof(tg3_tso_header);\n\t\tnum_pkts = DIV_ROUND_UP(val, TG3_TSO_MSS);\n\n\t\t/* Set the total length field in the IP header */\n\t\tiph->tot_len = htons((u16)(mss + hdr_len));\n\n\t\tbase_flags = (TXD_FLAG_CPU_PRE_DMA |\n\t\t\t      TXD_FLAG_CPU_POST_DMA);\n\n\t\tif (tg3_flag(tp, HW_TSO_1) ||\n\t\t    tg3_flag(tp, HW_TSO_2) ||\n\t\t    tg3_flag(tp, HW_TSO_3)) {\n\t\t\tstruct tcphdr *th;\n\t\t\tval = ETH_HLEN + TG3_TSO_IP_HDR_LEN;\n\t\t\tth = (struct tcphdr *)&tx_data[val];\n\t\t\tth->check = 0;\n\t\t} else\n\t\t\tbase_flags |= TXD_FLAG_TCPUDP_CSUM;\n\n\t\tif (tg3_flag(tp, HW_TSO_3)) {\n\t\t\tmss |= (hdr_len & 0xc) << 12;\n\t\t\tif (hdr_len & 0x10)\n\t\t\t\tbase_flags |= 0x00000010;\n\t\t\tbase_flags |= (hdr_len & 0x3e0) << 5;\n\t\t} else if (tg3_flag(tp, HW_TSO_2))\n\t\t\tmss |= hdr_len << 9;\n\t\telse if (tg3_flag(tp, HW_TSO_1) ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\t\tmss |= (TG3_TSO_TCP_OPT_LEN << 9);\n\t\t} else {\n\t\t\tbase_flags |= (TG3_TSO_TCP_OPT_LEN << 10);\n\t\t}\n\n\t\tdata_off = ETH_ALEN * 2 + sizeof(tg3_tso_header);\n\t} else {\n\t\tnum_pkts = 1;\n\t\tdata_off = ETH_HLEN;\n\n\t\tif (tg3_flag(tp, USE_JUMBO_BDFLAG) &&\n\t\t    tx_len > VLAN_ETH_FRAME_LEN)\n\t\t\tbase_flags |= TXD_FLAG_JMB_PKT;\n\t}\n\n\tfor (i = data_off; i < tx_len; i++)\n\t\ttx_data[i] = (u8) (i & 0xff);\n\n\tmap = pci_map_single(tp->pdev, skb->data, tx_len, PCI_DMA_TODEVICE);\n\tif (pci_dma_mapping_error(tp->pdev, map)) {\n\t\tdev_kfree_skb(skb);\n\t\treturn -EIO;\n\t}\n\n\tval = tnapi->tx_prod;\n\ttnapi->tx_buffers[val].skb = skb;\n\tdma_unmap_addr_set(&tnapi->tx_buffers[val], mapping, map);\n\n\ttw32_f(HOSTCC_MODE, tp->coalesce_mode | HOSTCC_MODE_ENABLE |\n\t       rnapi->coal_now);\n\n\tudelay(10);\n\n\trx_start_idx = rnapi->hw_status->idx[0].rx_producer;\n\n\tbudget = tg3_tx_avail(tnapi);\n\tif (tg3_tx_frag_set(tnapi, &val, &budget, map, tx_len,\n\t\t\t    base_flags | TXD_FLAG_END, mss, 0)) {\n\t\ttnapi->tx_buffers[val].skb = NULL;\n\t\tdev_kfree_skb(skb);\n\t\treturn -EIO;\n\t}\n\n\ttnapi->tx_prod++;\n\n\t/* Sync BD data before updating mailbox */\n\twmb();\n\n\ttw32_tx_mbox(tnapi->prodmbox, tnapi->tx_prod);\n\ttr32_mailbox(tnapi->prodmbox);\n\n\tudelay(10);\n\n\t/* 350 usec to allow enough time on some 10/100 Mbps devices.  */\n\tfor (i = 0; i < 35; i++) {\n\t\ttw32_f(HOSTCC_MODE, tp->coalesce_mode | HOSTCC_MODE_ENABLE |\n\t\t       coal_now);\n\n\t\tudelay(10);\n\n\t\ttx_idx = tnapi->hw_status->idx[0].tx_consumer;\n\t\trx_idx = rnapi->hw_status->idx[0].rx_producer;\n\t\tif ((tx_idx == tnapi->tx_prod) &&\n\t\t    (rx_idx == (rx_start_idx + num_pkts)))\n\t\t\tbreak;\n\t}\n\n\ttg3_tx_skb_unmap(tnapi, tnapi->tx_prod - 1, -1);\n\tdev_kfree_skb(skb);\n\n\tif (tx_idx != tnapi->tx_prod)\n\t\tgoto out;\n\n\tif (rx_idx != rx_start_idx + num_pkts)\n\t\tgoto out;\n\n\tval = data_off;\n\twhile (rx_idx != rx_start_idx) {\n\t\tdesc = &rnapi->rx_rcb[rx_start_idx++];\n\t\tdesc_idx = desc->opaque & RXD_OPAQUE_INDEX_MASK;\n\t\topaque_key = desc->opaque & RXD_OPAQUE_RING_MASK;\n\n\t\tif ((desc->err_vlan & RXD_ERR_MASK) != 0 &&\n\t\t    (desc->err_vlan != RXD_ERR_ODD_NIBBLE_RCVD_MII))\n\t\t\tgoto out;\n\n\t\trx_len = ((desc->idx_len & RXD_LEN_MASK) >> RXD_LEN_SHIFT)\n\t\t\t - ETH_FCS_LEN;\n\n\t\tif (!tso_loopback) {\n\t\t\tif (rx_len != tx_len)\n\t\t\t\tgoto out;\n\n\t\t\tif (pktsz <= TG3_RX_STD_DMA_SZ - ETH_FCS_LEN) {\n\t\t\t\tif (opaque_key != RXD_OPAQUE_RING_STD)\n\t\t\t\t\tgoto out;\n\t\t\t} else {\n\t\t\t\tif (opaque_key != RXD_OPAQUE_RING_JUMBO)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else if ((desc->type_flags & RXD_FLAG_TCPUDP_CSUM) &&\n\t\t\t   (desc->ip_tcp_csum & RXD_TCPCSUM_MASK)\n\t\t\t    >> RXD_TCPCSUM_SHIFT != 0xffff) {\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (opaque_key == RXD_OPAQUE_RING_STD) {\n\t\t\trx_data = tpr->rx_std_buffers[desc_idx].data;\n\t\t\tmap = dma_unmap_addr(&tpr->rx_std_buffers[desc_idx],\n\t\t\t\t\t     mapping);\n\t\t} else if (opaque_key == RXD_OPAQUE_RING_JUMBO) {\n\t\t\trx_data = tpr->rx_jmb_buffers[desc_idx].data;\n\t\t\tmap = dma_unmap_addr(&tpr->rx_jmb_buffers[desc_idx],\n\t\t\t\t\t     mapping);\n\t\t} else\n\t\t\tgoto out;\n\n\t\tpci_dma_sync_single_for_cpu(tp->pdev, map, rx_len,\n\t\t\t\t\t    PCI_DMA_FROMDEVICE);\n\n\t\trx_data += TG3_RX_OFFSET(tp);\n\t\tfor (i = data_off; i < rx_len; i++, val++) {\n\t\t\tif (*(rx_data + i) != (u8) (val & 0xff))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\terr = 0;\n\n\t/* tg3_free_rings will unmap and free the rx_data */\nout:\n\treturn err;\n}\n\n#define TG3_STD_LOOPBACK_FAILED\t\t1\n#define TG3_JMB_LOOPBACK_FAILED\t\t2\n#define TG3_TSO_LOOPBACK_FAILED\t\t4\n#define TG3_LOOPBACK_FAILED \\\n\t(TG3_STD_LOOPBACK_FAILED | \\\n\t TG3_JMB_LOOPBACK_FAILED | \\\n\t TG3_TSO_LOOPBACK_FAILED)\n\nstatic int tg3_test_loopback(struct tg3 *tp, u64 *data, bool do_extlpbk)\n{\n\tint err = -EIO;\n\tu32 eee_cap;\n\tu32 jmb_pkt_sz = 9000;\n\n\tif (tp->dma_limit)\n\t\tjmb_pkt_sz = tp->dma_limit - ETH_HLEN;\n\n\teee_cap = tp->phy_flags & TG3_PHYFLG_EEE_CAP;\n\ttp->phy_flags &= ~TG3_PHYFLG_EEE_CAP;\n\n\tif (!netif_running(tp->dev)) {\n\t\tdata[TG3_MAC_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tdata[TG3_PHY_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tif (do_extlpbk)\n\t\t\tdata[TG3_EXT_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tgoto done;\n\t}\n\n\terr = tg3_reset_hw(tp, 1);\n\tif (err) {\n\t\tdata[TG3_MAC_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tdata[TG3_PHY_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tif (do_extlpbk)\n\t\t\tdata[TG3_EXT_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tgoto done;\n\t}\n\n\tif (tg3_flag(tp, ENABLE_RSS)) {\n\t\tint i;\n\n\t\t/* Reroute all rx packets to the 1st queue */\n\t\tfor (i = MAC_RSS_INDIR_TBL_0;\n\t\t     i < MAC_RSS_INDIR_TBL_0 + TG3_RSS_INDIR_TBL_SIZE; i += 4)\n\t\t\ttw32(i, 0x0);\n\t}\n\n\t/* HW errata - mac loopback fails in some cases on 5780.\n\t * Normal traffic and PHY loopback are not affected by\n\t * errata.  Also, the MAC loopback test is deprecated for\n\t * all newer ASIC revisions.\n\t */\n\tif (tg3_asic_rev(tp) != ASIC_REV_5780 &&\n\t    !tg3_flag(tp, CPMU_PRESENT)) {\n\t\ttg3_mac_loopback(tp, true);\n\n\t\tif (tg3_run_loopback(tp, ETH_FRAME_LEN, false))\n\t\t\tdata[TG3_MAC_LOOPB_TEST] |= TG3_STD_LOOPBACK_FAILED;\n\n\t\tif (tg3_flag(tp, JUMBO_RING_ENABLE) &&\n\t\t    tg3_run_loopback(tp, jmb_pkt_sz + ETH_HLEN, false))\n\t\t\tdata[TG3_MAC_LOOPB_TEST] |= TG3_JMB_LOOPBACK_FAILED;\n\n\t\ttg3_mac_loopback(tp, false);\n\t}\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_PHY_SERDES) &&\n\t    !tg3_flag(tp, USE_PHYLIB)) {\n\t\tint i;\n\n\t\ttg3_phy_lpbk_set(tp, 0, false);\n\n\t\t/* Wait for link */\n\t\tfor (i = 0; i < 100; i++) {\n\t\t\tif (tr32(MAC_TX_STATUS) & TX_STATUS_LINK_UP)\n\t\t\t\tbreak;\n\t\t\tmdelay(1);\n\t\t}\n\n\t\tif (tg3_run_loopback(tp, ETH_FRAME_LEN, false))\n\t\t\tdata[TG3_PHY_LOOPB_TEST] |= TG3_STD_LOOPBACK_FAILED;\n\t\tif (tg3_flag(tp, TSO_CAPABLE) &&\n\t\t    tg3_run_loopback(tp, ETH_FRAME_LEN, true))\n\t\t\tdata[TG3_PHY_LOOPB_TEST] |= TG3_TSO_LOOPBACK_FAILED;\n\t\tif (tg3_flag(tp, JUMBO_RING_ENABLE) &&\n\t\t    tg3_run_loopback(tp, jmb_pkt_sz + ETH_HLEN, false))\n\t\t\tdata[TG3_PHY_LOOPB_TEST] |= TG3_JMB_LOOPBACK_FAILED;\n\n\t\tif (do_extlpbk) {\n\t\t\ttg3_phy_lpbk_set(tp, 0, true);\n\n\t\t\t/* All link indications report up, but the hardware\n\t\t\t * isn't really ready for about 20 msec.  Double it\n\t\t\t * to be sure.\n\t\t\t */\n\t\t\tmdelay(40);\n\n\t\t\tif (tg3_run_loopback(tp, ETH_FRAME_LEN, false))\n\t\t\t\tdata[TG3_EXT_LOOPB_TEST] |=\n\t\t\t\t\t\t\tTG3_STD_LOOPBACK_FAILED;\n\t\t\tif (tg3_flag(tp, TSO_CAPABLE) &&\n\t\t\t    tg3_run_loopback(tp, ETH_FRAME_LEN, true))\n\t\t\t\tdata[TG3_EXT_LOOPB_TEST] |=\n\t\t\t\t\t\t\tTG3_TSO_LOOPBACK_FAILED;\n\t\t\tif (tg3_flag(tp, JUMBO_RING_ENABLE) &&\n\t\t\t    tg3_run_loopback(tp, jmb_pkt_sz + ETH_HLEN, false))\n\t\t\t\tdata[TG3_EXT_LOOPB_TEST] |=\n\t\t\t\t\t\t\tTG3_JMB_LOOPBACK_FAILED;\n\t\t}\n\n\t\t/* Re-enable gphy autopowerdown. */\n\t\tif (tp->phy_flags & TG3_PHYFLG_ENABLE_APD)\n\t\t\ttg3_phy_toggle_apd(tp, true);\n\t}\n\n\terr = (data[TG3_MAC_LOOPB_TEST] | data[TG3_PHY_LOOPB_TEST] |\n\t       data[TG3_EXT_LOOPB_TEST]) ? -EIO : 0;\n\ndone:\n\ttp->phy_flags |= eee_cap;\n\n\treturn err;\n}\n\nstatic void tg3_self_test(struct net_device *dev, struct ethtool_test *etest,\n\t\t\t  u64 *data)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tbool doextlpbk = etest->flags & ETH_TEST_FL_EXTERNAL_LB;\n\n\tif ((tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER) &&\n\t    tg3_power_up(tp)) {\n\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\tmemset(data, 1, sizeof(u64) * TG3_NUM_TEST);\n\t\treturn;\n\t}\n\n\tmemset(data, 0, sizeof(u64) * TG3_NUM_TEST);\n\n\tif (tg3_test_nvram(tp) != 0) {\n\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\tdata[TG3_NVRAM_TEST] = 1;\n\t}\n\tif (!doextlpbk && tg3_test_link(tp)) {\n\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\tdata[TG3_LINK_TEST] = 1;\n\t}\n\tif (etest->flags & ETH_TEST_FL_OFFLINE) {\n\t\tint err, err2 = 0, irq_sync = 0;\n\n\t\tif (netif_running(dev)) {\n\t\t\ttg3_phy_stop(tp);\n\t\t\ttg3_netif_stop(tp);\n\t\t\tirq_sync = 1;\n\t\t}\n\n\t\ttg3_full_lock(tp, irq_sync);\n\t\ttg3_halt(tp, RESET_KIND_SUSPEND, 1);\n\t\terr = tg3_nvram_lock(tp);\n\t\ttg3_halt_cpu(tp, RX_CPU_BASE);\n\t\tif (!tg3_flag(tp, 5705_PLUS))\n\t\t\ttg3_halt_cpu(tp, TX_CPU_BASE);\n\t\tif (!err)\n\t\t\ttg3_nvram_unlock(tp);\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_MII_SERDES)\n\t\t\ttg3_phy_reset(tp);\n\n\t\tif (tg3_test_registers(tp) != 0) {\n\t\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tdata[TG3_REGISTER_TEST] = 1;\n\t\t}\n\n\t\tif (tg3_test_memory(tp) != 0) {\n\t\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tdata[TG3_MEMORY_TEST] = 1;\n\t\t}\n\n\t\tif (doextlpbk)\n\t\t\tetest->flags |= ETH_TEST_FL_EXTERNAL_LB_DONE;\n\n\t\tif (tg3_test_loopback(tp, data, doextlpbk))\n\t\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\n\t\ttg3_full_unlock(tp);\n\n\t\tif (tg3_test_interrupt(tp) != 0) {\n\t\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tdata[TG3_INTERRUPT_TEST] = 1;\n\t\t}\n\n\t\ttg3_full_lock(tp, 0);\n\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\tif (netif_running(dev)) {\n\t\t\ttg3_flag_set(tp, INIT_COMPLETE);\n\t\t\terr2 = tg3_restart_hw(tp, 1);\n\t\t\tif (!err2)\n\t\t\t\ttg3_netif_start(tp);\n\t\t}\n\n\t\ttg3_full_unlock(tp);\n\n\t\tif (irq_sync && !err2)\n\t\t\ttg3_phy_start(tp);\n\t}\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\ttg3_power_down(tp);\n\n}\n\nstatic int tg3_hwtstamp_ioctl(struct net_device *dev,\n\t\t\t      struct ifreq *ifr, int cmd)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tstruct hwtstamp_config stmpconf;\n\n\tif (!tg3_flag(tp, PTP_CAPABLE))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&stmpconf, ifr->ifr_data, sizeof(stmpconf)))\n\t\treturn -EFAULT;\n\n\tif (stmpconf.flags)\n\t\treturn -EINVAL;\n\n\tswitch (stmpconf.tx_type) {\n\tcase HWTSTAMP_TX_ON:\n\t\ttg3_flag_set(tp, TX_TSTAMP_EN);\n\t\tbreak;\n\tcase HWTSTAMP_TX_OFF:\n\t\ttg3_flag_clear(tp, TX_TSTAMP_EN);\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tswitch (stmpconf.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\ttp->rxptpctl = 0;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V1_EN |\n\t\t\t       TG3_RX_PTP_CTL_ALL_V1_EVENTS;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V1_EN |\n\t\t\t       TG3_RX_PTP_CTL_SYNC_EVNT;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V1_EN |\n\t\t\t       TG3_RX_PTP_CTL_DELAY_REQ;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_EN |\n\t\t\t       TG3_RX_PTP_CTL_ALL_V2_EVENTS;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L2_EN |\n\t\t\t       TG3_RX_PTP_CTL_ALL_V2_EVENTS;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L4_EN |\n\t\t\t       TG3_RX_PTP_CTL_ALL_V2_EVENTS;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_EN |\n\t\t\t       TG3_RX_PTP_CTL_SYNC_EVNT;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L2_EN |\n\t\t\t       TG3_RX_PTP_CTL_SYNC_EVNT;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L4_EN |\n\t\t\t       TG3_RX_PTP_CTL_SYNC_EVNT;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_EN |\n\t\t\t       TG3_RX_PTP_CTL_DELAY_REQ;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L2_EN |\n\t\t\t       TG3_RX_PTP_CTL_DELAY_REQ;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L4_EN |\n\t\t\t       TG3_RX_PTP_CTL_DELAY_REQ;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tif (netif_running(dev) && tp->rxptpctl)\n\t\ttw32(TG3_RX_PTP_CTL,\n\t\t     tp->rxptpctl | TG3_RX_PTP_CTL_HWTS_INTERLOCK);\n\n\treturn copy_to_user(ifr->ifr_data, &stmpconf, sizeof(stmpconf)) ?\n\t\t-EFAULT : 0;\n}\n\nstatic int tg3_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tstruct mii_ioctl_data *data = if_mii(ifr);\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err;\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tstruct phy_device *phydev;\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\t\treturn -EAGAIN;\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\t\treturn phy_mii_ioctl(phydev, ifr, cmd);\n\t}\n\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\t\tdata->phy_id = tp->phy_addr;\n\n\t\t/* fallthru */\n\tcase SIOCGMIIREG: {\n\t\tu32 mii_regval;\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\t\tbreak;\t\t\t/* We have no PHY */\n\n\t\tif (!netif_running(dev))\n\t\t\treturn -EAGAIN;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\terr = __tg3_readphy(tp, data->phy_id & 0x1f,\n\t\t\t\t    data->reg_num & 0x1f, &mii_regval);\n\t\tspin_unlock_bh(&tp->lock);\n\n\t\tdata->val_out = mii_regval;\n\n\t\treturn err;\n\t}\n\n\tcase SIOCSMIIREG:\n\t\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\t\tbreak;\t\t\t/* We have no PHY */\n\n\t\tif (!netif_running(dev))\n\t\t\treturn -EAGAIN;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\terr = __tg3_writephy(tp, data->phy_id & 0x1f,\n\t\t\t\t     data->reg_num & 0x1f, data->val_in);\n\t\tspin_unlock_bh(&tp->lock);\n\n\t\treturn err;\n\n\tcase SIOCSHWTSTAMP:\n\t\treturn tg3_hwtstamp_ioctl(dev, ifr, cmd);\n\n\tdefault:\n\t\t/* do nothing */\n\t\tbreak;\n\t}\n\treturn -EOPNOTSUPP;\n}\n\nstatic int tg3_get_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tmemcpy(ec, &tp->coal, sizeof(*ec));\n\treturn 0;\n}\n\nstatic int tg3_set_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 max_rxcoal_tick_int = 0, max_txcoal_tick_int = 0;\n\tu32 max_stat_coal_ticks = 0, min_stat_coal_ticks = 0;\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\tmax_rxcoal_tick_int = MAX_RXCOAL_TICK_INT;\n\t\tmax_txcoal_tick_int = MAX_TXCOAL_TICK_INT;\n\t\tmax_stat_coal_ticks = MAX_STAT_COAL_TICKS;\n\t\tmin_stat_coal_ticks = MIN_STAT_COAL_TICKS;\n\t}\n\n\tif ((ec->rx_coalesce_usecs > MAX_RXCOL_TICKS) ||\n\t    (ec->tx_coalesce_usecs > MAX_TXCOL_TICKS) ||\n\t    (ec->rx_max_coalesced_frames > MAX_RXMAX_FRAMES) ||\n\t    (ec->tx_max_coalesced_frames > MAX_TXMAX_FRAMES) ||\n\t    (ec->rx_coalesce_usecs_irq > max_rxcoal_tick_int) ||\n\t    (ec->tx_coalesce_usecs_irq > max_txcoal_tick_int) ||\n\t    (ec->rx_max_coalesced_frames_irq > MAX_RXCOAL_MAXF_INT) ||\n\t    (ec->tx_max_coalesced_frames_irq > MAX_TXCOAL_MAXF_INT) ||\n\t    (ec->stats_block_coalesce_usecs > max_stat_coal_ticks) ||\n\t    (ec->stats_block_coalesce_usecs < min_stat_coal_ticks))\n\t\treturn -EINVAL;\n\n\t/* No rx interrupts will be generated if both are zero */\n\tif ((ec->rx_coalesce_usecs == 0) &&\n\t    (ec->rx_max_coalesced_frames == 0))\n\t\treturn -EINVAL;\n\n\t/* No tx interrupts will be generated if both are zero */\n\tif ((ec->tx_coalesce_usecs == 0) &&\n\t    (ec->tx_max_coalesced_frames == 0))\n\t\treturn -EINVAL;\n\n\t/* Only copy relevant parameters, ignore all others. */\n\ttp->coal.rx_coalesce_usecs = ec->rx_coalesce_usecs;\n\ttp->coal.tx_coalesce_usecs = ec->tx_coalesce_usecs;\n\ttp->coal.rx_max_coalesced_frames = ec->rx_max_coalesced_frames;\n\ttp->coal.tx_max_coalesced_frames = ec->tx_max_coalesced_frames;\n\ttp->coal.rx_coalesce_usecs_irq = ec->rx_coalesce_usecs_irq;\n\ttp->coal.tx_coalesce_usecs_irq = ec->tx_coalesce_usecs_irq;\n\ttp->coal.rx_max_coalesced_frames_irq = ec->rx_max_coalesced_frames_irq;\n\ttp->coal.tx_max_coalesced_frames_irq = ec->tx_max_coalesced_frames_irq;\n\ttp->coal.stats_block_coalesce_usecs = ec->stats_block_coalesce_usecs;\n\n\tif (netif_running(dev)) {\n\t\ttg3_full_lock(tp, 0);\n\t\t__tg3_set_coalesce(tp, &tp->coal);\n\t\ttg3_full_unlock(tp);\n\t}\n\treturn 0;\n}\n\nstatic const struct ethtool_ops tg3_ethtool_ops = {\n\t.get_settings\t\t= tg3_get_settings,\n\t.set_settings\t\t= tg3_set_settings,\n\t.get_drvinfo\t\t= tg3_get_drvinfo,\n\t.get_regs_len\t\t= tg3_get_regs_len,\n\t.get_regs\t\t= tg3_get_regs,\n\t.get_wol\t\t= tg3_get_wol,\n\t.set_wol\t\t= tg3_set_wol,\n\t.get_msglevel\t\t= tg3_get_msglevel,\n\t.set_msglevel\t\t= tg3_set_msglevel,\n\t.nway_reset\t\t= tg3_nway_reset,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_eeprom_len\t\t= tg3_get_eeprom_len,\n\t.get_eeprom\t\t= tg3_get_eeprom,\n\t.set_eeprom\t\t= tg3_set_eeprom,\n\t.get_ringparam\t\t= tg3_get_ringparam,\n\t.set_ringparam\t\t= tg3_set_ringparam,\n\t.get_pauseparam\t\t= tg3_get_pauseparam,\n\t.set_pauseparam\t\t= tg3_set_pauseparam,\n\t.self_test\t\t= tg3_self_test,\n\t.get_strings\t\t= tg3_get_strings,\n\t.set_phys_id\t\t= tg3_set_phys_id,\n\t.get_ethtool_stats\t= tg3_get_ethtool_stats,\n\t.get_coalesce\t\t= tg3_get_coalesce,\n\t.set_coalesce\t\t= tg3_set_coalesce,\n\t.get_sset_count\t\t= tg3_get_sset_count,\n\t.get_rxnfc\t\t= tg3_get_rxnfc,\n\t.get_rxfh_indir_size    = tg3_get_rxfh_indir_size,\n\t.get_rxfh_indir\t\t= tg3_get_rxfh_indir,\n\t.set_rxfh_indir\t\t= tg3_set_rxfh_indir,\n\t.get_channels\t\t= tg3_get_channels,\n\t.set_channels\t\t= tg3_set_channels,\n\t.get_ts_info\t\t= tg3_get_ts_info,\n};\n\nstatic struct rtnl_link_stats64 *tg3_get_stats64(struct net_device *dev,\n\t\t\t\t\t\tstruct rtnl_link_stats64 *stats)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tspin_lock_bh(&tp->lock);\n\tif (!tp->hw_stats) {\n\t\tspin_unlock_bh(&tp->lock);\n\t\treturn &tp->net_stats_prev;\n\t}\n\n\ttg3_get_nstats(tp, stats);\n\tspin_unlock_bh(&tp->lock);\n\n\treturn stats;\n}\n\nstatic void tg3_set_rx_mode(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\treturn;\n\n\ttg3_full_lock(tp, 0);\n\t__tg3_set_rx_mode(dev);\n\ttg3_full_unlock(tp);\n}\n\nstatic inline void tg3_set_mtu(struct net_device *dev, struct tg3 *tp,\n\t\t\t       int new_mtu)\n{\n\tdev->mtu = new_mtu;\n\n\tif (new_mtu > ETH_DATA_LEN) {\n\t\tif (tg3_flag(tp, 5780_CLASS)) {\n\t\t\tnetdev_update_features(dev);\n\t\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\t} else {\n\t\t\ttg3_flag_set(tp, JUMBO_RING_ENABLE);\n\t\t}\n\t} else {\n\t\tif (tg3_flag(tp, 5780_CLASS)) {\n\t\t\ttg3_flag_set(tp, TSO_CAPABLE);\n\t\t\tnetdev_update_features(dev);\n\t\t}\n\t\ttg3_flag_clear(tp, JUMBO_RING_ENABLE);\n\t}\n}\n\nstatic int tg3_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err, reset_phy = 0;\n\n\tif (new_mtu < TG3_MIN_MTU || new_mtu > TG3_MAX_MTU(tp))\n\t\treturn -EINVAL;\n\n\tif (!netif_running(dev)) {\n\t\t/* We'll just catch it later when the\n\t\t * device is up'd.\n\t\t */\n\t\ttg3_set_mtu(dev, tp, new_mtu);\n\t\treturn 0;\n\t}\n\n\ttg3_phy_stop(tp);\n\n\ttg3_netif_stop(tp);\n\n\ttg3_full_lock(tp, 1);\n\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\n\ttg3_set_mtu(dev, tp, new_mtu);\n\n\t/* Reset PHY, otherwise the read DMA engine will be in a mode that\n\t * breaks all requests to 256 bytes.\n\t */\n\tif (tg3_asic_rev(tp) == ASIC_REV_57766)\n\t\treset_phy = 1;\n\n\terr = tg3_restart_hw(tp, reset_phy);\n\n\tif (!err)\n\t\ttg3_netif_start(tp);\n\n\ttg3_full_unlock(tp);\n\n\tif (!err)\n\t\ttg3_phy_start(tp);\n\n\treturn err;\n}\n\nstatic const struct net_device_ops tg3_netdev_ops = {\n\t.ndo_open\t\t= tg3_open,\n\t.ndo_stop\t\t= tg3_close,\n\t.ndo_start_xmit\t\t= tg3_start_xmit,\n\t.ndo_get_stats64\t= tg3_get_stats64,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_rx_mode\t= tg3_set_rx_mode,\n\t.ndo_set_mac_address\t= tg3_set_mac_addr,\n\t.ndo_do_ioctl\t\t= tg3_ioctl,\n\t.ndo_tx_timeout\t\t= tg3_tx_timeout,\n\t.ndo_change_mtu\t\t= tg3_change_mtu,\n\t.ndo_fix_features\t= tg3_fix_features,\n\t.ndo_set_features\t= tg3_set_features,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= tg3_poll_controller,\n#endif\n};\n\nstatic void tg3_get_eeprom_size(struct tg3 *tp)\n{\n\tu32 cursize, val, magic;\n\n\ttp->nvram_size = EEPROM_CHIP_SIZE;\n\n\tif (tg3_nvram_read(tp, 0, &magic) != 0)\n\t\treturn;\n\n\tif ((magic != TG3_EEPROM_MAGIC) &&\n\t    ((magic & TG3_EEPROM_MAGIC_FW_MSK) != TG3_EEPROM_MAGIC_FW) &&\n\t    ((magic & TG3_EEPROM_MAGIC_HW_MSK) != TG3_EEPROM_MAGIC_HW))\n\t\treturn;\n\n\t/*\n\t * Size the chip by reading offsets at increasing powers of two.\n\t * When we encounter our validation signature, we know the addressing\n\t * has wrapped around, and thus have our chip size.\n\t */\n\tcursize = 0x10;\n\n\twhile (cursize < tp->nvram_size) {\n\t\tif (tg3_nvram_read(tp, cursize, &val) != 0)\n\t\t\treturn;\n\n\t\tif (val == magic)\n\t\t\tbreak;\n\n\t\tcursize <<= 1;\n\t}\n\n\ttp->nvram_size = cursize;\n}\n\nstatic void tg3_get_nvram_size(struct tg3 *tp)\n{\n\tu32 val;\n\n\tif (tg3_flag(tp, NO_NVRAM) || tg3_nvram_read(tp, 0, &val) != 0)\n\t\treturn;\n\n\t/* Selfboot format */\n\tif (val != TG3_EEPROM_MAGIC) {\n\t\ttg3_get_eeprom_size(tp);\n\t\treturn;\n\t}\n\n\tif (tg3_nvram_read(tp, 0xf0, &val) == 0) {\n\t\tif (val != 0) {\n\t\t\t/* This is confusing.  We want to operate on the\n\t\t\t * 16-bit value at offset 0xf2.  The tg3_nvram_read()\n\t\t\t * call will read from NVRAM and byteswap the data\n\t\t\t * according to the byteswapping settings for all\n\t\t\t * other register accesses.  This ensures the data we\n\t\t\t * want will always reside in the lower 16-bits.\n\t\t\t * However, the data in NVRAM is in LE format, which\n\t\t\t * means the data from the NVRAM read will always be\n\t\t\t * opposite the endianness of the CPU.  The 16-bit\n\t\t\t * byteswap then brings the data to CPU endianness.\n\t\t\t */\n\t\t\ttp->nvram_size = swab16((u16)(val & 0x0000ffff)) * 1024;\n\t\t\treturn;\n\t\t}\n\t}\n\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n}\n\nstatic void tg3_get_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\tif (nvcfg1 & NVRAM_CFG1_FLASHIF_ENAB) {\n\t\ttg3_flag_set(tp, FLASH);\n\t} else {\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5750 ||\n\t    tg3_flag(tp, 5780_CLASS)) {\n\t\tswitch (nvcfg1 & NVRAM_CFG1_VENDOR_MASK) {\n\t\tcase FLASH_VENDOR_ATMEL_FLASH_BUFFERED:\n\t\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\t\ttp->nvram_pagesize = ATMEL_AT45DB0X1B_PAGE_SIZE;\n\t\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_ATMEL_FLASH_UNBUFFERED:\n\t\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\t\ttp->nvram_pagesize = ATMEL_AT25F512_PAGE_SIZE;\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_ATMEL_EEPROM:\n\t\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\t\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_ST:\n\t\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\t\ttp->nvram_pagesize = ST_M45PEX0_PAGE_SIZE;\n\t\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_SAIFUN:\n\t\t\ttp->nvram_jedecnum = JEDEC_SAIFUN;\n\t\t\ttp->nvram_pagesize = SAIFUN_SA25F0XX_PAGE_SIZE;\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_SST_SMALL:\n\t\tcase FLASH_VENDOR_SST_LARGE:\n\t\t\ttp->nvram_jedecnum = JEDEC_SST;\n\t\t\ttp->nvram_pagesize = SST_25VF0X0_PAGE_SIZE;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttp->nvram_pagesize = ATMEL_AT45DB0X1B_PAGE_SIZE;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t}\n}\n\nstatic void tg3_nvram_get_pagesize(struct tg3 *tp, u32 nvmcfg1)\n{\n\tswitch (nvmcfg1 & NVRAM_CFG1_5752PAGE_SIZE_MASK) {\n\tcase FLASH_5752PAGE_SIZE_256:\n\t\ttp->nvram_pagesize = 256;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_512:\n\t\ttp->nvram_pagesize = 512;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_1K:\n\t\ttp->nvram_pagesize = 1024;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_2K:\n\t\ttp->nvram_pagesize = 2048;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_4K:\n\t\ttp->nvram_pagesize = 4096;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_264:\n\t\ttp->nvram_pagesize = 264;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_528:\n\t\ttp->nvram_pagesize = 528;\n\t\tbreak;\n\t}\n}\n\nstatic void tg3_get_5752_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\t/* NVRAM protection for TPM */\n\tif (nvcfg1 & (1 << 27))\n\t\ttg3_flag_set(tp, PROTECTED_NVRAM);\n\n\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\tcase FLASH_5752VENDOR_ATMEL_EEPROM_64KHZ:\n\tcase FLASH_5752VENDOR_ATMEL_EEPROM_376KHZ:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ATMEL_FLASH_BUFFERED:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ST_M45PE10:\n\tcase FLASH_5752VENDOR_ST_M45PE20:\n\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\tbreak;\n\t}\n\n\tif (tg3_flag(tp, FLASH)) {\n\t\ttg3_nvram_get_pagesize(tp, nvcfg1);\n\t} else {\n\t\t/* For eeprom, set pagesize to maximum eeprom size */\n\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t}\n}\n\nstatic void tg3_get_5755_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1, protect = 0;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\t/* NVRAM protection for TPM */\n\tif (nvcfg1 & (1 << 27)) {\n\t\ttg3_flag_set(tp, PROTECTED_NVRAM);\n\t\tprotect = 1;\n\t}\n\n\tnvcfg1 &= NVRAM_CFG1_5752VENDOR_MASK;\n\tswitch (nvcfg1) {\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_1:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_2:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_3:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_5:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 264;\n\t\tif (nvcfg1 == FLASH_5755VENDOR_ATMEL_FLASH_1 ||\n\t\t    nvcfg1 == FLASH_5755VENDOR_ATMEL_FLASH_5)\n\t\t\ttp->nvram_size = (protect ? 0x3e200 :\n\t\t\t\t\t  TG3_NVRAM_SIZE_512KB);\n\t\telse if (nvcfg1 == FLASH_5755VENDOR_ATMEL_FLASH_2)\n\t\t\ttp->nvram_size = (protect ? 0x1f200 :\n\t\t\t\t\t  TG3_NVRAM_SIZE_256KB);\n\t\telse\n\t\t\ttp->nvram_size = (protect ? 0x1f200 :\n\t\t\t\t\t  TG3_NVRAM_SIZE_128KB);\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ST_M45PE10:\n\tcase FLASH_5752VENDOR_ST_M45PE20:\n\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 256;\n\t\tif (nvcfg1 == FLASH_5752VENDOR_ST_M45PE10)\n\t\t\ttp->nvram_size = (protect ?\n\t\t\t\t\t  TG3_NVRAM_SIZE_64KB :\n\t\t\t\t\t  TG3_NVRAM_SIZE_128KB);\n\t\telse if (nvcfg1 == FLASH_5752VENDOR_ST_M45PE20)\n\t\t\ttp->nvram_size = (protect ?\n\t\t\t\t\t  TG3_NVRAM_SIZE_64KB :\n\t\t\t\t\t  TG3_NVRAM_SIZE_256KB);\n\t\telse\n\t\t\ttp->nvram_size = (protect ?\n\t\t\t\t\t  TG3_NVRAM_SIZE_128KB :\n\t\t\t\t\t  TG3_NVRAM_SIZE_512KB);\n\t\tbreak;\n\t}\n}\n\nstatic void tg3_get_5787_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\tcase FLASH_5787VENDOR_ATMEL_EEPROM_64KHZ:\n\tcase FLASH_5787VENDOR_ATMEL_EEPROM_376KHZ:\n\tcase FLASH_5787VENDOR_MICRO_EEPROM_64KHZ:\n\tcase FLASH_5787VENDOR_MICRO_EEPROM_376KHZ:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ATMEL_FLASH_BUFFERED:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_1:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_2:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_3:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 264;\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ST_M45PE10:\n\tcase FLASH_5752VENDOR_ST_M45PE20:\n\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 256;\n\t\tbreak;\n\t}\n}\n\nstatic void tg3_get_5761_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1, protect = 0;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\t/* NVRAM protection for TPM */\n\tif (nvcfg1 & (1 << 27)) {\n\t\ttg3_flag_set(tp, PROTECTED_NVRAM);\n\t\tprotect = 1;\n\t}\n\n\tnvcfg1 &= NVRAM_CFG1_5752VENDOR_MASK;\n\tswitch (nvcfg1) {\n\tcase FLASH_5761VENDOR_ATMEL_ADB021D:\n\tcase FLASH_5761VENDOR_ATMEL_ADB041D:\n\tcase FLASH_5761VENDOR_ATMEL_ADB081D:\n\tcase FLASH_5761VENDOR_ATMEL_ADB161D:\n\tcase FLASH_5761VENDOR_ATMEL_MDB021D:\n\tcase FLASH_5761VENDOR_ATMEL_MDB041D:\n\tcase FLASH_5761VENDOR_ATMEL_MDB081D:\n\tcase FLASH_5761VENDOR_ATMEL_MDB161D:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttg3_flag_set(tp, NO_NVRAM_ADDR_TRANS);\n\t\ttp->nvram_pagesize = 256;\n\t\tbreak;\n\tcase FLASH_5761VENDOR_ST_A_M45PE20:\n\tcase FLASH_5761VENDOR_ST_A_M45PE40:\n\tcase FLASH_5761VENDOR_ST_A_M45PE80:\n\tcase FLASH_5761VENDOR_ST_A_M45PE16:\n\tcase FLASH_5761VENDOR_ST_M_M45PE20:\n\tcase FLASH_5761VENDOR_ST_M_M45PE40:\n\tcase FLASH_5761VENDOR_ST_M_M45PE80:\n\tcase FLASH_5761VENDOR_ST_M_M45PE16:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 256;\n\t\tbreak;\n\t}\n\n\tif (protect) {\n\t\ttp->nvram_size = tr32(NVRAM_ADDR_LOCKOUT);\n\t} else {\n\t\tswitch (nvcfg1) {\n\t\tcase FLASH_5761VENDOR_ATMEL_ADB161D:\n\t\tcase FLASH_5761VENDOR_ATMEL_MDB161D:\n\t\tcase FLASH_5761VENDOR_ST_A_M45PE16:\n\t\tcase FLASH_5761VENDOR_ST_M_M45PE16:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_2MB;\n\t\t\tbreak;\n\t\tcase FLASH_5761VENDOR_ATMEL_ADB081D:\n\t\tcase FLASH_5761VENDOR_ATMEL_MDB081D:\n\t\tcase FLASH_5761VENDOR_ST_A_M45PE80:\n\t\tcase FLASH_5761VENDOR_ST_M_M45PE80:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_1MB;\n\t\t\tbreak;\n\t\tcase FLASH_5761VENDOR_ATMEL_ADB041D:\n\t\tcase FLASH_5761VENDOR_ATMEL_MDB041D:\n\t\tcase FLASH_5761VENDOR_ST_A_M45PE40:\n\t\tcase FLASH_5761VENDOR_ST_M_M45PE40:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\tcase FLASH_5761VENDOR_ATMEL_ADB021D:\n\t\tcase FLASH_5761VENDOR_ATMEL_MDB021D:\n\t\tcase FLASH_5761VENDOR_ST_A_M45PE20:\n\t\tcase FLASH_5761VENDOR_ST_M_M45PE20:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void tg3_get_5906_nvram_info(struct tg3 *tp)\n{\n\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n}\n\nstatic void tg3_get_57780_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\tcase FLASH_5787VENDOR_ATMEL_EEPROM_376KHZ:\n\tcase FLASH_5787VENDOR_MICRO_EEPROM_376KHZ:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t\treturn;\n\tcase FLASH_5752VENDOR_ATMEL_FLASH_BUFFERED:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB011D:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB011B:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB021D:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB021B:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB041D:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB041B:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\t\tcase FLASH_5752VENDOR_ATMEL_FLASH_BUFFERED:\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB011D:\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB011B:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB021D:\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB021B:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB041D:\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB041B:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ST_M45PE10:\n\tcase FLASH_5752VENDOR_ST_M45PE20:\n\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\t\tcase FLASH_5752VENDOR_ST_M45PE10:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\tcase FLASH_5752VENDOR_ST_M45PE20:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\treturn;\n\t}\n\n\ttg3_nvram_get_pagesize(tp, nvcfg1);\n\tif (tp->nvram_pagesize != 264 && tp->nvram_pagesize != 528)\n\t\ttg3_flag_set(tp, NO_NVRAM_ADDR_TRANS);\n}\n\n\nstatic void tg3_get_5717_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\tcase FLASH_5717VENDOR_ATMEL_EEPROM:\n\tcase FLASH_5717VENDOR_MICRO_EEPROM:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t\treturn;\n\tcase FLASH_5717VENDOR_ATMEL_MDB011D:\n\tcase FLASH_5717VENDOR_ATMEL_ADB011B:\n\tcase FLASH_5717VENDOR_ATMEL_ADB011D:\n\tcase FLASH_5717VENDOR_ATMEL_MDB021D:\n\tcase FLASH_5717VENDOR_ATMEL_ADB021B:\n\tcase FLASH_5717VENDOR_ATMEL_ADB021D:\n\tcase FLASH_5717VENDOR_ATMEL_45USPT:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\t\tcase FLASH_5717VENDOR_ATMEL_MDB021D:\n\t\t\t/* Detect size with tg3_nvram_get_size() */\n\t\t\tbreak;\n\t\tcase FLASH_5717VENDOR_ATMEL_ADB021B:\n\t\tcase FLASH_5717VENDOR_ATMEL_ADB021D:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase FLASH_5717VENDOR_ST_M_M25PE10:\n\tcase FLASH_5717VENDOR_ST_A_M25PE10:\n\tcase FLASH_5717VENDOR_ST_M_M45PE10:\n\tcase FLASH_5717VENDOR_ST_A_M45PE10:\n\tcase FLASH_5717VENDOR_ST_M_M25PE20:\n\tcase FLASH_5717VENDOR_ST_A_M25PE20:\n\tcase FLASH_5717VENDOR_ST_M_M45PE20:\n\tcase FLASH_5717VENDOR_ST_A_M45PE20:\n\tcase FLASH_5717VENDOR_ST_25USPT:\n\tcase FLASH_5717VENDOR_ST_45USPT:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\t\tcase FLASH_5717VENDOR_ST_M_M25PE20:\n\t\tcase FLASH_5717VENDOR_ST_M_M45PE20:\n\t\t\t/* Detect size with tg3_nvram_get_size() */\n\t\t\tbreak;\n\t\tcase FLASH_5717VENDOR_ST_A_M25PE20:\n\t\tcase FLASH_5717VENDOR_ST_A_M45PE20:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\treturn;\n\t}\n\n\ttg3_nvram_get_pagesize(tp, nvcfg1);\n\tif (tp->nvram_pagesize != 264 && tp->nvram_pagesize != 528)\n\t\ttg3_flag_set(tp, NO_NVRAM_ADDR_TRANS);\n}\n\nstatic void tg3_get_5720_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1, nvmpinstrp;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\tnvmpinstrp = nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\tif (!(nvcfg1 & NVRAM_CFG1_5762VENDOR_MASK)) {\n\t\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\t\treturn;\n\t\t}\n\n\t\tswitch (nvmpinstrp) {\n\t\tcase FLASH_5762_EEPROM_HD:\n\t\t\tnvmpinstrp = FLASH_5720_EEPROM_HD;\n\t\t\tbreak;\n\t\tcase FLASH_5762_EEPROM_LD:\n\t\t\tnvmpinstrp = FLASH_5720_EEPROM_LD;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tswitch (nvmpinstrp) {\n\tcase FLASH_5720_EEPROM_HD:\n\tcase FLASH_5720_EEPROM_LD:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t\tif (nvmpinstrp == FLASH_5720_EEPROM_HD)\n\t\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\t\telse\n\t\t\ttp->nvram_pagesize = ATMEL_AT24C02_CHIP_SIZE;\n\t\treturn;\n\tcase FLASH_5720VENDOR_M_ATMEL_DB011D:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB011B:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB011D:\n\tcase FLASH_5720VENDOR_M_ATMEL_DB021D:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB021B:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB021D:\n\tcase FLASH_5720VENDOR_M_ATMEL_DB041D:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB041B:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB041D:\n\tcase FLASH_5720VENDOR_M_ATMEL_DB081D:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB081D:\n\tcase FLASH_5720VENDOR_ATMEL_45USPT:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvmpinstrp) {\n\t\tcase FLASH_5720VENDOR_M_ATMEL_DB021D:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB021B:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB021D:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tcase FLASH_5720VENDOR_M_ATMEL_DB041D:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB041B:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB041D:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\tcase FLASH_5720VENDOR_M_ATMEL_DB081D:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB081D:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_1MB;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (tg3_asic_rev(tp) != ASIC_REV_5762)\n\t\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase FLASH_5720VENDOR_M_ST_M25PE10:\n\tcase FLASH_5720VENDOR_M_ST_M45PE10:\n\tcase FLASH_5720VENDOR_A_ST_M25PE10:\n\tcase FLASH_5720VENDOR_A_ST_M45PE10:\n\tcase FLASH_5720VENDOR_M_ST_M25PE20:\n\tcase FLASH_5720VENDOR_M_ST_M45PE20:\n\tcase FLASH_5720VENDOR_A_ST_M25PE20:\n\tcase FLASH_5720VENDOR_A_ST_M45PE20:\n\tcase FLASH_5720VENDOR_M_ST_M25PE40:\n\tcase FLASH_5720VENDOR_M_ST_M45PE40:\n\tcase FLASH_5720VENDOR_A_ST_M25PE40:\n\tcase FLASH_5720VENDOR_A_ST_M45PE40:\n\tcase FLASH_5720VENDOR_M_ST_M25PE80:\n\tcase FLASH_5720VENDOR_M_ST_M45PE80:\n\tcase FLASH_5720VENDOR_A_ST_M25PE80:\n\tcase FLASH_5720VENDOR_A_ST_M45PE80:\n\tcase FLASH_5720VENDOR_ST_25USPT:\n\tcase FLASH_5720VENDOR_ST_45USPT:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvmpinstrp) {\n\t\tcase FLASH_5720VENDOR_M_ST_M25PE20:\n\t\tcase FLASH_5720VENDOR_M_ST_M45PE20:\n\t\tcase FLASH_5720VENDOR_A_ST_M25PE20:\n\t\tcase FLASH_5720VENDOR_A_ST_M45PE20:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tcase FLASH_5720VENDOR_M_ST_M25PE40:\n\t\tcase FLASH_5720VENDOR_M_ST_M45PE40:\n\t\tcase FLASH_5720VENDOR_A_ST_M25PE40:\n\t\tcase FLASH_5720VENDOR_A_ST_M45PE40:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\tcase FLASH_5720VENDOR_M_ST_M25PE80:\n\t\tcase FLASH_5720VENDOR_M_ST_M45PE80:\n\t\tcase FLASH_5720VENDOR_A_ST_M25PE80:\n\t\tcase FLASH_5720VENDOR_A_ST_M45PE80:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_1MB;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (tg3_asic_rev(tp) != ASIC_REV_5762)\n\t\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\treturn;\n\t}\n\n\ttg3_nvram_get_pagesize(tp, nvcfg1);\n\tif (tp->nvram_pagesize != 264 && tp->nvram_pagesize != 528)\n\t\ttg3_flag_set(tp, NO_NVRAM_ADDR_TRANS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\tu32 val;\n\n\t\tif (tg3_nvram_read(tp, 0, &val))\n\t\t\treturn;\n\n\t\tif (val != TG3_EEPROM_MAGIC &&\n\t\t    (val & TG3_EEPROM_MAGIC_FW_MSK) != TG3_EEPROM_MAGIC_FW)\n\t\t\ttg3_flag_set(tp, NO_NVRAM);\n\t}\n}\n\n/* Chips other than 5700/5701 use the NVRAM for fetching info. */\nstatic void tg3_nvram_init(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, IS_SSB_CORE)) {\n\t\t/* No NVRAM and EEPROM on the SSB Broadcom GigE core. */\n\t\ttg3_flag_clear(tp, NVRAM);\n\t\ttg3_flag_clear(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\treturn;\n\t}\n\n\ttw32_f(GRC_EEPROM_ADDR,\n\t     (EEPROM_ADDR_FSM_RESET |\n\t      (EEPROM_DEFAULT_CLOCK_PERIOD <<\n\t       EEPROM_ADDR_CLKPERD_SHIFT)));\n\n\tmsleep(1);\n\n\t/* Enable seeprom accesses. */\n\ttw32_f(GRC_LOCAL_CTRL,\n\t     tr32(GRC_LOCAL_CTRL) | GRC_LCLCTRL_AUTO_SEEPROM);\n\tudelay(100);\n\n\tif (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5701) {\n\t\ttg3_flag_set(tp, NVRAM);\n\n\t\tif (tg3_nvram_lock(tp)) {\n\t\t\tnetdev_warn(tp->dev,\n\t\t\t\t    \"Cannot get nvram lock, %s failed\\n\",\n\t\t\t\t    __func__);\n\t\t\treturn;\n\t\t}\n\t\ttg3_enable_nvram_access(tp);\n\n\t\ttp->nvram_size = 0;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5752)\n\t\t\ttg3_get_5752_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5755)\n\t\t\ttg3_get_5755_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5787 ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\t\ttg3_get_5787_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\t\ttg3_get_5761_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\t\ttg3_get_5906_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_57780 ||\n\t\t\t tg3_flag(tp, 57765_CLASS))\n\t\t\ttg3_get_57780_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\t\ttg3_get_5717_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\t\ttg3_get_5720_nvram_info(tp);\n\t\telse\n\t\t\ttg3_get_nvram_info(tp);\n\n\t\tif (tp->nvram_size == 0)\n\t\t\ttg3_get_nvram_size(tp);\n\n\t\ttg3_disable_nvram_access(tp);\n\t\ttg3_nvram_unlock(tp);\n\n\t} else {\n\t\ttg3_flag_clear(tp, NVRAM);\n\t\ttg3_flag_clear(tp, NVRAM_BUFFERED);\n\n\t\ttg3_get_eeprom_size(tp);\n\t}\n}\n\nstruct subsys_tbl_ent {\n\tu16 subsys_vendor, subsys_devid;\n\tu32 phy_id;\n};\n\nstatic struct subsys_tbl_ent subsys_id_to_phy_id[] = {\n\t/* Broadcom boards. */\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95700A6, TG3_PHY_ID_BCM5401 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701A5, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95700T6, TG3_PHY_ID_BCM8002 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95700A9, 0 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701T1, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701T8, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701A7, 0 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701A10, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701A12, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95703AX1, TG3_PHY_ID_BCM5703 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95703AX2, TG3_PHY_ID_BCM5703 },\n\n\t/* 3com boards. */\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C996T, TG3_PHY_ID_BCM5401 },\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C996BT, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C996SX, 0 },\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C1000T, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C940BR01, TG3_PHY_ID_BCM5701 },\n\n\t/* DELL boards. */\n\t{ TG3PCI_SUBVENDOR_ID_DELL,\n\t  TG3PCI_SUBDEVICE_ID_DELL_VIPER, TG3_PHY_ID_BCM5401 },\n\t{ TG3PCI_SUBVENDOR_ID_DELL,\n\t  TG3PCI_SUBDEVICE_ID_DELL_JAGUAR, TG3_PHY_ID_BCM5401 },\n\t{ TG3PCI_SUBVENDOR_ID_DELL,\n\t  TG3PCI_SUBDEVICE_ID_DELL_MERLOT, TG3_PHY_ID_BCM5411 },\n\t{ TG3PCI_SUBVENDOR_ID_DELL,\n\t  TG3PCI_SUBDEVICE_ID_DELL_SLIM_MERLOT, TG3_PHY_ID_BCM5411 },\n\n\t/* Compaq boards. */\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_BANSHEE, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_BANSHEE_2, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_CHANGELING, 0 },\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_NC7780, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_NC7780_2, TG3_PHY_ID_BCM5701 },\n\n\t/* IBM boards. */\n\t{ TG3PCI_SUBVENDOR_ID_IBM,\n\t  TG3PCI_SUBDEVICE_ID_IBM_5703SAX2, 0 }\n};\n\nstatic struct subsys_tbl_ent *tg3_lookup_by_subsys(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(subsys_id_to_phy_id); i++) {\n\t\tif ((subsys_id_to_phy_id[i].subsys_vendor ==\n\t\t     tp->pdev->subsystem_vendor) &&\n\t\t    (subsys_id_to_phy_id[i].subsys_devid ==\n\t\t     tp->pdev->subsystem_device))\n\t\t\treturn &subsys_id_to_phy_id[i];\n\t}\n\treturn NULL;\n}\n\nstatic void tg3_get_eeprom_hw_cfg(struct tg3 *tp)\n{\n\tu32 val;\n\n\ttp->phy_id = TG3_PHY_ID_INVALID;\n\ttp->led_ctrl = LED_CTRL_MODE_PHY_1;\n\n\t/* Assume an onboard device and WOL capable by default.  */\n\ttg3_flag_set(tp, EEPROM_WRITE_PROT);\n\ttg3_flag_set(tp, WOL_CAP);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tif (!(tr32(PCIE_TRANSACTION_CFG) & PCIE_TRANS_CFG_LOM)) {\n\t\t\ttg3_flag_clear(tp, EEPROM_WRITE_PROT);\n\t\t\ttg3_flag_set(tp, IS_NIC);\n\t\t}\n\t\tval = tr32(VCPU_CFGSHDW);\n\t\tif (val & VCPU_CFGSHDW_ASPM_DBNC)\n\t\t\ttg3_flag_set(tp, ASPM_WORKAROUND);\n\t\tif ((val & VCPU_CFGSHDW_WOL_ENABLE) &&\n\t\t    (val & VCPU_CFGSHDW_WOL_MAGPKT)) {\n\t\t\ttg3_flag_set(tp, WOL_ENABLE);\n\t\t\tdevice_set_wakeup_enable(&tp->pdev->dev, true);\n\t\t}\n\t\tgoto done;\n\t}\n\n\ttg3_read_mem(tp, NIC_SRAM_DATA_SIG, &val);\n\tif (val == NIC_SRAM_DATA_SIG_MAGIC) {\n\t\tu32 nic_cfg, led_cfg;\n\t\tu32 nic_phy_id, ver, cfg2 = 0, cfg4 = 0, eeprom_phy_id;\n\t\tint eeprom_phy_serdes = 0;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG, &nic_cfg);\n\t\ttp->nic_sram_data_cfg = nic_cfg;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_DATA_VER, &ver);\n\t\tver >>= NIC_SRAM_DATA_VER_SHIFT;\n\t\tif (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5701 &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5703 &&\n\t\t    (ver > 0) && (ver < 0x100))\n\t\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG_2, &cfg2);\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG_4, &cfg4);\n\n\t\tif ((nic_cfg & NIC_SRAM_DATA_CFG_PHY_TYPE_MASK) ==\n\t\t    NIC_SRAM_DATA_CFG_PHY_TYPE_FIBER)\n\t\t\teeprom_phy_serdes = 1;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_DATA_PHY_ID, &nic_phy_id);\n\t\tif (nic_phy_id != 0) {\n\t\t\tu32 id1 = nic_phy_id & NIC_SRAM_DATA_PHY_ID1_MASK;\n\t\t\tu32 id2 = nic_phy_id & NIC_SRAM_DATA_PHY_ID2_MASK;\n\n\t\t\teeprom_phy_id  = (id1 >> 16) << 10;\n\t\t\teeprom_phy_id |= (id2 & 0xfc00) << 16;\n\t\t\teeprom_phy_id |= (id2 & 0x03ff) <<  0;\n\t\t} else\n\t\t\teeprom_phy_id = 0;\n\n\t\ttp->phy_id = eeprom_phy_id;\n\t\tif (eeprom_phy_serdes) {\n\t\t\tif (!tg3_flag(tp, 5705_PLUS))\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_PHY_SERDES;\n\t\t\telse\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_MII_SERDES;\n\t\t}\n\n\t\tif (tg3_flag(tp, 5750_PLUS))\n\t\t\tled_cfg = cfg2 & (NIC_SRAM_DATA_CFG_LED_MODE_MASK |\n\t\t\t\t    SHASTA_EXT_LED_MODE_MASK);\n\t\telse\n\t\t\tled_cfg = nic_cfg & NIC_SRAM_DATA_CFG_LED_MODE_MASK;\n\n\t\tswitch (led_cfg) {\n\t\tdefault:\n\t\tcase NIC_SRAM_DATA_CFG_LED_MODE_PHY_1:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_1;\n\t\t\tbreak;\n\n\t\tcase NIC_SRAM_DATA_CFG_LED_MODE_PHY_2:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_2;\n\t\t\tbreak;\n\n\t\tcase NIC_SRAM_DATA_CFG_LED_MODE_MAC:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_MAC;\n\n\t\t\t/* Default to PHY_1_MODE if 0 (MAC_MODE) is\n\t\t\t * read on some older 5700/5701 bootcode.\n\t\t\t */\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5701)\n\t\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_1;\n\n\t\t\tbreak;\n\n\t\tcase SHASTA_EXT_LED_SHARED:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_SHARED;\n\t\t\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A0 &&\n\t\t\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A1)\n\t\t\t\ttp->led_ctrl |= (LED_CTRL_MODE_PHY_1 |\n\t\t\t\t\t\t LED_CTRL_MODE_PHY_2);\n\t\t\tbreak;\n\n\t\tcase SHASTA_EXT_LED_MAC:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_SHASTA_MAC;\n\t\t\tbreak;\n\n\t\tcase SHASTA_EXT_LED_COMBO:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_COMBO;\n\t\t\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A0)\n\t\t\t\ttp->led_ctrl |= (LED_CTRL_MODE_PHY_1 |\n\t\t\t\t\t\t LED_CTRL_MODE_PHY_2);\n\t\t\tbreak;\n\n\t\t}\n\n\t\tif ((tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t\t     tg3_asic_rev(tp) == ASIC_REV_5701) &&\n\t\t    tp->pdev->subsystem_vendor == PCI_VENDOR_ID_DELL)\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_2;\n\n\t\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX)\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_1;\n\n\t\tif (nic_cfg & NIC_SRAM_DATA_CFG_EEPROM_WP) {\n\t\t\ttg3_flag_set(tp, EEPROM_WRITE_PROT);\n\t\t\tif ((tp->pdev->subsystem_vendor ==\n\t\t\t     PCI_VENDOR_ID_ARIMA) &&\n\t\t\t    (tp->pdev->subsystem_device == 0x205a ||\n\t\t\t     tp->pdev->subsystem_device == 0x2063))\n\t\t\t\ttg3_flag_clear(tp, EEPROM_WRITE_PROT);\n\t\t} else {\n\t\t\ttg3_flag_clear(tp, EEPROM_WRITE_PROT);\n\t\t\ttg3_flag_set(tp, IS_NIC);\n\t\t}\n\n\t\tif (nic_cfg & NIC_SRAM_DATA_CFG_ASF_ENABLE) {\n\t\t\ttg3_flag_set(tp, ENABLE_ASF);\n\t\t\tif (tg3_flag(tp, 5750_PLUS))\n\t\t\t\ttg3_flag_set(tp, ASF_NEW_HANDSHAKE);\n\t\t}\n\n\t\tif ((nic_cfg & NIC_SRAM_DATA_CFG_APE_ENABLE) &&\n\t\t    tg3_flag(tp, 5750_PLUS))\n\t\t\ttg3_flag_set(tp, ENABLE_APE);\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_ANY_SERDES &&\n\t\t    !(nic_cfg & NIC_SRAM_DATA_CFG_FIBER_WOL))\n\t\t\ttg3_flag_clear(tp, WOL_CAP);\n\n\t\tif (tg3_flag(tp, WOL_CAP) &&\n\t\t    (nic_cfg & NIC_SRAM_DATA_CFG_WOL_ENABLE)) {\n\t\t\ttg3_flag_set(tp, WOL_ENABLE);\n\t\t\tdevice_set_wakeup_enable(&tp->pdev->dev, true);\n\t\t}\n\n\t\tif (cfg2 & (1 << 17))\n\t\t\ttp->phy_flags |= TG3_PHYFLG_CAPACITIVE_COUPLING;\n\n\t\t/* serdes signal pre-emphasis in register 0x590 set by */\n\t\t/* bootcode if bit 18 is set */\n\t\tif (cfg2 & (1 << 18))\n\t\t\ttp->phy_flags |= TG3_PHYFLG_SERDES_PREEMPHASIS;\n\n\t\tif ((tg3_flag(tp, 57765_PLUS) ||\n\t\t     (tg3_asic_rev(tp) == ASIC_REV_5784 &&\n\t\t      tg3_chip_rev(tp) != CHIPREV_5784_AX)) &&\n\t\t    (cfg2 & NIC_SRAM_DATA_CFG_2_APD_EN))\n\t\t\ttp->phy_flags |= TG3_PHYFLG_ENABLE_APD;\n\n\t\tif (tg3_flag(tp, PCI_EXPRESS) &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5785 &&\n\t\t    !tg3_flag(tp, 57765_PLUS)) {\n\t\t\tu32 cfg3;\n\n\t\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG_3, &cfg3);\n\t\t\tif (cfg3 & NIC_SRAM_ASPM_DEBOUNCE)\n\t\t\t\ttg3_flag_set(tp, ASPM_WORKAROUND);\n\t\t}\n\n\t\tif (cfg4 & NIC_SRAM_RGMII_INBAND_DISABLE)\n\t\t\ttg3_flag_set(tp, RGMII_INBAND_DISABLE);\n\t\tif (cfg4 & NIC_SRAM_RGMII_EXT_IBND_RX_EN)\n\t\t\ttg3_flag_set(tp, RGMII_EXT_IBND_RX_EN);\n\t\tif (cfg4 & NIC_SRAM_RGMII_EXT_IBND_TX_EN)\n\t\t\ttg3_flag_set(tp, RGMII_EXT_IBND_TX_EN);\n\t}\ndone:\n\tif (tg3_flag(tp, WOL_CAP))\n\t\tdevice_set_wakeup_enable(&tp->pdev->dev,\n\t\t\t\t\t tg3_flag(tp, WOL_ENABLE));\n\telse\n\t\tdevice_set_wakeup_capable(&tp->pdev->dev, false);\n}\n\nstatic int tg3_ape_otp_read(struct tg3 *tp, u32 offset, u32 *val)\n{\n\tint i, err;\n\tu32 val2, off = offset * 8;\n\n\terr = tg3_nvram_lock(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_ape_write32(tp, TG3_APE_OTP_ADDR, off | APE_OTP_ADDR_CPU_ENABLE);\n\ttg3_ape_write32(tp, TG3_APE_OTP_CTRL, APE_OTP_CTRL_PROG_EN |\n\t\t\tAPE_OTP_CTRL_CMD_RD | APE_OTP_CTRL_START);\n\ttg3_ape_read32(tp, TG3_APE_OTP_CTRL);\n\tudelay(10);\n\n\tfor (i = 0; i < 100; i++) {\n\t\tval2 = tg3_ape_read32(tp, TG3_APE_OTP_STATUS);\n\t\tif (val2 & APE_OTP_STATUS_CMD_DONE) {\n\t\t\t*val = tg3_ape_read32(tp, TG3_APE_OTP_RD_DATA);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(10);\n\t}\n\n\ttg3_ape_write32(tp, TG3_APE_OTP_CTRL, 0);\n\n\ttg3_nvram_unlock(tp);\n\tif (val2 & APE_OTP_STATUS_CMD_DONE)\n\t\treturn 0;\n\n\treturn -EBUSY;\n}\n\nstatic int tg3_issue_otp_command(struct tg3 *tp, u32 cmd)\n{\n\tint i;\n\tu32 val;\n\n\ttw32(OTP_CTRL, cmd | OTP_CTRL_OTP_CMD_START);\n\ttw32(OTP_CTRL, cmd);\n\n\t/* Wait for up to 1 ms for command to execute. */\n\tfor (i = 0; i < 100; i++) {\n\t\tval = tr32(OTP_STATUS);\n\t\tif (val & OTP_STATUS_CMD_DONE)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\treturn (val & OTP_STATUS_CMD_DONE) ? 0 : -EBUSY;\n}\n\n/* Read the gphy configuration from the OTP region of the chip.  The gphy\n * configuration is a 32-bit value that straddles the alignment boundary.\n * We do two 32-bit reads and then shift and merge the results.\n */\nstatic u32 tg3_read_otp_phycfg(struct tg3 *tp)\n{\n\tu32 bhalf_otp, thalf_otp;\n\n\ttw32(OTP_MODE, OTP_MODE_OTP_THRU_GRC);\n\n\tif (tg3_issue_otp_command(tp, OTP_CTRL_OTP_CMD_INIT))\n\t\treturn 0;\n\n\ttw32(OTP_ADDRESS, OTP_ADDRESS_MAGIC1);\n\n\tif (tg3_issue_otp_command(tp, OTP_CTRL_OTP_CMD_READ))\n\t\treturn 0;\n\n\tthalf_otp = tr32(OTP_READ_DATA);\n\n\ttw32(OTP_ADDRESS, OTP_ADDRESS_MAGIC2);\n\n\tif (tg3_issue_otp_command(tp, OTP_CTRL_OTP_CMD_READ))\n\t\treturn 0;\n\n\tbhalf_otp = tr32(OTP_READ_DATA);\n\n\treturn ((thalf_otp & 0x0000ffff) << 16) | (bhalf_otp >> 16);\n}\n\nstatic void tg3_phy_init_link_config(struct tg3 *tp)\n{\n\tu32 adv = ADVERTISED_Autoneg;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY))\n\t\tadv |= ADVERTISED_1000baseT_Half |\n\t\t       ADVERTISED_1000baseT_Full;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\tadv |= ADVERTISED_100baseT_Half |\n\t\t       ADVERTISED_100baseT_Full |\n\t\t       ADVERTISED_10baseT_Half |\n\t\t       ADVERTISED_10baseT_Full |\n\t\t       ADVERTISED_TP;\n\telse\n\t\tadv |= ADVERTISED_FIBRE;\n\n\ttp->link_config.advertising = adv;\n\ttp->link_config.speed = SPEED_UNKNOWN;\n\ttp->link_config.duplex = DUPLEX_UNKNOWN;\n\ttp->link_config.autoneg = AUTONEG_ENABLE;\n\ttp->link_config.active_speed = SPEED_UNKNOWN;\n\ttp->link_config.active_duplex = DUPLEX_UNKNOWN;\n\n\ttp->old_link = -1;\n}\n\nstatic int tg3_phy_probe(struct tg3 *tp)\n{\n\tu32 hw_phy_id_1, hw_phy_id_2;\n\tu32 hw_phy_id, hw_phy_id_masked;\n\tint err;\n\n\t/* flow control autonegotiation is default behavior */\n\ttg3_flag_set(tp, PAUSE_AUTONEG);\n\ttp->link_config.flowctrl = FLOW_CTRL_TX | FLOW_CTRL_RX;\n\n\tif (tg3_flag(tp, ENABLE_APE)) {\n\t\tswitch (tp->pci_fn) {\n\t\tcase 0:\n\t\t\ttp->phy_ape_lock = TG3_APE_LOCK_PHY0;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\ttp->phy_ape_lock = TG3_APE_LOCK_PHY1;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\ttp->phy_ape_lock = TG3_APE_LOCK_PHY2;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\ttp->phy_ape_lock = TG3_APE_LOCK_PHY3;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, USE_PHYLIB))\n\t\treturn tg3_phy_init(tp);\n\n\t/* Reading the PHY ID register can conflict with ASF\n\t * firmware access to the PHY hardware.\n\t */\n\terr = 0;\n\tif (tg3_flag(tp, ENABLE_ASF) || tg3_flag(tp, ENABLE_APE)) {\n\t\thw_phy_id = hw_phy_id_masked = TG3_PHY_ID_INVALID;\n\t} else {\n\t\t/* Now read the physical PHY_ID from the chip and verify\n\t\t * that it is sane.  If it doesn't look good, we fall back\n\t\t * to either the hard-coded table based PHY_ID and failing\n\t\t * that the value found in the eeprom area.\n\t\t */\n\t\terr |= tg3_readphy(tp, MII_PHYSID1, &hw_phy_id_1);\n\t\terr |= tg3_readphy(tp, MII_PHYSID2, &hw_phy_id_2);\n\n\t\thw_phy_id  = (hw_phy_id_1 & 0xffff) << 10;\n\t\thw_phy_id |= (hw_phy_id_2 & 0xfc00) << 16;\n\t\thw_phy_id |= (hw_phy_id_2 & 0x03ff) <<  0;\n\n\t\thw_phy_id_masked = hw_phy_id & TG3_PHY_ID_MASK;\n\t}\n\n\tif (!err && TG3_KNOWN_PHY_ID(hw_phy_id_masked)) {\n\t\ttp->phy_id = hw_phy_id;\n\t\tif (hw_phy_id_masked == TG3_PHY_ID_BCM8002)\n\t\t\ttp->phy_flags |= TG3_PHYFLG_PHY_SERDES;\n\t\telse\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PHY_SERDES;\n\t} else {\n\t\tif (tp->phy_id != TG3_PHY_ID_INVALID) {\n\t\t\t/* Do nothing, phy ID already set up in\n\t\t\t * tg3_get_eeprom_hw_cfg().\n\t\t\t */\n\t\t} else {\n\t\t\tstruct subsys_tbl_ent *p;\n\n\t\t\t/* No eeprom signature?  Try the hardcoded\n\t\t\t * subsys device table.\n\t\t\t */\n\t\t\tp = tg3_lookup_by_subsys(tp);\n\t\t\tif (p) {\n\t\t\t\ttp->phy_id = p->phy_id;\n\t\t\t} else if (!tg3_flag(tp, IS_SSB_CORE)) {\n\t\t\t\t/* For now we saw the IDs 0xbc050cd0,\n\t\t\t\t * 0xbc050f80 and 0xbc050c30 on devices\n\t\t\t\t * connected to an BCM4785 and there are\n\t\t\t\t * probably more. Just assume that the phy is\n\t\t\t\t * supported when it is connected to a SSB core\n\t\t\t\t * for now.\n\t\t\t\t */\n\t\t\t\treturn -ENODEV;\n\t\t\t}\n\n\t\t\tif (!tp->phy_id ||\n\t\t\t    tp->phy_id == TG3_PHY_ID_BCM8002)\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_PHY_SERDES;\n\t\t}\n\t}\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES) &&\n\t    (tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5762 ||\n\t     (tg3_asic_rev(tp) == ASIC_REV_5717 &&\n\t      tg3_chip_rev_id(tp) != CHIPREV_ID_5717_A0) ||\n\t     (tg3_asic_rev(tp) == ASIC_REV_57765 &&\n\t      tg3_chip_rev_id(tp) != CHIPREV_ID_57765_A0)))\n\t\ttp->phy_flags |= TG3_PHYFLG_EEE_CAP;\n\n\ttg3_phy_init_link_config(tp);\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES) &&\n\t    !tg3_flag(tp, ENABLE_APE) &&\n\t    !tg3_flag(tp, ENABLE_ASF)) {\n\t\tu32 bmsr, dummy;\n\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif (!tg3_readphy(tp, MII_BMSR, &bmsr) &&\n\t\t    (bmsr & BMSR_LSTATUS))\n\t\t\tgoto skip_phy_reset;\n\n\t\terr = tg3_phy_reset(tp);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\ttg3_phy_set_wirespeed(tp);\n\n\t\tif (!tg3_phy_copper_an_config_ok(tp, &dummy)) {\n\t\t\ttg3_phy_autoneg_cfg(tp, tp->link_config.advertising,\n\t\t\t\t\t    tp->link_config.flowctrl);\n\n\t\t\ttg3_writephy(tp, MII_BMCR,\n\t\t\t\t     BMCR_ANENABLE | BMCR_ANRESTART);\n\t\t}\n\t}\n\nskip_phy_reset:\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5401) {\n\t\terr = tg3_init_5401phy_dsp(tp);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = tg3_init_5401phy_dsp(tp);\n\t}\n\n\treturn err;\n}\n\nstatic void tg3_read_vpd(struct tg3 *tp)\n{\n\tu8 *vpd_data;\n\tunsigned int block_end, rosize, len;\n\tu32 vpdlen;\n\tint j, i = 0;\n\n\tvpd_data = (u8 *)tg3_vpd_readblock(tp, &vpdlen);\n\tif (!vpd_data)\n\t\tgoto out_no_vpd;\n\n\ti = pci_vpd_find_tag(vpd_data, 0, vpdlen, PCI_VPD_LRDT_RO_DATA);\n\tif (i < 0)\n\t\tgoto out_not_found;\n\n\trosize = pci_vpd_lrdt_size(&vpd_data[i]);\n\tblock_end = i + PCI_VPD_LRDT_TAG_SIZE + rosize;\n\ti += PCI_VPD_LRDT_TAG_SIZE;\n\n\tif (block_end > vpdlen)\n\t\tgoto out_not_found;\n\n\tj = pci_vpd_find_info_keyword(vpd_data, i, rosize,\n\t\t\t\t      PCI_VPD_RO_KEYWORD_MFR_ID);\n\tif (j > 0) {\n\t\tlen = pci_vpd_info_field_size(&vpd_data[j]);\n\n\t\tj += PCI_VPD_INFO_FLD_HDR_SIZE;\n\t\tif (j + len > block_end || len != 4 ||\n\t\t    memcmp(&vpd_data[j], \"1028\", 4))\n\t\t\tgoto partno;\n\n\t\tj = pci_vpd_find_info_keyword(vpd_data, i, rosize,\n\t\t\t\t\t      PCI_VPD_RO_KEYWORD_VENDOR0);\n\t\tif (j < 0)\n\t\t\tgoto partno;\n\n\t\tlen = pci_vpd_info_field_size(&vpd_data[j]);\n\n\t\tj += PCI_VPD_INFO_FLD_HDR_SIZE;\n\t\tif (j + len > block_end)\n\t\t\tgoto partno;\n\n\t\tmemcpy(tp->fw_ver, &vpd_data[j], len);\n\t\tstrncat(tp->fw_ver, \" bc \", vpdlen - len - 1);\n\t}\n\npartno:\n\ti = pci_vpd_find_info_keyword(vpd_data, i, rosize,\n\t\t\t\t      PCI_VPD_RO_KEYWORD_PARTNO);\n\tif (i < 0)\n\t\tgoto out_not_found;\n\n\tlen = pci_vpd_info_field_size(&vpd_data[i]);\n\n\ti += PCI_VPD_INFO_FLD_HDR_SIZE;\n\tif (len > TG3_BPN_SIZE ||\n\t    (len + i) > vpdlen)\n\t\tgoto out_not_found;\n\n\tmemcpy(tp->board_part_number, &vpd_data[i], len);\n\nout_not_found:\n\tkfree(vpd_data);\n\tif (tp->board_part_number[0])\n\t\treturn;\n\nout_no_vpd:\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717) {\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717_C)\n\t\t\tstrcpy(tp->board_part_number, \"BCM5717\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_5718)\n\t\t\tstrcpy(tp->board_part_number, \"BCM5718\");\n\t\telse\n\t\t\tgoto nomatch;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_57780) {\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57780)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57780\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57760)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57760\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57790)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57790\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57788)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57788\");\n\t\telse\n\t\t\tgoto nomatch;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_57765) {\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57761)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57761\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57765)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57765\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57781)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57781\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57785)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57785\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57791)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57791\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57795)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57795\");\n\t\telse\n\t\t\tgoto nomatch;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_57766) {\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57762)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57762\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57766)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57766\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57782)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57782\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57786)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57786\");\n\t\telse\n\t\t\tgoto nomatch;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tstrcpy(tp->board_part_number, \"BCM95906\");\n\t} else {\nnomatch:\n\t\tstrcpy(tp->board_part_number, \"none\");\n\t}\n}\n\nstatic int tg3_fw_img_is_valid(struct tg3 *tp, u32 offset)\n{\n\tu32 val;\n\n\tif (tg3_nvram_read(tp, offset, &val) ||\n\t    (val & 0xfc000000) != 0x0c000000 ||\n\t    tg3_nvram_read(tp, offset + 4, &val) ||\n\t    val != 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic void tg3_read_bc_ver(struct tg3 *tp)\n{\n\tu32 val, offset, start, ver_offset;\n\tint i, dst_off;\n\tbool newver = false;\n\n\tif (tg3_nvram_read(tp, 0xc, &offset) ||\n\t    tg3_nvram_read(tp, 0x4, &start))\n\t\treturn;\n\n\toffset = tg3_nvram_logical_addr(tp, offset);\n\n\tif (tg3_nvram_read(tp, offset, &val))\n\t\treturn;\n\n\tif ((val & 0xfc000000) == 0x0c000000) {\n\t\tif (tg3_nvram_read(tp, offset + 4, &val))\n\t\t\treturn;\n\n\t\tif (val == 0)\n\t\t\tnewver = true;\n\t}\n\n\tdst_off = strlen(tp->fw_ver);\n\n\tif (newver) {\n\t\tif (TG3_VER_SIZE - dst_off < 16 ||\n\t\t    tg3_nvram_read(tp, offset + 8, &ver_offset))\n\t\t\treturn;\n\n\t\toffset = offset + ver_offset - start;\n\t\tfor (i = 0; i < 16; i += 4) {\n\t\t\t__be32 v;\n\t\t\tif (tg3_nvram_read_be32(tp, offset + i, &v))\n\t\t\t\treturn;\n\n\t\t\tmemcpy(tp->fw_ver + dst_off + i, &v, sizeof(v));\n\t\t}\n\t} else {\n\t\tu32 major, minor;\n\n\t\tif (tg3_nvram_read(tp, TG3_NVM_PTREV_BCVER, &ver_offset))\n\t\t\treturn;\n\n\t\tmajor = (ver_offset & TG3_NVM_BCVER_MAJMSK) >>\n\t\t\tTG3_NVM_BCVER_MAJSFT;\n\t\tminor = ver_offset & TG3_NVM_BCVER_MINMSK;\n\t\tsnprintf(&tp->fw_ver[dst_off], TG3_VER_SIZE - dst_off,\n\t\t\t \"v%d.%02d\", major, minor);\n\t}\n}\n\nstatic void tg3_read_hwsb_ver(struct tg3 *tp)\n{\n\tu32 val, major, minor;\n\n\t/* Use native endian representation */\n\tif (tg3_nvram_read(tp, TG3_NVM_HWSB_CFG1, &val))\n\t\treturn;\n\n\tmajor = (val & TG3_NVM_HWSB_CFG1_MAJMSK) >>\n\t\tTG3_NVM_HWSB_CFG1_MAJSFT;\n\tminor = (val & TG3_NVM_HWSB_CFG1_MINMSK) >>\n\t\tTG3_NVM_HWSB_CFG1_MINSFT;\n\n\tsnprintf(&tp->fw_ver[0], 32, \"sb v%d.%02d\", major, minor);\n}\n\nstatic void tg3_read_sb_ver(struct tg3 *tp, u32 val)\n{\n\tu32 offset, major, minor, build;\n\n\tstrncat(tp->fw_ver, \"sb\", TG3_VER_SIZE - strlen(tp->fw_ver) - 1);\n\n\tif ((val & TG3_EEPROM_SB_FORMAT_MASK) != TG3_EEPROM_SB_FORMAT_1)\n\t\treturn;\n\n\tswitch (val & TG3_EEPROM_SB_REVISION_MASK) {\n\tcase TG3_EEPROM_SB_REVISION_0:\n\t\toffset = TG3_EEPROM_SB_F1R0_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_2:\n\t\toffset = TG3_EEPROM_SB_F1R2_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_3:\n\t\toffset = TG3_EEPROM_SB_F1R3_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_4:\n\t\toffset = TG3_EEPROM_SB_F1R4_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_5:\n\t\toffset = TG3_EEPROM_SB_F1R5_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_6:\n\t\toffset = TG3_EEPROM_SB_F1R6_EDH_OFF;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (tg3_nvram_read(tp, offset, &val))\n\t\treturn;\n\n\tbuild = (val & TG3_EEPROM_SB_EDH_BLD_MASK) >>\n\t\tTG3_EEPROM_SB_EDH_BLD_SHFT;\n\tmajor = (val & TG3_EEPROM_SB_EDH_MAJ_MASK) >>\n\t\tTG3_EEPROM_SB_EDH_MAJ_SHFT;\n\tminor =  val & TG3_EEPROM_SB_EDH_MIN_MASK;\n\n\tif (minor > 99 || build > 26)\n\t\treturn;\n\n\toffset = strlen(tp->fw_ver);\n\tsnprintf(&tp->fw_ver[offset], TG3_VER_SIZE - offset,\n\t\t \" v%d.%02d\", major, minor);\n\n\tif (build > 0) {\n\t\toffset = strlen(tp->fw_ver);\n\t\tif (offset < TG3_VER_SIZE - 1)\n\t\t\ttp->fw_ver[offset] = 'a' + build - 1;\n\t}\n}\n\nstatic void tg3_read_mgmtfw_ver(struct tg3 *tp)\n{\n\tu32 val, offset, start;\n\tint i, vlen;\n\n\tfor (offset = TG3_NVM_DIR_START;\n\t     offset < TG3_NVM_DIR_END;\n\t     offset += TG3_NVM_DIRENT_SIZE) {\n\t\tif (tg3_nvram_read(tp, offset, &val))\n\t\t\treturn;\n\n\t\tif ((val >> TG3_NVM_DIRTYPE_SHIFT) == TG3_NVM_DIRTYPE_ASFINI)\n\t\t\tbreak;\n\t}\n\n\tif (offset == TG3_NVM_DIR_END)\n\t\treturn;\n\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\tstart = 0x08000000;\n\telse if (tg3_nvram_read(tp, offset - 4, &start))\n\t\treturn;\n\n\tif (tg3_nvram_read(tp, offset + 4, &offset) ||\n\t    !tg3_fw_img_is_valid(tp, offset) ||\n\t    tg3_nvram_read(tp, offset + 8, &val))\n\t\treturn;\n\n\toffset += val - start;\n\n\tvlen = strlen(tp->fw_ver);\n\n\ttp->fw_ver[vlen++] = ',';\n\ttp->fw_ver[vlen++] = ' ';\n\n\tfor (i = 0; i < 4; i++) {\n\t\t__be32 v;\n\t\tif (tg3_nvram_read_be32(tp, offset, &v))\n\t\t\treturn;\n\n\t\toffset += sizeof(v);\n\n\t\tif (vlen > TG3_VER_SIZE - sizeof(v)) {\n\t\t\tmemcpy(&tp->fw_ver[vlen], &v, TG3_VER_SIZE - vlen);\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(&tp->fw_ver[vlen], &v, sizeof(v));\n\t\tvlen += sizeof(v);\n\t}\n}\n\nstatic void tg3_probe_ncsi(struct tg3 *tp)\n{\n\tu32 apedata;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_SEG_SIG);\n\tif (apedata != APE_SEG_SIG_MAGIC)\n\t\treturn;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_FW_STATUS);\n\tif (!(apedata & APE_FW_STATUS_READY))\n\t\treturn;\n\n\tif (tg3_ape_read32(tp, TG3_APE_FW_FEATURES) & TG3_APE_FW_FEATURE_NCSI)\n\t\ttg3_flag_set(tp, APE_HAS_NCSI);\n}\n\nstatic void tg3_read_dash_ver(struct tg3 *tp)\n{\n\tint vlen;\n\tu32 apedata;\n\tchar *fwtype;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_FW_VERSION);\n\n\tif (tg3_flag(tp, APE_HAS_NCSI))\n\t\tfwtype = \"NCSI\";\n\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_5725)\n\t\tfwtype = \"SMASH\";\n\telse\n\t\tfwtype = \"DASH\";\n\n\tvlen = strlen(tp->fw_ver);\n\n\tsnprintf(&tp->fw_ver[vlen], TG3_VER_SIZE - vlen, \" %s v%d.%d.%d.%d\",\n\t\t fwtype,\n\t\t (apedata & APE_FW_VERSION_MAJMSK) >> APE_FW_VERSION_MAJSFT,\n\t\t (apedata & APE_FW_VERSION_MINMSK) >> APE_FW_VERSION_MINSFT,\n\t\t (apedata & APE_FW_VERSION_REVMSK) >> APE_FW_VERSION_REVSFT,\n\t\t (apedata & APE_FW_VERSION_BLDMSK));\n}\n\nstatic void tg3_read_otp_ver(struct tg3 *tp)\n{\n\tu32 val, val2;\n\n\tif (tg3_asic_rev(tp) != ASIC_REV_5762)\n\t\treturn;\n\n\tif (!tg3_ape_otp_read(tp, OTP_ADDRESS_MAGIC0, &val) &&\n\t    !tg3_ape_otp_read(tp, OTP_ADDRESS_MAGIC0 + 4, &val2) &&\n\t    TG3_OTP_MAGIC0_VALID(val)) {\n\t\tu64 val64 = (u64) val << 32 | val2;\n\t\tu32 ver = 0;\n\t\tint i, vlen;\n\n\t\tfor (i = 0; i < 7; i++) {\n\t\t\tif ((val64 & 0xff) == 0)\n\t\t\t\tbreak;\n\t\t\tver = val64 & 0xff;\n\t\t\tval64 >>= 8;\n\t\t}\n\t\tvlen = strlen(tp->fw_ver);\n\t\tsnprintf(&tp->fw_ver[vlen], TG3_VER_SIZE - vlen, \" .%02d\", ver);\n\t}\n}\n\nstatic void tg3_read_fw_ver(struct tg3 *tp)\n{\n\tu32 val;\n\tbool vpd_vers = false;\n\n\tif (tp->fw_ver[0] != 0)\n\t\tvpd_vers = true;\n\n\tif (tg3_flag(tp, NO_NVRAM)) {\n\t\tstrcat(tp->fw_ver, \"sb\");\n\t\ttg3_read_otp_ver(tp);\n\t\treturn;\n\t}\n\n\tif (tg3_nvram_read(tp, 0, &val))\n\t\treturn;\n\n\tif (val == TG3_EEPROM_MAGIC)\n\t\ttg3_read_bc_ver(tp);\n\telse if ((val & TG3_EEPROM_MAGIC_FW_MSK) == TG3_EEPROM_MAGIC_FW)\n\t\ttg3_read_sb_ver(tp, val);\n\telse if ((val & TG3_EEPROM_MAGIC_HW_MSK) == TG3_EEPROM_MAGIC_HW)\n\t\ttg3_read_hwsb_ver(tp);\n\n\tif (tg3_flag(tp, ENABLE_ASF)) {\n\t\tif (tg3_flag(tp, ENABLE_APE)) {\n\t\t\ttg3_probe_ncsi(tp);\n\t\t\tif (!vpd_vers)\n\t\t\t\ttg3_read_dash_ver(tp);\n\t\t} else if (!vpd_vers) {\n\t\t\ttg3_read_mgmtfw_ver(tp);\n\t\t}\n\t}\n\n\ttp->fw_ver[TG3_VER_SIZE - 1] = 0;\n}\n\nstatic inline u32 tg3_rx_ret_ring_size(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, LRG_PROD_RING_CAP))\n\t\treturn TG3_RX_RET_MAX_SIZE_5717;\n\telse if (tg3_flag(tp, JUMBO_CAPABLE) && !tg3_flag(tp, 5780_CLASS))\n\t\treturn TG3_RX_RET_MAX_SIZE_5700;\n\telse\n\t\treturn TG3_RX_RET_MAX_SIZE_5705;\n}\n\nstatic DEFINE_PCI_DEVICE_TABLE(tg3_write_reorder_chipsets) = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_FE_GATE_700C) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_8131_BRIDGE) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_VIA, PCI_DEVICE_ID_VIA_8385_0) },\n\t{ },\n};\n\nstatic struct pci_dev *tg3_find_peer(struct tg3 *tp)\n{\n\tstruct pci_dev *peer;\n\tunsigned int func, devnr = tp->pdev->devfn & ~7;\n\n\tfor (func = 0; func < 8; func++) {\n\t\tpeer = pci_get_slot(tp->pdev->bus, devnr | func);\n\t\tif (peer && peer != tp->pdev)\n\t\t\tbreak;\n\t\tpci_dev_put(peer);\n\t}\n\t/* 5704 can be configured in single-port mode, set peer to\n\t * tp->pdev in that case.\n\t */\n\tif (!peer) {\n\t\tpeer = tp->pdev;\n\t\treturn peer;\n\t}\n\n\t/*\n\t * We don't need to keep the refcount elevated; there's no way\n\t * to remove one half of this device without removing the other\n\t */\n\tpci_dev_put(peer);\n\n\treturn peer;\n}\n\nstatic void tg3_detect_asic_rev(struct tg3 *tp, u32 misc_ctrl_reg)\n{\n\ttp->pci_chip_rev_id = misc_ctrl_reg >> MISC_HOST_CTRL_CHIPREV_SHIFT;\n\tif (tg3_asic_rev(tp) == ASIC_REV_USE_PROD_ID_REG) {\n\t\tu32 reg;\n\n\t\t/* All devices that use the alternate\n\t\t * ASIC REV location have a CPMU.\n\t\t */\n\t\ttg3_flag_set(tp, CPMU_PRESENT);\n\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717_C ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5718 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5719 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5720 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5762 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5725 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5727)\n\t\t\treg = TG3PCI_GEN2_PRODID_ASICREV;\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57781 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57785 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57761 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57765 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57791 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57795 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57762 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57766 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57782 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57786)\n\t\t\treg = TG3PCI_GEN15_PRODID_ASICREV;\n\t\telse\n\t\t\treg = TG3PCI_PRODID_ASICREV;\n\n\t\tpci_read_config_dword(tp->pdev, reg, &tp->pci_chip_rev_id);\n\t}\n\n\t/* Wrong chip ID in 5752 A0. This code can be removed later\n\t * as A0 is not in production.\n\t */\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5752_A0_HW)\n\t\ttp->pci_chip_rev_id = CHIPREV_ID_5752_A0;\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5717_C0)\n\t\ttp->pci_chip_rev_id = CHIPREV_ID_5720_A0;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720)\n\t\ttg3_flag_set(tp, 5717_PLUS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_57765 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57766)\n\t\ttg3_flag_set(tp, 57765_CLASS);\n\n\tif (tg3_flag(tp, 57765_CLASS) || tg3_flag(tp, 5717_PLUS) ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\ttg3_flag_set(tp, 57765_PLUS);\n\n\t/* Intentionally exclude ASIC_REV_5906 */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5787 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5761 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780 ||\n\t    tg3_flag(tp, 57765_PLUS))\n\t\ttg3_flag_set(tp, 5755_PLUS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5780 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5714)\n\t\ttg3_flag_set(tp, 5780_CLASS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5750 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5752 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5906 ||\n\t    tg3_flag(tp, 5755_PLUS) ||\n\t    tg3_flag(tp, 5780_CLASS))\n\t\ttg3_flag_set(tp, 5750_PLUS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705 ||\n\t    tg3_flag(tp, 5750_PLUS))\n\t\ttg3_flag_set(tp, 5705_PLUS);\n}\n\nstatic bool tg3_10_100_only_device(struct tg3 *tp,\n\t\t\t\t   const struct pci_device_id *ent)\n{\n\tu32 grc_misc_cfg = tr32(GRC_MISC_CFG) & GRC_MISC_CFG_BOARD_ID_MASK;\n\n\tif ((tg3_asic_rev(tp) == ASIC_REV_5703 &&\n\t     (grc_misc_cfg == 0x8000 || grc_misc_cfg == 0x4000)) ||\n\t    (tp->phy_flags & TG3_PHYFLG_IS_FET))\n\t\treturn true;\n\n\tif (ent->driver_data & TG3_DRV_DATA_FLAG_10_100_ONLY) {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\t\tif (ent->driver_data & TG3_DRV_DATA_FLAG_5705_10_100)\n\t\t\t\treturn true;\n\t\t} else {\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int tg3_get_invariants(struct tg3 *tp, const struct pci_device_id *ent)\n{\n\tu32 misc_ctrl_reg;\n\tu32 pci_state_reg, grc_misc_cfg;\n\tu32 val;\n\tu16 pci_cmd;\n\tint err;\n\n\t/* Force memory write invalidate off.  If we leave it on,\n\t * then on 5700_BX chips we have to enable a workaround.\n\t * The workaround is to set the TG3PCI_DMA_RW_CTRL boundary\n\t * to match the cacheline size.  The Broadcom driver have this\n\t * workaround but turns MWI off all the times so never uses\n\t * it.  This seems to suggest that the workaround is insufficient.\n\t */\n\tpci_read_config_word(tp->pdev, PCI_COMMAND, &pci_cmd);\n\tpci_cmd &= ~PCI_COMMAND_INVALIDATE;\n\tpci_write_config_word(tp->pdev, PCI_COMMAND, pci_cmd);\n\n\t/* Important! -- Make sure register accesses are byteswapped\n\t * correctly.  Also, for those chips that require it, make\n\t * sure that indirect register accesses are enabled before\n\t * the first operation.\n\t */\n\tpci_read_config_dword(tp->pdev, TG3PCI_MISC_HOST_CTRL,\n\t\t\t      &misc_ctrl_reg);\n\ttp->misc_host_ctrl |= (misc_ctrl_reg &\n\t\t\t       MISC_HOST_CTRL_CHIPREV);\n\tpci_write_config_dword(tp->pdev, TG3PCI_MISC_HOST_CTRL,\n\t\t\t       tp->misc_host_ctrl);\n\n\ttg3_detect_asic_rev(tp, misc_ctrl_reg);\n\n\t/* If we have 5702/03 A1 or A2 on certain ICH chipsets,\n\t * we need to disable memory and use config. cycles\n\t * only to access all registers. The 5702/03 chips\n\t * can mistakenly decode the special cycles from the\n\t * ICH chipsets as memory write cycles, causing corruption\n\t * of register and memory space. Only certain ICH bridges\n\t * will drive special cycles with non-zero data during the\n\t * address phase which can fall within the 5703's address\n\t * range. This is not an ICH bug as the PCI spec allows\n\t * non-zero address during special cycles. However, only\n\t * these ICH bridges are known to drive non-zero addresses\n\t * during special cycles.\n\t *\n\t * Since special cycles do not cross PCI bridges, we only\n\t * enable this workaround if the 5703 is on the secondary\n\t * bus of these ICH bridges.\n\t */\n\tif ((tg3_chip_rev_id(tp) == CHIPREV_ID_5703_A1) ||\n\t    (tg3_chip_rev_id(tp) == CHIPREV_ID_5703_A2)) {\n\t\tstatic struct tg3_dev_id {\n\t\t\tu32\tvendor;\n\t\t\tu32\tdevice;\n\t\t\tu32\trev;\n\t\t} ich_chipsets[] = {\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801AA_8,\n\t\t\t  PCI_ANY_ID },\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801AB_8,\n\t\t\t  PCI_ANY_ID },\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801BA_11,\n\t\t\t  0xa },\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801BA_6,\n\t\t\t  PCI_ANY_ID },\n\t\t\t{ },\n\t\t};\n\t\tstruct tg3_dev_id *pci_id = &ich_chipsets[0];\n\t\tstruct pci_dev *bridge = NULL;\n\n\t\twhile (pci_id->vendor != 0) {\n\t\t\tbridge = pci_get_device(pci_id->vendor, pci_id->device,\n\t\t\t\t\t\tbridge);\n\t\t\tif (!bridge) {\n\t\t\t\tpci_id++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (pci_id->rev != PCI_ANY_ID) {\n\t\t\t\tif (bridge->revision > pci_id->rev)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (bridge->subordinate &&\n\t\t\t    (bridge->subordinate->number ==\n\t\t\t     tp->pdev->bus->number)) {\n\t\t\t\ttg3_flag_set(tp, ICH_WORKAROUND);\n\t\t\t\tpci_dev_put(bridge);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\tstatic struct tg3_dev_id {\n\t\t\tu32\tvendor;\n\t\t\tu32\tdevice;\n\t\t} bridge_chipsets[] = {\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_PXH_0 },\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_PXH_1 },\n\t\t\t{ },\n\t\t};\n\t\tstruct tg3_dev_id *pci_id = &bridge_chipsets[0];\n\t\tstruct pci_dev *bridge = NULL;\n\n\t\twhile (pci_id->vendor != 0) {\n\t\t\tbridge = pci_get_device(pci_id->vendor,\n\t\t\t\t\t\tpci_id->device,\n\t\t\t\t\t\tbridge);\n\t\t\tif (!bridge) {\n\t\t\t\tpci_id++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (bridge->subordinate &&\n\t\t\t    (bridge->subordinate->number <=\n\t\t\t     tp->pdev->bus->number) &&\n\t\t\t    (bridge->subordinate->busn_res.end >=\n\t\t\t     tp->pdev->bus->number)) {\n\t\t\t\ttg3_flag_set(tp, 5701_DMA_BUG);\n\t\t\t\tpci_dev_put(bridge);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* The EPB bridge inside 5714, 5715, and 5780 cannot support\n\t * DMA addresses > 40-bit. This bridge may have other additional\n\t * 57xx devices behind it in some 4-port NIC designs for example.\n\t * Any tg3 device found behind the bridge will also need the 40-bit\n\t * DMA workaround.\n\t */\n\tif (tg3_flag(tp, 5780_CLASS)) {\n\t\ttg3_flag_set(tp, 40BIT_DMA_BUG);\n\t\ttp->msi_cap = pci_find_capability(tp->pdev, PCI_CAP_ID_MSI);\n\t} else {\n\t\tstruct pci_dev *bridge = NULL;\n\n\t\tdo {\n\t\t\tbridge = pci_get_device(PCI_VENDOR_ID_SERVERWORKS,\n\t\t\t\t\t\tPCI_DEVICE_ID_SERVERWORKS_EPB,\n\t\t\t\t\t\tbridge);\n\t\t\tif (bridge && bridge->subordinate &&\n\t\t\t    (bridge->subordinate->number <=\n\t\t\t     tp->pdev->bus->number) &&\n\t\t\t    (bridge->subordinate->busn_res.end >=\n\t\t\t     tp->pdev->bus->number)) {\n\t\t\t\ttg3_flag_set(tp, 40BIT_DMA_BUG);\n\t\t\t\tpci_dev_put(bridge);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (bridge);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5714)\n\t\ttp->pdev_peer = tg3_find_peer(tp);\n\n\t/* Determine TSO capabilities */\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0)\n\t\t; /* Do nothing. HW bug. */\n\telse if (tg3_flag(tp, 57765_PLUS))\n\t\ttg3_flag_set(tp, HW_TSO_3);\n\telse if (tg3_flag(tp, 5755_PLUS) ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\ttg3_flag_set(tp, HW_TSO_2);\n\telse if (tg3_flag(tp, 5750_PLUS)) {\n\t\ttg3_flag_set(tp, HW_TSO_1);\n\t\ttg3_flag_set(tp, TSO_BUG);\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5750 &&\n\t\t    tg3_chip_rev_id(tp) >= CHIPREV_ID_5750_C2)\n\t\t\ttg3_flag_clear(tp, TSO_BUG);\n\t} else if (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t\t   tg3_asic_rev(tp) != ASIC_REV_5701 &&\n\t\t   tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A0) {\n\t\t\ttg3_flag_set(tp, TSO_BUG);\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5705)\n\t\t\ttp->fw_needed = FIRMWARE_TG3TSO5;\n\t\telse\n\t\t\ttp->fw_needed = FIRMWARE_TG3TSO;\n\t}\n\n\t/* Selectively allow TSO based on operating conditions */\n\tif (tg3_flag(tp, HW_TSO_1) ||\n\t    tg3_flag(tp, HW_TSO_2) ||\n\t    tg3_flag(tp, HW_TSO_3) ||\n\t    tp->fw_needed) {\n\t\t/* For firmware TSO, assume ASF is disabled.\n\t\t * We'll disable TSO later if we discover ASF\n\t\t * is enabled in tg3_get_eeprom_hw_cfg().\n\t\t */\n\t\ttg3_flag_set(tp, TSO_CAPABLE);\n\t} else {\n\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\ttg3_flag_clear(tp, TSO_BUG);\n\t\ttp->fw_needed = NULL;\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0)\n\t\ttp->fw_needed = FIRMWARE_TG3;\n\n\ttp->irq_max = 1;\n\n\tif (tg3_flag(tp, 5750_PLUS)) {\n\t\ttg3_flag_set(tp, SUPPORT_MSI);\n\t\tif (tg3_chip_rev(tp) == CHIPREV_5750_AX ||\n\t\t    tg3_chip_rev(tp) == CHIPREV_5750_BX ||\n\t\t    (tg3_asic_rev(tp) == ASIC_REV_5714 &&\n\t\t     tg3_chip_rev_id(tp) <= CHIPREV_ID_5714_A2 &&\n\t\t     tp->pdev_peer == tp->pdev))\n\t\t\ttg3_flag_clear(tp, SUPPORT_MSI);\n\n\t\tif (tg3_flag(tp, 5755_PLUS) ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t\ttg3_flag_set(tp, 1SHOT_MSI);\n\t\t}\n\n\t\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\t\ttg3_flag_set(tp, SUPPORT_MSIX);\n\t\t\ttp->irq_max = TG3_IRQ_MAX_VECS;\n\t\t}\n\t}\n\n\ttp->txq_max = 1;\n\ttp->rxq_max = 1;\n\tif (tp->irq_max > 1) {\n\t\ttp->rxq_max = TG3_RSS_MAX_NUM_QS;\n\t\ttg3_rss_init_dflt_indir_tbl(tp, TG3_RSS_MAX_NUM_QS);\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5720)\n\t\t\ttp->txq_max = tp->irq_max - 1;\n\t}\n\n\tif (tg3_flag(tp, 5755_PLUS) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\ttg3_flag_set(tp, SHORT_DMA_BUG);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\ttp->dma_limit = TG3_TX_BD_DMA_MAX_4K;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\ttg3_flag_set(tp, LRG_PROD_RING_CAP);\n\n\tif (tg3_flag(tp, 57765_PLUS) &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5719_A0)\n\t\ttg3_flag_set(tp, USE_JUMBO_BDFLAG);\n\n\tif (!tg3_flag(tp, 5705_PLUS) ||\n\t    tg3_flag(tp, 5780_CLASS) ||\n\t    tg3_flag(tp, USE_JUMBO_BDFLAG))\n\t\ttg3_flag_set(tp, JUMBO_CAPABLE);\n\n\tpci_read_config_dword(tp->pdev, TG3PCI_PCISTATE,\n\t\t\t      &pci_state_reg);\n\n\tif (pci_is_pcie(tp->pdev)) {\n\t\tu16 lnkctl;\n\n\t\ttg3_flag_set(tp, PCI_EXPRESS);\n\n\t\tpcie_capability_read_word(tp->pdev, PCI_EXP_LNKCTL, &lnkctl);\n\t\tif (lnkctl & PCI_EXP_LNKCTL_CLKREQ_EN) {\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t\t\ttg3_flag_clear(tp, HW_TSO_2);\n\t\t\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\t\t}\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5761 ||\n\t\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_57780_A0 ||\n\t\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_57780_A1)\n\t\t\t\ttg3_flag_set(tp, CLKREQ_BUG);\n\t\t} else if (tg3_chip_rev_id(tp) == CHIPREV_ID_5717_A0) {\n\t\t\ttg3_flag_set(tp, L1PLLPD_EN);\n\t\t}\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_5785) {\n\t\t/* BCM5785 devices are effectively PCIe devices, and should\n\t\t * follow PCIe codepaths, but do not have a PCIe capabilities\n\t\t * section.\n\t\t */\n\t\ttg3_flag_set(tp, PCI_EXPRESS);\n\t} else if (!tg3_flag(tp, 5705_PLUS) ||\n\t\t   tg3_flag(tp, 5780_CLASS)) {\n\t\ttp->pcix_cap = pci_find_capability(tp->pdev, PCI_CAP_ID_PCIX);\n\t\tif (!tp->pcix_cap) {\n\t\t\tdev_err(&tp->pdev->dev,\n\t\t\t\t\"Cannot find PCI-X capability, aborting\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (!(pci_state_reg & PCISTATE_CONV_PCI_MODE))\n\t\t\ttg3_flag_set(tp, PCIX_MODE);\n\t}\n\n\t/* If we have an AMD 762 or VIA K8T800 chipset, write\n\t * reordering to the mailbox registers done by the host\n\t * controller can cause major troubles.  We read back from\n\t * every mailbox register write to force the writes to be\n\t * posted to the chip in order.\n\t */\n\tif (pci_dev_present(tg3_write_reorder_chipsets) &&\n\t    !tg3_flag(tp, PCI_EXPRESS))\n\t\ttg3_flag_set(tp, MBOX_WRITE_REORDER);\n\n\tpci_read_config_byte(tp->pdev, PCI_CACHE_LINE_SIZE,\n\t\t\t     &tp->pci_cacheline_sz);\n\tpci_read_config_byte(tp->pdev, PCI_LATENCY_TIMER,\n\t\t\t     &tp->pci_lat_timer);\n\tif (tg3_asic_rev(tp) == ASIC_REV_5703 &&\n\t    tp->pci_lat_timer < 64) {\n\t\ttp->pci_lat_timer = 64;\n\t\tpci_write_config_byte(tp->pdev, PCI_LATENCY_TIMER,\n\t\t\t\t      tp->pci_lat_timer);\n\t}\n\n\t/* Important! -- It is critical that the PCI-X hw workaround\n\t * situation is decided before the first MMIO register access.\n\t */\n\tif (tg3_chip_rev(tp) == CHIPREV_5700_BX) {\n\t\t/* 5700 BX chips need to have their TX producer index\n\t\t * mailboxes written twice to workaround a bug.\n\t\t */\n\t\ttg3_flag_set(tp, TXD_MBOX_HWBUG);\n\n\t\t/* If we are in PCI-X mode, enable register write workaround.\n\t\t *\n\t\t * The workaround is to use indirect register accesses\n\t\t * for all chip writes not to mailbox registers.\n\t\t */\n\t\tif (tg3_flag(tp, PCIX_MODE)) {\n\t\t\tu32 pm_reg;\n\n\t\t\ttg3_flag_set(tp, PCIX_TARGET_HWBUG);\n\n\t\t\t/* The chip can have it's power management PCI config\n\t\t\t * space registers clobbered due to this bug.\n\t\t\t * So explicitly force the chip into D0 here.\n\t\t\t */\n\t\t\tpci_read_config_dword(tp->pdev,\n\t\t\t\t\t      tp->pm_cap + PCI_PM_CTRL,\n\t\t\t\t\t      &pm_reg);\n\t\t\tpm_reg &= ~PCI_PM_CTRL_STATE_MASK;\n\t\t\tpm_reg |= PCI_PM_CTRL_PME_ENABLE | 0 /* D0 */;\n\t\t\tpci_write_config_dword(tp->pdev,\n\t\t\t\t\t       tp->pm_cap + PCI_PM_CTRL,\n\t\t\t\t\t       pm_reg);\n\n\t\t\t/* Also, force SERR#/PERR# in PCI command. */\n\t\t\tpci_read_config_word(tp->pdev, PCI_COMMAND, &pci_cmd);\n\t\t\tpci_cmd |= PCI_COMMAND_PARITY | PCI_COMMAND_SERR;\n\t\t\tpci_write_config_word(tp->pdev, PCI_COMMAND, pci_cmd);\n\t\t}\n\t}\n\n\tif ((pci_state_reg & PCISTATE_BUS_SPEED_HIGH) != 0)\n\t\ttg3_flag_set(tp, PCI_HIGH_SPEED);\n\tif ((pci_state_reg & PCISTATE_BUS_32BIT) != 0)\n\t\ttg3_flag_set(tp, PCI_32BIT);\n\n\t/* Chip-specific fixup from Broadcom driver */\n\tif ((tg3_chip_rev_id(tp) == CHIPREV_ID_5704_A0) &&\n\t    (!(pci_state_reg & PCISTATE_RETRY_SAME_DMA))) {\n\t\tpci_state_reg |= PCISTATE_RETRY_SAME_DMA;\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_PCISTATE, pci_state_reg);\n\t}\n\n\t/* Default fast path register access methods */\n\ttp->read32 = tg3_read32;\n\ttp->write32 = tg3_write32;\n\ttp->read32_mbox = tg3_read32;\n\ttp->write32_mbox = tg3_write32;\n\ttp->write32_tx_mbox = tg3_write32;\n\ttp->write32_rx_mbox = tg3_write32;\n\n\t/* Various workaround register access methods */\n\tif (tg3_flag(tp, PCIX_TARGET_HWBUG))\n\t\ttp->write32 = tg3_write_indirect_reg32;\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5701 ||\n\t\t (tg3_flag(tp, PCI_EXPRESS) &&\n\t\t  tg3_chip_rev_id(tp) == CHIPREV_ID_5750_A0)) {\n\t\t/*\n\t\t * Back to back register writes can cause problems on these\n\t\t * chips, the workaround is to read back all reg writes\n\t\t * except those to mailbox regs.\n\t\t *\n\t\t * See tg3_write_indirect_reg32().\n\t\t */\n\t\ttp->write32 = tg3_write_flush_reg32;\n\t}\n\n\tif (tg3_flag(tp, TXD_MBOX_HWBUG) || tg3_flag(tp, MBOX_WRITE_REORDER)) {\n\t\ttp->write32_tx_mbox = tg3_write32_tx_mbox;\n\t\tif (tg3_flag(tp, MBOX_WRITE_REORDER))\n\t\t\ttp->write32_rx_mbox = tg3_write_flush_reg32;\n\t}\n\n\tif (tg3_flag(tp, ICH_WORKAROUND)) {\n\t\ttp->read32 = tg3_read_indirect_reg32;\n\t\ttp->write32 = tg3_write_indirect_reg32;\n\t\ttp->read32_mbox = tg3_read_indirect_mbox;\n\t\ttp->write32_mbox = tg3_write_indirect_mbox;\n\t\ttp->write32_tx_mbox = tg3_write_indirect_mbox;\n\t\ttp->write32_rx_mbox = tg3_write_indirect_mbox;\n\n\t\tiounmap(tp->regs);\n\t\ttp->regs = NULL;\n\n\t\tpci_read_config_word(tp->pdev, PCI_COMMAND, &pci_cmd);\n\t\tpci_cmd &= ~PCI_COMMAND_MEMORY;\n\t\tpci_write_config_word(tp->pdev, PCI_COMMAND, pci_cmd);\n\t}\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\ttp->read32_mbox = tg3_read32_mbox_5906;\n\t\ttp->write32_mbox = tg3_write32_mbox_5906;\n\t\ttp->write32_tx_mbox = tg3_write32_mbox_5906;\n\t\ttp->write32_rx_mbox = tg3_write32_mbox_5906;\n\t}\n\n\tif (tp->write32 == tg3_write_indirect_reg32 ||\n\t    (tg3_flag(tp, PCIX_MODE) &&\n\t     (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t      tg3_asic_rev(tp) == ASIC_REV_5701)))\n\t\ttg3_flag_set(tp, SRAM_USE_CONFIG);\n\n\t/* The memory arbiter has to be enabled in order for SRAM accesses\n\t * to succeed.  Normally on powerup the tg3 chip firmware will make\n\t * sure it is enabled, but other entities such as system netboot\n\t * code might disable it.\n\t */\n\tval = tr32(MEMARB_MODE);\n\ttw32(MEMARB_MODE, val | MEMARB_MODE_ENABLE);\n\n\ttp->pci_fn = PCI_FUNC(tp->pdev->devfn) & 3;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    tg3_flag(tp, 5780_CLASS)) {\n\t\tif (tg3_flag(tp, PCIX_MODE)) {\n\t\t\tpci_read_config_dword(tp->pdev,\n\t\t\t\t\t      tp->pcix_cap + PCI_X_STATUS,\n\t\t\t\t\t      &val);\n\t\t\ttp->pci_fn = val & 0x7;\n\t\t}\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t\t   tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t\t   tg3_asic_rev(tp) == ASIC_REV_5720) {\n\t\ttg3_read_mem(tp, NIC_SRAM_CPMU_STATUS, &val);\n\t\tif ((val & NIC_SRAM_CPMUSTAT_SIG_MSK) != NIC_SRAM_CPMUSTAT_SIG)\n\t\t\tval = tr32(TG3_CPMU_STATUS);\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5717)\n\t\t\ttp->pci_fn = (val & TG3_CPMU_STATUS_FMSK_5717) ? 1 : 0;\n\t\telse\n\t\t\ttp->pci_fn = (val & TG3_CPMU_STATUS_FMSK_5719) >>\n\t\t\t\t     TG3_CPMU_STATUS_FSHFT_5719;\n\t}\n\n\tif (tg3_flag(tp, FLUSH_POSTED_WRITES)) {\n\t\ttp->write32_tx_mbox = tg3_write_flush_reg32;\n\t\ttp->write32_rx_mbox = tg3_write_flush_reg32;\n\t}\n\n\t/* Get eeprom hw config before calling tg3_set_power_state().\n\t * In particular, the TG3_FLAG_IS_NIC flag must be\n\t * determined before calling tg3_set_power_state() so that\n\t * we know whether or not to switch out of Vaux power.\n\t * When the flag is set, it means that GPIO1 is used for eeprom\n\t * write protect and also implies that it is a LOM where GPIOs\n\t * are not used to switch power.\n\t */\n\ttg3_get_eeprom_hw_cfg(tp);\n\n\tif (tp->fw_needed && tg3_flag(tp, ENABLE_ASF)) {\n\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\ttg3_flag_clear(tp, TSO_BUG);\n\t\ttp->fw_needed = NULL;\n\t}\n\n\tif (tg3_flag(tp, ENABLE_APE)) {\n\t\t/* Allow reads and writes to the\n\t\t * APE register and memory space.\n\t\t */\n\t\tpci_state_reg |= PCISTATE_ALLOW_APE_CTLSPC_WR |\n\t\t\t\t PCISTATE_ALLOW_APE_SHMEM_WR |\n\t\t\t\t PCISTATE_ALLOW_APE_PSPACE_WR;\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_PCISTATE,\n\t\t\t\t       pci_state_reg);\n\n\t\ttg3_ape_lock_init(tp);\n\t}\n\n\t/* Set up tp->grc_local_ctrl before calling\n\t * tg3_pwrsrc_switch_to_vmain().  GPIO1 driven high\n\t * will bring 5700's external PHY out of reset.\n\t * It is also used as eeprom write protect on LOMs.\n\t */\n\ttp->grc_local_ctrl = GRC_LCLCTRL_INT_ON_ATTN | GRC_LCLCTRL_AUTO_SEEPROM;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_flag(tp, EEPROM_WRITE_PROT))\n\t\ttp->grc_local_ctrl |= (GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t\t       GRC_LCLCTRL_GPIO_OUTPUT1);\n\t/* Unused GPIO3 must be driven as output on 5752 because there\n\t * are no pull-up resistors on unused GPIO pins.\n\t */\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5752)\n\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_GPIO_OE3;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780 ||\n\t    tg3_flag(tp, 57765_CLASS))\n\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_GPIO_UART_SEL;\n\n\tif (tp->pdev->device == PCI_DEVICE_ID_TIGON3_5761 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5761S) {\n\t\t/* Turn off the debug UART. */\n\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_GPIO_UART_SEL;\n\t\tif (tg3_flag(tp, IS_NIC))\n\t\t\t/* Keep VMain power. */\n\t\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_GPIO_OE0 |\n\t\t\t\t\t      GRC_LCLCTRL_GPIO_OUTPUT0;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\ttp->grc_local_ctrl |=\n\t\t\ttr32(GRC_LOCAL_CTRL) & GRC_LCLCTRL_GPIO_UART_SEL;\n\n\t/* Switch out of Vaux if it is a NIC */\n\ttg3_pwrsrc_switch_to_vmain(tp);\n\n\t/* Derive initial jumbo mode from MTU assigned in\n\t * ether_setup() via the alloc_etherdev() call\n\t */\n\tif (tp->dev->mtu > ETH_DATA_LEN && !tg3_flag(tp, 5780_CLASS))\n\t\ttg3_flag_set(tp, JUMBO_RING_ENABLE);\n\n\t/* Determine WakeOnLan speed to use. */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B2) {\n\t\ttg3_flag_clear(tp, WOL_SPEED_100MB);\n\t} else {\n\t\ttg3_flag_set(tp, WOL_SPEED_100MB);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\ttp->phy_flags |= TG3_PHYFLG_IS_FET;\n\n\t/* A few boards don't want Ethernet@WireSpeed phy feature */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    (tg3_asic_rev(tp) == ASIC_REV_5705 &&\n\t     (tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A0) &&\n\t     (tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A1)) ||\n\t    (tp->phy_flags & TG3_PHYFLG_IS_FET) ||\n\t    (tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\ttp->phy_flags |= TG3_PHYFLG_NO_ETH_WIRE_SPEED;\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5703_AX ||\n\t    tg3_chip_rev(tp) == CHIPREV_5704_AX)\n\t\ttp->phy_flags |= TG3_PHYFLG_ADC_BUG;\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5704_A0)\n\t\ttp->phy_flags |= TG3_PHYFLG_5704_A0_BUG;\n\n\tif (tg3_flag(tp, 5705_PLUS) &&\n\t    !(tp->phy_flags & TG3_PHYFLG_IS_FET) &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5785 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_57780 &&\n\t    !tg3_flag(tp, 57765_PLUS)) {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5787 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5761) {\n\t\t\tif (tp->pdev->device != PCI_DEVICE_ID_TIGON3_5756 &&\n\t\t\t    tp->pdev->device != PCI_DEVICE_ID_TIGON3_5722)\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_JITTER_BUG;\n\t\t\tif (tp->pdev->device == PCI_DEVICE_ID_TIGON3_5755M)\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_ADJUST_TRIM;\n\t\t} else\n\t\t\ttp->phy_flags |= TG3_PHYFLG_BER_BUG;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5784 &&\n\t    tg3_chip_rev(tp) != CHIPREV_5784_AX) {\n\t\ttp->phy_otp = tg3_read_otp_phycfg(tp);\n\t\tif (tp->phy_otp == 0)\n\t\t\ttp->phy_otp = TG3_OTP_DEFAULT;\n\t}\n\n\tif (tg3_flag(tp, CPMU_PRESENT))\n\t\ttp->mi_mode = MAC_MI_MODE_500KHZ_CONST;\n\telse\n\t\ttp->mi_mode = MAC_MI_MODE_BASE;\n\n\ttp->coalesce_mode = 0;\n\tif (tg3_chip_rev(tp) != CHIPREV_5700_AX &&\n\t    tg3_chip_rev(tp) != CHIPREV_5700_BX)\n\t\ttp->coalesce_mode |= HOSTCC_MODE_32BYTE;\n\n\t/* Set these bits to enable statistics workaround. */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5720_A0) {\n\t\ttp->coalesce_mode |= HOSTCC_MODE_ATTN;\n\t\ttp->grc_mode |= GRC_MODE_IRQ_ON_FLOW_ATTN;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780)\n\t\ttg3_flag_set(tp, USE_PHYLIB);\n\n\terr = tg3_mdio_init(tp);\n\tif (err)\n\t\treturn err;\n\n\t/* Initialize data/descriptor byte/word swapping. */\n\tval = tr32(GRC_MODE);\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tval &= (GRC_MODE_BYTE_SWAP_B2HRX_DATA |\n\t\t\tGRC_MODE_WORD_SWAP_B2HRX_DATA |\n\t\t\tGRC_MODE_B2HRX_ENABLE |\n\t\t\tGRC_MODE_HTX2B_ENABLE |\n\t\t\tGRC_MODE_HOST_STACKUP);\n\telse\n\t\tval &= GRC_MODE_HOST_STACKUP;\n\n\ttw32(GRC_MODE, val | tp->grc_mode);\n\n\ttg3_switch_clocks(tp);\n\n\t/* Clear this out for sanity. */\n\ttw32(TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\n\tpci_read_config_dword(tp->pdev, TG3PCI_PCISTATE,\n\t\t\t      &pci_state_reg);\n\tif ((pci_state_reg & PCISTATE_CONV_PCI_MODE) == 0 &&\n\t    !tg3_flag(tp, PCIX_TARGET_HWBUG)) {\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0 ||\n\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B2 ||\n\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B5) {\n\t\t\tvoid __iomem *sram_base;\n\n\t\t\t/* Write some dummy words into the SRAM status block\n\t\t\t * area, see if it reads back correctly.  If the return\n\t\t\t * value is bad, force enable the PCIX workaround.\n\t\t\t */\n\t\t\tsram_base = tp->regs + NIC_SRAM_WIN_BASE + NIC_SRAM_STATS_BLK;\n\n\t\t\twritel(0x00000000, sram_base);\n\t\t\twritel(0x00000000, sram_base + 4);\n\t\t\twritel(0xffffffff, sram_base + 4);\n\t\t\tif (readl(sram_base) != 0x00000000)\n\t\t\t\ttg3_flag_set(tp, PCIX_TARGET_HWBUG);\n\t\t}\n\t}\n\n\tudelay(50);\n\ttg3_nvram_init(tp);\n\n\tgrc_misc_cfg = tr32(GRC_MISC_CFG);\n\tgrc_misc_cfg &= GRC_MISC_CFG_BOARD_ID_MASK;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705 &&\n\t    (grc_misc_cfg == GRC_MISC_CFG_BOARD_ID_5788 ||\n\t     grc_misc_cfg == GRC_MISC_CFG_BOARD_ID_5788M))\n\t\ttg3_flag_set(tp, IS_5788);\n\n\tif (!tg3_flag(tp, IS_5788) &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5700)\n\t\ttg3_flag_set(tp, TAGGED_STATUS);\n\tif (tg3_flag(tp, TAGGED_STATUS)) {\n\t\ttp->coalesce_mode |= (HOSTCC_MODE_CLRTICK_RXBD |\n\t\t\t\t      HOSTCC_MODE_CLRTICK_TXBD);\n\n\t\ttp->misc_host_ctrl |= MISC_HOST_CTRL_TAGGED_STATUS;\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MISC_HOST_CTRL,\n\t\t\t\t       tp->misc_host_ctrl);\n\t}\n\n\t/* Preserve the APE MAC_MODE bits */\n\tif (tg3_flag(tp, ENABLE_APE))\n\t\ttp->mac_mode = MAC_MODE_APE_TX_EN | MAC_MODE_APE_RX_EN;\n\telse\n\t\ttp->mac_mode = 0;\n\n\tif (tg3_10_100_only_device(tp, ent))\n\t\ttp->phy_flags |= TG3_PHYFLG_10_100_ONLY;\n\n\terr = tg3_phy_probe(tp);\n\tif (err) {\n\t\tdev_err(&tp->pdev->dev, \"phy probe failed, err %d\\n\", err);\n\t\t/* ... but do not return immediately ... */\n\t\ttg3_mdio_fini(tp);\n\t}\n\n\ttg3_read_vpd(tp);\n\ttg3_read_fw_ver(tp);\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\ttp->phy_flags &= ~TG3_PHYFLG_USE_MI_INTERRUPT;\n\t} else {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700)\n\t\t\ttp->phy_flags |= TG3_PHYFLG_USE_MI_INTERRUPT;\n\t\telse\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_USE_MI_INTERRUPT;\n\t}\n\n\t/* 5700 {AX,BX} chips have a broken status block link\n\t * change bit implementation, so we must use the\n\t * status register in those cases.\n\t */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700)\n\t\ttg3_flag_set(tp, USE_LINKCHG_REG);\n\telse\n\t\ttg3_flag_clear(tp, USE_LINKCHG_REG);\n\n\t/* The led_ctrl is set during tg3_phy_probe, here we might\n\t * have to force the link status polling mechanism based\n\t * upon subsystem IDs.\n\t */\n\tif (tp->pdev->subsystem_vendor == PCI_VENDOR_ID_DELL &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5701 &&\n\t    !(tp->phy_flags & TG3_PHYFLG_PHY_SERDES)) {\n\t\ttp->phy_flags |= TG3_PHYFLG_USE_MI_INTERRUPT;\n\t\ttg3_flag_set(tp, USE_LINKCHG_REG);\n\t}\n\n\t/* For all SERDES we poll the MAC status register. */\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\ttg3_flag_set(tp, POLL_SERDES);\n\telse\n\t\ttg3_flag_clear(tp, POLL_SERDES);\n\n\ttp->rx_offset = NET_SKB_PAD + NET_IP_ALIGN;\n\ttp->rx_copy_thresh = TG3_RX_COPY_THRESHOLD;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5701 &&\n\t    tg3_flag(tp, PCIX_MODE)) {\n\t\ttp->rx_offset = NET_SKB_PAD;\n#ifndef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS\n\t\ttp->rx_copy_thresh = ~(u16)0;\n#endif\n\t}\n\n\ttp->rx_std_ring_mask = TG3_RX_STD_RING_SIZE(tp) - 1;\n\ttp->rx_jmb_ring_mask = TG3_RX_JMB_RING_SIZE(tp) - 1;\n\ttp->rx_ret_ring_mask = tg3_rx_ret_ring_size(tp) - 1;\n\n\ttp->rx_std_max_post = tp->rx_std_ring_mask + 1;\n\n\t/* Increment the rx prod index on the rx std ring by at most\n\t * 8 for these chips to workaround hw errata.\n\t */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5750 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5752 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5755)\n\t\ttp->rx_std_max_post = 8;\n\n\tif (tg3_flag(tp, ASPM_WORKAROUND))\n\t\ttp->pwrmgmt_thresh = tr32(PCIE_PWR_MGMT_THRESH) &\n\t\t\t\t     PCIE_PWR_MGMT_L1_THRESH_MSK;\n\n\treturn err;\n}\n\n#ifdef CONFIG_SPARC\nstatic int tg3_get_macaddr_sparc(struct tg3 *tp)\n{\n\tstruct net_device *dev = tp->dev;\n\tstruct pci_dev *pdev = tp->pdev;\n\tstruct device_node *dp = pci_device_to_OF_node(pdev);\n\tconst unsigned char *addr;\n\tint len;\n\n\taddr = of_get_property(dp, \"local-mac-address\", &len);\n\tif (addr && len == 6) {\n\t\tmemcpy(dev->dev_addr, addr, 6);\n\t\treturn 0;\n\t}\n\treturn -ENODEV;\n}\n\nstatic int tg3_get_default_macaddr_sparc(struct tg3 *tp)\n{\n\tstruct net_device *dev = tp->dev;\n\n\tmemcpy(dev->dev_addr, idprom->id_ethaddr, 6);\n\treturn 0;\n}\n#endif\n\nstatic int tg3_get_device_address(struct tg3 *tp)\n{\n\tstruct net_device *dev = tp->dev;\n\tu32 hi, lo, mac_offset;\n\tint addr_ok = 0;\n\tint err;\n\n#ifdef CONFIG_SPARC\n\tif (!tg3_get_macaddr_sparc(tp))\n\t\treturn 0;\n#endif\n\n\tif (tg3_flag(tp, IS_SSB_CORE)) {\n\t\terr = ssb_gige_get_macaddr(tp->pdev, &dev->dev_addr[0]);\n\t\tif (!err && is_valid_ether_addr(&dev->dev_addr[0]))\n\t\t\treturn 0;\n\t}\n\n\tmac_offset = 0x7c;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    tg3_flag(tp, 5780_CLASS)) {\n\t\tif (tr32(TG3PCI_DUAL_MAC_CTRL) & DUAL_MAC_CTRL_ID)\n\t\t\tmac_offset = 0xcc;\n\t\tif (tg3_nvram_lock(tp))\n\t\t\ttw32_f(NVRAM_CMD, NVRAM_CMD_RESET);\n\t\telse\n\t\t\ttg3_nvram_unlock(tp);\n\t} else if (tg3_flag(tp, 5717_PLUS)) {\n\t\tif (tp->pci_fn & 1)\n\t\t\tmac_offset = 0xcc;\n\t\tif (tp->pci_fn > 1)\n\t\t\tmac_offset += 0x18c;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\tmac_offset = 0x10;\n\n\t/* First try to get it from MAC address mailbox. */\n\ttg3_read_mem(tp, NIC_SRAM_MAC_ADDR_HIGH_MBOX, &hi);\n\tif ((hi >> 16) == 0x484b) {\n\t\tdev->dev_addr[0] = (hi >>  8) & 0xff;\n\t\tdev->dev_addr[1] = (hi >>  0) & 0xff;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_MAC_ADDR_LOW_MBOX, &lo);\n\t\tdev->dev_addr[2] = (lo >> 24) & 0xff;\n\t\tdev->dev_addr[3] = (lo >> 16) & 0xff;\n\t\tdev->dev_addr[4] = (lo >>  8) & 0xff;\n\t\tdev->dev_addr[5] = (lo >>  0) & 0xff;\n\n\t\t/* Some old bootcode may report a 0 MAC address in SRAM */\n\t\taddr_ok = is_valid_ether_addr(&dev->dev_addr[0]);\n\t}\n\tif (!addr_ok) {\n\t\t/* Next, try NVRAM. */\n\t\tif (!tg3_flag(tp, NO_NVRAM) &&\n\t\t    !tg3_nvram_read_be32(tp, mac_offset + 0, &hi) &&\n\t\t    !tg3_nvram_read_be32(tp, mac_offset + 4, &lo)) {\n\t\t\tmemcpy(&dev->dev_addr[0], ((char *)&hi) + 2, 2);\n\t\t\tmemcpy(&dev->dev_addr[2], (char *)&lo, sizeof(lo));\n\t\t}\n\t\t/* Finally just fetch it out of the MAC control regs. */\n\t\telse {\n\t\t\thi = tr32(MAC_ADDR_0_HIGH);\n\t\t\tlo = tr32(MAC_ADDR_0_LOW);\n\n\t\t\tdev->dev_addr[5] = lo & 0xff;\n\t\t\tdev->dev_addr[4] = (lo >> 8) & 0xff;\n\t\t\tdev->dev_addr[3] = (lo >> 16) & 0xff;\n\t\t\tdev->dev_addr[2] = (lo >> 24) & 0xff;\n\t\t\tdev->dev_addr[1] = hi & 0xff;\n\t\t\tdev->dev_addr[0] = (hi >> 8) & 0xff;\n\t\t}\n\t}\n\n\tif (!is_valid_ether_addr(&dev->dev_addr[0])) {\n#ifdef CONFIG_SPARC\n\t\tif (!tg3_get_default_macaddr_sparc(tp))\n\t\t\treturn 0;\n#endif\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#define BOUNDARY_SINGLE_CACHELINE\t1\n#define BOUNDARY_MULTI_CACHELINE\t2\n\nstatic u32 tg3_calc_dma_bndry(struct tg3 *tp, u32 val)\n{\n\tint cacheline_size;\n\tu8 byte;\n\tint goal;\n\n\tpci_read_config_byte(tp->pdev, PCI_CACHE_LINE_SIZE, &byte);\n\tif (byte == 0)\n\t\tcacheline_size = 1024;\n\telse\n\t\tcacheline_size = (int) byte * 4;\n\n\t/* On 5703 and later chips, the boundary bits have no\n\t * effect.\n\t */\n\tif (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5701 &&\n\t    !tg3_flag(tp, PCI_EXPRESS))\n\t\tgoto out;\n\n#if defined(CONFIG_PPC64) || defined(CONFIG_IA64) || defined(CONFIG_PARISC)\n\tgoal = BOUNDARY_MULTI_CACHELINE;\n#else\n#if defined(CONFIG_SPARC64) || defined(CONFIG_ALPHA)\n\tgoal = BOUNDARY_SINGLE_CACHELINE;\n#else\n\tgoal = 0;\n#endif\n#endif\n\n\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\tval = goal ? 0 : DMA_RWCTRL_DIS_CACHE_ALIGNMENT;\n\t\tgoto out;\n\t}\n\n\tif (!goal)\n\t\tgoto out;\n\n\t/* PCI controllers on most RISC systems tend to disconnect\n\t * when a device tries to burst across a cache-line boundary.\n\t * Therefore, letting tg3 do so just wastes PCI bandwidth.\n\t *\n\t * Unfortunately, for PCI-E there are only limited\n\t * write-side controls for this, and thus for reads\n\t * we will still get the disconnects.  We'll also waste\n\t * these PCI cycles for both read and write for chips\n\t * other than 5700 and 5701 which do not implement the\n\t * boundary bits.\n\t */\n\tif (tg3_flag(tp, PCIX_MODE) && !tg3_flag(tp, PCI_EXPRESS)) {\n\t\tswitch (cacheline_size) {\n\t\tcase 16:\n\t\tcase 32:\n\t\tcase 64:\n\t\tcase 128:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_128_PCIX |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_128_PCIX);\n\t\t\t} else {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_384_PCIX |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_384_PCIX);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 256:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_256_PCIX |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_256_PCIX);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_384_PCIX |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_384_PCIX);\n\t\t\tbreak;\n\t\t}\n\t} else if (tg3_flag(tp, PCI_EXPRESS)) {\n\t\tswitch (cacheline_size) {\n\t\tcase 16:\n\t\tcase 32:\n\t\tcase 64:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval &= ~DMA_RWCTRL_WRITE_BNDRY_DISAB_PCIE;\n\t\t\t\tval |= DMA_RWCTRL_WRITE_BNDRY_64_PCIE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 128:\n\t\tdefault:\n\t\t\tval &= ~DMA_RWCTRL_WRITE_BNDRY_DISAB_PCIE;\n\t\t\tval |= DMA_RWCTRL_WRITE_BNDRY_128_PCIE;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tswitch (cacheline_size) {\n\t\tcase 16:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_16 |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_16);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 32:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_32 |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 64:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_64 |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_64);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 128:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_128 |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_128);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 256:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_256 |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_256);\n\t\t\tbreak;\n\t\tcase 512:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_512 |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_512);\n\t\t\tbreak;\n\t\tcase 1024:\n\t\tdefault:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_1024 |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_1024);\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\treturn val;\n}\n\nstatic int tg3_do_test_dma(struct tg3 *tp, u32 *buf, dma_addr_t buf_dma,\n\t\t\t   int size, int to_device)\n{\n\tstruct tg3_internal_buffer_desc test_desc;\n\tu32 sram_dma_descs;\n\tint i, ret;\n\n\tsram_dma_descs = NIC_SRAM_DMA_DESC_POOL_BASE;\n\n\ttw32(FTQ_RCVBD_COMP_FIFO_ENQDEQ, 0);\n\ttw32(FTQ_RCVDATA_COMP_FIFO_ENQDEQ, 0);\n\ttw32(RDMAC_STATUS, 0);\n\ttw32(WDMAC_STATUS, 0);\n\n\ttw32(BUFMGR_MODE, 0);\n\ttw32(FTQ_RESET, 0);\n\n\ttest_desc.addr_hi = ((u64) buf_dma) >> 32;\n\ttest_desc.addr_lo = buf_dma & 0xffffffff;\n\ttest_desc.nic_mbuf = 0x00002100;\n\ttest_desc.len = size;\n\n\t/*\n\t * HP ZX1 was seeing test failures for 5701 cards running at 33Mhz\n\t * the *second* time the tg3 driver was getting loaded after an\n\t * initial scan.\n\t *\n\t * Broadcom tells me:\n\t *   ...the DMA engine is connected to the GRC block and a DMA\n\t *   reset may affect the GRC block in some unpredictable way...\n\t *   The behavior of resets to individual blocks has not been tested.\n\t *\n\t * Broadcom noted the GRC reset will also reset all sub-components.\n\t */\n\tif (to_device) {\n\t\ttest_desc.cqid_sqid = (13 << 8) | 2;\n\n\t\ttw32_f(RDMAC_MODE, RDMAC_MODE_ENABLE);\n\t\tudelay(40);\n\t} else {\n\t\ttest_desc.cqid_sqid = (16 << 8) | 7;\n\n\t\ttw32_f(WDMAC_MODE, WDMAC_MODE_ENABLE);\n\t\tudelay(40);\n\t}\n\ttest_desc.flags = 0x00000005;\n\n\tfor (i = 0; i < (sizeof(test_desc) / sizeof(u32)); i++) {\n\t\tu32 val;\n\n\t\tval = *(((u32 *)&test_desc) + i);\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR,\n\t\t\t\t       sram_dma_descs + (i * sizeof(u32)));\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_DATA, val);\n\t}\n\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\n\tif (to_device)\n\t\ttw32(FTQ_DMA_HIGH_READ_FIFO_ENQDEQ, sram_dma_descs);\n\telse\n\t\ttw32(FTQ_DMA_HIGH_WRITE_FIFO_ENQDEQ, sram_dma_descs);\n\n\tret = -ENODEV;\n\tfor (i = 0; i < 40; i++) {\n\t\tu32 val;\n\n\t\tif (to_device)\n\t\t\tval = tr32(FTQ_RCVBD_COMP_FIFO_ENQDEQ);\n\t\telse\n\t\t\tval = tr32(FTQ_RCVDATA_COMP_FIFO_ENQDEQ);\n\t\tif ((val & 0xffff) == sram_dma_descs) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tudelay(100);\n\t}\n\n\treturn ret;\n}\n\n#define TEST_BUFFER_SIZE\t0x2000\n\nstatic DEFINE_PCI_DEVICE_TABLE(tg3_dma_wait_state_chipsets) = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_UNI_N_PCI15) },\n\t{ },\n};\n\nstatic int tg3_test_dma(struct tg3 *tp)\n{\n\tdma_addr_t buf_dma;\n\tu32 *buf, saved_dma_rwctrl;\n\tint ret = 0;\n\n\tbuf = dma_alloc_coherent(&tp->pdev->dev, TEST_BUFFER_SIZE,\n\t\t\t\t &buf_dma, GFP_KERNEL);\n\tif (!buf) {\n\t\tret = -ENOMEM;\n\t\tgoto out_nofree;\n\t}\n\n\ttp->dma_rwctrl = ((0x7 << DMA_RWCTRL_PCI_WRITE_CMD_SHIFT) |\n\t\t\t  (0x6 << DMA_RWCTRL_PCI_READ_CMD_SHIFT));\n\n\ttp->dma_rwctrl = tg3_calc_dma_bndry(tp, tp->dma_rwctrl);\n\n\tif (tg3_flag(tp, 57765_PLUS))\n\t\tgoto out;\n\n\tif (tg3_flag(tp, PCI_EXPRESS)) {\n\t\t/* DMA read watermark not used on PCIE */\n\t\ttp->dma_rwctrl |= 0x00180000;\n\t} else if (!tg3_flag(tp, PCIX_MODE)) {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5705 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5750)\n\t\t\ttp->dma_rwctrl |= 0x003f0000;\n\t\telse\n\t\t\ttp->dma_rwctrl |= 0x003f000f;\n\t} else {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5704) {\n\t\t\tu32 ccval = (tr32(TG3PCI_CLOCK_CTRL) & 0x1f);\n\t\t\tu32 read_water = 0x7;\n\n\t\t\t/* If the 5704 is behind the EPB bridge, we can\n\t\t\t * do the less restrictive ONE_DMA workaround for\n\t\t\t * better performance.\n\t\t\t */\n\t\t\tif (tg3_flag(tp, 40BIT_DMA_BUG) &&\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5704)\n\t\t\t\ttp->dma_rwctrl |= 0x8000;\n\t\t\telse if (ccval == 0x6 || ccval == 0x7)\n\t\t\t\ttp->dma_rwctrl |= DMA_RWCTRL_ONE_DMA;\n\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5703)\n\t\t\t\tread_water = 4;\n\t\t\t/* Set bit 23 to enable PCIX hw bug fix */\n\t\t\ttp->dma_rwctrl |=\n\t\t\t\t(read_water << DMA_RWCTRL_READ_WATER_SHIFT) |\n\t\t\t\t(0x3 << DMA_RWCTRL_WRITE_WATER_SHIFT) |\n\t\t\t\t(1 << 23);\n\t\t} else if (tg3_asic_rev(tp) == ASIC_REV_5780) {\n\t\t\t/* 5780 always in PCIX mode */\n\t\t\ttp->dma_rwctrl |= 0x00144000;\n\t\t} else if (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\t\t/* 5714 always in PCIX mode */\n\t\t\ttp->dma_rwctrl |= 0x00148000;\n\t\t} else {\n\t\t\ttp->dma_rwctrl |= 0x001b000f;\n\t\t}\n\t}\n\tif (tg3_flag(tp, ONE_DMA_AT_ONCE))\n\t\ttp->dma_rwctrl |= DMA_RWCTRL_ONE_DMA;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5704)\n\t\ttp->dma_rwctrl &= 0xfffffff0;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\t/* Remove this if it causes problems for some boards. */\n\t\ttp->dma_rwctrl |= DMA_RWCTRL_USE_MEM_READ_MULT;\n\n\t\t/* On 5700/5701 chips, we need to set this bit.\n\t\t * Otherwise the chip will issue cacheline transactions\n\t\t * to streamable DMA memory with not all the byte\n\t\t * enables turned on.  This is an error on several\n\t\t * RISC PCI controllers, in particular sparc64.\n\t\t *\n\t\t * On 5703/5704 chips, this bit has been reassigned\n\t\t * a different meaning.  In particular, it is used\n\t\t * on those chips to enable a PCI-X workaround.\n\t\t */\n\t\ttp->dma_rwctrl |= DMA_RWCTRL_ASSERT_ALL_BE;\n\t}\n\n\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\n#if 0\n\t/* Unneeded, already done by tg3_get_invariants.  */\n\ttg3_switch_clocks(tp);\n#endif\n\n\tif (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5701)\n\t\tgoto out;\n\n\t/* It is best to perform DMA test with maximum write burst size\n\t * to expose the 5700/5701 write DMA bug.\n\t */\n\tsaved_dma_rwctrl = tp->dma_rwctrl;\n\ttp->dma_rwctrl &= ~DMA_RWCTRL_WRITE_BNDRY_MASK;\n\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\n\twhile (1) {\n\t\tu32 *p = buf, i;\n\n\t\tfor (i = 0; i < TEST_BUFFER_SIZE / sizeof(u32); i++)\n\t\t\tp[i] = i;\n\n\t\t/* Send the buffer to the chip. */\n\t\tret = tg3_do_test_dma(tp, buf, buf_dma, TEST_BUFFER_SIZE, 1);\n\t\tif (ret) {\n\t\t\tdev_err(&tp->pdev->dev,\n\t\t\t\t\"%s: Buffer write failed. err = %d\\n\",\n\t\t\t\t__func__, ret);\n\t\t\tbreak;\n\t\t}\n\n#if 0\n\t\t/* validate data reached card RAM correctly. */\n\t\tfor (i = 0; i < TEST_BUFFER_SIZE / sizeof(u32); i++) {\n\t\t\tu32 val;\n\t\t\ttg3_read_mem(tp, 0x2100 + (i*4), &val);\n\t\t\tif (le32_to_cpu(val) != p[i]) {\n\t\t\t\tdev_err(&tp->pdev->dev,\n\t\t\t\t\t\"%s: Buffer corrupted on device! \"\n\t\t\t\t\t\"(%d != %d)\\n\", __func__, val, i);\n\t\t\t\t/* ret = -ENODEV here? */\n\t\t\t}\n\t\t\tp[i] = 0;\n\t\t}\n#endif\n\t\t/* Now read it back. */\n\t\tret = tg3_do_test_dma(tp, buf, buf_dma, TEST_BUFFER_SIZE, 0);\n\t\tif (ret) {\n\t\t\tdev_err(&tp->pdev->dev, \"%s: Buffer read failed. \"\n\t\t\t\t\"err = %d\\n\", __func__, ret);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Verify it. */\n\t\tfor (i = 0; i < TEST_BUFFER_SIZE / sizeof(u32); i++) {\n\t\t\tif (p[i] == i)\n\t\t\t\tcontinue;\n\n\t\t\tif ((tp->dma_rwctrl & DMA_RWCTRL_WRITE_BNDRY_MASK) !=\n\t\t\t    DMA_RWCTRL_WRITE_BNDRY_16) {\n\t\t\t\ttp->dma_rwctrl &= ~DMA_RWCTRL_WRITE_BNDRY_MASK;\n\t\t\t\ttp->dma_rwctrl |= DMA_RWCTRL_WRITE_BNDRY_16;\n\t\t\t\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tdev_err(&tp->pdev->dev,\n\t\t\t\t\t\"%s: Buffer corrupted on read back! \"\n\t\t\t\t\t\"(%d != %d)\\n\", __func__, p[i], i);\n\t\t\t\tret = -ENODEV;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (i == (TEST_BUFFER_SIZE / sizeof(u32))) {\n\t\t\t/* Success. */\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif ((tp->dma_rwctrl & DMA_RWCTRL_WRITE_BNDRY_MASK) !=\n\t    DMA_RWCTRL_WRITE_BNDRY_16) {\n\t\t/* DMA test passed without adjusting DMA boundary,\n\t\t * now look for chipsets that are known to expose the\n\t\t * DMA bug without failing the test.\n\t\t */\n\t\tif (pci_dev_present(tg3_dma_wait_state_chipsets)) {\n\t\t\ttp->dma_rwctrl &= ~DMA_RWCTRL_WRITE_BNDRY_MASK;\n\t\t\ttp->dma_rwctrl |= DMA_RWCTRL_WRITE_BNDRY_16;\n\t\t} else {\n\t\t\t/* Safe to use the calculated DMA boundary. */\n\t\t\ttp->dma_rwctrl = saved_dma_rwctrl;\n\t\t}\n\n\t\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\t}\n\nout:\n\tdma_free_coherent(&tp->pdev->dev, TEST_BUFFER_SIZE, buf, buf_dma);\nout_nofree:\n\treturn ret;\n}\n\nstatic void tg3_init_bufmgr_config(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_5705;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_57765;\n\t\ttp->bufmgr_config.mbuf_high_water =\n\t\t\tDEFAULT_MB_HIGH_WATER_57765;\n\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water_jumbo =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_5705;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water_jumbo =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_JUMBO_57765;\n\t\ttp->bufmgr_config.mbuf_high_water_jumbo =\n\t\t\tDEFAULT_MB_HIGH_WATER_JUMBO_57765;\n\t} else if (tg3_flag(tp, 5705_PLUS)) {\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_5705;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_5705;\n\t\ttp->bufmgr_config.mbuf_high_water =\n\t\t\tDEFAULT_MB_HIGH_WATER_5705;\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t\ttp->bufmgr_config.mbuf_mac_rx_low_water =\n\t\t\t\tDEFAULT_MB_MACRX_LOW_WATER_5906;\n\t\t\ttp->bufmgr_config.mbuf_high_water =\n\t\t\t\tDEFAULT_MB_HIGH_WATER_5906;\n\t\t}\n\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water_jumbo =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_JUMBO_5780;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water_jumbo =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_JUMBO_5780;\n\t\ttp->bufmgr_config.mbuf_high_water_jumbo =\n\t\t\tDEFAULT_MB_HIGH_WATER_JUMBO_5780;\n\t} else {\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER;\n\t\ttp->bufmgr_config.mbuf_high_water =\n\t\t\tDEFAULT_MB_HIGH_WATER;\n\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water_jumbo =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_JUMBO;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water_jumbo =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_JUMBO;\n\t\ttp->bufmgr_config.mbuf_high_water_jumbo =\n\t\t\tDEFAULT_MB_HIGH_WATER_JUMBO;\n\t}\n\n\ttp->bufmgr_config.dma_low_water = DEFAULT_DMA_LOW_WATER;\n\ttp->bufmgr_config.dma_high_water = DEFAULT_DMA_HIGH_WATER;\n}\n\nstatic char *tg3_phy_string(struct tg3 *tp)\n{\n\tswitch (tp->phy_id & TG3_PHY_ID_MASK) {\n\tcase TG3_PHY_ID_BCM5400:\treturn \"5400\";\n\tcase TG3_PHY_ID_BCM5401:\treturn \"5401\";\n\tcase TG3_PHY_ID_BCM5411:\treturn \"5411\";\n\tcase TG3_PHY_ID_BCM5701:\treturn \"5701\";\n\tcase TG3_PHY_ID_BCM5703:\treturn \"5703\";\n\tcase TG3_PHY_ID_BCM5704:\treturn \"5704\";\n\tcase TG3_PHY_ID_BCM5705:\treturn \"5705\";\n\tcase TG3_PHY_ID_BCM5750:\treturn \"5750\";\n\tcase TG3_PHY_ID_BCM5752:\treturn \"5752\";\n\tcase TG3_PHY_ID_BCM5714:\treturn \"5714\";\n\tcase TG3_PHY_ID_BCM5780:\treturn \"5780\";\n\tcase TG3_PHY_ID_BCM5755:\treturn \"5755\";\n\tcase TG3_PHY_ID_BCM5787:\treturn \"5787\";\n\tcase TG3_PHY_ID_BCM5784:\treturn \"5784\";\n\tcase TG3_PHY_ID_BCM5756:\treturn \"5722/5756\";\n\tcase TG3_PHY_ID_BCM5906:\treturn \"5906\";\n\tcase TG3_PHY_ID_BCM5761:\treturn \"5761\";\n\tcase TG3_PHY_ID_BCM5718C:\treturn \"5718C\";\n\tcase TG3_PHY_ID_BCM5718S:\treturn \"5718S\";\n\tcase TG3_PHY_ID_BCM57765:\treturn \"57765\";\n\tcase TG3_PHY_ID_BCM5719C:\treturn \"5719C\";\n\tcase TG3_PHY_ID_BCM5720C:\treturn \"5720C\";\n\tcase TG3_PHY_ID_BCM5762:\treturn \"5762C\";\n\tcase TG3_PHY_ID_BCM8002:\treturn \"8002/serdes\";\n\tcase 0:\t\t\treturn \"serdes\";\n\tdefault:\t\treturn \"unknown\";\n\t}\n}\n\nstatic char *tg3_bus_string(struct tg3 *tp, char *str)\n{\n\tif (tg3_flag(tp, PCI_EXPRESS)) {\n\t\tstrcpy(str, \"PCI Express\");\n\t\treturn str;\n\t} else if (tg3_flag(tp, PCIX_MODE)) {\n\t\tu32 clock_ctrl = tr32(TG3PCI_CLOCK_CTRL) & 0x1f;\n\n\t\tstrcpy(str, \"PCIX:\");\n\n\t\tif ((clock_ctrl == 7) ||\n\t\t    ((tr32(GRC_MISC_CFG) & GRC_MISC_CFG_BOARD_ID_MASK) ==\n\t\t     GRC_MISC_CFG_BOARD_ID_5704CIOBE))\n\t\t\tstrcat(str, \"133MHz\");\n\t\telse if (clock_ctrl == 0)\n\t\t\tstrcat(str, \"33MHz\");\n\t\telse if (clock_ctrl == 2)\n\t\t\tstrcat(str, \"50MHz\");\n\t\telse if (clock_ctrl == 4)\n\t\t\tstrcat(str, \"66MHz\");\n\t\telse if (clock_ctrl == 6)\n\t\t\tstrcat(str, \"100MHz\");\n\t} else {\n\t\tstrcpy(str, \"PCI:\");\n\t\tif (tg3_flag(tp, PCI_HIGH_SPEED))\n\t\t\tstrcat(str, \"66MHz\");\n\t\telse\n\t\t\tstrcat(str, \"33MHz\");\n\t}\n\tif (tg3_flag(tp, PCI_32BIT))\n\t\tstrcat(str, \":32-bit\");\n\telse\n\t\tstrcat(str, \":64-bit\");\n\treturn str;\n}\n\nstatic void tg3_init_coal(struct tg3 *tp)\n{\n\tstruct ethtool_coalesce *ec = &tp->coal;\n\n\tmemset(ec, 0, sizeof(*ec));\n\tec->cmd = ETHTOOL_GCOALESCE;\n\tec->rx_coalesce_usecs = LOW_RXCOL_TICKS;\n\tec->tx_coalesce_usecs = LOW_TXCOL_TICKS;\n\tec->rx_max_coalesced_frames = LOW_RXMAX_FRAMES;\n\tec->tx_max_coalesced_frames = LOW_TXMAX_FRAMES;\n\tec->rx_coalesce_usecs_irq = DEFAULT_RXCOAL_TICK_INT;\n\tec->tx_coalesce_usecs_irq = DEFAULT_TXCOAL_TICK_INT;\n\tec->rx_max_coalesced_frames_irq = DEFAULT_RXCOAL_MAXF_INT;\n\tec->tx_max_coalesced_frames_irq = DEFAULT_TXCOAL_MAXF_INT;\n\tec->stats_block_coalesce_usecs = DEFAULT_STAT_COAL_TICKS;\n\n\tif (tp->coalesce_mode & (HOSTCC_MODE_CLRTICK_RXBD |\n\t\t\t\t HOSTCC_MODE_CLRTICK_TXBD)) {\n\t\tec->rx_coalesce_usecs = LOW_RXCOL_TICKS_CLRTCKS;\n\t\tec->rx_coalesce_usecs_irq = DEFAULT_RXCOAL_TICK_INT_CLRTCKS;\n\t\tec->tx_coalesce_usecs = LOW_TXCOL_TICKS_CLRTCKS;\n\t\tec->tx_coalesce_usecs_irq = DEFAULT_TXCOAL_TICK_INT_CLRTCKS;\n\t}\n\n\tif (tg3_flag(tp, 5705_PLUS)) {\n\t\tec->rx_coalesce_usecs_irq = 0;\n\t\tec->tx_coalesce_usecs_irq = 0;\n\t\tec->stats_block_coalesce_usecs = 0;\n\t}\n}\n\nstatic int tg3_init_one(struct pci_dev *pdev,\n\t\t\t\t  const struct pci_device_id *ent)\n{\n\tstruct net_device *dev;\n\tstruct tg3 *tp;\n\tint i, err, pm_cap;\n\tu32 sndmbx, rcvmbx, intmbx;\n\tchar str[40];\n\tu64 dma_mask, persist_dma_mask;\n\tnetdev_features_t features = 0;\n\n\tprintk_once(KERN_INFO \"%s\\n\", version);\n\n\terr = pci_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot enable PCI device, aborting\\n\");\n\t\treturn err;\n\t}\n\n\terr = pci_request_regions(pdev, DRV_MODULE_NAME);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot obtain PCI resources, aborting\\n\");\n\t\tgoto err_out_disable_pdev;\n\t}\n\n\tpci_set_master(pdev);\n\n\t/* Find power-management capability. */\n\tpm_cap = pci_find_capability(pdev, PCI_CAP_ID_PM);\n\tif (pm_cap == 0) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Cannot find Power Management capability, aborting\\n\");\n\t\terr = -EIO;\n\t\tgoto err_out_free_res;\n\t}\n\n\terr = pci_set_power_state(pdev, PCI_D0);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Transition to D0 failed, aborting\\n\");\n\t\tgoto err_out_free_res;\n\t}\n\n\tdev = alloc_etherdev_mq(sizeof(*tp), TG3_IRQ_MAX_VECS);\n\tif (!dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_out_power_down;\n\t}\n\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\n\ttp = netdev_priv(dev);\n\ttp->pdev = pdev;\n\ttp->dev = dev;\n\ttp->pm_cap = pm_cap;\n\ttp->rx_mode = TG3_DEF_RX_MODE;\n\ttp->tx_mode = TG3_DEF_TX_MODE;\n\ttp->irq_sync = 1;\n\n\tif (tg3_debug > 0)\n\t\ttp->msg_enable = tg3_debug;\n\telse\n\t\ttp->msg_enable = TG3_DEF_MSG_ENABLE;\n\n\tif (pdev_is_ssb_gige_core(pdev)) {\n\t\ttg3_flag_set(tp, IS_SSB_CORE);\n\t\tif (ssb_gige_must_flush_posted_writes(pdev))\n\t\t\ttg3_flag_set(tp, FLUSH_POSTED_WRITES);\n\t\tif (ssb_gige_one_dma_at_once(pdev))\n\t\t\ttg3_flag_set(tp, ONE_DMA_AT_ONCE);\n\t\tif (ssb_gige_have_roboswitch(pdev))\n\t\t\ttg3_flag_set(tp, ROBOSWITCH);\n\t\tif (ssb_gige_is_rgmii(pdev))\n\t\t\ttg3_flag_set(tp, RGMII_MODE);\n\t}\n\n\t/* The word/byte swap controls here control register access byte\n\t * swapping.  DMA data byte swapping is controlled in the GRC_MODE\n\t * setting below.\n\t */\n\ttp->misc_host_ctrl =\n\t\tMISC_HOST_CTRL_MASK_PCI_INT |\n\t\tMISC_HOST_CTRL_WORD_SWAP |\n\t\tMISC_HOST_CTRL_INDIR_ACCESS |\n\t\tMISC_HOST_CTRL_PCISTATE_RW;\n\n\t/* The NONFRM (non-frame) byte/word swap controls take effect\n\t * on descriptor entries, anything which isn't packet data.\n\t *\n\t * The StrongARM chips on the board (one for tx, one for rx)\n\t * are running in big-endian mode.\n\t */\n\ttp->grc_mode = (GRC_MODE_WSWAP_DATA | GRC_MODE_BSWAP_DATA |\n\t\t\tGRC_MODE_WSWAP_NONFRM_DATA);\n#ifdef __BIG_ENDIAN\n\ttp->grc_mode |= GRC_MODE_BSWAP_NONFRM_DATA;\n#endif\n\tspin_lock_init(&tp->lock);\n\tspin_lock_init(&tp->indirect_lock);\n\tINIT_WORK(&tp->reset_task, tg3_reset_task);\n\n\ttp->regs = pci_ioremap_bar(pdev, BAR_0);\n\tif (!tp->regs) {\n\t\tdev_err(&pdev->dev, \"Cannot map device registers, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_out_free_dev;\n\t}\n\n\tif (tp->pdev->device == PCI_DEVICE_ID_TIGON3_5761 ||\n\t    tp->pdev->device == PCI_DEVICE_ID_TIGON3_5761E ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5761S ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5761SE ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717_C ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5718 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5719 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5720 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5762 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5725 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5727) {\n\t\ttg3_flag_set(tp, ENABLE_APE);\n\t\ttp->aperegs = pci_ioremap_bar(pdev, BAR_2);\n\t\tif (!tp->aperegs) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Cannot map APE registers, aborting\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_out_iounmap;\n\t\t}\n\t}\n\n\ttp->rx_pending = TG3_DEF_RX_RING_PENDING;\n\ttp->rx_jumbo_pending = TG3_DEF_RX_JUMBO_RING_PENDING;\n\n\tdev->ethtool_ops = &tg3_ethtool_ops;\n\tdev->watchdog_timeo = TG3_TX_TIMEOUT;\n\tdev->netdev_ops = &tg3_netdev_ops;\n\tdev->irq = pdev->irq;\n\n\terr = tg3_get_invariants(tp, ent);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Problem fetching invariants of chip, aborting\\n\");\n\t\tgoto err_out_apeunmap;\n\t}\n\n\t/* The EPB bridge inside 5714, 5715, and 5780 and any\n\t * device behind the EPB cannot support DMA addresses > 40-bit.\n\t * On 64-bit systems with IOMMU, use 40-bit dma_mask.\n\t * On 64-bit systems without IOMMU, use 64-bit dma_mask and\n\t * do DMA address check in tg3_start_xmit().\n\t */\n\tif (tg3_flag(tp, IS_5788))\n\t\tpersist_dma_mask = dma_mask = DMA_BIT_MASK(32);\n\telse if (tg3_flag(tp, 40BIT_DMA_BUG)) {\n\t\tpersist_dma_mask = dma_mask = DMA_BIT_MASK(40);\n#ifdef CONFIG_HIGHMEM\n\t\tdma_mask = DMA_BIT_MASK(64);\n#endif\n\t} else\n\t\tpersist_dma_mask = dma_mask = DMA_BIT_MASK(64);\n\n\t/* Configure DMA attributes. */\n\tif (dma_mask > DMA_BIT_MASK(32)) {\n\t\terr = pci_set_dma_mask(pdev, dma_mask);\n\t\tif (!err) {\n\t\t\tfeatures |= NETIF_F_HIGHDMA;\n\t\t\terr = pci_set_consistent_dma_mask(pdev,\n\t\t\t\t\t\t\t  persist_dma_mask);\n\t\t\tif (err < 0) {\n\t\t\t\tdev_err(&pdev->dev, \"Unable to obtain 64 bit \"\n\t\t\t\t\t\"DMA for consistent allocations\\n\");\n\t\t\t\tgoto err_out_apeunmap;\n\t\t\t}\n\t\t}\n\t}\n\tif (err || dma_mask == DMA_BIT_MASK(32)) {\n\t\terr = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"No usable DMA configuration, aborting\\n\");\n\t\t\tgoto err_out_apeunmap;\n\t\t}\n\t}\n\n\ttg3_init_bufmgr_config(tp);\n\n\tfeatures |= NETIF_F_HW_VLAN_TX | NETIF_F_HW_VLAN_RX;\n\n\t/* 5700 B0 chips do not support checksumming correctly due\n\t * to hardware bugs.\n\t */\n\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5700_B0) {\n\t\tfeatures |= NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_RXCSUM;\n\n\t\tif (tg3_flag(tp, 5755_PLUS))\n\t\t\tfeatures |= NETIF_F_IPV6_CSUM;\n\t}\n\n\t/* TSO is on by default on chips that support hardware TSO.\n\t * Firmware TSO on older chips gives lower performance, so it\n\t * is off by default, but can be enabled using ethtool.\n\t */\n\tif ((tg3_flag(tp, HW_TSO_1) ||\n\t     tg3_flag(tp, HW_TSO_2) ||\n\t     tg3_flag(tp, HW_TSO_3)) &&\n\t    (features & NETIF_F_IP_CSUM))\n\t\tfeatures |= NETIF_F_TSO;\n\tif (tg3_flag(tp, HW_TSO_2) || tg3_flag(tp, HW_TSO_3)) {\n\t\tif (features & NETIF_F_IPV6_CSUM)\n\t\t\tfeatures |= NETIF_F_TSO6;\n\t\tif (tg3_flag(tp, HW_TSO_3) ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5761 ||\n\t\t    (tg3_asic_rev(tp) == ASIC_REV_5784 &&\n\t\t     tg3_chip_rev(tp) != CHIPREV_5784_AX) ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_57780)\n\t\t\tfeatures |= NETIF_F_TSO_ECN;\n\t}\n\n\tdev->features |= features;\n\tdev->vlan_features |= features;\n\n\t/*\n\t * Add loopback capability only for a subset of devices that support\n\t * MAC-LOOPBACK. Eventually this need to be enhanced to allow INT-PHY\n\t * loopback for the remaining devices.\n\t */\n\tif (tg3_asic_rev(tp) != ASIC_REV_5780 &&\n\t    !tg3_flag(tp, CPMU_PRESENT))\n\t\t/* Add the loopback capability */\n\t\tfeatures |= NETIF_F_LOOPBACK;\n\n\tdev->hw_features |= features;\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A1 &&\n\t    !tg3_flag(tp, TSO_CAPABLE) &&\n\t    !(tr32(TG3PCI_PCISTATE) & PCISTATE_BUS_SPEED_HIGH)) {\n\t\ttg3_flag_set(tp, MAX_RXPEND_64);\n\t\ttp->rx_pending = 63;\n\t}\n\n\terr = tg3_get_device_address(tp);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Could not obtain valid ethernet address, aborting\\n\");\n\t\tgoto err_out_apeunmap;\n\t}\n\n\t/*\n\t * Reset chip in case UNDI or EFI driver did not shutdown\n\t * DMA self test will enable WDMAC and we'll see (spurious)\n\t * pending DMA on the PCI bus at that point.\n\t */\n\tif ((tr32(HOSTCC_MODE) & HOSTCC_MODE_ENABLE) ||\n\t    (tr32(WDMAC_MODE) & WDMAC_MODE_ENABLE)) {\n\t\ttw32(MEMARB_MODE, MEMARB_MODE_ENABLE);\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t}\n\n\terr = tg3_test_dma(tp);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"DMA engine test failed, aborting\\n\");\n\t\tgoto err_out_apeunmap;\n\t}\n\n\tintmbx = MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW;\n\trcvmbx = MAILBOX_RCVRET_CON_IDX_0 + TG3_64BIT_REG_LOW;\n\tsndmbx = MAILBOX_SNDHOST_PROD_IDX_0 + TG3_64BIT_REG_LOW;\n\tfor (i = 0; i < tp->irq_max; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\ttnapi->tp = tp;\n\t\ttnapi->tx_pending = TG3_DEF_TX_RING_PENDING;\n\n\t\ttnapi->int_mbox = intmbx;\n\t\tif (i <= 4)\n\t\t\tintmbx += 0x8;\n\t\telse\n\t\t\tintmbx += 0x4;\n\n\t\ttnapi->consmbox = rcvmbx;\n\t\ttnapi->prodmbox = sndmbx;\n\n\t\tif (i)\n\t\t\ttnapi->coal_now = HOSTCC_MODE_COAL_VEC1_NOW << (i - 1);\n\t\telse\n\t\t\ttnapi->coal_now = HOSTCC_MODE_NOW;\n\n\t\tif (!tg3_flag(tp, SUPPORT_MSIX))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If we support MSIX, we'll be using RSS.  If we're using\n\t\t * RSS, the first vector only handles link interrupts and the\n\t\t * remaining vectors handle rx and tx interrupts.  Reuse the\n\t\t * mailbox values for the next iteration.  The values we setup\n\t\t * above are still useful for the single vectored mode.\n\t\t */\n\t\tif (!i)\n\t\t\tcontinue;\n\n\t\trcvmbx += 0x8;\n\n\t\tif (sndmbx & 0x4)\n\t\t\tsndmbx -= 0x4;\n\t\telse\n\t\t\tsndmbx += 0xc;\n\t}\n\n\ttg3_init_coal(tp);\n\n\tpci_set_drvdata(pdev, dev);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\ttg3_flag_set(tp, PTP_CAPABLE);\n\n\tif (tg3_flag(tp, 5717_PLUS)) {\n\t\t/* Resume a low-power mode */\n\t\ttg3_frob_aux_power(tp, false);\n\t}\n\n\ttg3_timer_init(tp);\n\n\ttg3_carrier_off(tp);\n\n\terr = register_netdev(dev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot register net device, aborting\\n\");\n\t\tgoto err_out_apeunmap;\n\t}\n\n\tnetdev_info(dev, \"Tigon3 [partno(%s) rev %04x] (%s) MAC address %pM\\n\",\n\t\t    tp->board_part_number,\n\t\t    tg3_chip_rev_id(tp),\n\t\t    tg3_bus_string(tp, str),\n\t\t    dev->dev_addr);\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_CONNECTED) {\n\t\tstruct phy_device *phydev;\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\t\tnetdev_info(dev,\n\t\t\t    \"attached PHY driver [%s] (mii_bus:phy_addr=%s)\\n\",\n\t\t\t    phydev->drv->name, dev_name(&phydev->dev));\n\t} else {\n\t\tchar *ethtype;\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_10_100_ONLY)\n\t\t\tethtype = \"10/100Base-TX\";\n\t\telse if (tp->phy_flags & TG3_PHYFLG_ANY_SERDES)\n\t\t\tethtype = \"1000Base-SX\";\n\t\telse\n\t\t\tethtype = \"10/100/1000Base-T\";\n\n\t\tnetdev_info(dev, \"attached PHY is %s (%s Ethernet) \"\n\t\t\t    \"(WireSpeed[%d], EEE[%d])\\n\",\n\t\t\t    tg3_phy_string(tp), ethtype,\n\t\t\t    (tp->phy_flags & TG3_PHYFLG_NO_ETH_WIRE_SPEED) == 0,\n\t\t\t    (tp->phy_flags & TG3_PHYFLG_EEE_CAP) != 0);\n\t}\n\n\tnetdev_info(dev, \"RXcsums[%d] LinkChgREG[%d] MIirq[%d] ASF[%d] TSOcap[%d]\\n\",\n\t\t    (dev->features & NETIF_F_RXCSUM) != 0,\n\t\t    tg3_flag(tp, USE_LINKCHG_REG) != 0,\n\t\t    (tp->phy_flags & TG3_PHYFLG_USE_MI_INTERRUPT) != 0,\n\t\t    tg3_flag(tp, ENABLE_ASF) != 0,\n\t\t    tg3_flag(tp, TSO_CAPABLE) != 0);\n\tnetdev_info(dev, \"dma_rwctrl[%08x] dma_mask[%d-bit]\\n\",\n\t\t    tp->dma_rwctrl,\n\t\t    pdev->dma_mask == DMA_BIT_MASK(32) ? 32 :\n\t\t    ((u64)pdev->dma_mask) == DMA_BIT_MASK(40) ? 40 : 64);\n\n\tpci_save_state(pdev);\n\n\treturn 0;\n\nerr_out_apeunmap:\n\tif (tp->aperegs) {\n\t\tiounmap(tp->aperegs);\n\t\ttp->aperegs = NULL;\n\t}\n\nerr_out_iounmap:\n\tif (tp->regs) {\n\t\tiounmap(tp->regs);\n\t\ttp->regs = NULL;\n\t}\n\nerr_out_free_dev:\n\tfree_netdev(dev);\n\nerr_out_power_down:\n\tpci_set_power_state(pdev, PCI_D3hot);\n\nerr_out_free_res:\n\tpci_release_regions(pdev);\n\nerr_out_disable_pdev:\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\n\treturn err;\n}\n\nstatic void tg3_remove_one(struct pci_dev *pdev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\n\tif (dev) {\n\t\tstruct tg3 *tp = netdev_priv(dev);\n\n\t\trelease_firmware(tp->fw);\n\n\t\ttg3_reset_task_cancel(tp);\n\n\t\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\t\ttg3_phy_fini(tp);\n\t\t\ttg3_mdio_fini(tp);\n\t\t}\n\n\t\tunregister_netdev(dev);\n\t\tif (tp->aperegs) {\n\t\t\tiounmap(tp->aperegs);\n\t\t\ttp->aperegs = NULL;\n\t\t}\n\t\tif (tp->regs) {\n\t\t\tiounmap(tp->regs);\n\t\t\ttp->regs = NULL;\n\t\t}\n\t\tfree_netdev(dev);\n\t\tpci_release_regions(pdev);\n\t\tpci_disable_device(pdev);\n\t\tpci_set_drvdata(pdev, NULL);\n\t}\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int tg3_suspend(struct device *device)\n{\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\ttg3_reset_task_cancel(tp);\n\ttg3_phy_stop(tp);\n\ttg3_netif_stop(tp);\n\n\ttg3_timer_stop(tp);\n\n\ttg3_full_lock(tp, 1);\n\ttg3_disable_ints(tp);\n\ttg3_full_unlock(tp);\n\n\tnetif_device_detach(dev);\n\n\ttg3_full_lock(tp, 0);\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\ttg3_flag_clear(tp, INIT_COMPLETE);\n\ttg3_full_unlock(tp);\n\n\terr = tg3_power_down_prepare(tp);\n\tif (err) {\n\t\tint err2;\n\n\t\ttg3_full_lock(tp, 0);\n\n\t\ttg3_flag_set(tp, INIT_COMPLETE);\n\t\terr2 = tg3_restart_hw(tp, 1);\n\t\tif (err2)\n\t\t\tgoto out;\n\n\t\ttg3_timer_start(tp);\n\n\t\tnetif_device_attach(dev);\n\t\ttg3_netif_start(tp);\n\nout:\n\t\ttg3_full_unlock(tp);\n\n\t\tif (!err2)\n\t\t\ttg3_phy_start(tp);\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_resume(struct device *device)\n{\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tnetif_device_attach(dev);\n\n\ttg3_full_lock(tp, 0);\n\n\ttg3_flag_set(tp, INIT_COMPLETE);\n\terr = tg3_restart_hw(tp, 1);\n\tif (err)\n\t\tgoto out;\n\n\ttg3_timer_start(tp);\n\n\ttg3_netif_start(tp);\n\nout:\n\ttg3_full_unlock(tp);\n\n\tif (!err)\n\t\ttg3_phy_start(tp);\n\n\treturn err;\n}\n\nstatic SIMPLE_DEV_PM_OPS(tg3_pm_ops, tg3_suspend, tg3_resume);\n#define TG3_PM_OPS (&tg3_pm_ops)\n\n#else\n\n#define TG3_PM_OPS NULL\n\n#endif /* CONFIG_PM_SLEEP */\n\n/**\n * tg3_io_error_detected - called when PCI error is detected\n * @pdev: Pointer to PCI device\n * @state: The current pci connection state\n *\n * This function is called after a PCI bus error affecting\n * this device has been detected.\n */\nstatic pci_ers_result_t tg3_io_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t      pci_channel_state_t state)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(netdev);\n\tpci_ers_result_t err = PCI_ERS_RESULT_NEED_RESET;\n\n\tnetdev_info(netdev, \"PCI I/O error detected\\n\");\n\n\trtnl_lock();\n\n\tif (!netif_running(netdev))\n\t\tgoto done;\n\n\ttg3_phy_stop(tp);\n\n\ttg3_netif_stop(tp);\n\n\ttg3_timer_stop(tp);\n\n\t/* Want to make sure that the reset task doesn't run */\n\ttg3_reset_task_cancel(tp);\n\n\tnetif_device_detach(netdev);\n\n\t/* Clean up software state, even if MMIO is blocked */\n\ttg3_full_lock(tp, 0);\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 0);\n\ttg3_full_unlock(tp);\n\ndone:\n\tif (state == pci_channel_io_perm_failure)\n\t\terr = PCI_ERS_RESULT_DISCONNECT;\n\telse\n\t\tpci_disable_device(pdev);\n\n\trtnl_unlock();\n\n\treturn err;\n}\n\n/**\n * tg3_io_slot_reset - called after the pci bus has been reset.\n * @pdev: Pointer to PCI device\n *\n * Restart the card from scratch, as if from a cold-boot.\n * At this point, the card has exprienced a hard reset,\n * followed by fixups by BIOS, and has its config space\n * set up identically to what it was at cold boot.\n */\nstatic pci_ers_result_t tg3_io_slot_reset(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(netdev);\n\tpci_ers_result_t rc = PCI_ERS_RESULT_DISCONNECT;\n\tint err;\n\n\trtnl_lock();\n\n\tif (pci_enable_device(pdev)) {\n\t\tnetdev_err(netdev, \"Cannot re-enable PCI device after reset.\\n\");\n\t\tgoto done;\n\t}\n\n\tpci_set_master(pdev);\n\tpci_restore_state(pdev);\n\tpci_save_state(pdev);\n\n\tif (!netif_running(netdev)) {\n\t\trc = PCI_ERS_RESULT_RECOVERED;\n\t\tgoto done;\n\t}\n\n\terr = tg3_power_up(tp);\n\tif (err)\n\t\tgoto done;\n\n\trc = PCI_ERS_RESULT_RECOVERED;\n\ndone:\n\trtnl_unlock();\n\n\treturn rc;\n}\n\n/**\n * tg3_io_resume - called when traffic can start flowing again.\n * @pdev: Pointer to PCI device\n *\n * This callback is called when the error recovery driver tells\n * us that its OK to resume normal operation.\n */\nstatic void tg3_io_resume(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(netdev);\n\tint err;\n\n\trtnl_lock();\n\n\tif (!netif_running(netdev))\n\t\tgoto done;\n\n\ttg3_full_lock(tp, 0);\n\ttg3_flag_set(tp, INIT_COMPLETE);\n\terr = tg3_restart_hw(tp, 1);\n\tif (err) {\n\t\ttg3_full_unlock(tp);\n\t\tnetdev_err(netdev, \"Cannot restart hardware after reset.\\n\");\n\t\tgoto done;\n\t}\n\n\tnetif_device_attach(netdev);\n\n\ttg3_timer_start(tp);\n\n\ttg3_netif_start(tp);\n\n\ttg3_full_unlock(tp);\n\n\ttg3_phy_start(tp);\n\ndone:\n\trtnl_unlock();\n}\n\nstatic const struct pci_error_handlers tg3_err_handler = {\n\t.error_detected\t= tg3_io_error_detected,\n\t.slot_reset\t= tg3_io_slot_reset,\n\t.resume\t\t= tg3_io_resume\n};\n\nstatic struct pci_driver tg3_driver = {\n\t.name\t\t= DRV_MODULE_NAME,\n\t.id_table\t= tg3_pci_tbl,\n\t.probe\t\t= tg3_init_one,\n\t.remove\t\t= tg3_remove_one,\n\t.err_handler\t= &tg3_err_handler,\n\t.driver.pm\t= TG3_PM_OPS,\n};\n\nstatic int __init tg3_init(void)\n{\n\treturn pci_register_driver(&tg3_driver);\n}\n\nstatic void __exit tg3_cleanup(void)\n{\n\tpci_unregister_driver(&tg3_driver);\n}\n\nmodule_init(tg3_init);\nmodule_exit(tg3_cleanup);\n"], "fixing_code": ["/*\n * tg3.c: Broadcom Tigon3 ethernet driver.\n *\n * Copyright (C) 2001, 2002, 2003, 2004 David S. Miller (davem@redhat.com)\n * Copyright (C) 2001, 2002, 2003 Jeff Garzik (jgarzik@pobox.com)\n * Copyright (C) 2004 Sun Microsystems Inc.\n * Copyright (C) 2005-2013 Broadcom Corporation.\n *\n * Firmware is:\n *\tDerived from proprietary unpublished source code,\n *\tCopyright (C) 2000-2003 Broadcom Corporation.\n *\n *\tPermission is hereby granted for the distribution of this firmware\n *\tdata in hexadecimal or equivalent format, provided this copyright\n *\tnotice is accompanying it.\n */\n\n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/stringify.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/in.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/ioport.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/ethtool.h>\n#include <linux/mdio.h>\n#include <linux/mii.h>\n#include <linux/phy.h>\n#include <linux/brcmphy.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/workqueue.h>\n#include <linux/prefetch.h>\n#include <linux/dma-mapping.h>\n#include <linux/firmware.h>\n#include <linux/ssb/ssb_driver_gige.h>\n#include <linux/hwmon.h>\n#include <linux/hwmon-sysfs.h>\n\n#include <net/checksum.h>\n#include <net/ip.h>\n\n#include <linux/io.h>\n#include <asm/byteorder.h>\n#include <linux/uaccess.h>\n\n#include <uapi/linux/net_tstamp.h>\n#include <linux/ptp_clock_kernel.h>\n\n#ifdef CONFIG_SPARC\n#include <asm/idprom.h>\n#include <asm/prom.h>\n#endif\n\n#define BAR_0\t0\n#define BAR_2\t2\n\n#include \"tg3.h\"\n\n/* Functions & macros to verify TG3_FLAGS types */\n\nstatic inline int _tg3_flag(enum TG3_FLAGS flag, unsigned long *bits)\n{\n\treturn test_bit(flag, bits);\n}\n\nstatic inline void _tg3_flag_set(enum TG3_FLAGS flag, unsigned long *bits)\n{\n\tset_bit(flag, bits);\n}\n\nstatic inline void _tg3_flag_clear(enum TG3_FLAGS flag, unsigned long *bits)\n{\n\tclear_bit(flag, bits);\n}\n\n#define tg3_flag(tp, flag)\t\t\t\t\\\n\t_tg3_flag(TG3_FLAG_##flag, (tp)->tg3_flags)\n#define tg3_flag_set(tp, flag)\t\t\t\t\\\n\t_tg3_flag_set(TG3_FLAG_##flag, (tp)->tg3_flags)\n#define tg3_flag_clear(tp, flag)\t\t\t\\\n\t_tg3_flag_clear(TG3_FLAG_##flag, (tp)->tg3_flags)\n\n#define DRV_MODULE_NAME\t\t\"tg3\"\n#define TG3_MAJ_NUM\t\t\t3\n#define TG3_MIN_NUM\t\t\t130\n#define DRV_MODULE_VERSION\t\\\n\t__stringify(TG3_MAJ_NUM) \".\" __stringify(TG3_MIN_NUM)\n#define DRV_MODULE_RELDATE\t\"February 14, 2013\"\n\n#define RESET_KIND_SHUTDOWN\t0\n#define RESET_KIND_INIT\t\t1\n#define RESET_KIND_SUSPEND\t2\n\n#define TG3_DEF_RX_MODE\t\t0\n#define TG3_DEF_TX_MODE\t\t0\n#define TG3_DEF_MSG_ENABLE\t  \\\n\t(NETIF_MSG_DRV\t\t| \\\n\t NETIF_MSG_PROBE\t| \\\n\t NETIF_MSG_LINK\t\t| \\\n\t NETIF_MSG_TIMER\t| \\\n\t NETIF_MSG_IFDOWN\t| \\\n\t NETIF_MSG_IFUP\t\t| \\\n\t NETIF_MSG_RX_ERR\t| \\\n\t NETIF_MSG_TX_ERR)\n\n#define TG3_GRC_LCLCTL_PWRSW_DELAY\t100\n\n/* length of time before we decide the hardware is borked,\n * and dev->tx_timeout() should be called to fix the problem\n */\n\n#define TG3_TX_TIMEOUT\t\t\t(5 * HZ)\n\n/* hardware minimum and maximum for a single frame's data payload */\n#define TG3_MIN_MTU\t\t\t60\n#define TG3_MAX_MTU(tp)\t\\\n\t(tg3_flag(tp, JUMBO_CAPABLE) ? 9000 : 1500)\n\n/* These numbers seem to be hard coded in the NIC firmware somehow.\n * You can't change the ring sizes, but you can change where you place\n * them in the NIC onboard memory.\n */\n#define TG3_RX_STD_RING_SIZE(tp) \\\n\t(tg3_flag(tp, LRG_PROD_RING_CAP) ? \\\n\t TG3_RX_STD_MAX_SIZE_5717 : TG3_RX_STD_MAX_SIZE_5700)\n#define TG3_DEF_RX_RING_PENDING\t\t200\n#define TG3_RX_JMB_RING_SIZE(tp) \\\n\t(tg3_flag(tp, LRG_PROD_RING_CAP) ? \\\n\t TG3_RX_JMB_MAX_SIZE_5717 : TG3_RX_JMB_MAX_SIZE_5700)\n#define TG3_DEF_RX_JUMBO_RING_PENDING\t100\n\n/* Do not place this n-ring entries value into the tp struct itself,\n * we really want to expose these constants to GCC so that modulo et\n * al.  operations are done with shifts and masks instead of with\n * hw multiply/modulo instructions.  Another solution would be to\n * replace things like '% foo' with '& (foo - 1)'.\n */\n\n#define TG3_TX_RING_SIZE\t\t512\n#define TG3_DEF_TX_RING_PENDING\t\t(TG3_TX_RING_SIZE - 1)\n\n#define TG3_RX_STD_RING_BYTES(tp) \\\n\t(sizeof(struct tg3_rx_buffer_desc) * TG3_RX_STD_RING_SIZE(tp))\n#define TG3_RX_JMB_RING_BYTES(tp) \\\n\t(sizeof(struct tg3_ext_rx_buffer_desc) * TG3_RX_JMB_RING_SIZE(tp))\n#define TG3_RX_RCB_RING_BYTES(tp) \\\n\t(sizeof(struct tg3_rx_buffer_desc) * (tp->rx_ret_ring_mask + 1))\n#define TG3_TX_RING_BYTES\t(sizeof(struct tg3_tx_buffer_desc) * \\\n\t\t\t\t TG3_TX_RING_SIZE)\n#define NEXT_TX(N)\t\t(((N) + 1) & (TG3_TX_RING_SIZE - 1))\n\n#define TG3_DMA_BYTE_ENAB\t\t64\n\n#define TG3_RX_STD_DMA_SZ\t\t1536\n#define TG3_RX_JMB_DMA_SZ\t\t9046\n\n#define TG3_RX_DMA_TO_MAP_SZ(x)\t\t((x) + TG3_DMA_BYTE_ENAB)\n\n#define TG3_RX_STD_MAP_SZ\t\tTG3_RX_DMA_TO_MAP_SZ(TG3_RX_STD_DMA_SZ)\n#define TG3_RX_JMB_MAP_SZ\t\tTG3_RX_DMA_TO_MAP_SZ(TG3_RX_JMB_DMA_SZ)\n\n#define TG3_RX_STD_BUFF_RING_SIZE(tp) \\\n\t(sizeof(struct ring_info) * TG3_RX_STD_RING_SIZE(tp))\n\n#define TG3_RX_JMB_BUFF_RING_SIZE(tp) \\\n\t(sizeof(struct ring_info) * TG3_RX_JMB_RING_SIZE(tp))\n\n/* Due to a hardware bug, the 5701 can only DMA to memory addresses\n * that are at least dword aligned when used in PCIX mode.  The driver\n * works around this bug by double copying the packet.  This workaround\n * is built into the normal double copy length check for efficiency.\n *\n * However, the double copy is only necessary on those architectures\n * where unaligned memory accesses are inefficient.  For those architectures\n * where unaligned memory accesses incur little penalty, we can reintegrate\n * the 5701 in the normal rx path.  Doing so saves a device structure\n * dereference by hardcoding the double copy threshold in place.\n */\n#define TG3_RX_COPY_THRESHOLD\t\t256\n#if NET_IP_ALIGN == 0 || defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\t#define TG3_RX_COPY_THRESH(tp)\tTG3_RX_COPY_THRESHOLD\n#else\n\t#define TG3_RX_COPY_THRESH(tp)\t((tp)->rx_copy_thresh)\n#endif\n\n#if (NET_IP_ALIGN != 0)\n#define TG3_RX_OFFSET(tp)\t((tp)->rx_offset)\n#else\n#define TG3_RX_OFFSET(tp)\t(NET_SKB_PAD)\n#endif\n\n/* minimum number of free TX descriptors required to wake up TX process */\n#define TG3_TX_WAKEUP_THRESH(tnapi)\t\t((tnapi)->tx_pending / 4)\n#define TG3_TX_BD_DMA_MAX_2K\t\t2048\n#define TG3_TX_BD_DMA_MAX_4K\t\t4096\n\n#define TG3_RAW_IP_ALIGN 2\n\n#define TG3_FW_UPDATE_TIMEOUT_SEC\t5\n#define TG3_FW_UPDATE_FREQ_SEC\t\t(TG3_FW_UPDATE_TIMEOUT_SEC / 2)\n\n#define FIRMWARE_TG3\t\t\"tigon/tg3.bin\"\n#define FIRMWARE_TG3TSO\t\t\"tigon/tg3_tso.bin\"\n#define FIRMWARE_TG3TSO5\t\"tigon/tg3_tso5.bin\"\n\nstatic char version[] =\n\tDRV_MODULE_NAME \".c:v\" DRV_MODULE_VERSION \" (\" DRV_MODULE_RELDATE \")\";\n\nMODULE_AUTHOR(\"David S. Miller (davem@redhat.com) and Jeff Garzik (jgarzik@pobox.com)\");\nMODULE_DESCRIPTION(\"Broadcom Tigon3 ethernet driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(DRV_MODULE_VERSION);\nMODULE_FIRMWARE(FIRMWARE_TG3);\nMODULE_FIRMWARE(FIRMWARE_TG3TSO);\nMODULE_FIRMWARE(FIRMWARE_TG3TSO5);\n\nstatic int tg3_debug = -1;\t/* -1 == use TG3_DEF_MSG_ENABLE as value */\nmodule_param(tg3_debug, int, 0);\nMODULE_PARM_DESC(tg3_debug, \"Tigon3 bitmapped debugging message enable value\");\n\n#define TG3_DRV_DATA_FLAG_10_100_ONLY\t0x0001\n#define TG3_DRV_DATA_FLAG_5705_10_100\t0x0002\n\nstatic DEFINE_PCI_DEVICE_TABLE(tg3_pci_tbl) = {\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5700)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5701)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5702)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5703)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5704)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5702FE)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705_2)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705M_2)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5702X)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5703X)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5704S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5702A3)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5703A3)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5782)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5788)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5789)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5901),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY |\n\t\t\tTG3_DRV_DATA_FLAG_5705_10_100},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5901_2),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY |\n\t\t\tTG3_DRV_DATA_FLAG_5705_10_100},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5704S_2)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5705F),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY |\n\t\t\tTG3_DRV_DATA_FLAG_5705_10_100},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5721)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5722)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5750)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5751)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5751M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5751F),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5752)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5752M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5753)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5753M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5753F),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5754)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5754M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5755)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5755M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5756)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5786)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5787)},\n\t{PCI_DEVICE_SUB(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5787M,\n\t\t\tPCI_VENDOR_ID_LENOVO,\n\t\t\tTG3PCI_SUBDEVICE_ID_LENOVO_5787M),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5787M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5787F),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5714)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5714S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5715)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5715S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5780)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5780S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5781)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5906)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5906M)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5784)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5764)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5723)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5761)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, PCI_DEVICE_ID_TIGON3_5761E)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5761S)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5761SE)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5785_G)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5785_F)},\n\t{PCI_DEVICE_SUB(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57780,\n\t\t\tPCI_VENDOR_ID_AI, TG3PCI_SUBDEVICE_ID_ACER_57780_A),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE_SUB(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57780,\n\t\t\tPCI_VENDOR_ID_AI, TG3PCI_SUBDEVICE_ID_ACER_57780_B),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57780)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57760)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57790),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57788)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5717)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5717_C)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5718)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57781)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57785)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57761)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57765)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57791),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57795),\n\t .driver_data = TG3_DRV_DATA_FLAG_10_100_ONLY},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5719)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5720)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57762)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_57766)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5762)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5725)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_BROADCOM, TG3PCI_DEVICE_TIGON3_5727)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_SYSKONNECT, PCI_DEVICE_ID_SYSKONNECT_9DXX)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_SYSKONNECT, PCI_DEVICE_ID_SYSKONNECT_9MXX)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_ALTIMA, PCI_DEVICE_ID_ALTIMA_AC1000)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_ALTIMA, PCI_DEVICE_ID_ALTIMA_AC1001)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_ALTIMA, PCI_DEVICE_ID_ALTIMA_AC1003)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_ALTIMA, PCI_DEVICE_ID_ALTIMA_AC9100)},\n\t{PCI_DEVICE(PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_TIGON3)},\n\t{PCI_DEVICE(0x10cf, 0x11a2)}, /* Fujitsu 1000base-SX with BCM5703SKHB */\n\t{}\n};\n\nMODULE_DEVICE_TABLE(pci, tg3_pci_tbl);\n\nstatic const struct {\n\tconst char string[ETH_GSTRING_LEN];\n} ethtool_stats_keys[] = {\n\t{ \"rx_octets\" },\n\t{ \"rx_fragments\" },\n\t{ \"rx_ucast_packets\" },\n\t{ \"rx_mcast_packets\" },\n\t{ \"rx_bcast_packets\" },\n\t{ \"rx_fcs_errors\" },\n\t{ \"rx_align_errors\" },\n\t{ \"rx_xon_pause_rcvd\" },\n\t{ \"rx_xoff_pause_rcvd\" },\n\t{ \"rx_mac_ctrl_rcvd\" },\n\t{ \"rx_xoff_entered\" },\n\t{ \"rx_frame_too_long_errors\" },\n\t{ \"rx_jabbers\" },\n\t{ \"rx_undersize_packets\" },\n\t{ \"rx_in_length_errors\" },\n\t{ \"rx_out_length_errors\" },\n\t{ \"rx_64_or_less_octet_packets\" },\n\t{ \"rx_65_to_127_octet_packets\" },\n\t{ \"rx_128_to_255_octet_packets\" },\n\t{ \"rx_256_to_511_octet_packets\" },\n\t{ \"rx_512_to_1023_octet_packets\" },\n\t{ \"rx_1024_to_1522_octet_packets\" },\n\t{ \"rx_1523_to_2047_octet_packets\" },\n\t{ \"rx_2048_to_4095_octet_packets\" },\n\t{ \"rx_4096_to_8191_octet_packets\" },\n\t{ \"rx_8192_to_9022_octet_packets\" },\n\n\t{ \"tx_octets\" },\n\t{ \"tx_collisions\" },\n\n\t{ \"tx_xon_sent\" },\n\t{ \"tx_xoff_sent\" },\n\t{ \"tx_flow_control\" },\n\t{ \"tx_mac_errors\" },\n\t{ \"tx_single_collisions\" },\n\t{ \"tx_mult_collisions\" },\n\t{ \"tx_deferred\" },\n\t{ \"tx_excessive_collisions\" },\n\t{ \"tx_late_collisions\" },\n\t{ \"tx_collide_2times\" },\n\t{ \"tx_collide_3times\" },\n\t{ \"tx_collide_4times\" },\n\t{ \"tx_collide_5times\" },\n\t{ \"tx_collide_6times\" },\n\t{ \"tx_collide_7times\" },\n\t{ \"tx_collide_8times\" },\n\t{ \"tx_collide_9times\" },\n\t{ \"tx_collide_10times\" },\n\t{ \"tx_collide_11times\" },\n\t{ \"tx_collide_12times\" },\n\t{ \"tx_collide_13times\" },\n\t{ \"tx_collide_14times\" },\n\t{ \"tx_collide_15times\" },\n\t{ \"tx_ucast_packets\" },\n\t{ \"tx_mcast_packets\" },\n\t{ \"tx_bcast_packets\" },\n\t{ \"tx_carrier_sense_errors\" },\n\t{ \"tx_discards\" },\n\t{ \"tx_errors\" },\n\n\t{ \"dma_writeq_full\" },\n\t{ \"dma_write_prioq_full\" },\n\t{ \"rxbds_empty\" },\n\t{ \"rx_discards\" },\n\t{ \"rx_errors\" },\n\t{ \"rx_threshold_hit\" },\n\n\t{ \"dma_readq_full\" },\n\t{ \"dma_read_prioq_full\" },\n\t{ \"tx_comp_queue_full\" },\n\n\t{ \"ring_set_send_prod_index\" },\n\t{ \"ring_status_update\" },\n\t{ \"nic_irqs\" },\n\t{ \"nic_avoided_irqs\" },\n\t{ \"nic_tx_threshold_hit\" },\n\n\t{ \"mbuf_lwm_thresh_hit\" },\n};\n\n#define TG3_NUM_STATS\tARRAY_SIZE(ethtool_stats_keys)\n#define TG3_NVRAM_TEST\t\t0\n#define TG3_LINK_TEST\t\t1\n#define TG3_REGISTER_TEST\t2\n#define TG3_MEMORY_TEST\t\t3\n#define TG3_MAC_LOOPB_TEST\t4\n#define TG3_PHY_LOOPB_TEST\t5\n#define TG3_EXT_LOOPB_TEST\t6\n#define TG3_INTERRUPT_TEST\t7\n\n\nstatic const struct {\n\tconst char string[ETH_GSTRING_LEN];\n} ethtool_test_keys[] = {\n\t[TG3_NVRAM_TEST]\t= { \"nvram test        (online) \" },\n\t[TG3_LINK_TEST]\t\t= { \"link test         (online) \" },\n\t[TG3_REGISTER_TEST]\t= { \"register test     (offline)\" },\n\t[TG3_MEMORY_TEST]\t= { \"memory test       (offline)\" },\n\t[TG3_MAC_LOOPB_TEST]\t= { \"mac loopback test (offline)\" },\n\t[TG3_PHY_LOOPB_TEST]\t= { \"phy loopback test (offline)\" },\n\t[TG3_EXT_LOOPB_TEST]\t= { \"ext loopback test (offline)\" },\n\t[TG3_INTERRUPT_TEST]\t= { \"interrupt test    (offline)\" },\n};\n\n#define TG3_NUM_TEST\tARRAY_SIZE(ethtool_test_keys)\n\n\nstatic void tg3_write32(struct tg3 *tp, u32 off, u32 val)\n{\n\twritel(val, tp->regs + off);\n}\n\nstatic u32 tg3_read32(struct tg3 *tp, u32 off)\n{\n\treturn readl(tp->regs + off);\n}\n\nstatic void tg3_ape_write32(struct tg3 *tp, u32 off, u32 val)\n{\n\twritel(val, tp->aperegs + off);\n}\n\nstatic u32 tg3_ape_read32(struct tg3 *tp, u32 off)\n{\n\treturn readl(tp->aperegs + off);\n}\n\nstatic void tg3_write_indirect_reg32(struct tg3 *tp, u32 off, u32 val)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_BASE_ADDR, off);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_DATA, val);\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n}\n\nstatic void tg3_write_flush_reg32(struct tg3 *tp, u32 off, u32 val)\n{\n\twritel(val, tp->regs + off);\n\treadl(tp->regs + off);\n}\n\nstatic u32 tg3_read_indirect_reg32(struct tg3 *tp, u32 off)\n{\n\tunsigned long flags;\n\tu32 val;\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_BASE_ADDR, off);\n\tpci_read_config_dword(tp->pdev, TG3PCI_REG_DATA, &val);\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n\treturn val;\n}\n\nstatic void tg3_write_indirect_mbox(struct tg3 *tp, u32 off, u32 val)\n{\n\tunsigned long flags;\n\n\tif (off == (MAILBOX_RCVRET_CON_IDX_0 + TG3_64BIT_REG_LOW)) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_RCV_RET_RING_CON_IDX +\n\t\t\t\t       TG3_64BIT_REG_LOW, val);\n\t\treturn;\n\t}\n\tif (off == TG3_RX_STD_PROD_IDX_REG) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_STD_RING_PROD_IDX +\n\t\t\t\t       TG3_64BIT_REG_LOW, val);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_BASE_ADDR, off + 0x5600);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_DATA, val);\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n\n\t/* In indirect mode when disabling interrupts, we also need\n\t * to clear the interrupt bit in the GRC local ctrl register.\n\t */\n\tif ((off == (MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW)) &&\n\t    (val == 0x1)) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MISC_LOCAL_CTRL,\n\t\t\t\t       tp->grc_local_ctrl|GRC_LCLCTRL_CLEARINT);\n\t}\n}\n\nstatic u32 tg3_read_indirect_mbox(struct tg3 *tp, u32 off)\n{\n\tunsigned long flags;\n\tu32 val;\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tpci_write_config_dword(tp->pdev, TG3PCI_REG_BASE_ADDR, off + 0x5600);\n\tpci_read_config_dword(tp->pdev, TG3PCI_REG_DATA, &val);\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n\treturn val;\n}\n\n/* usec_wait specifies the wait time in usec when writing to certain registers\n * where it is unsafe to read back the register without some delay.\n * GRC_LOCAL_CTRL is one example if the GPIOs are toggled to switch power.\n * TG3PCI_CLOCK_CTRL is another example if the clock frequencies are changed.\n */\nstatic void _tw32_flush(struct tg3 *tp, u32 off, u32 val, u32 usec_wait)\n{\n\tif (tg3_flag(tp, PCIX_TARGET_HWBUG) || tg3_flag(tp, ICH_WORKAROUND))\n\t\t/* Non-posted methods */\n\t\ttp->write32(tp, off, val);\n\telse {\n\t\t/* Posted method */\n\t\ttg3_write32(tp, off, val);\n\t\tif (usec_wait)\n\t\t\tudelay(usec_wait);\n\t\ttp->read32(tp, off);\n\t}\n\t/* Wait again after the read for the posted method to guarantee that\n\t * the wait time is met.\n\t */\n\tif (usec_wait)\n\t\tudelay(usec_wait);\n}\n\nstatic inline void tw32_mailbox_flush(struct tg3 *tp, u32 off, u32 val)\n{\n\ttp->write32_mbox(tp, off, val);\n\tif (tg3_flag(tp, FLUSH_POSTED_WRITES) ||\n\t    (!tg3_flag(tp, MBOX_WRITE_REORDER) &&\n\t     !tg3_flag(tp, ICH_WORKAROUND)))\n\t\ttp->read32_mbox(tp, off);\n}\n\nstatic void tg3_write32_tx_mbox(struct tg3 *tp, u32 off, u32 val)\n{\n\tvoid __iomem *mbox = tp->regs + off;\n\twritel(val, mbox);\n\tif (tg3_flag(tp, TXD_MBOX_HWBUG))\n\t\twritel(val, mbox);\n\tif (tg3_flag(tp, MBOX_WRITE_REORDER) ||\n\t    tg3_flag(tp, FLUSH_POSTED_WRITES))\n\t\treadl(mbox);\n}\n\nstatic u32 tg3_read32_mbox_5906(struct tg3 *tp, u32 off)\n{\n\treturn readl(tp->regs + off + GRCMBOX_BASE);\n}\n\nstatic void tg3_write32_mbox_5906(struct tg3 *tp, u32 off, u32 val)\n{\n\twritel(val, tp->regs + off + GRCMBOX_BASE);\n}\n\n#define tw32_mailbox(reg, val)\t\ttp->write32_mbox(tp, reg, val)\n#define tw32_mailbox_f(reg, val)\ttw32_mailbox_flush(tp, (reg), (val))\n#define tw32_rx_mbox(reg, val)\t\ttp->write32_rx_mbox(tp, reg, val)\n#define tw32_tx_mbox(reg, val)\t\ttp->write32_tx_mbox(tp, reg, val)\n#define tr32_mailbox(reg)\t\ttp->read32_mbox(tp, reg)\n\n#define tw32(reg, val)\t\t\ttp->write32(tp, reg, val)\n#define tw32_f(reg, val)\t\t_tw32_flush(tp, (reg), (val), 0)\n#define tw32_wait_f(reg, val, us)\t_tw32_flush(tp, (reg), (val), (us))\n#define tr32(reg)\t\t\ttp->read32(tp, reg)\n\nstatic void tg3_write_mem(struct tg3 *tp, u32 off, u32 val)\n{\n\tunsigned long flags;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906 &&\n\t    (off >= NIC_SRAM_STATS_BLK) && (off < NIC_SRAM_TX_BUFFER_DESC))\n\t\treturn;\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tif (tg3_flag(tp, SRAM_USE_CONFIG)) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, off);\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_DATA, val);\n\n\t\t/* Always leave this as zero. */\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\t} else {\n\t\ttw32_f(TG3PCI_MEM_WIN_BASE_ADDR, off);\n\t\ttw32_f(TG3PCI_MEM_WIN_DATA, val);\n\n\t\t/* Always leave this as zero. */\n\t\ttw32_f(TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\t}\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n}\n\nstatic void tg3_read_mem(struct tg3 *tp, u32 off, u32 *val)\n{\n\tunsigned long flags;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906 &&\n\t    (off >= NIC_SRAM_STATS_BLK) && (off < NIC_SRAM_TX_BUFFER_DESC)) {\n\t\t*val = 0;\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&tp->indirect_lock, flags);\n\tif (tg3_flag(tp, SRAM_USE_CONFIG)) {\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, off);\n\t\tpci_read_config_dword(tp->pdev, TG3PCI_MEM_WIN_DATA, val);\n\n\t\t/* Always leave this as zero. */\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\t} else {\n\t\ttw32_f(TG3PCI_MEM_WIN_BASE_ADDR, off);\n\t\t*val = tr32(TG3PCI_MEM_WIN_DATA);\n\n\t\t/* Always leave this as zero. */\n\t\ttw32_f(TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\t}\n\tspin_unlock_irqrestore(&tp->indirect_lock, flags);\n}\n\nstatic void tg3_ape_lock_init(struct tg3 *tp)\n{\n\tint i;\n\tu32 regbase, bit;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\tregbase = TG3_APE_LOCK_GRANT;\n\telse\n\t\tregbase = TG3_APE_PER_LOCK_GRANT;\n\n\t/* Make sure the driver hasn't any stale locks. */\n\tfor (i = TG3_APE_LOCK_PHY0; i <= TG3_APE_LOCK_GPIO; i++) {\n\t\tswitch (i) {\n\t\tcase TG3_APE_LOCK_PHY0:\n\t\tcase TG3_APE_LOCK_PHY1:\n\t\tcase TG3_APE_LOCK_PHY2:\n\t\tcase TG3_APE_LOCK_PHY3:\n\t\t\tbit = APE_LOCK_GRANT_DRIVER;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (!tp->pci_fn)\n\t\t\t\tbit = APE_LOCK_GRANT_DRIVER;\n\t\t\telse\n\t\t\t\tbit = 1 << tp->pci_fn;\n\t\t}\n\t\ttg3_ape_write32(tp, regbase + 4 * i, bit);\n\t}\n\n}\n\nstatic int tg3_ape_lock(struct tg3 *tp, int locknum)\n{\n\tint i, off;\n\tint ret = 0;\n\tu32 status, req, gnt, bit;\n\n\tif (!tg3_flag(tp, ENABLE_APE))\n\t\treturn 0;\n\n\tswitch (locknum) {\n\tcase TG3_APE_LOCK_GPIO:\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\t\treturn 0;\n\tcase TG3_APE_LOCK_GRC:\n\tcase TG3_APE_LOCK_MEM:\n\t\tif (!tp->pci_fn)\n\t\t\tbit = APE_LOCK_REQ_DRIVER;\n\t\telse\n\t\t\tbit = 1 << tp->pci_fn;\n\t\tbreak;\n\tcase TG3_APE_LOCK_PHY0:\n\tcase TG3_APE_LOCK_PHY1:\n\tcase TG3_APE_LOCK_PHY2:\n\tcase TG3_APE_LOCK_PHY3:\n\t\tbit = APE_LOCK_REQ_DRIVER;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761) {\n\t\treq = TG3_APE_LOCK_REQ;\n\t\tgnt = TG3_APE_LOCK_GRANT;\n\t} else {\n\t\treq = TG3_APE_PER_LOCK_REQ;\n\t\tgnt = TG3_APE_PER_LOCK_GRANT;\n\t}\n\n\toff = 4 * locknum;\n\n\ttg3_ape_write32(tp, req + off, bit);\n\n\t/* Wait for up to 1 millisecond to acquire lock. */\n\tfor (i = 0; i < 100; i++) {\n\t\tstatus = tg3_ape_read32(tp, gnt + off);\n\t\tif (status == bit)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\tif (status != bit) {\n\t\t/* Revoke the lock request. */\n\t\ttg3_ape_write32(tp, gnt + off, bit);\n\t\tret = -EBUSY;\n\t}\n\n\treturn ret;\n}\n\nstatic void tg3_ape_unlock(struct tg3 *tp, int locknum)\n{\n\tu32 gnt, bit;\n\n\tif (!tg3_flag(tp, ENABLE_APE))\n\t\treturn;\n\n\tswitch (locknum) {\n\tcase TG3_APE_LOCK_GPIO:\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\t\treturn;\n\tcase TG3_APE_LOCK_GRC:\n\tcase TG3_APE_LOCK_MEM:\n\t\tif (!tp->pci_fn)\n\t\t\tbit = APE_LOCK_GRANT_DRIVER;\n\t\telse\n\t\t\tbit = 1 << tp->pci_fn;\n\t\tbreak;\n\tcase TG3_APE_LOCK_PHY0:\n\tcase TG3_APE_LOCK_PHY1:\n\tcase TG3_APE_LOCK_PHY2:\n\tcase TG3_APE_LOCK_PHY3:\n\t\tbit = APE_LOCK_GRANT_DRIVER;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\tgnt = TG3_APE_LOCK_GRANT;\n\telse\n\t\tgnt = TG3_APE_PER_LOCK_GRANT;\n\n\ttg3_ape_write32(tp, gnt + 4 * locknum, bit);\n}\n\nstatic int tg3_ape_event_lock(struct tg3 *tp, u32 timeout_us)\n{\n\tu32 apedata;\n\n\twhile (timeout_us) {\n\t\tif (tg3_ape_lock(tp, TG3_APE_LOCK_MEM))\n\t\t\treturn -EBUSY;\n\n\t\tapedata = tg3_ape_read32(tp, TG3_APE_EVENT_STATUS);\n\t\tif (!(apedata & APE_EVENT_STATUS_EVENT_PENDING))\n\t\t\tbreak;\n\n\t\ttg3_ape_unlock(tp, TG3_APE_LOCK_MEM);\n\n\t\tudelay(10);\n\t\ttimeout_us -= (timeout_us > 10) ? 10 : timeout_us;\n\t}\n\n\treturn timeout_us ? 0 : -EBUSY;\n}\n\nstatic int tg3_ape_wait_for_event(struct tg3 *tp, u32 timeout_us)\n{\n\tu32 i, apedata;\n\n\tfor (i = 0; i < timeout_us / 10; i++) {\n\t\tapedata = tg3_ape_read32(tp, TG3_APE_EVENT_STATUS);\n\n\t\tif (!(apedata & APE_EVENT_STATUS_EVENT_PENDING))\n\t\t\tbreak;\n\n\t\tudelay(10);\n\t}\n\n\treturn i == timeout_us / 10;\n}\n\nstatic int tg3_ape_scratchpad_read(struct tg3 *tp, u32 *data, u32 base_off,\n\t\t\t\t   u32 len)\n{\n\tint err;\n\tu32 i, bufoff, msgoff, maxlen, apedata;\n\n\tif (!tg3_flag(tp, APE_HAS_NCSI))\n\t\treturn 0;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_SEG_SIG);\n\tif (apedata != APE_SEG_SIG_MAGIC)\n\t\treturn -ENODEV;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_FW_STATUS);\n\tif (!(apedata & APE_FW_STATUS_READY))\n\t\treturn -EAGAIN;\n\n\tbufoff = tg3_ape_read32(tp, TG3_APE_SEG_MSG_BUF_OFF) +\n\t\t TG3_APE_SHMEM_BASE;\n\tmsgoff = bufoff + 2 * sizeof(u32);\n\tmaxlen = tg3_ape_read32(tp, TG3_APE_SEG_MSG_BUF_LEN);\n\n\twhile (len) {\n\t\tu32 length;\n\n\t\t/* Cap xfer sizes to scratchpad limits. */\n\t\tlength = (len > maxlen) ? maxlen : len;\n\t\tlen -= length;\n\n\t\tapedata = tg3_ape_read32(tp, TG3_APE_FW_STATUS);\n\t\tif (!(apedata & APE_FW_STATUS_READY))\n\t\t\treturn -EAGAIN;\n\n\t\t/* Wait for up to 1 msec for APE to service previous event. */\n\t\terr = tg3_ape_event_lock(tp, 1000);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tapedata = APE_EVENT_STATUS_DRIVER_EVNT |\n\t\t\t  APE_EVENT_STATUS_SCRTCHPD_READ |\n\t\t\t  APE_EVENT_STATUS_EVENT_PENDING;\n\t\ttg3_ape_write32(tp, TG3_APE_EVENT_STATUS, apedata);\n\n\t\ttg3_ape_write32(tp, bufoff, base_off);\n\t\ttg3_ape_write32(tp, bufoff + sizeof(u32), length);\n\n\t\ttg3_ape_unlock(tp, TG3_APE_LOCK_MEM);\n\t\ttg3_ape_write32(tp, TG3_APE_EVENT, APE_EVENT_1);\n\n\t\tbase_off += length;\n\n\t\tif (tg3_ape_wait_for_event(tp, 30000))\n\t\t\treturn -EAGAIN;\n\n\t\tfor (i = 0; length; i += 4, length -= 4) {\n\t\t\tu32 val = tg3_ape_read32(tp, msgoff + i);\n\t\t\tmemcpy(data, &val, sizeof(u32));\n\t\t\tdata++;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int tg3_ape_send_event(struct tg3 *tp, u32 event)\n{\n\tint err;\n\tu32 apedata;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_SEG_SIG);\n\tif (apedata != APE_SEG_SIG_MAGIC)\n\t\treturn -EAGAIN;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_FW_STATUS);\n\tif (!(apedata & APE_FW_STATUS_READY))\n\t\treturn -EAGAIN;\n\n\t/* Wait for up to 1 millisecond for APE to service previous event. */\n\terr = tg3_ape_event_lock(tp, 1000);\n\tif (err)\n\t\treturn err;\n\n\ttg3_ape_write32(tp, TG3_APE_EVENT_STATUS,\n\t\t\tevent | APE_EVENT_STATUS_EVENT_PENDING);\n\n\ttg3_ape_unlock(tp, TG3_APE_LOCK_MEM);\n\ttg3_ape_write32(tp, TG3_APE_EVENT, APE_EVENT_1);\n\n\treturn 0;\n}\n\nstatic void tg3_ape_driver_state_change(struct tg3 *tp, int kind)\n{\n\tu32 event;\n\tu32 apedata;\n\n\tif (!tg3_flag(tp, ENABLE_APE))\n\t\treturn;\n\n\tswitch (kind) {\n\tcase RESET_KIND_INIT:\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_SEG_SIG,\n\t\t\t\tAPE_HOST_SEG_SIG_MAGIC);\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_SEG_LEN,\n\t\t\t\tAPE_HOST_SEG_LEN_MAGIC);\n\t\tapedata = tg3_ape_read32(tp, TG3_APE_HOST_INIT_COUNT);\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_INIT_COUNT, ++apedata);\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_DRIVER_ID,\n\t\t\tAPE_HOST_DRIVER_ID_MAGIC(TG3_MAJ_NUM, TG3_MIN_NUM));\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_BEHAVIOR,\n\t\t\t\tAPE_HOST_BEHAV_NO_PHYLOCK);\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_DRVR_STATE,\n\t\t\t\t    TG3_APE_HOST_DRVR_STATE_START);\n\n\t\tevent = APE_EVENT_STATUS_STATE_START;\n\t\tbreak;\n\tcase RESET_KIND_SHUTDOWN:\n\t\t/* With the interface we are currently using,\n\t\t * APE does not track driver state.  Wiping\n\t\t * out the HOST SEGMENT SIGNATURE forces\n\t\t * the APE to assume OS absent status.\n\t\t */\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_SEG_SIG, 0x0);\n\n\t\tif (device_may_wakeup(&tp->pdev->dev) &&\n\t\t    tg3_flag(tp, WOL_ENABLE)) {\n\t\t\ttg3_ape_write32(tp, TG3_APE_HOST_WOL_SPEED,\n\t\t\t\t\t    TG3_APE_HOST_WOL_SPEED_AUTO);\n\t\t\tapedata = TG3_APE_HOST_DRVR_STATE_WOL;\n\t\t} else\n\t\t\tapedata = TG3_APE_HOST_DRVR_STATE_UNLOAD;\n\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_DRVR_STATE, apedata);\n\n\t\tevent = APE_EVENT_STATUS_STATE_UNLOAD;\n\t\tbreak;\n\tcase RESET_KIND_SUSPEND:\n\t\tevent = APE_EVENT_STATUS_STATE_SUSPEND;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tevent |= APE_EVENT_STATUS_DRIVER_EVNT | APE_EVENT_STATUS_STATE_CHNGE;\n\n\ttg3_ape_send_event(tp, event);\n}\n\nstatic void tg3_disable_ints(struct tg3 *tp)\n{\n\tint i;\n\n\ttw32(TG3PCI_MISC_HOST_CTRL,\n\t     (tp->misc_host_ctrl | MISC_HOST_CTRL_MASK_PCI_INT));\n\tfor (i = 0; i < tp->irq_max; i++)\n\t\ttw32_mailbox_f(tp->napi[i].int_mbox, 0x00000001);\n}\n\nstatic void tg3_enable_ints(struct tg3 *tp)\n{\n\tint i;\n\n\ttp->irq_sync = 0;\n\twmb();\n\n\ttw32(TG3PCI_MISC_HOST_CTRL,\n\t     (tp->misc_host_ctrl & ~MISC_HOST_CTRL_MASK_PCI_INT));\n\n\ttp->coal_now = tp->coalesce_mode | HOSTCC_MODE_ENABLE;\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\ttw32_mailbox_f(tnapi->int_mbox, tnapi->last_tag << 24);\n\t\tif (tg3_flag(tp, 1SHOT_MSI))\n\t\t\ttw32_mailbox_f(tnapi->int_mbox, tnapi->last_tag << 24);\n\n\t\ttp->coal_now |= tnapi->coal_now;\n\t}\n\n\t/* Force an initial interrupt */\n\tif (!tg3_flag(tp, TAGGED_STATUS) &&\n\t    (tp->napi[0].hw_status->status & SD_STATUS_UPDATED))\n\t\ttw32(GRC_LOCAL_CTRL, tp->grc_local_ctrl | GRC_LCLCTRL_SETINT);\n\telse\n\t\ttw32(HOSTCC_MODE, tp->coal_now);\n\n\ttp->coal_now &= ~(tp->napi[0].coal_now | tp->napi[1].coal_now);\n}\n\nstatic inline unsigned int tg3_has_work(struct tg3_napi *tnapi)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\tunsigned int work_exists = 0;\n\n\t/* check for phy events */\n\tif (!(tg3_flag(tp, USE_LINKCHG_REG) || tg3_flag(tp, POLL_SERDES))) {\n\t\tif (sblk->status & SD_STATUS_LINK_CHG)\n\t\t\twork_exists = 1;\n\t}\n\n\t/* check for TX work to do */\n\tif (sblk->idx[0].tx_consumer != tnapi->tx_cons)\n\t\twork_exists = 1;\n\n\t/* check for RX work to do */\n\tif (tnapi->rx_rcb_prod_idx &&\n\t    *(tnapi->rx_rcb_prod_idx) != tnapi->rx_rcb_ptr)\n\t\twork_exists = 1;\n\n\treturn work_exists;\n}\n\n/* tg3_int_reenable\n *  similar to tg3_enable_ints, but it accurately determines whether there\n *  is new work pending and can return without flushing the PIO write\n *  which reenables interrupts\n */\nstatic void tg3_int_reenable(struct tg3_napi *tnapi)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\n\ttw32_mailbox(tnapi->int_mbox, tnapi->last_tag << 24);\n\tmmiowb();\n\n\t/* When doing tagged status, this work check is unnecessary.\n\t * The last_tag we write above tells the chip which piece of\n\t * work we've completed.\n\t */\n\tif (!tg3_flag(tp, TAGGED_STATUS) && tg3_has_work(tnapi))\n\t\ttw32(HOSTCC_MODE, tp->coalesce_mode |\n\t\t     HOSTCC_MODE_ENABLE | tnapi->coal_now);\n}\n\nstatic void tg3_switch_clocks(struct tg3 *tp)\n{\n\tu32 clock_ctrl;\n\tu32 orig_clock_ctrl;\n\n\tif (tg3_flag(tp, CPMU_PRESENT) || tg3_flag(tp, 5780_CLASS))\n\t\treturn;\n\n\tclock_ctrl = tr32(TG3PCI_CLOCK_CTRL);\n\n\torig_clock_ctrl = clock_ctrl;\n\tclock_ctrl &= (CLOCK_CTRL_FORCE_CLKRUN |\n\t\t       CLOCK_CTRL_CLKRUN_OENABLE |\n\t\t       0x1f);\n\ttp->pci_clock_ctrl = clock_ctrl;\n\n\tif (tg3_flag(tp, 5705_PLUS)) {\n\t\tif (orig_clock_ctrl & CLOCK_CTRL_625_CORE) {\n\t\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL,\n\t\t\t\t    clock_ctrl | CLOCK_CTRL_625_CORE, 40);\n\t\t}\n\t} else if ((orig_clock_ctrl & CLOCK_CTRL_44MHZ_CORE) != 0) {\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL,\n\t\t\t    clock_ctrl |\n\t\t\t    (CLOCK_CTRL_44MHZ_CORE | CLOCK_CTRL_ALTCLK),\n\t\t\t    40);\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL,\n\t\t\t    clock_ctrl | (CLOCK_CTRL_ALTCLK),\n\t\t\t    40);\n\t}\n\ttw32_wait_f(TG3PCI_CLOCK_CTRL, clock_ctrl, 40);\n}\n\n#define PHY_BUSY_LOOPS\t5000\n\nstatic int __tg3_readphy(struct tg3 *tp, unsigned int phy_addr, int reg,\n\t\t\t u32 *val)\n{\n\tu32 frame_val;\n\tunsigned int loops;\n\tint ret;\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE,\n\t\t     (tp->mi_mode & ~MAC_MI_MODE_AUTO_POLL));\n\t\tudelay(80);\n\t}\n\n\ttg3_ape_lock(tp, tp->phy_ape_lock);\n\n\t*val = 0x0;\n\n\tframe_val  = ((phy_addr << MI_COM_PHY_ADDR_SHIFT) &\n\t\t      MI_COM_PHY_ADDR_MASK);\n\tframe_val |= ((reg << MI_COM_REG_ADDR_SHIFT) &\n\t\t      MI_COM_REG_ADDR_MASK);\n\tframe_val |= (MI_COM_CMD_READ | MI_COM_START);\n\n\ttw32_f(MAC_MI_COM, frame_val);\n\n\tloops = PHY_BUSY_LOOPS;\n\twhile (loops != 0) {\n\t\tudelay(10);\n\t\tframe_val = tr32(MAC_MI_COM);\n\n\t\tif ((frame_val & MI_COM_BUSY) == 0) {\n\t\t\tudelay(5);\n\t\t\tframe_val = tr32(MAC_MI_COM);\n\t\t\tbreak;\n\t\t}\n\t\tloops -= 1;\n\t}\n\n\tret = -EBUSY;\n\tif (loops != 0) {\n\t\t*val = frame_val & MI_COM_DATA_MASK;\n\t\tret = 0;\n\t}\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE, tp->mi_mode);\n\t\tudelay(80);\n\t}\n\n\ttg3_ape_unlock(tp, tp->phy_ape_lock);\n\n\treturn ret;\n}\n\nstatic int tg3_readphy(struct tg3 *tp, int reg, u32 *val)\n{\n\treturn __tg3_readphy(tp, tp->phy_addr, reg, val);\n}\n\nstatic int __tg3_writephy(struct tg3 *tp, unsigned int phy_addr, int reg,\n\t\t\t  u32 val)\n{\n\tu32 frame_val;\n\tunsigned int loops;\n\tint ret;\n\n\tif ((tp->phy_flags & TG3_PHYFLG_IS_FET) &&\n\t    (reg == MII_CTRL1000 || reg == MII_TG3_AUX_CTRL))\n\t\treturn 0;\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE,\n\t\t     (tp->mi_mode & ~MAC_MI_MODE_AUTO_POLL));\n\t\tudelay(80);\n\t}\n\n\ttg3_ape_lock(tp, tp->phy_ape_lock);\n\n\tframe_val  = ((phy_addr << MI_COM_PHY_ADDR_SHIFT) &\n\t\t      MI_COM_PHY_ADDR_MASK);\n\tframe_val |= ((reg << MI_COM_REG_ADDR_SHIFT) &\n\t\t      MI_COM_REG_ADDR_MASK);\n\tframe_val |= (val & MI_COM_DATA_MASK);\n\tframe_val |= (MI_COM_CMD_WRITE | MI_COM_START);\n\n\ttw32_f(MAC_MI_COM, frame_val);\n\n\tloops = PHY_BUSY_LOOPS;\n\twhile (loops != 0) {\n\t\tudelay(10);\n\t\tframe_val = tr32(MAC_MI_COM);\n\t\tif ((frame_val & MI_COM_BUSY) == 0) {\n\t\t\tudelay(5);\n\t\t\tframe_val = tr32(MAC_MI_COM);\n\t\t\tbreak;\n\t\t}\n\t\tloops -= 1;\n\t}\n\n\tret = -EBUSY;\n\tif (loops != 0)\n\t\tret = 0;\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE, tp->mi_mode);\n\t\tudelay(80);\n\t}\n\n\ttg3_ape_unlock(tp, tp->phy_ape_lock);\n\n\treturn ret;\n}\n\nstatic int tg3_writephy(struct tg3 *tp, int reg, u32 val)\n{\n\treturn __tg3_writephy(tp, tp->phy_addr, reg, val);\n}\n\nstatic int tg3_phy_cl45_write(struct tg3 *tp, u32 devad, u32 addr, u32 val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_CTRL, devad);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_ADDRESS, addr);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_CTRL,\n\t\t\t   MII_TG3_MMD_CTRL_DATA_NOINC | devad);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_ADDRESS, val);\n\ndone:\n\treturn err;\n}\n\nstatic int tg3_phy_cl45_read(struct tg3 *tp, u32 devad, u32 addr, u32 *val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_CTRL, devad);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_ADDRESS, addr);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_writephy(tp, MII_TG3_MMD_CTRL,\n\t\t\t   MII_TG3_MMD_CTRL_DATA_NOINC | devad);\n\tif (err)\n\t\tgoto done;\n\n\terr = tg3_readphy(tp, MII_TG3_MMD_ADDRESS, val);\n\ndone:\n\treturn err;\n}\n\nstatic int tg3_phydsp_read(struct tg3 *tp, u32 reg, u32 *val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_DSP_ADDRESS, reg);\n\tif (!err)\n\t\terr = tg3_readphy(tp, MII_TG3_DSP_RW_PORT, val);\n\n\treturn err;\n}\n\nstatic int tg3_phydsp_write(struct tg3 *tp, u32 reg, u32 val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_DSP_ADDRESS, reg);\n\tif (!err)\n\t\terr = tg3_writephy(tp, MII_TG3_DSP_RW_PORT, val);\n\n\treturn err;\n}\n\nstatic int tg3_phy_auxctl_read(struct tg3 *tp, int reg, u32 *val)\n{\n\tint err;\n\n\terr = tg3_writephy(tp, MII_TG3_AUX_CTRL,\n\t\t\t   (reg << MII_TG3_AUXCTL_MISC_RDSEL_SHIFT) |\n\t\t\t   MII_TG3_AUXCTL_SHDWSEL_MISC);\n\tif (!err)\n\t\terr = tg3_readphy(tp, MII_TG3_AUX_CTRL, val);\n\n\treturn err;\n}\n\nstatic int tg3_phy_auxctl_write(struct tg3 *tp, int reg, u32 set)\n{\n\tif (reg == MII_TG3_AUXCTL_SHDWSEL_MISC)\n\t\tset |= MII_TG3_AUXCTL_MISC_WREN;\n\n\treturn tg3_writephy(tp, MII_TG3_AUX_CTRL, set | reg);\n}\n\nstatic int tg3_phy_toggle_auxctl_smdsp(struct tg3 *tp, bool enable)\n{\n\tu32 val;\n\tint err;\n\n\terr = tg3_phy_auxctl_read(tp, MII_TG3_AUXCTL_SHDWSEL_AUXCTL, &val);\n\n\tif (err)\n\t\treturn err;\n\tif (enable)\n\n\t\tval |= MII_TG3_AUXCTL_ACTL_SMDSP_ENA;\n\telse\n\t\tval &= ~MII_TG3_AUXCTL_ACTL_SMDSP_ENA;\n\n\terr = tg3_phy_auxctl_write((tp), MII_TG3_AUXCTL_SHDWSEL_AUXCTL,\n\t\t\t\t   val | MII_TG3_AUXCTL_ACTL_TX_6DB);\n\n\treturn err;\n}\n\nstatic int tg3_bmcr_reset(struct tg3 *tp)\n{\n\tu32 phy_control;\n\tint limit, err;\n\n\t/* OK, reset it, and poll the BMCR_RESET bit until it\n\t * clears or we time out.\n\t */\n\tphy_control = BMCR_RESET;\n\terr = tg3_writephy(tp, MII_BMCR, phy_control);\n\tif (err != 0)\n\t\treturn -EBUSY;\n\n\tlimit = 5000;\n\twhile (limit--) {\n\t\terr = tg3_readphy(tp, MII_BMCR, &phy_control);\n\t\tif (err != 0)\n\t\t\treturn -EBUSY;\n\n\t\tif ((phy_control & BMCR_RESET) == 0) {\n\t\t\tudelay(40);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(10);\n\t}\n\tif (limit < 0)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic int tg3_mdio_read(struct mii_bus *bp, int mii_id, int reg)\n{\n\tstruct tg3 *tp = bp->priv;\n\tu32 val;\n\n\tspin_lock_bh(&tp->lock);\n\n\tif (tg3_readphy(tp, reg, &val))\n\t\tval = -EIO;\n\n\tspin_unlock_bh(&tp->lock);\n\n\treturn val;\n}\n\nstatic int tg3_mdio_write(struct mii_bus *bp, int mii_id, int reg, u16 val)\n{\n\tstruct tg3 *tp = bp->priv;\n\tu32 ret = 0;\n\n\tspin_lock_bh(&tp->lock);\n\n\tif (tg3_writephy(tp, reg, val))\n\t\tret = -EIO;\n\n\tspin_unlock_bh(&tp->lock);\n\n\treturn ret;\n}\n\nstatic int tg3_mdio_reset(struct mii_bus *bp)\n{\n\treturn 0;\n}\n\nstatic void tg3_mdio_config_5785(struct tg3 *tp)\n{\n\tu32 val;\n\tstruct phy_device *phydev;\n\n\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\tswitch (phydev->drv->phy_id & phydev->drv->phy_id_mask) {\n\tcase PHY_ID_BCM50610:\n\tcase PHY_ID_BCM50610M:\n\t\tval = MAC_PHYCFG2_50610_LED_MODES;\n\t\tbreak;\n\tcase PHY_ID_BCMAC131:\n\t\tval = MAC_PHYCFG2_AC131_LED_MODES;\n\t\tbreak;\n\tcase PHY_ID_RTL8211C:\n\t\tval = MAC_PHYCFG2_RTL8211C_LED_MODES;\n\t\tbreak;\n\tcase PHY_ID_RTL8201E:\n\t\tval = MAC_PHYCFG2_RTL8201E_LED_MODES;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (phydev->interface != PHY_INTERFACE_MODE_RGMII) {\n\t\ttw32(MAC_PHYCFG2, val);\n\n\t\tval = tr32(MAC_PHYCFG1);\n\t\tval &= ~(MAC_PHYCFG1_RGMII_INT |\n\t\t\t MAC_PHYCFG1_RXCLK_TO_MASK | MAC_PHYCFG1_TXCLK_TO_MASK);\n\t\tval |= MAC_PHYCFG1_RXCLK_TIMEOUT | MAC_PHYCFG1_TXCLK_TIMEOUT;\n\t\ttw32(MAC_PHYCFG1, val);\n\n\t\treturn;\n\t}\n\n\tif (!tg3_flag(tp, RGMII_INBAND_DISABLE))\n\t\tval |= MAC_PHYCFG2_EMODE_MASK_MASK |\n\t\t       MAC_PHYCFG2_FMODE_MASK_MASK |\n\t\t       MAC_PHYCFG2_GMODE_MASK_MASK |\n\t\t       MAC_PHYCFG2_ACT_MASK_MASK   |\n\t\t       MAC_PHYCFG2_QUAL_MASK_MASK |\n\t\t       MAC_PHYCFG2_INBAND_ENABLE;\n\n\ttw32(MAC_PHYCFG2, val);\n\n\tval = tr32(MAC_PHYCFG1);\n\tval &= ~(MAC_PHYCFG1_RXCLK_TO_MASK | MAC_PHYCFG1_TXCLK_TO_MASK |\n\t\t MAC_PHYCFG1_RGMII_EXT_RX_DEC | MAC_PHYCFG1_RGMII_SND_STAT_EN);\n\tif (!tg3_flag(tp, RGMII_INBAND_DISABLE)) {\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_RX_EN))\n\t\t\tval |= MAC_PHYCFG1_RGMII_EXT_RX_DEC;\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_TX_EN))\n\t\t\tval |= MAC_PHYCFG1_RGMII_SND_STAT_EN;\n\t}\n\tval |= MAC_PHYCFG1_RXCLK_TIMEOUT | MAC_PHYCFG1_TXCLK_TIMEOUT |\n\t       MAC_PHYCFG1_RGMII_INT | MAC_PHYCFG1_TXC_DRV;\n\ttw32(MAC_PHYCFG1, val);\n\n\tval = tr32(MAC_EXT_RGMII_MODE);\n\tval &= ~(MAC_RGMII_MODE_RX_INT_B |\n\t\t MAC_RGMII_MODE_RX_QUALITY |\n\t\t MAC_RGMII_MODE_RX_ACTIVITY |\n\t\t MAC_RGMII_MODE_RX_ENG_DET |\n\t\t MAC_RGMII_MODE_TX_ENABLE |\n\t\t MAC_RGMII_MODE_TX_LOWPWR |\n\t\t MAC_RGMII_MODE_TX_RESET);\n\tif (!tg3_flag(tp, RGMII_INBAND_DISABLE)) {\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_RX_EN))\n\t\t\tval |= MAC_RGMII_MODE_RX_INT_B |\n\t\t\t       MAC_RGMII_MODE_RX_QUALITY |\n\t\t\t       MAC_RGMII_MODE_RX_ACTIVITY |\n\t\t\t       MAC_RGMII_MODE_RX_ENG_DET;\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_TX_EN))\n\t\t\tval |= MAC_RGMII_MODE_TX_ENABLE |\n\t\t\t       MAC_RGMII_MODE_TX_LOWPWR |\n\t\t\t       MAC_RGMII_MODE_TX_RESET;\n\t}\n\ttw32(MAC_EXT_RGMII_MODE, val);\n}\n\nstatic void tg3_mdio_start(struct tg3 *tp)\n{\n\ttp->mi_mode &= ~MAC_MI_MODE_AUTO_POLL;\n\ttw32_f(MAC_MI_MODE, tp->mi_mode);\n\tudelay(80);\n\n\tif (tg3_flag(tp, MDIOBUS_INITED) &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\ttg3_mdio_config_5785(tp);\n}\n\nstatic int tg3_mdio_init(struct tg3 *tp)\n{\n\tint i;\n\tu32 reg;\n\tstruct phy_device *phydev;\n\n\tif (tg3_flag(tp, 5717_PLUS)) {\n\t\tu32 is_serdes;\n\n\t\ttp->phy_addr = tp->pci_fn + 1;\n\n\t\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5717_A0)\n\t\t\tis_serdes = tr32(SG_DIG_STATUS) & SG_DIG_IS_SERDES;\n\t\telse\n\t\t\tis_serdes = tr32(TG3_CPMU_PHY_STRAP) &\n\t\t\t\t    TG3_CPMU_PHY_STRAP_IS_SERDES;\n\t\tif (is_serdes)\n\t\t\ttp->phy_addr += 7;\n\t} else\n\t\ttp->phy_addr = TG3_PHY_MII_ADDR;\n\n\ttg3_mdio_start(tp);\n\n\tif (!tg3_flag(tp, USE_PHYLIB) || tg3_flag(tp, MDIOBUS_INITED))\n\t\treturn 0;\n\n\ttp->mdio_bus = mdiobus_alloc();\n\tif (tp->mdio_bus == NULL)\n\t\treturn -ENOMEM;\n\n\ttp->mdio_bus->name     = \"tg3 mdio bus\";\n\tsnprintf(tp->mdio_bus->id, MII_BUS_ID_SIZE, \"%x\",\n\t\t (tp->pdev->bus->number << 8) | tp->pdev->devfn);\n\ttp->mdio_bus->priv     = tp;\n\ttp->mdio_bus->parent   = &tp->pdev->dev;\n\ttp->mdio_bus->read     = &tg3_mdio_read;\n\ttp->mdio_bus->write    = &tg3_mdio_write;\n\ttp->mdio_bus->reset    = &tg3_mdio_reset;\n\ttp->mdio_bus->phy_mask = ~(1 << TG3_PHY_MII_ADDR);\n\ttp->mdio_bus->irq      = &tp->mdio_irq[0];\n\n\tfor (i = 0; i < PHY_MAX_ADDR; i++)\n\t\ttp->mdio_bus->irq[i] = PHY_POLL;\n\n\t/* The bus registration will look for all the PHYs on the mdio bus.\n\t * Unfortunately, it does not ensure the PHY is powered up before\n\t * accessing the PHY ID registers.  A chip reset is the\n\t * quickest way to bring the device back to an operational state..\n\t */\n\tif (tg3_readphy(tp, MII_BMCR, &reg) || (reg & BMCR_PDOWN))\n\t\ttg3_bmcr_reset(tp);\n\n\ti = mdiobus_register(tp->mdio_bus);\n\tif (i) {\n\t\tdev_warn(&tp->pdev->dev, \"mdiobus_reg failed (0x%x)\\n\", i);\n\t\tmdiobus_free(tp->mdio_bus);\n\t\treturn i;\n\t}\n\n\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\tif (!phydev || !phydev->drv) {\n\t\tdev_warn(&tp->pdev->dev, \"No PHY devices\\n\");\n\t\tmdiobus_unregister(tp->mdio_bus);\n\t\tmdiobus_free(tp->mdio_bus);\n\t\treturn -ENODEV;\n\t}\n\n\tswitch (phydev->drv->phy_id & phydev->drv->phy_id_mask) {\n\tcase PHY_ID_BCM57780:\n\t\tphydev->interface = PHY_INTERFACE_MODE_GMII;\n\t\tphydev->dev_flags |= PHY_BRCM_AUTO_PWRDWN_ENABLE;\n\t\tbreak;\n\tcase PHY_ID_BCM50610:\n\tcase PHY_ID_BCM50610M:\n\t\tphydev->dev_flags |= PHY_BRCM_CLEAR_RGMII_MODE |\n\t\t\t\t     PHY_BRCM_RX_REFCLK_UNUSED |\n\t\t\t\t     PHY_BRCM_DIS_TXCRXC_NOENRGY |\n\t\t\t\t     PHY_BRCM_AUTO_PWRDWN_ENABLE;\n\t\tif (tg3_flag(tp, RGMII_INBAND_DISABLE))\n\t\t\tphydev->dev_flags |= PHY_BRCM_STD_IBND_DISABLE;\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_RX_EN))\n\t\t\tphydev->dev_flags |= PHY_BRCM_EXT_IBND_RX_ENABLE;\n\t\tif (tg3_flag(tp, RGMII_EXT_IBND_TX_EN))\n\t\t\tphydev->dev_flags |= PHY_BRCM_EXT_IBND_TX_ENABLE;\n\t\t/* fallthru */\n\tcase PHY_ID_RTL8211C:\n\t\tphydev->interface = PHY_INTERFACE_MODE_RGMII;\n\t\tbreak;\n\tcase PHY_ID_RTL8201E:\n\tcase PHY_ID_BCMAC131:\n\t\tphydev->interface = PHY_INTERFACE_MODE_MII;\n\t\tphydev->dev_flags |= PHY_BRCM_AUTO_PWRDWN_ENABLE;\n\t\ttp->phy_flags |= TG3_PHYFLG_IS_FET;\n\t\tbreak;\n\t}\n\n\ttg3_flag_set(tp, MDIOBUS_INITED);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\ttg3_mdio_config_5785(tp);\n\n\treturn 0;\n}\n\nstatic void tg3_mdio_fini(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, MDIOBUS_INITED)) {\n\t\ttg3_flag_clear(tp, MDIOBUS_INITED);\n\t\tmdiobus_unregister(tp->mdio_bus);\n\t\tmdiobus_free(tp->mdio_bus);\n\t}\n}\n\n/* tp->lock is held. */\nstatic inline void tg3_generate_fw_event(struct tg3 *tp)\n{\n\tu32 val;\n\n\tval = tr32(GRC_RX_CPU_EVENT);\n\tval |= GRC_RX_CPU_DRIVER_EVENT;\n\ttw32_f(GRC_RX_CPU_EVENT, val);\n\n\ttp->last_event_jiffies = jiffies;\n}\n\n#define TG3_FW_EVENT_TIMEOUT_USEC 2500\n\n/* tp->lock is held. */\nstatic void tg3_wait_for_event_ack(struct tg3 *tp)\n{\n\tint i;\n\tunsigned int delay_cnt;\n\tlong time_remain;\n\n\t/* If enough time has passed, no wait is necessary. */\n\ttime_remain = (long)(tp->last_event_jiffies + 1 +\n\t\t      usecs_to_jiffies(TG3_FW_EVENT_TIMEOUT_USEC)) -\n\t\t      (long)jiffies;\n\tif (time_remain < 0)\n\t\treturn;\n\n\t/* Check if we can shorten the wait time. */\n\tdelay_cnt = jiffies_to_usecs(time_remain);\n\tif (delay_cnt > TG3_FW_EVENT_TIMEOUT_USEC)\n\t\tdelay_cnt = TG3_FW_EVENT_TIMEOUT_USEC;\n\tdelay_cnt = (delay_cnt >> 3) + 1;\n\n\tfor (i = 0; i < delay_cnt; i++) {\n\t\tif (!(tr32(GRC_RX_CPU_EVENT) & GRC_RX_CPU_DRIVER_EVENT))\n\t\t\tbreak;\n\t\tudelay(8);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_phy_gather_ump_data(struct tg3 *tp, u32 *data)\n{\n\tu32 reg, val;\n\n\tval = 0;\n\tif (!tg3_readphy(tp, MII_BMCR, &reg))\n\t\tval = reg << 16;\n\tif (!tg3_readphy(tp, MII_BMSR, &reg))\n\t\tval |= (reg & 0xffff);\n\t*data++ = val;\n\n\tval = 0;\n\tif (!tg3_readphy(tp, MII_ADVERTISE, &reg))\n\t\tval = reg << 16;\n\tif (!tg3_readphy(tp, MII_LPA, &reg))\n\t\tval |= (reg & 0xffff);\n\t*data++ = val;\n\n\tval = 0;\n\tif (!(tp->phy_flags & TG3_PHYFLG_MII_SERDES)) {\n\t\tif (!tg3_readphy(tp, MII_CTRL1000, &reg))\n\t\t\tval = reg << 16;\n\t\tif (!tg3_readphy(tp, MII_STAT1000, &reg))\n\t\t\tval |= (reg & 0xffff);\n\t}\n\t*data++ = val;\n\n\tif (!tg3_readphy(tp, MII_PHYADDR, &reg))\n\t\tval = reg << 16;\n\telse\n\t\tval = 0;\n\t*data++ = val;\n}\n\n/* tp->lock is held. */\nstatic void tg3_ump_link_report(struct tg3 *tp)\n{\n\tu32 data[4];\n\n\tif (!tg3_flag(tp, 5780_CLASS) || !tg3_flag(tp, ENABLE_ASF))\n\t\treturn;\n\n\ttg3_phy_gather_ump_data(tp, data);\n\n\ttg3_wait_for_event_ack(tp);\n\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_MBOX, FWCMD_NICDRV_LINK_UPDATE);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_LEN_MBOX, 14);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX + 0x0, data[0]);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX + 0x4, data[1]);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX + 0x8, data[2]);\n\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX + 0xc, data[3]);\n\n\ttg3_generate_fw_event(tp);\n}\n\n/* tp->lock is held. */\nstatic void tg3_stop_fw(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, ENABLE_ASF) && !tg3_flag(tp, ENABLE_APE)) {\n\t\t/* Wait for RX cpu to ACK the previous event. */\n\t\ttg3_wait_for_event_ack(tp);\n\n\t\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_MBOX, FWCMD_NICDRV_PAUSE_FW);\n\n\t\ttg3_generate_fw_event(tp);\n\n\t\t/* Wait for RX cpu to ACK this event. */\n\t\ttg3_wait_for_event_ack(tp);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_write_sig_pre_reset(struct tg3 *tp, int kind)\n{\n\ttg3_write_mem(tp, NIC_SRAM_FIRMWARE_MBOX,\n\t\t      NIC_SRAM_FIRMWARE_MBOX_MAGIC1);\n\n\tif (tg3_flag(tp, ASF_NEW_HANDSHAKE)) {\n\t\tswitch (kind) {\n\t\tcase RESET_KIND_INIT:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_START);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SHUTDOWN:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_UNLOAD);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SUSPEND:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_SUSPEND);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (kind == RESET_KIND_INIT ||\n\t    kind == RESET_KIND_SUSPEND)\n\t\ttg3_ape_driver_state_change(tp, kind);\n}\n\n/* tp->lock is held. */\nstatic void tg3_write_sig_post_reset(struct tg3 *tp, int kind)\n{\n\tif (tg3_flag(tp, ASF_NEW_HANDSHAKE)) {\n\t\tswitch (kind) {\n\t\tcase RESET_KIND_INIT:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_START_DONE);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SHUTDOWN:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_UNLOAD_DONE);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (kind == RESET_KIND_SHUTDOWN)\n\t\ttg3_ape_driver_state_change(tp, kind);\n}\n\n/* tp->lock is held. */\nstatic void tg3_write_sig_legacy(struct tg3 *tp, int kind)\n{\n\tif (tg3_flag(tp, ENABLE_ASF)) {\n\t\tswitch (kind) {\n\t\tcase RESET_KIND_INIT:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_START);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SHUTDOWN:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_UNLOAD);\n\t\t\tbreak;\n\n\t\tcase RESET_KIND_SUSPEND:\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_DRV_STATE_MBOX,\n\t\t\t\t      DRV_STATE_SUSPEND);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int tg3_poll_fw(struct tg3 *tp)\n{\n\tint i;\n\tu32 val;\n\n\tif (tg3_flag(tp, IS_SSB_CORE)) {\n\t\t/* We don't use firmware. */\n\t\treturn 0;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t/* Wait up to 20ms for init done. */\n\t\tfor (i = 0; i < 200; i++) {\n\t\t\tif (tr32(VCPU_STATUS) & VCPU_STATUS_INIT_DONE)\n\t\t\t\treturn 0;\n\t\t\tudelay(100);\n\t\t}\n\t\treturn -ENODEV;\n\t}\n\n\t/* Wait for firmware initialization to complete. */\n\tfor (i = 0; i < 100000; i++) {\n\t\ttg3_read_mem(tp, NIC_SRAM_FIRMWARE_MBOX, &val);\n\t\tif (val == ~NIC_SRAM_FIRMWARE_MBOX_MAGIC1)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\t/* Chip might not be fitted with firmware.  Some Sun onboard\n\t * parts are configured like that.  So don't signal the timeout\n\t * of the above loop as an error, but do report the lack of\n\t * running firmware once.\n\t */\n\tif (i >= 100000 && !tg3_flag(tp, NO_FWARE_REPORTED)) {\n\t\ttg3_flag_set(tp, NO_FWARE_REPORTED);\n\n\t\tnetdev_info(tp->dev, \"No firmware running\\n\");\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_57765_A0) {\n\t\t/* The 57765 A0 needs a little more\n\t\t * time to do some important work.\n\t\t */\n\t\tmdelay(10);\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_link_report(struct tg3 *tp)\n{\n\tif (!netif_carrier_ok(tp->dev)) {\n\t\tnetif_info(tp, link, tp->dev, \"Link is down\\n\");\n\t\ttg3_ump_link_report(tp);\n\t} else if (netif_msg_link(tp)) {\n\t\tnetdev_info(tp->dev, \"Link is up at %d Mbps, %s duplex\\n\",\n\t\t\t    (tp->link_config.active_speed == SPEED_1000 ?\n\t\t\t     1000 :\n\t\t\t     (tp->link_config.active_speed == SPEED_100 ?\n\t\t\t      100 : 10)),\n\t\t\t    (tp->link_config.active_duplex == DUPLEX_FULL ?\n\t\t\t     \"full\" : \"half\"));\n\n\t\tnetdev_info(tp->dev, \"Flow control is %s for TX and %s for RX\\n\",\n\t\t\t    (tp->link_config.active_flowctrl & FLOW_CTRL_TX) ?\n\t\t\t    \"on\" : \"off\",\n\t\t\t    (tp->link_config.active_flowctrl & FLOW_CTRL_RX) ?\n\t\t\t    \"on\" : \"off\");\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_EEE_CAP)\n\t\t\tnetdev_info(tp->dev, \"EEE is %s\\n\",\n\t\t\t\t    tp->setlpicnt ? \"enabled\" : \"disabled\");\n\n\t\ttg3_ump_link_report(tp);\n\t}\n\n\ttp->link_up = netif_carrier_ok(tp->dev);\n}\n\nstatic u16 tg3_advert_flowctrl_1000X(u8 flow_ctrl)\n{\n\tu16 miireg;\n\n\tif ((flow_ctrl & FLOW_CTRL_TX) && (flow_ctrl & FLOW_CTRL_RX))\n\t\tmiireg = ADVERTISE_1000XPAUSE;\n\telse if (flow_ctrl & FLOW_CTRL_TX)\n\t\tmiireg = ADVERTISE_1000XPSE_ASYM;\n\telse if (flow_ctrl & FLOW_CTRL_RX)\n\t\tmiireg = ADVERTISE_1000XPAUSE | ADVERTISE_1000XPSE_ASYM;\n\telse\n\t\tmiireg = 0;\n\n\treturn miireg;\n}\n\nstatic u8 tg3_resolve_flowctrl_1000X(u16 lcladv, u16 rmtadv)\n{\n\tu8 cap = 0;\n\n\tif (lcladv & rmtadv & ADVERTISE_1000XPAUSE) {\n\t\tcap = FLOW_CTRL_TX | FLOW_CTRL_RX;\n\t} else if (lcladv & rmtadv & ADVERTISE_1000XPSE_ASYM) {\n\t\tif (lcladv & ADVERTISE_1000XPAUSE)\n\t\t\tcap = FLOW_CTRL_RX;\n\t\tif (rmtadv & ADVERTISE_1000XPAUSE)\n\t\t\tcap = FLOW_CTRL_TX;\n\t}\n\n\treturn cap;\n}\n\nstatic void tg3_setup_flow_control(struct tg3 *tp, u32 lcladv, u32 rmtadv)\n{\n\tu8 autoneg;\n\tu8 flowctrl = 0;\n\tu32 old_rx_mode = tp->rx_mode;\n\tu32 old_tx_mode = tp->tx_mode;\n\n\tif (tg3_flag(tp, USE_PHYLIB))\n\t\tautoneg = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]->autoneg;\n\telse\n\t\tautoneg = tp->link_config.autoneg;\n\n\tif (autoneg == AUTONEG_ENABLE && tg3_flag(tp, PAUSE_AUTONEG)) {\n\t\tif (tp->phy_flags & TG3_PHYFLG_ANY_SERDES)\n\t\t\tflowctrl = tg3_resolve_flowctrl_1000X(lcladv, rmtadv);\n\t\telse\n\t\t\tflowctrl = mii_resolve_flowctrl_fdx(lcladv, rmtadv);\n\t} else\n\t\tflowctrl = tp->link_config.flowctrl;\n\n\ttp->link_config.active_flowctrl = flowctrl;\n\n\tif (flowctrl & FLOW_CTRL_RX)\n\t\ttp->rx_mode |= RX_MODE_FLOW_CTRL_ENABLE;\n\telse\n\t\ttp->rx_mode &= ~RX_MODE_FLOW_CTRL_ENABLE;\n\n\tif (old_rx_mode != tp->rx_mode)\n\t\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\n\tif (flowctrl & FLOW_CTRL_TX)\n\t\ttp->tx_mode |= TX_MODE_FLOW_CTRL_ENABLE;\n\telse\n\t\ttp->tx_mode &= ~TX_MODE_FLOW_CTRL_ENABLE;\n\n\tif (old_tx_mode != tp->tx_mode)\n\t\ttw32_f(MAC_TX_MODE, tp->tx_mode);\n}\n\nstatic void tg3_adjust_link(struct net_device *dev)\n{\n\tu8 oldflowctrl, linkmesg = 0;\n\tu32 mac_mode, lcl_adv, rmt_adv;\n\tstruct tg3 *tp = netdev_priv(dev);\n\tstruct phy_device *phydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\tspin_lock_bh(&tp->lock);\n\n\tmac_mode = tp->mac_mode & ~(MAC_MODE_PORT_MODE_MASK |\n\t\t\t\t    MAC_MODE_HALF_DUPLEX);\n\n\toldflowctrl = tp->link_config.active_flowctrl;\n\n\tif (phydev->link) {\n\t\tlcl_adv = 0;\n\t\trmt_adv = 0;\n\n\t\tif (phydev->speed == SPEED_100 || phydev->speed == SPEED_10)\n\t\t\tmac_mode |= MAC_MODE_PORT_MODE_MII;\n\t\telse if (phydev->speed == SPEED_1000 ||\n\t\t\t tg3_asic_rev(tp) != ASIC_REV_5785)\n\t\t\tmac_mode |= MAC_MODE_PORT_MODE_GMII;\n\t\telse\n\t\t\tmac_mode |= MAC_MODE_PORT_MODE_MII;\n\n\t\tif (phydev->duplex == DUPLEX_HALF)\n\t\t\tmac_mode |= MAC_MODE_HALF_DUPLEX;\n\t\telse {\n\t\t\tlcl_adv = mii_advertise_flowctrl(\n\t\t\t\t  tp->link_config.flowctrl);\n\n\t\t\tif (phydev->pause)\n\t\t\t\trmt_adv = LPA_PAUSE_CAP;\n\t\t\tif (phydev->asym_pause)\n\t\t\t\trmt_adv |= LPA_PAUSE_ASYM;\n\t\t}\n\n\t\ttg3_setup_flow_control(tp, lcl_adv, rmt_adv);\n\t} else\n\t\tmac_mode |= MAC_MODE_PORT_MODE_GMII;\n\n\tif (mac_mode != tp->mac_mode) {\n\t\ttp->mac_mode = mac_mode;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5785) {\n\t\tif (phydev->speed == SPEED_10)\n\t\t\ttw32(MAC_MI_STAT,\n\t\t\t     MAC_MI_STAT_10MBPS_MODE |\n\t\t\t     MAC_MI_STAT_LNKSTAT_ATTN_ENAB);\n\t\telse\n\t\t\ttw32(MAC_MI_STAT, MAC_MI_STAT_LNKSTAT_ATTN_ENAB);\n\t}\n\n\tif (phydev->speed == SPEED_1000 && phydev->duplex == DUPLEX_HALF)\n\t\ttw32(MAC_TX_LENGTHS,\n\t\t     ((2 << TX_LENGTHS_IPG_CRS_SHIFT) |\n\t\t      (6 << TX_LENGTHS_IPG_SHIFT) |\n\t\t      (0xff << TX_LENGTHS_SLOT_TIME_SHIFT)));\n\telse\n\t\ttw32(MAC_TX_LENGTHS,\n\t\t     ((2 << TX_LENGTHS_IPG_CRS_SHIFT) |\n\t\t      (6 << TX_LENGTHS_IPG_SHIFT) |\n\t\t      (32 << TX_LENGTHS_SLOT_TIME_SHIFT)));\n\n\tif (phydev->link != tp->old_link ||\n\t    phydev->speed != tp->link_config.active_speed ||\n\t    phydev->duplex != tp->link_config.active_duplex ||\n\t    oldflowctrl != tp->link_config.active_flowctrl)\n\t\tlinkmesg = 1;\n\n\ttp->old_link = phydev->link;\n\ttp->link_config.active_speed = phydev->speed;\n\ttp->link_config.active_duplex = phydev->duplex;\n\n\tspin_unlock_bh(&tp->lock);\n\n\tif (linkmesg)\n\t\ttg3_link_report(tp);\n}\n\nstatic int tg3_phy_init(struct tg3 *tp)\n{\n\tstruct phy_device *phydev;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_CONNECTED)\n\t\treturn 0;\n\n\t/* Bring the PHY back to a known state. */\n\ttg3_bmcr_reset(tp);\n\n\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\t/* Attach the MAC to the PHY. */\n\tphydev = phy_connect(tp->dev, dev_name(&phydev->dev),\n\t\t\t     tg3_adjust_link, phydev->interface);\n\tif (IS_ERR(phydev)) {\n\t\tdev_err(&tp->pdev->dev, \"Could not attach to PHY\\n\");\n\t\treturn PTR_ERR(phydev);\n\t}\n\n\t/* Mask with MAC supported features. */\n\tswitch (phydev->interface) {\n\tcase PHY_INTERFACE_MODE_GMII:\n\tcase PHY_INTERFACE_MODE_RGMII:\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY)) {\n\t\t\tphydev->supported &= (PHY_GBIT_FEATURES |\n\t\t\t\t\t      SUPPORTED_Pause |\n\t\t\t\t\t      SUPPORTED_Asym_Pause);\n\t\t\tbreak;\n\t\t}\n\t\t/* fallthru */\n\tcase PHY_INTERFACE_MODE_MII:\n\t\tphydev->supported &= (PHY_BASIC_FEATURES |\n\t\t\t\t      SUPPORTED_Pause |\n\t\t\t\t      SUPPORTED_Asym_Pause);\n\t\tbreak;\n\tdefault:\n\t\tphy_disconnect(tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]);\n\t\treturn -EINVAL;\n\t}\n\n\ttp->phy_flags |= TG3_PHYFLG_IS_CONNECTED;\n\n\tphydev->advertising = phydev->supported;\n\n\treturn 0;\n}\n\nstatic void tg3_phy_start(struct tg3 *tp)\n{\n\tstruct phy_device *phydev;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\treturn;\n\n\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER) {\n\t\ttp->phy_flags &= ~TG3_PHYFLG_IS_LOW_POWER;\n\t\tphydev->speed = tp->link_config.speed;\n\t\tphydev->duplex = tp->link_config.duplex;\n\t\tphydev->autoneg = tp->link_config.autoneg;\n\t\tphydev->advertising = tp->link_config.advertising;\n\t}\n\n\tphy_start(phydev);\n\n\tphy_start_aneg(phydev);\n}\n\nstatic void tg3_phy_stop(struct tg3 *tp)\n{\n\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\treturn;\n\n\tphy_stop(tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]);\n}\n\nstatic void tg3_phy_fini(struct tg3 *tp)\n{\n\tif (tp->phy_flags & TG3_PHYFLG_IS_CONNECTED) {\n\t\tphy_disconnect(tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]);\n\t\ttp->phy_flags &= ~TG3_PHYFLG_IS_CONNECTED;\n\t}\n}\n\nstatic int tg3_phy_set_extloopbk(struct tg3 *tp)\n{\n\tint err;\n\tu32 val;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_FET)\n\t\treturn 0;\n\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5401) {\n\t\t/* Cannot do read-modify-write on 5401 */\n\t\terr = tg3_phy_auxctl_write(tp,\n\t\t\t\t\t   MII_TG3_AUXCTL_SHDWSEL_AUXCTL,\n\t\t\t\t\t   MII_TG3_AUXCTL_ACTL_EXTLOOPBK |\n\t\t\t\t\t   0x4c20);\n\t\tgoto done;\n\t}\n\n\terr = tg3_phy_auxctl_read(tp,\n\t\t\t\t  MII_TG3_AUXCTL_SHDWSEL_AUXCTL, &val);\n\tif (err)\n\t\treturn err;\n\n\tval |= MII_TG3_AUXCTL_ACTL_EXTLOOPBK;\n\terr = tg3_phy_auxctl_write(tp,\n\t\t\t\t   MII_TG3_AUXCTL_SHDWSEL_AUXCTL, val);\n\ndone:\n\treturn err;\n}\n\nstatic void tg3_phy_fet_toggle_apd(struct tg3 *tp, bool enable)\n{\n\tu32 phytest;\n\n\tif (!tg3_readphy(tp, MII_TG3_FET_TEST, &phytest)) {\n\t\tu32 phy;\n\n\t\ttg3_writephy(tp, MII_TG3_FET_TEST,\n\t\t\t     phytest | MII_TG3_FET_SHADOW_EN);\n\t\tif (!tg3_readphy(tp, MII_TG3_FET_SHDW_AUXSTAT2, &phy)) {\n\t\t\tif (enable)\n\t\t\t\tphy |= MII_TG3_FET_SHDW_AUXSTAT2_APD;\n\t\t\telse\n\t\t\t\tphy &= ~MII_TG3_FET_SHDW_AUXSTAT2_APD;\n\t\t\ttg3_writephy(tp, MII_TG3_FET_SHDW_AUXSTAT2, phy);\n\t\t}\n\t\ttg3_writephy(tp, MII_TG3_FET_TEST, phytest);\n\t}\n}\n\nstatic void tg3_phy_toggle_apd(struct tg3 *tp, bool enable)\n{\n\tu32 reg;\n\n\tif (!tg3_flag(tp, 5705_PLUS) ||\n\t    (tg3_flag(tp, 5717_PLUS) &&\n\t     (tp->phy_flags & TG3_PHYFLG_MII_SERDES)))\n\t\treturn;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\ttg3_phy_fet_toggle_apd(tp, enable);\n\t\treturn;\n\t}\n\n\treg = MII_TG3_MISC_SHDW_WREN |\n\t      MII_TG3_MISC_SHDW_SCR5_SEL |\n\t      MII_TG3_MISC_SHDW_SCR5_LPED |\n\t      MII_TG3_MISC_SHDW_SCR5_DLPTLM |\n\t      MII_TG3_MISC_SHDW_SCR5_SDTL |\n\t      MII_TG3_MISC_SHDW_SCR5_C125OE;\n\tif (tg3_asic_rev(tp) != ASIC_REV_5784 || !enable)\n\t\treg |= MII_TG3_MISC_SHDW_SCR5_DLLAPD;\n\n\ttg3_writephy(tp, MII_TG3_MISC_SHDW, reg);\n\n\n\treg = MII_TG3_MISC_SHDW_WREN |\n\t      MII_TG3_MISC_SHDW_APD_SEL |\n\t      MII_TG3_MISC_SHDW_APD_WKTM_84MS;\n\tif (enable)\n\t\treg |= MII_TG3_MISC_SHDW_APD_ENABLE;\n\n\ttg3_writephy(tp, MII_TG3_MISC_SHDW, reg);\n}\n\nstatic void tg3_phy_toggle_automdix(struct tg3 *tp, int enable)\n{\n\tu32 phy;\n\n\tif (!tg3_flag(tp, 5705_PLUS) ||\n\t    (tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\treturn;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\tu32 ephy;\n\n\t\tif (!tg3_readphy(tp, MII_TG3_FET_TEST, &ephy)) {\n\t\t\tu32 reg = MII_TG3_FET_SHDW_MISCCTRL;\n\n\t\t\ttg3_writephy(tp, MII_TG3_FET_TEST,\n\t\t\t\t     ephy | MII_TG3_FET_SHADOW_EN);\n\t\t\tif (!tg3_readphy(tp, reg, &phy)) {\n\t\t\t\tif (enable)\n\t\t\t\t\tphy |= MII_TG3_FET_SHDW_MISCCTRL_MDIX;\n\t\t\t\telse\n\t\t\t\t\tphy &= ~MII_TG3_FET_SHDW_MISCCTRL_MDIX;\n\t\t\t\ttg3_writephy(tp, reg, phy);\n\t\t\t}\n\t\t\ttg3_writephy(tp, MII_TG3_FET_TEST, ephy);\n\t\t}\n\t} else {\n\t\tint ret;\n\n\t\tret = tg3_phy_auxctl_read(tp,\n\t\t\t\t\t  MII_TG3_AUXCTL_SHDWSEL_MISC, &phy);\n\t\tif (!ret) {\n\t\t\tif (enable)\n\t\t\t\tphy |= MII_TG3_AUXCTL_MISC_FORCE_AMDIX;\n\t\t\telse\n\t\t\t\tphy &= ~MII_TG3_AUXCTL_MISC_FORCE_AMDIX;\n\t\t\ttg3_phy_auxctl_write(tp,\n\t\t\t\t\t     MII_TG3_AUXCTL_SHDWSEL_MISC, phy);\n\t\t}\n\t}\n}\n\nstatic void tg3_phy_set_wirespeed(struct tg3 *tp)\n{\n\tint ret;\n\tu32 val;\n\n\tif (tp->phy_flags & TG3_PHYFLG_NO_ETH_WIRE_SPEED)\n\t\treturn;\n\n\tret = tg3_phy_auxctl_read(tp, MII_TG3_AUXCTL_SHDWSEL_MISC, &val);\n\tif (!ret)\n\t\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_MISC,\n\t\t\t\t     val | MII_TG3_AUXCTL_MISC_WIRESPD_EN);\n}\n\nstatic void tg3_phy_apply_otp(struct tg3 *tp)\n{\n\tu32 otp, phy;\n\n\tif (!tp->phy_otp)\n\t\treturn;\n\n\totp = tp->phy_otp;\n\n\tif (tg3_phy_toggle_auxctl_smdsp(tp, true))\n\t\treturn;\n\n\tphy = ((otp & TG3_OTP_AGCTGT_MASK) >> TG3_OTP_AGCTGT_SHIFT);\n\tphy |= MII_TG3_DSP_TAP1_AGCTGT_DFLT;\n\ttg3_phydsp_write(tp, MII_TG3_DSP_TAP1, phy);\n\n\tphy = ((otp & TG3_OTP_HPFFLTR_MASK) >> TG3_OTP_HPFFLTR_SHIFT) |\n\t      ((otp & TG3_OTP_HPFOVER_MASK) >> TG3_OTP_HPFOVER_SHIFT);\n\ttg3_phydsp_write(tp, MII_TG3_DSP_AADJ1CH0, phy);\n\n\tphy = ((otp & TG3_OTP_LPFDIS_MASK) >> TG3_OTP_LPFDIS_SHIFT);\n\tphy |= MII_TG3_DSP_AADJ1CH3_ADCCKADJ;\n\ttg3_phydsp_write(tp, MII_TG3_DSP_AADJ1CH3, phy);\n\n\tphy = ((otp & TG3_OTP_VDAC_MASK) >> TG3_OTP_VDAC_SHIFT);\n\ttg3_phydsp_write(tp, MII_TG3_DSP_EXP75, phy);\n\n\tphy = ((otp & TG3_OTP_10BTAMP_MASK) >> TG3_OTP_10BTAMP_SHIFT);\n\ttg3_phydsp_write(tp, MII_TG3_DSP_EXP96, phy);\n\n\tphy = ((otp & TG3_OTP_ROFF_MASK) >> TG3_OTP_ROFF_SHIFT) |\n\t      ((otp & TG3_OTP_RCOFF_MASK) >> TG3_OTP_RCOFF_SHIFT);\n\ttg3_phydsp_write(tp, MII_TG3_DSP_EXP97, phy);\n\n\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n}\n\nstatic void tg3_phy_eee_adjust(struct tg3 *tp, u32 current_link_up)\n{\n\tu32 val;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_EEE_CAP))\n\t\treturn;\n\n\ttp->setlpicnt = 0;\n\n\tif (tp->link_config.autoneg == AUTONEG_ENABLE &&\n\t    current_link_up == 1 &&\n\t    tp->link_config.active_duplex == DUPLEX_FULL &&\n\t    (tp->link_config.active_speed == SPEED_100 ||\n\t     tp->link_config.active_speed == SPEED_1000)) {\n\t\tu32 eeectl;\n\n\t\tif (tp->link_config.active_speed == SPEED_1000)\n\t\t\teeectl = TG3_CPMU_EEE_CTRL_EXIT_16_5_US;\n\t\telse\n\t\t\teeectl = TG3_CPMU_EEE_CTRL_EXIT_36_US;\n\n\t\ttw32(TG3_CPMU_EEE_CTRL, eeectl);\n\n\t\ttg3_phy_cl45_read(tp, MDIO_MMD_AN,\n\t\t\t\t  TG3_CL45_D7_EEERES_STAT, &val);\n\n\t\tif (val == TG3_CL45_D7_EEERES_STAT_LP_1000T ||\n\t\t    val == TG3_CL45_D7_EEERES_STAT_LP_100TX)\n\t\t\ttp->setlpicnt = 2;\n\t}\n\n\tif (!tp->setlpicnt) {\n\t\tif (current_link_up == 1 &&\n\t\t   !tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\t\ttg3_phydsp_write(tp, MII_TG3_DSP_TAP26, 0x0000);\n\t\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t\t}\n\n\t\tval = tr32(TG3_CPMU_EEE_MODE);\n\t\ttw32(TG3_CPMU_EEE_MODE, val & ~TG3_CPMU_EEEMD_LPI_ENABLE);\n\t}\n}\n\nstatic void tg3_phy_eee_enable(struct tg3 *tp)\n{\n\tu32 val;\n\n\tif (tp->link_config.active_speed == SPEED_1000 &&\n\t    (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t     tg3_flag(tp, 57765_CLASS)) &&\n\t    !tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\tval = MII_TG3_DSP_TAP26_ALNOKO |\n\t\t      MII_TG3_DSP_TAP26_RMRXSTO;\n\t\ttg3_phydsp_write(tp, MII_TG3_DSP_TAP26, val);\n\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t}\n\n\tval = tr32(TG3_CPMU_EEE_MODE);\n\ttw32(TG3_CPMU_EEE_MODE, val | TG3_CPMU_EEEMD_LPI_ENABLE);\n}\n\nstatic int tg3_wait_macro_done(struct tg3 *tp)\n{\n\tint limit = 100;\n\n\twhile (limit--) {\n\t\tu32 tmp32;\n\n\t\tif (!tg3_readphy(tp, MII_TG3_DSP_CONTROL, &tmp32)) {\n\t\t\tif ((tmp32 & 0x1000) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (limit < 0)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic int tg3_phy_write_and_check_testpat(struct tg3 *tp, int *resetp)\n{\n\tstatic const u32 test_pat[4][6] = {\n\t{ 0x00005555, 0x00000005, 0x00002aaa, 0x0000000a, 0x00003456, 0x00000003 },\n\t{ 0x00002aaa, 0x0000000a, 0x00003333, 0x00000003, 0x0000789a, 0x00000005 },\n\t{ 0x00005a5a, 0x00000005, 0x00002a6a, 0x0000000a, 0x00001bcd, 0x00000003 },\n\t{ 0x00002a5a, 0x0000000a, 0x000033c3, 0x00000003, 0x00002ef1, 0x00000005 }\n\t};\n\tint chan;\n\n\tfor (chan = 0; chan < 4; chan++) {\n\t\tint i;\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t     (chan * 0x2000) | 0x0200);\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0002);\n\n\t\tfor (i = 0; i < 6; i++)\n\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT,\n\t\t\t\t     test_pat[chan][i]);\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0202);\n\t\tif (tg3_wait_macro_done(tp)) {\n\t\t\t*resetp = 1;\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t     (chan * 0x2000) | 0x0200);\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0082);\n\t\tif (tg3_wait_macro_done(tp)) {\n\t\t\t*resetp = 1;\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0802);\n\t\tif (tg3_wait_macro_done(tp)) {\n\t\t\t*resetp = 1;\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\tfor (i = 0; i < 6; i += 2) {\n\t\t\tu32 low, high;\n\n\t\t\tif (tg3_readphy(tp, MII_TG3_DSP_RW_PORT, &low) ||\n\t\t\t    tg3_readphy(tp, MII_TG3_DSP_RW_PORT, &high) ||\n\t\t\t    tg3_wait_macro_done(tp)) {\n\t\t\t\t*resetp = 1;\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t\tlow &= 0x7fff;\n\t\t\thigh &= 0x000f;\n\t\t\tif (low != test_pat[chan][i] ||\n\t\t\t    high != test_pat[chan][i+1]) {\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS, 0x000b);\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x4001);\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x4005);\n\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int tg3_phy_reset_chanpat(struct tg3 *tp)\n{\n\tint chan;\n\n\tfor (chan = 0; chan < 4; chan++) {\n\t\tint i;\n\n\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t     (chan * 0x2000) | 0x0200);\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0002);\n\t\tfor (i = 0; i < 6; i++)\n\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x000);\n\t\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0202);\n\t\tif (tg3_wait_macro_done(tp))\n\t\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic int tg3_phy_reset_5703_4_5(struct tg3 *tp)\n{\n\tu32 reg32, phy9_orig;\n\tint retries, do_phy_reset, err;\n\n\tretries = 10;\n\tdo_phy_reset = 1;\n\tdo {\n\t\tif (do_phy_reset) {\n\t\t\terr = tg3_bmcr_reset(tp);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tdo_phy_reset = 0;\n\t\t}\n\n\t\t/* Disable transmitter and interrupt.  */\n\t\tif (tg3_readphy(tp, MII_TG3_EXT_CTRL, &reg32))\n\t\t\tcontinue;\n\n\t\treg32 |= 0x3000;\n\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL, reg32);\n\n\t\t/* Set full-duplex, 1000 mbps.  */\n\t\ttg3_writephy(tp, MII_BMCR,\n\t\t\t     BMCR_FULLDPLX | BMCR_SPEED1000);\n\n\t\t/* Set to master mode.  */\n\t\tif (tg3_readphy(tp, MII_CTRL1000, &phy9_orig))\n\t\t\tcontinue;\n\n\t\ttg3_writephy(tp, MII_CTRL1000,\n\t\t\t     CTL1000_AS_MASTER | CTL1000_ENABLE_MASTER);\n\n\t\terr = tg3_phy_toggle_auxctl_smdsp(tp, true);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/* Block the PHY control access.  */\n\t\ttg3_phydsp_write(tp, 0x8005, 0x0800);\n\n\t\terr = tg3_phy_write_and_check_testpat(tp, &do_phy_reset);\n\t\tif (!err)\n\t\t\tbreak;\n\t} while (--retries);\n\n\terr = tg3_phy_reset_chanpat(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_phydsp_write(tp, 0x8005, 0x0000);\n\n\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS, 0x8200);\n\ttg3_writephy(tp, MII_TG3_DSP_CONTROL, 0x0000);\n\n\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\n\ttg3_writephy(tp, MII_CTRL1000, phy9_orig);\n\n\tif (!tg3_readphy(tp, MII_TG3_EXT_CTRL, &reg32)) {\n\t\treg32 &= ~0x3000;\n\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL, reg32);\n\t} else if (!err)\n\t\terr = -EBUSY;\n\n\treturn err;\n}\n\nstatic void tg3_carrier_off(struct tg3 *tp)\n{\n\tnetif_carrier_off(tp->dev);\n\ttp->link_up = false;\n}\n\n/* This will reset the tigon3 PHY if there is no valid\n * link unless the FORCE argument is non-zero.\n */\nstatic int tg3_phy_reset(struct tg3 *tp)\n{\n\tu32 val, cpmuctrl;\n\tint err;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tval = tr32(GRC_MISC_CFG);\n\t\ttw32_f(GRC_MISC_CFG, val & ~GRC_MISC_CFG_EPHY_IDDQ);\n\t\tudelay(40);\n\t}\n\terr  = tg3_readphy(tp, MII_BMSR, &val);\n\terr |= tg3_readphy(tp, MII_BMSR, &val);\n\tif (err != 0)\n\t\treturn -EBUSY;\n\n\tif (netif_running(tp->dev) && tp->link_up) {\n\t\tnetif_carrier_off(tp->dev);\n\t\ttg3_link_report(tp);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\terr = tg3_phy_reset_5703_4_5(tp);\n\t\tif (err)\n\t\t\treturn err;\n\t\tgoto out;\n\t}\n\n\tcpmuctrl = 0;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5784 &&\n\t    tg3_chip_rev(tp) != CHIPREV_5784_AX) {\n\t\tcpmuctrl = tr32(TG3_CPMU_CTRL);\n\t\tif (cpmuctrl & CPMU_CTRL_GPHY_10MB_RXONLY)\n\t\t\ttw32(TG3_CPMU_CTRL,\n\t\t\t     cpmuctrl & ~CPMU_CTRL_GPHY_10MB_RXONLY);\n\t}\n\n\terr = tg3_bmcr_reset(tp);\n\tif (err)\n\t\treturn err;\n\n\tif (cpmuctrl & CPMU_CTRL_GPHY_10MB_RXONLY) {\n\t\tval = MII_TG3_DSP_EXP8_AEDW | MII_TG3_DSP_EXP8_REJ2MHz;\n\t\ttg3_phydsp_write(tp, MII_TG3_DSP_EXP8, val);\n\n\t\ttw32(TG3_CPMU_CTRL, cpmuctrl);\n\t}\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX ||\n\t    tg3_chip_rev(tp) == CHIPREV_5761_AX) {\n\t\tval = tr32(TG3_CPMU_LSPD_1000MB_CLK);\n\t\tif ((val & CPMU_LSPD_1000MB_MACCLK_MASK) ==\n\t\t    CPMU_LSPD_1000MB_MACCLK_12_5) {\n\t\t\tval &= ~CPMU_LSPD_1000MB_MACCLK_MASK;\n\t\t\tudelay(40);\n\t\t\ttw32_f(TG3_CPMU_LSPD_1000MB_CLK, val);\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, 5717_PLUS) &&\n\t    (tp->phy_flags & TG3_PHYFLG_MII_SERDES))\n\t\treturn 0;\n\n\ttg3_phy_apply_otp(tp);\n\n\tif (tp->phy_flags & TG3_PHYFLG_ENABLE_APD)\n\t\ttg3_phy_toggle_apd(tp, true);\n\telse\n\t\ttg3_phy_toggle_apd(tp, false);\n\nout:\n\tif ((tp->phy_flags & TG3_PHYFLG_ADC_BUG) &&\n\t    !tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\ttg3_phydsp_write(tp, 0x201f, 0x2aaa);\n\t\ttg3_phydsp_write(tp, 0x000a, 0x0323);\n\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t}\n\n\tif (tp->phy_flags & TG3_PHYFLG_5704_A0_BUG) {\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8d68);\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8d68);\n\t}\n\n\tif (tp->phy_flags & TG3_PHYFLG_BER_BUG) {\n\t\tif (!tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\t\ttg3_phydsp_write(tp, 0x000a, 0x310b);\n\t\t\ttg3_phydsp_write(tp, 0x201f, 0x9506);\n\t\t\ttg3_phydsp_write(tp, 0x401f, 0x14e2);\n\t\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t\t}\n\t} else if (tp->phy_flags & TG3_PHYFLG_JITTER_BUG) {\n\t\tif (!tg3_phy_toggle_auxctl_smdsp(tp, true)) {\n\t\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS, 0x000a);\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_ADJUST_TRIM) {\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x110b);\n\t\t\t\ttg3_writephy(tp, MII_TG3_TEST1,\n\t\t\t\t\t     MII_TG3_TEST1_TRIM_EN | 0x4);\n\t\t\t} else\n\t\t\t\ttg3_writephy(tp, MII_TG3_DSP_RW_PORT, 0x010b);\n\n\t\t\ttg3_phy_toggle_auxctl_smdsp(tp, false);\n\t\t}\n\t}\n\n\t/* Set Extended packet length bit (bit 14) on all chips that */\n\t/* support jumbo frames */\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5401) {\n\t\t/* Cannot do read-modify-write on 5401 */\n\t\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_AUXCTL, 0x4c20);\n\t} else if (tg3_flag(tp, JUMBO_CAPABLE)) {\n\t\t/* Set bit 14 with read-modify-write to preserve other bits */\n\t\terr = tg3_phy_auxctl_read(tp,\n\t\t\t\t\t  MII_TG3_AUXCTL_SHDWSEL_AUXCTL, &val);\n\t\tif (!err)\n\t\t\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_AUXCTL,\n\t\t\t\t\t   val | MII_TG3_AUXCTL_ACTL_EXTPKTLEN);\n\t}\n\n\t/* Set phy register 0x10 bit 0 to high fifo elasticity to support\n\t * jumbo frames transmission.\n\t */\n\tif (tg3_flag(tp, JUMBO_CAPABLE)) {\n\t\tif (!tg3_readphy(tp, MII_TG3_EXT_CTRL, &val))\n\t\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL,\n\t\t\t\t     val | MII_TG3_EXT_CTRL_FIFO_ELASTIC);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t/* adjust output voltage */\n\t\ttg3_writephy(tp, MII_TG3_FET_PTEST, 0x12);\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5762_A0)\n\t\ttg3_phydsp_write(tp, 0xffb, 0x4000);\n\n\ttg3_phy_toggle_automdix(tp, 1);\n\ttg3_phy_set_wirespeed(tp);\n\treturn 0;\n}\n\n#define TG3_GPIO_MSG_DRVR_PRES\t\t 0x00000001\n#define TG3_GPIO_MSG_NEED_VAUX\t\t 0x00000002\n#define TG3_GPIO_MSG_MASK\t\t (TG3_GPIO_MSG_DRVR_PRES | \\\n\t\t\t\t\t  TG3_GPIO_MSG_NEED_VAUX)\n#define TG3_GPIO_MSG_ALL_DRVR_PRES_MASK \\\n\t((TG3_GPIO_MSG_DRVR_PRES << 0) | \\\n\t (TG3_GPIO_MSG_DRVR_PRES << 4) | \\\n\t (TG3_GPIO_MSG_DRVR_PRES << 8) | \\\n\t (TG3_GPIO_MSG_DRVR_PRES << 12))\n\n#define TG3_GPIO_MSG_ALL_NEED_VAUX_MASK \\\n\t((TG3_GPIO_MSG_NEED_VAUX << 0) | \\\n\t (TG3_GPIO_MSG_NEED_VAUX << 4) | \\\n\t (TG3_GPIO_MSG_NEED_VAUX << 8) | \\\n\t (TG3_GPIO_MSG_NEED_VAUX << 12))\n\nstatic inline u32 tg3_set_function_status(struct tg3 *tp, u32 newstat)\n{\n\tu32 status, shift;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\tstatus = tg3_ape_read32(tp, TG3_APE_GPIO_MSG);\n\telse\n\t\tstatus = tr32(TG3_CPMU_DRV_STATUS);\n\n\tshift = TG3_APE_GPIO_MSG_SHIFT + 4 * tp->pci_fn;\n\tstatus &= ~(TG3_GPIO_MSG_MASK << shift);\n\tstatus |= (newstat << shift);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\ttg3_ape_write32(tp, TG3_APE_GPIO_MSG, status);\n\telse\n\t\ttw32(TG3_CPMU_DRV_STATUS, status);\n\n\treturn status >> TG3_APE_GPIO_MSG_SHIFT;\n}\n\nstatic inline int tg3_pwrsrc_switch_to_vmain(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, IS_NIC))\n\t\treturn 0;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720) {\n\t\tif (tg3_ape_lock(tp, TG3_APE_LOCK_GPIO))\n\t\t\treturn -EIO;\n\n\t\ttg3_set_function_status(tp, TG3_GPIO_MSG_DRVR_PRES);\n\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\ttg3_ape_unlock(tp, TG3_APE_LOCK_GPIO);\n\t} else {\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_pwrsrc_die_with_vmain(struct tg3 *tp)\n{\n\tu32 grc_local_ctrl;\n\n\tif (!tg3_flag(tp, IS_NIC) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5701)\n\t\treturn;\n\n\tgrc_local_ctrl = tp->grc_local_ctrl | GRC_LCLCTRL_GPIO_OE1;\n\n\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t    grc_local_ctrl | GRC_LCLCTRL_GPIO_OUTPUT1,\n\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t    grc_local_ctrl,\n\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t    grc_local_ctrl | GRC_LCLCTRL_GPIO_OUTPUT1,\n\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n}\n\nstatic void tg3_pwrsrc_switch_to_vaux(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, IS_NIC))\n\t\treturn;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl |\n\t\t\t    (GRC_LCLCTRL_GPIO_OE0 |\n\t\t\t     GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t     GRC_LCLCTRL_GPIO_OE2 |\n\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT0 |\n\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT1),\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t} else if (tp->pdev->device == PCI_DEVICE_ID_TIGON3_5761 ||\n\t\t   tp->pdev->device == TG3PCI_DEVICE_TIGON3_5761S) {\n\t\t/* The 5761 non-e device swaps GPIO 0 and GPIO 2. */\n\t\tu32 grc_local_ctrl = GRC_LCLCTRL_GPIO_OE0 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OE2 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT0 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT1 |\n\t\t\t\t     tp->grc_local_ctrl;\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\tgrc_local_ctrl |= GRC_LCLCTRL_GPIO_OUTPUT2;\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\tgrc_local_ctrl &= ~GRC_LCLCTRL_GPIO_OUTPUT0;\n\t\ttw32_wait_f(GRC_LOCAL_CTRL, grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t} else {\n\t\tu32 no_gpio2;\n\t\tu32 grc_local_ctrl = 0;\n\n\t\t/* Workaround to prevent overdrawing Amps. */\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\t\tgrc_local_ctrl |= GRC_LCLCTRL_GPIO_OE3;\n\t\t\ttw32_wait_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl |\n\t\t\t\t    grc_local_ctrl,\n\t\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t\t}\n\n\t\t/* On 5753 and variants, GPIO2 cannot be used. */\n\t\tno_gpio2 = tp->nic_sram_data_cfg &\n\t\t\t   NIC_SRAM_DATA_CFG_NO_GPIO2;\n\n\t\tgrc_local_ctrl |= GRC_LCLCTRL_GPIO_OE0 |\n\t\t\t\t  GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t\t  GRC_LCLCTRL_GPIO_OE2 |\n\t\t\t\t  GRC_LCLCTRL_GPIO_OUTPUT1 |\n\t\t\t\t  GRC_LCLCTRL_GPIO_OUTPUT2;\n\t\tif (no_gpio2) {\n\t\t\tgrc_local_ctrl &= ~(GRC_LCLCTRL_GPIO_OE2 |\n\t\t\t\t\t    GRC_LCLCTRL_GPIO_OUTPUT2);\n\t\t}\n\t\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t\t    tp->grc_local_ctrl | grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\tgrc_local_ctrl |= GRC_LCLCTRL_GPIO_OUTPUT0;\n\n\t\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t\t    tp->grc_local_ctrl | grc_local_ctrl,\n\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\n\t\tif (!no_gpio2) {\n\t\t\tgrc_local_ctrl &= ~GRC_LCLCTRL_GPIO_OUTPUT2;\n\t\t\ttw32_wait_f(GRC_LOCAL_CTRL,\n\t\t\t\t    tp->grc_local_ctrl | grc_local_ctrl,\n\t\t\t\t    TG3_GRC_LCLCTL_PWRSW_DELAY);\n\t\t}\n\t}\n}\n\nstatic void tg3_frob_aux_power_5717(struct tg3 *tp, bool wol_enable)\n{\n\tu32 msg = 0;\n\n\t/* Serialize power state transitions */\n\tif (tg3_ape_lock(tp, TG3_APE_LOCK_GPIO))\n\t\treturn;\n\n\tif (tg3_flag(tp, ENABLE_ASF) || tg3_flag(tp, ENABLE_APE) || wol_enable)\n\t\tmsg = TG3_GPIO_MSG_NEED_VAUX;\n\n\tmsg = tg3_set_function_status(tp, msg);\n\n\tif (msg & TG3_GPIO_MSG_ALL_DRVR_PRES_MASK)\n\t\tgoto done;\n\n\tif (msg & TG3_GPIO_MSG_ALL_NEED_VAUX_MASK)\n\t\ttg3_pwrsrc_switch_to_vaux(tp);\n\telse\n\t\ttg3_pwrsrc_die_with_vmain(tp);\n\ndone:\n\ttg3_ape_unlock(tp, TG3_APE_LOCK_GPIO);\n}\n\nstatic void tg3_frob_aux_power(struct tg3 *tp, bool include_wol)\n{\n\tbool need_vaux = false;\n\n\t/* The GPIOs do something completely different on 57765. */\n\tif (!tg3_flag(tp, IS_NIC) || tg3_flag(tp, 57765_CLASS))\n\t\treturn;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720) {\n\t\ttg3_frob_aux_power_5717(tp, include_wol ?\n\t\t\t\t\ttg3_flag(tp, WOL_ENABLE) != 0 : 0);\n\t\treturn;\n\t}\n\n\tif (tp->pdev_peer && tp->pdev_peer != tp->pdev) {\n\t\tstruct net_device *dev_peer;\n\n\t\tdev_peer = pci_get_drvdata(tp->pdev_peer);\n\n\t\t/* remove_one() may have been run on the peer. */\n\t\tif (dev_peer) {\n\t\t\tstruct tg3 *tp_peer = netdev_priv(dev_peer);\n\n\t\t\tif (tg3_flag(tp_peer, INIT_COMPLETE))\n\t\t\t\treturn;\n\n\t\t\tif ((include_wol && tg3_flag(tp_peer, WOL_ENABLE)) ||\n\t\t\t    tg3_flag(tp_peer, ENABLE_ASF))\n\t\t\t\tneed_vaux = true;\n\t\t}\n\t}\n\n\tif ((include_wol && tg3_flag(tp, WOL_ENABLE)) ||\n\t    tg3_flag(tp, ENABLE_ASF))\n\t\tneed_vaux = true;\n\n\tif (need_vaux)\n\t\ttg3_pwrsrc_switch_to_vaux(tp);\n\telse\n\t\ttg3_pwrsrc_die_with_vmain(tp);\n}\n\nstatic int tg3_5700_link_polarity(struct tg3 *tp, u32 speed)\n{\n\tif (tp->led_ctrl == LED_CTRL_MODE_PHY_2)\n\t\treturn 1;\n\telse if ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5411) {\n\t\tif (speed != SPEED_10)\n\t\t\treturn 1;\n\t} else if (speed == SPEED_10)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void tg3_power_down_phy(struct tg3 *tp, bool do_low_power)\n{\n\tu32 val;\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5704) {\n\t\t\tu32 sg_dig_ctrl = tr32(SG_DIG_CTRL);\n\t\t\tu32 serdes_cfg = tr32(MAC_SERDES_CFG);\n\n\t\t\tsg_dig_ctrl |=\n\t\t\t\tSG_DIG_USING_HW_AUTONEG | SG_DIG_SOFT_RESET;\n\t\t\ttw32(SG_DIG_CTRL, sg_dig_ctrl);\n\t\t\ttw32(MAC_SERDES_CFG, serdes_cfg | (1 << 15));\n\t\t}\n\t\treturn;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\ttg3_bmcr_reset(tp);\n\t\tval = tr32(GRC_MISC_CFG);\n\t\ttw32_f(GRC_MISC_CFG, val | GRC_MISC_CFG_EPHY_IDDQ);\n\t\tudelay(40);\n\t\treturn;\n\t} else if (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\tu32 phytest;\n\t\tif (!tg3_readphy(tp, MII_TG3_FET_TEST, &phytest)) {\n\t\t\tu32 phy;\n\n\t\t\ttg3_writephy(tp, MII_ADVERTISE, 0);\n\t\t\ttg3_writephy(tp, MII_BMCR,\n\t\t\t\t     BMCR_ANENABLE | BMCR_ANRESTART);\n\n\t\t\ttg3_writephy(tp, MII_TG3_FET_TEST,\n\t\t\t\t     phytest | MII_TG3_FET_SHADOW_EN);\n\t\t\tif (!tg3_readphy(tp, MII_TG3_FET_SHDW_AUXMODE4, &phy)) {\n\t\t\t\tphy |= MII_TG3_FET_SHDW_AUXMODE4_SBPD;\n\t\t\t\ttg3_writephy(tp,\n\t\t\t\t\t     MII_TG3_FET_SHDW_AUXMODE4,\n\t\t\t\t\t     phy);\n\t\t\t}\n\t\t\ttg3_writephy(tp, MII_TG3_FET_TEST, phytest);\n\t\t}\n\t\treturn;\n\t} else if (do_low_power) {\n\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL,\n\t\t\t     MII_TG3_EXT_CTRL_FORCE_LED_OFF);\n\n\t\tval = MII_TG3_AUXCTL_PCTL_100TX_LPWR |\n\t\t      MII_TG3_AUXCTL_PCTL_SPR_ISOLATE |\n\t\t      MII_TG3_AUXCTL_PCTL_VREG_11V;\n\t\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_PWRCTL, val);\n\t}\n\n\t/* The PHY should not be powered down on some chips because\n\t * of bugs.\n\t */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    (tg3_asic_rev(tp) == ASIC_REV_5780 &&\n\t     (tp->phy_flags & TG3_PHYFLG_MII_SERDES)) ||\n\t    (tg3_asic_rev(tp) == ASIC_REV_5717 &&\n\t     !tp->pci_fn))\n\t\treturn;\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX ||\n\t    tg3_chip_rev(tp) == CHIPREV_5761_AX) {\n\t\tval = tr32(TG3_CPMU_LSPD_1000MB_CLK);\n\t\tval &= ~CPMU_LSPD_1000MB_MACCLK_MASK;\n\t\tval |= CPMU_LSPD_1000MB_MACCLK_12_5;\n\t\ttw32_f(TG3_CPMU_LSPD_1000MB_CLK, val);\n\t}\n\n\ttg3_writephy(tp, MII_BMCR, BMCR_PDOWN);\n}\n\n/* tp->lock is held. */\nstatic int tg3_nvram_lock(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, NVRAM)) {\n\t\tint i;\n\n\t\tif (tp->nvram_lock_cnt == 0) {\n\t\t\ttw32(NVRAM_SWARB, SWARB_REQ_SET1);\n\t\t\tfor (i = 0; i < 8000; i++) {\n\t\t\t\tif (tr32(NVRAM_SWARB) & SWARB_GNT1)\n\t\t\t\t\tbreak;\n\t\t\t\tudelay(20);\n\t\t\t}\n\t\t\tif (i == 8000) {\n\t\t\t\ttw32(NVRAM_SWARB, SWARB_REQ_CLR1);\n\t\t\t\treturn -ENODEV;\n\t\t\t}\n\t\t}\n\t\ttp->nvram_lock_cnt++;\n\t}\n\treturn 0;\n}\n\n/* tp->lock is held. */\nstatic void tg3_nvram_unlock(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, NVRAM)) {\n\t\tif (tp->nvram_lock_cnt > 0)\n\t\t\ttp->nvram_lock_cnt--;\n\t\tif (tp->nvram_lock_cnt == 0)\n\t\t\ttw32_f(NVRAM_SWARB, SWARB_REQ_CLR1);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_enable_nvram_access(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, 5750_PLUS) && !tg3_flag(tp, PROTECTED_NVRAM)) {\n\t\tu32 nvaccess = tr32(NVRAM_ACCESS);\n\n\t\ttw32(NVRAM_ACCESS, nvaccess | ACCESS_ENABLE);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_disable_nvram_access(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, 5750_PLUS) && !tg3_flag(tp, PROTECTED_NVRAM)) {\n\t\tu32 nvaccess = tr32(NVRAM_ACCESS);\n\n\t\ttw32(NVRAM_ACCESS, nvaccess & ~ACCESS_ENABLE);\n\t}\n}\n\nstatic int tg3_nvram_read_using_eeprom(struct tg3 *tp,\n\t\t\t\t\tu32 offset, u32 *val)\n{\n\tu32 tmp;\n\tint i;\n\n\tif (offset > EEPROM_ADDR_ADDR_MASK || (offset % 4) != 0)\n\t\treturn -EINVAL;\n\n\ttmp = tr32(GRC_EEPROM_ADDR) & ~(EEPROM_ADDR_ADDR_MASK |\n\t\t\t\t\tEEPROM_ADDR_DEVID_MASK |\n\t\t\t\t\tEEPROM_ADDR_READ);\n\ttw32(GRC_EEPROM_ADDR,\n\t     tmp |\n\t     (0 << EEPROM_ADDR_DEVID_SHIFT) |\n\t     ((offset << EEPROM_ADDR_ADDR_SHIFT) &\n\t      EEPROM_ADDR_ADDR_MASK) |\n\t     EEPROM_ADDR_READ | EEPROM_ADDR_START);\n\n\tfor (i = 0; i < 1000; i++) {\n\t\ttmp = tr32(GRC_EEPROM_ADDR);\n\n\t\tif (tmp & EEPROM_ADDR_COMPLETE)\n\t\t\tbreak;\n\t\tmsleep(1);\n\t}\n\tif (!(tmp & EEPROM_ADDR_COMPLETE))\n\t\treturn -EBUSY;\n\n\ttmp = tr32(GRC_EEPROM_DATA);\n\n\t/*\n\t * The data will always be opposite the native endian\n\t * format.  Perform a blind byteswap to compensate.\n\t */\n\t*val = swab32(tmp);\n\n\treturn 0;\n}\n\n#define NVRAM_CMD_TIMEOUT 10000\n\nstatic int tg3_nvram_exec_cmd(struct tg3 *tp, u32 nvram_cmd)\n{\n\tint i;\n\n\ttw32(NVRAM_CMD, nvram_cmd);\n\tfor (i = 0; i < NVRAM_CMD_TIMEOUT; i++) {\n\t\tudelay(10);\n\t\tif (tr32(NVRAM_CMD) & NVRAM_CMD_DONE) {\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == NVRAM_CMD_TIMEOUT)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic u32 tg3_nvram_phys_addr(struct tg3 *tp, u32 addr)\n{\n\tif (tg3_flag(tp, NVRAM) &&\n\t    tg3_flag(tp, NVRAM_BUFFERED) &&\n\t    tg3_flag(tp, FLASH) &&\n\t    !tg3_flag(tp, NO_NVRAM_ADDR_TRANS) &&\n\t    (tp->nvram_jedecnum == JEDEC_ATMEL))\n\n\t\taddr = ((addr / tp->nvram_pagesize) <<\n\t\t\tATMEL_AT45DB0X1B_PAGE_POS) +\n\t\t       (addr % tp->nvram_pagesize);\n\n\treturn addr;\n}\n\nstatic u32 tg3_nvram_logical_addr(struct tg3 *tp, u32 addr)\n{\n\tif (tg3_flag(tp, NVRAM) &&\n\t    tg3_flag(tp, NVRAM_BUFFERED) &&\n\t    tg3_flag(tp, FLASH) &&\n\t    !tg3_flag(tp, NO_NVRAM_ADDR_TRANS) &&\n\t    (tp->nvram_jedecnum == JEDEC_ATMEL))\n\n\t\taddr = ((addr >> ATMEL_AT45DB0X1B_PAGE_POS) *\n\t\t\ttp->nvram_pagesize) +\n\t\t       (addr & ((1 << ATMEL_AT45DB0X1B_PAGE_POS) - 1));\n\n\treturn addr;\n}\n\n/* NOTE: Data read in from NVRAM is byteswapped according to\n * the byteswapping settings for all other register accesses.\n * tg3 devices are BE devices, so on a BE machine, the data\n * returned will be exactly as it is seen in NVRAM.  On a LE\n * machine, the 32-bit value will be byteswapped.\n */\nstatic int tg3_nvram_read(struct tg3 *tp, u32 offset, u32 *val)\n{\n\tint ret;\n\n\tif (!tg3_flag(tp, NVRAM))\n\t\treturn tg3_nvram_read_using_eeprom(tp, offset, val);\n\n\toffset = tg3_nvram_phys_addr(tp, offset);\n\n\tif (offset > NVRAM_ADDR_MSK)\n\t\treturn -EINVAL;\n\n\tret = tg3_nvram_lock(tp);\n\tif (ret)\n\t\treturn ret;\n\n\ttg3_enable_nvram_access(tp);\n\n\ttw32(NVRAM_ADDR, offset);\n\tret = tg3_nvram_exec_cmd(tp, NVRAM_CMD_RD | NVRAM_CMD_GO |\n\t\tNVRAM_CMD_FIRST | NVRAM_CMD_LAST | NVRAM_CMD_DONE);\n\n\tif (ret == 0)\n\t\t*val = tr32(NVRAM_RDDATA);\n\n\ttg3_disable_nvram_access(tp);\n\n\ttg3_nvram_unlock(tp);\n\n\treturn ret;\n}\n\n/* Ensures NVRAM data is in bytestream format. */\nstatic int tg3_nvram_read_be32(struct tg3 *tp, u32 offset, __be32 *val)\n{\n\tu32 v;\n\tint res = tg3_nvram_read(tp, offset, &v);\n\tif (!res)\n\t\t*val = cpu_to_be32(v);\n\treturn res;\n}\n\nstatic int tg3_nvram_write_block_using_eeprom(struct tg3 *tp,\n\t\t\t\t    u32 offset, u32 len, u8 *buf)\n{\n\tint i, j, rc = 0;\n\tu32 val;\n\n\tfor (i = 0; i < len; i += 4) {\n\t\tu32 addr;\n\t\t__be32 data;\n\n\t\taddr = offset + i;\n\n\t\tmemcpy(&data, buf + i, 4);\n\n\t\t/*\n\t\t * The SEEPROM interface expects the data to always be opposite\n\t\t * the native endian format.  We accomplish this by reversing\n\t\t * all the operations that would have been performed on the\n\t\t * data from a call to tg3_nvram_read_be32().\n\t\t */\n\t\ttw32(GRC_EEPROM_DATA, swab32(be32_to_cpu(data)));\n\n\t\tval = tr32(GRC_EEPROM_ADDR);\n\t\ttw32(GRC_EEPROM_ADDR, val | EEPROM_ADDR_COMPLETE);\n\n\t\tval &= ~(EEPROM_ADDR_ADDR_MASK | EEPROM_ADDR_DEVID_MASK |\n\t\t\tEEPROM_ADDR_READ);\n\t\ttw32(GRC_EEPROM_ADDR, val |\n\t\t\t(0 << EEPROM_ADDR_DEVID_SHIFT) |\n\t\t\t(addr & EEPROM_ADDR_ADDR_MASK) |\n\t\t\tEEPROM_ADDR_START |\n\t\t\tEEPROM_ADDR_WRITE);\n\n\t\tfor (j = 0; j < 1000; j++) {\n\t\t\tval = tr32(GRC_EEPROM_ADDR);\n\n\t\t\tif (val & EEPROM_ADDR_COMPLETE)\n\t\t\t\tbreak;\n\t\t\tmsleep(1);\n\t\t}\n\t\tif (!(val & EEPROM_ADDR_COMPLETE)) {\n\t\t\trc = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\n/* offset and length are dword aligned */\nstatic int tg3_nvram_write_block_unbuffered(struct tg3 *tp, u32 offset, u32 len,\n\t\tu8 *buf)\n{\n\tint ret = 0;\n\tu32 pagesize = tp->nvram_pagesize;\n\tu32 pagemask = pagesize - 1;\n\tu32 nvram_cmd;\n\tu8 *tmp;\n\n\ttmp = kmalloc(pagesize, GFP_KERNEL);\n\tif (tmp == NULL)\n\t\treturn -ENOMEM;\n\n\twhile (len) {\n\t\tint j;\n\t\tu32 phy_addr, page_off, size;\n\n\t\tphy_addr = offset & ~pagemask;\n\n\t\tfor (j = 0; j < pagesize; j += 4) {\n\t\t\tret = tg3_nvram_read_be32(tp, phy_addr + j,\n\t\t\t\t\t\t  (__be32 *) (tmp + j));\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tpage_off = offset & pagemask;\n\t\tsize = pagesize;\n\t\tif (len < size)\n\t\t\tsize = len;\n\n\t\tlen -= size;\n\n\t\tmemcpy(tmp + page_off, buf, size);\n\n\t\toffset = offset + (pagesize - page_off);\n\n\t\ttg3_enable_nvram_access(tp);\n\n\t\t/*\n\t\t * Before we can erase the flash page, we need\n\t\t * to issue a special \"write enable\" command.\n\t\t */\n\t\tnvram_cmd = NVRAM_CMD_WREN | NVRAM_CMD_GO | NVRAM_CMD_DONE;\n\n\t\tif (tg3_nvram_exec_cmd(tp, nvram_cmd))\n\t\t\tbreak;\n\n\t\t/* Erase the target page */\n\t\ttw32(NVRAM_ADDR, phy_addr);\n\n\t\tnvram_cmd = NVRAM_CMD_GO | NVRAM_CMD_DONE | NVRAM_CMD_WR |\n\t\t\tNVRAM_CMD_FIRST | NVRAM_CMD_LAST | NVRAM_CMD_ERASE;\n\n\t\tif (tg3_nvram_exec_cmd(tp, nvram_cmd))\n\t\t\tbreak;\n\n\t\t/* Issue another write enable to start the write. */\n\t\tnvram_cmd = NVRAM_CMD_WREN | NVRAM_CMD_GO | NVRAM_CMD_DONE;\n\n\t\tif (tg3_nvram_exec_cmd(tp, nvram_cmd))\n\t\t\tbreak;\n\n\t\tfor (j = 0; j < pagesize; j += 4) {\n\t\t\t__be32 data;\n\n\t\t\tdata = *((__be32 *) (tmp + j));\n\n\t\t\ttw32(NVRAM_WRDATA, be32_to_cpu(data));\n\n\t\t\ttw32(NVRAM_ADDR, phy_addr + j);\n\n\t\t\tnvram_cmd = NVRAM_CMD_GO | NVRAM_CMD_DONE |\n\t\t\t\tNVRAM_CMD_WR;\n\n\t\t\tif (j == 0)\n\t\t\t\tnvram_cmd |= NVRAM_CMD_FIRST;\n\t\t\telse if (j == (pagesize - 4))\n\t\t\t\tnvram_cmd |= NVRAM_CMD_LAST;\n\n\t\t\tret = tg3_nvram_exec_cmd(tp, nvram_cmd);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tnvram_cmd = NVRAM_CMD_WRDI | NVRAM_CMD_GO | NVRAM_CMD_DONE;\n\ttg3_nvram_exec_cmd(tp, nvram_cmd);\n\n\tkfree(tmp);\n\n\treturn ret;\n}\n\n/* offset and length are dword aligned */\nstatic int tg3_nvram_write_block_buffered(struct tg3 *tp, u32 offset, u32 len,\n\t\tu8 *buf)\n{\n\tint i, ret = 0;\n\n\tfor (i = 0; i < len; i += 4, offset += 4) {\n\t\tu32 page_off, phy_addr, nvram_cmd;\n\t\t__be32 data;\n\n\t\tmemcpy(&data, buf + i, 4);\n\t\ttw32(NVRAM_WRDATA, be32_to_cpu(data));\n\n\t\tpage_off = offset % tp->nvram_pagesize;\n\n\t\tphy_addr = tg3_nvram_phys_addr(tp, offset);\n\n\t\tnvram_cmd = NVRAM_CMD_GO | NVRAM_CMD_DONE | NVRAM_CMD_WR;\n\n\t\tif (page_off == 0 || i == 0)\n\t\t\tnvram_cmd |= NVRAM_CMD_FIRST;\n\t\tif (page_off == (tp->nvram_pagesize - 4))\n\t\t\tnvram_cmd |= NVRAM_CMD_LAST;\n\n\t\tif (i == (len - 4))\n\t\t\tnvram_cmd |= NVRAM_CMD_LAST;\n\n\t\tif ((nvram_cmd & NVRAM_CMD_FIRST) ||\n\t\t    !tg3_flag(tp, FLASH) ||\n\t\t    !tg3_flag(tp, 57765_PLUS))\n\t\t\ttw32(NVRAM_ADDR, phy_addr);\n\n\t\tif (tg3_asic_rev(tp) != ASIC_REV_5752 &&\n\t\t    !tg3_flag(tp, 5755_PLUS) &&\n\t\t    (tp->nvram_jedecnum == JEDEC_ST) &&\n\t\t    (nvram_cmd & NVRAM_CMD_FIRST)) {\n\t\t\tu32 cmd;\n\n\t\t\tcmd = NVRAM_CMD_WREN | NVRAM_CMD_GO | NVRAM_CMD_DONE;\n\t\t\tret = tg3_nvram_exec_cmd(tp, cmd);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!tg3_flag(tp, FLASH)) {\n\t\t\t/* We always do complete word writes to eeprom. */\n\t\t\tnvram_cmd |= (NVRAM_CMD_FIRST | NVRAM_CMD_LAST);\n\t\t}\n\n\t\tret = tg3_nvram_exec_cmd(tp, nvram_cmd);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/* offset and length are dword aligned */\nstatic int tg3_nvram_write_block(struct tg3 *tp, u32 offset, u32 len, u8 *buf)\n{\n\tint ret;\n\n\tif (tg3_flag(tp, EEPROM_WRITE_PROT)) {\n\t\ttw32_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl &\n\t\t       ~GRC_LCLCTRL_GPIO_OUTPUT1);\n\t\tudelay(40);\n\t}\n\n\tif (!tg3_flag(tp, NVRAM)) {\n\t\tret = tg3_nvram_write_block_using_eeprom(tp, offset, len, buf);\n\t} else {\n\t\tu32 grc_mode;\n\n\t\tret = tg3_nvram_lock(tp);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\ttg3_enable_nvram_access(tp);\n\t\tif (tg3_flag(tp, 5750_PLUS) && !tg3_flag(tp, PROTECTED_NVRAM))\n\t\t\ttw32(NVRAM_WRITE1, 0x406);\n\n\t\tgrc_mode = tr32(GRC_MODE);\n\t\ttw32(GRC_MODE, grc_mode | GRC_MODE_NVRAM_WR_ENABLE);\n\n\t\tif (tg3_flag(tp, NVRAM_BUFFERED) || !tg3_flag(tp, FLASH)) {\n\t\t\tret = tg3_nvram_write_block_buffered(tp, offset, len,\n\t\t\t\tbuf);\n\t\t} else {\n\t\t\tret = tg3_nvram_write_block_unbuffered(tp, offset, len,\n\t\t\t\tbuf);\n\t\t}\n\n\t\tgrc_mode = tr32(GRC_MODE);\n\t\ttw32(GRC_MODE, grc_mode & ~GRC_MODE_NVRAM_WR_ENABLE);\n\n\t\ttg3_disable_nvram_access(tp);\n\t\ttg3_nvram_unlock(tp);\n\t}\n\n\tif (tg3_flag(tp, EEPROM_WRITE_PROT)) {\n\t\ttw32_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl);\n\t\tudelay(40);\n\t}\n\n\treturn ret;\n}\n\n#define RX_CPU_SCRATCH_BASE\t0x30000\n#define RX_CPU_SCRATCH_SIZE\t0x04000\n#define TX_CPU_SCRATCH_BASE\t0x34000\n#define TX_CPU_SCRATCH_SIZE\t0x04000\n\n/* tp->lock is held. */\nstatic int tg3_halt_cpu(struct tg3 *tp, u32 offset)\n{\n\tint i;\n\n\tBUG_ON(offset == TX_CPU_BASE && tg3_flag(tp, 5705_PLUS));\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tu32 val = tr32(GRC_VCPU_EXT_CTRL);\n\n\t\ttw32(GRC_VCPU_EXT_CTRL, val | GRC_VCPU_EXT_CTRL_HALT_CPU);\n\t\treturn 0;\n\t}\n\tif (offset == RX_CPU_BASE) {\n\t\tfor (i = 0; i < 10000; i++) {\n\t\t\ttw32(offset + CPU_STATE, 0xffffffff);\n\t\t\ttw32(offset + CPU_MODE,  CPU_MODE_HALT);\n\t\t\tif (tr32(offset + CPU_MODE) & CPU_MODE_HALT)\n\t\t\t\tbreak;\n\t\t}\n\n\t\ttw32(offset + CPU_STATE, 0xffffffff);\n\t\ttw32_f(offset + CPU_MODE,  CPU_MODE_HALT);\n\t\tudelay(10);\n\t} else {\n\t\t/*\n\t\t * There is only an Rx CPU for the 5750 derivative in the\n\t\t * BCM4785.\n\t\t */\n\t\tif (tg3_flag(tp, IS_SSB_CORE))\n\t\t\treturn 0;\n\n\t\tfor (i = 0; i < 10000; i++) {\n\t\t\ttw32(offset + CPU_STATE, 0xffffffff);\n\t\t\ttw32(offset + CPU_MODE,  CPU_MODE_HALT);\n\t\t\tif (tr32(offset + CPU_MODE) & CPU_MODE_HALT)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i >= 10000) {\n\t\tnetdev_err(tp->dev, \"%s timed out, %s CPU\\n\",\n\t\t\t   __func__, offset == RX_CPU_BASE ? \"RX\" : \"TX\");\n\t\treturn -ENODEV;\n\t}\n\n\t/* Clear firmware's nvram arbitration. */\n\tif (tg3_flag(tp, NVRAM))\n\t\ttw32(NVRAM_SWARB, SWARB_REQ_CLR0);\n\treturn 0;\n}\n\nstruct fw_info {\n\tunsigned int fw_base;\n\tunsigned int fw_len;\n\tconst __be32 *fw_data;\n};\n\n/* tp->lock is held. */\nstatic int tg3_load_firmware_cpu(struct tg3 *tp, u32 cpu_base,\n\t\t\t\t u32 cpu_scratch_base, int cpu_scratch_size,\n\t\t\t\t struct fw_info *info)\n{\n\tint err, lock_err, i;\n\tvoid (*write_op)(struct tg3 *, u32, u32);\n\n\tif (cpu_base == TX_CPU_BASE && tg3_flag(tp, 5705_PLUS)) {\n\t\tnetdev_err(tp->dev,\n\t\t\t   \"%s: Trying to load TX cpu firmware which is 5705\\n\",\n\t\t\t   __func__);\n\t\treturn -EINVAL;\n\t}\n\n\tif (tg3_flag(tp, 5705_PLUS))\n\t\twrite_op = tg3_write_mem;\n\telse\n\t\twrite_op = tg3_write_indirect_reg32;\n\n\t/* It is possible that bootcode is still loading at this point.\n\t * Get the nvram lock first before halting the cpu.\n\t */\n\tlock_err = tg3_nvram_lock(tp);\n\terr = tg3_halt_cpu(tp, cpu_base);\n\tif (!lock_err)\n\t\ttg3_nvram_unlock(tp);\n\tif (err)\n\t\tgoto out;\n\n\tfor (i = 0; i < cpu_scratch_size; i += sizeof(u32))\n\t\twrite_op(tp, cpu_scratch_base + i, 0);\n\ttw32(cpu_base + CPU_STATE, 0xffffffff);\n\ttw32(cpu_base + CPU_MODE, tr32(cpu_base+CPU_MODE)|CPU_MODE_HALT);\n\tfor (i = 0; i < (info->fw_len / sizeof(u32)); i++)\n\t\twrite_op(tp, (cpu_scratch_base +\n\t\t\t      (info->fw_base & 0xffff) +\n\t\t\t      (i * sizeof(u32))),\n\t\t\t      be32_to_cpu(info->fw_data[i]));\n\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/* tp->lock is held. */\nstatic int tg3_load_5701_a0_firmware_fix(struct tg3 *tp)\n{\n\tstruct fw_info info;\n\tconst __be32 *fw_data;\n\tint err, i;\n\n\tfw_data = (void *)tp->fw->data;\n\n\t/* Firmware blob starts with version numbers, followed by\n\t   start address and length. We are setting complete length.\n\t   length = end_address_of_bss - start_address_of_text.\n\t   Remainder is the blob to be loaded contiguously\n\t   from start address. */\n\n\tinfo.fw_base = be32_to_cpu(fw_data[1]);\n\tinfo.fw_len = tp->fw->size - 12;\n\tinfo.fw_data = &fw_data[3];\n\n\terr = tg3_load_firmware_cpu(tp, RX_CPU_BASE,\n\t\t\t\t    RX_CPU_SCRATCH_BASE, RX_CPU_SCRATCH_SIZE,\n\t\t\t\t    &info);\n\tif (err)\n\t\treturn err;\n\n\terr = tg3_load_firmware_cpu(tp, TX_CPU_BASE,\n\t\t\t\t    TX_CPU_SCRATCH_BASE, TX_CPU_SCRATCH_SIZE,\n\t\t\t\t    &info);\n\tif (err)\n\t\treturn err;\n\n\t/* Now startup only the RX cpu. */\n\ttw32(RX_CPU_BASE + CPU_STATE, 0xffffffff);\n\ttw32_f(RX_CPU_BASE + CPU_PC, info.fw_base);\n\n\tfor (i = 0; i < 5; i++) {\n\t\tif (tr32(RX_CPU_BASE + CPU_PC) == info.fw_base)\n\t\t\tbreak;\n\t\ttw32(RX_CPU_BASE + CPU_STATE, 0xffffffff);\n\t\ttw32(RX_CPU_BASE + CPU_MODE,  CPU_MODE_HALT);\n\t\ttw32_f(RX_CPU_BASE + CPU_PC, info.fw_base);\n\t\tudelay(1000);\n\t}\n\tif (i >= 5) {\n\t\tnetdev_err(tp->dev, \"%s fails to set RX CPU PC, is %08x \"\n\t\t\t   \"should be %08x\\n\", __func__,\n\t\t\t   tr32(RX_CPU_BASE + CPU_PC), info.fw_base);\n\t\treturn -ENODEV;\n\t}\n\ttw32(RX_CPU_BASE + CPU_STATE, 0xffffffff);\n\ttw32_f(RX_CPU_BASE + CPU_MODE,  0x00000000);\n\n\treturn 0;\n}\n\n/* tp->lock is held. */\nstatic int tg3_load_tso_firmware(struct tg3 *tp)\n{\n\tstruct fw_info info;\n\tconst __be32 *fw_data;\n\tunsigned long cpu_base, cpu_scratch_base, cpu_scratch_size;\n\tint err, i;\n\n\tif (tg3_flag(tp, HW_TSO_1) ||\n\t    tg3_flag(tp, HW_TSO_2) ||\n\t    tg3_flag(tp, HW_TSO_3))\n\t\treturn 0;\n\n\tfw_data = (void *)tp->fw->data;\n\n\t/* Firmware blob starts with version numbers, followed by\n\t   start address and length. We are setting complete length.\n\t   length = end_address_of_bss - start_address_of_text.\n\t   Remainder is the blob to be loaded contiguously\n\t   from start address. */\n\n\tinfo.fw_base = be32_to_cpu(fw_data[1]);\n\tcpu_scratch_size = tp->fw_len;\n\tinfo.fw_len = tp->fw->size - 12;\n\tinfo.fw_data = &fw_data[3];\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\tcpu_base = RX_CPU_BASE;\n\t\tcpu_scratch_base = NIC_SRAM_MBUF_POOL_BASE5705;\n\t} else {\n\t\tcpu_base = TX_CPU_BASE;\n\t\tcpu_scratch_base = TX_CPU_SCRATCH_BASE;\n\t\tcpu_scratch_size = TX_CPU_SCRATCH_SIZE;\n\t}\n\n\terr = tg3_load_firmware_cpu(tp, cpu_base,\n\t\t\t\t    cpu_scratch_base, cpu_scratch_size,\n\t\t\t\t    &info);\n\tif (err)\n\t\treturn err;\n\n\t/* Now startup the cpu. */\n\ttw32(cpu_base + CPU_STATE, 0xffffffff);\n\ttw32_f(cpu_base + CPU_PC, info.fw_base);\n\n\tfor (i = 0; i < 5; i++) {\n\t\tif (tr32(cpu_base + CPU_PC) == info.fw_base)\n\t\t\tbreak;\n\t\ttw32(cpu_base + CPU_STATE, 0xffffffff);\n\t\ttw32(cpu_base + CPU_MODE,  CPU_MODE_HALT);\n\t\ttw32_f(cpu_base + CPU_PC, info.fw_base);\n\t\tudelay(1000);\n\t}\n\tif (i >= 5) {\n\t\tnetdev_err(tp->dev,\n\t\t\t   \"%s fails to set CPU PC, is %08x should be %08x\\n\",\n\t\t\t   __func__, tr32(cpu_base + CPU_PC), info.fw_base);\n\t\treturn -ENODEV;\n\t}\n\ttw32(cpu_base + CPU_STATE, 0xffffffff);\n\ttw32_f(cpu_base + CPU_MODE,  0x00000000);\n\treturn 0;\n}\n\n\n/* tp->lock is held. */\nstatic void __tg3_set_mac_addr(struct tg3 *tp, int skip_mac_1)\n{\n\tu32 addr_high, addr_low;\n\tint i;\n\n\taddr_high = ((tp->dev->dev_addr[0] << 8) |\n\t\t     tp->dev->dev_addr[1]);\n\taddr_low = ((tp->dev->dev_addr[2] << 24) |\n\t\t    (tp->dev->dev_addr[3] << 16) |\n\t\t    (tp->dev->dev_addr[4] <<  8) |\n\t\t    (tp->dev->dev_addr[5] <<  0));\n\tfor (i = 0; i < 4; i++) {\n\t\tif (i == 1 && skip_mac_1)\n\t\t\tcontinue;\n\t\ttw32(MAC_ADDR_0_HIGH + (i * 8), addr_high);\n\t\ttw32(MAC_ADDR_0_LOW + (i * 8), addr_low);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5704) {\n\t\tfor (i = 0; i < 12; i++) {\n\t\t\ttw32(MAC_EXTADDR_0_HIGH + (i * 8), addr_high);\n\t\t\ttw32(MAC_EXTADDR_0_LOW + (i * 8), addr_low);\n\t\t}\n\t}\n\n\taddr_high = (tp->dev->dev_addr[0] +\n\t\t     tp->dev->dev_addr[1] +\n\t\t     tp->dev->dev_addr[2] +\n\t\t     tp->dev->dev_addr[3] +\n\t\t     tp->dev->dev_addr[4] +\n\t\t     tp->dev->dev_addr[5]) &\n\t\tTX_BACKOFF_SEED_MASK;\n\ttw32(MAC_TX_BACKOFF_SEED, addr_high);\n}\n\nstatic void tg3_enable_register_access(struct tg3 *tp)\n{\n\t/*\n\t * Make sure register accesses (indirect or otherwise) will function\n\t * correctly.\n\t */\n\tpci_write_config_dword(tp->pdev,\n\t\t\t       TG3PCI_MISC_HOST_CTRL, tp->misc_host_ctrl);\n}\n\nstatic int tg3_power_up(struct tg3 *tp)\n{\n\tint err;\n\n\ttg3_enable_register_access(tp);\n\n\terr = pci_set_power_state(tp->pdev, PCI_D0);\n\tif (!err) {\n\t\t/* Switch out of Vaux if it is a NIC */\n\t\ttg3_pwrsrc_switch_to_vmain(tp);\n\t} else {\n\t\tnetdev_err(tp->dev, \"Transition to D0 failed\\n\");\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_setup_phy(struct tg3 *, int);\n\nstatic int tg3_power_down_prepare(struct tg3 *tp)\n{\n\tu32 misc_host_ctrl;\n\tbool device_should_wake, do_low_power;\n\n\ttg3_enable_register_access(tp);\n\n\t/* Restore the CLKREQ setting. */\n\tif (tg3_flag(tp, CLKREQ_BUG))\n\t\tpcie_capability_set_word(tp->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t PCI_EXP_LNKCTL_CLKREQ_EN);\n\n\tmisc_host_ctrl = tr32(TG3PCI_MISC_HOST_CTRL);\n\ttw32(TG3PCI_MISC_HOST_CTRL,\n\t     misc_host_ctrl | MISC_HOST_CTRL_MASK_PCI_INT);\n\n\tdevice_should_wake = device_may_wakeup(&tp->pdev->dev) &&\n\t\t\t     tg3_flag(tp, WOL_ENABLE);\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tdo_low_power = false;\n\t\tif ((tp->phy_flags & TG3_PHYFLG_IS_CONNECTED) &&\n\t\t    !(tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)) {\n\t\t\tstruct phy_device *phydev;\n\t\t\tu32 phyid, advertising;\n\n\t\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\t\t\ttp->phy_flags |= TG3_PHYFLG_IS_LOW_POWER;\n\n\t\t\ttp->link_config.speed = phydev->speed;\n\t\t\ttp->link_config.duplex = phydev->duplex;\n\t\t\ttp->link_config.autoneg = phydev->autoneg;\n\t\t\ttp->link_config.advertising = phydev->advertising;\n\n\t\t\tadvertising = ADVERTISED_TP |\n\t\t\t\t      ADVERTISED_Pause |\n\t\t\t\t      ADVERTISED_Autoneg |\n\t\t\t\t      ADVERTISED_10baseT_Half;\n\n\t\t\tif (tg3_flag(tp, ENABLE_ASF) || device_should_wake) {\n\t\t\t\tif (tg3_flag(tp, WOL_SPEED_100MB))\n\t\t\t\t\tadvertising |=\n\t\t\t\t\t\tADVERTISED_100baseT_Half |\n\t\t\t\t\t\tADVERTISED_100baseT_Full |\n\t\t\t\t\t\tADVERTISED_10baseT_Full;\n\t\t\t\telse\n\t\t\t\t\tadvertising |= ADVERTISED_10baseT_Full;\n\t\t\t}\n\n\t\t\tphydev->advertising = advertising;\n\n\t\t\tphy_start_aneg(phydev);\n\n\t\t\tphyid = phydev->drv->phy_id & phydev->drv->phy_id_mask;\n\t\t\tif (phyid != PHY_ID_BCMAC131) {\n\t\t\t\tphyid &= PHY_BCM_OUI_MASK;\n\t\t\t\tif (phyid == PHY_BCM_OUI_1 ||\n\t\t\t\t    phyid == PHY_BCM_OUI_2 ||\n\t\t\t\t    phyid == PHY_BCM_OUI_3)\n\t\t\t\t\tdo_low_power = true;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tdo_low_power = true;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER))\n\t\t\ttp->phy_flags |= TG3_PHYFLG_IS_LOW_POWER;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\t\ttg3_setup_phy(tp, 0);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tu32 val;\n\n\t\tval = tr32(GRC_VCPU_EXT_CTRL);\n\t\ttw32(GRC_VCPU_EXT_CTRL, val | GRC_VCPU_EXT_CTRL_DISABLE_WOL);\n\t} else if (!tg3_flag(tp, ENABLE_ASF)) {\n\t\tint i;\n\t\tu32 val;\n\n\t\tfor (i = 0; i < 200; i++) {\n\t\t\ttg3_read_mem(tp, NIC_SRAM_FW_ASF_STATUS_MBOX, &val);\n\t\t\tif (val == ~NIC_SRAM_FIRMWARE_MBOX_MAGIC1)\n\t\t\t\tbreak;\n\t\t\tmsleep(1);\n\t\t}\n\t}\n\tif (tg3_flag(tp, WOL_CAP))\n\t\ttg3_write_mem(tp, NIC_SRAM_WOL_MBOX, WOL_SIGNATURE |\n\t\t\t\t\t\t     WOL_DRV_STATE_SHUTDOWN |\n\t\t\t\t\t\t     WOL_DRV_WOL |\n\t\t\t\t\t\t     WOL_SET_MAGIC_PKT);\n\n\tif (device_should_wake) {\n\t\tu32 mac_mode;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_PHY_SERDES)) {\n\t\t\tif (do_low_power &&\n\t\t\t    !(tp->phy_flags & TG3_PHYFLG_IS_FET)) {\n\t\t\t\ttg3_phy_auxctl_write(tp,\n\t\t\t\t\t       MII_TG3_AUXCTL_SHDWSEL_PWRCTL,\n\t\t\t\t\t       MII_TG3_AUXCTL_PCTL_WOL_EN |\n\t\t\t\t\t       MII_TG3_AUXCTL_PCTL_100TX_LPWR |\n\t\t\t\t\t       MII_TG3_AUXCTL_PCTL_CL_AB_TXDAC);\n\t\t\t\tudelay(40);\n\t\t\t}\n\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_MII_SERDES)\n\t\t\t\tmac_mode = MAC_MODE_PORT_MODE_GMII;\n\t\t\telse\n\t\t\t\tmac_mode = MAC_MODE_PORT_MODE_MII;\n\n\t\t\tmac_mode |= tp->mac_mode & MAC_MODE_LINK_POLARITY;\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700) {\n\t\t\t\tu32 speed = tg3_flag(tp, WOL_SPEED_100MB) ?\n\t\t\t\t\t     SPEED_100 : SPEED_10;\n\t\t\t\tif (tg3_5700_link_polarity(tp, speed))\n\t\t\t\t\tmac_mode |= MAC_MODE_LINK_POLARITY;\n\t\t\t\telse\n\t\t\t\t\tmac_mode &= ~MAC_MODE_LINK_POLARITY;\n\t\t\t}\n\t\t} else {\n\t\t\tmac_mode = MAC_MODE_PORT_MODE_TBI;\n\t\t}\n\n\t\tif (!tg3_flag(tp, 5750_PLUS))\n\t\t\ttw32(MAC_LED_CTRL, tp->led_ctrl);\n\n\t\tmac_mode |= MAC_MODE_MAGIC_PKT_ENABLE;\n\t\tif ((tg3_flag(tp, 5705_PLUS) && !tg3_flag(tp, 5780_CLASS)) &&\n\t\t    (tg3_flag(tp, ENABLE_ASF) || tg3_flag(tp, ENABLE_APE)))\n\t\t\tmac_mode |= MAC_MODE_KEEP_FRAME_IN_WOL;\n\n\t\tif (tg3_flag(tp, ENABLE_APE))\n\t\t\tmac_mode |= MAC_MODE_APE_TX_EN |\n\t\t\t\t    MAC_MODE_APE_RX_EN |\n\t\t\t\t    MAC_MODE_TDE_ENABLE;\n\n\t\ttw32_f(MAC_MODE, mac_mode);\n\t\tudelay(100);\n\n\t\ttw32_f(MAC_RX_MODE, RX_MODE_ENABLE);\n\t\tudelay(10);\n\t}\n\n\tif (!tg3_flag(tp, WOL_SPEED_100MB) &&\n\t    (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5701)) {\n\t\tu32 base_val;\n\n\t\tbase_val = tp->pci_clock_ctrl;\n\t\tbase_val |= (CLOCK_CTRL_RXCLK_DISABLE |\n\t\t\t     CLOCK_CTRL_TXCLK_DISABLE);\n\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL, base_val | CLOCK_CTRL_ALTCLK |\n\t\t\t    CLOCK_CTRL_PWRDOWN_PLL133, 40);\n\t} else if (tg3_flag(tp, 5780_CLASS) ||\n\t\t   tg3_flag(tp, CPMU_PRESENT) ||\n\t\t   tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t/* do nothing */\n\t} else if (!(tg3_flag(tp, 5750_PLUS) && tg3_flag(tp, ENABLE_ASF))) {\n\t\tu32 newbits1, newbits2;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\t\tnewbits1 = (CLOCK_CTRL_RXCLK_DISABLE |\n\t\t\t\t    CLOCK_CTRL_TXCLK_DISABLE |\n\t\t\t\t    CLOCK_CTRL_ALTCLK);\n\t\t\tnewbits2 = newbits1 | CLOCK_CTRL_44MHZ_CORE;\n\t\t} else if (tg3_flag(tp, 5705_PLUS)) {\n\t\t\tnewbits1 = CLOCK_CTRL_625_CORE;\n\t\t\tnewbits2 = newbits1 | CLOCK_CTRL_ALTCLK;\n\t\t} else {\n\t\t\tnewbits1 = CLOCK_CTRL_ALTCLK;\n\t\t\tnewbits2 = newbits1 | CLOCK_CTRL_44MHZ_CORE;\n\t\t}\n\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL, tp->pci_clock_ctrl | newbits1,\n\t\t\t    40);\n\n\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL, tp->pci_clock_ctrl | newbits2,\n\t\t\t    40);\n\n\t\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\t\tu32 newbits3;\n\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\t\t\tnewbits3 = (CLOCK_CTRL_RXCLK_DISABLE |\n\t\t\t\t\t    CLOCK_CTRL_TXCLK_DISABLE |\n\t\t\t\t\t    CLOCK_CTRL_44MHZ_CORE);\n\t\t\t} else {\n\t\t\t\tnewbits3 = CLOCK_CTRL_44MHZ_CORE;\n\t\t\t}\n\n\t\t\ttw32_wait_f(TG3PCI_CLOCK_CTRL,\n\t\t\t\t    tp->pci_clock_ctrl | newbits3, 40);\n\t\t}\n\t}\n\n\tif (!(device_should_wake) && !tg3_flag(tp, ENABLE_ASF))\n\t\ttg3_power_down_phy(tp, do_low_power);\n\n\ttg3_frob_aux_power(tp, true);\n\n\t/* Workaround for unstable PLL clock */\n\tif ((!tg3_flag(tp, IS_SSB_CORE)) &&\n\t    ((tg3_chip_rev(tp) == CHIPREV_5750_AX) ||\n\t     (tg3_chip_rev(tp) == CHIPREV_5750_BX))) {\n\t\tu32 val = tr32(0x7d00);\n\n\t\tval &= ~((1 << 16) | (1 << 4) | (1 << 2) | (1 << 1) | 1);\n\t\ttw32(0x7d00, val);\n\t\tif (!tg3_flag(tp, ENABLE_ASF)) {\n\t\t\tint err;\n\n\t\t\terr = tg3_nvram_lock(tp);\n\t\t\ttg3_halt_cpu(tp, RX_CPU_BASE);\n\t\t\tif (!err)\n\t\t\t\ttg3_nvram_unlock(tp);\n\t\t}\n\t}\n\n\ttg3_write_sig_post_reset(tp, RESET_KIND_SHUTDOWN);\n\n\treturn 0;\n}\n\nstatic void tg3_power_down(struct tg3 *tp)\n{\n\ttg3_power_down_prepare(tp);\n\n\tpci_wake_from_d3(tp->pdev, tg3_flag(tp, WOL_ENABLE));\n\tpci_set_power_state(tp->pdev, PCI_D3hot);\n}\n\nstatic void tg3_aux_stat_to_speed_duplex(struct tg3 *tp, u32 val, u16 *speed, u8 *duplex)\n{\n\tswitch (val & MII_TG3_AUX_STAT_SPDMASK) {\n\tcase MII_TG3_AUX_STAT_10HALF:\n\t\t*speed = SPEED_10;\n\t\t*duplex = DUPLEX_HALF;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_10FULL:\n\t\t*speed = SPEED_10;\n\t\t*duplex = DUPLEX_FULL;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_100HALF:\n\t\t*speed = SPEED_100;\n\t\t*duplex = DUPLEX_HALF;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_100FULL:\n\t\t*speed = SPEED_100;\n\t\t*duplex = DUPLEX_FULL;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_1000HALF:\n\t\t*speed = SPEED_1000;\n\t\t*duplex = DUPLEX_HALF;\n\t\tbreak;\n\n\tcase MII_TG3_AUX_STAT_1000FULL:\n\t\t*speed = SPEED_1000;\n\t\t*duplex = DUPLEX_FULL;\n\t\tbreak;\n\n\tdefault:\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\t\t*speed = (val & MII_TG3_AUX_STAT_100) ? SPEED_100 :\n\t\t\t\t SPEED_10;\n\t\t\t*duplex = (val & MII_TG3_AUX_STAT_FULL) ? DUPLEX_FULL :\n\t\t\t\t  DUPLEX_HALF;\n\t\t\tbreak;\n\t\t}\n\t\t*speed = SPEED_UNKNOWN;\n\t\t*duplex = DUPLEX_UNKNOWN;\n\t\tbreak;\n\t}\n}\n\nstatic int tg3_phy_autoneg_cfg(struct tg3 *tp, u32 advertise, u32 flowctrl)\n{\n\tint err = 0;\n\tu32 val, new_adv;\n\n\tnew_adv = ADVERTISE_CSMA;\n\tnew_adv |= ethtool_adv_to_mii_adv_t(advertise) & ADVERTISE_ALL;\n\tnew_adv |= mii_advertise_flowctrl(flowctrl);\n\n\terr = tg3_writephy(tp, MII_ADVERTISE, new_adv);\n\tif (err)\n\t\tgoto done;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY)) {\n\t\tnew_adv = ethtool_adv_to_mii_ctrl1000_t(advertise);\n\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0)\n\t\t\tnew_adv |= CTL1000_AS_MASTER | CTL1000_ENABLE_MASTER;\n\n\t\terr = tg3_writephy(tp, MII_CTRL1000, new_adv);\n\t\tif (err)\n\t\t\tgoto done;\n\t}\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_EEE_CAP))\n\t\tgoto done;\n\n\ttw32(TG3_CPMU_EEE_MODE,\n\t     tr32(TG3_CPMU_EEE_MODE) & ~TG3_CPMU_EEEMD_LPI_ENABLE);\n\n\terr = tg3_phy_toggle_auxctl_smdsp(tp, true);\n\tif (!err) {\n\t\tu32 err2;\n\n\t\tval = 0;\n\t\t/* Advertise 100-BaseTX EEE ability */\n\t\tif (advertise & ADVERTISED_100baseT_Full)\n\t\t\tval |= MDIO_AN_EEE_ADV_100TX;\n\t\t/* Advertise 1000-BaseT EEE ability */\n\t\tif (advertise & ADVERTISED_1000baseT_Full)\n\t\t\tval |= MDIO_AN_EEE_ADV_1000T;\n\t\terr = tg3_phy_cl45_write(tp, MDIO_MMD_AN, MDIO_AN_EEE_ADV, val);\n\t\tif (err)\n\t\t\tval = 0;\n\n\t\tswitch (tg3_asic_rev(tp)) {\n\t\tcase ASIC_REV_5717:\n\t\tcase ASIC_REV_57765:\n\t\tcase ASIC_REV_57766:\n\t\tcase ASIC_REV_5719:\n\t\t\t/* If we advertised any eee advertisements above... */\n\t\t\tif (val)\n\t\t\t\tval = MII_TG3_DSP_TAP26_ALNOKO |\n\t\t\t\t      MII_TG3_DSP_TAP26_RMRXSTO |\n\t\t\t\t      MII_TG3_DSP_TAP26_OPCSINPT;\n\t\t\ttg3_phydsp_write(tp, MII_TG3_DSP_TAP26, val);\n\t\t\t/* Fall through */\n\t\tcase ASIC_REV_5720:\n\t\tcase ASIC_REV_5762:\n\t\t\tif (!tg3_phydsp_read(tp, MII_TG3_DSP_CH34TP2, &val))\n\t\t\t\ttg3_phydsp_write(tp, MII_TG3_DSP_CH34TP2, val |\n\t\t\t\t\t\t MII_TG3_DSP_CH34TP2_HIBW01);\n\t\t}\n\n\t\terr2 = tg3_phy_toggle_auxctl_smdsp(tp, false);\n\t\tif (!err)\n\t\t\terr = err2;\n\t}\n\ndone:\n\treturn err;\n}\n\nstatic void tg3_phy_copper_begin(struct tg3 *tp)\n{\n\tif (tp->link_config.autoneg == AUTONEG_ENABLE ||\n\t    (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)) {\n\t\tu32 adv, fc;\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER) {\n\t\t\tadv = ADVERTISED_10baseT_Half |\n\t\t\t      ADVERTISED_10baseT_Full;\n\t\t\tif (tg3_flag(tp, WOL_SPEED_100MB))\n\t\t\t\tadv |= ADVERTISED_100baseT_Half |\n\t\t\t\t       ADVERTISED_100baseT_Full;\n\n\t\t\tfc = FLOW_CTRL_TX | FLOW_CTRL_RX;\n\t\t} else {\n\t\t\tadv = tp->link_config.advertising;\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_10_100_ONLY)\n\t\t\t\tadv &= ~(ADVERTISED_1000baseT_Half |\n\t\t\t\t\t ADVERTISED_1000baseT_Full);\n\n\t\t\tfc = tp->link_config.flowctrl;\n\t\t}\n\n\t\ttg3_phy_autoneg_cfg(tp, adv, fc);\n\n\t\ttg3_writephy(tp, MII_BMCR,\n\t\t\t     BMCR_ANENABLE | BMCR_ANRESTART);\n\t} else {\n\t\tint i;\n\t\tu32 bmcr, orig_bmcr;\n\n\t\ttp->link_config.active_speed = tp->link_config.speed;\n\t\ttp->link_config.active_duplex = tp->link_config.duplex;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\t\t/* With autoneg disabled, 5715 only links up when the\n\t\t\t * advertisement register has the configured speed\n\t\t\t * enabled.\n\t\t\t */\n\t\t\ttg3_writephy(tp, MII_ADVERTISE, ADVERTISE_ALL);\n\t\t}\n\n\t\tbmcr = 0;\n\t\tswitch (tp->link_config.speed) {\n\t\tdefault:\n\t\tcase SPEED_10:\n\t\t\tbreak;\n\n\t\tcase SPEED_100:\n\t\t\tbmcr |= BMCR_SPEED100;\n\t\t\tbreak;\n\n\t\tcase SPEED_1000:\n\t\t\tbmcr |= BMCR_SPEED1000;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tp->link_config.duplex == DUPLEX_FULL)\n\t\t\tbmcr |= BMCR_FULLDPLX;\n\n\t\tif (!tg3_readphy(tp, MII_BMCR, &orig_bmcr) &&\n\t\t    (bmcr != orig_bmcr)) {\n\t\t\ttg3_writephy(tp, MII_BMCR, BMCR_LOOPBACK);\n\t\t\tfor (i = 0; i < 1500; i++) {\n\t\t\t\tu32 tmp;\n\n\t\t\t\tudelay(10);\n\t\t\t\tif (tg3_readphy(tp, MII_BMSR, &tmp) ||\n\t\t\t\t    tg3_readphy(tp, MII_BMSR, &tmp))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!(tmp & BMSR_LSTATUS)) {\n\t\t\t\t\tudelay(40);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\ttg3_writephy(tp, MII_BMCR, bmcr);\n\t\t\tudelay(40);\n\t\t}\n\t}\n}\n\nstatic int tg3_init_5401phy_dsp(struct tg3 *tp)\n{\n\tint err;\n\n\t/* Turn off tap power management. */\n\t/* Set Extended packet length bit */\n\terr = tg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_AUXCTL, 0x4c20);\n\n\terr |= tg3_phydsp_write(tp, 0x0012, 0x1804);\n\terr |= tg3_phydsp_write(tp, 0x0013, 0x1204);\n\terr |= tg3_phydsp_write(tp, 0x8006, 0x0132);\n\terr |= tg3_phydsp_write(tp, 0x8006, 0x0232);\n\terr |= tg3_phydsp_write(tp, 0x201f, 0x0a20);\n\n\tudelay(40);\n\n\treturn err;\n}\n\nstatic bool tg3_phy_copper_an_config_ok(struct tg3 *tp, u32 *lcladv)\n{\n\tu32 advmsk, tgtadv, advertising;\n\n\tadvertising = tp->link_config.advertising;\n\ttgtadv = ethtool_adv_to_mii_adv_t(advertising) & ADVERTISE_ALL;\n\n\tadvmsk = ADVERTISE_ALL;\n\tif (tp->link_config.active_duplex == DUPLEX_FULL) {\n\t\ttgtadv |= mii_advertise_flowctrl(tp->link_config.flowctrl);\n\t\tadvmsk |= ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\n\t}\n\n\tif (tg3_readphy(tp, MII_ADVERTISE, lcladv))\n\t\treturn false;\n\n\tif ((*lcladv & advmsk) != tgtadv)\n\t\treturn false;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY)) {\n\t\tu32 tg3_ctrl;\n\n\t\ttgtadv = ethtool_adv_to_mii_ctrl1000_t(advertising);\n\n\t\tif (tg3_readphy(tp, MII_CTRL1000, &tg3_ctrl))\n\t\t\treturn false;\n\n\t\tif (tgtadv &&\n\t\t    (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t\t     tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0)) {\n\t\t\ttgtadv |= CTL1000_AS_MASTER | CTL1000_ENABLE_MASTER;\n\t\t\ttg3_ctrl &= (ADVERTISE_1000HALF | ADVERTISE_1000FULL |\n\t\t\t\t     CTL1000_AS_MASTER | CTL1000_ENABLE_MASTER);\n\t\t} else {\n\t\t\ttg3_ctrl &= (ADVERTISE_1000HALF | ADVERTISE_1000FULL);\n\t\t}\n\n\t\tif (tg3_ctrl != tgtadv)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool tg3_phy_copper_fetch_rmtadv(struct tg3 *tp, u32 *rmtadv)\n{\n\tu32 lpeth = 0;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY)) {\n\t\tu32 val;\n\n\t\tif (tg3_readphy(tp, MII_STAT1000, &val))\n\t\t\treturn false;\n\n\t\tlpeth = mii_stat1000_to_ethtool_lpa_t(val);\n\t}\n\n\tif (tg3_readphy(tp, MII_LPA, rmtadv))\n\t\treturn false;\n\n\tlpeth |= mii_lpa_to_ethtool_lpa_t(*rmtadv);\n\ttp->link_config.rmt_adv = lpeth;\n\n\treturn true;\n}\n\nstatic bool tg3_test_and_report_link_chg(struct tg3 *tp, int curr_link_up)\n{\n\tif (curr_link_up != tp->link_up) {\n\t\tif (curr_link_up) {\n\t\t\tnetif_carrier_on(tp->dev);\n\t\t} else {\n\t\t\tnetif_carrier_off(tp->dev);\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_MII_SERDES)\n\t\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t\t}\n\n\t\ttg3_link_report(tp);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int tg3_setup_copper_phy(struct tg3 *tp, int force_reset)\n{\n\tint current_link_up;\n\tu32 bmsr, val;\n\tu32 lcl_adv, rmt_adv;\n\tu16 current_speed;\n\tu8 current_duplex;\n\tint i, err;\n\n\ttw32(MAC_EVENT, 0);\n\n\ttw32_f(MAC_STATUS,\n\t     (MAC_STATUS_SYNC_CHANGED |\n\t      MAC_STATUS_CFG_CHANGED |\n\t      MAC_STATUS_MI_COMPLETION |\n\t      MAC_STATUS_LNKSTATE_CHANGED));\n\tudelay(40);\n\n\tif ((tp->mi_mode & MAC_MI_MODE_AUTO_POLL) != 0) {\n\t\ttw32_f(MAC_MI_MODE,\n\t\t     (tp->mi_mode & ~MAC_MI_MODE_AUTO_POLL));\n\t\tudelay(80);\n\t}\n\n\ttg3_phy_auxctl_write(tp, MII_TG3_AUXCTL_SHDWSEL_PWRCTL, 0);\n\n\t/* Some third-party PHYs need to be reset on link going\n\t * down.\n\t */\n\tif ((tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5705) &&\n\t    tp->link_up) {\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif (!tg3_readphy(tp, MII_BMSR, &bmsr) &&\n\t\t    !(bmsr & BMSR_LSTATUS))\n\t\t\tforce_reset = 1;\n\t}\n\tif (force_reset)\n\t\ttg3_phy_reset(tp);\n\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5401) {\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif (tg3_readphy(tp, MII_BMSR, &bmsr) ||\n\t\t    !tg3_flag(tp, INIT_COMPLETE))\n\t\t\tbmsr = 0;\n\n\t\tif (!(bmsr & BMSR_LSTATUS)) {\n\t\t\terr = tg3_init_5401phy_dsp(tp);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\t\tfor (i = 0; i < 1000; i++) {\n\t\t\t\tudelay(10);\n\t\t\t\tif (!tg3_readphy(tp, MII_BMSR, &bmsr) &&\n\t\t\t\t    (bmsr & BMSR_LSTATUS)) {\n\t\t\t\t\tudelay(40);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif ((tp->phy_id & TG3_PHY_ID_REV_MASK) ==\n\t\t\t    TG3_PHY_REV_BCM5401_B0 &&\n\t\t\t    !(bmsr & BMSR_LSTATUS) &&\n\t\t\t    tp->link_config.active_speed == SPEED_1000) {\n\t\t\t\terr = tg3_phy_reset(tp);\n\t\t\t\tif (!err)\n\t\t\t\t\terr = tg3_init_5401phy_dsp(tp);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t} else if (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t\t   tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0) {\n\t\t/* 5701 {A0,B0} CRC bug workaround */\n\t\ttg3_writephy(tp, 0x15, 0x0a75);\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8c68);\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8d68);\n\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x8c68);\n\t}\n\n\t/* Clear pending interrupts... */\n\ttg3_readphy(tp, MII_TG3_ISTAT, &val);\n\ttg3_readphy(tp, MII_TG3_ISTAT, &val);\n\n\tif (tp->phy_flags & TG3_PHYFLG_USE_MI_INTERRUPT)\n\t\ttg3_writephy(tp, MII_TG3_IMASK, ~MII_TG3_INT_LINKCHG);\n\telse if (!(tp->phy_flags & TG3_PHYFLG_IS_FET))\n\t\ttg3_writephy(tp, MII_TG3_IMASK, ~0);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\tif (tp->led_ctrl == LED_CTRL_MODE_PHY_1)\n\t\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL,\n\t\t\t\t     MII_TG3_EXT_CTRL_LNK3_LED_MODE);\n\t\telse\n\t\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL, 0);\n\t}\n\n\tcurrent_link_up = 0;\n\tcurrent_speed = SPEED_UNKNOWN;\n\tcurrent_duplex = DUPLEX_UNKNOWN;\n\ttp->phy_flags &= ~TG3_PHYFLG_MDIX_STATE;\n\ttp->link_config.rmt_adv = 0;\n\n\tif (tp->phy_flags & TG3_PHYFLG_CAPACITIVE_COUPLING) {\n\t\terr = tg3_phy_auxctl_read(tp,\n\t\t\t\t\t  MII_TG3_AUXCTL_SHDWSEL_MISCTEST,\n\t\t\t\t\t  &val);\n\t\tif (!err && !(val & (1 << 10))) {\n\t\t\ttg3_phy_auxctl_write(tp,\n\t\t\t\t\t     MII_TG3_AUXCTL_SHDWSEL_MISCTEST,\n\t\t\t\t\t     val | (1 << 10));\n\t\t\tgoto relink;\n\t\t}\n\t}\n\n\tbmsr = 0;\n\tfor (i = 0; i < 100; i++) {\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif (!tg3_readphy(tp, MII_BMSR, &bmsr) &&\n\t\t    (bmsr & BMSR_LSTATUS))\n\t\t\tbreak;\n\t\tudelay(40);\n\t}\n\n\tif (bmsr & BMSR_LSTATUS) {\n\t\tu32 aux_stat, bmcr;\n\n\t\ttg3_readphy(tp, MII_TG3_AUX_STAT, &aux_stat);\n\t\tfor (i = 0; i < 2000; i++) {\n\t\t\tudelay(10);\n\t\t\tif (!tg3_readphy(tp, MII_TG3_AUX_STAT, &aux_stat) &&\n\t\t\t    aux_stat)\n\t\t\t\tbreak;\n\t\t}\n\n\t\ttg3_aux_stat_to_speed_duplex(tp, aux_stat,\n\t\t\t\t\t     &current_speed,\n\t\t\t\t\t     &current_duplex);\n\n\t\tbmcr = 0;\n\t\tfor (i = 0; i < 200; i++) {\n\t\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\t\t\tif (tg3_readphy(tp, MII_BMCR, &bmcr))\n\t\t\t\tcontinue;\n\t\t\tif (bmcr && bmcr != 0x7fff)\n\t\t\t\tbreak;\n\t\t\tudelay(10);\n\t\t}\n\n\t\tlcl_adv = 0;\n\t\trmt_adv = 0;\n\n\t\ttp->link_config.active_speed = current_speed;\n\t\ttp->link_config.active_duplex = current_duplex;\n\n\t\tif (tp->link_config.autoneg == AUTONEG_ENABLE) {\n\t\t\tif ((bmcr & BMCR_ANENABLE) &&\n\t\t\t    tg3_phy_copper_an_config_ok(tp, &lcl_adv) &&\n\t\t\t    tg3_phy_copper_fetch_rmtadv(tp, &rmt_adv))\n\t\t\t\tcurrent_link_up = 1;\n\t\t} else {\n\t\t\tif (!(bmcr & BMCR_ANENABLE) &&\n\t\t\t    tp->link_config.speed == current_speed &&\n\t\t\t    tp->link_config.duplex == current_duplex &&\n\t\t\t    tp->link_config.flowctrl ==\n\t\t\t    tp->link_config.active_flowctrl) {\n\t\t\t\tcurrent_link_up = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (current_link_up == 1 &&\n\t\t    tp->link_config.active_duplex == DUPLEX_FULL) {\n\t\t\tu32 reg, bit;\n\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\t\t\treg = MII_TG3_FET_GEN_STAT;\n\t\t\t\tbit = MII_TG3_FET_GEN_STAT_MDIXSTAT;\n\t\t\t} else {\n\t\t\t\treg = MII_TG3_EXT_STAT;\n\t\t\t\tbit = MII_TG3_EXT_STAT_MDIX;\n\t\t\t}\n\n\t\t\tif (!tg3_readphy(tp, reg, &val) && (val & bit))\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_MDIX_STATE;\n\n\t\t\ttg3_setup_flow_control(tp, lcl_adv, rmt_adv);\n\t\t}\n\t}\n\nrelink:\n\tif (current_link_up == 0 || (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)) {\n\t\ttg3_phy_copper_begin(tp);\n\n\t\tif (tg3_flag(tp, ROBOSWITCH)) {\n\t\t\tcurrent_link_up = 1;\n\t\t\t/* FIXME: when BCM5325 switch is used use 100 MBit/s */\n\t\t\tcurrent_speed = SPEED_1000;\n\t\t\tcurrent_duplex = DUPLEX_FULL;\n\t\t\ttp->link_config.active_speed = current_speed;\n\t\t\ttp->link_config.active_duplex = current_duplex;\n\t\t}\n\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif ((!tg3_readphy(tp, MII_BMSR, &bmsr) && (bmsr & BMSR_LSTATUS)) ||\n\t\t    (tp->mac_mode & MAC_MODE_PORT_INT_LPBACK))\n\t\t\tcurrent_link_up = 1;\n\t}\n\n\ttp->mac_mode &= ~MAC_MODE_PORT_MODE_MASK;\n\tif (current_link_up == 1) {\n\t\tif (tp->link_config.active_speed == SPEED_100 ||\n\t\t    tp->link_config.active_speed == SPEED_10)\n\t\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_MII;\n\t\telse\n\t\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_GMII;\n\t} else if (tp->phy_flags & TG3_PHYFLG_IS_FET)\n\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_MII;\n\telse\n\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_GMII;\n\n\t/* In order for the 5750 core in BCM4785 chip to work properly\n\t * in RGMII mode, the Led Control Register must be set up.\n\t */\n\tif (tg3_flag(tp, RGMII_MODE)) {\n\t\tu32 led_ctrl = tr32(MAC_LED_CTRL);\n\t\tled_ctrl &= ~(LED_CTRL_1000MBPS_ON | LED_CTRL_100MBPS_ON);\n\n\t\tif (tp->link_config.active_speed == SPEED_10)\n\t\t\tled_ctrl |= LED_CTRL_LNKLED_OVERRIDE;\n\t\telse if (tp->link_config.active_speed == SPEED_100)\n\t\t\tled_ctrl |= (LED_CTRL_LNKLED_OVERRIDE |\n\t\t\t\t     LED_CTRL_100MBPS_ON);\n\t\telse if (tp->link_config.active_speed == SPEED_1000)\n\t\t\tled_ctrl |= (LED_CTRL_LNKLED_OVERRIDE |\n\t\t\t\t     LED_CTRL_1000MBPS_ON);\n\n\t\ttw32(MAC_LED_CTRL, led_ctrl);\n\t\tudelay(40);\n\t}\n\n\ttp->mac_mode &= ~MAC_MODE_HALF_DUPLEX;\n\tif (tp->link_config.active_duplex == DUPLEX_HALF)\n\t\ttp->mac_mode |= MAC_MODE_HALF_DUPLEX;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700) {\n\t\tif (current_link_up == 1 &&\n\t\t    tg3_5700_link_polarity(tp, tp->link_config.active_speed))\n\t\t\ttp->mac_mode |= MAC_MODE_LINK_POLARITY;\n\t\telse\n\t\t\ttp->mac_mode &= ~MAC_MODE_LINK_POLARITY;\n\t}\n\n\t/* ??? Without this setting Netgear GA302T PHY does not\n\t * ??? send/receive packets...\n\t */\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5411 &&\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5700_ALTIMA) {\n\t\ttp->mi_mode |= MAC_MI_MODE_AUTO_POLL;\n\t\ttw32_f(MAC_MI_MODE, tp->mi_mode);\n\t\tudelay(80);\n\t}\n\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\ttg3_phy_eee_adjust(tp, current_link_up);\n\n\tif (tg3_flag(tp, USE_LINKCHG_REG)) {\n\t\t/* Polled via timer. */\n\t\ttw32_f(MAC_EVENT, 0);\n\t} else {\n\t\ttw32_f(MAC_EVENT, MAC_EVENT_LNKSTATE_CHANGED);\n\t}\n\tudelay(40);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 &&\n\t    current_link_up == 1 &&\n\t    tp->link_config.active_speed == SPEED_1000 &&\n\t    (tg3_flag(tp, PCIX_MODE) || tg3_flag(tp, PCI_HIGH_SPEED))) {\n\t\tudelay(120);\n\t\ttw32_f(MAC_STATUS,\n\t\t     (MAC_STATUS_SYNC_CHANGED |\n\t\t      MAC_STATUS_CFG_CHANGED));\n\t\tudelay(40);\n\t\ttg3_write_mem(tp,\n\t\t\t      NIC_SRAM_FIRMWARE_MBOX,\n\t\t\t      NIC_SRAM_FIRMWARE_MBOX_MAGIC2);\n\t}\n\n\t/* Prevent send BD corruption. */\n\tif (tg3_flag(tp, CLKREQ_BUG)) {\n\t\tif (tp->link_config.active_speed == SPEED_100 ||\n\t\t    tp->link_config.active_speed == SPEED_10)\n\t\t\tpcie_capability_clear_word(tp->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t   PCI_EXP_LNKCTL_CLKREQ_EN);\n\t\telse\n\t\t\tpcie_capability_set_word(tp->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t PCI_EXP_LNKCTL_CLKREQ_EN);\n\t}\n\n\ttg3_test_and_report_link_chg(tp, current_link_up);\n\n\treturn 0;\n}\n\nstruct tg3_fiber_aneginfo {\n\tint state;\n#define ANEG_STATE_UNKNOWN\t\t0\n#define ANEG_STATE_AN_ENABLE\t\t1\n#define ANEG_STATE_RESTART_INIT\t\t2\n#define ANEG_STATE_RESTART\t\t3\n#define ANEG_STATE_DISABLE_LINK_OK\t4\n#define ANEG_STATE_ABILITY_DETECT_INIT\t5\n#define ANEG_STATE_ABILITY_DETECT\t6\n#define ANEG_STATE_ACK_DETECT_INIT\t7\n#define ANEG_STATE_ACK_DETECT\t\t8\n#define ANEG_STATE_COMPLETE_ACK_INIT\t9\n#define ANEG_STATE_COMPLETE_ACK\t\t10\n#define ANEG_STATE_IDLE_DETECT_INIT\t11\n#define ANEG_STATE_IDLE_DETECT\t\t12\n#define ANEG_STATE_LINK_OK\t\t13\n#define ANEG_STATE_NEXT_PAGE_WAIT_INIT\t14\n#define ANEG_STATE_NEXT_PAGE_WAIT\t15\n\n\tu32 flags;\n#define MR_AN_ENABLE\t\t0x00000001\n#define MR_RESTART_AN\t\t0x00000002\n#define MR_AN_COMPLETE\t\t0x00000004\n#define MR_PAGE_RX\t\t0x00000008\n#define MR_NP_LOADED\t\t0x00000010\n#define MR_TOGGLE_TX\t\t0x00000020\n#define MR_LP_ADV_FULL_DUPLEX\t0x00000040\n#define MR_LP_ADV_HALF_DUPLEX\t0x00000080\n#define MR_LP_ADV_SYM_PAUSE\t0x00000100\n#define MR_LP_ADV_ASYM_PAUSE\t0x00000200\n#define MR_LP_ADV_REMOTE_FAULT1\t0x00000400\n#define MR_LP_ADV_REMOTE_FAULT2\t0x00000800\n#define MR_LP_ADV_NEXT_PAGE\t0x00001000\n#define MR_TOGGLE_RX\t\t0x00002000\n#define MR_NP_RX\t\t0x00004000\n\n#define MR_LINK_OK\t\t0x80000000\n\n\tunsigned long link_time, cur_time;\n\n\tu32 ability_match_cfg;\n\tint ability_match_count;\n\n\tchar ability_match, idle_match, ack_match;\n\n\tu32 txconfig, rxconfig;\n#define ANEG_CFG_NP\t\t0x00000080\n#define ANEG_CFG_ACK\t\t0x00000040\n#define ANEG_CFG_RF2\t\t0x00000020\n#define ANEG_CFG_RF1\t\t0x00000010\n#define ANEG_CFG_PS2\t\t0x00000001\n#define ANEG_CFG_PS1\t\t0x00008000\n#define ANEG_CFG_HD\t\t0x00004000\n#define ANEG_CFG_FD\t\t0x00002000\n#define ANEG_CFG_INVAL\t\t0x00001f06\n\n};\n#define ANEG_OK\t\t0\n#define ANEG_DONE\t1\n#define ANEG_TIMER_ENAB\t2\n#define ANEG_FAILED\t-1\n\n#define ANEG_STATE_SETTLE_TIME\t10000\n\nstatic int tg3_fiber_aneg_smachine(struct tg3 *tp,\n\t\t\t\t   struct tg3_fiber_aneginfo *ap)\n{\n\tu16 flowctrl;\n\tunsigned long delta;\n\tu32 rx_cfg_reg;\n\tint ret;\n\n\tif (ap->state == ANEG_STATE_UNKNOWN) {\n\t\tap->rxconfig = 0;\n\t\tap->link_time = 0;\n\t\tap->cur_time = 0;\n\t\tap->ability_match_cfg = 0;\n\t\tap->ability_match_count = 0;\n\t\tap->ability_match = 0;\n\t\tap->idle_match = 0;\n\t\tap->ack_match = 0;\n\t}\n\tap->cur_time++;\n\n\tif (tr32(MAC_STATUS) & MAC_STATUS_RCVD_CFG) {\n\t\trx_cfg_reg = tr32(MAC_RX_AUTO_NEG);\n\n\t\tif (rx_cfg_reg != ap->ability_match_cfg) {\n\t\t\tap->ability_match_cfg = rx_cfg_reg;\n\t\t\tap->ability_match = 0;\n\t\t\tap->ability_match_count = 0;\n\t\t} else {\n\t\t\tif (++ap->ability_match_count > 1) {\n\t\t\t\tap->ability_match = 1;\n\t\t\t\tap->ability_match_cfg = rx_cfg_reg;\n\t\t\t}\n\t\t}\n\t\tif (rx_cfg_reg & ANEG_CFG_ACK)\n\t\t\tap->ack_match = 1;\n\t\telse\n\t\t\tap->ack_match = 0;\n\n\t\tap->idle_match = 0;\n\t} else {\n\t\tap->idle_match = 1;\n\t\tap->ability_match_cfg = 0;\n\t\tap->ability_match_count = 0;\n\t\tap->ability_match = 0;\n\t\tap->ack_match = 0;\n\n\t\trx_cfg_reg = 0;\n\t}\n\n\tap->rxconfig = rx_cfg_reg;\n\tret = ANEG_OK;\n\n\tswitch (ap->state) {\n\tcase ANEG_STATE_UNKNOWN:\n\t\tif (ap->flags & (MR_AN_ENABLE | MR_RESTART_AN))\n\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\n\t\t/* fallthru */\n\tcase ANEG_STATE_AN_ENABLE:\n\t\tap->flags &= ~(MR_AN_COMPLETE | MR_PAGE_RX);\n\t\tif (ap->flags & MR_AN_ENABLE) {\n\t\t\tap->link_time = 0;\n\t\t\tap->cur_time = 0;\n\t\t\tap->ability_match_cfg = 0;\n\t\t\tap->ability_match_count = 0;\n\t\t\tap->ability_match = 0;\n\t\t\tap->idle_match = 0;\n\t\t\tap->ack_match = 0;\n\n\t\t\tap->state = ANEG_STATE_RESTART_INIT;\n\t\t} else {\n\t\t\tap->state = ANEG_STATE_DISABLE_LINK_OK;\n\t\t}\n\t\tbreak;\n\n\tcase ANEG_STATE_RESTART_INIT:\n\t\tap->link_time = ap->cur_time;\n\t\tap->flags &= ~(MR_NP_LOADED);\n\t\tap->txconfig = 0;\n\t\ttw32(MAC_TX_AUTO_NEG, 0);\n\t\ttp->mac_mode |= MAC_MODE_SEND_CONFIGS;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\n\t\tret = ANEG_TIMER_ENAB;\n\t\tap->state = ANEG_STATE_RESTART;\n\n\t\t/* fallthru */\n\tcase ANEG_STATE_RESTART:\n\t\tdelta = ap->cur_time - ap->link_time;\n\t\tif (delta > ANEG_STATE_SETTLE_TIME)\n\t\t\tap->state = ANEG_STATE_ABILITY_DETECT_INIT;\n\t\telse\n\t\t\tret = ANEG_TIMER_ENAB;\n\t\tbreak;\n\n\tcase ANEG_STATE_DISABLE_LINK_OK:\n\t\tret = ANEG_DONE;\n\t\tbreak;\n\n\tcase ANEG_STATE_ABILITY_DETECT_INIT:\n\t\tap->flags &= ~(MR_TOGGLE_TX);\n\t\tap->txconfig = ANEG_CFG_FD;\n\t\tflowctrl = tg3_advert_flowctrl_1000X(tp->link_config.flowctrl);\n\t\tif (flowctrl & ADVERTISE_1000XPAUSE)\n\t\t\tap->txconfig |= ANEG_CFG_PS1;\n\t\tif (flowctrl & ADVERTISE_1000XPSE_ASYM)\n\t\t\tap->txconfig |= ANEG_CFG_PS2;\n\t\ttw32(MAC_TX_AUTO_NEG, ap->txconfig);\n\t\ttp->mac_mode |= MAC_MODE_SEND_CONFIGS;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\n\t\tap->state = ANEG_STATE_ABILITY_DETECT;\n\t\tbreak;\n\n\tcase ANEG_STATE_ABILITY_DETECT:\n\t\tif (ap->ability_match != 0 && ap->rxconfig != 0)\n\t\t\tap->state = ANEG_STATE_ACK_DETECT_INIT;\n\t\tbreak;\n\n\tcase ANEG_STATE_ACK_DETECT_INIT:\n\t\tap->txconfig |= ANEG_CFG_ACK;\n\t\ttw32(MAC_TX_AUTO_NEG, ap->txconfig);\n\t\ttp->mac_mode |= MAC_MODE_SEND_CONFIGS;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\n\t\tap->state = ANEG_STATE_ACK_DETECT;\n\n\t\t/* fallthru */\n\tcase ANEG_STATE_ACK_DETECT:\n\t\tif (ap->ack_match != 0) {\n\t\t\tif ((ap->rxconfig & ~ANEG_CFG_ACK) ==\n\t\t\t    (ap->ability_match_cfg & ~ANEG_CFG_ACK)) {\n\t\t\t\tap->state = ANEG_STATE_COMPLETE_ACK_INIT;\n\t\t\t} else {\n\t\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\t\t\t}\n\t\t} else if (ap->ability_match != 0 &&\n\t\t\t   ap->rxconfig == 0) {\n\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\t\t}\n\t\tbreak;\n\n\tcase ANEG_STATE_COMPLETE_ACK_INIT:\n\t\tif (ap->rxconfig & ANEG_CFG_INVAL) {\n\t\t\tret = ANEG_FAILED;\n\t\t\tbreak;\n\t\t}\n\t\tap->flags &= ~(MR_LP_ADV_FULL_DUPLEX |\n\t\t\t       MR_LP_ADV_HALF_DUPLEX |\n\t\t\t       MR_LP_ADV_SYM_PAUSE |\n\t\t\t       MR_LP_ADV_ASYM_PAUSE |\n\t\t\t       MR_LP_ADV_REMOTE_FAULT1 |\n\t\t\t       MR_LP_ADV_REMOTE_FAULT2 |\n\t\t\t       MR_LP_ADV_NEXT_PAGE |\n\t\t\t       MR_TOGGLE_RX |\n\t\t\t       MR_NP_RX);\n\t\tif (ap->rxconfig & ANEG_CFG_FD)\n\t\t\tap->flags |= MR_LP_ADV_FULL_DUPLEX;\n\t\tif (ap->rxconfig & ANEG_CFG_HD)\n\t\t\tap->flags |= MR_LP_ADV_HALF_DUPLEX;\n\t\tif (ap->rxconfig & ANEG_CFG_PS1)\n\t\t\tap->flags |= MR_LP_ADV_SYM_PAUSE;\n\t\tif (ap->rxconfig & ANEG_CFG_PS2)\n\t\t\tap->flags |= MR_LP_ADV_ASYM_PAUSE;\n\t\tif (ap->rxconfig & ANEG_CFG_RF1)\n\t\t\tap->flags |= MR_LP_ADV_REMOTE_FAULT1;\n\t\tif (ap->rxconfig & ANEG_CFG_RF2)\n\t\t\tap->flags |= MR_LP_ADV_REMOTE_FAULT2;\n\t\tif (ap->rxconfig & ANEG_CFG_NP)\n\t\t\tap->flags |= MR_LP_ADV_NEXT_PAGE;\n\n\t\tap->link_time = ap->cur_time;\n\n\t\tap->flags ^= (MR_TOGGLE_TX);\n\t\tif (ap->rxconfig & 0x0008)\n\t\t\tap->flags |= MR_TOGGLE_RX;\n\t\tif (ap->rxconfig & ANEG_CFG_NP)\n\t\t\tap->flags |= MR_NP_RX;\n\t\tap->flags |= MR_PAGE_RX;\n\n\t\tap->state = ANEG_STATE_COMPLETE_ACK;\n\t\tret = ANEG_TIMER_ENAB;\n\t\tbreak;\n\n\tcase ANEG_STATE_COMPLETE_ACK:\n\t\tif (ap->ability_match != 0 &&\n\t\t    ap->rxconfig == 0) {\n\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\t\t\tbreak;\n\t\t}\n\t\tdelta = ap->cur_time - ap->link_time;\n\t\tif (delta > ANEG_STATE_SETTLE_TIME) {\n\t\t\tif (!(ap->flags & (MR_LP_ADV_NEXT_PAGE))) {\n\t\t\t\tap->state = ANEG_STATE_IDLE_DETECT_INIT;\n\t\t\t} else {\n\t\t\t\tif ((ap->txconfig & ANEG_CFG_NP) == 0 &&\n\t\t\t\t    !(ap->flags & MR_NP_RX)) {\n\t\t\t\t\tap->state = ANEG_STATE_IDLE_DETECT_INIT;\n\t\t\t\t} else {\n\t\t\t\t\tret = ANEG_FAILED;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase ANEG_STATE_IDLE_DETECT_INIT:\n\t\tap->link_time = ap->cur_time;\n\t\ttp->mac_mode &= ~MAC_MODE_SEND_CONFIGS;\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\n\t\tap->state = ANEG_STATE_IDLE_DETECT;\n\t\tret = ANEG_TIMER_ENAB;\n\t\tbreak;\n\n\tcase ANEG_STATE_IDLE_DETECT:\n\t\tif (ap->ability_match != 0 &&\n\t\t    ap->rxconfig == 0) {\n\t\t\tap->state = ANEG_STATE_AN_ENABLE;\n\t\t\tbreak;\n\t\t}\n\t\tdelta = ap->cur_time - ap->link_time;\n\t\tif (delta > ANEG_STATE_SETTLE_TIME) {\n\t\t\t/* XXX another gem from the Broadcom driver :( */\n\t\t\tap->state = ANEG_STATE_LINK_OK;\n\t\t}\n\t\tbreak;\n\n\tcase ANEG_STATE_LINK_OK:\n\t\tap->flags |= (MR_AN_COMPLETE | MR_LINK_OK);\n\t\tret = ANEG_DONE;\n\t\tbreak;\n\n\tcase ANEG_STATE_NEXT_PAGE_WAIT_INIT:\n\t\t/* ??? unimplemented */\n\t\tbreak;\n\n\tcase ANEG_STATE_NEXT_PAGE_WAIT:\n\t\t/* ??? unimplemented */\n\t\tbreak;\n\n\tdefault:\n\t\tret = ANEG_FAILED;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int fiber_autoneg(struct tg3 *tp, u32 *txflags, u32 *rxflags)\n{\n\tint res = 0;\n\tstruct tg3_fiber_aneginfo aninfo;\n\tint status = ANEG_FAILED;\n\tunsigned int tick;\n\tu32 tmp;\n\n\ttw32_f(MAC_TX_AUTO_NEG, 0);\n\n\ttmp = tp->mac_mode & ~MAC_MODE_PORT_MODE_MASK;\n\ttw32_f(MAC_MODE, tmp | MAC_MODE_PORT_MODE_GMII);\n\tudelay(40);\n\n\ttw32_f(MAC_MODE, tp->mac_mode | MAC_MODE_SEND_CONFIGS);\n\tudelay(40);\n\n\tmemset(&aninfo, 0, sizeof(aninfo));\n\taninfo.flags |= MR_AN_ENABLE;\n\taninfo.state = ANEG_STATE_UNKNOWN;\n\taninfo.cur_time = 0;\n\ttick = 0;\n\twhile (++tick < 195000) {\n\t\tstatus = tg3_fiber_aneg_smachine(tp, &aninfo);\n\t\tif (status == ANEG_DONE || status == ANEG_FAILED)\n\t\t\tbreak;\n\n\t\tudelay(1);\n\t}\n\n\ttp->mac_mode &= ~MAC_MODE_SEND_CONFIGS;\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\t*txflags = aninfo.txconfig;\n\t*rxflags = aninfo.flags;\n\n\tif (status == ANEG_DONE &&\n\t    (aninfo.flags & (MR_AN_COMPLETE | MR_LINK_OK |\n\t\t\t     MR_LP_ADV_FULL_DUPLEX)))\n\t\tres = 1;\n\n\treturn res;\n}\n\nstatic void tg3_init_bcm8002(struct tg3 *tp)\n{\n\tu32 mac_status = tr32(MAC_STATUS);\n\tint i;\n\n\t/* Reset when initting first time or we have a link. */\n\tif (tg3_flag(tp, INIT_COMPLETE) &&\n\t    !(mac_status & MAC_STATUS_PCS_SYNCED))\n\t\treturn;\n\n\t/* Set PLL lock range. */\n\ttg3_writephy(tp, 0x16, 0x8007);\n\n\t/* SW reset */\n\ttg3_writephy(tp, MII_BMCR, BMCR_RESET);\n\n\t/* Wait for reset to complete. */\n\t/* XXX schedule_timeout() ... */\n\tfor (i = 0; i < 500; i++)\n\t\tudelay(10);\n\n\t/* Config mode; select PMA/Ch 1 regs. */\n\ttg3_writephy(tp, 0x10, 0x8411);\n\n\t/* Enable auto-lock and comdet, select txclk for tx. */\n\ttg3_writephy(tp, 0x11, 0x0a10);\n\n\ttg3_writephy(tp, 0x18, 0x00a0);\n\ttg3_writephy(tp, 0x16, 0x41ff);\n\n\t/* Assert and deassert POR. */\n\ttg3_writephy(tp, 0x13, 0x0400);\n\tudelay(40);\n\ttg3_writephy(tp, 0x13, 0x0000);\n\n\ttg3_writephy(tp, 0x11, 0x0a50);\n\tudelay(40);\n\ttg3_writephy(tp, 0x11, 0x0a10);\n\n\t/* Wait for signal to stabilize */\n\t/* XXX schedule_timeout() ... */\n\tfor (i = 0; i < 15000; i++)\n\t\tudelay(10);\n\n\t/* Deselect the channel register so we can read the PHYID\n\t * later.\n\t */\n\ttg3_writephy(tp, 0x10, 0x8011);\n}\n\nstatic int tg3_setup_fiber_hw_autoneg(struct tg3 *tp, u32 mac_status)\n{\n\tu16 flowctrl;\n\tu32 sg_dig_ctrl, sg_dig_status;\n\tu32 serdes_cfg, expected_sg_dig_ctrl;\n\tint workaround, port_a;\n\tint current_link_up;\n\n\tserdes_cfg = 0;\n\texpected_sg_dig_ctrl = 0;\n\tworkaround = 0;\n\tport_a = 1;\n\tcurrent_link_up = 0;\n\n\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5704_A0 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5704_A1) {\n\t\tworkaround = 1;\n\t\tif (tr32(TG3PCI_DUAL_MAC_CTRL) & DUAL_MAC_CTRL_ID)\n\t\t\tport_a = 0;\n\n\t\t/* preserve bits 0-11,13,14 for signal pre-emphasis */\n\t\t/* preserve bits 20-23 for voltage regulator */\n\t\tserdes_cfg = tr32(MAC_SERDES_CFG) & 0x00f06fff;\n\t}\n\n\tsg_dig_ctrl = tr32(SG_DIG_CTRL);\n\n\tif (tp->link_config.autoneg != AUTONEG_ENABLE) {\n\t\tif (sg_dig_ctrl & SG_DIG_USING_HW_AUTONEG) {\n\t\t\tif (workaround) {\n\t\t\t\tu32 val = serdes_cfg;\n\n\t\t\t\tif (port_a)\n\t\t\t\t\tval |= 0xc010000;\n\t\t\t\telse\n\t\t\t\t\tval |= 0x4010000;\n\t\t\t\ttw32_f(MAC_SERDES_CFG, val);\n\t\t\t}\n\n\t\t\ttw32_f(SG_DIG_CTRL, SG_DIG_COMMON_SETUP);\n\t\t}\n\t\tif (mac_status & MAC_STATUS_PCS_SYNCED) {\n\t\t\ttg3_setup_flow_control(tp, 0, 0);\n\t\t\tcurrent_link_up = 1;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Want auto-negotiation.  */\n\texpected_sg_dig_ctrl = SG_DIG_USING_HW_AUTONEG | SG_DIG_COMMON_SETUP;\n\n\tflowctrl = tg3_advert_flowctrl_1000X(tp->link_config.flowctrl);\n\tif (flowctrl & ADVERTISE_1000XPAUSE)\n\t\texpected_sg_dig_ctrl |= SG_DIG_PAUSE_CAP;\n\tif (flowctrl & ADVERTISE_1000XPSE_ASYM)\n\t\texpected_sg_dig_ctrl |= SG_DIG_ASYM_PAUSE;\n\n\tif (sg_dig_ctrl != expected_sg_dig_ctrl) {\n\t\tif ((tp->phy_flags & TG3_PHYFLG_PARALLEL_DETECT) &&\n\t\t    tp->serdes_counter &&\n\t\t    ((mac_status & (MAC_STATUS_PCS_SYNCED |\n\t\t\t\t    MAC_STATUS_RCVD_CFG)) ==\n\t\t     MAC_STATUS_PCS_SYNCED)) {\n\t\t\ttp->serdes_counter--;\n\t\t\tcurrent_link_up = 1;\n\t\t\tgoto out;\n\t\t}\nrestart_autoneg:\n\t\tif (workaround)\n\t\t\ttw32_f(MAC_SERDES_CFG, serdes_cfg | 0xc011000);\n\t\ttw32_f(SG_DIG_CTRL, expected_sg_dig_ctrl | SG_DIG_SOFT_RESET);\n\t\tudelay(5);\n\t\ttw32_f(SG_DIG_CTRL, expected_sg_dig_ctrl);\n\n\t\ttp->serdes_counter = SERDES_AN_TIMEOUT_5704S;\n\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t} else if (mac_status & (MAC_STATUS_PCS_SYNCED |\n\t\t\t\t MAC_STATUS_SIGNAL_DET)) {\n\t\tsg_dig_status = tr32(SG_DIG_STATUS);\n\t\tmac_status = tr32(MAC_STATUS);\n\n\t\tif ((sg_dig_status & SG_DIG_AUTONEG_COMPLETE) &&\n\t\t    (mac_status & MAC_STATUS_PCS_SYNCED)) {\n\t\t\tu32 local_adv = 0, remote_adv = 0;\n\n\t\t\tif (sg_dig_ctrl & SG_DIG_PAUSE_CAP)\n\t\t\t\tlocal_adv |= ADVERTISE_1000XPAUSE;\n\t\t\tif (sg_dig_ctrl & SG_DIG_ASYM_PAUSE)\n\t\t\t\tlocal_adv |= ADVERTISE_1000XPSE_ASYM;\n\n\t\t\tif (sg_dig_status & SG_DIG_PARTNER_PAUSE_CAPABLE)\n\t\t\t\tremote_adv |= LPA_1000XPAUSE;\n\t\t\tif (sg_dig_status & SG_DIG_PARTNER_ASYM_PAUSE)\n\t\t\t\tremote_adv |= LPA_1000XPAUSE_ASYM;\n\n\t\t\ttp->link_config.rmt_adv =\n\t\t\t\t\t   mii_adv_to_ethtool_adv_x(remote_adv);\n\n\t\t\ttg3_setup_flow_control(tp, local_adv, remote_adv);\n\t\t\tcurrent_link_up = 1;\n\t\t\ttp->serdes_counter = 0;\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t\t} else if (!(sg_dig_status & SG_DIG_AUTONEG_COMPLETE)) {\n\t\t\tif (tp->serdes_counter)\n\t\t\t\ttp->serdes_counter--;\n\t\t\telse {\n\t\t\t\tif (workaround) {\n\t\t\t\t\tu32 val = serdes_cfg;\n\n\t\t\t\t\tif (port_a)\n\t\t\t\t\t\tval |= 0xc010000;\n\t\t\t\t\telse\n\t\t\t\t\t\tval |= 0x4010000;\n\n\t\t\t\t\ttw32_f(MAC_SERDES_CFG, val);\n\t\t\t\t}\n\n\t\t\t\ttw32_f(SG_DIG_CTRL, SG_DIG_COMMON_SETUP);\n\t\t\t\tudelay(40);\n\n\t\t\t\t/* Link parallel detection - link is up */\n\t\t\t\t/* only if we have PCS_SYNC and not */\n\t\t\t\t/* receiving config code words */\n\t\t\t\tmac_status = tr32(MAC_STATUS);\n\t\t\t\tif ((mac_status & MAC_STATUS_PCS_SYNCED) &&\n\t\t\t\t    !(mac_status & MAC_STATUS_RCVD_CFG)) {\n\t\t\t\t\ttg3_setup_flow_control(tp, 0, 0);\n\t\t\t\t\tcurrent_link_up = 1;\n\t\t\t\t\ttp->phy_flags |=\n\t\t\t\t\t\tTG3_PHYFLG_PARALLEL_DETECT;\n\t\t\t\t\ttp->serdes_counter =\n\t\t\t\t\t\tSERDES_PARALLEL_DET_TIMEOUT;\n\t\t\t\t} else\n\t\t\t\t\tgoto restart_autoneg;\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttp->serdes_counter = SERDES_AN_TIMEOUT_5704S;\n\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t}\n\nout:\n\treturn current_link_up;\n}\n\nstatic int tg3_setup_fiber_by_hand(struct tg3 *tp, u32 mac_status)\n{\n\tint current_link_up = 0;\n\n\tif (!(mac_status & MAC_STATUS_PCS_SYNCED))\n\t\tgoto out;\n\n\tif (tp->link_config.autoneg == AUTONEG_ENABLE) {\n\t\tu32 txflags, rxflags;\n\t\tint i;\n\n\t\tif (fiber_autoneg(tp, &txflags, &rxflags)) {\n\t\t\tu32 local_adv = 0, remote_adv = 0;\n\n\t\t\tif (txflags & ANEG_CFG_PS1)\n\t\t\t\tlocal_adv |= ADVERTISE_1000XPAUSE;\n\t\t\tif (txflags & ANEG_CFG_PS2)\n\t\t\t\tlocal_adv |= ADVERTISE_1000XPSE_ASYM;\n\n\t\t\tif (rxflags & MR_LP_ADV_SYM_PAUSE)\n\t\t\t\tremote_adv |= LPA_1000XPAUSE;\n\t\t\tif (rxflags & MR_LP_ADV_ASYM_PAUSE)\n\t\t\t\tremote_adv |= LPA_1000XPAUSE_ASYM;\n\n\t\t\ttp->link_config.rmt_adv =\n\t\t\t\t\t   mii_adv_to_ethtool_adv_x(remote_adv);\n\n\t\t\ttg3_setup_flow_control(tp, local_adv, remote_adv);\n\n\t\t\tcurrent_link_up = 1;\n\t\t}\n\t\tfor (i = 0; i < 30; i++) {\n\t\t\tudelay(20);\n\t\t\ttw32_f(MAC_STATUS,\n\t\t\t       (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\tMAC_STATUS_CFG_CHANGED));\n\t\t\tudelay(40);\n\t\t\tif ((tr32(MAC_STATUS) &\n\t\t\t     (MAC_STATUS_SYNC_CHANGED |\n\t\t\t      MAC_STATUS_CFG_CHANGED)) == 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tmac_status = tr32(MAC_STATUS);\n\t\tif (current_link_up == 0 &&\n\t\t    (mac_status & MAC_STATUS_PCS_SYNCED) &&\n\t\t    !(mac_status & MAC_STATUS_RCVD_CFG))\n\t\t\tcurrent_link_up = 1;\n\t} else {\n\t\ttg3_setup_flow_control(tp, 0, 0);\n\n\t\t/* Forcing 1000FD link up. */\n\t\tcurrent_link_up = 1;\n\n\t\ttw32_f(MAC_MODE, (tp->mac_mode | MAC_MODE_SEND_CONFIGS));\n\t\tudelay(40);\n\n\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\tudelay(40);\n\t}\n\nout:\n\treturn current_link_up;\n}\n\nstatic int tg3_setup_fiber_phy(struct tg3 *tp, int force_reset)\n{\n\tu32 orig_pause_cfg;\n\tu16 orig_active_speed;\n\tu8 orig_active_duplex;\n\tu32 mac_status;\n\tint current_link_up;\n\tint i;\n\n\torig_pause_cfg = tp->link_config.active_flowctrl;\n\torig_active_speed = tp->link_config.active_speed;\n\torig_active_duplex = tp->link_config.active_duplex;\n\n\tif (!tg3_flag(tp, HW_AUTONEG) &&\n\t    tp->link_up &&\n\t    tg3_flag(tp, INIT_COMPLETE)) {\n\t\tmac_status = tr32(MAC_STATUS);\n\t\tmac_status &= (MAC_STATUS_PCS_SYNCED |\n\t\t\t       MAC_STATUS_SIGNAL_DET |\n\t\t\t       MAC_STATUS_CFG_CHANGED |\n\t\t\t       MAC_STATUS_RCVD_CFG);\n\t\tif (mac_status == (MAC_STATUS_PCS_SYNCED |\n\t\t\t\t   MAC_STATUS_SIGNAL_DET)) {\n\t\t\ttw32_f(MAC_STATUS, (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\t\t    MAC_STATUS_CFG_CHANGED));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\ttw32_f(MAC_TX_AUTO_NEG, 0);\n\n\ttp->mac_mode &= ~(MAC_MODE_PORT_MODE_MASK | MAC_MODE_HALF_DUPLEX);\n\ttp->mac_mode |= MAC_MODE_PORT_MODE_TBI;\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\tif (tp->phy_id == TG3_PHY_ID_BCM8002)\n\t\ttg3_init_bcm8002(tp);\n\n\t/* Enable link change event even when serdes polling.  */\n\ttw32_f(MAC_EVENT, MAC_EVENT_LNKSTATE_CHANGED);\n\tudelay(40);\n\n\tcurrent_link_up = 0;\n\ttp->link_config.rmt_adv = 0;\n\tmac_status = tr32(MAC_STATUS);\n\n\tif (tg3_flag(tp, HW_AUTONEG))\n\t\tcurrent_link_up = tg3_setup_fiber_hw_autoneg(tp, mac_status);\n\telse\n\t\tcurrent_link_up = tg3_setup_fiber_by_hand(tp, mac_status);\n\n\ttp->napi[0].hw_status->status =\n\t\t(SD_STATUS_UPDATED |\n\t\t (tp->napi[0].hw_status->status & ~SD_STATUS_LINK_CHG));\n\n\tfor (i = 0; i < 100; i++) {\n\t\ttw32_f(MAC_STATUS, (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\t    MAC_STATUS_CFG_CHANGED));\n\t\tudelay(5);\n\t\tif ((tr32(MAC_STATUS) & (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\t\t MAC_STATUS_CFG_CHANGED |\n\t\t\t\t\t MAC_STATUS_LNKSTATE_CHANGED)) == 0)\n\t\t\tbreak;\n\t}\n\n\tmac_status = tr32(MAC_STATUS);\n\tif ((mac_status & MAC_STATUS_PCS_SYNCED) == 0) {\n\t\tcurrent_link_up = 0;\n\t\tif (tp->link_config.autoneg == AUTONEG_ENABLE &&\n\t\t    tp->serdes_counter == 0) {\n\t\t\ttw32_f(MAC_MODE, (tp->mac_mode |\n\t\t\t\t\t  MAC_MODE_SEND_CONFIGS));\n\t\t\tudelay(1);\n\t\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\t}\n\t}\n\n\tif (current_link_up == 1) {\n\t\ttp->link_config.active_speed = SPEED_1000;\n\t\ttp->link_config.active_duplex = DUPLEX_FULL;\n\t\ttw32(MAC_LED_CTRL, (tp->led_ctrl |\n\t\t\t\t    LED_CTRL_LNKLED_OVERRIDE |\n\t\t\t\t    LED_CTRL_1000MBPS_ON));\n\t} else {\n\t\ttp->link_config.active_speed = SPEED_UNKNOWN;\n\t\ttp->link_config.active_duplex = DUPLEX_UNKNOWN;\n\t\ttw32(MAC_LED_CTRL, (tp->led_ctrl |\n\t\t\t\t    LED_CTRL_LNKLED_OVERRIDE |\n\t\t\t\t    LED_CTRL_TRAFFIC_OVERRIDE));\n\t}\n\n\tif (!tg3_test_and_report_link_chg(tp, current_link_up)) {\n\t\tu32 now_pause_cfg = tp->link_config.active_flowctrl;\n\t\tif (orig_pause_cfg != now_pause_cfg ||\n\t\t    orig_active_speed != tp->link_config.active_speed ||\n\t\t    orig_active_duplex != tp->link_config.active_duplex)\n\t\t\ttg3_link_report(tp);\n\t}\n\n\treturn 0;\n}\n\nstatic int tg3_setup_fiber_mii_phy(struct tg3 *tp, int force_reset)\n{\n\tint current_link_up, err = 0;\n\tu32 bmsr, bmcr;\n\tu16 current_speed;\n\tu8 current_duplex;\n\tu32 local_adv, remote_adv;\n\n\ttp->mac_mode |= MAC_MODE_PORT_MODE_GMII;\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\ttw32(MAC_EVENT, 0);\n\n\ttw32_f(MAC_STATUS,\n\t     (MAC_STATUS_SYNC_CHANGED |\n\t      MAC_STATUS_CFG_CHANGED |\n\t      MAC_STATUS_MI_COMPLETION |\n\t      MAC_STATUS_LNKSTATE_CHANGED));\n\tudelay(40);\n\n\tif (force_reset)\n\t\ttg3_phy_reset(tp);\n\n\tcurrent_link_up = 0;\n\tcurrent_speed = SPEED_UNKNOWN;\n\tcurrent_duplex = DUPLEX_UNKNOWN;\n\ttp->link_config.rmt_adv = 0;\n\n\terr |= tg3_readphy(tp, MII_BMSR, &bmsr);\n\terr |= tg3_readphy(tp, MII_BMSR, &bmsr);\n\tif (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\tif (tr32(MAC_TX_STATUS) & TX_STATUS_LINK_UP)\n\t\t\tbmsr |= BMSR_LSTATUS;\n\t\telse\n\t\t\tbmsr &= ~BMSR_LSTATUS;\n\t}\n\n\terr |= tg3_readphy(tp, MII_BMCR, &bmcr);\n\n\tif ((tp->link_config.autoneg == AUTONEG_ENABLE) && !force_reset &&\n\t    (tp->phy_flags & TG3_PHYFLG_PARALLEL_DETECT)) {\n\t\t/* do nothing, just check for link up at the end */\n\t} else if (tp->link_config.autoneg == AUTONEG_ENABLE) {\n\t\tu32 adv, newadv;\n\n\t\terr |= tg3_readphy(tp, MII_ADVERTISE, &adv);\n\t\tnewadv = adv & ~(ADVERTISE_1000XFULL | ADVERTISE_1000XHALF |\n\t\t\t\t ADVERTISE_1000XPAUSE |\n\t\t\t\t ADVERTISE_1000XPSE_ASYM |\n\t\t\t\t ADVERTISE_SLCT);\n\n\t\tnewadv |= tg3_advert_flowctrl_1000X(tp->link_config.flowctrl);\n\t\tnewadv |= ethtool_adv_to_mii_adv_x(tp->link_config.advertising);\n\n\t\tif ((newadv != adv) || !(bmcr & BMCR_ANENABLE)) {\n\t\t\ttg3_writephy(tp, MII_ADVERTISE, newadv);\n\t\t\tbmcr |= BMCR_ANENABLE | BMCR_ANRESTART;\n\t\t\ttg3_writephy(tp, MII_BMCR, bmcr);\n\n\t\t\ttw32_f(MAC_EVENT, MAC_EVENT_LNKSTATE_CHANGED);\n\t\t\ttp->serdes_counter = SERDES_AN_TIMEOUT_5714S;\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\n\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tu32 new_bmcr;\n\n\t\tbmcr &= ~BMCR_SPEED1000;\n\t\tnew_bmcr = bmcr & ~(BMCR_ANENABLE | BMCR_FULLDPLX);\n\n\t\tif (tp->link_config.duplex == DUPLEX_FULL)\n\t\t\tnew_bmcr |= BMCR_FULLDPLX;\n\n\t\tif (new_bmcr != bmcr) {\n\t\t\t/* BMCR_SPEED1000 is a reserved bit that needs\n\t\t\t * to be set on write.\n\t\t\t */\n\t\t\tnew_bmcr |= BMCR_SPEED1000;\n\n\t\t\t/* Force a linkdown */\n\t\t\tif (tp->link_up) {\n\t\t\t\tu32 adv;\n\n\t\t\t\terr |= tg3_readphy(tp, MII_ADVERTISE, &adv);\n\t\t\t\tadv &= ~(ADVERTISE_1000XFULL |\n\t\t\t\t\t ADVERTISE_1000XHALF |\n\t\t\t\t\t ADVERTISE_SLCT);\n\t\t\t\ttg3_writephy(tp, MII_ADVERTISE, adv);\n\t\t\t\ttg3_writephy(tp, MII_BMCR, bmcr |\n\t\t\t\t\t\t\t   BMCR_ANRESTART |\n\t\t\t\t\t\t\t   BMCR_ANENABLE);\n\t\t\t\tudelay(10);\n\t\t\t\ttg3_carrier_off(tp);\n\t\t\t}\n\t\t\ttg3_writephy(tp, MII_BMCR, new_bmcr);\n\t\t\tbmcr = new_bmcr;\n\t\t\terr |= tg3_readphy(tp, MII_BMSR, &bmsr);\n\t\t\terr |= tg3_readphy(tp, MII_BMSR, &bmsr);\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\t\t\tif (tr32(MAC_TX_STATUS) & TX_STATUS_LINK_UP)\n\t\t\t\t\tbmsr |= BMSR_LSTATUS;\n\t\t\t\telse\n\t\t\t\t\tbmsr &= ~BMSR_LSTATUS;\n\t\t\t}\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t\t}\n\t}\n\n\tif (bmsr & BMSR_LSTATUS) {\n\t\tcurrent_speed = SPEED_1000;\n\t\tcurrent_link_up = 1;\n\t\tif (bmcr & BMCR_FULLDPLX)\n\t\t\tcurrent_duplex = DUPLEX_FULL;\n\t\telse\n\t\t\tcurrent_duplex = DUPLEX_HALF;\n\n\t\tlocal_adv = 0;\n\t\tremote_adv = 0;\n\n\t\tif (bmcr & BMCR_ANENABLE) {\n\t\t\tu32 common;\n\n\t\t\terr |= tg3_readphy(tp, MII_ADVERTISE, &local_adv);\n\t\t\terr |= tg3_readphy(tp, MII_LPA, &remote_adv);\n\t\t\tcommon = local_adv & remote_adv;\n\t\t\tif (common & (ADVERTISE_1000XHALF |\n\t\t\t\t      ADVERTISE_1000XFULL)) {\n\t\t\t\tif (common & ADVERTISE_1000XFULL)\n\t\t\t\t\tcurrent_duplex = DUPLEX_FULL;\n\t\t\t\telse\n\t\t\t\t\tcurrent_duplex = DUPLEX_HALF;\n\n\t\t\t\ttp->link_config.rmt_adv =\n\t\t\t\t\t   mii_adv_to_ethtool_adv_x(remote_adv);\n\t\t\t} else if (!tg3_flag(tp, 5780_CLASS)) {\n\t\t\t\t/* Link is up via parallel detect */\n\t\t\t} else {\n\t\t\t\tcurrent_link_up = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (current_link_up == 1 && current_duplex == DUPLEX_FULL)\n\t\ttg3_setup_flow_control(tp, local_adv, remote_adv);\n\n\ttp->mac_mode &= ~MAC_MODE_HALF_DUPLEX;\n\tif (tp->link_config.active_duplex == DUPLEX_HALF)\n\t\ttp->mac_mode |= MAC_MODE_HALF_DUPLEX;\n\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\ttw32_f(MAC_EVENT, MAC_EVENT_LNKSTATE_CHANGED);\n\n\ttp->link_config.active_speed = current_speed;\n\ttp->link_config.active_duplex = current_duplex;\n\n\ttg3_test_and_report_link_chg(tp, current_link_up);\n\treturn err;\n}\n\nstatic void tg3_serdes_parallel_detect(struct tg3 *tp)\n{\n\tif (tp->serdes_counter) {\n\t\t/* Give autoneg time to complete. */\n\t\ttp->serdes_counter--;\n\t\treturn;\n\t}\n\n\tif (!tp->link_up &&\n\t    (tp->link_config.autoneg == AUTONEG_ENABLE)) {\n\t\tu32 bmcr;\n\n\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\t\tif (bmcr & BMCR_ANENABLE) {\n\t\t\tu32 phy1, phy2;\n\n\t\t\t/* Select shadow register 0x1f */\n\t\t\ttg3_writephy(tp, MII_TG3_MISC_SHDW, 0x7c00);\n\t\t\ttg3_readphy(tp, MII_TG3_MISC_SHDW, &phy1);\n\n\t\t\t/* Select expansion interrupt status register */\n\t\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t\t\t MII_TG3_DSP_EXP1_INT_STAT);\n\t\t\ttg3_readphy(tp, MII_TG3_DSP_RW_PORT, &phy2);\n\t\t\ttg3_readphy(tp, MII_TG3_DSP_RW_PORT, &phy2);\n\n\t\t\tif ((phy1 & 0x10) && !(phy2 & 0x20)) {\n\t\t\t\t/* We have signal detect and not receiving\n\t\t\t\t * config code words, link is up by parallel\n\t\t\t\t * detection.\n\t\t\t\t */\n\n\t\t\t\tbmcr &= ~BMCR_ANENABLE;\n\t\t\t\tbmcr |= BMCR_SPEED1000 | BMCR_FULLDPLX;\n\t\t\t\ttg3_writephy(tp, MII_BMCR, bmcr);\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_PARALLEL_DETECT;\n\t\t\t}\n\t\t}\n\t} else if (tp->link_up &&\n\t\t   (tp->link_config.autoneg == AUTONEG_ENABLE) &&\n\t\t   (tp->phy_flags & TG3_PHYFLG_PARALLEL_DETECT)) {\n\t\tu32 phy2;\n\n\t\t/* Select expansion interrupt status register */\n\t\ttg3_writephy(tp, MII_TG3_DSP_ADDRESS,\n\t\t\t\t MII_TG3_DSP_EXP1_INT_STAT);\n\t\ttg3_readphy(tp, MII_TG3_DSP_RW_PORT, &phy2);\n\t\tif (phy2 & 0x20) {\n\t\t\tu32 bmcr;\n\n\t\t\t/* Config code words received, turn on autoneg. */\n\t\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\t\t\ttg3_writephy(tp, MII_BMCR, bmcr | BMCR_ANENABLE);\n\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\n\t\t}\n\t}\n}\n\nstatic int tg3_setup_phy(struct tg3 *tp, int force_reset)\n{\n\tu32 val;\n\tint err;\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\terr = tg3_setup_fiber_phy(tp, force_reset);\n\telse if (tp->phy_flags & TG3_PHYFLG_MII_SERDES)\n\t\terr = tg3_setup_fiber_mii_phy(tp, force_reset);\n\telse\n\t\terr = tg3_setup_copper_phy(tp, force_reset);\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX) {\n\t\tu32 scale;\n\n\t\tval = tr32(TG3_CPMU_CLCK_STAT) & CPMU_CLCK_STAT_MAC_CLCK_MASK;\n\t\tif (val == CPMU_CLCK_STAT_MAC_CLCK_62_5)\n\t\t\tscale = 65;\n\t\telse if (val == CPMU_CLCK_STAT_MAC_CLCK_6_25)\n\t\t\tscale = 6;\n\t\telse\n\t\t\tscale = 12;\n\n\t\tval = tr32(GRC_MISC_CFG) & ~GRC_MISC_CFG_PRESCALAR_MASK;\n\t\tval |= (scale << GRC_MISC_CFG_PRESCALAR_SHIFT);\n\t\ttw32(GRC_MISC_CFG, val);\n\t}\n\n\tval = (2 << TX_LENGTHS_IPG_CRS_SHIFT) |\n\t      (6 << TX_LENGTHS_IPG_SHIFT);\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tval |= tr32(MAC_TX_LENGTHS) &\n\t\t       (TX_LENGTHS_JMB_FRM_LEN_MSK |\n\t\t\tTX_LENGTHS_CNT_DWN_VAL_MSK);\n\n\tif (tp->link_config.active_speed == SPEED_1000 &&\n\t    tp->link_config.active_duplex == DUPLEX_HALF)\n\t\ttw32(MAC_TX_LENGTHS, val |\n\t\t     (0xff << TX_LENGTHS_SLOT_TIME_SHIFT));\n\telse\n\t\ttw32(MAC_TX_LENGTHS, val |\n\t\t     (32 << TX_LENGTHS_SLOT_TIME_SHIFT));\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\tif (tp->link_up) {\n\t\t\ttw32(HOSTCC_STAT_COAL_TICKS,\n\t\t\t     tp->coal.stats_block_coalesce_usecs);\n\t\t} else {\n\t\t\ttw32(HOSTCC_STAT_COAL_TICKS, 0);\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, ASPM_WORKAROUND)) {\n\t\tval = tr32(PCIE_PWR_MGMT_THRESH);\n\t\tif (!tp->link_up)\n\t\t\tval = (val & ~PCIE_PWR_MGMT_L1_THRESH_MSK) |\n\t\t\t      tp->pwrmgmt_thresh;\n\t\telse\n\t\t\tval |= PCIE_PWR_MGMT_L1_THRESH_MSK;\n\t\ttw32(PCIE_PWR_MGMT_THRESH, val);\n\t}\n\n\treturn err;\n}\n\n/* tp->lock must be held */\nstatic u64 tg3_refclk_read(struct tg3 *tp)\n{\n\tu64 stamp = tr32(TG3_EAV_REF_CLCK_LSB);\n\treturn stamp | (u64)tr32(TG3_EAV_REF_CLCK_MSB) << 32;\n}\n\n/* tp->lock must be held */\nstatic void tg3_refclk_write(struct tg3 *tp, u64 newval)\n{\n\ttw32(TG3_EAV_REF_CLCK_CTL, TG3_EAV_REF_CLCK_CTL_STOP);\n\ttw32(TG3_EAV_REF_CLCK_LSB, newval & 0xffffffff);\n\ttw32(TG3_EAV_REF_CLCK_MSB, newval >> 32);\n\ttw32_f(TG3_EAV_REF_CLCK_CTL, TG3_EAV_REF_CLCK_CTL_RESUME);\n}\n\nstatic inline void tg3_full_lock(struct tg3 *tp, int irq_sync);\nstatic inline void tg3_full_unlock(struct tg3 *tp);\nstatic int tg3_get_ts_info(struct net_device *dev, struct ethtool_ts_info *info)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tinfo->so_timestamping = SOF_TIMESTAMPING_TX_SOFTWARE |\n\t\t\t\tSOF_TIMESTAMPING_RX_SOFTWARE |\n\t\t\t\tSOF_TIMESTAMPING_SOFTWARE    |\n\t\t\t\tSOF_TIMESTAMPING_TX_HARDWARE |\n\t\t\t\tSOF_TIMESTAMPING_RX_HARDWARE |\n\t\t\t\tSOF_TIMESTAMPING_RAW_HARDWARE;\n\n\tif (tp->ptp_clock)\n\t\tinfo->phc_index = ptp_clock_index(tp->ptp_clock);\n\telse\n\t\tinfo->phc_index = -1;\n\n\tinfo->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON);\n\n\tinfo->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |\n\t\t\t   (1 << HWTSTAMP_FILTER_PTP_V1_L4_EVENT) |\n\t\t\t   (1 << HWTSTAMP_FILTER_PTP_V2_L2_EVENT) |\n\t\t\t   (1 << HWTSTAMP_FILTER_PTP_V2_L4_EVENT);\n\treturn 0;\n}\n\nstatic int tg3_ptp_adjfreq(struct ptp_clock_info *ptp, s32 ppb)\n{\n\tstruct tg3 *tp = container_of(ptp, struct tg3, ptp_info);\n\tbool neg_adj = false;\n\tu32 correction = 0;\n\n\tif (ppb < 0) {\n\t\tneg_adj = true;\n\t\tppb = -ppb;\n\t}\n\n\t/* Frequency adjustment is performed using hardware with a 24 bit\n\t * accumulator and a programmable correction value. On each clk, the\n\t * correction value gets added to the accumulator and when it\n\t * overflows, the time counter is incremented/decremented.\n\t *\n\t * So conversion from ppb to correction value is\n\t *\t\tppb * (1 << 24) / 1000000000\n\t */\n\tcorrection = div_u64((u64)ppb * (1 << 24), 1000000000ULL) &\n\t\t     TG3_EAV_REF_CLK_CORRECT_MASK;\n\n\ttg3_full_lock(tp, 0);\n\n\tif (correction)\n\t\ttw32(TG3_EAV_REF_CLK_CORRECT_CTL,\n\t\t     TG3_EAV_REF_CLK_CORRECT_EN |\n\t\t     (neg_adj ? TG3_EAV_REF_CLK_CORRECT_NEG : 0) | correction);\n\telse\n\t\ttw32(TG3_EAV_REF_CLK_CORRECT_CTL, 0);\n\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic int tg3_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)\n{\n\tstruct tg3 *tp = container_of(ptp, struct tg3, ptp_info);\n\n\ttg3_full_lock(tp, 0);\n\ttp->ptp_adjust += delta;\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic int tg3_ptp_gettime(struct ptp_clock_info *ptp, struct timespec *ts)\n{\n\tu64 ns;\n\tu32 remainder;\n\tstruct tg3 *tp = container_of(ptp, struct tg3, ptp_info);\n\n\ttg3_full_lock(tp, 0);\n\tns = tg3_refclk_read(tp);\n\tns += tp->ptp_adjust;\n\ttg3_full_unlock(tp);\n\n\tts->tv_sec = div_u64_rem(ns, 1000000000, &remainder);\n\tts->tv_nsec = remainder;\n\n\treturn 0;\n}\n\nstatic int tg3_ptp_settime(struct ptp_clock_info *ptp,\n\t\t\t   const struct timespec *ts)\n{\n\tu64 ns;\n\tstruct tg3 *tp = container_of(ptp, struct tg3, ptp_info);\n\n\tns = timespec_to_ns(ts);\n\n\ttg3_full_lock(tp, 0);\n\ttg3_refclk_write(tp, ns);\n\ttp->ptp_adjust = 0;\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic int tg3_ptp_enable(struct ptp_clock_info *ptp,\n\t\t\t  struct ptp_clock_request *rq, int on)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic const struct ptp_clock_info tg3_ptp_caps = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"tg3 clock\",\n\t.max_adj\t= 250000000,\n\t.n_alarm\t= 0,\n\t.n_ext_ts\t= 0,\n\t.n_per_out\t= 0,\n\t.pps\t\t= 0,\n\t.adjfreq\t= tg3_ptp_adjfreq,\n\t.adjtime\t= tg3_ptp_adjtime,\n\t.gettime\t= tg3_ptp_gettime,\n\t.settime\t= tg3_ptp_settime,\n\t.enable\t\t= tg3_ptp_enable,\n};\n\nstatic void tg3_hwclock_to_timestamp(struct tg3 *tp, u64 hwclock,\n\t\t\t\t     struct skb_shared_hwtstamps *timestamp)\n{\n\tmemset(timestamp, 0, sizeof(struct skb_shared_hwtstamps));\n\ttimestamp->hwtstamp  = ns_to_ktime((hwclock & TG3_TSTAMP_MASK) +\n\t\t\t\t\t   tp->ptp_adjust);\n}\n\n/* tp->lock must be held */\nstatic void tg3_ptp_init(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, PTP_CAPABLE))\n\t\treturn;\n\n\t/* Initialize the hardware clock to the system time. */\n\ttg3_refclk_write(tp, ktime_to_ns(ktime_get_real()));\n\ttp->ptp_adjust = 0;\n\ttp->ptp_info = tg3_ptp_caps;\n}\n\n/* tp->lock must be held */\nstatic void tg3_ptp_resume(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, PTP_CAPABLE))\n\t\treturn;\n\n\ttg3_refclk_write(tp, ktime_to_ns(ktime_get_real()) + tp->ptp_adjust);\n\ttp->ptp_adjust = 0;\n}\n\nstatic void tg3_ptp_fini(struct tg3 *tp)\n{\n\tif (!tg3_flag(tp, PTP_CAPABLE) || !tp->ptp_clock)\n\t\treturn;\n\n\tptp_clock_unregister(tp->ptp_clock);\n\ttp->ptp_clock = NULL;\n\ttp->ptp_adjust = 0;\n}\n\nstatic inline int tg3_irq_sync(struct tg3 *tp)\n{\n\treturn tp->irq_sync;\n}\n\nstatic inline void tg3_rd32_loop(struct tg3 *tp, u32 *dst, u32 off, u32 len)\n{\n\tint i;\n\n\tdst = (u32 *)((u8 *)dst + off);\n\tfor (i = 0; i < len; i += sizeof(u32))\n\t\t*dst++ = tr32(off + i);\n}\n\nstatic void tg3_dump_legacy_regs(struct tg3 *tp, u32 *regs)\n{\n\ttg3_rd32_loop(tp, regs, TG3PCI_VENDOR, 0xb0);\n\ttg3_rd32_loop(tp, regs, MAILBOX_INTERRUPT_0, 0x200);\n\ttg3_rd32_loop(tp, regs, MAC_MODE, 0x4f0);\n\ttg3_rd32_loop(tp, regs, SNDDATAI_MODE, 0xe0);\n\ttg3_rd32_loop(tp, regs, SNDDATAC_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, SNDBDS_MODE, 0x80);\n\ttg3_rd32_loop(tp, regs, SNDBDI_MODE, 0x48);\n\ttg3_rd32_loop(tp, regs, SNDBDC_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, RCVLPC_MODE, 0x20);\n\ttg3_rd32_loop(tp, regs, RCVLPC_SELLST_BASE, 0x15c);\n\ttg3_rd32_loop(tp, regs, RCVDBDI_MODE, 0x0c);\n\ttg3_rd32_loop(tp, regs, RCVDBDI_JUMBO_BD, 0x3c);\n\ttg3_rd32_loop(tp, regs, RCVDBDI_BD_PROD_IDX_0, 0x44);\n\ttg3_rd32_loop(tp, regs, RCVDCC_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, RCVBDI_MODE, 0x20);\n\ttg3_rd32_loop(tp, regs, RCVCC_MODE, 0x14);\n\ttg3_rd32_loop(tp, regs, RCVLSC_MODE, 0x08);\n\ttg3_rd32_loop(tp, regs, MBFREE_MODE, 0x08);\n\ttg3_rd32_loop(tp, regs, HOSTCC_MODE, 0x100);\n\n\tif (tg3_flag(tp, SUPPORT_MSIX))\n\t\ttg3_rd32_loop(tp, regs, HOSTCC_RXCOL_TICKS_VEC1, 0x180);\n\n\ttg3_rd32_loop(tp, regs, MEMARB_MODE, 0x10);\n\ttg3_rd32_loop(tp, regs, BUFMGR_MODE, 0x58);\n\ttg3_rd32_loop(tp, regs, RDMAC_MODE, 0x08);\n\ttg3_rd32_loop(tp, regs, WDMAC_MODE, 0x08);\n\ttg3_rd32_loop(tp, regs, RX_CPU_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, RX_CPU_STATE, 0x04);\n\ttg3_rd32_loop(tp, regs, RX_CPU_PGMCTR, 0x04);\n\ttg3_rd32_loop(tp, regs, RX_CPU_HWBKPT, 0x04);\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\ttg3_rd32_loop(tp, regs, TX_CPU_MODE, 0x04);\n\t\ttg3_rd32_loop(tp, regs, TX_CPU_STATE, 0x04);\n\t\ttg3_rd32_loop(tp, regs, TX_CPU_PGMCTR, 0x04);\n\t}\n\n\ttg3_rd32_loop(tp, regs, GRCMBOX_INTERRUPT_0, 0x110);\n\ttg3_rd32_loop(tp, regs, FTQ_RESET, 0x120);\n\ttg3_rd32_loop(tp, regs, MSGINT_MODE, 0x0c);\n\ttg3_rd32_loop(tp, regs, DMAC_MODE, 0x04);\n\ttg3_rd32_loop(tp, regs, GRC_MODE, 0x4c);\n\n\tif (tg3_flag(tp, NVRAM))\n\t\ttg3_rd32_loop(tp, regs, NVRAM_CMD, 0x24);\n}\n\nstatic void tg3_dump_state(struct tg3 *tp)\n{\n\tint i;\n\tu32 *regs;\n\n\tregs = kzalloc(TG3_REG_BLK_SIZE, GFP_ATOMIC);\n\tif (!regs)\n\t\treturn;\n\n\tif (tg3_flag(tp, PCI_EXPRESS)) {\n\t\t/* Read up to but not including private PCI registers */\n\t\tfor (i = 0; i < TG3_PCIE_TLDLPL_PORT; i += sizeof(u32))\n\t\t\tregs[i / sizeof(u32)] = tr32(i);\n\t} else\n\t\ttg3_dump_legacy_regs(tp, regs);\n\n\tfor (i = 0; i < TG3_REG_BLK_SIZE / sizeof(u32); i += 4) {\n\t\tif (!regs[i + 0] && !regs[i + 1] &&\n\t\t    !regs[i + 2] && !regs[i + 3])\n\t\t\tcontinue;\n\n\t\tnetdev_err(tp->dev, \"0x%08x: 0x%08x, 0x%08x, 0x%08x, 0x%08x\\n\",\n\t\t\t   i * 4,\n\t\t\t   regs[i + 0], regs[i + 1], regs[i + 2], regs[i + 3]);\n\t}\n\n\tkfree(regs);\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\t/* SW status block */\n\t\tnetdev_err(tp->dev,\n\t\t\t \"%d: Host status block [%08x:%08x:(%04x:%04x:%04x):(%04x:%04x)]\\n\",\n\t\t\t   i,\n\t\t\t   tnapi->hw_status->status,\n\t\t\t   tnapi->hw_status->status_tag,\n\t\t\t   tnapi->hw_status->rx_jumbo_consumer,\n\t\t\t   tnapi->hw_status->rx_consumer,\n\t\t\t   tnapi->hw_status->rx_mini_consumer,\n\t\t\t   tnapi->hw_status->idx[0].rx_producer,\n\t\t\t   tnapi->hw_status->idx[0].tx_consumer);\n\n\t\tnetdev_err(tp->dev,\n\t\t\"%d: NAPI info [%08x:%08x:(%04x:%04x:%04x):%04x:(%04x:%04x:%04x:%04x)]\\n\",\n\t\t\t   i,\n\t\t\t   tnapi->last_tag, tnapi->last_irq_tag,\n\t\t\t   tnapi->tx_prod, tnapi->tx_cons, tnapi->tx_pending,\n\t\t\t   tnapi->rx_rcb_ptr,\n\t\t\t   tnapi->prodring.rx_std_prod_idx,\n\t\t\t   tnapi->prodring.rx_std_cons_idx,\n\t\t\t   tnapi->prodring.rx_jmb_prod_idx,\n\t\t\t   tnapi->prodring.rx_jmb_cons_idx);\n\t}\n}\n\n/* This is called whenever we suspect that the system chipset is re-\n * ordering the sequence of MMIO to the tx send mailbox. The symptom\n * is bogus tx completions. We try to recover by setting the\n * TG3_FLAG_MBOX_WRITE_REORDER flag and resetting the chip later\n * in the workqueue.\n */\nstatic void tg3_tx_recover(struct tg3 *tp)\n{\n\tBUG_ON(tg3_flag(tp, MBOX_WRITE_REORDER) ||\n\t       tp->write32_tx_mbox == tg3_write_indirect_mbox);\n\n\tnetdev_warn(tp->dev,\n\t\t    \"The system may be re-ordering memory-mapped I/O \"\n\t\t    \"cycles to the network device, attempting to recover. \"\n\t\t    \"Please report the problem to the driver maintainer \"\n\t\t    \"and include system chipset information.\\n\");\n\n\tspin_lock(&tp->lock);\n\ttg3_flag_set(tp, TX_RECOVERY_PENDING);\n\tspin_unlock(&tp->lock);\n}\n\nstatic inline u32 tg3_tx_avail(struct tg3_napi *tnapi)\n{\n\t/* Tell compiler to fetch tx indices from memory. */\n\tbarrier();\n\treturn tnapi->tx_pending -\n\t       ((tnapi->tx_prod - tnapi->tx_cons) & (TG3_TX_RING_SIZE - 1));\n}\n\n/* Tigon3 never reports partial packet sends.  So we do not\n * need special logic to handle SKBs that have not had all\n * of their frags sent yet, like SunGEM does.\n */\nstatic void tg3_tx(struct tg3_napi *tnapi)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tu32 hw_idx = tnapi->hw_status->idx[0].tx_consumer;\n\tu32 sw_idx = tnapi->tx_cons;\n\tstruct netdev_queue *txq;\n\tint index = tnapi - tp->napi;\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\n\tif (tg3_flag(tp, ENABLE_TSS))\n\t\tindex--;\n\n\ttxq = netdev_get_tx_queue(tp->dev, index);\n\n\twhile (sw_idx != hw_idx) {\n\t\tstruct tg3_tx_ring_info *ri = &tnapi->tx_buffers[sw_idx];\n\t\tstruct sk_buff *skb = ri->skb;\n\t\tint i, tx_bug = 0;\n\n\t\tif (unlikely(skb == NULL)) {\n\t\t\ttg3_tx_recover(tp);\n\t\t\treturn;\n\t\t}\n\n\t\tif (tnapi->tx_ring[sw_idx].len_flags & TXD_FLAG_HWTSTAMP) {\n\t\t\tstruct skb_shared_hwtstamps timestamp;\n\t\t\tu64 hwclock = tr32(TG3_TX_TSTAMP_LSB);\n\t\t\thwclock |= (u64)tr32(TG3_TX_TSTAMP_MSB) << 32;\n\n\t\t\ttg3_hwclock_to_timestamp(tp, hwclock, &timestamp);\n\n\t\t\tskb_tstamp_tx(skb, &timestamp);\n\t\t}\n\n\t\tpci_unmap_single(tp->pdev,\n\t\t\t\t dma_unmap_addr(ri, mapping),\n\t\t\t\t skb_headlen(skb),\n\t\t\t\t PCI_DMA_TODEVICE);\n\n\t\tri->skb = NULL;\n\n\t\twhile (ri->fragmented) {\n\t\t\tri->fragmented = false;\n\t\t\tsw_idx = NEXT_TX(sw_idx);\n\t\t\tri = &tnapi->tx_buffers[sw_idx];\n\t\t}\n\n\t\tsw_idx = NEXT_TX(sw_idx);\n\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tri = &tnapi->tx_buffers[sw_idx];\n\t\t\tif (unlikely(ri->skb != NULL || sw_idx == hw_idx))\n\t\t\t\ttx_bug = 1;\n\n\t\t\tpci_unmap_page(tp->pdev,\n\t\t\t\t       dma_unmap_addr(ri, mapping),\n\t\t\t\t       skb_frag_size(&skb_shinfo(skb)->frags[i]),\n\t\t\t\t       PCI_DMA_TODEVICE);\n\n\t\t\twhile (ri->fragmented) {\n\t\t\t\tri->fragmented = false;\n\t\t\t\tsw_idx = NEXT_TX(sw_idx);\n\t\t\t\tri = &tnapi->tx_buffers[sw_idx];\n\t\t\t}\n\n\t\t\tsw_idx = NEXT_TX(sw_idx);\n\t\t}\n\n\t\tpkts_compl++;\n\t\tbytes_compl += skb->len;\n\n\t\tdev_kfree_skb(skb);\n\n\t\tif (unlikely(tx_bug)) {\n\t\t\ttg3_tx_recover(tp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tnetdev_tx_completed_queue(txq, pkts_compl, bytes_compl);\n\n\ttnapi->tx_cons = sw_idx;\n\n\t/* Need to make the tx_cons update visible to tg3_start_xmit()\n\t * before checking for netif_queue_stopped().  Without the\n\t * memory barrier, there is a small possibility that tg3_start_xmit()\n\t * will miss it and cause the queue to be stopped forever.\n\t */\n\tsmp_mb();\n\n\tif (unlikely(netif_tx_queue_stopped(txq) &&\n\t\t     (tg3_tx_avail(tnapi) > TG3_TX_WAKEUP_THRESH(tnapi)))) {\n\t\t__netif_tx_lock(txq, smp_processor_id());\n\t\tif (netif_tx_queue_stopped(txq) &&\n\t\t    (tg3_tx_avail(tnapi) > TG3_TX_WAKEUP_THRESH(tnapi)))\n\t\t\tnetif_tx_wake_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic void tg3_frag_free(bool is_frag, void *data)\n{\n\tif (is_frag)\n\t\tput_page(virt_to_head_page(data));\n\telse\n\t\tkfree(data);\n}\n\nstatic void tg3_rx_data_free(struct tg3 *tp, struct ring_info *ri, u32 map_sz)\n{\n\tunsigned int skb_size = SKB_DATA_ALIGN(map_sz + TG3_RX_OFFSET(tp)) +\n\t\t   SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tif (!ri->data)\n\t\treturn;\n\n\tpci_unmap_single(tp->pdev, dma_unmap_addr(ri, mapping),\n\t\t\t map_sz, PCI_DMA_FROMDEVICE);\n\ttg3_frag_free(skb_size <= PAGE_SIZE, ri->data);\n\tri->data = NULL;\n}\n\n\n/* Returns size of skb allocated or < 0 on error.\n *\n * We only need to fill in the address because the other members\n * of the RX descriptor are invariant, see tg3_init_rings.\n *\n * Note the purposeful assymetry of cpu vs. chip accesses.  For\n * posting buffers we only dirty the first cache line of the RX\n * descriptor (containing the address).  Whereas for the RX status\n * buffers the cpu only reads the last cacheline of the RX descriptor\n * (to fetch the error flags, vlan tag, checksum, and opaque cookie).\n */\nstatic int tg3_alloc_rx_data(struct tg3 *tp, struct tg3_rx_prodring_set *tpr,\n\t\t\t     u32 opaque_key, u32 dest_idx_unmasked,\n\t\t\t     unsigned int *frag_size)\n{\n\tstruct tg3_rx_buffer_desc *desc;\n\tstruct ring_info *map;\n\tu8 *data;\n\tdma_addr_t mapping;\n\tint skb_size, data_size, dest_idx;\n\n\tswitch (opaque_key) {\n\tcase RXD_OPAQUE_RING_STD:\n\t\tdest_idx = dest_idx_unmasked & tp->rx_std_ring_mask;\n\t\tdesc = &tpr->rx_std[dest_idx];\n\t\tmap = &tpr->rx_std_buffers[dest_idx];\n\t\tdata_size = tp->rx_pkt_map_sz;\n\t\tbreak;\n\n\tcase RXD_OPAQUE_RING_JUMBO:\n\t\tdest_idx = dest_idx_unmasked & tp->rx_jmb_ring_mask;\n\t\tdesc = &tpr->rx_jmb[dest_idx].std;\n\t\tmap = &tpr->rx_jmb_buffers[dest_idx];\n\t\tdata_size = TG3_RX_JMB_MAP_SZ;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* Do not overwrite any of the map or rp information\n\t * until we are sure we can commit to a new buffer.\n\t *\n\t * Callers depend upon this behavior and assume that\n\t * we leave everything unchanged if we fail.\n\t */\n\tskb_size = SKB_DATA_ALIGN(data_size + TG3_RX_OFFSET(tp)) +\n\t\t   SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tif (skb_size <= PAGE_SIZE) {\n\t\tdata = netdev_alloc_frag(skb_size);\n\t\t*frag_size = skb_size;\n\t} else {\n\t\tdata = kmalloc(skb_size, GFP_ATOMIC);\n\t\t*frag_size = 0;\n\t}\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tmapping = pci_map_single(tp->pdev,\n\t\t\t\t data + TG3_RX_OFFSET(tp),\n\t\t\t\t data_size,\n\t\t\t\t PCI_DMA_FROMDEVICE);\n\tif (unlikely(pci_dma_mapping_error(tp->pdev, mapping))) {\n\t\ttg3_frag_free(skb_size <= PAGE_SIZE, data);\n\t\treturn -EIO;\n\t}\n\n\tmap->data = data;\n\tdma_unmap_addr_set(map, mapping, mapping);\n\n\tdesc->addr_hi = ((u64)mapping >> 32);\n\tdesc->addr_lo = ((u64)mapping & 0xffffffff);\n\n\treturn data_size;\n}\n\n/* We only need to move over in the address because the other\n * members of the RX descriptor are invariant.  See notes above\n * tg3_alloc_rx_data for full details.\n */\nstatic void tg3_recycle_rx(struct tg3_napi *tnapi,\n\t\t\t   struct tg3_rx_prodring_set *dpr,\n\t\t\t   u32 opaque_key, int src_idx,\n\t\t\t   u32 dest_idx_unmasked)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_rx_buffer_desc *src_desc, *dest_desc;\n\tstruct ring_info *src_map, *dest_map;\n\tstruct tg3_rx_prodring_set *spr = &tp->napi[0].prodring;\n\tint dest_idx;\n\n\tswitch (opaque_key) {\n\tcase RXD_OPAQUE_RING_STD:\n\t\tdest_idx = dest_idx_unmasked & tp->rx_std_ring_mask;\n\t\tdest_desc = &dpr->rx_std[dest_idx];\n\t\tdest_map = &dpr->rx_std_buffers[dest_idx];\n\t\tsrc_desc = &spr->rx_std[src_idx];\n\t\tsrc_map = &spr->rx_std_buffers[src_idx];\n\t\tbreak;\n\n\tcase RXD_OPAQUE_RING_JUMBO:\n\t\tdest_idx = dest_idx_unmasked & tp->rx_jmb_ring_mask;\n\t\tdest_desc = &dpr->rx_jmb[dest_idx].std;\n\t\tdest_map = &dpr->rx_jmb_buffers[dest_idx];\n\t\tsrc_desc = &spr->rx_jmb[src_idx].std;\n\t\tsrc_map = &spr->rx_jmb_buffers[src_idx];\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n\n\tdest_map->data = src_map->data;\n\tdma_unmap_addr_set(dest_map, mapping,\n\t\t\t   dma_unmap_addr(src_map, mapping));\n\tdest_desc->addr_hi = src_desc->addr_hi;\n\tdest_desc->addr_lo = src_desc->addr_lo;\n\n\t/* Ensure that the update to the skb happens after the physical\n\t * addresses have been transferred to the new BD location.\n\t */\n\tsmp_wmb();\n\n\tsrc_map->data = NULL;\n}\n\n/* The RX ring scheme is composed of multiple rings which post fresh\n * buffers to the chip, and one special ring the chip uses to report\n * status back to the host.\n *\n * The special ring reports the status of received packets to the\n * host.  The chip does not write into the original descriptor the\n * RX buffer was obtained from.  The chip simply takes the original\n * descriptor as provided by the host, updates the status and length\n * field, then writes this into the next status ring entry.\n *\n * Each ring the host uses to post buffers to the chip is described\n * by a TG3_BDINFO entry in the chips SRAM area.  When a packet arrives,\n * it is first placed into the on-chip ram.  When the packet's length\n * is known, it walks down the TG3_BDINFO entries to select the ring.\n * Each TG3_BDINFO specifies a MAXLEN field and the first TG3_BDINFO\n * which is within the range of the new packet's length is chosen.\n *\n * The \"separate ring for rx status\" scheme may sound queer, but it makes\n * sense from a cache coherency perspective.  If only the host writes\n * to the buffer post rings, and only the chip writes to the rx status\n * rings, then cache lines never move beyond shared-modified state.\n * If both the host and chip were to write into the same ring, cache line\n * eviction could occur since both entities want it in an exclusive state.\n */\nstatic int tg3_rx(struct tg3_napi *tnapi, int budget)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tu32 work_mask, rx_std_posted = 0;\n\tu32 std_prod_idx, jmb_prod_idx;\n\tu32 sw_idx = tnapi->rx_rcb_ptr;\n\tu16 hw_idx;\n\tint received;\n\tstruct tg3_rx_prodring_set *tpr = &tnapi->prodring;\n\n\thw_idx = *(tnapi->rx_rcb_prod_idx);\n\t/*\n\t * We need to order the read of hw_idx and the read of\n\t * the opaque cookie.\n\t */\n\trmb();\n\twork_mask = 0;\n\treceived = 0;\n\tstd_prod_idx = tpr->rx_std_prod_idx;\n\tjmb_prod_idx = tpr->rx_jmb_prod_idx;\n\twhile (sw_idx != hw_idx && budget > 0) {\n\t\tstruct ring_info *ri;\n\t\tstruct tg3_rx_buffer_desc *desc = &tnapi->rx_rcb[sw_idx];\n\t\tunsigned int len;\n\t\tstruct sk_buff *skb;\n\t\tdma_addr_t dma_addr;\n\t\tu32 opaque_key, desc_idx, *post_ptr;\n\t\tu8 *data;\n\t\tu64 tstamp = 0;\n\n\t\tdesc_idx = desc->opaque & RXD_OPAQUE_INDEX_MASK;\n\t\topaque_key = desc->opaque & RXD_OPAQUE_RING_MASK;\n\t\tif (opaque_key == RXD_OPAQUE_RING_STD) {\n\t\t\tri = &tp->napi[0].prodring.rx_std_buffers[desc_idx];\n\t\t\tdma_addr = dma_unmap_addr(ri, mapping);\n\t\t\tdata = ri->data;\n\t\t\tpost_ptr = &std_prod_idx;\n\t\t\trx_std_posted++;\n\t\t} else if (opaque_key == RXD_OPAQUE_RING_JUMBO) {\n\t\t\tri = &tp->napi[0].prodring.rx_jmb_buffers[desc_idx];\n\t\t\tdma_addr = dma_unmap_addr(ri, mapping);\n\t\t\tdata = ri->data;\n\t\t\tpost_ptr = &jmb_prod_idx;\n\t\t} else\n\t\t\tgoto next_pkt_nopost;\n\n\t\twork_mask |= opaque_key;\n\n\t\tif ((desc->err_vlan & RXD_ERR_MASK) != 0 &&\n\t\t    (desc->err_vlan != RXD_ERR_ODD_NIBBLE_RCVD_MII)) {\n\t\tdrop_it:\n\t\t\ttg3_recycle_rx(tnapi, tpr, opaque_key,\n\t\t\t\t       desc_idx, *post_ptr);\n\t\tdrop_it_no_recycle:\n\t\t\t/* Other statistics kept track of by card. */\n\t\t\ttp->rx_dropped++;\n\t\t\tgoto next_pkt;\n\t\t}\n\n\t\tprefetch(data + TG3_RX_OFFSET(tp));\n\t\tlen = ((desc->idx_len & RXD_LEN_MASK) >> RXD_LEN_SHIFT) -\n\t\t      ETH_FCS_LEN;\n\n\t\tif ((desc->type_flags & RXD_FLAG_PTPSTAT_MASK) ==\n\t\t     RXD_FLAG_PTPSTAT_PTPV1 ||\n\t\t    (desc->type_flags & RXD_FLAG_PTPSTAT_MASK) ==\n\t\t     RXD_FLAG_PTPSTAT_PTPV2) {\n\t\t\ttstamp = tr32(TG3_RX_TSTAMP_LSB);\n\t\t\ttstamp |= (u64)tr32(TG3_RX_TSTAMP_MSB) << 32;\n\t\t}\n\n\t\tif (len > TG3_RX_COPY_THRESH(tp)) {\n\t\t\tint skb_size;\n\t\t\tunsigned int frag_size;\n\n\t\t\tskb_size = tg3_alloc_rx_data(tp, tpr, opaque_key,\n\t\t\t\t\t\t    *post_ptr, &frag_size);\n\t\t\tif (skb_size < 0)\n\t\t\t\tgoto drop_it;\n\n\t\t\tpci_unmap_single(tp->pdev, dma_addr, skb_size,\n\t\t\t\t\t PCI_DMA_FROMDEVICE);\n\n\t\t\tskb = build_skb(data, frag_size);\n\t\t\tif (!skb) {\n\t\t\t\ttg3_frag_free(frag_size != 0, data);\n\t\t\t\tgoto drop_it_no_recycle;\n\t\t\t}\n\t\t\tskb_reserve(skb, TG3_RX_OFFSET(tp));\n\t\t\t/* Ensure that the update to the data happens\n\t\t\t * after the usage of the old DMA mapping.\n\t\t\t */\n\t\t\tsmp_wmb();\n\n\t\t\tri->data = NULL;\n\n\t\t} else {\n\t\t\ttg3_recycle_rx(tnapi, tpr, opaque_key,\n\t\t\t\t       desc_idx, *post_ptr);\n\n\t\t\tskb = netdev_alloc_skb(tp->dev,\n\t\t\t\t\t       len + TG3_RAW_IP_ALIGN);\n\t\t\tif (skb == NULL)\n\t\t\t\tgoto drop_it_no_recycle;\n\n\t\t\tskb_reserve(skb, TG3_RAW_IP_ALIGN);\n\t\t\tpci_dma_sync_single_for_cpu(tp->pdev, dma_addr, len, PCI_DMA_FROMDEVICE);\n\t\t\tmemcpy(skb->data,\n\t\t\t       data + TG3_RX_OFFSET(tp),\n\t\t\t       len);\n\t\t\tpci_dma_sync_single_for_device(tp->pdev, dma_addr, len, PCI_DMA_FROMDEVICE);\n\t\t}\n\n\t\tskb_put(skb, len);\n\t\tif (tstamp)\n\t\t\ttg3_hwclock_to_timestamp(tp, tstamp,\n\t\t\t\t\t\t skb_hwtstamps(skb));\n\n\t\tif ((tp->dev->features & NETIF_F_RXCSUM) &&\n\t\t    (desc->type_flags & RXD_FLAG_TCPUDP_CSUM) &&\n\t\t    (((desc->ip_tcp_csum & RXD_TCPCSUM_MASK)\n\t\t      >> RXD_TCPCSUM_SHIFT) == 0xffff))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\telse\n\t\t\tskb_checksum_none_assert(skb);\n\n\t\tskb->protocol = eth_type_trans(skb, tp->dev);\n\n\t\tif (len > (tp->dev->mtu + ETH_HLEN) &&\n\t\t    skb->protocol != htons(ETH_P_8021Q)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\tgoto drop_it_no_recycle;\n\t\t}\n\n\t\tif (desc->type_flags & RXD_FLAG_VLAN &&\n\t\t    !(tp->rx_mode & RX_MODE_KEEP_VLAN_TAG))\n\t\t\t__vlan_hwaccel_put_tag(skb,\n\t\t\t\t\t       desc->err_vlan & RXD_VLAN_MASK);\n\n\t\tnapi_gro_receive(&tnapi->napi, skb);\n\n\t\treceived++;\n\t\tbudget--;\n\nnext_pkt:\n\t\t(*post_ptr)++;\n\n\t\tif (unlikely(rx_std_posted >= tp->rx_std_max_post)) {\n\t\t\ttpr->rx_std_prod_idx = std_prod_idx &\n\t\t\t\t\t       tp->rx_std_ring_mask;\n\t\t\ttw32_rx_mbox(TG3_RX_STD_PROD_IDX_REG,\n\t\t\t\t     tpr->rx_std_prod_idx);\n\t\t\twork_mask &= ~RXD_OPAQUE_RING_STD;\n\t\t\trx_std_posted = 0;\n\t\t}\nnext_pkt_nopost:\n\t\tsw_idx++;\n\t\tsw_idx &= tp->rx_ret_ring_mask;\n\n\t\t/* Refresh hw_idx to see if there is new work */\n\t\tif (sw_idx == hw_idx) {\n\t\t\thw_idx = *(tnapi->rx_rcb_prod_idx);\n\t\t\trmb();\n\t\t}\n\t}\n\n\t/* ACK the status ring. */\n\ttnapi->rx_rcb_ptr = sw_idx;\n\ttw32_rx_mbox(tnapi->consmbox, sw_idx);\n\n\t/* Refill RX ring(s). */\n\tif (!tg3_flag(tp, ENABLE_RSS)) {\n\t\t/* Sync BD data before updating mailbox */\n\t\twmb();\n\n\t\tif (work_mask & RXD_OPAQUE_RING_STD) {\n\t\t\ttpr->rx_std_prod_idx = std_prod_idx &\n\t\t\t\t\t       tp->rx_std_ring_mask;\n\t\t\ttw32_rx_mbox(TG3_RX_STD_PROD_IDX_REG,\n\t\t\t\t     tpr->rx_std_prod_idx);\n\t\t}\n\t\tif (work_mask & RXD_OPAQUE_RING_JUMBO) {\n\t\t\ttpr->rx_jmb_prod_idx = jmb_prod_idx &\n\t\t\t\t\t       tp->rx_jmb_ring_mask;\n\t\t\ttw32_rx_mbox(TG3_RX_JMB_PROD_IDX_REG,\n\t\t\t\t     tpr->rx_jmb_prod_idx);\n\t\t}\n\t\tmmiowb();\n\t} else if (work_mask) {\n\t\t/* rx_std_buffers[] and rx_jmb_buffers[] entries must be\n\t\t * updated before the producer indices can be updated.\n\t\t */\n\t\tsmp_wmb();\n\n\t\ttpr->rx_std_prod_idx = std_prod_idx & tp->rx_std_ring_mask;\n\t\ttpr->rx_jmb_prod_idx = jmb_prod_idx & tp->rx_jmb_ring_mask;\n\n\t\tif (tnapi != &tp->napi[1]) {\n\t\t\ttp->rx_refill = true;\n\t\t\tnapi_schedule(&tp->napi[1].napi);\n\t\t}\n\t}\n\n\treturn received;\n}\n\nstatic void tg3_poll_link(struct tg3 *tp)\n{\n\t/* handle link change and other phy events */\n\tif (!(tg3_flag(tp, USE_LINKCHG_REG) || tg3_flag(tp, POLL_SERDES))) {\n\t\tstruct tg3_hw_status *sblk = tp->napi[0].hw_status;\n\n\t\tif (sblk->status & SD_STATUS_LINK_CHG) {\n\t\t\tsblk->status = SD_STATUS_UPDATED |\n\t\t\t\t       (sblk->status & ~SD_STATUS_LINK_CHG);\n\t\t\tspin_lock(&tp->lock);\n\t\t\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\t\t\ttw32_f(MAC_STATUS,\n\t\t\t\t     (MAC_STATUS_SYNC_CHANGED |\n\t\t\t\t      MAC_STATUS_CFG_CHANGED |\n\t\t\t\t      MAC_STATUS_MI_COMPLETION |\n\t\t\t\t      MAC_STATUS_LNKSTATE_CHANGED));\n\t\t\t\tudelay(40);\n\t\t\t} else\n\t\t\t\ttg3_setup_phy(tp, 0);\n\t\t\tspin_unlock(&tp->lock);\n\t\t}\n\t}\n}\n\nstatic int tg3_rx_prodring_xfer(struct tg3 *tp,\n\t\t\t\tstruct tg3_rx_prodring_set *dpr,\n\t\t\t\tstruct tg3_rx_prodring_set *spr)\n{\n\tu32 si, di, cpycnt, src_prod_idx;\n\tint i, err = 0;\n\n\twhile (1) {\n\t\tsrc_prod_idx = spr->rx_std_prod_idx;\n\n\t\t/* Make sure updates to the rx_std_buffers[] entries and the\n\t\t * standard producer index are seen in the correct order.\n\t\t */\n\t\tsmp_rmb();\n\n\t\tif (spr->rx_std_cons_idx == src_prod_idx)\n\t\t\tbreak;\n\n\t\tif (spr->rx_std_cons_idx < src_prod_idx)\n\t\t\tcpycnt = src_prod_idx - spr->rx_std_cons_idx;\n\t\telse\n\t\t\tcpycnt = tp->rx_std_ring_mask + 1 -\n\t\t\t\t spr->rx_std_cons_idx;\n\n\t\tcpycnt = min(cpycnt,\n\t\t\t     tp->rx_std_ring_mask + 1 - dpr->rx_std_prod_idx);\n\n\t\tsi = spr->rx_std_cons_idx;\n\t\tdi = dpr->rx_std_prod_idx;\n\n\t\tfor (i = di; i < di + cpycnt; i++) {\n\t\t\tif (dpr->rx_std_buffers[i].data) {\n\t\t\t\tcpycnt = i - di;\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!cpycnt)\n\t\t\tbreak;\n\n\t\t/* Ensure that updates to the rx_std_buffers ring and the\n\t\t * shadowed hardware producer ring from tg3_recycle_skb() are\n\t\t * ordered correctly WRT the skb check above.\n\t\t */\n\t\tsmp_rmb();\n\n\t\tmemcpy(&dpr->rx_std_buffers[di],\n\t\t       &spr->rx_std_buffers[si],\n\t\t       cpycnt * sizeof(struct ring_info));\n\n\t\tfor (i = 0; i < cpycnt; i++, di++, si++) {\n\t\t\tstruct tg3_rx_buffer_desc *sbd, *dbd;\n\t\t\tsbd = &spr->rx_std[si];\n\t\t\tdbd = &dpr->rx_std[di];\n\t\t\tdbd->addr_hi = sbd->addr_hi;\n\t\t\tdbd->addr_lo = sbd->addr_lo;\n\t\t}\n\n\t\tspr->rx_std_cons_idx = (spr->rx_std_cons_idx + cpycnt) &\n\t\t\t\t       tp->rx_std_ring_mask;\n\t\tdpr->rx_std_prod_idx = (dpr->rx_std_prod_idx + cpycnt) &\n\t\t\t\t       tp->rx_std_ring_mask;\n\t}\n\n\twhile (1) {\n\t\tsrc_prod_idx = spr->rx_jmb_prod_idx;\n\n\t\t/* Make sure updates to the rx_jmb_buffers[] entries and\n\t\t * the jumbo producer index are seen in the correct order.\n\t\t */\n\t\tsmp_rmb();\n\n\t\tif (spr->rx_jmb_cons_idx == src_prod_idx)\n\t\t\tbreak;\n\n\t\tif (spr->rx_jmb_cons_idx < src_prod_idx)\n\t\t\tcpycnt = src_prod_idx - spr->rx_jmb_cons_idx;\n\t\telse\n\t\t\tcpycnt = tp->rx_jmb_ring_mask + 1 -\n\t\t\t\t spr->rx_jmb_cons_idx;\n\n\t\tcpycnt = min(cpycnt,\n\t\t\t     tp->rx_jmb_ring_mask + 1 - dpr->rx_jmb_prod_idx);\n\n\t\tsi = spr->rx_jmb_cons_idx;\n\t\tdi = dpr->rx_jmb_prod_idx;\n\n\t\tfor (i = di; i < di + cpycnt; i++) {\n\t\t\tif (dpr->rx_jmb_buffers[i].data) {\n\t\t\t\tcpycnt = i - di;\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!cpycnt)\n\t\t\tbreak;\n\n\t\t/* Ensure that updates to the rx_jmb_buffers ring and the\n\t\t * shadowed hardware producer ring from tg3_recycle_skb() are\n\t\t * ordered correctly WRT the skb check above.\n\t\t */\n\t\tsmp_rmb();\n\n\t\tmemcpy(&dpr->rx_jmb_buffers[di],\n\t\t       &spr->rx_jmb_buffers[si],\n\t\t       cpycnt * sizeof(struct ring_info));\n\n\t\tfor (i = 0; i < cpycnt; i++, di++, si++) {\n\t\t\tstruct tg3_rx_buffer_desc *sbd, *dbd;\n\t\t\tsbd = &spr->rx_jmb[si].std;\n\t\t\tdbd = &dpr->rx_jmb[di].std;\n\t\t\tdbd->addr_hi = sbd->addr_hi;\n\t\t\tdbd->addr_lo = sbd->addr_lo;\n\t\t}\n\n\t\tspr->rx_jmb_cons_idx = (spr->rx_jmb_cons_idx + cpycnt) &\n\t\t\t\t       tp->rx_jmb_ring_mask;\n\t\tdpr->rx_jmb_prod_idx = (dpr->rx_jmb_prod_idx + cpycnt) &\n\t\t\t\t       tp->rx_jmb_ring_mask;\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_poll_work(struct tg3_napi *tnapi, int work_done, int budget)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\n\t/* run TX completion thread */\n\tif (tnapi->hw_status->idx[0].tx_consumer != tnapi->tx_cons) {\n\t\ttg3_tx(tnapi);\n\t\tif (unlikely(tg3_flag(tp, TX_RECOVERY_PENDING)))\n\t\t\treturn work_done;\n\t}\n\n\tif (!tnapi->rx_rcb_prod_idx)\n\t\treturn work_done;\n\n\t/* run RX thread, within the bounds set by NAPI.\n\t * All RX \"locking\" is done by ensuring outside\n\t * code synchronizes with tg3->napi.poll()\n\t */\n\tif (*(tnapi->rx_rcb_prod_idx) != tnapi->rx_rcb_ptr)\n\t\twork_done += tg3_rx(tnapi, budget - work_done);\n\n\tif (tg3_flag(tp, ENABLE_RSS) && tnapi == &tp->napi[1]) {\n\t\tstruct tg3_rx_prodring_set *dpr = &tp->napi[0].prodring;\n\t\tint i, err = 0;\n\t\tu32 std_prod_idx = dpr->rx_std_prod_idx;\n\t\tu32 jmb_prod_idx = dpr->rx_jmb_prod_idx;\n\n\t\ttp->rx_refill = false;\n\t\tfor (i = 1; i <= tp->rxq_cnt; i++)\n\t\t\terr |= tg3_rx_prodring_xfer(tp, dpr,\n\t\t\t\t\t\t    &tp->napi[i].prodring);\n\n\t\twmb();\n\n\t\tif (std_prod_idx != dpr->rx_std_prod_idx)\n\t\t\ttw32_rx_mbox(TG3_RX_STD_PROD_IDX_REG,\n\t\t\t\t     dpr->rx_std_prod_idx);\n\n\t\tif (jmb_prod_idx != dpr->rx_jmb_prod_idx)\n\t\t\ttw32_rx_mbox(TG3_RX_JMB_PROD_IDX_REG,\n\t\t\t\t     dpr->rx_jmb_prod_idx);\n\n\t\tmmiowb();\n\n\t\tif (err)\n\t\t\ttw32_f(HOSTCC_MODE, tp->coal_now);\n\t}\n\n\treturn work_done;\n}\n\nstatic inline void tg3_reset_task_schedule(struct tg3 *tp)\n{\n\tif (!test_and_set_bit(TG3_FLAG_RESET_TASK_PENDING, tp->tg3_flags))\n\t\tschedule_work(&tp->reset_task);\n}\n\nstatic inline void tg3_reset_task_cancel(struct tg3 *tp)\n{\n\tcancel_work_sync(&tp->reset_task);\n\ttg3_flag_clear(tp, RESET_TASK_PENDING);\n\ttg3_flag_clear(tp, TX_RECOVERY_PENDING);\n}\n\nstatic int tg3_poll_msix(struct napi_struct *napi, int budget)\n{\n\tstruct tg3_napi *tnapi = container_of(napi, struct tg3_napi, napi);\n\tstruct tg3 *tp = tnapi->tp;\n\tint work_done = 0;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\n\twhile (1) {\n\t\twork_done = tg3_poll_work(tnapi, work_done, budget);\n\n\t\tif (unlikely(tg3_flag(tp, TX_RECOVERY_PENDING)))\n\t\t\tgoto tx_recovery;\n\n\t\tif (unlikely(work_done >= budget))\n\t\t\tbreak;\n\n\t\t/* tp->last_tag is used in tg3_int_reenable() below\n\t\t * to tell the hw how much work has been processed,\n\t\t * so we must read it before checking for more work.\n\t\t */\n\t\ttnapi->last_tag = sblk->status_tag;\n\t\ttnapi->last_irq_tag = tnapi->last_tag;\n\t\trmb();\n\n\t\t/* check for RX/TX work to do */\n\t\tif (likely(sblk->idx[0].tx_consumer == tnapi->tx_cons &&\n\t\t\t   *(tnapi->rx_rcb_prod_idx) == tnapi->rx_rcb_ptr)) {\n\n\t\t\t/* This test here is not race free, but will reduce\n\t\t\t * the number of interrupts by looping again.\n\t\t\t */\n\t\t\tif (tnapi == &tp->napi[1] && tp->rx_refill)\n\t\t\t\tcontinue;\n\n\t\t\tnapi_complete(napi);\n\t\t\t/* Reenable interrupts. */\n\t\t\ttw32_mailbox(tnapi->int_mbox, tnapi->last_tag << 24);\n\n\t\t\t/* This test here is synchronized by napi_schedule()\n\t\t\t * and napi_complete() to close the race condition.\n\t\t\t */\n\t\t\tif (unlikely(tnapi == &tp->napi[1] && tp->rx_refill)) {\n\t\t\t\ttw32(HOSTCC_MODE, tp->coalesce_mode |\n\t\t\t\t\t\t  HOSTCC_MODE_ENABLE |\n\t\t\t\t\t\t  tnapi->coal_now);\n\t\t\t}\n\t\t\tmmiowb();\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn work_done;\n\ntx_recovery:\n\t/* work_done is guaranteed to be less than budget. */\n\tnapi_complete(napi);\n\ttg3_reset_task_schedule(tp);\n\treturn work_done;\n}\n\nstatic void tg3_process_error(struct tg3 *tp)\n{\n\tu32 val;\n\tbool real_error = false;\n\n\tif (tg3_flag(tp, ERROR_PROCESSED))\n\t\treturn;\n\n\t/* Check Flow Attention register */\n\tval = tr32(HOSTCC_FLOW_ATTN);\n\tif (val & ~HOSTCC_FLOW_ATTN_MBUF_LWM) {\n\t\tnetdev_err(tp->dev, \"FLOW Attention error.  Resetting chip.\\n\");\n\t\treal_error = true;\n\t}\n\n\tif (tr32(MSGINT_STATUS) & ~MSGINT_STATUS_MSI_REQ) {\n\t\tnetdev_err(tp->dev, \"MSI Status error.  Resetting chip.\\n\");\n\t\treal_error = true;\n\t}\n\n\tif (tr32(RDMAC_STATUS) || tr32(WDMAC_STATUS)) {\n\t\tnetdev_err(tp->dev, \"DMA Status error.  Resetting chip.\\n\");\n\t\treal_error = true;\n\t}\n\n\tif (!real_error)\n\t\treturn;\n\n\ttg3_dump_state(tp);\n\n\ttg3_flag_set(tp, ERROR_PROCESSED);\n\ttg3_reset_task_schedule(tp);\n}\n\nstatic int tg3_poll(struct napi_struct *napi, int budget)\n{\n\tstruct tg3_napi *tnapi = container_of(napi, struct tg3_napi, napi);\n\tstruct tg3 *tp = tnapi->tp;\n\tint work_done = 0;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\n\twhile (1) {\n\t\tif (sblk->status & SD_STATUS_ERROR)\n\t\t\ttg3_process_error(tp);\n\n\t\ttg3_poll_link(tp);\n\n\t\twork_done = tg3_poll_work(tnapi, work_done, budget);\n\n\t\tif (unlikely(tg3_flag(tp, TX_RECOVERY_PENDING)))\n\t\t\tgoto tx_recovery;\n\n\t\tif (unlikely(work_done >= budget))\n\t\t\tbreak;\n\n\t\tif (tg3_flag(tp, TAGGED_STATUS)) {\n\t\t\t/* tp->last_tag is used in tg3_int_reenable() below\n\t\t\t * to tell the hw how much work has been processed,\n\t\t\t * so we must read it before checking for more work.\n\t\t\t */\n\t\t\ttnapi->last_tag = sblk->status_tag;\n\t\t\ttnapi->last_irq_tag = tnapi->last_tag;\n\t\t\trmb();\n\t\t} else\n\t\t\tsblk->status &= ~SD_STATUS_UPDATED;\n\n\t\tif (likely(!tg3_has_work(tnapi))) {\n\t\t\tnapi_complete(napi);\n\t\t\ttg3_int_reenable(tnapi);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn work_done;\n\ntx_recovery:\n\t/* work_done is guaranteed to be less than budget. */\n\tnapi_complete(napi);\n\ttg3_reset_task_schedule(tp);\n\treturn work_done;\n}\n\nstatic void tg3_napi_disable(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = tp->irq_cnt - 1; i >= 0; i--)\n\t\tnapi_disable(&tp->napi[i].napi);\n}\n\nstatic void tg3_napi_enable(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\tnapi_enable(&tp->napi[i].napi);\n}\n\nstatic void tg3_napi_init(struct tg3 *tp)\n{\n\tint i;\n\n\tnetif_napi_add(tp->dev, &tp->napi[0].napi, tg3_poll, 64);\n\tfor (i = 1; i < tp->irq_cnt; i++)\n\t\tnetif_napi_add(tp->dev, &tp->napi[i].napi, tg3_poll_msix, 64);\n}\n\nstatic void tg3_napi_fini(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\tnetif_napi_del(&tp->napi[i].napi);\n}\n\nstatic inline void tg3_netif_stop(struct tg3 *tp)\n{\n\ttp->dev->trans_start = jiffies;\t/* prevent tx timeout */\n\ttg3_napi_disable(tp);\n\tnetif_carrier_off(tp->dev);\n\tnetif_tx_disable(tp->dev);\n}\n\n/* tp->lock must be held */\nstatic inline void tg3_netif_start(struct tg3 *tp)\n{\n\ttg3_ptp_resume(tp);\n\n\t/* NOTE: unconditional netif_tx_wake_all_queues is only\n\t * appropriate so long as all callers are assured to\n\t * have free tx slots (such as after tg3_init_hw)\n\t */\n\tnetif_tx_wake_all_queues(tp->dev);\n\n\tif (tp->link_up)\n\t\tnetif_carrier_on(tp->dev);\n\n\ttg3_napi_enable(tp);\n\ttp->napi[0].hw_status->status |= SD_STATUS_UPDATED;\n\ttg3_enable_ints(tp);\n}\n\nstatic void tg3_irq_quiesce(struct tg3 *tp)\n{\n\tint i;\n\n\tBUG_ON(tp->irq_sync);\n\n\ttp->irq_sync = 1;\n\tsmp_mb();\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\tsynchronize_irq(tp->napi[i].irq_vec);\n}\n\n/* Fully shutdown all tg3 driver activity elsewhere in the system.\n * If irq_sync is non-zero, then the IRQ handler must be synchronized\n * with as well.  Most of the time, this is not necessary except when\n * shutting down the device.\n */\nstatic inline void tg3_full_lock(struct tg3 *tp, int irq_sync)\n{\n\tspin_lock_bh(&tp->lock);\n\tif (irq_sync)\n\t\ttg3_irq_quiesce(tp);\n}\n\nstatic inline void tg3_full_unlock(struct tg3 *tp)\n{\n\tspin_unlock_bh(&tp->lock);\n}\n\n/* One-shot MSI handler - Chip automatically disables interrupt\n * after sending MSI so driver doesn't have to do it.\n */\nstatic irqreturn_t tg3_msi_1shot(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\n\tprefetch(tnapi->hw_status);\n\tif (tnapi->rx_rcb)\n\t\tprefetch(&tnapi->rx_rcb[tnapi->rx_rcb_ptr]);\n\n\tif (likely(!tg3_irq_sync(tp)))\n\t\tnapi_schedule(&tnapi->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n/* MSI ISR - No need to check for interrupt sharing and no need to\n * flush status block and interrupt mailbox. PCI ordering rules\n * guarantee that MSI will arrive after the status block.\n */\nstatic irqreturn_t tg3_msi(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\n\tprefetch(tnapi->hw_status);\n\tif (tnapi->rx_rcb)\n\t\tprefetch(&tnapi->rx_rcb[tnapi->rx_rcb_ptr]);\n\t/*\n\t * Writing any value to intr-mbox-0 clears PCI INTA# and\n\t * chip-internal interrupt pending events.\n\t * Writing non-zero to intr-mbox-0 additional tells the\n\t * NIC to stop sending us irqs, engaging \"in-intr-handler\"\n\t * event coalescing.\n\t */\n\ttw32_mailbox(tnapi->int_mbox, 0x00000001);\n\tif (likely(!tg3_irq_sync(tp)))\n\t\tnapi_schedule(&tnapi->napi);\n\n\treturn IRQ_RETVAL(1);\n}\n\nstatic irqreturn_t tg3_interrupt(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\tunsigned int handled = 1;\n\n\t/* In INTx mode, it is possible for the interrupt to arrive at\n\t * the CPU before the status block posted prior to the interrupt.\n\t * Reading the PCI State register will confirm whether the\n\t * interrupt is ours and will flush the status block.\n\t */\n\tif (unlikely(!(sblk->status & SD_STATUS_UPDATED))) {\n\t\tif (tg3_flag(tp, CHIP_RESETTING) ||\n\t\t    (tr32(TG3PCI_PCISTATE) & PCISTATE_INT_NOT_ACTIVE)) {\n\t\t\thandled = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * Writing any value to intr-mbox-0 clears PCI INTA# and\n\t * chip-internal interrupt pending events.\n\t * Writing non-zero to intr-mbox-0 additional tells the\n\t * NIC to stop sending us irqs, engaging \"in-intr-handler\"\n\t * event coalescing.\n\t *\n\t * Flush the mailbox to de-assert the IRQ immediately to prevent\n\t * spurious interrupts.  The flush impacts performance but\n\t * excessive spurious interrupts can be worse in some cases.\n\t */\n\ttw32_mailbox_f(MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW, 0x00000001);\n\tif (tg3_irq_sync(tp))\n\t\tgoto out;\n\tsblk->status &= ~SD_STATUS_UPDATED;\n\tif (likely(tg3_has_work(tnapi))) {\n\t\tprefetch(&tnapi->rx_rcb[tnapi->rx_rcb_ptr]);\n\t\tnapi_schedule(&tnapi->napi);\n\t} else {\n\t\t/* No work, shared interrupt perhaps?  re-enable\n\t\t * interrupts, and flush that PCI write\n\t\t */\n\t\ttw32_mailbox_f(MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW,\n\t\t\t       0x00000000);\n\t}\nout:\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic irqreturn_t tg3_interrupt_tagged(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\tunsigned int handled = 1;\n\n\t/* In INTx mode, it is possible for the interrupt to arrive at\n\t * the CPU before the status block posted prior to the interrupt.\n\t * Reading the PCI State register will confirm whether the\n\t * interrupt is ours and will flush the status block.\n\t */\n\tif (unlikely(sblk->status_tag == tnapi->last_irq_tag)) {\n\t\tif (tg3_flag(tp, CHIP_RESETTING) ||\n\t\t    (tr32(TG3PCI_PCISTATE) & PCISTATE_INT_NOT_ACTIVE)) {\n\t\t\thandled = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * writing any value to intr-mbox-0 clears PCI INTA# and\n\t * chip-internal interrupt pending events.\n\t * writing non-zero to intr-mbox-0 additional tells the\n\t * NIC to stop sending us irqs, engaging \"in-intr-handler\"\n\t * event coalescing.\n\t *\n\t * Flush the mailbox to de-assert the IRQ immediately to prevent\n\t * spurious interrupts.  The flush impacts performance but\n\t * excessive spurious interrupts can be worse in some cases.\n\t */\n\ttw32_mailbox_f(MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW, 0x00000001);\n\n\t/*\n\t * In a shared interrupt configuration, sometimes other devices'\n\t * interrupts will scream.  We record the current status tag here\n\t * so that the above check can report that the screaming interrupts\n\t * are unhandled.  Eventually they will be silenced.\n\t */\n\ttnapi->last_irq_tag = sblk->status_tag;\n\n\tif (tg3_irq_sync(tp))\n\t\tgoto out;\n\n\tprefetch(&tnapi->rx_rcb[tnapi->rx_rcb_ptr]);\n\n\tnapi_schedule(&tnapi->napi);\n\nout:\n\treturn IRQ_RETVAL(handled);\n}\n\n/* ISR for interrupt test */\nstatic irqreturn_t tg3_test_isr(int irq, void *dev_id)\n{\n\tstruct tg3_napi *tnapi = dev_id;\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct tg3_hw_status *sblk = tnapi->hw_status;\n\n\tif ((sblk->status & SD_STATUS_UPDATED) ||\n\t    !(tr32(TG3PCI_PCISTATE) & PCISTATE_INT_NOT_ACTIVE)) {\n\t\ttg3_disable_ints(tp);\n\t\treturn IRQ_RETVAL(1);\n\t}\n\treturn IRQ_RETVAL(0);\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void tg3_poll_controller(struct net_device *dev)\n{\n\tint i;\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tg3_irq_sync(tp))\n\t\treturn;\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\ttg3_interrupt(tp->napi[i].irq_vec, &tp->napi[i]);\n}\n#endif\n\nstatic void tg3_tx_timeout(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (netif_msg_tx_err(tp)) {\n\t\tnetdev_err(dev, \"transmit timed out, resetting\\n\");\n\t\ttg3_dump_state(tp);\n\t}\n\n\ttg3_reset_task_schedule(tp);\n}\n\n/* Test for DMA buffers crossing any 4GB boundaries: 4G, 8G, etc */\nstatic inline int tg3_4g_overflow_test(dma_addr_t mapping, int len)\n{\n\tu32 base = (u32) mapping & 0xffffffff;\n\n\treturn (base > 0xffffdcc0) && (base + len + 8 < base);\n}\n\n/* Test for DMA addresses > 40-bit */\nstatic inline int tg3_40bit_overflow_test(struct tg3 *tp, dma_addr_t mapping,\n\t\t\t\t\t  int len)\n{\n#if defined(CONFIG_HIGHMEM) && (BITS_PER_LONG == 64)\n\tif (tg3_flag(tp, 40BIT_DMA_BUG))\n\t\treturn ((u64) mapping + len) > DMA_BIT_MASK(40);\n\treturn 0;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline void tg3_tx_set_bd(struct tg3_tx_buffer_desc *txbd,\n\t\t\t\t dma_addr_t mapping, u32 len, u32 flags,\n\t\t\t\t u32 mss, u32 vlan)\n{\n\ttxbd->addr_hi = ((u64) mapping >> 32);\n\ttxbd->addr_lo = ((u64) mapping & 0xffffffff);\n\ttxbd->len_flags = (len << TXD_LEN_SHIFT) | (flags & 0x0000ffff);\n\ttxbd->vlan_tag = (mss << TXD_MSS_SHIFT) | (vlan << TXD_VLAN_TAG_SHIFT);\n}\n\nstatic bool tg3_tx_frag_set(struct tg3_napi *tnapi, u32 *entry, u32 *budget,\n\t\t\t    dma_addr_t map, u32 len, u32 flags,\n\t\t\t    u32 mss, u32 vlan)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tbool hwbug = false;\n\n\tif (tg3_flag(tp, SHORT_DMA_BUG) && len <= 8)\n\t\thwbug = true;\n\n\tif (tg3_4g_overflow_test(map, len))\n\t\thwbug = true;\n\n\tif (tg3_40bit_overflow_test(tp, map, len))\n\t\thwbug = true;\n\n\tif (tp->dma_limit) {\n\t\tu32 prvidx = *entry;\n\t\tu32 tmp_flag = flags & ~TXD_FLAG_END;\n\t\twhile (len > tp->dma_limit && *budget) {\n\t\t\tu32 frag_len = tp->dma_limit;\n\t\t\tlen -= tp->dma_limit;\n\n\t\t\t/* Avoid the 8byte DMA problem */\n\t\t\tif (len <= 8) {\n\t\t\t\tlen += tp->dma_limit / 2;\n\t\t\t\tfrag_len = tp->dma_limit / 2;\n\t\t\t}\n\n\t\t\ttnapi->tx_buffers[*entry].fragmented = true;\n\n\t\t\ttg3_tx_set_bd(&tnapi->tx_ring[*entry], map,\n\t\t\t\t      frag_len, tmp_flag, mss, vlan);\n\t\t\t*budget -= 1;\n\t\t\tprvidx = *entry;\n\t\t\t*entry = NEXT_TX(*entry);\n\n\t\t\tmap += frag_len;\n\t\t}\n\n\t\tif (len) {\n\t\t\tif (*budget) {\n\t\t\t\ttg3_tx_set_bd(&tnapi->tx_ring[*entry], map,\n\t\t\t\t\t      len, flags, mss, vlan);\n\t\t\t\t*budget -= 1;\n\t\t\t\t*entry = NEXT_TX(*entry);\n\t\t\t} else {\n\t\t\t\thwbug = true;\n\t\t\t\ttnapi->tx_buffers[prvidx].fragmented = false;\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttg3_tx_set_bd(&tnapi->tx_ring[*entry], map,\n\t\t\t      len, flags, mss, vlan);\n\t\t*entry = NEXT_TX(*entry);\n\t}\n\n\treturn hwbug;\n}\n\nstatic void tg3_tx_skb_unmap(struct tg3_napi *tnapi, u32 entry, int last)\n{\n\tint i;\n\tstruct sk_buff *skb;\n\tstruct tg3_tx_ring_info *txb = &tnapi->tx_buffers[entry];\n\n\tskb = txb->skb;\n\ttxb->skb = NULL;\n\n\tpci_unmap_single(tnapi->tp->pdev,\n\t\t\t dma_unmap_addr(txb, mapping),\n\t\t\t skb_headlen(skb),\n\t\t\t PCI_DMA_TODEVICE);\n\n\twhile (txb->fragmented) {\n\t\ttxb->fragmented = false;\n\t\tentry = NEXT_TX(entry);\n\t\ttxb = &tnapi->tx_buffers[entry];\n\t}\n\n\tfor (i = 0; i <= last; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tentry = NEXT_TX(entry);\n\t\ttxb = &tnapi->tx_buffers[entry];\n\n\t\tpci_unmap_page(tnapi->tp->pdev,\n\t\t\t       dma_unmap_addr(txb, mapping),\n\t\t\t       skb_frag_size(frag), PCI_DMA_TODEVICE);\n\n\t\twhile (txb->fragmented) {\n\t\t\ttxb->fragmented = false;\n\t\t\tentry = NEXT_TX(entry);\n\t\t\ttxb = &tnapi->tx_buffers[entry];\n\t\t}\n\t}\n}\n\n/* Workaround 4GB and 40-bit hardware DMA bugs. */\nstatic int tigon3_dma_hwbug_workaround(struct tg3_napi *tnapi,\n\t\t\t\t       struct sk_buff **pskb,\n\t\t\t\t       u32 *entry, u32 *budget,\n\t\t\t\t       u32 base_flags, u32 mss, u32 vlan)\n{\n\tstruct tg3 *tp = tnapi->tp;\n\tstruct sk_buff *new_skb, *skb = *pskb;\n\tdma_addr_t new_addr = 0;\n\tint ret = 0;\n\n\tif (tg3_asic_rev(tp) != ASIC_REV_5701)\n\t\tnew_skb = skb_copy(skb, GFP_ATOMIC);\n\telse {\n\t\tint more_headroom = 4 - ((unsigned long)skb->data & 3);\n\n\t\tnew_skb = skb_copy_expand(skb,\n\t\t\t\t\t  skb_headroom(skb) + more_headroom,\n\t\t\t\t\t  skb_tailroom(skb), GFP_ATOMIC);\n\t}\n\n\tif (!new_skb) {\n\t\tret = -1;\n\t} else {\n\t\t/* New SKB is guaranteed to be linear. */\n\t\tnew_addr = pci_map_single(tp->pdev, new_skb->data, new_skb->len,\n\t\t\t\t\t  PCI_DMA_TODEVICE);\n\t\t/* Make sure the mapping succeeded */\n\t\tif (pci_dma_mapping_error(tp->pdev, new_addr)) {\n\t\t\tdev_kfree_skb(new_skb);\n\t\t\tret = -1;\n\t\t} else {\n\t\t\tu32 save_entry = *entry;\n\n\t\t\tbase_flags |= TXD_FLAG_END;\n\n\t\t\ttnapi->tx_buffers[*entry].skb = new_skb;\n\t\t\tdma_unmap_addr_set(&tnapi->tx_buffers[*entry],\n\t\t\t\t\t   mapping, new_addr);\n\n\t\t\tif (tg3_tx_frag_set(tnapi, entry, budget, new_addr,\n\t\t\t\t\t    new_skb->len, base_flags,\n\t\t\t\t\t    mss, vlan)) {\n\t\t\t\ttg3_tx_skb_unmap(tnapi, save_entry, -1);\n\t\t\t\tdev_kfree_skb(new_skb);\n\t\t\t\tret = -1;\n\t\t\t}\n\t\t}\n\t}\n\n\tdev_kfree_skb(skb);\n\t*pskb = new_skb;\n\treturn ret;\n}\n\nstatic netdev_tx_t tg3_start_xmit(struct sk_buff *, struct net_device *);\n\n/* Use GSO to workaround a rare TSO bug that may be triggered when the\n * TSO header is greater than 80 bytes.\n */\nstatic int tg3_tso_bug(struct tg3 *tp, struct sk_buff *skb)\n{\n\tstruct sk_buff *segs, *nskb;\n\tu32 frag_cnt_est = skb_shinfo(skb)->gso_segs * 3;\n\n\t/* Estimate the number of fragments in the worst case */\n\tif (unlikely(tg3_tx_avail(&tp->napi[0]) <= frag_cnt_est)) {\n\t\tnetif_stop_queue(tp->dev);\n\n\t\t/* netif_tx_stop_queue() must be done before checking\n\t\t * checking tx index in tg3_tx_avail() below, because in\n\t\t * tg3_tx(), we update tx index before checking for\n\t\t * netif_tx_queue_stopped().\n\t\t */\n\t\tsmp_mb();\n\t\tif (tg3_tx_avail(&tp->napi[0]) <= frag_cnt_est)\n\t\t\treturn NETDEV_TX_BUSY;\n\n\t\tnetif_wake_queue(tp->dev);\n\t}\n\n\tsegs = skb_gso_segment(skb, tp->dev->features & ~NETIF_F_TSO);\n\tif (IS_ERR(segs))\n\t\tgoto tg3_tso_bug_end;\n\n\tdo {\n\t\tnskb = segs;\n\t\tsegs = segs->next;\n\t\tnskb->next = NULL;\n\t\ttg3_start_xmit(nskb, tp->dev);\n\t} while (segs);\n\ntg3_tso_bug_end:\n\tdev_kfree_skb(skb);\n\n\treturn NETDEV_TX_OK;\n}\n\n/* hard_start_xmit for devices that have the 4G bug and/or 40-bit bug and\n * support TG3_FLAG_HW_TSO_1 or firmware TSO only.\n */\nstatic netdev_tx_t tg3_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 len, entry, base_flags, mss, vlan = 0;\n\tu32 budget;\n\tint i = -1, would_hit_hwbug;\n\tdma_addr_t mapping;\n\tstruct tg3_napi *tnapi;\n\tstruct netdev_queue *txq;\n\tunsigned int last;\n\n\ttxq = netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\n\ttnapi = &tp->napi[skb_get_queue_mapping(skb)];\n\tif (tg3_flag(tp, ENABLE_TSS))\n\t\ttnapi++;\n\n\tbudget = tg3_tx_avail(tnapi);\n\n\t/* We are running in BH disabled context with netif_tx_lock\n\t * and TX reclaim runs via tp->napi.poll inside of a software\n\t * interrupt.  Furthermore, IRQ processing runs lockless so we have\n\t * no IRQ context deadlocks to worry about either.  Rejoice!\n\t */\n\tif (unlikely(budget <= (skb_shinfo(skb)->nr_frags + 1))) {\n\t\tif (!netif_tx_queue_stopped(txq)) {\n\t\t\tnetif_tx_stop_queue(txq);\n\n\t\t\t/* This is a hard error, log it. */\n\t\t\tnetdev_err(dev,\n\t\t\t\t   \"BUG! Tx Ring full when queue awake!\\n\");\n\t\t}\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tentry = tnapi->tx_prod;\n\tbase_flags = 0;\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tbase_flags |= TXD_FLAG_TCPUDP_CSUM;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (mss) {\n\t\tstruct iphdr *iph;\n\t\tu32 tcp_opt_len, hdr_len;\n\n\t\tif (skb_header_cloned(skb) &&\n\t\t    pskb_expand_head(skb, 0, 0, GFP_ATOMIC))\n\t\t\tgoto drop;\n\n\t\tiph = ip_hdr(skb);\n\t\ttcp_opt_len = tcp_optlen(skb);\n\n\t\thdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb) - ETH_HLEN;\n\n\t\tif (!skb_is_gso_v6(skb)) {\n\t\t\tiph->check = 0;\n\t\t\tiph->tot_len = htons(mss + hdr_len);\n\t\t}\n\n\t\tif (unlikely((ETH_HLEN + hdr_len) > 80) &&\n\t\t    tg3_flag(tp, TSO_BUG))\n\t\t\treturn tg3_tso_bug(tp, skb);\n\n\t\tbase_flags |= (TXD_FLAG_CPU_PRE_DMA |\n\t\t\t       TXD_FLAG_CPU_POST_DMA);\n\n\t\tif (tg3_flag(tp, HW_TSO_1) ||\n\t\t    tg3_flag(tp, HW_TSO_2) ||\n\t\t    tg3_flag(tp, HW_TSO_3)) {\n\t\t\ttcp_hdr(skb)->check = 0;\n\t\t\tbase_flags &= ~TXD_FLAG_TCPUDP_CSUM;\n\t\t} else\n\t\t\ttcp_hdr(skb)->check = ~csum_tcpudp_magic(iph->saddr,\n\t\t\t\t\t\t\t\t iph->daddr, 0,\n\t\t\t\t\t\t\t\t IPPROTO_TCP,\n\t\t\t\t\t\t\t\t 0);\n\n\t\tif (tg3_flag(tp, HW_TSO_3)) {\n\t\t\tmss |= (hdr_len & 0xc) << 12;\n\t\t\tif (hdr_len & 0x10)\n\t\t\t\tbase_flags |= 0x00000010;\n\t\t\tbase_flags |= (hdr_len & 0x3e0) << 5;\n\t\t} else if (tg3_flag(tp, HW_TSO_2))\n\t\t\tmss |= hdr_len << 9;\n\t\telse if (tg3_flag(tp, HW_TSO_1) ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\t\tif (tcp_opt_len || iph->ihl > 5) {\n\t\t\t\tint tsflags;\n\n\t\t\t\ttsflags = (iph->ihl - 5) + (tcp_opt_len >> 2);\n\t\t\t\tmss |= (tsflags << 11);\n\t\t\t}\n\t\t} else {\n\t\t\tif (tcp_opt_len || iph->ihl > 5) {\n\t\t\t\tint tsflags;\n\n\t\t\t\ttsflags = (iph->ihl - 5) + (tcp_opt_len >> 2);\n\t\t\t\tbase_flags |= tsflags << 12;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, USE_JUMBO_BDFLAG) &&\n\t    !mss && skb->len > VLAN_ETH_FRAME_LEN)\n\t\tbase_flags |= TXD_FLAG_JMB_PKT;\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tbase_flags |= TXD_FLAG_VLAN;\n\t\tvlan = vlan_tx_tag_get(skb);\n\t}\n\n\tif ((unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) &&\n\t    tg3_flag(tp, TX_TSTAMP_EN)) {\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\tbase_flags |= TXD_FLAG_HWTSTAMP;\n\t}\n\n\tlen = skb_headlen(skb);\n\n\tmapping = pci_map_single(tp->pdev, skb->data, len, PCI_DMA_TODEVICE);\n\tif (pci_dma_mapping_error(tp->pdev, mapping))\n\t\tgoto drop;\n\n\n\ttnapi->tx_buffers[entry].skb = skb;\n\tdma_unmap_addr_set(&tnapi->tx_buffers[entry], mapping, mapping);\n\n\twould_hit_hwbug = 0;\n\n\tif (tg3_flag(tp, 5701_DMA_BUG))\n\t\twould_hit_hwbug = 1;\n\n\tif (tg3_tx_frag_set(tnapi, &entry, &budget, mapping, len, base_flags |\n\t\t\t  ((skb_shinfo(skb)->nr_frags == 0) ? TXD_FLAG_END : 0),\n\t\t\t    mss, vlan)) {\n\t\twould_hit_hwbug = 1;\n\t} else if (skb_shinfo(skb)->nr_frags > 0) {\n\t\tu32 tmp_mss = mss;\n\n\t\tif (!tg3_flag(tp, HW_TSO_1) &&\n\t\t    !tg3_flag(tp, HW_TSO_2) &&\n\t\t    !tg3_flag(tp, HW_TSO_3))\n\t\t\ttmp_mss = 0;\n\n\t\t/* Now loop through additional data\n\t\t * fragments, and queue them.\n\t\t */\n\t\tlast = skb_shinfo(skb)->nr_frags - 1;\n\t\tfor (i = 0; i <= last; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tlen = skb_frag_size(frag);\n\t\t\tmapping = skb_frag_dma_map(&tp->pdev->dev, frag, 0,\n\t\t\t\t\t\t   len, DMA_TO_DEVICE);\n\n\t\t\ttnapi->tx_buffers[entry].skb = NULL;\n\t\t\tdma_unmap_addr_set(&tnapi->tx_buffers[entry], mapping,\n\t\t\t\t\t   mapping);\n\t\t\tif (dma_mapping_error(&tp->pdev->dev, mapping))\n\t\t\t\tgoto dma_error;\n\n\t\t\tif (!budget ||\n\t\t\t    tg3_tx_frag_set(tnapi, &entry, &budget, mapping,\n\t\t\t\t\t    len, base_flags |\n\t\t\t\t\t    ((i == last) ? TXD_FLAG_END : 0),\n\t\t\t\t\t    tmp_mss, vlan)) {\n\t\t\t\twould_hit_hwbug = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (would_hit_hwbug) {\n\t\ttg3_tx_skb_unmap(tnapi, tnapi->tx_prod, i);\n\n\t\t/* If the workaround fails due to memory/mapping\n\t\t * failure, silently drop this packet.\n\t\t */\n\t\tentry = tnapi->tx_prod;\n\t\tbudget = tg3_tx_avail(tnapi);\n\t\tif (tigon3_dma_hwbug_workaround(tnapi, &skb, &entry, &budget,\n\t\t\t\t\t\tbase_flags, mss, vlan))\n\t\t\tgoto drop_nofree;\n\t}\n\n\tskb_tx_timestamp(skb);\n\tnetdev_tx_sent_queue(txq, skb->len);\n\n\t/* Sync BD data before updating mailbox */\n\twmb();\n\n\t/* Packets are ready, update Tx producer idx local and on card. */\n\ttw32_tx_mbox(tnapi->prodmbox, entry);\n\n\ttnapi->tx_prod = entry;\n\tif (unlikely(tg3_tx_avail(tnapi) <= (MAX_SKB_FRAGS + 1))) {\n\t\tnetif_tx_stop_queue(txq);\n\n\t\t/* netif_tx_stop_queue() must be done before checking\n\t\t * checking tx index in tg3_tx_avail() below, because in\n\t\t * tg3_tx(), we update tx index before checking for\n\t\t * netif_tx_queue_stopped().\n\t\t */\n\t\tsmp_mb();\n\t\tif (tg3_tx_avail(tnapi) > TG3_TX_WAKEUP_THRESH(tnapi))\n\t\t\tnetif_tx_wake_queue(txq);\n\t}\n\n\tmmiowb();\n\treturn NETDEV_TX_OK;\n\ndma_error:\n\ttg3_tx_skb_unmap(tnapi, tnapi->tx_prod, --i);\n\ttnapi->tx_buffers[tnapi->tx_prod].skb = NULL;\ndrop:\n\tdev_kfree_skb(skb);\ndrop_nofree:\n\ttp->tx_dropped++;\n\treturn NETDEV_TX_OK;\n}\n\nstatic void tg3_mac_loopback(struct tg3 *tp, bool enable)\n{\n\tif (enable) {\n\t\ttp->mac_mode &= ~(MAC_MODE_HALF_DUPLEX |\n\t\t\t\t  MAC_MODE_PORT_MODE_MASK);\n\n\t\ttp->mac_mode |= MAC_MODE_PORT_INT_LPBACK;\n\n\t\tif (!tg3_flag(tp, 5705_PLUS))\n\t\t\ttp->mac_mode |= MAC_MODE_LINK_POLARITY;\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_10_100_ONLY)\n\t\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_MII;\n\t\telse\n\t\t\ttp->mac_mode |= MAC_MODE_PORT_MODE_GMII;\n\t} else {\n\t\ttp->mac_mode &= ~MAC_MODE_PORT_INT_LPBACK;\n\n\t\tif (tg3_flag(tp, 5705_PLUS) ||\n\t\t    (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5700)\n\t\t\ttp->mac_mode &= ~MAC_MODE_LINK_POLARITY;\n\t}\n\n\ttw32(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n}\n\nstatic int tg3_phy_lpbk_set(struct tg3 *tp, u32 speed, bool extlpbk)\n{\n\tu32 val, bmcr, mac_mode, ptest = 0;\n\n\ttg3_phy_toggle_apd(tp, false);\n\ttg3_phy_toggle_automdix(tp, 0);\n\n\tif (extlpbk && tg3_phy_set_extloopbk(tp))\n\t\treturn -EIO;\n\n\tbmcr = BMCR_FULLDPLX;\n\tswitch (speed) {\n\tcase SPEED_10:\n\t\tbreak;\n\tcase SPEED_100:\n\t\tbmcr |= BMCR_SPEED100;\n\t\tbreak;\n\tcase SPEED_1000:\n\tdefault:\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_FET) {\n\t\t\tspeed = SPEED_100;\n\t\t\tbmcr |= BMCR_SPEED100;\n\t\t} else {\n\t\t\tspeed = SPEED_1000;\n\t\t\tbmcr |= BMCR_SPEED1000;\n\t\t}\n\t}\n\n\tif (extlpbk) {\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_FET)) {\n\t\t\ttg3_readphy(tp, MII_CTRL1000, &val);\n\t\t\tval |= CTL1000_AS_MASTER |\n\t\t\t       CTL1000_ENABLE_MASTER;\n\t\t\ttg3_writephy(tp, MII_CTRL1000, val);\n\t\t} else {\n\t\t\tptest = MII_TG3_FET_PTEST_TRIM_SEL |\n\t\t\t\tMII_TG3_FET_PTEST_TRIM_2;\n\t\t\ttg3_writephy(tp, MII_TG3_FET_PTEST, ptest);\n\t\t}\n\t} else\n\t\tbmcr |= BMCR_LOOPBACK;\n\n\ttg3_writephy(tp, MII_BMCR, bmcr);\n\n\t/* The write needs to be flushed for the FETs */\n\tif (tp->phy_flags & TG3_PHYFLG_IS_FET)\n\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\n\tudelay(40);\n\n\tif ((tp->phy_flags & TG3_PHYFLG_IS_FET) &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5785) {\n\t\ttg3_writephy(tp, MII_TG3_FET_PTEST, ptest |\n\t\t\t     MII_TG3_FET_PTEST_FRC_TX_LINK |\n\t\t\t     MII_TG3_FET_PTEST_FRC_TX_LOCK);\n\n\t\t/* The write needs to be flushed for the AC131 */\n\t\ttg3_readphy(tp, MII_TG3_FET_PTEST, &val);\n\t}\n\n\t/* Reset to prevent losing 1st rx packet intermittently */\n\tif ((tp->phy_flags & TG3_PHYFLG_MII_SERDES) &&\n\t    tg3_flag(tp, 5780_CLASS)) {\n\t\ttw32_f(MAC_RX_MODE, RX_MODE_RESET);\n\t\tudelay(10);\n\t\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\t}\n\n\tmac_mode = tp->mac_mode &\n\t\t   ~(MAC_MODE_PORT_MODE_MASK | MAC_MODE_HALF_DUPLEX);\n\tif (speed == SPEED_1000)\n\t\tmac_mode |= MAC_MODE_PORT_MODE_GMII;\n\telse\n\t\tmac_mode |= MAC_MODE_PORT_MODE_MII;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700) {\n\t\tu32 masked_phy_id = tp->phy_id & TG3_PHY_ID_MASK;\n\n\t\tif (masked_phy_id == TG3_PHY_ID_BCM5401)\n\t\t\tmac_mode &= ~MAC_MODE_LINK_POLARITY;\n\t\telse if (masked_phy_id == TG3_PHY_ID_BCM5411)\n\t\t\tmac_mode |= MAC_MODE_LINK_POLARITY;\n\n\t\ttg3_writephy(tp, MII_TG3_EXT_CTRL,\n\t\t\t     MII_TG3_EXT_CTRL_LNK3_LED_MODE);\n\t}\n\n\ttw32(MAC_MODE, mac_mode);\n\tudelay(40);\n\n\treturn 0;\n}\n\nstatic void tg3_set_loopback(struct net_device *dev, netdev_features_t features)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (features & NETIF_F_LOOPBACK) {\n\t\tif (tp->mac_mode & MAC_MODE_PORT_INT_LPBACK)\n\t\t\treturn;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\ttg3_mac_loopback(tp, true);\n\t\tnetif_carrier_on(tp->dev);\n\t\tspin_unlock_bh(&tp->lock);\n\t\tnetdev_info(dev, \"Internal MAC loopback mode enabled.\\n\");\n\t} else {\n\t\tif (!(tp->mac_mode & MAC_MODE_PORT_INT_LPBACK))\n\t\t\treturn;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\ttg3_mac_loopback(tp, false);\n\t\t/* Force link status check */\n\t\ttg3_setup_phy(tp, 1);\n\t\tspin_unlock_bh(&tp->lock);\n\t\tnetdev_info(dev, \"Internal MAC loopback mode disabled.\\n\");\n\t}\n}\n\nstatic netdev_features_t tg3_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (dev->mtu > ETH_DATA_LEN && tg3_flag(tp, 5780_CLASS))\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\n\treturn features;\n}\n\nstatic int tg3_set_features(struct net_device *dev, netdev_features_t features)\n{\n\tnetdev_features_t changed = dev->features ^ features;\n\n\tif ((changed & NETIF_F_LOOPBACK) && netif_running(dev))\n\t\ttg3_set_loopback(dev, features);\n\n\treturn 0;\n}\n\nstatic void tg3_rx_prodring_free(struct tg3 *tp,\n\t\t\t\t struct tg3_rx_prodring_set *tpr)\n{\n\tint i;\n\n\tif (tpr != &tp->napi[0].prodring) {\n\t\tfor (i = tpr->rx_std_cons_idx; i != tpr->rx_std_prod_idx;\n\t\t     i = (i + 1) & tp->rx_std_ring_mask)\n\t\t\ttg3_rx_data_free(tp, &tpr->rx_std_buffers[i],\n\t\t\t\t\ttp->rx_pkt_map_sz);\n\n\t\tif (tg3_flag(tp, JUMBO_CAPABLE)) {\n\t\t\tfor (i = tpr->rx_jmb_cons_idx;\n\t\t\t     i != tpr->rx_jmb_prod_idx;\n\t\t\t     i = (i + 1) & tp->rx_jmb_ring_mask) {\n\t\t\t\ttg3_rx_data_free(tp, &tpr->rx_jmb_buffers[i],\n\t\t\t\t\t\tTG3_RX_JMB_MAP_SZ);\n\t\t\t}\n\t\t}\n\n\t\treturn;\n\t}\n\n\tfor (i = 0; i <= tp->rx_std_ring_mask; i++)\n\t\ttg3_rx_data_free(tp, &tpr->rx_std_buffers[i],\n\t\t\t\ttp->rx_pkt_map_sz);\n\n\tif (tg3_flag(tp, JUMBO_CAPABLE) && !tg3_flag(tp, 5780_CLASS)) {\n\t\tfor (i = 0; i <= tp->rx_jmb_ring_mask; i++)\n\t\t\ttg3_rx_data_free(tp, &tpr->rx_jmb_buffers[i],\n\t\t\t\t\tTG3_RX_JMB_MAP_SZ);\n\t}\n}\n\n/* Initialize rx rings for packet processing.\n *\n * The chip has been shut down and the driver detached from\n * the networking, so no interrupts or new tx packets will\n * end up in the driver.  tp->{tx,}lock are held and thus\n * we may not sleep.\n */\nstatic int tg3_rx_prodring_alloc(struct tg3 *tp,\n\t\t\t\t struct tg3_rx_prodring_set *tpr)\n{\n\tu32 i, rx_pkt_dma_sz;\n\n\ttpr->rx_std_cons_idx = 0;\n\ttpr->rx_std_prod_idx = 0;\n\ttpr->rx_jmb_cons_idx = 0;\n\ttpr->rx_jmb_prod_idx = 0;\n\n\tif (tpr != &tp->napi[0].prodring) {\n\t\tmemset(&tpr->rx_std_buffers[0], 0,\n\t\t       TG3_RX_STD_BUFF_RING_SIZE(tp));\n\t\tif (tpr->rx_jmb_buffers)\n\t\t\tmemset(&tpr->rx_jmb_buffers[0], 0,\n\t\t\t       TG3_RX_JMB_BUFF_RING_SIZE(tp));\n\t\tgoto done;\n\t}\n\n\t/* Zero out all descriptors. */\n\tmemset(tpr->rx_std, 0, TG3_RX_STD_RING_BYTES(tp));\n\n\trx_pkt_dma_sz = TG3_RX_STD_DMA_SZ;\n\tif (tg3_flag(tp, 5780_CLASS) &&\n\t    tp->dev->mtu > ETH_DATA_LEN)\n\t\trx_pkt_dma_sz = TG3_RX_JMB_DMA_SZ;\n\ttp->rx_pkt_map_sz = TG3_RX_DMA_TO_MAP_SZ(rx_pkt_dma_sz);\n\n\t/* Initialize invariants of the rings, we only set this\n\t * stuff once.  This works because the card does not\n\t * write into the rx buffer posting rings.\n\t */\n\tfor (i = 0; i <= tp->rx_std_ring_mask; i++) {\n\t\tstruct tg3_rx_buffer_desc *rxd;\n\n\t\trxd = &tpr->rx_std[i];\n\t\trxd->idx_len = rx_pkt_dma_sz << RXD_LEN_SHIFT;\n\t\trxd->type_flags = (RXD_FLAG_END << RXD_FLAGS_SHIFT);\n\t\trxd->opaque = (RXD_OPAQUE_RING_STD |\n\t\t\t       (i << RXD_OPAQUE_INDEX_SHIFT));\n\t}\n\n\t/* Now allocate fresh SKBs for each rx ring. */\n\tfor (i = 0; i < tp->rx_pending; i++) {\n\t\tunsigned int frag_size;\n\n\t\tif (tg3_alloc_rx_data(tp, tpr, RXD_OPAQUE_RING_STD, i,\n\t\t\t\t      &frag_size) < 0) {\n\t\t\tnetdev_warn(tp->dev,\n\t\t\t\t    \"Using a smaller RX standard ring. Only \"\n\t\t\t\t    \"%d out of %d buffers were allocated \"\n\t\t\t\t    \"successfully\\n\", i, tp->rx_pending);\n\t\t\tif (i == 0)\n\t\t\t\tgoto initfail;\n\t\t\ttp->rx_pending = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!tg3_flag(tp, JUMBO_CAPABLE) || tg3_flag(tp, 5780_CLASS))\n\t\tgoto done;\n\n\tmemset(tpr->rx_jmb, 0, TG3_RX_JMB_RING_BYTES(tp));\n\n\tif (!tg3_flag(tp, JUMBO_RING_ENABLE))\n\t\tgoto done;\n\n\tfor (i = 0; i <= tp->rx_jmb_ring_mask; i++) {\n\t\tstruct tg3_rx_buffer_desc *rxd;\n\n\t\trxd = &tpr->rx_jmb[i].std;\n\t\trxd->idx_len = TG3_RX_JMB_DMA_SZ << RXD_LEN_SHIFT;\n\t\trxd->type_flags = (RXD_FLAG_END << RXD_FLAGS_SHIFT) |\n\t\t\t\t  RXD_FLAG_JUMBO;\n\t\trxd->opaque = (RXD_OPAQUE_RING_JUMBO |\n\t\t       (i << RXD_OPAQUE_INDEX_SHIFT));\n\t}\n\n\tfor (i = 0; i < tp->rx_jumbo_pending; i++) {\n\t\tunsigned int frag_size;\n\n\t\tif (tg3_alloc_rx_data(tp, tpr, RXD_OPAQUE_RING_JUMBO, i,\n\t\t\t\t      &frag_size) < 0) {\n\t\t\tnetdev_warn(tp->dev,\n\t\t\t\t    \"Using a smaller RX jumbo ring. Only %d \"\n\t\t\t\t    \"out of %d buffers were allocated \"\n\t\t\t\t    \"successfully\\n\", i, tp->rx_jumbo_pending);\n\t\t\tif (i == 0)\n\t\t\t\tgoto initfail;\n\t\t\ttp->rx_jumbo_pending = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\ndone:\n\treturn 0;\n\ninitfail:\n\ttg3_rx_prodring_free(tp, tpr);\n\treturn -ENOMEM;\n}\n\nstatic void tg3_rx_prodring_fini(struct tg3 *tp,\n\t\t\t\t struct tg3_rx_prodring_set *tpr)\n{\n\tkfree(tpr->rx_std_buffers);\n\ttpr->rx_std_buffers = NULL;\n\tkfree(tpr->rx_jmb_buffers);\n\ttpr->rx_jmb_buffers = NULL;\n\tif (tpr->rx_std) {\n\t\tdma_free_coherent(&tp->pdev->dev, TG3_RX_STD_RING_BYTES(tp),\n\t\t\t\t  tpr->rx_std, tpr->rx_std_mapping);\n\t\ttpr->rx_std = NULL;\n\t}\n\tif (tpr->rx_jmb) {\n\t\tdma_free_coherent(&tp->pdev->dev, TG3_RX_JMB_RING_BYTES(tp),\n\t\t\t\t  tpr->rx_jmb, tpr->rx_jmb_mapping);\n\t\ttpr->rx_jmb = NULL;\n\t}\n}\n\nstatic int tg3_rx_prodring_init(struct tg3 *tp,\n\t\t\t\tstruct tg3_rx_prodring_set *tpr)\n{\n\ttpr->rx_std_buffers = kzalloc(TG3_RX_STD_BUFF_RING_SIZE(tp),\n\t\t\t\t      GFP_KERNEL);\n\tif (!tpr->rx_std_buffers)\n\t\treturn -ENOMEM;\n\n\ttpr->rx_std = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t TG3_RX_STD_RING_BYTES(tp),\n\t\t\t\t\t &tpr->rx_std_mapping,\n\t\t\t\t\t GFP_KERNEL);\n\tif (!tpr->rx_std)\n\t\tgoto err_out;\n\n\tif (tg3_flag(tp, JUMBO_CAPABLE) && !tg3_flag(tp, 5780_CLASS)) {\n\t\ttpr->rx_jmb_buffers = kzalloc(TG3_RX_JMB_BUFF_RING_SIZE(tp),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!tpr->rx_jmb_buffers)\n\t\t\tgoto err_out;\n\n\t\ttpr->rx_jmb = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t\t TG3_RX_JMB_RING_BYTES(tp),\n\t\t\t\t\t\t &tpr->rx_jmb_mapping,\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (!tpr->rx_jmb)\n\t\t\tgoto err_out;\n\t}\n\n\treturn 0;\n\nerr_out:\n\ttg3_rx_prodring_fini(tp, tpr);\n\treturn -ENOMEM;\n}\n\n/* Free up pending packets in all rx/tx rings.\n *\n * The chip has been shut down and the driver detached from\n * the networking, so no interrupts or new tx packets will\n * end up in the driver.  tp->{tx,}lock is not held and we are not\n * in an interrupt context and thus may sleep.\n */\nstatic void tg3_free_rings(struct tg3 *tp)\n{\n\tint i, j;\n\n\tfor (j = 0; j < tp->irq_cnt; j++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[j];\n\n\t\ttg3_rx_prodring_free(tp, &tnapi->prodring);\n\n\t\tif (!tnapi->tx_buffers)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < TG3_TX_RING_SIZE; i++) {\n\t\t\tstruct sk_buff *skb = tnapi->tx_buffers[i].skb;\n\n\t\t\tif (!skb)\n\t\t\t\tcontinue;\n\n\t\t\ttg3_tx_skb_unmap(tnapi, i,\n\t\t\t\t\t skb_shinfo(skb)->nr_frags - 1);\n\n\t\t\tdev_kfree_skb_any(skb);\n\t\t}\n\t\tnetdev_tx_reset_queue(netdev_get_tx_queue(tp->dev, j));\n\t}\n}\n\n/* Initialize tx/rx rings for packet processing.\n *\n * The chip has been shut down and the driver detached from\n * the networking, so no interrupts or new tx packets will\n * end up in the driver.  tp->{tx,}lock are held and thus\n * we may not sleep.\n */\nstatic int tg3_init_rings(struct tg3 *tp)\n{\n\tint i;\n\n\t/* Free up all the SKBs. */\n\ttg3_free_rings(tp);\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\ttnapi->last_tag = 0;\n\t\ttnapi->last_irq_tag = 0;\n\t\ttnapi->hw_status->status = 0;\n\t\ttnapi->hw_status->status_tag = 0;\n\t\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\n\t\ttnapi->tx_prod = 0;\n\t\ttnapi->tx_cons = 0;\n\t\tif (tnapi->tx_ring)\n\t\t\tmemset(tnapi->tx_ring, 0, TG3_TX_RING_BYTES);\n\n\t\ttnapi->rx_rcb_ptr = 0;\n\t\tif (tnapi->rx_rcb)\n\t\t\tmemset(tnapi->rx_rcb, 0, TG3_RX_RCB_RING_BYTES(tp));\n\n\t\tif (tg3_rx_prodring_alloc(tp, &tnapi->prodring)) {\n\t\t\ttg3_free_rings(tp);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_mem_tx_release(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_max; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\tif (tnapi->tx_ring) {\n\t\t\tdma_free_coherent(&tp->pdev->dev, TG3_TX_RING_BYTES,\n\t\t\t\ttnapi->tx_ring, tnapi->tx_desc_mapping);\n\t\t\ttnapi->tx_ring = NULL;\n\t\t}\n\n\t\tkfree(tnapi->tx_buffers);\n\t\ttnapi->tx_buffers = NULL;\n\t}\n}\n\nstatic int tg3_mem_tx_acquire(struct tg3 *tp)\n{\n\tint i;\n\tstruct tg3_napi *tnapi = &tp->napi[0];\n\n\t/* If multivector TSS is enabled, vector 0 does not handle\n\t * tx interrupts.  Don't allocate any resources for it.\n\t */\n\tif (tg3_flag(tp, ENABLE_TSS))\n\t\ttnapi++;\n\n\tfor (i = 0; i < tp->txq_cnt; i++, tnapi++) {\n\t\ttnapi->tx_buffers = kzalloc(sizeof(struct tg3_tx_ring_info) *\n\t\t\t\t\t    TG3_TX_RING_SIZE, GFP_KERNEL);\n\t\tif (!tnapi->tx_buffers)\n\t\t\tgoto err_out;\n\n\t\ttnapi->tx_ring = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t\t    TG3_TX_RING_BYTES,\n\t\t\t\t\t\t    &tnapi->tx_desc_mapping,\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!tnapi->tx_ring)\n\t\t\tgoto err_out;\n\t}\n\n\treturn 0;\n\nerr_out:\n\ttg3_mem_tx_release(tp);\n\treturn -ENOMEM;\n}\n\nstatic void tg3_mem_rx_release(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_max; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\ttg3_rx_prodring_fini(tp, &tnapi->prodring);\n\n\t\tif (!tnapi->rx_rcb)\n\t\t\tcontinue;\n\n\t\tdma_free_coherent(&tp->pdev->dev,\n\t\t\t\t  TG3_RX_RCB_RING_BYTES(tp),\n\t\t\t\t  tnapi->rx_rcb,\n\t\t\t\t  tnapi->rx_rcb_mapping);\n\t\ttnapi->rx_rcb = NULL;\n\t}\n}\n\nstatic int tg3_mem_rx_acquire(struct tg3 *tp)\n{\n\tunsigned int i, limit;\n\n\tlimit = tp->rxq_cnt;\n\n\t/* If RSS is enabled, we need a (dummy) producer ring\n\t * set on vector zero.  This is the true hw prodring.\n\t */\n\tif (tg3_flag(tp, ENABLE_RSS))\n\t\tlimit++;\n\n\tfor (i = 0; i < limit; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\tif (tg3_rx_prodring_init(tp, &tnapi->prodring))\n\t\t\tgoto err_out;\n\n\t\t/* If multivector RSS is enabled, vector 0\n\t\t * does not handle rx or tx interrupts.\n\t\t * Don't allocate any resources for it.\n\t\t */\n\t\tif (!i && tg3_flag(tp, ENABLE_RSS))\n\t\t\tcontinue;\n\n\t\ttnapi->rx_rcb = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t\t   TG3_RX_RCB_RING_BYTES(tp),\n\t\t\t\t\t\t   &tnapi->rx_rcb_mapping,\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!tnapi->rx_rcb)\n\t\t\tgoto err_out;\n\n\t\tmemset(tnapi->rx_rcb, 0, TG3_RX_RCB_RING_BYTES(tp));\n\t}\n\n\treturn 0;\n\nerr_out:\n\ttg3_mem_rx_release(tp);\n\treturn -ENOMEM;\n}\n\n/*\n * Must not be invoked with interrupt sources disabled and\n * the hardware shutdown down.\n */\nstatic void tg3_free_consistent(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\tif (tnapi->hw_status) {\n\t\t\tdma_free_coherent(&tp->pdev->dev, TG3_HW_STATUS_SIZE,\n\t\t\t\t\t  tnapi->hw_status,\n\t\t\t\t\t  tnapi->status_mapping);\n\t\t\ttnapi->hw_status = NULL;\n\t\t}\n\t}\n\n\ttg3_mem_rx_release(tp);\n\ttg3_mem_tx_release(tp);\n\n\tif (tp->hw_stats) {\n\t\tdma_free_coherent(&tp->pdev->dev, sizeof(struct tg3_hw_stats),\n\t\t\t\t  tp->hw_stats, tp->stats_mapping);\n\t\ttp->hw_stats = NULL;\n\t}\n}\n\n/*\n * Must not be invoked with interrupt sources disabled and\n * the hardware shutdown down.  Can sleep.\n */\nstatic int tg3_alloc_consistent(struct tg3 *tp)\n{\n\tint i;\n\n\ttp->hw_stats = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t  sizeof(struct tg3_hw_stats),\n\t\t\t\t\t  &tp->stats_mapping,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!tp->hw_stats)\n\t\tgoto err_out;\n\n\tmemset(tp->hw_stats, 0, sizeof(struct tg3_hw_stats));\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tstruct tg3_hw_status *sblk;\n\n\t\ttnapi->hw_status = dma_alloc_coherent(&tp->pdev->dev,\n\t\t\t\t\t\t      TG3_HW_STATUS_SIZE,\n\t\t\t\t\t\t      &tnapi->status_mapping,\n\t\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!tnapi->hw_status)\n\t\t\tgoto err_out;\n\n\t\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\t\tsblk = tnapi->hw_status;\n\n\t\tif (tg3_flag(tp, ENABLE_RSS)) {\n\t\t\tu16 *prodptr = NULL;\n\n\t\t\t/*\n\t\t\t * When RSS is enabled, the status block format changes\n\t\t\t * slightly.  The \"rx_jumbo_consumer\", \"reserved\",\n\t\t\t * and \"rx_mini_consumer\" members get mapped to the\n\t\t\t * other three rx return ring producer indexes.\n\t\t\t */\n\t\t\tswitch (i) {\n\t\t\tcase 1:\n\t\t\t\tprodptr = &sblk->idx[0].rx_producer;\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tprodptr = &sblk->rx_jumbo_consumer;\n\t\t\t\tbreak;\n\t\t\tcase 3:\n\t\t\t\tprodptr = &sblk->reserved;\n\t\t\t\tbreak;\n\t\t\tcase 4:\n\t\t\t\tprodptr = &sblk->rx_mini_consumer;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttnapi->rx_rcb_prod_idx = prodptr;\n\t\t} else {\n\t\t\ttnapi->rx_rcb_prod_idx = &sblk->idx[0].rx_producer;\n\t\t}\n\t}\n\n\tif (tg3_mem_tx_acquire(tp) || tg3_mem_rx_acquire(tp))\n\t\tgoto err_out;\n\n\treturn 0;\n\nerr_out:\n\ttg3_free_consistent(tp);\n\treturn -ENOMEM;\n}\n\n#define MAX_WAIT_CNT 1000\n\n/* To stop a block, clear the enable bit and poll till it\n * clears.  tp->lock is held.\n */\nstatic int tg3_stop_block(struct tg3 *tp, unsigned long ofs, u32 enable_bit, int silent)\n{\n\tunsigned int i;\n\tu32 val;\n\n\tif (tg3_flag(tp, 5705_PLUS)) {\n\t\tswitch (ofs) {\n\t\tcase RCVLSC_MODE:\n\t\tcase DMAC_MODE:\n\t\tcase MBFREE_MODE:\n\t\tcase BUFMGR_MODE:\n\t\tcase MEMARB_MODE:\n\t\t\t/* We can't enable/disable these bits of the\n\t\t\t * 5705/5750, just say success.\n\t\t\t */\n\t\t\treturn 0;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tval = tr32(ofs);\n\tval &= ~enable_bit;\n\ttw32_f(ofs, val);\n\n\tfor (i = 0; i < MAX_WAIT_CNT; i++) {\n\t\tudelay(100);\n\t\tval = tr32(ofs);\n\t\tif ((val & enable_bit) == 0)\n\t\t\tbreak;\n\t}\n\n\tif (i == MAX_WAIT_CNT && !silent) {\n\t\tdev_err(&tp->pdev->dev,\n\t\t\t\"tg3_stop_block timed out, ofs=%lx enable_bit=%x\\n\",\n\t\t\tofs, enable_bit);\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\n/* tp->lock is held. */\nstatic int tg3_abort_hw(struct tg3 *tp, int silent)\n{\n\tint i, err;\n\n\ttg3_disable_ints(tp);\n\n\ttp->rx_mode &= ~RX_MODE_ENABLE;\n\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\tudelay(10);\n\n\terr  = tg3_stop_block(tp, RCVBDI_MODE, RCVBDI_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVLPC_MODE, RCVLPC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVLSC_MODE, RCVLSC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVDBDI_MODE, RCVDBDI_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVDCC_MODE, RCVDCC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RCVCC_MODE, RCVCC_MODE_ENABLE, silent);\n\n\terr |= tg3_stop_block(tp, SNDBDS_MODE, SNDBDS_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, SNDBDI_MODE, SNDBDI_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, SNDDATAI_MODE, SNDDATAI_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, RDMAC_MODE, RDMAC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, SNDDATAC_MODE, SNDDATAC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, DMAC_MODE, DMAC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, SNDBDC_MODE, SNDBDC_MODE_ENABLE, silent);\n\n\ttp->mac_mode &= ~MAC_MODE_TDE_ENABLE;\n\ttw32_f(MAC_MODE, tp->mac_mode);\n\tudelay(40);\n\n\ttp->tx_mode &= ~TX_MODE_ENABLE;\n\ttw32_f(MAC_TX_MODE, tp->tx_mode);\n\n\tfor (i = 0; i < MAX_WAIT_CNT; i++) {\n\t\tudelay(100);\n\t\tif (!(tr32(MAC_TX_MODE) & TX_MODE_ENABLE))\n\t\t\tbreak;\n\t}\n\tif (i >= MAX_WAIT_CNT) {\n\t\tdev_err(&tp->pdev->dev,\n\t\t\t\"%s timed out, TX_MODE_ENABLE will not clear \"\n\t\t\t\"MAC_TX_MODE=%08x\\n\", __func__, tr32(MAC_TX_MODE));\n\t\terr |= -ENODEV;\n\t}\n\n\terr |= tg3_stop_block(tp, HOSTCC_MODE, HOSTCC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, WDMAC_MODE, WDMAC_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, MBFREE_MODE, MBFREE_MODE_ENABLE, silent);\n\n\ttw32(FTQ_RESET, 0xffffffff);\n\ttw32(FTQ_RESET, 0x00000000);\n\n\terr |= tg3_stop_block(tp, BUFMGR_MODE, BUFMGR_MODE_ENABLE, silent);\n\terr |= tg3_stop_block(tp, MEMARB_MODE, MEMARB_MODE_ENABLE, silent);\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tif (tnapi->hw_status)\n\t\t\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\t}\n\n\treturn err;\n}\n\n/* Save PCI command register before chip reset */\nstatic void tg3_save_pci_state(struct tg3 *tp)\n{\n\tpci_read_config_word(tp->pdev, PCI_COMMAND, &tp->pci_cmd);\n}\n\n/* Restore PCI state after chip reset */\nstatic void tg3_restore_pci_state(struct tg3 *tp)\n{\n\tu32 val;\n\n\t/* Re-enable indirect register accesses. */\n\tpci_write_config_dword(tp->pdev, TG3PCI_MISC_HOST_CTRL,\n\t\t\t       tp->misc_host_ctrl);\n\n\t/* Set MAX PCI retry to zero. */\n\tval = (PCISTATE_ROM_ENABLE | PCISTATE_ROM_RETRY_ENABLE);\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5704_A0 &&\n\t    tg3_flag(tp, PCIX_MODE))\n\t\tval |= PCISTATE_RETRY_SAME_DMA;\n\t/* Allow reads and writes to the APE register and memory space. */\n\tif (tg3_flag(tp, ENABLE_APE))\n\t\tval |= PCISTATE_ALLOW_APE_CTLSPC_WR |\n\t\t       PCISTATE_ALLOW_APE_SHMEM_WR |\n\t\t       PCISTATE_ALLOW_APE_PSPACE_WR;\n\tpci_write_config_dword(tp->pdev, TG3PCI_PCISTATE, val);\n\n\tpci_write_config_word(tp->pdev, PCI_COMMAND, tp->pci_cmd);\n\n\tif (!tg3_flag(tp, PCI_EXPRESS)) {\n\t\tpci_write_config_byte(tp->pdev, PCI_CACHE_LINE_SIZE,\n\t\t\t\t      tp->pci_cacheline_sz);\n\t\tpci_write_config_byte(tp->pdev, PCI_LATENCY_TIMER,\n\t\t\t\t      tp->pci_lat_timer);\n\t}\n\n\t/* Make sure PCI-X relaxed ordering bit is clear. */\n\tif (tg3_flag(tp, PCIX_MODE)) {\n\t\tu16 pcix_cmd;\n\n\t\tpci_read_config_word(tp->pdev, tp->pcix_cap + PCI_X_CMD,\n\t\t\t\t     &pcix_cmd);\n\t\tpcix_cmd &= ~PCI_X_CMD_ERO;\n\t\tpci_write_config_word(tp->pdev, tp->pcix_cap + PCI_X_CMD,\n\t\t\t\t      pcix_cmd);\n\t}\n\n\tif (tg3_flag(tp, 5780_CLASS)) {\n\n\t\t/* Chip reset on 5780 will reset MSI enable bit,\n\t\t * so need to restore it.\n\t\t */\n\t\tif (tg3_flag(tp, USING_MSI)) {\n\t\t\tu16 ctrl;\n\n\t\t\tpci_read_config_word(tp->pdev,\n\t\t\t\t\t     tp->msi_cap + PCI_MSI_FLAGS,\n\t\t\t\t\t     &ctrl);\n\t\t\tpci_write_config_word(tp->pdev,\n\t\t\t\t\t      tp->msi_cap + PCI_MSI_FLAGS,\n\t\t\t\t\t      ctrl | PCI_MSI_FLAGS_ENABLE);\n\t\t\tval = tr32(MSGINT_MODE);\n\t\t\ttw32(MSGINT_MODE, val | MSGINT_MODE_ENABLE);\n\t\t}\n\t}\n}\n\n/* tp->lock is held. */\nstatic int tg3_chip_reset(struct tg3 *tp)\n{\n\tu32 val;\n\tvoid (*write_op)(struct tg3 *, u32, u32);\n\tint i, err;\n\n\ttg3_nvram_lock(tp);\n\n\ttg3_ape_lock(tp, TG3_APE_LOCK_GRC);\n\n\t/* No matching tg3_nvram_unlock() after this because\n\t * chip reset below will undo the nvram lock.\n\t */\n\ttp->nvram_lock_cnt = 0;\n\n\t/* GRC_MISC_CFG core clock reset will clear the memory\n\t * enable bit in PCI register 4 and the MSI enable bit\n\t * on some chips, so we save relevant registers here.\n\t */\n\ttg3_save_pci_state(tp);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5752 ||\n\t    tg3_flag(tp, 5755_PLUS))\n\t\ttw32(GRC_FASTBOOT_PC, 0);\n\n\t/*\n\t * We must avoid the readl() that normally takes place.\n\t * It locks machines, causes machine checks, and other\n\t * fun things.  So, temporarily disable the 5701\n\t * hardware workaround, while we do the reset.\n\t */\n\twrite_op = tp->write32;\n\tif (write_op == tg3_write_flush_reg32)\n\t\ttp->write32 = tg3_write32;\n\n\t/* Prevent the irq handler from reading or writing PCI registers\n\t * during chip reset when the memory enable bit in the PCI command\n\t * register may be cleared.  The chip does not generate interrupt\n\t * at this time, but the irq handler may still be called due to irq\n\t * sharing or irqpoll.\n\t */\n\ttg3_flag_set(tp, CHIP_RESETTING);\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tif (tnapi->hw_status) {\n\t\t\ttnapi->hw_status->status = 0;\n\t\t\ttnapi->hw_status->status_tag = 0;\n\t\t}\n\t\ttnapi->last_tag = 0;\n\t\ttnapi->last_irq_tag = 0;\n\t}\n\tsmp_mb();\n\n\tfor (i = 0; i < tp->irq_cnt; i++)\n\t\tsynchronize_irq(tp->napi[i].irq_vec);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_57780) {\n\t\tval = tr32(TG3_PCIE_LNKCTL) & ~TG3_PCIE_LNKCTL_L1_PLL_PD_EN;\n\t\ttw32(TG3_PCIE_LNKCTL, val | TG3_PCIE_LNKCTL_L1_PLL_PD_DIS);\n\t}\n\n\t/* do the reset */\n\tval = GRC_MISC_CFG_CORECLK_RESET;\n\n\tif (tg3_flag(tp, PCI_EXPRESS)) {\n\t\t/* Force PCIe 1.0a mode */\n\t\tif (tg3_asic_rev(tp) != ASIC_REV_5785 &&\n\t\t    !tg3_flag(tp, 57765_PLUS) &&\n\t\t    tr32(TG3_PCIE_PHY_TSTCTL) ==\n\t\t    (TG3_PCIE_PHY_TSTCTL_PCIE10 | TG3_PCIE_PHY_TSTCTL_PSCRAM))\n\t\t\ttw32(TG3_PCIE_PHY_TSTCTL, TG3_PCIE_PHY_TSTCTL_PSCRAM);\n\n\t\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A0) {\n\t\t\ttw32(GRC_MISC_CFG, (1 << 29));\n\t\t\tval |= (1 << 29);\n\t\t}\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\ttw32(VCPU_STATUS, tr32(VCPU_STATUS) | VCPU_STATUS_DRV_RESET);\n\t\ttw32(GRC_VCPU_EXT_CTRL,\n\t\t     tr32(GRC_VCPU_EXT_CTRL) & ~GRC_VCPU_EXT_CTRL_HALT_CPU);\n\t}\n\n\t/* Manage gphy power for all CPMU absent PCIe devices. */\n\tif (tg3_flag(tp, 5705_PLUS) && !tg3_flag(tp, CPMU_PRESENT))\n\t\tval |= GRC_MISC_CFG_KEEP_GPHY_POWER;\n\n\ttw32(GRC_MISC_CFG, val);\n\n\t/* restore 5701 hardware bug workaround write method */\n\ttp->write32 = write_op;\n\n\t/* Unfortunately, we have to delay before the PCI read back.\n\t * Some 575X chips even will not respond to a PCI cfg access\n\t * when the reset command is given to the chip.\n\t *\n\t * How do these hardware designers expect things to work\n\t * properly if the PCI write is posted for a long period\n\t * of time?  It is always necessary to have some method by\n\t * which a register read back can occur to push the write\n\t * out which does the reset.\n\t *\n\t * For most tg3 variants the trick below was working.\n\t * Ho hum...\n\t */\n\tudelay(120);\n\n\t/* Flush PCI posted writes.  The normal MMIO registers\n\t * are inaccessible at this time so this is the only\n\t * way to make this reliably (actually, this is no longer\n\t * the case, see above).  I tried to use indirect\n\t * register read/write but this upset some 5701 variants.\n\t */\n\tpci_read_config_dword(tp->pdev, PCI_COMMAND, &val);\n\n\tudelay(120);\n\n\tif (tg3_flag(tp, PCI_EXPRESS) && pci_is_pcie(tp->pdev)) {\n\t\tu16 val16;\n\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5750_A0) {\n\t\t\tint j;\n\t\t\tu32 cfg_val;\n\n\t\t\t/* Wait for link training to complete.  */\n\t\t\tfor (j = 0; j < 5000; j++)\n\t\t\t\tudelay(100);\n\n\t\t\tpci_read_config_dword(tp->pdev, 0xc4, &cfg_val);\n\t\t\tpci_write_config_dword(tp->pdev, 0xc4,\n\t\t\t\t\t       cfg_val | (1 << 15));\n\t\t}\n\n\t\t/* Clear the \"no snoop\" and \"relaxed ordering\" bits. */\n\t\tval16 = PCI_EXP_DEVCTL_RELAX_EN | PCI_EXP_DEVCTL_NOSNOOP_EN;\n\t\t/*\n\t\t * Older PCIe devices only support the 128 byte\n\t\t * MPS setting.  Enforce the restriction.\n\t\t */\n\t\tif (!tg3_flag(tp, CPMU_PRESENT))\n\t\t\tval16 |= PCI_EXP_DEVCTL_PAYLOAD;\n\t\tpcie_capability_clear_word(tp->pdev, PCI_EXP_DEVCTL, val16);\n\n\t\t/* Clear error status */\n\t\tpcie_capability_write_word(tp->pdev, PCI_EXP_DEVSTA,\n\t\t\t\t      PCI_EXP_DEVSTA_CED |\n\t\t\t\t      PCI_EXP_DEVSTA_NFED |\n\t\t\t\t      PCI_EXP_DEVSTA_FED |\n\t\t\t\t      PCI_EXP_DEVSTA_URD);\n\t}\n\n\ttg3_restore_pci_state(tp);\n\n\ttg3_flag_clear(tp, CHIP_RESETTING);\n\ttg3_flag_clear(tp, ERROR_PROCESSED);\n\n\tval = 0;\n\tif (tg3_flag(tp, 5780_CLASS))\n\t\tval = tr32(MEMARB_MODE);\n\ttw32(MEMARB_MODE, val | MEMARB_MODE_ENABLE);\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5750_A3) {\n\t\ttg3_stop_fw(tp);\n\t\ttw32(0x5000, 0x400);\n\t}\n\n\tif (tg3_flag(tp, IS_SSB_CORE)) {\n\t\t/*\n\t\t * BCM4785: In order to avoid repercussions from using\n\t\t * potentially defective internal ROM, stop the Rx RISC CPU,\n\t\t * which is not required.\n\t\t */\n\t\ttg3_stop_fw(tp);\n\t\ttg3_halt_cpu(tp, RX_CPU_BASE);\n\t}\n\n\ttw32(GRC_MODE, tp->grc_mode);\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A0) {\n\t\tval = tr32(0xc4);\n\n\t\ttw32(0xc4, val | (1 << 15));\n\t}\n\n\tif ((tp->nic_sram_data_cfg & NIC_SRAM_DATA_CFG_MINI_PCI) != 0 &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\ttp->pci_clock_ctrl |= CLOCK_CTRL_CLKRUN_OENABLE;\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A0)\n\t\t\ttp->pci_clock_ctrl |= CLOCK_CTRL_FORCE_CLKRUN;\n\t\ttw32(TG3PCI_CLOCK_CTRL, tp->pci_clock_ctrl);\n\t}\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\ttp->mac_mode = MAC_MODE_PORT_MODE_TBI;\n\t\tval = tp->mac_mode;\n\t} else if (tp->phy_flags & TG3_PHYFLG_MII_SERDES) {\n\t\ttp->mac_mode = MAC_MODE_PORT_MODE_GMII;\n\t\tval = tp->mac_mode;\n\t} else\n\t\tval = 0;\n\n\ttw32_f(MAC_MODE, val);\n\tudelay(40);\n\n\ttg3_ape_unlock(tp, TG3_APE_LOCK_GRC);\n\n\terr = tg3_poll_fw(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_mdio_start(tp);\n\n\tif (tg3_flag(tp, PCI_EXPRESS) &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A0 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5785 &&\n\t    !tg3_flag(tp, 57765_PLUS)) {\n\t\tval = tr32(0x7c00);\n\n\t\ttw32(0x7c00, val | (1 << 25));\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720) {\n\t\tval = tr32(TG3_CPMU_CLCK_ORIDE);\n\t\ttw32(TG3_CPMU_CLCK_ORIDE, val & ~CPMU_CLCK_ORIDE_MAC_ORIDE_EN);\n\t}\n\n\t/* Reprobe ASF enable state.  */\n\ttg3_flag_clear(tp, ENABLE_ASF);\n\ttg3_flag_clear(tp, ASF_NEW_HANDSHAKE);\n\ttg3_read_mem(tp, NIC_SRAM_DATA_SIG, &val);\n\tif (val == NIC_SRAM_DATA_SIG_MAGIC) {\n\t\tu32 nic_cfg;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG, &nic_cfg);\n\t\tif (nic_cfg & NIC_SRAM_DATA_CFG_ASF_ENABLE) {\n\t\t\ttg3_flag_set(tp, ENABLE_ASF);\n\t\t\ttp->last_event_jiffies = jiffies;\n\t\t\tif (tg3_flag(tp, 5750_PLUS))\n\t\t\t\ttg3_flag_set(tp, ASF_NEW_HANDSHAKE);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_get_nstats(struct tg3 *, struct rtnl_link_stats64 *);\nstatic void tg3_get_estats(struct tg3 *, struct tg3_ethtool_stats *);\n\n/* tp->lock is held. */\nstatic int tg3_halt(struct tg3 *tp, int kind, int silent)\n{\n\tint err;\n\n\ttg3_stop_fw(tp);\n\n\ttg3_write_sig_pre_reset(tp, kind);\n\n\ttg3_abort_hw(tp, silent);\n\terr = tg3_chip_reset(tp);\n\n\t__tg3_set_mac_addr(tp, 0);\n\n\ttg3_write_sig_legacy(tp, kind);\n\ttg3_write_sig_post_reset(tp, kind);\n\n\tif (tp->hw_stats) {\n\t\t/* Save the stats across chip resets... */\n\t\ttg3_get_nstats(tp, &tp->net_stats_prev);\n\t\ttg3_get_estats(tp, &tp->estats_prev);\n\n\t\t/* And make sure the next sample is new data */\n\t\tmemset(tp->hw_stats, 0, sizeof(struct tg3_hw_stats));\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int tg3_set_mac_addr(struct net_device *dev, void *p)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tstruct sockaddr *addr = p;\n\tint err = 0, skip_mac_1 = 0;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tif (tg3_flag(tp, ENABLE_ASF)) {\n\t\tu32 addr0_high, addr0_low, addr1_high, addr1_low;\n\n\t\taddr0_high = tr32(MAC_ADDR_0_HIGH);\n\t\taddr0_low = tr32(MAC_ADDR_0_LOW);\n\t\taddr1_high = tr32(MAC_ADDR_1_HIGH);\n\t\taddr1_low = tr32(MAC_ADDR_1_LOW);\n\n\t\t/* Skip MAC addr 1 if ASF is using it. */\n\t\tif ((addr0_high != addr1_high || addr0_low != addr1_low) &&\n\t\t    !(addr1_high == 0 && addr1_low == 0))\n\t\t\tskip_mac_1 = 1;\n\t}\n\tspin_lock_bh(&tp->lock);\n\t__tg3_set_mac_addr(tp, skip_mac_1);\n\tspin_unlock_bh(&tp->lock);\n\n\treturn err;\n}\n\n/* tp->lock is held. */\nstatic void tg3_set_bdinfo(struct tg3 *tp, u32 bdinfo_addr,\n\t\t\t   dma_addr_t mapping, u32 maxlen_flags,\n\t\t\t   u32 nic_addr)\n{\n\ttg3_write_mem(tp,\n\t\t      (bdinfo_addr + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_HIGH),\n\t\t      ((u64) mapping >> 32));\n\ttg3_write_mem(tp,\n\t\t      (bdinfo_addr + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_LOW),\n\t\t      ((u64) mapping & 0xffffffff));\n\ttg3_write_mem(tp,\n\t\t      (bdinfo_addr + TG3_BDINFO_MAXLEN_FLAGS),\n\t\t       maxlen_flags);\n\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\ttg3_write_mem(tp,\n\t\t\t      (bdinfo_addr + TG3_BDINFO_NIC_ADDR),\n\t\t\t      nic_addr);\n}\n\n\nstatic void tg3_coal_tx_init(struct tg3 *tp, struct ethtool_coalesce *ec)\n{\n\tint i = 0;\n\n\tif (!tg3_flag(tp, ENABLE_TSS)) {\n\t\ttw32(HOSTCC_TXCOL_TICKS, ec->tx_coalesce_usecs);\n\t\ttw32(HOSTCC_TXMAX_FRAMES, ec->tx_max_coalesced_frames);\n\t\ttw32(HOSTCC_TXCOAL_MAXF_INT, ec->tx_max_coalesced_frames_irq);\n\t} else {\n\t\ttw32(HOSTCC_TXCOL_TICKS, 0);\n\t\ttw32(HOSTCC_TXMAX_FRAMES, 0);\n\t\ttw32(HOSTCC_TXCOAL_MAXF_INT, 0);\n\n\t\tfor (; i < tp->txq_cnt; i++) {\n\t\t\tu32 reg;\n\n\t\t\treg = HOSTCC_TXCOL_TICKS_VEC1 + i * 0x18;\n\t\t\ttw32(reg, ec->tx_coalesce_usecs);\n\t\t\treg = HOSTCC_TXMAX_FRAMES_VEC1 + i * 0x18;\n\t\t\ttw32(reg, ec->tx_max_coalesced_frames);\n\t\t\treg = HOSTCC_TXCOAL_MAXF_INT_VEC1 + i * 0x18;\n\t\t\ttw32(reg, ec->tx_max_coalesced_frames_irq);\n\t\t}\n\t}\n\n\tfor (; i < tp->irq_max - 1; i++) {\n\t\ttw32(HOSTCC_TXCOL_TICKS_VEC1 + i * 0x18, 0);\n\t\ttw32(HOSTCC_TXMAX_FRAMES_VEC1 + i * 0x18, 0);\n\t\ttw32(HOSTCC_TXCOAL_MAXF_INT_VEC1 + i * 0x18, 0);\n\t}\n}\n\nstatic void tg3_coal_rx_init(struct tg3 *tp, struct ethtool_coalesce *ec)\n{\n\tint i = 0;\n\tu32 limit = tp->rxq_cnt;\n\n\tif (!tg3_flag(tp, ENABLE_RSS)) {\n\t\ttw32(HOSTCC_RXCOL_TICKS, ec->rx_coalesce_usecs);\n\t\ttw32(HOSTCC_RXMAX_FRAMES, ec->rx_max_coalesced_frames);\n\t\ttw32(HOSTCC_RXCOAL_MAXF_INT, ec->rx_max_coalesced_frames_irq);\n\t\tlimit--;\n\t} else {\n\t\ttw32(HOSTCC_RXCOL_TICKS, 0);\n\t\ttw32(HOSTCC_RXMAX_FRAMES, 0);\n\t\ttw32(HOSTCC_RXCOAL_MAXF_INT, 0);\n\t}\n\n\tfor (; i < limit; i++) {\n\t\tu32 reg;\n\n\t\treg = HOSTCC_RXCOL_TICKS_VEC1 + i * 0x18;\n\t\ttw32(reg, ec->rx_coalesce_usecs);\n\t\treg = HOSTCC_RXMAX_FRAMES_VEC1 + i * 0x18;\n\t\ttw32(reg, ec->rx_max_coalesced_frames);\n\t\treg = HOSTCC_RXCOAL_MAXF_INT_VEC1 + i * 0x18;\n\t\ttw32(reg, ec->rx_max_coalesced_frames_irq);\n\t}\n\n\tfor (; i < tp->irq_max - 1; i++) {\n\t\ttw32(HOSTCC_RXCOL_TICKS_VEC1 + i * 0x18, 0);\n\t\ttw32(HOSTCC_RXMAX_FRAMES_VEC1 + i * 0x18, 0);\n\t\ttw32(HOSTCC_RXCOAL_MAXF_INT_VEC1 + i * 0x18, 0);\n\t}\n}\n\nstatic void __tg3_set_coalesce(struct tg3 *tp, struct ethtool_coalesce *ec)\n{\n\ttg3_coal_tx_init(tp, ec);\n\ttg3_coal_rx_init(tp, ec);\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\tu32 val = ec->stats_block_coalesce_usecs;\n\n\t\ttw32(HOSTCC_RXCOAL_TICK_INT, ec->rx_coalesce_usecs_irq);\n\t\ttw32(HOSTCC_TXCOAL_TICK_INT, ec->tx_coalesce_usecs_irq);\n\n\t\tif (!tp->link_up)\n\t\t\tval = 0;\n\n\t\ttw32(HOSTCC_STAT_COAL_TICKS, val);\n\t}\n}\n\n/* tp->lock is held. */\nstatic void tg3_rings_reset(struct tg3 *tp)\n{\n\tint i;\n\tu32 stblk, txrcb, rxrcb, limit;\n\tstruct tg3_napi *tnapi = &tp->napi[0];\n\n\t/* Disable all transmit rings but the first. */\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\tlimit = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE * 16;\n\telse if (tg3_flag(tp, 5717_PLUS))\n\t\tlimit = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE * 4;\n\telse if (tg3_flag(tp, 57765_CLASS) ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tlimit = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE * 2;\n\telse\n\t\tlimit = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE;\n\n\tfor (txrcb = NIC_SRAM_SEND_RCB + TG3_BDINFO_SIZE;\n\t     txrcb < limit; txrcb += TG3_BDINFO_SIZE)\n\t\ttg3_write_mem(tp, txrcb + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t\t      BDINFO_FLAGS_DISABLED);\n\n\n\t/* Disable all receive return rings but the first. */\n\tif (tg3_flag(tp, 5717_PLUS))\n\t\tlimit = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE * 17;\n\telse if (!tg3_flag(tp, 5705_PLUS))\n\t\tlimit = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE * 16;\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5762 ||\n\t\t tg3_flag(tp, 57765_CLASS))\n\t\tlimit = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE * 4;\n\telse\n\t\tlimit = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE;\n\n\tfor (rxrcb = NIC_SRAM_RCV_RET_RCB + TG3_BDINFO_SIZE;\n\t     rxrcb < limit; rxrcb += TG3_BDINFO_SIZE)\n\t\ttg3_write_mem(tp, rxrcb + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t\t      BDINFO_FLAGS_DISABLED);\n\n\t/* Disable interrupts */\n\ttw32_mailbox_f(tp->napi[0].int_mbox, 1);\n\ttp->napi[0].chk_msi_cnt = 0;\n\ttp->napi[0].last_rx_cons = 0;\n\ttp->napi[0].last_tx_cons = 0;\n\n\t/* Zero mailbox registers. */\n\tif (tg3_flag(tp, SUPPORT_MSIX)) {\n\t\tfor (i = 1; i < tp->irq_max; i++) {\n\t\t\ttp->napi[i].tx_prod = 0;\n\t\t\ttp->napi[i].tx_cons = 0;\n\t\t\tif (tg3_flag(tp, ENABLE_TSS))\n\t\t\t\ttw32_mailbox(tp->napi[i].prodmbox, 0);\n\t\t\ttw32_rx_mbox(tp->napi[i].consmbox, 0);\n\t\t\ttw32_mailbox_f(tp->napi[i].int_mbox, 1);\n\t\t\ttp->napi[i].chk_msi_cnt = 0;\n\t\t\ttp->napi[i].last_rx_cons = 0;\n\t\t\ttp->napi[i].last_tx_cons = 0;\n\t\t}\n\t\tif (!tg3_flag(tp, ENABLE_TSS))\n\t\t\ttw32_mailbox(tp->napi[0].prodmbox, 0);\n\t} else {\n\t\ttp->napi[0].tx_prod = 0;\n\t\ttp->napi[0].tx_cons = 0;\n\t\ttw32_mailbox(tp->napi[0].prodmbox, 0);\n\t\ttw32_rx_mbox(tp->napi[0].consmbox, 0);\n\t}\n\n\t/* Make sure the NIC-based send BD rings are disabled. */\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\tu32 mbox = MAILBOX_SNDNIC_PROD_IDX_0 + TG3_64BIT_REG_LOW;\n\t\tfor (i = 0; i < 16; i++)\n\t\t\ttw32_tx_mbox(mbox + i * 8, 0);\n\t}\n\n\ttxrcb = NIC_SRAM_SEND_RCB;\n\trxrcb = NIC_SRAM_RCV_RET_RCB;\n\n\t/* Clear status block in ram. */\n\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\n\t/* Set status block DMA address */\n\ttw32(HOSTCC_STATUS_BLK_HOST_ADDR + TG3_64BIT_REG_HIGH,\n\t     ((u64) tnapi->status_mapping >> 32));\n\ttw32(HOSTCC_STATUS_BLK_HOST_ADDR + TG3_64BIT_REG_LOW,\n\t     ((u64) tnapi->status_mapping & 0xffffffff));\n\n\tif (tnapi->tx_ring) {\n\t\ttg3_set_bdinfo(tp, txrcb, tnapi->tx_desc_mapping,\n\t\t\t       (TG3_TX_RING_SIZE <<\n\t\t\t\tBDINFO_FLAGS_MAXLEN_SHIFT),\n\t\t\t       NIC_SRAM_TX_BUFFER_DESC);\n\t\ttxrcb += TG3_BDINFO_SIZE;\n\t}\n\n\tif (tnapi->rx_rcb) {\n\t\ttg3_set_bdinfo(tp, rxrcb, tnapi->rx_rcb_mapping,\n\t\t\t       (tp->rx_ret_ring_mask + 1) <<\n\t\t\t\tBDINFO_FLAGS_MAXLEN_SHIFT, 0);\n\t\trxrcb += TG3_BDINFO_SIZE;\n\t}\n\n\tstblk = HOSTCC_STATBLCK_RING1;\n\n\tfor (i = 1, tnapi++; i < tp->irq_cnt; i++, tnapi++) {\n\t\tu64 mapping = (u64)tnapi->status_mapping;\n\t\ttw32(stblk + TG3_64BIT_REG_HIGH, mapping >> 32);\n\t\ttw32(stblk + TG3_64BIT_REG_LOW, mapping & 0xffffffff);\n\n\t\t/* Clear status block in ram. */\n\t\tmemset(tnapi->hw_status, 0, TG3_HW_STATUS_SIZE);\n\n\t\tif (tnapi->tx_ring) {\n\t\t\ttg3_set_bdinfo(tp, txrcb, tnapi->tx_desc_mapping,\n\t\t\t\t       (TG3_TX_RING_SIZE <<\n\t\t\t\t\tBDINFO_FLAGS_MAXLEN_SHIFT),\n\t\t\t\t       NIC_SRAM_TX_BUFFER_DESC);\n\t\t\ttxrcb += TG3_BDINFO_SIZE;\n\t\t}\n\n\t\ttg3_set_bdinfo(tp, rxrcb, tnapi->rx_rcb_mapping,\n\t\t\t       ((tp->rx_ret_ring_mask + 1) <<\n\t\t\t\tBDINFO_FLAGS_MAXLEN_SHIFT), 0);\n\n\t\tstblk += 8;\n\t\trxrcb += TG3_BDINFO_SIZE;\n\t}\n}\n\nstatic void tg3_setup_rxbd_thresholds(struct tg3 *tp)\n{\n\tu32 val, bdcache_maxcnt, host_rep_thresh, nic_rep_thresh;\n\n\tif (!tg3_flag(tp, 5750_PLUS) ||\n\t    tg3_flag(tp, 5780_CLASS) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5750 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5752 ||\n\t    tg3_flag(tp, 57765_PLUS))\n\t\tbdcache_maxcnt = TG3_SRAM_RX_STD_BDCACHE_SIZE_5700;\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5787)\n\t\tbdcache_maxcnt = TG3_SRAM_RX_STD_BDCACHE_SIZE_5755;\n\telse\n\t\tbdcache_maxcnt = TG3_SRAM_RX_STD_BDCACHE_SIZE_5906;\n\n\tnic_rep_thresh = min(bdcache_maxcnt / 2, tp->rx_std_max_post);\n\thost_rep_thresh = max_t(u32, tp->rx_pending / 8, 1);\n\n\tval = min(nic_rep_thresh, host_rep_thresh);\n\ttw32(RCVBDI_STD_THRESH, val);\n\n\tif (tg3_flag(tp, 57765_PLUS))\n\t\ttw32(STD_REPLENISH_LWM, bdcache_maxcnt);\n\n\tif (!tg3_flag(tp, JUMBO_CAPABLE) || tg3_flag(tp, 5780_CLASS))\n\t\treturn;\n\n\tbdcache_maxcnt = TG3_SRAM_RX_JMB_BDCACHE_SIZE_5700;\n\n\thost_rep_thresh = max_t(u32, tp->rx_jumbo_pending / 8, 1);\n\n\tval = min(bdcache_maxcnt / 2, host_rep_thresh);\n\ttw32(RCVBDI_JUMBO_THRESH, val);\n\n\tif (tg3_flag(tp, 57765_PLUS))\n\t\ttw32(JMB_REPLENISH_LWM, bdcache_maxcnt);\n}\n\nstatic inline u32 calc_crc(unsigned char *buf, int len)\n{\n\tu32 reg;\n\tu32 tmp;\n\tint j, k;\n\n\treg = 0xffffffff;\n\n\tfor (j = 0; j < len; j++) {\n\t\treg ^= buf[j];\n\n\t\tfor (k = 0; k < 8; k++) {\n\t\t\ttmp = reg & 0x01;\n\n\t\t\treg >>= 1;\n\n\t\t\tif (tmp)\n\t\t\t\treg ^= 0xedb88320;\n\t\t}\n\t}\n\n\treturn ~reg;\n}\n\nstatic void tg3_set_multi(struct tg3 *tp, unsigned int accept_all)\n{\n\t/* accept or reject all multicast frames */\n\ttw32(MAC_HASH_REG_0, accept_all ? 0xffffffff : 0);\n\ttw32(MAC_HASH_REG_1, accept_all ? 0xffffffff : 0);\n\ttw32(MAC_HASH_REG_2, accept_all ? 0xffffffff : 0);\n\ttw32(MAC_HASH_REG_3, accept_all ? 0xffffffff : 0);\n}\n\nstatic void __tg3_set_rx_mode(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 rx_mode;\n\n\trx_mode = tp->rx_mode & ~(RX_MODE_PROMISC |\n\t\t\t\t  RX_MODE_KEEP_VLAN_TAG);\n\n#if !defined(CONFIG_VLAN_8021Q) && !defined(CONFIG_VLAN_8021Q_MODULE)\n\t/* When ASF is in use, we always keep the RX_MODE_KEEP_VLAN_TAG\n\t * flag clear.\n\t */\n\tif (!tg3_flag(tp, ENABLE_ASF))\n\t\trx_mode |= RX_MODE_KEEP_VLAN_TAG;\n#endif\n\n\tif (dev->flags & IFF_PROMISC) {\n\t\t/* Promiscuous mode. */\n\t\trx_mode |= RX_MODE_PROMISC;\n\t} else if (dev->flags & IFF_ALLMULTI) {\n\t\t/* Accept all multicast. */\n\t\ttg3_set_multi(tp, 1);\n\t} else if (netdev_mc_empty(dev)) {\n\t\t/* Reject all multicast. */\n\t\ttg3_set_multi(tp, 0);\n\t} else {\n\t\t/* Accept one or more multicast(s). */\n\t\tstruct netdev_hw_addr *ha;\n\t\tu32 mc_filter[4] = { 0, };\n\t\tu32 regidx;\n\t\tu32 bit;\n\t\tu32 crc;\n\n\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\tcrc = calc_crc(ha->addr, ETH_ALEN);\n\t\t\tbit = ~crc & 0x7f;\n\t\t\tregidx = (bit & 0x60) >> 5;\n\t\t\tbit &= 0x1f;\n\t\t\tmc_filter[regidx] |= (1 << bit);\n\t\t}\n\n\t\ttw32(MAC_HASH_REG_0, mc_filter[0]);\n\t\ttw32(MAC_HASH_REG_1, mc_filter[1]);\n\t\ttw32(MAC_HASH_REG_2, mc_filter[2]);\n\t\ttw32(MAC_HASH_REG_3, mc_filter[3]);\n\t}\n\n\tif (rx_mode != tp->rx_mode) {\n\t\ttp->rx_mode = rx_mode;\n\t\ttw32_f(MAC_RX_MODE, rx_mode);\n\t\tudelay(10);\n\t}\n}\n\nstatic void tg3_rss_init_dflt_indir_tbl(struct tg3 *tp, u32 qcnt)\n{\n\tint i;\n\n\tfor (i = 0; i < TG3_RSS_INDIR_TBL_SIZE; i++)\n\t\ttp->rss_ind_tbl[i] = ethtool_rxfh_indir_default(i, qcnt);\n}\n\nstatic void tg3_rss_check_indir_tbl(struct tg3 *tp)\n{\n\tint i;\n\n\tif (!tg3_flag(tp, SUPPORT_MSIX))\n\t\treturn;\n\n\tif (tp->rxq_cnt == 1) {\n\t\tmemset(&tp->rss_ind_tbl[0], 0, sizeof(tp->rss_ind_tbl));\n\t\treturn;\n\t}\n\n\t/* Validate table against current IRQ count */\n\tfor (i = 0; i < TG3_RSS_INDIR_TBL_SIZE; i++) {\n\t\tif (tp->rss_ind_tbl[i] >= tp->rxq_cnt)\n\t\t\tbreak;\n\t}\n\n\tif (i != TG3_RSS_INDIR_TBL_SIZE)\n\t\ttg3_rss_init_dflt_indir_tbl(tp, tp->rxq_cnt);\n}\n\nstatic void tg3_rss_write_indir_tbl(struct tg3 *tp)\n{\n\tint i = 0;\n\tu32 reg = MAC_RSS_INDIR_TBL_0;\n\n\twhile (i < TG3_RSS_INDIR_TBL_SIZE) {\n\t\tu32 val = tp->rss_ind_tbl[i];\n\t\ti++;\n\t\tfor (; i % 8; i++) {\n\t\t\tval <<= 4;\n\t\t\tval |= tp->rss_ind_tbl[i];\n\t\t}\n\t\ttw32(reg, val);\n\t\treg += 4;\n\t}\n}\n\n/* tp->lock is held. */\nstatic int tg3_reset_hw(struct tg3 *tp, int reset_phy)\n{\n\tu32 val, rdmac_mode;\n\tint i, err, limit;\n\tstruct tg3_rx_prodring_set *tpr = &tp->napi[0].prodring;\n\n\ttg3_disable_ints(tp);\n\n\ttg3_stop_fw(tp);\n\n\ttg3_write_sig_pre_reset(tp, RESET_KIND_INIT);\n\n\tif (tg3_flag(tp, INIT_COMPLETE))\n\t\ttg3_abort_hw(tp, 1);\n\n\t/* Enable MAC control of LPI */\n\tif (tp->phy_flags & TG3_PHYFLG_EEE_CAP) {\n\t\tval = TG3_CPMU_EEE_LNKIDL_PCIE_NL0 |\n\t\t      TG3_CPMU_EEE_LNKIDL_UART_IDL;\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_57765_A0)\n\t\t\tval |= TG3_CPMU_EEE_LNKIDL_APE_TX_MT;\n\n\t\ttw32_f(TG3_CPMU_EEE_LNKIDL_CTRL, val);\n\n\t\ttw32_f(TG3_CPMU_EEE_CTRL,\n\t\t       TG3_CPMU_EEE_CTRL_EXIT_20_1_US);\n\n\t\tval = TG3_CPMU_EEEMD_ERLY_L1_XIT_DET |\n\t\t      TG3_CPMU_EEEMD_LPI_IN_TX |\n\t\t      TG3_CPMU_EEEMD_LPI_IN_RX |\n\t\t      TG3_CPMU_EEEMD_EEE_ENABLE;\n\n\t\tif (tg3_asic_rev(tp) != ASIC_REV_5717)\n\t\t\tval |= TG3_CPMU_EEEMD_SND_IDX_DET_EN;\n\n\t\tif (tg3_flag(tp, ENABLE_APE))\n\t\t\tval |= TG3_CPMU_EEEMD_APE_TX_DET_EN;\n\n\t\ttw32_f(TG3_CPMU_EEE_MODE, val);\n\n\t\ttw32_f(TG3_CPMU_EEE_DBTMR1,\n\t\t       TG3_CPMU_DBTMR1_PCIEXIT_2047US |\n\t\t       TG3_CPMU_DBTMR1_LNKIDLE_2047US);\n\n\t\ttw32_f(TG3_CPMU_EEE_DBTMR2,\n\t\t       TG3_CPMU_DBTMR2_APE_TX_2047US |\n\t\t       TG3_CPMU_DBTMR2_TXIDXEQ_2047US);\n\t}\n\n\tif (reset_phy)\n\t\ttg3_phy_reset(tp);\n\n\terr = tg3_chip_reset(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_write_sig_legacy(tp, RESET_KIND_INIT);\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX) {\n\t\tval = tr32(TG3_CPMU_CTRL);\n\t\tval &= ~(CPMU_CTRL_LINK_AWARE_MODE | CPMU_CTRL_LINK_IDLE_MODE);\n\t\ttw32(TG3_CPMU_CTRL, val);\n\n\t\tval = tr32(TG3_CPMU_LSPD_10MB_CLK);\n\t\tval &= ~CPMU_LSPD_10MB_MACCLK_MASK;\n\t\tval |= CPMU_LSPD_10MB_MACCLK_6_25;\n\t\ttw32(TG3_CPMU_LSPD_10MB_CLK, val);\n\n\t\tval = tr32(TG3_CPMU_LNK_AWARE_PWRMD);\n\t\tval &= ~CPMU_LNK_AWARE_MACCLK_MASK;\n\t\tval |= CPMU_LNK_AWARE_MACCLK_6_25;\n\t\ttw32(TG3_CPMU_LNK_AWARE_PWRMD, val);\n\n\t\tval = tr32(TG3_CPMU_HST_ACC);\n\t\tval &= ~CPMU_HST_ACC_MACCLK_MASK;\n\t\tval |= CPMU_HST_ACC_MACCLK_6_25;\n\t\ttw32(TG3_CPMU_HST_ACC, val);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_57780) {\n\t\tval = tr32(PCIE_PWR_MGMT_THRESH) & ~PCIE_PWR_MGMT_L1_THRESH_MSK;\n\t\tval |= PCIE_PWR_MGMT_EXT_ASPM_TMR_EN |\n\t\t       PCIE_PWR_MGMT_L1_THRESH_4MS;\n\t\ttw32(PCIE_PWR_MGMT_THRESH, val);\n\n\t\tval = tr32(TG3_PCIE_EIDLE_DELAY) & ~TG3_PCIE_EIDLE_DELAY_MASK;\n\t\ttw32(TG3_PCIE_EIDLE_DELAY, val | TG3_PCIE_EIDLE_DELAY_13_CLKS);\n\n\t\ttw32(TG3_CORR_ERR_STAT, TG3_CORR_ERR_STAT_CLEAR);\n\n\t\tval = tr32(TG3_PCIE_LNKCTL) & ~TG3_PCIE_LNKCTL_L1_PLL_PD_EN;\n\t\ttw32(TG3_PCIE_LNKCTL, val | TG3_PCIE_LNKCTL_L1_PLL_PD_DIS);\n\t}\n\n\tif (tg3_flag(tp, L1PLLPD_EN)) {\n\t\tu32 grc_mode = tr32(GRC_MODE);\n\n\t\t/* Access the lower 1K of PL PCIE block registers. */\n\t\tval = grc_mode & ~GRC_MODE_PCIE_PORT_MASK;\n\t\ttw32(GRC_MODE, val | GRC_MODE_PCIE_PL_SEL);\n\n\t\tval = tr32(TG3_PCIE_TLDLPL_PORT + TG3_PCIE_PL_LO_PHYCTL1);\n\t\ttw32(TG3_PCIE_TLDLPL_PORT + TG3_PCIE_PL_LO_PHYCTL1,\n\t\t     val | TG3_PCIE_PL_LO_PHYCTL1_L1PLLPD_EN);\n\n\t\ttw32(GRC_MODE, grc_mode);\n\t}\n\n\tif (tg3_flag(tp, 57765_CLASS)) {\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_57765_A0) {\n\t\t\tu32 grc_mode = tr32(GRC_MODE);\n\n\t\t\t/* Access the lower 1K of PL PCIE block registers. */\n\t\t\tval = grc_mode & ~GRC_MODE_PCIE_PORT_MASK;\n\t\t\ttw32(GRC_MODE, val | GRC_MODE_PCIE_PL_SEL);\n\n\t\t\tval = tr32(TG3_PCIE_TLDLPL_PORT +\n\t\t\t\t   TG3_PCIE_PL_LO_PHYCTL5);\n\t\t\ttw32(TG3_PCIE_TLDLPL_PORT + TG3_PCIE_PL_LO_PHYCTL5,\n\t\t\t     val | TG3_PCIE_PL_LO_PHYCTL5_DIS_L2CLKREQ);\n\n\t\t\ttw32(GRC_MODE, grc_mode);\n\t\t}\n\n\t\tif (tg3_chip_rev(tp) != CHIPREV_57765_AX) {\n\t\t\tu32 grc_mode;\n\n\t\t\t/* Fix transmit hangs */\n\t\t\tval = tr32(TG3_CPMU_PADRNG_CTL);\n\t\t\tval |= TG3_CPMU_PADRNG_CTL_RDIV2;\n\t\t\ttw32(TG3_CPMU_PADRNG_CTL, val);\n\n\t\t\tgrc_mode = tr32(GRC_MODE);\n\n\t\t\t/* Access the lower 1K of DL PCIE block registers. */\n\t\t\tval = grc_mode & ~GRC_MODE_PCIE_PORT_MASK;\n\t\t\ttw32(GRC_MODE, val | GRC_MODE_PCIE_DL_SEL);\n\n\t\t\tval = tr32(TG3_PCIE_TLDLPL_PORT +\n\t\t\t\t   TG3_PCIE_DL_LO_FTSMAX);\n\t\t\tval &= ~TG3_PCIE_DL_LO_FTSMAX_MSK;\n\t\t\ttw32(TG3_PCIE_TLDLPL_PORT + TG3_PCIE_DL_LO_FTSMAX,\n\t\t\t     val | TG3_PCIE_DL_LO_FTSMAX_VAL);\n\n\t\t\ttw32(GRC_MODE, grc_mode);\n\t\t}\n\n\t\tval = tr32(TG3_CPMU_LSPD_10MB_CLK);\n\t\tval &= ~CPMU_LSPD_10MB_MACCLK_MASK;\n\t\tval |= CPMU_LSPD_10MB_MACCLK_6_25;\n\t\ttw32(TG3_CPMU_LSPD_10MB_CLK, val);\n\t}\n\n\t/* This works around an issue with Athlon chipsets on\n\t * B3 tigon3 silicon.  This bit has no effect on any\n\t * other revision.  But do not set this on PCI Express\n\t * chips and don't even touch the clocks if the CPMU is present.\n\t */\n\tif (!tg3_flag(tp, CPMU_PRESENT)) {\n\t\tif (!tg3_flag(tp, PCI_EXPRESS))\n\t\t\ttp->pci_clock_ctrl |= CLOCK_CTRL_DELAY_PCI_GRANT;\n\t\ttw32_f(TG3PCI_CLOCK_CTRL, tp->pci_clock_ctrl);\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5704_A0 &&\n\t    tg3_flag(tp, PCIX_MODE)) {\n\t\tval = tr32(TG3PCI_PCISTATE);\n\t\tval |= PCISTATE_RETRY_SAME_DMA;\n\t\ttw32(TG3PCI_PCISTATE, val);\n\t}\n\n\tif (tg3_flag(tp, ENABLE_APE)) {\n\t\t/* Allow reads and writes to the\n\t\t * APE register and memory space.\n\t\t */\n\t\tval = tr32(TG3PCI_PCISTATE);\n\t\tval |= PCISTATE_ALLOW_APE_CTLSPC_WR |\n\t\t       PCISTATE_ALLOW_APE_SHMEM_WR |\n\t\t       PCISTATE_ALLOW_APE_PSPACE_WR;\n\t\ttw32(TG3PCI_PCISTATE, val);\n\t}\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5704_BX) {\n\t\t/* Enable some hw fixes.  */\n\t\tval = tr32(TG3PCI_MSI_DATA);\n\t\tval |= (1 << 26) | (1 << 28) | (1 << 29);\n\t\ttw32(TG3PCI_MSI_DATA, val);\n\t}\n\n\t/* Descriptor ring init may make accesses to the\n\t * NIC SRAM area to setup the TX descriptors, so we\n\t * can only do this after the hardware has been\n\t * successfully reset.\n\t */\n\terr = tg3_init_rings(tp);\n\tif (err)\n\t\treturn err;\n\n\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\tval = tr32(TG3PCI_DMA_RW_CTRL) &\n\t\t      ~DMA_RWCTRL_DIS_CACHE_ALIGNMENT;\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_57765_A0)\n\t\t\tval &= ~DMA_RWCTRL_CRDRDR_RDMA_MRRS_MSK;\n\t\tif (!tg3_flag(tp, 57765_CLASS) &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5717 &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5762)\n\t\t\tval |= DMA_RWCTRL_TAGGED_STAT_WA;\n\t\ttw32(TG3PCI_DMA_RW_CTRL, val | tp->dma_rwctrl);\n\t} else if (tg3_asic_rev(tp) != ASIC_REV_5784 &&\n\t\t   tg3_asic_rev(tp) != ASIC_REV_5761) {\n\t\t/* This value is determined during the probe time DMA\n\t\t * engine test, tg3_test_dma.\n\t\t */\n\t\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\t}\n\n\ttp->grc_mode &= ~(GRC_MODE_HOST_SENDBDS |\n\t\t\t  GRC_MODE_4X_NIC_SEND_RINGS |\n\t\t\t  GRC_MODE_NO_TX_PHDR_CSUM |\n\t\t\t  GRC_MODE_NO_RX_PHDR_CSUM);\n\ttp->grc_mode |= GRC_MODE_HOST_SENDBDS;\n\n\t/* Pseudo-header checksum is done by hardware logic and not\n\t * the offload processers, so make the chip do the pseudo-\n\t * header checksums on receive.  For transmit it is more\n\t * convenient to do the pseudo-header checksum in software\n\t * as Linux does that on transmit for us in all cases.\n\t */\n\ttp->grc_mode |= GRC_MODE_NO_TX_PHDR_CSUM;\n\n\tval = GRC_MODE_IRQ_ON_MAC_ATTN | GRC_MODE_HOST_STACKUP;\n\tif (tp->rxptpctl)\n\t\ttw32(TG3_RX_PTP_CTL,\n\t\t     tp->rxptpctl | TG3_RX_PTP_CTL_HWTS_INTERLOCK);\n\n\tif (tg3_flag(tp, PTP_CAPABLE))\n\t\tval |= GRC_MODE_TIME_SYNC_ENABLE;\n\n\ttw32(GRC_MODE, tp->grc_mode | val);\n\n\t/* Setup the timer prescalar register.  Clock is always 66Mhz. */\n\tval = tr32(GRC_MISC_CFG);\n\tval &= ~0xff;\n\tval |= (65 << GRC_MISC_CFG_PRESCALAR_SHIFT);\n\ttw32(GRC_MISC_CFG, val);\n\n\t/* Initialize MBUF/DESC pool. */\n\tif (tg3_flag(tp, 5750_PLUS)) {\n\t\t/* Do nothing.  */\n\t} else if (tg3_asic_rev(tp) != ASIC_REV_5705) {\n\t\ttw32(BUFMGR_MB_POOL_ADDR, NIC_SRAM_MBUF_POOL_BASE);\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5704)\n\t\t\ttw32(BUFMGR_MB_POOL_SIZE, NIC_SRAM_MBUF_POOL_SIZE64);\n\t\telse\n\t\t\ttw32(BUFMGR_MB_POOL_SIZE, NIC_SRAM_MBUF_POOL_SIZE96);\n\t\ttw32(BUFMGR_DMA_DESC_POOL_ADDR, NIC_SRAM_DMA_DESC_POOL_BASE);\n\t\ttw32(BUFMGR_DMA_DESC_POOL_SIZE, NIC_SRAM_DMA_DESC_POOL_SIZE);\n\t} else if (tg3_flag(tp, TSO_CAPABLE)) {\n\t\tint fw_len;\n\n\t\tfw_len = tp->fw_len;\n\t\tfw_len = (fw_len + (0x80 - 1)) & ~(0x80 - 1);\n\t\ttw32(BUFMGR_MB_POOL_ADDR,\n\t\t     NIC_SRAM_MBUF_POOL_BASE5705 + fw_len);\n\t\ttw32(BUFMGR_MB_POOL_SIZE,\n\t\t     NIC_SRAM_MBUF_POOL_SIZE5705 - fw_len - 0xa00);\n\t}\n\n\tif (tp->dev->mtu <= ETH_DATA_LEN) {\n\t\ttw32(BUFMGR_MB_RDMA_LOW_WATER,\n\t\t     tp->bufmgr_config.mbuf_read_dma_low_water);\n\t\ttw32(BUFMGR_MB_MACRX_LOW_WATER,\n\t\t     tp->bufmgr_config.mbuf_mac_rx_low_water);\n\t\ttw32(BUFMGR_MB_HIGH_WATER,\n\t\t     tp->bufmgr_config.mbuf_high_water);\n\t} else {\n\t\ttw32(BUFMGR_MB_RDMA_LOW_WATER,\n\t\t     tp->bufmgr_config.mbuf_read_dma_low_water_jumbo);\n\t\ttw32(BUFMGR_MB_MACRX_LOW_WATER,\n\t\t     tp->bufmgr_config.mbuf_mac_rx_low_water_jumbo);\n\t\ttw32(BUFMGR_MB_HIGH_WATER,\n\t\t     tp->bufmgr_config.mbuf_high_water_jumbo);\n\t}\n\ttw32(BUFMGR_DMA_LOW_WATER,\n\t     tp->bufmgr_config.dma_low_water);\n\ttw32(BUFMGR_DMA_HIGH_WATER,\n\t     tp->bufmgr_config.dma_high_water);\n\n\tval = BUFMGR_MODE_ENABLE | BUFMGR_MODE_ATTN_ENABLE;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\tval |= BUFMGR_MODE_NO_TX_UNDERRUN;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5720_A0)\n\t\tval |= BUFMGR_MODE_MBLOW_ATTN_ENAB;\n\ttw32(BUFMGR_MODE, val);\n\tfor (i = 0; i < 2000; i++) {\n\t\tif (tr32(BUFMGR_MODE) & BUFMGR_MODE_ENABLE)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\tif (i >= 2000) {\n\t\tnetdev_err(tp->dev, \"%s cannot enable BUFMGR\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5906_A1)\n\t\ttw32(ISO_PKT_TX, (tr32(ISO_PKT_TX) & ~0x3) | 0x2);\n\n\ttg3_setup_rxbd_thresholds(tp);\n\n\t/* Initialize TG3_BDINFO's at:\n\t *  RCVDBDI_STD_BD:\tstandard eth size rx ring\n\t *  RCVDBDI_JUMBO_BD:\tjumbo frame rx ring\n\t *  RCVDBDI_MINI_BD:\tsmall frame rx ring (??? does not work)\n\t *\n\t * like so:\n\t *  TG3_BDINFO_HOST_ADDR:\thigh/low parts of DMA address of ring\n\t *  TG3_BDINFO_MAXLEN_FLAGS:\t(rx max buffer size << 16) |\n\t *                              ring attribute flags\n\t *  TG3_BDINFO_NIC_ADDR:\tlocation of descriptors in nic SRAM\n\t *\n\t * Standard receive ring @ NIC_SRAM_RX_BUFFER_DESC, 512 entries.\n\t * Jumbo receive ring @ NIC_SRAM_RX_JUMBO_BUFFER_DESC, 256 entries.\n\t *\n\t * The size of each ring is fixed in the firmware, but the location is\n\t * configurable.\n\t */\n\ttw32(RCVDBDI_STD_BD + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_HIGH,\n\t     ((u64) tpr->rx_std_mapping >> 32));\n\ttw32(RCVDBDI_STD_BD + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_LOW,\n\t     ((u64) tpr->rx_std_mapping & 0xffffffff));\n\tif (!tg3_flag(tp, 5717_PLUS))\n\t\ttw32(RCVDBDI_STD_BD + TG3_BDINFO_NIC_ADDR,\n\t\t     NIC_SRAM_RX_BUFFER_DESC);\n\n\t/* Disable the mini ring */\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\ttw32(RCVDBDI_MINI_BD + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t     BDINFO_FLAGS_DISABLED);\n\n\t/* Program the jumbo buffer descriptor ring control\n\t * blocks on those devices that have them.\n\t */\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0 ||\n\t    (tg3_flag(tp, JUMBO_CAPABLE) && !tg3_flag(tp, 5780_CLASS))) {\n\n\t\tif (tg3_flag(tp, JUMBO_RING_ENABLE)) {\n\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_HIGH,\n\t\t\t     ((u64) tpr->rx_jmb_mapping >> 32));\n\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_HOST_ADDR + TG3_64BIT_REG_LOW,\n\t\t\t     ((u64) tpr->rx_jmb_mapping & 0xffffffff));\n\t\t\tval = TG3_RX_JMB_RING_SIZE(tp) <<\n\t\t\t      BDINFO_FLAGS_MAXLEN_SHIFT;\n\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t\t     val | BDINFO_FLAGS_USE_EXT_RECV);\n\t\t\tif (!tg3_flag(tp, USE_JUMBO_BDFLAG) ||\n\t\t\t    tg3_flag(tp, 57765_CLASS) ||\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_NIC_ADDR,\n\t\t\t\t     NIC_SRAM_RX_JUMBO_BUFFER_DESC);\n\t\t} else {\n\t\t\ttw32(RCVDBDI_JUMBO_BD + TG3_BDINFO_MAXLEN_FLAGS,\n\t\t\t     BDINFO_FLAGS_DISABLED);\n\t\t}\n\n\t\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\t\tval = TG3_RX_STD_RING_SIZE(tp);\n\t\t\tval <<= BDINFO_FLAGS_MAXLEN_SHIFT;\n\t\t\tval |= (TG3_RX_STD_DMA_SZ << 2);\n\t\t} else\n\t\t\tval = TG3_RX_STD_DMA_SZ << BDINFO_FLAGS_MAXLEN_SHIFT;\n\t} else\n\t\tval = TG3_RX_STD_MAX_SIZE_5700 << BDINFO_FLAGS_MAXLEN_SHIFT;\n\n\ttw32(RCVDBDI_STD_BD + TG3_BDINFO_MAXLEN_FLAGS, val);\n\n\ttpr->rx_std_prod_idx = tp->rx_pending;\n\ttw32_rx_mbox(TG3_RX_STD_PROD_IDX_REG, tpr->rx_std_prod_idx);\n\n\ttpr->rx_jmb_prod_idx =\n\t\ttg3_flag(tp, JUMBO_RING_ENABLE) ? tp->rx_jumbo_pending : 0;\n\ttw32_rx_mbox(TG3_RX_JMB_PROD_IDX_REG, tpr->rx_jmb_prod_idx);\n\n\ttg3_rings_reset(tp);\n\n\t/* Initialize MAC address and backoff seed. */\n\t__tg3_set_mac_addr(tp, 0);\n\n\t/* MTU + ethernet header + FCS + optional VLAN tag */\n\ttw32(MAC_RX_MTU_SIZE,\n\t     tp->dev->mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);\n\n\t/* The slot time is changed by tg3_setup_phy if we\n\t * run at gigabit with half duplex.\n\t */\n\tval = (2 << TX_LENGTHS_IPG_CRS_SHIFT) |\n\t      (6 << TX_LENGTHS_IPG_SHIFT) |\n\t      (32 << TX_LENGTHS_SLOT_TIME_SHIFT);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tval |= tr32(MAC_TX_LENGTHS) &\n\t\t       (TX_LENGTHS_JMB_FRM_LEN_MSK |\n\t\t\tTX_LENGTHS_CNT_DWN_VAL_MSK);\n\n\ttw32(MAC_TX_LENGTHS, val);\n\n\t/* Receive rules. */\n\ttw32(MAC_RCV_RULE_CFG, RCV_RULE_CFG_DEFAULT_CLASS);\n\ttw32(RCVLPC_CONFIG, 0x0181);\n\n\t/* Calculate RDMAC_MODE setting early, we need it to determine\n\t * the RCVLPC_STATE_ENABLE mask.\n\t */\n\trdmac_mode = (RDMAC_MODE_ENABLE | RDMAC_MODE_TGTABORT_ENAB |\n\t\t      RDMAC_MODE_MSTABORT_ENAB | RDMAC_MODE_PARITYERR_ENAB |\n\t\t      RDMAC_MODE_ADDROFLOW_ENAB | RDMAC_MODE_FIFOOFLOW_ENAB |\n\t\t      RDMAC_MODE_FIFOURUN_ENAB | RDMAC_MODE_FIFOOREAD_ENAB |\n\t\t      RDMAC_MODE_LNGREAD_ENAB);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717)\n\t\trdmac_mode |= RDMAC_MODE_MULT_DMA_RD_DIS;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780)\n\t\trdmac_mode |= RDMAC_MODE_BD_SBD_CRPT_ENAB |\n\t\t\t      RDMAC_MODE_MBUF_RBD_CRPT_ENAB |\n\t\t\t      RDMAC_MODE_MBUF_SBD_CRPT_ENAB;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A0) {\n\t\tif (tg3_flag(tp, TSO_CAPABLE) &&\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\t\trdmac_mode |= RDMAC_MODE_FIFO_SIZE_128;\n\t\t} else if (!(tr32(TG3PCI_PCISTATE) & PCISTATE_BUS_SPEED_HIGH) &&\n\t\t\t   !tg3_flag(tp, IS_5788)) {\n\t\t\trdmac_mode |= RDMAC_MODE_FIFO_LONG_BURST;\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, PCI_EXPRESS))\n\t\trdmac_mode |= RDMAC_MODE_FIFO_LONG_BURST;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_57766) {\n\t\ttp->dma_limit = 0;\n\t\tif (tp->dev->mtu <= ETH_DATA_LEN) {\n\t\t\trdmac_mode |= RDMAC_MODE_JMB_2K_MMRR;\n\t\t\ttp->dma_limit = TG3_TX_BD_DMA_MAX_2K;\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, HW_TSO_1) ||\n\t    tg3_flag(tp, HW_TSO_2) ||\n\t    tg3_flag(tp, HW_TSO_3))\n\t\trdmac_mode |= RDMAC_MODE_IPV4_LSO_EN;\n\n\tif (tg3_flag(tp, 57765_PLUS) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780)\n\t\trdmac_mode |= RDMAC_MODE_IPV6_LSO_EN;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\trdmac_mode |= tr32(RDMAC_MODE) & RDMAC_MODE_H2BNC_VLAN_DET;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780 ||\n\t    tg3_flag(tp, 57765_PLUS)) {\n\t\tu32 tgtreg;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\t\ttgtreg = TG3_RDMA_RSRVCTRL_REG2;\n\t\telse\n\t\t\ttgtreg = TG3_RDMA_RSRVCTRL_REG;\n\n\t\tval = tr32(tgtreg);\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\t\tval &= ~(TG3_RDMA_RSRVCTRL_TXMRGN_MASK |\n\t\t\t\t TG3_RDMA_RSRVCTRL_FIFO_LWM_MASK |\n\t\t\t\t TG3_RDMA_RSRVCTRL_FIFO_HWM_MASK);\n\t\t\tval |= TG3_RDMA_RSRVCTRL_TXMRGN_320B |\n\t\t\t       TG3_RDMA_RSRVCTRL_FIFO_LWM_1_5K |\n\t\t\t       TG3_RDMA_RSRVCTRL_FIFO_HWM_1_5K;\n\t\t}\n\t\ttw32(tgtreg, val | TG3_RDMA_RSRVCTRL_FIFO_OFLW_FIX);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\tu32 tgtreg;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\t\ttgtreg = TG3_LSO_RD_DMA_CRPTEN_CTRL2;\n\t\telse\n\t\t\ttgtreg = TG3_LSO_RD_DMA_CRPTEN_CTRL;\n\n\t\tval = tr32(tgtreg);\n\t\ttw32(tgtreg, val |\n\t\t     TG3_LSO_RD_DMA_CRPTEN_CTRL_BLEN_BD_4K |\n\t\t     TG3_LSO_RD_DMA_CRPTEN_CTRL_BLEN_LSO_4K);\n\t}\n\n\t/* Receive/send statistics. */\n\tif (tg3_flag(tp, 5750_PLUS)) {\n\t\tval = tr32(RCVLPC_STATS_ENABLE);\n\t\tval &= ~RCVLPC_STATSENAB_DACK_FIX;\n\t\ttw32(RCVLPC_STATS_ENABLE, val);\n\t} else if ((rdmac_mode & RDMAC_MODE_FIFO_SIZE_128) &&\n\t\t   tg3_flag(tp, TSO_CAPABLE)) {\n\t\tval = tr32(RCVLPC_STATS_ENABLE);\n\t\tval &= ~RCVLPC_STATSENAB_LNGBRST_RFIX;\n\t\ttw32(RCVLPC_STATS_ENABLE, val);\n\t} else {\n\t\ttw32(RCVLPC_STATS_ENABLE, 0xffffff);\n\t}\n\ttw32(RCVLPC_STATSCTRL, RCVLPC_STATSCTRL_ENABLE);\n\ttw32(SNDDATAI_STATSENAB, 0xffffff);\n\ttw32(SNDDATAI_STATSCTRL,\n\t     (SNDDATAI_SCTRL_ENABLE |\n\t      SNDDATAI_SCTRL_FASTUPD));\n\n\t/* Setup host coalescing engine. */\n\ttw32(HOSTCC_MODE, 0);\n\tfor (i = 0; i < 2000; i++) {\n\t\tif (!(tr32(HOSTCC_MODE) & HOSTCC_MODE_ENABLE))\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\t__tg3_set_coalesce(tp, &tp->coal);\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\t/* Status/statistics block address.  See tg3_timer,\n\t\t * the tg3_periodic_fetch_stats call there, and\n\t\t * tg3_get_stats to see how this works for 5705/5750 chips.\n\t\t */\n\t\ttw32(HOSTCC_STATS_BLK_HOST_ADDR + TG3_64BIT_REG_HIGH,\n\t\t     ((u64) tp->stats_mapping >> 32));\n\t\ttw32(HOSTCC_STATS_BLK_HOST_ADDR + TG3_64BIT_REG_LOW,\n\t\t     ((u64) tp->stats_mapping & 0xffffffff));\n\t\ttw32(HOSTCC_STATS_BLK_NIC_ADDR, NIC_SRAM_STATS_BLK);\n\n\t\ttw32(HOSTCC_STATUS_BLK_NIC_ADDR, NIC_SRAM_STATUS_BLK);\n\n\t\t/* Clear statistics and status block memory areas */\n\t\tfor (i = NIC_SRAM_STATS_BLK;\n\t\t     i < NIC_SRAM_STATUS_BLK + TG3_HW_STATUS_SIZE;\n\t\t     i += sizeof(u32)) {\n\t\t\ttg3_write_mem(tp, i, 0);\n\t\t\tudelay(40);\n\t\t}\n\t}\n\n\ttw32(HOSTCC_MODE, HOSTCC_MODE_ENABLE | tp->coalesce_mode);\n\n\ttw32(RCVCC_MODE, RCVCC_MODE_ENABLE | RCVCC_MODE_ATTN_ENABLE);\n\ttw32(RCVLPC_MODE, RCVLPC_MODE_ENABLE);\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\ttw32(RCVLSC_MODE, RCVLSC_MODE_ENABLE | RCVLSC_MODE_ATTN_ENABLE);\n\n\tif (tp->phy_flags & TG3_PHYFLG_MII_SERDES) {\n\t\ttp->phy_flags &= ~TG3_PHYFLG_PARALLEL_DETECT;\n\t\t/* reset to prevent losing 1st rx packet intermittently */\n\t\ttw32_f(MAC_RX_MODE, RX_MODE_RESET);\n\t\tudelay(10);\n\t}\n\n\ttp->mac_mode |= MAC_MODE_TXSTAT_ENABLE | MAC_MODE_RXSTAT_ENABLE |\n\t\t\tMAC_MODE_TDE_ENABLE | MAC_MODE_RDE_ENABLE |\n\t\t\tMAC_MODE_FHDE_ENABLE;\n\tif (tg3_flag(tp, ENABLE_APE))\n\t\ttp->mac_mode |= MAC_MODE_APE_TX_EN | MAC_MODE_APE_RX_EN;\n\tif (!tg3_flag(tp, 5705_PLUS) &&\n\t    !(tp->phy_flags & TG3_PHYFLG_PHY_SERDES) &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5700)\n\t\ttp->mac_mode |= MAC_MODE_LINK_POLARITY;\n\ttw32_f(MAC_MODE, tp->mac_mode | MAC_MODE_RXSTAT_CLEAR | MAC_MODE_TXSTAT_CLEAR);\n\tudelay(40);\n\n\t/* tp->grc_local_ctrl is partially set up during tg3_get_invariants().\n\t * If TG3_FLAG_IS_NIC is zero, we should read the\n\t * register to preserve the GPIO settings for LOMs. The GPIOs,\n\t * whether used as inputs or outputs, are set by boot code after\n\t * reset.\n\t */\n\tif (!tg3_flag(tp, IS_NIC)) {\n\t\tu32 gpio_mask;\n\n\t\tgpio_mask = GRC_LCLCTRL_GPIO_OE0 | GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t    GRC_LCLCTRL_GPIO_OE2 | GRC_LCLCTRL_GPIO_OUTPUT0 |\n\t\t\t    GRC_LCLCTRL_GPIO_OUTPUT1 | GRC_LCLCTRL_GPIO_OUTPUT2;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5752)\n\t\t\tgpio_mask |= GRC_LCLCTRL_GPIO_OE3 |\n\t\t\t\t     GRC_LCLCTRL_GPIO_OUTPUT3;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5755)\n\t\t\tgpio_mask |= GRC_LCLCTRL_GPIO_UART_SEL;\n\n\t\ttp->grc_local_ctrl &= ~gpio_mask;\n\t\ttp->grc_local_ctrl |= tr32(GRC_LOCAL_CTRL) & gpio_mask;\n\n\t\t/* GPIO1 must be driven high for eeprom write protect */\n\t\tif (tg3_flag(tp, EEPROM_WRITE_PROT))\n\t\t\ttp->grc_local_ctrl |= (GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t\t\t       GRC_LCLCTRL_GPIO_OUTPUT1);\n\t}\n\ttw32_f(GRC_LOCAL_CTRL, tp->grc_local_ctrl);\n\tudelay(100);\n\n\tif (tg3_flag(tp, USING_MSIX)) {\n\t\tval = tr32(MSGINT_MODE);\n\t\tval |= MSGINT_MODE_ENABLE;\n\t\tif (tp->irq_cnt > 1)\n\t\t\tval |= MSGINT_MODE_MULTIVEC_EN;\n\t\tif (!tg3_flag(tp, 1SHOT_MSI))\n\t\t\tval |= MSGINT_MODE_ONE_SHOT_DISABLE;\n\t\ttw32(MSGINT_MODE, val);\n\t}\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\ttw32_f(DMAC_MODE, DMAC_MODE_ENABLE);\n\t\tudelay(40);\n\t}\n\n\tval = (WDMAC_MODE_ENABLE | WDMAC_MODE_TGTABORT_ENAB |\n\t       WDMAC_MODE_MSTABORT_ENAB | WDMAC_MODE_PARITYERR_ENAB |\n\t       WDMAC_MODE_ADDROFLOW_ENAB | WDMAC_MODE_FIFOOFLOW_ENAB |\n\t       WDMAC_MODE_FIFOURUN_ENAB | WDMAC_MODE_FIFOOREAD_ENAB |\n\t       WDMAC_MODE_LNGREAD_ENAB);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A0) {\n\t\tif (tg3_flag(tp, TSO_CAPABLE) &&\n\t\t    (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A1 ||\n\t\t     tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A2)) {\n\t\t\t/* nothing */\n\t\t} else if (!(tr32(TG3PCI_PCISTATE) & PCISTATE_BUS_SPEED_HIGH) &&\n\t\t\t   !tg3_flag(tp, IS_5788)) {\n\t\t\tval |= WDMAC_MODE_RX_ACCEL;\n\t\t}\n\t}\n\n\t/* Enable host coalescing bug fix */\n\tif (tg3_flag(tp, 5755_PLUS))\n\t\tval |= WDMAC_MODE_STATUS_TAG_FIX;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\tval |= WDMAC_MODE_BURST_ALL_DATA;\n\n\ttw32_f(WDMAC_MODE, val);\n\tudelay(40);\n\n\tif (tg3_flag(tp, PCIX_MODE)) {\n\t\tu16 pcix_cmd;\n\n\t\tpci_read_config_word(tp->pdev, tp->pcix_cap + PCI_X_CMD,\n\t\t\t\t     &pcix_cmd);\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5703) {\n\t\t\tpcix_cmd &= ~PCI_X_CMD_MAX_READ;\n\t\t\tpcix_cmd |= PCI_X_CMD_READ_2K;\n\t\t} else if (tg3_asic_rev(tp) == ASIC_REV_5704) {\n\t\t\tpcix_cmd &= ~(PCI_X_CMD_MAX_SPLIT | PCI_X_CMD_MAX_READ);\n\t\t\tpcix_cmd |= PCI_X_CMD_READ_2K;\n\t\t}\n\t\tpci_write_config_word(tp->pdev, tp->pcix_cap + PCI_X_CMD,\n\t\t\t\t      pcix_cmd);\n\t}\n\n\ttw32_f(RDMAC_MODE, rdmac_mode);\n\tudelay(40);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719) {\n\t\tfor (i = 0; i < TG3_NUM_RDMA_CHANNELS; i++) {\n\t\t\tif (tr32(TG3_RDMA_LENGTH + (i << 2)) > TG3_MAX_MTU(tp))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i < TG3_NUM_RDMA_CHANNELS) {\n\t\t\tval = tr32(TG3_LSO_RD_DMA_CRPTEN_CTRL);\n\t\t\tval |= TG3_LSO_RD_DMA_TX_LENGTH_WA;\n\t\t\ttw32(TG3_LSO_RD_DMA_CRPTEN_CTRL, val);\n\t\t\ttg3_flag_set(tp, 5719_RDMA_BUG);\n\t\t}\n\t}\n\n\ttw32(RCVDCC_MODE, RCVDCC_MODE_ENABLE | RCVDCC_MODE_ATTN_ENABLE);\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\ttw32(MBFREE_MODE, MBFREE_MODE_ENABLE);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\ttw32(SNDDATAC_MODE,\n\t\t     SNDDATAC_MODE_ENABLE | SNDDATAC_MODE_CDELAY);\n\telse\n\t\ttw32(SNDDATAC_MODE, SNDDATAC_MODE_ENABLE);\n\n\ttw32(SNDBDC_MODE, SNDBDC_MODE_ENABLE | SNDBDC_MODE_ATTN_ENABLE);\n\ttw32(RCVBDI_MODE, RCVBDI_MODE_ENABLE | RCVBDI_MODE_RCB_ATTN_ENAB);\n\tval = RCVDBDI_MODE_ENABLE | RCVDBDI_MODE_INV_RING_SZ;\n\tif (tg3_flag(tp, LRG_PROD_RING_CAP))\n\t\tval |= RCVDBDI_MODE_LRG_RING_SZ;\n\ttw32(RCVDBDI_MODE, val);\n\ttw32(SNDDATAI_MODE, SNDDATAI_MODE_ENABLE);\n\tif (tg3_flag(tp, HW_TSO_1) ||\n\t    tg3_flag(tp, HW_TSO_2) ||\n\t    tg3_flag(tp, HW_TSO_3))\n\t\ttw32(SNDDATAI_MODE, SNDDATAI_MODE_ENABLE | 0x8);\n\tval = SNDBDI_MODE_ENABLE | SNDBDI_MODE_ATTN_ENABLE;\n\tif (tg3_flag(tp, ENABLE_TSS))\n\t\tval |= SNDBDI_MODE_MULTI_TXQ_EN;\n\ttw32(SNDBDI_MODE, val);\n\ttw32(SNDBDS_MODE, SNDBDS_MODE_ENABLE | SNDBDS_MODE_ATTN_ENABLE);\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0) {\n\t\terr = tg3_load_5701_a0_firmware_fix(tp);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (tg3_flag(tp, TSO_CAPABLE)) {\n\t\terr = tg3_load_tso_firmware(tp);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\ttp->tx_mode = TX_MODE_ENABLE;\n\n\tif (tg3_flag(tp, 5755_PLUS) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\ttp->tx_mode |= TX_MODE_MBUF_LOCKUP_FIX;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\tval = TX_MODE_JMB_FRM_LEN | TX_MODE_CNT_DN_MODE;\n\t\ttp->tx_mode &= ~val;\n\t\ttp->tx_mode |= tr32(MAC_TX_MODE) & val;\n\t}\n\n\ttw32_f(MAC_TX_MODE, tp->tx_mode);\n\tudelay(100);\n\n\tif (tg3_flag(tp, ENABLE_RSS)) {\n\t\ttg3_rss_write_indir_tbl(tp);\n\n\t\t/* Setup the \"secret\" hash key. */\n\t\ttw32(MAC_RSS_HASH_KEY_0, 0x5f865437);\n\t\ttw32(MAC_RSS_HASH_KEY_1, 0xe4ac62cc);\n\t\ttw32(MAC_RSS_HASH_KEY_2, 0x50103a45);\n\t\ttw32(MAC_RSS_HASH_KEY_3, 0x36621985);\n\t\ttw32(MAC_RSS_HASH_KEY_4, 0xbf14c0e8);\n\t\ttw32(MAC_RSS_HASH_KEY_5, 0x1bc27a1e);\n\t\ttw32(MAC_RSS_HASH_KEY_6, 0x84f4b556);\n\t\ttw32(MAC_RSS_HASH_KEY_7, 0x094ea6fe);\n\t\ttw32(MAC_RSS_HASH_KEY_8, 0x7dda01e7);\n\t\ttw32(MAC_RSS_HASH_KEY_9, 0xc04d7481);\n\t}\n\n\ttp->rx_mode = RX_MODE_ENABLE;\n\tif (tg3_flag(tp, 5755_PLUS))\n\t\ttp->rx_mode |= RX_MODE_IPV6_CSUM_ENABLE;\n\n\tif (tg3_flag(tp, ENABLE_RSS))\n\t\ttp->rx_mode |= RX_MODE_RSS_ENABLE |\n\t\t\t       RX_MODE_RSS_ITBL_HASH_BITS_7 |\n\t\t\t       RX_MODE_RSS_IPV6_HASH_EN |\n\t\t\t       RX_MODE_RSS_TCP_IPV6_HASH_EN |\n\t\t\t       RX_MODE_RSS_IPV4_HASH_EN |\n\t\t\t       RX_MODE_RSS_TCP_IPV4_HASH_EN;\n\n\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\tudelay(10);\n\n\ttw32(MAC_LED_CTRL, tp->led_ctrl);\n\n\ttw32(MAC_MI_STAT, MAC_MI_STAT_LNKSTAT_ATTN_ENAB);\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\ttw32_f(MAC_RX_MODE, RX_MODE_RESET);\n\t\tudelay(10);\n\t}\n\ttw32_f(MAC_RX_MODE, tp->rx_mode);\n\tudelay(10);\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\tif ((tg3_asic_rev(tp) == ASIC_REV_5704) &&\n\t\t    !(tp->phy_flags & TG3_PHYFLG_SERDES_PREEMPHASIS)) {\n\t\t\t/* Set drive transmission level to 1.2V  */\n\t\t\t/* only if the signal pre-emphasis bit is not set  */\n\t\t\tval = tr32(MAC_SERDES_CFG);\n\t\t\tval &= 0xfffff000;\n\t\t\tval |= 0x880;\n\t\t\ttw32(MAC_SERDES_CFG, val);\n\t\t}\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5703_A1)\n\t\t\ttw32(MAC_SERDES_CFG, 0x616000);\n\t}\n\n\t/* Prevent chip from dropping frames when flow control\n\t * is enabled.\n\t */\n\tif (tg3_flag(tp, 57765_CLASS))\n\t\tval = 1;\n\telse\n\t\tval = 2;\n\ttw32_f(MAC_LOW_WMARK_MAX_RX_FRAME, val);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5704 &&\n\t    (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)) {\n\t\t/* Use hardware link auto-negotiation */\n\t\ttg3_flag_set(tp, HW_AUTONEG);\n\t}\n\n\tif ((tp->phy_flags & TG3_PHYFLG_MII_SERDES) &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\tu32 tmp;\n\n\t\ttmp = tr32(SERDES_RX_CTRL);\n\t\ttw32(SERDES_RX_CTRL, tmp | SERDES_RX_SIG_DETECT);\n\t\ttp->grc_local_ctrl &= ~GRC_LCLCTRL_USE_EXT_SIG_DETECT;\n\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_USE_SIG_DETECT;\n\t\ttw32(GRC_LOCAL_CTRL, tp->grc_local_ctrl);\n\t}\n\n\tif (!tg3_flag(tp, USE_PHYLIB)) {\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_IS_LOW_POWER;\n\n\t\terr = tg3_setup_phy(tp, 0);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_PHY_SERDES) &&\n\t\t    !(tp->phy_flags & TG3_PHYFLG_IS_FET)) {\n\t\t\tu32 tmp;\n\n\t\t\t/* Clear CRC stats. */\n\t\t\tif (!tg3_readphy(tp, MII_TG3_TEST1, &tmp)) {\n\t\t\t\ttg3_writephy(tp, MII_TG3_TEST1,\n\t\t\t\t\t     tmp | MII_TG3_TEST1_CRC_EN);\n\t\t\t\ttg3_readphy(tp, MII_TG3_RXR_COUNTERS, &tmp);\n\t\t\t}\n\t\t}\n\t}\n\n\t__tg3_set_rx_mode(tp->dev);\n\n\t/* Initialize receive rules. */\n\ttw32(MAC_RCV_RULE_0,  0xc2000000 & RCV_RULE_DISABLE_MASK);\n\ttw32(MAC_RCV_VALUE_0, 0xffffffff & RCV_RULE_DISABLE_MASK);\n\ttw32(MAC_RCV_RULE_1,  0x86000004 & RCV_RULE_DISABLE_MASK);\n\ttw32(MAC_RCV_VALUE_1, 0xffffffff & RCV_RULE_DISABLE_MASK);\n\n\tif (tg3_flag(tp, 5705_PLUS) && !tg3_flag(tp, 5780_CLASS))\n\t\tlimit = 8;\n\telse\n\t\tlimit = 16;\n\tif (tg3_flag(tp, ENABLE_ASF))\n\t\tlimit -= 4;\n\tswitch (limit) {\n\tcase 16:\n\t\ttw32(MAC_RCV_RULE_15,  0); tw32(MAC_RCV_VALUE_15,  0);\n\tcase 15:\n\t\ttw32(MAC_RCV_RULE_14,  0); tw32(MAC_RCV_VALUE_14,  0);\n\tcase 14:\n\t\ttw32(MAC_RCV_RULE_13,  0); tw32(MAC_RCV_VALUE_13,  0);\n\tcase 13:\n\t\ttw32(MAC_RCV_RULE_12,  0); tw32(MAC_RCV_VALUE_12,  0);\n\tcase 12:\n\t\ttw32(MAC_RCV_RULE_11,  0); tw32(MAC_RCV_VALUE_11,  0);\n\tcase 11:\n\t\ttw32(MAC_RCV_RULE_10,  0); tw32(MAC_RCV_VALUE_10,  0);\n\tcase 10:\n\t\ttw32(MAC_RCV_RULE_9,  0); tw32(MAC_RCV_VALUE_9,  0);\n\tcase 9:\n\t\ttw32(MAC_RCV_RULE_8,  0); tw32(MAC_RCV_VALUE_8,  0);\n\tcase 8:\n\t\ttw32(MAC_RCV_RULE_7,  0); tw32(MAC_RCV_VALUE_7,  0);\n\tcase 7:\n\t\ttw32(MAC_RCV_RULE_6,  0); tw32(MAC_RCV_VALUE_6,  0);\n\tcase 6:\n\t\ttw32(MAC_RCV_RULE_5,  0); tw32(MAC_RCV_VALUE_5,  0);\n\tcase 5:\n\t\ttw32(MAC_RCV_RULE_4,  0); tw32(MAC_RCV_VALUE_4,  0);\n\tcase 4:\n\t\t/* tw32(MAC_RCV_RULE_3,  0); tw32(MAC_RCV_VALUE_3,  0); */\n\tcase 3:\n\t\t/* tw32(MAC_RCV_RULE_2,  0); tw32(MAC_RCV_VALUE_2,  0); */\n\tcase 2:\n\tcase 1:\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (tg3_flag(tp, ENABLE_APE))\n\t\t/* Write our heartbeat update interval to APE. */\n\t\ttg3_ape_write32(tp, TG3_APE_HOST_HEARTBEAT_INT_MS,\n\t\t\t\tAPE_HOST_HEARTBEAT_INT_DISABLE);\n\n\ttg3_write_sig_post_reset(tp, RESET_KIND_INIT);\n\n\treturn 0;\n}\n\n/* Called at device open time to get the chip ready for\n * packet processing.  Invoked with tp->lock held.\n */\nstatic int tg3_init_hw(struct tg3 *tp, int reset_phy)\n{\n\ttg3_switch_clocks(tp);\n\n\ttw32(TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\n\treturn tg3_reset_hw(tp, reset_phy);\n}\n\nstatic void tg3_sd_scan_scratchpad(struct tg3 *tp, struct tg3_ocir *ocir)\n{\n\tint i;\n\n\tfor (i = 0; i < TG3_SD_NUM_RECS; i++, ocir++) {\n\t\tu32 off = i * TG3_OCIR_LEN, len = TG3_OCIR_LEN;\n\n\t\ttg3_ape_scratchpad_read(tp, (u32 *) ocir, off, len);\n\t\toff += len;\n\n\t\tif (ocir->signature != TG3_OCIR_SIG_MAGIC ||\n\t\t    !(ocir->version_flags & TG3_OCIR_FLAG_ACTIVE))\n\t\t\tmemset(ocir, 0, TG3_OCIR_LEN);\n\t}\n}\n\n/* sysfs attributes for hwmon */\nstatic ssize_t tg3_show_temp(struct device *dev,\n\t\t\t     struct device_attribute *devattr, char *buf)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(netdev);\n\tstruct sensor_device_attribute *attr = to_sensor_dev_attr(devattr);\n\tu32 temperature;\n\n\tspin_lock_bh(&tp->lock);\n\ttg3_ape_scratchpad_read(tp, &temperature, attr->index,\n\t\t\t\tsizeof(temperature));\n\tspin_unlock_bh(&tp->lock);\n\treturn sprintf(buf, \"%u\\n\", temperature);\n}\n\n\nstatic SENSOR_DEVICE_ATTR(temp1_input, S_IRUGO, tg3_show_temp, NULL,\n\t\t\t  TG3_TEMP_SENSOR_OFFSET);\nstatic SENSOR_DEVICE_ATTR(temp1_crit, S_IRUGO, tg3_show_temp, NULL,\n\t\t\t  TG3_TEMP_CAUTION_OFFSET);\nstatic SENSOR_DEVICE_ATTR(temp1_max, S_IRUGO, tg3_show_temp, NULL,\n\t\t\t  TG3_TEMP_MAX_OFFSET);\n\nstatic struct attribute *tg3_attributes[] = {\n\t&sensor_dev_attr_temp1_input.dev_attr.attr,\n\t&sensor_dev_attr_temp1_crit.dev_attr.attr,\n\t&sensor_dev_attr_temp1_max.dev_attr.attr,\n\tNULL\n};\n\nstatic const struct attribute_group tg3_group = {\n\t.attrs = tg3_attributes,\n};\n\nstatic void tg3_hwmon_close(struct tg3 *tp)\n{\n\tif (tp->hwmon_dev) {\n\t\thwmon_device_unregister(tp->hwmon_dev);\n\t\ttp->hwmon_dev = NULL;\n\t\tsysfs_remove_group(&tp->pdev->dev.kobj, &tg3_group);\n\t}\n}\n\nstatic void tg3_hwmon_open(struct tg3 *tp)\n{\n\tint i, err;\n\tu32 size = 0;\n\tstruct pci_dev *pdev = tp->pdev;\n\tstruct tg3_ocir ocirs[TG3_SD_NUM_RECS];\n\n\ttg3_sd_scan_scratchpad(tp, ocirs);\n\n\tfor (i = 0; i < TG3_SD_NUM_RECS; i++) {\n\t\tif (!ocirs[i].src_data_length)\n\t\t\tcontinue;\n\n\t\tsize += ocirs[i].src_hdr_length;\n\t\tsize += ocirs[i].src_data_length;\n\t}\n\n\tif (!size)\n\t\treturn;\n\n\t/* Register hwmon sysfs hooks */\n\terr = sysfs_create_group(&pdev->dev.kobj, &tg3_group);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot create sysfs group, aborting\\n\");\n\t\treturn;\n\t}\n\n\ttp->hwmon_dev = hwmon_device_register(&pdev->dev);\n\tif (IS_ERR(tp->hwmon_dev)) {\n\t\ttp->hwmon_dev = NULL;\n\t\tdev_err(&pdev->dev, \"Cannot register hwmon device, aborting\\n\");\n\t\tsysfs_remove_group(&pdev->dev.kobj, &tg3_group);\n\t}\n}\n\n\n#define TG3_STAT_ADD32(PSTAT, REG) \\\ndo {\tu32 __val = tr32(REG); \\\n\t(PSTAT)->low += __val; \\\n\tif ((PSTAT)->low < __val) \\\n\t\t(PSTAT)->high += 1; \\\n} while (0)\n\nstatic void tg3_periodic_fetch_stats(struct tg3 *tp)\n{\n\tstruct tg3_hw_stats *sp = tp->hw_stats;\n\n\tif (!tp->link_up)\n\t\treturn;\n\n\tTG3_STAT_ADD32(&sp->tx_octets, MAC_TX_STATS_OCTETS);\n\tTG3_STAT_ADD32(&sp->tx_collisions, MAC_TX_STATS_COLLISIONS);\n\tTG3_STAT_ADD32(&sp->tx_xon_sent, MAC_TX_STATS_XON_SENT);\n\tTG3_STAT_ADD32(&sp->tx_xoff_sent, MAC_TX_STATS_XOFF_SENT);\n\tTG3_STAT_ADD32(&sp->tx_mac_errors, MAC_TX_STATS_MAC_ERRORS);\n\tTG3_STAT_ADD32(&sp->tx_single_collisions, MAC_TX_STATS_SINGLE_COLLISIONS);\n\tTG3_STAT_ADD32(&sp->tx_mult_collisions, MAC_TX_STATS_MULT_COLLISIONS);\n\tTG3_STAT_ADD32(&sp->tx_deferred, MAC_TX_STATS_DEFERRED);\n\tTG3_STAT_ADD32(&sp->tx_excessive_collisions, MAC_TX_STATS_EXCESSIVE_COL);\n\tTG3_STAT_ADD32(&sp->tx_late_collisions, MAC_TX_STATS_LATE_COL);\n\tTG3_STAT_ADD32(&sp->tx_ucast_packets, MAC_TX_STATS_UCAST);\n\tTG3_STAT_ADD32(&sp->tx_mcast_packets, MAC_TX_STATS_MCAST);\n\tTG3_STAT_ADD32(&sp->tx_bcast_packets, MAC_TX_STATS_BCAST);\n\tif (unlikely(tg3_flag(tp, 5719_RDMA_BUG) &&\n\t\t     (sp->tx_ucast_packets.low + sp->tx_mcast_packets.low +\n\t\t      sp->tx_bcast_packets.low) > TG3_NUM_RDMA_CHANNELS)) {\n\t\tu32 val;\n\n\t\tval = tr32(TG3_LSO_RD_DMA_CRPTEN_CTRL);\n\t\tval &= ~TG3_LSO_RD_DMA_TX_LENGTH_WA;\n\t\ttw32(TG3_LSO_RD_DMA_CRPTEN_CTRL, val);\n\t\ttg3_flag_clear(tp, 5719_RDMA_BUG);\n\t}\n\n\tTG3_STAT_ADD32(&sp->rx_octets, MAC_RX_STATS_OCTETS);\n\tTG3_STAT_ADD32(&sp->rx_fragments, MAC_RX_STATS_FRAGMENTS);\n\tTG3_STAT_ADD32(&sp->rx_ucast_packets, MAC_RX_STATS_UCAST);\n\tTG3_STAT_ADD32(&sp->rx_mcast_packets, MAC_RX_STATS_MCAST);\n\tTG3_STAT_ADD32(&sp->rx_bcast_packets, MAC_RX_STATS_BCAST);\n\tTG3_STAT_ADD32(&sp->rx_fcs_errors, MAC_RX_STATS_FCS_ERRORS);\n\tTG3_STAT_ADD32(&sp->rx_align_errors, MAC_RX_STATS_ALIGN_ERRORS);\n\tTG3_STAT_ADD32(&sp->rx_xon_pause_rcvd, MAC_RX_STATS_XON_PAUSE_RECVD);\n\tTG3_STAT_ADD32(&sp->rx_xoff_pause_rcvd, MAC_RX_STATS_XOFF_PAUSE_RECVD);\n\tTG3_STAT_ADD32(&sp->rx_mac_ctrl_rcvd, MAC_RX_STATS_MAC_CTRL_RECVD);\n\tTG3_STAT_ADD32(&sp->rx_xoff_entered, MAC_RX_STATS_XOFF_ENTERED);\n\tTG3_STAT_ADD32(&sp->rx_frame_too_long_errors, MAC_RX_STATS_FRAME_TOO_LONG);\n\tTG3_STAT_ADD32(&sp->rx_jabbers, MAC_RX_STATS_JABBERS);\n\tTG3_STAT_ADD32(&sp->rx_undersize_packets, MAC_RX_STATS_UNDERSIZE);\n\n\tTG3_STAT_ADD32(&sp->rxbds_empty, RCVLPC_NO_RCV_BD_CNT);\n\tif (tg3_asic_rev(tp) != ASIC_REV_5717 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5719_A0 &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5720_A0) {\n\t\tTG3_STAT_ADD32(&sp->rx_discards, RCVLPC_IN_DISCARDS_CNT);\n\t} else {\n\t\tu32 val = tr32(HOSTCC_FLOW_ATTN);\n\t\tval = (val & HOSTCC_FLOW_ATTN_MBUF_LWM) ? 1 : 0;\n\t\tif (val) {\n\t\t\ttw32(HOSTCC_FLOW_ATTN, HOSTCC_FLOW_ATTN_MBUF_LWM);\n\t\t\tsp->rx_discards.low += val;\n\t\t\tif (sp->rx_discards.low < val)\n\t\t\t\tsp->rx_discards.high += 1;\n\t\t}\n\t\tsp->mbuf_lwm_thresh_hit = sp->rx_discards;\n\t}\n\tTG3_STAT_ADD32(&sp->rx_errors, RCVLPC_IN_ERRORS_CNT);\n}\n\nstatic void tg3_chk_missed_msi(struct tg3 *tp)\n{\n\tu32 i;\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\tif (tg3_has_work(tnapi)) {\n\t\t\tif (tnapi->last_rx_cons == tnapi->rx_rcb_ptr &&\n\t\t\t    tnapi->last_tx_cons == tnapi->tx_cons) {\n\t\t\t\tif (tnapi->chk_msi_cnt < 1) {\n\t\t\t\t\ttnapi->chk_msi_cnt++;\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\ttg3_msi(0, tnapi);\n\t\t\t}\n\t\t}\n\t\ttnapi->chk_msi_cnt = 0;\n\t\ttnapi->last_rx_cons = tnapi->rx_rcb_ptr;\n\t\ttnapi->last_tx_cons = tnapi->tx_cons;\n\t}\n}\n\nstatic void tg3_timer(unsigned long __opaque)\n{\n\tstruct tg3 *tp = (struct tg3 *) __opaque;\n\n\tif (tp->irq_sync || tg3_flag(tp, RESET_TASK_PENDING))\n\t\tgoto restart_timer;\n\n\tspin_lock(&tp->lock);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_flag(tp, 57765_CLASS))\n\t\ttg3_chk_missed_msi(tp);\n\n\tif (tg3_flag(tp, FLUSH_POSTED_WRITES)) {\n\t\t/* BCM4785: Flush posted writes from GbE to host memory. */\n\t\ttr32(HOSTCC_MODE);\n\t}\n\n\tif (!tg3_flag(tp, TAGGED_STATUS)) {\n\t\t/* All of this garbage is because when using non-tagged\n\t\t * IRQ status the mailbox/status_block protocol the chip\n\t\t * uses with the cpu is race prone.\n\t\t */\n\t\tif (tp->napi[0].hw_status->status & SD_STATUS_UPDATED) {\n\t\t\ttw32(GRC_LOCAL_CTRL,\n\t\t\t     tp->grc_local_ctrl | GRC_LCLCTRL_SETINT);\n\t\t} else {\n\t\t\ttw32(HOSTCC_MODE, tp->coalesce_mode |\n\t\t\t     HOSTCC_MODE_ENABLE | HOSTCC_MODE_NOW);\n\t\t}\n\n\t\tif (!(tr32(WDMAC_MODE) & WDMAC_MODE_ENABLE)) {\n\t\t\tspin_unlock(&tp->lock);\n\t\t\ttg3_reset_task_schedule(tp);\n\t\t\tgoto restart_timer;\n\t\t}\n\t}\n\n\t/* This part only runs once per second. */\n\tif (!--tp->timer_counter) {\n\t\tif (tg3_flag(tp, 5705_PLUS))\n\t\t\ttg3_periodic_fetch_stats(tp);\n\n\t\tif (tp->setlpicnt && !--tp->setlpicnt)\n\t\t\ttg3_phy_eee_enable(tp);\n\n\t\tif (tg3_flag(tp, USE_LINKCHG_REG)) {\n\t\t\tu32 mac_stat;\n\t\t\tint phy_event;\n\n\t\t\tmac_stat = tr32(MAC_STATUS);\n\n\t\t\tphy_event = 0;\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_USE_MI_INTERRUPT) {\n\t\t\t\tif (mac_stat & MAC_STATUS_MI_INTERRUPT)\n\t\t\t\t\tphy_event = 1;\n\t\t\t} else if (mac_stat & MAC_STATUS_LNKSTATE_CHANGED)\n\t\t\t\tphy_event = 1;\n\n\t\t\tif (phy_event)\n\t\t\t\ttg3_setup_phy(tp, 0);\n\t\t} else if (tg3_flag(tp, POLL_SERDES)) {\n\t\t\tu32 mac_stat = tr32(MAC_STATUS);\n\t\t\tint need_setup = 0;\n\n\t\t\tif (tp->link_up &&\n\t\t\t    (mac_stat & MAC_STATUS_LNKSTATE_CHANGED)) {\n\t\t\t\tneed_setup = 1;\n\t\t\t}\n\t\t\tif (!tp->link_up &&\n\t\t\t    (mac_stat & (MAC_STATUS_PCS_SYNCED |\n\t\t\t\t\t MAC_STATUS_SIGNAL_DET))) {\n\t\t\t\tneed_setup = 1;\n\t\t\t}\n\t\t\tif (need_setup) {\n\t\t\t\tif (!tp->serdes_counter) {\n\t\t\t\t\ttw32_f(MAC_MODE,\n\t\t\t\t\t     (tp->mac_mode &\n\t\t\t\t\t      ~MAC_MODE_PORT_MODE_MASK));\n\t\t\t\t\tudelay(40);\n\t\t\t\t\ttw32_f(MAC_MODE, tp->mac_mode);\n\t\t\t\t\tudelay(40);\n\t\t\t\t}\n\t\t\t\ttg3_setup_phy(tp, 0);\n\t\t\t}\n\t\t} else if ((tp->phy_flags & TG3_PHYFLG_MII_SERDES) &&\n\t\t\t   tg3_flag(tp, 5780_CLASS)) {\n\t\t\ttg3_serdes_parallel_detect(tp);\n\t\t}\n\n\t\ttp->timer_counter = tp->timer_multiplier;\n\t}\n\n\t/* Heartbeat is only sent once every 2 seconds.\n\t *\n\t * The heartbeat is to tell the ASF firmware that the host\n\t * driver is still alive.  In the event that the OS crashes,\n\t * ASF needs to reset the hardware to free up the FIFO space\n\t * that may be filled with rx packets destined for the host.\n\t * If the FIFO is full, ASF will no longer function properly.\n\t *\n\t * Unintended resets have been reported on real time kernels\n\t * where the timer doesn't run on time.  Netpoll will also have\n\t * same problem.\n\t *\n\t * The new FWCMD_NICDRV_ALIVE3 command tells the ASF firmware\n\t * to check the ring condition when the heartbeat is expiring\n\t * before doing the reset.  This will prevent most unintended\n\t * resets.\n\t */\n\tif (!--tp->asf_counter) {\n\t\tif (tg3_flag(tp, ENABLE_ASF) && !tg3_flag(tp, ENABLE_APE)) {\n\t\t\ttg3_wait_for_event_ack(tp);\n\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_MBOX,\n\t\t\t\t      FWCMD_NICDRV_ALIVE3);\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_LEN_MBOX, 4);\n\t\t\ttg3_write_mem(tp, NIC_SRAM_FW_CMD_DATA_MBOX,\n\t\t\t\t      TG3_FW_UPDATE_TIMEOUT_SEC);\n\n\t\t\ttg3_generate_fw_event(tp);\n\t\t}\n\t\ttp->asf_counter = tp->asf_multiplier;\n\t}\n\n\tspin_unlock(&tp->lock);\n\nrestart_timer:\n\ttp->timer.expires = jiffies + tp->timer_offset;\n\tadd_timer(&tp->timer);\n}\n\nstatic void tg3_timer_init(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, TAGGED_STATUS) &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5717 &&\n\t    !tg3_flag(tp, 57765_CLASS))\n\t\ttp->timer_offset = HZ;\n\telse\n\t\ttp->timer_offset = HZ / 10;\n\n\tBUG_ON(tp->timer_offset > HZ);\n\n\ttp->timer_multiplier = (HZ / tp->timer_offset);\n\ttp->asf_multiplier = (HZ / tp->timer_offset) *\n\t\t\t     TG3_FW_UPDATE_FREQ_SEC;\n\n\tinit_timer(&tp->timer);\n\ttp->timer.data = (unsigned long) tp;\n\ttp->timer.function = tg3_timer;\n}\n\nstatic void tg3_timer_start(struct tg3 *tp)\n{\n\ttp->asf_counter   = tp->asf_multiplier;\n\ttp->timer_counter = tp->timer_multiplier;\n\n\ttp->timer.expires = jiffies + tp->timer_offset;\n\tadd_timer(&tp->timer);\n}\n\nstatic void tg3_timer_stop(struct tg3 *tp)\n{\n\tdel_timer_sync(&tp->timer);\n}\n\n/* Restart hardware after configuration changes, self-test, etc.\n * Invoked with tp->lock held.\n */\nstatic int tg3_restart_hw(struct tg3 *tp, int reset_phy)\n\t__releases(tp->lock)\n\t__acquires(tp->lock)\n{\n\tint err;\n\n\terr = tg3_init_hw(tp, reset_phy);\n\tif (err) {\n\t\tnetdev_err(tp->dev,\n\t\t\t   \"Failed to re-initialize device, aborting\\n\");\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\ttg3_full_unlock(tp);\n\t\ttg3_timer_stop(tp);\n\t\ttp->irq_sync = 0;\n\t\ttg3_napi_enable(tp);\n\t\tdev_close(tp->dev);\n\t\ttg3_full_lock(tp, 0);\n\t}\n\treturn err;\n}\n\nstatic void tg3_reset_task(struct work_struct *work)\n{\n\tstruct tg3 *tp = container_of(work, struct tg3, reset_task);\n\tint err;\n\n\ttg3_full_lock(tp, 0);\n\n\tif (!netif_running(tp->dev)) {\n\t\ttg3_flag_clear(tp, RESET_TASK_PENDING);\n\t\ttg3_full_unlock(tp);\n\t\treturn;\n\t}\n\n\ttg3_full_unlock(tp);\n\n\ttg3_phy_stop(tp);\n\n\ttg3_netif_stop(tp);\n\n\ttg3_full_lock(tp, 1);\n\n\tif (tg3_flag(tp, TX_RECOVERY_PENDING)) {\n\t\ttp->write32_tx_mbox = tg3_write32_tx_mbox;\n\t\ttp->write32_rx_mbox = tg3_write_flush_reg32;\n\t\ttg3_flag_set(tp, MBOX_WRITE_REORDER);\n\t\ttg3_flag_clear(tp, TX_RECOVERY_PENDING);\n\t}\n\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 0);\n\terr = tg3_init_hw(tp, 1);\n\tif (err)\n\t\tgoto out;\n\n\ttg3_netif_start(tp);\n\nout:\n\ttg3_full_unlock(tp);\n\n\tif (!err)\n\t\ttg3_phy_start(tp);\n\n\ttg3_flag_clear(tp, RESET_TASK_PENDING);\n}\n\nstatic int tg3_request_irq(struct tg3 *tp, int irq_num)\n{\n\tirq_handler_t fn;\n\tunsigned long flags;\n\tchar *name;\n\tstruct tg3_napi *tnapi = &tp->napi[irq_num];\n\n\tif (tp->irq_cnt == 1)\n\t\tname = tp->dev->name;\n\telse {\n\t\tname = &tnapi->irq_lbl[0];\n\t\tsnprintf(name, IFNAMSIZ, \"%s-%d\", tp->dev->name, irq_num);\n\t\tname[IFNAMSIZ-1] = 0;\n\t}\n\n\tif (tg3_flag(tp, USING_MSI) || tg3_flag(tp, USING_MSIX)) {\n\t\tfn = tg3_msi;\n\t\tif (tg3_flag(tp, 1SHOT_MSI))\n\t\t\tfn = tg3_msi_1shot;\n\t\tflags = 0;\n\t} else {\n\t\tfn = tg3_interrupt;\n\t\tif (tg3_flag(tp, TAGGED_STATUS))\n\t\t\tfn = tg3_interrupt_tagged;\n\t\tflags = IRQF_SHARED;\n\t}\n\n\treturn request_irq(tnapi->irq_vec, fn, flags, name, tnapi);\n}\n\nstatic int tg3_test_interrupt(struct tg3 *tp)\n{\n\tstruct tg3_napi *tnapi = &tp->napi[0];\n\tstruct net_device *dev = tp->dev;\n\tint err, i, intr_ok = 0;\n\tu32 val;\n\n\tif (!netif_running(dev))\n\t\treturn -ENODEV;\n\n\ttg3_disable_ints(tp);\n\n\tfree_irq(tnapi->irq_vec, tnapi);\n\n\t/*\n\t * Turn off MSI one shot mode.  Otherwise this test has no\n\t * observable way to know whether the interrupt was delivered.\n\t */\n\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\tval = tr32(MSGINT_MODE) | MSGINT_MODE_ONE_SHOT_DISABLE;\n\t\ttw32(MSGINT_MODE, val);\n\t}\n\n\terr = request_irq(tnapi->irq_vec, tg3_test_isr,\n\t\t\t  IRQF_SHARED, dev->name, tnapi);\n\tif (err)\n\t\treturn err;\n\n\ttnapi->hw_status->status &= ~SD_STATUS_UPDATED;\n\ttg3_enable_ints(tp);\n\n\ttw32_f(HOSTCC_MODE, tp->coalesce_mode | HOSTCC_MODE_ENABLE |\n\t       tnapi->coal_now);\n\n\tfor (i = 0; i < 5; i++) {\n\t\tu32 int_mbox, misc_host_ctrl;\n\n\t\tint_mbox = tr32_mailbox(tnapi->int_mbox);\n\t\tmisc_host_ctrl = tr32(TG3PCI_MISC_HOST_CTRL);\n\n\t\tif ((int_mbox != 0) ||\n\t\t    (misc_host_ctrl & MISC_HOST_CTRL_MASK_PCI_INT)) {\n\t\t\tintr_ok = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tg3_flag(tp, 57765_PLUS) &&\n\t\t    tnapi->hw_status->status_tag != tnapi->last_tag)\n\t\t\ttw32_mailbox_f(tnapi->int_mbox, tnapi->last_tag << 24);\n\n\t\tmsleep(10);\n\t}\n\n\ttg3_disable_ints(tp);\n\n\tfree_irq(tnapi->irq_vec, tnapi);\n\n\terr = tg3_request_irq(tp, 0);\n\n\tif (err)\n\t\treturn err;\n\n\tif (intr_ok) {\n\t\t/* Reenable MSI one shot mode. */\n\t\tif (tg3_flag(tp, 57765_PLUS) && tg3_flag(tp, 1SHOT_MSI)) {\n\t\t\tval = tr32(MSGINT_MODE) & ~MSGINT_MODE_ONE_SHOT_DISABLE;\n\t\t\ttw32(MSGINT_MODE, val);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -EIO;\n}\n\n/* Returns 0 if MSI test succeeds or MSI test fails and INTx mode is\n * successfully restored\n */\nstatic int tg3_test_msi(struct tg3 *tp)\n{\n\tint err;\n\tu16 pci_cmd;\n\n\tif (!tg3_flag(tp, USING_MSI))\n\t\treturn 0;\n\n\t/* Turn off SERR reporting in case MSI terminates with Master\n\t * Abort.\n\t */\n\tpci_read_config_word(tp->pdev, PCI_COMMAND, &pci_cmd);\n\tpci_write_config_word(tp->pdev, PCI_COMMAND,\n\t\t\t      pci_cmd & ~PCI_COMMAND_SERR);\n\n\terr = tg3_test_interrupt(tp);\n\n\tpci_write_config_word(tp->pdev, PCI_COMMAND, pci_cmd);\n\n\tif (!err)\n\t\treturn 0;\n\n\t/* other failures */\n\tif (err != -EIO)\n\t\treturn err;\n\n\t/* MSI test failed, go back to INTx mode */\n\tnetdev_warn(tp->dev, \"No interrupt was generated using MSI. Switching \"\n\t\t    \"to INTx mode. Please report this failure to the PCI \"\n\t\t    \"maintainer and include system chipset information\\n\");\n\n\tfree_irq(tp->napi[0].irq_vec, &tp->napi[0]);\n\n\tpci_disable_msi(tp->pdev);\n\n\ttg3_flag_clear(tp, USING_MSI);\n\ttp->napi[0].irq_vec = tp->pdev->irq;\n\n\terr = tg3_request_irq(tp, 0);\n\tif (err)\n\t\treturn err;\n\n\t/* Need to reset the chip because the MSI cycle may have terminated\n\t * with Master Abort.\n\t */\n\ttg3_full_lock(tp, 1);\n\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\terr = tg3_init_hw(tp, 1);\n\n\ttg3_full_unlock(tp);\n\n\tif (err)\n\t\tfree_irq(tp->napi[0].irq_vec, &tp->napi[0]);\n\n\treturn err;\n}\n\nstatic int tg3_request_firmware(struct tg3 *tp)\n{\n\tconst __be32 *fw_data;\n\n\tif (request_firmware(&tp->fw, tp->fw_needed, &tp->pdev->dev)) {\n\t\tnetdev_err(tp->dev, \"Failed to load firmware \\\"%s\\\"\\n\",\n\t\t\t   tp->fw_needed);\n\t\treturn -ENOENT;\n\t}\n\n\tfw_data = (void *)tp->fw->data;\n\n\t/* Firmware blob starts with version numbers, followed by\n\t * start address and _full_ length including BSS sections\n\t * (which must be longer than the actual data, of course\n\t */\n\n\ttp->fw_len = be32_to_cpu(fw_data[2]);\t/* includes bss */\n\tif (tp->fw_len < (tp->fw->size - 12)) {\n\t\tnetdev_err(tp->dev, \"bogus length %d in \\\"%s\\\"\\n\",\n\t\t\t   tp->fw_len, tp->fw_needed);\n\t\trelease_firmware(tp->fw);\n\t\ttp->fw = NULL;\n\t\treturn -EINVAL;\n\t}\n\n\t/* We no longer need firmware; we have it. */\n\ttp->fw_needed = NULL;\n\treturn 0;\n}\n\nstatic u32 tg3_irq_count(struct tg3 *tp)\n{\n\tu32 irq_cnt = max(tp->rxq_cnt, tp->txq_cnt);\n\n\tif (irq_cnt > 1) {\n\t\t/* We want as many rx rings enabled as there are cpus.\n\t\t * In multiqueue MSI-X mode, the first MSI-X vector\n\t\t * only deals with link interrupts, etc, so we add\n\t\t * one to the number of vectors we are requesting.\n\t\t */\n\t\tirq_cnt = min_t(unsigned, irq_cnt + 1, tp->irq_max);\n\t}\n\n\treturn irq_cnt;\n}\n\nstatic bool tg3_enable_msix(struct tg3 *tp)\n{\n\tint i, rc;\n\tstruct msix_entry msix_ent[TG3_IRQ_MAX_VECS];\n\n\ttp->txq_cnt = tp->txq_req;\n\ttp->rxq_cnt = tp->rxq_req;\n\tif (!tp->rxq_cnt)\n\t\ttp->rxq_cnt = netif_get_num_default_rss_queues();\n\tif (tp->rxq_cnt > tp->rxq_max)\n\t\ttp->rxq_cnt = tp->rxq_max;\n\n\t/* Disable multiple TX rings by default.  Simple round-robin hardware\n\t * scheduling of the TX rings can cause starvation of rings with\n\t * small packets when other rings have TSO or jumbo packets.\n\t */\n\tif (!tp->txq_req)\n\t\ttp->txq_cnt = 1;\n\n\ttp->irq_cnt = tg3_irq_count(tp);\n\n\tfor (i = 0; i < tp->irq_max; i++) {\n\t\tmsix_ent[i].entry  = i;\n\t\tmsix_ent[i].vector = 0;\n\t}\n\n\trc = pci_enable_msix(tp->pdev, msix_ent, tp->irq_cnt);\n\tif (rc < 0) {\n\t\treturn false;\n\t} else if (rc != 0) {\n\t\tif (pci_enable_msix(tp->pdev, msix_ent, rc))\n\t\t\treturn false;\n\t\tnetdev_notice(tp->dev, \"Requested %d MSI-X vectors, received %d\\n\",\n\t\t\t      tp->irq_cnt, rc);\n\t\ttp->irq_cnt = rc;\n\t\ttp->rxq_cnt = max(rc - 1, 1);\n\t\tif (tp->txq_cnt)\n\t\t\ttp->txq_cnt = min(tp->rxq_cnt, tp->txq_max);\n\t}\n\n\tfor (i = 0; i < tp->irq_max; i++)\n\t\ttp->napi[i].irq_vec = msix_ent[i].vector;\n\n\tif (netif_set_real_num_rx_queues(tp->dev, tp->rxq_cnt)) {\n\t\tpci_disable_msix(tp->pdev);\n\t\treturn false;\n\t}\n\n\tif (tp->irq_cnt == 1)\n\t\treturn true;\n\n\ttg3_flag_set(tp, ENABLE_RSS);\n\n\tif (tp->txq_cnt > 1)\n\t\ttg3_flag_set(tp, ENABLE_TSS);\n\n\tnetif_set_real_num_tx_queues(tp->dev, tp->txq_cnt);\n\n\treturn true;\n}\n\nstatic void tg3_ints_init(struct tg3 *tp)\n{\n\tif ((tg3_flag(tp, SUPPORT_MSI) || tg3_flag(tp, SUPPORT_MSIX)) &&\n\t    !tg3_flag(tp, TAGGED_STATUS)) {\n\t\t/* All MSI supporting chips should support tagged\n\t\t * status.  Assert that this is the case.\n\t\t */\n\t\tnetdev_warn(tp->dev,\n\t\t\t    \"MSI without TAGGED_STATUS? Not using MSI\\n\");\n\t\tgoto defcfg;\n\t}\n\n\tif (tg3_flag(tp, SUPPORT_MSIX) && tg3_enable_msix(tp))\n\t\ttg3_flag_set(tp, USING_MSIX);\n\telse if (tg3_flag(tp, SUPPORT_MSI) && pci_enable_msi(tp->pdev) == 0)\n\t\ttg3_flag_set(tp, USING_MSI);\n\n\tif (tg3_flag(tp, USING_MSI) || tg3_flag(tp, USING_MSIX)) {\n\t\tu32 msi_mode = tr32(MSGINT_MODE);\n\t\tif (tg3_flag(tp, USING_MSIX) && tp->irq_cnt > 1)\n\t\t\tmsi_mode |= MSGINT_MODE_MULTIVEC_EN;\n\t\tif (!tg3_flag(tp, 1SHOT_MSI))\n\t\t\tmsi_mode |= MSGINT_MODE_ONE_SHOT_DISABLE;\n\t\ttw32(MSGINT_MODE, msi_mode | MSGINT_MODE_ENABLE);\n\t}\ndefcfg:\n\tif (!tg3_flag(tp, USING_MSIX)) {\n\t\ttp->irq_cnt = 1;\n\t\ttp->napi[0].irq_vec = tp->pdev->irq;\n\t}\n\n\tif (tp->irq_cnt == 1) {\n\t\ttp->txq_cnt = 1;\n\t\ttp->rxq_cnt = 1;\n\t\tnetif_set_real_num_tx_queues(tp->dev, 1);\n\t\tnetif_set_real_num_rx_queues(tp->dev, 1);\n\t}\n}\n\nstatic void tg3_ints_fini(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, USING_MSIX))\n\t\tpci_disable_msix(tp->pdev);\n\telse if (tg3_flag(tp, USING_MSI))\n\t\tpci_disable_msi(tp->pdev);\n\ttg3_flag_clear(tp, USING_MSI);\n\ttg3_flag_clear(tp, USING_MSIX);\n\ttg3_flag_clear(tp, ENABLE_RSS);\n\ttg3_flag_clear(tp, ENABLE_TSS);\n}\n\nstatic int tg3_start(struct tg3 *tp, bool reset_phy, bool test_irq,\n\t\t     bool init)\n{\n\tstruct net_device *dev = tp->dev;\n\tint i, err;\n\n\t/*\n\t * Setup interrupts first so we know how\n\t * many NAPI resources to allocate\n\t */\n\ttg3_ints_init(tp);\n\n\ttg3_rss_check_indir_tbl(tp);\n\n\t/* The placement of this call is tied\n\t * to the setup and use of Host TX descriptors.\n\t */\n\terr = tg3_alloc_consistent(tp);\n\tif (err)\n\t\tgoto err_out1;\n\n\ttg3_napi_init(tp);\n\n\ttg3_napi_enable(tp);\n\n\tfor (i = 0; i < tp->irq_cnt; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\terr = tg3_request_irq(tp, i);\n\t\tif (err) {\n\t\t\tfor (i--; i >= 0; i--) {\n\t\t\t\ttnapi = &tp->napi[i];\n\t\t\t\tfree_irq(tnapi->irq_vec, tnapi);\n\t\t\t}\n\t\t\tgoto err_out2;\n\t\t}\n\t}\n\n\ttg3_full_lock(tp, 0);\n\n\terr = tg3_init_hw(tp, reset_phy);\n\tif (err) {\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\ttg3_free_rings(tp);\n\t}\n\n\ttg3_full_unlock(tp);\n\n\tif (err)\n\t\tgoto err_out3;\n\n\tif (test_irq && tg3_flag(tp, USING_MSI)) {\n\t\terr = tg3_test_msi(tp);\n\n\t\tif (err) {\n\t\t\ttg3_full_lock(tp, 0);\n\t\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\t\ttg3_free_rings(tp);\n\t\t\ttg3_full_unlock(tp);\n\n\t\t\tgoto err_out2;\n\t\t}\n\n\t\tif (!tg3_flag(tp, 57765_PLUS) && tg3_flag(tp, USING_MSI)) {\n\t\t\tu32 val = tr32(PCIE_TRANSACTION_CFG);\n\n\t\t\ttw32(PCIE_TRANSACTION_CFG,\n\t\t\t     val | PCIE_TRANS_CFG_1SHOT_MSI);\n\t\t}\n\t}\n\n\ttg3_phy_start(tp);\n\n\ttg3_hwmon_open(tp);\n\n\ttg3_full_lock(tp, 0);\n\n\ttg3_timer_start(tp);\n\ttg3_flag_set(tp, INIT_COMPLETE);\n\ttg3_enable_ints(tp);\n\n\tif (init)\n\t\ttg3_ptp_init(tp);\n\telse\n\t\ttg3_ptp_resume(tp);\n\n\n\ttg3_full_unlock(tp);\n\n\tnetif_tx_start_all_queues(dev);\n\n\t/*\n\t * Reset loopback feature if it was turned on while the device was down\n\t * make sure that it's installed properly now.\n\t */\n\tif (dev->features & NETIF_F_LOOPBACK)\n\t\ttg3_set_loopback(dev, dev->features);\n\n\treturn 0;\n\nerr_out3:\n\tfor (i = tp->irq_cnt - 1; i >= 0; i--) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tfree_irq(tnapi->irq_vec, tnapi);\n\t}\n\nerr_out2:\n\ttg3_napi_disable(tp);\n\ttg3_napi_fini(tp);\n\ttg3_free_consistent(tp);\n\nerr_out1:\n\ttg3_ints_fini(tp);\n\n\treturn err;\n}\n\nstatic void tg3_stop(struct tg3 *tp)\n{\n\tint i;\n\n\ttg3_reset_task_cancel(tp);\n\ttg3_netif_stop(tp);\n\n\ttg3_timer_stop(tp);\n\n\ttg3_hwmon_close(tp);\n\n\ttg3_phy_stop(tp);\n\n\ttg3_full_lock(tp, 1);\n\n\ttg3_disable_ints(tp);\n\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\ttg3_free_rings(tp);\n\ttg3_flag_clear(tp, INIT_COMPLETE);\n\n\ttg3_full_unlock(tp);\n\n\tfor (i = tp->irq_cnt - 1; i >= 0; i--) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\t\tfree_irq(tnapi->irq_vec, tnapi);\n\t}\n\n\ttg3_ints_fini(tp);\n\n\ttg3_napi_fini(tp);\n\n\ttg3_free_consistent(tp);\n}\n\nstatic int tg3_open(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err;\n\n\tif (tp->fw_needed) {\n\t\terr = tg3_request_firmware(tp);\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0) {\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else if (err) {\n\t\t\tnetdev_warn(tp->dev, \"TSO capability disabled\\n\");\n\t\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\t} else if (!tg3_flag(tp, TSO_CAPABLE)) {\n\t\t\tnetdev_notice(tp->dev, \"TSO capability restored\\n\");\n\t\t\ttg3_flag_set(tp, TSO_CAPABLE);\n\t\t}\n\t}\n\n\ttg3_carrier_off(tp);\n\n\terr = tg3_power_up(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_full_lock(tp, 0);\n\n\ttg3_disable_ints(tp);\n\ttg3_flag_clear(tp, INIT_COMPLETE);\n\n\ttg3_full_unlock(tp);\n\n\terr = tg3_start(tp, true, true, true);\n\tif (err) {\n\t\ttg3_frob_aux_power(tp, false);\n\t\tpci_set_power_state(tp->pdev, PCI_D3hot);\n\t}\n\n\tif (tg3_flag(tp, PTP_CAPABLE)) {\n\t\ttp->ptp_clock = ptp_clock_register(&tp->ptp_info,\n\t\t\t\t\t\t   &tp->pdev->dev);\n\t\tif (IS_ERR(tp->ptp_clock))\n\t\t\ttp->ptp_clock = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_close(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\ttg3_ptp_fini(tp);\n\n\ttg3_stop(tp);\n\n\t/* Clear stats across close / open calls */\n\tmemset(&tp->net_stats_prev, 0, sizeof(tp->net_stats_prev));\n\tmemset(&tp->estats_prev, 0, sizeof(tp->estats_prev));\n\n\ttg3_power_down(tp);\n\n\ttg3_carrier_off(tp);\n\n\treturn 0;\n}\n\nstatic inline u64 get_stat64(tg3_stat64_t *val)\n{\n       return ((u64)val->high << 32) | ((u64)val->low);\n}\n\nstatic u64 tg3_calc_crc_errors(struct tg3 *tp)\n{\n\tstruct tg3_hw_stats *hw_stats = tp->hw_stats;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_PHY_SERDES) &&\n\t    (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5701)) {\n\t\tu32 val;\n\n\t\tif (!tg3_readphy(tp, MII_TG3_TEST1, &val)) {\n\t\t\ttg3_writephy(tp, MII_TG3_TEST1,\n\t\t\t\t     val | MII_TG3_TEST1_CRC_EN);\n\t\t\ttg3_readphy(tp, MII_TG3_RXR_COUNTERS, &val);\n\t\t} else\n\t\t\tval = 0;\n\n\t\ttp->phy_crc_errors += val;\n\n\t\treturn tp->phy_crc_errors;\n\t}\n\n\treturn get_stat64(&hw_stats->rx_fcs_errors);\n}\n\n#define ESTAT_ADD(member) \\\n\testats->member =\told_estats->member + \\\n\t\t\t\tget_stat64(&hw_stats->member)\n\nstatic void tg3_get_estats(struct tg3 *tp, struct tg3_ethtool_stats *estats)\n{\n\tstruct tg3_ethtool_stats *old_estats = &tp->estats_prev;\n\tstruct tg3_hw_stats *hw_stats = tp->hw_stats;\n\n\tESTAT_ADD(rx_octets);\n\tESTAT_ADD(rx_fragments);\n\tESTAT_ADD(rx_ucast_packets);\n\tESTAT_ADD(rx_mcast_packets);\n\tESTAT_ADD(rx_bcast_packets);\n\tESTAT_ADD(rx_fcs_errors);\n\tESTAT_ADD(rx_align_errors);\n\tESTAT_ADD(rx_xon_pause_rcvd);\n\tESTAT_ADD(rx_xoff_pause_rcvd);\n\tESTAT_ADD(rx_mac_ctrl_rcvd);\n\tESTAT_ADD(rx_xoff_entered);\n\tESTAT_ADD(rx_frame_too_long_errors);\n\tESTAT_ADD(rx_jabbers);\n\tESTAT_ADD(rx_undersize_packets);\n\tESTAT_ADD(rx_in_length_errors);\n\tESTAT_ADD(rx_out_length_errors);\n\tESTAT_ADD(rx_64_or_less_octet_packets);\n\tESTAT_ADD(rx_65_to_127_octet_packets);\n\tESTAT_ADD(rx_128_to_255_octet_packets);\n\tESTAT_ADD(rx_256_to_511_octet_packets);\n\tESTAT_ADD(rx_512_to_1023_octet_packets);\n\tESTAT_ADD(rx_1024_to_1522_octet_packets);\n\tESTAT_ADD(rx_1523_to_2047_octet_packets);\n\tESTAT_ADD(rx_2048_to_4095_octet_packets);\n\tESTAT_ADD(rx_4096_to_8191_octet_packets);\n\tESTAT_ADD(rx_8192_to_9022_octet_packets);\n\n\tESTAT_ADD(tx_octets);\n\tESTAT_ADD(tx_collisions);\n\tESTAT_ADD(tx_xon_sent);\n\tESTAT_ADD(tx_xoff_sent);\n\tESTAT_ADD(tx_flow_control);\n\tESTAT_ADD(tx_mac_errors);\n\tESTAT_ADD(tx_single_collisions);\n\tESTAT_ADD(tx_mult_collisions);\n\tESTAT_ADD(tx_deferred);\n\tESTAT_ADD(tx_excessive_collisions);\n\tESTAT_ADD(tx_late_collisions);\n\tESTAT_ADD(tx_collide_2times);\n\tESTAT_ADD(tx_collide_3times);\n\tESTAT_ADD(tx_collide_4times);\n\tESTAT_ADD(tx_collide_5times);\n\tESTAT_ADD(tx_collide_6times);\n\tESTAT_ADD(tx_collide_7times);\n\tESTAT_ADD(tx_collide_8times);\n\tESTAT_ADD(tx_collide_9times);\n\tESTAT_ADD(tx_collide_10times);\n\tESTAT_ADD(tx_collide_11times);\n\tESTAT_ADD(tx_collide_12times);\n\tESTAT_ADD(tx_collide_13times);\n\tESTAT_ADD(tx_collide_14times);\n\tESTAT_ADD(tx_collide_15times);\n\tESTAT_ADD(tx_ucast_packets);\n\tESTAT_ADD(tx_mcast_packets);\n\tESTAT_ADD(tx_bcast_packets);\n\tESTAT_ADD(tx_carrier_sense_errors);\n\tESTAT_ADD(tx_discards);\n\tESTAT_ADD(tx_errors);\n\n\tESTAT_ADD(dma_writeq_full);\n\tESTAT_ADD(dma_write_prioq_full);\n\tESTAT_ADD(rxbds_empty);\n\tESTAT_ADD(rx_discards);\n\tESTAT_ADD(rx_errors);\n\tESTAT_ADD(rx_threshold_hit);\n\n\tESTAT_ADD(dma_readq_full);\n\tESTAT_ADD(dma_read_prioq_full);\n\tESTAT_ADD(tx_comp_queue_full);\n\n\tESTAT_ADD(ring_set_send_prod_index);\n\tESTAT_ADD(ring_status_update);\n\tESTAT_ADD(nic_irqs);\n\tESTAT_ADD(nic_avoided_irqs);\n\tESTAT_ADD(nic_tx_threshold_hit);\n\n\tESTAT_ADD(mbuf_lwm_thresh_hit);\n}\n\nstatic void tg3_get_nstats(struct tg3 *tp, struct rtnl_link_stats64 *stats)\n{\n\tstruct rtnl_link_stats64 *old_stats = &tp->net_stats_prev;\n\tstruct tg3_hw_stats *hw_stats = tp->hw_stats;\n\n\tstats->rx_packets = old_stats->rx_packets +\n\t\tget_stat64(&hw_stats->rx_ucast_packets) +\n\t\tget_stat64(&hw_stats->rx_mcast_packets) +\n\t\tget_stat64(&hw_stats->rx_bcast_packets);\n\n\tstats->tx_packets = old_stats->tx_packets +\n\t\tget_stat64(&hw_stats->tx_ucast_packets) +\n\t\tget_stat64(&hw_stats->tx_mcast_packets) +\n\t\tget_stat64(&hw_stats->tx_bcast_packets);\n\n\tstats->rx_bytes = old_stats->rx_bytes +\n\t\tget_stat64(&hw_stats->rx_octets);\n\tstats->tx_bytes = old_stats->tx_bytes +\n\t\tget_stat64(&hw_stats->tx_octets);\n\n\tstats->rx_errors = old_stats->rx_errors +\n\t\tget_stat64(&hw_stats->rx_errors);\n\tstats->tx_errors = old_stats->tx_errors +\n\t\tget_stat64(&hw_stats->tx_errors) +\n\t\tget_stat64(&hw_stats->tx_mac_errors) +\n\t\tget_stat64(&hw_stats->tx_carrier_sense_errors) +\n\t\tget_stat64(&hw_stats->tx_discards);\n\n\tstats->multicast = old_stats->multicast +\n\t\tget_stat64(&hw_stats->rx_mcast_packets);\n\tstats->collisions = old_stats->collisions +\n\t\tget_stat64(&hw_stats->tx_collisions);\n\n\tstats->rx_length_errors = old_stats->rx_length_errors +\n\t\tget_stat64(&hw_stats->rx_frame_too_long_errors) +\n\t\tget_stat64(&hw_stats->rx_undersize_packets);\n\n\tstats->rx_over_errors = old_stats->rx_over_errors +\n\t\tget_stat64(&hw_stats->rxbds_empty);\n\tstats->rx_frame_errors = old_stats->rx_frame_errors +\n\t\tget_stat64(&hw_stats->rx_align_errors);\n\tstats->tx_aborted_errors = old_stats->tx_aborted_errors +\n\t\tget_stat64(&hw_stats->tx_discards);\n\tstats->tx_carrier_errors = old_stats->tx_carrier_errors +\n\t\tget_stat64(&hw_stats->tx_carrier_sense_errors);\n\n\tstats->rx_crc_errors = old_stats->rx_crc_errors +\n\t\ttg3_calc_crc_errors(tp);\n\n\tstats->rx_missed_errors = old_stats->rx_missed_errors +\n\t\tget_stat64(&hw_stats->rx_discards);\n\n\tstats->rx_dropped = tp->rx_dropped;\n\tstats->tx_dropped = tp->tx_dropped;\n}\n\nstatic int tg3_get_regs_len(struct net_device *dev)\n{\n\treturn TG3_REG_BLK_SIZE;\n}\n\nstatic void tg3_get_regs(struct net_device *dev,\n\t\tstruct ethtool_regs *regs, void *_p)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tregs->version = 0;\n\n\tmemset(_p, 0, TG3_REG_BLK_SIZE);\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\treturn;\n\n\ttg3_full_lock(tp, 0);\n\n\ttg3_dump_legacy_regs(tp, (u32 *)_p);\n\n\ttg3_full_unlock(tp);\n}\n\nstatic int tg3_get_eeprom_len(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\treturn tp->nvram_size;\n}\n\nstatic int tg3_get_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom, u8 *data)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint ret;\n\tu8  *pd;\n\tu32 i, offset, len, b_offset, b_count;\n\t__be32 val;\n\n\tif (tg3_flag(tp, NO_NVRAM))\n\t\treturn -EINVAL;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\treturn -EAGAIN;\n\n\toffset = eeprom->offset;\n\tlen = eeprom->len;\n\teeprom->len = 0;\n\n\teeprom->magic = TG3_EEPROM_MAGIC;\n\n\tif (offset & 3) {\n\t\t/* adjustments to start on required 4 byte boundary */\n\t\tb_offset = offset & 3;\n\t\tb_count = 4 - b_offset;\n\t\tif (b_count > len) {\n\t\t\t/* i.e. offset=1 len=2 */\n\t\t\tb_count = len;\n\t\t}\n\t\tret = tg3_nvram_read_be32(tp, offset-b_offset, &val);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmemcpy(data, ((char *)&val) + b_offset, b_count);\n\t\tlen -= b_count;\n\t\toffset += b_count;\n\t\teeprom->len += b_count;\n\t}\n\n\t/* read bytes up to the last 4 byte boundary */\n\tpd = &data[eeprom->len];\n\tfor (i = 0; i < (len - (len & 3)); i += 4) {\n\t\tret = tg3_nvram_read_be32(tp, offset + i, &val);\n\t\tif (ret) {\n\t\t\teeprom->len += i;\n\t\t\treturn ret;\n\t\t}\n\t\tmemcpy(pd + i, &val, 4);\n\t}\n\teeprom->len += i;\n\n\tif (len & 3) {\n\t\t/* read last bytes not ending on 4 byte boundary */\n\t\tpd = &data[eeprom->len];\n\t\tb_count = len & 3;\n\t\tb_offset = offset + len - b_count;\n\t\tret = tg3_nvram_read_be32(tp, b_offset, &val);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmemcpy(pd, &val, b_count);\n\t\teeprom->len += b_count;\n\t}\n\treturn 0;\n}\n\nstatic int tg3_set_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom, u8 *data)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint ret;\n\tu32 offset, len, b_offset, odd_len;\n\tu8 *buf;\n\t__be32 start, end;\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\treturn -EAGAIN;\n\n\tif (tg3_flag(tp, NO_NVRAM) ||\n\t    eeprom->magic != TG3_EEPROM_MAGIC)\n\t\treturn -EINVAL;\n\n\toffset = eeprom->offset;\n\tlen = eeprom->len;\n\n\tif ((b_offset = (offset & 3))) {\n\t\t/* adjustments to start on required 4 byte boundary */\n\t\tret = tg3_nvram_read_be32(tp, offset-b_offset, &start);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tlen += b_offset;\n\t\toffset &= ~3;\n\t\tif (len < 4)\n\t\t\tlen = 4;\n\t}\n\n\todd_len = 0;\n\tif (len & 3) {\n\t\t/* adjustments to end on required 4 byte boundary */\n\t\todd_len = 1;\n\t\tlen = (len + 3) & ~3;\n\t\tret = tg3_nvram_read_be32(tp, offset+len-4, &end);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbuf = data;\n\tif (b_offset || odd_len) {\n\t\tbuf = kmalloc(len, GFP_KERNEL);\n\t\tif (!buf)\n\t\t\treturn -ENOMEM;\n\t\tif (b_offset)\n\t\t\tmemcpy(buf, &start, 4);\n\t\tif (odd_len)\n\t\t\tmemcpy(buf+len-4, &end, 4);\n\t\tmemcpy(buf + b_offset, data, eeprom->len);\n\t}\n\n\tret = tg3_nvram_write_block(tp, offset, len, buf);\n\n\tif (buf != data)\n\t\tkfree(buf);\n\n\treturn ret;\n}\n\nstatic int tg3_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tstruct phy_device *phydev;\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\t\treturn -EAGAIN;\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\t\treturn phy_ethtool_gset(phydev, cmd);\n\t}\n\n\tcmd->supported = (SUPPORTED_Autoneg);\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY))\n\t\tcmd->supported |= (SUPPORTED_1000baseT_Half |\n\t\t\t\t   SUPPORTED_1000baseT_Full);\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES)) {\n\t\tcmd->supported |= (SUPPORTED_100baseT_Half |\n\t\t\t\t  SUPPORTED_100baseT_Full |\n\t\t\t\t  SUPPORTED_10baseT_Half |\n\t\t\t\t  SUPPORTED_10baseT_Full |\n\t\t\t\t  SUPPORTED_TP);\n\t\tcmd->port = PORT_TP;\n\t} else {\n\t\tcmd->supported |= SUPPORTED_FIBRE;\n\t\tcmd->port = PORT_FIBRE;\n\t}\n\n\tcmd->advertising = tp->link_config.advertising;\n\tif (tg3_flag(tp, PAUSE_AUTONEG)) {\n\t\tif (tp->link_config.flowctrl & FLOW_CTRL_RX) {\n\t\t\tif (tp->link_config.flowctrl & FLOW_CTRL_TX) {\n\t\t\t\tcmd->advertising |= ADVERTISED_Pause;\n\t\t\t} else {\n\t\t\t\tcmd->advertising |= ADVERTISED_Pause |\n\t\t\t\t\t\t    ADVERTISED_Asym_Pause;\n\t\t\t}\n\t\t} else if (tp->link_config.flowctrl & FLOW_CTRL_TX) {\n\t\t\tcmd->advertising |= ADVERTISED_Asym_Pause;\n\t\t}\n\t}\n\tif (netif_running(dev) && tp->link_up) {\n\t\tethtool_cmd_speed_set(cmd, tp->link_config.active_speed);\n\t\tcmd->duplex = tp->link_config.active_duplex;\n\t\tcmd->lp_advertising = tp->link_config.rmt_adv;\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES)) {\n\t\t\tif (tp->phy_flags & TG3_PHYFLG_MDIX_STATE)\n\t\t\t\tcmd->eth_tp_mdix = ETH_TP_MDI_X;\n\t\t\telse\n\t\t\t\tcmd->eth_tp_mdix = ETH_TP_MDI;\n\t\t}\n\t} else {\n\t\tethtool_cmd_speed_set(cmd, SPEED_UNKNOWN);\n\t\tcmd->duplex = DUPLEX_UNKNOWN;\n\t\tcmd->eth_tp_mdix = ETH_TP_MDI_INVALID;\n\t}\n\tcmd->phy_address = tp->phy_addr;\n\tcmd->transceiver = XCVR_INTERNAL;\n\tcmd->autoneg = tp->link_config.autoneg;\n\tcmd->maxtxpkt = 0;\n\tcmd->maxrxpkt = 0;\n\treturn 0;\n}\n\nstatic int tg3_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 speed = ethtool_cmd_speed(cmd);\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tstruct phy_device *phydev;\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\t\treturn -EAGAIN;\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\t\treturn phy_ethtool_sset(phydev, cmd);\n\t}\n\n\tif (cmd->autoneg != AUTONEG_ENABLE &&\n\t    cmd->autoneg != AUTONEG_DISABLE)\n\t\treturn -EINVAL;\n\n\tif (cmd->autoneg == AUTONEG_DISABLE &&\n\t    cmd->duplex != DUPLEX_FULL &&\n\t    cmd->duplex != DUPLEX_HALF)\n\t\treturn -EINVAL;\n\n\tif (cmd->autoneg == AUTONEG_ENABLE) {\n\t\tu32 mask = ADVERTISED_Autoneg |\n\t\t\t   ADVERTISED_Pause |\n\t\t\t   ADVERTISED_Asym_Pause;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY))\n\t\t\tmask |= ADVERTISED_1000baseT_Half |\n\t\t\t\tADVERTISED_1000baseT_Full;\n\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\t\tmask |= ADVERTISED_100baseT_Half |\n\t\t\t\tADVERTISED_100baseT_Full |\n\t\t\t\tADVERTISED_10baseT_Half |\n\t\t\t\tADVERTISED_10baseT_Full |\n\t\t\t\tADVERTISED_TP;\n\t\telse\n\t\t\tmask |= ADVERTISED_FIBRE;\n\n\t\tif (cmd->advertising & ~mask)\n\t\t\treturn -EINVAL;\n\n\t\tmask &= (ADVERTISED_1000baseT_Half |\n\t\t\t ADVERTISED_1000baseT_Full |\n\t\t\t ADVERTISED_100baseT_Half |\n\t\t\t ADVERTISED_100baseT_Full |\n\t\t\t ADVERTISED_10baseT_Half |\n\t\t\t ADVERTISED_10baseT_Full);\n\n\t\tcmd->advertising &= mask;\n\t} else {\n\t\tif (tp->phy_flags & TG3_PHYFLG_ANY_SERDES) {\n\t\t\tif (speed != SPEED_1000)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (cmd->duplex != DUPLEX_FULL)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\tif (speed != SPEED_100 &&\n\t\t\t    speed != SPEED_10)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\ttg3_full_lock(tp, 0);\n\n\ttp->link_config.autoneg = cmd->autoneg;\n\tif (cmd->autoneg == AUTONEG_ENABLE) {\n\t\ttp->link_config.advertising = (cmd->advertising |\n\t\t\t\t\t      ADVERTISED_Autoneg);\n\t\ttp->link_config.speed = SPEED_UNKNOWN;\n\t\ttp->link_config.duplex = DUPLEX_UNKNOWN;\n\t} else {\n\t\ttp->link_config.advertising = 0;\n\t\ttp->link_config.speed = speed;\n\t\ttp->link_config.duplex = cmd->duplex;\n\t}\n\n\tif (netif_running(dev))\n\t\ttg3_setup_phy(tp, 1);\n\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic void tg3_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tstrlcpy(info->driver, DRV_MODULE_NAME, sizeof(info->driver));\n\tstrlcpy(info->version, DRV_MODULE_VERSION, sizeof(info->version));\n\tstrlcpy(info->fw_version, tp->fw_ver, sizeof(info->fw_version));\n\tstrlcpy(info->bus_info, pci_name(tp->pdev), sizeof(info->bus_info));\n}\n\nstatic void tg3_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tg3_flag(tp, WOL_CAP) && device_can_wakeup(&tp->pdev->dev))\n\t\twol->supported = WAKE_MAGIC;\n\telse\n\t\twol->supported = 0;\n\twol->wolopts = 0;\n\tif (tg3_flag(tp, WOL_ENABLE) && device_can_wakeup(&tp->pdev->dev))\n\t\twol->wolopts = WAKE_MAGIC;\n\tmemset(&wol->sopass, 0, sizeof(wol->sopass));\n}\n\nstatic int tg3_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tstruct device *dp = &tp->pdev->dev;\n\n\tif (wol->wolopts & ~WAKE_MAGIC)\n\t\treturn -EINVAL;\n\tif ((wol->wolopts & WAKE_MAGIC) &&\n\t    !(tg3_flag(tp, WOL_CAP) && device_can_wakeup(dp)))\n\t\treturn -EINVAL;\n\n\tdevice_set_wakeup_enable(dp, wol->wolopts & WAKE_MAGIC);\n\n\tspin_lock_bh(&tp->lock);\n\tif (device_may_wakeup(dp))\n\t\ttg3_flag_set(tp, WOL_ENABLE);\n\telse\n\t\ttg3_flag_clear(tp, WOL_ENABLE);\n\tspin_unlock_bh(&tp->lock);\n\n\treturn 0;\n}\n\nstatic u32 tg3_get_msglevel(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\treturn tp->msg_enable;\n}\n\nstatic void tg3_set_msglevel(struct net_device *dev, u32 value)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\ttp->msg_enable = value;\n}\n\nstatic int tg3_nway_reset(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint r;\n\n\tif (!netif_running(dev))\n\t\treturn -EAGAIN;\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\treturn -EINVAL;\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\t\treturn -EAGAIN;\n\t\tr = phy_start_aneg(tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR]);\n\t} else {\n\t\tu32 bmcr;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\tr = -EINVAL;\n\t\ttg3_readphy(tp, MII_BMCR, &bmcr);\n\t\tif (!tg3_readphy(tp, MII_BMCR, &bmcr) &&\n\t\t    ((bmcr & BMCR_ANENABLE) ||\n\t\t     (tp->phy_flags & TG3_PHYFLG_PARALLEL_DETECT))) {\n\t\t\ttg3_writephy(tp, MII_BMCR, bmcr | BMCR_ANRESTART |\n\t\t\t\t\t\t   BMCR_ANENABLE);\n\t\t\tr = 0;\n\t\t}\n\t\tspin_unlock_bh(&tp->lock);\n\t}\n\n\treturn r;\n}\n\nstatic void tg3_get_ringparam(struct net_device *dev, struct ethtool_ringparam *ering)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tering->rx_max_pending = tp->rx_std_ring_mask;\n\tif (tg3_flag(tp, JUMBO_RING_ENABLE))\n\t\tering->rx_jumbo_max_pending = tp->rx_jmb_ring_mask;\n\telse\n\t\tering->rx_jumbo_max_pending = 0;\n\n\tering->tx_max_pending = TG3_TX_RING_SIZE - 1;\n\n\tering->rx_pending = tp->rx_pending;\n\tif (tg3_flag(tp, JUMBO_RING_ENABLE))\n\t\tering->rx_jumbo_pending = tp->rx_jumbo_pending;\n\telse\n\t\tering->rx_jumbo_pending = 0;\n\n\tering->tx_pending = tp->napi[0].tx_pending;\n}\n\nstatic int tg3_set_ringparam(struct net_device *dev, struct ethtool_ringparam *ering)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint i, irq_sync = 0, err = 0;\n\n\tif ((ering->rx_pending > tp->rx_std_ring_mask) ||\n\t    (ering->rx_jumbo_pending > tp->rx_jmb_ring_mask) ||\n\t    (ering->tx_pending > TG3_TX_RING_SIZE - 1) ||\n\t    (ering->tx_pending <= MAX_SKB_FRAGS) ||\n\t    (tg3_flag(tp, TSO_BUG) &&\n\t     (ering->tx_pending <= (MAX_SKB_FRAGS * 3))))\n\t\treturn -EINVAL;\n\n\tif (netif_running(dev)) {\n\t\ttg3_phy_stop(tp);\n\t\ttg3_netif_stop(tp);\n\t\tirq_sync = 1;\n\t}\n\n\ttg3_full_lock(tp, irq_sync);\n\n\ttp->rx_pending = ering->rx_pending;\n\n\tif (tg3_flag(tp, MAX_RXPEND_64) &&\n\t    tp->rx_pending > 63)\n\t\ttp->rx_pending = 63;\n\ttp->rx_jumbo_pending = ering->rx_jumbo_pending;\n\n\tfor (i = 0; i < tp->irq_max; i++)\n\t\ttp->napi[i].tx_pending = ering->tx_pending;\n\n\tif (netif_running(dev)) {\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\terr = tg3_restart_hw(tp, 1);\n\t\tif (!err)\n\t\t\ttg3_netif_start(tp);\n\t}\n\n\ttg3_full_unlock(tp);\n\n\tif (irq_sync && !err)\n\t\ttg3_phy_start(tp);\n\n\treturn err;\n}\n\nstatic void tg3_get_pauseparam(struct net_device *dev, struct ethtool_pauseparam *epause)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tepause->autoneg = !!tg3_flag(tp, PAUSE_AUTONEG);\n\n\tif (tp->link_config.flowctrl & FLOW_CTRL_RX)\n\t\tepause->rx_pause = 1;\n\telse\n\t\tepause->rx_pause = 0;\n\n\tif (tp->link_config.flowctrl & FLOW_CTRL_TX)\n\t\tepause->tx_pause = 1;\n\telse\n\t\tepause->tx_pause = 0;\n}\n\nstatic int tg3_set_pauseparam(struct net_device *dev, struct ethtool_pauseparam *epause)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err = 0;\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tu32 newadv;\n\t\tstruct phy_device *phydev;\n\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\n\t\tif (!(phydev->supported & SUPPORTED_Pause) ||\n\t\t    (!(phydev->supported & SUPPORTED_Asym_Pause) &&\n\t\t     (epause->rx_pause != epause->tx_pause)))\n\t\t\treturn -EINVAL;\n\n\t\ttp->link_config.flowctrl = 0;\n\t\tif (epause->rx_pause) {\n\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_RX;\n\n\t\t\tif (epause->tx_pause) {\n\t\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_TX;\n\t\t\t\tnewadv = ADVERTISED_Pause;\n\t\t\t} else\n\t\t\t\tnewadv = ADVERTISED_Pause |\n\t\t\t\t\t ADVERTISED_Asym_Pause;\n\t\t} else if (epause->tx_pause) {\n\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_TX;\n\t\t\tnewadv = ADVERTISED_Asym_Pause;\n\t\t} else\n\t\t\tnewadv = 0;\n\n\t\tif (epause->autoneg)\n\t\t\ttg3_flag_set(tp, PAUSE_AUTONEG);\n\t\telse\n\t\t\ttg3_flag_clear(tp, PAUSE_AUTONEG);\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_IS_CONNECTED) {\n\t\t\tu32 oldadv = phydev->advertising &\n\t\t\t\t     (ADVERTISED_Pause | ADVERTISED_Asym_Pause);\n\t\t\tif (oldadv != newadv) {\n\t\t\t\tphydev->advertising &=\n\t\t\t\t\t~(ADVERTISED_Pause |\n\t\t\t\t\t  ADVERTISED_Asym_Pause);\n\t\t\t\tphydev->advertising |= newadv;\n\t\t\t\tif (phydev->autoneg) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Always renegotiate the link to\n\t\t\t\t\t * inform our link partner of our\n\t\t\t\t\t * flow control settings, even if the\n\t\t\t\t\t * flow control is forced.  Let\n\t\t\t\t\t * tg3_adjust_link() do the final\n\t\t\t\t\t * flow control setup.\n\t\t\t\t\t */\n\t\t\t\t\treturn phy_start_aneg(phydev);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!epause->autoneg)\n\t\t\t\ttg3_setup_flow_control(tp, 0, 0);\n\t\t} else {\n\t\t\ttp->link_config.advertising &=\n\t\t\t\t\t~(ADVERTISED_Pause |\n\t\t\t\t\t  ADVERTISED_Asym_Pause);\n\t\t\ttp->link_config.advertising |= newadv;\n\t\t}\n\t} else {\n\t\tint irq_sync = 0;\n\n\t\tif (netif_running(dev)) {\n\t\t\ttg3_netif_stop(tp);\n\t\t\tirq_sync = 1;\n\t\t}\n\n\t\ttg3_full_lock(tp, irq_sync);\n\n\t\tif (epause->autoneg)\n\t\t\ttg3_flag_set(tp, PAUSE_AUTONEG);\n\t\telse\n\t\t\ttg3_flag_clear(tp, PAUSE_AUTONEG);\n\t\tif (epause->rx_pause)\n\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_RX;\n\t\telse\n\t\t\ttp->link_config.flowctrl &= ~FLOW_CTRL_RX;\n\t\tif (epause->tx_pause)\n\t\t\ttp->link_config.flowctrl |= FLOW_CTRL_TX;\n\t\telse\n\t\t\ttp->link_config.flowctrl &= ~FLOW_CTRL_TX;\n\n\t\tif (netif_running(dev)) {\n\t\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\t\terr = tg3_restart_hw(tp, 1);\n\t\t\tif (!err)\n\t\t\t\ttg3_netif_start(tp);\n\t\t}\n\n\t\ttg3_full_unlock(tp);\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_get_sset_count(struct net_device *dev, int sset)\n{\n\tswitch (sset) {\n\tcase ETH_SS_TEST:\n\t\treturn TG3_NUM_TEST;\n\tcase ETH_SS_STATS:\n\t\treturn TG3_NUM_STATS;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int tg3_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info,\n\t\t\t u32 *rules __always_unused)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (!tg3_flag(tp, SUPPORT_MSIX))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (info->cmd) {\n\tcase ETHTOOL_GRXRINGS:\n\t\tif (netif_running(tp->dev))\n\t\t\tinfo->data = tp->rxq_cnt;\n\t\telse {\n\t\t\tinfo->data = num_online_cpus();\n\t\t\tif (info->data > TG3_RSS_MAX_NUM_QS)\n\t\t\t\tinfo->data = TG3_RSS_MAX_NUM_QS;\n\t\t}\n\n\t\t/* The first interrupt vector only\n\t\t * handles link interrupts.\n\t\t */\n\t\tinfo->data -= 1;\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic u32 tg3_get_rxfh_indir_size(struct net_device *dev)\n{\n\tu32 size = 0;\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tg3_flag(tp, SUPPORT_MSIX))\n\t\tsize = TG3_RSS_INDIR_TBL_SIZE;\n\n\treturn size;\n}\n\nstatic int tg3_get_rxfh_indir(struct net_device *dev, u32 *indir)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < TG3_RSS_INDIR_TBL_SIZE; i++)\n\t\tindir[i] = tp->rss_ind_tbl[i];\n\n\treturn 0;\n}\n\nstatic int tg3_set_rxfh_indir(struct net_device *dev, const u32 *indir)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tsize_t i;\n\n\tfor (i = 0; i < TG3_RSS_INDIR_TBL_SIZE; i++)\n\t\ttp->rss_ind_tbl[i] = indir[i];\n\n\tif (!netif_running(dev) || !tg3_flag(tp, ENABLE_RSS))\n\t\treturn 0;\n\n\t/* It is legal to write the indirection\n\t * table while the device is running.\n\t */\n\ttg3_full_lock(tp, 0);\n\ttg3_rss_write_indir_tbl(tp);\n\ttg3_full_unlock(tp);\n\n\treturn 0;\n}\n\nstatic void tg3_get_channels(struct net_device *dev,\n\t\t\t     struct ethtool_channels *channel)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 deflt_qs = netif_get_num_default_rss_queues();\n\n\tchannel->max_rx = tp->rxq_max;\n\tchannel->max_tx = tp->txq_max;\n\n\tif (netif_running(dev)) {\n\t\tchannel->rx_count = tp->rxq_cnt;\n\t\tchannel->tx_count = tp->txq_cnt;\n\t} else {\n\t\tif (tp->rxq_req)\n\t\t\tchannel->rx_count = tp->rxq_req;\n\t\telse\n\t\t\tchannel->rx_count = min(deflt_qs, tp->rxq_max);\n\n\t\tif (tp->txq_req)\n\t\t\tchannel->tx_count = tp->txq_req;\n\t\telse\n\t\t\tchannel->tx_count = min(deflt_qs, tp->txq_max);\n\t}\n}\n\nstatic int tg3_set_channels(struct net_device *dev,\n\t\t\t    struct ethtool_channels *channel)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (!tg3_flag(tp, SUPPORT_MSIX))\n\t\treturn -EOPNOTSUPP;\n\n\tif (channel->rx_count > tp->rxq_max ||\n\t    channel->tx_count > tp->txq_max)\n\t\treturn -EINVAL;\n\n\ttp->rxq_req = channel->rx_count;\n\ttp->txq_req = channel->tx_count;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\ttg3_stop(tp);\n\n\ttg3_carrier_off(tp);\n\n\ttg3_start(tp, true, false, false);\n\n\treturn 0;\n}\n\nstatic void tg3_get_strings(struct net_device *dev, u32 stringset, u8 *buf)\n{\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tmemcpy(buf, &ethtool_stats_keys, sizeof(ethtool_stats_keys));\n\t\tbreak;\n\tcase ETH_SS_TEST:\n\t\tmemcpy(buf, &ethtool_test_keys, sizeof(ethtool_test_keys));\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\t/* we need a WARN() */\n\t\tbreak;\n\t}\n}\n\nstatic int tg3_set_phys_id(struct net_device *dev,\n\t\t\t    enum ethtool_phys_id_state state)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (!netif_running(tp->dev))\n\t\treturn -EAGAIN;\n\n\tswitch (state) {\n\tcase ETHTOOL_ID_ACTIVE:\n\t\treturn 1;\t/* cycle on/off once per second */\n\n\tcase ETHTOOL_ID_ON:\n\t\ttw32(MAC_LED_CTRL, LED_CTRL_LNKLED_OVERRIDE |\n\t\t     LED_CTRL_1000MBPS_ON |\n\t\t     LED_CTRL_100MBPS_ON |\n\t\t     LED_CTRL_10MBPS_ON |\n\t\t     LED_CTRL_TRAFFIC_OVERRIDE |\n\t\t     LED_CTRL_TRAFFIC_BLINK |\n\t\t     LED_CTRL_TRAFFIC_LED);\n\t\tbreak;\n\n\tcase ETHTOOL_ID_OFF:\n\t\ttw32(MAC_LED_CTRL, LED_CTRL_LNKLED_OVERRIDE |\n\t\t     LED_CTRL_TRAFFIC_OVERRIDE);\n\t\tbreak;\n\n\tcase ETHTOOL_ID_INACTIVE:\n\t\ttw32(MAC_LED_CTRL, tp->led_ctrl);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic void tg3_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t   struct ethtool_stats *estats, u64 *tmp_stats)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (tp->hw_stats)\n\t\ttg3_get_estats(tp, (struct tg3_ethtool_stats *)tmp_stats);\n\telse\n\t\tmemset(tmp_stats, 0, sizeof(struct tg3_ethtool_stats));\n}\n\nstatic __be32 *tg3_vpd_readblock(struct tg3 *tp, u32 *vpdlen)\n{\n\tint i;\n\t__be32 *buf;\n\tu32 offset = 0, len = 0;\n\tu32 magic, val;\n\n\tif (tg3_flag(tp, NO_NVRAM) || tg3_nvram_read(tp, 0, &magic))\n\t\treturn NULL;\n\n\tif (magic == TG3_EEPROM_MAGIC) {\n\t\tfor (offset = TG3_NVM_DIR_START;\n\t\t     offset < TG3_NVM_DIR_END;\n\t\t     offset += TG3_NVM_DIRENT_SIZE) {\n\t\t\tif (tg3_nvram_read(tp, offset, &val))\n\t\t\t\treturn NULL;\n\n\t\t\tif ((val >> TG3_NVM_DIRTYPE_SHIFT) ==\n\t\t\t    TG3_NVM_DIRTYPE_EXTVPD)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (offset != TG3_NVM_DIR_END) {\n\t\t\tlen = (val & TG3_NVM_DIRTYPE_LENMSK) * 4;\n\t\t\tif (tg3_nvram_read(tp, offset + 4, &offset))\n\t\t\t\treturn NULL;\n\n\t\t\toffset = tg3_nvram_logical_addr(tp, offset);\n\t\t}\n\t}\n\n\tif (!offset || !len) {\n\t\toffset = TG3_NVM_VPD_OFF;\n\t\tlen = TG3_NVM_VPD_LEN;\n\t}\n\n\tbuf = kmalloc(len, GFP_KERNEL);\n\tif (buf == NULL)\n\t\treturn NULL;\n\n\tif (magic == TG3_EEPROM_MAGIC) {\n\t\tfor (i = 0; i < len; i += 4) {\n\t\t\t/* The data is in little-endian format in NVRAM.\n\t\t\t * Use the big-endian read routines to preserve\n\t\t\t * the byte order as it exists in NVRAM.\n\t\t\t */\n\t\t\tif (tg3_nvram_read_be32(tp, offset + i, &buf[i/4]))\n\t\t\t\tgoto error;\n\t\t}\n\t} else {\n\t\tu8 *ptr;\n\t\tssize_t cnt;\n\t\tunsigned int pos = 0;\n\n\t\tptr = (u8 *)&buf[0];\n\t\tfor (i = 0; pos < len && i < 3; i++, pos += cnt, ptr += cnt) {\n\t\t\tcnt = pci_read_vpd(tp->pdev, pos,\n\t\t\t\t\t   len - pos, ptr);\n\t\t\tif (cnt == -ETIMEDOUT || cnt == -EINTR)\n\t\t\t\tcnt = 0;\n\t\t\telse if (cnt < 0)\n\t\t\t\tgoto error;\n\t\t}\n\t\tif (pos != len)\n\t\t\tgoto error;\n\t}\n\n\t*vpdlen = len;\n\n\treturn buf;\n\nerror:\n\tkfree(buf);\n\treturn NULL;\n}\n\n#define NVRAM_TEST_SIZE 0x100\n#define NVRAM_SELFBOOT_FORMAT1_0_SIZE\t0x14\n#define NVRAM_SELFBOOT_FORMAT1_2_SIZE\t0x18\n#define NVRAM_SELFBOOT_FORMAT1_3_SIZE\t0x1c\n#define NVRAM_SELFBOOT_FORMAT1_4_SIZE\t0x20\n#define NVRAM_SELFBOOT_FORMAT1_5_SIZE\t0x24\n#define NVRAM_SELFBOOT_FORMAT1_6_SIZE\t0x50\n#define NVRAM_SELFBOOT_HW_SIZE 0x20\n#define NVRAM_SELFBOOT_DATA_SIZE 0x1c\n\nstatic int tg3_test_nvram(struct tg3 *tp)\n{\n\tu32 csum, magic, len;\n\t__be32 *buf;\n\tint i, j, k, err = 0, size;\n\n\tif (tg3_flag(tp, NO_NVRAM))\n\t\treturn 0;\n\n\tif (tg3_nvram_read(tp, 0, &magic) != 0)\n\t\treturn -EIO;\n\n\tif (magic == TG3_EEPROM_MAGIC)\n\t\tsize = NVRAM_TEST_SIZE;\n\telse if ((magic & TG3_EEPROM_MAGIC_FW_MSK) == TG3_EEPROM_MAGIC_FW) {\n\t\tif ((magic & TG3_EEPROM_SB_FORMAT_MASK) ==\n\t\t    TG3_EEPROM_SB_FORMAT_1) {\n\t\t\tswitch (magic & TG3_EEPROM_SB_REVISION_MASK) {\n\t\t\tcase TG3_EEPROM_SB_REVISION_0:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_0_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_2:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_2_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_3:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_3_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_4:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_4_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_5:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_5_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase TG3_EEPROM_SB_REVISION_6:\n\t\t\t\tsize = NVRAM_SELFBOOT_FORMAT1_6_SIZE;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t} else\n\t\t\treturn 0;\n\t} else if ((magic & TG3_EEPROM_MAGIC_HW_MSK) == TG3_EEPROM_MAGIC_HW)\n\t\tsize = NVRAM_SELFBOOT_HW_SIZE;\n\telse\n\t\treturn -EIO;\n\n\tbuf = kmalloc(size, GFP_KERNEL);\n\tif (buf == NULL)\n\t\treturn -ENOMEM;\n\n\terr = -EIO;\n\tfor (i = 0, j = 0; i < size; i += 4, j++) {\n\t\terr = tg3_nvram_read_be32(tp, i, &buf[j]);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tif (i < size)\n\t\tgoto out;\n\n\t/* Selfboot format */\n\tmagic = be32_to_cpu(buf[0]);\n\tif ((magic & TG3_EEPROM_MAGIC_FW_MSK) ==\n\t    TG3_EEPROM_MAGIC_FW) {\n\t\tu8 *buf8 = (u8 *) buf, csum8 = 0;\n\n\t\tif ((magic & TG3_EEPROM_SB_REVISION_MASK) ==\n\t\t    TG3_EEPROM_SB_REVISION_2) {\n\t\t\t/* For rev 2, the csum doesn't include the MBA. */\n\t\t\tfor (i = 0; i < TG3_EEPROM_SB_F1R2_MBA_OFF; i++)\n\t\t\t\tcsum8 += buf8[i];\n\t\t\tfor (i = TG3_EEPROM_SB_F1R2_MBA_OFF + 4; i < size; i++)\n\t\t\t\tcsum8 += buf8[i];\n\t\t} else {\n\t\t\tfor (i = 0; i < size; i++)\n\t\t\t\tcsum8 += buf8[i];\n\t\t}\n\n\t\tif (csum8 == 0) {\n\t\t\terr = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tif ((magic & TG3_EEPROM_MAGIC_HW_MSK) ==\n\t    TG3_EEPROM_MAGIC_HW) {\n\t\tu8 data[NVRAM_SELFBOOT_DATA_SIZE];\n\t\tu8 parity[NVRAM_SELFBOOT_DATA_SIZE];\n\t\tu8 *buf8 = (u8 *) buf;\n\n\t\t/* Separate the parity bits and the data bytes.  */\n\t\tfor (i = 0, j = 0, k = 0; i < NVRAM_SELFBOOT_HW_SIZE; i++) {\n\t\t\tif ((i == 0) || (i == 8)) {\n\t\t\t\tint l;\n\t\t\t\tu8 msk;\n\n\t\t\t\tfor (l = 0, msk = 0x80; l < 7; l++, msk >>= 1)\n\t\t\t\t\tparity[k++] = buf8[i] & msk;\n\t\t\t\ti++;\n\t\t\t} else if (i == 16) {\n\t\t\t\tint l;\n\t\t\t\tu8 msk;\n\n\t\t\t\tfor (l = 0, msk = 0x20; l < 6; l++, msk >>= 1)\n\t\t\t\t\tparity[k++] = buf8[i] & msk;\n\t\t\t\ti++;\n\n\t\t\t\tfor (l = 0, msk = 0x80; l < 8; l++, msk >>= 1)\n\t\t\t\t\tparity[k++] = buf8[i] & msk;\n\t\t\t\ti++;\n\t\t\t}\n\t\t\tdata[j++] = buf8[i];\n\t\t}\n\n\t\terr = -EIO;\n\t\tfor (i = 0; i < NVRAM_SELFBOOT_DATA_SIZE; i++) {\n\t\t\tu8 hw8 = hweight8(data[i]);\n\n\t\t\tif ((hw8 & 0x1) && parity[i])\n\t\t\t\tgoto out;\n\t\t\telse if (!(hw8 & 0x1) && !parity[i])\n\t\t\t\tgoto out;\n\t\t}\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\terr = -EIO;\n\n\t/* Bootstrap checksum at offset 0x10 */\n\tcsum = calc_crc((unsigned char *) buf, 0x10);\n\tif (csum != le32_to_cpu(buf[0x10/4]))\n\t\tgoto out;\n\n\t/* Manufacturing block starts at offset 0x74, checksum at 0xfc */\n\tcsum = calc_crc((unsigned char *) &buf[0x74/4], 0x88);\n\tif (csum != le32_to_cpu(buf[0xfc/4]))\n\t\tgoto out;\n\n\tkfree(buf);\n\n\tbuf = tg3_vpd_readblock(tp, &len);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\ti = pci_vpd_find_tag((u8 *)buf, 0, len, PCI_VPD_LRDT_RO_DATA);\n\tif (i > 0) {\n\t\tj = pci_vpd_lrdt_size(&((u8 *)buf)[i]);\n\t\tif (j < 0)\n\t\t\tgoto out;\n\n\t\tif (i + PCI_VPD_LRDT_TAG_SIZE + j > len)\n\t\t\tgoto out;\n\n\t\ti += PCI_VPD_LRDT_TAG_SIZE;\n\t\tj = pci_vpd_find_info_keyword((u8 *)buf, i, j,\n\t\t\t\t\t      PCI_VPD_RO_KEYWORD_CHKSUM);\n\t\tif (j > 0) {\n\t\t\tu8 csum8 = 0;\n\n\t\t\tj += PCI_VPD_INFO_FLD_HDR_SIZE;\n\n\t\t\tfor (i = 0; i <= j; i++)\n\t\t\t\tcsum8 += ((u8 *)buf)[i];\n\n\t\t\tif (csum8)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\terr = 0;\n\nout:\n\tkfree(buf);\n\treturn err;\n}\n\n#define TG3_SERDES_TIMEOUT_SEC\t2\n#define TG3_COPPER_TIMEOUT_SEC\t6\n\nstatic int tg3_test_link(struct tg3 *tp)\n{\n\tint i, max;\n\n\tif (!netif_running(tp->dev))\n\t\treturn -ENODEV;\n\n\tif (tp->phy_flags & TG3_PHYFLG_ANY_SERDES)\n\t\tmax = TG3_SERDES_TIMEOUT_SEC;\n\telse\n\t\tmax = TG3_COPPER_TIMEOUT_SEC;\n\n\tfor (i = 0; i < max; i++) {\n\t\tif (tp->link_up)\n\t\t\treturn 0;\n\n\t\tif (msleep_interruptible(1000))\n\t\t\tbreak;\n\t}\n\n\treturn -EIO;\n}\n\n/* Only test the commonly used registers */\nstatic int tg3_test_registers(struct tg3 *tp)\n{\n\tint i, is_5705, is_5750;\n\tu32 offset, read_mask, write_mask, val, save_val, read_val;\n\tstatic struct {\n\t\tu16 offset;\n\t\tu16 flags;\n#define TG3_FL_5705\t0x1\n#define TG3_FL_NOT_5705\t0x2\n#define TG3_FL_NOT_5788\t0x4\n#define TG3_FL_NOT_5750\t0x8\n\t\tu32 read_mask;\n\t\tu32 write_mask;\n\t} reg_tbl[] = {\n\t\t/* MAC Control Registers */\n\t\t{ MAC_MODE, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x00ef6f8c },\n\t\t{ MAC_MODE, TG3_FL_5705,\n\t\t\t0x00000000, 0x01ef6b8c },\n\t\t{ MAC_STATUS, TG3_FL_NOT_5705,\n\t\t\t0x03800107, 0x00000000 },\n\t\t{ MAC_STATUS, TG3_FL_5705,\n\t\t\t0x03800100, 0x00000000 },\n\t\t{ MAC_ADDR_0_HIGH, 0x0000,\n\t\t\t0x00000000, 0x0000ffff },\n\t\t{ MAC_ADDR_0_LOW, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ MAC_RX_MTU_SIZE, 0x0000,\n\t\t\t0x00000000, 0x0000ffff },\n\t\t{ MAC_TX_MODE, 0x0000,\n\t\t\t0x00000000, 0x00000070 },\n\t\t{ MAC_TX_LENGTHS, 0x0000,\n\t\t\t0x00000000, 0x00003fff },\n\t\t{ MAC_RX_MODE, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x000007fc },\n\t\t{ MAC_RX_MODE, TG3_FL_5705,\n\t\t\t0x00000000, 0x000007dc },\n\t\t{ MAC_HASH_REG_0, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ MAC_HASH_REG_1, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ MAC_HASH_REG_2, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ MAC_HASH_REG_3, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\n\t\t/* Receive Data and Receive BD Initiator Control Registers. */\n\t\t{ RCVDBDI_JUMBO_BD+0, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_JUMBO_BD+4, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_JUMBO_BD+8, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x00000003 },\n\t\t{ RCVDBDI_JUMBO_BD+0xc, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_STD_BD+0, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_STD_BD+4, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVDBDI_STD_BD+8, 0x0000,\n\t\t\t0x00000000, 0xffff0002 },\n\t\t{ RCVDBDI_STD_BD+0xc, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\n\t\t/* Receive BD Initiator Control Registers. */\n\t\t{ RCVBDI_STD_THRESH, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ RCVBDI_STD_THRESH, TG3_FL_5705,\n\t\t\t0x00000000, 0x000003ff },\n\t\t{ RCVBDI_JUMBO_THRESH, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\n\t\t/* Host Coalescing Control Registers. */\n\t\t{ HOSTCC_MODE, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x00000004 },\n\t\t{ HOSTCC_MODE, TG3_FL_5705,\n\t\t\t0x00000000, 0x000000f6 },\n\t\t{ HOSTCC_RXCOL_TICKS, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_RXCOL_TICKS, TG3_FL_5705,\n\t\t\t0x00000000, 0x000003ff },\n\t\t{ HOSTCC_TXCOL_TICKS, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_TXCOL_TICKS, TG3_FL_5705,\n\t\t\t0x00000000, 0x000003ff },\n\t\t{ HOSTCC_RXMAX_FRAMES, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_RXMAX_FRAMES, TG3_FL_5705 | TG3_FL_NOT_5788,\n\t\t\t0x00000000, 0x000000ff },\n\t\t{ HOSTCC_TXMAX_FRAMES, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_TXMAX_FRAMES, TG3_FL_5705 | TG3_FL_NOT_5788,\n\t\t\t0x00000000, 0x000000ff },\n\t\t{ HOSTCC_RXCOAL_TICK_INT, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_TXCOAL_TICK_INT, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_RXCOAL_MAXF_INT, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_RXCOAL_MAXF_INT, TG3_FL_5705 | TG3_FL_NOT_5788,\n\t\t\t0x00000000, 0x000000ff },\n\t\t{ HOSTCC_TXCOAL_MAXF_INT, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_TXCOAL_MAXF_INT, TG3_FL_5705 | TG3_FL_NOT_5788,\n\t\t\t0x00000000, 0x000000ff },\n\t\t{ HOSTCC_STAT_COAL_TICKS, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATS_BLK_HOST_ADDR, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATS_BLK_HOST_ADDR+4, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATUS_BLK_HOST_ADDR, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATUS_BLK_HOST_ADDR+4, 0x0000,\n\t\t\t0x00000000, 0xffffffff },\n\t\t{ HOSTCC_STATS_BLK_NIC_ADDR, 0x0000,\n\t\t\t0xffffffff, 0x00000000 },\n\t\t{ HOSTCC_STATUS_BLK_NIC_ADDR, 0x0000,\n\t\t\t0xffffffff, 0x00000000 },\n\n\t\t/* Buffer Manager Control Registers. */\n\t\t{ BUFMGR_MB_POOL_ADDR, TG3_FL_NOT_5750,\n\t\t\t0x00000000, 0x007fff80 },\n\t\t{ BUFMGR_MB_POOL_SIZE, TG3_FL_NOT_5750,\n\t\t\t0x00000000, 0x007fffff },\n\t\t{ BUFMGR_MB_RDMA_LOW_WATER, 0x0000,\n\t\t\t0x00000000, 0x0000003f },\n\t\t{ BUFMGR_MB_MACRX_LOW_WATER, 0x0000,\n\t\t\t0x00000000, 0x000001ff },\n\t\t{ BUFMGR_MB_HIGH_WATER, 0x0000,\n\t\t\t0x00000000, 0x000001ff },\n\t\t{ BUFMGR_DMA_DESC_POOL_ADDR, TG3_FL_NOT_5705,\n\t\t\t0xffffffff, 0x00000000 },\n\t\t{ BUFMGR_DMA_DESC_POOL_SIZE, TG3_FL_NOT_5705,\n\t\t\t0xffffffff, 0x00000000 },\n\n\t\t/* Mailbox Registers */\n\t\t{ GRCMBOX_RCVSTD_PROD_IDX+4, 0x0000,\n\t\t\t0x00000000, 0x000001ff },\n\t\t{ GRCMBOX_RCVJUMBO_PROD_IDX+4, TG3_FL_NOT_5705,\n\t\t\t0x00000000, 0x000001ff },\n\t\t{ GRCMBOX_RCVRET_CON_IDX_0+4, 0x0000,\n\t\t\t0x00000000, 0x000007ff },\n\t\t{ GRCMBOX_SNDHOST_PROD_IDX_0+4, 0x0000,\n\t\t\t0x00000000, 0x000001ff },\n\n\t\t{ 0xffff, 0x0000, 0x00000000, 0x00000000 },\n\t};\n\n\tis_5705 = is_5750 = 0;\n\tif (tg3_flag(tp, 5705_PLUS)) {\n\t\tis_5705 = 1;\n\t\tif (tg3_flag(tp, 5750_PLUS))\n\t\t\tis_5750 = 1;\n\t}\n\n\tfor (i = 0; reg_tbl[i].offset != 0xffff; i++) {\n\t\tif (is_5705 && (reg_tbl[i].flags & TG3_FL_NOT_5705))\n\t\t\tcontinue;\n\n\t\tif (!is_5705 && (reg_tbl[i].flags & TG3_FL_5705))\n\t\t\tcontinue;\n\n\t\tif (tg3_flag(tp, IS_5788) &&\n\t\t    (reg_tbl[i].flags & TG3_FL_NOT_5788))\n\t\t\tcontinue;\n\n\t\tif (is_5750 && (reg_tbl[i].flags & TG3_FL_NOT_5750))\n\t\t\tcontinue;\n\n\t\toffset = (u32) reg_tbl[i].offset;\n\t\tread_mask = reg_tbl[i].read_mask;\n\t\twrite_mask = reg_tbl[i].write_mask;\n\n\t\t/* Save the original register content */\n\t\tsave_val = tr32(offset);\n\n\t\t/* Determine the read-only value. */\n\t\tread_val = save_val & read_mask;\n\n\t\t/* Write zero to the register, then make sure the read-only bits\n\t\t * are not changed and the read/write bits are all zeros.\n\t\t */\n\t\ttw32(offset, 0);\n\n\t\tval = tr32(offset);\n\n\t\t/* Test the read-only and read/write bits. */\n\t\tif (((val & read_mask) != read_val) || (val & write_mask))\n\t\t\tgoto out;\n\n\t\t/* Write ones to all the bits defined by RdMask and WrMask, then\n\t\t * make sure the read-only bits are not changed and the\n\t\t * read/write bits are all ones.\n\t\t */\n\t\ttw32(offset, read_mask | write_mask);\n\n\t\tval = tr32(offset);\n\n\t\t/* Test the read-only bits. */\n\t\tif ((val & read_mask) != read_val)\n\t\t\tgoto out;\n\n\t\t/* Test the read/write bits. */\n\t\tif ((val & write_mask) != write_mask)\n\t\t\tgoto out;\n\n\t\ttw32(offset, save_val);\n\t}\n\n\treturn 0;\n\nout:\n\tif (netif_msg_hw(tp))\n\t\tnetdev_err(tp->dev,\n\t\t\t   \"Register test failed at offset %x\\n\", offset);\n\ttw32(offset, save_val);\n\treturn -EIO;\n}\n\nstatic int tg3_do_mem_test(struct tg3 *tp, u32 offset, u32 len)\n{\n\tstatic const u32 test_pattern[] = { 0x00000000, 0xffffffff, 0xaa55a55a };\n\tint i;\n\tu32 j;\n\n\tfor (i = 0; i < ARRAY_SIZE(test_pattern); i++) {\n\t\tfor (j = 0; j < len; j += 4) {\n\t\t\tu32 val;\n\n\t\t\ttg3_write_mem(tp, offset + j, test_pattern[i]);\n\t\t\ttg3_read_mem(tp, offset + j, &val);\n\t\t\tif (val != test_pattern[i])\n\t\t\t\treturn -EIO;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int tg3_test_memory(struct tg3 *tp)\n{\n\tstatic struct mem_entry {\n\t\tu32 offset;\n\t\tu32 len;\n\t} mem_tbl_570x[] = {\n\t\t{ 0x00000000, 0x00b50},\n\t\t{ 0x00002000, 0x1c000},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_5705[] = {\n\t\t{ 0x00000100, 0x0000c},\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00004000, 0x00800},\n\t\t{ 0x00006000, 0x01000},\n\t\t{ 0x00008000, 0x02000},\n\t\t{ 0x00010000, 0x0e000},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_5755[] = {\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00004000, 0x00800},\n\t\t{ 0x00006000, 0x00800},\n\t\t{ 0x00008000, 0x02000},\n\t\t{ 0x00010000, 0x0c000},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_5906[] = {\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00004000, 0x00400},\n\t\t{ 0x00006000, 0x00400},\n\t\t{ 0x00008000, 0x01000},\n\t\t{ 0x00010000, 0x01000},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_5717[] = {\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00010000, 0x0a000},\n\t\t{ 0x00020000, 0x13c00},\n\t\t{ 0xffffffff, 0x00000}\n\t}, mem_tbl_57765[] = {\n\t\t{ 0x00000200, 0x00008},\n\t\t{ 0x00004000, 0x00800},\n\t\t{ 0x00006000, 0x09800},\n\t\t{ 0x00010000, 0x0a000},\n\t\t{ 0xffffffff, 0x00000}\n\t};\n\tstruct mem_entry *mem_tbl;\n\tint err = 0;\n\tint i;\n\n\tif (tg3_flag(tp, 5717_PLUS))\n\t\tmem_tbl = mem_tbl_5717;\n\telse if (tg3_flag(tp, 57765_CLASS) ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tmem_tbl = mem_tbl_57765;\n\telse if (tg3_flag(tp, 5755_PLUS))\n\t\tmem_tbl = mem_tbl_5755;\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\tmem_tbl = mem_tbl_5906;\n\telse if (tg3_flag(tp, 5705_PLUS))\n\t\tmem_tbl = mem_tbl_5705;\n\telse\n\t\tmem_tbl = mem_tbl_570x;\n\n\tfor (i = 0; mem_tbl[i].offset != 0xffffffff; i++) {\n\t\terr = tg3_do_mem_test(tp, mem_tbl[i].offset, mem_tbl[i].len);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n#define TG3_TSO_MSS\t\t500\n\n#define TG3_TSO_IP_HDR_LEN\t20\n#define TG3_TSO_TCP_HDR_LEN\t20\n#define TG3_TSO_TCP_OPT_LEN\t12\n\nstatic const u8 tg3_tso_header[] = {\n0x08, 0x00,\n0x45, 0x00, 0x00, 0x00,\n0x00, 0x00, 0x40, 0x00,\n0x40, 0x06, 0x00, 0x00,\n0x0a, 0x00, 0x00, 0x01,\n0x0a, 0x00, 0x00, 0x02,\n0x0d, 0x00, 0xe0, 0x00,\n0x00, 0x00, 0x01, 0x00,\n0x00, 0x00, 0x02, 0x00,\n0x80, 0x10, 0x10, 0x00,\n0x14, 0x09, 0x00, 0x00,\n0x01, 0x01, 0x08, 0x0a,\n0x11, 0x11, 0x11, 0x11,\n0x11, 0x11, 0x11, 0x11,\n};\n\nstatic int tg3_run_loopback(struct tg3 *tp, u32 pktsz, bool tso_loopback)\n{\n\tu32 rx_start_idx, rx_idx, tx_idx, opaque_key;\n\tu32 base_flags = 0, mss = 0, desc_idx, coal_now, data_off, val;\n\tu32 budget;\n\tstruct sk_buff *skb;\n\tu8 *tx_data, *rx_data;\n\tdma_addr_t map;\n\tint num_pkts, tx_len, rx_len, i, err;\n\tstruct tg3_rx_buffer_desc *desc;\n\tstruct tg3_napi *tnapi, *rnapi;\n\tstruct tg3_rx_prodring_set *tpr = &tp->napi[0].prodring;\n\n\ttnapi = &tp->napi[0];\n\trnapi = &tp->napi[0];\n\tif (tp->irq_cnt > 1) {\n\t\tif (tg3_flag(tp, ENABLE_RSS))\n\t\t\trnapi = &tp->napi[1];\n\t\tif (tg3_flag(tp, ENABLE_TSS))\n\t\t\ttnapi = &tp->napi[1];\n\t}\n\tcoal_now = tnapi->coal_now | rnapi->coal_now;\n\n\terr = -EIO;\n\n\ttx_len = pktsz;\n\tskb = netdev_alloc_skb(tp->dev, tx_len);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\ttx_data = skb_put(skb, tx_len);\n\tmemcpy(tx_data, tp->dev->dev_addr, 6);\n\tmemset(tx_data + 6, 0x0, 8);\n\n\ttw32(MAC_RX_MTU_SIZE, tx_len + ETH_FCS_LEN);\n\n\tif (tso_loopback) {\n\t\tstruct iphdr *iph = (struct iphdr *)&tx_data[ETH_HLEN];\n\n\t\tu32 hdr_len = TG3_TSO_IP_HDR_LEN + TG3_TSO_TCP_HDR_LEN +\n\t\t\t      TG3_TSO_TCP_OPT_LEN;\n\n\t\tmemcpy(tx_data + ETH_ALEN * 2, tg3_tso_header,\n\t\t       sizeof(tg3_tso_header));\n\t\tmss = TG3_TSO_MSS;\n\n\t\tval = tx_len - ETH_ALEN * 2 - sizeof(tg3_tso_header);\n\t\tnum_pkts = DIV_ROUND_UP(val, TG3_TSO_MSS);\n\n\t\t/* Set the total length field in the IP header */\n\t\tiph->tot_len = htons((u16)(mss + hdr_len));\n\n\t\tbase_flags = (TXD_FLAG_CPU_PRE_DMA |\n\t\t\t      TXD_FLAG_CPU_POST_DMA);\n\n\t\tif (tg3_flag(tp, HW_TSO_1) ||\n\t\t    tg3_flag(tp, HW_TSO_2) ||\n\t\t    tg3_flag(tp, HW_TSO_3)) {\n\t\t\tstruct tcphdr *th;\n\t\t\tval = ETH_HLEN + TG3_TSO_IP_HDR_LEN;\n\t\t\tth = (struct tcphdr *)&tx_data[val];\n\t\t\tth->check = 0;\n\t\t} else\n\t\t\tbase_flags |= TXD_FLAG_TCPUDP_CSUM;\n\n\t\tif (tg3_flag(tp, HW_TSO_3)) {\n\t\t\tmss |= (hdr_len & 0xc) << 12;\n\t\t\tif (hdr_len & 0x10)\n\t\t\t\tbase_flags |= 0x00000010;\n\t\t\tbase_flags |= (hdr_len & 0x3e0) << 5;\n\t\t} else if (tg3_flag(tp, HW_TSO_2))\n\t\t\tmss |= hdr_len << 9;\n\t\telse if (tg3_flag(tp, HW_TSO_1) ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\t\tmss |= (TG3_TSO_TCP_OPT_LEN << 9);\n\t\t} else {\n\t\t\tbase_flags |= (TG3_TSO_TCP_OPT_LEN << 10);\n\t\t}\n\n\t\tdata_off = ETH_ALEN * 2 + sizeof(tg3_tso_header);\n\t} else {\n\t\tnum_pkts = 1;\n\t\tdata_off = ETH_HLEN;\n\n\t\tif (tg3_flag(tp, USE_JUMBO_BDFLAG) &&\n\t\t    tx_len > VLAN_ETH_FRAME_LEN)\n\t\t\tbase_flags |= TXD_FLAG_JMB_PKT;\n\t}\n\n\tfor (i = data_off; i < tx_len; i++)\n\t\ttx_data[i] = (u8) (i & 0xff);\n\n\tmap = pci_map_single(tp->pdev, skb->data, tx_len, PCI_DMA_TODEVICE);\n\tif (pci_dma_mapping_error(tp->pdev, map)) {\n\t\tdev_kfree_skb(skb);\n\t\treturn -EIO;\n\t}\n\n\tval = tnapi->tx_prod;\n\ttnapi->tx_buffers[val].skb = skb;\n\tdma_unmap_addr_set(&tnapi->tx_buffers[val], mapping, map);\n\n\ttw32_f(HOSTCC_MODE, tp->coalesce_mode | HOSTCC_MODE_ENABLE |\n\t       rnapi->coal_now);\n\n\tudelay(10);\n\n\trx_start_idx = rnapi->hw_status->idx[0].rx_producer;\n\n\tbudget = tg3_tx_avail(tnapi);\n\tif (tg3_tx_frag_set(tnapi, &val, &budget, map, tx_len,\n\t\t\t    base_flags | TXD_FLAG_END, mss, 0)) {\n\t\ttnapi->tx_buffers[val].skb = NULL;\n\t\tdev_kfree_skb(skb);\n\t\treturn -EIO;\n\t}\n\n\ttnapi->tx_prod++;\n\n\t/* Sync BD data before updating mailbox */\n\twmb();\n\n\ttw32_tx_mbox(tnapi->prodmbox, tnapi->tx_prod);\n\ttr32_mailbox(tnapi->prodmbox);\n\n\tudelay(10);\n\n\t/* 350 usec to allow enough time on some 10/100 Mbps devices.  */\n\tfor (i = 0; i < 35; i++) {\n\t\ttw32_f(HOSTCC_MODE, tp->coalesce_mode | HOSTCC_MODE_ENABLE |\n\t\t       coal_now);\n\n\t\tudelay(10);\n\n\t\ttx_idx = tnapi->hw_status->idx[0].tx_consumer;\n\t\trx_idx = rnapi->hw_status->idx[0].rx_producer;\n\t\tif ((tx_idx == tnapi->tx_prod) &&\n\t\t    (rx_idx == (rx_start_idx + num_pkts)))\n\t\t\tbreak;\n\t}\n\n\ttg3_tx_skb_unmap(tnapi, tnapi->tx_prod - 1, -1);\n\tdev_kfree_skb(skb);\n\n\tif (tx_idx != tnapi->tx_prod)\n\t\tgoto out;\n\n\tif (rx_idx != rx_start_idx + num_pkts)\n\t\tgoto out;\n\n\tval = data_off;\n\twhile (rx_idx != rx_start_idx) {\n\t\tdesc = &rnapi->rx_rcb[rx_start_idx++];\n\t\tdesc_idx = desc->opaque & RXD_OPAQUE_INDEX_MASK;\n\t\topaque_key = desc->opaque & RXD_OPAQUE_RING_MASK;\n\n\t\tif ((desc->err_vlan & RXD_ERR_MASK) != 0 &&\n\t\t    (desc->err_vlan != RXD_ERR_ODD_NIBBLE_RCVD_MII))\n\t\t\tgoto out;\n\n\t\trx_len = ((desc->idx_len & RXD_LEN_MASK) >> RXD_LEN_SHIFT)\n\t\t\t - ETH_FCS_LEN;\n\n\t\tif (!tso_loopback) {\n\t\t\tif (rx_len != tx_len)\n\t\t\t\tgoto out;\n\n\t\t\tif (pktsz <= TG3_RX_STD_DMA_SZ - ETH_FCS_LEN) {\n\t\t\t\tif (opaque_key != RXD_OPAQUE_RING_STD)\n\t\t\t\t\tgoto out;\n\t\t\t} else {\n\t\t\t\tif (opaque_key != RXD_OPAQUE_RING_JUMBO)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else if ((desc->type_flags & RXD_FLAG_TCPUDP_CSUM) &&\n\t\t\t   (desc->ip_tcp_csum & RXD_TCPCSUM_MASK)\n\t\t\t    >> RXD_TCPCSUM_SHIFT != 0xffff) {\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (opaque_key == RXD_OPAQUE_RING_STD) {\n\t\t\trx_data = tpr->rx_std_buffers[desc_idx].data;\n\t\t\tmap = dma_unmap_addr(&tpr->rx_std_buffers[desc_idx],\n\t\t\t\t\t     mapping);\n\t\t} else if (opaque_key == RXD_OPAQUE_RING_JUMBO) {\n\t\t\trx_data = tpr->rx_jmb_buffers[desc_idx].data;\n\t\t\tmap = dma_unmap_addr(&tpr->rx_jmb_buffers[desc_idx],\n\t\t\t\t\t     mapping);\n\t\t} else\n\t\t\tgoto out;\n\n\t\tpci_dma_sync_single_for_cpu(tp->pdev, map, rx_len,\n\t\t\t\t\t    PCI_DMA_FROMDEVICE);\n\n\t\trx_data += TG3_RX_OFFSET(tp);\n\t\tfor (i = data_off; i < rx_len; i++, val++) {\n\t\t\tif (*(rx_data + i) != (u8) (val & 0xff))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\terr = 0;\n\n\t/* tg3_free_rings will unmap and free the rx_data */\nout:\n\treturn err;\n}\n\n#define TG3_STD_LOOPBACK_FAILED\t\t1\n#define TG3_JMB_LOOPBACK_FAILED\t\t2\n#define TG3_TSO_LOOPBACK_FAILED\t\t4\n#define TG3_LOOPBACK_FAILED \\\n\t(TG3_STD_LOOPBACK_FAILED | \\\n\t TG3_JMB_LOOPBACK_FAILED | \\\n\t TG3_TSO_LOOPBACK_FAILED)\n\nstatic int tg3_test_loopback(struct tg3 *tp, u64 *data, bool do_extlpbk)\n{\n\tint err = -EIO;\n\tu32 eee_cap;\n\tu32 jmb_pkt_sz = 9000;\n\n\tif (tp->dma_limit)\n\t\tjmb_pkt_sz = tp->dma_limit - ETH_HLEN;\n\n\teee_cap = tp->phy_flags & TG3_PHYFLG_EEE_CAP;\n\ttp->phy_flags &= ~TG3_PHYFLG_EEE_CAP;\n\n\tif (!netif_running(tp->dev)) {\n\t\tdata[TG3_MAC_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tdata[TG3_PHY_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tif (do_extlpbk)\n\t\t\tdata[TG3_EXT_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tgoto done;\n\t}\n\n\terr = tg3_reset_hw(tp, 1);\n\tif (err) {\n\t\tdata[TG3_MAC_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tdata[TG3_PHY_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tif (do_extlpbk)\n\t\t\tdata[TG3_EXT_LOOPB_TEST] = TG3_LOOPBACK_FAILED;\n\t\tgoto done;\n\t}\n\n\tif (tg3_flag(tp, ENABLE_RSS)) {\n\t\tint i;\n\n\t\t/* Reroute all rx packets to the 1st queue */\n\t\tfor (i = MAC_RSS_INDIR_TBL_0;\n\t\t     i < MAC_RSS_INDIR_TBL_0 + TG3_RSS_INDIR_TBL_SIZE; i += 4)\n\t\t\ttw32(i, 0x0);\n\t}\n\n\t/* HW errata - mac loopback fails in some cases on 5780.\n\t * Normal traffic and PHY loopback are not affected by\n\t * errata.  Also, the MAC loopback test is deprecated for\n\t * all newer ASIC revisions.\n\t */\n\tif (tg3_asic_rev(tp) != ASIC_REV_5780 &&\n\t    !tg3_flag(tp, CPMU_PRESENT)) {\n\t\ttg3_mac_loopback(tp, true);\n\n\t\tif (tg3_run_loopback(tp, ETH_FRAME_LEN, false))\n\t\t\tdata[TG3_MAC_LOOPB_TEST] |= TG3_STD_LOOPBACK_FAILED;\n\n\t\tif (tg3_flag(tp, JUMBO_RING_ENABLE) &&\n\t\t    tg3_run_loopback(tp, jmb_pkt_sz + ETH_HLEN, false))\n\t\t\tdata[TG3_MAC_LOOPB_TEST] |= TG3_JMB_LOOPBACK_FAILED;\n\n\t\ttg3_mac_loopback(tp, false);\n\t}\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_PHY_SERDES) &&\n\t    !tg3_flag(tp, USE_PHYLIB)) {\n\t\tint i;\n\n\t\ttg3_phy_lpbk_set(tp, 0, false);\n\n\t\t/* Wait for link */\n\t\tfor (i = 0; i < 100; i++) {\n\t\t\tif (tr32(MAC_TX_STATUS) & TX_STATUS_LINK_UP)\n\t\t\t\tbreak;\n\t\t\tmdelay(1);\n\t\t}\n\n\t\tif (tg3_run_loopback(tp, ETH_FRAME_LEN, false))\n\t\t\tdata[TG3_PHY_LOOPB_TEST] |= TG3_STD_LOOPBACK_FAILED;\n\t\tif (tg3_flag(tp, TSO_CAPABLE) &&\n\t\t    tg3_run_loopback(tp, ETH_FRAME_LEN, true))\n\t\t\tdata[TG3_PHY_LOOPB_TEST] |= TG3_TSO_LOOPBACK_FAILED;\n\t\tif (tg3_flag(tp, JUMBO_RING_ENABLE) &&\n\t\t    tg3_run_loopback(tp, jmb_pkt_sz + ETH_HLEN, false))\n\t\t\tdata[TG3_PHY_LOOPB_TEST] |= TG3_JMB_LOOPBACK_FAILED;\n\n\t\tif (do_extlpbk) {\n\t\t\ttg3_phy_lpbk_set(tp, 0, true);\n\n\t\t\t/* All link indications report up, but the hardware\n\t\t\t * isn't really ready for about 20 msec.  Double it\n\t\t\t * to be sure.\n\t\t\t */\n\t\t\tmdelay(40);\n\n\t\t\tif (tg3_run_loopback(tp, ETH_FRAME_LEN, false))\n\t\t\t\tdata[TG3_EXT_LOOPB_TEST] |=\n\t\t\t\t\t\t\tTG3_STD_LOOPBACK_FAILED;\n\t\t\tif (tg3_flag(tp, TSO_CAPABLE) &&\n\t\t\t    tg3_run_loopback(tp, ETH_FRAME_LEN, true))\n\t\t\t\tdata[TG3_EXT_LOOPB_TEST] |=\n\t\t\t\t\t\t\tTG3_TSO_LOOPBACK_FAILED;\n\t\t\tif (tg3_flag(tp, JUMBO_RING_ENABLE) &&\n\t\t\t    tg3_run_loopback(tp, jmb_pkt_sz + ETH_HLEN, false))\n\t\t\t\tdata[TG3_EXT_LOOPB_TEST] |=\n\t\t\t\t\t\t\tTG3_JMB_LOOPBACK_FAILED;\n\t\t}\n\n\t\t/* Re-enable gphy autopowerdown. */\n\t\tif (tp->phy_flags & TG3_PHYFLG_ENABLE_APD)\n\t\t\ttg3_phy_toggle_apd(tp, true);\n\t}\n\n\terr = (data[TG3_MAC_LOOPB_TEST] | data[TG3_PHY_LOOPB_TEST] |\n\t       data[TG3_EXT_LOOPB_TEST]) ? -EIO : 0;\n\ndone:\n\ttp->phy_flags |= eee_cap;\n\n\treturn err;\n}\n\nstatic void tg3_self_test(struct net_device *dev, struct ethtool_test *etest,\n\t\t\t  u64 *data)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tbool doextlpbk = etest->flags & ETH_TEST_FL_EXTERNAL_LB;\n\n\tif ((tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER) &&\n\t    tg3_power_up(tp)) {\n\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\tmemset(data, 1, sizeof(u64) * TG3_NUM_TEST);\n\t\treturn;\n\t}\n\n\tmemset(data, 0, sizeof(u64) * TG3_NUM_TEST);\n\n\tif (tg3_test_nvram(tp) != 0) {\n\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\tdata[TG3_NVRAM_TEST] = 1;\n\t}\n\tif (!doextlpbk && tg3_test_link(tp)) {\n\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\tdata[TG3_LINK_TEST] = 1;\n\t}\n\tif (etest->flags & ETH_TEST_FL_OFFLINE) {\n\t\tint err, err2 = 0, irq_sync = 0;\n\n\t\tif (netif_running(dev)) {\n\t\t\ttg3_phy_stop(tp);\n\t\t\ttg3_netif_stop(tp);\n\t\t\tirq_sync = 1;\n\t\t}\n\n\t\ttg3_full_lock(tp, irq_sync);\n\t\ttg3_halt(tp, RESET_KIND_SUSPEND, 1);\n\t\terr = tg3_nvram_lock(tp);\n\t\ttg3_halt_cpu(tp, RX_CPU_BASE);\n\t\tif (!tg3_flag(tp, 5705_PLUS))\n\t\t\ttg3_halt_cpu(tp, TX_CPU_BASE);\n\t\tif (!err)\n\t\t\ttg3_nvram_unlock(tp);\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_MII_SERDES)\n\t\t\ttg3_phy_reset(tp);\n\n\t\tif (tg3_test_registers(tp) != 0) {\n\t\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tdata[TG3_REGISTER_TEST] = 1;\n\t\t}\n\n\t\tif (tg3_test_memory(tp) != 0) {\n\t\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tdata[TG3_MEMORY_TEST] = 1;\n\t\t}\n\n\t\tif (doextlpbk)\n\t\t\tetest->flags |= ETH_TEST_FL_EXTERNAL_LB_DONE;\n\n\t\tif (tg3_test_loopback(tp, data, doextlpbk))\n\t\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\n\t\ttg3_full_unlock(tp);\n\n\t\tif (tg3_test_interrupt(tp) != 0) {\n\t\t\tetest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tdata[TG3_INTERRUPT_TEST] = 1;\n\t\t}\n\n\t\ttg3_full_lock(tp, 0);\n\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t\tif (netif_running(dev)) {\n\t\t\ttg3_flag_set(tp, INIT_COMPLETE);\n\t\t\terr2 = tg3_restart_hw(tp, 1);\n\t\t\tif (!err2)\n\t\t\t\ttg3_netif_start(tp);\n\t\t}\n\n\t\ttg3_full_unlock(tp);\n\n\t\tif (irq_sync && !err2)\n\t\t\ttg3_phy_start(tp);\n\t}\n\tif (tp->phy_flags & TG3_PHYFLG_IS_LOW_POWER)\n\t\ttg3_power_down(tp);\n\n}\n\nstatic int tg3_hwtstamp_ioctl(struct net_device *dev,\n\t\t\t      struct ifreq *ifr, int cmd)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tstruct hwtstamp_config stmpconf;\n\n\tif (!tg3_flag(tp, PTP_CAPABLE))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&stmpconf, ifr->ifr_data, sizeof(stmpconf)))\n\t\treturn -EFAULT;\n\n\tif (stmpconf.flags)\n\t\treturn -EINVAL;\n\n\tswitch (stmpconf.tx_type) {\n\tcase HWTSTAMP_TX_ON:\n\t\ttg3_flag_set(tp, TX_TSTAMP_EN);\n\t\tbreak;\n\tcase HWTSTAMP_TX_OFF:\n\t\ttg3_flag_clear(tp, TX_TSTAMP_EN);\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tswitch (stmpconf.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\ttp->rxptpctl = 0;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V1_EN |\n\t\t\t       TG3_RX_PTP_CTL_ALL_V1_EVENTS;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V1_EN |\n\t\t\t       TG3_RX_PTP_CTL_SYNC_EVNT;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V1_EN |\n\t\t\t       TG3_RX_PTP_CTL_DELAY_REQ;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_EN |\n\t\t\t       TG3_RX_PTP_CTL_ALL_V2_EVENTS;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L2_EN |\n\t\t\t       TG3_RX_PTP_CTL_ALL_V2_EVENTS;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L4_EN |\n\t\t\t       TG3_RX_PTP_CTL_ALL_V2_EVENTS;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_EN |\n\t\t\t       TG3_RX_PTP_CTL_SYNC_EVNT;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L2_EN |\n\t\t\t       TG3_RX_PTP_CTL_SYNC_EVNT;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L4_EN |\n\t\t\t       TG3_RX_PTP_CTL_SYNC_EVNT;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_EN |\n\t\t\t       TG3_RX_PTP_CTL_DELAY_REQ;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L2_EN |\n\t\t\t       TG3_RX_PTP_CTL_DELAY_REQ;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\t\ttp->rxptpctl = TG3_RX_PTP_CTL_RX_PTP_V2_L4_EN |\n\t\t\t       TG3_RX_PTP_CTL_DELAY_REQ;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tif (netif_running(dev) && tp->rxptpctl)\n\t\ttw32(TG3_RX_PTP_CTL,\n\t\t     tp->rxptpctl | TG3_RX_PTP_CTL_HWTS_INTERLOCK);\n\n\treturn copy_to_user(ifr->ifr_data, &stmpconf, sizeof(stmpconf)) ?\n\t\t-EFAULT : 0;\n}\n\nstatic int tg3_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tstruct mii_ioctl_data *data = if_mii(ifr);\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err;\n\n\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\tstruct phy_device *phydev;\n\t\tif (!(tp->phy_flags & TG3_PHYFLG_IS_CONNECTED))\n\t\t\treturn -EAGAIN;\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\t\treturn phy_mii_ioctl(phydev, ifr, cmd);\n\t}\n\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\t\tdata->phy_id = tp->phy_addr;\n\n\t\t/* fallthru */\n\tcase SIOCGMIIREG: {\n\t\tu32 mii_regval;\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\t\tbreak;\t\t\t/* We have no PHY */\n\n\t\tif (!netif_running(dev))\n\t\t\treturn -EAGAIN;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\terr = __tg3_readphy(tp, data->phy_id & 0x1f,\n\t\t\t\t    data->reg_num & 0x1f, &mii_regval);\n\t\tspin_unlock_bh(&tp->lock);\n\n\t\tdata->val_out = mii_regval;\n\n\t\treturn err;\n\t}\n\n\tcase SIOCSMIIREG:\n\t\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\t\tbreak;\t\t\t/* We have no PHY */\n\n\t\tif (!netif_running(dev))\n\t\t\treturn -EAGAIN;\n\n\t\tspin_lock_bh(&tp->lock);\n\t\terr = __tg3_writephy(tp, data->phy_id & 0x1f,\n\t\t\t\t     data->reg_num & 0x1f, data->val_in);\n\t\tspin_unlock_bh(&tp->lock);\n\n\t\treturn err;\n\n\tcase SIOCSHWTSTAMP:\n\t\treturn tg3_hwtstamp_ioctl(dev, ifr, cmd);\n\n\tdefault:\n\t\t/* do nothing */\n\t\tbreak;\n\t}\n\treturn -EOPNOTSUPP;\n}\n\nstatic int tg3_get_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tmemcpy(ec, &tp->coal, sizeof(*ec));\n\treturn 0;\n}\n\nstatic int tg3_set_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tu32 max_rxcoal_tick_int = 0, max_txcoal_tick_int = 0;\n\tu32 max_stat_coal_ticks = 0, min_stat_coal_ticks = 0;\n\n\tif (!tg3_flag(tp, 5705_PLUS)) {\n\t\tmax_rxcoal_tick_int = MAX_RXCOAL_TICK_INT;\n\t\tmax_txcoal_tick_int = MAX_TXCOAL_TICK_INT;\n\t\tmax_stat_coal_ticks = MAX_STAT_COAL_TICKS;\n\t\tmin_stat_coal_ticks = MIN_STAT_COAL_TICKS;\n\t}\n\n\tif ((ec->rx_coalesce_usecs > MAX_RXCOL_TICKS) ||\n\t    (ec->tx_coalesce_usecs > MAX_TXCOL_TICKS) ||\n\t    (ec->rx_max_coalesced_frames > MAX_RXMAX_FRAMES) ||\n\t    (ec->tx_max_coalesced_frames > MAX_TXMAX_FRAMES) ||\n\t    (ec->rx_coalesce_usecs_irq > max_rxcoal_tick_int) ||\n\t    (ec->tx_coalesce_usecs_irq > max_txcoal_tick_int) ||\n\t    (ec->rx_max_coalesced_frames_irq > MAX_RXCOAL_MAXF_INT) ||\n\t    (ec->tx_max_coalesced_frames_irq > MAX_TXCOAL_MAXF_INT) ||\n\t    (ec->stats_block_coalesce_usecs > max_stat_coal_ticks) ||\n\t    (ec->stats_block_coalesce_usecs < min_stat_coal_ticks))\n\t\treturn -EINVAL;\n\n\t/* No rx interrupts will be generated if both are zero */\n\tif ((ec->rx_coalesce_usecs == 0) &&\n\t    (ec->rx_max_coalesced_frames == 0))\n\t\treturn -EINVAL;\n\n\t/* No tx interrupts will be generated if both are zero */\n\tif ((ec->tx_coalesce_usecs == 0) &&\n\t    (ec->tx_max_coalesced_frames == 0))\n\t\treturn -EINVAL;\n\n\t/* Only copy relevant parameters, ignore all others. */\n\ttp->coal.rx_coalesce_usecs = ec->rx_coalesce_usecs;\n\ttp->coal.tx_coalesce_usecs = ec->tx_coalesce_usecs;\n\ttp->coal.rx_max_coalesced_frames = ec->rx_max_coalesced_frames;\n\ttp->coal.tx_max_coalesced_frames = ec->tx_max_coalesced_frames;\n\ttp->coal.rx_coalesce_usecs_irq = ec->rx_coalesce_usecs_irq;\n\ttp->coal.tx_coalesce_usecs_irq = ec->tx_coalesce_usecs_irq;\n\ttp->coal.rx_max_coalesced_frames_irq = ec->rx_max_coalesced_frames_irq;\n\ttp->coal.tx_max_coalesced_frames_irq = ec->tx_max_coalesced_frames_irq;\n\ttp->coal.stats_block_coalesce_usecs = ec->stats_block_coalesce_usecs;\n\n\tif (netif_running(dev)) {\n\t\ttg3_full_lock(tp, 0);\n\t\t__tg3_set_coalesce(tp, &tp->coal);\n\t\ttg3_full_unlock(tp);\n\t}\n\treturn 0;\n}\n\nstatic const struct ethtool_ops tg3_ethtool_ops = {\n\t.get_settings\t\t= tg3_get_settings,\n\t.set_settings\t\t= tg3_set_settings,\n\t.get_drvinfo\t\t= tg3_get_drvinfo,\n\t.get_regs_len\t\t= tg3_get_regs_len,\n\t.get_regs\t\t= tg3_get_regs,\n\t.get_wol\t\t= tg3_get_wol,\n\t.set_wol\t\t= tg3_set_wol,\n\t.get_msglevel\t\t= tg3_get_msglevel,\n\t.set_msglevel\t\t= tg3_set_msglevel,\n\t.nway_reset\t\t= tg3_nway_reset,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_eeprom_len\t\t= tg3_get_eeprom_len,\n\t.get_eeprom\t\t= tg3_get_eeprom,\n\t.set_eeprom\t\t= tg3_set_eeprom,\n\t.get_ringparam\t\t= tg3_get_ringparam,\n\t.set_ringparam\t\t= tg3_set_ringparam,\n\t.get_pauseparam\t\t= tg3_get_pauseparam,\n\t.set_pauseparam\t\t= tg3_set_pauseparam,\n\t.self_test\t\t= tg3_self_test,\n\t.get_strings\t\t= tg3_get_strings,\n\t.set_phys_id\t\t= tg3_set_phys_id,\n\t.get_ethtool_stats\t= tg3_get_ethtool_stats,\n\t.get_coalesce\t\t= tg3_get_coalesce,\n\t.set_coalesce\t\t= tg3_set_coalesce,\n\t.get_sset_count\t\t= tg3_get_sset_count,\n\t.get_rxnfc\t\t= tg3_get_rxnfc,\n\t.get_rxfh_indir_size    = tg3_get_rxfh_indir_size,\n\t.get_rxfh_indir\t\t= tg3_get_rxfh_indir,\n\t.set_rxfh_indir\t\t= tg3_set_rxfh_indir,\n\t.get_channels\t\t= tg3_get_channels,\n\t.set_channels\t\t= tg3_set_channels,\n\t.get_ts_info\t\t= tg3_get_ts_info,\n};\n\nstatic struct rtnl_link_stats64 *tg3_get_stats64(struct net_device *dev,\n\t\t\t\t\t\tstruct rtnl_link_stats64 *stats)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tspin_lock_bh(&tp->lock);\n\tif (!tp->hw_stats) {\n\t\tspin_unlock_bh(&tp->lock);\n\t\treturn &tp->net_stats_prev;\n\t}\n\n\ttg3_get_nstats(tp, stats);\n\tspin_unlock_bh(&tp->lock);\n\n\treturn stats;\n}\n\nstatic void tg3_set_rx_mode(struct net_device *dev)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\treturn;\n\n\ttg3_full_lock(tp, 0);\n\t__tg3_set_rx_mode(dev);\n\ttg3_full_unlock(tp);\n}\n\nstatic inline void tg3_set_mtu(struct net_device *dev, struct tg3 *tp,\n\t\t\t       int new_mtu)\n{\n\tdev->mtu = new_mtu;\n\n\tif (new_mtu > ETH_DATA_LEN) {\n\t\tif (tg3_flag(tp, 5780_CLASS)) {\n\t\t\tnetdev_update_features(dev);\n\t\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\t} else {\n\t\t\ttg3_flag_set(tp, JUMBO_RING_ENABLE);\n\t\t}\n\t} else {\n\t\tif (tg3_flag(tp, 5780_CLASS)) {\n\t\t\ttg3_flag_set(tp, TSO_CAPABLE);\n\t\t\tnetdev_update_features(dev);\n\t\t}\n\t\ttg3_flag_clear(tp, JUMBO_RING_ENABLE);\n\t}\n}\n\nstatic int tg3_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err, reset_phy = 0;\n\n\tif (new_mtu < TG3_MIN_MTU || new_mtu > TG3_MAX_MTU(tp))\n\t\treturn -EINVAL;\n\n\tif (!netif_running(dev)) {\n\t\t/* We'll just catch it later when the\n\t\t * device is up'd.\n\t\t */\n\t\ttg3_set_mtu(dev, tp, new_mtu);\n\t\treturn 0;\n\t}\n\n\ttg3_phy_stop(tp);\n\n\ttg3_netif_stop(tp);\n\n\ttg3_full_lock(tp, 1);\n\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\n\ttg3_set_mtu(dev, tp, new_mtu);\n\n\t/* Reset PHY, otherwise the read DMA engine will be in a mode that\n\t * breaks all requests to 256 bytes.\n\t */\n\tif (tg3_asic_rev(tp) == ASIC_REV_57766)\n\t\treset_phy = 1;\n\n\terr = tg3_restart_hw(tp, reset_phy);\n\n\tif (!err)\n\t\ttg3_netif_start(tp);\n\n\ttg3_full_unlock(tp);\n\n\tif (!err)\n\t\ttg3_phy_start(tp);\n\n\treturn err;\n}\n\nstatic const struct net_device_ops tg3_netdev_ops = {\n\t.ndo_open\t\t= tg3_open,\n\t.ndo_stop\t\t= tg3_close,\n\t.ndo_start_xmit\t\t= tg3_start_xmit,\n\t.ndo_get_stats64\t= tg3_get_stats64,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_rx_mode\t= tg3_set_rx_mode,\n\t.ndo_set_mac_address\t= tg3_set_mac_addr,\n\t.ndo_do_ioctl\t\t= tg3_ioctl,\n\t.ndo_tx_timeout\t\t= tg3_tx_timeout,\n\t.ndo_change_mtu\t\t= tg3_change_mtu,\n\t.ndo_fix_features\t= tg3_fix_features,\n\t.ndo_set_features\t= tg3_set_features,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= tg3_poll_controller,\n#endif\n};\n\nstatic void tg3_get_eeprom_size(struct tg3 *tp)\n{\n\tu32 cursize, val, magic;\n\n\ttp->nvram_size = EEPROM_CHIP_SIZE;\n\n\tif (tg3_nvram_read(tp, 0, &magic) != 0)\n\t\treturn;\n\n\tif ((magic != TG3_EEPROM_MAGIC) &&\n\t    ((magic & TG3_EEPROM_MAGIC_FW_MSK) != TG3_EEPROM_MAGIC_FW) &&\n\t    ((magic & TG3_EEPROM_MAGIC_HW_MSK) != TG3_EEPROM_MAGIC_HW))\n\t\treturn;\n\n\t/*\n\t * Size the chip by reading offsets at increasing powers of two.\n\t * When we encounter our validation signature, we know the addressing\n\t * has wrapped around, and thus have our chip size.\n\t */\n\tcursize = 0x10;\n\n\twhile (cursize < tp->nvram_size) {\n\t\tif (tg3_nvram_read(tp, cursize, &val) != 0)\n\t\t\treturn;\n\n\t\tif (val == magic)\n\t\t\tbreak;\n\n\t\tcursize <<= 1;\n\t}\n\n\ttp->nvram_size = cursize;\n}\n\nstatic void tg3_get_nvram_size(struct tg3 *tp)\n{\n\tu32 val;\n\n\tif (tg3_flag(tp, NO_NVRAM) || tg3_nvram_read(tp, 0, &val) != 0)\n\t\treturn;\n\n\t/* Selfboot format */\n\tif (val != TG3_EEPROM_MAGIC) {\n\t\ttg3_get_eeprom_size(tp);\n\t\treturn;\n\t}\n\n\tif (tg3_nvram_read(tp, 0xf0, &val) == 0) {\n\t\tif (val != 0) {\n\t\t\t/* This is confusing.  We want to operate on the\n\t\t\t * 16-bit value at offset 0xf2.  The tg3_nvram_read()\n\t\t\t * call will read from NVRAM and byteswap the data\n\t\t\t * according to the byteswapping settings for all\n\t\t\t * other register accesses.  This ensures the data we\n\t\t\t * want will always reside in the lower 16-bits.\n\t\t\t * However, the data in NVRAM is in LE format, which\n\t\t\t * means the data from the NVRAM read will always be\n\t\t\t * opposite the endianness of the CPU.  The 16-bit\n\t\t\t * byteswap then brings the data to CPU endianness.\n\t\t\t */\n\t\t\ttp->nvram_size = swab16((u16)(val & 0x0000ffff)) * 1024;\n\t\t\treturn;\n\t\t}\n\t}\n\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n}\n\nstatic void tg3_get_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\tif (nvcfg1 & NVRAM_CFG1_FLASHIF_ENAB) {\n\t\ttg3_flag_set(tp, FLASH);\n\t} else {\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5750 ||\n\t    tg3_flag(tp, 5780_CLASS)) {\n\t\tswitch (nvcfg1 & NVRAM_CFG1_VENDOR_MASK) {\n\t\tcase FLASH_VENDOR_ATMEL_FLASH_BUFFERED:\n\t\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\t\ttp->nvram_pagesize = ATMEL_AT45DB0X1B_PAGE_SIZE;\n\t\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_ATMEL_FLASH_UNBUFFERED:\n\t\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\t\ttp->nvram_pagesize = ATMEL_AT25F512_PAGE_SIZE;\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_ATMEL_EEPROM:\n\t\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\t\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_ST:\n\t\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\t\ttp->nvram_pagesize = ST_M45PEX0_PAGE_SIZE;\n\t\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_SAIFUN:\n\t\t\ttp->nvram_jedecnum = JEDEC_SAIFUN;\n\t\t\ttp->nvram_pagesize = SAIFUN_SA25F0XX_PAGE_SIZE;\n\t\t\tbreak;\n\t\tcase FLASH_VENDOR_SST_SMALL:\n\t\tcase FLASH_VENDOR_SST_LARGE:\n\t\t\ttp->nvram_jedecnum = JEDEC_SST;\n\t\t\ttp->nvram_pagesize = SST_25VF0X0_PAGE_SIZE;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttp->nvram_pagesize = ATMEL_AT45DB0X1B_PAGE_SIZE;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t}\n}\n\nstatic void tg3_nvram_get_pagesize(struct tg3 *tp, u32 nvmcfg1)\n{\n\tswitch (nvmcfg1 & NVRAM_CFG1_5752PAGE_SIZE_MASK) {\n\tcase FLASH_5752PAGE_SIZE_256:\n\t\ttp->nvram_pagesize = 256;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_512:\n\t\ttp->nvram_pagesize = 512;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_1K:\n\t\ttp->nvram_pagesize = 1024;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_2K:\n\t\ttp->nvram_pagesize = 2048;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_4K:\n\t\ttp->nvram_pagesize = 4096;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_264:\n\t\ttp->nvram_pagesize = 264;\n\t\tbreak;\n\tcase FLASH_5752PAGE_SIZE_528:\n\t\ttp->nvram_pagesize = 528;\n\t\tbreak;\n\t}\n}\n\nstatic void tg3_get_5752_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\t/* NVRAM protection for TPM */\n\tif (nvcfg1 & (1 << 27))\n\t\ttg3_flag_set(tp, PROTECTED_NVRAM);\n\n\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\tcase FLASH_5752VENDOR_ATMEL_EEPROM_64KHZ:\n\tcase FLASH_5752VENDOR_ATMEL_EEPROM_376KHZ:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ATMEL_FLASH_BUFFERED:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ST_M45PE10:\n\tcase FLASH_5752VENDOR_ST_M45PE20:\n\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\tbreak;\n\t}\n\n\tif (tg3_flag(tp, FLASH)) {\n\t\ttg3_nvram_get_pagesize(tp, nvcfg1);\n\t} else {\n\t\t/* For eeprom, set pagesize to maximum eeprom size */\n\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t}\n}\n\nstatic void tg3_get_5755_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1, protect = 0;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\t/* NVRAM protection for TPM */\n\tif (nvcfg1 & (1 << 27)) {\n\t\ttg3_flag_set(tp, PROTECTED_NVRAM);\n\t\tprotect = 1;\n\t}\n\n\tnvcfg1 &= NVRAM_CFG1_5752VENDOR_MASK;\n\tswitch (nvcfg1) {\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_1:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_2:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_3:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_5:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 264;\n\t\tif (nvcfg1 == FLASH_5755VENDOR_ATMEL_FLASH_1 ||\n\t\t    nvcfg1 == FLASH_5755VENDOR_ATMEL_FLASH_5)\n\t\t\ttp->nvram_size = (protect ? 0x3e200 :\n\t\t\t\t\t  TG3_NVRAM_SIZE_512KB);\n\t\telse if (nvcfg1 == FLASH_5755VENDOR_ATMEL_FLASH_2)\n\t\t\ttp->nvram_size = (protect ? 0x1f200 :\n\t\t\t\t\t  TG3_NVRAM_SIZE_256KB);\n\t\telse\n\t\t\ttp->nvram_size = (protect ? 0x1f200 :\n\t\t\t\t\t  TG3_NVRAM_SIZE_128KB);\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ST_M45PE10:\n\tcase FLASH_5752VENDOR_ST_M45PE20:\n\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 256;\n\t\tif (nvcfg1 == FLASH_5752VENDOR_ST_M45PE10)\n\t\t\ttp->nvram_size = (protect ?\n\t\t\t\t\t  TG3_NVRAM_SIZE_64KB :\n\t\t\t\t\t  TG3_NVRAM_SIZE_128KB);\n\t\telse if (nvcfg1 == FLASH_5752VENDOR_ST_M45PE20)\n\t\t\ttp->nvram_size = (protect ?\n\t\t\t\t\t  TG3_NVRAM_SIZE_64KB :\n\t\t\t\t\t  TG3_NVRAM_SIZE_256KB);\n\t\telse\n\t\t\ttp->nvram_size = (protect ?\n\t\t\t\t\t  TG3_NVRAM_SIZE_128KB :\n\t\t\t\t\t  TG3_NVRAM_SIZE_512KB);\n\t\tbreak;\n\t}\n}\n\nstatic void tg3_get_5787_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\tcase FLASH_5787VENDOR_ATMEL_EEPROM_64KHZ:\n\tcase FLASH_5787VENDOR_ATMEL_EEPROM_376KHZ:\n\tcase FLASH_5787VENDOR_MICRO_EEPROM_64KHZ:\n\tcase FLASH_5787VENDOR_MICRO_EEPROM_376KHZ:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ATMEL_FLASH_BUFFERED:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_1:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_2:\n\tcase FLASH_5755VENDOR_ATMEL_FLASH_3:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 264;\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ST_M45PE10:\n\tcase FLASH_5752VENDOR_ST_M45PE20:\n\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 256;\n\t\tbreak;\n\t}\n}\n\nstatic void tg3_get_5761_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1, protect = 0;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\t/* NVRAM protection for TPM */\n\tif (nvcfg1 & (1 << 27)) {\n\t\ttg3_flag_set(tp, PROTECTED_NVRAM);\n\t\tprotect = 1;\n\t}\n\n\tnvcfg1 &= NVRAM_CFG1_5752VENDOR_MASK;\n\tswitch (nvcfg1) {\n\tcase FLASH_5761VENDOR_ATMEL_ADB021D:\n\tcase FLASH_5761VENDOR_ATMEL_ADB041D:\n\tcase FLASH_5761VENDOR_ATMEL_ADB081D:\n\tcase FLASH_5761VENDOR_ATMEL_ADB161D:\n\tcase FLASH_5761VENDOR_ATMEL_MDB021D:\n\tcase FLASH_5761VENDOR_ATMEL_MDB041D:\n\tcase FLASH_5761VENDOR_ATMEL_MDB081D:\n\tcase FLASH_5761VENDOR_ATMEL_MDB161D:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttg3_flag_set(tp, NO_NVRAM_ADDR_TRANS);\n\t\ttp->nvram_pagesize = 256;\n\t\tbreak;\n\tcase FLASH_5761VENDOR_ST_A_M45PE20:\n\tcase FLASH_5761VENDOR_ST_A_M45PE40:\n\tcase FLASH_5761VENDOR_ST_A_M45PE80:\n\tcase FLASH_5761VENDOR_ST_A_M45PE16:\n\tcase FLASH_5761VENDOR_ST_M_M45PE20:\n\tcase FLASH_5761VENDOR_ST_M_M45PE40:\n\tcase FLASH_5761VENDOR_ST_M_M45PE80:\n\tcase FLASH_5761VENDOR_ST_M_M45PE16:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\t\ttp->nvram_pagesize = 256;\n\t\tbreak;\n\t}\n\n\tif (protect) {\n\t\ttp->nvram_size = tr32(NVRAM_ADDR_LOCKOUT);\n\t} else {\n\t\tswitch (nvcfg1) {\n\t\tcase FLASH_5761VENDOR_ATMEL_ADB161D:\n\t\tcase FLASH_5761VENDOR_ATMEL_MDB161D:\n\t\tcase FLASH_5761VENDOR_ST_A_M45PE16:\n\t\tcase FLASH_5761VENDOR_ST_M_M45PE16:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_2MB;\n\t\t\tbreak;\n\t\tcase FLASH_5761VENDOR_ATMEL_ADB081D:\n\t\tcase FLASH_5761VENDOR_ATMEL_MDB081D:\n\t\tcase FLASH_5761VENDOR_ST_A_M45PE80:\n\t\tcase FLASH_5761VENDOR_ST_M_M45PE80:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_1MB;\n\t\t\tbreak;\n\t\tcase FLASH_5761VENDOR_ATMEL_ADB041D:\n\t\tcase FLASH_5761VENDOR_ATMEL_MDB041D:\n\t\tcase FLASH_5761VENDOR_ST_A_M45PE40:\n\t\tcase FLASH_5761VENDOR_ST_M_M45PE40:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\tcase FLASH_5761VENDOR_ATMEL_ADB021D:\n\t\tcase FLASH_5761VENDOR_ATMEL_MDB021D:\n\t\tcase FLASH_5761VENDOR_ST_A_M45PE20:\n\t\tcase FLASH_5761VENDOR_ST_M_M45PE20:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void tg3_get_5906_nvram_info(struct tg3 *tp)\n{\n\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n}\n\nstatic void tg3_get_57780_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\tcase FLASH_5787VENDOR_ATMEL_EEPROM_376KHZ:\n\tcase FLASH_5787VENDOR_MICRO_EEPROM_376KHZ:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t\treturn;\n\tcase FLASH_5752VENDOR_ATMEL_FLASH_BUFFERED:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB011D:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB011B:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB021D:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB021B:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB041D:\n\tcase FLASH_57780VENDOR_ATMEL_AT45DB041B:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\t\tcase FLASH_5752VENDOR_ATMEL_FLASH_BUFFERED:\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB011D:\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB011B:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB021D:\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB021B:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB041D:\n\t\tcase FLASH_57780VENDOR_ATMEL_AT45DB041B:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase FLASH_5752VENDOR_ST_M45PE10:\n\tcase FLASH_5752VENDOR_ST_M45PE20:\n\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\t\tcase FLASH_5752VENDOR_ST_M45PE10:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\tcase FLASH_5752VENDOR_ST_M45PE20:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tcase FLASH_5752VENDOR_ST_M45PE40:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\treturn;\n\t}\n\n\ttg3_nvram_get_pagesize(tp, nvcfg1);\n\tif (tp->nvram_pagesize != 264 && tp->nvram_pagesize != 528)\n\t\ttg3_flag_set(tp, NO_NVRAM_ADDR_TRANS);\n}\n\n\nstatic void tg3_get_5717_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\n\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\tcase FLASH_5717VENDOR_ATMEL_EEPROM:\n\tcase FLASH_5717VENDOR_MICRO_EEPROM:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t\treturn;\n\tcase FLASH_5717VENDOR_ATMEL_MDB011D:\n\tcase FLASH_5717VENDOR_ATMEL_ADB011B:\n\tcase FLASH_5717VENDOR_ATMEL_ADB011D:\n\tcase FLASH_5717VENDOR_ATMEL_MDB021D:\n\tcase FLASH_5717VENDOR_ATMEL_ADB021B:\n\tcase FLASH_5717VENDOR_ATMEL_ADB021D:\n\tcase FLASH_5717VENDOR_ATMEL_45USPT:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\t\tcase FLASH_5717VENDOR_ATMEL_MDB021D:\n\t\t\t/* Detect size with tg3_nvram_get_size() */\n\t\t\tbreak;\n\t\tcase FLASH_5717VENDOR_ATMEL_ADB021B:\n\t\tcase FLASH_5717VENDOR_ATMEL_ADB021D:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase FLASH_5717VENDOR_ST_M_M25PE10:\n\tcase FLASH_5717VENDOR_ST_A_M25PE10:\n\tcase FLASH_5717VENDOR_ST_M_M45PE10:\n\tcase FLASH_5717VENDOR_ST_A_M45PE10:\n\tcase FLASH_5717VENDOR_ST_M_M25PE20:\n\tcase FLASH_5717VENDOR_ST_A_M25PE20:\n\tcase FLASH_5717VENDOR_ST_M_M45PE20:\n\tcase FLASH_5717VENDOR_ST_A_M45PE20:\n\tcase FLASH_5717VENDOR_ST_25USPT:\n\tcase FLASH_5717VENDOR_ST_45USPT:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK) {\n\t\tcase FLASH_5717VENDOR_ST_M_M25PE20:\n\t\tcase FLASH_5717VENDOR_ST_M_M45PE20:\n\t\t\t/* Detect size with tg3_nvram_get_size() */\n\t\t\tbreak;\n\t\tcase FLASH_5717VENDOR_ST_A_M25PE20:\n\t\tcase FLASH_5717VENDOR_ST_A_M45PE20:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\treturn;\n\t}\n\n\ttg3_nvram_get_pagesize(tp, nvcfg1);\n\tif (tp->nvram_pagesize != 264 && tp->nvram_pagesize != 528)\n\t\ttg3_flag_set(tp, NO_NVRAM_ADDR_TRANS);\n}\n\nstatic void tg3_get_5720_nvram_info(struct tg3 *tp)\n{\n\tu32 nvcfg1, nvmpinstrp;\n\n\tnvcfg1 = tr32(NVRAM_CFG1);\n\tnvmpinstrp = nvcfg1 & NVRAM_CFG1_5752VENDOR_MASK;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\tif (!(nvcfg1 & NVRAM_CFG1_5762VENDOR_MASK)) {\n\t\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\t\treturn;\n\t\t}\n\n\t\tswitch (nvmpinstrp) {\n\t\tcase FLASH_5762_EEPROM_HD:\n\t\t\tnvmpinstrp = FLASH_5720_EEPROM_HD;\n\t\t\tbreak;\n\t\tcase FLASH_5762_EEPROM_LD:\n\t\t\tnvmpinstrp = FLASH_5720_EEPROM_LD;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tswitch (nvmpinstrp) {\n\tcase FLASH_5720_EEPROM_HD:\n\tcase FLASH_5720_EEPROM_LD:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\n\t\tnvcfg1 &= ~NVRAM_CFG1_COMPAT_BYPASS;\n\t\ttw32(NVRAM_CFG1, nvcfg1);\n\t\tif (nvmpinstrp == FLASH_5720_EEPROM_HD)\n\t\t\ttp->nvram_pagesize = ATMEL_AT24C512_CHIP_SIZE;\n\t\telse\n\t\t\ttp->nvram_pagesize = ATMEL_AT24C02_CHIP_SIZE;\n\t\treturn;\n\tcase FLASH_5720VENDOR_M_ATMEL_DB011D:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB011B:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB011D:\n\tcase FLASH_5720VENDOR_M_ATMEL_DB021D:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB021B:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB021D:\n\tcase FLASH_5720VENDOR_M_ATMEL_DB041D:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB041B:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB041D:\n\tcase FLASH_5720VENDOR_M_ATMEL_DB081D:\n\tcase FLASH_5720VENDOR_A_ATMEL_DB081D:\n\tcase FLASH_5720VENDOR_ATMEL_45USPT:\n\t\ttp->nvram_jedecnum = JEDEC_ATMEL;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvmpinstrp) {\n\t\tcase FLASH_5720VENDOR_M_ATMEL_DB021D:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB021B:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB021D:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tcase FLASH_5720VENDOR_M_ATMEL_DB041D:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB041B:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB041D:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\tcase FLASH_5720VENDOR_M_ATMEL_DB081D:\n\t\tcase FLASH_5720VENDOR_A_ATMEL_DB081D:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_1MB;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (tg3_asic_rev(tp) != ASIC_REV_5762)\n\t\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase FLASH_5720VENDOR_M_ST_M25PE10:\n\tcase FLASH_5720VENDOR_M_ST_M45PE10:\n\tcase FLASH_5720VENDOR_A_ST_M25PE10:\n\tcase FLASH_5720VENDOR_A_ST_M45PE10:\n\tcase FLASH_5720VENDOR_M_ST_M25PE20:\n\tcase FLASH_5720VENDOR_M_ST_M45PE20:\n\tcase FLASH_5720VENDOR_A_ST_M25PE20:\n\tcase FLASH_5720VENDOR_A_ST_M45PE20:\n\tcase FLASH_5720VENDOR_M_ST_M25PE40:\n\tcase FLASH_5720VENDOR_M_ST_M45PE40:\n\tcase FLASH_5720VENDOR_A_ST_M25PE40:\n\tcase FLASH_5720VENDOR_A_ST_M45PE40:\n\tcase FLASH_5720VENDOR_M_ST_M25PE80:\n\tcase FLASH_5720VENDOR_M_ST_M45PE80:\n\tcase FLASH_5720VENDOR_A_ST_M25PE80:\n\tcase FLASH_5720VENDOR_A_ST_M45PE80:\n\tcase FLASH_5720VENDOR_ST_25USPT:\n\tcase FLASH_5720VENDOR_ST_45USPT:\n\t\ttp->nvram_jedecnum = JEDEC_ST;\n\t\ttg3_flag_set(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, FLASH);\n\n\t\tswitch (nvmpinstrp) {\n\t\tcase FLASH_5720VENDOR_M_ST_M25PE20:\n\t\tcase FLASH_5720VENDOR_M_ST_M45PE20:\n\t\tcase FLASH_5720VENDOR_A_ST_M25PE20:\n\t\tcase FLASH_5720VENDOR_A_ST_M45PE20:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_256KB;\n\t\t\tbreak;\n\t\tcase FLASH_5720VENDOR_M_ST_M25PE40:\n\t\tcase FLASH_5720VENDOR_M_ST_M45PE40:\n\t\tcase FLASH_5720VENDOR_A_ST_M25PE40:\n\t\tcase FLASH_5720VENDOR_A_ST_M45PE40:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_512KB;\n\t\t\tbreak;\n\t\tcase FLASH_5720VENDOR_M_ST_M25PE80:\n\t\tcase FLASH_5720VENDOR_M_ST_M45PE80:\n\t\tcase FLASH_5720VENDOR_A_ST_M25PE80:\n\t\tcase FLASH_5720VENDOR_A_ST_M45PE80:\n\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_1MB;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (tg3_asic_rev(tp) != ASIC_REV_5762)\n\t\t\t\ttp->nvram_size = TG3_NVRAM_SIZE_128KB;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\treturn;\n\t}\n\n\ttg3_nvram_get_pagesize(tp, nvcfg1);\n\tif (tp->nvram_pagesize != 264 && tp->nvram_pagesize != 528)\n\t\ttg3_flag_set(tp, NO_NVRAM_ADDR_TRANS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5762) {\n\t\tu32 val;\n\n\t\tif (tg3_nvram_read(tp, 0, &val))\n\t\t\treturn;\n\n\t\tif (val != TG3_EEPROM_MAGIC &&\n\t\t    (val & TG3_EEPROM_MAGIC_FW_MSK) != TG3_EEPROM_MAGIC_FW)\n\t\t\ttg3_flag_set(tp, NO_NVRAM);\n\t}\n}\n\n/* Chips other than 5700/5701 use the NVRAM for fetching info. */\nstatic void tg3_nvram_init(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, IS_SSB_CORE)) {\n\t\t/* No NVRAM and EEPROM on the SSB Broadcom GigE core. */\n\t\ttg3_flag_clear(tp, NVRAM);\n\t\ttg3_flag_clear(tp, NVRAM_BUFFERED);\n\t\ttg3_flag_set(tp, NO_NVRAM);\n\t\treturn;\n\t}\n\n\ttw32_f(GRC_EEPROM_ADDR,\n\t     (EEPROM_ADDR_FSM_RESET |\n\t      (EEPROM_DEFAULT_CLOCK_PERIOD <<\n\t       EEPROM_ADDR_CLKPERD_SHIFT)));\n\n\tmsleep(1);\n\n\t/* Enable seeprom accesses. */\n\ttw32_f(GRC_LOCAL_CTRL,\n\t     tr32(GRC_LOCAL_CTRL) | GRC_LCLCTRL_AUTO_SEEPROM);\n\tudelay(100);\n\n\tif (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5701) {\n\t\ttg3_flag_set(tp, NVRAM);\n\n\t\tif (tg3_nvram_lock(tp)) {\n\t\t\tnetdev_warn(tp->dev,\n\t\t\t\t    \"Cannot get nvram lock, %s failed\\n\",\n\t\t\t\t    __func__);\n\t\t\treturn;\n\t\t}\n\t\ttg3_enable_nvram_access(tp);\n\n\t\ttp->nvram_size = 0;\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5752)\n\t\t\ttg3_get_5752_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5755)\n\t\t\ttg3_get_5755_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5787 ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\t\ttg3_get_5787_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5761)\n\t\t\ttg3_get_5761_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\t\ttg3_get_5906_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_57780 ||\n\t\t\t tg3_flag(tp, 57765_CLASS))\n\t\t\ttg3_get_57780_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\t\ttg3_get_5717_nvram_info(tp);\n\t\telse if (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t\t\t tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\t\ttg3_get_5720_nvram_info(tp);\n\t\telse\n\t\t\ttg3_get_nvram_info(tp);\n\n\t\tif (tp->nvram_size == 0)\n\t\t\ttg3_get_nvram_size(tp);\n\n\t\ttg3_disable_nvram_access(tp);\n\t\ttg3_nvram_unlock(tp);\n\n\t} else {\n\t\ttg3_flag_clear(tp, NVRAM);\n\t\ttg3_flag_clear(tp, NVRAM_BUFFERED);\n\n\t\ttg3_get_eeprom_size(tp);\n\t}\n}\n\nstruct subsys_tbl_ent {\n\tu16 subsys_vendor, subsys_devid;\n\tu32 phy_id;\n};\n\nstatic struct subsys_tbl_ent subsys_id_to_phy_id[] = {\n\t/* Broadcom boards. */\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95700A6, TG3_PHY_ID_BCM5401 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701A5, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95700T6, TG3_PHY_ID_BCM8002 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95700A9, 0 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701T1, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701T8, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701A7, 0 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701A10, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95701A12, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95703AX1, TG3_PHY_ID_BCM5703 },\n\t{ TG3PCI_SUBVENDOR_ID_BROADCOM,\n\t  TG3PCI_SUBDEVICE_ID_BROADCOM_95703AX2, TG3_PHY_ID_BCM5703 },\n\n\t/* 3com boards. */\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C996T, TG3_PHY_ID_BCM5401 },\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C996BT, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C996SX, 0 },\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C1000T, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_3COM,\n\t  TG3PCI_SUBDEVICE_ID_3COM_3C940BR01, TG3_PHY_ID_BCM5701 },\n\n\t/* DELL boards. */\n\t{ TG3PCI_SUBVENDOR_ID_DELL,\n\t  TG3PCI_SUBDEVICE_ID_DELL_VIPER, TG3_PHY_ID_BCM5401 },\n\t{ TG3PCI_SUBVENDOR_ID_DELL,\n\t  TG3PCI_SUBDEVICE_ID_DELL_JAGUAR, TG3_PHY_ID_BCM5401 },\n\t{ TG3PCI_SUBVENDOR_ID_DELL,\n\t  TG3PCI_SUBDEVICE_ID_DELL_MERLOT, TG3_PHY_ID_BCM5411 },\n\t{ TG3PCI_SUBVENDOR_ID_DELL,\n\t  TG3PCI_SUBDEVICE_ID_DELL_SLIM_MERLOT, TG3_PHY_ID_BCM5411 },\n\n\t/* Compaq boards. */\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_BANSHEE, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_BANSHEE_2, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_CHANGELING, 0 },\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_NC7780, TG3_PHY_ID_BCM5701 },\n\t{ TG3PCI_SUBVENDOR_ID_COMPAQ,\n\t  TG3PCI_SUBDEVICE_ID_COMPAQ_NC7780_2, TG3_PHY_ID_BCM5701 },\n\n\t/* IBM boards. */\n\t{ TG3PCI_SUBVENDOR_ID_IBM,\n\t  TG3PCI_SUBDEVICE_ID_IBM_5703SAX2, 0 }\n};\n\nstatic struct subsys_tbl_ent *tg3_lookup_by_subsys(struct tg3 *tp)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(subsys_id_to_phy_id); i++) {\n\t\tif ((subsys_id_to_phy_id[i].subsys_vendor ==\n\t\t     tp->pdev->subsystem_vendor) &&\n\t\t    (subsys_id_to_phy_id[i].subsys_devid ==\n\t\t     tp->pdev->subsystem_device))\n\t\t\treturn &subsys_id_to_phy_id[i];\n\t}\n\treturn NULL;\n}\n\nstatic void tg3_get_eeprom_hw_cfg(struct tg3 *tp)\n{\n\tu32 val;\n\n\ttp->phy_id = TG3_PHY_ID_INVALID;\n\ttp->led_ctrl = LED_CTRL_MODE_PHY_1;\n\n\t/* Assume an onboard device and WOL capable by default.  */\n\ttg3_flag_set(tp, EEPROM_WRITE_PROT);\n\ttg3_flag_set(tp, WOL_CAP);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tif (!(tr32(PCIE_TRANSACTION_CFG) & PCIE_TRANS_CFG_LOM)) {\n\t\t\ttg3_flag_clear(tp, EEPROM_WRITE_PROT);\n\t\t\ttg3_flag_set(tp, IS_NIC);\n\t\t}\n\t\tval = tr32(VCPU_CFGSHDW);\n\t\tif (val & VCPU_CFGSHDW_ASPM_DBNC)\n\t\t\ttg3_flag_set(tp, ASPM_WORKAROUND);\n\t\tif ((val & VCPU_CFGSHDW_WOL_ENABLE) &&\n\t\t    (val & VCPU_CFGSHDW_WOL_MAGPKT)) {\n\t\t\ttg3_flag_set(tp, WOL_ENABLE);\n\t\t\tdevice_set_wakeup_enable(&tp->pdev->dev, true);\n\t\t}\n\t\tgoto done;\n\t}\n\n\ttg3_read_mem(tp, NIC_SRAM_DATA_SIG, &val);\n\tif (val == NIC_SRAM_DATA_SIG_MAGIC) {\n\t\tu32 nic_cfg, led_cfg;\n\t\tu32 nic_phy_id, ver, cfg2 = 0, cfg4 = 0, eeprom_phy_id;\n\t\tint eeprom_phy_serdes = 0;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG, &nic_cfg);\n\t\ttp->nic_sram_data_cfg = nic_cfg;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_DATA_VER, &ver);\n\t\tver >>= NIC_SRAM_DATA_VER_SHIFT;\n\t\tif (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5701 &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5703 &&\n\t\t    (ver > 0) && (ver < 0x100))\n\t\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG_2, &cfg2);\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5785)\n\t\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG_4, &cfg4);\n\n\t\tif ((nic_cfg & NIC_SRAM_DATA_CFG_PHY_TYPE_MASK) ==\n\t\t    NIC_SRAM_DATA_CFG_PHY_TYPE_FIBER)\n\t\t\teeprom_phy_serdes = 1;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_DATA_PHY_ID, &nic_phy_id);\n\t\tif (nic_phy_id != 0) {\n\t\t\tu32 id1 = nic_phy_id & NIC_SRAM_DATA_PHY_ID1_MASK;\n\t\t\tu32 id2 = nic_phy_id & NIC_SRAM_DATA_PHY_ID2_MASK;\n\n\t\t\teeprom_phy_id  = (id1 >> 16) << 10;\n\t\t\teeprom_phy_id |= (id2 & 0xfc00) << 16;\n\t\t\teeprom_phy_id |= (id2 & 0x03ff) <<  0;\n\t\t} else\n\t\t\teeprom_phy_id = 0;\n\n\t\ttp->phy_id = eeprom_phy_id;\n\t\tif (eeprom_phy_serdes) {\n\t\t\tif (!tg3_flag(tp, 5705_PLUS))\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_PHY_SERDES;\n\t\t\telse\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_MII_SERDES;\n\t\t}\n\n\t\tif (tg3_flag(tp, 5750_PLUS))\n\t\t\tled_cfg = cfg2 & (NIC_SRAM_DATA_CFG_LED_MODE_MASK |\n\t\t\t\t    SHASTA_EXT_LED_MODE_MASK);\n\t\telse\n\t\t\tled_cfg = nic_cfg & NIC_SRAM_DATA_CFG_LED_MODE_MASK;\n\n\t\tswitch (led_cfg) {\n\t\tdefault:\n\t\tcase NIC_SRAM_DATA_CFG_LED_MODE_PHY_1:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_1;\n\t\t\tbreak;\n\n\t\tcase NIC_SRAM_DATA_CFG_LED_MODE_PHY_2:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_2;\n\t\t\tbreak;\n\n\t\tcase NIC_SRAM_DATA_CFG_LED_MODE_MAC:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_MAC;\n\n\t\t\t/* Default to PHY_1_MODE if 0 (MAC_MODE) is\n\t\t\t * read on some older 5700/5701 bootcode.\n\t\t\t */\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5701)\n\t\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_1;\n\n\t\t\tbreak;\n\n\t\tcase SHASTA_EXT_LED_SHARED:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_SHARED;\n\t\t\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A0 &&\n\t\t\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A1)\n\t\t\t\ttp->led_ctrl |= (LED_CTRL_MODE_PHY_1 |\n\t\t\t\t\t\t LED_CTRL_MODE_PHY_2);\n\t\t\tbreak;\n\n\t\tcase SHASTA_EXT_LED_MAC:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_SHASTA_MAC;\n\t\t\tbreak;\n\n\t\tcase SHASTA_EXT_LED_COMBO:\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_COMBO;\n\t\t\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5750_A0)\n\t\t\t\ttp->led_ctrl |= (LED_CTRL_MODE_PHY_1 |\n\t\t\t\t\t\t LED_CTRL_MODE_PHY_2);\n\t\t\tbreak;\n\n\t\t}\n\n\t\tif ((tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t\t     tg3_asic_rev(tp) == ASIC_REV_5701) &&\n\t\t    tp->pdev->subsystem_vendor == PCI_VENDOR_ID_DELL)\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_2;\n\n\t\tif (tg3_chip_rev(tp) == CHIPREV_5784_AX)\n\t\t\ttp->led_ctrl = LED_CTRL_MODE_PHY_1;\n\n\t\tif (nic_cfg & NIC_SRAM_DATA_CFG_EEPROM_WP) {\n\t\t\ttg3_flag_set(tp, EEPROM_WRITE_PROT);\n\t\t\tif ((tp->pdev->subsystem_vendor ==\n\t\t\t     PCI_VENDOR_ID_ARIMA) &&\n\t\t\t    (tp->pdev->subsystem_device == 0x205a ||\n\t\t\t     tp->pdev->subsystem_device == 0x2063))\n\t\t\t\ttg3_flag_clear(tp, EEPROM_WRITE_PROT);\n\t\t} else {\n\t\t\ttg3_flag_clear(tp, EEPROM_WRITE_PROT);\n\t\t\ttg3_flag_set(tp, IS_NIC);\n\t\t}\n\n\t\tif (nic_cfg & NIC_SRAM_DATA_CFG_ASF_ENABLE) {\n\t\t\ttg3_flag_set(tp, ENABLE_ASF);\n\t\t\tif (tg3_flag(tp, 5750_PLUS))\n\t\t\t\ttg3_flag_set(tp, ASF_NEW_HANDSHAKE);\n\t\t}\n\n\t\tif ((nic_cfg & NIC_SRAM_DATA_CFG_APE_ENABLE) &&\n\t\t    tg3_flag(tp, 5750_PLUS))\n\t\t\ttg3_flag_set(tp, ENABLE_APE);\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_ANY_SERDES &&\n\t\t    !(nic_cfg & NIC_SRAM_DATA_CFG_FIBER_WOL))\n\t\t\ttg3_flag_clear(tp, WOL_CAP);\n\n\t\tif (tg3_flag(tp, WOL_CAP) &&\n\t\t    (nic_cfg & NIC_SRAM_DATA_CFG_WOL_ENABLE)) {\n\t\t\ttg3_flag_set(tp, WOL_ENABLE);\n\t\t\tdevice_set_wakeup_enable(&tp->pdev->dev, true);\n\t\t}\n\n\t\tif (cfg2 & (1 << 17))\n\t\t\ttp->phy_flags |= TG3_PHYFLG_CAPACITIVE_COUPLING;\n\n\t\t/* serdes signal pre-emphasis in register 0x590 set by */\n\t\t/* bootcode if bit 18 is set */\n\t\tif (cfg2 & (1 << 18))\n\t\t\ttp->phy_flags |= TG3_PHYFLG_SERDES_PREEMPHASIS;\n\n\t\tif ((tg3_flag(tp, 57765_PLUS) ||\n\t\t     (tg3_asic_rev(tp) == ASIC_REV_5784 &&\n\t\t      tg3_chip_rev(tp) != CHIPREV_5784_AX)) &&\n\t\t    (cfg2 & NIC_SRAM_DATA_CFG_2_APD_EN))\n\t\t\ttp->phy_flags |= TG3_PHYFLG_ENABLE_APD;\n\n\t\tif (tg3_flag(tp, PCI_EXPRESS) &&\n\t\t    tg3_asic_rev(tp) != ASIC_REV_5785 &&\n\t\t    !tg3_flag(tp, 57765_PLUS)) {\n\t\t\tu32 cfg3;\n\n\t\t\ttg3_read_mem(tp, NIC_SRAM_DATA_CFG_3, &cfg3);\n\t\t\tif (cfg3 & NIC_SRAM_ASPM_DEBOUNCE)\n\t\t\t\ttg3_flag_set(tp, ASPM_WORKAROUND);\n\t\t}\n\n\t\tif (cfg4 & NIC_SRAM_RGMII_INBAND_DISABLE)\n\t\t\ttg3_flag_set(tp, RGMII_INBAND_DISABLE);\n\t\tif (cfg4 & NIC_SRAM_RGMII_EXT_IBND_RX_EN)\n\t\t\ttg3_flag_set(tp, RGMII_EXT_IBND_RX_EN);\n\t\tif (cfg4 & NIC_SRAM_RGMII_EXT_IBND_TX_EN)\n\t\t\ttg3_flag_set(tp, RGMII_EXT_IBND_TX_EN);\n\t}\ndone:\n\tif (tg3_flag(tp, WOL_CAP))\n\t\tdevice_set_wakeup_enable(&tp->pdev->dev,\n\t\t\t\t\t tg3_flag(tp, WOL_ENABLE));\n\telse\n\t\tdevice_set_wakeup_capable(&tp->pdev->dev, false);\n}\n\nstatic int tg3_ape_otp_read(struct tg3 *tp, u32 offset, u32 *val)\n{\n\tint i, err;\n\tu32 val2, off = offset * 8;\n\n\terr = tg3_nvram_lock(tp);\n\tif (err)\n\t\treturn err;\n\n\ttg3_ape_write32(tp, TG3_APE_OTP_ADDR, off | APE_OTP_ADDR_CPU_ENABLE);\n\ttg3_ape_write32(tp, TG3_APE_OTP_CTRL, APE_OTP_CTRL_PROG_EN |\n\t\t\tAPE_OTP_CTRL_CMD_RD | APE_OTP_CTRL_START);\n\ttg3_ape_read32(tp, TG3_APE_OTP_CTRL);\n\tudelay(10);\n\n\tfor (i = 0; i < 100; i++) {\n\t\tval2 = tg3_ape_read32(tp, TG3_APE_OTP_STATUS);\n\t\tif (val2 & APE_OTP_STATUS_CMD_DONE) {\n\t\t\t*val = tg3_ape_read32(tp, TG3_APE_OTP_RD_DATA);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(10);\n\t}\n\n\ttg3_ape_write32(tp, TG3_APE_OTP_CTRL, 0);\n\n\ttg3_nvram_unlock(tp);\n\tif (val2 & APE_OTP_STATUS_CMD_DONE)\n\t\treturn 0;\n\n\treturn -EBUSY;\n}\n\nstatic int tg3_issue_otp_command(struct tg3 *tp, u32 cmd)\n{\n\tint i;\n\tu32 val;\n\n\ttw32(OTP_CTRL, cmd | OTP_CTRL_OTP_CMD_START);\n\ttw32(OTP_CTRL, cmd);\n\n\t/* Wait for up to 1 ms for command to execute. */\n\tfor (i = 0; i < 100; i++) {\n\t\tval = tr32(OTP_STATUS);\n\t\tif (val & OTP_STATUS_CMD_DONE)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\treturn (val & OTP_STATUS_CMD_DONE) ? 0 : -EBUSY;\n}\n\n/* Read the gphy configuration from the OTP region of the chip.  The gphy\n * configuration is a 32-bit value that straddles the alignment boundary.\n * We do two 32-bit reads and then shift and merge the results.\n */\nstatic u32 tg3_read_otp_phycfg(struct tg3 *tp)\n{\n\tu32 bhalf_otp, thalf_otp;\n\n\ttw32(OTP_MODE, OTP_MODE_OTP_THRU_GRC);\n\n\tif (tg3_issue_otp_command(tp, OTP_CTRL_OTP_CMD_INIT))\n\t\treturn 0;\n\n\ttw32(OTP_ADDRESS, OTP_ADDRESS_MAGIC1);\n\n\tif (tg3_issue_otp_command(tp, OTP_CTRL_OTP_CMD_READ))\n\t\treturn 0;\n\n\tthalf_otp = tr32(OTP_READ_DATA);\n\n\ttw32(OTP_ADDRESS, OTP_ADDRESS_MAGIC2);\n\n\tif (tg3_issue_otp_command(tp, OTP_CTRL_OTP_CMD_READ))\n\t\treturn 0;\n\n\tbhalf_otp = tr32(OTP_READ_DATA);\n\n\treturn ((thalf_otp & 0x0000ffff) << 16) | (bhalf_otp >> 16);\n}\n\nstatic void tg3_phy_init_link_config(struct tg3 *tp)\n{\n\tu32 adv = ADVERTISED_Autoneg;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_10_100_ONLY))\n\t\tadv |= ADVERTISED_1000baseT_Half |\n\t\t       ADVERTISED_1000baseT_Full;\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\tadv |= ADVERTISED_100baseT_Half |\n\t\t       ADVERTISED_100baseT_Full |\n\t\t       ADVERTISED_10baseT_Half |\n\t\t       ADVERTISED_10baseT_Full |\n\t\t       ADVERTISED_TP;\n\telse\n\t\tadv |= ADVERTISED_FIBRE;\n\n\ttp->link_config.advertising = adv;\n\ttp->link_config.speed = SPEED_UNKNOWN;\n\ttp->link_config.duplex = DUPLEX_UNKNOWN;\n\ttp->link_config.autoneg = AUTONEG_ENABLE;\n\ttp->link_config.active_speed = SPEED_UNKNOWN;\n\ttp->link_config.active_duplex = DUPLEX_UNKNOWN;\n\n\ttp->old_link = -1;\n}\n\nstatic int tg3_phy_probe(struct tg3 *tp)\n{\n\tu32 hw_phy_id_1, hw_phy_id_2;\n\tu32 hw_phy_id, hw_phy_id_masked;\n\tint err;\n\n\t/* flow control autonegotiation is default behavior */\n\ttg3_flag_set(tp, PAUSE_AUTONEG);\n\ttp->link_config.flowctrl = FLOW_CTRL_TX | FLOW_CTRL_RX;\n\n\tif (tg3_flag(tp, ENABLE_APE)) {\n\t\tswitch (tp->pci_fn) {\n\t\tcase 0:\n\t\t\ttp->phy_ape_lock = TG3_APE_LOCK_PHY0;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\ttp->phy_ape_lock = TG3_APE_LOCK_PHY1;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\ttp->phy_ape_lock = TG3_APE_LOCK_PHY2;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\ttp->phy_ape_lock = TG3_APE_LOCK_PHY3;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (tg3_flag(tp, USE_PHYLIB))\n\t\treturn tg3_phy_init(tp);\n\n\t/* Reading the PHY ID register can conflict with ASF\n\t * firmware access to the PHY hardware.\n\t */\n\terr = 0;\n\tif (tg3_flag(tp, ENABLE_ASF) || tg3_flag(tp, ENABLE_APE)) {\n\t\thw_phy_id = hw_phy_id_masked = TG3_PHY_ID_INVALID;\n\t} else {\n\t\t/* Now read the physical PHY_ID from the chip and verify\n\t\t * that it is sane.  If it doesn't look good, we fall back\n\t\t * to either the hard-coded table based PHY_ID and failing\n\t\t * that the value found in the eeprom area.\n\t\t */\n\t\terr |= tg3_readphy(tp, MII_PHYSID1, &hw_phy_id_1);\n\t\terr |= tg3_readphy(tp, MII_PHYSID2, &hw_phy_id_2);\n\n\t\thw_phy_id  = (hw_phy_id_1 & 0xffff) << 10;\n\t\thw_phy_id |= (hw_phy_id_2 & 0xfc00) << 16;\n\t\thw_phy_id |= (hw_phy_id_2 & 0x03ff) <<  0;\n\n\t\thw_phy_id_masked = hw_phy_id & TG3_PHY_ID_MASK;\n\t}\n\n\tif (!err && TG3_KNOWN_PHY_ID(hw_phy_id_masked)) {\n\t\ttp->phy_id = hw_phy_id;\n\t\tif (hw_phy_id_masked == TG3_PHY_ID_BCM8002)\n\t\t\ttp->phy_flags |= TG3_PHYFLG_PHY_SERDES;\n\t\telse\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_PHY_SERDES;\n\t} else {\n\t\tif (tp->phy_id != TG3_PHY_ID_INVALID) {\n\t\t\t/* Do nothing, phy ID already set up in\n\t\t\t * tg3_get_eeprom_hw_cfg().\n\t\t\t */\n\t\t} else {\n\t\t\tstruct subsys_tbl_ent *p;\n\n\t\t\t/* No eeprom signature?  Try the hardcoded\n\t\t\t * subsys device table.\n\t\t\t */\n\t\t\tp = tg3_lookup_by_subsys(tp);\n\t\t\tif (p) {\n\t\t\t\ttp->phy_id = p->phy_id;\n\t\t\t} else if (!tg3_flag(tp, IS_SSB_CORE)) {\n\t\t\t\t/* For now we saw the IDs 0xbc050cd0,\n\t\t\t\t * 0xbc050f80 and 0xbc050c30 on devices\n\t\t\t\t * connected to an BCM4785 and there are\n\t\t\t\t * probably more. Just assume that the phy is\n\t\t\t\t * supported when it is connected to a SSB core\n\t\t\t\t * for now.\n\t\t\t\t */\n\t\t\t\treturn -ENODEV;\n\t\t\t}\n\n\t\t\tif (!tp->phy_id ||\n\t\t\t    tp->phy_id == TG3_PHY_ID_BCM8002)\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_PHY_SERDES;\n\t\t}\n\t}\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES) &&\n\t    (tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5762 ||\n\t     (tg3_asic_rev(tp) == ASIC_REV_5717 &&\n\t      tg3_chip_rev_id(tp) != CHIPREV_ID_5717_A0) ||\n\t     (tg3_asic_rev(tp) == ASIC_REV_57765 &&\n\t      tg3_chip_rev_id(tp) != CHIPREV_ID_57765_A0)))\n\t\ttp->phy_flags |= TG3_PHYFLG_EEE_CAP;\n\n\ttg3_phy_init_link_config(tp);\n\n\tif (!(tp->phy_flags & TG3_PHYFLG_ANY_SERDES) &&\n\t    !tg3_flag(tp, ENABLE_APE) &&\n\t    !tg3_flag(tp, ENABLE_ASF)) {\n\t\tu32 bmsr, dummy;\n\n\t\ttg3_readphy(tp, MII_BMSR, &bmsr);\n\t\tif (!tg3_readphy(tp, MII_BMSR, &bmsr) &&\n\t\t    (bmsr & BMSR_LSTATUS))\n\t\t\tgoto skip_phy_reset;\n\n\t\terr = tg3_phy_reset(tp);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\ttg3_phy_set_wirespeed(tp);\n\n\t\tif (!tg3_phy_copper_an_config_ok(tp, &dummy)) {\n\t\t\ttg3_phy_autoneg_cfg(tp, tp->link_config.advertising,\n\t\t\t\t\t    tp->link_config.flowctrl);\n\n\t\t\ttg3_writephy(tp, MII_BMCR,\n\t\t\t\t     BMCR_ANENABLE | BMCR_ANRESTART);\n\t\t}\n\t}\n\nskip_phy_reset:\n\tif ((tp->phy_id & TG3_PHY_ID_MASK) == TG3_PHY_ID_BCM5401) {\n\t\terr = tg3_init_5401phy_dsp(tp);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = tg3_init_5401phy_dsp(tp);\n\t}\n\n\treturn err;\n}\n\nstatic void tg3_read_vpd(struct tg3 *tp)\n{\n\tu8 *vpd_data;\n\tunsigned int block_end, rosize, len;\n\tu32 vpdlen;\n\tint j, i = 0;\n\n\tvpd_data = (u8 *)tg3_vpd_readblock(tp, &vpdlen);\n\tif (!vpd_data)\n\t\tgoto out_no_vpd;\n\n\ti = pci_vpd_find_tag(vpd_data, 0, vpdlen, PCI_VPD_LRDT_RO_DATA);\n\tif (i < 0)\n\t\tgoto out_not_found;\n\n\trosize = pci_vpd_lrdt_size(&vpd_data[i]);\n\tblock_end = i + PCI_VPD_LRDT_TAG_SIZE + rosize;\n\ti += PCI_VPD_LRDT_TAG_SIZE;\n\n\tif (block_end > vpdlen)\n\t\tgoto out_not_found;\n\n\tj = pci_vpd_find_info_keyword(vpd_data, i, rosize,\n\t\t\t\t      PCI_VPD_RO_KEYWORD_MFR_ID);\n\tif (j > 0) {\n\t\tlen = pci_vpd_info_field_size(&vpd_data[j]);\n\n\t\tj += PCI_VPD_INFO_FLD_HDR_SIZE;\n\t\tif (j + len > block_end || len != 4 ||\n\t\t    memcmp(&vpd_data[j], \"1028\", 4))\n\t\t\tgoto partno;\n\n\t\tj = pci_vpd_find_info_keyword(vpd_data, i, rosize,\n\t\t\t\t\t      PCI_VPD_RO_KEYWORD_VENDOR0);\n\t\tif (j < 0)\n\t\t\tgoto partno;\n\n\t\tlen = pci_vpd_info_field_size(&vpd_data[j]);\n\n\t\tj += PCI_VPD_INFO_FLD_HDR_SIZE;\n\t\tif (j + len > block_end)\n\t\t\tgoto partno;\n\n\t\tif (len >= sizeof(tp->fw_ver))\n\t\t\tlen = sizeof(tp->fw_ver) - 1;\n\t\tmemset(tp->fw_ver, 0, sizeof(tp->fw_ver));\n\t\tsnprintf(tp->fw_ver, sizeof(tp->fw_ver), \"%.*s bc \", len,\n\t\t\t &vpd_data[j]);\n\t}\n\npartno:\n\ti = pci_vpd_find_info_keyword(vpd_data, i, rosize,\n\t\t\t\t      PCI_VPD_RO_KEYWORD_PARTNO);\n\tif (i < 0)\n\t\tgoto out_not_found;\n\n\tlen = pci_vpd_info_field_size(&vpd_data[i]);\n\n\ti += PCI_VPD_INFO_FLD_HDR_SIZE;\n\tif (len > TG3_BPN_SIZE ||\n\t    (len + i) > vpdlen)\n\t\tgoto out_not_found;\n\n\tmemcpy(tp->board_part_number, &vpd_data[i], len);\n\nout_not_found:\n\tkfree(vpd_data);\n\tif (tp->board_part_number[0])\n\t\treturn;\n\nout_no_vpd:\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717) {\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717_C)\n\t\t\tstrcpy(tp->board_part_number, \"BCM5717\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_5718)\n\t\t\tstrcpy(tp->board_part_number, \"BCM5718\");\n\t\telse\n\t\t\tgoto nomatch;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_57780) {\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57780)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57780\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57760)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57760\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57790)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57790\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57788)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57788\");\n\t\telse\n\t\t\tgoto nomatch;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_57765) {\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57761)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57761\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57765)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57765\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57781)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57781\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57785)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57785\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57791)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57791\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57795)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57795\");\n\t\telse\n\t\t\tgoto nomatch;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_57766) {\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57762)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57762\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57766)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57766\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57782)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57782\");\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57786)\n\t\t\tstrcpy(tp->board_part_number, \"BCM57786\");\n\t\telse\n\t\t\tgoto nomatch;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\tstrcpy(tp->board_part_number, \"BCM95906\");\n\t} else {\nnomatch:\n\t\tstrcpy(tp->board_part_number, \"none\");\n\t}\n}\n\nstatic int tg3_fw_img_is_valid(struct tg3 *tp, u32 offset)\n{\n\tu32 val;\n\n\tif (tg3_nvram_read(tp, offset, &val) ||\n\t    (val & 0xfc000000) != 0x0c000000 ||\n\t    tg3_nvram_read(tp, offset + 4, &val) ||\n\t    val != 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic void tg3_read_bc_ver(struct tg3 *tp)\n{\n\tu32 val, offset, start, ver_offset;\n\tint i, dst_off;\n\tbool newver = false;\n\n\tif (tg3_nvram_read(tp, 0xc, &offset) ||\n\t    tg3_nvram_read(tp, 0x4, &start))\n\t\treturn;\n\n\toffset = tg3_nvram_logical_addr(tp, offset);\n\n\tif (tg3_nvram_read(tp, offset, &val))\n\t\treturn;\n\n\tif ((val & 0xfc000000) == 0x0c000000) {\n\t\tif (tg3_nvram_read(tp, offset + 4, &val))\n\t\t\treturn;\n\n\t\tif (val == 0)\n\t\t\tnewver = true;\n\t}\n\n\tdst_off = strlen(tp->fw_ver);\n\n\tif (newver) {\n\t\tif (TG3_VER_SIZE - dst_off < 16 ||\n\t\t    tg3_nvram_read(tp, offset + 8, &ver_offset))\n\t\t\treturn;\n\n\t\toffset = offset + ver_offset - start;\n\t\tfor (i = 0; i < 16; i += 4) {\n\t\t\t__be32 v;\n\t\t\tif (tg3_nvram_read_be32(tp, offset + i, &v))\n\t\t\t\treturn;\n\n\t\t\tmemcpy(tp->fw_ver + dst_off + i, &v, sizeof(v));\n\t\t}\n\t} else {\n\t\tu32 major, minor;\n\n\t\tif (tg3_nvram_read(tp, TG3_NVM_PTREV_BCVER, &ver_offset))\n\t\t\treturn;\n\n\t\tmajor = (ver_offset & TG3_NVM_BCVER_MAJMSK) >>\n\t\t\tTG3_NVM_BCVER_MAJSFT;\n\t\tminor = ver_offset & TG3_NVM_BCVER_MINMSK;\n\t\tsnprintf(&tp->fw_ver[dst_off], TG3_VER_SIZE - dst_off,\n\t\t\t \"v%d.%02d\", major, minor);\n\t}\n}\n\nstatic void tg3_read_hwsb_ver(struct tg3 *tp)\n{\n\tu32 val, major, minor;\n\n\t/* Use native endian representation */\n\tif (tg3_nvram_read(tp, TG3_NVM_HWSB_CFG1, &val))\n\t\treturn;\n\n\tmajor = (val & TG3_NVM_HWSB_CFG1_MAJMSK) >>\n\t\tTG3_NVM_HWSB_CFG1_MAJSFT;\n\tminor = (val & TG3_NVM_HWSB_CFG1_MINMSK) >>\n\t\tTG3_NVM_HWSB_CFG1_MINSFT;\n\n\tsnprintf(&tp->fw_ver[0], 32, \"sb v%d.%02d\", major, minor);\n}\n\nstatic void tg3_read_sb_ver(struct tg3 *tp, u32 val)\n{\n\tu32 offset, major, minor, build;\n\n\tstrncat(tp->fw_ver, \"sb\", TG3_VER_SIZE - strlen(tp->fw_ver) - 1);\n\n\tif ((val & TG3_EEPROM_SB_FORMAT_MASK) != TG3_EEPROM_SB_FORMAT_1)\n\t\treturn;\n\n\tswitch (val & TG3_EEPROM_SB_REVISION_MASK) {\n\tcase TG3_EEPROM_SB_REVISION_0:\n\t\toffset = TG3_EEPROM_SB_F1R0_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_2:\n\t\toffset = TG3_EEPROM_SB_F1R2_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_3:\n\t\toffset = TG3_EEPROM_SB_F1R3_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_4:\n\t\toffset = TG3_EEPROM_SB_F1R4_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_5:\n\t\toffset = TG3_EEPROM_SB_F1R5_EDH_OFF;\n\t\tbreak;\n\tcase TG3_EEPROM_SB_REVISION_6:\n\t\toffset = TG3_EEPROM_SB_F1R6_EDH_OFF;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (tg3_nvram_read(tp, offset, &val))\n\t\treturn;\n\n\tbuild = (val & TG3_EEPROM_SB_EDH_BLD_MASK) >>\n\t\tTG3_EEPROM_SB_EDH_BLD_SHFT;\n\tmajor = (val & TG3_EEPROM_SB_EDH_MAJ_MASK) >>\n\t\tTG3_EEPROM_SB_EDH_MAJ_SHFT;\n\tminor =  val & TG3_EEPROM_SB_EDH_MIN_MASK;\n\n\tif (minor > 99 || build > 26)\n\t\treturn;\n\n\toffset = strlen(tp->fw_ver);\n\tsnprintf(&tp->fw_ver[offset], TG3_VER_SIZE - offset,\n\t\t \" v%d.%02d\", major, minor);\n\n\tif (build > 0) {\n\t\toffset = strlen(tp->fw_ver);\n\t\tif (offset < TG3_VER_SIZE - 1)\n\t\t\ttp->fw_ver[offset] = 'a' + build - 1;\n\t}\n}\n\nstatic void tg3_read_mgmtfw_ver(struct tg3 *tp)\n{\n\tu32 val, offset, start;\n\tint i, vlen;\n\n\tfor (offset = TG3_NVM_DIR_START;\n\t     offset < TG3_NVM_DIR_END;\n\t     offset += TG3_NVM_DIRENT_SIZE) {\n\t\tif (tg3_nvram_read(tp, offset, &val))\n\t\t\treturn;\n\n\t\tif ((val >> TG3_NVM_DIRTYPE_SHIFT) == TG3_NVM_DIRTYPE_ASFINI)\n\t\t\tbreak;\n\t}\n\n\tif (offset == TG3_NVM_DIR_END)\n\t\treturn;\n\n\tif (!tg3_flag(tp, 5705_PLUS))\n\t\tstart = 0x08000000;\n\telse if (tg3_nvram_read(tp, offset - 4, &start))\n\t\treturn;\n\n\tif (tg3_nvram_read(tp, offset + 4, &offset) ||\n\t    !tg3_fw_img_is_valid(tp, offset) ||\n\t    tg3_nvram_read(tp, offset + 8, &val))\n\t\treturn;\n\n\toffset += val - start;\n\n\tvlen = strlen(tp->fw_ver);\n\n\ttp->fw_ver[vlen++] = ',';\n\ttp->fw_ver[vlen++] = ' ';\n\n\tfor (i = 0; i < 4; i++) {\n\t\t__be32 v;\n\t\tif (tg3_nvram_read_be32(tp, offset, &v))\n\t\t\treturn;\n\n\t\toffset += sizeof(v);\n\n\t\tif (vlen > TG3_VER_SIZE - sizeof(v)) {\n\t\t\tmemcpy(&tp->fw_ver[vlen], &v, TG3_VER_SIZE - vlen);\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(&tp->fw_ver[vlen], &v, sizeof(v));\n\t\tvlen += sizeof(v);\n\t}\n}\n\nstatic void tg3_probe_ncsi(struct tg3 *tp)\n{\n\tu32 apedata;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_SEG_SIG);\n\tif (apedata != APE_SEG_SIG_MAGIC)\n\t\treturn;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_FW_STATUS);\n\tif (!(apedata & APE_FW_STATUS_READY))\n\t\treturn;\n\n\tif (tg3_ape_read32(tp, TG3_APE_FW_FEATURES) & TG3_APE_FW_FEATURE_NCSI)\n\t\ttg3_flag_set(tp, APE_HAS_NCSI);\n}\n\nstatic void tg3_read_dash_ver(struct tg3 *tp)\n{\n\tint vlen;\n\tu32 apedata;\n\tchar *fwtype;\n\n\tapedata = tg3_ape_read32(tp, TG3_APE_FW_VERSION);\n\n\tif (tg3_flag(tp, APE_HAS_NCSI))\n\t\tfwtype = \"NCSI\";\n\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_5725)\n\t\tfwtype = \"SMASH\";\n\telse\n\t\tfwtype = \"DASH\";\n\n\tvlen = strlen(tp->fw_ver);\n\n\tsnprintf(&tp->fw_ver[vlen], TG3_VER_SIZE - vlen, \" %s v%d.%d.%d.%d\",\n\t\t fwtype,\n\t\t (apedata & APE_FW_VERSION_MAJMSK) >> APE_FW_VERSION_MAJSFT,\n\t\t (apedata & APE_FW_VERSION_MINMSK) >> APE_FW_VERSION_MINSFT,\n\t\t (apedata & APE_FW_VERSION_REVMSK) >> APE_FW_VERSION_REVSFT,\n\t\t (apedata & APE_FW_VERSION_BLDMSK));\n}\n\nstatic void tg3_read_otp_ver(struct tg3 *tp)\n{\n\tu32 val, val2;\n\n\tif (tg3_asic_rev(tp) != ASIC_REV_5762)\n\t\treturn;\n\n\tif (!tg3_ape_otp_read(tp, OTP_ADDRESS_MAGIC0, &val) &&\n\t    !tg3_ape_otp_read(tp, OTP_ADDRESS_MAGIC0 + 4, &val2) &&\n\t    TG3_OTP_MAGIC0_VALID(val)) {\n\t\tu64 val64 = (u64) val << 32 | val2;\n\t\tu32 ver = 0;\n\t\tint i, vlen;\n\n\t\tfor (i = 0; i < 7; i++) {\n\t\t\tif ((val64 & 0xff) == 0)\n\t\t\t\tbreak;\n\t\t\tver = val64 & 0xff;\n\t\t\tval64 >>= 8;\n\t\t}\n\t\tvlen = strlen(tp->fw_ver);\n\t\tsnprintf(&tp->fw_ver[vlen], TG3_VER_SIZE - vlen, \" .%02d\", ver);\n\t}\n}\n\nstatic void tg3_read_fw_ver(struct tg3 *tp)\n{\n\tu32 val;\n\tbool vpd_vers = false;\n\n\tif (tp->fw_ver[0] != 0)\n\t\tvpd_vers = true;\n\n\tif (tg3_flag(tp, NO_NVRAM)) {\n\t\tstrcat(tp->fw_ver, \"sb\");\n\t\ttg3_read_otp_ver(tp);\n\t\treturn;\n\t}\n\n\tif (tg3_nvram_read(tp, 0, &val))\n\t\treturn;\n\n\tif (val == TG3_EEPROM_MAGIC)\n\t\ttg3_read_bc_ver(tp);\n\telse if ((val & TG3_EEPROM_MAGIC_FW_MSK) == TG3_EEPROM_MAGIC_FW)\n\t\ttg3_read_sb_ver(tp, val);\n\telse if ((val & TG3_EEPROM_MAGIC_HW_MSK) == TG3_EEPROM_MAGIC_HW)\n\t\ttg3_read_hwsb_ver(tp);\n\n\tif (tg3_flag(tp, ENABLE_ASF)) {\n\t\tif (tg3_flag(tp, ENABLE_APE)) {\n\t\t\ttg3_probe_ncsi(tp);\n\t\t\tif (!vpd_vers)\n\t\t\t\ttg3_read_dash_ver(tp);\n\t\t} else if (!vpd_vers) {\n\t\t\ttg3_read_mgmtfw_ver(tp);\n\t\t}\n\t}\n\n\ttp->fw_ver[TG3_VER_SIZE - 1] = 0;\n}\n\nstatic inline u32 tg3_rx_ret_ring_size(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, LRG_PROD_RING_CAP))\n\t\treturn TG3_RX_RET_MAX_SIZE_5717;\n\telse if (tg3_flag(tp, JUMBO_CAPABLE) && !tg3_flag(tp, 5780_CLASS))\n\t\treturn TG3_RX_RET_MAX_SIZE_5700;\n\telse\n\t\treturn TG3_RX_RET_MAX_SIZE_5705;\n}\n\nstatic DEFINE_PCI_DEVICE_TABLE(tg3_write_reorder_chipsets) = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_FE_GATE_700C) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_8131_BRIDGE) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_VIA, PCI_DEVICE_ID_VIA_8385_0) },\n\t{ },\n};\n\nstatic struct pci_dev *tg3_find_peer(struct tg3 *tp)\n{\n\tstruct pci_dev *peer;\n\tunsigned int func, devnr = tp->pdev->devfn & ~7;\n\n\tfor (func = 0; func < 8; func++) {\n\t\tpeer = pci_get_slot(tp->pdev->bus, devnr | func);\n\t\tif (peer && peer != tp->pdev)\n\t\t\tbreak;\n\t\tpci_dev_put(peer);\n\t}\n\t/* 5704 can be configured in single-port mode, set peer to\n\t * tp->pdev in that case.\n\t */\n\tif (!peer) {\n\t\tpeer = tp->pdev;\n\t\treturn peer;\n\t}\n\n\t/*\n\t * We don't need to keep the refcount elevated; there's no way\n\t * to remove one half of this device without removing the other\n\t */\n\tpci_dev_put(peer);\n\n\treturn peer;\n}\n\nstatic void tg3_detect_asic_rev(struct tg3 *tp, u32 misc_ctrl_reg)\n{\n\ttp->pci_chip_rev_id = misc_ctrl_reg >> MISC_HOST_CTRL_CHIPREV_SHIFT;\n\tif (tg3_asic_rev(tp) == ASIC_REV_USE_PROD_ID_REG) {\n\t\tu32 reg;\n\n\t\t/* All devices that use the alternate\n\t\t * ASIC REV location have a CPMU.\n\t\t */\n\t\ttg3_flag_set(tp, CPMU_PRESENT);\n\n\t\tif (tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717_C ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5718 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5719 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5720 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5762 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5725 ||\n\t\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5727)\n\t\t\treg = TG3PCI_GEN2_PRODID_ASICREV;\n\t\telse if (tp->pdev->device == TG3PCI_DEVICE_TIGON3_57781 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57785 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57761 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57765 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57791 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57795 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57762 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57766 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57782 ||\n\t\t\t tp->pdev->device == TG3PCI_DEVICE_TIGON3_57786)\n\t\t\treg = TG3PCI_GEN15_PRODID_ASICREV;\n\t\telse\n\t\t\treg = TG3PCI_PRODID_ASICREV;\n\n\t\tpci_read_config_dword(tp->pdev, reg, &tp->pci_chip_rev_id);\n\t}\n\n\t/* Wrong chip ID in 5752 A0. This code can be removed later\n\t * as A0 is not in production.\n\t */\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5752_A0_HW)\n\t\ttp->pci_chip_rev_id = CHIPREV_ID_5752_A0;\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5717_C0)\n\t\ttp->pci_chip_rev_id = CHIPREV_ID_5720_A0;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720)\n\t\ttg3_flag_set(tp, 5717_PLUS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_57765 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57766)\n\t\ttg3_flag_set(tp, 57765_CLASS);\n\n\tif (tg3_flag(tp, 57765_CLASS) || tg3_flag(tp, 5717_PLUS) ||\n\t     tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\ttg3_flag_set(tp, 57765_PLUS);\n\n\t/* Intentionally exclude ASIC_REV_5906 */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5787 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5761 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780 ||\n\t    tg3_flag(tp, 57765_PLUS))\n\t\ttg3_flag_set(tp, 5755_PLUS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5780 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5714)\n\t\ttg3_flag_set(tp, 5780_CLASS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5750 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5752 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5906 ||\n\t    tg3_flag(tp, 5755_PLUS) ||\n\t    tg3_flag(tp, 5780_CLASS))\n\t\ttg3_flag_set(tp, 5750_PLUS);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705 ||\n\t    tg3_flag(tp, 5750_PLUS))\n\t\ttg3_flag_set(tp, 5705_PLUS);\n}\n\nstatic bool tg3_10_100_only_device(struct tg3 *tp,\n\t\t\t\t   const struct pci_device_id *ent)\n{\n\tu32 grc_misc_cfg = tr32(GRC_MISC_CFG) & GRC_MISC_CFG_BOARD_ID_MASK;\n\n\tif ((tg3_asic_rev(tp) == ASIC_REV_5703 &&\n\t     (grc_misc_cfg == 0x8000 || grc_misc_cfg == 0x4000)) ||\n\t    (tp->phy_flags & TG3_PHYFLG_IS_FET))\n\t\treturn true;\n\n\tif (ent->driver_data & TG3_DRV_DATA_FLAG_10_100_ONLY) {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5705) {\n\t\t\tif (ent->driver_data & TG3_DRV_DATA_FLAG_5705_10_100)\n\t\t\t\treturn true;\n\t\t} else {\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int tg3_get_invariants(struct tg3 *tp, const struct pci_device_id *ent)\n{\n\tu32 misc_ctrl_reg;\n\tu32 pci_state_reg, grc_misc_cfg;\n\tu32 val;\n\tu16 pci_cmd;\n\tint err;\n\n\t/* Force memory write invalidate off.  If we leave it on,\n\t * then on 5700_BX chips we have to enable a workaround.\n\t * The workaround is to set the TG3PCI_DMA_RW_CTRL boundary\n\t * to match the cacheline size.  The Broadcom driver have this\n\t * workaround but turns MWI off all the times so never uses\n\t * it.  This seems to suggest that the workaround is insufficient.\n\t */\n\tpci_read_config_word(tp->pdev, PCI_COMMAND, &pci_cmd);\n\tpci_cmd &= ~PCI_COMMAND_INVALIDATE;\n\tpci_write_config_word(tp->pdev, PCI_COMMAND, pci_cmd);\n\n\t/* Important! -- Make sure register accesses are byteswapped\n\t * correctly.  Also, for those chips that require it, make\n\t * sure that indirect register accesses are enabled before\n\t * the first operation.\n\t */\n\tpci_read_config_dword(tp->pdev, TG3PCI_MISC_HOST_CTRL,\n\t\t\t      &misc_ctrl_reg);\n\ttp->misc_host_ctrl |= (misc_ctrl_reg &\n\t\t\t       MISC_HOST_CTRL_CHIPREV);\n\tpci_write_config_dword(tp->pdev, TG3PCI_MISC_HOST_CTRL,\n\t\t\t       tp->misc_host_ctrl);\n\n\ttg3_detect_asic_rev(tp, misc_ctrl_reg);\n\n\t/* If we have 5702/03 A1 or A2 on certain ICH chipsets,\n\t * we need to disable memory and use config. cycles\n\t * only to access all registers. The 5702/03 chips\n\t * can mistakenly decode the special cycles from the\n\t * ICH chipsets as memory write cycles, causing corruption\n\t * of register and memory space. Only certain ICH bridges\n\t * will drive special cycles with non-zero data during the\n\t * address phase which can fall within the 5703's address\n\t * range. This is not an ICH bug as the PCI spec allows\n\t * non-zero address during special cycles. However, only\n\t * these ICH bridges are known to drive non-zero addresses\n\t * during special cycles.\n\t *\n\t * Since special cycles do not cross PCI bridges, we only\n\t * enable this workaround if the 5703 is on the secondary\n\t * bus of these ICH bridges.\n\t */\n\tif ((tg3_chip_rev_id(tp) == CHIPREV_ID_5703_A1) ||\n\t    (tg3_chip_rev_id(tp) == CHIPREV_ID_5703_A2)) {\n\t\tstatic struct tg3_dev_id {\n\t\t\tu32\tvendor;\n\t\t\tu32\tdevice;\n\t\t\tu32\trev;\n\t\t} ich_chipsets[] = {\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801AA_8,\n\t\t\t  PCI_ANY_ID },\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801AB_8,\n\t\t\t  PCI_ANY_ID },\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801BA_11,\n\t\t\t  0xa },\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801BA_6,\n\t\t\t  PCI_ANY_ID },\n\t\t\t{ },\n\t\t};\n\t\tstruct tg3_dev_id *pci_id = &ich_chipsets[0];\n\t\tstruct pci_dev *bridge = NULL;\n\n\t\twhile (pci_id->vendor != 0) {\n\t\t\tbridge = pci_get_device(pci_id->vendor, pci_id->device,\n\t\t\t\t\t\tbridge);\n\t\t\tif (!bridge) {\n\t\t\t\tpci_id++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (pci_id->rev != PCI_ANY_ID) {\n\t\t\t\tif (bridge->revision > pci_id->rev)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (bridge->subordinate &&\n\t\t\t    (bridge->subordinate->number ==\n\t\t\t     tp->pdev->bus->number)) {\n\t\t\t\ttg3_flag_set(tp, ICH_WORKAROUND);\n\t\t\t\tpci_dev_put(bridge);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\tstatic struct tg3_dev_id {\n\t\t\tu32\tvendor;\n\t\t\tu32\tdevice;\n\t\t} bridge_chipsets[] = {\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_PXH_0 },\n\t\t\t{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_PXH_1 },\n\t\t\t{ },\n\t\t};\n\t\tstruct tg3_dev_id *pci_id = &bridge_chipsets[0];\n\t\tstruct pci_dev *bridge = NULL;\n\n\t\twhile (pci_id->vendor != 0) {\n\t\t\tbridge = pci_get_device(pci_id->vendor,\n\t\t\t\t\t\tpci_id->device,\n\t\t\t\t\t\tbridge);\n\t\t\tif (!bridge) {\n\t\t\t\tpci_id++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (bridge->subordinate &&\n\t\t\t    (bridge->subordinate->number <=\n\t\t\t     tp->pdev->bus->number) &&\n\t\t\t    (bridge->subordinate->busn_res.end >=\n\t\t\t     tp->pdev->bus->number)) {\n\t\t\t\ttg3_flag_set(tp, 5701_DMA_BUG);\n\t\t\t\tpci_dev_put(bridge);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* The EPB bridge inside 5714, 5715, and 5780 cannot support\n\t * DMA addresses > 40-bit. This bridge may have other additional\n\t * 57xx devices behind it in some 4-port NIC designs for example.\n\t * Any tg3 device found behind the bridge will also need the 40-bit\n\t * DMA workaround.\n\t */\n\tif (tg3_flag(tp, 5780_CLASS)) {\n\t\ttg3_flag_set(tp, 40BIT_DMA_BUG);\n\t\ttp->msi_cap = pci_find_capability(tp->pdev, PCI_CAP_ID_MSI);\n\t} else {\n\t\tstruct pci_dev *bridge = NULL;\n\n\t\tdo {\n\t\t\tbridge = pci_get_device(PCI_VENDOR_ID_SERVERWORKS,\n\t\t\t\t\t\tPCI_DEVICE_ID_SERVERWORKS_EPB,\n\t\t\t\t\t\tbridge);\n\t\t\tif (bridge && bridge->subordinate &&\n\t\t\t    (bridge->subordinate->number <=\n\t\t\t     tp->pdev->bus->number) &&\n\t\t\t    (bridge->subordinate->busn_res.end >=\n\t\t\t     tp->pdev->bus->number)) {\n\t\t\t\ttg3_flag_set(tp, 40BIT_DMA_BUG);\n\t\t\t\tpci_dev_put(bridge);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (bridge);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5714)\n\t\ttp->pdev_peer = tg3_find_peer(tp);\n\n\t/* Determine TSO capabilities */\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0)\n\t\t; /* Do nothing. HW bug. */\n\telse if (tg3_flag(tp, 57765_PLUS))\n\t\ttg3_flag_set(tp, HW_TSO_3);\n\telse if (tg3_flag(tp, 5755_PLUS) ||\n\t\t tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\ttg3_flag_set(tp, HW_TSO_2);\n\telse if (tg3_flag(tp, 5750_PLUS)) {\n\t\ttg3_flag_set(tp, HW_TSO_1);\n\t\ttg3_flag_set(tp, TSO_BUG);\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5750 &&\n\t\t    tg3_chip_rev_id(tp) >= CHIPREV_ID_5750_C2)\n\t\t\ttg3_flag_clear(tp, TSO_BUG);\n\t} else if (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t\t   tg3_asic_rev(tp) != ASIC_REV_5701 &&\n\t\t   tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A0) {\n\t\t\ttg3_flag_set(tp, TSO_BUG);\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5705)\n\t\t\ttp->fw_needed = FIRMWARE_TG3TSO5;\n\t\telse\n\t\t\ttp->fw_needed = FIRMWARE_TG3TSO;\n\t}\n\n\t/* Selectively allow TSO based on operating conditions */\n\tif (tg3_flag(tp, HW_TSO_1) ||\n\t    tg3_flag(tp, HW_TSO_2) ||\n\t    tg3_flag(tp, HW_TSO_3) ||\n\t    tp->fw_needed) {\n\t\t/* For firmware TSO, assume ASF is disabled.\n\t\t * We'll disable TSO later if we discover ASF\n\t\t * is enabled in tg3_get_eeprom_hw_cfg().\n\t\t */\n\t\ttg3_flag_set(tp, TSO_CAPABLE);\n\t} else {\n\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\ttg3_flag_clear(tp, TSO_BUG);\n\t\ttp->fw_needed = NULL;\n\t}\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0)\n\t\ttp->fw_needed = FIRMWARE_TG3;\n\n\ttp->irq_max = 1;\n\n\tif (tg3_flag(tp, 5750_PLUS)) {\n\t\ttg3_flag_set(tp, SUPPORT_MSI);\n\t\tif (tg3_chip_rev(tp) == CHIPREV_5750_AX ||\n\t\t    tg3_chip_rev(tp) == CHIPREV_5750_BX ||\n\t\t    (tg3_asic_rev(tp) == ASIC_REV_5714 &&\n\t\t     tg3_chip_rev_id(tp) <= CHIPREV_ID_5714_A2 &&\n\t\t     tp->pdev_peer == tp->pdev))\n\t\t\ttg3_flag_clear(tp, SUPPORT_MSI);\n\n\t\tif (tg3_flag(tp, 5755_PLUS) ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t\ttg3_flag_set(tp, 1SHOT_MSI);\n\t\t}\n\n\t\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\t\ttg3_flag_set(tp, SUPPORT_MSIX);\n\t\t\ttp->irq_max = TG3_IRQ_MAX_VECS;\n\t\t}\n\t}\n\n\ttp->txq_max = 1;\n\ttp->rxq_max = 1;\n\tif (tp->irq_max > 1) {\n\t\ttp->rxq_max = TG3_RSS_MAX_NUM_QS;\n\t\ttg3_rss_init_dflt_indir_tbl(tp, TG3_RSS_MAX_NUM_QS);\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5720)\n\t\t\ttp->txq_max = tp->irq_max - 1;\n\t}\n\n\tif (tg3_flag(tp, 5755_PLUS) ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\ttg3_flag_set(tp, SHORT_DMA_BUG);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719)\n\t\ttp->dma_limit = TG3_TX_BD_DMA_MAX_4K;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\ttg3_flag_set(tp, LRG_PROD_RING_CAP);\n\n\tif (tg3_flag(tp, 57765_PLUS) &&\n\t    tg3_chip_rev_id(tp) != CHIPREV_ID_5719_A0)\n\t\ttg3_flag_set(tp, USE_JUMBO_BDFLAG);\n\n\tif (!tg3_flag(tp, 5705_PLUS) ||\n\t    tg3_flag(tp, 5780_CLASS) ||\n\t    tg3_flag(tp, USE_JUMBO_BDFLAG))\n\t\ttg3_flag_set(tp, JUMBO_CAPABLE);\n\n\tpci_read_config_dword(tp->pdev, TG3PCI_PCISTATE,\n\t\t\t      &pci_state_reg);\n\n\tif (pci_is_pcie(tp->pdev)) {\n\t\tu16 lnkctl;\n\n\t\ttg3_flag_set(tp, PCI_EXPRESS);\n\n\t\tpcie_capability_read_word(tp->pdev, PCI_EXP_LNKCTL, &lnkctl);\n\t\tif (lnkctl & PCI_EXP_LNKCTL_CLKREQ_EN) {\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t\t\ttg3_flag_clear(tp, HW_TSO_2);\n\t\t\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\t\t}\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5761 ||\n\t\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_57780_A0 ||\n\t\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_57780_A1)\n\t\t\t\ttg3_flag_set(tp, CLKREQ_BUG);\n\t\t} else if (tg3_chip_rev_id(tp) == CHIPREV_ID_5717_A0) {\n\t\t\ttg3_flag_set(tp, L1PLLPD_EN);\n\t\t}\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_5785) {\n\t\t/* BCM5785 devices are effectively PCIe devices, and should\n\t\t * follow PCIe codepaths, but do not have a PCIe capabilities\n\t\t * section.\n\t\t */\n\t\ttg3_flag_set(tp, PCI_EXPRESS);\n\t} else if (!tg3_flag(tp, 5705_PLUS) ||\n\t\t   tg3_flag(tp, 5780_CLASS)) {\n\t\ttp->pcix_cap = pci_find_capability(tp->pdev, PCI_CAP_ID_PCIX);\n\t\tif (!tp->pcix_cap) {\n\t\t\tdev_err(&tp->pdev->dev,\n\t\t\t\t\"Cannot find PCI-X capability, aborting\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (!(pci_state_reg & PCISTATE_CONV_PCI_MODE))\n\t\t\ttg3_flag_set(tp, PCIX_MODE);\n\t}\n\n\t/* If we have an AMD 762 or VIA K8T800 chipset, write\n\t * reordering to the mailbox registers done by the host\n\t * controller can cause major troubles.  We read back from\n\t * every mailbox register write to force the writes to be\n\t * posted to the chip in order.\n\t */\n\tif (pci_dev_present(tg3_write_reorder_chipsets) &&\n\t    !tg3_flag(tp, PCI_EXPRESS))\n\t\ttg3_flag_set(tp, MBOX_WRITE_REORDER);\n\n\tpci_read_config_byte(tp->pdev, PCI_CACHE_LINE_SIZE,\n\t\t\t     &tp->pci_cacheline_sz);\n\tpci_read_config_byte(tp->pdev, PCI_LATENCY_TIMER,\n\t\t\t     &tp->pci_lat_timer);\n\tif (tg3_asic_rev(tp) == ASIC_REV_5703 &&\n\t    tp->pci_lat_timer < 64) {\n\t\ttp->pci_lat_timer = 64;\n\t\tpci_write_config_byte(tp->pdev, PCI_LATENCY_TIMER,\n\t\t\t\t      tp->pci_lat_timer);\n\t}\n\n\t/* Important! -- It is critical that the PCI-X hw workaround\n\t * situation is decided before the first MMIO register access.\n\t */\n\tif (tg3_chip_rev(tp) == CHIPREV_5700_BX) {\n\t\t/* 5700 BX chips need to have their TX producer index\n\t\t * mailboxes written twice to workaround a bug.\n\t\t */\n\t\ttg3_flag_set(tp, TXD_MBOX_HWBUG);\n\n\t\t/* If we are in PCI-X mode, enable register write workaround.\n\t\t *\n\t\t * The workaround is to use indirect register accesses\n\t\t * for all chip writes not to mailbox registers.\n\t\t */\n\t\tif (tg3_flag(tp, PCIX_MODE)) {\n\t\t\tu32 pm_reg;\n\n\t\t\ttg3_flag_set(tp, PCIX_TARGET_HWBUG);\n\n\t\t\t/* The chip can have it's power management PCI config\n\t\t\t * space registers clobbered due to this bug.\n\t\t\t * So explicitly force the chip into D0 here.\n\t\t\t */\n\t\t\tpci_read_config_dword(tp->pdev,\n\t\t\t\t\t      tp->pm_cap + PCI_PM_CTRL,\n\t\t\t\t\t      &pm_reg);\n\t\t\tpm_reg &= ~PCI_PM_CTRL_STATE_MASK;\n\t\t\tpm_reg |= PCI_PM_CTRL_PME_ENABLE | 0 /* D0 */;\n\t\t\tpci_write_config_dword(tp->pdev,\n\t\t\t\t\t       tp->pm_cap + PCI_PM_CTRL,\n\t\t\t\t\t       pm_reg);\n\n\t\t\t/* Also, force SERR#/PERR# in PCI command. */\n\t\t\tpci_read_config_word(tp->pdev, PCI_COMMAND, &pci_cmd);\n\t\t\tpci_cmd |= PCI_COMMAND_PARITY | PCI_COMMAND_SERR;\n\t\t\tpci_write_config_word(tp->pdev, PCI_COMMAND, pci_cmd);\n\t\t}\n\t}\n\n\tif ((pci_state_reg & PCISTATE_BUS_SPEED_HIGH) != 0)\n\t\ttg3_flag_set(tp, PCI_HIGH_SPEED);\n\tif ((pci_state_reg & PCISTATE_BUS_32BIT) != 0)\n\t\ttg3_flag_set(tp, PCI_32BIT);\n\n\t/* Chip-specific fixup from Broadcom driver */\n\tif ((tg3_chip_rev_id(tp) == CHIPREV_ID_5704_A0) &&\n\t    (!(pci_state_reg & PCISTATE_RETRY_SAME_DMA))) {\n\t\tpci_state_reg |= PCISTATE_RETRY_SAME_DMA;\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_PCISTATE, pci_state_reg);\n\t}\n\n\t/* Default fast path register access methods */\n\ttp->read32 = tg3_read32;\n\ttp->write32 = tg3_write32;\n\ttp->read32_mbox = tg3_read32;\n\ttp->write32_mbox = tg3_write32;\n\ttp->write32_tx_mbox = tg3_write32;\n\ttp->write32_rx_mbox = tg3_write32;\n\n\t/* Various workaround register access methods */\n\tif (tg3_flag(tp, PCIX_TARGET_HWBUG))\n\t\ttp->write32 = tg3_write_indirect_reg32;\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5701 ||\n\t\t (tg3_flag(tp, PCI_EXPRESS) &&\n\t\t  tg3_chip_rev_id(tp) == CHIPREV_ID_5750_A0)) {\n\t\t/*\n\t\t * Back to back register writes can cause problems on these\n\t\t * chips, the workaround is to read back all reg writes\n\t\t * except those to mailbox regs.\n\t\t *\n\t\t * See tg3_write_indirect_reg32().\n\t\t */\n\t\ttp->write32 = tg3_write_flush_reg32;\n\t}\n\n\tif (tg3_flag(tp, TXD_MBOX_HWBUG) || tg3_flag(tp, MBOX_WRITE_REORDER)) {\n\t\ttp->write32_tx_mbox = tg3_write32_tx_mbox;\n\t\tif (tg3_flag(tp, MBOX_WRITE_REORDER))\n\t\t\ttp->write32_rx_mbox = tg3_write_flush_reg32;\n\t}\n\n\tif (tg3_flag(tp, ICH_WORKAROUND)) {\n\t\ttp->read32 = tg3_read_indirect_reg32;\n\t\ttp->write32 = tg3_write_indirect_reg32;\n\t\ttp->read32_mbox = tg3_read_indirect_mbox;\n\t\ttp->write32_mbox = tg3_write_indirect_mbox;\n\t\ttp->write32_tx_mbox = tg3_write_indirect_mbox;\n\t\ttp->write32_rx_mbox = tg3_write_indirect_mbox;\n\n\t\tiounmap(tp->regs);\n\t\ttp->regs = NULL;\n\n\t\tpci_read_config_word(tp->pdev, PCI_COMMAND, &pci_cmd);\n\t\tpci_cmd &= ~PCI_COMMAND_MEMORY;\n\t\tpci_write_config_word(tp->pdev, PCI_COMMAND, pci_cmd);\n\t}\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\ttp->read32_mbox = tg3_read32_mbox_5906;\n\t\ttp->write32_mbox = tg3_write32_mbox_5906;\n\t\ttp->write32_tx_mbox = tg3_write32_mbox_5906;\n\t\ttp->write32_rx_mbox = tg3_write32_mbox_5906;\n\t}\n\n\tif (tp->write32 == tg3_write_indirect_reg32 ||\n\t    (tg3_flag(tp, PCIX_MODE) &&\n\t     (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t      tg3_asic_rev(tp) == ASIC_REV_5701)))\n\t\ttg3_flag_set(tp, SRAM_USE_CONFIG);\n\n\t/* The memory arbiter has to be enabled in order for SRAM accesses\n\t * to succeed.  Normally on powerup the tg3 chip firmware will make\n\t * sure it is enabled, but other entities such as system netboot\n\t * code might disable it.\n\t */\n\tval = tr32(MEMARB_MODE);\n\ttw32(MEMARB_MODE, val | MEMARB_MODE_ENABLE);\n\n\ttp->pci_fn = PCI_FUNC(tp->pdev->devfn) & 3;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    tg3_flag(tp, 5780_CLASS)) {\n\t\tif (tg3_flag(tp, PCIX_MODE)) {\n\t\t\tpci_read_config_dword(tp->pdev,\n\t\t\t\t\t      tp->pcix_cap + PCI_X_STATUS,\n\t\t\t\t\t      &val);\n\t\t\ttp->pci_fn = val & 0x7;\n\t\t}\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t\t   tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t\t   tg3_asic_rev(tp) == ASIC_REV_5720) {\n\t\ttg3_read_mem(tp, NIC_SRAM_CPMU_STATUS, &val);\n\t\tif ((val & NIC_SRAM_CPMUSTAT_SIG_MSK) != NIC_SRAM_CPMUSTAT_SIG)\n\t\t\tval = tr32(TG3_CPMU_STATUS);\n\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5717)\n\t\t\ttp->pci_fn = (val & TG3_CPMU_STATUS_FMSK_5717) ? 1 : 0;\n\t\telse\n\t\t\ttp->pci_fn = (val & TG3_CPMU_STATUS_FMSK_5719) >>\n\t\t\t\t     TG3_CPMU_STATUS_FSHFT_5719;\n\t}\n\n\tif (tg3_flag(tp, FLUSH_POSTED_WRITES)) {\n\t\ttp->write32_tx_mbox = tg3_write_flush_reg32;\n\t\ttp->write32_rx_mbox = tg3_write_flush_reg32;\n\t}\n\n\t/* Get eeprom hw config before calling tg3_set_power_state().\n\t * In particular, the TG3_FLAG_IS_NIC flag must be\n\t * determined before calling tg3_set_power_state() so that\n\t * we know whether or not to switch out of Vaux power.\n\t * When the flag is set, it means that GPIO1 is used for eeprom\n\t * write protect and also implies that it is a LOM where GPIOs\n\t * are not used to switch power.\n\t */\n\ttg3_get_eeprom_hw_cfg(tp);\n\n\tif (tp->fw_needed && tg3_flag(tp, ENABLE_ASF)) {\n\t\ttg3_flag_clear(tp, TSO_CAPABLE);\n\t\ttg3_flag_clear(tp, TSO_BUG);\n\t\ttp->fw_needed = NULL;\n\t}\n\n\tif (tg3_flag(tp, ENABLE_APE)) {\n\t\t/* Allow reads and writes to the\n\t\t * APE register and memory space.\n\t\t */\n\t\tpci_state_reg |= PCISTATE_ALLOW_APE_CTLSPC_WR |\n\t\t\t\t PCISTATE_ALLOW_APE_SHMEM_WR |\n\t\t\t\t PCISTATE_ALLOW_APE_PSPACE_WR;\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_PCISTATE,\n\t\t\t\t       pci_state_reg);\n\n\t\ttg3_ape_lock_init(tp);\n\t}\n\n\t/* Set up tp->grc_local_ctrl before calling\n\t * tg3_pwrsrc_switch_to_vmain().  GPIO1 driven high\n\t * will bring 5700's external PHY out of reset.\n\t * It is also used as eeprom write protect on LOMs.\n\t */\n\ttp->grc_local_ctrl = GRC_LCLCTRL_INT_ON_ATTN | GRC_LCLCTRL_AUTO_SEEPROM;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_flag(tp, EEPROM_WRITE_PROT))\n\t\ttp->grc_local_ctrl |= (GRC_LCLCTRL_GPIO_OE1 |\n\t\t\t\t       GRC_LCLCTRL_GPIO_OUTPUT1);\n\t/* Unused GPIO3 must be driven as output on 5752 because there\n\t * are no pull-up resistors on unused GPIO pins.\n\t */\n\telse if (tg3_asic_rev(tp) == ASIC_REV_5752)\n\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_GPIO_OE3;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780 ||\n\t    tg3_flag(tp, 57765_CLASS))\n\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_GPIO_UART_SEL;\n\n\tif (tp->pdev->device == PCI_DEVICE_ID_TIGON3_5761 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5761S) {\n\t\t/* Turn off the debug UART. */\n\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_GPIO_UART_SEL;\n\t\tif (tg3_flag(tp, IS_NIC))\n\t\t\t/* Keep VMain power. */\n\t\t\ttp->grc_local_ctrl |= GRC_LCLCTRL_GPIO_OE0 |\n\t\t\t\t\t      GRC_LCLCTRL_GPIO_OUTPUT0;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\ttp->grc_local_ctrl |=\n\t\t\ttr32(GRC_LOCAL_CTRL) & GRC_LCLCTRL_GPIO_UART_SEL;\n\n\t/* Switch out of Vaux if it is a NIC */\n\ttg3_pwrsrc_switch_to_vmain(tp);\n\n\t/* Derive initial jumbo mode from MTU assigned in\n\t * ether_setup() via the alloc_etherdev() call\n\t */\n\tif (tp->dev->mtu > ETH_DATA_LEN && !tg3_flag(tp, 5780_CLASS))\n\t\ttg3_flag_set(tp, JUMBO_RING_ENABLE);\n\n\t/* Determine WakeOnLan speed to use. */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B2) {\n\t\ttg3_flag_clear(tp, WOL_SPEED_100MB);\n\t} else {\n\t\ttg3_flag_set(tp, WOL_SPEED_100MB);\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\ttp->phy_flags |= TG3_PHYFLG_IS_FET;\n\n\t/* A few boards don't want Ethernet@WireSpeed phy feature */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    (tg3_asic_rev(tp) == ASIC_REV_5705 &&\n\t     (tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A0) &&\n\t     (tg3_chip_rev_id(tp) != CHIPREV_ID_5705_A1)) ||\n\t    (tp->phy_flags & TG3_PHYFLG_IS_FET) ||\n\t    (tp->phy_flags & TG3_PHYFLG_ANY_SERDES))\n\t\ttp->phy_flags |= TG3_PHYFLG_NO_ETH_WIRE_SPEED;\n\n\tif (tg3_chip_rev(tp) == CHIPREV_5703_AX ||\n\t    tg3_chip_rev(tp) == CHIPREV_5704_AX)\n\t\ttp->phy_flags |= TG3_PHYFLG_ADC_BUG;\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5704_A0)\n\t\ttp->phy_flags |= TG3_PHYFLG_5704_A0_BUG;\n\n\tif (tg3_flag(tp, 5705_PLUS) &&\n\t    !(tp->phy_flags & TG3_PHYFLG_IS_FET) &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5785 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_57780 &&\n\t    !tg3_flag(tp, 57765_PLUS)) {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5755 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5787 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5784 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5761) {\n\t\t\tif (tp->pdev->device != PCI_DEVICE_ID_TIGON3_5756 &&\n\t\t\t    tp->pdev->device != PCI_DEVICE_ID_TIGON3_5722)\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_JITTER_BUG;\n\t\t\tif (tp->pdev->device == PCI_DEVICE_ID_TIGON3_5755M)\n\t\t\t\ttp->phy_flags |= TG3_PHYFLG_ADJUST_TRIM;\n\t\t} else\n\t\t\ttp->phy_flags |= TG3_PHYFLG_BER_BUG;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5784 &&\n\t    tg3_chip_rev(tp) != CHIPREV_5784_AX) {\n\t\ttp->phy_otp = tg3_read_otp_phycfg(tp);\n\t\tif (tp->phy_otp == 0)\n\t\t\ttp->phy_otp = TG3_OTP_DEFAULT;\n\t}\n\n\tif (tg3_flag(tp, CPMU_PRESENT))\n\t\ttp->mi_mode = MAC_MI_MODE_500KHZ_CONST;\n\telse\n\t\ttp->mi_mode = MAC_MI_MODE_BASE;\n\n\ttp->coalesce_mode = 0;\n\tif (tg3_chip_rev(tp) != CHIPREV_5700_AX &&\n\t    tg3_chip_rev(tp) != CHIPREV_5700_BX)\n\t\ttp->coalesce_mode |= HOSTCC_MODE_32BYTE;\n\n\t/* Set these bits to enable statistics workaround. */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5717 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5719_A0 ||\n\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5720_A0) {\n\t\ttp->coalesce_mode |= HOSTCC_MODE_ATTN;\n\t\ttp->grc_mode |= GRC_MODE_IRQ_ON_FLOW_ATTN;\n\t}\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_57780)\n\t\ttg3_flag_set(tp, USE_PHYLIB);\n\n\terr = tg3_mdio_init(tp);\n\tif (err)\n\t\treturn err;\n\n\t/* Initialize data/descriptor byte/word swapping. */\n\tval = tr32(GRC_MODE);\n\tif (tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\tval &= (GRC_MODE_BYTE_SWAP_B2HRX_DATA |\n\t\t\tGRC_MODE_WORD_SWAP_B2HRX_DATA |\n\t\t\tGRC_MODE_B2HRX_ENABLE |\n\t\t\tGRC_MODE_HTX2B_ENABLE |\n\t\t\tGRC_MODE_HOST_STACKUP);\n\telse\n\t\tval &= GRC_MODE_HOST_STACKUP;\n\n\ttw32(GRC_MODE, val | tp->grc_mode);\n\n\ttg3_switch_clocks(tp);\n\n\t/* Clear this out for sanity. */\n\ttw32(TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\n\tpci_read_config_dword(tp->pdev, TG3PCI_PCISTATE,\n\t\t\t      &pci_state_reg);\n\tif ((pci_state_reg & PCISTATE_CONV_PCI_MODE) == 0 &&\n\t    !tg3_flag(tp, PCIX_TARGET_HWBUG)) {\n\t\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5701_A0 ||\n\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B0 ||\n\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B2 ||\n\t\t    tg3_chip_rev_id(tp) == CHIPREV_ID_5701_B5) {\n\t\t\tvoid __iomem *sram_base;\n\n\t\t\t/* Write some dummy words into the SRAM status block\n\t\t\t * area, see if it reads back correctly.  If the return\n\t\t\t * value is bad, force enable the PCIX workaround.\n\t\t\t */\n\t\t\tsram_base = tp->regs + NIC_SRAM_WIN_BASE + NIC_SRAM_STATS_BLK;\n\n\t\t\twritel(0x00000000, sram_base);\n\t\t\twritel(0x00000000, sram_base + 4);\n\t\t\twritel(0xffffffff, sram_base + 4);\n\t\t\tif (readl(sram_base) != 0x00000000)\n\t\t\t\ttg3_flag_set(tp, PCIX_TARGET_HWBUG);\n\t\t}\n\t}\n\n\tudelay(50);\n\ttg3_nvram_init(tp);\n\n\tgrc_misc_cfg = tr32(GRC_MISC_CFG);\n\tgrc_misc_cfg &= GRC_MISC_CFG_BOARD_ID_MASK;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5705 &&\n\t    (grc_misc_cfg == GRC_MISC_CFG_BOARD_ID_5788 ||\n\t     grc_misc_cfg == GRC_MISC_CFG_BOARD_ID_5788M))\n\t\ttg3_flag_set(tp, IS_5788);\n\n\tif (!tg3_flag(tp, IS_5788) &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5700)\n\t\ttg3_flag_set(tp, TAGGED_STATUS);\n\tif (tg3_flag(tp, TAGGED_STATUS)) {\n\t\ttp->coalesce_mode |= (HOSTCC_MODE_CLRTICK_RXBD |\n\t\t\t\t      HOSTCC_MODE_CLRTICK_TXBD);\n\n\t\ttp->misc_host_ctrl |= MISC_HOST_CTRL_TAGGED_STATUS;\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MISC_HOST_CTRL,\n\t\t\t\t       tp->misc_host_ctrl);\n\t}\n\n\t/* Preserve the APE MAC_MODE bits */\n\tif (tg3_flag(tp, ENABLE_APE))\n\t\ttp->mac_mode = MAC_MODE_APE_TX_EN | MAC_MODE_APE_RX_EN;\n\telse\n\t\ttp->mac_mode = 0;\n\n\tif (tg3_10_100_only_device(tp, ent))\n\t\ttp->phy_flags |= TG3_PHYFLG_10_100_ONLY;\n\n\terr = tg3_phy_probe(tp);\n\tif (err) {\n\t\tdev_err(&tp->pdev->dev, \"phy probe failed, err %d\\n\", err);\n\t\t/* ... but do not return immediately ... */\n\t\ttg3_mdio_fini(tp);\n\t}\n\n\ttg3_read_vpd(tp);\n\ttg3_read_fw_ver(tp);\n\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES) {\n\t\ttp->phy_flags &= ~TG3_PHYFLG_USE_MI_INTERRUPT;\n\t} else {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5700)\n\t\t\ttp->phy_flags |= TG3_PHYFLG_USE_MI_INTERRUPT;\n\t\telse\n\t\t\ttp->phy_flags &= ~TG3_PHYFLG_USE_MI_INTERRUPT;\n\t}\n\n\t/* 5700 {AX,BX} chips have a broken status block link\n\t * change bit implementation, so we must use the\n\t * status register in those cases.\n\t */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700)\n\t\ttg3_flag_set(tp, USE_LINKCHG_REG);\n\telse\n\t\ttg3_flag_clear(tp, USE_LINKCHG_REG);\n\n\t/* The led_ctrl is set during tg3_phy_probe, here we might\n\t * have to force the link status polling mechanism based\n\t * upon subsystem IDs.\n\t */\n\tif (tp->pdev->subsystem_vendor == PCI_VENDOR_ID_DELL &&\n\t    tg3_asic_rev(tp) == ASIC_REV_5701 &&\n\t    !(tp->phy_flags & TG3_PHYFLG_PHY_SERDES)) {\n\t\ttp->phy_flags |= TG3_PHYFLG_USE_MI_INTERRUPT;\n\t\ttg3_flag_set(tp, USE_LINKCHG_REG);\n\t}\n\n\t/* For all SERDES we poll the MAC status register. */\n\tif (tp->phy_flags & TG3_PHYFLG_PHY_SERDES)\n\t\ttg3_flag_set(tp, POLL_SERDES);\n\telse\n\t\ttg3_flag_clear(tp, POLL_SERDES);\n\n\ttp->rx_offset = NET_SKB_PAD + NET_IP_ALIGN;\n\ttp->rx_copy_thresh = TG3_RX_COPY_THRESHOLD;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5701 &&\n\t    tg3_flag(tp, PCIX_MODE)) {\n\t\ttp->rx_offset = NET_SKB_PAD;\n#ifndef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS\n\t\ttp->rx_copy_thresh = ~(u16)0;\n#endif\n\t}\n\n\ttp->rx_std_ring_mask = TG3_RX_STD_RING_SIZE(tp) - 1;\n\ttp->rx_jmb_ring_mask = TG3_RX_JMB_RING_SIZE(tp) - 1;\n\ttp->rx_ret_ring_mask = tg3_rx_ret_ring_size(tp) - 1;\n\n\ttp->rx_std_max_post = tp->rx_std_ring_mask + 1;\n\n\t/* Increment the rx prod index on the rx std ring by at most\n\t * 8 for these chips to workaround hw errata.\n\t */\n\tif (tg3_asic_rev(tp) == ASIC_REV_5750 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5752 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5755)\n\t\ttp->rx_std_max_post = 8;\n\n\tif (tg3_flag(tp, ASPM_WORKAROUND))\n\t\ttp->pwrmgmt_thresh = tr32(PCIE_PWR_MGMT_THRESH) &\n\t\t\t\t     PCIE_PWR_MGMT_L1_THRESH_MSK;\n\n\treturn err;\n}\n\n#ifdef CONFIG_SPARC\nstatic int tg3_get_macaddr_sparc(struct tg3 *tp)\n{\n\tstruct net_device *dev = tp->dev;\n\tstruct pci_dev *pdev = tp->pdev;\n\tstruct device_node *dp = pci_device_to_OF_node(pdev);\n\tconst unsigned char *addr;\n\tint len;\n\n\taddr = of_get_property(dp, \"local-mac-address\", &len);\n\tif (addr && len == 6) {\n\t\tmemcpy(dev->dev_addr, addr, 6);\n\t\treturn 0;\n\t}\n\treturn -ENODEV;\n}\n\nstatic int tg3_get_default_macaddr_sparc(struct tg3 *tp)\n{\n\tstruct net_device *dev = tp->dev;\n\n\tmemcpy(dev->dev_addr, idprom->id_ethaddr, 6);\n\treturn 0;\n}\n#endif\n\nstatic int tg3_get_device_address(struct tg3 *tp)\n{\n\tstruct net_device *dev = tp->dev;\n\tu32 hi, lo, mac_offset;\n\tint addr_ok = 0;\n\tint err;\n\n#ifdef CONFIG_SPARC\n\tif (!tg3_get_macaddr_sparc(tp))\n\t\treturn 0;\n#endif\n\n\tif (tg3_flag(tp, IS_SSB_CORE)) {\n\t\terr = ssb_gige_get_macaddr(tp->pdev, &dev->dev_addr[0]);\n\t\tif (!err && is_valid_ether_addr(&dev->dev_addr[0]))\n\t\t\treturn 0;\n\t}\n\n\tmac_offset = 0x7c;\n\tif (tg3_asic_rev(tp) == ASIC_REV_5704 ||\n\t    tg3_flag(tp, 5780_CLASS)) {\n\t\tif (tr32(TG3PCI_DUAL_MAC_CTRL) & DUAL_MAC_CTRL_ID)\n\t\t\tmac_offset = 0xcc;\n\t\tif (tg3_nvram_lock(tp))\n\t\t\ttw32_f(NVRAM_CMD, NVRAM_CMD_RESET);\n\t\telse\n\t\t\ttg3_nvram_unlock(tp);\n\t} else if (tg3_flag(tp, 5717_PLUS)) {\n\t\tif (tp->pci_fn & 1)\n\t\t\tmac_offset = 0xcc;\n\t\tif (tp->pci_fn > 1)\n\t\t\tmac_offset += 0x18c;\n\t} else if (tg3_asic_rev(tp) == ASIC_REV_5906)\n\t\tmac_offset = 0x10;\n\n\t/* First try to get it from MAC address mailbox. */\n\ttg3_read_mem(tp, NIC_SRAM_MAC_ADDR_HIGH_MBOX, &hi);\n\tif ((hi >> 16) == 0x484b) {\n\t\tdev->dev_addr[0] = (hi >>  8) & 0xff;\n\t\tdev->dev_addr[1] = (hi >>  0) & 0xff;\n\n\t\ttg3_read_mem(tp, NIC_SRAM_MAC_ADDR_LOW_MBOX, &lo);\n\t\tdev->dev_addr[2] = (lo >> 24) & 0xff;\n\t\tdev->dev_addr[3] = (lo >> 16) & 0xff;\n\t\tdev->dev_addr[4] = (lo >>  8) & 0xff;\n\t\tdev->dev_addr[5] = (lo >>  0) & 0xff;\n\n\t\t/* Some old bootcode may report a 0 MAC address in SRAM */\n\t\taddr_ok = is_valid_ether_addr(&dev->dev_addr[0]);\n\t}\n\tif (!addr_ok) {\n\t\t/* Next, try NVRAM. */\n\t\tif (!tg3_flag(tp, NO_NVRAM) &&\n\t\t    !tg3_nvram_read_be32(tp, mac_offset + 0, &hi) &&\n\t\t    !tg3_nvram_read_be32(tp, mac_offset + 4, &lo)) {\n\t\t\tmemcpy(&dev->dev_addr[0], ((char *)&hi) + 2, 2);\n\t\t\tmemcpy(&dev->dev_addr[2], (char *)&lo, sizeof(lo));\n\t\t}\n\t\t/* Finally just fetch it out of the MAC control regs. */\n\t\telse {\n\t\t\thi = tr32(MAC_ADDR_0_HIGH);\n\t\t\tlo = tr32(MAC_ADDR_0_LOW);\n\n\t\t\tdev->dev_addr[5] = lo & 0xff;\n\t\t\tdev->dev_addr[4] = (lo >> 8) & 0xff;\n\t\t\tdev->dev_addr[3] = (lo >> 16) & 0xff;\n\t\t\tdev->dev_addr[2] = (lo >> 24) & 0xff;\n\t\t\tdev->dev_addr[1] = hi & 0xff;\n\t\t\tdev->dev_addr[0] = (hi >> 8) & 0xff;\n\t\t}\n\t}\n\n\tif (!is_valid_ether_addr(&dev->dev_addr[0])) {\n#ifdef CONFIG_SPARC\n\t\tif (!tg3_get_default_macaddr_sparc(tp))\n\t\t\treturn 0;\n#endif\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#define BOUNDARY_SINGLE_CACHELINE\t1\n#define BOUNDARY_MULTI_CACHELINE\t2\n\nstatic u32 tg3_calc_dma_bndry(struct tg3 *tp, u32 val)\n{\n\tint cacheline_size;\n\tu8 byte;\n\tint goal;\n\n\tpci_read_config_byte(tp->pdev, PCI_CACHE_LINE_SIZE, &byte);\n\tif (byte == 0)\n\t\tcacheline_size = 1024;\n\telse\n\t\tcacheline_size = (int) byte * 4;\n\n\t/* On 5703 and later chips, the boundary bits have no\n\t * effect.\n\t */\n\tif (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5701 &&\n\t    !tg3_flag(tp, PCI_EXPRESS))\n\t\tgoto out;\n\n#if defined(CONFIG_PPC64) || defined(CONFIG_IA64) || defined(CONFIG_PARISC)\n\tgoal = BOUNDARY_MULTI_CACHELINE;\n#else\n#if defined(CONFIG_SPARC64) || defined(CONFIG_ALPHA)\n\tgoal = BOUNDARY_SINGLE_CACHELINE;\n#else\n\tgoal = 0;\n#endif\n#endif\n\n\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\tval = goal ? 0 : DMA_RWCTRL_DIS_CACHE_ALIGNMENT;\n\t\tgoto out;\n\t}\n\n\tif (!goal)\n\t\tgoto out;\n\n\t/* PCI controllers on most RISC systems tend to disconnect\n\t * when a device tries to burst across a cache-line boundary.\n\t * Therefore, letting tg3 do so just wastes PCI bandwidth.\n\t *\n\t * Unfortunately, for PCI-E there are only limited\n\t * write-side controls for this, and thus for reads\n\t * we will still get the disconnects.  We'll also waste\n\t * these PCI cycles for both read and write for chips\n\t * other than 5700 and 5701 which do not implement the\n\t * boundary bits.\n\t */\n\tif (tg3_flag(tp, PCIX_MODE) && !tg3_flag(tp, PCI_EXPRESS)) {\n\t\tswitch (cacheline_size) {\n\t\tcase 16:\n\t\tcase 32:\n\t\tcase 64:\n\t\tcase 128:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_128_PCIX |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_128_PCIX);\n\t\t\t} else {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_384_PCIX |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_384_PCIX);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 256:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_256_PCIX |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_256_PCIX);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_384_PCIX |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_384_PCIX);\n\t\t\tbreak;\n\t\t}\n\t} else if (tg3_flag(tp, PCI_EXPRESS)) {\n\t\tswitch (cacheline_size) {\n\t\tcase 16:\n\t\tcase 32:\n\t\tcase 64:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval &= ~DMA_RWCTRL_WRITE_BNDRY_DISAB_PCIE;\n\t\t\t\tval |= DMA_RWCTRL_WRITE_BNDRY_64_PCIE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 128:\n\t\tdefault:\n\t\t\tval &= ~DMA_RWCTRL_WRITE_BNDRY_DISAB_PCIE;\n\t\t\tval |= DMA_RWCTRL_WRITE_BNDRY_128_PCIE;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tswitch (cacheline_size) {\n\t\tcase 16:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_16 |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_16);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 32:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_32 |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 64:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_64 |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_64);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 128:\n\t\t\tif (goal == BOUNDARY_SINGLE_CACHELINE) {\n\t\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_128 |\n\t\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_128);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fallthrough */\n\t\tcase 256:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_256 |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_256);\n\t\t\tbreak;\n\t\tcase 512:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_512 |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_512);\n\t\t\tbreak;\n\t\tcase 1024:\n\t\tdefault:\n\t\t\tval |= (DMA_RWCTRL_READ_BNDRY_1024 |\n\t\t\t\tDMA_RWCTRL_WRITE_BNDRY_1024);\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\treturn val;\n}\n\nstatic int tg3_do_test_dma(struct tg3 *tp, u32 *buf, dma_addr_t buf_dma,\n\t\t\t   int size, int to_device)\n{\n\tstruct tg3_internal_buffer_desc test_desc;\n\tu32 sram_dma_descs;\n\tint i, ret;\n\n\tsram_dma_descs = NIC_SRAM_DMA_DESC_POOL_BASE;\n\n\ttw32(FTQ_RCVBD_COMP_FIFO_ENQDEQ, 0);\n\ttw32(FTQ_RCVDATA_COMP_FIFO_ENQDEQ, 0);\n\ttw32(RDMAC_STATUS, 0);\n\ttw32(WDMAC_STATUS, 0);\n\n\ttw32(BUFMGR_MODE, 0);\n\ttw32(FTQ_RESET, 0);\n\n\ttest_desc.addr_hi = ((u64) buf_dma) >> 32;\n\ttest_desc.addr_lo = buf_dma & 0xffffffff;\n\ttest_desc.nic_mbuf = 0x00002100;\n\ttest_desc.len = size;\n\n\t/*\n\t * HP ZX1 was seeing test failures for 5701 cards running at 33Mhz\n\t * the *second* time the tg3 driver was getting loaded after an\n\t * initial scan.\n\t *\n\t * Broadcom tells me:\n\t *   ...the DMA engine is connected to the GRC block and a DMA\n\t *   reset may affect the GRC block in some unpredictable way...\n\t *   The behavior of resets to individual blocks has not been tested.\n\t *\n\t * Broadcom noted the GRC reset will also reset all sub-components.\n\t */\n\tif (to_device) {\n\t\ttest_desc.cqid_sqid = (13 << 8) | 2;\n\n\t\ttw32_f(RDMAC_MODE, RDMAC_MODE_ENABLE);\n\t\tudelay(40);\n\t} else {\n\t\ttest_desc.cqid_sqid = (16 << 8) | 7;\n\n\t\ttw32_f(WDMAC_MODE, WDMAC_MODE_ENABLE);\n\t\tudelay(40);\n\t}\n\ttest_desc.flags = 0x00000005;\n\n\tfor (i = 0; i < (sizeof(test_desc) / sizeof(u32)); i++) {\n\t\tu32 val;\n\n\t\tval = *(((u32 *)&test_desc) + i);\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR,\n\t\t\t\t       sram_dma_descs + (i * sizeof(u32)));\n\t\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_DATA, val);\n\t}\n\tpci_write_config_dword(tp->pdev, TG3PCI_MEM_WIN_BASE_ADDR, 0);\n\n\tif (to_device)\n\t\ttw32(FTQ_DMA_HIGH_READ_FIFO_ENQDEQ, sram_dma_descs);\n\telse\n\t\ttw32(FTQ_DMA_HIGH_WRITE_FIFO_ENQDEQ, sram_dma_descs);\n\n\tret = -ENODEV;\n\tfor (i = 0; i < 40; i++) {\n\t\tu32 val;\n\n\t\tif (to_device)\n\t\t\tval = tr32(FTQ_RCVBD_COMP_FIFO_ENQDEQ);\n\t\telse\n\t\t\tval = tr32(FTQ_RCVDATA_COMP_FIFO_ENQDEQ);\n\t\tif ((val & 0xffff) == sram_dma_descs) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tudelay(100);\n\t}\n\n\treturn ret;\n}\n\n#define TEST_BUFFER_SIZE\t0x2000\n\nstatic DEFINE_PCI_DEVICE_TABLE(tg3_dma_wait_state_chipsets) = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_UNI_N_PCI15) },\n\t{ },\n};\n\nstatic int tg3_test_dma(struct tg3 *tp)\n{\n\tdma_addr_t buf_dma;\n\tu32 *buf, saved_dma_rwctrl;\n\tint ret = 0;\n\n\tbuf = dma_alloc_coherent(&tp->pdev->dev, TEST_BUFFER_SIZE,\n\t\t\t\t &buf_dma, GFP_KERNEL);\n\tif (!buf) {\n\t\tret = -ENOMEM;\n\t\tgoto out_nofree;\n\t}\n\n\ttp->dma_rwctrl = ((0x7 << DMA_RWCTRL_PCI_WRITE_CMD_SHIFT) |\n\t\t\t  (0x6 << DMA_RWCTRL_PCI_READ_CMD_SHIFT));\n\n\ttp->dma_rwctrl = tg3_calc_dma_bndry(tp, tp->dma_rwctrl);\n\n\tif (tg3_flag(tp, 57765_PLUS))\n\t\tgoto out;\n\n\tif (tg3_flag(tp, PCI_EXPRESS)) {\n\t\t/* DMA read watermark not used on PCIE */\n\t\ttp->dma_rwctrl |= 0x00180000;\n\t} else if (!tg3_flag(tp, PCIX_MODE)) {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5705 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5750)\n\t\t\ttp->dma_rwctrl |= 0x003f0000;\n\t\telse\n\t\t\ttp->dma_rwctrl |= 0x003f000f;\n\t} else {\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5704) {\n\t\t\tu32 ccval = (tr32(TG3PCI_CLOCK_CTRL) & 0x1f);\n\t\t\tu32 read_water = 0x7;\n\n\t\t\t/* If the 5704 is behind the EPB bridge, we can\n\t\t\t * do the less restrictive ONE_DMA workaround for\n\t\t\t * better performance.\n\t\t\t */\n\t\t\tif (tg3_flag(tp, 40BIT_DMA_BUG) &&\n\t\t\t    tg3_asic_rev(tp) == ASIC_REV_5704)\n\t\t\t\ttp->dma_rwctrl |= 0x8000;\n\t\t\telse if (ccval == 0x6 || ccval == 0x7)\n\t\t\t\ttp->dma_rwctrl |= DMA_RWCTRL_ONE_DMA;\n\n\t\t\tif (tg3_asic_rev(tp) == ASIC_REV_5703)\n\t\t\t\tread_water = 4;\n\t\t\t/* Set bit 23 to enable PCIX hw bug fix */\n\t\t\ttp->dma_rwctrl |=\n\t\t\t\t(read_water << DMA_RWCTRL_READ_WATER_SHIFT) |\n\t\t\t\t(0x3 << DMA_RWCTRL_WRITE_WATER_SHIFT) |\n\t\t\t\t(1 << 23);\n\t\t} else if (tg3_asic_rev(tp) == ASIC_REV_5780) {\n\t\t\t/* 5780 always in PCIX mode */\n\t\t\ttp->dma_rwctrl |= 0x00144000;\n\t\t} else if (tg3_asic_rev(tp) == ASIC_REV_5714) {\n\t\t\t/* 5714 always in PCIX mode */\n\t\t\ttp->dma_rwctrl |= 0x00148000;\n\t\t} else {\n\t\t\ttp->dma_rwctrl |= 0x001b000f;\n\t\t}\n\t}\n\tif (tg3_flag(tp, ONE_DMA_AT_ONCE))\n\t\ttp->dma_rwctrl |= DMA_RWCTRL_ONE_DMA;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5703 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5704)\n\t\ttp->dma_rwctrl &= 0xfffffff0;\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5700 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5701) {\n\t\t/* Remove this if it causes problems for some boards. */\n\t\ttp->dma_rwctrl |= DMA_RWCTRL_USE_MEM_READ_MULT;\n\n\t\t/* On 5700/5701 chips, we need to set this bit.\n\t\t * Otherwise the chip will issue cacheline transactions\n\t\t * to streamable DMA memory with not all the byte\n\t\t * enables turned on.  This is an error on several\n\t\t * RISC PCI controllers, in particular sparc64.\n\t\t *\n\t\t * On 5703/5704 chips, this bit has been reassigned\n\t\t * a different meaning.  In particular, it is used\n\t\t * on those chips to enable a PCI-X workaround.\n\t\t */\n\t\ttp->dma_rwctrl |= DMA_RWCTRL_ASSERT_ALL_BE;\n\t}\n\n\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\n#if 0\n\t/* Unneeded, already done by tg3_get_invariants.  */\n\ttg3_switch_clocks(tp);\n#endif\n\n\tif (tg3_asic_rev(tp) != ASIC_REV_5700 &&\n\t    tg3_asic_rev(tp) != ASIC_REV_5701)\n\t\tgoto out;\n\n\t/* It is best to perform DMA test with maximum write burst size\n\t * to expose the 5700/5701 write DMA bug.\n\t */\n\tsaved_dma_rwctrl = tp->dma_rwctrl;\n\ttp->dma_rwctrl &= ~DMA_RWCTRL_WRITE_BNDRY_MASK;\n\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\n\twhile (1) {\n\t\tu32 *p = buf, i;\n\n\t\tfor (i = 0; i < TEST_BUFFER_SIZE / sizeof(u32); i++)\n\t\t\tp[i] = i;\n\n\t\t/* Send the buffer to the chip. */\n\t\tret = tg3_do_test_dma(tp, buf, buf_dma, TEST_BUFFER_SIZE, 1);\n\t\tif (ret) {\n\t\t\tdev_err(&tp->pdev->dev,\n\t\t\t\t\"%s: Buffer write failed. err = %d\\n\",\n\t\t\t\t__func__, ret);\n\t\t\tbreak;\n\t\t}\n\n#if 0\n\t\t/* validate data reached card RAM correctly. */\n\t\tfor (i = 0; i < TEST_BUFFER_SIZE / sizeof(u32); i++) {\n\t\t\tu32 val;\n\t\t\ttg3_read_mem(tp, 0x2100 + (i*4), &val);\n\t\t\tif (le32_to_cpu(val) != p[i]) {\n\t\t\t\tdev_err(&tp->pdev->dev,\n\t\t\t\t\t\"%s: Buffer corrupted on device! \"\n\t\t\t\t\t\"(%d != %d)\\n\", __func__, val, i);\n\t\t\t\t/* ret = -ENODEV here? */\n\t\t\t}\n\t\t\tp[i] = 0;\n\t\t}\n#endif\n\t\t/* Now read it back. */\n\t\tret = tg3_do_test_dma(tp, buf, buf_dma, TEST_BUFFER_SIZE, 0);\n\t\tif (ret) {\n\t\t\tdev_err(&tp->pdev->dev, \"%s: Buffer read failed. \"\n\t\t\t\t\"err = %d\\n\", __func__, ret);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Verify it. */\n\t\tfor (i = 0; i < TEST_BUFFER_SIZE / sizeof(u32); i++) {\n\t\t\tif (p[i] == i)\n\t\t\t\tcontinue;\n\n\t\t\tif ((tp->dma_rwctrl & DMA_RWCTRL_WRITE_BNDRY_MASK) !=\n\t\t\t    DMA_RWCTRL_WRITE_BNDRY_16) {\n\t\t\t\ttp->dma_rwctrl &= ~DMA_RWCTRL_WRITE_BNDRY_MASK;\n\t\t\t\ttp->dma_rwctrl |= DMA_RWCTRL_WRITE_BNDRY_16;\n\t\t\t\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tdev_err(&tp->pdev->dev,\n\t\t\t\t\t\"%s: Buffer corrupted on read back! \"\n\t\t\t\t\t\"(%d != %d)\\n\", __func__, p[i], i);\n\t\t\t\tret = -ENODEV;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (i == (TEST_BUFFER_SIZE / sizeof(u32))) {\n\t\t\t/* Success. */\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif ((tp->dma_rwctrl & DMA_RWCTRL_WRITE_BNDRY_MASK) !=\n\t    DMA_RWCTRL_WRITE_BNDRY_16) {\n\t\t/* DMA test passed without adjusting DMA boundary,\n\t\t * now look for chipsets that are known to expose the\n\t\t * DMA bug without failing the test.\n\t\t */\n\t\tif (pci_dev_present(tg3_dma_wait_state_chipsets)) {\n\t\t\ttp->dma_rwctrl &= ~DMA_RWCTRL_WRITE_BNDRY_MASK;\n\t\t\ttp->dma_rwctrl |= DMA_RWCTRL_WRITE_BNDRY_16;\n\t\t} else {\n\t\t\t/* Safe to use the calculated DMA boundary. */\n\t\t\ttp->dma_rwctrl = saved_dma_rwctrl;\n\t\t}\n\n\t\ttw32(TG3PCI_DMA_RW_CTRL, tp->dma_rwctrl);\n\t}\n\nout:\n\tdma_free_coherent(&tp->pdev->dev, TEST_BUFFER_SIZE, buf, buf_dma);\nout_nofree:\n\treturn ret;\n}\n\nstatic void tg3_init_bufmgr_config(struct tg3 *tp)\n{\n\tif (tg3_flag(tp, 57765_PLUS)) {\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_5705;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_57765;\n\t\ttp->bufmgr_config.mbuf_high_water =\n\t\t\tDEFAULT_MB_HIGH_WATER_57765;\n\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water_jumbo =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_5705;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water_jumbo =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_JUMBO_57765;\n\t\ttp->bufmgr_config.mbuf_high_water_jumbo =\n\t\t\tDEFAULT_MB_HIGH_WATER_JUMBO_57765;\n\t} else if (tg3_flag(tp, 5705_PLUS)) {\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_5705;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_5705;\n\t\ttp->bufmgr_config.mbuf_high_water =\n\t\t\tDEFAULT_MB_HIGH_WATER_5705;\n\t\tif (tg3_asic_rev(tp) == ASIC_REV_5906) {\n\t\t\ttp->bufmgr_config.mbuf_mac_rx_low_water =\n\t\t\t\tDEFAULT_MB_MACRX_LOW_WATER_5906;\n\t\t\ttp->bufmgr_config.mbuf_high_water =\n\t\t\t\tDEFAULT_MB_HIGH_WATER_5906;\n\t\t}\n\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water_jumbo =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_JUMBO_5780;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water_jumbo =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_JUMBO_5780;\n\t\ttp->bufmgr_config.mbuf_high_water_jumbo =\n\t\t\tDEFAULT_MB_HIGH_WATER_JUMBO_5780;\n\t} else {\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER;\n\t\ttp->bufmgr_config.mbuf_high_water =\n\t\t\tDEFAULT_MB_HIGH_WATER;\n\n\t\ttp->bufmgr_config.mbuf_read_dma_low_water_jumbo =\n\t\t\tDEFAULT_MB_RDMA_LOW_WATER_JUMBO;\n\t\ttp->bufmgr_config.mbuf_mac_rx_low_water_jumbo =\n\t\t\tDEFAULT_MB_MACRX_LOW_WATER_JUMBO;\n\t\ttp->bufmgr_config.mbuf_high_water_jumbo =\n\t\t\tDEFAULT_MB_HIGH_WATER_JUMBO;\n\t}\n\n\ttp->bufmgr_config.dma_low_water = DEFAULT_DMA_LOW_WATER;\n\ttp->bufmgr_config.dma_high_water = DEFAULT_DMA_HIGH_WATER;\n}\n\nstatic char *tg3_phy_string(struct tg3 *tp)\n{\n\tswitch (tp->phy_id & TG3_PHY_ID_MASK) {\n\tcase TG3_PHY_ID_BCM5400:\treturn \"5400\";\n\tcase TG3_PHY_ID_BCM5401:\treturn \"5401\";\n\tcase TG3_PHY_ID_BCM5411:\treturn \"5411\";\n\tcase TG3_PHY_ID_BCM5701:\treturn \"5701\";\n\tcase TG3_PHY_ID_BCM5703:\treturn \"5703\";\n\tcase TG3_PHY_ID_BCM5704:\treturn \"5704\";\n\tcase TG3_PHY_ID_BCM5705:\treturn \"5705\";\n\tcase TG3_PHY_ID_BCM5750:\treturn \"5750\";\n\tcase TG3_PHY_ID_BCM5752:\treturn \"5752\";\n\tcase TG3_PHY_ID_BCM5714:\treturn \"5714\";\n\tcase TG3_PHY_ID_BCM5780:\treturn \"5780\";\n\tcase TG3_PHY_ID_BCM5755:\treturn \"5755\";\n\tcase TG3_PHY_ID_BCM5787:\treturn \"5787\";\n\tcase TG3_PHY_ID_BCM5784:\treturn \"5784\";\n\tcase TG3_PHY_ID_BCM5756:\treturn \"5722/5756\";\n\tcase TG3_PHY_ID_BCM5906:\treturn \"5906\";\n\tcase TG3_PHY_ID_BCM5761:\treturn \"5761\";\n\tcase TG3_PHY_ID_BCM5718C:\treturn \"5718C\";\n\tcase TG3_PHY_ID_BCM5718S:\treturn \"5718S\";\n\tcase TG3_PHY_ID_BCM57765:\treturn \"57765\";\n\tcase TG3_PHY_ID_BCM5719C:\treturn \"5719C\";\n\tcase TG3_PHY_ID_BCM5720C:\treturn \"5720C\";\n\tcase TG3_PHY_ID_BCM5762:\treturn \"5762C\";\n\tcase TG3_PHY_ID_BCM8002:\treturn \"8002/serdes\";\n\tcase 0:\t\t\treturn \"serdes\";\n\tdefault:\t\treturn \"unknown\";\n\t}\n}\n\nstatic char *tg3_bus_string(struct tg3 *tp, char *str)\n{\n\tif (tg3_flag(tp, PCI_EXPRESS)) {\n\t\tstrcpy(str, \"PCI Express\");\n\t\treturn str;\n\t} else if (tg3_flag(tp, PCIX_MODE)) {\n\t\tu32 clock_ctrl = tr32(TG3PCI_CLOCK_CTRL) & 0x1f;\n\n\t\tstrcpy(str, \"PCIX:\");\n\n\t\tif ((clock_ctrl == 7) ||\n\t\t    ((tr32(GRC_MISC_CFG) & GRC_MISC_CFG_BOARD_ID_MASK) ==\n\t\t     GRC_MISC_CFG_BOARD_ID_5704CIOBE))\n\t\t\tstrcat(str, \"133MHz\");\n\t\telse if (clock_ctrl == 0)\n\t\t\tstrcat(str, \"33MHz\");\n\t\telse if (clock_ctrl == 2)\n\t\t\tstrcat(str, \"50MHz\");\n\t\telse if (clock_ctrl == 4)\n\t\t\tstrcat(str, \"66MHz\");\n\t\telse if (clock_ctrl == 6)\n\t\t\tstrcat(str, \"100MHz\");\n\t} else {\n\t\tstrcpy(str, \"PCI:\");\n\t\tif (tg3_flag(tp, PCI_HIGH_SPEED))\n\t\t\tstrcat(str, \"66MHz\");\n\t\telse\n\t\t\tstrcat(str, \"33MHz\");\n\t}\n\tif (tg3_flag(tp, PCI_32BIT))\n\t\tstrcat(str, \":32-bit\");\n\telse\n\t\tstrcat(str, \":64-bit\");\n\treturn str;\n}\n\nstatic void tg3_init_coal(struct tg3 *tp)\n{\n\tstruct ethtool_coalesce *ec = &tp->coal;\n\n\tmemset(ec, 0, sizeof(*ec));\n\tec->cmd = ETHTOOL_GCOALESCE;\n\tec->rx_coalesce_usecs = LOW_RXCOL_TICKS;\n\tec->tx_coalesce_usecs = LOW_TXCOL_TICKS;\n\tec->rx_max_coalesced_frames = LOW_RXMAX_FRAMES;\n\tec->tx_max_coalesced_frames = LOW_TXMAX_FRAMES;\n\tec->rx_coalesce_usecs_irq = DEFAULT_RXCOAL_TICK_INT;\n\tec->tx_coalesce_usecs_irq = DEFAULT_TXCOAL_TICK_INT;\n\tec->rx_max_coalesced_frames_irq = DEFAULT_RXCOAL_MAXF_INT;\n\tec->tx_max_coalesced_frames_irq = DEFAULT_TXCOAL_MAXF_INT;\n\tec->stats_block_coalesce_usecs = DEFAULT_STAT_COAL_TICKS;\n\n\tif (tp->coalesce_mode & (HOSTCC_MODE_CLRTICK_RXBD |\n\t\t\t\t HOSTCC_MODE_CLRTICK_TXBD)) {\n\t\tec->rx_coalesce_usecs = LOW_RXCOL_TICKS_CLRTCKS;\n\t\tec->rx_coalesce_usecs_irq = DEFAULT_RXCOAL_TICK_INT_CLRTCKS;\n\t\tec->tx_coalesce_usecs = LOW_TXCOL_TICKS_CLRTCKS;\n\t\tec->tx_coalesce_usecs_irq = DEFAULT_TXCOAL_TICK_INT_CLRTCKS;\n\t}\n\n\tif (tg3_flag(tp, 5705_PLUS)) {\n\t\tec->rx_coalesce_usecs_irq = 0;\n\t\tec->tx_coalesce_usecs_irq = 0;\n\t\tec->stats_block_coalesce_usecs = 0;\n\t}\n}\n\nstatic int tg3_init_one(struct pci_dev *pdev,\n\t\t\t\t  const struct pci_device_id *ent)\n{\n\tstruct net_device *dev;\n\tstruct tg3 *tp;\n\tint i, err, pm_cap;\n\tu32 sndmbx, rcvmbx, intmbx;\n\tchar str[40];\n\tu64 dma_mask, persist_dma_mask;\n\tnetdev_features_t features = 0;\n\n\tprintk_once(KERN_INFO \"%s\\n\", version);\n\n\terr = pci_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot enable PCI device, aborting\\n\");\n\t\treturn err;\n\t}\n\n\terr = pci_request_regions(pdev, DRV_MODULE_NAME);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot obtain PCI resources, aborting\\n\");\n\t\tgoto err_out_disable_pdev;\n\t}\n\n\tpci_set_master(pdev);\n\n\t/* Find power-management capability. */\n\tpm_cap = pci_find_capability(pdev, PCI_CAP_ID_PM);\n\tif (pm_cap == 0) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Cannot find Power Management capability, aborting\\n\");\n\t\terr = -EIO;\n\t\tgoto err_out_free_res;\n\t}\n\n\terr = pci_set_power_state(pdev, PCI_D0);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Transition to D0 failed, aborting\\n\");\n\t\tgoto err_out_free_res;\n\t}\n\n\tdev = alloc_etherdev_mq(sizeof(*tp), TG3_IRQ_MAX_VECS);\n\tif (!dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_out_power_down;\n\t}\n\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\n\ttp = netdev_priv(dev);\n\ttp->pdev = pdev;\n\ttp->dev = dev;\n\ttp->pm_cap = pm_cap;\n\ttp->rx_mode = TG3_DEF_RX_MODE;\n\ttp->tx_mode = TG3_DEF_TX_MODE;\n\ttp->irq_sync = 1;\n\n\tif (tg3_debug > 0)\n\t\ttp->msg_enable = tg3_debug;\n\telse\n\t\ttp->msg_enable = TG3_DEF_MSG_ENABLE;\n\n\tif (pdev_is_ssb_gige_core(pdev)) {\n\t\ttg3_flag_set(tp, IS_SSB_CORE);\n\t\tif (ssb_gige_must_flush_posted_writes(pdev))\n\t\t\ttg3_flag_set(tp, FLUSH_POSTED_WRITES);\n\t\tif (ssb_gige_one_dma_at_once(pdev))\n\t\t\ttg3_flag_set(tp, ONE_DMA_AT_ONCE);\n\t\tif (ssb_gige_have_roboswitch(pdev))\n\t\t\ttg3_flag_set(tp, ROBOSWITCH);\n\t\tif (ssb_gige_is_rgmii(pdev))\n\t\t\ttg3_flag_set(tp, RGMII_MODE);\n\t}\n\n\t/* The word/byte swap controls here control register access byte\n\t * swapping.  DMA data byte swapping is controlled in the GRC_MODE\n\t * setting below.\n\t */\n\ttp->misc_host_ctrl =\n\t\tMISC_HOST_CTRL_MASK_PCI_INT |\n\t\tMISC_HOST_CTRL_WORD_SWAP |\n\t\tMISC_HOST_CTRL_INDIR_ACCESS |\n\t\tMISC_HOST_CTRL_PCISTATE_RW;\n\n\t/* The NONFRM (non-frame) byte/word swap controls take effect\n\t * on descriptor entries, anything which isn't packet data.\n\t *\n\t * The StrongARM chips on the board (one for tx, one for rx)\n\t * are running in big-endian mode.\n\t */\n\ttp->grc_mode = (GRC_MODE_WSWAP_DATA | GRC_MODE_BSWAP_DATA |\n\t\t\tGRC_MODE_WSWAP_NONFRM_DATA);\n#ifdef __BIG_ENDIAN\n\ttp->grc_mode |= GRC_MODE_BSWAP_NONFRM_DATA;\n#endif\n\tspin_lock_init(&tp->lock);\n\tspin_lock_init(&tp->indirect_lock);\n\tINIT_WORK(&tp->reset_task, tg3_reset_task);\n\n\ttp->regs = pci_ioremap_bar(pdev, BAR_0);\n\tif (!tp->regs) {\n\t\tdev_err(&pdev->dev, \"Cannot map device registers, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_out_free_dev;\n\t}\n\n\tif (tp->pdev->device == PCI_DEVICE_ID_TIGON3_5761 ||\n\t    tp->pdev->device == PCI_DEVICE_ID_TIGON3_5761E ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5761S ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5761SE ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5717_C ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5718 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5719 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5720 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5762 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5725 ||\n\t    tp->pdev->device == TG3PCI_DEVICE_TIGON3_5727) {\n\t\ttg3_flag_set(tp, ENABLE_APE);\n\t\ttp->aperegs = pci_ioremap_bar(pdev, BAR_2);\n\t\tif (!tp->aperegs) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Cannot map APE registers, aborting\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_out_iounmap;\n\t\t}\n\t}\n\n\ttp->rx_pending = TG3_DEF_RX_RING_PENDING;\n\ttp->rx_jumbo_pending = TG3_DEF_RX_JUMBO_RING_PENDING;\n\n\tdev->ethtool_ops = &tg3_ethtool_ops;\n\tdev->watchdog_timeo = TG3_TX_TIMEOUT;\n\tdev->netdev_ops = &tg3_netdev_ops;\n\tdev->irq = pdev->irq;\n\n\terr = tg3_get_invariants(tp, ent);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Problem fetching invariants of chip, aborting\\n\");\n\t\tgoto err_out_apeunmap;\n\t}\n\n\t/* The EPB bridge inside 5714, 5715, and 5780 and any\n\t * device behind the EPB cannot support DMA addresses > 40-bit.\n\t * On 64-bit systems with IOMMU, use 40-bit dma_mask.\n\t * On 64-bit systems without IOMMU, use 64-bit dma_mask and\n\t * do DMA address check in tg3_start_xmit().\n\t */\n\tif (tg3_flag(tp, IS_5788))\n\t\tpersist_dma_mask = dma_mask = DMA_BIT_MASK(32);\n\telse if (tg3_flag(tp, 40BIT_DMA_BUG)) {\n\t\tpersist_dma_mask = dma_mask = DMA_BIT_MASK(40);\n#ifdef CONFIG_HIGHMEM\n\t\tdma_mask = DMA_BIT_MASK(64);\n#endif\n\t} else\n\t\tpersist_dma_mask = dma_mask = DMA_BIT_MASK(64);\n\n\t/* Configure DMA attributes. */\n\tif (dma_mask > DMA_BIT_MASK(32)) {\n\t\terr = pci_set_dma_mask(pdev, dma_mask);\n\t\tif (!err) {\n\t\t\tfeatures |= NETIF_F_HIGHDMA;\n\t\t\terr = pci_set_consistent_dma_mask(pdev,\n\t\t\t\t\t\t\t  persist_dma_mask);\n\t\t\tif (err < 0) {\n\t\t\t\tdev_err(&pdev->dev, \"Unable to obtain 64 bit \"\n\t\t\t\t\t\"DMA for consistent allocations\\n\");\n\t\t\t\tgoto err_out_apeunmap;\n\t\t\t}\n\t\t}\n\t}\n\tif (err || dma_mask == DMA_BIT_MASK(32)) {\n\t\terr = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"No usable DMA configuration, aborting\\n\");\n\t\t\tgoto err_out_apeunmap;\n\t\t}\n\t}\n\n\ttg3_init_bufmgr_config(tp);\n\n\tfeatures |= NETIF_F_HW_VLAN_TX | NETIF_F_HW_VLAN_RX;\n\n\t/* 5700 B0 chips do not support checksumming correctly due\n\t * to hardware bugs.\n\t */\n\tif (tg3_chip_rev_id(tp) != CHIPREV_ID_5700_B0) {\n\t\tfeatures |= NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_RXCSUM;\n\n\t\tif (tg3_flag(tp, 5755_PLUS))\n\t\t\tfeatures |= NETIF_F_IPV6_CSUM;\n\t}\n\n\t/* TSO is on by default on chips that support hardware TSO.\n\t * Firmware TSO on older chips gives lower performance, so it\n\t * is off by default, but can be enabled using ethtool.\n\t */\n\tif ((tg3_flag(tp, HW_TSO_1) ||\n\t     tg3_flag(tp, HW_TSO_2) ||\n\t     tg3_flag(tp, HW_TSO_3)) &&\n\t    (features & NETIF_F_IP_CSUM))\n\t\tfeatures |= NETIF_F_TSO;\n\tif (tg3_flag(tp, HW_TSO_2) || tg3_flag(tp, HW_TSO_3)) {\n\t\tif (features & NETIF_F_IPV6_CSUM)\n\t\t\tfeatures |= NETIF_F_TSO6;\n\t\tif (tg3_flag(tp, HW_TSO_3) ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5761 ||\n\t\t    (tg3_asic_rev(tp) == ASIC_REV_5784 &&\n\t\t     tg3_chip_rev(tp) != CHIPREV_5784_AX) ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_5785 ||\n\t\t    tg3_asic_rev(tp) == ASIC_REV_57780)\n\t\t\tfeatures |= NETIF_F_TSO_ECN;\n\t}\n\n\tdev->features |= features;\n\tdev->vlan_features |= features;\n\n\t/*\n\t * Add loopback capability only for a subset of devices that support\n\t * MAC-LOOPBACK. Eventually this need to be enhanced to allow INT-PHY\n\t * loopback for the remaining devices.\n\t */\n\tif (tg3_asic_rev(tp) != ASIC_REV_5780 &&\n\t    !tg3_flag(tp, CPMU_PRESENT))\n\t\t/* Add the loopback capability */\n\t\tfeatures |= NETIF_F_LOOPBACK;\n\n\tdev->hw_features |= features;\n\n\tif (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A1 &&\n\t    !tg3_flag(tp, TSO_CAPABLE) &&\n\t    !(tr32(TG3PCI_PCISTATE) & PCISTATE_BUS_SPEED_HIGH)) {\n\t\ttg3_flag_set(tp, MAX_RXPEND_64);\n\t\ttp->rx_pending = 63;\n\t}\n\n\terr = tg3_get_device_address(tp);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Could not obtain valid ethernet address, aborting\\n\");\n\t\tgoto err_out_apeunmap;\n\t}\n\n\t/*\n\t * Reset chip in case UNDI or EFI driver did not shutdown\n\t * DMA self test will enable WDMAC and we'll see (spurious)\n\t * pending DMA on the PCI bus at that point.\n\t */\n\tif ((tr32(HOSTCC_MODE) & HOSTCC_MODE_ENABLE) ||\n\t    (tr32(WDMAC_MODE) & WDMAC_MODE_ENABLE)) {\n\t\ttw32(MEMARB_MODE, MEMARB_MODE_ENABLE);\n\t\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\t}\n\n\terr = tg3_test_dma(tp);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"DMA engine test failed, aborting\\n\");\n\t\tgoto err_out_apeunmap;\n\t}\n\n\tintmbx = MAILBOX_INTERRUPT_0 + TG3_64BIT_REG_LOW;\n\trcvmbx = MAILBOX_RCVRET_CON_IDX_0 + TG3_64BIT_REG_LOW;\n\tsndmbx = MAILBOX_SNDHOST_PROD_IDX_0 + TG3_64BIT_REG_LOW;\n\tfor (i = 0; i < tp->irq_max; i++) {\n\t\tstruct tg3_napi *tnapi = &tp->napi[i];\n\n\t\ttnapi->tp = tp;\n\t\ttnapi->tx_pending = TG3_DEF_TX_RING_PENDING;\n\n\t\ttnapi->int_mbox = intmbx;\n\t\tif (i <= 4)\n\t\t\tintmbx += 0x8;\n\t\telse\n\t\t\tintmbx += 0x4;\n\n\t\ttnapi->consmbox = rcvmbx;\n\t\ttnapi->prodmbox = sndmbx;\n\n\t\tif (i)\n\t\t\ttnapi->coal_now = HOSTCC_MODE_COAL_VEC1_NOW << (i - 1);\n\t\telse\n\t\t\ttnapi->coal_now = HOSTCC_MODE_NOW;\n\n\t\tif (!tg3_flag(tp, SUPPORT_MSIX))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If we support MSIX, we'll be using RSS.  If we're using\n\t\t * RSS, the first vector only handles link interrupts and the\n\t\t * remaining vectors handle rx and tx interrupts.  Reuse the\n\t\t * mailbox values for the next iteration.  The values we setup\n\t\t * above are still useful for the single vectored mode.\n\t\t */\n\t\tif (!i)\n\t\t\tcontinue;\n\n\t\trcvmbx += 0x8;\n\n\t\tif (sndmbx & 0x4)\n\t\t\tsndmbx -= 0x4;\n\t\telse\n\t\t\tsndmbx += 0xc;\n\t}\n\n\ttg3_init_coal(tp);\n\n\tpci_set_drvdata(pdev, dev);\n\n\tif (tg3_asic_rev(tp) == ASIC_REV_5719 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5720 ||\n\t    tg3_asic_rev(tp) == ASIC_REV_5762)\n\t\ttg3_flag_set(tp, PTP_CAPABLE);\n\n\tif (tg3_flag(tp, 5717_PLUS)) {\n\t\t/* Resume a low-power mode */\n\t\ttg3_frob_aux_power(tp, false);\n\t}\n\n\ttg3_timer_init(tp);\n\n\ttg3_carrier_off(tp);\n\n\terr = register_netdev(dev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot register net device, aborting\\n\");\n\t\tgoto err_out_apeunmap;\n\t}\n\n\tnetdev_info(dev, \"Tigon3 [partno(%s) rev %04x] (%s) MAC address %pM\\n\",\n\t\t    tp->board_part_number,\n\t\t    tg3_chip_rev_id(tp),\n\t\t    tg3_bus_string(tp, str),\n\t\t    dev->dev_addr);\n\n\tif (tp->phy_flags & TG3_PHYFLG_IS_CONNECTED) {\n\t\tstruct phy_device *phydev;\n\t\tphydev = tp->mdio_bus->phy_map[TG3_PHY_MII_ADDR];\n\t\tnetdev_info(dev,\n\t\t\t    \"attached PHY driver [%s] (mii_bus:phy_addr=%s)\\n\",\n\t\t\t    phydev->drv->name, dev_name(&phydev->dev));\n\t} else {\n\t\tchar *ethtype;\n\n\t\tif (tp->phy_flags & TG3_PHYFLG_10_100_ONLY)\n\t\t\tethtype = \"10/100Base-TX\";\n\t\telse if (tp->phy_flags & TG3_PHYFLG_ANY_SERDES)\n\t\t\tethtype = \"1000Base-SX\";\n\t\telse\n\t\t\tethtype = \"10/100/1000Base-T\";\n\n\t\tnetdev_info(dev, \"attached PHY is %s (%s Ethernet) \"\n\t\t\t    \"(WireSpeed[%d], EEE[%d])\\n\",\n\t\t\t    tg3_phy_string(tp), ethtype,\n\t\t\t    (tp->phy_flags & TG3_PHYFLG_NO_ETH_WIRE_SPEED) == 0,\n\t\t\t    (tp->phy_flags & TG3_PHYFLG_EEE_CAP) != 0);\n\t}\n\n\tnetdev_info(dev, \"RXcsums[%d] LinkChgREG[%d] MIirq[%d] ASF[%d] TSOcap[%d]\\n\",\n\t\t    (dev->features & NETIF_F_RXCSUM) != 0,\n\t\t    tg3_flag(tp, USE_LINKCHG_REG) != 0,\n\t\t    (tp->phy_flags & TG3_PHYFLG_USE_MI_INTERRUPT) != 0,\n\t\t    tg3_flag(tp, ENABLE_ASF) != 0,\n\t\t    tg3_flag(tp, TSO_CAPABLE) != 0);\n\tnetdev_info(dev, \"dma_rwctrl[%08x] dma_mask[%d-bit]\\n\",\n\t\t    tp->dma_rwctrl,\n\t\t    pdev->dma_mask == DMA_BIT_MASK(32) ? 32 :\n\t\t    ((u64)pdev->dma_mask) == DMA_BIT_MASK(40) ? 40 : 64);\n\n\tpci_save_state(pdev);\n\n\treturn 0;\n\nerr_out_apeunmap:\n\tif (tp->aperegs) {\n\t\tiounmap(tp->aperegs);\n\t\ttp->aperegs = NULL;\n\t}\n\nerr_out_iounmap:\n\tif (tp->regs) {\n\t\tiounmap(tp->regs);\n\t\ttp->regs = NULL;\n\t}\n\nerr_out_free_dev:\n\tfree_netdev(dev);\n\nerr_out_power_down:\n\tpci_set_power_state(pdev, PCI_D3hot);\n\nerr_out_free_res:\n\tpci_release_regions(pdev);\n\nerr_out_disable_pdev:\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\n\treturn err;\n}\n\nstatic void tg3_remove_one(struct pci_dev *pdev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\n\tif (dev) {\n\t\tstruct tg3 *tp = netdev_priv(dev);\n\n\t\trelease_firmware(tp->fw);\n\n\t\ttg3_reset_task_cancel(tp);\n\n\t\tif (tg3_flag(tp, USE_PHYLIB)) {\n\t\t\ttg3_phy_fini(tp);\n\t\t\ttg3_mdio_fini(tp);\n\t\t}\n\n\t\tunregister_netdev(dev);\n\t\tif (tp->aperegs) {\n\t\t\tiounmap(tp->aperegs);\n\t\t\ttp->aperegs = NULL;\n\t\t}\n\t\tif (tp->regs) {\n\t\t\tiounmap(tp->regs);\n\t\t\ttp->regs = NULL;\n\t\t}\n\t\tfree_netdev(dev);\n\t\tpci_release_regions(pdev);\n\t\tpci_disable_device(pdev);\n\t\tpci_set_drvdata(pdev, NULL);\n\t}\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int tg3_suspend(struct device *device)\n{\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\ttg3_reset_task_cancel(tp);\n\ttg3_phy_stop(tp);\n\ttg3_netif_stop(tp);\n\n\ttg3_timer_stop(tp);\n\n\ttg3_full_lock(tp, 1);\n\ttg3_disable_ints(tp);\n\ttg3_full_unlock(tp);\n\n\tnetif_device_detach(dev);\n\n\ttg3_full_lock(tp, 0);\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 1);\n\ttg3_flag_clear(tp, INIT_COMPLETE);\n\ttg3_full_unlock(tp);\n\n\terr = tg3_power_down_prepare(tp);\n\tif (err) {\n\t\tint err2;\n\n\t\ttg3_full_lock(tp, 0);\n\n\t\ttg3_flag_set(tp, INIT_COMPLETE);\n\t\terr2 = tg3_restart_hw(tp, 1);\n\t\tif (err2)\n\t\t\tgoto out;\n\n\t\ttg3_timer_start(tp);\n\n\t\tnetif_device_attach(dev);\n\t\ttg3_netif_start(tp);\n\nout:\n\t\ttg3_full_unlock(tp);\n\n\t\tif (!err2)\n\t\t\ttg3_phy_start(tp);\n\t}\n\n\treturn err;\n}\n\nstatic int tg3_resume(struct device *device)\n{\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(dev);\n\tint err;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tnetif_device_attach(dev);\n\n\ttg3_full_lock(tp, 0);\n\n\ttg3_flag_set(tp, INIT_COMPLETE);\n\terr = tg3_restart_hw(tp, 1);\n\tif (err)\n\t\tgoto out;\n\n\ttg3_timer_start(tp);\n\n\ttg3_netif_start(tp);\n\nout:\n\ttg3_full_unlock(tp);\n\n\tif (!err)\n\t\ttg3_phy_start(tp);\n\n\treturn err;\n}\n\nstatic SIMPLE_DEV_PM_OPS(tg3_pm_ops, tg3_suspend, tg3_resume);\n#define TG3_PM_OPS (&tg3_pm_ops)\n\n#else\n\n#define TG3_PM_OPS NULL\n\n#endif /* CONFIG_PM_SLEEP */\n\n/**\n * tg3_io_error_detected - called when PCI error is detected\n * @pdev: Pointer to PCI device\n * @state: The current pci connection state\n *\n * This function is called after a PCI bus error affecting\n * this device has been detected.\n */\nstatic pci_ers_result_t tg3_io_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t      pci_channel_state_t state)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(netdev);\n\tpci_ers_result_t err = PCI_ERS_RESULT_NEED_RESET;\n\n\tnetdev_info(netdev, \"PCI I/O error detected\\n\");\n\n\trtnl_lock();\n\n\tif (!netif_running(netdev))\n\t\tgoto done;\n\n\ttg3_phy_stop(tp);\n\n\ttg3_netif_stop(tp);\n\n\ttg3_timer_stop(tp);\n\n\t/* Want to make sure that the reset task doesn't run */\n\ttg3_reset_task_cancel(tp);\n\n\tnetif_device_detach(netdev);\n\n\t/* Clean up software state, even if MMIO is blocked */\n\ttg3_full_lock(tp, 0);\n\ttg3_halt(tp, RESET_KIND_SHUTDOWN, 0);\n\ttg3_full_unlock(tp);\n\ndone:\n\tif (state == pci_channel_io_perm_failure)\n\t\terr = PCI_ERS_RESULT_DISCONNECT;\n\telse\n\t\tpci_disable_device(pdev);\n\n\trtnl_unlock();\n\n\treturn err;\n}\n\n/**\n * tg3_io_slot_reset - called after the pci bus has been reset.\n * @pdev: Pointer to PCI device\n *\n * Restart the card from scratch, as if from a cold-boot.\n * At this point, the card has exprienced a hard reset,\n * followed by fixups by BIOS, and has its config space\n * set up identically to what it was at cold boot.\n */\nstatic pci_ers_result_t tg3_io_slot_reset(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(netdev);\n\tpci_ers_result_t rc = PCI_ERS_RESULT_DISCONNECT;\n\tint err;\n\n\trtnl_lock();\n\n\tif (pci_enable_device(pdev)) {\n\t\tnetdev_err(netdev, \"Cannot re-enable PCI device after reset.\\n\");\n\t\tgoto done;\n\t}\n\n\tpci_set_master(pdev);\n\tpci_restore_state(pdev);\n\tpci_save_state(pdev);\n\n\tif (!netif_running(netdev)) {\n\t\trc = PCI_ERS_RESULT_RECOVERED;\n\t\tgoto done;\n\t}\n\n\terr = tg3_power_up(tp);\n\tif (err)\n\t\tgoto done;\n\n\trc = PCI_ERS_RESULT_RECOVERED;\n\ndone:\n\trtnl_unlock();\n\n\treturn rc;\n}\n\n/**\n * tg3_io_resume - called when traffic can start flowing again.\n * @pdev: Pointer to PCI device\n *\n * This callback is called when the error recovery driver tells\n * us that its OK to resume normal operation.\n */\nstatic void tg3_io_resume(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct tg3 *tp = netdev_priv(netdev);\n\tint err;\n\n\trtnl_lock();\n\n\tif (!netif_running(netdev))\n\t\tgoto done;\n\n\ttg3_full_lock(tp, 0);\n\ttg3_flag_set(tp, INIT_COMPLETE);\n\terr = tg3_restart_hw(tp, 1);\n\tif (err) {\n\t\ttg3_full_unlock(tp);\n\t\tnetdev_err(netdev, \"Cannot restart hardware after reset.\\n\");\n\t\tgoto done;\n\t}\n\n\tnetif_device_attach(netdev);\n\n\ttg3_timer_start(tp);\n\n\ttg3_netif_start(tp);\n\n\ttg3_full_unlock(tp);\n\n\ttg3_phy_start(tp);\n\ndone:\n\trtnl_unlock();\n}\n\nstatic const struct pci_error_handlers tg3_err_handler = {\n\t.error_detected\t= tg3_io_error_detected,\n\t.slot_reset\t= tg3_io_slot_reset,\n\t.resume\t\t= tg3_io_resume\n};\n\nstatic struct pci_driver tg3_driver = {\n\t.name\t\t= DRV_MODULE_NAME,\n\t.id_table\t= tg3_pci_tbl,\n\t.probe\t\t= tg3_init_one,\n\t.remove\t\t= tg3_remove_one,\n\t.err_handler\t= &tg3_err_handler,\n\t.driver.pm\t= TG3_PM_OPS,\n};\n\nstatic int __init tg3_init(void)\n{\n\treturn pci_register_driver(&tg3_driver);\n}\n\nstatic void __exit tg3_cleanup(void)\n{\n\tpci_unregister_driver(&tg3_driver);\n}\n\nmodule_init(tg3_init);\nmodule_exit(tg3_cleanup);\n"], "filenames": ["drivers/net/ethernet/broadcom/tg3.c"], "buggy_code_start_loc": [14607], "buggy_code_end_loc": [14609], "fixing_code_start_loc": [14607], "fixing_code_end_loc": [14612], "type": "CWE-119", "message": "Heap-based buffer overflow in the tg3_read_vpd function in drivers/net/ethernet/broadcom/tg3.c in the Linux kernel before 3.8.6 allows physically proximate attackers to cause a denial of service (system crash) or possibly execute arbitrary code via crafted firmware that specifies a long string in the Vital Product Data (VPD) data structure.", "other": {"cve": {"id": "CVE-2013-1929", "sourceIdentifier": "secalert@redhat.com", "published": "2013-06-07T14:03:18.517", "lastModified": "2023-02-13T04:42:06.480", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Heap-based buffer overflow in the tg3_read_vpd function in drivers/net/ethernet/broadcom/tg3.c in the Linux kernel before 3.8.6 allows physically proximate attackers to cause a denial of service (system crash) or possibly execute arbitrary code via crafted firmware that specifies a long string in the Vital Product Data (VPD) data structure."}, {"lang": "es", "value": "Desbordamiento de b\u00fafer basado en memoria din\u00e1mica en la funci\u00f3n tg3_read_vpd en drivers/net/ethernet/broadcom/tg3.c en el kernel de Linux anterior a v3.8.3 que permite a a atacantes f\u00edsicamente cercanos causar una denegaci\u00f3n de servicios (ca\u00edda del sistema) o posiblemente ejecutar c\u00f3digo arbitrario a trav\u00e9s de firmware manipulado que especifica una cadena larga en la estructura de datos Vital Prduct Data (VPD)"}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.8.5", "matchCriteriaId": "53B3256C-E89F-417D-8852-98994504C2A4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.0:*:*:*:*:*:*:*", "matchCriteriaId": "1A6E41FB-38CE-49F2-B796-9A5AA648E73F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.1:*:*:*:*:*:*:*", "matchCriteriaId": "93523FE1-5993-46CB-9299-7C8C1A04E873"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.2:*:*:*:*:*:*:*", "matchCriteriaId": "27ADC356-6BE9-43A3-9E0B-393DC4B1559A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.3:*:*:*:*:*:*:*", "matchCriteriaId": "4F543D23-1774-4D14-A7D1-AD49EDEA94DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.4:*:*:*:*:*:*:*", "matchCriteriaId": "FC323F58-CA00-4C3C-BA4D-CC2C0A6E5F43"}]}]}], "references": [{"url": "http://cansecwest.com/slides/2013/PrivateCore%20CSW%202013.pdf", "source": "secalert@redhat.com", "tags": ["Exploit"]}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=715230a44310a8cf66fbfb5a46f9a62a9b2de424", "source": "secalert@redhat.com"}, {"url": "http://lists.fedoraproject.org/pipermail/package-announce/2013-April/101836.html", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2013-09/msg00003.html", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2013-09/msg00004.html", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-updates/2013-12/msg00129.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-1645.html", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.8.6", "source": "secalert@redhat.com"}, {"url": "http://www.mandriva.com/security/advisories?name=MDVSA-2013:176", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/04/06/3", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1834-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1835-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1836-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1838-1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=949932", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/715230a44310a8cf66fbfb5a46f9a62a9b2de424", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/715230a44310a8cf66fbfb5a46f9a62a9b2de424"}}