{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/array_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/type_index.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/gtl/array_slice.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\n// Forward declarations of functors that will be defined in tile_ops_impl.h\nnamespace functor {\ntemplate <typename Device, typename T, typename Tmultiple>\nstruct Tile {\n  void operator()(const Device& d, Tensor* out, const Tensor& in,\n                  const gtl::ArraySlice<Tmultiple> broadcast_array) const;\n};\n\ntemplate <typename Device, typename T, int NDIM>\nstruct TileGrad {\n  void operator()(const Device& d, typename TTypes<T, NDIM>::Tensor out,\n                  typename TTypes<T, NDIM>::ConstTensor in,\n                  const Eigen::DSizes<Eigen::DenseIndex, NDIM>& indices,\n                  const Eigen::DSizes<Eigen::DenseIndex, NDIM>& sizes,\n                  bool first) const;\n};\n\ntemplate <typename Device, typename T>\nstruct TileGrad<Device, T, 0> {\n  void operator()(const Device& d, typename TTypes<T, 0>::Tensor out,\n                  typename TTypes<T, 0>::ConstTensor in,\n                  const Eigen::DSizes<Eigen::DenseIndex, 0>&,\n                  const Eigen::DSizes<Eigen::DenseIndex, 0>&, bool first) const;\n};\n\ntemplate <typename Device, typename T, int NDIM, int REDUCEDNDIM>\nstruct ReduceAndReshape {\n  void operator()(\n      const Device& d, typename TTypes<T, NDIM>::Tensor out,\n      typename TTypes<T, NDIM>::ConstTensor in,\n      const Eigen::DSizes<Eigen::DenseIndex, REDUCEDNDIM>& reduce_dim,\n      const Eigen::DSizes<Eigen::DenseIndex, NDIM>& reshape_dim) const;\n};\n\n// Explicit instantiations are defined in tile_ops_{cpu,gpu}_impl.*,\n// below are their declarations.\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nextern template struct Tile<GPUDevice, bool, int32>;\nextern template struct Tile<GPUDevice, bool, int64_t>;\nextern template struct Tile<GPUDevice, float, int32>;\nextern template struct Tile<GPUDevice, float, int64_t>;\nextern template struct Tile<GPUDevice, double, int32>;\nextern template struct Tile<GPUDevice, double, int64_t>;\nextern template struct Tile<GPUDevice, complex64, int32>;\nextern template struct Tile<GPUDevice, complex64, int64_t>;\nextern template struct Tile<GPUDevice, complex128, int32>;\nextern template struct Tile<GPUDevice, complex128, int64_t>;\nextern template struct Tile<GPUDevice, Eigen::half, int32>;\nextern template struct Tile<GPUDevice, Eigen::half, int64_t>;\nextern template struct Tile<GPUDevice, int16, int32>;\nextern template struct Tile<GPUDevice, int16, int64_t>;\nextern template struct Tile<GPUDevice, int32, int32>;\nextern template struct Tile<GPUDevice, int32, int64_t>;\nextern template struct Tile<GPUDevice, int64_t, int32>;\nextern template struct Tile<GPUDevice, int64_t, int64_t>;\n#define DECLARE_CUDA_DIM(T, NDIM)                      \\\n  extern template struct TileGrad<GPUDevice, T, NDIM>; \\\n  extern template struct ReduceAndReshape<GPUDevice, T, NDIM, 1>\n#else  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define DECLARE_CUDA_DIM(T, NDIM)\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define DECLARE_TYPE(T)                             \\\n  extern template struct Tile<CPUDevice, T, int32>; \\\n  extern template struct Tile<CPUDevice, T, int64_t>;\nTF_CALL_bool(DECLARE_TYPE);\nTF_CALL_float(DECLARE_TYPE);\nTF_CALL_bfloat16(DECLARE_TYPE);\nTF_CALL_double(DECLARE_TYPE);\nTF_CALL_uint8(DECLARE_TYPE);\nTF_CALL_int32(DECLARE_TYPE);\nTF_CALL_int16(DECLARE_TYPE);\nTF_CALL_int64(DECLARE_TYPE);\nTF_CALL_uint32(DECLARE_TYPE);\nTF_CALL_uint64(DECLARE_TYPE);\nTF_CALL_half(DECLARE_TYPE);\nTF_CALL_complex64(DECLARE_TYPE);\nTF_CALL_complex128(DECLARE_TYPE);\nTF_CALL_tstring(DECLARE_TYPE);\nTF_CALL_variant(DECLARE_TYPE);\n#undef DECLARE_TYPE\n\n#define DECLARE_DIM(T, NDIM)                           \\\n  DECLARE_CUDA_DIM(T, NDIM);                           \\\n  extern template struct TileGrad<CPUDevice, T, NDIM>; \\\n  extern template struct ReduceAndReshape<CPUDevice, T, NDIM, 1>;\n\n#define DECLARE_TYPE(T) \\\n  DECLARE_DIM(T, 1)     \\\n  DECLARE_DIM(T, 2)     \\\n  DECLARE_DIM(T, 3)     \\\n  DECLARE_DIM(T, 4)     \\\n  DECLARE_DIM(T, 5)     \\\n  DECLARE_DIM(T, 6)     \\\n  DECLARE_DIM(T, 7)\nTF_CALL_float(DECLARE_TYPE);\nTF_CALL_bfloat16(DECLARE_TYPE);\nTF_CALL_double(DECLARE_TYPE);\nTF_CALL_int16(DECLARE_TYPE);\nTF_CALL_int32(DECLARE_TYPE);\nTF_CALL_int64(DECLARE_TYPE);\nTF_CALL_half(DECLARE_TYPE);\nTF_CALL_complex64(DECLARE_TYPE);\nTF_CALL_complex128(DECLARE_TYPE);\n#undef DECLARE_TYPE\n\n#undef DECLARE_DIM\n#undef DECLARE_CUDA_DIM\n\n}  // namespace functor\n\n// --------------------------------------------------------------------------\ntemplate <typename Device, typename Tmultiples>\nclass TileOp : public OpKernel {\n public:\n  explicit TileOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& multiples = context->input(1);\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(multiples.shape()),\n        errors::InvalidArgument(\"Expected multiples to be 1-D, but got shape \",\n                                multiples.shape().DebugString()));\n    OP_REQUIRES(context, input.dims() == multiples.NumElements(),\n                errors::InvalidArgument(\n                    \"Expected multiples argument to be a vector of length \",\n                    input.dims(), \" but got length \", multiples.dim_size(0)));\n    const int input_dims = input.dims();\n\n    // Eigen doesn't support scalars on the GPU, so handle 0-D specially\n    if (input_dims == 0) {\n      context->set_output(0, input);\n      return;\n    }\n\n    const gtl::ArraySlice<Tmultiples> multiples_array(\n        multiples.flat<Tmultiples>().data(), input_dims);\n    TensorShape output_shape;\n    for (int i = 0; i < input_dims; ++i) {\n      OP_REQUIRES(\n          context, multiples_array[i] >= 0,\n          errors::InvalidArgument(\"Expected multiples[\", i, \"] >= 0, but got \",\n                                  multiples_array[i]));\n      output_shape.AddDim(input.dim_size(i) * multiples_array[i]);\n    }\n    if (output_shape == input.shape()) {\n      context->set_output(0, input);\n      return;\n    }\n    Tensor* result = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &result));\n\n    // If there's no output, there's nothing to do.\n    if (output_shape.num_elements() == 0) return;\n\n#define HANDLE_TYPE(DT)                               \\\n  if (context->input(0).dtype() == DT) {              \\\n    HandleCase<DT>(context, multiples_array, result); \\\n    return;                                           \\\n  }\n\n#define HANDLE_TYPE_NAME(T) HANDLE_TYPE(DataTypeToEnum<T>::value)\n\n    // Invoke macro using TF_CALL_* so type-filtering for platform applies.\n    TF_CALL_bool(HANDLE_TYPE_NAME);\n    TF_CALL_bfloat16(HANDLE_TYPE_NAME);\n    TF_CALL_float(HANDLE_TYPE_NAME);\n    TF_CALL_double(HANDLE_TYPE_NAME);\n    TF_CALL_uint8(HANDLE_TYPE_NAME);\n    TF_CALL_int8(HANDLE_TYPE_NAME);\n    TF_CALL_int32(HANDLE_TYPE_NAME);\n    TF_CALL_int16(HANDLE_TYPE_NAME);\n    TF_CALL_int64(HANDLE_TYPE_NAME);\n    TF_CALL_uint32(HANDLE_TYPE_NAME);\n    TF_CALL_uint64(HANDLE_TYPE_NAME);\n    TF_CALL_half(HANDLE_TYPE_NAME);\n    TF_CALL_tstring(HANDLE_TYPE_NAME);  // when DEVICE=CPUDevice.\n    TF_CALL_complex64(HANDLE_TYPE_NAME);\n    TF_CALL_complex128(HANDLE_TYPE_NAME);\n    TF_CALL_variant(HANDLE_TYPE_NAME);  // when DEVICE=CPUDevice\n\n#undef HANDLE_TYPE_NAME\n#undef HANDLE_TYPE\n\n    OP_REQUIRES(\n        context, false,\n        errors::Unimplemented(\n            \"TileOp : The input data type is not supported, DataType : \",\n            DataTypeString(context->input(0).dtype()),\n            \", Dimension : \", input_dims));\n  }\n\n private:\n  template <DataType DT>\n  void HandleCaseImpl(OpKernelContext* context,\n                      const gtl::ArraySlice<Tmultiples> multiples_array,\n                      Tensor* result) {\n    typedef typename EnumToDataType<DT>::Type T;\n    functor::Tile<Device, T, Tmultiples>()(context->eigen_device<Device>(),\n                                           result, context->input(0),\n                                           multiples_array);\n  }\n\n  template <DataType DT>\n  void HandleCase(OpKernelContext* context,\n                  const gtl::ArraySlice<Tmultiples> multiples_array,\n                  Tensor* result);\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TileOp);\n};\n\ntemplate <typename Device, typename Tmultiples>\ntemplate <DataType DT>\ninline void TileOp<Device, Tmultiples>::HandleCase(\n    OpKernelContext* context, const gtl::ArraySlice<Tmultiples> multiples_array,\n    Tensor* result) {\n  // TODO(vrv): print out the device name if useful. Currently disabled to avoid\n  // having to use RTTI.\n  LOG(FATAL) << \"TileOp: Invalid combination of Device, DT: \"\n             // << typeid(Device).name() << \", \"\n             << DataTypeString(DT);\n}\n\n#define HANDLE_CASE(device, dtype, Tmultiples)                             \\\n  template <>                                                              \\\n  template <>                                                              \\\n  void TileOp<device, Tmultiples>::HandleCase<dtype>(                      \\\n      OpKernelContext * context,                                           \\\n      const gtl::ArraySlice<Tmultiples> multiples_array, Tensor* result) { \\\n    HandleCaseImpl<dtype>(context, multiples_array, result);               \\\n  }\n\n#define HANDLE_TYPE_NAME_CPU(T)                            \\\n  HANDLE_CASE(CPUDevice, DataTypeToEnum<T>::value, int32); \\\n  HANDLE_CASE(CPUDevice, DataTypeToEnum<T>::value, int64_t);\n\n#define HANDLE_TYPE_NAME_GPU(T)                            \\\n  HANDLE_CASE(GPUDevice, DataTypeToEnum<T>::value, int32); \\\n  HANDLE_CASE(GPUDevice, DataTypeToEnum<T>::value, int64_t);\n\nTF_CALL_bool(HANDLE_TYPE_NAME_CPU);\nTF_CALL_float(HANDLE_TYPE_NAME_CPU);\nTF_CALL_bfloat16(HANDLE_TYPE_NAME_CPU);\nTF_CALL_double(HANDLE_TYPE_NAME_CPU);\nTF_CALL_uint8(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int8(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int32(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int16(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_uint32(HANDLE_TYPE_NAME_CPU);\nTF_CALL_uint64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_half(HANDLE_TYPE_NAME_CPU);\nTF_CALL_complex64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_complex128(HANDLE_TYPE_NAME_CPU);\nTF_CALL_tstring(HANDLE_TYPE_NAME_CPU);\nTF_CALL_variant(HANDLE_TYPE_NAME_CPU);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nTF_CALL_bool(HANDLE_TYPE_NAME_GPU);\nTF_CALL_float(HANDLE_TYPE_NAME_GPU);\nTF_CALL_double(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int16(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int32(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int64(HANDLE_TYPE_NAME_GPU);\nTF_CALL_half(HANDLE_TYPE_NAME_GPU);\nTF_CALL_complex64(HANDLE_TYPE_NAME_GPU);\nTF_CALL_complex128(HANDLE_TYPE_NAME_GPU);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n#undef HANDLE_TYPE_NAME_CPU\n#undef HANDLE_TYPE_NAME_GPU\n#undef HANDLE_CASE\n\n// --------------------------------------------------------------------------\ntemplate <typename Device, typename Tmultiples>\nclass TileGradientOp : public OpKernel {\n public:\n  explicit TileGradientOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& multiples = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(multiples.shape()),\n        errors::InvalidArgument(\"Expected multiples to be 1-D, but got shape \",\n                                multiples.shape().DebugString()));\n    OP_REQUIRES(context, input.dims() == multiples.NumElements(),\n                errors::InvalidArgument(\n                    \"Expected multiples argument to be a vector of length \",\n                    input.dims(), \" but got length \", multiples.dim_size(0)));\n\n    const int input_dims = input.dims();\n\n    // Eigen doesn't support scalars on the GPU, so handle 0-D specially\n    if (input_dims == 0) {\n      context->set_output(0, input);\n      return;\n    }\n\n    const gtl::ArraySlice<Tmultiples> multiples_array(\n        multiples.flat<Tmultiples>().data(), input_dims);\n    TensorShape output_shape;\n    std::vector<Tmultiples> input_dim_size_vec;\n    for (int i = 0; i < input_dims; ++i) {\n      OP_REQUIRES(\n          context, multiples_array[i] > 0,\n          errors::InvalidArgument(\"Expected multiples[\", i, \"] > 0, but got \",\n                                  multiples_array[i]));\n      OP_REQUIRES(context, input.dim_size(i) % multiples_array[i] == 0,\n                  errors::InvalidArgument(\"Expected input_dim[\", i,\n                                          \"] to be divisible by multiples[\", i,\n                                          \"], but \", input.dim_size(i), \" % \",\n                                          multiples_array[i], \" != 0\"));\n      output_shape.AddDim(input.dim_size(i) / multiples_array[i]);\n      input_dim_size_vec.push_back(input.dim_size(i));\n    }\n    if (output_shape == input.shape()) {\n      context->set_output(0, input);\n      return;\n    }\n    Tensor* result = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &result));\n\n#define HANDLE_DIM(DT, NDIM)                                           \\\n  if (context->input(0).dtype() == DT && input_dims == NDIM) {         \\\n    HandleCase<DT, NDIM>(context, input_dim_size_vec, multiples_array, \\\n                         result);                                      \\\n    return;                                                            \\\n  }\n\n#define HANDLE_TYPE(T) \\\n  HANDLE_DIM(T, 1)     \\\n  HANDLE_DIM(T, 2)     \\\n  HANDLE_DIM(T, 3)     \\\n  HANDLE_DIM(T, 4)     \\\n  HANDLE_DIM(T, 5)     \\\n  HANDLE_DIM(T, 6)     \\\n  HANDLE_DIM(T, 7)\n\n#define HANDLE_TYPE_NAME(T) HANDLE_TYPE(DataTypeToEnum<T>::value)\n\n    TF_CALL_float(HANDLE_TYPE_NAME);\n    TF_CALL_double(HANDLE_TYPE_NAME);\n    TF_CALL_int32(HANDLE_TYPE_NAME);\n    TF_CALL_int16(HANDLE_TYPE_NAME);\n    TF_CALL_int64(HANDLE_TYPE_NAME);\n    TF_CALL_half(HANDLE_TYPE_NAME);\n    TF_CALL_bfloat16(HANDLE_TYPE_NAME);\n    TF_CALL_complex64(HANDLE_TYPE_NAME);\n    TF_CALL_complex128(HANDLE_TYPE_NAME);\n\n#undef HANDLE_TYPE_NAME\n#undef HANDLE_TYPE\n#undef HANDLE_DIM\n\n    OP_REQUIRES(context, false,\n                errors::Unimplemented(\"TileGradientOp : The input data type or \"\n                                      \"dimension is not supported, DataType : \",\n                                      DataTypeString(context->input(0).dtype()),\n                                      \", Dimension : \", input_dims));\n  }\n\n private:\n  template <DataType DT, int NDIM>\n  void HandleCase(OpKernelContext* context,\n                  const std::vector<Tmultiples>& input_dims,\n                  const gtl::ArraySlice<Tmultiples> multiples_array,\n                  Tensor* result);\n\n  template <DataType DT, int NDIM>\n  void HandleCaseImpl(OpKernelContext* context,\n                      const std::vector<Tmultiples>& input_dims,\n                      const gtl::ArraySlice<Tmultiples> multiples_array,\n                      Tensor* result) {\n    typedef typename EnumToDataType<DT>::Type T;\n\n    bool reduction_only = true;\n    std::vector<Tmultiples> reduction_dims;\n\n    for (int i = 0; i < NDIM; ++i) {\n      if (input_dims[i] > multiples_array[i] && multiples_array[i] > 1) {\n        reduction_only = false;\n        break;\n      } else {\n        if (multiples_array[i] == input_dims[i]) {\n          reduction_dims.push_back(i);\n        }\n      }\n    }\n\n    if (reduction_only) {\n#define HANDLE_DIM(D)                                            \\\n  if (reduction_dims.size() == (D)) {                            \\\n    HandleReduce<T, NDIM, (D)>(context, reduction_dims, result); \\\n    return;                                                      \\\n  }\n      // NOTE(keveman): Handling the most common case here.\n      // Adding more cases here would require more templating and code\n      // explosion. For instance, HANDLE_DIM(2) wouldn't make sense for NDIM=1.\n      HANDLE_DIM(1);\n\n// Fall through to the unoptimized version.\n#undef HANDLE_DIM\n    }\n\n    Eigen::DSizes<Eigen::DenseIndex, NDIM> indices;\n    Eigen::DSizes<Eigen::DenseIndex, NDIM> sizes;\n\n    // Accumulate slices along the dimensions into the output. The number of\n    // slices along dimension 'i' is simply the multiple along dimension 'i'\n    // passed to the original Tile op.\n    for (int i = 0; i < NDIM; ++i) {\n      sizes[i] = input_dims[i] / multiples_array[i];\n      indices[i] = 0;\n    }\n\n    bool first = true;\n    while (true) {\n      functor::TileGrad<Device, T, NDIM>()(\n          context->eigen_device<Device>(), result->tensor<T, NDIM>(),\n          context->input(0).tensor<T, NDIM>(), indices, sizes, first);\n      first = false;\n      // Increment the begin indices.\n      int i = 0;\n      while (i < NDIM && indices[i] / sizes[i] == multiples_array[i] - 1) {\n        indices[i] = 0;\n        ++i;\n      }\n      // We are finished if we have iterated to the maximum along all\n      // dimensions.\n      if (i == NDIM) {\n        break;\n      }\n      indices[i] += sizes[i];\n    }\n  }\n\n  template <typename T, int NDIM, int REDUCENDIM>\n  void HandleReduce(OpKernelContext* context,\n                    const std::vector<Tmultiples>& reduce_dim_in,\n                    Tensor* result) {\n    static_assert(NDIM >= REDUCENDIM, \"Too many reduced dimensions\");\n    Eigen::DSizes<Eigen::DenseIndex, REDUCENDIM> reduce_dim;\n    Eigen::DSizes<Eigen::DenseIndex, NDIM> reshape_dim;\n\n    for (int i = 0; i < REDUCENDIM; ++i) {\n      reduce_dim[i] = reduce_dim_in[i];\n    }\n\n    for (int i = 0; i < NDIM; ++i) {\n      reshape_dim[i] = result->dim_size(i);\n    }\n\n    functor::ReduceAndReshape<Device, T, NDIM, REDUCENDIM>()(\n        context->eigen_device<Device>(), result->tensor<T, NDIM>(),\n        context->input(0).tensor<T, NDIM>(), reduce_dim, reshape_dim);\n  }\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TileGradientOp);\n};\n\ntemplate <typename Device, typename Tmultiples>\ntemplate <DataType DT, int NDIM>\ninline void TileGradientOp<Device, Tmultiples>::HandleCase(\n    OpKernelContext* context, const std::vector<Tmultiples>& input_dims,\n    const gtl::ArraySlice<Tmultiples> multiples_array, Tensor* result) {\n  LOG(FATAL) << \"TileGradientOp: Invalid combination of Device, DT and NDIM: \"\n             << TypeIndex::Make<Device>().name() << \", \" << DataTypeString(DT)\n             << \", \" << NDIM;\n}\n\n#define HANDLE_CASE(device, T, dtype, Tmultiples, ndim)                        \\\n  template <>                                                                  \\\n  template <>                                                                  \\\n  void TileGradientOp<device, Tmultiples>::HandleCase<dtype, ndim>(            \\\n      OpKernelContext * context, const std::vector<Tmultiples>& input_dims,    \\\n      const gtl::ArraySlice<Tmultiples> multiples_array, Tensor* result) {     \\\n    HandleCaseImpl<dtype, ndim>(context, input_dims, multiples_array, result); \\\n  }\n\n// 0-D handled specially above\n#define HANDLE_CASE_DIM(device, T, dtype)    \\\n  HANDLE_CASE(device, T, dtype, int32, 1);   \\\n  HANDLE_CASE(device, T, dtype, int32, 2);   \\\n  HANDLE_CASE(device, T, dtype, int32, 3);   \\\n  HANDLE_CASE(device, T, dtype, int32, 4);   \\\n  HANDLE_CASE(device, T, dtype, int32, 5);   \\\n  HANDLE_CASE(device, T, dtype, int32, 6);   \\\n  HANDLE_CASE(device, T, dtype, int32, 7);   \\\n  HANDLE_CASE(device, T, dtype, int64_t, 1); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 2); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 3); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 4); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 5); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 6); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 7);\n\n#define HANDLE_TYPE_NAME_CPU(T) \\\n  HANDLE_CASE_DIM(CPUDevice, T, DataTypeToEnum<T>::value);\n\n#define HANDLE_TYPE_NAME_GPU(T) \\\n  HANDLE_CASE_DIM(GPUDevice, T, DataTypeToEnum<T>::value);\n\nTF_CALL_float(HANDLE_TYPE_NAME_CPU);\nTF_CALL_double(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int16(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int32(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_half(HANDLE_TYPE_NAME_CPU);\nTF_CALL_complex64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_complex128(HANDLE_TYPE_NAME_CPU);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nTF_CALL_float(HANDLE_TYPE_NAME_GPU);\nTF_CALL_double(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int16(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int32(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int64(HANDLE_TYPE_NAME_GPU);\nTF_CALL_half(HANDLE_TYPE_NAME_GPU);\nTF_CALL_complex64(HANDLE_TYPE_NAME_GPU);\nTF_CALL_complex128(HANDLE_TYPE_NAME_GPU);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n#undef HANDLE_TYPE_NAME_CPU\n#undef HANDLE_TYPE_NAME_GPU\n#undef HANDLE_CASE_DIM\n#undef HANDLE_CASE\n\nREGISTER_KERNEL_BUILDER(Name(\"Tile\")\n                            .Device(DEVICE_CPU)\n                            .HostMemory(\"multiples\")\n                            .TypeConstraint<int32>(\"Tmultiples\"),\n                        TileOp<CPUDevice, int32>);\nREGISTER_KERNEL_BUILDER(Name(\"Tile\")\n                            .Device(DEVICE_CPU)\n                            .HostMemory(\"multiples\")\n                            .TypeConstraint<int64_t>(\"Tmultiples\"),\n                        TileOp<CPUDevice, int64>);\nREGISTER_KERNEL_BUILDER(Name(\"TileGrad\")\n                            .Device(DEVICE_CPU)\n                            .HostMemory(\"multiples\")\n                            .TypeConstraint<int32>(\"Tmultiples\"),\n                        TileGradientOp<CPUDevice, int32>);\nREGISTER_KERNEL_BUILDER(Name(\"TileGrad\")\n                            .Device(DEVICE_CPU)\n                            .HostMemory(\"multiples\")\n                            .TypeConstraint<int64_t>(\"Tmultiples\"),\n                        TileGradientOp<CPUDevice, int64>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_TILE(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"Tile\")                               \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<type>(\"T\")             \\\n                              .TypeConstraint<int32>(\"Tmultiples\")   \\\n                              .HostMemory(\"multiples\"),              \\\n                          TileOp<GPUDevice, int32>);                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"Tile\")                               \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<type>(\"T\")             \\\n                              .TypeConstraint<int64_t>(\"Tmultiples\") \\\n                              .HostMemory(\"multiples\"),              \\\n                          TileOp<GPUDevice, int64>);\n\n#define REGISTER_GPU_TILE_GRAD(type)                                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TileGrad\")                           \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<type>(\"T\")             \\\n                              .TypeConstraint<int32>(\"Tmultiples\")   \\\n                              .HostMemory(\"multiples\"),              \\\n                          TileGradientOp<GPUDevice, int32>);         \\\n  REGISTER_KERNEL_BUILDER(Name(\"TileGrad\")                           \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<type>(\"T\")             \\\n                              .TypeConstraint<int64_t>(\"Tmultiples\") \\\n                              .HostMemory(\"multiples\"),              \\\n                          TileGradientOp<GPUDevice, int64>);\n\n#define REGISTER_GPU(type) \\\n  REGISTER_GPU_TILE(type); \\\n  REGISTER_GPU_TILE_GRAD(type);\n\nTF_CALL_bool(REGISTER_GPU_TILE);\nTF_CALL_float(REGISTER_GPU);\nTF_CALL_double(REGISTER_GPU);\nTF_CALL_half(REGISTER_GPU);\nTF_CALL_int16(REGISTER_GPU);\nTF_CALL_int32(REGISTER_GPU);\nTF_CALL_int64(REGISTER_GPU);\nTF_CALL_complex64(REGISTER_GPU);\nTF_CALL_complex128(REGISTER_GPU)\n\n#undef REGISTER_GPU_TILE\n#undef REGISTER_GPU_TILE_GRAD\n#undef REGISTER_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for various tensorflow.ops.tf.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass ShapeOpsTest(test.TestCase):\n\n  def _compareShape(self, x, use_gpu=False):\n    np_ans = np.array(np.shape(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.shape(x)\n      tf_ans_64 = array_ops.shape(x, out_type=dtypes.int64)\n      result = self.evaluate(tf_ans)\n      result_64 = self.evaluate(tf_ans_64)\n    self.assertAllEqual(np_ans, result)\n    self.assertAllEqual(np_ans, result_64)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareShapeSparse(self, x_np, use_gpu=False):\n    np_ans = np.array(np.shape(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.shape(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareShapeN(self, x, use_gpu=False):\n    np_ans = np.array(np.shape(x))\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      tf_ans = array_ops.shape_n([x, x, x])\n      tf_ans_64 = array_ops.shape_n([x, x, x], out_type=dtypes.int64)\n      result = self.evaluate(tf_ans)\n      result_64 = self.evaluate(tf_ans_64)\n    for i in range(3):\n      self.assertAllEqual(np_ans, result[i])\n      self.assertAllEqual(np_ans, result_64[i])\n      self.assertShapeEqual(np_ans, tf_ans[i])\n\n  def _compareRank(self, x, use_gpu=False):\n    np_ans = np.asarray(np.ndim(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.rank(x)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareRankSparse(self, x_np, use_gpu=False):\n    np_ans = np.asarray(np.ndim(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.rank(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareSize(self, x, use_gpu=False):\n    np_ans = np.asarray(np.size(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.size(x)\n      result = self.evaluate(tf_ans)\n      tf_ans_64 = array_ops.size(x, out_type=dtypes.int64)\n      result_64 = self.evaluate(tf_ans_64)\n    self.assertAllEqual(np_ans, result)\n    self.assertAllEqual(np_ans, result_64)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareSizeSparse(self, x_np, use_gpu=False):\n    np_ans = np.asarray(np.size(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.size(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _testCpu(self, x):\n    self._compareShape(x, use_gpu=False)\n    self._compareShapeN(x, use_gpu=False)\n    self._compareRank(x, use_gpu=False)\n    self._compareSize(x, use_gpu=False)\n    self._compareShapeSparse(x, use_gpu=False)\n    self._compareRankSparse(x, use_gpu=False)\n    self._compareSizeSparse(x, use_gpu=False)\n\n  def _testGpu(self, x):\n    self._compareShape(x, use_gpu=True)\n    self._compareShapeN(x, use_gpu=True)\n    self._compareRank(x, use_gpu=True)\n    self._compareSize(x, use_gpu=True)\n    self._compareShapeSparse(x, use_gpu=True)\n    self._compareRankSparse(x, use_gpu=True)\n    self._compareSizeSparse(x, use_gpu=True)\n\n  def _testAll(self, x):\n    self._testCpu(x)\n    self._testGpu(x)\n\n  def testBasic(self):\n    self._testAll(np.random.randn(2))\n    self._testAll(np.random.randn(2, 3))\n    self._testAll(np.random.randn(2, 3, 5))\n    self._testAll(np.random.randn(2, 3, 5, 7))\n    self._testAll(np.random.randn(2, 3, 5, 7, 11))\n    self._testAll(np.random.randn(2, 3, 5, 7, 11, 13))\n\n  def testBool(self):\n    self._testAll(np.random.choice((False, True), size=(2,)))\n    self._testAll(np.random.choice((False, True), size=(2, 3)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7, 11)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7, 11, 13)))\n\n  # Disabled because it takes too long to run, but manually verified\n  # as passing at time of writing.\n  def _test64BitOutput(self):\n    with self.cached_session():\n      inp = array_ops.zeros([2**31])\n      num_elements = array_ops.size_internal(\n          inp, optimize=False, out_type=dtypes.int64)\n      self.assertEqual(2**31, self.evaluate(num_elements))\n\n    # Too large for tf.int32 output.\n    with self.assertRaises(errors_impl.InvalidArgumentError):\n      with self.cached_session():\n        inp = array_ops.zeros([2**31])\n        num_elements = array_ops.size_internal(\n            inp, optimize=False, out_type=dtypes.int32)\n        self.assertEqual(2**31, self.evaluate(num_elements))\n\n  def _compareExpandDims(self, x, dim, use_gpu):\n    np_ans = np.expand_dims(x, axis=dim)\n    with self.cached_session(use_gpu=use_gpu):\n      tensor = array_ops.expand_dims(x, dim)\n      tf_ans = self.evaluate(tensor)\n    self.assertShapeEqual(np_ans, tensor)\n    self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareExpandDimsAll(self, x, dim):\n    self._compareExpandDims(x, dim, False)\n    self._compareExpandDims(x, dim, True)\n\n  def testExpandDims(self):\n    self._compareExpandDimsAll(np.zeros([2]), 0)\n    self._compareExpandDimsAll(np.zeros([2]), 1)\n    self._compareExpandDimsAll(np.zeros([2]), -1)\n\n    self._compareExpandDimsAll(np.zeros([2, 3]), 0)\n    self._compareExpandDimsAll(np.zeros([2, 3]), 1)\n    self._compareExpandDimsAll(np.zeros([2, 3]), 2)\n    self._compareExpandDimsAll(np.zeros([2, 3]), -1)\n    self._compareExpandDimsAll(np.zeros([2, 3]), -2)\n\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 0)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 1)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 2)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 3)\n\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -1)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -2)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -3)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -4)\n\n  def testExpandDimsBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    self._compareExpandDimsAll(choice([2]), 0)\n    self._compareExpandDimsAll(choice([2]), 1)\n    self._compareExpandDimsAll(choice([2]), -1)\n\n    self._compareExpandDimsAll(choice([2, 3]), 0)\n    self._compareExpandDimsAll(choice([2, 3]), 1)\n    self._compareExpandDimsAll(choice([2, 3]), 2)\n    self._compareExpandDimsAll(choice([2, 3]), -1)\n    self._compareExpandDimsAll(choice([2, 3]), -2)\n\n    self._compareExpandDimsAll(choice([2, 3, 5]), 0)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 1)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 2)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 3)\n\n    self._compareExpandDimsAll(choice([2, 3, 5]), -1)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -2)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -3)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -4)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsErrors(self):\n    with self.cached_session():\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        np.zeros([2, 3, 5]), -5)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        [False, True, True], -5)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        np.zeros([2, 3, 5]), 4)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        [False, True, True], 4)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsGradient(self):\n    with self.cached_session():\n      inp = constant_op.constant(\n          np.random.rand(4, 2).astype(\"f\"), dtype=dtypes.float32)\n      squeezed = array_ops.expand_dims(inp, 1)\n\n      err = gradient_checker.compute_gradient_error(inp, [4, 2], squeezed,\n                                                    [4, 1, 2])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsScalar(self):\n    with self.cached_session():\n      inp = constant_op.constant(7)\n      self.assertAllEqual([7], array_ops.expand_dims(inp, 0))\n      self.assertAllEqual([7], array_ops.expand_dims(inp, -1))\n\n      inp = constant_op.constant(True)\n      self.assertAllEqual([True], array_ops.expand_dims(inp, 0))\n      self.assertAllEqual([True], array_ops.expand_dims(inp, -1))\n\n  def testExpandDimsDimType(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      x = np.zeros([2])\n      np_ans = np.expand_dims(x, axis=0)\n      with self.cached_session():\n        tensor = array_ops.expand_dims(x, constant_op.constant(0, dtype))\n        tf_ans = self.evaluate(tensor)\n      self.assertShapeEqual(np_ans, tensor)\n      self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareSqueeze(self, x, squeeze_dims, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      if squeeze_dims:\n        np_ans = np.squeeze(x, axis=tuple(squeeze_dims))\n        tensor = array_ops.squeeze(x, squeeze_dims)\n        tf_ans = self.evaluate(tensor)\n      else:\n        np_ans = np.squeeze(x)\n        tensor = array_ops.squeeze(x)\n        tf_ans = self.evaluate(tensor)\n    self.assertShapeEqual(np_ans, tensor)\n    self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareSqueezeAll(self, x, squeeze_dims=None):\n    if squeeze_dims is None:\n      squeeze_dims = []\n    self._compareSqueeze(x, squeeze_dims, False)\n    self._compareSqueeze(x, squeeze_dims, True)\n\n  def testSqueeze(self):\n    # Nothing to squeeze.\n    self._compareSqueezeAll(np.zeros([2]))\n    self._compareSqueezeAll(np.zeros([2, 3]))\n\n    # Squeeze the middle element away.\n    self._compareSqueezeAll(np.zeros([2, 1, 2]))\n\n    # Squeeze on both ends.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]))\n\n  def testSqueezeBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    # Nothing to squeeze.\n    self._compareSqueezeAll(choice([2]))\n    self._compareSqueezeAll(choice([2, 3]))\n\n    # Squeeze the middle element away.\n    self._compareSqueezeAll(choice([2, 1, 2]))\n\n    # Squeeze on both ends.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]))\n\n  def testSqueezeSpecificDimension(self):\n    # Positive squeeze dim index.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [0])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [2, 4])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [0, 4, 2])\n\n    # Negative squeeze dim index.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-1])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-3, -5])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-3, -5, -1])\n\n  def testSqueezeSpecificDimensionBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    # Positive squeeze dim index.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [0])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [2, 4])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [0, 4, 2])\n\n    # Negative squeeze dim index.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-1])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-3, -5])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-3, -5, -1])\n\n  def testSqueezeAllOnes(self):\n    # Numpy squeezes a 1 element tensor into a zero dimensional tensor.\n    # Verify that we do the same.\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        tensor = array_ops.squeeze(np.zeros([1, 1, 1]), [])\n        self.assertEqual(np.shape(1), tensor.get_shape())\n        tf_ans = self.evaluate(tensor)\n        self.assertEqual(np.shape(1), tf_ans.shape)\n\n  def testSqueezeAllOnesBool(self):\n    # Numpy squeezes a 1 element tensor into a zero dimensional tensor.\n    # Verify that we do the same.\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        tensor = array_ops.squeeze([[[False]]], [])\n        self.assertEqual(np.shape(1), tensor.get_shape())\n        tf_ans = self.evaluate(tensor)\n        self.assertEqual(np.shape(1), tf_ans.shape)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeOnlyOnes(self):\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        input_1x1x3 = np.zeros([1, 1, 3])\n        self._compareSqueezeAll(input_1x1x3)\n        self._compareSqueezeAll(input_1x1x3, [0])\n        self._compareSqueezeAll(input_1x1x3, [1])\n        self.assertRaises(ValueError, array_ops.squeeze, input_1x1x3, [2])\n\n  @test_util.run_deprecated_v1\n  def testSqueezeErrors(self):\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [-4])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [0, -4])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [3])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [2, 3])\n\n  @test_util.run_deprecated_v1\n  def testSqueezeGradient(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = array_ops.reshape(inp, [4, 1, 2])\n      squeezed = array_ops.squeeze(a, [])\n\n      err = gradient_checker.compute_gradient_error(a, [4, 1, 2], squeezed,\n                                                    [4, 2])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeGradientWithSqueezeDims(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = array_ops.reshape(inp, [4, 1, 2, 1])\n      squeezed = array_ops.squeeze(a, [1])\n\n      err = gradient_checker.compute_gradient_error(a, [4, 1, 2, 1], squeezed,\n                                                    [4, 2, 1])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeWithUnknownShape(self):\n    with self.cached_session():\n      a = array_ops.placeholder(dtypes.float32, shape=[2, None])\n\n      squeezed = array_ops.squeeze(a, [1])\n      self.assertEqual([2], squeezed.get_shape().as_list())\n\n      squeezed = array_ops.squeeze(a)\n      self.assertEqual(None, squeezed.get_shape())\n\n      self.assertRaises(ValueError, array_ops.squeeze, a, [0])\n      self.assertRaises(ValueError, array_ops.squeeze, a, [100])\n\n\nclass TileTest(test.TestCase, parameterized.TestCase):\n\n  def testScalar(self):\n    for use_gpu in False, True:\n      with self.cached_session(use_gpu=use_gpu):\n        a = constant_op.constant(7, shape=[], dtype=dtypes.float32)\n        tiled = array_ops.tile(a, [])\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, ())\n      self.assertEqual([], tiled.get_shape())\n      self.assertEqual(7, result)\n\n  def testSimple(self):\n    # multiples could be int32 or int64\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.cached_session():\n        inp = np.random.rand(4, 1).astype(np.float32)\n        a = constant_op.constant(inp)\n        tiled = array_ops.tile(a, constant_op.constant([1, 4], dtype=dtype))\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, (4, 4))\n      self.assertEqual([4, 4], tiled.get_shape())\n      self.assertTrue((result == np.tile(inp, (1, 4))).all())\n\n  def testIdentityTileAndGrad(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(np.float32)\n      a = constant_op.constant(inp)\n      tiled = array_ops.tile(a, [1, 1])\n      result = self.evaluate(tiled)\n    self.assertEqual(result.shape, (4, 1))\n    self.assertEqual([4, 1], tiled.get_shape())\n    self.assertTrue((result == np.tile(inp, (1, 1))).all())\n\n  def testEmpty(self):\n    with self.cached_session():\n      inp = np.random.rand(2, 3).astype(np.float32)\n      a = constant_op.constant(inp)\n      tiled = array_ops.tile(a, [5, 0])\n      result = self.evaluate(tiled)\n    self.assertEqual(result.shape, (10, 0))\n    self.assertEqual([10, 0], tiled.get_shape())\n\n  @test_util.run_deprecated_v1\n  def testUnknownInputShape(self):\n    \"\"\"Importing can call _TileShape without shape of <multiples> known.\"\"\"\n    with self.cached_session():\n      inp = array_ops.placeholder(dtypes.float32)  # unknown shape\n      multiples = constant_op.constant([1, 2, 3, 4], dtype=np.int32)\n      tiled = array_ops.tile(inp, multiples)\n      gdef = tiled.graph.as_graph_def()\n\n      # Move the tile op to the start of the graph so that shapes of its inputs\n      # are not available when the shape function runs on import.\n      swapped = False\n      for i, n in enumerate(gdef.node):\n        if n.op == \"Tile\":\n          # Swap tile op to be first in gdef.node\n          assert i != 0\n          new_node = node_def_pb2.NodeDef()\n          new_node.CopyFrom(gdef.node[i])\n          gdef.node[i].CopyFrom(gdef.node[0])\n          gdef.node[0].CopyFrom(new_node)\n          swapped = True\n      assert swapped\n\n      tiled_imported, = importer.import_graph_def(\n          gdef, return_elements=[tiled.name])\n      self.assertEqual(4, tiled_imported.get_shape().ndims)\n\n  def testTypes(self):\n    types_to_test = {\n        \"bool\": (dtypes.bool, bool),\n        \"float32\": (dtypes.float32, float),\n        \"float64\": (dtypes.float64, float),\n        \"complex64\": (dtypes.complex64, complex),\n        \"complex128\": (dtypes.complex128, complex),\n        \"uint8\": (dtypes.uint8, int),\n        \"int8\": (dtypes.int8, int),\n        \"int16\": (dtypes.int16, int),\n        \"int32\": (dtypes.int32, int),\n        \"int64\": (dtypes.int64, int),\n        \"uint32\": (dtypes.uint32, int),\n        \"uint64\": (dtypes.uint64, int),\n        bytes: (dtypes.string, bytes)\n    }\n    for dtype_np, (dtype_tf, cast) in types_to_test.items():\n      with self.cached_session():\n        inp = np.random.rand(4, 1).astype(dtype_np)\n        a = constant_op.constant(\n            [cast(x) for x in inp.ravel(order=\"C\")],\n            shape=[4, 1],\n            dtype=dtype_tf)\n        tiled = array_ops.tile(a, [1, 4])\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, (4, 4))\n      self.assertEqual([4, 4], tiled.get_shape())\n      self.assertAllEqual(result, np.tile(inp, (1, 4)))\n\n  @test_util.run_deprecated_v1\n  def testInvalidDim(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.ravel(order=\"C\")],\n          shape=[4, 1],\n          dtype=dtypes.float32)\n      # Wrong length of multiples.\n      with self.assertRaises(ValueError):\n        array_ops.tile(a, [1, 4, 2])\n      # Wrong rank for multiples.\n      with self.assertRaises(ValueError):\n        array_ops.tile(a, [[2, 3], [3, 4]]).eval()\n\n  def _RunAndVerifyResult(self, rank, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      # Random dims of given rank\n      input_shape = np.random.randint(1, 4, size=rank)\n      inp = np.random.rand(*input_shape).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.ravel(order=\"C\")],\n          shape=input_shape,\n          dtype=dtypes.float32)\n      multiples = np.random.randint(1, 4, size=rank).astype(np.int32)\n      tiled = array_ops.tile(a, multiples)\n      result = self.evaluate(tiled)\n    self.assertTrue((np.array(multiples) * np.array(inp.shape) == np.array(\n        result.shape)).all())\n    self.assertAllEqual(result, np.tile(inp, tuple(multiples)))\n    self.assertShapeEqual(result, tiled)\n\n  def testRandom(self):\n    # test low rank, like 5\n    for _ in range(5):\n      self._RunAndVerifyResult(5, use_gpu=False)\n    for _ in range(5):\n      self._RunAndVerifyResult(5, use_gpu=True)\n    # test high rank, like 10\n    for _ in range(5):\n      self._RunAndVerifyResult(10, use_gpu=False)\n    for _ in range(5):\n      self._RunAndVerifyResult(10, use_gpu=True)\n\n  @parameterized.parameters(dtypes.int32, dtypes.int64)\n  @test_util.run_deprecated_v1\n  def testGradientSimpleReduction(self, multiples_dtype):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 1], dtype=dtypes.float32)\n      multiples = constant_op.constant([1, 4], dtype=multiples_dtype)\n      tiled = array_ops.tile(a, multiples)\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      self.assertShapeEqual(inp, grad)\n      result = self.evaluate(grad)\n    self.assertAllClose(np.sum(grad_inp, axis=1).reshape(4, 1), result, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReduction(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      self.assertShapeEqual(inp, grad)\n      result = self.evaluate(grad)\n    expected_shape = [4, 2]\n    expected = np.zeros(expected_shape)\n    expected[:, 0] = grad_inp[:, 0] + grad_inp[:, 2]\n    expected[:, 1] = grad_inp[:, 1] + grad_inp[:, 3]\n    self.assertTrue((np.abs(expected - result) < 1e-3).all())\n\n  @test_util.run_deprecated_v1\n  def testGradientSimpleReductionOnGPU(self):\n    with self.session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 1], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 4])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      result = self.evaluate(grad)\n    self.assertAllClose(np.sum(grad_inp, axis=1).reshape(4, 1), result, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReductionOnGPU(self):\n    with self.session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      result = self.evaluate(grad)\n    expected_shape = [4, 2]\n    expected = np.zeros(expected_shape)\n    expected[:, 0] = grad_inp[:, 0] + grad_inp[:, 2]\n    expected[:, 1] = grad_inp[:, 1] + grad_inp[:, 3]\n    self.assertAllClose(expected, result, 1e-3)\n\n  def _RunAndVerifyGradientResult(self, input_shape, multiples):\n    for use_gpu in False, True:\n      with self.cached_session(use_gpu=use_gpu):\n        # Random values\n        inp = np.asarray(np.random.rand(*input_shape))\n        a = constant_op.constant(inp, dtype=dtypes.float64)\n        tiled = array_ops.tile(a, multiples)\n        grad_shape = list(np.array(multiples) * np.array(inp.shape))\n        err = gradient_checker.compute_gradient_error(\n            a, list(input_shape), tiled, grad_shape, x_init_value=inp)\n      print(\"tile(float) error = \", err)\n      self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientRandomScalar(self):\n    self._RunAndVerifyGradientResult([], [])\n\n  @test_util.run_deprecated_v1\n  def testGradientRandom(self):\n    self._RunAndVerifyGradientResult([2, 2, 1, 1, 3], [1, 1, 1, 1, 1])\n    self._RunAndVerifyGradientResult([2, 2, 1, 1, 3], [1, 2, 1, 3, 1])\n    self._RunAndVerifyGradientResult([2, 3, 1, 1, 3], [3, 1, 1, 2, 2])\n    self._RunAndVerifyGradientResult([2, 1, 3, 3, 2], [1, 3, 3, 1, 2])\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReductionGC(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      err = gradient_checker.compute_gradient_error(a, [4, 2], tiled, [4, 4])\n    self.assertLess(err, 1e-3)\n\n  @parameterized.parameters(dtypes.int32, dtypes.int64)\n  @test_util.run_deprecated_v1\n  def testGradientWithSparseGradWithRank1(self, multiples_dtype):\n    inputs = constant_op.constant([1.0, 2.0, 3.0, 4.0],\n                                  dtype=dtypes.float32)\n    multiples = constant_op.constant([3], dtype=dtypes.int64)\n    outputs = array_ops.gather(array_ops.tile(inputs, multiples),\n                               [1, 5, 9, 3, 7, 2, 2, 2])\n    with self.cached_session():\n      error = gradient_checker.compute_gradient_error(\n          inputs, inputs.get_shape().as_list(),\n          outputs, outputs.get_shape().as_list())\n      self.assertLess(error, 1e-4)\n\n  @test_util.run_deprecated_v1\n  def testGradientWithSparseGradWithRank3(self):\n    inputs = constant_op.constant([1.0, 2.0, 3.0, 4.0],\n                                  dtype=dtypes.float32)\n    inputs = array_ops.reshape(inputs, [-1, 1, 1])\n    outputs = array_ops.gather(array_ops.tile(inputs, [3, 4, 2]),\n                               [1, 5, 9, 3, 7, 2, 2, 2])\n    with self.cached_session():\n      error = gradient_checker.compute_gradient_error(\n          inputs, inputs.get_shape().as_list(),\n          outputs, outputs.get_shape().as_list())\n      self.assertLess(error, 1e-4)\n\n  @test_util.run_deprecated_v1\n  def testShapeFunctionEdgeCases(self):\n    # Unknown multiples shape.\n    inp = constant_op.constant(0.0, shape=[4, 4, 4, 4])\n    tiled = array_ops.tile(inp, array_ops.placeholder(dtypes.int32))\n    self.assertEqual([None, None, None, None], tiled.get_shape().as_list())\n\n    # Unknown input shape.\n    inp = array_ops.placeholder(dtypes.float32)\n    tiled = array_ops.tile(inp, [2, 2, 2, 2])\n    self.assertEqual([None, None, None, None], tiled.get_shape().as_list())\n\n    # Unknown input and multiples shape.\n    inp = array_ops.placeholder(dtypes.float32)\n    tiled = array_ops.tile(inp, array_ops.placeholder(dtypes.int32))\n    self.assertIs(None, tiled.get_shape().ndims)\n\n    # Known input and partially known multiples.\n    inp = constant_op.constant(0.0, shape=[1, 1])\n    tiled = array_ops.tile(inp, [array_ops.placeholder(dtypes.int32), 7])\n    self.assertEqual([None, 7], tiled.get_shape().as_list())\n\n    # Mismatched input rank and multiples length.\n    inp = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    with self.assertRaises(ValueError):\n      tiled = array_ops.tile(\n          inp, array_ops.placeholder(\n              dtypes.int32, shape=[3]))\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/array_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/type_index.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/gtl/array_slice.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\n// Forward declarations of functors that will be defined in tile_ops_impl.h\nnamespace functor {\ntemplate <typename Device, typename T, typename Tmultiple>\nstruct Tile {\n  void operator()(const Device& d, Tensor* out, const Tensor& in,\n                  const gtl::ArraySlice<Tmultiple> broadcast_array) const;\n};\n\ntemplate <typename Device, typename T, int NDIM>\nstruct TileGrad {\n  void operator()(const Device& d, typename TTypes<T, NDIM>::Tensor out,\n                  typename TTypes<T, NDIM>::ConstTensor in,\n                  const Eigen::DSizes<Eigen::DenseIndex, NDIM>& indices,\n                  const Eigen::DSizes<Eigen::DenseIndex, NDIM>& sizes,\n                  bool first) const;\n};\n\ntemplate <typename Device, typename T>\nstruct TileGrad<Device, T, 0> {\n  void operator()(const Device& d, typename TTypes<T, 0>::Tensor out,\n                  typename TTypes<T, 0>::ConstTensor in,\n                  const Eigen::DSizes<Eigen::DenseIndex, 0>&,\n                  const Eigen::DSizes<Eigen::DenseIndex, 0>&, bool first) const;\n};\n\ntemplate <typename Device, typename T, int NDIM, int REDUCEDNDIM>\nstruct ReduceAndReshape {\n  void operator()(\n      const Device& d, typename TTypes<T, NDIM>::Tensor out,\n      typename TTypes<T, NDIM>::ConstTensor in,\n      const Eigen::DSizes<Eigen::DenseIndex, REDUCEDNDIM>& reduce_dim,\n      const Eigen::DSizes<Eigen::DenseIndex, NDIM>& reshape_dim) const;\n};\n\n// Explicit instantiations are defined in tile_ops_{cpu,gpu}_impl.*,\n// below are their declarations.\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nextern template struct Tile<GPUDevice, bool, int32>;\nextern template struct Tile<GPUDevice, bool, int64_t>;\nextern template struct Tile<GPUDevice, float, int32>;\nextern template struct Tile<GPUDevice, float, int64_t>;\nextern template struct Tile<GPUDevice, double, int32>;\nextern template struct Tile<GPUDevice, double, int64_t>;\nextern template struct Tile<GPUDevice, complex64, int32>;\nextern template struct Tile<GPUDevice, complex64, int64_t>;\nextern template struct Tile<GPUDevice, complex128, int32>;\nextern template struct Tile<GPUDevice, complex128, int64_t>;\nextern template struct Tile<GPUDevice, Eigen::half, int32>;\nextern template struct Tile<GPUDevice, Eigen::half, int64_t>;\nextern template struct Tile<GPUDevice, int16, int32>;\nextern template struct Tile<GPUDevice, int16, int64_t>;\nextern template struct Tile<GPUDevice, int32, int32>;\nextern template struct Tile<GPUDevice, int32, int64_t>;\nextern template struct Tile<GPUDevice, int64_t, int32>;\nextern template struct Tile<GPUDevice, int64_t, int64_t>;\n#define DECLARE_CUDA_DIM(T, NDIM)                      \\\n  extern template struct TileGrad<GPUDevice, T, NDIM>; \\\n  extern template struct ReduceAndReshape<GPUDevice, T, NDIM, 1>\n#else  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define DECLARE_CUDA_DIM(T, NDIM)\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define DECLARE_TYPE(T)                             \\\n  extern template struct Tile<CPUDevice, T, int32>; \\\n  extern template struct Tile<CPUDevice, T, int64_t>;\nTF_CALL_bool(DECLARE_TYPE);\nTF_CALL_float(DECLARE_TYPE);\nTF_CALL_bfloat16(DECLARE_TYPE);\nTF_CALL_double(DECLARE_TYPE);\nTF_CALL_uint8(DECLARE_TYPE);\nTF_CALL_int32(DECLARE_TYPE);\nTF_CALL_int16(DECLARE_TYPE);\nTF_CALL_int64(DECLARE_TYPE);\nTF_CALL_uint32(DECLARE_TYPE);\nTF_CALL_uint64(DECLARE_TYPE);\nTF_CALL_half(DECLARE_TYPE);\nTF_CALL_complex64(DECLARE_TYPE);\nTF_CALL_complex128(DECLARE_TYPE);\nTF_CALL_tstring(DECLARE_TYPE);\nTF_CALL_variant(DECLARE_TYPE);\n#undef DECLARE_TYPE\n\n#define DECLARE_DIM(T, NDIM)                           \\\n  DECLARE_CUDA_DIM(T, NDIM);                           \\\n  extern template struct TileGrad<CPUDevice, T, NDIM>; \\\n  extern template struct ReduceAndReshape<CPUDevice, T, NDIM, 1>;\n\n#define DECLARE_TYPE(T) \\\n  DECLARE_DIM(T, 1)     \\\n  DECLARE_DIM(T, 2)     \\\n  DECLARE_DIM(T, 3)     \\\n  DECLARE_DIM(T, 4)     \\\n  DECLARE_DIM(T, 5)     \\\n  DECLARE_DIM(T, 6)     \\\n  DECLARE_DIM(T, 7)\nTF_CALL_float(DECLARE_TYPE);\nTF_CALL_bfloat16(DECLARE_TYPE);\nTF_CALL_double(DECLARE_TYPE);\nTF_CALL_int16(DECLARE_TYPE);\nTF_CALL_int32(DECLARE_TYPE);\nTF_CALL_int64(DECLARE_TYPE);\nTF_CALL_half(DECLARE_TYPE);\nTF_CALL_complex64(DECLARE_TYPE);\nTF_CALL_complex128(DECLARE_TYPE);\n#undef DECLARE_TYPE\n\n#undef DECLARE_DIM\n#undef DECLARE_CUDA_DIM\n\n}  // namespace functor\n\n// --------------------------------------------------------------------------\ntemplate <typename Device, typename Tmultiples>\nclass TileOp : public OpKernel {\n public:\n  explicit TileOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& multiples = context->input(1);\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(multiples.shape()),\n        errors::InvalidArgument(\"Expected multiples to be 1-D, but got shape \",\n                                multiples.shape().DebugString()));\n    OP_REQUIRES(context, input.dims() == multiples.NumElements(),\n                errors::InvalidArgument(\n                    \"Expected multiples argument to be a vector of length \",\n                    input.dims(), \" but got length \", multiples.dim_size(0)));\n    const int input_dims = input.dims();\n\n    // Eigen doesn't support scalars on the GPU, so handle 0-D specially\n    if (input_dims == 0) {\n      context->set_output(0, input);\n      return;\n    }\n\n    const gtl::ArraySlice<Tmultiples> multiples_array(\n        multiples.flat<Tmultiples>().data(), input_dims);\n    TensorShape output_shape;\n    for (int i = 0; i < input_dims; ++i) {\n      OP_REQUIRES(\n          context, multiples_array[i] >= 0,\n          errors::InvalidArgument(\"Expected multiples[\", i, \"] >= 0, but got \",\n                                  multiples_array[i]));\n      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(\n                                  input.dim_size(i) * multiples_array[i]));\n    }\n    if (output_shape == input.shape()) {\n      context->set_output(0, input);\n      return;\n    }\n    Tensor* result = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &result));\n\n    // If there's no output, there's nothing to do.\n    if (output_shape.num_elements() == 0) return;\n\n#define HANDLE_TYPE(DT)                               \\\n  if (context->input(0).dtype() == DT) {              \\\n    HandleCase<DT>(context, multiples_array, result); \\\n    return;                                           \\\n  }\n\n#define HANDLE_TYPE_NAME(T) HANDLE_TYPE(DataTypeToEnum<T>::value)\n\n    // Invoke macro using TF_CALL_* so type-filtering for platform applies.\n    TF_CALL_bool(HANDLE_TYPE_NAME);\n    TF_CALL_bfloat16(HANDLE_TYPE_NAME);\n    TF_CALL_float(HANDLE_TYPE_NAME);\n    TF_CALL_double(HANDLE_TYPE_NAME);\n    TF_CALL_uint8(HANDLE_TYPE_NAME);\n    TF_CALL_int8(HANDLE_TYPE_NAME);\n    TF_CALL_int32(HANDLE_TYPE_NAME);\n    TF_CALL_int16(HANDLE_TYPE_NAME);\n    TF_CALL_int64(HANDLE_TYPE_NAME);\n    TF_CALL_uint32(HANDLE_TYPE_NAME);\n    TF_CALL_uint64(HANDLE_TYPE_NAME);\n    TF_CALL_half(HANDLE_TYPE_NAME);\n    TF_CALL_tstring(HANDLE_TYPE_NAME);  // when DEVICE=CPUDevice.\n    TF_CALL_complex64(HANDLE_TYPE_NAME);\n    TF_CALL_complex128(HANDLE_TYPE_NAME);\n    TF_CALL_variant(HANDLE_TYPE_NAME);  // when DEVICE=CPUDevice\n\n#undef HANDLE_TYPE_NAME\n#undef HANDLE_TYPE\n\n    OP_REQUIRES(\n        context, false,\n        errors::Unimplemented(\n            \"TileOp : The input data type is not supported, DataType : \",\n            DataTypeString(context->input(0).dtype()),\n            \", Dimension : \", input_dims));\n  }\n\n private:\n  template <DataType DT>\n  void HandleCaseImpl(OpKernelContext* context,\n                      const gtl::ArraySlice<Tmultiples> multiples_array,\n                      Tensor* result) {\n    typedef typename EnumToDataType<DT>::Type T;\n    functor::Tile<Device, T, Tmultiples>()(context->eigen_device<Device>(),\n                                           result, context->input(0),\n                                           multiples_array);\n  }\n\n  template <DataType DT>\n  void HandleCase(OpKernelContext* context,\n                  const gtl::ArraySlice<Tmultiples> multiples_array,\n                  Tensor* result);\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TileOp);\n};\n\ntemplate <typename Device, typename Tmultiples>\ntemplate <DataType DT>\ninline void TileOp<Device, Tmultiples>::HandleCase(\n    OpKernelContext* context, const gtl::ArraySlice<Tmultiples> multiples_array,\n    Tensor* result) {\n  // TODO(vrv): print out the device name if useful. Currently disabled to avoid\n  // having to use RTTI.\n  LOG(FATAL) << \"TileOp: Invalid combination of Device, DT: \"\n             // << typeid(Device).name() << \", \"\n             << DataTypeString(DT);\n}\n\n#define HANDLE_CASE(device, dtype, Tmultiples)                             \\\n  template <>                                                              \\\n  template <>                                                              \\\n  void TileOp<device, Tmultiples>::HandleCase<dtype>(                      \\\n      OpKernelContext * context,                                           \\\n      const gtl::ArraySlice<Tmultiples> multiples_array, Tensor* result) { \\\n    HandleCaseImpl<dtype>(context, multiples_array, result);               \\\n  }\n\n#define HANDLE_TYPE_NAME_CPU(T)                            \\\n  HANDLE_CASE(CPUDevice, DataTypeToEnum<T>::value, int32); \\\n  HANDLE_CASE(CPUDevice, DataTypeToEnum<T>::value, int64_t);\n\n#define HANDLE_TYPE_NAME_GPU(T)                            \\\n  HANDLE_CASE(GPUDevice, DataTypeToEnum<T>::value, int32); \\\n  HANDLE_CASE(GPUDevice, DataTypeToEnum<T>::value, int64_t);\n\nTF_CALL_bool(HANDLE_TYPE_NAME_CPU);\nTF_CALL_float(HANDLE_TYPE_NAME_CPU);\nTF_CALL_bfloat16(HANDLE_TYPE_NAME_CPU);\nTF_CALL_double(HANDLE_TYPE_NAME_CPU);\nTF_CALL_uint8(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int8(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int32(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int16(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_uint32(HANDLE_TYPE_NAME_CPU);\nTF_CALL_uint64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_half(HANDLE_TYPE_NAME_CPU);\nTF_CALL_complex64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_complex128(HANDLE_TYPE_NAME_CPU);\nTF_CALL_tstring(HANDLE_TYPE_NAME_CPU);\nTF_CALL_variant(HANDLE_TYPE_NAME_CPU);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nTF_CALL_bool(HANDLE_TYPE_NAME_GPU);\nTF_CALL_float(HANDLE_TYPE_NAME_GPU);\nTF_CALL_double(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int16(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int32(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int64(HANDLE_TYPE_NAME_GPU);\nTF_CALL_half(HANDLE_TYPE_NAME_GPU);\nTF_CALL_complex64(HANDLE_TYPE_NAME_GPU);\nTF_CALL_complex128(HANDLE_TYPE_NAME_GPU);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n#undef HANDLE_TYPE_NAME_CPU\n#undef HANDLE_TYPE_NAME_GPU\n#undef HANDLE_CASE\n\n// --------------------------------------------------------------------------\ntemplate <typename Device, typename Tmultiples>\nclass TileGradientOp : public OpKernel {\n public:\n  explicit TileGradientOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& multiples = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(multiples.shape()),\n        errors::InvalidArgument(\"Expected multiples to be 1-D, but got shape \",\n                                multiples.shape().DebugString()));\n    OP_REQUIRES(context, input.dims() == multiples.NumElements(),\n                errors::InvalidArgument(\n                    \"Expected multiples argument to be a vector of length \",\n                    input.dims(), \" but got length \", multiples.dim_size(0)));\n\n    const int input_dims = input.dims();\n\n    // Eigen doesn't support scalars on the GPU, so handle 0-D specially\n    if (input_dims == 0) {\n      context->set_output(0, input);\n      return;\n    }\n\n    const gtl::ArraySlice<Tmultiples> multiples_array(\n        multiples.flat<Tmultiples>().data(), input_dims);\n    TensorShape output_shape;\n    std::vector<Tmultiples> input_dim_size_vec;\n    for (int i = 0; i < input_dims; ++i) {\n      OP_REQUIRES(\n          context, multiples_array[i] > 0,\n          errors::InvalidArgument(\"Expected multiples[\", i, \"] > 0, but got \",\n                                  multiples_array[i]));\n      OP_REQUIRES(context, input.dim_size(i) % multiples_array[i] == 0,\n                  errors::InvalidArgument(\"Expected input_dim[\", i,\n                                          \"] to be divisible by multiples[\", i,\n                                          \"], but \", input.dim_size(i), \" % \",\n                                          multiples_array[i], \" != 0\"));\n      output_shape.AddDim(input.dim_size(i) / multiples_array[i]);\n      input_dim_size_vec.push_back(input.dim_size(i));\n    }\n    if (output_shape == input.shape()) {\n      context->set_output(0, input);\n      return;\n    }\n    Tensor* result = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &result));\n\n#define HANDLE_DIM(DT, NDIM)                                           \\\n  if (context->input(0).dtype() == DT && input_dims == NDIM) {         \\\n    HandleCase<DT, NDIM>(context, input_dim_size_vec, multiples_array, \\\n                         result);                                      \\\n    return;                                                            \\\n  }\n\n#define HANDLE_TYPE(T) \\\n  HANDLE_DIM(T, 1)     \\\n  HANDLE_DIM(T, 2)     \\\n  HANDLE_DIM(T, 3)     \\\n  HANDLE_DIM(T, 4)     \\\n  HANDLE_DIM(T, 5)     \\\n  HANDLE_DIM(T, 6)     \\\n  HANDLE_DIM(T, 7)\n\n#define HANDLE_TYPE_NAME(T) HANDLE_TYPE(DataTypeToEnum<T>::value)\n\n    TF_CALL_float(HANDLE_TYPE_NAME);\n    TF_CALL_double(HANDLE_TYPE_NAME);\n    TF_CALL_int32(HANDLE_TYPE_NAME);\n    TF_CALL_int16(HANDLE_TYPE_NAME);\n    TF_CALL_int64(HANDLE_TYPE_NAME);\n    TF_CALL_half(HANDLE_TYPE_NAME);\n    TF_CALL_bfloat16(HANDLE_TYPE_NAME);\n    TF_CALL_complex64(HANDLE_TYPE_NAME);\n    TF_CALL_complex128(HANDLE_TYPE_NAME);\n\n#undef HANDLE_TYPE_NAME\n#undef HANDLE_TYPE\n#undef HANDLE_DIM\n\n    OP_REQUIRES(context, false,\n                errors::Unimplemented(\"TileGradientOp : The input data type or \"\n                                      \"dimension is not supported, DataType : \",\n                                      DataTypeString(context->input(0).dtype()),\n                                      \", Dimension : \", input_dims));\n  }\n\n private:\n  template <DataType DT, int NDIM>\n  void HandleCase(OpKernelContext* context,\n                  const std::vector<Tmultiples>& input_dims,\n                  const gtl::ArraySlice<Tmultiples> multiples_array,\n                  Tensor* result);\n\n  template <DataType DT, int NDIM>\n  void HandleCaseImpl(OpKernelContext* context,\n                      const std::vector<Tmultiples>& input_dims,\n                      const gtl::ArraySlice<Tmultiples> multiples_array,\n                      Tensor* result) {\n    typedef typename EnumToDataType<DT>::Type T;\n\n    bool reduction_only = true;\n    std::vector<Tmultiples> reduction_dims;\n\n    for (int i = 0; i < NDIM; ++i) {\n      if (input_dims[i] > multiples_array[i] && multiples_array[i] > 1) {\n        reduction_only = false;\n        break;\n      } else {\n        if (multiples_array[i] == input_dims[i]) {\n          reduction_dims.push_back(i);\n        }\n      }\n    }\n\n    if (reduction_only) {\n#define HANDLE_DIM(D)                                            \\\n  if (reduction_dims.size() == (D)) {                            \\\n    HandleReduce<T, NDIM, (D)>(context, reduction_dims, result); \\\n    return;                                                      \\\n  }\n      // NOTE(keveman): Handling the most common case here.\n      // Adding more cases here would require more templating and code\n      // explosion. For instance, HANDLE_DIM(2) wouldn't make sense for NDIM=1.\n      HANDLE_DIM(1);\n\n// Fall through to the unoptimized version.\n#undef HANDLE_DIM\n    }\n\n    Eigen::DSizes<Eigen::DenseIndex, NDIM> indices;\n    Eigen::DSizes<Eigen::DenseIndex, NDIM> sizes;\n\n    // Accumulate slices along the dimensions into the output. The number of\n    // slices along dimension 'i' is simply the multiple along dimension 'i'\n    // passed to the original Tile op.\n    for (int i = 0; i < NDIM; ++i) {\n      sizes[i] = input_dims[i] / multiples_array[i];\n      indices[i] = 0;\n    }\n\n    bool first = true;\n    while (true) {\n      functor::TileGrad<Device, T, NDIM>()(\n          context->eigen_device<Device>(), result->tensor<T, NDIM>(),\n          context->input(0).tensor<T, NDIM>(), indices, sizes, first);\n      first = false;\n      // Increment the begin indices.\n      int i = 0;\n      while (i < NDIM && indices[i] / sizes[i] == multiples_array[i] - 1) {\n        indices[i] = 0;\n        ++i;\n      }\n      // We are finished if we have iterated to the maximum along all\n      // dimensions.\n      if (i == NDIM) {\n        break;\n      }\n      indices[i] += sizes[i];\n    }\n  }\n\n  template <typename T, int NDIM, int REDUCENDIM>\n  void HandleReduce(OpKernelContext* context,\n                    const std::vector<Tmultiples>& reduce_dim_in,\n                    Tensor* result) {\n    static_assert(NDIM >= REDUCENDIM, \"Too many reduced dimensions\");\n    Eigen::DSizes<Eigen::DenseIndex, REDUCENDIM> reduce_dim;\n    Eigen::DSizes<Eigen::DenseIndex, NDIM> reshape_dim;\n\n    for (int i = 0; i < REDUCENDIM; ++i) {\n      reduce_dim[i] = reduce_dim_in[i];\n    }\n\n    for (int i = 0; i < NDIM; ++i) {\n      reshape_dim[i] = result->dim_size(i);\n    }\n\n    functor::ReduceAndReshape<Device, T, NDIM, REDUCENDIM>()(\n        context->eigen_device<Device>(), result->tensor<T, NDIM>(),\n        context->input(0).tensor<T, NDIM>(), reduce_dim, reshape_dim);\n  }\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TileGradientOp);\n};\n\ntemplate <typename Device, typename Tmultiples>\ntemplate <DataType DT, int NDIM>\ninline void TileGradientOp<Device, Tmultiples>::HandleCase(\n    OpKernelContext* context, const std::vector<Tmultiples>& input_dims,\n    const gtl::ArraySlice<Tmultiples> multiples_array, Tensor* result) {\n  LOG(FATAL) << \"TileGradientOp: Invalid combination of Device, DT and NDIM: \"\n             << TypeIndex::Make<Device>().name() << \", \" << DataTypeString(DT)\n             << \", \" << NDIM;\n}\n\n#define HANDLE_CASE(device, T, dtype, Tmultiples, ndim)                        \\\n  template <>                                                                  \\\n  template <>                                                                  \\\n  void TileGradientOp<device, Tmultiples>::HandleCase<dtype, ndim>(            \\\n      OpKernelContext * context, const std::vector<Tmultiples>& input_dims,    \\\n      const gtl::ArraySlice<Tmultiples> multiples_array, Tensor* result) {     \\\n    HandleCaseImpl<dtype, ndim>(context, input_dims, multiples_array, result); \\\n  }\n\n// 0-D handled specially above\n#define HANDLE_CASE_DIM(device, T, dtype)    \\\n  HANDLE_CASE(device, T, dtype, int32, 1);   \\\n  HANDLE_CASE(device, T, dtype, int32, 2);   \\\n  HANDLE_CASE(device, T, dtype, int32, 3);   \\\n  HANDLE_CASE(device, T, dtype, int32, 4);   \\\n  HANDLE_CASE(device, T, dtype, int32, 5);   \\\n  HANDLE_CASE(device, T, dtype, int32, 6);   \\\n  HANDLE_CASE(device, T, dtype, int32, 7);   \\\n  HANDLE_CASE(device, T, dtype, int64_t, 1); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 2); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 3); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 4); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 5); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 6); \\\n  HANDLE_CASE(device, T, dtype, int64_t, 7);\n\n#define HANDLE_TYPE_NAME_CPU(T) \\\n  HANDLE_CASE_DIM(CPUDevice, T, DataTypeToEnum<T>::value);\n\n#define HANDLE_TYPE_NAME_GPU(T) \\\n  HANDLE_CASE_DIM(GPUDevice, T, DataTypeToEnum<T>::value);\n\nTF_CALL_float(HANDLE_TYPE_NAME_CPU);\nTF_CALL_double(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int16(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int32(HANDLE_TYPE_NAME_CPU);\nTF_CALL_int64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_half(HANDLE_TYPE_NAME_CPU);\nTF_CALL_complex64(HANDLE_TYPE_NAME_CPU);\nTF_CALL_complex128(HANDLE_TYPE_NAME_CPU);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nTF_CALL_float(HANDLE_TYPE_NAME_GPU);\nTF_CALL_double(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int16(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int32(HANDLE_TYPE_NAME_GPU);\nTF_CALL_int64(HANDLE_TYPE_NAME_GPU);\nTF_CALL_half(HANDLE_TYPE_NAME_GPU);\nTF_CALL_complex64(HANDLE_TYPE_NAME_GPU);\nTF_CALL_complex128(HANDLE_TYPE_NAME_GPU);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n#undef HANDLE_TYPE_NAME_CPU\n#undef HANDLE_TYPE_NAME_GPU\n#undef HANDLE_CASE_DIM\n#undef HANDLE_CASE\n\nREGISTER_KERNEL_BUILDER(Name(\"Tile\")\n                            .Device(DEVICE_CPU)\n                            .HostMemory(\"multiples\")\n                            .TypeConstraint<int32>(\"Tmultiples\"),\n                        TileOp<CPUDevice, int32>);\nREGISTER_KERNEL_BUILDER(Name(\"Tile\")\n                            .Device(DEVICE_CPU)\n                            .HostMemory(\"multiples\")\n                            .TypeConstraint<int64_t>(\"Tmultiples\"),\n                        TileOp<CPUDevice, int64>);\nREGISTER_KERNEL_BUILDER(Name(\"TileGrad\")\n                            .Device(DEVICE_CPU)\n                            .HostMemory(\"multiples\")\n                            .TypeConstraint<int32>(\"Tmultiples\"),\n                        TileGradientOp<CPUDevice, int32>);\nREGISTER_KERNEL_BUILDER(Name(\"TileGrad\")\n                            .Device(DEVICE_CPU)\n                            .HostMemory(\"multiples\")\n                            .TypeConstraint<int64_t>(\"Tmultiples\"),\n                        TileGradientOp<CPUDevice, int64>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_TILE(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"Tile\")                               \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<type>(\"T\")             \\\n                              .TypeConstraint<int32>(\"Tmultiples\")   \\\n                              .HostMemory(\"multiples\"),              \\\n                          TileOp<GPUDevice, int32>);                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"Tile\")                               \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<type>(\"T\")             \\\n                              .TypeConstraint<int64_t>(\"Tmultiples\") \\\n                              .HostMemory(\"multiples\"),              \\\n                          TileOp<GPUDevice, int64>);\n\n#define REGISTER_GPU_TILE_GRAD(type)                                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TileGrad\")                           \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<type>(\"T\")             \\\n                              .TypeConstraint<int32>(\"Tmultiples\")   \\\n                              .HostMemory(\"multiples\"),              \\\n                          TileGradientOp<GPUDevice, int32>);         \\\n  REGISTER_KERNEL_BUILDER(Name(\"TileGrad\")                           \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<type>(\"T\")             \\\n                              .TypeConstraint<int64_t>(\"Tmultiples\") \\\n                              .HostMemory(\"multiples\"),              \\\n                          TileGradientOp<GPUDevice, int64>);\n\n#define REGISTER_GPU(type) \\\n  REGISTER_GPU_TILE(type); \\\n  REGISTER_GPU_TILE_GRAD(type);\n\nTF_CALL_bool(REGISTER_GPU_TILE);\nTF_CALL_float(REGISTER_GPU);\nTF_CALL_double(REGISTER_GPU);\nTF_CALL_half(REGISTER_GPU);\nTF_CALL_int16(REGISTER_GPU);\nTF_CALL_int32(REGISTER_GPU);\nTF_CALL_int64(REGISTER_GPU);\nTF_CALL_complex64(REGISTER_GPU);\nTF_CALL_complex128(REGISTER_GPU)\n\n#undef REGISTER_GPU_TILE\n#undef REGISTER_GPU_TILE_GRAD\n#undef REGISTER_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for various tensorflow.ops.tf.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass ShapeOpsTest(test.TestCase):\n\n  def _compareShape(self, x, use_gpu=False):\n    np_ans = np.array(np.shape(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.shape(x)\n      tf_ans_64 = array_ops.shape(x, out_type=dtypes.int64)\n      result = self.evaluate(tf_ans)\n      result_64 = self.evaluate(tf_ans_64)\n    self.assertAllEqual(np_ans, result)\n    self.assertAllEqual(np_ans, result_64)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareShapeSparse(self, x_np, use_gpu=False):\n    np_ans = np.array(np.shape(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.shape(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareShapeN(self, x, use_gpu=False):\n    np_ans = np.array(np.shape(x))\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      tf_ans = array_ops.shape_n([x, x, x])\n      tf_ans_64 = array_ops.shape_n([x, x, x], out_type=dtypes.int64)\n      result = self.evaluate(tf_ans)\n      result_64 = self.evaluate(tf_ans_64)\n    for i in range(3):\n      self.assertAllEqual(np_ans, result[i])\n      self.assertAllEqual(np_ans, result_64[i])\n      self.assertShapeEqual(np_ans, tf_ans[i])\n\n  def _compareRank(self, x, use_gpu=False):\n    np_ans = np.asarray(np.ndim(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.rank(x)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareRankSparse(self, x_np, use_gpu=False):\n    np_ans = np.asarray(np.ndim(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.rank(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareSize(self, x, use_gpu=False):\n    np_ans = np.asarray(np.size(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.size(x)\n      result = self.evaluate(tf_ans)\n      tf_ans_64 = array_ops.size(x, out_type=dtypes.int64)\n      result_64 = self.evaluate(tf_ans_64)\n    self.assertAllEqual(np_ans, result)\n    self.assertAllEqual(np_ans, result_64)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareSizeSparse(self, x_np, use_gpu=False):\n    np_ans = np.asarray(np.size(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.size(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _testCpu(self, x):\n    self._compareShape(x, use_gpu=False)\n    self._compareShapeN(x, use_gpu=False)\n    self._compareRank(x, use_gpu=False)\n    self._compareSize(x, use_gpu=False)\n    self._compareShapeSparse(x, use_gpu=False)\n    self._compareRankSparse(x, use_gpu=False)\n    self._compareSizeSparse(x, use_gpu=False)\n\n  def _testGpu(self, x):\n    self._compareShape(x, use_gpu=True)\n    self._compareShapeN(x, use_gpu=True)\n    self._compareRank(x, use_gpu=True)\n    self._compareSize(x, use_gpu=True)\n    self._compareShapeSparse(x, use_gpu=True)\n    self._compareRankSparse(x, use_gpu=True)\n    self._compareSizeSparse(x, use_gpu=True)\n\n  def _testAll(self, x):\n    self._testCpu(x)\n    self._testGpu(x)\n\n  def testBasic(self):\n    self._testAll(np.random.randn(2))\n    self._testAll(np.random.randn(2, 3))\n    self._testAll(np.random.randn(2, 3, 5))\n    self._testAll(np.random.randn(2, 3, 5, 7))\n    self._testAll(np.random.randn(2, 3, 5, 7, 11))\n    self._testAll(np.random.randn(2, 3, 5, 7, 11, 13))\n\n  def testBool(self):\n    self._testAll(np.random.choice((False, True), size=(2,)))\n    self._testAll(np.random.choice((False, True), size=(2, 3)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7, 11)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7, 11, 13)))\n\n  # Disabled because it takes too long to run, but manually verified\n  # as passing at time of writing.\n  def _test64BitOutput(self):\n    with self.cached_session():\n      inp = array_ops.zeros([2**31])\n      num_elements = array_ops.size_internal(\n          inp, optimize=False, out_type=dtypes.int64)\n      self.assertEqual(2**31, self.evaluate(num_elements))\n\n    # Too large for tf.int32 output.\n    with self.assertRaises(errors_impl.InvalidArgumentError):\n      with self.cached_session():\n        inp = array_ops.zeros([2**31])\n        num_elements = array_ops.size_internal(\n            inp, optimize=False, out_type=dtypes.int32)\n        self.assertEqual(2**31, self.evaluate(num_elements))\n\n  def _compareExpandDims(self, x, dim, use_gpu):\n    np_ans = np.expand_dims(x, axis=dim)\n    with self.cached_session(use_gpu=use_gpu):\n      tensor = array_ops.expand_dims(x, dim)\n      tf_ans = self.evaluate(tensor)\n    self.assertShapeEqual(np_ans, tensor)\n    self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareExpandDimsAll(self, x, dim):\n    self._compareExpandDims(x, dim, False)\n    self._compareExpandDims(x, dim, True)\n\n  def testExpandDims(self):\n    self._compareExpandDimsAll(np.zeros([2]), 0)\n    self._compareExpandDimsAll(np.zeros([2]), 1)\n    self._compareExpandDimsAll(np.zeros([2]), -1)\n\n    self._compareExpandDimsAll(np.zeros([2, 3]), 0)\n    self._compareExpandDimsAll(np.zeros([2, 3]), 1)\n    self._compareExpandDimsAll(np.zeros([2, 3]), 2)\n    self._compareExpandDimsAll(np.zeros([2, 3]), -1)\n    self._compareExpandDimsAll(np.zeros([2, 3]), -2)\n\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 0)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 1)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 2)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 3)\n\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -1)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -2)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -3)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -4)\n\n  def testExpandDimsBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    self._compareExpandDimsAll(choice([2]), 0)\n    self._compareExpandDimsAll(choice([2]), 1)\n    self._compareExpandDimsAll(choice([2]), -1)\n\n    self._compareExpandDimsAll(choice([2, 3]), 0)\n    self._compareExpandDimsAll(choice([2, 3]), 1)\n    self._compareExpandDimsAll(choice([2, 3]), 2)\n    self._compareExpandDimsAll(choice([2, 3]), -1)\n    self._compareExpandDimsAll(choice([2, 3]), -2)\n\n    self._compareExpandDimsAll(choice([2, 3, 5]), 0)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 1)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 2)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 3)\n\n    self._compareExpandDimsAll(choice([2, 3, 5]), -1)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -2)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -3)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -4)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsErrors(self):\n    with self.cached_session():\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        np.zeros([2, 3, 5]), -5)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        [False, True, True], -5)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        np.zeros([2, 3, 5]), 4)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        [False, True, True], 4)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsGradient(self):\n    with self.cached_session():\n      inp = constant_op.constant(\n          np.random.rand(4, 2).astype(\"f\"), dtype=dtypes.float32)\n      squeezed = array_ops.expand_dims(inp, 1)\n\n      err = gradient_checker.compute_gradient_error(inp, [4, 2], squeezed,\n                                                    [4, 1, 2])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsScalar(self):\n    with self.cached_session():\n      inp = constant_op.constant(7)\n      self.assertAllEqual([7], array_ops.expand_dims(inp, 0))\n      self.assertAllEqual([7], array_ops.expand_dims(inp, -1))\n\n      inp = constant_op.constant(True)\n      self.assertAllEqual([True], array_ops.expand_dims(inp, 0))\n      self.assertAllEqual([True], array_ops.expand_dims(inp, -1))\n\n  def testExpandDimsDimType(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      x = np.zeros([2])\n      np_ans = np.expand_dims(x, axis=0)\n      with self.cached_session():\n        tensor = array_ops.expand_dims(x, constant_op.constant(0, dtype))\n        tf_ans = self.evaluate(tensor)\n      self.assertShapeEqual(np_ans, tensor)\n      self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareSqueeze(self, x, squeeze_dims, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      if squeeze_dims:\n        np_ans = np.squeeze(x, axis=tuple(squeeze_dims))\n        tensor = array_ops.squeeze(x, squeeze_dims)\n        tf_ans = self.evaluate(tensor)\n      else:\n        np_ans = np.squeeze(x)\n        tensor = array_ops.squeeze(x)\n        tf_ans = self.evaluate(tensor)\n    self.assertShapeEqual(np_ans, tensor)\n    self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareSqueezeAll(self, x, squeeze_dims=None):\n    if squeeze_dims is None:\n      squeeze_dims = []\n    self._compareSqueeze(x, squeeze_dims, False)\n    self._compareSqueeze(x, squeeze_dims, True)\n\n  def testSqueeze(self):\n    # Nothing to squeeze.\n    self._compareSqueezeAll(np.zeros([2]))\n    self._compareSqueezeAll(np.zeros([2, 3]))\n\n    # Squeeze the middle element away.\n    self._compareSqueezeAll(np.zeros([2, 1, 2]))\n\n    # Squeeze on both ends.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]))\n\n  def testSqueezeBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    # Nothing to squeeze.\n    self._compareSqueezeAll(choice([2]))\n    self._compareSqueezeAll(choice([2, 3]))\n\n    # Squeeze the middle element away.\n    self._compareSqueezeAll(choice([2, 1, 2]))\n\n    # Squeeze on both ends.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]))\n\n  def testSqueezeSpecificDimension(self):\n    # Positive squeeze dim index.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [0])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [2, 4])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [0, 4, 2])\n\n    # Negative squeeze dim index.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-1])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-3, -5])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-3, -5, -1])\n\n  def testSqueezeSpecificDimensionBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    # Positive squeeze dim index.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [0])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [2, 4])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [0, 4, 2])\n\n    # Negative squeeze dim index.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-1])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-3, -5])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-3, -5, -1])\n\n  def testSqueezeAllOnes(self):\n    # Numpy squeezes a 1 element tensor into a zero dimensional tensor.\n    # Verify that we do the same.\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        tensor = array_ops.squeeze(np.zeros([1, 1, 1]), [])\n        self.assertEqual(np.shape(1), tensor.get_shape())\n        tf_ans = self.evaluate(tensor)\n        self.assertEqual(np.shape(1), tf_ans.shape)\n\n  def testSqueezeAllOnesBool(self):\n    # Numpy squeezes a 1 element tensor into a zero dimensional tensor.\n    # Verify that we do the same.\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        tensor = array_ops.squeeze([[[False]]], [])\n        self.assertEqual(np.shape(1), tensor.get_shape())\n        tf_ans = self.evaluate(tensor)\n        self.assertEqual(np.shape(1), tf_ans.shape)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeOnlyOnes(self):\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        input_1x1x3 = np.zeros([1, 1, 3])\n        self._compareSqueezeAll(input_1x1x3)\n        self._compareSqueezeAll(input_1x1x3, [0])\n        self._compareSqueezeAll(input_1x1x3, [1])\n        self.assertRaises(ValueError, array_ops.squeeze, input_1x1x3, [2])\n\n  @test_util.run_deprecated_v1\n  def testSqueezeErrors(self):\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [-4])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [0, -4])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [3])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [2, 3])\n\n  @test_util.run_deprecated_v1\n  def testSqueezeGradient(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = array_ops.reshape(inp, [4, 1, 2])\n      squeezed = array_ops.squeeze(a, [])\n\n      err = gradient_checker.compute_gradient_error(a, [4, 1, 2], squeezed,\n                                                    [4, 2])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeGradientWithSqueezeDims(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = array_ops.reshape(inp, [4, 1, 2, 1])\n      squeezed = array_ops.squeeze(a, [1])\n\n      err = gradient_checker.compute_gradient_error(a, [4, 1, 2, 1], squeezed,\n                                                    [4, 2, 1])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeWithUnknownShape(self):\n    with self.cached_session():\n      a = array_ops.placeholder(dtypes.float32, shape=[2, None])\n\n      squeezed = array_ops.squeeze(a, [1])\n      self.assertEqual([2], squeezed.get_shape().as_list())\n\n      squeezed = array_ops.squeeze(a)\n      self.assertEqual(None, squeezed.get_shape())\n\n      self.assertRaises(ValueError, array_ops.squeeze, a, [0])\n      self.assertRaises(ValueError, array_ops.squeeze, a, [100])\n\n\nclass TileTest(test.TestCase, parameterized.TestCase):\n\n  def testScalar(self):\n    for use_gpu in False, True:\n      with self.cached_session(use_gpu=use_gpu):\n        a = constant_op.constant(7, shape=[], dtype=dtypes.float32)\n        tiled = array_ops.tile(a, [])\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, ())\n      self.assertEqual([], tiled.get_shape())\n      self.assertEqual(7, result)\n\n  def testSimple(self):\n    # multiples could be int32 or int64\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.cached_session():\n        inp = np.random.rand(4, 1).astype(np.float32)\n        a = constant_op.constant(inp)\n        tiled = array_ops.tile(a, constant_op.constant([1, 4], dtype=dtype))\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, (4, 4))\n      self.assertEqual([4, 4], tiled.get_shape())\n      self.assertTrue((result == np.tile(inp, (1, 4))).all())\n\n  def testIdentityTileAndGrad(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(np.float32)\n      a = constant_op.constant(inp)\n      tiled = array_ops.tile(a, [1, 1])\n      result = self.evaluate(tiled)\n    self.assertEqual(result.shape, (4, 1))\n    self.assertEqual([4, 1], tiled.get_shape())\n    self.assertTrue((result == np.tile(inp, (1, 1))).all())\n\n  def testEmpty(self):\n    with self.cached_session():\n      inp = np.random.rand(2, 3).astype(np.float32)\n      a = constant_op.constant(inp)\n      tiled = array_ops.tile(a, [5, 0])\n      result = self.evaluate(tiled)\n    self.assertEqual(result.shape, (10, 0))\n    self.assertEqual([10, 0], tiled.get_shape())\n\n  @test_util.run_deprecated_v1\n  def testUnknownInputShape(self):\n    \"\"\"Importing can call _TileShape without shape of <multiples> known.\"\"\"\n    with self.cached_session():\n      inp = array_ops.placeholder(dtypes.float32)  # unknown shape\n      multiples = constant_op.constant([1, 2, 3, 4], dtype=np.int32)\n      tiled = array_ops.tile(inp, multiples)\n      gdef = tiled.graph.as_graph_def()\n\n      # Move the tile op to the start of the graph so that shapes of its inputs\n      # are not available when the shape function runs on import.\n      swapped = False\n      for i, n in enumerate(gdef.node):\n        if n.op == \"Tile\":\n          # Swap tile op to be first in gdef.node\n          assert i != 0\n          new_node = node_def_pb2.NodeDef()\n          new_node.CopyFrom(gdef.node[i])\n          gdef.node[i].CopyFrom(gdef.node[0])\n          gdef.node[0].CopyFrom(new_node)\n          swapped = True\n      assert swapped\n\n      tiled_imported, = importer.import_graph_def(\n          gdef, return_elements=[tiled.name])\n      self.assertEqual(4, tiled_imported.get_shape().ndims)\n\n  def testTypes(self):\n    types_to_test = {\n        \"bool\": (dtypes.bool, bool),\n        \"float32\": (dtypes.float32, float),\n        \"float64\": (dtypes.float64, float),\n        \"complex64\": (dtypes.complex64, complex),\n        \"complex128\": (dtypes.complex128, complex),\n        \"uint8\": (dtypes.uint8, int),\n        \"int8\": (dtypes.int8, int),\n        \"int16\": (dtypes.int16, int),\n        \"int32\": (dtypes.int32, int),\n        \"int64\": (dtypes.int64, int),\n        \"uint32\": (dtypes.uint32, int),\n        \"uint64\": (dtypes.uint64, int),\n        bytes: (dtypes.string, bytes)\n    }\n    for dtype_np, (dtype_tf, cast) in types_to_test.items():\n      with self.cached_session():\n        inp = np.random.rand(4, 1).astype(dtype_np)\n        a = constant_op.constant(\n            [cast(x) for x in inp.ravel(order=\"C\")],\n            shape=[4, 1],\n            dtype=dtype_tf)\n        tiled = array_ops.tile(a, [1, 4])\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, (4, 4))\n      self.assertEqual([4, 4], tiled.get_shape())\n      self.assertAllEqual(result, np.tile(inp, (1, 4)))\n\n  @test_util.run_deprecated_v1\n  def testInvalidDim(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.ravel(order=\"C\")],\n          shape=[4, 1],\n          dtype=dtypes.float32)\n      # Wrong length of multiples.\n      with self.assertRaises(ValueError):\n        array_ops.tile(a, [1, 4, 2])\n      # Wrong rank for multiples.\n      with self.assertRaises(ValueError):\n        array_ops.tile(a, [[2, 3], [3, 4]]).eval()\n\n  def _RunAndVerifyResult(self, rank, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      # Random dims of given rank\n      input_shape = np.random.randint(1, 4, size=rank)\n      inp = np.random.rand(*input_shape).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.ravel(order=\"C\")],\n          shape=input_shape,\n          dtype=dtypes.float32)\n      multiples = np.random.randint(1, 4, size=rank).astype(np.int32)\n      tiled = array_ops.tile(a, multiples)\n      result = self.evaluate(tiled)\n    self.assertTrue((np.array(multiples) * np.array(inp.shape) == np.array(\n        result.shape)).all())\n    self.assertAllEqual(result, np.tile(inp, tuple(multiples)))\n    self.assertShapeEqual(result, tiled)\n\n  def testRandom(self):\n    # test low rank, like 5\n    for _ in range(5):\n      self._RunAndVerifyResult(5, use_gpu=False)\n    for _ in range(5):\n      self._RunAndVerifyResult(5, use_gpu=True)\n    # test high rank, like 10\n    for _ in range(5):\n      self._RunAndVerifyResult(10, use_gpu=False)\n    for _ in range(5):\n      self._RunAndVerifyResult(10, use_gpu=True)\n\n  @parameterized.parameters(dtypes.int32, dtypes.int64)\n  @test_util.run_deprecated_v1\n  def testGradientSimpleReduction(self, multiples_dtype):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 1], dtype=dtypes.float32)\n      multiples = constant_op.constant([1, 4], dtype=multiples_dtype)\n      tiled = array_ops.tile(a, multiples)\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      self.assertShapeEqual(inp, grad)\n      result = self.evaluate(grad)\n    self.assertAllClose(np.sum(grad_inp, axis=1).reshape(4, 1), result, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReduction(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      self.assertShapeEqual(inp, grad)\n      result = self.evaluate(grad)\n    expected_shape = [4, 2]\n    expected = np.zeros(expected_shape)\n    expected[:, 0] = grad_inp[:, 0] + grad_inp[:, 2]\n    expected[:, 1] = grad_inp[:, 1] + grad_inp[:, 3]\n    self.assertTrue((np.abs(expected - result) < 1e-3).all())\n\n  @test_util.run_deprecated_v1\n  def testGradientSimpleReductionOnGPU(self):\n    with self.session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 1], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 4])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      result = self.evaluate(grad)\n    self.assertAllClose(np.sum(grad_inp, axis=1).reshape(4, 1), result, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReductionOnGPU(self):\n    with self.session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      result = self.evaluate(grad)\n    expected_shape = [4, 2]\n    expected = np.zeros(expected_shape)\n    expected[:, 0] = grad_inp[:, 0] + grad_inp[:, 2]\n    expected[:, 1] = grad_inp[:, 1] + grad_inp[:, 3]\n    self.assertAllClose(expected, result, 1e-3)\n\n  def _RunAndVerifyGradientResult(self, input_shape, multiples):\n    for use_gpu in False, True:\n      with self.cached_session(use_gpu=use_gpu):\n        # Random values\n        inp = np.asarray(np.random.rand(*input_shape))\n        a = constant_op.constant(inp, dtype=dtypes.float64)\n        tiled = array_ops.tile(a, multiples)\n        grad_shape = list(np.array(multiples) * np.array(inp.shape))\n        err = gradient_checker.compute_gradient_error(\n            a, list(input_shape), tiled, grad_shape, x_init_value=inp)\n      print(\"tile(float) error = \", err)\n      self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientRandomScalar(self):\n    self._RunAndVerifyGradientResult([], [])\n\n  @test_util.run_deprecated_v1\n  def testGradientRandom(self):\n    self._RunAndVerifyGradientResult([2, 2, 1, 1, 3], [1, 1, 1, 1, 1])\n    self._RunAndVerifyGradientResult([2, 2, 1, 1, 3], [1, 2, 1, 3, 1])\n    self._RunAndVerifyGradientResult([2, 3, 1, 1, 3], [3, 1, 1, 2, 2])\n    self._RunAndVerifyGradientResult([2, 1, 3, 3, 2], [1, 3, 3, 1, 2])\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReductionGC(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      err = gradient_checker.compute_gradient_error(a, [4, 2], tiled, [4, 4])\n    self.assertLess(err, 1e-3)\n\n  @parameterized.parameters(dtypes.int32, dtypes.int64)\n  @test_util.run_deprecated_v1\n  def testGradientWithSparseGradWithRank1(self, multiples_dtype):\n    inputs = constant_op.constant([1.0, 2.0, 3.0, 4.0],\n                                  dtype=dtypes.float32)\n    multiples = constant_op.constant([3], dtype=dtypes.int64)\n    outputs = array_ops.gather(array_ops.tile(inputs, multiples),\n                               [1, 5, 9, 3, 7, 2, 2, 2])\n    with self.cached_session():\n      error = gradient_checker.compute_gradient_error(\n          inputs, inputs.get_shape().as_list(),\n          outputs, outputs.get_shape().as_list())\n      self.assertLess(error, 1e-4)\n\n  @test_util.run_deprecated_v1\n  def testGradientWithSparseGradWithRank3(self):\n    inputs = constant_op.constant([1.0, 2.0, 3.0, 4.0],\n                                  dtype=dtypes.float32)\n    inputs = array_ops.reshape(inputs, [-1, 1, 1])\n    outputs = array_ops.gather(array_ops.tile(inputs, [3, 4, 2]),\n                               [1, 5, 9, 3, 7, 2, 2, 2])\n    with self.cached_session():\n      error = gradient_checker.compute_gradient_error(\n          inputs, inputs.get_shape().as_list(),\n          outputs, outputs.get_shape().as_list())\n      self.assertLess(error, 1e-4)\n\n  @test_util.run_deprecated_v1\n  def testShapeFunctionEdgeCases(self):\n    # Unknown multiples shape.\n    inp = constant_op.constant(0.0, shape=[4, 4, 4, 4])\n    tiled = array_ops.tile(inp, array_ops.placeholder(dtypes.int32))\n    self.assertEqual([None, None, None, None], tiled.get_shape().as_list())\n\n    # Unknown input shape.\n    inp = array_ops.placeholder(dtypes.float32)\n    tiled = array_ops.tile(inp, [2, 2, 2, 2])\n    self.assertEqual([None, None, None, None], tiled.get_shape().as_list())\n\n    # Unknown input and multiples shape.\n    inp = array_ops.placeholder(dtypes.float32)\n    tiled = array_ops.tile(inp, array_ops.placeholder(dtypes.int32))\n    self.assertIs(None, tiled.get_shape().ndims)\n\n    # Known input and partially known multiples.\n    inp = constant_op.constant(0.0, shape=[1, 1])\n    tiled = array_ops.tile(inp, [array_ops.placeholder(dtypes.int32), 7])\n    self.assertEqual([None, 7], tiled.get_shape().as_list())\n\n    # Mismatched input rank and multiples length.\n    inp = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    with self.assertRaises(ValueError):\n      tiled = array_ops.tile(\n          inp, array_ops.placeholder(\n              dtypes.int32, shape=[3]))\n\n  def testLargeTensor(self):\n    # Test case for GItHub issue 46911.\n    if test_util.is_xla_enabled():\n      # The following test fails with XLA enabled.\n      return\n    with self.assertRaises(errors_impl.InternalError):\n      with self.cached_session():\n        tiled = array_ops.tile(\n            np.ones((1, 1, 1)), [100000000, 100000000, 100000000])\n        self.evaluate(tiled)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/tile_ops.cc", "tensorflow/python/kernel_tests/shape_ops_test.py"], "buggy_code_start_loc": [191, 725], "buggy_code_end_loc": [192, 725], "fixing_code_start_loc": [191, 726], "fixing_code_end_loc": [193, 737], "type": "CWE-190", "message": "TensorFlow is an open source platform for machine learning. In affected versions if `tf.tile` is called with a large input argument then the TensorFlow process will crash due to a `CHECK`-failure caused by an overflow. The number of elements in the output tensor is too much for the `int64_t` type and the overflow is detected via a `CHECK` statement. This aborts the process. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41198", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T20:15:07.907", "lastModified": "2021-11-09T14:40:40.990", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions if `tf.tile` is called with a large input argument then the TensorFlow process will crash due to a `CHECK`-failure caused by an overflow. The number of elements in the output tensor is too much for the `int64_t` type and the overflow is detected via a `CHECK` statement. This aborts the process. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, si se llama a \"tf.tile\" con un argumento de entrada grande, el proceso de TensorFlow ser\u00e1 bloqueado debido a un fallo de \"CHECK\" causado por un desbordamiento. El n\u00famero de elementos en el tensor de salida es demasiado para el tipo \"int64_t\" y el desbordamiento es detectado por medio de una sentencia \"CHECK\". Esto aborta el proceso. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n ser\u00e1 incluida este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.4.4", "matchCriteriaId": "455FB550-4C9C-4BD6-9F76-A627B62AB332"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndExcluding": "2.6.1", "matchCriteriaId": "5D68D8D1-DB27-4395-9D3D-2BED901B852C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/9294094df6fea79271778eb7e7ae1bad8b5ef98f", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/issues/46911", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-2p25-55c9-h58q", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/9294094df6fea79271778eb7e7ae1bad8b5ef98f"}}