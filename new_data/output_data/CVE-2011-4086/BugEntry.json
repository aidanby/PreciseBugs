{"buggy_code": ["/*\n * linux/fs/jbd2/transaction.c\n *\n * Written by Stephen C. Tweedie <sct@redhat.com>, 1998\n *\n * Copyright 1998 Red Hat corp --- All Rights Reserved\n *\n * This file is part of the Linux kernel and is made available under\n * the terms of the GNU General Public License, version 2, or at your\n * option, any later version, incorporated herein by reference.\n *\n * Generic filesystem transaction handling code; part of the ext2fs\n * journaling system.\n *\n * This file manages transactions (compound commits managed by the\n * journaling code) and handles (individual atomic operations by the\n * filesystem).\n */\n\n#include <linux/time.h>\n#include <linux/fs.h>\n#include <linux/jbd2.h>\n#include <linux/errno.h>\n#include <linux/slab.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/hrtimer.h>\n#include <linux/backing-dev.h>\n#include <linux/bug.h>\n#include <linux/module.h>\n\nstatic void __jbd2_journal_temp_unlink_buffer(struct journal_head *jh);\nstatic void __jbd2_journal_unfile_buffer(struct journal_head *jh);\n\n/*\n * jbd2_get_transaction: obtain a new transaction_t object.\n *\n * Simply allocate and initialise a new transaction.  Create it in\n * RUNNING state and add it to the current journal (which should not\n * have an existing running transaction: we only make a new transaction\n * once we have started to commit the old one).\n *\n * Preconditions:\n *\tThe journal MUST be locked.  We don't perform atomic mallocs on the\n *\tnew transaction\tand we can't block without protecting against other\n *\tprocesses trying to touch the journal while it is in transition.\n *\n */\n\nstatic transaction_t *\njbd2_get_transaction(journal_t *journal, transaction_t *transaction)\n{\n\ttransaction->t_journal = journal;\n\ttransaction->t_state = T_RUNNING;\n\ttransaction->t_start_time = ktime_get();\n\ttransaction->t_tid = journal->j_transaction_sequence++;\n\ttransaction->t_expires = jiffies + journal->j_commit_interval;\n\tspin_lock_init(&transaction->t_handle_lock);\n\tatomic_set(&transaction->t_updates, 0);\n\tatomic_set(&transaction->t_outstanding_credits, 0);\n\tatomic_set(&transaction->t_handle_count, 0);\n\tINIT_LIST_HEAD(&transaction->t_inode_list);\n\tINIT_LIST_HEAD(&transaction->t_private_list);\n\n\t/* Set up the commit timer for the new transaction. */\n\tjournal->j_commit_timer.expires = round_jiffies_up(transaction->t_expires);\n\tadd_timer(&journal->j_commit_timer);\n\n\tJ_ASSERT(journal->j_running_transaction == NULL);\n\tjournal->j_running_transaction = transaction;\n\ttransaction->t_max_wait = 0;\n\ttransaction->t_start = jiffies;\n\n\treturn transaction;\n}\n\n/*\n * Handle management.\n *\n * A handle_t is an object which represents a single atomic update to a\n * filesystem, and which tracks all of the modifications which form part\n * of that one update.\n */\n\n/*\n * Update transaction's maximum wait time, if debugging is enabled.\n *\n * In order for t_max_wait to be reliable, it must be protected by a\n * lock.  But doing so will mean that start_this_handle() can not be\n * run in parallel on SMP systems, which limits our scalability.  So\n * unless debugging is enabled, we no longer update t_max_wait, which\n * means that maximum wait time reported by the jbd2_run_stats\n * tracepoint will always be zero.\n */\nstatic inline void update_t_max_wait(transaction_t *transaction,\n\t\t\t\t     unsigned long ts)\n{\n#ifdef CONFIG_JBD2_DEBUG\n\tif (jbd2_journal_enable_debug &&\n\t    time_after(transaction->t_start, ts)) {\n\t\tts = jbd2_time_diff(ts, transaction->t_start);\n\t\tspin_lock(&transaction->t_handle_lock);\n\t\tif (ts > transaction->t_max_wait)\n\t\t\ttransaction->t_max_wait = ts;\n\t\tspin_unlock(&transaction->t_handle_lock);\n\t}\n#endif\n}\n\n/*\n * start_this_handle: Given a handle, deal with any locking or stalling\n * needed to make sure that there is enough journal space for the handle\n * to begin.  Attach the handle to a transaction and set up the\n * transaction's buffer credits.\n */\n\nstatic int start_this_handle(journal_t *journal, handle_t *handle,\n\t\t\t     gfp_t gfp_mask)\n{\n\ttransaction_t\t*transaction, *new_transaction = NULL;\n\ttid_t\t\ttid;\n\tint\t\tneeded, need_to_start;\n\tint\t\tnblocks = handle->h_buffer_credits;\n\tunsigned long ts = jiffies;\n\n\tif (nblocks > journal->j_max_transaction_buffers) {\n\t\tprintk(KERN_ERR \"JBD2: %s wants too many credits (%d > %d)\\n\",\n\t\t       current->comm, nblocks,\n\t\t       journal->j_max_transaction_buffers);\n\t\treturn -ENOSPC;\n\t}\n\nalloc_transaction:\n\tif (!journal->j_running_transaction) {\n\t\tnew_transaction = kzalloc(sizeof(*new_transaction), gfp_mask);\n\t\tif (!new_transaction) {\n\t\t\t/*\n\t\t\t * If __GFP_FS is not present, then we may be\n\t\t\t * being called from inside the fs writeback\n\t\t\t * layer, so we MUST NOT fail.  Since\n\t\t\t * __GFP_NOFAIL is going away, we will arrange\n\t\t\t * to retry the allocation ourselves.\n\t\t\t */\n\t\t\tif ((gfp_mask & __GFP_FS) == 0) {\n\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\t\tgoto alloc_transaction;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tjbd_debug(3, \"New handle %p going live.\\n\", handle);\n\n\t/*\n\t * We need to hold j_state_lock until t_updates has been incremented,\n\t * for proper journal barrier handling\n\t */\nrepeat:\n\tread_lock(&journal->j_state_lock);\n\tBUG_ON(journal->j_flags & JBD2_UNMOUNT);\n\tif (is_journal_aborted(journal) ||\n\t    (journal->j_errno != 0 && !(journal->j_flags & JBD2_ACK_ERR))) {\n\t\tread_unlock(&journal->j_state_lock);\n\t\tkfree(new_transaction);\n\t\treturn -EROFS;\n\t}\n\n\t/* Wait on the journal's transaction barrier if necessary */\n\tif (journal->j_barrier_count) {\n\t\tread_unlock(&journal->j_state_lock);\n\t\twait_event(journal->j_wait_transaction_locked,\n\t\t\t\tjournal->j_barrier_count == 0);\n\t\tgoto repeat;\n\t}\n\n\tif (!journal->j_running_transaction) {\n\t\tread_unlock(&journal->j_state_lock);\n\t\tif (!new_transaction)\n\t\t\tgoto alloc_transaction;\n\t\twrite_lock(&journal->j_state_lock);\n\t\tif (!journal->j_running_transaction) {\n\t\t\tjbd2_get_transaction(journal, new_transaction);\n\t\t\tnew_transaction = NULL;\n\t\t}\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tgoto repeat;\n\t}\n\n\ttransaction = journal->j_running_transaction;\n\n\t/*\n\t * If the current transaction is locked down for commit, wait for the\n\t * lock to be released.\n\t */\n\tif (transaction->t_state == T_LOCKED) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tprepare_to_wait(&journal->j_wait_transaction_locked,\n\t\t\t\t\t&wait, TASK_UNINTERRUPTIBLE);\n\t\tread_unlock(&journal->j_state_lock);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_transaction_locked, &wait);\n\t\tgoto repeat;\n\t}\n\n\t/*\n\t * If there is not enough space left in the log to write all potential\n\t * buffers requested by this operation, we need to stall pending a log\n\t * checkpoint to free some more log space.\n\t */\n\tneeded = atomic_add_return(nblocks,\n\t\t\t\t   &transaction->t_outstanding_credits);\n\n\tif (needed > journal->j_max_transaction_buffers) {\n\t\t/*\n\t\t * If the current transaction is already too large, then start\n\t\t * to commit it: we can then go back and attach this handle to\n\t\t * a new transaction.\n\t\t */\n\t\tDEFINE_WAIT(wait);\n\n\t\tjbd_debug(2, \"Handle %p starting new commit...\\n\", handle);\n\t\tatomic_sub(nblocks, &transaction->t_outstanding_credits);\n\t\tprepare_to_wait(&journal->j_wait_transaction_locked, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\ttid = transaction->t_tid;\n\t\tneed_to_start = !tid_geq(journal->j_commit_request, tid);\n\t\tread_unlock(&journal->j_state_lock);\n\t\tif (need_to_start)\n\t\t\tjbd2_log_start_commit(journal, tid);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_transaction_locked, &wait);\n\t\tgoto repeat;\n\t}\n\n\t/*\n\t * The commit code assumes that it can get enough log space\n\t * without forcing a checkpoint.  This is *critical* for\n\t * correctness: a checkpoint of a buffer which is also\n\t * associated with a committing transaction creates a deadlock,\n\t * so commit simply cannot force through checkpoints.\n\t *\n\t * We must therefore ensure the necessary space in the journal\n\t * *before* starting to dirty potentially checkpointed buffers\n\t * in the new transaction.\n\t *\n\t * The worst part is, any transaction currently committing can\n\t * reduce the free space arbitrarily.  Be careful to account for\n\t * those buffers when checkpointing.\n\t */\n\n\t/*\n\t * @@@ AKPM: This seems rather over-defensive.  We're giving commit\n\t * a _lot_ of headroom: 1/4 of the journal plus the size of\n\t * the committing transaction.  Really, we only need to give it\n\t * committing_transaction->t_outstanding_credits plus \"enough\" for\n\t * the log control blocks.\n\t * Also, this test is inconsistent with the matching one in\n\t * jbd2_journal_extend().\n\t */\n\tif (__jbd2_log_space_left(journal) < jbd_space_needed(journal)) {\n\t\tjbd_debug(2, \"Handle %p waiting for checkpoint...\\n\", handle);\n\t\tatomic_sub(nblocks, &transaction->t_outstanding_credits);\n\t\tread_unlock(&journal->j_state_lock);\n\t\twrite_lock(&journal->j_state_lock);\n\t\tif (__jbd2_log_space_left(journal) < jbd_space_needed(journal))\n\t\t\t__jbd2_log_wait_for_space(journal);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tgoto repeat;\n\t}\n\n\t/* OK, account for the buffers that this operation expects to\n\t * use and add the handle to the running transaction. \n\t */\n\tupdate_t_max_wait(transaction, ts);\n\thandle->h_transaction = transaction;\n\tatomic_inc(&transaction->t_updates);\n\tatomic_inc(&transaction->t_handle_count);\n\tjbd_debug(4, \"Handle %p given %d credits (total %d, free %d)\\n\",\n\t\t  handle, nblocks,\n\t\t  atomic_read(&transaction->t_outstanding_credits),\n\t\t  __jbd2_log_space_left(journal));\n\tread_unlock(&journal->j_state_lock);\n\n\tlock_map_acquire(&handle->h_lockdep_map);\n\tkfree(new_transaction);\n\treturn 0;\n}\n\nstatic struct lock_class_key jbd2_handle_key;\n\n/* Allocate a new handle.  This should probably be in a slab... */\nstatic handle_t *new_handle(int nblocks)\n{\n\thandle_t *handle = jbd2_alloc_handle(GFP_NOFS);\n\tif (!handle)\n\t\treturn NULL;\n\tmemset(handle, 0, sizeof(*handle));\n\thandle->h_buffer_credits = nblocks;\n\thandle->h_ref = 1;\n\n\tlockdep_init_map(&handle->h_lockdep_map, \"jbd2_handle\",\n\t\t\t\t\t\t&jbd2_handle_key, 0);\n\n\treturn handle;\n}\n\n/**\n * handle_t *jbd2_journal_start() - Obtain a new handle.\n * @journal: Journal to start transaction on.\n * @nblocks: number of block buffer we might modify\n *\n * We make sure that the transaction can guarantee at least nblocks of\n * modified buffers in the log.  We block until the log can guarantee\n * that much space.\n *\n * This function is visible to journal users (like ext3fs), so is not\n * called with the journal already locked.\n *\n * Return a pointer to a newly allocated handle, or an ERR_PTR() value\n * on failure.\n */\nhandle_t *jbd2__journal_start(journal_t *journal, int nblocks, gfp_t gfp_mask)\n{\n\thandle_t *handle = journal_current_handle();\n\tint err;\n\n\tif (!journal)\n\t\treturn ERR_PTR(-EROFS);\n\n\tif (handle) {\n\t\tJ_ASSERT(handle->h_transaction->t_journal == journal);\n\t\thandle->h_ref++;\n\t\treturn handle;\n\t}\n\n\thandle = new_handle(nblocks);\n\tif (!handle)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcurrent->journal_info = handle;\n\n\terr = start_this_handle(journal, handle, gfp_mask);\n\tif (err < 0) {\n\t\tjbd2_free_handle(handle);\n\t\tcurrent->journal_info = NULL;\n\t\thandle = ERR_PTR(err);\n\t}\n\treturn handle;\n}\nEXPORT_SYMBOL(jbd2__journal_start);\n\n\nhandle_t *jbd2_journal_start(journal_t *journal, int nblocks)\n{\n\treturn jbd2__journal_start(journal, nblocks, GFP_NOFS);\n}\nEXPORT_SYMBOL(jbd2_journal_start);\n\n\n/**\n * int jbd2_journal_extend() - extend buffer credits.\n * @handle:  handle to 'extend'\n * @nblocks: nr blocks to try to extend by.\n *\n * Some transactions, such as large extends and truncates, can be done\n * atomically all at once or in several stages.  The operation requests\n * a credit for a number of buffer modications in advance, but can\n * extend its credit if it needs more.\n *\n * jbd2_journal_extend tries to give the running handle more buffer credits.\n * It does not guarantee that allocation - this is a best-effort only.\n * The calling process MUST be able to deal cleanly with a failure to\n * extend here.\n *\n * Return 0 on success, non-zero on failure.\n *\n * return code < 0 implies an error\n * return code > 0 implies normal transaction-full status.\n */\nint jbd2_journal_extend(handle_t *handle, int nblocks)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tint result;\n\tint wanted;\n\n\tresult = -EIO;\n\tif (is_handle_aborted(handle))\n\t\tgoto out;\n\n\tresult = 1;\n\n\tread_lock(&journal->j_state_lock);\n\n\t/* Don't extend a locked-down transaction! */\n\tif (handle->h_transaction->t_state != T_RUNNING) {\n\t\tjbd_debug(3, \"denied handle %p %d blocks: \"\n\t\t\t  \"transaction not running\\n\", handle, nblocks);\n\t\tgoto error_out;\n\t}\n\n\tspin_lock(&transaction->t_handle_lock);\n\twanted = atomic_read(&transaction->t_outstanding_credits) + nblocks;\n\n\tif (wanted > journal->j_max_transaction_buffers) {\n\t\tjbd_debug(3, \"denied handle %p %d blocks: \"\n\t\t\t  \"transaction too large\\n\", handle, nblocks);\n\t\tgoto unlock;\n\t}\n\n\tif (wanted > __jbd2_log_space_left(journal)) {\n\t\tjbd_debug(3, \"denied handle %p %d blocks: \"\n\t\t\t  \"insufficient log space\\n\", handle, nblocks);\n\t\tgoto unlock;\n\t}\n\n\thandle->h_buffer_credits += nblocks;\n\tatomic_add(nblocks, &transaction->t_outstanding_credits);\n\tresult = 0;\n\n\tjbd_debug(3, \"extended handle %p by %d\\n\", handle, nblocks);\nunlock:\n\tspin_unlock(&transaction->t_handle_lock);\nerror_out:\n\tread_unlock(&journal->j_state_lock);\nout:\n\treturn result;\n}\n\n\n/**\n * int jbd2_journal_restart() - restart a handle .\n * @handle:  handle to restart\n * @nblocks: nr credits requested\n *\n * Restart a handle for a multi-transaction filesystem\n * operation.\n *\n * If the jbd2_journal_extend() call above fails to grant new buffer credits\n * to a running handle, a call to jbd2_journal_restart will commit the\n * handle's transaction so far and reattach the handle to a new\n * transaction capabable of guaranteeing the requested number of\n * credits.\n */\nint jbd2__journal_restart(handle_t *handle, int nblocks, gfp_t gfp_mask)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\ttid_t\t\ttid;\n\tint\t\tneed_to_start, ret;\n\n\t/* If we've had an abort of any type, don't even think about\n\t * actually doing the restart! */\n\tif (is_handle_aborted(handle))\n\t\treturn 0;\n\n\t/*\n\t * First unlink the handle from its current transaction, and start the\n\t * commit on that.\n\t */\n\tJ_ASSERT(atomic_read(&transaction->t_updates) > 0);\n\tJ_ASSERT(journal_current_handle() == handle);\n\n\tread_lock(&journal->j_state_lock);\n\tspin_lock(&transaction->t_handle_lock);\n\tatomic_sub(handle->h_buffer_credits,\n\t\t   &transaction->t_outstanding_credits);\n\tif (atomic_dec_and_test(&transaction->t_updates))\n\t\twake_up(&journal->j_wait_updates);\n\tspin_unlock(&transaction->t_handle_lock);\n\n\tjbd_debug(2, \"restarting handle %p\\n\", handle);\n\ttid = transaction->t_tid;\n\tneed_to_start = !tid_geq(journal->j_commit_request, tid);\n\tread_unlock(&journal->j_state_lock);\n\tif (need_to_start)\n\t\tjbd2_log_start_commit(journal, tid);\n\n\tlock_map_release(&handle->h_lockdep_map);\n\thandle->h_buffer_credits = nblocks;\n\tret = start_this_handle(journal, handle, gfp_mask);\n\treturn ret;\n}\nEXPORT_SYMBOL(jbd2__journal_restart);\n\n\nint jbd2_journal_restart(handle_t *handle, int nblocks)\n{\n\treturn jbd2__journal_restart(handle, nblocks, GFP_NOFS);\n}\nEXPORT_SYMBOL(jbd2_journal_restart);\n\n/**\n * void jbd2_journal_lock_updates () - establish a transaction barrier.\n * @journal:  Journal to establish a barrier on.\n *\n * This locks out any further updates from being started, and blocks\n * until all existing updates have completed, returning only once the\n * journal is in a quiescent state with no updates running.\n *\n * The journal lock should not be held on entry.\n */\nvoid jbd2_journal_lock_updates(journal_t *journal)\n{\n\tDEFINE_WAIT(wait);\n\n\twrite_lock(&journal->j_state_lock);\n\t++journal->j_barrier_count;\n\n\t/* Wait until there are no running updates */\n\twhile (1) {\n\t\ttransaction_t *transaction = journal->j_running_transaction;\n\n\t\tif (!transaction)\n\t\t\tbreak;\n\n\t\tspin_lock(&transaction->t_handle_lock);\n\t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (!atomic_read(&transaction->t_updates)) {\n\t\t\tspin_unlock(&transaction->t_handle_lock);\n\t\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&transaction->t_handle_lock);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n\twrite_unlock(&journal->j_state_lock);\n\n\t/*\n\t * We have now established a barrier against other normal updates, but\n\t * we also need to barrier against other jbd2_journal_lock_updates() calls\n\t * to make sure that we serialise special journal-locked operations\n\t * too.\n\t */\n\tmutex_lock(&journal->j_barrier);\n}\n\n/**\n * void jbd2_journal_unlock_updates (journal_t* journal) - release barrier\n * @journal:  Journal to release the barrier on.\n *\n * Release a transaction barrier obtained with jbd2_journal_lock_updates().\n *\n * Should be called without the journal lock held.\n */\nvoid jbd2_journal_unlock_updates (journal_t *journal)\n{\n\tJ_ASSERT(journal->j_barrier_count != 0);\n\n\tmutex_unlock(&journal->j_barrier);\n\twrite_lock(&journal->j_state_lock);\n\t--journal->j_barrier_count;\n\twrite_unlock(&journal->j_state_lock);\n\twake_up(&journal->j_wait_transaction_locked);\n}\n\nstatic void warn_dirty_buffer(struct buffer_head *bh)\n{\n\tchar b[BDEVNAME_SIZE];\n\n\tprintk(KERN_WARNING\n\t       \"JBD2: Spotted dirty metadata buffer (dev = %s, blocknr = %llu). \"\n\t       \"There's a risk of filesystem corruption in case of system \"\n\t       \"crash.\\n\",\n\t       bdevname(bh->b_bdev, b), (unsigned long long)bh->b_blocknr);\n}\n\n/*\n * If the buffer is already part of the current transaction, then there\n * is nothing we need to do.  If it is already part of a prior\n * transaction which we are still committing to disk, then we need to\n * make sure that we do not overwrite the old copy: we do copy-out to\n * preserve the copy going to disk.  We also account the buffer against\n * the handle's metadata buffer credits (unless the buffer is already\n * part of the transaction, that is).\n *\n */\nstatic int\ndo_get_write_access(handle_t *handle, struct journal_head *jh,\n\t\t\tint force_copy)\n{\n\tstruct buffer_head *bh;\n\ttransaction_t *transaction;\n\tjournal_t *journal;\n\tint error;\n\tchar *frozen_buffer = NULL;\n\tint need_copy = 0;\n\n\tif (is_handle_aborted(handle))\n\t\treturn -EROFS;\n\n\ttransaction = handle->h_transaction;\n\tjournal = transaction->t_journal;\n\n\tjbd_debug(5, \"journal_head %p, force_copy %d\\n\", jh, force_copy);\n\n\tJBUFFER_TRACE(jh, \"entry\");\nrepeat:\n\tbh = jh2bh(jh);\n\n\t/* @@@ Need to check for errors here at some point. */\n\n\tlock_buffer(bh);\n\tjbd_lock_bh_state(bh);\n\n\t/* We now hold the buffer lock so it is safe to query the buffer\n\t * state.  Is the buffer dirty?\n\t *\n\t * If so, there are two possibilities.  The buffer may be\n\t * non-journaled, and undergoing a quite legitimate writeback.\n\t * Otherwise, it is journaled, and we don't expect dirty buffers\n\t * in that state (the buffers should be marked JBD_Dirty\n\t * instead.)  So either the IO is being done under our own\n\t * control and this is a bug, or it's a third party IO such as\n\t * dump(8) (which may leave the buffer scheduled for read ---\n\t * ie. locked but not dirty) or tune2fs (which may actually have\n\t * the buffer dirtied, ugh.)  */\n\n\tif (buffer_dirty(bh)) {\n\t\t/*\n\t\t * First question: is this buffer already part of the current\n\t\t * transaction or the existing committing transaction?\n\t\t */\n\t\tif (jh->b_transaction) {\n\t\t\tJ_ASSERT_JH(jh,\n\t\t\t\tjh->b_transaction == transaction ||\n\t\t\t\tjh->b_transaction ==\n\t\t\t\t\tjournal->j_committing_transaction);\n\t\t\tif (jh->b_next_transaction)\n\t\t\t\tJ_ASSERT_JH(jh, jh->b_next_transaction ==\n\t\t\t\t\t\t\ttransaction);\n\t\t\twarn_dirty_buffer(bh);\n\t\t}\n\t\t/*\n\t\t * In any case we need to clean the dirty flag and we must\n\t\t * do it under the buffer lock to be sure we don't race\n\t\t * with running write-out.\n\t\t */\n\t\tJBUFFER_TRACE(jh, \"Journalling dirty buffer\");\n\t\tclear_buffer_dirty(bh);\n\t\tset_buffer_jbddirty(bh);\n\t}\n\n\tunlock_buffer(bh);\n\n\terror = -EROFS;\n\tif (is_handle_aborted(handle)) {\n\t\tjbd_unlock_bh_state(bh);\n\t\tgoto out;\n\t}\n\terror = 0;\n\n\t/*\n\t * The buffer is already part of this transaction if b_transaction or\n\t * b_next_transaction points to it\n\t */\n\tif (jh->b_transaction == transaction ||\n\t    jh->b_next_transaction == transaction)\n\t\tgoto done;\n\n\t/*\n\t * this is the first time this transaction is touching this buffer,\n\t * reset the modified flag\n\t */\n       jh->b_modified = 0;\n\n\t/*\n\t * If there is already a copy-out version of this buffer, then we don't\n\t * need to make another one\n\t */\n\tif (jh->b_frozen_data) {\n\t\tJBUFFER_TRACE(jh, \"has frozen data\");\n\t\tJ_ASSERT_JH(jh, jh->b_next_transaction == NULL);\n\t\tjh->b_next_transaction = transaction;\n\t\tgoto done;\n\t}\n\n\t/* Is there data here we need to preserve? */\n\n\tif (jh->b_transaction && jh->b_transaction != transaction) {\n\t\tJBUFFER_TRACE(jh, \"owned by older transaction\");\n\t\tJ_ASSERT_JH(jh, jh->b_next_transaction == NULL);\n\t\tJ_ASSERT_JH(jh, jh->b_transaction ==\n\t\t\t\t\tjournal->j_committing_transaction);\n\n\t\t/* There is one case we have to be very careful about.\n\t\t * If the committing transaction is currently writing\n\t\t * this buffer out to disk and has NOT made a copy-out,\n\t\t * then we cannot modify the buffer contents at all\n\t\t * right now.  The essence of copy-out is that it is the\n\t\t * extra copy, not the primary copy, which gets\n\t\t * journaled.  If the primary copy is already going to\n\t\t * disk then we cannot do copy-out here. */\n\n\t\tif (jh->b_jlist == BJ_Shadow) {\n\t\t\tDEFINE_WAIT_BIT(wait, &bh->b_state, BH_Unshadow);\n\t\t\twait_queue_head_t *wqh;\n\n\t\t\twqh = bit_waitqueue(&bh->b_state, BH_Unshadow);\n\n\t\t\tJBUFFER_TRACE(jh, \"on shadow: sleep\");\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t\t/* commit wakes up all shadow buffers after IO */\n\t\t\tfor ( ; ; ) {\n\t\t\t\tprepare_to_wait(wqh, &wait.wait,\n\t\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\t\t\tif (jh->b_jlist != BJ_Shadow)\n\t\t\t\t\tbreak;\n\t\t\t\tschedule();\n\t\t\t}\n\t\t\tfinish_wait(wqh, &wait.wait);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\t/* Only do the copy if the currently-owning transaction\n\t\t * still needs it.  If it is on the Forget list, the\n\t\t * committing transaction is past that stage.  The\n\t\t * buffer had better remain locked during the kmalloc,\n\t\t * but that should be true --- we hold the journal lock\n\t\t * still and the buffer is already on the BUF_JOURNAL\n\t\t * list so won't be flushed.\n\t\t *\n\t\t * Subtle point, though: if this is a get_undo_access,\n\t\t * then we will be relying on the frozen_data to contain\n\t\t * the new value of the committed_data record after the\n\t\t * transaction, so we HAVE to force the frozen_data copy\n\t\t * in that case. */\n\n\t\tif (jh->b_jlist != BJ_Forget || force_copy) {\n\t\t\tJBUFFER_TRACE(jh, \"generate frozen data\");\n\t\t\tif (!frozen_buffer) {\n\t\t\t\tJBUFFER_TRACE(jh, \"allocate memory for buffer\");\n\t\t\t\tjbd_unlock_bh_state(bh);\n\t\t\t\tfrozen_buffer =\n\t\t\t\t\tjbd2_alloc(jh2bh(jh)->b_size,\n\t\t\t\t\t\t\t GFP_NOFS);\n\t\t\t\tif (!frozen_buffer) {\n\t\t\t\t\tprintk(KERN_EMERG\n\t\t\t\t\t       \"%s: OOM for frozen_buffer\\n\",\n\t\t\t\t\t       __func__);\n\t\t\t\t\tJBUFFER_TRACE(jh, \"oom!\");\n\t\t\t\t\terror = -ENOMEM;\n\t\t\t\t\tjbd_lock_bh_state(bh);\n\t\t\t\t\tgoto done;\n\t\t\t\t}\n\t\t\t\tgoto repeat;\n\t\t\t}\n\t\t\tjh->b_frozen_data = frozen_buffer;\n\t\t\tfrozen_buffer = NULL;\n\t\t\tneed_copy = 1;\n\t\t}\n\t\tjh->b_next_transaction = transaction;\n\t}\n\n\n\t/*\n\t * Finally, if the buffer is not journaled right now, we need to make\n\t * sure it doesn't get written to disk before the caller actually\n\t * commits the new data\n\t */\n\tif (!jh->b_transaction) {\n\t\tJBUFFER_TRACE(jh, \"no transaction\");\n\t\tJ_ASSERT_JH(jh, !jh->b_next_transaction);\n\t\tJBUFFER_TRACE(jh, \"file as BJ_Reserved\");\n\t\tspin_lock(&journal->j_list_lock);\n\t\t__jbd2_journal_file_buffer(jh, transaction, BJ_Reserved);\n\t\tspin_unlock(&journal->j_list_lock);\n\t}\n\ndone:\n\tif (need_copy) {\n\t\tstruct page *page;\n\t\tint offset;\n\t\tchar *source;\n\n\t\tJ_EXPECT_JH(jh, buffer_uptodate(jh2bh(jh)),\n\t\t\t    \"Possible IO failure.\\n\");\n\t\tpage = jh2bh(jh)->b_page;\n\t\toffset = offset_in_page(jh2bh(jh)->b_data);\n\t\tsource = kmap_atomic(page, KM_USER0);\n\t\t/* Fire data frozen trigger just before we copy the data */\n\t\tjbd2_buffer_frozen_trigger(jh, source + offset,\n\t\t\t\t\t   jh->b_triggers);\n\t\tmemcpy(jh->b_frozen_data, source+offset, jh2bh(jh)->b_size);\n\t\tkunmap_atomic(source, KM_USER0);\n\n\t\t/*\n\t\t * Now that the frozen data is saved off, we need to store\n\t\t * any matching triggers.\n\t\t */\n\t\tjh->b_frozen_triggers = jh->b_triggers;\n\t}\n\tjbd_unlock_bh_state(bh);\n\n\t/*\n\t * If we are about to journal a buffer, then any revoke pending on it is\n\t * no longer valid\n\t */\n\tjbd2_journal_cancel_revoke(handle, jh);\n\nout:\n\tif (unlikely(frozen_buffer))\t/* It's usually NULL */\n\t\tjbd2_free(frozen_buffer, bh->b_size);\n\n\tJBUFFER_TRACE(jh, \"exit\");\n\treturn error;\n}\n\n/**\n * int jbd2_journal_get_write_access() - notify intent to modify a buffer for metadata (not data) update.\n * @handle: transaction to add buffer modifications to\n * @bh:     bh to be used for metadata writes\n *\n * Returns an error code or 0 on success.\n *\n * In full data journalling mode the buffer may be of type BJ_AsyncData,\n * because we're write()ing a buffer which is also part of a shared mapping.\n */\n\nint jbd2_journal_get_write_access(handle_t *handle, struct buffer_head *bh)\n{\n\tstruct journal_head *jh = jbd2_journal_add_journal_head(bh);\n\tint rc;\n\n\t/* We do not want to get caught playing with fields which the\n\t * log thread also manipulates.  Make sure that the buffer\n\t * completes any outstanding IO before proceeding. */\n\trc = do_get_write_access(handle, jh, 0);\n\tjbd2_journal_put_journal_head(jh);\n\treturn rc;\n}\n\n\n/*\n * When the user wants to journal a newly created buffer_head\n * (ie. getblk() returned a new buffer and we are going to populate it\n * manually rather than reading off disk), then we need to keep the\n * buffer_head locked until it has been completely filled with new\n * data.  In this case, we should be able to make the assertion that\n * the bh is not already part of an existing transaction.\n *\n * The buffer should already be locked by the caller by this point.\n * There is no lock ranking violation: it was a newly created,\n * unlocked buffer beforehand. */\n\n/**\n * int jbd2_journal_get_create_access () - notify intent to use newly created bh\n * @handle: transaction to new buffer to\n * @bh: new buffer.\n *\n * Call this if you create a new bh.\n */\nint jbd2_journal_get_create_access(handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tstruct journal_head *jh = jbd2_journal_add_journal_head(bh);\n\tint err;\n\n\tjbd_debug(5, \"journal_head %p\\n\", jh);\n\terr = -EROFS;\n\tif (is_handle_aborted(handle))\n\t\tgoto out;\n\terr = 0;\n\n\tJBUFFER_TRACE(jh, \"entry\");\n\t/*\n\t * The buffer may already belong to this transaction due to pre-zeroing\n\t * in the filesystem's new_block code.  It may also be on the previous,\n\t * committing transaction's lists, but it HAS to be in Forget state in\n\t * that case: the transaction must have deleted the buffer for it to be\n\t * reused here.\n\t */\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\tJ_ASSERT_JH(jh, (jh->b_transaction == transaction ||\n\t\tjh->b_transaction == NULL ||\n\t\t(jh->b_transaction == journal->j_committing_transaction &&\n\t\t\t  jh->b_jlist == BJ_Forget)));\n\n\tJ_ASSERT_JH(jh, jh->b_next_transaction == NULL);\n\tJ_ASSERT_JH(jh, buffer_locked(jh2bh(jh)));\n\n\tif (jh->b_transaction == NULL) {\n\t\t/*\n\t\t * Previous jbd2_journal_forget() could have left the buffer\n\t\t * with jbddirty bit set because it was being committed. When\n\t\t * the commit finished, we've filed the buffer for\n\t\t * checkpointing and marked it dirty. Now we are reallocating\n\t\t * the buffer so the transaction freeing it must have\n\t\t * committed and so it's safe to clear the dirty bit.\n\t\t */\n\t\tclear_buffer_dirty(jh2bh(jh));\n\t\t/* first access by this transaction */\n\t\tjh->b_modified = 0;\n\n\t\tJBUFFER_TRACE(jh, \"file as BJ_Reserved\");\n\t\t__jbd2_journal_file_buffer(jh, transaction, BJ_Reserved);\n\t} else if (jh->b_transaction == journal->j_committing_transaction) {\n\t\t/* first access by this transaction */\n\t\tjh->b_modified = 0;\n\n\t\tJBUFFER_TRACE(jh, \"set next transaction\");\n\t\tjh->b_next_transaction = transaction;\n\t}\n\tspin_unlock(&journal->j_list_lock);\n\tjbd_unlock_bh_state(bh);\n\n\t/*\n\t * akpm: I added this.  ext3_alloc_branch can pick up new indirect\n\t * blocks which contain freed but then revoked metadata.  We need\n\t * to cancel the revoke in case we end up freeing it yet again\n\t * and the reallocating as data - this would cause a second revoke,\n\t * which hits an assertion error.\n\t */\n\tJBUFFER_TRACE(jh, \"cancelling revoke\");\n\tjbd2_journal_cancel_revoke(handle, jh);\nout:\n\tjbd2_journal_put_journal_head(jh);\n\treturn err;\n}\n\n/**\n * int jbd2_journal_get_undo_access() -  Notify intent to modify metadata with\n *     non-rewindable consequences\n * @handle: transaction\n * @bh: buffer to undo\n *\n * Sometimes there is a need to distinguish between metadata which has\n * been committed to disk and that which has not.  The ext3fs code uses\n * this for freeing and allocating space, we have to make sure that we\n * do not reuse freed space until the deallocation has been committed,\n * since if we overwrote that space we would make the delete\n * un-rewindable in case of a crash.\n *\n * To deal with that, jbd2_journal_get_undo_access requests write access to a\n * buffer for parts of non-rewindable operations such as delete\n * operations on the bitmaps.  The journaling code must keep a copy of\n * the buffer's contents prior to the undo_access call until such time\n * as we know that the buffer has definitely been committed to disk.\n *\n * We never need to know which transaction the committed data is part\n * of, buffers touched here are guaranteed to be dirtied later and so\n * will be committed to a new transaction in due course, at which point\n * we can discard the old committed data pointer.\n *\n * Returns error number or 0 on success.\n */\nint jbd2_journal_get_undo_access(handle_t *handle, struct buffer_head *bh)\n{\n\tint err;\n\tstruct journal_head *jh = jbd2_journal_add_journal_head(bh);\n\tchar *committed_data = NULL;\n\n\tJBUFFER_TRACE(jh, \"entry\");\n\n\t/*\n\t * Do this first --- it can drop the journal lock, so we want to\n\t * make sure that obtaining the committed_data is done\n\t * atomically wrt. completion of any outstanding commits.\n\t */\n\terr = do_get_write_access(handle, jh, 1);\n\tif (err)\n\t\tgoto out;\n\nrepeat:\n\tif (!jh->b_committed_data) {\n\t\tcommitted_data = jbd2_alloc(jh2bh(jh)->b_size, GFP_NOFS);\n\t\tif (!committed_data) {\n\t\t\tprintk(KERN_EMERG \"%s: No memory for committed data\\n\",\n\t\t\t\t__func__);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tjbd_lock_bh_state(bh);\n\tif (!jh->b_committed_data) {\n\t\t/* Copy out the current buffer contents into the\n\t\t * preserved, committed copy. */\n\t\tJBUFFER_TRACE(jh, \"generate b_committed data\");\n\t\tif (!committed_data) {\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tjh->b_committed_data = committed_data;\n\t\tcommitted_data = NULL;\n\t\tmemcpy(jh->b_committed_data, bh->b_data, bh->b_size);\n\t}\n\tjbd_unlock_bh_state(bh);\nout:\n\tjbd2_journal_put_journal_head(jh);\n\tif (unlikely(committed_data))\n\t\tjbd2_free(committed_data, bh->b_size);\n\treturn err;\n}\n\n/**\n * void jbd2_journal_set_triggers() - Add triggers for commit writeout\n * @bh: buffer to trigger on\n * @type: struct jbd2_buffer_trigger_type containing the trigger(s).\n *\n * Set any triggers on this journal_head.  This is always safe, because\n * triggers for a committing buffer will be saved off, and triggers for\n * a running transaction will match the buffer in that transaction.\n *\n * Call with NULL to clear the triggers.\n */\nvoid jbd2_journal_set_triggers(struct buffer_head *bh,\n\t\t\t       struct jbd2_buffer_trigger_type *type)\n{\n\tstruct journal_head *jh = bh2jh(bh);\n\n\tjh->b_triggers = type;\n}\n\nvoid jbd2_buffer_frozen_trigger(struct journal_head *jh, void *mapped_data,\n\t\t\t\tstruct jbd2_buffer_trigger_type *triggers)\n{\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tif (!triggers || !triggers->t_frozen)\n\t\treturn;\n\n\ttriggers->t_frozen(triggers, bh, mapped_data, bh->b_size);\n}\n\nvoid jbd2_buffer_abort_trigger(struct journal_head *jh,\n\t\t\t       struct jbd2_buffer_trigger_type *triggers)\n{\n\tif (!triggers || !triggers->t_abort)\n\t\treturn;\n\n\ttriggers->t_abort(triggers, jh2bh(jh));\n}\n\n\n\n/**\n * int jbd2_journal_dirty_metadata() -  mark a buffer as containing dirty metadata\n * @handle: transaction to add buffer to.\n * @bh: buffer to mark\n *\n * mark dirty metadata which needs to be journaled as part of the current\n * transaction.\n *\n * The buffer must have previously had jbd2_journal_get_write_access()\n * called so that it has a valid journal_head attached to the buffer\n * head.\n *\n * The buffer is placed on the transaction's metadata list and is marked\n * as belonging to the transaction.\n *\n * Returns error number or 0 on success.\n *\n * Special care needs to be taken if the buffer already belongs to the\n * current committing transaction (in which case we should have frozen\n * data present for that commit).  In that case, we don't relink the\n * buffer: that only gets done when the old transaction finally\n * completes its commit.\n */\nint jbd2_journal_dirty_metadata(handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tstruct journal_head *jh = bh2jh(bh);\n\tint ret = 0;\n\n\tjbd_debug(5, \"journal_head %p\\n\", jh);\n\tJBUFFER_TRACE(jh, \"entry\");\n\tif (is_handle_aborted(handle))\n\t\tgoto out;\n\tif (!buffer_jbd(bh)) {\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tjbd_lock_bh_state(bh);\n\n\tif (jh->b_modified == 0) {\n\t\t/*\n\t\t * This buffer's got modified and becoming part\n\t\t * of the transaction. This needs to be done\n\t\t * once a transaction -bzzz\n\t\t */\n\t\tjh->b_modified = 1;\n\t\tJ_ASSERT_JH(jh, handle->h_buffer_credits > 0);\n\t\thandle->h_buffer_credits--;\n\t}\n\n\t/*\n\t * fastpath, to avoid expensive locking.  If this buffer is already\n\t * on the running transaction's metadata list there is nothing to do.\n\t * Nobody can take it off again because there is a handle open.\n\t * I _think_ we're OK here with SMP barriers - a mistaken decision will\n\t * result in this test being false, so we go in and take the locks.\n\t */\n\tif (jh->b_transaction == transaction && jh->b_jlist == BJ_Metadata) {\n\t\tJBUFFER_TRACE(jh, \"fastpath\");\n\t\tif (unlikely(jh->b_transaction !=\n\t\t\t     journal->j_running_transaction)) {\n\t\t\tprintk(KERN_EMERG \"JBD: %s: \"\n\t\t\t       \"jh->b_transaction (%llu, %p, %u) != \"\n\t\t\t       \"journal->j_running_transaction (%p, %u)\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ? jh->b_transaction->t_tid : 0,\n\t\t\t       journal->j_running_transaction,\n\t\t\t       journal->j_running_transaction ?\n\t\t\t       journal->j_running_transaction->t_tid : 0);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tgoto out_unlock_bh;\n\t}\n\n\tset_buffer_jbddirty(bh);\n\n\t/*\n\t * Metadata already on the current transaction list doesn't\n\t * need to be filed.  Metadata on another transaction's list must\n\t * be committing, and will be refiled once the commit completes:\n\t * leave it alone for now.\n\t */\n\tif (jh->b_transaction != transaction) {\n\t\tJBUFFER_TRACE(jh, \"already on other transaction\");\n\t\tif (unlikely(jh->b_transaction !=\n\t\t\t     journal->j_committing_transaction)) {\n\t\t\tprintk(KERN_EMERG \"JBD: %s: \"\n\t\t\t       \"jh->b_transaction (%llu, %p, %u) != \"\n\t\t\t       \"journal->j_committing_transaction (%p, %u)\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ? jh->b_transaction->t_tid : 0,\n\t\t\t       journal->j_committing_transaction,\n\t\t\t       journal->j_committing_transaction ?\n\t\t\t       journal->j_committing_transaction->t_tid : 0);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tif (unlikely(jh->b_next_transaction != transaction)) {\n\t\t\tprintk(KERN_EMERG \"JBD: %s: \"\n\t\t\t       \"jh->b_next_transaction (%llu, %p, %u) != \"\n\t\t\t       \"transaction (%p, %u)\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_next_transaction,\n\t\t\t       jh->b_next_transaction ?\n\t\t\t       jh->b_next_transaction->t_tid : 0,\n\t\t\t       transaction, transaction->t_tid);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\t/* And this case is illegal: we can't reuse another\n\t\t * transaction's data buffer, ever. */\n\t\tgoto out_unlock_bh;\n\t}\n\n\t/* That test should have eliminated the following case: */\n\tJ_ASSERT_JH(jh, jh->b_frozen_data == NULL);\n\n\tJBUFFER_TRACE(jh, \"file as BJ_Metadata\");\n\tspin_lock(&journal->j_list_lock);\n\t__jbd2_journal_file_buffer(jh, handle->h_transaction, BJ_Metadata);\n\tspin_unlock(&journal->j_list_lock);\nout_unlock_bh:\n\tjbd_unlock_bh_state(bh);\nout:\n\tJBUFFER_TRACE(jh, \"exit\");\n\tWARN_ON(ret);\t/* All errors are bugs, so dump the stack */\n\treturn ret;\n}\n\n/*\n * jbd2_journal_release_buffer: undo a get_write_access without any buffer\n * updates, if the update decided in the end that it didn't need access.\n *\n */\nvoid\njbd2_journal_release_buffer(handle_t *handle, struct buffer_head *bh)\n{\n\tBUFFER_TRACE(bh, \"entry\");\n}\n\n/**\n * void jbd2_journal_forget() - bforget() for potentially-journaled buffers.\n * @handle: transaction handle\n * @bh:     bh to 'forget'\n *\n * We can only do the bforget if there are no commits pending against the\n * buffer.  If the buffer is dirty in the current running transaction we\n * can safely unlink it.\n *\n * bh may not be a journalled buffer at all - it may be a non-JBD\n * buffer which came off the hashtable.  Check for this.\n *\n * Decrements bh->b_count by one.\n *\n * Allow this call even if the handle has aborted --- it may be part of\n * the caller's cleanup after an abort.\n */\nint jbd2_journal_forget (handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tstruct journal_head *jh;\n\tint drop_reserve = 0;\n\tint err = 0;\n\tint was_modified = 0;\n\n\tBUFFER_TRACE(bh, \"entry\");\n\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\n\tif (!buffer_jbd(bh))\n\t\tgoto not_jbd;\n\tjh = bh2jh(bh);\n\n\t/* Critical error: attempting to delete a bitmap buffer, maybe?\n\t * Don't do any jbd operations, and return an error. */\n\tif (!J_EXPECT_JH(jh, !jh->b_committed_data,\n\t\t\t \"inconsistent data on disk\")) {\n\t\terr = -EIO;\n\t\tgoto not_jbd;\n\t}\n\n\t/* keep track of wether or not this transaction modified us */\n\twas_modified = jh->b_modified;\n\n\t/*\n\t * The buffer's going from the transaction, we must drop\n\t * all references -bzzz\n\t */\n\tjh->b_modified = 0;\n\n\tif (jh->b_transaction == handle->h_transaction) {\n\t\tJ_ASSERT_JH(jh, !jh->b_frozen_data);\n\n\t\t/* If we are forgetting a buffer which is already part\n\t\t * of this transaction, then we can just drop it from\n\t\t * the transaction immediately. */\n\t\tclear_buffer_dirty(bh);\n\t\tclear_buffer_jbddirty(bh);\n\n\t\tJBUFFER_TRACE(jh, \"belongs to current transaction: unfile\");\n\n\t\t/*\n\t\t * we only want to drop a reference if this transaction\n\t\t * modified the buffer\n\t\t */\n\t\tif (was_modified)\n\t\t\tdrop_reserve = 1;\n\n\t\t/*\n\t\t * We are no longer going to journal this buffer.\n\t\t * However, the commit of this transaction is still\n\t\t * important to the buffer: the delete that we are now\n\t\t * processing might obsolete an old log entry, so by\n\t\t * committing, we can satisfy the buffer's checkpoint.\n\t\t *\n\t\t * So, if we have a checkpoint on the buffer, we should\n\t\t * now refile the buffer on our BJ_Forget list so that\n\t\t * we know to remove the checkpoint after we commit.\n\t\t */\n\n\t\tif (jh->b_cp_transaction) {\n\t\t\t__jbd2_journal_temp_unlink_buffer(jh);\n\t\t\t__jbd2_journal_file_buffer(jh, transaction, BJ_Forget);\n\t\t} else {\n\t\t\t__jbd2_journal_unfile_buffer(jh);\n\t\t\tif (!buffer_jbd(bh)) {\n\t\t\t\tspin_unlock(&journal->j_list_lock);\n\t\t\t\tjbd_unlock_bh_state(bh);\n\t\t\t\t__bforget(bh);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\t} else if (jh->b_transaction) {\n\t\tJ_ASSERT_JH(jh, (jh->b_transaction ==\n\t\t\t\t journal->j_committing_transaction));\n\t\t/* However, if the buffer is still owned by a prior\n\t\t * (committing) transaction, we can't drop it yet... */\n\t\tJBUFFER_TRACE(jh, \"belongs to older transaction\");\n\t\t/* ... but we CAN drop it from the new transaction if we\n\t\t * have also modified it since the original commit. */\n\n\t\tif (jh->b_next_transaction) {\n\t\t\tJ_ASSERT(jh->b_next_transaction == transaction);\n\t\t\tjh->b_next_transaction = NULL;\n\n\t\t\t/*\n\t\t\t * only drop a reference if this transaction modified\n\t\t\t * the buffer\n\t\t\t */\n\t\t\tif (was_modified)\n\t\t\t\tdrop_reserve = 1;\n\t\t}\n\t}\n\nnot_jbd:\n\tspin_unlock(&journal->j_list_lock);\n\tjbd_unlock_bh_state(bh);\n\t__brelse(bh);\ndrop:\n\tif (drop_reserve) {\n\t\t/* no need to reserve log space for this block -bzzz */\n\t\thandle->h_buffer_credits++;\n\t}\n\treturn err;\n}\n\n/**\n * int jbd2_journal_stop() - complete a transaction\n * @handle: tranaction to complete.\n *\n * All done for a particular handle.\n *\n * There is not much action needed here.  We just return any remaining\n * buffer credits to the transaction and remove the handle.  The only\n * complication is that we need to start a commit operation if the\n * filesystem is marked for synchronous update.\n *\n * jbd2_journal_stop itself will not usually return an error, but it may\n * do so in unusual circumstances.  In particular, expect it to\n * return -EIO if a jbd2_journal_abort has been executed since the\n * transaction began.\n */\nint jbd2_journal_stop(handle_t *handle)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tint err, wait_for_commit = 0;\n\ttid_t tid;\n\tpid_t pid;\n\n\tJ_ASSERT(journal_current_handle() == handle);\n\n\tif (is_handle_aborted(handle))\n\t\terr = -EIO;\n\telse {\n\t\tJ_ASSERT(atomic_read(&transaction->t_updates) > 0);\n\t\terr = 0;\n\t}\n\n\tif (--handle->h_ref > 0) {\n\t\tjbd_debug(4, \"h_ref %d -> %d\\n\", handle->h_ref + 1,\n\t\t\t  handle->h_ref);\n\t\treturn err;\n\t}\n\n\tjbd_debug(4, \"Handle %p going down\\n\", handle);\n\n\t/*\n\t * Implement synchronous transaction batching.  If the handle\n\t * was synchronous, don't force a commit immediately.  Let's\n\t * yield and let another thread piggyback onto this\n\t * transaction.  Keep doing that while new threads continue to\n\t * arrive.  It doesn't cost much - we're about to run a commit\n\t * and sleep on IO anyway.  Speeds up many-threaded, many-dir\n\t * operations by 30x or more...\n\t *\n\t * We try and optimize the sleep time against what the\n\t * underlying disk can do, instead of having a static sleep\n\t * time.  This is useful for the case where our storage is so\n\t * fast that it is more optimal to go ahead and force a flush\n\t * and wait for the transaction to be committed than it is to\n\t * wait for an arbitrary amount of time for new writers to\n\t * join the transaction.  We achieve this by measuring how\n\t * long it takes to commit a transaction, and compare it with\n\t * how long this transaction has been running, and if run time\n\t * < commit time then we sleep for the delta and commit.  This\n\t * greatly helps super fast disks that would see slowdowns as\n\t * more threads started doing fsyncs.\n\t *\n\t * But don't do this if this process was the most recent one\n\t * to perform a synchronous write.  We do this to detect the\n\t * case where a single process is doing a stream of sync\n\t * writes.  No point in waiting for joiners in that case.\n\t */\n\tpid = current->pid;\n\tif (handle->h_sync && journal->j_last_sync_writer != pid) {\n\t\tu64 commit_time, trans_time;\n\n\t\tjournal->j_last_sync_writer = pid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tcommit_time = journal->j_average_commit_time;\n\t\tread_unlock(&journal->j_state_lock);\n\n\t\ttrans_time = ktime_to_ns(ktime_sub(ktime_get(),\n\t\t\t\t\t\t   transaction->t_start_time));\n\n\t\tcommit_time = max_t(u64, commit_time,\n\t\t\t\t    1000*journal->j_min_batch_time);\n\t\tcommit_time = min_t(u64, commit_time,\n\t\t\t\t    1000*journal->j_max_batch_time);\n\n\t\tif (trans_time < commit_time) {\n\t\t\tktime_t expires = ktime_add_ns(ktime_get(),\n\t\t\t\t\t\t       commit_time);\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tschedule_hrtimeout(&expires, HRTIMER_MODE_ABS);\n\t\t}\n\t}\n\n\tif (handle->h_sync)\n\t\ttransaction->t_synchronous_commit = 1;\n\tcurrent->journal_info = NULL;\n\tatomic_sub(handle->h_buffer_credits,\n\t\t   &transaction->t_outstanding_credits);\n\n\t/*\n\t * If the handle is marked SYNC, we need to set another commit\n\t * going!  We also want to force a commit if the current\n\t * transaction is occupying too much of the log, or if the\n\t * transaction is too old now.\n\t */\n\tif (handle->h_sync ||\n\t    (atomic_read(&transaction->t_outstanding_credits) >\n\t     journal->j_max_transaction_buffers) ||\n\t    time_after_eq(jiffies, transaction->t_expires)) {\n\t\t/* Do this even for aborted journals: an abort still\n\t\t * completes the commit thread, it just doesn't write\n\t\t * anything to disk. */\n\n\t\tjbd_debug(2, \"transaction too old, requesting commit for \"\n\t\t\t\t\t\"handle %p\\n\", handle);\n\t\t/* This is non-blocking */\n\t\tjbd2_log_start_commit(journal, transaction->t_tid);\n\n\t\t/*\n\t\t * Special case: JBD2_SYNC synchronous updates require us\n\t\t * to wait for the commit to complete.\n\t\t */\n\t\tif (handle->h_sync && !(current->flags & PF_MEMALLOC))\n\t\t\twait_for_commit = 1;\n\t}\n\n\t/*\n\t * Once we drop t_updates, if it goes to zero the transaction\n\t * could start committing on us and eventually disappear.  So\n\t * once we do this, we must not dereference transaction\n\t * pointer again.\n\t */\n\ttid = transaction->t_tid;\n\tif (atomic_dec_and_test(&transaction->t_updates)) {\n\t\twake_up(&journal->j_wait_updates);\n\t\tif (journal->j_barrier_count)\n\t\t\twake_up(&journal->j_wait_transaction_locked);\n\t}\n\n\tif (wait_for_commit)\n\t\terr = jbd2_log_wait_commit(journal, tid);\n\n\tlock_map_release(&handle->h_lockdep_map);\n\n\tjbd2_free_handle(handle);\n\treturn err;\n}\n\n/**\n * int jbd2_journal_force_commit() - force any uncommitted transactions\n * @journal: journal to force\n *\n * For synchronous operations: force any uncommitted transactions\n * to disk.  May seem kludgy, but it reuses all the handle batching\n * code in a very simple manner.\n */\nint jbd2_journal_force_commit(journal_t *journal)\n{\n\thandle_t *handle;\n\tint ret;\n\n\thandle = jbd2_journal_start(journal, 1);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t} else {\n\t\thandle->h_sync = 1;\n\t\tret = jbd2_journal_stop(handle);\n\t}\n\treturn ret;\n}\n\n/*\n *\n * List management code snippets: various functions for manipulating the\n * transaction buffer lists.\n *\n */\n\n/*\n * Append a buffer to a transaction list, given the transaction's list head\n * pointer.\n *\n * j_list_lock is held.\n *\n * jbd_lock_bh_state(jh2bh(jh)) is held.\n */\n\nstatic inline void\n__blist_add_buffer(struct journal_head **list, struct journal_head *jh)\n{\n\tif (!*list) {\n\t\tjh->b_tnext = jh->b_tprev = jh;\n\t\t*list = jh;\n\t} else {\n\t\t/* Insert at the tail of the list to preserve order */\n\t\tstruct journal_head *first = *list, *last = first->b_tprev;\n\t\tjh->b_tprev = last;\n\t\tjh->b_tnext = first;\n\t\tlast->b_tnext = first->b_tprev = jh;\n\t}\n}\n\n/*\n * Remove a buffer from a transaction list, given the transaction's list\n * head pointer.\n *\n * Called with j_list_lock held, and the journal may not be locked.\n *\n * jbd_lock_bh_state(jh2bh(jh)) is held.\n */\n\nstatic inline void\n__blist_del_buffer(struct journal_head **list, struct journal_head *jh)\n{\n\tif (*list == jh) {\n\t\t*list = jh->b_tnext;\n\t\tif (*list == jh)\n\t\t\t*list = NULL;\n\t}\n\tjh->b_tprev->b_tnext = jh->b_tnext;\n\tjh->b_tnext->b_tprev = jh->b_tprev;\n}\n\n/*\n * Remove a buffer from the appropriate transaction list.\n *\n * Note that this function can *change* the value of\n * bh->b_transaction->t_buffers, t_forget, t_iobuf_list, t_shadow_list,\n * t_log_list or t_reserved_list.  If the caller is holding onto a copy of one\n * of these pointers, it could go bad.  Generally the caller needs to re-read\n * the pointer from the transaction_t.\n *\n * Called under j_list_lock.  The journal may not be locked.\n */\nvoid __jbd2_journal_temp_unlink_buffer(struct journal_head *jh)\n{\n\tstruct journal_head **list = NULL;\n\ttransaction_t *transaction;\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tJ_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));\n\ttransaction = jh->b_transaction;\n\tif (transaction)\n\t\tassert_spin_locked(&transaction->t_journal->j_list_lock);\n\n\tJ_ASSERT_JH(jh, jh->b_jlist < BJ_Types);\n\tif (jh->b_jlist != BJ_None)\n\t\tJ_ASSERT_JH(jh, transaction != NULL);\n\n\tswitch (jh->b_jlist) {\n\tcase BJ_None:\n\t\treturn;\n\tcase BJ_Metadata:\n\t\ttransaction->t_nr_buffers--;\n\t\tJ_ASSERT_JH(jh, transaction->t_nr_buffers >= 0);\n\t\tlist = &transaction->t_buffers;\n\t\tbreak;\n\tcase BJ_Forget:\n\t\tlist = &transaction->t_forget;\n\t\tbreak;\n\tcase BJ_IO:\n\t\tlist = &transaction->t_iobuf_list;\n\t\tbreak;\n\tcase BJ_Shadow:\n\t\tlist = &transaction->t_shadow_list;\n\t\tbreak;\n\tcase BJ_LogCtl:\n\t\tlist = &transaction->t_log_list;\n\t\tbreak;\n\tcase BJ_Reserved:\n\t\tlist = &transaction->t_reserved_list;\n\t\tbreak;\n\t}\n\n\t__blist_del_buffer(list, jh);\n\tjh->b_jlist = BJ_None;\n\tif (test_clear_buffer_jbddirty(bh))\n\t\tmark_buffer_dirty(bh);\t/* Expose it to the VM */\n}\n\n/*\n * Remove buffer from all transactions.\n *\n * Called with bh_state lock and j_list_lock\n *\n * jh and bh may be already freed when this function returns.\n */\nstatic void __jbd2_journal_unfile_buffer(struct journal_head *jh)\n{\n\t__jbd2_journal_temp_unlink_buffer(jh);\n\tjh->b_transaction = NULL;\n\tjbd2_journal_put_journal_head(jh);\n}\n\nvoid jbd2_journal_unfile_buffer(journal_t *journal, struct journal_head *jh)\n{\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\t/* Get reference so that buffer cannot be freed before we unlock it */\n\tget_bh(bh);\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\t__jbd2_journal_unfile_buffer(jh);\n\tspin_unlock(&journal->j_list_lock);\n\tjbd_unlock_bh_state(bh);\n\t__brelse(bh);\n}\n\n/*\n * Called from jbd2_journal_try_to_free_buffers().\n *\n * Called under jbd_lock_bh_state(bh)\n */\nstatic void\n__journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)\n{\n\tstruct journal_head *jh;\n\n\tjh = bh2jh(bh);\n\n\tif (buffer_locked(bh) || buffer_dirty(bh))\n\t\tgoto out;\n\n\tif (jh->b_next_transaction != NULL)\n\t\tgoto out;\n\n\tspin_lock(&journal->j_list_lock);\n\tif (jh->b_cp_transaction != NULL && jh->b_transaction == NULL) {\n\t\t/* written-back checkpointed metadata buffer */\n\t\tif (jh->b_jlist == BJ_None) {\n\t\t\tJBUFFER_TRACE(jh, \"remove from checkpoint list\");\n\t\t\t__jbd2_journal_remove_checkpoint(jh);\n\t\t}\n\t}\n\tspin_unlock(&journal->j_list_lock);\nout:\n\treturn;\n}\n\n/**\n * int jbd2_journal_try_to_free_buffers() - try to free page buffers.\n * @journal: journal for operation\n * @page: to try and free\n * @gfp_mask: we use the mask to detect how hard should we try to release\n * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to\n * release the buffers.\n *\n *\n * For all the buffers on this page,\n * if they are fully written out ordered data, move them onto BUF_CLEAN\n * so try_to_free_buffers() can reap them.\n *\n * This function returns non-zero if we wish try_to_free_buffers()\n * to be called. We do this if the page is releasable by try_to_free_buffers().\n * We also do it if the page has locked or dirty buffers and the caller wants\n * us to perform sync or async writeout.\n *\n * This complicates JBD locking somewhat.  We aren't protected by the\n * BKL here.  We wish to remove the buffer from its committing or\n * running transaction's ->t_datalist via __jbd2_journal_unfile_buffer.\n *\n * This may *change* the value of transaction_t->t_datalist, so anyone\n * who looks at t_datalist needs to lock against this function.\n *\n * Even worse, someone may be doing a jbd2_journal_dirty_data on this\n * buffer.  So we need to lock against that.  jbd2_journal_dirty_data()\n * will come out of the lock with the buffer dirty, which makes it\n * ineligible for release here.\n *\n * Who else is affected by this?  hmm...  Really the only contender\n * is do_get_write_access() - it could be looking at the buffer while\n * journal_try_to_free_buffer() is changing its state.  But that\n * cannot happen because we never reallocate freed data as metadata\n * while the data is part of a transaction.  Yes?\n *\n * Return 0 on failure, 1 on success\n */\nint jbd2_journal_try_to_free_buffers(journal_t *journal,\n\t\t\t\tstruct page *page, gfp_t gfp_mask)\n{\n\tstruct buffer_head *head;\n\tstruct buffer_head *bh;\n\tint ret = 0;\n\n\tJ_ASSERT(PageLocked(page));\n\n\thead = page_buffers(page);\n\tbh = head;\n\tdo {\n\t\tstruct journal_head *jh;\n\n\t\t/*\n\t\t * We take our own ref against the journal_head here to avoid\n\t\t * having to add tons of locking around each instance of\n\t\t * jbd2_journal_put_journal_head().\n\t\t */\n\t\tjh = jbd2_journal_grab_journal_head(bh);\n\t\tif (!jh)\n\t\t\tcontinue;\n\n\t\tjbd_lock_bh_state(bh);\n\t\t__journal_try_to_free_buffer(journal, bh);\n\t\tjbd2_journal_put_journal_head(jh);\n\t\tjbd_unlock_bh_state(bh);\n\t\tif (buffer_jbd(bh))\n\t\t\tgoto busy;\n\t} while ((bh = bh->b_this_page) != head);\n\n\tret = try_to_free_buffers(page);\n\nbusy:\n\treturn ret;\n}\n\n/*\n * This buffer is no longer needed.  If it is on an older transaction's\n * checkpoint list we need to record it on this transaction's forget list\n * to pin this buffer (and hence its checkpointing transaction) down until\n * this transaction commits.  If the buffer isn't on a checkpoint list, we\n * release it.\n * Returns non-zero if JBD no longer has an interest in the buffer.\n *\n * Called under j_list_lock.\n *\n * Called under jbd_lock_bh_state(bh).\n */\nstatic int __dispose_buffer(struct journal_head *jh, transaction_t *transaction)\n{\n\tint may_free = 1;\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tif (jh->b_cp_transaction) {\n\t\tJBUFFER_TRACE(jh, \"on running+cp transaction\");\n\t\t__jbd2_journal_temp_unlink_buffer(jh);\n\t\t/*\n\t\t * We don't want to write the buffer anymore, clear the\n\t\t * bit so that we don't confuse checks in\n\t\t * __journal_file_buffer\n\t\t */\n\t\tclear_buffer_dirty(bh);\n\t\t__jbd2_journal_file_buffer(jh, transaction, BJ_Forget);\n\t\tmay_free = 0;\n\t} else {\n\t\tJBUFFER_TRACE(jh, \"on running transaction\");\n\t\t__jbd2_journal_unfile_buffer(jh);\n\t}\n\treturn may_free;\n}\n\n/*\n * jbd2_journal_invalidatepage\n *\n * This code is tricky.  It has a number of cases to deal with.\n *\n * There are two invariants which this code relies on:\n *\n * i_size must be updated on disk before we start calling invalidatepage on the\n * data.\n *\n *  This is done in ext3 by defining an ext3_setattr method which\n *  updates i_size before truncate gets going.  By maintaining this\n *  invariant, we can be sure that it is safe to throw away any buffers\n *  attached to the current transaction: once the transaction commits,\n *  we know that the data will not be needed.\n *\n *  Note however that we can *not* throw away data belonging to the\n *  previous, committing transaction!\n *\n * Any disk blocks which *are* part of the previous, committing\n * transaction (and which therefore cannot be discarded immediately) are\n * not going to be reused in the new running transaction\n *\n *  The bitmap committed_data images guarantee this: any block which is\n *  allocated in one transaction and removed in the next will be marked\n *  as in-use in the committed_data bitmap, so cannot be reused until\n *  the next transaction to delete the block commits.  This means that\n *  leaving committing buffers dirty is quite safe: the disk blocks\n *  cannot be reallocated to a different file and so buffer aliasing is\n *  not possible.\n *\n *\n * The above applies mainly to ordered data mode.  In writeback mode we\n * don't make guarantees about the order in which data hits disk --- in\n * particular we don't guarantee that new dirty data is flushed before\n * transaction commit --- so it is always safe just to discard data\n * immediately in that mode.  --sct\n */\n\n/*\n * The journal_unmap_buffer helper function returns zero if the buffer\n * concerned remains pinned as an anonymous buffer belonging to an older\n * transaction.\n *\n * We're outside-transaction here.  Either or both of j_running_transaction\n * and j_committing_transaction may be NULL.\n */\nstatic int journal_unmap_buffer(journal_t *journal, struct buffer_head *bh)\n{\n\ttransaction_t *transaction;\n\tstruct journal_head *jh;\n\tint may_free = 1;\n\tint ret;\n\n\tBUFFER_TRACE(bh, \"entry\");\n\n\t/*\n\t * It is safe to proceed here without the j_list_lock because the\n\t * buffers cannot be stolen by try_to_free_buffers as long as we are\n\t * holding the page lock. --sct\n\t */\n\n\tif (!buffer_jbd(bh))\n\t\tgoto zap_buffer_unlocked;\n\n\t/* OK, we have data buffer in journaled mode */\n\twrite_lock(&journal->j_state_lock);\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\n\tjh = jbd2_journal_grab_journal_head(bh);\n\tif (!jh)\n\t\tgoto zap_buffer_no_jh;\n\n\t/*\n\t * We cannot remove the buffer from checkpoint lists until the\n\t * transaction adding inode to orphan list (let's call it T)\n\t * is committed.  Otherwise if the transaction changing the\n\t * buffer would be cleaned from the journal before T is\n\t * committed, a crash will cause that the correct contents of\n\t * the buffer will be lost.  On the other hand we have to\n\t * clear the buffer dirty bit at latest at the moment when the\n\t * transaction marking the buffer as freed in the filesystem\n\t * structures is committed because from that moment on the\n\t * buffer can be reallocated and used by a different page.\n\t * Since the block hasn't been freed yet but the inode has\n\t * already been added to orphan list, it is safe for us to add\n\t * the buffer to BJ_Forget list of the newest transaction.\n\t */\n\ttransaction = jh->b_transaction;\n\tif (transaction == NULL) {\n\t\t/* First case: not on any transaction.  If it\n\t\t * has no checkpoint link, then we can zap it:\n\t\t * it's a writeback-mode buffer so we don't care\n\t\t * if it hits disk safely. */\n\t\tif (!jh->b_cp_transaction) {\n\t\t\tJBUFFER_TRACE(jh, \"not on any transaction: zap\");\n\t\t\tgoto zap_buffer;\n\t\t}\n\n\t\tif (!buffer_dirty(bh)) {\n\t\t\t/* bdflush has written it.  We can drop it now */\n\t\t\tgoto zap_buffer;\n\t\t}\n\n\t\t/* OK, it must be in the journal but still not\n\t\t * written fully to disk: it's metadata or\n\t\t * journaled data... */\n\n\t\tif (journal->j_running_transaction) {\n\t\t\t/* ... and once the current transaction has\n\t\t\t * committed, the buffer won't be needed any\n\t\t\t * longer. */\n\t\t\tJBUFFER_TRACE(jh, \"checkpointed: add to BJ_Forget\");\n\t\t\tret = __dispose_buffer(jh,\n\t\t\t\t\tjournal->j_running_transaction);\n\t\t\tjbd2_journal_put_journal_head(jh);\n\t\t\tspin_unlock(&journal->j_list_lock);\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t\twrite_unlock(&journal->j_state_lock);\n\t\t\treturn ret;\n\t\t} else {\n\t\t\t/* There is no currently-running transaction. So the\n\t\t\t * orphan record which we wrote for this file must have\n\t\t\t * passed into commit.  We must attach this buffer to\n\t\t\t * the committing transaction, if it exists. */\n\t\t\tif (journal->j_committing_transaction) {\n\t\t\t\tJBUFFER_TRACE(jh, \"give to committing trans\");\n\t\t\t\tret = __dispose_buffer(jh,\n\t\t\t\t\tjournal->j_committing_transaction);\n\t\t\t\tjbd2_journal_put_journal_head(jh);\n\t\t\t\tspin_unlock(&journal->j_list_lock);\n\t\t\t\tjbd_unlock_bh_state(bh);\n\t\t\t\twrite_unlock(&journal->j_state_lock);\n\t\t\t\treturn ret;\n\t\t\t} else {\n\t\t\t\t/* The orphan record's transaction has\n\t\t\t\t * committed.  We can cleanse this buffer */\n\t\t\t\tclear_buffer_jbddirty(bh);\n\t\t\t\tgoto zap_buffer;\n\t\t\t}\n\t\t}\n\t} else if (transaction == journal->j_committing_transaction) {\n\t\tJBUFFER_TRACE(jh, \"on committing transaction\");\n\t\t/*\n\t\t * The buffer is committing, we simply cannot touch\n\t\t * it. So we just set j_next_transaction to the\n\t\t * running transaction (if there is one) and mark\n\t\t * buffer as freed so that commit code knows it should\n\t\t * clear dirty bits when it is done with the buffer.\n\t\t */\n\t\tset_buffer_freed(bh);\n\t\tif (journal->j_running_transaction && buffer_jbddirty(bh))\n\t\t\tjh->b_next_transaction = journal->j_running_transaction;\n\t\tjbd2_journal_put_journal_head(jh);\n\t\tspin_unlock(&journal->j_list_lock);\n\t\tjbd_unlock_bh_state(bh);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\treturn 0;\n\t} else {\n\t\t/* Good, the buffer belongs to the running transaction.\n\t\t * We are writing our own transaction's data, not any\n\t\t * previous one's, so it is safe to throw it away\n\t\t * (remember that we expect the filesystem to have set\n\t\t * i_size already for this truncate so recovery will not\n\t\t * expose the disk blocks we are discarding here.) */\n\t\tJ_ASSERT_JH(jh, transaction == journal->j_running_transaction);\n\t\tJBUFFER_TRACE(jh, \"on running transaction\");\n\t\tmay_free = __dispose_buffer(jh, transaction);\n\t}\n\nzap_buffer:\n\tjbd2_journal_put_journal_head(jh);\nzap_buffer_no_jh:\n\tspin_unlock(&journal->j_list_lock);\n\tjbd_unlock_bh_state(bh);\n\twrite_unlock(&journal->j_state_lock);\nzap_buffer_unlocked:\n\tclear_buffer_dirty(bh);\n\tJ_ASSERT_BH(bh, !buffer_jbddirty(bh));\n\tclear_buffer_mapped(bh);\n\tclear_buffer_req(bh);\n\tclear_buffer_new(bh);\n\tbh->b_bdev = NULL;\n\treturn may_free;\n}\n\n/**\n * void jbd2_journal_invalidatepage()\n * @journal: journal to use for flush...\n * @page:    page to flush\n * @offset:  length of page to invalidate.\n *\n * Reap page buffers containing data after offset in page.\n *\n */\nvoid jbd2_journal_invalidatepage(journal_t *journal,\n\t\t      struct page *page,\n\t\t      unsigned long offset)\n{\n\tstruct buffer_head *head, *bh, *next;\n\tunsigned int curr_off = 0;\n\tint may_free = 1;\n\n\tif (!PageLocked(page))\n\t\tBUG();\n\tif (!page_has_buffers(page))\n\t\treturn;\n\n\t/* We will potentially be playing with lists other than just the\n\t * data lists (especially for journaled data mode), so be\n\t * cautious in our locking. */\n\n\thead = bh = page_buffers(page);\n\tdo {\n\t\tunsigned int next_off = curr_off + bh->b_size;\n\t\tnext = bh->b_this_page;\n\n\t\tif (offset <= curr_off) {\n\t\t\t/* This block is wholly outside the truncation point */\n\t\t\tlock_buffer(bh);\n\t\t\tmay_free &= journal_unmap_buffer(journal, bh);\n\t\t\tunlock_buffer(bh);\n\t\t}\n\t\tcurr_off = next_off;\n\t\tbh = next;\n\n\t} while (bh != head);\n\n\tif (!offset) {\n\t\tif (may_free && try_to_free_buffers(page))\n\t\t\tJ_ASSERT(!page_has_buffers(page));\n\t}\n}\n\n/*\n * File a buffer on the given transaction list.\n */\nvoid __jbd2_journal_file_buffer(struct journal_head *jh,\n\t\t\ttransaction_t *transaction, int jlist)\n{\n\tstruct journal_head **list = NULL;\n\tint was_dirty = 0;\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tJ_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));\n\tassert_spin_locked(&transaction->t_journal->j_list_lock);\n\n\tJ_ASSERT_JH(jh, jh->b_jlist < BJ_Types);\n\tJ_ASSERT_JH(jh, jh->b_transaction == transaction ||\n\t\t\t\tjh->b_transaction == NULL);\n\n\tif (jh->b_transaction && jh->b_jlist == jlist)\n\t\treturn;\n\n\tif (jlist == BJ_Metadata || jlist == BJ_Reserved ||\n\t    jlist == BJ_Shadow || jlist == BJ_Forget) {\n\t\t/*\n\t\t * For metadata buffers, we track dirty bit in buffer_jbddirty\n\t\t * instead of buffer_dirty. We should not see a dirty bit set\n\t\t * here because we clear it in do_get_write_access but e.g.\n\t\t * tune2fs can modify the sb and set the dirty bit at any time\n\t\t * so we try to gracefully handle that.\n\t\t */\n\t\tif (buffer_dirty(bh))\n\t\t\twarn_dirty_buffer(bh);\n\t\tif (test_clear_buffer_dirty(bh) ||\n\t\t    test_clear_buffer_jbddirty(bh))\n\t\t\twas_dirty = 1;\n\t}\n\n\tif (jh->b_transaction)\n\t\t__jbd2_journal_temp_unlink_buffer(jh);\n\telse\n\t\tjbd2_journal_grab_journal_head(bh);\n\tjh->b_transaction = transaction;\n\n\tswitch (jlist) {\n\tcase BJ_None:\n\t\tJ_ASSERT_JH(jh, !jh->b_committed_data);\n\t\tJ_ASSERT_JH(jh, !jh->b_frozen_data);\n\t\treturn;\n\tcase BJ_Metadata:\n\t\ttransaction->t_nr_buffers++;\n\t\tlist = &transaction->t_buffers;\n\t\tbreak;\n\tcase BJ_Forget:\n\t\tlist = &transaction->t_forget;\n\t\tbreak;\n\tcase BJ_IO:\n\t\tlist = &transaction->t_iobuf_list;\n\t\tbreak;\n\tcase BJ_Shadow:\n\t\tlist = &transaction->t_shadow_list;\n\t\tbreak;\n\tcase BJ_LogCtl:\n\t\tlist = &transaction->t_log_list;\n\t\tbreak;\n\tcase BJ_Reserved:\n\t\tlist = &transaction->t_reserved_list;\n\t\tbreak;\n\t}\n\n\t__blist_add_buffer(list, jh);\n\tjh->b_jlist = jlist;\n\n\tif (was_dirty)\n\t\tset_buffer_jbddirty(bh);\n}\n\nvoid jbd2_journal_file_buffer(struct journal_head *jh,\n\t\t\t\ttransaction_t *transaction, int jlist)\n{\n\tjbd_lock_bh_state(jh2bh(jh));\n\tspin_lock(&transaction->t_journal->j_list_lock);\n\t__jbd2_journal_file_buffer(jh, transaction, jlist);\n\tspin_unlock(&transaction->t_journal->j_list_lock);\n\tjbd_unlock_bh_state(jh2bh(jh));\n}\n\n/*\n * Remove a buffer from its current buffer list in preparation for\n * dropping it from its current transaction entirely.  If the buffer has\n * already started to be used by a subsequent transaction, refile the\n * buffer on that transaction's metadata list.\n *\n * Called under j_list_lock\n * Called under jbd_lock_bh_state(jh2bh(jh))\n *\n * jh and bh may be already free when this function returns\n */\nvoid __jbd2_journal_refile_buffer(struct journal_head *jh)\n{\n\tint was_dirty, jlist;\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tJ_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));\n\tif (jh->b_transaction)\n\t\tassert_spin_locked(&jh->b_transaction->t_journal->j_list_lock);\n\n\t/* If the buffer is now unused, just drop it. */\n\tif (jh->b_next_transaction == NULL) {\n\t\t__jbd2_journal_unfile_buffer(jh);\n\t\treturn;\n\t}\n\n\t/*\n\t * It has been modified by a later transaction: add it to the new\n\t * transaction's metadata list.\n\t */\n\n\twas_dirty = test_clear_buffer_jbddirty(bh);\n\t__jbd2_journal_temp_unlink_buffer(jh);\n\t/*\n\t * We set b_transaction here because b_next_transaction will inherit\n\t * our jh reference and thus __jbd2_journal_file_buffer() must not\n\t * take a new one.\n\t */\n\tjh->b_transaction = jh->b_next_transaction;\n\tjh->b_next_transaction = NULL;\n\tif (buffer_freed(bh))\n\t\tjlist = BJ_Forget;\n\telse if (jh->b_modified)\n\t\tjlist = BJ_Metadata;\n\telse\n\t\tjlist = BJ_Reserved;\n\t__jbd2_journal_file_buffer(jh, jh->b_transaction, jlist);\n\tJ_ASSERT_JH(jh, jh->b_transaction->t_state == T_RUNNING);\n\n\tif (was_dirty)\n\t\tset_buffer_jbddirty(bh);\n}\n\n/*\n * __jbd2_journal_refile_buffer() with necessary locking added. We take our\n * bh reference so that we can safely unlock bh.\n *\n * The jh and bh may be freed by this call.\n */\nvoid jbd2_journal_refile_buffer(journal_t *journal, struct journal_head *jh)\n{\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\t/* Get reference so that buffer cannot be freed before we unlock it */\n\tget_bh(bh);\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\t__jbd2_journal_refile_buffer(jh);\n\tjbd_unlock_bh_state(bh);\n\tspin_unlock(&journal->j_list_lock);\n\t__brelse(bh);\n}\n\n/*\n * File inode in the inode list of the handle's transaction\n */\nint jbd2_journal_file_inode(handle_t *handle, struct jbd2_inode *jinode)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\n\tif (is_handle_aborted(handle))\n\t\treturn -EIO;\n\n\tjbd_debug(4, \"Adding inode %lu, tid:%d\\n\", jinode->i_vfs_inode->i_ino,\n\t\t\ttransaction->t_tid);\n\n\t/*\n\t * First check whether inode isn't already on the transaction's\n\t * lists without taking the lock. Note that this check is safe\n\t * without the lock as we cannot race with somebody removing inode\n\t * from the transaction. The reason is that we remove inode from the\n\t * transaction only in journal_release_jbd_inode() and when we commit\n\t * the transaction. We are guarded from the first case by holding\n\t * a reference to the inode. We are safe against the second case\n\t * because if jinode->i_transaction == transaction, commit code\n\t * cannot touch the transaction because we hold reference to it,\n\t * and if jinode->i_next_transaction == transaction, commit code\n\t * will only file the inode where we want it.\n\t */\n\tif (jinode->i_transaction == transaction ||\n\t    jinode->i_next_transaction == transaction)\n\t\treturn 0;\n\n\tspin_lock(&journal->j_list_lock);\n\n\tif (jinode->i_transaction == transaction ||\n\t    jinode->i_next_transaction == transaction)\n\t\tgoto done;\n\n\t/*\n\t * We only ever set this variable to 1 so the test is safe. Since\n\t * t_need_data_flush is likely to be set, we do the test to save some\n\t * cacheline bouncing\n\t */\n\tif (!transaction->t_need_data_flush)\n\t\ttransaction->t_need_data_flush = 1;\n\t/* On some different transaction's list - should be\n\t * the committing one */\n\tif (jinode->i_transaction) {\n\t\tJ_ASSERT(jinode->i_next_transaction == NULL);\n\t\tJ_ASSERT(jinode->i_transaction ==\n\t\t\t\t\tjournal->j_committing_transaction);\n\t\tjinode->i_next_transaction = transaction;\n\t\tgoto done;\n\t}\n\t/* Not on any transaction list... */\n\tJ_ASSERT(!jinode->i_next_transaction);\n\tjinode->i_transaction = transaction;\n\tlist_add(&jinode->i_list, &transaction->t_inode_list);\ndone:\n\tspin_unlock(&journal->j_list_lock);\n\n\treturn 0;\n}\n\n/*\n * File truncate and transaction commit interact with each other in a\n * non-trivial way.  If a transaction writing data block A is\n * committing, we cannot discard the data by truncate until we have\n * written them.  Otherwise if we crashed after the transaction with\n * write has committed but before the transaction with truncate has\n * committed, we could see stale data in block A.  This function is a\n * helper to solve this problem.  It starts writeout of the truncated\n * part in case it is in the committing transaction.\n *\n * Filesystem code must call this function when inode is journaled in\n * ordered mode before truncation happens and after the inode has been\n * placed on orphan list with the new inode size. The second condition\n * avoids the race that someone writes new data and we start\n * committing the transaction after this function has been called but\n * before a transaction for truncate is started (and furthermore it\n * allows us to optimize the case where the addition to orphan list\n * happens in the same transaction as write --- we don't have to write\n * any data in such case).\n */\nint jbd2_journal_begin_ordered_truncate(journal_t *journal,\n\t\t\t\t\tstruct jbd2_inode *jinode,\n\t\t\t\t\tloff_t new_size)\n{\n\ttransaction_t *inode_trans, *commit_trans;\n\tint ret = 0;\n\n\t/* This is a quick check to avoid locking if not necessary */\n\tif (!jinode->i_transaction)\n\t\tgoto out;\n\t/* Locks are here just to force reading of recent values, it is\n\t * enough that the transaction was not committing before we started\n\t * a transaction adding the inode to orphan list */\n\tread_lock(&journal->j_state_lock);\n\tcommit_trans = journal->j_committing_transaction;\n\tread_unlock(&journal->j_state_lock);\n\tspin_lock(&journal->j_list_lock);\n\tinode_trans = jinode->i_transaction;\n\tspin_unlock(&journal->j_list_lock);\n\tif (inode_trans == commit_trans) {\n\t\tret = filemap_fdatawrite_range(jinode->i_vfs_inode->i_mapping,\n\t\t\tnew_size, LLONG_MAX);\n\t\tif (ret)\n\t\t\tjbd2_journal_abort(journal, ret);\n\t}\nout:\n\treturn ret;\n}\n"], "fixing_code": ["/*\n * linux/fs/jbd2/transaction.c\n *\n * Written by Stephen C. Tweedie <sct@redhat.com>, 1998\n *\n * Copyright 1998 Red Hat corp --- All Rights Reserved\n *\n * This file is part of the Linux kernel and is made available under\n * the terms of the GNU General Public License, version 2, or at your\n * option, any later version, incorporated herein by reference.\n *\n * Generic filesystem transaction handling code; part of the ext2fs\n * journaling system.\n *\n * This file manages transactions (compound commits managed by the\n * journaling code) and handles (individual atomic operations by the\n * filesystem).\n */\n\n#include <linux/time.h>\n#include <linux/fs.h>\n#include <linux/jbd2.h>\n#include <linux/errno.h>\n#include <linux/slab.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/hrtimer.h>\n#include <linux/backing-dev.h>\n#include <linux/bug.h>\n#include <linux/module.h>\n\nstatic void __jbd2_journal_temp_unlink_buffer(struct journal_head *jh);\nstatic void __jbd2_journal_unfile_buffer(struct journal_head *jh);\n\n/*\n * jbd2_get_transaction: obtain a new transaction_t object.\n *\n * Simply allocate and initialise a new transaction.  Create it in\n * RUNNING state and add it to the current journal (which should not\n * have an existing running transaction: we only make a new transaction\n * once we have started to commit the old one).\n *\n * Preconditions:\n *\tThe journal MUST be locked.  We don't perform atomic mallocs on the\n *\tnew transaction\tand we can't block without protecting against other\n *\tprocesses trying to touch the journal while it is in transition.\n *\n */\n\nstatic transaction_t *\njbd2_get_transaction(journal_t *journal, transaction_t *transaction)\n{\n\ttransaction->t_journal = journal;\n\ttransaction->t_state = T_RUNNING;\n\ttransaction->t_start_time = ktime_get();\n\ttransaction->t_tid = journal->j_transaction_sequence++;\n\ttransaction->t_expires = jiffies + journal->j_commit_interval;\n\tspin_lock_init(&transaction->t_handle_lock);\n\tatomic_set(&transaction->t_updates, 0);\n\tatomic_set(&transaction->t_outstanding_credits, 0);\n\tatomic_set(&transaction->t_handle_count, 0);\n\tINIT_LIST_HEAD(&transaction->t_inode_list);\n\tINIT_LIST_HEAD(&transaction->t_private_list);\n\n\t/* Set up the commit timer for the new transaction. */\n\tjournal->j_commit_timer.expires = round_jiffies_up(transaction->t_expires);\n\tadd_timer(&journal->j_commit_timer);\n\n\tJ_ASSERT(journal->j_running_transaction == NULL);\n\tjournal->j_running_transaction = transaction;\n\ttransaction->t_max_wait = 0;\n\ttransaction->t_start = jiffies;\n\n\treturn transaction;\n}\n\n/*\n * Handle management.\n *\n * A handle_t is an object which represents a single atomic update to a\n * filesystem, and which tracks all of the modifications which form part\n * of that one update.\n */\n\n/*\n * Update transaction's maximum wait time, if debugging is enabled.\n *\n * In order for t_max_wait to be reliable, it must be protected by a\n * lock.  But doing so will mean that start_this_handle() can not be\n * run in parallel on SMP systems, which limits our scalability.  So\n * unless debugging is enabled, we no longer update t_max_wait, which\n * means that maximum wait time reported by the jbd2_run_stats\n * tracepoint will always be zero.\n */\nstatic inline void update_t_max_wait(transaction_t *transaction,\n\t\t\t\t     unsigned long ts)\n{\n#ifdef CONFIG_JBD2_DEBUG\n\tif (jbd2_journal_enable_debug &&\n\t    time_after(transaction->t_start, ts)) {\n\t\tts = jbd2_time_diff(ts, transaction->t_start);\n\t\tspin_lock(&transaction->t_handle_lock);\n\t\tif (ts > transaction->t_max_wait)\n\t\t\ttransaction->t_max_wait = ts;\n\t\tspin_unlock(&transaction->t_handle_lock);\n\t}\n#endif\n}\n\n/*\n * start_this_handle: Given a handle, deal with any locking or stalling\n * needed to make sure that there is enough journal space for the handle\n * to begin.  Attach the handle to a transaction and set up the\n * transaction's buffer credits.\n */\n\nstatic int start_this_handle(journal_t *journal, handle_t *handle,\n\t\t\t     gfp_t gfp_mask)\n{\n\ttransaction_t\t*transaction, *new_transaction = NULL;\n\ttid_t\t\ttid;\n\tint\t\tneeded, need_to_start;\n\tint\t\tnblocks = handle->h_buffer_credits;\n\tunsigned long ts = jiffies;\n\n\tif (nblocks > journal->j_max_transaction_buffers) {\n\t\tprintk(KERN_ERR \"JBD2: %s wants too many credits (%d > %d)\\n\",\n\t\t       current->comm, nblocks,\n\t\t       journal->j_max_transaction_buffers);\n\t\treturn -ENOSPC;\n\t}\n\nalloc_transaction:\n\tif (!journal->j_running_transaction) {\n\t\tnew_transaction = kzalloc(sizeof(*new_transaction), gfp_mask);\n\t\tif (!new_transaction) {\n\t\t\t/*\n\t\t\t * If __GFP_FS is not present, then we may be\n\t\t\t * being called from inside the fs writeback\n\t\t\t * layer, so we MUST NOT fail.  Since\n\t\t\t * __GFP_NOFAIL is going away, we will arrange\n\t\t\t * to retry the allocation ourselves.\n\t\t\t */\n\t\t\tif ((gfp_mask & __GFP_FS) == 0) {\n\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\t\tgoto alloc_transaction;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tjbd_debug(3, \"New handle %p going live.\\n\", handle);\n\n\t/*\n\t * We need to hold j_state_lock until t_updates has been incremented,\n\t * for proper journal barrier handling\n\t */\nrepeat:\n\tread_lock(&journal->j_state_lock);\n\tBUG_ON(journal->j_flags & JBD2_UNMOUNT);\n\tif (is_journal_aborted(journal) ||\n\t    (journal->j_errno != 0 && !(journal->j_flags & JBD2_ACK_ERR))) {\n\t\tread_unlock(&journal->j_state_lock);\n\t\tkfree(new_transaction);\n\t\treturn -EROFS;\n\t}\n\n\t/* Wait on the journal's transaction barrier if necessary */\n\tif (journal->j_barrier_count) {\n\t\tread_unlock(&journal->j_state_lock);\n\t\twait_event(journal->j_wait_transaction_locked,\n\t\t\t\tjournal->j_barrier_count == 0);\n\t\tgoto repeat;\n\t}\n\n\tif (!journal->j_running_transaction) {\n\t\tread_unlock(&journal->j_state_lock);\n\t\tif (!new_transaction)\n\t\t\tgoto alloc_transaction;\n\t\twrite_lock(&journal->j_state_lock);\n\t\tif (!journal->j_running_transaction) {\n\t\t\tjbd2_get_transaction(journal, new_transaction);\n\t\t\tnew_transaction = NULL;\n\t\t}\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tgoto repeat;\n\t}\n\n\ttransaction = journal->j_running_transaction;\n\n\t/*\n\t * If the current transaction is locked down for commit, wait for the\n\t * lock to be released.\n\t */\n\tif (transaction->t_state == T_LOCKED) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tprepare_to_wait(&journal->j_wait_transaction_locked,\n\t\t\t\t\t&wait, TASK_UNINTERRUPTIBLE);\n\t\tread_unlock(&journal->j_state_lock);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_transaction_locked, &wait);\n\t\tgoto repeat;\n\t}\n\n\t/*\n\t * If there is not enough space left in the log to write all potential\n\t * buffers requested by this operation, we need to stall pending a log\n\t * checkpoint to free some more log space.\n\t */\n\tneeded = atomic_add_return(nblocks,\n\t\t\t\t   &transaction->t_outstanding_credits);\n\n\tif (needed > journal->j_max_transaction_buffers) {\n\t\t/*\n\t\t * If the current transaction is already too large, then start\n\t\t * to commit it: we can then go back and attach this handle to\n\t\t * a new transaction.\n\t\t */\n\t\tDEFINE_WAIT(wait);\n\n\t\tjbd_debug(2, \"Handle %p starting new commit...\\n\", handle);\n\t\tatomic_sub(nblocks, &transaction->t_outstanding_credits);\n\t\tprepare_to_wait(&journal->j_wait_transaction_locked, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\ttid = transaction->t_tid;\n\t\tneed_to_start = !tid_geq(journal->j_commit_request, tid);\n\t\tread_unlock(&journal->j_state_lock);\n\t\tif (need_to_start)\n\t\t\tjbd2_log_start_commit(journal, tid);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_transaction_locked, &wait);\n\t\tgoto repeat;\n\t}\n\n\t/*\n\t * The commit code assumes that it can get enough log space\n\t * without forcing a checkpoint.  This is *critical* for\n\t * correctness: a checkpoint of a buffer which is also\n\t * associated with a committing transaction creates a deadlock,\n\t * so commit simply cannot force through checkpoints.\n\t *\n\t * We must therefore ensure the necessary space in the journal\n\t * *before* starting to dirty potentially checkpointed buffers\n\t * in the new transaction.\n\t *\n\t * The worst part is, any transaction currently committing can\n\t * reduce the free space arbitrarily.  Be careful to account for\n\t * those buffers when checkpointing.\n\t */\n\n\t/*\n\t * @@@ AKPM: This seems rather over-defensive.  We're giving commit\n\t * a _lot_ of headroom: 1/4 of the journal plus the size of\n\t * the committing transaction.  Really, we only need to give it\n\t * committing_transaction->t_outstanding_credits plus \"enough\" for\n\t * the log control blocks.\n\t * Also, this test is inconsistent with the matching one in\n\t * jbd2_journal_extend().\n\t */\n\tif (__jbd2_log_space_left(journal) < jbd_space_needed(journal)) {\n\t\tjbd_debug(2, \"Handle %p waiting for checkpoint...\\n\", handle);\n\t\tatomic_sub(nblocks, &transaction->t_outstanding_credits);\n\t\tread_unlock(&journal->j_state_lock);\n\t\twrite_lock(&journal->j_state_lock);\n\t\tif (__jbd2_log_space_left(journal) < jbd_space_needed(journal))\n\t\t\t__jbd2_log_wait_for_space(journal);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tgoto repeat;\n\t}\n\n\t/* OK, account for the buffers that this operation expects to\n\t * use and add the handle to the running transaction. \n\t */\n\tupdate_t_max_wait(transaction, ts);\n\thandle->h_transaction = transaction;\n\tatomic_inc(&transaction->t_updates);\n\tatomic_inc(&transaction->t_handle_count);\n\tjbd_debug(4, \"Handle %p given %d credits (total %d, free %d)\\n\",\n\t\t  handle, nblocks,\n\t\t  atomic_read(&transaction->t_outstanding_credits),\n\t\t  __jbd2_log_space_left(journal));\n\tread_unlock(&journal->j_state_lock);\n\n\tlock_map_acquire(&handle->h_lockdep_map);\n\tkfree(new_transaction);\n\treturn 0;\n}\n\nstatic struct lock_class_key jbd2_handle_key;\n\n/* Allocate a new handle.  This should probably be in a slab... */\nstatic handle_t *new_handle(int nblocks)\n{\n\thandle_t *handle = jbd2_alloc_handle(GFP_NOFS);\n\tif (!handle)\n\t\treturn NULL;\n\tmemset(handle, 0, sizeof(*handle));\n\thandle->h_buffer_credits = nblocks;\n\thandle->h_ref = 1;\n\n\tlockdep_init_map(&handle->h_lockdep_map, \"jbd2_handle\",\n\t\t\t\t\t\t&jbd2_handle_key, 0);\n\n\treturn handle;\n}\n\n/**\n * handle_t *jbd2_journal_start() - Obtain a new handle.\n * @journal: Journal to start transaction on.\n * @nblocks: number of block buffer we might modify\n *\n * We make sure that the transaction can guarantee at least nblocks of\n * modified buffers in the log.  We block until the log can guarantee\n * that much space.\n *\n * This function is visible to journal users (like ext3fs), so is not\n * called with the journal already locked.\n *\n * Return a pointer to a newly allocated handle, or an ERR_PTR() value\n * on failure.\n */\nhandle_t *jbd2__journal_start(journal_t *journal, int nblocks, gfp_t gfp_mask)\n{\n\thandle_t *handle = journal_current_handle();\n\tint err;\n\n\tif (!journal)\n\t\treturn ERR_PTR(-EROFS);\n\n\tif (handle) {\n\t\tJ_ASSERT(handle->h_transaction->t_journal == journal);\n\t\thandle->h_ref++;\n\t\treturn handle;\n\t}\n\n\thandle = new_handle(nblocks);\n\tif (!handle)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcurrent->journal_info = handle;\n\n\terr = start_this_handle(journal, handle, gfp_mask);\n\tif (err < 0) {\n\t\tjbd2_free_handle(handle);\n\t\tcurrent->journal_info = NULL;\n\t\thandle = ERR_PTR(err);\n\t}\n\treturn handle;\n}\nEXPORT_SYMBOL(jbd2__journal_start);\n\n\nhandle_t *jbd2_journal_start(journal_t *journal, int nblocks)\n{\n\treturn jbd2__journal_start(journal, nblocks, GFP_NOFS);\n}\nEXPORT_SYMBOL(jbd2_journal_start);\n\n\n/**\n * int jbd2_journal_extend() - extend buffer credits.\n * @handle:  handle to 'extend'\n * @nblocks: nr blocks to try to extend by.\n *\n * Some transactions, such as large extends and truncates, can be done\n * atomically all at once or in several stages.  The operation requests\n * a credit for a number of buffer modications in advance, but can\n * extend its credit if it needs more.\n *\n * jbd2_journal_extend tries to give the running handle more buffer credits.\n * It does not guarantee that allocation - this is a best-effort only.\n * The calling process MUST be able to deal cleanly with a failure to\n * extend here.\n *\n * Return 0 on success, non-zero on failure.\n *\n * return code < 0 implies an error\n * return code > 0 implies normal transaction-full status.\n */\nint jbd2_journal_extend(handle_t *handle, int nblocks)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tint result;\n\tint wanted;\n\n\tresult = -EIO;\n\tif (is_handle_aborted(handle))\n\t\tgoto out;\n\n\tresult = 1;\n\n\tread_lock(&journal->j_state_lock);\n\n\t/* Don't extend a locked-down transaction! */\n\tif (handle->h_transaction->t_state != T_RUNNING) {\n\t\tjbd_debug(3, \"denied handle %p %d blocks: \"\n\t\t\t  \"transaction not running\\n\", handle, nblocks);\n\t\tgoto error_out;\n\t}\n\n\tspin_lock(&transaction->t_handle_lock);\n\twanted = atomic_read(&transaction->t_outstanding_credits) + nblocks;\n\n\tif (wanted > journal->j_max_transaction_buffers) {\n\t\tjbd_debug(3, \"denied handle %p %d blocks: \"\n\t\t\t  \"transaction too large\\n\", handle, nblocks);\n\t\tgoto unlock;\n\t}\n\n\tif (wanted > __jbd2_log_space_left(journal)) {\n\t\tjbd_debug(3, \"denied handle %p %d blocks: \"\n\t\t\t  \"insufficient log space\\n\", handle, nblocks);\n\t\tgoto unlock;\n\t}\n\n\thandle->h_buffer_credits += nblocks;\n\tatomic_add(nblocks, &transaction->t_outstanding_credits);\n\tresult = 0;\n\n\tjbd_debug(3, \"extended handle %p by %d\\n\", handle, nblocks);\nunlock:\n\tspin_unlock(&transaction->t_handle_lock);\nerror_out:\n\tread_unlock(&journal->j_state_lock);\nout:\n\treturn result;\n}\n\n\n/**\n * int jbd2_journal_restart() - restart a handle .\n * @handle:  handle to restart\n * @nblocks: nr credits requested\n *\n * Restart a handle for a multi-transaction filesystem\n * operation.\n *\n * If the jbd2_journal_extend() call above fails to grant new buffer credits\n * to a running handle, a call to jbd2_journal_restart will commit the\n * handle's transaction so far and reattach the handle to a new\n * transaction capabable of guaranteeing the requested number of\n * credits.\n */\nint jbd2__journal_restart(handle_t *handle, int nblocks, gfp_t gfp_mask)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\ttid_t\t\ttid;\n\tint\t\tneed_to_start, ret;\n\n\t/* If we've had an abort of any type, don't even think about\n\t * actually doing the restart! */\n\tif (is_handle_aborted(handle))\n\t\treturn 0;\n\n\t/*\n\t * First unlink the handle from its current transaction, and start the\n\t * commit on that.\n\t */\n\tJ_ASSERT(atomic_read(&transaction->t_updates) > 0);\n\tJ_ASSERT(journal_current_handle() == handle);\n\n\tread_lock(&journal->j_state_lock);\n\tspin_lock(&transaction->t_handle_lock);\n\tatomic_sub(handle->h_buffer_credits,\n\t\t   &transaction->t_outstanding_credits);\n\tif (atomic_dec_and_test(&transaction->t_updates))\n\t\twake_up(&journal->j_wait_updates);\n\tspin_unlock(&transaction->t_handle_lock);\n\n\tjbd_debug(2, \"restarting handle %p\\n\", handle);\n\ttid = transaction->t_tid;\n\tneed_to_start = !tid_geq(journal->j_commit_request, tid);\n\tread_unlock(&journal->j_state_lock);\n\tif (need_to_start)\n\t\tjbd2_log_start_commit(journal, tid);\n\n\tlock_map_release(&handle->h_lockdep_map);\n\thandle->h_buffer_credits = nblocks;\n\tret = start_this_handle(journal, handle, gfp_mask);\n\treturn ret;\n}\nEXPORT_SYMBOL(jbd2__journal_restart);\n\n\nint jbd2_journal_restart(handle_t *handle, int nblocks)\n{\n\treturn jbd2__journal_restart(handle, nblocks, GFP_NOFS);\n}\nEXPORT_SYMBOL(jbd2_journal_restart);\n\n/**\n * void jbd2_journal_lock_updates () - establish a transaction barrier.\n * @journal:  Journal to establish a barrier on.\n *\n * This locks out any further updates from being started, and blocks\n * until all existing updates have completed, returning only once the\n * journal is in a quiescent state with no updates running.\n *\n * The journal lock should not be held on entry.\n */\nvoid jbd2_journal_lock_updates(journal_t *journal)\n{\n\tDEFINE_WAIT(wait);\n\n\twrite_lock(&journal->j_state_lock);\n\t++journal->j_barrier_count;\n\n\t/* Wait until there are no running updates */\n\twhile (1) {\n\t\ttransaction_t *transaction = journal->j_running_transaction;\n\n\t\tif (!transaction)\n\t\t\tbreak;\n\n\t\tspin_lock(&transaction->t_handle_lock);\n\t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (!atomic_read(&transaction->t_updates)) {\n\t\t\tspin_unlock(&transaction->t_handle_lock);\n\t\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&transaction->t_handle_lock);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n\twrite_unlock(&journal->j_state_lock);\n\n\t/*\n\t * We have now established a barrier against other normal updates, but\n\t * we also need to barrier against other jbd2_journal_lock_updates() calls\n\t * to make sure that we serialise special journal-locked operations\n\t * too.\n\t */\n\tmutex_lock(&journal->j_barrier);\n}\n\n/**\n * void jbd2_journal_unlock_updates (journal_t* journal) - release barrier\n * @journal:  Journal to release the barrier on.\n *\n * Release a transaction barrier obtained with jbd2_journal_lock_updates().\n *\n * Should be called without the journal lock held.\n */\nvoid jbd2_journal_unlock_updates (journal_t *journal)\n{\n\tJ_ASSERT(journal->j_barrier_count != 0);\n\n\tmutex_unlock(&journal->j_barrier);\n\twrite_lock(&journal->j_state_lock);\n\t--journal->j_barrier_count;\n\twrite_unlock(&journal->j_state_lock);\n\twake_up(&journal->j_wait_transaction_locked);\n}\n\nstatic void warn_dirty_buffer(struct buffer_head *bh)\n{\n\tchar b[BDEVNAME_SIZE];\n\n\tprintk(KERN_WARNING\n\t       \"JBD2: Spotted dirty metadata buffer (dev = %s, blocknr = %llu). \"\n\t       \"There's a risk of filesystem corruption in case of system \"\n\t       \"crash.\\n\",\n\t       bdevname(bh->b_bdev, b), (unsigned long long)bh->b_blocknr);\n}\n\n/*\n * If the buffer is already part of the current transaction, then there\n * is nothing we need to do.  If it is already part of a prior\n * transaction which we are still committing to disk, then we need to\n * make sure that we do not overwrite the old copy: we do copy-out to\n * preserve the copy going to disk.  We also account the buffer against\n * the handle's metadata buffer credits (unless the buffer is already\n * part of the transaction, that is).\n *\n */\nstatic int\ndo_get_write_access(handle_t *handle, struct journal_head *jh,\n\t\t\tint force_copy)\n{\n\tstruct buffer_head *bh;\n\ttransaction_t *transaction;\n\tjournal_t *journal;\n\tint error;\n\tchar *frozen_buffer = NULL;\n\tint need_copy = 0;\n\n\tif (is_handle_aborted(handle))\n\t\treturn -EROFS;\n\n\ttransaction = handle->h_transaction;\n\tjournal = transaction->t_journal;\n\n\tjbd_debug(5, \"journal_head %p, force_copy %d\\n\", jh, force_copy);\n\n\tJBUFFER_TRACE(jh, \"entry\");\nrepeat:\n\tbh = jh2bh(jh);\n\n\t/* @@@ Need to check for errors here at some point. */\n\n\tlock_buffer(bh);\n\tjbd_lock_bh_state(bh);\n\n\t/* We now hold the buffer lock so it is safe to query the buffer\n\t * state.  Is the buffer dirty?\n\t *\n\t * If so, there are two possibilities.  The buffer may be\n\t * non-journaled, and undergoing a quite legitimate writeback.\n\t * Otherwise, it is journaled, and we don't expect dirty buffers\n\t * in that state (the buffers should be marked JBD_Dirty\n\t * instead.)  So either the IO is being done under our own\n\t * control and this is a bug, or it's a third party IO such as\n\t * dump(8) (which may leave the buffer scheduled for read ---\n\t * ie. locked but not dirty) or tune2fs (which may actually have\n\t * the buffer dirtied, ugh.)  */\n\n\tif (buffer_dirty(bh)) {\n\t\t/*\n\t\t * First question: is this buffer already part of the current\n\t\t * transaction or the existing committing transaction?\n\t\t */\n\t\tif (jh->b_transaction) {\n\t\t\tJ_ASSERT_JH(jh,\n\t\t\t\tjh->b_transaction == transaction ||\n\t\t\t\tjh->b_transaction ==\n\t\t\t\t\tjournal->j_committing_transaction);\n\t\t\tif (jh->b_next_transaction)\n\t\t\t\tJ_ASSERT_JH(jh, jh->b_next_transaction ==\n\t\t\t\t\t\t\ttransaction);\n\t\t\twarn_dirty_buffer(bh);\n\t\t}\n\t\t/*\n\t\t * In any case we need to clean the dirty flag and we must\n\t\t * do it under the buffer lock to be sure we don't race\n\t\t * with running write-out.\n\t\t */\n\t\tJBUFFER_TRACE(jh, \"Journalling dirty buffer\");\n\t\tclear_buffer_dirty(bh);\n\t\tset_buffer_jbddirty(bh);\n\t}\n\n\tunlock_buffer(bh);\n\n\terror = -EROFS;\n\tif (is_handle_aborted(handle)) {\n\t\tjbd_unlock_bh_state(bh);\n\t\tgoto out;\n\t}\n\terror = 0;\n\n\t/*\n\t * The buffer is already part of this transaction if b_transaction or\n\t * b_next_transaction points to it\n\t */\n\tif (jh->b_transaction == transaction ||\n\t    jh->b_next_transaction == transaction)\n\t\tgoto done;\n\n\t/*\n\t * this is the first time this transaction is touching this buffer,\n\t * reset the modified flag\n\t */\n       jh->b_modified = 0;\n\n\t/*\n\t * If there is already a copy-out version of this buffer, then we don't\n\t * need to make another one\n\t */\n\tif (jh->b_frozen_data) {\n\t\tJBUFFER_TRACE(jh, \"has frozen data\");\n\t\tJ_ASSERT_JH(jh, jh->b_next_transaction == NULL);\n\t\tjh->b_next_transaction = transaction;\n\t\tgoto done;\n\t}\n\n\t/* Is there data here we need to preserve? */\n\n\tif (jh->b_transaction && jh->b_transaction != transaction) {\n\t\tJBUFFER_TRACE(jh, \"owned by older transaction\");\n\t\tJ_ASSERT_JH(jh, jh->b_next_transaction == NULL);\n\t\tJ_ASSERT_JH(jh, jh->b_transaction ==\n\t\t\t\t\tjournal->j_committing_transaction);\n\n\t\t/* There is one case we have to be very careful about.\n\t\t * If the committing transaction is currently writing\n\t\t * this buffer out to disk and has NOT made a copy-out,\n\t\t * then we cannot modify the buffer contents at all\n\t\t * right now.  The essence of copy-out is that it is the\n\t\t * extra copy, not the primary copy, which gets\n\t\t * journaled.  If the primary copy is already going to\n\t\t * disk then we cannot do copy-out here. */\n\n\t\tif (jh->b_jlist == BJ_Shadow) {\n\t\t\tDEFINE_WAIT_BIT(wait, &bh->b_state, BH_Unshadow);\n\t\t\twait_queue_head_t *wqh;\n\n\t\t\twqh = bit_waitqueue(&bh->b_state, BH_Unshadow);\n\n\t\t\tJBUFFER_TRACE(jh, \"on shadow: sleep\");\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t\t/* commit wakes up all shadow buffers after IO */\n\t\t\tfor ( ; ; ) {\n\t\t\t\tprepare_to_wait(wqh, &wait.wait,\n\t\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\t\t\tif (jh->b_jlist != BJ_Shadow)\n\t\t\t\t\tbreak;\n\t\t\t\tschedule();\n\t\t\t}\n\t\t\tfinish_wait(wqh, &wait.wait);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\t/* Only do the copy if the currently-owning transaction\n\t\t * still needs it.  If it is on the Forget list, the\n\t\t * committing transaction is past that stage.  The\n\t\t * buffer had better remain locked during the kmalloc,\n\t\t * but that should be true --- we hold the journal lock\n\t\t * still and the buffer is already on the BUF_JOURNAL\n\t\t * list so won't be flushed.\n\t\t *\n\t\t * Subtle point, though: if this is a get_undo_access,\n\t\t * then we will be relying on the frozen_data to contain\n\t\t * the new value of the committed_data record after the\n\t\t * transaction, so we HAVE to force the frozen_data copy\n\t\t * in that case. */\n\n\t\tif (jh->b_jlist != BJ_Forget || force_copy) {\n\t\t\tJBUFFER_TRACE(jh, \"generate frozen data\");\n\t\t\tif (!frozen_buffer) {\n\t\t\t\tJBUFFER_TRACE(jh, \"allocate memory for buffer\");\n\t\t\t\tjbd_unlock_bh_state(bh);\n\t\t\t\tfrozen_buffer =\n\t\t\t\t\tjbd2_alloc(jh2bh(jh)->b_size,\n\t\t\t\t\t\t\t GFP_NOFS);\n\t\t\t\tif (!frozen_buffer) {\n\t\t\t\t\tprintk(KERN_EMERG\n\t\t\t\t\t       \"%s: OOM for frozen_buffer\\n\",\n\t\t\t\t\t       __func__);\n\t\t\t\t\tJBUFFER_TRACE(jh, \"oom!\");\n\t\t\t\t\terror = -ENOMEM;\n\t\t\t\t\tjbd_lock_bh_state(bh);\n\t\t\t\t\tgoto done;\n\t\t\t\t}\n\t\t\t\tgoto repeat;\n\t\t\t}\n\t\t\tjh->b_frozen_data = frozen_buffer;\n\t\t\tfrozen_buffer = NULL;\n\t\t\tneed_copy = 1;\n\t\t}\n\t\tjh->b_next_transaction = transaction;\n\t}\n\n\n\t/*\n\t * Finally, if the buffer is not journaled right now, we need to make\n\t * sure it doesn't get written to disk before the caller actually\n\t * commits the new data\n\t */\n\tif (!jh->b_transaction) {\n\t\tJBUFFER_TRACE(jh, \"no transaction\");\n\t\tJ_ASSERT_JH(jh, !jh->b_next_transaction);\n\t\tJBUFFER_TRACE(jh, \"file as BJ_Reserved\");\n\t\tspin_lock(&journal->j_list_lock);\n\t\t__jbd2_journal_file_buffer(jh, transaction, BJ_Reserved);\n\t\tspin_unlock(&journal->j_list_lock);\n\t}\n\ndone:\n\tif (need_copy) {\n\t\tstruct page *page;\n\t\tint offset;\n\t\tchar *source;\n\n\t\tJ_EXPECT_JH(jh, buffer_uptodate(jh2bh(jh)),\n\t\t\t    \"Possible IO failure.\\n\");\n\t\tpage = jh2bh(jh)->b_page;\n\t\toffset = offset_in_page(jh2bh(jh)->b_data);\n\t\tsource = kmap_atomic(page, KM_USER0);\n\t\t/* Fire data frozen trigger just before we copy the data */\n\t\tjbd2_buffer_frozen_trigger(jh, source + offset,\n\t\t\t\t\t   jh->b_triggers);\n\t\tmemcpy(jh->b_frozen_data, source+offset, jh2bh(jh)->b_size);\n\t\tkunmap_atomic(source, KM_USER0);\n\n\t\t/*\n\t\t * Now that the frozen data is saved off, we need to store\n\t\t * any matching triggers.\n\t\t */\n\t\tjh->b_frozen_triggers = jh->b_triggers;\n\t}\n\tjbd_unlock_bh_state(bh);\n\n\t/*\n\t * If we are about to journal a buffer, then any revoke pending on it is\n\t * no longer valid\n\t */\n\tjbd2_journal_cancel_revoke(handle, jh);\n\nout:\n\tif (unlikely(frozen_buffer))\t/* It's usually NULL */\n\t\tjbd2_free(frozen_buffer, bh->b_size);\n\n\tJBUFFER_TRACE(jh, \"exit\");\n\treturn error;\n}\n\n/**\n * int jbd2_journal_get_write_access() - notify intent to modify a buffer for metadata (not data) update.\n * @handle: transaction to add buffer modifications to\n * @bh:     bh to be used for metadata writes\n *\n * Returns an error code or 0 on success.\n *\n * In full data journalling mode the buffer may be of type BJ_AsyncData,\n * because we're write()ing a buffer which is also part of a shared mapping.\n */\n\nint jbd2_journal_get_write_access(handle_t *handle, struct buffer_head *bh)\n{\n\tstruct journal_head *jh = jbd2_journal_add_journal_head(bh);\n\tint rc;\n\n\t/* We do not want to get caught playing with fields which the\n\t * log thread also manipulates.  Make sure that the buffer\n\t * completes any outstanding IO before proceeding. */\n\trc = do_get_write_access(handle, jh, 0);\n\tjbd2_journal_put_journal_head(jh);\n\treturn rc;\n}\n\n\n/*\n * When the user wants to journal a newly created buffer_head\n * (ie. getblk() returned a new buffer and we are going to populate it\n * manually rather than reading off disk), then we need to keep the\n * buffer_head locked until it has been completely filled with new\n * data.  In this case, we should be able to make the assertion that\n * the bh is not already part of an existing transaction.\n *\n * The buffer should already be locked by the caller by this point.\n * There is no lock ranking violation: it was a newly created,\n * unlocked buffer beforehand. */\n\n/**\n * int jbd2_journal_get_create_access () - notify intent to use newly created bh\n * @handle: transaction to new buffer to\n * @bh: new buffer.\n *\n * Call this if you create a new bh.\n */\nint jbd2_journal_get_create_access(handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tstruct journal_head *jh = jbd2_journal_add_journal_head(bh);\n\tint err;\n\n\tjbd_debug(5, \"journal_head %p\\n\", jh);\n\terr = -EROFS;\n\tif (is_handle_aborted(handle))\n\t\tgoto out;\n\terr = 0;\n\n\tJBUFFER_TRACE(jh, \"entry\");\n\t/*\n\t * The buffer may already belong to this transaction due to pre-zeroing\n\t * in the filesystem's new_block code.  It may also be on the previous,\n\t * committing transaction's lists, but it HAS to be in Forget state in\n\t * that case: the transaction must have deleted the buffer for it to be\n\t * reused here.\n\t */\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\tJ_ASSERT_JH(jh, (jh->b_transaction == transaction ||\n\t\tjh->b_transaction == NULL ||\n\t\t(jh->b_transaction == journal->j_committing_transaction &&\n\t\t\t  jh->b_jlist == BJ_Forget)));\n\n\tJ_ASSERT_JH(jh, jh->b_next_transaction == NULL);\n\tJ_ASSERT_JH(jh, buffer_locked(jh2bh(jh)));\n\n\tif (jh->b_transaction == NULL) {\n\t\t/*\n\t\t * Previous jbd2_journal_forget() could have left the buffer\n\t\t * with jbddirty bit set because it was being committed. When\n\t\t * the commit finished, we've filed the buffer for\n\t\t * checkpointing and marked it dirty. Now we are reallocating\n\t\t * the buffer so the transaction freeing it must have\n\t\t * committed and so it's safe to clear the dirty bit.\n\t\t */\n\t\tclear_buffer_dirty(jh2bh(jh));\n\t\t/* first access by this transaction */\n\t\tjh->b_modified = 0;\n\n\t\tJBUFFER_TRACE(jh, \"file as BJ_Reserved\");\n\t\t__jbd2_journal_file_buffer(jh, transaction, BJ_Reserved);\n\t} else if (jh->b_transaction == journal->j_committing_transaction) {\n\t\t/* first access by this transaction */\n\t\tjh->b_modified = 0;\n\n\t\tJBUFFER_TRACE(jh, \"set next transaction\");\n\t\tjh->b_next_transaction = transaction;\n\t}\n\tspin_unlock(&journal->j_list_lock);\n\tjbd_unlock_bh_state(bh);\n\n\t/*\n\t * akpm: I added this.  ext3_alloc_branch can pick up new indirect\n\t * blocks which contain freed but then revoked metadata.  We need\n\t * to cancel the revoke in case we end up freeing it yet again\n\t * and the reallocating as data - this would cause a second revoke,\n\t * which hits an assertion error.\n\t */\n\tJBUFFER_TRACE(jh, \"cancelling revoke\");\n\tjbd2_journal_cancel_revoke(handle, jh);\nout:\n\tjbd2_journal_put_journal_head(jh);\n\treturn err;\n}\n\n/**\n * int jbd2_journal_get_undo_access() -  Notify intent to modify metadata with\n *     non-rewindable consequences\n * @handle: transaction\n * @bh: buffer to undo\n *\n * Sometimes there is a need to distinguish between metadata which has\n * been committed to disk and that which has not.  The ext3fs code uses\n * this for freeing and allocating space, we have to make sure that we\n * do not reuse freed space until the deallocation has been committed,\n * since if we overwrote that space we would make the delete\n * un-rewindable in case of a crash.\n *\n * To deal with that, jbd2_journal_get_undo_access requests write access to a\n * buffer for parts of non-rewindable operations such as delete\n * operations on the bitmaps.  The journaling code must keep a copy of\n * the buffer's contents prior to the undo_access call until such time\n * as we know that the buffer has definitely been committed to disk.\n *\n * We never need to know which transaction the committed data is part\n * of, buffers touched here are guaranteed to be dirtied later and so\n * will be committed to a new transaction in due course, at which point\n * we can discard the old committed data pointer.\n *\n * Returns error number or 0 on success.\n */\nint jbd2_journal_get_undo_access(handle_t *handle, struct buffer_head *bh)\n{\n\tint err;\n\tstruct journal_head *jh = jbd2_journal_add_journal_head(bh);\n\tchar *committed_data = NULL;\n\n\tJBUFFER_TRACE(jh, \"entry\");\n\n\t/*\n\t * Do this first --- it can drop the journal lock, so we want to\n\t * make sure that obtaining the committed_data is done\n\t * atomically wrt. completion of any outstanding commits.\n\t */\n\terr = do_get_write_access(handle, jh, 1);\n\tif (err)\n\t\tgoto out;\n\nrepeat:\n\tif (!jh->b_committed_data) {\n\t\tcommitted_data = jbd2_alloc(jh2bh(jh)->b_size, GFP_NOFS);\n\t\tif (!committed_data) {\n\t\t\tprintk(KERN_EMERG \"%s: No memory for committed data\\n\",\n\t\t\t\t__func__);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tjbd_lock_bh_state(bh);\n\tif (!jh->b_committed_data) {\n\t\t/* Copy out the current buffer contents into the\n\t\t * preserved, committed copy. */\n\t\tJBUFFER_TRACE(jh, \"generate b_committed data\");\n\t\tif (!committed_data) {\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tjh->b_committed_data = committed_data;\n\t\tcommitted_data = NULL;\n\t\tmemcpy(jh->b_committed_data, bh->b_data, bh->b_size);\n\t}\n\tjbd_unlock_bh_state(bh);\nout:\n\tjbd2_journal_put_journal_head(jh);\n\tif (unlikely(committed_data))\n\t\tjbd2_free(committed_data, bh->b_size);\n\treturn err;\n}\n\n/**\n * void jbd2_journal_set_triggers() - Add triggers for commit writeout\n * @bh: buffer to trigger on\n * @type: struct jbd2_buffer_trigger_type containing the trigger(s).\n *\n * Set any triggers on this journal_head.  This is always safe, because\n * triggers for a committing buffer will be saved off, and triggers for\n * a running transaction will match the buffer in that transaction.\n *\n * Call with NULL to clear the triggers.\n */\nvoid jbd2_journal_set_triggers(struct buffer_head *bh,\n\t\t\t       struct jbd2_buffer_trigger_type *type)\n{\n\tstruct journal_head *jh = bh2jh(bh);\n\n\tjh->b_triggers = type;\n}\n\nvoid jbd2_buffer_frozen_trigger(struct journal_head *jh, void *mapped_data,\n\t\t\t\tstruct jbd2_buffer_trigger_type *triggers)\n{\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tif (!triggers || !triggers->t_frozen)\n\t\treturn;\n\n\ttriggers->t_frozen(triggers, bh, mapped_data, bh->b_size);\n}\n\nvoid jbd2_buffer_abort_trigger(struct journal_head *jh,\n\t\t\t       struct jbd2_buffer_trigger_type *triggers)\n{\n\tif (!triggers || !triggers->t_abort)\n\t\treturn;\n\n\ttriggers->t_abort(triggers, jh2bh(jh));\n}\n\n\n\n/**\n * int jbd2_journal_dirty_metadata() -  mark a buffer as containing dirty metadata\n * @handle: transaction to add buffer to.\n * @bh: buffer to mark\n *\n * mark dirty metadata which needs to be journaled as part of the current\n * transaction.\n *\n * The buffer must have previously had jbd2_journal_get_write_access()\n * called so that it has a valid journal_head attached to the buffer\n * head.\n *\n * The buffer is placed on the transaction's metadata list and is marked\n * as belonging to the transaction.\n *\n * Returns error number or 0 on success.\n *\n * Special care needs to be taken if the buffer already belongs to the\n * current committing transaction (in which case we should have frozen\n * data present for that commit).  In that case, we don't relink the\n * buffer: that only gets done when the old transaction finally\n * completes its commit.\n */\nint jbd2_journal_dirty_metadata(handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tstruct journal_head *jh = bh2jh(bh);\n\tint ret = 0;\n\n\tjbd_debug(5, \"journal_head %p\\n\", jh);\n\tJBUFFER_TRACE(jh, \"entry\");\n\tif (is_handle_aborted(handle))\n\t\tgoto out;\n\tif (!buffer_jbd(bh)) {\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tjbd_lock_bh_state(bh);\n\n\tif (jh->b_modified == 0) {\n\t\t/*\n\t\t * This buffer's got modified and becoming part\n\t\t * of the transaction. This needs to be done\n\t\t * once a transaction -bzzz\n\t\t */\n\t\tjh->b_modified = 1;\n\t\tJ_ASSERT_JH(jh, handle->h_buffer_credits > 0);\n\t\thandle->h_buffer_credits--;\n\t}\n\n\t/*\n\t * fastpath, to avoid expensive locking.  If this buffer is already\n\t * on the running transaction's metadata list there is nothing to do.\n\t * Nobody can take it off again because there is a handle open.\n\t * I _think_ we're OK here with SMP barriers - a mistaken decision will\n\t * result in this test being false, so we go in and take the locks.\n\t */\n\tif (jh->b_transaction == transaction && jh->b_jlist == BJ_Metadata) {\n\t\tJBUFFER_TRACE(jh, \"fastpath\");\n\t\tif (unlikely(jh->b_transaction !=\n\t\t\t     journal->j_running_transaction)) {\n\t\t\tprintk(KERN_EMERG \"JBD: %s: \"\n\t\t\t       \"jh->b_transaction (%llu, %p, %u) != \"\n\t\t\t       \"journal->j_running_transaction (%p, %u)\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ? jh->b_transaction->t_tid : 0,\n\t\t\t       journal->j_running_transaction,\n\t\t\t       journal->j_running_transaction ?\n\t\t\t       journal->j_running_transaction->t_tid : 0);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tgoto out_unlock_bh;\n\t}\n\n\tset_buffer_jbddirty(bh);\n\n\t/*\n\t * Metadata already on the current transaction list doesn't\n\t * need to be filed.  Metadata on another transaction's list must\n\t * be committing, and will be refiled once the commit completes:\n\t * leave it alone for now.\n\t */\n\tif (jh->b_transaction != transaction) {\n\t\tJBUFFER_TRACE(jh, \"already on other transaction\");\n\t\tif (unlikely(jh->b_transaction !=\n\t\t\t     journal->j_committing_transaction)) {\n\t\t\tprintk(KERN_EMERG \"JBD: %s: \"\n\t\t\t       \"jh->b_transaction (%llu, %p, %u) != \"\n\t\t\t       \"journal->j_committing_transaction (%p, %u)\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ? jh->b_transaction->t_tid : 0,\n\t\t\t       journal->j_committing_transaction,\n\t\t\t       journal->j_committing_transaction ?\n\t\t\t       journal->j_committing_transaction->t_tid : 0);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tif (unlikely(jh->b_next_transaction != transaction)) {\n\t\t\tprintk(KERN_EMERG \"JBD: %s: \"\n\t\t\t       \"jh->b_next_transaction (%llu, %p, %u) != \"\n\t\t\t       \"transaction (%p, %u)\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_next_transaction,\n\t\t\t       jh->b_next_transaction ?\n\t\t\t       jh->b_next_transaction->t_tid : 0,\n\t\t\t       transaction, transaction->t_tid);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\t/* And this case is illegal: we can't reuse another\n\t\t * transaction's data buffer, ever. */\n\t\tgoto out_unlock_bh;\n\t}\n\n\t/* That test should have eliminated the following case: */\n\tJ_ASSERT_JH(jh, jh->b_frozen_data == NULL);\n\n\tJBUFFER_TRACE(jh, \"file as BJ_Metadata\");\n\tspin_lock(&journal->j_list_lock);\n\t__jbd2_journal_file_buffer(jh, handle->h_transaction, BJ_Metadata);\n\tspin_unlock(&journal->j_list_lock);\nout_unlock_bh:\n\tjbd_unlock_bh_state(bh);\nout:\n\tJBUFFER_TRACE(jh, \"exit\");\n\tWARN_ON(ret);\t/* All errors are bugs, so dump the stack */\n\treturn ret;\n}\n\n/*\n * jbd2_journal_release_buffer: undo a get_write_access without any buffer\n * updates, if the update decided in the end that it didn't need access.\n *\n */\nvoid\njbd2_journal_release_buffer(handle_t *handle, struct buffer_head *bh)\n{\n\tBUFFER_TRACE(bh, \"entry\");\n}\n\n/**\n * void jbd2_journal_forget() - bforget() for potentially-journaled buffers.\n * @handle: transaction handle\n * @bh:     bh to 'forget'\n *\n * We can only do the bforget if there are no commits pending against the\n * buffer.  If the buffer is dirty in the current running transaction we\n * can safely unlink it.\n *\n * bh may not be a journalled buffer at all - it may be a non-JBD\n * buffer which came off the hashtable.  Check for this.\n *\n * Decrements bh->b_count by one.\n *\n * Allow this call even if the handle has aborted --- it may be part of\n * the caller's cleanup after an abort.\n */\nint jbd2_journal_forget (handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tstruct journal_head *jh;\n\tint drop_reserve = 0;\n\tint err = 0;\n\tint was_modified = 0;\n\n\tBUFFER_TRACE(bh, \"entry\");\n\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\n\tif (!buffer_jbd(bh))\n\t\tgoto not_jbd;\n\tjh = bh2jh(bh);\n\n\t/* Critical error: attempting to delete a bitmap buffer, maybe?\n\t * Don't do any jbd operations, and return an error. */\n\tif (!J_EXPECT_JH(jh, !jh->b_committed_data,\n\t\t\t \"inconsistent data on disk\")) {\n\t\terr = -EIO;\n\t\tgoto not_jbd;\n\t}\n\n\t/* keep track of wether or not this transaction modified us */\n\twas_modified = jh->b_modified;\n\n\t/*\n\t * The buffer's going from the transaction, we must drop\n\t * all references -bzzz\n\t */\n\tjh->b_modified = 0;\n\n\tif (jh->b_transaction == handle->h_transaction) {\n\t\tJ_ASSERT_JH(jh, !jh->b_frozen_data);\n\n\t\t/* If we are forgetting a buffer which is already part\n\t\t * of this transaction, then we can just drop it from\n\t\t * the transaction immediately. */\n\t\tclear_buffer_dirty(bh);\n\t\tclear_buffer_jbddirty(bh);\n\n\t\tJBUFFER_TRACE(jh, \"belongs to current transaction: unfile\");\n\n\t\t/*\n\t\t * we only want to drop a reference if this transaction\n\t\t * modified the buffer\n\t\t */\n\t\tif (was_modified)\n\t\t\tdrop_reserve = 1;\n\n\t\t/*\n\t\t * We are no longer going to journal this buffer.\n\t\t * However, the commit of this transaction is still\n\t\t * important to the buffer: the delete that we are now\n\t\t * processing might obsolete an old log entry, so by\n\t\t * committing, we can satisfy the buffer's checkpoint.\n\t\t *\n\t\t * So, if we have a checkpoint on the buffer, we should\n\t\t * now refile the buffer on our BJ_Forget list so that\n\t\t * we know to remove the checkpoint after we commit.\n\t\t */\n\n\t\tif (jh->b_cp_transaction) {\n\t\t\t__jbd2_journal_temp_unlink_buffer(jh);\n\t\t\t__jbd2_journal_file_buffer(jh, transaction, BJ_Forget);\n\t\t} else {\n\t\t\t__jbd2_journal_unfile_buffer(jh);\n\t\t\tif (!buffer_jbd(bh)) {\n\t\t\t\tspin_unlock(&journal->j_list_lock);\n\t\t\t\tjbd_unlock_bh_state(bh);\n\t\t\t\t__bforget(bh);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\t} else if (jh->b_transaction) {\n\t\tJ_ASSERT_JH(jh, (jh->b_transaction ==\n\t\t\t\t journal->j_committing_transaction));\n\t\t/* However, if the buffer is still owned by a prior\n\t\t * (committing) transaction, we can't drop it yet... */\n\t\tJBUFFER_TRACE(jh, \"belongs to older transaction\");\n\t\t/* ... but we CAN drop it from the new transaction if we\n\t\t * have also modified it since the original commit. */\n\n\t\tif (jh->b_next_transaction) {\n\t\t\tJ_ASSERT(jh->b_next_transaction == transaction);\n\t\t\tjh->b_next_transaction = NULL;\n\n\t\t\t/*\n\t\t\t * only drop a reference if this transaction modified\n\t\t\t * the buffer\n\t\t\t */\n\t\t\tif (was_modified)\n\t\t\t\tdrop_reserve = 1;\n\t\t}\n\t}\n\nnot_jbd:\n\tspin_unlock(&journal->j_list_lock);\n\tjbd_unlock_bh_state(bh);\n\t__brelse(bh);\ndrop:\n\tif (drop_reserve) {\n\t\t/* no need to reserve log space for this block -bzzz */\n\t\thandle->h_buffer_credits++;\n\t}\n\treturn err;\n}\n\n/**\n * int jbd2_journal_stop() - complete a transaction\n * @handle: tranaction to complete.\n *\n * All done for a particular handle.\n *\n * There is not much action needed here.  We just return any remaining\n * buffer credits to the transaction and remove the handle.  The only\n * complication is that we need to start a commit operation if the\n * filesystem is marked for synchronous update.\n *\n * jbd2_journal_stop itself will not usually return an error, but it may\n * do so in unusual circumstances.  In particular, expect it to\n * return -EIO if a jbd2_journal_abort has been executed since the\n * transaction began.\n */\nint jbd2_journal_stop(handle_t *handle)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\tint err, wait_for_commit = 0;\n\ttid_t tid;\n\tpid_t pid;\n\n\tJ_ASSERT(journal_current_handle() == handle);\n\n\tif (is_handle_aborted(handle))\n\t\terr = -EIO;\n\telse {\n\t\tJ_ASSERT(atomic_read(&transaction->t_updates) > 0);\n\t\terr = 0;\n\t}\n\n\tif (--handle->h_ref > 0) {\n\t\tjbd_debug(4, \"h_ref %d -> %d\\n\", handle->h_ref + 1,\n\t\t\t  handle->h_ref);\n\t\treturn err;\n\t}\n\n\tjbd_debug(4, \"Handle %p going down\\n\", handle);\n\n\t/*\n\t * Implement synchronous transaction batching.  If the handle\n\t * was synchronous, don't force a commit immediately.  Let's\n\t * yield and let another thread piggyback onto this\n\t * transaction.  Keep doing that while new threads continue to\n\t * arrive.  It doesn't cost much - we're about to run a commit\n\t * and sleep on IO anyway.  Speeds up many-threaded, many-dir\n\t * operations by 30x or more...\n\t *\n\t * We try and optimize the sleep time against what the\n\t * underlying disk can do, instead of having a static sleep\n\t * time.  This is useful for the case where our storage is so\n\t * fast that it is more optimal to go ahead and force a flush\n\t * and wait for the transaction to be committed than it is to\n\t * wait for an arbitrary amount of time for new writers to\n\t * join the transaction.  We achieve this by measuring how\n\t * long it takes to commit a transaction, and compare it with\n\t * how long this transaction has been running, and if run time\n\t * < commit time then we sleep for the delta and commit.  This\n\t * greatly helps super fast disks that would see slowdowns as\n\t * more threads started doing fsyncs.\n\t *\n\t * But don't do this if this process was the most recent one\n\t * to perform a synchronous write.  We do this to detect the\n\t * case where a single process is doing a stream of sync\n\t * writes.  No point in waiting for joiners in that case.\n\t */\n\tpid = current->pid;\n\tif (handle->h_sync && journal->j_last_sync_writer != pid) {\n\t\tu64 commit_time, trans_time;\n\n\t\tjournal->j_last_sync_writer = pid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tcommit_time = journal->j_average_commit_time;\n\t\tread_unlock(&journal->j_state_lock);\n\n\t\ttrans_time = ktime_to_ns(ktime_sub(ktime_get(),\n\t\t\t\t\t\t   transaction->t_start_time));\n\n\t\tcommit_time = max_t(u64, commit_time,\n\t\t\t\t    1000*journal->j_min_batch_time);\n\t\tcommit_time = min_t(u64, commit_time,\n\t\t\t\t    1000*journal->j_max_batch_time);\n\n\t\tif (trans_time < commit_time) {\n\t\t\tktime_t expires = ktime_add_ns(ktime_get(),\n\t\t\t\t\t\t       commit_time);\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tschedule_hrtimeout(&expires, HRTIMER_MODE_ABS);\n\t\t}\n\t}\n\n\tif (handle->h_sync)\n\t\ttransaction->t_synchronous_commit = 1;\n\tcurrent->journal_info = NULL;\n\tatomic_sub(handle->h_buffer_credits,\n\t\t   &transaction->t_outstanding_credits);\n\n\t/*\n\t * If the handle is marked SYNC, we need to set another commit\n\t * going!  We also want to force a commit if the current\n\t * transaction is occupying too much of the log, or if the\n\t * transaction is too old now.\n\t */\n\tif (handle->h_sync ||\n\t    (atomic_read(&transaction->t_outstanding_credits) >\n\t     journal->j_max_transaction_buffers) ||\n\t    time_after_eq(jiffies, transaction->t_expires)) {\n\t\t/* Do this even for aborted journals: an abort still\n\t\t * completes the commit thread, it just doesn't write\n\t\t * anything to disk. */\n\n\t\tjbd_debug(2, \"transaction too old, requesting commit for \"\n\t\t\t\t\t\"handle %p\\n\", handle);\n\t\t/* This is non-blocking */\n\t\tjbd2_log_start_commit(journal, transaction->t_tid);\n\n\t\t/*\n\t\t * Special case: JBD2_SYNC synchronous updates require us\n\t\t * to wait for the commit to complete.\n\t\t */\n\t\tif (handle->h_sync && !(current->flags & PF_MEMALLOC))\n\t\t\twait_for_commit = 1;\n\t}\n\n\t/*\n\t * Once we drop t_updates, if it goes to zero the transaction\n\t * could start committing on us and eventually disappear.  So\n\t * once we do this, we must not dereference transaction\n\t * pointer again.\n\t */\n\ttid = transaction->t_tid;\n\tif (atomic_dec_and_test(&transaction->t_updates)) {\n\t\twake_up(&journal->j_wait_updates);\n\t\tif (journal->j_barrier_count)\n\t\t\twake_up(&journal->j_wait_transaction_locked);\n\t}\n\n\tif (wait_for_commit)\n\t\terr = jbd2_log_wait_commit(journal, tid);\n\n\tlock_map_release(&handle->h_lockdep_map);\n\n\tjbd2_free_handle(handle);\n\treturn err;\n}\n\n/**\n * int jbd2_journal_force_commit() - force any uncommitted transactions\n * @journal: journal to force\n *\n * For synchronous operations: force any uncommitted transactions\n * to disk.  May seem kludgy, but it reuses all the handle batching\n * code in a very simple manner.\n */\nint jbd2_journal_force_commit(journal_t *journal)\n{\n\thandle_t *handle;\n\tint ret;\n\n\thandle = jbd2_journal_start(journal, 1);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t} else {\n\t\thandle->h_sync = 1;\n\t\tret = jbd2_journal_stop(handle);\n\t}\n\treturn ret;\n}\n\n/*\n *\n * List management code snippets: various functions for manipulating the\n * transaction buffer lists.\n *\n */\n\n/*\n * Append a buffer to a transaction list, given the transaction's list head\n * pointer.\n *\n * j_list_lock is held.\n *\n * jbd_lock_bh_state(jh2bh(jh)) is held.\n */\n\nstatic inline void\n__blist_add_buffer(struct journal_head **list, struct journal_head *jh)\n{\n\tif (!*list) {\n\t\tjh->b_tnext = jh->b_tprev = jh;\n\t\t*list = jh;\n\t} else {\n\t\t/* Insert at the tail of the list to preserve order */\n\t\tstruct journal_head *first = *list, *last = first->b_tprev;\n\t\tjh->b_tprev = last;\n\t\tjh->b_tnext = first;\n\t\tlast->b_tnext = first->b_tprev = jh;\n\t}\n}\n\n/*\n * Remove a buffer from a transaction list, given the transaction's list\n * head pointer.\n *\n * Called with j_list_lock held, and the journal may not be locked.\n *\n * jbd_lock_bh_state(jh2bh(jh)) is held.\n */\n\nstatic inline void\n__blist_del_buffer(struct journal_head **list, struct journal_head *jh)\n{\n\tif (*list == jh) {\n\t\t*list = jh->b_tnext;\n\t\tif (*list == jh)\n\t\t\t*list = NULL;\n\t}\n\tjh->b_tprev->b_tnext = jh->b_tnext;\n\tjh->b_tnext->b_tprev = jh->b_tprev;\n}\n\n/*\n * Remove a buffer from the appropriate transaction list.\n *\n * Note that this function can *change* the value of\n * bh->b_transaction->t_buffers, t_forget, t_iobuf_list, t_shadow_list,\n * t_log_list or t_reserved_list.  If the caller is holding onto a copy of one\n * of these pointers, it could go bad.  Generally the caller needs to re-read\n * the pointer from the transaction_t.\n *\n * Called under j_list_lock.  The journal may not be locked.\n */\nvoid __jbd2_journal_temp_unlink_buffer(struct journal_head *jh)\n{\n\tstruct journal_head **list = NULL;\n\ttransaction_t *transaction;\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tJ_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));\n\ttransaction = jh->b_transaction;\n\tif (transaction)\n\t\tassert_spin_locked(&transaction->t_journal->j_list_lock);\n\n\tJ_ASSERT_JH(jh, jh->b_jlist < BJ_Types);\n\tif (jh->b_jlist != BJ_None)\n\t\tJ_ASSERT_JH(jh, transaction != NULL);\n\n\tswitch (jh->b_jlist) {\n\tcase BJ_None:\n\t\treturn;\n\tcase BJ_Metadata:\n\t\ttransaction->t_nr_buffers--;\n\t\tJ_ASSERT_JH(jh, transaction->t_nr_buffers >= 0);\n\t\tlist = &transaction->t_buffers;\n\t\tbreak;\n\tcase BJ_Forget:\n\t\tlist = &transaction->t_forget;\n\t\tbreak;\n\tcase BJ_IO:\n\t\tlist = &transaction->t_iobuf_list;\n\t\tbreak;\n\tcase BJ_Shadow:\n\t\tlist = &transaction->t_shadow_list;\n\t\tbreak;\n\tcase BJ_LogCtl:\n\t\tlist = &transaction->t_log_list;\n\t\tbreak;\n\tcase BJ_Reserved:\n\t\tlist = &transaction->t_reserved_list;\n\t\tbreak;\n\t}\n\n\t__blist_del_buffer(list, jh);\n\tjh->b_jlist = BJ_None;\n\tif (test_clear_buffer_jbddirty(bh))\n\t\tmark_buffer_dirty(bh);\t/* Expose it to the VM */\n}\n\n/*\n * Remove buffer from all transactions.\n *\n * Called with bh_state lock and j_list_lock\n *\n * jh and bh may be already freed when this function returns.\n */\nstatic void __jbd2_journal_unfile_buffer(struct journal_head *jh)\n{\n\t__jbd2_journal_temp_unlink_buffer(jh);\n\tjh->b_transaction = NULL;\n\tjbd2_journal_put_journal_head(jh);\n}\n\nvoid jbd2_journal_unfile_buffer(journal_t *journal, struct journal_head *jh)\n{\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\t/* Get reference so that buffer cannot be freed before we unlock it */\n\tget_bh(bh);\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\t__jbd2_journal_unfile_buffer(jh);\n\tspin_unlock(&journal->j_list_lock);\n\tjbd_unlock_bh_state(bh);\n\t__brelse(bh);\n}\n\n/*\n * Called from jbd2_journal_try_to_free_buffers().\n *\n * Called under jbd_lock_bh_state(bh)\n */\nstatic void\n__journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)\n{\n\tstruct journal_head *jh;\n\n\tjh = bh2jh(bh);\n\n\tif (buffer_locked(bh) || buffer_dirty(bh))\n\t\tgoto out;\n\n\tif (jh->b_next_transaction != NULL)\n\t\tgoto out;\n\n\tspin_lock(&journal->j_list_lock);\n\tif (jh->b_cp_transaction != NULL && jh->b_transaction == NULL) {\n\t\t/* written-back checkpointed metadata buffer */\n\t\tif (jh->b_jlist == BJ_None) {\n\t\t\tJBUFFER_TRACE(jh, \"remove from checkpoint list\");\n\t\t\t__jbd2_journal_remove_checkpoint(jh);\n\t\t}\n\t}\n\tspin_unlock(&journal->j_list_lock);\nout:\n\treturn;\n}\n\n/**\n * int jbd2_journal_try_to_free_buffers() - try to free page buffers.\n * @journal: journal for operation\n * @page: to try and free\n * @gfp_mask: we use the mask to detect how hard should we try to release\n * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to\n * release the buffers.\n *\n *\n * For all the buffers on this page,\n * if they are fully written out ordered data, move them onto BUF_CLEAN\n * so try_to_free_buffers() can reap them.\n *\n * This function returns non-zero if we wish try_to_free_buffers()\n * to be called. We do this if the page is releasable by try_to_free_buffers().\n * We also do it if the page has locked or dirty buffers and the caller wants\n * us to perform sync or async writeout.\n *\n * This complicates JBD locking somewhat.  We aren't protected by the\n * BKL here.  We wish to remove the buffer from its committing or\n * running transaction's ->t_datalist via __jbd2_journal_unfile_buffer.\n *\n * This may *change* the value of transaction_t->t_datalist, so anyone\n * who looks at t_datalist needs to lock against this function.\n *\n * Even worse, someone may be doing a jbd2_journal_dirty_data on this\n * buffer.  So we need to lock against that.  jbd2_journal_dirty_data()\n * will come out of the lock with the buffer dirty, which makes it\n * ineligible for release here.\n *\n * Who else is affected by this?  hmm...  Really the only contender\n * is do_get_write_access() - it could be looking at the buffer while\n * journal_try_to_free_buffer() is changing its state.  But that\n * cannot happen because we never reallocate freed data as metadata\n * while the data is part of a transaction.  Yes?\n *\n * Return 0 on failure, 1 on success\n */\nint jbd2_journal_try_to_free_buffers(journal_t *journal,\n\t\t\t\tstruct page *page, gfp_t gfp_mask)\n{\n\tstruct buffer_head *head;\n\tstruct buffer_head *bh;\n\tint ret = 0;\n\n\tJ_ASSERT(PageLocked(page));\n\n\thead = page_buffers(page);\n\tbh = head;\n\tdo {\n\t\tstruct journal_head *jh;\n\n\t\t/*\n\t\t * We take our own ref against the journal_head here to avoid\n\t\t * having to add tons of locking around each instance of\n\t\t * jbd2_journal_put_journal_head().\n\t\t */\n\t\tjh = jbd2_journal_grab_journal_head(bh);\n\t\tif (!jh)\n\t\t\tcontinue;\n\n\t\tjbd_lock_bh_state(bh);\n\t\t__journal_try_to_free_buffer(journal, bh);\n\t\tjbd2_journal_put_journal_head(jh);\n\t\tjbd_unlock_bh_state(bh);\n\t\tif (buffer_jbd(bh))\n\t\t\tgoto busy;\n\t} while ((bh = bh->b_this_page) != head);\n\n\tret = try_to_free_buffers(page);\n\nbusy:\n\treturn ret;\n}\n\n/*\n * This buffer is no longer needed.  If it is on an older transaction's\n * checkpoint list we need to record it on this transaction's forget list\n * to pin this buffer (and hence its checkpointing transaction) down until\n * this transaction commits.  If the buffer isn't on a checkpoint list, we\n * release it.\n * Returns non-zero if JBD no longer has an interest in the buffer.\n *\n * Called under j_list_lock.\n *\n * Called under jbd_lock_bh_state(bh).\n */\nstatic int __dispose_buffer(struct journal_head *jh, transaction_t *transaction)\n{\n\tint may_free = 1;\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tif (jh->b_cp_transaction) {\n\t\tJBUFFER_TRACE(jh, \"on running+cp transaction\");\n\t\t__jbd2_journal_temp_unlink_buffer(jh);\n\t\t/*\n\t\t * We don't want to write the buffer anymore, clear the\n\t\t * bit so that we don't confuse checks in\n\t\t * __journal_file_buffer\n\t\t */\n\t\tclear_buffer_dirty(bh);\n\t\t__jbd2_journal_file_buffer(jh, transaction, BJ_Forget);\n\t\tmay_free = 0;\n\t} else {\n\t\tJBUFFER_TRACE(jh, \"on running transaction\");\n\t\t__jbd2_journal_unfile_buffer(jh);\n\t}\n\treturn may_free;\n}\n\n/*\n * jbd2_journal_invalidatepage\n *\n * This code is tricky.  It has a number of cases to deal with.\n *\n * There are two invariants which this code relies on:\n *\n * i_size must be updated on disk before we start calling invalidatepage on the\n * data.\n *\n *  This is done in ext3 by defining an ext3_setattr method which\n *  updates i_size before truncate gets going.  By maintaining this\n *  invariant, we can be sure that it is safe to throw away any buffers\n *  attached to the current transaction: once the transaction commits,\n *  we know that the data will not be needed.\n *\n *  Note however that we can *not* throw away data belonging to the\n *  previous, committing transaction!\n *\n * Any disk blocks which *are* part of the previous, committing\n * transaction (and which therefore cannot be discarded immediately) are\n * not going to be reused in the new running transaction\n *\n *  The bitmap committed_data images guarantee this: any block which is\n *  allocated in one transaction and removed in the next will be marked\n *  as in-use in the committed_data bitmap, so cannot be reused until\n *  the next transaction to delete the block commits.  This means that\n *  leaving committing buffers dirty is quite safe: the disk blocks\n *  cannot be reallocated to a different file and so buffer aliasing is\n *  not possible.\n *\n *\n * The above applies mainly to ordered data mode.  In writeback mode we\n * don't make guarantees about the order in which data hits disk --- in\n * particular we don't guarantee that new dirty data is flushed before\n * transaction commit --- so it is always safe just to discard data\n * immediately in that mode.  --sct\n */\n\n/*\n * The journal_unmap_buffer helper function returns zero if the buffer\n * concerned remains pinned as an anonymous buffer belonging to an older\n * transaction.\n *\n * We're outside-transaction here.  Either or both of j_running_transaction\n * and j_committing_transaction may be NULL.\n */\nstatic int journal_unmap_buffer(journal_t *journal, struct buffer_head *bh)\n{\n\ttransaction_t *transaction;\n\tstruct journal_head *jh;\n\tint may_free = 1;\n\tint ret;\n\n\tBUFFER_TRACE(bh, \"entry\");\n\n\t/*\n\t * It is safe to proceed here without the j_list_lock because the\n\t * buffers cannot be stolen by try_to_free_buffers as long as we are\n\t * holding the page lock. --sct\n\t */\n\n\tif (!buffer_jbd(bh))\n\t\tgoto zap_buffer_unlocked;\n\n\t/* OK, we have data buffer in journaled mode */\n\twrite_lock(&journal->j_state_lock);\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\n\tjh = jbd2_journal_grab_journal_head(bh);\n\tif (!jh)\n\t\tgoto zap_buffer_no_jh;\n\n\t/*\n\t * We cannot remove the buffer from checkpoint lists until the\n\t * transaction adding inode to orphan list (let's call it T)\n\t * is committed.  Otherwise if the transaction changing the\n\t * buffer would be cleaned from the journal before T is\n\t * committed, a crash will cause that the correct contents of\n\t * the buffer will be lost.  On the other hand we have to\n\t * clear the buffer dirty bit at latest at the moment when the\n\t * transaction marking the buffer as freed in the filesystem\n\t * structures is committed because from that moment on the\n\t * buffer can be reallocated and used by a different page.\n\t * Since the block hasn't been freed yet but the inode has\n\t * already been added to orphan list, it is safe for us to add\n\t * the buffer to BJ_Forget list of the newest transaction.\n\t */\n\ttransaction = jh->b_transaction;\n\tif (transaction == NULL) {\n\t\t/* First case: not on any transaction.  If it\n\t\t * has no checkpoint link, then we can zap it:\n\t\t * it's a writeback-mode buffer so we don't care\n\t\t * if it hits disk safely. */\n\t\tif (!jh->b_cp_transaction) {\n\t\t\tJBUFFER_TRACE(jh, \"not on any transaction: zap\");\n\t\t\tgoto zap_buffer;\n\t\t}\n\n\t\tif (!buffer_dirty(bh)) {\n\t\t\t/* bdflush has written it.  We can drop it now */\n\t\t\tgoto zap_buffer;\n\t\t}\n\n\t\t/* OK, it must be in the journal but still not\n\t\t * written fully to disk: it's metadata or\n\t\t * journaled data... */\n\n\t\tif (journal->j_running_transaction) {\n\t\t\t/* ... and once the current transaction has\n\t\t\t * committed, the buffer won't be needed any\n\t\t\t * longer. */\n\t\t\tJBUFFER_TRACE(jh, \"checkpointed: add to BJ_Forget\");\n\t\t\tret = __dispose_buffer(jh,\n\t\t\t\t\tjournal->j_running_transaction);\n\t\t\tjbd2_journal_put_journal_head(jh);\n\t\t\tspin_unlock(&journal->j_list_lock);\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t\twrite_unlock(&journal->j_state_lock);\n\t\t\treturn ret;\n\t\t} else {\n\t\t\t/* There is no currently-running transaction. So the\n\t\t\t * orphan record which we wrote for this file must have\n\t\t\t * passed into commit.  We must attach this buffer to\n\t\t\t * the committing transaction, if it exists. */\n\t\t\tif (journal->j_committing_transaction) {\n\t\t\t\tJBUFFER_TRACE(jh, \"give to committing trans\");\n\t\t\t\tret = __dispose_buffer(jh,\n\t\t\t\t\tjournal->j_committing_transaction);\n\t\t\t\tjbd2_journal_put_journal_head(jh);\n\t\t\t\tspin_unlock(&journal->j_list_lock);\n\t\t\t\tjbd_unlock_bh_state(bh);\n\t\t\t\twrite_unlock(&journal->j_state_lock);\n\t\t\t\treturn ret;\n\t\t\t} else {\n\t\t\t\t/* The orphan record's transaction has\n\t\t\t\t * committed.  We can cleanse this buffer */\n\t\t\t\tclear_buffer_jbddirty(bh);\n\t\t\t\tgoto zap_buffer;\n\t\t\t}\n\t\t}\n\t} else if (transaction == journal->j_committing_transaction) {\n\t\tJBUFFER_TRACE(jh, \"on committing transaction\");\n\t\t/*\n\t\t * The buffer is committing, we simply cannot touch\n\t\t * it. So we just set j_next_transaction to the\n\t\t * running transaction (if there is one) and mark\n\t\t * buffer as freed so that commit code knows it should\n\t\t * clear dirty bits when it is done with the buffer.\n\t\t */\n\t\tset_buffer_freed(bh);\n\t\tif (journal->j_running_transaction && buffer_jbddirty(bh))\n\t\t\tjh->b_next_transaction = journal->j_running_transaction;\n\t\tjbd2_journal_put_journal_head(jh);\n\t\tspin_unlock(&journal->j_list_lock);\n\t\tjbd_unlock_bh_state(bh);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\treturn 0;\n\t} else {\n\t\t/* Good, the buffer belongs to the running transaction.\n\t\t * We are writing our own transaction's data, not any\n\t\t * previous one's, so it is safe to throw it away\n\t\t * (remember that we expect the filesystem to have set\n\t\t * i_size already for this truncate so recovery will not\n\t\t * expose the disk blocks we are discarding here.) */\n\t\tJ_ASSERT_JH(jh, transaction == journal->j_running_transaction);\n\t\tJBUFFER_TRACE(jh, \"on running transaction\");\n\t\tmay_free = __dispose_buffer(jh, transaction);\n\t}\n\nzap_buffer:\n\tjbd2_journal_put_journal_head(jh);\nzap_buffer_no_jh:\n\tspin_unlock(&journal->j_list_lock);\n\tjbd_unlock_bh_state(bh);\n\twrite_unlock(&journal->j_state_lock);\nzap_buffer_unlocked:\n\tclear_buffer_dirty(bh);\n\tJ_ASSERT_BH(bh, !buffer_jbddirty(bh));\n\tclear_buffer_mapped(bh);\n\tclear_buffer_req(bh);\n\tclear_buffer_new(bh);\n\tclear_buffer_delay(bh);\n\tclear_buffer_unwritten(bh);\n\tbh->b_bdev = NULL;\n\treturn may_free;\n}\n\n/**\n * void jbd2_journal_invalidatepage()\n * @journal: journal to use for flush...\n * @page:    page to flush\n * @offset:  length of page to invalidate.\n *\n * Reap page buffers containing data after offset in page.\n *\n */\nvoid jbd2_journal_invalidatepage(journal_t *journal,\n\t\t      struct page *page,\n\t\t      unsigned long offset)\n{\n\tstruct buffer_head *head, *bh, *next;\n\tunsigned int curr_off = 0;\n\tint may_free = 1;\n\n\tif (!PageLocked(page))\n\t\tBUG();\n\tif (!page_has_buffers(page))\n\t\treturn;\n\n\t/* We will potentially be playing with lists other than just the\n\t * data lists (especially for journaled data mode), so be\n\t * cautious in our locking. */\n\n\thead = bh = page_buffers(page);\n\tdo {\n\t\tunsigned int next_off = curr_off + bh->b_size;\n\t\tnext = bh->b_this_page;\n\n\t\tif (offset <= curr_off) {\n\t\t\t/* This block is wholly outside the truncation point */\n\t\t\tlock_buffer(bh);\n\t\t\tmay_free &= journal_unmap_buffer(journal, bh);\n\t\t\tunlock_buffer(bh);\n\t\t}\n\t\tcurr_off = next_off;\n\t\tbh = next;\n\n\t} while (bh != head);\n\n\tif (!offset) {\n\t\tif (may_free && try_to_free_buffers(page))\n\t\t\tJ_ASSERT(!page_has_buffers(page));\n\t}\n}\n\n/*\n * File a buffer on the given transaction list.\n */\nvoid __jbd2_journal_file_buffer(struct journal_head *jh,\n\t\t\ttransaction_t *transaction, int jlist)\n{\n\tstruct journal_head **list = NULL;\n\tint was_dirty = 0;\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tJ_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));\n\tassert_spin_locked(&transaction->t_journal->j_list_lock);\n\n\tJ_ASSERT_JH(jh, jh->b_jlist < BJ_Types);\n\tJ_ASSERT_JH(jh, jh->b_transaction == transaction ||\n\t\t\t\tjh->b_transaction == NULL);\n\n\tif (jh->b_transaction && jh->b_jlist == jlist)\n\t\treturn;\n\n\tif (jlist == BJ_Metadata || jlist == BJ_Reserved ||\n\t    jlist == BJ_Shadow || jlist == BJ_Forget) {\n\t\t/*\n\t\t * For metadata buffers, we track dirty bit in buffer_jbddirty\n\t\t * instead of buffer_dirty. We should not see a dirty bit set\n\t\t * here because we clear it in do_get_write_access but e.g.\n\t\t * tune2fs can modify the sb and set the dirty bit at any time\n\t\t * so we try to gracefully handle that.\n\t\t */\n\t\tif (buffer_dirty(bh))\n\t\t\twarn_dirty_buffer(bh);\n\t\tif (test_clear_buffer_dirty(bh) ||\n\t\t    test_clear_buffer_jbddirty(bh))\n\t\t\twas_dirty = 1;\n\t}\n\n\tif (jh->b_transaction)\n\t\t__jbd2_journal_temp_unlink_buffer(jh);\n\telse\n\t\tjbd2_journal_grab_journal_head(bh);\n\tjh->b_transaction = transaction;\n\n\tswitch (jlist) {\n\tcase BJ_None:\n\t\tJ_ASSERT_JH(jh, !jh->b_committed_data);\n\t\tJ_ASSERT_JH(jh, !jh->b_frozen_data);\n\t\treturn;\n\tcase BJ_Metadata:\n\t\ttransaction->t_nr_buffers++;\n\t\tlist = &transaction->t_buffers;\n\t\tbreak;\n\tcase BJ_Forget:\n\t\tlist = &transaction->t_forget;\n\t\tbreak;\n\tcase BJ_IO:\n\t\tlist = &transaction->t_iobuf_list;\n\t\tbreak;\n\tcase BJ_Shadow:\n\t\tlist = &transaction->t_shadow_list;\n\t\tbreak;\n\tcase BJ_LogCtl:\n\t\tlist = &transaction->t_log_list;\n\t\tbreak;\n\tcase BJ_Reserved:\n\t\tlist = &transaction->t_reserved_list;\n\t\tbreak;\n\t}\n\n\t__blist_add_buffer(list, jh);\n\tjh->b_jlist = jlist;\n\n\tif (was_dirty)\n\t\tset_buffer_jbddirty(bh);\n}\n\nvoid jbd2_journal_file_buffer(struct journal_head *jh,\n\t\t\t\ttransaction_t *transaction, int jlist)\n{\n\tjbd_lock_bh_state(jh2bh(jh));\n\tspin_lock(&transaction->t_journal->j_list_lock);\n\t__jbd2_journal_file_buffer(jh, transaction, jlist);\n\tspin_unlock(&transaction->t_journal->j_list_lock);\n\tjbd_unlock_bh_state(jh2bh(jh));\n}\n\n/*\n * Remove a buffer from its current buffer list in preparation for\n * dropping it from its current transaction entirely.  If the buffer has\n * already started to be used by a subsequent transaction, refile the\n * buffer on that transaction's metadata list.\n *\n * Called under j_list_lock\n * Called under jbd_lock_bh_state(jh2bh(jh))\n *\n * jh and bh may be already free when this function returns\n */\nvoid __jbd2_journal_refile_buffer(struct journal_head *jh)\n{\n\tint was_dirty, jlist;\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\tJ_ASSERT_JH(jh, jbd_is_locked_bh_state(bh));\n\tif (jh->b_transaction)\n\t\tassert_spin_locked(&jh->b_transaction->t_journal->j_list_lock);\n\n\t/* If the buffer is now unused, just drop it. */\n\tif (jh->b_next_transaction == NULL) {\n\t\t__jbd2_journal_unfile_buffer(jh);\n\t\treturn;\n\t}\n\n\t/*\n\t * It has been modified by a later transaction: add it to the new\n\t * transaction's metadata list.\n\t */\n\n\twas_dirty = test_clear_buffer_jbddirty(bh);\n\t__jbd2_journal_temp_unlink_buffer(jh);\n\t/*\n\t * We set b_transaction here because b_next_transaction will inherit\n\t * our jh reference and thus __jbd2_journal_file_buffer() must not\n\t * take a new one.\n\t */\n\tjh->b_transaction = jh->b_next_transaction;\n\tjh->b_next_transaction = NULL;\n\tif (buffer_freed(bh))\n\t\tjlist = BJ_Forget;\n\telse if (jh->b_modified)\n\t\tjlist = BJ_Metadata;\n\telse\n\t\tjlist = BJ_Reserved;\n\t__jbd2_journal_file_buffer(jh, jh->b_transaction, jlist);\n\tJ_ASSERT_JH(jh, jh->b_transaction->t_state == T_RUNNING);\n\n\tif (was_dirty)\n\t\tset_buffer_jbddirty(bh);\n}\n\n/*\n * __jbd2_journal_refile_buffer() with necessary locking added. We take our\n * bh reference so that we can safely unlock bh.\n *\n * The jh and bh may be freed by this call.\n */\nvoid jbd2_journal_refile_buffer(journal_t *journal, struct journal_head *jh)\n{\n\tstruct buffer_head *bh = jh2bh(jh);\n\n\t/* Get reference so that buffer cannot be freed before we unlock it */\n\tget_bh(bh);\n\tjbd_lock_bh_state(bh);\n\tspin_lock(&journal->j_list_lock);\n\t__jbd2_journal_refile_buffer(jh);\n\tjbd_unlock_bh_state(bh);\n\tspin_unlock(&journal->j_list_lock);\n\t__brelse(bh);\n}\n\n/*\n * File inode in the inode list of the handle's transaction\n */\nint jbd2_journal_file_inode(handle_t *handle, struct jbd2_inode *jinode)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal = transaction->t_journal;\n\n\tif (is_handle_aborted(handle))\n\t\treturn -EIO;\n\n\tjbd_debug(4, \"Adding inode %lu, tid:%d\\n\", jinode->i_vfs_inode->i_ino,\n\t\t\ttransaction->t_tid);\n\n\t/*\n\t * First check whether inode isn't already on the transaction's\n\t * lists without taking the lock. Note that this check is safe\n\t * without the lock as we cannot race with somebody removing inode\n\t * from the transaction. The reason is that we remove inode from the\n\t * transaction only in journal_release_jbd_inode() and when we commit\n\t * the transaction. We are guarded from the first case by holding\n\t * a reference to the inode. We are safe against the second case\n\t * because if jinode->i_transaction == transaction, commit code\n\t * cannot touch the transaction because we hold reference to it,\n\t * and if jinode->i_next_transaction == transaction, commit code\n\t * will only file the inode where we want it.\n\t */\n\tif (jinode->i_transaction == transaction ||\n\t    jinode->i_next_transaction == transaction)\n\t\treturn 0;\n\n\tspin_lock(&journal->j_list_lock);\n\n\tif (jinode->i_transaction == transaction ||\n\t    jinode->i_next_transaction == transaction)\n\t\tgoto done;\n\n\t/*\n\t * We only ever set this variable to 1 so the test is safe. Since\n\t * t_need_data_flush is likely to be set, we do the test to save some\n\t * cacheline bouncing\n\t */\n\tif (!transaction->t_need_data_flush)\n\t\ttransaction->t_need_data_flush = 1;\n\t/* On some different transaction's list - should be\n\t * the committing one */\n\tif (jinode->i_transaction) {\n\t\tJ_ASSERT(jinode->i_next_transaction == NULL);\n\t\tJ_ASSERT(jinode->i_transaction ==\n\t\t\t\t\tjournal->j_committing_transaction);\n\t\tjinode->i_next_transaction = transaction;\n\t\tgoto done;\n\t}\n\t/* Not on any transaction list... */\n\tJ_ASSERT(!jinode->i_next_transaction);\n\tjinode->i_transaction = transaction;\n\tlist_add(&jinode->i_list, &transaction->t_inode_list);\ndone:\n\tspin_unlock(&journal->j_list_lock);\n\n\treturn 0;\n}\n\n/*\n * File truncate and transaction commit interact with each other in a\n * non-trivial way.  If a transaction writing data block A is\n * committing, we cannot discard the data by truncate until we have\n * written them.  Otherwise if we crashed after the transaction with\n * write has committed but before the transaction with truncate has\n * committed, we could see stale data in block A.  This function is a\n * helper to solve this problem.  It starts writeout of the truncated\n * part in case it is in the committing transaction.\n *\n * Filesystem code must call this function when inode is journaled in\n * ordered mode before truncation happens and after the inode has been\n * placed on orphan list with the new inode size. The second condition\n * avoids the race that someone writes new data and we start\n * committing the transaction after this function has been called but\n * before a transaction for truncate is started (and furthermore it\n * allows us to optimize the case where the addition to orphan list\n * happens in the same transaction as write --- we don't have to write\n * any data in such case).\n */\nint jbd2_journal_begin_ordered_truncate(journal_t *journal,\n\t\t\t\t\tstruct jbd2_inode *jinode,\n\t\t\t\t\tloff_t new_size)\n{\n\ttransaction_t *inode_trans, *commit_trans;\n\tint ret = 0;\n\n\t/* This is a quick check to avoid locking if not necessary */\n\tif (!jinode->i_transaction)\n\t\tgoto out;\n\t/* Locks are here just to force reading of recent values, it is\n\t * enough that the transaction was not committing before we started\n\t * a transaction adding the inode to orphan list */\n\tread_lock(&journal->j_state_lock);\n\tcommit_trans = journal->j_committing_transaction;\n\tread_unlock(&journal->j_state_lock);\n\tspin_lock(&journal->j_list_lock);\n\tinode_trans = jinode->i_transaction;\n\tspin_unlock(&journal->j_list_lock);\n\tif (inode_trans == commit_trans) {\n\t\tret = filemap_fdatawrite_range(jinode->i_vfs_inode->i_mapping,\n\t\t\tnew_size, LLONG_MAX);\n\t\tif (ret)\n\t\t\tjbd2_journal_abort(journal, ret);\n\t}\nout:\n\treturn ret;\n}\n"], "filenames": ["fs/jbd2/transaction.c"], "buggy_code_start_loc": [1951], "buggy_code_end_loc": [1951], "fixing_code_start_loc": [1952], "fixing_code_end_loc": [1954], "type": "CWE-119", "message": "The journal_unmap_buffer function in fs/jbd2/transaction.c in the Linux kernel before 3.3.1 does not properly handle the _Delay and _Unwritten buffer head states, which allows local users to cause a denial of service (system crash) by leveraging the presence of an ext4 filesystem that was mounted with a journal.", "other": {"cve": {"id": "CVE-2011-4086", "sourceIdentifier": "secalert@redhat.com", "published": "2012-07-03T16:40:31.020", "lastModified": "2023-02-13T01:21:17.457", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The journal_unmap_buffer function in fs/jbd2/transaction.c in the Linux kernel before 3.3.1 does not properly handle the _Delay and _Unwritten buffer head states, which allows local users to cause a denial of service (system crash) by leveraging the presence of an ext4 filesystem that was mounted with a journal."}, {"lang": "es", "value": "La funci\u00f3n journal_unmap_buffer en  fs/jbd2/transaction.c  en el kernel de linux anterior a v3.3.1 no maneja correctamente el \"buffer head states\" _Delay y _Unwritten, permitiendo a usuarios locales causar una denegaci\u00f3n de servicio aprovech\u00e1ndose de la presencia de un sistema de ficheros ext4 que est\u00e1 montado con journal"}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.3", "matchCriteriaId": "46B7FB28-475B-4275-B5ED-6598BA0294DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=15291164b22a357cb211b618adfef4fa82fc0de3", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2012-04/msg00021.html", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2012-05/msg00013.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2012-0571.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2012-0670.html", "source": "secalert@redhat.com"}, {"url": "http://www.debian.org/security/2012/dsa-2469", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.3.1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=749143", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/15291164b22a357cb211b618adfef4fa82fc0de3", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/15291164b22a357cb211b618adfef4fa82fc0de3"}}