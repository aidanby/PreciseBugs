{"buggy_code": ["/*\n *  linux/arch/arm/mm/dma-mapping.c\n *\n *  Copyright (C) 2000-2004 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  DMA uncached mapping support.\n */\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/gfp.h>\n#include <linux/errno.h>\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/dma-contiguous.h>\n#include <linux/highmem.h>\n#include <linux/memblock.h>\n#include <linux/slab.h>\n#include <linux/iommu.h>\n#include <linux/io.h>\n#include <linux/vmalloc.h>\n#include <linux/sizes.h>\n\n#include <asm/memory.h>\n#include <asm/highmem.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n#include <asm/mach/arch.h>\n#include <asm/dma-iommu.h>\n#include <asm/mach/map.h>\n#include <asm/system_info.h>\n#include <asm/dma-contiguous.h>\n\n#include \"mm.h\"\n\n/*\n * The DMA API is built upon the notion of \"buffer ownership\".  A buffer\n * is either exclusively owned by the CPU (and therefore may be accessed\n * by it) or exclusively owned by the DMA device.  These helper functions\n * represent the transitions between these two ownership states.\n *\n * Note, however, that on later ARMs, this notion does not work due to\n * speculative prefetches.  We model our approach on the assumption that\n * the CPU does do speculative prefetches, which means we clean caches\n * before transfers and delay cache invalidation until transfer completion.\n *\n */\nstatic void __dma_page_cpu_to_dev(struct page *, unsigned long,\n\t\tsize_t, enum dma_data_direction);\nstatic void __dma_page_dev_to_cpu(struct page *, unsigned long,\n\t\tsize_t, enum dma_data_direction);\n\n/**\n * arm_dma_map_page - map a portion of a page for streaming DMA\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * Ensure that any data held in the cache is appropriately discarded\n * or written back.\n *\n * The device owns this memory once this call has completed.  The CPU\n * can regain ownership by calling dma_unmap_page().\n */\nstatic dma_addr_t arm_dma_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_cpu_to_dev(page, offset, size, dir);\n\treturn pfn_to_dma(dev, page_to_pfn(page)) + offset;\n}\n\nstatic dma_addr_t arm_coherent_dma_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\treturn pfn_to_dma(dev, page_to_pfn(page)) + offset;\n}\n\n/**\n * arm_dma_unmap_page - unmap a buffer previously mapped through dma_map_page()\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * Unmap a page streaming mode DMA translation.  The handle and size\n * must match what was provided in the previous dma_map_page() call.\n * All other usages are undefined.\n *\n * After this call, reads by the CPU to the buffer are guaranteed to see\n * whatever the device wrote there.\n */\nstatic void arm_dma_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),\n\t\t\t\t      handle & ~PAGE_MASK, size, dir);\n}\n\nstatic void arm_dma_sync_single_for_cpu(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tunsigned int offset = handle & (PAGE_SIZE - 1);\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle-offset));\n\t__dma_page_dev_to_cpu(page, offset, size, dir);\n}\n\nstatic void arm_dma_sync_single_for_device(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tunsigned int offset = handle & (PAGE_SIZE - 1);\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle-offset));\n\t__dma_page_cpu_to_dev(page, offset, size, dir);\n}\n\nstruct dma_map_ops arm_dma_ops = {\n\t.alloc\t\t\t= arm_dma_alloc,\n\t.free\t\t\t= arm_dma_free,\n\t.mmap\t\t\t= arm_dma_mmap,\n\t.get_sgtable\t\t= arm_dma_get_sgtable,\n\t.map_page\t\t= arm_dma_map_page,\n\t.unmap_page\t\t= arm_dma_unmap_page,\n\t.map_sg\t\t\t= arm_dma_map_sg,\n\t.unmap_sg\t\t= arm_dma_unmap_sg,\n\t.sync_single_for_cpu\t= arm_dma_sync_single_for_cpu,\n\t.sync_single_for_device\t= arm_dma_sync_single_for_device,\n\t.sync_sg_for_cpu\t= arm_dma_sync_sg_for_cpu,\n\t.sync_sg_for_device\t= arm_dma_sync_sg_for_device,\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\nEXPORT_SYMBOL(arm_dma_ops);\n\nstatic void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n\tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs);\nstatic void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t\t  dma_addr_t handle, struct dma_attrs *attrs);\n\nstruct dma_map_ops arm_coherent_dma_ops = {\n\t.alloc\t\t\t= arm_coherent_dma_alloc,\n\t.free\t\t\t= arm_coherent_dma_free,\n\t.mmap\t\t\t= arm_dma_mmap,\n\t.get_sgtable\t\t= arm_dma_get_sgtable,\n\t.map_page\t\t= arm_coherent_dma_map_page,\n\t.map_sg\t\t\t= arm_dma_map_sg,\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\nEXPORT_SYMBOL(arm_coherent_dma_ops);\n\nstatic u64 get_coherent_dma_mask(struct device *dev)\n{\n\tu64 mask = (u64)arm_dma_limit;\n\n\tif (dev) {\n\t\tmask = dev->coherent_dma_mask;\n\n\t\t/*\n\t\t * Sanity check the DMA mask - it must be non-zero, and\n\t\t * must be able to be satisfied by a DMA allocation.\n\t\t */\n\t\tif (mask == 0) {\n\t\t\tdev_warn(dev, \"coherent DMA mask is unset\\n\");\n\t\t\treturn 0;\n\t\t}\n\n\t\tif ((~mask) & (u64)arm_dma_limit) {\n\t\t\tdev_warn(dev, \"coherent DMA mask %#llx is smaller \"\n\t\t\t\t \"than system GFP_DMA mask %#llx\\n\",\n\t\t\t\t mask, (u64)arm_dma_limit);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn mask;\n}\n\nstatic void __dma_clear_buffer(struct page *page, size_t size)\n{\n\t/*\n\t * Ensure that the allocated pages are zeroed, and that any data\n\t * lurking in the kernel direct-mapped region is invalidated.\n\t */\n\tif (PageHighMem(page)) {\n\t\tphys_addr_t base = __pfn_to_phys(page_to_pfn(page));\n\t\tphys_addr_t end = base + size;\n\t\twhile (size > 0) {\n\t\t\tvoid *ptr = kmap_atomic(page);\n\t\t\tmemset(ptr, 0, PAGE_SIZE);\n\t\t\tdmac_flush_range(ptr, ptr + PAGE_SIZE);\n\t\t\tkunmap_atomic(ptr);\n\t\t\tpage++;\n\t\t\tsize -= PAGE_SIZE;\n\t\t}\n\t\touter_flush_range(base, end);\n\t} else {\n\t\tvoid *ptr = page_address(page);\n\t\tmemset(ptr, 0, size);\n\t\tdmac_flush_range(ptr, ptr + size);\n\t\touter_flush_range(__pa(ptr), __pa(ptr) + size);\n\t}\n}\n\n/*\n * Allocate a DMA buffer for 'dev' of size 'size' using the\n * specified gfp mask.  Note that 'size' must be page aligned.\n */\nstatic struct page *__dma_alloc_buffer(struct device *dev, size_t size, gfp_t gfp)\n{\n\tunsigned long order = get_order(size);\n\tstruct page *page, *p, *e;\n\n\tpage = alloc_pages(gfp, order);\n\tif (!page)\n\t\treturn NULL;\n\n\t/*\n\t * Now split the huge page and free the excess pages\n\t */\n\tsplit_page(page, order);\n\tfor (p = page + (size >> PAGE_SHIFT), e = page + (1 << order); p < e; p++)\n\t\t__free_page(p);\n\n\t__dma_clear_buffer(page, size);\n\n\treturn page;\n}\n\n/*\n * Free a DMA buffer.  'size' must be page aligned.\n */\nstatic void __dma_free_buffer(struct page *page, size_t size)\n{\n\tstruct page *e = page + (size >> PAGE_SHIFT);\n\n\twhile (page < e) {\n\t\t__free_page(page);\n\t\tpage++;\n\t}\n}\n\n#ifdef CONFIG_MMU\n#ifdef CONFIG_HUGETLB_PAGE\n#warning ARM Coherent DMA allocator does not (yet) support huge TLB\n#endif\n\nstatic void *__alloc_from_contiguous(struct device *dev, size_t size,\n\t\t\t\t     pgprot_t prot, struct page **ret_page,\n\t\t\t\t     const void *caller);\n\nstatic void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t pgprot_t prot, struct page **ret_page,\n\t\t\t\t const void *caller);\n\nstatic void *\n__dma_alloc_remap(struct page *page, size_t size, gfp_t gfp, pgprot_t prot,\n\tconst void *caller)\n{\n\tstruct vm_struct *area;\n\tunsigned long addr;\n\n\t/*\n\t * DMA allocation can be mapped to user space, so lets\n\t * set VM_USERMAP flags too.\n\t */\n\tarea = get_vm_area_caller(size, VM_ARM_DMA_CONSISTENT | VM_USERMAP,\n\t\t\t\t  caller);\n\tif (!area)\n\t\treturn NULL;\n\taddr = (unsigned long)area->addr;\n\tarea->phys_addr = __pfn_to_phys(page_to_pfn(page));\n\n\tif (ioremap_page_range(addr, addr + size, area->phys_addr, prot)) {\n\t\tvunmap((void *)addr);\n\t\treturn NULL;\n\t}\n\treturn (void *)addr;\n}\n\nstatic void __dma_free_remap(void *cpu_addr, size_t size)\n{\n\tunsigned int flags = VM_ARM_DMA_CONSISTENT | VM_USERMAP;\n\tstruct vm_struct *area = find_vm_area(cpu_addr);\n\tif (!area || (area->flags & flags) != flags) {\n\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);\n\t\treturn;\n\t}\n\tunmap_kernel_range((unsigned long)cpu_addr, size);\n\tvunmap(cpu_addr);\n}\n\n#define DEFAULT_DMA_COHERENT_POOL_SIZE\tSZ_256K\n\nstruct dma_pool {\n\tsize_t size;\n\tspinlock_t lock;\n\tunsigned long *bitmap;\n\tunsigned long nr_pages;\n\tvoid *vaddr;\n\tstruct page **pages;\n};\n\nstatic struct dma_pool atomic_pool = {\n\t.size = DEFAULT_DMA_COHERENT_POOL_SIZE,\n};\n\nstatic int __init early_coherent_pool(char *p)\n{\n\tatomic_pool.size = memparse(p, &p);\n\treturn 0;\n}\nearly_param(\"coherent_pool\", early_coherent_pool);\n\nvoid __init init_dma_coherent_pool_size(unsigned long size)\n{\n\t/*\n\t * Catch any attempt to set the pool size too late.\n\t */\n\tBUG_ON(atomic_pool.vaddr);\n\n\t/*\n\t * Set architecture specific coherent pool size only if\n\t * it has not been changed by kernel command line parameter.\n\t */\n\tif (atomic_pool.size == DEFAULT_DMA_COHERENT_POOL_SIZE)\n\t\tatomic_pool.size = size;\n}\n\n/*\n * Initialise the coherent pool for atomic allocations.\n */\nstatic int __init atomic_pool_init(void)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tpgprot_t prot = pgprot_dmacoherent(pgprot_kernel);\n\tgfp_t gfp = GFP_KERNEL | GFP_DMA;\n\tunsigned long nr_pages = pool->size >> PAGE_SHIFT;\n\tunsigned long *bitmap;\n\tstruct page *page;\n\tstruct page **pages;\n\tvoid *ptr;\n\tint bitmap_size = BITS_TO_LONGS(nr_pages) * sizeof(long);\n\n\tbitmap = kzalloc(bitmap_size, GFP_KERNEL);\n\tif (!bitmap)\n\t\tgoto no_bitmap;\n\n\tpages = kzalloc(nr_pages * sizeof(struct page *), GFP_KERNEL);\n\tif (!pages)\n\t\tgoto no_pages;\n\n\tif (IS_ENABLED(CONFIG_DMA_CMA))\n\t\tptr = __alloc_from_contiguous(NULL, pool->size, prot, &page,\n\t\t\t\t\t      atomic_pool_init);\n\telse\n\t\tptr = __alloc_remap_buffer(NULL, pool->size, gfp, prot, &page,\n\t\t\t\t\t   atomic_pool_init);\n\tif (ptr) {\n\t\tint i;\n\n\t\tfor (i = 0; i < nr_pages; i++)\n\t\t\tpages[i] = page + i;\n\n\t\tspin_lock_init(&pool->lock);\n\t\tpool->vaddr = ptr;\n\t\tpool->pages = pages;\n\t\tpool->bitmap = bitmap;\n\t\tpool->nr_pages = nr_pages;\n\t\tpr_info(\"DMA: preallocated %u KiB pool for atomic coherent allocations\\n\",\n\t\t       (unsigned)pool->size / 1024);\n\t\treturn 0;\n\t}\n\n\tkfree(pages);\nno_pages:\n\tkfree(bitmap);\nno_bitmap:\n\tpr_err(\"DMA: failed to allocate %u KiB pool for atomic coherent allocation\\n\",\n\t       (unsigned)pool->size / 1024);\n\treturn -ENOMEM;\n}\n/*\n * CMA is activated by core_initcall, so we must be called after it.\n */\npostcore_initcall(atomic_pool_init);\n\nstruct dma_contig_early_reserve {\n\tphys_addr_t base;\n\tunsigned long size;\n};\n\nstatic struct dma_contig_early_reserve dma_mmu_remap[MAX_CMA_AREAS] __initdata;\n\nstatic int dma_mmu_remap_num __initdata;\n\nvoid __init dma_contiguous_early_fixup(phys_addr_t base, unsigned long size)\n{\n\tdma_mmu_remap[dma_mmu_remap_num].base = base;\n\tdma_mmu_remap[dma_mmu_remap_num].size = size;\n\tdma_mmu_remap_num++;\n}\n\nvoid __init dma_contiguous_remap(void)\n{\n\tint i;\n\tfor (i = 0; i < dma_mmu_remap_num; i++) {\n\t\tphys_addr_t start = dma_mmu_remap[i].base;\n\t\tphys_addr_t end = start + dma_mmu_remap[i].size;\n\t\tstruct map_desc map;\n\t\tunsigned long addr;\n\n\t\tif (end > arm_lowmem_limit)\n\t\t\tend = arm_lowmem_limit;\n\t\tif (start >= end)\n\t\t\tcontinue;\n\n\t\tmap.pfn = __phys_to_pfn(start);\n\t\tmap.virtual = __phys_to_virt(start);\n\t\tmap.length = end - start;\n\t\tmap.type = MT_MEMORY_DMA_READY;\n\n\t\t/*\n\t\t * Clear previous low-memory mapping\n\t\t */\n\t\tfor (addr = __phys_to_virt(start); addr < __phys_to_virt(end);\n\t\t     addr += PMD_SIZE)\n\t\t\tpmd_clear(pmd_off_k(addr));\n\n\t\tiotable_init(&map, 1);\n\t}\n}\n\nstatic int __dma_update_pte(pte_t *pte, pgtable_t token, unsigned long addr,\n\t\t\t    void *data)\n{\n\tstruct page *page = virt_to_page(addr);\n\tpgprot_t prot = *(pgprot_t *)data;\n\n\tset_pte_ext(pte, mk_pte(page, prot), 0);\n\treturn 0;\n}\n\nstatic void __dma_remap(struct page *page, size_t size, pgprot_t prot)\n{\n\tunsigned long start = (unsigned long) page_address(page);\n\tunsigned end = start + size;\n\n\tapply_to_page_range(&init_mm, start, size, __dma_update_pte, &prot);\n\tflush_tlb_kernel_range(start, end);\n}\n\nstatic void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t pgprot_t prot, struct page **ret_page,\n\t\t\t\t const void *caller)\n{\n\tstruct page *page;\n\tvoid *ptr;\n\tpage = __dma_alloc_buffer(dev, size, gfp);\n\tif (!page)\n\t\treturn NULL;\n\n\tptr = __dma_alloc_remap(page, size, gfp, prot, caller);\n\tif (!ptr) {\n\t\t__dma_free_buffer(page, size);\n\t\treturn NULL;\n\t}\n\n\t*ret_page = page;\n\treturn ptr;\n}\n\nstatic void *__alloc_from_pool(size_t size, struct page **ret_page)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned int pageno;\n\tunsigned long flags;\n\tvoid *ptr = NULL;\n\tunsigned long align_mask;\n\n\tif (!pool->vaddr) {\n\t\tWARN(1, \"coherent pool not initialised!\\n\");\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * Align the region allocation - allocations from pool are rather\n\t * small, so align them to their order in pages, minimum is a page\n\t * size. This helps reduce fragmentation of the DMA space.\n\t */\n\talign_mask = (1 << get_order(size)) - 1;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tpageno = bitmap_find_next_zero_area(pool->bitmap, pool->nr_pages,\n\t\t\t\t\t    0, count, align_mask);\n\tif (pageno < pool->nr_pages) {\n\t\tbitmap_set(pool->bitmap, pageno, count);\n\t\tptr = pool->vaddr + PAGE_SIZE * pageno;\n\t\t*ret_page = pool->pages[pageno];\n\t} else {\n\t\tpr_err_once(\"ERROR: %u KiB atomic DMA coherent pool is too small!\\n\"\n\t\t\t    \"Please increase it with coherent_pool= kernel parameter!\\n\",\n\t\t\t    (unsigned)pool->size / 1024);\n\t}\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\treturn ptr;\n}\n\nstatic bool __in_atomic_pool(void *start, size_t size)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tvoid *end = start + size;\n\tvoid *pool_start = pool->vaddr;\n\tvoid *pool_end = pool->vaddr + pool->size;\n\n\tif (start < pool_start || start >= pool_end)\n\t\treturn false;\n\n\tif (end <= pool_end)\n\t\treturn true;\n\n\tWARN(1, \"Wrong coherent size(%p-%p) from atomic pool(%p-%p)\\n\",\n\t     start, end - 1, pool_start, pool_end - 1);\n\n\treturn false;\n}\n\nstatic int __free_from_pool(void *start, size_t size)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tunsigned long pageno, count;\n\tunsigned long flags;\n\n\tif (!__in_atomic_pool(start, size))\n\t\treturn 0;\n\n\tpageno = (start - pool->vaddr) >> PAGE_SHIFT;\n\tcount = size >> PAGE_SHIFT;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tbitmap_clear(pool->bitmap, pageno, count);\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\treturn 1;\n}\n\nstatic void *__alloc_from_contiguous(struct device *dev, size_t size,\n\t\t\t\t     pgprot_t prot, struct page **ret_page,\n\t\t\t\t     const void *caller)\n{\n\tunsigned long order = get_order(size);\n\tsize_t count = size >> PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tpage = dma_alloc_from_contiguous(dev, count, order);\n\tif (!page)\n\t\treturn NULL;\n\n\t__dma_clear_buffer(page, size);\n\n\tif (PageHighMem(page)) {\n\t\tptr = __dma_alloc_remap(page, size, GFP_KERNEL, prot, caller);\n\t\tif (!ptr) {\n\t\t\tdma_release_from_contiguous(dev, page, count);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\t__dma_remap(page, size, prot);\n\t\tptr = page_address(page);\n\t}\n\t*ret_page = page;\n\treturn ptr;\n}\n\nstatic void __free_from_contiguous(struct device *dev, struct page *page,\n\t\t\t\t   void *cpu_addr, size_t size)\n{\n\tif (PageHighMem(page))\n\t\t__dma_free_remap(cpu_addr, size);\n\telse\n\t\t__dma_remap(page, size, pgprot_kernel);\n\tdma_release_from_contiguous(dev, page, size >> PAGE_SHIFT);\n}\n\nstatic inline pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot)\n{\n\tprot = dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs) ?\n\t\t\t    pgprot_writecombine(prot) :\n\t\t\t    pgprot_dmacoherent(prot);\n\treturn prot;\n}\n\n#define nommu() 0\n\n#else\t/* !CONFIG_MMU */\n\n#define nommu() 1\n\n#define __get_dma_pgprot(attrs, prot)\t__pgprot(0)\n#define __alloc_remap_buffer(dev, size, gfp, prot, ret, c)\tNULL\n#define __alloc_from_pool(size, ret_page)\t\t\tNULL\n#define __alloc_from_contiguous(dev, size, prot, ret, c)\tNULL\n#define __free_from_pool(cpu_addr, size)\t\t\t0\n#define __free_from_contiguous(dev, page, cpu_addr, size)\tdo { } while (0)\n#define __dma_free_remap(cpu_addr, size)\t\t\tdo { } while (0)\n\n#endif\t/* CONFIG_MMU */\n\nstatic void *__alloc_simple_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t   struct page **ret_page)\n{\n\tstruct page *page;\n\tpage = __dma_alloc_buffer(dev, size, gfp);\n\tif (!page)\n\t\treturn NULL;\n\n\t*ret_page = page;\n\treturn page_address(page);\n}\n\n\n\nstatic void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t\t gfp_t gfp, pgprot_t prot, bool is_coherent, const void *caller)\n{\n\tu64 mask = get_coherent_dma_mask(dev);\n\tstruct page *page = NULL;\n\tvoid *addr;\n\n#ifdef CONFIG_DMA_API_DEBUG\n\tu64 limit = (mask + 1) & ~mask;\n\tif (limit && size >= limit) {\n\t\tdev_warn(dev, \"coherent allocation too big (requested %#x mask %#llx)\\n\",\n\t\t\tsize, mask);\n\t\treturn NULL;\n\t}\n#endif\n\n\tif (!mask)\n\t\treturn NULL;\n\n\tif (mask < 0xffffffffULL)\n\t\tgfp |= GFP_DMA;\n\n\t/*\n\t * Following is a work-around (a.k.a. hack) to prevent pages\n\t * with __GFP_COMP being passed to split_page() which cannot\n\t * handle them.  The real problem is that this flag probably\n\t * should be 0 on ARM as it is not supported on this\n\t * platform; see CONFIG_HUGETLBFS.\n\t */\n\tgfp &= ~(__GFP_COMP);\n\n\t*handle = DMA_ERROR_CODE;\n\tsize = PAGE_ALIGN(size);\n\n\tif (is_coherent || nommu())\n\t\taddr = __alloc_simple_buffer(dev, size, gfp, &page);\n\telse if (!(gfp & __GFP_WAIT))\n\t\taddr = __alloc_from_pool(size, &page);\n\telse if (!IS_ENABLED(CONFIG_DMA_CMA))\n\t\taddr = __alloc_remap_buffer(dev, size, gfp, prot, &page, caller);\n\telse\n\t\taddr = __alloc_from_contiguous(dev, size, prot, &page, caller);\n\n\tif (addr)\n\t\t*handle = pfn_to_dma(dev, page_to_pfn(page));\n\n\treturn addr;\n}\n\n/*\n * Allocate DMA-coherent memory space and return both the kernel remapped\n * virtual and bus address for that space.\n */\nvoid *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t    gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, false,\n\t\t\t   __builtin_return_address(0));\n}\n\nstatic void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n\tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, true,\n\t\t\t   __builtin_return_address(0));\n}\n\n/*\n * Create userspace mapping for the DMA-coherent memory.\n */\nint arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,\n\t\t void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t struct dma_attrs *attrs)\n{\n\tint ret = -ENXIO;\n#ifdef CONFIG_MMU\n\tunsigned long nr_vma_pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n\tunsigned long nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned long pfn = dma_to_pfn(dev, dma_addr);\n\tunsigned long off = vma->vm_pgoff;\n\n\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);\n\n\tif (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &ret))\n\t\treturn ret;\n\n\tif (off < nr_pages && nr_vma_pages <= (nr_pages - off)) {\n\t\tret = remap_pfn_range(vma, vma->vm_start,\n\t\t\t\t      pfn + off,\n\t\t\t\t      vma->vm_end - vma->vm_start,\n\t\t\t\t      vma->vm_page_prot);\n\t}\n#endif\t/* CONFIG_MMU */\n\n\treturn ret;\n}\n\n/*\n * Free a buffer as defined by the above mapping.\n */\nstatic void __arm_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t   dma_addr_t handle, struct dma_attrs *attrs,\n\t\t\t   bool is_coherent)\n{\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle));\n\n\tif (dma_release_from_coherent(dev, get_order(size), cpu_addr))\n\t\treturn;\n\n\tsize = PAGE_ALIGN(size);\n\n\tif (is_coherent || nommu()) {\n\t\t__dma_free_buffer(page, size);\n\t} else if (__free_from_pool(cpu_addr, size)) {\n\t\treturn;\n\t} else if (!IS_ENABLED(CONFIG_DMA_CMA)) {\n\t\t__dma_free_remap(cpu_addr, size);\n\t\t__dma_free_buffer(page, size);\n\t} else {\n\t\t/*\n\t\t * Non-atomic allocations cannot be freed with IRQs disabled\n\t\t */\n\t\tWARN_ON(irqs_disabled());\n\t\t__free_from_contiguous(dev, page, cpu_addr, size);\n\t}\n}\n\nvoid arm_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\t__arm_dma_free(dev, size, cpu_addr, handle, attrs, false);\n}\n\nstatic void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\t__arm_dma_free(dev, size, cpu_addr, handle, attrs, true);\n}\n\nint arm_dma_get_sgtable(struct device *dev, struct sg_table *sgt,\n\t\t void *cpu_addr, dma_addr_t handle, size_t size,\n\t\t struct dma_attrs *attrs)\n{\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle));\n\tint ret;\n\n\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);\n\treturn 0;\n}\n\nstatic void dma_cache_maint_page(struct page *page, unsigned long offset,\n\tsize_t size, enum dma_data_direction dir,\n\tvoid (*op)(const void *, size_t, int))\n{\n\tunsigned long pfn;\n\tsize_t left = size;\n\n\tpfn = page_to_pfn(page) + offset / PAGE_SIZE;\n\toffset %= PAGE_SIZE;\n\n\t/*\n\t * A single sg entry may refer to multiple physically contiguous\n\t * pages.  But we still need to process highmem pages individually.\n\t * If highmem is not configured then the bulk of this loop gets\n\t * optimized out.\n\t */\n\tdo {\n\t\tsize_t len = left;\n\t\tvoid *vaddr;\n\n\t\tpage = pfn_to_page(pfn);\n\n\t\tif (PageHighMem(page)) {\n\t\t\tif (len + offset > PAGE_SIZE)\n\t\t\t\tlen = PAGE_SIZE - offset;\n\n\t\t\tif (cache_is_vipt_nonaliasing()) {\n\t\t\t\tvaddr = kmap_atomic(page);\n\t\t\t\top(vaddr + offset, len, dir);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t} else {\n\t\t\t\tvaddr = kmap_high_get(page);\n\t\t\t\tif (vaddr) {\n\t\t\t\t\top(vaddr + offset, len, dir);\n\t\t\t\t\tkunmap_high(page);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tvaddr = page_address(page) + offset;\n\t\t\top(vaddr, len, dir);\n\t\t}\n\t\toffset = 0;\n\t\tpfn++;\n\t\tleft -= len;\n\t} while (left);\n}\n\n/*\n * Make an area consistent for devices.\n * Note: Drivers should NOT use this function directly, as it will break\n * platforms with CONFIG_DMABOUNCE.\n * Use the driver DMA support - see dma-mapping.h (dma_sync_*)\n */\nstatic void __dma_page_cpu_to_dev(struct page *page, unsigned long off,\n\tsize_t size, enum dma_data_direction dir)\n{\n\tunsigned long paddr;\n\n\tdma_cache_maint_page(page, off, size, dir, dmac_map_area);\n\n\tpaddr = page_to_phys(page) + off;\n\tif (dir == DMA_FROM_DEVICE) {\n\t\touter_inv_range(paddr, paddr + size);\n\t} else {\n\t\touter_clean_range(paddr, paddr + size);\n\t}\n\t/* FIXME: non-speculating: flush on bidirectional mappings? */\n}\n\nstatic void __dma_page_dev_to_cpu(struct page *page, unsigned long off,\n\tsize_t size, enum dma_data_direction dir)\n{\n\tunsigned long paddr = page_to_phys(page) + off;\n\n\t/* FIXME: non-speculating: not required */\n\t/* don't bother invalidating if DMA to device */\n\tif (dir != DMA_TO_DEVICE)\n\t\touter_inv_range(paddr, paddr + size);\n\n\tdma_cache_maint_page(page, off, size, dir, dmac_unmap_area);\n\n\t/*\n\t * Mark the D-cache clean for these pages to avoid extra flushing.\n\t */\n\tif (dir != DMA_TO_DEVICE && size >= PAGE_SIZE) {\n\t\tunsigned long pfn;\n\t\tsize_t left = size;\n\n\t\tpfn = page_to_pfn(page) + off / PAGE_SIZE;\n\t\toff %= PAGE_SIZE;\n\t\tif (off) {\n\t\t\tpfn++;\n\t\t\tleft -= PAGE_SIZE - off;\n\t\t}\n\t\twhile (left >= PAGE_SIZE) {\n\t\t\tpage = pfn_to_page(pfn++);\n\t\t\tset_bit(PG_dcache_clean, &page->flags);\n\t\t\tleft -= PAGE_SIZE;\n\t\t}\n\t}\n}\n\n/**\n * arm_dma_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of buffers described by scatterlist in streaming mode for DMA.\n * This is the scatter-gather version of the dma_map_single interface.\n * Here the scatter gather list elements are each tagged with the\n * appropriate dma address and length.  They are obtained via\n * sg_dma_{address,length}.\n *\n * Device ownership issues as mentioned for dma_map_single are the same\n * here.\n */\nint arm_dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i, j;\n\n\tfor_each_sg(sg, s, nents, i) {\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\t\ts->dma_length = s->length;\n#endif\n\t\ts->dma_address = ops->map_page(dev, sg_page(s), s->offset,\n\t\t\t\t\t\ts->length, dir, attrs);\n\t\tif (dma_mapping_error(dev, s->dma_address))\n\t\t\tgoto bad_mapping;\n\t}\n\treturn nents;\n\n bad_mapping:\n\tfor_each_sg(sg, s, i, j)\n\t\tops->unmap_page(dev, sg_dma_address(s), sg_dma_len(s), dir, attrs);\n\treturn 0;\n}\n\n/**\n * arm_dma_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->unmap_page(dev, sg_dma_address(s), sg_dma_len(s), dir, attrs);\n}\n\n/**\n * arm_dma_sync_sg_for_cpu\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->sync_single_for_cpu(dev, sg_dma_address(s), s->length,\n\t\t\t\t\t dir);\n}\n\n/**\n * arm_dma_sync_sg_for_device\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->sync_single_for_device(dev, sg_dma_address(s), s->length,\n\t\t\t\t\t    dir);\n}\n\n/*\n * Return whether the given device DMA address mask can be supported\n * properly.  For example, if your device can only drive the low 24-bits\n * during bus mastering, then you would pass 0x00ffffff as the mask\n * to this function.\n */\nint dma_supported(struct device *dev, u64 mask)\n{\n\tif (mask < (u64)arm_dma_limit)\n\t\treturn 0;\n\treturn 1;\n}\nEXPORT_SYMBOL(dma_supported);\n\nint arm_dma_set_mask(struct device *dev, u64 dma_mask)\n{\n\tif (!dev->dma_mask || !dma_supported(dev, dma_mask))\n\t\treturn -EIO;\n\n\t*dev->dma_mask = dma_mask;\n\n\treturn 0;\n}\n\n#define PREALLOC_DMA_DEBUG_ENTRIES\t4096\n\nstatic int __init dma_debug_do_init(void)\n{\n\tdma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);\n\treturn 0;\n}\nfs_initcall(dma_debug_do_init);\n\n#ifdef CONFIG_ARM_DMA_USE_IOMMU\n\n/* IOMMU */\n\nstatic inline dma_addr_t __alloc_iova(struct dma_iommu_mapping *mapping,\n\t\t\t\t      size_t size)\n{\n\tunsigned int order = get_order(size);\n\tunsigned int align = 0;\n\tunsigned int count, start;\n\tunsigned long flags;\n\n\tif (order > CONFIG_ARM_DMA_IOMMU_ALIGNMENT)\n\t\torder = CONFIG_ARM_DMA_IOMMU_ALIGNMENT;\n\n\tcount = ((PAGE_ALIGN(size) >> PAGE_SHIFT) +\n\t\t (1 << mapping->order) - 1) >> mapping->order;\n\n\tif (order > mapping->order)\n\t\talign = (1 << (order - mapping->order)) - 1;\n\n\tspin_lock_irqsave(&mapping->lock, flags);\n\tstart = bitmap_find_next_zero_area(mapping->bitmap, mapping->bits, 0,\n\t\t\t\t\t   count, align);\n\tif (start > mapping->bits) {\n\t\tspin_unlock_irqrestore(&mapping->lock, flags);\n\t\treturn DMA_ERROR_CODE;\n\t}\n\n\tbitmap_set(mapping->bitmap, start, count);\n\tspin_unlock_irqrestore(&mapping->lock, flags);\n\n\treturn mapping->base + (start << (mapping->order + PAGE_SHIFT));\n}\n\nstatic inline void __free_iova(struct dma_iommu_mapping *mapping,\n\t\t\t       dma_addr_t addr, size_t size)\n{\n\tunsigned int start = (addr - mapping->base) >>\n\t\t\t     (mapping->order + PAGE_SHIFT);\n\tunsigned int count = ((size >> PAGE_SHIFT) +\n\t\t\t      (1 << mapping->order) - 1) >> mapping->order;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mapping->lock, flags);\n\tbitmap_clear(mapping->bitmap, start, count);\n\tspin_unlock_irqrestore(&mapping->lock, flags);\n}\n\nstatic struct page **__iommu_alloc_buffer(struct device *dev, size_t size,\n\t\t\t\t\t  gfp_t gfp, struct dma_attrs *attrs)\n{\n\tstruct page **pages;\n\tint count = size >> PAGE_SHIFT;\n\tint array_size = count * sizeof(struct page *);\n\tint i = 0;\n\n\tif (array_size <= PAGE_SIZE)\n\t\tpages = kzalloc(array_size, gfp);\n\telse\n\t\tpages = vzalloc(array_size);\n\tif (!pages)\n\t\treturn NULL;\n\n\tif (dma_get_attr(DMA_ATTR_FORCE_CONTIGUOUS, attrs))\n\t{\n\t\tunsigned long order = get_order(size);\n\t\tstruct page *page;\n\n\t\tpage = dma_alloc_from_contiguous(dev, count, order);\n\t\tif (!page)\n\t\t\tgoto error;\n\n\t\t__dma_clear_buffer(page, size);\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tpages[i] = page + i;\n\n\t\treturn pages;\n\t}\n\n\t/*\n\t * IOMMU can map any pages, so himem can also be used here\n\t */\n\tgfp |= __GFP_NOWARN | __GFP_HIGHMEM;\n\n\twhile (count) {\n\t\tint j, order = __fls(count);\n\n\t\tpages[i] = alloc_pages(gfp, order);\n\t\twhile (!pages[i] && order)\n\t\t\tpages[i] = alloc_pages(gfp, --order);\n\t\tif (!pages[i])\n\t\t\tgoto error;\n\n\t\tif (order) {\n\t\t\tsplit_page(pages[i], order);\n\t\t\tj = 1 << order;\n\t\t\twhile (--j)\n\t\t\t\tpages[i + j] = pages[i] + j;\n\t\t}\n\n\t\t__dma_clear_buffer(pages[i], PAGE_SIZE << order);\n\t\ti += 1 << order;\n\t\tcount -= 1 << order;\n\t}\n\n\treturn pages;\nerror:\n\twhile (i--)\n\t\tif (pages[i])\n\t\t\t__free_pages(pages[i], 0);\n\tif (array_size <= PAGE_SIZE)\n\t\tkfree(pages);\n\telse\n\t\tvfree(pages);\n\treturn NULL;\n}\n\nstatic int __iommu_free_buffer(struct device *dev, struct page **pages,\n\t\t\t       size_t size, struct dma_attrs *attrs)\n{\n\tint count = size >> PAGE_SHIFT;\n\tint array_size = count * sizeof(struct page *);\n\tint i;\n\n\tif (dma_get_attr(DMA_ATTR_FORCE_CONTIGUOUS, attrs)) {\n\t\tdma_release_from_contiguous(dev, pages[0], count);\n\t} else {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tif (pages[i])\n\t\t\t\t__free_pages(pages[i], 0);\n\t}\n\n\tif (array_size <= PAGE_SIZE)\n\t\tkfree(pages);\n\telse\n\t\tvfree(pages);\n\treturn 0;\n}\n\n/*\n * Create a CPU mapping for a specified pages\n */\nstatic void *\n__iommu_alloc_remap(struct page **pages, size_t size, gfp_t gfp, pgprot_t prot,\n\t\t    const void *caller)\n{\n\tunsigned int i, nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tstruct vm_struct *area;\n\tunsigned long p;\n\n\tarea = get_vm_area_caller(size, VM_ARM_DMA_CONSISTENT | VM_USERMAP,\n\t\t\t\t  caller);\n\tif (!area)\n\t\treturn NULL;\n\n\tarea->pages = pages;\n\tarea->nr_pages = nr_pages;\n\tp = (unsigned long)area->addr;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tphys_addr_t phys = __pfn_to_phys(page_to_pfn(pages[i]));\n\t\tif (ioremap_page_range(p, p + PAGE_SIZE, phys, prot))\n\t\t\tgoto err;\n\t\tp += PAGE_SIZE;\n\t}\n\treturn area->addr;\nerr:\n\tunmap_kernel_range((unsigned long)area->addr, size);\n\tvunmap(area->addr);\n\treturn NULL;\n}\n\n/*\n * Create a mapping in device IO address space for specified pages\n */\nstatic dma_addr_t\n__iommu_create_mapping(struct device *dev, struct page **pages, size_t size)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tdma_addr_t dma_addr, iova;\n\tint i, ret = DMA_ERROR_CODE;\n\n\tdma_addr = __alloc_iova(mapping, size);\n\tif (dma_addr == DMA_ERROR_CODE)\n\t\treturn dma_addr;\n\n\tiova = dma_addr;\n\tfor (i = 0; i < count; ) {\n\t\tunsigned int next_pfn = page_to_pfn(pages[i]) + 1;\n\t\tphys_addr_t phys = page_to_phys(pages[i]);\n\t\tunsigned int len, j;\n\n\t\tfor (j = i + 1; j < count; j++, next_pfn++)\n\t\t\tif (page_to_pfn(pages[j]) != next_pfn)\n\t\t\t\tbreak;\n\n\t\tlen = (j - i) << PAGE_SHIFT;\n\t\tret = iommu_map(mapping->domain, iova, phys, len, 0);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\t\tiova += len;\n\t\ti = j;\n\t}\n\treturn dma_addr;\nfail:\n\tiommu_unmap(mapping->domain, dma_addr, iova-dma_addr);\n\t__free_iova(mapping, dma_addr, size);\n\treturn DMA_ERROR_CODE;\n}\n\nstatic int __iommu_remove_mapping(struct device *dev, dma_addr_t iova, size_t size)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\n\t/*\n\t * add optional in-page offset from iova to size and align\n\t * result to page size\n\t */\n\tsize = PAGE_ALIGN((iova & ~PAGE_MASK) + size);\n\tiova &= PAGE_MASK;\n\n\tiommu_unmap(mapping->domain, iova, size);\n\t__free_iova(mapping, iova, size);\n\treturn 0;\n}\n\nstatic struct page **__atomic_get_pages(void *addr)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tstruct page **pages = pool->pages;\n\tint offs = (addr - pool->vaddr) >> PAGE_SHIFT;\n\n\treturn pages + offs;\n}\n\nstatic struct page **__iommu_get_pages(void *cpu_addr, struct dma_attrs *attrs)\n{\n\tstruct vm_struct *area;\n\n\tif (__in_atomic_pool(cpu_addr, PAGE_SIZE))\n\t\treturn __atomic_get_pages(cpu_addr);\n\n\tif (dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs))\n\t\treturn cpu_addr;\n\n\tarea = find_vm_area(cpu_addr);\n\tif (area && (area->flags & VM_ARM_DMA_CONSISTENT))\n\t\treturn area->pages;\n\treturn NULL;\n}\n\nstatic void *__iommu_alloc_atomic(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *handle)\n{\n\tstruct page *page;\n\tvoid *addr;\n\n\taddr = __alloc_from_pool(size, &page);\n\tif (!addr)\n\t\treturn NULL;\n\n\t*handle = __iommu_create_mapping(dev, &page, size);\n\tif (*handle == DMA_ERROR_CODE)\n\t\tgoto err_mapping;\n\n\treturn addr;\n\nerr_mapping:\n\t__free_from_pool(addr, size);\n\treturn NULL;\n}\n\nstatic void __iommu_free_atomic(struct device *dev, void *cpu_addr,\n\t\t\t\tdma_addr_t handle, size_t size)\n{\n\t__iommu_remove_mapping(dev, handle, size);\n\t__free_from_pool(cpu_addr, size);\n}\n\nstatic void *arm_iommu_alloc_attrs(struct device *dev, size_t size,\n\t    dma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n\tstruct page **pages;\n\tvoid *addr = NULL;\n\n\t*handle = DMA_ERROR_CODE;\n\tsize = PAGE_ALIGN(size);\n\n\tif (gfp & GFP_ATOMIC)\n\t\treturn __iommu_alloc_atomic(dev, size, handle);\n\n\t/*\n\t * Following is a work-around (a.k.a. hack) to prevent pages\n\t * with __GFP_COMP being passed to split_page() which cannot\n\t * handle them.  The real problem is that this flag probably\n\t * should be 0 on ARM as it is not supported on this\n\t * platform; see CONFIG_HUGETLBFS.\n\t */\n\tgfp &= ~(__GFP_COMP);\n\n\tpages = __iommu_alloc_buffer(dev, size, gfp, attrs);\n\tif (!pages)\n\t\treturn NULL;\n\n\t*handle = __iommu_create_mapping(dev, pages, size);\n\tif (*handle == DMA_ERROR_CODE)\n\t\tgoto err_buffer;\n\n\tif (dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs))\n\t\treturn pages;\n\n\taddr = __iommu_alloc_remap(pages, size, gfp, prot,\n\t\t\t\t   __builtin_return_address(0));\n\tif (!addr)\n\t\tgoto err_mapping;\n\n\treturn addr;\n\nerr_mapping:\n\t__iommu_remove_mapping(dev, *handle, size);\nerr_buffer:\n\t__iommu_free_buffer(dev, pages, size, attrs);\n\treturn NULL;\n}\n\nstatic int arm_iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,\n\t\t    void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t    struct dma_attrs *attrs)\n{\n\tunsigned long uaddr = vma->vm_start;\n\tunsigned long usize = vma->vm_end - vma->vm_start;\n\tstruct page **pages = __iommu_get_pages(cpu_addr, attrs);\n\n\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);\n\n\tif (!pages)\n\t\treturn -ENXIO;\n\n\tdo {\n\t\tint ret = vm_insert_page(vma, uaddr, *pages++);\n\t\tif (ret) {\n\t\t\tpr_err(\"Remapping memory failed: %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t\tuaddr += PAGE_SIZE;\n\t\tusize -= PAGE_SIZE;\n\t} while (usize > 0);\n\n\treturn 0;\n}\n\n/*\n * free a page as defined by the above mapping.\n * Must not be called with IRQs disabled.\n */\nvoid arm_iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\tstruct page **pages;\n\tsize = PAGE_ALIGN(size);\n\n\tif (__in_atomic_pool(cpu_addr, size)) {\n\t\t__iommu_free_atomic(dev, cpu_addr, handle, size);\n\t\treturn;\n\t}\n\n\tpages = __iommu_get_pages(cpu_addr, attrs);\n\tif (!pages) {\n\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);\n\t\treturn;\n\t}\n\n\tif (!dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs)) {\n\t\tunmap_kernel_range((unsigned long)cpu_addr, size);\n\t\tvunmap(cpu_addr);\n\t}\n\n\t__iommu_remove_mapping(dev, handle, size);\n\t__iommu_free_buffer(dev, pages, size, attrs);\n}\n\nstatic int arm_iommu_get_sgtable(struct device *dev, struct sg_table *sgt,\n\t\t\t\t void *cpu_addr, dma_addr_t dma_addr,\n\t\t\t\t size_t size, struct dma_attrs *attrs)\n{\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tstruct page **pages = __iommu_get_pages(cpu_addr, attrs);\n\n\tif (!pages)\n\t\treturn -ENXIO;\n\n\treturn sg_alloc_table_from_pages(sgt, pages, count, 0, size,\n\t\t\t\t\t GFP_KERNEL);\n}\n\n/*\n * Map a part of the scatter-gather list into contiguous io address space\n */\nstatic int __map_sg_chunk(struct device *dev, struct scatterlist *sg,\n\t\t\t  size_t size, dma_addr_t *handle,\n\t\t\t  enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\t\t  bool is_coherent)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova, iova_base;\n\tint ret = 0;\n\tunsigned int count;\n\tstruct scatterlist *s;\n\n\tsize = PAGE_ALIGN(size);\n\t*handle = DMA_ERROR_CODE;\n\n\tiova_base = iova = __alloc_iova(mapping, size);\n\tif (iova == DMA_ERROR_CODE)\n\t\treturn -ENOMEM;\n\n\tfor (count = 0, s = sg; count < (size >> PAGE_SHIFT); s = sg_next(s)) {\n\t\tphys_addr_t phys = page_to_phys(sg_page(s));\n\t\tunsigned int len = PAGE_ALIGN(s->offset + s->length);\n\n\t\tif (!is_coherent &&\n\t\t\t!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t\t__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);\n\n\t\tret = iommu_map(mapping->domain, iova, phys, len, 0);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\t\tcount += len >> PAGE_SHIFT;\n\t\tiova += len;\n\t}\n\t*handle = iova_base;\n\n\treturn 0;\nfail:\n\tiommu_unmap(mapping->domain, iova_base, count * PAGE_SIZE);\n\t__free_iova(mapping, iova_base, size);\n\treturn ret;\n}\n\nstatic int __iommu_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\t     enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\t     bool is_coherent)\n{\n\tstruct scatterlist *s = sg, *dma = sg, *start = sg;\n\tint i, count = 0;\n\tunsigned int offset = s->offset;\n\tunsigned int size = s->offset + s->length;\n\tunsigned int max = dma_get_max_seg_size(dev);\n\n\tfor (i = 1; i < nents; i++) {\n\t\ts = sg_next(s);\n\n\t\ts->dma_address = DMA_ERROR_CODE;\n\t\ts->dma_length = 0;\n\n\t\tif (s->offset || (size & ~PAGE_MASK) || size + s->length > max) {\n\t\t\tif (__map_sg_chunk(dev, start, size, &dma->dma_address,\n\t\t\t    dir, attrs, is_coherent) < 0)\n\t\t\t\tgoto bad_mapping;\n\n\t\t\tdma->dma_address += offset;\n\t\t\tdma->dma_length = size - offset;\n\n\t\t\tsize = offset = s->offset;\n\t\t\tstart = s;\n\t\t\tdma = sg_next(dma);\n\t\t\tcount += 1;\n\t\t}\n\t\tsize += s->length;\n\t}\n\tif (__map_sg_chunk(dev, start, size, &dma->dma_address, dir, attrs,\n\t\tis_coherent) < 0)\n\t\tgoto bad_mapping;\n\n\tdma->dma_address += offset;\n\tdma->dma_length = size - offset;\n\n\treturn count+1;\n\nbad_mapping:\n\tfor_each_sg(sg, s, count, i)\n\t\t__iommu_remove_mapping(dev, sg_dma_address(s), sg_dma_len(s));\n\treturn 0;\n}\n\n/**\n * arm_coherent_iommu_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of i/o coherent buffers described by scatterlist in streaming\n * mode for DMA. The scatter gather list elements are merged together (if\n * possible) and tagged with the appropriate dma address and length. They are\n * obtained via sg_dma_{address,length}.\n */\nint arm_coherent_iommu_map_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\treturn __iommu_map_sg(dev, sg, nents, dir, attrs, true);\n}\n\n/**\n * arm_iommu_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of buffers described by scatterlist in streaming mode for DMA.\n * The scatter gather list elements are merged together (if possible) and\n * tagged with the appropriate dma address and length. They are obtained via\n * sg_dma_{address,length}.\n */\nint arm_iommu_map_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\treturn __iommu_map_sg(dev, sg, nents, dir, attrs, false);\n}\n\nstatic void __iommu_unmap_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\tbool is_coherent)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tif (sg_dma_len(s))\n\t\t\t__iommu_remove_mapping(dev, sg_dma_address(s),\n\t\t\t\t\t       sg_dma_len(s));\n\t\tif (!is_coherent &&\n\t\t    !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t\t__dma_page_dev_to_cpu(sg_page(s), s->offset,\n\t\t\t\t\t      s->length, dir);\n\t}\n}\n\n/**\n * arm_coherent_iommu_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_coherent_iommu_unmap_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\t__iommu_unmap_sg(dev, sg, nents, dir, attrs, true);\n}\n\n/**\n * arm_iommu_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_iommu_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\t__iommu_unmap_sg(dev, sg, nents, dir, attrs, false);\n}\n\n/**\n * arm_iommu_sync_sg_for_cpu\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_iommu_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\t__dma_page_dev_to_cpu(sg_page(s), s->offset, s->length, dir);\n\n}\n\n/**\n * arm_iommu_sync_sg_for_device\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_iommu_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\t__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);\n}\n\n\n/**\n * arm_coherent_iommu_map_page\n * @dev: valid struct device pointer\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * Coherent IOMMU aware version of arm_dma_map_page()\n */\nstatic dma_addr_t arm_coherent_iommu_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t dma_addr;\n\tint ret, prot, len = PAGE_ALIGN(size + offset);\n\n\tdma_addr = __alloc_iova(mapping, len);\n\tif (dma_addr == DMA_ERROR_CODE)\n\t\treturn dma_addr;\n\n\tswitch (dir) {\n\tcase DMA_BIDIRECTIONAL:\n\t\tprot = IOMMU_READ | IOMMU_WRITE;\n\t\tbreak;\n\tcase DMA_TO_DEVICE:\n\t\tprot = IOMMU_READ;\n\t\tbreak;\n\tcase DMA_FROM_DEVICE:\n\t\tprot = IOMMU_WRITE;\n\t\tbreak;\n\tdefault:\n\t\tprot = 0;\n\t}\n\n\tret = iommu_map(mapping->domain, dma_addr, page_to_phys(page), len, prot);\n\tif (ret < 0)\n\t\tgoto fail;\n\n\treturn dma_addr + offset;\nfail:\n\t__free_iova(mapping, dma_addr, len);\n\treturn DMA_ERROR_CODE;\n}\n\n/**\n * arm_iommu_map_page\n * @dev: valid struct device pointer\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * IOMMU aware version of arm_dma_map_page()\n */\nstatic dma_addr_t arm_iommu_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_cpu_to_dev(page, offset, size, dir);\n\n\treturn arm_coherent_iommu_map_page(dev, page, offset, size, dir, attrs);\n}\n\n/**\n * arm_coherent_iommu_unmap_page\n * @dev: valid struct device pointer\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * Coherent IOMMU aware version of arm_dma_unmap_page()\n */\nstatic void arm_coherent_iommu_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tint offset = handle & ~PAGE_MASK;\n\tint len = PAGE_ALIGN(size + offset);\n\n\tif (!iova)\n\t\treturn;\n\n\tiommu_unmap(mapping->domain, iova, len);\n\t__free_iova(mapping, iova, len);\n}\n\n/**\n * arm_iommu_unmap_page\n * @dev: valid struct device pointer\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * IOMMU aware version of arm_dma_unmap_page()\n */\nstatic void arm_iommu_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tint offset = handle & ~PAGE_MASK;\n\tint len = PAGE_ALIGN(size + offset);\n\n\tif (!iova)\n\t\treturn;\n\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_dev_to_cpu(page, offset, size, dir);\n\n\tiommu_unmap(mapping->domain, iova, len);\n\t__free_iova(mapping, iova, len);\n}\n\nstatic void arm_iommu_sync_single_for_cpu(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tunsigned int offset = handle & ~PAGE_MASK;\n\n\tif (!iova)\n\t\treturn;\n\n\t__dma_page_dev_to_cpu(page, offset, size, dir);\n}\n\nstatic void arm_iommu_sync_single_for_device(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tunsigned int offset = handle & ~PAGE_MASK;\n\n\tif (!iova)\n\t\treturn;\n\n\t__dma_page_cpu_to_dev(page, offset, size, dir);\n}\n\nstruct dma_map_ops iommu_ops = {\n\t.alloc\t\t= arm_iommu_alloc_attrs,\n\t.free\t\t= arm_iommu_free_attrs,\n\t.mmap\t\t= arm_iommu_mmap_attrs,\n\t.get_sgtable\t= arm_iommu_get_sgtable,\n\n\t.map_page\t\t= arm_iommu_map_page,\n\t.unmap_page\t\t= arm_iommu_unmap_page,\n\t.sync_single_for_cpu\t= arm_iommu_sync_single_for_cpu,\n\t.sync_single_for_device\t= arm_iommu_sync_single_for_device,\n\n\t.map_sg\t\t\t= arm_iommu_map_sg,\n\t.unmap_sg\t\t= arm_iommu_unmap_sg,\n\t.sync_sg_for_cpu\t= arm_iommu_sync_sg_for_cpu,\n\t.sync_sg_for_device\t= arm_iommu_sync_sg_for_device,\n\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\n\nstruct dma_map_ops iommu_coherent_ops = {\n\t.alloc\t\t= arm_iommu_alloc_attrs,\n\t.free\t\t= arm_iommu_free_attrs,\n\t.mmap\t\t= arm_iommu_mmap_attrs,\n\t.get_sgtable\t= arm_iommu_get_sgtable,\n\n\t.map_page\t= arm_coherent_iommu_map_page,\n\t.unmap_page\t= arm_coherent_iommu_unmap_page,\n\n\t.map_sg\t\t= arm_coherent_iommu_map_sg,\n\t.unmap_sg\t= arm_coherent_iommu_unmap_sg,\n\n\t.set_dma_mask\t= arm_dma_set_mask,\n};\n\n/**\n * arm_iommu_create_mapping\n * @bus: pointer to the bus holding the client device (for IOMMU calls)\n * @base: start address of the valid IO address space\n * @size: size of the valid IO address space\n * @order: accuracy of the IO addresses allocations\n *\n * Creates a mapping structure which holds information about used/unused\n * IO address ranges, which is required to perform memory allocation and\n * mapping with IOMMU aware functions.\n *\n * The client device need to be attached to the mapping with\n * arm_iommu_attach_device function.\n */\nstruct dma_iommu_mapping *\narm_iommu_create_mapping(struct bus_type *bus, dma_addr_t base, size_t size,\n\t\t\t int order)\n{\n\tunsigned int count = size >> (PAGE_SHIFT + order);\n\tunsigned int bitmap_size = BITS_TO_LONGS(count) * sizeof(long);\n\tstruct dma_iommu_mapping *mapping;\n\tint err = -ENOMEM;\n\n\tif (!count)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmapping = kzalloc(sizeof(struct dma_iommu_mapping), GFP_KERNEL);\n\tif (!mapping)\n\t\tgoto err;\n\n\tmapping->bitmap = kzalloc(bitmap_size, GFP_KERNEL);\n\tif (!mapping->bitmap)\n\t\tgoto err2;\n\n\tmapping->base = base;\n\tmapping->bits = BITS_PER_BYTE * bitmap_size;\n\tmapping->order = order;\n\tspin_lock_init(&mapping->lock);\n\n\tmapping->domain = iommu_domain_alloc(bus);\n\tif (!mapping->domain)\n\t\tgoto err3;\n\n\tkref_init(&mapping->kref);\n\treturn mapping;\nerr3:\n\tkfree(mapping->bitmap);\nerr2:\n\tkfree(mapping);\nerr:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(arm_iommu_create_mapping);\n\nstatic void release_iommu_mapping(struct kref *kref)\n{\n\tstruct dma_iommu_mapping *mapping =\n\t\tcontainer_of(kref, struct dma_iommu_mapping, kref);\n\n\tiommu_domain_free(mapping->domain);\n\tkfree(mapping->bitmap);\n\tkfree(mapping);\n}\n\nvoid arm_iommu_release_mapping(struct dma_iommu_mapping *mapping)\n{\n\tif (mapping)\n\t\tkref_put(&mapping->kref, release_iommu_mapping);\n}\nEXPORT_SYMBOL_GPL(arm_iommu_release_mapping);\n\n/**\n * arm_iommu_attach_device\n * @dev: valid struct device pointer\n * @mapping: io address space mapping structure (returned from\n *\tarm_iommu_create_mapping)\n *\n * Attaches specified io address space mapping to the provided device,\n * this replaces the dma operations (dma_map_ops pointer) with the\n * IOMMU aware version. More than one client might be attached to\n * the same io address space mapping.\n */\nint arm_iommu_attach_device(struct device *dev,\n\t\t\t    struct dma_iommu_mapping *mapping)\n{\n\tint err;\n\n\terr = iommu_attach_device(mapping->domain, dev);\n\tif (err)\n\t\treturn err;\n\n\tkref_get(&mapping->kref);\n\tdev->archdata.mapping = mapping;\n\tset_dma_ops(dev, &iommu_ops);\n\n\tpr_debug(\"Attached IOMMU controller to %s device.\\n\", dev_name(dev));\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(arm_iommu_attach_device);\n\n/**\n * arm_iommu_detach_device\n * @dev: valid struct device pointer\n *\n * Detaches the provided device from a previously attached map.\n * This voids the dma operations (dma_map_ops pointer)\n */\nvoid arm_iommu_detach_device(struct device *dev)\n{\n\tstruct dma_iommu_mapping *mapping;\n\n\tmapping = to_dma_iommu_mapping(dev);\n\tif (!mapping) {\n\t\tdev_warn(dev, \"Not attached\\n\");\n\t\treturn;\n\t}\n\n\tiommu_detach_device(mapping->domain, dev);\n\tkref_put(&mapping->kref, release_iommu_mapping);\n\tdev->archdata.mapping = NULL;\n\tset_dma_ops(dev, NULL);\n\n\tpr_debug(\"Detached IOMMU controller from %s device.\\n\", dev_name(dev));\n}\nEXPORT_SYMBOL_GPL(arm_iommu_detach_device);\n\n#endif\n"], "fixing_code": ["/*\n *  linux/arch/arm/mm/dma-mapping.c\n *\n *  Copyright (C) 2000-2004 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  DMA uncached mapping support.\n */\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/gfp.h>\n#include <linux/errno.h>\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/dma-contiguous.h>\n#include <linux/highmem.h>\n#include <linux/memblock.h>\n#include <linux/slab.h>\n#include <linux/iommu.h>\n#include <linux/io.h>\n#include <linux/vmalloc.h>\n#include <linux/sizes.h>\n\n#include <asm/memory.h>\n#include <asm/highmem.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n#include <asm/mach/arch.h>\n#include <asm/dma-iommu.h>\n#include <asm/mach/map.h>\n#include <asm/system_info.h>\n#include <asm/dma-contiguous.h>\n\n#include \"mm.h\"\n\n/*\n * The DMA API is built upon the notion of \"buffer ownership\".  A buffer\n * is either exclusively owned by the CPU (and therefore may be accessed\n * by it) or exclusively owned by the DMA device.  These helper functions\n * represent the transitions between these two ownership states.\n *\n * Note, however, that on later ARMs, this notion does not work due to\n * speculative prefetches.  We model our approach on the assumption that\n * the CPU does do speculative prefetches, which means we clean caches\n * before transfers and delay cache invalidation until transfer completion.\n *\n */\nstatic void __dma_page_cpu_to_dev(struct page *, unsigned long,\n\t\tsize_t, enum dma_data_direction);\nstatic void __dma_page_dev_to_cpu(struct page *, unsigned long,\n\t\tsize_t, enum dma_data_direction);\n\n/**\n * arm_dma_map_page - map a portion of a page for streaming DMA\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * Ensure that any data held in the cache is appropriately discarded\n * or written back.\n *\n * The device owns this memory once this call has completed.  The CPU\n * can regain ownership by calling dma_unmap_page().\n */\nstatic dma_addr_t arm_dma_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_cpu_to_dev(page, offset, size, dir);\n\treturn pfn_to_dma(dev, page_to_pfn(page)) + offset;\n}\n\nstatic dma_addr_t arm_coherent_dma_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\treturn pfn_to_dma(dev, page_to_pfn(page)) + offset;\n}\n\n/**\n * arm_dma_unmap_page - unmap a buffer previously mapped through dma_map_page()\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * Unmap a page streaming mode DMA translation.  The handle and size\n * must match what was provided in the previous dma_map_page() call.\n * All other usages are undefined.\n *\n * After this call, reads by the CPU to the buffer are guaranteed to see\n * whatever the device wrote there.\n */\nstatic void arm_dma_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),\n\t\t\t\t      handle & ~PAGE_MASK, size, dir);\n}\n\nstatic void arm_dma_sync_single_for_cpu(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tunsigned int offset = handle & (PAGE_SIZE - 1);\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle-offset));\n\t__dma_page_dev_to_cpu(page, offset, size, dir);\n}\n\nstatic void arm_dma_sync_single_for_device(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tunsigned int offset = handle & (PAGE_SIZE - 1);\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle-offset));\n\t__dma_page_cpu_to_dev(page, offset, size, dir);\n}\n\nstruct dma_map_ops arm_dma_ops = {\n\t.alloc\t\t\t= arm_dma_alloc,\n\t.free\t\t\t= arm_dma_free,\n\t.mmap\t\t\t= arm_dma_mmap,\n\t.get_sgtable\t\t= arm_dma_get_sgtable,\n\t.map_page\t\t= arm_dma_map_page,\n\t.unmap_page\t\t= arm_dma_unmap_page,\n\t.map_sg\t\t\t= arm_dma_map_sg,\n\t.unmap_sg\t\t= arm_dma_unmap_sg,\n\t.sync_single_for_cpu\t= arm_dma_sync_single_for_cpu,\n\t.sync_single_for_device\t= arm_dma_sync_single_for_device,\n\t.sync_sg_for_cpu\t= arm_dma_sync_sg_for_cpu,\n\t.sync_sg_for_device\t= arm_dma_sync_sg_for_device,\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\nEXPORT_SYMBOL(arm_dma_ops);\n\nstatic void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n\tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs);\nstatic void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t\t  dma_addr_t handle, struct dma_attrs *attrs);\n\nstruct dma_map_ops arm_coherent_dma_ops = {\n\t.alloc\t\t\t= arm_coherent_dma_alloc,\n\t.free\t\t\t= arm_coherent_dma_free,\n\t.mmap\t\t\t= arm_dma_mmap,\n\t.get_sgtable\t\t= arm_dma_get_sgtable,\n\t.map_page\t\t= arm_coherent_dma_map_page,\n\t.map_sg\t\t\t= arm_dma_map_sg,\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\nEXPORT_SYMBOL(arm_coherent_dma_ops);\n\nstatic u64 get_coherent_dma_mask(struct device *dev)\n{\n\tu64 mask = (u64)arm_dma_limit;\n\n\tif (dev) {\n\t\tmask = dev->coherent_dma_mask;\n\n\t\t/*\n\t\t * Sanity check the DMA mask - it must be non-zero, and\n\t\t * must be able to be satisfied by a DMA allocation.\n\t\t */\n\t\tif (mask == 0) {\n\t\t\tdev_warn(dev, \"coherent DMA mask is unset\\n\");\n\t\t\treturn 0;\n\t\t}\n\n\t\tif ((~mask) & (u64)arm_dma_limit) {\n\t\t\tdev_warn(dev, \"coherent DMA mask %#llx is smaller \"\n\t\t\t\t \"than system GFP_DMA mask %#llx\\n\",\n\t\t\t\t mask, (u64)arm_dma_limit);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn mask;\n}\n\nstatic void __dma_clear_buffer(struct page *page, size_t size)\n{\n\t/*\n\t * Ensure that the allocated pages are zeroed, and that any data\n\t * lurking in the kernel direct-mapped region is invalidated.\n\t */\n\tif (PageHighMem(page)) {\n\t\tphys_addr_t base = __pfn_to_phys(page_to_pfn(page));\n\t\tphys_addr_t end = base + size;\n\t\twhile (size > 0) {\n\t\t\tvoid *ptr = kmap_atomic(page);\n\t\t\tmemset(ptr, 0, PAGE_SIZE);\n\t\t\tdmac_flush_range(ptr, ptr + PAGE_SIZE);\n\t\t\tkunmap_atomic(ptr);\n\t\t\tpage++;\n\t\t\tsize -= PAGE_SIZE;\n\t\t}\n\t\touter_flush_range(base, end);\n\t} else {\n\t\tvoid *ptr = page_address(page);\n\t\tmemset(ptr, 0, size);\n\t\tdmac_flush_range(ptr, ptr + size);\n\t\touter_flush_range(__pa(ptr), __pa(ptr) + size);\n\t}\n}\n\n/*\n * Allocate a DMA buffer for 'dev' of size 'size' using the\n * specified gfp mask.  Note that 'size' must be page aligned.\n */\nstatic struct page *__dma_alloc_buffer(struct device *dev, size_t size, gfp_t gfp)\n{\n\tunsigned long order = get_order(size);\n\tstruct page *page, *p, *e;\n\n\tpage = alloc_pages(gfp, order);\n\tif (!page)\n\t\treturn NULL;\n\n\t/*\n\t * Now split the huge page and free the excess pages\n\t */\n\tsplit_page(page, order);\n\tfor (p = page + (size >> PAGE_SHIFT), e = page + (1 << order); p < e; p++)\n\t\t__free_page(p);\n\n\t__dma_clear_buffer(page, size);\n\n\treturn page;\n}\n\n/*\n * Free a DMA buffer.  'size' must be page aligned.\n */\nstatic void __dma_free_buffer(struct page *page, size_t size)\n{\n\tstruct page *e = page + (size >> PAGE_SHIFT);\n\n\twhile (page < e) {\n\t\t__free_page(page);\n\t\tpage++;\n\t}\n}\n\n#ifdef CONFIG_MMU\n#ifdef CONFIG_HUGETLB_PAGE\n#warning ARM Coherent DMA allocator does not (yet) support huge TLB\n#endif\n\nstatic void *__alloc_from_contiguous(struct device *dev, size_t size,\n\t\t\t\t     pgprot_t prot, struct page **ret_page,\n\t\t\t\t     const void *caller);\n\nstatic void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t pgprot_t prot, struct page **ret_page,\n\t\t\t\t const void *caller);\n\nstatic void *\n__dma_alloc_remap(struct page *page, size_t size, gfp_t gfp, pgprot_t prot,\n\tconst void *caller)\n{\n\tstruct vm_struct *area;\n\tunsigned long addr;\n\n\t/*\n\t * DMA allocation can be mapped to user space, so lets\n\t * set VM_USERMAP flags too.\n\t */\n\tarea = get_vm_area_caller(size, VM_ARM_DMA_CONSISTENT | VM_USERMAP,\n\t\t\t\t  caller);\n\tif (!area)\n\t\treturn NULL;\n\taddr = (unsigned long)area->addr;\n\tarea->phys_addr = __pfn_to_phys(page_to_pfn(page));\n\n\tif (ioremap_page_range(addr, addr + size, area->phys_addr, prot)) {\n\t\tvunmap((void *)addr);\n\t\treturn NULL;\n\t}\n\treturn (void *)addr;\n}\n\nstatic void __dma_free_remap(void *cpu_addr, size_t size)\n{\n\tunsigned int flags = VM_ARM_DMA_CONSISTENT | VM_USERMAP;\n\tstruct vm_struct *area = find_vm_area(cpu_addr);\n\tif (!area || (area->flags & flags) != flags) {\n\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);\n\t\treturn;\n\t}\n\tunmap_kernel_range((unsigned long)cpu_addr, size);\n\tvunmap(cpu_addr);\n}\n\n#define DEFAULT_DMA_COHERENT_POOL_SIZE\tSZ_256K\n\nstruct dma_pool {\n\tsize_t size;\n\tspinlock_t lock;\n\tunsigned long *bitmap;\n\tunsigned long nr_pages;\n\tvoid *vaddr;\n\tstruct page **pages;\n};\n\nstatic struct dma_pool atomic_pool = {\n\t.size = DEFAULT_DMA_COHERENT_POOL_SIZE,\n};\n\nstatic int __init early_coherent_pool(char *p)\n{\n\tatomic_pool.size = memparse(p, &p);\n\treturn 0;\n}\nearly_param(\"coherent_pool\", early_coherent_pool);\n\nvoid __init init_dma_coherent_pool_size(unsigned long size)\n{\n\t/*\n\t * Catch any attempt to set the pool size too late.\n\t */\n\tBUG_ON(atomic_pool.vaddr);\n\n\t/*\n\t * Set architecture specific coherent pool size only if\n\t * it has not been changed by kernel command line parameter.\n\t */\n\tif (atomic_pool.size == DEFAULT_DMA_COHERENT_POOL_SIZE)\n\t\tatomic_pool.size = size;\n}\n\n/*\n * Initialise the coherent pool for atomic allocations.\n */\nstatic int __init atomic_pool_init(void)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tpgprot_t prot = pgprot_dmacoherent(pgprot_kernel);\n\tgfp_t gfp = GFP_KERNEL | GFP_DMA;\n\tunsigned long nr_pages = pool->size >> PAGE_SHIFT;\n\tunsigned long *bitmap;\n\tstruct page *page;\n\tstruct page **pages;\n\tvoid *ptr;\n\tint bitmap_size = BITS_TO_LONGS(nr_pages) * sizeof(long);\n\n\tbitmap = kzalloc(bitmap_size, GFP_KERNEL);\n\tif (!bitmap)\n\t\tgoto no_bitmap;\n\n\tpages = kzalloc(nr_pages * sizeof(struct page *), GFP_KERNEL);\n\tif (!pages)\n\t\tgoto no_pages;\n\n\tif (IS_ENABLED(CONFIG_DMA_CMA))\n\t\tptr = __alloc_from_contiguous(NULL, pool->size, prot, &page,\n\t\t\t\t\t      atomic_pool_init);\n\telse\n\t\tptr = __alloc_remap_buffer(NULL, pool->size, gfp, prot, &page,\n\t\t\t\t\t   atomic_pool_init);\n\tif (ptr) {\n\t\tint i;\n\n\t\tfor (i = 0; i < nr_pages; i++)\n\t\t\tpages[i] = page + i;\n\n\t\tspin_lock_init(&pool->lock);\n\t\tpool->vaddr = ptr;\n\t\tpool->pages = pages;\n\t\tpool->bitmap = bitmap;\n\t\tpool->nr_pages = nr_pages;\n\t\tpr_info(\"DMA: preallocated %u KiB pool for atomic coherent allocations\\n\",\n\t\t       (unsigned)pool->size / 1024);\n\t\treturn 0;\n\t}\n\n\tkfree(pages);\nno_pages:\n\tkfree(bitmap);\nno_bitmap:\n\tpr_err(\"DMA: failed to allocate %u KiB pool for atomic coherent allocation\\n\",\n\t       (unsigned)pool->size / 1024);\n\treturn -ENOMEM;\n}\n/*\n * CMA is activated by core_initcall, so we must be called after it.\n */\npostcore_initcall(atomic_pool_init);\n\nstruct dma_contig_early_reserve {\n\tphys_addr_t base;\n\tunsigned long size;\n};\n\nstatic struct dma_contig_early_reserve dma_mmu_remap[MAX_CMA_AREAS] __initdata;\n\nstatic int dma_mmu_remap_num __initdata;\n\nvoid __init dma_contiguous_early_fixup(phys_addr_t base, unsigned long size)\n{\n\tdma_mmu_remap[dma_mmu_remap_num].base = base;\n\tdma_mmu_remap[dma_mmu_remap_num].size = size;\n\tdma_mmu_remap_num++;\n}\n\nvoid __init dma_contiguous_remap(void)\n{\n\tint i;\n\tfor (i = 0; i < dma_mmu_remap_num; i++) {\n\t\tphys_addr_t start = dma_mmu_remap[i].base;\n\t\tphys_addr_t end = start + dma_mmu_remap[i].size;\n\t\tstruct map_desc map;\n\t\tunsigned long addr;\n\n\t\tif (end > arm_lowmem_limit)\n\t\t\tend = arm_lowmem_limit;\n\t\tif (start >= end)\n\t\t\tcontinue;\n\n\t\tmap.pfn = __phys_to_pfn(start);\n\t\tmap.virtual = __phys_to_virt(start);\n\t\tmap.length = end - start;\n\t\tmap.type = MT_MEMORY_DMA_READY;\n\n\t\t/*\n\t\t * Clear previous low-memory mapping\n\t\t */\n\t\tfor (addr = __phys_to_virt(start); addr < __phys_to_virt(end);\n\t\t     addr += PMD_SIZE)\n\t\t\tpmd_clear(pmd_off_k(addr));\n\n\t\tiotable_init(&map, 1);\n\t}\n}\n\nstatic int __dma_update_pte(pte_t *pte, pgtable_t token, unsigned long addr,\n\t\t\t    void *data)\n{\n\tstruct page *page = virt_to_page(addr);\n\tpgprot_t prot = *(pgprot_t *)data;\n\n\tset_pte_ext(pte, mk_pte(page, prot), 0);\n\treturn 0;\n}\n\nstatic void __dma_remap(struct page *page, size_t size, pgprot_t prot)\n{\n\tunsigned long start = (unsigned long) page_address(page);\n\tunsigned end = start + size;\n\n\tapply_to_page_range(&init_mm, start, size, __dma_update_pte, &prot);\n\tflush_tlb_kernel_range(start, end);\n}\n\nstatic void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t pgprot_t prot, struct page **ret_page,\n\t\t\t\t const void *caller)\n{\n\tstruct page *page;\n\tvoid *ptr;\n\tpage = __dma_alloc_buffer(dev, size, gfp);\n\tif (!page)\n\t\treturn NULL;\n\n\tptr = __dma_alloc_remap(page, size, gfp, prot, caller);\n\tif (!ptr) {\n\t\t__dma_free_buffer(page, size);\n\t\treturn NULL;\n\t}\n\n\t*ret_page = page;\n\treturn ptr;\n}\n\nstatic void *__alloc_from_pool(size_t size, struct page **ret_page)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned int pageno;\n\tunsigned long flags;\n\tvoid *ptr = NULL;\n\tunsigned long align_mask;\n\n\tif (!pool->vaddr) {\n\t\tWARN(1, \"coherent pool not initialised!\\n\");\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * Align the region allocation - allocations from pool are rather\n\t * small, so align them to their order in pages, minimum is a page\n\t * size. This helps reduce fragmentation of the DMA space.\n\t */\n\talign_mask = (1 << get_order(size)) - 1;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tpageno = bitmap_find_next_zero_area(pool->bitmap, pool->nr_pages,\n\t\t\t\t\t    0, count, align_mask);\n\tif (pageno < pool->nr_pages) {\n\t\tbitmap_set(pool->bitmap, pageno, count);\n\t\tptr = pool->vaddr + PAGE_SIZE * pageno;\n\t\t*ret_page = pool->pages[pageno];\n\t} else {\n\t\tpr_err_once(\"ERROR: %u KiB atomic DMA coherent pool is too small!\\n\"\n\t\t\t    \"Please increase it with coherent_pool= kernel parameter!\\n\",\n\t\t\t    (unsigned)pool->size / 1024);\n\t}\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\treturn ptr;\n}\n\nstatic bool __in_atomic_pool(void *start, size_t size)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tvoid *end = start + size;\n\tvoid *pool_start = pool->vaddr;\n\tvoid *pool_end = pool->vaddr + pool->size;\n\n\tif (start < pool_start || start >= pool_end)\n\t\treturn false;\n\n\tif (end <= pool_end)\n\t\treturn true;\n\n\tWARN(1, \"Wrong coherent size(%p-%p) from atomic pool(%p-%p)\\n\",\n\t     start, end - 1, pool_start, pool_end - 1);\n\n\treturn false;\n}\n\nstatic int __free_from_pool(void *start, size_t size)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tunsigned long pageno, count;\n\tunsigned long flags;\n\n\tif (!__in_atomic_pool(start, size))\n\t\treturn 0;\n\n\tpageno = (start - pool->vaddr) >> PAGE_SHIFT;\n\tcount = size >> PAGE_SHIFT;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tbitmap_clear(pool->bitmap, pageno, count);\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\treturn 1;\n}\n\nstatic void *__alloc_from_contiguous(struct device *dev, size_t size,\n\t\t\t\t     pgprot_t prot, struct page **ret_page,\n\t\t\t\t     const void *caller)\n{\n\tunsigned long order = get_order(size);\n\tsize_t count = size >> PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tpage = dma_alloc_from_contiguous(dev, count, order);\n\tif (!page)\n\t\treturn NULL;\n\n\t__dma_clear_buffer(page, size);\n\n\tif (PageHighMem(page)) {\n\t\tptr = __dma_alloc_remap(page, size, GFP_KERNEL, prot, caller);\n\t\tif (!ptr) {\n\t\t\tdma_release_from_contiguous(dev, page, count);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\t__dma_remap(page, size, prot);\n\t\tptr = page_address(page);\n\t}\n\t*ret_page = page;\n\treturn ptr;\n}\n\nstatic void __free_from_contiguous(struct device *dev, struct page *page,\n\t\t\t\t   void *cpu_addr, size_t size)\n{\n\tif (PageHighMem(page))\n\t\t__dma_free_remap(cpu_addr, size);\n\telse\n\t\t__dma_remap(page, size, pgprot_kernel);\n\tdma_release_from_contiguous(dev, page, size >> PAGE_SHIFT);\n}\n\nstatic inline pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot)\n{\n\tprot = dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs) ?\n\t\t\t    pgprot_writecombine(prot) :\n\t\t\t    pgprot_dmacoherent(prot);\n\treturn prot;\n}\n\n#define nommu() 0\n\n#else\t/* !CONFIG_MMU */\n\n#define nommu() 1\n\n#define __get_dma_pgprot(attrs, prot)\t__pgprot(0)\n#define __alloc_remap_buffer(dev, size, gfp, prot, ret, c)\tNULL\n#define __alloc_from_pool(size, ret_page)\t\t\tNULL\n#define __alloc_from_contiguous(dev, size, prot, ret, c)\tNULL\n#define __free_from_pool(cpu_addr, size)\t\t\t0\n#define __free_from_contiguous(dev, page, cpu_addr, size)\tdo { } while (0)\n#define __dma_free_remap(cpu_addr, size)\t\t\tdo { } while (0)\n\n#endif\t/* CONFIG_MMU */\n\nstatic void *__alloc_simple_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t   struct page **ret_page)\n{\n\tstruct page *page;\n\tpage = __dma_alloc_buffer(dev, size, gfp);\n\tif (!page)\n\t\treturn NULL;\n\n\t*ret_page = page;\n\treturn page_address(page);\n}\n\n\n\nstatic void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t\t gfp_t gfp, pgprot_t prot, bool is_coherent, const void *caller)\n{\n\tu64 mask = get_coherent_dma_mask(dev);\n\tstruct page *page = NULL;\n\tvoid *addr;\n\n#ifdef CONFIG_DMA_API_DEBUG\n\tu64 limit = (mask + 1) & ~mask;\n\tif (limit && size >= limit) {\n\t\tdev_warn(dev, \"coherent allocation too big (requested %#x mask %#llx)\\n\",\n\t\t\tsize, mask);\n\t\treturn NULL;\n\t}\n#endif\n\n\tif (!mask)\n\t\treturn NULL;\n\n\tif (mask < 0xffffffffULL)\n\t\tgfp |= GFP_DMA;\n\n\t/*\n\t * Following is a work-around (a.k.a. hack) to prevent pages\n\t * with __GFP_COMP being passed to split_page() which cannot\n\t * handle them.  The real problem is that this flag probably\n\t * should be 0 on ARM as it is not supported on this\n\t * platform; see CONFIG_HUGETLBFS.\n\t */\n\tgfp &= ~(__GFP_COMP);\n\n\t*handle = DMA_ERROR_CODE;\n\tsize = PAGE_ALIGN(size);\n\n\tif (is_coherent || nommu())\n\t\taddr = __alloc_simple_buffer(dev, size, gfp, &page);\n\telse if (!(gfp & __GFP_WAIT))\n\t\taddr = __alloc_from_pool(size, &page);\n\telse if (!IS_ENABLED(CONFIG_DMA_CMA))\n\t\taddr = __alloc_remap_buffer(dev, size, gfp, prot, &page, caller);\n\telse\n\t\taddr = __alloc_from_contiguous(dev, size, prot, &page, caller);\n\n\tif (addr)\n\t\t*handle = pfn_to_dma(dev, page_to_pfn(page));\n\n\treturn addr;\n}\n\n/*\n * Allocate DMA-coherent memory space and return both the kernel remapped\n * virtual and bus address for that space.\n */\nvoid *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t    gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, false,\n\t\t\t   __builtin_return_address(0));\n}\n\nstatic void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n\tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, true,\n\t\t\t   __builtin_return_address(0));\n}\n\n/*\n * Create userspace mapping for the DMA-coherent memory.\n */\nint arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,\n\t\t void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t struct dma_attrs *attrs)\n{\n\tint ret = -ENXIO;\n#ifdef CONFIG_MMU\n\tunsigned long nr_vma_pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n\tunsigned long nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned long pfn = dma_to_pfn(dev, dma_addr);\n\tunsigned long off = vma->vm_pgoff;\n\n\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);\n\n\tif (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &ret))\n\t\treturn ret;\n\n\tif (off < nr_pages && nr_vma_pages <= (nr_pages - off)) {\n\t\tret = remap_pfn_range(vma, vma->vm_start,\n\t\t\t\t      pfn + off,\n\t\t\t\t      vma->vm_end - vma->vm_start,\n\t\t\t\t      vma->vm_page_prot);\n\t}\n#endif\t/* CONFIG_MMU */\n\n\treturn ret;\n}\n\n/*\n * Free a buffer as defined by the above mapping.\n */\nstatic void __arm_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t   dma_addr_t handle, struct dma_attrs *attrs,\n\t\t\t   bool is_coherent)\n{\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle));\n\n\tif (dma_release_from_coherent(dev, get_order(size), cpu_addr))\n\t\treturn;\n\n\tsize = PAGE_ALIGN(size);\n\n\tif (is_coherent || nommu()) {\n\t\t__dma_free_buffer(page, size);\n\t} else if (__free_from_pool(cpu_addr, size)) {\n\t\treturn;\n\t} else if (!IS_ENABLED(CONFIG_DMA_CMA)) {\n\t\t__dma_free_remap(cpu_addr, size);\n\t\t__dma_free_buffer(page, size);\n\t} else {\n\t\t/*\n\t\t * Non-atomic allocations cannot be freed with IRQs disabled\n\t\t */\n\t\tWARN_ON(irqs_disabled());\n\t\t__free_from_contiguous(dev, page, cpu_addr, size);\n\t}\n}\n\nvoid arm_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\t__arm_dma_free(dev, size, cpu_addr, handle, attrs, false);\n}\n\nstatic void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\t__arm_dma_free(dev, size, cpu_addr, handle, attrs, true);\n}\n\nint arm_dma_get_sgtable(struct device *dev, struct sg_table *sgt,\n\t\t void *cpu_addr, dma_addr_t handle, size_t size,\n\t\t struct dma_attrs *attrs)\n{\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle));\n\tint ret;\n\n\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);\n\treturn 0;\n}\n\nstatic void dma_cache_maint_page(struct page *page, unsigned long offset,\n\tsize_t size, enum dma_data_direction dir,\n\tvoid (*op)(const void *, size_t, int))\n{\n\tunsigned long pfn;\n\tsize_t left = size;\n\n\tpfn = page_to_pfn(page) + offset / PAGE_SIZE;\n\toffset %= PAGE_SIZE;\n\n\t/*\n\t * A single sg entry may refer to multiple physically contiguous\n\t * pages.  But we still need to process highmem pages individually.\n\t * If highmem is not configured then the bulk of this loop gets\n\t * optimized out.\n\t */\n\tdo {\n\t\tsize_t len = left;\n\t\tvoid *vaddr;\n\n\t\tpage = pfn_to_page(pfn);\n\n\t\tif (PageHighMem(page)) {\n\t\t\tif (len + offset > PAGE_SIZE)\n\t\t\t\tlen = PAGE_SIZE - offset;\n\n\t\t\tif (cache_is_vipt_nonaliasing()) {\n\t\t\t\tvaddr = kmap_atomic(page);\n\t\t\t\top(vaddr + offset, len, dir);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t} else {\n\t\t\t\tvaddr = kmap_high_get(page);\n\t\t\t\tif (vaddr) {\n\t\t\t\t\top(vaddr + offset, len, dir);\n\t\t\t\t\tkunmap_high(page);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tvaddr = page_address(page) + offset;\n\t\t\top(vaddr, len, dir);\n\t\t}\n\t\toffset = 0;\n\t\tpfn++;\n\t\tleft -= len;\n\t} while (left);\n}\n\n/*\n * Make an area consistent for devices.\n * Note: Drivers should NOT use this function directly, as it will break\n * platforms with CONFIG_DMABOUNCE.\n * Use the driver DMA support - see dma-mapping.h (dma_sync_*)\n */\nstatic void __dma_page_cpu_to_dev(struct page *page, unsigned long off,\n\tsize_t size, enum dma_data_direction dir)\n{\n\tunsigned long paddr;\n\n\tdma_cache_maint_page(page, off, size, dir, dmac_map_area);\n\n\tpaddr = page_to_phys(page) + off;\n\tif (dir == DMA_FROM_DEVICE) {\n\t\touter_inv_range(paddr, paddr + size);\n\t} else {\n\t\touter_clean_range(paddr, paddr + size);\n\t}\n\t/* FIXME: non-speculating: flush on bidirectional mappings? */\n}\n\nstatic void __dma_page_dev_to_cpu(struct page *page, unsigned long off,\n\tsize_t size, enum dma_data_direction dir)\n{\n\tunsigned long paddr = page_to_phys(page) + off;\n\n\t/* FIXME: non-speculating: not required */\n\t/* don't bother invalidating if DMA to device */\n\tif (dir != DMA_TO_DEVICE)\n\t\touter_inv_range(paddr, paddr + size);\n\n\tdma_cache_maint_page(page, off, size, dir, dmac_unmap_area);\n\n\t/*\n\t * Mark the D-cache clean for these pages to avoid extra flushing.\n\t */\n\tif (dir != DMA_TO_DEVICE && size >= PAGE_SIZE) {\n\t\tunsigned long pfn;\n\t\tsize_t left = size;\n\n\t\tpfn = page_to_pfn(page) + off / PAGE_SIZE;\n\t\toff %= PAGE_SIZE;\n\t\tif (off) {\n\t\t\tpfn++;\n\t\t\tleft -= PAGE_SIZE - off;\n\t\t}\n\t\twhile (left >= PAGE_SIZE) {\n\t\t\tpage = pfn_to_page(pfn++);\n\t\t\tset_bit(PG_dcache_clean, &page->flags);\n\t\t\tleft -= PAGE_SIZE;\n\t\t}\n\t}\n}\n\n/**\n * arm_dma_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of buffers described by scatterlist in streaming mode for DMA.\n * This is the scatter-gather version of the dma_map_single interface.\n * Here the scatter gather list elements are each tagged with the\n * appropriate dma address and length.  They are obtained via\n * sg_dma_{address,length}.\n *\n * Device ownership issues as mentioned for dma_map_single are the same\n * here.\n */\nint arm_dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i, j;\n\n\tfor_each_sg(sg, s, nents, i) {\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\t\ts->dma_length = s->length;\n#endif\n\t\ts->dma_address = ops->map_page(dev, sg_page(s), s->offset,\n\t\t\t\t\t\ts->length, dir, attrs);\n\t\tif (dma_mapping_error(dev, s->dma_address))\n\t\t\tgoto bad_mapping;\n\t}\n\treturn nents;\n\n bad_mapping:\n\tfor_each_sg(sg, s, i, j)\n\t\tops->unmap_page(dev, sg_dma_address(s), sg_dma_len(s), dir, attrs);\n\treturn 0;\n}\n\n/**\n * arm_dma_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->unmap_page(dev, sg_dma_address(s), sg_dma_len(s), dir, attrs);\n}\n\n/**\n * arm_dma_sync_sg_for_cpu\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->sync_single_for_cpu(dev, sg_dma_address(s), s->length,\n\t\t\t\t\t dir);\n}\n\n/**\n * arm_dma_sync_sg_for_device\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->sync_single_for_device(dev, sg_dma_address(s), s->length,\n\t\t\t\t\t    dir);\n}\n\n/*\n * Return whether the given device DMA address mask can be supported\n * properly.  For example, if your device can only drive the low 24-bits\n * during bus mastering, then you would pass 0x00ffffff as the mask\n * to this function.\n */\nint dma_supported(struct device *dev, u64 mask)\n{\n\tif (mask < (u64)arm_dma_limit)\n\t\treturn 0;\n\treturn 1;\n}\nEXPORT_SYMBOL(dma_supported);\n\nint arm_dma_set_mask(struct device *dev, u64 dma_mask)\n{\n\tif (!dev->dma_mask || !dma_supported(dev, dma_mask))\n\t\treturn -EIO;\n\n\t*dev->dma_mask = dma_mask;\n\n\treturn 0;\n}\n\n#define PREALLOC_DMA_DEBUG_ENTRIES\t4096\n\nstatic int __init dma_debug_do_init(void)\n{\n\tdma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);\n\treturn 0;\n}\nfs_initcall(dma_debug_do_init);\n\n#ifdef CONFIG_ARM_DMA_USE_IOMMU\n\n/* IOMMU */\n\nstatic inline dma_addr_t __alloc_iova(struct dma_iommu_mapping *mapping,\n\t\t\t\t      size_t size)\n{\n\tunsigned int order = get_order(size);\n\tunsigned int align = 0;\n\tunsigned int count, start;\n\tunsigned long flags;\n\n\tif (order > CONFIG_ARM_DMA_IOMMU_ALIGNMENT)\n\t\torder = CONFIG_ARM_DMA_IOMMU_ALIGNMENT;\n\n\tcount = ((PAGE_ALIGN(size) >> PAGE_SHIFT) +\n\t\t (1 << mapping->order) - 1) >> mapping->order;\n\n\tif (order > mapping->order)\n\t\talign = (1 << (order - mapping->order)) - 1;\n\n\tspin_lock_irqsave(&mapping->lock, flags);\n\tstart = bitmap_find_next_zero_area(mapping->bitmap, mapping->bits, 0,\n\t\t\t\t\t   count, align);\n\tif (start > mapping->bits) {\n\t\tspin_unlock_irqrestore(&mapping->lock, flags);\n\t\treturn DMA_ERROR_CODE;\n\t}\n\n\tbitmap_set(mapping->bitmap, start, count);\n\tspin_unlock_irqrestore(&mapping->lock, flags);\n\n\treturn mapping->base + (start << (mapping->order + PAGE_SHIFT));\n}\n\nstatic inline void __free_iova(struct dma_iommu_mapping *mapping,\n\t\t\t       dma_addr_t addr, size_t size)\n{\n\tunsigned int start = (addr - mapping->base) >>\n\t\t\t     (mapping->order + PAGE_SHIFT);\n\tunsigned int count = ((size >> PAGE_SHIFT) +\n\t\t\t      (1 << mapping->order) - 1) >> mapping->order;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mapping->lock, flags);\n\tbitmap_clear(mapping->bitmap, start, count);\n\tspin_unlock_irqrestore(&mapping->lock, flags);\n}\n\nstatic struct page **__iommu_alloc_buffer(struct device *dev, size_t size,\n\t\t\t\t\t  gfp_t gfp, struct dma_attrs *attrs)\n{\n\tstruct page **pages;\n\tint count = size >> PAGE_SHIFT;\n\tint array_size = count * sizeof(struct page *);\n\tint i = 0;\n\n\tif (array_size <= PAGE_SIZE)\n\t\tpages = kzalloc(array_size, gfp);\n\telse\n\t\tpages = vzalloc(array_size);\n\tif (!pages)\n\t\treturn NULL;\n\n\tif (dma_get_attr(DMA_ATTR_FORCE_CONTIGUOUS, attrs))\n\t{\n\t\tunsigned long order = get_order(size);\n\t\tstruct page *page;\n\n\t\tpage = dma_alloc_from_contiguous(dev, count, order);\n\t\tif (!page)\n\t\t\tgoto error;\n\n\t\t__dma_clear_buffer(page, size);\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tpages[i] = page + i;\n\n\t\treturn pages;\n\t}\n\n\t/*\n\t * IOMMU can map any pages, so himem can also be used here\n\t */\n\tgfp |= __GFP_NOWARN | __GFP_HIGHMEM;\n\n\twhile (count) {\n\t\tint j, order = __fls(count);\n\n\t\tpages[i] = alloc_pages(gfp, order);\n\t\twhile (!pages[i] && order)\n\t\t\tpages[i] = alloc_pages(gfp, --order);\n\t\tif (!pages[i])\n\t\t\tgoto error;\n\n\t\tif (order) {\n\t\t\tsplit_page(pages[i], order);\n\t\t\tj = 1 << order;\n\t\t\twhile (--j)\n\t\t\t\tpages[i + j] = pages[i] + j;\n\t\t}\n\n\t\t__dma_clear_buffer(pages[i], PAGE_SIZE << order);\n\t\ti += 1 << order;\n\t\tcount -= 1 << order;\n\t}\n\n\treturn pages;\nerror:\n\twhile (i--)\n\t\tif (pages[i])\n\t\t\t__free_pages(pages[i], 0);\n\tif (array_size <= PAGE_SIZE)\n\t\tkfree(pages);\n\telse\n\t\tvfree(pages);\n\treturn NULL;\n}\n\nstatic int __iommu_free_buffer(struct device *dev, struct page **pages,\n\t\t\t       size_t size, struct dma_attrs *attrs)\n{\n\tint count = size >> PAGE_SHIFT;\n\tint array_size = count * sizeof(struct page *);\n\tint i;\n\n\tif (dma_get_attr(DMA_ATTR_FORCE_CONTIGUOUS, attrs)) {\n\t\tdma_release_from_contiguous(dev, pages[0], count);\n\t} else {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tif (pages[i])\n\t\t\t\t__free_pages(pages[i], 0);\n\t}\n\n\tif (array_size <= PAGE_SIZE)\n\t\tkfree(pages);\n\telse\n\t\tvfree(pages);\n\treturn 0;\n}\n\n/*\n * Create a CPU mapping for a specified pages\n */\nstatic void *\n__iommu_alloc_remap(struct page **pages, size_t size, gfp_t gfp, pgprot_t prot,\n\t\t    const void *caller)\n{\n\tunsigned int i, nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tstruct vm_struct *area;\n\tunsigned long p;\n\n\tarea = get_vm_area_caller(size, VM_ARM_DMA_CONSISTENT | VM_USERMAP,\n\t\t\t\t  caller);\n\tif (!area)\n\t\treturn NULL;\n\n\tarea->pages = pages;\n\tarea->nr_pages = nr_pages;\n\tp = (unsigned long)area->addr;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tphys_addr_t phys = __pfn_to_phys(page_to_pfn(pages[i]));\n\t\tif (ioremap_page_range(p, p + PAGE_SIZE, phys, prot))\n\t\t\tgoto err;\n\t\tp += PAGE_SIZE;\n\t}\n\treturn area->addr;\nerr:\n\tunmap_kernel_range((unsigned long)area->addr, size);\n\tvunmap(area->addr);\n\treturn NULL;\n}\n\n/*\n * Create a mapping in device IO address space for specified pages\n */\nstatic dma_addr_t\n__iommu_create_mapping(struct device *dev, struct page **pages, size_t size)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tdma_addr_t dma_addr, iova;\n\tint i, ret = DMA_ERROR_CODE;\n\n\tdma_addr = __alloc_iova(mapping, size);\n\tif (dma_addr == DMA_ERROR_CODE)\n\t\treturn dma_addr;\n\n\tiova = dma_addr;\n\tfor (i = 0; i < count; ) {\n\t\tunsigned int next_pfn = page_to_pfn(pages[i]) + 1;\n\t\tphys_addr_t phys = page_to_phys(pages[i]);\n\t\tunsigned int len, j;\n\n\t\tfor (j = i + 1; j < count; j++, next_pfn++)\n\t\t\tif (page_to_pfn(pages[j]) != next_pfn)\n\t\t\t\tbreak;\n\n\t\tlen = (j - i) << PAGE_SHIFT;\n\t\tret = iommu_map(mapping->domain, iova, phys, len, 0);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\t\tiova += len;\n\t\ti = j;\n\t}\n\treturn dma_addr;\nfail:\n\tiommu_unmap(mapping->domain, dma_addr, iova-dma_addr);\n\t__free_iova(mapping, dma_addr, size);\n\treturn DMA_ERROR_CODE;\n}\n\nstatic int __iommu_remove_mapping(struct device *dev, dma_addr_t iova, size_t size)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\n\t/*\n\t * add optional in-page offset from iova to size and align\n\t * result to page size\n\t */\n\tsize = PAGE_ALIGN((iova & ~PAGE_MASK) + size);\n\tiova &= PAGE_MASK;\n\n\tiommu_unmap(mapping->domain, iova, size);\n\t__free_iova(mapping, iova, size);\n\treturn 0;\n}\n\nstatic struct page **__atomic_get_pages(void *addr)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tstruct page **pages = pool->pages;\n\tint offs = (addr - pool->vaddr) >> PAGE_SHIFT;\n\n\treturn pages + offs;\n}\n\nstatic struct page **__iommu_get_pages(void *cpu_addr, struct dma_attrs *attrs)\n{\n\tstruct vm_struct *area;\n\n\tif (__in_atomic_pool(cpu_addr, PAGE_SIZE))\n\t\treturn __atomic_get_pages(cpu_addr);\n\n\tif (dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs))\n\t\treturn cpu_addr;\n\n\tarea = find_vm_area(cpu_addr);\n\tif (area && (area->flags & VM_ARM_DMA_CONSISTENT))\n\t\treturn area->pages;\n\treturn NULL;\n}\n\nstatic void *__iommu_alloc_atomic(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *handle)\n{\n\tstruct page *page;\n\tvoid *addr;\n\n\taddr = __alloc_from_pool(size, &page);\n\tif (!addr)\n\t\treturn NULL;\n\n\t*handle = __iommu_create_mapping(dev, &page, size);\n\tif (*handle == DMA_ERROR_CODE)\n\t\tgoto err_mapping;\n\n\treturn addr;\n\nerr_mapping:\n\t__free_from_pool(addr, size);\n\treturn NULL;\n}\n\nstatic void __iommu_free_atomic(struct device *dev, void *cpu_addr,\n\t\t\t\tdma_addr_t handle, size_t size)\n{\n\t__iommu_remove_mapping(dev, handle, size);\n\t__free_from_pool(cpu_addr, size);\n}\n\nstatic void *arm_iommu_alloc_attrs(struct device *dev, size_t size,\n\t    dma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n\tstruct page **pages;\n\tvoid *addr = NULL;\n\n\t*handle = DMA_ERROR_CODE;\n\tsize = PAGE_ALIGN(size);\n\n\tif (gfp & GFP_ATOMIC)\n\t\treturn __iommu_alloc_atomic(dev, size, handle);\n\n\t/*\n\t * Following is a work-around (a.k.a. hack) to prevent pages\n\t * with __GFP_COMP being passed to split_page() which cannot\n\t * handle them.  The real problem is that this flag probably\n\t * should be 0 on ARM as it is not supported on this\n\t * platform; see CONFIG_HUGETLBFS.\n\t */\n\tgfp &= ~(__GFP_COMP);\n\n\tpages = __iommu_alloc_buffer(dev, size, gfp, attrs);\n\tif (!pages)\n\t\treturn NULL;\n\n\t*handle = __iommu_create_mapping(dev, pages, size);\n\tif (*handle == DMA_ERROR_CODE)\n\t\tgoto err_buffer;\n\n\tif (dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs))\n\t\treturn pages;\n\n\taddr = __iommu_alloc_remap(pages, size, gfp, prot,\n\t\t\t\t   __builtin_return_address(0));\n\tif (!addr)\n\t\tgoto err_mapping;\n\n\treturn addr;\n\nerr_mapping:\n\t__iommu_remove_mapping(dev, *handle, size);\nerr_buffer:\n\t__iommu_free_buffer(dev, pages, size, attrs);\n\treturn NULL;\n}\n\nstatic int arm_iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,\n\t\t    void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t    struct dma_attrs *attrs)\n{\n\tunsigned long uaddr = vma->vm_start;\n\tunsigned long usize = vma->vm_end - vma->vm_start;\n\tstruct page **pages = __iommu_get_pages(cpu_addr, attrs);\n\n\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);\n\n\tif (!pages)\n\t\treturn -ENXIO;\n\n\tdo {\n\t\tint ret = vm_insert_page(vma, uaddr, *pages++);\n\t\tif (ret) {\n\t\t\tpr_err(\"Remapping memory failed: %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t\tuaddr += PAGE_SIZE;\n\t\tusize -= PAGE_SIZE;\n\t} while (usize > 0);\n\n\treturn 0;\n}\n\n/*\n * free a page as defined by the above mapping.\n * Must not be called with IRQs disabled.\n */\nvoid arm_iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\tstruct page **pages;\n\tsize = PAGE_ALIGN(size);\n\n\tif (__in_atomic_pool(cpu_addr, size)) {\n\t\t__iommu_free_atomic(dev, cpu_addr, handle, size);\n\t\treturn;\n\t}\n\n\tpages = __iommu_get_pages(cpu_addr, attrs);\n\tif (!pages) {\n\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);\n\t\treturn;\n\t}\n\n\tif (!dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs)) {\n\t\tunmap_kernel_range((unsigned long)cpu_addr, size);\n\t\tvunmap(cpu_addr);\n\t}\n\n\t__iommu_remove_mapping(dev, handle, size);\n\t__iommu_free_buffer(dev, pages, size, attrs);\n}\n\nstatic int arm_iommu_get_sgtable(struct device *dev, struct sg_table *sgt,\n\t\t\t\t void *cpu_addr, dma_addr_t dma_addr,\n\t\t\t\t size_t size, struct dma_attrs *attrs)\n{\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tstruct page **pages = __iommu_get_pages(cpu_addr, attrs);\n\n\tif (!pages)\n\t\treturn -ENXIO;\n\n\treturn sg_alloc_table_from_pages(sgt, pages, count, 0, size,\n\t\t\t\t\t GFP_KERNEL);\n}\n\n/*\n * Map a part of the scatter-gather list into contiguous io address space\n */\nstatic int __map_sg_chunk(struct device *dev, struct scatterlist *sg,\n\t\t\t  size_t size, dma_addr_t *handle,\n\t\t\t  enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\t\t  bool is_coherent)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova, iova_base;\n\tint ret = 0;\n\tunsigned int count;\n\tstruct scatterlist *s;\n\n\tsize = PAGE_ALIGN(size);\n\t*handle = DMA_ERROR_CODE;\n\n\tiova_base = iova = __alloc_iova(mapping, size);\n\tif (iova == DMA_ERROR_CODE)\n\t\treturn -ENOMEM;\n\n\tfor (count = 0, s = sg; count < (size >> PAGE_SHIFT); s = sg_next(s)) {\n\t\tphys_addr_t phys = page_to_phys(sg_page(s));\n\t\tunsigned int len = PAGE_ALIGN(s->offset + s->length);\n\n\t\tif (!is_coherent &&\n\t\t\t!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t\t__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);\n\n\t\tret = iommu_map(mapping->domain, iova, phys, len, 0);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\t\tcount += len >> PAGE_SHIFT;\n\t\tiova += len;\n\t}\n\t*handle = iova_base;\n\n\treturn 0;\nfail:\n\tiommu_unmap(mapping->domain, iova_base, count * PAGE_SIZE);\n\t__free_iova(mapping, iova_base, size);\n\treturn ret;\n}\n\nstatic int __iommu_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\t     enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\t     bool is_coherent)\n{\n\tstruct scatterlist *s = sg, *dma = sg, *start = sg;\n\tint i, count = 0;\n\tunsigned int offset = s->offset;\n\tunsigned int size = s->offset + s->length;\n\tunsigned int max = dma_get_max_seg_size(dev);\n\n\tfor (i = 1; i < nents; i++) {\n\t\ts = sg_next(s);\n\n\t\ts->dma_address = DMA_ERROR_CODE;\n\t\ts->dma_length = 0;\n\n\t\tif (s->offset || (size & ~PAGE_MASK) || size + s->length > max) {\n\t\t\tif (__map_sg_chunk(dev, start, size, &dma->dma_address,\n\t\t\t    dir, attrs, is_coherent) < 0)\n\t\t\t\tgoto bad_mapping;\n\n\t\t\tdma->dma_address += offset;\n\t\t\tdma->dma_length = size - offset;\n\n\t\t\tsize = offset = s->offset;\n\t\t\tstart = s;\n\t\t\tdma = sg_next(dma);\n\t\t\tcount += 1;\n\t\t}\n\t\tsize += s->length;\n\t}\n\tif (__map_sg_chunk(dev, start, size, &dma->dma_address, dir, attrs,\n\t\tis_coherent) < 0)\n\t\tgoto bad_mapping;\n\n\tdma->dma_address += offset;\n\tdma->dma_length = size - offset;\n\n\treturn count+1;\n\nbad_mapping:\n\tfor_each_sg(sg, s, count, i)\n\t\t__iommu_remove_mapping(dev, sg_dma_address(s), sg_dma_len(s));\n\treturn 0;\n}\n\n/**\n * arm_coherent_iommu_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of i/o coherent buffers described by scatterlist in streaming\n * mode for DMA. The scatter gather list elements are merged together (if\n * possible) and tagged with the appropriate dma address and length. They are\n * obtained via sg_dma_{address,length}.\n */\nint arm_coherent_iommu_map_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\treturn __iommu_map_sg(dev, sg, nents, dir, attrs, true);\n}\n\n/**\n * arm_iommu_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of buffers described by scatterlist in streaming mode for DMA.\n * The scatter gather list elements are merged together (if possible) and\n * tagged with the appropriate dma address and length. They are obtained via\n * sg_dma_{address,length}.\n */\nint arm_iommu_map_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\treturn __iommu_map_sg(dev, sg, nents, dir, attrs, false);\n}\n\nstatic void __iommu_unmap_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\tbool is_coherent)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tif (sg_dma_len(s))\n\t\t\t__iommu_remove_mapping(dev, sg_dma_address(s),\n\t\t\t\t\t       sg_dma_len(s));\n\t\tif (!is_coherent &&\n\t\t    !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t\t__dma_page_dev_to_cpu(sg_page(s), s->offset,\n\t\t\t\t\t      s->length, dir);\n\t}\n}\n\n/**\n * arm_coherent_iommu_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_coherent_iommu_unmap_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\t__iommu_unmap_sg(dev, sg, nents, dir, attrs, true);\n}\n\n/**\n * arm_iommu_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_iommu_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\t__iommu_unmap_sg(dev, sg, nents, dir, attrs, false);\n}\n\n/**\n * arm_iommu_sync_sg_for_cpu\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_iommu_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\t__dma_page_dev_to_cpu(sg_page(s), s->offset, s->length, dir);\n\n}\n\n/**\n * arm_iommu_sync_sg_for_device\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_iommu_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\t__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);\n}\n\n\n/**\n * arm_coherent_iommu_map_page\n * @dev: valid struct device pointer\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * Coherent IOMMU aware version of arm_dma_map_page()\n */\nstatic dma_addr_t arm_coherent_iommu_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t dma_addr;\n\tint ret, prot, len = PAGE_ALIGN(size + offset);\n\n\tdma_addr = __alloc_iova(mapping, len);\n\tif (dma_addr == DMA_ERROR_CODE)\n\t\treturn dma_addr;\n\n\tswitch (dir) {\n\tcase DMA_BIDIRECTIONAL:\n\t\tprot = IOMMU_READ | IOMMU_WRITE;\n\t\tbreak;\n\tcase DMA_TO_DEVICE:\n\t\tprot = IOMMU_READ;\n\t\tbreak;\n\tcase DMA_FROM_DEVICE:\n\t\tprot = IOMMU_WRITE;\n\t\tbreak;\n\tdefault:\n\t\tprot = 0;\n\t}\n\n\tret = iommu_map(mapping->domain, dma_addr, page_to_phys(page), len, prot);\n\tif (ret < 0)\n\t\tgoto fail;\n\n\treturn dma_addr + offset;\nfail:\n\t__free_iova(mapping, dma_addr, len);\n\treturn DMA_ERROR_CODE;\n}\n\n/**\n * arm_iommu_map_page\n * @dev: valid struct device pointer\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * IOMMU aware version of arm_dma_map_page()\n */\nstatic dma_addr_t arm_iommu_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_cpu_to_dev(page, offset, size, dir);\n\n\treturn arm_coherent_iommu_map_page(dev, page, offset, size, dir, attrs);\n}\n\n/**\n * arm_coherent_iommu_unmap_page\n * @dev: valid struct device pointer\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * Coherent IOMMU aware version of arm_dma_unmap_page()\n */\nstatic void arm_coherent_iommu_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tint offset = handle & ~PAGE_MASK;\n\tint len = PAGE_ALIGN(size + offset);\n\n\tif (!iova)\n\t\treturn;\n\n\tiommu_unmap(mapping->domain, iova, len);\n\t__free_iova(mapping, iova, len);\n}\n\n/**\n * arm_iommu_unmap_page\n * @dev: valid struct device pointer\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * IOMMU aware version of arm_dma_unmap_page()\n */\nstatic void arm_iommu_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tint offset = handle & ~PAGE_MASK;\n\tint len = PAGE_ALIGN(size + offset);\n\n\tif (!iova)\n\t\treturn;\n\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_dev_to_cpu(page, offset, size, dir);\n\n\tiommu_unmap(mapping->domain, iova, len);\n\t__free_iova(mapping, iova, len);\n}\n\nstatic void arm_iommu_sync_single_for_cpu(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tunsigned int offset = handle & ~PAGE_MASK;\n\n\tif (!iova)\n\t\treturn;\n\n\t__dma_page_dev_to_cpu(page, offset, size, dir);\n}\n\nstatic void arm_iommu_sync_single_for_device(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tunsigned int offset = handle & ~PAGE_MASK;\n\n\tif (!iova)\n\t\treturn;\n\n\t__dma_page_cpu_to_dev(page, offset, size, dir);\n}\n\nstruct dma_map_ops iommu_ops = {\n\t.alloc\t\t= arm_iommu_alloc_attrs,\n\t.free\t\t= arm_iommu_free_attrs,\n\t.mmap\t\t= arm_iommu_mmap_attrs,\n\t.get_sgtable\t= arm_iommu_get_sgtable,\n\n\t.map_page\t\t= arm_iommu_map_page,\n\t.unmap_page\t\t= arm_iommu_unmap_page,\n\t.sync_single_for_cpu\t= arm_iommu_sync_single_for_cpu,\n\t.sync_single_for_device\t= arm_iommu_sync_single_for_device,\n\n\t.map_sg\t\t\t= arm_iommu_map_sg,\n\t.unmap_sg\t\t= arm_iommu_unmap_sg,\n\t.sync_sg_for_cpu\t= arm_iommu_sync_sg_for_cpu,\n\t.sync_sg_for_device\t= arm_iommu_sync_sg_for_device,\n\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\n\nstruct dma_map_ops iommu_coherent_ops = {\n\t.alloc\t\t= arm_iommu_alloc_attrs,\n\t.free\t\t= arm_iommu_free_attrs,\n\t.mmap\t\t= arm_iommu_mmap_attrs,\n\t.get_sgtable\t= arm_iommu_get_sgtable,\n\n\t.map_page\t= arm_coherent_iommu_map_page,\n\t.unmap_page\t= arm_coherent_iommu_unmap_page,\n\n\t.map_sg\t\t= arm_coherent_iommu_map_sg,\n\t.unmap_sg\t= arm_coherent_iommu_unmap_sg,\n\n\t.set_dma_mask\t= arm_dma_set_mask,\n};\n\n/**\n * arm_iommu_create_mapping\n * @bus: pointer to the bus holding the client device (for IOMMU calls)\n * @base: start address of the valid IO address space\n * @size: size of the valid IO address space\n * @order: accuracy of the IO addresses allocations\n *\n * Creates a mapping structure which holds information about used/unused\n * IO address ranges, which is required to perform memory allocation and\n * mapping with IOMMU aware functions.\n *\n * The client device need to be attached to the mapping with\n * arm_iommu_attach_device function.\n */\nstruct dma_iommu_mapping *\narm_iommu_create_mapping(struct bus_type *bus, dma_addr_t base, size_t size,\n\t\t\t int order)\n{\n\tunsigned int count = size >> (PAGE_SHIFT + order);\n\tunsigned int bitmap_size = BITS_TO_LONGS(count) * sizeof(long);\n\tstruct dma_iommu_mapping *mapping;\n\tint err = -ENOMEM;\n\n\tif (!count)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmapping = kzalloc(sizeof(struct dma_iommu_mapping), GFP_KERNEL);\n\tif (!mapping)\n\t\tgoto err;\n\n\tmapping->bitmap = kzalloc(bitmap_size, GFP_KERNEL);\n\tif (!mapping->bitmap)\n\t\tgoto err2;\n\n\tmapping->base = base;\n\tmapping->bits = BITS_PER_BYTE * bitmap_size;\n\tmapping->order = order;\n\tspin_lock_init(&mapping->lock);\n\n\tmapping->domain = iommu_domain_alloc(bus);\n\tif (!mapping->domain)\n\t\tgoto err3;\n\n\tkref_init(&mapping->kref);\n\treturn mapping;\nerr3:\n\tkfree(mapping->bitmap);\nerr2:\n\tkfree(mapping);\nerr:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(arm_iommu_create_mapping);\n\nstatic void release_iommu_mapping(struct kref *kref)\n{\n\tstruct dma_iommu_mapping *mapping =\n\t\tcontainer_of(kref, struct dma_iommu_mapping, kref);\n\n\tiommu_domain_free(mapping->domain);\n\tkfree(mapping->bitmap);\n\tkfree(mapping);\n}\n\nvoid arm_iommu_release_mapping(struct dma_iommu_mapping *mapping)\n{\n\tif (mapping)\n\t\tkref_put(&mapping->kref, release_iommu_mapping);\n}\nEXPORT_SYMBOL_GPL(arm_iommu_release_mapping);\n\n/**\n * arm_iommu_attach_device\n * @dev: valid struct device pointer\n * @mapping: io address space mapping structure (returned from\n *\tarm_iommu_create_mapping)\n *\n * Attaches specified io address space mapping to the provided device,\n * this replaces the dma operations (dma_map_ops pointer) with the\n * IOMMU aware version. More than one client might be attached to\n * the same io address space mapping.\n */\nint arm_iommu_attach_device(struct device *dev,\n\t\t\t    struct dma_iommu_mapping *mapping)\n{\n\tint err;\n\n\terr = iommu_attach_device(mapping->domain, dev);\n\tif (err)\n\t\treturn err;\n\n\tkref_get(&mapping->kref);\n\tdev->archdata.mapping = mapping;\n\tset_dma_ops(dev, &iommu_ops);\n\n\tpr_debug(\"Attached IOMMU controller to %s device.\\n\", dev_name(dev));\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(arm_iommu_attach_device);\n\n/**\n * arm_iommu_detach_device\n * @dev: valid struct device pointer\n *\n * Detaches the provided device from a previously attached map.\n * This voids the dma operations (dma_map_ops pointer)\n */\nvoid arm_iommu_detach_device(struct device *dev)\n{\n\tstruct dma_iommu_mapping *mapping;\n\n\tmapping = to_dma_iommu_mapping(dev);\n\tif (!mapping) {\n\t\tdev_warn(dev, \"Not attached\\n\");\n\t\treturn;\n\t}\n\n\tiommu_detach_device(mapping->domain, dev);\n\tkref_put(&mapping->kref, release_iommu_mapping);\n\tdev->archdata.mapping = NULL;\n\tset_dma_ops(dev, NULL);\n\n\tpr_debug(\"Detached IOMMU controller from %s device.\\n\", dev_name(dev));\n}\nEXPORT_SYMBOL_GPL(arm_iommu_detach_device);\n\n#endif\n"], "filenames": ["arch/arm/mm/dma-mapping.c"], "buggy_code_start_loc": [690], "buggy_code_end_loc": [704], "fixing_code_start_loc": [690], "fixing_code_end_loc": [704], "type": "CWE-264", "message": "arch/arm/mm/dma-mapping.c in the Linux kernel before 3.13 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not prevent executable DMA mappings, which might allow local users to gain privileges via a crafted application, aka Android internal bug 28803642 and Qualcomm internal bug CR642735.", "other": {"cve": {"id": "CVE-2014-9888", "sourceIdentifier": "security@android.com", "published": "2016-08-06T10:59:31.293", "lastModified": "2016-11-28T19:15:30.547", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "arch/arm/mm/dma-mapping.c in the Linux kernel before 3.13 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not prevent executable DMA mappings, which might allow local users to gain privileges via a crafted application, aka Android internal bug 28803642 and Qualcomm internal bug CR642735."}, {"lang": "es", "value": "arch/arm/mm/dma-mapping.c en el kernel de Linux en versiones anteriores 3.13 en las plataformas ARM, como se utiliza en Android en versiones anteriores a 2016-08-05 en dispositivos Nexus 5 y 7 (2013), no impide asignaciones DMA ejecutables, lo que permite a usuarios locales obtener privilegios a trav\u00e9s de una aplicacion manipulada, tambi\u00e9n conocido como error interno de Android 28803642 y error interno de Qualcomm CR642735."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": true, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.12.9", "matchCriteriaId": "9F95E1BD-CF7F-4FE7-A0A5-A342E6C36FB9"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:google:nexus_5:-:*:*:*:*:*:*:*", "matchCriteriaId": "25DB8689-116F-49B5-91F5-BCBA8854BD42"}, {"vulnerable": false, "criteria": "cpe:2.3:h:google:nexus_7:-:*:*:*:*:*:*:*", "matchCriteriaId": "12357E84-6FAE-4C6B-8FD3-BD5457DBBBCE"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=0ea1ec713f04bdfac343c9702b21cd3a7c711826", "source": "security@android.com"}, {"url": "http://source.android.com/security/bulletin/2016-08-01.html", "source": "security@android.com", "tags": ["Mitigation", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/92219", "source": "security@android.com"}, {"url": "https://github.com/torvalds/linux/commit/0ea1ec713f04bdfac343c9702b21cd3a7c711826", "source": "security@android.com", "tags": ["Patch"]}, {"url": "https://source.codeaurora.org/quic/la/kernel/msm/commit/?id=f044936caab337a4384fbfe64a4cbae33c7e22a1", "source": "security@android.com"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0ea1ec713f04bdfac343c9702b21cd3a7c711826"}}