{"buggy_code": ["#! /usr/bin/env perl\n# Copyright 2013-2016 The OpenSSL Project Authors. All Rights Reserved.\n#\n# Licensed under the OpenSSL license (the \"License\").  You may not use\n# this file except in compliance with the License.  You can obtain a copy\n# in the file LICENSE in the source distribution or at\n# https://www.openssl.org/source/license.html\n\n\n##############################################################################\n#                                                                            #\n#  Copyright (c) 2012, Intel Corporation                                     #\n#                                                                            #\n#  All rights reserved.                                                      #\n#                                                                            #\n#  Redistribution and use in source and binary forms, with or without        #\n#  modification, are permitted provided that the following conditions are    #\n#  met:                                                                      #\n#                                                                            #\n#  *  Redistributions of source code must retain the above copyright         #\n#     notice, this list of conditions and the following disclaimer.          #\n#                                                                            #\n#  *  Redistributions in binary form must reproduce the above copyright      #\n#     notice, this list of conditions and the following disclaimer in the    #\n#     documentation and/or other materials provided with the                 #\n#     distribution.                                                          #\n#                                                                            #\n#  *  Neither the name of the Intel Corporation nor the names of its         #\n#     contributors may be used to endorse or promote products derived from   #\n#     this software without specific prior written permission.               #\n#                                                                            #\n#                                                                            #\n#  THIS SOFTWARE IS PROVIDED BY INTEL CORPORATION \"\"AS IS\"\" AND ANY          #\n#  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE         #\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR        #\n#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL INTEL CORPORATION OR            #\n#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,     #\n#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,       #\n#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR        #\n#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF    #\n#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING      #\n#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS        #\n#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.              #\n#                                                                            #\n##############################################################################\n# Developers and authors:                                                    #\n# Shay Gueron (1, 2), and Vlad Krasnov (1)                                   #\n# (1) Intel Corporation, Israel Development Center, Haifa, Israel            #\n# (2) University of Haifa, Israel                                            #\n##############################################################################\n# Reference:                                                                 #\n# [1] S. Gueron, V. Krasnov: \"Software Implementation of Modular             #\n#     Exponentiation,  Using Advanced Vector Instructions Architectures\",    #\n#     F. Ozbudak and F. Rodriguez-Henriquez (Eds.): WAIFI 2012, LNCS 7369,   #\n#     pp. 119?135, 2012. Springer-Verlag Berlin Heidelberg 2012              #\n# [2] S. Gueron: \"Efficient Software Implementations of Modular              #\n#     Exponentiation\", Journal of Cryptographic Engineering 2:31-43 (2012).  #\n# [3] S. Gueron, V. Krasnov: \"Speeding up Big-numbers Squaring\",IEEE         #\n#     Proceedings of 9th International Conference on Information Technology: #\n#     New Generations (ITNG 2012), pp.821-823 (2012)                         #\n# [4] S. Gueron, V. Krasnov: \"[PATCH] Efficient and side channel analysis    #\n#     resistant 1024-bit modular exponentiation, for optimizing RSA2048      #\n#     on AVX2 capable x86_64 platforms\",                                     #\n#     http://rt.openssl.org/Ticket/Display.html?id=2850&user=guest&pass=guest#\n##############################################################################\n#\n# +13% improvement over original submission by <appro@openssl.org>\n#\n# rsa2048 sign/sec\tOpenSSL 1.0.1\tscalar(*)\tthis\n# 2.3GHz Haswell\t621\t\t765/+23%\t1113/+79%\n# 2.3GHz Broadwell(**)\t688\t\t1200(***)/+74%\t1120/+63%\n#\n# (*)\tif system doesn't support AVX2, for reference purposes;\n# (**)\tscaled to 2.3GHz to simplify comparison;\n# (***)\tscalar AD*X code is faster than AVX2 and is preferred code\n#\tpath for Broadwell;\n\n$flavour = shift;\n$output  = shift;\nif ($flavour =~ /\\./) { $output = $flavour; undef $flavour; }\n\n$win64=0; $win64=1 if ($flavour =~ /[nm]asm|mingw64/ || $output =~ /\\.asm$/);\n\n$0 =~ m/(.*[\\/\\\\])[^\\/\\\\]+$/; $dir=$1;\n( $xlate=\"${dir}x86_64-xlate.pl\" and -f $xlate ) or\n( $xlate=\"${dir}../../perlasm/x86_64-xlate.pl\" and -f $xlate) or\ndie \"can't locate x86_64-xlate.pl\";\n\nif (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`\n\t\t=~ /GNU assembler version ([2-9]\\.[0-9]+)/) {\n\t$avx = ($1>=2.19) + ($1>=2.22);\n\t$addx = ($1>=2.23);\n}\n\nif (!$avx && $win64 && ($flavour =~ /nasm/ || $ENV{ASM} =~ /nasm/) &&\n\t    `nasm -v 2>&1` =~ /NASM version ([2-9]\\.[0-9]+)/) {\n\t$avx = ($1>=2.09) + ($1>=2.10);\n\t$addx = ($1>=2.10);\n}\n\nif (!$avx && $win64 && ($flavour =~ /masm/ || $ENV{ASM} =~ /ml64/) &&\n\t    `ml64 2>&1` =~ /Version ([0-9]+)\\./) {\n\t$avx = ($1>=10) + ($1>=11);\n\t$addx = ($1>=11);\n}\n\nif (!$avx && `$ENV{CC} -v 2>&1` =~ /(^clang version|based on LLVM) ([3-9])\\.([0-9]+)/) {\n\tmy $ver = $2 + $3/100.0;\t# 3.1->3.01, 3.10->3.10\n\t$avx = ($ver>=3.0) + ($ver>=3.01);\n\t$addx = ($ver>=3.03);\n}\n\nopen OUT,\"| \\\"$^X\\\" \\\"$xlate\\\" $flavour \\\"$output\\\"\";\n*STDOUT = *OUT;\n\nif ($avx>1) {{{\n{ # void AMS_WW(\nmy $rp=\"%rdi\";\t# BN_ULONG *rp,\nmy $ap=\"%rsi\";\t# const BN_ULONG *ap,\nmy $np=\"%rdx\";\t# const BN_ULONG *np,\nmy $n0=\"%ecx\";\t# const BN_ULONG n0,\nmy $rep=\"%r8d\";\t# int repeat);\n\n# The registers that hold the accumulated redundant result\n# The AMM works on 1024 bit operands, and redundant word size is 29\n# Therefore: ceil(1024/29)/4 = 9\nmy $ACC0=\"%ymm0\";\nmy $ACC1=\"%ymm1\";\nmy $ACC2=\"%ymm2\";\nmy $ACC3=\"%ymm3\";\nmy $ACC4=\"%ymm4\";\nmy $ACC5=\"%ymm5\";\nmy $ACC6=\"%ymm6\";\nmy $ACC7=\"%ymm7\";\nmy $ACC8=\"%ymm8\";\nmy $ACC9=\"%ymm9\";\n# Registers that hold the broadcasted words of bp, currently used\nmy $B1=\"%ymm10\";\nmy $B2=\"%ymm11\";\n# Registers that hold the broadcasted words of Y, currently used\nmy $Y1=\"%ymm12\";\nmy $Y2=\"%ymm13\";\n# Helper registers\nmy $TEMP1=\"%ymm14\";\nmy $AND_MASK=\"%ymm15\";\n# alu registers that hold the first words of the ACC\nmy $r0=\"%r9\";\nmy $r1=\"%r10\";\nmy $r2=\"%r11\";\nmy $r3=\"%r12\";\n\nmy $i=\"%r14d\";\t\t\t# loop counter\nmy $tmp = \"%r15\";\n\nmy $FrameSize=32*18+32*8;\t# place for A^2 and 2*A\n\nmy $aap=$r0;\nmy $tp0=\"%rbx\";\nmy $tp1=$r3;\nmy $tpa=$tmp;\n\n$np=\"%r13\";\t\t\t# reassigned argument\n\n$code.=<<___;\n.text\n\n.globl\trsaz_1024_sqr_avx2\n.type\trsaz_1024_sqr_avx2,\\@function,5\n.align\t64\nrsaz_1024_sqr_avx2:\t\t# 702 cycles, 14% faster than rsaz_1024_mul_avx2\n\tlea\t(%rsp), %rax\n\tpush\t%rbx\n\tpush\t%rbp\n\tpush\t%r12\n\tpush\t%r13\n\tpush\t%r14\n\tpush\t%r15\n\tvzeroupper\n___\n$code.=<<___ if ($win64);\n\tlea\t-0xa8(%rsp),%rsp\n\tvmovaps\t%xmm6,-0xd8(%rax)\n\tvmovaps\t%xmm7,-0xc8(%rax)\n\tvmovaps\t%xmm8,-0xb8(%rax)\n\tvmovaps\t%xmm9,-0xa8(%rax)\n\tvmovaps\t%xmm10,-0x98(%rax)\n\tvmovaps\t%xmm11,-0x88(%rax)\n\tvmovaps\t%xmm12,-0x78(%rax)\n\tvmovaps\t%xmm13,-0x68(%rax)\n\tvmovaps\t%xmm14,-0x58(%rax)\n\tvmovaps\t%xmm15,-0x48(%rax)\n.Lsqr_1024_body:\n___\n$code.=<<___;\n\tmov\t%rax,%rbp\n\tmov\t%rdx, $np\t\t\t# reassigned argument\n\tsub\t\\$$FrameSize, %rsp\n\tmov\t$np, $tmp\n\tsub\t\\$-128, $rp\t\t\t# size optimization\n\tsub\t\\$-128, $ap\n\tsub\t\\$-128, $np\n\n\tand\t\\$4095, $tmp\t\t\t# see if $np crosses page\n\tadd\t\\$32*10, $tmp\n\tshr\t\\$12, $tmp\n\tvpxor\t$ACC9,$ACC9,$ACC9\n\tjz\t.Lsqr_1024_no_n_copy\n\n\t# unaligned 256-bit load that crosses page boundary can\n\t# cause >2x performance degradation here, so if $np does\n\t# cross page boundary, copy it to stack and make sure stack\n\t# frame doesn't...\n\tsub\t\t\\$32*10,%rsp\n\tvmovdqu\t\t32*0-128($np), $ACC0\n\tand\t\t\\$-2048, %rsp\n\tvmovdqu\t\t32*1-128($np), $ACC1\n\tvmovdqu\t\t32*2-128($np), $ACC2\n\tvmovdqu\t\t32*3-128($np), $ACC3\n\tvmovdqu\t\t32*4-128($np), $ACC4\n\tvmovdqu\t\t32*5-128($np), $ACC5\n\tvmovdqu\t\t32*6-128($np), $ACC6\n\tvmovdqu\t\t32*7-128($np), $ACC7\n\tvmovdqu\t\t32*8-128($np), $ACC8\n\tlea\t\t$FrameSize+128(%rsp),$np\n\tvmovdqu\t\t$ACC0, 32*0-128($np)\n\tvmovdqu\t\t$ACC1, 32*1-128($np)\n\tvmovdqu\t\t$ACC2, 32*2-128($np)\n\tvmovdqu\t\t$ACC3, 32*3-128($np)\n\tvmovdqu\t\t$ACC4, 32*4-128($np)\n\tvmovdqu\t\t$ACC5, 32*5-128($np)\n\tvmovdqu\t\t$ACC6, 32*6-128($np)\n\tvmovdqu\t\t$ACC7, 32*7-128($np)\n\tvmovdqu\t\t$ACC8, 32*8-128($np)\n\tvmovdqu\t\t$ACC9, 32*9-128($np)\t# $ACC9 is zero\n\n.Lsqr_1024_no_n_copy:\n\tand\t\t\\$-1024, %rsp\n\n\tvmovdqu\t\t32*1-128($ap), $ACC1\n\tvmovdqu\t\t32*2-128($ap), $ACC2\n\tvmovdqu\t\t32*3-128($ap), $ACC3\n\tvmovdqu\t\t32*4-128($ap), $ACC4\n\tvmovdqu\t\t32*5-128($ap), $ACC5\n\tvmovdqu\t\t32*6-128($ap), $ACC6\n\tvmovdqu\t\t32*7-128($ap), $ACC7\n\tvmovdqu\t\t32*8-128($ap), $ACC8\n\n\tlea\t192(%rsp), $tp0\t\t\t# 64+128=192\n\tvpbroadcastq\t.Land_mask(%rip), $AND_MASK\n\tjmp\t.LOOP_GRANDE_SQR_1024\n\n.align\t32\n.LOOP_GRANDE_SQR_1024:\n\tlea\t32*18+128(%rsp), $aap\t\t# size optimization\n\tlea\t448(%rsp), $tp1\t\t\t# 64+128+256=448\n\n\t# the squaring is performed as described in Variant B of\n\t# \"Speeding up Big-Number Squaring\", so start by calculating\n\t# the A*2=A+A vector\n\tvpaddq\t\t$ACC1, $ACC1, $ACC1\n\t vpbroadcastq\t32*0-128($ap), $B1\n\tvpaddq\t\t$ACC2, $ACC2, $ACC2\n\tvmovdqa\t\t$ACC1, 32*0-128($aap)\n\tvpaddq\t\t$ACC3, $ACC3, $ACC3\n\tvmovdqa\t\t$ACC2, 32*1-128($aap)\n\tvpaddq\t\t$ACC4, $ACC4, $ACC4\n\tvmovdqa\t\t$ACC3, 32*2-128($aap)\n\tvpaddq\t\t$ACC5, $ACC5, $ACC5\n\tvmovdqa\t\t$ACC4, 32*3-128($aap)\n\tvpaddq\t\t$ACC6, $ACC6, $ACC6\n\tvmovdqa\t\t$ACC5, 32*4-128($aap)\n\tvpaddq\t\t$ACC7, $ACC7, $ACC7\n\tvmovdqa\t\t$ACC6, 32*5-128($aap)\n\tvpaddq\t\t$ACC8, $ACC8, $ACC8\n\tvmovdqa\t\t$ACC7, 32*6-128($aap)\n\tvpxor\t\t$ACC9, $ACC9, $ACC9\n\tvmovdqa\t\t$ACC8, 32*7-128($aap)\n\n\tvpmuludq\t32*0-128($ap), $B1, $ACC0\n\t vpbroadcastq\t32*1-128($ap), $B2\n\t vmovdqu\t$ACC9, 32*9-192($tp0)\t# zero upper half\n\tvpmuludq\t$B1, $ACC1, $ACC1\n\t vmovdqu\t$ACC9, 32*10-448($tp1)\n\tvpmuludq\t$B1, $ACC2, $ACC2\n\t vmovdqu\t$ACC9, 32*11-448($tp1)\n\tvpmuludq\t$B1, $ACC3, $ACC3\n\t vmovdqu\t$ACC9, 32*12-448($tp1)\n\tvpmuludq\t$B1, $ACC4, $ACC4\n\t vmovdqu\t$ACC9, 32*13-448($tp1)\n\tvpmuludq\t$B1, $ACC5, $ACC5\n\t vmovdqu\t$ACC9, 32*14-448($tp1)\n\tvpmuludq\t$B1, $ACC6, $ACC6\n\t vmovdqu\t$ACC9, 32*15-448($tp1)\n\tvpmuludq\t$B1, $ACC7, $ACC7\n\t vmovdqu\t$ACC9, 32*16-448($tp1)\n\tvpmuludq\t$B1, $ACC8, $ACC8\n\t vpbroadcastq\t32*2-128($ap), $B1\n\t vmovdqu\t$ACC9, 32*17-448($tp1)\n\n\tmov\t$ap, $tpa\n\tmov \t\\$4, $i\n\tjmp\t.Lsqr_entry_1024\n___\n$TEMP0=$Y1;\n$TEMP2=$Y2;\n$code.=<<___;\n.align\t32\n.LOOP_SQR_1024:\n\t vpbroadcastq\t32*1-128($tpa), $B2\n\tvpmuludq\t32*0-128($ap), $B1, $ACC0\n\tvpaddq\t\t32*0-192($tp0), $ACC0, $ACC0\n\tvpmuludq\t32*0-128($aap), $B1, $ACC1\n\tvpaddq\t\t32*1-192($tp0), $ACC1, $ACC1\n\tvpmuludq\t32*1-128($aap), $B1, $ACC2\n\tvpaddq\t\t32*2-192($tp0), $ACC2, $ACC2\n\tvpmuludq\t32*2-128($aap), $B1, $ACC3\n\tvpaddq\t\t32*3-192($tp0), $ACC3, $ACC3\n\tvpmuludq\t32*3-128($aap), $B1, $ACC4\n\tvpaddq\t\t32*4-192($tp0), $ACC4, $ACC4\n\tvpmuludq\t32*4-128($aap), $B1, $ACC5\n\tvpaddq\t\t32*5-192($tp0), $ACC5, $ACC5\n\tvpmuludq\t32*5-128($aap), $B1, $ACC6\n\tvpaddq\t\t32*6-192($tp0), $ACC6, $ACC6\n\tvpmuludq\t32*6-128($aap), $B1, $ACC7\n\tvpaddq\t\t32*7-192($tp0), $ACC7, $ACC7\n\tvpmuludq\t32*7-128($aap), $B1, $ACC8\n\t vpbroadcastq\t32*2-128($tpa), $B1\n\tvpaddq\t\t32*8-192($tp0), $ACC8, $ACC8\n.Lsqr_entry_1024:\n\tvmovdqu\t\t$ACC0, 32*0-192($tp0)\n\tvmovdqu\t\t$ACC1, 32*1-192($tp0)\n\n\tvpmuludq\t32*1-128($ap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC2, $ACC2\n\tvpmuludq\t32*1-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC3, $ACC3\n\tvpmuludq\t32*2-128($aap), $B2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC4, $ACC4\n\tvpmuludq\t32*3-128($aap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC5, $ACC5\n\tvpmuludq\t32*4-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC6, $ACC6\n\tvpmuludq\t32*5-128($aap), $B2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC7, $ACC7\n\tvpmuludq\t32*6-128($aap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpmuludq\t32*7-128($aap), $B2, $ACC0\n\t vpbroadcastq\t32*3-128($tpa), $B2\n\tvpaddq\t\t32*9-192($tp0), $ACC0, $ACC0\n\n\tvmovdqu\t\t$ACC2, 32*2-192($tp0)\n\tvmovdqu\t\t$ACC3, 32*3-192($tp0)\n\n\tvpmuludq\t32*2-128($ap), $B1, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC4, $ACC4\n\tvpmuludq\t32*2-128($aap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC5, $ACC5\n\tvpmuludq\t32*3-128($aap), $B1, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC6, $ACC6\n\tvpmuludq\t32*4-128($aap), $B1, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC7, $ACC7\n\tvpmuludq\t32*5-128($aap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpmuludq\t32*6-128($aap), $B1, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC0, $ACC0\n\tvpmuludq\t32*7-128($aap), $B1, $ACC1\n\t vpbroadcastq\t32*4-128($tpa), $B1\n\tvpaddq\t\t32*10-448($tp1), $ACC1, $ACC1\n\n\tvmovdqu\t\t$ACC4, 32*4-192($tp0)\n\tvmovdqu\t\t$ACC5, 32*5-192($tp0)\n\n\tvpmuludq\t32*3-128($ap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC6, $ACC6\n\tvpmuludq\t32*3-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC7, $ACC7\n\tvpmuludq\t32*4-128($aap), $B2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC8, $ACC8\n\tvpmuludq\t32*5-128($aap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpmuludq\t32*6-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpmuludq\t32*7-128($aap), $B2, $ACC2\n\t vpbroadcastq\t32*5-128($tpa), $B2\n\tvpaddq\t\t32*11-448($tp1), $ACC2, $ACC2\t\n\n\tvmovdqu\t\t$ACC6, 32*6-192($tp0)\n\tvmovdqu\t\t$ACC7, 32*7-192($tp0)\n\n\tvpmuludq\t32*4-128($ap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpmuludq\t32*4-128($aap), $B1, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC0, $ACC0\n\tvpmuludq\t32*5-128($aap), $B1, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC1, $ACC1\n\tvpmuludq\t32*6-128($aap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC2, $ACC2\n\tvpmuludq\t32*7-128($aap), $B1, $ACC3\n\t vpbroadcastq\t32*6-128($tpa), $B1\n\tvpaddq\t\t32*12-448($tp1), $ACC3, $ACC3\n\n\tvmovdqu\t\t$ACC8, 32*8-192($tp0)\n\tvmovdqu\t\t$ACC0, 32*9-192($tp0)\n\tlea\t\t8($tp0), $tp0\n\n\tvpmuludq\t32*5-128($ap), $B2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC1, $ACC1\n\tvpmuludq\t32*5-128($aap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC2, $ACC2\n\tvpmuludq\t32*6-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC3, $ACC3\n\tvpmuludq\t32*7-128($aap), $B2, $ACC4\n\t vpbroadcastq\t32*7-128($tpa), $B2\n\tvpaddq\t\t32*13-448($tp1), $ACC4, $ACC4\n\n\tvmovdqu\t\t$ACC1, 32*10-448($tp1)\n\tvmovdqu\t\t$ACC2, 32*11-448($tp1)\n\n\tvpmuludq\t32*6-128($ap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC3, $ACC3\n\tvpmuludq\t32*6-128($aap), $B1, $TEMP1\n\t vpbroadcastq\t32*8-128($tpa), $ACC0\t\t# borrow $ACC0 for $B1\n\tvpaddq\t\t$TEMP1, $ACC4, $ACC4\n\tvpmuludq\t32*7-128($aap), $B1, $ACC5\n\t vpbroadcastq\t32*0+8-128($tpa), $B1\t\t# for next iteration\n\tvpaddq\t\t32*14-448($tp1), $ACC5, $ACC5\n\n\tvmovdqu\t\t$ACC3, 32*12-448($tp1)\n\tvmovdqu\t\t$ACC4, 32*13-448($tp1)\n\tlea\t\t8($tpa), $tpa\n\n\tvpmuludq\t32*7-128($ap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC5, $ACC5\n\tvpmuludq\t32*7-128($aap), $B2, $ACC6\n\tvpaddq\t\t32*15-448($tp1), $ACC6, $ACC6\n\n\tvpmuludq\t32*8-128($ap), $ACC0, $ACC7\n\tvmovdqu\t\t$ACC5, 32*14-448($tp1)\n\tvpaddq\t\t32*16-448($tp1), $ACC7, $ACC7\n\tvmovdqu\t\t$ACC6, 32*15-448($tp1)\n\tvmovdqu\t\t$ACC7, 32*16-448($tp1)\n\tlea\t\t8($tp1), $tp1\n\n\tdec\t$i        \n\tjnz\t.LOOP_SQR_1024\n___\n$ZERO = $ACC9;\n$TEMP0 = $B1;\n$TEMP2 = $B2;\n$TEMP3 = $Y1;\n$TEMP4 = $Y2;\n$code.=<<___;\n\t# we need to fix indices 32-39 to avoid overflow\n\tvmovdqu\t\t32*8(%rsp), $ACC8\t\t# 32*8-192($tp0),\n\tvmovdqu\t\t32*9(%rsp), $ACC1\t\t# 32*9-192($tp0)\n\tvmovdqu\t\t32*10(%rsp), $ACC2\t\t# 32*10-192($tp0)\n\tlea\t\t192(%rsp), $tp0\t\t\t# 64+128=192\n\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpxor\t\t$ZERO, $ZERO, $ZERO\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpblendd\t\\$3, $TEMP2, $ZERO, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvmovdqu\t\t$ACC1, 32*9-192($tp0)\n\tvmovdqu\t\t$ACC2, 32*10-192($tp0)\n\n\tmov\t(%rsp), %rax\n\tmov\t8(%rsp), $r1\n\tmov\t16(%rsp), $r2\n\tmov\t24(%rsp), $r3\n\tvmovdqu\t32*1(%rsp), $ACC1\n\tvmovdqu\t32*2-192($tp0), $ACC2\n\tvmovdqu\t32*3-192($tp0), $ACC3\n\tvmovdqu\t32*4-192($tp0), $ACC4\n\tvmovdqu\t32*5-192($tp0), $ACC5\n\tvmovdqu\t32*6-192($tp0), $ACC6\n\tvmovdqu\t32*7-192($tp0), $ACC7\n\n\tmov\t%rax, $r0\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\tvmovd\t%eax, $Y1\n\n\tmov\t%rax, %rdx\n\timulq\t-128($np), %rax\n\t vpbroadcastq\t$Y1, $Y1\n\tadd\t%rax, $r0\n\tmov\t%rdx, %rax\n\timulq\t8-128($np), %rax\n\tshr\t\\$29, $r0\n\tadd\t%rax, $r1\n\tmov\t%rdx, %rax\n\timulq\t16-128($np), %rax\n\tadd\t$r0, $r1\n\tadd\t%rax, $r2\n\timulq\t24-128($np), %rdx\n\tadd\t%rdx, $r3\n\n\tmov\t$r1, %rax\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\tmov \\$9, $i\n\tjmp .LOOP_REDUCE_1024\n\n.align\t32\n.LOOP_REDUCE_1024:\n\tvmovd\t%eax, $Y2\n\tvpbroadcastq\t$Y2, $Y2\n\n\tvpmuludq\t32*1-128($np), $Y1, $TEMP0\n\t mov\t%rax, %rdx\n\t imulq\t-128($np), %rax\n\tvpaddq\t\t$TEMP0, $ACC1, $ACC1\n\t add\t%rax, $r1\n\tvpmuludq\t32*2-128($np), $Y1, $TEMP1\n\t mov\t%rdx, %rax\n\t imulq\t8-128($np), %rax\n\tvpaddq\t\t$TEMP1, $ACC2, $ACC2\n\tvpmuludq\t32*3-128($np), $Y1, $TEMP2\n\t .byte\t0x67\n\t add\t%rax, $r2\n\t .byte\t0x67\n\t mov\t%rdx, %rax\n\t imulq\t16-128($np), %rax\n\t shr\t\\$29, $r1\n\tvpaddq\t\t$TEMP2, $ACC3, $ACC3\n\tvpmuludq\t32*4-128($np), $Y1, $TEMP0\n\t add\t%rax, $r3\n\t add\t$r1, $r2\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpmuludq\t32*5-128($np), $Y1, $TEMP1\n\t mov\t$r2, %rax\n\t imull\t$n0, %eax\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvpmuludq\t32*6-128($np), $Y1, $TEMP2\n\t and\t\\$0x1fffffff, %eax\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvpmuludq\t32*7-128($np), $Y1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC7, $ACC7\n\tvpmuludq\t32*8-128($np), $Y1, $TEMP1\n\t vmovd\t%eax, $Y1\n\t #vmovdqu\t32*1-8-128($np), $TEMP2\t\t# moved below\n\tvpaddq\t\t$TEMP1, $ACC8, $ACC8\n\t #vmovdqu\t32*2-8-128($np), $TEMP0\t\t# moved below\n\t vpbroadcastq\t$Y1, $Y1\n\n\tvpmuludq\t32*1-8-128($np), $Y2, $TEMP2\t# see above\n\tvmovdqu\t\t32*3-8-128($np), $TEMP1\n\t mov\t%rax, %rdx\n\t imulq\t-128($np), %rax\n\tvpaddq\t\t$TEMP2, $ACC1, $ACC1\n\tvpmuludq\t32*2-8-128($np), $Y2, $TEMP0\t# see above\n\tvmovdqu\t\t32*4-8-128($np), $TEMP2\n\t add\t%rax, $r2\n\t mov\t%rdx, %rax\n\t imulq\t8-128($np), %rax\n\tvpaddq\t\t$TEMP0, $ACC2, $ACC2\n\t add\t$r3, %rax\n\t shr\t\\$29, $r2\n\tvpmuludq\t$Y2, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*5-8-128($np), $TEMP0\n\t add\t$r2, %rax\n\tvpaddq\t\t$TEMP1, $ACC3, $ACC3\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*6-8-128($np), $TEMP1\n\t .byte\t0x67\n\t mov\t%rax, $r3\n\t imull\t$n0, %eax\n\tvpaddq\t\t$TEMP2, $ACC4, $ACC4\n\tvpmuludq\t$Y2, $TEMP0, $TEMP0\n\t.byte\t0xc4,0x41,0x7e,0x6f,0x9d,0x58,0x00,0x00,0x00\t# vmovdqu\t\t32*7-8-128($np), $TEMP2\n\t and\t\\$0x1fffffff, %eax\n\tvpaddq\t\t$TEMP0, $ACC5, $ACC5\n\tvpmuludq\t$Y2, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*8-8-128($np), $TEMP0\n\tvpaddq\t\t$TEMP1, $ACC6, $ACC6\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*9-8-128($np), $ACC9\n\t vmovd\t%eax, $ACC0\t\t\t# borrow ACC0 for Y2\n\t imulq\t-128($np), %rax\n\tvpaddq\t\t$TEMP2, $ACC7, $ACC7\n\tvpmuludq\t$Y2, $TEMP0, $TEMP0\n\t vmovdqu\t32*1-16-128($np), $TEMP1\n\t vpbroadcastq\t$ACC0, $ACC0\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpmuludq\t$Y2, $ACC9, $ACC9\n\t vmovdqu\t32*2-16-128($np), $TEMP2\n\t add\t%rax, $r3\n\n___\n($ACC0,$Y2)=($Y2,$ACC0);\n$code.=<<___;\n\t vmovdqu\t32*1-24-128($np), $ACC0\n\tvpmuludq\t$Y1, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*3-16-128($np), $TEMP0\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\t vpmuludq\t$Y2, $ACC0, $ACC0\n\tvpmuludq\t$Y1, $TEMP2, $TEMP2\n\t.byte\t0xc4,0x41,0x7e,0x6f,0xb5,0xf0,0xff,0xff,0xff\t# vmovdqu\t\t32*4-16-128($np), $TEMP1\n\t vpaddq\t\t$ACC1, $ACC0, $ACC0\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvpmuludq\t$Y1, $TEMP0, $TEMP0\n\tvmovdqu\t\t32*5-16-128($np), $TEMP2\n\t .byte\t0x67\n\t vmovq\t\t$ACC0, %rax\n\t vmovdqu\t$ACC0, (%rsp)\t\t# transfer $r0-$r3\n\tvpaddq\t\t$TEMP0, $ACC3, $ACC3\n\tvpmuludq\t$Y1, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*6-16-128($np), $TEMP0\n\tvpaddq\t\t$TEMP1, $ACC4, $ACC4\n\tvpmuludq\t$Y1, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*7-16-128($np), $TEMP1\n\tvpaddq\t\t$TEMP2, $ACC5, $ACC5\n\tvpmuludq\t$Y1, $TEMP0, $TEMP0\n\tvmovdqu\t\t32*8-16-128($np), $TEMP2\n\tvpaddq\t\t$TEMP0, $ACC6, $ACC6\n\tvpmuludq\t$Y1, $TEMP1, $TEMP1\n\t shr\t\\$29, $r3\n\tvmovdqu\t\t32*9-16-128($np), $TEMP0\n\t add\t$r3, %rax\n\tvpaddq\t\t$TEMP1, $ACC7, $ACC7\n\tvpmuludq\t$Y1, $TEMP2, $TEMP2\n\t #vmovdqu\t32*2-24-128($np), $TEMP1\t# moved below\n\t mov\t%rax, $r0\n\t imull\t$n0, %eax\n\tvpaddq\t\t$TEMP2, $ACC8, $ACC8\n\tvpmuludq\t$Y1, $TEMP0, $TEMP0\n\t and\t\\$0x1fffffff, %eax\n\t vmovd\t%eax, $Y1\n\t vmovdqu\t32*3-24-128($np), $TEMP2\n\t.byte\t0x67\n\tvpaddq\t\t$TEMP0, $ACC9, $ACC9\n\t vpbroadcastq\t$Y1, $Y1\n\n\tvpmuludq\t32*2-24-128($np), $Y2, $TEMP1\t# see above\n\tvmovdqu\t\t32*4-24-128($np), $TEMP0\n\t mov\t%rax, %rdx\n\t imulq\t-128($np), %rax\n\t mov\t8(%rsp), $r1\n\tvpaddq\t\t$TEMP1, $ACC2, $ACC1\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*5-24-128($np), $TEMP1\n\t add\t%rax, $r0\n\t mov\t%rdx, %rax\n\t imulq\t8-128($np), %rax\n\t .byte\t0x67\n\t shr\t\\$29, $r0\n\t mov\t16(%rsp), $r2\n\tvpaddq\t\t$TEMP2, $ACC3, $ACC2\n\tvpmuludq\t$Y2, $TEMP0, $TEMP0\n\tvmovdqu\t\t32*6-24-128($np), $TEMP2\n\t add\t%rax, $r1\n\t mov\t%rdx, %rax\n\t imulq\t16-128($np), %rax\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC3\n\tvpmuludq\t$Y2, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*7-24-128($np), $TEMP0\n\t imulq\t24-128($np), %rdx\t\t# future $r3\n\t add\t%rax, $r2\n\t lea\t($r0,$r1), %rax\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC4\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*8-24-128($np), $TEMP1\n\t mov\t%rax, $r1\n\t imull\t$n0, %eax\n\tvpmuludq\t$Y2, $TEMP0, $TEMP0\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC5\n\tvmovdqu\t\t32*9-24-128($np), $TEMP2\n\t and\t\\$0x1fffffff, %eax\n\tvpaddq\t\t$TEMP0, $ACC7, $ACC6\n\tvpmuludq\t$Y2, $TEMP1, $TEMP1\n\t add\t24(%rsp), %rdx\n\tvpaddq\t\t$TEMP1, $ACC8, $ACC7\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC9, $ACC8\n\t vmovq\t$r3, $ACC9\n\t mov\t%rdx, $r3\n\n\tdec\t$i\n\tjnz\t.LOOP_REDUCE_1024\n___\n($ACC0,$Y2)=($Y2,$ACC0);\n$code.=<<___;\n\tlea\t448(%rsp), $tp1\t\t\t# size optimization\n\tvpaddq\t$ACC9, $Y2, $ACC0\n\tvpxor\t$ZERO, $ZERO, $ZERO\n\n\tvpaddq\t\t32*9-192($tp0), $ACC0, $ACC0\n\tvpaddq\t\t32*10-448($tp1), $ACC1, $ACC1\n\tvpaddq\t\t32*11-448($tp1), $ACC2, $ACC2\n\tvpaddq\t\t32*12-448($tp1), $ACC3, $ACC3\n\tvpaddq\t\t32*13-448($tp1), $ACC4, $ACC4\n\tvpaddq\t\t32*14-448($tp1), $ACC5, $ACC5\n\tvpaddq\t\t32*15-448($tp1), $ACC6, $ACC6\n\tvpaddq\t\t32*16-448($tp1), $ACC7, $ACC7\n\tvpaddq\t\t32*17-448($tp1), $ACC8, $ACC8\n\n\tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC0, $ACC0\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\tvpsrlq\t\t\\$29, $ACC2, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC2, $ACC2\n\tvpsrlq\t\t\\$29, $ACC3, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC3, $ACC3\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvpblendd\t\\$3, $TEMP4, $ZERO, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC3, $ACC3\n\tvpaddq\t\t$TEMP4, $ACC4, $ACC4\n\n\tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC0, $ACC0\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\tvpsrlq\t\t\\$29, $ACC2, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC2, $ACC2\n\tvpsrlq\t\t\\$29, $ACC3, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC3, $ACC3\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvmovdqu\t\t$ACC0, 32*0-128($rp)\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvmovdqu\t\t$ACC1, 32*1-128($rp)\n\tvpblendd\t\\$3, $TEMP4, $ZERO, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC3, $ACC3\n\tvmovdqu\t\t$ACC2, 32*2-128($rp)\n\tvpaddq\t\t$TEMP4, $ACC4, $ACC4\n\tvmovdqu\t\t$ACC3, 32*3-128($rp)\n___\n$TEMP5=$ACC0;\n$code.=<<___;\n\tvpsrlq\t\t\\$29, $ACC4, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC4, $ACC4\n\tvpsrlq\t\t\\$29, $ACC5, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC5, $ACC5\n\tvpsrlq\t\t\\$29, $ACC6, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC6, $ACC6\n\tvpsrlq\t\t\\$29, $ACC7, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC7, $ACC7\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP5\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP5, $TEMP5\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvpblendd\t\\$3, $TEMP4, $TEMP5, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC7, $ACC7\n\tvpaddq\t\t$TEMP4, $ACC8, $ACC8\n     \n\tvpsrlq\t\t\\$29, $ACC4, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC4, $ACC4\n\tvpsrlq\t\t\\$29, $ACC5, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC5, $ACC5\n\tvpsrlq\t\t\\$29, $ACC6, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC6, $ACC6\n\tvpsrlq\t\t\\$29, $ACC7, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC7, $ACC7\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP5\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP5, $TEMP5\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvmovdqu\t\t$ACC4, 32*4-128($rp)\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvmovdqu\t\t$ACC5, 32*5-128($rp)\n\tvpblendd\t\\$3, $TEMP4, $TEMP5, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC7, $ACC7\n\tvmovdqu\t\t$ACC6, 32*6-128($rp)\n\tvpaddq\t\t$TEMP4, $ACC8, $ACC8\n\tvmovdqu\t\t$ACC7, 32*7-128($rp)\n\tvmovdqu\t\t$ACC8, 32*8-128($rp)\n\n\tmov\t$rp, $ap\n\tdec\t$rep\n\tjne\t.LOOP_GRANDE_SQR_1024\n\n\tvzeroall\n\tmov\t%rbp, %rax\n___\n$code.=<<___ if ($win64);\n\tmovaps\t-0xd8(%rax),%xmm6\n\tmovaps\t-0xc8(%rax),%xmm7\n\tmovaps\t-0xb8(%rax),%xmm8\n\tmovaps\t-0xa8(%rax),%xmm9\n\tmovaps\t-0x98(%rax),%xmm10\n\tmovaps\t-0x88(%rax),%xmm11\n\tmovaps\t-0x78(%rax),%xmm12\n\tmovaps\t-0x68(%rax),%xmm13\n\tmovaps\t-0x58(%rax),%xmm14\n\tmovaps\t-0x48(%rax),%xmm15\n___\n$code.=<<___;\n\tmov\t-48(%rax),%r15\n\tmov\t-40(%rax),%r14\n\tmov\t-32(%rax),%r13\n\tmov\t-24(%rax),%r12\n\tmov\t-16(%rax),%rbp\n\tmov\t-8(%rax),%rbx\n\tlea\t(%rax),%rsp\t\t# restore %rsp\n.Lsqr_1024_epilogue:\n\tret\n.size\trsaz_1024_sqr_avx2,.-rsaz_1024_sqr_avx2\n___\n}\n\n{ # void AMM_WW(\nmy $rp=\"%rdi\";\t# BN_ULONG *rp,\nmy $ap=\"%rsi\";\t# const BN_ULONG *ap,\nmy $bp=\"%rdx\";\t# const BN_ULONG *bp,\nmy $np=\"%rcx\";\t# const BN_ULONG *np,\nmy $n0=\"%r8d\";\t# unsigned int n0);\n\n# The registers that hold the accumulated redundant result\n# The AMM works on 1024 bit operands, and redundant word size is 29\n# Therefore: ceil(1024/29)/4 = 9\nmy $ACC0=\"%ymm0\";\nmy $ACC1=\"%ymm1\";\nmy $ACC2=\"%ymm2\";\nmy $ACC3=\"%ymm3\";\nmy $ACC4=\"%ymm4\";\nmy $ACC5=\"%ymm5\";\nmy $ACC6=\"%ymm6\";\nmy $ACC7=\"%ymm7\";\nmy $ACC8=\"%ymm8\";\nmy $ACC9=\"%ymm9\";\n\n# Registers that hold the broadcasted words of multiplier, currently used\nmy $Bi=\"%ymm10\";\nmy $Yi=\"%ymm11\";\n\n# Helper registers\nmy $TEMP0=$ACC0;\nmy $TEMP1=\"%ymm12\";\nmy $TEMP2=\"%ymm13\";\nmy $ZERO=\"%ymm14\";\nmy $AND_MASK=\"%ymm15\";\n\n# alu registers that hold the first words of the ACC\nmy $r0=\"%r9\";\nmy $r1=\"%r10\";\nmy $r2=\"%r11\";\nmy $r3=\"%r12\";\n\nmy $i=\"%r14d\";\nmy $tmp=\"%r15\";\n\n$bp=\"%r13\";\t# reassigned argument\n\n$code.=<<___;\n.globl\trsaz_1024_mul_avx2\n.type\trsaz_1024_mul_avx2,\\@function,5\n.align\t64\nrsaz_1024_mul_avx2:\n\tlea\t(%rsp), %rax\n\tpush\t%rbx\n\tpush\t%rbp\n\tpush\t%r12\n\tpush\t%r13\n\tpush\t%r14\n\tpush\t%r15\n___\n$code.=<<___ if ($win64);\n\tvzeroupper\n\tlea\t-0xa8(%rsp),%rsp\n\tvmovaps\t%xmm6,-0xd8(%rax)\n\tvmovaps\t%xmm7,-0xc8(%rax)\n\tvmovaps\t%xmm8,-0xb8(%rax)\n\tvmovaps\t%xmm9,-0xa8(%rax)\n\tvmovaps\t%xmm10,-0x98(%rax)\n\tvmovaps\t%xmm11,-0x88(%rax)\n\tvmovaps\t%xmm12,-0x78(%rax)\n\tvmovaps\t%xmm13,-0x68(%rax)\n\tvmovaps\t%xmm14,-0x58(%rax)\n\tvmovaps\t%xmm15,-0x48(%rax)\n.Lmul_1024_body:\n___\n$code.=<<___;\n\tmov\t%rax,%rbp\n\tvzeroall\n\tmov\t%rdx, $bp\t# reassigned argument\n\tsub\t\\$64,%rsp\n\n\t# unaligned 256-bit load that crosses page boundary can\n\t# cause severe performance degradation here, so if $ap does\n\t# cross page boundary, swap it with $bp [meaning that caller\n\t# is advised to lay down $ap and $bp next to each other, so\n\t# that only one can cross page boundary].\n\t.byte\t0x67,0x67\n\tmov\t$ap, $tmp\n\tand\t\\$4095, $tmp\n\tadd\t\\$32*10, $tmp\n\tshr\t\\$12, $tmp\n\tmov\t$ap, $tmp\n\tcmovnz\t$bp, $ap\n\tcmovnz\t$tmp, $bp\n\n\tmov\t$np, $tmp\n\tsub\t\\$-128,$ap\t# size optimization\n\tsub\t\\$-128,$np\n\tsub\t\\$-128,$rp\n\n\tand\t\\$4095, $tmp\t# see if $np crosses page\n\tadd\t\\$32*10, $tmp\n\t.byte\t0x67,0x67\n\tshr\t\\$12, $tmp\n\tjz\t.Lmul_1024_no_n_copy\n\n\t# unaligned 256-bit load that crosses page boundary can\n\t# cause severe performance degradation here, so if $np does\n\t# cross page boundary, copy it to stack and make sure stack\n\t# frame doesn't...\n\tsub\t\t\\$32*10,%rsp\n\tvmovdqu\t\t32*0-128($np), $ACC0\n\tand\t\t\\$-512, %rsp\n\tvmovdqu\t\t32*1-128($np), $ACC1\n\tvmovdqu\t\t32*2-128($np), $ACC2\n\tvmovdqu\t\t32*3-128($np), $ACC3\n\tvmovdqu\t\t32*4-128($np), $ACC4\n\tvmovdqu\t\t32*5-128($np), $ACC5\n\tvmovdqu\t\t32*6-128($np), $ACC6\n\tvmovdqu\t\t32*7-128($np), $ACC7\n\tvmovdqu\t\t32*8-128($np), $ACC8\n\tlea\t\t64+128(%rsp),$np\n\tvmovdqu\t\t$ACC0, 32*0-128($np)\n\tvpxor\t\t$ACC0, $ACC0, $ACC0\n\tvmovdqu\t\t$ACC1, 32*1-128($np)\n\tvpxor\t\t$ACC1, $ACC1, $ACC1\n\tvmovdqu\t\t$ACC2, 32*2-128($np)\n\tvpxor\t\t$ACC2, $ACC2, $ACC2\n\tvmovdqu\t\t$ACC3, 32*3-128($np)\n\tvpxor\t\t$ACC3, $ACC3, $ACC3\n\tvmovdqu\t\t$ACC4, 32*4-128($np)\n\tvpxor\t\t$ACC4, $ACC4, $ACC4\n\tvmovdqu\t\t$ACC5, 32*5-128($np)\n\tvpxor\t\t$ACC5, $ACC5, $ACC5\n\tvmovdqu\t\t$ACC6, 32*6-128($np)\n\tvpxor\t\t$ACC6, $ACC6, $ACC6\n\tvmovdqu\t\t$ACC7, 32*7-128($np)\n\tvpxor\t\t$ACC7, $ACC7, $ACC7\n\tvmovdqu\t\t$ACC8, 32*8-128($np)\n\tvmovdqa\t\t$ACC0, $ACC8\n\tvmovdqu\t\t$ACC9, 32*9-128($np)\t# $ACC9 is zero after vzeroall\n.Lmul_1024_no_n_copy:\n\tand\t\\$-64,%rsp\n\n\tmov\t($bp), %rbx\n\tvpbroadcastq ($bp), $Bi\n\tvmovdqu\t$ACC0, (%rsp)\t\t\t# clear top of stack\n\txor\t$r0, $r0\n\t.byte\t0x67\n\txor\t$r1, $r1\n\txor\t$r2, $r2\n\txor\t$r3, $r3\n\n\tvmovdqu\t.Land_mask(%rip), $AND_MASK\n\tmov\t\\$9, $i\n\tvmovdqu\t$ACC9, 32*9-128($rp)\t\t# $ACC9 is zero after vzeroall\n\tjmp\t.Loop_mul_1024\n\n.align\t32\n.Loop_mul_1024:\n\t vpsrlq\t\t\\$29, $ACC3, $ACC9\t\t# correct $ACC3(*)\n\tmov\t%rbx, %rax\n\timulq\t-128($ap), %rax\n\tadd\t$r0, %rax\n\tmov\t%rbx, $r1\n\timulq\t8-128($ap), $r1\n\tadd\t8(%rsp), $r1\n\n\tmov\t%rax, $r0\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\t mov\t%rbx, $r2\n\t imulq\t16-128($ap), $r2\n\t add\t16(%rsp), $r2\n\n\t mov\t%rbx, $r3\n\t imulq\t24-128($ap), $r3\n\t add\t24(%rsp), $r3\n\tvpmuludq\t32*1-128($ap),$Bi,$TEMP0\n\t vmovd\t\t%eax, $Yi\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t32*2-128($ap),$Bi,$TEMP1\n\t vpbroadcastq\t$Yi, $Yi\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t32*3-128($ap),$Bi,$TEMP2\n\t vpand\t\t$AND_MASK, $ACC3, $ACC3\t\t# correct $ACC3\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t32*4-128($ap),$Bi,$TEMP0\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t32*5-128($ap),$Bi,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t32*6-128($ap),$Bi,$TEMP2\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t32*7-128($ap),$Bi,$TEMP0\n\t vpermq\t\t\\$0x93, $ACC9, $ACC9\t\t# correct $ACC3\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t32*8-128($ap),$Bi,$TEMP1\n\t vpbroadcastq\t8($bp), $Bi\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\n\tmov\t%rax,%rdx\n\timulq\t-128($np),%rax\n\tadd\t%rax,$r0\n\tmov\t%rdx,%rax\n\timulq\t8-128($np),%rax\n\tadd\t%rax,$r1\n\tmov\t%rdx,%rax\n\timulq\t16-128($np),%rax\n\tadd\t%rax,$r2\n\tshr\t\\$29, $r0\n\timulq\t24-128($np),%rdx\n\tadd\t%rdx,$r3\n\tadd\t$r0, $r1\n\n\tvpmuludq\t32*1-128($np),$Yi,$TEMP2\n\t vmovq\t\t$Bi, %rbx\n\tvpaddq\t\t$TEMP2,$ACC1,$ACC1\n\tvpmuludq\t32*2-128($np),$Yi,$TEMP0\n\tvpaddq\t\t$TEMP0,$ACC2,$ACC2\n\tvpmuludq\t32*3-128($np),$Yi,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC3,$ACC3\n\tvpmuludq\t32*4-128($np),$Yi,$TEMP2\n\tvpaddq\t\t$TEMP2,$ACC4,$ACC4\n\tvpmuludq\t32*5-128($np),$Yi,$TEMP0\n\tvpaddq\t\t$TEMP0,$ACC5,$ACC5\n\tvpmuludq\t32*6-128($np),$Yi,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC6,$ACC6\n\tvpmuludq\t32*7-128($np),$Yi,$TEMP2\n\t vpblendd\t\\$3, $ZERO, $ACC9, $ACC9\t# correct $ACC3\n\tvpaddq\t\t$TEMP2,$ACC7,$ACC7\n\tvpmuludq\t32*8-128($np),$Yi,$TEMP0\n\t vpaddq\t\t$ACC9, $ACC3, $ACC3\t\t# correct $ACC3\n\tvpaddq\t\t$TEMP0,$ACC8,$ACC8\n\n\tmov\t%rbx, %rax\n\timulq\t-128($ap),%rax\n\tadd\t%rax,$r1\n\t vmovdqu\t-8+32*1-128($ap),$TEMP1\n\tmov\t%rbx, %rax\n\timulq\t8-128($ap),%rax\n\tadd\t%rax,$r2\n\t vmovdqu\t-8+32*2-128($ap),$TEMP2\n\n\tmov\t$r1, %rax\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\t imulq\t16-128($ap),%rbx\n\t add\t%rbx,$r3\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\t vmovd\t\t%eax, $Yi\n\tvmovdqu\t\t-8+32*3-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC1,$ACC1\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\t vpbroadcastq\t$Yi, $Yi\n\tvmovdqu\t\t-8+32*4-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC2,$ACC2\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-8+32*5-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC3,$ACC3\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-8+32*6-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC4,$ACC4\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-8+32*7-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC5,$ACC5\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-8+32*8-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC6,$ACC6\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-8+32*9-128($ap),$ACC9\n\tvpaddq\t\t$TEMP1,$ACC7,$ACC7\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvpaddq\t\t$TEMP2,$ACC8,$ACC8\n\tvpmuludq\t$Bi,$ACC9,$ACC9\n\t vpbroadcastq\t16($bp), $Bi\n\n\tmov\t%rax,%rdx\n\timulq\t-128($np),%rax\n\tadd\t%rax,$r1\n\t vmovdqu\t-8+32*1-128($np),$TEMP0\n\tmov\t%rdx,%rax\n\timulq\t8-128($np),%rax\n\tadd\t%rax,$r2\n\t vmovdqu\t-8+32*2-128($np),$TEMP1\n\tshr\t\\$29, $r1\n\timulq\t16-128($np),%rdx\n\tadd\t%rdx,$r3\n\tadd\t$r1, $r2\n\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\t vmovq\t\t$Bi, %rbx\n\tvmovdqu\t\t-8+32*3-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-8+32*4-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-8+32*5-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-8+32*6-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-8+32*7-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-8+32*8-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-8+32*9-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC9\n\n\t vmovdqu\t-16+32*1-128($ap),$TEMP0\n\tmov\t%rbx,%rax\n\timulq\t-128($ap),%rax\n\tadd\t$r2,%rax\n\n\t vmovdqu\t-16+32*2-128($ap),$TEMP1\n\tmov\t%rax,$r2\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\t imulq\t8-128($ap),%rbx\n\t add\t%rbx,$r3\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\t vmovd\t\t%eax, $Yi\n\tvmovdqu\t\t-16+32*3-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\t vpbroadcastq\t$Yi, $Yi\n\tvmovdqu\t\t-16+32*4-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-16+32*5-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-16+32*6-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-16+32*7-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-16+32*8-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-16+32*9-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\t vpbroadcastq\t24($bp), $Bi\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC9\n\n\t vmovdqu\t-16+32*1-128($np),$TEMP0\n\tmov\t%rax,%rdx\n\timulq\t-128($np),%rax\n\tadd\t%rax,$r2\n\t vmovdqu\t-16+32*2-128($np),$TEMP1\n\timulq\t8-128($np),%rdx\n\tadd\t%rdx,$r3\n\tshr\t\\$29, $r2\n\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\t vmovq\t\t$Bi, %rbx\n\tvmovdqu\t\t-16+32*3-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-16+32*4-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-16+32*5-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-16+32*6-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-16+32*7-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-16+32*8-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-16+32*9-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\t vmovdqu\t-24+32*1-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\t vmovdqu\t-24+32*2-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC9\n\n\tadd\t$r2, $r3\n\timulq\t-128($ap),%rbx\n\tadd\t%rbx,$r3\n\n\tmov\t$r3, %rax\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\t vmovd\t\t%eax, $Yi\n\tvmovdqu\t\t-24+32*3-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\t vpbroadcastq\t$Yi, $Yi\n\tvmovdqu\t\t-24+32*4-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-24+32*5-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-24+32*6-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-24+32*7-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-24+32*8-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-24+32*9-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\t vpbroadcastq\t32($bp), $Bi\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC9\n\t add\t\t\\$32, $bp\t\t\t# $bp++\n\n\tvmovdqu\t\t-24+32*1-128($np),$TEMP0\n\timulq\t-128($np),%rax\n\tadd\t%rax,$r3\n\tshr\t\\$29, $r3\n\n\tvmovdqu\t\t-24+32*2-128($np),$TEMP1\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\t vmovq\t\t$Bi, %rbx\n\tvmovdqu\t\t-24+32*3-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC0\t\t# $ACC0==$TEMP0\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\t vmovdqu\t$ACC0, (%rsp)\t\t\t# transfer $r0-$r3\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC1\n\tvmovdqu\t\t-24+32*4-128($np),$TEMP0\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-24+32*5-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC2\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-24+32*6-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC3\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-24+32*7-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC4\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-24+32*8-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC5\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-24+32*9-128($np),$TEMP2\n\t mov\t$r3, $r0\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC6\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\t add\t(%rsp), $r0\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC7\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\t vmovq\t$r3, $TEMP1\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC8\n\n\tdec\t$i\n\tjnz\t.Loop_mul_1024\n___\n\n# (*)\tOriginal implementation was correcting ACC1-ACC3 for overflow\n#\tafter 7 loop runs, or after 28 iterations, or 56 additions.\n#\tBut as we underutilize resources, it's possible to correct in\n#\teach iteration with marginal performance loss. But then, as\n#\twe do it in each iteration, we can correct less digits, and\n#\tavoid performance penalties completely. Also note that we\n#\tcorrect only three digits out of four. This works because\n#\tmost significant digit is subjected to less additions.\n\n$TEMP0 = $ACC9;\n$TEMP3 = $Bi;\n$TEMP4 = $Yi;\n$code.=<<___;\n\tvpermq\t\t\\$0, $AND_MASK, $AND_MASK\n\tvpaddq\t\t(%rsp), $TEMP1, $ACC0\n\n\tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC0, $ACC0\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\tvpsrlq\t\t\\$29, $ACC2, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC2, $ACC2\n\tvpsrlq\t\t\\$29, $ACC3, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC3, $ACC3\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvpblendd\t\\$3, $TEMP4, $ZERO, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC3, $ACC3\n\tvpaddq\t\t$TEMP4, $ACC4, $ACC4\n\n\tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC0, $ACC0\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\tvpsrlq\t\t\\$29, $ACC2, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC2, $ACC2\n\tvpsrlq\t\t\\$29, $ACC3, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC3, $ACC3\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvpblendd\t\\$3, $TEMP4, $ZERO, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC3, $ACC3\n\tvpaddq\t\t$TEMP4, $ACC4, $ACC4\n\n\tvmovdqu\t\t$ACC0, 0-128($rp)\n\tvmovdqu\t\t$ACC1, 32-128($rp)\n\tvmovdqu\t\t$ACC2, 64-128($rp)\n\tvmovdqu\t\t$ACC3, 96-128($rp)\n___\n\n$TEMP5=$ACC0;\n$code.=<<___;\n\tvpsrlq\t\t\\$29, $ACC4, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC4, $ACC4\n\tvpsrlq\t\t\\$29, $ACC5, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC5, $ACC5\n\tvpsrlq\t\t\\$29, $ACC6, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC6, $ACC6\n\tvpsrlq\t\t\\$29, $ACC7, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC7, $ACC7\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP5\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP5, $TEMP5\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvpblendd\t\\$3, $TEMP4, $TEMP5, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC7, $ACC7\n\tvpaddq\t\t$TEMP4, $ACC8, $ACC8\n\n\tvpsrlq\t\t\\$29, $ACC4, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC4, $ACC4\n\tvpsrlq\t\t\\$29, $ACC5, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC5, $ACC5\n\tvpsrlq\t\t\\$29, $ACC6, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC6, $ACC6\n\tvpsrlq\t\t\\$29, $ACC7, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC7, $ACC7\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP5\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP5, $TEMP5\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvpblendd\t\\$3, $TEMP4, $TEMP5, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC7, $ACC7\n\tvpaddq\t\t$TEMP4, $ACC8, $ACC8\n\n\tvmovdqu\t\t$ACC4, 128-128($rp)\n\tvmovdqu\t\t$ACC5, 160-128($rp)    \n\tvmovdqu\t\t$ACC6, 192-128($rp)\n\tvmovdqu\t\t$ACC7, 224-128($rp)\n\tvmovdqu\t\t$ACC8, 256-128($rp)\n\tvzeroupper\n\n\tmov\t%rbp, %rax\n___\n$code.=<<___ if ($win64);\n\tmovaps\t-0xd8(%rax),%xmm6\n\tmovaps\t-0xc8(%rax),%xmm7\n\tmovaps\t-0xb8(%rax),%xmm8\n\tmovaps\t-0xa8(%rax),%xmm9\n\tmovaps\t-0x98(%rax),%xmm10\n\tmovaps\t-0x88(%rax),%xmm11\n\tmovaps\t-0x78(%rax),%xmm12\n\tmovaps\t-0x68(%rax),%xmm13\n\tmovaps\t-0x58(%rax),%xmm14\n\tmovaps\t-0x48(%rax),%xmm15\n___\n$code.=<<___;\n\tmov\t-48(%rax),%r15\n\tmov\t-40(%rax),%r14\n\tmov\t-32(%rax),%r13\n\tmov\t-24(%rax),%r12\n\tmov\t-16(%rax),%rbp\n\tmov\t-8(%rax),%rbx\n\tlea\t(%rax),%rsp\t\t# restore %rsp\n.Lmul_1024_epilogue:\n\tret\n.size\trsaz_1024_mul_avx2,.-rsaz_1024_mul_avx2\n___\n}\n{\nmy ($out,$inp) = $win64 ? (\"%rcx\",\"%rdx\") : (\"%rdi\",\"%rsi\");\nmy @T = map(\"%r$_\",(8..11));\n\n$code.=<<___;\n.globl\trsaz_1024_red2norm_avx2\n.type\trsaz_1024_red2norm_avx2,\\@abi-omnipotent\n.align\t32\nrsaz_1024_red2norm_avx2:\n\tsub\t\\$-128,$inp\t# size optimization\n\txor\t%rax,%rax\n___\n\nfor ($j=0,$i=0; $i<16; $i++) {\n    my $k=0;\n    while (29*$j<64*($i+1)) {\t# load data till boundary\n\t$code.=\"\tmov\t`8*$j-128`($inp), @T[0]\\n\";\n\t$j++; $k++; push(@T,shift(@T));\n    }\n    $l=$k;\n    while ($k>1) {\t\t# shift loaded data but last value\n\t$code.=\"\tshl\t\\$`29*($j-$k)`,@T[-$k]\\n\";\n\t$k--;\n    }\n    $code.=<<___;\t\t# shift last value\n\tmov\t@T[-1], @T[0]\n\tshl\t\\$`29*($j-1)`, @T[-1]\n\tshr\t\\$`-29*($j-1)`, @T[0]\n___\n    while ($l) {\t\t# accumulate all values\n\t$code.=\"\tadd\t@T[-$l], %rax\\n\";\n\t$l--;\n    }\n\t$code.=<<___;\n\tadc\t\\$0, @T[0]\t# consume eventual carry\n\tmov\t%rax, 8*$i($out)\n\tmov\t@T[0], %rax\n___\n    push(@T,shift(@T));\n}\n$code.=<<___;\n\tret\n.size\trsaz_1024_red2norm_avx2,.-rsaz_1024_red2norm_avx2\n\n.globl\trsaz_1024_norm2red_avx2\n.type\trsaz_1024_norm2red_avx2,\\@abi-omnipotent\n.align\t32\nrsaz_1024_norm2red_avx2:\n\tsub\t\\$-128,$out\t# size optimization\n\tmov\t($inp),@T[0]\n\tmov\t\\$0x1fffffff,%eax\n___\nfor ($j=0,$i=0; $i<16; $i++) {\n    $code.=\"\tmov\t`8*($i+1)`($inp),@T[1]\\n\"\tif ($i<15);\n    $code.=\"\txor\t@T[1],@T[1]\\n\"\t\t\tif ($i==15);\n    my $k=1;\n    while (29*($j+1)<64*($i+1)) {\n    \t$code.=<<___;\n\tmov\t@T[0],@T[-$k]\n\tshr\t\\$`29*$j`,@T[-$k]\n\tand\t%rax,@T[-$k]\t\t\t\t# &0x1fffffff\n\tmov\t@T[-$k],`8*$j-128`($out)\n___\n\t$j++; $k++;\n    }\n    $code.=<<___;\n\tshrd\t\\$`29*$j`,@T[1],@T[0]\n\tand\t%rax,@T[0]\n\tmov\t@T[0],`8*$j-128`($out)\n___\n    $j++;\n    push(@T,shift(@T));\n}\n$code.=<<___;\n\tmov\t@T[0],`8*$j-128`($out)\t\t\t# zero\n\tmov\t@T[0],`8*($j+1)-128`($out)\n\tmov\t@T[0],`8*($j+2)-128`($out)\n\tmov\t@T[0],`8*($j+3)-128`($out)\n\tret\n.size\trsaz_1024_norm2red_avx2,.-rsaz_1024_norm2red_avx2\n___\n}\n{\nmy ($out,$inp,$power) = $win64 ? (\"%rcx\",\"%rdx\",\"%r8d\") : (\"%rdi\",\"%rsi\",\"%edx\");\n\n$code.=<<___;\n.globl\trsaz_1024_scatter5_avx2\n.type\trsaz_1024_scatter5_avx2,\\@abi-omnipotent\n.align\t32\nrsaz_1024_scatter5_avx2:\n\tvzeroupper\n\tvmovdqu\t.Lscatter_permd(%rip),%ymm5\n\tshl\t\\$4,$power\n\tlea\t($out,$power),$out\n\tmov\t\\$9,%eax\n\tjmp\t.Loop_scatter_1024\n\n.align\t32\n.Loop_scatter_1024:\n\tvmovdqu\t\t($inp),%ymm0\n\tlea\t\t32($inp),$inp\n\tvpermd\t\t%ymm0,%ymm5,%ymm0\n\tvmovdqu\t\t%xmm0,($out)\n\tlea\t\t16*32($out),$out\n\tdec\t%eax\n\tjnz\t.Loop_scatter_1024\n\n\tvzeroupper\n\tret\n.size\trsaz_1024_scatter5_avx2,.-rsaz_1024_scatter5_avx2\n\n.globl\trsaz_1024_gather5_avx2\n.type\trsaz_1024_gather5_avx2,\\@abi-omnipotent\n.align\t32\nrsaz_1024_gather5_avx2:\n\tvzeroupper\n\tmov\t%rsp,%r11\n___\n$code.=<<___ if ($win64);\n\tlea\t-0x88(%rsp),%rax\n.LSEH_begin_rsaz_1024_gather5:\n\t# I can't trust assembler to use specific encoding:-(\n\t.byte\t0x48,0x8d,0x60,0xe0\t\t# lea\t-0x20(%rax),%rsp\n\t.byte\t0xc5,0xf8,0x29,0x70,0xe0\t# vmovaps %xmm6,-0x20(%rax)\n\t.byte\t0xc5,0xf8,0x29,0x78,0xf0\t# vmovaps %xmm7,-0x10(%rax)\n\t.byte\t0xc5,0x78,0x29,0x40,0x00\t# vmovaps %xmm8,0(%rax)\n\t.byte\t0xc5,0x78,0x29,0x48,0x10\t# vmovaps %xmm9,0x10(%rax)\n\t.byte\t0xc5,0x78,0x29,0x50,0x20\t# vmovaps %xmm10,0x20(%rax)\n\t.byte\t0xc5,0x78,0x29,0x58,0x30\t# vmovaps %xmm11,0x30(%rax)\n\t.byte\t0xc5,0x78,0x29,0x60,0x40\t# vmovaps %xmm12,0x40(%rax)\n\t.byte\t0xc5,0x78,0x29,0x68,0x50\t# vmovaps %xmm13,0x50(%rax)\n\t.byte\t0xc5,0x78,0x29,0x70,0x60\t# vmovaps %xmm14,0x60(%rax)\n\t.byte\t0xc5,0x78,0x29,0x78,0x70\t# vmovaps %xmm15,0x70(%rax)\n___\n$code.=<<___;\n\tlea\t-0x100(%rsp),%rsp\n\tand\t\\$-32, %rsp\n\tlea\t.Linc(%rip), %r10\n\tlea\t-128(%rsp),%rax\t\t\t# control u-op density\n\n\tvmovd\t\t$power, %xmm4\n\tvmovdqa\t\t(%r10),%ymm0\n\tvmovdqa\t\t32(%r10),%ymm1\n\tvmovdqa\t\t64(%r10),%ymm5\n\tvpbroadcastd\t%xmm4,%ymm4\n\n\tvpaddd\t\t%ymm5, %ymm0, %ymm2\n\tvpcmpeqd\t%ymm4, %ymm0, %ymm0\n\tvpaddd\t\t%ymm5, %ymm1, %ymm3\n\tvpcmpeqd\t%ymm4, %ymm1, %ymm1\n\tvmovdqa\t\t%ymm0, 32*0+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm2, %ymm0\n\tvpcmpeqd\t%ymm4, %ymm2, %ymm2\n\tvmovdqa\t\t%ymm1, 32*1+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm3, %ymm1\n\tvpcmpeqd\t%ymm4, %ymm3, %ymm3\n\tvmovdqa\t\t%ymm2, 32*2+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm0, %ymm2\n\tvpcmpeqd\t%ymm4, %ymm0, %ymm0\n\tvmovdqa\t\t%ymm3, 32*3+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm1, %ymm3\n\tvpcmpeqd\t%ymm4, %ymm1, %ymm1\n\tvmovdqa\t\t%ymm0, 32*4+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm2, %ymm8\n\tvpcmpeqd\t%ymm4, %ymm2, %ymm2\n\tvmovdqa\t\t%ymm1, 32*5+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm3, %ymm9\n\tvpcmpeqd\t%ymm4, %ymm3, %ymm3\n\tvmovdqa\t\t%ymm2, 32*6+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm8, %ymm10\n\tvpcmpeqd\t%ymm4, %ymm8, %ymm8\n\tvmovdqa\t\t%ymm3, 32*7+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm9, %ymm11\n\tvpcmpeqd\t%ymm4, %ymm9, %ymm9\n\tvpaddd\t\t%ymm5, %ymm10, %ymm12\n\tvpcmpeqd\t%ymm4, %ymm10, %ymm10\n\tvpaddd\t\t%ymm5, %ymm11, %ymm13\n\tvpcmpeqd\t%ymm4, %ymm11, %ymm11\n\tvpaddd\t\t%ymm5, %ymm12, %ymm14\n\tvpcmpeqd\t%ymm4, %ymm12, %ymm12\n\tvpaddd\t\t%ymm5, %ymm13, %ymm15\n\tvpcmpeqd\t%ymm4, %ymm13, %ymm13\n\tvpcmpeqd\t%ymm4, %ymm14, %ymm14\n\tvpcmpeqd\t%ymm4, %ymm15, %ymm15\n\n\tvmovdqa\t-32(%r10),%ymm7\t\t\t# .Lgather_permd\n\tlea\t128($inp), $inp\n\tmov\t\\$9,$power\n\n.Loop_gather_1024:\n\tvmovdqa\t\t32*0-128($inp),\t%ymm0\n\tvmovdqa\t\t32*1-128($inp),\t%ymm1\n\tvmovdqa\t\t32*2-128($inp),\t%ymm2\n\tvmovdqa\t\t32*3-128($inp),\t%ymm3\n\tvpand\t\t32*0+128(%rax),\t%ymm0,\t%ymm0\n\tvpand\t\t32*1+128(%rax),\t%ymm1,\t%ymm1\n\tvpand\t\t32*2+128(%rax),\t%ymm2,\t%ymm2\n\tvpor\t\t%ymm0, %ymm1, %ymm4\n\tvpand\t\t32*3+128(%rax),\t%ymm3,\t%ymm3\n\tvmovdqa\t\t32*4-128($inp),\t%ymm0\n\tvmovdqa\t\t32*5-128($inp),\t%ymm1\n\tvpor\t\t%ymm2, %ymm3, %ymm5\n\tvmovdqa\t\t32*6-128($inp),\t%ymm2\n\tvmovdqa\t\t32*7-128($inp),\t%ymm3\n\tvpand\t\t32*4+128(%rax),\t%ymm0,\t%ymm0\n\tvpand\t\t32*5+128(%rax),\t%ymm1,\t%ymm1\n\tvpand\t\t32*6+128(%rax),\t%ymm2,\t%ymm2\n\tvpor\t\t%ymm0, %ymm4, %ymm4\n\tvpand\t\t32*7+128(%rax),\t%ymm3,\t%ymm3\n\tvpand\t\t32*8-128($inp),\t%ymm8,\t%ymm0\n\tvpor\t\t%ymm1, %ymm5, %ymm5\n\tvpand\t\t32*9-128($inp),\t%ymm9,\t%ymm1\n\tvpor\t\t%ymm2, %ymm4, %ymm4\n\tvpand\t\t32*10-128($inp),%ymm10,\t%ymm2\n\tvpor\t\t%ymm3, %ymm5, %ymm5\n\tvpand\t\t32*11-128($inp),%ymm11,\t%ymm3\n\tvpor\t\t%ymm0, %ymm4, %ymm4\n\tvpand\t\t32*12-128($inp),%ymm12,\t%ymm0\n\tvpor\t\t%ymm1, %ymm5, %ymm5\n\tvpand\t\t32*13-128($inp),%ymm13,\t%ymm1\n\tvpor\t\t%ymm2, %ymm4, %ymm4\n\tvpand\t\t32*14-128($inp),%ymm14,\t%ymm2\n\tvpor\t\t%ymm3, %ymm5, %ymm5\n\tvpand\t\t32*15-128($inp),%ymm15,\t%ymm3\n\tlea\t\t32*16($inp), $inp\n\tvpor\t\t%ymm0, %ymm4, %ymm4\n\tvpor\t\t%ymm1, %ymm5, %ymm5\n\tvpor\t\t%ymm2, %ymm4, %ymm4\n\tvpor\t\t%ymm3, %ymm5, %ymm5\n\n\tvpor\t\t%ymm5, %ymm4, %ymm4\n\tvextracti128\t\\$1, %ymm4, %xmm5\t# upper half is cleared\n\tvpor\t\t%xmm4, %xmm5, %xmm5\n\tvpermd\t\t%ymm5,%ymm7,%ymm5\n\tvmovdqu\t\t%ymm5,($out)\n\tlea\t\t32($out),$out\n\tdec\t$power\n\tjnz\t.Loop_gather_1024\n\n\tvpxor\t%ymm0,%ymm0,%ymm0\n\tvmovdqu\t%ymm0,($out)\n\tvzeroupper\n___\n$code.=<<___ if ($win64);\n\tmovaps\t-0xa8(%r11),%xmm6\n\tmovaps\t-0x98(%r11),%xmm7\n\tmovaps\t-0x88(%r11),%xmm8\n\tmovaps\t-0x78(%r11),%xmm9\n\tmovaps\t-0x68(%r11),%xmm10\n\tmovaps\t-0x58(%r11),%xmm11\n\tmovaps\t-0x48(%r11),%xmm12\n\tmovaps\t-0x38(%r11),%xmm13\n\tmovaps\t-0x28(%r11),%xmm14\n\tmovaps\t-0x18(%r11),%xmm15\n.LSEH_end_rsaz_1024_gather5:\n___\n$code.=<<___;\n\tlea\t(%r11),%rsp\n\tret\n.size\trsaz_1024_gather5_avx2,.-rsaz_1024_gather5_avx2\n___\n}\n\n$code.=<<___;\n.extern\tOPENSSL_ia32cap_P\n.globl\trsaz_avx2_eligible\n.type\trsaz_avx2_eligible,\\@abi-omnipotent\n.align\t32\nrsaz_avx2_eligible:\n\tmov\tOPENSSL_ia32cap_P+8(%rip),%eax\n___\n$code.=<<___\tif ($addx);\n\tmov\t\\$`1<<8|1<<19`,%ecx\n\tmov\t\\$0,%edx\n\tand\t%eax,%ecx\n\tcmp\t\\$`1<<8|1<<19`,%ecx\t# check for BMI2+AD*X\n\tcmove\t%edx,%eax\n___\n$code.=<<___;\n\tand\t\\$`1<<5`,%eax\n\tshr\t\\$5,%eax\n\tret\n.size\trsaz_avx2_eligible,.-rsaz_avx2_eligible\n\n.align\t64\n.Land_mask:\n\t.quad\t0x1fffffff,0x1fffffff,0x1fffffff,-1\n.Lscatter_permd:\n\t.long\t0,2,4,6,7,7,7,7\n.Lgather_permd:\n\t.long\t0,7,1,7,2,7,3,7\n.Linc:\n\t.long\t0,0,0,0, 1,1,1,1\n\t.long\t2,2,2,2, 3,3,3,3\n\t.long\t4,4,4,4, 4,4,4,4\n.align\t64\n___\n\nif ($win64) {\n$rec=\"%rcx\";\n$frame=\"%rdx\";\n$context=\"%r8\";\n$disp=\"%r9\";\n\n$code.=<<___\n.extern\t__imp_RtlVirtualUnwind\n.type\trsaz_se_handler,\\@abi-omnipotent\n.align\t16\nrsaz_se_handler:\n\tpush\t%rsi\n\tpush\t%rdi\n\tpush\t%rbx\n\tpush\t%rbp\n\tpush\t%r12\n\tpush\t%r13\n\tpush\t%r14\n\tpush\t%r15\n\tpushfq\n\tsub\t\\$64,%rsp\n\n\tmov\t120($context),%rax\t# pull context->Rax\n\tmov\t248($context),%rbx\t# pull context->Rip\n\n\tmov\t8($disp),%rsi\t\t# disp->ImageBase\n\tmov\t56($disp),%r11\t\t# disp->HandlerData\n\n\tmov\t0(%r11),%r10d\t\t# HandlerData[0]\n\tlea\t(%rsi,%r10),%r10\t# prologue label\n\tcmp\t%r10,%rbx\t\t# context->Rip<prologue label\n\tjb\t.Lcommon_seh_tail\n\n\tmov\t152($context),%rax\t# pull context->Rsp\n\n\tmov\t4(%r11),%r10d\t\t# HandlerData[1]\n\tlea\t(%rsi,%r10),%r10\t# epilogue label\n\tcmp\t%r10,%rbx\t\t# context->Rip>=epilogue label\n\tjae\t.Lcommon_seh_tail\n\n\tmov\t160($context),%rax\t# pull context->Rbp\n\n\tmov\t-48(%rax),%r15\n\tmov\t-40(%rax),%r14\n\tmov\t-32(%rax),%r13\n\tmov\t-24(%rax),%r12\n\tmov\t-16(%rax),%rbp\n\tmov\t-8(%rax),%rbx\n\tmov\t%r15,240($context)\n\tmov\t%r14,232($context)\n\tmov\t%r13,224($context)\n\tmov\t%r12,216($context)\n\tmov\t%rbp,160($context)\n\tmov\t%rbx,144($context)\n\n\tlea\t-0xd8(%rax),%rsi\t# %xmm save area\n\tlea\t512($context),%rdi\t# & context.Xmm6\n\tmov\t\\$20,%ecx\t\t# 10*sizeof(%xmm0)/sizeof(%rax)\n\t.long\t0xa548f3fc\t\t# cld; rep movsq\n\n.Lcommon_seh_tail:\n\tmov\t8(%rax),%rdi\n\tmov\t16(%rax),%rsi\n\tmov\t%rax,152($context)\t# restore context->Rsp\n\tmov\t%rsi,168($context)\t# restore context->Rsi\n\tmov\t%rdi,176($context)\t# restore context->Rdi\n\n\tmov\t40($disp),%rdi\t\t# disp->ContextRecord\n\tmov\t$context,%rsi\t\t# context\n\tmov\t\\$154,%ecx\t\t# sizeof(CONTEXT)\n\t.long\t0xa548f3fc\t\t# cld; rep movsq\n\n\tmov\t$disp,%rsi\n\txor\t%rcx,%rcx\t\t# arg1, UNW_FLAG_NHANDLER\n\tmov\t8(%rsi),%rdx\t\t# arg2, disp->ImageBase\n\tmov\t0(%rsi),%r8\t\t# arg3, disp->ControlPc\n\tmov\t16(%rsi),%r9\t\t# arg4, disp->FunctionEntry\n\tmov\t40(%rsi),%r10\t\t# disp->ContextRecord\n\tlea\t56(%rsi),%r11\t\t# &disp->HandlerData\n\tlea\t24(%rsi),%r12\t\t# &disp->EstablisherFrame\n\tmov\t%r10,32(%rsp)\t\t# arg5\n\tmov\t%r11,40(%rsp)\t\t# arg6\n\tmov\t%r12,48(%rsp)\t\t# arg7\n\tmov\t%rcx,56(%rsp)\t\t# arg8, (NULL)\n\tcall\t*__imp_RtlVirtualUnwind(%rip)\n\n\tmov\t\\$1,%eax\t\t# ExceptionContinueSearch\n\tadd\t\\$64,%rsp\n\tpopfq\n\tpop\t%r15\n\tpop\t%r14\n\tpop\t%r13\n\tpop\t%r12\n\tpop\t%rbp\n\tpop\t%rbx\n\tpop\t%rdi\n\tpop\t%rsi\n\tret\n.size\trsaz_se_handler,.-rsaz_se_handler\n\n.section\t.pdata\n.align\t4\n\t.rva\t.LSEH_begin_rsaz_1024_sqr_avx2\n\t.rva\t.LSEH_end_rsaz_1024_sqr_avx2\n\t.rva\t.LSEH_info_rsaz_1024_sqr_avx2\n\n\t.rva\t.LSEH_begin_rsaz_1024_mul_avx2\n\t.rva\t.LSEH_end_rsaz_1024_mul_avx2\n\t.rva\t.LSEH_info_rsaz_1024_mul_avx2\n\n\t.rva\t.LSEH_begin_rsaz_1024_gather5\n\t.rva\t.LSEH_end_rsaz_1024_gather5\n\t.rva\t.LSEH_info_rsaz_1024_gather5\n.section\t.xdata\n.align\t8\n.LSEH_info_rsaz_1024_sqr_avx2:\n\t.byte\t9,0,0,0\n\t.rva\trsaz_se_handler\n\t.rva\t.Lsqr_1024_body,.Lsqr_1024_epilogue\n.LSEH_info_rsaz_1024_mul_avx2:\n\t.byte\t9,0,0,0\n\t.rva\trsaz_se_handler\n\t.rva\t.Lmul_1024_body,.Lmul_1024_epilogue\n.LSEH_info_rsaz_1024_gather5:\n\t.byte\t0x01,0x36,0x17,0x0b\n\t.byte\t0x36,0xf8,0x09,0x00\t# vmovaps 0x90(rsp),xmm15\n\t.byte\t0x31,0xe8,0x08,0x00\t# vmovaps 0x80(rsp),xmm14\n\t.byte\t0x2c,0xd8,0x07,0x00\t# vmovaps 0x70(rsp),xmm13\n\t.byte\t0x27,0xc8,0x06,0x00\t# vmovaps 0x60(rsp),xmm12\n\t.byte\t0x22,0xb8,0x05,0x00\t# vmovaps 0x50(rsp),xmm11\n\t.byte\t0x1d,0xa8,0x04,0x00\t# vmovaps 0x40(rsp),xmm10\n\t.byte\t0x18,0x98,0x03,0x00\t# vmovaps 0x30(rsp),xmm9\n\t.byte\t0x13,0x88,0x02,0x00\t# vmovaps 0x20(rsp),xmm8\n\t.byte\t0x0e,0x78,0x01,0x00\t# vmovaps 0x10(rsp),xmm7\n\t.byte\t0x09,0x68,0x00,0x00\t# vmovaps 0x00(rsp),xmm6\n\t.byte\t0x04,0x01,0x15,0x00\t# sub\t  rsp,0xa8\n\t.byte\t0x00,0xb3,0x00,0x00\t# set_frame r11\n___\n}\n\nforeach (split(\"\\n\",$code)) {\n\ts/\\`([^\\`]*)\\`/eval($1)/ge;\n\n\ts/\\b(sh[rl]d?\\s+\\$)(-?[0-9]+)/$1.$2%64/ge\t\tor\n\n\ts/\\b(vmov[dq])\\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go\t\tor\n\ts/\\b(vmovdqu)\\b(.+)%x%ymm([0-9]+)/$1$2%xmm$3/go\t\tor\n\ts/\\b(vpinsr[qd])\\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go\tor\n\ts/\\b(vpextr[qd])\\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go\tor\n\ts/\\b(vpbroadcast[qd]\\s+)%ymm([0-9]+)/$1%xmm$2/go;\n\tprint $_,\"\\n\";\n}\n\n}}} else {{{\nprint <<___;\t# assembler is too old\n.text\n\n.globl\trsaz_avx2_eligible\n.type\trsaz_avx2_eligible,\\@abi-omnipotent\nrsaz_avx2_eligible:\n\txor\t%eax,%eax\n\tret\n.size\trsaz_avx2_eligible,.-rsaz_avx2_eligible\n\n.globl\trsaz_1024_sqr_avx2\n.globl\trsaz_1024_mul_avx2\n.globl\trsaz_1024_norm2red_avx2\n.globl\trsaz_1024_red2norm_avx2\n.globl\trsaz_1024_scatter5_avx2\n.globl\trsaz_1024_gather5_avx2\n.type\trsaz_1024_sqr_avx2,\\@abi-omnipotent\nrsaz_1024_sqr_avx2:\nrsaz_1024_mul_avx2:\nrsaz_1024_norm2red_avx2:\nrsaz_1024_red2norm_avx2:\nrsaz_1024_scatter5_avx2:\nrsaz_1024_gather5_avx2:\n\t.byte\t0x0f,0x0b\t# ud2\n\tret\n.size\trsaz_1024_sqr_avx2,.-rsaz_1024_sqr_avx2\n___\n}}}\n\nclose STDOUT;\n"], "fixing_code": ["#! /usr/bin/env perl\n# Copyright 2013-2016 The OpenSSL Project Authors. All Rights Reserved.\n#\n# Licensed under the OpenSSL license (the \"License\").  You may not use\n# this file except in compliance with the License.  You can obtain a copy\n# in the file LICENSE in the source distribution or at\n# https://www.openssl.org/source/license.html\n\n\n##############################################################################\n#                                                                            #\n#  Copyright (c) 2012, Intel Corporation                                     #\n#                                                                            #\n#  All rights reserved.                                                      #\n#                                                                            #\n#  Redistribution and use in source and binary forms, with or without        #\n#  modification, are permitted provided that the following conditions are    #\n#  met:                                                                      #\n#                                                                            #\n#  *  Redistributions of source code must retain the above copyright         #\n#     notice, this list of conditions and the following disclaimer.          #\n#                                                                            #\n#  *  Redistributions in binary form must reproduce the above copyright      #\n#     notice, this list of conditions and the following disclaimer in the    #\n#     documentation and/or other materials provided with the                 #\n#     distribution.                                                          #\n#                                                                            #\n#  *  Neither the name of the Intel Corporation nor the names of its         #\n#     contributors may be used to endorse or promote products derived from   #\n#     this software without specific prior written permission.               #\n#                                                                            #\n#                                                                            #\n#  THIS SOFTWARE IS PROVIDED BY INTEL CORPORATION \"\"AS IS\"\" AND ANY          #\n#  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE         #\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR        #\n#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL INTEL CORPORATION OR            #\n#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,     #\n#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,       #\n#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR        #\n#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF    #\n#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING      #\n#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS        #\n#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.              #\n#                                                                            #\n##############################################################################\n# Developers and authors:                                                    #\n# Shay Gueron (1, 2), and Vlad Krasnov (1)                                   #\n# (1) Intel Corporation, Israel Development Center, Haifa, Israel            #\n# (2) University of Haifa, Israel                                            #\n##############################################################################\n# Reference:                                                                 #\n# [1] S. Gueron, V. Krasnov: \"Software Implementation of Modular             #\n#     Exponentiation,  Using Advanced Vector Instructions Architectures\",    #\n#     F. Ozbudak and F. Rodriguez-Henriquez (Eds.): WAIFI 2012, LNCS 7369,   #\n#     pp. 119?135, 2012. Springer-Verlag Berlin Heidelberg 2012              #\n# [2] S. Gueron: \"Efficient Software Implementations of Modular              #\n#     Exponentiation\", Journal of Cryptographic Engineering 2:31-43 (2012).  #\n# [3] S. Gueron, V. Krasnov: \"Speeding up Big-numbers Squaring\",IEEE         #\n#     Proceedings of 9th International Conference on Information Technology: #\n#     New Generations (ITNG 2012), pp.821-823 (2012)                         #\n# [4] S. Gueron, V. Krasnov: \"[PATCH] Efficient and side channel analysis    #\n#     resistant 1024-bit modular exponentiation, for optimizing RSA2048      #\n#     on AVX2 capable x86_64 platforms\",                                     #\n#     http://rt.openssl.org/Ticket/Display.html?id=2850&user=guest&pass=guest#\n##############################################################################\n#\n# +13% improvement over original submission by <appro@openssl.org>\n#\n# rsa2048 sign/sec\tOpenSSL 1.0.1\tscalar(*)\tthis\n# 2.3GHz Haswell\t621\t\t765/+23%\t1113/+79%\n# 2.3GHz Broadwell(**)\t688\t\t1200(***)/+74%\t1120/+63%\n#\n# (*)\tif system doesn't support AVX2, for reference purposes;\n# (**)\tscaled to 2.3GHz to simplify comparison;\n# (***)\tscalar AD*X code is faster than AVX2 and is preferred code\n#\tpath for Broadwell;\n\n$flavour = shift;\n$output  = shift;\nif ($flavour =~ /\\./) { $output = $flavour; undef $flavour; }\n\n$win64=0; $win64=1 if ($flavour =~ /[nm]asm|mingw64/ || $output =~ /\\.asm$/);\n\n$0 =~ m/(.*[\\/\\\\])[^\\/\\\\]+$/; $dir=$1;\n( $xlate=\"${dir}x86_64-xlate.pl\" and -f $xlate ) or\n( $xlate=\"${dir}../../perlasm/x86_64-xlate.pl\" and -f $xlate) or\ndie \"can't locate x86_64-xlate.pl\";\n\nif (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`\n\t\t=~ /GNU assembler version ([2-9]\\.[0-9]+)/) {\n\t$avx = ($1>=2.19) + ($1>=2.22);\n\t$addx = ($1>=2.23);\n}\n\nif (!$avx && $win64 && ($flavour =~ /nasm/ || $ENV{ASM} =~ /nasm/) &&\n\t    `nasm -v 2>&1` =~ /NASM version ([2-9]\\.[0-9]+)/) {\n\t$avx = ($1>=2.09) + ($1>=2.10);\n\t$addx = ($1>=2.10);\n}\n\nif (!$avx && $win64 && ($flavour =~ /masm/ || $ENV{ASM} =~ /ml64/) &&\n\t    `ml64 2>&1` =~ /Version ([0-9]+)\\./) {\n\t$avx = ($1>=10) + ($1>=11);\n\t$addx = ($1>=11);\n}\n\nif (!$avx && `$ENV{CC} -v 2>&1` =~ /(^clang version|based on LLVM) ([3-9])\\.([0-9]+)/) {\n\tmy $ver = $2 + $3/100.0;\t# 3.1->3.01, 3.10->3.10\n\t$avx = ($ver>=3.0) + ($ver>=3.01);\n\t$addx = ($ver>=3.03);\n}\n\nopen OUT,\"| \\\"$^X\\\" \\\"$xlate\\\" $flavour \\\"$output\\\"\";\n*STDOUT = *OUT;\n\nif ($avx>1) {{{\n{ # void AMS_WW(\nmy $rp=\"%rdi\";\t# BN_ULONG *rp,\nmy $ap=\"%rsi\";\t# const BN_ULONG *ap,\nmy $np=\"%rdx\";\t# const BN_ULONG *np,\nmy $n0=\"%ecx\";\t# const BN_ULONG n0,\nmy $rep=\"%r8d\";\t# int repeat);\n\n# The registers that hold the accumulated redundant result\n# The AMM works on 1024 bit operands, and redundant word size is 29\n# Therefore: ceil(1024/29)/4 = 9\nmy $ACC0=\"%ymm0\";\nmy $ACC1=\"%ymm1\";\nmy $ACC2=\"%ymm2\";\nmy $ACC3=\"%ymm3\";\nmy $ACC4=\"%ymm4\";\nmy $ACC5=\"%ymm5\";\nmy $ACC6=\"%ymm6\";\nmy $ACC7=\"%ymm7\";\nmy $ACC8=\"%ymm8\";\nmy $ACC9=\"%ymm9\";\n# Registers that hold the broadcasted words of bp, currently used\nmy $B1=\"%ymm10\";\nmy $B2=\"%ymm11\";\n# Registers that hold the broadcasted words of Y, currently used\nmy $Y1=\"%ymm12\";\nmy $Y2=\"%ymm13\";\n# Helper registers\nmy $TEMP1=\"%ymm14\";\nmy $AND_MASK=\"%ymm15\";\n# alu registers that hold the first words of the ACC\nmy $r0=\"%r9\";\nmy $r1=\"%r10\";\nmy $r2=\"%r11\";\nmy $r3=\"%r12\";\n\nmy $i=\"%r14d\";\t\t\t# loop counter\nmy $tmp = \"%r15\";\n\nmy $FrameSize=32*18+32*8;\t# place for A^2 and 2*A\n\nmy $aap=$r0;\nmy $tp0=\"%rbx\";\nmy $tp1=$r3;\nmy $tpa=$tmp;\n\n$np=\"%r13\";\t\t\t# reassigned argument\n\n$code.=<<___;\n.text\n\n.globl\trsaz_1024_sqr_avx2\n.type\trsaz_1024_sqr_avx2,\\@function,5\n.align\t64\nrsaz_1024_sqr_avx2:\t\t# 702 cycles, 14% faster than rsaz_1024_mul_avx2\n\tlea\t(%rsp), %rax\n\tpush\t%rbx\n\tpush\t%rbp\n\tpush\t%r12\n\tpush\t%r13\n\tpush\t%r14\n\tpush\t%r15\n\tvzeroupper\n___\n$code.=<<___ if ($win64);\n\tlea\t-0xa8(%rsp),%rsp\n\tvmovaps\t%xmm6,-0xd8(%rax)\n\tvmovaps\t%xmm7,-0xc8(%rax)\n\tvmovaps\t%xmm8,-0xb8(%rax)\n\tvmovaps\t%xmm9,-0xa8(%rax)\n\tvmovaps\t%xmm10,-0x98(%rax)\n\tvmovaps\t%xmm11,-0x88(%rax)\n\tvmovaps\t%xmm12,-0x78(%rax)\n\tvmovaps\t%xmm13,-0x68(%rax)\n\tvmovaps\t%xmm14,-0x58(%rax)\n\tvmovaps\t%xmm15,-0x48(%rax)\n.Lsqr_1024_body:\n___\n$code.=<<___;\n\tmov\t%rax,%rbp\n\tmov\t%rdx, $np\t\t\t# reassigned argument\n\tsub\t\\$$FrameSize, %rsp\n\tmov\t$np, $tmp\n\tsub\t\\$-128, $rp\t\t\t# size optimization\n\tsub\t\\$-128, $ap\n\tsub\t\\$-128, $np\n\n\tand\t\\$4095, $tmp\t\t\t# see if $np crosses page\n\tadd\t\\$32*10, $tmp\n\tshr\t\\$12, $tmp\n\tvpxor\t$ACC9,$ACC9,$ACC9\n\tjz\t.Lsqr_1024_no_n_copy\n\n\t# unaligned 256-bit load that crosses page boundary can\n\t# cause >2x performance degradation here, so if $np does\n\t# cross page boundary, copy it to stack and make sure stack\n\t# frame doesn't...\n\tsub\t\t\\$32*10,%rsp\n\tvmovdqu\t\t32*0-128($np), $ACC0\n\tand\t\t\\$-2048, %rsp\n\tvmovdqu\t\t32*1-128($np), $ACC1\n\tvmovdqu\t\t32*2-128($np), $ACC2\n\tvmovdqu\t\t32*3-128($np), $ACC3\n\tvmovdqu\t\t32*4-128($np), $ACC4\n\tvmovdqu\t\t32*5-128($np), $ACC5\n\tvmovdqu\t\t32*6-128($np), $ACC6\n\tvmovdqu\t\t32*7-128($np), $ACC7\n\tvmovdqu\t\t32*8-128($np), $ACC8\n\tlea\t\t$FrameSize+128(%rsp),$np\n\tvmovdqu\t\t$ACC0, 32*0-128($np)\n\tvmovdqu\t\t$ACC1, 32*1-128($np)\n\tvmovdqu\t\t$ACC2, 32*2-128($np)\n\tvmovdqu\t\t$ACC3, 32*3-128($np)\n\tvmovdqu\t\t$ACC4, 32*4-128($np)\n\tvmovdqu\t\t$ACC5, 32*5-128($np)\n\tvmovdqu\t\t$ACC6, 32*6-128($np)\n\tvmovdqu\t\t$ACC7, 32*7-128($np)\n\tvmovdqu\t\t$ACC8, 32*8-128($np)\n\tvmovdqu\t\t$ACC9, 32*9-128($np)\t# $ACC9 is zero\n\n.Lsqr_1024_no_n_copy:\n\tand\t\t\\$-1024, %rsp\n\n\tvmovdqu\t\t32*1-128($ap), $ACC1\n\tvmovdqu\t\t32*2-128($ap), $ACC2\n\tvmovdqu\t\t32*3-128($ap), $ACC3\n\tvmovdqu\t\t32*4-128($ap), $ACC4\n\tvmovdqu\t\t32*5-128($ap), $ACC5\n\tvmovdqu\t\t32*6-128($ap), $ACC6\n\tvmovdqu\t\t32*7-128($ap), $ACC7\n\tvmovdqu\t\t32*8-128($ap), $ACC8\n\n\tlea\t192(%rsp), $tp0\t\t\t# 64+128=192\n\tvmovdqu\t.Land_mask(%rip), $AND_MASK\n\tjmp\t.LOOP_GRANDE_SQR_1024\n\n.align\t32\n.LOOP_GRANDE_SQR_1024:\n\tlea\t32*18+128(%rsp), $aap\t\t# size optimization\n\tlea\t448(%rsp), $tp1\t\t\t# 64+128+256=448\n\n\t# the squaring is performed as described in Variant B of\n\t# \"Speeding up Big-Number Squaring\", so start by calculating\n\t# the A*2=A+A vector\n\tvpaddq\t\t$ACC1, $ACC1, $ACC1\n\t vpbroadcastq\t32*0-128($ap), $B1\n\tvpaddq\t\t$ACC2, $ACC2, $ACC2\n\tvmovdqa\t\t$ACC1, 32*0-128($aap)\n\tvpaddq\t\t$ACC3, $ACC3, $ACC3\n\tvmovdqa\t\t$ACC2, 32*1-128($aap)\n\tvpaddq\t\t$ACC4, $ACC4, $ACC4\n\tvmovdqa\t\t$ACC3, 32*2-128($aap)\n\tvpaddq\t\t$ACC5, $ACC5, $ACC5\n\tvmovdqa\t\t$ACC4, 32*3-128($aap)\n\tvpaddq\t\t$ACC6, $ACC6, $ACC6\n\tvmovdqa\t\t$ACC5, 32*4-128($aap)\n\tvpaddq\t\t$ACC7, $ACC7, $ACC7\n\tvmovdqa\t\t$ACC6, 32*5-128($aap)\n\tvpaddq\t\t$ACC8, $ACC8, $ACC8\n\tvmovdqa\t\t$ACC7, 32*6-128($aap)\n\tvpxor\t\t$ACC9, $ACC9, $ACC9\n\tvmovdqa\t\t$ACC8, 32*7-128($aap)\n\n\tvpmuludq\t32*0-128($ap), $B1, $ACC0\n\t vpbroadcastq\t32*1-128($ap), $B2\n\t vmovdqu\t$ACC9, 32*9-192($tp0)\t# zero upper half\n\tvpmuludq\t$B1, $ACC1, $ACC1\n\t vmovdqu\t$ACC9, 32*10-448($tp1)\n\tvpmuludq\t$B1, $ACC2, $ACC2\n\t vmovdqu\t$ACC9, 32*11-448($tp1)\n\tvpmuludq\t$B1, $ACC3, $ACC3\n\t vmovdqu\t$ACC9, 32*12-448($tp1)\n\tvpmuludq\t$B1, $ACC4, $ACC4\n\t vmovdqu\t$ACC9, 32*13-448($tp1)\n\tvpmuludq\t$B1, $ACC5, $ACC5\n\t vmovdqu\t$ACC9, 32*14-448($tp1)\n\tvpmuludq\t$B1, $ACC6, $ACC6\n\t vmovdqu\t$ACC9, 32*15-448($tp1)\n\tvpmuludq\t$B1, $ACC7, $ACC7\n\t vmovdqu\t$ACC9, 32*16-448($tp1)\n\tvpmuludq\t$B1, $ACC8, $ACC8\n\t vpbroadcastq\t32*2-128($ap), $B1\n\t vmovdqu\t$ACC9, 32*17-448($tp1)\n\n\tmov\t$ap, $tpa\n\tmov \t\\$4, $i\n\tjmp\t.Lsqr_entry_1024\n___\n$TEMP0=$Y1;\n$TEMP2=$Y2;\n$code.=<<___;\n.align\t32\n.LOOP_SQR_1024:\n\t vpbroadcastq\t32*1-128($tpa), $B2\n\tvpmuludq\t32*0-128($ap), $B1, $ACC0\n\tvpaddq\t\t32*0-192($tp0), $ACC0, $ACC0\n\tvpmuludq\t32*0-128($aap), $B1, $ACC1\n\tvpaddq\t\t32*1-192($tp0), $ACC1, $ACC1\n\tvpmuludq\t32*1-128($aap), $B1, $ACC2\n\tvpaddq\t\t32*2-192($tp0), $ACC2, $ACC2\n\tvpmuludq\t32*2-128($aap), $B1, $ACC3\n\tvpaddq\t\t32*3-192($tp0), $ACC3, $ACC3\n\tvpmuludq\t32*3-128($aap), $B1, $ACC4\n\tvpaddq\t\t32*4-192($tp0), $ACC4, $ACC4\n\tvpmuludq\t32*4-128($aap), $B1, $ACC5\n\tvpaddq\t\t32*5-192($tp0), $ACC5, $ACC5\n\tvpmuludq\t32*5-128($aap), $B1, $ACC6\n\tvpaddq\t\t32*6-192($tp0), $ACC6, $ACC6\n\tvpmuludq\t32*6-128($aap), $B1, $ACC7\n\tvpaddq\t\t32*7-192($tp0), $ACC7, $ACC7\n\tvpmuludq\t32*7-128($aap), $B1, $ACC8\n\t vpbroadcastq\t32*2-128($tpa), $B1\n\tvpaddq\t\t32*8-192($tp0), $ACC8, $ACC8\n.Lsqr_entry_1024:\n\tvmovdqu\t\t$ACC0, 32*0-192($tp0)\n\tvmovdqu\t\t$ACC1, 32*1-192($tp0)\n\n\tvpmuludq\t32*1-128($ap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC2, $ACC2\n\tvpmuludq\t32*1-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC3, $ACC3\n\tvpmuludq\t32*2-128($aap), $B2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC4, $ACC4\n\tvpmuludq\t32*3-128($aap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC5, $ACC5\n\tvpmuludq\t32*4-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC6, $ACC6\n\tvpmuludq\t32*5-128($aap), $B2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC7, $ACC7\n\tvpmuludq\t32*6-128($aap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpmuludq\t32*7-128($aap), $B2, $ACC0\n\t vpbroadcastq\t32*3-128($tpa), $B2\n\tvpaddq\t\t32*9-192($tp0), $ACC0, $ACC0\n\n\tvmovdqu\t\t$ACC2, 32*2-192($tp0)\n\tvmovdqu\t\t$ACC3, 32*3-192($tp0)\n\n\tvpmuludq\t32*2-128($ap), $B1, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC4, $ACC4\n\tvpmuludq\t32*2-128($aap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC5, $ACC5\n\tvpmuludq\t32*3-128($aap), $B1, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC6, $ACC6\n\tvpmuludq\t32*4-128($aap), $B1, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC7, $ACC7\n\tvpmuludq\t32*5-128($aap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpmuludq\t32*6-128($aap), $B1, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC0, $ACC0\n\tvpmuludq\t32*7-128($aap), $B1, $ACC1\n\t vpbroadcastq\t32*4-128($tpa), $B1\n\tvpaddq\t\t32*10-448($tp1), $ACC1, $ACC1\n\n\tvmovdqu\t\t$ACC4, 32*4-192($tp0)\n\tvmovdqu\t\t$ACC5, 32*5-192($tp0)\n\n\tvpmuludq\t32*3-128($ap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC6, $ACC6\n\tvpmuludq\t32*3-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC7, $ACC7\n\tvpmuludq\t32*4-128($aap), $B2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC8, $ACC8\n\tvpmuludq\t32*5-128($aap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpmuludq\t32*6-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpmuludq\t32*7-128($aap), $B2, $ACC2\n\t vpbroadcastq\t32*5-128($tpa), $B2\n\tvpaddq\t\t32*11-448($tp1), $ACC2, $ACC2\t\n\n\tvmovdqu\t\t$ACC6, 32*6-192($tp0)\n\tvmovdqu\t\t$ACC7, 32*7-192($tp0)\n\n\tvpmuludq\t32*4-128($ap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpmuludq\t32*4-128($aap), $B1, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC0, $ACC0\n\tvpmuludq\t32*5-128($aap), $B1, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC1, $ACC1\n\tvpmuludq\t32*6-128($aap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC2, $ACC2\n\tvpmuludq\t32*7-128($aap), $B1, $ACC3\n\t vpbroadcastq\t32*6-128($tpa), $B1\n\tvpaddq\t\t32*12-448($tp1), $ACC3, $ACC3\n\n\tvmovdqu\t\t$ACC8, 32*8-192($tp0)\n\tvmovdqu\t\t$ACC0, 32*9-192($tp0)\n\tlea\t\t8($tp0), $tp0\n\n\tvpmuludq\t32*5-128($ap), $B2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC1, $ACC1\n\tvpmuludq\t32*5-128($aap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC2, $ACC2\n\tvpmuludq\t32*6-128($aap), $B2, $TEMP1\n\tvpaddq\t\t$TEMP1, $ACC3, $ACC3\n\tvpmuludq\t32*7-128($aap), $B2, $ACC4\n\t vpbroadcastq\t32*7-128($tpa), $B2\n\tvpaddq\t\t32*13-448($tp1), $ACC4, $ACC4\n\n\tvmovdqu\t\t$ACC1, 32*10-448($tp1)\n\tvmovdqu\t\t$ACC2, 32*11-448($tp1)\n\n\tvpmuludq\t32*6-128($ap), $B1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC3, $ACC3\n\tvpmuludq\t32*6-128($aap), $B1, $TEMP1\n\t vpbroadcastq\t32*8-128($tpa), $ACC0\t\t# borrow $ACC0 for $B1\n\tvpaddq\t\t$TEMP1, $ACC4, $ACC4\n\tvpmuludq\t32*7-128($aap), $B1, $ACC5\n\t vpbroadcastq\t32*0+8-128($tpa), $B1\t\t# for next iteration\n\tvpaddq\t\t32*14-448($tp1), $ACC5, $ACC5\n\n\tvmovdqu\t\t$ACC3, 32*12-448($tp1)\n\tvmovdqu\t\t$ACC4, 32*13-448($tp1)\n\tlea\t\t8($tpa), $tpa\n\n\tvpmuludq\t32*7-128($ap), $B2, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC5, $ACC5\n\tvpmuludq\t32*7-128($aap), $B2, $ACC6\n\tvpaddq\t\t32*15-448($tp1), $ACC6, $ACC6\n\n\tvpmuludq\t32*8-128($ap), $ACC0, $ACC7\n\tvmovdqu\t\t$ACC5, 32*14-448($tp1)\n\tvpaddq\t\t32*16-448($tp1), $ACC7, $ACC7\n\tvmovdqu\t\t$ACC6, 32*15-448($tp1)\n\tvmovdqu\t\t$ACC7, 32*16-448($tp1)\n\tlea\t\t8($tp1), $tp1\n\n\tdec\t$i        \n\tjnz\t.LOOP_SQR_1024\n___\n$ZERO = $ACC9;\n$TEMP0 = $B1;\n$TEMP2 = $B2;\n$TEMP3 = $Y1;\n$TEMP4 = $Y2;\n$code.=<<___;\n\t# we need to fix indices 32-39 to avoid overflow\n\tvmovdqu\t\t32*8(%rsp), $ACC8\t\t# 32*8-192($tp0),\n\tvmovdqu\t\t32*9(%rsp), $ACC1\t\t# 32*9-192($tp0)\n\tvmovdqu\t\t32*10(%rsp), $ACC2\t\t# 32*10-192($tp0)\n\tlea\t\t192(%rsp), $tp0\t\t\t# 64+128=192\n\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpxor\t\t$ZERO, $ZERO, $ZERO\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpblendd\t\\$3, $TEMP2, $ZERO, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvmovdqu\t\t$ACC1, 32*9-192($tp0)\n\tvmovdqu\t\t$ACC2, 32*10-192($tp0)\n\n\tmov\t(%rsp), %rax\n\tmov\t8(%rsp), $r1\n\tmov\t16(%rsp), $r2\n\tmov\t24(%rsp), $r3\n\tvmovdqu\t32*1(%rsp), $ACC1\n\tvmovdqu\t32*2-192($tp0), $ACC2\n\tvmovdqu\t32*3-192($tp0), $ACC3\n\tvmovdqu\t32*4-192($tp0), $ACC4\n\tvmovdqu\t32*5-192($tp0), $ACC5\n\tvmovdqu\t32*6-192($tp0), $ACC6\n\tvmovdqu\t32*7-192($tp0), $ACC7\n\n\tmov\t%rax, $r0\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\tvmovd\t%eax, $Y1\n\n\tmov\t%rax, %rdx\n\timulq\t-128($np), %rax\n\t vpbroadcastq\t$Y1, $Y1\n\tadd\t%rax, $r0\n\tmov\t%rdx, %rax\n\timulq\t8-128($np), %rax\n\tshr\t\\$29, $r0\n\tadd\t%rax, $r1\n\tmov\t%rdx, %rax\n\timulq\t16-128($np), %rax\n\tadd\t$r0, $r1\n\tadd\t%rax, $r2\n\timulq\t24-128($np), %rdx\n\tadd\t%rdx, $r3\n\n\tmov\t$r1, %rax\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\tmov \\$9, $i\n\tjmp .LOOP_REDUCE_1024\n\n.align\t32\n.LOOP_REDUCE_1024:\n\tvmovd\t%eax, $Y2\n\tvpbroadcastq\t$Y2, $Y2\n\n\tvpmuludq\t32*1-128($np), $Y1, $TEMP0\n\t mov\t%rax, %rdx\n\t imulq\t-128($np), %rax\n\tvpaddq\t\t$TEMP0, $ACC1, $ACC1\n\t add\t%rax, $r1\n\tvpmuludq\t32*2-128($np), $Y1, $TEMP1\n\t mov\t%rdx, %rax\n\t imulq\t8-128($np), %rax\n\tvpaddq\t\t$TEMP1, $ACC2, $ACC2\n\tvpmuludq\t32*3-128($np), $Y1, $TEMP2\n\t .byte\t0x67\n\t add\t%rax, $r2\n\t .byte\t0x67\n\t mov\t%rdx, %rax\n\t imulq\t16-128($np), %rax\n\t shr\t\\$29, $r1\n\tvpaddq\t\t$TEMP2, $ACC3, $ACC3\n\tvpmuludq\t32*4-128($np), $Y1, $TEMP0\n\t add\t%rax, $r3\n\t add\t$r1, $r2\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpmuludq\t32*5-128($np), $Y1, $TEMP1\n\t mov\t$r2, %rax\n\t imull\t$n0, %eax\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvpmuludq\t32*6-128($np), $Y1, $TEMP2\n\t and\t\\$0x1fffffff, %eax\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvpmuludq\t32*7-128($np), $Y1, $TEMP0\n\tvpaddq\t\t$TEMP0, $ACC7, $ACC7\n\tvpmuludq\t32*8-128($np), $Y1, $TEMP1\n\t vmovd\t%eax, $Y1\n\t #vmovdqu\t32*1-8-128($np), $TEMP2\t\t# moved below\n\tvpaddq\t\t$TEMP1, $ACC8, $ACC8\n\t #vmovdqu\t32*2-8-128($np), $TEMP0\t\t# moved below\n\t vpbroadcastq\t$Y1, $Y1\n\n\tvpmuludq\t32*1-8-128($np), $Y2, $TEMP2\t# see above\n\tvmovdqu\t\t32*3-8-128($np), $TEMP1\n\t mov\t%rax, %rdx\n\t imulq\t-128($np), %rax\n\tvpaddq\t\t$TEMP2, $ACC1, $ACC1\n\tvpmuludq\t32*2-8-128($np), $Y2, $TEMP0\t# see above\n\tvmovdqu\t\t32*4-8-128($np), $TEMP2\n\t add\t%rax, $r2\n\t mov\t%rdx, %rax\n\t imulq\t8-128($np), %rax\n\tvpaddq\t\t$TEMP0, $ACC2, $ACC2\n\t add\t$r3, %rax\n\t shr\t\\$29, $r2\n\tvpmuludq\t$Y2, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*5-8-128($np), $TEMP0\n\t add\t$r2, %rax\n\tvpaddq\t\t$TEMP1, $ACC3, $ACC3\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*6-8-128($np), $TEMP1\n\t .byte\t0x67\n\t mov\t%rax, $r3\n\t imull\t$n0, %eax\n\tvpaddq\t\t$TEMP2, $ACC4, $ACC4\n\tvpmuludq\t$Y2, $TEMP0, $TEMP0\n\t.byte\t0xc4,0x41,0x7e,0x6f,0x9d,0x58,0x00,0x00,0x00\t# vmovdqu\t\t32*7-8-128($np), $TEMP2\n\t and\t\\$0x1fffffff, %eax\n\tvpaddq\t\t$TEMP0, $ACC5, $ACC5\n\tvpmuludq\t$Y2, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*8-8-128($np), $TEMP0\n\tvpaddq\t\t$TEMP1, $ACC6, $ACC6\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*9-8-128($np), $ACC9\n\t vmovd\t%eax, $ACC0\t\t\t# borrow ACC0 for Y2\n\t imulq\t-128($np), %rax\n\tvpaddq\t\t$TEMP2, $ACC7, $ACC7\n\tvpmuludq\t$Y2, $TEMP0, $TEMP0\n\t vmovdqu\t32*1-16-128($np), $TEMP1\n\t vpbroadcastq\t$ACC0, $ACC0\n\tvpaddq\t\t$TEMP0, $ACC8, $ACC8\n\tvpmuludq\t$Y2, $ACC9, $ACC9\n\t vmovdqu\t32*2-16-128($np), $TEMP2\n\t add\t%rax, $r3\n\n___\n($ACC0,$Y2)=($Y2,$ACC0);\n$code.=<<___;\n\t vmovdqu\t32*1-24-128($np), $ACC0\n\tvpmuludq\t$Y1, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*3-16-128($np), $TEMP0\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\t vpmuludq\t$Y2, $ACC0, $ACC0\n\tvpmuludq\t$Y1, $TEMP2, $TEMP2\n\t.byte\t0xc4,0x41,0x7e,0x6f,0xb5,0xf0,0xff,0xff,0xff\t# vmovdqu\t\t32*4-16-128($np), $TEMP1\n\t vpaddq\t\t$ACC1, $ACC0, $ACC0\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvpmuludq\t$Y1, $TEMP0, $TEMP0\n\tvmovdqu\t\t32*5-16-128($np), $TEMP2\n\t .byte\t0x67\n\t vmovq\t\t$ACC0, %rax\n\t vmovdqu\t$ACC0, (%rsp)\t\t# transfer $r0-$r3\n\tvpaddq\t\t$TEMP0, $ACC3, $ACC3\n\tvpmuludq\t$Y1, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*6-16-128($np), $TEMP0\n\tvpaddq\t\t$TEMP1, $ACC4, $ACC4\n\tvpmuludq\t$Y1, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*7-16-128($np), $TEMP1\n\tvpaddq\t\t$TEMP2, $ACC5, $ACC5\n\tvpmuludq\t$Y1, $TEMP0, $TEMP0\n\tvmovdqu\t\t32*8-16-128($np), $TEMP2\n\tvpaddq\t\t$TEMP0, $ACC6, $ACC6\n\tvpmuludq\t$Y1, $TEMP1, $TEMP1\n\t shr\t\\$29, $r3\n\tvmovdqu\t\t32*9-16-128($np), $TEMP0\n\t add\t$r3, %rax\n\tvpaddq\t\t$TEMP1, $ACC7, $ACC7\n\tvpmuludq\t$Y1, $TEMP2, $TEMP2\n\t #vmovdqu\t32*2-24-128($np), $TEMP1\t# moved below\n\t mov\t%rax, $r0\n\t imull\t$n0, %eax\n\tvpaddq\t\t$TEMP2, $ACC8, $ACC8\n\tvpmuludq\t$Y1, $TEMP0, $TEMP0\n\t and\t\\$0x1fffffff, %eax\n\t vmovd\t%eax, $Y1\n\t vmovdqu\t32*3-24-128($np), $TEMP2\n\t.byte\t0x67\n\tvpaddq\t\t$TEMP0, $ACC9, $ACC9\n\t vpbroadcastq\t$Y1, $Y1\n\n\tvpmuludq\t32*2-24-128($np), $Y2, $TEMP1\t# see above\n\tvmovdqu\t\t32*4-24-128($np), $TEMP0\n\t mov\t%rax, %rdx\n\t imulq\t-128($np), %rax\n\t mov\t8(%rsp), $r1\n\tvpaddq\t\t$TEMP1, $ACC2, $ACC1\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*5-24-128($np), $TEMP1\n\t add\t%rax, $r0\n\t mov\t%rdx, %rax\n\t imulq\t8-128($np), %rax\n\t .byte\t0x67\n\t shr\t\\$29, $r0\n\t mov\t16(%rsp), $r2\n\tvpaddq\t\t$TEMP2, $ACC3, $ACC2\n\tvpmuludq\t$Y2, $TEMP0, $TEMP0\n\tvmovdqu\t\t32*6-24-128($np), $TEMP2\n\t add\t%rax, $r1\n\t mov\t%rdx, %rax\n\t imulq\t16-128($np), %rax\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC3\n\tvpmuludq\t$Y2, $TEMP1, $TEMP1\n\tvmovdqu\t\t32*7-24-128($np), $TEMP0\n\t imulq\t24-128($np), %rdx\t\t# future $r3\n\t add\t%rax, $r2\n\t lea\t($r0,$r1), %rax\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC4\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvmovdqu\t\t32*8-24-128($np), $TEMP1\n\t mov\t%rax, $r1\n\t imull\t$n0, %eax\n\tvpmuludq\t$Y2, $TEMP0, $TEMP0\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC5\n\tvmovdqu\t\t32*9-24-128($np), $TEMP2\n\t and\t\\$0x1fffffff, %eax\n\tvpaddq\t\t$TEMP0, $ACC7, $ACC6\n\tvpmuludq\t$Y2, $TEMP1, $TEMP1\n\t add\t24(%rsp), %rdx\n\tvpaddq\t\t$TEMP1, $ACC8, $ACC7\n\tvpmuludq\t$Y2, $TEMP2, $TEMP2\n\tvpaddq\t\t$TEMP2, $ACC9, $ACC8\n\t vmovq\t$r3, $ACC9\n\t mov\t%rdx, $r3\n\n\tdec\t$i\n\tjnz\t.LOOP_REDUCE_1024\n___\n($ACC0,$Y2)=($Y2,$ACC0);\n$code.=<<___;\n\tlea\t448(%rsp), $tp1\t\t\t# size optimization\n\tvpaddq\t$ACC9, $Y2, $ACC0\n\tvpxor\t$ZERO, $ZERO, $ZERO\n\n\tvpaddq\t\t32*9-192($tp0), $ACC0, $ACC0\n\tvpaddq\t\t32*10-448($tp1), $ACC1, $ACC1\n\tvpaddq\t\t32*11-448($tp1), $ACC2, $ACC2\n\tvpaddq\t\t32*12-448($tp1), $ACC3, $ACC3\n\tvpaddq\t\t32*13-448($tp1), $ACC4, $ACC4\n\tvpaddq\t\t32*14-448($tp1), $ACC5, $ACC5\n\tvpaddq\t\t32*15-448($tp1), $ACC6, $ACC6\n\tvpaddq\t\t32*16-448($tp1), $ACC7, $ACC7\n\tvpaddq\t\t32*17-448($tp1), $ACC8, $ACC8\n\n\tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC0, $ACC0\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\tvpsrlq\t\t\\$29, $ACC2, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC2, $ACC2\n\tvpsrlq\t\t\\$29, $ACC3, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC3, $ACC3\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvpblendd\t\\$3, $TEMP4, $ZERO, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC3, $ACC3\n\tvpaddq\t\t$TEMP4, $ACC4, $ACC4\n\n\tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC0, $ACC0\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\tvpsrlq\t\t\\$29, $ACC2, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC2, $ACC2\n\tvpsrlq\t\t\\$29, $ACC3, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC3, $ACC3\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvmovdqu\t\t$ACC0, 32*0-128($rp)\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvmovdqu\t\t$ACC1, 32*1-128($rp)\n\tvpblendd\t\\$3, $TEMP4, $ZERO, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC3, $ACC3\n\tvmovdqu\t\t$ACC2, 32*2-128($rp)\n\tvpaddq\t\t$TEMP4, $ACC4, $ACC4\n\tvmovdqu\t\t$ACC3, 32*3-128($rp)\n___\n$TEMP5=$ACC0;\n$code.=<<___;\n\tvpsrlq\t\t\\$29, $ACC4, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC4, $ACC4\n\tvpsrlq\t\t\\$29, $ACC5, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC5, $ACC5\n\tvpsrlq\t\t\\$29, $ACC6, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC6, $ACC6\n\tvpsrlq\t\t\\$29, $ACC7, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC7, $ACC7\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP5\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP5, $TEMP5\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvpblendd\t\\$3, $TEMP4, $TEMP5, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC7, $ACC7\n\tvpaddq\t\t$TEMP4, $ACC8, $ACC8\n     \n\tvpsrlq\t\t\\$29, $ACC4, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC4, $ACC4\n\tvpsrlq\t\t\\$29, $ACC5, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC5, $ACC5\n\tvpsrlq\t\t\\$29, $ACC6, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC6, $ACC6\n\tvpsrlq\t\t\\$29, $ACC7, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC7, $ACC7\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP5\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP5, $TEMP5\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvmovdqu\t\t$ACC4, 32*4-128($rp)\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvmovdqu\t\t$ACC5, 32*5-128($rp)\n\tvpblendd\t\\$3, $TEMP4, $TEMP5, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC7, $ACC7\n\tvmovdqu\t\t$ACC6, 32*6-128($rp)\n\tvpaddq\t\t$TEMP4, $ACC8, $ACC8\n\tvmovdqu\t\t$ACC7, 32*7-128($rp)\n\tvmovdqu\t\t$ACC8, 32*8-128($rp)\n\n\tmov\t$rp, $ap\n\tdec\t$rep\n\tjne\t.LOOP_GRANDE_SQR_1024\n\n\tvzeroall\n\tmov\t%rbp, %rax\n___\n$code.=<<___ if ($win64);\n\tmovaps\t-0xd8(%rax),%xmm6\n\tmovaps\t-0xc8(%rax),%xmm7\n\tmovaps\t-0xb8(%rax),%xmm8\n\tmovaps\t-0xa8(%rax),%xmm9\n\tmovaps\t-0x98(%rax),%xmm10\n\tmovaps\t-0x88(%rax),%xmm11\n\tmovaps\t-0x78(%rax),%xmm12\n\tmovaps\t-0x68(%rax),%xmm13\n\tmovaps\t-0x58(%rax),%xmm14\n\tmovaps\t-0x48(%rax),%xmm15\n___\n$code.=<<___;\n\tmov\t-48(%rax),%r15\n\tmov\t-40(%rax),%r14\n\tmov\t-32(%rax),%r13\n\tmov\t-24(%rax),%r12\n\tmov\t-16(%rax),%rbp\n\tmov\t-8(%rax),%rbx\n\tlea\t(%rax),%rsp\t\t# restore %rsp\n.Lsqr_1024_epilogue:\n\tret\n.size\trsaz_1024_sqr_avx2,.-rsaz_1024_sqr_avx2\n___\n}\n\n{ # void AMM_WW(\nmy $rp=\"%rdi\";\t# BN_ULONG *rp,\nmy $ap=\"%rsi\";\t# const BN_ULONG *ap,\nmy $bp=\"%rdx\";\t# const BN_ULONG *bp,\nmy $np=\"%rcx\";\t# const BN_ULONG *np,\nmy $n0=\"%r8d\";\t# unsigned int n0);\n\n# The registers that hold the accumulated redundant result\n# The AMM works on 1024 bit operands, and redundant word size is 29\n# Therefore: ceil(1024/29)/4 = 9\nmy $ACC0=\"%ymm0\";\nmy $ACC1=\"%ymm1\";\nmy $ACC2=\"%ymm2\";\nmy $ACC3=\"%ymm3\";\nmy $ACC4=\"%ymm4\";\nmy $ACC5=\"%ymm5\";\nmy $ACC6=\"%ymm6\";\nmy $ACC7=\"%ymm7\";\nmy $ACC8=\"%ymm8\";\nmy $ACC9=\"%ymm9\";\n\n# Registers that hold the broadcasted words of multiplier, currently used\nmy $Bi=\"%ymm10\";\nmy $Yi=\"%ymm11\";\n\n# Helper registers\nmy $TEMP0=$ACC0;\nmy $TEMP1=\"%ymm12\";\nmy $TEMP2=\"%ymm13\";\nmy $ZERO=\"%ymm14\";\nmy $AND_MASK=\"%ymm15\";\n\n# alu registers that hold the first words of the ACC\nmy $r0=\"%r9\";\nmy $r1=\"%r10\";\nmy $r2=\"%r11\";\nmy $r3=\"%r12\";\n\nmy $i=\"%r14d\";\nmy $tmp=\"%r15\";\n\n$bp=\"%r13\";\t# reassigned argument\n\n$code.=<<___;\n.globl\trsaz_1024_mul_avx2\n.type\trsaz_1024_mul_avx2,\\@function,5\n.align\t64\nrsaz_1024_mul_avx2:\n\tlea\t(%rsp), %rax\n\tpush\t%rbx\n\tpush\t%rbp\n\tpush\t%r12\n\tpush\t%r13\n\tpush\t%r14\n\tpush\t%r15\n___\n$code.=<<___ if ($win64);\n\tvzeroupper\n\tlea\t-0xa8(%rsp),%rsp\n\tvmovaps\t%xmm6,-0xd8(%rax)\n\tvmovaps\t%xmm7,-0xc8(%rax)\n\tvmovaps\t%xmm8,-0xb8(%rax)\n\tvmovaps\t%xmm9,-0xa8(%rax)\n\tvmovaps\t%xmm10,-0x98(%rax)\n\tvmovaps\t%xmm11,-0x88(%rax)\n\tvmovaps\t%xmm12,-0x78(%rax)\n\tvmovaps\t%xmm13,-0x68(%rax)\n\tvmovaps\t%xmm14,-0x58(%rax)\n\tvmovaps\t%xmm15,-0x48(%rax)\n.Lmul_1024_body:\n___\n$code.=<<___;\n\tmov\t%rax,%rbp\n\tvzeroall\n\tmov\t%rdx, $bp\t# reassigned argument\n\tsub\t\\$64,%rsp\n\n\t# unaligned 256-bit load that crosses page boundary can\n\t# cause severe performance degradation here, so if $ap does\n\t# cross page boundary, swap it with $bp [meaning that caller\n\t# is advised to lay down $ap and $bp next to each other, so\n\t# that only one can cross page boundary].\n\t.byte\t0x67,0x67\n\tmov\t$ap, $tmp\n\tand\t\\$4095, $tmp\n\tadd\t\\$32*10, $tmp\n\tshr\t\\$12, $tmp\n\tmov\t$ap, $tmp\n\tcmovnz\t$bp, $ap\n\tcmovnz\t$tmp, $bp\n\n\tmov\t$np, $tmp\n\tsub\t\\$-128,$ap\t# size optimization\n\tsub\t\\$-128,$np\n\tsub\t\\$-128,$rp\n\n\tand\t\\$4095, $tmp\t# see if $np crosses page\n\tadd\t\\$32*10, $tmp\n\t.byte\t0x67,0x67\n\tshr\t\\$12, $tmp\n\tjz\t.Lmul_1024_no_n_copy\n\n\t# unaligned 256-bit load that crosses page boundary can\n\t# cause severe performance degradation here, so if $np does\n\t# cross page boundary, copy it to stack and make sure stack\n\t# frame doesn't...\n\tsub\t\t\\$32*10,%rsp\n\tvmovdqu\t\t32*0-128($np), $ACC0\n\tand\t\t\\$-512, %rsp\n\tvmovdqu\t\t32*1-128($np), $ACC1\n\tvmovdqu\t\t32*2-128($np), $ACC2\n\tvmovdqu\t\t32*3-128($np), $ACC3\n\tvmovdqu\t\t32*4-128($np), $ACC4\n\tvmovdqu\t\t32*5-128($np), $ACC5\n\tvmovdqu\t\t32*6-128($np), $ACC6\n\tvmovdqu\t\t32*7-128($np), $ACC7\n\tvmovdqu\t\t32*8-128($np), $ACC8\n\tlea\t\t64+128(%rsp),$np\n\tvmovdqu\t\t$ACC0, 32*0-128($np)\n\tvpxor\t\t$ACC0, $ACC0, $ACC0\n\tvmovdqu\t\t$ACC1, 32*1-128($np)\n\tvpxor\t\t$ACC1, $ACC1, $ACC1\n\tvmovdqu\t\t$ACC2, 32*2-128($np)\n\tvpxor\t\t$ACC2, $ACC2, $ACC2\n\tvmovdqu\t\t$ACC3, 32*3-128($np)\n\tvpxor\t\t$ACC3, $ACC3, $ACC3\n\tvmovdqu\t\t$ACC4, 32*4-128($np)\n\tvpxor\t\t$ACC4, $ACC4, $ACC4\n\tvmovdqu\t\t$ACC5, 32*5-128($np)\n\tvpxor\t\t$ACC5, $ACC5, $ACC5\n\tvmovdqu\t\t$ACC6, 32*6-128($np)\n\tvpxor\t\t$ACC6, $ACC6, $ACC6\n\tvmovdqu\t\t$ACC7, 32*7-128($np)\n\tvpxor\t\t$ACC7, $ACC7, $ACC7\n\tvmovdqu\t\t$ACC8, 32*8-128($np)\n\tvmovdqa\t\t$ACC0, $ACC8\n\tvmovdqu\t\t$ACC9, 32*9-128($np)\t# $ACC9 is zero after vzeroall\n.Lmul_1024_no_n_copy:\n\tand\t\\$-64,%rsp\n\n\tmov\t($bp), %rbx\n\tvpbroadcastq ($bp), $Bi\n\tvmovdqu\t$ACC0, (%rsp)\t\t\t# clear top of stack\n\txor\t$r0, $r0\n\t.byte\t0x67\n\txor\t$r1, $r1\n\txor\t$r2, $r2\n\txor\t$r3, $r3\n\n\tvmovdqu\t.Land_mask(%rip), $AND_MASK\n\tmov\t\\$9, $i\n\tvmovdqu\t$ACC9, 32*9-128($rp)\t\t# $ACC9 is zero after vzeroall\n\tjmp\t.Loop_mul_1024\n\n.align\t32\n.Loop_mul_1024:\n\t vpsrlq\t\t\\$29, $ACC3, $ACC9\t\t# correct $ACC3(*)\n\tmov\t%rbx, %rax\n\timulq\t-128($ap), %rax\n\tadd\t$r0, %rax\n\tmov\t%rbx, $r1\n\timulq\t8-128($ap), $r1\n\tadd\t8(%rsp), $r1\n\n\tmov\t%rax, $r0\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\t mov\t%rbx, $r2\n\t imulq\t16-128($ap), $r2\n\t add\t16(%rsp), $r2\n\n\t mov\t%rbx, $r3\n\t imulq\t24-128($ap), $r3\n\t add\t24(%rsp), $r3\n\tvpmuludq\t32*1-128($ap),$Bi,$TEMP0\n\t vmovd\t\t%eax, $Yi\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t32*2-128($ap),$Bi,$TEMP1\n\t vpbroadcastq\t$Yi, $Yi\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t32*3-128($ap),$Bi,$TEMP2\n\t vpand\t\t$AND_MASK, $ACC3, $ACC3\t\t# correct $ACC3\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t32*4-128($ap),$Bi,$TEMP0\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t32*5-128($ap),$Bi,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t32*6-128($ap),$Bi,$TEMP2\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t32*7-128($ap),$Bi,$TEMP0\n\t vpermq\t\t\\$0x93, $ACC9, $ACC9\t\t# correct $ACC3\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t32*8-128($ap),$Bi,$TEMP1\n\t vpbroadcastq\t8($bp), $Bi\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\n\tmov\t%rax,%rdx\n\timulq\t-128($np),%rax\n\tadd\t%rax,$r0\n\tmov\t%rdx,%rax\n\timulq\t8-128($np),%rax\n\tadd\t%rax,$r1\n\tmov\t%rdx,%rax\n\timulq\t16-128($np),%rax\n\tadd\t%rax,$r2\n\tshr\t\\$29, $r0\n\timulq\t24-128($np),%rdx\n\tadd\t%rdx,$r3\n\tadd\t$r0, $r1\n\n\tvpmuludq\t32*1-128($np),$Yi,$TEMP2\n\t vmovq\t\t$Bi, %rbx\n\tvpaddq\t\t$TEMP2,$ACC1,$ACC1\n\tvpmuludq\t32*2-128($np),$Yi,$TEMP0\n\tvpaddq\t\t$TEMP0,$ACC2,$ACC2\n\tvpmuludq\t32*3-128($np),$Yi,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC3,$ACC3\n\tvpmuludq\t32*4-128($np),$Yi,$TEMP2\n\tvpaddq\t\t$TEMP2,$ACC4,$ACC4\n\tvpmuludq\t32*5-128($np),$Yi,$TEMP0\n\tvpaddq\t\t$TEMP0,$ACC5,$ACC5\n\tvpmuludq\t32*6-128($np),$Yi,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC6,$ACC6\n\tvpmuludq\t32*7-128($np),$Yi,$TEMP2\n\t vpblendd\t\\$3, $ZERO, $ACC9, $TEMP1\t# correct $ACC3\n\tvpaddq\t\t$TEMP2,$ACC7,$ACC7\n\tvpmuludq\t32*8-128($np),$Yi,$TEMP0\n\t vpaddq\t\t$TEMP1, $ACC3, $ACC3\t\t# correct $ACC3\n\tvpaddq\t\t$TEMP0,$ACC8,$ACC8\n\n\tmov\t%rbx, %rax\n\timulq\t-128($ap),%rax\n\tadd\t%rax,$r1\n\t vmovdqu\t-8+32*1-128($ap),$TEMP1\n\tmov\t%rbx, %rax\n\timulq\t8-128($ap),%rax\n\tadd\t%rax,$r2\n\t vmovdqu\t-8+32*2-128($ap),$TEMP2\n\n\tmov\t$r1, %rax\n\t vpblendd\t\\$0xfc, $ZERO, $ACC9, $ACC9\t# correct $ACC3\n\timull\t$n0, %eax\n\t vpaddq\t\t$ACC9,$ACC4,$ACC4\t\t# correct $ACC3\n\tand\t\\$0x1fffffff, %eax\n\n\t imulq\t16-128($ap),%rbx\n\t add\t%rbx,$r3\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\t vmovd\t\t%eax, $Yi\n\tvmovdqu\t\t-8+32*3-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC1,$ACC1\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\t vpbroadcastq\t$Yi, $Yi\n\tvmovdqu\t\t-8+32*4-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC2,$ACC2\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-8+32*5-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC3,$ACC3\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-8+32*6-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC4,$ACC4\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-8+32*7-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC5,$ACC5\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-8+32*8-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC6,$ACC6\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-8+32*9-128($ap),$ACC9\n\tvpaddq\t\t$TEMP1,$ACC7,$ACC7\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvpaddq\t\t$TEMP2,$ACC8,$ACC8\n\tvpmuludq\t$Bi,$ACC9,$ACC9\n\t vpbroadcastq\t16($bp), $Bi\n\n\tmov\t%rax,%rdx\n\timulq\t-128($np),%rax\n\tadd\t%rax,$r1\n\t vmovdqu\t-8+32*1-128($np),$TEMP0\n\tmov\t%rdx,%rax\n\timulq\t8-128($np),%rax\n\tadd\t%rax,$r2\n\t vmovdqu\t-8+32*2-128($np),$TEMP1\n\tshr\t\\$29, $r1\n\timulq\t16-128($np),%rdx\n\tadd\t%rdx,$r3\n\tadd\t$r1, $r2\n\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\t vmovq\t\t$Bi, %rbx\n\tvmovdqu\t\t-8+32*3-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-8+32*4-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-8+32*5-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-8+32*6-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-8+32*7-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-8+32*8-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-8+32*9-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC9\n\n\t vmovdqu\t-16+32*1-128($ap),$TEMP0\n\tmov\t%rbx,%rax\n\timulq\t-128($ap),%rax\n\tadd\t$r2,%rax\n\n\t vmovdqu\t-16+32*2-128($ap),$TEMP1\n\tmov\t%rax,$r2\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\t imulq\t8-128($ap),%rbx\n\t add\t%rbx,$r3\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\t vmovd\t\t%eax, $Yi\n\tvmovdqu\t\t-16+32*3-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\t vpbroadcastq\t$Yi, $Yi\n\tvmovdqu\t\t-16+32*4-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-16+32*5-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-16+32*6-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-16+32*7-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-16+32*8-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-16+32*9-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\t vpbroadcastq\t24($bp), $Bi\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC9\n\n\t vmovdqu\t-16+32*1-128($np),$TEMP0\n\tmov\t%rax,%rdx\n\timulq\t-128($np),%rax\n\tadd\t%rax,$r2\n\t vmovdqu\t-16+32*2-128($np),$TEMP1\n\timulq\t8-128($np),%rdx\n\tadd\t%rdx,$r3\n\tshr\t\\$29, $r2\n\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\t vmovq\t\t$Bi, %rbx\n\tvmovdqu\t\t-16+32*3-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-16+32*4-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-16+32*5-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-16+32*6-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-16+32*7-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-16+32*8-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-16+32*9-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\t vmovdqu\t-24+32*1-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\t vmovdqu\t-24+32*2-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC9\n\n\tadd\t$r2, $r3\n\timulq\t-128($ap),%rbx\n\tadd\t%rbx,$r3\n\n\tmov\t$r3, %rax\n\timull\t$n0, %eax\n\tand\t\\$0x1fffffff, %eax\n\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\t vmovd\t\t%eax, $Yi\n\tvmovdqu\t\t-24+32*3-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC1\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\t vpbroadcastq\t$Yi, $Yi\n\tvmovdqu\t\t-24+32*4-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC2\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-24+32*5-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC3\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-24+32*6-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC4\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-24+32*7-128($ap),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC5\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-24+32*8-128($ap),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC6\n\tvpmuludq\t$Bi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-24+32*9-128($ap),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC7\n\tvpmuludq\t$Bi,$TEMP1,$TEMP1\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC8\n\tvpmuludq\t$Bi,$TEMP2,$TEMP2\n\t vpbroadcastq\t32($bp), $Bi\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC9\n\t add\t\t\\$32, $bp\t\t\t# $bp++\n\n\tvmovdqu\t\t-24+32*1-128($np),$TEMP0\n\timulq\t-128($np),%rax\n\tadd\t%rax,$r3\n\tshr\t\\$29, $r3\n\n\tvmovdqu\t\t-24+32*2-128($np),$TEMP1\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\t vmovq\t\t$Bi, %rbx\n\tvmovdqu\t\t-24+32*3-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC1,$ACC0\t\t# $ACC0==$TEMP0\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\t vmovdqu\t$ACC0, (%rsp)\t\t\t# transfer $r0-$r3\n\tvpaddq\t\t$TEMP1,$ACC2,$ACC1\n\tvmovdqu\t\t-24+32*4-128($np),$TEMP0\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-24+32*5-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC3,$ACC2\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-24+32*6-128($np),$TEMP2\n\tvpaddq\t\t$TEMP0,$ACC4,$ACC3\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\tvmovdqu\t\t-24+32*7-128($np),$TEMP0\n\tvpaddq\t\t$TEMP1,$ACC5,$ACC4\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\tvmovdqu\t\t-24+32*8-128($np),$TEMP1\n\tvpaddq\t\t$TEMP2,$ACC6,$ACC5\n\tvpmuludq\t$Yi,$TEMP0,$TEMP0\n\tvmovdqu\t\t-24+32*9-128($np),$TEMP2\n\t mov\t$r3, $r0\n\tvpaddq\t\t$TEMP0,$ACC7,$ACC6\n\tvpmuludq\t$Yi,$TEMP1,$TEMP1\n\t add\t(%rsp), $r0\n\tvpaddq\t\t$TEMP1,$ACC8,$ACC7\n\tvpmuludq\t$Yi,$TEMP2,$TEMP2\n\t vmovq\t$r3, $TEMP1\n\tvpaddq\t\t$TEMP2,$ACC9,$ACC8\n\n\tdec\t$i\n\tjnz\t.Loop_mul_1024\n___\n\n# (*)\tOriginal implementation was correcting ACC1-ACC3 for overflow\n#\tafter 7 loop runs, or after 28 iterations, or 56 additions.\n#\tBut as we underutilize resources, it's possible to correct in\n#\teach iteration with marginal performance loss. But then, as\n#\twe do it in each iteration, we can correct less digits, and\n#\tavoid performance penalties completely.\n\n$TEMP0 = $ACC9;\n$TEMP3 = $Bi;\n$TEMP4 = $Yi;\n$code.=<<___;\n\tvpaddq\t\t(%rsp), $TEMP1, $ACC0\n\n\tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC0, $ACC0\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\tvpsrlq\t\t\\$29, $ACC2, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC2, $ACC2\n\tvpsrlq\t\t\\$29, $ACC3, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC3, $ACC3\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvpblendd\t\\$3, $TEMP4, $ZERO, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC3, $ACC3\n\tvpaddq\t\t$TEMP4, $ACC4, $ACC4\n\n\tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC0, $ACC0\n\tvpsrlq\t\t\\$29, $ACC1, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC1, $ACC1\n\tvpsrlq\t\t\\$29, $ACC2, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC2, $ACC2\n\tvpsrlq\t\t\\$29, $ACC3, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC3, $ACC3\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC0, $ACC0\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC1, $ACC1\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC2, $ACC2\n\tvpblendd\t\\$3, $TEMP4, $ZERO, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC3, $ACC3\n\tvpaddq\t\t$TEMP4, $ACC4, $ACC4\n\n\tvmovdqu\t\t$ACC0, 0-128($rp)\n\tvmovdqu\t\t$ACC1, 32-128($rp)\n\tvmovdqu\t\t$ACC2, 64-128($rp)\n\tvmovdqu\t\t$ACC3, 96-128($rp)\n___\n\n$TEMP5=$ACC0;\n$code.=<<___;\n\tvpsrlq\t\t\\$29, $ACC4, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC4, $ACC4\n\tvpsrlq\t\t\\$29, $ACC5, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC5, $ACC5\n\tvpsrlq\t\t\\$29, $ACC6, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC6, $ACC6\n\tvpsrlq\t\t\\$29, $ACC7, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC7, $ACC7\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP5\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP5, $TEMP5\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvpblendd\t\\$3, $TEMP4, $TEMP5, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC7, $ACC7\n\tvpaddq\t\t$TEMP4, $ACC8, $ACC8\n\n\tvpsrlq\t\t\\$29, $ACC4, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC4, $ACC4\n\tvpsrlq\t\t\\$29, $ACC5, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC5, $ACC5\n\tvpsrlq\t\t\\$29, $ACC6, $TEMP3\n\tvpermq\t\t\\$0x93, $TEMP1, $TEMP1\n\tvpand\t\t$AND_MASK, $ACC6, $ACC6\n\tvpsrlq\t\t\\$29, $ACC7, $TEMP4\n\tvpermq\t\t\\$0x93, $TEMP2, $TEMP2\n\tvpand\t\t$AND_MASK, $ACC7, $ACC7\n\tvpsrlq\t\t\\$29, $ACC8, $TEMP5\n\tvpermq\t\t\\$0x93, $TEMP3, $TEMP3\n\tvpand\t\t$AND_MASK, $ACC8, $ACC8\n\tvpermq\t\t\\$0x93, $TEMP4, $TEMP4\n\n\tvpblendd\t\\$3, $ZERO, $TEMP1, $TEMP0\n\tvpermq\t\t\\$0x93, $TEMP5, $TEMP5\n\tvpblendd\t\\$3, $TEMP1, $TEMP2, $TEMP1\n\tvpaddq\t\t$TEMP0, $ACC4, $ACC4\n\tvpblendd\t\\$3, $TEMP2, $TEMP3, $TEMP2\n\tvpaddq\t\t$TEMP1, $ACC5, $ACC5\n\tvpblendd\t\\$3, $TEMP3, $TEMP4, $TEMP3\n\tvpaddq\t\t$TEMP2, $ACC6, $ACC6\n\tvpblendd\t\\$3, $TEMP4, $TEMP5, $TEMP4\n\tvpaddq\t\t$TEMP3, $ACC7, $ACC7\n\tvpaddq\t\t$TEMP4, $ACC8, $ACC8\n\n\tvmovdqu\t\t$ACC4, 128-128($rp)\n\tvmovdqu\t\t$ACC5, 160-128($rp)    \n\tvmovdqu\t\t$ACC6, 192-128($rp)\n\tvmovdqu\t\t$ACC7, 224-128($rp)\n\tvmovdqu\t\t$ACC8, 256-128($rp)\n\tvzeroupper\n\n\tmov\t%rbp, %rax\n___\n$code.=<<___ if ($win64);\n\tmovaps\t-0xd8(%rax),%xmm6\n\tmovaps\t-0xc8(%rax),%xmm7\n\tmovaps\t-0xb8(%rax),%xmm8\n\tmovaps\t-0xa8(%rax),%xmm9\n\tmovaps\t-0x98(%rax),%xmm10\n\tmovaps\t-0x88(%rax),%xmm11\n\tmovaps\t-0x78(%rax),%xmm12\n\tmovaps\t-0x68(%rax),%xmm13\n\tmovaps\t-0x58(%rax),%xmm14\n\tmovaps\t-0x48(%rax),%xmm15\n___\n$code.=<<___;\n\tmov\t-48(%rax),%r15\n\tmov\t-40(%rax),%r14\n\tmov\t-32(%rax),%r13\n\tmov\t-24(%rax),%r12\n\tmov\t-16(%rax),%rbp\n\tmov\t-8(%rax),%rbx\n\tlea\t(%rax),%rsp\t\t# restore %rsp\n.Lmul_1024_epilogue:\n\tret\n.size\trsaz_1024_mul_avx2,.-rsaz_1024_mul_avx2\n___\n}\n{\nmy ($out,$inp) = $win64 ? (\"%rcx\",\"%rdx\") : (\"%rdi\",\"%rsi\");\nmy @T = map(\"%r$_\",(8..11));\n\n$code.=<<___;\n.globl\trsaz_1024_red2norm_avx2\n.type\trsaz_1024_red2norm_avx2,\\@abi-omnipotent\n.align\t32\nrsaz_1024_red2norm_avx2:\n\tsub\t\\$-128,$inp\t# size optimization\n\txor\t%rax,%rax\n___\n\nfor ($j=0,$i=0; $i<16; $i++) {\n    my $k=0;\n    while (29*$j<64*($i+1)) {\t# load data till boundary\n\t$code.=\"\tmov\t`8*$j-128`($inp), @T[0]\\n\";\n\t$j++; $k++; push(@T,shift(@T));\n    }\n    $l=$k;\n    while ($k>1) {\t\t# shift loaded data but last value\n\t$code.=\"\tshl\t\\$`29*($j-$k)`,@T[-$k]\\n\";\n\t$k--;\n    }\n    $code.=<<___;\t\t# shift last value\n\tmov\t@T[-1], @T[0]\n\tshl\t\\$`29*($j-1)`, @T[-1]\n\tshr\t\\$`-29*($j-1)`, @T[0]\n___\n    while ($l) {\t\t# accumulate all values\n\t$code.=\"\tadd\t@T[-$l], %rax\\n\";\n\t$l--;\n    }\n\t$code.=<<___;\n\tadc\t\\$0, @T[0]\t# consume eventual carry\n\tmov\t%rax, 8*$i($out)\n\tmov\t@T[0], %rax\n___\n    push(@T,shift(@T));\n}\n$code.=<<___;\n\tret\n.size\trsaz_1024_red2norm_avx2,.-rsaz_1024_red2norm_avx2\n\n.globl\trsaz_1024_norm2red_avx2\n.type\trsaz_1024_norm2red_avx2,\\@abi-omnipotent\n.align\t32\nrsaz_1024_norm2red_avx2:\n\tsub\t\\$-128,$out\t# size optimization\n\tmov\t($inp),@T[0]\n\tmov\t\\$0x1fffffff,%eax\n___\nfor ($j=0,$i=0; $i<16; $i++) {\n    $code.=\"\tmov\t`8*($i+1)`($inp),@T[1]\\n\"\tif ($i<15);\n    $code.=\"\txor\t@T[1],@T[1]\\n\"\t\t\tif ($i==15);\n    my $k=1;\n    while (29*($j+1)<64*($i+1)) {\n    \t$code.=<<___;\n\tmov\t@T[0],@T[-$k]\n\tshr\t\\$`29*$j`,@T[-$k]\n\tand\t%rax,@T[-$k]\t\t\t\t# &0x1fffffff\n\tmov\t@T[-$k],`8*$j-128`($out)\n___\n\t$j++; $k++;\n    }\n    $code.=<<___;\n\tshrd\t\\$`29*$j`,@T[1],@T[0]\n\tand\t%rax,@T[0]\n\tmov\t@T[0],`8*$j-128`($out)\n___\n    $j++;\n    push(@T,shift(@T));\n}\n$code.=<<___;\n\tmov\t@T[0],`8*$j-128`($out)\t\t\t# zero\n\tmov\t@T[0],`8*($j+1)-128`($out)\n\tmov\t@T[0],`8*($j+2)-128`($out)\n\tmov\t@T[0],`8*($j+3)-128`($out)\n\tret\n.size\trsaz_1024_norm2red_avx2,.-rsaz_1024_norm2red_avx2\n___\n}\n{\nmy ($out,$inp,$power) = $win64 ? (\"%rcx\",\"%rdx\",\"%r8d\") : (\"%rdi\",\"%rsi\",\"%edx\");\n\n$code.=<<___;\n.globl\trsaz_1024_scatter5_avx2\n.type\trsaz_1024_scatter5_avx2,\\@abi-omnipotent\n.align\t32\nrsaz_1024_scatter5_avx2:\n\tvzeroupper\n\tvmovdqu\t.Lscatter_permd(%rip),%ymm5\n\tshl\t\\$4,$power\n\tlea\t($out,$power),$out\n\tmov\t\\$9,%eax\n\tjmp\t.Loop_scatter_1024\n\n.align\t32\n.Loop_scatter_1024:\n\tvmovdqu\t\t($inp),%ymm0\n\tlea\t\t32($inp),$inp\n\tvpermd\t\t%ymm0,%ymm5,%ymm0\n\tvmovdqu\t\t%xmm0,($out)\n\tlea\t\t16*32($out),$out\n\tdec\t%eax\n\tjnz\t.Loop_scatter_1024\n\n\tvzeroupper\n\tret\n.size\trsaz_1024_scatter5_avx2,.-rsaz_1024_scatter5_avx2\n\n.globl\trsaz_1024_gather5_avx2\n.type\trsaz_1024_gather5_avx2,\\@abi-omnipotent\n.align\t32\nrsaz_1024_gather5_avx2:\n\tvzeroupper\n\tmov\t%rsp,%r11\n___\n$code.=<<___ if ($win64);\n\tlea\t-0x88(%rsp),%rax\n.LSEH_begin_rsaz_1024_gather5:\n\t# I can't trust assembler to use specific encoding:-(\n\t.byte\t0x48,0x8d,0x60,0xe0\t\t# lea\t-0x20(%rax),%rsp\n\t.byte\t0xc5,0xf8,0x29,0x70,0xe0\t# vmovaps %xmm6,-0x20(%rax)\n\t.byte\t0xc5,0xf8,0x29,0x78,0xf0\t# vmovaps %xmm7,-0x10(%rax)\n\t.byte\t0xc5,0x78,0x29,0x40,0x00\t# vmovaps %xmm8,0(%rax)\n\t.byte\t0xc5,0x78,0x29,0x48,0x10\t# vmovaps %xmm9,0x10(%rax)\n\t.byte\t0xc5,0x78,0x29,0x50,0x20\t# vmovaps %xmm10,0x20(%rax)\n\t.byte\t0xc5,0x78,0x29,0x58,0x30\t# vmovaps %xmm11,0x30(%rax)\n\t.byte\t0xc5,0x78,0x29,0x60,0x40\t# vmovaps %xmm12,0x40(%rax)\n\t.byte\t0xc5,0x78,0x29,0x68,0x50\t# vmovaps %xmm13,0x50(%rax)\n\t.byte\t0xc5,0x78,0x29,0x70,0x60\t# vmovaps %xmm14,0x60(%rax)\n\t.byte\t0xc5,0x78,0x29,0x78,0x70\t# vmovaps %xmm15,0x70(%rax)\n___\n$code.=<<___;\n\tlea\t-0x100(%rsp),%rsp\n\tand\t\\$-32, %rsp\n\tlea\t.Linc(%rip), %r10\n\tlea\t-128(%rsp),%rax\t\t\t# control u-op density\n\n\tvmovd\t\t$power, %xmm4\n\tvmovdqa\t\t(%r10),%ymm0\n\tvmovdqa\t\t32(%r10),%ymm1\n\tvmovdqa\t\t64(%r10),%ymm5\n\tvpbroadcastd\t%xmm4,%ymm4\n\n\tvpaddd\t\t%ymm5, %ymm0, %ymm2\n\tvpcmpeqd\t%ymm4, %ymm0, %ymm0\n\tvpaddd\t\t%ymm5, %ymm1, %ymm3\n\tvpcmpeqd\t%ymm4, %ymm1, %ymm1\n\tvmovdqa\t\t%ymm0, 32*0+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm2, %ymm0\n\tvpcmpeqd\t%ymm4, %ymm2, %ymm2\n\tvmovdqa\t\t%ymm1, 32*1+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm3, %ymm1\n\tvpcmpeqd\t%ymm4, %ymm3, %ymm3\n\tvmovdqa\t\t%ymm2, 32*2+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm0, %ymm2\n\tvpcmpeqd\t%ymm4, %ymm0, %ymm0\n\tvmovdqa\t\t%ymm3, 32*3+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm1, %ymm3\n\tvpcmpeqd\t%ymm4, %ymm1, %ymm1\n\tvmovdqa\t\t%ymm0, 32*4+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm2, %ymm8\n\tvpcmpeqd\t%ymm4, %ymm2, %ymm2\n\tvmovdqa\t\t%ymm1, 32*5+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm3, %ymm9\n\tvpcmpeqd\t%ymm4, %ymm3, %ymm3\n\tvmovdqa\t\t%ymm2, 32*6+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm8, %ymm10\n\tvpcmpeqd\t%ymm4, %ymm8, %ymm8\n\tvmovdqa\t\t%ymm3, 32*7+128(%rax)\n\tvpaddd\t\t%ymm5, %ymm9, %ymm11\n\tvpcmpeqd\t%ymm4, %ymm9, %ymm9\n\tvpaddd\t\t%ymm5, %ymm10, %ymm12\n\tvpcmpeqd\t%ymm4, %ymm10, %ymm10\n\tvpaddd\t\t%ymm5, %ymm11, %ymm13\n\tvpcmpeqd\t%ymm4, %ymm11, %ymm11\n\tvpaddd\t\t%ymm5, %ymm12, %ymm14\n\tvpcmpeqd\t%ymm4, %ymm12, %ymm12\n\tvpaddd\t\t%ymm5, %ymm13, %ymm15\n\tvpcmpeqd\t%ymm4, %ymm13, %ymm13\n\tvpcmpeqd\t%ymm4, %ymm14, %ymm14\n\tvpcmpeqd\t%ymm4, %ymm15, %ymm15\n\n\tvmovdqa\t-32(%r10),%ymm7\t\t\t# .Lgather_permd\n\tlea\t128($inp), $inp\n\tmov\t\\$9,$power\n\n.Loop_gather_1024:\n\tvmovdqa\t\t32*0-128($inp),\t%ymm0\n\tvmovdqa\t\t32*1-128($inp),\t%ymm1\n\tvmovdqa\t\t32*2-128($inp),\t%ymm2\n\tvmovdqa\t\t32*3-128($inp),\t%ymm3\n\tvpand\t\t32*0+128(%rax),\t%ymm0,\t%ymm0\n\tvpand\t\t32*1+128(%rax),\t%ymm1,\t%ymm1\n\tvpand\t\t32*2+128(%rax),\t%ymm2,\t%ymm2\n\tvpor\t\t%ymm0, %ymm1, %ymm4\n\tvpand\t\t32*3+128(%rax),\t%ymm3,\t%ymm3\n\tvmovdqa\t\t32*4-128($inp),\t%ymm0\n\tvmovdqa\t\t32*5-128($inp),\t%ymm1\n\tvpor\t\t%ymm2, %ymm3, %ymm5\n\tvmovdqa\t\t32*6-128($inp),\t%ymm2\n\tvmovdqa\t\t32*7-128($inp),\t%ymm3\n\tvpand\t\t32*4+128(%rax),\t%ymm0,\t%ymm0\n\tvpand\t\t32*5+128(%rax),\t%ymm1,\t%ymm1\n\tvpand\t\t32*6+128(%rax),\t%ymm2,\t%ymm2\n\tvpor\t\t%ymm0, %ymm4, %ymm4\n\tvpand\t\t32*7+128(%rax),\t%ymm3,\t%ymm3\n\tvpand\t\t32*8-128($inp),\t%ymm8,\t%ymm0\n\tvpor\t\t%ymm1, %ymm5, %ymm5\n\tvpand\t\t32*9-128($inp),\t%ymm9,\t%ymm1\n\tvpor\t\t%ymm2, %ymm4, %ymm4\n\tvpand\t\t32*10-128($inp),%ymm10,\t%ymm2\n\tvpor\t\t%ymm3, %ymm5, %ymm5\n\tvpand\t\t32*11-128($inp),%ymm11,\t%ymm3\n\tvpor\t\t%ymm0, %ymm4, %ymm4\n\tvpand\t\t32*12-128($inp),%ymm12,\t%ymm0\n\tvpor\t\t%ymm1, %ymm5, %ymm5\n\tvpand\t\t32*13-128($inp),%ymm13,\t%ymm1\n\tvpor\t\t%ymm2, %ymm4, %ymm4\n\tvpand\t\t32*14-128($inp),%ymm14,\t%ymm2\n\tvpor\t\t%ymm3, %ymm5, %ymm5\n\tvpand\t\t32*15-128($inp),%ymm15,\t%ymm3\n\tlea\t\t32*16($inp), $inp\n\tvpor\t\t%ymm0, %ymm4, %ymm4\n\tvpor\t\t%ymm1, %ymm5, %ymm5\n\tvpor\t\t%ymm2, %ymm4, %ymm4\n\tvpor\t\t%ymm3, %ymm5, %ymm5\n\n\tvpor\t\t%ymm5, %ymm4, %ymm4\n\tvextracti128\t\\$1, %ymm4, %xmm5\t# upper half is cleared\n\tvpor\t\t%xmm4, %xmm5, %xmm5\n\tvpermd\t\t%ymm5,%ymm7,%ymm5\n\tvmovdqu\t\t%ymm5,($out)\n\tlea\t\t32($out),$out\n\tdec\t$power\n\tjnz\t.Loop_gather_1024\n\n\tvpxor\t%ymm0,%ymm0,%ymm0\n\tvmovdqu\t%ymm0,($out)\n\tvzeroupper\n___\n$code.=<<___ if ($win64);\n\tmovaps\t-0xa8(%r11),%xmm6\n\tmovaps\t-0x98(%r11),%xmm7\n\tmovaps\t-0x88(%r11),%xmm8\n\tmovaps\t-0x78(%r11),%xmm9\n\tmovaps\t-0x68(%r11),%xmm10\n\tmovaps\t-0x58(%r11),%xmm11\n\tmovaps\t-0x48(%r11),%xmm12\n\tmovaps\t-0x38(%r11),%xmm13\n\tmovaps\t-0x28(%r11),%xmm14\n\tmovaps\t-0x18(%r11),%xmm15\n.LSEH_end_rsaz_1024_gather5:\n___\n$code.=<<___;\n\tlea\t(%r11),%rsp\n\tret\n.size\trsaz_1024_gather5_avx2,.-rsaz_1024_gather5_avx2\n___\n}\n\n$code.=<<___;\n.extern\tOPENSSL_ia32cap_P\n.globl\trsaz_avx2_eligible\n.type\trsaz_avx2_eligible,\\@abi-omnipotent\n.align\t32\nrsaz_avx2_eligible:\n\tmov\tOPENSSL_ia32cap_P+8(%rip),%eax\n___\n$code.=<<___\tif ($addx);\n\tmov\t\\$`1<<8|1<<19`,%ecx\n\tmov\t\\$0,%edx\n\tand\t%eax,%ecx\n\tcmp\t\\$`1<<8|1<<19`,%ecx\t# check for BMI2+AD*X\n\tcmove\t%edx,%eax\n___\n$code.=<<___;\n\tand\t\\$`1<<5`,%eax\n\tshr\t\\$5,%eax\n\tret\n.size\trsaz_avx2_eligible,.-rsaz_avx2_eligible\n\n.align\t64\n.Land_mask:\n\t.quad\t0x1fffffff,0x1fffffff,0x1fffffff,0x1fffffff\n.Lscatter_permd:\n\t.long\t0,2,4,6,7,7,7,7\n.Lgather_permd:\n\t.long\t0,7,1,7,2,7,3,7\n.Linc:\n\t.long\t0,0,0,0, 1,1,1,1\n\t.long\t2,2,2,2, 3,3,3,3\n\t.long\t4,4,4,4, 4,4,4,4\n.align\t64\n___\n\nif ($win64) {\n$rec=\"%rcx\";\n$frame=\"%rdx\";\n$context=\"%r8\";\n$disp=\"%r9\";\n\n$code.=<<___\n.extern\t__imp_RtlVirtualUnwind\n.type\trsaz_se_handler,\\@abi-omnipotent\n.align\t16\nrsaz_se_handler:\n\tpush\t%rsi\n\tpush\t%rdi\n\tpush\t%rbx\n\tpush\t%rbp\n\tpush\t%r12\n\tpush\t%r13\n\tpush\t%r14\n\tpush\t%r15\n\tpushfq\n\tsub\t\\$64,%rsp\n\n\tmov\t120($context),%rax\t# pull context->Rax\n\tmov\t248($context),%rbx\t# pull context->Rip\n\n\tmov\t8($disp),%rsi\t\t# disp->ImageBase\n\tmov\t56($disp),%r11\t\t# disp->HandlerData\n\n\tmov\t0(%r11),%r10d\t\t# HandlerData[0]\n\tlea\t(%rsi,%r10),%r10\t# prologue label\n\tcmp\t%r10,%rbx\t\t# context->Rip<prologue label\n\tjb\t.Lcommon_seh_tail\n\n\tmov\t152($context),%rax\t# pull context->Rsp\n\n\tmov\t4(%r11),%r10d\t\t# HandlerData[1]\n\tlea\t(%rsi,%r10),%r10\t# epilogue label\n\tcmp\t%r10,%rbx\t\t# context->Rip>=epilogue label\n\tjae\t.Lcommon_seh_tail\n\n\tmov\t160($context),%rax\t# pull context->Rbp\n\n\tmov\t-48(%rax),%r15\n\tmov\t-40(%rax),%r14\n\tmov\t-32(%rax),%r13\n\tmov\t-24(%rax),%r12\n\tmov\t-16(%rax),%rbp\n\tmov\t-8(%rax),%rbx\n\tmov\t%r15,240($context)\n\tmov\t%r14,232($context)\n\tmov\t%r13,224($context)\n\tmov\t%r12,216($context)\n\tmov\t%rbp,160($context)\n\tmov\t%rbx,144($context)\n\n\tlea\t-0xd8(%rax),%rsi\t# %xmm save area\n\tlea\t512($context),%rdi\t# & context.Xmm6\n\tmov\t\\$20,%ecx\t\t# 10*sizeof(%xmm0)/sizeof(%rax)\n\t.long\t0xa548f3fc\t\t# cld; rep movsq\n\n.Lcommon_seh_tail:\n\tmov\t8(%rax),%rdi\n\tmov\t16(%rax),%rsi\n\tmov\t%rax,152($context)\t# restore context->Rsp\n\tmov\t%rsi,168($context)\t# restore context->Rsi\n\tmov\t%rdi,176($context)\t# restore context->Rdi\n\n\tmov\t40($disp),%rdi\t\t# disp->ContextRecord\n\tmov\t$context,%rsi\t\t# context\n\tmov\t\\$154,%ecx\t\t# sizeof(CONTEXT)\n\t.long\t0xa548f3fc\t\t# cld; rep movsq\n\n\tmov\t$disp,%rsi\n\txor\t%rcx,%rcx\t\t# arg1, UNW_FLAG_NHANDLER\n\tmov\t8(%rsi),%rdx\t\t# arg2, disp->ImageBase\n\tmov\t0(%rsi),%r8\t\t# arg3, disp->ControlPc\n\tmov\t16(%rsi),%r9\t\t# arg4, disp->FunctionEntry\n\tmov\t40(%rsi),%r10\t\t# disp->ContextRecord\n\tlea\t56(%rsi),%r11\t\t# &disp->HandlerData\n\tlea\t24(%rsi),%r12\t\t# &disp->EstablisherFrame\n\tmov\t%r10,32(%rsp)\t\t# arg5\n\tmov\t%r11,40(%rsp)\t\t# arg6\n\tmov\t%r12,48(%rsp)\t\t# arg7\n\tmov\t%rcx,56(%rsp)\t\t# arg8, (NULL)\n\tcall\t*__imp_RtlVirtualUnwind(%rip)\n\n\tmov\t\\$1,%eax\t\t# ExceptionContinueSearch\n\tadd\t\\$64,%rsp\n\tpopfq\n\tpop\t%r15\n\tpop\t%r14\n\tpop\t%r13\n\tpop\t%r12\n\tpop\t%rbp\n\tpop\t%rbx\n\tpop\t%rdi\n\tpop\t%rsi\n\tret\n.size\trsaz_se_handler,.-rsaz_se_handler\n\n.section\t.pdata\n.align\t4\n\t.rva\t.LSEH_begin_rsaz_1024_sqr_avx2\n\t.rva\t.LSEH_end_rsaz_1024_sqr_avx2\n\t.rva\t.LSEH_info_rsaz_1024_sqr_avx2\n\n\t.rva\t.LSEH_begin_rsaz_1024_mul_avx2\n\t.rva\t.LSEH_end_rsaz_1024_mul_avx2\n\t.rva\t.LSEH_info_rsaz_1024_mul_avx2\n\n\t.rva\t.LSEH_begin_rsaz_1024_gather5\n\t.rva\t.LSEH_end_rsaz_1024_gather5\n\t.rva\t.LSEH_info_rsaz_1024_gather5\n.section\t.xdata\n.align\t8\n.LSEH_info_rsaz_1024_sqr_avx2:\n\t.byte\t9,0,0,0\n\t.rva\trsaz_se_handler\n\t.rva\t.Lsqr_1024_body,.Lsqr_1024_epilogue\n.LSEH_info_rsaz_1024_mul_avx2:\n\t.byte\t9,0,0,0\n\t.rva\trsaz_se_handler\n\t.rva\t.Lmul_1024_body,.Lmul_1024_epilogue\n.LSEH_info_rsaz_1024_gather5:\n\t.byte\t0x01,0x36,0x17,0x0b\n\t.byte\t0x36,0xf8,0x09,0x00\t# vmovaps 0x90(rsp),xmm15\n\t.byte\t0x31,0xe8,0x08,0x00\t# vmovaps 0x80(rsp),xmm14\n\t.byte\t0x2c,0xd8,0x07,0x00\t# vmovaps 0x70(rsp),xmm13\n\t.byte\t0x27,0xc8,0x06,0x00\t# vmovaps 0x60(rsp),xmm12\n\t.byte\t0x22,0xb8,0x05,0x00\t# vmovaps 0x50(rsp),xmm11\n\t.byte\t0x1d,0xa8,0x04,0x00\t# vmovaps 0x40(rsp),xmm10\n\t.byte\t0x18,0x98,0x03,0x00\t# vmovaps 0x30(rsp),xmm9\n\t.byte\t0x13,0x88,0x02,0x00\t# vmovaps 0x20(rsp),xmm8\n\t.byte\t0x0e,0x78,0x01,0x00\t# vmovaps 0x10(rsp),xmm7\n\t.byte\t0x09,0x68,0x00,0x00\t# vmovaps 0x00(rsp),xmm6\n\t.byte\t0x04,0x01,0x15,0x00\t# sub\t  rsp,0xa8\n\t.byte\t0x00,0xb3,0x00,0x00\t# set_frame r11\n___\n}\n\nforeach (split(\"\\n\",$code)) {\n\ts/\\`([^\\`]*)\\`/eval($1)/ge;\n\n\ts/\\b(sh[rl]d?\\s+\\$)(-?[0-9]+)/$1.$2%64/ge\t\tor\n\n\ts/\\b(vmov[dq])\\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go\t\tor\n\ts/\\b(vmovdqu)\\b(.+)%x%ymm([0-9]+)/$1$2%xmm$3/go\t\tor\n\ts/\\b(vpinsr[qd])\\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go\tor\n\ts/\\b(vpextr[qd])\\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go\tor\n\ts/\\b(vpbroadcast[qd]\\s+)%ymm([0-9]+)/$1%xmm$2/go;\n\tprint $_,\"\\n\";\n}\n\n}}} else {{{\nprint <<___;\t# assembler is too old\n.text\n\n.globl\trsaz_avx2_eligible\n.type\trsaz_avx2_eligible,\\@abi-omnipotent\nrsaz_avx2_eligible:\n\txor\t%eax,%eax\n\tret\n.size\trsaz_avx2_eligible,.-rsaz_avx2_eligible\n\n.globl\trsaz_1024_sqr_avx2\n.globl\trsaz_1024_mul_avx2\n.globl\trsaz_1024_norm2red_avx2\n.globl\trsaz_1024_red2norm_avx2\n.globl\trsaz_1024_scatter5_avx2\n.globl\trsaz_1024_gather5_avx2\n.type\trsaz_1024_sqr_avx2,\\@abi-omnipotent\nrsaz_1024_sqr_avx2:\nrsaz_1024_mul_avx2:\nrsaz_1024_norm2red_avx2:\nrsaz_1024_red2norm_avx2:\nrsaz_1024_scatter5_avx2:\nrsaz_1024_gather5_avx2:\n\t.byte\t0x0f,0x0b\t# ud2\n\tret\n.size\trsaz_1024_sqr_avx2,.-rsaz_1024_sqr_avx2\n___\n}}}\n\nclose STDOUT;\n"], "filenames": ["crypto/bn/asm/rsaz-avx2.pl"], "buggy_code_start_loc": [249], "buggy_code_end_loc": [1774], "fixing_code_start_loc": [249], "fixing_code_end_loc": [1773], "type": "CWE-200", "message": "There is an overflow bug in the AVX2 Montgomery multiplication procedure used in exponentiation with 1024-bit moduli. No EC algorithms are affected. Analysis suggests that attacks against RSA and DSA as a result of this defect would be very difficult to perform and are not believed likely. Attacks against DH1024 are considered just feasible, because most of the work necessary to deduce information about a private key may be performed offline. The amount of resources required for such an attack would be significant. However, for an attack on TLS to be meaningful, the server would have to share the DH1024 private key among multiple clients, which is no longer an option since CVE-2016-0701. This only affects processors that support the AVX2 but not ADX extensions like Intel Haswell (4th generation). Note: The impact from this issue is similar to CVE-2017-3736, CVE-2017-3732 and CVE-2015-3193. OpenSSL version 1.0.2-1.0.2m and 1.1.0-1.1.0g are affected. Fixed in OpenSSL 1.0.2n. Due to the low severity of this issue we are not issuing a new release of OpenSSL 1.1.0 at this time. The fix will be included in OpenSSL 1.1.0h when it becomes available. The fix is also available in commit e502cc86d in the OpenSSL git repository.", "other": {"cve": {"id": "CVE-2017-3738", "sourceIdentifier": "openssl-security@openssl.org", "published": "2017-12-07T16:29:00.240", "lastModified": "2022-08-19T11:49:42.737", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "There is an overflow bug in the AVX2 Montgomery multiplication procedure used in exponentiation with 1024-bit moduli. No EC algorithms are affected. Analysis suggests that attacks against RSA and DSA as a result of this defect would be very difficult to perform and are not believed likely. Attacks against DH1024 are considered just feasible, because most of the work necessary to deduce information about a private key may be performed offline. The amount of resources required for such an attack would be significant. However, for an attack on TLS to be meaningful, the server would have to share the DH1024 private key among multiple clients, which is no longer an option since CVE-2016-0701. This only affects processors that support the AVX2 but not ADX extensions like Intel Haswell (4th generation). Note: The impact from this issue is similar to CVE-2017-3736, CVE-2017-3732 and CVE-2015-3193. OpenSSL version 1.0.2-1.0.2m and 1.1.0-1.1.0g are affected. Fixed in OpenSSL 1.0.2n. Due to the low severity of this issue we are not issuing a new release of OpenSSL 1.1.0 at this time. The fix will be included in OpenSSL 1.1.0h when it becomes available. The fix is also available in commit e502cc86d in the OpenSSL git repository."}, {"lang": "es", "value": "Existe un error de desbordamiento en el procedimiento de multiplicaci\u00f3n AVX2 Montgomery empleado en la exponenciaci\u00f3n con m\u00f3dulos de 1024 bits. Los algoritmos EC no se han visto afectados. Los an\u00e1lisis sugieren que los ataques contra RSA y DSA como resultado de este defecto ser\u00edan muy dif\u00edciles de realizar y se cree que son improbables. Los ataques contra DH102 se consideran solo posibles, ya que la mayor parte del trabajo necesario para deducir informaci\u00f3n sobre una clave privada puede realizarse sin conexi\u00f3n. La cantidad de recursos necesarios para realizar tal ataque ser\u00eda significativa. Sin embargo, para que un ataque sobre TLS sea significativo, el servidor tendr\u00eda que compartir la clave privada DH1024 entre m\u00faltiples clientes, lo que ya no es una opci\u00f3n desde CVE-2016-0701. Esto solo afecta a procesadores compatibles con la extensi\u00f3n AVX2, pero no la ADX, como Intel Haswell (cuarta generaci\u00f3n). Nota: El impacto de este problema es similar a CVE-2017-3736, CVE-2017-3732 y CVE-2015-3193. Se han visto afectadas las versiones 1.0.2-1.0.2m y 1.1.0-1.1.0g de OpenSSL. Se ha solucionado en OpenSSL 1.0.2n. Debido a la baja gravedad de este problema, no se va a lanzar una nueva versi\u00f3n de OpenSSL 1.1.0 en este momento. La correcci\u00f3n se aplicar\u00e1 en OpenSSL 1.1.0h cuando est\u00e9 disponible. La correcci\u00f3n tambi\u00e9n estar\u00e1 disponible en el commit con ID e502cc86d en el repositorio Git de OpenSSL."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "AD3E5C1B-EC63-4214-A0BD-0B8681CE6C8B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2:beta1:*:*:*:*:*:*", "matchCriteriaId": "18797BEE-417D-4959-9AAD-C5A7C051B524"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2:beta2:*:*:*:*:*:*", "matchCriteriaId": "6FAA3C31-BD9D-45A9-A502-837FECA6D479"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2:beta3:*:*:*:*:*:*", "matchCriteriaId": "6455A421-9956-4846-AC7C-3431E0D37D23"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2a:*:*:*:*:*:*:*", "matchCriteriaId": "60F946FD-F564-49DA-B043-5943308BA9EE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2b:*:*:*:*:*:*:*", "matchCriteriaId": "4847BCF3-EFCE-41AF-8E7D-3D51EB9DCC5B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2c:*:*:*:*:*:*:*", "matchCriteriaId": "9B89180B-FB68-4DD8-B076-16E51CC7FB91"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2d:*:*:*:*:*:*:*", "matchCriteriaId": "4C986592-4086-4A39-9767-EF34DBAA6A53"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2e:*:*:*:*:*:*:*", "matchCriteriaId": "7B23181C-03DB-4E92-B3F6-6B585B5231B4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2f:*:*:*:*:*:*:*", "matchCriteriaId": "94D9EC1C-4843-4026-9B05-E060E9391734"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2g:*:*:*:*:*:*:*", "matchCriteriaId": "B066401C-21CF-4BE9-9C55-C9F1E0C7BE3F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2h:*:*:*:*:*:*:*", "matchCriteriaId": "036FB24F-7D86-4730-8BC9-722875BEC807"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2i:*:*:*:*:*:*:*", "matchCriteriaId": "FDF148A3-1AA7-4F27-85AB-414C609C626F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2j:*:*:*:*:*:*:*", "matchCriteriaId": "E15B749E-6808-4788-AE42-7A1587D8697E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2k:*:*:*:*:*:*:*", "matchCriteriaId": "58F80C8D-BCA2-40AD-BD22-B70C7BE1B298"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2l:*:*:*:*:*:*:*", "matchCriteriaId": "70B78EDF-6BB7-42C4-9423-9332C62C6E43"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.0.2m:*:*:*:*:*:*:*", "matchCriteriaId": "E2354F82-A01B-43D2-84F4-4E94B258E091"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.1.0:*:*:*:*:*:*:*", "matchCriteriaId": "73104834-5810-48DD-9B97-549D223853F1"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.1.0a:*:*:*:*:*:*:*", "matchCriteriaId": "C9D7A18A-116B-4F68-BEA3-A4E9DDDA55C6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.1.0b:*:*:*:*:*:*:*", "matchCriteriaId": "CFC70262-0DCD-4B46-9C96-FD18D0207511"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.1.0c:*:*:*:*:*:*:*", "matchCriteriaId": "B2E07A34-08A0-4765-AF81-46A3BDC5648A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.1.0d:*:*:*:*:*:*:*", "matchCriteriaId": "83B0A3D8-60C7-4F42-9DD6-C535F983D98B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.1.0e:*:*:*:*:*:*:*", "matchCriteriaId": "CD08E859-BB6D-4909-A873-C2609FA2821A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.1.0f:*:*:*:*:*:*:*", "matchCriteriaId": "C2BF7D67-EAF4-4D01-9185-0DB69F2C543B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openssl:openssl:1.1.0g:*:*:*:*:*:*:*", "matchCriteriaId": "179144A7-D263-4BD8-A019-35DE39C777FC"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:nodejs:node.js:*:*:*:*:-:*:*:*", "versionStartIncluding": "4.0.0", "versionEndIncluding": "4.1.2", "matchCriteriaId": "A47FC4F7-1F77-4314-B4B3-3C5D8E335379"}, {"vulnerable": true, "criteria": "cpe:2.3:a:nodejs:node.js:*:*:*:*:lts:*:*:*", "versionStartIncluding": "4.2.0", "versionEndExcluding": "4.8.7", "matchCriteriaId": "3818E441-8DC4-42E6-8D11-E58D195CBE8A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:nodejs:node.js:*:*:*:*:-:*:*:*", "versionStartIncluding": "6.0.0", "versionEndIncluding": "6.8.1", "matchCriteriaId": "D107EC29-67E7-40C3-8E5A-324C9105C5E4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:nodejs:node.js:*:*:*:*:lts:*:*:*", "versionStartIncluding": "6.9.0", "versionEndExcluding": "6.12.2", "matchCriteriaId": "BEA03114-7288-4E7C-9220-C0ABCD5F0389"}, {"vulnerable": true, "criteria": "cpe:2.3:a:nodejs:node.js:*:*:*:*:-:*:*:*", "versionStartIncluding": "8.0.0", "versionEndIncluding": "8.8.1", "matchCriteriaId": "74FB695D-2C76-47AB-988E-5629D2E695E5"}, {"vulnerable": true, "criteria": "cpe:2.3:a:nodejs:node.js:*:*:*:*:lts:*:*:*", "versionStartIncluding": "8.9.0", "versionEndExcluding": "8.9.3", "matchCriteriaId": "C45E9D50-CD3D-480B-B9B8-451ADFF26505"}, {"vulnerable": true, "criteria": "cpe:2.3:a:nodejs:node.js:*:*:*:*:-:*:*:*", "versionStartIncluding": "9.0.0", "versionEndExcluding": "9.2.1", "matchCriteriaId": "82FDBB10-3298-4C9A-9CC0-D34643AEC868"}]}]}], "references": [{"url": "http://www.oracle.com/technetwork/security-advisory/cpuapr2018-3678067.html", "source": "openssl-security@openssl.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.oracle.com/technetwork/security-advisory/cpujan2018-3236628.html", "source": "openssl-security@openssl.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.oracle.com/technetwork/security-advisory/cpujul2018-4258247.html", "source": "openssl-security@openssl.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.oracle.com/technetwork/security-advisory/cpuoct2018-4428296.html", "source": "openssl-security@openssl.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/102118", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1039978", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:0998", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2185", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2186", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2187", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/openssl/openssl/commit/e502cc86df9dafded1694fceb3228ee34d11c11a", "source": "openssl-security@openssl.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://nodejs.org/en/blog/vulnerability/december-2017-security-releases/", "source": "openssl-security@openssl.org", "tags": ["Vendor Advisory"]}, {"url": "https://security.FreeBSD.org/advisories/FreeBSD-SA-17:12.openssl.asc", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://security.gentoo.org/glsa/201712-03", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20171208-0001/", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://support.hpe.com/hpsc/doc/public/display?docLocale=en_US&docId=emr_na-hpesbst03881en_us", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2017/dsa-4065", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4157", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.openssl.org/news/secadv/20171207.txt", "source": "openssl-security@openssl.org", "tags": ["Vendor Advisory"]}, {"url": "https://www.openssl.org/news/secadv/20180327.txt", "source": "openssl-security@openssl.org", "tags": ["Vendor Advisory"]}, {"url": "https://www.oracle.com/technetwork/security-advisory/cpuapr2019-5072813.html", "source": "openssl-security@openssl.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.oracle.com/technetwork/security-advisory/cpujan2019-5072801.html", "source": "openssl-security@openssl.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.oracle.com/technetwork/security-advisory/cpujul2019-5072835.html", "source": "openssl-security@openssl.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.tenable.com/security/tns-2017-16", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.tenable.com/security/tns-2018-04", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.tenable.com/security/tns-2018-06", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.tenable.com/security/tns-2018-07", "source": "openssl-security@openssl.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/openssl/openssl/commit/e502cc86df9dafded1694fceb3228ee34d11c11a"}}