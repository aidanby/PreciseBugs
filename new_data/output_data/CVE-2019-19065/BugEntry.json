{"buggy_code": ["/*\n * Copyright(c) 2015 - 2018 Intel Corporation.\n *\n * This file is provided under a dual BSD/GPLv2 license.  When using or\n * redistributing this file, you may do so under either license.\n *\n * GPL LICENSE SUMMARY\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of version 2 of the GNU General Public License as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * BSD LICENSE\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *  - Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n *  - Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in\n *    the documentation and/or other materials provided with the\n *    distribution.\n *  - Neither the name of Intel Corporation nor the names of its\n *    contributors may be used to endorse or promote products derived\n *    from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n */\n\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/netdevice.h>\n#include <linux/moduleparam.h>\n#include <linux/bitops.h>\n#include <linux/timer.h>\n#include <linux/vmalloc.h>\n#include <linux/highmem.h>\n\n#include \"hfi.h\"\n#include \"common.h\"\n#include \"qp.h\"\n#include \"sdma.h\"\n#include \"iowait.h\"\n#include \"trace.h\"\n\n/* must be a power of 2 >= 64 <= 32768 */\n#define SDMA_DESCQ_CNT 2048\n#define SDMA_DESC_INTR 64\n#define INVALID_TAIL 0xffff\n\nstatic uint sdma_descq_cnt = SDMA_DESCQ_CNT;\nmodule_param(sdma_descq_cnt, uint, S_IRUGO);\nMODULE_PARM_DESC(sdma_descq_cnt, \"Number of SDMA descq entries\");\n\nstatic uint sdma_idle_cnt = 250;\nmodule_param(sdma_idle_cnt, uint, S_IRUGO);\nMODULE_PARM_DESC(sdma_idle_cnt, \"sdma interrupt idle delay (ns,default 250)\");\n\nuint mod_num_sdma;\nmodule_param_named(num_sdma, mod_num_sdma, uint, S_IRUGO);\nMODULE_PARM_DESC(num_sdma, \"Set max number SDMA engines to use\");\n\nstatic uint sdma_desct_intr = SDMA_DESC_INTR;\nmodule_param_named(desct_intr, sdma_desct_intr, uint, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(desct_intr, \"Number of SDMA descriptor before interrupt\");\n\n#define SDMA_WAIT_BATCH_SIZE 20\n/* max wait time for a SDMA engine to indicate it has halted */\n#define SDMA_ERR_HALT_TIMEOUT 10 /* ms */\n/* all SDMA engine errors that cause a halt */\n\n#define SD(name) SEND_DMA_##name\n#define ALL_SDMA_ENG_HALT_ERRS \\\n\t(SD(ENG_ERR_STATUS_SDMA_WRONG_DW_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_GEN_MISMATCH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TOO_LONG_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TAIL_OUT_OF_BOUNDS_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_FIRST_DESC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_MEM_READ_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HALT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_LENGTH_MISMATCH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_PACKET_DESC_OVERFLOW_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_SELECT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_ADDRESS_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_LENGTH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TIMEOUT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_DESC_TABLE_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_ASSEMBLY_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_PACKET_TRACKING_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_STORAGE_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_SMASK))\n\n/* sdma_sendctrl operations */\n#define SDMA_SENDCTRL_OP_ENABLE    BIT(0)\n#define SDMA_SENDCTRL_OP_INTENABLE BIT(1)\n#define SDMA_SENDCTRL_OP_HALT      BIT(2)\n#define SDMA_SENDCTRL_OP_CLEANUP   BIT(3)\n\n/* handle long defines */\n#define SDMA_EGRESS_PACKET_OCCUPANCY_SMASK \\\nSEND_EGRESS_SEND_DMA_STATUS_SDMA_EGRESS_PACKET_OCCUPANCY_SMASK\n#define SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT \\\nSEND_EGRESS_SEND_DMA_STATUS_SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT\n\nstatic const char * const sdma_state_names[] = {\n\t[sdma_state_s00_hw_down]                = \"s00_HwDown\",\n\t[sdma_state_s10_hw_start_up_halt_wait]  = \"s10_HwStartUpHaltWait\",\n\t[sdma_state_s15_hw_start_up_clean_wait] = \"s15_HwStartUpCleanWait\",\n\t[sdma_state_s20_idle]                   = \"s20_Idle\",\n\t[sdma_state_s30_sw_clean_up_wait]       = \"s30_SwCleanUpWait\",\n\t[sdma_state_s40_hw_clean_up_wait]       = \"s40_HwCleanUpWait\",\n\t[sdma_state_s50_hw_halt_wait]           = \"s50_HwHaltWait\",\n\t[sdma_state_s60_idle_halt_wait]         = \"s60_IdleHaltWait\",\n\t[sdma_state_s80_hw_freeze]\t\t= \"s80_HwFreeze\",\n\t[sdma_state_s82_freeze_sw_clean]\t= \"s82_FreezeSwClean\",\n\t[sdma_state_s99_running]                = \"s99_Running\",\n};\n\n#ifdef CONFIG_SDMA_VERBOSITY\nstatic const char * const sdma_event_names[] = {\n\t[sdma_event_e00_go_hw_down]   = \"e00_GoHwDown\",\n\t[sdma_event_e10_go_hw_start]  = \"e10_GoHwStart\",\n\t[sdma_event_e15_hw_halt_done] = \"e15_HwHaltDone\",\n\t[sdma_event_e25_hw_clean_up_done] = \"e25_HwCleanUpDone\",\n\t[sdma_event_e30_go_running]   = \"e30_GoRunning\",\n\t[sdma_event_e40_sw_cleaned]   = \"e40_SwCleaned\",\n\t[sdma_event_e50_hw_cleaned]   = \"e50_HwCleaned\",\n\t[sdma_event_e60_hw_halted]    = \"e60_HwHalted\",\n\t[sdma_event_e70_go_idle]      = \"e70_GoIdle\",\n\t[sdma_event_e80_hw_freeze]    = \"e80_HwFreeze\",\n\t[sdma_event_e81_hw_frozen]    = \"e81_HwFrozen\",\n\t[sdma_event_e82_hw_unfreeze]  = \"e82_HwUnfreeze\",\n\t[sdma_event_e85_link_down]    = \"e85_LinkDown\",\n\t[sdma_event_e90_sw_halted]    = \"e90_SwHalted\",\n};\n#endif\n\nstatic const struct sdma_set_state_action sdma_action_table[] = {\n\t[sdma_state_s00_hw_down] = {\n\t\t.go_s99_running_tofalse = 1,\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s10_hw_start_up_halt_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 1,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s15_hw_start_up_clean_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 1,\n\t},\n\t[sdma_state_s20_idle] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s30_sw_clean_up_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s40_hw_clean_up_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 1,\n\t},\n\t[sdma_state_s50_hw_halt_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s60_idle_halt_wait] = {\n\t\t.go_s99_running_tofalse = 1,\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 1,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s80_hw_freeze] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s82_freeze_sw_clean] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s99_running] = {\n\t\t.op_enable = 1,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t\t.go_s99_running_totrue = 1,\n\t},\n};\n\n#define SDMA_TAIL_UPDATE_THRESH 0x1F\n\n/* declare all statics here rather than keep sorting */\nstatic void sdma_complete(struct kref *);\nstatic void sdma_finalput(struct sdma_state *);\nstatic void sdma_get(struct sdma_state *);\nstatic void sdma_hw_clean_up_task(unsigned long);\nstatic void sdma_put(struct sdma_state *);\nstatic void sdma_set_state(struct sdma_engine *, enum sdma_states);\nstatic void sdma_start_hw_clean_up(struct sdma_engine *);\nstatic void sdma_sw_clean_up_task(unsigned long);\nstatic void sdma_sendctrl(struct sdma_engine *, unsigned);\nstatic void init_sdma_regs(struct sdma_engine *, u32, uint);\nstatic void sdma_process_event(\n\tstruct sdma_engine *sde,\n\tenum sdma_events event);\nstatic void __sdma_process_event(\n\tstruct sdma_engine *sde,\n\tenum sdma_events event);\nstatic void dump_sdma_state(struct sdma_engine *sde);\nstatic void sdma_make_progress(struct sdma_engine *sde, u64 status);\nstatic void sdma_desc_avail(struct sdma_engine *sde, uint avail);\nstatic void sdma_flush_descq(struct sdma_engine *sde);\n\n/**\n * sdma_state_name() - return state string from enum\n * @state: state\n */\nstatic const char *sdma_state_name(enum sdma_states state)\n{\n\treturn sdma_state_names[state];\n}\n\nstatic void sdma_get(struct sdma_state *ss)\n{\n\tkref_get(&ss->kref);\n}\n\nstatic void sdma_complete(struct kref *kref)\n{\n\tstruct sdma_state *ss =\n\t\tcontainer_of(kref, struct sdma_state, kref);\n\n\tcomplete(&ss->comp);\n}\n\nstatic void sdma_put(struct sdma_state *ss)\n{\n\tkref_put(&ss->kref, sdma_complete);\n}\n\nstatic void sdma_finalput(struct sdma_state *ss)\n{\n\tsdma_put(ss);\n\twait_for_completion(&ss->comp);\n}\n\nstatic inline void write_sde_csr(\n\tstruct sdma_engine *sde,\n\tu32 offset0,\n\tu64 value)\n{\n\twrite_kctxt_csr(sde->dd, sde->this_idx, offset0, value);\n}\n\nstatic inline u64 read_sde_csr(\n\tstruct sdma_engine *sde,\n\tu32 offset0)\n{\n\treturn read_kctxt_csr(sde->dd, sde->this_idx, offset0);\n}\n\n/*\n * sdma_wait_for_packet_egress() - wait for the VL FIFO occupancy for\n * sdma engine 'sde' to drop to 0.\n */\nstatic void sdma_wait_for_packet_egress(struct sdma_engine *sde,\n\t\t\t\t\tint pause)\n{\n\tu64 off = 8 * sde->this_idx;\n\tstruct hfi1_devdata *dd = sde->dd;\n\tint lcnt = 0;\n\tu64 reg_prev;\n\tu64 reg = 0;\n\n\twhile (1) {\n\t\treg_prev = reg;\n\t\treg = read_csr(dd, off + SEND_EGRESS_SEND_DMA_STATUS);\n\n\t\treg &= SDMA_EGRESS_PACKET_OCCUPANCY_SMASK;\n\t\treg >>= SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT;\n\t\tif (reg == 0)\n\t\t\tbreak;\n\t\t/* counter is reest if accupancy count changes */\n\t\tif (reg != reg_prev)\n\t\t\tlcnt = 0;\n\t\tif (lcnt++ > 500) {\n\t\t\t/* timed out - bounce the link */\n\t\t\tdd_dev_err(dd, \"%s: engine %u timeout waiting for packets to egress, remaining count %u, bouncing link\\n\",\n\t\t\t\t   __func__, sde->this_idx, (u32)reg);\n\t\t\tqueue_work(dd->pport->link_wq,\n\t\t\t\t   &dd->pport->link_bounce_work);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n}\n\n/*\n * sdma_wait() - wait for packet egress to complete for all SDMA engines,\n * and pause for credit return.\n */\nvoid sdma_wait(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\tfor (i = 0; i < dd->num_sdma; i++) {\n\t\tstruct sdma_engine *sde = &dd->per_sdma[i];\n\n\t\tsdma_wait_for_packet_egress(sde, 0);\n\t}\n}\n\nstatic inline void sdma_set_desc_cnt(struct sdma_engine *sde, unsigned cnt)\n{\n\tu64 reg;\n\n\tif (!(sde->dd->flags & HFI1_HAS_SDMA_TIMEOUT))\n\t\treturn;\n\treg = cnt;\n\treg &= SD(DESC_CNT_CNT_MASK);\n\treg <<= SD(DESC_CNT_CNT_SHIFT);\n\twrite_sde_csr(sde, SD(DESC_CNT), reg);\n}\n\nstatic inline void complete_tx(struct sdma_engine *sde,\n\t\t\t       struct sdma_txreq *tx,\n\t\t\t       int res)\n{\n\t/* protect against complete modifying */\n\tstruct iowait *wait = tx->wait;\n\tcallback_t complete = tx->complete;\n\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttrace_hfi1_sdma_out_sn(sde, tx->sn);\n\tif (WARN_ON_ONCE(sde->head_sn != tx->sn))\n\t\tdd_dev_err(sde->dd, \"expected %llu got %llu\\n\",\n\t\t\t   sde->head_sn, tx->sn);\n\tsde->head_sn++;\n#endif\n\t__sdma_txclean(sde->dd, tx);\n\tif (complete)\n\t\t(*complete)(tx, res);\n\tif (iowait_sdma_dec(wait))\n\t\tiowait_drain_wakeup(wait);\n}\n\n/*\n * Complete all the sdma requests with a SDMA_TXREQ_S_ABORTED status\n *\n * Depending on timing there can be txreqs in two places:\n * - in the descq ring\n * - in the flush list\n *\n * To avoid ordering issues the descq ring needs to be flushed\n * first followed by the flush list.\n *\n * This routine is called from two places\n * - From a work queue item\n * - Directly from the state machine just before setting the\n *   state to running\n *\n * Must be called with head_lock held\n *\n */\nstatic void sdma_flush(struct sdma_engine *sde)\n{\n\tstruct sdma_txreq *txp, *txp_next;\n\tLIST_HEAD(flushlist);\n\tunsigned long flags;\n\tuint seq;\n\n\t/* flush from head to tail */\n\tsdma_flush_descq(sde);\n\tspin_lock_irqsave(&sde->flushlist_lock, flags);\n\t/* copy flush list */\n\tlist_splice_init(&sde->flushlist, &flushlist);\n\tspin_unlock_irqrestore(&sde->flushlist_lock, flags);\n\t/* flush from flush list */\n\tlist_for_each_entry_safe(txp, txp_next, &flushlist, list)\n\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_ABORTED);\n\t/* wakeup QPs orphaned on the dmawait list */\n\tdo {\n\t\tstruct iowait *w, *nw;\n\n\t\tseq = read_seqbegin(&sde->waitlock);\n\t\tif (!list_empty(&sde->dmawait)) {\n\t\t\twrite_seqlock(&sde->waitlock);\n\t\t\tlist_for_each_entry_safe(w, nw, &sde->dmawait, list) {\n\t\t\t\tif (w->wakeup) {\n\t\t\t\t\tw->wakeup(w, SDMA_AVAIL_REASON);\n\t\t\t\t\tlist_del_init(&w->list);\n\t\t\t\t}\n\t\t\t}\n\t\t\twrite_sequnlock(&sde->waitlock);\n\t\t}\n\t} while (read_seqretry(&sde->waitlock, seq));\n}\n\n/*\n * Fields a work request for flushing the descq ring\n * and the flush list\n *\n * If the engine has been brought to running during\n * the scheduling delay, the flush is ignored, assuming\n * that the process of bringing the engine to running\n * would have done this flush prior to going to running.\n *\n */\nstatic void sdma_field_flush(struct work_struct *work)\n{\n\tunsigned long flags;\n\tstruct sdma_engine *sde =\n\t\tcontainer_of(work, struct sdma_engine, flush_worker);\n\n\twrite_seqlock_irqsave(&sde->head_lock, flags);\n\tif (!__sdma_running(sde))\n\t\tsdma_flush(sde);\n\twrite_sequnlock_irqrestore(&sde->head_lock, flags);\n}\n\nstatic void sdma_err_halt_wait(struct work_struct *work)\n{\n\tstruct sdma_engine *sde = container_of(work, struct sdma_engine,\n\t\t\t\t\t\terr_halt_worker);\n\tu64 statuscsr;\n\tunsigned long timeout;\n\n\ttimeout = jiffies + msecs_to_jiffies(SDMA_ERR_HALT_TIMEOUT);\n\twhile (1) {\n\t\tstatuscsr = read_sde_csr(sde, SD(STATUS));\n\t\tstatuscsr &= SD(STATUS_ENG_HALTED_SMASK);\n\t\tif (statuscsr)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(sde->dd,\n\t\t\t\t   \"SDMA engine %d - timeout waiting for engine to halt\\n\",\n\t\t\t\t   sde->this_idx);\n\t\t\t/*\n\t\t\t * Continue anyway.  This could happen if there was\n\t\t\t * an uncorrectable error in the wrong spot.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tusleep_range(80, 120);\n\t}\n\n\tsdma_process_event(sde, sdma_event_e15_hw_halt_done);\n}\n\nstatic void sdma_err_progress_check_schedule(struct sdma_engine *sde)\n{\n\tif (!is_bx(sde->dd) && HFI1_CAP_IS_KSET(SDMA_AHG)) {\n\t\tunsigned index;\n\t\tstruct hfi1_devdata *dd = sde->dd;\n\n\t\tfor (index = 0; index < dd->num_sdma; index++) {\n\t\t\tstruct sdma_engine *curr_sdma = &dd->per_sdma[index];\n\n\t\t\tif (curr_sdma != sde)\n\t\t\t\tcurr_sdma->progress_check_head =\n\t\t\t\t\t\t\tcurr_sdma->descq_head;\n\t\t}\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA engine %d - check scheduled\\n\",\n\t\t\t\tsde->this_idx);\n\t\tmod_timer(&sde->err_progress_check_timer, jiffies + 10);\n\t}\n}\n\nstatic void sdma_err_progress_check(struct timer_list *t)\n{\n\tunsigned index;\n\tstruct sdma_engine *sde = from_timer(sde, t, err_progress_check_timer);\n\n\tdd_dev_err(sde->dd, \"SDE progress check event\\n\");\n\tfor (index = 0; index < sde->dd->num_sdma; index++) {\n\t\tstruct sdma_engine *curr_sde = &sde->dd->per_sdma[index];\n\t\tunsigned long flags;\n\n\t\t/* check progress on each engine except the current one */\n\t\tif (curr_sde == sde)\n\t\t\tcontinue;\n\t\t/*\n\t\t * We must lock interrupts when acquiring sde->lock,\n\t\t * to avoid a deadlock if interrupt triggers and spins on\n\t\t * the same lock on same CPU\n\t\t */\n\t\tspin_lock_irqsave(&curr_sde->tail_lock, flags);\n\t\twrite_seqlock(&curr_sde->head_lock);\n\n\t\t/* skip non-running queues */\n\t\tif (curr_sde->state.current_state != sdma_state_s99_running) {\n\t\t\twrite_sequnlock(&curr_sde->head_lock);\n\t\t\tspin_unlock_irqrestore(&curr_sde->tail_lock, flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif ((curr_sde->descq_head != curr_sde->descq_tail) &&\n\t\t    (curr_sde->descq_head ==\n\t\t\t\tcurr_sde->progress_check_head))\n\t\t\t__sdma_process_event(curr_sde,\n\t\t\t\t\t     sdma_event_e90_sw_halted);\n\t\twrite_sequnlock(&curr_sde->head_lock);\n\t\tspin_unlock_irqrestore(&curr_sde->tail_lock, flags);\n\t}\n\tschedule_work(&sde->err_halt_worker);\n}\n\nstatic void sdma_hw_clean_up_task(unsigned long opaque)\n{\n\tstruct sdma_engine *sde = (struct sdma_engine *)opaque;\n\tu64 statuscsr;\n\n\twhile (1) {\n#ifdef CONFIG_SDMA_VERBOSITY\n\t\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__,\n\t\t\t__func__);\n#endif\n\t\tstatuscsr = read_sde_csr(sde, SD(STATUS));\n\t\tstatuscsr &= SD(STATUS_ENG_CLEANED_UP_SMASK);\n\t\tif (statuscsr)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\tsdma_process_event(sde, sdma_event_e25_hw_clean_up_done);\n}\n\nstatic inline struct sdma_txreq *get_txhead(struct sdma_engine *sde)\n{\n\treturn sde->tx_ring[sde->tx_head & sde->sdma_mask];\n}\n\n/*\n * flush ring for recovery\n */\nstatic void sdma_flush_descq(struct sdma_engine *sde)\n{\n\tu16 head, tail;\n\tint progress = 0;\n\tstruct sdma_txreq *txp = get_txhead(sde);\n\n\t/* The reason for some of the complexity of this code is that\n\t * not all descriptors have corresponding txps.  So, we have to\n\t * be able to skip over descs until we wander into the range of\n\t * the next txp on the list.\n\t */\n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = sde->descq_tail & sde->sdma_mask;\n\twhile (head != tail) {\n\t\t/* advance head, wrap if needed */\n\t\thead = ++sde->descq_head & sde->sdma_mask;\n\t\t/* if now past this txp's descs, do the callback */\n\t\tif (txp && txp->next_descq_idx == head) {\n\t\t\t/* remove from list */\n\t\t\tsde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;\n\t\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_ABORTED);\n\t\t\ttrace_hfi1_sdma_progress(sde, head, tail, txp);\n\t\t\ttxp = get_txhead(sde);\n\t\t}\n\t\tprogress++;\n\t}\n\tif (progress)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n}\n\nstatic void sdma_sw_clean_up_task(unsigned long opaque)\n{\n\tstruct sdma_engine *sde = (struct sdma_engine *)opaque;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\n\t/*\n\t * At this point, the following should always be true:\n\t * - We are halted, so no more descriptors are getting retired.\n\t * - We are not running, so no one is submitting new work.\n\t * - Only we can send the e40_sw_cleaned, so we can't start\n\t *   running again until we say so.  So, the active list and\n\t *   descq are ours to play with.\n\t */\n\n\t/*\n\t * In the error clean up sequence, software clean must be called\n\t * before the hardware clean so we can use the hardware head in\n\t * the progress routine.  A hardware clean or SPC unfreeze will\n\t * reset the hardware head.\n\t *\n\t * Process all retired requests. The progress routine will use the\n\t * latest physical hardware head - we are not running so speed does\n\t * not matter.\n\t */\n\tsdma_make_progress(sde, 0);\n\n\tsdma_flush(sde);\n\n\t/*\n\t * Reset our notion of head and tail.\n\t * Note that the HW registers have been reset via an earlier\n\t * clean up.\n\t */\n\tsde->descq_tail = 0;\n\tsde->descq_head = 0;\n\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t*sde->head_dma = 0;\n\n\t__sdma_process_event(sde, sdma_event_e40_sw_cleaned);\n\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void sdma_sw_tear_down(struct sdma_engine *sde)\n{\n\tstruct sdma_state *ss = &sde->state;\n\n\t/* Releasing this reference means the state machine has stopped. */\n\tsdma_put(ss);\n\n\t/* stop waiting for all unfreeze events to complete */\n\tatomic_set(&sde->dd->sdma_unfreeze_count, -1);\n\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n}\n\nstatic void sdma_start_hw_clean_up(struct sdma_engine *sde)\n{\n\ttasklet_hi_schedule(&sde->sdma_hw_clean_up_task);\n}\n\nstatic void sdma_set_state(struct sdma_engine *sde,\n\t\t\t   enum sdma_states next_state)\n{\n\tstruct sdma_state *ss = &sde->state;\n\tconst struct sdma_set_state_action *action = sdma_action_table;\n\tunsigned op = 0;\n\n\ttrace_hfi1_sdma_state(\n\t\tsde,\n\t\tsdma_state_names[ss->current_state],\n\t\tsdma_state_names[next_state]);\n\n\t/* debugging bookkeeping */\n\tss->previous_state = ss->current_state;\n\tss->previous_op = ss->current_op;\n\tss->current_state = next_state;\n\n\tif (ss->previous_state != sdma_state_s99_running &&\n\t    next_state == sdma_state_s99_running)\n\t\tsdma_flush(sde);\n\n\tif (action[next_state].op_enable)\n\t\top |= SDMA_SENDCTRL_OP_ENABLE;\n\n\tif (action[next_state].op_intenable)\n\t\top |= SDMA_SENDCTRL_OP_INTENABLE;\n\n\tif (action[next_state].op_halt)\n\t\top |= SDMA_SENDCTRL_OP_HALT;\n\n\tif (action[next_state].op_cleanup)\n\t\top |= SDMA_SENDCTRL_OP_CLEANUP;\n\n\tif (action[next_state].go_s99_running_tofalse)\n\t\tss->go_s99_running = 0;\n\n\tif (action[next_state].go_s99_running_totrue)\n\t\tss->go_s99_running = 1;\n\n\tss->current_op = op;\n\tsdma_sendctrl(sde, ss->current_op);\n}\n\n/**\n * sdma_get_descq_cnt() - called when device probed\n *\n * Return a validated descq count.\n *\n * This is currently only used in the verbs initialization to build the tx\n * list.\n *\n * This will probably be deleted in favor of a more scalable approach to\n * alloc tx's.\n *\n */\nu16 sdma_get_descq_cnt(void)\n{\n\tu16 count = sdma_descq_cnt;\n\n\tif (!count)\n\t\treturn SDMA_DESCQ_CNT;\n\t/* count must be a power of 2 greater than 64 and less than\n\t * 32768.   Otherwise return default.\n\t */\n\tif (!is_power_of_2(count))\n\t\treturn SDMA_DESCQ_CNT;\n\tif (count < 64 || count > 32768)\n\t\treturn SDMA_DESCQ_CNT;\n\treturn count;\n}\n\n/**\n * sdma_engine_get_vl() - return vl for a given sdma engine\n * @sde: sdma engine\n *\n * This function returns the vl mapped to a given engine, or an error if\n * the mapping can't be found. The mapping fields are protected by RCU.\n */\nint sdma_engine_get_vl(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tstruct sdma_vl_map *m;\n\tu8 vl;\n\n\tif (sde->this_idx >= TXE_NUM_SDMA_ENGINES)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tm = rcu_dereference(dd->sdma_map);\n\tif (unlikely(!m)) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\tvl = m->engine_to_vl[sde->this_idx];\n\trcu_read_unlock();\n\n\treturn vl;\n}\n\n/**\n * sdma_select_engine_vl() - select sdma engine\n * @dd: devdata\n * @selector: a spreading factor\n * @vl: this vl\n *\n *\n * This function returns an engine based on the selector and a vl.  The\n * mapping fields are protected by RCU.\n */\nstruct sdma_engine *sdma_select_engine_vl(\n\tstruct hfi1_devdata *dd,\n\tu32 selector,\n\tu8 vl)\n{\n\tstruct sdma_vl_map *m;\n\tstruct sdma_map_elem *e;\n\tstruct sdma_engine *rval;\n\n\t/* NOTE This should only happen if SC->VL changed after the initial\n\t *      checks on the QP/AH\n\t *      Default will return engine 0 below\n\t */\n\tif (vl >= num_vls) {\n\t\trval = NULL;\n\t\tgoto done;\n\t}\n\n\trcu_read_lock();\n\tm = rcu_dereference(dd->sdma_map);\n\tif (unlikely(!m)) {\n\t\trcu_read_unlock();\n\t\treturn &dd->per_sdma[0];\n\t}\n\te = m->map[vl & m->mask];\n\trval = e->sde[selector & e->mask];\n\trcu_read_unlock();\n\ndone:\n\trval =  !rval ? &dd->per_sdma[0] : rval;\n\ttrace_hfi1_sdma_engine_select(dd, selector, vl, rval->this_idx);\n\treturn rval;\n}\n\n/**\n * sdma_select_engine_sc() - select sdma engine\n * @dd: devdata\n * @selector: a spreading factor\n * @sc5: the 5 bit sc\n *\n *\n * This function returns an engine based on the selector and an sc.\n */\nstruct sdma_engine *sdma_select_engine_sc(\n\tstruct hfi1_devdata *dd,\n\tu32 selector,\n\tu8 sc5)\n{\n\tu8 vl = sc_to_vlt(dd, sc5);\n\n\treturn sdma_select_engine_vl(dd, selector, vl);\n}\n\nstruct sdma_rht_map_elem {\n\tu32 mask;\n\tu8 ctr;\n\tstruct sdma_engine *sde[0];\n};\n\nstruct sdma_rht_node {\n\tunsigned long cpu_id;\n\tstruct sdma_rht_map_elem *map[HFI1_MAX_VLS_SUPPORTED];\n\tstruct rhash_head node;\n};\n\n#define NR_CPUS_HINT 192\n\nstatic const struct rhashtable_params sdma_rht_params = {\n\t.nelem_hint = NR_CPUS_HINT,\n\t.head_offset = offsetof(struct sdma_rht_node, node),\n\t.key_offset = offsetof(struct sdma_rht_node, cpu_id),\n\t.key_len = FIELD_SIZEOF(struct sdma_rht_node, cpu_id),\n\t.max_size = NR_CPUS,\n\t.min_size = 8,\n\t.automatic_shrinking = true,\n};\n\n/*\n * sdma_select_user_engine() - select sdma engine based on user setup\n * @dd: devdata\n * @selector: a spreading factor\n * @vl: this vl\n *\n * This function returns an sdma engine for a user sdma request.\n * User defined sdma engine affinity setting is honored when applicable,\n * otherwise system default sdma engine mapping is used. To ensure correct\n * ordering, the mapping from <selector, vl> to sde must remain unchanged.\n */\nstruct sdma_engine *sdma_select_user_engine(struct hfi1_devdata *dd,\n\t\t\t\t\t    u32 selector, u8 vl)\n{\n\tstruct sdma_rht_node *rht_node;\n\tstruct sdma_engine *sde = NULL;\n\tunsigned long cpu_id;\n\n\t/*\n\t * To ensure that always the same sdma engine(s) will be\n\t * selected make sure the process is pinned to this CPU only.\n\t */\n\tif (current->nr_cpus_allowed != 1)\n\t\tgoto out;\n\n\tcpu_id = smp_processor_id();\n\trcu_read_lock();\n\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpu_id,\n\t\t\t\t\t  sdma_rht_params);\n\n\tif (rht_node && rht_node->map[vl]) {\n\t\tstruct sdma_rht_map_elem *map = rht_node->map[vl];\n\n\t\tsde = map->sde[selector & map->mask];\n\t}\n\trcu_read_unlock();\n\n\tif (sde)\n\t\treturn sde;\n\nout:\n\treturn sdma_select_engine_vl(dd, selector, vl);\n}\n\nstatic void sdma_populate_sde_map(struct sdma_rht_map_elem *map)\n{\n\tint i;\n\n\tfor (i = 0; i < roundup_pow_of_two(map->ctr ? : 1) - map->ctr; i++)\n\t\tmap->sde[map->ctr + i] = map->sde[i];\n}\n\nstatic void sdma_cleanup_sde_map(struct sdma_rht_map_elem *map,\n\t\t\t\t struct sdma_engine *sde)\n{\n\tunsigned int i, pow;\n\n\t/* only need to check the first ctr entries for a match */\n\tfor (i = 0; i < map->ctr; i++) {\n\t\tif (map->sde[i] == sde) {\n\t\t\tmemmove(&map->sde[i], &map->sde[i + 1],\n\t\t\t\t(map->ctr - i - 1) * sizeof(map->sde[0]));\n\t\t\tmap->ctr--;\n\t\t\tpow = roundup_pow_of_two(map->ctr ? : 1);\n\t\t\tmap->mask = pow - 1;\n\t\t\tsdma_populate_sde_map(map);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * Prevents concurrent reads and writes of the sdma engine cpu_mask\n */\nstatic DEFINE_MUTEX(process_to_sde_mutex);\n\nssize_t sdma_set_cpu_to_sde_map(struct sdma_engine *sde, const char *buf,\n\t\t\t\tsize_t count)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tcpumask_var_t mask, new_mask;\n\tunsigned long cpu;\n\tint ret, vl, sz;\n\tstruct sdma_rht_node *rht_node;\n\n\tvl = sdma_engine_get_vl(sde);\n\tif (unlikely(vl < 0 || vl >= ARRAY_SIZE(rht_node->map)))\n\t\treturn -EINVAL;\n\n\tret = zalloc_cpumask_var(&mask, GFP_KERNEL);\n\tif (!ret)\n\t\treturn -ENOMEM;\n\n\tret = zalloc_cpumask_var(&new_mask, GFP_KERNEL);\n\tif (!ret) {\n\t\tfree_cpumask_var(mask);\n\t\treturn -ENOMEM;\n\t}\n\tret = cpulist_parse(buf, mask);\n\tif (ret)\n\t\tgoto out_free;\n\n\tif (!cpumask_subset(mask, cpu_online_mask)) {\n\t\tdd_dev_warn(sde->dd, \"Invalid CPU mask\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tsz = sizeof(struct sdma_rht_map_elem) +\n\t\t\t(TXE_NUM_SDMA_ENGINES * sizeof(struct sdma_engine *));\n\n\tmutex_lock(&process_to_sde_mutex);\n\n\tfor_each_cpu(cpu, mask) {\n\t\t/* Check if we have this already mapped */\n\t\tif (cpumask_test_cpu(cpu, &sde->cpu_mask)) {\n\t\t\tcpumask_set_cpu(cpu, new_mask);\n\t\t\tcontinue;\n\t\t}\n\n\t\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpu,\n\t\t\t\t\t\t  sdma_rht_params);\n\t\tif (!rht_node) {\n\t\t\trht_node = kzalloc(sizeof(*rht_node), GFP_KERNEL);\n\t\t\tif (!rht_node) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\trht_node->map[vl] = kzalloc(sz, GFP_KERNEL);\n\t\t\tif (!rht_node->map[vl]) {\n\t\t\t\tkfree(rht_node);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\trht_node->cpu_id = cpu;\n\t\t\trht_node->map[vl]->mask = 0;\n\t\t\trht_node->map[vl]->ctr = 1;\n\t\t\trht_node->map[vl]->sde[0] = sde;\n\n\t\t\tret = rhashtable_insert_fast(dd->sdma_rht,\n\t\t\t\t\t\t     &rht_node->node,\n\t\t\t\t\t\t     sdma_rht_params);\n\t\t\tif (ret) {\n\t\t\t\tkfree(rht_node->map[vl]);\n\t\t\t\tkfree(rht_node);\n\t\t\t\tdd_dev_err(sde->dd, \"Failed to set process to sde affinity for cpu %lu\\n\",\n\t\t\t\t\t   cpu);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t} else {\n\t\t\tint ctr, pow;\n\n\t\t\t/* Add new user mappings */\n\t\t\tif (!rht_node->map[vl])\n\t\t\t\trht_node->map[vl] = kzalloc(sz, GFP_KERNEL);\n\n\t\t\tif (!rht_node->map[vl]) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\trht_node->map[vl]->ctr++;\n\t\t\tctr = rht_node->map[vl]->ctr;\n\t\t\trht_node->map[vl]->sde[ctr - 1] = sde;\n\t\t\tpow = roundup_pow_of_two(ctr);\n\t\t\trht_node->map[vl]->mask = pow - 1;\n\n\t\t\t/* Populate the sde map table */\n\t\t\tsdma_populate_sde_map(rht_node->map[vl]);\n\t\t}\n\t\tcpumask_set_cpu(cpu, new_mask);\n\t}\n\n\t/* Clean up old mappings */\n\tfor_each_cpu(cpu, cpu_online_mask) {\n\t\tstruct sdma_rht_node *rht_node;\n\n\t\t/* Don't cleanup sdes that are set in the new mask */\n\t\tif (cpumask_test_cpu(cpu, mask))\n\t\t\tcontinue;\n\n\t\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpu,\n\t\t\t\t\t\t  sdma_rht_params);\n\t\tif (rht_node) {\n\t\t\tbool empty = true;\n\t\t\tint i;\n\n\t\t\t/* Remove mappings for old sde */\n\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\t\t\tif (rht_node->map[i])\n\t\t\t\t\tsdma_cleanup_sde_map(rht_node->map[i],\n\t\t\t\t\t\t\t     sde);\n\n\t\t\t/* Free empty hash table entries */\n\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++) {\n\t\t\t\tif (!rht_node->map[i])\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (rht_node->map[i]->ctr) {\n\t\t\t\t\tempty = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (empty) {\n\t\t\t\tret = rhashtable_remove_fast(dd->sdma_rht,\n\t\t\t\t\t\t\t     &rht_node->node,\n\t\t\t\t\t\t\t     sdma_rht_params);\n\t\t\t\tWARN_ON(ret);\n\n\t\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\t\t\t\tkfree(rht_node->map[i]);\n\n\t\t\t\tkfree(rht_node);\n\t\t\t}\n\t\t}\n\t}\n\n\tcpumask_copy(&sde->cpu_mask, new_mask);\nout:\n\tmutex_unlock(&process_to_sde_mutex);\nout_free:\n\tfree_cpumask_var(mask);\n\tfree_cpumask_var(new_mask);\n\treturn ret ? : strnlen(buf, PAGE_SIZE);\n}\n\nssize_t sdma_get_cpu_to_sde_map(struct sdma_engine *sde, char *buf)\n{\n\tmutex_lock(&process_to_sde_mutex);\n\tif (cpumask_empty(&sde->cpu_mask))\n\t\tsnprintf(buf, PAGE_SIZE, \"%s\\n\", \"empty\");\n\telse\n\t\tcpumap_print_to_pagebuf(true, buf, &sde->cpu_mask);\n\tmutex_unlock(&process_to_sde_mutex);\n\treturn strnlen(buf, PAGE_SIZE);\n}\n\nstatic void sdma_rht_free(void *ptr, void *arg)\n{\n\tstruct sdma_rht_node *rht_node = ptr;\n\tint i;\n\n\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\tkfree(rht_node->map[i]);\n\n\tkfree(rht_node);\n}\n\n/**\n * sdma_seqfile_dump_cpu_list() - debugfs dump the cpu to sdma mappings\n * @s: seq file\n * @dd: hfi1_devdata\n * @cpuid: cpu id\n *\n * This routine dumps the process to sde mappings per cpu\n */\nvoid sdma_seqfile_dump_cpu_list(struct seq_file *s,\n\t\t\t\tstruct hfi1_devdata *dd,\n\t\t\t\tunsigned long cpuid)\n{\n\tstruct sdma_rht_node *rht_node;\n\tint i, j;\n\n\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpuid,\n\t\t\t\t\t  sdma_rht_params);\n\tif (!rht_node)\n\t\treturn;\n\n\tseq_printf(s, \"cpu%3lu: \", cpuid);\n\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++) {\n\t\tif (!rht_node->map[i] || !rht_node->map[i]->ctr)\n\t\t\tcontinue;\n\n\t\tseq_printf(s, \" vl%d: [\", i);\n\n\t\tfor (j = 0; j < rht_node->map[i]->ctr; j++) {\n\t\t\tif (!rht_node->map[i]->sde[j])\n\t\t\t\tcontinue;\n\n\t\t\tif (j > 0)\n\t\t\t\tseq_puts(s, \",\");\n\n\t\t\tseq_printf(s, \" sdma%2d\",\n\t\t\t\t   rht_node->map[i]->sde[j]->this_idx);\n\t\t}\n\t\tseq_puts(s, \" ]\");\n\t}\n\n\tseq_puts(s, \"\\n\");\n}\n\n/*\n * Free the indicated map struct\n */\nstatic void sdma_map_free(struct sdma_vl_map *m)\n{\n\tint i;\n\n\tfor (i = 0; m && i < m->actual_vls; i++)\n\t\tkfree(m->map[i]);\n\tkfree(m);\n}\n\n/*\n * Handle RCU callback\n */\nstatic void sdma_map_rcu_callback(struct rcu_head *list)\n{\n\tstruct sdma_vl_map *m = container_of(list, struct sdma_vl_map, list);\n\n\tsdma_map_free(m);\n}\n\n/**\n * sdma_map_init - called when # vls change\n * @dd: hfi1_devdata\n * @port: port number\n * @num_vls: number of vls\n * @vl_engines: per vl engine mapping (optional)\n *\n * This routine changes the mapping based on the number of vls.\n *\n * vl_engines is used to specify a non-uniform vl/engine loading. NULL\n * implies auto computing the loading and giving each VLs a uniform\n * distribution of engines per VL.\n *\n * The auto algorithm computes the sde_per_vl and the number of extra\n * engines.  Any extra engines are added from the last VL on down.\n *\n * rcu locking is used here to control access to the mapping fields.\n *\n * If either the num_vls or num_sdma are non-power of 2, the array sizes\n * in the struct sdma_vl_map and the struct sdma_map_elem are rounded\n * up to the next highest power of 2 and the first entry is reused\n * in a round robin fashion.\n *\n * If an error occurs the map change is not done and the mapping is\n * not changed.\n *\n */\nint sdma_map_init(struct hfi1_devdata *dd, u8 port, u8 num_vls, u8 *vl_engines)\n{\n\tint i, j;\n\tint extra, sde_per_vl;\n\tint engine = 0;\n\tu8 lvl_engines[OPA_MAX_VLS];\n\tstruct sdma_vl_map *oldmap, *newmap;\n\n\tif (!(dd->flags & HFI1_HAS_SEND_DMA))\n\t\treturn 0;\n\n\tif (!vl_engines) {\n\t\t/* truncate divide */\n\t\tsde_per_vl = dd->num_sdma / num_vls;\n\t\t/* extras */\n\t\textra = dd->num_sdma % num_vls;\n\t\tvl_engines = lvl_engines;\n\t\t/* add extras from last vl down */\n\t\tfor (i = num_vls - 1; i >= 0; i--, extra--)\n\t\t\tvl_engines[i] = sde_per_vl + (extra > 0 ? 1 : 0);\n\t}\n\t/* build new map */\n\tnewmap = kzalloc(\n\t\tsizeof(struct sdma_vl_map) +\n\t\t\troundup_pow_of_two(num_vls) *\n\t\t\tsizeof(struct sdma_map_elem *),\n\t\tGFP_KERNEL);\n\tif (!newmap)\n\t\tgoto bail;\n\tnewmap->actual_vls = num_vls;\n\tnewmap->vls = roundup_pow_of_two(num_vls);\n\tnewmap->mask = (1 << ilog2(newmap->vls)) - 1;\n\t/* initialize back-map */\n\tfor (i = 0; i < TXE_NUM_SDMA_ENGINES; i++)\n\t\tnewmap->engine_to_vl[i] = -1;\n\tfor (i = 0; i < newmap->vls; i++) {\n\t\t/* save for wrap around */\n\t\tint first_engine = engine;\n\n\t\tif (i < newmap->actual_vls) {\n\t\t\tint sz = roundup_pow_of_two(vl_engines[i]);\n\n\t\t\t/* only allocate once */\n\t\t\tnewmap->map[i] = kzalloc(\n\t\t\t\tsizeof(struct sdma_map_elem) +\n\t\t\t\t\tsz * sizeof(struct sdma_engine *),\n\t\t\t\tGFP_KERNEL);\n\t\t\tif (!newmap->map[i])\n\t\t\t\tgoto bail;\n\t\t\tnewmap->map[i]->mask = (1 << ilog2(sz)) - 1;\n\t\t\t/* assign engines */\n\t\t\tfor (j = 0; j < sz; j++) {\n\t\t\t\tnewmap->map[i]->sde[j] =\n\t\t\t\t\t&dd->per_sdma[engine];\n\t\t\t\tif (++engine >= first_engine + vl_engines[i])\n\t\t\t\t\t/* wrap back to first engine */\n\t\t\t\t\tengine = first_engine;\n\t\t\t}\n\t\t\t/* assign back-map */\n\t\t\tfor (j = 0; j < vl_engines[i]; j++)\n\t\t\t\tnewmap->engine_to_vl[first_engine + j] = i;\n\t\t} else {\n\t\t\t/* just re-use entry without allocating */\n\t\t\tnewmap->map[i] = newmap->map[i % num_vls];\n\t\t}\n\t\tengine = first_engine + vl_engines[i];\n\t}\n\t/* newmap in hand, save old map */\n\tspin_lock_irq(&dd->sde_map_lock);\n\toldmap = rcu_dereference_protected(dd->sdma_map,\n\t\t\t\t\t   lockdep_is_held(&dd->sde_map_lock));\n\n\t/* publish newmap */\n\trcu_assign_pointer(dd->sdma_map, newmap);\n\n\tspin_unlock_irq(&dd->sde_map_lock);\n\t/* success, free any old map after grace period */\n\tif (oldmap)\n\t\tcall_rcu(&oldmap->list, sdma_map_rcu_callback);\n\treturn 0;\nbail:\n\t/* free any partial allocation */\n\tsdma_map_free(newmap);\n\treturn -ENOMEM;\n}\n\n/**\n * sdma_clean()  Clean up allocated memory\n * @dd:          struct hfi1_devdata\n * @num_engines: num sdma engines\n *\n * This routine can be called regardless of the success of\n * sdma_init()\n */\nvoid sdma_clean(struct hfi1_devdata *dd, size_t num_engines)\n{\n\tsize_t i;\n\tstruct sdma_engine *sde;\n\n\tif (dd->sdma_pad_dma) {\n\t\tdma_free_coherent(&dd->pcidev->dev, 4,\n\t\t\t\t  (void *)dd->sdma_pad_dma,\n\t\t\t\t  dd->sdma_pad_phys);\n\t\tdd->sdma_pad_dma = NULL;\n\t\tdd->sdma_pad_phys = 0;\n\t}\n\tif (dd->sdma_heads_dma) {\n\t\tdma_free_coherent(&dd->pcidev->dev, dd->sdma_heads_size,\n\t\t\t\t  (void *)dd->sdma_heads_dma,\n\t\t\t\t  dd->sdma_heads_phys);\n\t\tdd->sdma_heads_dma = NULL;\n\t\tdd->sdma_heads_phys = 0;\n\t}\n\tfor (i = 0; dd->per_sdma && i < num_engines; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\n\t\tsde->head_dma = NULL;\n\t\tsde->head_phys = 0;\n\n\t\tif (sde->descq) {\n\t\t\tdma_free_coherent(\n\t\t\t\t&dd->pcidev->dev,\n\t\t\t\tsde->descq_cnt * sizeof(u64[2]),\n\t\t\t\tsde->descq,\n\t\t\t\tsde->descq_phys\n\t\t\t);\n\t\t\tsde->descq = NULL;\n\t\t\tsde->descq_phys = 0;\n\t\t}\n\t\tkvfree(sde->tx_ring);\n\t\tsde->tx_ring = NULL;\n\t}\n\tspin_lock_irq(&dd->sde_map_lock);\n\tsdma_map_free(rcu_access_pointer(dd->sdma_map));\n\tRCU_INIT_POINTER(dd->sdma_map, NULL);\n\tspin_unlock_irq(&dd->sde_map_lock);\n\tsynchronize_rcu();\n\tkfree(dd->per_sdma);\n\tdd->per_sdma = NULL;\n\n\tif (dd->sdma_rht) {\n\t\trhashtable_free_and_destroy(dd->sdma_rht, sdma_rht_free, NULL);\n\t\tkfree(dd->sdma_rht);\n\t\tdd->sdma_rht = NULL;\n\t}\n}\n\n/**\n * sdma_init() - called when device probed\n * @dd: hfi1_devdata\n * @port: port number (currently only zero)\n *\n * Initializes each sde and its csrs.\n * Interrupts are not required to be enabled.\n *\n * Returns:\n * 0 - success, -errno on failure\n */\nint sdma_init(struct hfi1_devdata *dd, u8 port)\n{\n\tunsigned this_idx;\n\tstruct sdma_engine *sde;\n\tstruct rhashtable *tmp_sdma_rht;\n\tu16 descq_cnt;\n\tvoid *curr_head;\n\tstruct hfi1_pportdata *ppd = dd->pport + port;\n\tu32 per_sdma_credits;\n\tuint idle_cnt = sdma_idle_cnt;\n\tsize_t num_engines = chip_sdma_engines(dd);\n\tint ret = -ENOMEM;\n\n\tif (!HFI1_CAP_IS_KSET(SDMA)) {\n\t\tHFI1_CAP_CLEAR(SDMA_AHG);\n\t\treturn 0;\n\t}\n\tif (mod_num_sdma &&\n\t    /* can't exceed chip support */\n\t    mod_num_sdma <= chip_sdma_engines(dd) &&\n\t    /* count must be >= vls */\n\t    mod_num_sdma >= num_vls)\n\t\tnum_engines = mod_num_sdma;\n\n\tdd_dev_info(dd, \"SDMA mod_num_sdma: %u\\n\", mod_num_sdma);\n\tdd_dev_info(dd, \"SDMA chip_sdma_engines: %u\\n\", chip_sdma_engines(dd));\n\tdd_dev_info(dd, \"SDMA chip_sdma_mem_size: %u\\n\",\n\t\t    chip_sdma_mem_size(dd));\n\n\tper_sdma_credits =\n\t\tchip_sdma_mem_size(dd) / (num_engines * SDMA_BLOCK_SIZE);\n\n\t/* set up freeze waitqueue */\n\tinit_waitqueue_head(&dd->sdma_unfreeze_wq);\n\tatomic_set(&dd->sdma_unfreeze_count, 0);\n\n\tdescq_cnt = sdma_get_descq_cnt();\n\tdd_dev_info(dd, \"SDMA engines %zu descq_cnt %u\\n\",\n\t\t    num_engines, descq_cnt);\n\n\t/* alloc memory for array of send engines */\n\tdd->per_sdma = kcalloc_node(num_engines, sizeof(*dd->per_sdma),\n\t\t\t\t    GFP_KERNEL, dd->node);\n\tif (!dd->per_sdma)\n\t\treturn ret;\n\n\tidle_cnt = ns_to_cclock(dd, idle_cnt);\n\tif (idle_cnt)\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_HEAD_TO_HOST_FLAG;\n\telse\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_INT_REQ_FLAG;\n\n\tif (!sdma_desct_intr)\n\t\tsdma_desct_intr = SDMA_DESC_INTR;\n\n\t/* Allocate memory for SendDMA descriptor FIFOs */\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tsde = &dd->per_sdma[this_idx];\n\t\tsde->dd = dd;\n\t\tsde->ppd = ppd;\n\t\tsde->this_idx = this_idx;\n\t\tsde->descq_cnt = descq_cnt;\n\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t\tsde->sdma_shift = ilog2(descq_cnt);\n\t\tsde->sdma_mask = (1 << sde->sdma_shift) - 1;\n\n\t\t/* Create a mask specifically for each interrupt source */\n\t\tsde->int_mask = (u64)1 << (0 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t   this_idx);\n\t\tsde->progress_mask = (u64)1 << (1 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t\tthis_idx);\n\t\tsde->idle_mask = (u64)1 << (2 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t    this_idx);\n\t\t/* Create a combined mask to cover all 3 interrupt sources */\n\t\tsde->imask = sde->int_mask | sde->progress_mask |\n\t\t\t     sde->idle_mask;\n\n\t\tspin_lock_init(&sde->tail_lock);\n\t\tseqlock_init(&sde->head_lock);\n\t\tspin_lock_init(&sde->senddmactrl_lock);\n\t\tspin_lock_init(&sde->flushlist_lock);\n\t\tseqlock_init(&sde->waitlock);\n\t\t/* insure there is always a zero bit */\n\t\tsde->ahg_bits = 0xfffffffe00000000ULL;\n\n\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\n\t\t/* set up reference counting */\n\t\tkref_init(&sde->state.kref);\n\t\tinit_completion(&sde->state.comp);\n\n\t\tINIT_LIST_HEAD(&sde->flushlist);\n\t\tINIT_LIST_HEAD(&sde->dmawait);\n\n\t\tsde->tail_csr =\n\t\t\tget_kctxt_csr_addr(dd, this_idx, SD(TAIL));\n\n\t\ttasklet_init(&sde->sdma_hw_clean_up_task, sdma_hw_clean_up_task,\n\t\t\t     (unsigned long)sde);\n\n\t\ttasklet_init(&sde->sdma_sw_clean_up_task, sdma_sw_clean_up_task,\n\t\t\t     (unsigned long)sde);\n\t\tINIT_WORK(&sde->err_halt_worker, sdma_err_halt_wait);\n\t\tINIT_WORK(&sde->flush_worker, sdma_field_flush);\n\n\t\tsde->progress_check_head = 0;\n\n\t\ttimer_setup(&sde->err_progress_check_timer,\n\t\t\t    sdma_err_progress_check, 0);\n\n\t\tsde->descq = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdescq_cnt * sizeof(u64[2]),\n\t\t\t\t\t\t&sde->descq_phys, GFP_KERNEL);\n\t\tif (!sde->descq)\n\t\t\tgoto bail;\n\t\tsde->tx_ring =\n\t\t\tkvzalloc_node(array_size(descq_cnt,\n\t\t\t\t\t\t sizeof(struct sdma_txreq *)),\n\t\t\t\t      GFP_KERNEL, dd->node);\n\t\tif (!sde->tx_ring)\n\t\t\tgoto bail;\n\t}\n\n\tdd->sdma_heads_size = L1_CACHE_BYTES * num_engines;\n\t/* Allocate memory for DMA of head registers to memory */\n\tdd->sdma_heads_dma = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdd->sdma_heads_size,\n\t\t\t\t\t\t&dd->sdma_heads_phys,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!dd->sdma_heads_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA head memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t/* Allocate memory for pad */\n\tdd->sdma_pad_dma = dma_alloc_coherent(&dd->pcidev->dev, sizeof(u32),\n\t\t\t\t\t      &dd->sdma_pad_phys, GFP_KERNEL);\n\tif (!dd->sdma_pad_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA pad memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t/* assign each engine to different cacheline and init registers */\n\tcurr_head = (void *)dd->sdma_heads_dma;\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tunsigned long phys_offset;\n\n\t\tsde = &dd->per_sdma[this_idx];\n\n\t\tsde->head_dma = curr_head;\n\t\tcurr_head += L1_CACHE_BYTES;\n\t\tphys_offset = (unsigned long)sde->head_dma -\n\t\t\t      (unsigned long)dd->sdma_heads_dma;\n\t\tsde->head_phys = dd->sdma_heads_phys + phys_offset;\n\t\tinit_sdma_regs(sde, per_sdma_credits, idle_cnt);\n\t}\n\tdd->flags |= HFI1_HAS_SEND_DMA;\n\tdd->flags |= idle_cnt ? HFI1_HAS_SDMA_TIMEOUT : 0;\n\tdd->num_sdma = num_engines;\n\tret = sdma_map_init(dd, port, ppd->vls_operational, NULL);\n\tif (ret < 0)\n\t\tgoto bail;\n\n\ttmp_sdma_rht = kzalloc(sizeof(*tmp_sdma_rht), GFP_KERNEL);\n\tif (!tmp_sdma_rht) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tret = rhashtable_init(tmp_sdma_rht, &sdma_rht_params);\n\tif (ret < 0)\n\t\tgoto bail;\n\tdd->sdma_rht = tmp_sdma_rht;\n\n\tdd_dev_info(dd, \"SDMA num_sdma: %u\\n\", dd->num_sdma);\n\treturn 0;\n\nbail:\n\tsdma_clean(dd, num_engines);\n\treturn ret;\n}\n\n/**\n * sdma_all_running() - called when the link goes up\n * @dd: hfi1_devdata\n *\n * This routine moves all engines to the running state.\n */\nvoid sdma_all_running(struct hfi1_devdata *dd)\n{\n\tstruct sdma_engine *sde;\n\tunsigned int i;\n\n\t/* move all engines to running */\n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e30_go_running);\n\t}\n}\n\n/**\n * sdma_all_idle() - called when the link goes down\n * @dd: hfi1_devdata\n *\n * This routine moves all engines to the idle state.\n */\nvoid sdma_all_idle(struct hfi1_devdata *dd)\n{\n\tstruct sdma_engine *sde;\n\tunsigned int i;\n\n\t/* idle all engines */\n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e70_go_idle);\n\t}\n}\n\n/**\n * sdma_start() - called to kick off state processing for all engines\n * @dd: hfi1_devdata\n *\n * This routine is for kicking off the state processing for all required\n * sdma engines.  Interrupts need to be working at this point.\n *\n */\nvoid sdma_start(struct hfi1_devdata *dd)\n{\n\tunsigned i;\n\tstruct sdma_engine *sde;\n\n\t/* kick off the engines state processing */\n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e10_go_hw_start);\n\t}\n}\n\n/**\n * sdma_exit() - used when module is removed\n * @dd: hfi1_devdata\n */\nvoid sdma_exit(struct hfi1_devdata *dd)\n{\n\tunsigned this_idx;\n\tstruct sdma_engine *sde;\n\n\tfor (this_idx = 0; dd->per_sdma && this_idx < dd->num_sdma;\n\t\t\t++this_idx) {\n\t\tsde = &dd->per_sdma[this_idx];\n\t\tif (!list_empty(&sde->dmawait))\n\t\t\tdd_dev_err(dd, \"sde %u: dmawait list not empty!\\n\",\n\t\t\t\t   sde->this_idx);\n\t\tsdma_process_event(sde, sdma_event_e00_go_hw_down);\n\n\t\tdel_timer_sync(&sde->err_progress_check_timer);\n\n\t\t/*\n\t\t * This waits for the state machine to exit so it is not\n\t\t * necessary to kill the sdma_sw_clean_up_task to make sure\n\t\t * it is not running.\n\t\t */\n\t\tsdma_finalput(&sde->state);\n\t}\n}\n\n/*\n * unmap the indicated descriptor\n */\nstatic inline void sdma_unmap_desc(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_desc *descp)\n{\n\tswitch (sdma_mapping_type(descp)) {\n\tcase SDMA_MAP_SINGLE:\n\t\tdma_unmap_single(\n\t\t\t&dd->pcidev->dev,\n\t\t\tsdma_mapping_addr(descp),\n\t\t\tsdma_mapping_len(descp),\n\t\t\tDMA_TO_DEVICE);\n\t\tbreak;\n\tcase SDMA_MAP_PAGE:\n\t\tdma_unmap_page(\n\t\t\t&dd->pcidev->dev,\n\t\t\tsdma_mapping_addr(descp),\n\t\t\tsdma_mapping_len(descp),\n\t\t\tDMA_TO_DEVICE);\n\t\tbreak;\n\t}\n}\n\n/*\n * return the mode as indicated by the first\n * descriptor in the tx.\n */\nstatic inline u8 ahg_mode(struct sdma_txreq *tx)\n{\n\treturn (tx->descp[0].qw[1] & SDMA_DESC1_HEADER_MODE_SMASK)\n\t\t>> SDMA_DESC1_HEADER_MODE_SHIFT;\n}\n\n/**\n * __sdma_txclean() - clean tx of mappings, descp *kmalloc's\n * @dd: hfi1_devdata for unmapping\n * @tx: tx request to clean\n *\n * This is used in the progress routine to clean the tx or\n * by the ULP to toss an in-process tx build.\n *\n * The code can be called multiple times without issue.\n *\n */\nvoid __sdma_txclean(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_txreq *tx)\n{\n\tu16 i;\n\n\tif (tx->num_desc) {\n\t\tu8 skip = 0, mode = ahg_mode(tx);\n\n\t\t/* unmap first */\n\t\tsdma_unmap_desc(dd, &tx->descp[0]);\n\t\t/* determine number of AHG descriptors to skip */\n\t\tif (mode > SDMA_AHG_APPLY_UPDATE1)\n\t\t\tskip = mode >> 1;\n\t\tfor (i = 1 + skip; i < tx->num_desc; i++)\n\t\t\tsdma_unmap_desc(dd, &tx->descp[i]);\n\t\ttx->num_desc = 0;\n\t}\n\tkfree(tx->coalesce_buf);\n\ttx->coalesce_buf = NULL;\n\t/* kmalloc'ed descp */\n\tif (unlikely(tx->desc_limit > ARRAY_SIZE(tx->descs))) {\n\t\ttx->desc_limit = ARRAY_SIZE(tx->descs);\n\t\tkfree(tx->descp);\n\t}\n}\n\nstatic inline u16 sdma_gethead(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tint use_dmahead;\n\tu16 hwhead;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\nretry:\n\tuse_dmahead = HFI1_CAP_IS_KSET(USE_SDMA_HEAD) && __sdma_running(sde) &&\n\t\t\t\t\t(dd->flags & HFI1_HAS_SDMA_TIMEOUT);\n\thwhead = use_dmahead ?\n\t\t(u16)le64_to_cpu(*sde->head_dma) :\n\t\t(u16)read_sde_csr(sde, SD(HEAD));\n\n\tif (unlikely(HFI1_CAP_IS_KSET(SDMA_HEAD_CHECK))) {\n\t\tu16 cnt;\n\t\tu16 swtail;\n\t\tu16 swhead;\n\t\tint sane;\n\n\t\tswhead = sde->descq_head & sde->sdma_mask;\n\t\t/* this code is really bad for cache line trading */\n\t\tswtail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\t\tcnt = sde->descq_cnt;\n\n\t\tif (swhead < swtail)\n\t\t\t/* not wrapped */\n\t\t\tsane = (hwhead >= swhead) & (hwhead <= swtail);\n\t\telse if (swhead > swtail)\n\t\t\t/* wrapped around */\n\t\t\tsane = ((hwhead >= swhead) && (hwhead < cnt)) ||\n\t\t\t\t(hwhead <= swtail);\n\t\telse\n\t\t\t/* empty */\n\t\t\tsane = (hwhead == swhead);\n\n\t\tif (unlikely(!sane)) {\n\t\t\tdd_dev_err(dd, \"SDMA(%u) bad head (%s) hwhd=%hu swhd=%hu swtl=%hu cnt=%hu\\n\",\n\t\t\t\t   sde->this_idx,\n\t\t\t\t   use_dmahead ? \"dma\" : \"kreg\",\n\t\t\t\t   hwhead, swhead, swtail, cnt);\n\t\t\tif (use_dmahead) {\n\t\t\t\t/* try one more time, using csr */\n\t\t\t\tuse_dmahead = 0;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\t/* proceed as if no progress */\n\t\t\thwhead = swhead;\n\t\t}\n\t}\n\treturn hwhead;\n}\n\n/*\n * This is called when there are send DMA descriptors that might be\n * available.\n *\n * This is called with head_lock held.\n */\nstatic void sdma_desc_avail(struct sdma_engine *sde, uint avail)\n{\n\tstruct iowait *wait, *nw, *twait;\n\tstruct iowait *waits[SDMA_WAIT_BATCH_SIZE];\n\tuint i, n = 0, seq, tidx = 0;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\", sde->this_idx,\n\t\t   slashstrip(__FILE__), __LINE__, __func__);\n\tdd_dev_err(sde->dd, \"avail: %u\\n\", avail);\n#endif\n\n\tdo {\n\t\tseq = read_seqbegin(&sde->waitlock);\n\t\tif (!list_empty(&sde->dmawait)) {\n\t\t\t/* at least one item */\n\t\t\twrite_seqlock(&sde->waitlock);\n\t\t\t/* Harvest waiters wanting DMA descriptors */\n\t\t\tlist_for_each_entry_safe(\n\t\t\t\t\twait,\n\t\t\t\t\tnw,\n\t\t\t\t\t&sde->dmawait,\n\t\t\t\t\tlist) {\n\t\t\t\tu32 num_desc;\n\n\t\t\t\tif (!wait->wakeup)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (n == ARRAY_SIZE(waits))\n\t\t\t\t\tbreak;\n\t\t\t\tiowait_init_priority(wait);\n\t\t\t\tnum_desc = iowait_get_all_desc(wait);\n\t\t\t\tif (num_desc > avail)\n\t\t\t\t\tbreak;\n\t\t\t\tavail -= num_desc;\n\t\t\t\t/* Find the top-priority wait memeber */\n\t\t\t\tif (n) {\n\t\t\t\t\ttwait = waits[tidx];\n\t\t\t\t\ttidx =\n\t\t\t\t\t    iowait_priority_update_top(wait,\n\t\t\t\t\t\t\t\t       twait,\n\t\t\t\t\t\t\t\t       n,\n\t\t\t\t\t\t\t\t       tidx);\n\t\t\t\t}\n\t\t\t\tlist_del_init(&wait->list);\n\t\t\t\twaits[n++] = wait;\n\t\t\t}\n\t\t\twrite_sequnlock(&sde->waitlock);\n\t\t\tbreak;\n\t\t}\n\t} while (read_seqretry(&sde->waitlock, seq));\n\n\t/* Schedule the top-priority entry first */\n\tif (n)\n\t\twaits[tidx]->wakeup(waits[tidx], SDMA_AVAIL_REASON);\n\n\tfor (i = 0; i < n; i++)\n\t\tif (i != tidx)\n\t\t\twaits[i]->wakeup(waits[i], SDMA_AVAIL_REASON);\n}\n\n/* head_lock must be held */\nstatic void sdma_make_progress(struct sdma_engine *sde, u64 status)\n{\n\tstruct sdma_txreq *txp = NULL;\n\tint progress = 0;\n\tu16 hwhead, swhead;\n\tint idle_check_done = 0;\n\n\thwhead = sdma_gethead(sde);\n\n\t/* The reason for some of the complexity of this code is that\n\t * not all descriptors have corresponding txps.  So, we have to\n\t * be able to skip over descs until we wander into the range of\n\t * the next txp on the list.\n\t */\n\nretry:\n\ttxp = get_txhead(sde);\n\tswhead = sde->descq_head & sde->sdma_mask;\n\ttrace_hfi1_sdma_progress(sde, hwhead, swhead, txp);\n\twhile (swhead != hwhead) {\n\t\t/* advance head, wrap if needed */\n\t\tswhead = ++sde->descq_head & sde->sdma_mask;\n\n\t\t/* if now past this txp's descs, do the callback */\n\t\tif (txp && txp->next_descq_idx == swhead) {\n\t\t\t/* remove from list */\n\t\t\tsde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;\n\t\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_OK);\n\t\t\t/* see if there is another txp */\n\t\t\ttxp = get_txhead(sde);\n\t\t}\n\t\ttrace_hfi1_sdma_progress(sde, hwhead, swhead, txp);\n\t\tprogress++;\n\t}\n\n\t/*\n\t * The SDMA idle interrupt is not guaranteed to be ordered with respect\n\t * to updates to the the dma_head location in host memory. The head\n\t * value read might not be fully up to date. If there are pending\n\t * descriptors and the SDMA idle interrupt fired then read from the\n\t * CSR SDMA head instead to get the latest value from the hardware.\n\t * The hardware SDMA head should be read at most once in this invocation\n\t * of sdma_make_progress(..) which is ensured by idle_check_done flag\n\t */\n\tif ((status & sde->idle_mask) && !idle_check_done) {\n\t\tu16 swtail;\n\n\t\tswtail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\t\tif (swtail != hwhead) {\n\t\t\thwhead = (u16)read_sde_csr(sde, SD(HEAD));\n\t\t\tidle_check_done = 1;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tsde->last_status = status;\n\tif (progress)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n}\n\n/*\n * sdma_engine_interrupt() - interrupt handler for engine\n * @sde: sdma engine\n * @status: sdma interrupt reason\n *\n * Status is a mask of the 3 possible interrupts for this engine.  It will\n * contain bits _only_ for this SDMA engine.  It will contain at least one\n * bit, it may contain more.\n */\nvoid sdma_engine_interrupt(struct sdma_engine *sde, u64 status)\n{\n\ttrace_hfi1_sdma_engine_interrupt(sde, status);\n\twrite_seqlock(&sde->head_lock);\n\tsdma_set_desc_cnt(sde, sdma_desct_intr);\n\tif (status & sde->idle_mask)\n\t\tsde->idle_int_cnt++;\n\telse if (status & sde->progress_mask)\n\t\tsde->progress_int_cnt++;\n\telse if (status & sde->int_mask)\n\t\tsde->sdma_int_cnt++;\n\tsdma_make_progress(sde, status);\n\twrite_sequnlock(&sde->head_lock);\n}\n\n/**\n * sdma_engine_error() - error handler for engine\n * @sde: sdma engine\n * @status: sdma interrupt reason\n */\nvoid sdma_engine_error(struct sdma_engine *sde, u64 status)\n{\n\tunsigned long flags;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) error status 0x%llx state %s\\n\",\n\t\t   sde->this_idx,\n\t\t   (unsigned long long)status,\n\t\t   sdma_state_names[sde->state.current_state]);\n#endif\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\tif (status & ALL_SDMA_ENG_HALT_ERRS)\n\t\t__sdma_process_event(sde, sdma_event_e60_hw_halted);\n\tif (status & ~SD(ENG_ERR_STATUS_SDMA_HALT_ERR_SMASK)) {\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA (%u) engine error: 0x%llx state %s\\n\",\n\t\t\t   sde->this_idx,\n\t\t\t   (unsigned long long)status,\n\t\t\t   sdma_state_names[sde->state.current_state]);\n\t\tdump_sdma_state(sde);\n\t}\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void sdma_sendctrl(struct sdma_engine *sde, unsigned op)\n{\n\tu64 set_senddmactrl = 0;\n\tu64 clr_senddmactrl = 0;\n\tunsigned long flags;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) senddmactrl E=%d I=%d H=%d C=%d\\n\",\n\t\t   sde->this_idx,\n\t\t   (op & SDMA_SENDCTRL_OP_ENABLE) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_INTENABLE) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_HALT) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_CLEANUP) ? 1 : 0);\n#endif\n\n\tif (op & SDMA_SENDCTRL_OP_ENABLE)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_ENABLE_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_ENABLE_SMASK);\n\n\tif (op & SDMA_SENDCTRL_OP_INTENABLE)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_INT_ENABLE_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_INT_ENABLE_SMASK);\n\n\tif (op & SDMA_SENDCTRL_OP_HALT)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_HALT_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_HALT_SMASK);\n\n\tspin_lock_irqsave(&sde->senddmactrl_lock, flags);\n\n\tsde->p_senddmactrl |= set_senddmactrl;\n\tsde->p_senddmactrl &= ~clr_senddmactrl;\n\n\tif (op & SDMA_SENDCTRL_OP_CLEANUP)\n\t\twrite_sde_csr(sde, SD(CTRL),\n\t\t\t      sde->p_senddmactrl |\n\t\t\t      SD(CTRL_SDMA_CLEANUP_SMASK));\n\telse\n\t\twrite_sde_csr(sde, SD(CTRL), sde->p_senddmactrl);\n\n\tspin_unlock_irqrestore(&sde->senddmactrl_lock, flags);\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tsdma_dumpstate(sde);\n#endif\n}\n\nstatic void sdma_setlengen(struct sdma_engine *sde)\n{\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\t/*\n\t * Set SendDmaLenGen and clear-then-set the MSB of the generation\n\t * count to enable generation checking and load the internal\n\t * generation counter.\n\t */\n\twrite_sde_csr(sde, SD(LEN_GEN),\n\t\t      (sde->descq_cnt / 64) << SD(LEN_GEN_LENGTH_SHIFT));\n\twrite_sde_csr(sde, SD(LEN_GEN),\n\t\t      ((sde->descq_cnt / 64) << SD(LEN_GEN_LENGTH_SHIFT)) |\n\t\t      (4ULL << SD(LEN_GEN_GENERATION_SHIFT)));\n}\n\nstatic inline void sdma_update_tail(struct sdma_engine *sde, u16 tail)\n{\n\t/* Commit writes to memory and advance the tail on the chip */\n\tsmp_wmb(); /* see get_txhead() */\n\twriteq(tail, sde->tail_csr);\n}\n\n/*\n * This is called when changing to state s10_hw_start_up_halt_wait as\n * a result of send buffer errors or send DMA descriptor errors.\n */\nstatic void sdma_hw_start_up(struct sdma_engine *sde)\n{\n\tu64 reg;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\tsdma_setlengen(sde);\n\tsdma_update_tail(sde, 0); /* Set SendDmaTail */\n\t*sde->head_dma = 0;\n\n\treg = SD(ENG_ERR_CLEAR_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_MASK) <<\n\t      SD(ENG_ERR_CLEAR_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_SHIFT);\n\twrite_sde_csr(sde, SD(ENG_ERR_CLEAR), reg);\n}\n\n/*\n * set_sdma_integrity\n *\n * Set the SEND_DMA_CHECK_ENABLE register for send DMA engine 'sde'.\n */\nstatic void set_sdma_integrity(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\n\twrite_sde_csr(sde, SD(CHECK_ENABLE),\n\t\t      hfi1_pkt_base_sdma_integrity(dd));\n}\n\nstatic void init_sdma_regs(\n\tstruct sdma_engine *sde,\n\tu32 credits,\n\tuint idle_cnt)\n{\n\tu8 opval, opmask;\n#ifdef CONFIG_SDMA_VERBOSITY\n\tstruct hfi1_devdata *dd = sde->dd;\n\n\tdd_dev_err(dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\twrite_sde_csr(sde, SD(BASE_ADDR), sde->descq_phys);\n\tsdma_setlengen(sde);\n\tsdma_update_tail(sde, 0); /* Set SendDmaTail */\n\twrite_sde_csr(sde, SD(RELOAD_CNT), idle_cnt);\n\twrite_sde_csr(sde, SD(DESC_CNT), 0);\n\twrite_sde_csr(sde, SD(HEAD_ADDR), sde->head_phys);\n\twrite_sde_csr(sde, SD(MEMORY),\n\t\t      ((u64)credits << SD(MEMORY_SDMA_MEMORY_CNT_SHIFT)) |\n\t\t      ((u64)(credits * sde->this_idx) <<\n\t\t       SD(MEMORY_SDMA_MEMORY_INDEX_SHIFT)));\n\twrite_sde_csr(sde, SD(ENG_ERR_MASK), ~0ull);\n\tset_sdma_integrity(sde);\n\topmask = OPCODE_CHECK_MASK_DISABLED;\n\topval = OPCODE_CHECK_VAL_DISABLED;\n\twrite_sde_csr(sde, SD(CHECK_OPCODE),\n\t\t      (opmask << SEND_CTXT_CHECK_OPCODE_MASK_SHIFT) |\n\t\t      (opval << SEND_CTXT_CHECK_OPCODE_VALUE_SHIFT));\n}\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\n#define sdma_dumpstate_helper0(reg) do { \\\n\t\tcsr = read_csr(sde->dd, reg); \\\n\t\tdd_dev_err(sde->dd, \"%36s     0x%016llx\\n\", #reg, csr); \\\n\t} while (0)\n\n#define sdma_dumpstate_helper(reg) do { \\\n\t\tcsr = read_sde_csr(sde, reg); \\\n\t\tdd_dev_err(sde->dd, \"%36s[%02u] 0x%016llx\\n\", \\\n\t\t\t#reg, sde->this_idx, csr); \\\n\t} while (0)\n\n#define sdma_dumpstate_helper2(reg) do { \\\n\t\tcsr = read_csr(sde->dd, reg + (8 * i)); \\\n\t\tdd_dev_err(sde->dd, \"%33s_%02u     0x%016llx\\n\", \\\n\t\t\t\t#reg, i, csr); \\\n\t} while (0)\n\nvoid sdma_dumpstate(struct sdma_engine *sde)\n{\n\tu64 csr;\n\tunsigned i;\n\n\tsdma_dumpstate_helper(SD(CTRL));\n\tsdma_dumpstate_helper(SD(STATUS));\n\tsdma_dumpstate_helper0(SD(ERR_STATUS));\n\tsdma_dumpstate_helper0(SD(ERR_MASK));\n\tsdma_dumpstate_helper(SD(ENG_ERR_STATUS));\n\tsdma_dumpstate_helper(SD(ENG_ERR_MASK));\n\n\tfor (i = 0; i < CCE_NUM_INT_CSRS; ++i) {\n\t\tsdma_dumpstate_helper2(CCE_INT_STATUS);\n\t\tsdma_dumpstate_helper2(CCE_INT_MASK);\n\t\tsdma_dumpstate_helper2(CCE_INT_BLOCKED);\n\t}\n\n\tsdma_dumpstate_helper(SD(TAIL));\n\tsdma_dumpstate_helper(SD(HEAD));\n\tsdma_dumpstate_helper(SD(PRIORITY_THLD));\n\tsdma_dumpstate_helper(SD(IDLE_CNT));\n\tsdma_dumpstate_helper(SD(RELOAD_CNT));\n\tsdma_dumpstate_helper(SD(DESC_CNT));\n\tsdma_dumpstate_helper(SD(DESC_FETCHED_CNT));\n\tsdma_dumpstate_helper(SD(MEMORY));\n\tsdma_dumpstate_helper0(SD(ENGINES));\n\tsdma_dumpstate_helper0(SD(MEM_SIZE));\n\t/* sdma_dumpstate_helper(SEND_EGRESS_SEND_DMA_STATUS);  */\n\tsdma_dumpstate_helper(SD(BASE_ADDR));\n\tsdma_dumpstate_helper(SD(LEN_GEN));\n\tsdma_dumpstate_helper(SD(HEAD_ADDR));\n\tsdma_dumpstate_helper(SD(CHECK_ENABLE));\n\tsdma_dumpstate_helper(SD(CHECK_VL));\n\tsdma_dumpstate_helper(SD(CHECK_JOB_KEY));\n\tsdma_dumpstate_helper(SD(CHECK_PARTITION_KEY));\n\tsdma_dumpstate_helper(SD(CHECK_SLID));\n\tsdma_dumpstate_helper(SD(CHECK_OPCODE));\n}\n#endif\n\nstatic void dump_sdma_state(struct sdma_engine *sde)\n{\n\tstruct hw_sdma_desc *descqp;\n\tu64 desc[2];\n\tu64 addr;\n\tu8 gen;\n\tu16 len;\n\tu16 head, tail, cnt;\n\n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = sde->descq_tail & sde->sdma_mask;\n\tcnt = sdma_descq_freecnt(sde);\n\n\tdd_dev_err(sde->dd,\n\t\t   \"SDMA (%u) descq_head: %u descq_tail: %u freecnt: %u FLE %d\\n\",\n\t\t   sde->this_idx, head, tail, cnt,\n\t\t   !list_empty(&sde->flushlist));\n\n\t/* print info for each entry in the descriptor queue */\n\twhile (head != tail) {\n\t\tchar flags[6] = { 'x', 'x', 'x', 'x', 0 };\n\n\t\tdescqp = &sde->descq[head];\n\t\tdesc[0] = le64_to_cpu(descqp->qw[0]);\n\t\tdesc[1] = le64_to_cpu(descqp->qw[1]);\n\t\tflags[0] = (desc[1] & SDMA_DESC1_INT_REQ_FLAG) ? 'I' : '-';\n\t\tflags[1] = (desc[1] & SDMA_DESC1_HEAD_TO_HOST_FLAG) ?\n\t\t\t\t'H' : '-';\n\t\tflags[2] = (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG) ? 'F' : '-';\n\t\tflags[3] = (desc[0] & SDMA_DESC0_LAST_DESC_FLAG) ? 'L' : '-';\n\t\taddr = (desc[0] >> SDMA_DESC0_PHY_ADDR_SHIFT)\n\t\t\t& SDMA_DESC0_PHY_ADDR_MASK;\n\t\tgen = (desc[1] >> SDMA_DESC1_GENERATION_SHIFT)\n\t\t\t& SDMA_DESC1_GENERATION_MASK;\n\t\tlen = (desc[0] >> SDMA_DESC0_BYTE_COUNT_SHIFT)\n\t\t\t& SDMA_DESC0_BYTE_COUNT_MASK;\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA sdmadesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes\\n\",\n\t\t\t   head, flags, addr, gen, len);\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"\\tdesc0:0x%016llx desc1 0x%016llx\\n\",\n\t\t\t   desc[0], desc[1]);\n\t\tif (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG)\n\t\t\tdd_dev_err(sde->dd,\n\t\t\t\t   \"\\taidx: %u amode: %u alen: %u\\n\",\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_INDEX_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_INDEX_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_MODE_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_MODE_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_DWS_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_DWS_SHIFT));\n\t\thead++;\n\t\thead &= sde->sdma_mask;\n\t}\n}\n\n#define SDE_FMT \\\n\t\"SDE %u CPU %d STE %s C 0x%llx S 0x%016llx E 0x%llx T(HW) 0x%llx T(SW) 0x%x H(HW) 0x%llx H(SW) 0x%x H(D) 0x%llx DM 0x%llx GL 0x%llx R 0x%llx LIS 0x%llx AHGI 0x%llx TXT %u TXH %u DT %u DH %u FLNE %d DQF %u SLC 0x%llx\\n\"\n/**\n * sdma_seqfile_dump_sde() - debugfs dump of sde\n * @s: seq file\n * @sde: send dma engine to dump\n *\n * This routine dumps the sde to the indicated seq file.\n */\nvoid sdma_seqfile_dump_sde(struct seq_file *s, struct sdma_engine *sde)\n{\n\tu16 head, tail;\n\tstruct hw_sdma_desc *descqp;\n\tu64 desc[2];\n\tu64 addr;\n\tu8 gen;\n\tu16 len;\n\n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\tseq_printf(s, SDE_FMT, sde->this_idx,\n\t\t   sde->cpu,\n\t\t   sdma_state_name(sde->state.current_state),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(CTRL)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(STATUS)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(ENG_ERR_STATUS)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(TAIL)), tail,\n\t\t   (unsigned long long)read_sde_csr(sde, SD(HEAD)), head,\n\t\t   (unsigned long long)le64_to_cpu(*sde->head_dma),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(MEMORY)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(LEN_GEN)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(RELOAD_CNT)),\n\t\t   (unsigned long long)sde->last_status,\n\t\t   (unsigned long long)sde->ahg_bits,\n\t\t   sde->tx_tail,\n\t\t   sde->tx_head,\n\t\t   sde->descq_tail,\n\t\t   sde->descq_head,\n\t\t   !list_empty(&sde->flushlist),\n\t\t   sde->descq_full_count,\n\t\t   (unsigned long long)read_sde_csr(sde, SEND_DMA_CHECK_SLID));\n\n\t/* print info for each entry in the descriptor queue */\n\twhile (head != tail) {\n\t\tchar flags[6] = { 'x', 'x', 'x', 'x', 0 };\n\n\t\tdescqp = &sde->descq[head];\n\t\tdesc[0] = le64_to_cpu(descqp->qw[0]);\n\t\tdesc[1] = le64_to_cpu(descqp->qw[1]);\n\t\tflags[0] = (desc[1] & SDMA_DESC1_INT_REQ_FLAG) ? 'I' : '-';\n\t\tflags[1] = (desc[1] & SDMA_DESC1_HEAD_TO_HOST_FLAG) ?\n\t\t\t\t'H' : '-';\n\t\tflags[2] = (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG) ? 'F' : '-';\n\t\tflags[3] = (desc[0] & SDMA_DESC0_LAST_DESC_FLAG) ? 'L' : '-';\n\t\taddr = (desc[0] >> SDMA_DESC0_PHY_ADDR_SHIFT)\n\t\t\t& SDMA_DESC0_PHY_ADDR_MASK;\n\t\tgen = (desc[1] >> SDMA_DESC1_GENERATION_SHIFT)\n\t\t\t& SDMA_DESC1_GENERATION_MASK;\n\t\tlen = (desc[0] >> SDMA_DESC0_BYTE_COUNT_SHIFT)\n\t\t\t& SDMA_DESC0_BYTE_COUNT_MASK;\n\t\tseq_printf(s,\n\t\t\t   \"\\tdesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes\\n\",\n\t\t\t   head, flags, addr, gen, len);\n\t\tif (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG)\n\t\t\tseq_printf(s, \"\\t\\tahgidx: %u ahgmode: %u\\n\",\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_INDEX_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_INDEX_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_MODE_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_MODE_SHIFT));\n\t\thead = (head + 1) & sde->sdma_mask;\n\t}\n}\n\n/*\n * add the generation number into\n * the qw1 and return\n */\nstatic inline u64 add_gen(struct sdma_engine *sde, u64 qw1)\n{\n\tu8 generation = (sde->descq_tail >> sde->sdma_shift) & 3;\n\n\tqw1 &= ~SDMA_DESC1_GENERATION_SMASK;\n\tqw1 |= ((u64)generation & SDMA_DESC1_GENERATION_MASK)\n\t\t\t<< SDMA_DESC1_GENERATION_SHIFT;\n\treturn qw1;\n}\n\n/*\n * This routine submits the indicated tx\n *\n * Space has already been guaranteed and\n * tail side of ring is locked.\n *\n * The hardware tail update is done\n * in the caller and that is facilitated\n * by returning the new tail.\n *\n * There is special case logic for ahg\n * to not add the generation number for\n * up to 2 descriptors that follow the\n * first descriptor.\n *\n */\nstatic inline u16 submit_tx(struct sdma_engine *sde, struct sdma_txreq *tx)\n{\n\tint i;\n\tu16 tail;\n\tstruct sdma_desc *descp = tx->descp;\n\tu8 skip = 0, mode = ahg_mode(tx);\n\n\ttail = sde->descq_tail & sde->sdma_mask;\n\tsde->descq[tail].qw[0] = cpu_to_le64(descp->qw[0]);\n\tsde->descq[tail].qw[1] = cpu_to_le64(add_gen(sde, descp->qw[1]));\n\ttrace_hfi1_sdma_descriptor(sde, descp->qw[0], descp->qw[1],\n\t\t\t\t   tail, &sde->descq[tail]);\n\ttail = ++sde->descq_tail & sde->sdma_mask;\n\tdescp++;\n\tif (mode > SDMA_AHG_APPLY_UPDATE1)\n\t\tskip = mode >> 1;\n\tfor (i = 1; i < tx->num_desc; i++, descp++) {\n\t\tu64 qw1;\n\n\t\tsde->descq[tail].qw[0] = cpu_to_le64(descp->qw[0]);\n\t\tif (skip) {\n\t\t\t/* edits don't have generation */\n\t\t\tqw1 = descp->qw[1];\n\t\t\tskip--;\n\t\t} else {\n\t\t\t/* replace generation with real one for non-edits */\n\t\t\tqw1 = add_gen(sde, descp->qw[1]);\n\t\t}\n\t\tsde->descq[tail].qw[1] = cpu_to_le64(qw1);\n\t\ttrace_hfi1_sdma_descriptor(sde, descp->qw[0], qw1,\n\t\t\t\t\t   tail, &sde->descq[tail]);\n\t\ttail = ++sde->descq_tail & sde->sdma_mask;\n\t}\n\ttx->next_descq_idx = tail;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttx->sn = sde->tail_sn++;\n\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n\tWARN_ON_ONCE(sde->tx_ring[sde->tx_tail & sde->sdma_mask]);\n#endif\n\tsde->tx_ring[sde->tx_tail++ & sde->sdma_mask] = tx;\n\tsde->desc_avail -= tx->num_desc;\n\treturn tail;\n}\n\n/*\n * Check for progress\n */\nstatic int sdma_check_progress(\n\tstruct sdma_engine *sde,\n\tstruct iowait_work *wait,\n\tstruct sdma_txreq *tx,\n\tbool pkts_sent)\n{\n\tint ret;\n\n\tsde->desc_avail = sdma_descq_freecnt(sde);\n\tif (tx->num_desc <= sde->desc_avail)\n\t\treturn -EAGAIN;\n\t/* pulse the head_lock */\n\tif (wait && iowait_ioww_to_iow(wait)->sleep) {\n\t\tunsigned seq;\n\n\t\tseq = raw_seqcount_begin(\n\t\t\t(const seqcount_t *)&sde->head_lock.seqcount);\n\t\tret = wait->iow->sleep(sde, wait, tx, seq, pkts_sent);\n\t\tif (ret == -EAGAIN)\n\t\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t} else {\n\t\tret = -EBUSY;\n\t}\n\treturn ret;\n}\n\n/**\n * sdma_send_txreq() - submit a tx req to ring\n * @sde: sdma engine to use\n * @wait: SE wait structure to use when full (may be NULL)\n * @tx: sdma_txreq to submit\n * @pkts_sent: has any packet been sent yet?\n *\n * The call submits the tx into the ring.  If a iowait structure is non-NULL\n * the packet will be queued to the list in wait.\n *\n * Return:\n * 0 - Success, -EINVAL - sdma_txreq incomplete, -EBUSY - no space in\n * ring (wait == NULL)\n * -EIOCBQUEUED - tx queued to iowait, -ECOMM bad sdma state\n */\nint sdma_send_txreq(struct sdma_engine *sde,\n\t\t    struct iowait_work *wait,\n\t\t    struct sdma_txreq *tx,\n\t\t    bool pkts_sent)\n{\n\tint ret = 0;\n\tu16 tail;\n\tunsigned long flags;\n\n\t/* user should have supplied entire packet */\n\tif (unlikely(tx->tlen))\n\t\treturn -EINVAL;\n\ttx->wait = iowait_ioww_to_iow(wait);\n\tspin_lock_irqsave(&sde->tail_lock, flags);\nretry:\n\tif (unlikely(!__sdma_running(sde)))\n\t\tgoto unlock_noconn;\n\tif (unlikely(tx->num_desc > sde->desc_avail))\n\t\tgoto nodesc;\n\ttail = submit_tx(sde, tx);\n\tif (wait)\n\t\tiowait_sdma_inc(iowait_ioww_to_iow(wait));\n\tsdma_update_tail(sde, tail);\nunlock:\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n\treturn ret;\nunlock_noconn:\n\tif (wait)\n\t\tiowait_sdma_inc(iowait_ioww_to_iow(wait));\n\ttx->next_descq_idx = 0;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttx->sn = sde->tail_sn++;\n\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n#endif\n\tspin_lock(&sde->flushlist_lock);\n\tlist_add_tail(&tx->list, &sde->flushlist);\n\tspin_unlock(&sde->flushlist_lock);\n\tiowait_inc_wait_count(wait, tx->num_desc);\n\tqueue_work_on(sde->cpu, system_highpri_wq, &sde->flush_worker);\n\tret = -ECOMM;\n\tgoto unlock;\nnodesc:\n\tret = sdma_check_progress(sde, wait, tx, pkts_sent);\n\tif (ret == -EAGAIN) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tsde->descq_full_count++;\n\tgoto unlock;\n}\n\n/**\n * sdma_send_txlist() - submit a list of tx req to ring\n * @sde: sdma engine to use\n * @wait: SE wait structure to use when full (may be NULL)\n * @tx_list: list of sdma_txreqs to submit\n * @count: pointer to a u16 which, after return will contain the total number of\n *         sdma_txreqs removed from the tx_list. This will include sdma_txreqs\n *         whose SDMA descriptors are submitted to the ring and the sdma_txreqs\n *         which are added to SDMA engine flush list if the SDMA engine state is\n *         not running.\n *\n * The call submits the list into the ring.\n *\n * If the iowait structure is non-NULL and not equal to the iowait list\n * the unprocessed part of the list  will be appended to the list in wait.\n *\n * In all cases, the tx_list will be updated so the head of the tx_list is\n * the list of descriptors that have yet to be transmitted.\n *\n * The intent of this call is to provide a more efficient\n * way of submitting multiple packets to SDMA while holding the tail\n * side locking.\n *\n * Return:\n * 0 - Success,\n * -EINVAL - sdma_txreq incomplete, -EBUSY - no space in ring (wait == NULL)\n * -EIOCBQUEUED - tx queued to iowait, -ECOMM bad sdma state\n */\nint sdma_send_txlist(struct sdma_engine *sde, struct iowait_work *wait,\n\t\t     struct list_head *tx_list, u16 *count_out)\n{\n\tstruct sdma_txreq *tx, *tx_next;\n\tint ret = 0;\n\tunsigned long flags;\n\tu16 tail = INVALID_TAIL;\n\tu32 submit_count = 0, flush_count = 0, total_count;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\nretry:\n\tlist_for_each_entry_safe(tx, tx_next, tx_list, list) {\n\t\ttx->wait = iowait_ioww_to_iow(wait);\n\t\tif (unlikely(!__sdma_running(sde)))\n\t\t\tgoto unlock_noconn;\n\t\tif (unlikely(tx->num_desc > sde->desc_avail))\n\t\t\tgoto nodesc;\n\t\tif (unlikely(tx->tlen)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto update_tail;\n\t\t}\n\t\tlist_del_init(&tx->list);\n\t\ttail = submit_tx(sde, tx);\n\t\tsubmit_count++;\n\t\tif (tail != INVALID_TAIL &&\n\t\t    (submit_count & SDMA_TAIL_UPDATE_THRESH) == 0) {\n\t\t\tsdma_update_tail(sde, tail);\n\t\t\ttail = INVALID_TAIL;\n\t\t}\n\t}\nupdate_tail:\n\ttotal_count = submit_count + flush_count;\n\tif (wait) {\n\t\tiowait_sdma_add(iowait_ioww_to_iow(wait), total_count);\n\t\tiowait_starve_clear(submit_count > 0,\n\t\t\t\t    iowait_ioww_to_iow(wait));\n\t}\n\tif (tail != INVALID_TAIL)\n\t\tsdma_update_tail(sde, tail);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n\t*count_out = total_count;\n\treturn ret;\nunlock_noconn:\n\tspin_lock(&sde->flushlist_lock);\n\tlist_for_each_entry_safe(tx, tx_next, tx_list, list) {\n\t\ttx->wait = iowait_ioww_to_iow(wait);\n\t\tlist_del_init(&tx->list);\n\t\ttx->next_descq_idx = 0;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\t\ttx->sn = sde->tail_sn++;\n\t\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n#endif\n\t\tlist_add_tail(&tx->list, &sde->flushlist);\n\t\tflush_count++;\n\t\tiowait_inc_wait_count(wait, tx->num_desc);\n\t}\n\tspin_unlock(&sde->flushlist_lock);\n\tqueue_work_on(sde->cpu, system_highpri_wq, &sde->flush_worker);\n\tret = -ECOMM;\n\tgoto update_tail;\nnodesc:\n\tret = sdma_check_progress(sde, wait, tx, submit_count > 0);\n\tif (ret == -EAGAIN) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tsde->descq_full_count++;\n\tgoto update_tail;\n}\n\nstatic void sdma_process_event(struct sdma_engine *sde, enum sdma_events event)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\n\t__sdma_process_event(sde, event);\n\n\tif (sde->state.current_state == sdma_state_s99_running)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void __sdma_process_event(struct sdma_engine *sde,\n\t\t\t\t enum sdma_events event)\n{\n\tstruct sdma_state *ss = &sde->state;\n\tint need_progress = 0;\n\n\t/* CONFIG SDMA temporary */\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) [%s] %s\\n\", sde->this_idx,\n\t\t   sdma_state_names[ss->current_state],\n\t\t   sdma_event_names[event]);\n#endif\n\n\tswitch (ss->current_state) {\n\tcase sdma_state_s00_hw_down:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\t/*\n\t\t\t * If down, but running requested (usually result\n\t\t\t * of link up, then we need to start up.\n\t\t\t * This can happen when hw down is requested while\n\t\t\t * bringing the link up with traffic active on\n\t\t\t * 7220, e.g.\n\t\t\t */\n\t\t\tss->go_s99_running = 1;\n\t\t\t/* fall through -- and start dma engine */\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\t/* This reference means the state machine is started */\n\t\t\tsdma_get(&sde->state);\n\t\t\tsdma_set_state(sde,\n\t\t\t\t       sdma_state_s10_hw_start_up_halt_wait);\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s10_hw_start_up_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde,\n\t\t\t\t       sdma_state_s15_hw_start_up_clean_wait);\n\t\t\tsdma_start_hw_clean_up(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s15_hw_start_up_clean_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s20_idle:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tsdma_set_state(sde, sdma_state_s99_running);\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tsdma_set_state(sde, sdma_state_s50_hw_halt_wait);\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\t/* fall through */\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tsdma_set_state(sde, sdma_state_s80_hw_freeze);\n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s30_sw_clean_up_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tsdma_set_state(sde, sdma_state_s40_hw_clean_up_wait);\n\t\t\tsdma_start_hw_clean_up(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s40_hw_clean_up_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s50_hw_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde, sdma_state_s30_sw_clean_up_wait);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s60_idle_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde, sdma_state_s30_sw_clean_up_wait);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s80_hw_freeze:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tsdma_set_state(sde, sdma_state_s82_freeze_sw_clean);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s82_freeze_sw_clean:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\t/* notify caller this engine is done cleaning */\n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s99_running:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tneed_progress = 1;\n\t\t\tsdma_err_progress_check_schedule(sde);\n\t\t\t/* fall through */\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\t/*\n\t\t\t* SW initiated halt does not perform engines\n\t\t\t* progress check\n\t\t\t*/\n\t\t\tsdma_set_state(sde, sdma_state_s50_hw_halt_wait);\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tsdma_set_state(sde, sdma_state_s60_idle_halt_wait);\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\t/* fall through */\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tsdma_set_state(sde, sdma_state_s80_hw_freeze);\n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tss->last_event = event;\n\tif (need_progress)\n\t\tsdma_make_progress(sde, 0);\n}\n\n/*\n * _extend_sdma_tx_descs() - helper to extend txreq\n *\n * This is called once the initial nominal allocation\n * of descriptors in the sdma_txreq is exhausted.\n *\n * The code will bump the allocation up to the max\n * of MAX_DESC (64) descriptors. There doesn't seem\n * much point in an interim step. The last descriptor\n * is reserved for coalesce buffer in order to support\n * cases where input packet has >MAX_DESC iovecs.\n *\n */\nstatic int _extend_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx)\n{\n\tint i;\n\n\t/* Handle last descriptor */\n\tif (unlikely((tx->num_desc == (MAX_DESC - 1)))) {\n\t\t/* if tlen is 0, it is for padding, release last descriptor */\n\t\tif (!tx->tlen) {\n\t\t\ttx->desc_limit = MAX_DESC;\n\t\t} else if (!tx->coalesce_buf) {\n\t\t\t/* allocate coalesce buffer with space for padding */\n\t\t\ttx->coalesce_buf = kmalloc(tx->tlen + sizeof(u32),\n\t\t\t\t\t\t   GFP_ATOMIC);\n\t\t\tif (!tx->coalesce_buf)\n\t\t\t\tgoto enomem;\n\t\t\ttx->coalesce_idx = 0;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(tx->num_desc == MAX_DESC))\n\t\tgoto enomem;\n\n\ttx->descp = kmalloc_array(\n\t\t\tMAX_DESC,\n\t\t\tsizeof(struct sdma_desc),\n\t\t\tGFP_ATOMIC);\n\tif (!tx->descp)\n\t\tgoto enomem;\n\n\t/* reserve last descriptor for coalescing */\n\ttx->desc_limit = MAX_DESC - 1;\n\t/* copy ones already built */\n\tfor (i = 0; i < tx->num_desc; i++)\n\t\ttx->descp[i] = tx->descs[i];\n\treturn 0;\nenomem:\n\t__sdma_txclean(dd, tx);\n\treturn -ENOMEM;\n}\n\n/*\n * ext_coal_sdma_tx_descs() - extend or coalesce sdma tx descriptors\n *\n * This is called once the initial nominal allocation of descriptors\n * in the sdma_txreq is exhausted.\n *\n * This function calls _extend_sdma_tx_descs to extend or allocate\n * coalesce buffer. If there is a allocated coalesce buffer, it will\n * copy the input packet data into the coalesce buffer. It also adds\n * coalesce buffer descriptor once when whole packet is received.\n *\n * Return:\n * <0 - error\n * 0 - coalescing, don't populate descriptor\n * 1 - continue with populating descriptor\n */\nint ext_coal_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx,\n\t\t\t   int type, void *kvaddr, struct page *page,\n\t\t\t   unsigned long offset, u16 len)\n{\n\tint pad_len, rval;\n\tdma_addr_t addr;\n\n\trval = _extend_sdma_tx_descs(dd, tx);\n\tif (rval) {\n\t\t__sdma_txclean(dd, tx);\n\t\treturn rval;\n\t}\n\n\t/* If coalesce buffer is allocated, copy data into it */\n\tif (tx->coalesce_buf) {\n\t\tif (type == SDMA_MAP_NONE) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (type == SDMA_MAP_PAGE) {\n\t\t\tkvaddr = kmap(page);\n\t\t\tkvaddr += offset;\n\t\t} else if (WARN_ON(!kvaddr)) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmemcpy(tx->coalesce_buf + tx->coalesce_idx, kvaddr, len);\n\t\ttx->coalesce_idx += len;\n\t\tif (type == SDMA_MAP_PAGE)\n\t\t\tkunmap(page);\n\n\t\t/* If there is more data, return */\n\t\tif (tx->tlen - tx->coalesce_idx)\n\t\t\treturn 0;\n\n\t\t/* Whole packet is received; add any padding */\n\t\tpad_len = tx->packet_len & (sizeof(u32) - 1);\n\t\tif (pad_len) {\n\t\t\tpad_len = sizeof(u32) - pad_len;\n\t\t\tmemset(tx->coalesce_buf + tx->coalesce_idx, 0, pad_len);\n\t\t\t/* padding is taken care of for coalescing case */\n\t\t\ttx->packet_len += pad_len;\n\t\t\ttx->tlen += pad_len;\n\t\t}\n\n\t\t/* dma map the coalesce buffer */\n\t\taddr = dma_map_single(&dd->pcidev->dev,\n\t\t\t\t      tx->coalesce_buf,\n\t\t\t\t      tx->tlen,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\t\tif (unlikely(dma_mapping_error(&dd->pcidev->dev, addr))) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\t/* Add descriptor for coalesce buffer */\n\t\ttx->desc_limit = MAX_DESC;\n\t\treturn _sdma_txadd_daddr(dd, SDMA_MAP_SINGLE, tx,\n\t\t\t\t\t addr, tx->tlen);\n\t}\n\n\treturn 1;\n}\n\n/* Update sdes when the lmc changes */\nvoid sdma_update_lmc(struct hfi1_devdata *dd, u64 mask, u32 lid)\n{\n\tstruct sdma_engine *sde;\n\tint i;\n\tu64 sreg;\n\n\tsreg = ((mask & SD(CHECK_SLID_MASK_MASK)) <<\n\t\tSD(CHECK_SLID_MASK_SHIFT)) |\n\t\t(((lid & mask) & SD(CHECK_SLID_VALUE_MASK)) <<\n\t\tSD(CHECK_SLID_VALUE_SHIFT));\n\n\tfor (i = 0; i < dd->num_sdma; i++) {\n\t\thfi1_cdbg(LINKVERB, \"SendDmaEngine[%d].SLID_CHECK = 0x%x\",\n\t\t\t  i, (u32)sreg);\n\t\tsde = &dd->per_sdma[i];\n\t\twrite_sde_csr(sde, SD(CHECK_SLID), sreg);\n\t}\n}\n\n/* tx not dword sized - pad */\nint _pad_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx)\n{\n\tint rval = 0;\n\n\ttx->num_desc++;\n\tif ((unlikely(tx->num_desc == tx->desc_limit))) {\n\t\trval = _extend_sdma_tx_descs(dd, tx);\n\t\tif (rval) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn rval;\n\t\t}\n\t}\n\t/* finish the one just added */\n\tmake_tx_sdma_desc(\n\t\ttx,\n\t\tSDMA_MAP_NONE,\n\t\tdd->sdma_pad_phys,\n\t\tsizeof(u32) - (tx->packet_len & (sizeof(u32) - 1)));\n\t_sdma_close_tx(dd, tx);\n\treturn rval;\n}\n\n/*\n * Add ahg to the sdma_txreq\n *\n * The logic will consume up to 3\n * descriptors at the beginning of\n * sdma_txreq.\n */\nvoid _sdma_txreq_ahgadd(\n\tstruct sdma_txreq *tx,\n\tu8 num_ahg,\n\tu8 ahg_entry,\n\tu32 *ahg,\n\tu8 ahg_hlen)\n{\n\tu32 i, shift = 0, desc = 0;\n\tu8 mode;\n\n\tWARN_ON_ONCE(num_ahg > 9 || (ahg_hlen & 3) || ahg_hlen == 4);\n\t/* compute mode */\n\tif (num_ahg == 1)\n\t\tmode = SDMA_AHG_APPLY_UPDATE1;\n\telse if (num_ahg <= 5)\n\t\tmode = SDMA_AHG_APPLY_UPDATE2;\n\telse\n\t\tmode = SDMA_AHG_APPLY_UPDATE3;\n\ttx->num_desc++;\n\t/* initialize to consumed descriptors to zero */\n\tswitch (mode) {\n\tcase SDMA_AHG_APPLY_UPDATE3:\n\t\ttx->num_desc++;\n\t\ttx->descs[2].qw[0] = 0;\n\t\ttx->descs[2].qw[1] = 0;\n\t\t/* FALLTHROUGH */\n\tcase SDMA_AHG_APPLY_UPDATE2:\n\t\ttx->num_desc++;\n\t\ttx->descs[1].qw[0] = 0;\n\t\ttx->descs[1].qw[1] = 0;\n\t\tbreak;\n\t}\n\tahg_hlen >>= 2;\n\ttx->descs[0].qw[1] |=\n\t\t(((u64)ahg_entry & SDMA_DESC1_HEADER_INDEX_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_INDEX_SHIFT) |\n\t\t(((u64)ahg_hlen & SDMA_DESC1_HEADER_DWS_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_DWS_SHIFT) |\n\t\t(((u64)mode & SDMA_DESC1_HEADER_MODE_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_MODE_SHIFT) |\n\t\t(((u64)ahg[0] & SDMA_DESC1_HEADER_UPDATE1_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_UPDATE1_SHIFT);\n\tfor (i = 0; i < (num_ahg - 1); i++) {\n\t\tif (!shift && !(i & 2))\n\t\t\tdesc++;\n\t\ttx->descs[desc].qw[!!(i & 2)] |=\n\t\t\t(((u64)ahg[i + 1])\n\t\t\t\t<< shift);\n\t\tshift = (shift + 32) & 63;\n\t}\n}\n\n/**\n * sdma_ahg_alloc - allocate an AHG entry\n * @sde: engine to allocate from\n *\n * Return:\n * 0-31 when successful, -EOPNOTSUPP if AHG is not enabled,\n * -ENOSPC if an entry is not available\n */\nint sdma_ahg_alloc(struct sdma_engine *sde)\n{\n\tint nr;\n\tint oldbit;\n\n\tif (!sde) {\n\t\ttrace_hfi1_ahg_allocate(sde, -EINVAL);\n\t\treturn -EINVAL;\n\t}\n\twhile (1) {\n\t\tnr = ffz(READ_ONCE(sde->ahg_bits));\n\t\tif (nr > 31) {\n\t\t\ttrace_hfi1_ahg_allocate(sde, -ENOSPC);\n\t\t\treturn -ENOSPC;\n\t\t}\n\t\toldbit = test_and_set_bit(nr, &sde->ahg_bits);\n\t\tif (!oldbit)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\ttrace_hfi1_ahg_allocate(sde, nr);\n\treturn nr;\n}\n\n/**\n * sdma_ahg_free - free an AHG entry\n * @sde: engine to return AHG entry\n * @ahg_index: index to free\n *\n * This routine frees the indicate AHG entry.\n */\nvoid sdma_ahg_free(struct sdma_engine *sde, int ahg_index)\n{\n\tif (!sde)\n\t\treturn;\n\ttrace_hfi1_ahg_deallocate(sde, ahg_index);\n\tif (ahg_index < 0 || ahg_index > 31)\n\t\treturn;\n\tclear_bit(ahg_index, &sde->ahg_bits);\n}\n\n/*\n * SPC freeze handling for SDMA engines.  Called when the driver knows\n * the SPC is going into a freeze but before the freeze is fully\n * settled.  Generally an error interrupt.\n *\n * This event will pull the engine out of running so no more entries can be\n * added to the engine's queue.\n */\nvoid sdma_freeze_notify(struct hfi1_devdata *dd, int link_down)\n{\n\tint i;\n\tenum sdma_events event = link_down ? sdma_event_e85_link_down :\n\t\t\t\t\t     sdma_event_e80_hw_freeze;\n\n\t/* set up the wait but do not wait here */\n\tatomic_set(&dd->sdma_unfreeze_count, dd->num_sdma);\n\n\t/* tell all engines to stop running and wait */\n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i], event);\n\n\t/* sdma_freeze() will wait for all engines to have stopped */\n}\n\n/*\n * SPC freeze handling for SDMA engines.  Called when the driver knows\n * the SPC is fully frozen.\n */\nvoid sdma_freeze(struct hfi1_devdata *dd)\n{\n\tint i;\n\tint ret;\n\n\t/*\n\t * Make sure all engines have moved out of the running state before\n\t * continuing.\n\t */\n\tret = wait_event_interruptible(dd->sdma_unfreeze_wq,\n\t\t\t\t       atomic_read(&dd->sdma_unfreeze_count) <=\n\t\t\t\t       0);\n\t/* interrupted or count is negative, then unloading - just exit */\n\tif (ret || atomic_read(&dd->sdma_unfreeze_count) < 0)\n\t\treturn;\n\n\t/* set up the count for the next wait */\n\tatomic_set(&dd->sdma_unfreeze_count, dd->num_sdma);\n\n\t/* tell all engines that the SPC is frozen, they can start cleaning */\n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i], sdma_event_e81_hw_frozen);\n\n\t/*\n\t * Wait for everyone to finish software clean before exiting.  The\n\t * software clean will read engine CSRs, so must be completed before\n\t * the next step, which will clear the engine CSRs.\n\t */\n\t(void)wait_event_interruptible(dd->sdma_unfreeze_wq,\n\t\t\t\tatomic_read(&dd->sdma_unfreeze_count) <= 0);\n\t/* no need to check results - done no matter what */\n}\n\n/*\n * SPC freeze handling for the SDMA engines.  Called after the SPC is unfrozen.\n *\n * The SPC freeze acts like a SDMA halt and a hardware clean combined.  All\n * that is left is a software clean.  We could do it after the SPC is fully\n * frozen, but then we'd have to add another state to wait for the unfreeze.\n * Instead, just defer the software clean until the unfreeze step.\n */\nvoid sdma_unfreeze(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t/* tell all engines start freeze clean up */\n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i],\n\t\t\t\t   sdma_event_e82_hw_unfreeze);\n}\n\n/**\n * _sdma_engine_progress_schedule() - schedule progress on engine\n * @sde: sdma_engine to schedule progress\n *\n */\nvoid _sdma_engine_progress_schedule(\n\tstruct sdma_engine *sde)\n{\n\ttrace_hfi1_sdma_engine_progress(sde, sde->progress_mask);\n\t/* assume we have selected a good cpu */\n\twrite_csr(sde->dd,\n\t\t  CCE_INT_FORCE + (8 * (IS_SDMA_START / 64)),\n\t\t  sde->progress_mask);\n}\n"], "fixing_code": ["/*\n * Copyright(c) 2015 - 2018 Intel Corporation.\n *\n * This file is provided under a dual BSD/GPLv2 license.  When using or\n * redistributing this file, you may do so under either license.\n *\n * GPL LICENSE SUMMARY\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of version 2 of the GNU General Public License as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * BSD LICENSE\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *  - Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n *  - Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in\n *    the documentation and/or other materials provided with the\n *    distribution.\n *  - Neither the name of Intel Corporation nor the names of its\n *    contributors may be used to endorse or promote products derived\n *    from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n */\n\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/netdevice.h>\n#include <linux/moduleparam.h>\n#include <linux/bitops.h>\n#include <linux/timer.h>\n#include <linux/vmalloc.h>\n#include <linux/highmem.h>\n\n#include \"hfi.h\"\n#include \"common.h\"\n#include \"qp.h\"\n#include \"sdma.h\"\n#include \"iowait.h\"\n#include \"trace.h\"\n\n/* must be a power of 2 >= 64 <= 32768 */\n#define SDMA_DESCQ_CNT 2048\n#define SDMA_DESC_INTR 64\n#define INVALID_TAIL 0xffff\n\nstatic uint sdma_descq_cnt = SDMA_DESCQ_CNT;\nmodule_param(sdma_descq_cnt, uint, S_IRUGO);\nMODULE_PARM_DESC(sdma_descq_cnt, \"Number of SDMA descq entries\");\n\nstatic uint sdma_idle_cnt = 250;\nmodule_param(sdma_idle_cnt, uint, S_IRUGO);\nMODULE_PARM_DESC(sdma_idle_cnt, \"sdma interrupt idle delay (ns,default 250)\");\n\nuint mod_num_sdma;\nmodule_param_named(num_sdma, mod_num_sdma, uint, S_IRUGO);\nMODULE_PARM_DESC(num_sdma, \"Set max number SDMA engines to use\");\n\nstatic uint sdma_desct_intr = SDMA_DESC_INTR;\nmodule_param_named(desct_intr, sdma_desct_intr, uint, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(desct_intr, \"Number of SDMA descriptor before interrupt\");\n\n#define SDMA_WAIT_BATCH_SIZE 20\n/* max wait time for a SDMA engine to indicate it has halted */\n#define SDMA_ERR_HALT_TIMEOUT 10 /* ms */\n/* all SDMA engine errors that cause a halt */\n\n#define SD(name) SEND_DMA_##name\n#define ALL_SDMA_ENG_HALT_ERRS \\\n\t(SD(ENG_ERR_STATUS_SDMA_WRONG_DW_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_GEN_MISMATCH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TOO_LONG_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TAIL_OUT_OF_BOUNDS_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_FIRST_DESC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_MEM_READ_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HALT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_LENGTH_MISMATCH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_PACKET_DESC_OVERFLOW_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_SELECT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_ADDRESS_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_LENGTH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TIMEOUT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_DESC_TABLE_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_ASSEMBLY_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_PACKET_TRACKING_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_STORAGE_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_SMASK))\n\n/* sdma_sendctrl operations */\n#define SDMA_SENDCTRL_OP_ENABLE    BIT(0)\n#define SDMA_SENDCTRL_OP_INTENABLE BIT(1)\n#define SDMA_SENDCTRL_OP_HALT      BIT(2)\n#define SDMA_SENDCTRL_OP_CLEANUP   BIT(3)\n\n/* handle long defines */\n#define SDMA_EGRESS_PACKET_OCCUPANCY_SMASK \\\nSEND_EGRESS_SEND_DMA_STATUS_SDMA_EGRESS_PACKET_OCCUPANCY_SMASK\n#define SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT \\\nSEND_EGRESS_SEND_DMA_STATUS_SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT\n\nstatic const char * const sdma_state_names[] = {\n\t[sdma_state_s00_hw_down]                = \"s00_HwDown\",\n\t[sdma_state_s10_hw_start_up_halt_wait]  = \"s10_HwStartUpHaltWait\",\n\t[sdma_state_s15_hw_start_up_clean_wait] = \"s15_HwStartUpCleanWait\",\n\t[sdma_state_s20_idle]                   = \"s20_Idle\",\n\t[sdma_state_s30_sw_clean_up_wait]       = \"s30_SwCleanUpWait\",\n\t[sdma_state_s40_hw_clean_up_wait]       = \"s40_HwCleanUpWait\",\n\t[sdma_state_s50_hw_halt_wait]           = \"s50_HwHaltWait\",\n\t[sdma_state_s60_idle_halt_wait]         = \"s60_IdleHaltWait\",\n\t[sdma_state_s80_hw_freeze]\t\t= \"s80_HwFreeze\",\n\t[sdma_state_s82_freeze_sw_clean]\t= \"s82_FreezeSwClean\",\n\t[sdma_state_s99_running]                = \"s99_Running\",\n};\n\n#ifdef CONFIG_SDMA_VERBOSITY\nstatic const char * const sdma_event_names[] = {\n\t[sdma_event_e00_go_hw_down]   = \"e00_GoHwDown\",\n\t[sdma_event_e10_go_hw_start]  = \"e10_GoHwStart\",\n\t[sdma_event_e15_hw_halt_done] = \"e15_HwHaltDone\",\n\t[sdma_event_e25_hw_clean_up_done] = \"e25_HwCleanUpDone\",\n\t[sdma_event_e30_go_running]   = \"e30_GoRunning\",\n\t[sdma_event_e40_sw_cleaned]   = \"e40_SwCleaned\",\n\t[sdma_event_e50_hw_cleaned]   = \"e50_HwCleaned\",\n\t[sdma_event_e60_hw_halted]    = \"e60_HwHalted\",\n\t[sdma_event_e70_go_idle]      = \"e70_GoIdle\",\n\t[sdma_event_e80_hw_freeze]    = \"e80_HwFreeze\",\n\t[sdma_event_e81_hw_frozen]    = \"e81_HwFrozen\",\n\t[sdma_event_e82_hw_unfreeze]  = \"e82_HwUnfreeze\",\n\t[sdma_event_e85_link_down]    = \"e85_LinkDown\",\n\t[sdma_event_e90_sw_halted]    = \"e90_SwHalted\",\n};\n#endif\n\nstatic const struct sdma_set_state_action sdma_action_table[] = {\n\t[sdma_state_s00_hw_down] = {\n\t\t.go_s99_running_tofalse = 1,\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s10_hw_start_up_halt_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 1,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s15_hw_start_up_clean_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 1,\n\t},\n\t[sdma_state_s20_idle] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s30_sw_clean_up_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s40_hw_clean_up_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 1,\n\t},\n\t[sdma_state_s50_hw_halt_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s60_idle_halt_wait] = {\n\t\t.go_s99_running_tofalse = 1,\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 1,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s80_hw_freeze] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s82_freeze_sw_clean] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s99_running] = {\n\t\t.op_enable = 1,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t\t.go_s99_running_totrue = 1,\n\t},\n};\n\n#define SDMA_TAIL_UPDATE_THRESH 0x1F\n\n/* declare all statics here rather than keep sorting */\nstatic void sdma_complete(struct kref *);\nstatic void sdma_finalput(struct sdma_state *);\nstatic void sdma_get(struct sdma_state *);\nstatic void sdma_hw_clean_up_task(unsigned long);\nstatic void sdma_put(struct sdma_state *);\nstatic void sdma_set_state(struct sdma_engine *, enum sdma_states);\nstatic void sdma_start_hw_clean_up(struct sdma_engine *);\nstatic void sdma_sw_clean_up_task(unsigned long);\nstatic void sdma_sendctrl(struct sdma_engine *, unsigned);\nstatic void init_sdma_regs(struct sdma_engine *, u32, uint);\nstatic void sdma_process_event(\n\tstruct sdma_engine *sde,\n\tenum sdma_events event);\nstatic void __sdma_process_event(\n\tstruct sdma_engine *sde,\n\tenum sdma_events event);\nstatic void dump_sdma_state(struct sdma_engine *sde);\nstatic void sdma_make_progress(struct sdma_engine *sde, u64 status);\nstatic void sdma_desc_avail(struct sdma_engine *sde, uint avail);\nstatic void sdma_flush_descq(struct sdma_engine *sde);\n\n/**\n * sdma_state_name() - return state string from enum\n * @state: state\n */\nstatic const char *sdma_state_name(enum sdma_states state)\n{\n\treturn sdma_state_names[state];\n}\n\nstatic void sdma_get(struct sdma_state *ss)\n{\n\tkref_get(&ss->kref);\n}\n\nstatic void sdma_complete(struct kref *kref)\n{\n\tstruct sdma_state *ss =\n\t\tcontainer_of(kref, struct sdma_state, kref);\n\n\tcomplete(&ss->comp);\n}\n\nstatic void sdma_put(struct sdma_state *ss)\n{\n\tkref_put(&ss->kref, sdma_complete);\n}\n\nstatic void sdma_finalput(struct sdma_state *ss)\n{\n\tsdma_put(ss);\n\twait_for_completion(&ss->comp);\n}\n\nstatic inline void write_sde_csr(\n\tstruct sdma_engine *sde,\n\tu32 offset0,\n\tu64 value)\n{\n\twrite_kctxt_csr(sde->dd, sde->this_idx, offset0, value);\n}\n\nstatic inline u64 read_sde_csr(\n\tstruct sdma_engine *sde,\n\tu32 offset0)\n{\n\treturn read_kctxt_csr(sde->dd, sde->this_idx, offset0);\n}\n\n/*\n * sdma_wait_for_packet_egress() - wait for the VL FIFO occupancy for\n * sdma engine 'sde' to drop to 0.\n */\nstatic void sdma_wait_for_packet_egress(struct sdma_engine *sde,\n\t\t\t\t\tint pause)\n{\n\tu64 off = 8 * sde->this_idx;\n\tstruct hfi1_devdata *dd = sde->dd;\n\tint lcnt = 0;\n\tu64 reg_prev;\n\tu64 reg = 0;\n\n\twhile (1) {\n\t\treg_prev = reg;\n\t\treg = read_csr(dd, off + SEND_EGRESS_SEND_DMA_STATUS);\n\n\t\treg &= SDMA_EGRESS_PACKET_OCCUPANCY_SMASK;\n\t\treg >>= SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT;\n\t\tif (reg == 0)\n\t\t\tbreak;\n\t\t/* counter is reest if accupancy count changes */\n\t\tif (reg != reg_prev)\n\t\t\tlcnt = 0;\n\t\tif (lcnt++ > 500) {\n\t\t\t/* timed out - bounce the link */\n\t\t\tdd_dev_err(dd, \"%s: engine %u timeout waiting for packets to egress, remaining count %u, bouncing link\\n\",\n\t\t\t\t   __func__, sde->this_idx, (u32)reg);\n\t\t\tqueue_work(dd->pport->link_wq,\n\t\t\t\t   &dd->pport->link_bounce_work);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n}\n\n/*\n * sdma_wait() - wait for packet egress to complete for all SDMA engines,\n * and pause for credit return.\n */\nvoid sdma_wait(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\tfor (i = 0; i < dd->num_sdma; i++) {\n\t\tstruct sdma_engine *sde = &dd->per_sdma[i];\n\n\t\tsdma_wait_for_packet_egress(sde, 0);\n\t}\n}\n\nstatic inline void sdma_set_desc_cnt(struct sdma_engine *sde, unsigned cnt)\n{\n\tu64 reg;\n\n\tif (!(sde->dd->flags & HFI1_HAS_SDMA_TIMEOUT))\n\t\treturn;\n\treg = cnt;\n\treg &= SD(DESC_CNT_CNT_MASK);\n\treg <<= SD(DESC_CNT_CNT_SHIFT);\n\twrite_sde_csr(sde, SD(DESC_CNT), reg);\n}\n\nstatic inline void complete_tx(struct sdma_engine *sde,\n\t\t\t       struct sdma_txreq *tx,\n\t\t\t       int res)\n{\n\t/* protect against complete modifying */\n\tstruct iowait *wait = tx->wait;\n\tcallback_t complete = tx->complete;\n\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttrace_hfi1_sdma_out_sn(sde, tx->sn);\n\tif (WARN_ON_ONCE(sde->head_sn != tx->sn))\n\t\tdd_dev_err(sde->dd, \"expected %llu got %llu\\n\",\n\t\t\t   sde->head_sn, tx->sn);\n\tsde->head_sn++;\n#endif\n\t__sdma_txclean(sde->dd, tx);\n\tif (complete)\n\t\t(*complete)(tx, res);\n\tif (iowait_sdma_dec(wait))\n\t\tiowait_drain_wakeup(wait);\n}\n\n/*\n * Complete all the sdma requests with a SDMA_TXREQ_S_ABORTED status\n *\n * Depending on timing there can be txreqs in two places:\n * - in the descq ring\n * - in the flush list\n *\n * To avoid ordering issues the descq ring needs to be flushed\n * first followed by the flush list.\n *\n * This routine is called from two places\n * - From a work queue item\n * - Directly from the state machine just before setting the\n *   state to running\n *\n * Must be called with head_lock held\n *\n */\nstatic void sdma_flush(struct sdma_engine *sde)\n{\n\tstruct sdma_txreq *txp, *txp_next;\n\tLIST_HEAD(flushlist);\n\tunsigned long flags;\n\tuint seq;\n\n\t/* flush from head to tail */\n\tsdma_flush_descq(sde);\n\tspin_lock_irqsave(&sde->flushlist_lock, flags);\n\t/* copy flush list */\n\tlist_splice_init(&sde->flushlist, &flushlist);\n\tspin_unlock_irqrestore(&sde->flushlist_lock, flags);\n\t/* flush from flush list */\n\tlist_for_each_entry_safe(txp, txp_next, &flushlist, list)\n\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_ABORTED);\n\t/* wakeup QPs orphaned on the dmawait list */\n\tdo {\n\t\tstruct iowait *w, *nw;\n\n\t\tseq = read_seqbegin(&sde->waitlock);\n\t\tif (!list_empty(&sde->dmawait)) {\n\t\t\twrite_seqlock(&sde->waitlock);\n\t\t\tlist_for_each_entry_safe(w, nw, &sde->dmawait, list) {\n\t\t\t\tif (w->wakeup) {\n\t\t\t\t\tw->wakeup(w, SDMA_AVAIL_REASON);\n\t\t\t\t\tlist_del_init(&w->list);\n\t\t\t\t}\n\t\t\t}\n\t\t\twrite_sequnlock(&sde->waitlock);\n\t\t}\n\t} while (read_seqretry(&sde->waitlock, seq));\n}\n\n/*\n * Fields a work request for flushing the descq ring\n * and the flush list\n *\n * If the engine has been brought to running during\n * the scheduling delay, the flush is ignored, assuming\n * that the process of bringing the engine to running\n * would have done this flush prior to going to running.\n *\n */\nstatic void sdma_field_flush(struct work_struct *work)\n{\n\tunsigned long flags;\n\tstruct sdma_engine *sde =\n\t\tcontainer_of(work, struct sdma_engine, flush_worker);\n\n\twrite_seqlock_irqsave(&sde->head_lock, flags);\n\tif (!__sdma_running(sde))\n\t\tsdma_flush(sde);\n\twrite_sequnlock_irqrestore(&sde->head_lock, flags);\n}\n\nstatic void sdma_err_halt_wait(struct work_struct *work)\n{\n\tstruct sdma_engine *sde = container_of(work, struct sdma_engine,\n\t\t\t\t\t\terr_halt_worker);\n\tu64 statuscsr;\n\tunsigned long timeout;\n\n\ttimeout = jiffies + msecs_to_jiffies(SDMA_ERR_HALT_TIMEOUT);\n\twhile (1) {\n\t\tstatuscsr = read_sde_csr(sde, SD(STATUS));\n\t\tstatuscsr &= SD(STATUS_ENG_HALTED_SMASK);\n\t\tif (statuscsr)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(sde->dd,\n\t\t\t\t   \"SDMA engine %d - timeout waiting for engine to halt\\n\",\n\t\t\t\t   sde->this_idx);\n\t\t\t/*\n\t\t\t * Continue anyway.  This could happen if there was\n\t\t\t * an uncorrectable error in the wrong spot.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tusleep_range(80, 120);\n\t}\n\n\tsdma_process_event(sde, sdma_event_e15_hw_halt_done);\n}\n\nstatic void sdma_err_progress_check_schedule(struct sdma_engine *sde)\n{\n\tif (!is_bx(sde->dd) && HFI1_CAP_IS_KSET(SDMA_AHG)) {\n\t\tunsigned index;\n\t\tstruct hfi1_devdata *dd = sde->dd;\n\n\t\tfor (index = 0; index < dd->num_sdma; index++) {\n\t\t\tstruct sdma_engine *curr_sdma = &dd->per_sdma[index];\n\n\t\t\tif (curr_sdma != sde)\n\t\t\t\tcurr_sdma->progress_check_head =\n\t\t\t\t\t\t\tcurr_sdma->descq_head;\n\t\t}\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA engine %d - check scheduled\\n\",\n\t\t\t\tsde->this_idx);\n\t\tmod_timer(&sde->err_progress_check_timer, jiffies + 10);\n\t}\n}\n\nstatic void sdma_err_progress_check(struct timer_list *t)\n{\n\tunsigned index;\n\tstruct sdma_engine *sde = from_timer(sde, t, err_progress_check_timer);\n\n\tdd_dev_err(sde->dd, \"SDE progress check event\\n\");\n\tfor (index = 0; index < sde->dd->num_sdma; index++) {\n\t\tstruct sdma_engine *curr_sde = &sde->dd->per_sdma[index];\n\t\tunsigned long flags;\n\n\t\t/* check progress on each engine except the current one */\n\t\tif (curr_sde == sde)\n\t\t\tcontinue;\n\t\t/*\n\t\t * We must lock interrupts when acquiring sde->lock,\n\t\t * to avoid a deadlock if interrupt triggers and spins on\n\t\t * the same lock on same CPU\n\t\t */\n\t\tspin_lock_irqsave(&curr_sde->tail_lock, flags);\n\t\twrite_seqlock(&curr_sde->head_lock);\n\n\t\t/* skip non-running queues */\n\t\tif (curr_sde->state.current_state != sdma_state_s99_running) {\n\t\t\twrite_sequnlock(&curr_sde->head_lock);\n\t\t\tspin_unlock_irqrestore(&curr_sde->tail_lock, flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif ((curr_sde->descq_head != curr_sde->descq_tail) &&\n\t\t    (curr_sde->descq_head ==\n\t\t\t\tcurr_sde->progress_check_head))\n\t\t\t__sdma_process_event(curr_sde,\n\t\t\t\t\t     sdma_event_e90_sw_halted);\n\t\twrite_sequnlock(&curr_sde->head_lock);\n\t\tspin_unlock_irqrestore(&curr_sde->tail_lock, flags);\n\t}\n\tschedule_work(&sde->err_halt_worker);\n}\n\nstatic void sdma_hw_clean_up_task(unsigned long opaque)\n{\n\tstruct sdma_engine *sde = (struct sdma_engine *)opaque;\n\tu64 statuscsr;\n\n\twhile (1) {\n#ifdef CONFIG_SDMA_VERBOSITY\n\t\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__,\n\t\t\t__func__);\n#endif\n\t\tstatuscsr = read_sde_csr(sde, SD(STATUS));\n\t\tstatuscsr &= SD(STATUS_ENG_CLEANED_UP_SMASK);\n\t\tif (statuscsr)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\tsdma_process_event(sde, sdma_event_e25_hw_clean_up_done);\n}\n\nstatic inline struct sdma_txreq *get_txhead(struct sdma_engine *sde)\n{\n\treturn sde->tx_ring[sde->tx_head & sde->sdma_mask];\n}\n\n/*\n * flush ring for recovery\n */\nstatic void sdma_flush_descq(struct sdma_engine *sde)\n{\n\tu16 head, tail;\n\tint progress = 0;\n\tstruct sdma_txreq *txp = get_txhead(sde);\n\n\t/* The reason for some of the complexity of this code is that\n\t * not all descriptors have corresponding txps.  So, we have to\n\t * be able to skip over descs until we wander into the range of\n\t * the next txp on the list.\n\t */\n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = sde->descq_tail & sde->sdma_mask;\n\twhile (head != tail) {\n\t\t/* advance head, wrap if needed */\n\t\thead = ++sde->descq_head & sde->sdma_mask;\n\t\t/* if now past this txp's descs, do the callback */\n\t\tif (txp && txp->next_descq_idx == head) {\n\t\t\t/* remove from list */\n\t\t\tsde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;\n\t\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_ABORTED);\n\t\t\ttrace_hfi1_sdma_progress(sde, head, tail, txp);\n\t\t\ttxp = get_txhead(sde);\n\t\t}\n\t\tprogress++;\n\t}\n\tif (progress)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n}\n\nstatic void sdma_sw_clean_up_task(unsigned long opaque)\n{\n\tstruct sdma_engine *sde = (struct sdma_engine *)opaque;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\n\t/*\n\t * At this point, the following should always be true:\n\t * - We are halted, so no more descriptors are getting retired.\n\t * - We are not running, so no one is submitting new work.\n\t * - Only we can send the e40_sw_cleaned, so we can't start\n\t *   running again until we say so.  So, the active list and\n\t *   descq are ours to play with.\n\t */\n\n\t/*\n\t * In the error clean up sequence, software clean must be called\n\t * before the hardware clean so we can use the hardware head in\n\t * the progress routine.  A hardware clean or SPC unfreeze will\n\t * reset the hardware head.\n\t *\n\t * Process all retired requests. The progress routine will use the\n\t * latest physical hardware head - we are not running so speed does\n\t * not matter.\n\t */\n\tsdma_make_progress(sde, 0);\n\n\tsdma_flush(sde);\n\n\t/*\n\t * Reset our notion of head and tail.\n\t * Note that the HW registers have been reset via an earlier\n\t * clean up.\n\t */\n\tsde->descq_tail = 0;\n\tsde->descq_head = 0;\n\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t*sde->head_dma = 0;\n\n\t__sdma_process_event(sde, sdma_event_e40_sw_cleaned);\n\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void sdma_sw_tear_down(struct sdma_engine *sde)\n{\n\tstruct sdma_state *ss = &sde->state;\n\n\t/* Releasing this reference means the state machine has stopped. */\n\tsdma_put(ss);\n\n\t/* stop waiting for all unfreeze events to complete */\n\tatomic_set(&sde->dd->sdma_unfreeze_count, -1);\n\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n}\n\nstatic void sdma_start_hw_clean_up(struct sdma_engine *sde)\n{\n\ttasklet_hi_schedule(&sde->sdma_hw_clean_up_task);\n}\n\nstatic void sdma_set_state(struct sdma_engine *sde,\n\t\t\t   enum sdma_states next_state)\n{\n\tstruct sdma_state *ss = &sde->state;\n\tconst struct sdma_set_state_action *action = sdma_action_table;\n\tunsigned op = 0;\n\n\ttrace_hfi1_sdma_state(\n\t\tsde,\n\t\tsdma_state_names[ss->current_state],\n\t\tsdma_state_names[next_state]);\n\n\t/* debugging bookkeeping */\n\tss->previous_state = ss->current_state;\n\tss->previous_op = ss->current_op;\n\tss->current_state = next_state;\n\n\tif (ss->previous_state != sdma_state_s99_running &&\n\t    next_state == sdma_state_s99_running)\n\t\tsdma_flush(sde);\n\n\tif (action[next_state].op_enable)\n\t\top |= SDMA_SENDCTRL_OP_ENABLE;\n\n\tif (action[next_state].op_intenable)\n\t\top |= SDMA_SENDCTRL_OP_INTENABLE;\n\n\tif (action[next_state].op_halt)\n\t\top |= SDMA_SENDCTRL_OP_HALT;\n\n\tif (action[next_state].op_cleanup)\n\t\top |= SDMA_SENDCTRL_OP_CLEANUP;\n\n\tif (action[next_state].go_s99_running_tofalse)\n\t\tss->go_s99_running = 0;\n\n\tif (action[next_state].go_s99_running_totrue)\n\t\tss->go_s99_running = 1;\n\n\tss->current_op = op;\n\tsdma_sendctrl(sde, ss->current_op);\n}\n\n/**\n * sdma_get_descq_cnt() - called when device probed\n *\n * Return a validated descq count.\n *\n * This is currently only used in the verbs initialization to build the tx\n * list.\n *\n * This will probably be deleted in favor of a more scalable approach to\n * alloc tx's.\n *\n */\nu16 sdma_get_descq_cnt(void)\n{\n\tu16 count = sdma_descq_cnt;\n\n\tif (!count)\n\t\treturn SDMA_DESCQ_CNT;\n\t/* count must be a power of 2 greater than 64 and less than\n\t * 32768.   Otherwise return default.\n\t */\n\tif (!is_power_of_2(count))\n\t\treturn SDMA_DESCQ_CNT;\n\tif (count < 64 || count > 32768)\n\t\treturn SDMA_DESCQ_CNT;\n\treturn count;\n}\n\n/**\n * sdma_engine_get_vl() - return vl for a given sdma engine\n * @sde: sdma engine\n *\n * This function returns the vl mapped to a given engine, or an error if\n * the mapping can't be found. The mapping fields are protected by RCU.\n */\nint sdma_engine_get_vl(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tstruct sdma_vl_map *m;\n\tu8 vl;\n\n\tif (sde->this_idx >= TXE_NUM_SDMA_ENGINES)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tm = rcu_dereference(dd->sdma_map);\n\tif (unlikely(!m)) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\tvl = m->engine_to_vl[sde->this_idx];\n\trcu_read_unlock();\n\n\treturn vl;\n}\n\n/**\n * sdma_select_engine_vl() - select sdma engine\n * @dd: devdata\n * @selector: a spreading factor\n * @vl: this vl\n *\n *\n * This function returns an engine based on the selector and a vl.  The\n * mapping fields are protected by RCU.\n */\nstruct sdma_engine *sdma_select_engine_vl(\n\tstruct hfi1_devdata *dd,\n\tu32 selector,\n\tu8 vl)\n{\n\tstruct sdma_vl_map *m;\n\tstruct sdma_map_elem *e;\n\tstruct sdma_engine *rval;\n\n\t/* NOTE This should only happen if SC->VL changed after the initial\n\t *      checks on the QP/AH\n\t *      Default will return engine 0 below\n\t */\n\tif (vl >= num_vls) {\n\t\trval = NULL;\n\t\tgoto done;\n\t}\n\n\trcu_read_lock();\n\tm = rcu_dereference(dd->sdma_map);\n\tif (unlikely(!m)) {\n\t\trcu_read_unlock();\n\t\treturn &dd->per_sdma[0];\n\t}\n\te = m->map[vl & m->mask];\n\trval = e->sde[selector & e->mask];\n\trcu_read_unlock();\n\ndone:\n\trval =  !rval ? &dd->per_sdma[0] : rval;\n\ttrace_hfi1_sdma_engine_select(dd, selector, vl, rval->this_idx);\n\treturn rval;\n}\n\n/**\n * sdma_select_engine_sc() - select sdma engine\n * @dd: devdata\n * @selector: a spreading factor\n * @sc5: the 5 bit sc\n *\n *\n * This function returns an engine based on the selector and an sc.\n */\nstruct sdma_engine *sdma_select_engine_sc(\n\tstruct hfi1_devdata *dd,\n\tu32 selector,\n\tu8 sc5)\n{\n\tu8 vl = sc_to_vlt(dd, sc5);\n\n\treturn sdma_select_engine_vl(dd, selector, vl);\n}\n\nstruct sdma_rht_map_elem {\n\tu32 mask;\n\tu8 ctr;\n\tstruct sdma_engine *sde[0];\n};\n\nstruct sdma_rht_node {\n\tunsigned long cpu_id;\n\tstruct sdma_rht_map_elem *map[HFI1_MAX_VLS_SUPPORTED];\n\tstruct rhash_head node;\n};\n\n#define NR_CPUS_HINT 192\n\nstatic const struct rhashtable_params sdma_rht_params = {\n\t.nelem_hint = NR_CPUS_HINT,\n\t.head_offset = offsetof(struct sdma_rht_node, node),\n\t.key_offset = offsetof(struct sdma_rht_node, cpu_id),\n\t.key_len = FIELD_SIZEOF(struct sdma_rht_node, cpu_id),\n\t.max_size = NR_CPUS,\n\t.min_size = 8,\n\t.automatic_shrinking = true,\n};\n\n/*\n * sdma_select_user_engine() - select sdma engine based on user setup\n * @dd: devdata\n * @selector: a spreading factor\n * @vl: this vl\n *\n * This function returns an sdma engine for a user sdma request.\n * User defined sdma engine affinity setting is honored when applicable,\n * otherwise system default sdma engine mapping is used. To ensure correct\n * ordering, the mapping from <selector, vl> to sde must remain unchanged.\n */\nstruct sdma_engine *sdma_select_user_engine(struct hfi1_devdata *dd,\n\t\t\t\t\t    u32 selector, u8 vl)\n{\n\tstruct sdma_rht_node *rht_node;\n\tstruct sdma_engine *sde = NULL;\n\tunsigned long cpu_id;\n\n\t/*\n\t * To ensure that always the same sdma engine(s) will be\n\t * selected make sure the process is pinned to this CPU only.\n\t */\n\tif (current->nr_cpus_allowed != 1)\n\t\tgoto out;\n\n\tcpu_id = smp_processor_id();\n\trcu_read_lock();\n\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpu_id,\n\t\t\t\t\t  sdma_rht_params);\n\n\tif (rht_node && rht_node->map[vl]) {\n\t\tstruct sdma_rht_map_elem *map = rht_node->map[vl];\n\n\t\tsde = map->sde[selector & map->mask];\n\t}\n\trcu_read_unlock();\n\n\tif (sde)\n\t\treturn sde;\n\nout:\n\treturn sdma_select_engine_vl(dd, selector, vl);\n}\n\nstatic void sdma_populate_sde_map(struct sdma_rht_map_elem *map)\n{\n\tint i;\n\n\tfor (i = 0; i < roundup_pow_of_two(map->ctr ? : 1) - map->ctr; i++)\n\t\tmap->sde[map->ctr + i] = map->sde[i];\n}\n\nstatic void sdma_cleanup_sde_map(struct sdma_rht_map_elem *map,\n\t\t\t\t struct sdma_engine *sde)\n{\n\tunsigned int i, pow;\n\n\t/* only need to check the first ctr entries for a match */\n\tfor (i = 0; i < map->ctr; i++) {\n\t\tif (map->sde[i] == sde) {\n\t\t\tmemmove(&map->sde[i], &map->sde[i + 1],\n\t\t\t\t(map->ctr - i - 1) * sizeof(map->sde[0]));\n\t\t\tmap->ctr--;\n\t\t\tpow = roundup_pow_of_two(map->ctr ? : 1);\n\t\t\tmap->mask = pow - 1;\n\t\t\tsdma_populate_sde_map(map);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * Prevents concurrent reads and writes of the sdma engine cpu_mask\n */\nstatic DEFINE_MUTEX(process_to_sde_mutex);\n\nssize_t sdma_set_cpu_to_sde_map(struct sdma_engine *sde, const char *buf,\n\t\t\t\tsize_t count)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tcpumask_var_t mask, new_mask;\n\tunsigned long cpu;\n\tint ret, vl, sz;\n\tstruct sdma_rht_node *rht_node;\n\n\tvl = sdma_engine_get_vl(sde);\n\tif (unlikely(vl < 0 || vl >= ARRAY_SIZE(rht_node->map)))\n\t\treturn -EINVAL;\n\n\tret = zalloc_cpumask_var(&mask, GFP_KERNEL);\n\tif (!ret)\n\t\treturn -ENOMEM;\n\n\tret = zalloc_cpumask_var(&new_mask, GFP_KERNEL);\n\tif (!ret) {\n\t\tfree_cpumask_var(mask);\n\t\treturn -ENOMEM;\n\t}\n\tret = cpulist_parse(buf, mask);\n\tif (ret)\n\t\tgoto out_free;\n\n\tif (!cpumask_subset(mask, cpu_online_mask)) {\n\t\tdd_dev_warn(sde->dd, \"Invalid CPU mask\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tsz = sizeof(struct sdma_rht_map_elem) +\n\t\t\t(TXE_NUM_SDMA_ENGINES * sizeof(struct sdma_engine *));\n\n\tmutex_lock(&process_to_sde_mutex);\n\n\tfor_each_cpu(cpu, mask) {\n\t\t/* Check if we have this already mapped */\n\t\tif (cpumask_test_cpu(cpu, &sde->cpu_mask)) {\n\t\t\tcpumask_set_cpu(cpu, new_mask);\n\t\t\tcontinue;\n\t\t}\n\n\t\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpu,\n\t\t\t\t\t\t  sdma_rht_params);\n\t\tif (!rht_node) {\n\t\t\trht_node = kzalloc(sizeof(*rht_node), GFP_KERNEL);\n\t\t\tif (!rht_node) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\trht_node->map[vl] = kzalloc(sz, GFP_KERNEL);\n\t\t\tif (!rht_node->map[vl]) {\n\t\t\t\tkfree(rht_node);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\trht_node->cpu_id = cpu;\n\t\t\trht_node->map[vl]->mask = 0;\n\t\t\trht_node->map[vl]->ctr = 1;\n\t\t\trht_node->map[vl]->sde[0] = sde;\n\n\t\t\tret = rhashtable_insert_fast(dd->sdma_rht,\n\t\t\t\t\t\t     &rht_node->node,\n\t\t\t\t\t\t     sdma_rht_params);\n\t\t\tif (ret) {\n\t\t\t\tkfree(rht_node->map[vl]);\n\t\t\t\tkfree(rht_node);\n\t\t\t\tdd_dev_err(sde->dd, \"Failed to set process to sde affinity for cpu %lu\\n\",\n\t\t\t\t\t   cpu);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t} else {\n\t\t\tint ctr, pow;\n\n\t\t\t/* Add new user mappings */\n\t\t\tif (!rht_node->map[vl])\n\t\t\t\trht_node->map[vl] = kzalloc(sz, GFP_KERNEL);\n\n\t\t\tif (!rht_node->map[vl]) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\trht_node->map[vl]->ctr++;\n\t\t\tctr = rht_node->map[vl]->ctr;\n\t\t\trht_node->map[vl]->sde[ctr - 1] = sde;\n\t\t\tpow = roundup_pow_of_two(ctr);\n\t\t\trht_node->map[vl]->mask = pow - 1;\n\n\t\t\t/* Populate the sde map table */\n\t\t\tsdma_populate_sde_map(rht_node->map[vl]);\n\t\t}\n\t\tcpumask_set_cpu(cpu, new_mask);\n\t}\n\n\t/* Clean up old mappings */\n\tfor_each_cpu(cpu, cpu_online_mask) {\n\t\tstruct sdma_rht_node *rht_node;\n\n\t\t/* Don't cleanup sdes that are set in the new mask */\n\t\tif (cpumask_test_cpu(cpu, mask))\n\t\t\tcontinue;\n\n\t\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpu,\n\t\t\t\t\t\t  sdma_rht_params);\n\t\tif (rht_node) {\n\t\t\tbool empty = true;\n\t\t\tint i;\n\n\t\t\t/* Remove mappings for old sde */\n\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\t\t\tif (rht_node->map[i])\n\t\t\t\t\tsdma_cleanup_sde_map(rht_node->map[i],\n\t\t\t\t\t\t\t     sde);\n\n\t\t\t/* Free empty hash table entries */\n\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++) {\n\t\t\t\tif (!rht_node->map[i])\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (rht_node->map[i]->ctr) {\n\t\t\t\t\tempty = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (empty) {\n\t\t\t\tret = rhashtable_remove_fast(dd->sdma_rht,\n\t\t\t\t\t\t\t     &rht_node->node,\n\t\t\t\t\t\t\t     sdma_rht_params);\n\t\t\t\tWARN_ON(ret);\n\n\t\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\t\t\t\tkfree(rht_node->map[i]);\n\n\t\t\t\tkfree(rht_node);\n\t\t\t}\n\t\t}\n\t}\n\n\tcpumask_copy(&sde->cpu_mask, new_mask);\nout:\n\tmutex_unlock(&process_to_sde_mutex);\nout_free:\n\tfree_cpumask_var(mask);\n\tfree_cpumask_var(new_mask);\n\treturn ret ? : strnlen(buf, PAGE_SIZE);\n}\n\nssize_t sdma_get_cpu_to_sde_map(struct sdma_engine *sde, char *buf)\n{\n\tmutex_lock(&process_to_sde_mutex);\n\tif (cpumask_empty(&sde->cpu_mask))\n\t\tsnprintf(buf, PAGE_SIZE, \"%s\\n\", \"empty\");\n\telse\n\t\tcpumap_print_to_pagebuf(true, buf, &sde->cpu_mask);\n\tmutex_unlock(&process_to_sde_mutex);\n\treturn strnlen(buf, PAGE_SIZE);\n}\n\nstatic void sdma_rht_free(void *ptr, void *arg)\n{\n\tstruct sdma_rht_node *rht_node = ptr;\n\tint i;\n\n\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\tkfree(rht_node->map[i]);\n\n\tkfree(rht_node);\n}\n\n/**\n * sdma_seqfile_dump_cpu_list() - debugfs dump the cpu to sdma mappings\n * @s: seq file\n * @dd: hfi1_devdata\n * @cpuid: cpu id\n *\n * This routine dumps the process to sde mappings per cpu\n */\nvoid sdma_seqfile_dump_cpu_list(struct seq_file *s,\n\t\t\t\tstruct hfi1_devdata *dd,\n\t\t\t\tunsigned long cpuid)\n{\n\tstruct sdma_rht_node *rht_node;\n\tint i, j;\n\n\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpuid,\n\t\t\t\t\t  sdma_rht_params);\n\tif (!rht_node)\n\t\treturn;\n\n\tseq_printf(s, \"cpu%3lu: \", cpuid);\n\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++) {\n\t\tif (!rht_node->map[i] || !rht_node->map[i]->ctr)\n\t\t\tcontinue;\n\n\t\tseq_printf(s, \" vl%d: [\", i);\n\n\t\tfor (j = 0; j < rht_node->map[i]->ctr; j++) {\n\t\t\tif (!rht_node->map[i]->sde[j])\n\t\t\t\tcontinue;\n\n\t\t\tif (j > 0)\n\t\t\t\tseq_puts(s, \",\");\n\n\t\t\tseq_printf(s, \" sdma%2d\",\n\t\t\t\t   rht_node->map[i]->sde[j]->this_idx);\n\t\t}\n\t\tseq_puts(s, \" ]\");\n\t}\n\n\tseq_puts(s, \"\\n\");\n}\n\n/*\n * Free the indicated map struct\n */\nstatic void sdma_map_free(struct sdma_vl_map *m)\n{\n\tint i;\n\n\tfor (i = 0; m && i < m->actual_vls; i++)\n\t\tkfree(m->map[i]);\n\tkfree(m);\n}\n\n/*\n * Handle RCU callback\n */\nstatic void sdma_map_rcu_callback(struct rcu_head *list)\n{\n\tstruct sdma_vl_map *m = container_of(list, struct sdma_vl_map, list);\n\n\tsdma_map_free(m);\n}\n\n/**\n * sdma_map_init - called when # vls change\n * @dd: hfi1_devdata\n * @port: port number\n * @num_vls: number of vls\n * @vl_engines: per vl engine mapping (optional)\n *\n * This routine changes the mapping based on the number of vls.\n *\n * vl_engines is used to specify a non-uniform vl/engine loading. NULL\n * implies auto computing the loading and giving each VLs a uniform\n * distribution of engines per VL.\n *\n * The auto algorithm computes the sde_per_vl and the number of extra\n * engines.  Any extra engines are added from the last VL on down.\n *\n * rcu locking is used here to control access to the mapping fields.\n *\n * If either the num_vls or num_sdma are non-power of 2, the array sizes\n * in the struct sdma_vl_map and the struct sdma_map_elem are rounded\n * up to the next highest power of 2 and the first entry is reused\n * in a round robin fashion.\n *\n * If an error occurs the map change is not done and the mapping is\n * not changed.\n *\n */\nint sdma_map_init(struct hfi1_devdata *dd, u8 port, u8 num_vls, u8 *vl_engines)\n{\n\tint i, j;\n\tint extra, sde_per_vl;\n\tint engine = 0;\n\tu8 lvl_engines[OPA_MAX_VLS];\n\tstruct sdma_vl_map *oldmap, *newmap;\n\n\tif (!(dd->flags & HFI1_HAS_SEND_DMA))\n\t\treturn 0;\n\n\tif (!vl_engines) {\n\t\t/* truncate divide */\n\t\tsde_per_vl = dd->num_sdma / num_vls;\n\t\t/* extras */\n\t\textra = dd->num_sdma % num_vls;\n\t\tvl_engines = lvl_engines;\n\t\t/* add extras from last vl down */\n\t\tfor (i = num_vls - 1; i >= 0; i--, extra--)\n\t\t\tvl_engines[i] = sde_per_vl + (extra > 0 ? 1 : 0);\n\t}\n\t/* build new map */\n\tnewmap = kzalloc(\n\t\tsizeof(struct sdma_vl_map) +\n\t\t\troundup_pow_of_two(num_vls) *\n\t\t\tsizeof(struct sdma_map_elem *),\n\t\tGFP_KERNEL);\n\tif (!newmap)\n\t\tgoto bail;\n\tnewmap->actual_vls = num_vls;\n\tnewmap->vls = roundup_pow_of_two(num_vls);\n\tnewmap->mask = (1 << ilog2(newmap->vls)) - 1;\n\t/* initialize back-map */\n\tfor (i = 0; i < TXE_NUM_SDMA_ENGINES; i++)\n\t\tnewmap->engine_to_vl[i] = -1;\n\tfor (i = 0; i < newmap->vls; i++) {\n\t\t/* save for wrap around */\n\t\tint first_engine = engine;\n\n\t\tif (i < newmap->actual_vls) {\n\t\t\tint sz = roundup_pow_of_two(vl_engines[i]);\n\n\t\t\t/* only allocate once */\n\t\t\tnewmap->map[i] = kzalloc(\n\t\t\t\tsizeof(struct sdma_map_elem) +\n\t\t\t\t\tsz * sizeof(struct sdma_engine *),\n\t\t\t\tGFP_KERNEL);\n\t\t\tif (!newmap->map[i])\n\t\t\t\tgoto bail;\n\t\t\tnewmap->map[i]->mask = (1 << ilog2(sz)) - 1;\n\t\t\t/* assign engines */\n\t\t\tfor (j = 0; j < sz; j++) {\n\t\t\t\tnewmap->map[i]->sde[j] =\n\t\t\t\t\t&dd->per_sdma[engine];\n\t\t\t\tif (++engine >= first_engine + vl_engines[i])\n\t\t\t\t\t/* wrap back to first engine */\n\t\t\t\t\tengine = first_engine;\n\t\t\t}\n\t\t\t/* assign back-map */\n\t\t\tfor (j = 0; j < vl_engines[i]; j++)\n\t\t\t\tnewmap->engine_to_vl[first_engine + j] = i;\n\t\t} else {\n\t\t\t/* just re-use entry without allocating */\n\t\t\tnewmap->map[i] = newmap->map[i % num_vls];\n\t\t}\n\t\tengine = first_engine + vl_engines[i];\n\t}\n\t/* newmap in hand, save old map */\n\tspin_lock_irq(&dd->sde_map_lock);\n\toldmap = rcu_dereference_protected(dd->sdma_map,\n\t\t\t\t\t   lockdep_is_held(&dd->sde_map_lock));\n\n\t/* publish newmap */\n\trcu_assign_pointer(dd->sdma_map, newmap);\n\n\tspin_unlock_irq(&dd->sde_map_lock);\n\t/* success, free any old map after grace period */\n\tif (oldmap)\n\t\tcall_rcu(&oldmap->list, sdma_map_rcu_callback);\n\treturn 0;\nbail:\n\t/* free any partial allocation */\n\tsdma_map_free(newmap);\n\treturn -ENOMEM;\n}\n\n/**\n * sdma_clean()  Clean up allocated memory\n * @dd:          struct hfi1_devdata\n * @num_engines: num sdma engines\n *\n * This routine can be called regardless of the success of\n * sdma_init()\n */\nvoid sdma_clean(struct hfi1_devdata *dd, size_t num_engines)\n{\n\tsize_t i;\n\tstruct sdma_engine *sde;\n\n\tif (dd->sdma_pad_dma) {\n\t\tdma_free_coherent(&dd->pcidev->dev, 4,\n\t\t\t\t  (void *)dd->sdma_pad_dma,\n\t\t\t\t  dd->sdma_pad_phys);\n\t\tdd->sdma_pad_dma = NULL;\n\t\tdd->sdma_pad_phys = 0;\n\t}\n\tif (dd->sdma_heads_dma) {\n\t\tdma_free_coherent(&dd->pcidev->dev, dd->sdma_heads_size,\n\t\t\t\t  (void *)dd->sdma_heads_dma,\n\t\t\t\t  dd->sdma_heads_phys);\n\t\tdd->sdma_heads_dma = NULL;\n\t\tdd->sdma_heads_phys = 0;\n\t}\n\tfor (i = 0; dd->per_sdma && i < num_engines; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\n\t\tsde->head_dma = NULL;\n\t\tsde->head_phys = 0;\n\n\t\tif (sde->descq) {\n\t\t\tdma_free_coherent(\n\t\t\t\t&dd->pcidev->dev,\n\t\t\t\tsde->descq_cnt * sizeof(u64[2]),\n\t\t\t\tsde->descq,\n\t\t\t\tsde->descq_phys\n\t\t\t);\n\t\t\tsde->descq = NULL;\n\t\t\tsde->descq_phys = 0;\n\t\t}\n\t\tkvfree(sde->tx_ring);\n\t\tsde->tx_ring = NULL;\n\t}\n\tspin_lock_irq(&dd->sde_map_lock);\n\tsdma_map_free(rcu_access_pointer(dd->sdma_map));\n\tRCU_INIT_POINTER(dd->sdma_map, NULL);\n\tspin_unlock_irq(&dd->sde_map_lock);\n\tsynchronize_rcu();\n\tkfree(dd->per_sdma);\n\tdd->per_sdma = NULL;\n\n\tif (dd->sdma_rht) {\n\t\trhashtable_free_and_destroy(dd->sdma_rht, sdma_rht_free, NULL);\n\t\tkfree(dd->sdma_rht);\n\t\tdd->sdma_rht = NULL;\n\t}\n}\n\n/**\n * sdma_init() - called when device probed\n * @dd: hfi1_devdata\n * @port: port number (currently only zero)\n *\n * Initializes each sde and its csrs.\n * Interrupts are not required to be enabled.\n *\n * Returns:\n * 0 - success, -errno on failure\n */\nint sdma_init(struct hfi1_devdata *dd, u8 port)\n{\n\tunsigned this_idx;\n\tstruct sdma_engine *sde;\n\tstruct rhashtable *tmp_sdma_rht;\n\tu16 descq_cnt;\n\tvoid *curr_head;\n\tstruct hfi1_pportdata *ppd = dd->pport + port;\n\tu32 per_sdma_credits;\n\tuint idle_cnt = sdma_idle_cnt;\n\tsize_t num_engines = chip_sdma_engines(dd);\n\tint ret = -ENOMEM;\n\n\tif (!HFI1_CAP_IS_KSET(SDMA)) {\n\t\tHFI1_CAP_CLEAR(SDMA_AHG);\n\t\treturn 0;\n\t}\n\tif (mod_num_sdma &&\n\t    /* can't exceed chip support */\n\t    mod_num_sdma <= chip_sdma_engines(dd) &&\n\t    /* count must be >= vls */\n\t    mod_num_sdma >= num_vls)\n\t\tnum_engines = mod_num_sdma;\n\n\tdd_dev_info(dd, \"SDMA mod_num_sdma: %u\\n\", mod_num_sdma);\n\tdd_dev_info(dd, \"SDMA chip_sdma_engines: %u\\n\", chip_sdma_engines(dd));\n\tdd_dev_info(dd, \"SDMA chip_sdma_mem_size: %u\\n\",\n\t\t    chip_sdma_mem_size(dd));\n\n\tper_sdma_credits =\n\t\tchip_sdma_mem_size(dd) / (num_engines * SDMA_BLOCK_SIZE);\n\n\t/* set up freeze waitqueue */\n\tinit_waitqueue_head(&dd->sdma_unfreeze_wq);\n\tatomic_set(&dd->sdma_unfreeze_count, 0);\n\n\tdescq_cnt = sdma_get_descq_cnt();\n\tdd_dev_info(dd, \"SDMA engines %zu descq_cnt %u\\n\",\n\t\t    num_engines, descq_cnt);\n\n\t/* alloc memory for array of send engines */\n\tdd->per_sdma = kcalloc_node(num_engines, sizeof(*dd->per_sdma),\n\t\t\t\t    GFP_KERNEL, dd->node);\n\tif (!dd->per_sdma)\n\t\treturn ret;\n\n\tidle_cnt = ns_to_cclock(dd, idle_cnt);\n\tif (idle_cnt)\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_HEAD_TO_HOST_FLAG;\n\telse\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_INT_REQ_FLAG;\n\n\tif (!sdma_desct_intr)\n\t\tsdma_desct_intr = SDMA_DESC_INTR;\n\n\t/* Allocate memory for SendDMA descriptor FIFOs */\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tsde = &dd->per_sdma[this_idx];\n\t\tsde->dd = dd;\n\t\tsde->ppd = ppd;\n\t\tsde->this_idx = this_idx;\n\t\tsde->descq_cnt = descq_cnt;\n\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t\tsde->sdma_shift = ilog2(descq_cnt);\n\t\tsde->sdma_mask = (1 << sde->sdma_shift) - 1;\n\n\t\t/* Create a mask specifically for each interrupt source */\n\t\tsde->int_mask = (u64)1 << (0 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t   this_idx);\n\t\tsde->progress_mask = (u64)1 << (1 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t\tthis_idx);\n\t\tsde->idle_mask = (u64)1 << (2 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t    this_idx);\n\t\t/* Create a combined mask to cover all 3 interrupt sources */\n\t\tsde->imask = sde->int_mask | sde->progress_mask |\n\t\t\t     sde->idle_mask;\n\n\t\tspin_lock_init(&sde->tail_lock);\n\t\tseqlock_init(&sde->head_lock);\n\t\tspin_lock_init(&sde->senddmactrl_lock);\n\t\tspin_lock_init(&sde->flushlist_lock);\n\t\tseqlock_init(&sde->waitlock);\n\t\t/* insure there is always a zero bit */\n\t\tsde->ahg_bits = 0xfffffffe00000000ULL;\n\n\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\n\t\t/* set up reference counting */\n\t\tkref_init(&sde->state.kref);\n\t\tinit_completion(&sde->state.comp);\n\n\t\tINIT_LIST_HEAD(&sde->flushlist);\n\t\tINIT_LIST_HEAD(&sde->dmawait);\n\n\t\tsde->tail_csr =\n\t\t\tget_kctxt_csr_addr(dd, this_idx, SD(TAIL));\n\n\t\ttasklet_init(&sde->sdma_hw_clean_up_task, sdma_hw_clean_up_task,\n\t\t\t     (unsigned long)sde);\n\n\t\ttasklet_init(&sde->sdma_sw_clean_up_task, sdma_sw_clean_up_task,\n\t\t\t     (unsigned long)sde);\n\t\tINIT_WORK(&sde->err_halt_worker, sdma_err_halt_wait);\n\t\tINIT_WORK(&sde->flush_worker, sdma_field_flush);\n\n\t\tsde->progress_check_head = 0;\n\n\t\ttimer_setup(&sde->err_progress_check_timer,\n\t\t\t    sdma_err_progress_check, 0);\n\n\t\tsde->descq = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdescq_cnt * sizeof(u64[2]),\n\t\t\t\t\t\t&sde->descq_phys, GFP_KERNEL);\n\t\tif (!sde->descq)\n\t\t\tgoto bail;\n\t\tsde->tx_ring =\n\t\t\tkvzalloc_node(array_size(descq_cnt,\n\t\t\t\t\t\t sizeof(struct sdma_txreq *)),\n\t\t\t\t      GFP_KERNEL, dd->node);\n\t\tif (!sde->tx_ring)\n\t\t\tgoto bail;\n\t}\n\n\tdd->sdma_heads_size = L1_CACHE_BYTES * num_engines;\n\t/* Allocate memory for DMA of head registers to memory */\n\tdd->sdma_heads_dma = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdd->sdma_heads_size,\n\t\t\t\t\t\t&dd->sdma_heads_phys,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!dd->sdma_heads_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA head memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t/* Allocate memory for pad */\n\tdd->sdma_pad_dma = dma_alloc_coherent(&dd->pcidev->dev, sizeof(u32),\n\t\t\t\t\t      &dd->sdma_pad_phys, GFP_KERNEL);\n\tif (!dd->sdma_pad_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA pad memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t/* assign each engine to different cacheline and init registers */\n\tcurr_head = (void *)dd->sdma_heads_dma;\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tunsigned long phys_offset;\n\n\t\tsde = &dd->per_sdma[this_idx];\n\n\t\tsde->head_dma = curr_head;\n\t\tcurr_head += L1_CACHE_BYTES;\n\t\tphys_offset = (unsigned long)sde->head_dma -\n\t\t\t      (unsigned long)dd->sdma_heads_dma;\n\t\tsde->head_phys = dd->sdma_heads_phys + phys_offset;\n\t\tinit_sdma_regs(sde, per_sdma_credits, idle_cnt);\n\t}\n\tdd->flags |= HFI1_HAS_SEND_DMA;\n\tdd->flags |= idle_cnt ? HFI1_HAS_SDMA_TIMEOUT : 0;\n\tdd->num_sdma = num_engines;\n\tret = sdma_map_init(dd, port, ppd->vls_operational, NULL);\n\tif (ret < 0)\n\t\tgoto bail;\n\n\ttmp_sdma_rht = kzalloc(sizeof(*tmp_sdma_rht), GFP_KERNEL);\n\tif (!tmp_sdma_rht) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tret = rhashtable_init(tmp_sdma_rht, &sdma_rht_params);\n\tif (ret < 0) {\n\t\tkfree(tmp_sdma_rht);\n\t\tgoto bail;\n\t}\n\n\tdd->sdma_rht = tmp_sdma_rht;\n\n\tdd_dev_info(dd, \"SDMA num_sdma: %u\\n\", dd->num_sdma);\n\treturn 0;\n\nbail:\n\tsdma_clean(dd, num_engines);\n\treturn ret;\n}\n\n/**\n * sdma_all_running() - called when the link goes up\n * @dd: hfi1_devdata\n *\n * This routine moves all engines to the running state.\n */\nvoid sdma_all_running(struct hfi1_devdata *dd)\n{\n\tstruct sdma_engine *sde;\n\tunsigned int i;\n\n\t/* move all engines to running */\n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e30_go_running);\n\t}\n}\n\n/**\n * sdma_all_idle() - called when the link goes down\n * @dd: hfi1_devdata\n *\n * This routine moves all engines to the idle state.\n */\nvoid sdma_all_idle(struct hfi1_devdata *dd)\n{\n\tstruct sdma_engine *sde;\n\tunsigned int i;\n\n\t/* idle all engines */\n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e70_go_idle);\n\t}\n}\n\n/**\n * sdma_start() - called to kick off state processing for all engines\n * @dd: hfi1_devdata\n *\n * This routine is for kicking off the state processing for all required\n * sdma engines.  Interrupts need to be working at this point.\n *\n */\nvoid sdma_start(struct hfi1_devdata *dd)\n{\n\tunsigned i;\n\tstruct sdma_engine *sde;\n\n\t/* kick off the engines state processing */\n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e10_go_hw_start);\n\t}\n}\n\n/**\n * sdma_exit() - used when module is removed\n * @dd: hfi1_devdata\n */\nvoid sdma_exit(struct hfi1_devdata *dd)\n{\n\tunsigned this_idx;\n\tstruct sdma_engine *sde;\n\n\tfor (this_idx = 0; dd->per_sdma && this_idx < dd->num_sdma;\n\t\t\t++this_idx) {\n\t\tsde = &dd->per_sdma[this_idx];\n\t\tif (!list_empty(&sde->dmawait))\n\t\t\tdd_dev_err(dd, \"sde %u: dmawait list not empty!\\n\",\n\t\t\t\t   sde->this_idx);\n\t\tsdma_process_event(sde, sdma_event_e00_go_hw_down);\n\n\t\tdel_timer_sync(&sde->err_progress_check_timer);\n\n\t\t/*\n\t\t * This waits for the state machine to exit so it is not\n\t\t * necessary to kill the sdma_sw_clean_up_task to make sure\n\t\t * it is not running.\n\t\t */\n\t\tsdma_finalput(&sde->state);\n\t}\n}\n\n/*\n * unmap the indicated descriptor\n */\nstatic inline void sdma_unmap_desc(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_desc *descp)\n{\n\tswitch (sdma_mapping_type(descp)) {\n\tcase SDMA_MAP_SINGLE:\n\t\tdma_unmap_single(\n\t\t\t&dd->pcidev->dev,\n\t\t\tsdma_mapping_addr(descp),\n\t\t\tsdma_mapping_len(descp),\n\t\t\tDMA_TO_DEVICE);\n\t\tbreak;\n\tcase SDMA_MAP_PAGE:\n\t\tdma_unmap_page(\n\t\t\t&dd->pcidev->dev,\n\t\t\tsdma_mapping_addr(descp),\n\t\t\tsdma_mapping_len(descp),\n\t\t\tDMA_TO_DEVICE);\n\t\tbreak;\n\t}\n}\n\n/*\n * return the mode as indicated by the first\n * descriptor in the tx.\n */\nstatic inline u8 ahg_mode(struct sdma_txreq *tx)\n{\n\treturn (tx->descp[0].qw[1] & SDMA_DESC1_HEADER_MODE_SMASK)\n\t\t>> SDMA_DESC1_HEADER_MODE_SHIFT;\n}\n\n/**\n * __sdma_txclean() - clean tx of mappings, descp *kmalloc's\n * @dd: hfi1_devdata for unmapping\n * @tx: tx request to clean\n *\n * This is used in the progress routine to clean the tx or\n * by the ULP to toss an in-process tx build.\n *\n * The code can be called multiple times without issue.\n *\n */\nvoid __sdma_txclean(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_txreq *tx)\n{\n\tu16 i;\n\n\tif (tx->num_desc) {\n\t\tu8 skip = 0, mode = ahg_mode(tx);\n\n\t\t/* unmap first */\n\t\tsdma_unmap_desc(dd, &tx->descp[0]);\n\t\t/* determine number of AHG descriptors to skip */\n\t\tif (mode > SDMA_AHG_APPLY_UPDATE1)\n\t\t\tskip = mode >> 1;\n\t\tfor (i = 1 + skip; i < tx->num_desc; i++)\n\t\t\tsdma_unmap_desc(dd, &tx->descp[i]);\n\t\ttx->num_desc = 0;\n\t}\n\tkfree(tx->coalesce_buf);\n\ttx->coalesce_buf = NULL;\n\t/* kmalloc'ed descp */\n\tif (unlikely(tx->desc_limit > ARRAY_SIZE(tx->descs))) {\n\t\ttx->desc_limit = ARRAY_SIZE(tx->descs);\n\t\tkfree(tx->descp);\n\t}\n}\n\nstatic inline u16 sdma_gethead(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tint use_dmahead;\n\tu16 hwhead;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\nretry:\n\tuse_dmahead = HFI1_CAP_IS_KSET(USE_SDMA_HEAD) && __sdma_running(sde) &&\n\t\t\t\t\t(dd->flags & HFI1_HAS_SDMA_TIMEOUT);\n\thwhead = use_dmahead ?\n\t\t(u16)le64_to_cpu(*sde->head_dma) :\n\t\t(u16)read_sde_csr(sde, SD(HEAD));\n\n\tif (unlikely(HFI1_CAP_IS_KSET(SDMA_HEAD_CHECK))) {\n\t\tu16 cnt;\n\t\tu16 swtail;\n\t\tu16 swhead;\n\t\tint sane;\n\n\t\tswhead = sde->descq_head & sde->sdma_mask;\n\t\t/* this code is really bad for cache line trading */\n\t\tswtail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\t\tcnt = sde->descq_cnt;\n\n\t\tif (swhead < swtail)\n\t\t\t/* not wrapped */\n\t\t\tsane = (hwhead >= swhead) & (hwhead <= swtail);\n\t\telse if (swhead > swtail)\n\t\t\t/* wrapped around */\n\t\t\tsane = ((hwhead >= swhead) && (hwhead < cnt)) ||\n\t\t\t\t(hwhead <= swtail);\n\t\telse\n\t\t\t/* empty */\n\t\t\tsane = (hwhead == swhead);\n\n\t\tif (unlikely(!sane)) {\n\t\t\tdd_dev_err(dd, \"SDMA(%u) bad head (%s) hwhd=%hu swhd=%hu swtl=%hu cnt=%hu\\n\",\n\t\t\t\t   sde->this_idx,\n\t\t\t\t   use_dmahead ? \"dma\" : \"kreg\",\n\t\t\t\t   hwhead, swhead, swtail, cnt);\n\t\t\tif (use_dmahead) {\n\t\t\t\t/* try one more time, using csr */\n\t\t\t\tuse_dmahead = 0;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\t/* proceed as if no progress */\n\t\t\thwhead = swhead;\n\t\t}\n\t}\n\treturn hwhead;\n}\n\n/*\n * This is called when there are send DMA descriptors that might be\n * available.\n *\n * This is called with head_lock held.\n */\nstatic void sdma_desc_avail(struct sdma_engine *sde, uint avail)\n{\n\tstruct iowait *wait, *nw, *twait;\n\tstruct iowait *waits[SDMA_WAIT_BATCH_SIZE];\n\tuint i, n = 0, seq, tidx = 0;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\", sde->this_idx,\n\t\t   slashstrip(__FILE__), __LINE__, __func__);\n\tdd_dev_err(sde->dd, \"avail: %u\\n\", avail);\n#endif\n\n\tdo {\n\t\tseq = read_seqbegin(&sde->waitlock);\n\t\tif (!list_empty(&sde->dmawait)) {\n\t\t\t/* at least one item */\n\t\t\twrite_seqlock(&sde->waitlock);\n\t\t\t/* Harvest waiters wanting DMA descriptors */\n\t\t\tlist_for_each_entry_safe(\n\t\t\t\t\twait,\n\t\t\t\t\tnw,\n\t\t\t\t\t&sde->dmawait,\n\t\t\t\t\tlist) {\n\t\t\t\tu32 num_desc;\n\n\t\t\t\tif (!wait->wakeup)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (n == ARRAY_SIZE(waits))\n\t\t\t\t\tbreak;\n\t\t\t\tiowait_init_priority(wait);\n\t\t\t\tnum_desc = iowait_get_all_desc(wait);\n\t\t\t\tif (num_desc > avail)\n\t\t\t\t\tbreak;\n\t\t\t\tavail -= num_desc;\n\t\t\t\t/* Find the top-priority wait memeber */\n\t\t\t\tif (n) {\n\t\t\t\t\ttwait = waits[tidx];\n\t\t\t\t\ttidx =\n\t\t\t\t\t    iowait_priority_update_top(wait,\n\t\t\t\t\t\t\t\t       twait,\n\t\t\t\t\t\t\t\t       n,\n\t\t\t\t\t\t\t\t       tidx);\n\t\t\t\t}\n\t\t\t\tlist_del_init(&wait->list);\n\t\t\t\twaits[n++] = wait;\n\t\t\t}\n\t\t\twrite_sequnlock(&sde->waitlock);\n\t\t\tbreak;\n\t\t}\n\t} while (read_seqretry(&sde->waitlock, seq));\n\n\t/* Schedule the top-priority entry first */\n\tif (n)\n\t\twaits[tidx]->wakeup(waits[tidx], SDMA_AVAIL_REASON);\n\n\tfor (i = 0; i < n; i++)\n\t\tif (i != tidx)\n\t\t\twaits[i]->wakeup(waits[i], SDMA_AVAIL_REASON);\n}\n\n/* head_lock must be held */\nstatic void sdma_make_progress(struct sdma_engine *sde, u64 status)\n{\n\tstruct sdma_txreq *txp = NULL;\n\tint progress = 0;\n\tu16 hwhead, swhead;\n\tint idle_check_done = 0;\n\n\thwhead = sdma_gethead(sde);\n\n\t/* The reason for some of the complexity of this code is that\n\t * not all descriptors have corresponding txps.  So, we have to\n\t * be able to skip over descs until we wander into the range of\n\t * the next txp on the list.\n\t */\n\nretry:\n\ttxp = get_txhead(sde);\n\tswhead = sde->descq_head & sde->sdma_mask;\n\ttrace_hfi1_sdma_progress(sde, hwhead, swhead, txp);\n\twhile (swhead != hwhead) {\n\t\t/* advance head, wrap if needed */\n\t\tswhead = ++sde->descq_head & sde->sdma_mask;\n\n\t\t/* if now past this txp's descs, do the callback */\n\t\tif (txp && txp->next_descq_idx == swhead) {\n\t\t\t/* remove from list */\n\t\t\tsde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;\n\t\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_OK);\n\t\t\t/* see if there is another txp */\n\t\t\ttxp = get_txhead(sde);\n\t\t}\n\t\ttrace_hfi1_sdma_progress(sde, hwhead, swhead, txp);\n\t\tprogress++;\n\t}\n\n\t/*\n\t * The SDMA idle interrupt is not guaranteed to be ordered with respect\n\t * to updates to the the dma_head location in host memory. The head\n\t * value read might not be fully up to date. If there are pending\n\t * descriptors and the SDMA idle interrupt fired then read from the\n\t * CSR SDMA head instead to get the latest value from the hardware.\n\t * The hardware SDMA head should be read at most once in this invocation\n\t * of sdma_make_progress(..) which is ensured by idle_check_done flag\n\t */\n\tif ((status & sde->idle_mask) && !idle_check_done) {\n\t\tu16 swtail;\n\n\t\tswtail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\t\tif (swtail != hwhead) {\n\t\t\thwhead = (u16)read_sde_csr(sde, SD(HEAD));\n\t\t\tidle_check_done = 1;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tsde->last_status = status;\n\tif (progress)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n}\n\n/*\n * sdma_engine_interrupt() - interrupt handler for engine\n * @sde: sdma engine\n * @status: sdma interrupt reason\n *\n * Status is a mask of the 3 possible interrupts for this engine.  It will\n * contain bits _only_ for this SDMA engine.  It will contain at least one\n * bit, it may contain more.\n */\nvoid sdma_engine_interrupt(struct sdma_engine *sde, u64 status)\n{\n\ttrace_hfi1_sdma_engine_interrupt(sde, status);\n\twrite_seqlock(&sde->head_lock);\n\tsdma_set_desc_cnt(sde, sdma_desct_intr);\n\tif (status & sde->idle_mask)\n\t\tsde->idle_int_cnt++;\n\telse if (status & sde->progress_mask)\n\t\tsde->progress_int_cnt++;\n\telse if (status & sde->int_mask)\n\t\tsde->sdma_int_cnt++;\n\tsdma_make_progress(sde, status);\n\twrite_sequnlock(&sde->head_lock);\n}\n\n/**\n * sdma_engine_error() - error handler for engine\n * @sde: sdma engine\n * @status: sdma interrupt reason\n */\nvoid sdma_engine_error(struct sdma_engine *sde, u64 status)\n{\n\tunsigned long flags;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) error status 0x%llx state %s\\n\",\n\t\t   sde->this_idx,\n\t\t   (unsigned long long)status,\n\t\t   sdma_state_names[sde->state.current_state]);\n#endif\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\tif (status & ALL_SDMA_ENG_HALT_ERRS)\n\t\t__sdma_process_event(sde, sdma_event_e60_hw_halted);\n\tif (status & ~SD(ENG_ERR_STATUS_SDMA_HALT_ERR_SMASK)) {\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA (%u) engine error: 0x%llx state %s\\n\",\n\t\t\t   sde->this_idx,\n\t\t\t   (unsigned long long)status,\n\t\t\t   sdma_state_names[sde->state.current_state]);\n\t\tdump_sdma_state(sde);\n\t}\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void sdma_sendctrl(struct sdma_engine *sde, unsigned op)\n{\n\tu64 set_senddmactrl = 0;\n\tu64 clr_senddmactrl = 0;\n\tunsigned long flags;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) senddmactrl E=%d I=%d H=%d C=%d\\n\",\n\t\t   sde->this_idx,\n\t\t   (op & SDMA_SENDCTRL_OP_ENABLE) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_INTENABLE) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_HALT) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_CLEANUP) ? 1 : 0);\n#endif\n\n\tif (op & SDMA_SENDCTRL_OP_ENABLE)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_ENABLE_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_ENABLE_SMASK);\n\n\tif (op & SDMA_SENDCTRL_OP_INTENABLE)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_INT_ENABLE_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_INT_ENABLE_SMASK);\n\n\tif (op & SDMA_SENDCTRL_OP_HALT)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_HALT_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_HALT_SMASK);\n\n\tspin_lock_irqsave(&sde->senddmactrl_lock, flags);\n\n\tsde->p_senddmactrl |= set_senddmactrl;\n\tsde->p_senddmactrl &= ~clr_senddmactrl;\n\n\tif (op & SDMA_SENDCTRL_OP_CLEANUP)\n\t\twrite_sde_csr(sde, SD(CTRL),\n\t\t\t      sde->p_senddmactrl |\n\t\t\t      SD(CTRL_SDMA_CLEANUP_SMASK));\n\telse\n\t\twrite_sde_csr(sde, SD(CTRL), sde->p_senddmactrl);\n\n\tspin_unlock_irqrestore(&sde->senddmactrl_lock, flags);\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tsdma_dumpstate(sde);\n#endif\n}\n\nstatic void sdma_setlengen(struct sdma_engine *sde)\n{\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\t/*\n\t * Set SendDmaLenGen and clear-then-set the MSB of the generation\n\t * count to enable generation checking and load the internal\n\t * generation counter.\n\t */\n\twrite_sde_csr(sde, SD(LEN_GEN),\n\t\t      (sde->descq_cnt / 64) << SD(LEN_GEN_LENGTH_SHIFT));\n\twrite_sde_csr(sde, SD(LEN_GEN),\n\t\t      ((sde->descq_cnt / 64) << SD(LEN_GEN_LENGTH_SHIFT)) |\n\t\t      (4ULL << SD(LEN_GEN_GENERATION_SHIFT)));\n}\n\nstatic inline void sdma_update_tail(struct sdma_engine *sde, u16 tail)\n{\n\t/* Commit writes to memory and advance the tail on the chip */\n\tsmp_wmb(); /* see get_txhead() */\n\twriteq(tail, sde->tail_csr);\n}\n\n/*\n * This is called when changing to state s10_hw_start_up_halt_wait as\n * a result of send buffer errors or send DMA descriptor errors.\n */\nstatic void sdma_hw_start_up(struct sdma_engine *sde)\n{\n\tu64 reg;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\tsdma_setlengen(sde);\n\tsdma_update_tail(sde, 0); /* Set SendDmaTail */\n\t*sde->head_dma = 0;\n\n\treg = SD(ENG_ERR_CLEAR_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_MASK) <<\n\t      SD(ENG_ERR_CLEAR_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_SHIFT);\n\twrite_sde_csr(sde, SD(ENG_ERR_CLEAR), reg);\n}\n\n/*\n * set_sdma_integrity\n *\n * Set the SEND_DMA_CHECK_ENABLE register for send DMA engine 'sde'.\n */\nstatic void set_sdma_integrity(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\n\twrite_sde_csr(sde, SD(CHECK_ENABLE),\n\t\t      hfi1_pkt_base_sdma_integrity(dd));\n}\n\nstatic void init_sdma_regs(\n\tstruct sdma_engine *sde,\n\tu32 credits,\n\tuint idle_cnt)\n{\n\tu8 opval, opmask;\n#ifdef CONFIG_SDMA_VERBOSITY\n\tstruct hfi1_devdata *dd = sde->dd;\n\n\tdd_dev_err(dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\twrite_sde_csr(sde, SD(BASE_ADDR), sde->descq_phys);\n\tsdma_setlengen(sde);\n\tsdma_update_tail(sde, 0); /* Set SendDmaTail */\n\twrite_sde_csr(sde, SD(RELOAD_CNT), idle_cnt);\n\twrite_sde_csr(sde, SD(DESC_CNT), 0);\n\twrite_sde_csr(sde, SD(HEAD_ADDR), sde->head_phys);\n\twrite_sde_csr(sde, SD(MEMORY),\n\t\t      ((u64)credits << SD(MEMORY_SDMA_MEMORY_CNT_SHIFT)) |\n\t\t      ((u64)(credits * sde->this_idx) <<\n\t\t       SD(MEMORY_SDMA_MEMORY_INDEX_SHIFT)));\n\twrite_sde_csr(sde, SD(ENG_ERR_MASK), ~0ull);\n\tset_sdma_integrity(sde);\n\topmask = OPCODE_CHECK_MASK_DISABLED;\n\topval = OPCODE_CHECK_VAL_DISABLED;\n\twrite_sde_csr(sde, SD(CHECK_OPCODE),\n\t\t      (opmask << SEND_CTXT_CHECK_OPCODE_MASK_SHIFT) |\n\t\t      (opval << SEND_CTXT_CHECK_OPCODE_VALUE_SHIFT));\n}\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\n#define sdma_dumpstate_helper0(reg) do { \\\n\t\tcsr = read_csr(sde->dd, reg); \\\n\t\tdd_dev_err(sde->dd, \"%36s     0x%016llx\\n\", #reg, csr); \\\n\t} while (0)\n\n#define sdma_dumpstate_helper(reg) do { \\\n\t\tcsr = read_sde_csr(sde, reg); \\\n\t\tdd_dev_err(sde->dd, \"%36s[%02u] 0x%016llx\\n\", \\\n\t\t\t#reg, sde->this_idx, csr); \\\n\t} while (0)\n\n#define sdma_dumpstate_helper2(reg) do { \\\n\t\tcsr = read_csr(sde->dd, reg + (8 * i)); \\\n\t\tdd_dev_err(sde->dd, \"%33s_%02u     0x%016llx\\n\", \\\n\t\t\t\t#reg, i, csr); \\\n\t} while (0)\n\nvoid sdma_dumpstate(struct sdma_engine *sde)\n{\n\tu64 csr;\n\tunsigned i;\n\n\tsdma_dumpstate_helper(SD(CTRL));\n\tsdma_dumpstate_helper(SD(STATUS));\n\tsdma_dumpstate_helper0(SD(ERR_STATUS));\n\tsdma_dumpstate_helper0(SD(ERR_MASK));\n\tsdma_dumpstate_helper(SD(ENG_ERR_STATUS));\n\tsdma_dumpstate_helper(SD(ENG_ERR_MASK));\n\n\tfor (i = 0; i < CCE_NUM_INT_CSRS; ++i) {\n\t\tsdma_dumpstate_helper2(CCE_INT_STATUS);\n\t\tsdma_dumpstate_helper2(CCE_INT_MASK);\n\t\tsdma_dumpstate_helper2(CCE_INT_BLOCKED);\n\t}\n\n\tsdma_dumpstate_helper(SD(TAIL));\n\tsdma_dumpstate_helper(SD(HEAD));\n\tsdma_dumpstate_helper(SD(PRIORITY_THLD));\n\tsdma_dumpstate_helper(SD(IDLE_CNT));\n\tsdma_dumpstate_helper(SD(RELOAD_CNT));\n\tsdma_dumpstate_helper(SD(DESC_CNT));\n\tsdma_dumpstate_helper(SD(DESC_FETCHED_CNT));\n\tsdma_dumpstate_helper(SD(MEMORY));\n\tsdma_dumpstate_helper0(SD(ENGINES));\n\tsdma_dumpstate_helper0(SD(MEM_SIZE));\n\t/* sdma_dumpstate_helper(SEND_EGRESS_SEND_DMA_STATUS);  */\n\tsdma_dumpstate_helper(SD(BASE_ADDR));\n\tsdma_dumpstate_helper(SD(LEN_GEN));\n\tsdma_dumpstate_helper(SD(HEAD_ADDR));\n\tsdma_dumpstate_helper(SD(CHECK_ENABLE));\n\tsdma_dumpstate_helper(SD(CHECK_VL));\n\tsdma_dumpstate_helper(SD(CHECK_JOB_KEY));\n\tsdma_dumpstate_helper(SD(CHECK_PARTITION_KEY));\n\tsdma_dumpstate_helper(SD(CHECK_SLID));\n\tsdma_dumpstate_helper(SD(CHECK_OPCODE));\n}\n#endif\n\nstatic void dump_sdma_state(struct sdma_engine *sde)\n{\n\tstruct hw_sdma_desc *descqp;\n\tu64 desc[2];\n\tu64 addr;\n\tu8 gen;\n\tu16 len;\n\tu16 head, tail, cnt;\n\n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = sde->descq_tail & sde->sdma_mask;\n\tcnt = sdma_descq_freecnt(sde);\n\n\tdd_dev_err(sde->dd,\n\t\t   \"SDMA (%u) descq_head: %u descq_tail: %u freecnt: %u FLE %d\\n\",\n\t\t   sde->this_idx, head, tail, cnt,\n\t\t   !list_empty(&sde->flushlist));\n\n\t/* print info for each entry in the descriptor queue */\n\twhile (head != tail) {\n\t\tchar flags[6] = { 'x', 'x', 'x', 'x', 0 };\n\n\t\tdescqp = &sde->descq[head];\n\t\tdesc[0] = le64_to_cpu(descqp->qw[0]);\n\t\tdesc[1] = le64_to_cpu(descqp->qw[1]);\n\t\tflags[0] = (desc[1] & SDMA_DESC1_INT_REQ_FLAG) ? 'I' : '-';\n\t\tflags[1] = (desc[1] & SDMA_DESC1_HEAD_TO_HOST_FLAG) ?\n\t\t\t\t'H' : '-';\n\t\tflags[2] = (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG) ? 'F' : '-';\n\t\tflags[3] = (desc[0] & SDMA_DESC0_LAST_DESC_FLAG) ? 'L' : '-';\n\t\taddr = (desc[0] >> SDMA_DESC0_PHY_ADDR_SHIFT)\n\t\t\t& SDMA_DESC0_PHY_ADDR_MASK;\n\t\tgen = (desc[1] >> SDMA_DESC1_GENERATION_SHIFT)\n\t\t\t& SDMA_DESC1_GENERATION_MASK;\n\t\tlen = (desc[0] >> SDMA_DESC0_BYTE_COUNT_SHIFT)\n\t\t\t& SDMA_DESC0_BYTE_COUNT_MASK;\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA sdmadesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes\\n\",\n\t\t\t   head, flags, addr, gen, len);\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"\\tdesc0:0x%016llx desc1 0x%016llx\\n\",\n\t\t\t   desc[0], desc[1]);\n\t\tif (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG)\n\t\t\tdd_dev_err(sde->dd,\n\t\t\t\t   \"\\taidx: %u amode: %u alen: %u\\n\",\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_INDEX_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_INDEX_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_MODE_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_MODE_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_DWS_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_DWS_SHIFT));\n\t\thead++;\n\t\thead &= sde->sdma_mask;\n\t}\n}\n\n#define SDE_FMT \\\n\t\"SDE %u CPU %d STE %s C 0x%llx S 0x%016llx E 0x%llx T(HW) 0x%llx T(SW) 0x%x H(HW) 0x%llx H(SW) 0x%x H(D) 0x%llx DM 0x%llx GL 0x%llx R 0x%llx LIS 0x%llx AHGI 0x%llx TXT %u TXH %u DT %u DH %u FLNE %d DQF %u SLC 0x%llx\\n\"\n/**\n * sdma_seqfile_dump_sde() - debugfs dump of sde\n * @s: seq file\n * @sde: send dma engine to dump\n *\n * This routine dumps the sde to the indicated seq file.\n */\nvoid sdma_seqfile_dump_sde(struct seq_file *s, struct sdma_engine *sde)\n{\n\tu16 head, tail;\n\tstruct hw_sdma_desc *descqp;\n\tu64 desc[2];\n\tu64 addr;\n\tu8 gen;\n\tu16 len;\n\n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\tseq_printf(s, SDE_FMT, sde->this_idx,\n\t\t   sde->cpu,\n\t\t   sdma_state_name(sde->state.current_state),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(CTRL)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(STATUS)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(ENG_ERR_STATUS)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(TAIL)), tail,\n\t\t   (unsigned long long)read_sde_csr(sde, SD(HEAD)), head,\n\t\t   (unsigned long long)le64_to_cpu(*sde->head_dma),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(MEMORY)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(LEN_GEN)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(RELOAD_CNT)),\n\t\t   (unsigned long long)sde->last_status,\n\t\t   (unsigned long long)sde->ahg_bits,\n\t\t   sde->tx_tail,\n\t\t   sde->tx_head,\n\t\t   sde->descq_tail,\n\t\t   sde->descq_head,\n\t\t   !list_empty(&sde->flushlist),\n\t\t   sde->descq_full_count,\n\t\t   (unsigned long long)read_sde_csr(sde, SEND_DMA_CHECK_SLID));\n\n\t/* print info for each entry in the descriptor queue */\n\twhile (head != tail) {\n\t\tchar flags[6] = { 'x', 'x', 'x', 'x', 0 };\n\n\t\tdescqp = &sde->descq[head];\n\t\tdesc[0] = le64_to_cpu(descqp->qw[0]);\n\t\tdesc[1] = le64_to_cpu(descqp->qw[1]);\n\t\tflags[0] = (desc[1] & SDMA_DESC1_INT_REQ_FLAG) ? 'I' : '-';\n\t\tflags[1] = (desc[1] & SDMA_DESC1_HEAD_TO_HOST_FLAG) ?\n\t\t\t\t'H' : '-';\n\t\tflags[2] = (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG) ? 'F' : '-';\n\t\tflags[3] = (desc[0] & SDMA_DESC0_LAST_DESC_FLAG) ? 'L' : '-';\n\t\taddr = (desc[0] >> SDMA_DESC0_PHY_ADDR_SHIFT)\n\t\t\t& SDMA_DESC0_PHY_ADDR_MASK;\n\t\tgen = (desc[1] >> SDMA_DESC1_GENERATION_SHIFT)\n\t\t\t& SDMA_DESC1_GENERATION_MASK;\n\t\tlen = (desc[0] >> SDMA_DESC0_BYTE_COUNT_SHIFT)\n\t\t\t& SDMA_DESC0_BYTE_COUNT_MASK;\n\t\tseq_printf(s,\n\t\t\t   \"\\tdesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes\\n\",\n\t\t\t   head, flags, addr, gen, len);\n\t\tif (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG)\n\t\t\tseq_printf(s, \"\\t\\tahgidx: %u ahgmode: %u\\n\",\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_INDEX_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_INDEX_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_MODE_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_MODE_SHIFT));\n\t\thead = (head + 1) & sde->sdma_mask;\n\t}\n}\n\n/*\n * add the generation number into\n * the qw1 and return\n */\nstatic inline u64 add_gen(struct sdma_engine *sde, u64 qw1)\n{\n\tu8 generation = (sde->descq_tail >> sde->sdma_shift) & 3;\n\n\tqw1 &= ~SDMA_DESC1_GENERATION_SMASK;\n\tqw1 |= ((u64)generation & SDMA_DESC1_GENERATION_MASK)\n\t\t\t<< SDMA_DESC1_GENERATION_SHIFT;\n\treturn qw1;\n}\n\n/*\n * This routine submits the indicated tx\n *\n * Space has already been guaranteed and\n * tail side of ring is locked.\n *\n * The hardware tail update is done\n * in the caller and that is facilitated\n * by returning the new tail.\n *\n * There is special case logic for ahg\n * to not add the generation number for\n * up to 2 descriptors that follow the\n * first descriptor.\n *\n */\nstatic inline u16 submit_tx(struct sdma_engine *sde, struct sdma_txreq *tx)\n{\n\tint i;\n\tu16 tail;\n\tstruct sdma_desc *descp = tx->descp;\n\tu8 skip = 0, mode = ahg_mode(tx);\n\n\ttail = sde->descq_tail & sde->sdma_mask;\n\tsde->descq[tail].qw[0] = cpu_to_le64(descp->qw[0]);\n\tsde->descq[tail].qw[1] = cpu_to_le64(add_gen(sde, descp->qw[1]));\n\ttrace_hfi1_sdma_descriptor(sde, descp->qw[0], descp->qw[1],\n\t\t\t\t   tail, &sde->descq[tail]);\n\ttail = ++sde->descq_tail & sde->sdma_mask;\n\tdescp++;\n\tif (mode > SDMA_AHG_APPLY_UPDATE1)\n\t\tskip = mode >> 1;\n\tfor (i = 1; i < tx->num_desc; i++, descp++) {\n\t\tu64 qw1;\n\n\t\tsde->descq[tail].qw[0] = cpu_to_le64(descp->qw[0]);\n\t\tif (skip) {\n\t\t\t/* edits don't have generation */\n\t\t\tqw1 = descp->qw[1];\n\t\t\tskip--;\n\t\t} else {\n\t\t\t/* replace generation with real one for non-edits */\n\t\t\tqw1 = add_gen(sde, descp->qw[1]);\n\t\t}\n\t\tsde->descq[tail].qw[1] = cpu_to_le64(qw1);\n\t\ttrace_hfi1_sdma_descriptor(sde, descp->qw[0], qw1,\n\t\t\t\t\t   tail, &sde->descq[tail]);\n\t\ttail = ++sde->descq_tail & sde->sdma_mask;\n\t}\n\ttx->next_descq_idx = tail;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttx->sn = sde->tail_sn++;\n\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n\tWARN_ON_ONCE(sde->tx_ring[sde->tx_tail & sde->sdma_mask]);\n#endif\n\tsde->tx_ring[sde->tx_tail++ & sde->sdma_mask] = tx;\n\tsde->desc_avail -= tx->num_desc;\n\treturn tail;\n}\n\n/*\n * Check for progress\n */\nstatic int sdma_check_progress(\n\tstruct sdma_engine *sde,\n\tstruct iowait_work *wait,\n\tstruct sdma_txreq *tx,\n\tbool pkts_sent)\n{\n\tint ret;\n\n\tsde->desc_avail = sdma_descq_freecnt(sde);\n\tif (tx->num_desc <= sde->desc_avail)\n\t\treturn -EAGAIN;\n\t/* pulse the head_lock */\n\tif (wait && iowait_ioww_to_iow(wait)->sleep) {\n\t\tunsigned seq;\n\n\t\tseq = raw_seqcount_begin(\n\t\t\t(const seqcount_t *)&sde->head_lock.seqcount);\n\t\tret = wait->iow->sleep(sde, wait, tx, seq, pkts_sent);\n\t\tif (ret == -EAGAIN)\n\t\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t} else {\n\t\tret = -EBUSY;\n\t}\n\treturn ret;\n}\n\n/**\n * sdma_send_txreq() - submit a tx req to ring\n * @sde: sdma engine to use\n * @wait: SE wait structure to use when full (may be NULL)\n * @tx: sdma_txreq to submit\n * @pkts_sent: has any packet been sent yet?\n *\n * The call submits the tx into the ring.  If a iowait structure is non-NULL\n * the packet will be queued to the list in wait.\n *\n * Return:\n * 0 - Success, -EINVAL - sdma_txreq incomplete, -EBUSY - no space in\n * ring (wait == NULL)\n * -EIOCBQUEUED - tx queued to iowait, -ECOMM bad sdma state\n */\nint sdma_send_txreq(struct sdma_engine *sde,\n\t\t    struct iowait_work *wait,\n\t\t    struct sdma_txreq *tx,\n\t\t    bool pkts_sent)\n{\n\tint ret = 0;\n\tu16 tail;\n\tunsigned long flags;\n\n\t/* user should have supplied entire packet */\n\tif (unlikely(tx->tlen))\n\t\treturn -EINVAL;\n\ttx->wait = iowait_ioww_to_iow(wait);\n\tspin_lock_irqsave(&sde->tail_lock, flags);\nretry:\n\tif (unlikely(!__sdma_running(sde)))\n\t\tgoto unlock_noconn;\n\tif (unlikely(tx->num_desc > sde->desc_avail))\n\t\tgoto nodesc;\n\ttail = submit_tx(sde, tx);\n\tif (wait)\n\t\tiowait_sdma_inc(iowait_ioww_to_iow(wait));\n\tsdma_update_tail(sde, tail);\nunlock:\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n\treturn ret;\nunlock_noconn:\n\tif (wait)\n\t\tiowait_sdma_inc(iowait_ioww_to_iow(wait));\n\ttx->next_descq_idx = 0;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttx->sn = sde->tail_sn++;\n\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n#endif\n\tspin_lock(&sde->flushlist_lock);\n\tlist_add_tail(&tx->list, &sde->flushlist);\n\tspin_unlock(&sde->flushlist_lock);\n\tiowait_inc_wait_count(wait, tx->num_desc);\n\tqueue_work_on(sde->cpu, system_highpri_wq, &sde->flush_worker);\n\tret = -ECOMM;\n\tgoto unlock;\nnodesc:\n\tret = sdma_check_progress(sde, wait, tx, pkts_sent);\n\tif (ret == -EAGAIN) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tsde->descq_full_count++;\n\tgoto unlock;\n}\n\n/**\n * sdma_send_txlist() - submit a list of tx req to ring\n * @sde: sdma engine to use\n * @wait: SE wait structure to use when full (may be NULL)\n * @tx_list: list of sdma_txreqs to submit\n * @count: pointer to a u16 which, after return will contain the total number of\n *         sdma_txreqs removed from the tx_list. This will include sdma_txreqs\n *         whose SDMA descriptors are submitted to the ring and the sdma_txreqs\n *         which are added to SDMA engine flush list if the SDMA engine state is\n *         not running.\n *\n * The call submits the list into the ring.\n *\n * If the iowait structure is non-NULL and not equal to the iowait list\n * the unprocessed part of the list  will be appended to the list in wait.\n *\n * In all cases, the tx_list will be updated so the head of the tx_list is\n * the list of descriptors that have yet to be transmitted.\n *\n * The intent of this call is to provide a more efficient\n * way of submitting multiple packets to SDMA while holding the tail\n * side locking.\n *\n * Return:\n * 0 - Success,\n * -EINVAL - sdma_txreq incomplete, -EBUSY - no space in ring (wait == NULL)\n * -EIOCBQUEUED - tx queued to iowait, -ECOMM bad sdma state\n */\nint sdma_send_txlist(struct sdma_engine *sde, struct iowait_work *wait,\n\t\t     struct list_head *tx_list, u16 *count_out)\n{\n\tstruct sdma_txreq *tx, *tx_next;\n\tint ret = 0;\n\tunsigned long flags;\n\tu16 tail = INVALID_TAIL;\n\tu32 submit_count = 0, flush_count = 0, total_count;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\nretry:\n\tlist_for_each_entry_safe(tx, tx_next, tx_list, list) {\n\t\ttx->wait = iowait_ioww_to_iow(wait);\n\t\tif (unlikely(!__sdma_running(sde)))\n\t\t\tgoto unlock_noconn;\n\t\tif (unlikely(tx->num_desc > sde->desc_avail))\n\t\t\tgoto nodesc;\n\t\tif (unlikely(tx->tlen)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto update_tail;\n\t\t}\n\t\tlist_del_init(&tx->list);\n\t\ttail = submit_tx(sde, tx);\n\t\tsubmit_count++;\n\t\tif (tail != INVALID_TAIL &&\n\t\t    (submit_count & SDMA_TAIL_UPDATE_THRESH) == 0) {\n\t\t\tsdma_update_tail(sde, tail);\n\t\t\ttail = INVALID_TAIL;\n\t\t}\n\t}\nupdate_tail:\n\ttotal_count = submit_count + flush_count;\n\tif (wait) {\n\t\tiowait_sdma_add(iowait_ioww_to_iow(wait), total_count);\n\t\tiowait_starve_clear(submit_count > 0,\n\t\t\t\t    iowait_ioww_to_iow(wait));\n\t}\n\tif (tail != INVALID_TAIL)\n\t\tsdma_update_tail(sde, tail);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n\t*count_out = total_count;\n\treturn ret;\nunlock_noconn:\n\tspin_lock(&sde->flushlist_lock);\n\tlist_for_each_entry_safe(tx, tx_next, tx_list, list) {\n\t\ttx->wait = iowait_ioww_to_iow(wait);\n\t\tlist_del_init(&tx->list);\n\t\ttx->next_descq_idx = 0;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\t\ttx->sn = sde->tail_sn++;\n\t\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n#endif\n\t\tlist_add_tail(&tx->list, &sde->flushlist);\n\t\tflush_count++;\n\t\tiowait_inc_wait_count(wait, tx->num_desc);\n\t}\n\tspin_unlock(&sde->flushlist_lock);\n\tqueue_work_on(sde->cpu, system_highpri_wq, &sde->flush_worker);\n\tret = -ECOMM;\n\tgoto update_tail;\nnodesc:\n\tret = sdma_check_progress(sde, wait, tx, submit_count > 0);\n\tif (ret == -EAGAIN) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tsde->descq_full_count++;\n\tgoto update_tail;\n}\n\nstatic void sdma_process_event(struct sdma_engine *sde, enum sdma_events event)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\n\t__sdma_process_event(sde, event);\n\n\tif (sde->state.current_state == sdma_state_s99_running)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void __sdma_process_event(struct sdma_engine *sde,\n\t\t\t\t enum sdma_events event)\n{\n\tstruct sdma_state *ss = &sde->state;\n\tint need_progress = 0;\n\n\t/* CONFIG SDMA temporary */\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) [%s] %s\\n\", sde->this_idx,\n\t\t   sdma_state_names[ss->current_state],\n\t\t   sdma_event_names[event]);\n#endif\n\n\tswitch (ss->current_state) {\n\tcase sdma_state_s00_hw_down:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\t/*\n\t\t\t * If down, but running requested (usually result\n\t\t\t * of link up, then we need to start up.\n\t\t\t * This can happen when hw down is requested while\n\t\t\t * bringing the link up with traffic active on\n\t\t\t * 7220, e.g.\n\t\t\t */\n\t\t\tss->go_s99_running = 1;\n\t\t\t/* fall through -- and start dma engine */\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\t/* This reference means the state machine is started */\n\t\t\tsdma_get(&sde->state);\n\t\t\tsdma_set_state(sde,\n\t\t\t\t       sdma_state_s10_hw_start_up_halt_wait);\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s10_hw_start_up_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde,\n\t\t\t\t       sdma_state_s15_hw_start_up_clean_wait);\n\t\t\tsdma_start_hw_clean_up(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s15_hw_start_up_clean_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s20_idle:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tsdma_set_state(sde, sdma_state_s99_running);\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tsdma_set_state(sde, sdma_state_s50_hw_halt_wait);\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\t/* fall through */\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tsdma_set_state(sde, sdma_state_s80_hw_freeze);\n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s30_sw_clean_up_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tsdma_set_state(sde, sdma_state_s40_hw_clean_up_wait);\n\t\t\tsdma_start_hw_clean_up(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s40_hw_clean_up_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s50_hw_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde, sdma_state_s30_sw_clean_up_wait);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s60_idle_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde, sdma_state_s30_sw_clean_up_wait);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s80_hw_freeze:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tsdma_set_state(sde, sdma_state_s82_freeze_sw_clean);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s82_freeze_sw_clean:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\t/* notify caller this engine is done cleaning */\n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s99_running:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tneed_progress = 1;\n\t\t\tsdma_err_progress_check_schedule(sde);\n\t\t\t/* fall through */\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\t/*\n\t\t\t* SW initiated halt does not perform engines\n\t\t\t* progress check\n\t\t\t*/\n\t\t\tsdma_set_state(sde, sdma_state_s50_hw_halt_wait);\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tsdma_set_state(sde, sdma_state_s60_idle_halt_wait);\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\t/* fall through */\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tsdma_set_state(sde, sdma_state_s80_hw_freeze);\n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tss->last_event = event;\n\tif (need_progress)\n\t\tsdma_make_progress(sde, 0);\n}\n\n/*\n * _extend_sdma_tx_descs() - helper to extend txreq\n *\n * This is called once the initial nominal allocation\n * of descriptors in the sdma_txreq is exhausted.\n *\n * The code will bump the allocation up to the max\n * of MAX_DESC (64) descriptors. There doesn't seem\n * much point in an interim step. The last descriptor\n * is reserved for coalesce buffer in order to support\n * cases where input packet has >MAX_DESC iovecs.\n *\n */\nstatic int _extend_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx)\n{\n\tint i;\n\n\t/* Handle last descriptor */\n\tif (unlikely((tx->num_desc == (MAX_DESC - 1)))) {\n\t\t/* if tlen is 0, it is for padding, release last descriptor */\n\t\tif (!tx->tlen) {\n\t\t\ttx->desc_limit = MAX_DESC;\n\t\t} else if (!tx->coalesce_buf) {\n\t\t\t/* allocate coalesce buffer with space for padding */\n\t\t\ttx->coalesce_buf = kmalloc(tx->tlen + sizeof(u32),\n\t\t\t\t\t\t   GFP_ATOMIC);\n\t\t\tif (!tx->coalesce_buf)\n\t\t\t\tgoto enomem;\n\t\t\ttx->coalesce_idx = 0;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(tx->num_desc == MAX_DESC))\n\t\tgoto enomem;\n\n\ttx->descp = kmalloc_array(\n\t\t\tMAX_DESC,\n\t\t\tsizeof(struct sdma_desc),\n\t\t\tGFP_ATOMIC);\n\tif (!tx->descp)\n\t\tgoto enomem;\n\n\t/* reserve last descriptor for coalescing */\n\ttx->desc_limit = MAX_DESC - 1;\n\t/* copy ones already built */\n\tfor (i = 0; i < tx->num_desc; i++)\n\t\ttx->descp[i] = tx->descs[i];\n\treturn 0;\nenomem:\n\t__sdma_txclean(dd, tx);\n\treturn -ENOMEM;\n}\n\n/*\n * ext_coal_sdma_tx_descs() - extend or coalesce sdma tx descriptors\n *\n * This is called once the initial nominal allocation of descriptors\n * in the sdma_txreq is exhausted.\n *\n * This function calls _extend_sdma_tx_descs to extend or allocate\n * coalesce buffer. If there is a allocated coalesce buffer, it will\n * copy the input packet data into the coalesce buffer. It also adds\n * coalesce buffer descriptor once when whole packet is received.\n *\n * Return:\n * <0 - error\n * 0 - coalescing, don't populate descriptor\n * 1 - continue with populating descriptor\n */\nint ext_coal_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx,\n\t\t\t   int type, void *kvaddr, struct page *page,\n\t\t\t   unsigned long offset, u16 len)\n{\n\tint pad_len, rval;\n\tdma_addr_t addr;\n\n\trval = _extend_sdma_tx_descs(dd, tx);\n\tif (rval) {\n\t\t__sdma_txclean(dd, tx);\n\t\treturn rval;\n\t}\n\n\t/* If coalesce buffer is allocated, copy data into it */\n\tif (tx->coalesce_buf) {\n\t\tif (type == SDMA_MAP_NONE) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (type == SDMA_MAP_PAGE) {\n\t\t\tkvaddr = kmap(page);\n\t\t\tkvaddr += offset;\n\t\t} else if (WARN_ON(!kvaddr)) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmemcpy(tx->coalesce_buf + tx->coalesce_idx, kvaddr, len);\n\t\ttx->coalesce_idx += len;\n\t\tif (type == SDMA_MAP_PAGE)\n\t\t\tkunmap(page);\n\n\t\t/* If there is more data, return */\n\t\tif (tx->tlen - tx->coalesce_idx)\n\t\t\treturn 0;\n\n\t\t/* Whole packet is received; add any padding */\n\t\tpad_len = tx->packet_len & (sizeof(u32) - 1);\n\t\tif (pad_len) {\n\t\t\tpad_len = sizeof(u32) - pad_len;\n\t\t\tmemset(tx->coalesce_buf + tx->coalesce_idx, 0, pad_len);\n\t\t\t/* padding is taken care of for coalescing case */\n\t\t\ttx->packet_len += pad_len;\n\t\t\ttx->tlen += pad_len;\n\t\t}\n\n\t\t/* dma map the coalesce buffer */\n\t\taddr = dma_map_single(&dd->pcidev->dev,\n\t\t\t\t      tx->coalesce_buf,\n\t\t\t\t      tx->tlen,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\t\tif (unlikely(dma_mapping_error(&dd->pcidev->dev, addr))) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\t/* Add descriptor for coalesce buffer */\n\t\ttx->desc_limit = MAX_DESC;\n\t\treturn _sdma_txadd_daddr(dd, SDMA_MAP_SINGLE, tx,\n\t\t\t\t\t addr, tx->tlen);\n\t}\n\n\treturn 1;\n}\n\n/* Update sdes when the lmc changes */\nvoid sdma_update_lmc(struct hfi1_devdata *dd, u64 mask, u32 lid)\n{\n\tstruct sdma_engine *sde;\n\tint i;\n\tu64 sreg;\n\n\tsreg = ((mask & SD(CHECK_SLID_MASK_MASK)) <<\n\t\tSD(CHECK_SLID_MASK_SHIFT)) |\n\t\t(((lid & mask) & SD(CHECK_SLID_VALUE_MASK)) <<\n\t\tSD(CHECK_SLID_VALUE_SHIFT));\n\n\tfor (i = 0; i < dd->num_sdma; i++) {\n\t\thfi1_cdbg(LINKVERB, \"SendDmaEngine[%d].SLID_CHECK = 0x%x\",\n\t\t\t  i, (u32)sreg);\n\t\tsde = &dd->per_sdma[i];\n\t\twrite_sde_csr(sde, SD(CHECK_SLID), sreg);\n\t}\n}\n\n/* tx not dword sized - pad */\nint _pad_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx)\n{\n\tint rval = 0;\n\n\ttx->num_desc++;\n\tif ((unlikely(tx->num_desc == tx->desc_limit))) {\n\t\trval = _extend_sdma_tx_descs(dd, tx);\n\t\tif (rval) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn rval;\n\t\t}\n\t}\n\t/* finish the one just added */\n\tmake_tx_sdma_desc(\n\t\ttx,\n\t\tSDMA_MAP_NONE,\n\t\tdd->sdma_pad_phys,\n\t\tsizeof(u32) - (tx->packet_len & (sizeof(u32) - 1)));\n\t_sdma_close_tx(dd, tx);\n\treturn rval;\n}\n\n/*\n * Add ahg to the sdma_txreq\n *\n * The logic will consume up to 3\n * descriptors at the beginning of\n * sdma_txreq.\n */\nvoid _sdma_txreq_ahgadd(\n\tstruct sdma_txreq *tx,\n\tu8 num_ahg,\n\tu8 ahg_entry,\n\tu32 *ahg,\n\tu8 ahg_hlen)\n{\n\tu32 i, shift = 0, desc = 0;\n\tu8 mode;\n\n\tWARN_ON_ONCE(num_ahg > 9 || (ahg_hlen & 3) || ahg_hlen == 4);\n\t/* compute mode */\n\tif (num_ahg == 1)\n\t\tmode = SDMA_AHG_APPLY_UPDATE1;\n\telse if (num_ahg <= 5)\n\t\tmode = SDMA_AHG_APPLY_UPDATE2;\n\telse\n\t\tmode = SDMA_AHG_APPLY_UPDATE3;\n\ttx->num_desc++;\n\t/* initialize to consumed descriptors to zero */\n\tswitch (mode) {\n\tcase SDMA_AHG_APPLY_UPDATE3:\n\t\ttx->num_desc++;\n\t\ttx->descs[2].qw[0] = 0;\n\t\ttx->descs[2].qw[1] = 0;\n\t\t/* FALLTHROUGH */\n\tcase SDMA_AHG_APPLY_UPDATE2:\n\t\ttx->num_desc++;\n\t\ttx->descs[1].qw[0] = 0;\n\t\ttx->descs[1].qw[1] = 0;\n\t\tbreak;\n\t}\n\tahg_hlen >>= 2;\n\ttx->descs[0].qw[1] |=\n\t\t(((u64)ahg_entry & SDMA_DESC1_HEADER_INDEX_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_INDEX_SHIFT) |\n\t\t(((u64)ahg_hlen & SDMA_DESC1_HEADER_DWS_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_DWS_SHIFT) |\n\t\t(((u64)mode & SDMA_DESC1_HEADER_MODE_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_MODE_SHIFT) |\n\t\t(((u64)ahg[0] & SDMA_DESC1_HEADER_UPDATE1_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_UPDATE1_SHIFT);\n\tfor (i = 0; i < (num_ahg - 1); i++) {\n\t\tif (!shift && !(i & 2))\n\t\t\tdesc++;\n\t\ttx->descs[desc].qw[!!(i & 2)] |=\n\t\t\t(((u64)ahg[i + 1])\n\t\t\t\t<< shift);\n\t\tshift = (shift + 32) & 63;\n\t}\n}\n\n/**\n * sdma_ahg_alloc - allocate an AHG entry\n * @sde: engine to allocate from\n *\n * Return:\n * 0-31 when successful, -EOPNOTSUPP if AHG is not enabled,\n * -ENOSPC if an entry is not available\n */\nint sdma_ahg_alloc(struct sdma_engine *sde)\n{\n\tint nr;\n\tint oldbit;\n\n\tif (!sde) {\n\t\ttrace_hfi1_ahg_allocate(sde, -EINVAL);\n\t\treturn -EINVAL;\n\t}\n\twhile (1) {\n\t\tnr = ffz(READ_ONCE(sde->ahg_bits));\n\t\tif (nr > 31) {\n\t\t\ttrace_hfi1_ahg_allocate(sde, -ENOSPC);\n\t\t\treturn -ENOSPC;\n\t\t}\n\t\toldbit = test_and_set_bit(nr, &sde->ahg_bits);\n\t\tif (!oldbit)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\ttrace_hfi1_ahg_allocate(sde, nr);\n\treturn nr;\n}\n\n/**\n * sdma_ahg_free - free an AHG entry\n * @sde: engine to return AHG entry\n * @ahg_index: index to free\n *\n * This routine frees the indicate AHG entry.\n */\nvoid sdma_ahg_free(struct sdma_engine *sde, int ahg_index)\n{\n\tif (!sde)\n\t\treturn;\n\ttrace_hfi1_ahg_deallocate(sde, ahg_index);\n\tif (ahg_index < 0 || ahg_index > 31)\n\t\treturn;\n\tclear_bit(ahg_index, &sde->ahg_bits);\n}\n\n/*\n * SPC freeze handling for SDMA engines.  Called when the driver knows\n * the SPC is going into a freeze but before the freeze is fully\n * settled.  Generally an error interrupt.\n *\n * This event will pull the engine out of running so no more entries can be\n * added to the engine's queue.\n */\nvoid sdma_freeze_notify(struct hfi1_devdata *dd, int link_down)\n{\n\tint i;\n\tenum sdma_events event = link_down ? sdma_event_e85_link_down :\n\t\t\t\t\t     sdma_event_e80_hw_freeze;\n\n\t/* set up the wait but do not wait here */\n\tatomic_set(&dd->sdma_unfreeze_count, dd->num_sdma);\n\n\t/* tell all engines to stop running and wait */\n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i], event);\n\n\t/* sdma_freeze() will wait for all engines to have stopped */\n}\n\n/*\n * SPC freeze handling for SDMA engines.  Called when the driver knows\n * the SPC is fully frozen.\n */\nvoid sdma_freeze(struct hfi1_devdata *dd)\n{\n\tint i;\n\tint ret;\n\n\t/*\n\t * Make sure all engines have moved out of the running state before\n\t * continuing.\n\t */\n\tret = wait_event_interruptible(dd->sdma_unfreeze_wq,\n\t\t\t\t       atomic_read(&dd->sdma_unfreeze_count) <=\n\t\t\t\t       0);\n\t/* interrupted or count is negative, then unloading - just exit */\n\tif (ret || atomic_read(&dd->sdma_unfreeze_count) < 0)\n\t\treturn;\n\n\t/* set up the count for the next wait */\n\tatomic_set(&dd->sdma_unfreeze_count, dd->num_sdma);\n\n\t/* tell all engines that the SPC is frozen, they can start cleaning */\n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i], sdma_event_e81_hw_frozen);\n\n\t/*\n\t * Wait for everyone to finish software clean before exiting.  The\n\t * software clean will read engine CSRs, so must be completed before\n\t * the next step, which will clear the engine CSRs.\n\t */\n\t(void)wait_event_interruptible(dd->sdma_unfreeze_wq,\n\t\t\t\tatomic_read(&dd->sdma_unfreeze_count) <= 0);\n\t/* no need to check results - done no matter what */\n}\n\n/*\n * SPC freeze handling for the SDMA engines.  Called after the SPC is unfrozen.\n *\n * The SPC freeze acts like a SDMA halt and a hardware clean combined.  All\n * that is left is a software clean.  We could do it after the SPC is fully\n * frozen, but then we'd have to add another state to wait for the unfreeze.\n * Instead, just defer the software clean until the unfreeze step.\n */\nvoid sdma_unfreeze(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t/* tell all engines start freeze clean up */\n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i],\n\t\t\t\t   sdma_event_e82_hw_unfreeze);\n}\n\n/**\n * _sdma_engine_progress_schedule() - schedule progress on engine\n * @sde: sdma_engine to schedule progress\n *\n */\nvoid _sdma_engine_progress_schedule(\n\tstruct sdma_engine *sde)\n{\n\ttrace_hfi1_sdma_engine_progress(sde, sde->progress_mask);\n\t/* assume we have selected a good cpu */\n\twrite_csr(sde->dd,\n\t\t  CCE_INT_FORCE + (8 * (IS_SDMA_START / 64)),\n\t\t  sde->progress_mask);\n}\n"], "filenames": ["drivers/infiniband/hw/hfi1/sdma.c"], "buggy_code_start_loc": [1529], "buggy_code_end_loc": [1530], "fixing_code_start_loc": [1529], "fixing_code_end_loc": [1534], "type": "CWE-401", "message": "** DISPUTED ** A memory leak in the sdma_init() function in drivers/infiniband/hw/hfi1/sdma.c in the Linux kernel before 5.3.9 allows attackers to cause a denial of service (memory consumption) by triggering rhashtable_init() failures, aka CID-34b3be18a04e. NOTE: This has been disputed as not a vulnerability because \"rhashtable_init() can only fail if it is passed invalid values in the second parameter's struct, but when invoked from sdma_init() that is a pointer to a static const struct, so an attacker could only trigger failure if they could corrupt kernel memory (in which case a small memory leak is not a significant problem).\"", "other": {"cve": {"id": "CVE-2019-19065", "sourceIdentifier": "cve@mitre.org", "published": "2019-11-18T06:15:12.670", "lastModified": "2020-08-24T17:37:01.140", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "** DISPUTED ** A memory leak in the sdma_init() function in drivers/infiniband/hw/hfi1/sdma.c in the Linux kernel before 5.3.9 allows attackers to cause a denial of service (memory consumption) by triggering rhashtable_init() failures, aka CID-34b3be18a04e. NOTE: This has been disputed as not a vulnerability because \"rhashtable_init() can only fail if it is passed invalid values in the second parameter's struct, but when invoked from sdma_init() that is a pointer to a static const struct, so an attacker could only trigger failure if they could corrupt kernel memory (in which case a small memory leak is not a significant problem).\""}, {"lang": "es", "value": "** EN DISPUTA ** Una p\u00e9rdida de memoria en la funci\u00f3n sdma_init () en drivers / infiniband / hw / hfi1 / sdma.c en el kernel de Linux anterior a la versi\u00f3n 5.3.9 permite a los atacantes causar una denegaci\u00f3n de servicio (consumo de memoria) al activar rhashtable_init ( ) fallos, tambi\u00e9n conocido como CID-34b3be18a04e. NOTA: Esto se ha discutido como no una vulnerabilidad porque \"rhashtable_init () solo puede fallar si se pasan valores no v\u00e1lidos en la estructura del segundo par\u00e1metro, pero cuando se invoca desde sdma_init () es un puntero a una estructura de const est\u00e1tica, por lo que un atacante \"solo podr\u00eda provocar una fallo si pudieran da\u00f1ar la memoria del n\u00facleo (en cuyo caso una peque\u00f1a p\u00e9rdida de memoria no es un problema importante)\"."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.7}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-401"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.3.9", "matchCriteriaId": "72AC7518-B872-42E3-A43B-F2D010211A8D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:19.04:*:*:*:*:*:*:*", "matchCriteriaId": "CD783B0C-9246-47D9-A937-6144FE8BFF0F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:19.10:*:*:*:*:*:*:*", "matchCriteriaId": "A31C8344-3E02-4EB8-8BD8-4C84B7959624"}, {"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.1:*:*:*:*:*:*:*", "matchCriteriaId": "B620311B-34A3-48A6-82DF-6F078D7A4493"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-12/msg00029.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.3.9", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/34b3be18a04ecdc610aae4c48e5d1b799d8689f6", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20191205-0001/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4208-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4210-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4226-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/34b3be18a04ecdc610aae4c48e5d1b799d8689f6"}}