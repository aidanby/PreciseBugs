{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/rnn/lstm_ops.h\"\n\n#include <memory>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename T, GateLayout gate_layout>\nvoid LSTMBlockCellFpropWithEigen(\n    const LSTMBlockCell& cell, OpKernelContext* ctx, const CPUDevice& d,\n    const float forget_bias, const float cell_clip, bool use_peephole,\n    typename TTypes<T>::ConstMatrix x, typename TTypes<T>::ConstMatrix cs_prev,\n    typename TTypes<T>::ConstMatrix h_prev, typename TTypes<T>::ConstMatrix w,\n    typename TTypes<T>::ConstVec wci, typename TTypes<T>::ConstVec wcf,\n    typename TTypes<T>::ConstVec wco, typename TTypes<T>::ConstVec b,\n    typename TTypes<T>::Matrix xh, typename TTypes<T>::Matrix i,\n    typename TTypes<T>::Matrix cs, typename TTypes<T>::Matrix f,\n    typename TTypes<T>::Matrix o, typename TTypes<T>::Matrix ci,\n    typename TTypes<T>::Matrix co, typename TTypes<T>::Matrix gates,\n    typename TTypes<T>::Matrix h) {\n  // Concat xh = [x, h].\n  xh.slice(cell.xh_x_offsets(), cell.xh_x_extents()).device(d) = x;\n  xh.slice(cell.xh_h_offsets(), cell.xh_h_extents()).device(d) = h_prev;\n\n  // states1 = xh * w + b\n  typename TTypes<T>::ConstMatrix const_xh(xh.data(), xh.dimensions());\n  TensorBlasGemm<CPUDevice, T, false /* USE_CUBLAS */>::compute(\n      ctx, d, false, false, typename gemm_compute_type<T>::type(1.f), const_xh,\n      w, typename gemm_compute_type<T>::type(0.f), gates);\n  Eigen::array<Eigen::DenseIndex, 2> b_shape({1, b.dimensions()[0]});\n  Eigen::array<Eigen::DenseIndex, 2> broadcast_shape({cell.batch_size(), 1});\n  gates.device(d) += b.reshape(b_shape).broadcast(broadcast_shape);\n\n  Eigen::array<Eigen::DenseIndex, 2> p_shape({1, cell.cell_size()});\n  Eigen::array<Eigen::DenseIndex, 2> p_broadcast_shape({cell.batch_size(), 1});\n\n  // Input gate.\n  if (use_peephole) {\n    auto i_peep = cs_prev * wci.reshape(p_shape).broadcast(p_broadcast_shape);\n    i.device(d) =\n        (gates.slice(cell.gates_i_offsets(), cell.cell_extents()) + i_peep)\n            .sigmoid();\n  } else {\n    i.device(d) =\n        gates.slice(cell.gates_i_offsets(), cell.cell_extents()).sigmoid();\n  }\n\n  // Cell input.\n  ci.device(d) =\n      gates.slice(cell.gates_c_offsets(gate_layout), cell.cell_extents())\n          .tanh();\n\n  // Forget gate (w/ bias).\n  if (use_peephole) {\n    auto f_peep = cs_prev * wcf.reshape(p_shape).broadcast(p_broadcast_shape);\n    f.device(d) =\n        (gates.slice(cell.gates_f_offsets(gate_layout), cell.cell_extents()) +\n         f.constant(T(forget_bias)) + f_peep)\n            .sigmoid();\n  } else {\n    f.device(d) =\n        (gates.slice(cell.gates_f_offsets(gate_layout), cell.cell_extents()) +\n         f.constant(T(forget_bias)))\n            .sigmoid();\n  }\n\n  // cs = ci .* i + f .* cs_prev\n  cs.device(d) = i * ci + f * cs_prev;\n\n  if (cell_clip > 0.0f) {\n    cs.device(d) =\n        cs.binaryExpr(cs.constant(T(cell_clip)), Eigen::scalar_clip_op<T>());\n  }\n\n  // co = tanh(cs)\n  co.device(d) = cs.tanh();\n\n  // Output gate.\n  if (use_peephole) {\n    auto o_peep = cs * wco.reshape(p_shape).broadcast(p_broadcast_shape);\n    o.device(d) =\n        (gates.slice(cell.gates_o_offsets(), cell.cell_extents()) + o_peep)\n            .sigmoid();\n  } else {\n    o.device(d) =\n        gates.slice(cell.gates_o_offsets(), cell.cell_extents()).sigmoid();\n  }\n\n  // h = o .* co\n  h.device(d) = o * co;\n}\n\ntemplate <typename Device, typename T, GateLayout gate_layout>\nvoid LSTMBlockCellBpropWithEigen(\n    const LSTMBlockCell& cell, OpKernelContext* ctx, const Device& d,\n    bool use_peephole, typename TTypes<T>::ConstMatrix x,\n    typename TTypes<T>::ConstMatrix cs_prev,\n    typename TTypes<T>::ConstMatrix h_prev, typename TTypes<T>::ConstMatrix w,\n    typename TTypes<T>::ConstVec wci, typename TTypes<T>::ConstVec wcf,\n    typename TTypes<T>::ConstVec wco, typename TTypes<T>::ConstVec b,\n    typename TTypes<T>::ConstMatrix i, typename TTypes<T>::ConstMatrix cs,\n    typename TTypes<T>::ConstMatrix f, typename TTypes<T>::ConstMatrix o,\n    typename TTypes<T>::ConstMatrix ci, typename TTypes<T>::ConstMatrix co,\n    typename TTypes<T>::ConstMatrix cs_grad,\n    typename TTypes<T>::ConstMatrix h_grad, typename TTypes<T>::Matrix do_,\n    typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci,\n    typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di,\n    typename TTypes<T>::Matrix dgates, typename TTypes<T>::Matrix cs_prev_grad,\n    typename TTypes<T>::Vec wci_grad, typename TTypes<T>::Vec wcf_grad,\n    typename TTypes<T>::Vec wco_grad) {\n  // do[t] = sigm'(o[t]) .* dh[t] .* co[t]\n  do_.device(d) = o * (o.constant(T(1)) - o) * h_grad * co;\n\n  // dcs[t] += tanh'(cs[t]) .* dh[t] .* o[t] + dcs[t + 1] .* f[t + 1]\n  dcs.device(d) = (co.constant(T(1)) - co * co) * h_grad * o + cs_grad;\n\n  Eigen::array<Eigen::DenseIndex, 2> p_shape({1, cell.cell_size()});\n  Eigen::array<Eigen::DenseIndex, 2> p_broadcast_shape({cell.batch_size(), 1});\n  if (use_peephole) {\n    dcs.device(d) =\n        dcs + do_ * wco.reshape(p_shape).broadcast(p_broadcast_shape);\n  }\n\n  // dci[t] = tanh'(ci[t]) dcs[t] i[t]\n  dci.device(d) = (ci.constant(T(1)) - ci * ci) * dcs * i;\n\n  // df[t] = sigm'(f[t]) dcs[t] cs[t - 1]\n  df.device(d) = f * (f.constant(T(1)) - f) * dcs * cs_prev;\n\n  // di[t] = sigm'(i[t]) dcs[t] ci[t]\n  di.device(d) = i * (i.constant(T(1)) - i) * dcs * ci;\n\n  dgates.slice(cell.gates_i_offsets(), cell.cell_extents()).device(d) = di;\n  dgates.slice(cell.gates_c_offsets(gate_layout), cell.cell_extents())\n      .device(d) = dci;\n  dgates.slice(cell.gates_f_offsets(gate_layout), cell.cell_extents())\n      .device(d) = df;\n  dgates.slice(cell.gates_o_offsets(), cell.cell_extents()).device(d) = do_;\n\n  cs_prev_grad.device(d) = dcs * f;\n  if (use_peephole) {\n    cs_prev_grad.device(d) =\n        cs_prev_grad + di * wci.reshape(p_shape).broadcast(p_broadcast_shape) +\n        df * wcf.reshape(p_shape).broadcast(p_broadcast_shape);\n    wci_grad.device(d) = (di * cs_prev).sum(Eigen::array<int, 1>({0}));\n    wcf_grad.device(d) = (df * cs_prev).sum(Eigen::array<int, 1>({0}));\n    wco_grad.device(d) = (do_ * cs).sum(Eigen::array<int, 1>({0}));\n  }\n}\n\n#define DECLARE_CPU_FBPROP(T, GATE_LAYOUT)                                     \\\n  template <>                                                                  \\\n  void LSTMBlockCellFprop<CPUDevice, T, false /* USE_CUBLAS */, GATE_LAYOUT>:: \\\n  operator()(                                                                  \\\n      OpKernelContext* ctx, const CPUDevice& d, const float forget_bias,       \\\n      const float cell_clip, bool use_peephole,                                \\\n      typename TTypes<T>::ConstMatrix x,                                       \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                 \\\n      typename TTypes<T>::ConstMatrix h_prev,                                  \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,     \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,      \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh,           \\\n      typename TTypes<T>::Matrix i, typename TTypes<T>::Matrix cs,             \\\n      typename TTypes<T>::Matrix f, typename TTypes<T>::Matrix o,              \\\n      typename TTypes<T>::Matrix ci, typename TTypes<T>::Matrix co,            \\\n      typename TTypes<T>::Matrix gates, typename TTypes<T>::Matrix h) {        \\\n    LSTMBlockCellFpropWithEigen<T, GATE_LAYOUT>(                               \\\n        *this, ctx, d, forget_bias, cell_clip, use_peephole, x, cs_prev,       \\\n        h_prev, w, wci, wcf, wco, b, xh, i, cs, f, o, ci, co, gates, h);       \\\n  }                                                                            \\\n  template <>                                                                  \\\n  void LSTMBlockCellBprop<CPUDevice, T, false /* USE_CUBLAS */, GATE_LAYOUT>:: \\\n  operator()(                                                                  \\\n      OpKernelContext* ctx, const CPUDevice& d, bool use_peephole,             \\\n      typename TTypes<T>::ConstMatrix x,                                       \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                 \\\n      typename TTypes<T>::ConstMatrix h_prev,                                  \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,     \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,      \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::ConstMatrix i,       \\\n      typename TTypes<T>::ConstMatrix cs, typename TTypes<T>::ConstMatrix f,   \\\n      typename TTypes<T>::ConstMatrix o, typename TTypes<T>::ConstMatrix ci,   \\\n      typename TTypes<T>::ConstMatrix co,                                      \\\n      typename TTypes<T>::ConstMatrix cs_grad,                                 \\\n      typename TTypes<T>::ConstMatrix h_grad, typename TTypes<T>::Matrix do_,  \\\n      typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci,          \\\n      typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di,            \\\n      typename TTypes<T>::Matrix dgates,                                       \\\n      typename TTypes<T>::Matrix cs_prev_grad,                                 \\\n      typename TTypes<T>::Vec wci_grad, typename TTypes<T>::Vec wcf_grad,      \\\n      typename TTypes<T>::Vec wco_grad) {                                      \\\n    LSTMBlockCellBpropWithEigen<CPUDevice, T, GATE_LAYOUT>(                    \\\n        *this, ctx, d, use_peephole, x, cs_prev, h_prev, w, wci, wcf, wco, b,  \\\n        i, cs, f, o, ci, co, cs_grad, h_grad, do_, dcs, dci, df, di, dgates,   \\\n        cs_prev_grad, wci_grad, wcf_grad, wco_grad);                           \\\n  }                                                                            \\\n  template struct LSTMBlockCellFprop<CPUDevice, T, false /* USE_CUBLAS */,     \\\n                                     GATE_LAYOUT>;                             \\\n  template struct LSTMBlockCellBprop<CPUDevice, T, false /* USE_CUBLAS */,     \\\n                                     GATE_LAYOUT>;\n\n#define DECLARE_CPU_SPECS(T)   \\\n  DECLARE_CPU_FBPROP(T, ICFO); \\\n  DECLARE_CPU_FBPROP(T, IFCO);\n\nDECLARE_CPU_SPECS(Eigen::half);\nDECLARE_CPU_SPECS(float);\n#undef DECLARE_CPU_SPECS\n#undef DECLARE_CPU_FBPROP\n\n#if GOOGLE_CUDA\n#define DECLARE_GPU_FBPROP(T, GATE_LAYOUT)                                    \\\n  template <>                                                                 \\\n  void LSTMBlockCellFprop<GPUDevice, T, true, GATE_LAYOUT>::operator()(       \\\n      OpKernelContext* ctx, const GPUDevice& d, const float forget_bias,      \\\n      const float cell_clip, bool use_peephole,                               \\\n      typename TTypes<T>::ConstMatrix x,                                      \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                \\\n      typename TTypes<T>::ConstMatrix h_prev,                                 \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,    \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,     \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh,          \\\n      typename TTypes<T>::Matrix i, typename TTypes<T>::Matrix cs,            \\\n      typename TTypes<T>::Matrix f, typename TTypes<T>::Matrix o,             \\\n      typename TTypes<T>::Matrix ci, typename TTypes<T>::Matrix co,           \\\n      typename TTypes<T>::Matrix gates, typename TTypes<T>::Matrix h);        \\\n  template <>                                                                 \\\n  void LSTMBlockCellBprop<GPUDevice, T, true, GATE_LAYOUT>::operator()(       \\\n      OpKernelContext* ctx, const GPUDevice& d, bool use_peephole,            \\\n      typename TTypes<T>::ConstMatrix x,                                      \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                \\\n      typename TTypes<T>::ConstMatrix h_prev,                                 \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,    \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,     \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::ConstMatrix i,      \\\n      typename TTypes<T>::ConstMatrix cs, typename TTypes<T>::ConstMatrix f,  \\\n      typename TTypes<T>::ConstMatrix o, typename TTypes<T>::ConstMatrix ci,  \\\n      typename TTypes<T>::ConstMatrix co,                                     \\\n      typename TTypes<T>::ConstMatrix cs_grad,                                \\\n      typename TTypes<T>::ConstMatrix h_grad, typename TTypes<T>::Matrix do_, \\\n      typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci,         \\\n      typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di,           \\\n      typename TTypes<T>::Matrix dgates,                                      \\\n      typename TTypes<T>::Matrix cs_prev_grad,                                \\\n      typename TTypes<T>::Vec wci_grad, typename TTypes<T>::Vec wcf_grad,     \\\n      typename TTypes<T>::Vec wco_grad);                                      \\\n                                                                              \\\n  extern template struct LSTMBlockCellBprop<                                  \\\n      GPUDevice, T, true /* USE_CUBLAS */, GATE_LAYOUT>;                      \\\n  extern template struct LSTMBlockCellFprop<GPUDevice, T, true, GATE_LAYOUT>;\n\n#define DECLARE_GPU_SPECS(T) DECLARE_GPU_FBPROP(T, ICFO);\n\nDECLARE_GPU_SPECS(float);\nDECLARE_GPU_SPECS(Eigen::half);\n#undef DECLARE_GPU_SPECS\n#undef DECLARE_GPU_FBROP\n#endif  // GOOGLE_CUDA\n}  // namespace functor\n\ntemplate <typename Device, typename T, bool USE_CUBLAS, GateLayout gate_layout>\nclass LSTMBlockCellOp : public OpKernel {\n public:\n  explicit LSTMBlockCellOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_clip\", &cell_clip_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_peephole\", &use_peephole_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* x_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n\n    const Tensor* cs_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n\n    const Tensor* w_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n\n    const Tensor* wci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n\n    const Tensor* wcf_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n\n    const Tensor* wco_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n\n    const Tensor* b_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n\n    const int64_t batch_size = x_tensor->dim_size(0);\n    const int64_t input_size = x_tensor->dim_size(1);\n    const int64_t cell_size = cs_prev_tensor->dim_size(1);\n\n    // Sanity checks for our input shapes.\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"cs_prev.dims(0) != batch_size: \",\n                                        cs_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\"cs_prev.dims(1) != cell_size: \",\n                                        cs_prev_tensor->dim_size(1), \" vs. \",\n                                        cell_size));\n\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                        h_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size,\n                errors::InvalidArgument(\n                    \"w.dim_size(0) != input_size + cell_size: \",\n                    w_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"w.dim_size(1) != cell_size * 4: \", w_tensor->dim_size(1),\n                    \" vs. \", cell_size * 4));\n\n    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"b.dim_size(0) != cell_size * 4: \", b_tensor->dim_size(0),\n                    \" vs. \", cell_size * 4));\n\n    // Allocate our output tensors.\n    Tensor* i_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->forward_input_or_allocate_output(\n                            {\"h_prev\"}, \"i\",\n                            TensorShape({batch_size, cell_size}), &i_tensor));\n\n    Tensor* cs_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"cs\", TensorShape({batch_size, cell_size}),\n                                  &cs_tensor));\n\n    Tensor* f_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"f\", TensorShape({batch_size, cell_size}),\n                                  &f_tensor));\n\n    Tensor* o_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->forward_input_or_allocate_output(\n                            {\"cs_prev\"}, \"o\",\n                            TensorShape({batch_size, cell_size}), &o_tensor));\n\n    Tensor* ci_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"ci\", TensorShape({batch_size, cell_size}),\n                                  &ci_tensor));\n\n    Tensor* co_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"co\", TensorShape({batch_size, cell_size}),\n                                  &co_tensor));\n\n    Tensor* h_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"h\", TensorShape({batch_size, cell_size}),\n                                  &h_tensor));\n\n    // Allocate our temp tensors.\n    Tensor xh_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::v(),\n                            TensorShape({batch_size, input_size + cell_size}),\n                            &xh_tensor));\n\n    Tensor gates_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                      TensorShape({batch_size, cell_size * 4}),\n                                      &gates_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n        batch_size, input_size, cell_size)(\n        ctx, device, forget_bias_, cell_clip_, use_peephole_,\n        x_tensor->matrix<T>(), cs_prev_tensor->matrix<T>(),\n        h_prev_tensor->matrix<T>(), w_tensor->matrix<T>(), wci_tensor->vec<T>(),\n        wcf_tensor->vec<T>(), wco_tensor->vec<T>(), b_tensor->vec<T>(),\n        xh_tensor.matrix<T>(), i_tensor->matrix<T>(), cs_tensor->matrix<T>(),\n        f_tensor->matrix<T>(), o_tensor->matrix<T>(), ci_tensor->matrix<T>(),\n        co_tensor->matrix<T>(), gates_tensor.matrix<T>(),\n        h_tensor->matrix<T>());\n  }\n\n private:\n  float forget_bias_;\n  float cell_clip_;\n  bool use_peephole_;\n};\n\n#define REGISTER_KERNEL(T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                             \\\n      Name(\"LSTMBlockCell\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      LSTMBlockCellOp<CPUDevice, T, false, ICFO>);\n\nREGISTER_KERNEL(Eigen::half);\nREGISTER_KERNEL(float);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_KERNEL(T)                                         \\\n  REGISTER_KERNEL_BUILDER(                                             \\\n      Name(\"LSTMBlockCell\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n      LSTMBlockCellOp<GPUDevice, T, true, ICFO>);\n\nREGISTER_GPU_KERNEL(Eigen::half);\nREGISTER_GPU_KERNEL(float);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename T, bool USE_CUBLAS, GateLayout gate_layout>\nclass LSTMBlockCellGradOp : public OpKernel {\n public:\n  explicit LSTMBlockCellGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_peephole\", &use_peephole_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* x_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n\n    const Tensor* cs_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n\n    const Tensor* w_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n\n    const Tensor* wci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n\n    const Tensor* wcf_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n\n    const Tensor* wco_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n\n    const Tensor* b_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n\n    const Tensor* i_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"i\", &i_tensor));\n\n    const Tensor* cs_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs\", &cs_tensor));\n\n    const Tensor* f_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"f\", &f_tensor));\n\n    const Tensor* o_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"o\", &o_tensor));\n\n    const Tensor* ci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"ci\", &ci_tensor));\n\n    const Tensor* co_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"co\", &co_tensor));\n\n    const Tensor* cs_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_grad\", &cs_grad_tensor));\n\n    const Tensor* h_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_grad\", &h_grad_tensor));\n\n    const int64_t batch_size = x_tensor->dim_size(0);\n    const int64_t input_size = x_tensor->dim_size(1);\n    const int64_t cell_size = cs_prev_tensor->dim_size(1);\n\n    // Sanity checks for our input shapes.\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"cs_prev.dims(0) != batch_size: \",\n                                        cs_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\"cs_prev.dims(1) != cell_size: \",\n                                        cs_prev_tensor->dim_size(1), \" vs. \",\n                                        cell_size));\n\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                        h_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size,\n                errors::InvalidArgument(\n                    \"w.dim_size(0) != input_size + cell_size: \",\n                    w_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"w.dim_size(1) != cell_size * 4: \", w_tensor->dim_size(1),\n                    \" vs. \", cell_size * 4));\n\n    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"b.dim_size(0) != cell_size * 4: \", b_tensor->dim_size(0),\n                    \" vs. \", cell_size * 4));\n\n    OP_REQUIRES(ctx, i_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"i.dim_size(0) != batch_size: \", i_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, i_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"i.dim_size(1) != cell_size: \", i_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, cs_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"cs.dim_size(0) != batch_size: \", cs_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, cs_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"cs.dim_size(1) != cell_size: \", cs_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, f_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"f.dim_size(0) != batch_size: \", f_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, f_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"i.dim_size(1) != cell_size: \", f_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, o_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"o.dim_size(0) != batch_size: \", o_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, o_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"o.dim_size(1) != cell_size: \", o_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, ci_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"ci.dim_size(0) != batch_size: \", ci_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, ci_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"ci.dim_size(1) != cell_size: \", ci_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, co_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"co.dim_size(0) != batch_size: \", co_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, co_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"co.dim_size(1) != cell_size: \", co_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, cs_grad_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"cs_grad_tensor.dims(0) != batch_size: \",\n                    cs_grad_tensor->dim_size(0), \" vs. \", batch_size));\n    OP_REQUIRES(ctx, cs_grad_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\"cs_grad_tensor.dims(1) != cell_size: \",\n                                        cs_grad_tensor->dim_size(1), \" vs. \",\n                                        cell_size));\n\n    OP_REQUIRES(ctx, h_grad_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_grad_tensor.dims(0) != batch_size: \",\n                                        h_grad_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, h_grad_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\"h_grad_tensor.dims(1) != cell_size: \",\n                                        h_grad_tensor->dim_size(1), \" vs. \",\n                                        cell_size));\n\n    // Allocate our output tensors.\n    Tensor* cs_prev_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->forward_input_or_allocate_output(\n                 {\"cs_grad\"}, \"cs_prev_grad\",\n                 TensorShape({batch_size, cell_size}), &cs_prev_grad_tensor));\n\n    Tensor* dgates_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\n                            \"dicfo\", TensorShape({batch_size, cell_size * 4}),\n                            &dgates_tensor));\n\n    Tensor* wci_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->forward_input_or_allocate_output(\n                 {\"wci\"}, \"wci_grad\", wci_tensor->shape(), &wci_grad_tensor));\n\n    Tensor* wcf_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->forward_input_or_allocate_output(\n                 {\"wcf\"}, \"wcf_grad\", wcf_tensor->shape(), &wcf_grad_tensor));\n\n    Tensor* wco_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->forward_input_or_allocate_output(\n                 {\"wco\"}, \"wco_grad\", wco_tensor->shape(), &wco_grad_tensor));\n\n    // Allocate our temp tensors.\n    Tensor do_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &do_tensor));\n\n    Tensor dcs_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &dcs_tensor));\n\n    Tensor dci_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &dci_tensor));\n\n    Tensor df_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &df_tensor));\n\n    Tensor di_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &di_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    functor::TensorZero<Device, T>()(device, wci_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wcf_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wco_grad_tensor->flat<T>());\n\n    functor::LSTMBlockCellBprop<Device, T, USE_CUBLAS, gate_layout>(\n        batch_size, input_size, cell_size)(\n        ctx, device, use_peephole_, x_tensor->matrix<T>(),\n        cs_prev_tensor->matrix<T>(), h_prev_tensor->matrix<T>(),\n        w_tensor->matrix<T>(), wci_tensor->vec<T>(), wcf_tensor->vec<T>(),\n        wco_tensor->vec<T>(), b_tensor->vec<T>(), i_tensor->matrix<T>(),\n        cs_tensor->matrix<T>(), f_tensor->matrix<T>(), o_tensor->matrix<T>(),\n        ci_tensor->matrix<T>(), co_tensor->matrix<T>(),\n        cs_grad_tensor->matrix<T>(), h_grad_tensor->matrix<T>(),\n        do_tensor.matrix<T>(), dcs_tensor.matrix<T>(), dci_tensor.matrix<T>(),\n        df_tensor.matrix<T>(), di_tensor.matrix<T>(),\n        dgates_tensor->matrix<T>(), cs_prev_grad_tensor->matrix<T>(),\n        wci_grad_tensor->vec<T>(), wcf_grad_tensor->vec<T>(),\n        wco_grad_tensor->vec<T>());\n  }\n\n protected:\n  bool use_peephole_;\n};\n\n#define REGISTER_KERNEL(T)                                                 \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"LSTMBlockCellGrad\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      LSTMBlockCellGradOp<CPUDevice, T, false, ICFO>);\nREGISTER_KERNEL(float);\nREGISTER_KERNEL(Eigen::half);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_KERNEL(T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"LSTMBlockCellGrad\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n      LSTMBlockCellGradOp<GPUDevice, T, true, ICFO>);\n\nREGISTER_GPU_KERNEL(Eigen::half);\nREGISTER_GPU_KERNEL(float);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nnamespace {\n\n// This helper class can be used to access timeslices of a 3D tensor. If a slice\n// happens to be unaligned (usually because both batch size and number of cells\n// are odd - this isn't common) this involves overhead, since data needs to be\n// copied. However, if all slices are aligned, the bits aren't copied. In the\n// cases where copying is needed, the outputs have to be recopied back.\n// At the end of each time step you should call FinishTimeStep which does this,\n// and also allows for reuse of temporary tensors.\ntemplate <typename Device, typename T>\nclass SliceHelper {\n public:\n  explicit SliceHelper(OpKernelContext* ctx)\n      : ctx_(ctx), device_(ctx_->eigen_device<Device>()) {}\n\n  ~SliceHelper() {\n    CHECK(copy_out_.empty());\n    for (const auto& entry : pool_) {\n      CHECK(!entry.second.second);  // nothing is in use\n    }\n  }\n\n  // Slice through an input tensor. This may copy unaligned slices, but no\n  // copying back will be done at the end.\n  const Tensor InputSlice(const Tensor& t, int pos, const string& name) {\n    Tensor res = UnalignedSlice(t, pos);\n    if (res.IsAligned()) {\n      return res;\n    } else {\n      return AlignTensor(res, name);\n    }\n  }\n\n  // Slice through an output tensor. This may copy unaligned slices, and\n  // schedule copying back on destruction.\n  Tensor OutputSlice(Tensor* t, int pos, const string& name) {\n    Tensor res = UnalignedSlice(*t, pos);\n    if (res.IsAligned()) {\n      return res;\n    } else {\n      Tensor aligned = AlignTensor(res, name);\n      copy_out_.emplace_back(res, aligned);\n      return aligned;\n    }\n  }\n\n  void FinishTimeStep() {\n    for (const auto& p : copy_out_) {\n      const Tensor& aligned = p.second;\n      Tensor original = p.first;\n      // Copy from aligned back to original.\n      functor::TensorCopyToUnaligned<Device, T>()(device_, aligned.flat<T>(),\n                                                  original.unaligned_flat<T>());\n    }\n    copy_out_.clear();\n    // Mark all entries as not in use.\n    for (auto& entry : pool_) {\n      entry.second.second = false;\n    }\n  }\n\n private:\n  // Return a slice at position 'pos'. Result may be unaligned. The resulting\n  // tensor always shares data with the source tensor.\n  Tensor UnalignedSlice(const Tensor& t, int pos) const {\n    Tensor res;\n    // CHECK should never fail here, since the number of elements must match\n    CHECK(res.CopyFrom(t.Slice(pos, pos + 1), {t.dim_size(1), t.dim_size(2)}));\n    return res;\n  }\n\n  // Assumes input is not aligned, creates a temporary aligned tensor of the\n  // same shape and copies the original tensor's content into it.\n  Tensor AlignTensor(const Tensor& t, const string& name) {\n    VLOG(1) << \"AlignTensor called for \" << name << \", shape \"\n            << t.shape().DebugString()\n            << \". This is unnecessary copying. Consider using shapes with even \"\n            << \"sizes\";\n    Tensor aligned;\n    auto found = pool_.find(name);\n    if (found != pool_.end()) {  // found in pool\n      CHECK(!found->second.second) << \"Tensor \" << name << \" is in use\";\n      found->second.second = true;  // mark in use\n      aligned = found->second.first;\n      CHECK(aligned.shape().IsSameSize(t.shape()));\n      CHECK_EQ(aligned.dtype(), t.dtype());\n    } else {  // allocate a new temporary tensor\n      TF_CHECK_OK(ctx_->allocate_temp(t.dtype(), t.shape(), &aligned));\n      pool_.emplace(name, std::make_pair(aligned, true));\n    }\n    functor::TensorCopyUnaligned<Device, T>()(device_, t.unaligned_flat<T>(),\n                                              aligned.flat<T>());\n    return aligned;\n  }\n\n  // Tensors to be copied.\n  std::vector<std::pair<Tensor, const Tensor>> copy_out_;\n  // A pool of pre-allocated temporary tensors, with an indicator for whether\n  // it's in use.\n  std::map<string, std::pair<Tensor, bool>> pool_;\n  // Op context\n  OpKernelContext* ctx_ = nullptr;\n  // Device\n  const Device& device_;\n};\n\n}  // namespace\n\ntemplate <typename Device, typename T, bool USE_CUBLAS, GateLayout gate_layout>\nclass BlockLSTMOp : public OpKernel {\n public:\n  explicit BlockLSTMOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    if (ctx->HasAttr(\"forget_bias\")) {\n      OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n    } else {\n      // V2 version does not have \"forget_bias\" attribute.\n      forget_bias_ = 0.0;\n    }\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_clip\", &cell_clip_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_peephole\", &use_peephole_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* seq_len_max_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"seq_len_max\", &seq_len_max_tensor));\n\n    const Tensor* x;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));\n    OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));\n    const int64_t timelen = x->dim_size(0);\n    const int64_t batch_size = x->dim_size(1);\n    const int64_t input_size = x->dim_size(2);\n\n    const Tensor* cs_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n    OP_REQUIRES(ctx, cs_prev_tensor->dims() == 2,\n                errors::InvalidArgument(\"cs_prev must be 2D\"));\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"cs_prev.dims(0) != batch_size: \",\n                                        cs_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    const int64_t cell_size = cs_prev_tensor->dim_size(1);\n\n    if (batch_size * input_size % 2 == 1) {\n      LOG(WARNING) << \"BlockLSTMOp is inefficient when both batch_size and \"\n                   << \"input_size are odd. You are using: batch_size=\"\n                   << batch_size << \", input_size=\" << input_size;\n    }\n    if (batch_size * cell_size % 2 == 1) {\n      LOG(WARNING) << \"BlockLSTMOp is inefficient when both batch_size and \"\n                   << \"cell_size are odd. You are using: batch_size=\"\n                   << batch_size << \", cell_size=\" << cell_size;\n    }\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n    OP_REQUIRES(ctx, h_prev_tensor->dims() == 2,\n                errors::InvalidArgument(\"h_prev must be 2D\"));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                        h_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    const Tensor* w_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n    OP_REQUIRES(ctx, w_tensor->dims() == 2,\n                errors::InvalidArgument(\"w must be 2D\"));\n    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size,\n                errors::InvalidArgument(\n                    \"w.dim_size(0) != input_size + cell_size: \",\n                    w_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"w.dim_size(1) != cell_size * 4: \", w_tensor->dim_size(1),\n                    \" vs. \", cell_size * 4));\n\n    const Tensor* wci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n    OP_REQUIRES(ctx, wci_tensor->dims() == 1,\n                errors::InvalidArgument(\"wci must be 1D\"));\n    OP_REQUIRES(ctx, wci_tensor->dim_size(0) == cell_size,\n                errors::InvalidArgument(\n                    \"wci.dim_size(0) != cell_size: \", wci_tensor->dim_size(0),\n                    \" vs. \", cell_size));\n\n    const Tensor* wcf_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n    OP_REQUIRES(ctx, wcf_tensor->dims() == 1,\n                errors::InvalidArgument(\"wcf must be 1D\"));\n    OP_REQUIRES(ctx, wcf_tensor->dim_size(0) == cell_size,\n                errors::InvalidArgument(\n                    \"wcf.dim_size(0) != cell_size: \", wcf_tensor->dim_size(0),\n                    \" vs. \", cell_size));\n\n    const Tensor* wco_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n    OP_REQUIRES(ctx, wco_tensor->dims() == 1,\n                errors::InvalidArgument(\"wco must be 1D\"));\n    OP_REQUIRES(ctx, wco_tensor->dim_size(0) == cell_size,\n                errors::InvalidArgument(\n                    \"wco.dim_size(0) != cell_size: \", wco_tensor->dim_size(0),\n                    \" vs. \", cell_size));\n\n    const Tensor* b_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n    OP_REQUIRES(ctx, b_tensor->dims() == 1,\n                errors::InvalidArgument(\"b must be 1D\"));\n    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"b.dim_size(0) != cell_size * 4: \", b_tensor->dim_size(0),\n                    \" vs. \", cell_size * 4));\n\n    TensorShape batch_cell_shape({timelen, batch_size, cell_size});\n    Tensor* i_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"i\", batch_cell_shape, &i_out));\n\n    Tensor* cs_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"cs\", batch_cell_shape, &cs_out));\n\n    Tensor* f_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"f\", batch_cell_shape, &f_out));\n\n    Tensor* o_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"o\", batch_cell_shape, &o_out));\n\n    Tensor* ci_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"ci\", batch_cell_shape, &ci_out));\n\n    Tensor* co_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"co\", batch_cell_shape, &co_out));\n\n    Tensor* h_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"h\", batch_cell_shape, &h_out));\n\n    Tensor xh_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::v(),\n                            TensorShape({batch_size, input_size + cell_size}),\n                            &xh_tensor));\n\n    Tensor gates_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                      TensorShape({batch_size, cell_size * 4}),\n                                      &gates_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    const int64_t seq_len_max = seq_len_max_tensor->scalar<int64_t>()();\n    SliceHelper<Device, T> slicer(ctx);\n    for (int64_t t = 0; t < seq_len_max; ++t) {\n      const Tensor x_tensor = slicer.InputSlice(*x, t, \"x\");\n      const Tensor& cs_prev_tensor2 =\n          t == 0 ? *cs_prev_tensor\n                 : slicer.OutputSlice(cs_out, t - 1, \"cs_prev\");\n      const Tensor& h_prev_tensor2 =\n          t == 0 ? *h_prev_tensor : slicer.OutputSlice(h_out, t - 1, \"h_prev\");\n\n      Tensor i_tensor = slicer.OutputSlice(i_out, t, \"i_out\");\n      Tensor cs_tensor = slicer.OutputSlice(cs_out, t, \"cs_out\");\n      Tensor f_tensor = slicer.OutputSlice(f_out, t, \"f_out\");\n      Tensor o_tensor = slicer.OutputSlice(o_out, t, \"o_out\");\n      Tensor ci_tensor = slicer.OutputSlice(ci_out, t, \"ci_out\");\n      Tensor co_tensor = slicer.OutputSlice(co_out, t, \"co_out\");\n      Tensor h_tensor = slicer.OutputSlice(h_out, t, \"h_out\");\n\n      functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n          batch_size, input_size, cell_size)(\n          ctx, device, forget_bias_, cell_clip_, use_peephole_,\n          x_tensor.matrix<T>(), cs_prev_tensor2.matrix<T>(),\n          h_prev_tensor2.matrix<T>(), w_tensor->matrix<T>(),\n          wci_tensor->vec<T>(), wcf_tensor->vec<T>(), wco_tensor->vec<T>(),\n          b_tensor->vec<T>(), xh_tensor.matrix<T>(), i_tensor.matrix<T>(),\n          cs_tensor.matrix<T>(), f_tensor.matrix<T>(), o_tensor.matrix<T>(),\n          ci_tensor.matrix<T>(), co_tensor.matrix<T>(),\n          gates_tensor.matrix<T>(), h_tensor.matrix<T>());\n      slicer.FinishTimeStep();\n    }\n\n    if (seq_len_max < timelen) {\n      Tensor cs_tensor = cs_out->Slice(seq_len_max, timelen);\n      Tensor h_tensor = h_out->Slice(seq_len_max, timelen);\n\n      functor::TensorUnalignedZero<Device, T>()(device,\n                                                cs_tensor.unaligned_flat<T>());\n      functor::TensorUnalignedZero<Device, T>()(device,\n                                                h_tensor.unaligned_flat<T>());\n    }\n  }\n\n private:\n  float forget_bias_;\n  float cell_clip_;\n  bool use_peephole_;\n};\n\n#define REGISTER_KERNEL(T)                                           \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(\"BlockLSTM\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"),   \\\n      BlockLSTMOp<CPUDevice, T, false, ICFO>);                       \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(\"BlockLSTMV2\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      BlockLSTMOp<CPUDevice, T, false, IFCO>);\n\nREGISTER_KERNEL(Eigen::half);\nREGISTER_KERNEL(float);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nnamespace functor {\n#define DECLARE_GPU_SPECS(T)                                             \\\n  template <>                                                            \\\n  void TensorZero<GPUDevice, T>::operator()(const GPUDevice& d,          \\\n                                            typename TTypes<T>::Flat t); \\\n                                                                         \\\n  extern template struct TensorZero<GPUDevice, T>;                       \\\n                                                                         \\\n  template <>                                                            \\\n  void TensorUnalignedZero<GPUDevice, T>::operator()(                    \\\n      const GPUDevice& d, typename TTypes<T>::UnalignedFlat t);          \\\n                                                                         \\\n  extern template struct TensorUnalignedZero<GPUDevice, T>;\n\nDECLARE_GPU_SPECS(Eigen::half);\nDECLARE_GPU_SPECS(float);\n#undef DECLARE_GPU_SPECS\n}  // end namespace functor\n\n#define REGISTER_GPU_KERNEL(T)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"BlockLSTM\")                       \\\n                              .Device(DEVICE_GPU)                 \\\n                              .HostMemory(\"seq_len_max\")          \\\n                              .TypeConstraint<T>(\"T\"),            \\\n                          BlockLSTMOp<GPUDevice, T, true, ICFO>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"BlockLSTMV2\")                     \\\n                              .Device(DEVICE_GPU)                 \\\n                              .HostMemory(\"seq_len_max\")          \\\n                              .TypeConstraint<T>(\"T\"),            \\\n                          BlockLSTMOp<GPUDevice, T, true, IFCO>);\n\nREGISTER_GPU_KERNEL(Eigen::half);\nREGISTER_GPU_KERNEL(float);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename T, bool USE_CUBLAS, GateLayout gate_layout>\nclass BlockLSTMGradOp : public OpKernel {\n public:\n  explicit BlockLSTMGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_peephole\", &use_peephole_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* seq_len_max_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"seq_len_max\", &seq_len_max_tensor));\n\n    const Tensor* x;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));\n    OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));\n    const int64_t timelen = x->dim_size(0);\n    const int64_t batch_size = x->dim_size(1);\n    const int64_t input_size = x->dim_size(2);\n\n    const Tensor* cs_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n\n    const Tensor* w_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n    const int64_t cell_size = w_tensor->dim_size(1) / 4;\n    OP_REQUIRES(ctx, input_size + cell_size == w_tensor->dim_size(0),\n                errors::InvalidArgument(\n                    \"w matrix rows don't match: \", input_size + cell_size,\n                    \" vs. \", w_tensor->dim_size(0)));\n\n    const Tensor* wci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n\n    const Tensor* wcf_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n\n    const Tensor* wco_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n\n    const Tensor* b_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n    OP_REQUIRES(\n        ctx, cell_size == b_tensor->dim_size(0) / 4,\n        errors::InvalidArgument(\"w and b cell_size don't match: \", cell_size,\n                                \" vs. \", b_tensor->dim_size(0)));\n\n    const Tensor* i_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"i\", &i_out));\n\n    const Tensor* cs_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs\", &cs_out));\n\n    const Tensor* f_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"f\", &f_out));\n\n    const Tensor* o_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"o\", &o_out));\n\n    const Tensor* ci_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"ci\", &ci_out));\n\n    const Tensor* co_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"co\", &co_out));\n\n    const Tensor* h_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h\", &h_out));\n\n    const Tensor* cs_grad = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_grad\", &cs_grad));\n\n    const Tensor* h_grad = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_grad\", &h_grad));\n\n    TensorShape batch_input_shape({timelen, batch_size, input_size});\n    Tensor* x_grad;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(\"x_grad\", batch_input_shape, &x_grad));\n\n    Tensor* cs_prev_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(\"cs_prev_grad\", cs_prev_tensor->shape(),\n                                        &cs_prev_grad_tensor));\n\n    Tensor* h_prev_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(\"h_prev_grad\", h_prev_tensor->shape(),\n                                        &h_prev_grad_tensor));\n\n    Tensor* w_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"w_grad\", w_tensor->shape(), &w_grad_tensor));\n\n    Tensor* wci_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"wci_grad\", wci_tensor->shape(),\n                                             &wci_grad_tensor));\n\n    Tensor* wcf_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"wcf_grad\", wcf_tensor->shape(),\n                                             &wcf_grad_tensor));\n\n    Tensor* wco_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"wco_grad\", wco_tensor->shape(),\n                                             &wco_grad_tensor));\n\n    Tensor* b_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"b_grad\", b_tensor->shape(), &b_grad_tensor));\n\n    TensorShape batch_cell_shape({batch_size, cell_size});\n\n    Tensor xh_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::v(),\n                            TensorShape({batch_size, input_size + cell_size}),\n                            &xh_tensor));\n\n    Tensor xh_grad_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           xh_tensor.shape(), &xh_grad_tensor));\n\n    Tensor do_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &do_tensor));\n\n    Tensor dcs_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &dcs_tensor));\n\n    Tensor dci_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &dci_tensor));\n\n    Tensor df_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &df_tensor));\n\n    Tensor di_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &di_tensor));\n\n    Tensor dgates_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                      TensorShape({batch_size, cell_size * 4}),\n                                      &dgates_tensor));\n\n    Tensor cs_grad_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &cs_grad_tensor));\n\n    Tensor h_grad_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &h_grad_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    functor::TensorZero<Device, T>()(device, cs_grad_tensor.flat<T>());\n    functor::TensorZero<Device, T>()(device, cs_prev_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, h_grad_tensor.flat<T>());\n    functor::TensorZero<Device, T>()(device, h_prev_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, w_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wci_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wcf_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wco_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, b_grad_tensor->flat<T>());\n\n    const int64_t seq_len_max = seq_len_max_tensor->scalar<int64_t>()();\n    SliceHelper<Device, T> slicer(ctx);\n    for (int64_t t = seq_len_max - 1; t >= 0; --t) {\n      const Tensor& x_tensor = slicer.InputSlice(*x, t, \"x\");\n      const Tensor& cs_prev_tensor2 =\n          t == 0 ? *cs_prev_tensor\n                 : slicer.InputSlice(*cs_out, t - 1, \"cs_prev\");\n      const Tensor& h_prev_tensor2 =\n          t == 0 ? *h_prev_tensor : slicer.InputSlice(*h_out, t - 1, \"h_prev\");\n      const Tensor& i_tensor = slicer.InputSlice(*i_out, t, \"i_out\");\n      const Tensor& cs_tensor = slicer.InputSlice(*cs_out, t, \"cs_out\");\n      const Tensor& f_tensor = slicer.InputSlice(*f_out, t, \"f_out\");\n      const Tensor& o_tensor = slicer.InputSlice(*o_out, t, \"o_out\");\n      const Tensor& ci_tensor = slicer.InputSlice(*ci_out, t, \"ci_out\");\n      const Tensor& co_tensor = slicer.InputSlice(*co_out, t, \"co_out\");\n\n      // Grab previous CS grad.\n      const Tensor& const_cs_prev_grad_tensor = *cs_prev_grad_tensor;\n      const Tensor const_cs_grad_slice =\n          slicer.InputSlice(*cs_grad, t, \"cs_grad\");\n      functor::TensorAdd<Device, T>()(\n          device, const_cs_prev_grad_tensor.flat<T>(),\n          const_cs_grad_slice.flat<T>(), cs_grad_tensor.flat<T>());\n\n      // Combine previous h grad and h grad coming on top.\n      const Tensor& const_h_prev_grad_tensor = *h_prev_grad_tensor;\n      const Tensor const_h_grad_slice = slicer.InputSlice(*h_grad, t, \"h_grad\");\n      functor::TensorAdd<Device, T>()(\n          device, const_h_prev_grad_tensor.flat<T>(),\n          const_h_grad_slice.flat<T>(), h_grad_tensor.flat<T>());\n\n      const Tensor& const_cs_grad_tensor = cs_grad_tensor;\n      const Tensor& const_h_grad_tensor = h_grad_tensor;\n\n      Tensor x_grad_tensor = slicer.OutputSlice(x_grad, t, \"x_grad\");\n      functor::BlockLSTMBprop<Device, T, USE_CUBLAS, gate_layout>(\n          batch_size, input_size, cell_size)(\n          ctx, device, use_peephole_, x_tensor.matrix<T>(),\n          cs_prev_tensor2.matrix<T>(), h_prev_tensor2.matrix<T>(),\n          w_tensor->matrix<T>(), wci_tensor->vec<T>(), wcf_tensor->vec<T>(),\n          wco_tensor->vec<T>(), b_tensor->vec<T>(), xh_tensor.matrix<T>(),\n          i_tensor.matrix<T>(), cs_tensor.matrix<T>(), f_tensor.matrix<T>(),\n          o_tensor.matrix<T>(), ci_tensor.matrix<T>(), co_tensor.matrix<T>(),\n          const_cs_grad_tensor.matrix<T>(), const_h_grad_tensor.matrix<T>(),\n          do_tensor.matrix<T>(), dcs_tensor.matrix<T>(), dci_tensor.matrix<T>(),\n          df_tensor.matrix<T>(), di_tensor.matrix<T>(),\n          dgates_tensor.matrix<T>(), cs_prev_grad_tensor->matrix<T>(),\n          h_prev_grad_tensor->matrix<T>(), xh_grad_tensor.matrix<T>(),\n          x_grad_tensor.matrix<T>(), w_grad_tensor->matrix<T>(),\n          wci_grad_tensor->vec<T>(), wcf_grad_tensor->vec<T>(),\n          wco_grad_tensor->vec<T>(), b_grad_tensor->vec<T>());\n      slicer.FinishTimeStep();\n    }\n\n    if (seq_len_max < timelen) {\n      Tensor x_grad_tensor = x_grad->Slice(seq_len_max, timelen);\n      functor::TensorUnalignedZero<Device, T>()(\n          device, x_grad_tensor.unaligned_flat<T>());\n    }\n  }\n\n private:\n  bool use_peephole_;\n};\n\n#define REGISTER_KERNEL(T)                                               \\\n  REGISTER_KERNEL_BUILDER(                                               \\\n      Name(\"BlockLSTMGrad\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"),   \\\n      BlockLSTMGradOp<CPUDevice, T, false, ICFO>);                       \\\n  REGISTER_KERNEL_BUILDER(                                               \\\n      Name(\"BlockLSTMGradV2\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      BlockLSTMGradOp<CPUDevice, T, false, IFCO>);\n\nREGISTER_KERNEL(Eigen::half);\nREGISTER_KERNEL(float);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nnamespace functor {\n#define DECLARE_GPU_BPROP(T, GATE_LAYOUT)                                     \\\n  template <>                                                                 \\\n  void BlockLSTMBprop<GPUDevice, T, true, GATE_LAYOUT>::operator()(           \\\n      OpKernelContext* ctx, const GPUDevice& d, bool use_peephole,            \\\n      typename TTypes<T>::ConstMatrix x,                                      \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                \\\n      typename TTypes<T>::ConstMatrix h_prev,                                 \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,    \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,     \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh,          \\\n      typename TTypes<T>::ConstMatrix i, typename TTypes<T>::ConstMatrix cs,  \\\n      typename TTypes<T>::ConstMatrix f, typename TTypes<T>::ConstMatrix o,   \\\n      typename TTypes<T>::ConstMatrix ci, typename TTypes<T>::ConstMatrix co, \\\n      typename TTypes<T>::ConstMatrix cs_grad,                                \\\n      typename TTypes<T>::ConstMatrix h_grad, typename TTypes<T>::Matrix do_, \\\n      typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci,         \\\n      typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di,           \\\n      typename TTypes<T>::Matrix dgates,                                      \\\n      typename TTypes<T>::Matrix cs_prev_grad,                                \\\n      typename TTypes<T>::Matrix h_prev_grad,                                 \\\n      typename TTypes<T>::Matrix xh_grad, typename TTypes<T>::Matrix x_grad,  \\\n      typename TTypes<T>::Matrix w_grad, typename TTypes<T>::Vec wci_grad,    \\\n      typename TTypes<T>::Vec wcf_grad, typename TTypes<T>::Vec wco_grad,     \\\n      typename TTypes<T>::Vec b_grad);                                        \\\n  extern template struct BlockLSTMBprop<GPUDevice, T, true, GATE_LAYOUT>;\n\n#define DECLARE_GPU_SPECS(T)                                                   \\\n  template <>                                                                  \\\n  void TensorCopy<GPUDevice, T>::operator()(const GPUDevice& d,                \\\n                                            typename TTypes<T>::ConstFlat src, \\\n                                            typename TTypes<T>::Flat dst);     \\\n                                                                               \\\n  template <>                                                                  \\\n  void TensorCopyUnaligned<GPUDevice, T>::operator()(                          \\\n      const GPUDevice& d, typename TTypes<T>::UnalignedConstFlat src,          \\\n      typename TTypes<T>::Flat dst);                                           \\\n                                                                               \\\n  template <>                                                                  \\\n  void TensorCopyToUnaligned<GPUDevice, T>::operator()(                        \\\n      const GPUDevice& d, typename TTypes<T>::ConstFlat src,                   \\\n      typename TTypes<T>::UnalignedFlat dst);                                  \\\n                                                                               \\\n  template <>                                                                  \\\n  void TensorAdd<GPUDevice, T>::operator()(                                    \\\n      const GPUDevice& d, typename TTypes<T>::ConstFlat a,                     \\\n      typename TTypes<T>::ConstFlat b, typename TTypes<T>::Flat c);            \\\n                                                                               \\\n  extern template struct TensorCopy<GPUDevice, T>;                             \\\n  extern template struct TensorAdd<GPUDevice, T>;                              \\\n                                                                               \\\n  DECLARE_GPU_BPROP(T, ICFO);                                                  \\\n  DECLARE_GPU_BPROP(T, IFCO);\n\nDECLARE_GPU_SPECS(Eigen::half);\nDECLARE_GPU_SPECS(float);\n#undef DECLARE_GPU_SPECS\n#undef DECLARE_GPU_BPROP\n}  // end namespace functor\n\n#define REGISTER_GPU_KERNEL(T)                                        \\\n  REGISTER_KERNEL_BUILDER(Name(\"BlockLSTMGrad\")                       \\\n                              .Device(DEVICE_GPU)                     \\\n                              .HostMemory(\"seq_len_max\")              \\\n                              .TypeConstraint<T>(\"T\"),                \\\n                          BlockLSTMGradOp<GPUDevice, T, true, ICFO>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"BlockLSTMGradV2\")                     \\\n                              .Device(DEVICE_GPU)                     \\\n                              .HostMemory(\"seq_len_max\")              \\\n                              .TypeConstraint<T>(\"T\"),                \\\n                          BlockLSTMGradOp<GPUDevice, T, true, IFCO>);\n\nREGISTER_GPU_KERNEL(Eigen::half);\nREGISTER_GPU_KERNEL(float);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // end namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for RNN cells.\"\"\"\n\nimport itertools\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import   array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables as variables_lib\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.saved_model import load\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.training.tracking import tracking\nfrom tensorflow.python.training.tracking import util as trackable_utils\nfrom tensorflow.python.util import nest\n\n\nclass Plus1RNNCell(rnn_cell.RNNCell):\n  \"\"\"RNN Cell generating (output, new_state) = (input + 1, state + 1).\"\"\"\n\n  @property\n  def output_size(self):\n    return 5\n\n  @property\n  def state_size(self):\n    return 5\n\n  def __call__(self, input_, state, scope=None):\n    return (input_ + 1, state + 1)\n\n\nclass DummyMultiDimensionalLSTM(rnn_cell.RNNCell):\n  \"\"\"LSTM Cell generating (output, new_state) = (input + 1, state + 1).\n\n  The input to this cell may have an arbitrary number of dimensions that follow\n  the preceding 'Time' and 'Batch' dimensions.\n  \"\"\"\n\n  def __init__(self, dims):\n    \"\"\"Initialize the Multi-dimensional LSTM cell.\n\n    Args:\n      dims: tuple that contains the dimensions of the output of the cell,\n      without including 'Time' or 'Batch' dimensions.\n    \"\"\"\n    if not isinstance(dims, tuple):\n      raise TypeError(\"The dimensions passed to DummyMultiDimensionalLSTM \"\n                      \"should be a tuple of ints.\")\n    self._dims = dims\n    self._output_size = tensor_shape.TensorShape(self._dims)\n    self._state_size = (tensor_shape.TensorShape(self._dims),\n                        tensor_shape.TensorShape(self._dims))\n\n  @property\n  def output_size(self):\n    return self._output_size\n\n  @property\n  def state_size(self):\n    return self._state_size\n\n  def __call__(self, input_, state, scope=None):\n    h, c = state\n    return (input_ + 1, (h + 1, c + 1))\n\n\nclass NestedRNNCell(rnn_cell.RNNCell):\n  \"\"\"RNN Cell generating (output, new_state) = (input + 1, state + 1).\n\n  The input, output and state of this cell is a tuple of two tensors.\n  \"\"\"\n\n  @property\n  def output_size(self):\n    return (5, 5)\n\n  @property\n  def state_size(self):\n    return (6, 6)\n\n  def __call__(self, input_, state, scope=None):\n    h, c = state\n    x, y = input_\n    return ((x + 1, y + 1), (h + 1, c + 1))\n\n\nclass TestStateSaver(object):\n\n  def __init__(self, batch_size, state_size):\n    self._batch_size = batch_size\n    self._state_size = state_size\n    self.saved_state = {}\n\n  def state(self, name):\n\n    if isinstance(self._state_size, dict):\n      state_size = self._state_size[name]\n    else:\n      state_size = self._state_size\n    if isinstance(state_size, int):\n      state_size = (state_size,)\n    elif isinstance(state_size, tuple):\n      pass\n    else:\n      raise TypeError(\"state_size should either be an int or a tuple\")\n\n    return array_ops.zeros((self._batch_size,) + state_size)\n\n  def save_state(self, name, state):\n    self.saved_state[name] = state\n    return array_ops.identity(state)\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def state_size(self):\n    return self._state_size\n\n\nclass TestStateSaverWithCounters(TestStateSaver):\n  \"\"\"Class wrapper around TestStateSaver.\n\n  A dummy class used for testing of static_state_saving_rnn. It helps test if\n  save_state and state functions got called same number of time when we\n  evaluate output of rnn cell and state or either of them separately. It\n  inherits from the TestStateSaver and adds the counters for calls of functions.\n  \"\"\"\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def __init__(self, batch_size, state_size):\n    super(TestStateSaverWithCounters, self).__init__(batch_size, state_size)\n    self._num_state_calls = variables_lib.VariableV1(0)\n    self._num_save_state_calls = variables_lib.VariableV1(0)\n\n  def state(self, name):\n    with ops.control_dependencies(\n        [state_ops.assign_add(self._num_state_calls, 1)]):\n      return super(TestStateSaverWithCounters, self).state(name)\n\n  def save_state(self, name, state):\n    with ops.control_dependencies([state_ops.assign_add(\n        self._num_save_state_calls, 1)]):\n      return super(TestStateSaverWithCounters, self).save_state(name, state)\n\n  @property\n  def num_state_calls(self):\n    return self._num_state_calls\n\n  @property\n  def num_save_state_calls(self):\n    return self._num_save_state_calls\n\n\nclass RNNTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testInvalidSequenceLengthShape(self):\n    cell = Plus1RNNCell()\n    inputs = [array_ops.placeholder(dtypes.float32, shape=(3, 4))]\n    with self.assertRaisesRegex(ValueError, \"must be a vector\"):\n      rnn.static_rnn(cell, inputs, dtype=dtypes.float32, sequence_length=4)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRNN(self):\n    cell = Plus1RNNCell()\n    batch_size = 2\n    input_size = 5\n    max_length = 8  # unrolled up to this length\n    inputs = max_length * [\n        array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n    ]\n    outputs, state = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n    self.assertEqual(len(outputs), len(inputs))\n    for out, inp in zip(outputs, inputs):\n      self.assertEqual(out.get_shape(), inp.get_shape())\n      self.assertEqual(out.dtype, inp.dtype)\n\n    with self.session() as sess:\n      input_value = np.random.randn(batch_size, input_size)\n      values = sess.run(outputs + [state], feed_dict={inputs[0]: input_value})\n\n      # Outputs\n      for v in values[:-1]:\n        self.assertAllClose(v, input_value + 1.0)\n\n      # Final state\n      self.assertAllClose(values[-1],\n                          max_length * np.ones(\n                              (batch_size, input_size), dtype=np.float32))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDropout(self):\n    cell = Plus1RNNCell()\n    full_dropout_cell = rnn_cell.DropoutWrapper(\n        cell, input_keep_prob=1e-6, seed=0)\n    self.assertIn(\"cell\", full_dropout_cell._trackable_children())\n    self.assertIs(full_dropout_cell._trackable_children()[\"cell\"], cell)\n    batch_size = 2\n    input_size = 5\n    max_length = 8\n    inputs = max_length * [\n        array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n    ]\n    with variable_scope.variable_scope(\"share_scope\"):\n      outputs, state = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n    with variable_scope.variable_scope(\"drop_scope\"):\n      dropped_outputs, _ = rnn.static_rnn(\n          full_dropout_cell, inputs, dtype=dtypes.float32)\n    self.assertEqual(len(outputs), len(inputs))\n    for out, inp in zip(outputs, inputs):\n      self.assertEqual(out.get_shape().as_list(), inp.get_shape().as_list())\n      self.assertEqual(out.dtype, inp.dtype)\n\n    with self.session() as sess:\n      input_value = np.random.randn(batch_size, input_size)\n      values = sess.run(outputs + [state], feed_dict={inputs[0]: input_value})\n      full_dropout_values = sess.run(\n          dropped_outputs, feed_dict={\n              inputs[0]: input_value\n          })\n\n      for v in values[:-1]:\n        self.assertAllClose(v, input_value + 1.0)\n      for d_v in full_dropout_values[:-1]:  # Add 1.0 to dropped_out (all zeros)\n        self.assertAllClose(d_v, np.ones_like(input_value))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDynamicCalculation(self):\n    cell = Plus1RNNCell()\n    sequence_length = array_ops.placeholder(dtypes.int64)\n    batch_size = 2\n    input_size = 5\n    max_length = 8\n    inputs = max_length * [\n        array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n    ]\n    with variable_scope.variable_scope(\"drop_scope\"):\n      dynamic_outputs, dynamic_state = rnn.static_rnn(\n          cell, inputs, sequence_length=sequence_length, dtype=dtypes.float32)\n    self.assertEqual(len(dynamic_outputs), len(inputs))\n\n    with self.session() as sess:\n      input_value = np.random.randn(batch_size, input_size)\n      dynamic_values = sess.run(\n          dynamic_outputs,\n          feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n      dynamic_state_value = sess.run(\n          [dynamic_state],\n          feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n\n      # outputs are fully calculated for t = 0, 1\n      for v in dynamic_values[:2]:\n        self.assertAllClose(v, input_value + 1.0)\n\n      # outputs at t = 2 are zero for entry 0, calculated for entry 1\n      self.assertAllClose(dynamic_values[2],\n                          np.vstack((np.zeros((input_size)),\n                                     1.0 + input_value[1, :])))\n\n      # outputs at t = 3+ are zero\n      for v in dynamic_values[3:]:\n        self.assertAllEqual(v, np.zeros_like(input_value))\n\n      # the final states are:\n      #  entry 0: the values from the calculation at t=1\n      #  entry 1: the values from the calculation at t=2\n      self.assertAllEqual(dynamic_state_value[0],\n                          np.vstack((1.0 * (1 + 1) * np.ones((input_size)),\n                                     1.0 * (2 + 1) * np.ones((input_size)))))\n\n  def _testScope(self, factory, prefix=\"prefix\", use_outer_scope=True):\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          factory(scope)\n      else:\n        factory(prefix)\n\n      # check that all the variables names starts\n      # with the proper scope.\n      variables_lib.global_variables_initializer()\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"RNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testScope(self):\n\n    def factory(scope):\n      cell = Plus1RNNCell()\n      batch_size = 2\n      input_size = 5\n      max_length = 8  # unrolled up to this length\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      return rnn.static_rnn(cell, inputs, dtype=dtypes.float32, scope=scope)\n\n    self._testScope(factory, use_outer_scope=True)\n    self._testScope(factory, use_outer_scope=False)\n    self._testScope(factory, prefix=None, use_outer_scope=False)\n\n\nclass LSTMTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  def testDType(self):\n    # Test case for GitHub issue 16228\n    # Not passing dtype in constructor results in default float32\n    lstm = rnn_cell.LSTMCell(10)\n    input_tensor = array_ops.ones([10, 50])\n    lstm.build(input_tensor.get_shape())\n    self.assertEqual(lstm._bias.dtype.base_dtype, dtypes.float32)\n\n    # Explicitly pass dtype in constructor\n    for dtype in [dtypes.float16, dtypes.float32, dtypes.float64]:\n      lstm = rnn_cell.LSTMCell(10, dtype=dtype)\n      input_tensor = array_ops.ones([10, 50])\n      lstm.build(input_tensor.get_shape())\n      self.assertEqual(lstm._bias.dtype.base_dtype, dtype)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNoProjNoSharding(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      cell = rnn_cell.LSTMCell(\n          num_units, initializer=initializer, state_is_tuple=False)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      outputs, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      self.assertEqual(len(outputs), len(inputs))\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      sess.run(outputs, feed_dict={inputs[0]: input_value})\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testCellClipping(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          cell_clip=0.0,\n          initializer=initializer,\n          state_is_tuple=False)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      outputs, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      self.assertEqual(len(outputs), len(inputs))\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      values = sess.run(outputs, feed_dict={inputs[0]: input_value})\n\n    for value in values:\n      # if cell c is clipped to 0, tanh(c) = 0 => m==0\n      self.assertAllEqual(value, np.zeros((batch_size, num_units)))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNoProjNoShardingSimpleStateSaver(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      state_saver = TestStateSaver(batch_size, 2 * num_units)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=False,\n          initializer=initializer,\n          state_is_tuple=False)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      with variable_scope.variable_scope(\"share_scope\"):\n        outputs, state = rnn.static_state_saving_rnn(\n            cell, inputs, state_saver=state_saver, state_name=\"save_lstm\")\n      self.assertEqual(len(outputs), len(inputs))\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      (last_state_value, saved_state_value) = sess.run(\n          [state, state_saver.saved_state[\"save_lstm\"]],\n          feed_dict={\n              inputs[0]: input_value\n          })\n      self.assertAllEqual(last_state_value, saved_state_value)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNoProjNoShardingTupleStateSaver(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      state_saver = TestStateSaver(batch_size, num_units)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=False,\n          initializer=initializer,\n          state_is_tuple=True)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      with variable_scope.variable_scope(\"share_scope\"):\n        outputs, state = rnn.static_state_saving_rnn(\n            cell, inputs, state_saver=state_saver, state_name=(\"c\", \"m\"))\n      self.assertEqual(len(outputs), len(inputs))\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      last_and_saved_states = sess.run(\n          state + (state_saver.saved_state[\"c\"], state_saver.saved_state[\"m\"]),\n          feed_dict={\n              inputs[0]: input_value\n          })\n      self.assertEqual(4, len(last_and_saved_states))\n      self.assertAllEqual(last_and_saved_states[:2], last_and_saved_states[2:])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNoProjNoShardingNestedTupleStateSaver(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      state_saver = TestStateSaver(\n          batch_size, {\n              \"c0\": num_units,\n              \"m0\": num_units,\n              \"c1\": num_units + 1,\n              \"m1\": num_units + 1,\n              \"c2\": num_units + 2,\n              \"m2\": num_units + 2,\n              \"c3\": num_units + 3,\n              \"m3\": num_units + 3\n          })\n\n      def _cell(i):\n        return rnn_cell.LSTMCell(\n            num_units + i,\n            use_peepholes=False,\n            initializer=initializer,\n            state_is_tuple=True)\n\n      # This creates a state tuple which has 4 sub-tuples of length 2 each.\n      cell = rnn_cell.MultiRNNCell(\n          [_cell(i) for i in range(4)], state_is_tuple=True)\n\n      self.assertEqual(len(cell.state_size), 4)\n      for i in range(4):\n        self.assertEqual(len(cell.state_size[i]), 2)\n\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n\n      state_names = ((\"c0\", \"m0\"), (\"c1\", \"m1\"), (\"c2\", \"m2\"), (\"c3\", \"m3\"))\n      with variable_scope.variable_scope(\"share_scope\"):\n        outputs, state = rnn.static_state_saving_rnn(\n            cell, inputs, state_saver=state_saver, state_name=state_names)\n      self.assertEqual(len(outputs), len(inputs))\n\n      # Final output comes from _cell(3) which has state size num_units + 3\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units + 3])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      last_states = sess.run(\n          list(nest.flatten(state)), feed_dict={\n              inputs[0]: input_value\n          })\n      saved_states = sess.run(\n          list(state_saver.saved_state.values()),\n          feed_dict={\n              inputs[0]: input_value\n          })\n      self.assertEqual(8, len(last_states))\n      self.assertEqual(8, len(saved_states))\n      flat_state_names = nest.flatten(state_names)\n      named_saved_states = dict(\n          zip(state_saver.saved_state.keys(), saved_states))\n\n      for i in range(8):\n        self.assertAllEqual(last_states[i],\n                            named_saved_states[flat_state_names[i]])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testProjNoSharding(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=False)\n      outputs, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      self.assertEqual(len(outputs), len(inputs))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      sess.run(outputs, feed_dict={inputs[0]: input_value})\n\n  def _testStateTupleWithProjAndSequenceLength(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    sequence_length = [4, 6]\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      cell_notuple = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=False)\n      cell_tuple = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=True)\n      with variable_scope.variable_scope(\"root\") as scope:\n        outputs_notuple, state_notuple = rnn.static_rnn(\n            cell_notuple,\n            inputs,\n            dtype=dtypes.float32,\n            sequence_length=sequence_length,\n            scope=scope)\n        scope.reuse_variables()\n        # TODO(ebrevdo): For this test, we ensure values are identical and\n        # therefore the weights here are tied.  In the future, we may consider\n        # making the state_is_tuple property mutable so we can avoid\n        # having to do this - especially if users ever need to reuse\n        # the parameters from different RNNCell instances.  Right now,\n        # this seems an unrealistic use case except for testing.\n        cell_tuple._scope = cell_notuple._scope  # pylint: disable=protected-access\n        outputs_tuple, state_tuple = rnn.static_rnn(\n            cell_tuple,\n            inputs,\n            dtype=dtypes.float32,\n            sequence_length=sequence_length,\n            scope=scope)\n      self.assertEqual(len(outputs_notuple), len(inputs))\n      self.assertEqual(len(outputs_tuple), len(inputs))\n      self.assertTrue(isinstance(state_tuple, tuple))\n      self.assertTrue(isinstance(state_notuple, ops.Tensor))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      outputs_notuple_v = sess.run(\n          outputs_notuple, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs_tuple_v = sess.run(\n          outputs_tuple, feed_dict={\n              inputs[0]: input_value\n          })\n      self.assertAllEqual(outputs_notuple_v, outputs_tuple_v)\n\n      (state_notuple_v,) = sess.run(\n          (state_notuple,), feed_dict={\n              inputs[0]: input_value\n          })\n      state_tuple_v = sess.run(state_tuple, feed_dict={inputs[0]: input_value})\n      self.assertAllEqual(state_notuple_v, np.hstack(state_tuple_v))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testProjSharding(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    num_proj_shards = 3\n    num_unit_shards = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          num_unit_shards=num_unit_shards,\n          num_proj_shards=num_proj_shards,\n          initializer=initializer,\n          state_is_tuple=False)\n\n      outputs, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n\n      self.assertEqual(len(outputs), len(inputs))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      sess.run(outputs, feed_dict={inputs[0]: input_value})\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDoubleInput(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    num_proj_shards = 3\n    num_unit_shards = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(-1, 1, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float64, shape=(None, input_size))\n      ]\n\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          num_unit_shards=num_unit_shards,\n          num_proj_shards=num_proj_shards,\n          initializer=initializer,\n          state_is_tuple=False)\n\n      outputs, _ = rnn.static_rnn(\n          cell,\n          inputs,\n          initial_state=cell.zero_state(batch_size, dtypes.float64))\n\n      self.assertEqual(len(outputs), len(inputs))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.asarray(\n          np.random.randn(batch_size, input_size), dtype=np.float64)\n      values = sess.run(outputs, feed_dict={inputs[0]: input_value})\n      self.assertEqual(values[0].dtype, input_value.dtype)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testShardNoShardEquivalentOutput(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    num_proj_shards = 3\n    num_unit_shards = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      initializer = init_ops.constant_initializer(0.001)\n\n      cell_noshard = rnn_cell.LSTMCell(\n          num_units,\n          num_proj=num_proj,\n          use_peepholes=True,\n          initializer=initializer,\n          num_unit_shards=num_unit_shards,\n          num_proj_shards=num_proj_shards,\n          state_is_tuple=False)\n\n      cell_shard = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          initializer=initializer,\n          num_proj=num_proj,\n          state_is_tuple=False)\n\n      with variable_scope.variable_scope(\"noshard_scope\"):\n        outputs_noshard, state_noshard = rnn.static_rnn(\n            cell_noshard, inputs, dtype=dtypes.float32)\n      with variable_scope.variable_scope(\"shard_scope\"):\n        outputs_shard, state_shard = rnn.static_rnn(\n            cell_shard, inputs, dtype=dtypes.float32)\n\n      self.assertEqual(len(outputs_noshard), len(inputs))\n      self.assertEqual(len(outputs_noshard), len(outputs_shard))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      feeds = dict((x, input_value) for x in inputs)\n      values_noshard = sess.run(outputs_noshard, feed_dict=feeds)\n      values_shard = sess.run(outputs_shard, feed_dict=feeds)\n      state_values_noshard = sess.run([state_noshard], feed_dict=feeds)\n      state_values_shard = sess.run([state_shard], feed_dict=feeds)\n      self.assertEqual(len(values_noshard), len(values_shard))\n      self.assertEqual(len(state_values_noshard), len(state_values_shard))\n      for (v_noshard, v_shard) in zip(values_noshard, values_shard):\n        self.assertAllClose(v_noshard, v_shard, atol=1e-3)\n      for (s_noshard, s_shard) in zip(state_values_noshard, state_values_shard):\n        self.assertAllClose(s_noshard, s_shard, atol=1e-3)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDoubleInputWithDropoutAndDynamicCalculation(self):\n    \"\"\"Smoke test for using LSTM with doubles, dropout, dynamic calculation.\"\"\"\n\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    num_proj_shards = 3\n    num_unit_shards = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      sequence_length = array_ops.placeholder(dtypes.int64)\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float64, shape=(None, input_size))\n      ]\n\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          num_unit_shards=num_unit_shards,\n          num_proj_shards=num_proj_shards,\n          initializer=initializer,\n          state_is_tuple=False)\n      dropout_cell = rnn_cell.DropoutWrapper(cell, 0.5, seed=0)\n\n      outputs, state = rnn.static_rnn(\n          dropout_cell,\n          inputs,\n          sequence_length=sequence_length,\n          initial_state=cell.zero_state(batch_size, dtypes.float64))\n\n      self.assertEqual(len(outputs), len(inputs))\n\n      variables_lib.global_variables_initializer().run(feed_dict={\n          sequence_length: [2, 3]\n      })\n      input_value = np.asarray(\n          np.random.randn(batch_size, input_size), dtype=np.float64)\n      values = sess.run(\n          outputs, feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n      state_value = sess.run(\n          [state], feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n      self.assertEqual(values[0].dtype, input_value.dtype)\n      self.assertEqual(state_value[0].dtype, input_value.dtype)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testSharingWeightsWithReuse(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(-1, 1, seed=self._seed)\n      initializer_d = init_ops.random_uniform_initializer(\n          -1, 1, seed=self._seed + 1)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=False)\n      cell_d = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer_d,\n          state_is_tuple=False)\n\n      with variable_scope.variable_scope(\"share_scope\"):\n        outputs0, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      with variable_scope.variable_scope(\"share_scope\", reuse=True):\n        outputs1, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      with variable_scope.variable_scope(\"diff_scope\"):\n        outputs2, _ = rnn.static_rnn(cell_d, inputs, dtype=dtypes.float32)\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      output_values = sess.run(\n          outputs0 + outputs1 + outputs2, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs0_values = output_values[:max_length]\n      outputs1_values = output_values[max_length:2 * max_length]\n      outputs2_values = output_values[2 * max_length:]\n      self.assertEqual(len(outputs0_values), len(outputs1_values))\n      self.assertEqual(len(outputs0_values), len(outputs2_values))\n      for o1, o2, o3 in zip(outputs0_values, outputs1_values, outputs2_values):\n        # Same weights used by both RNNs so outputs should be the same.\n        self.assertAllEqual(o1, o2)\n        # Different weights used so outputs should be different.\n        self.assertTrue(np.linalg.norm(o1 - o3) > 1e-6)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testSharingWeightsWithDifferentNamescope(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(-1, 1, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=False)\n\n      with ops.name_scope(\"scope0\"):\n        with variable_scope.variable_scope(\"share_scope\"):\n          outputs0, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      with ops.name_scope(\"scope1\"):\n        with variable_scope.variable_scope(\"share_scope\", reuse=True):\n          outputs1, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      output_values = sess.run(\n          outputs0 + outputs1, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs0_values = output_values[:max_length]\n      outputs1_values = output_values[max_length:]\n      self.assertEqual(len(outputs0_values), len(outputs1_values))\n      for out0, out1 in zip(outputs0_values, outputs1_values):\n        self.assertAllEqual(out0, out1)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDynamicRNNAllowsUnknownTimeDimension(self):\n    inputs = array_ops.placeholder(dtypes.float32, shape=[1, None, 20])\n    cell = rnn_cell.GRUCell(30)\n    # Smoke test, this should not raise an error\n    rnn.dynamic_rnn(cell, inputs, dtype=dtypes.float32)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDynamicRNNWithTupleStates(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    sequence_length = [4, 6]\n    in_graph_mode = not context.executing_eagerly()\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      if in_graph_mode:\n        inputs = max_length * [\n            array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n        ]\n      else:\n        inputs = max_length * [\n            constant_op.constant(\n                np.random.randn(batch_size, input_size).astype(np.float32))\n        ]\n      inputs_c = array_ops.stack(inputs)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=True)\n      with variable_scope.variable_scope(\"root\") as scope:\n        outputs_static, state_static = rnn.static_rnn(\n            cell,\n            inputs,\n            dtype=dtypes.float32,\n            sequence_length=sequence_length,\n            scope=scope)\n        scope.reuse_variables()\n        outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n            cell,\n            inputs_c,\n            dtype=dtypes.float32,\n            time_major=True,\n            sequence_length=sequence_length,\n            scope=scope)\n      self.assertTrue(isinstance(state_static, rnn_cell.LSTMStateTuple))\n      self.assertTrue(isinstance(state_dynamic, rnn_cell.LSTMStateTuple))\n      self.assertIs(state_static[0], state_static.c)\n      self.assertIs(state_static[1], state_static.h)\n      self.assertIs(state_dynamic[0], state_dynamic.c)\n      self.assertIs(state_dynamic[1], state_dynamic.h)\n\n      if in_graph_mode:\n        variables_lib.global_variables_initializer().run()\n        input_value = np.random.randn(batch_size, input_size)\n        outputs_static = sess.run(\n            outputs_static, feed_dict={\n                inputs[0]: input_value\n            })\n        outputs_dynamic = sess.run(\n            outputs_dynamic, feed_dict={\n                inputs[0]: input_value\n            })\n        state_static = sess.run(\n            state_static, feed_dict={\n                inputs[0]: input_value\n            })\n        state_dynamic = sess.run(\n            state_dynamic, feed_dict={\n                inputs[0]: input_value\n            })\n\n      comparison_fn = self.assertAllEqual\n      if test_util.is_xla_enabled():\n        comparison_fn = self.assertAllClose\n      if in_graph_mode:\n        comparison_fn(outputs_static, outputs_dynamic)\n      else:\n        self.assertAllEqual(array_ops.stack(outputs_static), outputs_dynamic)\n      comparison_fn(np.hstack(state_static), np.hstack(state_dynamic))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDynamicRNNWithNestedTupleStates(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    sequence_length = [4, 6]\n    in_graph_mode = not context.executing_eagerly()\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      if in_graph_mode:\n        inputs = max_length * [\n            array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n        ]\n      else:\n        inputs = max_length * [\n            constant_op.constant(\n                np.random.randn(batch_size, input_size).astype(np.float32))\n        ]\n      inputs_c = array_ops.stack(inputs)\n\n      def _cell(i):\n        return rnn_cell.LSTMCell(\n            num_units + i,\n            use_peepholes=True,\n            num_proj=num_proj + i,\n            initializer=initializer,\n            state_is_tuple=True)\n\n      # This creates a state tuple which has 4 sub-tuples of length 2 each.\n      cell = rnn_cell.MultiRNNCell(\n          [_cell(i) for i in range(4)], state_is_tuple=True)\n\n      self.assertEqual(len(cell.state_size), 4)\n      for i in range(4):\n        self.assertEqual(len(cell.state_size[i]), 2)\n\n      test_zero = cell.zero_state(1, dtypes.float32)\n      self.assertEqual(len(test_zero), 4)\n      for i in range(4):\n        self.assertEqual(test_zero[i][0].get_shape()[1], cell.state_size[i][0])\n        self.assertEqual(test_zero[i][1].get_shape()[1], cell.state_size[i][1])\n\n      with variable_scope.variable_scope(\"root\") as scope:\n        outputs_static, state_static = rnn.static_rnn(\n            cell,\n            inputs,\n            dtype=dtypes.float32,\n            sequence_length=sequence_length,\n            scope=scope)\n        scope.reuse_variables()\n        outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n            cell,\n            inputs_c,\n            dtype=dtypes.float32,\n            time_major=True,\n            sequence_length=sequence_length,\n            scope=scope)\n\n      if in_graph_mode:\n        input_value = np.random.randn(batch_size, input_size)\n        variables_lib.global_variables_initializer().run()\n        outputs_static = sess.run(\n            outputs_static, feed_dict={\n                inputs[0]: input_value\n            })\n        outputs_dynamic = sess.run(\n            outputs_dynamic, feed_dict={\n                inputs[0]: input_value\n            })\n        state_static = sess.run(\n            nest.flatten(state_static), feed_dict={\n                inputs[0]: input_value\n            })\n        state_dynamic = sess.run(\n            nest.flatten(state_dynamic), feed_dict={\n                inputs[0]: input_value\n            })\n\n      comparison_fn = self.assertAllEqual\n      if test_util.is_xla_enabled():\n        comparison_fn = self.assertAllClose\n      if in_graph_mode:\n        comparison_fn(outputs_static, outputs_dynamic)\n      else:\n        self.assertAllEqual(array_ops.stack(outputs_static), outputs_dynamic)\n        state_static = nest.flatten(state_static)\n        state_dynamic = nest.flatten(state_dynamic)\n      comparison_fn(np.hstack(state_static), np.hstack(state_dynamic))\n\n  def _testDynamicEquivalentToStaticRNN(self, use_sequence_length):\n    time_steps = 8\n    num_units = 3\n    num_proj = 4\n    input_size = 5\n    batch_size = 2\n\n    input_values = np.random.randn(time_steps, batch_size, input_size).astype(\n        np.float32)\n\n    if use_sequence_length:\n      sequence_length = np.random.randint(0, time_steps, size=batch_size)\n    else:\n      sequence_length = None\n\n    in_graph_mode = not context.executing_eagerly()\n\n    # TODO(b/68017812): Eager ignores operation seeds, so we need to create a\n    # single cell and reuse it across the static and dynamic RNNs. Remove this\n    # special case once is fixed.\n    if not in_graph_mode:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          initializer=initializer,\n          num_proj=num_proj,\n          state_is_tuple=False)\n\n    ########### Step 1: Run static graph and generate readouts\n    with self.session(graph=ops.Graph()) as sess:\n      if in_graph_mode:\n        concat_inputs = array_ops.placeholder(\n            dtypes.float32, shape=(time_steps, batch_size, input_size))\n      else:\n        concat_inputs = constant_op.constant(input_values)\n      inputs = array_ops.unstack(concat_inputs)\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n\n      # TODO(akshayka): Remove special case once b/68017812 is fixed.\n      if in_graph_mode:\n        cell = rnn_cell.LSTMCell(\n            num_units,\n            use_peepholes=True,\n            initializer=initializer,\n            num_proj=num_proj,\n            state_is_tuple=False)\n\n      with variable_scope.variable_scope(\"dynamic_scope\"):\n        outputs_static, state_static = rnn.static_rnn(\n            cell, inputs, sequence_length=sequence_length, dtype=dtypes.float32)\n\n      if in_graph_mode:\n        # Generate gradients of sum of outputs w.r.t. inputs\n        static_gradients = gradients_impl.gradients(\n            outputs_static + [state_static], [concat_inputs])\n        # Generate gradients of individual outputs w.r.t. inputs\n        static_individual_gradients = nest.flatten([\n            gradients_impl.gradients(y, [concat_inputs])\n            for y in [outputs_static[0], outputs_static[-1], state_static]\n        ])\n        # Generate gradients of individual variables w.r.t. inputs\n        trainable_variables = ops.get_collection(\n            ops.GraphKeys.TRAINABLE_VARIABLES)\n        assert len(trainable_variables) > 1, (\n            \"Count of trainable variables: %d\" % len(trainable_variables))\n        # pylint: disable=bad-builtin\n        static_individual_variable_gradients = nest.flatten([\n            gradients_impl.gradients(y, trainable_variables)\n            for y in [outputs_static[0], outputs_static[-1], state_static]\n        ])\n        # Generate gradients and run sessions to obtain outputs\n        feeds = {concat_inputs: input_values}\n        # Initialize\n        variables_lib.global_variables_initializer().run(feed_dict=feeds)\n        # Test forward pass\n        values_static = sess.run(outputs_static, feed_dict=feeds)\n        (state_value_static,) = sess.run((state_static,), feed_dict=feeds)\n\n        # Test gradients to inputs and variables w.r.t. outputs & final state\n        static_grad_values = sess.run(static_gradients, feed_dict=feeds)\n\n        static_individual_grad_values = sess.run(\n            static_individual_gradients, feed_dict=feeds)\n\n        static_individual_var_grad_values = sess.run(\n            static_individual_variable_gradients, feed_dict=feeds)\n\n    ########## Step 2: Run dynamic graph and generate readouts\n    with self.session(graph=ops.Graph()) as sess:\n      if in_graph_mode:\n        concat_inputs = array_ops.placeholder(\n            dtypes.float32, shape=(time_steps, batch_size, input_size))\n      else:\n        concat_inputs = constant_op.constant(input_values)\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n\n      # TODO(akshayka): Remove this special case once b/68017812 is\n      # fixed.\n      if in_graph_mode:\n        cell = rnn_cell.LSTMCell(\n            num_units,\n            use_peepholes=True,\n            initializer=initializer,\n            num_proj=num_proj,\n            state_is_tuple=False)\n\n      with variable_scope.variable_scope(\"dynamic_scope\"):\n        outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n            cell,\n            inputs=concat_inputs,\n            sequence_length=sequence_length,\n            time_major=True,\n            dtype=dtypes.float32)\n        split_outputs_dynamic = array_ops.unstack(outputs_dynamic, time_steps)\n\n      if in_graph_mode:\n\n        # Generate gradients of sum of outputs w.r.t. inputs\n        dynamic_gradients = gradients_impl.gradients(\n            split_outputs_dynamic + [state_dynamic], [concat_inputs])\n\n        # Generate gradients of several individual outputs w.r.t. inputs\n        dynamic_individual_gradients = nest.flatten([\n            gradients_impl.gradients(y, [concat_inputs])\n            for y in [\n                split_outputs_dynamic[0], split_outputs_dynamic[-1],\n                state_dynamic\n            ]\n        ])\n\n        # Generate gradients of individual variables w.r.t. inputs\n        trainable_variables = ops.get_collection(\n            ops.GraphKeys.TRAINABLE_VARIABLES)\n        assert len(trainable_variables) > 1, (\n            \"Count of trainable variables: %d\" % len(trainable_variables))\n        dynamic_individual_variable_gradients = nest.flatten([\n            gradients_impl.gradients(y, trainable_variables)\n            for y in [\n                split_outputs_dynamic[0], split_outputs_dynamic[-1],\n                state_dynamic\n            ]\n        ])\n\n        feeds = {concat_inputs: input_values}\n\n        # Initialize\n        variables_lib.global_variables_initializer().run(feed_dict=feeds)\n\n        # Test forward pass\n        values_dynamic = sess.run(split_outputs_dynamic, feed_dict=feeds)\n        (state_value_dynamic,) = sess.run((state_dynamic,), feed_dict=feeds)\n\n        # Test gradients to inputs and variables w.r.t. outputs & final state\n        dynamic_grad_values = sess.run(dynamic_gradients, feed_dict=feeds)\n\n        dynamic_individual_grad_values = sess.run(\n            dynamic_individual_gradients, feed_dict=feeds)\n\n        dynamic_individual_var_grad_values = sess.run(\n            dynamic_individual_variable_gradients, feed_dict=feeds)\n\n    ######### Step 3: Comparisons\n    if not in_graph_mode:\n      values_static = outputs_static\n      values_dynamic = split_outputs_dynamic\n      state_value_static = state_static\n      state_value_dynamic = state_dynamic\n\n    self.assertEqual(len(values_static), len(values_dynamic))\n    for (value_static, value_dynamic) in zip(values_static, values_dynamic):\n      self.assertAllClose(value_static, value_dynamic)\n    self.assertAllClose(state_value_static, state_value_dynamic)\n\n    if in_graph_mode:\n\n      self.assertAllClose(static_grad_values, dynamic_grad_values)\n\n      self.assertEqual(\n          len(static_individual_grad_values),\n          len(dynamic_individual_grad_values))\n      self.assertEqual(\n          len(static_individual_var_grad_values),\n          len(dynamic_individual_var_grad_values))\n\n      for i, (a, b) in enumerate(\n          zip(static_individual_grad_values, dynamic_individual_grad_values)):\n        tf_logging.info(\"Comparing individual gradients iteration %d\" % i)\n        self.assertAllClose(a, b)\n\n      for i, (a, b) in enumerate(\n          zip(static_individual_var_grad_values,\n              dynamic_individual_var_grad_values)):\n        tf_logging.info(\n            \"Comparing individual variable gradients iteration %d\" % i)\n        self.assertAllClose(a, b)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDynamicEquivalentToStaticRNN(self):\n    self._testDynamicEquivalentToStaticRNN(use_sequence_length=False)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDynamicEquivalentToStaticRNNWithSequenceLength(self):\n    self._testDynamicEquivalentToStaticRNN(use_sequence_length=True)\n\n\nclass BidirectionalRNNTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  def _createBidirectionalRNN(self, use_shape, use_sequence_length, scope=None):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n\n    initializer = init_ops.random_uniform_initializer(\n        -0.01, 0.01, seed=self._seed)\n    sequence_length = array_ops.placeholder(\n        dtypes.int64) if use_sequence_length else None\n    cell_fw = rnn_cell.LSTMCell(\n        num_units, input_size, initializer=initializer, state_is_tuple=False)\n    cell_bw = rnn_cell.LSTMCell(\n        num_units, input_size, initializer=initializer, state_is_tuple=False)\n    inputs = max_length * [\n        array_ops.placeholder(\n            dtypes.float32,\n            shape=(batch_size, input_size) if use_shape else (None, input_size))\n    ]\n    outputs, state_fw, state_bw = rnn.static_bidirectional_rnn(\n        cell_fw,\n        cell_bw,\n        inputs,\n        dtype=dtypes.float32,\n        sequence_length=sequence_length,\n        scope=scope)\n    self.assertEqual(len(outputs), len(inputs))\n    for out in outputs:\n      self.assertEqual(out.get_shape().as_list(),\n                       [batch_size if use_shape else None, 2 * num_units])\n\n    input_value = np.random.randn(batch_size, input_size)\n    outputs = array_ops.stack(outputs)\n\n    return input_value, inputs, outputs, state_fw, state_bw, sequence_length\n\n  def _testBidirectionalRNN(self, use_shape):\n    with self.session(graph=ops.Graph()) as sess:\n      input_value, inputs, outputs, state_fw, state_bw, sequence_length = (\n          self._createBidirectionalRNN(use_shape, True))\n      variables_lib.global_variables_initializer().run()\n      # Run with pre-specified sequence length of 2, 3\n      out, s_fw, s_bw = sess.run(\n          [outputs, state_fw, state_bw],\n          feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n\n      # Since the forward and backward LSTM cells were initialized with the\n      # same parameters, the forward and backward output has to be the same,\n      # but reversed in time. The format is output[time][batch][depth], and\n      # due to depth concatenation (as num_units=3 for both RNNs):\n      # - forward output:  out[][][depth] for 0 <= depth < 3\n      # - backward output: out[][][depth] for 4 <= depth < 6\n      #\n      # First sequence in batch is length=2\n      # Check that the time=0 forward output is equal to time=1 backward output\n      self.assertAllClose(out[0][0][0], out[1][0][3])\n      self.assertAllClose(out[0][0][1], out[1][0][4])\n      self.assertAllClose(out[0][0][2], out[1][0][5])\n      # Check that the time=1 forward output is equal to time=0 backward output\n      self.assertAllClose(out[1][0][0], out[0][0][3])\n      self.assertAllClose(out[1][0][1], out[0][0][4])\n      self.assertAllClose(out[1][0][2], out[0][0][5])\n\n      # Second sequence in batch is length=3\n      # Check that the time=0 forward output is equal to time=2 backward output\n      self.assertAllClose(out[0][1][0], out[2][1][3])\n      self.assertAllClose(out[0][1][1], out[2][1][4])\n      self.assertAllClose(out[0][1][2], out[2][1][5])\n      # Check that the time=1 forward output is equal to time=1 backward output\n      self.assertAllClose(out[1][1][0], out[1][1][3])\n      self.assertAllClose(out[1][1][1], out[1][1][4])\n      self.assertAllClose(out[1][1][2], out[1][1][5])\n      # Check that the time=2 forward output is equal to time=0 backward output\n      self.assertAllClose(out[2][1][0], out[0][1][3])\n      self.assertAllClose(out[2][1][1], out[0][1][4])\n      self.assertAllClose(out[2][1][2], out[0][1][5])\n      # Via the reasoning above, the forward and backward final state should be\n      # exactly the same\n      self.assertAllClose(s_fw, s_bw)\n\n  def _testBidirectionalRNNWithoutSequenceLength(self, use_shape):\n    with self.session(graph=ops.Graph()) as sess:\n      input_value, inputs, outputs, state_fw, state_bw, _ = (\n          self._createBidirectionalRNN(use_shape, False))\n      variables_lib.global_variables_initializer().run()\n      out, s_fw, s_bw = sess.run(\n          [outputs, state_fw, state_bw], feed_dict={\n              inputs[0]: input_value\n          })\n\n      # Since the forward and backward LSTM cells were initialized with the\n      # same parameters, the forward and backward output has to be the same,\n      # but reversed in time. The format is output[time][batch][depth], and\n      # due to depth concatenation (as num_units=3 for both RNNs):\n      # - forward output:  out[][][depth] for 0 <= depth < 3\n      # - backward output: out[][][depth] for 4 <= depth < 6\n      #\n      # Both sequences in batch are length=8.  Check that the time=i\n      # forward output is equal to time=8-1-i backward output\n      for i in range(8):\n        self.assertAllClose(out[i][0][0:3], out[8 - 1 - i][0][3:6])\n        self.assertAllClose(out[i][1][0:3], out[8 - 1 - i][1][3:6])\n      # Via the reasoning above, the forward and backward final state should be\n      # exactly the same\n      self.assertAllClose(s_fw, s_bw)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalRNN(self):\n    self._testBidirectionalRNN(use_shape=False)\n    self._testBidirectionalRNN(use_shape=True)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalRNNWithoutSequenceLength(self):\n    self._testBidirectionalRNNWithoutSequenceLength(use_shape=False)\n    self._testBidirectionalRNNWithoutSequenceLength(use_shape=True)\n\n  def _createBidirectionalDynamicRNN(self,\n                                     use_shape,\n                                     use_state_tuple,\n                                     use_time_major,\n                                     use_sequence_length,\n                                     scope=None):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n\n    initializer = init_ops.random_uniform_initializer(\n        -0.01, 0.01, seed=self._seed)\n    sequence_length = (\n        array_ops.placeholder(dtypes.int64) if use_sequence_length else None)\n    cell_fw = rnn_cell.LSTMCell(\n        num_units, initializer=initializer, state_is_tuple=use_state_tuple)\n    cell_bw = rnn_cell.LSTMCell(\n        num_units, initializer=initializer, state_is_tuple=use_state_tuple)\n    inputs = max_length * [\n        array_ops.placeholder(\n            dtypes.float32,\n            shape=(batch_size if use_shape else None, input_size))\n    ]\n    inputs_c = array_ops.stack(inputs)\n    if not use_time_major:\n      inputs_c = array_ops.transpose(inputs_c, [1, 0, 2])\n    outputs, states = rnn.bidirectional_dynamic_rnn(\n        cell_fw,\n        cell_bw,\n        inputs_c,\n        sequence_length,\n        dtype=dtypes.float32,\n        time_major=use_time_major,\n        scope=scope)\n    outputs = array_ops.concat(outputs, 2)\n    state_fw, state_bw = states\n    outputs_shape = [None, max_length, 2 * num_units]\n    if use_shape:\n      outputs_shape[0] = batch_size\n    if use_time_major:\n      outputs_shape[0], outputs_shape[1] = outputs_shape[1], outputs_shape[0]\n    self.assertEqual(outputs.get_shape().as_list(), outputs_shape)\n\n    input_value = np.random.randn(batch_size, input_size)\n\n    return input_value, inputs, outputs, state_fw, state_bw, sequence_length\n\n  def _testBidirectionalDynamicRNN(self, use_shape, use_state_tuple,\n                                   use_time_major, use_sequence_length):\n    with self.session(graph=ops.Graph()) as sess:\n      input_value, inputs, outputs, state_fw, state_bw, sequence_length = (\n          self._createBidirectionalDynamicRNN(\n              use_shape, use_state_tuple, use_time_major, use_sequence_length))\n      variables_lib.global_variables_initializer().run()\n      # Run with pre-specified sequence length of 2, 3\n      feed_dict = ({sequence_length: [2, 3]} if use_sequence_length else {})\n      feed_dict.update({inputs[0]: input_value})\n      if use_state_tuple:\n        out, c_fw, m_fw, c_bw, m_bw = sess.run(\n            [outputs, state_fw[0], state_fw[1], state_bw[0], state_bw[1]],\n            feed_dict=feed_dict)\n        s_fw = (c_fw, m_fw)\n        s_bw = (c_bw, m_bw)\n      else:\n        feed_dict.update({inputs[0]: input_value})\n        out, s_fw, s_bw = sess.run(\n            [outputs, state_fw, state_bw], feed_dict=feed_dict)\n\n      # Since the forward and backward LSTM cells were initialized with the\n      # same parameters, the forward and backward output has to be the same,\n      # but reversed in time. The format is output[time][batch][depth], and\n      # due to depth concatenation (as num_units=3 for both RNNs):\n      # - forward output:  out[][][depth] for 0 <= depth < 3\n      # - backward output: out[][][depth] for 4 <= depth < 6\n      #\n      if not use_time_major:\n        out = np.swapaxes(out, 0, 1)\n\n      if use_sequence_length:\n        # First sequence in batch is length=2\n        # Check that the t=0 forward output is equal to t=1 backward output\n        self.assertEqual(out[0][0][0], out[1][0][3])\n        self.assertEqual(out[0][0][1], out[1][0][4])\n        self.assertEqual(out[0][0][2], out[1][0][5])\n        # Check that the t=1 forward output is equal to t=0 backward output\n        self.assertEqual(out[1][0][0], out[0][0][3])\n        self.assertEqual(out[1][0][1], out[0][0][4])\n        self.assertEqual(out[1][0][2], out[0][0][5])\n\n        # Second sequence in batch is length=3\n        # Check that the t=0 forward output is equal to t=2 backward output\n        self.assertEqual(out[0][1][0], out[2][1][3])\n        self.assertEqual(out[0][1][1], out[2][1][4])\n        self.assertEqual(out[0][1][2], out[2][1][5])\n        # Check that the t=1 forward output is equal to t=1 backward output\n        self.assertEqual(out[1][1][0], out[1][1][3])\n        self.assertEqual(out[1][1][1], out[1][1][4])\n        self.assertEqual(out[1][1][2], out[1][1][5])\n        # Check that the t=2 forward output is equal to t=0 backward output\n        self.assertEqual(out[2][1][0], out[0][1][3])\n        self.assertEqual(out[2][1][1], out[0][1][4])\n        self.assertEqual(out[2][1][2], out[0][1][5])\n        # Via the reasoning above, the forward and backward final state should\n        # be exactly the same\n        self.assertAllClose(s_fw, s_bw)\n      else:  # not use_sequence_length\n        max_length = 8  # from createBidirectionalDynamicRNN\n        for t in range(max_length):\n          self.assertAllEqual(out[t, :, 0:3], out[max_length - t - 1, :, 3:6])\n        self.assertAllClose(s_fw, s_bw)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalDynamicRNN(self):\n    # Generate 2^5 option values\n    # from [True, True, True, True, True] to [False, False, False, False, False]\n    options = itertools.product([True, False], repeat=4)\n    for option in options:\n      self._testBidirectionalDynamicRNN(\n          use_shape=option[0],\n          use_state_tuple=option[1],\n          use_time_major=option[2],\n          use_sequence_length=option[3])\n\n  def _testScope(self, factory, prefix=\"prefix\", use_outer_scope=True):\n    # REMARKS: factory(scope) is a function accepting a scope\n    #          as an argument, such scope can be None, a string\n    #          or a VariableScope instance.\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          factory(scope)\n      else:\n        factory(prefix)\n\n      # check that all the variables names starts\n      # with the proper scope.\n      variables_lib.global_variables_initializer()\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"bidirectional_rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"BiRNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalRNNScope(self):\n\n    def factory(scope):\n      return self._createBidirectionalRNN(\n          use_shape=True, use_sequence_length=True, scope=scope)\n\n    self._testScope(factory, use_outer_scope=True)\n    self._testScope(factory, use_outer_scope=False)\n    self._testScope(factory, prefix=None, use_outer_scope=False)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalDynamicRNNScope(self):\n\n    def get_factory(use_time_major):\n\n      def factory(scope):\n        return self._createBidirectionalDynamicRNN(\n            use_shape=True,\n            use_state_tuple=True,\n            use_sequence_length=True,\n            use_time_major=use_time_major,\n            scope=scope)\n\n      return factory\n\n    self._testScope(get_factory(True), use_outer_scope=True)\n    self._testScope(get_factory(True), use_outer_scope=False)\n    self._testScope(get_factory(True), prefix=None, use_outer_scope=False)\n    self._testScope(get_factory(False), use_outer_scope=True)\n    self._testScope(get_factory(False), use_outer_scope=False)\n    self._testScope(get_factory(False), prefix=None, use_outer_scope=False)\n\n\nclass MultiDimensionalLSTMTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testMultiDimensionalLSTMAllRNNContainers(self):\n    feature_dims = (3, 4, 5)\n    input_size = feature_dims\n    batch_size = 2\n    max_length = 8\n    sequence_length = [4, 6]\n    with self.session(graph=ops.Graph()) as sess:\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None,) + input_size)\n      ]\n      inputs_using_dim = max_length * [\n          array_ops.placeholder(\n              dtypes.float32, shape=(batch_size,) + input_size)\n      ]\n      inputs_c = array_ops.stack(inputs)\n      # Create a cell for the whole test. This is fine because the cell has no\n      # variables.\n      cell = DummyMultiDimensionalLSTM(feature_dims)\n      state_saver = TestStateSaver(batch_size, input_size)\n      outputs_static, state_static = rnn.static_rnn(\n          cell, inputs, dtype=dtypes.float32, sequence_length=sequence_length)\n      outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n          cell,\n          inputs_c,\n          dtype=dtypes.float32,\n          time_major=True,\n          sequence_length=sequence_length)\n      outputs_bid, state_fw, state_bw = rnn.static_bidirectional_rnn(\n          cell,\n          cell,\n          inputs_using_dim,\n          dtype=dtypes.float32,\n          sequence_length=sequence_length)\n      outputs_sav, state_sav = rnn.static_state_saving_rnn(\n          cell,\n          inputs_using_dim,\n          sequence_length=sequence_length,\n          state_saver=state_saver,\n          state_name=(\"h\", \"c\"))\n\n      self.assertEqual(outputs_dynamic.get_shape().as_list(),\n                       inputs_c.get_shape().as_list())\n      for out, inp in zip(outputs_static, inputs):\n        self.assertEqual(out.get_shape().as_list(), inp.get_shape().as_list())\n      for out, inp in zip(outputs_bid, inputs_using_dim):\n        input_shape_list = inp.get_shape().as_list()\n        # fwd and bwd activations are concatenated along the second dim.\n        input_shape_list[1] *= 2\n        self.assertEqual(out.get_shape().as_list(), input_shape_list)\n\n      variables_lib.global_variables_initializer().run()\n\n      input_total_size = (batch_size,) + input_size\n      input_value = np.random.randn(*input_total_size)\n      outputs_static_v = sess.run(\n          outputs_static, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs_dynamic_v = sess.run(\n          outputs_dynamic, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs_bid_v = sess.run(\n          outputs_bid, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n      outputs_sav_v = sess.run(\n          outputs_sav, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n\n      self.assertAllEqual(outputs_static_v, outputs_dynamic_v)\n      self.assertAllEqual(outputs_static_v, outputs_sav_v)\n      outputs_static_array = np.array(outputs_static_v)\n      outputs_static_array_double = np.concatenate(\n          (outputs_static_array, outputs_static_array), axis=2)\n      outputs_bid_array = np.array(outputs_bid_v)\n      self.assertAllEqual(outputs_static_array_double, outputs_bid_array)\n\n      state_static_v = sess.run(\n          state_static, feed_dict={\n              inputs[0]: input_value\n          })\n      state_dynamic_v = sess.run(\n          state_dynamic, feed_dict={\n              inputs[0]: input_value\n          })\n      state_bid_fw_v = sess.run(\n          state_fw, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n      state_bid_bw_v = sess.run(\n          state_bw, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n      state_sav_v = sess.run(\n          state_sav, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_dynamic_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_sav_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_bid_fw_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_bid_bw_v))\n\n\nclass NestedLSTMTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNestedIOLSTMAllRNNContainers(self):\n    input_size = 5\n    batch_size = 2\n    state_size = 6\n    max_length = 8\n    sequence_length = [4, 6]\n    with self.session(graph=ops.Graph()) as sess:\n      state_saver = TestStateSaver(batch_size, state_size)\n      single_input = (array_ops.placeholder(\n          dtypes.float32, shape=(None, input_size)),\n                      array_ops.placeholder(\n                          dtypes.float32, shape=(None, input_size)))\n      inputs = max_length * [single_input]\n      inputs_c = (array_ops.stack([input_[0] for input_ in inputs]),\n                  array_ops.stack([input_[1] for input_ in inputs]))\n      single_input_using_dim = (array_ops.placeholder(\n          dtypes.float32, shape=(batch_size, input_size)),\n                                array_ops.placeholder(\n                                    dtypes.float32,\n                                    shape=(batch_size, input_size)))\n      inputs_using_dim = max_length * [single_input_using_dim]\n\n      # Create a cell for the whole test. This is fine because the cell has no\n      # variables.\n      cell = NestedRNNCell()\n      outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n          cell,\n          inputs_c,\n          dtype=dtypes.float32,\n          time_major=True,\n          sequence_length=sequence_length)\n      outputs_static, state_static = rnn.static_rnn(\n          cell, inputs, dtype=dtypes.float32, sequence_length=sequence_length)\n      outputs_bid, state_fw, state_bw = rnn.static_bidirectional_rnn(\n          cell,\n          cell,\n          inputs_using_dim,\n          dtype=dtypes.float32,\n          sequence_length=sequence_length)\n      outputs_sav, state_sav = rnn.static_state_saving_rnn(\n          cell,\n          inputs_using_dim,\n          sequence_length=sequence_length,\n          state_saver=state_saver,\n          state_name=(\"h\", \"c\"))\n\n      def _assert_same_shape(input1, input2, double=False):\n        flat_input1 = nest.flatten(input1)\n        flat_input2 = nest.flatten(input2)\n        for inp1, inp2 in zip(flat_input1, flat_input2):\n          input_shape = inp1.get_shape().as_list()\n          if double:\n            input_shape[1] *= 2\n          self.assertEqual(input_shape, inp2.get_shape().as_list())\n\n      _assert_same_shape(inputs_c, outputs_dynamic)\n      _assert_same_shape(inputs, outputs_static)\n      _assert_same_shape(inputs_using_dim, outputs_sav)\n      _assert_same_shape(inputs_using_dim, outputs_bid, double=True)\n\n      variables_lib.global_variables_initializer().run()\n\n      input_total_size = (batch_size, input_size)\n      input_value = (np.random.randn(*input_total_size),\n                     np.random.randn(*input_total_size))\n      outputs_dynamic_v = sess.run(\n          outputs_dynamic, feed_dict={\n              single_input: input_value\n          })\n      outputs_static_v = sess.run(\n          outputs_static, feed_dict={\n              single_input: input_value\n          })\n      outputs_sav_v = sess.run(\n          outputs_sav, feed_dict={\n              single_input_using_dim: input_value\n          })\n      outputs_bid_v = sess.run(\n          outputs_bid, feed_dict={\n              single_input_using_dim: input_value\n          })\n\n      self.assertAllEqual(outputs_static_v,\n                          np.transpose(outputs_dynamic_v, (1, 0, 2, 3)))\n      self.assertAllEqual(outputs_static_v, outputs_sav_v)\n      outputs_static_array = np.array(outputs_static_v)\n      outputs_static_array_double = np.concatenate(\n          (outputs_static_array, outputs_static_array), axis=3)\n      outputs_bid_array = np.array(outputs_bid_v)\n      self.assertAllEqual(outputs_static_array_double, outputs_bid_array)\n\n      state_dynamic_v = sess.run(\n          state_dynamic, feed_dict={\n              single_input: input_value\n          })\n      state_static_v = sess.run(\n          state_static, feed_dict={\n              single_input: input_value\n          })\n      state_bid_fw_v = sess.run(\n          state_fw, feed_dict={\n              single_input_using_dim: input_value\n          })\n      state_bid_bw_v = sess.run(\n          state_bw, feed_dict={\n              single_input_using_dim: input_value\n          })\n      state_sav_v = sess.run(\n          state_sav, feed_dict={\n              single_input_using_dim: input_value\n          })\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_dynamic_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_sav_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_bid_fw_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_bid_bw_v))\n\n\nclass StateSaverRNNTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  def _factory(self, scope, state_saver):\n    num_units = state_saver.state_size // 2\n    batch_size = state_saver.batch_size\n    input_size = 5\n    max_length = 8\n    initializer = init_ops.random_uniform_initializer(\n        -0.01, 0.01, seed=self._seed)\n    cell = rnn_cell.LSTMCell(\n        num_units,\n        use_peepholes=False,\n        initializer=initializer,\n        state_is_tuple=False)\n    inputs = max_length * [\n        array_ops.zeros(dtype=dtypes.float32, shape=(batch_size, input_size))\n    ]\n    out, state = rnn.static_state_saving_rnn(\n        cell,\n        inputs,\n        state_saver=state_saver,\n        state_name=\"save_lstm\",\n        scope=scope)\n    return out, state, state_saver\n\n  def _testScope(self, prefix=\"prefix\", use_outer_scope=True):\n    num_units = 3\n    batch_size = 2\n    state_saver = TestStateSaver(batch_size, 2 * num_units)\n\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          self._factory(scope=scope, state_saver=state_saver)\n      else:\n        self._factory(scope=prefix, state_saver=state_saver)\n        variables_lib.global_variables_initializer()\n\n      # check that all the variables names starts\n      # with the proper scope.\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"RNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  def testStateSaverRNNScope(self):\n    self._testScope(use_outer_scope=True)\n    self._testScope(use_outer_scope=False)\n    self._testScope(prefix=None, use_outer_scope=False)\n\n  def testStateSaverCallsSaveState(self):\n    \"\"\"Test that number of calls to state and save_state is equal.\n\n    Test if the order of actual evaluating or skipping evaluation of out,\n    state tensors, which are the output tensors from static_state_saving_rnn,\n    have influence on number of calls to save_state and state methods of\n    state_saver object (the number of calls should be same.)\n    \"\"\"\n    self.skipTest(\"b/124196246 Breakage for sess.run([out, ...]): 2 != 1\")\n\n    num_units = 3\n    batch_size = 2\n    state_saver = TestStateSaverWithCounters(batch_size, 2 * num_units)\n    out, state, state_saver = self._factory(scope=None, state_saver=state_saver)\n\n    with self.cached_session() as sess:\n      sess.run(variables_lib.global_variables_initializer())\n      sess.run(variables_lib.local_variables_initializer())\n\n      _, _, num_state_calls, num_save_state_calls = sess.run([\n          out,\n          state,\n          state_saver.num_state_calls,\n          state_saver.num_save_state_calls])\n      self.assertEqual(num_state_calls, num_save_state_calls)\n\n      _, num_state_calls, num_save_state_calls = sess.run([\n          out,\n          state_saver.num_state_calls,\n          state_saver.num_save_state_calls])\n      self.assertEqual(num_state_calls, num_save_state_calls)\n\n      _, num_state_calls, num_save_state_calls = sess.run([\n          state,\n          state_saver.num_state_calls,\n          state_saver.num_save_state_calls])\n      self.assertEqual(num_state_calls, num_save_state_calls)\n\nclass GRUTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDynamic(self):\n    time_steps = 8\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n\n    input_values = np.random.randn(time_steps, batch_size, input_size)\n\n    sequence_length = np.random.randint(0, time_steps, size=batch_size)\n\n    with self.session(graph=ops.Graph()) as sess:\n      concat_inputs = array_ops.placeholder(\n          dtypes.float32, shape=(time_steps, batch_size, input_size))\n\n      cell = rnn_cell.GRUCell(num_units=num_units)\n\n      with variable_scope.variable_scope(\"dynamic_scope\"):\n        outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n            cell,\n            inputs=concat_inputs,\n            sequence_length=sequence_length,\n            time_major=True,\n            dtype=dtypes.float32)\n\n      feeds = {concat_inputs: input_values}\n\n      # Initialize\n      variables_lib.global_variables_initializer().run(feed_dict=feeds)\n\n      sess.run([outputs_dynamic, state_dynamic], feed_dict=feeds)\n\n  def _testScope(self, factory, prefix=\"prefix\", use_outer_scope=True):\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          factory(scope)\n      else:\n        factory(prefix)\n        variables_lib.global_variables_initializer()\n\n      # check that all the variables names starts\n      # with the proper scope.\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"RNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDynamicScope(self):\n    time_steps = 8\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    sequence_length = np.random.randint(0, time_steps, size=batch_size)\n\n    def factory(scope):\n      concat_inputs = array_ops.placeholder(\n          dtypes.float32, shape=(time_steps, batch_size, input_size))\n      cell = rnn_cell.GRUCell(num_units=num_units)\n      return rnn.dynamic_rnn(\n          cell,\n          inputs=concat_inputs,\n          sequence_length=sequence_length,\n          time_major=True,\n          dtype=dtypes.float32,\n          scope=scope)\n\n    self._testScope(factory, use_outer_scope=True)\n    self._testScope(factory, use_outer_scope=False)\n    self._testScope(factory, prefix=None, use_outer_scope=False)\n\n\nclass RawRNNTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def _testRawRNN(self, max_time):\n    with self.session(graph=ops.Graph()) as sess:\n      batch_size = 16\n      input_depth = 4\n      num_units = 3\n\n      inputs = array_ops.placeholder(\n          shape=(max_time, batch_size, input_depth), dtype=dtypes.float32)\n      sequence_length = array_ops.placeholder(\n          shape=(batch_size,), dtype=dtypes.int32)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, unused_loop_state):\n        emit_output = cell_output  # == None for time == 0\n        if cell_output is None:  # time == 0\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          next_state = cell_state  # copy state through\n        elements_finished = (time_ >= sequence_length)\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output, None)\n\n      reuse_scope = variable_scope.get_variable_scope()\n\n      outputs_ta, final_state, _ = rnn.raw_rnn(cell, loop_fn, scope=reuse_scope)\n      outputs = outputs_ta.stack()\n\n      reuse_scope.reuse_variables()\n      outputs_dynamic_rnn, final_state_dynamic_rnn = rnn.dynamic_rnn(\n          cell,\n          inputs,\n          time_major=True,\n          dtype=dtypes.float32,\n          sequence_length=sequence_length,\n          scope=reuse_scope)\n\n      variables = variables_lib.trainable_variables()\n      gradients = gradients_impl.gradients([outputs, final_state],\n                                           [inputs] + variables)\n      gradients_dynamic_rnn = gradients_impl.gradients(\n          [outputs_dynamic_rnn, final_state_dynamic_rnn], [inputs] + variables)\n\n      variables_lib.global_variables_initializer().run()\n\n      rand_input = np.random.randn(max_time, batch_size, input_depth)\n      if max_time == 0:\n        rand_seq_len = np.zeros(batch_size)\n      else:\n        rand_seq_len = np.random.randint(max_time, size=batch_size)\n\n      # To ensure same output lengths for dynamic_rnn and raw_rnn\n      rand_seq_len[0] = max_time\n\n      (outputs_val, outputs_dynamic_rnn_val, final_state_val,\n       final_state_dynamic_rnn_val) = sess.run(\n           [outputs, outputs_dynamic_rnn, final_state, final_state_dynamic_rnn],\n           feed_dict={\n               inputs: rand_input,\n               sequence_length: rand_seq_len\n           })\n\n      self.assertAllClose(outputs_dynamic_rnn_val, outputs_val)\n      self.assertAllClose(final_state_dynamic_rnn_val, final_state_val)\n\n      # NOTE: Because with 0 time steps, raw_rnn does not have shape\n      # information about the input, it is impossible to perform\n      # gradients comparisons as the gradients eval will fail.  So\n      # this case skips the gradients test.\n      if max_time > 0:\n        self.assertEqual(len(gradients), len(gradients_dynamic_rnn))\n        gradients_val = sess.run(\n            gradients,\n            feed_dict={\n                inputs: rand_input,\n                sequence_length: rand_seq_len\n            })\n        gradients_dynamic_rnn_val = sess.run(\n            gradients_dynamic_rnn,\n            feed_dict={\n                inputs: rand_input,\n                sequence_length: rand_seq_len\n            })\n        self.assertEqual(len(gradients_val), len(gradients_dynamic_rnn_val))\n        input_gradients_val = gradients_val[0]\n        input_gradients_dynamic_rnn_val = gradients_dynamic_rnn_val[0]\n        self.assertAllClose(input_gradients_val,\n                            input_gradients_dynamic_rnn_val)\n        for i in range(1, len(gradients_val)):\n          self.assertAllClose(gradients_dynamic_rnn_val[i], gradients_val[i])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRawRNNZeroLength(self):\n    # NOTE: Because with 0 time steps, raw_rnn does not have shape\n    # information about the input, it is impossible to perform\n    # gradients comparisons as the gradients eval will fail.  So this\n    # case skips the gradients test.\n    self._testRawRNN(max_time=0)\n\n  def testRawRNN(self):\n    self._testRawRNN(max_time=10)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testLoopState(self):\n    with self.session(graph=ops.Graph()):\n      max_time = 10\n      batch_size = 16\n      input_depth = 4\n      num_units = 3\n\n      inputs = np.random.randn(max_time, batch_size, input_depth)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, loop_state):\n        if cell_output is None:\n          loop_state = constant_op.constant([0])\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          loop_state = array_ops.stack([array_ops.squeeze(loop_state) + 1])\n          next_state = cell_state\n        emit_output = cell_output  # == None for time == 0\n        elements_finished = array_ops.tile([time_ >= max_time], [batch_size])\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output,\n                loop_state)\n\n      r = rnn.raw_rnn(cell, loop_fn)\n      loop_state = r[-1]\n      self.assertEqual([10], self.evaluate(loop_state))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testLoopStateWithTensorArray(self):\n    with self.session(graph=ops.Graph()):\n      max_time = 4\n      batch_size = 16\n      input_depth = 4\n      num_units = 3\n\n      inputs = np.random.randn(max_time, batch_size, input_depth)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, loop_state):\n        if cell_output is None:\n          loop_state = tensor_array_ops.TensorArray(\n              dynamic_size=True,\n              size=0,\n              dtype=dtypes.int32,\n              clear_after_read=False)\n          loop_state = loop_state.write(0, 1)\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          loop_state = loop_state.write(time_,\n                                        loop_state.read(time_ - 1) + time_)\n          next_state = cell_state\n        emit_output = cell_output  # == None for time == 0\n        elements_finished = array_ops.tile([time_ >= max_time], [batch_size])\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output,\n                loop_state)\n\n      r = rnn.raw_rnn(cell, loop_fn)\n      loop_state = r[-1]\n      loop_state = loop_state.stack()\n      self.assertAllEqual([1, 2, 2 + 2, 4 + 3, 7 + 4], loop_state)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testEmitDifferentStructureThanCellOutput(self):\n    with self.session(graph=ops.Graph()) as sess:\n      max_time = 10\n      batch_size = 16\n      input_depth = 4\n      num_units = 3\n\n      inputs = np.random.randn(max_time, batch_size, input_depth)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n      # Verify emit shapes may be unknown by feeding a placeholder that\n      # determines an emit shape.\n      unknown_dim = array_ops.placeholder(dtype=dtypes.int32)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, _):\n        if cell_output is None:\n          emit_output = (array_ops.zeros([2, 3], dtype=dtypes.int32),\n                         array_ops.zeros([unknown_dim], dtype=dtypes.int64))\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          emit_output = (array_ops.ones([batch_size, 2, 3], dtype=dtypes.int32),\n                         array_ops.ones(\n                             [batch_size, unknown_dim], dtype=dtypes.int64))\n          next_state = cell_state\n        elements_finished = array_ops.tile([time_ >= max_time], [batch_size])\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output, None)\n\n      r = rnn.raw_rnn(cell, loop_fn)\n      output_ta = r[0]\n      self.assertEqual(2, len(output_ta))\n      self.assertEqual([dtypes.int32, dtypes.int64],\n                       [ta.dtype for ta in output_ta])\n      output = [ta.stack() for ta in output_ta]\n      output_vals = sess.run(output, feed_dict={unknown_dim: 1})\n      self.assertAllEqual(\n          np.ones((max_time, batch_size, 2, 3), np.int32), output_vals[0])\n      self.assertAllEqual(\n          np.ones((max_time, batch_size, 1), np.int64), output_vals[1])\n\n  def _testScope(self, factory, prefix=\"prefix\", use_outer_scope=True):\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          factory(scope)\n      else:\n        factory(prefix)\n        variables_lib.global_variables_initializer()\n\n      # check that all the variables names starts\n      # with the proper scope.\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"RNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRawRNNScope(self):\n    max_time = 10\n    batch_size = 16\n    input_depth = 4\n    num_units = 3\n\n    def factory(scope):\n      inputs = array_ops.placeholder(\n          shape=(max_time, batch_size, input_depth), dtype=dtypes.float32)\n      sequence_length = array_ops.placeholder(\n          shape=(batch_size,), dtype=dtypes.int32)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, unused_loop_state):\n        emit_output = cell_output  # == None for time == 0\n        if cell_output is None:  # time == 0\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          next_state = cell_state\n\n        elements_finished = (time_ >= sequence_length)\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output, None)\n\n      return rnn.raw_rnn(cell, loop_fn, scope=scope)\n\n    self._testScope(factory, use_outer_scope=True)\n    self._testScope(factory, use_outer_scope=False)\n    self._testScope(factory, prefix=None, use_outer_scope=False)\n\n\nclass DeviceWrapperCell(rnn_cell.RNNCell):\n  \"\"\"Class to ensure cell calculation happens on a specific device.\"\"\"\n\n  def __init__(self, cell, device):\n    self._cell = cell\n    self._device = device\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  def __call__(self, input_, state, scope=None):\n    if self._device is not None:\n      with ops.device(self._device):\n        return self._cell(input_, state, scope=scope)\n    else:\n      return self._cell(input_, state, scope=scope)\n\n\nclass TensorArrayOnCorrectDeviceTest(test.TestCase):\n\n  def _execute_rnn_on(self,\n                      rnn_device=None,\n                      cell_device=None,\n                      input_device=None):\n    batch_size = 3\n    time_steps = 7\n    input_size = 5\n    num_units = 10\n\n    cell = rnn_cell.LSTMCell(num_units, use_peepholes=True)\n    gpu_cell = DeviceWrapperCell(cell, cell_device)\n    inputs = np.random.randn(batch_size, time_steps, input_size).astype(\n        np.float32)\n    sequence_length = np.random.randint(0, time_steps, size=batch_size)\n\n    if input_device is not None:\n      with ops.device(input_device):\n        inputs = constant_op.constant(inputs)\n\n    if rnn_device is not None:\n      with ops.device(rnn_device):\n        outputs, _ = rnn.dynamic_rnn(\n            gpu_cell,\n            inputs,\n            sequence_length=sequence_length,\n            dtype=dtypes.float32)\n    else:\n      outputs, _ = rnn.dynamic_rnn(\n          gpu_cell,\n          inputs,\n          sequence_length=sequence_length,\n          dtype=dtypes.float32)\n\n    with self.session() as sess:\n      opts = config_pb2.RunOptions(trace_level=config_pb2.RunOptions.FULL_TRACE)\n      run_metadata = config_pb2.RunMetadata()\n      variables_lib.global_variables_initializer().run()\n      sess.run(outputs, options=opts, run_metadata=run_metadata)\n\n    return run_metadata\n\n  def _retrieve_cpu_gpu_stats(self, run_metadata):\n    cpu_stats = None\n    gpu_stats = None\n    step_stats = run_metadata.step_stats\n    for ds in step_stats.dev_stats:\n      if \"cpu:0\" in ds.device[-5:].lower():\n        cpu_stats = ds.node_stats\n      if \"gpu:0\" == ds.device[-5:].lower():\n        gpu_stats = ds.node_stats\n    return cpu_stats, gpu_stats\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRNNOnCPUCellOnGPU(self):\n    if not test.is_gpu_available():\n      return  # Test requires access to a GPU\n\n    gpu_dev = test.gpu_device_name()\n    run_metadata = self._execute_rnn_on(\n        rnn_device=\"/cpu:0\", cell_device=gpu_dev)\n    cpu_stats, gpu_stats = self._retrieve_cpu_gpu_stats(run_metadata)\n\n    def _assert_in(op_str, in_stats, out_stats):\n      self.assertTrue(any(op_str in s.node_name for s in in_stats))\n      self.assertFalse(any(op_str in s.node_name for s in out_stats))\n\n    # Writes happen at output of RNN cell\n    _assert_in(\"TensorArrayWrite\", gpu_stats, cpu_stats)\n    # Gather happens on final TensorArray\n    _assert_in(\"TensorArrayGather\", gpu_stats, cpu_stats)\n    # Reads happen at input to RNN cell\n    _assert_in(\"TensorArrayRead\", cpu_stats, gpu_stats)\n    # Scatters happen to get initial input into TensorArray\n    _assert_in(\"TensorArrayScatter\", cpu_stats, gpu_stats)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRNNOnCPUCellOnCPU(self):\n    if not test.is_gpu_available():\n      return  # Test requires access to a GPU\n\n    gpu_dev = test.gpu_device_name()\n    run_metadata = self._execute_rnn_on(\n        rnn_device=\"/cpu:0\", cell_device=\"/cpu:0\", input_device=gpu_dev)\n    cpu_stats, gpu_stats = self._retrieve_cpu_gpu_stats(run_metadata)\n\n    def _assert_in(op_str, in_stats, out_stats):\n      self.assertTrue(any(op_str in s.node_name for s in in_stats))\n      self.assertFalse(any(op_str in s.node_name for s in out_stats))\n\n    # All TensorArray operations happen on CPU\n    _assert_in(\"TensorArray\", cpu_stats, gpu_stats)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testInputOnGPUCellNotDeclared(self):\n    if not test.is_gpu_available():\n      return  # Test requires access to a GPU\n\n    gpu_dev = test.gpu_device_name()\n    run_metadata = self._execute_rnn_on(input_device=gpu_dev)\n    cpu_stats, gpu_stats = self._retrieve_cpu_gpu_stats(run_metadata)\n\n    def _assert_in(op_str, in_stats, out_stats):\n      self.assertTrue(any(op_str in s.node_name for s in in_stats))\n      self.assertFalse(any(op_str in s.node_name for s in out_stats))\n\n    # Everything happens on GPU\n    _assert_in(\"TensorArray\", gpu_stats, cpu_stats)\n\n\nclass RNNCellTest(test.TestCase, parameterized.TestCase):\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicRNNCell(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m = array_ops.zeros([1, 2])\n        cell = rnn_cell_impl.BasicRNNCell(2)\n        g, _ = cell(x, m)\n        self.assertEqual([\n            \"root/basic_rnn_cell/%s:0\" % rnn_cell_impl._WEIGHTS_VARIABLE_NAME,\n            \"root/basic_rnn_cell/%s:0\" % rnn_cell_impl._BIAS_VARIABLE_NAME\n        ], [v.name for v in cell.trainable_variables])\n        self.assertFalse(cell.non_trainable_variables)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run([g], {\n            x: np.array([[1., 1.]]),\n            m: np.array([[0.1, 0.1]])\n        })\n        self.assertEqual(res[0].shape, (1, 2))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicRNNCellNotTrainable(self):\n    with self.cached_session() as sess:\n\n      def not_trainable_getter(getter, *args, **kwargs):\n        kwargs[\"trainable\"] = False\n        return getter(*args, **kwargs)\n\n      with variable_scope.variable_scope(\n          \"root\",\n          initializer=init_ops.constant_initializer(0.5),\n          custom_getter=not_trainable_getter):\n        x = array_ops.zeros([1, 2])\n        m = array_ops.zeros([1, 2])\n        cell = rnn_cell_impl.BasicRNNCell(2)\n        g, _ = cell(x, m)\n        self.assertFalse(cell.trainable_variables)\n        self.assertEqual([\n            \"root/basic_rnn_cell/%s:0\" % rnn_cell_impl._WEIGHTS_VARIABLE_NAME,\n            \"root/basic_rnn_cell/%s:0\" % rnn_cell_impl._BIAS_VARIABLE_NAME\n        ], [v.name for v in cell.non_trainable_variables])\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run([g], {\n            x: np.array([[1., 1.]]),\n            m: np.array([[0.1, 0.1]])\n        })\n        self.assertEqual(res[0].shape, (1, 2))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testGRUCell(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m = array_ops.zeros([1, 2])\n        g, _ = rnn_cell_impl.GRUCell(2)(x, m)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run([g], {\n            x: np.array([[1., 1.]]),\n            m: np.array([[0.1, 0.1]])\n        })\n        # Smoke test\n        self.assertAllClose(res[0], [[0.175991, 0.175991]])\n      with variable_scope.variable_scope(\n          \"other\", initializer=init_ops.constant_initializer(0.5)):\n        # Test GRUCell with input_size != num_units.\n        x = array_ops.zeros([1, 3])\n        m = array_ops.zeros([1, 2])\n        g, _ = rnn_cell_impl.GRUCell(2)(x, m)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run([g], {\n            x: np.array([[1., 1., 1.]]),\n            m: np.array([[0.1, 0.1]])\n        })\n        # Smoke test\n        self.assertAllClose(res[0], [[0.156736, 0.156736]])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicLSTMCell(self):\n    for dtype in [dtypes.float16, dtypes.float32]:\n      np_dtype = dtype.as_numpy_dtype\n      with self.session(graph=ops.Graph()) as sess:\n        with variable_scope.variable_scope(\n            \"root\", initializer=init_ops.constant_initializer(0.5)):\n          x = array_ops.zeros([1, 2], dtype=dtype)\n          m = array_ops.zeros([1, 8], dtype=dtype)\n          cell = rnn_cell_impl.MultiRNNCell(\n              [\n                  rnn_cell_impl.BasicLSTMCell(2, state_is_tuple=False)\n                  for _ in range(2)\n              ],\n              state_is_tuple=False)\n          self.assertEqual(cell.dtype, None)\n          self.assertIn(\"cell-0\", cell._trackable_children())\n          self.assertIn(\"cell-1\", cell._trackable_children())\n          cell.get_config()  # Should not throw an error\n          g, out_m = cell(x, m)\n          # Layer infers the input type.\n          self.assertEqual(cell.dtype, dtype.name)\n          expected_variable_names = [\n              \"root/multi_rnn_cell/cell_0/basic_lstm_cell/%s:0\" %\n              rnn_cell_impl._WEIGHTS_VARIABLE_NAME,\n              \"root/multi_rnn_cell/cell_0/basic_lstm_cell/%s:0\" %\n              rnn_cell_impl._BIAS_VARIABLE_NAME,\n              \"root/multi_rnn_cell/cell_1/basic_lstm_cell/%s:0\" %\n              rnn_cell_impl._WEIGHTS_VARIABLE_NAME,\n              \"root/multi_rnn_cell/cell_1/basic_lstm_cell/%s:0\" %\n              rnn_cell_impl._BIAS_VARIABLE_NAME\n          ]\n          self.assertEqual(expected_variable_names,\n                           [v.name for v in cell.trainable_variables])\n          self.assertFalse(cell.non_trainable_variables)\n          sess.run([variables_lib.global_variables_initializer()])\n          res = sess.run([g, out_m], {\n              x: np.array([[1., 1.]]),\n              m: 0.1 * np.ones([1, 8])\n          })\n          self.assertEqual(len(res), 2)\n          variables = variables_lib.global_variables()\n          self.assertEqual(expected_variable_names, [v.name for v in variables])\n          # The numbers in results were not calculated, this is just a\n          # smoke test.\n          self.assertAllClose(res[0], np.array(\n              [[0.240, 0.240]], dtype=np_dtype), 1e-2)\n          expected_mem = np.array(\n              [[0.689, 0.689, 0.448, 0.448, 0.398, 0.398, 0.240, 0.240]],\n              dtype=np_dtype)\n          self.assertAllClose(res[1], expected_mem, 1e-2)\n        with variable_scope.variable_scope(\n            \"other\", initializer=init_ops.constant_initializer(0.5)):\n          # Test BasicLSTMCell with input_size != num_units.\n          x = array_ops.zeros([1, 3], dtype=dtype)\n          m = array_ops.zeros([1, 4], dtype=dtype)\n          g, out_m = rnn_cell_impl.BasicLSTMCell(2, state_is_tuple=False)(x, m)\n          sess.run([variables_lib.global_variables_initializer()])\n          res = sess.run(\n              [g, out_m], {\n                  x: np.array([[1., 1., 1.]], dtype=np_dtype),\n                  m: 0.1 * np.ones([1, 4], dtype=np_dtype)\n              })\n          self.assertEqual(len(res), 2)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicLSTMCellDimension0Error(self):\n    \"\"\"Tests that dimension 0 in both(x and m) shape must be equal.\"\"\"\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        num_units = 2\n        state_size = num_units * 2\n        batch_size = 3\n        input_size = 4\n        x = array_ops.zeros([batch_size, input_size])\n        m = array_ops.zeros([batch_size - 1, state_size])\n        with self.assertRaises(ValueError):\n          g, out_m = rnn_cell_impl.BasicLSTMCell(\n              num_units, state_is_tuple=False)(x, m)\n          sess.run([variables_lib.global_variables_initializer()])\n          sess.run(\n              [g, out_m], {\n                  x: 1 * np.ones([batch_size, input_size]),\n                  m: 0.1 * np.ones([batch_size - 1, state_size])\n              })\n\n  def testBasicLSTMCellStateSizeError(self):\n    \"\"\"Tests that state_size must be num_units * 2.\"\"\"\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        num_units = 2\n        state_size = num_units * 3  # state_size must be num_units * 2\n        batch_size = 3\n        input_size = 4\n        x = array_ops.zeros([batch_size, input_size])\n        m = array_ops.zeros([batch_size, state_size])\n        with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n          g, out_m = rnn_cell_impl.BasicLSTMCell(\n              num_units, state_is_tuple=False)(x, m)\n          sess.run([variables_lib.global_variables_initializer()])\n          sess.run(\n              [g, out_m], {\n                  x: 1 * np.ones([batch_size, input_size]),\n                  m: 0.1 * np.ones([batch_size, state_size])\n              })\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicLSTMCellStateTupleType(self):\n    with self.cached_session():\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m0 = (array_ops.zeros([1, 2]),) * 2\n        m1 = (array_ops.zeros([1, 2]),) * 2\n        cell = rnn_cell_impl.MultiRNNCell(\n            [rnn_cell_impl.BasicLSTMCell(2) for _ in range(2)],\n            state_is_tuple=True)\n        self.assertTrue(isinstance(cell.state_size, tuple))\n        self.assertTrue(\n            isinstance(cell.state_size[0], rnn_cell_impl.LSTMStateTuple))\n        self.assertTrue(\n            isinstance(cell.state_size[1], rnn_cell_impl.LSTMStateTuple))\n\n        # Pass in regular tuples\n        _, (out_m0, out_m1) = cell(x, (m0, m1))\n        self.assertTrue(isinstance(out_m0, rnn_cell_impl.LSTMStateTuple))\n        self.assertTrue(isinstance(out_m1, rnn_cell_impl.LSTMStateTuple))\n\n        # Pass in LSTMStateTuples\n        variable_scope.get_variable_scope().reuse_variables()\n        zero_state = cell.zero_state(1, dtypes.float32)\n        self.assertTrue(isinstance(zero_state, tuple))\n        self.assertTrue(isinstance(zero_state[0], rnn_cell_impl.LSTMStateTuple))\n        self.assertTrue(isinstance(zero_state[1], rnn_cell_impl.LSTMStateTuple))\n        _, (out_m0, out_m1) = cell(x, zero_state)\n        self.assertTrue(isinstance(out_m0, rnn_cell_impl.LSTMStateTuple))\n        self.assertTrue(isinstance(out_m1, rnn_cell_impl.LSTMStateTuple))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicLSTMCellWithStateTuple(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m0 = array_ops.zeros([1, 4])\n        m1 = array_ops.zeros([1, 4])\n        cell = rnn_cell_impl.MultiRNNCell(\n            [\n                rnn_cell_impl.BasicLSTMCell(2, state_is_tuple=False)\n                for _ in range(2)\n            ],\n            state_is_tuple=True)\n        g, (out_m0, out_m1) = cell(x, (m0, m1))\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run(\n            [g, out_m0, out_m1], {\n                x: np.array([[1., 1.]]),\n                m0: 0.1 * np.ones([1, 4]),\n                m1: 0.1 * np.ones([1, 4])\n            })\n        self.assertEqual(len(res), 3)\n        # The numbers in results were not calculated, this is just a smoke test.\n        # Note, however, these values should match the original\n        # version having state_is_tuple=False.\n        self.assertAllClose(res[0], [[0.24024698, 0.24024698]])\n        expected_mem0 = np.array(\n            [[0.68967271, 0.68967271, 0.44848421, 0.44848421]])\n        expected_mem1 = np.array(\n            [[0.39897051, 0.39897051, 0.24024698, 0.24024698]])\n        self.assertAllClose(res[1], expected_mem0)\n        self.assertAllClose(res[2], expected_mem1)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testLSTMCell(self):\n    with self.cached_session() as sess:\n      num_units = 8\n      num_proj = 6\n      state_size = num_units + num_proj\n      batch_size = 3\n      input_size = 2\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([batch_size, input_size])\n        m = array_ops.zeros([batch_size, state_size])\n        cell = rnn_cell_impl.LSTMCell(\n            num_units=num_units,\n            num_proj=num_proj,\n            forget_bias=1.0,\n            state_is_tuple=False)\n        output, state = cell(x, m)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run(\n            [output, state], {\n                x: np.array([[1., 1.], [2., 2.], [3., 3.]]),\n                m: 0.1 * np.ones((batch_size, state_size))\n            })\n        self.assertEqual(len(res), 2)\n        # The numbers in results were not calculated, this is mostly just a\n        # smoke test.\n        self.assertEqual(res[0].shape, (batch_size, num_proj))\n        self.assertEqual(res[1].shape, (batch_size, state_size))\n        # Different inputs so different outputs and states\n        for i in range(1, batch_size):\n          self.assertTrue(\n              float(np.linalg.norm((res[0][0, :] - res[0][i, :]))) > 1e-6)\n          self.assertTrue(\n              float(np.linalg.norm((res[1][0, :] - res[1][i, :]))) > 1e-6)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testLSTMCellVariables(self):\n    with self.cached_session():\n      num_units = 8\n      num_proj = 6\n      state_size = num_units + num_proj\n      batch_size = 3\n      input_size = 2\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([batch_size, input_size])\n        m = array_ops.zeros([batch_size, state_size])\n        cell = rnn_cell_impl.LSTMCell(\n            num_units=num_units,\n            num_proj=num_proj,\n            forget_bias=1.0,\n            state_is_tuple=False)\n        cell(x, m)  # Execute to create variables\n      variables = variables_lib.global_variables()\n      self.assertEqual(variables[0].op.name, \"root/lstm_cell/kernel\")\n      self.assertEqual(variables[1].op.name, \"root/lstm_cell/bias\")\n      self.assertEqual(variables[2].op.name, \"root/lstm_cell/projection/kernel\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWrapperCheckpointing(self):\n    for wrapper_type in [\n        rnn_cell_impl.DropoutWrapper,\n        rnn_cell_impl.ResidualWrapper,\n        lambda cell: rnn_cell_impl.MultiRNNCell([cell])]:\n      cell = rnn_cell_impl.BasicRNNCell(1)\n      wrapper = wrapper_type(cell)\n      wrapper(array_ops.ones([1, 1]),\n              state=wrapper.zero_state(batch_size=1, dtype=dtypes.float32))\n      self.evaluate([v.initializer for v in cell.variables])\n      checkpoint = trackable_utils.Checkpoint(wrapper=wrapper)\n      prefix = os.path.join(self.get_temp_dir(), \"ckpt\")\n      self.evaluate(cell._bias.assign([40.]))\n      save_path = checkpoint.save(prefix)\n      self.evaluate(cell._bias.assign([0.]))\n      checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n      self.assertAllEqual([40.], self.evaluate(cell._bias))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testResidualWrapper(self):\n    wrapper_type = rnn_cell_impl.ResidualWrapper\n    x = ops.convert_to_tensor(np.array([[1., 1., 1.]]))\n    m = ops.convert_to_tensor(np.array([[0.1, 0.1, 0.1]]))\n    base_cell = rnn_cell_impl.GRUCell(\n        3, kernel_initializer=init_ops.constant_initializer(0.5),\n        bias_initializer=init_ops.constant_initializer(0.5))\n    g, m_new = base_cell(x, m)\n    wrapper_object = wrapper_type(base_cell)\n    wrapper_object.get_config()  # Should not throw an error\n\n    self.assertIn(\"cell\", wrapper_object._trackable_children())\n    self.assertIs(wrapper_object._trackable_children()[\"cell\"], base_cell)\n\n    g_res, m_new_res = wrapper_object(x, m)\n    self.evaluate([variables_lib.global_variables_initializer()])\n    res = self.evaluate([g, g_res, m_new, m_new_res])\n    # Residual connections\n    self.assertAllClose(res[1], res[0] + [1., 1., 1.])\n    # States are left untouched\n    self.assertAllClose(res[2], res[3])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testResidualWrapperWithSlice(self):\n    wrapper_type = rnn_cell_impl.ResidualWrapper\n    x = ops.convert_to_tensor(np.array([[1., 1., 1., 1., 1.]]))\n    m = ops.convert_to_tensor(np.array([[0.1, 0.1, 0.1]]))\n    base_cell = rnn_cell_impl.GRUCell(\n        3, kernel_initializer=init_ops.constant_initializer(0.5),\n        bias_initializer=init_ops.constant_initializer(0.5))\n    g, m_new = base_cell(x, m)\n\n    def residual_with_slice_fn(inp, out):\n      inp_sliced = array_ops.slice(inp, [0, 0], [-1, 3])\n      return inp_sliced + out\n\n    g_res, m_new_res = wrapper_type(\n        base_cell, residual_with_slice_fn)(x, m)\n    self.evaluate([variables_lib.global_variables_initializer()])\n    res_g, res_g_res, res_m_new, res_m_new_res = self.evaluate(\n        [g, g_res, m_new, m_new_res])\n    # Residual connections\n    self.assertAllClose(res_g_res, res_g + [1., 1., 1.])\n    # States are left untouched\n    self.assertAllClose(res_m_new, res_m_new_res)\n\n  def testDeviceWrapper(self):\n    wrapper_type = rnn_cell_impl.DeviceWrapper\n    x = array_ops.zeros([1, 3])\n    m = array_ops.zeros([1, 3])\n    cell = rnn_cell_impl.GRUCell(3)\n    wrapped_cell = wrapper_type(cell, \"/cpu:0\")\n    wrapped_cell.get_config()  # Should not throw an error\n    self.assertEqual(wrapped_cell._trackable_children()[\"cell\"], cell)\n\n    outputs, _ = wrapped_cell(x, m)\n    self.assertIn(\"cpu:0\", outputs.device.lower())\n\n  def _retrieve_cpu_gpu_stats(self, run_metadata):\n    cpu_stats = None\n    gpu_stats = None\n    step_stats = run_metadata.step_stats\n    for ds in step_stats.dev_stats:\n      if \"cpu:0\" in ds.device[-5:].lower():\n        cpu_stats = ds.node_stats\n      if \"gpu:0\" == ds.device[-5:].lower():\n        gpu_stats = ds.node_stats\n    return cpu_stats, gpu_stats\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDeviceWrapperDynamicExecutionNodesAreAllProperlyLocated(self):\n    if not test.is_gpu_available():\n      # Can't perform this test w/o a GPU\n      return\n\n    gpu_dev = test.gpu_device_name()\n    with self.session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 1, 3])\n        cell = rnn_cell_impl.DeviceWrapper(rnn_cell_impl.GRUCell(3), gpu_dev)\n        with ops.device(\"/cpu:0\"):\n          outputs, _ = rnn.dynamic_rnn(\n              cell=cell, inputs=x, dtype=dtypes.float32)\n        run_metadata = config_pb2.RunMetadata()\n        opts = config_pb2.RunOptions(\n            trace_level=config_pb2.RunOptions.FULL_TRACE)\n\n        sess.run([variables_lib.global_variables_initializer()])\n        _ = sess.run(outputs, options=opts, run_metadata=run_metadata)\n\n      cpu_stats, gpu_stats = self._retrieve_cpu_gpu_stats(run_metadata)\n      self.assertFalse([s for s in cpu_stats if \"gru_cell\" in s.node_name])\n      self.assertTrue([s for s in gpu_stats if \"gru_cell\" in s.node_name])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testMultiRNNCell(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m = array_ops.zeros([1, 4])\n        multi_rnn_cell = rnn_cell_impl.MultiRNNCell(\n            [rnn_cell_impl.GRUCell(2) for _ in range(2)],\n            state_is_tuple=False)\n        _, ml = multi_rnn_cell(x, m)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run(ml, {\n            x: np.array([[1., 1.]]),\n            m: np.array([[0.1, 0.1, 0.1, 0.1]])\n        })\n        # The numbers in results were not calculated, this is just a smoke test.\n        self.assertAllClose(res, [[0.175991, 0.175991, 0.13248, 0.13248]])\n        self.assertEqual(len(multi_rnn_cell.weights), 2 * 4)\n        self.assertTrue(\n            [x.dtype == dtypes.float32 for x in multi_rnn_cell.weights])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testMultiRNNCellWithStateTuple(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m_bad = array_ops.zeros([1, 4])\n        m_good = (array_ops.zeros([1, 2]), array_ops.zeros([1, 2]))\n\n        # Test incorrectness of state\n        with self.assertRaisesRegex(ValueError, \"Expected state .* a tuple\"):\n          rnn_cell_impl.MultiRNNCell(\n              [rnn_cell_impl.GRUCell(2) for _ in range(2)],\n              state_is_tuple=True)(x, m_bad)\n\n        _, ml = rnn_cell_impl.MultiRNNCell(\n            [rnn_cell_impl.GRUCell(2) for _ in range(2)],\n            state_is_tuple=True)(x, m_good)\n\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run(\n            ml, {\n                x: np.array([[1., 1.]]),\n                m_good[0]: np.array([[0.1, 0.1]]),\n                m_good[1]: np.array([[0.1, 0.1]])\n            })\n\n        # The numbers in results were not calculated, this is just a\n        # smoke test.  However, these numbers should match those of\n        # the test testMultiRNNCell.\n        self.assertAllClose(res[0], [[0.175991, 0.175991]])\n        self.assertAllClose(res[1], [[0.13248, 0.13248]])\n\n  def testDeviceWrapperSerialization(self):\n    wrapper_cls = rnn_cell_impl.DeviceWrapper\n    cell = rnn_cell_impl.LSTMCell(10)\n    wrapper = wrapper_cls(cell, \"/cpu:0\")\n    config = wrapper.get_config()\n\n    # Replace the cell in the config with real cell instance to work around the\n    # reverse keras dependency issue.\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertDictEqual(config, reconstructed_wrapper.get_config())\n    self.assertIsInstance(reconstructed_wrapper, wrapper_cls)\n\n  def testResidualWrapperSerialization(self):\n    wrapper_cls = rnn_cell_impl.ResidualWrapper\n    cell = rnn_cell_impl.LSTMCell(10)\n    wrapper = wrapper_cls(cell)\n    config = wrapper.get_config()\n\n    # Replace the cell in the config with real cell instance to work around the\n    # reverse keras dependency issue.\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertDictEqual(config, reconstructed_wrapper.get_config())\n    self.assertIsInstance(reconstructed_wrapper, wrapper_cls)\n\n    wrapper = wrapper_cls(cell, residual_fn=lambda i, o: i + i + o)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    # Assert the reconstructed function will perform the math correctly.\n    self.assertEqual(reconstructed_wrapper._residual_fn(1, 2), 4)\n\n    def residual_fn(inputs, outputs):\n      return inputs * 3 + outputs\n\n    wrapper = wrapper_cls(cell, residual_fn=residual_fn)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    # Assert the reconstructed function will perform the math correctly.\n    self.assertEqual(reconstructed_wrapper._residual_fn(1, 2), 5)\n\n  def testDropoutWrapperSerialization(self):\n    wrapper_cls = rnn_cell_impl.DropoutWrapper\n    cell = rnn_cell_impl.LSTMCell(10)\n    wrapper = wrapper_cls(cell)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertDictEqual(config, reconstructed_wrapper.get_config())\n    self.assertIsInstance(reconstructed_wrapper, wrapper_cls)\n\n    wrapper = wrapper_cls(cell, dropout_state_filter_visitor=lambda s: True)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertTrue(reconstructed_wrapper._dropout_state_filter(None))\n\n    def dropout_state_filter_visitor(unused_state):\n      return False\n\n    wrapper = wrapper_cls(\n        cell, dropout_state_filter_visitor=dropout_state_filter_visitor)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertFalse(reconstructed_wrapper._dropout_state_filter(None))\n\n  def testSavedModel(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/175887901\")\n\n    with self.cached_session():\n      root = tracking.AutoTrackable()\n      root.cell = rnn_cell_impl.LSTMCell(8)\n      @def_function.function(input_signature=[tensor_spec.TensorSpec([3, 8])])\n      def call(x):\n        state = root.cell.zero_state(3, dtype=x.dtype)\n        y, _ = root.cell(x, state)\n        return y\n      root.call = call\n      expected = root.call(array_ops.zeros((3, 8)))\n      self.evaluate(variables_lib.global_variables_initializer())\n\n      save_dir = os.path.join(self.get_temp_dir(), \"saved_model\")\n      save.save(root, save_dir)\n      loaded = load.load(save_dir)\n      self.evaluate(variables_lib.global_variables_initializer())\n      self.assertAllClose(\n          expected, loaded.call(array_ops.zeros((3, 8))))\n\n\n@test_util.run_all_in_graph_and_eager_modes\n@test_util.run_all_without_tensor_float_32(\n    \"Uses an LSTMCell, which calls matmul\")\nclass DropoutWrapperTest(test.TestCase, parameterized.TestCase):\n\n  def _testDropoutWrapper(self,\n                          batch_size=None,\n                          time_steps=None,\n                          parallel_iterations=None,\n                          wrapper_type=None,\n                          scope=\"root\",\n                          **kwargs):\n    if batch_size is None and time_steps is None:\n      # 2 time steps, batch size 1, depth 3\n      batch_size = 1\n      time_steps = 2\n      x = constant_op.constant(\n          [[[2., 2., 2.]], [[1., 1., 1.]]], dtype=dtypes.float32)\n      m = rnn_cell_impl.LSTMStateTuple(\n          *[constant_op.constant([[0.1, 0.1, 0.1]], dtype=dtypes.float32)] * 2)\n    else:\n      x = constant_op.constant(\n          np.random.randn(time_steps, batch_size, 3).astype(np.float32))\n      m = rnn_cell_impl.LSTMStateTuple(*[\n          constant_op.\n          constant([[0.1, 0.1, 0.1]] * batch_size, dtype=dtypes.float32)] * 2)\n    outputs, final_state = rnn.dynamic_rnn(\n        cell=wrapper_type(\n            rnn_cell_impl.LSTMCell(\n                3, initializer=init_ops.constant_initializer(0.5)),\n            dtype=x.dtype, **kwargs),\n        time_major=True,\n        parallel_iterations=parallel_iterations,\n        inputs=x,\n        initial_state=m,\n        scope=scope)\n    self.evaluate([variables_lib.global_variables_initializer()])\n    res = self.evaluate([outputs, final_state])\n    self.assertEqual(res[0].shape, (time_steps, batch_size, 3))\n    self.assertEqual(res[1].c.shape, (batch_size, 3))\n    self.assertEqual(res[1].h.shape, (batch_size, 3))\n    return res\n\n  def testDropoutWrapperProperties(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    cell = rnn_cell_impl.BasicRNNCell(10)\n    wrapper = wrapper_type(cell)\n    # Github issue 15810\n    self.assertEqual(wrapper.wrapped_cell, cell)\n    self.assertEqual(wrapper.state_size, 10)\n    self.assertEqual(wrapper.output_size, 10)\n\n  def testDropoutWrapperZeroState(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n\n    class _Cell(rnn_cell_impl.BasicRNNCell):\n\n      def zero_state(self, batch_size=None, dtype=None):\n        return \"wrapped_cell_zero_state\"\n    wrapper = wrapper_type(_Cell(10))\n    self.assertEqual(wrapper.zero_state(10, dtypes.float32),\n                     \"wrapped_cell_zero_state\")\n\n  def testDropoutWrapperKeepAllConstantInput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep = array_ops.ones([])\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep, output_keep_prob=keep, state_keep_prob=keep,\n        wrapper_type=wrapper_type)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    true_full_final_c = np.array(\n        [[1.949385, 1.949385, 1.949385]], dtype=np.float32)\n    self.assertAllClose(true_full_output, res[0])\n    self.assertAllClose(true_full_output[1], res[1].h)\n    self.assertAllClose(true_full_final_c, res[1].c)\n\n  def testDropoutWrapperKeepAll(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep = variable_scope.get_variable(\"all\", initializer=1.0)\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep, output_keep_prob=keep, state_keep_prob=keep,\n        wrapper_type=wrapper_type)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    true_full_final_c = np.array(\n        [[1.949385, 1.949385, 1.949385]], dtype=np.float32)\n    self.assertAllClose(true_full_output, res[0])\n    self.assertAllClose(true_full_output[1], res[1].h)\n    self.assertAllClose(true_full_final_c, res[1].c)\n\n  def testDropoutWrapperWithSeed(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_some = 0.5\n    random_seed.set_random_seed(2)\n    ## Use parallel_iterations = 1 in both calls to\n    ## _testDropoutWrapper to ensure the (per-time step) dropout is\n    ## consistent across both calls.  Otherwise the seed may not end\n    ## up being munged consistently across both graphs.\n    res_standard_1 = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        seed=10,\n        parallel_iterations=1,\n        wrapper_type=wrapper_type,\n        scope=\"root_1\")\n    random_seed.set_random_seed(2)\n    res_standard_2 = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        seed=10,\n        parallel_iterations=1,\n        wrapper_type=wrapper_type,\n        scope=\"root_2\")\n    self.assertAllClose(res_standard_1[0], res_standard_2[0])\n    self.assertAllClose(res_standard_1[1].c, res_standard_2[1].c)\n    self.assertAllClose(res_standard_1[1].h, res_standard_2[1].h)\n\n  def testDropoutWrapperKeepNoOutput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_all = variable_scope.get_variable(\"all\", initializer=1.0)\n    keep_none = variable_scope.get_variable(\"none\", initializer=1e-6)\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_all,\n        output_keep_prob=keep_none,\n        state_keep_prob=keep_all,\n        wrapper_type=wrapper_type)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    true_full_final_c = np.array(\n        [[1.949385, 1.949385, 1.949385]], dtype=np.float32)\n    self.assertAllClose(np.zeros(res[0].shape), res[0])\n    self.assertAllClose(true_full_output[1], res[1].h)\n    self.assertAllClose(true_full_final_c, res[1].c)\n\n  def testDropoutWrapperKeepNoStateExceptLSTMCellMemory(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_all = variable_scope.get_variable(\"all\", initializer=1.0)\n    keep_none = variable_scope.get_variable(\"none\", initializer=1e-6)\n    # Even though we dropout state, by default DropoutWrapper never\n    # drops out the memory (\"c\") term of an LSTMStateTuple.\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_all,\n        output_keep_prob=keep_all,\n        state_keep_prob=keep_none,\n        wrapper_type=wrapper_type)\n    true_c_state = np.array([[1.713925, 1.713925, 1.713925]], dtype=np.float32)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    self.assertAllClose(true_full_output[0], res[0][0])\n    # Second output is modified by zero input state\n    self.assertGreater(np.linalg.norm(true_full_output[1] - res[0][1]), 1e-4)\n    # h state has been set to zero\n    self.assertAllClose(np.zeros(res[1].h.shape), res[1].h)\n    # c state of an LSTMStateTuple is NEVER modified.\n    self.assertAllClose(true_c_state, res[1].c)\n\n  def testDropoutWrapperKeepNoInput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_all = variable_scope.get_variable(\"all\", initializer=1.0)\n    keep_none = variable_scope.get_variable(\"none\", initializer=1e-6)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    true_full_final_c = np.array(\n        [[1.949385, 1.949385, 1.949385]], dtype=np.float32)\n    # All outputs are different because inputs are zeroed out\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_none,\n        output_keep_prob=keep_all,\n        state_keep_prob=keep_all,\n        wrapper_type=wrapper_type)\n    self.assertGreater(np.linalg.norm(res[0] - true_full_output), 1e-4)\n    self.assertGreater(np.linalg.norm(res[1].h - true_full_output[1]), 1e-4)\n    self.assertGreater(np.linalg.norm(res[1].c - true_full_final_c), 1e-4)\n\n  def testDropoutWrapperRecurrentOutput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_some = 0.8\n    keep_all = variable_scope.get_variable(\"all\", initializer=1.0)\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_all,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_all,\n        variational_recurrent=True,\n        wrapper_type=wrapper_type,\n        input_size=3,\n        batch_size=5,\n        time_steps=7)\n    # Ensure the same dropout pattern for all time steps\n    output_mask = np.abs(res[0]) > 1e-6\n    for m in output_mask[1:]:\n      self.assertAllClose(output_mask[0], m)\n\n  def testDropoutWrapperRecurrentStateInputAndOutput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_some = 0.9\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        variational_recurrent=True,\n        wrapper_type=wrapper_type,\n        input_size=3,\n        batch_size=5,\n        time_steps=7)\n\n    # Smoke test for the state/input masks.\n    output_mask = np.abs(res[0]) > 1e-6\n    for time_step in output_mask:\n      # Ensure the same dropout output pattern for all time steps\n      self.assertAllClose(output_mask[0], time_step)\n      for batch_entry in time_step:\n        # Assert all batch entries get the same mask\n        self.assertAllClose(batch_entry, time_step[0])\n\n    # For state, ensure all batch entries have the same mask\n    state_c_mask = np.abs(res[1].c) > 1e-6\n    state_h_mask = np.abs(res[1].h) > 1e-6\n    for batch_entry in state_c_mask:\n      self.assertAllClose(batch_entry, state_c_mask[0])\n    for batch_entry in state_h_mask:\n      self.assertAllClose(batch_entry, state_h_mask[0])\n\n  def testDropoutWrapperRecurrentStateInputAndOutputWithSeed(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_some = 0.9\n    random_seed.set_random_seed(2347)\n    np.random.seed(23487)\n    res0 = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        variational_recurrent=True,\n        wrapper_type=wrapper_type,\n        input_size=3,\n        batch_size=5,\n        time_steps=7,\n        seed=-234987,\n        scope=\"root_0\")\n    random_seed.set_random_seed(2347)\n    np.random.seed(23487)\n    res1 = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        variational_recurrent=True,\n        wrapper_type=wrapper_type,\n        input_size=3,\n        batch_size=5,\n        time_steps=7,\n        seed=-234987,\n        scope=\"root_1\")\n\n    output_mask = np.abs(res0[0]) > 1e-6\n    for time_step in output_mask:\n      # Ensure the same dropout output pattern for all time steps\n      self.assertAllClose(output_mask[0], time_step)\n      for batch_entry in time_step:\n        # Assert all batch entries get the same mask\n        self.assertAllClose(batch_entry, time_step[0])\n\n    # For state, ensure all batch entries have the same mask\n    state_c_mask = np.abs(res0[1].c) > 1e-6\n    state_h_mask = np.abs(res0[1].h) > 1e-6\n    for batch_entry in state_c_mask:\n      self.assertAllClose(batch_entry, state_c_mask[0])\n    for batch_entry in state_h_mask:\n      self.assertAllClose(batch_entry, state_h_mask[0])\n\n    # Ensure seeded calculation is identical.\n    self.assertAllClose(res0[0], res1[0])\n    self.assertAllClose(res0[1].c, res1[1].c)\n    self.assertAllClose(res0[1].h, res1[1].h)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/rnn/lstm_ops.h\"\n\n#include <memory>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename T, GateLayout gate_layout>\nvoid LSTMBlockCellFpropWithEigen(\n    const LSTMBlockCell& cell, OpKernelContext* ctx, const CPUDevice& d,\n    const float forget_bias, const float cell_clip, bool use_peephole,\n    typename TTypes<T>::ConstMatrix x, typename TTypes<T>::ConstMatrix cs_prev,\n    typename TTypes<T>::ConstMatrix h_prev, typename TTypes<T>::ConstMatrix w,\n    typename TTypes<T>::ConstVec wci, typename TTypes<T>::ConstVec wcf,\n    typename TTypes<T>::ConstVec wco, typename TTypes<T>::ConstVec b,\n    typename TTypes<T>::Matrix xh, typename TTypes<T>::Matrix i,\n    typename TTypes<T>::Matrix cs, typename TTypes<T>::Matrix f,\n    typename TTypes<T>::Matrix o, typename TTypes<T>::Matrix ci,\n    typename TTypes<T>::Matrix co, typename TTypes<T>::Matrix gates,\n    typename TTypes<T>::Matrix h) {\n  // Concat xh = [x, h].\n  xh.slice(cell.xh_x_offsets(), cell.xh_x_extents()).device(d) = x;\n  xh.slice(cell.xh_h_offsets(), cell.xh_h_extents()).device(d) = h_prev;\n\n  // states1 = xh * w + b\n  typename TTypes<T>::ConstMatrix const_xh(xh.data(), xh.dimensions());\n  TensorBlasGemm<CPUDevice, T, false /* USE_CUBLAS */>::compute(\n      ctx, d, false, false, typename gemm_compute_type<T>::type(1.f), const_xh,\n      w, typename gemm_compute_type<T>::type(0.f), gates);\n  Eigen::array<Eigen::DenseIndex, 2> b_shape({1, b.dimensions()[0]});\n  Eigen::array<Eigen::DenseIndex, 2> broadcast_shape({cell.batch_size(), 1});\n  gates.device(d) += b.reshape(b_shape).broadcast(broadcast_shape);\n\n  Eigen::array<Eigen::DenseIndex, 2> p_shape({1, cell.cell_size()});\n  Eigen::array<Eigen::DenseIndex, 2> p_broadcast_shape({cell.batch_size(), 1});\n\n  // Input gate.\n  if (use_peephole) {\n    auto i_peep = cs_prev * wci.reshape(p_shape).broadcast(p_broadcast_shape);\n    i.device(d) =\n        (gates.slice(cell.gates_i_offsets(), cell.cell_extents()) + i_peep)\n            .sigmoid();\n  } else {\n    i.device(d) =\n        gates.slice(cell.gates_i_offsets(), cell.cell_extents()).sigmoid();\n  }\n\n  // Cell input.\n  ci.device(d) =\n      gates.slice(cell.gates_c_offsets(gate_layout), cell.cell_extents())\n          .tanh();\n\n  // Forget gate (w/ bias).\n  if (use_peephole) {\n    auto f_peep = cs_prev * wcf.reshape(p_shape).broadcast(p_broadcast_shape);\n    f.device(d) =\n        (gates.slice(cell.gates_f_offsets(gate_layout), cell.cell_extents()) +\n         f.constant(T(forget_bias)) + f_peep)\n            .sigmoid();\n  } else {\n    f.device(d) =\n        (gates.slice(cell.gates_f_offsets(gate_layout), cell.cell_extents()) +\n         f.constant(T(forget_bias)))\n            .sigmoid();\n  }\n\n  // cs = ci .* i + f .* cs_prev\n  cs.device(d) = i * ci + f * cs_prev;\n\n  if (cell_clip > 0.0f) {\n    cs.device(d) =\n        cs.binaryExpr(cs.constant(T(cell_clip)), Eigen::scalar_clip_op<T>());\n  }\n\n  // co = tanh(cs)\n  co.device(d) = cs.tanh();\n\n  // Output gate.\n  if (use_peephole) {\n    auto o_peep = cs * wco.reshape(p_shape).broadcast(p_broadcast_shape);\n    o.device(d) =\n        (gates.slice(cell.gates_o_offsets(), cell.cell_extents()) + o_peep)\n            .sigmoid();\n  } else {\n    o.device(d) =\n        gates.slice(cell.gates_o_offsets(), cell.cell_extents()).sigmoid();\n  }\n\n  // h = o .* co\n  h.device(d) = o * co;\n}\n\ntemplate <typename Device, typename T, GateLayout gate_layout>\nvoid LSTMBlockCellBpropWithEigen(\n    const LSTMBlockCell& cell, OpKernelContext* ctx, const Device& d,\n    bool use_peephole, typename TTypes<T>::ConstMatrix x,\n    typename TTypes<T>::ConstMatrix cs_prev,\n    typename TTypes<T>::ConstMatrix h_prev, typename TTypes<T>::ConstMatrix w,\n    typename TTypes<T>::ConstVec wci, typename TTypes<T>::ConstVec wcf,\n    typename TTypes<T>::ConstVec wco, typename TTypes<T>::ConstVec b,\n    typename TTypes<T>::ConstMatrix i, typename TTypes<T>::ConstMatrix cs,\n    typename TTypes<T>::ConstMatrix f, typename TTypes<T>::ConstMatrix o,\n    typename TTypes<T>::ConstMatrix ci, typename TTypes<T>::ConstMatrix co,\n    typename TTypes<T>::ConstMatrix cs_grad,\n    typename TTypes<T>::ConstMatrix h_grad, typename TTypes<T>::Matrix do_,\n    typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci,\n    typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di,\n    typename TTypes<T>::Matrix dgates, typename TTypes<T>::Matrix cs_prev_grad,\n    typename TTypes<T>::Vec wci_grad, typename TTypes<T>::Vec wcf_grad,\n    typename TTypes<T>::Vec wco_grad) {\n  // do[t] = sigm'(o[t]) .* dh[t] .* co[t]\n  do_.device(d) = o * (o.constant(T(1)) - o) * h_grad * co;\n\n  // dcs[t] += tanh'(cs[t]) .* dh[t] .* o[t] + dcs[t + 1] .* f[t + 1]\n  dcs.device(d) = (co.constant(T(1)) - co * co) * h_grad * o + cs_grad;\n\n  Eigen::array<Eigen::DenseIndex, 2> p_shape({1, cell.cell_size()});\n  Eigen::array<Eigen::DenseIndex, 2> p_broadcast_shape({cell.batch_size(), 1});\n  if (use_peephole) {\n    dcs.device(d) =\n        dcs + do_ * wco.reshape(p_shape).broadcast(p_broadcast_shape);\n  }\n\n  // dci[t] = tanh'(ci[t]) dcs[t] i[t]\n  dci.device(d) = (ci.constant(T(1)) - ci * ci) * dcs * i;\n\n  // df[t] = sigm'(f[t]) dcs[t] cs[t - 1]\n  df.device(d) = f * (f.constant(T(1)) - f) * dcs * cs_prev;\n\n  // di[t] = sigm'(i[t]) dcs[t] ci[t]\n  di.device(d) = i * (i.constant(T(1)) - i) * dcs * ci;\n\n  dgates.slice(cell.gates_i_offsets(), cell.cell_extents()).device(d) = di;\n  dgates.slice(cell.gates_c_offsets(gate_layout), cell.cell_extents())\n      .device(d) = dci;\n  dgates.slice(cell.gates_f_offsets(gate_layout), cell.cell_extents())\n      .device(d) = df;\n  dgates.slice(cell.gates_o_offsets(), cell.cell_extents()).device(d) = do_;\n\n  cs_prev_grad.device(d) = dcs * f;\n  if (use_peephole) {\n    cs_prev_grad.device(d) =\n        cs_prev_grad + di * wci.reshape(p_shape).broadcast(p_broadcast_shape) +\n        df * wcf.reshape(p_shape).broadcast(p_broadcast_shape);\n    wci_grad.device(d) = (di * cs_prev).sum(Eigen::array<int, 1>({0}));\n    wcf_grad.device(d) = (df * cs_prev).sum(Eigen::array<int, 1>({0}));\n    wco_grad.device(d) = (do_ * cs).sum(Eigen::array<int, 1>({0}));\n  }\n}\n\n#define DECLARE_CPU_FBPROP(T, GATE_LAYOUT)                                     \\\n  template <>                                                                  \\\n  void LSTMBlockCellFprop<CPUDevice, T, false /* USE_CUBLAS */, GATE_LAYOUT>:: \\\n  operator()(                                                                  \\\n      OpKernelContext* ctx, const CPUDevice& d, const float forget_bias,       \\\n      const float cell_clip, bool use_peephole,                                \\\n      typename TTypes<T>::ConstMatrix x,                                       \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                 \\\n      typename TTypes<T>::ConstMatrix h_prev,                                  \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,     \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,      \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh,           \\\n      typename TTypes<T>::Matrix i, typename TTypes<T>::Matrix cs,             \\\n      typename TTypes<T>::Matrix f, typename TTypes<T>::Matrix o,              \\\n      typename TTypes<T>::Matrix ci, typename TTypes<T>::Matrix co,            \\\n      typename TTypes<T>::Matrix gates, typename TTypes<T>::Matrix h) {        \\\n    LSTMBlockCellFpropWithEigen<T, GATE_LAYOUT>(                               \\\n        *this, ctx, d, forget_bias, cell_clip, use_peephole, x, cs_prev,       \\\n        h_prev, w, wci, wcf, wco, b, xh, i, cs, f, o, ci, co, gates, h);       \\\n  }                                                                            \\\n  template <>                                                                  \\\n  void LSTMBlockCellBprop<CPUDevice, T, false /* USE_CUBLAS */, GATE_LAYOUT>:: \\\n  operator()(                                                                  \\\n      OpKernelContext* ctx, const CPUDevice& d, bool use_peephole,             \\\n      typename TTypes<T>::ConstMatrix x,                                       \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                 \\\n      typename TTypes<T>::ConstMatrix h_prev,                                  \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,     \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,      \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::ConstMatrix i,       \\\n      typename TTypes<T>::ConstMatrix cs, typename TTypes<T>::ConstMatrix f,   \\\n      typename TTypes<T>::ConstMatrix o, typename TTypes<T>::ConstMatrix ci,   \\\n      typename TTypes<T>::ConstMatrix co,                                      \\\n      typename TTypes<T>::ConstMatrix cs_grad,                                 \\\n      typename TTypes<T>::ConstMatrix h_grad, typename TTypes<T>::Matrix do_,  \\\n      typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci,          \\\n      typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di,            \\\n      typename TTypes<T>::Matrix dgates,                                       \\\n      typename TTypes<T>::Matrix cs_prev_grad,                                 \\\n      typename TTypes<T>::Vec wci_grad, typename TTypes<T>::Vec wcf_grad,      \\\n      typename TTypes<T>::Vec wco_grad) {                                      \\\n    LSTMBlockCellBpropWithEigen<CPUDevice, T, GATE_LAYOUT>(                    \\\n        *this, ctx, d, use_peephole, x, cs_prev, h_prev, w, wci, wcf, wco, b,  \\\n        i, cs, f, o, ci, co, cs_grad, h_grad, do_, dcs, dci, df, di, dgates,   \\\n        cs_prev_grad, wci_grad, wcf_grad, wco_grad);                           \\\n  }                                                                            \\\n  template struct LSTMBlockCellFprop<CPUDevice, T, false /* USE_CUBLAS */,     \\\n                                     GATE_LAYOUT>;                             \\\n  template struct LSTMBlockCellBprop<CPUDevice, T, false /* USE_CUBLAS */,     \\\n                                     GATE_LAYOUT>;\n\n#define DECLARE_CPU_SPECS(T)   \\\n  DECLARE_CPU_FBPROP(T, ICFO); \\\n  DECLARE_CPU_FBPROP(T, IFCO);\n\nDECLARE_CPU_SPECS(Eigen::half);\nDECLARE_CPU_SPECS(float);\n#undef DECLARE_CPU_SPECS\n#undef DECLARE_CPU_FBPROP\n\n#if GOOGLE_CUDA\n#define DECLARE_GPU_FBPROP(T, GATE_LAYOUT)                                    \\\n  template <>                                                                 \\\n  void LSTMBlockCellFprop<GPUDevice, T, true, GATE_LAYOUT>::operator()(       \\\n      OpKernelContext* ctx, const GPUDevice& d, const float forget_bias,      \\\n      const float cell_clip, bool use_peephole,                               \\\n      typename TTypes<T>::ConstMatrix x,                                      \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                \\\n      typename TTypes<T>::ConstMatrix h_prev,                                 \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,    \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,     \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh,          \\\n      typename TTypes<T>::Matrix i, typename TTypes<T>::Matrix cs,            \\\n      typename TTypes<T>::Matrix f, typename TTypes<T>::Matrix o,             \\\n      typename TTypes<T>::Matrix ci, typename TTypes<T>::Matrix co,           \\\n      typename TTypes<T>::Matrix gates, typename TTypes<T>::Matrix h);        \\\n  template <>                                                                 \\\n  void LSTMBlockCellBprop<GPUDevice, T, true, GATE_LAYOUT>::operator()(       \\\n      OpKernelContext* ctx, const GPUDevice& d, bool use_peephole,            \\\n      typename TTypes<T>::ConstMatrix x,                                      \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                \\\n      typename TTypes<T>::ConstMatrix h_prev,                                 \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,    \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,     \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::ConstMatrix i,      \\\n      typename TTypes<T>::ConstMatrix cs, typename TTypes<T>::ConstMatrix f,  \\\n      typename TTypes<T>::ConstMatrix o, typename TTypes<T>::ConstMatrix ci,  \\\n      typename TTypes<T>::ConstMatrix co,                                     \\\n      typename TTypes<T>::ConstMatrix cs_grad,                                \\\n      typename TTypes<T>::ConstMatrix h_grad, typename TTypes<T>::Matrix do_, \\\n      typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci,         \\\n      typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di,           \\\n      typename TTypes<T>::Matrix dgates,                                      \\\n      typename TTypes<T>::Matrix cs_prev_grad,                                \\\n      typename TTypes<T>::Vec wci_grad, typename TTypes<T>::Vec wcf_grad,     \\\n      typename TTypes<T>::Vec wco_grad);                                      \\\n                                                                              \\\n  extern template struct LSTMBlockCellBprop<                                  \\\n      GPUDevice, T, true /* USE_CUBLAS */, GATE_LAYOUT>;                      \\\n  extern template struct LSTMBlockCellFprop<GPUDevice, T, true, GATE_LAYOUT>;\n\n#define DECLARE_GPU_SPECS(T) DECLARE_GPU_FBPROP(T, ICFO);\n\nDECLARE_GPU_SPECS(float);\nDECLARE_GPU_SPECS(Eigen::half);\n#undef DECLARE_GPU_SPECS\n#undef DECLARE_GPU_FBROP\n#endif  // GOOGLE_CUDA\n}  // namespace functor\n\ntemplate <typename Device, typename T, bool USE_CUBLAS, GateLayout gate_layout>\nclass LSTMBlockCellOp : public OpKernel {\n public:\n  explicit LSTMBlockCellOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_clip\", &cell_clip_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_peephole\", &use_peephole_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* x_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n\n    const Tensor* cs_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n\n    const Tensor* w_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n\n    const Tensor* wci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n\n    const Tensor* wcf_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n\n    const Tensor* wco_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n\n    const Tensor* b_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n\n    const int64_t batch_size = x_tensor->dim_size(0);\n    const int64_t input_size = x_tensor->dim_size(1);\n    const int64_t cell_size = cs_prev_tensor->dim_size(1);\n\n    // Sanity checks for our input shapes.\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"cs_prev.dims(0) != batch_size: \",\n                                        cs_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\"cs_prev.dims(1) != cell_size: \",\n                                        cs_prev_tensor->dim_size(1), \" vs. \",\n                                        cell_size));\n\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                        h_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size,\n                errors::InvalidArgument(\n                    \"w.dim_size(0) != input_size + cell_size: \",\n                    w_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"w.dim_size(1) != cell_size * 4: \", w_tensor->dim_size(1),\n                    \" vs. \", cell_size * 4));\n\n    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"b.dim_size(0) != cell_size * 4: \", b_tensor->dim_size(0),\n                    \" vs. \", cell_size * 4));\n\n    // Allocate our output tensors.\n    Tensor* i_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->forward_input_or_allocate_output(\n                            {\"h_prev\"}, \"i\",\n                            TensorShape({batch_size, cell_size}), &i_tensor));\n\n    Tensor* cs_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"cs\", TensorShape({batch_size, cell_size}),\n                                  &cs_tensor));\n\n    Tensor* f_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"f\", TensorShape({batch_size, cell_size}),\n                                  &f_tensor));\n\n    Tensor* o_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->forward_input_or_allocate_output(\n                            {\"cs_prev\"}, \"o\",\n                            TensorShape({batch_size, cell_size}), &o_tensor));\n\n    Tensor* ci_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"ci\", TensorShape({batch_size, cell_size}),\n                                  &ci_tensor));\n\n    Tensor* co_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"co\", TensorShape({batch_size, cell_size}),\n                                  &co_tensor));\n\n    Tensor* h_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"h\", TensorShape({batch_size, cell_size}),\n                                  &h_tensor));\n\n    // Allocate our temp tensors.\n    Tensor xh_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::v(),\n                            TensorShape({batch_size, input_size + cell_size}),\n                            &xh_tensor));\n\n    Tensor gates_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                      TensorShape({batch_size, cell_size * 4}),\n                                      &gates_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    // Sanity check that each of the tensors have the required NDIMS.\n    OP_REQUIRES(ctx, x_tensor->dims() == 2,\n                errors::InvalidArgument(\"x_tensor must be rank 2 but is rank \",\n                                        x_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, cs_prev_tensor->dims() == 2,\n        errors::InvalidArgument(\"cs_prev_tensor must be rank 2 but is rank \",\n                                cs_prev_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, h_prev_tensor->dims() == 2,\n        errors::InvalidArgument(\"h_prev_tensor must be rank 2 but is rank \",\n                                h_prev_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, w_tensor->dims() == 2,\n                errors::InvalidArgument(\"w_tensor must be rank 2 but is rank \",\n                                        w_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, wci_tensor->dims() == 1,\n        errors::InvalidArgument(\"wci_tensor must be rank 1 but is rank \",\n                                wci_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, wcf_tensor->dims() == 1,\n        errors::InvalidArgument(\"wcf_tensor must be rank 1 but is rank \",\n                                wci_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, wco_tensor->dims() == 1,\n        errors::InvalidArgument(\"wco_tensor must be rank 1 but is rank \",\n                                wco_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, b_tensor->dims() == 1,\n                errors::InvalidArgument(\"b_tensor must be rank 1 but is rank \",\n                                        b_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, xh_tensor.dims() == 2,\n                errors::InvalidArgument(\"xh_tensor must be rank 2 but is rank \",\n                                        xh_tensor.dims(), \".\"));\n    OP_REQUIRES(ctx, i_tensor->dims() == 2,\n                errors::InvalidArgument(\"i_tensor must be rank 2 but is rank \",\n                                        i_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, cs_tensor->dims() == 2,\n                errors::InvalidArgument(\"cs_tensor must be rank 2 but is rank \",\n                                        cs_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, f_tensor->dims() == 2,\n                errors::InvalidArgument(\"f_tensor must be rank 2 but is rank \",\n                                        f_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, o_tensor->dims() == 2,\n                errors::InvalidArgument(\"o_tensor must be rank 2 but is rank \",\n                                        o_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, ci_tensor->dims() == 2,\n                errors::InvalidArgument(\"ci_tensor must be rank 2 but is rank \",\n                                        ci_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, co_tensor->dims() == 2,\n                errors::InvalidArgument(\"co_tensor must be rank 2 but is rank \",\n                                        co_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, gates_tensor.dims() == 2,\n        errors::InvalidArgument(\"gates_tensor must be rank 2 but is rank \",\n                                gates_tensor.dims(), \".\"));\n    OP_REQUIRES(ctx, h_tensor->dims() == 2,\n                errors::InvalidArgument(\"h_tensor must be rank 2 but is rank \",\n                                        h_tensor->dims(), \".\"));\n\n    functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n        batch_size, input_size, cell_size)(\n        ctx, device, forget_bias_, cell_clip_, use_peephole_,\n        x_tensor->matrix<T>(), cs_prev_tensor->matrix<T>(),\n        h_prev_tensor->matrix<T>(), w_tensor->matrix<T>(), wci_tensor->vec<T>(),\n        wcf_tensor->vec<T>(), wco_tensor->vec<T>(), b_tensor->vec<T>(),\n        xh_tensor.matrix<T>(), i_tensor->matrix<T>(), cs_tensor->matrix<T>(),\n        f_tensor->matrix<T>(), o_tensor->matrix<T>(), ci_tensor->matrix<T>(),\n        co_tensor->matrix<T>(), gates_tensor.matrix<T>(),\n        h_tensor->matrix<T>());\n  }\n\n private:\n  float forget_bias_;\n  float cell_clip_;\n  bool use_peephole_;\n};\n\n#define REGISTER_KERNEL(T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                             \\\n      Name(\"LSTMBlockCell\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      LSTMBlockCellOp<CPUDevice, T, false, ICFO>);\n\nREGISTER_KERNEL(Eigen::half);\nREGISTER_KERNEL(float);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_KERNEL(T)                                         \\\n  REGISTER_KERNEL_BUILDER(                                             \\\n      Name(\"LSTMBlockCell\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n      LSTMBlockCellOp<GPUDevice, T, true, ICFO>);\n\nREGISTER_GPU_KERNEL(Eigen::half);\nREGISTER_GPU_KERNEL(float);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename T, bool USE_CUBLAS, GateLayout gate_layout>\nclass LSTMBlockCellGradOp : public OpKernel {\n public:\n  explicit LSTMBlockCellGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_peephole\", &use_peephole_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* x_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n\n    const Tensor* cs_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n\n    const Tensor* w_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n\n    const Tensor* wci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n\n    const Tensor* wcf_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n\n    const Tensor* wco_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n\n    const Tensor* b_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n\n    const Tensor* i_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"i\", &i_tensor));\n\n    const Tensor* cs_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs\", &cs_tensor));\n\n    const Tensor* f_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"f\", &f_tensor));\n\n    const Tensor* o_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"o\", &o_tensor));\n\n    const Tensor* ci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"ci\", &ci_tensor));\n\n    const Tensor* co_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"co\", &co_tensor));\n\n    const Tensor* cs_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_grad\", &cs_grad_tensor));\n\n    const Tensor* h_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_grad\", &h_grad_tensor));\n\n    const int64_t batch_size = x_tensor->dim_size(0);\n    const int64_t input_size = x_tensor->dim_size(1);\n    const int64_t cell_size = cs_prev_tensor->dim_size(1);\n\n    // Sanity checks for our input shapes.\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"cs_prev.dims(0) != batch_size: \",\n                                        cs_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\"cs_prev.dims(1) != cell_size: \",\n                                        cs_prev_tensor->dim_size(1), \" vs. \",\n                                        cell_size));\n\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                        h_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size,\n                errors::InvalidArgument(\n                    \"w.dim_size(0) != input_size + cell_size: \",\n                    w_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"w.dim_size(1) != cell_size * 4: \", w_tensor->dim_size(1),\n                    \" vs. \", cell_size * 4));\n\n    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"b.dim_size(0) != cell_size * 4: \", b_tensor->dim_size(0),\n                    \" vs. \", cell_size * 4));\n\n    OP_REQUIRES(ctx, i_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"i.dim_size(0) != batch_size: \", i_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, i_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"i.dim_size(1) != cell_size: \", i_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, cs_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"cs.dim_size(0) != batch_size: \", cs_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, cs_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"cs.dim_size(1) != cell_size: \", cs_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, f_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"f.dim_size(0) != batch_size: \", f_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, f_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"i.dim_size(1) != cell_size: \", f_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, o_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"o.dim_size(0) != batch_size: \", o_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, o_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"o.dim_size(1) != cell_size: \", o_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, ci_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"ci.dim_size(0) != batch_size: \", ci_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, ci_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"ci.dim_size(1) != cell_size: \", ci_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, co_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"co.dim_size(0) != batch_size: \", co_tensor->dim_size(0),\n                    \" vs. \", batch_size));\n    OP_REQUIRES(ctx, co_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"co.dim_size(1) != cell_size: \", co_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    OP_REQUIRES(ctx, cs_grad_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\n                    \"cs_grad_tensor.dims(0) != batch_size: \",\n                    cs_grad_tensor->dim_size(0), \" vs. \", batch_size));\n    OP_REQUIRES(ctx, cs_grad_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\"cs_grad_tensor.dims(1) != cell_size: \",\n                                        cs_grad_tensor->dim_size(1), \" vs. \",\n                                        cell_size));\n\n    OP_REQUIRES(ctx, h_grad_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_grad_tensor.dims(0) != batch_size: \",\n                                        h_grad_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, h_grad_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\"h_grad_tensor.dims(1) != cell_size: \",\n                                        h_grad_tensor->dim_size(1), \" vs. \",\n                                        cell_size));\n\n    // Allocate our output tensors.\n    Tensor* cs_prev_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->forward_input_or_allocate_output(\n                 {\"cs_grad\"}, \"cs_prev_grad\",\n                 TensorShape({batch_size, cell_size}), &cs_prev_grad_tensor));\n\n    Tensor* dgates_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\n                            \"dicfo\", TensorShape({batch_size, cell_size * 4}),\n                            &dgates_tensor));\n\n    Tensor* wci_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->forward_input_or_allocate_output(\n                 {\"wci\"}, \"wci_grad\", wci_tensor->shape(), &wci_grad_tensor));\n\n    Tensor* wcf_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->forward_input_or_allocate_output(\n                 {\"wcf\"}, \"wcf_grad\", wcf_tensor->shape(), &wcf_grad_tensor));\n\n    Tensor* wco_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->forward_input_or_allocate_output(\n                 {\"wco\"}, \"wco_grad\", wco_tensor->shape(), &wco_grad_tensor));\n\n    // Allocate our temp tensors.\n    Tensor do_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &do_tensor));\n\n    Tensor dcs_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &dcs_tensor));\n\n    Tensor dci_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &dci_tensor));\n\n    Tensor df_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &df_tensor));\n\n    Tensor di_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           TensorShape({batch_size, cell_size}),\n                                           &di_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    functor::TensorZero<Device, T>()(device, wci_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wcf_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wco_grad_tensor->flat<T>());\n\n    functor::LSTMBlockCellBprop<Device, T, USE_CUBLAS, gate_layout>(\n        batch_size, input_size, cell_size)(\n        ctx, device, use_peephole_, x_tensor->matrix<T>(),\n        cs_prev_tensor->matrix<T>(), h_prev_tensor->matrix<T>(),\n        w_tensor->matrix<T>(), wci_tensor->vec<T>(), wcf_tensor->vec<T>(),\n        wco_tensor->vec<T>(), b_tensor->vec<T>(), i_tensor->matrix<T>(),\n        cs_tensor->matrix<T>(), f_tensor->matrix<T>(), o_tensor->matrix<T>(),\n        ci_tensor->matrix<T>(), co_tensor->matrix<T>(),\n        cs_grad_tensor->matrix<T>(), h_grad_tensor->matrix<T>(),\n        do_tensor.matrix<T>(), dcs_tensor.matrix<T>(), dci_tensor.matrix<T>(),\n        df_tensor.matrix<T>(), di_tensor.matrix<T>(),\n        dgates_tensor->matrix<T>(), cs_prev_grad_tensor->matrix<T>(),\n        wci_grad_tensor->vec<T>(), wcf_grad_tensor->vec<T>(),\n        wco_grad_tensor->vec<T>());\n  }\n\n protected:\n  bool use_peephole_;\n};\n\n#define REGISTER_KERNEL(T)                                                 \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"LSTMBlockCellGrad\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      LSTMBlockCellGradOp<CPUDevice, T, false, ICFO>);\nREGISTER_KERNEL(float);\nREGISTER_KERNEL(Eigen::half);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_KERNEL(T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"LSTMBlockCellGrad\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n      LSTMBlockCellGradOp<GPUDevice, T, true, ICFO>);\n\nREGISTER_GPU_KERNEL(Eigen::half);\nREGISTER_GPU_KERNEL(float);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nnamespace {\n\n// This helper class can be used to access timeslices of a 3D tensor. If a slice\n// happens to be unaligned (usually because both batch size and number of cells\n// are odd - this isn't common) this involves overhead, since data needs to be\n// copied. However, if all slices are aligned, the bits aren't copied. In the\n// cases where copying is needed, the outputs have to be recopied back.\n// At the end of each time step you should call FinishTimeStep which does this,\n// and also allows for reuse of temporary tensors.\ntemplate <typename Device, typename T>\nclass SliceHelper {\n public:\n  explicit SliceHelper(OpKernelContext* ctx)\n      : ctx_(ctx), device_(ctx_->eigen_device<Device>()) {}\n\n  ~SliceHelper() {\n    CHECK(copy_out_.empty());\n    for (const auto& entry : pool_) {\n      CHECK(!entry.second.second);  // nothing is in use\n    }\n  }\n\n  // Slice through an input tensor. This may copy unaligned slices, but no\n  // copying back will be done at the end.\n  const Tensor InputSlice(const Tensor& t, int pos, const string& name) {\n    Tensor res = UnalignedSlice(t, pos);\n    if (res.IsAligned()) {\n      return res;\n    } else {\n      return AlignTensor(res, name);\n    }\n  }\n\n  // Slice through an output tensor. This may copy unaligned slices, and\n  // schedule copying back on destruction.\n  Tensor OutputSlice(Tensor* t, int pos, const string& name) {\n    Tensor res = UnalignedSlice(*t, pos);\n    if (res.IsAligned()) {\n      return res;\n    } else {\n      Tensor aligned = AlignTensor(res, name);\n      copy_out_.emplace_back(res, aligned);\n      return aligned;\n    }\n  }\n\n  void FinishTimeStep() {\n    for (const auto& p : copy_out_) {\n      const Tensor& aligned = p.second;\n      Tensor original = p.first;\n      // Copy from aligned back to original.\n      functor::TensorCopyToUnaligned<Device, T>()(device_, aligned.flat<T>(),\n                                                  original.unaligned_flat<T>());\n    }\n    copy_out_.clear();\n    // Mark all entries as not in use.\n    for (auto& entry : pool_) {\n      entry.second.second = false;\n    }\n  }\n\n private:\n  // Return a slice at position 'pos'. Result may be unaligned. The resulting\n  // tensor always shares data with the source tensor.\n  Tensor UnalignedSlice(const Tensor& t, int pos) const {\n    Tensor res;\n    // CHECK should never fail here, since the number of elements must match\n    CHECK(res.CopyFrom(t.Slice(pos, pos + 1), {t.dim_size(1), t.dim_size(2)}));\n    return res;\n  }\n\n  // Assumes input is not aligned, creates a temporary aligned tensor of the\n  // same shape and copies the original tensor's content into it.\n  Tensor AlignTensor(const Tensor& t, const string& name) {\n    VLOG(1) << \"AlignTensor called for \" << name << \", shape \"\n            << t.shape().DebugString()\n            << \". This is unnecessary copying. Consider using shapes with even \"\n            << \"sizes\";\n    Tensor aligned;\n    auto found = pool_.find(name);\n    if (found != pool_.end()) {  // found in pool\n      CHECK(!found->second.second) << \"Tensor \" << name << \" is in use\";\n      found->second.second = true;  // mark in use\n      aligned = found->second.first;\n      CHECK(aligned.shape().IsSameSize(t.shape()));\n      CHECK_EQ(aligned.dtype(), t.dtype());\n    } else {  // allocate a new temporary tensor\n      TF_CHECK_OK(ctx_->allocate_temp(t.dtype(), t.shape(), &aligned));\n      pool_.emplace(name, std::make_pair(aligned, true));\n    }\n    functor::TensorCopyUnaligned<Device, T>()(device_, t.unaligned_flat<T>(),\n                                              aligned.flat<T>());\n    return aligned;\n  }\n\n  // Tensors to be copied.\n  std::vector<std::pair<Tensor, const Tensor>> copy_out_;\n  // A pool of pre-allocated temporary tensors, with an indicator for whether\n  // it's in use.\n  std::map<string, std::pair<Tensor, bool>> pool_;\n  // Op context\n  OpKernelContext* ctx_ = nullptr;\n  // Device\n  const Device& device_;\n};\n\n}  // namespace\n\ntemplate <typename Device, typename T, bool USE_CUBLAS, GateLayout gate_layout>\nclass BlockLSTMOp : public OpKernel {\n public:\n  explicit BlockLSTMOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    if (ctx->HasAttr(\"forget_bias\")) {\n      OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n    } else {\n      // V2 version does not have \"forget_bias\" attribute.\n      forget_bias_ = 0.0;\n    }\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_clip\", &cell_clip_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_peephole\", &use_peephole_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* seq_len_max_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"seq_len_max\", &seq_len_max_tensor));\n\n    const Tensor* x;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));\n    OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));\n    const int64_t timelen = x->dim_size(0);\n    const int64_t batch_size = x->dim_size(1);\n    const int64_t input_size = x->dim_size(2);\n\n    const Tensor* cs_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n    OP_REQUIRES(ctx, cs_prev_tensor->dims() == 2,\n                errors::InvalidArgument(\"cs_prev must be 2D\"));\n    OP_REQUIRES(ctx, cs_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"cs_prev.dims(0) != batch_size: \",\n                                        cs_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    const int64_t cell_size = cs_prev_tensor->dim_size(1);\n\n    if (batch_size * input_size % 2 == 1) {\n      LOG(WARNING) << \"BlockLSTMOp is inefficient when both batch_size and \"\n                   << \"input_size are odd. You are using: batch_size=\"\n                   << batch_size << \", input_size=\" << input_size;\n    }\n    if (batch_size * cell_size % 2 == 1) {\n      LOG(WARNING) << \"BlockLSTMOp is inefficient when both batch_size and \"\n                   << \"cell_size are odd. You are using: batch_size=\"\n                   << batch_size << \", cell_size=\" << cell_size;\n    }\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n    OP_REQUIRES(ctx, h_prev_tensor->dims() == 2,\n                errors::InvalidArgument(\"h_prev must be 2D\"));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                        h_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument(\n                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n                    \" vs. \", cell_size));\n\n    const Tensor* w_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n    OP_REQUIRES(ctx, w_tensor->dims() == 2,\n                errors::InvalidArgument(\"w must be 2D\"));\n    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size,\n                errors::InvalidArgument(\n                    \"w.dim_size(0) != input_size + cell_size: \",\n                    w_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"w.dim_size(1) != cell_size * 4: \", w_tensor->dim_size(1),\n                    \" vs. \", cell_size * 4));\n\n    const Tensor* wci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n    OP_REQUIRES(ctx, wci_tensor->dims() == 1,\n                errors::InvalidArgument(\"wci must be 1D\"));\n    OP_REQUIRES(ctx, wci_tensor->dim_size(0) == cell_size,\n                errors::InvalidArgument(\n                    \"wci.dim_size(0) != cell_size: \", wci_tensor->dim_size(0),\n                    \" vs. \", cell_size));\n\n    const Tensor* wcf_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n    OP_REQUIRES(ctx, wcf_tensor->dims() == 1,\n                errors::InvalidArgument(\"wcf must be 1D\"));\n    OP_REQUIRES(ctx, wcf_tensor->dim_size(0) == cell_size,\n                errors::InvalidArgument(\n                    \"wcf.dim_size(0) != cell_size: \", wcf_tensor->dim_size(0),\n                    \" vs. \", cell_size));\n\n    const Tensor* wco_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n    OP_REQUIRES(ctx, wco_tensor->dims() == 1,\n                errors::InvalidArgument(\"wco must be 1D\"));\n    OP_REQUIRES(ctx, wco_tensor->dim_size(0) == cell_size,\n                errors::InvalidArgument(\n                    \"wco.dim_size(0) != cell_size: \", wco_tensor->dim_size(0),\n                    \" vs. \", cell_size));\n\n    const Tensor* b_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n    OP_REQUIRES(ctx, b_tensor->dims() == 1,\n                errors::InvalidArgument(\"b must be 1D\"));\n    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size * 4,\n                errors::InvalidArgument(\n                    \"b.dim_size(0) != cell_size * 4: \", b_tensor->dim_size(0),\n                    \" vs. \", cell_size * 4));\n\n    TensorShape batch_cell_shape({timelen, batch_size, cell_size});\n    Tensor* i_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"i\", batch_cell_shape, &i_out));\n\n    Tensor* cs_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"cs\", batch_cell_shape, &cs_out));\n\n    Tensor* f_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"f\", batch_cell_shape, &f_out));\n\n    Tensor* o_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"o\", batch_cell_shape, &o_out));\n\n    Tensor* ci_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"ci\", batch_cell_shape, &ci_out));\n\n    Tensor* co_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"co\", batch_cell_shape, &co_out));\n\n    Tensor* h_out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"h\", batch_cell_shape, &h_out));\n\n    Tensor xh_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::v(),\n                            TensorShape({batch_size, input_size + cell_size}),\n                            &xh_tensor));\n\n    Tensor gates_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                      TensorShape({batch_size, cell_size * 4}),\n                                      &gates_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    const int64_t seq_len_max = seq_len_max_tensor->scalar<int64_t>()();\n    SliceHelper<Device, T> slicer(ctx);\n    for (int64_t t = 0; t < seq_len_max; ++t) {\n      const Tensor x_tensor = slicer.InputSlice(*x, t, \"x\");\n      const Tensor& cs_prev_tensor2 =\n          t == 0 ? *cs_prev_tensor\n                 : slicer.OutputSlice(cs_out, t - 1, \"cs_prev\");\n      const Tensor& h_prev_tensor2 =\n          t == 0 ? *h_prev_tensor : slicer.OutputSlice(h_out, t - 1, \"h_prev\");\n\n      Tensor i_tensor = slicer.OutputSlice(i_out, t, \"i_out\");\n      Tensor cs_tensor = slicer.OutputSlice(cs_out, t, \"cs_out\");\n      Tensor f_tensor = slicer.OutputSlice(f_out, t, \"f_out\");\n      Tensor o_tensor = slicer.OutputSlice(o_out, t, \"o_out\");\n      Tensor ci_tensor = slicer.OutputSlice(ci_out, t, \"ci_out\");\n      Tensor co_tensor = slicer.OutputSlice(co_out, t, \"co_out\");\n      Tensor h_tensor = slicer.OutputSlice(h_out, t, \"h_out\");\n\n      functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n          batch_size, input_size, cell_size)(\n          ctx, device, forget_bias_, cell_clip_, use_peephole_,\n          x_tensor.matrix<T>(), cs_prev_tensor2.matrix<T>(),\n          h_prev_tensor2.matrix<T>(), w_tensor->matrix<T>(),\n          wci_tensor->vec<T>(), wcf_tensor->vec<T>(), wco_tensor->vec<T>(),\n          b_tensor->vec<T>(), xh_tensor.matrix<T>(), i_tensor.matrix<T>(),\n          cs_tensor.matrix<T>(), f_tensor.matrix<T>(), o_tensor.matrix<T>(),\n          ci_tensor.matrix<T>(), co_tensor.matrix<T>(),\n          gates_tensor.matrix<T>(), h_tensor.matrix<T>());\n      slicer.FinishTimeStep();\n    }\n\n    if (seq_len_max < timelen) {\n      Tensor cs_tensor = cs_out->Slice(seq_len_max, timelen);\n      Tensor h_tensor = h_out->Slice(seq_len_max, timelen);\n\n      functor::TensorUnalignedZero<Device, T>()(device,\n                                                cs_tensor.unaligned_flat<T>());\n      functor::TensorUnalignedZero<Device, T>()(device,\n                                                h_tensor.unaligned_flat<T>());\n    }\n  }\n\n private:\n  float forget_bias_;\n  float cell_clip_;\n  bool use_peephole_;\n};\n\n#define REGISTER_KERNEL(T)                                           \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(\"BlockLSTM\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"),   \\\n      BlockLSTMOp<CPUDevice, T, false, ICFO>);                       \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(\"BlockLSTMV2\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      BlockLSTMOp<CPUDevice, T, false, IFCO>);\n\nREGISTER_KERNEL(Eigen::half);\nREGISTER_KERNEL(float);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nnamespace functor {\n#define DECLARE_GPU_SPECS(T)                                             \\\n  template <>                                                            \\\n  void TensorZero<GPUDevice, T>::operator()(const GPUDevice& d,          \\\n                                            typename TTypes<T>::Flat t); \\\n                                                                         \\\n  extern template struct TensorZero<GPUDevice, T>;                       \\\n                                                                         \\\n  template <>                                                            \\\n  void TensorUnalignedZero<GPUDevice, T>::operator()(                    \\\n      const GPUDevice& d, typename TTypes<T>::UnalignedFlat t);          \\\n                                                                         \\\n  extern template struct TensorUnalignedZero<GPUDevice, T>;\n\nDECLARE_GPU_SPECS(Eigen::half);\nDECLARE_GPU_SPECS(float);\n#undef DECLARE_GPU_SPECS\n}  // end namespace functor\n\n#define REGISTER_GPU_KERNEL(T)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"BlockLSTM\")                       \\\n                              .Device(DEVICE_GPU)                 \\\n                              .HostMemory(\"seq_len_max\")          \\\n                              .TypeConstraint<T>(\"T\"),            \\\n                          BlockLSTMOp<GPUDevice, T, true, ICFO>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"BlockLSTMV2\")                     \\\n                              .Device(DEVICE_GPU)                 \\\n                              .HostMemory(\"seq_len_max\")          \\\n                              .TypeConstraint<T>(\"T\"),            \\\n                          BlockLSTMOp<GPUDevice, T, true, IFCO>);\n\nREGISTER_GPU_KERNEL(Eigen::half);\nREGISTER_GPU_KERNEL(float);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename T, bool USE_CUBLAS, GateLayout gate_layout>\nclass BlockLSTMGradOp : public OpKernel {\n public:\n  explicit BlockLSTMGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_peephole\", &use_peephole_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor* seq_len_max_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"seq_len_max\", &seq_len_max_tensor));\n\n    const Tensor* x;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));\n    OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));\n    const int64_t timelen = x->dim_size(0);\n    const int64_t batch_size = x->dim_size(1);\n    const int64_t input_size = x->dim_size(2);\n\n    const Tensor* cs_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n\n    const Tensor* w_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n    const int64_t cell_size = w_tensor->dim_size(1) / 4;\n    OP_REQUIRES(ctx, input_size + cell_size == w_tensor->dim_size(0),\n                errors::InvalidArgument(\n                    \"w matrix rows don't match: \", input_size + cell_size,\n                    \" vs. \", w_tensor->dim_size(0)));\n\n    const Tensor* wci_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n\n    const Tensor* wcf_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n\n    const Tensor* wco_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n\n    const Tensor* b_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n    OP_REQUIRES(\n        ctx, cell_size == b_tensor->dim_size(0) / 4,\n        errors::InvalidArgument(\"w and b cell_size don't match: \", cell_size,\n                                \" vs. \", b_tensor->dim_size(0)));\n\n    const Tensor* i_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"i\", &i_out));\n\n    const Tensor* cs_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs\", &cs_out));\n\n    const Tensor* f_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"f\", &f_out));\n\n    const Tensor* o_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"o\", &o_out));\n\n    const Tensor* ci_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"ci\", &ci_out));\n\n    const Tensor* co_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"co\", &co_out));\n\n    const Tensor* h_out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h\", &h_out));\n\n    const Tensor* cs_grad = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"cs_grad\", &cs_grad));\n\n    const Tensor* h_grad = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_grad\", &h_grad));\n\n    TensorShape batch_input_shape({timelen, batch_size, input_size});\n    Tensor* x_grad;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(\"x_grad\", batch_input_shape, &x_grad));\n\n    Tensor* cs_prev_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(\"cs_prev_grad\", cs_prev_tensor->shape(),\n                                        &cs_prev_grad_tensor));\n\n    Tensor* h_prev_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(\"h_prev_grad\", h_prev_tensor->shape(),\n                                        &h_prev_grad_tensor));\n\n    Tensor* w_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"w_grad\", w_tensor->shape(), &w_grad_tensor));\n\n    Tensor* wci_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"wci_grad\", wci_tensor->shape(),\n                                             &wci_grad_tensor));\n\n    Tensor* wcf_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"wcf_grad\", wcf_tensor->shape(),\n                                             &wcf_grad_tensor));\n\n    Tensor* wco_grad_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"wco_grad\", wco_tensor->shape(),\n                                             &wco_grad_tensor));\n\n    Tensor* b_grad_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"b_grad\", b_tensor->shape(), &b_grad_tensor));\n\n    TensorShape batch_cell_shape({batch_size, cell_size});\n\n    Tensor xh_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::v(),\n                            TensorShape({batch_size, input_size + cell_size}),\n                            &xh_tensor));\n\n    Tensor xh_grad_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           xh_tensor.shape(), &xh_grad_tensor));\n\n    Tensor do_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &do_tensor));\n\n    Tensor dcs_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &dcs_tensor));\n\n    Tensor dci_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &dci_tensor));\n\n    Tensor df_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &df_tensor));\n\n    Tensor di_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &di_tensor));\n\n    Tensor dgates_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                      TensorShape({batch_size, cell_size * 4}),\n                                      &dgates_tensor));\n\n    Tensor cs_grad_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &cs_grad_tensor));\n\n    Tensor h_grad_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                           batch_cell_shape, &h_grad_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    functor::TensorZero<Device, T>()(device, cs_grad_tensor.flat<T>());\n    functor::TensorZero<Device, T>()(device, cs_prev_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, h_grad_tensor.flat<T>());\n    functor::TensorZero<Device, T>()(device, h_prev_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, w_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wci_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wcf_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, wco_grad_tensor->flat<T>());\n    functor::TensorZero<Device, T>()(device, b_grad_tensor->flat<T>());\n\n    const int64_t seq_len_max = seq_len_max_tensor->scalar<int64_t>()();\n    SliceHelper<Device, T> slicer(ctx);\n    for (int64_t t = seq_len_max - 1; t >= 0; --t) {\n      const Tensor& x_tensor = slicer.InputSlice(*x, t, \"x\");\n      const Tensor& cs_prev_tensor2 =\n          t == 0 ? *cs_prev_tensor\n                 : slicer.InputSlice(*cs_out, t - 1, \"cs_prev\");\n      const Tensor& h_prev_tensor2 =\n          t == 0 ? *h_prev_tensor : slicer.InputSlice(*h_out, t - 1, \"h_prev\");\n      const Tensor& i_tensor = slicer.InputSlice(*i_out, t, \"i_out\");\n      const Tensor& cs_tensor = slicer.InputSlice(*cs_out, t, \"cs_out\");\n      const Tensor& f_tensor = slicer.InputSlice(*f_out, t, \"f_out\");\n      const Tensor& o_tensor = slicer.InputSlice(*o_out, t, \"o_out\");\n      const Tensor& ci_tensor = slicer.InputSlice(*ci_out, t, \"ci_out\");\n      const Tensor& co_tensor = slicer.InputSlice(*co_out, t, \"co_out\");\n\n      // Grab previous CS grad.\n      const Tensor& const_cs_prev_grad_tensor = *cs_prev_grad_tensor;\n      const Tensor const_cs_grad_slice =\n          slicer.InputSlice(*cs_grad, t, \"cs_grad\");\n      functor::TensorAdd<Device, T>()(\n          device, const_cs_prev_grad_tensor.flat<T>(),\n          const_cs_grad_slice.flat<T>(), cs_grad_tensor.flat<T>());\n\n      // Combine previous h grad and h grad coming on top.\n      const Tensor& const_h_prev_grad_tensor = *h_prev_grad_tensor;\n      const Tensor const_h_grad_slice = slicer.InputSlice(*h_grad, t, \"h_grad\");\n      functor::TensorAdd<Device, T>()(\n          device, const_h_prev_grad_tensor.flat<T>(),\n          const_h_grad_slice.flat<T>(), h_grad_tensor.flat<T>());\n\n      const Tensor& const_cs_grad_tensor = cs_grad_tensor;\n      const Tensor& const_h_grad_tensor = h_grad_tensor;\n\n      Tensor x_grad_tensor = slicer.OutputSlice(x_grad, t, \"x_grad\");\n      functor::BlockLSTMBprop<Device, T, USE_CUBLAS, gate_layout>(\n          batch_size, input_size, cell_size)(\n          ctx, device, use_peephole_, x_tensor.matrix<T>(),\n          cs_prev_tensor2.matrix<T>(), h_prev_tensor2.matrix<T>(),\n          w_tensor->matrix<T>(), wci_tensor->vec<T>(), wcf_tensor->vec<T>(),\n          wco_tensor->vec<T>(), b_tensor->vec<T>(), xh_tensor.matrix<T>(),\n          i_tensor.matrix<T>(), cs_tensor.matrix<T>(), f_tensor.matrix<T>(),\n          o_tensor.matrix<T>(), ci_tensor.matrix<T>(), co_tensor.matrix<T>(),\n          const_cs_grad_tensor.matrix<T>(), const_h_grad_tensor.matrix<T>(),\n          do_tensor.matrix<T>(), dcs_tensor.matrix<T>(), dci_tensor.matrix<T>(),\n          df_tensor.matrix<T>(), di_tensor.matrix<T>(),\n          dgates_tensor.matrix<T>(), cs_prev_grad_tensor->matrix<T>(),\n          h_prev_grad_tensor->matrix<T>(), xh_grad_tensor.matrix<T>(),\n          x_grad_tensor.matrix<T>(), w_grad_tensor->matrix<T>(),\n          wci_grad_tensor->vec<T>(), wcf_grad_tensor->vec<T>(),\n          wco_grad_tensor->vec<T>(), b_grad_tensor->vec<T>());\n      slicer.FinishTimeStep();\n    }\n\n    if (seq_len_max < timelen) {\n      Tensor x_grad_tensor = x_grad->Slice(seq_len_max, timelen);\n      functor::TensorUnalignedZero<Device, T>()(\n          device, x_grad_tensor.unaligned_flat<T>());\n    }\n  }\n\n private:\n  bool use_peephole_;\n};\n\n#define REGISTER_KERNEL(T)                                               \\\n  REGISTER_KERNEL_BUILDER(                                               \\\n      Name(\"BlockLSTMGrad\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"),   \\\n      BlockLSTMGradOp<CPUDevice, T, false, ICFO>);                       \\\n  REGISTER_KERNEL_BUILDER(                                               \\\n      Name(\"BlockLSTMGradV2\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      BlockLSTMGradOp<CPUDevice, T, false, IFCO>);\n\nREGISTER_KERNEL(Eigen::half);\nREGISTER_KERNEL(float);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nnamespace functor {\n#define DECLARE_GPU_BPROP(T, GATE_LAYOUT)                                     \\\n  template <>                                                                 \\\n  void BlockLSTMBprop<GPUDevice, T, true, GATE_LAYOUT>::operator()(           \\\n      OpKernelContext* ctx, const GPUDevice& d, bool use_peephole,            \\\n      typename TTypes<T>::ConstMatrix x,                                      \\\n      typename TTypes<T>::ConstMatrix cs_prev,                                \\\n      typename TTypes<T>::ConstMatrix h_prev,                                 \\\n      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec wci,    \\\n      typename TTypes<T>::ConstVec wcf, typename TTypes<T>::ConstVec wco,     \\\n      typename TTypes<T>::ConstVec b, typename TTypes<T>::Matrix xh,          \\\n      typename TTypes<T>::ConstMatrix i, typename TTypes<T>::ConstMatrix cs,  \\\n      typename TTypes<T>::ConstMatrix f, typename TTypes<T>::ConstMatrix o,   \\\n      typename TTypes<T>::ConstMatrix ci, typename TTypes<T>::ConstMatrix co, \\\n      typename TTypes<T>::ConstMatrix cs_grad,                                \\\n      typename TTypes<T>::ConstMatrix h_grad, typename TTypes<T>::Matrix do_, \\\n      typename TTypes<T>::Matrix dcs, typename TTypes<T>::Matrix dci,         \\\n      typename TTypes<T>::Matrix df, typename TTypes<T>::Matrix di,           \\\n      typename TTypes<T>::Matrix dgates,                                      \\\n      typename TTypes<T>::Matrix cs_prev_grad,                                \\\n      typename TTypes<T>::Matrix h_prev_grad,                                 \\\n      typename TTypes<T>::Matrix xh_grad, typename TTypes<T>::Matrix x_grad,  \\\n      typename TTypes<T>::Matrix w_grad, typename TTypes<T>::Vec wci_grad,    \\\n      typename TTypes<T>::Vec wcf_grad, typename TTypes<T>::Vec wco_grad,     \\\n      typename TTypes<T>::Vec b_grad);                                        \\\n  extern template struct BlockLSTMBprop<GPUDevice, T, true, GATE_LAYOUT>;\n\n#define DECLARE_GPU_SPECS(T)                                                   \\\n  template <>                                                                  \\\n  void TensorCopy<GPUDevice, T>::operator()(const GPUDevice& d,                \\\n                                            typename TTypes<T>::ConstFlat src, \\\n                                            typename TTypes<T>::Flat dst);     \\\n                                                                               \\\n  template <>                                                                  \\\n  void TensorCopyUnaligned<GPUDevice, T>::operator()(                          \\\n      const GPUDevice& d, typename TTypes<T>::UnalignedConstFlat src,          \\\n      typename TTypes<T>::Flat dst);                                           \\\n                                                                               \\\n  template <>                                                                  \\\n  void TensorCopyToUnaligned<GPUDevice, T>::operator()(                        \\\n      const GPUDevice& d, typename TTypes<T>::ConstFlat src,                   \\\n      typename TTypes<T>::UnalignedFlat dst);                                  \\\n                                                                               \\\n  template <>                                                                  \\\n  void TensorAdd<GPUDevice, T>::operator()(                                    \\\n      const GPUDevice& d, typename TTypes<T>::ConstFlat a,                     \\\n      typename TTypes<T>::ConstFlat b, typename TTypes<T>::Flat c);            \\\n                                                                               \\\n  extern template struct TensorCopy<GPUDevice, T>;                             \\\n  extern template struct TensorAdd<GPUDevice, T>;                              \\\n                                                                               \\\n  DECLARE_GPU_BPROP(T, ICFO);                                                  \\\n  DECLARE_GPU_BPROP(T, IFCO);\n\nDECLARE_GPU_SPECS(Eigen::half);\nDECLARE_GPU_SPECS(float);\n#undef DECLARE_GPU_SPECS\n#undef DECLARE_GPU_BPROP\n}  // end namespace functor\n\n#define REGISTER_GPU_KERNEL(T)                                        \\\n  REGISTER_KERNEL_BUILDER(Name(\"BlockLSTMGrad\")                       \\\n                              .Device(DEVICE_GPU)                     \\\n                              .HostMemory(\"seq_len_max\")              \\\n                              .TypeConstraint<T>(\"T\"),                \\\n                          BlockLSTMGradOp<GPUDevice, T, true, ICFO>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"BlockLSTMGradV2\")                     \\\n                              .Device(DEVICE_GPU)                     \\\n                              .HostMemory(\"seq_len_max\")              \\\n                              .TypeConstraint<T>(\"T\"),                \\\n                          BlockLSTMGradOp<GPUDevice, T, true, IFCO>);\n\nREGISTER_GPU_KERNEL(Eigen::half);\nREGISTER_GPU_KERNEL(float);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // end namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for RNN cells.\"\"\"\n\nimport itertools\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import   array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_rnn_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables as variables_lib\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.saved_model import load\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.training.tracking import tracking\nfrom tensorflow.python.training.tracking import util as trackable_utils\nfrom tensorflow.python.util import nest\n\n\nclass Plus1RNNCell(rnn_cell.RNNCell):\n  \"\"\"RNN Cell generating (output, new_state) = (input + 1, state + 1).\"\"\"\n\n  @property\n  def output_size(self):\n    return 5\n\n  @property\n  def state_size(self):\n    return 5\n\n  def __call__(self, input_, state, scope=None):\n    return (input_ + 1, state + 1)\n\n\nclass DummyMultiDimensionalLSTM(rnn_cell.RNNCell):\n  \"\"\"LSTM Cell generating (output, new_state) = (input + 1, state + 1).\n\n  The input to this cell may have an arbitrary number of dimensions that follow\n  the preceding 'Time' and 'Batch' dimensions.\n  \"\"\"\n\n  def __init__(self, dims):\n    \"\"\"Initialize the Multi-dimensional LSTM cell.\n\n    Args:\n      dims: tuple that contains the dimensions of the output of the cell,\n      without including 'Time' or 'Batch' dimensions.\n    \"\"\"\n    if not isinstance(dims, tuple):\n      raise TypeError(\"The dimensions passed to DummyMultiDimensionalLSTM \"\n                      \"should be a tuple of ints.\")\n    self._dims = dims\n    self._output_size = tensor_shape.TensorShape(self._dims)\n    self._state_size = (tensor_shape.TensorShape(self._dims),\n                        tensor_shape.TensorShape(self._dims))\n\n  @property\n  def output_size(self):\n    return self._output_size\n\n  @property\n  def state_size(self):\n    return self._state_size\n\n  def __call__(self, input_, state, scope=None):\n    h, c = state\n    return (input_ + 1, (h + 1, c + 1))\n\n\nclass NestedRNNCell(rnn_cell.RNNCell):\n  \"\"\"RNN Cell generating (output, new_state) = (input + 1, state + 1).\n\n  The input, output and state of this cell is a tuple of two tensors.\n  \"\"\"\n\n  @property\n  def output_size(self):\n    return (5, 5)\n\n  @property\n  def state_size(self):\n    return (6, 6)\n\n  def __call__(self, input_, state, scope=None):\n    h, c = state\n    x, y = input_\n    return ((x + 1, y + 1), (h + 1, c + 1))\n\n\nclass TestStateSaver(object):\n\n  def __init__(self, batch_size, state_size):\n    self._batch_size = batch_size\n    self._state_size = state_size\n    self.saved_state = {}\n\n  def state(self, name):\n\n    if isinstance(self._state_size, dict):\n      state_size = self._state_size[name]\n    else:\n      state_size = self._state_size\n    if isinstance(state_size, int):\n      state_size = (state_size,)\n    elif isinstance(state_size, tuple):\n      pass\n    else:\n      raise TypeError(\"state_size should either be an int or a tuple\")\n\n    return array_ops.zeros((self._batch_size,) + state_size)\n\n  def save_state(self, name, state):\n    self.saved_state[name] = state\n    return array_ops.identity(state)\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def state_size(self):\n    return self._state_size\n\n\nclass TestStateSaverWithCounters(TestStateSaver):\n  \"\"\"Class wrapper around TestStateSaver.\n\n  A dummy class used for testing of static_state_saving_rnn. It helps test if\n  save_state and state functions got called same number of time when we\n  evaluate output of rnn cell and state or either of them separately. It\n  inherits from the TestStateSaver and adds the counters for calls of functions.\n  \"\"\"\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def __init__(self, batch_size, state_size):\n    super(TestStateSaverWithCounters, self).__init__(batch_size, state_size)\n    self._num_state_calls = variables_lib.VariableV1(0)\n    self._num_save_state_calls = variables_lib.VariableV1(0)\n\n  def state(self, name):\n    with ops.control_dependencies(\n        [state_ops.assign_add(self._num_state_calls, 1)]):\n      return super(TestStateSaverWithCounters, self).state(name)\n\n  def save_state(self, name, state):\n    with ops.control_dependencies([state_ops.assign_add(\n        self._num_save_state_calls, 1)]):\n      return super(TestStateSaverWithCounters, self).save_state(name, state)\n\n  @property\n  def num_state_calls(self):\n    return self._num_state_calls\n\n  @property\n  def num_save_state_calls(self):\n    return self._num_save_state_calls\n\n\nclass RNNTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testInvalidSequenceLengthShape(self):\n    cell = Plus1RNNCell()\n    inputs = [array_ops.placeholder(dtypes.float32, shape=(3, 4))]\n    with self.assertRaisesRegex(ValueError, \"must be a vector\"):\n      rnn.static_rnn(cell, inputs, dtype=dtypes.float32, sequence_length=4)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRNN(self):\n    cell = Plus1RNNCell()\n    batch_size = 2\n    input_size = 5\n    max_length = 8  # unrolled up to this length\n    inputs = max_length * [\n        array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n    ]\n    outputs, state = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n    self.assertEqual(len(outputs), len(inputs))\n    for out, inp in zip(outputs, inputs):\n      self.assertEqual(out.get_shape(), inp.get_shape())\n      self.assertEqual(out.dtype, inp.dtype)\n\n    with self.session() as sess:\n      input_value = np.random.randn(batch_size, input_size)\n      values = sess.run(outputs + [state], feed_dict={inputs[0]: input_value})\n\n      # Outputs\n      for v in values[:-1]:\n        self.assertAllClose(v, input_value + 1.0)\n\n      # Final state\n      self.assertAllClose(values[-1],\n                          max_length * np.ones(\n                              (batch_size, input_size), dtype=np.float32))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDropout(self):\n    cell = Plus1RNNCell()\n    full_dropout_cell = rnn_cell.DropoutWrapper(\n        cell, input_keep_prob=1e-6, seed=0)\n    self.assertIn(\"cell\", full_dropout_cell._trackable_children())\n    self.assertIs(full_dropout_cell._trackable_children()[\"cell\"], cell)\n    batch_size = 2\n    input_size = 5\n    max_length = 8\n    inputs = max_length * [\n        array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n    ]\n    with variable_scope.variable_scope(\"share_scope\"):\n      outputs, state = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n    with variable_scope.variable_scope(\"drop_scope\"):\n      dropped_outputs, _ = rnn.static_rnn(\n          full_dropout_cell, inputs, dtype=dtypes.float32)\n    self.assertEqual(len(outputs), len(inputs))\n    for out, inp in zip(outputs, inputs):\n      self.assertEqual(out.get_shape().as_list(), inp.get_shape().as_list())\n      self.assertEqual(out.dtype, inp.dtype)\n\n    with self.session() as sess:\n      input_value = np.random.randn(batch_size, input_size)\n      values = sess.run(outputs + [state], feed_dict={inputs[0]: input_value})\n      full_dropout_values = sess.run(\n          dropped_outputs, feed_dict={\n              inputs[0]: input_value\n          })\n\n      for v in values[:-1]:\n        self.assertAllClose(v, input_value + 1.0)\n      for d_v in full_dropout_values[:-1]:  # Add 1.0 to dropped_out (all zeros)\n        self.assertAllClose(d_v, np.ones_like(input_value))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDynamicCalculation(self):\n    cell = Plus1RNNCell()\n    sequence_length = array_ops.placeholder(dtypes.int64)\n    batch_size = 2\n    input_size = 5\n    max_length = 8\n    inputs = max_length * [\n        array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n    ]\n    with variable_scope.variable_scope(\"drop_scope\"):\n      dynamic_outputs, dynamic_state = rnn.static_rnn(\n          cell, inputs, sequence_length=sequence_length, dtype=dtypes.float32)\n    self.assertEqual(len(dynamic_outputs), len(inputs))\n\n    with self.session() as sess:\n      input_value = np.random.randn(batch_size, input_size)\n      dynamic_values = sess.run(\n          dynamic_outputs,\n          feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n      dynamic_state_value = sess.run(\n          [dynamic_state],\n          feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n\n      # outputs are fully calculated for t = 0, 1\n      for v in dynamic_values[:2]:\n        self.assertAllClose(v, input_value + 1.0)\n\n      # outputs at t = 2 are zero for entry 0, calculated for entry 1\n      self.assertAllClose(dynamic_values[2],\n                          np.vstack((np.zeros((input_size)),\n                                     1.0 + input_value[1, :])))\n\n      # outputs at t = 3+ are zero\n      for v in dynamic_values[3:]:\n        self.assertAllEqual(v, np.zeros_like(input_value))\n\n      # the final states are:\n      #  entry 0: the values from the calculation at t=1\n      #  entry 1: the values from the calculation at t=2\n      self.assertAllEqual(dynamic_state_value[0],\n                          np.vstack((1.0 * (1 + 1) * np.ones((input_size)),\n                                     1.0 * (2 + 1) * np.ones((input_size)))))\n\n  def _testScope(self, factory, prefix=\"prefix\", use_outer_scope=True):\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          factory(scope)\n      else:\n        factory(prefix)\n\n      # check that all the variables names starts\n      # with the proper scope.\n      variables_lib.global_variables_initializer()\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"RNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testScope(self):\n\n    def factory(scope):\n      cell = Plus1RNNCell()\n      batch_size = 2\n      input_size = 5\n      max_length = 8  # unrolled up to this length\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      return rnn.static_rnn(cell, inputs, dtype=dtypes.float32, scope=scope)\n\n    self._testScope(factory, use_outer_scope=True)\n    self._testScope(factory, use_outer_scope=False)\n    self._testScope(factory, prefix=None, use_outer_scope=False)\n\n\nclass LSTMTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  def testDType(self):\n    # Test case for GitHub issue 16228\n    # Not passing dtype in constructor results in default float32\n    lstm = rnn_cell.LSTMCell(10)\n    input_tensor = array_ops.ones([10, 50])\n    lstm.build(input_tensor.get_shape())\n    self.assertEqual(lstm._bias.dtype.base_dtype, dtypes.float32)\n\n    # Explicitly pass dtype in constructor\n    for dtype in [dtypes.float16, dtypes.float32, dtypes.float64]:\n      lstm = rnn_cell.LSTMCell(10, dtype=dtype)\n      input_tensor = array_ops.ones([10, 50])\n      lstm.build(input_tensor.get_shape())\n      self.assertEqual(lstm._bias.dtype.base_dtype, dtype)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNoProjNoSharding(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      cell = rnn_cell.LSTMCell(\n          num_units, initializer=initializer, state_is_tuple=False)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      outputs, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      self.assertEqual(len(outputs), len(inputs))\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      sess.run(outputs, feed_dict={inputs[0]: input_value})\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testCellClipping(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          cell_clip=0.0,\n          initializer=initializer,\n          state_is_tuple=False)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      outputs, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      self.assertEqual(len(outputs), len(inputs))\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      values = sess.run(outputs, feed_dict={inputs[0]: input_value})\n\n    for value in values:\n      # if cell c is clipped to 0, tanh(c) = 0 => m==0\n      self.assertAllEqual(value, np.zeros((batch_size, num_units)))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNoProjNoShardingSimpleStateSaver(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      state_saver = TestStateSaver(batch_size, 2 * num_units)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=False,\n          initializer=initializer,\n          state_is_tuple=False)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      with variable_scope.variable_scope(\"share_scope\"):\n        outputs, state = rnn.static_state_saving_rnn(\n            cell, inputs, state_saver=state_saver, state_name=\"save_lstm\")\n      self.assertEqual(len(outputs), len(inputs))\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      (last_state_value, saved_state_value) = sess.run(\n          [state, state_saver.saved_state[\"save_lstm\"]],\n          feed_dict={\n              inputs[0]: input_value\n          })\n      self.assertAllEqual(last_state_value, saved_state_value)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNoProjNoShardingTupleStateSaver(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      state_saver = TestStateSaver(batch_size, num_units)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=False,\n          initializer=initializer,\n          state_is_tuple=True)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n      with variable_scope.variable_scope(\"share_scope\"):\n        outputs, state = rnn.static_state_saving_rnn(\n            cell, inputs, state_saver=state_saver, state_name=(\"c\", \"m\"))\n      self.assertEqual(len(outputs), len(inputs))\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      last_and_saved_states = sess.run(\n          state + (state_saver.saved_state[\"c\"], state_saver.saved_state[\"m\"]),\n          feed_dict={\n              inputs[0]: input_value\n          })\n      self.assertEqual(4, len(last_and_saved_states))\n      self.assertAllEqual(last_and_saved_states[:2], last_and_saved_states[2:])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNoProjNoShardingNestedTupleStateSaver(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      state_saver = TestStateSaver(\n          batch_size, {\n              \"c0\": num_units,\n              \"m0\": num_units,\n              \"c1\": num_units + 1,\n              \"m1\": num_units + 1,\n              \"c2\": num_units + 2,\n              \"m2\": num_units + 2,\n              \"c3\": num_units + 3,\n              \"m3\": num_units + 3\n          })\n\n      def _cell(i):\n        return rnn_cell.LSTMCell(\n            num_units + i,\n            use_peepholes=False,\n            initializer=initializer,\n            state_is_tuple=True)\n\n      # This creates a state tuple which has 4 sub-tuples of length 2 each.\n      cell = rnn_cell.MultiRNNCell(\n          [_cell(i) for i in range(4)], state_is_tuple=True)\n\n      self.assertEqual(len(cell.state_size), 4)\n      for i in range(4):\n        self.assertEqual(len(cell.state_size[i]), 2)\n\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(batch_size, input_size))\n      ]\n\n      state_names = ((\"c0\", \"m0\"), (\"c1\", \"m1\"), (\"c2\", \"m2\"), (\"c3\", \"m3\"))\n      with variable_scope.variable_scope(\"share_scope\"):\n        outputs, state = rnn.static_state_saving_rnn(\n            cell, inputs, state_saver=state_saver, state_name=state_names)\n      self.assertEqual(len(outputs), len(inputs))\n\n      # Final output comes from _cell(3) which has state size num_units + 3\n      for out in outputs:\n        self.assertEqual(out.get_shape().as_list(), [batch_size, num_units + 3])\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      last_states = sess.run(\n          list(nest.flatten(state)), feed_dict={\n              inputs[0]: input_value\n          })\n      saved_states = sess.run(\n          list(state_saver.saved_state.values()),\n          feed_dict={\n              inputs[0]: input_value\n          })\n      self.assertEqual(8, len(last_states))\n      self.assertEqual(8, len(saved_states))\n      flat_state_names = nest.flatten(state_names)\n      named_saved_states = dict(\n          zip(state_saver.saved_state.keys(), saved_states))\n\n      for i in range(8):\n        self.assertAllEqual(last_states[i],\n                            named_saved_states[flat_state_names[i]])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testProjNoSharding(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=False)\n      outputs, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      self.assertEqual(len(outputs), len(inputs))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      sess.run(outputs, feed_dict={inputs[0]: input_value})\n\n  def _testStateTupleWithProjAndSequenceLength(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    sequence_length = [4, 6]\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      cell_notuple = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=False)\n      cell_tuple = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=True)\n      with variable_scope.variable_scope(\"root\") as scope:\n        outputs_notuple, state_notuple = rnn.static_rnn(\n            cell_notuple,\n            inputs,\n            dtype=dtypes.float32,\n            sequence_length=sequence_length,\n            scope=scope)\n        scope.reuse_variables()\n        # TODO(ebrevdo): For this test, we ensure values are identical and\n        # therefore the weights here are tied.  In the future, we may consider\n        # making the state_is_tuple property mutable so we can avoid\n        # having to do this - especially if users ever need to reuse\n        # the parameters from different RNNCell instances.  Right now,\n        # this seems an unrealistic use case except for testing.\n        cell_tuple._scope = cell_notuple._scope  # pylint: disable=protected-access\n        outputs_tuple, state_tuple = rnn.static_rnn(\n            cell_tuple,\n            inputs,\n            dtype=dtypes.float32,\n            sequence_length=sequence_length,\n            scope=scope)\n      self.assertEqual(len(outputs_notuple), len(inputs))\n      self.assertEqual(len(outputs_tuple), len(inputs))\n      self.assertTrue(isinstance(state_tuple, tuple))\n      self.assertTrue(isinstance(state_notuple, ops.Tensor))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      outputs_notuple_v = sess.run(\n          outputs_notuple, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs_tuple_v = sess.run(\n          outputs_tuple, feed_dict={\n              inputs[0]: input_value\n          })\n      self.assertAllEqual(outputs_notuple_v, outputs_tuple_v)\n\n      (state_notuple_v,) = sess.run(\n          (state_notuple,), feed_dict={\n              inputs[0]: input_value\n          })\n      state_tuple_v = sess.run(state_tuple, feed_dict={inputs[0]: input_value})\n      self.assertAllEqual(state_notuple_v, np.hstack(state_tuple_v))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testProjSharding(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    num_proj_shards = 3\n    num_unit_shards = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          num_unit_shards=num_unit_shards,\n          num_proj_shards=num_proj_shards,\n          initializer=initializer,\n          state_is_tuple=False)\n\n      outputs, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n\n      self.assertEqual(len(outputs), len(inputs))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      sess.run(outputs, feed_dict={inputs[0]: input_value})\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDoubleInput(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    num_proj_shards = 3\n    num_unit_shards = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(-1, 1, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float64, shape=(None, input_size))\n      ]\n\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          num_unit_shards=num_unit_shards,\n          num_proj_shards=num_proj_shards,\n          initializer=initializer,\n          state_is_tuple=False)\n\n      outputs, _ = rnn.static_rnn(\n          cell,\n          inputs,\n          initial_state=cell.zero_state(batch_size, dtypes.float64))\n\n      self.assertEqual(len(outputs), len(inputs))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.asarray(\n          np.random.randn(batch_size, input_size), dtype=np.float64)\n      values = sess.run(outputs, feed_dict={inputs[0]: input_value})\n      self.assertEqual(values[0].dtype, input_value.dtype)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testShardNoShardEquivalentOutput(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    num_proj_shards = 3\n    num_unit_shards = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      initializer = init_ops.constant_initializer(0.001)\n\n      cell_noshard = rnn_cell.LSTMCell(\n          num_units,\n          num_proj=num_proj,\n          use_peepholes=True,\n          initializer=initializer,\n          num_unit_shards=num_unit_shards,\n          num_proj_shards=num_proj_shards,\n          state_is_tuple=False)\n\n      cell_shard = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          initializer=initializer,\n          num_proj=num_proj,\n          state_is_tuple=False)\n\n      with variable_scope.variable_scope(\"noshard_scope\"):\n        outputs_noshard, state_noshard = rnn.static_rnn(\n            cell_noshard, inputs, dtype=dtypes.float32)\n      with variable_scope.variable_scope(\"shard_scope\"):\n        outputs_shard, state_shard = rnn.static_rnn(\n            cell_shard, inputs, dtype=dtypes.float32)\n\n      self.assertEqual(len(outputs_noshard), len(inputs))\n      self.assertEqual(len(outputs_noshard), len(outputs_shard))\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      feeds = dict((x, input_value) for x in inputs)\n      values_noshard = sess.run(outputs_noshard, feed_dict=feeds)\n      values_shard = sess.run(outputs_shard, feed_dict=feeds)\n      state_values_noshard = sess.run([state_noshard], feed_dict=feeds)\n      state_values_shard = sess.run([state_shard], feed_dict=feeds)\n      self.assertEqual(len(values_noshard), len(values_shard))\n      self.assertEqual(len(state_values_noshard), len(state_values_shard))\n      for (v_noshard, v_shard) in zip(values_noshard, values_shard):\n        self.assertAllClose(v_noshard, v_shard, atol=1e-3)\n      for (s_noshard, s_shard) in zip(state_values_noshard, state_values_shard):\n        self.assertAllClose(s_noshard, s_shard, atol=1e-3)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDoubleInputWithDropoutAndDynamicCalculation(self):\n    \"\"\"Smoke test for using LSTM with doubles, dropout, dynamic calculation.\"\"\"\n\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    num_proj_shards = 3\n    num_unit_shards = 2\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      sequence_length = array_ops.placeholder(dtypes.int64)\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float64, shape=(None, input_size))\n      ]\n\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          num_unit_shards=num_unit_shards,\n          num_proj_shards=num_proj_shards,\n          initializer=initializer,\n          state_is_tuple=False)\n      dropout_cell = rnn_cell.DropoutWrapper(cell, 0.5, seed=0)\n\n      outputs, state = rnn.static_rnn(\n          dropout_cell,\n          inputs,\n          sequence_length=sequence_length,\n          initial_state=cell.zero_state(batch_size, dtypes.float64))\n\n      self.assertEqual(len(outputs), len(inputs))\n\n      variables_lib.global_variables_initializer().run(feed_dict={\n          sequence_length: [2, 3]\n      })\n      input_value = np.asarray(\n          np.random.randn(batch_size, input_size), dtype=np.float64)\n      values = sess.run(\n          outputs, feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n      state_value = sess.run(\n          [state], feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n      self.assertEqual(values[0].dtype, input_value.dtype)\n      self.assertEqual(state_value[0].dtype, input_value.dtype)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testSharingWeightsWithReuse(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(-1, 1, seed=self._seed)\n      initializer_d = init_ops.random_uniform_initializer(\n          -1, 1, seed=self._seed + 1)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=False)\n      cell_d = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer_d,\n          state_is_tuple=False)\n\n      with variable_scope.variable_scope(\"share_scope\"):\n        outputs0, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      with variable_scope.variable_scope(\"share_scope\", reuse=True):\n        outputs1, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      with variable_scope.variable_scope(\"diff_scope\"):\n        outputs2, _ = rnn.static_rnn(cell_d, inputs, dtype=dtypes.float32)\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      output_values = sess.run(\n          outputs0 + outputs1 + outputs2, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs0_values = output_values[:max_length]\n      outputs1_values = output_values[max_length:2 * max_length]\n      outputs2_values = output_values[2 * max_length:]\n      self.assertEqual(len(outputs0_values), len(outputs1_values))\n      self.assertEqual(len(outputs0_values), len(outputs2_values))\n      for o1, o2, o3 in zip(outputs0_values, outputs1_values, outputs2_values):\n        # Same weights used by both RNNs so outputs should be the same.\n        self.assertAllEqual(o1, o2)\n        # Different weights used so outputs should be different.\n        self.assertTrue(np.linalg.norm(o1 - o3) > 1e-6)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testSharingWeightsWithDifferentNamescope(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(-1, 1, seed=self._seed)\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n      ]\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=False)\n\n      with ops.name_scope(\"scope0\"):\n        with variable_scope.variable_scope(\"share_scope\"):\n          outputs0, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n      with ops.name_scope(\"scope1\"):\n        with variable_scope.variable_scope(\"share_scope\", reuse=True):\n          outputs1, _ = rnn.static_rnn(cell, inputs, dtype=dtypes.float32)\n\n      variables_lib.global_variables_initializer().run()\n      input_value = np.random.randn(batch_size, input_size)\n      output_values = sess.run(\n          outputs0 + outputs1, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs0_values = output_values[:max_length]\n      outputs1_values = output_values[max_length:]\n      self.assertEqual(len(outputs0_values), len(outputs1_values))\n      for out0, out1 in zip(outputs0_values, outputs1_values):\n        self.assertAllEqual(out0, out1)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDynamicRNNAllowsUnknownTimeDimension(self):\n    inputs = array_ops.placeholder(dtypes.float32, shape=[1, None, 20])\n    cell = rnn_cell.GRUCell(30)\n    # Smoke test, this should not raise an error\n    rnn.dynamic_rnn(cell, inputs, dtype=dtypes.float32)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDynamicRNNWithTupleStates(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    sequence_length = [4, 6]\n    in_graph_mode = not context.executing_eagerly()\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      if in_graph_mode:\n        inputs = max_length * [\n            array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n        ]\n      else:\n        inputs = max_length * [\n            constant_op.constant(\n                np.random.randn(batch_size, input_size).astype(np.float32))\n        ]\n      inputs_c = array_ops.stack(inputs)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          num_proj=num_proj,\n          initializer=initializer,\n          state_is_tuple=True)\n      with variable_scope.variable_scope(\"root\") as scope:\n        outputs_static, state_static = rnn.static_rnn(\n            cell,\n            inputs,\n            dtype=dtypes.float32,\n            sequence_length=sequence_length,\n            scope=scope)\n        scope.reuse_variables()\n        outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n            cell,\n            inputs_c,\n            dtype=dtypes.float32,\n            time_major=True,\n            sequence_length=sequence_length,\n            scope=scope)\n      self.assertTrue(isinstance(state_static, rnn_cell.LSTMStateTuple))\n      self.assertTrue(isinstance(state_dynamic, rnn_cell.LSTMStateTuple))\n      self.assertIs(state_static[0], state_static.c)\n      self.assertIs(state_static[1], state_static.h)\n      self.assertIs(state_dynamic[0], state_dynamic.c)\n      self.assertIs(state_dynamic[1], state_dynamic.h)\n\n      if in_graph_mode:\n        variables_lib.global_variables_initializer().run()\n        input_value = np.random.randn(batch_size, input_size)\n        outputs_static = sess.run(\n            outputs_static, feed_dict={\n                inputs[0]: input_value\n            })\n        outputs_dynamic = sess.run(\n            outputs_dynamic, feed_dict={\n                inputs[0]: input_value\n            })\n        state_static = sess.run(\n            state_static, feed_dict={\n                inputs[0]: input_value\n            })\n        state_dynamic = sess.run(\n            state_dynamic, feed_dict={\n                inputs[0]: input_value\n            })\n\n      comparison_fn = self.assertAllEqual\n      if test_util.is_xla_enabled():\n        comparison_fn = self.assertAllClose\n      if in_graph_mode:\n        comparison_fn(outputs_static, outputs_dynamic)\n      else:\n        self.assertAllEqual(array_ops.stack(outputs_static), outputs_dynamic)\n      comparison_fn(np.hstack(state_static), np.hstack(state_dynamic))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDynamicRNNWithNestedTupleStates(self):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    num_proj = 4\n    max_length = 8\n    sequence_length = [4, 6]\n    in_graph_mode = not context.executing_eagerly()\n    with self.session(graph=ops.Graph()) as sess:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      if in_graph_mode:\n        inputs = max_length * [\n            array_ops.placeholder(dtypes.float32, shape=(None, input_size))\n        ]\n      else:\n        inputs = max_length * [\n            constant_op.constant(\n                np.random.randn(batch_size, input_size).astype(np.float32))\n        ]\n      inputs_c = array_ops.stack(inputs)\n\n      def _cell(i):\n        return rnn_cell.LSTMCell(\n            num_units + i,\n            use_peepholes=True,\n            num_proj=num_proj + i,\n            initializer=initializer,\n            state_is_tuple=True)\n\n      # This creates a state tuple which has 4 sub-tuples of length 2 each.\n      cell = rnn_cell.MultiRNNCell(\n          [_cell(i) for i in range(4)], state_is_tuple=True)\n\n      self.assertEqual(len(cell.state_size), 4)\n      for i in range(4):\n        self.assertEqual(len(cell.state_size[i]), 2)\n\n      test_zero = cell.zero_state(1, dtypes.float32)\n      self.assertEqual(len(test_zero), 4)\n      for i in range(4):\n        self.assertEqual(test_zero[i][0].get_shape()[1], cell.state_size[i][0])\n        self.assertEqual(test_zero[i][1].get_shape()[1], cell.state_size[i][1])\n\n      with variable_scope.variable_scope(\"root\") as scope:\n        outputs_static, state_static = rnn.static_rnn(\n            cell,\n            inputs,\n            dtype=dtypes.float32,\n            sequence_length=sequence_length,\n            scope=scope)\n        scope.reuse_variables()\n        outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n            cell,\n            inputs_c,\n            dtype=dtypes.float32,\n            time_major=True,\n            sequence_length=sequence_length,\n            scope=scope)\n\n      if in_graph_mode:\n        input_value = np.random.randn(batch_size, input_size)\n        variables_lib.global_variables_initializer().run()\n        outputs_static = sess.run(\n            outputs_static, feed_dict={\n                inputs[0]: input_value\n            })\n        outputs_dynamic = sess.run(\n            outputs_dynamic, feed_dict={\n                inputs[0]: input_value\n            })\n        state_static = sess.run(\n            nest.flatten(state_static), feed_dict={\n                inputs[0]: input_value\n            })\n        state_dynamic = sess.run(\n            nest.flatten(state_dynamic), feed_dict={\n                inputs[0]: input_value\n            })\n\n      comparison_fn = self.assertAllEqual\n      if test_util.is_xla_enabled():\n        comparison_fn = self.assertAllClose\n      if in_graph_mode:\n        comparison_fn(outputs_static, outputs_dynamic)\n      else:\n        self.assertAllEqual(array_ops.stack(outputs_static), outputs_dynamic)\n        state_static = nest.flatten(state_static)\n        state_dynamic = nest.flatten(state_dynamic)\n      comparison_fn(np.hstack(state_static), np.hstack(state_dynamic))\n\n  def _testDynamicEquivalentToStaticRNN(self, use_sequence_length):\n    time_steps = 8\n    num_units = 3\n    num_proj = 4\n    input_size = 5\n    batch_size = 2\n\n    input_values = np.random.randn(time_steps, batch_size, input_size).astype(\n        np.float32)\n\n    if use_sequence_length:\n      sequence_length = np.random.randint(0, time_steps, size=batch_size)\n    else:\n      sequence_length = None\n\n    in_graph_mode = not context.executing_eagerly()\n\n    # TODO(b/68017812): Eager ignores operation seeds, so we need to create a\n    # single cell and reuse it across the static and dynamic RNNs. Remove this\n    # special case once is fixed.\n    if not in_graph_mode:\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n      cell = rnn_cell.LSTMCell(\n          num_units,\n          use_peepholes=True,\n          initializer=initializer,\n          num_proj=num_proj,\n          state_is_tuple=False)\n\n    ########### Step 1: Run static graph and generate readouts\n    with self.session(graph=ops.Graph()) as sess:\n      if in_graph_mode:\n        concat_inputs = array_ops.placeholder(\n            dtypes.float32, shape=(time_steps, batch_size, input_size))\n      else:\n        concat_inputs = constant_op.constant(input_values)\n      inputs = array_ops.unstack(concat_inputs)\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n\n      # TODO(akshayka): Remove special case once b/68017812 is fixed.\n      if in_graph_mode:\n        cell = rnn_cell.LSTMCell(\n            num_units,\n            use_peepholes=True,\n            initializer=initializer,\n            num_proj=num_proj,\n            state_is_tuple=False)\n\n      with variable_scope.variable_scope(\"dynamic_scope\"):\n        outputs_static, state_static = rnn.static_rnn(\n            cell, inputs, sequence_length=sequence_length, dtype=dtypes.float32)\n\n      if in_graph_mode:\n        # Generate gradients of sum of outputs w.r.t. inputs\n        static_gradients = gradients_impl.gradients(\n            outputs_static + [state_static], [concat_inputs])\n        # Generate gradients of individual outputs w.r.t. inputs\n        static_individual_gradients = nest.flatten([\n            gradients_impl.gradients(y, [concat_inputs])\n            for y in [outputs_static[0], outputs_static[-1], state_static]\n        ])\n        # Generate gradients of individual variables w.r.t. inputs\n        trainable_variables = ops.get_collection(\n            ops.GraphKeys.TRAINABLE_VARIABLES)\n        assert len(trainable_variables) > 1, (\n            \"Count of trainable variables: %d\" % len(trainable_variables))\n        # pylint: disable=bad-builtin\n        static_individual_variable_gradients = nest.flatten([\n            gradients_impl.gradients(y, trainable_variables)\n            for y in [outputs_static[0], outputs_static[-1], state_static]\n        ])\n        # Generate gradients and run sessions to obtain outputs\n        feeds = {concat_inputs: input_values}\n        # Initialize\n        variables_lib.global_variables_initializer().run(feed_dict=feeds)\n        # Test forward pass\n        values_static = sess.run(outputs_static, feed_dict=feeds)\n        (state_value_static,) = sess.run((state_static,), feed_dict=feeds)\n\n        # Test gradients to inputs and variables w.r.t. outputs & final state\n        static_grad_values = sess.run(static_gradients, feed_dict=feeds)\n\n        static_individual_grad_values = sess.run(\n            static_individual_gradients, feed_dict=feeds)\n\n        static_individual_var_grad_values = sess.run(\n            static_individual_variable_gradients, feed_dict=feeds)\n\n    ########## Step 2: Run dynamic graph and generate readouts\n    with self.session(graph=ops.Graph()) as sess:\n      if in_graph_mode:\n        concat_inputs = array_ops.placeholder(\n            dtypes.float32, shape=(time_steps, batch_size, input_size))\n      else:\n        concat_inputs = constant_op.constant(input_values)\n      initializer = init_ops.random_uniform_initializer(\n          -0.01, 0.01, seed=self._seed)\n\n      # TODO(akshayka): Remove this special case once b/68017812 is\n      # fixed.\n      if in_graph_mode:\n        cell = rnn_cell.LSTMCell(\n            num_units,\n            use_peepholes=True,\n            initializer=initializer,\n            num_proj=num_proj,\n            state_is_tuple=False)\n\n      with variable_scope.variable_scope(\"dynamic_scope\"):\n        outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n            cell,\n            inputs=concat_inputs,\n            sequence_length=sequence_length,\n            time_major=True,\n            dtype=dtypes.float32)\n        split_outputs_dynamic = array_ops.unstack(outputs_dynamic, time_steps)\n\n      if in_graph_mode:\n\n        # Generate gradients of sum of outputs w.r.t. inputs\n        dynamic_gradients = gradients_impl.gradients(\n            split_outputs_dynamic + [state_dynamic], [concat_inputs])\n\n        # Generate gradients of several individual outputs w.r.t. inputs\n        dynamic_individual_gradients = nest.flatten([\n            gradients_impl.gradients(y, [concat_inputs])\n            for y in [\n                split_outputs_dynamic[0], split_outputs_dynamic[-1],\n                state_dynamic\n            ]\n        ])\n\n        # Generate gradients of individual variables w.r.t. inputs\n        trainable_variables = ops.get_collection(\n            ops.GraphKeys.TRAINABLE_VARIABLES)\n        assert len(trainable_variables) > 1, (\n            \"Count of trainable variables: %d\" % len(trainable_variables))\n        dynamic_individual_variable_gradients = nest.flatten([\n            gradients_impl.gradients(y, trainable_variables)\n            for y in [\n                split_outputs_dynamic[0], split_outputs_dynamic[-1],\n                state_dynamic\n            ]\n        ])\n\n        feeds = {concat_inputs: input_values}\n\n        # Initialize\n        variables_lib.global_variables_initializer().run(feed_dict=feeds)\n\n        # Test forward pass\n        values_dynamic = sess.run(split_outputs_dynamic, feed_dict=feeds)\n        (state_value_dynamic,) = sess.run((state_dynamic,), feed_dict=feeds)\n\n        # Test gradients to inputs and variables w.r.t. outputs & final state\n        dynamic_grad_values = sess.run(dynamic_gradients, feed_dict=feeds)\n\n        dynamic_individual_grad_values = sess.run(\n            dynamic_individual_gradients, feed_dict=feeds)\n\n        dynamic_individual_var_grad_values = sess.run(\n            dynamic_individual_variable_gradients, feed_dict=feeds)\n\n    ######### Step 3: Comparisons\n    if not in_graph_mode:\n      values_static = outputs_static\n      values_dynamic = split_outputs_dynamic\n      state_value_static = state_static\n      state_value_dynamic = state_dynamic\n\n    self.assertEqual(len(values_static), len(values_dynamic))\n    for (value_static, value_dynamic) in zip(values_static, values_dynamic):\n      self.assertAllClose(value_static, value_dynamic)\n    self.assertAllClose(state_value_static, state_value_dynamic)\n\n    if in_graph_mode:\n\n      self.assertAllClose(static_grad_values, dynamic_grad_values)\n\n      self.assertEqual(\n          len(static_individual_grad_values),\n          len(dynamic_individual_grad_values))\n      self.assertEqual(\n          len(static_individual_var_grad_values),\n          len(dynamic_individual_var_grad_values))\n\n      for i, (a, b) in enumerate(\n          zip(static_individual_grad_values, dynamic_individual_grad_values)):\n        tf_logging.info(\"Comparing individual gradients iteration %d\" % i)\n        self.assertAllClose(a, b)\n\n      for i, (a, b) in enumerate(\n          zip(static_individual_var_grad_values,\n              dynamic_individual_var_grad_values)):\n        tf_logging.info(\n            \"Comparing individual variable gradients iteration %d\" % i)\n        self.assertAllClose(a, b)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDynamicEquivalentToStaticRNN(self):\n    self._testDynamicEquivalentToStaticRNN(use_sequence_length=False)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDynamicEquivalentToStaticRNNWithSequenceLength(self):\n    self._testDynamicEquivalentToStaticRNN(use_sequence_length=True)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLSTMBlockCellErrorHandling(self):\n    forget_bias = 1\n    cell_clip = 0\n    use_peephole = False\n    x = constant_op.constant(0.837607, shape=[28, 29], dtype=dtypes.float32)\n    cs_prev = constant_op.constant(0, shape=[28, 17], dtype=dtypes.float32)\n    h_prev = constant_op.constant(\n        0.592631638, shape=[28, 17], dtype=dtypes.float32)\n    w = constant_op.constant(0.887386262, shape=[46, 68], dtype=dtypes.float32)\n    wci = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n    wcf = constant_op.constant(0, shape=[17], dtype=dtypes.float32)\n    wco = constant_op.constant(\n        0.592631638, shape=[28, 17], dtype=dtypes.float32)\n    b = constant_op.constant(0.75259006, shape=[68], dtype=dtypes.float32)\n    with self.assertRaises(errors_impl.InvalidArgumentError):\n      self.evaluate(\n          gen_rnn_ops.lstm_block_cell(\n              x=x,\n              cs_prev=cs_prev,\n              h_prev=h_prev,\n              w=w,\n              wci=wci,\n              wcf=wcf,\n              wco=wco,\n              b=b,\n              forget_bias=forget_bias,\n              cell_clip=cell_clip,\n              use_peephole=use_peephole))\n\n\nclass BidirectionalRNNTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  def _createBidirectionalRNN(self, use_shape, use_sequence_length, scope=None):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n\n    initializer = init_ops.random_uniform_initializer(\n        -0.01, 0.01, seed=self._seed)\n    sequence_length = array_ops.placeholder(\n        dtypes.int64) if use_sequence_length else None\n    cell_fw = rnn_cell.LSTMCell(\n        num_units, input_size, initializer=initializer, state_is_tuple=False)\n    cell_bw = rnn_cell.LSTMCell(\n        num_units, input_size, initializer=initializer, state_is_tuple=False)\n    inputs = max_length * [\n        array_ops.placeholder(\n            dtypes.float32,\n            shape=(batch_size, input_size) if use_shape else (None, input_size))\n    ]\n    outputs, state_fw, state_bw = rnn.static_bidirectional_rnn(\n        cell_fw,\n        cell_bw,\n        inputs,\n        dtype=dtypes.float32,\n        sequence_length=sequence_length,\n        scope=scope)\n    self.assertEqual(len(outputs), len(inputs))\n    for out in outputs:\n      self.assertEqual(out.get_shape().as_list(),\n                       [batch_size if use_shape else None, 2 * num_units])\n\n    input_value = np.random.randn(batch_size, input_size)\n    outputs = array_ops.stack(outputs)\n\n    return input_value, inputs, outputs, state_fw, state_bw, sequence_length\n\n  def _testBidirectionalRNN(self, use_shape):\n    with self.session(graph=ops.Graph()) as sess:\n      input_value, inputs, outputs, state_fw, state_bw, sequence_length = (\n          self._createBidirectionalRNN(use_shape, True))\n      variables_lib.global_variables_initializer().run()\n      # Run with pre-specified sequence length of 2, 3\n      out, s_fw, s_bw = sess.run(\n          [outputs, state_fw, state_bw],\n          feed_dict={\n              inputs[0]: input_value,\n              sequence_length: [2, 3]\n          })\n\n      # Since the forward and backward LSTM cells were initialized with the\n      # same parameters, the forward and backward output has to be the same,\n      # but reversed in time. The format is output[time][batch][depth], and\n      # due to depth concatenation (as num_units=3 for both RNNs):\n      # - forward output:  out[][][depth] for 0 <= depth < 3\n      # - backward output: out[][][depth] for 4 <= depth < 6\n      #\n      # First sequence in batch is length=2\n      # Check that the time=0 forward output is equal to time=1 backward output\n      self.assertAllClose(out[0][0][0], out[1][0][3])\n      self.assertAllClose(out[0][0][1], out[1][0][4])\n      self.assertAllClose(out[0][0][2], out[1][0][5])\n      # Check that the time=1 forward output is equal to time=0 backward output\n      self.assertAllClose(out[1][0][0], out[0][0][3])\n      self.assertAllClose(out[1][0][1], out[0][0][4])\n      self.assertAllClose(out[1][0][2], out[0][0][5])\n\n      # Second sequence in batch is length=3\n      # Check that the time=0 forward output is equal to time=2 backward output\n      self.assertAllClose(out[0][1][0], out[2][1][3])\n      self.assertAllClose(out[0][1][1], out[2][1][4])\n      self.assertAllClose(out[0][1][2], out[2][1][5])\n      # Check that the time=1 forward output is equal to time=1 backward output\n      self.assertAllClose(out[1][1][0], out[1][1][3])\n      self.assertAllClose(out[1][1][1], out[1][1][4])\n      self.assertAllClose(out[1][1][2], out[1][1][5])\n      # Check that the time=2 forward output is equal to time=0 backward output\n      self.assertAllClose(out[2][1][0], out[0][1][3])\n      self.assertAllClose(out[2][1][1], out[0][1][4])\n      self.assertAllClose(out[2][1][2], out[0][1][5])\n      # Via the reasoning above, the forward and backward final state should be\n      # exactly the same\n      self.assertAllClose(s_fw, s_bw)\n\n  def _testBidirectionalRNNWithoutSequenceLength(self, use_shape):\n    with self.session(graph=ops.Graph()) as sess:\n      input_value, inputs, outputs, state_fw, state_bw, _ = (\n          self._createBidirectionalRNN(use_shape, False))\n      variables_lib.global_variables_initializer().run()\n      out, s_fw, s_bw = sess.run(\n          [outputs, state_fw, state_bw], feed_dict={\n              inputs[0]: input_value\n          })\n\n      # Since the forward and backward LSTM cells were initialized with the\n      # same parameters, the forward and backward output has to be the same,\n      # but reversed in time. The format is output[time][batch][depth], and\n      # due to depth concatenation (as num_units=3 for both RNNs):\n      # - forward output:  out[][][depth] for 0 <= depth < 3\n      # - backward output: out[][][depth] for 4 <= depth < 6\n      #\n      # Both sequences in batch are length=8.  Check that the time=i\n      # forward output is equal to time=8-1-i backward output\n      for i in range(8):\n        self.assertAllClose(out[i][0][0:3], out[8 - 1 - i][0][3:6])\n        self.assertAllClose(out[i][1][0:3], out[8 - 1 - i][1][3:6])\n      # Via the reasoning above, the forward and backward final state should be\n      # exactly the same\n      self.assertAllClose(s_fw, s_bw)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalRNN(self):\n    self._testBidirectionalRNN(use_shape=False)\n    self._testBidirectionalRNN(use_shape=True)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalRNNWithoutSequenceLength(self):\n    self._testBidirectionalRNNWithoutSequenceLength(use_shape=False)\n    self._testBidirectionalRNNWithoutSequenceLength(use_shape=True)\n\n  def _createBidirectionalDynamicRNN(self,\n                                     use_shape,\n                                     use_state_tuple,\n                                     use_time_major,\n                                     use_sequence_length,\n                                     scope=None):\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    max_length = 8\n\n    initializer = init_ops.random_uniform_initializer(\n        -0.01, 0.01, seed=self._seed)\n    sequence_length = (\n        array_ops.placeholder(dtypes.int64) if use_sequence_length else None)\n    cell_fw = rnn_cell.LSTMCell(\n        num_units, initializer=initializer, state_is_tuple=use_state_tuple)\n    cell_bw = rnn_cell.LSTMCell(\n        num_units, initializer=initializer, state_is_tuple=use_state_tuple)\n    inputs = max_length * [\n        array_ops.placeholder(\n            dtypes.float32,\n            shape=(batch_size if use_shape else None, input_size))\n    ]\n    inputs_c = array_ops.stack(inputs)\n    if not use_time_major:\n      inputs_c = array_ops.transpose(inputs_c, [1, 0, 2])\n    outputs, states = rnn.bidirectional_dynamic_rnn(\n        cell_fw,\n        cell_bw,\n        inputs_c,\n        sequence_length,\n        dtype=dtypes.float32,\n        time_major=use_time_major,\n        scope=scope)\n    outputs = array_ops.concat(outputs, 2)\n    state_fw, state_bw = states\n    outputs_shape = [None, max_length, 2 * num_units]\n    if use_shape:\n      outputs_shape[0] = batch_size\n    if use_time_major:\n      outputs_shape[0], outputs_shape[1] = outputs_shape[1], outputs_shape[0]\n    self.assertEqual(outputs.get_shape().as_list(), outputs_shape)\n\n    input_value = np.random.randn(batch_size, input_size)\n\n    return input_value, inputs, outputs, state_fw, state_bw, sequence_length\n\n  def _testBidirectionalDynamicRNN(self, use_shape, use_state_tuple,\n                                   use_time_major, use_sequence_length):\n    with self.session(graph=ops.Graph()) as sess:\n      input_value, inputs, outputs, state_fw, state_bw, sequence_length = (\n          self._createBidirectionalDynamicRNN(\n              use_shape, use_state_tuple, use_time_major, use_sequence_length))\n      variables_lib.global_variables_initializer().run()\n      # Run with pre-specified sequence length of 2, 3\n      feed_dict = ({sequence_length: [2, 3]} if use_sequence_length else {})\n      feed_dict.update({inputs[0]: input_value})\n      if use_state_tuple:\n        out, c_fw, m_fw, c_bw, m_bw = sess.run(\n            [outputs, state_fw[0], state_fw[1], state_bw[0], state_bw[1]],\n            feed_dict=feed_dict)\n        s_fw = (c_fw, m_fw)\n        s_bw = (c_bw, m_bw)\n      else:\n        feed_dict.update({inputs[0]: input_value})\n        out, s_fw, s_bw = sess.run(\n            [outputs, state_fw, state_bw], feed_dict=feed_dict)\n\n      # Since the forward and backward LSTM cells were initialized with the\n      # same parameters, the forward and backward output has to be the same,\n      # but reversed in time. The format is output[time][batch][depth], and\n      # due to depth concatenation (as num_units=3 for both RNNs):\n      # - forward output:  out[][][depth] for 0 <= depth < 3\n      # - backward output: out[][][depth] for 4 <= depth < 6\n      #\n      if not use_time_major:\n        out = np.swapaxes(out, 0, 1)\n\n      if use_sequence_length:\n        # First sequence in batch is length=2\n        # Check that the t=0 forward output is equal to t=1 backward output\n        self.assertEqual(out[0][0][0], out[1][0][3])\n        self.assertEqual(out[0][0][1], out[1][0][4])\n        self.assertEqual(out[0][0][2], out[1][0][5])\n        # Check that the t=1 forward output is equal to t=0 backward output\n        self.assertEqual(out[1][0][0], out[0][0][3])\n        self.assertEqual(out[1][0][1], out[0][0][4])\n        self.assertEqual(out[1][0][2], out[0][0][5])\n\n        # Second sequence in batch is length=3\n        # Check that the t=0 forward output is equal to t=2 backward output\n        self.assertEqual(out[0][1][0], out[2][1][3])\n        self.assertEqual(out[0][1][1], out[2][1][4])\n        self.assertEqual(out[0][1][2], out[2][1][5])\n        # Check that the t=1 forward output is equal to t=1 backward output\n        self.assertEqual(out[1][1][0], out[1][1][3])\n        self.assertEqual(out[1][1][1], out[1][1][4])\n        self.assertEqual(out[1][1][2], out[1][1][5])\n        # Check that the t=2 forward output is equal to t=0 backward output\n        self.assertEqual(out[2][1][0], out[0][1][3])\n        self.assertEqual(out[2][1][1], out[0][1][4])\n        self.assertEqual(out[2][1][2], out[0][1][5])\n        # Via the reasoning above, the forward and backward final state should\n        # be exactly the same\n        self.assertAllClose(s_fw, s_bw)\n      else:  # not use_sequence_length\n        max_length = 8  # from createBidirectionalDynamicRNN\n        for t in range(max_length):\n          self.assertAllEqual(out[t, :, 0:3], out[max_length - t - 1, :, 3:6])\n        self.assertAllClose(s_fw, s_bw)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalDynamicRNN(self):\n    # Generate 2^5 option values\n    # from [True, True, True, True, True] to [False, False, False, False, False]\n    options = itertools.product([True, False], repeat=4)\n    for option in options:\n      self._testBidirectionalDynamicRNN(\n          use_shape=option[0],\n          use_state_tuple=option[1],\n          use_time_major=option[2],\n          use_sequence_length=option[3])\n\n  def _testScope(self, factory, prefix=\"prefix\", use_outer_scope=True):\n    # REMARKS: factory(scope) is a function accepting a scope\n    #          as an argument, such scope can be None, a string\n    #          or a VariableScope instance.\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          factory(scope)\n      else:\n        factory(prefix)\n\n      # check that all the variables names starts\n      # with the proper scope.\n      variables_lib.global_variables_initializer()\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"bidirectional_rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"BiRNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalRNNScope(self):\n\n    def factory(scope):\n      return self._createBidirectionalRNN(\n          use_shape=True, use_sequence_length=True, scope=scope)\n\n    self._testScope(factory, use_outer_scope=True)\n    self._testScope(factory, use_outer_scope=False)\n    self._testScope(factory, prefix=None, use_outer_scope=False)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBidirectionalDynamicRNNScope(self):\n\n    def get_factory(use_time_major):\n\n      def factory(scope):\n        return self._createBidirectionalDynamicRNN(\n            use_shape=True,\n            use_state_tuple=True,\n            use_sequence_length=True,\n            use_time_major=use_time_major,\n            scope=scope)\n\n      return factory\n\n    self._testScope(get_factory(True), use_outer_scope=True)\n    self._testScope(get_factory(True), use_outer_scope=False)\n    self._testScope(get_factory(True), prefix=None, use_outer_scope=False)\n    self._testScope(get_factory(False), use_outer_scope=True)\n    self._testScope(get_factory(False), use_outer_scope=False)\n    self._testScope(get_factory(False), prefix=None, use_outer_scope=False)\n\n\nclass MultiDimensionalLSTMTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testMultiDimensionalLSTMAllRNNContainers(self):\n    feature_dims = (3, 4, 5)\n    input_size = feature_dims\n    batch_size = 2\n    max_length = 8\n    sequence_length = [4, 6]\n    with self.session(graph=ops.Graph()) as sess:\n      inputs = max_length * [\n          array_ops.placeholder(dtypes.float32, shape=(None,) + input_size)\n      ]\n      inputs_using_dim = max_length * [\n          array_ops.placeholder(\n              dtypes.float32, shape=(batch_size,) + input_size)\n      ]\n      inputs_c = array_ops.stack(inputs)\n      # Create a cell for the whole test. This is fine because the cell has no\n      # variables.\n      cell = DummyMultiDimensionalLSTM(feature_dims)\n      state_saver = TestStateSaver(batch_size, input_size)\n      outputs_static, state_static = rnn.static_rnn(\n          cell, inputs, dtype=dtypes.float32, sequence_length=sequence_length)\n      outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n          cell,\n          inputs_c,\n          dtype=dtypes.float32,\n          time_major=True,\n          sequence_length=sequence_length)\n      outputs_bid, state_fw, state_bw = rnn.static_bidirectional_rnn(\n          cell,\n          cell,\n          inputs_using_dim,\n          dtype=dtypes.float32,\n          sequence_length=sequence_length)\n      outputs_sav, state_sav = rnn.static_state_saving_rnn(\n          cell,\n          inputs_using_dim,\n          sequence_length=sequence_length,\n          state_saver=state_saver,\n          state_name=(\"h\", \"c\"))\n\n      self.assertEqual(outputs_dynamic.get_shape().as_list(),\n                       inputs_c.get_shape().as_list())\n      for out, inp in zip(outputs_static, inputs):\n        self.assertEqual(out.get_shape().as_list(), inp.get_shape().as_list())\n      for out, inp in zip(outputs_bid, inputs_using_dim):\n        input_shape_list = inp.get_shape().as_list()\n        # fwd and bwd activations are concatenated along the second dim.\n        input_shape_list[1] *= 2\n        self.assertEqual(out.get_shape().as_list(), input_shape_list)\n\n      variables_lib.global_variables_initializer().run()\n\n      input_total_size = (batch_size,) + input_size\n      input_value = np.random.randn(*input_total_size)\n      outputs_static_v = sess.run(\n          outputs_static, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs_dynamic_v = sess.run(\n          outputs_dynamic, feed_dict={\n              inputs[0]: input_value\n          })\n      outputs_bid_v = sess.run(\n          outputs_bid, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n      outputs_sav_v = sess.run(\n          outputs_sav, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n\n      self.assertAllEqual(outputs_static_v, outputs_dynamic_v)\n      self.assertAllEqual(outputs_static_v, outputs_sav_v)\n      outputs_static_array = np.array(outputs_static_v)\n      outputs_static_array_double = np.concatenate(\n          (outputs_static_array, outputs_static_array), axis=2)\n      outputs_bid_array = np.array(outputs_bid_v)\n      self.assertAllEqual(outputs_static_array_double, outputs_bid_array)\n\n      state_static_v = sess.run(\n          state_static, feed_dict={\n              inputs[0]: input_value\n          })\n      state_dynamic_v = sess.run(\n          state_dynamic, feed_dict={\n              inputs[0]: input_value\n          })\n      state_bid_fw_v = sess.run(\n          state_fw, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n      state_bid_bw_v = sess.run(\n          state_bw, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n      state_sav_v = sess.run(\n          state_sav, feed_dict={\n              inputs_using_dim[0]: input_value\n          })\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_dynamic_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_sav_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_bid_fw_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_bid_bw_v))\n\n\nclass NestedLSTMTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testNestedIOLSTMAllRNNContainers(self):\n    input_size = 5\n    batch_size = 2\n    state_size = 6\n    max_length = 8\n    sequence_length = [4, 6]\n    with self.session(graph=ops.Graph()) as sess:\n      state_saver = TestStateSaver(batch_size, state_size)\n      single_input = (array_ops.placeholder(\n          dtypes.float32, shape=(None, input_size)),\n                      array_ops.placeholder(\n                          dtypes.float32, shape=(None, input_size)))\n      inputs = max_length * [single_input]\n      inputs_c = (array_ops.stack([input_[0] for input_ in inputs]),\n                  array_ops.stack([input_[1] for input_ in inputs]))\n      single_input_using_dim = (array_ops.placeholder(\n          dtypes.float32, shape=(batch_size, input_size)),\n                                array_ops.placeholder(\n                                    dtypes.float32,\n                                    shape=(batch_size, input_size)))\n      inputs_using_dim = max_length * [single_input_using_dim]\n\n      # Create a cell for the whole test. This is fine because the cell has no\n      # variables.\n      cell = NestedRNNCell()\n      outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n          cell,\n          inputs_c,\n          dtype=dtypes.float32,\n          time_major=True,\n          sequence_length=sequence_length)\n      outputs_static, state_static = rnn.static_rnn(\n          cell, inputs, dtype=dtypes.float32, sequence_length=sequence_length)\n      outputs_bid, state_fw, state_bw = rnn.static_bidirectional_rnn(\n          cell,\n          cell,\n          inputs_using_dim,\n          dtype=dtypes.float32,\n          sequence_length=sequence_length)\n      outputs_sav, state_sav = rnn.static_state_saving_rnn(\n          cell,\n          inputs_using_dim,\n          sequence_length=sequence_length,\n          state_saver=state_saver,\n          state_name=(\"h\", \"c\"))\n\n      def _assert_same_shape(input1, input2, double=False):\n        flat_input1 = nest.flatten(input1)\n        flat_input2 = nest.flatten(input2)\n        for inp1, inp2 in zip(flat_input1, flat_input2):\n          input_shape = inp1.get_shape().as_list()\n          if double:\n            input_shape[1] *= 2\n          self.assertEqual(input_shape, inp2.get_shape().as_list())\n\n      _assert_same_shape(inputs_c, outputs_dynamic)\n      _assert_same_shape(inputs, outputs_static)\n      _assert_same_shape(inputs_using_dim, outputs_sav)\n      _assert_same_shape(inputs_using_dim, outputs_bid, double=True)\n\n      variables_lib.global_variables_initializer().run()\n\n      input_total_size = (batch_size, input_size)\n      input_value = (np.random.randn(*input_total_size),\n                     np.random.randn(*input_total_size))\n      outputs_dynamic_v = sess.run(\n          outputs_dynamic, feed_dict={\n              single_input: input_value\n          })\n      outputs_static_v = sess.run(\n          outputs_static, feed_dict={\n              single_input: input_value\n          })\n      outputs_sav_v = sess.run(\n          outputs_sav, feed_dict={\n              single_input_using_dim: input_value\n          })\n      outputs_bid_v = sess.run(\n          outputs_bid, feed_dict={\n              single_input_using_dim: input_value\n          })\n\n      self.assertAllEqual(outputs_static_v,\n                          np.transpose(outputs_dynamic_v, (1, 0, 2, 3)))\n      self.assertAllEqual(outputs_static_v, outputs_sav_v)\n      outputs_static_array = np.array(outputs_static_v)\n      outputs_static_array_double = np.concatenate(\n          (outputs_static_array, outputs_static_array), axis=3)\n      outputs_bid_array = np.array(outputs_bid_v)\n      self.assertAllEqual(outputs_static_array_double, outputs_bid_array)\n\n      state_dynamic_v = sess.run(\n          state_dynamic, feed_dict={\n              single_input: input_value\n          })\n      state_static_v = sess.run(\n          state_static, feed_dict={\n              single_input: input_value\n          })\n      state_bid_fw_v = sess.run(\n          state_fw, feed_dict={\n              single_input_using_dim: input_value\n          })\n      state_bid_bw_v = sess.run(\n          state_bw, feed_dict={\n              single_input_using_dim: input_value\n          })\n      state_sav_v = sess.run(\n          state_sav, feed_dict={\n              single_input_using_dim: input_value\n          })\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_dynamic_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_sav_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_bid_fw_v))\n      self.assertAllEqual(np.hstack(state_static_v), np.hstack(state_bid_bw_v))\n\n\nclass StateSaverRNNTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  def _factory(self, scope, state_saver):\n    num_units = state_saver.state_size // 2\n    batch_size = state_saver.batch_size\n    input_size = 5\n    max_length = 8\n    initializer = init_ops.random_uniform_initializer(\n        -0.01, 0.01, seed=self._seed)\n    cell = rnn_cell.LSTMCell(\n        num_units,\n        use_peepholes=False,\n        initializer=initializer,\n        state_is_tuple=False)\n    inputs = max_length * [\n        array_ops.zeros(dtype=dtypes.float32, shape=(batch_size, input_size))\n    ]\n    out, state = rnn.static_state_saving_rnn(\n        cell,\n        inputs,\n        state_saver=state_saver,\n        state_name=\"save_lstm\",\n        scope=scope)\n    return out, state, state_saver\n\n  def _testScope(self, prefix=\"prefix\", use_outer_scope=True):\n    num_units = 3\n    batch_size = 2\n    state_saver = TestStateSaver(batch_size, 2 * num_units)\n\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          self._factory(scope=scope, state_saver=state_saver)\n      else:\n        self._factory(scope=prefix, state_saver=state_saver)\n        variables_lib.global_variables_initializer()\n\n      # check that all the variables names starts\n      # with the proper scope.\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"RNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  def testStateSaverRNNScope(self):\n    self._testScope(use_outer_scope=True)\n    self._testScope(use_outer_scope=False)\n    self._testScope(prefix=None, use_outer_scope=False)\n\n  def testStateSaverCallsSaveState(self):\n    \"\"\"Test that number of calls to state and save_state is equal.\n\n    Test if the order of actual evaluating or skipping evaluation of out,\n    state tensors, which are the output tensors from static_state_saving_rnn,\n    have influence on number of calls to save_state and state methods of\n    state_saver object (the number of calls should be same.)\n    \"\"\"\n    self.skipTest(\"b/124196246 Breakage for sess.run([out, ...]): 2 != 1\")\n\n    num_units = 3\n    batch_size = 2\n    state_saver = TestStateSaverWithCounters(batch_size, 2 * num_units)\n    out, state, state_saver = self._factory(scope=None, state_saver=state_saver)\n\n    with self.cached_session() as sess:\n      sess.run(variables_lib.global_variables_initializer())\n      sess.run(variables_lib.local_variables_initializer())\n\n      _, _, num_state_calls, num_save_state_calls = sess.run([\n          out,\n          state,\n          state_saver.num_state_calls,\n          state_saver.num_save_state_calls])\n      self.assertEqual(num_state_calls, num_save_state_calls)\n\n      _, num_state_calls, num_save_state_calls = sess.run([\n          out,\n          state_saver.num_state_calls,\n          state_saver.num_save_state_calls])\n      self.assertEqual(num_state_calls, num_save_state_calls)\n\n      _, num_state_calls, num_save_state_calls = sess.run([\n          state,\n          state_saver.num_state_calls,\n          state_saver.num_save_state_calls])\n      self.assertEqual(num_state_calls, num_save_state_calls)\n\nclass GRUTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDynamic(self):\n    time_steps = 8\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n\n    input_values = np.random.randn(time_steps, batch_size, input_size)\n\n    sequence_length = np.random.randint(0, time_steps, size=batch_size)\n\n    with self.session(graph=ops.Graph()) as sess:\n      concat_inputs = array_ops.placeholder(\n          dtypes.float32, shape=(time_steps, batch_size, input_size))\n\n      cell = rnn_cell.GRUCell(num_units=num_units)\n\n      with variable_scope.variable_scope(\"dynamic_scope\"):\n        outputs_dynamic, state_dynamic = rnn.dynamic_rnn(\n            cell,\n            inputs=concat_inputs,\n            sequence_length=sequence_length,\n            time_major=True,\n            dtype=dtypes.float32)\n\n      feeds = {concat_inputs: input_values}\n\n      # Initialize\n      variables_lib.global_variables_initializer().run(feed_dict=feeds)\n\n      sess.run([outputs_dynamic, state_dynamic], feed_dict=feeds)\n\n  def _testScope(self, factory, prefix=\"prefix\", use_outer_scope=True):\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          factory(scope)\n      else:\n        factory(prefix)\n        variables_lib.global_variables_initializer()\n\n      # check that all the variables names starts\n      # with the proper scope.\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"RNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDynamicScope(self):\n    time_steps = 8\n    num_units = 3\n    input_size = 5\n    batch_size = 2\n    sequence_length = np.random.randint(0, time_steps, size=batch_size)\n\n    def factory(scope):\n      concat_inputs = array_ops.placeholder(\n          dtypes.float32, shape=(time_steps, batch_size, input_size))\n      cell = rnn_cell.GRUCell(num_units=num_units)\n      return rnn.dynamic_rnn(\n          cell,\n          inputs=concat_inputs,\n          sequence_length=sequence_length,\n          time_major=True,\n          dtype=dtypes.float32,\n          scope=scope)\n\n    self._testScope(factory, use_outer_scope=True)\n    self._testScope(factory, use_outer_scope=False)\n    self._testScope(factory, prefix=None, use_outer_scope=False)\n\n\nclass RawRNNTest(test.TestCase):\n\n  def setUp(self):\n    self._seed = 23489\n    np.random.seed(self._seed)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def _testRawRNN(self, max_time):\n    with self.session(graph=ops.Graph()) as sess:\n      batch_size = 16\n      input_depth = 4\n      num_units = 3\n\n      inputs = array_ops.placeholder(\n          shape=(max_time, batch_size, input_depth), dtype=dtypes.float32)\n      sequence_length = array_ops.placeholder(\n          shape=(batch_size,), dtype=dtypes.int32)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, unused_loop_state):\n        emit_output = cell_output  # == None for time == 0\n        if cell_output is None:  # time == 0\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          next_state = cell_state  # copy state through\n        elements_finished = (time_ >= sequence_length)\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output, None)\n\n      reuse_scope = variable_scope.get_variable_scope()\n\n      outputs_ta, final_state, _ = rnn.raw_rnn(cell, loop_fn, scope=reuse_scope)\n      outputs = outputs_ta.stack()\n\n      reuse_scope.reuse_variables()\n      outputs_dynamic_rnn, final_state_dynamic_rnn = rnn.dynamic_rnn(\n          cell,\n          inputs,\n          time_major=True,\n          dtype=dtypes.float32,\n          sequence_length=sequence_length,\n          scope=reuse_scope)\n\n      variables = variables_lib.trainable_variables()\n      gradients = gradients_impl.gradients([outputs, final_state],\n                                           [inputs] + variables)\n      gradients_dynamic_rnn = gradients_impl.gradients(\n          [outputs_dynamic_rnn, final_state_dynamic_rnn], [inputs] + variables)\n\n      variables_lib.global_variables_initializer().run()\n\n      rand_input = np.random.randn(max_time, batch_size, input_depth)\n      if max_time == 0:\n        rand_seq_len = np.zeros(batch_size)\n      else:\n        rand_seq_len = np.random.randint(max_time, size=batch_size)\n\n      # To ensure same output lengths for dynamic_rnn and raw_rnn\n      rand_seq_len[0] = max_time\n\n      (outputs_val, outputs_dynamic_rnn_val, final_state_val,\n       final_state_dynamic_rnn_val) = sess.run(\n           [outputs, outputs_dynamic_rnn, final_state, final_state_dynamic_rnn],\n           feed_dict={\n               inputs: rand_input,\n               sequence_length: rand_seq_len\n           })\n\n      self.assertAllClose(outputs_dynamic_rnn_val, outputs_val)\n      self.assertAllClose(final_state_dynamic_rnn_val, final_state_val)\n\n      # NOTE: Because with 0 time steps, raw_rnn does not have shape\n      # information about the input, it is impossible to perform\n      # gradients comparisons as the gradients eval will fail.  So\n      # this case skips the gradients test.\n      if max_time > 0:\n        self.assertEqual(len(gradients), len(gradients_dynamic_rnn))\n        gradients_val = sess.run(\n            gradients,\n            feed_dict={\n                inputs: rand_input,\n                sequence_length: rand_seq_len\n            })\n        gradients_dynamic_rnn_val = sess.run(\n            gradients_dynamic_rnn,\n            feed_dict={\n                inputs: rand_input,\n                sequence_length: rand_seq_len\n            })\n        self.assertEqual(len(gradients_val), len(gradients_dynamic_rnn_val))\n        input_gradients_val = gradients_val[0]\n        input_gradients_dynamic_rnn_val = gradients_dynamic_rnn_val[0]\n        self.assertAllClose(input_gradients_val,\n                            input_gradients_dynamic_rnn_val)\n        for i in range(1, len(gradients_val)):\n          self.assertAllClose(gradients_dynamic_rnn_val[i], gradients_val[i])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRawRNNZeroLength(self):\n    # NOTE: Because with 0 time steps, raw_rnn does not have shape\n    # information about the input, it is impossible to perform\n    # gradients comparisons as the gradients eval will fail.  So this\n    # case skips the gradients test.\n    self._testRawRNN(max_time=0)\n\n  def testRawRNN(self):\n    self._testRawRNN(max_time=10)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testLoopState(self):\n    with self.session(graph=ops.Graph()):\n      max_time = 10\n      batch_size = 16\n      input_depth = 4\n      num_units = 3\n\n      inputs = np.random.randn(max_time, batch_size, input_depth)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, loop_state):\n        if cell_output is None:\n          loop_state = constant_op.constant([0])\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          loop_state = array_ops.stack([array_ops.squeeze(loop_state) + 1])\n          next_state = cell_state\n        emit_output = cell_output  # == None for time == 0\n        elements_finished = array_ops.tile([time_ >= max_time], [batch_size])\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output,\n                loop_state)\n\n      r = rnn.raw_rnn(cell, loop_fn)\n      loop_state = r[-1]\n      self.assertEqual([10], self.evaluate(loop_state))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testLoopStateWithTensorArray(self):\n    with self.session(graph=ops.Graph()):\n      max_time = 4\n      batch_size = 16\n      input_depth = 4\n      num_units = 3\n\n      inputs = np.random.randn(max_time, batch_size, input_depth)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, loop_state):\n        if cell_output is None:\n          loop_state = tensor_array_ops.TensorArray(\n              dynamic_size=True,\n              size=0,\n              dtype=dtypes.int32,\n              clear_after_read=False)\n          loop_state = loop_state.write(0, 1)\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          loop_state = loop_state.write(time_,\n                                        loop_state.read(time_ - 1) + time_)\n          next_state = cell_state\n        emit_output = cell_output  # == None for time == 0\n        elements_finished = array_ops.tile([time_ >= max_time], [batch_size])\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output,\n                loop_state)\n\n      r = rnn.raw_rnn(cell, loop_fn)\n      loop_state = r[-1]\n      loop_state = loop_state.stack()\n      self.assertAllEqual([1, 2, 2 + 2, 4 + 3, 7 + 4], loop_state)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testEmitDifferentStructureThanCellOutput(self):\n    with self.session(graph=ops.Graph()) as sess:\n      max_time = 10\n      batch_size = 16\n      input_depth = 4\n      num_units = 3\n\n      inputs = np.random.randn(max_time, batch_size, input_depth)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n      # Verify emit shapes may be unknown by feeding a placeholder that\n      # determines an emit shape.\n      unknown_dim = array_ops.placeholder(dtype=dtypes.int32)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, _):\n        if cell_output is None:\n          emit_output = (array_ops.zeros([2, 3], dtype=dtypes.int32),\n                         array_ops.zeros([unknown_dim], dtype=dtypes.int64))\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          emit_output = (array_ops.ones([batch_size, 2, 3], dtype=dtypes.int32),\n                         array_ops.ones(\n                             [batch_size, unknown_dim], dtype=dtypes.int64))\n          next_state = cell_state\n        elements_finished = array_ops.tile([time_ >= max_time], [batch_size])\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output, None)\n\n      r = rnn.raw_rnn(cell, loop_fn)\n      output_ta = r[0]\n      self.assertEqual(2, len(output_ta))\n      self.assertEqual([dtypes.int32, dtypes.int64],\n                       [ta.dtype for ta in output_ta])\n      output = [ta.stack() for ta in output_ta]\n      output_vals = sess.run(output, feed_dict={unknown_dim: 1})\n      self.assertAllEqual(\n          np.ones((max_time, batch_size, 2, 3), np.int32), output_vals[0])\n      self.assertAllEqual(\n          np.ones((max_time, batch_size, 1), np.int64), output_vals[1])\n\n  def _testScope(self, factory, prefix=\"prefix\", use_outer_scope=True):\n    with self.session(graph=ops.Graph()):\n      if use_outer_scope:\n        with variable_scope.variable_scope(prefix) as scope:\n          factory(scope)\n      else:\n        factory(prefix)\n        variables_lib.global_variables_initializer()\n\n      # check that all the variables names starts\n      # with the proper scope.\n      all_vars = variables_lib.global_variables()\n      prefix = prefix or \"rnn\"\n      scope_vars = [v for v in all_vars if v.name.startswith(prefix + \"/\")]\n      tf_logging.info(\"RNN with scope: %s (%s)\" %\n                      (prefix, \"scope\" if use_outer_scope else \"str\"))\n      for v in scope_vars:\n        tf_logging.info(v.name)\n      self.assertEqual(len(scope_vars), len(all_vars))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRawRNNScope(self):\n    max_time = 10\n    batch_size = 16\n    input_depth = 4\n    num_units = 3\n\n    def factory(scope):\n      inputs = array_ops.placeholder(\n          shape=(max_time, batch_size, input_depth), dtype=dtypes.float32)\n      sequence_length = array_ops.placeholder(\n          shape=(batch_size,), dtype=dtypes.int32)\n      inputs_ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n      inputs_ta = inputs_ta.unstack(inputs)\n\n      cell = rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n\n      def loop_fn(time_, cell_output, cell_state, unused_loop_state):\n        emit_output = cell_output  # == None for time == 0\n        if cell_output is None:  # time == 0\n          next_state = cell.zero_state(batch_size, dtypes.float32)\n        else:\n          next_state = cell_state\n\n        elements_finished = (time_ >= sequence_length)\n        finished = math_ops.reduce_all(elements_finished)\n        # For the very final iteration, we must emit a dummy input\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time_))\n        return (elements_finished, next_input, next_state, emit_output, None)\n\n      return rnn.raw_rnn(cell, loop_fn, scope=scope)\n\n    self._testScope(factory, use_outer_scope=True)\n    self._testScope(factory, use_outer_scope=False)\n    self._testScope(factory, prefix=None, use_outer_scope=False)\n\n\nclass DeviceWrapperCell(rnn_cell.RNNCell):\n  \"\"\"Class to ensure cell calculation happens on a specific device.\"\"\"\n\n  def __init__(self, cell, device):\n    self._cell = cell\n    self._device = device\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  def __call__(self, input_, state, scope=None):\n    if self._device is not None:\n      with ops.device(self._device):\n        return self._cell(input_, state, scope=scope)\n    else:\n      return self._cell(input_, state, scope=scope)\n\n\nclass TensorArrayOnCorrectDeviceTest(test.TestCase):\n\n  def _execute_rnn_on(self,\n                      rnn_device=None,\n                      cell_device=None,\n                      input_device=None):\n    batch_size = 3\n    time_steps = 7\n    input_size = 5\n    num_units = 10\n\n    cell = rnn_cell.LSTMCell(num_units, use_peepholes=True)\n    gpu_cell = DeviceWrapperCell(cell, cell_device)\n    inputs = np.random.randn(batch_size, time_steps, input_size).astype(\n        np.float32)\n    sequence_length = np.random.randint(0, time_steps, size=batch_size)\n\n    if input_device is not None:\n      with ops.device(input_device):\n        inputs = constant_op.constant(inputs)\n\n    if rnn_device is not None:\n      with ops.device(rnn_device):\n        outputs, _ = rnn.dynamic_rnn(\n            gpu_cell,\n            inputs,\n            sequence_length=sequence_length,\n            dtype=dtypes.float32)\n    else:\n      outputs, _ = rnn.dynamic_rnn(\n          gpu_cell,\n          inputs,\n          sequence_length=sequence_length,\n          dtype=dtypes.float32)\n\n    with self.session() as sess:\n      opts = config_pb2.RunOptions(trace_level=config_pb2.RunOptions.FULL_TRACE)\n      run_metadata = config_pb2.RunMetadata()\n      variables_lib.global_variables_initializer().run()\n      sess.run(outputs, options=opts, run_metadata=run_metadata)\n\n    return run_metadata\n\n  def _retrieve_cpu_gpu_stats(self, run_metadata):\n    cpu_stats = None\n    gpu_stats = None\n    step_stats = run_metadata.step_stats\n    for ds in step_stats.dev_stats:\n      if \"cpu:0\" in ds.device[-5:].lower():\n        cpu_stats = ds.node_stats\n      if \"gpu:0\" == ds.device[-5:].lower():\n        gpu_stats = ds.node_stats\n    return cpu_stats, gpu_stats\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRNNOnCPUCellOnGPU(self):\n    if not test.is_gpu_available():\n      return  # Test requires access to a GPU\n\n    gpu_dev = test.gpu_device_name()\n    run_metadata = self._execute_rnn_on(\n        rnn_device=\"/cpu:0\", cell_device=gpu_dev)\n    cpu_stats, gpu_stats = self._retrieve_cpu_gpu_stats(run_metadata)\n\n    def _assert_in(op_str, in_stats, out_stats):\n      self.assertTrue(any(op_str in s.node_name for s in in_stats))\n      self.assertFalse(any(op_str in s.node_name for s in out_stats))\n\n    # Writes happen at output of RNN cell\n    _assert_in(\"TensorArrayWrite\", gpu_stats, cpu_stats)\n    # Gather happens on final TensorArray\n    _assert_in(\"TensorArrayGather\", gpu_stats, cpu_stats)\n    # Reads happen at input to RNN cell\n    _assert_in(\"TensorArrayRead\", cpu_stats, gpu_stats)\n    # Scatters happen to get initial input into TensorArray\n    _assert_in(\"TensorArrayScatter\", cpu_stats, gpu_stats)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testRNNOnCPUCellOnCPU(self):\n    if not test.is_gpu_available():\n      return  # Test requires access to a GPU\n\n    gpu_dev = test.gpu_device_name()\n    run_metadata = self._execute_rnn_on(\n        rnn_device=\"/cpu:0\", cell_device=\"/cpu:0\", input_device=gpu_dev)\n    cpu_stats, gpu_stats = self._retrieve_cpu_gpu_stats(run_metadata)\n\n    def _assert_in(op_str, in_stats, out_stats):\n      self.assertTrue(any(op_str in s.node_name for s in in_stats))\n      self.assertFalse(any(op_str in s.node_name for s in out_stats))\n\n    # All TensorArray operations happen on CPU\n    _assert_in(\"TensorArray\", cpu_stats, gpu_stats)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testInputOnGPUCellNotDeclared(self):\n    if not test.is_gpu_available():\n      return  # Test requires access to a GPU\n\n    gpu_dev = test.gpu_device_name()\n    run_metadata = self._execute_rnn_on(input_device=gpu_dev)\n    cpu_stats, gpu_stats = self._retrieve_cpu_gpu_stats(run_metadata)\n\n    def _assert_in(op_str, in_stats, out_stats):\n      self.assertTrue(any(op_str in s.node_name for s in in_stats))\n      self.assertFalse(any(op_str in s.node_name for s in out_stats))\n\n    # Everything happens on GPU\n    _assert_in(\"TensorArray\", gpu_stats, cpu_stats)\n\n\nclass RNNCellTest(test.TestCase, parameterized.TestCase):\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicRNNCell(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m = array_ops.zeros([1, 2])\n        cell = rnn_cell_impl.BasicRNNCell(2)\n        g, _ = cell(x, m)\n        self.assertEqual([\n            \"root/basic_rnn_cell/%s:0\" % rnn_cell_impl._WEIGHTS_VARIABLE_NAME,\n            \"root/basic_rnn_cell/%s:0\" % rnn_cell_impl._BIAS_VARIABLE_NAME\n        ], [v.name for v in cell.trainable_variables])\n        self.assertFalse(cell.non_trainable_variables)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run([g], {\n            x: np.array([[1., 1.]]),\n            m: np.array([[0.1, 0.1]])\n        })\n        self.assertEqual(res[0].shape, (1, 2))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicRNNCellNotTrainable(self):\n    with self.cached_session() as sess:\n\n      def not_trainable_getter(getter, *args, **kwargs):\n        kwargs[\"trainable\"] = False\n        return getter(*args, **kwargs)\n\n      with variable_scope.variable_scope(\n          \"root\",\n          initializer=init_ops.constant_initializer(0.5),\n          custom_getter=not_trainable_getter):\n        x = array_ops.zeros([1, 2])\n        m = array_ops.zeros([1, 2])\n        cell = rnn_cell_impl.BasicRNNCell(2)\n        g, _ = cell(x, m)\n        self.assertFalse(cell.trainable_variables)\n        self.assertEqual([\n            \"root/basic_rnn_cell/%s:0\" % rnn_cell_impl._WEIGHTS_VARIABLE_NAME,\n            \"root/basic_rnn_cell/%s:0\" % rnn_cell_impl._BIAS_VARIABLE_NAME\n        ], [v.name for v in cell.non_trainable_variables])\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run([g], {\n            x: np.array([[1., 1.]]),\n            m: np.array([[0.1, 0.1]])\n        })\n        self.assertEqual(res[0].shape, (1, 2))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testGRUCell(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m = array_ops.zeros([1, 2])\n        g, _ = rnn_cell_impl.GRUCell(2)(x, m)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run([g], {\n            x: np.array([[1., 1.]]),\n            m: np.array([[0.1, 0.1]])\n        })\n        # Smoke test\n        self.assertAllClose(res[0], [[0.175991, 0.175991]])\n      with variable_scope.variable_scope(\n          \"other\", initializer=init_ops.constant_initializer(0.5)):\n        # Test GRUCell with input_size != num_units.\n        x = array_ops.zeros([1, 3])\n        m = array_ops.zeros([1, 2])\n        g, _ = rnn_cell_impl.GRUCell(2)(x, m)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run([g], {\n            x: np.array([[1., 1., 1.]]),\n            m: np.array([[0.1, 0.1]])\n        })\n        # Smoke test\n        self.assertAllClose(res[0], [[0.156736, 0.156736]])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicLSTMCell(self):\n    for dtype in [dtypes.float16, dtypes.float32]:\n      np_dtype = dtype.as_numpy_dtype\n      with self.session(graph=ops.Graph()) as sess:\n        with variable_scope.variable_scope(\n            \"root\", initializer=init_ops.constant_initializer(0.5)):\n          x = array_ops.zeros([1, 2], dtype=dtype)\n          m = array_ops.zeros([1, 8], dtype=dtype)\n          cell = rnn_cell_impl.MultiRNNCell(\n              [\n                  rnn_cell_impl.BasicLSTMCell(2, state_is_tuple=False)\n                  for _ in range(2)\n              ],\n              state_is_tuple=False)\n          self.assertEqual(cell.dtype, None)\n          self.assertIn(\"cell-0\", cell._trackable_children())\n          self.assertIn(\"cell-1\", cell._trackable_children())\n          cell.get_config()  # Should not throw an error\n          g, out_m = cell(x, m)\n          # Layer infers the input type.\n          self.assertEqual(cell.dtype, dtype.name)\n          expected_variable_names = [\n              \"root/multi_rnn_cell/cell_0/basic_lstm_cell/%s:0\" %\n              rnn_cell_impl._WEIGHTS_VARIABLE_NAME,\n              \"root/multi_rnn_cell/cell_0/basic_lstm_cell/%s:0\" %\n              rnn_cell_impl._BIAS_VARIABLE_NAME,\n              \"root/multi_rnn_cell/cell_1/basic_lstm_cell/%s:0\" %\n              rnn_cell_impl._WEIGHTS_VARIABLE_NAME,\n              \"root/multi_rnn_cell/cell_1/basic_lstm_cell/%s:0\" %\n              rnn_cell_impl._BIAS_VARIABLE_NAME\n          ]\n          self.assertEqual(expected_variable_names,\n                           [v.name for v in cell.trainable_variables])\n          self.assertFalse(cell.non_trainable_variables)\n          sess.run([variables_lib.global_variables_initializer()])\n          res = sess.run([g, out_m], {\n              x: np.array([[1., 1.]]),\n              m: 0.1 * np.ones([1, 8])\n          })\n          self.assertEqual(len(res), 2)\n          variables = variables_lib.global_variables()\n          self.assertEqual(expected_variable_names, [v.name for v in variables])\n          # The numbers in results were not calculated, this is just a\n          # smoke test.\n          self.assertAllClose(res[0], np.array(\n              [[0.240, 0.240]], dtype=np_dtype), 1e-2)\n          expected_mem = np.array(\n              [[0.689, 0.689, 0.448, 0.448, 0.398, 0.398, 0.240, 0.240]],\n              dtype=np_dtype)\n          self.assertAllClose(res[1], expected_mem, 1e-2)\n        with variable_scope.variable_scope(\n            \"other\", initializer=init_ops.constant_initializer(0.5)):\n          # Test BasicLSTMCell with input_size != num_units.\n          x = array_ops.zeros([1, 3], dtype=dtype)\n          m = array_ops.zeros([1, 4], dtype=dtype)\n          g, out_m = rnn_cell_impl.BasicLSTMCell(2, state_is_tuple=False)(x, m)\n          sess.run([variables_lib.global_variables_initializer()])\n          res = sess.run(\n              [g, out_m], {\n                  x: np.array([[1., 1., 1.]], dtype=np_dtype),\n                  m: 0.1 * np.ones([1, 4], dtype=np_dtype)\n              })\n          self.assertEqual(len(res), 2)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicLSTMCellDimension0Error(self):\n    \"\"\"Tests that dimension 0 in both(x and m) shape must be equal.\"\"\"\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        num_units = 2\n        state_size = num_units * 2\n        batch_size = 3\n        input_size = 4\n        x = array_ops.zeros([batch_size, input_size])\n        m = array_ops.zeros([batch_size - 1, state_size])\n        with self.assertRaises(ValueError):\n          g, out_m = rnn_cell_impl.BasicLSTMCell(\n              num_units, state_is_tuple=False)(x, m)\n          sess.run([variables_lib.global_variables_initializer()])\n          sess.run(\n              [g, out_m], {\n                  x: 1 * np.ones([batch_size, input_size]),\n                  m: 0.1 * np.ones([batch_size - 1, state_size])\n              })\n\n  def testBasicLSTMCellStateSizeError(self):\n    \"\"\"Tests that state_size must be num_units * 2.\"\"\"\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        num_units = 2\n        state_size = num_units * 3  # state_size must be num_units * 2\n        batch_size = 3\n        input_size = 4\n        x = array_ops.zeros([batch_size, input_size])\n        m = array_ops.zeros([batch_size, state_size])\n        with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n          g, out_m = rnn_cell_impl.BasicLSTMCell(\n              num_units, state_is_tuple=False)(x, m)\n          sess.run([variables_lib.global_variables_initializer()])\n          sess.run(\n              [g, out_m], {\n                  x: 1 * np.ones([batch_size, input_size]),\n                  m: 0.1 * np.ones([batch_size, state_size])\n              })\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicLSTMCellStateTupleType(self):\n    with self.cached_session():\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m0 = (array_ops.zeros([1, 2]),) * 2\n        m1 = (array_ops.zeros([1, 2]),) * 2\n        cell = rnn_cell_impl.MultiRNNCell(\n            [rnn_cell_impl.BasicLSTMCell(2) for _ in range(2)],\n            state_is_tuple=True)\n        self.assertTrue(isinstance(cell.state_size, tuple))\n        self.assertTrue(\n            isinstance(cell.state_size[0], rnn_cell_impl.LSTMStateTuple))\n        self.assertTrue(\n            isinstance(cell.state_size[1], rnn_cell_impl.LSTMStateTuple))\n\n        # Pass in regular tuples\n        _, (out_m0, out_m1) = cell(x, (m0, m1))\n        self.assertTrue(isinstance(out_m0, rnn_cell_impl.LSTMStateTuple))\n        self.assertTrue(isinstance(out_m1, rnn_cell_impl.LSTMStateTuple))\n\n        # Pass in LSTMStateTuples\n        variable_scope.get_variable_scope().reuse_variables()\n        zero_state = cell.zero_state(1, dtypes.float32)\n        self.assertTrue(isinstance(zero_state, tuple))\n        self.assertTrue(isinstance(zero_state[0], rnn_cell_impl.LSTMStateTuple))\n        self.assertTrue(isinstance(zero_state[1], rnn_cell_impl.LSTMStateTuple))\n        _, (out_m0, out_m1) = cell(x, zero_state)\n        self.assertTrue(isinstance(out_m0, rnn_cell_impl.LSTMStateTuple))\n        self.assertTrue(isinstance(out_m1, rnn_cell_impl.LSTMStateTuple))\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testBasicLSTMCellWithStateTuple(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m0 = array_ops.zeros([1, 4])\n        m1 = array_ops.zeros([1, 4])\n        cell = rnn_cell_impl.MultiRNNCell(\n            [\n                rnn_cell_impl.BasicLSTMCell(2, state_is_tuple=False)\n                for _ in range(2)\n            ],\n            state_is_tuple=True)\n        g, (out_m0, out_m1) = cell(x, (m0, m1))\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run(\n            [g, out_m0, out_m1], {\n                x: np.array([[1., 1.]]),\n                m0: 0.1 * np.ones([1, 4]),\n                m1: 0.1 * np.ones([1, 4])\n            })\n        self.assertEqual(len(res), 3)\n        # The numbers in results were not calculated, this is just a smoke test.\n        # Note, however, these values should match the original\n        # version having state_is_tuple=False.\n        self.assertAllClose(res[0], [[0.24024698, 0.24024698]])\n        expected_mem0 = np.array(\n            [[0.68967271, 0.68967271, 0.44848421, 0.44848421]])\n        expected_mem1 = np.array(\n            [[0.39897051, 0.39897051, 0.24024698, 0.24024698]])\n        self.assertAllClose(res[1], expected_mem0)\n        self.assertAllClose(res[2], expected_mem1)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testLSTMCell(self):\n    with self.cached_session() as sess:\n      num_units = 8\n      num_proj = 6\n      state_size = num_units + num_proj\n      batch_size = 3\n      input_size = 2\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([batch_size, input_size])\n        m = array_ops.zeros([batch_size, state_size])\n        cell = rnn_cell_impl.LSTMCell(\n            num_units=num_units,\n            num_proj=num_proj,\n            forget_bias=1.0,\n            state_is_tuple=False)\n        output, state = cell(x, m)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run(\n            [output, state], {\n                x: np.array([[1., 1.], [2., 2.], [3., 3.]]),\n                m: 0.1 * np.ones((batch_size, state_size))\n            })\n        self.assertEqual(len(res), 2)\n        # The numbers in results were not calculated, this is mostly just a\n        # smoke test.\n        self.assertEqual(res[0].shape, (batch_size, num_proj))\n        self.assertEqual(res[1].shape, (batch_size, state_size))\n        # Different inputs so different outputs and states\n        for i in range(1, batch_size):\n          self.assertTrue(\n              float(np.linalg.norm((res[0][0, :] - res[0][i, :]))) > 1e-6)\n          self.assertTrue(\n              float(np.linalg.norm((res[1][0, :] - res[1][i, :]))) > 1e-6)\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testLSTMCellVariables(self):\n    with self.cached_session():\n      num_units = 8\n      num_proj = 6\n      state_size = num_units + num_proj\n      batch_size = 3\n      input_size = 2\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([batch_size, input_size])\n        m = array_ops.zeros([batch_size, state_size])\n        cell = rnn_cell_impl.LSTMCell(\n            num_units=num_units,\n            num_proj=num_proj,\n            forget_bias=1.0,\n            state_is_tuple=False)\n        cell(x, m)  # Execute to create variables\n      variables = variables_lib.global_variables()\n      self.assertEqual(variables[0].op.name, \"root/lstm_cell/kernel\")\n      self.assertEqual(variables[1].op.name, \"root/lstm_cell/bias\")\n      self.assertEqual(variables[2].op.name, \"root/lstm_cell/projection/kernel\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWrapperCheckpointing(self):\n    for wrapper_type in [\n        rnn_cell_impl.DropoutWrapper,\n        rnn_cell_impl.ResidualWrapper,\n        lambda cell: rnn_cell_impl.MultiRNNCell([cell])]:\n      cell = rnn_cell_impl.BasicRNNCell(1)\n      wrapper = wrapper_type(cell)\n      wrapper(array_ops.ones([1, 1]),\n              state=wrapper.zero_state(batch_size=1, dtype=dtypes.float32))\n      self.evaluate([v.initializer for v in cell.variables])\n      checkpoint = trackable_utils.Checkpoint(wrapper=wrapper)\n      prefix = os.path.join(self.get_temp_dir(), \"ckpt\")\n      self.evaluate(cell._bias.assign([40.]))\n      save_path = checkpoint.save(prefix)\n      self.evaluate(cell._bias.assign([0.]))\n      checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n      self.assertAllEqual([40.], self.evaluate(cell._bias))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testResidualWrapper(self):\n    wrapper_type = rnn_cell_impl.ResidualWrapper\n    x = ops.convert_to_tensor(np.array([[1., 1., 1.]]))\n    m = ops.convert_to_tensor(np.array([[0.1, 0.1, 0.1]]))\n    base_cell = rnn_cell_impl.GRUCell(\n        3, kernel_initializer=init_ops.constant_initializer(0.5),\n        bias_initializer=init_ops.constant_initializer(0.5))\n    g, m_new = base_cell(x, m)\n    wrapper_object = wrapper_type(base_cell)\n    wrapper_object.get_config()  # Should not throw an error\n\n    self.assertIn(\"cell\", wrapper_object._trackable_children())\n    self.assertIs(wrapper_object._trackable_children()[\"cell\"], base_cell)\n\n    g_res, m_new_res = wrapper_object(x, m)\n    self.evaluate([variables_lib.global_variables_initializer()])\n    res = self.evaluate([g, g_res, m_new, m_new_res])\n    # Residual connections\n    self.assertAllClose(res[1], res[0] + [1., 1., 1.])\n    # States are left untouched\n    self.assertAllClose(res[2], res[3])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testResidualWrapperWithSlice(self):\n    wrapper_type = rnn_cell_impl.ResidualWrapper\n    x = ops.convert_to_tensor(np.array([[1., 1., 1., 1., 1.]]))\n    m = ops.convert_to_tensor(np.array([[0.1, 0.1, 0.1]]))\n    base_cell = rnn_cell_impl.GRUCell(\n        3, kernel_initializer=init_ops.constant_initializer(0.5),\n        bias_initializer=init_ops.constant_initializer(0.5))\n    g, m_new = base_cell(x, m)\n\n    def residual_with_slice_fn(inp, out):\n      inp_sliced = array_ops.slice(inp, [0, 0], [-1, 3])\n      return inp_sliced + out\n\n    g_res, m_new_res = wrapper_type(\n        base_cell, residual_with_slice_fn)(x, m)\n    self.evaluate([variables_lib.global_variables_initializer()])\n    res_g, res_g_res, res_m_new, res_m_new_res = self.evaluate(\n        [g, g_res, m_new, m_new_res])\n    # Residual connections\n    self.assertAllClose(res_g_res, res_g + [1., 1., 1.])\n    # States are left untouched\n    self.assertAllClose(res_m_new, res_m_new_res)\n\n  def testDeviceWrapper(self):\n    wrapper_type = rnn_cell_impl.DeviceWrapper\n    x = array_ops.zeros([1, 3])\n    m = array_ops.zeros([1, 3])\n    cell = rnn_cell_impl.GRUCell(3)\n    wrapped_cell = wrapper_type(cell, \"/cpu:0\")\n    wrapped_cell.get_config()  # Should not throw an error\n    self.assertEqual(wrapped_cell._trackable_children()[\"cell\"], cell)\n\n    outputs, _ = wrapped_cell(x, m)\n    self.assertIn(\"cpu:0\", outputs.device.lower())\n\n  def _retrieve_cpu_gpu_stats(self, run_metadata):\n    cpu_stats = None\n    gpu_stats = None\n    step_stats = run_metadata.step_stats\n    for ds in step_stats.dev_stats:\n      if \"cpu:0\" in ds.device[-5:].lower():\n        cpu_stats = ds.node_stats\n      if \"gpu:0\" == ds.device[-5:].lower():\n        gpu_stats = ds.node_stats\n    return cpu_stats, gpu_stats\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testDeviceWrapperDynamicExecutionNodesAreAllProperlyLocated(self):\n    if not test.is_gpu_available():\n      # Can't perform this test w/o a GPU\n      return\n\n    gpu_dev = test.gpu_device_name()\n    with self.session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 1, 3])\n        cell = rnn_cell_impl.DeviceWrapper(rnn_cell_impl.GRUCell(3), gpu_dev)\n        with ops.device(\"/cpu:0\"):\n          outputs, _ = rnn.dynamic_rnn(\n              cell=cell, inputs=x, dtype=dtypes.float32)\n        run_metadata = config_pb2.RunMetadata()\n        opts = config_pb2.RunOptions(\n            trace_level=config_pb2.RunOptions.FULL_TRACE)\n\n        sess.run([variables_lib.global_variables_initializer()])\n        _ = sess.run(outputs, options=opts, run_metadata=run_metadata)\n\n      cpu_stats, gpu_stats = self._retrieve_cpu_gpu_stats(run_metadata)\n      self.assertFalse([s for s in cpu_stats if \"gru_cell\" in s.node_name])\n      self.assertTrue([s for s in gpu_stats if \"gru_cell\" in s.node_name])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testMultiRNNCell(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m = array_ops.zeros([1, 4])\n        multi_rnn_cell = rnn_cell_impl.MultiRNNCell(\n            [rnn_cell_impl.GRUCell(2) for _ in range(2)],\n            state_is_tuple=False)\n        _, ml = multi_rnn_cell(x, m)\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run(ml, {\n            x: np.array([[1., 1.]]),\n            m: np.array([[0.1, 0.1, 0.1, 0.1]])\n        })\n        # The numbers in results were not calculated, this is just a smoke test.\n        self.assertAllClose(res, [[0.175991, 0.175991, 0.13248, 0.13248]])\n        self.assertEqual(len(multi_rnn_cell.weights), 2 * 4)\n        self.assertTrue(\n            [x.dtype == dtypes.float32 for x in multi_rnn_cell.weights])\n\n  @test_util.run_v1_only(\"b/124229375\")\n  def testMultiRNNCellWithStateTuple(self):\n    with self.cached_session() as sess:\n      with variable_scope.variable_scope(\n          \"root\", initializer=init_ops.constant_initializer(0.5)):\n        x = array_ops.zeros([1, 2])\n        m_bad = array_ops.zeros([1, 4])\n        m_good = (array_ops.zeros([1, 2]), array_ops.zeros([1, 2]))\n\n        # Test incorrectness of state\n        with self.assertRaisesRegex(ValueError, \"Expected state .* a tuple\"):\n          rnn_cell_impl.MultiRNNCell(\n              [rnn_cell_impl.GRUCell(2) for _ in range(2)],\n              state_is_tuple=True)(x, m_bad)\n\n        _, ml = rnn_cell_impl.MultiRNNCell(\n            [rnn_cell_impl.GRUCell(2) for _ in range(2)],\n            state_is_tuple=True)(x, m_good)\n\n        sess.run([variables_lib.global_variables_initializer()])\n        res = sess.run(\n            ml, {\n                x: np.array([[1., 1.]]),\n                m_good[0]: np.array([[0.1, 0.1]]),\n                m_good[1]: np.array([[0.1, 0.1]])\n            })\n\n        # The numbers in results were not calculated, this is just a\n        # smoke test.  However, these numbers should match those of\n        # the test testMultiRNNCell.\n        self.assertAllClose(res[0], [[0.175991, 0.175991]])\n        self.assertAllClose(res[1], [[0.13248, 0.13248]])\n\n  def testDeviceWrapperSerialization(self):\n    wrapper_cls = rnn_cell_impl.DeviceWrapper\n    cell = rnn_cell_impl.LSTMCell(10)\n    wrapper = wrapper_cls(cell, \"/cpu:0\")\n    config = wrapper.get_config()\n\n    # Replace the cell in the config with real cell instance to work around the\n    # reverse keras dependency issue.\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertDictEqual(config, reconstructed_wrapper.get_config())\n    self.assertIsInstance(reconstructed_wrapper, wrapper_cls)\n\n  def testResidualWrapperSerialization(self):\n    wrapper_cls = rnn_cell_impl.ResidualWrapper\n    cell = rnn_cell_impl.LSTMCell(10)\n    wrapper = wrapper_cls(cell)\n    config = wrapper.get_config()\n\n    # Replace the cell in the config with real cell instance to work around the\n    # reverse keras dependency issue.\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertDictEqual(config, reconstructed_wrapper.get_config())\n    self.assertIsInstance(reconstructed_wrapper, wrapper_cls)\n\n    wrapper = wrapper_cls(cell, residual_fn=lambda i, o: i + i + o)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    # Assert the reconstructed function will perform the math correctly.\n    self.assertEqual(reconstructed_wrapper._residual_fn(1, 2), 4)\n\n    def residual_fn(inputs, outputs):\n      return inputs * 3 + outputs\n\n    wrapper = wrapper_cls(cell, residual_fn=residual_fn)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    # Assert the reconstructed function will perform the math correctly.\n    self.assertEqual(reconstructed_wrapper._residual_fn(1, 2), 5)\n\n  def testDropoutWrapperSerialization(self):\n    wrapper_cls = rnn_cell_impl.DropoutWrapper\n    cell = rnn_cell_impl.LSTMCell(10)\n    wrapper = wrapper_cls(cell)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertDictEqual(config, reconstructed_wrapper.get_config())\n    self.assertIsInstance(reconstructed_wrapper, wrapper_cls)\n\n    wrapper = wrapper_cls(cell, dropout_state_filter_visitor=lambda s: True)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertTrue(reconstructed_wrapper._dropout_state_filter(None))\n\n    def dropout_state_filter_visitor(unused_state):\n      return False\n\n    wrapper = wrapper_cls(\n        cell, dropout_state_filter_visitor=dropout_state_filter_visitor)\n    config = wrapper.get_config()\n\n    config_copy = config.copy()\n    config_copy[\"cell\"] = rnn_cell_impl.LSTMCell.from_config(\n        config_copy[\"cell\"][\"config\"])\n    reconstructed_wrapper = wrapper_cls.from_config(config_copy)\n    self.assertFalse(reconstructed_wrapper._dropout_state_filter(None))\n\n  def testSavedModel(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/175887901\")\n\n    with self.cached_session():\n      root = tracking.AutoTrackable()\n      root.cell = rnn_cell_impl.LSTMCell(8)\n      @def_function.function(input_signature=[tensor_spec.TensorSpec([3, 8])])\n      def call(x):\n        state = root.cell.zero_state(3, dtype=x.dtype)\n        y, _ = root.cell(x, state)\n        return y\n      root.call = call\n      expected = root.call(array_ops.zeros((3, 8)))\n      self.evaluate(variables_lib.global_variables_initializer())\n\n      save_dir = os.path.join(self.get_temp_dir(), \"saved_model\")\n      save.save(root, save_dir)\n      loaded = load.load(save_dir)\n      self.evaluate(variables_lib.global_variables_initializer())\n      self.assertAllClose(\n          expected, loaded.call(array_ops.zeros((3, 8))))\n\n\n@test_util.run_all_in_graph_and_eager_modes\n@test_util.run_all_without_tensor_float_32(\n    \"Uses an LSTMCell, which calls matmul\")\nclass DropoutWrapperTest(test.TestCase, parameterized.TestCase):\n\n  def _testDropoutWrapper(self,\n                          batch_size=None,\n                          time_steps=None,\n                          parallel_iterations=None,\n                          wrapper_type=None,\n                          scope=\"root\",\n                          **kwargs):\n    if batch_size is None and time_steps is None:\n      # 2 time steps, batch size 1, depth 3\n      batch_size = 1\n      time_steps = 2\n      x = constant_op.constant(\n          [[[2., 2., 2.]], [[1., 1., 1.]]], dtype=dtypes.float32)\n      m = rnn_cell_impl.LSTMStateTuple(\n          *[constant_op.constant([[0.1, 0.1, 0.1]], dtype=dtypes.float32)] * 2)\n    else:\n      x = constant_op.constant(\n          np.random.randn(time_steps, batch_size, 3).astype(np.float32))\n      m = rnn_cell_impl.LSTMStateTuple(*[\n          constant_op.\n          constant([[0.1, 0.1, 0.1]] * batch_size, dtype=dtypes.float32)] * 2)\n    outputs, final_state = rnn.dynamic_rnn(\n        cell=wrapper_type(\n            rnn_cell_impl.LSTMCell(\n                3, initializer=init_ops.constant_initializer(0.5)),\n            dtype=x.dtype, **kwargs),\n        time_major=True,\n        parallel_iterations=parallel_iterations,\n        inputs=x,\n        initial_state=m,\n        scope=scope)\n    self.evaluate([variables_lib.global_variables_initializer()])\n    res = self.evaluate([outputs, final_state])\n    self.assertEqual(res[0].shape, (time_steps, batch_size, 3))\n    self.assertEqual(res[1].c.shape, (batch_size, 3))\n    self.assertEqual(res[1].h.shape, (batch_size, 3))\n    return res\n\n  def testDropoutWrapperProperties(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    cell = rnn_cell_impl.BasicRNNCell(10)\n    wrapper = wrapper_type(cell)\n    # Github issue 15810\n    self.assertEqual(wrapper.wrapped_cell, cell)\n    self.assertEqual(wrapper.state_size, 10)\n    self.assertEqual(wrapper.output_size, 10)\n\n  def testDropoutWrapperZeroState(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n\n    class _Cell(rnn_cell_impl.BasicRNNCell):\n\n      def zero_state(self, batch_size=None, dtype=None):\n        return \"wrapped_cell_zero_state\"\n    wrapper = wrapper_type(_Cell(10))\n    self.assertEqual(wrapper.zero_state(10, dtypes.float32),\n                     \"wrapped_cell_zero_state\")\n\n  def testDropoutWrapperKeepAllConstantInput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep = array_ops.ones([])\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep, output_keep_prob=keep, state_keep_prob=keep,\n        wrapper_type=wrapper_type)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    true_full_final_c = np.array(\n        [[1.949385, 1.949385, 1.949385]], dtype=np.float32)\n    self.assertAllClose(true_full_output, res[0])\n    self.assertAllClose(true_full_output[1], res[1].h)\n    self.assertAllClose(true_full_final_c, res[1].c)\n\n  def testDropoutWrapperKeepAll(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep = variable_scope.get_variable(\"all\", initializer=1.0)\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep, output_keep_prob=keep, state_keep_prob=keep,\n        wrapper_type=wrapper_type)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    true_full_final_c = np.array(\n        [[1.949385, 1.949385, 1.949385]], dtype=np.float32)\n    self.assertAllClose(true_full_output, res[0])\n    self.assertAllClose(true_full_output[1], res[1].h)\n    self.assertAllClose(true_full_final_c, res[1].c)\n\n  def testDropoutWrapperWithSeed(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_some = 0.5\n    random_seed.set_random_seed(2)\n    ## Use parallel_iterations = 1 in both calls to\n    ## _testDropoutWrapper to ensure the (per-time step) dropout is\n    ## consistent across both calls.  Otherwise the seed may not end\n    ## up being munged consistently across both graphs.\n    res_standard_1 = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        seed=10,\n        parallel_iterations=1,\n        wrapper_type=wrapper_type,\n        scope=\"root_1\")\n    random_seed.set_random_seed(2)\n    res_standard_2 = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        seed=10,\n        parallel_iterations=1,\n        wrapper_type=wrapper_type,\n        scope=\"root_2\")\n    self.assertAllClose(res_standard_1[0], res_standard_2[0])\n    self.assertAllClose(res_standard_1[1].c, res_standard_2[1].c)\n    self.assertAllClose(res_standard_1[1].h, res_standard_2[1].h)\n\n  def testDropoutWrapperKeepNoOutput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_all = variable_scope.get_variable(\"all\", initializer=1.0)\n    keep_none = variable_scope.get_variable(\"none\", initializer=1e-6)\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_all,\n        output_keep_prob=keep_none,\n        state_keep_prob=keep_all,\n        wrapper_type=wrapper_type)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    true_full_final_c = np.array(\n        [[1.949385, 1.949385, 1.949385]], dtype=np.float32)\n    self.assertAllClose(np.zeros(res[0].shape), res[0])\n    self.assertAllClose(true_full_output[1], res[1].h)\n    self.assertAllClose(true_full_final_c, res[1].c)\n\n  def testDropoutWrapperKeepNoStateExceptLSTMCellMemory(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_all = variable_scope.get_variable(\"all\", initializer=1.0)\n    keep_none = variable_scope.get_variable(\"none\", initializer=1e-6)\n    # Even though we dropout state, by default DropoutWrapper never\n    # drops out the memory (\"c\") term of an LSTMStateTuple.\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_all,\n        output_keep_prob=keep_all,\n        state_keep_prob=keep_none,\n        wrapper_type=wrapper_type)\n    true_c_state = np.array([[1.713925, 1.713925, 1.713925]], dtype=np.float32)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    self.assertAllClose(true_full_output[0], res[0][0])\n    # Second output is modified by zero input state\n    self.assertGreater(np.linalg.norm(true_full_output[1] - res[0][1]), 1e-4)\n    # h state has been set to zero\n    self.assertAllClose(np.zeros(res[1].h.shape), res[1].h)\n    # c state of an LSTMStateTuple is NEVER modified.\n    self.assertAllClose(true_c_state, res[1].c)\n\n  def testDropoutWrapperKeepNoInput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_all = variable_scope.get_variable(\"all\", initializer=1.0)\n    keep_none = variable_scope.get_variable(\"none\", initializer=1e-6)\n    true_full_output = np.array(\n        [[[0.751109, 0.751109, 0.751109]], [[0.895509, 0.895509, 0.895509]]],\n        dtype=np.float32)\n    true_full_final_c = np.array(\n        [[1.949385, 1.949385, 1.949385]], dtype=np.float32)\n    # All outputs are different because inputs are zeroed out\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_none,\n        output_keep_prob=keep_all,\n        state_keep_prob=keep_all,\n        wrapper_type=wrapper_type)\n    self.assertGreater(np.linalg.norm(res[0] - true_full_output), 1e-4)\n    self.assertGreater(np.linalg.norm(res[1].h - true_full_output[1]), 1e-4)\n    self.assertGreater(np.linalg.norm(res[1].c - true_full_final_c), 1e-4)\n\n  def testDropoutWrapperRecurrentOutput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_some = 0.8\n    keep_all = variable_scope.get_variable(\"all\", initializer=1.0)\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_all,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_all,\n        variational_recurrent=True,\n        wrapper_type=wrapper_type,\n        input_size=3,\n        batch_size=5,\n        time_steps=7)\n    # Ensure the same dropout pattern for all time steps\n    output_mask = np.abs(res[0]) > 1e-6\n    for m in output_mask[1:]:\n      self.assertAllClose(output_mask[0], m)\n\n  def testDropoutWrapperRecurrentStateInputAndOutput(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_some = 0.9\n    res = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        variational_recurrent=True,\n        wrapper_type=wrapper_type,\n        input_size=3,\n        batch_size=5,\n        time_steps=7)\n\n    # Smoke test for the state/input masks.\n    output_mask = np.abs(res[0]) > 1e-6\n    for time_step in output_mask:\n      # Ensure the same dropout output pattern for all time steps\n      self.assertAllClose(output_mask[0], time_step)\n      for batch_entry in time_step:\n        # Assert all batch entries get the same mask\n        self.assertAllClose(batch_entry, time_step[0])\n\n    # For state, ensure all batch entries have the same mask\n    state_c_mask = np.abs(res[1].c) > 1e-6\n    state_h_mask = np.abs(res[1].h) > 1e-6\n    for batch_entry in state_c_mask:\n      self.assertAllClose(batch_entry, state_c_mask[0])\n    for batch_entry in state_h_mask:\n      self.assertAllClose(batch_entry, state_h_mask[0])\n\n  def testDropoutWrapperRecurrentStateInputAndOutputWithSeed(self):\n    wrapper_type = rnn_cell_impl.DropoutWrapper\n    keep_some = 0.9\n    random_seed.set_random_seed(2347)\n    np.random.seed(23487)\n    res0 = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        variational_recurrent=True,\n        wrapper_type=wrapper_type,\n        input_size=3,\n        batch_size=5,\n        time_steps=7,\n        seed=-234987,\n        scope=\"root_0\")\n    random_seed.set_random_seed(2347)\n    np.random.seed(23487)\n    res1 = self._testDropoutWrapper(\n        input_keep_prob=keep_some,\n        output_keep_prob=keep_some,\n        state_keep_prob=keep_some,\n        variational_recurrent=True,\n        wrapper_type=wrapper_type,\n        input_size=3,\n        batch_size=5,\n        time_steps=7,\n        seed=-234987,\n        scope=\"root_1\")\n\n    output_mask = np.abs(res0[0]) > 1e-6\n    for time_step in output_mask:\n      # Ensure the same dropout output pattern for all time steps\n      self.assertAllClose(output_mask[0], time_step)\n      for batch_entry in time_step:\n        # Assert all batch entries get the same mask\n        self.assertAllClose(batch_entry, time_step[0])\n\n    # For state, ensure all batch entries have the same mask\n    state_c_mask = np.abs(res0[1].c) > 1e-6\n    state_h_mask = np.abs(res0[1].h) > 1e-6\n    for batch_entry in state_c_mask:\n      self.assertAllClose(batch_entry, state_c_mask[0])\n    for batch_entry in state_h_mask:\n      self.assertAllClose(batch_entry, state_h_mask[0])\n\n    # Ensure seeded calculation is identical.\n    self.assertAllClose(res0[0], res1[0])\n    self.assertAllClose(res0[1].c, res1[1].c)\n    self.assertAllClose(res0[1].h, res1[1].h)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/rnn/lstm_ops.cc", "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"], "buggy_code_start_loc": [418, 35], "buggy_code_end_loc": [418, 1325], "fixing_code_start_loc": [419, 36], "fixing_code_end_loc": [478, 1357], "type": "CWE-20", "message": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.LSTMBlockCell` does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. The code does not validate the ranks of any of the arguments to this API call. This results in `CHECK`-failures when the elements of the tensor are accessed. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "other": {"cve": {"id": "CVE-2022-29200", "sourceIdentifier": "security-advisories@github.com", "published": "2022-05-20T22:16:40.933", "lastModified": "2022-06-02T17:54:38.687", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.LSTMBlockCell` does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. The code does not validate the ranks of any of the arguments to this API call. This results in `CHECK`-failures when the elements of the tensor are accessed. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En versiones anteriores a 2.9.0, 2.8.1, 2.7.2 y 2.6.4, la implementaci\u00f3n de \"tf.raw_ops.LSTMBlockCell\" no comprueba completamente los argumentos de entrada. Esto resulta en un fallo de \"CHECK\" que puede ser usado para desencadenar un ataque de denegaci\u00f3n de servicio. El c\u00f3digo no comprueba los rangos de ninguno de los argumentos de esta llamada a la API. Esto resulta en fallos de \"CHECK\" cuando son accedidos a los elementos del tensor. Las versiones 2.9.0, 2.8.1, 2.7.2 y 2.6.4 contienen un parche para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.4", "matchCriteriaId": "D9359D32-D090-44CF-AC43-2046084A28BB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:-:*:*:*:*:*:*", "matchCriteriaId": "E9EA1898-ACAA-4699-8BAE-54D62C1819FB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "130DE3C9-6842-456F-A259-BF8FF8457217"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "BBF2FCEF-989C-409D-9F4C-81418C65B972"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "9CFB1CFC-579D-4647-A472-6DE8BE1951DE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "F3F3F37E-D27F-4060-830C-0AFF16150777"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/rnn/lstm_ops.cc", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/803404044ae7a1efac48ba82d74111fce1ddb09a", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.6.4", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.7.2", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.8.1", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.9.0", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-2vv3-56qg-g2cf", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/803404044ae7a1efac48ba82d74111fce1ddb09a"}}