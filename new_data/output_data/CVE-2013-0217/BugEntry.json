{"buggy_code": ["/*\n * Back-end of the driver for virtual network devices. This portion of the\n * driver exports a 'unified' network-device interface that can be accessed\n * by any operating system that implements a compatible front end. A\n * reference front-end implementation can be found in:\n *  drivers/net/xen-netfront.c\n *\n * Copyright (c) 2002-2005, K A Fraser\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License version 2\n * as published by the Free Software Foundation; or, when distributed\n * separately from the Linux kernel or incorporated into other\n * software packages, subject to the following license:\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this source file (the \"Software\"), to deal in the Software without\n * restriction, including without limitation the rights to use, copy, modify,\n * merge, publish, distribute, sublicense, and/or sell copies of the Software,\n * and to permit persons to whom the Software is furnished to do so, subject to\n * the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n */\n\n#include \"common.h\"\n\n#include <linux/kthread.h>\n#include <linux/if_vlan.h>\n#include <linux/udp.h>\n\n#include <net/tcp.h>\n\n#include <xen/xen.h>\n#include <xen/events.h>\n#include <xen/interface/memory.h>\n\n#include <asm/xen/hypercall.h>\n#include <asm/xen/page.h>\n\nstruct pending_tx_info {\n\tstruct xen_netif_tx_request req;\n\tstruct xenvif *vif;\n};\ntypedef unsigned int pending_ring_idx_t;\n\nstruct netbk_rx_meta {\n\tint id;\n\tint size;\n\tint gso_size;\n};\n\n#define MAX_PENDING_REQS 256\n\n/* Discriminate from any valid pending_idx value. */\n#define INVALID_PENDING_IDX 0xFFFF\n\n#define MAX_BUFFER_OFFSET PAGE_SIZE\n\n/* extra field used in struct page */\nunion page_ext {\n\tstruct {\n#if BITS_PER_LONG < 64\n#define IDX_WIDTH   8\n#define GROUP_WIDTH (BITS_PER_LONG - IDX_WIDTH)\n\t\tunsigned int group:GROUP_WIDTH;\n\t\tunsigned int idx:IDX_WIDTH;\n#else\n\t\tunsigned int group, idx;\n#endif\n\t} e;\n\tvoid *mapping;\n};\n\nstruct xen_netbk {\n\twait_queue_head_t wq;\n\tstruct task_struct *task;\n\n\tstruct sk_buff_head rx_queue;\n\tstruct sk_buff_head tx_queue;\n\n\tstruct timer_list net_timer;\n\n\tstruct page *mmap_pages[MAX_PENDING_REQS];\n\n\tpending_ring_idx_t pending_prod;\n\tpending_ring_idx_t pending_cons;\n\tstruct list_head net_schedule_list;\n\n\t/* Protect the net_schedule_list in netif. */\n\tspinlock_t net_schedule_list_lock;\n\n\tatomic_t netfront_count;\n\n\tstruct pending_tx_info pending_tx_info[MAX_PENDING_REQS];\n\tstruct gnttab_copy tx_copy_ops[MAX_PENDING_REQS];\n\n\tu16 pending_ring[MAX_PENDING_REQS];\n\n\t/*\n\t * Given MAX_BUFFER_OFFSET of 4096 the worst case is that each\n\t * head/fragment page uses 2 copy operations because it\n\t * straddles two buffers in the frontend.\n\t */\n\tstruct gnttab_copy grant_copy_op[2*XEN_NETIF_RX_RING_SIZE];\n\tstruct netbk_rx_meta meta[2*XEN_NETIF_RX_RING_SIZE];\n};\n\nstatic struct xen_netbk *xen_netbk;\nstatic int xen_netbk_group_nr;\n\nvoid xen_netbk_add_xenvif(struct xenvif *vif)\n{\n\tint i;\n\tint min_netfront_count;\n\tint min_group = 0;\n\tstruct xen_netbk *netbk;\n\n\tmin_netfront_count = atomic_read(&xen_netbk[0].netfront_count);\n\tfor (i = 0; i < xen_netbk_group_nr; i++) {\n\t\tint netfront_count = atomic_read(&xen_netbk[i].netfront_count);\n\t\tif (netfront_count < min_netfront_count) {\n\t\t\tmin_group = i;\n\t\t\tmin_netfront_count = netfront_count;\n\t\t}\n\t}\n\n\tnetbk = &xen_netbk[min_group];\n\n\tvif->netbk = netbk;\n\tatomic_inc(&netbk->netfront_count);\n}\n\nvoid xen_netbk_remove_xenvif(struct xenvif *vif)\n{\n\tstruct xen_netbk *netbk = vif->netbk;\n\tvif->netbk = NULL;\n\tatomic_dec(&netbk->netfront_count);\n}\n\nstatic void xen_netbk_idx_release(struct xen_netbk *netbk, u16 pending_idx);\nstatic void make_tx_response(struct xenvif *vif,\n\t\t\t     struct xen_netif_tx_request *txp,\n\t\t\t     s8       st);\nstatic struct xen_netif_rx_response *make_rx_response(struct xenvif *vif,\n\t\t\t\t\t     u16      id,\n\t\t\t\t\t     s8       st,\n\t\t\t\t\t     u16      offset,\n\t\t\t\t\t     u16      size,\n\t\t\t\t\t     u16      flags);\n\nstatic inline unsigned long idx_to_pfn(struct xen_netbk *netbk,\n\t\t\t\t       u16 idx)\n{\n\treturn page_to_pfn(netbk->mmap_pages[idx]);\n}\n\nstatic inline unsigned long idx_to_kaddr(struct xen_netbk *netbk,\n\t\t\t\t\t u16 idx)\n{\n\treturn (unsigned long)pfn_to_kaddr(idx_to_pfn(netbk, idx));\n}\n\n/* extra field used in struct page */\nstatic inline void set_page_ext(struct page *pg, struct xen_netbk *netbk,\n\t\t\t\tunsigned int idx)\n{\n\tunsigned int group = netbk - xen_netbk;\n\tunion page_ext ext = { .e = { .group = group + 1, .idx = idx } };\n\n\tBUILD_BUG_ON(sizeof(ext) > sizeof(ext.mapping));\n\tpg->mapping = ext.mapping;\n}\n\nstatic int get_page_ext(struct page *pg,\n\t\t\tunsigned int *pgroup, unsigned int *pidx)\n{\n\tunion page_ext ext = { .mapping = pg->mapping };\n\tstruct xen_netbk *netbk;\n\tunsigned int group, idx;\n\n\tgroup = ext.e.group - 1;\n\n\tif (group < 0 || group >= xen_netbk_group_nr)\n\t\treturn 0;\n\n\tnetbk = &xen_netbk[group];\n\n\tidx = ext.e.idx;\n\n\tif ((idx < 0) || (idx >= MAX_PENDING_REQS))\n\t\treturn 0;\n\n\tif (netbk->mmap_pages[idx] != pg)\n\t\treturn 0;\n\n\t*pgroup = group;\n\t*pidx = idx;\n\n\treturn 1;\n}\n\n/*\n * This is the amount of packet we copy rather than map, so that the\n * guest can't fiddle with the contents of the headers while we do\n * packet processing on them (netfilter, routing, etc).\n */\n#define PKT_PROT_LEN    (ETH_HLEN + \\\n\t\t\t VLAN_HLEN + \\\n\t\t\t sizeof(struct iphdr) + MAX_IPOPTLEN + \\\n\t\t\t sizeof(struct tcphdr) + MAX_TCP_OPTION_SPACE)\n\nstatic u16 frag_get_pending_idx(skb_frag_t *frag)\n{\n\treturn (u16)frag->page_offset;\n}\n\nstatic void frag_set_pending_idx(skb_frag_t *frag, u16 pending_idx)\n{\n\tfrag->page_offset = pending_idx;\n}\n\nstatic inline pending_ring_idx_t pending_index(unsigned i)\n{\n\treturn i & (MAX_PENDING_REQS-1);\n}\n\nstatic inline pending_ring_idx_t nr_pending_reqs(struct xen_netbk *netbk)\n{\n\treturn MAX_PENDING_REQS -\n\t\tnetbk->pending_prod + netbk->pending_cons;\n}\n\nstatic void xen_netbk_kick_thread(struct xen_netbk *netbk)\n{\n\twake_up(&netbk->wq);\n}\n\nstatic int max_required_rx_slots(struct xenvif *vif)\n{\n\tint max = DIV_ROUND_UP(vif->dev->mtu, PAGE_SIZE);\n\n\tif (vif->can_sg || vif->gso || vif->gso_prefix)\n\t\tmax += MAX_SKB_FRAGS + 1; /* extra_info + frags */\n\n\treturn max;\n}\n\nint xen_netbk_rx_ring_full(struct xenvif *vif)\n{\n\tRING_IDX peek   = vif->rx_req_cons_peek;\n\tRING_IDX needed = max_required_rx_slots(vif);\n\n\treturn ((vif->rx.sring->req_prod - peek) < needed) ||\n\t       ((vif->rx.rsp_prod_pvt + XEN_NETIF_RX_RING_SIZE - peek) < needed);\n}\n\nint xen_netbk_must_stop_queue(struct xenvif *vif)\n{\n\tif (!xen_netbk_rx_ring_full(vif))\n\t\treturn 0;\n\n\tvif->rx.sring->req_event = vif->rx_req_cons_peek +\n\t\tmax_required_rx_slots(vif);\n\tmb(); /* request notification /then/ check the queue */\n\n\treturn xen_netbk_rx_ring_full(vif);\n}\n\n/*\n * Returns true if we should start a new receive buffer instead of\n * adding 'size' bytes to a buffer which currently contains 'offset'\n * bytes.\n */\nstatic bool start_new_rx_buffer(int offset, unsigned long size, int head)\n{\n\t/* simple case: we have completely filled the current buffer. */\n\tif (offset == MAX_BUFFER_OFFSET)\n\t\treturn true;\n\n\t/*\n\t * complex case: start a fresh buffer if the current frag\n\t * would overflow the current buffer but only if:\n\t *     (i)   this frag would fit completely in the next buffer\n\t * and (ii)  there is already some data in the current buffer\n\t * and (iii) this is not the head buffer.\n\t *\n\t * Where:\n\t * - (i) stops us splitting a frag into two copies\n\t *   unless the frag is too large for a single buffer.\n\t * - (ii) stops us from leaving a buffer pointlessly empty.\n\t * - (iii) stops us leaving the first buffer\n\t *   empty. Strictly speaking this is already covered\n\t *   by (ii) but is explicitly checked because\n\t *   netfront relies on the first buffer being\n\t *   non-empty and can crash otherwise.\n\t *\n\t * This means we will effectively linearise small\n\t * frags but do not needlessly split large buffers\n\t * into multiple copies tend to give large frags their\n\t * own buffers as before.\n\t */\n\tif ((offset + size > MAX_BUFFER_OFFSET) &&\n\t    (size <= MAX_BUFFER_OFFSET) && offset && !head)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Figure out how many ring slots we're going to need to send @skb to\n * the guest. This function is essentially a dry run of\n * netbk_gop_frag_copy.\n */\nunsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb)\n{\n\tunsigned int count;\n\tint i, copy_off;\n\n\tcount = DIV_ROUND_UP(skb_headlen(skb), PAGE_SIZE);\n\n\tcopy_off = skb_headlen(skb) % PAGE_SIZE;\n\n\tif (skb_shinfo(skb)->gso_size)\n\t\tcount++;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tunsigned long size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tunsigned long offset = skb_shinfo(skb)->frags[i].page_offset;\n\t\tunsigned long bytes;\n\n\t\toffset &= ~PAGE_MASK;\n\n\t\twhile (size > 0) {\n\t\t\tBUG_ON(offset >= PAGE_SIZE);\n\t\t\tBUG_ON(copy_off > MAX_BUFFER_OFFSET);\n\n\t\t\tbytes = PAGE_SIZE - offset;\n\n\t\t\tif (bytes > size)\n\t\t\t\tbytes = size;\n\n\t\t\tif (start_new_rx_buffer(copy_off, bytes, 0)) {\n\t\t\t\tcount++;\n\t\t\t\tcopy_off = 0;\n\t\t\t}\n\n\t\t\tif (copy_off + bytes > MAX_BUFFER_OFFSET)\n\t\t\t\tbytes = MAX_BUFFER_OFFSET - copy_off;\n\n\t\t\tcopy_off += bytes;\n\n\t\t\toffset += bytes;\n\t\t\tsize -= bytes;\n\n\t\t\tif (offset == PAGE_SIZE)\n\t\t\t\toffset = 0;\n\t\t}\n\t}\n\treturn count;\n}\n\nstruct netrx_pending_operations {\n\tunsigned copy_prod, copy_cons;\n\tunsigned meta_prod, meta_cons;\n\tstruct gnttab_copy *copy;\n\tstruct netbk_rx_meta *meta;\n\tint copy_off;\n\tgrant_ref_t copy_gref;\n};\n\nstatic struct netbk_rx_meta *get_next_rx_buffer(struct xenvif *vif,\n\t\t\t\t\t\tstruct netrx_pending_operations *npo)\n{\n\tstruct netbk_rx_meta *meta;\n\tstruct xen_netif_rx_request *req;\n\n\treq = RING_GET_REQUEST(&vif->rx, vif->rx.req_cons++);\n\n\tmeta = npo->meta + npo->meta_prod++;\n\tmeta->gso_size = 0;\n\tmeta->size = 0;\n\tmeta->id = req->id;\n\n\tnpo->copy_off = 0;\n\tnpo->copy_gref = req->gref;\n\n\treturn meta;\n}\n\n/*\n * Set up the grant operations for this fragment. If it's a flipping\n * interface, we also set up the unmap request from here.\n */\nstatic void netbk_gop_frag_copy(struct xenvif *vif, struct sk_buff *skb,\n\t\t\t\tstruct netrx_pending_operations *npo,\n\t\t\t\tstruct page *page, unsigned long size,\n\t\t\t\tunsigned long offset, int *head)\n{\n\tstruct gnttab_copy *copy_gop;\n\tstruct netbk_rx_meta *meta;\n\t/*\n\t * These variables are used iff get_page_ext returns true,\n\t * in which case they are guaranteed to be initialized.\n\t */\n\tunsigned int uninitialized_var(group), uninitialized_var(idx);\n\tint foreign = get_page_ext(page, &group, &idx);\n\tunsigned long bytes;\n\n\t/* Data must not cross a page boundary. */\n\tBUG_ON(size + offset > PAGE_SIZE<<compound_order(page));\n\n\tmeta = npo->meta + npo->meta_prod - 1;\n\n\t/* Skip unused frames from start of page */\n\tpage += offset >> PAGE_SHIFT;\n\toffset &= ~PAGE_MASK;\n\n\twhile (size > 0) {\n\t\tBUG_ON(offset >= PAGE_SIZE);\n\t\tBUG_ON(npo->copy_off > MAX_BUFFER_OFFSET);\n\n\t\tbytes = PAGE_SIZE - offset;\n\n\t\tif (bytes > size)\n\t\t\tbytes = size;\n\n\t\tif (start_new_rx_buffer(npo->copy_off, bytes, *head)) {\n\t\t\t/*\n\t\t\t * Netfront requires there to be some data in the head\n\t\t\t * buffer.\n\t\t\t */\n\t\t\tBUG_ON(*head);\n\n\t\t\tmeta = get_next_rx_buffer(vif, npo);\n\t\t}\n\n\t\tif (npo->copy_off + bytes > MAX_BUFFER_OFFSET)\n\t\t\tbytes = MAX_BUFFER_OFFSET - npo->copy_off;\n\n\t\tcopy_gop = npo->copy + npo->copy_prod++;\n\t\tcopy_gop->flags = GNTCOPY_dest_gref;\n\t\tif (foreign) {\n\t\t\tstruct xen_netbk *netbk = &xen_netbk[group];\n\t\t\tstruct pending_tx_info *src_pend;\n\n\t\t\tsrc_pend = &netbk->pending_tx_info[idx];\n\n\t\t\tcopy_gop->source.domid = src_pend->vif->domid;\n\t\t\tcopy_gop->source.u.ref = src_pend->req.gref;\n\t\t\tcopy_gop->flags |= GNTCOPY_source_gref;\n\t\t} else {\n\t\t\tvoid *vaddr = page_address(page);\n\t\t\tcopy_gop->source.domid = DOMID_SELF;\n\t\t\tcopy_gop->source.u.gmfn = virt_to_mfn(vaddr);\n\t\t}\n\t\tcopy_gop->source.offset = offset;\n\t\tcopy_gop->dest.domid = vif->domid;\n\n\t\tcopy_gop->dest.offset = npo->copy_off;\n\t\tcopy_gop->dest.u.ref = npo->copy_gref;\n\t\tcopy_gop->len = bytes;\n\n\t\tnpo->copy_off += bytes;\n\t\tmeta->size += bytes;\n\n\t\toffset += bytes;\n\t\tsize -= bytes;\n\n\t\t/* Next frame */\n\t\tif (offset == PAGE_SIZE && size) {\n\t\t\tBUG_ON(!PageCompound(page));\n\t\t\tpage++;\n\t\t\toffset = 0;\n\t\t}\n\n\t\t/* Leave a gap for the GSO descriptor. */\n\t\tif (*head && skb_shinfo(skb)->gso_size && !vif->gso_prefix)\n\t\t\tvif->rx.req_cons++;\n\n\t\t*head = 0; /* There must be something in this buffer now. */\n\n\t}\n}\n\n/*\n * Prepare an SKB to be transmitted to the frontend.\n *\n * This function is responsible for allocating grant operations, meta\n * structures, etc.\n *\n * It returns the number of meta structures consumed. The number of\n * ring slots used is always equal to the number of meta slots used\n * plus the number of GSO descriptors used. Currently, we use either\n * zero GSO descriptors (for non-GSO packets) or one descriptor (for\n * frontend-side LRO).\n */\nstatic int netbk_gop_skb(struct sk_buff *skb,\n\t\t\t struct netrx_pending_operations *npo)\n{\n\tstruct xenvif *vif = netdev_priv(skb->dev);\n\tint nr_frags = skb_shinfo(skb)->nr_frags;\n\tint i;\n\tstruct xen_netif_rx_request *req;\n\tstruct netbk_rx_meta *meta;\n\tunsigned char *data;\n\tint head = 1;\n\tint old_meta_prod;\n\n\told_meta_prod = npo->meta_prod;\n\n\t/* Set up a GSO prefix descriptor, if necessary */\n\tif (skb_shinfo(skb)->gso_size && vif->gso_prefix) {\n\t\treq = RING_GET_REQUEST(&vif->rx, vif->rx.req_cons++);\n\t\tmeta = npo->meta + npo->meta_prod++;\n\t\tmeta->gso_size = skb_shinfo(skb)->gso_size;\n\t\tmeta->size = 0;\n\t\tmeta->id = req->id;\n\t}\n\n\treq = RING_GET_REQUEST(&vif->rx, vif->rx.req_cons++);\n\tmeta = npo->meta + npo->meta_prod++;\n\n\tif (!vif->gso_prefix)\n\t\tmeta->gso_size = skb_shinfo(skb)->gso_size;\n\telse\n\t\tmeta->gso_size = 0;\n\n\tmeta->size = 0;\n\tmeta->id = req->id;\n\tnpo->copy_off = 0;\n\tnpo->copy_gref = req->gref;\n\n\tdata = skb->data;\n\twhile (data < skb_tail_pointer(skb)) {\n\t\tunsigned int offset = offset_in_page(data);\n\t\tunsigned int len = PAGE_SIZE - offset;\n\n\t\tif (data + len > skb_tail_pointer(skb))\n\t\t\tlen = skb_tail_pointer(skb) - data;\n\n\t\tnetbk_gop_frag_copy(vif, skb, npo,\n\t\t\t\t    virt_to_page(data), len, offset, &head);\n\t\tdata += len;\n\t}\n\n\tfor (i = 0; i < nr_frags; i++) {\n\t\tnetbk_gop_frag_copy(vif, skb, npo,\n\t\t\t\t    skb_frag_page(&skb_shinfo(skb)->frags[i]),\n\t\t\t\t    skb_frag_size(&skb_shinfo(skb)->frags[i]),\n\t\t\t\t    skb_shinfo(skb)->frags[i].page_offset,\n\t\t\t\t    &head);\n\t}\n\n\treturn npo->meta_prod - old_meta_prod;\n}\n\n/*\n * This is a twin to netbk_gop_skb.  Assume that netbk_gop_skb was\n * used to set up the operations on the top of\n * netrx_pending_operations, which have since been done.  Check that\n * they didn't give any errors and advance over them.\n */\nstatic int netbk_check_gop(struct xenvif *vif, int nr_meta_slots,\n\t\t\t   struct netrx_pending_operations *npo)\n{\n\tstruct gnttab_copy     *copy_op;\n\tint status = XEN_NETIF_RSP_OKAY;\n\tint i;\n\n\tfor (i = 0; i < nr_meta_slots; i++) {\n\t\tcopy_op = npo->copy + npo->copy_cons++;\n\t\tif (copy_op->status != GNTST_okay) {\n\t\t\tnetdev_dbg(vif->dev,\n\t\t\t\t   \"Bad status %d from copy to DOM%d.\\n\",\n\t\t\t\t   copy_op->status, vif->domid);\n\t\t\tstatus = XEN_NETIF_RSP_ERROR;\n\t\t}\n\t}\n\n\treturn status;\n}\n\nstatic void netbk_add_frag_responses(struct xenvif *vif, int status,\n\t\t\t\t     struct netbk_rx_meta *meta,\n\t\t\t\t     int nr_meta_slots)\n{\n\tint i;\n\tunsigned long offset;\n\n\t/* No fragments used */\n\tif (nr_meta_slots <= 1)\n\t\treturn;\n\n\tnr_meta_slots--;\n\n\tfor (i = 0; i < nr_meta_slots; i++) {\n\t\tint flags;\n\t\tif (i == nr_meta_slots - 1)\n\t\t\tflags = 0;\n\t\telse\n\t\t\tflags = XEN_NETRXF_more_data;\n\n\t\toffset = 0;\n\t\tmake_rx_response(vif, meta[i].id, status, offset,\n\t\t\t\t meta[i].size, flags);\n\t}\n}\n\nstruct skb_cb_overlay {\n\tint meta_slots_used;\n};\n\nstatic void xen_netbk_rx_action(struct xen_netbk *netbk)\n{\n\tstruct xenvif *vif = NULL, *tmp;\n\ts8 status;\n\tu16 irq, flags;\n\tstruct xen_netif_rx_response *resp;\n\tstruct sk_buff_head rxq;\n\tstruct sk_buff *skb;\n\tLIST_HEAD(notify);\n\tint ret;\n\tint nr_frags;\n\tint count;\n\tunsigned long offset;\n\tstruct skb_cb_overlay *sco;\n\n\tstruct netrx_pending_operations npo = {\n\t\t.copy  = netbk->grant_copy_op,\n\t\t.meta  = netbk->meta,\n\t};\n\n\tskb_queue_head_init(&rxq);\n\n\tcount = 0;\n\n\twhile ((skb = skb_dequeue(&netbk->rx_queue)) != NULL) {\n\t\tvif = netdev_priv(skb->dev);\n\t\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t\tsco = (struct skb_cb_overlay *)skb->cb;\n\t\tsco->meta_slots_used = netbk_gop_skb(skb, &npo);\n\n\t\tcount += nr_frags + 1;\n\n\t\t__skb_queue_tail(&rxq, skb);\n\n\t\t/* Filled the batch queue? */\n\t\tif (count + MAX_SKB_FRAGS >= XEN_NETIF_RX_RING_SIZE)\n\t\t\tbreak;\n\t}\n\n\tBUG_ON(npo.meta_prod > ARRAY_SIZE(netbk->meta));\n\n\tif (!npo.copy_prod)\n\t\treturn;\n\n\tBUG_ON(npo.copy_prod > ARRAY_SIZE(netbk->grant_copy_op));\n\tgnttab_batch_copy(netbk->grant_copy_op, npo.copy_prod);\n\n\twhile ((skb = __skb_dequeue(&rxq)) != NULL) {\n\t\tsco = (struct skb_cb_overlay *)skb->cb;\n\n\t\tvif = netdev_priv(skb->dev);\n\n\t\tif (netbk->meta[npo.meta_cons].gso_size && vif->gso_prefix) {\n\t\t\tresp = RING_GET_RESPONSE(&vif->rx,\n\t\t\t\t\t\tvif->rx.rsp_prod_pvt++);\n\n\t\t\tresp->flags = XEN_NETRXF_gso_prefix | XEN_NETRXF_more_data;\n\n\t\t\tresp->offset = netbk->meta[npo.meta_cons].gso_size;\n\t\t\tresp->id = netbk->meta[npo.meta_cons].id;\n\t\t\tresp->status = sco->meta_slots_used;\n\n\t\t\tnpo.meta_cons++;\n\t\t\tsco->meta_slots_used--;\n\t\t}\n\n\n\t\tvif->dev->stats.tx_bytes += skb->len;\n\t\tvif->dev->stats.tx_packets++;\n\n\t\tstatus = netbk_check_gop(vif, sco->meta_slots_used, &npo);\n\n\t\tif (sco->meta_slots_used == 1)\n\t\t\tflags = 0;\n\t\telse\n\t\t\tflags = XEN_NETRXF_more_data;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) /* local packet? */\n\t\t\tflags |= XEN_NETRXF_csum_blank | XEN_NETRXF_data_validated;\n\t\telse if (skb->ip_summed == CHECKSUM_UNNECESSARY)\n\t\t\t/* remote but checksummed. */\n\t\t\tflags |= XEN_NETRXF_data_validated;\n\n\t\toffset = 0;\n\t\tresp = make_rx_response(vif, netbk->meta[npo.meta_cons].id,\n\t\t\t\t\tstatus, offset,\n\t\t\t\t\tnetbk->meta[npo.meta_cons].size,\n\t\t\t\t\tflags);\n\n\t\tif (netbk->meta[npo.meta_cons].gso_size && !vif->gso_prefix) {\n\t\t\tstruct xen_netif_extra_info *gso =\n\t\t\t\t(struct xen_netif_extra_info *)\n\t\t\t\tRING_GET_RESPONSE(&vif->rx,\n\t\t\t\t\t\t  vif->rx.rsp_prod_pvt++);\n\n\t\t\tresp->flags |= XEN_NETRXF_extra_info;\n\n\t\t\tgso->u.gso.size = netbk->meta[npo.meta_cons].gso_size;\n\t\t\tgso->u.gso.type = XEN_NETIF_GSO_TYPE_TCPV4;\n\t\t\tgso->u.gso.pad = 0;\n\t\t\tgso->u.gso.features = 0;\n\n\t\t\tgso->type = XEN_NETIF_EXTRA_TYPE_GSO;\n\t\t\tgso->flags = 0;\n\t\t}\n\n\t\tnetbk_add_frag_responses(vif, status,\n\t\t\t\t\t netbk->meta + npo.meta_cons + 1,\n\t\t\t\t\t sco->meta_slots_used);\n\n\t\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&vif->rx, ret);\n\t\tirq = vif->irq;\n\t\tif (ret && list_empty(&vif->notify_list))\n\t\t\tlist_add_tail(&vif->notify_list, &notify);\n\n\t\txenvif_notify_tx_completion(vif);\n\n\t\txenvif_put(vif);\n\t\tnpo.meta_cons += sco->meta_slots_used;\n\t\tdev_kfree_skb(skb);\n\t}\n\n\tlist_for_each_entry_safe(vif, tmp, &notify, notify_list) {\n\t\tnotify_remote_via_irq(vif->irq);\n\t\tlist_del_init(&vif->notify_list);\n\t}\n\n\t/* More work to do? */\n\tif (!skb_queue_empty(&netbk->rx_queue) &&\n\t\t\t!timer_pending(&netbk->net_timer))\n\t\txen_netbk_kick_thread(netbk);\n}\n\nvoid xen_netbk_queue_tx_skb(struct xenvif *vif, struct sk_buff *skb)\n{\n\tstruct xen_netbk *netbk = vif->netbk;\n\n\tskb_queue_tail(&netbk->rx_queue, skb);\n\n\txen_netbk_kick_thread(netbk);\n}\n\nstatic void xen_netbk_alarm(unsigned long data)\n{\n\tstruct xen_netbk *netbk = (struct xen_netbk *)data;\n\txen_netbk_kick_thread(netbk);\n}\n\nstatic int __on_net_schedule_list(struct xenvif *vif)\n{\n\treturn !list_empty(&vif->schedule_list);\n}\n\n/* Must be called with net_schedule_list_lock held */\nstatic void remove_from_net_schedule_list(struct xenvif *vif)\n{\n\tif (likely(__on_net_schedule_list(vif))) {\n\t\tlist_del_init(&vif->schedule_list);\n\t\txenvif_put(vif);\n\t}\n}\n\nstatic struct xenvif *poll_net_schedule_list(struct xen_netbk *netbk)\n{\n\tstruct xenvif *vif = NULL;\n\n\tspin_lock_irq(&netbk->net_schedule_list_lock);\n\tif (list_empty(&netbk->net_schedule_list))\n\t\tgoto out;\n\n\tvif = list_first_entry(&netbk->net_schedule_list,\n\t\t\t       struct xenvif, schedule_list);\n\tif (!vif)\n\t\tgoto out;\n\n\txenvif_get(vif);\n\n\tremove_from_net_schedule_list(vif);\nout:\n\tspin_unlock_irq(&netbk->net_schedule_list_lock);\n\treturn vif;\n}\n\nvoid xen_netbk_schedule_xenvif(struct xenvif *vif)\n{\n\tunsigned long flags;\n\tstruct xen_netbk *netbk = vif->netbk;\n\n\tif (__on_net_schedule_list(vif))\n\t\tgoto kick;\n\n\tspin_lock_irqsave(&netbk->net_schedule_list_lock, flags);\n\tif (!__on_net_schedule_list(vif) &&\n\t    likely(xenvif_schedulable(vif))) {\n\t\tlist_add_tail(&vif->schedule_list, &netbk->net_schedule_list);\n\t\txenvif_get(vif);\n\t}\n\tspin_unlock_irqrestore(&netbk->net_schedule_list_lock, flags);\n\nkick:\n\tsmp_mb();\n\tif ((nr_pending_reqs(netbk) < (MAX_PENDING_REQS/2)) &&\n\t    !list_empty(&netbk->net_schedule_list))\n\t\txen_netbk_kick_thread(netbk);\n}\n\nvoid xen_netbk_deschedule_xenvif(struct xenvif *vif)\n{\n\tstruct xen_netbk *netbk = vif->netbk;\n\tspin_lock_irq(&netbk->net_schedule_list_lock);\n\tremove_from_net_schedule_list(vif);\n\tspin_unlock_irq(&netbk->net_schedule_list_lock);\n}\n\nvoid xen_netbk_check_rx_xenvif(struct xenvif *vif)\n{\n\tint more_to_do;\n\n\tRING_FINAL_CHECK_FOR_REQUESTS(&vif->tx, more_to_do);\n\n\tif (more_to_do)\n\t\txen_netbk_schedule_xenvif(vif);\n}\n\nstatic void tx_add_credit(struct xenvif *vif)\n{\n\tunsigned long max_burst, max_credit;\n\n\t/*\n\t * Allow a burst big enough to transmit a jumbo packet of up to 128kB.\n\t * Otherwise the interface can seize up due to insufficient credit.\n\t */\n\tmax_burst = RING_GET_REQUEST(&vif->tx, vif->tx.req_cons)->size;\n\tmax_burst = min(max_burst, 131072UL);\n\tmax_burst = max(max_burst, vif->credit_bytes);\n\n\t/* Take care that adding a new chunk of credit doesn't wrap to zero. */\n\tmax_credit = vif->remaining_credit + vif->credit_bytes;\n\tif (max_credit < vif->remaining_credit)\n\t\tmax_credit = ULONG_MAX; /* wrapped: clamp to ULONG_MAX */\n\n\tvif->remaining_credit = min(max_credit, max_burst);\n}\n\nstatic void tx_credit_callback(unsigned long data)\n{\n\tstruct xenvif *vif = (struct xenvif *)data;\n\ttx_add_credit(vif);\n\txen_netbk_check_rx_xenvif(vif);\n}\n\nstatic void netbk_tx_err(struct xenvif *vif,\n\t\t\t struct xen_netif_tx_request *txp, RING_IDX end)\n{\n\tRING_IDX cons = vif->tx.req_cons;\n\n\tdo {\n\t\tmake_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);\n\t\tif (cons >= end)\n\t\t\tbreak;\n\t\ttxp = RING_GET_REQUEST(&vif->tx, cons++);\n\t} while (1);\n\tvif->tx.req_cons = cons;\n\txen_netbk_check_rx_xenvif(vif);\n\txenvif_put(vif);\n}\n\nstatic void netbk_fatal_tx_err(struct xenvif *vif)\n{\n\tnetdev_err(vif->dev, \"fatal error; disabling device\\n\");\n\txenvif_carrier_off(vif);\n\txenvif_put(vif);\n}\n\nstatic int netbk_count_requests(struct xenvif *vif,\n\t\t\t\tstruct xen_netif_tx_request *first,\n\t\t\t\tstruct xen_netif_tx_request *txp,\n\t\t\t\tint work_to_do)\n{\n\tRING_IDX cons = vif->tx.req_cons;\n\tint frags = 0;\n\n\tif (!(first->flags & XEN_NETTXF_more_data))\n\t\treturn 0;\n\n\tdo {\n\t\tif (frags >= work_to_do) {\n\t\t\tnetdev_err(vif->dev, \"Need more frags\\n\");\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -frags;\n\t\t}\n\n\t\tif (unlikely(frags >= MAX_SKB_FRAGS)) {\n\t\t\tnetdev_err(vif->dev, \"Too many frags\\n\");\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -frags;\n\t\t}\n\n\t\tmemcpy(txp, RING_GET_REQUEST(&vif->tx, cons + frags),\n\t\t       sizeof(*txp));\n\t\tif (txp->size > first->size) {\n\t\t\tnetdev_err(vif->dev, \"Frag is bigger than frame.\\n\");\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -frags;\n\t\t}\n\n\t\tfirst->size -= txp->size;\n\t\tfrags++;\n\n\t\tif (unlikely((txp->offset + txp->size) > PAGE_SIZE)) {\n\t\t\tnetdev_err(vif->dev, \"txp->offset: %x, size: %u\\n\",\n\t\t\t\t txp->offset, txp->size);\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -frags;\n\t\t}\n\t} while ((txp++)->flags & XEN_NETTXF_more_data);\n\treturn frags;\n}\n\nstatic struct page *xen_netbk_alloc_page(struct xen_netbk *netbk,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t u16 pending_idx)\n{\n\tstruct page *page;\n\tpage = alloc_page(GFP_KERNEL|__GFP_COLD);\n\tif (!page)\n\t\treturn NULL;\n\tset_page_ext(page, netbk, pending_idx);\n\tnetbk->mmap_pages[pending_idx] = page;\n\treturn page;\n}\n\nstatic struct gnttab_copy *xen_netbk_get_requests(struct xen_netbk *netbk,\n\t\t\t\t\t\t  struct xenvif *vif,\n\t\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t\t  struct xen_netif_tx_request *txp,\n\t\t\t\t\t\t  struct gnttab_copy *gop)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tskb_frag_t *frags = shinfo->frags;\n\tu16 pending_idx = *((u16 *)skb->data);\n\tint i, start;\n\n\t/* Skip first skb fragment if it is on same page as header fragment. */\n\tstart = (frag_get_pending_idx(&shinfo->frags[0]) == pending_idx);\n\n\tfor (i = start; i < shinfo->nr_frags; i++, txp++) {\n\t\tstruct page *page;\n\t\tpending_ring_idx_t index;\n\t\tstruct pending_tx_info *pending_tx_info =\n\t\t\tnetbk->pending_tx_info;\n\n\t\tindex = pending_index(netbk->pending_cons++);\n\t\tpending_idx = netbk->pending_ring[index];\n\t\tpage = xen_netbk_alloc_page(netbk, skb, pending_idx);\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\tgop->source.u.ref = txp->gref;\n\t\tgop->source.domid = vif->domid;\n\t\tgop->source.offset = txp->offset;\n\n\t\tgop->dest.u.gmfn = virt_to_mfn(page_address(page));\n\t\tgop->dest.domid = DOMID_SELF;\n\t\tgop->dest.offset = txp->offset;\n\n\t\tgop->len = txp->size;\n\t\tgop->flags = GNTCOPY_source_gref;\n\n\t\tgop++;\n\n\t\tmemcpy(&pending_tx_info[pending_idx].req, txp, sizeof(*txp));\n\t\txenvif_get(vif);\n\t\tpending_tx_info[pending_idx].vif = vif;\n\t\tfrag_set_pending_idx(&frags[i], pending_idx);\n\t}\n\n\treturn gop;\n}\n\nstatic int xen_netbk_tx_check_gop(struct xen_netbk *netbk,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  struct gnttab_copy **gopp)\n{\n\tstruct gnttab_copy *gop = *gopp;\n\tu16 pending_idx = *((u16 *)skb->data);\n\tstruct pending_tx_info *pending_tx_info = netbk->pending_tx_info;\n\tstruct xenvif *vif = pending_tx_info[pending_idx].vif;\n\tstruct xen_netif_tx_request *txp;\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint nr_frags = shinfo->nr_frags;\n\tint i, err, start;\n\n\t/* Check status of header. */\n\terr = gop->status;\n\tif (unlikely(err)) {\n\t\tpending_ring_idx_t index;\n\t\tindex = pending_index(netbk->pending_prod++);\n\t\ttxp = &pending_tx_info[pending_idx].req;\n\t\tmake_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);\n\t\tnetbk->pending_ring[index] = pending_idx;\n\t\txenvif_put(vif);\n\t}\n\n\t/* Skip first skb fragment if it is on same page as header fragment. */\n\tstart = (frag_get_pending_idx(&shinfo->frags[0]) == pending_idx);\n\n\tfor (i = start; i < nr_frags; i++) {\n\t\tint j, newerr;\n\t\tpending_ring_idx_t index;\n\n\t\tpending_idx = frag_get_pending_idx(&shinfo->frags[i]);\n\n\t\t/* Check error status: if okay then remember grant handle. */\n\t\tnewerr = (++gop)->status;\n\t\tif (likely(!newerr)) {\n\t\t\t/* Had a previous error? Invalidate this fragment. */\n\t\t\tif (unlikely(err))\n\t\t\t\txen_netbk_idx_release(netbk, pending_idx);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Error on this fragment: respond to client with an error. */\n\t\ttxp = &netbk->pending_tx_info[pending_idx].req;\n\t\tmake_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);\n\t\tindex = pending_index(netbk->pending_prod++);\n\t\tnetbk->pending_ring[index] = pending_idx;\n\t\txenvif_put(vif);\n\n\t\t/* Not the first error? Preceding frags already invalidated. */\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\t/* First error: invalidate header and preceding fragments. */\n\t\tpending_idx = *((u16 *)skb->data);\n\t\txen_netbk_idx_release(netbk, pending_idx);\n\t\tfor (j = start; j < i; j++) {\n\t\t\tpending_idx = frag_get_pending_idx(&shinfo->frags[j]);\n\t\t\txen_netbk_idx_release(netbk, pending_idx);\n\t\t}\n\n\t\t/* Remember the error: invalidate all subsequent fragments. */\n\t\terr = newerr;\n\t}\n\n\t*gopp = gop + 1;\n\treturn err;\n}\n\nstatic void xen_netbk_fill_frags(struct xen_netbk *netbk, struct sk_buff *skb)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint nr_frags = shinfo->nr_frags;\n\tint i;\n\n\tfor (i = 0; i < nr_frags; i++) {\n\t\tskb_frag_t *frag = shinfo->frags + i;\n\t\tstruct xen_netif_tx_request *txp;\n\t\tstruct page *page;\n\t\tu16 pending_idx;\n\n\t\tpending_idx = frag_get_pending_idx(frag);\n\n\t\ttxp = &netbk->pending_tx_info[pending_idx].req;\n\t\tpage = virt_to_page(idx_to_kaddr(netbk, pending_idx));\n\t\t__skb_fill_page_desc(skb, i, page, txp->offset, txp->size);\n\t\tskb->len += txp->size;\n\t\tskb->data_len += txp->size;\n\t\tskb->truesize += txp->size;\n\n\t\t/* Take an extra reference to offset xen_netbk_idx_release */\n\t\tget_page(netbk->mmap_pages[pending_idx]);\n\t\txen_netbk_idx_release(netbk, pending_idx);\n\t}\n}\n\nstatic int xen_netbk_get_extras(struct xenvif *vif,\n\t\t\t\tstruct xen_netif_extra_info *extras,\n\t\t\t\tint work_to_do)\n{\n\tstruct xen_netif_extra_info extra;\n\tRING_IDX cons = vif->tx.req_cons;\n\n\tdo {\n\t\tif (unlikely(work_to_do-- <= 0)) {\n\t\t\tnetdev_err(vif->dev, \"Missing extra info\\n\");\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -EBADR;\n\t\t}\n\n\t\tmemcpy(&extra, RING_GET_REQUEST(&vif->tx, cons),\n\t\t       sizeof(extra));\n\t\tif (unlikely(!extra.type ||\n\t\t\t     extra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {\n\t\t\tvif->tx.req_cons = ++cons;\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"Invalid extra type: %d\\n\", extra.type);\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmemcpy(&extras[extra.type - 1], &extra, sizeof(extra));\n\t\tvif->tx.req_cons = ++cons;\n\t} while (extra.flags & XEN_NETIF_EXTRA_FLAG_MORE);\n\n\treturn work_to_do;\n}\n\nstatic int netbk_set_skb_gso(struct xenvif *vif,\n\t\t\t     struct sk_buff *skb,\n\t\t\t     struct xen_netif_extra_info *gso)\n{\n\tif (!gso->u.gso.size) {\n\t\tnetdev_err(vif->dev, \"GSO size must not be zero.\\n\");\n\t\tnetbk_fatal_tx_err(vif);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Currently only TCPv4 S.O. is supported. */\n\tif (gso->u.gso.type != XEN_NETIF_GSO_TYPE_TCPV4) {\n\t\tnetdev_err(vif->dev, \"Bad GSO type %d.\\n\", gso->u.gso.type);\n\t\tnetbk_fatal_tx_err(vif);\n\t\treturn -EINVAL;\n\t}\n\n\tskb_shinfo(skb)->gso_size = gso->u.gso.size;\n\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n\n\t/* Header must be checked, and gso_segs computed. */\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;\n\tskb_shinfo(skb)->gso_segs = 0;\n\n\treturn 0;\n}\n\nstatic int checksum_setup(struct xenvif *vif, struct sk_buff *skb)\n{\n\tstruct iphdr *iph;\n\tunsigned char *th;\n\tint err = -EPROTO;\n\tint recalculate_partial_csum = 0;\n\n\t/*\n\t * A GSO SKB must be CHECKSUM_PARTIAL. However some buggy\n\t * peers can fail to set NETRXF_csum_blank when sending a GSO\n\t * frame. In this case force the SKB to CHECKSUM_PARTIAL and\n\t * recalculate the partial checksum.\n\t */\n\tif (skb->ip_summed != CHECKSUM_PARTIAL && skb_is_gso(skb)) {\n\t\tvif->rx_gso_checksum_fixup++;\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\trecalculate_partial_csum = 1;\n\t}\n\n\t/* A non-CHECKSUM_PARTIAL SKB does not require setup. */\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (skb->protocol != htons(ETH_P_IP))\n\t\tgoto out;\n\n\tiph = (void *)skb->data;\n\tth = skb->data + 4 * iph->ihl;\n\tif (th >= skb_tail_pointer(skb))\n\t\tgoto out;\n\n\tskb->csum_start = th - skb->head;\n\tswitch (iph->protocol) {\n\tcase IPPROTO_TCP:\n\t\tskb->csum_offset = offsetof(struct tcphdr, check);\n\n\t\tif (recalculate_partial_csum) {\n\t\t\tstruct tcphdr *tcph = (struct tcphdr *)th;\n\t\t\ttcph->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr,\n\t\t\t\t\t\t\t skb->len - iph->ihl*4,\n\t\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t\t}\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\n\t\tif (recalculate_partial_csum) {\n\t\t\tstruct udphdr *udph = (struct udphdr *)th;\n\t\t\tudph->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr,\n\t\t\t\t\t\t\t skb->len - iph->ihl*4,\n\t\t\t\t\t\t\t IPPROTO_UDP, 0);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tif (net_ratelimit())\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"Attempting to checksum a non-TCP/UDP packet, dropping a protocol %d packet\\n\",\n\t\t\t\t   iph->protocol);\n\t\tgoto out;\n\t}\n\n\tif ((th + skb->csum_offset + 2) > skb_tail_pointer(skb))\n\t\tgoto out;\n\n\terr = 0;\n\nout:\n\treturn err;\n}\n\nstatic bool tx_credit_exceeded(struct xenvif *vif, unsigned size)\n{\n\tunsigned long now = jiffies;\n\tunsigned long next_credit =\n\t\tvif->credit_timeout.expires +\n\t\tmsecs_to_jiffies(vif->credit_usec / 1000);\n\n\t/* Timer could already be pending in rare cases. */\n\tif (timer_pending(&vif->credit_timeout))\n\t\treturn true;\n\n\t/* Passed the point where we can replenish credit? */\n\tif (time_after_eq(now, next_credit)) {\n\t\tvif->credit_timeout.expires = now;\n\t\ttx_add_credit(vif);\n\t}\n\n\t/* Still too big to send right now? Set a callback. */\n\tif (size > vif->remaining_credit) {\n\t\tvif->credit_timeout.data     =\n\t\t\t(unsigned long)vif;\n\t\tvif->credit_timeout.function =\n\t\t\ttx_credit_callback;\n\t\tmod_timer(&vif->credit_timeout,\n\t\t\t  next_credit);\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic unsigned xen_netbk_tx_build_gops(struct xen_netbk *netbk)\n{\n\tstruct gnttab_copy *gop = netbk->tx_copy_ops, *request_gop;\n\tstruct sk_buff *skb;\n\tint ret;\n\n\twhile (((nr_pending_reqs(netbk) + MAX_SKB_FRAGS) < MAX_PENDING_REQS) &&\n\t\t!list_empty(&netbk->net_schedule_list)) {\n\t\tstruct xenvif *vif;\n\t\tstruct xen_netif_tx_request txreq;\n\t\tstruct xen_netif_tx_request txfrags[MAX_SKB_FRAGS];\n\t\tstruct page *page;\n\t\tstruct xen_netif_extra_info extras[XEN_NETIF_EXTRA_TYPE_MAX-1];\n\t\tu16 pending_idx;\n\t\tRING_IDX idx;\n\t\tint work_to_do;\n\t\tunsigned int data_len;\n\t\tpending_ring_idx_t index;\n\n\t\t/* Get a netif from the list with work to do. */\n\t\tvif = poll_net_schedule_list(netbk);\n\t\t/* This can sometimes happen because the test of\n\t\t * list_empty(net_schedule_list) at the top of the\n\t\t * loop is unlocked.  Just go back and have another\n\t\t * look.\n\t\t */\n\t\tif (!vif)\n\t\t\tcontinue;\n\n\t\tif (vif->tx.sring->req_prod - vif->tx.req_cons >\n\t\t    XEN_NETIF_TX_RING_SIZE) {\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"Impossible number of requests. \"\n\t\t\t\t   \"req_prod %d, req_cons %d, size %ld\\n\",\n\t\t\t\t   vif->tx.sring->req_prod, vif->tx.req_cons,\n\t\t\t\t   XEN_NETIF_TX_RING_SIZE);\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\tcontinue;\n\t\t}\n\n\t\tRING_FINAL_CHECK_FOR_REQUESTS(&vif->tx, work_to_do);\n\t\tif (!work_to_do) {\n\t\t\txenvif_put(vif);\n\t\t\tcontinue;\n\t\t}\n\n\t\tidx = vif->tx.req_cons;\n\t\trmb(); /* Ensure that we see the request before we copy it. */\n\t\tmemcpy(&txreq, RING_GET_REQUEST(&vif->tx, idx), sizeof(txreq));\n\n\t\t/* Credit-based scheduling. */\n\t\tif (txreq.size > vif->remaining_credit &&\n\t\t    tx_credit_exceeded(vif, txreq.size)) {\n\t\t\txenvif_put(vif);\n\t\t\tcontinue;\n\t\t}\n\n\t\tvif->remaining_credit -= txreq.size;\n\n\t\twork_to_do--;\n\t\tvif->tx.req_cons = ++idx;\n\n\t\tmemset(extras, 0, sizeof(extras));\n\t\tif (txreq.flags & XEN_NETTXF_extra_info) {\n\t\t\twork_to_do = xen_netbk_get_extras(vif, extras,\n\t\t\t\t\t\t\t  work_to_do);\n\t\t\tidx = vif->tx.req_cons;\n\t\t\tif (unlikely(work_to_do < 0))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tret = netbk_count_requests(vif, &txreq, txfrags, work_to_do);\n\t\tif (unlikely(ret < 0))\n\t\t\tcontinue;\n\n\t\tidx += ret;\n\n\t\tif (unlikely(txreq.size < ETH_HLEN)) {\n\t\t\tnetdev_dbg(vif->dev,\n\t\t\t\t   \"Bad packet size: %d\\n\", txreq.size);\n\t\t\tnetbk_tx_err(vif, &txreq, idx);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* No crossing a page as the payload mustn't fragment. */\n\t\tif (unlikely((txreq.offset + txreq.size) > PAGE_SIZE)) {\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"txreq.offset: %x, size: %u, end: %lu\\n\",\n\t\t\t\t   txreq.offset, txreq.size,\n\t\t\t\t   (txreq.offset&~PAGE_MASK) + txreq.size);\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\tcontinue;\n\t\t}\n\n\t\tindex = pending_index(netbk->pending_cons);\n\t\tpending_idx = netbk->pending_ring[index];\n\n\t\tdata_len = (txreq.size > PKT_PROT_LEN &&\n\t\t\t    ret < MAX_SKB_FRAGS) ?\n\t\t\tPKT_PROT_LEN : txreq.size;\n\n\t\tskb = alloc_skb(data_len + NET_SKB_PAD + NET_IP_ALIGN,\n\t\t\t\tGFP_ATOMIC | __GFP_NOWARN);\n\t\tif (unlikely(skb == NULL)) {\n\t\t\tnetdev_dbg(vif->dev,\n\t\t\t\t   \"Can't allocate a skb in start_xmit.\\n\");\n\t\t\tnetbk_tx_err(vif, &txreq, idx);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Packets passed to netif_rx() must have some headroom. */\n\t\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_GSO - 1].type) {\n\t\t\tstruct xen_netif_extra_info *gso;\n\t\t\tgso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];\n\n\t\t\tif (netbk_set_skb_gso(vif, skb, gso)) {\n\t\t\t\t/* Failure in netbk_set_skb_gso is fatal. */\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t/* XXX could copy straight to head */\n\t\tpage = xen_netbk_alloc_page(netbk, skb, pending_idx);\n\t\tif (!page) {\n\t\t\tkfree_skb(skb);\n\t\t\tnetbk_tx_err(vif, &txreq, idx);\n\t\t\tcontinue;\n\t\t}\n\n\t\tgop->source.u.ref = txreq.gref;\n\t\tgop->source.domid = vif->domid;\n\t\tgop->source.offset = txreq.offset;\n\n\t\tgop->dest.u.gmfn = virt_to_mfn(page_address(page));\n\t\tgop->dest.domid = DOMID_SELF;\n\t\tgop->dest.offset = txreq.offset;\n\n\t\tgop->len = txreq.size;\n\t\tgop->flags = GNTCOPY_source_gref;\n\n\t\tgop++;\n\n\t\tmemcpy(&netbk->pending_tx_info[pending_idx].req,\n\t\t       &txreq, sizeof(txreq));\n\t\tnetbk->pending_tx_info[pending_idx].vif = vif;\n\t\t*((u16 *)skb->data) = pending_idx;\n\n\t\t__skb_put(skb, data_len);\n\n\t\tskb_shinfo(skb)->nr_frags = ret;\n\t\tif (data_len < txreq.size) {\n\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\t\tfrag_set_pending_idx(&skb_shinfo(skb)->frags[0],\n\t\t\t\t\t     pending_idx);\n\t\t} else {\n\t\t\tfrag_set_pending_idx(&skb_shinfo(skb)->frags[0],\n\t\t\t\t\t     INVALID_PENDING_IDX);\n\t\t}\n\n\t\tnetbk->pending_cons++;\n\n\t\trequest_gop = xen_netbk_get_requests(netbk, vif,\n\t\t\t\t\t\t     skb, txfrags, gop);\n\t\tif (request_gop == NULL) {\n\t\t\tkfree_skb(skb);\n\t\t\tnetbk_tx_err(vif, &txreq, idx);\n\t\t\tcontinue;\n\t\t}\n\t\tgop = request_gop;\n\n\t\t__skb_queue_tail(&netbk->tx_queue, skb);\n\n\t\tvif->tx.req_cons = idx;\n\t\txen_netbk_check_rx_xenvif(vif);\n\n\t\tif ((gop-netbk->tx_copy_ops) >= ARRAY_SIZE(netbk->tx_copy_ops))\n\t\t\tbreak;\n\t}\n\n\treturn gop - netbk->tx_copy_ops;\n}\n\nstatic void xen_netbk_tx_submit(struct xen_netbk *netbk)\n{\n\tstruct gnttab_copy *gop = netbk->tx_copy_ops;\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue(&netbk->tx_queue)) != NULL) {\n\t\tstruct xen_netif_tx_request *txp;\n\t\tstruct xenvif *vif;\n\t\tu16 pending_idx;\n\t\tunsigned data_len;\n\n\t\tpending_idx = *((u16 *)skb->data);\n\t\tvif = netbk->pending_tx_info[pending_idx].vif;\n\t\ttxp = &netbk->pending_tx_info[pending_idx].req;\n\n\t\t/* Check the remap error code. */\n\t\tif (unlikely(xen_netbk_tx_check_gop(netbk, skb, &gop))) {\n\t\t\tnetdev_dbg(vif->dev, \"netback grant failed.\\n\");\n\t\t\tskb_shinfo(skb)->nr_frags = 0;\n\t\t\tkfree_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdata_len = skb->len;\n\t\tmemcpy(skb->data,\n\t\t       (void *)(idx_to_kaddr(netbk, pending_idx)|txp->offset),\n\t\t       data_len);\n\t\tif (data_len < txp->size) {\n\t\t\t/* Append the packet payload as a fragment. */\n\t\t\ttxp->offset += data_len;\n\t\t\ttxp->size -= data_len;\n\t\t} else {\n\t\t\t/* Schedule a response immediately. */\n\t\t\txen_netbk_idx_release(netbk, pending_idx);\n\t\t}\n\n\t\tif (txp->flags & XEN_NETTXF_csum_blank)\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\telse if (txp->flags & XEN_NETTXF_data_validated)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\txen_netbk_fill_frags(netbk, skb);\n\n\t\t/*\n\t\t * If the initial fragment was < PKT_PROT_LEN then\n\t\t * pull through some bytes from the other fragments to\n\t\t * increase the linear region to PKT_PROT_LEN bytes.\n\t\t */\n\t\tif (skb_headlen(skb) < PKT_PROT_LEN && skb_is_nonlinear(skb)) {\n\t\t\tint target = min_t(int, skb->len, PKT_PROT_LEN);\n\t\t\t__pskb_pull_tail(skb, target - skb_headlen(skb));\n\t\t}\n\n\t\tskb->dev      = vif->dev;\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\n\t\tif (checksum_setup(vif, skb)) {\n\t\t\tnetdev_dbg(vif->dev,\n\t\t\t\t   \"Can't setup checksum in net_tx_action\\n\");\n\t\t\tkfree_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tvif->dev->stats.rx_bytes += skb->len;\n\t\tvif->dev->stats.rx_packets++;\n\n\t\txenvif_receive_skb(vif, skb);\n\t}\n}\n\n/* Called after netfront has transmitted */\nstatic void xen_netbk_tx_action(struct xen_netbk *netbk)\n{\n\tunsigned nr_gops;\n\n\tnr_gops = xen_netbk_tx_build_gops(netbk);\n\n\tif (nr_gops == 0)\n\t\treturn;\n\n\tgnttab_batch_copy(netbk->tx_copy_ops, nr_gops);\n\n\txen_netbk_tx_submit(netbk);\n}\n\nstatic void xen_netbk_idx_release(struct xen_netbk *netbk, u16 pending_idx)\n{\n\tstruct xenvif *vif;\n\tstruct pending_tx_info *pending_tx_info;\n\tpending_ring_idx_t index;\n\n\t/* Already complete? */\n\tif (netbk->mmap_pages[pending_idx] == NULL)\n\t\treturn;\n\n\tpending_tx_info = &netbk->pending_tx_info[pending_idx];\n\n\tvif = pending_tx_info->vif;\n\n\tmake_tx_response(vif, &pending_tx_info->req, XEN_NETIF_RSP_OKAY);\n\n\tindex = pending_index(netbk->pending_prod++);\n\tnetbk->pending_ring[index] = pending_idx;\n\n\txenvif_put(vif);\n\n\tnetbk->mmap_pages[pending_idx]->mapping = 0;\n\tput_page(netbk->mmap_pages[pending_idx]);\n\tnetbk->mmap_pages[pending_idx] = NULL;\n}\n\nstatic void make_tx_response(struct xenvif *vif,\n\t\t\t     struct xen_netif_tx_request *txp,\n\t\t\t     s8       st)\n{\n\tRING_IDX i = vif->tx.rsp_prod_pvt;\n\tstruct xen_netif_tx_response *resp;\n\tint notify;\n\n\tresp = RING_GET_RESPONSE(&vif->tx, i);\n\tresp->id     = txp->id;\n\tresp->status = st;\n\n\tif (txp->flags & XEN_NETTXF_extra_info)\n\t\tRING_GET_RESPONSE(&vif->tx, ++i)->status = XEN_NETIF_RSP_NULL;\n\n\tvif->tx.rsp_prod_pvt = ++i;\n\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&vif->tx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(vif->irq);\n}\n\nstatic struct xen_netif_rx_response *make_rx_response(struct xenvif *vif,\n\t\t\t\t\t     u16      id,\n\t\t\t\t\t     s8       st,\n\t\t\t\t\t     u16      offset,\n\t\t\t\t\t     u16      size,\n\t\t\t\t\t     u16      flags)\n{\n\tRING_IDX i = vif->rx.rsp_prod_pvt;\n\tstruct xen_netif_rx_response *resp;\n\n\tresp = RING_GET_RESPONSE(&vif->rx, i);\n\tresp->offset     = offset;\n\tresp->flags      = flags;\n\tresp->id         = id;\n\tresp->status     = (s16)size;\n\tif (st < 0)\n\t\tresp->status = (s16)st;\n\n\tvif->rx.rsp_prod_pvt = ++i;\n\n\treturn resp;\n}\n\nstatic inline int rx_work_todo(struct xen_netbk *netbk)\n{\n\treturn !skb_queue_empty(&netbk->rx_queue);\n}\n\nstatic inline int tx_work_todo(struct xen_netbk *netbk)\n{\n\n\tif (((nr_pending_reqs(netbk) + MAX_SKB_FRAGS) < MAX_PENDING_REQS) &&\n\t\t\t!list_empty(&netbk->net_schedule_list))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int xen_netbk_kthread(void *data)\n{\n\tstruct xen_netbk *netbk = data;\n\twhile (!kthread_should_stop()) {\n\t\twait_event_interruptible(netbk->wq,\n\t\t\t\trx_work_todo(netbk) ||\n\t\t\t\ttx_work_todo(netbk) ||\n\t\t\t\tkthread_should_stop());\n\t\tcond_resched();\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (rx_work_todo(netbk))\n\t\t\txen_netbk_rx_action(netbk);\n\n\t\tif (tx_work_todo(netbk))\n\t\t\txen_netbk_tx_action(netbk);\n\t}\n\n\treturn 0;\n}\n\nvoid xen_netbk_unmap_frontend_rings(struct xenvif *vif)\n{\n\tif (vif->tx.sring)\n\t\txenbus_unmap_ring_vfree(xenvif_to_xenbus_device(vif),\n\t\t\t\t\tvif->tx.sring);\n\tif (vif->rx.sring)\n\t\txenbus_unmap_ring_vfree(xenvif_to_xenbus_device(vif),\n\t\t\t\t\tvif->rx.sring);\n}\n\nint xen_netbk_map_frontend_rings(struct xenvif *vif,\n\t\t\t\t grant_ref_t tx_ring_ref,\n\t\t\t\t grant_ref_t rx_ring_ref)\n{\n\tvoid *addr;\n\tstruct xen_netif_tx_sring *txs;\n\tstruct xen_netif_rx_sring *rxs;\n\n\tint err = -ENOMEM;\n\n\terr = xenbus_map_ring_valloc(xenvif_to_xenbus_device(vif),\n\t\t\t\t     tx_ring_ref, &addr);\n\tif (err)\n\t\tgoto err;\n\n\ttxs = (struct xen_netif_tx_sring *)addr;\n\tBACK_RING_INIT(&vif->tx, txs, PAGE_SIZE);\n\n\terr = xenbus_map_ring_valloc(xenvif_to_xenbus_device(vif),\n\t\t\t\t     rx_ring_ref, &addr);\n\tif (err)\n\t\tgoto err;\n\n\trxs = (struct xen_netif_rx_sring *)addr;\n\tBACK_RING_INIT(&vif->rx, rxs, PAGE_SIZE);\n\n\tvif->rx_req_cons_peek = 0;\n\n\treturn 0;\n\nerr:\n\txen_netbk_unmap_frontend_rings(vif);\n\treturn err;\n}\n\nstatic int __init netback_init(void)\n{\n\tint i;\n\tint rc = 0;\n\tint group;\n\n\tif (!xen_domain())\n\t\treturn -ENODEV;\n\n\txen_netbk_group_nr = num_online_cpus();\n\txen_netbk = vzalloc(sizeof(struct xen_netbk) * xen_netbk_group_nr);\n\tif (!xen_netbk)\n\t\treturn -ENOMEM;\n\n\tfor (group = 0; group < xen_netbk_group_nr; group++) {\n\t\tstruct xen_netbk *netbk = &xen_netbk[group];\n\t\tskb_queue_head_init(&netbk->rx_queue);\n\t\tskb_queue_head_init(&netbk->tx_queue);\n\n\t\tinit_timer(&netbk->net_timer);\n\t\tnetbk->net_timer.data = (unsigned long)netbk;\n\t\tnetbk->net_timer.function = xen_netbk_alarm;\n\n\t\tnetbk->pending_cons = 0;\n\t\tnetbk->pending_prod = MAX_PENDING_REQS;\n\t\tfor (i = 0; i < MAX_PENDING_REQS; i++)\n\t\t\tnetbk->pending_ring[i] = i;\n\n\t\tinit_waitqueue_head(&netbk->wq);\n\t\tnetbk->task = kthread_create(xen_netbk_kthread,\n\t\t\t\t\t     (void *)netbk,\n\t\t\t\t\t     \"netback/%u\", group);\n\n\t\tif (IS_ERR(netbk->task)) {\n\t\t\tprintk(KERN_ALERT \"kthread_create() fails at netback\\n\");\n\t\t\tdel_timer(&netbk->net_timer);\n\t\t\trc = PTR_ERR(netbk->task);\n\t\t\tgoto failed_init;\n\t\t}\n\n\t\tkthread_bind(netbk->task, group);\n\n\t\tINIT_LIST_HEAD(&netbk->net_schedule_list);\n\n\t\tspin_lock_init(&netbk->net_schedule_list_lock);\n\n\t\tatomic_set(&netbk->netfront_count, 0);\n\n\t\twake_up_process(netbk->task);\n\t}\n\n\trc = xenvif_xenbus_init();\n\tif (rc)\n\t\tgoto failed_init;\n\n\treturn 0;\n\nfailed_init:\n\twhile (--group >= 0) {\n\t\tstruct xen_netbk *netbk = &xen_netbk[group];\n\t\tfor (i = 0; i < MAX_PENDING_REQS; i++) {\n\t\t\tif (netbk->mmap_pages[i])\n\t\t\t\t__free_page(netbk->mmap_pages[i]);\n\t\t}\n\t\tdel_timer(&netbk->net_timer);\n\t\tkthread_stop(netbk->task);\n\t}\n\tvfree(xen_netbk);\n\treturn rc;\n\n}\n\nmodule_init(netback_init);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_ALIAS(\"xen-backend:vif\");\n"], "fixing_code": ["/*\n * Back-end of the driver for virtual network devices. This portion of the\n * driver exports a 'unified' network-device interface that can be accessed\n * by any operating system that implements a compatible front end. A\n * reference front-end implementation can be found in:\n *  drivers/net/xen-netfront.c\n *\n * Copyright (c) 2002-2005, K A Fraser\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License version 2\n * as published by the Free Software Foundation; or, when distributed\n * separately from the Linux kernel or incorporated into other\n * software packages, subject to the following license:\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this source file (the \"Software\"), to deal in the Software without\n * restriction, including without limitation the rights to use, copy, modify,\n * merge, publish, distribute, sublicense, and/or sell copies of the Software,\n * and to permit persons to whom the Software is furnished to do so, subject to\n * the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n */\n\n#include \"common.h\"\n\n#include <linux/kthread.h>\n#include <linux/if_vlan.h>\n#include <linux/udp.h>\n\n#include <net/tcp.h>\n\n#include <xen/xen.h>\n#include <xen/events.h>\n#include <xen/interface/memory.h>\n\n#include <asm/xen/hypercall.h>\n#include <asm/xen/page.h>\n\nstruct pending_tx_info {\n\tstruct xen_netif_tx_request req;\n\tstruct xenvif *vif;\n};\ntypedef unsigned int pending_ring_idx_t;\n\nstruct netbk_rx_meta {\n\tint id;\n\tint size;\n\tint gso_size;\n};\n\n#define MAX_PENDING_REQS 256\n\n/* Discriminate from any valid pending_idx value. */\n#define INVALID_PENDING_IDX 0xFFFF\n\n#define MAX_BUFFER_OFFSET PAGE_SIZE\n\n/* extra field used in struct page */\nunion page_ext {\n\tstruct {\n#if BITS_PER_LONG < 64\n#define IDX_WIDTH   8\n#define GROUP_WIDTH (BITS_PER_LONG - IDX_WIDTH)\n\t\tunsigned int group:GROUP_WIDTH;\n\t\tunsigned int idx:IDX_WIDTH;\n#else\n\t\tunsigned int group, idx;\n#endif\n\t} e;\n\tvoid *mapping;\n};\n\nstruct xen_netbk {\n\twait_queue_head_t wq;\n\tstruct task_struct *task;\n\n\tstruct sk_buff_head rx_queue;\n\tstruct sk_buff_head tx_queue;\n\n\tstruct timer_list net_timer;\n\n\tstruct page *mmap_pages[MAX_PENDING_REQS];\n\n\tpending_ring_idx_t pending_prod;\n\tpending_ring_idx_t pending_cons;\n\tstruct list_head net_schedule_list;\n\n\t/* Protect the net_schedule_list in netif. */\n\tspinlock_t net_schedule_list_lock;\n\n\tatomic_t netfront_count;\n\n\tstruct pending_tx_info pending_tx_info[MAX_PENDING_REQS];\n\tstruct gnttab_copy tx_copy_ops[MAX_PENDING_REQS];\n\n\tu16 pending_ring[MAX_PENDING_REQS];\n\n\t/*\n\t * Given MAX_BUFFER_OFFSET of 4096 the worst case is that each\n\t * head/fragment page uses 2 copy operations because it\n\t * straddles two buffers in the frontend.\n\t */\n\tstruct gnttab_copy grant_copy_op[2*XEN_NETIF_RX_RING_SIZE];\n\tstruct netbk_rx_meta meta[2*XEN_NETIF_RX_RING_SIZE];\n};\n\nstatic struct xen_netbk *xen_netbk;\nstatic int xen_netbk_group_nr;\n\nvoid xen_netbk_add_xenvif(struct xenvif *vif)\n{\n\tint i;\n\tint min_netfront_count;\n\tint min_group = 0;\n\tstruct xen_netbk *netbk;\n\n\tmin_netfront_count = atomic_read(&xen_netbk[0].netfront_count);\n\tfor (i = 0; i < xen_netbk_group_nr; i++) {\n\t\tint netfront_count = atomic_read(&xen_netbk[i].netfront_count);\n\t\tif (netfront_count < min_netfront_count) {\n\t\t\tmin_group = i;\n\t\t\tmin_netfront_count = netfront_count;\n\t\t}\n\t}\n\n\tnetbk = &xen_netbk[min_group];\n\n\tvif->netbk = netbk;\n\tatomic_inc(&netbk->netfront_count);\n}\n\nvoid xen_netbk_remove_xenvif(struct xenvif *vif)\n{\n\tstruct xen_netbk *netbk = vif->netbk;\n\tvif->netbk = NULL;\n\tatomic_dec(&netbk->netfront_count);\n}\n\nstatic void xen_netbk_idx_release(struct xen_netbk *netbk, u16 pending_idx,\n\t\t\t\t  u8 status);\nstatic void make_tx_response(struct xenvif *vif,\n\t\t\t     struct xen_netif_tx_request *txp,\n\t\t\t     s8       st);\nstatic struct xen_netif_rx_response *make_rx_response(struct xenvif *vif,\n\t\t\t\t\t     u16      id,\n\t\t\t\t\t     s8       st,\n\t\t\t\t\t     u16      offset,\n\t\t\t\t\t     u16      size,\n\t\t\t\t\t     u16      flags);\n\nstatic inline unsigned long idx_to_pfn(struct xen_netbk *netbk,\n\t\t\t\t       u16 idx)\n{\n\treturn page_to_pfn(netbk->mmap_pages[idx]);\n}\n\nstatic inline unsigned long idx_to_kaddr(struct xen_netbk *netbk,\n\t\t\t\t\t u16 idx)\n{\n\treturn (unsigned long)pfn_to_kaddr(idx_to_pfn(netbk, idx));\n}\n\n/* extra field used in struct page */\nstatic inline void set_page_ext(struct page *pg, struct xen_netbk *netbk,\n\t\t\t\tunsigned int idx)\n{\n\tunsigned int group = netbk - xen_netbk;\n\tunion page_ext ext = { .e = { .group = group + 1, .idx = idx } };\n\n\tBUILD_BUG_ON(sizeof(ext) > sizeof(ext.mapping));\n\tpg->mapping = ext.mapping;\n}\n\nstatic int get_page_ext(struct page *pg,\n\t\t\tunsigned int *pgroup, unsigned int *pidx)\n{\n\tunion page_ext ext = { .mapping = pg->mapping };\n\tstruct xen_netbk *netbk;\n\tunsigned int group, idx;\n\n\tgroup = ext.e.group - 1;\n\n\tif (group < 0 || group >= xen_netbk_group_nr)\n\t\treturn 0;\n\n\tnetbk = &xen_netbk[group];\n\n\tidx = ext.e.idx;\n\n\tif ((idx < 0) || (idx >= MAX_PENDING_REQS))\n\t\treturn 0;\n\n\tif (netbk->mmap_pages[idx] != pg)\n\t\treturn 0;\n\n\t*pgroup = group;\n\t*pidx = idx;\n\n\treturn 1;\n}\n\n/*\n * This is the amount of packet we copy rather than map, so that the\n * guest can't fiddle with the contents of the headers while we do\n * packet processing on them (netfilter, routing, etc).\n */\n#define PKT_PROT_LEN    (ETH_HLEN + \\\n\t\t\t VLAN_HLEN + \\\n\t\t\t sizeof(struct iphdr) + MAX_IPOPTLEN + \\\n\t\t\t sizeof(struct tcphdr) + MAX_TCP_OPTION_SPACE)\n\nstatic u16 frag_get_pending_idx(skb_frag_t *frag)\n{\n\treturn (u16)frag->page_offset;\n}\n\nstatic void frag_set_pending_idx(skb_frag_t *frag, u16 pending_idx)\n{\n\tfrag->page_offset = pending_idx;\n}\n\nstatic inline pending_ring_idx_t pending_index(unsigned i)\n{\n\treturn i & (MAX_PENDING_REQS-1);\n}\n\nstatic inline pending_ring_idx_t nr_pending_reqs(struct xen_netbk *netbk)\n{\n\treturn MAX_PENDING_REQS -\n\t\tnetbk->pending_prod + netbk->pending_cons;\n}\n\nstatic void xen_netbk_kick_thread(struct xen_netbk *netbk)\n{\n\twake_up(&netbk->wq);\n}\n\nstatic int max_required_rx_slots(struct xenvif *vif)\n{\n\tint max = DIV_ROUND_UP(vif->dev->mtu, PAGE_SIZE);\n\n\tif (vif->can_sg || vif->gso || vif->gso_prefix)\n\t\tmax += MAX_SKB_FRAGS + 1; /* extra_info + frags */\n\n\treturn max;\n}\n\nint xen_netbk_rx_ring_full(struct xenvif *vif)\n{\n\tRING_IDX peek   = vif->rx_req_cons_peek;\n\tRING_IDX needed = max_required_rx_slots(vif);\n\n\treturn ((vif->rx.sring->req_prod - peek) < needed) ||\n\t       ((vif->rx.rsp_prod_pvt + XEN_NETIF_RX_RING_SIZE - peek) < needed);\n}\n\nint xen_netbk_must_stop_queue(struct xenvif *vif)\n{\n\tif (!xen_netbk_rx_ring_full(vif))\n\t\treturn 0;\n\n\tvif->rx.sring->req_event = vif->rx_req_cons_peek +\n\t\tmax_required_rx_slots(vif);\n\tmb(); /* request notification /then/ check the queue */\n\n\treturn xen_netbk_rx_ring_full(vif);\n}\n\n/*\n * Returns true if we should start a new receive buffer instead of\n * adding 'size' bytes to a buffer which currently contains 'offset'\n * bytes.\n */\nstatic bool start_new_rx_buffer(int offset, unsigned long size, int head)\n{\n\t/* simple case: we have completely filled the current buffer. */\n\tif (offset == MAX_BUFFER_OFFSET)\n\t\treturn true;\n\n\t/*\n\t * complex case: start a fresh buffer if the current frag\n\t * would overflow the current buffer but only if:\n\t *     (i)   this frag would fit completely in the next buffer\n\t * and (ii)  there is already some data in the current buffer\n\t * and (iii) this is not the head buffer.\n\t *\n\t * Where:\n\t * - (i) stops us splitting a frag into two copies\n\t *   unless the frag is too large for a single buffer.\n\t * - (ii) stops us from leaving a buffer pointlessly empty.\n\t * - (iii) stops us leaving the first buffer\n\t *   empty. Strictly speaking this is already covered\n\t *   by (ii) but is explicitly checked because\n\t *   netfront relies on the first buffer being\n\t *   non-empty and can crash otherwise.\n\t *\n\t * This means we will effectively linearise small\n\t * frags but do not needlessly split large buffers\n\t * into multiple copies tend to give large frags their\n\t * own buffers as before.\n\t */\n\tif ((offset + size > MAX_BUFFER_OFFSET) &&\n\t    (size <= MAX_BUFFER_OFFSET) && offset && !head)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Figure out how many ring slots we're going to need to send @skb to\n * the guest. This function is essentially a dry run of\n * netbk_gop_frag_copy.\n */\nunsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb)\n{\n\tunsigned int count;\n\tint i, copy_off;\n\n\tcount = DIV_ROUND_UP(skb_headlen(skb), PAGE_SIZE);\n\n\tcopy_off = skb_headlen(skb) % PAGE_SIZE;\n\n\tif (skb_shinfo(skb)->gso_size)\n\t\tcount++;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tunsigned long size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tunsigned long offset = skb_shinfo(skb)->frags[i].page_offset;\n\t\tunsigned long bytes;\n\n\t\toffset &= ~PAGE_MASK;\n\n\t\twhile (size > 0) {\n\t\t\tBUG_ON(offset >= PAGE_SIZE);\n\t\t\tBUG_ON(copy_off > MAX_BUFFER_OFFSET);\n\n\t\t\tbytes = PAGE_SIZE - offset;\n\n\t\t\tif (bytes > size)\n\t\t\t\tbytes = size;\n\n\t\t\tif (start_new_rx_buffer(copy_off, bytes, 0)) {\n\t\t\t\tcount++;\n\t\t\t\tcopy_off = 0;\n\t\t\t}\n\n\t\t\tif (copy_off + bytes > MAX_BUFFER_OFFSET)\n\t\t\t\tbytes = MAX_BUFFER_OFFSET - copy_off;\n\n\t\t\tcopy_off += bytes;\n\n\t\t\toffset += bytes;\n\t\t\tsize -= bytes;\n\n\t\t\tif (offset == PAGE_SIZE)\n\t\t\t\toffset = 0;\n\t\t}\n\t}\n\treturn count;\n}\n\nstruct netrx_pending_operations {\n\tunsigned copy_prod, copy_cons;\n\tunsigned meta_prod, meta_cons;\n\tstruct gnttab_copy *copy;\n\tstruct netbk_rx_meta *meta;\n\tint copy_off;\n\tgrant_ref_t copy_gref;\n};\n\nstatic struct netbk_rx_meta *get_next_rx_buffer(struct xenvif *vif,\n\t\t\t\t\t\tstruct netrx_pending_operations *npo)\n{\n\tstruct netbk_rx_meta *meta;\n\tstruct xen_netif_rx_request *req;\n\n\treq = RING_GET_REQUEST(&vif->rx, vif->rx.req_cons++);\n\n\tmeta = npo->meta + npo->meta_prod++;\n\tmeta->gso_size = 0;\n\tmeta->size = 0;\n\tmeta->id = req->id;\n\n\tnpo->copy_off = 0;\n\tnpo->copy_gref = req->gref;\n\n\treturn meta;\n}\n\n/*\n * Set up the grant operations for this fragment. If it's a flipping\n * interface, we also set up the unmap request from here.\n */\nstatic void netbk_gop_frag_copy(struct xenvif *vif, struct sk_buff *skb,\n\t\t\t\tstruct netrx_pending_operations *npo,\n\t\t\t\tstruct page *page, unsigned long size,\n\t\t\t\tunsigned long offset, int *head)\n{\n\tstruct gnttab_copy *copy_gop;\n\tstruct netbk_rx_meta *meta;\n\t/*\n\t * These variables are used iff get_page_ext returns true,\n\t * in which case they are guaranteed to be initialized.\n\t */\n\tunsigned int uninitialized_var(group), uninitialized_var(idx);\n\tint foreign = get_page_ext(page, &group, &idx);\n\tunsigned long bytes;\n\n\t/* Data must not cross a page boundary. */\n\tBUG_ON(size + offset > PAGE_SIZE<<compound_order(page));\n\n\tmeta = npo->meta + npo->meta_prod - 1;\n\n\t/* Skip unused frames from start of page */\n\tpage += offset >> PAGE_SHIFT;\n\toffset &= ~PAGE_MASK;\n\n\twhile (size > 0) {\n\t\tBUG_ON(offset >= PAGE_SIZE);\n\t\tBUG_ON(npo->copy_off > MAX_BUFFER_OFFSET);\n\n\t\tbytes = PAGE_SIZE - offset;\n\n\t\tif (bytes > size)\n\t\t\tbytes = size;\n\n\t\tif (start_new_rx_buffer(npo->copy_off, bytes, *head)) {\n\t\t\t/*\n\t\t\t * Netfront requires there to be some data in the head\n\t\t\t * buffer.\n\t\t\t */\n\t\t\tBUG_ON(*head);\n\n\t\t\tmeta = get_next_rx_buffer(vif, npo);\n\t\t}\n\n\t\tif (npo->copy_off + bytes > MAX_BUFFER_OFFSET)\n\t\t\tbytes = MAX_BUFFER_OFFSET - npo->copy_off;\n\n\t\tcopy_gop = npo->copy + npo->copy_prod++;\n\t\tcopy_gop->flags = GNTCOPY_dest_gref;\n\t\tif (foreign) {\n\t\t\tstruct xen_netbk *netbk = &xen_netbk[group];\n\t\t\tstruct pending_tx_info *src_pend;\n\n\t\t\tsrc_pend = &netbk->pending_tx_info[idx];\n\n\t\t\tcopy_gop->source.domid = src_pend->vif->domid;\n\t\t\tcopy_gop->source.u.ref = src_pend->req.gref;\n\t\t\tcopy_gop->flags |= GNTCOPY_source_gref;\n\t\t} else {\n\t\t\tvoid *vaddr = page_address(page);\n\t\t\tcopy_gop->source.domid = DOMID_SELF;\n\t\t\tcopy_gop->source.u.gmfn = virt_to_mfn(vaddr);\n\t\t}\n\t\tcopy_gop->source.offset = offset;\n\t\tcopy_gop->dest.domid = vif->domid;\n\n\t\tcopy_gop->dest.offset = npo->copy_off;\n\t\tcopy_gop->dest.u.ref = npo->copy_gref;\n\t\tcopy_gop->len = bytes;\n\n\t\tnpo->copy_off += bytes;\n\t\tmeta->size += bytes;\n\n\t\toffset += bytes;\n\t\tsize -= bytes;\n\n\t\t/* Next frame */\n\t\tif (offset == PAGE_SIZE && size) {\n\t\t\tBUG_ON(!PageCompound(page));\n\t\t\tpage++;\n\t\t\toffset = 0;\n\t\t}\n\n\t\t/* Leave a gap for the GSO descriptor. */\n\t\tif (*head && skb_shinfo(skb)->gso_size && !vif->gso_prefix)\n\t\t\tvif->rx.req_cons++;\n\n\t\t*head = 0; /* There must be something in this buffer now. */\n\n\t}\n}\n\n/*\n * Prepare an SKB to be transmitted to the frontend.\n *\n * This function is responsible for allocating grant operations, meta\n * structures, etc.\n *\n * It returns the number of meta structures consumed. The number of\n * ring slots used is always equal to the number of meta slots used\n * plus the number of GSO descriptors used. Currently, we use either\n * zero GSO descriptors (for non-GSO packets) or one descriptor (for\n * frontend-side LRO).\n */\nstatic int netbk_gop_skb(struct sk_buff *skb,\n\t\t\t struct netrx_pending_operations *npo)\n{\n\tstruct xenvif *vif = netdev_priv(skb->dev);\n\tint nr_frags = skb_shinfo(skb)->nr_frags;\n\tint i;\n\tstruct xen_netif_rx_request *req;\n\tstruct netbk_rx_meta *meta;\n\tunsigned char *data;\n\tint head = 1;\n\tint old_meta_prod;\n\n\told_meta_prod = npo->meta_prod;\n\n\t/* Set up a GSO prefix descriptor, if necessary */\n\tif (skb_shinfo(skb)->gso_size && vif->gso_prefix) {\n\t\treq = RING_GET_REQUEST(&vif->rx, vif->rx.req_cons++);\n\t\tmeta = npo->meta + npo->meta_prod++;\n\t\tmeta->gso_size = skb_shinfo(skb)->gso_size;\n\t\tmeta->size = 0;\n\t\tmeta->id = req->id;\n\t}\n\n\treq = RING_GET_REQUEST(&vif->rx, vif->rx.req_cons++);\n\tmeta = npo->meta + npo->meta_prod++;\n\n\tif (!vif->gso_prefix)\n\t\tmeta->gso_size = skb_shinfo(skb)->gso_size;\n\telse\n\t\tmeta->gso_size = 0;\n\n\tmeta->size = 0;\n\tmeta->id = req->id;\n\tnpo->copy_off = 0;\n\tnpo->copy_gref = req->gref;\n\n\tdata = skb->data;\n\twhile (data < skb_tail_pointer(skb)) {\n\t\tunsigned int offset = offset_in_page(data);\n\t\tunsigned int len = PAGE_SIZE - offset;\n\n\t\tif (data + len > skb_tail_pointer(skb))\n\t\t\tlen = skb_tail_pointer(skb) - data;\n\n\t\tnetbk_gop_frag_copy(vif, skb, npo,\n\t\t\t\t    virt_to_page(data), len, offset, &head);\n\t\tdata += len;\n\t}\n\n\tfor (i = 0; i < nr_frags; i++) {\n\t\tnetbk_gop_frag_copy(vif, skb, npo,\n\t\t\t\t    skb_frag_page(&skb_shinfo(skb)->frags[i]),\n\t\t\t\t    skb_frag_size(&skb_shinfo(skb)->frags[i]),\n\t\t\t\t    skb_shinfo(skb)->frags[i].page_offset,\n\t\t\t\t    &head);\n\t}\n\n\treturn npo->meta_prod - old_meta_prod;\n}\n\n/*\n * This is a twin to netbk_gop_skb.  Assume that netbk_gop_skb was\n * used to set up the operations on the top of\n * netrx_pending_operations, which have since been done.  Check that\n * they didn't give any errors and advance over them.\n */\nstatic int netbk_check_gop(struct xenvif *vif, int nr_meta_slots,\n\t\t\t   struct netrx_pending_operations *npo)\n{\n\tstruct gnttab_copy     *copy_op;\n\tint status = XEN_NETIF_RSP_OKAY;\n\tint i;\n\n\tfor (i = 0; i < nr_meta_slots; i++) {\n\t\tcopy_op = npo->copy + npo->copy_cons++;\n\t\tif (copy_op->status != GNTST_okay) {\n\t\t\tnetdev_dbg(vif->dev,\n\t\t\t\t   \"Bad status %d from copy to DOM%d.\\n\",\n\t\t\t\t   copy_op->status, vif->domid);\n\t\t\tstatus = XEN_NETIF_RSP_ERROR;\n\t\t}\n\t}\n\n\treturn status;\n}\n\nstatic void netbk_add_frag_responses(struct xenvif *vif, int status,\n\t\t\t\t     struct netbk_rx_meta *meta,\n\t\t\t\t     int nr_meta_slots)\n{\n\tint i;\n\tunsigned long offset;\n\n\t/* No fragments used */\n\tif (nr_meta_slots <= 1)\n\t\treturn;\n\n\tnr_meta_slots--;\n\n\tfor (i = 0; i < nr_meta_slots; i++) {\n\t\tint flags;\n\t\tif (i == nr_meta_slots - 1)\n\t\t\tflags = 0;\n\t\telse\n\t\t\tflags = XEN_NETRXF_more_data;\n\n\t\toffset = 0;\n\t\tmake_rx_response(vif, meta[i].id, status, offset,\n\t\t\t\t meta[i].size, flags);\n\t}\n}\n\nstruct skb_cb_overlay {\n\tint meta_slots_used;\n};\n\nstatic void xen_netbk_rx_action(struct xen_netbk *netbk)\n{\n\tstruct xenvif *vif = NULL, *tmp;\n\ts8 status;\n\tu16 irq, flags;\n\tstruct xen_netif_rx_response *resp;\n\tstruct sk_buff_head rxq;\n\tstruct sk_buff *skb;\n\tLIST_HEAD(notify);\n\tint ret;\n\tint nr_frags;\n\tint count;\n\tunsigned long offset;\n\tstruct skb_cb_overlay *sco;\n\n\tstruct netrx_pending_operations npo = {\n\t\t.copy  = netbk->grant_copy_op,\n\t\t.meta  = netbk->meta,\n\t};\n\n\tskb_queue_head_init(&rxq);\n\n\tcount = 0;\n\n\twhile ((skb = skb_dequeue(&netbk->rx_queue)) != NULL) {\n\t\tvif = netdev_priv(skb->dev);\n\t\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t\tsco = (struct skb_cb_overlay *)skb->cb;\n\t\tsco->meta_slots_used = netbk_gop_skb(skb, &npo);\n\n\t\tcount += nr_frags + 1;\n\n\t\t__skb_queue_tail(&rxq, skb);\n\n\t\t/* Filled the batch queue? */\n\t\tif (count + MAX_SKB_FRAGS >= XEN_NETIF_RX_RING_SIZE)\n\t\t\tbreak;\n\t}\n\n\tBUG_ON(npo.meta_prod > ARRAY_SIZE(netbk->meta));\n\n\tif (!npo.copy_prod)\n\t\treturn;\n\n\tBUG_ON(npo.copy_prod > ARRAY_SIZE(netbk->grant_copy_op));\n\tgnttab_batch_copy(netbk->grant_copy_op, npo.copy_prod);\n\n\twhile ((skb = __skb_dequeue(&rxq)) != NULL) {\n\t\tsco = (struct skb_cb_overlay *)skb->cb;\n\n\t\tvif = netdev_priv(skb->dev);\n\n\t\tif (netbk->meta[npo.meta_cons].gso_size && vif->gso_prefix) {\n\t\t\tresp = RING_GET_RESPONSE(&vif->rx,\n\t\t\t\t\t\tvif->rx.rsp_prod_pvt++);\n\n\t\t\tresp->flags = XEN_NETRXF_gso_prefix | XEN_NETRXF_more_data;\n\n\t\t\tresp->offset = netbk->meta[npo.meta_cons].gso_size;\n\t\t\tresp->id = netbk->meta[npo.meta_cons].id;\n\t\t\tresp->status = sco->meta_slots_used;\n\n\t\t\tnpo.meta_cons++;\n\t\t\tsco->meta_slots_used--;\n\t\t}\n\n\n\t\tvif->dev->stats.tx_bytes += skb->len;\n\t\tvif->dev->stats.tx_packets++;\n\n\t\tstatus = netbk_check_gop(vif, sco->meta_slots_used, &npo);\n\n\t\tif (sco->meta_slots_used == 1)\n\t\t\tflags = 0;\n\t\telse\n\t\t\tflags = XEN_NETRXF_more_data;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) /* local packet? */\n\t\t\tflags |= XEN_NETRXF_csum_blank | XEN_NETRXF_data_validated;\n\t\telse if (skb->ip_summed == CHECKSUM_UNNECESSARY)\n\t\t\t/* remote but checksummed. */\n\t\t\tflags |= XEN_NETRXF_data_validated;\n\n\t\toffset = 0;\n\t\tresp = make_rx_response(vif, netbk->meta[npo.meta_cons].id,\n\t\t\t\t\tstatus, offset,\n\t\t\t\t\tnetbk->meta[npo.meta_cons].size,\n\t\t\t\t\tflags);\n\n\t\tif (netbk->meta[npo.meta_cons].gso_size && !vif->gso_prefix) {\n\t\t\tstruct xen_netif_extra_info *gso =\n\t\t\t\t(struct xen_netif_extra_info *)\n\t\t\t\tRING_GET_RESPONSE(&vif->rx,\n\t\t\t\t\t\t  vif->rx.rsp_prod_pvt++);\n\n\t\t\tresp->flags |= XEN_NETRXF_extra_info;\n\n\t\t\tgso->u.gso.size = netbk->meta[npo.meta_cons].gso_size;\n\t\t\tgso->u.gso.type = XEN_NETIF_GSO_TYPE_TCPV4;\n\t\t\tgso->u.gso.pad = 0;\n\t\t\tgso->u.gso.features = 0;\n\n\t\t\tgso->type = XEN_NETIF_EXTRA_TYPE_GSO;\n\t\t\tgso->flags = 0;\n\t\t}\n\n\t\tnetbk_add_frag_responses(vif, status,\n\t\t\t\t\t netbk->meta + npo.meta_cons + 1,\n\t\t\t\t\t sco->meta_slots_used);\n\n\t\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&vif->rx, ret);\n\t\tirq = vif->irq;\n\t\tif (ret && list_empty(&vif->notify_list))\n\t\t\tlist_add_tail(&vif->notify_list, &notify);\n\n\t\txenvif_notify_tx_completion(vif);\n\n\t\txenvif_put(vif);\n\t\tnpo.meta_cons += sco->meta_slots_used;\n\t\tdev_kfree_skb(skb);\n\t}\n\n\tlist_for_each_entry_safe(vif, tmp, &notify, notify_list) {\n\t\tnotify_remote_via_irq(vif->irq);\n\t\tlist_del_init(&vif->notify_list);\n\t}\n\n\t/* More work to do? */\n\tif (!skb_queue_empty(&netbk->rx_queue) &&\n\t\t\t!timer_pending(&netbk->net_timer))\n\t\txen_netbk_kick_thread(netbk);\n}\n\nvoid xen_netbk_queue_tx_skb(struct xenvif *vif, struct sk_buff *skb)\n{\n\tstruct xen_netbk *netbk = vif->netbk;\n\n\tskb_queue_tail(&netbk->rx_queue, skb);\n\n\txen_netbk_kick_thread(netbk);\n}\n\nstatic void xen_netbk_alarm(unsigned long data)\n{\n\tstruct xen_netbk *netbk = (struct xen_netbk *)data;\n\txen_netbk_kick_thread(netbk);\n}\n\nstatic int __on_net_schedule_list(struct xenvif *vif)\n{\n\treturn !list_empty(&vif->schedule_list);\n}\n\n/* Must be called with net_schedule_list_lock held */\nstatic void remove_from_net_schedule_list(struct xenvif *vif)\n{\n\tif (likely(__on_net_schedule_list(vif))) {\n\t\tlist_del_init(&vif->schedule_list);\n\t\txenvif_put(vif);\n\t}\n}\n\nstatic struct xenvif *poll_net_schedule_list(struct xen_netbk *netbk)\n{\n\tstruct xenvif *vif = NULL;\n\n\tspin_lock_irq(&netbk->net_schedule_list_lock);\n\tif (list_empty(&netbk->net_schedule_list))\n\t\tgoto out;\n\n\tvif = list_first_entry(&netbk->net_schedule_list,\n\t\t\t       struct xenvif, schedule_list);\n\tif (!vif)\n\t\tgoto out;\n\n\txenvif_get(vif);\n\n\tremove_from_net_schedule_list(vif);\nout:\n\tspin_unlock_irq(&netbk->net_schedule_list_lock);\n\treturn vif;\n}\n\nvoid xen_netbk_schedule_xenvif(struct xenvif *vif)\n{\n\tunsigned long flags;\n\tstruct xen_netbk *netbk = vif->netbk;\n\n\tif (__on_net_schedule_list(vif))\n\t\tgoto kick;\n\n\tspin_lock_irqsave(&netbk->net_schedule_list_lock, flags);\n\tif (!__on_net_schedule_list(vif) &&\n\t    likely(xenvif_schedulable(vif))) {\n\t\tlist_add_tail(&vif->schedule_list, &netbk->net_schedule_list);\n\t\txenvif_get(vif);\n\t}\n\tspin_unlock_irqrestore(&netbk->net_schedule_list_lock, flags);\n\nkick:\n\tsmp_mb();\n\tif ((nr_pending_reqs(netbk) < (MAX_PENDING_REQS/2)) &&\n\t    !list_empty(&netbk->net_schedule_list))\n\t\txen_netbk_kick_thread(netbk);\n}\n\nvoid xen_netbk_deschedule_xenvif(struct xenvif *vif)\n{\n\tstruct xen_netbk *netbk = vif->netbk;\n\tspin_lock_irq(&netbk->net_schedule_list_lock);\n\tremove_from_net_schedule_list(vif);\n\tspin_unlock_irq(&netbk->net_schedule_list_lock);\n}\n\nvoid xen_netbk_check_rx_xenvif(struct xenvif *vif)\n{\n\tint more_to_do;\n\n\tRING_FINAL_CHECK_FOR_REQUESTS(&vif->tx, more_to_do);\n\n\tif (more_to_do)\n\t\txen_netbk_schedule_xenvif(vif);\n}\n\nstatic void tx_add_credit(struct xenvif *vif)\n{\n\tunsigned long max_burst, max_credit;\n\n\t/*\n\t * Allow a burst big enough to transmit a jumbo packet of up to 128kB.\n\t * Otherwise the interface can seize up due to insufficient credit.\n\t */\n\tmax_burst = RING_GET_REQUEST(&vif->tx, vif->tx.req_cons)->size;\n\tmax_burst = min(max_burst, 131072UL);\n\tmax_burst = max(max_burst, vif->credit_bytes);\n\n\t/* Take care that adding a new chunk of credit doesn't wrap to zero. */\n\tmax_credit = vif->remaining_credit + vif->credit_bytes;\n\tif (max_credit < vif->remaining_credit)\n\t\tmax_credit = ULONG_MAX; /* wrapped: clamp to ULONG_MAX */\n\n\tvif->remaining_credit = min(max_credit, max_burst);\n}\n\nstatic void tx_credit_callback(unsigned long data)\n{\n\tstruct xenvif *vif = (struct xenvif *)data;\n\ttx_add_credit(vif);\n\txen_netbk_check_rx_xenvif(vif);\n}\n\nstatic void netbk_tx_err(struct xenvif *vif,\n\t\t\t struct xen_netif_tx_request *txp, RING_IDX end)\n{\n\tRING_IDX cons = vif->tx.req_cons;\n\n\tdo {\n\t\tmake_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);\n\t\tif (cons >= end)\n\t\t\tbreak;\n\t\ttxp = RING_GET_REQUEST(&vif->tx, cons++);\n\t} while (1);\n\tvif->tx.req_cons = cons;\n\txen_netbk_check_rx_xenvif(vif);\n\txenvif_put(vif);\n}\n\nstatic void netbk_fatal_tx_err(struct xenvif *vif)\n{\n\tnetdev_err(vif->dev, \"fatal error; disabling device\\n\");\n\txenvif_carrier_off(vif);\n\txenvif_put(vif);\n}\n\nstatic int netbk_count_requests(struct xenvif *vif,\n\t\t\t\tstruct xen_netif_tx_request *first,\n\t\t\t\tstruct xen_netif_tx_request *txp,\n\t\t\t\tint work_to_do)\n{\n\tRING_IDX cons = vif->tx.req_cons;\n\tint frags = 0;\n\n\tif (!(first->flags & XEN_NETTXF_more_data))\n\t\treturn 0;\n\n\tdo {\n\t\tif (frags >= work_to_do) {\n\t\t\tnetdev_err(vif->dev, \"Need more frags\\n\");\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -frags;\n\t\t}\n\n\t\tif (unlikely(frags >= MAX_SKB_FRAGS)) {\n\t\t\tnetdev_err(vif->dev, \"Too many frags\\n\");\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -frags;\n\t\t}\n\n\t\tmemcpy(txp, RING_GET_REQUEST(&vif->tx, cons + frags),\n\t\t       sizeof(*txp));\n\t\tif (txp->size > first->size) {\n\t\t\tnetdev_err(vif->dev, \"Frag is bigger than frame.\\n\");\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -frags;\n\t\t}\n\n\t\tfirst->size -= txp->size;\n\t\tfrags++;\n\n\t\tif (unlikely((txp->offset + txp->size) > PAGE_SIZE)) {\n\t\t\tnetdev_err(vif->dev, \"txp->offset: %x, size: %u\\n\",\n\t\t\t\t txp->offset, txp->size);\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -frags;\n\t\t}\n\t} while ((txp++)->flags & XEN_NETTXF_more_data);\n\treturn frags;\n}\n\nstatic struct page *xen_netbk_alloc_page(struct xen_netbk *netbk,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t u16 pending_idx)\n{\n\tstruct page *page;\n\tpage = alloc_page(GFP_KERNEL|__GFP_COLD);\n\tif (!page)\n\t\treturn NULL;\n\tset_page_ext(page, netbk, pending_idx);\n\tnetbk->mmap_pages[pending_idx] = page;\n\treturn page;\n}\n\nstatic struct gnttab_copy *xen_netbk_get_requests(struct xen_netbk *netbk,\n\t\t\t\t\t\t  struct xenvif *vif,\n\t\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t\t  struct xen_netif_tx_request *txp,\n\t\t\t\t\t\t  struct gnttab_copy *gop)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tskb_frag_t *frags = shinfo->frags;\n\tu16 pending_idx = *((u16 *)skb->data);\n\tint i, start;\n\n\t/* Skip first skb fragment if it is on same page as header fragment. */\n\tstart = (frag_get_pending_idx(&shinfo->frags[0]) == pending_idx);\n\n\tfor (i = start; i < shinfo->nr_frags; i++, txp++) {\n\t\tstruct page *page;\n\t\tpending_ring_idx_t index;\n\t\tstruct pending_tx_info *pending_tx_info =\n\t\t\tnetbk->pending_tx_info;\n\n\t\tindex = pending_index(netbk->pending_cons++);\n\t\tpending_idx = netbk->pending_ring[index];\n\t\tpage = xen_netbk_alloc_page(netbk, skb, pending_idx);\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\tgop->source.u.ref = txp->gref;\n\t\tgop->source.domid = vif->domid;\n\t\tgop->source.offset = txp->offset;\n\n\t\tgop->dest.u.gmfn = virt_to_mfn(page_address(page));\n\t\tgop->dest.domid = DOMID_SELF;\n\t\tgop->dest.offset = txp->offset;\n\n\t\tgop->len = txp->size;\n\t\tgop->flags = GNTCOPY_source_gref;\n\n\t\tgop++;\n\n\t\tmemcpy(&pending_tx_info[pending_idx].req, txp, sizeof(*txp));\n\t\txenvif_get(vif);\n\t\tpending_tx_info[pending_idx].vif = vif;\n\t\tfrag_set_pending_idx(&frags[i], pending_idx);\n\t}\n\n\treturn gop;\n}\n\nstatic int xen_netbk_tx_check_gop(struct xen_netbk *netbk,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  struct gnttab_copy **gopp)\n{\n\tstruct gnttab_copy *gop = *gopp;\n\tu16 pending_idx = *((u16 *)skb->data);\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint nr_frags = shinfo->nr_frags;\n\tint i, err, start;\n\n\t/* Check status of header. */\n\terr = gop->status;\n\tif (unlikely(err))\n\t\txen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_ERROR);\n\n\t/* Skip first skb fragment if it is on same page as header fragment. */\n\tstart = (frag_get_pending_idx(&shinfo->frags[0]) == pending_idx);\n\n\tfor (i = start; i < nr_frags; i++) {\n\t\tint j, newerr;\n\n\t\tpending_idx = frag_get_pending_idx(&shinfo->frags[i]);\n\n\t\t/* Check error status: if okay then remember grant handle. */\n\t\tnewerr = (++gop)->status;\n\t\tif (likely(!newerr)) {\n\t\t\t/* Had a previous error? Invalidate this fragment. */\n\t\t\tif (unlikely(err))\n\t\t\t\txen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Error on this fragment: respond to client with an error. */\n\t\txen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_ERROR);\n\n\t\t/* Not the first error? Preceding frags already invalidated. */\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\t/* First error: invalidate header and preceding fragments. */\n\t\tpending_idx = *((u16 *)skb->data);\n\t\txen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);\n\t\tfor (j = start; j < i; j++) {\n\t\t\tpending_idx = frag_get_pending_idx(&shinfo->frags[j]);\n\t\t\txen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);\n\t\t}\n\n\t\t/* Remember the error: invalidate all subsequent fragments. */\n\t\terr = newerr;\n\t}\n\n\t*gopp = gop + 1;\n\treturn err;\n}\n\nstatic void xen_netbk_fill_frags(struct xen_netbk *netbk, struct sk_buff *skb)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint nr_frags = shinfo->nr_frags;\n\tint i;\n\n\tfor (i = 0; i < nr_frags; i++) {\n\t\tskb_frag_t *frag = shinfo->frags + i;\n\t\tstruct xen_netif_tx_request *txp;\n\t\tstruct page *page;\n\t\tu16 pending_idx;\n\n\t\tpending_idx = frag_get_pending_idx(frag);\n\n\t\ttxp = &netbk->pending_tx_info[pending_idx].req;\n\t\tpage = virt_to_page(idx_to_kaddr(netbk, pending_idx));\n\t\t__skb_fill_page_desc(skb, i, page, txp->offset, txp->size);\n\t\tskb->len += txp->size;\n\t\tskb->data_len += txp->size;\n\t\tskb->truesize += txp->size;\n\n\t\t/* Take an extra reference to offset xen_netbk_idx_release */\n\t\tget_page(netbk->mmap_pages[pending_idx]);\n\t\txen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);\n\t}\n}\n\nstatic int xen_netbk_get_extras(struct xenvif *vif,\n\t\t\t\tstruct xen_netif_extra_info *extras,\n\t\t\t\tint work_to_do)\n{\n\tstruct xen_netif_extra_info extra;\n\tRING_IDX cons = vif->tx.req_cons;\n\n\tdo {\n\t\tif (unlikely(work_to_do-- <= 0)) {\n\t\t\tnetdev_err(vif->dev, \"Missing extra info\\n\");\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -EBADR;\n\t\t}\n\n\t\tmemcpy(&extra, RING_GET_REQUEST(&vif->tx, cons),\n\t\t       sizeof(extra));\n\t\tif (unlikely(!extra.type ||\n\t\t\t     extra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {\n\t\t\tvif->tx.req_cons = ++cons;\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"Invalid extra type: %d\\n\", extra.type);\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmemcpy(&extras[extra.type - 1], &extra, sizeof(extra));\n\t\tvif->tx.req_cons = ++cons;\n\t} while (extra.flags & XEN_NETIF_EXTRA_FLAG_MORE);\n\n\treturn work_to_do;\n}\n\nstatic int netbk_set_skb_gso(struct xenvif *vif,\n\t\t\t     struct sk_buff *skb,\n\t\t\t     struct xen_netif_extra_info *gso)\n{\n\tif (!gso->u.gso.size) {\n\t\tnetdev_err(vif->dev, \"GSO size must not be zero.\\n\");\n\t\tnetbk_fatal_tx_err(vif);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Currently only TCPv4 S.O. is supported. */\n\tif (gso->u.gso.type != XEN_NETIF_GSO_TYPE_TCPV4) {\n\t\tnetdev_err(vif->dev, \"Bad GSO type %d.\\n\", gso->u.gso.type);\n\t\tnetbk_fatal_tx_err(vif);\n\t\treturn -EINVAL;\n\t}\n\n\tskb_shinfo(skb)->gso_size = gso->u.gso.size;\n\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n\n\t/* Header must be checked, and gso_segs computed. */\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;\n\tskb_shinfo(skb)->gso_segs = 0;\n\n\treturn 0;\n}\n\nstatic int checksum_setup(struct xenvif *vif, struct sk_buff *skb)\n{\n\tstruct iphdr *iph;\n\tunsigned char *th;\n\tint err = -EPROTO;\n\tint recalculate_partial_csum = 0;\n\n\t/*\n\t * A GSO SKB must be CHECKSUM_PARTIAL. However some buggy\n\t * peers can fail to set NETRXF_csum_blank when sending a GSO\n\t * frame. In this case force the SKB to CHECKSUM_PARTIAL and\n\t * recalculate the partial checksum.\n\t */\n\tif (skb->ip_summed != CHECKSUM_PARTIAL && skb_is_gso(skb)) {\n\t\tvif->rx_gso_checksum_fixup++;\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\trecalculate_partial_csum = 1;\n\t}\n\n\t/* A non-CHECKSUM_PARTIAL SKB does not require setup. */\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (skb->protocol != htons(ETH_P_IP))\n\t\tgoto out;\n\n\tiph = (void *)skb->data;\n\tth = skb->data + 4 * iph->ihl;\n\tif (th >= skb_tail_pointer(skb))\n\t\tgoto out;\n\n\tskb->csum_start = th - skb->head;\n\tswitch (iph->protocol) {\n\tcase IPPROTO_TCP:\n\t\tskb->csum_offset = offsetof(struct tcphdr, check);\n\n\t\tif (recalculate_partial_csum) {\n\t\t\tstruct tcphdr *tcph = (struct tcphdr *)th;\n\t\t\ttcph->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr,\n\t\t\t\t\t\t\t skb->len - iph->ihl*4,\n\t\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t\t}\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\n\t\tif (recalculate_partial_csum) {\n\t\t\tstruct udphdr *udph = (struct udphdr *)th;\n\t\t\tudph->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr,\n\t\t\t\t\t\t\t skb->len - iph->ihl*4,\n\t\t\t\t\t\t\t IPPROTO_UDP, 0);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tif (net_ratelimit())\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"Attempting to checksum a non-TCP/UDP packet, dropping a protocol %d packet\\n\",\n\t\t\t\t   iph->protocol);\n\t\tgoto out;\n\t}\n\n\tif ((th + skb->csum_offset + 2) > skb_tail_pointer(skb))\n\t\tgoto out;\n\n\terr = 0;\n\nout:\n\treturn err;\n}\n\nstatic bool tx_credit_exceeded(struct xenvif *vif, unsigned size)\n{\n\tunsigned long now = jiffies;\n\tunsigned long next_credit =\n\t\tvif->credit_timeout.expires +\n\t\tmsecs_to_jiffies(vif->credit_usec / 1000);\n\n\t/* Timer could already be pending in rare cases. */\n\tif (timer_pending(&vif->credit_timeout))\n\t\treturn true;\n\n\t/* Passed the point where we can replenish credit? */\n\tif (time_after_eq(now, next_credit)) {\n\t\tvif->credit_timeout.expires = now;\n\t\ttx_add_credit(vif);\n\t}\n\n\t/* Still too big to send right now? Set a callback. */\n\tif (size > vif->remaining_credit) {\n\t\tvif->credit_timeout.data     =\n\t\t\t(unsigned long)vif;\n\t\tvif->credit_timeout.function =\n\t\t\ttx_credit_callback;\n\t\tmod_timer(&vif->credit_timeout,\n\t\t\t  next_credit);\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic unsigned xen_netbk_tx_build_gops(struct xen_netbk *netbk)\n{\n\tstruct gnttab_copy *gop = netbk->tx_copy_ops, *request_gop;\n\tstruct sk_buff *skb;\n\tint ret;\n\n\twhile (((nr_pending_reqs(netbk) + MAX_SKB_FRAGS) < MAX_PENDING_REQS) &&\n\t\t!list_empty(&netbk->net_schedule_list)) {\n\t\tstruct xenvif *vif;\n\t\tstruct xen_netif_tx_request txreq;\n\t\tstruct xen_netif_tx_request txfrags[MAX_SKB_FRAGS];\n\t\tstruct page *page;\n\t\tstruct xen_netif_extra_info extras[XEN_NETIF_EXTRA_TYPE_MAX-1];\n\t\tu16 pending_idx;\n\t\tRING_IDX idx;\n\t\tint work_to_do;\n\t\tunsigned int data_len;\n\t\tpending_ring_idx_t index;\n\n\t\t/* Get a netif from the list with work to do. */\n\t\tvif = poll_net_schedule_list(netbk);\n\t\t/* This can sometimes happen because the test of\n\t\t * list_empty(net_schedule_list) at the top of the\n\t\t * loop is unlocked.  Just go back and have another\n\t\t * look.\n\t\t */\n\t\tif (!vif)\n\t\t\tcontinue;\n\n\t\tif (vif->tx.sring->req_prod - vif->tx.req_cons >\n\t\t    XEN_NETIF_TX_RING_SIZE) {\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"Impossible number of requests. \"\n\t\t\t\t   \"req_prod %d, req_cons %d, size %ld\\n\",\n\t\t\t\t   vif->tx.sring->req_prod, vif->tx.req_cons,\n\t\t\t\t   XEN_NETIF_TX_RING_SIZE);\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\tcontinue;\n\t\t}\n\n\t\tRING_FINAL_CHECK_FOR_REQUESTS(&vif->tx, work_to_do);\n\t\tif (!work_to_do) {\n\t\t\txenvif_put(vif);\n\t\t\tcontinue;\n\t\t}\n\n\t\tidx = vif->tx.req_cons;\n\t\trmb(); /* Ensure that we see the request before we copy it. */\n\t\tmemcpy(&txreq, RING_GET_REQUEST(&vif->tx, idx), sizeof(txreq));\n\n\t\t/* Credit-based scheduling. */\n\t\tif (txreq.size > vif->remaining_credit &&\n\t\t    tx_credit_exceeded(vif, txreq.size)) {\n\t\t\txenvif_put(vif);\n\t\t\tcontinue;\n\t\t}\n\n\t\tvif->remaining_credit -= txreq.size;\n\n\t\twork_to_do--;\n\t\tvif->tx.req_cons = ++idx;\n\n\t\tmemset(extras, 0, sizeof(extras));\n\t\tif (txreq.flags & XEN_NETTXF_extra_info) {\n\t\t\twork_to_do = xen_netbk_get_extras(vif, extras,\n\t\t\t\t\t\t\t  work_to_do);\n\t\t\tidx = vif->tx.req_cons;\n\t\t\tif (unlikely(work_to_do < 0))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tret = netbk_count_requests(vif, &txreq, txfrags, work_to_do);\n\t\tif (unlikely(ret < 0))\n\t\t\tcontinue;\n\n\t\tidx += ret;\n\n\t\tif (unlikely(txreq.size < ETH_HLEN)) {\n\t\t\tnetdev_dbg(vif->dev,\n\t\t\t\t   \"Bad packet size: %d\\n\", txreq.size);\n\t\t\tnetbk_tx_err(vif, &txreq, idx);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* No crossing a page as the payload mustn't fragment. */\n\t\tif (unlikely((txreq.offset + txreq.size) > PAGE_SIZE)) {\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"txreq.offset: %x, size: %u, end: %lu\\n\",\n\t\t\t\t   txreq.offset, txreq.size,\n\t\t\t\t   (txreq.offset&~PAGE_MASK) + txreq.size);\n\t\t\tnetbk_fatal_tx_err(vif);\n\t\t\tcontinue;\n\t\t}\n\n\t\tindex = pending_index(netbk->pending_cons);\n\t\tpending_idx = netbk->pending_ring[index];\n\n\t\tdata_len = (txreq.size > PKT_PROT_LEN &&\n\t\t\t    ret < MAX_SKB_FRAGS) ?\n\t\t\tPKT_PROT_LEN : txreq.size;\n\n\t\tskb = alloc_skb(data_len + NET_SKB_PAD + NET_IP_ALIGN,\n\t\t\t\tGFP_ATOMIC | __GFP_NOWARN);\n\t\tif (unlikely(skb == NULL)) {\n\t\t\tnetdev_dbg(vif->dev,\n\t\t\t\t   \"Can't allocate a skb in start_xmit.\\n\");\n\t\t\tnetbk_tx_err(vif, &txreq, idx);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Packets passed to netif_rx() must have some headroom. */\n\t\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_GSO - 1].type) {\n\t\t\tstruct xen_netif_extra_info *gso;\n\t\t\tgso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];\n\n\t\t\tif (netbk_set_skb_gso(vif, skb, gso)) {\n\t\t\t\t/* Failure in netbk_set_skb_gso is fatal. */\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t/* XXX could copy straight to head */\n\t\tpage = xen_netbk_alloc_page(netbk, skb, pending_idx);\n\t\tif (!page) {\n\t\t\tkfree_skb(skb);\n\t\t\tnetbk_tx_err(vif, &txreq, idx);\n\t\t\tcontinue;\n\t\t}\n\n\t\tgop->source.u.ref = txreq.gref;\n\t\tgop->source.domid = vif->domid;\n\t\tgop->source.offset = txreq.offset;\n\n\t\tgop->dest.u.gmfn = virt_to_mfn(page_address(page));\n\t\tgop->dest.domid = DOMID_SELF;\n\t\tgop->dest.offset = txreq.offset;\n\n\t\tgop->len = txreq.size;\n\t\tgop->flags = GNTCOPY_source_gref;\n\n\t\tgop++;\n\n\t\tmemcpy(&netbk->pending_tx_info[pending_idx].req,\n\t\t       &txreq, sizeof(txreq));\n\t\tnetbk->pending_tx_info[pending_idx].vif = vif;\n\t\t*((u16 *)skb->data) = pending_idx;\n\n\t\t__skb_put(skb, data_len);\n\n\t\tskb_shinfo(skb)->nr_frags = ret;\n\t\tif (data_len < txreq.size) {\n\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\t\tfrag_set_pending_idx(&skb_shinfo(skb)->frags[0],\n\t\t\t\t\t     pending_idx);\n\t\t} else {\n\t\t\tfrag_set_pending_idx(&skb_shinfo(skb)->frags[0],\n\t\t\t\t\t     INVALID_PENDING_IDX);\n\t\t}\n\n\t\tnetbk->pending_cons++;\n\n\t\trequest_gop = xen_netbk_get_requests(netbk, vif,\n\t\t\t\t\t\t     skb, txfrags, gop);\n\t\tif (request_gop == NULL) {\n\t\t\tkfree_skb(skb);\n\t\t\tnetbk_tx_err(vif, &txreq, idx);\n\t\t\tcontinue;\n\t\t}\n\t\tgop = request_gop;\n\n\t\t__skb_queue_tail(&netbk->tx_queue, skb);\n\n\t\tvif->tx.req_cons = idx;\n\t\txen_netbk_check_rx_xenvif(vif);\n\n\t\tif ((gop-netbk->tx_copy_ops) >= ARRAY_SIZE(netbk->tx_copy_ops))\n\t\t\tbreak;\n\t}\n\n\treturn gop - netbk->tx_copy_ops;\n}\n\nstatic void xen_netbk_tx_submit(struct xen_netbk *netbk)\n{\n\tstruct gnttab_copy *gop = netbk->tx_copy_ops;\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue(&netbk->tx_queue)) != NULL) {\n\t\tstruct xen_netif_tx_request *txp;\n\t\tstruct xenvif *vif;\n\t\tu16 pending_idx;\n\t\tunsigned data_len;\n\n\t\tpending_idx = *((u16 *)skb->data);\n\t\tvif = netbk->pending_tx_info[pending_idx].vif;\n\t\ttxp = &netbk->pending_tx_info[pending_idx].req;\n\n\t\t/* Check the remap error code. */\n\t\tif (unlikely(xen_netbk_tx_check_gop(netbk, skb, &gop))) {\n\t\t\tnetdev_dbg(vif->dev, \"netback grant failed.\\n\");\n\t\t\tskb_shinfo(skb)->nr_frags = 0;\n\t\t\tkfree_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdata_len = skb->len;\n\t\tmemcpy(skb->data,\n\t\t       (void *)(idx_to_kaddr(netbk, pending_idx)|txp->offset),\n\t\t       data_len);\n\t\tif (data_len < txp->size) {\n\t\t\t/* Append the packet payload as a fragment. */\n\t\t\ttxp->offset += data_len;\n\t\t\ttxp->size -= data_len;\n\t\t} else {\n\t\t\t/* Schedule a response immediately. */\n\t\t\txen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);\n\t\t}\n\n\t\tif (txp->flags & XEN_NETTXF_csum_blank)\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\telse if (txp->flags & XEN_NETTXF_data_validated)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\txen_netbk_fill_frags(netbk, skb);\n\n\t\t/*\n\t\t * If the initial fragment was < PKT_PROT_LEN then\n\t\t * pull through some bytes from the other fragments to\n\t\t * increase the linear region to PKT_PROT_LEN bytes.\n\t\t */\n\t\tif (skb_headlen(skb) < PKT_PROT_LEN && skb_is_nonlinear(skb)) {\n\t\t\tint target = min_t(int, skb->len, PKT_PROT_LEN);\n\t\t\t__pskb_pull_tail(skb, target - skb_headlen(skb));\n\t\t}\n\n\t\tskb->dev      = vif->dev;\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\n\t\tif (checksum_setup(vif, skb)) {\n\t\t\tnetdev_dbg(vif->dev,\n\t\t\t\t   \"Can't setup checksum in net_tx_action\\n\");\n\t\t\tkfree_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tvif->dev->stats.rx_bytes += skb->len;\n\t\tvif->dev->stats.rx_packets++;\n\n\t\txenvif_receive_skb(vif, skb);\n\t}\n}\n\n/* Called after netfront has transmitted */\nstatic void xen_netbk_tx_action(struct xen_netbk *netbk)\n{\n\tunsigned nr_gops;\n\n\tnr_gops = xen_netbk_tx_build_gops(netbk);\n\n\tif (nr_gops == 0)\n\t\treturn;\n\n\tgnttab_batch_copy(netbk->tx_copy_ops, nr_gops);\n\n\txen_netbk_tx_submit(netbk);\n}\n\nstatic void xen_netbk_idx_release(struct xen_netbk *netbk, u16 pending_idx,\n\t\t\t\t  u8 status)\n{\n\tstruct xenvif *vif;\n\tstruct pending_tx_info *pending_tx_info;\n\tpending_ring_idx_t index;\n\n\t/* Already complete? */\n\tif (netbk->mmap_pages[pending_idx] == NULL)\n\t\treturn;\n\n\tpending_tx_info = &netbk->pending_tx_info[pending_idx];\n\n\tvif = pending_tx_info->vif;\n\n\tmake_tx_response(vif, &pending_tx_info->req, status);\n\n\tindex = pending_index(netbk->pending_prod++);\n\tnetbk->pending_ring[index] = pending_idx;\n\n\txenvif_put(vif);\n\n\tnetbk->mmap_pages[pending_idx]->mapping = 0;\n\tput_page(netbk->mmap_pages[pending_idx]);\n\tnetbk->mmap_pages[pending_idx] = NULL;\n}\n\nstatic void make_tx_response(struct xenvif *vif,\n\t\t\t     struct xen_netif_tx_request *txp,\n\t\t\t     s8       st)\n{\n\tRING_IDX i = vif->tx.rsp_prod_pvt;\n\tstruct xen_netif_tx_response *resp;\n\tint notify;\n\n\tresp = RING_GET_RESPONSE(&vif->tx, i);\n\tresp->id     = txp->id;\n\tresp->status = st;\n\n\tif (txp->flags & XEN_NETTXF_extra_info)\n\t\tRING_GET_RESPONSE(&vif->tx, ++i)->status = XEN_NETIF_RSP_NULL;\n\n\tvif->tx.rsp_prod_pvt = ++i;\n\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&vif->tx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(vif->irq);\n}\n\nstatic struct xen_netif_rx_response *make_rx_response(struct xenvif *vif,\n\t\t\t\t\t     u16      id,\n\t\t\t\t\t     s8       st,\n\t\t\t\t\t     u16      offset,\n\t\t\t\t\t     u16      size,\n\t\t\t\t\t     u16      flags)\n{\n\tRING_IDX i = vif->rx.rsp_prod_pvt;\n\tstruct xen_netif_rx_response *resp;\n\n\tresp = RING_GET_RESPONSE(&vif->rx, i);\n\tresp->offset     = offset;\n\tresp->flags      = flags;\n\tresp->id         = id;\n\tresp->status     = (s16)size;\n\tif (st < 0)\n\t\tresp->status = (s16)st;\n\n\tvif->rx.rsp_prod_pvt = ++i;\n\n\treturn resp;\n}\n\nstatic inline int rx_work_todo(struct xen_netbk *netbk)\n{\n\treturn !skb_queue_empty(&netbk->rx_queue);\n}\n\nstatic inline int tx_work_todo(struct xen_netbk *netbk)\n{\n\n\tif (((nr_pending_reqs(netbk) + MAX_SKB_FRAGS) < MAX_PENDING_REQS) &&\n\t\t\t!list_empty(&netbk->net_schedule_list))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int xen_netbk_kthread(void *data)\n{\n\tstruct xen_netbk *netbk = data;\n\twhile (!kthread_should_stop()) {\n\t\twait_event_interruptible(netbk->wq,\n\t\t\t\trx_work_todo(netbk) ||\n\t\t\t\ttx_work_todo(netbk) ||\n\t\t\t\tkthread_should_stop());\n\t\tcond_resched();\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (rx_work_todo(netbk))\n\t\t\txen_netbk_rx_action(netbk);\n\n\t\tif (tx_work_todo(netbk))\n\t\t\txen_netbk_tx_action(netbk);\n\t}\n\n\treturn 0;\n}\n\nvoid xen_netbk_unmap_frontend_rings(struct xenvif *vif)\n{\n\tif (vif->tx.sring)\n\t\txenbus_unmap_ring_vfree(xenvif_to_xenbus_device(vif),\n\t\t\t\t\tvif->tx.sring);\n\tif (vif->rx.sring)\n\t\txenbus_unmap_ring_vfree(xenvif_to_xenbus_device(vif),\n\t\t\t\t\tvif->rx.sring);\n}\n\nint xen_netbk_map_frontend_rings(struct xenvif *vif,\n\t\t\t\t grant_ref_t tx_ring_ref,\n\t\t\t\t grant_ref_t rx_ring_ref)\n{\n\tvoid *addr;\n\tstruct xen_netif_tx_sring *txs;\n\tstruct xen_netif_rx_sring *rxs;\n\n\tint err = -ENOMEM;\n\n\terr = xenbus_map_ring_valloc(xenvif_to_xenbus_device(vif),\n\t\t\t\t     tx_ring_ref, &addr);\n\tif (err)\n\t\tgoto err;\n\n\ttxs = (struct xen_netif_tx_sring *)addr;\n\tBACK_RING_INIT(&vif->tx, txs, PAGE_SIZE);\n\n\terr = xenbus_map_ring_valloc(xenvif_to_xenbus_device(vif),\n\t\t\t\t     rx_ring_ref, &addr);\n\tif (err)\n\t\tgoto err;\n\n\trxs = (struct xen_netif_rx_sring *)addr;\n\tBACK_RING_INIT(&vif->rx, rxs, PAGE_SIZE);\n\n\tvif->rx_req_cons_peek = 0;\n\n\treturn 0;\n\nerr:\n\txen_netbk_unmap_frontend_rings(vif);\n\treturn err;\n}\n\nstatic int __init netback_init(void)\n{\n\tint i;\n\tint rc = 0;\n\tint group;\n\n\tif (!xen_domain())\n\t\treturn -ENODEV;\n\n\txen_netbk_group_nr = num_online_cpus();\n\txen_netbk = vzalloc(sizeof(struct xen_netbk) * xen_netbk_group_nr);\n\tif (!xen_netbk)\n\t\treturn -ENOMEM;\n\n\tfor (group = 0; group < xen_netbk_group_nr; group++) {\n\t\tstruct xen_netbk *netbk = &xen_netbk[group];\n\t\tskb_queue_head_init(&netbk->rx_queue);\n\t\tskb_queue_head_init(&netbk->tx_queue);\n\n\t\tinit_timer(&netbk->net_timer);\n\t\tnetbk->net_timer.data = (unsigned long)netbk;\n\t\tnetbk->net_timer.function = xen_netbk_alarm;\n\n\t\tnetbk->pending_cons = 0;\n\t\tnetbk->pending_prod = MAX_PENDING_REQS;\n\t\tfor (i = 0; i < MAX_PENDING_REQS; i++)\n\t\t\tnetbk->pending_ring[i] = i;\n\n\t\tinit_waitqueue_head(&netbk->wq);\n\t\tnetbk->task = kthread_create(xen_netbk_kthread,\n\t\t\t\t\t     (void *)netbk,\n\t\t\t\t\t     \"netback/%u\", group);\n\n\t\tif (IS_ERR(netbk->task)) {\n\t\t\tprintk(KERN_ALERT \"kthread_create() fails at netback\\n\");\n\t\t\tdel_timer(&netbk->net_timer);\n\t\t\trc = PTR_ERR(netbk->task);\n\t\t\tgoto failed_init;\n\t\t}\n\n\t\tkthread_bind(netbk->task, group);\n\n\t\tINIT_LIST_HEAD(&netbk->net_schedule_list);\n\n\t\tspin_lock_init(&netbk->net_schedule_list_lock);\n\n\t\tatomic_set(&netbk->netfront_count, 0);\n\n\t\twake_up_process(netbk->task);\n\t}\n\n\trc = xenvif_xenbus_init();\n\tif (rc)\n\t\tgoto failed_init;\n\n\treturn 0;\n\nfailed_init:\n\twhile (--group >= 0) {\n\t\tstruct xen_netbk *netbk = &xen_netbk[group];\n\t\tfor (i = 0; i < MAX_PENDING_REQS; i++) {\n\t\t\tif (netbk->mmap_pages[i])\n\t\t\t\t__free_page(netbk->mmap_pages[i]);\n\t\t}\n\t\tdel_timer(&netbk->net_timer);\n\t\tkthread_stop(netbk->task);\n\t}\n\tvfree(xen_netbk);\n\treturn rc;\n\n}\n\nmodule_init(netback_init);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_ALIAS(\"xen-backend:vif\");\n"], "filenames": ["drivers/net/xen-netback/netback.c"], "buggy_code_start_loc": [150], "buggy_code_end_loc": [1546], "fixing_code_start_loc": [150], "fixing_code_end_loc": [1534], "type": "CWE-399", "message": "Memory leak in drivers/net/xen-netback/netback.c in the Xen netback functionality in the Linux kernel before 3.7.8 allows guest OS users to cause a denial of service (memory consumption) by triggering certain error conditions.", "other": {"cve": {"id": "CVE-2013-0217", "sourceIdentifier": "secalert@redhat.com", "published": "2013-02-18T04:41:50.367", "lastModified": "2023-02-13T04:38:30.597", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Memory leak in drivers/net/xen-netback/netback.c in the Xen netback functionality in the Linux kernel before 3.7.8 allows guest OS users to cause a denial of service (memory consumption) by triggering certain error conditions."}, {"lang": "es", "value": "Fuga de memoria (memory leaks) en drivers/net/xen-netback/netback.c en la funcionalidad Xen netback en el kernel de Linux anterior a v3.7.8 permite a usuarios invitados generar una denegaci\u00f3n de servicio (corrupci\u00f3n de memoria) cuando se producen determinados errores de condici\u00f3n."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:A/AC:M/Au:S/C:N/I:N/A:C", "accessVector": "ADJACENT_NETWORK", "accessComplexity": "MEDIUM", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 5.2}, "baseSeverity": "MEDIUM", "exploitabilityScore": 4.4, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-399"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.7.8", "matchCriteriaId": "D73089E5-7EBB-403A-A689-F6111BFEBCE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.35:*:*:*:*:*:*:*", "matchCriteriaId": "F72412E3-8DA9-4CC9-A426-B534202ADBA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.36:*:*:*:*:*:*:*", "matchCriteriaId": "FCAA9D7A-3C3E-4C0B-9D38-EA80E68C2E46"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.37:*:*:*:*:*:*:*", "matchCriteriaId": "4A9E3AE5-3FCF-4CBB-A30B-082BCFBFB0CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.38:*:*:*:*:*:*:*", "matchCriteriaId": "CF715657-4C3A-4392-B85D-1BBF4DE45D89"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.39:*:*:*:*:*:*:*", "matchCriteriaId": "4B63C618-AC3D-4EF7-AFDF-27B9BF482B78"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.40:*:*:*:*:*:*:*", "matchCriteriaId": "C33DA5A9-5E40-4365-9602-82FB4DCD15B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.41:*:*:*:*:*:*:*", "matchCriteriaId": "EFAFDB74-40BD-46FA-89AC-617EB2C7160B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.42:*:*:*:*:*:*:*", "matchCriteriaId": "CF5F17DA-30A7-40CF-BD7C-CEDF06D64617"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.43:*:*:*:*:*:*:*", "matchCriteriaId": "71A276F5-BD9D-4C1B-90DF-9B0C15B6F7DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.44:*:*:*:*:*:*:*", "matchCriteriaId": "F8F6EBEC-3C29-444B-BB85-6EF239B59EC1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.21:*:*:*:*:*:*:*", "matchCriteriaId": "1FF841F3-48A7-41D7-9C45-A8170435A5EB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.22:*:*:*:*:*:*:*", "matchCriteriaId": "EF506916-A6DC-4B1E-90E5-959492AF55F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.23:*:*:*:*:*:*:*", "matchCriteriaId": "B3CDAD1F-2C6A-48C0-8FAB-C2659373FA25"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.24:*:*:*:*:*:*:*", "matchCriteriaId": "4FFE4B22-C96A-43D0-B993-F51EDD9C5E0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.25:*:*:*:*:*:*:*", "matchCriteriaId": "F571CC8B-B212-4553-B463-1DB01D616E8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.26:*:*:*:*:*:*:*", "matchCriteriaId": "84E3E151-D437-48ED-A529-731EEFF88567"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.27:*:*:*:*:*:*:*", "matchCriteriaId": "E9E3EA3C-CCA5-4433-86E0-3D02C4757A0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.28:*:*:*:*:*:*:*", "matchCriteriaId": "F7AC4F7D-9FA6-4CF1-B2E9-70BF7D4D177C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.29:*:*:*:*:*:*:*", "matchCriteriaId": "3CE3A80D-9648-43CC-8F99-D741ED6552BF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.30:*:*:*:*:*:*:*", "matchCriteriaId": "C8A98C03-A465-41B4-A551-A26FEC7FFD94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "916E97D4-1FAB-42F5-826B-653B1C0909A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.5:*:*:*:*:*:*:*", "matchCriteriaId": "33FD2217-C5D0-48C1-AD74-3527127FEF9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.6:*:*:*:*:*:*:*", "matchCriteriaId": "2E92971F-B629-4E0A-9A50-8B235F9704B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.7:*:*:*:*:*:*:*", "matchCriteriaId": "EDD3A069-3829-4EE2-9D5A-29459F29D4C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.8:*:*:*:*:*:*:*", "matchCriteriaId": "A4A0964C-CEB2-41D7-A69C-1599B05B6171"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "A0F36FAC-141D-476D-84C5-A558C199F904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "51D64824-25F6-4761-BD6A-29038A143744"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "E284C8A1-740F-454D-A774-99CD3A21B594"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "C70D72AE-0CBF-4324-9935-57E28EC6279C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F674B06B-7E86-4E41-9126-8152D0DDABAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.6:*:*:*:*:*:*:*", "matchCriteriaId": "7039B3EC-8B22-413E-B582-B4BEC6181241"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.7:*:*:*:*:*:*:*", "matchCriteriaId": "35CF1DD2-80B9-4476-8963-5C3EF52B33F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.8:*:*:*:*:*:*:*", "matchCriteriaId": "BFB0B05B-A5CE-4B9C-AE7F-83062868D35B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.9:*:*:*:*:*:*:*", "matchCriteriaId": "D166A66E-7454-47EC-BB56-861A9AFEAFE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7DA94F50-2A62-4300-BF4D-A342AAE35629"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.11:*:*:*:*:*:*:*", "matchCriteriaId": "252D937B-50DC-444F-AE73-5FCF6203DF27"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.12:*:*:*:*:*:*:*", "matchCriteriaId": "F6D8EE51-02C1-47BC-A92C-0A8ABEFD28FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.13:*:*:*:*:*:*:*", "matchCriteriaId": "7F20A5D7-3B38-4911-861A-04C8310D5916"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.14:*:*:*:*:*:*:*", "matchCriteriaId": "D472DE3A-71D8-4F40-9DDE-85929A2B047D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.15:*:*:*:*:*:*:*", "matchCriteriaId": "B2AED943-65A8-4FDB-BBD0-CCEF8682A48C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.16:*:*:*:*:*:*:*", "matchCriteriaId": "D4640185-F3D8-4575-A71D-4C889A93DE2C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.17:*:*:*:*:*:*:*", "matchCriteriaId": "144CCF7C-025E-4879-B2E7-ABB8E4390BE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.18:*:*:*:*:*:*:*", "matchCriteriaId": "B6FAA052-0B2B-40CE-8C98-919B8D08A5ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.19:*:*:*:*:*:*:*", "matchCriteriaId": "4B5A53DE-9C83-4A6B-96F3-23C03BF445D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.20:*:*:*:*:*:*:*", "matchCriteriaId": "063EB879-CB05-4E33-AA90-9E43516839B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.21:*:*:*:*:*:*:*", "matchCriteriaId": "2D25764F-4B02-4C65-954E-8C7D6632DE00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.22:*:*:*:*:*:*:*", "matchCriteriaId": "F31F5BF3-CD0A-465C-857F-273841BCD28A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.23:*:*:*:*:*:*:*", "matchCriteriaId": "FF302C8A-079B-42B9-B455-CD9083BFA067"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.24:*:*:*:*:*:*:*", "matchCriteriaId": "744999C0-33D3-4363-B3DB-E0D02CDD3918"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.1:*:*:*:*:*:*:*", "matchCriteriaId": "962B0C45-AB29-4383-AC16-C6E8245D0FF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.2:*:*:*:*:*:*:*", "matchCriteriaId": "A0EE126B-74B2-4F79-BFE1-3DC169F3F9B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.3:*:*:*:*:*:*:*", "matchCriteriaId": "392075E0-A9C7-4B4A-90F9-7F1ADFF5EFA7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.4:*:*:*:*:*:*:*", "matchCriteriaId": "ECC66968-06F0-4874-A95A-A292C36E45C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.5:*:*:*:*:*:*:*", "matchCriteriaId": "5FE986E6-1068-4E1B-8EAB-DF1EAF32B4E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.6:*:*:*:*:*:*:*", "matchCriteriaId": "543E8536-1A8E-4E76-B89F-1B1F9F26FAB8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.7:*:*:*:*:*:*:*", "matchCriteriaId": "EC2B45E3-31E1-4B46-85FA-3A84E75B8F84"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.9:*:*:*:*:*:*:*", "matchCriteriaId": "5AC4A13E-F560-4D01-98A3-E2A2B82EB25B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.10:*:*:*:*:*:*:*", "matchCriteriaId": "942C462A-5398-4BB9-A792-598682E1FEF2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.11:*:*:*:*:*:*:*", "matchCriteriaId": "B852F7E0-0282-483D-BB4D-18CB7A4F1392"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7:*:*:*:*:*:*:*", "matchCriteriaId": "53ED9A31-99CC-41C8-8B72-5B2A9B49AA6C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.1:*:*:*:*:*:*:*", "matchCriteriaId": "EFD646BC-62F7-47CF-B0BE-768F701F7D9A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.2:*:*:*:*:*:*:*", "matchCriteriaId": "F43D418E-87C1-4C83-9FF1-4F45B4F452DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.3:*:*:*:*:*:*:*", "matchCriteriaId": "680D0E00-F29A-487C-8770-8E7EAC672B7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.4:*:*:*:*:*:*:*", "matchCriteriaId": "2DCA96A4-A836-4E94-A39C-3AD3EA1D9611"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.5:*:*:*:*:*:*:*", "matchCriteriaId": "753C05E3-B603-4E36-B9BA-FAEDCBF62A7D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.6:*:*:*:*:*:*:*", "matchCriteriaId": "E385C2E0-B9F1-4564-8E6D-56FD9E762405"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.7:*:*:*:*:*:*:*", "matchCriteriaId": "041335D4-05E1-4004-9381-28AAD5994B47"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=7d5145d8eb2b9791533ffe4dc003b129b9696c48", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.7.8", "source": "secalert@redhat.com"}, {"url": "http://www.mandriva.com/security/advisories?name=MDVSA-2013:176", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/02/05/12", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=910883", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/7d5145d8eb2b9791533ffe4dc003b129b9696c48", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/7d5145d8eb2b9791533ffe4dc003b129b9696c48"}}