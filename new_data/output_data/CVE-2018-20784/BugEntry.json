{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH)\n *\n *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>\n *\n *  Interactivity improvements by Mike Galbraith\n *  (C) 2007 Mike Galbraith <efault@gmx.de>\n *\n *  Various enhancements by Dmitry Adamushko.\n *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com>\n *\n *  Group scheduling enhancements by Srivatsa Vaddagiri\n *  Copyright IBM Corporation, 2007\n *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>\n *\n *  Scaled math optimizations by Thomas Gleixner\n *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de>\n *\n *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra\n *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra\n */\n#include \"sched.h\"\n\n#include <trace/events/sched.h>\n\n/*\n * Targeted preemption latency for CPU-bound tasks:\n *\n * NOTE: this latency value is not the same as the concept of\n * 'timeslice length' - timeslices in CFS are of variable length\n * and have no persistent notion like in traditional, time-slice\n * based scheduling concepts.\n *\n * (to see the precise effective timeslice length of your workload,\n *  run vmstat and monitor the context-switches (cs) field)\n *\n * (default: 6ms * (1 + ilog(ncpus)), units: nanoseconds)\n */\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nstatic unsigned int normalized_sysctl_sched_latency\t= 6000000ULL;\n\n/*\n * The initial- and re-scaling of tunables is configurable\n *\n * Options are:\n *\n *   SCHED_TUNABLESCALING_NONE - unscaled, always *1\n *   SCHED_TUNABLESCALING_LOG - scaled logarithmical, *1+ilog(ncpus)\n *   SCHED_TUNABLESCALING_LINEAR - scaled linear, *ncpus\n *\n * (default SCHED_TUNABLESCALING_LOG = *(1+ilog(ncpus))\n */\nenum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;\n\n/*\n * Minimal preemption granularity for CPU-bound tasks:\n *\n * (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds)\n */\nunsigned int sysctl_sched_min_granularity\t\t\t= 750000ULL;\nstatic unsigned int normalized_sysctl_sched_min_granularity\t= 750000ULL;\n\n/*\n * This value is kept at sysctl_sched_latency/sysctl_sched_min_granularity\n */\nstatic unsigned int sched_nr_latency = 8;\n\n/*\n * After fork, child runs first. If set to 0 (default) then\n * parent will (try to) run first.\n */\nunsigned int sysctl_sched_child_runs_first __read_mostly;\n\n/*\n * SCHED_OTHER wake-up granularity.\n *\n * This option delays the preemption effects of decoupled workloads\n * and reduces their over-scheduling. Synchronous workloads will still\n * have immediate wakeup/sleep latencies.\n *\n * (default: 1 msec * (1 + ilog(ncpus)), units: nanoseconds)\n */\nunsigned int sysctl_sched_wakeup_granularity\t\t\t= 1000000UL;\nstatic unsigned int normalized_sysctl_sched_wakeup_granularity\t= 1000000UL;\n\nconst_debug unsigned int sysctl_sched_migration_cost\t= 500000UL;\n\n#ifdef CONFIG_SMP\n/*\n * For asym packing, by default the lower numbered CPU has higher priority.\n */\nint __weak arch_asym_cpu_priority(int cpu)\n{\n\treturn -cpu;\n}\n\n/*\n * The margin used when comparing utilization with CPU capacity:\n * util * margin < capacity * 1024\n *\n * (default: ~20%)\n */\nstatic unsigned int capacity_margin\t\t\t= 1280;\n#endif\n\n#ifdef CONFIG_CFS_BANDWIDTH\n/*\n * Amount of runtime to allocate from global (tg) to local (per-cfs_rq) pool\n * each time a cfs_rq requests quota.\n *\n * Note: in the case that the slice exceeds the runtime remaining (either due\n * to consumption or the quota being specified to be smaller than the slice)\n * we will always only issue the remaining available time.\n *\n * (default: 5 msec, units: microseconds)\n */\nunsigned int sysctl_sched_cfs_bandwidth_slice\t\t= 5000UL;\n#endif\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}\n\n/*\n * Increase the granularity value when there are more CPUs,\n * because with more CPUs the 'effective latency' as visible\n * to users decreases. But the relationship is not linear,\n * so pick a second-best guess by going with the log2 of the\n * number of CPUs.\n *\n * This idea comes from the SD scheduler of Con Kolivas:\n */\nstatic unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}\n\nvoid sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}\n\n#define WMULT_CONST\t(~0U)\n#define WMULT_SHIFT\t32\n\nstatic void __update_inv_weight(struct load_weight *lw)\n{\n\tunsigned long w;\n\n\tif (likely(lw->inv_weight))\n\t\treturn;\n\n\tw = scale_load_down(lw->weight);\n\n\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\tlw->inv_weight = 1;\n\telse if (unlikely(!w))\n\t\tlw->inv_weight = WMULT_CONST;\n\telse\n\t\tlw->inv_weight = WMULT_CONST / w;\n}\n\n/*\n * delta_exec * weight / lw.weight\n *   OR\n * (delta_exec * (weight * lw->inv_weight)) >> WMULT_SHIFT\n *\n * Either weight := NICE_0_LOAD and lw \\e sched_prio_to_wmult[], in which case\n * we're guaranteed shift stays positive because inv_weight is guaranteed to\n * fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift >= 22.\n *\n * Or, weight =< lw.weight (because lw.weight is the runqueue weight), thus\n * weight/lw.weight <= 1, and therefore our shift will also be positive.\n */\nstatic u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tint shift = WMULT_SHIFT;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact >> 32)) {\n\t\twhile (fact >> 32) {\n\t\t\tfact >>= 1;\n\t\t\tshift--;\n\t\t}\n\t}\n\n\t/* hint to use a 32x32->64 mul */\n\tfact = (u64)(u32)fact * lw->inv_weight;\n\n\twhile (fact >> 32) {\n\t\tfact >>= 1;\n\t\tshift--;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}\n\n\nconst struct sched_class fair_sched_class;\n\n/**************************************************************\n * CFS operations on generic schedulable entities:\n */\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n/* cpu runqueue to which this cfs_rq is attached */\nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->rq;\n}\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\tSCHED_WARN_ON(!entity_is_task(se));\n\treturn container_of(se, struct task_struct, se);\n}\n\n/* Walk up scheduling entities hierarchy */\n#define for_each_sched_entity(se) \\\n\t\tfor (; se; se = se->parent)\n\nstatic inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn p->se.cfs_rq;\n}\n\n/* runqueue on which this entity is (to be) queued */\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\treturn se->cfs_rq;\n}\n\n/* runqueue \"owned\" by this group */\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn grp->my_q;\n}\n\nstatic inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_rq->on_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tint cpu = cpu_of(rq);\n\t\t/*\n\t\t * Ensure we either appear before our parent (if already\n\t\t * enqueued) or force our parent to appear after us when it is\n\t\t * enqueued. The fact that we always enqueue bottom-up\n\t\t * reduces this to two cases and a special case for the root\n\t\t * cfs_rq. Furthermore, it also means that we will always reset\n\t\t * tmp_alone_branch either when the branch is connected\n\t\t * to a tree or when we reach the beg of the tree\n\t\t */\n\t\tif (cfs_rq->tg->parent &&\n\t\t    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {\n\t\t\t/*\n\t\t\t * If parent is already on the list, we add the child\n\t\t\t * just before. Thanks to circular linked property of\n\t\t\t * the list, this means to put the child at the tail\n\t\t\t * of the list that starts by parent.\n\t\t\t */\n\t\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\t&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));\n\t\t\t/*\n\t\t\t * The branch is now connected to its tree so we can\n\t\t\t * reset tmp_alone_branch to the beginning of the\n\t\t\t * list.\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t} else if (!cfs_rq->tg->parent) {\n\t\t\t/*\n\t\t\t * cfs rq without parent should be put\n\t\t\t * at the tail of the list.\n\t\t\t */\n\t\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\t&rq->leaf_cfs_rq_list);\n\t\t\t/*\n\t\t\t * We have reach the beg of a tree so we can reset\n\t\t\t * tmp_alone_branch to the beginning of the list.\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t} else {\n\t\t\t/*\n\t\t\t * The parent has not already been added so we want to\n\t\t\t * make sure that it will be put after us.\n\t\t\t * tmp_alone_branch points to the beg of the branch\n\t\t\t * where we will add parent.\n\t\t\t */\n\t\t\tlist_add_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\trq->tmp_alone_branch);\n\t\t\t/*\n\t\t\t * update tmp_alone_branch to points to the new beg\n\t\t\t * of the branch\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;\n\t\t}\n\n\t\tcfs_rq->on_list = 1;\n\t}\n}\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->on_list) {\n\t\tlist_del_rcu(&cfs_rq->leaf_cfs_rq_list);\n\t\tcfs_rq->on_list = 0;\n\t}\n}\n\n/* Iterate thr' all leaf cfs_rq's on a runqueue */\n#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)\t\t\t\\\n\tlist_for_each_entry_safe(cfs_rq, pos, &rq->leaf_cfs_rq_list,\t\\\n\t\t\t\t leaf_cfs_rq_list)\n\n/* Do the two (enqueued) entities belong to the same group ? */\nstatic inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn se->parent;\n}\n\nstatic void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n\tint se_depth, pse_depth;\n\n\t/*\n\t * preemption test can be made between sibling entities who are in the\n\t * same cfs_rq i.e who have a common parent. Walk up the hierarchy of\n\t * both tasks until we find their ancestors who are siblings of common\n\t * parent.\n\t */\n\n\t/* First walk up until both entities are at same depth */\n\tse_depth = (*se)->depth;\n\tpse_depth = (*pse)->depth;\n\n\twhile (se_depth > pse_depth) {\n\t\tse_depth--;\n\t\t*se = parent_entity(*se);\n\t}\n\n\twhile (pse_depth > se_depth) {\n\t\tpse_depth--;\n\t\t*pse = parent_entity(*pse);\n\t}\n\n\twhile (!is_same_group(*se, *pse)) {\n\t\t*se = parent_entity(*se);\n\t\t*pse = parent_entity(*pse);\n\t}\n}\n\n#else\t/* !CONFIG_FAIR_GROUP_SCHED */\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}\n\nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn container_of(cfs_rq, struct rq, cfs);\n}\n\n\n#define for_each_sched_entity(se) \\\n\t\tfor (; se; se = NULL)\n\nstatic inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn &task_rq(p)->cfs;\n}\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}\n\n/* runqueue \"owned\" by this group */\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}\n\nstatic inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}\n\n#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)\t\\\n\t\tfor (cfs_rq = &rq->cfs, pos = NULL; cfs_rq; cfs_rq = pos)\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}\n\nstatic inline void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n}\n\n#endif\t/* CONFIG_FAIR_GROUP_SCHED */\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\n\n/**************************************************************\n * Scheduling class tree data structure manipulation methods:\n */\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}\n\nstatic inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}\n\nstatic void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (leftmost) { /* non-empty tree */\n\t\tstruct sched_entity *se;\n\t\tse = rb_entry(leftmost, struct sched_entity, run_node);\n\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tcfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n}\n\n/*\n * Enqueue an entity into the rb-tree:\n */\nstatic void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sched_entity *entry;\n\tbool leftmost = true;\n\n\t/*\n\t * Find the right place in the rbtree:\n\t */\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct sched_entity, run_node);\n\t\t/*\n\t\t * We dont care about collisions. Nodes with\n\t\t * the same key stay together.\n\t\t */\n\t\tif (entity_before(se, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&se->run_node, parent, link);\n\trb_insert_color_cached(&se->run_node,\n\t\t\t       &cfs_rq->tasks_timeline, leftmost);\n}\n\nstatic void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);\n}\n\nstruct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn rb_entry(left, struct sched_entity, run_node);\n}\n\nstatic struct sched_entity *__pick_next_entity(struct sched_entity *se)\n{\n\tstruct rb_node *next = rb_next(&se->run_node);\n\n\tif (!next)\n\t\treturn NULL;\n\n\treturn rb_entry(next, struct sched_entity, run_node);\n}\n\n#ifdef CONFIG_SCHED_DEBUG\nstruct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *last = rb_last(&cfs_rq->tasks_timeline.rb_root);\n\n\tif (!last)\n\t\treturn NULL;\n\n\treturn rb_entry(last, struct sched_entity, run_node);\n}\n\n/**************************************************************\n * Scheduling class statistics methods:\n */\n\nint sched_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tunsigned int factor = get_update_sysctl_factor();\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tsched_nr_latency = DIV_ROUND_UP(sysctl_sched_latency,\n\t\t\t\t\tsysctl_sched_min_granularity);\n\n#define WRT_SYSCTL(name) \\\n\t(normalized_sysctl_##name = sysctl_##name / (factor))\n\tWRT_SYSCTL(sched_min_granularity);\n\tWRT_SYSCTL(sched_latency);\n\tWRT_SYSCTL(sched_wakeup_granularity);\n#undef WRT_SYSCTL\n\n\treturn 0;\n}\n#endif\n\n/*\n * delta /= w\n */\nstatic inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}\n\n/*\n * The idea is to set a period in which each task runs once.\n *\n * When there are too many tasks (sched_nr_latency) we have to stretch\n * this period because otherwise the slices get too small.\n *\n * p = (nr <= nl) ? l : l*nr/nl\n */\nstatic u64 __sched_period(unsigned long nr_running)\n{\n\tif (unlikely(nr_running > sched_nr_latency))\n\t\treturn nr_running * sysctl_sched_min_granularity;\n\telse\n\t\treturn sysctl_sched_latency;\n}\n\n/*\n * We calculate the wall-time slice from the period by taking a part\n * proportional to the weight.\n *\n * s = p*P[w/rw]\n */\nstatic u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}\n\n/*\n * We calculate the vruntime slice of a to-be-inserted task.\n *\n * vs = s/w\n */\nstatic u64 sched_vslice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\treturn calc_delta_fair(sched_slice(cfs_rq, se), se);\n}\n\n#ifdef CONFIG_SMP\n#include \"pelt.h\"\n#include \"sched-pelt.h\"\n\nstatic int select_idle_sibling(struct task_struct *p, int prev_cpu, int cpu);\nstatic unsigned long task_h_load(struct task_struct *p);\nstatic unsigned long capacity_of(int cpu);\n\n/* Give new sched_entity start runnable values to heavy its load in infant time */\nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n\tstruct sched_avg *sa = &se->avg;\n\n\tmemset(sa, 0, sizeof(*sa));\n\n\t/*\n\t * Tasks are initialized with full load to be seen as heavy tasks until\n\t * they get a chance to stabilize to their real load level.\n\t * Group entities are initialized with zero load to reflect the fact that\n\t * nothing has been attached to the task group yet.\n\t */\n\tif (entity_is_task(se))\n\t\tsa->runnable_load_avg = sa->load_avg = scale_load_down(se->load.weight);\n\n\tse->runnable_weight = se->load.weight;\n\n\t/* when this task enqueue'ed, it will contribute to its cfs_rq's load_avg */\n}\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq);\nstatic void attach_entity_cfs_rq(struct sched_entity *se);\n\n/*\n * With new tasks being created, their initial util_avgs are extrapolated\n * based on the cfs_rq's current util_avg:\n *\n *   util_avg = cfs_rq->util_avg / (cfs_rq->load_avg + 1) * se.load.weight\n *\n * However, in many cases, the above util_avg does not give a desired\n * value. Moreover, the sum of the util_avgs may be divergent, such\n * as when the series is a harmonic series.\n *\n * To solve this problem, we also cap the util_avg of successive tasks to\n * only 1/2 of the left utilization budget:\n *\n *   util_avg_cap = (cpu_scale - cfs_rq->avg.util_avg) / 2^n\n *\n * where n denotes the nth task and cpu_scale the CPU capacity.\n *\n * For example, for a CPU with 1024 of capacity, a simplest series from\n * the beginning would be like:\n *\n *  task  util_avg: 512, 256, 128,  64,  32,   16,    8, ...\n * cfs_rq util_avg: 512, 768, 896, 960, 992, 1008, 1016, ...\n *\n * Finally, that extrapolated util_avg is clamped to the cap (util_avg_cap)\n * if util_avg > util_avg_cap.\n */\nvoid post_init_entity_util_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct sched_avg *sa = &se->avg;\n\tlong cpu_scale = arch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\tlong cap = (long)(cpu_scale - cfs_rq->avg.util_avg) / 2;\n\n\tif (cap > 0) {\n\t\tif (cfs_rq->avg.util_avg != 0) {\n\t\t\tsa->util_avg  = cfs_rq->avg.util_avg * se->load.weight;\n\t\t\tsa->util_avg /= (cfs_rq->avg.load_avg + 1);\n\n\t\t\tif (sa->util_avg > cap)\n\t\t\t\tsa->util_avg = cap;\n\t\t} else {\n\t\t\tsa->util_avg = cap;\n\t\t}\n\t}\n\n\tif (entity_is_task(se)) {\n\t\tstruct task_struct *p = task_of(se);\n\t\tif (p->sched_class != &fair_sched_class) {\n\t\t\t/*\n\t\t\t * For !fair tasks do:\n\t\t\t *\n\t\t\tupdate_cfs_rq_load_avg(now, cfs_rq);\n\t\t\tattach_entity_load_avg(cfs_rq, se, 0);\n\t\t\tswitched_from_fair(rq, p);\n\t\t\t *\n\t\t\t * such that the next switched_to_fair() has the\n\t\t\t * expected state.\n\t\t\t */\n\t\t\tse->avg.last_update_time = cfs_rq_clock_task(cfs_rq);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tattach_entity_cfs_rq(se);\n}\n\n#else /* !CONFIG_SMP */\nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n}\nvoid post_init_entity_util_avg(struct sched_entity *se)\n{\n}\nstatic void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)\n{\n}\n#endif /* CONFIG_SMP */\n\n/*\n * Update the current task's runtime statistics.\n */\nstatic void update_curr(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tu64 now = rq_clock_task(rq_of(cfs_rq));\n\tu64 delta_exec;\n\n\tif (unlikely(!curr))\n\t\treturn;\n\n\tdelta_exec = now - curr->exec_start;\n\tif (unlikely((s64)delta_exec <= 0))\n\t\treturn;\n\n\tcurr->exec_start = now;\n\n\tschedstat_set(curr->statistics.exec_max,\n\t\t      max(delta_exec, curr->statistics.exec_max));\n\n\tcurr->sum_exec_runtime += delta_exec;\n\tschedstat_add(cfs_rq->exec_clock, delta_exec);\n\n\tcurr->vruntime += calc_delta_fair(delta_exec, curr);\n\tupdate_min_vruntime(cfs_rq);\n\n\tif (entity_is_task(curr)) {\n\t\tstruct task_struct *curtask = task_of(curr);\n\n\t\ttrace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);\n\t\tcgroup_account_cputime(curtask, delta_exec);\n\t\taccount_group_exec_runtime(curtask, delta_exec);\n\t}\n\n\taccount_cfs_rq_runtime(cfs_rq, delta_exec);\n}\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}\n\nstatic inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 wait_start, prev_wait_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\twait_start = rq_clock(rq_of(cfs_rq));\n\tprev_wait_start = schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)) &&\n\t    likely(wait_start > prev_wait_start))\n\t\twait_start -= prev_wait_start;\n\n\t__schedstat_set(se->statistics.wait_start, wait_start);\n}\n\nstatic inline void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *p;\n\tu64 delta;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se)) {\n\t\tp = task_of(se);\n\t\tif (task_on_rq_migrating(p)) {\n\t\t\t/*\n\t\t\t * Preserve migrating task's wait time so wait_start\n\t\t\t * time stamp can be adjusted to accumulate wait time\n\t\t\t * prior to migration.\n\t\t\t */\n\t\t\t__schedstat_set(se->statistics.wait_start, delta);\n\t\t\treturn;\n\t\t}\n\t\ttrace_sched_stat_wait(p, delta);\n\t}\n\n\t__schedstat_set(se->statistics.wait_max,\n\t\t      max(schedstat_val(se->statistics.wait_max), delta));\n\t__schedstat_inc(se->statistics.wait_count);\n\t__schedstat_add(se->statistics.wait_sum, delta);\n\t__schedstat_set(se->statistics.wait_start, 0);\n}\n\nstatic inline void\nupdate_stats_enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *tsk = NULL;\n\tu64 sleep_start, block_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tsleep_start = schedstat_val(se->statistics.sleep_start);\n\tblock_start = schedstat_val(se->statistics.block_start);\n\n\tif (entity_is_task(se))\n\t\ttsk = task_of(se);\n\n\tif (sleep_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - sleep_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.sleep_max)))\n\t\t\t__schedstat_set(se->statistics.sleep_max, delta);\n\n\t\t__schedstat_set(se->statistics.sleep_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 1);\n\t\t\ttrace_sched_stat_sleep(tsk, delta);\n\t\t}\n\t}\n\tif (block_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - block_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.block_max)))\n\t\t\t__schedstat_set(se->statistics.block_max, delta);\n\n\t\t__schedstat_set(se->statistics.block_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\tif (tsk->in_iowait) {\n\t\t\t\t__schedstat_add(se->statistics.iowait_sum, delta);\n\t\t\t\t__schedstat_inc(se->statistics.iowait_count);\n\t\t\t\ttrace_sched_stat_iowait(tsk, delta);\n\t\t\t}\n\n\t\t\ttrace_sched_stat_blocked(tsk, delta);\n\n\t\t\t/*\n\t\t\t * Blocking time is in units of nanosecs, so shift by\n\t\t\t * 20 to get a milliseconds-range estimation of the\n\t\t\t * amount of time that the task spent sleeping:\n\t\t\t */\n\t\t\tif (unlikely(prof_on == SLEEP_PROFILING)) {\n\t\t\t\tprofile_hits(SLEEP_PROFILING,\n\t\t\t\t\t\t(void *)get_wchan(tsk),\n\t\t\t\t\t\tdelta >> 20);\n\t\t\t}\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 0);\n\t\t}\n\t}\n}\n\n/*\n * Task is being enqueued - update stats:\n */\nstatic inline void\nupdate_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Are we enqueueing a waiting task? (for current tasks\n\t * a dequeue/enqueue event is a NOP)\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_start(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tupdate_stats_enqueue_sleeper(cfs_rq, se);\n}\n\nstatic inline void\nupdate_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Mark the end of the wait period if dequeueing a\n\t * waiting task:\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\n\tif ((flags & DEQUEUE_SLEEP) && entity_is_task(se)) {\n\t\tstruct task_struct *tsk = task_of(se);\n\n\t\tif (tsk->state & TASK_INTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.sleep_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t\tif (tsk->state & TASK_UNINTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.block_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t}\n}\n\n/*\n * We are picking a new current task - update its stats:\n */\nstatic inline void\nupdate_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/*\n\t * We are starting a new run period:\n\t */\n\tse->exec_start = rq_clock_task(rq_of(cfs_rq));\n}\n\n/**************************************************\n * Scheduling class queueing methods:\n */\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Approximate time to scan a full NUMA task in ms. The task scan period is\n * calculated based on the tasks virtual memory size and\n * numa_balancing_scan_size.\n */\nunsigned int sysctl_numa_balancing_scan_period_min = 1000;\nunsigned int sysctl_numa_balancing_scan_period_max = 60000;\n\n/* Portion of address space to scan in MB */\nunsigned int sysctl_numa_balancing_scan_size = 256;\n\n/* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */\nunsigned int sysctl_numa_balancing_scan_delay = 1000;\n\nstruct numa_group {\n\tatomic_t refcount;\n\n\tspinlock_t lock; /* nr_tasks, tasks */\n\tint nr_tasks;\n\tpid_t gid;\n\tint active_nodes;\n\n\tstruct rcu_head rcu;\n\tunsigned long total_faults;\n\tunsigned long max_faults_cpu;\n\t/*\n\t * Faults_cpu is used to decide whether memory should move\n\t * towards the CPU. As a consequence, these stats are weighted\n\t * more by CPU use than by memory faults.\n\t */\n\tunsigned long *faults_cpu;\n\tunsigned long faults[0];\n};\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng);\nstatic inline unsigned long group_faults_shared(struct numa_group *ng);\n\nstatic unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t/*\n\t * Calculations based on RSS as non-present and empty pages are skipped\n\t * by the PTE scanner and NUMA hinting faults should be trapped based\n\t * on resident pages\n\t */\n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}\n\n/* For sanitys sake, never scan more PTEs than MAX_SCAN_WINDOW MB/sec. */\n#define MAX_SCAN_WINDOW 2560\n\nstatic unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}\n\nstatic unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}\n\nstatic unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}\n\nvoid init_numa_balancing(unsigned long clone_flags, struct task_struct *p)\n{\n\tint mm_users = 0;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (mm) {\n\t\tmm_users = atomic_read(&mm->mm_users);\n\t\tif (mm_users == 1) {\n\t\t\tmm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t\t\tmm->numa_scan_seq = 0;\n\t\t}\n\t}\n\tp->node_stamp\t\t\t= 0;\n\tp->numa_scan_seq\t\t= mm ? mm->numa_scan_seq : 0;\n\tp->numa_scan_period\t\t= sysctl_numa_balancing_scan_delay;\n\tp->numa_work.next\t\t= &p->numa_work;\n\tp->numa_faults\t\t\t= NULL;\n\tp->numa_group\t\t\t= NULL;\n\tp->last_task_numa_placement\t= 0;\n\tp->last_sum_exec_runtime\t= 0;\n\n\t/* New address space, reset the preferred nid */\n\tif (!(clone_flags & CLONE_VM)) {\n\t\tp->numa_preferred_nid = -1;\n\t\treturn;\n\t}\n\n\t/*\n\t * New thread, keep existing numa_preferred_nid which should be copied\n\t * already by arch_dup_task_struct but stagger when scans start.\n\t */\n\tif (mm) {\n\t\tunsigned int delay;\n\n\t\tdelay = min_t(unsigned int, task_scan_max(current),\n\t\t\tcurrent->numa_scan_period * mm_users * NSEC_PER_MSEC);\n\t\tdelay += 2 * TICK_NSEC;\n\t\tp->node_stamp = delay;\n\t}\n}\n\nstatic void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running += (p->numa_preferred_nid != -1);\n\trq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));\n}\n\nstatic void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running -= (p->numa_preferred_nid != -1);\n\trq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));\n}\n\n/* Shared or private faults. */\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\n/* Memory and CPU locality */\n#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)\n\n/* Averaged statistics, and temporary buffers. */\n#define NR_NUMA_HINT_FAULT_BUCKETS (NR_NUMA_HINT_FAULT_STATS * 2)\n\npid_t task_numa_group_id(struct task_struct *p)\n{\n\treturn p->numa_group ? p->numa_group->gid : 0;\n}\n\n/*\n * The averaged statistics, shared & private, memory & CPU,\n * occupy the first half of the array. The second half of the\n * array is for current counters, which are averaged into the\n * first set by task_numa_placement.\n */\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}\n\nstatic inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}\n\nstatic inline unsigned long group_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\treturn p->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}\n\nstatic inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}\n\n/*\n * A node triggering more than 1/3 as many NUMA faults as the maximum is\n * considered part of a numa group's pseudo-interleaving set. Migrations\n * between these nodes are slowed down, to allow things to settle down.\n */\n#define ACTIVE_NODE_FRACTION 3\n\nstatic bool numa_is_active_node(int nid, struct numa_group *ng)\n{\n\treturn group_faults_cpu(ng, nid) * ACTIVE_NODE_FRACTION > ng->max_faults_cpu;\n}\n\n/* Handle placement on systems where not all nodes are directly connected. */\nstatic unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint maxdist, bool task)\n{\n\tunsigned long score = 0;\n\tint node;\n\n\t/*\n\t * All nodes are directly connected, and the same distance\n\t * from each other. No need for fancy placement algorithms.\n\t */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t/*\n\t * This code is called for each node, introducing N^2 complexity,\n\t * which should be ok given the number of nodes rarely exceeds 8.\n\t */\n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t/*\n\t\t * The furthest away nodes in the system are not interesting\n\t\t * for placement; nid was already counted.\n\t\t */\n\t\tif (dist == sched_max_numa_distance || node == nid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * On systems with a backplane NUMA topology, compare groups\n\t\t * of nodes, and move tasks towards the group with the most\n\t\t * memory accesses. When comparing two nodes at distance\n\t\t * \"hoplimit\", only nodes closer by than \"hoplimit\" are part\n\t\t * of each group. Skip other nodes.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\tdist >= maxdist)\n\t\t\tcontinue;\n\n\t\t/* Add up the faults from nearby nodes. */\n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t/*\n\t\t * On systems with a glueless mesh NUMA topology, there are\n\t\t * no fixed \"groups of nodes\". Instead, nodes that are not\n\t\t * directly connected bounce traffic through intermediate\n\t\t * nodes; a numa_group can occupy any set of nodes.\n\t\t * The further away a node is, the less the faults count.\n\t\t * This seems to result in good task placement.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (sched_max_numa_distance - dist);\n\t\t\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}\n\n/*\n * These return the fraction of accesses done by a particular task, or\n * task group, on a particular numa node.  The group weight is given a\n * larger multiplier, in order to group tasks together that are almost\n * evenly spread out between numa nodes.\n */\nstatic inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}\n\nstatic inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}\n\nbool should_numa_migrate_memory(struct task_struct *p, struct page * page,\n\t\t\t\tint src_nid, int dst_cpu)\n{\n\tstruct numa_group *ng = p->numa_group;\n\tint dst_nid = cpu_to_node(dst_cpu);\n\tint last_cpupid, this_cpupid;\n\n\tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);\n\tlast_cpupid = page_cpupid_xchg_last(page, this_cpupid);\n\n\t/*\n\t * Allow first faults or private faults to migrate immediately early in\n\t * the lifetime of a task. The magic number 4 is based on waiting for\n\t * two full passes of the \"multi-stage node selection\" test that is\n\t * executed below.\n\t */\n\tif ((p->numa_preferred_nid == -1 || p->numa_scan_seq <= 4) &&\n\t    (cpupid_pid_unset(last_cpupid) || cpupid_match_pid(p, last_cpupid)))\n\t\treturn true;\n\n\t/*\n\t * Multi-stage node selection is used in conjunction with a periodic\n\t * migration fault to build a temporal task<->page relation. By using\n\t * a two-stage filter we remove short/unlikely relations.\n\t *\n\t * Using P(p) ~ n_p / n_t as per frequentist probability, we can equate\n\t * a task's usage of a particular page (n_p) per total usage of this\n\t * page (n_t) (in a given time-span) to a probability.\n\t *\n\t * Our periodic faults will sample this probability and getting the\n\t * same result twice in a row, given these samples are fully\n\t * independent, is then given by P(n)^2, provided our sample period\n\t * is sufficiently short compared to the usage pattern.\n\t *\n\t * This quadric squishes small probabilities, making it less likely we\n\t * act on an unlikely task<->page relation.\n\t */\n\tif (!cpupid_pid_unset(last_cpupid) &&\n\t\t\t\tcpupid_to_nid(last_cpupid) != dst_nid)\n\t\treturn false;\n\n\t/* Always allow migrate on private faults */\n\tif (cpupid_match_pid(p, last_cpupid))\n\t\treturn true;\n\n\t/* A shared fault, but p->numa_group has not been set up yet. */\n\tif (!ng)\n\t\treturn true;\n\n\t/*\n\t * Destination node is much more heavily used than the source\n\t * node? Allow migration.\n\t */\n\tif (group_faults_cpu(ng, dst_nid) > group_faults_cpu(ng, src_nid) *\n\t\t\t\t\tACTIVE_NODE_FRACTION)\n\t\treturn true;\n\n\t/*\n\t * Distribute memory according to CPU & memory use on each node,\n\t * with 3/4 hysteresis to avoid unnecessary memory migrations:\n\t *\n\t * faults_cpu(dst)   3   faults_cpu(src)\n\t * --------------- * - > ---------------\n\t * faults_mem(dst)   4   faults_mem(src)\n\t */\n\treturn group_faults_cpu(ng, dst_nid) * group_faults(p, src_nid) * 3 >\n\t       group_faults_cpu(ng, src_nid) * group_faults(p, dst_nid) * 4;\n}\n\nstatic unsigned long weighted_cpuload(struct rq *rq);\nstatic unsigned long source_load(int cpu, int type);\nstatic unsigned long target_load(int cpu, int type);\n\n/* Cached statistics for all CPUs within a node */\nstruct numa_stats {\n\tunsigned long load;\n\n\t/* Total compute capacity of CPUs on a node */\n\tunsigned long compute_capacity;\n};\n\n/*\n * XXX borrowed from update_sg_lb_stats\n */\nstatic void update_numa_stats(struct numa_stats *ns, int nid)\n{\n\tint cpu;\n\n\tmemset(ns, 0, sizeof(*ns));\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\tns->load += weighted_cpuload(rq);\n\t\tns->compute_capacity += capacity_of(cpu);\n\t}\n\n}\n\nstruct task_numa_env {\n\tstruct task_struct *p;\n\n\tint src_cpu, src_nid;\n\tint dst_cpu, dst_nid;\n\n\tstruct numa_stats src_stats, dst_stats;\n\n\tint imbalance_pct;\n\tint dist;\n\n\tstruct task_struct *best_task;\n\tlong best_imp;\n\tint best_cpu;\n};\n\nstatic void task_numa_assign(struct task_numa_env *env,\n\t\t\t     struct task_struct *p, long imp)\n{\n\tstruct rq *rq = cpu_rq(env->dst_cpu);\n\n\t/* Bail out if run-queue part of active NUMA balance. */\n\tif (xchg(&rq->numa_migrate_on, 1))\n\t\treturn;\n\n\t/*\n\t * Clear previous best_cpu/rq numa-migrate flag, since task now\n\t * found a better CPU to move/swap.\n\t */\n\tif (env->best_cpu != -1) {\n\t\trq = cpu_rq(env->best_cpu);\n\t\tWRITE_ONCE(rq->numa_migrate_on, 0);\n\t}\n\n\tif (env->best_task)\n\t\tput_task_struct(env->best_task);\n\tif (p)\n\t\tget_task_struct(p);\n\n\tenv->best_task = p;\n\tenv->best_imp = imp;\n\tenv->best_cpu = env->dst_cpu;\n}\n\nstatic bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t/*\n\t * The load is corrected for the CPU capacity available on each node.\n\t *\n\t * src_load        dst_load\n\t * ------------ vs ---------\n\t * src_capacity    dst_capacity\n\t */\n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t/* Would this change make things worse? */\n\treturn (imb > old_imb);\n}\n\n/*\n * Maximum NUMA importance can be 1998 (2*999);\n * SMALLIMP @ 30 would be close to 1998/64.\n * Used to deter task migration.\n */\n#define SMALLIMP\t30\n\n/*\n * This checks if the overall compute and NUMA accesses of the system would\n * be improved if the source tasks was migrated to the target dst_cpu taking\n * into account that it might be best if task running on the dst_cpu should\n * be exchanged with the source task\n */\nstatic void task_numa_compare(struct task_numa_env *env,\n\t\t\t      long taskimp, long groupimp, bool maymove)\n{\n\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);\n\tstruct task_struct *cur;\n\tlong src_load, dst_load;\n\tlong load;\n\tlong imp = env->p->numa_group ? groupimp : taskimp;\n\tlong moveimp = imp;\n\tint dist = env->dist;\n\n\tif (READ_ONCE(dst_rq->numa_migrate_on))\n\t\treturn;\n\n\trcu_read_lock();\n\tcur = task_rcu_dereference(&dst_rq->curr);\n\tif (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))\n\t\tcur = NULL;\n\n\t/*\n\t * Because we have preemption enabled we can get migrated around and\n\t * end try selecting ourselves (current == env->p) as a swap candidate.\n\t */\n\tif (cur == env->p)\n\t\tgoto unlock;\n\n\tif (!cur) {\n\t\tif (maymove && moveimp >= env->best_imp)\n\t\t\tgoto assign;\n\t\telse\n\t\t\tgoto unlock;\n\t}\n\n\t/*\n\t * \"imp\" is the fault differential for the source task between the\n\t * source and destination node. Calculate the total differential for\n\t * the source task and potential destination task. The more negative\n\t * the value is, the more remote accesses that would be expected to\n\t * be incurred if the tasks were swapped.\n\t */\n\t/* Skip this swap candidate if cannot move to the source cpu */\n\tif (!cpumask_test_cpu(env->src_cpu, &cur->cpus_allowed))\n\t\tgoto unlock;\n\n\t/*\n\t * If dst and source tasks are in the same NUMA group, or not\n\t * in any group then look only at task weights.\n\t */\n\tif (cur->numa_group == env->p->numa_group) {\n\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -\n\t\t      task_weight(cur, env->dst_nid, dist);\n\t\t/*\n\t\t * Add some hysteresis to prevent swapping the\n\t\t * tasks within a group over tiny differences.\n\t\t */\n\t\tif (cur->numa_group)\n\t\t\timp -= imp / 16;\n\t} else {\n\t\t/*\n\t\t * Compare the group weights. If a task is all by itself\n\t\t * (not part of a group), use the task weight instead.\n\t\t */\n\t\tif (cur->numa_group && env->p->numa_group)\n\t\t\timp += group_weight(cur, env->src_nid, dist) -\n\t\t\t       group_weight(cur, env->dst_nid, dist);\n\t\telse\n\t\t\timp += task_weight(cur, env->src_nid, dist) -\n\t\t\t       task_weight(cur, env->dst_nid, dist);\n\t}\n\n\tif (maymove && moveimp > imp && moveimp > env->best_imp) {\n\t\timp = moveimp;\n\t\tcur = NULL;\n\t\tgoto assign;\n\t}\n\n\t/*\n\t * If the NUMA importance is less than SMALLIMP,\n\t * task migration might only result in ping pong\n\t * of tasks and also hurt performance due to cache\n\t * misses.\n\t */\n\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)\n\t\tgoto unlock;\n\n\t/*\n\t * In the overloaded case, try and keep the load balanced.\n\t */\n\tload = task_h_load(env->p) - task_h_load(cur);\n\tif (!load)\n\t\tgoto assign;\n\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\tif (load_too_imbalanced(src_load, dst_load, env))\n\t\tgoto unlock;\n\nassign:\n\t/*\n\t * One idle CPU per node is evaluated for a task numa move.\n\t * Call select_idle_sibling to maybe find a better one.\n\t */\n\tif (!cur) {\n\t\t/*\n\t\t * select_idle_siblings() uses an per-CPU cpumask that\n\t\t * can be used from IRQ context.\n\t\t */\n\t\tlocal_irq_disable();\n\t\tenv->dst_cpu = select_idle_sibling(env->p, env->src_cpu,\n\t\t\t\t\t\t   env->dst_cpu);\n\t\tlocal_irq_enable();\n\t}\n\n\ttask_numa_assign(env, cur, imp);\nunlock:\n\trcu_read_unlock();\n}\n\nstatic void task_numa_find_cpu(struct task_numa_env *env,\n\t\t\t\tlong taskimp, long groupimp)\n{\n\tlong src_load, dst_load, load;\n\tbool maymove = false;\n\tint cpu;\n\n\tload = task_h_load(env->p);\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\t/*\n\t * If the improvement from just moving env->p direction is better\n\t * than swapping tasks around, check if a move is possible.\n\t */\n\tmaymove = !load_too_imbalanced(src_load, dst_load, env);\n\n\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {\n\t\t/* Skip this CPU if the source task cannot migrate */\n\t\tif (!cpumask_test_cpu(cpu, &env->p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tenv->dst_cpu = cpu;\n\t\ttask_numa_compare(env, taskimp, groupimp, maymove);\n\t}\n}\n\nstatic int task_numa_migrate(struct task_struct *p)\n{\n\tstruct task_numa_env env = {\n\t\t.p = p,\n\n\t\t.src_cpu = task_cpu(p),\n\t\t.src_nid = task_node(p),\n\n\t\t.imbalance_pct = 112,\n\n\t\t.best_task = NULL,\n\t\t.best_imp = 0,\n\t\t.best_cpu = -1,\n\t};\n\tstruct sched_domain *sd;\n\tstruct rq *best_rq;\n\tunsigned long taskweight, groupweight;\n\tint nid, ret, dist;\n\tlong taskimp, groupimp;\n\n\t/*\n\t * Pick the lowest SD_NUMA domain, as that would have the smallest\n\t * imbalance and would be the first to start moving tasks about.\n\t *\n\t * And we want to avoid any moving of tasks about, as that would create\n\t * random movement of tasks -- counter the numa conditions we're trying\n\t * to satisfy here.\n\t */\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));\n\tif (sd)\n\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;\n\trcu_read_unlock();\n\n\t/*\n\t * Cpusets can break the scheduler domain tree into smaller\n\t * balance domains, some of which do not cross NUMA boundaries.\n\t * Tasks that are \"trapped\" in such domains cannot be migrated\n\t * elsewhere, so there is no point in (re)trying.\n\t */\n\tif (unlikely(!sd)) {\n\t\tsched_setnuma(p, task_node(p));\n\t\treturn -EINVAL;\n\t}\n\n\tenv.dst_nid = p->numa_preferred_nid;\n\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);\n\ttaskweight = task_weight(p, env.src_nid, dist);\n\tgroupweight = group_weight(p, env.src_nid, dist);\n\tupdate_numa_stats(&env.src_stats, env.src_nid);\n\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;\n\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;\n\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\n\t/* Try to find a spot on the preferred nid. */\n\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\n\t/*\n\t * Look at other nodes in these cases:\n\t * - there is no space available on the preferred_nid\n\t * - the task is part of a numa_group that is interleaved across\n\t *   multiple NUMA nodes; in order to better consolidate the group,\n\t *   we need to check other locations.\n\t */\n\tif (env.best_cpu == -1 || (p->numa_group && p->numa_group->active_nodes > 1)) {\n\t\tfor_each_online_node(nid) {\n\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)\n\t\t\t\tcontinue;\n\n\t\t\tdist = node_distance(env.src_nid, env.dst_nid);\n\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\t\tdist != env.dist) {\n\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);\n\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);\n\t\t\t}\n\n\t\t\t/* Only consider nodes where both task and groups benefit */\n\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;\n\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;\n\t\t\tif (taskimp < 0 && groupimp < 0)\n\t\t\t\tcontinue;\n\n\t\t\tenv.dist = dist;\n\t\t\tenv.dst_nid = nid;\n\t\t\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\t\t}\n\t}\n\n\t/*\n\t * If the task is part of a workload that spans multiple NUMA nodes,\n\t * and is migrating into one of the workload's active nodes, remember\n\t * this node as the task's preferred numa node, so the workload can\n\t * settle down.\n\t * A task that migrated to a second choice node will be better off\n\t * trying for a better one later. Do not set the preferred node here.\n\t */\n\tif (p->numa_group) {\n\t\tif (env.best_cpu == -1)\n\t\t\tnid = env.src_nid;\n\t\telse\n\t\t\tnid = cpu_to_node(env.best_cpu);\n\n\t\tif (nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, nid);\n\t}\n\n\t/* No better CPU than the current one was found. */\n\tif (env.best_cpu == -1)\n\t\treturn -EAGAIN;\n\n\tbest_rq = cpu_rq(env.best_cpu);\n\tif (env.best_task == NULL) {\n\t\tret = migrate_task_to(p, env.best_cpu);\n\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\t\tif (ret != 0)\n\t\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_cpu);\n\t\treturn ret;\n\t}\n\n\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);\n\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\n\tif (ret != 0)\n\t\ttrace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));\n\tput_task_struct(env.best_task);\n\treturn ret;\n}\n\n/* Attempt to migrate a task to a CPU on the preferred node. */\nstatic void numa_migrate_preferred(struct task_struct *p)\n{\n\tunsigned long interval = HZ;\n\n\t/* This task has no NUMA fault statistics yet */\n\tif (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults))\n\t\treturn;\n\n\t/* Periodically retry migrating the task to the preferred node */\n\tinterval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);\n\tp->numa_migrate_retry = jiffies + interval;\n\n\t/* Success if task is already running on preferred CPU */\n\tif (task_node(p) == p->numa_preferred_nid)\n\t\treturn;\n\n\t/* Otherwise, try migrate to a CPU on the preferred node */\n\ttask_numa_migrate(p);\n}\n\n/*\n * Find out how many nodes on the workload is actively running on. Do this by\n * tracking the nodes from which NUMA hinting faults are triggered. This can\n * be different from the set of nodes where the workload's memory is currently\n * located.\n */\nstatic void numa_group_count_active_nodes(struct numa_group *numa_group)\n{\n\tunsigned long faults, max_faults = 0;\n\tint nid, active_nodes = 0;\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults > max_faults)\n\t\t\tmax_faults = faults;\n\t}\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults * ACTIVE_NODE_FRACTION > max_faults)\n\t\t\tactive_nodes++;\n\t}\n\n\tnuma_group->max_faults_cpu = max_faults;\n\tnuma_group->active_nodes = active_nodes;\n}\n\n/*\n * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS\n * increments. The more local the fault statistics are, the higher the scan\n * period will be for the next scan window. If local/(local+remote) ratio is\n * below NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS)\n * the scan period will decrease. Aim for 70% local accesses.\n */\n#define NUMA_PERIOD_SLOTS 10\n#define NUMA_PERIOD_THRESHOLD 7\n\n/*\n * Increase the scan period (slow down scanning) if the majority of\n * our memory is already on our local node, or if the majority of\n * the page accesses are shared with other processes.\n * Otherwise, decrease the scan period.\n */\nstatic void update_task_scan_period(struct task_struct *p,\n\t\t\tunsigned long shared, unsigned long private)\n{\n\tunsigned int period_slot;\n\tint lr_ratio, ps_ratio;\n\tint diff;\n\n\tunsigned long remote = p->numa_faults_locality[0];\n\tunsigned long local = p->numa_faults_locality[1];\n\n\t/*\n\t * If there were no record hinting faults then either the task is\n\t * completely idle or all activity is areas that are not of interest\n\t * to automatic numa balancing. Related to that, if there were failed\n\t * migration then it implies we are migrating too quickly or the local\n\t * node is overloaded. In either case, scan slower\n\t */\n\tif (local + shared == 0 || p->numa_faults_locality[2]) {\n\t\tp->numa_scan_period = min(p->numa_scan_period_max,\n\t\t\tp->numa_scan_period << 1);\n\n\t\tp->mm->numa_next_scan = jiffies +\n\t\t\tmsecs_to_jiffies(p->numa_scan_period);\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Prepare to scale scan period relative to the current period.\n\t *\t == NUMA_PERIOD_THRESHOLD scan period stays the same\n\t *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)\n\t *\t >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)\n\t */\n\tperiod_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);\n\tlr_ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);\n\tps_ratio = (private * NUMA_PERIOD_SLOTS) / (private + shared);\n\n\tif (ps_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are local. There is no need to\n\t\t * do fast NUMA scanning, since memory is already local.\n\t\t */\n\t\tint slot = ps_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else if (lr_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are shared with other tasks.\n\t\t * There is no point in continuing fast NUMA scanning,\n\t\t * since other tasks may just move the memory elsewhere.\n\t\t */\n\t\tint slot = lr_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else {\n\t\t/*\n\t\t * Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS,\n\t\t * yet they are not on the local NUMA node. Speed up\n\t\t * NUMA scanning to get the memory moved over.\n\t\t */\n\t\tint ratio = max(lr_ratio, ps_ratio);\n\t\tdiff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;\n\t}\n\n\tp->numa_scan_period = clamp(p->numa_scan_period + diff,\n\t\t\ttask_scan_min(p), task_scan_max(p));\n\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n}\n\n/*\n * Get the fraction of time the task has been running since the last\n * NUMA placement cycle. The scheduler keeps similar statistics, but\n * decays those on a 32ms period, which is orders of magnitude off\n * from the dozens-of-seconds NUMA balancing period. Use the scheduler\n * stats only if the task is so new there are no NUMA statistics yet.\n */\nstatic u64 numa_get_avg_runtime(struct task_struct *p, u64 *period)\n{\n\tu64 runtime, delta, now;\n\t/* Use the start of this time slice to avoid calculations. */\n\tnow = p->se.exec_start;\n\truntime = p->se.sum_exec_runtime;\n\n\tif (p->last_task_numa_placement) {\n\t\tdelta = runtime - p->last_sum_exec_runtime;\n\t\t*period = now - p->last_task_numa_placement;\n\t} else {\n\t\tdelta = p->se.avg.load_sum;\n\t\t*period = LOAD_AVG_MAX;\n\t}\n\n\tp->last_sum_exec_runtime = runtime;\n\tp->last_task_numa_placement = now;\n\n\treturn delta;\n}\n\n/*\n * Determine the preferred nid for a task in a numa_group. This needs to\n * be done in a way that produces consistent results with group_weight,\n * otherwise workloads might not converge.\n */\nstatic int preferred_group_nid(struct task_struct *p, int nid)\n{\n\tnodemask_t nodes;\n\tint dist;\n\n\t/* Direct connections between all NUMA nodes. */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn nid;\n\n\t/*\n\t * On a system with glueless mesh NUMA topology, group_weight\n\t * scores nodes according to the number of NUMA hinting faults on\n\t * both the node itself, and on nearby nodes.\n\t */\n\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\tunsigned long score, max_score = 0;\n\t\tint node, max_node = nid;\n\n\t\tdist = sched_max_numa_distance;\n\n\t\tfor_each_online_node(node) {\n\t\t\tscore = group_weight(p, node, dist);\n\t\t\tif (score > max_score) {\n\t\t\t\tmax_score = score;\n\t\t\t\tmax_node = node;\n\t\t\t}\n\t\t}\n\t\treturn max_node;\n\t}\n\n\t/*\n\t * Finding the preferred nid in a system with NUMA backplane\n\t * interconnect topology is more involved. The goal is to locate\n\t * tasks from numa_groups near each other in the system, and\n\t * untangle workloads from different sides of the system. This requires\n\t * searching down the hierarchy of node groups, recursively searching\n\t * inside the highest scoring group of nodes. The nodemask tricks\n\t * keep the complexity of the search down.\n\t */\n\tnodes = node_online_map;\n\tfor (dist = sched_max_numa_distance; dist > LOCAL_DISTANCE; dist--) {\n\t\tunsigned long max_faults = 0;\n\t\tnodemask_t max_group = NODE_MASK_NONE;\n\t\tint a, b;\n\n\t\t/* Are there nodes at this distance from each other? */\n\t\tif (!find_numa_distance(dist))\n\t\t\tcontinue;\n\n\t\tfor_each_node_mask(a, nodes) {\n\t\t\tunsigned long faults = 0;\n\t\t\tnodemask_t this_group;\n\t\t\tnodes_clear(this_group);\n\n\t\t\t/* Sum group's NUMA faults; includes a==b case. */\n\t\t\tfor_each_node_mask(b, nodes) {\n\t\t\t\tif (node_distance(a, b) < dist) {\n\t\t\t\t\tfaults += group_faults(p, b);\n\t\t\t\t\tnode_set(b, this_group);\n\t\t\t\t\tnode_clear(b, nodes);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Remember the top group. */\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_group = this_group;\n\t\t\t\t/*\n\t\t\t\t * subtle: at the smallest distance there is\n\t\t\t\t * just one node left in each \"group\", the\n\t\t\t\t * winner is the preferred nid.\n\t\t\t\t */\n\t\t\t\tnid = a;\n\t\t\t}\n\t\t}\n\t\t/* Next round, evaluate the nodes within max_group. */\n\t\tif (!max_faults)\n\t\t\tbreak;\n\t\tnodes = max_group;\n\t}\n\treturn nid;\n}\n\nstatic void task_numa_placement(struct task_struct *p)\n{\n\tint seq, nid, max_nid = -1;\n\tunsigned long max_faults = 0;\n\tunsigned long fault_types[2] = { 0, 0 };\n\tunsigned long total_faults;\n\tu64 runtime, period;\n\tspinlock_t *group_lock = NULL;\n\n\t/*\n\t * The p->mm->numa_scan_seq field gets updated without\n\t * exclusive access. Use READ_ONCE() here to ensure\n\t * that the field is read in a single access:\n\t */\n\tseq = READ_ONCE(p->mm->numa_scan_seq);\n\tif (p->numa_scan_seq == seq)\n\t\treturn;\n\tp->numa_scan_seq = seq;\n\tp->numa_scan_period_max = task_scan_max(p);\n\n\ttotal_faults = p->numa_faults_locality[0] +\n\t\t       p->numa_faults_locality[1];\n\truntime = numa_get_avg_runtime(p, &period);\n\n\t/* If the task is part of a group prevent parallel updates to group stats */\n\tif (p->numa_group) {\n\t\tgroup_lock = &p->numa_group->lock;\n\t\tspin_lock_irq(group_lock);\n\t}\n\n\t/* Find the node with the highest number of faults */\n\tfor_each_online_node(nid) {\n\t\t/* Keep track of the offsets in numa_faults array */\n\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;\n\t\tunsigned long faults = 0, group_faults = 0;\n\t\tint priv;\n\n\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {\n\t\t\tlong diff, f_diff, f_weight;\n\n\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);\n\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);\n\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);\n\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);\n\n\t\t\t/* Decay existing window, copy faults since last scan */\n\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;\n\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];\n\t\t\tp->numa_faults[membuf_idx] = 0;\n\n\t\t\t/*\n\t\t\t * Normalize the faults_from, so all tasks in a group\n\t\t\t * count according to CPU use, instead of by the raw\n\t\t\t * number of faults. Tasks with little runtime have\n\t\t\t * little over-all impact on throughput, and thus their\n\t\t\t * faults are less important.\n\t\t\t */\n\t\t\tf_weight = div64_u64(runtime << 16, period + 1);\n\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /\n\t\t\t\t   (total_faults + 1);\n\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;\n\t\t\tp->numa_faults[cpubuf_idx] = 0;\n\n\t\t\tp->numa_faults[mem_idx] += diff;\n\t\t\tp->numa_faults[cpu_idx] += f_diff;\n\t\t\tfaults += p->numa_faults[mem_idx];\n\t\t\tp->total_numa_faults += diff;\n\t\t\tif (p->numa_group) {\n\t\t\t\t/*\n\t\t\t\t * safe because we can only change our own group\n\t\t\t\t *\n\t\t\t\t * mem_idx represents the offset for a given\n\t\t\t\t * nid and priv in a specific region because it\n\t\t\t\t * is at the beginning of the numa_faults array.\n\t\t\t\t */\n\t\t\t\tp->numa_group->faults[mem_idx] += diff;\n\t\t\t\tp->numa_group->faults_cpu[mem_idx] += f_diff;\n\t\t\t\tp->numa_group->total_faults += diff;\n\t\t\t\tgroup_faults += p->numa_group->faults[mem_idx];\n\t\t\t}\n\t\t}\n\n\t\tif (!p->numa_group) {\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_nid = nid;\n\t\t\t}\n\t\t} else if (group_faults > max_faults) {\n\t\t\tmax_faults = group_faults;\n\t\t\tmax_nid = nid;\n\t\t}\n\t}\n\n\tif (p->numa_group) {\n\t\tnuma_group_count_active_nodes(p->numa_group);\n\t\tspin_unlock_irq(group_lock);\n\t\tmax_nid = preferred_group_nid(p, max_nid);\n\t}\n\n\tif (max_faults) {\n\t\t/* Set the new preferred node */\n\t\tif (max_nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, max_nid);\n\t}\n\n\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);\n}\n\nstatic inline int get_numa_group(struct numa_group *grp)\n{\n\treturn atomic_inc_not_zero(&grp->refcount);\n}\n\nstatic inline void put_numa_group(struct numa_group *grp)\n{\n\tif (atomic_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}\n\nstatic void task_numa_group(struct task_struct *p, int cpupid, int flags,\n\t\t\tint *priv)\n{\n\tstruct numa_group *grp, *my_grp;\n\tstruct task_struct *tsk;\n\tbool join = false;\n\tint cpu = cpupid_to_cpu(cpupid);\n\tint i;\n\n\tif (unlikely(!p->numa_group)) {\n\t\tunsigned int size = sizeof(struct numa_group) +\n\t\t\t\t    4*nr_node_ids*sizeof(unsigned long);\n\n\t\tgrp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!grp)\n\t\t\treturn;\n\n\t\tatomic_set(&grp->refcount, 1);\n\t\tgrp->active_nodes = 1;\n\t\tgrp->max_faults_cpu = 0;\n\t\tspin_lock_init(&grp->lock);\n\t\tgrp->gid = p->pid;\n\t\t/* Second half of the array tracks nids where faults happen */\n\t\tgrp->faults_cpu = grp->faults + NR_NUMA_HINT_FAULT_TYPES *\n\t\t\t\t\t\tnr_node_ids;\n\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] = p->numa_faults[i];\n\n\t\tgrp->total_faults = p->total_numa_faults;\n\n\t\tgrp->nr_tasks++;\n\t\trcu_assign_pointer(p->numa_group, grp);\n\t}\n\n\trcu_read_lock();\n\ttsk = READ_ONCE(cpu_rq(cpu)->curr);\n\n\tif (!cpupid_match_pid(tsk, cpupid))\n\t\tgoto no_join;\n\n\tgrp = rcu_dereference(tsk->numa_group);\n\tif (!grp)\n\t\tgoto no_join;\n\n\tmy_grp = p->numa_group;\n\tif (grp == my_grp)\n\t\tgoto no_join;\n\n\t/*\n\t * Only join the other group if its bigger; if we're the bigger group,\n\t * the other task will join us.\n\t */\n\tif (my_grp->nr_tasks > grp->nr_tasks)\n\t\tgoto no_join;\n\n\t/*\n\t * Tie-break on the grp address.\n\t */\n\tif (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)\n\t\tgoto no_join;\n\n\t/* Always join threads in the same process. */\n\tif (tsk->mm == current->mm)\n\t\tjoin = true;\n\n\t/* Simple filter to avoid false positives due to PID collisions */\n\tif (flags & TNF_SHARED)\n\t\tjoin = true;\n\n\t/* Update priv based on whether false sharing was detected */\n\t*priv = !join;\n\n\tif (join && !get_numa_group(grp))\n\t\tgoto no_join;\n\n\trcu_read_unlock();\n\n\tif (!join)\n\t\treturn;\n\n\tBUG_ON(irqs_disabled());\n\tdouble_lock_irq(&my_grp->lock, &grp->lock);\n\n\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) {\n\t\tmy_grp->faults[i] -= p->numa_faults[i];\n\t\tgrp->faults[i] += p->numa_faults[i];\n\t}\n\tmy_grp->total_faults -= p->total_numa_faults;\n\tgrp->total_faults += p->total_numa_faults;\n\n\tmy_grp->nr_tasks--;\n\tgrp->nr_tasks++;\n\n\tspin_unlock(&my_grp->lock);\n\tspin_unlock_irq(&grp->lock);\n\n\trcu_assign_pointer(p->numa_group, grp);\n\n\tput_numa_group(my_grp);\n\treturn;\n\nno_join:\n\trcu_read_unlock();\n\treturn;\n}\n\nvoid task_numa_free(struct task_struct *p)\n{\n\tstruct numa_group *grp = p->numa_group;\n\tvoid *numa_faults = p->numa_faults;\n\tunsigned long flags;\n\tint i;\n\n\tif (grp) {\n\t\tspin_lock_irqsave(&grp->lock, flags);\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] -= p->numa_faults[i];\n\t\tgrp->total_faults -= p->total_numa_faults;\n\n\t\tgrp->nr_tasks--;\n\t\tspin_unlock_irqrestore(&grp->lock, flags);\n\t\tRCU_INIT_POINTER(p->numa_group, NULL);\n\t\tput_numa_group(grp);\n\t}\n\n\tp->numa_faults = NULL;\n\tkfree(numa_faults);\n}\n\n/*\n * Got a PROT_NONE fault for a page on @node.\n */\nvoid task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)\n{\n\tstruct task_struct *p = current;\n\tbool migrated = flags & TNF_MIGRATED;\n\tint cpu_node = task_node(current);\n\tint local = !!(flags & TNF_FAULT_LOCAL);\n\tstruct numa_group *ng;\n\tint priv;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\t/* for example, ksmd faulting in a user's mm */\n\tif (!p->mm)\n\t\treturn;\n\n\t/* Allocate buffer to track faults on a per-node basis */\n\tif (unlikely(!p->numa_faults)) {\n\t\tint size = sizeof(*p->numa_faults) *\n\t\t\t   NR_NUMA_HINT_FAULT_BUCKETS * nr_node_ids;\n\n\t\tp->numa_faults = kzalloc(size, GFP_KERNEL|__GFP_NOWARN);\n\t\tif (!p->numa_faults)\n\t\t\treturn;\n\n\t\tp->total_numa_faults = 0;\n\t\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n\t}\n\n\t/*\n\t * First accesses are treated as private, otherwise consider accesses\n\t * to be private if the accessing pid has not changed\n\t */\n\tif (unlikely(last_cpupid == (-1 & LAST_CPUPID_MASK))) {\n\t\tpriv = 1;\n\t} else {\n\t\tpriv = cpupid_match_pid(p, last_cpupid);\n\t\tif (!priv && !(flags & TNF_NO_GROUP))\n\t\t\ttask_numa_group(p, last_cpupid, flags, &priv);\n\t}\n\n\t/*\n\t * If a workload spans multiple NUMA nodes, a shared fault that\n\t * occurs wholly within the set of nodes that the workload is\n\t * actively using should be counted as local. This allows the\n\t * scan rate to slow down when a workload has settled down.\n\t */\n\tng = p->numa_group;\n\tif (!priv && !local && ng && ng->active_nodes > 1 &&\n\t\t\t\tnuma_is_active_node(cpu_node, ng) &&\n\t\t\t\tnuma_is_active_node(mem_node, ng))\n\t\tlocal = 1;\n\n\t/*\n\t * Retry to migrate task to preferred node periodically, in case it\n\t * previously failed, or the scheduler moved us.\n\t */\n\tif (time_after(jiffies, p->numa_migrate_retry)) {\n\t\ttask_numa_placement(p);\n\t\tnuma_migrate_preferred(p);\n\t}\n\n\tif (migrated)\n\t\tp->numa_pages_migrated += pages;\n\tif (flags & TNF_MIGRATE_FAIL)\n\t\tp->numa_faults_locality[2] += pages;\n\n\tp->numa_faults[task_faults_idx(NUMA_MEMBUF, mem_node, priv)] += pages;\n\tp->numa_faults[task_faults_idx(NUMA_CPUBUF, cpu_node, priv)] += pages;\n\tp->numa_faults_locality[local] += pages;\n}\n\nstatic void reset_ptenuma_scan(struct task_struct *p)\n{\n\t/*\n\t * We only did a read acquisition of the mmap sem, so\n\t * p->mm->numa_scan_seq is written to without exclusive access\n\t * and the update is not guaranteed to be atomic. That's not\n\t * much of an issue though, since this is just used for\n\t * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not\n\t * expensive, to avoid any form of compiler optimizations:\n\t */\n\tWRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);\n\tp->mm->numa_scan_offset = 0;\n}\n\n/*\n * The expensive part of numa migration is done from task_work context.\n * Triggered from task_tick_numa().\n */\nvoid task_numa_work(struct callback_head *work)\n{\n\tunsigned long migrate, next_scan, now = jiffies;\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tu64 runtime = p->se.sum_exec_runtime;\n\tstruct vm_area_struct *vma;\n\tunsigned long start, end;\n\tunsigned long nr_pte_updates = 0;\n\tlong pages, virtpages;\n\n\tSCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));\n\n\twork->next = work; /* protect against double add */\n\t/*\n\t * Who cares about NUMA placement when they're dying.\n\t *\n\t * NOTE: make sure not to dereference p->mm before this check,\n\t * exit_task_work() happens _after_ exit_mm() so we could be called\n\t * without p->mm even though we still had it when we enqueued this\n\t * work.\n\t */\n\tif (p->flags & PF_EXITING)\n\t\treturn;\n\n\tif (!mm->numa_next_scan) {\n\t\tmm->numa_next_scan = now +\n\t\t\tmsecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t}\n\n\t/*\n\t * Enforce maximal scan/migration frequency..\n\t */\n\tmigrate = mm->numa_next_scan;\n\tif (time_before(now, migrate))\n\t\treturn;\n\n\tif (p->numa_scan_period == 0) {\n\t\tp->numa_scan_period_max = task_scan_max(p);\n\t\tp->numa_scan_period = task_scan_start(p);\n\t}\n\n\tnext_scan = now + msecs_to_jiffies(p->numa_scan_period);\n\tif (cmpxchg(&mm->numa_next_scan, migrate, next_scan) != migrate)\n\t\treturn;\n\n\t/*\n\t * Delay this task enough that another task of this mm will likely win\n\t * the next time around.\n\t */\n\tp->node_stamp += 2 * TICK_NSEC;\n\n\tstart = mm->numa_scan_offset;\n\tpages = sysctl_numa_balancing_scan_size;\n\tpages <<= 20 - PAGE_SHIFT; /* MB in pages */\n\tvirtpages = pages * 8;\t   /* Scan up to this much virtual space */\n\tif (!pages)\n\t\treturn;\n\n\n\tif (!down_read_trylock(&mm->mmap_sem))\n\t\treturn;\n\tvma = find_vma(mm, start);\n\tif (!vma) {\n\t\treset_ptenuma_scan(p);\n\t\tstart = 0;\n\t\tvma = mm->mmap;\n\t}\n\tfor (; vma; vma = vma->vm_next) {\n\t\tif (!vma_migratable(vma) || !vma_policy_mof(vma) ||\n\t\t\tis_vm_hugetlb_page(vma) || (vma->vm_flags & VM_MIXEDMAP)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Shared library pages mapped by multiple processes are not\n\t\t * migrated as it is expected they are cache replicated. Avoid\n\t\t * hinting faults in read-only file-backed mappings or the vdso\n\t\t * as migrating the pages will be of marginal benefit.\n\t\t */\n\t\tif (!vma->vm_mm ||\n\t\t    (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Skip inaccessible VMAs to avoid any confusion between\n\t\t * PROT_NONE and NUMA hinting ptes\n\t\t */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tcontinue;\n\n\t\tdo {\n\t\t\tstart = max(start, vma->vm_start);\n\t\t\tend = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);\n\t\t\tend = min(end, vma->vm_end);\n\t\t\tnr_pte_updates = change_prot_numa(vma, start, end);\n\n\t\t\t/*\n\t\t\t * Try to scan sysctl_numa_balancing_size worth of\n\t\t\t * hpages that have at least one present PTE that\n\t\t\t * is not already pte-numa. If the VMA contains\n\t\t\t * areas that are unused or already full of prot_numa\n\t\t\t * PTEs, scan up to virtpages, to skip through those\n\t\t\t * areas faster.\n\t\t\t */\n\t\t\tif (nr_pte_updates)\n\t\t\t\tpages -= (end - start) >> PAGE_SHIFT;\n\t\t\tvirtpages -= (end - start) >> PAGE_SHIFT;\n\n\t\t\tstart = end;\n\t\t\tif (pages <= 0 || virtpages <= 0)\n\t\t\t\tgoto out;\n\n\t\t\tcond_resched();\n\t\t} while (end != vma->vm_end);\n\t}\n\nout:\n\t/*\n\t * It is possible to reach the end of the VMA list but the last few\n\t * VMAs are not guaranteed to the vma_migratable. If they are not, we\n\t * would find the !migratable VMA on the next scan but not reset the\n\t * scanner to the start so check it now.\n\t */\n\tif (vma)\n\t\tmm->numa_scan_offset = start;\n\telse\n\t\treset_ptenuma_scan(p);\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Make sure tasks use at least 32x as much time to run other code\n\t * than they used here, to limit NUMA PTE scanning overhead to 3% max.\n\t * Usually update_task_scan_period slows down scanning enough; on an\n\t * overloaded system we need to limit overhead on a per task basis.\n\t */\n\tif (unlikely(p->se.sum_exec_runtime != runtime)) {\n\t\tu64 diff = p->se.sum_exec_runtime - runtime;\n\t\tp->node_stamp += 32 * diff;\n\t}\n}\n\n/*\n * Drive the periodic memory faults..\n */\nvoid task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n\tstruct callback_head *work = &curr->numa_work;\n\tu64 period, now;\n\n\t/*\n\t * We don't care about NUMA placement if we don't have memory.\n\t */\n\tif (!curr->mm || (curr->flags & PF_EXITING) || work->next != work)\n\t\treturn;\n\n\t/*\n\t * Using runtime rather than walltime has the dual advantage that\n\t * we (mostly) drive the selection from busy threads and that the\n\t * task needs to have done some actual work before we bother with\n\t * NUMA placement.\n\t */\n\tnow = curr->se.sum_exec_runtime;\n\tperiod = (u64)curr->numa_scan_period * NSEC_PER_MSEC;\n\n\tif (now > curr->node_stamp + period) {\n\t\tif (!curr->node_stamp)\n\t\t\tcurr->numa_scan_period = task_scan_start(curr);\n\t\tcurr->node_stamp += period;\n\n\t\tif (!time_before(jiffies, curr->mm->numa_next_scan)) {\n\t\t\tinit_task_work(work, task_numa_work); /* TODO: move this into sched_fork() */\n\t\t\ttask_work_add(curr, work, true);\n\t\t}\n\t}\n}\n\nstatic void update_scan_period(struct task_struct *p, int new_cpu)\n{\n\tint src_nid = cpu_to_node(task_cpu(p));\n\tint dst_nid = cpu_to_node(new_cpu);\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\tif (!p->mm || !p->numa_faults || (p->flags & PF_EXITING))\n\t\treturn;\n\n\tif (src_nid == dst_nid)\n\t\treturn;\n\n\t/*\n\t * Allow resets if faults have been trapped before one scan\n\t * has completed. This is most likely due to a new task that\n\t * is pulled cross-node due to wakeups or load balancing.\n\t */\n\tif (p->numa_scan_seq) {\n\t\t/*\n\t\t * Avoid scan adjustments if moving to the preferred\n\t\t * node or if the task was not previously running on\n\t\t * the preferred node.\n\t\t */\n\t\tif (dst_nid == p->numa_preferred_nid ||\n\t\t    (p->numa_preferred_nid != -1 && src_nid != p->numa_preferred_nid))\n\t\t\treturn;\n\t}\n\n\tp->numa_scan_period = task_scan_start(p);\n}\n\n#else\nstatic void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n}\n\nstatic inline void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void update_scan_period(struct task_struct *p, int new_cpu)\n{\n}\n\n#endif /* CONFIG_NUMA_BALANCING */\n\nstatic void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_add(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n}\n\nstatic void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_sub(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n}\n\n/*\n * Signed add and clamp on underflow.\n *\n * Explicitly do a load-store to ensure the intermediate value never hits\n * memory. This allows lockless observations without ever seeing the negative\n * values.\n */\n#define add_positive(_ptr, _val) do {                           \\\n\ttypeof(_ptr) ptr = (_ptr);                              \\\n\ttypeof(_val) val = (_val);                              \\\n\ttypeof(*ptr) res, var = READ_ONCE(*ptr);                \\\n\t\t\t\t\t\t\t\t\\\n\tres = var + val;                                        \\\n\t\t\t\t\t\t\t\t\\\n\tif (val < 0 && res > var)                               \\\n\t\tres = 0;                                        \\\n\t\t\t\t\t\t\t\t\\\n\tWRITE_ONCE(*ptr, res);                                  \\\n} while (0)\n\n/*\n * Unsigned subtract and clamp on underflow.\n *\n * Explicitly do a load-store to ensure the intermediate value never hits\n * memory. This allows lockless observations without ever seeing the negative\n * values.\n */\n#define sub_positive(_ptr, _val) do {\t\t\t\t\\\n\ttypeof(_ptr) ptr = (_ptr);\t\t\t\t\\\n\ttypeof(*ptr) val = (_val);\t\t\t\t\\\n\ttypeof(*ptr) res, var = READ_ONCE(*ptr);\t\t\\\n\tres = var - val;\t\t\t\t\t\\\n\tif (res > var)\t\t\t\t\t\t\\\n\t\tres = 0;\t\t\t\t\t\\\n\tWRITE_ONCE(*ptr, res);\t\t\t\t\t\\\n} while (0)\n\n/*\n * Remove and clamp on negative, from a local variable.\n *\n * A variant of sub_positive(), which does not use explicit load-store\n * and is thus optimized for local variable updates.\n */\n#define lsub_positive(_ptr, _val) do {\t\t\t\t\\\n\ttypeof(_ptr) ptr = (_ptr);\t\t\t\t\\\n\t*ptr -= min_t(typeof(*ptr), *ptr, _val);\t\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\nstatic inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->runnable_weight += se->runnable_weight;\n\n\tcfs_rq->avg.runnable_load_avg += se->avg.runnable_load_avg;\n\tcfs_rq->avg.runnable_load_sum += se_runnable(se) * se->avg.runnable_load_sum;\n}\n\nstatic inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->runnable_weight -= se->runnable_weight;\n\n\tsub_positive(&cfs_rq->avg.runnable_load_avg, se->avg.runnable_load_avg);\n\tsub_positive(&cfs_rq->avg.runnable_load_sum,\n\t\t     se_runnable(se) * se->avg.runnable_load_sum);\n}\n\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->avg.load_avg += se->avg.load_avg;\n\tcfs_rq->avg.load_sum += se_weight(se) * se->avg.load_sum;\n}\n\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tsub_positive(&cfs_rq->avg.load_avg, se->avg.load_avg);\n\tsub_positive(&cfs_rq->avg.load_sum, se_weight(se) * se->avg.load_sum);\n}\n#else\nstatic inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\nstatic inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\n#endif\n\nstatic void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight, unsigned long runnable)\n{\n\tif (se->on_rq) {\n\t\t/* commit outstanding execution time */\n\t\tif (cfs_rq->curr == se)\n\t\t\tupdate_curr(cfs_rq);\n\t\taccount_entity_dequeue(cfs_rq, se);\n\t\tdequeue_runnable_load_avg(cfs_rq, se);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tse->runnable_weight = runnable;\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t\tse->avg.runnable_load_avg =\n\t\t\tdiv_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\taccount_entity_enqueue(cfs_rq, se);\n\t\tenqueue_runnable_load_avg(cfs_rq, se);\n\t}\n}\n\nvoid reweight_task(struct task_struct *p, int prio)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct load_weight *load = &se->load;\n\tunsigned long weight = scale_load(sched_prio_to_weight[prio]);\n\n\treweight_entity(cfs_rq, se, weight, weight);\n\tload->inv_weight = sched_prio_to_wmult[prio];\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n#ifdef CONFIG_SMP\n/*\n * All this does is approximate the hierarchical proportion which includes that\n * global sum we all love to hate.\n *\n * That is, the weight of a group entity, is the proportional share of the\n * group weight based on the group runqueue weights. That is:\n *\n *                     tg->weight * grq->load.weight\n *   ge->load.weight = -----------------------------               (1)\n *\t\t\t  \\Sum grq->load.weight\n *\n * Now, because computing that sum is prohibitively expensive to compute (been\n * there, done that) we approximate it with this average stuff. The average\n * moves slower and therefore the approximation is cheaper and more stable.\n *\n * So instead of the above, we substitute:\n *\n *   grq->load.weight -> grq->avg.load_avg                         (2)\n *\n * which yields the following:\n *\n *                     tg->weight * grq->avg.load_avg\n *   ge->load.weight = ------------------------------              (3)\n *\t\t\t\ttg->load_avg\n *\n * Where: tg->load_avg ~= \\Sum grq->avg.load_avg\n *\n * That is shares_avg, and it is right (given the approximation (2)).\n *\n * The problem with it is that because the average is slow -- it was designed\n * to be exactly that of course -- this leads to transients in boundary\n * conditions. In specific, the case where the group was idle and we start the\n * one task. It takes time for our CPU's grq->avg.load_avg to build up,\n * yielding bad latency etc..\n *\n * Now, in that special case (1) reduces to:\n *\n *                     tg->weight * grq->load.weight\n *   ge->load.weight = ----------------------------- = tg->weight   (4)\n *\t\t\t    grp->load.weight\n *\n * That is, the sum collapses because all other CPUs are idle; the UP scenario.\n *\n * So what we do is modify our approximation (3) to approach (4) in the (near)\n * UP case, like:\n *\n *   ge->load.weight =\n *\n *              tg->weight * grq->load.weight\n *     ---------------------------------------------------         (5)\n *     tg->load_avg - grq->avg.load_avg + grq->load.weight\n *\n * But because grq->load.weight can drop to 0, resulting in a divide by zero,\n * we need to use grq->avg.load_avg as its lower bound, which then gives:\n *\n *\n *                     tg->weight * grq->load.weight\n *   ge->load.weight = -----------------------------\t\t   (6)\n *\t\t\t\ttg_load_avg'\n *\n * Where:\n *\n *   tg_load_avg' = tg->load_avg - grq->avg.load_avg +\n *                  max(grq->load.weight, grq->avg.load_avg)\n *\n * And that is shares_weight and is icky. In the (near) UP case it approaches\n * (4) while in the normal case it approaches (3). It consistently\n * overestimates the ge->load.weight and therefore:\n *\n *   \\Sum ge->load.weight >= tg->weight\n *\n * hence icky!\n */\nstatic long calc_group_shares(struct cfs_rq *cfs_rq)\n{\n\tlong tg_weight, tg_shares, load, shares;\n\tstruct task_group *tg = cfs_rq->tg;\n\n\ttg_shares = READ_ONCE(tg->shares);\n\n\tload = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);\n\n\ttg_weight = atomic_long_read(&tg->load_avg);\n\n\t/* Ensure tg_weight >= load */\n\ttg_weight -= cfs_rq->tg_load_avg_contrib;\n\ttg_weight += load;\n\n\tshares = (tg_shares * load);\n\tif (tg_weight)\n\t\tshares /= tg_weight;\n\n\t/*\n\t * MIN_SHARES has to be unscaled here to support per-CPU partitioning\n\t * of a group with small tg->shares value. It is a floor value which is\n\t * assigned as a minimum load.weight to the sched_entity representing\n\t * the group on a CPU.\n\t *\n\t * E.g. on 64-bit for a group with tg->shares of scale_load(15)=15*1024\n\t * on an 8-core system with 8 tasks each runnable on one CPU shares has\n\t * to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In\n\t * case no task is runnable on a CPU MIN_SHARES=2 should be returned\n\t * instead of 0.\n\t */\n\treturn clamp_t(long, shares, MIN_SHARES, tg_shares);\n}\n\n/*\n * This calculates the effective runnable weight for a group entity based on\n * the group entity weight calculated above.\n *\n * Because of the above approximation (2), our group entity weight is\n * an load_avg based ratio (3). This means that it includes blocked load and\n * does not represent the runnable weight.\n *\n * Approximate the group entity's runnable weight per ratio from the group\n * runqueue:\n *\n *\t\t\t\t\t     grq->avg.runnable_load_avg\n *   ge->runnable_weight = ge->load.weight * -------------------------- (7)\n *\t\t\t\t\t\t grq->avg.load_avg\n *\n * However, analogous to above, since the avg numbers are slow, this leads to\n * transients in the from-idle case. Instead we use:\n *\n *   ge->runnable_weight = ge->load.weight *\n *\n *\t\tmax(grq->avg.runnable_load_avg, grq->runnable_weight)\n *\t\t-----------------------------------------------------\t(8)\n *\t\t      max(grq->avg.load_avg, grq->load.weight)\n *\n * Where these max() serve both to use the 'instant' values to fix the slow\n * from-idle and avoid the /0 on to-idle, similar to (6).\n */\nstatic long calc_group_runnable(struct cfs_rq *cfs_rq, long shares)\n{\n\tlong runnable, load_avg;\n\n\tload_avg = max(cfs_rq->avg.load_avg,\n\t\t       scale_load_down(cfs_rq->load.weight));\n\n\trunnable = max(cfs_rq->avg.runnable_load_avg,\n\t\t       scale_load_down(cfs_rq->runnable_weight));\n\n\trunnable *= shares;\n\tif (load_avg)\n\t\trunnable /= load_avg;\n\n\treturn clamp_t(long, runnable, MIN_SHARES, shares);\n}\n#endif /* CONFIG_SMP */\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq);\n\n/*\n * Recomputes the group entity based on the current state of its group\n * runqueue.\n */\nstatic void update_cfs_group(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\tlong shares, runnable;\n\n\tif (!gcfs_rq)\n\t\treturn;\n\n\tif (throttled_hierarchy(gcfs_rq))\n\t\treturn;\n\n#ifndef CONFIG_SMP\n\trunnable = shares = READ_ONCE(gcfs_rq->tg->shares);\n\n\tif (likely(se->load.weight == shares))\n\t\treturn;\n#else\n\tshares   = calc_group_shares(gcfs_rq);\n\trunnable = calc_group_runnable(gcfs_rq, shares);\n#endif\n\n\treweight_entity(cfs_rq_of(se), se, shares, runnable);\n}\n\n#else /* CONFIG_FAIR_GROUP_SCHED */\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\nstatic inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}\n\n#ifdef CONFIG_SMP\n#ifdef CONFIG_FAIR_GROUP_SCHED\n/**\n * update_tg_load_avg - update the tg's load avg\n * @cfs_rq: the cfs_rq whose avg changed\n * @force: update regardless of how small the difference\n *\n * This function 'ensures': tg->load_avg := \\Sum tg->cfs_rq[]->avg.load.\n * However, because tg->load_avg is a global value there are performance\n * considerations.\n *\n * In order to avoid having to look at the other cfs_rq's, we use a\n * differential update where we store the last value we propagated. This in\n * turn allows skipping updates if the differential is 'small'.\n *\n * Updating tg's load_avg is necessary before update_cfs_share().\n */\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)\n{\n\tlong delta = cfs_rq->avg.load_avg - cfs_rq->tg_load_avg_contrib;\n\n\t/*\n\t * No need to update load_avg for root_task_group as it is not used.\n\t */\n\tif (cfs_rq->tg == &root_task_group)\n\t\treturn;\n\n\tif (force || abs(delta) > cfs_rq->tg_load_avg_contrib / 64) {\n\t\tatomic_long_add(delta, &cfs_rq->tg->load_avg);\n\t\tcfs_rq->tg_load_avg_contrib = cfs_rq->avg.load_avg;\n\t}\n}\n\n/*\n * Called within set_task_rq() right before setting a task's CPU. The\n * caller only guarantees p->pi_lock is held; no other assumptions,\n * including the state of rq->lock, should be made.\n */\nvoid set_task_rq_fair(struct sched_entity *se,\n\t\t      struct cfs_rq *prev, struct cfs_rq *next)\n{\n\tu64 p_last_update_time;\n\tu64 n_last_update_time;\n\n\tif (!sched_feat(ATTACH_AGE_LOAD))\n\t\treturn;\n\n\t/*\n\t * We are supposed to update the task to \"current\" time, then its up to\n\t * date and ready to go to new CPU/cfs_rq. But we have difficulty in\n\t * getting what current time is, so simply throw away the out-of-date\n\t * time. This will result in the wakee task is less decayed, but giving\n\t * the wakee more load sounds not bad.\n\t */\n\tif (!(se->avg.last_update_time && prev))\n\t\treturn;\n\n#ifndef CONFIG_64BIT\n\t{\n\t\tu64 p_last_update_time_copy;\n\t\tu64 n_last_update_time_copy;\n\n\t\tdo {\n\t\t\tp_last_update_time_copy = prev->load_last_update_time_copy;\n\t\t\tn_last_update_time_copy = next->load_last_update_time_copy;\n\n\t\t\tsmp_rmb();\n\n\t\t\tp_last_update_time = prev->avg.last_update_time;\n\t\t\tn_last_update_time = next->avg.last_update_time;\n\n\t\t} while (p_last_update_time != p_last_update_time_copy ||\n\t\t\t n_last_update_time != n_last_update_time_copy);\n\t}\n#else\n\tp_last_update_time = prev->avg.last_update_time;\n\tn_last_update_time = next->avg.last_update_time;\n#endif\n\t__update_load_avg_blocked_se(p_last_update_time, cpu_of(rq_of(prev)), se);\n\tse->avg.last_update_time = n_last_update_time;\n}\n\n\n/*\n * When on migration a sched_entity joins/leaves the PELT hierarchy, we need to\n * propagate its contribution. The key to this propagation is the invariant\n * that for each group:\n *\n *   ge->avg == grq->avg\t\t\t\t\t\t(1)\n *\n * _IFF_ we look at the pure running and runnable sums. Because they\n * represent the very same entity, just at different points in the hierarchy.\n *\n * Per the above update_tg_cfs_util() is trivial and simply copies the running\n * sum over (but still wrong, because the group entity and group rq do not have\n * their PELT windows aligned).\n *\n * However, update_tg_cfs_runnable() is more complex. So we have:\n *\n *   ge->avg.load_avg = ge->load.weight * ge->avg.runnable_avg\t\t(2)\n *\n * And since, like util, the runnable part should be directly transferable,\n * the following would _appear_ to be the straight forward approach:\n *\n *   grq->avg.load_avg = grq->load.weight * grq->avg.runnable_avg\t(3)\n *\n * And per (1) we have:\n *\n *   ge->avg.runnable_avg == grq->avg.runnable_avg\n *\n * Which gives:\n *\n *                      ge->load.weight * grq->avg.load_avg\n *   ge->avg.load_avg = -----------------------------------\t\t(4)\n *                               grq->load.weight\n *\n * Except that is wrong!\n *\n * Because while for entities historical weight is not important and we\n * really only care about our future and therefore can consider a pure\n * runnable sum, runqueues can NOT do this.\n *\n * We specifically want runqueues to have a load_avg that includes\n * historical weights. Those represent the blocked load, the load we expect\n * to (shortly) return to us. This only works by keeping the weights as\n * integral part of the sum. We therefore cannot decompose as per (3).\n *\n * Another reason this doesn't work is that runnable isn't a 0-sum entity.\n * Imagine a rq with 2 tasks that each are runnable 2/3 of the time. Then the\n * rq itself is runnable anywhere between 2/3 and 1 depending on how the\n * runnable section of these tasks overlap (or not). If they were to perfectly\n * align the rq as a whole would be runnable 2/3 of the time. If however we\n * always have at least 1 runnable task, the rq as a whole is always runnable.\n *\n * So we'll have to approximate.. :/\n *\n * Given the constraint:\n *\n *   ge->avg.running_sum <= ge->avg.runnable_sum <= LOAD_AVG_MAX\n *\n * We can construct a rule that adds runnable to a rq by assuming minimal\n * overlap.\n *\n * On removal, we'll assume each task is equally runnable; which yields:\n *\n *   grq->avg.runnable_sum = grq->avg.load_sum / grq->load.weight\n *\n * XXX: only do this for the part of runnable > running ?\n *\n */\n\nstatic inline void\nupdate_tg_cfs_util(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta = gcfs_rq->avg.util_avg - se->avg.util_avg;\n\n\t/* Nothing to update */\n\tif (!delta)\n\t\treturn;\n\n\t/*\n\t * The relation between sum and avg is:\n\t *\n\t *   LOAD_AVG_MAX - 1024 + sa->period_contrib\n\t *\n\t * however, the PELT windows are not aligned between grq and gse.\n\t */\n\n\t/* Set new sched_entity's utilization */\n\tse->avg.util_avg = gcfs_rq->avg.util_avg;\n\tse->avg.util_sum = se->avg.util_avg * LOAD_AVG_MAX;\n\n\t/* Update parent cfs_rq utilization */\n\tadd_positive(&cfs_rq->avg.util_avg, delta);\n\tcfs_rq->avg.util_sum = cfs_rq->avg.util_avg * LOAD_AVG_MAX;\n}\n\nstatic inline void\nupdate_tg_cfs_runnable(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;\n\tunsigned long runnable_load_avg, load_avg;\n\tu64 runnable_load_sum, load_sum = 0;\n\ts64 delta_sum;\n\n\tif (!runnable_sum)\n\t\treturn;\n\n\tgcfs_rq->prop_runnable_sum = 0;\n\n\tif (runnable_sum >= 0) {\n\t\t/*\n\t\t * Add runnable; clip at LOAD_AVG_MAX. Reflects that until\n\t\t * the CPU is saturated running == runnable.\n\t\t */\n\t\trunnable_sum += se->avg.load_sum;\n\t\trunnable_sum = min(runnable_sum, (long)LOAD_AVG_MAX);\n\t} else {\n\t\t/*\n\t\t * Estimate the new unweighted runnable_sum of the gcfs_rq by\n\t\t * assuming all tasks are equally runnable.\n\t\t */\n\t\tif (scale_load_down(gcfs_rq->load.weight)) {\n\t\t\tload_sum = div_s64(gcfs_rq->avg.load_sum,\n\t\t\t\tscale_load_down(gcfs_rq->load.weight));\n\t\t}\n\n\t\t/* But make sure to not inflate se's runnable */\n\t\trunnable_sum = min(se->avg.load_sum, load_sum);\n\t}\n\n\t/*\n\t * runnable_sum can't be lower than running_sum\n\t * As running sum is scale with CPU capacity wehreas the runnable sum\n\t * is not we rescale running_sum 1st\n\t */\n\trunning_sum = se->avg.util_sum /\n\t\tarch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\trunnable_sum = max(runnable_sum, running_sum);\n\n\tload_sum = (s64)se_weight(se) * runnable_sum;\n\tload_avg = div_s64(load_sum, LOAD_AVG_MAX);\n\n\tdelta_sum = load_sum - (s64)se_weight(se) * se->avg.load_sum;\n\tdelta_avg = load_avg - se->avg.load_avg;\n\n\tse->avg.load_sum = runnable_sum;\n\tse->avg.load_avg = load_avg;\n\tadd_positive(&cfs_rq->avg.load_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.load_sum, delta_sum);\n\n\trunnable_load_sum = (s64)se_runnable(se) * runnable_sum;\n\trunnable_load_avg = div_s64(runnable_load_sum, LOAD_AVG_MAX);\n\tdelta_sum = runnable_load_sum - se_weight(se) * se->avg.runnable_load_sum;\n\tdelta_avg = runnable_load_avg - se->avg.runnable_load_avg;\n\n\tse->avg.runnable_load_sum = runnable_sum;\n\tse->avg.runnable_load_avg = runnable_load_avg;\n\n\tif (se->on_rq) {\n\t\tadd_positive(&cfs_rq->avg.runnable_load_avg, delta_avg);\n\t\tadd_positive(&cfs_rq->avg.runnable_load_sum, delta_sum);\n\t}\n}\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum)\n{\n\tcfs_rq->propagate = 1;\n\tcfs_rq->prop_runnable_sum += runnable_sum;\n}\n\n/* Update task and its cfs_rq load average */\nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq, *gcfs_rq;\n\n\tif (entity_is_task(se))\n\t\treturn 0;\n\n\tgcfs_rq = group_cfs_rq(se);\n\tif (!gcfs_rq->propagate)\n\t\treturn 0;\n\n\tgcfs_rq->propagate = 0;\n\n\tcfs_rq = cfs_rq_of(se);\n\n\tadd_tg_cfs_propagate(cfs_rq, gcfs_rq->prop_runnable_sum);\n\n\tupdate_tg_cfs_util(cfs_rq, se, gcfs_rq);\n\tupdate_tg_cfs_runnable(cfs_rq, se, gcfs_rq);\n\n\treturn 1;\n}\n\n/*\n * Check if we need to update the load and the utilization of a blocked\n * group_entity:\n */\nstatic inline bool skip_blocked_update(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\n\t/*\n\t * If sched_entity still have not zero load or utilization, we have to\n\t * decay it:\n\t */\n\tif (se->avg.load_avg || se->avg.util_avg)\n\t\treturn false;\n\n\t/*\n\t * If there is a pending propagation, we have to update the load and\n\t * the utilization of the sched_entity:\n\t */\n\tif (gcfs_rq->propagate)\n\t\treturn false;\n\n\t/*\n\t * Otherwise, the load and the utilization of the sched_entity is\n\t * already zero and there is no pending propagation, so it will be a\n\t * waste of time to try to decay it:\n\t */\n\treturn true;\n}\n\n#else /* CONFIG_FAIR_GROUP_SCHED */\n\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}\n\nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\treturn 0;\n}\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n/**\n * update_cfs_rq_load_avg - update the cfs_rq's load/util averages\n * @now: current time, as per cfs_rq_clock_task()\n * @cfs_rq: cfs_rq to update\n *\n * The cfs_rq avg is the direct sum of all its entities (blocked and runnable)\n * avg. The immediate corollary is that all (fair) tasks must be attached, see\n * post_init_entity_util_avg().\n *\n * cfs_rq->avg is used for task_h_load() and update_cfs_share() for example.\n *\n * Returns true if the load decayed or we removed load.\n *\n * Since both these conditions indicate a changed cfs_rq->avg.load we should\n * call update_tg_load_avg() when this function returns true.\n */\nstatic inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_sum, removed_runnable_sum);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\n\t\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);\n\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->load_last_update_time_copy = sa->last_update_time;\n#endif\n\n\tif (decayed)\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\treturn decayed;\n}\n\n/**\n * attach_entity_load_avg - attach this entity to its cfs_rq load avg\n * @cfs_rq: cfs_rq to attach to\n * @se: sched_entity to attach\n * @flags: migration hints\n *\n * Must call update_cfs_rq_load_avg() before this, since we rely on\n * cfs_rq->avg.last_update_time being current.\n */\nstatic void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu32 divider = LOAD_AVG_MAX - 1024 + cfs_rq->avg.period_contrib;\n\n\t/*\n\t * When we attach the @se to the @cfs_rq, we must align the decay\n\t * window because without that, really weird and wonderful things can\n\t * happen.\n\t *\n\t * XXX illustrate\n\t */\n\tse->avg.last_update_time = cfs_rq->avg.last_update_time;\n\tse->avg.period_contrib = cfs_rq->avg.period_contrib;\n\n\t/*\n\t * Hell(o) Nasty stuff.. we need to recompute _sum based on the new\n\t * period_contrib. This isn't strictly correct, but since we're\n\t * entirely outside of the PELT hierarchy, nobody cares if we truncate\n\t * _sum a little.\n\t */\n\tse->avg.util_sum = se->avg.util_avg * divider;\n\n\tse->avg.load_sum = divider;\n\tif (se_weight(se)) {\n\t\tse->avg.load_sum =\n\t\t\tdiv_u64(se->avg.load_avg * se->avg.load_sum, se_weight(se));\n\t}\n\n\tse->avg.runnable_load_sum = se->avg.load_sum;\n\n\tenqueue_load_avg(cfs_rq, se);\n\tcfs_rq->avg.util_avg += se->avg.util_avg;\n\tcfs_rq->avg.util_sum += se->avg.util_sum;\n\n\tadd_tg_cfs_propagate(cfs_rq, se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, flags);\n}\n\n/**\n * detach_entity_load_avg - detach this entity from its cfs_rq load avg\n * @cfs_rq: cfs_rq to detach from\n * @se: sched_entity to detach\n *\n * Must call update_cfs_rq_load_avg() before this, since we rely on\n * cfs_rq->avg.last_update_time being current.\n */\nstatic void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tdequeue_load_avg(cfs_rq, se);\n\tsub_positive(&cfs_rq->avg.util_avg, se->avg.util_avg);\n\tsub_positive(&cfs_rq->avg.util_sum, se->avg.util_sum);\n\n\tadd_tg_cfs_propagate(cfs_rq, -se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, 0);\n}\n\n/*\n * Optional action to be done while updating the load average\n */\n#define UPDATE_TG\t0x1\n#define SKIP_AGE_LOAD\t0x2\n#define DO_ATTACH\t0x4\n\n/* Update task and its cfs_rq load average */\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu64 now = cfs_rq_clock_task(cfs_rq);\n\tstruct rq *rq = rq_of(cfs_rq);\n\tint cpu = cpu_of(rq);\n\tint decayed;\n\n\t/*\n\t * Track task load average for carrying it to new CPU after migrated, and\n\t * track group sched_entity load average for task_h_load calc in migration\n\t */\n\tif (se->avg.last_update_time && !(flags & SKIP_AGE_LOAD))\n\t\t__update_load_avg_se(now, cpu, cfs_rq, se);\n\n\tdecayed  = update_cfs_rq_load_avg(now, cfs_rq);\n\tdecayed |= propagate_entity_load_avg(se);\n\n\tif (!se->avg.last_update_time && (flags & DO_ATTACH)) {\n\n\t\t/*\n\t\t * DO_ATTACH means we're here from enqueue_entity().\n\t\t * !last_update_time means we've passed through\n\t\t * migrate_task_rq_fair() indicating we migrated.\n\t\t *\n\t\t * IOW we're enqueueing a task on a new CPU.\n\t\t */\n\t\tattach_entity_load_avg(cfs_rq, se, SCHED_CPUFREQ_MIGRATION);\n\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t} else if (decayed && (flags & UPDATE_TG))\n\t\tupdate_tg_load_avg(cfs_rq, 0);\n}\n\n#ifndef CONFIG_64BIT\nstatic inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\tu64 last_update_time_copy;\n\tu64 last_update_time;\n\n\tdo {\n\t\tlast_update_time_copy = cfs_rq->load_last_update_time_copy;\n\t\tsmp_rmb();\n\t\tlast_update_time = cfs_rq->avg.last_update_time;\n\t} while (last_update_time != last_update_time_copy);\n\n\treturn last_update_time;\n}\n#else\nstatic inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.last_update_time;\n}\n#endif\n\n/*\n * Synchronize entity load avg of dequeued entity without locking\n * the previous rq.\n */\nvoid sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}\n\n/*\n * Task first catches up with cfs_rq, and then subtract\n * itself from the cfs_rq (task must be off the queue now).\n */\nvoid remove_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tunsigned long flags;\n\n\t/*\n\t * tasks cannot exit without having gone through wake_up_new_task() ->\n\t * post_init_entity_util_avg() which will have added things to the\n\t * cfs_rq, so we can remove unconditionally.\n\t *\n\t * Similarly for groups, they will have passed through\n\t * post_init_entity_util_avg() before unregister_sched_fair_group()\n\t * calls this.\n\t */\n\n\tsync_entity_load_avg(se);\n\n\traw_spin_lock_irqsave(&cfs_rq->removed.lock, flags);\n\t++cfs_rq->removed.nr;\n\tcfs_rq->removed.util_avg\t+= se->avg.util_avg;\n\tcfs_rq->removed.load_avg\t+= se->avg.load_avg;\n\tcfs_rq->removed.runnable_sum\t+= se->avg.load_sum; /* == runnable_sum */\n\traw_spin_unlock_irqrestore(&cfs_rq->removed.lock, flags);\n}\n\nstatic inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.runnable_load_avg;\n}\n\nstatic inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}\n\nstatic int idle_balance(struct rq *this_rq, struct rq_flags *rf);\n\nstatic inline unsigned long task_util(struct task_struct *p)\n{\n\treturn READ_ONCE(p->se.avg.util_avg);\n}\n\nstatic inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn (max(ue.ewma, ue.enqueued) | UTIL_AVG_UNCHANGED);\n}\n\nstatic inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}\n\nstatic inline void util_est_enqueue(struct cfs_rq *cfs_rq,\n\t\t\t\t    struct task_struct *p)\n{\n\tunsigned int enqueued;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t/* Update root cfs_rq's estimated utilization */\n\tenqueued  = cfs_rq->avg.util_est.enqueued;\n\tenqueued += _task_util_est(p);\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);\n}\n\n/*\n * Check if a (signed) value is within a specified (unsigned) margin,\n * based on the observation that:\n *\n *     abs(x) < y := (unsigned)(x + y - 1) < (2 * y - 1)\n *\n * NOTE: this only works when value + maring < INT_MAX.\n */\nstatic inline bool within_margin(int value, int margin)\n{\n\treturn ((unsigned int)(value + margin - 1) < (2 * margin - 1));\n}\n\nstatic void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p, bool task_sleep)\n{\n\tlong last_ewma_diff;\n\tstruct util_est ue;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t/* Update root cfs_rq's estimated utilization */\n\tue.enqueued  = cfs_rq->avg.util_est.enqueued;\n\tue.enqueued -= min_t(unsigned int, ue.enqueued, _task_util_est(p));\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, ue.enqueued);\n\n\t/*\n\t * Skip update of task's estimated utilization when the task has not\n\t * yet completed an activation, e.g. being migrated.\n\t */\n\tif (!task_sleep)\n\t\treturn;\n\n\t/*\n\t * If the PELT values haven't changed since enqueue time,\n\t * skip the util_est update.\n\t */\n\tue = p->se.avg.util_est;\n\tif (ue.enqueued & UTIL_AVG_UNCHANGED)\n\t\treturn;\n\n\t/*\n\t * Skip update of task's estimated utilization when its EWMA is\n\t * already ~1% close to its last activation value.\n\t */\n\tue.enqueued = (task_util(p) | UTIL_AVG_UNCHANGED);\n\tlast_ewma_diff = ue.enqueued - ue.ewma;\n\tif (within_margin(last_ewma_diff, (SCHED_CAPACITY_SCALE / 100)))\n\t\treturn;\n\n\t/*\n\t * Update Task's estimated utilization\n\t *\n\t * When *p completes an activation we can consolidate another sample\n\t * of the task size. This is done by storing the current PELT value\n\t * as ue.enqueued and by using this value to update the Exponential\n\t * Weighted Moving Average (EWMA):\n\t *\n\t *  ewma(t) = w *  task_util(p) + (1-w) * ewma(t-1)\n\t *          = w *  task_util(p) +         ewma(t-1)  - w * ewma(t-1)\n\t *          = w * (task_util(p) -         ewma(t-1)) +     ewma(t-1)\n\t *          = w * (      last_ewma_diff            ) +     ewma(t-1)\n\t *          = w * (last_ewma_diff  +  ewma(t-1) / w)\n\t *\n\t * Where 'w' is the weight of new samples, which is configured to be\n\t * 0.25, thus making w=1/4 ( >>= UTIL_EST_WEIGHT_SHIFT)\n\t */\n\tue.ewma <<= UTIL_EST_WEIGHT_SHIFT;\n\tue.ewma  += last_ewma_diff;\n\tue.ewma >>= UTIL_EST_WEIGHT_SHIFT;\n\tWRITE_ONCE(p->se.avg.util_est, ue);\n}\n\nstatic inline int task_fits_capacity(struct task_struct *p, long capacity)\n{\n\treturn capacity * 1024 > task_util_est(p) * capacity_margin;\n}\n\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq)\n{\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn;\n\n\tif (!p) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\tif (task_fits_capacity(p, capacity_of(cpu_of(rq)))) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\trq->misfit_task_load = task_h_load(p);\n}\n\n#else /* CONFIG_SMP */\n\n#define UPDATE_TG\t0x0\n#define SKIP_AGE_LOAD\t0x0\n#define DO_ATTACH\t0x0\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}\n\nstatic inline void remove_entity_load_avg(struct sched_entity *se) {}\n\nstatic inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {}\nstatic inline void\ndetach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}\n\nstatic inline int idle_balance(struct rq *rq, struct rq_flags *rf)\n{\n\treturn 0;\n}\n\nstatic inline void\nutil_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}\n\nstatic inline void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p,\n\t\t bool task_sleep) {}\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}\n\n#endif /* CONFIG_SMP */\n\nstatic void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n\n\tif (d < 0)\n\t\td = -d;\n\n\tif (d > 3*sysctl_sched_latency)\n\t\tschedstat_inc(cfs_rq->nr_spread_over);\n#endif\n}\n\nstatic void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n{\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\t/*\n\t * The 'current' period is already promised to the current tasks,\n\t * however the extra weight of the new task will slow them down a\n\t * little, place the new task so that it fits in the slot that\n\t * stays open at the end.\n\t */\n\tif (initial && sched_feat(START_DEBIT))\n\t\tvruntime += sched_vslice(cfs_rq, se);\n\n\t/* sleeps up to a single latency don't count. */\n\tif (!initial) {\n\t\tunsigned long thresh = sysctl_sched_latency;\n\n\t\t/*\n\t\t * Halve their sleep time's effect, to allow\n\t\t * for a gentler effect of sleepers:\n\t\t */\n\t\tif (sched_feat(GENTLE_FAIR_SLEEPERS))\n\t\t\tthresh >>= 1;\n\n\t\tvruntime -= thresh;\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tse->vruntime = max_vruntime(se->vruntime, vruntime);\n}\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\n\nstatic inline void check_schedstat_required(void)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tif (schedstat_enabled())\n\t\treturn;\n\n\t/* Force schedstat enabled if a dependent tracepoint is active */\n\tif (trace_sched_stat_wait_enabled()    ||\n\t\t\ttrace_sched_stat_sleep_enabled()   ||\n\t\t\ttrace_sched_stat_iowait_enabled()  ||\n\t\t\ttrace_sched_stat_blocked_enabled() ||\n\t\t\ttrace_sched_stat_runtime_enabled())  {\n\t\tprintk_deferred_once(\"Scheduler tracepoints stat_sleep, stat_iowait, \"\n\t\t\t     \"stat_blocked and stat_runtime require the \"\n\t\t\t     \"kernel parameter schedstats=enable or \"\n\t\t\t     \"kernel.sched_schedstats=1\\n\");\n\t}\n#endif\n}\n\n\n/*\n * MIGRATION\n *\n *\tdequeue\n *\t  update_curr()\n *\t    update_min_vruntime()\n *\t  vruntime -= min_vruntime\n *\n *\tenqueue\n *\t  update_curr()\n *\t    update_min_vruntime()\n *\t  vruntime += min_vruntime\n *\n * this way the vruntime transition between RQs is done when both\n * min_vruntime are up-to-date.\n *\n * WAKEUP (remote)\n *\n *\t->migrate_task_rq_fair() (p->state == TASK_WAKING)\n *\t  vruntime -= min_vruntime\n *\n *\tenqueue\n *\t  update_curr()\n *\t    update_min_vruntime()\n *\t  vruntime += min_vruntime\n *\n * this way we don't have the most up-to-date min_vruntime on the originating\n * CPU and an up-to-date min_vruntime on the destination CPU.\n */\n\nstatic void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);\n\tbool curr = cfs_rq->curr == se;\n\n\t/*\n\t * If we're the current task, we must renormalise before calling\n\t * update_curr().\n\t */\n\tif (renorm && curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Otherwise, renormalise after, such that we're placed at the current\n\t * moment in time, instead of some random moment in the past. Being\n\t * placed in the past could significantly boost this task to the\n\t * fairness detriment of existing tasks.\n\t */\n\tif (renorm && !curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\t/*\n\t * When enqueuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Add its load to cfs_rq->runnable_avg\n\t *   - For group_entity, update its weight to reflect the new share of\n\t *     its group cfs_rq\n\t *   - Add its new weight to cfs_rq->load.weight\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tupdate_cfs_group(se);\n\tenqueue_runnable_load_avg(cfs_rq, se);\n\taccount_entity_enqueue(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tplace_entity(cfs_rq, se, 0);\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue(cfs_rq, se, flags);\n\tcheck_spread(cfs_rq, se);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t}\n}\n\nstatic void __clear_buddies_last(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->last != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->last = NULL;\n\t}\n}\n\nstatic void __clear_buddies_next(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->next != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->next = NULL;\n\t}\n}\n\nstatic void __clear_buddies_skip(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->skip != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->skip = NULL;\n\t}\n}\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * When dequeuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Subtract its load from the cfs_rq->runnable_avg.\n\t *   - Subtract its previous weight from cfs_rq->load.weight.\n\t *   - For group entity, update its weight to reflect the new share\n\t *     of its group cfs_rq.\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\tdequeue_runnable_load_avg(cfs_rq, se);\n\n\tupdate_stats_dequeue(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t/*\n\t * Normalize after update_curr(); which will also have moved\n\t * min_vruntime if @se is the one holding it back. But before doing\n\t * update_min_vruntime() again, which will discount @se's position and\n\t * can move min_vruntime forward still more.\n\t */\n\tif (!(flags & DEQUEUE_SLEEP))\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\n\t/* return excess runtime on last dequeue */\n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t/*\n\t * Now advance min_vruntime if @se was the entity holding it back,\n\t * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be\n\t * put back on, and if we advance min_vruntime, we'll be placed back\n\t * further than we started -- ie. we'll be penalized.\n\t */\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n}\n\n/*\n * Preempt the current task with a newly woken task if needed:\n */\nstatic void\ncheck_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tunsigned long ideal_runtime, delta_exec;\n\tstruct sched_entity *se;\n\ts64 delta;\n\n\tideal_runtime = sched_slice(cfs_rq, curr);\n\tdelta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;\n\tif (delta_exec > ideal_runtime) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\t/*\n\t\t * The current task ran long enough, ensure it doesn't get\n\t\t * re-elected due to buddy favours.\n\t\t */\n\t\tclear_buddies(cfs_rq, curr);\n\t\treturn;\n\t}\n\n\t/*\n\t * Ensure that a task that missed wakeup preemption by a\n\t * narrow margin doesn't have to wait for a full slice.\n\t * This also mitigates buddy induced latencies under load.\n\t */\n\tif (delta_exec < sysctl_sched_min_granularity)\n\t\treturn;\n\n\tse = __pick_first_entity(cfs_rq);\n\tdelta = curr->vruntime - se->vruntime;\n\n\tif (delta < 0)\n\t\treturn;\n\n\tif (delta > ideal_runtime)\n\t\tresched_curr(rq_of(cfs_rq));\n}\n\nstatic void\nset_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/* 'current' is not kept within the tree. */\n\tif (se->on_rq) {\n\t\t/*\n\t\t * Any task has to be enqueued before it get to execute on\n\t\t * a CPU. So account for the time it spent waiting on the\n\t\t * runqueue.\n\t\t */\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\t\t__dequeue_entity(cfs_rq, se);\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n\n\tupdate_stats_curr_start(cfs_rq, se);\n\tcfs_rq->curr = se;\n\n\t/*\n\t * Track our maximum slice length, if the CPU's load is at\n\t * least twice that of our own weight (i.e. dont track it\n\t * when there are only lesser-weight tasks around):\n\t */\n\tif (schedstat_enabled() && rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {\n\t\tschedstat_set(se->statistics.slice_max,\n\t\t\tmax((u64)schedstat_val(se->statistics.slice_max),\n\t\t\t    se->sum_exec_runtime - se->prev_sum_exec_runtime));\n\t}\n\n\tse->prev_sum_exec_runtime = se->sum_exec_runtime;\n}\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\n\n/*\n * Pick the next process, keeping these things in mind, in this order:\n * 1) keep things fair between processes/task groups\n * 2) pick the \"next\" process, since someone really wants that to run\n * 3) pick the \"last\" process, for cache locality\n * 4) do not run the \"skip\" process, if something else is available\n */\nstatic struct sched_entity *\npick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tstruct sched_entity *left = __pick_first_entity(cfs_rq);\n\tstruct sched_entity *se;\n\n\t/*\n\t * If curr is set we have to see if its left of the leftmost entity\n\t * still in the tree, provided there was anything in the tree at all.\n\t */\n\tif (!left || (curr && entity_before(curr, left)))\n\t\tleft = curr;\n\n\tse = left; /* ideally we run the leftmost entity */\n\n\t/*\n\t * Avoid running the skip buddy, if running something else can\n\t * be done without getting too unfair.\n\t */\n\tif (cfs_rq->skip == se) {\n\t\tstruct sched_entity *second;\n\n\t\tif (se == curr) {\n\t\t\tsecond = __pick_first_entity(cfs_rq);\n\t\t} else {\n\t\t\tsecond = __pick_next_entity(se);\n\t\t\tif (!second || (curr && entity_before(curr, second)))\n\t\t\t\tsecond = curr;\n\t\t}\n\n\t\tif (second && wakeup_preempt_entity(second, left) < 1)\n\t\t\tse = second;\n\t}\n\n\t/*\n\t * Prefer last buddy, try to return the CPU to a preempted task.\n\t */\n\tif (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1)\n\t\tse = cfs_rq->last;\n\n\t/*\n\t * Someone really wants this to run. If it's not unfair, run it.\n\t */\n\tif (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1)\n\t\tse = cfs_rq->next;\n\n\tclear_buddies(cfs_rq, se);\n\n\treturn se;\n}\n\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)\n{\n\t/*\n\t * If still on the runqueue then deactivate_task()\n\t * was not called and update_curr() has to be done:\n\t */\n\tif (prev->on_rq)\n\t\tupdate_curr(cfs_rq);\n\n\t/* throttle cfs_rqs exceeding runtime */\n\tcheck_cfs_rq_runtime(cfs_rq);\n\n\tcheck_spread(cfs_rq, prev);\n\n\tif (prev->on_rq) {\n\t\tupdate_stats_wait_start(cfs_rq, prev);\n\t\t/* Put 'current' back into the tree. */\n\t\t__enqueue_entity(cfs_rq, prev);\n\t\t/* in !on_rq case, update occurred at dequeue */\n\t\tupdate_load_avg(cfs_rq, prev, 0);\n\t}\n\tcfs_rq->curr = NULL;\n}\n\nstatic void\nentity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Ensure that runnable average is periodically updated.\n\t */\n\tupdate_load_avg(cfs_rq, curr, UPDATE_TG);\n\tupdate_cfs_group(curr);\n\n#ifdef CONFIG_SCHED_HRTICK\n\t/*\n\t * queued ticks are scheduled to match the slice, so don't bother\n\t * validating it and just reschedule.\n\t */\n\tif (queued) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\treturn;\n\t}\n\t/*\n\t * don't let the period tick interfere with the hrtick preemption\n\t */\n\tif (!sched_feat(DOUBLE_TICK) &&\n\t\t\thrtimer_active(&rq_of(cfs_rq)->hrtick_timer))\n\t\treturn;\n#endif\n\n\tif (cfs_rq->nr_running > 1)\n\t\tcheck_preempt_tick(cfs_rq, curr);\n}\n\n\n/**************************************************\n * CFS bandwidth control machinery\n */\n\n#ifdef CONFIG_CFS_BANDWIDTH\n\n#ifdef HAVE_JUMP_LABEL\nstatic struct static_key __cfs_bandwidth_used;\n\nstatic inline bool cfs_bandwidth_used(void)\n{\n\treturn static_key_false(&__cfs_bandwidth_used);\n}\n\nvoid cfs_bandwidth_usage_inc(void)\n{\n\tstatic_key_slow_inc_cpuslocked(&__cfs_bandwidth_used);\n}\n\nvoid cfs_bandwidth_usage_dec(void)\n{\n\tstatic_key_slow_dec_cpuslocked(&__cfs_bandwidth_used);\n}\n#else /* HAVE_JUMP_LABEL */\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}\n\nvoid cfs_bandwidth_usage_inc(void) {}\nvoid cfs_bandwidth_usage_dec(void) {}\n#endif /* HAVE_JUMP_LABEL */\n\n/*\n * default period for cfs group bandwidth.\n * default: 0.1s, units: nanoseconds\n */\nstatic inline u64 default_cfs_period(void)\n{\n\treturn 100000000ULL;\n}\n\nstatic inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}\n\n/*\n * Replenish runtime according to assigned quota and update expiration time.\n * We use sched_clock_cpu directly instead of rq->clock to avoid adding\n * additional synchronization around rq->lock.\n *\n * requires cfs_b->lock\n */\nvoid __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tu64 now;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\treturn;\n\n\tnow = sched_clock_cpu(smp_processor_id());\n\tcfs_b->runtime = cfs_b->quota;\n\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n}\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn &tg->cfs_bandwidth;\n}\n\n/* rq->task_clock normalized against any time this cfs_rq has spent throttled */\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\tif (unlikely(cfs_rq->throttle_count))\n\t\treturn cfs_rq->throttled_clock_task - cfs_rq->throttled_clock_task_time;\n\n\treturn rq_clock_task(rq_of(cfs_rq)) - cfs_rq->throttled_clock_task_time;\n}\n\n/* returns 0 on failure to allocate runtime */\nstatic int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct task_group *tg = cfs_rq->tg;\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n\tu64 amount = 0, min_amount, expires;\n\tint expires_seq;\n\n\t/* note: this is a positive sum as runtime_remaining <= 0 */\n\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\texpires_seq = cfs_b->expires_seq;\n\texpires = cfs_b->runtime_expires;\n\traw_spin_unlock(&cfs_b->lock);\n\n\tcfs_rq->runtime_remaining += amount;\n\t/*\n\t * we may have advanced our local expiration to account for allowed\n\t * spread between our sched_clock and the one on which runtime was\n\t * issued.\n\t */\n\tif (cfs_rq->expires_seq != expires_seq) {\n\t\tcfs_rq->expires_seq = expires_seq;\n\t\tcfs_rq->runtime_expires = expires;\n\t}\n\n\treturn cfs_rq->runtime_remaining > 0;\n}\n\n/*\n * Note: This depends on the synchronization provided by sched_clock and the\n * fact that rq->clock snapshots this value.\n */\nstatic void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\n\t/* if the deadline is ahead of our clock, nothing to do */\n\tif (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))\n\t\treturn;\n\n\tif (cfs_rq->runtime_remaining < 0)\n\t\treturn;\n\n\t/*\n\t * If the local deadline has passed we have to consider the\n\t * possibility that our sched_clock is 'fast' and the global deadline\n\t * has not truly expired.\n\t *\n\t * Fortunately we can check determine whether this the case by checking\n\t * whether the global deadline(cfs_b->expires_seq) has advanced.\n\t */\n\tif (cfs_rq->expires_seq == cfs_b->expires_seq) {\n\t\t/* extend local deadline, drift is bounded above by 2 ticks */\n\t\tcfs_rq->runtime_expires += TICK_NSEC;\n\t} else {\n\t\t/* global deadline is ahead, expiration has passed */\n\t\tcfs_rq->runtime_remaining = 0;\n\t}\n}\n\nstatic void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t/* dock delta_exec before expiring quota (as it could span periods) */\n\tcfs_rq->runtime_remaining -= delta_exec;\n\texpire_cfs_rq_runtime(cfs_rq);\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\t/*\n\t * if we're unable to extend our runtime we resched so that the active\n\t * hierarchy can be throttled\n\t */\n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\tif (!cfs_bandwidth_used() || !cfs_rq->runtime_enabled)\n\t\treturn;\n\n\t__account_cfs_rq_runtime(cfs_rq, delta_exec);\n}\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttled;\n}\n\n/* check whether cfs_rq, or any parent, is throttled */\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttle_count;\n}\n\n/*\n * Ensure that neither of the group entities corresponding to src_cpu or\n * dest_cpu are members of a throttled hierarchy when performing group\n * load-balance operations.\n */\nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\tstruct cfs_rq *src_cfs_rq, *dest_cfs_rq;\n\n\tsrc_cfs_rq = tg->cfs_rq[src_cpu];\n\tdest_cfs_rq = tg->cfs_rq[dest_cpu];\n\n\treturn throttled_hierarchy(src_cfs_rq) ||\n\t       throttled_hierarchy(dest_cfs_rq);\n}\n\nstatic int tg_unthrottle_up(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\tcfs_rq->throttle_count--;\n\tif (!cfs_rq->throttle_count) {\n\t\t/* adjust cfs_rq_clock_task() */\n\t\tcfs_rq->throttled_clock_task_time += rq_clock_task(rq) -\n\t\t\t\t\t     cfs_rq->throttled_clock_task;\n\t}\n\n\treturn 0;\n}\n\nstatic int tg_throttle_down(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t/* group is entering throttled state, stop time */\n\tif (!cfs_rq->throttle_count)\n\t\tcfs_rq->throttled_clock_task = rq_clock_task(rq);\n\tcfs_rq->throttle_count++;\n\n\treturn 0;\n}\n\nstatic void throttle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tlong task_delta, dequeue = 1;\n\tbool empty;\n\n\tse = cfs_rq->tg->se[cpu_of(rq_of(cfs_rq))];\n\n\t/* freeze hierarchy runnable averages while throttled */\n\trcu_read_lock();\n\twalk_tg_tree_from(cfs_rq->tg, tg_throttle_down, tg_nop, (void *)rq);\n\trcu_read_unlock();\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se);\n\t\t/* throttled entity or throttle-on-deactivate */\n\t\tif (!se->on_rq)\n\t\t\tbreak;\n\n\t\tif (dequeue)\n\t\t\tdequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP);\n\t\tqcfs_rq->h_nr_running -= task_delta;\n\n\t\tif (qcfs_rq->load.weight)\n\t\t\tdequeue = 0;\n\t}\n\n\tif (!se)\n\t\tsub_nr_running(rq, task_delta);\n\n\tcfs_rq->throttled = 1;\n\tcfs_rq->throttled_clock = rq_clock(rq);\n\traw_spin_lock(&cfs_b->lock);\n\tempty = list_empty(&cfs_b->throttled_cfs_rq);\n\n\t/*\n\t * Add to the _head_ of the list, so that an already-started\n\t * distribute_cfs_runtime will not see us. If disribute_cfs_runtime is\n\t * not running add to the tail so that later runqueues don't get starved.\n\t */\n\tif (cfs_b->distribute_running)\n\t\tlist_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);\n\telse\n\t\tlist_add_tail_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);\n\n\t/*\n\t * If we're the first throttled task, make sure the bandwidth\n\t * timer is running.\n\t */\n\tif (empty)\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\traw_spin_unlock(&cfs_b->lock);\n}\n\nvoid unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}\n\nstatic u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}\n\n/*\n * Responsible for refilling a task_group's bandwidth and unthrottling its\n * cfs_rqs as appropriate. If there has been no activity within the last\n * period the timer is deactivated until scheduling resumes; cfs_b->idle is\n * used to track this state.\n */\nstatic int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)\n{\n\tu64 runtime, runtime_expires;\n\tint throttled;\n\n\t/* no need to continue the timer with no bandwidth constraint */\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t/*\n\t * idle depends on !throttled (for the case of a large deficit), and if\n\t * we're going inactive then everything else can be deferred\n\t */\n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\tif (!throttled) {\n\t\t/* mark as potentially idle for the upcoming period */\n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t/* account preceding periods in which throttling occurred */\n\tcfs_b->nr_throttled += overrun;\n\n\truntime_expires = cfs_b->runtime_expires;\n\n\t/*\n\t * This check is repeated as we are holding onto the new bandwidth while\n\t * we unthrottle. This can potentially race with an unthrottled group\n\t * trying to acquire new bandwidth from the global pool. This can result\n\t * in us over-using our runtime if it is all used during this loop, but\n\t * only by limited amounts in that extreme case.\n\t */\n\twhile (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {\n\t\truntime = cfs_b->runtime;\n\t\tcfs_b->distribute_running = 1;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n\t\truntime = distribute_cfs_runtime(cfs_b, runtime,\n\t\t\t\t\t\t runtime_expires);\n\t\traw_spin_lock(&cfs_b->lock);\n\n\t\tcfs_b->distribute_running = 0;\n\t\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\n\t\tlsub_positive(&cfs_b->runtime, runtime);\n\t}\n\n\t/*\n\t * While we are ensured activity in the period following an\n\t * unthrottle, this also covers the case in which the new bandwidth is\n\t * insufficient to cover the existing bandwidth deficit.  (Forcing the\n\t * timer to remain active while there are any throttled entities.)\n\t */\n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}\n\n/* a cfs_rq won't donate quota below this amount */\nstatic const u64 min_cfs_rq_runtime = 1 * NSEC_PER_MSEC;\n/* minimum remaining period time to redistribute slack quota */\nstatic const u64 min_bandwidth_expiration = 2 * NSEC_PER_MSEC;\n/* how long we wait to gather additional slack before distributing */\nstatic const u64 cfs_bandwidth_slack_period = 5 * NSEC_PER_MSEC;\n\n/*\n * Are we near the end of the current quota period?\n *\n * Requires cfs_b->lock for hrtimer_expires_remaining to be safe against the\n * hrtimer base being cleared by hrtimer_start. In the case of\n * migrate_hrtimers, base is never cleared, so we are fine.\n */\nstatic int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\tu64 remaining;\n\n\t/* if the call-back is running a quota refresh is already occurring */\n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t/* is a quota refresh about to occur? */\n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 min_left = cfs_bandwidth_slack_period + min_bandwidth_expiration;\n\n\t/* if there's a quota refresh soon don't bother with slack */\n\tif (runtime_refresh_within(cfs_b, min_left))\n\t\treturn;\n\n\thrtimer_start(&cfs_b->slack_timer,\n\t\t\tns_to_ktime(cfs_bandwidth_slack_period),\n\t\t\tHRTIMER_MODE_REL);\n}\n\n/* we know any runtime found here is valid as update_curr() precedes return */\nstatic void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF &&\n\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t/* we are under rq->lock, defer unthrottling using a timer */\n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* even if it's not valid for return we don't want to try again */\n\tcfs_rq->runtime_remaining -= slack_runtime;\n}\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!cfs_rq->runtime_enabled || cfs_rq->nr_running)\n\t\treturn;\n\n\t__return_cfs_rq_runtime(cfs_rq);\n}\n\n/*\n * This is done with a timer (instead of inline with bandwidth return) since\n * it's necessary to juggle rq->locks to unthrottle their respective cfs_rqs.\n */\nstatic void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tu64 expires;\n\n\t/* confirm we're still not at a refresh boundary */\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->distribute_running) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\texpires = cfs_b->runtime_expires;\n\tif (runtime)\n\t\tcfs_b->distribute_running = 1;\n\n\traw_spin_unlock(&cfs_b->lock);\n\n\tif (!runtime)\n\t\treturn;\n\n\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (expires == cfs_b->runtime_expires)\n\t\tlsub_positive(&cfs_b->runtime, runtime);\n\tcfs_b->distribute_running = 0;\n\traw_spin_unlock(&cfs_b->lock);\n}\n\n/*\n * When a group wakes up we want to make sure that its quota is not already\n * expired/exceeded, otherwise it may be allowed to steal additional ticks of\n * runtime as update_curr() throttling can not not trigger until it's on-rq.\n */\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\t/* an active group must be handled by the update_curr()->put() path */\n\tif (!cfs_rq->runtime_enabled || cfs_rq->curr)\n\t\treturn;\n\n\t/* ensure the group is not already throttled */\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn;\n\n\t/* update runtime allocation */\n\taccount_cfs_rq_runtime(cfs_rq, 0);\n\tif (cfs_rq->runtime_remaining <= 0)\n\t\tthrottle_cfs_rq(cfs_rq);\n}\n\nstatic void sync_throttle(struct task_group *tg, int cpu)\n{\n\tstruct cfs_rq *pcfs_rq, *cfs_rq;\n\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!tg->parent)\n\t\treturn;\n\n\tcfs_rq = tg->cfs_rq[cpu];\n\tpcfs_rq = tg->parent->cfs_rq[cpu];\n\n\tcfs_rq->throttle_count = pcfs_rq->throttle_count;\n\tcfs_rq->throttled_clock_task = rq_clock_task(cpu_rq(cpu));\n}\n\n/* conditionally throttle active cfs_rq's from put_prev_entity() */\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn false;\n\n\tif (likely(!cfs_rq->runtime_enabled || cfs_rq->runtime_remaining > 0))\n\t\treturn false;\n\n\t/*\n\t * it's possible for a throttled entity to be forced into a running\n\t * state (e.g. set_curr_task), in this case we're finished.\n\t */\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn true;\n\n\tthrottle_cfs_rq(cfs_rq);\n\treturn true;\n}\n\nstatic enum hrtimer_restart sched_cfs_slack_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, slack_timer);\n\n\tdo_sched_cfs_slack_timer(cfs_b);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, period_timer);\n\tint overrun;\n\tint idle = 0;\n\n\traw_spin_lock(&cfs_b->lock);\n\tfor (;;) {\n\t\toverrun = hrtimer_forward_now(timer, cfs_b->period);\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_cfs_period_timer(cfs_b, overrun);\n\t}\n\tif (idle)\n\t\tcfs_b->period_active = 0;\n\traw_spin_unlock(&cfs_b->lock);\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}\n\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\traw_spin_lock_init(&cfs_b->lock);\n\tcfs_b->runtime = 0;\n\tcfs_b->quota = RUNTIME_INF;\n\tcfs_b->period = ns_to_ktime(default_cfs_period());\n\n\tINIT_LIST_HEAD(&cfs_b->throttled_cfs_rq);\n\thrtimer_init(&cfs_b->period_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);\n\tcfs_b->period_timer.function = sched_cfs_period_timer;\n\thrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tcfs_b->slack_timer.function = sched_cfs_slack_timer;\n\tcfs_b->distribute_running = 0;\n}\n\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->runtime_enabled = 0;\n\tINIT_LIST_HEAD(&cfs_rq->throttled_list);\n}\n\nvoid start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}\n\nstatic void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\t/* init_cfs_bandwidth() was not called */\n\tif (!cfs_b->throttled_cfs_rq.next)\n\t\treturn;\n\n\thrtimer_cancel(&cfs_b->period_timer);\n\thrtimer_cancel(&cfs_b->slack_timer);\n}\n\n/*\n * Both these CPU hotplug callbacks race against unregister_fair_sched_group()\n *\n * The race is harmless, since modifying bandwidth settings of unhooked group\n * bits doesn't do much.\n */\n\n/* cpu online calback */\nstatic void __maybe_unused update_runtime_enabled(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_held(&rq->lock);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\traw_spin_lock(&cfs_b->lock);\n\t\tcfs_rq->runtime_enabled = cfs_b->quota != RUNTIME_INF;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t}\n\trcu_read_unlock();\n}\n\n/* cpu offline callback */\nstatic void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_held(&rq->lock);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\tif (!cfs_rq->runtime_enabled)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * clock_task is not advancing so we just need to make sure\n\t\t * there's some valid quota amount\n\t\t */\n\t\tcfs_rq->runtime_remaining = 1;\n\t\t/*\n\t\t * Offline rq is schedulable till CPU is completely disabled\n\t\t * in take_cpu_down(), so we prevent new cfs throttling here.\n\t\t */\n\t\tcfs_rq->runtime_enabled = 0;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\t}\n\trcu_read_unlock();\n}\n\n#else /* CONFIG_CFS_BANDWIDTH */\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}\n\nstatic void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}\nstatic inline void sync_throttle(struct task_group *tg, int cpu) {}\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}\n\nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\treturn 0;\n}\n\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}\n#endif\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}\nstatic inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}\nstatic inline void update_runtime_enabled(struct rq *rq) {}\nstatic inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}\n\n#endif /* CONFIG_CFS_BANDWIDTH */\n\n/**************************************************\n * CFS operations on tasks:\n */\n\n#ifdef CONFIG_SCHED_HRTICK\nstatic void hrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tSCHED_WARN_ON(task_rq(p) != rq);\n\n\tif (rq->cfs.h_nr_running > 1) {\n\t\tu64 slice = sched_slice(cfs_rq, se);\n\t\tu64 ran = se->sum_exec_runtime - se->prev_sum_exec_runtime;\n\t\ts64 delta = slice - ran;\n\n\t\tif (delta < 0) {\n\t\t\tif (rq->curr == p)\n\t\t\t\tresched_curr(rq);\n\t\t\treturn;\n\t\t}\n\t\thrtick_start(rq, delta);\n\t}\n}\n\n/*\n * called from enqueue/dequeue and updates the hrtick when the\n * current task is from our class and nr_running is low enough\n * to matter.\n */\nstatic void hrtick_update(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\n\tif (!hrtick_enabled(rq) || curr->sched_class != &fair_sched_class)\n\t\treturn;\n\n\tif (cfs_rq_of(&curr->se)->nr_running < sched_nr_latency)\n\t\thrtick_start_fair(rq, curr);\n}\n#else /* !CONFIG_SCHED_HRTICK */\nstatic inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void hrtick_update(struct rq *rq)\n{\n}\n#endif\n\n#ifdef CONFIG_SMP\nstatic inline unsigned long cpu_util(int cpu);\nstatic unsigned long capacity_of(int cpu);\n\nstatic inline bool cpu_overutilized(int cpu)\n{\n\treturn (capacity_of(cpu) * 1024) < (cpu_util(cpu) * capacity_margin);\n}\n\nstatic inline void update_overutilized_status(struct rq *rq)\n{\n\tif (!READ_ONCE(rq->rd->overutilized) && cpu_overutilized(rq->cpu))\n\t\tWRITE_ONCE(rq->rd->overutilized, SG_OVERUTILIZED);\n}\n#else\nstatic inline void update_overutilized_status(struct rq *rq) { }\n#endif\n\n/*\n * The enqueue_task method is called before nr_running is\n * increased. Here we update the fair scheduling stats and\n * then put the task into the rbtree:\n */\nstatic void\nenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * The code below (indirectly) updates schedutil which looks at\n\t * the cfs_rq utilization to select a frequency.\n\t * Let's add the task's estimated utilization to the cfs_rq's\n\t * estimated utilization, before we update schedutil.\n\t */\n\tutil_est_enqueue(&rq->cfs, p);\n\n\t/*\n\t * If in_iowait is set, the code below may not trigger any cpufreq\n\t * utilization updates, so do it here explicitly with the IOWAIT flag\n\t * passed.\n\t */\n\tif (p->in_iowait)\n\t\tcpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT);\n\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tbreak;\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tenqueue_entity(cfs_rq, se, flags);\n\n\t\t/*\n\t\t * end evaluation on encountering a throttled cfs_rq\n\t\t *\n\t\t * note: in the case of encountering a throttled cfs_rq we will\n\t\t * post the final h_nr_running increment below.\n\t\t */\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t\tcfs_rq->h_nr_running++;\n\n\t\tflags = ENQUEUE_WAKEUP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_nr_running++;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tupdate_cfs_group(se);\n\t}\n\n\tif (!se) {\n\t\tadd_nr_running(rq, 1);\n\t\t/*\n\t\t * Since new tasks are assigned an initial util_avg equal to\n\t\t * half of the spare capacity of their CPU, tiny tasks have the\n\t\t * ability to cross the overutilized threshold, which will\n\t\t * result in the load balancer ruining all the task placement\n\t\t * done by EAS. As a way to mitigate that effect, do not account\n\t\t * for the first enqueue operation of new tasks during the\n\t\t * overutilized flag detection.\n\t\t *\n\t\t * A better way of solving this problem would be to wait for\n\t\t * the PELT signals of tasks to converge before taking them\n\t\t * into account, but that is not straightforward to implement,\n\t\t * and the following generally works well enough in practice.\n\t\t */\n\t\tif (flags & ENQUEUE_WAKEUP)\n\t\t\tupdate_overutilized_status(rq);\n\n\t}\n\n\thrtick_update(rq);\n}\n\nstatic void set_next_buddy(struct sched_entity *se);\n\n/*\n * The dequeue_task method is called before nr_running is\n * decreased. We remove the task from the rbtree and\n * update the fair scheduling stats:\n */\nstatic void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\tint task_sleep = flags & DEQUEUE_SLEEP;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tdequeue_entity(cfs_rq, se, flags);\n\n\t\t/*\n\t\t * end evaluation on encountering a throttled cfs_rq\n\t\t *\n\t\t * note: in the case of encountering a throttled cfs_rq we will\n\t\t * post the final h_nr_running decrement below.\n\t\t*/\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t\tcfs_rq->h_nr_running--;\n\n\t\t/* Don't dequeue parent if it has other entities besides us */\n\t\tif (cfs_rq->load.weight) {\n\t\t\t/* Avoid re-evaluating load for this entity: */\n\t\t\tse = parent_entity(se);\n\t\t\t/*\n\t\t\t * Bias pick_next to pick a task from this cfs_rq, as\n\t\t\t * p is sleeping when it is within its sched_slice.\n\t\t\t */\n\t\t\tif (task_sleep && se && !throttled_hierarchy(cfs_rq))\n\t\t\t\tset_next_buddy(se);\n\t\t\tbreak;\n\t\t}\n\t\tflags |= DEQUEUE_SLEEP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_nr_running--;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tupdate_cfs_group(se);\n\t}\n\n\tif (!se)\n\t\tsub_nr_running(rq, 1);\n\n\tutil_est_dequeue(&rq->cfs, p, task_sleep);\n\thrtick_update(rq);\n}\n\n#ifdef CONFIG_SMP\n\n/* Working cpumask for: load_balance, load_balance_newidle. */\nDEFINE_PER_CPU(cpumask_var_t, load_balance_mask);\nDEFINE_PER_CPU(cpumask_var_t, select_idle_mask);\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * per rq 'load' arrray crap; XXX kill this.\n */\n\n/*\n * The exact cpuload calculated at every tick would be:\n *\n *   load' = (1 - 1/2^i) * load + (1/2^i) * cur_load\n *\n * If a CPU misses updates for n ticks (as it was idle) and update gets\n * called on the n+1-th tick when CPU may be busy, then we have:\n *\n *   load_n   = (1 - 1/2^i)^n * load_0\n *   load_n+1 = (1 - 1/2^i)   * load_n + (1/2^i) * cur_load\n *\n * decay_load_missed() below does efficient calculation of\n *\n *   load' = (1 - 1/2^i)^n * load\n *\n * Because x^(n+m) := x^n * x^m we can decompose any x^n in power-of-2 factors.\n * This allows us to precompute the above in said factors, thereby allowing the\n * reduction of an arbitrary n in O(log_2 n) steps. (See also\n * fixed_power_int())\n *\n * The calculation is approximated on a 128 point scale.\n */\n#define DEGRADE_SHIFT\t\t7\n\nstatic const u8 degrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};\nstatic const u8 degrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {\n\t{   0,   0,  0,  0,  0,  0, 0, 0 },\n\t{  64,  32,  8,  0,  0,  0, 0, 0 },\n\t{  96,  72, 40, 12,  1,  0, 0, 0 },\n\t{ 112,  98, 75, 43, 15,  1, 0, 0 },\n\t{ 120, 112, 98, 76, 45, 16, 2, 0 }\n};\n\n/*\n * Update cpu_load for any missed ticks, due to tickless idle. The backlog\n * would be when CPU is idle and so we just decay the old load without\n * adding any new load.\n */\nstatic unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}\n\nstatic struct {\n\tcpumask_var_t idle_cpus_mask;\n\tatomic_t nr_cpus;\n\tint has_blocked;\t\t/* Idle CPUS has blocked load */\n\tunsigned long next_balance;     /* in jiffy units */\n\tunsigned long next_blocked;\t/* Next update of blocked load in jiffies */\n} nohz ____cacheline_aligned;\n\n#endif /* CONFIG_NO_HZ_COMMON */\n\n/**\n * __cpu_load_update - update the rq->cpu_load[] statistics\n * @this_rq: The rq to update statistics for\n * @this_load: The current load\n * @pending_updates: The number of missed updates\n *\n * Update rq->cpu_load[] statistics. This function is usually called every\n * scheduler tick (TICK_NSEC).\n *\n * This function computes a decaying average:\n *\n *   load[i]' = (1 - 1/2^i) * load[i] + (1/2^i) * load\n *\n * Because of NOHZ it might not get called on every tick which gives need for\n * the @pending_updates argument.\n *\n *   load[i]_n = (1 - 1/2^i) * load[i]_n-1 + (1/2^i) * load_n-1\n *             = A * load[i]_n-1 + B ; A := (1 - 1/2^i), B := (1/2^i) * load\n *             = A * (A * load[i]_n-2 + B) + B\n *             = A * (A * (A * load[i]_n-3 + B) + B) + B\n *             = A^3 * load[i]_n-3 + (A^2 + A + 1) * B\n *             = A^n * load[i]_0 + (A^(n-1) + A^(n-2) + ... + 1) * B\n *             = A^n * load[i]_0 + ((1 - A^n) / (1 - A)) * B\n *             = (1 - 1/2^i)^n * (load[i]_0 - load) + load\n *\n * In the above we've assumed load_n := load, which is true for NOHZ_FULL as\n * any change in load would have resulted in the tick being turned back on.\n *\n * For regular NOHZ, this reduces to:\n *\n *   load[i]_n = (1 - 1/2^i)^n * load[i]_0\n *\n * see decay_load_misses(). For NOHZ_FULL we get to subtract and add the extra\n * term.\n */\nstatic void cpu_load_update(struct rq *this_rq, unsigned long this_load,\n\t\t\t    unsigned long pending_updates)\n{\n\tunsigned long __maybe_unused tickless_load = this_rq->cpu_load[0];\n\tint i, scale;\n\n\tthis_rq->nr_load_updates++;\n\n\t/* Update our load: */\n\tthis_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */\n\tfor (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n\t\tunsigned long old_load, new_load;\n\n\t\t/* scale is effectively 1 << i now, and >> i divides by scale */\n\n\t\told_load = this_rq->cpu_load[i];\n#ifdef CONFIG_NO_HZ_COMMON\n\t\told_load = decay_load_missed(old_load, pending_updates - 1, i);\n\t\tif (tickless_load) {\n\t\t\told_load -= decay_load_missed(tickless_load, pending_updates - 1, i);\n\t\t\t/*\n\t\t\t * old_load can never be a negative value because a\n\t\t\t * decayed tickless_load cannot be greater than the\n\t\t\t * original tickless_load.\n\t\t\t */\n\t\t\told_load += tickless_load;\n\t\t}\n#endif\n\t\tnew_load = this_load;\n\t\t/*\n\t\t * Round up the averaging division if load is increasing. This\n\t\t * prevents us from getting stuck on 9 if the load is 10, for\n\t\t * example.\n\t\t */\n\t\tif (new_load > old_load)\n\t\t\tnew_load += scale - 1;\n\n\t\tthis_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;\n\t}\n}\n\n/* Used instead of source_load when we know the type == 0 */\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * There is no sane way to deal with nohz on smp when using jiffies because the\n * CPU doing the jiffies update might drift wrt the CPU doing the jiffy reading\n * causing off-by-one errors in observed deltas; {0,2} instead of {1,1}.\n *\n * Therefore we need to avoid the delta approach from the regular tick when\n * possible since that would seriously skew the load calculation. This is why we\n * use cpu_load_update_periodic() for CPUs out of nohz. However we'll rely on\n * jiffies deltas for updates happening while in nohz mode (idle ticks, idle\n * loop exit, nohz_idle_balance, nohz full exit...)\n *\n * This means we might still be one tick off for nohz periods.\n */\n\nstatic void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t unsigned long curr_jiffies,\n\t\t\t\t unsigned long load)\n{\n\tunsigned long pending_updates;\n\n\tpending_updates = curr_jiffies - this_rq->last_load_update_tick;\n\tif (pending_updates) {\n\t\tthis_rq->last_load_update_tick = curr_jiffies;\n\t\t/*\n\t\t * In the regular NOHZ case, we were idle, this means load 0.\n\t\t * In the NOHZ_FULL case, we were non-idle, we should consider\n\t\t * its weighted load.\n\t\t */\n\t\tcpu_load_update(this_rq, load, pending_updates);\n\t}\n}\n\n/*\n * Called from nohz_idle_balance() to update the load ratings before doing the\n * idle balance.\n */\nstatic void cpu_load_update_idle(struct rq *this_rq)\n{\n\t/*\n\t * bail if there's load or we're actually up-to-date.\n\t */\n\tif (weighted_cpuload(this_rq))\n\t\treturn;\n\n\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0);\n}\n\n/*\n * Record CPU load on nohz entry so we know the tickless load to account\n * on nohz exit. cpu_load[0] happens then to be updated more frequently\n * than other cpu_load[idx] but it should be fine as cpu_load readers\n * shouldn't rely into synchronized cpu_load[*] updates.\n */\nvoid cpu_load_update_nohz_start(void)\n{\n\tstruct rq *this_rq = this_rq();\n\n\t/*\n\t * This is all lockless but should be fine. If weighted_cpuload changes\n\t * concurrently we'll exit nohz. And cpu_load write can race with\n\t * cpu_load_update_idle() but both updater would be writing the same.\n\t */\n\tthis_rq->cpu_load[0] = weighted_cpuload(this_rq);\n}\n\n/*\n * Account the tickless load in the end of a nohz frame.\n */\nvoid cpu_load_update_nohz_stop(void)\n{\n\tunsigned long curr_jiffies = READ_ONCE(jiffies);\n\tstruct rq *this_rq = this_rq();\n\tunsigned long load;\n\tstruct rq_flags rf;\n\n\tif (curr_jiffies == this_rq->last_load_update_tick)\n\t\treturn;\n\n\tload = weighted_cpuload(this_rq);\n\trq_lock(this_rq, &rf);\n\tupdate_rq_clock(this_rq);\n\tcpu_load_update_nohz(this_rq, curr_jiffies, load);\n\trq_unlock(this_rq, &rf);\n}\n#else /* !CONFIG_NO_HZ_COMMON */\nstatic inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\nstatic void cpu_load_update_periodic(struct rq *this_rq, unsigned long load)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\t/* See the mess around cpu_load_update_nohz(). */\n\tthis_rq->last_load_update_tick = READ_ONCE(jiffies);\n#endif\n\tcpu_load_update(this_rq, load, 1);\n}\n\n/*\n * Called from scheduler_tick()\n */\nvoid cpu_load_update_active(struct rq *this_rq)\n{\n\tunsigned long load = weighted_cpuload(this_rq);\n\n\tif (tick_nohz_tick_stopped())\n\t\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), load);\n\telse\n\t\tcpu_load_update_periodic(this_rq, load);\n}\n\n/*\n * Return a low guess at the load of a migration-source CPU weighted\n * according to the scheduling class and \"nice\" value.\n *\n * We want to under-estimate the load of migration sources, to\n * balance conservatively.\n */\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}\n\n/*\n * Return a high guess at the load of a migration-target CPU weighted\n * according to the scheduling class and \"nice\" value.\n */\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}\n\nstatic unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}\n\nstatic unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = READ_ONCE(rq->cfs.h_nr_running);\n\tunsigned long load_avg = weighted_cpuload(rq);\n\n\tif (nr_running)\n\t\treturn load_avg / nr_running;\n\n\treturn 0;\n}\n\nstatic void record_wakee(struct task_struct *p)\n{\n\t/*\n\t * Only decay a single time; tasks that have less then 1 wakeup per\n\t * jiffy will not have built up many flips.\n\t */\n\tif (time_after(jiffies, current->wakee_flip_decay_ts + HZ)) {\n\t\tcurrent->wakee_flips >>= 1;\n\t\tcurrent->wakee_flip_decay_ts = jiffies;\n\t}\n\n\tif (current->last_wakee != p) {\n\t\tcurrent->last_wakee = p;\n\t\tcurrent->wakee_flips++;\n\t}\n}\n\n/*\n * Detect M:N waker/wakee relationships via a switching-frequency heuristic.\n *\n * A waker of many should wake a different task than the one last awakened\n * at a frequency roughly N times higher than one of its wakees.\n *\n * In order to determine whether we should let the load spread vs consolidating\n * to shared cache, we look for a minimum 'flip' frequency of llc_size in one\n * partner, and a factor of lls_size higher frequency in the other.\n *\n * With both conditions met, we can be relatively sure that the relationship is\n * non-monogamous, with partner count exceeding socket size.\n *\n * Waker/wakee being client/server, worker/dispatcher, interrupt source or\n * whatever is irrelevant, spread criteria is apparent partner count exceeds\n * socket size.\n */\nstatic int wake_wide(struct task_struct *p)\n{\n\tunsigned int master = current->wakee_flips;\n\tunsigned int slave = p->wakee_flips;\n\tint factor = this_cpu_read(sd_llc_size);\n\n\tif (master < slave)\n\t\tswap(master, slave);\n\tif (slave < factor || master < slave * factor)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * The purpose of wake_affine() is to quickly determine on which CPU we can run\n * soonest. For the purpose of speed we only consider the waking and previous\n * CPU.\n *\n * wake_affine_idle() - only considers 'now', it check if the waking CPU is\n *\t\t\tcache-affine and is (or\twill be) idle.\n *\n * wake_affine_weight() - considers the weight to reflect the average\n *\t\t\t  scheduling latency of the CPUs. This seems to work\n *\t\t\t  for the overloaded case.\n */\nstatic int\nwake_affine_idle(int this_cpu, int prev_cpu, int sync)\n{\n\t/*\n\t * If this_cpu is idle, it implies the wakeup is from interrupt\n\t * context. Only allow the move if cache is shared. Otherwise an\n\t * interrupt intensive workload could force all tasks onto one\n\t * node depending on the IO topology or IRQ affinity settings.\n\t *\n\t * If the prev_cpu is idle and cache affine then avoid a migration.\n\t * There is no guarantee that the cache hot data from an interrupt\n\t * is more important than cache hot data on the prev_cpu and from\n\t * a cpufreq perspective, it's better to have higher utilisation\n\t * on one CPU.\n\t */\n\tif (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))\n\t\treturn available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;\n\n\tif (sync && cpu_rq(this_cpu)->nr_running == 1)\n\t\treturn this_cpu;\n\n\treturn nr_cpumask_bits;\n}\n\nstatic int\nwake_affine_weight(struct sched_domain *sd, struct task_struct *p,\n\t\t   int this_cpu, int prev_cpu, int sync)\n{\n\ts64 this_eff_load, prev_eff_load;\n\tunsigned long task_load;\n\n\tthis_eff_load = target_load(this_cpu, sd->wake_idx);\n\n\tif (sync) {\n\t\tunsigned long current_load = task_h_load(current);\n\n\t\tif (current_load > this_eff_load)\n\t\t\treturn this_cpu;\n\n\t\tthis_eff_load -= current_load;\n\t}\n\n\ttask_load = task_h_load(p);\n\n\tthis_eff_load += task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tthis_eff_load *= 100;\n\tthis_eff_load *= capacity_of(prev_cpu);\n\n\tprev_eff_load = source_load(prev_cpu, sd->wake_idx);\n\tprev_eff_load -= task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;\n\tprev_eff_load *= capacity_of(this_cpu);\n\n\t/*\n\t * If sync, adjust the weight of prev_eff_load such that if\n\t * prev_eff == this_eff that select_idle_sibling() will consider\n\t * stacking the wakee on top of the waker if no other CPU is\n\t * idle.\n\t */\n\tif (sync)\n\t\tprev_eff_load += 1;\n\n\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;\n}\n\nstatic int wake_affine(struct sched_domain *sd, struct task_struct *p,\n\t\t       int this_cpu, int prev_cpu, int sync)\n{\n\tint target = nr_cpumask_bits;\n\n\tif (sched_feat(WA_IDLE))\n\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);\n\n\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)\n\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);\n\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine_attempts);\n\tif (target == nr_cpumask_bits)\n\t\treturn prev_cpu;\n\n\tschedstat_inc(sd->ttwu_move_affine);\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine);\n\treturn target;\n}\n\nstatic unsigned long cpu_util_without(int cpu, struct task_struct *p);\n\nstatic unsigned long capacity_spare_without(int cpu, struct task_struct *p)\n{\n\treturn max_t(long, capacity_of(cpu) - cpu_util_without(cpu, p), 0);\n}\n\n/*\n * find_idlest_group finds and returns the least busy CPU group within the\n * domain.\n *\n * Assumes p is allowed on at least one CPU in sd.\n */\nstatic struct sched_group *\nfind_idlest_group(struct sched_domain *sd, struct task_struct *p,\n\t\t  int this_cpu, int sd_flag)\n{\n\tstruct sched_group *idlest = NULL, *group = sd->groups;\n\tstruct sched_group *most_spare_sg = NULL;\n\tunsigned long min_runnable_load = ULONG_MAX;\n\tunsigned long this_runnable_load = ULONG_MAX;\n\tunsigned long min_avg_load = ULONG_MAX, this_avg_load = ULONG_MAX;\n\tunsigned long most_spare = 0, this_spare = 0;\n\tint load_idx = sd->forkexec_idx;\n\tint imbalance_scale = 100 + (sd->imbalance_pct-100)/2;\n\tunsigned long imbalance = scale_load_down(NICE_0_LOAD) *\n\t\t\t\t(sd->imbalance_pct-100) / 100;\n\n\tif (sd_flag & SD_BALANCE_WAKE)\n\t\tload_idx = sd->wake_idx;\n\n\tdo {\n\t\tunsigned long load, avg_load, runnable_load;\n\t\tunsigned long spare_cap, max_spare_cap;\n\t\tint local_group;\n\t\tint i;\n\n\t\t/* Skip over this group if it has no CPUs allowed */\n\t\tif (!cpumask_intersects(sched_group_span(group),\n\t\t\t\t\t&p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tlocal_group = cpumask_test_cpu(this_cpu,\n\t\t\t\t\t       sched_group_span(group));\n\n\t\t/*\n\t\t * Tally up the load of all CPUs in the group and find\n\t\t * the group containing the CPU with most spare capacity.\n\t\t */\n\t\tavg_load = 0;\n\t\trunnable_load = 0;\n\t\tmax_spare_cap = 0;\n\n\t\tfor_each_cpu(i, sched_group_span(group)) {\n\t\t\t/* Bias balancing toward CPUs of our domain */\n\t\t\tif (local_group)\n\t\t\t\tload = source_load(i, load_idx);\n\t\t\telse\n\t\t\t\tload = target_load(i, load_idx);\n\n\t\t\trunnable_load += load;\n\n\t\t\tavg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);\n\n\t\t\tspare_cap = capacity_spare_without(i, p);\n\n\t\t\tif (spare_cap > max_spare_cap)\n\t\t\t\tmax_spare_cap = spare_cap;\n\t\t}\n\n\t\t/* Adjust by relative CPU capacity of the group */\n\t\tavg_load = (avg_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\t\trunnable_load = (runnable_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\n\t\tif (local_group) {\n\t\t\tthis_runnable_load = runnable_load;\n\t\t\tthis_avg_load = avg_load;\n\t\t\tthis_spare = max_spare_cap;\n\t\t} else {\n\t\t\tif (min_runnable_load > (runnable_load + imbalance)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable load is significantly smaller\n\t\t\t\t * so we can pick this new CPU:\n\t\t\t\t */\n\t\t\t\tmin_runnable_load = runnable_load;\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t} else if ((runnable_load < (min_runnable_load + imbalance)) &&\n\t\t\t\t   (100*min_avg_load > imbalance_scale*avg_load)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable loads are close so take the\n\t\t\t\t * blocked load into account through avg_load:\n\t\t\t\t */\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t}\n\n\t\t\tif (most_spare < max_spare_cap) {\n\t\t\t\tmost_spare = max_spare_cap;\n\t\t\t\tmost_spare_sg = group;\n\t\t\t}\n\t\t}\n\t} while (group = group->next, group != sd->groups);\n\n\t/*\n\t * The cross-over point between using spare capacity or least load\n\t * is too conservative for high utilization tasks on partially\n\t * utilized systems if we require spare_capacity > task_util(p),\n\t * so we allow for some task stuffing by using\n\t * spare_capacity > task_util(p)/2.\n\t *\n\t * Spare capacity can't be used for fork because the utilization has\n\t * not been set yet, we must first select a rq to compute the initial\n\t * utilization.\n\t */\n\tif (sd_flag & SD_BALANCE_FORK)\n\t\tgoto skip_spare;\n\n\tif (this_spare > task_util(p) / 2 &&\n\t    imbalance_scale*this_spare > 100*most_spare)\n\t\treturn NULL;\n\n\tif (most_spare > task_util(p) / 2)\n\t\treturn most_spare_sg;\n\nskip_spare:\n\tif (!idlest)\n\t\treturn NULL;\n\n\t/*\n\t * When comparing groups across NUMA domains, it's possible for the\n\t * local domain to be very lightly loaded relative to the remote\n\t * domains but \"imbalance\" skews the comparison making remote CPUs\n\t * look much more favourable. When considering cross-domain, add\n\t * imbalance to the runnable load on the remote node and consider\n\t * staying local.\n\t */\n\tif ((sd->flags & SD_NUMA) &&\n\t    min_runnable_load + imbalance >= this_runnable_load)\n\t\treturn NULL;\n\n\tif (min_runnable_load > (this_runnable_load + imbalance))\n\t\treturn NULL;\n\n\tif ((this_runnable_load < (min_runnable_load + imbalance)) &&\n\t     (100*this_avg_load < imbalance_scale*min_avg_load))\n\t\treturn NULL;\n\n\treturn idlest;\n}\n\n/*\n * find_idlest_group_cpu - find the idlest CPU among the CPUs in the group.\n */\nstatic int\nfind_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)\n{\n\tunsigned long load, min_load = ULONG_MAX;\n\tunsigned int min_exit_latency = UINT_MAX;\n\tu64 latest_idle_timestamp = 0;\n\tint least_loaded_cpu = this_cpu;\n\tint shallowest_idle_cpu = -1;\n\tint i;\n\n\t/* Check if we have any choice: */\n\tif (group->group_weight == 1)\n\t\treturn cpumask_first(sched_group_span(group));\n\n\t/* Traverse only the allowed CPUs */\n\tfor_each_cpu_and(i, sched_group_span(group), &p->cpus_allowed) {\n\t\tif (available_idle_cpu(i)) {\n\t\t\tstruct rq *rq = cpu_rq(i);\n\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);\n\t\t\tif (idle && idle->exit_latency < min_exit_latency) {\n\t\t\t\t/*\n\t\t\t\t * We give priority to a CPU whose idle state\n\t\t\t\t * has the smallest exit latency irrespective\n\t\t\t\t * of any idle timestamp.\n\t\t\t\t */\n\t\t\t\tmin_exit_latency = idle->exit_latency;\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&\n\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {\n\t\t\t\t/*\n\t\t\t\t * If equal or no active idle state, then\n\t\t\t\t * the most recently idled CPU might have\n\t\t\t\t * a warmer cache.\n\t\t\t\t */\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t}\n\t\t} else if (shallowest_idle_cpu == -1) {\n\t\t\tload = weighted_cpuload(cpu_rq(i));\n\t\t\tif (load < min_load) {\n\t\t\t\tmin_load = load;\n\t\t\t\tleast_loaded_cpu = i;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;\n}\n\nstatic inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p,\n\t\t\t\t  int cpu, int prev_cpu, int sd_flag)\n{\n\tint new_cpu = cpu;\n\n\tif (!cpumask_intersects(sched_domain_span(sd), &p->cpus_allowed))\n\t\treturn prev_cpu;\n\n\t/*\n\t * We need task's util for capacity_spare_without, sync it up to\n\t * prev_cpu's last_update_time.\n\t */\n\tif (!(sd_flag & SD_BALANCE_FORK))\n\t\tsync_entity_load_avg(&p->se);\n\n\twhile (sd) {\n\t\tstruct sched_group *group;\n\t\tstruct sched_domain *tmp;\n\t\tint weight;\n\n\t\tif (!(sd->flags & sd_flag)) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgroup = find_idlest_group(sd, p, cpu, sd_flag);\n\t\tif (!group) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew_cpu = find_idlest_group_cpu(group, p, cpu);\n\t\tif (new_cpu == cpu) {\n\t\t\t/* Now try balancing at a lower domain level of 'cpu': */\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Now try balancing at a lower domain level of 'new_cpu': */\n\t\tcpu = new_cpu;\n\t\tweight = sd->span_weight;\n\t\tsd = NULL;\n\t\tfor_each_domain(cpu, tmp) {\n\t\t\tif (weight <= tmp->span_weight)\n\t\t\t\tbreak;\n\t\t\tif (tmp->flags & sd_flag)\n\t\t\t\tsd = tmp;\n\t\t}\n\t}\n\n\treturn new_cpu;\n}\n\n#ifdef CONFIG_SCHED_SMT\nDEFINE_STATIC_KEY_FALSE(sched_smt_present);\n\nstatic inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}\n\nstatic inline bool test_idle_cores(int cpu, bool def)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn def;\n}\n\n/*\n * Scans the local SMT mask to see if the entire core is idle, and records this\n * information in sd_llc_shared->has_idle_cores.\n *\n * Since SMT siblings share all cache levels, inspecting this limited remote\n * state should be fairly cheap.\n */\nvoid __update_idle_core(struct rq *rq)\n{\n\tint core = cpu_of(rq);\n\tint cpu;\n\n\trcu_read_lock();\n\tif (test_idle_cores(core, true))\n\t\tgoto unlock;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\tif (cpu == core)\n\t\t\tcontinue;\n\n\t\tif (!available_idle_cpu(cpu))\n\t\t\tgoto unlock;\n\t}\n\n\tset_idle_cores(core, 1);\nunlock:\n\trcu_read_unlock();\n}\n\n/*\n * Scan the entire LLC domain for idle cores; this dynamically switches off if\n * there are no idle cores left in the system; tracked through\n * sd_llc->shared->has_idle_cores and enabled through update_idle_core() above.\n */\nstatic int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask);\n\tint core, cpu;\n\n\tif (!static_branch_likely(&sched_smt_present))\n\t\treturn -1;\n\n\tif (!test_idle_cores(target, false))\n\t\treturn -1;\n\n\tcpumask_and(cpus, sched_domain_span(sd), &p->cpus_allowed);\n\n\tfor_each_cpu_wrap(core, cpus, target) {\n\t\tbool idle = true;\n\n\t\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\t\tcpumask_clear_cpu(cpu, cpus);\n\t\t\tif (!available_idle_cpu(cpu))\n\t\t\t\tidle = false;\n\t\t}\n\n\t\tif (idle)\n\t\t\treturn core;\n\t}\n\n\t/*\n\t * Failed to find an idle core; stop looking for one.\n\t */\n\tset_idle_cores(target, 0);\n\n\treturn -1;\n}\n\n/*\n * Scan the local SMT mask for idle CPUs.\n */\nstatic int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tint cpu;\n\n\tif (!static_branch_likely(&sched_smt_present))\n\t\treturn -1;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(target)) {\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\treturn cpu;\n\t}\n\n\treturn -1;\n}\n\n#else /* CONFIG_SCHED_SMT */\n\nstatic inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}\n\nstatic inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}\n\n#endif /* CONFIG_SCHED_SMT */\n\n/*\n * Scan the LLC domain for idle CPUs; this is dynamically regulated by\n * comparing the average scan cost (tracked in sd->avg_scan_cost) against the\n * average idle time for this rq (as found in rq->avg_idle).\n */\nstatic int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct sched_domain *this_sd;\n\tu64 avg_cost, avg_idle;\n\tu64 time, cost;\n\ts64 delta;\n\tint cpu, nr = INT_MAX;\n\n\tthis_sd = rcu_dereference(*this_cpu_ptr(&sd_llc));\n\tif (!this_sd)\n\t\treturn -1;\n\n\t/*\n\t * Due to large variance we need a large fuzz factor; hackbench in\n\t * particularly is sensitive here.\n\t */\n\tavg_idle = this_rq()->avg_idle / 512;\n\tavg_cost = this_sd->avg_scan_cost + 1;\n\n\tif (sched_feat(SIS_AVG_CPU) && avg_idle < avg_cost)\n\t\treturn -1;\n\n\tif (sched_feat(SIS_PROP)) {\n\t\tu64 span_avg = sd->span_weight * avg_idle;\n\t\tif (span_avg > 4*avg_cost)\n\t\t\tnr = div_u64(span_avg, avg_cost);\n\t\telse\n\t\t\tnr = 4;\n\t}\n\n\ttime = local_clock();\n\n\tfor_each_cpu_wrap(cpu, sched_domain_span(sd), target) {\n\t\tif (!--nr)\n\t\t\treturn -1;\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\tbreak;\n\t}\n\n\ttime = local_clock() - time;\n\tcost = this_sd->avg_scan_cost;\n\tdelta = (s64)(time - cost) / 8;\n\tthis_sd->avg_scan_cost += delta;\n\n\treturn cpu;\n}\n\n/*\n * Try and locate an idle core/thread in the LLC cache domain.\n */\nstatic int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tstruct sched_domain *sd;\n\tint i, recent_used_cpu;\n\n\tif (available_idle_cpu(target))\n\t\treturn target;\n\n\t/*\n\t * If the previous CPU is cache affine and idle, don't be stupid:\n\t */\n\tif (prev != target && cpus_share_cache(prev, target) && available_idle_cpu(prev))\n\t\treturn prev;\n\n\t/* Check a recently used CPU as a potential idle candidate: */\n\trecent_used_cpu = p->recent_used_cpu;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    available_idle_cpu(recent_used_cpu) &&\n\t    cpumask_test_cpu(p->recent_used_cpu, &p->cpus_allowed)) {\n\t\t/*\n\t\t * Replace recent_used_cpu with prev as it is a potential\n\t\t * candidate for the next wake:\n\t\t */\n\t\tp->recent_used_cpu = prev;\n\t\treturn recent_used_cpu;\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\ti = select_idle_core(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_cpu(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_smt(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}\n\n/**\n * Amount of capacity of a CPU that is (estimated to be) used by CFS tasks\n * @cpu: the CPU to get the utilization of\n *\n * The unit of the return value must be the one of capacity so we can compare\n * the utilization with the capacity of the CPU that is available for CFS task\n * (ie cpu_capacity).\n *\n * cfs_rq.avg.util_avg is the sum of running time of runnable tasks plus the\n * recent utilization of currently non-runnable tasks on a CPU. It represents\n * the amount of utilization of a CPU in the range [0..capacity_orig] where\n * capacity_orig is the cpu_capacity available at the highest frequency\n * (arch_scale_freq_capacity()).\n * The utilization of a CPU converges towards a sum equal to or less than the\n * current capacity (capacity_curr <= capacity_orig) of the CPU because it is\n * the running time on this CPU scaled by capacity_curr.\n *\n * The estimated utilization of a CPU is defined to be the maximum between its\n * cfs_rq.avg.util_avg and the sum of the estimated utilization of the tasks\n * currently RUNNABLE on that CPU.\n * This allows to properly represent the expected utilization of a CPU which\n * has just got a big task running since a long sleep period. At the same time\n * however it preserves the benefits of the \"blocked utilization\" in\n * describing the potential for other tasks waking up on the same CPU.\n *\n * Nevertheless, cfs_rq.avg.util_avg can be higher than capacity_curr or even\n * higher than capacity_orig because of unfortunate rounding in\n * cfs.avg.util_avg or just after migrating tasks and new task wakeups until\n * the average stabilizes with the new running time. We need to check that the\n * utilization stays within the range of [0..capacity_orig] and cap it if\n * necessary. Without utilization capping, a group could be seen as overloaded\n * (CPU0 utilization at 121% + CPU1 utilization at 80%) whereas CPU1 has 20% of\n * available capacity. We allow utilization to overshoot capacity_curr (but not\n * capacity_orig) as it useful for predicting the capacity required after task\n * migrations (scheduler-driven DVFS).\n *\n * Return: the (estimated) utilization for the specified CPU\n */\nstatic inline unsigned long cpu_util(int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}\n\n/*\n * cpu_util_without: compute cpu utilization without any contributions from *p\n * @cpu: the CPU which utilization is requested\n * @p: the task which utilization should be discounted\n *\n * The utilization of a CPU is defined by the utilization of tasks currently\n * enqueued on that CPU as well as tasks which are currently sleeping after an\n * execution on that CPU.\n *\n * This method returns the utilization of the specified CPU by discounting the\n * utilization of the specified task, whenever the task is currently\n * contributing to the CPU utilization.\n */\nstatic unsigned long cpu_util_without(int cpu, struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\t/* Task has no contribution or is new */\n\tif (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn cpu_util(cpu);\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\t/* Discount task's util from CPU's util */\n\tlsub_positive(&util, task_util(p));\n\n\t/*\n\t * Covered cases:\n\t *\n\t * a) if *p is the only task sleeping on this CPU, then:\n\t *      cpu_util (== task_util) > util_est (== 0)\n\t *    and thus we return:\n\t *      cpu_util_without = (cpu_util - task_util) = 0\n\t *\n\t * b) if other tasks are SLEEPING on this CPU, which is now exiting\n\t *    IDLE, then:\n\t *      cpu_util >= task_util\n\t *      cpu_util > util_est (== 0)\n\t *    and thus we discount *p's blocked utilization to return:\n\t *      cpu_util_without = (cpu_util - task_util) >= 0\n\t *\n\t * c) if other tasks are RUNNABLE on that CPU and\n\t *      util_est > cpu_util\n\t *    then we use util_est since it returns a more restrictive\n\t *    estimation of the spare capacity on that CPU, by just\n\t *    considering the expected utilization of tasks already\n\t *    runnable on that CPU.\n\t *\n\t * Cases a) and b) are covered by the above code, while case c) is\n\t * covered by the following code when estimated utilization is\n\t * enabled.\n\t */\n\tif (sched_feat(UTIL_EST)) {\n\t\tunsigned int estimated =\n\t\t\tREAD_ONCE(cfs_rq->avg.util_est.enqueued);\n\n\t\t/*\n\t\t * Despite the following checks we still have a small window\n\t\t * for a possible race, when an execl's select_task_rq_fair()\n\t\t * races with LB's detach_task():\n\t\t *\n\t\t *   detach_task()\n\t\t *     p->on_rq = TASK_ON_RQ_MIGRATING;\n\t\t *     ---------------------------------- A\n\t\t *     deactivate_task()                   \\\n\t\t *       dequeue_task()                     + RaceTime\n\t\t *         util_est_dequeue()              /\n\t\t *     ---------------------------------- B\n\t\t *\n\t\t * The additional check on \"current == p\" it's required to\n\t\t * properly fix the execl regression and it helps in further\n\t\t * reducing the chances for the above race.\n\t\t */\n\t\tif (unlikely(task_on_rq_queued(p) || current == p))\n\t\t\tlsub_positive(&estimated, _task_util_est(p));\n\n\t\tutil = max(util, estimated);\n\t}\n\n\t/*\n\t * Utilization (estimated) can exceed the CPU capacity, thus let's\n\t * clamp to the maximum CPU capacity to ensure consistency with\n\t * the cpu_util call.\n\t */\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}\n\n/*\n * Disable WAKE_AFFINE in the case where task @p doesn't fit in the\n * capacity of either the waking CPU @cpu or the previous CPU @prev_cpu.\n *\n * In that case WAKE_AFFINE doesn't make sense and we'll let\n * BALANCE_WAKE sort things out.\n */\nstatic int wake_cap(struct task_struct *p, int cpu, int prev_cpu)\n{\n\tlong min_cap, max_cap;\n\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn 0;\n\n\tmin_cap = min(capacity_orig_of(prev_cpu), capacity_orig_of(cpu));\n\tmax_cap = cpu_rq(cpu)->rd->max_cpu_capacity;\n\n\t/* Minimum capacity is close to max, no need to abort wake_affine */\n\tif (max_cap - min_cap < max_cap >> 3)\n\t\treturn 0;\n\n\t/* Bring task utilization in sync with prev_cpu */\n\tsync_entity_load_avg(&p->se);\n\n\treturn !task_fits_capacity(p, min_cap);\n}\n\n/*\n * Predicts what cpu_util(@cpu) would return if @p was migrated (and enqueued)\n * to @dst_cpu.\n */\nstatic unsigned long cpu_util_next(int cpu, struct task_struct *p, int dst_cpu)\n{\n\tstruct cfs_rq *cfs_rq = &cpu_rq(cpu)->cfs;\n\tunsigned long util_est, util = READ_ONCE(cfs_rq->avg.util_avg);\n\n\t/*\n\t * If @p migrates from @cpu to another, remove its contribution. Or,\n\t * if @p migrates from another CPU to @cpu, add its contribution. In\n\t * the other cases, @cpu is not impacted by the migration, so the\n\t * util_avg should already be correct.\n\t */\n\tif (task_cpu(p) == cpu && dst_cpu != cpu)\n\t\tsub_positive(&util, task_util(p));\n\telse if (task_cpu(p) != cpu && dst_cpu == cpu)\n\t\tutil += task_util(p);\n\n\tif (sched_feat(UTIL_EST)) {\n\t\tutil_est = READ_ONCE(cfs_rq->avg.util_est.enqueued);\n\n\t\t/*\n\t\t * During wake-up, the task isn't enqueued yet and doesn't\n\t\t * appear in the cfs_rq->avg.util_est.enqueued of any rq,\n\t\t * so just add it (if needed) to \"simulate\" what will be\n\t\t * cpu_util() after the task has been enqueued.\n\t\t */\n\t\tif (dst_cpu == cpu)\n\t\t\tutil_est += _task_util_est(p);\n\n\t\tutil = max(util, util_est);\n\t}\n\n\treturn min(util, capacity_orig_of(cpu));\n}\n\n/*\n * compute_energy(): Estimates the energy that would be consumed if @p was\n * migrated to @dst_cpu. compute_energy() predicts what will be the utilization\n * landscape of the * CPUs after the task migration, and uses the Energy Model\n * to compute what would be the energy if we decided to actually migrate that\n * task.\n */\nstatic long\ncompute_energy(struct task_struct *p, int dst_cpu, struct perf_domain *pd)\n{\n\tlong util, max_util, sum_util, energy = 0;\n\tint cpu;\n\n\tfor (; pd; pd = pd->next) {\n\t\tmax_util = sum_util = 0;\n\t\t/*\n\t\t * The capacity state of CPUs of the current rd can be driven by\n\t\t * CPUs of another rd if they belong to the same performance\n\t\t * domain. So, account for the utilization of these CPUs too\n\t\t * by masking pd with cpu_online_mask instead of the rd span.\n\t\t *\n\t\t * If an entire performance domain is outside of the current rd,\n\t\t * it will not appear in its pd list and will not be accounted\n\t\t * by compute_energy().\n\t\t */\n\t\tfor_each_cpu_and(cpu, perf_domain_span(pd), cpu_online_mask) {\n\t\t\tutil = cpu_util_next(cpu, p, dst_cpu);\n\t\t\tutil = schedutil_energy_util(cpu, util);\n\t\t\tmax_util = max(util, max_util);\n\t\t\tsum_util += util;\n\t\t}\n\n\t\tenergy += em_pd_energy(pd->em_pd, max_util, sum_util);\n\t}\n\n\treturn energy;\n}\n\n/*\n * find_energy_efficient_cpu(): Find most energy-efficient target CPU for the\n * waking task. find_energy_efficient_cpu() looks for the CPU with maximum\n * spare capacity in each performance domain and uses it as a potential\n * candidate to execute the task. Then, it uses the Energy Model to figure\n * out which of the CPU candidates is the most energy-efficient.\n *\n * The rationale for this heuristic is as follows. In a performance domain,\n * all the most energy efficient CPU candidates (according to the Energy\n * Model) are those for which we'll request a low frequency. When there are\n * several CPUs for which the frequency request will be the same, we don't\n * have enough data to break the tie between them, because the Energy Model\n * only includes active power costs. With this model, if we assume that\n * frequency requests follow utilization (e.g. using schedutil), the CPU with\n * the maximum spare capacity in a performance domain is guaranteed to be among\n * the best candidates of the performance domain.\n *\n * In practice, it could be preferable from an energy standpoint to pack\n * small tasks on a CPU in order to let other CPUs go in deeper idle states,\n * but that could also hurt our chances to go cluster idle, and we have no\n * ways to tell with the current Energy Model if this is actually a good\n * idea or not. So, find_energy_efficient_cpu() basically favors\n * cluster-packing, and spreading inside a cluster. That should at least be\n * a good thing for latency, and this is consistent with the idea that most\n * of the energy savings of EAS come from the asymmetry of the system, and\n * not so much from breaking the tie between identical CPUs. That's also the\n * reason why EAS is enabled in the topology code only for systems where\n * SD_ASYM_CPUCAPACITY is set.\n *\n * NOTE: Forkees are not accepted in the energy-aware wake-up path because\n * they don't have any useful utilization data yet and it's not possible to\n * forecast their impact on energy consumption. Consequently, they will be\n * placed by find_idlest_cpu() on the least loaded CPU, which might turn out\n * to be energy-inefficient in some use-cases. The alternative would be to\n * bias new tasks towards specific types of CPUs first, or to try to infer\n * their util_avg from the parent task, but those heuristics could hurt\n * other use-cases too. So, until someone finds a better way to solve this,\n * let's keep things simple by re-using the existing slow path.\n */\n\nstatic int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)\n{\n\tunsigned long prev_energy = ULONG_MAX, best_energy = ULONG_MAX;\n\tstruct root_domain *rd = cpu_rq(smp_processor_id())->rd;\n\tint cpu, best_energy_cpu = prev_cpu;\n\tstruct perf_domain *head, *pd;\n\tunsigned long cpu_cap, util;\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tpd = rcu_dereference(rd->pd);\n\tif (!pd || READ_ONCE(rd->overutilized))\n\t\tgoto fail;\n\thead = pd;\n\n\t/*\n\t * Energy-aware wake-up happens on the lowest sched_domain starting\n\t * from sd_asym_cpucapacity spanning over this_cpu and prev_cpu.\n\t */\n\tsd = rcu_dereference(*this_cpu_ptr(&sd_asym_cpucapacity));\n\twhile (sd && !cpumask_test_cpu(prev_cpu, sched_domain_span(sd)))\n\t\tsd = sd->parent;\n\tif (!sd)\n\t\tgoto fail;\n\n\tsync_entity_load_avg(&p->se);\n\tif (!task_util_est(p))\n\t\tgoto unlock;\n\n\tfor (; pd; pd = pd->next) {\n\t\tunsigned long cur_energy, spare_cap, max_spare_cap = 0;\n\t\tint max_spare_cap_cpu = -1;\n\n\t\tfor_each_cpu_and(cpu, perf_domain_span(pd), sched_domain_span(sd)) {\n\t\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\t\tcontinue;\n\n\t\t\t/* Skip CPUs that will be overutilized. */\n\t\t\tutil = cpu_util_next(cpu, p, cpu);\n\t\t\tcpu_cap = capacity_of(cpu);\n\t\t\tif (cpu_cap * 1024 < util * capacity_margin)\n\t\t\t\tcontinue;\n\n\t\t\t/* Always use prev_cpu as a candidate. */\n\t\t\tif (cpu == prev_cpu) {\n\t\t\t\tprev_energy = compute_energy(p, prev_cpu, head);\n\t\t\t\tbest_energy = min(best_energy, prev_energy);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Find the CPU with the maximum spare capacity in\n\t\t\t * the performance domain\n\t\t\t */\n\t\t\tspare_cap = cpu_cap - util;\n\t\t\tif (spare_cap > max_spare_cap) {\n\t\t\t\tmax_spare_cap = spare_cap;\n\t\t\t\tmax_spare_cap_cpu = cpu;\n\t\t\t}\n\t\t}\n\n\t\t/* Evaluate the energy impact of using this CPU. */\n\t\tif (max_spare_cap_cpu >= 0) {\n\t\t\tcur_energy = compute_energy(p, max_spare_cap_cpu, head);\n\t\t\tif (cur_energy < best_energy) {\n\t\t\t\tbest_energy = cur_energy;\n\t\t\t\tbest_energy_cpu = max_spare_cap_cpu;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\n\n\t/*\n\t * Pick the best CPU if prev_cpu cannot be used, or if it saves at\n\t * least 6% of the energy used by prev_cpu.\n\t */\n\tif (prev_energy == ULONG_MAX)\n\t\treturn best_energy_cpu;\n\n\tif ((prev_energy - best_energy) > (prev_energy >> 4))\n\t\treturn best_energy_cpu;\n\n\treturn prev_cpu;\n\nfail:\n\trcu_read_unlock();\n\n\treturn -1;\n}\n\n/*\n * select_task_rq_fair: Select target runqueue for the waking task in domains\n * that have the 'sd_flag' flag set. In practice, this is SD_BALANCE_WAKE,\n * SD_BALANCE_FORK, or SD_BALANCE_EXEC.\n *\n * Balances load by selecting the idlest CPU in the idlest group, or under\n * certain conditions an idle sibling CPU if the domain has SD_WAKE_AFFINE set.\n *\n * Returns the target CPU number.\n *\n * preempt must be disabled.\n */\nstatic int\nselect_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)\n{\n\tstruct sched_domain *tmp, *sd = NULL;\n\tint cpu = smp_processor_id();\n\tint new_cpu = prev_cpu;\n\tint want_affine = 0;\n\tint sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);\n\n\tif (sd_flag & SD_BALANCE_WAKE) {\n\t\trecord_wakee(p);\n\n\t\tif (static_branch_unlikely(&sched_energy_present)) {\n\t\t\tnew_cpu = find_energy_efficient_cpu(p, prev_cpu);\n\t\t\tif (new_cpu >= 0)\n\t\t\t\treturn new_cpu;\n\t\t\tnew_cpu = prev_cpu;\n\t\t}\n\n\t\twant_affine = !wake_wide(p) && !wake_cap(p, cpu, prev_cpu) &&\n\t\t\t      cpumask_test_cpu(cpu, &p->cpus_allowed);\n\t}\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, tmp) {\n\t\tif (!(tmp->flags & SD_LOAD_BALANCE))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If both 'cpu' and 'prev_cpu' are part of this domain,\n\t\t * cpu is a valid SD_WAKE_AFFINE target.\n\t\t */\n\t\tif (want_affine && (tmp->flags & SD_WAKE_AFFINE) &&\n\t\t    cpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) {\n\t\t\tif (cpu != prev_cpu)\n\t\t\t\tnew_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync);\n\n\t\t\tsd = NULL; /* Prefer wake_affine over balance flags */\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tmp->flags & sd_flag)\n\t\t\tsd = tmp;\n\t\telse if (!want_affine)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(sd)) {\n\t\t/* Slow path */\n\t\tnew_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag);\n\t} else if (sd_flag & SD_BALANCE_WAKE) { /* XXX always ? */\n\t\t/* Fast path */\n\n\t\tnew_cpu = select_idle_sibling(p, prev_cpu, new_cpu);\n\n\t\tif (want_affine)\n\t\t\tcurrent->recent_used_cpu = cpu;\n\t}\n\trcu_read_unlock();\n\n\treturn new_cpu;\n}\n\nstatic void detach_entity_cfs_rq(struct sched_entity *se);\n\n/*\n * Called immediately before a task is migrated to a new CPU; task_cpu(p) and\n * cfs_rq_of(p) references at time of call are still valid and identify the\n * previous CPU. The caller guarantees p->pi_lock or task_rq(p)->lock is held.\n */\nstatic void migrate_task_rq_fair(struct task_struct *p, int new_cpu)\n{\n\t/*\n\t * As blocked tasks retain absolute vruntime the migration needs to\n\t * deal with this by subtracting the old and adding the new\n\t * min_vruntime -- the latter is done by enqueue_entity() when placing\n\t * the task on the new runqueue.\n\t */\n\tif (p->state == TASK_WAKING) {\n\t\tstruct sched_entity *se = &p->se;\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tu64 min_vruntime;\n\n#ifndef CONFIG_64BIT\n\t\tu64 min_vruntime_copy;\n\n\t\tdo {\n\t\t\tmin_vruntime_copy = cfs_rq->min_vruntime_copy;\n\t\t\tsmp_rmb();\n\t\t\tmin_vruntime = cfs_rq->min_vruntime;\n\t\t} while (min_vruntime != min_vruntime_copy);\n#else\n\t\tmin_vruntime = cfs_rq->min_vruntime;\n#endif\n\n\t\tse->vruntime -= min_vruntime;\n\t}\n\n\tif (p->on_rq == TASK_ON_RQ_MIGRATING) {\n\t\t/*\n\t\t * In case of TASK_ON_RQ_MIGRATING we in fact hold the 'old'\n\t\t * rq->lock and can modify state directly.\n\t\t */\n\t\tlockdep_assert_held(&task_rq(p)->lock);\n\t\tdetach_entity_cfs_rq(&p->se);\n\n\t} else {\n\t\t/*\n\t\t * We are supposed to update the task to \"current\" time, then\n\t\t * its up to date and ready to go to new CPU/cfs_rq. But we\n\t\t * have difficulty in getting what current time is, so simply\n\t\t * throw away the out-of-date time. This will result in the\n\t\t * wakee task is less decayed, but giving the wakee more load\n\t\t * sounds not bad.\n\t\t */\n\t\tremove_entity_load_avg(&p->se);\n\t}\n\n\t/* Tell new CPU we are migrated */\n\tp->se.avg.last_update_time = 0;\n\n\t/* We have migrated, no longer consider this task hot */\n\tp->se.exec_start = 0;\n\n\tupdate_scan_period(p, new_cpu);\n}\n\nstatic void task_dead_fair(struct task_struct *p)\n{\n\tremove_entity_load_avg(&p->se);\n}\n#endif /* CONFIG_SMP */\n\nstatic unsigned long wakeup_gran(struct sched_entity *se)\n{\n\tunsigned long gran = sysctl_sched_wakeup_granularity;\n\n\t/*\n\t * Since its curr running now, convert the gran from real-time\n\t * to virtual-time in his units.\n\t *\n\t * By using 'se' instead of 'curr' we penalize light tasks, so\n\t * they get preempted easier. That is, if 'se' < 'curr' then\n\t * the resulting gran will be larger, therefore penalizing the\n\t * lighter, if otoh 'se' > 'curr' then the resulting gran will\n\t * be smaller, again penalizing the lighter task.\n\t *\n\t * This is especially important for buddies when the leftmost\n\t * task is higher priority than the buddy.\n\t */\n\treturn calc_delta_fair(gran, se);\n}\n\n/*\n * Should 'se' preempt 'curr'.\n *\n *             |s1\n *        |s2\n *   |s3\n *         g\n *      |<--->|c\n *\n *  w(c, s1) = -1\n *  w(c, s2) =  0\n *  w(c, s3) =  1\n *\n */\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)\n{\n\ts64 gran, vdiff = curr->vruntime - se->vruntime;\n\n\tif (vdiff <= 0)\n\t\treturn -1;\n\n\tgran = wakeup_gran(se);\n\tif (vdiff > gran)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void set_last_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_has_idle_policy(task_of(se))))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->last = se;\n\t}\n}\n\nstatic void set_next_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_has_idle_policy(task_of(se))))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}\n\nstatic void set_skip_buddy(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se)\n\t\tcfs_rq_of(se)->skip = se;\n}\n\n/*\n * Preempt the current task with a newly woken task if needed:\n */\nstatic void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct sched_entity *se = &curr->se, *pse = &p->se;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tint scale = cfs_rq->nr_running >= sched_nr_latency;\n\tint next_buddy_marked = 0;\n\n\tif (unlikely(se == pse))\n\t\treturn;\n\n\t/*\n\t * This is possible from callers such as attach_tasks(), in which we\n\t * unconditionally check_prempt_curr() after an enqueue (which may have\n\t * lead to a throttle).  This both saves work and prevents false\n\t * next-buddy nomination below.\n\t */\n\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))\n\t\treturn;\n\n\tif (sched_feat(NEXT_BUDDY) && scale && !(wake_flags & WF_FORK)) {\n\t\tset_next_buddy(pse);\n\t\tnext_buddy_marked = 1;\n\t}\n\n\t/*\n\t * We can come here with TIF_NEED_RESCHED already set from new task\n\t * wake up path.\n\t *\n\t * Note: this also catches the edge-case of curr being in a throttled\n\t * group (e.g. via set_curr_task), since update_curr() (in the\n\t * enqueue of curr) will have resulted in resched being set.  This\n\t * prevents us from potentially nominating it as a false LAST_BUDDY\n\t * below.\n\t */\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\t/* Idle tasks are by definition preempted by non-idle tasks. */\n\tif (unlikely(task_has_idle_policy(curr)) &&\n\t    likely(!task_has_idle_policy(p)))\n\t\tgoto preempt;\n\n\t/*\n\t * Batch and idle tasks do not preempt non-idle tasks (their preemption\n\t * is driven by the tick):\n\t */\n\tif (unlikely(p->policy != SCHED_NORMAL) || !sched_feat(WAKEUP_PREEMPTION))\n\t\treturn;\n\n\tfind_matching_se(&se, &pse);\n\tupdate_curr(cfs_rq_of(se));\n\tBUG_ON(!pse);\n\tif (wakeup_preempt_entity(se, pse) == 1) {\n\t\t/*\n\t\t * Bias pick_next to pick the sched entity that is\n\t\t * triggering this preemption.\n\t\t */\n\t\tif (!next_buddy_marked)\n\t\t\tset_next_buddy(pse);\n\t\tgoto preempt;\n\t}\n\n\treturn;\n\npreempt:\n\tresched_curr(rq);\n\t/*\n\t * Only set the backward buddy when the current task is still\n\t * on the rq. This can happen when a wakeup gets interleaved\n\t * with schedule on the ->pre_schedule() or idle_balance()\n\t * point, either of which can * drop the rq lock.\n\t *\n\t * Also, during early boot the idle thread is in the fair class,\n\t * for obvious reasons its a bad idea to schedule back to it.\n\t */\n\tif (unlikely(!se->on_rq || curr == rq->idle))\n\t\treturn;\n\n\tif (sched_feat(LAST_BUDDY) && scale && entity_is_task(se))\n\t\tset_last_buddy(se);\n}\n\nstatic struct task_struct *\npick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tstruct sched_entity *se;\n\tstruct task_struct *p;\n\tint new_tasks;\n\nagain:\n\tif (!cfs_rq->nr_running)\n\t\tgoto idle;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (prev->sched_class != &fair_sched_class)\n\t\tgoto simple;\n\n\t/*\n\t * Because of the set_next_buddy() in dequeue_task_fair() it is rather\n\t * likely that a next task is from the same cgroup as the current.\n\t *\n\t * Therefore attempt to avoid putting and setting the entire cgroup\n\t * hierarchy, only change the part that actually changes.\n\t */\n\n\tdo {\n\t\tstruct sched_entity *curr = cfs_rq->curr;\n\n\t\t/*\n\t\t * Since we got here without doing put_prev_entity() we also\n\t\t * have to consider cfs_rq->curr. If it is still a runnable\n\t\t * entity, update_curr() will update its vruntime, otherwise\n\t\t * forget we've ever seen it.\n\t\t */\n\t\tif (curr) {\n\t\t\tif (curr->on_rq)\n\t\t\t\tupdate_curr(cfs_rq);\n\t\t\telse\n\t\t\t\tcurr = NULL;\n\n\t\t\t/*\n\t\t\t * This call to check_cfs_rq_runtime() will do the\n\t\t\t * throttle and dequeue its entity in the parent(s).\n\t\t\t * Therefore the nr_running test will indeed\n\t\t\t * be correct.\n\t\t\t */\n\t\t\tif (unlikely(check_cfs_rq_runtime(cfs_rq))) {\n\t\t\t\tcfs_rq = &rq->cfs;\n\n\t\t\t\tif (!cfs_rq->nr_running)\n\t\t\t\t\tgoto idle;\n\n\t\t\t\tgoto simple;\n\t\t\t}\n\t\t}\n\n\t\tse = pick_next_entity(cfs_rq, curr);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\n\t/*\n\t * Since we haven't yet done put_prev_entity and if the selected task\n\t * is a different task than we started out with, try and touch the\n\t * least amount of cfs_rqs.\n\t */\n\tif (prev != p) {\n\t\tstruct sched_entity *pse = &prev->se;\n\n\t\twhile (!(cfs_rq = is_same_group(se, pse))) {\n\t\t\tint se_depth = se->depth;\n\t\t\tint pse_depth = pse->depth;\n\n\t\t\tif (se_depth <= pse_depth) {\n\t\t\t\tput_prev_entity(cfs_rq_of(pse), pse);\n\t\t\t\tpse = parent_entity(pse);\n\t\t\t}\n\t\t\tif (se_depth >= pse_depth) {\n\t\t\t\tset_next_entity(cfs_rq_of(se), se);\n\t\t\t\tse = parent_entity(se);\n\t\t\t}\n\t\t}\n\n\t\tput_prev_entity(cfs_rq, pse);\n\t\tset_next_entity(cfs_rq, se);\n\t}\n\n\tgoto done;\nsimple:\n#endif\n\n\tput_prev_task(rq, prev);\n\n\tdo {\n\t\tse = pick_next_entity(cfs_rq, NULL);\n\t\tset_next_entity(cfs_rq, se);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\ndone: __maybe_unused;\n#ifdef CONFIG_SMP\n\t/*\n\t * Move the next running task to the front of\n\t * the list, so our cfs_tasks list becomes MRU\n\t * one.\n\t */\n\tlist_move(&p->se.group_node, &rq->cfs_tasks);\n#endif\n\n\tif (hrtick_enabled(rq))\n\t\thrtick_start_fair(rq, p);\n\n\tupdate_misfit_status(p, rq);\n\n\treturn p;\n\nidle:\n\tupdate_misfit_status(NULL, rq);\n\tnew_tasks = idle_balance(rq, rf);\n\n\t/*\n\t * Because idle_balance() releases (and re-acquires) rq->lock, it is\n\t * possible for any higher priority task to appear. In that case we\n\t * must re-start the pick_next_entity() loop.\n\t */\n\tif (new_tasks < 0)\n\t\treturn RETRY_TASK;\n\n\tif (new_tasks > 0)\n\t\tgoto again;\n\n\treturn NULL;\n}\n\n/*\n * Account for a descheduled task:\n */\nstatic void put_prev_task_fair(struct rq *rq, struct task_struct *prev)\n{\n\tstruct sched_entity *se = &prev->se;\n\tstruct cfs_rq *cfs_rq;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tput_prev_entity(cfs_rq, se);\n\t}\n}\n\n/*\n * sched_yield() is very simple\n *\n * The magic of dealing with the ->skip buddy is in pick_next_entity.\n */\nstatic void yield_task_fair(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tstruct sched_entity *se = &curr->se;\n\n\t/*\n\t * Are we the only task in the tree?\n\t */\n\tif (unlikely(rq->nr_running == 1))\n\t\treturn;\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (curr->policy != SCHED_BATCH) {\n\t\tupdate_rq_clock(rq);\n\t\t/*\n\t\t * Update run-time statistics of the 'current'.\n\t\t */\n\t\tupdate_curr(cfs_rq);\n\t\t/*\n\t\t * Tell update_rq_clock() that we've just updated,\n\t\t * so we don't do microscopic update in schedule()\n\t\t * and double the fastpath cost.\n\t\t */\n\t\trq_clock_skip_update(rq);\n\t}\n\n\tset_skip_buddy(se);\n}\n\nstatic bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/* throttled hierarchies are not runnable */\n\tif (!se->on_rq || throttled_hierarchy(cfs_rq_of(se)))\n\t\treturn false;\n\n\t/* Tell the scheduler that we'd really like pse to run next. */\n\tset_next_buddy(se);\n\n\tyield_task_fair(rq);\n\n\treturn true;\n}\n\n#ifdef CONFIG_SMP\n/**************************************************\n * Fair scheduling class load-balancing methods.\n *\n * BASICS\n *\n * The purpose of load-balancing is to achieve the same basic fairness the\n * per-CPU scheduler provides, namely provide a proportional amount of compute\n * time to each task. This is expressed in the following equation:\n *\n *   W_i,n/P_i == W_j,n/P_j for all i,j                               (1)\n *\n * Where W_i,n is the n-th weight average for CPU i. The instantaneous weight\n * W_i,0 is defined as:\n *\n *   W_i,0 = \\Sum_j w_i,j                                             (2)\n *\n * Where w_i,j is the weight of the j-th runnable task on CPU i. This weight\n * is derived from the nice value as per sched_prio_to_weight[].\n *\n * The weight average is an exponential decay average of the instantaneous\n * weight:\n *\n *   W'_i,n = (2^n - 1) / 2^n * W_i,n + 1 / 2^n * W_i,0               (3)\n *\n * C_i is the compute capacity of CPU i, typically it is the\n * fraction of 'recent' time available for SCHED_OTHER task execution. But it\n * can also include other factors [XXX].\n *\n * To achieve this balance we define a measure of imbalance which follows\n * directly from (1):\n *\n *   imb_i,j = max{ avg(W/C), W_i/C_i } - min{ avg(W/C), W_j/C_j }    (4)\n *\n * We them move tasks around to minimize the imbalance. In the continuous\n * function space it is obvious this converges, in the discrete case we get\n * a few fun cases generally called infeasible weight scenarios.\n *\n * [XXX expand on:\n *     - infeasible weights;\n *     - local vs global optima in the discrete case. ]\n *\n *\n * SCHED DOMAINS\n *\n * In order to solve the imbalance equation (4), and avoid the obvious O(n^2)\n * for all i,j solution, we create a tree of CPUs that follows the hardware\n * topology where each level pairs two lower groups (or better). This results\n * in O(log n) layers. Furthermore we reduce the number of CPUs going up the\n * tree to only the first of the previous level and we decrease the frequency\n * of load-balance at each level inv. proportional to the number of CPUs in\n * the groups.\n *\n * This yields:\n *\n *     log_2 n     1     n\n *   \\Sum       { --- * --- * 2^i } = O(n)                            (5)\n *     i = 0      2^i   2^i\n *                               `- size of each group\n *         |         |     `- number of CPUs doing load-balance\n *         |         `- freq\n *         `- sum over all levels\n *\n * Coupled with a limit on how many tasks we can migrate every balance pass,\n * this makes (5) the runtime complexity of the balancer.\n *\n * An important property here is that each CPU is still (indirectly) connected\n * to every other CPU in at most O(log n) steps:\n *\n * The adjacency matrix of the resulting graph is given by:\n *\n *             log_2 n\n *   A_i,j = \\Union     (i % 2^k == 0) && i / 2^(k+1) == j / 2^(k+1)  (6)\n *             k = 0\n *\n * And you'll find that:\n *\n *   A^(log_2 n)_i,j != 0  for all i,j                                (7)\n *\n * Showing there's indeed a path between every CPU in at most O(log n) steps.\n * The task movement gives a factor of O(m), giving a convergence complexity\n * of:\n *\n *   O(nm log n),  n := nr_cpus, m := nr_tasks                        (8)\n *\n *\n * WORK CONSERVING\n *\n * In order to avoid CPUs going idle while there's still work to do, new idle\n * balancing is more aggressive and has the newly idle CPU iterate up the domain\n * tree itself instead of relying on other CPUs to bring it work.\n *\n * This adds some complexity to both (5) and (8) but it reduces the total idle\n * time.\n *\n * [XXX more?]\n *\n *\n * CGROUPS\n *\n * Cgroups make a horror show out of (2), instead of a simple sum we get:\n *\n *                                s_k,i\n *   W_i,0 = \\Sum_j \\Prod_k w_k * -----                               (9)\n *                                 S_k\n *\n * Where\n *\n *   s_k,i = \\Sum_j w_i,j,k  and  S_k = \\Sum_i s_k,i                 (10)\n *\n * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i.\n *\n * The big problem is S_k, its a global sum needed to compute a local (W_i)\n * property.\n *\n * [XXX write more on how we solve this.. _after_ merging pjt's patches that\n *      rewrite all of this once again.]\n */\n\nstatic unsigned long __read_mostly max_load_balance_interval = HZ/10;\n\nenum fbq_type { regular, remote, all };\n\nenum group_type {\n\tgroup_other = 0,\n\tgroup_misfit_task,\n\tgroup_imbalanced,\n\tgroup_overloaded,\n};\n\n#define LBF_ALL_PINNED\t0x01\n#define LBF_NEED_BREAK\t0x02\n#define LBF_DST_PINNED  0x04\n#define LBF_SOME_PINNED\t0x08\n#define LBF_NOHZ_STATS\t0x10\n#define LBF_NOHZ_AGAIN\t0x20\n\nstruct lb_env {\n\tstruct sched_domain\t*sd;\n\n\tstruct rq\t\t*src_rq;\n\tint\t\t\tsrc_cpu;\n\n\tint\t\t\tdst_cpu;\n\tstruct rq\t\t*dst_rq;\n\n\tstruct cpumask\t\t*dst_grpmask;\n\tint\t\t\tnew_dst_cpu;\n\tenum cpu_idle_type\tidle;\n\tlong\t\t\timbalance;\n\t/* The set of CPUs under consideration for load-balancing */\n\tstruct cpumask\t\t*cpus;\n\n\tunsigned int\t\tflags;\n\n\tunsigned int\t\tloop;\n\tunsigned int\t\tloop_break;\n\tunsigned int\t\tloop_max;\n\n\tenum fbq_type\t\tfbq_type;\n\tenum group_type\t\tsrc_grp_type;\n\tstruct list_head\ttasks;\n};\n\n/*\n * Is this task likely cache-hot:\n */\nstatic int task_hot(struct task_struct *p, struct lb_env *env)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(task_has_idle_policy(p)))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && env->dst_rq->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = rq_clock_task(env->src_rq) - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns 1, if task migration degrades locality\n * Returns 0, if task migration improves locality i.e migration preferred.\n * Returns -1, if task migration is not affected by locality.\n */\nstatic int migrate_degrades_locality(struct task_struct *p, struct lb_env *env)\n{\n\tstruct numa_group *numa_group = rcu_dereference(p->numa_group);\n\tunsigned long src_weight, dst_weight;\n\tint src_nid, dst_nid, dist;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn -1;\n\n\tif (!p->numa_faults || !(env->sd->flags & SD_NUMA))\n\t\treturn -1;\n\n\tsrc_nid = cpu_to_node(env->src_cpu);\n\tdst_nid = cpu_to_node(env->dst_cpu);\n\n\tif (src_nid == dst_nid)\n\t\treturn -1;\n\n\t/* Migrating away from the preferred node is always bad. */\n\tif (src_nid == p->numa_preferred_nid) {\n\t\tif (env->src_rq->nr_running > env->src_rq->nr_preferred_running)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\t/* Encourage migration to the preferred node. */\n\tif (dst_nid == p->numa_preferred_nid)\n\t\treturn 0;\n\n\t/* Leaving a core idle is often worse than degrading locality. */\n\tif (env->idle == CPU_IDLE)\n\t\treturn -1;\n\n\tdist = node_distance(src_nid, dst_nid);\n\tif (numa_group) {\n\t\tsrc_weight = group_weight(p, src_nid, dist);\n\t\tdst_weight = group_weight(p, dst_nid, dist);\n\t} else {\n\t\tsrc_weight = task_weight(p, src_nid, dist);\n\t\tdst_weight = task_weight(p, dst_nid, dist);\n\t}\n\n\treturn dst_weight < src_weight;\n}\n\n#else\nstatic inline int migrate_degrades_locality(struct task_struct *p,\n\t\t\t\t\t     struct lb_env *env)\n{\n\treturn -1;\n}\n#endif\n\n/*\n * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?\n */\nstatic\nint can_migrate_task(struct task_struct *p, struct lb_env *env)\n{\n\tint tsk_cache_hot;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\t/*\n\t * We do not migrate tasks that are:\n\t * 1) throttled_lb_pair, or\n\t * 2) cannot be migrated to this CPU due to cpus_allowed, or\n\t * 3) running (obviously), or\n\t * 4) are cache-hot on their current CPU.\n\t */\n\tif (throttled_lb_pair(task_group(p), env->src_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(env->dst_cpu, &p->cpus_allowed)) {\n\t\tint cpu;\n\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_affine);\n\n\t\tenv->flags |= LBF_SOME_PINNED;\n\n\t\t/*\n\t\t * Remember if this task can be migrated to any other CPU in\n\t\t * our sched_group. We may want to revisit it if we couldn't\n\t\t * meet load balance goals by pulling other tasks on src_cpu.\n\t\t *\n\t\t * Avoid computing new_dst_cpu for NEWLY_IDLE or if we have\n\t\t * already computed one in current iteration.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE || (env->flags & LBF_DST_PINNED))\n\t\t\treturn 0;\n\n\t\t/* Prevent to re-select dst_cpu via env's CPUs: */\n\t\tfor_each_cpu_and(cpu, env->dst_grpmask, env->cpus) {\n\t\t\tif (cpumask_test_cpu(cpu, &p->cpus_allowed)) {\n\t\t\t\tenv->flags |= LBF_DST_PINNED;\n\t\t\t\tenv->new_dst_cpu = cpu;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* Record that we found atleast one task that could run on dst_cpu */\n\tenv->flags &= ~LBF_ALL_PINNED;\n\n\tif (task_running(env->src_rq, p)) {\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_running);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Aggressive migration if:\n\t * 1) destination numa is preferred\n\t * 2) task is cache cold, or\n\t * 3) too many balance attempts have failed.\n\t */\n\ttsk_cache_hot = migrate_degrades_locality(p, env);\n\tif (tsk_cache_hot == -1)\n\t\ttsk_cache_hot = task_hot(p, env);\n\n\tif (tsk_cache_hot <= 0 ||\n\t    env->sd->nr_balance_failed > env->sd->cache_nice_tries) {\n\t\tif (tsk_cache_hot == 1) {\n\t\t\tschedstat_inc(env->sd->lb_hot_gained[env->idle]);\n\t\t\tschedstat_inc(p->se.statistics.nr_forced_migrations);\n\t\t}\n\t\treturn 1;\n\t}\n\n\tschedstat_inc(p->se.statistics.nr_failed_migrations_hot);\n\treturn 0;\n}\n\n/*\n * detach_task() -- detach the task for the migration specified in env\n */\nstatic void detach_task(struct task_struct *p, struct lb_env *env)\n{\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\tdeactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);\n\tset_task_cpu(p, env->dst_cpu);\n}\n\n/*\n * detach_one_task() -- tries to dequeue exactly one task from env->src_rq, as\n * part of active balancing operations within \"domain\".\n *\n * Returns a task if successful and NULL otherwise.\n */\nstatic struct task_struct *detach_one_task(struct lb_env *env)\n{\n\tstruct task_struct *p;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tlist_for_each_entry_reverse(p,\n\t\t\t&env->src_rq->cfs_tasks, se.group_node) {\n\t\tif (!can_migrate_task(p, env))\n\t\t\tcontinue;\n\n\t\tdetach_task(p, env);\n\n\t\t/*\n\t\t * Right now, this is only the second place where\n\t\t * lb_gained[env->idle] is updated (other is detach_tasks)\n\t\t * so we can safely collect stats here rather than\n\t\t * inside detach_tasks().\n\t\t */\n\t\tschedstat_inc(env->sd->lb_gained[env->idle]);\n\t\treturn p;\n\t}\n\treturn NULL;\n}\n\nstatic const unsigned int sched_nr_migrate_break = 32;\n\n/*\n * detach_tasks() -- tries to detach up to imbalance weighted load from\n * busiest_rq, as part of a balancing operation within domain \"sd\".\n *\n * Returns number of detached tasks if successful and 0 otherwise.\n */\nstatic int detach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->src_rq->cfs_tasks;\n\tstruct task_struct *p;\n\tunsigned long load;\n\tint detached = 0;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (env->imbalance <= 0)\n\t\treturn 0;\n\n\twhile (!list_empty(tasks)) {\n\t\t/*\n\t\t * We don't want to steal all, otherwise we may be treated likewise,\n\t\t * which could at worst lead to a livelock crash.\n\t\t */\n\t\tif (env->idle != CPU_NOT_IDLE && env->src_rq->nr_running <= 1)\n\t\t\tbreak;\n\n\t\tp = list_last_entry(tasks, struct task_struct, se.group_node);\n\n\t\tenv->loop++;\n\t\t/* We've more or less seen every task there is, call it quits */\n\t\tif (env->loop > env->loop_max)\n\t\t\tbreak;\n\n\t\t/* take a breather every nr_migrate tasks */\n\t\tif (env->loop > env->loop_break) {\n\t\t\tenv->loop_break += sched_nr_migrate_break;\n\t\t\tenv->flags |= LBF_NEED_BREAK;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!can_migrate_task(p, env))\n\t\t\tgoto next;\n\n\t\tload = task_h_load(p);\n\n\t\tif (sched_feat(LB_MIN) && load < 16 && !env->sd->nr_balance_failed)\n\t\t\tgoto next;\n\n\t\tif ((load / 2) > env->imbalance)\n\t\t\tgoto next;\n\n\t\tdetach_task(p, env);\n\t\tlist_add(&p->se.group_node, &env->tasks);\n\n\t\tdetached++;\n\t\tenv->imbalance -= load;\n\n#ifdef CONFIG_PREEMPT\n\t\t/*\n\t\t * NEWIDLE balancing is a source of latency, so preemptible\n\t\t * kernels will stop after the first task is detached to minimize\n\t\t * the critical section.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE)\n\t\t\tbreak;\n#endif\n\n\t\t/*\n\t\t * We only want to steal up to the prescribed amount of\n\t\t * weighted load.\n\t\t */\n\t\tif (env->imbalance <= 0)\n\t\t\tbreak;\n\n\t\tcontinue;\nnext:\n\t\tlist_move(&p->se.group_node, tasks);\n\t}\n\n\t/*\n\t * Right now, this is one of only two places we collect this stat\n\t * so we can safely collect detach_one_task() stats here rather\n\t * than inside detach_one_task().\n\t */\n\tschedstat_add(env->sd->lb_gained[env->idle], detached);\n\n\treturn detached;\n}\n\n/*\n * attach_task() -- attach the task detached by detach_task() to its new rq.\n */\nstatic void attach_task(struct rq *rq, struct task_struct *p)\n{\n\tlockdep_assert_held(&rq->lock);\n\n\tBUG_ON(task_rq(p) != rq);\n\tactivate_task(rq, p, ENQUEUE_NOCLOCK);\n\tp->on_rq = TASK_ON_RQ_QUEUED;\n\tcheck_preempt_curr(rq, p, 0);\n}\n\n/*\n * attach_one_task() -- attaches the task returned from detach_one_task() to\n * its new rq.\n */\nstatic void attach_one_task(struct rq *rq, struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\tattach_task(rq, p);\n\trq_unlock(rq, &rf);\n}\n\n/*\n * attach_tasks() -- attaches all tasks detached by detach_tasks() to their\n * new rq.\n */\nstatic void attach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->tasks;\n\tstruct task_struct *p;\n\tstruct rq_flags rf;\n\n\trq_lock(env->dst_rq, &rf);\n\tupdate_rq_clock(env->dst_rq);\n\n\twhile (!list_empty(tasks)) {\n\t\tp = list_first_entry(tasks, struct task_struct, se.group_node);\n\t\tlist_del_init(&p->se.group_node);\n\n\t\tattach_task(env->dst_rq, p);\n\t}\n\n\trq_unlock(env->dst_rq, &rf);\n}\n\nstatic inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->avg.load_avg)\n\t\treturn true;\n\n\tif (cfs_rq->avg.util_avg)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool others_have_blocked(struct rq *rq)\n{\n\tif (READ_ONCE(rq->avg_rt.util_avg))\n\t\treturn true;\n\n\tif (READ_ONCE(rq->avg_dl.util_avg))\n\t\treturn true;\n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\n\tif (READ_ONCE(rq->avg_irq.util_avg))\n\t\treturn true;\n#endif\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\nstatic inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->load.weight)\n\t\treturn false;\n\n\tif (cfs_rq->avg.load_sum)\n\t\treturn false;\n\n\tif (cfs_rq->avg.util_sum)\n\t\treturn false;\n\n\tif (cfs_rq->avg.runnable_load_sum)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq, *pos;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\tbool done = true;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\t/*\n\t * Iterates the task_group tree in a bottom up fashion, see\n\t * list_add_leaf_cfs_rq() for details.\n\t */\n\tfor_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {\n\t\tstruct sched_entity *se;\n\n\t\t/* throttled entities do not contribute to load */\n\t\tif (throttled_hierarchy(cfs_rq))\n\t\t\tcontinue;\n\n\t\tif (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq))\n\t\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t\t/* Propagate pending load changes to the parent, if any: */\n\t\tse = cfs_rq->tg->se[cpu];\n\t\tif (se && !skip_blocked_update(se))\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, 0);\n\n\t\t/*\n\t\t * There can be a lot of idle CPU cgroups.  Don't let fully\n\t\t * decayed cfs_rqs linger on the list.\n\t\t */\n\t\tif (cfs_rq_is_decayed(cfs_rq))\n\t\t\tlist_del_leaf_cfs_rq(cfs_rq);\n\n\t\t/* Don't need periodic decay once load/util_avg are null */\n\t\tif (cfs_rq_has_blocked(cfs_rq))\n\t\t\tdone = false;\n\t}\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n\t/* Don't need periodic decay once load/util_avg are null */\n\tif (others_have_blocked(rq))\n\t\tdone = false;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (done)\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\n/*\n * Compute the hierarchical load factor for cfs_rq and all its ascendants.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];\n\tunsigned long now = jiffies;\n\tunsigned long load;\n\n\tif (cfs_rq->last_h_load_update == now)\n\t\treturn;\n\n\tcfs_rq->h_load_next = NULL;\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_load_next = se;\n\t\tif (cfs_rq->last_h_load_update == now)\n\t\t\tbreak;\n\t}\n\n\tif (!se) {\n\t\tcfs_rq->h_load = cfs_rq_load_avg(cfs_rq);\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n\n\twhile ((se = cfs_rq->h_load_next) != NULL) {\n\t\tload = cfs_rq->h_load;\n\t\tload = div64_ul(load * se->avg.load_avg,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n\t\tcfs_rq = group_cfs_rq(se);\n\t\tcfs_rq->h_load = load;\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n\n\tupdate_cfs_rq_h_load(cfs_rq);\n\treturn div64_ul(p->se.avg.load_avg * cfs_rq->h_load,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n}\n#else\nstatic inline void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\tupdate_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq);\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (!cfs_rq_has_blocked(cfs_rq) && !others_have_blocked(rq))\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\treturn p->se.avg.load_avg;\n}\n#endif\n\n/********** Helpers for find_busiest_group ************************/\n\n/*\n * sg_lb_stats - stats of a sched_group required for load_balancing\n */\nstruct sg_lb_stats {\n\tunsigned long avg_load; /*Avg load across the CPUs of the group */\n\tunsigned long group_load; /* Total load over the CPUs of the group */\n\tunsigned long sum_weighted_load; /* Weighted load of group's tasks */\n\tunsigned long load_per_task;\n\tunsigned long group_capacity;\n\tunsigned long group_util; /* Total utilization of the group */\n\tunsigned int sum_nr_running; /* Nr tasks running in the group */\n\tunsigned int idle_cpus;\n\tunsigned int group_weight;\n\tenum group_type group_type;\n\tint group_no_capacity;\n\tunsigned long group_misfit_task_load; /* A CPU has a task too big for its capacity */\n#ifdef CONFIG_NUMA_BALANCING\n\tunsigned int nr_numa_running;\n\tunsigned int nr_preferred_running;\n#endif\n};\n\n/*\n * sd_lb_stats - Structure to store the statistics of a sched_domain\n *\t\t during load balancing.\n */\nstruct sd_lb_stats {\n\tstruct sched_group *busiest;\t/* Busiest group in this sd */\n\tstruct sched_group *local;\t/* Local group in this sd */\n\tunsigned long total_running;\n\tunsigned long total_load;\t/* Total load of all groups in sd */\n\tunsigned long total_capacity;\t/* Total capacity of all groups in sd */\n\tunsigned long avg_load;\t/* Average load across all groups in sd */\n\n\tstruct sg_lb_stats busiest_stat;/* Statistics of the busiest group */\n\tstruct sg_lb_stats local_stat;\t/* Statistics of the local group */\n};\n\nstatic inline void init_sd_lb_stats(struct sd_lb_stats *sds)\n{\n\t/*\n\t * Skimp on the clearing to avoid duplicate work. We can avoid clearing\n\t * local_stat because update_sg_lb_stats() does a full clear/assignment.\n\t * We must however clear busiest_stat::avg_load because\n\t * update_sd_pick_busiest() reads this before assignment.\n\t */\n\t*sds = (struct sd_lb_stats){\n\t\t.busiest = NULL,\n\t\t.local = NULL,\n\t\t.total_running = 0UL,\n\t\t.total_load = 0UL,\n\t\t.total_capacity = 0UL,\n\t\t.busiest_stat = {\n\t\t\t.avg_load = 0UL,\n\t\t\t.sum_nr_running = 0,\n\t\t\t.group_type = group_other,\n\t\t},\n\t};\n}\n\n/**\n * get_sd_load_idx - Obtain the load index for a given sched domain.\n * @sd: The sched_domain whose load_idx is to be obtained.\n * @idle: The idle status of the CPU for whose sd load_idx is obtained.\n *\n * Return: The load index.\n */\nstatic inline int get_sd_load_idx(struct sched_domain *sd,\n\t\t\t\t\tenum cpu_idle_type idle)\n{\n\tint load_idx;\n\n\tswitch (idle) {\n\tcase CPU_NOT_IDLE:\n\t\tload_idx = sd->busy_idx;\n\t\tbreak;\n\n\tcase CPU_NEWLY_IDLE:\n\t\tload_idx = sd->newidle_idx;\n\t\tbreak;\n\tdefault:\n\t\tload_idx = sd->idle_idx;\n\t\tbreak;\n\t}\n\n\treturn load_idx;\n}\n\nstatic unsigned long scale_rt_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long max = arch_scale_cpu_capacity(sd, cpu);\n\tunsigned long used, free;\n\tunsigned long irq;\n\n\tirq = cpu_util_irq(rq);\n\n\tif (unlikely(irq >= max))\n\t\treturn 1;\n\n\tused = READ_ONCE(rq->avg_rt.util_avg);\n\tused += READ_ONCE(rq->avg_dl.util_avg);\n\n\tif (unlikely(used >= max))\n\t\treturn 1;\n\n\tfree = max - used;\n\n\treturn scale_irq_capacity(free, irq, max);\n}\n\nstatic void update_cpu_capacity(struct sched_domain *sd, int cpu)\n{\n\tunsigned long capacity = scale_rt_capacity(sd, cpu);\n\tstruct sched_group *sdg = sd->groups;\n\n\tcpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(sd, cpu);\n\n\tif (!capacity)\n\t\tcapacity = 1;\n\n\tcpu_rq(cpu)->cpu_capacity = capacity;\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = capacity;\n\tsdg->sgc->max_capacity = capacity;\n}\n\nvoid update_group_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_domain *child = sd->child;\n\tstruct sched_group *group, *sdg = sd->groups;\n\tunsigned long capacity, min_capacity, max_capacity;\n\tunsigned long interval;\n\n\tinterval = msecs_to_jiffies(sd->balance_interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\tsdg->sgc->next_update = jiffies + interval;\n\n\tif (!child) {\n\t\tupdate_cpu_capacity(sd, cpu);\n\t\treturn;\n\t}\n\n\tcapacity = 0;\n\tmin_capacity = ULONG_MAX;\n\tmax_capacity = 0;\n\n\tif (child->flags & SD_OVERLAP) {\n\t\t/*\n\t\t * SD_OVERLAP domains cannot assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tfor_each_cpu(cpu, sched_group_span(sdg)) {\n\t\t\tstruct sched_group_capacity *sgc;\n\t\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\t\t/*\n\t\t\t * build_sched_domains() -> init_sched_groups_capacity()\n\t\t\t * gets here before we've attached the domains to the\n\t\t\t * runqueues.\n\t\t\t *\n\t\t\t * Use capacity_of(), which is set irrespective of domains\n\t\t\t * in update_cpu_capacity().\n\t\t\t *\n\t\t\t * This avoids capacity from being 0 and\n\t\t\t * causing divide-by-zero issues on boot.\n\t\t\t */\n\t\t\tif (unlikely(!rq->sd)) {\n\t\t\t\tcapacity += capacity_of(cpu);\n\t\t\t} else {\n\t\t\t\tsgc = rq->sd->groups->sgc;\n\t\t\t\tcapacity += sgc->capacity;\n\t\t\t}\n\n\t\t\tmin_capacity = min(capacity, min_capacity);\n\t\t\tmax_capacity = max(capacity, max_capacity);\n\t\t}\n\t} else  {\n\t\t/*\n\t\t * !SD_OVERLAP domains can assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tgroup = child->groups;\n\t\tdo {\n\t\t\tstruct sched_group_capacity *sgc = group->sgc;\n\n\t\t\tcapacity += sgc->capacity;\n\t\t\tmin_capacity = min(sgc->min_capacity, min_capacity);\n\t\t\tmax_capacity = max(sgc->max_capacity, max_capacity);\n\t\t\tgroup = group->next;\n\t\t} while (group != child->groups);\n\t}\n\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = min_capacity;\n\tsdg->sgc->max_capacity = max_capacity;\n}\n\n/*\n * Check whether the capacity of the rq has been noticeably reduced by side\n * activity. The imbalance_pct is used for the threshold.\n * Return true is the capacity is reduced\n */\nstatic inline int\ncheck_cpu_capacity(struct rq *rq, struct sched_domain *sd)\n{\n\treturn ((rq->cpu_capacity * sd->imbalance_pct) <\n\t\t\t\t(rq->cpu_capacity_orig * 100));\n}\n\n/*\n * Group imbalance indicates (and tries to solve) the problem where balancing\n * groups is inadequate due to ->cpus_allowed constraints.\n *\n * Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a\n * cpumask covering 1 CPU of the first group and 3 CPUs of the second group.\n * Something like:\n *\n *\t{ 0 1 2 3 } { 4 5 6 7 }\n *\t        *     * * *\n *\n * If we were to balance group-wise we'd place two tasks in the first group and\n * two tasks in the second group. Clearly this is undesired as it will overload\n * cpu 3 and leave one of the CPUs in the second group unused.\n *\n * The current solution to this issue is detecting the skew in the first group\n * by noticing the lower domain failed to reach balance and had difficulty\n * moving tasks due to affinity constraints.\n *\n * When this is so detected; this group becomes a candidate for busiest; see\n * update_sd_pick_busiest(). And calculate_imbalance() and\n * find_busiest_group() avoid some of the usual balance conditions to allow it\n * to create an effective group imbalance.\n *\n * This is a somewhat tricky proposition since the next run might not find the\n * group imbalance and decide the groups need to be balanced again. A most\n * subtle and fragile situation.\n */\n\nstatic inline int sg_imbalanced(struct sched_group *group)\n{\n\treturn group->sgc->imbalance;\n}\n\n/*\n * group_has_capacity returns true if the group has spare capacity that could\n * be used by some tasks.\n * We consider that a group has spare capacity if the  * number of task is\n * smaller than the number of CPUs or if the utilization is lower than the\n * available capacity for CFS tasks.\n * For the latter, we use a threshold to stabilize the state, to take into\n * account the variance of the tasks' load and to return true if the available\n * capacity in meaningful for the load balancer.\n * As an example, an available capacity of 1% can appear but it doesn't make\n * any benefit for the load balance.\n */\nstatic inline bool\ngroup_has_capacity(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running < sgs->group_weight)\n\t\treturn true;\n\n\tif ((sgs->group_capacity * 100) >\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *  group_is_overloaded returns true if the group has more tasks than it can\n *  handle.\n *  group_is_overloaded is not equals to !group_has_capacity because a group\n *  with the exact right number of tasks, has no more spare capacity but is not\n *  overloaded so both group_has_capacity and group_is_overloaded return\n *  false.\n */\nstatic inline bool\ngroup_is_overloaded(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running <= sgs->group_weight)\n\t\treturn false;\n\n\tif ((sgs->group_capacity * 100) <\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * group_smaller_min_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_min_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->min_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->min_capacity * 1024;\n}\n\n/*\n * group_smaller_max_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity_orig than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_max_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->max_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->max_capacity * 1024;\n}\n\nstatic inline enum\ngroup_type group_classify(struct sched_group *group,\n\t\t\t  struct sg_lb_stats *sgs)\n{\n\tif (sgs->group_no_capacity)\n\t\treturn group_overloaded;\n\n\tif (sg_imbalanced(group))\n\t\treturn group_imbalanced;\n\n\tif (sgs->group_misfit_task_load)\n\t\treturn group_misfit_task;\n\n\treturn group_other;\n}\n\nstatic bool update_nohz_stats(struct rq *rq, bool force)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tunsigned int cpu = rq->cpu;\n\n\tif (!rq->has_blocked_load)\n\t\treturn false;\n\n\tif (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask))\n\t\treturn false;\n\n\tif (!force && !time_after(jiffies, rq->last_blocked_load_update_tick))\n\t\treturn true;\n\n\tupdate_blocked_averages(cpu);\n\n\treturn rq->has_blocked_load;\n#else\n\treturn false;\n#endif\n}\n\n/**\n * update_sg_lb_stats - Update sched_group's statistics for load balancing.\n * @env: The load balancing environment.\n * @group: sched_group whose statistics are to be updated.\n * @sgs: variable to hold the statistics for this group.\n * @sg_status: Holds flag indicating the status of the sched_group\n */\nstatic inline void update_sg_lb_stats(struct lb_env *env,\n\t\t\t\t      struct sched_group *group,\n\t\t\t\t      struct sg_lb_stats *sgs,\n\t\t\t\t      int *sg_status)\n{\n\tint local_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(group));\n\tint load_idx = get_sd_load_idx(env->sd, env->idle);\n\tunsigned long load;\n\tint i, nr_running;\n\n\tmemset(sgs, 0, sizeof(*sgs));\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tstruct rq *rq = cpu_rq(i);\n\n\t\tif ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))\n\t\t\tenv->flags |= LBF_NOHZ_AGAIN;\n\n\t\t/* Bias balancing toward CPUs of our domain: */\n\t\tif (local_group)\n\t\t\tload = target_load(i, load_idx);\n\t\telse\n\t\t\tload = source_load(i, load_idx);\n\n\t\tsgs->group_load += load;\n\t\tsgs->group_util += cpu_util(i);\n\t\tsgs->sum_nr_running += rq->cfs.h_nr_running;\n\n\t\tnr_running = rq->nr_running;\n\t\tif (nr_running > 1)\n\t\t\t*sg_status |= SG_OVERLOAD;\n\n\t\tif (cpu_overutilized(i))\n\t\t\t*sg_status |= SG_OVERUTILIZED;\n\n#ifdef CONFIG_NUMA_BALANCING\n\t\tsgs->nr_numa_running += rq->nr_numa_running;\n\t\tsgs->nr_preferred_running += rq->nr_preferred_running;\n#endif\n\t\tsgs->sum_weighted_load += weighted_cpuload(rq);\n\t\t/*\n\t\t * No need to call idle_cpu() if nr_running is not 0\n\t\t */\n\t\tif (!nr_running && idle_cpu(i))\n\t\t\tsgs->idle_cpus++;\n\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    sgs->group_misfit_task_load < rq->misfit_task_load) {\n\t\t\tsgs->group_misfit_task_load = rq->misfit_task_load;\n\t\t\t*sg_status |= SG_OVERLOAD;\n\t\t}\n\t}\n\n\t/* Adjust by relative CPU capacity of the group */\n\tsgs->group_capacity = group->sgc->capacity;\n\tsgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;\n\n\tif (sgs->sum_nr_running)\n\t\tsgs->load_per_task = sgs->sum_weighted_load / sgs->sum_nr_running;\n\n\tsgs->group_weight = group->group_weight;\n\n\tsgs->group_no_capacity = group_is_overloaded(env, sgs);\n\tsgs->group_type = group_classify(group, sgs);\n}\n\n/**\n * update_sd_pick_busiest - return 1 on busiest group\n * @env: The load balancing environment.\n * @sds: sched_domain statistics\n * @sg: sched_group candidate to be checked for being the busiest\n * @sgs: sched_group statistics\n *\n * Determine if @sg is a busier group than the previously selected\n * busiest group.\n *\n * Return: %true if @sg is a busier group than the previously selected\n * busiest group. %false otherwise.\n */\nstatic bool update_sd_pick_busiest(struct lb_env *env,\n\t\t\t\t   struct sd_lb_stats *sds,\n\t\t\t\t   struct sched_group *sg,\n\t\t\t\t   struct sg_lb_stats *sgs)\n{\n\tstruct sg_lb_stats *busiest = &sds->busiest_stat;\n\n\t/*\n\t * Don't try to pull misfit tasks we can't help.\n\t * We can use max_capacity here as reduction in capacity on some\n\t * CPUs in the group should either be possible to resolve\n\t * internally or be covered by avg_load imbalance (eventually).\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    (!group_smaller_max_cpu_capacity(sg, sds->local) ||\n\t     !group_has_capacity(env, &sds->local_stat)))\n\t\treturn false;\n\n\tif (sgs->group_type > busiest->group_type)\n\t\treturn true;\n\n\tif (sgs->group_type < busiest->group_type)\n\t\treturn false;\n\n\tif (sgs->avg_load <= busiest->avg_load)\n\t\treturn false;\n\n\tif (!(env->sd->flags & SD_ASYM_CPUCAPACITY))\n\t\tgoto asym_packing;\n\n\t/*\n\t * Candidate sg has no more than one task per CPU and\n\t * has higher per-CPU capacity. Migrating tasks to less\n\t * capable CPUs may harm throughput. Maximize throughput,\n\t * power/energy consequences are not considered.\n\t */\n\tif (sgs->sum_nr_running <= sgs->group_weight &&\n\t    group_smaller_min_cpu_capacity(sds->local, sg))\n\t\treturn false;\n\n\t/*\n\t * If we have more than one misfit sg go with the biggest misfit.\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    sgs->group_misfit_task_load < busiest->group_misfit_task_load)\n\t\treturn false;\n\nasym_packing:\n\t/* This is the busiest node in its class. */\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn true;\n\n\t/* No ASYM_PACKING if target CPU is already busy */\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn true;\n\t/*\n\t * ASYM_PACKING needs to move all the work to the highest\n\t * prority CPUs in the group, therefore mark all groups\n\t * of lower priority than ourself as busy.\n\t */\n\tif (sgs->sum_nr_running &&\n\t    sched_asym_prefer(env->dst_cpu, sg->asym_prefer_cpu)) {\n\t\tif (!sds->busiest)\n\t\t\treturn true;\n\n\t\t/* Prefer to move from lowest priority CPU's work */\n\t\tif (sched_asym_prefer(sds->busiest->asym_prefer_cpu,\n\t\t\t\t      sg->asym_prefer_cpu))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running > sgs->nr_numa_running)\n\t\treturn regular;\n\tif (sgs->sum_nr_running > sgs->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\tif (rq->nr_running > rq->nr_numa_running)\n\t\treturn regular;\n\tif (rq->nr_running > rq->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n#else\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\treturn regular;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n/**\n * update_sd_lb_stats - Update sched_domain's statistics for load balancing.\n * @env: The load balancing environment.\n * @sds: variable to hold the statistics for this sched_domain.\n */\nstatic inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tstruct sched_domain *child = env->sd->child;\n\tstruct sched_group *sg = env->sd->groups;\n\tstruct sg_lb_stats *local = &sds->local_stat;\n\tstruct sg_lb_stats tmp_sgs;\n\tbool prefer_sibling = child && child->flags & SD_PREFER_SIBLING;\n\tint sg_status = 0;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))\n\t\tenv->flags |= LBF_NOHZ_STATS;\n#endif\n\n\tdo {\n\t\tstruct sg_lb_stats *sgs = &tmp_sgs;\n\t\tint local_group;\n\n\t\tlocal_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));\n\t\tif (local_group) {\n\t\t\tsds->local = sg;\n\t\t\tsgs = local;\n\n\t\t\tif (env->idle != CPU_NEWLY_IDLE ||\n\t\t\t    time_after_eq(jiffies, sg->sgc->next_update))\n\t\t\t\tupdate_group_capacity(env->sd, env->dst_cpu);\n\t\t}\n\n\t\tupdate_sg_lb_stats(env, sg, sgs, &sg_status);\n\n\t\tif (local_group)\n\t\t\tgoto next_group;\n\n\t\t/*\n\t\t * In case the child domain prefers tasks go to siblings\n\t\t * first, lower the sg capacity so that we'll try\n\t\t * and move all the excess tasks away. We lower the capacity\n\t\t * of a group only if the local group has the capacity to fit\n\t\t * these excess tasks. The extra check prevents the case where\n\t\t * you always pull from the heaviest group when it is already\n\t\t * under-utilized (possible with a large weight task outweighs\n\t\t * the tasks on the system).\n\t\t */\n\t\tif (prefer_sibling && sds->local &&\n\t\t    group_has_capacity(env, local) &&\n\t\t    (sgs->sum_nr_running > local->sum_nr_running + 1)) {\n\t\t\tsgs->group_no_capacity = 1;\n\t\t\tsgs->group_type = group_classify(sg, sgs);\n\t\t}\n\n\t\tif (update_sd_pick_busiest(env, sds, sg, sgs)) {\n\t\t\tsds->busiest = sg;\n\t\t\tsds->busiest_stat = *sgs;\n\t\t}\n\nnext_group:\n\t\t/* Now, start updating sd_lb_stats */\n\t\tsds->total_running += sgs->sum_nr_running;\n\t\tsds->total_load += sgs->group_load;\n\t\tsds->total_capacity += sgs->group_capacity;\n\n\t\tsg = sg->next;\n\t} while (sg != env->sd->groups);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif ((env->flags & LBF_NOHZ_AGAIN) &&\n\t    cpumask_subset(nohz.idle_cpus_mask, sched_domain_span(env->sd))) {\n\n\t\tWRITE_ONCE(nohz.next_blocked,\n\t\t\t   jiffies + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\t}\n#endif\n\n\tif (env->sd->flags & SD_NUMA)\n\t\tenv->fbq_type = fbq_classify_group(&sds->busiest_stat);\n\n\tif (!env->sd->parent) {\n\t\tstruct root_domain *rd = env->dst_rq->rd;\n\n\t\t/* update overload indicator if we are at root domain */\n\t\tWRITE_ONCE(rd->overload, sg_status & SG_OVERLOAD);\n\n\t\t/* Update over-utilization (tipping point, U >= 0) indicator */\n\t\tWRITE_ONCE(rd->overutilized, sg_status & SG_OVERUTILIZED);\n\t} else if (sg_status & SG_OVERUTILIZED) {\n\t\tWRITE_ONCE(env->dst_rq->rd->overutilized, SG_OVERUTILIZED);\n\t}\n}\n\n/**\n * check_asym_packing - Check to see if the group is packed into the\n *\t\t\tsched domain.\n *\n * This is primarily intended to used at the sibling level.  Some\n * cores like POWER7 prefer to use lower numbered SMT threads.  In the\n * case of POWER7, it can move to lower SMT modes only when higher\n * threads are idle.  When in lower SMT modes, the threads will\n * perform better since they share less core resources.  Hence when we\n * have idle threads, we want them to be the higher ones.\n *\n * This packing function is run on idle threads.  It checks to see if\n * the busiest CPU in this domain (core in the P7 case) has a higher\n * CPU number than the packing function is being run on.  Here we are\n * assuming lower CPU number will be equivalent to lower a SMT thread\n * number.\n *\n * Return: 1 when packing is required and a task should be moved to\n * this CPU.  The amount of the imbalance is returned in env->imbalance.\n *\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain which is to be packed\n */\nstatic int check_asym_packing(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tint busiest_cpu;\n\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn 0;\n\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn 0;\n\n\tif (!sds->busiest)\n\t\treturn 0;\n\n\tbusiest_cpu = sds->busiest->asym_prefer_cpu;\n\tif (sched_asym_prefer(busiest_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tenv->imbalance = DIV_ROUND_CLOSEST(\n\t\tsds->busiest_stat.avg_load * sds->busiest_stat.group_capacity,\n\t\tSCHED_CAPACITY_SCALE);\n\n\treturn 1;\n}\n\n/**\n * fix_small_imbalance - Calculate the minor imbalance that exists\n *\t\t\tamongst the groups of a sched_domain, during\n *\t\t\tload balancing.\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline\nvoid fix_small_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long tmp, capa_now = 0, capa_move = 0;\n\tunsigned int imbn = 2;\n\tunsigned long scaled_busy_load_per_task;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (!local->sum_nr_running)\n\t\tlocal->load_per_task = cpu_avg_load_per_task(env->dst_cpu);\n\telse if (busiest->load_per_task > local->load_per_task)\n\t\timbn = 1;\n\n\tscaled_busy_load_per_task =\n\t\t(busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\tbusiest->group_capacity;\n\n\tif (busiest->avg_load + scaled_busy_load_per_task >=\n\t    local->avg_load + (scaled_busy_load_per_task * imbn)) {\n\t\tenv->imbalance = busiest->load_per_task;\n\t\treturn;\n\t}\n\n\t/*\n\t * OK, we don't have enough imbalance to justify moving tasks,\n\t * however we may be able to increase total CPU capacity used by\n\t * moving them.\n\t */\n\n\tcapa_now += busiest->group_capacity *\n\t\t\tmin(busiest->load_per_task, busiest->avg_load);\n\tcapa_now += local->group_capacity *\n\t\t\tmin(local->load_per_task, local->avg_load);\n\tcapa_now /= SCHED_CAPACITY_SCALE;\n\n\t/* Amount of load we'd subtract */\n\tif (busiest->avg_load > scaled_busy_load_per_task) {\n\t\tcapa_move += busiest->group_capacity *\n\t\t\t    min(busiest->load_per_task,\n\t\t\t\tbusiest->avg_load - scaled_busy_load_per_task);\n\t}\n\n\t/* Amount of load we'd add */\n\tif (busiest->avg_load * busiest->group_capacity <\n\t    busiest->load_per_task * SCHED_CAPACITY_SCALE) {\n\t\ttmp = (busiest->avg_load * busiest->group_capacity) /\n\t\t      local->group_capacity;\n\t} else {\n\t\ttmp = (busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\t      local->group_capacity;\n\t}\n\tcapa_move += local->group_capacity *\n\t\t    min(local->load_per_task, local->avg_load + tmp);\n\tcapa_move /= SCHED_CAPACITY_SCALE;\n\n\t/* Move if we gain throughput */\n\tif (capa_move > capa_now)\n\t\tenv->imbalance = busiest->load_per_task;\n}\n\n/**\n * calculate_imbalance - Calculate the amount of imbalance present within the\n *\t\t\t groups of a given sched_domain during load balance.\n * @env: load balance environment\n * @sds: statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long max_pull, load_above_capacity = ~0UL;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (busiest->group_type == group_imbalanced) {\n\t\t/*\n\t\t * In the group_imb case we cannot rely on group-wide averages\n\t\t * to ensure CPU-load equilibrium, look at wider averages. XXX\n\t\t */\n\t\tbusiest->load_per_task =\n\t\t\tmin(busiest->load_per_task, sds->avg_load);\n\t}\n\n\t/*\n\t * Avg load of busiest sg can be less and avg load of local sg can\n\t * be greater than avg load across all sgs of sd because avg load\n\t * factors in sg capacity and sgs with smaller group_type are\n\t * skipped when updating the busiest sg:\n\t */\n\tif (busiest->group_type != group_misfit_task &&\n\t    (busiest->avg_load <= sds->avg_load ||\n\t     local->avg_load >= sds->avg_load)) {\n\t\tenv->imbalance = 0;\n\t\treturn fix_small_imbalance(env, sds);\n\t}\n\n\t/*\n\t * If there aren't any idle CPUs, avoid creating some.\n\t */\n\tif (busiest->group_type == group_overloaded &&\n\t    local->group_type   == group_overloaded) {\n\t\tload_above_capacity = busiest->sum_nr_running * SCHED_CAPACITY_SCALE;\n\t\tif (load_above_capacity > busiest->group_capacity) {\n\t\t\tload_above_capacity -= busiest->group_capacity;\n\t\t\tload_above_capacity *= scale_load_down(NICE_0_LOAD);\n\t\t\tload_above_capacity /= busiest->group_capacity;\n\t\t} else\n\t\t\tload_above_capacity = ~0UL;\n\t}\n\n\t/*\n\t * We're trying to get all the CPUs to the average_load, so we don't\n\t * want to push ourselves above the average load, nor do we wish to\n\t * reduce the max loaded CPU below the average load. At the same time,\n\t * we also don't want to reduce the group load below the group\n\t * capacity. Thus we look for the minimum possible imbalance.\n\t */\n\tmax_pull = min(busiest->avg_load - sds->avg_load, load_above_capacity);\n\n\t/* How much load to actually move to equalise the imbalance */\n\tenv->imbalance = min(\n\t\tmax_pull * busiest->group_capacity,\n\t\t(sds->avg_load - local->avg_load) * local->group_capacity\n\t) / SCHED_CAPACITY_SCALE;\n\n\t/* Boost imbalance to allow misfit task to be balanced. */\n\tif (busiest->group_type == group_misfit_task) {\n\t\tenv->imbalance = max_t(long, env->imbalance,\n\t\t\t\t       busiest->group_misfit_task_load);\n\t}\n\n\t/*\n\t * if *imbalance is less than the average load per runnable task\n\t * there is no guarantee that any tasks will be moved so we'll have\n\t * a think about bumping its value to force at least one task to be\n\t * moved\n\t */\n\tif (env->imbalance < busiest->load_per_task)\n\t\treturn fix_small_imbalance(env, sds);\n}\n\n/******* find_busiest_group() helpers end here *********************/\n\n/**\n * find_busiest_group - Returns the busiest group within the sched_domain\n * if there is an imbalance.\n *\n * Also calculates the amount of weighted load which should be moved\n * to restore balance.\n *\n * @env: The load balancing environment.\n *\n * Return:\t- The busiest group if imbalance exists.\n */\nstatic struct sched_group *find_busiest_group(struct lb_env *env)\n{\n\tstruct sg_lb_stats *local, *busiest;\n\tstruct sd_lb_stats sds;\n\n\tinit_sd_lb_stats(&sds);\n\n\t/*\n\t * Compute the various statistics relavent for load balancing at\n\t * this level.\n\t */\n\tupdate_sd_lb_stats(env, &sds);\n\n\tif (static_branch_unlikely(&sched_energy_present)) {\n\t\tstruct root_domain *rd = env->dst_rq->rd;\n\n\t\tif (rcu_dereference(rd->pd) && !READ_ONCE(rd->overutilized))\n\t\t\tgoto out_balanced;\n\t}\n\n\tlocal = &sds.local_stat;\n\tbusiest = &sds.busiest_stat;\n\n\t/* ASYM feature bypasses nice load balance check */\n\tif (check_asym_packing(env, &sds))\n\t\treturn sds.busiest;\n\n\t/* There is no busy sibling group to pull tasks from */\n\tif (!sds.busiest || busiest->sum_nr_running == 0)\n\t\tgoto out_balanced;\n\n\t/* XXX broken for overlapping NUMA groups */\n\tsds.avg_load = (SCHED_CAPACITY_SCALE * sds.total_load)\n\t\t\t\t\t\t/ sds.total_capacity;\n\n\t/*\n\t * If the busiest group is imbalanced the below checks don't\n\t * work because they assume all things are equal, which typically\n\t * isn't true due to cpus_allowed constraints and the like.\n\t */\n\tif (busiest->group_type == group_imbalanced)\n\t\tgoto force_balance;\n\n\t/*\n\t * When dst_cpu is idle, prevent SMP nice and/or asymmetric group\n\t * capacities from resulting in underutilization due to avg_load.\n\t */\n\tif (env->idle != CPU_NOT_IDLE && group_has_capacity(env, local) &&\n\t    busiest->group_no_capacity)\n\t\tgoto force_balance;\n\n\t/* Misfit tasks should be dealt with regardless of the avg load */\n\tif (busiest->group_type == group_misfit_task)\n\t\tgoto force_balance;\n\n\t/*\n\t * If the local group is busier than the selected busiest group\n\t * don't try and pull any tasks.\n\t */\n\tif (local->avg_load >= busiest->avg_load)\n\t\tgoto out_balanced;\n\n\t/*\n\t * Don't pull any tasks if this group is already above the domain\n\t * average load.\n\t */\n\tif (local->avg_load >= sds.avg_load)\n\t\tgoto out_balanced;\n\n\tif (env->idle == CPU_IDLE) {\n\t\t/*\n\t\t * This CPU is idle. If the busiest group is not overloaded\n\t\t * and there is no imbalance between this and busiest group\n\t\t * wrt idle CPUs, it is balanced. The imbalance becomes\n\t\t * significant if the diff is greater than 1 otherwise we\n\t\t * might end up to just move the imbalance on another group\n\t\t */\n\t\tif ((busiest->group_type != group_overloaded) &&\n\t\t\t\t(local->idle_cpus <= (busiest->idle_cpus + 1)))\n\t\t\tgoto out_balanced;\n\t} else {\n\t\t/*\n\t\t * In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use\n\t\t * imbalance_pct to be conservative.\n\t\t */\n\t\tif (100 * busiest->avg_load <=\n\t\t\t\tenv->sd->imbalance_pct * local->avg_load)\n\t\t\tgoto out_balanced;\n\t}\n\nforce_balance:\n\t/* Looks like there is an imbalance. Compute it */\n\tenv->src_grp_type = busiest->group_type;\n\tcalculate_imbalance(env, &sds);\n\treturn env->imbalance ? sds.busiest : NULL;\n\nout_balanced:\n\tenv->imbalance = 0;\n\treturn NULL;\n}\n\n/*\n * find_busiest_queue - find the busiest runqueue among the CPUs in the group.\n */\nstatic struct rq *find_busiest_queue(struct lb_env *env,\n\t\t\t\t     struct sched_group *group)\n{\n\tstruct rq *busiest = NULL, *rq;\n\tunsigned long busiest_load = 0, busiest_capacity = 1;\n\tint i;\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tunsigned long capacity, wl;\n\t\tenum fbq_type rt;\n\n\t\trq = cpu_rq(i);\n\t\trt = fbq_classify_rq(rq);\n\n\t\t/*\n\t\t * We classify groups/runqueues into three groups:\n\t\t *  - regular: there are !numa tasks\n\t\t *  - remote:  there are numa tasks that run on the 'wrong' node\n\t\t *  - all:     there is no distinction\n\t\t *\n\t\t * In order to avoid migrating ideally placed numa tasks,\n\t\t * ignore those when there's better options.\n\t\t *\n\t\t * If we ignore the actual busiest queue to migrate another\n\t\t * task, the next balance pass can still reduce the busiest\n\t\t * queue by moving tasks around inside the node.\n\t\t *\n\t\t * If we cannot move enough load due to this classification\n\t\t * the next pass will adjust the group classification and\n\t\t * allow migration of more tasks.\n\t\t *\n\t\t * Both cases only affect the total convergence complexity.\n\t\t */\n\t\tif (rt > env->fbq_type)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains with misfit tasks we simply\n\t\t * seek the \"biggest\" misfit task.\n\t\t */\n\t\tif (env->src_grp_type == group_misfit_task) {\n\t\t\tif (rq->misfit_task_load > busiest_load) {\n\t\t\t\tbusiest_load = rq->misfit_task_load;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tcapacity = capacity_of(i);\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains, don't pick a CPU that could\n\t\t * eventually lead to active_balancing high->low capacity.\n\t\t * Higher per-CPU capacity is considered better than balancing\n\t\t * average load.\n\t\t */\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    capacity_of(env->dst_cpu) < capacity &&\n\t\t    rq->nr_running == 1)\n\t\t\tcontinue;\n\n\t\twl = weighted_cpuload(rq);\n\n\t\t/*\n\t\t * When comparing with imbalance, use weighted_cpuload()\n\t\t * which is not scaled with the CPU capacity.\n\t\t */\n\n\t\tif (rq->nr_running == 1 && wl > env->imbalance &&\n\t\t    !check_cpu_capacity(rq, env->sd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For the load comparisons with the other CPU's, consider\n\t\t * the weighted_cpuload() scaled with the CPU capacity, so\n\t\t * that the load can be moved away from the CPU that is\n\t\t * potentially running at a lower capacity.\n\t\t *\n\t\t * Thus we're looking for max(wl_i / capacity_i), crosswise\n\t\t * multiplication to rid ourselves of the division works out\n\t\t * to: wl_i * capacity_j > wl_j * capacity_i;  where j is\n\t\t * our previous maximum.\n\t\t */\n\t\tif (wl * busiest_capacity > busiest_load * capacity) {\n\t\t\tbusiest_load = wl;\n\t\t\tbusiest_capacity = capacity;\n\t\t\tbusiest = rq;\n\t\t}\n\t}\n\n\treturn busiest;\n}\n\n/*\n * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but\n * so long as it is large enough.\n */\n#define MAX_PINNED_INTERVAL\t512\n\nstatic int need_active_balance(struct lb_env *env)\n{\n\tstruct sched_domain *sd = env->sd;\n\n\tif (env->idle == CPU_NEWLY_IDLE) {\n\n\t\t/*\n\t\t * ASYM_PACKING needs to force migrate tasks from busy but\n\t\t * lower priority CPUs in order to pack all tasks in the\n\t\t * highest priority CPUs.\n\t\t */\n\t\tif ((sd->flags & SD_ASYM_PACKING) &&\n\t\t    sched_asym_prefer(env->dst_cpu, env->src_cpu))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * The dst_cpu is idle and the src_cpu CPU has only 1 CFS task.\n\t * It's worth migrating the task if the src_cpu's capacity is reduced\n\t * because of other sched_class or IRQs if more capacity stays\n\t * available on dst_cpu.\n\t */\n\tif ((env->idle != CPU_NOT_IDLE) &&\n\t    (env->src_rq->cfs.h_nr_running == 1)) {\n\t\tif ((check_cpu_capacity(env->src_rq, sd)) &&\n\t\t    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))\n\t\t\treturn 1;\n\t}\n\n\tif (env->src_grp_type == group_misfit_task)\n\t\treturn 1;\n\n\treturn unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);\n}\n\nstatic int active_load_balance_cpu_stop(void *data);\n\nstatic int should_we_balance(struct lb_env *env)\n{\n\tstruct sched_group *sg = env->sd->groups;\n\tint cpu, balance_cpu = -1;\n\n\t/*\n\t * Ensure the balancing environment is consistent; can happen\n\t * when the softirq triggers 'during' hotplug.\n\t */\n\tif (!cpumask_test_cpu(env->dst_cpu, env->cpus))\n\t\treturn 0;\n\n\t/*\n\t * In the newly idle case, we will allow all the CPUs\n\t * to do the newly idle load balance.\n\t */\n\tif (env->idle == CPU_NEWLY_IDLE)\n\t\treturn 1;\n\n\t/* Try to find first idle CPU */\n\tfor_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {\n\t\tif (!idle_cpu(cpu))\n\t\t\tcontinue;\n\n\t\tbalance_cpu = cpu;\n\t\tbreak;\n\t}\n\n\tif (balance_cpu == -1)\n\t\tbalance_cpu = group_balance_cpu(sg);\n\n\t/*\n\t * First idle CPU or the first CPU(busiest) in this sched group\n\t * is eligible for doing load balancing at this and above domains.\n\t */\n\treturn balance_cpu == env->dst_cpu;\n}\n\n/*\n * Check this_cpu to ensure it is balanced within domain. Attempt to move\n * tasks if there is an imbalance.\n */\nstatic int load_balance(int this_cpu, struct rq *this_rq,\n\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n\t\t\tint *continue_balancing)\n{\n\tint ld_moved, cur_ld_moved, active_balance = 0;\n\tstruct sched_domain *sd_parent = sd->parent;\n\tstruct sched_group *group;\n\tstruct rq *busiest;\n\tstruct rq_flags rf;\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);\n\n\tstruct lb_env env = {\n\t\t.sd\t\t= sd,\n\t\t.dst_cpu\t= this_cpu,\n\t\t.dst_rq\t\t= this_rq,\n\t\t.dst_grpmask    = sched_group_span(sd->groups),\n\t\t.idle\t\t= idle,\n\t\t.loop_break\t= sched_nr_migrate_break,\n\t\t.cpus\t\t= cpus,\n\t\t.fbq_type\t= all,\n\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),\n\t};\n\n\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);\n\n\tschedstat_inc(sd->lb_count[idle]);\n\nredo:\n\tif (!should_we_balance(&env)) {\n\t\t*continue_balancing = 0;\n\t\tgoto out_balanced;\n\t}\n\n\tgroup = find_busiest_group(&env);\n\tif (!group) {\n\t\tschedstat_inc(sd->lb_nobusyg[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tbusiest = find_busiest_queue(&env, group);\n\tif (!busiest) {\n\t\tschedstat_inc(sd->lb_nobusyq[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tBUG_ON(busiest == env.dst_rq);\n\n\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);\n\n\tenv.src_cpu = busiest->cpu;\n\tenv.src_rq = busiest;\n\n\tld_moved = 0;\n\tif (busiest->nr_running > 1) {\n\t\t/*\n\t\t * Attempt to move tasks. If find_busiest_group has found\n\t\t * an imbalance but busiest->nr_running <= 1, the group is\n\t\t * still unbalanced. ld_moved simply stays zero, so it is\n\t\t * correctly treated as an imbalance.\n\t\t */\n\t\tenv.flags |= LBF_ALL_PINNED;\n\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);\n\nmore_balance:\n\t\trq_lock_irqsave(busiest, &rf);\n\t\tupdate_rq_clock(busiest);\n\n\t\t/*\n\t\t * cur_ld_moved - load moved in current iteration\n\t\t * ld_moved     - cumulative load moved across iterations\n\t\t */\n\t\tcur_ld_moved = detach_tasks(&env);\n\n\t\t/*\n\t\t * We've detached some tasks from busiest_rq. Every\n\t\t * task is masked \"TASK_ON_RQ_MIGRATING\", so we can safely\n\t\t * unlock busiest->lock, and we are able to be sure\n\t\t * that nobody can manipulate the tasks in parallel.\n\t\t * See task_rq_lock() family for the details.\n\t\t */\n\n\t\trq_unlock(busiest, &rf);\n\n\t\tif (cur_ld_moved) {\n\t\t\tattach_tasks(&env);\n\t\t\tld_moved += cur_ld_moved;\n\t\t}\n\n\t\tlocal_irq_restore(rf.flags);\n\n\t\tif (env.flags & LBF_NEED_BREAK) {\n\t\t\tenv.flags &= ~LBF_NEED_BREAK;\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * Revisit (affine) tasks on src_cpu that couldn't be moved to\n\t\t * us and move them to an alternate dst_cpu in our sched_group\n\t\t * where they can run. The upper limit on how many times we\n\t\t * iterate on same src_cpu is dependent on number of CPUs in our\n\t\t * sched_group.\n\t\t *\n\t\t * This changes load balance semantics a bit on who can move\n\t\t * load to a given_cpu. In addition to the given_cpu itself\n\t\t * (or a ilb_cpu acting on its behalf where given_cpu is\n\t\t * nohz-idle), we now have balance_cpu in a position to move\n\t\t * load to given_cpu. In rare situations, this may cause\n\t\t * conflicts (balance_cpu and given_cpu/ilb_cpu deciding\n\t\t * _independently_ and at _same_ time to move some load to\n\t\t * given_cpu) causing exceess load to be moved to given_cpu.\n\t\t * This however should not happen so much in practice and\n\t\t * moreover subsequent load balance cycles should correct the\n\t\t * excess load moved.\n\t\t */\n\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {\n\n\t\t\t/* Prevent to re-select dst_cpu via env's CPUs */\n\t\t\tcpumask_clear_cpu(env.dst_cpu, env.cpus);\n\n\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);\n\t\t\tenv.dst_cpu\t = env.new_dst_cpu;\n\t\t\tenv.flags\t&= ~LBF_DST_PINNED;\n\t\t\tenv.loop\t = 0;\n\t\t\tenv.loop_break\t = sched_nr_migrate_break;\n\n\t\t\t/*\n\t\t\t * Go back to \"more_balance\" rather than \"redo\" since we\n\t\t\t * need to continue with same src_cpu.\n\t\t\t */\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * We failed to reach balance because of affinity.\n\t\t */\n\t\tif (sd_parent) {\n\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)\n\t\t\t\t*group_imbalance = 1;\n\t\t}\n\n\t\t/* All tasks on this runqueue were pinned by CPU affinity */\n\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {\n\t\t\tcpumask_clear_cpu(cpu_of(busiest), cpus);\n\t\t\t/*\n\t\t\t * Attempting to continue load balancing at the current\n\t\t\t * sched_domain level only makes sense if there are\n\t\t\t * active CPUs remaining as possible busiest CPUs to\n\t\t\t * pull load from which are not contained within the\n\t\t\t * destination group that is receiving any migrated\n\t\t\t * load.\n\t\t\t */\n\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {\n\t\t\t\tenv.loop = 0;\n\t\t\t\tenv.loop_break = sched_nr_migrate_break;\n\t\t\t\tgoto redo;\n\t\t\t}\n\t\t\tgoto out_all_pinned;\n\t\t}\n\t}\n\n\tif (!ld_moved) {\n\t\tschedstat_inc(sd->lb_failed[idle]);\n\t\t/*\n\t\t * Increment the failure counter only on periodic balance.\n\t\t * We do not want newidle balance, which can be very\n\t\t * frequent, pollute the failure counter causing\n\t\t * excessive cache_hot migrations and active balances.\n\t\t */\n\t\tif (idle != CPU_NEWLY_IDLE)\n\t\t\tsd->nr_balance_failed++;\n\n\t\tif (need_active_balance(&env)) {\n\t\t\tunsigned long flags;\n\n\t\t\traw_spin_lock_irqsave(&busiest->lock, flags);\n\n\t\t\t/*\n\t\t\t * Don't kick the active_load_balance_cpu_stop,\n\t\t\t * if the curr task on busiest CPU can't be\n\t\t\t * moved to this_cpu:\n\t\t\t */\n\t\t\tif (!cpumask_test_cpu(this_cpu, &busiest->curr->cpus_allowed)) {\n\t\t\t\traw_spin_unlock_irqrestore(&busiest->lock,\n\t\t\t\t\t\t\t    flags);\n\t\t\t\tenv.flags |= LBF_ALL_PINNED;\n\t\t\t\tgoto out_one_pinned;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * ->active_balance synchronizes accesses to\n\t\t\t * ->active_balance_work.  Once set, it's cleared\n\t\t\t * only after active load balance is finished.\n\t\t\t */\n\t\t\tif (!busiest->active_balance) {\n\t\t\t\tbusiest->active_balance = 1;\n\t\t\t\tbusiest->push_cpu = this_cpu;\n\t\t\t\tactive_balance = 1;\n\t\t\t}\n\t\t\traw_spin_unlock_irqrestore(&busiest->lock, flags);\n\n\t\t\tif (active_balance) {\n\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),\n\t\t\t\t\tactive_load_balance_cpu_stop, busiest,\n\t\t\t\t\t&busiest->active_balance_work);\n\t\t\t}\n\n\t\t\t/* We've kicked active balancing, force task migration. */\n\t\t\tsd->nr_balance_failed = sd->cache_nice_tries+1;\n\t\t}\n\t} else\n\t\tsd->nr_balance_failed = 0;\n\n\tif (likely(!active_balance)) {\n\t\t/* We were unbalanced, so reset the balancing interval */\n\t\tsd->balance_interval = sd->min_interval;\n\t} else {\n\t\t/*\n\t\t * If we've begun active balancing, start to back off. This\n\t\t * case may not be covered by the all_pinned logic if there\n\t\t * is only 1 task on the busy runqueue (because we don't call\n\t\t * detach_tasks).\n\t\t */\n\t\tif (sd->balance_interval < sd->max_interval)\n\t\t\tsd->balance_interval *= 2;\n\t}\n\n\tgoto out;\n\nout_balanced:\n\t/*\n\t * We reach balance although we may have faced some affinity\n\t * constraints. Clear the imbalance flag if it was set.\n\t */\n\tif (sd_parent) {\n\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\tif (*group_imbalance)\n\t\t\t*group_imbalance = 0;\n\t}\n\nout_all_pinned:\n\t/*\n\t * We reach balance because all tasks are pinned at this level so\n\t * we can't migrate them. Let the imbalance flag set so parent level\n\t * can try to migrate them.\n\t */\n\tschedstat_inc(sd->lb_balanced[idle]);\n\n\tsd->nr_balance_failed = 0;\n\nout_one_pinned:\n\tld_moved = 0;\n\n\t/*\n\t * idle_balance() disregards balance intervals, so we could repeatedly\n\t * reach this code, which would lead to balance_interval skyrocketting\n\t * in a short amount of time. Skip the balance_interval increase logic\n\t * to avoid that.\n\t */\n\tif (env.idle == CPU_NEWLY_IDLE)\n\t\tgoto out;\n\n\t/* tune up the balancing interval */\n\tif ((env.flags & LBF_ALL_PINNED &&\n\t     sd->balance_interval < MAX_PINNED_INTERVAL) ||\n\t    sd->balance_interval < sd->max_interval)\n\t\tsd->balance_interval *= 2;\nout:\n\treturn ld_moved;\n}\n\nstatic inline unsigned long\nget_sd_balance_interval(struct sched_domain *sd, int cpu_busy)\n{\n\tunsigned long interval = sd->balance_interval;\n\n\tif (cpu_busy)\n\t\tinterval *= sd->busy_factor;\n\n\t/* scale ms to jiffies */\n\tinterval = msecs_to_jiffies(interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\n\treturn interval;\n}\n\nstatic inline void\nupdate_next_balance(struct sched_domain *sd, unsigned long *next_balance)\n{\n\tunsigned long interval, next;\n\n\t/* used by idle balance, so cpu_busy = 0 */\n\tinterval = get_sd_balance_interval(sd, 0);\n\tnext = sd->last_balance + interval;\n\n\tif (time_after(*next_balance, next))\n\t\t*next_balance = next;\n}\n\n/*\n * active_load_balance_cpu_stop is run by the CPU stopper. It pushes\n * running tasks off the busiest CPU onto idle CPUs. It requires at\n * least 1 task to be running on each physical CPU where possible, and\n * avoids physical / logical imbalances.\n */\nstatic int active_load_balance_cpu_stop(void *data)\n{\n\tstruct rq *busiest_rq = data;\n\tint busiest_cpu = cpu_of(busiest_rq);\n\tint target_cpu = busiest_rq->push_cpu;\n\tstruct rq *target_rq = cpu_rq(target_cpu);\n\tstruct sched_domain *sd;\n\tstruct task_struct *p = NULL;\n\tstruct rq_flags rf;\n\n\trq_lock_irq(busiest_rq, &rf);\n\t/*\n\t * Between queueing the stop-work and running it is a hole in which\n\t * CPUs can become inactive. We should not move tasks from or to\n\t * inactive CPUs.\n\t */\n\tif (!cpu_active(busiest_cpu) || !cpu_active(target_cpu))\n\t\tgoto out_unlock;\n\n\t/* Make sure the requested CPU hasn't gone down in the meantime: */\n\tif (unlikely(busiest_cpu != smp_processor_id() ||\n\t\t     !busiest_rq->active_balance))\n\t\tgoto out_unlock;\n\n\t/* Is there any task to move? */\n\tif (busiest_rq->nr_running <= 1)\n\t\tgoto out_unlock;\n\n\t/*\n\t * This condition is \"impossible\", if it occurs\n\t * we need to fix it. Originally reported by\n\t * Bjorn Helgaas on a 128-CPU setup.\n\t */\n\tBUG_ON(busiest_rq == target_rq);\n\n\t/* Search for an sd spanning us and the target CPU. */\n\trcu_read_lock();\n\tfor_each_domain(target_cpu, sd) {\n\t\tif ((sd->flags & SD_LOAD_BALANCE) &&\n\t\t    cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))\n\t\t\t\tbreak;\n\t}\n\n\tif (likely(sd)) {\n\t\tstruct lb_env env = {\n\t\t\t.sd\t\t= sd,\n\t\t\t.dst_cpu\t= target_cpu,\n\t\t\t.dst_rq\t\t= target_rq,\n\t\t\t.src_cpu\t= busiest_rq->cpu,\n\t\t\t.src_rq\t\t= busiest_rq,\n\t\t\t.idle\t\t= CPU_IDLE,\n\t\t\t/*\n\t\t\t * can_migrate_task() doesn't need to compute new_dst_cpu\n\t\t\t * for active balancing. Since we have CPU_IDLE, but no\n\t\t\t * @dst_grpmask we need to make that test go away with lying\n\t\t\t * about DST_PINNED.\n\t\t\t */\n\t\t\t.flags\t\t= LBF_DST_PINNED,\n\t\t};\n\n\t\tschedstat_inc(sd->alb_count);\n\t\tupdate_rq_clock(busiest_rq);\n\n\t\tp = detach_one_task(&env);\n\t\tif (p) {\n\t\t\tschedstat_inc(sd->alb_pushed);\n\t\t\t/* Active balancing done, reset the failure counter. */\n\t\t\tsd->nr_balance_failed = 0;\n\t\t} else {\n\t\t\tschedstat_inc(sd->alb_failed);\n\t\t}\n\t}\n\trcu_read_unlock();\nout_unlock:\n\tbusiest_rq->active_balance = 0;\n\trq_unlock(busiest_rq, &rf);\n\n\tif (p)\n\t\tattach_one_task(target_rq, p);\n\n\tlocal_irq_enable();\n\n\treturn 0;\n}\n\nstatic DEFINE_SPINLOCK(balancing);\n\n/*\n * Scale the max load_balance interval with the number of CPUs in the system.\n * This trades load-balance latency on larger machines for less cross talk.\n */\nvoid update_max_interval(void)\n{\n\tmax_load_balance_interval = HZ*num_online_cpus()/10;\n}\n\n/*\n * It checks each scheduling domain to see if it is due to be balanced,\n * and initiates a balancing operation if so.\n *\n * Balancing parameters are set up in init_sched_domains.\n */\nstatic void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)\n{\n\tint continue_balancing = 1;\n\tint cpu = rq->cpu;\n\tunsigned long interval;\n\tstruct sched_domain *sd;\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long next_balance = jiffies + 60*HZ;\n\tint update_next_balance = 0;\n\tint need_serialize, need_decay = 0;\n\tu64 max_cost = 0;\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\t/*\n\t\t * Decay the newidle max times here because this is a regular\n\t\t * visit to all the domains. Decay ~1% per second.\n\t\t */\n\t\tif (time_after(jiffies, sd->next_decay_max_lb_cost)) {\n\t\t\tsd->max_newidle_lb_cost =\n\t\t\t\t(sd->max_newidle_lb_cost * 253) / 256;\n\t\t\tsd->next_decay_max_lb_cost = jiffies + HZ;\n\t\t\tneed_decay = 1;\n\t\t}\n\t\tmax_cost += sd->max_newidle_lb_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Stop the load balance at this level. There is another\n\t\t * CPU in our sched group which is doing load balancing more\n\t\t * actively.\n\t\t */\n\t\tif (!continue_balancing) {\n\t\t\tif (need_decay)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\n\t\tneed_serialize = sd->flags & SD_SERIALIZE;\n\t\tif (need_serialize) {\n\t\t\tif (!spin_trylock(&balancing))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (time_after_eq(jiffies, sd->last_balance + interval)) {\n\t\t\tif (load_balance(cpu, rq, sd, idle, &continue_balancing)) {\n\t\t\t\t/*\n\t\t\t\t * The LBF_DST_PINNED logic could have changed\n\t\t\t\t * env->dst_cpu, so we can't know our idle\n\t\t\t\t * state even if we migrated tasks. Update it.\n\t\t\t\t */\n\t\t\t\tidle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;\n\t\t\t}\n\t\t\tsd->last_balance = jiffies;\n\t\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\t\t}\n\t\tif (need_serialize)\n\t\t\tspin_unlock(&balancing);\nout:\n\t\tif (time_after(next_balance, sd->last_balance + interval)) {\n\t\t\tnext_balance = sd->last_balance + interval;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\tif (need_decay) {\n\t\t/*\n\t\t * Ensure the rq-wide value also decays but keep it at a\n\t\t * reasonable floor to avoid funnies with rq->avg_idle.\n\t\t */\n\t\trq->max_idle_balance_cost =\n\t\t\tmax((u64)sysctl_sched_migration_cost, max_cost);\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the cpu is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance)) {\n\t\trq->next_balance = next_balance;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\t\t/*\n\t\t * If this CPU has been elected to perform the nohz idle\n\t\t * balance. Other idle CPUs have already rebalanced with\n\t\t * nohz_idle_balance() and nohz.next_balance has been\n\t\t * updated accordingly. This CPU is now running the idle load\n\t\t * balance for itself and we need to update the\n\t\t * nohz.next_balance accordingly.\n\t\t */\n\t\tif ((idle == CPU_IDLE) && time_after(nohz.next_balance, rq->next_balance))\n\t\t\tnohz.next_balance = rq->next_balance;\n#endif\n\t}\n}\n\nstatic inline int on_null_domain(struct rq *rq)\n{\n\treturn unlikely(!rcu_dereference_sched(rq->sd));\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * idle load balancing details\n * - When one of the busy CPUs notice that there may be an idle rebalancing\n *   needed, they will kick the idle load balancer, which then does idle\n *   load balancing for all the idle CPUs.\n */\n\nstatic inline int find_new_ilb(void)\n{\n\tint ilb = cpumask_first(nohz.idle_cpus_mask);\n\n\tif (ilb < nr_cpu_ids && idle_cpu(ilb))\n\t\treturn ilb;\n\n\treturn nr_cpu_ids;\n}\n\n/*\n * Kick a CPU to do the nohz balancing, if it is time for it. We pick the\n * nohz_load_balancer CPU (if there is one) otherwise fallback to any idle\n * CPU (if there is one).\n */\nstatic void kick_ilb(unsigned int flags)\n{\n\tint ilb_cpu;\n\n\tnohz.next_balance++;\n\n\tilb_cpu = find_new_ilb();\n\n\tif (ilb_cpu >= nr_cpu_ids)\n\t\treturn;\n\n\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));\n\tif (flags & NOHZ_KICK_MASK)\n\t\treturn;\n\n\t/*\n\t * Use smp_send_reschedule() instead of resched_cpu().\n\t * This way we generate a sched IPI on the target CPU which\n\t * is idle. And the softirq performing nohz idle load balance\n\t * will be run before returning from the IPI.\n\t */\n\tsmp_send_reschedule(ilb_cpu);\n}\n\n/*\n * Current heuristic for kicking the idle load balancer in the presence\n * of an idle cpu in the system.\n *   - This rq has more than one task.\n *   - This rq has at least one CFS task and the capacity of the CPU is\n *     significantly reduced because of RT tasks or IRQs.\n *   - At parent of LLC scheduler domain level, this cpu's scheduler group has\n *     multiple busy cpu.\n *   - For SD_ASYM_PACKING, if the lower numbered cpu's in the scheduler\n *     domain span are idle.\n */\nstatic void nohz_balancer_kick(struct rq *rq)\n{\n\tunsigned long now = jiffies;\n\tstruct sched_domain_shared *sds;\n\tstruct sched_domain *sd;\n\tint nr_busy, i, cpu = rq->cpu;\n\tunsigned int flags = 0;\n\n\tif (unlikely(rq->idle_balance))\n\t\treturn;\n\n\t/*\n\t * We may be recently in ticked or tickless idle mode. At the first\n\t * busy tick after returning from idle, we will update the busy stats.\n\t */\n\tnohz_balance_exit_idle(rq);\n\n\t/*\n\t * None are in tickless mode and hence no need for NOHZ idle load\n\t * balancing.\n\t */\n\tif (likely(!atomic_read(&nohz.nr_cpus)))\n\t\treturn;\n\n\tif (READ_ONCE(nohz.has_blocked) &&\n\t    time_after(now, READ_ONCE(nohz.next_blocked)))\n\t\tflags = NOHZ_STATS_KICK;\n\n\tif (time_before(now, nohz.next_balance))\n\t\tgoto out;\n\n\tif (rq->nr_running >= 2 || rq->misfit_task_load) {\n\t\tflags = NOHZ_KICK_MASK;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds) {\n\t\t/*\n\t\t * XXX: write a coherent comment on why we do this.\n\t\t * See also: http://lkml.kernel.org/r/20111202010832.602203411@sbsiddha-desk.sc.intel.com\n\t\t */\n\t\tnr_busy = atomic_read(&sds->nr_busy_cpus);\n\t\tif (nr_busy > 1) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\n\t}\n\n\tsd = rcu_dereference(rq->sd);\n\tif (sd) {\n\t\tif ((rq->cfs.h_nr_running >= 1) &&\n\t\t\t\tcheck_cpu_capacity(rq, sd)) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_asym_packing, cpu));\n\tif (sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\t\tif (i == cpu ||\n\t\t\t    !cpumask_test_cpu(i, nohz.idle_cpus_mask))\n\t\t\t\tcontinue;\n\n\t\t\tif (sched_asym_prefer(i, cpu)) {\n\t\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\nout:\n\tif (flags)\n\t\tkick_ilb(flags);\n}\n\nstatic void set_cpu_sd_state_busy(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || !sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 0;\n\n\tatomic_inc(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\nvoid nohz_balance_exit_idle(struct rq *rq)\n{\n\tSCHED_WARN_ON(rq != this_rq());\n\n\tif (likely(!rq->nohz_tick_stopped))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 0;\n\tcpumask_clear_cpu(rq->cpu, nohz.idle_cpus_mask);\n\tatomic_dec(&nohz.nr_cpus);\n\n\tset_cpu_sd_state_busy(rq->cpu);\n}\n\nstatic void set_cpu_sd_state_idle(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 1;\n\n\tatomic_dec(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\n/*\n * This routine will record that the CPU is going idle with tick stopped.\n * This info will be used in performing idle load balancing in the future.\n */\nvoid nohz_balance_enter_idle(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tSCHED_WARN_ON(cpu != smp_processor_id());\n\n\t/* If this CPU is going down, then nothing needs to be done: */\n\tif (!cpu_active(cpu))\n\t\treturn;\n\n\t/* Spare idle load balancing on CPUs that don't want to be disturbed: */\n\tif (!housekeeping_cpu(cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/*\n\t * Can be set safely without rq->lock held\n\t * If a clear happens, it will have evaluated last additions because\n\t * rq->lock is held during the check and the clear\n\t */\n\trq->has_blocked_load = 1;\n\n\t/*\n\t * The tick is still stopped but load could have been added in the\n\t * meantime. We set the nohz.has_blocked flag to trig a check of the\n\t * *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear\n\t * of nohz.has_blocked can only happen after checking the new load\n\t */\n\tif (rq->nohz_tick_stopped)\n\t\tgoto out;\n\n\t/* If we're a completely isolated CPU, we don't play: */\n\tif (on_null_domain(rq))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 1;\n\n\tcpumask_set_cpu(cpu, nohz.idle_cpus_mask);\n\tatomic_inc(&nohz.nr_cpus);\n\n\t/*\n\t * Ensures that if nohz_idle_balance() fails to observe our\n\t * @idle_cpus_mask store, it must observe the @has_blocked\n\t * store.\n\t */\n\tsmp_mb__after_atomic();\n\n\tset_cpu_sd_state_idle(cpu);\n\nout:\n\t/*\n\t * Each time a cpu enter idle, we assume that it has blocked load and\n\t * enable the periodic update of the load of idle cpus\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 1);\n}\n\n/*\n * Internal function that runs load balance for all idle cpus. The load balance\n * can be a simple update of blocked load or a complete load balance with\n * tasks movement depending of flags.\n * The function returns false if the loop has stopped before running\n * through all idle CPUs.\n */\nstatic bool _nohz_idle_balance(struct rq *this_rq, unsigned int flags,\n\t\t\t       enum cpu_idle_type idle)\n{\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long now = jiffies;\n\tunsigned long next_balance = now + 60*HZ;\n\tbool has_blocked_load = false;\n\tint update_next_balance = 0;\n\tint this_cpu = this_rq->cpu;\n\tint balance_cpu;\n\tint ret = false;\n\tstruct rq *rq;\n\n\tSCHED_WARN_ON((flags & NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK);\n\n\t/*\n\t * We assume there will be no idle load after this update and clear\n\t * the has_blocked flag. If a cpu enters idle in the mean time, it will\n\t * set the has_blocked flag and trig another update of idle load.\n\t * Because a cpu that becomes idle, is added to idle_cpus_mask before\n\t * setting the flag, we are sure to not clear the state and not\n\t * check the load of an idle cpu.\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 0);\n\n\t/*\n\t * Ensures that if we miss the CPU, we must see the has_blocked\n\t * store from nohz_balance_enter_idle().\n\t */\n\tsmp_mb();\n\n\tfor_each_cpu(balance_cpu, nohz.idle_cpus_mask) {\n\t\tif (balance_cpu == this_cpu || !idle_cpu(balance_cpu))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If this CPU gets work to do, stop the load balancing\n\t\t * work being done for other CPUs. Next load\n\t\t * balancing owner will pick it up.\n\t\t */\n\t\tif (need_resched()) {\n\t\t\thas_blocked_load = true;\n\t\t\tgoto abort;\n\t\t}\n\n\t\trq = cpu_rq(balance_cpu);\n\n\t\thas_blocked_load |= update_nohz_stats(rq, true);\n\n\t\t/*\n\t\t * If time for next balance is due,\n\t\t * do the balance.\n\t\t */\n\t\tif (time_after_eq(jiffies, rq->next_balance)) {\n\t\t\tstruct rq_flags rf;\n\n\t\t\trq_lock_irqsave(rq, &rf);\n\t\t\tupdate_rq_clock(rq);\n\t\t\tcpu_load_update_idle(rq);\n\t\t\trq_unlock_irqrestore(rq, &rf);\n\n\t\t\tif (flags & NOHZ_BALANCE_KICK)\n\t\t\t\trebalance_domains(rq, CPU_IDLE);\n\t\t}\n\n\t\tif (time_after(next_balance, rq->next_balance)) {\n\t\t\tnext_balance = rq->next_balance;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\n\t/* Newly idle CPU doesn't need an update */\n\tif (idle != CPU_NEWLY_IDLE) {\n\t\tupdate_blocked_averages(this_cpu);\n\t\thas_blocked_load |= this_rq->has_blocked_load;\n\t}\n\n\tif (flags & NOHZ_BALANCE_KICK)\n\t\trebalance_domains(this_rq, CPU_IDLE);\n\n\tWRITE_ONCE(nohz.next_blocked,\n\t\tnow + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\n\t/* The full idle balance loop has been done */\n\tret = true;\n\nabort:\n\t/* There is still blocked load, enable periodic update */\n\tif (has_blocked_load)\n\t\tWRITE_ONCE(nohz.has_blocked, 1);\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the CPU is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance))\n\t\tnohz.next_balance = next_balance;\n\n\treturn ret;\n}\n\n/*\n * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the\n * rebalancing for all the cpus for whom scheduler ticks are stopped.\n */\nstatic bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\tint this_cpu = this_rq->cpu;\n\tunsigned int flags;\n\n\tif (!(atomic_read(nohz_flags(this_cpu)) & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\tif (idle != CPU_IDLE) {\n\t\tatomic_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\t\treturn false;\n\t}\n\n\t/* could be _relaxed() */\n\tflags = atomic_fetch_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\tif (!(flags & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\t_nohz_idle_balance(this_rq, flags, idle);\n\n\treturn true;\n}\n\nstatic void nohz_newidle_balance(struct rq *this_rq)\n{\n\tint this_cpu = this_rq->cpu;\n\n\t/*\n\t * This CPU doesn't want to be disturbed by scheduler\n\t * housekeeping\n\t */\n\tif (!housekeeping_cpu(this_cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/* Will wake up very soon. No time for doing anything else*/\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost)\n\t\treturn;\n\n\t/* Don't need to update blocked load of idle CPUs*/\n\tif (!READ_ONCE(nohz.has_blocked) ||\n\t    time_before(jiffies, READ_ONCE(nohz.next_blocked)))\n\t\treturn;\n\n\traw_spin_unlock(&this_rq->lock);\n\t/*\n\t * This CPU is going to be idle and blocked load of idle CPUs\n\t * need to be updated. Run the ilb locally as it is a good\n\t * candidate for ilb instead of waking up another idle CPU.\n\t * Kick an normal ilb if we failed to do the update.\n\t */\n\tif (!_nohz_idle_balance(this_rq, NOHZ_STATS_KICK, CPU_NEWLY_IDLE))\n\t\tkick_ilb(NOHZ_STATS_KICK);\n\traw_spin_lock(&this_rq->lock);\n}\n\n#else /* !CONFIG_NO_HZ_COMMON */\nstatic inline void nohz_balancer_kick(struct rq *rq) { }\n\nstatic inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\treturn false;\n}\n\nstatic inline void nohz_newidle_balance(struct rq *this_rq) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\n/*\n * idle_balance is called by schedule() if this_cpu is about to become\n * idle. Attempts to pull tasks from other CPUs.\n */\nstatic int idle_balance(struct rq *this_rq, struct rq_flags *rf)\n{\n\tunsigned long next_balance = jiffies + HZ;\n\tint this_cpu = this_rq->cpu;\n\tstruct sched_domain *sd;\n\tint pulled_task = 0;\n\tu64 curr_cost = 0;\n\n\t/*\n\t * We must set idle_stamp _before_ calling idle_balance(), such that we\n\t * measure the duration of idle_balance() as idle time.\n\t */\n\tthis_rq->idle_stamp = rq_clock(this_rq);\n\n\t/*\n\t * Do not pull tasks towards !active CPUs...\n\t */\n\tif (!cpu_active(this_cpu))\n\t\treturn 0;\n\n\t/*\n\t * This is OK, because current is on_cpu, which avoids it being picked\n\t * for load-balance and preemption/IRQs are still disabled avoiding\n\t * further scheduler activity on it and we're being very careful to\n\t * re-start the picking loop.\n\t */\n\trq_unpin_lock(this_rq, rf);\n\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost ||\n\t    !READ_ONCE(this_rq->rd->overload)) {\n\n\t\trcu_read_lock();\n\t\tsd = rcu_dereference_check_sched_domain(this_rq->sd);\n\t\tif (sd)\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\trcu_read_unlock();\n\n\t\tnohz_newidle_balance(this_rq);\n\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock(&this_rq->lock);\n\n\tupdate_blocked_averages(this_cpu);\n\trcu_read_lock();\n\tfor_each_domain(this_cpu, sd) {\n\t\tint continue_balancing = 1;\n\t\tu64 t0, domain_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\tif (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost) {\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (sd->flags & SD_BALANCE_NEWIDLE) {\n\t\t\tt0 = sched_clock_cpu(this_cpu);\n\n\t\t\tpulled_task = load_balance(this_cpu, this_rq,\n\t\t\t\t\t\t   sd, CPU_NEWLY_IDLE,\n\t\t\t\t\t\t   &continue_balancing);\n\n\t\t\tdomain_cost = sched_clock_cpu(this_cpu) - t0;\n\t\t\tif (domain_cost > sd->max_newidle_lb_cost)\n\t\t\t\tsd->max_newidle_lb_cost = domain_cost;\n\n\t\t\tcurr_cost += domain_cost;\n\t\t}\n\n\t\tupdate_next_balance(sd, &next_balance);\n\n\t\t/*\n\t\t * Stop searching for tasks to pull if there are\n\t\t * now runnable tasks on this rq.\n\t\t */\n\t\tif (pulled_task || this_rq->nr_running > 0)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\traw_spin_lock(&this_rq->lock);\n\n\tif (curr_cost > this_rq->max_idle_balance_cost)\n\t\tthis_rq->max_idle_balance_cost = curr_cost;\n\nout:\n\t/*\n\t * While browsing the domains, we released the rq lock, a task could\n\t * have been enqueued in the meantime. Since we're not going idle,\n\t * pretend we pulled a task.\n\t */\n\tif (this_rq->cfs.h_nr_running && !pulled_task)\n\t\tpulled_task = 1;\n\n\t/* Move the next balance forward */\n\tif (time_after(this_rq->next_balance, next_balance))\n\t\tthis_rq->next_balance = next_balance;\n\n\t/* Is there a task of a high priority class? */\n\tif (this_rq->nr_running != this_rq->cfs.h_nr_running)\n\t\tpulled_task = -1;\n\n\tif (pulled_task)\n\t\tthis_rq->idle_stamp = 0;\n\n\trq_repin_lock(this_rq, rf);\n\n\treturn pulled_task;\n}\n\n/*\n * run_rebalance_domains is triggered when needed from the scheduler tick.\n * Also triggered for nohz idle balancing (with nohz_balancing_kick set).\n */\nstatic __latent_entropy void run_rebalance_domains(struct softirq_action *h)\n{\n\tstruct rq *this_rq = this_rq();\n\tenum cpu_idle_type idle = this_rq->idle_balance ?\n\t\t\t\t\t\tCPU_IDLE : CPU_NOT_IDLE;\n\n\t/*\n\t * If this CPU has a pending nohz_balance_kick, then do the\n\t * balancing on behalf of the other idle CPUs whose ticks are\n\t * stopped. Do nohz_idle_balance *before* rebalance_domains to\n\t * give the idle CPUs a chance to load balance. Else we may\n\t * load balance only within the local sched_domain hierarchy\n\t * and abort nohz_idle_balance altogether if we pull some load.\n\t */\n\tif (nohz_idle_balance(this_rq, idle))\n\t\treturn;\n\n\t/* normal load balance */\n\tupdate_blocked_averages(this_rq->cpu);\n\trebalance_domains(this_rq, idle);\n}\n\n/*\n * Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing.\n */\nvoid trigger_load_balance(struct rq *rq)\n{\n\t/* Don't need to rebalance while attached to NULL domain */\n\tif (unlikely(on_null_domain(rq)))\n\t\treturn;\n\n\tif (time_after_eq(jiffies, rq->next_balance))\n\t\traise_softirq(SCHED_SOFTIRQ);\n\n\tnohz_balancer_kick(rq);\n}\n\nstatic void rq_online_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\tupdate_runtime_enabled(rq);\n}\n\nstatic void rq_offline_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\t/* Ensure any throttled groups are reachable by pick_next_task */\n\tunthrottle_offline_cfs_rqs(rq);\n}\n\n#endif /* CONFIG_SMP */\n\n/*\n * scheduler tick hitting a task of our scheduling class.\n *\n * NOTE: This function can be called remotely by the tick offload that\n * goes along full dynticks. Therefore no local assumption can be made\n * and everything must be accessed through the @rq and @curr passed in\n * parameters.\n */\nstatic void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tentity_tick(cfs_rq, se, queued);\n\t}\n\n\tif (static_branch_unlikely(&sched_numa_balancing))\n\t\ttask_tick_numa(rq, curr);\n\n\tupdate_misfit_status(curr, rq);\n\tupdate_overutilized_status(task_rq(curr));\n}\n\n/*\n * called on fork with the child task as argument from the parent's context\n *  - child not yet on the tasklist\n *  - preemption disabled\n */\nstatic void task_fork_fair(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se, *curr;\n\tstruct rq *rq = this_rq();\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\tcfs_rq = task_cfs_rq(current);\n\tcurr = cfs_rq->curr;\n\tif (curr) {\n\t\tupdate_curr(cfs_rq);\n\t\tse->vruntime = curr->vruntime;\n\t}\n\tplace_entity(cfs_rq, se, 1);\n\n\tif (sysctl_sched_child_runs_first && curr && entity_before(curr, se)) {\n\t\t/*\n\t\t * Upon rescheduling, sched_class::put_prev_task() will place\n\t\t * 'current' within the tree based on its new key value.\n\t\t */\n\t\tswap(curr->vruntime, se->vruntime);\n\t\tresched_curr(rq);\n\t}\n\n\tse->vruntime -= cfs_rq->min_vruntime;\n\trq_unlock(rq, &rf);\n}\n\n/*\n * Priority of the task has changed. Check to see if we preempt\n * the current task.\n */\nstatic void\nprio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)\n{\n\tif (!task_on_rq_queued(p))\n\t\treturn;\n\n\t/*\n\t * Reschedule if we are currently running on this runqueue and\n\t * our priority decreased, or if we are not currently running on\n\t * this runqueue and our priority is higher than the current's\n\t */\n\tif (rq->curr == p) {\n\t\tif (p->prio > oldprio)\n\t\t\tresched_curr(rq);\n\t} else\n\t\tcheck_preempt_curr(rq, p, 0);\n}\n\nstatic inline bool vruntime_normalized(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases,\n\t * the dequeue_entity(.flags=0) will already have normalized the\n\t * vruntime.\n\t */\n\tif (p->on_rq)\n\t\treturn true;\n\n\t/*\n\t * When !on_rq, vruntime of the task has usually NOT been normalized.\n\t * But there are some cases where it has already been normalized:\n\t *\n\t * - A forked child which is waiting for being woken up by\n\t *   wake_up_new_task().\n\t * - A task which has been woken up by try_to_wake_up() and\n\t *   waiting for actually being woken up by sched_ttwu_pending().\n\t */\n\tif (!se->sum_exec_runtime ||\n\t    (p->state == TASK_WAKING && p->sched_remote_wakeup))\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n/*\n * Propagate the changes of the sched_entity across the tg tree to make it\n * visible to the root\n */\nstatic void propagate_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq;\n\n\t/* Start to propagate at parent */\n\tse = se->parent;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n}\n#else\nstatic void propagate_entity_cfs_rq(struct sched_entity *se) { }\n#endif\n\nstatic void detach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t/* Catch up with the cfs_rq and remove our load when we leave */\n\tupdate_load_avg(cfs_rq, se, 0);\n\tdetach_entity_load_avg(cfs_rq, se);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void attach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/*\n\t * Since the real-depth could have been changed (only FAIR\n\t * class maintain depth value), reset depth properly.\n\t */\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n#endif\n\n\t/* Synchronize entity with its cfs_rq */\n\tupdate_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);\n\tattach_entity_load_avg(cfs_rq, se, 0);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void detach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tif (!vruntime_normalized(p)) {\n\t\t/*\n\t\t * Fix up our vruntime so that the current sleep doesn't\n\t\t * cause 'unlimited' sleep bonus.\n\t\t */\n\t\tplace_entity(cfs_rq, se, 0);\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\t}\n\n\tdetach_entity_cfs_rq(se);\n}\n\nstatic void attach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tattach_entity_cfs_rq(se);\n\n\tif (!vruntime_normalized(p))\n\t\tse->vruntime += cfs_rq->min_vruntime;\n}\n\nstatic void switched_from_fair(struct rq *rq, struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n}\n\nstatic void switched_to_fair(struct rq *rq, struct task_struct *p)\n{\n\tattach_task_cfs_rq(p);\n\n\tif (task_on_rq_queued(p)) {\n\t\t/*\n\t\t * We were most likely switched from sched_rt, so\n\t\t * kick off the schedule if running, otherwise just see\n\t\t * if we can still preempt the current task.\n\t\t */\n\t\tif (rq->curr == p)\n\t\t\tresched_curr(rq);\n\t\telse\n\t\t\tcheck_preempt_curr(rq, p, 0);\n\t}\n}\n\n/* Account for a task changing its policy or group.\n *\n * This routine is mostly called to set cfs_rq->curr field when a task\n * migrates between groups/classes.\n */\nstatic void set_curr_task_fair(struct rq *rq)\n{\n\tstruct sched_entity *se = &rq->curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t\tset_next_entity(cfs_rq, se);\n\t\t/* ensure bandwidth has been allocated on our new cfs_rq */\n\t\taccount_cfs_rq_runtime(cfs_rq, 0);\n\t}\n}\n\nvoid init_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT_CACHED;\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n#ifndef CONFIG_64BIT\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n#ifdef CONFIG_SMP\n\traw_spin_lock_init(&cfs_rq->removed.lock);\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void task_set_group_fair(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tset_task_rq(p, task_cpu(p));\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n}\n\nstatic void task_move_group_fair(struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n\tset_task_rq(p, task_cpu(p));\n\n#ifdef CONFIG_SMP\n\t/* Tell se's cfs_rq has been changed -- migrated */\n\tp->se.avg.last_update_time = 0;\n#endif\n\tattach_task_cfs_rq(p);\n}\n\nstatic void task_change_group_fair(struct task_struct *p, int type)\n{\n\tswitch (type) {\n\tcase TASK_SET_GROUP:\n\t\ttask_set_group_fair(p);\n\t\tbreak;\n\n\tcase TASK_MOVE_GROUP:\n\t\ttask_move_group_fair(p);\n\t\tbreak;\n\t}\n}\n\nvoid free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct sched_entity *se;\n\tstruct cfs_rq *cfs_rq;\n\tint i;\n\n\ttg->cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tinit_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_cfs_rq(cfs_rq);\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);\n\t\tinit_entity_runnable_average(se);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nvoid online_fair_sched_group(struct task_group *tg)\n{\n\tstruct sched_entity *se;\n\tstruct rq *rq;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\t\tse = tg->se[i];\n\n\t\traw_spin_lock_irq(&rq->lock);\n\t\tupdate_rq_clock(rq);\n\t\tattach_entity_cfs_rq(se);\n\t\tsync_throttle(tg, i);\n\t\traw_spin_unlock_irq(&rq->lock);\n\t}\n}\n\nvoid unregister_fair_sched_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (tg->se[cpu])\n\t\t\tremove_entity_load_avg(tg->se[cpu]);\n\n\t\t/*\n\t\t * Only empty task groups can be destroyed; so we can speculatively\n\t\t * check on_list without danger of it being re-added.\n\t\t */\n\t\tif (!tg->cfs_rq[cpu]->on_list)\n\t\t\tcontinue;\n\n\t\trq = cpu_rq(cpu);\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tlist_del_leaf_cfs_rq(tg->cfs_rq[cpu]);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n}\n\nvoid init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\tstruct sched_entity *se, int cpu,\n\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tcfs_rq->tg = tg;\n\tcfs_rq->rq = rq;\n\tinit_cfs_rq_runtime(cfs_rq);\n\n\ttg->cfs_rq[cpu] = cfs_rq;\n\ttg->se[cpu] = se;\n\n\t/* se could be NULL for root_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent) {\n\t\tse->cfs_rq = &rq->cfs;\n\t\tse->depth = 0;\n\t} else {\n\t\tse->cfs_rq = parent->my_q;\n\t\tse->depth = parent->depth + 1;\n\t}\n\n\tse->my_q = cfs_rq;\n\t/* guarantee group entities always have weight */\n\tupdate_load_set(&se->load, NICE_0_LOAD);\n\tse->parent = parent;\n}\n\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tshares = clamp(shares, scale_load(MIN_SHARES), scale_load(MAX_SHARES));\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se = tg->se[i];\n\t\tstruct rq_flags rf;\n\n\t\t/* Propagate contribution to hierarchy */\n\t\trq_lock_irqsave(rq, &rf);\n\t\tupdate_rq_clock(rq);\n\t\tfor_each_sched_entity(se) {\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, UPDATE_TG);\n\t\t\tupdate_cfs_group(se);\n\t\t}\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n#else /* CONFIG_FAIR_GROUP_SCHED */\n\nvoid free_fair_sched_group(struct task_group *tg) { }\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nvoid online_fair_sched_group(struct task_group *tg) { }\n\nvoid unregister_fair_sched_group(struct task_group *tg) { }\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\nstatic unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task)\n{\n\tstruct sched_entity *se = &task->se;\n\tunsigned int rr_interval = 0;\n\n\t/*\n\t * Time slice is 0 for SCHED_OTHER tasks that are on an otherwise\n\t * idle runqueue:\n\t */\n\tif (rq->cfs.load.weight)\n\t\trr_interval = NS_TO_JIFFIES(sched_slice(cfs_rq_of(se), se));\n\n\treturn rr_interval;\n}\n\n/*\n * All the scheduling class methods:\n */\nconst struct sched_class fair_sched_class = {\n\t.next\t\t\t= &idle_sched_class,\n\t.enqueue_task\t\t= enqueue_task_fair,\n\t.dequeue_task\t\t= dequeue_task_fair,\n\t.yield_task\t\t= yield_task_fair,\n\t.yield_to_task\t\t= yield_to_task_fair,\n\n\t.check_preempt_curr\t= check_preempt_wakeup,\n\n\t.pick_next_task\t\t= pick_next_task_fair,\n\t.put_prev_task\t\t= put_prev_task_fair,\n\n#ifdef CONFIG_SMP\n\t.select_task_rq\t\t= select_task_rq_fair,\n\t.migrate_task_rq\t= migrate_task_rq_fair,\n\n\t.rq_online\t\t= rq_online_fair,\n\t.rq_offline\t\t= rq_offline_fair,\n\n\t.task_dead\t\t= task_dead_fair,\n\t.set_cpus_allowed\t= set_cpus_allowed_common,\n#endif\n\n\t.set_curr_task          = set_curr_task_fair,\n\t.task_tick\t\t= task_tick_fair,\n\t.task_fork\t\t= task_fork_fair,\n\n\t.prio_changed\t\t= prio_changed_fair,\n\t.switched_from\t\t= switched_from_fair,\n\t.switched_to\t\t= switched_to_fair,\n\n\t.get_rr_interval\t= get_rr_interval_fair,\n\n\t.update_curr\t\t= update_curr_fair,\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t.task_change_group\t= task_change_group_fair,\n#endif\n};\n\n#ifdef CONFIG_SCHED_DEBUG\nvoid print_cfs_stats(struct seq_file *m, int cpu)\n{\n\tstruct cfs_rq *cfs_rq, *pos;\n\n\trcu_read_lock();\n\tfor_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos)\n\t\tprint_cfs_rq(m, cpu, cfs_rq);\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nvoid show_numa_stats(struct task_struct *p, struct seq_file *m)\n{\n\tint node;\n\tunsigned long tsf = 0, tpf = 0, gsf = 0, gpf = 0;\n\n\tfor_each_online_node(node) {\n\t\tif (p->numa_faults) {\n\t\t\ttsf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t\t\ttpf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tif (p->numa_group) {\n\t\t\tgsf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 0)],\n\t\t\tgpf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tprint_numa_stats(m, node, tsf, tpf, gsf, gpf);\n\t}\n}\n#endif /* CONFIG_NUMA_BALANCING */\n#endif /* CONFIG_SCHED_DEBUG */\n\n__init void init_sched_fair_class(void)\n{\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tnohz.next_balance = jiffies;\n\tnohz.next_blocked = jiffies;\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n#endif\n#endif /* SMP */\n\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH)\n *\n *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>\n *\n *  Interactivity improvements by Mike Galbraith\n *  (C) 2007 Mike Galbraith <efault@gmx.de>\n *\n *  Various enhancements by Dmitry Adamushko.\n *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com>\n *\n *  Group scheduling enhancements by Srivatsa Vaddagiri\n *  Copyright IBM Corporation, 2007\n *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>\n *\n *  Scaled math optimizations by Thomas Gleixner\n *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de>\n *\n *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra\n *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra\n */\n#include \"sched.h\"\n\n#include <trace/events/sched.h>\n\n/*\n * Targeted preemption latency for CPU-bound tasks:\n *\n * NOTE: this latency value is not the same as the concept of\n * 'timeslice length' - timeslices in CFS are of variable length\n * and have no persistent notion like in traditional, time-slice\n * based scheduling concepts.\n *\n * (to see the precise effective timeslice length of your workload,\n *  run vmstat and monitor the context-switches (cs) field)\n *\n * (default: 6ms * (1 + ilog(ncpus)), units: nanoseconds)\n */\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nstatic unsigned int normalized_sysctl_sched_latency\t= 6000000ULL;\n\n/*\n * The initial- and re-scaling of tunables is configurable\n *\n * Options are:\n *\n *   SCHED_TUNABLESCALING_NONE - unscaled, always *1\n *   SCHED_TUNABLESCALING_LOG - scaled logarithmical, *1+ilog(ncpus)\n *   SCHED_TUNABLESCALING_LINEAR - scaled linear, *ncpus\n *\n * (default SCHED_TUNABLESCALING_LOG = *(1+ilog(ncpus))\n */\nenum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;\n\n/*\n * Minimal preemption granularity for CPU-bound tasks:\n *\n * (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds)\n */\nunsigned int sysctl_sched_min_granularity\t\t\t= 750000ULL;\nstatic unsigned int normalized_sysctl_sched_min_granularity\t= 750000ULL;\n\n/*\n * This value is kept at sysctl_sched_latency/sysctl_sched_min_granularity\n */\nstatic unsigned int sched_nr_latency = 8;\n\n/*\n * After fork, child runs first. If set to 0 (default) then\n * parent will (try to) run first.\n */\nunsigned int sysctl_sched_child_runs_first __read_mostly;\n\n/*\n * SCHED_OTHER wake-up granularity.\n *\n * This option delays the preemption effects of decoupled workloads\n * and reduces their over-scheduling. Synchronous workloads will still\n * have immediate wakeup/sleep latencies.\n *\n * (default: 1 msec * (1 + ilog(ncpus)), units: nanoseconds)\n */\nunsigned int sysctl_sched_wakeup_granularity\t\t\t= 1000000UL;\nstatic unsigned int normalized_sysctl_sched_wakeup_granularity\t= 1000000UL;\n\nconst_debug unsigned int sysctl_sched_migration_cost\t= 500000UL;\n\n#ifdef CONFIG_SMP\n/*\n * For asym packing, by default the lower numbered CPU has higher priority.\n */\nint __weak arch_asym_cpu_priority(int cpu)\n{\n\treturn -cpu;\n}\n\n/*\n * The margin used when comparing utilization with CPU capacity:\n * util * margin < capacity * 1024\n *\n * (default: ~20%)\n */\nstatic unsigned int capacity_margin\t\t\t= 1280;\n#endif\n\n#ifdef CONFIG_CFS_BANDWIDTH\n/*\n * Amount of runtime to allocate from global (tg) to local (per-cfs_rq) pool\n * each time a cfs_rq requests quota.\n *\n * Note: in the case that the slice exceeds the runtime remaining (either due\n * to consumption or the quota being specified to be smaller than the slice)\n * we will always only issue the remaining available time.\n *\n * (default: 5 msec, units: microseconds)\n */\nunsigned int sysctl_sched_cfs_bandwidth_slice\t\t= 5000UL;\n#endif\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}\n\n/*\n * Increase the granularity value when there are more CPUs,\n * because with more CPUs the 'effective latency' as visible\n * to users decreases. But the relationship is not linear,\n * so pick a second-best guess by going with the log2 of the\n * number of CPUs.\n *\n * This idea comes from the SD scheduler of Con Kolivas:\n */\nstatic unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}\n\nvoid sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}\n\n#define WMULT_CONST\t(~0U)\n#define WMULT_SHIFT\t32\n\nstatic void __update_inv_weight(struct load_weight *lw)\n{\n\tunsigned long w;\n\n\tif (likely(lw->inv_weight))\n\t\treturn;\n\n\tw = scale_load_down(lw->weight);\n\n\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\tlw->inv_weight = 1;\n\telse if (unlikely(!w))\n\t\tlw->inv_weight = WMULT_CONST;\n\telse\n\t\tlw->inv_weight = WMULT_CONST / w;\n}\n\n/*\n * delta_exec * weight / lw.weight\n *   OR\n * (delta_exec * (weight * lw->inv_weight)) >> WMULT_SHIFT\n *\n * Either weight := NICE_0_LOAD and lw \\e sched_prio_to_wmult[], in which case\n * we're guaranteed shift stays positive because inv_weight is guaranteed to\n * fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift >= 22.\n *\n * Or, weight =< lw.weight (because lw.weight is the runqueue weight), thus\n * weight/lw.weight <= 1, and therefore our shift will also be positive.\n */\nstatic u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tint shift = WMULT_SHIFT;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact >> 32)) {\n\t\twhile (fact >> 32) {\n\t\t\tfact >>= 1;\n\t\t\tshift--;\n\t\t}\n\t}\n\n\t/* hint to use a 32x32->64 mul */\n\tfact = (u64)(u32)fact * lw->inv_weight;\n\n\twhile (fact >> 32) {\n\t\tfact >>= 1;\n\t\tshift--;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}\n\n\nconst struct sched_class fair_sched_class;\n\n/**************************************************************\n * CFS operations on generic schedulable entities:\n */\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n/* cpu runqueue to which this cfs_rq is attached */\nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->rq;\n}\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\tSCHED_WARN_ON(!entity_is_task(se));\n\treturn container_of(se, struct task_struct, se);\n}\n\n/* Walk up scheduling entities hierarchy */\n#define for_each_sched_entity(se) \\\n\t\tfor (; se; se = se->parent)\n\nstatic inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn p->se.cfs_rq;\n}\n\n/* runqueue on which this entity is (to be) queued */\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\treturn se->cfs_rq;\n}\n\n/* runqueue \"owned\" by this group */\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn grp->my_q;\n}\n\nstatic inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_rq->on_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tint cpu = cpu_of(rq);\n\t\t/*\n\t\t * Ensure we either appear before our parent (if already\n\t\t * enqueued) or force our parent to appear after us when it is\n\t\t * enqueued. The fact that we always enqueue bottom-up\n\t\t * reduces this to two cases and a special case for the root\n\t\t * cfs_rq. Furthermore, it also means that we will always reset\n\t\t * tmp_alone_branch either when the branch is connected\n\t\t * to a tree or when we reach the beg of the tree\n\t\t */\n\t\tif (cfs_rq->tg->parent &&\n\t\t    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {\n\t\t\t/*\n\t\t\t * If parent is already on the list, we add the child\n\t\t\t * just before. Thanks to circular linked property of\n\t\t\t * the list, this means to put the child at the tail\n\t\t\t * of the list that starts by parent.\n\t\t\t */\n\t\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\t&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));\n\t\t\t/*\n\t\t\t * The branch is now connected to its tree so we can\n\t\t\t * reset tmp_alone_branch to the beginning of the\n\t\t\t * list.\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t} else if (!cfs_rq->tg->parent) {\n\t\t\t/*\n\t\t\t * cfs rq without parent should be put\n\t\t\t * at the tail of the list.\n\t\t\t */\n\t\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\t&rq->leaf_cfs_rq_list);\n\t\t\t/*\n\t\t\t * We have reach the beg of a tree so we can reset\n\t\t\t * tmp_alone_branch to the beginning of the list.\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t} else {\n\t\t\t/*\n\t\t\t * The parent has not already been added so we want to\n\t\t\t * make sure that it will be put after us.\n\t\t\t * tmp_alone_branch points to the beg of the branch\n\t\t\t * where we will add parent.\n\t\t\t */\n\t\t\tlist_add_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\trq->tmp_alone_branch);\n\t\t\t/*\n\t\t\t * update tmp_alone_branch to points to the new beg\n\t\t\t * of the branch\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;\n\t\t}\n\n\t\tcfs_rq->on_list = 1;\n\t}\n}\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->on_list) {\n\t\tlist_del_rcu(&cfs_rq->leaf_cfs_rq_list);\n\t\tcfs_rq->on_list = 0;\n\t}\n}\n\n/* Iterate through all leaf cfs_rq's on a runqueue: */\n#define for_each_leaf_cfs_rq(rq, cfs_rq) \\\n\tlist_for_each_entry_rcu(cfs_rq, &rq->leaf_cfs_rq_list, leaf_cfs_rq_list)\n\n/* Do the two (enqueued) entities belong to the same group ? */\nstatic inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn se->parent;\n}\n\nstatic void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n\tint se_depth, pse_depth;\n\n\t/*\n\t * preemption test can be made between sibling entities who are in the\n\t * same cfs_rq i.e who have a common parent. Walk up the hierarchy of\n\t * both tasks until we find their ancestors who are siblings of common\n\t * parent.\n\t */\n\n\t/* First walk up until both entities are at same depth */\n\tse_depth = (*se)->depth;\n\tpse_depth = (*pse)->depth;\n\n\twhile (se_depth > pse_depth) {\n\t\tse_depth--;\n\t\t*se = parent_entity(*se);\n\t}\n\n\twhile (pse_depth > se_depth) {\n\t\tpse_depth--;\n\t\t*pse = parent_entity(*pse);\n\t}\n\n\twhile (!is_same_group(*se, *pse)) {\n\t\t*se = parent_entity(*se);\n\t\t*pse = parent_entity(*pse);\n\t}\n}\n\n#else\t/* !CONFIG_FAIR_GROUP_SCHED */\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}\n\nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn container_of(cfs_rq, struct rq, cfs);\n}\n\n\n#define for_each_sched_entity(se) \\\n\t\tfor (; se; se = NULL)\n\nstatic inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn &task_rq(p)->cfs;\n}\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}\n\n/* runqueue \"owned\" by this group */\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}\n\nstatic inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}\n\n#define for_each_leaf_cfs_rq(rq, cfs_rq)\t\\\n\t\tfor (cfs_rq = &rq->cfs; cfs_rq; cfs_rq = NULL)\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}\n\nstatic inline void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n}\n\n#endif\t/* CONFIG_FAIR_GROUP_SCHED */\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\n\n/**************************************************************\n * Scheduling class tree data structure manipulation methods:\n */\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}\n\nstatic inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}\n\nstatic void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (leftmost) { /* non-empty tree */\n\t\tstruct sched_entity *se;\n\t\tse = rb_entry(leftmost, struct sched_entity, run_node);\n\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tcfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n}\n\n/*\n * Enqueue an entity into the rb-tree:\n */\nstatic void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sched_entity *entry;\n\tbool leftmost = true;\n\n\t/*\n\t * Find the right place in the rbtree:\n\t */\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct sched_entity, run_node);\n\t\t/*\n\t\t * We dont care about collisions. Nodes with\n\t\t * the same key stay together.\n\t\t */\n\t\tif (entity_before(se, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&se->run_node, parent, link);\n\trb_insert_color_cached(&se->run_node,\n\t\t\t       &cfs_rq->tasks_timeline, leftmost);\n}\n\nstatic void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);\n}\n\nstruct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn rb_entry(left, struct sched_entity, run_node);\n}\n\nstatic struct sched_entity *__pick_next_entity(struct sched_entity *se)\n{\n\tstruct rb_node *next = rb_next(&se->run_node);\n\n\tif (!next)\n\t\treturn NULL;\n\n\treturn rb_entry(next, struct sched_entity, run_node);\n}\n\n#ifdef CONFIG_SCHED_DEBUG\nstruct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *last = rb_last(&cfs_rq->tasks_timeline.rb_root);\n\n\tif (!last)\n\t\treturn NULL;\n\n\treturn rb_entry(last, struct sched_entity, run_node);\n}\n\n/**************************************************************\n * Scheduling class statistics methods:\n */\n\nint sched_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tunsigned int factor = get_update_sysctl_factor();\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tsched_nr_latency = DIV_ROUND_UP(sysctl_sched_latency,\n\t\t\t\t\tsysctl_sched_min_granularity);\n\n#define WRT_SYSCTL(name) \\\n\t(normalized_sysctl_##name = sysctl_##name / (factor))\n\tWRT_SYSCTL(sched_min_granularity);\n\tWRT_SYSCTL(sched_latency);\n\tWRT_SYSCTL(sched_wakeup_granularity);\n#undef WRT_SYSCTL\n\n\treturn 0;\n}\n#endif\n\n/*\n * delta /= w\n */\nstatic inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}\n\n/*\n * The idea is to set a period in which each task runs once.\n *\n * When there are too many tasks (sched_nr_latency) we have to stretch\n * this period because otherwise the slices get too small.\n *\n * p = (nr <= nl) ? l : l*nr/nl\n */\nstatic u64 __sched_period(unsigned long nr_running)\n{\n\tif (unlikely(nr_running > sched_nr_latency))\n\t\treturn nr_running * sysctl_sched_min_granularity;\n\telse\n\t\treturn sysctl_sched_latency;\n}\n\n/*\n * We calculate the wall-time slice from the period by taking a part\n * proportional to the weight.\n *\n * s = p*P[w/rw]\n */\nstatic u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}\n\n/*\n * We calculate the vruntime slice of a to-be-inserted task.\n *\n * vs = s/w\n */\nstatic u64 sched_vslice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\treturn calc_delta_fair(sched_slice(cfs_rq, se), se);\n}\n\n#ifdef CONFIG_SMP\n#include \"pelt.h\"\n#include \"sched-pelt.h\"\n\nstatic int select_idle_sibling(struct task_struct *p, int prev_cpu, int cpu);\nstatic unsigned long task_h_load(struct task_struct *p);\nstatic unsigned long capacity_of(int cpu);\n\n/* Give new sched_entity start runnable values to heavy its load in infant time */\nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n\tstruct sched_avg *sa = &se->avg;\n\n\tmemset(sa, 0, sizeof(*sa));\n\n\t/*\n\t * Tasks are initialized with full load to be seen as heavy tasks until\n\t * they get a chance to stabilize to their real load level.\n\t * Group entities are initialized with zero load to reflect the fact that\n\t * nothing has been attached to the task group yet.\n\t */\n\tif (entity_is_task(se))\n\t\tsa->runnable_load_avg = sa->load_avg = scale_load_down(se->load.weight);\n\n\tse->runnable_weight = se->load.weight;\n\n\t/* when this task enqueue'ed, it will contribute to its cfs_rq's load_avg */\n}\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq);\nstatic void attach_entity_cfs_rq(struct sched_entity *se);\n\n/*\n * With new tasks being created, their initial util_avgs are extrapolated\n * based on the cfs_rq's current util_avg:\n *\n *   util_avg = cfs_rq->util_avg / (cfs_rq->load_avg + 1) * se.load.weight\n *\n * However, in many cases, the above util_avg does not give a desired\n * value. Moreover, the sum of the util_avgs may be divergent, such\n * as when the series is a harmonic series.\n *\n * To solve this problem, we also cap the util_avg of successive tasks to\n * only 1/2 of the left utilization budget:\n *\n *   util_avg_cap = (cpu_scale - cfs_rq->avg.util_avg) / 2^n\n *\n * where n denotes the nth task and cpu_scale the CPU capacity.\n *\n * For example, for a CPU with 1024 of capacity, a simplest series from\n * the beginning would be like:\n *\n *  task  util_avg: 512, 256, 128,  64,  32,   16,    8, ...\n * cfs_rq util_avg: 512, 768, 896, 960, 992, 1008, 1016, ...\n *\n * Finally, that extrapolated util_avg is clamped to the cap (util_avg_cap)\n * if util_avg > util_avg_cap.\n */\nvoid post_init_entity_util_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct sched_avg *sa = &se->avg;\n\tlong cpu_scale = arch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\tlong cap = (long)(cpu_scale - cfs_rq->avg.util_avg) / 2;\n\n\tif (cap > 0) {\n\t\tif (cfs_rq->avg.util_avg != 0) {\n\t\t\tsa->util_avg  = cfs_rq->avg.util_avg * se->load.weight;\n\t\t\tsa->util_avg /= (cfs_rq->avg.load_avg + 1);\n\n\t\t\tif (sa->util_avg > cap)\n\t\t\t\tsa->util_avg = cap;\n\t\t} else {\n\t\t\tsa->util_avg = cap;\n\t\t}\n\t}\n\n\tif (entity_is_task(se)) {\n\t\tstruct task_struct *p = task_of(se);\n\t\tif (p->sched_class != &fair_sched_class) {\n\t\t\t/*\n\t\t\t * For !fair tasks do:\n\t\t\t *\n\t\t\tupdate_cfs_rq_load_avg(now, cfs_rq);\n\t\t\tattach_entity_load_avg(cfs_rq, se, 0);\n\t\t\tswitched_from_fair(rq, p);\n\t\t\t *\n\t\t\t * such that the next switched_to_fair() has the\n\t\t\t * expected state.\n\t\t\t */\n\t\t\tse->avg.last_update_time = cfs_rq_clock_task(cfs_rq);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tattach_entity_cfs_rq(se);\n}\n\n#else /* !CONFIG_SMP */\nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n}\nvoid post_init_entity_util_avg(struct sched_entity *se)\n{\n}\nstatic void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)\n{\n}\n#endif /* CONFIG_SMP */\n\n/*\n * Update the current task's runtime statistics.\n */\nstatic void update_curr(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tu64 now = rq_clock_task(rq_of(cfs_rq));\n\tu64 delta_exec;\n\n\tif (unlikely(!curr))\n\t\treturn;\n\n\tdelta_exec = now - curr->exec_start;\n\tif (unlikely((s64)delta_exec <= 0))\n\t\treturn;\n\n\tcurr->exec_start = now;\n\n\tschedstat_set(curr->statistics.exec_max,\n\t\t      max(delta_exec, curr->statistics.exec_max));\n\n\tcurr->sum_exec_runtime += delta_exec;\n\tschedstat_add(cfs_rq->exec_clock, delta_exec);\n\n\tcurr->vruntime += calc_delta_fair(delta_exec, curr);\n\tupdate_min_vruntime(cfs_rq);\n\n\tif (entity_is_task(curr)) {\n\t\tstruct task_struct *curtask = task_of(curr);\n\n\t\ttrace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);\n\t\tcgroup_account_cputime(curtask, delta_exec);\n\t\taccount_group_exec_runtime(curtask, delta_exec);\n\t}\n\n\taccount_cfs_rq_runtime(cfs_rq, delta_exec);\n}\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}\n\nstatic inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 wait_start, prev_wait_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\twait_start = rq_clock(rq_of(cfs_rq));\n\tprev_wait_start = schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)) &&\n\t    likely(wait_start > prev_wait_start))\n\t\twait_start -= prev_wait_start;\n\n\t__schedstat_set(se->statistics.wait_start, wait_start);\n}\n\nstatic inline void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *p;\n\tu64 delta;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se)) {\n\t\tp = task_of(se);\n\t\tif (task_on_rq_migrating(p)) {\n\t\t\t/*\n\t\t\t * Preserve migrating task's wait time so wait_start\n\t\t\t * time stamp can be adjusted to accumulate wait time\n\t\t\t * prior to migration.\n\t\t\t */\n\t\t\t__schedstat_set(se->statistics.wait_start, delta);\n\t\t\treturn;\n\t\t}\n\t\ttrace_sched_stat_wait(p, delta);\n\t}\n\n\t__schedstat_set(se->statistics.wait_max,\n\t\t      max(schedstat_val(se->statistics.wait_max), delta));\n\t__schedstat_inc(se->statistics.wait_count);\n\t__schedstat_add(se->statistics.wait_sum, delta);\n\t__schedstat_set(se->statistics.wait_start, 0);\n}\n\nstatic inline void\nupdate_stats_enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *tsk = NULL;\n\tu64 sleep_start, block_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tsleep_start = schedstat_val(se->statistics.sleep_start);\n\tblock_start = schedstat_val(se->statistics.block_start);\n\n\tif (entity_is_task(se))\n\t\ttsk = task_of(se);\n\n\tif (sleep_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - sleep_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.sleep_max)))\n\t\t\t__schedstat_set(se->statistics.sleep_max, delta);\n\n\t\t__schedstat_set(se->statistics.sleep_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 1);\n\t\t\ttrace_sched_stat_sleep(tsk, delta);\n\t\t}\n\t}\n\tif (block_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - block_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.block_max)))\n\t\t\t__schedstat_set(se->statistics.block_max, delta);\n\n\t\t__schedstat_set(se->statistics.block_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\tif (tsk->in_iowait) {\n\t\t\t\t__schedstat_add(se->statistics.iowait_sum, delta);\n\t\t\t\t__schedstat_inc(se->statistics.iowait_count);\n\t\t\t\ttrace_sched_stat_iowait(tsk, delta);\n\t\t\t}\n\n\t\t\ttrace_sched_stat_blocked(tsk, delta);\n\n\t\t\t/*\n\t\t\t * Blocking time is in units of nanosecs, so shift by\n\t\t\t * 20 to get a milliseconds-range estimation of the\n\t\t\t * amount of time that the task spent sleeping:\n\t\t\t */\n\t\t\tif (unlikely(prof_on == SLEEP_PROFILING)) {\n\t\t\t\tprofile_hits(SLEEP_PROFILING,\n\t\t\t\t\t\t(void *)get_wchan(tsk),\n\t\t\t\t\t\tdelta >> 20);\n\t\t\t}\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 0);\n\t\t}\n\t}\n}\n\n/*\n * Task is being enqueued - update stats:\n */\nstatic inline void\nupdate_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Are we enqueueing a waiting task? (for current tasks\n\t * a dequeue/enqueue event is a NOP)\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_start(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tupdate_stats_enqueue_sleeper(cfs_rq, se);\n}\n\nstatic inline void\nupdate_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Mark the end of the wait period if dequeueing a\n\t * waiting task:\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\n\tif ((flags & DEQUEUE_SLEEP) && entity_is_task(se)) {\n\t\tstruct task_struct *tsk = task_of(se);\n\n\t\tif (tsk->state & TASK_INTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.sleep_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t\tif (tsk->state & TASK_UNINTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.block_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t}\n}\n\n/*\n * We are picking a new current task - update its stats:\n */\nstatic inline void\nupdate_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/*\n\t * We are starting a new run period:\n\t */\n\tse->exec_start = rq_clock_task(rq_of(cfs_rq));\n}\n\n/**************************************************\n * Scheduling class queueing methods:\n */\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Approximate time to scan a full NUMA task in ms. The task scan period is\n * calculated based on the tasks virtual memory size and\n * numa_balancing_scan_size.\n */\nunsigned int sysctl_numa_balancing_scan_period_min = 1000;\nunsigned int sysctl_numa_balancing_scan_period_max = 60000;\n\n/* Portion of address space to scan in MB */\nunsigned int sysctl_numa_balancing_scan_size = 256;\n\n/* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */\nunsigned int sysctl_numa_balancing_scan_delay = 1000;\n\nstruct numa_group {\n\tatomic_t refcount;\n\n\tspinlock_t lock; /* nr_tasks, tasks */\n\tint nr_tasks;\n\tpid_t gid;\n\tint active_nodes;\n\n\tstruct rcu_head rcu;\n\tunsigned long total_faults;\n\tunsigned long max_faults_cpu;\n\t/*\n\t * Faults_cpu is used to decide whether memory should move\n\t * towards the CPU. As a consequence, these stats are weighted\n\t * more by CPU use than by memory faults.\n\t */\n\tunsigned long *faults_cpu;\n\tunsigned long faults[0];\n};\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng);\nstatic inline unsigned long group_faults_shared(struct numa_group *ng);\n\nstatic unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t/*\n\t * Calculations based on RSS as non-present and empty pages are skipped\n\t * by the PTE scanner and NUMA hinting faults should be trapped based\n\t * on resident pages\n\t */\n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}\n\n/* For sanitys sake, never scan more PTEs than MAX_SCAN_WINDOW MB/sec. */\n#define MAX_SCAN_WINDOW 2560\n\nstatic unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}\n\nstatic unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}\n\nstatic unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}\n\nvoid init_numa_balancing(unsigned long clone_flags, struct task_struct *p)\n{\n\tint mm_users = 0;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (mm) {\n\t\tmm_users = atomic_read(&mm->mm_users);\n\t\tif (mm_users == 1) {\n\t\t\tmm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t\t\tmm->numa_scan_seq = 0;\n\t\t}\n\t}\n\tp->node_stamp\t\t\t= 0;\n\tp->numa_scan_seq\t\t= mm ? mm->numa_scan_seq : 0;\n\tp->numa_scan_period\t\t= sysctl_numa_balancing_scan_delay;\n\tp->numa_work.next\t\t= &p->numa_work;\n\tp->numa_faults\t\t\t= NULL;\n\tp->numa_group\t\t\t= NULL;\n\tp->last_task_numa_placement\t= 0;\n\tp->last_sum_exec_runtime\t= 0;\n\n\t/* New address space, reset the preferred nid */\n\tif (!(clone_flags & CLONE_VM)) {\n\t\tp->numa_preferred_nid = -1;\n\t\treturn;\n\t}\n\n\t/*\n\t * New thread, keep existing numa_preferred_nid which should be copied\n\t * already by arch_dup_task_struct but stagger when scans start.\n\t */\n\tif (mm) {\n\t\tunsigned int delay;\n\n\t\tdelay = min_t(unsigned int, task_scan_max(current),\n\t\t\tcurrent->numa_scan_period * mm_users * NSEC_PER_MSEC);\n\t\tdelay += 2 * TICK_NSEC;\n\t\tp->node_stamp = delay;\n\t}\n}\n\nstatic void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running += (p->numa_preferred_nid != -1);\n\trq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));\n}\n\nstatic void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running -= (p->numa_preferred_nid != -1);\n\trq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));\n}\n\n/* Shared or private faults. */\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\n/* Memory and CPU locality */\n#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)\n\n/* Averaged statistics, and temporary buffers. */\n#define NR_NUMA_HINT_FAULT_BUCKETS (NR_NUMA_HINT_FAULT_STATS * 2)\n\npid_t task_numa_group_id(struct task_struct *p)\n{\n\treturn p->numa_group ? p->numa_group->gid : 0;\n}\n\n/*\n * The averaged statistics, shared & private, memory & CPU,\n * occupy the first half of the array. The second half of the\n * array is for current counters, which are averaged into the\n * first set by task_numa_placement.\n */\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}\n\nstatic inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}\n\nstatic inline unsigned long group_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\treturn p->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}\n\nstatic inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}\n\n/*\n * A node triggering more than 1/3 as many NUMA faults as the maximum is\n * considered part of a numa group's pseudo-interleaving set. Migrations\n * between these nodes are slowed down, to allow things to settle down.\n */\n#define ACTIVE_NODE_FRACTION 3\n\nstatic bool numa_is_active_node(int nid, struct numa_group *ng)\n{\n\treturn group_faults_cpu(ng, nid) * ACTIVE_NODE_FRACTION > ng->max_faults_cpu;\n}\n\n/* Handle placement on systems where not all nodes are directly connected. */\nstatic unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint maxdist, bool task)\n{\n\tunsigned long score = 0;\n\tint node;\n\n\t/*\n\t * All nodes are directly connected, and the same distance\n\t * from each other. No need for fancy placement algorithms.\n\t */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t/*\n\t * This code is called for each node, introducing N^2 complexity,\n\t * which should be ok given the number of nodes rarely exceeds 8.\n\t */\n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t/*\n\t\t * The furthest away nodes in the system are not interesting\n\t\t * for placement; nid was already counted.\n\t\t */\n\t\tif (dist == sched_max_numa_distance || node == nid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * On systems with a backplane NUMA topology, compare groups\n\t\t * of nodes, and move tasks towards the group with the most\n\t\t * memory accesses. When comparing two nodes at distance\n\t\t * \"hoplimit\", only nodes closer by than \"hoplimit\" are part\n\t\t * of each group. Skip other nodes.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\tdist >= maxdist)\n\t\t\tcontinue;\n\n\t\t/* Add up the faults from nearby nodes. */\n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t/*\n\t\t * On systems with a glueless mesh NUMA topology, there are\n\t\t * no fixed \"groups of nodes\". Instead, nodes that are not\n\t\t * directly connected bounce traffic through intermediate\n\t\t * nodes; a numa_group can occupy any set of nodes.\n\t\t * The further away a node is, the less the faults count.\n\t\t * This seems to result in good task placement.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (sched_max_numa_distance - dist);\n\t\t\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}\n\n/*\n * These return the fraction of accesses done by a particular task, or\n * task group, on a particular numa node.  The group weight is given a\n * larger multiplier, in order to group tasks together that are almost\n * evenly spread out between numa nodes.\n */\nstatic inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}\n\nstatic inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}\n\nbool should_numa_migrate_memory(struct task_struct *p, struct page * page,\n\t\t\t\tint src_nid, int dst_cpu)\n{\n\tstruct numa_group *ng = p->numa_group;\n\tint dst_nid = cpu_to_node(dst_cpu);\n\tint last_cpupid, this_cpupid;\n\n\tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);\n\tlast_cpupid = page_cpupid_xchg_last(page, this_cpupid);\n\n\t/*\n\t * Allow first faults or private faults to migrate immediately early in\n\t * the lifetime of a task. The magic number 4 is based on waiting for\n\t * two full passes of the \"multi-stage node selection\" test that is\n\t * executed below.\n\t */\n\tif ((p->numa_preferred_nid == -1 || p->numa_scan_seq <= 4) &&\n\t    (cpupid_pid_unset(last_cpupid) || cpupid_match_pid(p, last_cpupid)))\n\t\treturn true;\n\n\t/*\n\t * Multi-stage node selection is used in conjunction with a periodic\n\t * migration fault to build a temporal task<->page relation. By using\n\t * a two-stage filter we remove short/unlikely relations.\n\t *\n\t * Using P(p) ~ n_p / n_t as per frequentist probability, we can equate\n\t * a task's usage of a particular page (n_p) per total usage of this\n\t * page (n_t) (in a given time-span) to a probability.\n\t *\n\t * Our periodic faults will sample this probability and getting the\n\t * same result twice in a row, given these samples are fully\n\t * independent, is then given by P(n)^2, provided our sample period\n\t * is sufficiently short compared to the usage pattern.\n\t *\n\t * This quadric squishes small probabilities, making it less likely we\n\t * act on an unlikely task<->page relation.\n\t */\n\tif (!cpupid_pid_unset(last_cpupid) &&\n\t\t\t\tcpupid_to_nid(last_cpupid) != dst_nid)\n\t\treturn false;\n\n\t/* Always allow migrate on private faults */\n\tif (cpupid_match_pid(p, last_cpupid))\n\t\treturn true;\n\n\t/* A shared fault, but p->numa_group has not been set up yet. */\n\tif (!ng)\n\t\treturn true;\n\n\t/*\n\t * Destination node is much more heavily used than the source\n\t * node? Allow migration.\n\t */\n\tif (group_faults_cpu(ng, dst_nid) > group_faults_cpu(ng, src_nid) *\n\t\t\t\t\tACTIVE_NODE_FRACTION)\n\t\treturn true;\n\n\t/*\n\t * Distribute memory according to CPU & memory use on each node,\n\t * with 3/4 hysteresis to avoid unnecessary memory migrations:\n\t *\n\t * faults_cpu(dst)   3   faults_cpu(src)\n\t * --------------- * - > ---------------\n\t * faults_mem(dst)   4   faults_mem(src)\n\t */\n\treturn group_faults_cpu(ng, dst_nid) * group_faults(p, src_nid) * 3 >\n\t       group_faults_cpu(ng, src_nid) * group_faults(p, dst_nid) * 4;\n}\n\nstatic unsigned long weighted_cpuload(struct rq *rq);\nstatic unsigned long source_load(int cpu, int type);\nstatic unsigned long target_load(int cpu, int type);\n\n/* Cached statistics for all CPUs within a node */\nstruct numa_stats {\n\tunsigned long load;\n\n\t/* Total compute capacity of CPUs on a node */\n\tunsigned long compute_capacity;\n};\n\n/*\n * XXX borrowed from update_sg_lb_stats\n */\nstatic void update_numa_stats(struct numa_stats *ns, int nid)\n{\n\tint cpu;\n\n\tmemset(ns, 0, sizeof(*ns));\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\tns->load += weighted_cpuload(rq);\n\t\tns->compute_capacity += capacity_of(cpu);\n\t}\n\n}\n\nstruct task_numa_env {\n\tstruct task_struct *p;\n\n\tint src_cpu, src_nid;\n\tint dst_cpu, dst_nid;\n\n\tstruct numa_stats src_stats, dst_stats;\n\n\tint imbalance_pct;\n\tint dist;\n\n\tstruct task_struct *best_task;\n\tlong best_imp;\n\tint best_cpu;\n};\n\nstatic void task_numa_assign(struct task_numa_env *env,\n\t\t\t     struct task_struct *p, long imp)\n{\n\tstruct rq *rq = cpu_rq(env->dst_cpu);\n\n\t/* Bail out if run-queue part of active NUMA balance. */\n\tif (xchg(&rq->numa_migrate_on, 1))\n\t\treturn;\n\n\t/*\n\t * Clear previous best_cpu/rq numa-migrate flag, since task now\n\t * found a better CPU to move/swap.\n\t */\n\tif (env->best_cpu != -1) {\n\t\trq = cpu_rq(env->best_cpu);\n\t\tWRITE_ONCE(rq->numa_migrate_on, 0);\n\t}\n\n\tif (env->best_task)\n\t\tput_task_struct(env->best_task);\n\tif (p)\n\t\tget_task_struct(p);\n\n\tenv->best_task = p;\n\tenv->best_imp = imp;\n\tenv->best_cpu = env->dst_cpu;\n}\n\nstatic bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t/*\n\t * The load is corrected for the CPU capacity available on each node.\n\t *\n\t * src_load        dst_load\n\t * ------------ vs ---------\n\t * src_capacity    dst_capacity\n\t */\n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t/* Would this change make things worse? */\n\treturn (imb > old_imb);\n}\n\n/*\n * Maximum NUMA importance can be 1998 (2*999);\n * SMALLIMP @ 30 would be close to 1998/64.\n * Used to deter task migration.\n */\n#define SMALLIMP\t30\n\n/*\n * This checks if the overall compute and NUMA accesses of the system would\n * be improved if the source tasks was migrated to the target dst_cpu taking\n * into account that it might be best if task running on the dst_cpu should\n * be exchanged with the source task\n */\nstatic void task_numa_compare(struct task_numa_env *env,\n\t\t\t      long taskimp, long groupimp, bool maymove)\n{\n\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);\n\tstruct task_struct *cur;\n\tlong src_load, dst_load;\n\tlong load;\n\tlong imp = env->p->numa_group ? groupimp : taskimp;\n\tlong moveimp = imp;\n\tint dist = env->dist;\n\n\tif (READ_ONCE(dst_rq->numa_migrate_on))\n\t\treturn;\n\n\trcu_read_lock();\n\tcur = task_rcu_dereference(&dst_rq->curr);\n\tif (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))\n\t\tcur = NULL;\n\n\t/*\n\t * Because we have preemption enabled we can get migrated around and\n\t * end try selecting ourselves (current == env->p) as a swap candidate.\n\t */\n\tif (cur == env->p)\n\t\tgoto unlock;\n\n\tif (!cur) {\n\t\tif (maymove && moveimp >= env->best_imp)\n\t\t\tgoto assign;\n\t\telse\n\t\t\tgoto unlock;\n\t}\n\n\t/*\n\t * \"imp\" is the fault differential for the source task between the\n\t * source and destination node. Calculate the total differential for\n\t * the source task and potential destination task. The more negative\n\t * the value is, the more remote accesses that would be expected to\n\t * be incurred if the tasks were swapped.\n\t */\n\t/* Skip this swap candidate if cannot move to the source cpu */\n\tif (!cpumask_test_cpu(env->src_cpu, &cur->cpus_allowed))\n\t\tgoto unlock;\n\n\t/*\n\t * If dst and source tasks are in the same NUMA group, or not\n\t * in any group then look only at task weights.\n\t */\n\tif (cur->numa_group == env->p->numa_group) {\n\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -\n\t\t      task_weight(cur, env->dst_nid, dist);\n\t\t/*\n\t\t * Add some hysteresis to prevent swapping the\n\t\t * tasks within a group over tiny differences.\n\t\t */\n\t\tif (cur->numa_group)\n\t\t\timp -= imp / 16;\n\t} else {\n\t\t/*\n\t\t * Compare the group weights. If a task is all by itself\n\t\t * (not part of a group), use the task weight instead.\n\t\t */\n\t\tif (cur->numa_group && env->p->numa_group)\n\t\t\timp += group_weight(cur, env->src_nid, dist) -\n\t\t\t       group_weight(cur, env->dst_nid, dist);\n\t\telse\n\t\t\timp += task_weight(cur, env->src_nid, dist) -\n\t\t\t       task_weight(cur, env->dst_nid, dist);\n\t}\n\n\tif (maymove && moveimp > imp && moveimp > env->best_imp) {\n\t\timp = moveimp;\n\t\tcur = NULL;\n\t\tgoto assign;\n\t}\n\n\t/*\n\t * If the NUMA importance is less than SMALLIMP,\n\t * task migration might only result in ping pong\n\t * of tasks and also hurt performance due to cache\n\t * misses.\n\t */\n\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)\n\t\tgoto unlock;\n\n\t/*\n\t * In the overloaded case, try and keep the load balanced.\n\t */\n\tload = task_h_load(env->p) - task_h_load(cur);\n\tif (!load)\n\t\tgoto assign;\n\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\tif (load_too_imbalanced(src_load, dst_load, env))\n\t\tgoto unlock;\n\nassign:\n\t/*\n\t * One idle CPU per node is evaluated for a task numa move.\n\t * Call select_idle_sibling to maybe find a better one.\n\t */\n\tif (!cur) {\n\t\t/*\n\t\t * select_idle_siblings() uses an per-CPU cpumask that\n\t\t * can be used from IRQ context.\n\t\t */\n\t\tlocal_irq_disable();\n\t\tenv->dst_cpu = select_idle_sibling(env->p, env->src_cpu,\n\t\t\t\t\t\t   env->dst_cpu);\n\t\tlocal_irq_enable();\n\t}\n\n\ttask_numa_assign(env, cur, imp);\nunlock:\n\trcu_read_unlock();\n}\n\nstatic void task_numa_find_cpu(struct task_numa_env *env,\n\t\t\t\tlong taskimp, long groupimp)\n{\n\tlong src_load, dst_load, load;\n\tbool maymove = false;\n\tint cpu;\n\n\tload = task_h_load(env->p);\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\t/*\n\t * If the improvement from just moving env->p direction is better\n\t * than swapping tasks around, check if a move is possible.\n\t */\n\tmaymove = !load_too_imbalanced(src_load, dst_load, env);\n\n\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {\n\t\t/* Skip this CPU if the source task cannot migrate */\n\t\tif (!cpumask_test_cpu(cpu, &env->p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tenv->dst_cpu = cpu;\n\t\ttask_numa_compare(env, taskimp, groupimp, maymove);\n\t}\n}\n\nstatic int task_numa_migrate(struct task_struct *p)\n{\n\tstruct task_numa_env env = {\n\t\t.p = p,\n\n\t\t.src_cpu = task_cpu(p),\n\t\t.src_nid = task_node(p),\n\n\t\t.imbalance_pct = 112,\n\n\t\t.best_task = NULL,\n\t\t.best_imp = 0,\n\t\t.best_cpu = -1,\n\t};\n\tstruct sched_domain *sd;\n\tstruct rq *best_rq;\n\tunsigned long taskweight, groupweight;\n\tint nid, ret, dist;\n\tlong taskimp, groupimp;\n\n\t/*\n\t * Pick the lowest SD_NUMA domain, as that would have the smallest\n\t * imbalance and would be the first to start moving tasks about.\n\t *\n\t * And we want to avoid any moving of tasks about, as that would create\n\t * random movement of tasks -- counter the numa conditions we're trying\n\t * to satisfy here.\n\t */\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));\n\tif (sd)\n\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;\n\trcu_read_unlock();\n\n\t/*\n\t * Cpusets can break the scheduler domain tree into smaller\n\t * balance domains, some of which do not cross NUMA boundaries.\n\t * Tasks that are \"trapped\" in such domains cannot be migrated\n\t * elsewhere, so there is no point in (re)trying.\n\t */\n\tif (unlikely(!sd)) {\n\t\tsched_setnuma(p, task_node(p));\n\t\treturn -EINVAL;\n\t}\n\n\tenv.dst_nid = p->numa_preferred_nid;\n\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);\n\ttaskweight = task_weight(p, env.src_nid, dist);\n\tgroupweight = group_weight(p, env.src_nid, dist);\n\tupdate_numa_stats(&env.src_stats, env.src_nid);\n\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;\n\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;\n\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\n\t/* Try to find a spot on the preferred nid. */\n\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\n\t/*\n\t * Look at other nodes in these cases:\n\t * - there is no space available on the preferred_nid\n\t * - the task is part of a numa_group that is interleaved across\n\t *   multiple NUMA nodes; in order to better consolidate the group,\n\t *   we need to check other locations.\n\t */\n\tif (env.best_cpu == -1 || (p->numa_group && p->numa_group->active_nodes > 1)) {\n\t\tfor_each_online_node(nid) {\n\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)\n\t\t\t\tcontinue;\n\n\t\t\tdist = node_distance(env.src_nid, env.dst_nid);\n\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\t\tdist != env.dist) {\n\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);\n\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);\n\t\t\t}\n\n\t\t\t/* Only consider nodes where both task and groups benefit */\n\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;\n\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;\n\t\t\tif (taskimp < 0 && groupimp < 0)\n\t\t\t\tcontinue;\n\n\t\t\tenv.dist = dist;\n\t\t\tenv.dst_nid = nid;\n\t\t\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\t\t}\n\t}\n\n\t/*\n\t * If the task is part of a workload that spans multiple NUMA nodes,\n\t * and is migrating into one of the workload's active nodes, remember\n\t * this node as the task's preferred numa node, so the workload can\n\t * settle down.\n\t * A task that migrated to a second choice node will be better off\n\t * trying for a better one later. Do not set the preferred node here.\n\t */\n\tif (p->numa_group) {\n\t\tif (env.best_cpu == -1)\n\t\t\tnid = env.src_nid;\n\t\telse\n\t\t\tnid = cpu_to_node(env.best_cpu);\n\n\t\tif (nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, nid);\n\t}\n\n\t/* No better CPU than the current one was found. */\n\tif (env.best_cpu == -1)\n\t\treturn -EAGAIN;\n\n\tbest_rq = cpu_rq(env.best_cpu);\n\tif (env.best_task == NULL) {\n\t\tret = migrate_task_to(p, env.best_cpu);\n\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\t\tif (ret != 0)\n\t\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_cpu);\n\t\treturn ret;\n\t}\n\n\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);\n\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\n\tif (ret != 0)\n\t\ttrace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));\n\tput_task_struct(env.best_task);\n\treturn ret;\n}\n\n/* Attempt to migrate a task to a CPU on the preferred node. */\nstatic void numa_migrate_preferred(struct task_struct *p)\n{\n\tunsigned long interval = HZ;\n\n\t/* This task has no NUMA fault statistics yet */\n\tif (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults))\n\t\treturn;\n\n\t/* Periodically retry migrating the task to the preferred node */\n\tinterval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);\n\tp->numa_migrate_retry = jiffies + interval;\n\n\t/* Success if task is already running on preferred CPU */\n\tif (task_node(p) == p->numa_preferred_nid)\n\t\treturn;\n\n\t/* Otherwise, try migrate to a CPU on the preferred node */\n\ttask_numa_migrate(p);\n}\n\n/*\n * Find out how many nodes on the workload is actively running on. Do this by\n * tracking the nodes from which NUMA hinting faults are triggered. This can\n * be different from the set of nodes where the workload's memory is currently\n * located.\n */\nstatic void numa_group_count_active_nodes(struct numa_group *numa_group)\n{\n\tunsigned long faults, max_faults = 0;\n\tint nid, active_nodes = 0;\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults > max_faults)\n\t\t\tmax_faults = faults;\n\t}\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults * ACTIVE_NODE_FRACTION > max_faults)\n\t\t\tactive_nodes++;\n\t}\n\n\tnuma_group->max_faults_cpu = max_faults;\n\tnuma_group->active_nodes = active_nodes;\n}\n\n/*\n * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS\n * increments. The more local the fault statistics are, the higher the scan\n * period will be for the next scan window. If local/(local+remote) ratio is\n * below NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS)\n * the scan period will decrease. Aim for 70% local accesses.\n */\n#define NUMA_PERIOD_SLOTS 10\n#define NUMA_PERIOD_THRESHOLD 7\n\n/*\n * Increase the scan period (slow down scanning) if the majority of\n * our memory is already on our local node, or if the majority of\n * the page accesses are shared with other processes.\n * Otherwise, decrease the scan period.\n */\nstatic void update_task_scan_period(struct task_struct *p,\n\t\t\tunsigned long shared, unsigned long private)\n{\n\tunsigned int period_slot;\n\tint lr_ratio, ps_ratio;\n\tint diff;\n\n\tunsigned long remote = p->numa_faults_locality[0];\n\tunsigned long local = p->numa_faults_locality[1];\n\n\t/*\n\t * If there were no record hinting faults then either the task is\n\t * completely idle or all activity is areas that are not of interest\n\t * to automatic numa balancing. Related to that, if there were failed\n\t * migration then it implies we are migrating too quickly or the local\n\t * node is overloaded. In either case, scan slower\n\t */\n\tif (local + shared == 0 || p->numa_faults_locality[2]) {\n\t\tp->numa_scan_period = min(p->numa_scan_period_max,\n\t\t\tp->numa_scan_period << 1);\n\n\t\tp->mm->numa_next_scan = jiffies +\n\t\t\tmsecs_to_jiffies(p->numa_scan_period);\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Prepare to scale scan period relative to the current period.\n\t *\t == NUMA_PERIOD_THRESHOLD scan period stays the same\n\t *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)\n\t *\t >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)\n\t */\n\tperiod_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);\n\tlr_ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);\n\tps_ratio = (private * NUMA_PERIOD_SLOTS) / (private + shared);\n\n\tif (ps_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are local. There is no need to\n\t\t * do fast NUMA scanning, since memory is already local.\n\t\t */\n\t\tint slot = ps_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else if (lr_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are shared with other tasks.\n\t\t * There is no point in continuing fast NUMA scanning,\n\t\t * since other tasks may just move the memory elsewhere.\n\t\t */\n\t\tint slot = lr_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else {\n\t\t/*\n\t\t * Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS,\n\t\t * yet they are not on the local NUMA node. Speed up\n\t\t * NUMA scanning to get the memory moved over.\n\t\t */\n\t\tint ratio = max(lr_ratio, ps_ratio);\n\t\tdiff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;\n\t}\n\n\tp->numa_scan_period = clamp(p->numa_scan_period + diff,\n\t\t\ttask_scan_min(p), task_scan_max(p));\n\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n}\n\n/*\n * Get the fraction of time the task has been running since the last\n * NUMA placement cycle. The scheduler keeps similar statistics, but\n * decays those on a 32ms period, which is orders of magnitude off\n * from the dozens-of-seconds NUMA balancing period. Use the scheduler\n * stats only if the task is so new there are no NUMA statistics yet.\n */\nstatic u64 numa_get_avg_runtime(struct task_struct *p, u64 *period)\n{\n\tu64 runtime, delta, now;\n\t/* Use the start of this time slice to avoid calculations. */\n\tnow = p->se.exec_start;\n\truntime = p->se.sum_exec_runtime;\n\n\tif (p->last_task_numa_placement) {\n\t\tdelta = runtime - p->last_sum_exec_runtime;\n\t\t*period = now - p->last_task_numa_placement;\n\t} else {\n\t\tdelta = p->se.avg.load_sum;\n\t\t*period = LOAD_AVG_MAX;\n\t}\n\n\tp->last_sum_exec_runtime = runtime;\n\tp->last_task_numa_placement = now;\n\n\treturn delta;\n}\n\n/*\n * Determine the preferred nid for a task in a numa_group. This needs to\n * be done in a way that produces consistent results with group_weight,\n * otherwise workloads might not converge.\n */\nstatic int preferred_group_nid(struct task_struct *p, int nid)\n{\n\tnodemask_t nodes;\n\tint dist;\n\n\t/* Direct connections between all NUMA nodes. */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn nid;\n\n\t/*\n\t * On a system with glueless mesh NUMA topology, group_weight\n\t * scores nodes according to the number of NUMA hinting faults on\n\t * both the node itself, and on nearby nodes.\n\t */\n\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\tunsigned long score, max_score = 0;\n\t\tint node, max_node = nid;\n\n\t\tdist = sched_max_numa_distance;\n\n\t\tfor_each_online_node(node) {\n\t\t\tscore = group_weight(p, node, dist);\n\t\t\tif (score > max_score) {\n\t\t\t\tmax_score = score;\n\t\t\t\tmax_node = node;\n\t\t\t}\n\t\t}\n\t\treturn max_node;\n\t}\n\n\t/*\n\t * Finding the preferred nid in a system with NUMA backplane\n\t * interconnect topology is more involved. The goal is to locate\n\t * tasks from numa_groups near each other in the system, and\n\t * untangle workloads from different sides of the system. This requires\n\t * searching down the hierarchy of node groups, recursively searching\n\t * inside the highest scoring group of nodes. The nodemask tricks\n\t * keep the complexity of the search down.\n\t */\n\tnodes = node_online_map;\n\tfor (dist = sched_max_numa_distance; dist > LOCAL_DISTANCE; dist--) {\n\t\tunsigned long max_faults = 0;\n\t\tnodemask_t max_group = NODE_MASK_NONE;\n\t\tint a, b;\n\n\t\t/* Are there nodes at this distance from each other? */\n\t\tif (!find_numa_distance(dist))\n\t\t\tcontinue;\n\n\t\tfor_each_node_mask(a, nodes) {\n\t\t\tunsigned long faults = 0;\n\t\t\tnodemask_t this_group;\n\t\t\tnodes_clear(this_group);\n\n\t\t\t/* Sum group's NUMA faults; includes a==b case. */\n\t\t\tfor_each_node_mask(b, nodes) {\n\t\t\t\tif (node_distance(a, b) < dist) {\n\t\t\t\t\tfaults += group_faults(p, b);\n\t\t\t\t\tnode_set(b, this_group);\n\t\t\t\t\tnode_clear(b, nodes);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Remember the top group. */\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_group = this_group;\n\t\t\t\t/*\n\t\t\t\t * subtle: at the smallest distance there is\n\t\t\t\t * just one node left in each \"group\", the\n\t\t\t\t * winner is the preferred nid.\n\t\t\t\t */\n\t\t\t\tnid = a;\n\t\t\t}\n\t\t}\n\t\t/* Next round, evaluate the nodes within max_group. */\n\t\tif (!max_faults)\n\t\t\tbreak;\n\t\tnodes = max_group;\n\t}\n\treturn nid;\n}\n\nstatic void task_numa_placement(struct task_struct *p)\n{\n\tint seq, nid, max_nid = -1;\n\tunsigned long max_faults = 0;\n\tunsigned long fault_types[2] = { 0, 0 };\n\tunsigned long total_faults;\n\tu64 runtime, period;\n\tspinlock_t *group_lock = NULL;\n\n\t/*\n\t * The p->mm->numa_scan_seq field gets updated without\n\t * exclusive access. Use READ_ONCE() here to ensure\n\t * that the field is read in a single access:\n\t */\n\tseq = READ_ONCE(p->mm->numa_scan_seq);\n\tif (p->numa_scan_seq == seq)\n\t\treturn;\n\tp->numa_scan_seq = seq;\n\tp->numa_scan_period_max = task_scan_max(p);\n\n\ttotal_faults = p->numa_faults_locality[0] +\n\t\t       p->numa_faults_locality[1];\n\truntime = numa_get_avg_runtime(p, &period);\n\n\t/* If the task is part of a group prevent parallel updates to group stats */\n\tif (p->numa_group) {\n\t\tgroup_lock = &p->numa_group->lock;\n\t\tspin_lock_irq(group_lock);\n\t}\n\n\t/* Find the node with the highest number of faults */\n\tfor_each_online_node(nid) {\n\t\t/* Keep track of the offsets in numa_faults array */\n\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;\n\t\tunsigned long faults = 0, group_faults = 0;\n\t\tint priv;\n\n\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {\n\t\t\tlong diff, f_diff, f_weight;\n\n\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);\n\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);\n\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);\n\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);\n\n\t\t\t/* Decay existing window, copy faults since last scan */\n\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;\n\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];\n\t\t\tp->numa_faults[membuf_idx] = 0;\n\n\t\t\t/*\n\t\t\t * Normalize the faults_from, so all tasks in a group\n\t\t\t * count according to CPU use, instead of by the raw\n\t\t\t * number of faults. Tasks with little runtime have\n\t\t\t * little over-all impact on throughput, and thus their\n\t\t\t * faults are less important.\n\t\t\t */\n\t\t\tf_weight = div64_u64(runtime << 16, period + 1);\n\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /\n\t\t\t\t   (total_faults + 1);\n\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;\n\t\t\tp->numa_faults[cpubuf_idx] = 0;\n\n\t\t\tp->numa_faults[mem_idx] += diff;\n\t\t\tp->numa_faults[cpu_idx] += f_diff;\n\t\t\tfaults += p->numa_faults[mem_idx];\n\t\t\tp->total_numa_faults += diff;\n\t\t\tif (p->numa_group) {\n\t\t\t\t/*\n\t\t\t\t * safe because we can only change our own group\n\t\t\t\t *\n\t\t\t\t * mem_idx represents the offset for a given\n\t\t\t\t * nid and priv in a specific region because it\n\t\t\t\t * is at the beginning of the numa_faults array.\n\t\t\t\t */\n\t\t\t\tp->numa_group->faults[mem_idx] += diff;\n\t\t\t\tp->numa_group->faults_cpu[mem_idx] += f_diff;\n\t\t\t\tp->numa_group->total_faults += diff;\n\t\t\t\tgroup_faults += p->numa_group->faults[mem_idx];\n\t\t\t}\n\t\t}\n\n\t\tif (!p->numa_group) {\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_nid = nid;\n\t\t\t}\n\t\t} else if (group_faults > max_faults) {\n\t\t\tmax_faults = group_faults;\n\t\t\tmax_nid = nid;\n\t\t}\n\t}\n\n\tif (p->numa_group) {\n\t\tnuma_group_count_active_nodes(p->numa_group);\n\t\tspin_unlock_irq(group_lock);\n\t\tmax_nid = preferred_group_nid(p, max_nid);\n\t}\n\n\tif (max_faults) {\n\t\t/* Set the new preferred node */\n\t\tif (max_nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, max_nid);\n\t}\n\n\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);\n}\n\nstatic inline int get_numa_group(struct numa_group *grp)\n{\n\treturn atomic_inc_not_zero(&grp->refcount);\n}\n\nstatic inline void put_numa_group(struct numa_group *grp)\n{\n\tif (atomic_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}\n\nstatic void task_numa_group(struct task_struct *p, int cpupid, int flags,\n\t\t\tint *priv)\n{\n\tstruct numa_group *grp, *my_grp;\n\tstruct task_struct *tsk;\n\tbool join = false;\n\tint cpu = cpupid_to_cpu(cpupid);\n\tint i;\n\n\tif (unlikely(!p->numa_group)) {\n\t\tunsigned int size = sizeof(struct numa_group) +\n\t\t\t\t    4*nr_node_ids*sizeof(unsigned long);\n\n\t\tgrp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!grp)\n\t\t\treturn;\n\n\t\tatomic_set(&grp->refcount, 1);\n\t\tgrp->active_nodes = 1;\n\t\tgrp->max_faults_cpu = 0;\n\t\tspin_lock_init(&grp->lock);\n\t\tgrp->gid = p->pid;\n\t\t/* Second half of the array tracks nids where faults happen */\n\t\tgrp->faults_cpu = grp->faults + NR_NUMA_HINT_FAULT_TYPES *\n\t\t\t\t\t\tnr_node_ids;\n\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] = p->numa_faults[i];\n\n\t\tgrp->total_faults = p->total_numa_faults;\n\n\t\tgrp->nr_tasks++;\n\t\trcu_assign_pointer(p->numa_group, grp);\n\t}\n\n\trcu_read_lock();\n\ttsk = READ_ONCE(cpu_rq(cpu)->curr);\n\n\tif (!cpupid_match_pid(tsk, cpupid))\n\t\tgoto no_join;\n\n\tgrp = rcu_dereference(tsk->numa_group);\n\tif (!grp)\n\t\tgoto no_join;\n\n\tmy_grp = p->numa_group;\n\tif (grp == my_grp)\n\t\tgoto no_join;\n\n\t/*\n\t * Only join the other group if its bigger; if we're the bigger group,\n\t * the other task will join us.\n\t */\n\tif (my_grp->nr_tasks > grp->nr_tasks)\n\t\tgoto no_join;\n\n\t/*\n\t * Tie-break on the grp address.\n\t */\n\tif (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)\n\t\tgoto no_join;\n\n\t/* Always join threads in the same process. */\n\tif (tsk->mm == current->mm)\n\t\tjoin = true;\n\n\t/* Simple filter to avoid false positives due to PID collisions */\n\tif (flags & TNF_SHARED)\n\t\tjoin = true;\n\n\t/* Update priv based on whether false sharing was detected */\n\t*priv = !join;\n\n\tif (join && !get_numa_group(grp))\n\t\tgoto no_join;\n\n\trcu_read_unlock();\n\n\tif (!join)\n\t\treturn;\n\n\tBUG_ON(irqs_disabled());\n\tdouble_lock_irq(&my_grp->lock, &grp->lock);\n\n\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) {\n\t\tmy_grp->faults[i] -= p->numa_faults[i];\n\t\tgrp->faults[i] += p->numa_faults[i];\n\t}\n\tmy_grp->total_faults -= p->total_numa_faults;\n\tgrp->total_faults += p->total_numa_faults;\n\n\tmy_grp->nr_tasks--;\n\tgrp->nr_tasks++;\n\n\tspin_unlock(&my_grp->lock);\n\tspin_unlock_irq(&grp->lock);\n\n\trcu_assign_pointer(p->numa_group, grp);\n\n\tput_numa_group(my_grp);\n\treturn;\n\nno_join:\n\trcu_read_unlock();\n\treturn;\n}\n\nvoid task_numa_free(struct task_struct *p)\n{\n\tstruct numa_group *grp = p->numa_group;\n\tvoid *numa_faults = p->numa_faults;\n\tunsigned long flags;\n\tint i;\n\n\tif (grp) {\n\t\tspin_lock_irqsave(&grp->lock, flags);\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] -= p->numa_faults[i];\n\t\tgrp->total_faults -= p->total_numa_faults;\n\n\t\tgrp->nr_tasks--;\n\t\tspin_unlock_irqrestore(&grp->lock, flags);\n\t\tRCU_INIT_POINTER(p->numa_group, NULL);\n\t\tput_numa_group(grp);\n\t}\n\n\tp->numa_faults = NULL;\n\tkfree(numa_faults);\n}\n\n/*\n * Got a PROT_NONE fault for a page on @node.\n */\nvoid task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)\n{\n\tstruct task_struct *p = current;\n\tbool migrated = flags & TNF_MIGRATED;\n\tint cpu_node = task_node(current);\n\tint local = !!(flags & TNF_FAULT_LOCAL);\n\tstruct numa_group *ng;\n\tint priv;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\t/* for example, ksmd faulting in a user's mm */\n\tif (!p->mm)\n\t\treturn;\n\n\t/* Allocate buffer to track faults on a per-node basis */\n\tif (unlikely(!p->numa_faults)) {\n\t\tint size = sizeof(*p->numa_faults) *\n\t\t\t   NR_NUMA_HINT_FAULT_BUCKETS * nr_node_ids;\n\n\t\tp->numa_faults = kzalloc(size, GFP_KERNEL|__GFP_NOWARN);\n\t\tif (!p->numa_faults)\n\t\t\treturn;\n\n\t\tp->total_numa_faults = 0;\n\t\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n\t}\n\n\t/*\n\t * First accesses are treated as private, otherwise consider accesses\n\t * to be private if the accessing pid has not changed\n\t */\n\tif (unlikely(last_cpupid == (-1 & LAST_CPUPID_MASK))) {\n\t\tpriv = 1;\n\t} else {\n\t\tpriv = cpupid_match_pid(p, last_cpupid);\n\t\tif (!priv && !(flags & TNF_NO_GROUP))\n\t\t\ttask_numa_group(p, last_cpupid, flags, &priv);\n\t}\n\n\t/*\n\t * If a workload spans multiple NUMA nodes, a shared fault that\n\t * occurs wholly within the set of nodes that the workload is\n\t * actively using should be counted as local. This allows the\n\t * scan rate to slow down when a workload has settled down.\n\t */\n\tng = p->numa_group;\n\tif (!priv && !local && ng && ng->active_nodes > 1 &&\n\t\t\t\tnuma_is_active_node(cpu_node, ng) &&\n\t\t\t\tnuma_is_active_node(mem_node, ng))\n\t\tlocal = 1;\n\n\t/*\n\t * Retry to migrate task to preferred node periodically, in case it\n\t * previously failed, or the scheduler moved us.\n\t */\n\tif (time_after(jiffies, p->numa_migrate_retry)) {\n\t\ttask_numa_placement(p);\n\t\tnuma_migrate_preferred(p);\n\t}\n\n\tif (migrated)\n\t\tp->numa_pages_migrated += pages;\n\tif (flags & TNF_MIGRATE_FAIL)\n\t\tp->numa_faults_locality[2] += pages;\n\n\tp->numa_faults[task_faults_idx(NUMA_MEMBUF, mem_node, priv)] += pages;\n\tp->numa_faults[task_faults_idx(NUMA_CPUBUF, cpu_node, priv)] += pages;\n\tp->numa_faults_locality[local] += pages;\n}\n\nstatic void reset_ptenuma_scan(struct task_struct *p)\n{\n\t/*\n\t * We only did a read acquisition of the mmap sem, so\n\t * p->mm->numa_scan_seq is written to without exclusive access\n\t * and the update is not guaranteed to be atomic. That's not\n\t * much of an issue though, since this is just used for\n\t * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not\n\t * expensive, to avoid any form of compiler optimizations:\n\t */\n\tWRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);\n\tp->mm->numa_scan_offset = 0;\n}\n\n/*\n * The expensive part of numa migration is done from task_work context.\n * Triggered from task_tick_numa().\n */\nvoid task_numa_work(struct callback_head *work)\n{\n\tunsigned long migrate, next_scan, now = jiffies;\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tu64 runtime = p->se.sum_exec_runtime;\n\tstruct vm_area_struct *vma;\n\tunsigned long start, end;\n\tunsigned long nr_pte_updates = 0;\n\tlong pages, virtpages;\n\n\tSCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));\n\n\twork->next = work; /* protect against double add */\n\t/*\n\t * Who cares about NUMA placement when they're dying.\n\t *\n\t * NOTE: make sure not to dereference p->mm before this check,\n\t * exit_task_work() happens _after_ exit_mm() so we could be called\n\t * without p->mm even though we still had it when we enqueued this\n\t * work.\n\t */\n\tif (p->flags & PF_EXITING)\n\t\treturn;\n\n\tif (!mm->numa_next_scan) {\n\t\tmm->numa_next_scan = now +\n\t\t\tmsecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t}\n\n\t/*\n\t * Enforce maximal scan/migration frequency..\n\t */\n\tmigrate = mm->numa_next_scan;\n\tif (time_before(now, migrate))\n\t\treturn;\n\n\tif (p->numa_scan_period == 0) {\n\t\tp->numa_scan_period_max = task_scan_max(p);\n\t\tp->numa_scan_period = task_scan_start(p);\n\t}\n\n\tnext_scan = now + msecs_to_jiffies(p->numa_scan_period);\n\tif (cmpxchg(&mm->numa_next_scan, migrate, next_scan) != migrate)\n\t\treturn;\n\n\t/*\n\t * Delay this task enough that another task of this mm will likely win\n\t * the next time around.\n\t */\n\tp->node_stamp += 2 * TICK_NSEC;\n\n\tstart = mm->numa_scan_offset;\n\tpages = sysctl_numa_balancing_scan_size;\n\tpages <<= 20 - PAGE_SHIFT; /* MB in pages */\n\tvirtpages = pages * 8;\t   /* Scan up to this much virtual space */\n\tif (!pages)\n\t\treturn;\n\n\n\tif (!down_read_trylock(&mm->mmap_sem))\n\t\treturn;\n\tvma = find_vma(mm, start);\n\tif (!vma) {\n\t\treset_ptenuma_scan(p);\n\t\tstart = 0;\n\t\tvma = mm->mmap;\n\t}\n\tfor (; vma; vma = vma->vm_next) {\n\t\tif (!vma_migratable(vma) || !vma_policy_mof(vma) ||\n\t\t\tis_vm_hugetlb_page(vma) || (vma->vm_flags & VM_MIXEDMAP)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Shared library pages mapped by multiple processes are not\n\t\t * migrated as it is expected they are cache replicated. Avoid\n\t\t * hinting faults in read-only file-backed mappings or the vdso\n\t\t * as migrating the pages will be of marginal benefit.\n\t\t */\n\t\tif (!vma->vm_mm ||\n\t\t    (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Skip inaccessible VMAs to avoid any confusion between\n\t\t * PROT_NONE and NUMA hinting ptes\n\t\t */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tcontinue;\n\n\t\tdo {\n\t\t\tstart = max(start, vma->vm_start);\n\t\t\tend = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);\n\t\t\tend = min(end, vma->vm_end);\n\t\t\tnr_pte_updates = change_prot_numa(vma, start, end);\n\n\t\t\t/*\n\t\t\t * Try to scan sysctl_numa_balancing_size worth of\n\t\t\t * hpages that have at least one present PTE that\n\t\t\t * is not already pte-numa. If the VMA contains\n\t\t\t * areas that are unused or already full of prot_numa\n\t\t\t * PTEs, scan up to virtpages, to skip through those\n\t\t\t * areas faster.\n\t\t\t */\n\t\t\tif (nr_pte_updates)\n\t\t\t\tpages -= (end - start) >> PAGE_SHIFT;\n\t\t\tvirtpages -= (end - start) >> PAGE_SHIFT;\n\n\t\t\tstart = end;\n\t\t\tif (pages <= 0 || virtpages <= 0)\n\t\t\t\tgoto out;\n\n\t\t\tcond_resched();\n\t\t} while (end != vma->vm_end);\n\t}\n\nout:\n\t/*\n\t * It is possible to reach the end of the VMA list but the last few\n\t * VMAs are not guaranteed to the vma_migratable. If they are not, we\n\t * would find the !migratable VMA on the next scan but not reset the\n\t * scanner to the start so check it now.\n\t */\n\tif (vma)\n\t\tmm->numa_scan_offset = start;\n\telse\n\t\treset_ptenuma_scan(p);\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Make sure tasks use at least 32x as much time to run other code\n\t * than they used here, to limit NUMA PTE scanning overhead to 3% max.\n\t * Usually update_task_scan_period slows down scanning enough; on an\n\t * overloaded system we need to limit overhead on a per task basis.\n\t */\n\tif (unlikely(p->se.sum_exec_runtime != runtime)) {\n\t\tu64 diff = p->se.sum_exec_runtime - runtime;\n\t\tp->node_stamp += 32 * diff;\n\t}\n}\n\n/*\n * Drive the periodic memory faults..\n */\nvoid task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n\tstruct callback_head *work = &curr->numa_work;\n\tu64 period, now;\n\n\t/*\n\t * We don't care about NUMA placement if we don't have memory.\n\t */\n\tif (!curr->mm || (curr->flags & PF_EXITING) || work->next != work)\n\t\treturn;\n\n\t/*\n\t * Using runtime rather than walltime has the dual advantage that\n\t * we (mostly) drive the selection from busy threads and that the\n\t * task needs to have done some actual work before we bother with\n\t * NUMA placement.\n\t */\n\tnow = curr->se.sum_exec_runtime;\n\tperiod = (u64)curr->numa_scan_period * NSEC_PER_MSEC;\n\n\tif (now > curr->node_stamp + period) {\n\t\tif (!curr->node_stamp)\n\t\t\tcurr->numa_scan_period = task_scan_start(curr);\n\t\tcurr->node_stamp += period;\n\n\t\tif (!time_before(jiffies, curr->mm->numa_next_scan)) {\n\t\t\tinit_task_work(work, task_numa_work); /* TODO: move this into sched_fork() */\n\t\t\ttask_work_add(curr, work, true);\n\t\t}\n\t}\n}\n\nstatic void update_scan_period(struct task_struct *p, int new_cpu)\n{\n\tint src_nid = cpu_to_node(task_cpu(p));\n\tint dst_nid = cpu_to_node(new_cpu);\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\tif (!p->mm || !p->numa_faults || (p->flags & PF_EXITING))\n\t\treturn;\n\n\tif (src_nid == dst_nid)\n\t\treturn;\n\n\t/*\n\t * Allow resets if faults have been trapped before one scan\n\t * has completed. This is most likely due to a new task that\n\t * is pulled cross-node due to wakeups or load balancing.\n\t */\n\tif (p->numa_scan_seq) {\n\t\t/*\n\t\t * Avoid scan adjustments if moving to the preferred\n\t\t * node or if the task was not previously running on\n\t\t * the preferred node.\n\t\t */\n\t\tif (dst_nid == p->numa_preferred_nid ||\n\t\t    (p->numa_preferred_nid != -1 && src_nid != p->numa_preferred_nid))\n\t\t\treturn;\n\t}\n\n\tp->numa_scan_period = task_scan_start(p);\n}\n\n#else\nstatic void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n}\n\nstatic inline void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void update_scan_period(struct task_struct *p, int new_cpu)\n{\n}\n\n#endif /* CONFIG_NUMA_BALANCING */\n\nstatic void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_add(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n}\n\nstatic void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_sub(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n}\n\n/*\n * Signed add and clamp on underflow.\n *\n * Explicitly do a load-store to ensure the intermediate value never hits\n * memory. This allows lockless observations without ever seeing the negative\n * values.\n */\n#define add_positive(_ptr, _val) do {                           \\\n\ttypeof(_ptr) ptr = (_ptr);                              \\\n\ttypeof(_val) val = (_val);                              \\\n\ttypeof(*ptr) res, var = READ_ONCE(*ptr);                \\\n\t\t\t\t\t\t\t\t\\\n\tres = var + val;                                        \\\n\t\t\t\t\t\t\t\t\\\n\tif (val < 0 && res > var)                               \\\n\t\tres = 0;                                        \\\n\t\t\t\t\t\t\t\t\\\n\tWRITE_ONCE(*ptr, res);                                  \\\n} while (0)\n\n/*\n * Unsigned subtract and clamp on underflow.\n *\n * Explicitly do a load-store to ensure the intermediate value never hits\n * memory. This allows lockless observations without ever seeing the negative\n * values.\n */\n#define sub_positive(_ptr, _val) do {\t\t\t\t\\\n\ttypeof(_ptr) ptr = (_ptr);\t\t\t\t\\\n\ttypeof(*ptr) val = (_val);\t\t\t\t\\\n\ttypeof(*ptr) res, var = READ_ONCE(*ptr);\t\t\\\n\tres = var - val;\t\t\t\t\t\\\n\tif (res > var)\t\t\t\t\t\t\\\n\t\tres = 0;\t\t\t\t\t\\\n\tWRITE_ONCE(*ptr, res);\t\t\t\t\t\\\n} while (0)\n\n/*\n * Remove and clamp on negative, from a local variable.\n *\n * A variant of sub_positive(), which does not use explicit load-store\n * and is thus optimized for local variable updates.\n */\n#define lsub_positive(_ptr, _val) do {\t\t\t\t\\\n\ttypeof(_ptr) ptr = (_ptr);\t\t\t\t\\\n\t*ptr -= min_t(typeof(*ptr), *ptr, _val);\t\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\nstatic inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->runnable_weight += se->runnable_weight;\n\n\tcfs_rq->avg.runnable_load_avg += se->avg.runnable_load_avg;\n\tcfs_rq->avg.runnable_load_sum += se_runnable(se) * se->avg.runnable_load_sum;\n}\n\nstatic inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->runnable_weight -= se->runnable_weight;\n\n\tsub_positive(&cfs_rq->avg.runnable_load_avg, se->avg.runnable_load_avg);\n\tsub_positive(&cfs_rq->avg.runnable_load_sum,\n\t\t     se_runnable(se) * se->avg.runnable_load_sum);\n}\n\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->avg.load_avg += se->avg.load_avg;\n\tcfs_rq->avg.load_sum += se_weight(se) * se->avg.load_sum;\n}\n\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tsub_positive(&cfs_rq->avg.load_avg, se->avg.load_avg);\n\tsub_positive(&cfs_rq->avg.load_sum, se_weight(se) * se->avg.load_sum);\n}\n#else\nstatic inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\nstatic inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\n#endif\n\nstatic void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight, unsigned long runnable)\n{\n\tif (se->on_rq) {\n\t\t/* commit outstanding execution time */\n\t\tif (cfs_rq->curr == se)\n\t\t\tupdate_curr(cfs_rq);\n\t\taccount_entity_dequeue(cfs_rq, se);\n\t\tdequeue_runnable_load_avg(cfs_rq, se);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tse->runnable_weight = runnable;\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t\tse->avg.runnable_load_avg =\n\t\t\tdiv_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\taccount_entity_enqueue(cfs_rq, se);\n\t\tenqueue_runnable_load_avg(cfs_rq, se);\n\t}\n}\n\nvoid reweight_task(struct task_struct *p, int prio)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct load_weight *load = &se->load;\n\tunsigned long weight = scale_load(sched_prio_to_weight[prio]);\n\n\treweight_entity(cfs_rq, se, weight, weight);\n\tload->inv_weight = sched_prio_to_wmult[prio];\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n#ifdef CONFIG_SMP\n/*\n * All this does is approximate the hierarchical proportion which includes that\n * global sum we all love to hate.\n *\n * That is, the weight of a group entity, is the proportional share of the\n * group weight based on the group runqueue weights. That is:\n *\n *                     tg->weight * grq->load.weight\n *   ge->load.weight = -----------------------------               (1)\n *\t\t\t  \\Sum grq->load.weight\n *\n * Now, because computing that sum is prohibitively expensive to compute (been\n * there, done that) we approximate it with this average stuff. The average\n * moves slower and therefore the approximation is cheaper and more stable.\n *\n * So instead of the above, we substitute:\n *\n *   grq->load.weight -> grq->avg.load_avg                         (2)\n *\n * which yields the following:\n *\n *                     tg->weight * grq->avg.load_avg\n *   ge->load.weight = ------------------------------              (3)\n *\t\t\t\ttg->load_avg\n *\n * Where: tg->load_avg ~= \\Sum grq->avg.load_avg\n *\n * That is shares_avg, and it is right (given the approximation (2)).\n *\n * The problem with it is that because the average is slow -- it was designed\n * to be exactly that of course -- this leads to transients in boundary\n * conditions. In specific, the case where the group was idle and we start the\n * one task. It takes time for our CPU's grq->avg.load_avg to build up,\n * yielding bad latency etc..\n *\n * Now, in that special case (1) reduces to:\n *\n *                     tg->weight * grq->load.weight\n *   ge->load.weight = ----------------------------- = tg->weight   (4)\n *\t\t\t    grp->load.weight\n *\n * That is, the sum collapses because all other CPUs are idle; the UP scenario.\n *\n * So what we do is modify our approximation (3) to approach (4) in the (near)\n * UP case, like:\n *\n *   ge->load.weight =\n *\n *              tg->weight * grq->load.weight\n *     ---------------------------------------------------         (5)\n *     tg->load_avg - grq->avg.load_avg + grq->load.weight\n *\n * But because grq->load.weight can drop to 0, resulting in a divide by zero,\n * we need to use grq->avg.load_avg as its lower bound, which then gives:\n *\n *\n *                     tg->weight * grq->load.weight\n *   ge->load.weight = -----------------------------\t\t   (6)\n *\t\t\t\ttg_load_avg'\n *\n * Where:\n *\n *   tg_load_avg' = tg->load_avg - grq->avg.load_avg +\n *                  max(grq->load.weight, grq->avg.load_avg)\n *\n * And that is shares_weight and is icky. In the (near) UP case it approaches\n * (4) while in the normal case it approaches (3). It consistently\n * overestimates the ge->load.weight and therefore:\n *\n *   \\Sum ge->load.weight >= tg->weight\n *\n * hence icky!\n */\nstatic long calc_group_shares(struct cfs_rq *cfs_rq)\n{\n\tlong tg_weight, tg_shares, load, shares;\n\tstruct task_group *tg = cfs_rq->tg;\n\n\ttg_shares = READ_ONCE(tg->shares);\n\n\tload = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);\n\n\ttg_weight = atomic_long_read(&tg->load_avg);\n\n\t/* Ensure tg_weight >= load */\n\ttg_weight -= cfs_rq->tg_load_avg_contrib;\n\ttg_weight += load;\n\n\tshares = (tg_shares * load);\n\tif (tg_weight)\n\t\tshares /= tg_weight;\n\n\t/*\n\t * MIN_SHARES has to be unscaled here to support per-CPU partitioning\n\t * of a group with small tg->shares value. It is a floor value which is\n\t * assigned as a minimum load.weight to the sched_entity representing\n\t * the group on a CPU.\n\t *\n\t * E.g. on 64-bit for a group with tg->shares of scale_load(15)=15*1024\n\t * on an 8-core system with 8 tasks each runnable on one CPU shares has\n\t * to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In\n\t * case no task is runnable on a CPU MIN_SHARES=2 should be returned\n\t * instead of 0.\n\t */\n\treturn clamp_t(long, shares, MIN_SHARES, tg_shares);\n}\n\n/*\n * This calculates the effective runnable weight for a group entity based on\n * the group entity weight calculated above.\n *\n * Because of the above approximation (2), our group entity weight is\n * an load_avg based ratio (3). This means that it includes blocked load and\n * does not represent the runnable weight.\n *\n * Approximate the group entity's runnable weight per ratio from the group\n * runqueue:\n *\n *\t\t\t\t\t     grq->avg.runnable_load_avg\n *   ge->runnable_weight = ge->load.weight * -------------------------- (7)\n *\t\t\t\t\t\t grq->avg.load_avg\n *\n * However, analogous to above, since the avg numbers are slow, this leads to\n * transients in the from-idle case. Instead we use:\n *\n *   ge->runnable_weight = ge->load.weight *\n *\n *\t\tmax(grq->avg.runnable_load_avg, grq->runnable_weight)\n *\t\t-----------------------------------------------------\t(8)\n *\t\t      max(grq->avg.load_avg, grq->load.weight)\n *\n * Where these max() serve both to use the 'instant' values to fix the slow\n * from-idle and avoid the /0 on to-idle, similar to (6).\n */\nstatic long calc_group_runnable(struct cfs_rq *cfs_rq, long shares)\n{\n\tlong runnable, load_avg;\n\n\tload_avg = max(cfs_rq->avg.load_avg,\n\t\t       scale_load_down(cfs_rq->load.weight));\n\n\trunnable = max(cfs_rq->avg.runnable_load_avg,\n\t\t       scale_load_down(cfs_rq->runnable_weight));\n\n\trunnable *= shares;\n\tif (load_avg)\n\t\trunnable /= load_avg;\n\n\treturn clamp_t(long, runnable, MIN_SHARES, shares);\n}\n#endif /* CONFIG_SMP */\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq);\n\n/*\n * Recomputes the group entity based on the current state of its group\n * runqueue.\n */\nstatic void update_cfs_group(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\tlong shares, runnable;\n\n\tif (!gcfs_rq)\n\t\treturn;\n\n\tif (throttled_hierarchy(gcfs_rq))\n\t\treturn;\n\n#ifndef CONFIG_SMP\n\trunnable = shares = READ_ONCE(gcfs_rq->tg->shares);\n\n\tif (likely(se->load.weight == shares))\n\t\treturn;\n#else\n\tshares   = calc_group_shares(gcfs_rq);\n\trunnable = calc_group_runnable(gcfs_rq, shares);\n#endif\n\n\treweight_entity(cfs_rq_of(se), se, shares, runnable);\n}\n\n#else /* CONFIG_FAIR_GROUP_SCHED */\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\nstatic inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}\n\n#ifdef CONFIG_SMP\n#ifdef CONFIG_FAIR_GROUP_SCHED\n/**\n * update_tg_load_avg - update the tg's load avg\n * @cfs_rq: the cfs_rq whose avg changed\n * @force: update regardless of how small the difference\n *\n * This function 'ensures': tg->load_avg := \\Sum tg->cfs_rq[]->avg.load.\n * However, because tg->load_avg is a global value there are performance\n * considerations.\n *\n * In order to avoid having to look at the other cfs_rq's, we use a\n * differential update where we store the last value we propagated. This in\n * turn allows skipping updates if the differential is 'small'.\n *\n * Updating tg's load_avg is necessary before update_cfs_share().\n */\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)\n{\n\tlong delta = cfs_rq->avg.load_avg - cfs_rq->tg_load_avg_contrib;\n\n\t/*\n\t * No need to update load_avg for root_task_group as it is not used.\n\t */\n\tif (cfs_rq->tg == &root_task_group)\n\t\treturn;\n\n\tif (force || abs(delta) > cfs_rq->tg_load_avg_contrib / 64) {\n\t\tatomic_long_add(delta, &cfs_rq->tg->load_avg);\n\t\tcfs_rq->tg_load_avg_contrib = cfs_rq->avg.load_avg;\n\t}\n}\n\n/*\n * Called within set_task_rq() right before setting a task's CPU. The\n * caller only guarantees p->pi_lock is held; no other assumptions,\n * including the state of rq->lock, should be made.\n */\nvoid set_task_rq_fair(struct sched_entity *se,\n\t\t      struct cfs_rq *prev, struct cfs_rq *next)\n{\n\tu64 p_last_update_time;\n\tu64 n_last_update_time;\n\n\tif (!sched_feat(ATTACH_AGE_LOAD))\n\t\treturn;\n\n\t/*\n\t * We are supposed to update the task to \"current\" time, then its up to\n\t * date and ready to go to new CPU/cfs_rq. But we have difficulty in\n\t * getting what current time is, so simply throw away the out-of-date\n\t * time. This will result in the wakee task is less decayed, but giving\n\t * the wakee more load sounds not bad.\n\t */\n\tif (!(se->avg.last_update_time && prev))\n\t\treturn;\n\n#ifndef CONFIG_64BIT\n\t{\n\t\tu64 p_last_update_time_copy;\n\t\tu64 n_last_update_time_copy;\n\n\t\tdo {\n\t\t\tp_last_update_time_copy = prev->load_last_update_time_copy;\n\t\t\tn_last_update_time_copy = next->load_last_update_time_copy;\n\n\t\t\tsmp_rmb();\n\n\t\t\tp_last_update_time = prev->avg.last_update_time;\n\t\t\tn_last_update_time = next->avg.last_update_time;\n\n\t\t} while (p_last_update_time != p_last_update_time_copy ||\n\t\t\t n_last_update_time != n_last_update_time_copy);\n\t}\n#else\n\tp_last_update_time = prev->avg.last_update_time;\n\tn_last_update_time = next->avg.last_update_time;\n#endif\n\t__update_load_avg_blocked_se(p_last_update_time, cpu_of(rq_of(prev)), se);\n\tse->avg.last_update_time = n_last_update_time;\n}\n\n\n/*\n * When on migration a sched_entity joins/leaves the PELT hierarchy, we need to\n * propagate its contribution. The key to this propagation is the invariant\n * that for each group:\n *\n *   ge->avg == grq->avg\t\t\t\t\t\t(1)\n *\n * _IFF_ we look at the pure running and runnable sums. Because they\n * represent the very same entity, just at different points in the hierarchy.\n *\n * Per the above update_tg_cfs_util() is trivial and simply copies the running\n * sum over (but still wrong, because the group entity and group rq do not have\n * their PELT windows aligned).\n *\n * However, update_tg_cfs_runnable() is more complex. So we have:\n *\n *   ge->avg.load_avg = ge->load.weight * ge->avg.runnable_avg\t\t(2)\n *\n * And since, like util, the runnable part should be directly transferable,\n * the following would _appear_ to be the straight forward approach:\n *\n *   grq->avg.load_avg = grq->load.weight * grq->avg.runnable_avg\t(3)\n *\n * And per (1) we have:\n *\n *   ge->avg.runnable_avg == grq->avg.runnable_avg\n *\n * Which gives:\n *\n *                      ge->load.weight * grq->avg.load_avg\n *   ge->avg.load_avg = -----------------------------------\t\t(4)\n *                               grq->load.weight\n *\n * Except that is wrong!\n *\n * Because while for entities historical weight is not important and we\n * really only care about our future and therefore can consider a pure\n * runnable sum, runqueues can NOT do this.\n *\n * We specifically want runqueues to have a load_avg that includes\n * historical weights. Those represent the blocked load, the load we expect\n * to (shortly) return to us. This only works by keeping the weights as\n * integral part of the sum. We therefore cannot decompose as per (3).\n *\n * Another reason this doesn't work is that runnable isn't a 0-sum entity.\n * Imagine a rq with 2 tasks that each are runnable 2/3 of the time. Then the\n * rq itself is runnable anywhere between 2/3 and 1 depending on how the\n * runnable section of these tasks overlap (or not). If they were to perfectly\n * align the rq as a whole would be runnable 2/3 of the time. If however we\n * always have at least 1 runnable task, the rq as a whole is always runnable.\n *\n * So we'll have to approximate.. :/\n *\n * Given the constraint:\n *\n *   ge->avg.running_sum <= ge->avg.runnable_sum <= LOAD_AVG_MAX\n *\n * We can construct a rule that adds runnable to a rq by assuming minimal\n * overlap.\n *\n * On removal, we'll assume each task is equally runnable; which yields:\n *\n *   grq->avg.runnable_sum = grq->avg.load_sum / grq->load.weight\n *\n * XXX: only do this for the part of runnable > running ?\n *\n */\n\nstatic inline void\nupdate_tg_cfs_util(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta = gcfs_rq->avg.util_avg - se->avg.util_avg;\n\n\t/* Nothing to update */\n\tif (!delta)\n\t\treturn;\n\n\t/*\n\t * The relation between sum and avg is:\n\t *\n\t *   LOAD_AVG_MAX - 1024 + sa->period_contrib\n\t *\n\t * however, the PELT windows are not aligned between grq and gse.\n\t */\n\n\t/* Set new sched_entity's utilization */\n\tse->avg.util_avg = gcfs_rq->avg.util_avg;\n\tse->avg.util_sum = se->avg.util_avg * LOAD_AVG_MAX;\n\n\t/* Update parent cfs_rq utilization */\n\tadd_positive(&cfs_rq->avg.util_avg, delta);\n\tcfs_rq->avg.util_sum = cfs_rq->avg.util_avg * LOAD_AVG_MAX;\n}\n\nstatic inline void\nupdate_tg_cfs_runnable(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;\n\tunsigned long runnable_load_avg, load_avg;\n\tu64 runnable_load_sum, load_sum = 0;\n\ts64 delta_sum;\n\n\tif (!runnable_sum)\n\t\treturn;\n\n\tgcfs_rq->prop_runnable_sum = 0;\n\n\tif (runnable_sum >= 0) {\n\t\t/*\n\t\t * Add runnable; clip at LOAD_AVG_MAX. Reflects that until\n\t\t * the CPU is saturated running == runnable.\n\t\t */\n\t\trunnable_sum += se->avg.load_sum;\n\t\trunnable_sum = min(runnable_sum, (long)LOAD_AVG_MAX);\n\t} else {\n\t\t/*\n\t\t * Estimate the new unweighted runnable_sum of the gcfs_rq by\n\t\t * assuming all tasks are equally runnable.\n\t\t */\n\t\tif (scale_load_down(gcfs_rq->load.weight)) {\n\t\t\tload_sum = div_s64(gcfs_rq->avg.load_sum,\n\t\t\t\tscale_load_down(gcfs_rq->load.weight));\n\t\t}\n\n\t\t/* But make sure to not inflate se's runnable */\n\t\trunnable_sum = min(se->avg.load_sum, load_sum);\n\t}\n\n\t/*\n\t * runnable_sum can't be lower than running_sum\n\t * As running sum is scale with CPU capacity wehreas the runnable sum\n\t * is not we rescale running_sum 1st\n\t */\n\trunning_sum = se->avg.util_sum /\n\t\tarch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\trunnable_sum = max(runnable_sum, running_sum);\n\n\tload_sum = (s64)se_weight(se) * runnable_sum;\n\tload_avg = div_s64(load_sum, LOAD_AVG_MAX);\n\n\tdelta_sum = load_sum - (s64)se_weight(se) * se->avg.load_sum;\n\tdelta_avg = load_avg - se->avg.load_avg;\n\n\tse->avg.load_sum = runnable_sum;\n\tse->avg.load_avg = load_avg;\n\tadd_positive(&cfs_rq->avg.load_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.load_sum, delta_sum);\n\n\trunnable_load_sum = (s64)se_runnable(se) * runnable_sum;\n\trunnable_load_avg = div_s64(runnable_load_sum, LOAD_AVG_MAX);\n\tdelta_sum = runnable_load_sum - se_weight(se) * se->avg.runnable_load_sum;\n\tdelta_avg = runnable_load_avg - se->avg.runnable_load_avg;\n\n\tse->avg.runnable_load_sum = runnable_sum;\n\tse->avg.runnable_load_avg = runnable_load_avg;\n\n\tif (se->on_rq) {\n\t\tadd_positive(&cfs_rq->avg.runnable_load_avg, delta_avg);\n\t\tadd_positive(&cfs_rq->avg.runnable_load_sum, delta_sum);\n\t}\n}\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum)\n{\n\tcfs_rq->propagate = 1;\n\tcfs_rq->prop_runnable_sum += runnable_sum;\n}\n\n/* Update task and its cfs_rq load average */\nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq, *gcfs_rq;\n\n\tif (entity_is_task(se))\n\t\treturn 0;\n\n\tgcfs_rq = group_cfs_rq(se);\n\tif (!gcfs_rq->propagate)\n\t\treturn 0;\n\n\tgcfs_rq->propagate = 0;\n\n\tcfs_rq = cfs_rq_of(se);\n\n\tadd_tg_cfs_propagate(cfs_rq, gcfs_rq->prop_runnable_sum);\n\n\tupdate_tg_cfs_util(cfs_rq, se, gcfs_rq);\n\tupdate_tg_cfs_runnable(cfs_rq, se, gcfs_rq);\n\n\treturn 1;\n}\n\n/*\n * Check if we need to update the load and the utilization of a blocked\n * group_entity:\n */\nstatic inline bool skip_blocked_update(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\n\t/*\n\t * If sched_entity still have not zero load or utilization, we have to\n\t * decay it:\n\t */\n\tif (se->avg.load_avg || se->avg.util_avg)\n\t\treturn false;\n\n\t/*\n\t * If there is a pending propagation, we have to update the load and\n\t * the utilization of the sched_entity:\n\t */\n\tif (gcfs_rq->propagate)\n\t\treturn false;\n\n\t/*\n\t * Otherwise, the load and the utilization of the sched_entity is\n\t * already zero and there is no pending propagation, so it will be a\n\t * waste of time to try to decay it:\n\t */\n\treturn true;\n}\n\n#else /* CONFIG_FAIR_GROUP_SCHED */\n\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}\n\nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\treturn 0;\n}\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n/**\n * update_cfs_rq_load_avg - update the cfs_rq's load/util averages\n * @now: current time, as per cfs_rq_clock_task()\n * @cfs_rq: cfs_rq to update\n *\n * The cfs_rq avg is the direct sum of all its entities (blocked and runnable)\n * avg. The immediate corollary is that all (fair) tasks must be attached, see\n * post_init_entity_util_avg().\n *\n * cfs_rq->avg is used for task_h_load() and update_cfs_share() for example.\n *\n * Returns true if the load decayed or we removed load.\n *\n * Since both these conditions indicate a changed cfs_rq->avg.load we should\n * call update_tg_load_avg() when this function returns true.\n */\nstatic inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_sum, removed_runnable_sum);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\n\t\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);\n\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->load_last_update_time_copy = sa->last_update_time;\n#endif\n\n\tif (decayed)\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\treturn decayed;\n}\n\n/**\n * attach_entity_load_avg - attach this entity to its cfs_rq load avg\n * @cfs_rq: cfs_rq to attach to\n * @se: sched_entity to attach\n * @flags: migration hints\n *\n * Must call update_cfs_rq_load_avg() before this, since we rely on\n * cfs_rq->avg.last_update_time being current.\n */\nstatic void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu32 divider = LOAD_AVG_MAX - 1024 + cfs_rq->avg.period_contrib;\n\n\t/*\n\t * When we attach the @se to the @cfs_rq, we must align the decay\n\t * window because without that, really weird and wonderful things can\n\t * happen.\n\t *\n\t * XXX illustrate\n\t */\n\tse->avg.last_update_time = cfs_rq->avg.last_update_time;\n\tse->avg.period_contrib = cfs_rq->avg.period_contrib;\n\n\t/*\n\t * Hell(o) Nasty stuff.. we need to recompute _sum based on the new\n\t * period_contrib. This isn't strictly correct, but since we're\n\t * entirely outside of the PELT hierarchy, nobody cares if we truncate\n\t * _sum a little.\n\t */\n\tse->avg.util_sum = se->avg.util_avg * divider;\n\n\tse->avg.load_sum = divider;\n\tif (se_weight(se)) {\n\t\tse->avg.load_sum =\n\t\t\tdiv_u64(se->avg.load_avg * se->avg.load_sum, se_weight(se));\n\t}\n\n\tse->avg.runnable_load_sum = se->avg.load_sum;\n\n\tenqueue_load_avg(cfs_rq, se);\n\tcfs_rq->avg.util_avg += se->avg.util_avg;\n\tcfs_rq->avg.util_sum += se->avg.util_sum;\n\n\tadd_tg_cfs_propagate(cfs_rq, se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, flags);\n}\n\n/**\n * detach_entity_load_avg - detach this entity from its cfs_rq load avg\n * @cfs_rq: cfs_rq to detach from\n * @se: sched_entity to detach\n *\n * Must call update_cfs_rq_load_avg() before this, since we rely on\n * cfs_rq->avg.last_update_time being current.\n */\nstatic void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tdequeue_load_avg(cfs_rq, se);\n\tsub_positive(&cfs_rq->avg.util_avg, se->avg.util_avg);\n\tsub_positive(&cfs_rq->avg.util_sum, se->avg.util_sum);\n\n\tadd_tg_cfs_propagate(cfs_rq, -se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, 0);\n}\n\n/*\n * Optional action to be done while updating the load average\n */\n#define UPDATE_TG\t0x1\n#define SKIP_AGE_LOAD\t0x2\n#define DO_ATTACH\t0x4\n\n/* Update task and its cfs_rq load average */\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu64 now = cfs_rq_clock_task(cfs_rq);\n\tstruct rq *rq = rq_of(cfs_rq);\n\tint cpu = cpu_of(rq);\n\tint decayed;\n\n\t/*\n\t * Track task load average for carrying it to new CPU after migrated, and\n\t * track group sched_entity load average for task_h_load calc in migration\n\t */\n\tif (se->avg.last_update_time && !(flags & SKIP_AGE_LOAD))\n\t\t__update_load_avg_se(now, cpu, cfs_rq, se);\n\n\tdecayed  = update_cfs_rq_load_avg(now, cfs_rq);\n\tdecayed |= propagate_entity_load_avg(se);\n\n\tif (!se->avg.last_update_time && (flags & DO_ATTACH)) {\n\n\t\t/*\n\t\t * DO_ATTACH means we're here from enqueue_entity().\n\t\t * !last_update_time means we've passed through\n\t\t * migrate_task_rq_fair() indicating we migrated.\n\t\t *\n\t\t * IOW we're enqueueing a task on a new CPU.\n\t\t */\n\t\tattach_entity_load_avg(cfs_rq, se, SCHED_CPUFREQ_MIGRATION);\n\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t} else if (decayed && (flags & UPDATE_TG))\n\t\tupdate_tg_load_avg(cfs_rq, 0);\n}\n\n#ifndef CONFIG_64BIT\nstatic inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\tu64 last_update_time_copy;\n\tu64 last_update_time;\n\n\tdo {\n\t\tlast_update_time_copy = cfs_rq->load_last_update_time_copy;\n\t\tsmp_rmb();\n\t\tlast_update_time = cfs_rq->avg.last_update_time;\n\t} while (last_update_time != last_update_time_copy);\n\n\treturn last_update_time;\n}\n#else\nstatic inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.last_update_time;\n}\n#endif\n\n/*\n * Synchronize entity load avg of dequeued entity without locking\n * the previous rq.\n */\nvoid sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}\n\n/*\n * Task first catches up with cfs_rq, and then subtract\n * itself from the cfs_rq (task must be off the queue now).\n */\nvoid remove_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tunsigned long flags;\n\n\t/*\n\t * tasks cannot exit without having gone through wake_up_new_task() ->\n\t * post_init_entity_util_avg() which will have added things to the\n\t * cfs_rq, so we can remove unconditionally.\n\t *\n\t * Similarly for groups, they will have passed through\n\t * post_init_entity_util_avg() before unregister_sched_fair_group()\n\t * calls this.\n\t */\n\n\tsync_entity_load_avg(se);\n\n\traw_spin_lock_irqsave(&cfs_rq->removed.lock, flags);\n\t++cfs_rq->removed.nr;\n\tcfs_rq->removed.util_avg\t+= se->avg.util_avg;\n\tcfs_rq->removed.load_avg\t+= se->avg.load_avg;\n\tcfs_rq->removed.runnable_sum\t+= se->avg.load_sum; /* == runnable_sum */\n\traw_spin_unlock_irqrestore(&cfs_rq->removed.lock, flags);\n}\n\nstatic inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.runnable_load_avg;\n}\n\nstatic inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}\n\nstatic int idle_balance(struct rq *this_rq, struct rq_flags *rf);\n\nstatic inline unsigned long task_util(struct task_struct *p)\n{\n\treturn READ_ONCE(p->se.avg.util_avg);\n}\n\nstatic inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn (max(ue.ewma, ue.enqueued) | UTIL_AVG_UNCHANGED);\n}\n\nstatic inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}\n\nstatic inline void util_est_enqueue(struct cfs_rq *cfs_rq,\n\t\t\t\t    struct task_struct *p)\n{\n\tunsigned int enqueued;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t/* Update root cfs_rq's estimated utilization */\n\tenqueued  = cfs_rq->avg.util_est.enqueued;\n\tenqueued += _task_util_est(p);\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);\n}\n\n/*\n * Check if a (signed) value is within a specified (unsigned) margin,\n * based on the observation that:\n *\n *     abs(x) < y := (unsigned)(x + y - 1) < (2 * y - 1)\n *\n * NOTE: this only works when value + maring < INT_MAX.\n */\nstatic inline bool within_margin(int value, int margin)\n{\n\treturn ((unsigned int)(value + margin - 1) < (2 * margin - 1));\n}\n\nstatic void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p, bool task_sleep)\n{\n\tlong last_ewma_diff;\n\tstruct util_est ue;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t/* Update root cfs_rq's estimated utilization */\n\tue.enqueued  = cfs_rq->avg.util_est.enqueued;\n\tue.enqueued -= min_t(unsigned int, ue.enqueued, _task_util_est(p));\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, ue.enqueued);\n\n\t/*\n\t * Skip update of task's estimated utilization when the task has not\n\t * yet completed an activation, e.g. being migrated.\n\t */\n\tif (!task_sleep)\n\t\treturn;\n\n\t/*\n\t * If the PELT values haven't changed since enqueue time,\n\t * skip the util_est update.\n\t */\n\tue = p->se.avg.util_est;\n\tif (ue.enqueued & UTIL_AVG_UNCHANGED)\n\t\treturn;\n\n\t/*\n\t * Skip update of task's estimated utilization when its EWMA is\n\t * already ~1% close to its last activation value.\n\t */\n\tue.enqueued = (task_util(p) | UTIL_AVG_UNCHANGED);\n\tlast_ewma_diff = ue.enqueued - ue.ewma;\n\tif (within_margin(last_ewma_diff, (SCHED_CAPACITY_SCALE / 100)))\n\t\treturn;\n\n\t/*\n\t * Update Task's estimated utilization\n\t *\n\t * When *p completes an activation we can consolidate another sample\n\t * of the task size. This is done by storing the current PELT value\n\t * as ue.enqueued and by using this value to update the Exponential\n\t * Weighted Moving Average (EWMA):\n\t *\n\t *  ewma(t) = w *  task_util(p) + (1-w) * ewma(t-1)\n\t *          = w *  task_util(p) +         ewma(t-1)  - w * ewma(t-1)\n\t *          = w * (task_util(p) -         ewma(t-1)) +     ewma(t-1)\n\t *          = w * (      last_ewma_diff            ) +     ewma(t-1)\n\t *          = w * (last_ewma_diff  +  ewma(t-1) / w)\n\t *\n\t * Where 'w' is the weight of new samples, which is configured to be\n\t * 0.25, thus making w=1/4 ( >>= UTIL_EST_WEIGHT_SHIFT)\n\t */\n\tue.ewma <<= UTIL_EST_WEIGHT_SHIFT;\n\tue.ewma  += last_ewma_diff;\n\tue.ewma >>= UTIL_EST_WEIGHT_SHIFT;\n\tWRITE_ONCE(p->se.avg.util_est, ue);\n}\n\nstatic inline int task_fits_capacity(struct task_struct *p, long capacity)\n{\n\treturn capacity * 1024 > task_util_est(p) * capacity_margin;\n}\n\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq)\n{\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn;\n\n\tif (!p) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\tif (task_fits_capacity(p, capacity_of(cpu_of(rq)))) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\trq->misfit_task_load = task_h_load(p);\n}\n\n#else /* CONFIG_SMP */\n\n#define UPDATE_TG\t0x0\n#define SKIP_AGE_LOAD\t0x0\n#define DO_ATTACH\t0x0\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}\n\nstatic inline void remove_entity_load_avg(struct sched_entity *se) {}\n\nstatic inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {}\nstatic inline void\ndetach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}\n\nstatic inline int idle_balance(struct rq *rq, struct rq_flags *rf)\n{\n\treturn 0;\n}\n\nstatic inline void\nutil_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}\n\nstatic inline void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p,\n\t\t bool task_sleep) {}\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}\n\n#endif /* CONFIG_SMP */\n\nstatic void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n\n\tif (d < 0)\n\t\td = -d;\n\n\tif (d > 3*sysctl_sched_latency)\n\t\tschedstat_inc(cfs_rq->nr_spread_over);\n#endif\n}\n\nstatic void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n{\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\t/*\n\t * The 'current' period is already promised to the current tasks,\n\t * however the extra weight of the new task will slow them down a\n\t * little, place the new task so that it fits in the slot that\n\t * stays open at the end.\n\t */\n\tif (initial && sched_feat(START_DEBIT))\n\t\tvruntime += sched_vslice(cfs_rq, se);\n\n\t/* sleeps up to a single latency don't count. */\n\tif (!initial) {\n\t\tunsigned long thresh = sysctl_sched_latency;\n\n\t\t/*\n\t\t * Halve their sleep time's effect, to allow\n\t\t * for a gentler effect of sleepers:\n\t\t */\n\t\tif (sched_feat(GENTLE_FAIR_SLEEPERS))\n\t\t\tthresh >>= 1;\n\n\t\tvruntime -= thresh;\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tse->vruntime = max_vruntime(se->vruntime, vruntime);\n}\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\n\nstatic inline void check_schedstat_required(void)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tif (schedstat_enabled())\n\t\treturn;\n\n\t/* Force schedstat enabled if a dependent tracepoint is active */\n\tif (trace_sched_stat_wait_enabled()    ||\n\t\t\ttrace_sched_stat_sleep_enabled()   ||\n\t\t\ttrace_sched_stat_iowait_enabled()  ||\n\t\t\ttrace_sched_stat_blocked_enabled() ||\n\t\t\ttrace_sched_stat_runtime_enabled())  {\n\t\tprintk_deferred_once(\"Scheduler tracepoints stat_sleep, stat_iowait, \"\n\t\t\t     \"stat_blocked and stat_runtime require the \"\n\t\t\t     \"kernel parameter schedstats=enable or \"\n\t\t\t     \"kernel.sched_schedstats=1\\n\");\n\t}\n#endif\n}\n\n\n/*\n * MIGRATION\n *\n *\tdequeue\n *\t  update_curr()\n *\t    update_min_vruntime()\n *\t  vruntime -= min_vruntime\n *\n *\tenqueue\n *\t  update_curr()\n *\t    update_min_vruntime()\n *\t  vruntime += min_vruntime\n *\n * this way the vruntime transition between RQs is done when both\n * min_vruntime are up-to-date.\n *\n * WAKEUP (remote)\n *\n *\t->migrate_task_rq_fair() (p->state == TASK_WAKING)\n *\t  vruntime -= min_vruntime\n *\n *\tenqueue\n *\t  update_curr()\n *\t    update_min_vruntime()\n *\t  vruntime += min_vruntime\n *\n * this way we don't have the most up-to-date min_vruntime on the originating\n * CPU and an up-to-date min_vruntime on the destination CPU.\n */\n\nstatic void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);\n\tbool curr = cfs_rq->curr == se;\n\n\t/*\n\t * If we're the current task, we must renormalise before calling\n\t * update_curr().\n\t */\n\tif (renorm && curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Otherwise, renormalise after, such that we're placed at the current\n\t * moment in time, instead of some random moment in the past. Being\n\t * placed in the past could significantly boost this task to the\n\t * fairness detriment of existing tasks.\n\t */\n\tif (renorm && !curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\t/*\n\t * When enqueuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Add its load to cfs_rq->runnable_avg\n\t *   - For group_entity, update its weight to reflect the new share of\n\t *     its group cfs_rq\n\t *   - Add its new weight to cfs_rq->load.weight\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tupdate_cfs_group(se);\n\tenqueue_runnable_load_avg(cfs_rq, se);\n\taccount_entity_enqueue(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tplace_entity(cfs_rq, se, 0);\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue(cfs_rq, se, flags);\n\tcheck_spread(cfs_rq, se);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t}\n}\n\nstatic void __clear_buddies_last(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->last != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->last = NULL;\n\t}\n}\n\nstatic void __clear_buddies_next(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->next != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->next = NULL;\n\t}\n}\n\nstatic void __clear_buddies_skip(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->skip != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->skip = NULL;\n\t}\n}\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * When dequeuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Subtract its load from the cfs_rq->runnable_avg.\n\t *   - Subtract its previous weight from cfs_rq->load.weight.\n\t *   - For group entity, update its weight to reflect the new share\n\t *     of its group cfs_rq.\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\tdequeue_runnable_load_avg(cfs_rq, se);\n\n\tupdate_stats_dequeue(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t/*\n\t * Normalize after update_curr(); which will also have moved\n\t * min_vruntime if @se is the one holding it back. But before doing\n\t * update_min_vruntime() again, which will discount @se's position and\n\t * can move min_vruntime forward still more.\n\t */\n\tif (!(flags & DEQUEUE_SLEEP))\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\n\t/* return excess runtime on last dequeue */\n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t/*\n\t * Now advance min_vruntime if @se was the entity holding it back,\n\t * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be\n\t * put back on, and if we advance min_vruntime, we'll be placed back\n\t * further than we started -- ie. we'll be penalized.\n\t */\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n}\n\n/*\n * Preempt the current task with a newly woken task if needed:\n */\nstatic void\ncheck_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tunsigned long ideal_runtime, delta_exec;\n\tstruct sched_entity *se;\n\ts64 delta;\n\n\tideal_runtime = sched_slice(cfs_rq, curr);\n\tdelta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;\n\tif (delta_exec > ideal_runtime) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\t/*\n\t\t * The current task ran long enough, ensure it doesn't get\n\t\t * re-elected due to buddy favours.\n\t\t */\n\t\tclear_buddies(cfs_rq, curr);\n\t\treturn;\n\t}\n\n\t/*\n\t * Ensure that a task that missed wakeup preemption by a\n\t * narrow margin doesn't have to wait for a full slice.\n\t * This also mitigates buddy induced latencies under load.\n\t */\n\tif (delta_exec < sysctl_sched_min_granularity)\n\t\treturn;\n\n\tse = __pick_first_entity(cfs_rq);\n\tdelta = curr->vruntime - se->vruntime;\n\n\tif (delta < 0)\n\t\treturn;\n\n\tif (delta > ideal_runtime)\n\t\tresched_curr(rq_of(cfs_rq));\n}\n\nstatic void\nset_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/* 'current' is not kept within the tree. */\n\tif (se->on_rq) {\n\t\t/*\n\t\t * Any task has to be enqueued before it get to execute on\n\t\t * a CPU. So account for the time it spent waiting on the\n\t\t * runqueue.\n\t\t */\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\t\t__dequeue_entity(cfs_rq, se);\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n\n\tupdate_stats_curr_start(cfs_rq, se);\n\tcfs_rq->curr = se;\n\n\t/*\n\t * Track our maximum slice length, if the CPU's load is at\n\t * least twice that of our own weight (i.e. dont track it\n\t * when there are only lesser-weight tasks around):\n\t */\n\tif (schedstat_enabled() && rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {\n\t\tschedstat_set(se->statistics.slice_max,\n\t\t\tmax((u64)schedstat_val(se->statistics.slice_max),\n\t\t\t    se->sum_exec_runtime - se->prev_sum_exec_runtime));\n\t}\n\n\tse->prev_sum_exec_runtime = se->sum_exec_runtime;\n}\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\n\n/*\n * Pick the next process, keeping these things in mind, in this order:\n * 1) keep things fair between processes/task groups\n * 2) pick the \"next\" process, since someone really wants that to run\n * 3) pick the \"last\" process, for cache locality\n * 4) do not run the \"skip\" process, if something else is available\n */\nstatic struct sched_entity *\npick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tstruct sched_entity *left = __pick_first_entity(cfs_rq);\n\tstruct sched_entity *se;\n\n\t/*\n\t * If curr is set we have to see if its left of the leftmost entity\n\t * still in the tree, provided there was anything in the tree at all.\n\t */\n\tif (!left || (curr && entity_before(curr, left)))\n\t\tleft = curr;\n\n\tse = left; /* ideally we run the leftmost entity */\n\n\t/*\n\t * Avoid running the skip buddy, if running something else can\n\t * be done without getting too unfair.\n\t */\n\tif (cfs_rq->skip == se) {\n\t\tstruct sched_entity *second;\n\n\t\tif (se == curr) {\n\t\t\tsecond = __pick_first_entity(cfs_rq);\n\t\t} else {\n\t\t\tsecond = __pick_next_entity(se);\n\t\t\tif (!second || (curr && entity_before(curr, second)))\n\t\t\t\tsecond = curr;\n\t\t}\n\n\t\tif (second && wakeup_preempt_entity(second, left) < 1)\n\t\t\tse = second;\n\t}\n\n\t/*\n\t * Prefer last buddy, try to return the CPU to a preempted task.\n\t */\n\tif (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1)\n\t\tse = cfs_rq->last;\n\n\t/*\n\t * Someone really wants this to run. If it's not unfair, run it.\n\t */\n\tif (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1)\n\t\tse = cfs_rq->next;\n\n\tclear_buddies(cfs_rq, se);\n\n\treturn se;\n}\n\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)\n{\n\t/*\n\t * If still on the runqueue then deactivate_task()\n\t * was not called and update_curr() has to be done:\n\t */\n\tif (prev->on_rq)\n\t\tupdate_curr(cfs_rq);\n\n\t/* throttle cfs_rqs exceeding runtime */\n\tcheck_cfs_rq_runtime(cfs_rq);\n\n\tcheck_spread(cfs_rq, prev);\n\n\tif (prev->on_rq) {\n\t\tupdate_stats_wait_start(cfs_rq, prev);\n\t\t/* Put 'current' back into the tree. */\n\t\t__enqueue_entity(cfs_rq, prev);\n\t\t/* in !on_rq case, update occurred at dequeue */\n\t\tupdate_load_avg(cfs_rq, prev, 0);\n\t}\n\tcfs_rq->curr = NULL;\n}\n\nstatic void\nentity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Ensure that runnable average is periodically updated.\n\t */\n\tupdate_load_avg(cfs_rq, curr, UPDATE_TG);\n\tupdate_cfs_group(curr);\n\n#ifdef CONFIG_SCHED_HRTICK\n\t/*\n\t * queued ticks are scheduled to match the slice, so don't bother\n\t * validating it and just reschedule.\n\t */\n\tif (queued) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\treturn;\n\t}\n\t/*\n\t * don't let the period tick interfere with the hrtick preemption\n\t */\n\tif (!sched_feat(DOUBLE_TICK) &&\n\t\t\thrtimer_active(&rq_of(cfs_rq)->hrtick_timer))\n\t\treturn;\n#endif\n\n\tif (cfs_rq->nr_running > 1)\n\t\tcheck_preempt_tick(cfs_rq, curr);\n}\n\n\n/**************************************************\n * CFS bandwidth control machinery\n */\n\n#ifdef CONFIG_CFS_BANDWIDTH\n\n#ifdef HAVE_JUMP_LABEL\nstatic struct static_key __cfs_bandwidth_used;\n\nstatic inline bool cfs_bandwidth_used(void)\n{\n\treturn static_key_false(&__cfs_bandwidth_used);\n}\n\nvoid cfs_bandwidth_usage_inc(void)\n{\n\tstatic_key_slow_inc_cpuslocked(&__cfs_bandwidth_used);\n}\n\nvoid cfs_bandwidth_usage_dec(void)\n{\n\tstatic_key_slow_dec_cpuslocked(&__cfs_bandwidth_used);\n}\n#else /* HAVE_JUMP_LABEL */\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}\n\nvoid cfs_bandwidth_usage_inc(void) {}\nvoid cfs_bandwidth_usage_dec(void) {}\n#endif /* HAVE_JUMP_LABEL */\n\n/*\n * default period for cfs group bandwidth.\n * default: 0.1s, units: nanoseconds\n */\nstatic inline u64 default_cfs_period(void)\n{\n\treturn 100000000ULL;\n}\n\nstatic inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}\n\n/*\n * Replenish runtime according to assigned quota and update expiration time.\n * We use sched_clock_cpu directly instead of rq->clock to avoid adding\n * additional synchronization around rq->lock.\n *\n * requires cfs_b->lock\n */\nvoid __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tu64 now;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\treturn;\n\n\tnow = sched_clock_cpu(smp_processor_id());\n\tcfs_b->runtime = cfs_b->quota;\n\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n}\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn &tg->cfs_bandwidth;\n}\n\n/* rq->task_clock normalized against any time this cfs_rq has spent throttled */\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\tif (unlikely(cfs_rq->throttle_count))\n\t\treturn cfs_rq->throttled_clock_task - cfs_rq->throttled_clock_task_time;\n\n\treturn rq_clock_task(rq_of(cfs_rq)) - cfs_rq->throttled_clock_task_time;\n}\n\n/* returns 0 on failure to allocate runtime */\nstatic int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct task_group *tg = cfs_rq->tg;\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n\tu64 amount = 0, min_amount, expires;\n\tint expires_seq;\n\n\t/* note: this is a positive sum as runtime_remaining <= 0 */\n\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\texpires_seq = cfs_b->expires_seq;\n\texpires = cfs_b->runtime_expires;\n\traw_spin_unlock(&cfs_b->lock);\n\n\tcfs_rq->runtime_remaining += amount;\n\t/*\n\t * we may have advanced our local expiration to account for allowed\n\t * spread between our sched_clock and the one on which runtime was\n\t * issued.\n\t */\n\tif (cfs_rq->expires_seq != expires_seq) {\n\t\tcfs_rq->expires_seq = expires_seq;\n\t\tcfs_rq->runtime_expires = expires;\n\t}\n\n\treturn cfs_rq->runtime_remaining > 0;\n}\n\n/*\n * Note: This depends on the synchronization provided by sched_clock and the\n * fact that rq->clock snapshots this value.\n */\nstatic void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\n\t/* if the deadline is ahead of our clock, nothing to do */\n\tif (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))\n\t\treturn;\n\n\tif (cfs_rq->runtime_remaining < 0)\n\t\treturn;\n\n\t/*\n\t * If the local deadline has passed we have to consider the\n\t * possibility that our sched_clock is 'fast' and the global deadline\n\t * has not truly expired.\n\t *\n\t * Fortunately we can check determine whether this the case by checking\n\t * whether the global deadline(cfs_b->expires_seq) has advanced.\n\t */\n\tif (cfs_rq->expires_seq == cfs_b->expires_seq) {\n\t\t/* extend local deadline, drift is bounded above by 2 ticks */\n\t\tcfs_rq->runtime_expires += TICK_NSEC;\n\t} else {\n\t\t/* global deadline is ahead, expiration has passed */\n\t\tcfs_rq->runtime_remaining = 0;\n\t}\n}\n\nstatic void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t/* dock delta_exec before expiring quota (as it could span periods) */\n\tcfs_rq->runtime_remaining -= delta_exec;\n\texpire_cfs_rq_runtime(cfs_rq);\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\t/*\n\t * if we're unable to extend our runtime we resched so that the active\n\t * hierarchy can be throttled\n\t */\n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\tif (!cfs_bandwidth_used() || !cfs_rq->runtime_enabled)\n\t\treturn;\n\n\t__account_cfs_rq_runtime(cfs_rq, delta_exec);\n}\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttled;\n}\n\n/* check whether cfs_rq, or any parent, is throttled */\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttle_count;\n}\n\n/*\n * Ensure that neither of the group entities corresponding to src_cpu or\n * dest_cpu are members of a throttled hierarchy when performing group\n * load-balance operations.\n */\nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\tstruct cfs_rq *src_cfs_rq, *dest_cfs_rq;\n\n\tsrc_cfs_rq = tg->cfs_rq[src_cpu];\n\tdest_cfs_rq = tg->cfs_rq[dest_cpu];\n\n\treturn throttled_hierarchy(src_cfs_rq) ||\n\t       throttled_hierarchy(dest_cfs_rq);\n}\n\nstatic int tg_unthrottle_up(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\tcfs_rq->throttle_count--;\n\tif (!cfs_rq->throttle_count) {\n\t\t/* adjust cfs_rq_clock_task() */\n\t\tcfs_rq->throttled_clock_task_time += rq_clock_task(rq) -\n\t\t\t\t\t     cfs_rq->throttled_clock_task;\n\t}\n\n\treturn 0;\n}\n\nstatic int tg_throttle_down(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t/* group is entering throttled state, stop time */\n\tif (!cfs_rq->throttle_count)\n\t\tcfs_rq->throttled_clock_task = rq_clock_task(rq);\n\tcfs_rq->throttle_count++;\n\n\treturn 0;\n}\n\nstatic void throttle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tlong task_delta, dequeue = 1;\n\tbool empty;\n\n\tse = cfs_rq->tg->se[cpu_of(rq_of(cfs_rq))];\n\n\t/* freeze hierarchy runnable averages while throttled */\n\trcu_read_lock();\n\twalk_tg_tree_from(cfs_rq->tg, tg_throttle_down, tg_nop, (void *)rq);\n\trcu_read_unlock();\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se);\n\t\t/* throttled entity or throttle-on-deactivate */\n\t\tif (!se->on_rq)\n\t\t\tbreak;\n\n\t\tif (dequeue)\n\t\t\tdequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP);\n\t\tqcfs_rq->h_nr_running -= task_delta;\n\n\t\tif (qcfs_rq->load.weight)\n\t\t\tdequeue = 0;\n\t}\n\n\tif (!se)\n\t\tsub_nr_running(rq, task_delta);\n\n\tcfs_rq->throttled = 1;\n\tcfs_rq->throttled_clock = rq_clock(rq);\n\traw_spin_lock(&cfs_b->lock);\n\tempty = list_empty(&cfs_b->throttled_cfs_rq);\n\n\t/*\n\t * Add to the _head_ of the list, so that an already-started\n\t * distribute_cfs_runtime will not see us. If disribute_cfs_runtime is\n\t * not running add to the tail so that later runqueues don't get starved.\n\t */\n\tif (cfs_b->distribute_running)\n\t\tlist_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);\n\telse\n\t\tlist_add_tail_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);\n\n\t/*\n\t * If we're the first throttled task, make sure the bandwidth\n\t * timer is running.\n\t */\n\tif (empty)\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\traw_spin_unlock(&cfs_b->lock);\n}\n\nvoid unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}\n\nstatic u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}\n\n/*\n * Responsible for refilling a task_group's bandwidth and unthrottling its\n * cfs_rqs as appropriate. If there has been no activity within the last\n * period the timer is deactivated until scheduling resumes; cfs_b->idle is\n * used to track this state.\n */\nstatic int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)\n{\n\tu64 runtime, runtime_expires;\n\tint throttled;\n\n\t/* no need to continue the timer with no bandwidth constraint */\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t/*\n\t * idle depends on !throttled (for the case of a large deficit), and if\n\t * we're going inactive then everything else can be deferred\n\t */\n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\tif (!throttled) {\n\t\t/* mark as potentially idle for the upcoming period */\n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t/* account preceding periods in which throttling occurred */\n\tcfs_b->nr_throttled += overrun;\n\n\truntime_expires = cfs_b->runtime_expires;\n\n\t/*\n\t * This check is repeated as we are holding onto the new bandwidth while\n\t * we unthrottle. This can potentially race with an unthrottled group\n\t * trying to acquire new bandwidth from the global pool. This can result\n\t * in us over-using our runtime if it is all used during this loop, but\n\t * only by limited amounts in that extreme case.\n\t */\n\twhile (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {\n\t\truntime = cfs_b->runtime;\n\t\tcfs_b->distribute_running = 1;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n\t\truntime = distribute_cfs_runtime(cfs_b, runtime,\n\t\t\t\t\t\t runtime_expires);\n\t\traw_spin_lock(&cfs_b->lock);\n\n\t\tcfs_b->distribute_running = 0;\n\t\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\n\t\tlsub_positive(&cfs_b->runtime, runtime);\n\t}\n\n\t/*\n\t * While we are ensured activity in the period following an\n\t * unthrottle, this also covers the case in which the new bandwidth is\n\t * insufficient to cover the existing bandwidth deficit.  (Forcing the\n\t * timer to remain active while there are any throttled entities.)\n\t */\n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}\n\n/* a cfs_rq won't donate quota below this amount */\nstatic const u64 min_cfs_rq_runtime = 1 * NSEC_PER_MSEC;\n/* minimum remaining period time to redistribute slack quota */\nstatic const u64 min_bandwidth_expiration = 2 * NSEC_PER_MSEC;\n/* how long we wait to gather additional slack before distributing */\nstatic const u64 cfs_bandwidth_slack_period = 5 * NSEC_PER_MSEC;\n\n/*\n * Are we near the end of the current quota period?\n *\n * Requires cfs_b->lock for hrtimer_expires_remaining to be safe against the\n * hrtimer base being cleared by hrtimer_start. In the case of\n * migrate_hrtimers, base is never cleared, so we are fine.\n */\nstatic int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\tu64 remaining;\n\n\t/* if the call-back is running a quota refresh is already occurring */\n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t/* is a quota refresh about to occur? */\n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 min_left = cfs_bandwidth_slack_period + min_bandwidth_expiration;\n\n\t/* if there's a quota refresh soon don't bother with slack */\n\tif (runtime_refresh_within(cfs_b, min_left))\n\t\treturn;\n\n\thrtimer_start(&cfs_b->slack_timer,\n\t\t\tns_to_ktime(cfs_bandwidth_slack_period),\n\t\t\tHRTIMER_MODE_REL);\n}\n\n/* we know any runtime found here is valid as update_curr() precedes return */\nstatic void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF &&\n\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t/* we are under rq->lock, defer unthrottling using a timer */\n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* even if it's not valid for return we don't want to try again */\n\tcfs_rq->runtime_remaining -= slack_runtime;\n}\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!cfs_rq->runtime_enabled || cfs_rq->nr_running)\n\t\treturn;\n\n\t__return_cfs_rq_runtime(cfs_rq);\n}\n\n/*\n * This is done with a timer (instead of inline with bandwidth return) since\n * it's necessary to juggle rq->locks to unthrottle their respective cfs_rqs.\n */\nstatic void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tu64 expires;\n\n\t/* confirm we're still not at a refresh boundary */\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->distribute_running) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\texpires = cfs_b->runtime_expires;\n\tif (runtime)\n\t\tcfs_b->distribute_running = 1;\n\n\traw_spin_unlock(&cfs_b->lock);\n\n\tif (!runtime)\n\t\treturn;\n\n\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (expires == cfs_b->runtime_expires)\n\t\tlsub_positive(&cfs_b->runtime, runtime);\n\tcfs_b->distribute_running = 0;\n\traw_spin_unlock(&cfs_b->lock);\n}\n\n/*\n * When a group wakes up we want to make sure that its quota is not already\n * expired/exceeded, otherwise it may be allowed to steal additional ticks of\n * runtime as update_curr() throttling can not not trigger until it's on-rq.\n */\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\t/* an active group must be handled by the update_curr()->put() path */\n\tif (!cfs_rq->runtime_enabled || cfs_rq->curr)\n\t\treturn;\n\n\t/* ensure the group is not already throttled */\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn;\n\n\t/* update runtime allocation */\n\taccount_cfs_rq_runtime(cfs_rq, 0);\n\tif (cfs_rq->runtime_remaining <= 0)\n\t\tthrottle_cfs_rq(cfs_rq);\n}\n\nstatic void sync_throttle(struct task_group *tg, int cpu)\n{\n\tstruct cfs_rq *pcfs_rq, *cfs_rq;\n\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!tg->parent)\n\t\treturn;\n\n\tcfs_rq = tg->cfs_rq[cpu];\n\tpcfs_rq = tg->parent->cfs_rq[cpu];\n\n\tcfs_rq->throttle_count = pcfs_rq->throttle_count;\n\tcfs_rq->throttled_clock_task = rq_clock_task(cpu_rq(cpu));\n}\n\n/* conditionally throttle active cfs_rq's from put_prev_entity() */\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn false;\n\n\tif (likely(!cfs_rq->runtime_enabled || cfs_rq->runtime_remaining > 0))\n\t\treturn false;\n\n\t/*\n\t * it's possible for a throttled entity to be forced into a running\n\t * state (e.g. set_curr_task), in this case we're finished.\n\t */\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn true;\n\n\tthrottle_cfs_rq(cfs_rq);\n\treturn true;\n}\n\nstatic enum hrtimer_restart sched_cfs_slack_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, slack_timer);\n\n\tdo_sched_cfs_slack_timer(cfs_b);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, period_timer);\n\tint overrun;\n\tint idle = 0;\n\n\traw_spin_lock(&cfs_b->lock);\n\tfor (;;) {\n\t\toverrun = hrtimer_forward_now(timer, cfs_b->period);\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_cfs_period_timer(cfs_b, overrun);\n\t}\n\tif (idle)\n\t\tcfs_b->period_active = 0;\n\traw_spin_unlock(&cfs_b->lock);\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}\n\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\traw_spin_lock_init(&cfs_b->lock);\n\tcfs_b->runtime = 0;\n\tcfs_b->quota = RUNTIME_INF;\n\tcfs_b->period = ns_to_ktime(default_cfs_period());\n\n\tINIT_LIST_HEAD(&cfs_b->throttled_cfs_rq);\n\thrtimer_init(&cfs_b->period_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);\n\tcfs_b->period_timer.function = sched_cfs_period_timer;\n\thrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tcfs_b->slack_timer.function = sched_cfs_slack_timer;\n\tcfs_b->distribute_running = 0;\n}\n\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->runtime_enabled = 0;\n\tINIT_LIST_HEAD(&cfs_rq->throttled_list);\n}\n\nvoid start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}\n\nstatic void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\t/* init_cfs_bandwidth() was not called */\n\tif (!cfs_b->throttled_cfs_rq.next)\n\t\treturn;\n\n\thrtimer_cancel(&cfs_b->period_timer);\n\thrtimer_cancel(&cfs_b->slack_timer);\n}\n\n/*\n * Both these CPU hotplug callbacks race against unregister_fair_sched_group()\n *\n * The race is harmless, since modifying bandwidth settings of unhooked group\n * bits doesn't do much.\n */\n\n/* cpu online calback */\nstatic void __maybe_unused update_runtime_enabled(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_held(&rq->lock);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\traw_spin_lock(&cfs_b->lock);\n\t\tcfs_rq->runtime_enabled = cfs_b->quota != RUNTIME_INF;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t}\n\trcu_read_unlock();\n}\n\n/* cpu offline callback */\nstatic void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_held(&rq->lock);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\tif (!cfs_rq->runtime_enabled)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * clock_task is not advancing so we just need to make sure\n\t\t * there's some valid quota amount\n\t\t */\n\t\tcfs_rq->runtime_remaining = 1;\n\t\t/*\n\t\t * Offline rq is schedulable till CPU is completely disabled\n\t\t * in take_cpu_down(), so we prevent new cfs throttling here.\n\t\t */\n\t\tcfs_rq->runtime_enabled = 0;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\t}\n\trcu_read_unlock();\n}\n\n#else /* CONFIG_CFS_BANDWIDTH */\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}\n\nstatic void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}\nstatic inline void sync_throttle(struct task_group *tg, int cpu) {}\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}\n\nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\treturn 0;\n}\n\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}\n#endif\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}\nstatic inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}\nstatic inline void update_runtime_enabled(struct rq *rq) {}\nstatic inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}\n\n#endif /* CONFIG_CFS_BANDWIDTH */\n\n/**************************************************\n * CFS operations on tasks:\n */\n\n#ifdef CONFIG_SCHED_HRTICK\nstatic void hrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tSCHED_WARN_ON(task_rq(p) != rq);\n\n\tif (rq->cfs.h_nr_running > 1) {\n\t\tu64 slice = sched_slice(cfs_rq, se);\n\t\tu64 ran = se->sum_exec_runtime - se->prev_sum_exec_runtime;\n\t\ts64 delta = slice - ran;\n\n\t\tif (delta < 0) {\n\t\t\tif (rq->curr == p)\n\t\t\t\tresched_curr(rq);\n\t\t\treturn;\n\t\t}\n\t\thrtick_start(rq, delta);\n\t}\n}\n\n/*\n * called from enqueue/dequeue and updates the hrtick when the\n * current task is from our class and nr_running is low enough\n * to matter.\n */\nstatic void hrtick_update(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\n\tif (!hrtick_enabled(rq) || curr->sched_class != &fair_sched_class)\n\t\treturn;\n\n\tif (cfs_rq_of(&curr->se)->nr_running < sched_nr_latency)\n\t\thrtick_start_fair(rq, curr);\n}\n#else /* !CONFIG_SCHED_HRTICK */\nstatic inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void hrtick_update(struct rq *rq)\n{\n}\n#endif\n\n#ifdef CONFIG_SMP\nstatic inline unsigned long cpu_util(int cpu);\nstatic unsigned long capacity_of(int cpu);\n\nstatic inline bool cpu_overutilized(int cpu)\n{\n\treturn (capacity_of(cpu) * 1024) < (cpu_util(cpu) * capacity_margin);\n}\n\nstatic inline void update_overutilized_status(struct rq *rq)\n{\n\tif (!READ_ONCE(rq->rd->overutilized) && cpu_overutilized(rq->cpu))\n\t\tWRITE_ONCE(rq->rd->overutilized, SG_OVERUTILIZED);\n}\n#else\nstatic inline void update_overutilized_status(struct rq *rq) { }\n#endif\n\n/*\n * The enqueue_task method is called before nr_running is\n * increased. Here we update the fair scheduling stats and\n * then put the task into the rbtree:\n */\nstatic void\nenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * The code below (indirectly) updates schedutil which looks at\n\t * the cfs_rq utilization to select a frequency.\n\t * Let's add the task's estimated utilization to the cfs_rq's\n\t * estimated utilization, before we update schedutil.\n\t */\n\tutil_est_enqueue(&rq->cfs, p);\n\n\t/*\n\t * If in_iowait is set, the code below may not trigger any cpufreq\n\t * utilization updates, so do it here explicitly with the IOWAIT flag\n\t * passed.\n\t */\n\tif (p->in_iowait)\n\t\tcpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT);\n\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tbreak;\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tenqueue_entity(cfs_rq, se, flags);\n\n\t\t/*\n\t\t * end evaluation on encountering a throttled cfs_rq\n\t\t *\n\t\t * note: in the case of encountering a throttled cfs_rq we will\n\t\t * post the final h_nr_running increment below.\n\t\t */\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t\tcfs_rq->h_nr_running++;\n\n\t\tflags = ENQUEUE_WAKEUP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_nr_running++;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tupdate_cfs_group(se);\n\t}\n\n\tif (!se) {\n\t\tadd_nr_running(rq, 1);\n\t\t/*\n\t\t * Since new tasks are assigned an initial util_avg equal to\n\t\t * half of the spare capacity of their CPU, tiny tasks have the\n\t\t * ability to cross the overutilized threshold, which will\n\t\t * result in the load balancer ruining all the task placement\n\t\t * done by EAS. As a way to mitigate that effect, do not account\n\t\t * for the first enqueue operation of new tasks during the\n\t\t * overutilized flag detection.\n\t\t *\n\t\t * A better way of solving this problem would be to wait for\n\t\t * the PELT signals of tasks to converge before taking them\n\t\t * into account, but that is not straightforward to implement,\n\t\t * and the following generally works well enough in practice.\n\t\t */\n\t\tif (flags & ENQUEUE_WAKEUP)\n\t\t\tupdate_overutilized_status(rq);\n\n\t}\n\n\thrtick_update(rq);\n}\n\nstatic void set_next_buddy(struct sched_entity *se);\n\n/*\n * The dequeue_task method is called before nr_running is\n * decreased. We remove the task from the rbtree and\n * update the fair scheduling stats:\n */\nstatic void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\tint task_sleep = flags & DEQUEUE_SLEEP;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tdequeue_entity(cfs_rq, se, flags);\n\n\t\t/*\n\t\t * end evaluation on encountering a throttled cfs_rq\n\t\t *\n\t\t * note: in the case of encountering a throttled cfs_rq we will\n\t\t * post the final h_nr_running decrement below.\n\t\t*/\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t\tcfs_rq->h_nr_running--;\n\n\t\t/* Don't dequeue parent if it has other entities besides us */\n\t\tif (cfs_rq->load.weight) {\n\t\t\t/* Avoid re-evaluating load for this entity: */\n\t\t\tse = parent_entity(se);\n\t\t\t/*\n\t\t\t * Bias pick_next to pick a task from this cfs_rq, as\n\t\t\t * p is sleeping when it is within its sched_slice.\n\t\t\t */\n\t\t\tif (task_sleep && se && !throttled_hierarchy(cfs_rq))\n\t\t\t\tset_next_buddy(se);\n\t\t\tbreak;\n\t\t}\n\t\tflags |= DEQUEUE_SLEEP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_nr_running--;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tupdate_cfs_group(se);\n\t}\n\n\tif (!se)\n\t\tsub_nr_running(rq, 1);\n\n\tutil_est_dequeue(&rq->cfs, p, task_sleep);\n\thrtick_update(rq);\n}\n\n#ifdef CONFIG_SMP\n\n/* Working cpumask for: load_balance, load_balance_newidle. */\nDEFINE_PER_CPU(cpumask_var_t, load_balance_mask);\nDEFINE_PER_CPU(cpumask_var_t, select_idle_mask);\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * per rq 'load' arrray crap; XXX kill this.\n */\n\n/*\n * The exact cpuload calculated at every tick would be:\n *\n *   load' = (1 - 1/2^i) * load + (1/2^i) * cur_load\n *\n * If a CPU misses updates for n ticks (as it was idle) and update gets\n * called on the n+1-th tick when CPU may be busy, then we have:\n *\n *   load_n   = (1 - 1/2^i)^n * load_0\n *   load_n+1 = (1 - 1/2^i)   * load_n + (1/2^i) * cur_load\n *\n * decay_load_missed() below does efficient calculation of\n *\n *   load' = (1 - 1/2^i)^n * load\n *\n * Because x^(n+m) := x^n * x^m we can decompose any x^n in power-of-2 factors.\n * This allows us to precompute the above in said factors, thereby allowing the\n * reduction of an arbitrary n in O(log_2 n) steps. (See also\n * fixed_power_int())\n *\n * The calculation is approximated on a 128 point scale.\n */\n#define DEGRADE_SHIFT\t\t7\n\nstatic const u8 degrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};\nstatic const u8 degrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {\n\t{   0,   0,  0,  0,  0,  0, 0, 0 },\n\t{  64,  32,  8,  0,  0,  0, 0, 0 },\n\t{  96,  72, 40, 12,  1,  0, 0, 0 },\n\t{ 112,  98, 75, 43, 15,  1, 0, 0 },\n\t{ 120, 112, 98, 76, 45, 16, 2, 0 }\n};\n\n/*\n * Update cpu_load for any missed ticks, due to tickless idle. The backlog\n * would be when CPU is idle and so we just decay the old load without\n * adding any new load.\n */\nstatic unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}\n\nstatic struct {\n\tcpumask_var_t idle_cpus_mask;\n\tatomic_t nr_cpus;\n\tint has_blocked;\t\t/* Idle CPUS has blocked load */\n\tunsigned long next_balance;     /* in jiffy units */\n\tunsigned long next_blocked;\t/* Next update of blocked load in jiffies */\n} nohz ____cacheline_aligned;\n\n#endif /* CONFIG_NO_HZ_COMMON */\n\n/**\n * __cpu_load_update - update the rq->cpu_load[] statistics\n * @this_rq: The rq to update statistics for\n * @this_load: The current load\n * @pending_updates: The number of missed updates\n *\n * Update rq->cpu_load[] statistics. This function is usually called every\n * scheduler tick (TICK_NSEC).\n *\n * This function computes a decaying average:\n *\n *   load[i]' = (1 - 1/2^i) * load[i] + (1/2^i) * load\n *\n * Because of NOHZ it might not get called on every tick which gives need for\n * the @pending_updates argument.\n *\n *   load[i]_n = (1 - 1/2^i) * load[i]_n-1 + (1/2^i) * load_n-1\n *             = A * load[i]_n-1 + B ; A := (1 - 1/2^i), B := (1/2^i) * load\n *             = A * (A * load[i]_n-2 + B) + B\n *             = A * (A * (A * load[i]_n-3 + B) + B) + B\n *             = A^3 * load[i]_n-3 + (A^2 + A + 1) * B\n *             = A^n * load[i]_0 + (A^(n-1) + A^(n-2) + ... + 1) * B\n *             = A^n * load[i]_0 + ((1 - A^n) / (1 - A)) * B\n *             = (1 - 1/2^i)^n * (load[i]_0 - load) + load\n *\n * In the above we've assumed load_n := load, which is true for NOHZ_FULL as\n * any change in load would have resulted in the tick being turned back on.\n *\n * For regular NOHZ, this reduces to:\n *\n *   load[i]_n = (1 - 1/2^i)^n * load[i]_0\n *\n * see decay_load_misses(). For NOHZ_FULL we get to subtract and add the extra\n * term.\n */\nstatic void cpu_load_update(struct rq *this_rq, unsigned long this_load,\n\t\t\t    unsigned long pending_updates)\n{\n\tunsigned long __maybe_unused tickless_load = this_rq->cpu_load[0];\n\tint i, scale;\n\n\tthis_rq->nr_load_updates++;\n\n\t/* Update our load: */\n\tthis_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */\n\tfor (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n\t\tunsigned long old_load, new_load;\n\n\t\t/* scale is effectively 1 << i now, and >> i divides by scale */\n\n\t\told_load = this_rq->cpu_load[i];\n#ifdef CONFIG_NO_HZ_COMMON\n\t\told_load = decay_load_missed(old_load, pending_updates - 1, i);\n\t\tif (tickless_load) {\n\t\t\told_load -= decay_load_missed(tickless_load, pending_updates - 1, i);\n\t\t\t/*\n\t\t\t * old_load can never be a negative value because a\n\t\t\t * decayed tickless_load cannot be greater than the\n\t\t\t * original tickless_load.\n\t\t\t */\n\t\t\told_load += tickless_load;\n\t\t}\n#endif\n\t\tnew_load = this_load;\n\t\t/*\n\t\t * Round up the averaging division if load is increasing. This\n\t\t * prevents us from getting stuck on 9 if the load is 10, for\n\t\t * example.\n\t\t */\n\t\tif (new_load > old_load)\n\t\t\tnew_load += scale - 1;\n\n\t\tthis_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;\n\t}\n}\n\n/* Used instead of source_load when we know the type == 0 */\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * There is no sane way to deal with nohz on smp when using jiffies because the\n * CPU doing the jiffies update might drift wrt the CPU doing the jiffy reading\n * causing off-by-one errors in observed deltas; {0,2} instead of {1,1}.\n *\n * Therefore we need to avoid the delta approach from the regular tick when\n * possible since that would seriously skew the load calculation. This is why we\n * use cpu_load_update_periodic() for CPUs out of nohz. However we'll rely on\n * jiffies deltas for updates happening while in nohz mode (idle ticks, idle\n * loop exit, nohz_idle_balance, nohz full exit...)\n *\n * This means we might still be one tick off for nohz periods.\n */\n\nstatic void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t unsigned long curr_jiffies,\n\t\t\t\t unsigned long load)\n{\n\tunsigned long pending_updates;\n\n\tpending_updates = curr_jiffies - this_rq->last_load_update_tick;\n\tif (pending_updates) {\n\t\tthis_rq->last_load_update_tick = curr_jiffies;\n\t\t/*\n\t\t * In the regular NOHZ case, we were idle, this means load 0.\n\t\t * In the NOHZ_FULL case, we were non-idle, we should consider\n\t\t * its weighted load.\n\t\t */\n\t\tcpu_load_update(this_rq, load, pending_updates);\n\t}\n}\n\n/*\n * Called from nohz_idle_balance() to update the load ratings before doing the\n * idle balance.\n */\nstatic void cpu_load_update_idle(struct rq *this_rq)\n{\n\t/*\n\t * bail if there's load or we're actually up-to-date.\n\t */\n\tif (weighted_cpuload(this_rq))\n\t\treturn;\n\n\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0);\n}\n\n/*\n * Record CPU load on nohz entry so we know the tickless load to account\n * on nohz exit. cpu_load[0] happens then to be updated more frequently\n * than other cpu_load[idx] but it should be fine as cpu_load readers\n * shouldn't rely into synchronized cpu_load[*] updates.\n */\nvoid cpu_load_update_nohz_start(void)\n{\n\tstruct rq *this_rq = this_rq();\n\n\t/*\n\t * This is all lockless but should be fine. If weighted_cpuload changes\n\t * concurrently we'll exit nohz. And cpu_load write can race with\n\t * cpu_load_update_idle() but both updater would be writing the same.\n\t */\n\tthis_rq->cpu_load[0] = weighted_cpuload(this_rq);\n}\n\n/*\n * Account the tickless load in the end of a nohz frame.\n */\nvoid cpu_load_update_nohz_stop(void)\n{\n\tunsigned long curr_jiffies = READ_ONCE(jiffies);\n\tstruct rq *this_rq = this_rq();\n\tunsigned long load;\n\tstruct rq_flags rf;\n\n\tif (curr_jiffies == this_rq->last_load_update_tick)\n\t\treturn;\n\n\tload = weighted_cpuload(this_rq);\n\trq_lock(this_rq, &rf);\n\tupdate_rq_clock(this_rq);\n\tcpu_load_update_nohz(this_rq, curr_jiffies, load);\n\trq_unlock(this_rq, &rf);\n}\n#else /* !CONFIG_NO_HZ_COMMON */\nstatic inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\nstatic void cpu_load_update_periodic(struct rq *this_rq, unsigned long load)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\t/* See the mess around cpu_load_update_nohz(). */\n\tthis_rq->last_load_update_tick = READ_ONCE(jiffies);\n#endif\n\tcpu_load_update(this_rq, load, 1);\n}\n\n/*\n * Called from scheduler_tick()\n */\nvoid cpu_load_update_active(struct rq *this_rq)\n{\n\tunsigned long load = weighted_cpuload(this_rq);\n\n\tif (tick_nohz_tick_stopped())\n\t\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), load);\n\telse\n\t\tcpu_load_update_periodic(this_rq, load);\n}\n\n/*\n * Return a low guess at the load of a migration-source CPU weighted\n * according to the scheduling class and \"nice\" value.\n *\n * We want to under-estimate the load of migration sources, to\n * balance conservatively.\n */\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}\n\n/*\n * Return a high guess at the load of a migration-target CPU weighted\n * according to the scheduling class and \"nice\" value.\n */\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}\n\nstatic unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}\n\nstatic unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = READ_ONCE(rq->cfs.h_nr_running);\n\tunsigned long load_avg = weighted_cpuload(rq);\n\n\tif (nr_running)\n\t\treturn load_avg / nr_running;\n\n\treturn 0;\n}\n\nstatic void record_wakee(struct task_struct *p)\n{\n\t/*\n\t * Only decay a single time; tasks that have less then 1 wakeup per\n\t * jiffy will not have built up many flips.\n\t */\n\tif (time_after(jiffies, current->wakee_flip_decay_ts + HZ)) {\n\t\tcurrent->wakee_flips >>= 1;\n\t\tcurrent->wakee_flip_decay_ts = jiffies;\n\t}\n\n\tif (current->last_wakee != p) {\n\t\tcurrent->last_wakee = p;\n\t\tcurrent->wakee_flips++;\n\t}\n}\n\n/*\n * Detect M:N waker/wakee relationships via a switching-frequency heuristic.\n *\n * A waker of many should wake a different task than the one last awakened\n * at a frequency roughly N times higher than one of its wakees.\n *\n * In order to determine whether we should let the load spread vs consolidating\n * to shared cache, we look for a minimum 'flip' frequency of llc_size in one\n * partner, and a factor of lls_size higher frequency in the other.\n *\n * With both conditions met, we can be relatively sure that the relationship is\n * non-monogamous, with partner count exceeding socket size.\n *\n * Waker/wakee being client/server, worker/dispatcher, interrupt source or\n * whatever is irrelevant, spread criteria is apparent partner count exceeds\n * socket size.\n */\nstatic int wake_wide(struct task_struct *p)\n{\n\tunsigned int master = current->wakee_flips;\n\tunsigned int slave = p->wakee_flips;\n\tint factor = this_cpu_read(sd_llc_size);\n\n\tif (master < slave)\n\t\tswap(master, slave);\n\tif (slave < factor || master < slave * factor)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * The purpose of wake_affine() is to quickly determine on which CPU we can run\n * soonest. For the purpose of speed we only consider the waking and previous\n * CPU.\n *\n * wake_affine_idle() - only considers 'now', it check if the waking CPU is\n *\t\t\tcache-affine and is (or\twill be) idle.\n *\n * wake_affine_weight() - considers the weight to reflect the average\n *\t\t\t  scheduling latency of the CPUs. This seems to work\n *\t\t\t  for the overloaded case.\n */\nstatic int\nwake_affine_idle(int this_cpu, int prev_cpu, int sync)\n{\n\t/*\n\t * If this_cpu is idle, it implies the wakeup is from interrupt\n\t * context. Only allow the move if cache is shared. Otherwise an\n\t * interrupt intensive workload could force all tasks onto one\n\t * node depending on the IO topology or IRQ affinity settings.\n\t *\n\t * If the prev_cpu is idle and cache affine then avoid a migration.\n\t * There is no guarantee that the cache hot data from an interrupt\n\t * is more important than cache hot data on the prev_cpu and from\n\t * a cpufreq perspective, it's better to have higher utilisation\n\t * on one CPU.\n\t */\n\tif (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))\n\t\treturn available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;\n\n\tif (sync && cpu_rq(this_cpu)->nr_running == 1)\n\t\treturn this_cpu;\n\n\treturn nr_cpumask_bits;\n}\n\nstatic int\nwake_affine_weight(struct sched_domain *sd, struct task_struct *p,\n\t\t   int this_cpu, int prev_cpu, int sync)\n{\n\ts64 this_eff_load, prev_eff_load;\n\tunsigned long task_load;\n\n\tthis_eff_load = target_load(this_cpu, sd->wake_idx);\n\n\tif (sync) {\n\t\tunsigned long current_load = task_h_load(current);\n\n\t\tif (current_load > this_eff_load)\n\t\t\treturn this_cpu;\n\n\t\tthis_eff_load -= current_load;\n\t}\n\n\ttask_load = task_h_load(p);\n\n\tthis_eff_load += task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tthis_eff_load *= 100;\n\tthis_eff_load *= capacity_of(prev_cpu);\n\n\tprev_eff_load = source_load(prev_cpu, sd->wake_idx);\n\tprev_eff_load -= task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;\n\tprev_eff_load *= capacity_of(this_cpu);\n\n\t/*\n\t * If sync, adjust the weight of prev_eff_load such that if\n\t * prev_eff == this_eff that select_idle_sibling() will consider\n\t * stacking the wakee on top of the waker if no other CPU is\n\t * idle.\n\t */\n\tif (sync)\n\t\tprev_eff_load += 1;\n\n\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;\n}\n\nstatic int wake_affine(struct sched_domain *sd, struct task_struct *p,\n\t\t       int this_cpu, int prev_cpu, int sync)\n{\n\tint target = nr_cpumask_bits;\n\n\tif (sched_feat(WA_IDLE))\n\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);\n\n\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)\n\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);\n\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine_attempts);\n\tif (target == nr_cpumask_bits)\n\t\treturn prev_cpu;\n\n\tschedstat_inc(sd->ttwu_move_affine);\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine);\n\treturn target;\n}\n\nstatic unsigned long cpu_util_without(int cpu, struct task_struct *p);\n\nstatic unsigned long capacity_spare_without(int cpu, struct task_struct *p)\n{\n\treturn max_t(long, capacity_of(cpu) - cpu_util_without(cpu, p), 0);\n}\n\n/*\n * find_idlest_group finds and returns the least busy CPU group within the\n * domain.\n *\n * Assumes p is allowed on at least one CPU in sd.\n */\nstatic struct sched_group *\nfind_idlest_group(struct sched_domain *sd, struct task_struct *p,\n\t\t  int this_cpu, int sd_flag)\n{\n\tstruct sched_group *idlest = NULL, *group = sd->groups;\n\tstruct sched_group *most_spare_sg = NULL;\n\tunsigned long min_runnable_load = ULONG_MAX;\n\tunsigned long this_runnable_load = ULONG_MAX;\n\tunsigned long min_avg_load = ULONG_MAX, this_avg_load = ULONG_MAX;\n\tunsigned long most_spare = 0, this_spare = 0;\n\tint load_idx = sd->forkexec_idx;\n\tint imbalance_scale = 100 + (sd->imbalance_pct-100)/2;\n\tunsigned long imbalance = scale_load_down(NICE_0_LOAD) *\n\t\t\t\t(sd->imbalance_pct-100) / 100;\n\n\tif (sd_flag & SD_BALANCE_WAKE)\n\t\tload_idx = sd->wake_idx;\n\n\tdo {\n\t\tunsigned long load, avg_load, runnable_load;\n\t\tunsigned long spare_cap, max_spare_cap;\n\t\tint local_group;\n\t\tint i;\n\n\t\t/* Skip over this group if it has no CPUs allowed */\n\t\tif (!cpumask_intersects(sched_group_span(group),\n\t\t\t\t\t&p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tlocal_group = cpumask_test_cpu(this_cpu,\n\t\t\t\t\t       sched_group_span(group));\n\n\t\t/*\n\t\t * Tally up the load of all CPUs in the group and find\n\t\t * the group containing the CPU with most spare capacity.\n\t\t */\n\t\tavg_load = 0;\n\t\trunnable_load = 0;\n\t\tmax_spare_cap = 0;\n\n\t\tfor_each_cpu(i, sched_group_span(group)) {\n\t\t\t/* Bias balancing toward CPUs of our domain */\n\t\t\tif (local_group)\n\t\t\t\tload = source_load(i, load_idx);\n\t\t\telse\n\t\t\t\tload = target_load(i, load_idx);\n\n\t\t\trunnable_load += load;\n\n\t\t\tavg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);\n\n\t\t\tspare_cap = capacity_spare_without(i, p);\n\n\t\t\tif (spare_cap > max_spare_cap)\n\t\t\t\tmax_spare_cap = spare_cap;\n\t\t}\n\n\t\t/* Adjust by relative CPU capacity of the group */\n\t\tavg_load = (avg_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\t\trunnable_load = (runnable_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\n\t\tif (local_group) {\n\t\t\tthis_runnable_load = runnable_load;\n\t\t\tthis_avg_load = avg_load;\n\t\t\tthis_spare = max_spare_cap;\n\t\t} else {\n\t\t\tif (min_runnable_load > (runnable_load + imbalance)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable load is significantly smaller\n\t\t\t\t * so we can pick this new CPU:\n\t\t\t\t */\n\t\t\t\tmin_runnable_load = runnable_load;\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t} else if ((runnable_load < (min_runnable_load + imbalance)) &&\n\t\t\t\t   (100*min_avg_load > imbalance_scale*avg_load)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable loads are close so take the\n\t\t\t\t * blocked load into account through avg_load:\n\t\t\t\t */\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t}\n\n\t\t\tif (most_spare < max_spare_cap) {\n\t\t\t\tmost_spare = max_spare_cap;\n\t\t\t\tmost_spare_sg = group;\n\t\t\t}\n\t\t}\n\t} while (group = group->next, group != sd->groups);\n\n\t/*\n\t * The cross-over point between using spare capacity or least load\n\t * is too conservative for high utilization tasks on partially\n\t * utilized systems if we require spare_capacity > task_util(p),\n\t * so we allow for some task stuffing by using\n\t * spare_capacity > task_util(p)/2.\n\t *\n\t * Spare capacity can't be used for fork because the utilization has\n\t * not been set yet, we must first select a rq to compute the initial\n\t * utilization.\n\t */\n\tif (sd_flag & SD_BALANCE_FORK)\n\t\tgoto skip_spare;\n\n\tif (this_spare > task_util(p) / 2 &&\n\t    imbalance_scale*this_spare > 100*most_spare)\n\t\treturn NULL;\n\n\tif (most_spare > task_util(p) / 2)\n\t\treturn most_spare_sg;\n\nskip_spare:\n\tif (!idlest)\n\t\treturn NULL;\n\n\t/*\n\t * When comparing groups across NUMA domains, it's possible for the\n\t * local domain to be very lightly loaded relative to the remote\n\t * domains but \"imbalance\" skews the comparison making remote CPUs\n\t * look much more favourable. When considering cross-domain, add\n\t * imbalance to the runnable load on the remote node and consider\n\t * staying local.\n\t */\n\tif ((sd->flags & SD_NUMA) &&\n\t    min_runnable_load + imbalance >= this_runnable_load)\n\t\treturn NULL;\n\n\tif (min_runnable_load > (this_runnable_load + imbalance))\n\t\treturn NULL;\n\n\tif ((this_runnable_load < (min_runnable_load + imbalance)) &&\n\t     (100*this_avg_load < imbalance_scale*min_avg_load))\n\t\treturn NULL;\n\n\treturn idlest;\n}\n\n/*\n * find_idlest_group_cpu - find the idlest CPU among the CPUs in the group.\n */\nstatic int\nfind_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)\n{\n\tunsigned long load, min_load = ULONG_MAX;\n\tunsigned int min_exit_latency = UINT_MAX;\n\tu64 latest_idle_timestamp = 0;\n\tint least_loaded_cpu = this_cpu;\n\tint shallowest_idle_cpu = -1;\n\tint i;\n\n\t/* Check if we have any choice: */\n\tif (group->group_weight == 1)\n\t\treturn cpumask_first(sched_group_span(group));\n\n\t/* Traverse only the allowed CPUs */\n\tfor_each_cpu_and(i, sched_group_span(group), &p->cpus_allowed) {\n\t\tif (available_idle_cpu(i)) {\n\t\t\tstruct rq *rq = cpu_rq(i);\n\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);\n\t\t\tif (idle && idle->exit_latency < min_exit_latency) {\n\t\t\t\t/*\n\t\t\t\t * We give priority to a CPU whose idle state\n\t\t\t\t * has the smallest exit latency irrespective\n\t\t\t\t * of any idle timestamp.\n\t\t\t\t */\n\t\t\t\tmin_exit_latency = idle->exit_latency;\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&\n\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {\n\t\t\t\t/*\n\t\t\t\t * If equal or no active idle state, then\n\t\t\t\t * the most recently idled CPU might have\n\t\t\t\t * a warmer cache.\n\t\t\t\t */\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t}\n\t\t} else if (shallowest_idle_cpu == -1) {\n\t\t\tload = weighted_cpuload(cpu_rq(i));\n\t\t\tif (load < min_load) {\n\t\t\t\tmin_load = load;\n\t\t\t\tleast_loaded_cpu = i;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;\n}\n\nstatic inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p,\n\t\t\t\t  int cpu, int prev_cpu, int sd_flag)\n{\n\tint new_cpu = cpu;\n\n\tif (!cpumask_intersects(sched_domain_span(sd), &p->cpus_allowed))\n\t\treturn prev_cpu;\n\n\t/*\n\t * We need task's util for capacity_spare_without, sync it up to\n\t * prev_cpu's last_update_time.\n\t */\n\tif (!(sd_flag & SD_BALANCE_FORK))\n\t\tsync_entity_load_avg(&p->se);\n\n\twhile (sd) {\n\t\tstruct sched_group *group;\n\t\tstruct sched_domain *tmp;\n\t\tint weight;\n\n\t\tif (!(sd->flags & sd_flag)) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgroup = find_idlest_group(sd, p, cpu, sd_flag);\n\t\tif (!group) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew_cpu = find_idlest_group_cpu(group, p, cpu);\n\t\tif (new_cpu == cpu) {\n\t\t\t/* Now try balancing at a lower domain level of 'cpu': */\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Now try balancing at a lower domain level of 'new_cpu': */\n\t\tcpu = new_cpu;\n\t\tweight = sd->span_weight;\n\t\tsd = NULL;\n\t\tfor_each_domain(cpu, tmp) {\n\t\t\tif (weight <= tmp->span_weight)\n\t\t\t\tbreak;\n\t\t\tif (tmp->flags & sd_flag)\n\t\t\t\tsd = tmp;\n\t\t}\n\t}\n\n\treturn new_cpu;\n}\n\n#ifdef CONFIG_SCHED_SMT\nDEFINE_STATIC_KEY_FALSE(sched_smt_present);\n\nstatic inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}\n\nstatic inline bool test_idle_cores(int cpu, bool def)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn def;\n}\n\n/*\n * Scans the local SMT mask to see if the entire core is idle, and records this\n * information in sd_llc_shared->has_idle_cores.\n *\n * Since SMT siblings share all cache levels, inspecting this limited remote\n * state should be fairly cheap.\n */\nvoid __update_idle_core(struct rq *rq)\n{\n\tint core = cpu_of(rq);\n\tint cpu;\n\n\trcu_read_lock();\n\tif (test_idle_cores(core, true))\n\t\tgoto unlock;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\tif (cpu == core)\n\t\t\tcontinue;\n\n\t\tif (!available_idle_cpu(cpu))\n\t\t\tgoto unlock;\n\t}\n\n\tset_idle_cores(core, 1);\nunlock:\n\trcu_read_unlock();\n}\n\n/*\n * Scan the entire LLC domain for idle cores; this dynamically switches off if\n * there are no idle cores left in the system; tracked through\n * sd_llc->shared->has_idle_cores and enabled through update_idle_core() above.\n */\nstatic int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask);\n\tint core, cpu;\n\n\tif (!static_branch_likely(&sched_smt_present))\n\t\treturn -1;\n\n\tif (!test_idle_cores(target, false))\n\t\treturn -1;\n\n\tcpumask_and(cpus, sched_domain_span(sd), &p->cpus_allowed);\n\n\tfor_each_cpu_wrap(core, cpus, target) {\n\t\tbool idle = true;\n\n\t\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\t\tcpumask_clear_cpu(cpu, cpus);\n\t\t\tif (!available_idle_cpu(cpu))\n\t\t\t\tidle = false;\n\t\t}\n\n\t\tif (idle)\n\t\t\treturn core;\n\t}\n\n\t/*\n\t * Failed to find an idle core; stop looking for one.\n\t */\n\tset_idle_cores(target, 0);\n\n\treturn -1;\n}\n\n/*\n * Scan the local SMT mask for idle CPUs.\n */\nstatic int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tint cpu;\n\n\tif (!static_branch_likely(&sched_smt_present))\n\t\treturn -1;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(target)) {\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\treturn cpu;\n\t}\n\n\treturn -1;\n}\n\n#else /* CONFIG_SCHED_SMT */\n\nstatic inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}\n\nstatic inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}\n\n#endif /* CONFIG_SCHED_SMT */\n\n/*\n * Scan the LLC domain for idle CPUs; this is dynamically regulated by\n * comparing the average scan cost (tracked in sd->avg_scan_cost) against the\n * average idle time for this rq (as found in rq->avg_idle).\n */\nstatic int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct sched_domain *this_sd;\n\tu64 avg_cost, avg_idle;\n\tu64 time, cost;\n\ts64 delta;\n\tint cpu, nr = INT_MAX;\n\n\tthis_sd = rcu_dereference(*this_cpu_ptr(&sd_llc));\n\tif (!this_sd)\n\t\treturn -1;\n\n\t/*\n\t * Due to large variance we need a large fuzz factor; hackbench in\n\t * particularly is sensitive here.\n\t */\n\tavg_idle = this_rq()->avg_idle / 512;\n\tavg_cost = this_sd->avg_scan_cost + 1;\n\n\tif (sched_feat(SIS_AVG_CPU) && avg_idle < avg_cost)\n\t\treturn -1;\n\n\tif (sched_feat(SIS_PROP)) {\n\t\tu64 span_avg = sd->span_weight * avg_idle;\n\t\tif (span_avg > 4*avg_cost)\n\t\t\tnr = div_u64(span_avg, avg_cost);\n\t\telse\n\t\t\tnr = 4;\n\t}\n\n\ttime = local_clock();\n\n\tfor_each_cpu_wrap(cpu, sched_domain_span(sd), target) {\n\t\tif (!--nr)\n\t\t\treturn -1;\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\tbreak;\n\t}\n\n\ttime = local_clock() - time;\n\tcost = this_sd->avg_scan_cost;\n\tdelta = (s64)(time - cost) / 8;\n\tthis_sd->avg_scan_cost += delta;\n\n\treturn cpu;\n}\n\n/*\n * Try and locate an idle core/thread in the LLC cache domain.\n */\nstatic int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tstruct sched_domain *sd;\n\tint i, recent_used_cpu;\n\n\tif (available_idle_cpu(target))\n\t\treturn target;\n\n\t/*\n\t * If the previous CPU is cache affine and idle, don't be stupid:\n\t */\n\tif (prev != target && cpus_share_cache(prev, target) && available_idle_cpu(prev))\n\t\treturn prev;\n\n\t/* Check a recently used CPU as a potential idle candidate: */\n\trecent_used_cpu = p->recent_used_cpu;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    available_idle_cpu(recent_used_cpu) &&\n\t    cpumask_test_cpu(p->recent_used_cpu, &p->cpus_allowed)) {\n\t\t/*\n\t\t * Replace recent_used_cpu with prev as it is a potential\n\t\t * candidate for the next wake:\n\t\t */\n\t\tp->recent_used_cpu = prev;\n\t\treturn recent_used_cpu;\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\ti = select_idle_core(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_cpu(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_smt(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}\n\n/**\n * Amount of capacity of a CPU that is (estimated to be) used by CFS tasks\n * @cpu: the CPU to get the utilization of\n *\n * The unit of the return value must be the one of capacity so we can compare\n * the utilization with the capacity of the CPU that is available for CFS task\n * (ie cpu_capacity).\n *\n * cfs_rq.avg.util_avg is the sum of running time of runnable tasks plus the\n * recent utilization of currently non-runnable tasks on a CPU. It represents\n * the amount of utilization of a CPU in the range [0..capacity_orig] where\n * capacity_orig is the cpu_capacity available at the highest frequency\n * (arch_scale_freq_capacity()).\n * The utilization of a CPU converges towards a sum equal to or less than the\n * current capacity (capacity_curr <= capacity_orig) of the CPU because it is\n * the running time on this CPU scaled by capacity_curr.\n *\n * The estimated utilization of a CPU is defined to be the maximum between its\n * cfs_rq.avg.util_avg and the sum of the estimated utilization of the tasks\n * currently RUNNABLE on that CPU.\n * This allows to properly represent the expected utilization of a CPU which\n * has just got a big task running since a long sleep period. At the same time\n * however it preserves the benefits of the \"blocked utilization\" in\n * describing the potential for other tasks waking up on the same CPU.\n *\n * Nevertheless, cfs_rq.avg.util_avg can be higher than capacity_curr or even\n * higher than capacity_orig because of unfortunate rounding in\n * cfs.avg.util_avg or just after migrating tasks and new task wakeups until\n * the average stabilizes with the new running time. We need to check that the\n * utilization stays within the range of [0..capacity_orig] and cap it if\n * necessary. Without utilization capping, a group could be seen as overloaded\n * (CPU0 utilization at 121% + CPU1 utilization at 80%) whereas CPU1 has 20% of\n * available capacity. We allow utilization to overshoot capacity_curr (but not\n * capacity_orig) as it useful for predicting the capacity required after task\n * migrations (scheduler-driven DVFS).\n *\n * Return: the (estimated) utilization for the specified CPU\n */\nstatic inline unsigned long cpu_util(int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}\n\n/*\n * cpu_util_without: compute cpu utilization without any contributions from *p\n * @cpu: the CPU which utilization is requested\n * @p: the task which utilization should be discounted\n *\n * The utilization of a CPU is defined by the utilization of tasks currently\n * enqueued on that CPU as well as tasks which are currently sleeping after an\n * execution on that CPU.\n *\n * This method returns the utilization of the specified CPU by discounting the\n * utilization of the specified task, whenever the task is currently\n * contributing to the CPU utilization.\n */\nstatic unsigned long cpu_util_without(int cpu, struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\t/* Task has no contribution or is new */\n\tif (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn cpu_util(cpu);\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\t/* Discount task's util from CPU's util */\n\tlsub_positive(&util, task_util(p));\n\n\t/*\n\t * Covered cases:\n\t *\n\t * a) if *p is the only task sleeping on this CPU, then:\n\t *      cpu_util (== task_util) > util_est (== 0)\n\t *    and thus we return:\n\t *      cpu_util_without = (cpu_util - task_util) = 0\n\t *\n\t * b) if other tasks are SLEEPING on this CPU, which is now exiting\n\t *    IDLE, then:\n\t *      cpu_util >= task_util\n\t *      cpu_util > util_est (== 0)\n\t *    and thus we discount *p's blocked utilization to return:\n\t *      cpu_util_without = (cpu_util - task_util) >= 0\n\t *\n\t * c) if other tasks are RUNNABLE on that CPU and\n\t *      util_est > cpu_util\n\t *    then we use util_est since it returns a more restrictive\n\t *    estimation of the spare capacity on that CPU, by just\n\t *    considering the expected utilization of tasks already\n\t *    runnable on that CPU.\n\t *\n\t * Cases a) and b) are covered by the above code, while case c) is\n\t * covered by the following code when estimated utilization is\n\t * enabled.\n\t */\n\tif (sched_feat(UTIL_EST)) {\n\t\tunsigned int estimated =\n\t\t\tREAD_ONCE(cfs_rq->avg.util_est.enqueued);\n\n\t\t/*\n\t\t * Despite the following checks we still have a small window\n\t\t * for a possible race, when an execl's select_task_rq_fair()\n\t\t * races with LB's detach_task():\n\t\t *\n\t\t *   detach_task()\n\t\t *     p->on_rq = TASK_ON_RQ_MIGRATING;\n\t\t *     ---------------------------------- A\n\t\t *     deactivate_task()                   \\\n\t\t *       dequeue_task()                     + RaceTime\n\t\t *         util_est_dequeue()              /\n\t\t *     ---------------------------------- B\n\t\t *\n\t\t * The additional check on \"current == p\" it's required to\n\t\t * properly fix the execl regression and it helps in further\n\t\t * reducing the chances for the above race.\n\t\t */\n\t\tif (unlikely(task_on_rq_queued(p) || current == p))\n\t\t\tlsub_positive(&estimated, _task_util_est(p));\n\n\t\tutil = max(util, estimated);\n\t}\n\n\t/*\n\t * Utilization (estimated) can exceed the CPU capacity, thus let's\n\t * clamp to the maximum CPU capacity to ensure consistency with\n\t * the cpu_util call.\n\t */\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}\n\n/*\n * Disable WAKE_AFFINE in the case where task @p doesn't fit in the\n * capacity of either the waking CPU @cpu or the previous CPU @prev_cpu.\n *\n * In that case WAKE_AFFINE doesn't make sense and we'll let\n * BALANCE_WAKE sort things out.\n */\nstatic int wake_cap(struct task_struct *p, int cpu, int prev_cpu)\n{\n\tlong min_cap, max_cap;\n\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn 0;\n\n\tmin_cap = min(capacity_orig_of(prev_cpu), capacity_orig_of(cpu));\n\tmax_cap = cpu_rq(cpu)->rd->max_cpu_capacity;\n\n\t/* Minimum capacity is close to max, no need to abort wake_affine */\n\tif (max_cap - min_cap < max_cap >> 3)\n\t\treturn 0;\n\n\t/* Bring task utilization in sync with prev_cpu */\n\tsync_entity_load_avg(&p->se);\n\n\treturn !task_fits_capacity(p, min_cap);\n}\n\n/*\n * Predicts what cpu_util(@cpu) would return if @p was migrated (and enqueued)\n * to @dst_cpu.\n */\nstatic unsigned long cpu_util_next(int cpu, struct task_struct *p, int dst_cpu)\n{\n\tstruct cfs_rq *cfs_rq = &cpu_rq(cpu)->cfs;\n\tunsigned long util_est, util = READ_ONCE(cfs_rq->avg.util_avg);\n\n\t/*\n\t * If @p migrates from @cpu to another, remove its contribution. Or,\n\t * if @p migrates from another CPU to @cpu, add its contribution. In\n\t * the other cases, @cpu is not impacted by the migration, so the\n\t * util_avg should already be correct.\n\t */\n\tif (task_cpu(p) == cpu && dst_cpu != cpu)\n\t\tsub_positive(&util, task_util(p));\n\telse if (task_cpu(p) != cpu && dst_cpu == cpu)\n\t\tutil += task_util(p);\n\n\tif (sched_feat(UTIL_EST)) {\n\t\tutil_est = READ_ONCE(cfs_rq->avg.util_est.enqueued);\n\n\t\t/*\n\t\t * During wake-up, the task isn't enqueued yet and doesn't\n\t\t * appear in the cfs_rq->avg.util_est.enqueued of any rq,\n\t\t * so just add it (if needed) to \"simulate\" what will be\n\t\t * cpu_util() after the task has been enqueued.\n\t\t */\n\t\tif (dst_cpu == cpu)\n\t\t\tutil_est += _task_util_est(p);\n\n\t\tutil = max(util, util_est);\n\t}\n\n\treturn min(util, capacity_orig_of(cpu));\n}\n\n/*\n * compute_energy(): Estimates the energy that would be consumed if @p was\n * migrated to @dst_cpu. compute_energy() predicts what will be the utilization\n * landscape of the * CPUs after the task migration, and uses the Energy Model\n * to compute what would be the energy if we decided to actually migrate that\n * task.\n */\nstatic long\ncompute_energy(struct task_struct *p, int dst_cpu, struct perf_domain *pd)\n{\n\tlong util, max_util, sum_util, energy = 0;\n\tint cpu;\n\n\tfor (; pd; pd = pd->next) {\n\t\tmax_util = sum_util = 0;\n\t\t/*\n\t\t * The capacity state of CPUs of the current rd can be driven by\n\t\t * CPUs of another rd if they belong to the same performance\n\t\t * domain. So, account for the utilization of these CPUs too\n\t\t * by masking pd with cpu_online_mask instead of the rd span.\n\t\t *\n\t\t * If an entire performance domain is outside of the current rd,\n\t\t * it will not appear in its pd list and will not be accounted\n\t\t * by compute_energy().\n\t\t */\n\t\tfor_each_cpu_and(cpu, perf_domain_span(pd), cpu_online_mask) {\n\t\t\tutil = cpu_util_next(cpu, p, dst_cpu);\n\t\t\tutil = schedutil_energy_util(cpu, util);\n\t\t\tmax_util = max(util, max_util);\n\t\t\tsum_util += util;\n\t\t}\n\n\t\tenergy += em_pd_energy(pd->em_pd, max_util, sum_util);\n\t}\n\n\treturn energy;\n}\n\n/*\n * find_energy_efficient_cpu(): Find most energy-efficient target CPU for the\n * waking task. find_energy_efficient_cpu() looks for the CPU with maximum\n * spare capacity in each performance domain and uses it as a potential\n * candidate to execute the task. Then, it uses the Energy Model to figure\n * out which of the CPU candidates is the most energy-efficient.\n *\n * The rationale for this heuristic is as follows. In a performance domain,\n * all the most energy efficient CPU candidates (according to the Energy\n * Model) are those for which we'll request a low frequency. When there are\n * several CPUs for which the frequency request will be the same, we don't\n * have enough data to break the tie between them, because the Energy Model\n * only includes active power costs. With this model, if we assume that\n * frequency requests follow utilization (e.g. using schedutil), the CPU with\n * the maximum spare capacity in a performance domain is guaranteed to be among\n * the best candidates of the performance domain.\n *\n * In practice, it could be preferable from an energy standpoint to pack\n * small tasks on a CPU in order to let other CPUs go in deeper idle states,\n * but that could also hurt our chances to go cluster idle, and we have no\n * ways to tell with the current Energy Model if this is actually a good\n * idea or not. So, find_energy_efficient_cpu() basically favors\n * cluster-packing, and spreading inside a cluster. That should at least be\n * a good thing for latency, and this is consistent with the idea that most\n * of the energy savings of EAS come from the asymmetry of the system, and\n * not so much from breaking the tie between identical CPUs. That's also the\n * reason why EAS is enabled in the topology code only for systems where\n * SD_ASYM_CPUCAPACITY is set.\n *\n * NOTE: Forkees are not accepted in the energy-aware wake-up path because\n * they don't have any useful utilization data yet and it's not possible to\n * forecast their impact on energy consumption. Consequently, they will be\n * placed by find_idlest_cpu() on the least loaded CPU, which might turn out\n * to be energy-inefficient in some use-cases. The alternative would be to\n * bias new tasks towards specific types of CPUs first, or to try to infer\n * their util_avg from the parent task, but those heuristics could hurt\n * other use-cases too. So, until someone finds a better way to solve this,\n * let's keep things simple by re-using the existing slow path.\n */\n\nstatic int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)\n{\n\tunsigned long prev_energy = ULONG_MAX, best_energy = ULONG_MAX;\n\tstruct root_domain *rd = cpu_rq(smp_processor_id())->rd;\n\tint cpu, best_energy_cpu = prev_cpu;\n\tstruct perf_domain *head, *pd;\n\tunsigned long cpu_cap, util;\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tpd = rcu_dereference(rd->pd);\n\tif (!pd || READ_ONCE(rd->overutilized))\n\t\tgoto fail;\n\thead = pd;\n\n\t/*\n\t * Energy-aware wake-up happens on the lowest sched_domain starting\n\t * from sd_asym_cpucapacity spanning over this_cpu and prev_cpu.\n\t */\n\tsd = rcu_dereference(*this_cpu_ptr(&sd_asym_cpucapacity));\n\twhile (sd && !cpumask_test_cpu(prev_cpu, sched_domain_span(sd)))\n\t\tsd = sd->parent;\n\tif (!sd)\n\t\tgoto fail;\n\n\tsync_entity_load_avg(&p->se);\n\tif (!task_util_est(p))\n\t\tgoto unlock;\n\n\tfor (; pd; pd = pd->next) {\n\t\tunsigned long cur_energy, spare_cap, max_spare_cap = 0;\n\t\tint max_spare_cap_cpu = -1;\n\n\t\tfor_each_cpu_and(cpu, perf_domain_span(pd), sched_domain_span(sd)) {\n\t\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\t\tcontinue;\n\n\t\t\t/* Skip CPUs that will be overutilized. */\n\t\t\tutil = cpu_util_next(cpu, p, cpu);\n\t\t\tcpu_cap = capacity_of(cpu);\n\t\t\tif (cpu_cap * 1024 < util * capacity_margin)\n\t\t\t\tcontinue;\n\n\t\t\t/* Always use prev_cpu as a candidate. */\n\t\t\tif (cpu == prev_cpu) {\n\t\t\t\tprev_energy = compute_energy(p, prev_cpu, head);\n\t\t\t\tbest_energy = min(best_energy, prev_energy);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Find the CPU with the maximum spare capacity in\n\t\t\t * the performance domain\n\t\t\t */\n\t\t\tspare_cap = cpu_cap - util;\n\t\t\tif (spare_cap > max_spare_cap) {\n\t\t\t\tmax_spare_cap = spare_cap;\n\t\t\t\tmax_spare_cap_cpu = cpu;\n\t\t\t}\n\t\t}\n\n\t\t/* Evaluate the energy impact of using this CPU. */\n\t\tif (max_spare_cap_cpu >= 0) {\n\t\t\tcur_energy = compute_energy(p, max_spare_cap_cpu, head);\n\t\t\tif (cur_energy < best_energy) {\n\t\t\t\tbest_energy = cur_energy;\n\t\t\t\tbest_energy_cpu = max_spare_cap_cpu;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\n\n\t/*\n\t * Pick the best CPU if prev_cpu cannot be used, or if it saves at\n\t * least 6% of the energy used by prev_cpu.\n\t */\n\tif (prev_energy == ULONG_MAX)\n\t\treturn best_energy_cpu;\n\n\tif ((prev_energy - best_energy) > (prev_energy >> 4))\n\t\treturn best_energy_cpu;\n\n\treturn prev_cpu;\n\nfail:\n\trcu_read_unlock();\n\n\treturn -1;\n}\n\n/*\n * select_task_rq_fair: Select target runqueue for the waking task in domains\n * that have the 'sd_flag' flag set. In practice, this is SD_BALANCE_WAKE,\n * SD_BALANCE_FORK, or SD_BALANCE_EXEC.\n *\n * Balances load by selecting the idlest CPU in the idlest group, or under\n * certain conditions an idle sibling CPU if the domain has SD_WAKE_AFFINE set.\n *\n * Returns the target CPU number.\n *\n * preempt must be disabled.\n */\nstatic int\nselect_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)\n{\n\tstruct sched_domain *tmp, *sd = NULL;\n\tint cpu = smp_processor_id();\n\tint new_cpu = prev_cpu;\n\tint want_affine = 0;\n\tint sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);\n\n\tif (sd_flag & SD_BALANCE_WAKE) {\n\t\trecord_wakee(p);\n\n\t\tif (static_branch_unlikely(&sched_energy_present)) {\n\t\t\tnew_cpu = find_energy_efficient_cpu(p, prev_cpu);\n\t\t\tif (new_cpu >= 0)\n\t\t\t\treturn new_cpu;\n\t\t\tnew_cpu = prev_cpu;\n\t\t}\n\n\t\twant_affine = !wake_wide(p) && !wake_cap(p, cpu, prev_cpu) &&\n\t\t\t      cpumask_test_cpu(cpu, &p->cpus_allowed);\n\t}\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, tmp) {\n\t\tif (!(tmp->flags & SD_LOAD_BALANCE))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If both 'cpu' and 'prev_cpu' are part of this domain,\n\t\t * cpu is a valid SD_WAKE_AFFINE target.\n\t\t */\n\t\tif (want_affine && (tmp->flags & SD_WAKE_AFFINE) &&\n\t\t    cpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) {\n\t\t\tif (cpu != prev_cpu)\n\t\t\t\tnew_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync);\n\n\t\t\tsd = NULL; /* Prefer wake_affine over balance flags */\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tmp->flags & sd_flag)\n\t\t\tsd = tmp;\n\t\telse if (!want_affine)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(sd)) {\n\t\t/* Slow path */\n\t\tnew_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag);\n\t} else if (sd_flag & SD_BALANCE_WAKE) { /* XXX always ? */\n\t\t/* Fast path */\n\n\t\tnew_cpu = select_idle_sibling(p, prev_cpu, new_cpu);\n\n\t\tif (want_affine)\n\t\t\tcurrent->recent_used_cpu = cpu;\n\t}\n\trcu_read_unlock();\n\n\treturn new_cpu;\n}\n\nstatic void detach_entity_cfs_rq(struct sched_entity *se);\n\n/*\n * Called immediately before a task is migrated to a new CPU; task_cpu(p) and\n * cfs_rq_of(p) references at time of call are still valid and identify the\n * previous CPU. The caller guarantees p->pi_lock or task_rq(p)->lock is held.\n */\nstatic void migrate_task_rq_fair(struct task_struct *p, int new_cpu)\n{\n\t/*\n\t * As blocked tasks retain absolute vruntime the migration needs to\n\t * deal with this by subtracting the old and adding the new\n\t * min_vruntime -- the latter is done by enqueue_entity() when placing\n\t * the task on the new runqueue.\n\t */\n\tif (p->state == TASK_WAKING) {\n\t\tstruct sched_entity *se = &p->se;\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tu64 min_vruntime;\n\n#ifndef CONFIG_64BIT\n\t\tu64 min_vruntime_copy;\n\n\t\tdo {\n\t\t\tmin_vruntime_copy = cfs_rq->min_vruntime_copy;\n\t\t\tsmp_rmb();\n\t\t\tmin_vruntime = cfs_rq->min_vruntime;\n\t\t} while (min_vruntime != min_vruntime_copy);\n#else\n\t\tmin_vruntime = cfs_rq->min_vruntime;\n#endif\n\n\t\tse->vruntime -= min_vruntime;\n\t}\n\n\tif (p->on_rq == TASK_ON_RQ_MIGRATING) {\n\t\t/*\n\t\t * In case of TASK_ON_RQ_MIGRATING we in fact hold the 'old'\n\t\t * rq->lock and can modify state directly.\n\t\t */\n\t\tlockdep_assert_held(&task_rq(p)->lock);\n\t\tdetach_entity_cfs_rq(&p->se);\n\n\t} else {\n\t\t/*\n\t\t * We are supposed to update the task to \"current\" time, then\n\t\t * its up to date and ready to go to new CPU/cfs_rq. But we\n\t\t * have difficulty in getting what current time is, so simply\n\t\t * throw away the out-of-date time. This will result in the\n\t\t * wakee task is less decayed, but giving the wakee more load\n\t\t * sounds not bad.\n\t\t */\n\t\tremove_entity_load_avg(&p->se);\n\t}\n\n\t/* Tell new CPU we are migrated */\n\tp->se.avg.last_update_time = 0;\n\n\t/* We have migrated, no longer consider this task hot */\n\tp->se.exec_start = 0;\n\n\tupdate_scan_period(p, new_cpu);\n}\n\nstatic void task_dead_fair(struct task_struct *p)\n{\n\tremove_entity_load_avg(&p->se);\n}\n#endif /* CONFIG_SMP */\n\nstatic unsigned long wakeup_gran(struct sched_entity *se)\n{\n\tunsigned long gran = sysctl_sched_wakeup_granularity;\n\n\t/*\n\t * Since its curr running now, convert the gran from real-time\n\t * to virtual-time in his units.\n\t *\n\t * By using 'se' instead of 'curr' we penalize light tasks, so\n\t * they get preempted easier. That is, if 'se' < 'curr' then\n\t * the resulting gran will be larger, therefore penalizing the\n\t * lighter, if otoh 'se' > 'curr' then the resulting gran will\n\t * be smaller, again penalizing the lighter task.\n\t *\n\t * This is especially important for buddies when the leftmost\n\t * task is higher priority than the buddy.\n\t */\n\treturn calc_delta_fair(gran, se);\n}\n\n/*\n * Should 'se' preempt 'curr'.\n *\n *             |s1\n *        |s2\n *   |s3\n *         g\n *      |<--->|c\n *\n *  w(c, s1) = -1\n *  w(c, s2) =  0\n *  w(c, s3) =  1\n *\n */\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)\n{\n\ts64 gran, vdiff = curr->vruntime - se->vruntime;\n\n\tif (vdiff <= 0)\n\t\treturn -1;\n\n\tgran = wakeup_gran(se);\n\tif (vdiff > gran)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void set_last_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_has_idle_policy(task_of(se))))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->last = se;\n\t}\n}\n\nstatic void set_next_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_has_idle_policy(task_of(se))))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}\n\nstatic void set_skip_buddy(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se)\n\t\tcfs_rq_of(se)->skip = se;\n}\n\n/*\n * Preempt the current task with a newly woken task if needed:\n */\nstatic void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct sched_entity *se = &curr->se, *pse = &p->se;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tint scale = cfs_rq->nr_running >= sched_nr_latency;\n\tint next_buddy_marked = 0;\n\n\tif (unlikely(se == pse))\n\t\treturn;\n\n\t/*\n\t * This is possible from callers such as attach_tasks(), in which we\n\t * unconditionally check_prempt_curr() after an enqueue (which may have\n\t * lead to a throttle).  This both saves work and prevents false\n\t * next-buddy nomination below.\n\t */\n\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))\n\t\treturn;\n\n\tif (sched_feat(NEXT_BUDDY) && scale && !(wake_flags & WF_FORK)) {\n\t\tset_next_buddy(pse);\n\t\tnext_buddy_marked = 1;\n\t}\n\n\t/*\n\t * We can come here with TIF_NEED_RESCHED already set from new task\n\t * wake up path.\n\t *\n\t * Note: this also catches the edge-case of curr being in a throttled\n\t * group (e.g. via set_curr_task), since update_curr() (in the\n\t * enqueue of curr) will have resulted in resched being set.  This\n\t * prevents us from potentially nominating it as a false LAST_BUDDY\n\t * below.\n\t */\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\t/* Idle tasks are by definition preempted by non-idle tasks. */\n\tif (unlikely(task_has_idle_policy(curr)) &&\n\t    likely(!task_has_idle_policy(p)))\n\t\tgoto preempt;\n\n\t/*\n\t * Batch and idle tasks do not preempt non-idle tasks (their preemption\n\t * is driven by the tick):\n\t */\n\tif (unlikely(p->policy != SCHED_NORMAL) || !sched_feat(WAKEUP_PREEMPTION))\n\t\treturn;\n\n\tfind_matching_se(&se, &pse);\n\tupdate_curr(cfs_rq_of(se));\n\tBUG_ON(!pse);\n\tif (wakeup_preempt_entity(se, pse) == 1) {\n\t\t/*\n\t\t * Bias pick_next to pick the sched entity that is\n\t\t * triggering this preemption.\n\t\t */\n\t\tif (!next_buddy_marked)\n\t\t\tset_next_buddy(pse);\n\t\tgoto preempt;\n\t}\n\n\treturn;\n\npreempt:\n\tresched_curr(rq);\n\t/*\n\t * Only set the backward buddy when the current task is still\n\t * on the rq. This can happen when a wakeup gets interleaved\n\t * with schedule on the ->pre_schedule() or idle_balance()\n\t * point, either of which can * drop the rq lock.\n\t *\n\t * Also, during early boot the idle thread is in the fair class,\n\t * for obvious reasons its a bad idea to schedule back to it.\n\t */\n\tif (unlikely(!se->on_rq || curr == rq->idle))\n\t\treturn;\n\n\tif (sched_feat(LAST_BUDDY) && scale && entity_is_task(se))\n\t\tset_last_buddy(se);\n}\n\nstatic struct task_struct *\npick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tstruct sched_entity *se;\n\tstruct task_struct *p;\n\tint new_tasks;\n\nagain:\n\tif (!cfs_rq->nr_running)\n\t\tgoto idle;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (prev->sched_class != &fair_sched_class)\n\t\tgoto simple;\n\n\t/*\n\t * Because of the set_next_buddy() in dequeue_task_fair() it is rather\n\t * likely that a next task is from the same cgroup as the current.\n\t *\n\t * Therefore attempt to avoid putting and setting the entire cgroup\n\t * hierarchy, only change the part that actually changes.\n\t */\n\n\tdo {\n\t\tstruct sched_entity *curr = cfs_rq->curr;\n\n\t\t/*\n\t\t * Since we got here without doing put_prev_entity() we also\n\t\t * have to consider cfs_rq->curr. If it is still a runnable\n\t\t * entity, update_curr() will update its vruntime, otherwise\n\t\t * forget we've ever seen it.\n\t\t */\n\t\tif (curr) {\n\t\t\tif (curr->on_rq)\n\t\t\t\tupdate_curr(cfs_rq);\n\t\t\telse\n\t\t\t\tcurr = NULL;\n\n\t\t\t/*\n\t\t\t * This call to check_cfs_rq_runtime() will do the\n\t\t\t * throttle and dequeue its entity in the parent(s).\n\t\t\t * Therefore the nr_running test will indeed\n\t\t\t * be correct.\n\t\t\t */\n\t\t\tif (unlikely(check_cfs_rq_runtime(cfs_rq))) {\n\t\t\t\tcfs_rq = &rq->cfs;\n\n\t\t\t\tif (!cfs_rq->nr_running)\n\t\t\t\t\tgoto idle;\n\n\t\t\t\tgoto simple;\n\t\t\t}\n\t\t}\n\n\t\tse = pick_next_entity(cfs_rq, curr);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\n\t/*\n\t * Since we haven't yet done put_prev_entity and if the selected task\n\t * is a different task than we started out with, try and touch the\n\t * least amount of cfs_rqs.\n\t */\n\tif (prev != p) {\n\t\tstruct sched_entity *pse = &prev->se;\n\n\t\twhile (!(cfs_rq = is_same_group(se, pse))) {\n\t\t\tint se_depth = se->depth;\n\t\t\tint pse_depth = pse->depth;\n\n\t\t\tif (se_depth <= pse_depth) {\n\t\t\t\tput_prev_entity(cfs_rq_of(pse), pse);\n\t\t\t\tpse = parent_entity(pse);\n\t\t\t}\n\t\t\tif (se_depth >= pse_depth) {\n\t\t\t\tset_next_entity(cfs_rq_of(se), se);\n\t\t\t\tse = parent_entity(se);\n\t\t\t}\n\t\t}\n\n\t\tput_prev_entity(cfs_rq, pse);\n\t\tset_next_entity(cfs_rq, se);\n\t}\n\n\tgoto done;\nsimple:\n#endif\n\n\tput_prev_task(rq, prev);\n\n\tdo {\n\t\tse = pick_next_entity(cfs_rq, NULL);\n\t\tset_next_entity(cfs_rq, se);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\ndone: __maybe_unused;\n#ifdef CONFIG_SMP\n\t/*\n\t * Move the next running task to the front of\n\t * the list, so our cfs_tasks list becomes MRU\n\t * one.\n\t */\n\tlist_move(&p->se.group_node, &rq->cfs_tasks);\n#endif\n\n\tif (hrtick_enabled(rq))\n\t\thrtick_start_fair(rq, p);\n\n\tupdate_misfit_status(p, rq);\n\n\treturn p;\n\nidle:\n\tupdate_misfit_status(NULL, rq);\n\tnew_tasks = idle_balance(rq, rf);\n\n\t/*\n\t * Because idle_balance() releases (and re-acquires) rq->lock, it is\n\t * possible for any higher priority task to appear. In that case we\n\t * must re-start the pick_next_entity() loop.\n\t */\n\tif (new_tasks < 0)\n\t\treturn RETRY_TASK;\n\n\tif (new_tasks > 0)\n\t\tgoto again;\n\n\treturn NULL;\n}\n\n/*\n * Account for a descheduled task:\n */\nstatic void put_prev_task_fair(struct rq *rq, struct task_struct *prev)\n{\n\tstruct sched_entity *se = &prev->se;\n\tstruct cfs_rq *cfs_rq;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tput_prev_entity(cfs_rq, se);\n\t}\n}\n\n/*\n * sched_yield() is very simple\n *\n * The magic of dealing with the ->skip buddy is in pick_next_entity.\n */\nstatic void yield_task_fair(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tstruct sched_entity *se = &curr->se;\n\n\t/*\n\t * Are we the only task in the tree?\n\t */\n\tif (unlikely(rq->nr_running == 1))\n\t\treturn;\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (curr->policy != SCHED_BATCH) {\n\t\tupdate_rq_clock(rq);\n\t\t/*\n\t\t * Update run-time statistics of the 'current'.\n\t\t */\n\t\tupdate_curr(cfs_rq);\n\t\t/*\n\t\t * Tell update_rq_clock() that we've just updated,\n\t\t * so we don't do microscopic update in schedule()\n\t\t * and double the fastpath cost.\n\t\t */\n\t\trq_clock_skip_update(rq);\n\t}\n\n\tset_skip_buddy(se);\n}\n\nstatic bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/* throttled hierarchies are not runnable */\n\tif (!se->on_rq || throttled_hierarchy(cfs_rq_of(se)))\n\t\treturn false;\n\n\t/* Tell the scheduler that we'd really like pse to run next. */\n\tset_next_buddy(se);\n\n\tyield_task_fair(rq);\n\n\treturn true;\n}\n\n#ifdef CONFIG_SMP\n/**************************************************\n * Fair scheduling class load-balancing methods.\n *\n * BASICS\n *\n * The purpose of load-balancing is to achieve the same basic fairness the\n * per-CPU scheduler provides, namely provide a proportional amount of compute\n * time to each task. This is expressed in the following equation:\n *\n *   W_i,n/P_i == W_j,n/P_j for all i,j                               (1)\n *\n * Where W_i,n is the n-th weight average for CPU i. The instantaneous weight\n * W_i,0 is defined as:\n *\n *   W_i,0 = \\Sum_j w_i,j                                             (2)\n *\n * Where w_i,j is the weight of the j-th runnable task on CPU i. This weight\n * is derived from the nice value as per sched_prio_to_weight[].\n *\n * The weight average is an exponential decay average of the instantaneous\n * weight:\n *\n *   W'_i,n = (2^n - 1) / 2^n * W_i,n + 1 / 2^n * W_i,0               (3)\n *\n * C_i is the compute capacity of CPU i, typically it is the\n * fraction of 'recent' time available for SCHED_OTHER task execution. But it\n * can also include other factors [XXX].\n *\n * To achieve this balance we define a measure of imbalance which follows\n * directly from (1):\n *\n *   imb_i,j = max{ avg(W/C), W_i/C_i } - min{ avg(W/C), W_j/C_j }    (4)\n *\n * We them move tasks around to minimize the imbalance. In the continuous\n * function space it is obvious this converges, in the discrete case we get\n * a few fun cases generally called infeasible weight scenarios.\n *\n * [XXX expand on:\n *     - infeasible weights;\n *     - local vs global optima in the discrete case. ]\n *\n *\n * SCHED DOMAINS\n *\n * In order to solve the imbalance equation (4), and avoid the obvious O(n^2)\n * for all i,j solution, we create a tree of CPUs that follows the hardware\n * topology where each level pairs two lower groups (or better). This results\n * in O(log n) layers. Furthermore we reduce the number of CPUs going up the\n * tree to only the first of the previous level and we decrease the frequency\n * of load-balance at each level inv. proportional to the number of CPUs in\n * the groups.\n *\n * This yields:\n *\n *     log_2 n     1     n\n *   \\Sum       { --- * --- * 2^i } = O(n)                            (5)\n *     i = 0      2^i   2^i\n *                               `- size of each group\n *         |         |     `- number of CPUs doing load-balance\n *         |         `- freq\n *         `- sum over all levels\n *\n * Coupled with a limit on how many tasks we can migrate every balance pass,\n * this makes (5) the runtime complexity of the balancer.\n *\n * An important property here is that each CPU is still (indirectly) connected\n * to every other CPU in at most O(log n) steps:\n *\n * The adjacency matrix of the resulting graph is given by:\n *\n *             log_2 n\n *   A_i,j = \\Union     (i % 2^k == 0) && i / 2^(k+1) == j / 2^(k+1)  (6)\n *             k = 0\n *\n * And you'll find that:\n *\n *   A^(log_2 n)_i,j != 0  for all i,j                                (7)\n *\n * Showing there's indeed a path between every CPU in at most O(log n) steps.\n * The task movement gives a factor of O(m), giving a convergence complexity\n * of:\n *\n *   O(nm log n),  n := nr_cpus, m := nr_tasks                        (8)\n *\n *\n * WORK CONSERVING\n *\n * In order to avoid CPUs going idle while there's still work to do, new idle\n * balancing is more aggressive and has the newly idle CPU iterate up the domain\n * tree itself instead of relying on other CPUs to bring it work.\n *\n * This adds some complexity to both (5) and (8) but it reduces the total idle\n * time.\n *\n * [XXX more?]\n *\n *\n * CGROUPS\n *\n * Cgroups make a horror show out of (2), instead of a simple sum we get:\n *\n *                                s_k,i\n *   W_i,0 = \\Sum_j \\Prod_k w_k * -----                               (9)\n *                                 S_k\n *\n * Where\n *\n *   s_k,i = \\Sum_j w_i,j,k  and  S_k = \\Sum_i s_k,i                 (10)\n *\n * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i.\n *\n * The big problem is S_k, its a global sum needed to compute a local (W_i)\n * property.\n *\n * [XXX write more on how we solve this.. _after_ merging pjt's patches that\n *      rewrite all of this once again.]\n */\n\nstatic unsigned long __read_mostly max_load_balance_interval = HZ/10;\n\nenum fbq_type { regular, remote, all };\n\nenum group_type {\n\tgroup_other = 0,\n\tgroup_misfit_task,\n\tgroup_imbalanced,\n\tgroup_overloaded,\n};\n\n#define LBF_ALL_PINNED\t0x01\n#define LBF_NEED_BREAK\t0x02\n#define LBF_DST_PINNED  0x04\n#define LBF_SOME_PINNED\t0x08\n#define LBF_NOHZ_STATS\t0x10\n#define LBF_NOHZ_AGAIN\t0x20\n\nstruct lb_env {\n\tstruct sched_domain\t*sd;\n\n\tstruct rq\t\t*src_rq;\n\tint\t\t\tsrc_cpu;\n\n\tint\t\t\tdst_cpu;\n\tstruct rq\t\t*dst_rq;\n\n\tstruct cpumask\t\t*dst_grpmask;\n\tint\t\t\tnew_dst_cpu;\n\tenum cpu_idle_type\tidle;\n\tlong\t\t\timbalance;\n\t/* The set of CPUs under consideration for load-balancing */\n\tstruct cpumask\t\t*cpus;\n\n\tunsigned int\t\tflags;\n\n\tunsigned int\t\tloop;\n\tunsigned int\t\tloop_break;\n\tunsigned int\t\tloop_max;\n\n\tenum fbq_type\t\tfbq_type;\n\tenum group_type\t\tsrc_grp_type;\n\tstruct list_head\ttasks;\n};\n\n/*\n * Is this task likely cache-hot:\n */\nstatic int task_hot(struct task_struct *p, struct lb_env *env)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(task_has_idle_policy(p)))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && env->dst_rq->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = rq_clock_task(env->src_rq) - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns 1, if task migration degrades locality\n * Returns 0, if task migration improves locality i.e migration preferred.\n * Returns -1, if task migration is not affected by locality.\n */\nstatic int migrate_degrades_locality(struct task_struct *p, struct lb_env *env)\n{\n\tstruct numa_group *numa_group = rcu_dereference(p->numa_group);\n\tunsigned long src_weight, dst_weight;\n\tint src_nid, dst_nid, dist;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn -1;\n\n\tif (!p->numa_faults || !(env->sd->flags & SD_NUMA))\n\t\treturn -1;\n\n\tsrc_nid = cpu_to_node(env->src_cpu);\n\tdst_nid = cpu_to_node(env->dst_cpu);\n\n\tif (src_nid == dst_nid)\n\t\treturn -1;\n\n\t/* Migrating away from the preferred node is always bad. */\n\tif (src_nid == p->numa_preferred_nid) {\n\t\tif (env->src_rq->nr_running > env->src_rq->nr_preferred_running)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\t/* Encourage migration to the preferred node. */\n\tif (dst_nid == p->numa_preferred_nid)\n\t\treturn 0;\n\n\t/* Leaving a core idle is often worse than degrading locality. */\n\tif (env->idle == CPU_IDLE)\n\t\treturn -1;\n\n\tdist = node_distance(src_nid, dst_nid);\n\tif (numa_group) {\n\t\tsrc_weight = group_weight(p, src_nid, dist);\n\t\tdst_weight = group_weight(p, dst_nid, dist);\n\t} else {\n\t\tsrc_weight = task_weight(p, src_nid, dist);\n\t\tdst_weight = task_weight(p, dst_nid, dist);\n\t}\n\n\treturn dst_weight < src_weight;\n}\n\n#else\nstatic inline int migrate_degrades_locality(struct task_struct *p,\n\t\t\t\t\t     struct lb_env *env)\n{\n\treturn -1;\n}\n#endif\n\n/*\n * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?\n */\nstatic\nint can_migrate_task(struct task_struct *p, struct lb_env *env)\n{\n\tint tsk_cache_hot;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\t/*\n\t * We do not migrate tasks that are:\n\t * 1) throttled_lb_pair, or\n\t * 2) cannot be migrated to this CPU due to cpus_allowed, or\n\t * 3) running (obviously), or\n\t * 4) are cache-hot on their current CPU.\n\t */\n\tif (throttled_lb_pair(task_group(p), env->src_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(env->dst_cpu, &p->cpus_allowed)) {\n\t\tint cpu;\n\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_affine);\n\n\t\tenv->flags |= LBF_SOME_PINNED;\n\n\t\t/*\n\t\t * Remember if this task can be migrated to any other CPU in\n\t\t * our sched_group. We may want to revisit it if we couldn't\n\t\t * meet load balance goals by pulling other tasks on src_cpu.\n\t\t *\n\t\t * Avoid computing new_dst_cpu for NEWLY_IDLE or if we have\n\t\t * already computed one in current iteration.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE || (env->flags & LBF_DST_PINNED))\n\t\t\treturn 0;\n\n\t\t/* Prevent to re-select dst_cpu via env's CPUs: */\n\t\tfor_each_cpu_and(cpu, env->dst_grpmask, env->cpus) {\n\t\t\tif (cpumask_test_cpu(cpu, &p->cpus_allowed)) {\n\t\t\t\tenv->flags |= LBF_DST_PINNED;\n\t\t\t\tenv->new_dst_cpu = cpu;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* Record that we found atleast one task that could run on dst_cpu */\n\tenv->flags &= ~LBF_ALL_PINNED;\n\n\tif (task_running(env->src_rq, p)) {\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_running);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Aggressive migration if:\n\t * 1) destination numa is preferred\n\t * 2) task is cache cold, or\n\t * 3) too many balance attempts have failed.\n\t */\n\ttsk_cache_hot = migrate_degrades_locality(p, env);\n\tif (tsk_cache_hot == -1)\n\t\ttsk_cache_hot = task_hot(p, env);\n\n\tif (tsk_cache_hot <= 0 ||\n\t    env->sd->nr_balance_failed > env->sd->cache_nice_tries) {\n\t\tif (tsk_cache_hot == 1) {\n\t\t\tschedstat_inc(env->sd->lb_hot_gained[env->idle]);\n\t\t\tschedstat_inc(p->se.statistics.nr_forced_migrations);\n\t\t}\n\t\treturn 1;\n\t}\n\n\tschedstat_inc(p->se.statistics.nr_failed_migrations_hot);\n\treturn 0;\n}\n\n/*\n * detach_task() -- detach the task for the migration specified in env\n */\nstatic void detach_task(struct task_struct *p, struct lb_env *env)\n{\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\tdeactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);\n\tset_task_cpu(p, env->dst_cpu);\n}\n\n/*\n * detach_one_task() -- tries to dequeue exactly one task from env->src_rq, as\n * part of active balancing operations within \"domain\".\n *\n * Returns a task if successful and NULL otherwise.\n */\nstatic struct task_struct *detach_one_task(struct lb_env *env)\n{\n\tstruct task_struct *p;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tlist_for_each_entry_reverse(p,\n\t\t\t&env->src_rq->cfs_tasks, se.group_node) {\n\t\tif (!can_migrate_task(p, env))\n\t\t\tcontinue;\n\n\t\tdetach_task(p, env);\n\n\t\t/*\n\t\t * Right now, this is only the second place where\n\t\t * lb_gained[env->idle] is updated (other is detach_tasks)\n\t\t * so we can safely collect stats here rather than\n\t\t * inside detach_tasks().\n\t\t */\n\t\tschedstat_inc(env->sd->lb_gained[env->idle]);\n\t\treturn p;\n\t}\n\treturn NULL;\n}\n\nstatic const unsigned int sched_nr_migrate_break = 32;\n\n/*\n * detach_tasks() -- tries to detach up to imbalance weighted load from\n * busiest_rq, as part of a balancing operation within domain \"sd\".\n *\n * Returns number of detached tasks if successful and 0 otherwise.\n */\nstatic int detach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->src_rq->cfs_tasks;\n\tstruct task_struct *p;\n\tunsigned long load;\n\tint detached = 0;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (env->imbalance <= 0)\n\t\treturn 0;\n\n\twhile (!list_empty(tasks)) {\n\t\t/*\n\t\t * We don't want to steal all, otherwise we may be treated likewise,\n\t\t * which could at worst lead to a livelock crash.\n\t\t */\n\t\tif (env->idle != CPU_NOT_IDLE && env->src_rq->nr_running <= 1)\n\t\t\tbreak;\n\n\t\tp = list_last_entry(tasks, struct task_struct, se.group_node);\n\n\t\tenv->loop++;\n\t\t/* We've more or less seen every task there is, call it quits */\n\t\tif (env->loop > env->loop_max)\n\t\t\tbreak;\n\n\t\t/* take a breather every nr_migrate tasks */\n\t\tif (env->loop > env->loop_break) {\n\t\t\tenv->loop_break += sched_nr_migrate_break;\n\t\t\tenv->flags |= LBF_NEED_BREAK;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!can_migrate_task(p, env))\n\t\t\tgoto next;\n\n\t\tload = task_h_load(p);\n\n\t\tif (sched_feat(LB_MIN) && load < 16 && !env->sd->nr_balance_failed)\n\t\t\tgoto next;\n\n\t\tif ((load / 2) > env->imbalance)\n\t\t\tgoto next;\n\n\t\tdetach_task(p, env);\n\t\tlist_add(&p->se.group_node, &env->tasks);\n\n\t\tdetached++;\n\t\tenv->imbalance -= load;\n\n#ifdef CONFIG_PREEMPT\n\t\t/*\n\t\t * NEWIDLE balancing is a source of latency, so preemptible\n\t\t * kernels will stop after the first task is detached to minimize\n\t\t * the critical section.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE)\n\t\t\tbreak;\n#endif\n\n\t\t/*\n\t\t * We only want to steal up to the prescribed amount of\n\t\t * weighted load.\n\t\t */\n\t\tif (env->imbalance <= 0)\n\t\t\tbreak;\n\n\t\tcontinue;\nnext:\n\t\tlist_move(&p->se.group_node, tasks);\n\t}\n\n\t/*\n\t * Right now, this is one of only two places we collect this stat\n\t * so we can safely collect detach_one_task() stats here rather\n\t * than inside detach_one_task().\n\t */\n\tschedstat_add(env->sd->lb_gained[env->idle], detached);\n\n\treturn detached;\n}\n\n/*\n * attach_task() -- attach the task detached by detach_task() to its new rq.\n */\nstatic void attach_task(struct rq *rq, struct task_struct *p)\n{\n\tlockdep_assert_held(&rq->lock);\n\n\tBUG_ON(task_rq(p) != rq);\n\tactivate_task(rq, p, ENQUEUE_NOCLOCK);\n\tp->on_rq = TASK_ON_RQ_QUEUED;\n\tcheck_preempt_curr(rq, p, 0);\n}\n\n/*\n * attach_one_task() -- attaches the task returned from detach_one_task() to\n * its new rq.\n */\nstatic void attach_one_task(struct rq *rq, struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\tattach_task(rq, p);\n\trq_unlock(rq, &rf);\n}\n\n/*\n * attach_tasks() -- attaches all tasks detached by detach_tasks() to their\n * new rq.\n */\nstatic void attach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->tasks;\n\tstruct task_struct *p;\n\tstruct rq_flags rf;\n\n\trq_lock(env->dst_rq, &rf);\n\tupdate_rq_clock(env->dst_rq);\n\n\twhile (!list_empty(tasks)) {\n\t\tp = list_first_entry(tasks, struct task_struct, se.group_node);\n\t\tlist_del_init(&p->se.group_node);\n\n\t\tattach_task(env->dst_rq, p);\n\t}\n\n\trq_unlock(env->dst_rq, &rf);\n}\n\nstatic inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->avg.load_avg)\n\t\treturn true;\n\n\tif (cfs_rq->avg.util_avg)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool others_have_blocked(struct rq *rq)\n{\n\tif (READ_ONCE(rq->avg_rt.util_avg))\n\t\treturn true;\n\n\tif (READ_ONCE(rq->avg_dl.util_avg))\n\t\treturn true;\n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\n\tif (READ_ONCE(rq->avg_irq.util_avg))\n\t\treturn true;\n#endif\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\nstatic void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\tbool done = true;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\t/*\n\t * Iterates the task_group tree in a bottom up fashion, see\n\t * list_add_leaf_cfs_rq() for details.\n\t */\n\tfor_each_leaf_cfs_rq(rq, cfs_rq) {\n\t\tstruct sched_entity *se;\n\n\t\t/* throttled entities do not contribute to load */\n\t\tif (throttled_hierarchy(cfs_rq))\n\t\t\tcontinue;\n\n\t\tif (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq))\n\t\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t\t/* Propagate pending load changes to the parent, if any: */\n\t\tse = cfs_rq->tg->se[cpu];\n\t\tif (se && !skip_blocked_update(se))\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, 0);\n\n\t\t/* Don't need periodic decay once load/util_avg are null */\n\t\tif (cfs_rq_has_blocked(cfs_rq))\n\t\t\tdone = false;\n\t}\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n\t/* Don't need periodic decay once load/util_avg are null */\n\tif (others_have_blocked(rq))\n\t\tdone = false;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (done)\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\n/*\n * Compute the hierarchical load factor for cfs_rq and all its ascendants.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];\n\tunsigned long now = jiffies;\n\tunsigned long load;\n\n\tif (cfs_rq->last_h_load_update == now)\n\t\treturn;\n\n\tcfs_rq->h_load_next = NULL;\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_load_next = se;\n\t\tif (cfs_rq->last_h_load_update == now)\n\t\t\tbreak;\n\t}\n\n\tif (!se) {\n\t\tcfs_rq->h_load = cfs_rq_load_avg(cfs_rq);\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n\n\twhile ((se = cfs_rq->h_load_next) != NULL) {\n\t\tload = cfs_rq->h_load;\n\t\tload = div64_ul(load * se->avg.load_avg,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n\t\tcfs_rq = group_cfs_rq(se);\n\t\tcfs_rq->h_load = load;\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n\n\tupdate_cfs_rq_h_load(cfs_rq);\n\treturn div64_ul(p->se.avg.load_avg * cfs_rq->h_load,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n}\n#else\nstatic inline void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\tupdate_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq);\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (!cfs_rq_has_blocked(cfs_rq) && !others_have_blocked(rq))\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\treturn p->se.avg.load_avg;\n}\n#endif\n\n/********** Helpers for find_busiest_group ************************/\n\n/*\n * sg_lb_stats - stats of a sched_group required for load_balancing\n */\nstruct sg_lb_stats {\n\tunsigned long avg_load; /*Avg load across the CPUs of the group */\n\tunsigned long group_load; /* Total load over the CPUs of the group */\n\tunsigned long sum_weighted_load; /* Weighted load of group's tasks */\n\tunsigned long load_per_task;\n\tunsigned long group_capacity;\n\tunsigned long group_util; /* Total utilization of the group */\n\tunsigned int sum_nr_running; /* Nr tasks running in the group */\n\tunsigned int idle_cpus;\n\tunsigned int group_weight;\n\tenum group_type group_type;\n\tint group_no_capacity;\n\tunsigned long group_misfit_task_load; /* A CPU has a task too big for its capacity */\n#ifdef CONFIG_NUMA_BALANCING\n\tunsigned int nr_numa_running;\n\tunsigned int nr_preferred_running;\n#endif\n};\n\n/*\n * sd_lb_stats - Structure to store the statistics of a sched_domain\n *\t\t during load balancing.\n */\nstruct sd_lb_stats {\n\tstruct sched_group *busiest;\t/* Busiest group in this sd */\n\tstruct sched_group *local;\t/* Local group in this sd */\n\tunsigned long total_running;\n\tunsigned long total_load;\t/* Total load of all groups in sd */\n\tunsigned long total_capacity;\t/* Total capacity of all groups in sd */\n\tunsigned long avg_load;\t/* Average load across all groups in sd */\n\n\tstruct sg_lb_stats busiest_stat;/* Statistics of the busiest group */\n\tstruct sg_lb_stats local_stat;\t/* Statistics of the local group */\n};\n\nstatic inline void init_sd_lb_stats(struct sd_lb_stats *sds)\n{\n\t/*\n\t * Skimp on the clearing to avoid duplicate work. We can avoid clearing\n\t * local_stat because update_sg_lb_stats() does a full clear/assignment.\n\t * We must however clear busiest_stat::avg_load because\n\t * update_sd_pick_busiest() reads this before assignment.\n\t */\n\t*sds = (struct sd_lb_stats){\n\t\t.busiest = NULL,\n\t\t.local = NULL,\n\t\t.total_running = 0UL,\n\t\t.total_load = 0UL,\n\t\t.total_capacity = 0UL,\n\t\t.busiest_stat = {\n\t\t\t.avg_load = 0UL,\n\t\t\t.sum_nr_running = 0,\n\t\t\t.group_type = group_other,\n\t\t},\n\t};\n}\n\n/**\n * get_sd_load_idx - Obtain the load index for a given sched domain.\n * @sd: The sched_domain whose load_idx is to be obtained.\n * @idle: The idle status of the CPU for whose sd load_idx is obtained.\n *\n * Return: The load index.\n */\nstatic inline int get_sd_load_idx(struct sched_domain *sd,\n\t\t\t\t\tenum cpu_idle_type idle)\n{\n\tint load_idx;\n\n\tswitch (idle) {\n\tcase CPU_NOT_IDLE:\n\t\tload_idx = sd->busy_idx;\n\t\tbreak;\n\n\tcase CPU_NEWLY_IDLE:\n\t\tload_idx = sd->newidle_idx;\n\t\tbreak;\n\tdefault:\n\t\tload_idx = sd->idle_idx;\n\t\tbreak;\n\t}\n\n\treturn load_idx;\n}\n\nstatic unsigned long scale_rt_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long max = arch_scale_cpu_capacity(sd, cpu);\n\tunsigned long used, free;\n\tunsigned long irq;\n\n\tirq = cpu_util_irq(rq);\n\n\tif (unlikely(irq >= max))\n\t\treturn 1;\n\n\tused = READ_ONCE(rq->avg_rt.util_avg);\n\tused += READ_ONCE(rq->avg_dl.util_avg);\n\n\tif (unlikely(used >= max))\n\t\treturn 1;\n\n\tfree = max - used;\n\n\treturn scale_irq_capacity(free, irq, max);\n}\n\nstatic void update_cpu_capacity(struct sched_domain *sd, int cpu)\n{\n\tunsigned long capacity = scale_rt_capacity(sd, cpu);\n\tstruct sched_group *sdg = sd->groups;\n\n\tcpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(sd, cpu);\n\n\tif (!capacity)\n\t\tcapacity = 1;\n\n\tcpu_rq(cpu)->cpu_capacity = capacity;\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = capacity;\n\tsdg->sgc->max_capacity = capacity;\n}\n\nvoid update_group_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_domain *child = sd->child;\n\tstruct sched_group *group, *sdg = sd->groups;\n\tunsigned long capacity, min_capacity, max_capacity;\n\tunsigned long interval;\n\n\tinterval = msecs_to_jiffies(sd->balance_interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\tsdg->sgc->next_update = jiffies + interval;\n\n\tif (!child) {\n\t\tupdate_cpu_capacity(sd, cpu);\n\t\treturn;\n\t}\n\n\tcapacity = 0;\n\tmin_capacity = ULONG_MAX;\n\tmax_capacity = 0;\n\n\tif (child->flags & SD_OVERLAP) {\n\t\t/*\n\t\t * SD_OVERLAP domains cannot assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tfor_each_cpu(cpu, sched_group_span(sdg)) {\n\t\t\tstruct sched_group_capacity *sgc;\n\t\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\t\t/*\n\t\t\t * build_sched_domains() -> init_sched_groups_capacity()\n\t\t\t * gets here before we've attached the domains to the\n\t\t\t * runqueues.\n\t\t\t *\n\t\t\t * Use capacity_of(), which is set irrespective of domains\n\t\t\t * in update_cpu_capacity().\n\t\t\t *\n\t\t\t * This avoids capacity from being 0 and\n\t\t\t * causing divide-by-zero issues on boot.\n\t\t\t */\n\t\t\tif (unlikely(!rq->sd)) {\n\t\t\t\tcapacity += capacity_of(cpu);\n\t\t\t} else {\n\t\t\t\tsgc = rq->sd->groups->sgc;\n\t\t\t\tcapacity += sgc->capacity;\n\t\t\t}\n\n\t\t\tmin_capacity = min(capacity, min_capacity);\n\t\t\tmax_capacity = max(capacity, max_capacity);\n\t\t}\n\t} else  {\n\t\t/*\n\t\t * !SD_OVERLAP domains can assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tgroup = child->groups;\n\t\tdo {\n\t\t\tstruct sched_group_capacity *sgc = group->sgc;\n\n\t\t\tcapacity += sgc->capacity;\n\t\t\tmin_capacity = min(sgc->min_capacity, min_capacity);\n\t\t\tmax_capacity = max(sgc->max_capacity, max_capacity);\n\t\t\tgroup = group->next;\n\t\t} while (group != child->groups);\n\t}\n\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = min_capacity;\n\tsdg->sgc->max_capacity = max_capacity;\n}\n\n/*\n * Check whether the capacity of the rq has been noticeably reduced by side\n * activity. The imbalance_pct is used for the threshold.\n * Return true is the capacity is reduced\n */\nstatic inline int\ncheck_cpu_capacity(struct rq *rq, struct sched_domain *sd)\n{\n\treturn ((rq->cpu_capacity * sd->imbalance_pct) <\n\t\t\t\t(rq->cpu_capacity_orig * 100));\n}\n\n/*\n * Group imbalance indicates (and tries to solve) the problem where balancing\n * groups is inadequate due to ->cpus_allowed constraints.\n *\n * Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a\n * cpumask covering 1 CPU of the first group and 3 CPUs of the second group.\n * Something like:\n *\n *\t{ 0 1 2 3 } { 4 5 6 7 }\n *\t        *     * * *\n *\n * If we were to balance group-wise we'd place two tasks in the first group and\n * two tasks in the second group. Clearly this is undesired as it will overload\n * cpu 3 and leave one of the CPUs in the second group unused.\n *\n * The current solution to this issue is detecting the skew in the first group\n * by noticing the lower domain failed to reach balance and had difficulty\n * moving tasks due to affinity constraints.\n *\n * When this is so detected; this group becomes a candidate for busiest; see\n * update_sd_pick_busiest(). And calculate_imbalance() and\n * find_busiest_group() avoid some of the usual balance conditions to allow it\n * to create an effective group imbalance.\n *\n * This is a somewhat tricky proposition since the next run might not find the\n * group imbalance and decide the groups need to be balanced again. A most\n * subtle and fragile situation.\n */\n\nstatic inline int sg_imbalanced(struct sched_group *group)\n{\n\treturn group->sgc->imbalance;\n}\n\n/*\n * group_has_capacity returns true if the group has spare capacity that could\n * be used by some tasks.\n * We consider that a group has spare capacity if the  * number of task is\n * smaller than the number of CPUs or if the utilization is lower than the\n * available capacity for CFS tasks.\n * For the latter, we use a threshold to stabilize the state, to take into\n * account the variance of the tasks' load and to return true if the available\n * capacity in meaningful for the load balancer.\n * As an example, an available capacity of 1% can appear but it doesn't make\n * any benefit for the load balance.\n */\nstatic inline bool\ngroup_has_capacity(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running < sgs->group_weight)\n\t\treturn true;\n\n\tif ((sgs->group_capacity * 100) >\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *  group_is_overloaded returns true if the group has more tasks than it can\n *  handle.\n *  group_is_overloaded is not equals to !group_has_capacity because a group\n *  with the exact right number of tasks, has no more spare capacity but is not\n *  overloaded so both group_has_capacity and group_is_overloaded return\n *  false.\n */\nstatic inline bool\ngroup_is_overloaded(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running <= sgs->group_weight)\n\t\treturn false;\n\n\tif ((sgs->group_capacity * 100) <\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * group_smaller_min_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_min_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->min_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->min_capacity * 1024;\n}\n\n/*\n * group_smaller_max_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity_orig than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_max_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->max_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->max_capacity * 1024;\n}\n\nstatic inline enum\ngroup_type group_classify(struct sched_group *group,\n\t\t\t  struct sg_lb_stats *sgs)\n{\n\tif (sgs->group_no_capacity)\n\t\treturn group_overloaded;\n\n\tif (sg_imbalanced(group))\n\t\treturn group_imbalanced;\n\n\tif (sgs->group_misfit_task_load)\n\t\treturn group_misfit_task;\n\n\treturn group_other;\n}\n\nstatic bool update_nohz_stats(struct rq *rq, bool force)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tunsigned int cpu = rq->cpu;\n\n\tif (!rq->has_blocked_load)\n\t\treturn false;\n\n\tif (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask))\n\t\treturn false;\n\n\tif (!force && !time_after(jiffies, rq->last_blocked_load_update_tick))\n\t\treturn true;\n\n\tupdate_blocked_averages(cpu);\n\n\treturn rq->has_blocked_load;\n#else\n\treturn false;\n#endif\n}\n\n/**\n * update_sg_lb_stats - Update sched_group's statistics for load balancing.\n * @env: The load balancing environment.\n * @group: sched_group whose statistics are to be updated.\n * @sgs: variable to hold the statistics for this group.\n * @sg_status: Holds flag indicating the status of the sched_group\n */\nstatic inline void update_sg_lb_stats(struct lb_env *env,\n\t\t\t\t      struct sched_group *group,\n\t\t\t\t      struct sg_lb_stats *sgs,\n\t\t\t\t      int *sg_status)\n{\n\tint local_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(group));\n\tint load_idx = get_sd_load_idx(env->sd, env->idle);\n\tunsigned long load;\n\tint i, nr_running;\n\n\tmemset(sgs, 0, sizeof(*sgs));\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tstruct rq *rq = cpu_rq(i);\n\n\t\tif ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))\n\t\t\tenv->flags |= LBF_NOHZ_AGAIN;\n\n\t\t/* Bias balancing toward CPUs of our domain: */\n\t\tif (local_group)\n\t\t\tload = target_load(i, load_idx);\n\t\telse\n\t\t\tload = source_load(i, load_idx);\n\n\t\tsgs->group_load += load;\n\t\tsgs->group_util += cpu_util(i);\n\t\tsgs->sum_nr_running += rq->cfs.h_nr_running;\n\n\t\tnr_running = rq->nr_running;\n\t\tif (nr_running > 1)\n\t\t\t*sg_status |= SG_OVERLOAD;\n\n\t\tif (cpu_overutilized(i))\n\t\t\t*sg_status |= SG_OVERUTILIZED;\n\n#ifdef CONFIG_NUMA_BALANCING\n\t\tsgs->nr_numa_running += rq->nr_numa_running;\n\t\tsgs->nr_preferred_running += rq->nr_preferred_running;\n#endif\n\t\tsgs->sum_weighted_load += weighted_cpuload(rq);\n\t\t/*\n\t\t * No need to call idle_cpu() if nr_running is not 0\n\t\t */\n\t\tif (!nr_running && idle_cpu(i))\n\t\t\tsgs->idle_cpus++;\n\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    sgs->group_misfit_task_load < rq->misfit_task_load) {\n\t\t\tsgs->group_misfit_task_load = rq->misfit_task_load;\n\t\t\t*sg_status |= SG_OVERLOAD;\n\t\t}\n\t}\n\n\t/* Adjust by relative CPU capacity of the group */\n\tsgs->group_capacity = group->sgc->capacity;\n\tsgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;\n\n\tif (sgs->sum_nr_running)\n\t\tsgs->load_per_task = sgs->sum_weighted_load / sgs->sum_nr_running;\n\n\tsgs->group_weight = group->group_weight;\n\n\tsgs->group_no_capacity = group_is_overloaded(env, sgs);\n\tsgs->group_type = group_classify(group, sgs);\n}\n\n/**\n * update_sd_pick_busiest - return 1 on busiest group\n * @env: The load balancing environment.\n * @sds: sched_domain statistics\n * @sg: sched_group candidate to be checked for being the busiest\n * @sgs: sched_group statistics\n *\n * Determine if @sg is a busier group than the previously selected\n * busiest group.\n *\n * Return: %true if @sg is a busier group than the previously selected\n * busiest group. %false otherwise.\n */\nstatic bool update_sd_pick_busiest(struct lb_env *env,\n\t\t\t\t   struct sd_lb_stats *sds,\n\t\t\t\t   struct sched_group *sg,\n\t\t\t\t   struct sg_lb_stats *sgs)\n{\n\tstruct sg_lb_stats *busiest = &sds->busiest_stat;\n\n\t/*\n\t * Don't try to pull misfit tasks we can't help.\n\t * We can use max_capacity here as reduction in capacity on some\n\t * CPUs in the group should either be possible to resolve\n\t * internally or be covered by avg_load imbalance (eventually).\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    (!group_smaller_max_cpu_capacity(sg, sds->local) ||\n\t     !group_has_capacity(env, &sds->local_stat)))\n\t\treturn false;\n\n\tif (sgs->group_type > busiest->group_type)\n\t\treturn true;\n\n\tif (sgs->group_type < busiest->group_type)\n\t\treturn false;\n\n\tif (sgs->avg_load <= busiest->avg_load)\n\t\treturn false;\n\n\tif (!(env->sd->flags & SD_ASYM_CPUCAPACITY))\n\t\tgoto asym_packing;\n\n\t/*\n\t * Candidate sg has no more than one task per CPU and\n\t * has higher per-CPU capacity. Migrating tasks to less\n\t * capable CPUs may harm throughput. Maximize throughput,\n\t * power/energy consequences are not considered.\n\t */\n\tif (sgs->sum_nr_running <= sgs->group_weight &&\n\t    group_smaller_min_cpu_capacity(sds->local, sg))\n\t\treturn false;\n\n\t/*\n\t * If we have more than one misfit sg go with the biggest misfit.\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    sgs->group_misfit_task_load < busiest->group_misfit_task_load)\n\t\treturn false;\n\nasym_packing:\n\t/* This is the busiest node in its class. */\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn true;\n\n\t/* No ASYM_PACKING if target CPU is already busy */\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn true;\n\t/*\n\t * ASYM_PACKING needs to move all the work to the highest\n\t * prority CPUs in the group, therefore mark all groups\n\t * of lower priority than ourself as busy.\n\t */\n\tif (sgs->sum_nr_running &&\n\t    sched_asym_prefer(env->dst_cpu, sg->asym_prefer_cpu)) {\n\t\tif (!sds->busiest)\n\t\t\treturn true;\n\n\t\t/* Prefer to move from lowest priority CPU's work */\n\t\tif (sched_asym_prefer(sds->busiest->asym_prefer_cpu,\n\t\t\t\t      sg->asym_prefer_cpu))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running > sgs->nr_numa_running)\n\t\treturn regular;\n\tif (sgs->sum_nr_running > sgs->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\tif (rq->nr_running > rq->nr_numa_running)\n\t\treturn regular;\n\tif (rq->nr_running > rq->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n#else\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\treturn regular;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n/**\n * update_sd_lb_stats - Update sched_domain's statistics for load balancing.\n * @env: The load balancing environment.\n * @sds: variable to hold the statistics for this sched_domain.\n */\nstatic inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tstruct sched_domain *child = env->sd->child;\n\tstruct sched_group *sg = env->sd->groups;\n\tstruct sg_lb_stats *local = &sds->local_stat;\n\tstruct sg_lb_stats tmp_sgs;\n\tbool prefer_sibling = child && child->flags & SD_PREFER_SIBLING;\n\tint sg_status = 0;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))\n\t\tenv->flags |= LBF_NOHZ_STATS;\n#endif\n\n\tdo {\n\t\tstruct sg_lb_stats *sgs = &tmp_sgs;\n\t\tint local_group;\n\n\t\tlocal_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));\n\t\tif (local_group) {\n\t\t\tsds->local = sg;\n\t\t\tsgs = local;\n\n\t\t\tif (env->idle != CPU_NEWLY_IDLE ||\n\t\t\t    time_after_eq(jiffies, sg->sgc->next_update))\n\t\t\t\tupdate_group_capacity(env->sd, env->dst_cpu);\n\t\t}\n\n\t\tupdate_sg_lb_stats(env, sg, sgs, &sg_status);\n\n\t\tif (local_group)\n\t\t\tgoto next_group;\n\n\t\t/*\n\t\t * In case the child domain prefers tasks go to siblings\n\t\t * first, lower the sg capacity so that we'll try\n\t\t * and move all the excess tasks away. We lower the capacity\n\t\t * of a group only if the local group has the capacity to fit\n\t\t * these excess tasks. The extra check prevents the case where\n\t\t * you always pull from the heaviest group when it is already\n\t\t * under-utilized (possible with a large weight task outweighs\n\t\t * the tasks on the system).\n\t\t */\n\t\tif (prefer_sibling && sds->local &&\n\t\t    group_has_capacity(env, local) &&\n\t\t    (sgs->sum_nr_running > local->sum_nr_running + 1)) {\n\t\t\tsgs->group_no_capacity = 1;\n\t\t\tsgs->group_type = group_classify(sg, sgs);\n\t\t}\n\n\t\tif (update_sd_pick_busiest(env, sds, sg, sgs)) {\n\t\t\tsds->busiest = sg;\n\t\t\tsds->busiest_stat = *sgs;\n\t\t}\n\nnext_group:\n\t\t/* Now, start updating sd_lb_stats */\n\t\tsds->total_running += sgs->sum_nr_running;\n\t\tsds->total_load += sgs->group_load;\n\t\tsds->total_capacity += sgs->group_capacity;\n\n\t\tsg = sg->next;\n\t} while (sg != env->sd->groups);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif ((env->flags & LBF_NOHZ_AGAIN) &&\n\t    cpumask_subset(nohz.idle_cpus_mask, sched_domain_span(env->sd))) {\n\n\t\tWRITE_ONCE(nohz.next_blocked,\n\t\t\t   jiffies + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\t}\n#endif\n\n\tif (env->sd->flags & SD_NUMA)\n\t\tenv->fbq_type = fbq_classify_group(&sds->busiest_stat);\n\n\tif (!env->sd->parent) {\n\t\tstruct root_domain *rd = env->dst_rq->rd;\n\n\t\t/* update overload indicator if we are at root domain */\n\t\tWRITE_ONCE(rd->overload, sg_status & SG_OVERLOAD);\n\n\t\t/* Update over-utilization (tipping point, U >= 0) indicator */\n\t\tWRITE_ONCE(rd->overutilized, sg_status & SG_OVERUTILIZED);\n\t} else if (sg_status & SG_OVERUTILIZED) {\n\t\tWRITE_ONCE(env->dst_rq->rd->overutilized, SG_OVERUTILIZED);\n\t}\n}\n\n/**\n * check_asym_packing - Check to see if the group is packed into the\n *\t\t\tsched domain.\n *\n * This is primarily intended to used at the sibling level.  Some\n * cores like POWER7 prefer to use lower numbered SMT threads.  In the\n * case of POWER7, it can move to lower SMT modes only when higher\n * threads are idle.  When in lower SMT modes, the threads will\n * perform better since they share less core resources.  Hence when we\n * have idle threads, we want them to be the higher ones.\n *\n * This packing function is run on idle threads.  It checks to see if\n * the busiest CPU in this domain (core in the P7 case) has a higher\n * CPU number than the packing function is being run on.  Here we are\n * assuming lower CPU number will be equivalent to lower a SMT thread\n * number.\n *\n * Return: 1 when packing is required and a task should be moved to\n * this CPU.  The amount of the imbalance is returned in env->imbalance.\n *\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain which is to be packed\n */\nstatic int check_asym_packing(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tint busiest_cpu;\n\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn 0;\n\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn 0;\n\n\tif (!sds->busiest)\n\t\treturn 0;\n\n\tbusiest_cpu = sds->busiest->asym_prefer_cpu;\n\tif (sched_asym_prefer(busiest_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tenv->imbalance = DIV_ROUND_CLOSEST(\n\t\tsds->busiest_stat.avg_load * sds->busiest_stat.group_capacity,\n\t\tSCHED_CAPACITY_SCALE);\n\n\treturn 1;\n}\n\n/**\n * fix_small_imbalance - Calculate the minor imbalance that exists\n *\t\t\tamongst the groups of a sched_domain, during\n *\t\t\tload balancing.\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline\nvoid fix_small_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long tmp, capa_now = 0, capa_move = 0;\n\tunsigned int imbn = 2;\n\tunsigned long scaled_busy_load_per_task;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (!local->sum_nr_running)\n\t\tlocal->load_per_task = cpu_avg_load_per_task(env->dst_cpu);\n\telse if (busiest->load_per_task > local->load_per_task)\n\t\timbn = 1;\n\n\tscaled_busy_load_per_task =\n\t\t(busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\tbusiest->group_capacity;\n\n\tif (busiest->avg_load + scaled_busy_load_per_task >=\n\t    local->avg_load + (scaled_busy_load_per_task * imbn)) {\n\t\tenv->imbalance = busiest->load_per_task;\n\t\treturn;\n\t}\n\n\t/*\n\t * OK, we don't have enough imbalance to justify moving tasks,\n\t * however we may be able to increase total CPU capacity used by\n\t * moving them.\n\t */\n\n\tcapa_now += busiest->group_capacity *\n\t\t\tmin(busiest->load_per_task, busiest->avg_load);\n\tcapa_now += local->group_capacity *\n\t\t\tmin(local->load_per_task, local->avg_load);\n\tcapa_now /= SCHED_CAPACITY_SCALE;\n\n\t/* Amount of load we'd subtract */\n\tif (busiest->avg_load > scaled_busy_load_per_task) {\n\t\tcapa_move += busiest->group_capacity *\n\t\t\t    min(busiest->load_per_task,\n\t\t\t\tbusiest->avg_load - scaled_busy_load_per_task);\n\t}\n\n\t/* Amount of load we'd add */\n\tif (busiest->avg_load * busiest->group_capacity <\n\t    busiest->load_per_task * SCHED_CAPACITY_SCALE) {\n\t\ttmp = (busiest->avg_load * busiest->group_capacity) /\n\t\t      local->group_capacity;\n\t} else {\n\t\ttmp = (busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\t      local->group_capacity;\n\t}\n\tcapa_move += local->group_capacity *\n\t\t    min(local->load_per_task, local->avg_load + tmp);\n\tcapa_move /= SCHED_CAPACITY_SCALE;\n\n\t/* Move if we gain throughput */\n\tif (capa_move > capa_now)\n\t\tenv->imbalance = busiest->load_per_task;\n}\n\n/**\n * calculate_imbalance - Calculate the amount of imbalance present within the\n *\t\t\t groups of a given sched_domain during load balance.\n * @env: load balance environment\n * @sds: statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long max_pull, load_above_capacity = ~0UL;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (busiest->group_type == group_imbalanced) {\n\t\t/*\n\t\t * In the group_imb case we cannot rely on group-wide averages\n\t\t * to ensure CPU-load equilibrium, look at wider averages. XXX\n\t\t */\n\t\tbusiest->load_per_task =\n\t\t\tmin(busiest->load_per_task, sds->avg_load);\n\t}\n\n\t/*\n\t * Avg load of busiest sg can be less and avg load of local sg can\n\t * be greater than avg load across all sgs of sd because avg load\n\t * factors in sg capacity and sgs with smaller group_type are\n\t * skipped when updating the busiest sg:\n\t */\n\tif (busiest->group_type != group_misfit_task &&\n\t    (busiest->avg_load <= sds->avg_load ||\n\t     local->avg_load >= sds->avg_load)) {\n\t\tenv->imbalance = 0;\n\t\treturn fix_small_imbalance(env, sds);\n\t}\n\n\t/*\n\t * If there aren't any idle CPUs, avoid creating some.\n\t */\n\tif (busiest->group_type == group_overloaded &&\n\t    local->group_type   == group_overloaded) {\n\t\tload_above_capacity = busiest->sum_nr_running * SCHED_CAPACITY_SCALE;\n\t\tif (load_above_capacity > busiest->group_capacity) {\n\t\t\tload_above_capacity -= busiest->group_capacity;\n\t\t\tload_above_capacity *= scale_load_down(NICE_0_LOAD);\n\t\t\tload_above_capacity /= busiest->group_capacity;\n\t\t} else\n\t\t\tload_above_capacity = ~0UL;\n\t}\n\n\t/*\n\t * We're trying to get all the CPUs to the average_load, so we don't\n\t * want to push ourselves above the average load, nor do we wish to\n\t * reduce the max loaded CPU below the average load. At the same time,\n\t * we also don't want to reduce the group load below the group\n\t * capacity. Thus we look for the minimum possible imbalance.\n\t */\n\tmax_pull = min(busiest->avg_load - sds->avg_load, load_above_capacity);\n\n\t/* How much load to actually move to equalise the imbalance */\n\tenv->imbalance = min(\n\t\tmax_pull * busiest->group_capacity,\n\t\t(sds->avg_load - local->avg_load) * local->group_capacity\n\t) / SCHED_CAPACITY_SCALE;\n\n\t/* Boost imbalance to allow misfit task to be balanced. */\n\tif (busiest->group_type == group_misfit_task) {\n\t\tenv->imbalance = max_t(long, env->imbalance,\n\t\t\t\t       busiest->group_misfit_task_load);\n\t}\n\n\t/*\n\t * if *imbalance is less than the average load per runnable task\n\t * there is no guarantee that any tasks will be moved so we'll have\n\t * a think about bumping its value to force at least one task to be\n\t * moved\n\t */\n\tif (env->imbalance < busiest->load_per_task)\n\t\treturn fix_small_imbalance(env, sds);\n}\n\n/******* find_busiest_group() helpers end here *********************/\n\n/**\n * find_busiest_group - Returns the busiest group within the sched_domain\n * if there is an imbalance.\n *\n * Also calculates the amount of weighted load which should be moved\n * to restore balance.\n *\n * @env: The load balancing environment.\n *\n * Return:\t- The busiest group if imbalance exists.\n */\nstatic struct sched_group *find_busiest_group(struct lb_env *env)\n{\n\tstruct sg_lb_stats *local, *busiest;\n\tstruct sd_lb_stats sds;\n\n\tinit_sd_lb_stats(&sds);\n\n\t/*\n\t * Compute the various statistics relavent for load balancing at\n\t * this level.\n\t */\n\tupdate_sd_lb_stats(env, &sds);\n\n\tif (static_branch_unlikely(&sched_energy_present)) {\n\t\tstruct root_domain *rd = env->dst_rq->rd;\n\n\t\tif (rcu_dereference(rd->pd) && !READ_ONCE(rd->overutilized))\n\t\t\tgoto out_balanced;\n\t}\n\n\tlocal = &sds.local_stat;\n\tbusiest = &sds.busiest_stat;\n\n\t/* ASYM feature bypasses nice load balance check */\n\tif (check_asym_packing(env, &sds))\n\t\treturn sds.busiest;\n\n\t/* There is no busy sibling group to pull tasks from */\n\tif (!sds.busiest || busiest->sum_nr_running == 0)\n\t\tgoto out_balanced;\n\n\t/* XXX broken for overlapping NUMA groups */\n\tsds.avg_load = (SCHED_CAPACITY_SCALE * sds.total_load)\n\t\t\t\t\t\t/ sds.total_capacity;\n\n\t/*\n\t * If the busiest group is imbalanced the below checks don't\n\t * work because they assume all things are equal, which typically\n\t * isn't true due to cpus_allowed constraints and the like.\n\t */\n\tif (busiest->group_type == group_imbalanced)\n\t\tgoto force_balance;\n\n\t/*\n\t * When dst_cpu is idle, prevent SMP nice and/or asymmetric group\n\t * capacities from resulting in underutilization due to avg_load.\n\t */\n\tif (env->idle != CPU_NOT_IDLE && group_has_capacity(env, local) &&\n\t    busiest->group_no_capacity)\n\t\tgoto force_balance;\n\n\t/* Misfit tasks should be dealt with regardless of the avg load */\n\tif (busiest->group_type == group_misfit_task)\n\t\tgoto force_balance;\n\n\t/*\n\t * If the local group is busier than the selected busiest group\n\t * don't try and pull any tasks.\n\t */\n\tif (local->avg_load >= busiest->avg_load)\n\t\tgoto out_balanced;\n\n\t/*\n\t * Don't pull any tasks if this group is already above the domain\n\t * average load.\n\t */\n\tif (local->avg_load >= sds.avg_load)\n\t\tgoto out_balanced;\n\n\tif (env->idle == CPU_IDLE) {\n\t\t/*\n\t\t * This CPU is idle. If the busiest group is not overloaded\n\t\t * and there is no imbalance between this and busiest group\n\t\t * wrt idle CPUs, it is balanced. The imbalance becomes\n\t\t * significant if the diff is greater than 1 otherwise we\n\t\t * might end up to just move the imbalance on another group\n\t\t */\n\t\tif ((busiest->group_type != group_overloaded) &&\n\t\t\t\t(local->idle_cpus <= (busiest->idle_cpus + 1)))\n\t\t\tgoto out_balanced;\n\t} else {\n\t\t/*\n\t\t * In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use\n\t\t * imbalance_pct to be conservative.\n\t\t */\n\t\tif (100 * busiest->avg_load <=\n\t\t\t\tenv->sd->imbalance_pct * local->avg_load)\n\t\t\tgoto out_balanced;\n\t}\n\nforce_balance:\n\t/* Looks like there is an imbalance. Compute it */\n\tenv->src_grp_type = busiest->group_type;\n\tcalculate_imbalance(env, &sds);\n\treturn env->imbalance ? sds.busiest : NULL;\n\nout_balanced:\n\tenv->imbalance = 0;\n\treturn NULL;\n}\n\n/*\n * find_busiest_queue - find the busiest runqueue among the CPUs in the group.\n */\nstatic struct rq *find_busiest_queue(struct lb_env *env,\n\t\t\t\t     struct sched_group *group)\n{\n\tstruct rq *busiest = NULL, *rq;\n\tunsigned long busiest_load = 0, busiest_capacity = 1;\n\tint i;\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tunsigned long capacity, wl;\n\t\tenum fbq_type rt;\n\n\t\trq = cpu_rq(i);\n\t\trt = fbq_classify_rq(rq);\n\n\t\t/*\n\t\t * We classify groups/runqueues into three groups:\n\t\t *  - regular: there are !numa tasks\n\t\t *  - remote:  there are numa tasks that run on the 'wrong' node\n\t\t *  - all:     there is no distinction\n\t\t *\n\t\t * In order to avoid migrating ideally placed numa tasks,\n\t\t * ignore those when there's better options.\n\t\t *\n\t\t * If we ignore the actual busiest queue to migrate another\n\t\t * task, the next balance pass can still reduce the busiest\n\t\t * queue by moving tasks around inside the node.\n\t\t *\n\t\t * If we cannot move enough load due to this classification\n\t\t * the next pass will adjust the group classification and\n\t\t * allow migration of more tasks.\n\t\t *\n\t\t * Both cases only affect the total convergence complexity.\n\t\t */\n\t\tif (rt > env->fbq_type)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains with misfit tasks we simply\n\t\t * seek the \"biggest\" misfit task.\n\t\t */\n\t\tif (env->src_grp_type == group_misfit_task) {\n\t\t\tif (rq->misfit_task_load > busiest_load) {\n\t\t\t\tbusiest_load = rq->misfit_task_load;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tcapacity = capacity_of(i);\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains, don't pick a CPU that could\n\t\t * eventually lead to active_balancing high->low capacity.\n\t\t * Higher per-CPU capacity is considered better than balancing\n\t\t * average load.\n\t\t */\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    capacity_of(env->dst_cpu) < capacity &&\n\t\t    rq->nr_running == 1)\n\t\t\tcontinue;\n\n\t\twl = weighted_cpuload(rq);\n\n\t\t/*\n\t\t * When comparing with imbalance, use weighted_cpuload()\n\t\t * which is not scaled with the CPU capacity.\n\t\t */\n\n\t\tif (rq->nr_running == 1 && wl > env->imbalance &&\n\t\t    !check_cpu_capacity(rq, env->sd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For the load comparisons with the other CPU's, consider\n\t\t * the weighted_cpuload() scaled with the CPU capacity, so\n\t\t * that the load can be moved away from the CPU that is\n\t\t * potentially running at a lower capacity.\n\t\t *\n\t\t * Thus we're looking for max(wl_i / capacity_i), crosswise\n\t\t * multiplication to rid ourselves of the division works out\n\t\t * to: wl_i * capacity_j > wl_j * capacity_i;  where j is\n\t\t * our previous maximum.\n\t\t */\n\t\tif (wl * busiest_capacity > busiest_load * capacity) {\n\t\t\tbusiest_load = wl;\n\t\t\tbusiest_capacity = capacity;\n\t\t\tbusiest = rq;\n\t\t}\n\t}\n\n\treturn busiest;\n}\n\n/*\n * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but\n * so long as it is large enough.\n */\n#define MAX_PINNED_INTERVAL\t512\n\nstatic int need_active_balance(struct lb_env *env)\n{\n\tstruct sched_domain *sd = env->sd;\n\n\tif (env->idle == CPU_NEWLY_IDLE) {\n\n\t\t/*\n\t\t * ASYM_PACKING needs to force migrate tasks from busy but\n\t\t * lower priority CPUs in order to pack all tasks in the\n\t\t * highest priority CPUs.\n\t\t */\n\t\tif ((sd->flags & SD_ASYM_PACKING) &&\n\t\t    sched_asym_prefer(env->dst_cpu, env->src_cpu))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * The dst_cpu is idle and the src_cpu CPU has only 1 CFS task.\n\t * It's worth migrating the task if the src_cpu's capacity is reduced\n\t * because of other sched_class or IRQs if more capacity stays\n\t * available on dst_cpu.\n\t */\n\tif ((env->idle != CPU_NOT_IDLE) &&\n\t    (env->src_rq->cfs.h_nr_running == 1)) {\n\t\tif ((check_cpu_capacity(env->src_rq, sd)) &&\n\t\t    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))\n\t\t\treturn 1;\n\t}\n\n\tif (env->src_grp_type == group_misfit_task)\n\t\treturn 1;\n\n\treturn unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);\n}\n\nstatic int active_load_balance_cpu_stop(void *data);\n\nstatic int should_we_balance(struct lb_env *env)\n{\n\tstruct sched_group *sg = env->sd->groups;\n\tint cpu, balance_cpu = -1;\n\n\t/*\n\t * Ensure the balancing environment is consistent; can happen\n\t * when the softirq triggers 'during' hotplug.\n\t */\n\tif (!cpumask_test_cpu(env->dst_cpu, env->cpus))\n\t\treturn 0;\n\n\t/*\n\t * In the newly idle case, we will allow all the CPUs\n\t * to do the newly idle load balance.\n\t */\n\tif (env->idle == CPU_NEWLY_IDLE)\n\t\treturn 1;\n\n\t/* Try to find first idle CPU */\n\tfor_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {\n\t\tif (!idle_cpu(cpu))\n\t\t\tcontinue;\n\n\t\tbalance_cpu = cpu;\n\t\tbreak;\n\t}\n\n\tif (balance_cpu == -1)\n\t\tbalance_cpu = group_balance_cpu(sg);\n\n\t/*\n\t * First idle CPU or the first CPU(busiest) in this sched group\n\t * is eligible for doing load balancing at this and above domains.\n\t */\n\treturn balance_cpu == env->dst_cpu;\n}\n\n/*\n * Check this_cpu to ensure it is balanced within domain. Attempt to move\n * tasks if there is an imbalance.\n */\nstatic int load_balance(int this_cpu, struct rq *this_rq,\n\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n\t\t\tint *continue_balancing)\n{\n\tint ld_moved, cur_ld_moved, active_balance = 0;\n\tstruct sched_domain *sd_parent = sd->parent;\n\tstruct sched_group *group;\n\tstruct rq *busiest;\n\tstruct rq_flags rf;\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);\n\n\tstruct lb_env env = {\n\t\t.sd\t\t= sd,\n\t\t.dst_cpu\t= this_cpu,\n\t\t.dst_rq\t\t= this_rq,\n\t\t.dst_grpmask    = sched_group_span(sd->groups),\n\t\t.idle\t\t= idle,\n\t\t.loop_break\t= sched_nr_migrate_break,\n\t\t.cpus\t\t= cpus,\n\t\t.fbq_type\t= all,\n\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),\n\t};\n\n\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);\n\n\tschedstat_inc(sd->lb_count[idle]);\n\nredo:\n\tif (!should_we_balance(&env)) {\n\t\t*continue_balancing = 0;\n\t\tgoto out_balanced;\n\t}\n\n\tgroup = find_busiest_group(&env);\n\tif (!group) {\n\t\tschedstat_inc(sd->lb_nobusyg[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tbusiest = find_busiest_queue(&env, group);\n\tif (!busiest) {\n\t\tschedstat_inc(sd->lb_nobusyq[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tBUG_ON(busiest == env.dst_rq);\n\n\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);\n\n\tenv.src_cpu = busiest->cpu;\n\tenv.src_rq = busiest;\n\n\tld_moved = 0;\n\tif (busiest->nr_running > 1) {\n\t\t/*\n\t\t * Attempt to move tasks. If find_busiest_group has found\n\t\t * an imbalance but busiest->nr_running <= 1, the group is\n\t\t * still unbalanced. ld_moved simply stays zero, so it is\n\t\t * correctly treated as an imbalance.\n\t\t */\n\t\tenv.flags |= LBF_ALL_PINNED;\n\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);\n\nmore_balance:\n\t\trq_lock_irqsave(busiest, &rf);\n\t\tupdate_rq_clock(busiest);\n\n\t\t/*\n\t\t * cur_ld_moved - load moved in current iteration\n\t\t * ld_moved     - cumulative load moved across iterations\n\t\t */\n\t\tcur_ld_moved = detach_tasks(&env);\n\n\t\t/*\n\t\t * We've detached some tasks from busiest_rq. Every\n\t\t * task is masked \"TASK_ON_RQ_MIGRATING\", so we can safely\n\t\t * unlock busiest->lock, and we are able to be sure\n\t\t * that nobody can manipulate the tasks in parallel.\n\t\t * See task_rq_lock() family for the details.\n\t\t */\n\n\t\trq_unlock(busiest, &rf);\n\n\t\tif (cur_ld_moved) {\n\t\t\tattach_tasks(&env);\n\t\t\tld_moved += cur_ld_moved;\n\t\t}\n\n\t\tlocal_irq_restore(rf.flags);\n\n\t\tif (env.flags & LBF_NEED_BREAK) {\n\t\t\tenv.flags &= ~LBF_NEED_BREAK;\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * Revisit (affine) tasks on src_cpu that couldn't be moved to\n\t\t * us and move them to an alternate dst_cpu in our sched_group\n\t\t * where they can run. The upper limit on how many times we\n\t\t * iterate on same src_cpu is dependent on number of CPUs in our\n\t\t * sched_group.\n\t\t *\n\t\t * This changes load balance semantics a bit on who can move\n\t\t * load to a given_cpu. In addition to the given_cpu itself\n\t\t * (or a ilb_cpu acting on its behalf where given_cpu is\n\t\t * nohz-idle), we now have balance_cpu in a position to move\n\t\t * load to given_cpu. In rare situations, this may cause\n\t\t * conflicts (balance_cpu and given_cpu/ilb_cpu deciding\n\t\t * _independently_ and at _same_ time to move some load to\n\t\t * given_cpu) causing exceess load to be moved to given_cpu.\n\t\t * This however should not happen so much in practice and\n\t\t * moreover subsequent load balance cycles should correct the\n\t\t * excess load moved.\n\t\t */\n\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {\n\n\t\t\t/* Prevent to re-select dst_cpu via env's CPUs */\n\t\t\tcpumask_clear_cpu(env.dst_cpu, env.cpus);\n\n\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);\n\t\t\tenv.dst_cpu\t = env.new_dst_cpu;\n\t\t\tenv.flags\t&= ~LBF_DST_PINNED;\n\t\t\tenv.loop\t = 0;\n\t\t\tenv.loop_break\t = sched_nr_migrate_break;\n\n\t\t\t/*\n\t\t\t * Go back to \"more_balance\" rather than \"redo\" since we\n\t\t\t * need to continue with same src_cpu.\n\t\t\t */\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * We failed to reach balance because of affinity.\n\t\t */\n\t\tif (sd_parent) {\n\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)\n\t\t\t\t*group_imbalance = 1;\n\t\t}\n\n\t\t/* All tasks on this runqueue were pinned by CPU affinity */\n\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {\n\t\t\tcpumask_clear_cpu(cpu_of(busiest), cpus);\n\t\t\t/*\n\t\t\t * Attempting to continue load balancing at the current\n\t\t\t * sched_domain level only makes sense if there are\n\t\t\t * active CPUs remaining as possible busiest CPUs to\n\t\t\t * pull load from which are not contained within the\n\t\t\t * destination group that is receiving any migrated\n\t\t\t * load.\n\t\t\t */\n\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {\n\t\t\t\tenv.loop = 0;\n\t\t\t\tenv.loop_break = sched_nr_migrate_break;\n\t\t\t\tgoto redo;\n\t\t\t}\n\t\t\tgoto out_all_pinned;\n\t\t}\n\t}\n\n\tif (!ld_moved) {\n\t\tschedstat_inc(sd->lb_failed[idle]);\n\t\t/*\n\t\t * Increment the failure counter only on periodic balance.\n\t\t * We do not want newidle balance, which can be very\n\t\t * frequent, pollute the failure counter causing\n\t\t * excessive cache_hot migrations and active balances.\n\t\t */\n\t\tif (idle != CPU_NEWLY_IDLE)\n\t\t\tsd->nr_balance_failed++;\n\n\t\tif (need_active_balance(&env)) {\n\t\t\tunsigned long flags;\n\n\t\t\traw_spin_lock_irqsave(&busiest->lock, flags);\n\n\t\t\t/*\n\t\t\t * Don't kick the active_load_balance_cpu_stop,\n\t\t\t * if the curr task on busiest CPU can't be\n\t\t\t * moved to this_cpu:\n\t\t\t */\n\t\t\tif (!cpumask_test_cpu(this_cpu, &busiest->curr->cpus_allowed)) {\n\t\t\t\traw_spin_unlock_irqrestore(&busiest->lock,\n\t\t\t\t\t\t\t    flags);\n\t\t\t\tenv.flags |= LBF_ALL_PINNED;\n\t\t\t\tgoto out_one_pinned;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * ->active_balance synchronizes accesses to\n\t\t\t * ->active_balance_work.  Once set, it's cleared\n\t\t\t * only after active load balance is finished.\n\t\t\t */\n\t\t\tif (!busiest->active_balance) {\n\t\t\t\tbusiest->active_balance = 1;\n\t\t\t\tbusiest->push_cpu = this_cpu;\n\t\t\t\tactive_balance = 1;\n\t\t\t}\n\t\t\traw_spin_unlock_irqrestore(&busiest->lock, flags);\n\n\t\t\tif (active_balance) {\n\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),\n\t\t\t\t\tactive_load_balance_cpu_stop, busiest,\n\t\t\t\t\t&busiest->active_balance_work);\n\t\t\t}\n\n\t\t\t/* We've kicked active balancing, force task migration. */\n\t\t\tsd->nr_balance_failed = sd->cache_nice_tries+1;\n\t\t}\n\t} else\n\t\tsd->nr_balance_failed = 0;\n\n\tif (likely(!active_balance)) {\n\t\t/* We were unbalanced, so reset the balancing interval */\n\t\tsd->balance_interval = sd->min_interval;\n\t} else {\n\t\t/*\n\t\t * If we've begun active balancing, start to back off. This\n\t\t * case may not be covered by the all_pinned logic if there\n\t\t * is only 1 task on the busy runqueue (because we don't call\n\t\t * detach_tasks).\n\t\t */\n\t\tif (sd->balance_interval < sd->max_interval)\n\t\t\tsd->balance_interval *= 2;\n\t}\n\n\tgoto out;\n\nout_balanced:\n\t/*\n\t * We reach balance although we may have faced some affinity\n\t * constraints. Clear the imbalance flag if it was set.\n\t */\n\tif (sd_parent) {\n\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\tif (*group_imbalance)\n\t\t\t*group_imbalance = 0;\n\t}\n\nout_all_pinned:\n\t/*\n\t * We reach balance because all tasks are pinned at this level so\n\t * we can't migrate them. Let the imbalance flag set so parent level\n\t * can try to migrate them.\n\t */\n\tschedstat_inc(sd->lb_balanced[idle]);\n\n\tsd->nr_balance_failed = 0;\n\nout_one_pinned:\n\tld_moved = 0;\n\n\t/*\n\t * idle_balance() disregards balance intervals, so we could repeatedly\n\t * reach this code, which would lead to balance_interval skyrocketting\n\t * in a short amount of time. Skip the balance_interval increase logic\n\t * to avoid that.\n\t */\n\tif (env.idle == CPU_NEWLY_IDLE)\n\t\tgoto out;\n\n\t/* tune up the balancing interval */\n\tif ((env.flags & LBF_ALL_PINNED &&\n\t     sd->balance_interval < MAX_PINNED_INTERVAL) ||\n\t    sd->balance_interval < sd->max_interval)\n\t\tsd->balance_interval *= 2;\nout:\n\treturn ld_moved;\n}\n\nstatic inline unsigned long\nget_sd_balance_interval(struct sched_domain *sd, int cpu_busy)\n{\n\tunsigned long interval = sd->balance_interval;\n\n\tif (cpu_busy)\n\t\tinterval *= sd->busy_factor;\n\n\t/* scale ms to jiffies */\n\tinterval = msecs_to_jiffies(interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\n\treturn interval;\n}\n\nstatic inline void\nupdate_next_balance(struct sched_domain *sd, unsigned long *next_balance)\n{\n\tunsigned long interval, next;\n\n\t/* used by idle balance, so cpu_busy = 0 */\n\tinterval = get_sd_balance_interval(sd, 0);\n\tnext = sd->last_balance + interval;\n\n\tif (time_after(*next_balance, next))\n\t\t*next_balance = next;\n}\n\n/*\n * active_load_balance_cpu_stop is run by the CPU stopper. It pushes\n * running tasks off the busiest CPU onto idle CPUs. It requires at\n * least 1 task to be running on each physical CPU where possible, and\n * avoids physical / logical imbalances.\n */\nstatic int active_load_balance_cpu_stop(void *data)\n{\n\tstruct rq *busiest_rq = data;\n\tint busiest_cpu = cpu_of(busiest_rq);\n\tint target_cpu = busiest_rq->push_cpu;\n\tstruct rq *target_rq = cpu_rq(target_cpu);\n\tstruct sched_domain *sd;\n\tstruct task_struct *p = NULL;\n\tstruct rq_flags rf;\n\n\trq_lock_irq(busiest_rq, &rf);\n\t/*\n\t * Between queueing the stop-work and running it is a hole in which\n\t * CPUs can become inactive. We should not move tasks from or to\n\t * inactive CPUs.\n\t */\n\tif (!cpu_active(busiest_cpu) || !cpu_active(target_cpu))\n\t\tgoto out_unlock;\n\n\t/* Make sure the requested CPU hasn't gone down in the meantime: */\n\tif (unlikely(busiest_cpu != smp_processor_id() ||\n\t\t     !busiest_rq->active_balance))\n\t\tgoto out_unlock;\n\n\t/* Is there any task to move? */\n\tif (busiest_rq->nr_running <= 1)\n\t\tgoto out_unlock;\n\n\t/*\n\t * This condition is \"impossible\", if it occurs\n\t * we need to fix it. Originally reported by\n\t * Bjorn Helgaas on a 128-CPU setup.\n\t */\n\tBUG_ON(busiest_rq == target_rq);\n\n\t/* Search for an sd spanning us and the target CPU. */\n\trcu_read_lock();\n\tfor_each_domain(target_cpu, sd) {\n\t\tif ((sd->flags & SD_LOAD_BALANCE) &&\n\t\t    cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))\n\t\t\t\tbreak;\n\t}\n\n\tif (likely(sd)) {\n\t\tstruct lb_env env = {\n\t\t\t.sd\t\t= sd,\n\t\t\t.dst_cpu\t= target_cpu,\n\t\t\t.dst_rq\t\t= target_rq,\n\t\t\t.src_cpu\t= busiest_rq->cpu,\n\t\t\t.src_rq\t\t= busiest_rq,\n\t\t\t.idle\t\t= CPU_IDLE,\n\t\t\t/*\n\t\t\t * can_migrate_task() doesn't need to compute new_dst_cpu\n\t\t\t * for active balancing. Since we have CPU_IDLE, but no\n\t\t\t * @dst_grpmask we need to make that test go away with lying\n\t\t\t * about DST_PINNED.\n\t\t\t */\n\t\t\t.flags\t\t= LBF_DST_PINNED,\n\t\t};\n\n\t\tschedstat_inc(sd->alb_count);\n\t\tupdate_rq_clock(busiest_rq);\n\n\t\tp = detach_one_task(&env);\n\t\tif (p) {\n\t\t\tschedstat_inc(sd->alb_pushed);\n\t\t\t/* Active balancing done, reset the failure counter. */\n\t\t\tsd->nr_balance_failed = 0;\n\t\t} else {\n\t\t\tschedstat_inc(sd->alb_failed);\n\t\t}\n\t}\n\trcu_read_unlock();\nout_unlock:\n\tbusiest_rq->active_balance = 0;\n\trq_unlock(busiest_rq, &rf);\n\n\tif (p)\n\t\tattach_one_task(target_rq, p);\n\n\tlocal_irq_enable();\n\n\treturn 0;\n}\n\nstatic DEFINE_SPINLOCK(balancing);\n\n/*\n * Scale the max load_balance interval with the number of CPUs in the system.\n * This trades load-balance latency on larger machines for less cross talk.\n */\nvoid update_max_interval(void)\n{\n\tmax_load_balance_interval = HZ*num_online_cpus()/10;\n}\n\n/*\n * It checks each scheduling domain to see if it is due to be balanced,\n * and initiates a balancing operation if so.\n *\n * Balancing parameters are set up in init_sched_domains.\n */\nstatic void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)\n{\n\tint continue_balancing = 1;\n\tint cpu = rq->cpu;\n\tunsigned long interval;\n\tstruct sched_domain *sd;\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long next_balance = jiffies + 60*HZ;\n\tint update_next_balance = 0;\n\tint need_serialize, need_decay = 0;\n\tu64 max_cost = 0;\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\t/*\n\t\t * Decay the newidle max times here because this is a regular\n\t\t * visit to all the domains. Decay ~1% per second.\n\t\t */\n\t\tif (time_after(jiffies, sd->next_decay_max_lb_cost)) {\n\t\t\tsd->max_newidle_lb_cost =\n\t\t\t\t(sd->max_newidle_lb_cost * 253) / 256;\n\t\t\tsd->next_decay_max_lb_cost = jiffies + HZ;\n\t\t\tneed_decay = 1;\n\t\t}\n\t\tmax_cost += sd->max_newidle_lb_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Stop the load balance at this level. There is another\n\t\t * CPU in our sched group which is doing load balancing more\n\t\t * actively.\n\t\t */\n\t\tif (!continue_balancing) {\n\t\t\tif (need_decay)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\n\t\tneed_serialize = sd->flags & SD_SERIALIZE;\n\t\tif (need_serialize) {\n\t\t\tif (!spin_trylock(&balancing))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (time_after_eq(jiffies, sd->last_balance + interval)) {\n\t\t\tif (load_balance(cpu, rq, sd, idle, &continue_balancing)) {\n\t\t\t\t/*\n\t\t\t\t * The LBF_DST_PINNED logic could have changed\n\t\t\t\t * env->dst_cpu, so we can't know our idle\n\t\t\t\t * state even if we migrated tasks. Update it.\n\t\t\t\t */\n\t\t\t\tidle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;\n\t\t\t}\n\t\t\tsd->last_balance = jiffies;\n\t\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\t\t}\n\t\tif (need_serialize)\n\t\t\tspin_unlock(&balancing);\nout:\n\t\tif (time_after(next_balance, sd->last_balance + interval)) {\n\t\t\tnext_balance = sd->last_balance + interval;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\tif (need_decay) {\n\t\t/*\n\t\t * Ensure the rq-wide value also decays but keep it at a\n\t\t * reasonable floor to avoid funnies with rq->avg_idle.\n\t\t */\n\t\trq->max_idle_balance_cost =\n\t\t\tmax((u64)sysctl_sched_migration_cost, max_cost);\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the cpu is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance)) {\n\t\trq->next_balance = next_balance;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\t\t/*\n\t\t * If this CPU has been elected to perform the nohz idle\n\t\t * balance. Other idle CPUs have already rebalanced with\n\t\t * nohz_idle_balance() and nohz.next_balance has been\n\t\t * updated accordingly. This CPU is now running the idle load\n\t\t * balance for itself and we need to update the\n\t\t * nohz.next_balance accordingly.\n\t\t */\n\t\tif ((idle == CPU_IDLE) && time_after(nohz.next_balance, rq->next_balance))\n\t\t\tnohz.next_balance = rq->next_balance;\n#endif\n\t}\n}\n\nstatic inline int on_null_domain(struct rq *rq)\n{\n\treturn unlikely(!rcu_dereference_sched(rq->sd));\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * idle load balancing details\n * - When one of the busy CPUs notice that there may be an idle rebalancing\n *   needed, they will kick the idle load balancer, which then does idle\n *   load balancing for all the idle CPUs.\n */\n\nstatic inline int find_new_ilb(void)\n{\n\tint ilb = cpumask_first(nohz.idle_cpus_mask);\n\n\tif (ilb < nr_cpu_ids && idle_cpu(ilb))\n\t\treturn ilb;\n\n\treturn nr_cpu_ids;\n}\n\n/*\n * Kick a CPU to do the nohz balancing, if it is time for it. We pick the\n * nohz_load_balancer CPU (if there is one) otherwise fallback to any idle\n * CPU (if there is one).\n */\nstatic void kick_ilb(unsigned int flags)\n{\n\tint ilb_cpu;\n\n\tnohz.next_balance++;\n\n\tilb_cpu = find_new_ilb();\n\n\tif (ilb_cpu >= nr_cpu_ids)\n\t\treturn;\n\n\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));\n\tif (flags & NOHZ_KICK_MASK)\n\t\treturn;\n\n\t/*\n\t * Use smp_send_reschedule() instead of resched_cpu().\n\t * This way we generate a sched IPI on the target CPU which\n\t * is idle. And the softirq performing nohz idle load balance\n\t * will be run before returning from the IPI.\n\t */\n\tsmp_send_reschedule(ilb_cpu);\n}\n\n/*\n * Current heuristic for kicking the idle load balancer in the presence\n * of an idle cpu in the system.\n *   - This rq has more than one task.\n *   - This rq has at least one CFS task and the capacity of the CPU is\n *     significantly reduced because of RT tasks or IRQs.\n *   - At parent of LLC scheduler domain level, this cpu's scheduler group has\n *     multiple busy cpu.\n *   - For SD_ASYM_PACKING, if the lower numbered cpu's in the scheduler\n *     domain span are idle.\n */\nstatic void nohz_balancer_kick(struct rq *rq)\n{\n\tunsigned long now = jiffies;\n\tstruct sched_domain_shared *sds;\n\tstruct sched_domain *sd;\n\tint nr_busy, i, cpu = rq->cpu;\n\tunsigned int flags = 0;\n\n\tif (unlikely(rq->idle_balance))\n\t\treturn;\n\n\t/*\n\t * We may be recently in ticked or tickless idle mode. At the first\n\t * busy tick after returning from idle, we will update the busy stats.\n\t */\n\tnohz_balance_exit_idle(rq);\n\n\t/*\n\t * None are in tickless mode and hence no need for NOHZ idle load\n\t * balancing.\n\t */\n\tif (likely(!atomic_read(&nohz.nr_cpus)))\n\t\treturn;\n\n\tif (READ_ONCE(nohz.has_blocked) &&\n\t    time_after(now, READ_ONCE(nohz.next_blocked)))\n\t\tflags = NOHZ_STATS_KICK;\n\n\tif (time_before(now, nohz.next_balance))\n\t\tgoto out;\n\n\tif (rq->nr_running >= 2 || rq->misfit_task_load) {\n\t\tflags = NOHZ_KICK_MASK;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds) {\n\t\t/*\n\t\t * XXX: write a coherent comment on why we do this.\n\t\t * See also: http://lkml.kernel.org/r/20111202010832.602203411@sbsiddha-desk.sc.intel.com\n\t\t */\n\t\tnr_busy = atomic_read(&sds->nr_busy_cpus);\n\t\tif (nr_busy > 1) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\n\t}\n\n\tsd = rcu_dereference(rq->sd);\n\tif (sd) {\n\t\tif ((rq->cfs.h_nr_running >= 1) &&\n\t\t\t\tcheck_cpu_capacity(rq, sd)) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_asym_packing, cpu));\n\tif (sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\t\tif (i == cpu ||\n\t\t\t    !cpumask_test_cpu(i, nohz.idle_cpus_mask))\n\t\t\t\tcontinue;\n\n\t\t\tif (sched_asym_prefer(i, cpu)) {\n\t\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\nout:\n\tif (flags)\n\t\tkick_ilb(flags);\n}\n\nstatic void set_cpu_sd_state_busy(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || !sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 0;\n\n\tatomic_inc(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\nvoid nohz_balance_exit_idle(struct rq *rq)\n{\n\tSCHED_WARN_ON(rq != this_rq());\n\n\tif (likely(!rq->nohz_tick_stopped))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 0;\n\tcpumask_clear_cpu(rq->cpu, nohz.idle_cpus_mask);\n\tatomic_dec(&nohz.nr_cpus);\n\n\tset_cpu_sd_state_busy(rq->cpu);\n}\n\nstatic void set_cpu_sd_state_idle(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 1;\n\n\tatomic_dec(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\n/*\n * This routine will record that the CPU is going idle with tick stopped.\n * This info will be used in performing idle load balancing in the future.\n */\nvoid nohz_balance_enter_idle(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tSCHED_WARN_ON(cpu != smp_processor_id());\n\n\t/* If this CPU is going down, then nothing needs to be done: */\n\tif (!cpu_active(cpu))\n\t\treturn;\n\n\t/* Spare idle load balancing on CPUs that don't want to be disturbed: */\n\tif (!housekeeping_cpu(cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/*\n\t * Can be set safely without rq->lock held\n\t * If a clear happens, it will have evaluated last additions because\n\t * rq->lock is held during the check and the clear\n\t */\n\trq->has_blocked_load = 1;\n\n\t/*\n\t * The tick is still stopped but load could have been added in the\n\t * meantime. We set the nohz.has_blocked flag to trig a check of the\n\t * *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear\n\t * of nohz.has_blocked can only happen after checking the new load\n\t */\n\tif (rq->nohz_tick_stopped)\n\t\tgoto out;\n\n\t/* If we're a completely isolated CPU, we don't play: */\n\tif (on_null_domain(rq))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 1;\n\n\tcpumask_set_cpu(cpu, nohz.idle_cpus_mask);\n\tatomic_inc(&nohz.nr_cpus);\n\n\t/*\n\t * Ensures that if nohz_idle_balance() fails to observe our\n\t * @idle_cpus_mask store, it must observe the @has_blocked\n\t * store.\n\t */\n\tsmp_mb__after_atomic();\n\n\tset_cpu_sd_state_idle(cpu);\n\nout:\n\t/*\n\t * Each time a cpu enter idle, we assume that it has blocked load and\n\t * enable the periodic update of the load of idle cpus\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 1);\n}\n\n/*\n * Internal function that runs load balance for all idle cpus. The load balance\n * can be a simple update of blocked load or a complete load balance with\n * tasks movement depending of flags.\n * The function returns false if the loop has stopped before running\n * through all idle CPUs.\n */\nstatic bool _nohz_idle_balance(struct rq *this_rq, unsigned int flags,\n\t\t\t       enum cpu_idle_type idle)\n{\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long now = jiffies;\n\tunsigned long next_balance = now + 60*HZ;\n\tbool has_blocked_load = false;\n\tint update_next_balance = 0;\n\tint this_cpu = this_rq->cpu;\n\tint balance_cpu;\n\tint ret = false;\n\tstruct rq *rq;\n\n\tSCHED_WARN_ON((flags & NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK);\n\n\t/*\n\t * We assume there will be no idle load after this update and clear\n\t * the has_blocked flag. If a cpu enters idle in the mean time, it will\n\t * set the has_blocked flag and trig another update of idle load.\n\t * Because a cpu that becomes idle, is added to idle_cpus_mask before\n\t * setting the flag, we are sure to not clear the state and not\n\t * check the load of an idle cpu.\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 0);\n\n\t/*\n\t * Ensures that if we miss the CPU, we must see the has_blocked\n\t * store from nohz_balance_enter_idle().\n\t */\n\tsmp_mb();\n\n\tfor_each_cpu(balance_cpu, nohz.idle_cpus_mask) {\n\t\tif (balance_cpu == this_cpu || !idle_cpu(balance_cpu))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If this CPU gets work to do, stop the load balancing\n\t\t * work being done for other CPUs. Next load\n\t\t * balancing owner will pick it up.\n\t\t */\n\t\tif (need_resched()) {\n\t\t\thas_blocked_load = true;\n\t\t\tgoto abort;\n\t\t}\n\n\t\trq = cpu_rq(balance_cpu);\n\n\t\thas_blocked_load |= update_nohz_stats(rq, true);\n\n\t\t/*\n\t\t * If time for next balance is due,\n\t\t * do the balance.\n\t\t */\n\t\tif (time_after_eq(jiffies, rq->next_balance)) {\n\t\t\tstruct rq_flags rf;\n\n\t\t\trq_lock_irqsave(rq, &rf);\n\t\t\tupdate_rq_clock(rq);\n\t\t\tcpu_load_update_idle(rq);\n\t\t\trq_unlock_irqrestore(rq, &rf);\n\n\t\t\tif (flags & NOHZ_BALANCE_KICK)\n\t\t\t\trebalance_domains(rq, CPU_IDLE);\n\t\t}\n\n\t\tif (time_after(next_balance, rq->next_balance)) {\n\t\t\tnext_balance = rq->next_balance;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\n\t/* Newly idle CPU doesn't need an update */\n\tif (idle != CPU_NEWLY_IDLE) {\n\t\tupdate_blocked_averages(this_cpu);\n\t\thas_blocked_load |= this_rq->has_blocked_load;\n\t}\n\n\tif (flags & NOHZ_BALANCE_KICK)\n\t\trebalance_domains(this_rq, CPU_IDLE);\n\n\tWRITE_ONCE(nohz.next_blocked,\n\t\tnow + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\n\t/* The full idle balance loop has been done */\n\tret = true;\n\nabort:\n\t/* There is still blocked load, enable periodic update */\n\tif (has_blocked_load)\n\t\tWRITE_ONCE(nohz.has_blocked, 1);\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the CPU is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance))\n\t\tnohz.next_balance = next_balance;\n\n\treturn ret;\n}\n\n/*\n * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the\n * rebalancing for all the cpus for whom scheduler ticks are stopped.\n */\nstatic bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\tint this_cpu = this_rq->cpu;\n\tunsigned int flags;\n\n\tif (!(atomic_read(nohz_flags(this_cpu)) & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\tif (idle != CPU_IDLE) {\n\t\tatomic_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\t\treturn false;\n\t}\n\n\t/* could be _relaxed() */\n\tflags = atomic_fetch_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\tif (!(flags & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\t_nohz_idle_balance(this_rq, flags, idle);\n\n\treturn true;\n}\n\nstatic void nohz_newidle_balance(struct rq *this_rq)\n{\n\tint this_cpu = this_rq->cpu;\n\n\t/*\n\t * This CPU doesn't want to be disturbed by scheduler\n\t * housekeeping\n\t */\n\tif (!housekeeping_cpu(this_cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/* Will wake up very soon. No time for doing anything else*/\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost)\n\t\treturn;\n\n\t/* Don't need to update blocked load of idle CPUs*/\n\tif (!READ_ONCE(nohz.has_blocked) ||\n\t    time_before(jiffies, READ_ONCE(nohz.next_blocked)))\n\t\treturn;\n\n\traw_spin_unlock(&this_rq->lock);\n\t/*\n\t * This CPU is going to be idle and blocked load of idle CPUs\n\t * need to be updated. Run the ilb locally as it is a good\n\t * candidate for ilb instead of waking up another idle CPU.\n\t * Kick an normal ilb if we failed to do the update.\n\t */\n\tif (!_nohz_idle_balance(this_rq, NOHZ_STATS_KICK, CPU_NEWLY_IDLE))\n\t\tkick_ilb(NOHZ_STATS_KICK);\n\traw_spin_lock(&this_rq->lock);\n}\n\n#else /* !CONFIG_NO_HZ_COMMON */\nstatic inline void nohz_balancer_kick(struct rq *rq) { }\n\nstatic inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\treturn false;\n}\n\nstatic inline void nohz_newidle_balance(struct rq *this_rq) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\n/*\n * idle_balance is called by schedule() if this_cpu is about to become\n * idle. Attempts to pull tasks from other CPUs.\n */\nstatic int idle_balance(struct rq *this_rq, struct rq_flags *rf)\n{\n\tunsigned long next_balance = jiffies + HZ;\n\tint this_cpu = this_rq->cpu;\n\tstruct sched_domain *sd;\n\tint pulled_task = 0;\n\tu64 curr_cost = 0;\n\n\t/*\n\t * We must set idle_stamp _before_ calling idle_balance(), such that we\n\t * measure the duration of idle_balance() as idle time.\n\t */\n\tthis_rq->idle_stamp = rq_clock(this_rq);\n\n\t/*\n\t * Do not pull tasks towards !active CPUs...\n\t */\n\tif (!cpu_active(this_cpu))\n\t\treturn 0;\n\n\t/*\n\t * This is OK, because current is on_cpu, which avoids it being picked\n\t * for load-balance and preemption/IRQs are still disabled avoiding\n\t * further scheduler activity on it and we're being very careful to\n\t * re-start the picking loop.\n\t */\n\trq_unpin_lock(this_rq, rf);\n\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost ||\n\t    !READ_ONCE(this_rq->rd->overload)) {\n\n\t\trcu_read_lock();\n\t\tsd = rcu_dereference_check_sched_domain(this_rq->sd);\n\t\tif (sd)\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\trcu_read_unlock();\n\n\t\tnohz_newidle_balance(this_rq);\n\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock(&this_rq->lock);\n\n\tupdate_blocked_averages(this_cpu);\n\trcu_read_lock();\n\tfor_each_domain(this_cpu, sd) {\n\t\tint continue_balancing = 1;\n\t\tu64 t0, domain_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\tif (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost) {\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (sd->flags & SD_BALANCE_NEWIDLE) {\n\t\t\tt0 = sched_clock_cpu(this_cpu);\n\n\t\t\tpulled_task = load_balance(this_cpu, this_rq,\n\t\t\t\t\t\t   sd, CPU_NEWLY_IDLE,\n\t\t\t\t\t\t   &continue_balancing);\n\n\t\t\tdomain_cost = sched_clock_cpu(this_cpu) - t0;\n\t\t\tif (domain_cost > sd->max_newidle_lb_cost)\n\t\t\t\tsd->max_newidle_lb_cost = domain_cost;\n\n\t\t\tcurr_cost += domain_cost;\n\t\t}\n\n\t\tupdate_next_balance(sd, &next_balance);\n\n\t\t/*\n\t\t * Stop searching for tasks to pull if there are\n\t\t * now runnable tasks on this rq.\n\t\t */\n\t\tif (pulled_task || this_rq->nr_running > 0)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\traw_spin_lock(&this_rq->lock);\n\n\tif (curr_cost > this_rq->max_idle_balance_cost)\n\t\tthis_rq->max_idle_balance_cost = curr_cost;\n\nout:\n\t/*\n\t * While browsing the domains, we released the rq lock, a task could\n\t * have been enqueued in the meantime. Since we're not going idle,\n\t * pretend we pulled a task.\n\t */\n\tif (this_rq->cfs.h_nr_running && !pulled_task)\n\t\tpulled_task = 1;\n\n\t/* Move the next balance forward */\n\tif (time_after(this_rq->next_balance, next_balance))\n\t\tthis_rq->next_balance = next_balance;\n\n\t/* Is there a task of a high priority class? */\n\tif (this_rq->nr_running != this_rq->cfs.h_nr_running)\n\t\tpulled_task = -1;\n\n\tif (pulled_task)\n\t\tthis_rq->idle_stamp = 0;\n\n\trq_repin_lock(this_rq, rf);\n\n\treturn pulled_task;\n}\n\n/*\n * run_rebalance_domains is triggered when needed from the scheduler tick.\n * Also triggered for nohz idle balancing (with nohz_balancing_kick set).\n */\nstatic __latent_entropy void run_rebalance_domains(struct softirq_action *h)\n{\n\tstruct rq *this_rq = this_rq();\n\tenum cpu_idle_type idle = this_rq->idle_balance ?\n\t\t\t\t\t\tCPU_IDLE : CPU_NOT_IDLE;\n\n\t/*\n\t * If this CPU has a pending nohz_balance_kick, then do the\n\t * balancing on behalf of the other idle CPUs whose ticks are\n\t * stopped. Do nohz_idle_balance *before* rebalance_domains to\n\t * give the idle CPUs a chance to load balance. Else we may\n\t * load balance only within the local sched_domain hierarchy\n\t * and abort nohz_idle_balance altogether if we pull some load.\n\t */\n\tif (nohz_idle_balance(this_rq, idle))\n\t\treturn;\n\n\t/* normal load balance */\n\tupdate_blocked_averages(this_rq->cpu);\n\trebalance_domains(this_rq, idle);\n}\n\n/*\n * Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing.\n */\nvoid trigger_load_balance(struct rq *rq)\n{\n\t/* Don't need to rebalance while attached to NULL domain */\n\tif (unlikely(on_null_domain(rq)))\n\t\treturn;\n\n\tif (time_after_eq(jiffies, rq->next_balance))\n\t\traise_softirq(SCHED_SOFTIRQ);\n\n\tnohz_balancer_kick(rq);\n}\n\nstatic void rq_online_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\tupdate_runtime_enabled(rq);\n}\n\nstatic void rq_offline_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\t/* Ensure any throttled groups are reachable by pick_next_task */\n\tunthrottle_offline_cfs_rqs(rq);\n}\n\n#endif /* CONFIG_SMP */\n\n/*\n * scheduler tick hitting a task of our scheduling class.\n *\n * NOTE: This function can be called remotely by the tick offload that\n * goes along full dynticks. Therefore no local assumption can be made\n * and everything must be accessed through the @rq and @curr passed in\n * parameters.\n */\nstatic void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tentity_tick(cfs_rq, se, queued);\n\t}\n\n\tif (static_branch_unlikely(&sched_numa_balancing))\n\t\ttask_tick_numa(rq, curr);\n\n\tupdate_misfit_status(curr, rq);\n\tupdate_overutilized_status(task_rq(curr));\n}\n\n/*\n * called on fork with the child task as argument from the parent's context\n *  - child not yet on the tasklist\n *  - preemption disabled\n */\nstatic void task_fork_fair(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se, *curr;\n\tstruct rq *rq = this_rq();\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\tcfs_rq = task_cfs_rq(current);\n\tcurr = cfs_rq->curr;\n\tif (curr) {\n\t\tupdate_curr(cfs_rq);\n\t\tse->vruntime = curr->vruntime;\n\t}\n\tplace_entity(cfs_rq, se, 1);\n\n\tif (sysctl_sched_child_runs_first && curr && entity_before(curr, se)) {\n\t\t/*\n\t\t * Upon rescheduling, sched_class::put_prev_task() will place\n\t\t * 'current' within the tree based on its new key value.\n\t\t */\n\t\tswap(curr->vruntime, se->vruntime);\n\t\tresched_curr(rq);\n\t}\n\n\tse->vruntime -= cfs_rq->min_vruntime;\n\trq_unlock(rq, &rf);\n}\n\n/*\n * Priority of the task has changed. Check to see if we preempt\n * the current task.\n */\nstatic void\nprio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)\n{\n\tif (!task_on_rq_queued(p))\n\t\treturn;\n\n\t/*\n\t * Reschedule if we are currently running on this runqueue and\n\t * our priority decreased, or if we are not currently running on\n\t * this runqueue and our priority is higher than the current's\n\t */\n\tif (rq->curr == p) {\n\t\tif (p->prio > oldprio)\n\t\t\tresched_curr(rq);\n\t} else\n\t\tcheck_preempt_curr(rq, p, 0);\n}\n\nstatic inline bool vruntime_normalized(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases,\n\t * the dequeue_entity(.flags=0) will already have normalized the\n\t * vruntime.\n\t */\n\tif (p->on_rq)\n\t\treturn true;\n\n\t/*\n\t * When !on_rq, vruntime of the task has usually NOT been normalized.\n\t * But there are some cases where it has already been normalized:\n\t *\n\t * - A forked child which is waiting for being woken up by\n\t *   wake_up_new_task().\n\t * - A task which has been woken up by try_to_wake_up() and\n\t *   waiting for actually being woken up by sched_ttwu_pending().\n\t */\n\tif (!se->sum_exec_runtime ||\n\t    (p->state == TASK_WAKING && p->sched_remote_wakeup))\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n/*\n * Propagate the changes of the sched_entity across the tg tree to make it\n * visible to the root\n */\nstatic void propagate_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq;\n\n\t/* Start to propagate at parent */\n\tse = se->parent;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n}\n#else\nstatic void propagate_entity_cfs_rq(struct sched_entity *se) { }\n#endif\n\nstatic void detach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t/* Catch up with the cfs_rq and remove our load when we leave */\n\tupdate_load_avg(cfs_rq, se, 0);\n\tdetach_entity_load_avg(cfs_rq, se);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void attach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/*\n\t * Since the real-depth could have been changed (only FAIR\n\t * class maintain depth value), reset depth properly.\n\t */\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n#endif\n\n\t/* Synchronize entity with its cfs_rq */\n\tupdate_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);\n\tattach_entity_load_avg(cfs_rq, se, 0);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void detach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tif (!vruntime_normalized(p)) {\n\t\t/*\n\t\t * Fix up our vruntime so that the current sleep doesn't\n\t\t * cause 'unlimited' sleep bonus.\n\t\t */\n\t\tplace_entity(cfs_rq, se, 0);\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\t}\n\n\tdetach_entity_cfs_rq(se);\n}\n\nstatic void attach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tattach_entity_cfs_rq(se);\n\n\tif (!vruntime_normalized(p))\n\t\tse->vruntime += cfs_rq->min_vruntime;\n}\n\nstatic void switched_from_fair(struct rq *rq, struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n}\n\nstatic void switched_to_fair(struct rq *rq, struct task_struct *p)\n{\n\tattach_task_cfs_rq(p);\n\n\tif (task_on_rq_queued(p)) {\n\t\t/*\n\t\t * We were most likely switched from sched_rt, so\n\t\t * kick off the schedule if running, otherwise just see\n\t\t * if we can still preempt the current task.\n\t\t */\n\t\tif (rq->curr == p)\n\t\t\tresched_curr(rq);\n\t\telse\n\t\t\tcheck_preempt_curr(rq, p, 0);\n\t}\n}\n\n/* Account for a task changing its policy or group.\n *\n * This routine is mostly called to set cfs_rq->curr field when a task\n * migrates between groups/classes.\n */\nstatic void set_curr_task_fair(struct rq *rq)\n{\n\tstruct sched_entity *se = &rq->curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t\tset_next_entity(cfs_rq, se);\n\t\t/* ensure bandwidth has been allocated on our new cfs_rq */\n\t\taccount_cfs_rq_runtime(cfs_rq, 0);\n\t}\n}\n\nvoid init_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT_CACHED;\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n#ifndef CONFIG_64BIT\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n#ifdef CONFIG_SMP\n\traw_spin_lock_init(&cfs_rq->removed.lock);\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void task_set_group_fair(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tset_task_rq(p, task_cpu(p));\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n}\n\nstatic void task_move_group_fair(struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n\tset_task_rq(p, task_cpu(p));\n\n#ifdef CONFIG_SMP\n\t/* Tell se's cfs_rq has been changed -- migrated */\n\tp->se.avg.last_update_time = 0;\n#endif\n\tattach_task_cfs_rq(p);\n}\n\nstatic void task_change_group_fair(struct task_struct *p, int type)\n{\n\tswitch (type) {\n\tcase TASK_SET_GROUP:\n\t\ttask_set_group_fair(p);\n\t\tbreak;\n\n\tcase TASK_MOVE_GROUP:\n\t\ttask_move_group_fair(p);\n\t\tbreak;\n\t}\n}\n\nvoid free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct sched_entity *se;\n\tstruct cfs_rq *cfs_rq;\n\tint i;\n\n\ttg->cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tinit_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_cfs_rq(cfs_rq);\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);\n\t\tinit_entity_runnable_average(se);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nvoid online_fair_sched_group(struct task_group *tg)\n{\n\tstruct sched_entity *se;\n\tstruct rq *rq;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\t\tse = tg->se[i];\n\n\t\traw_spin_lock_irq(&rq->lock);\n\t\tupdate_rq_clock(rq);\n\t\tattach_entity_cfs_rq(se);\n\t\tsync_throttle(tg, i);\n\t\traw_spin_unlock_irq(&rq->lock);\n\t}\n}\n\nvoid unregister_fair_sched_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (tg->se[cpu])\n\t\t\tremove_entity_load_avg(tg->se[cpu]);\n\n\t\t/*\n\t\t * Only empty task groups can be destroyed; so we can speculatively\n\t\t * check on_list without danger of it being re-added.\n\t\t */\n\t\tif (!tg->cfs_rq[cpu]->on_list)\n\t\t\tcontinue;\n\n\t\trq = cpu_rq(cpu);\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tlist_del_leaf_cfs_rq(tg->cfs_rq[cpu]);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n}\n\nvoid init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\tstruct sched_entity *se, int cpu,\n\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tcfs_rq->tg = tg;\n\tcfs_rq->rq = rq;\n\tinit_cfs_rq_runtime(cfs_rq);\n\n\ttg->cfs_rq[cpu] = cfs_rq;\n\ttg->se[cpu] = se;\n\n\t/* se could be NULL for root_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent) {\n\t\tse->cfs_rq = &rq->cfs;\n\t\tse->depth = 0;\n\t} else {\n\t\tse->cfs_rq = parent->my_q;\n\t\tse->depth = parent->depth + 1;\n\t}\n\n\tse->my_q = cfs_rq;\n\t/* guarantee group entities always have weight */\n\tupdate_load_set(&se->load, NICE_0_LOAD);\n\tse->parent = parent;\n}\n\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tshares = clamp(shares, scale_load(MIN_SHARES), scale_load(MAX_SHARES));\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se = tg->se[i];\n\t\tstruct rq_flags rf;\n\n\t\t/* Propagate contribution to hierarchy */\n\t\trq_lock_irqsave(rq, &rf);\n\t\tupdate_rq_clock(rq);\n\t\tfor_each_sched_entity(se) {\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, UPDATE_TG);\n\t\t\tupdate_cfs_group(se);\n\t\t}\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n#else /* CONFIG_FAIR_GROUP_SCHED */\n\nvoid free_fair_sched_group(struct task_group *tg) { }\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nvoid online_fair_sched_group(struct task_group *tg) { }\n\nvoid unregister_fair_sched_group(struct task_group *tg) { }\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\nstatic unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task)\n{\n\tstruct sched_entity *se = &task->se;\n\tunsigned int rr_interval = 0;\n\n\t/*\n\t * Time slice is 0 for SCHED_OTHER tasks that are on an otherwise\n\t * idle runqueue:\n\t */\n\tif (rq->cfs.load.weight)\n\t\trr_interval = NS_TO_JIFFIES(sched_slice(cfs_rq_of(se), se));\n\n\treturn rr_interval;\n}\n\n/*\n * All the scheduling class methods:\n */\nconst struct sched_class fair_sched_class = {\n\t.next\t\t\t= &idle_sched_class,\n\t.enqueue_task\t\t= enqueue_task_fair,\n\t.dequeue_task\t\t= dequeue_task_fair,\n\t.yield_task\t\t= yield_task_fair,\n\t.yield_to_task\t\t= yield_to_task_fair,\n\n\t.check_preempt_curr\t= check_preempt_wakeup,\n\n\t.pick_next_task\t\t= pick_next_task_fair,\n\t.put_prev_task\t\t= put_prev_task_fair,\n\n#ifdef CONFIG_SMP\n\t.select_task_rq\t\t= select_task_rq_fair,\n\t.migrate_task_rq\t= migrate_task_rq_fair,\n\n\t.rq_online\t\t= rq_online_fair,\n\t.rq_offline\t\t= rq_offline_fair,\n\n\t.task_dead\t\t= task_dead_fair,\n\t.set_cpus_allowed\t= set_cpus_allowed_common,\n#endif\n\n\t.set_curr_task          = set_curr_task_fair,\n\t.task_tick\t\t= task_tick_fair,\n\t.task_fork\t\t= task_fork_fair,\n\n\t.prio_changed\t\t= prio_changed_fair,\n\t.switched_from\t\t= switched_from_fair,\n\t.switched_to\t\t= switched_to_fair,\n\n\t.get_rr_interval\t= get_rr_interval_fair,\n\n\t.update_curr\t\t= update_curr_fair,\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t.task_change_group\t= task_change_group_fair,\n#endif\n};\n\n#ifdef CONFIG_SCHED_DEBUG\nvoid print_cfs_stats(struct seq_file *m, int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\n\trcu_read_lock();\n\tfor_each_leaf_cfs_rq(cpu_rq(cpu), cfs_rq)\n\t\tprint_cfs_rq(m, cpu, cfs_rq);\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nvoid show_numa_stats(struct task_struct *p, struct seq_file *m)\n{\n\tint node;\n\tunsigned long tsf = 0, tpf = 0, gsf = 0, gpf = 0;\n\n\tfor_each_online_node(node) {\n\t\tif (p->numa_faults) {\n\t\t\ttsf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t\t\ttpf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tif (p->numa_group) {\n\t\t\tgsf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 0)],\n\t\t\tgpf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tprint_numa_stats(m, node, tsf, tpf, gsf, gpf);\n\t}\n}\n#endif /* CONFIG_NUMA_BALANCING */\n#endif /* CONFIG_SCHED_DEBUG */\n\n__init void init_sched_fair_class(void)\n{\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tnohz.next_balance = jiffies;\n\tnohz.next_blocked = jiffies;\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n#endif\n#endif /* SMP */\n\n}\n"], "filenames": ["kernel/sched/fair.c"], "buggy_code_start_loc": [355], "buggy_code_end_loc": [10577], "fixing_code_start_loc": [355], "fixing_code_end_loc": [10552], "type": "CWE-835", "message": "In the Linux kernel before 4.20.2, kernel/sched/fair.c mishandles leaf cfs_rq's, which allows attackers to cause a denial of service (infinite loop in update_blocked_averages) or possibly have unspecified other impact by inducing a high load.", "other": {"cve": {"id": "CVE-2018-20784", "sourceIdentifier": "cve@mitre.org", "published": "2019-02-22T15:29:00.237", "lastModified": "2021-06-02T15:28:24.530", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the Linux kernel before 4.20.2, kernel/sched/fair.c mishandles leaf cfs_rq's, which allows attackers to cause a denial of service (infinite loop in update_blocked_averages) or possibly have unspecified other impact by inducing a high load."}, {"lang": "es", "value": "En el kernel de Linux, en versiones anteriores a la 4.20.2, kernel/sched/fair.c gestiona leaf cfs_rq de manera incorrecta, lo que permite que los atacantes provoquen una denegaci\u00f3n de servicio (bucle infinito en update_blocked_averages) o, posiblemente, otro impacto sin especificar induciendo una carga alta."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 7.5}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-835"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.13", "versionEndExcluding": "4.14.93", "matchCriteriaId": "8C6AA6D9-6073-4F71-8AF8-5F2828D88398"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.19", "versionEndExcluding": "4.19.15", "matchCriteriaId": "7E5568CD-D4BA-4C72-AC99-A0D2E7112EDD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.20", "versionEndExcluding": "4.20.2", "matchCriteriaId": "22A4B1AF-0AE6-45BB-847A-470D03904BC4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "B5F099C8-DC7F-48C6-AAF8-C0DBFFD49620"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:esm:*:*:*", "matchCriteriaId": "7A5301BF-1402-4BE0-A0F8-69FBE79BC6D6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time:8:*:*:*:*:*:*:*", "matchCriteriaId": "CBF9BCF3-187F-410A-96CA-9C47D3ED6924"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=c40f7d74c741a907cfaeb73a7697081881c497d0", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:1959", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:1971", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.20.2", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/c40f7d74c741a907cfaeb73a7697081881c497d0", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4115-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4118-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4211-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4211-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c40f7d74c741a907cfaeb73a7697081881c497d0"}}