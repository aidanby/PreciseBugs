{"buggy_code": ["/*\n * Functions related to mapping data to requests\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/uio.h>\n\n#include \"blk.h\"\n\n/*\n * Append a bio to a passthrough request.  Only works can be merged into\n * the request based on the driver constraints.\n */\nint blk_rq_append_bio(struct request *rq, struct bio *bio)\n{\n\tif (!rq->bio) {\n\t\tblk_rq_bio_prep(rq->q, rq, bio);\n\t} else {\n\t\tif (!ll_back_merge_fn(rq->q, rq, bio))\n\t\t\treturn -EINVAL;\n\n\t\trq->biotail->bi_next = bio;\n\t\trq->biotail = bio;\n\t\trq->__data_len += bio->bi_iter.bi_size;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(blk_rq_append_bio);\n\nstatic int __blk_rq_unmap_user(struct bio *bio)\n{\n\tint ret = 0;\n\n\tif (bio) {\n\t\tif (bio_flagged(bio, BIO_USER_MAPPED))\n\t\t\tbio_unmap_user(bio);\n\t\telse\n\t\t\tret = bio_uncopy_user(bio);\n\t}\n\n\treturn ret;\n}\n\nstatic int __blk_rq_map_user_iov(struct request *rq,\n\t\tstruct rq_map_data *map_data, struct iov_iter *iter,\n\t\tgfp_t gfp_mask, bool copy)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct bio *bio, *orig_bio;\n\tint ret;\n\n\tif (copy)\n\t\tbio = bio_copy_user_iov(q, map_data, iter, gfp_mask);\n\telse\n\t\tbio = bio_map_user_iov(q, iter, gfp_mask);\n\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\tif (map_data && map_data->null_mapped)\n\t\tbio_set_flag(bio, BIO_NULL_MAPPED);\n\n\tiov_iter_advance(iter, bio->bi_iter.bi_size);\n\tif (map_data)\n\t\tmap_data->offset += bio->bi_iter.bi_size;\n\n\torig_bio = bio;\n\tblk_queue_bounce(q, &bio);\n\n\t/*\n\t * We link the bounce buffer in and could have to traverse it\n\t * later so we have to get a ref to prevent it from being freed\n\t */\n\tbio_get(bio);\n\n\tret = blk_rq_append_bio(rq, bio);\n\tif (ret) {\n\t\tbio_endio(bio);\n\t\t__blk_rq_unmap_user(orig_bio);\n\t\tbio_put(bio);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n/**\n * blk_rq_map_user_iov - map user data to a request, for REQ_TYPE_BLOCK_PC usage\n * @q:\t\trequest queue where request should be inserted\n * @rq:\t\trequest to map data to\n * @map_data:   pointer to the rq_map_data holding pages (if necessary)\n * @iter:\tiovec iterator\n * @gfp_mask:\tmemory allocation flags\n *\n * Description:\n *    Data will be mapped directly for zero copy I/O, if possible. Otherwise\n *    a kernel bounce buffer is used.\n *\n *    A matching blk_rq_unmap_user() must be issued at the end of I/O, while\n *    still in process context.\n *\n *    Note: The mapped bio may need to be bounced through blk_queue_bounce()\n *    before being submitted to the device, as pages mapped may be out of\n *    reach. It's the callers responsibility to make sure this happens. The\n *    original bio must be passed back in to blk_rq_unmap_user() for proper\n *    unmapping.\n */\nint blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\n\t\t\tstruct rq_map_data *map_data,\n\t\t\tconst struct iov_iter *iter, gfp_t gfp_mask)\n{\n\tbool copy = false;\n\tunsigned long align = q->dma_pad_mask | queue_dma_alignment(q);\n\tstruct bio *bio = NULL;\n\tstruct iov_iter i;\n\tint ret;\n\n\tif (map_data)\n\t\tcopy = true;\n\telse if (iov_iter_alignment(iter) & align)\n\t\tcopy = true;\n\telse if (queue_virt_boundary(q))\n\t\tcopy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);\n\n\ti = *iter;\n\tdo {\n\t\tret =__blk_rq_map_user_iov(rq, map_data, &i, gfp_mask, copy);\n\t\tif (ret)\n\t\t\tgoto unmap_rq;\n\t\tif (!bio)\n\t\t\tbio = rq->bio;\n\t} while (iov_iter_count(&i));\n\n\tif (!bio_flagged(bio, BIO_USER_MAPPED))\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\treturn 0;\n\nunmap_rq:\n\t__blk_rq_unmap_user(bio);\n\trq->bio = NULL;\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(blk_rq_map_user_iov);\n\nint blk_rq_map_user(struct request_queue *q, struct request *rq,\n\t\t    struct rq_map_data *map_data, void __user *ubuf,\n\t\t    unsigned long len, gfp_t gfp_mask)\n{\n\tstruct iovec iov;\n\tstruct iov_iter i;\n\tint ret = import_single_range(rq_data_dir(rq), ubuf, len, &iov, &i);\n\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\treturn blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);\n}\nEXPORT_SYMBOL(blk_rq_map_user);\n\n/**\n * blk_rq_unmap_user - unmap a request with user data\n * @bio:\t       start of bio list\n *\n * Description:\n *    Unmap a rq previously mapped by blk_rq_map_user(). The caller must\n *    supply the original rq->bio from the blk_rq_map_user() return, since\n *    the I/O completion may have changed rq->bio.\n */\nint blk_rq_unmap_user(struct bio *bio)\n{\n\tstruct bio *mapped_bio;\n\tint ret = 0, ret2;\n\n\twhile (bio) {\n\t\tmapped_bio = bio;\n\t\tif (unlikely(bio_flagged(bio, BIO_BOUNCED)))\n\t\t\tmapped_bio = bio->bi_private;\n\n\t\tret2 = __blk_rq_unmap_user(mapped_bio);\n\t\tif (ret2 && !ret)\n\t\t\tret = ret2;\n\n\t\tmapped_bio = bio;\n\t\tbio = bio->bi_next;\n\t\tbio_put(mapped_bio);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(blk_rq_unmap_user);\n\n/**\n * blk_rq_map_kern - map kernel data to a request, for REQ_TYPE_BLOCK_PC usage\n * @q:\t\trequest queue where request should be inserted\n * @rq:\t\trequest to fill\n * @kbuf:\tthe kernel buffer\n * @len:\tlength of user data\n * @gfp_mask:\tmemory allocation flags\n *\n * Description:\n *    Data will be mapped directly if possible. Otherwise a bounce\n *    buffer is used. Can be called multiple times to append multiple\n *    buffers.\n */\nint blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,\n\t\t    unsigned int len, gfp_t gfp_mask)\n{\n\tint reading = rq_data_dir(rq) == READ;\n\tunsigned long addr = (unsigned long) kbuf;\n\tint do_copy = 0;\n\tstruct bio *bio;\n\tint ret;\n\n\tif (len > (queue_max_hw_sectors(q) << 9))\n\t\treturn -EINVAL;\n\tif (!len || !kbuf)\n\t\treturn -EINVAL;\n\n\tdo_copy = !blk_rq_aligned(q, addr, len) || object_is_on_stack(kbuf);\n\tif (do_copy)\n\t\tbio = bio_copy_kern(q, kbuf, len, gfp_mask, reading);\n\telse\n\t\tbio = bio_map_kern(q, kbuf, len, gfp_mask);\n\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\tif (!reading)\n\t\tbio_set_op_attrs(bio, REQ_OP_WRITE, 0);\n\n\tif (do_copy)\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\n\tret = blk_rq_append_bio(rq, bio);\n\tif (unlikely(ret)) {\n\t\t/* request is too big */\n\t\tbio_put(bio);\n\t\treturn ret;\n\t}\n\n\tblk_queue_bounce(q, &rq->bio);\n\treturn 0;\n}\nEXPORT_SYMBOL(blk_rq_map_kern);\n"], "fixing_code": ["/*\n * Functions related to mapping data to requests\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/uio.h>\n\n#include \"blk.h\"\n\n/*\n * Append a bio to a passthrough request.  Only works can be merged into\n * the request based on the driver constraints.\n */\nint blk_rq_append_bio(struct request *rq, struct bio *bio)\n{\n\tif (!rq->bio) {\n\t\tblk_rq_bio_prep(rq->q, rq, bio);\n\t} else {\n\t\tif (!ll_back_merge_fn(rq->q, rq, bio))\n\t\t\treturn -EINVAL;\n\n\t\trq->biotail->bi_next = bio;\n\t\trq->biotail = bio;\n\t\trq->__data_len += bio->bi_iter.bi_size;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(blk_rq_append_bio);\n\nstatic int __blk_rq_unmap_user(struct bio *bio)\n{\n\tint ret = 0;\n\n\tif (bio) {\n\t\tif (bio_flagged(bio, BIO_USER_MAPPED))\n\t\t\tbio_unmap_user(bio);\n\t\telse\n\t\t\tret = bio_uncopy_user(bio);\n\t}\n\n\treturn ret;\n}\n\nstatic int __blk_rq_map_user_iov(struct request *rq,\n\t\tstruct rq_map_data *map_data, struct iov_iter *iter,\n\t\tgfp_t gfp_mask, bool copy)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct bio *bio, *orig_bio;\n\tint ret;\n\n\tif (copy)\n\t\tbio = bio_copy_user_iov(q, map_data, iter, gfp_mask);\n\telse\n\t\tbio = bio_map_user_iov(q, iter, gfp_mask);\n\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\tif (map_data && map_data->null_mapped)\n\t\tbio_set_flag(bio, BIO_NULL_MAPPED);\n\n\tiov_iter_advance(iter, bio->bi_iter.bi_size);\n\tif (map_data)\n\t\tmap_data->offset += bio->bi_iter.bi_size;\n\n\torig_bio = bio;\n\tblk_queue_bounce(q, &bio);\n\n\t/*\n\t * We link the bounce buffer in and could have to traverse it\n\t * later so we have to get a ref to prevent it from being freed\n\t */\n\tbio_get(bio);\n\n\tret = blk_rq_append_bio(rq, bio);\n\tif (ret) {\n\t\tbio_endio(bio);\n\t\t__blk_rq_unmap_user(orig_bio);\n\t\tbio_put(bio);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n/**\n * blk_rq_map_user_iov - map user data to a request, for REQ_TYPE_BLOCK_PC usage\n * @q:\t\trequest queue where request should be inserted\n * @rq:\t\trequest to map data to\n * @map_data:   pointer to the rq_map_data holding pages (if necessary)\n * @iter:\tiovec iterator\n * @gfp_mask:\tmemory allocation flags\n *\n * Description:\n *    Data will be mapped directly for zero copy I/O, if possible. Otherwise\n *    a kernel bounce buffer is used.\n *\n *    A matching blk_rq_unmap_user() must be issued at the end of I/O, while\n *    still in process context.\n *\n *    Note: The mapped bio may need to be bounced through blk_queue_bounce()\n *    before being submitted to the device, as pages mapped may be out of\n *    reach. It's the callers responsibility to make sure this happens. The\n *    original bio must be passed back in to blk_rq_unmap_user() for proper\n *    unmapping.\n */\nint blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\n\t\t\tstruct rq_map_data *map_data,\n\t\t\tconst struct iov_iter *iter, gfp_t gfp_mask)\n{\n\tbool copy = false;\n\tunsigned long align = q->dma_pad_mask | queue_dma_alignment(q);\n\tstruct bio *bio = NULL;\n\tstruct iov_iter i;\n\tint ret;\n\n\tif (!iter_is_iovec(iter))\n\t\tgoto fail;\n\n\tif (map_data)\n\t\tcopy = true;\n\telse if (iov_iter_alignment(iter) & align)\n\t\tcopy = true;\n\telse if (queue_virt_boundary(q))\n\t\tcopy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);\n\n\ti = *iter;\n\tdo {\n\t\tret =__blk_rq_map_user_iov(rq, map_data, &i, gfp_mask, copy);\n\t\tif (ret)\n\t\t\tgoto unmap_rq;\n\t\tif (!bio)\n\t\t\tbio = rq->bio;\n\t} while (iov_iter_count(&i));\n\n\tif (!bio_flagged(bio, BIO_USER_MAPPED))\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\treturn 0;\n\nunmap_rq:\n\t__blk_rq_unmap_user(bio);\nfail:\n\trq->bio = NULL;\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(blk_rq_map_user_iov);\n\nint blk_rq_map_user(struct request_queue *q, struct request *rq,\n\t\t    struct rq_map_data *map_data, void __user *ubuf,\n\t\t    unsigned long len, gfp_t gfp_mask)\n{\n\tstruct iovec iov;\n\tstruct iov_iter i;\n\tint ret = import_single_range(rq_data_dir(rq), ubuf, len, &iov, &i);\n\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\treturn blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);\n}\nEXPORT_SYMBOL(blk_rq_map_user);\n\n/**\n * blk_rq_unmap_user - unmap a request with user data\n * @bio:\t       start of bio list\n *\n * Description:\n *    Unmap a rq previously mapped by blk_rq_map_user(). The caller must\n *    supply the original rq->bio from the blk_rq_map_user() return, since\n *    the I/O completion may have changed rq->bio.\n */\nint blk_rq_unmap_user(struct bio *bio)\n{\n\tstruct bio *mapped_bio;\n\tint ret = 0, ret2;\n\n\twhile (bio) {\n\t\tmapped_bio = bio;\n\t\tif (unlikely(bio_flagged(bio, BIO_BOUNCED)))\n\t\t\tmapped_bio = bio->bi_private;\n\n\t\tret2 = __blk_rq_unmap_user(mapped_bio);\n\t\tif (ret2 && !ret)\n\t\t\tret = ret2;\n\n\t\tmapped_bio = bio;\n\t\tbio = bio->bi_next;\n\t\tbio_put(mapped_bio);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(blk_rq_unmap_user);\n\n/**\n * blk_rq_map_kern - map kernel data to a request, for REQ_TYPE_BLOCK_PC usage\n * @q:\t\trequest queue where request should be inserted\n * @rq:\t\trequest to fill\n * @kbuf:\tthe kernel buffer\n * @len:\tlength of user data\n * @gfp_mask:\tmemory allocation flags\n *\n * Description:\n *    Data will be mapped directly if possible. Otherwise a bounce\n *    buffer is used. Can be called multiple times to append multiple\n *    buffers.\n */\nint blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,\n\t\t    unsigned int len, gfp_t gfp_mask)\n{\n\tint reading = rq_data_dir(rq) == READ;\n\tunsigned long addr = (unsigned long) kbuf;\n\tint do_copy = 0;\n\tstruct bio *bio;\n\tint ret;\n\n\tif (len > (queue_max_hw_sectors(q) << 9))\n\t\treturn -EINVAL;\n\tif (!len || !kbuf)\n\t\treturn -EINVAL;\n\n\tdo_copy = !blk_rq_aligned(q, addr, len) || object_is_on_stack(kbuf);\n\tif (do_copy)\n\t\tbio = bio_copy_kern(q, kbuf, len, gfp_mask, reading);\n\telse\n\t\tbio = bio_map_kern(q, kbuf, len, gfp_mask);\n\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\tif (!reading)\n\t\tbio_set_op_attrs(bio, REQ_OP_WRITE, 0);\n\n\tif (do_copy)\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\n\tret = blk_rq_append_bio(rq, bio);\n\tif (unlikely(ret)) {\n\t\t/* request is too big */\n\t\tbio_put(bio);\n\t\treturn ret;\n\t}\n\n\tblk_queue_bounce(q, &rq->bio);\n\treturn 0;\n}\nEXPORT_SYMBOL(blk_rq_map_kern);\n"], "filenames": ["block/blk-map.c"], "buggy_code_start_loc": [119], "buggy_code_end_loc": [142], "fixing_code_start_loc": [120], "fixing_code_end_loc": [147], "type": "CWE-416", "message": "The blk_rq_map_user_iov function in block/blk-map.c in the Linux kernel before 4.8.14 does not properly restrict the type of iterator, which allows local users to read or write to arbitrary kernel memory locations or cause a denial of service (use-after-free) by leveraging access to a /dev/sg device.", "other": {"cve": {"id": "CVE-2016-9576", "sourceIdentifier": "secalert@redhat.com", "published": "2016-12-28T07:59:00.307", "lastModified": "2023-02-12T23:27:16.950", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The blk_rq_map_user_iov function in block/blk-map.c in the Linux kernel before 4.8.14 does not properly restrict the type of iterator, which allows local users to read or write to arbitrary kernel memory locations or cause a denial of service (use-after-free) by leveraging access to a /dev/sg device."}, {"lang": "es", "value": "La funci\u00f3n blk_rq_map_user_iov en block/blk-map.c en el kernel de Linux en versiones anteriores a 4.8.14 no restringe adecuadamente el tipo de iterador, lo que permite a usuarios locales leer o escribir a ubicaciones de memoria del kernel arbitrarias o provocar una denegaci\u00f3n de servicio (uso despu\u00e9s de liberaci\u00f3n de memoria) aprovechando acceso a un dispositivo /dev/sg."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.0", "versionEndExcluding": "4.4.38", "matchCriteriaId": "2D401B73-345B-49B5-B568-153C68F7F784"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.8.14", "matchCriteriaId": "2454EAB6-FC42-4FA4-BE76-CBAA81D4ADC4"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=a0ac402cfcdc904f9772e1762b3fda112dcc56a0", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00040.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00041.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00057.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00062.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00072.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00075.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00081.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00088.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-12/msg00091.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2017-0817.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.8.14", "source": "secalert@redhat.com", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/12/08/19", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/94821", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:1842", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2077", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2669", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1403145", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/a0ac402cfcdc904f9772e1762b3fda112dcc56a0", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/a0ac402cfcdc904f9772e1762b3fda112dcc56a0"}}