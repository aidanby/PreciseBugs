{"buggy_code": ["#include <linux/export.h>\n#include <linux/bvec.h>\n#include <linux/uio.h>\n#include <linux/pagemap.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/splice.h>\n#include <net/checksum.h>\n\n#define PIPE_PARANOIA /* for now */\n\n#define iterate_iovec(i, n, __v, __p, skip, STEP) {\t\\\n\tsize_t left;\t\t\t\t\t\\\n\tsize_t wanted = n;\t\t\t\t\\\n\t__p = i->iov;\t\t\t\t\t\\\n\t__v.iov_len = min(n, __p->iov_len - skip);\t\\\n\tif (likely(__v.iov_len)) {\t\t\t\\\n\t\t__v.iov_base = __p->iov_base + skip;\t\\\n\t\tleft = (STEP);\t\t\t\t\\\n\t\t__v.iov_len -= left;\t\t\t\\\n\t\tskip += __v.iov_len;\t\t\t\\\n\t\tn -= __v.iov_len;\t\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\tleft = 0;\t\t\t\t\\\n\t}\t\t\t\t\t\t\\\n\twhile (unlikely(!left && n)) {\t\t\t\\\n\t\t__p++;\t\t\t\t\t\\\n\t\t__v.iov_len = min(n, __p->iov_len);\t\\\n\t\tif (unlikely(!__v.iov_len))\t\t\\\n\t\t\tcontinue;\t\t\t\\\n\t\t__v.iov_base = __p->iov_base;\t\t\\\n\t\tleft = (STEP);\t\t\t\t\\\n\t\t__v.iov_len -= left;\t\t\t\\\n\t\tskip = __v.iov_len;\t\t\t\\\n\t\tn -= __v.iov_len;\t\t\t\\\n\t}\t\t\t\t\t\t\\\n\tn = wanted - n;\t\t\t\t\t\\\n}\n\n#define iterate_kvec(i, n, __v, __p, skip, STEP) {\t\\\n\tsize_t wanted = n;\t\t\t\t\\\n\t__p = i->kvec;\t\t\t\t\t\\\n\t__v.iov_len = min(n, __p->iov_len - skip);\t\\\n\tif (likely(__v.iov_len)) {\t\t\t\\\n\t\t__v.iov_base = __p->iov_base + skip;\t\\\n\t\t(void)(STEP);\t\t\t\t\\\n\t\tskip += __v.iov_len;\t\t\t\\\n\t\tn -= __v.iov_len;\t\t\t\\\n\t}\t\t\t\t\t\t\\\n\twhile (unlikely(n)) {\t\t\t\t\\\n\t\t__p++;\t\t\t\t\t\\\n\t\t__v.iov_len = min(n, __p->iov_len);\t\\\n\t\tif (unlikely(!__v.iov_len))\t\t\\\n\t\t\tcontinue;\t\t\t\\\n\t\t__v.iov_base = __p->iov_base;\t\t\\\n\t\t(void)(STEP);\t\t\t\t\\\n\t\tskip = __v.iov_len;\t\t\t\\\n\t\tn -= __v.iov_len;\t\t\t\\\n\t}\t\t\t\t\t\t\\\n\tn = wanted;\t\t\t\t\t\\\n}\n\n#define iterate_bvec(i, n, __v, __bi, skip, STEP) {\t\\\n\tstruct bvec_iter __start;\t\t\t\\\n\t__start.bi_size = n;\t\t\t\t\\\n\t__start.bi_bvec_done = skip;\t\t\t\\\n\t__start.bi_idx = 0;\t\t\t\t\\\n\tfor_each_bvec(__v, i->bvec, __bi, __start) {\t\\\n\t\tif (!__v.bv_len)\t\t\t\\\n\t\t\tcontinue;\t\t\t\\\n\t\t(void)(STEP);\t\t\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define iterate_all_kinds(i, n, v, I, B, K) {\t\t\t\\\n\tif (likely(n)) {\t\t\t\t\t\\\n\t\tsize_t skip = i->iov_offset;\t\t\t\\\n\t\tif (unlikely(i->type & ITER_BVEC)) {\t\t\\\n\t\t\tstruct bio_vec v;\t\t\t\\\n\t\t\tstruct bvec_iter __bi;\t\t\t\\\n\t\t\titerate_bvec(i, n, v, __bi, skip, (B))\t\\\n\t\t} else if (unlikely(i->type & ITER_KVEC)) {\t\\\n\t\t\tconst struct kvec *kvec;\t\t\\\n\t\t\tstruct kvec v;\t\t\t\t\\\n\t\t\titerate_kvec(i, n, v, kvec, skip, (K))\t\\\n\t\t} else {\t\t\t\t\t\\\n\t\t\tconst struct iovec *iov;\t\t\\\n\t\t\tstruct iovec v;\t\t\t\t\\\n\t\t\titerate_iovec(i, n, v, iov, skip, (I))\t\\\n\t\t}\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n}\n\n#define iterate_and_advance(i, n, v, I, B, K) {\t\t\t\\\n\tif (unlikely(i->count < n))\t\t\t\t\\\n\t\tn = i->count;\t\t\t\t\t\\\n\tif (i->count) {\t\t\t\t\t\t\\\n\t\tsize_t skip = i->iov_offset;\t\t\t\\\n\t\tif (unlikely(i->type & ITER_BVEC)) {\t\t\\\n\t\t\tconst struct bio_vec *bvec = i->bvec;\t\\\n\t\t\tstruct bio_vec v;\t\t\t\\\n\t\t\tstruct bvec_iter __bi;\t\t\t\\\n\t\t\titerate_bvec(i, n, v, __bi, skip, (B))\t\\\n\t\t\ti->bvec = __bvec_iter_bvec(i->bvec, __bi);\t\\\n\t\t\ti->nr_segs -= i->bvec - bvec;\t\t\\\n\t\t\tskip = __bi.bi_bvec_done;\t\t\\\n\t\t} else if (unlikely(i->type & ITER_KVEC)) {\t\\\n\t\t\tconst struct kvec *kvec;\t\t\\\n\t\t\tstruct kvec v;\t\t\t\t\\\n\t\t\titerate_kvec(i, n, v, kvec, skip, (K))\t\\\n\t\t\tif (skip == kvec->iov_len) {\t\t\\\n\t\t\t\tkvec++;\t\t\t\t\\\n\t\t\t\tskip = 0;\t\t\t\\\n\t\t\t}\t\t\t\t\t\\\n\t\t\ti->nr_segs -= kvec - i->kvec;\t\t\\\n\t\t\ti->kvec = kvec;\t\t\t\t\\\n\t\t} else {\t\t\t\t\t\\\n\t\t\tconst struct iovec *iov;\t\t\\\n\t\t\tstruct iovec v;\t\t\t\t\\\n\t\t\titerate_iovec(i, n, v, iov, skip, (I))\t\\\n\t\t\tif (skip == iov->iov_len) {\t\t\\\n\t\t\t\tiov++;\t\t\t\t\\\n\t\t\t\tskip = 0;\t\t\t\\\n\t\t\t}\t\t\t\t\t\\\n\t\t\ti->nr_segs -= iov - i->iov;\t\t\\\n\t\t\ti->iov = iov;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\ti->count -= n;\t\t\t\t\t\\\n\t\ti->iov_offset = skip;\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n}\n\nstatic size_t copy_page_to_iter_iovec(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tsize_t skip, copy, left, wanted;\n\tconst struct iovec *iov;\n\tchar __user *buf;\n\tvoid *kaddr, *from;\n\n\tif (unlikely(bytes > i->count))\n\t\tbytes = i->count;\n\n\tif (unlikely(!bytes))\n\t\treturn 0;\n\n\twanted = bytes;\n\tiov = i->iov;\n\tskip = i->iov_offset;\n\tbuf = iov->iov_base + skip;\n\tcopy = min(bytes, iov->iov_len - skip);\n\n\tif (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_pages_writeable(buf, copy)) {\n\t\tkaddr = kmap_atomic(page);\n\t\tfrom = kaddr + offset;\n\n\t\t/* first chunk, usually the only one */\n\t\tleft = __copy_to_user_inatomic(buf, from, copy);\n\t\tcopy -= left;\n\t\tskip += copy;\n\t\tfrom += copy;\n\t\tbytes -= copy;\n\n\t\twhile (unlikely(!left && bytes)) {\n\t\t\tiov++;\n\t\t\tbuf = iov->iov_base;\n\t\t\tcopy = min(bytes, iov->iov_len);\n\t\t\tleft = __copy_to_user_inatomic(buf, from, copy);\n\t\t\tcopy -= left;\n\t\t\tskip = copy;\n\t\t\tfrom += copy;\n\t\t\tbytes -= copy;\n\t\t}\n\t\tif (likely(!bytes)) {\n\t\t\tkunmap_atomic(kaddr);\n\t\t\tgoto done;\n\t\t}\n\t\toffset = from - kaddr;\n\t\tbuf += copy;\n\t\tkunmap_atomic(kaddr);\n\t\tcopy = min(bytes, iov->iov_len - skip);\n\t}\n\t/* Too bad - revert to non-atomic kmap */\n\n\tkaddr = kmap(page);\n\tfrom = kaddr + offset;\n\tleft = __copy_to_user(buf, from, copy);\n\tcopy -= left;\n\tskip += copy;\n\tfrom += copy;\n\tbytes -= copy;\n\twhile (unlikely(!left && bytes)) {\n\t\tiov++;\n\t\tbuf = iov->iov_base;\n\t\tcopy = min(bytes, iov->iov_len);\n\t\tleft = __copy_to_user(buf, from, copy);\n\t\tcopy -= left;\n\t\tskip = copy;\n\t\tfrom += copy;\n\t\tbytes -= copy;\n\t}\n\tkunmap(page);\n\ndone:\n\tif (skip == iov->iov_len) {\n\t\tiov++;\n\t\tskip = 0;\n\t}\n\ti->count -= wanted - bytes;\n\ti->nr_segs -= iov - i->iov;\n\ti->iov = iov;\n\ti->iov_offset = skip;\n\treturn wanted - bytes;\n}\n\nstatic size_t copy_page_from_iter_iovec(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tsize_t skip, copy, left, wanted;\n\tconst struct iovec *iov;\n\tchar __user *buf;\n\tvoid *kaddr, *to;\n\n\tif (unlikely(bytes > i->count))\n\t\tbytes = i->count;\n\n\tif (unlikely(!bytes))\n\t\treturn 0;\n\n\twanted = bytes;\n\tiov = i->iov;\n\tskip = i->iov_offset;\n\tbuf = iov->iov_base + skip;\n\tcopy = min(bytes, iov->iov_len - skip);\n\n\tif (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_pages_readable(buf, copy)) {\n\t\tkaddr = kmap_atomic(page);\n\t\tto = kaddr + offset;\n\n\t\t/* first chunk, usually the only one */\n\t\tleft = __copy_from_user_inatomic(to, buf, copy);\n\t\tcopy -= left;\n\t\tskip += copy;\n\t\tto += copy;\n\t\tbytes -= copy;\n\n\t\twhile (unlikely(!left && bytes)) {\n\t\t\tiov++;\n\t\t\tbuf = iov->iov_base;\n\t\t\tcopy = min(bytes, iov->iov_len);\n\t\t\tleft = __copy_from_user_inatomic(to, buf, copy);\n\t\t\tcopy -= left;\n\t\t\tskip = copy;\n\t\t\tto += copy;\n\t\t\tbytes -= copy;\n\t\t}\n\t\tif (likely(!bytes)) {\n\t\t\tkunmap_atomic(kaddr);\n\t\t\tgoto done;\n\t\t}\n\t\toffset = to - kaddr;\n\t\tbuf += copy;\n\t\tkunmap_atomic(kaddr);\n\t\tcopy = min(bytes, iov->iov_len - skip);\n\t}\n\t/* Too bad - revert to non-atomic kmap */\n\n\tkaddr = kmap(page);\n\tto = kaddr + offset;\n\tleft = __copy_from_user(to, buf, copy);\n\tcopy -= left;\n\tskip += copy;\n\tto += copy;\n\tbytes -= copy;\n\twhile (unlikely(!left && bytes)) {\n\t\tiov++;\n\t\tbuf = iov->iov_base;\n\t\tcopy = min(bytes, iov->iov_len);\n\t\tleft = __copy_from_user(to, buf, copy);\n\t\tcopy -= left;\n\t\tskip = copy;\n\t\tto += copy;\n\t\tbytes -= copy;\n\t}\n\tkunmap(page);\n\ndone:\n\tif (skip == iov->iov_len) {\n\t\tiov++;\n\t\tskip = 0;\n\t}\n\ti->count -= wanted - bytes;\n\ti->nr_segs -= iov - i->iov;\n\ti->iov = iov;\n\ti->iov_offset = skip;\n\treturn wanted - bytes;\n}\n\n#ifdef PIPE_PARANOIA\nstatic bool sanity(const struct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tint idx = i->idx;\n\tint next = pipe->curbuf + pipe->nrbufs;\n\tif (i->iov_offset) {\n\t\tstruct pipe_buffer *p;\n\t\tif (unlikely(!pipe->nrbufs))\n\t\t\tgoto Bad;\t// pipe must be non-empty\n\t\tif (unlikely(idx != ((next - 1) & (pipe->buffers - 1))))\n\t\t\tgoto Bad;\t// must be at the last buffer...\n\n\t\tp = &pipe->bufs[idx];\n\t\tif (unlikely(p->offset + p->len != i->iov_offset))\n\t\t\tgoto Bad;\t// ... at the end of segment\n\t} else {\n\t\tif (idx != (next & (pipe->buffers - 1)))\n\t\t\tgoto Bad;\t// must be right after the last buffer\n\t}\n\treturn true;\nBad:\n\tprintk(KERN_ERR \"idx = %d, offset = %zd\\n\", i->idx, i->iov_offset);\n\tprintk(KERN_ERR \"curbuf = %d, nrbufs = %d, buffers = %d\\n\",\n\t\t\tpipe->curbuf, pipe->nrbufs, pipe->buffers);\n\tfor (idx = 0; idx < pipe->buffers; idx++)\n\t\tprintk(KERN_ERR \"[%p %p %d %d]\\n\",\n\t\t\tpipe->bufs[idx].ops,\n\t\t\tpipe->bufs[idx].page,\n\t\t\tpipe->bufs[idx].offset,\n\t\t\tpipe->bufs[idx].len);\n\tWARN_ON(1);\n\treturn false;\n}\n#else\n#define sanity(i) true\n#endif\n\nstatic inline int next_idx(int idx, struct pipe_inode_info *pipe)\n{\n\treturn (idx + 1) & (pipe->buffers - 1);\n}\n\nstatic size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tstruct pipe_buffer *buf;\n\tsize_t off;\n\tint idx;\n\n\tif (unlikely(bytes > i->count))\n\t\tbytes = i->count;\n\n\tif (unlikely(!bytes))\n\t\treturn 0;\n\n\tif (!sanity(i))\n\t\treturn 0;\n\n\toff = i->iov_offset;\n\tidx = i->idx;\n\tbuf = &pipe->bufs[idx];\n\tif (off) {\n\t\tif (offset == off && buf->page == page) {\n\t\t\t/* merge with the last one */\n\t\t\tbuf->len += bytes;\n\t\t\ti->iov_offset += bytes;\n\t\t\tgoto out;\n\t\t}\n\t\tidx = next_idx(idx, pipe);\n\t\tbuf = &pipe->bufs[idx];\n\t}\n\tif (idx == pipe->curbuf && pipe->nrbufs)\n\t\treturn 0;\n\tpipe->nrbufs++;\n\tbuf->ops = &page_cache_pipe_buf_ops;\n\tget_page(buf->page = page);\n\tbuf->offset = offset;\n\tbuf->len = bytes;\n\ti->iov_offset = offset + bytes;\n\ti->idx = idx;\nout:\n\ti->count -= bytes;\n\treturn bytes;\n}\n\n/*\n * Fault in one or more iovecs of the given iov_iter, to a maximum length of\n * bytes.  For each iovec, fault in each page that constitutes the iovec.\n *\n * Return 0 on success, or non-zero if the memory could not be accessed (i.e.\n * because it is an invalid address).\n */\nint iov_iter_fault_in_readable(struct iov_iter *i, size_t bytes)\n{\n\tsize_t skip = i->iov_offset;\n\tconst struct iovec *iov;\n\tint err;\n\tstruct iovec v;\n\n\tif (!(i->type & (ITER_BVEC|ITER_KVEC))) {\n\t\titerate_iovec(i, bytes, v, iov, skip, ({\n\t\t\terr = fault_in_pages_readable(v.iov_base, v.iov_len);\n\t\t\tif (unlikely(err))\n\t\t\treturn err;\n\t\t0;}))\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(iov_iter_fault_in_readable);\n\nvoid iov_iter_init(struct iov_iter *i, int direction,\n\t\t\tconst struct iovec *iov, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\t/* It will get better.  Eventually... */\n\tif (segment_eq(get_fs(), KERNEL_DS)) {\n\t\tdirection |= ITER_KVEC;\n\t\ti->type = direction;\n\t\ti->kvec = (struct kvec *)iov;\n\t} else {\n\t\ti->type = direction;\n\t\ti->iov = iov;\n\t}\n\ti->nr_segs = nr_segs;\n\ti->iov_offset = 0;\n\ti->count = count;\n}\nEXPORT_SYMBOL(iov_iter_init);\n\nstatic void memcpy_from_page(char *to, struct page *page, size_t offset, size_t len)\n{\n\tchar *from = kmap_atomic(page);\n\tmemcpy(to, from + offset, len);\n\tkunmap_atomic(from);\n}\n\nstatic void memcpy_to_page(struct page *page, size_t offset, const char *from, size_t len)\n{\n\tchar *to = kmap_atomic(page);\n\tmemcpy(to + offset, from, len);\n\tkunmap_atomic(to);\n}\n\nstatic void memzero_page(struct page *page, size_t offset, size_t len)\n{\n\tchar *addr = kmap_atomic(page);\n\tmemset(addr + offset, 0, len);\n\tkunmap_atomic(addr);\n}\n\nstatic inline bool allocated(struct pipe_buffer *buf)\n{\n\treturn buf->ops == &default_pipe_buf_ops;\n}\n\nstatic inline void data_start(const struct iov_iter *i, int *idxp, size_t *offp)\n{\n\tsize_t off = i->iov_offset;\n\tint idx = i->idx;\n\tif (off && (!allocated(&i->pipe->bufs[idx]) || off == PAGE_SIZE)) {\n\t\tidx = next_idx(idx, i->pipe);\n\t\toff = 0;\n\t}\n\t*idxp = idx;\n\t*offp = off;\n}\n\nstatic size_t push_pipe(struct iov_iter *i, size_t size,\n\t\t\tint *idxp, size_t *offp)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tsize_t off;\n\tint idx;\n\tssize_t left;\n\n\tif (unlikely(size > i->count))\n\t\tsize = i->count;\n\tif (unlikely(!size))\n\t\treturn 0;\n\n\tleft = size;\n\tdata_start(i, &idx, &off);\n\t*idxp = idx;\n\t*offp = off;\n\tif (off) {\n\t\tleft -= PAGE_SIZE - off;\n\t\tif (left <= 0) {\n\t\t\tpipe->bufs[idx].len += size;\n\t\t\treturn size;\n\t\t}\n\t\tpipe->bufs[idx].len = PAGE_SIZE;\n\t\tidx = next_idx(idx, pipe);\n\t}\n\twhile (idx != pipe->curbuf || !pipe->nrbufs) {\n\t\tstruct page *page = alloc_page(GFP_USER);\n\t\tif (!page)\n\t\t\tbreak;\n\t\tpipe->nrbufs++;\n\t\tpipe->bufs[idx].ops = &default_pipe_buf_ops;\n\t\tpipe->bufs[idx].page = page;\n\t\tpipe->bufs[idx].offset = 0;\n\t\tif (left <= PAGE_SIZE) {\n\t\t\tpipe->bufs[idx].len = left;\n\t\t\treturn size;\n\t\t}\n\t\tpipe->bufs[idx].len = PAGE_SIZE;\n\t\tleft -= PAGE_SIZE;\n\t\tidx = next_idx(idx, pipe);\n\t}\n\treturn size - left;\n}\n\nstatic size_t copy_pipe_to_iter(const void *addr, size_t bytes,\n\t\t\t\tstruct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tsize_t n, off;\n\tint idx;\n\n\tif (!sanity(i))\n\t\treturn 0;\n\n\tbytes = n = push_pipe(i, bytes, &idx, &off);\n\tif (unlikely(!n))\n\t\treturn 0;\n\tfor ( ; n; idx = next_idx(idx, pipe), off = 0) {\n\t\tsize_t chunk = min_t(size_t, n, PAGE_SIZE - off);\n\t\tmemcpy_to_page(pipe->bufs[idx].page, off, addr, chunk);\n\t\ti->idx = idx;\n\t\ti->iov_offset = off + chunk;\n\t\tn -= chunk;\n\t\taddr += chunk;\n\t}\n\ti->count -= bytes;\n\treturn bytes;\n}\n\nsize_t copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i)\n{\n\tconst char *from = addr;\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn copy_pipe_to_iter(addr, bytes, i);\n\titerate_and_advance(i, bytes, v,\n\t\t__copy_to_user(v.iov_base, (from += v.iov_len) - v.iov_len,\n\t\t\t       v.iov_len),\n\t\tmemcpy_to_page(v.bv_page, v.bv_offset,\n\t\t\t       (from += v.bv_len) - v.bv_len, v.bv_len),\n\t\tmemcpy(v.iov_base, (from += v.iov_len) - v.iov_len, v.iov_len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(copy_to_iter);\n\nsize_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tchar *to = addr;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\titerate_and_advance(i, bytes, v,\n\t\t__copy_from_user((to += v.iov_len) - v.iov_len, v.iov_base,\n\t\t\t\t v.iov_len),\n\t\tmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(copy_from_iter);\n\nbool copy_from_iter_full(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tchar *to = addr;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn false;\n\t}\n\tif (unlikely(i->count < bytes))\n\t\treturn false;\n\n\titerate_all_kinds(i, bytes, v, ({\n\t\tif (__copy_from_user((to += v.iov_len) - v.iov_len,\n\t\t\t\t      v.iov_base, v.iov_len))\n\t\t\treturn false;\n\t\t0;}),\n\t\tmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\n\tiov_iter_advance(i, bytes);\n\treturn true;\n}\nEXPORT_SYMBOL(copy_from_iter_full);\n\nsize_t copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tchar *to = addr;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\titerate_and_advance(i, bytes, v,\n\t\t__copy_from_user_nocache((to += v.iov_len) - v.iov_len,\n\t\t\t\t\t v.iov_base, v.iov_len),\n\t\tmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(copy_from_iter_nocache);\n\nbool copy_from_iter_full_nocache(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tchar *to = addr;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn false;\n\t}\n\tif (unlikely(i->count < bytes))\n\t\treturn false;\n\titerate_all_kinds(i, bytes, v, ({\n\t\tif (__copy_from_user_nocache((to += v.iov_len) - v.iov_len,\n\t\t\t\t\t     v.iov_base, v.iov_len))\n\t\t\treturn false;\n\t\t0;}),\n\t\tmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\n\tiov_iter_advance(i, bytes);\n\treturn true;\n}\nEXPORT_SYMBOL(copy_from_iter_full_nocache);\n\nsize_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tif (i->type & (ITER_BVEC|ITER_KVEC)) {\n\t\tvoid *kaddr = kmap_atomic(page);\n\t\tsize_t wanted = copy_to_iter(kaddr + offset, bytes, i);\n\t\tkunmap_atomic(kaddr);\n\t\treturn wanted;\n\t} else if (likely(!(i->type & ITER_PIPE)))\n\t\treturn copy_page_to_iter_iovec(page, offset, bytes, i);\n\telse\n\t\treturn copy_page_to_iter_pipe(page, offset, bytes, i);\n}\nEXPORT_SYMBOL(copy_page_to_iter);\n\nsize_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\tif (i->type & (ITER_BVEC|ITER_KVEC)) {\n\t\tvoid *kaddr = kmap_atomic(page);\n\t\tsize_t wanted = copy_from_iter(kaddr + offset, bytes, i);\n\t\tkunmap_atomic(kaddr);\n\t\treturn wanted;\n\t} else\n\t\treturn copy_page_from_iter_iovec(page, offset, bytes, i);\n}\nEXPORT_SYMBOL(copy_page_from_iter);\n\nstatic size_t pipe_zero(size_t bytes, struct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tsize_t n, off;\n\tint idx;\n\n\tif (!sanity(i))\n\t\treturn 0;\n\n\tbytes = n = push_pipe(i, bytes, &idx, &off);\n\tif (unlikely(!n))\n\t\treturn 0;\n\n\tfor ( ; n; idx = next_idx(idx, pipe), off = 0) {\n\t\tsize_t chunk = min_t(size_t, n, PAGE_SIZE - off);\n\t\tmemzero_page(pipe->bufs[idx].page, off, chunk);\n\t\ti->idx = idx;\n\t\ti->iov_offset = off + chunk;\n\t\tn -= chunk;\n\t}\n\ti->count -= bytes;\n\treturn bytes;\n}\n\nsize_t iov_iter_zero(size_t bytes, struct iov_iter *i)\n{\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn pipe_zero(bytes, i);\n\titerate_and_advance(i, bytes, v,\n\t\t__clear_user(v.iov_base, v.iov_len),\n\t\tmemzero_page(v.bv_page, v.bv_offset, v.bv_len),\n\t\tmemset(v.iov_base, 0, v.iov_len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(iov_iter_zero);\n\nsize_t iov_iter_copy_from_user_atomic(struct page *page,\n\t\tstruct iov_iter *i, unsigned long offset, size_t bytes)\n{\n\tchar *kaddr = kmap_atomic(page), *p = kaddr + offset;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tkunmap_atomic(kaddr);\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\titerate_all_kinds(i, bytes, v,\n\t\t__copy_from_user_inatomic((p += v.iov_len) - v.iov_len,\n\t\t\t\t\t  v.iov_base, v.iov_len),\n\t\tmemcpy_from_page((p += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\tkunmap_atomic(kaddr);\n\treturn bytes;\n}\nEXPORT_SYMBOL(iov_iter_copy_from_user_atomic);\n\nstatic void pipe_advance(struct iov_iter *i, size_t size)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tstruct pipe_buffer *buf;\n\tint idx = i->idx;\n\tsize_t off = i->iov_offset, orig_sz;\n\t\n\tif (unlikely(i->count < size))\n\t\tsize = i->count;\n\torig_sz = size;\n\n\tif (size) {\n\t\tif (off) /* make it relative to the beginning of buffer */\n\t\t\tsize += off - pipe->bufs[idx].offset;\n\t\twhile (1) {\n\t\t\tbuf = &pipe->bufs[idx];\n\t\t\tif (size <= buf->len)\n\t\t\t\tbreak;\n\t\t\tsize -= buf->len;\n\t\t\tidx = next_idx(idx, pipe);\n\t\t}\n\t\tbuf->len = size;\n\t\ti->idx = idx;\n\t\toff = i->iov_offset = buf->offset + size;\n\t}\n\tif (off)\n\t\tidx = next_idx(idx, pipe);\n\tif (pipe->nrbufs) {\n\t\tint unused = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\t/* [curbuf,unused) is in use.  Free [idx,unused) */\n\t\twhile (idx != unused) {\n\t\t\tpipe_buf_release(pipe, &pipe->bufs[idx]);\n\t\t\tidx = next_idx(idx, pipe);\n\t\t\tpipe->nrbufs--;\n\t\t}\n\t}\n\ti->count -= orig_sz;\n}\n\nvoid iov_iter_advance(struct iov_iter *i, size_t size)\n{\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tpipe_advance(i, size);\n\t\treturn;\n\t}\n\titerate_and_advance(i, size, v, 0, 0, 0)\n}\nEXPORT_SYMBOL(iov_iter_advance);\n\n/*\n * Return the count of just the current iov_iter segment.\n */\nsize_t iov_iter_single_seg_count(const struct iov_iter *i)\n{\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn i->count;\t// it is a silly place, anyway\n\tif (i->nr_segs == 1)\n\t\treturn i->count;\n\telse if (i->type & ITER_BVEC)\n\t\treturn min(i->count, i->bvec->bv_len - i->iov_offset);\n\telse\n\t\treturn min(i->count, i->iov->iov_len - i->iov_offset);\n}\nEXPORT_SYMBOL(iov_iter_single_seg_count);\n\nvoid iov_iter_kvec(struct iov_iter *i, int direction,\n\t\t\tconst struct kvec *kvec, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\tBUG_ON(!(direction & ITER_KVEC));\n\ti->type = direction;\n\ti->kvec = kvec;\n\ti->nr_segs = nr_segs;\n\ti->iov_offset = 0;\n\ti->count = count;\n}\nEXPORT_SYMBOL(iov_iter_kvec);\n\nvoid iov_iter_bvec(struct iov_iter *i, int direction,\n\t\t\tconst struct bio_vec *bvec, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\tBUG_ON(!(direction & ITER_BVEC));\n\ti->type = direction;\n\ti->bvec = bvec;\n\ti->nr_segs = nr_segs;\n\ti->iov_offset = 0;\n\ti->count = count;\n}\nEXPORT_SYMBOL(iov_iter_bvec);\n\nvoid iov_iter_pipe(struct iov_iter *i, int direction,\n\t\t\tstruct pipe_inode_info *pipe,\n\t\t\tsize_t count)\n{\n\tBUG_ON(direction != ITER_PIPE);\n\ti->type = direction;\n\ti->pipe = pipe;\n\ti->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\ti->iov_offset = 0;\n\ti->count = count;\n}\nEXPORT_SYMBOL(iov_iter_pipe);\n\nunsigned long iov_iter_alignment(const struct iov_iter *i)\n{\n\tunsigned long res = 0;\n\tsize_t size = i->count;\n\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tif (size && i->iov_offset && allocated(&i->pipe->bufs[i->idx]))\n\t\t\treturn size | i->iov_offset;\n\t\treturn size;\n\t}\n\titerate_all_kinds(i, size, v,\n\t\t(res |= (unsigned long)v.iov_base | v.iov_len, 0),\n\t\tres |= v.bv_offset | v.bv_len,\n\t\tres |= (unsigned long)v.iov_base | v.iov_len\n\t)\n\treturn res;\n}\nEXPORT_SYMBOL(iov_iter_alignment);\n\nunsigned long iov_iter_gap_alignment(const struct iov_iter *i)\n{\n\tunsigned long res = 0;\n\tsize_t size = i->count;\n\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn ~0U;\n\t}\n\n\titerate_all_kinds(i, size, v,\n\t\t(res |= (!res ? 0 : (unsigned long)v.iov_base) |\n\t\t\t(size != v.iov_len ? size : 0), 0),\n\t\t(res |= (!res ? 0 : (unsigned long)v.bv_offset) |\n\t\t\t(size != v.bv_len ? size : 0)),\n\t\t(res |= (!res ? 0 : (unsigned long)v.iov_base) |\n\t\t\t(size != v.iov_len ? size : 0))\n\t\t);\n\treturn res;\n}\nEXPORT_SYMBOL(iov_iter_gap_alignment);\n\nstatic inline size_t __pipe_get_pages(struct iov_iter *i,\n\t\t\t\tsize_t maxsize,\n\t\t\t\tstruct page **pages,\n\t\t\t\tint idx,\n\t\t\t\tsize_t *start)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tssize_t n = push_pipe(i, maxsize, &idx, start);\n\tif (!n)\n\t\treturn -EFAULT;\n\n\tmaxsize = n;\n\tn += *start;\n\twhile (n > 0) {\n\t\tget_page(*pages++ = pipe->bufs[idx].page);\n\t\tidx = next_idx(idx, pipe);\n\t\tn -= PAGE_SIZE;\n\t}\n\n\treturn maxsize;\n}\n\nstatic ssize_t pipe_get_pages(struct iov_iter *i,\n\t\t   struct page **pages, size_t maxsize, unsigned maxpages,\n\t\t   size_t *start)\n{\n\tunsigned npages;\n\tsize_t capacity;\n\tint idx;\n\n\tif (!maxsize)\n\t\treturn 0;\n\n\tif (!sanity(i))\n\t\treturn -EFAULT;\n\n\tdata_start(i, &idx, start);\n\t/* some of this one + all after this one */\n\tnpages = ((i->pipe->curbuf - idx - 1) & (i->pipe->buffers - 1)) + 1;\n\tcapacity = min(npages,maxpages) * PAGE_SIZE - *start;\n\n\treturn __pipe_get_pages(i, min(maxsize, capacity), pages, idx, start);\n}\n\nssize_t iov_iter_get_pages(struct iov_iter *i,\n\t\t   struct page **pages, size_t maxsize, unsigned maxpages,\n\t\t   size_t *start)\n{\n\tif (maxsize > i->count)\n\t\tmaxsize = i->count;\n\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn pipe_get_pages(i, pages, maxsize, maxpages, start);\n\titerate_all_kinds(i, maxsize, v, ({\n\t\tunsigned long addr = (unsigned long)v.iov_base;\n\t\tsize_t len = v.iov_len + (*start = addr & (PAGE_SIZE - 1));\n\t\tint n;\n\t\tint res;\n\n\t\tif (len > maxpages * PAGE_SIZE)\n\t\t\tlen = maxpages * PAGE_SIZE;\n\t\taddr &= ~(PAGE_SIZE - 1);\n\t\tn = DIV_ROUND_UP(len, PAGE_SIZE);\n\t\tres = get_user_pages_fast(addr, n, (i->type & WRITE) != WRITE, pages);\n\t\tif (unlikely(res < 0))\n\t\t\treturn res;\n\t\treturn (res == n ? len : res * PAGE_SIZE) - *start;\n\t0;}),({\n\t\t/* can't be more than PAGE_SIZE */\n\t\t*start = v.bv_offset;\n\t\tget_page(*pages = v.bv_page);\n\t\treturn v.bv_len;\n\t}),({\n\t\treturn -EFAULT;\n\t})\n\t)\n\treturn 0;\n}\nEXPORT_SYMBOL(iov_iter_get_pages);\n\nstatic struct page **get_pages_array(size_t n)\n{\n\tstruct page **p = kmalloc(n * sizeof(struct page *), GFP_KERNEL);\n\tif (!p)\n\t\tp = vmalloc(n * sizeof(struct page *));\n\treturn p;\n}\n\nstatic ssize_t pipe_get_pages_alloc(struct iov_iter *i,\n\t\t   struct page ***pages, size_t maxsize,\n\t\t   size_t *start)\n{\n\tstruct page **p;\n\tsize_t n;\n\tint idx;\n\tint npages;\n\n\tif (!maxsize)\n\t\treturn 0;\n\n\tif (!sanity(i))\n\t\treturn -EFAULT;\n\n\tdata_start(i, &idx, start);\n\t/* some of this one + all after this one */\n\tnpages = ((i->pipe->curbuf - idx - 1) & (i->pipe->buffers - 1)) + 1;\n\tn = npages * PAGE_SIZE - *start;\n\tif (maxsize > n)\n\t\tmaxsize = n;\n\telse\n\t\tnpages = DIV_ROUND_UP(maxsize + *start, PAGE_SIZE);\n\tp = get_pages_array(npages);\n\tif (!p)\n\t\treturn -ENOMEM;\n\tn = __pipe_get_pages(i, maxsize, p, idx, start);\n\tif (n > 0)\n\t\t*pages = p;\n\telse\n\t\tkvfree(p);\n\treturn n;\n}\n\nssize_t iov_iter_get_pages_alloc(struct iov_iter *i,\n\t\t   struct page ***pages, size_t maxsize,\n\t\t   size_t *start)\n{\n\tstruct page **p;\n\n\tif (maxsize > i->count)\n\t\tmaxsize = i->count;\n\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn pipe_get_pages_alloc(i, pages, maxsize, start);\n\titerate_all_kinds(i, maxsize, v, ({\n\t\tunsigned long addr = (unsigned long)v.iov_base;\n\t\tsize_t len = v.iov_len + (*start = addr & (PAGE_SIZE - 1));\n\t\tint n;\n\t\tint res;\n\n\t\taddr &= ~(PAGE_SIZE - 1);\n\t\tn = DIV_ROUND_UP(len, PAGE_SIZE);\n\t\tp = get_pages_array(n);\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\t\tres = get_user_pages_fast(addr, n, (i->type & WRITE) != WRITE, p);\n\t\tif (unlikely(res < 0)) {\n\t\t\tkvfree(p);\n\t\t\treturn res;\n\t\t}\n\t\t*pages = p;\n\t\treturn (res == n ? len : res * PAGE_SIZE) - *start;\n\t0;}),({\n\t\t/* can't be more than PAGE_SIZE */\n\t\t*start = v.bv_offset;\n\t\t*pages = p = get_pages_array(1);\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\t\tget_page(*p = v.bv_page);\n\t\treturn v.bv_len;\n\t}),({\n\t\treturn -EFAULT;\n\t})\n\t)\n\treturn 0;\n}\nEXPORT_SYMBOL(iov_iter_get_pages_alloc);\n\nsize_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum,\n\t\t\t       struct iov_iter *i)\n{\n\tchar *to = addr;\n\t__wsum sum, next;\n\tsize_t off = 0;\n\tsum = *csum;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\titerate_and_advance(i, bytes, v, ({\n\t\tint err = 0;\n\t\tnext = csum_and_copy_from_user(v.iov_base,\n\t\t\t\t\t       (to += v.iov_len) - v.iov_len,\n\t\t\t\t\t       v.iov_len, 0, &err);\n\t\tif (!err) {\n\t\t\tsum = csum_block_add(sum, next, off);\n\t\t\toff += v.iov_len;\n\t\t}\n\t\terr ? v.iov_len : 0;\n\t}), ({\n\t\tchar *p = kmap_atomic(v.bv_page);\n\t\tnext = csum_partial_copy_nocheck(p + v.bv_offset,\n\t\t\t\t\t\t (to += v.bv_len) - v.bv_len,\n\t\t\t\t\t\t v.bv_len, 0);\n\t\tkunmap_atomic(p);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.bv_len;\n\t}),({\n\t\tnext = csum_partial_copy_nocheck(v.iov_base,\n\t\t\t\t\t\t (to += v.iov_len) - v.iov_len,\n\t\t\t\t\t\t v.iov_len, 0);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.iov_len;\n\t})\n\t)\n\t*csum = sum;\n\treturn bytes;\n}\nEXPORT_SYMBOL(csum_and_copy_from_iter);\n\nbool csum_and_copy_from_iter_full(void *addr, size_t bytes, __wsum *csum,\n\t\t\t       struct iov_iter *i)\n{\n\tchar *to = addr;\n\t__wsum sum, next;\n\tsize_t off = 0;\n\tsum = *csum;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn false;\n\t}\n\tif (unlikely(i->count < bytes))\n\t\treturn false;\n\titerate_all_kinds(i, bytes, v, ({\n\t\tint err = 0;\n\t\tnext = csum_and_copy_from_user(v.iov_base,\n\t\t\t\t\t       (to += v.iov_len) - v.iov_len,\n\t\t\t\t\t       v.iov_len, 0, &err);\n\t\tif (err)\n\t\t\treturn false;\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.iov_len;\n\t\t0;\n\t}), ({\n\t\tchar *p = kmap_atomic(v.bv_page);\n\t\tnext = csum_partial_copy_nocheck(p + v.bv_offset,\n\t\t\t\t\t\t (to += v.bv_len) - v.bv_len,\n\t\t\t\t\t\t v.bv_len, 0);\n\t\tkunmap_atomic(p);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.bv_len;\n\t}),({\n\t\tnext = csum_partial_copy_nocheck(v.iov_base,\n\t\t\t\t\t\t (to += v.iov_len) - v.iov_len,\n\t\t\t\t\t\t v.iov_len, 0);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.iov_len;\n\t})\n\t)\n\t*csum = sum;\n\tiov_iter_advance(i, bytes);\n\treturn true;\n}\nEXPORT_SYMBOL(csum_and_copy_from_iter_full);\n\nsize_t csum_and_copy_to_iter(const void *addr, size_t bytes, __wsum *csum,\n\t\t\t     struct iov_iter *i)\n{\n\tconst char *from = addr;\n\t__wsum sum, next;\n\tsize_t off = 0;\n\tsum = *csum;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\t/* for now */\n\t\treturn 0;\n\t}\n\titerate_and_advance(i, bytes, v, ({\n\t\tint err = 0;\n\t\tnext = csum_and_copy_to_user((from += v.iov_len) - v.iov_len,\n\t\t\t\t\t     v.iov_base,\n\t\t\t\t\t     v.iov_len, 0, &err);\n\t\tif (!err) {\n\t\t\tsum = csum_block_add(sum, next, off);\n\t\t\toff += v.iov_len;\n\t\t}\n\t\terr ? v.iov_len : 0;\n\t}), ({\n\t\tchar *p = kmap_atomic(v.bv_page);\n\t\tnext = csum_partial_copy_nocheck((from += v.bv_len) - v.bv_len,\n\t\t\t\t\t\t p + v.bv_offset,\n\t\t\t\t\t\t v.bv_len, 0);\n\t\tkunmap_atomic(p);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.bv_len;\n\t}),({\n\t\tnext = csum_partial_copy_nocheck((from += v.iov_len) - v.iov_len,\n\t\t\t\t\t\t v.iov_base,\n\t\t\t\t\t\t v.iov_len, 0);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.iov_len;\n\t})\n\t)\n\t*csum = sum;\n\treturn bytes;\n}\nEXPORT_SYMBOL(csum_and_copy_to_iter);\n\nint iov_iter_npages(const struct iov_iter *i, int maxpages)\n{\n\tsize_t size = i->count;\n\tint npages = 0;\n\n\tif (!size)\n\t\treturn 0;\n\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tstruct pipe_inode_info *pipe = i->pipe;\n\t\tsize_t off;\n\t\tint idx;\n\n\t\tif (!sanity(i))\n\t\t\treturn 0;\n\n\t\tdata_start(i, &idx, &off);\n\t\t/* some of this one + all after this one */\n\t\tnpages = ((pipe->curbuf - idx - 1) & (pipe->buffers - 1)) + 1;\n\t\tif (npages >= maxpages)\n\t\t\treturn maxpages;\n\t} else iterate_all_kinds(i, size, v, ({\n\t\tunsigned long p = (unsigned long)v.iov_base;\n\t\tnpages += DIV_ROUND_UP(p + v.iov_len, PAGE_SIZE)\n\t\t\t- p / PAGE_SIZE;\n\t\tif (npages >= maxpages)\n\t\t\treturn maxpages;\n\t0;}),({\n\t\tnpages++;\n\t\tif (npages >= maxpages)\n\t\t\treturn maxpages;\n\t}),({\n\t\tunsigned long p = (unsigned long)v.iov_base;\n\t\tnpages += DIV_ROUND_UP(p + v.iov_len, PAGE_SIZE)\n\t\t\t- p / PAGE_SIZE;\n\t\tif (npages >= maxpages)\n\t\t\treturn maxpages;\n\t})\n\t)\n\treturn npages;\n}\nEXPORT_SYMBOL(iov_iter_npages);\n\nconst void *dup_iter(struct iov_iter *new, struct iov_iter *old, gfp_t flags)\n{\n\t*new = *old;\n\tif (unlikely(new->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn NULL;\n\t}\n\tif (new->type & ITER_BVEC)\n\t\treturn new->bvec = kmemdup(new->bvec,\n\t\t\t\t    new->nr_segs * sizeof(struct bio_vec),\n\t\t\t\t    flags);\n\telse\n\t\t/* iovec and kvec have identical layout */\n\t\treturn new->iov = kmemdup(new->iov,\n\t\t\t\t   new->nr_segs * sizeof(struct iovec),\n\t\t\t\t   flags);\n}\nEXPORT_SYMBOL(dup_iter);\n\n/**\n * import_iovec() - Copy an array of &struct iovec from userspace\n *     into the kernel, check that it is valid, and initialize a new\n *     &struct iov_iter iterator to access it.\n *\n * @type: One of %READ or %WRITE.\n * @uvector: Pointer to the userspace array.\n * @nr_segs: Number of elements in userspace array.\n * @fast_segs: Number of elements in @iov.\n * @iov: (input and output parameter) Pointer to pointer to (usually small\n *     on-stack) kernel array.\n * @i: Pointer to iterator that will be initialized on success.\n *\n * If the array pointed to by *@iov is large enough to hold all @nr_segs,\n * then this function places %NULL in *@iov on return. Otherwise, a new\n * array will be allocated and the result placed in *@iov. This means that\n * the caller may call kfree() on *@iov regardless of whether the small\n * on-stack array was used or not (and regardless of whether this function\n * returns an error or not).\n *\n * Return: 0 on success or negative error code on error.\n */\nint import_iovec(int type, const struct iovec __user * uvector,\n\t\t unsigned nr_segs, unsigned fast_segs,\n\t\t struct iovec **iov, struct iov_iter *i)\n{\n\tssize_t n;\n\tstruct iovec *p;\n\tn = rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,\n\t\t\t\t  *iov, &p);\n\tif (n < 0) {\n\t\tif (p != *iov)\n\t\t\tkfree(p);\n\t\t*iov = NULL;\n\t\treturn n;\n\t}\n\tiov_iter_init(i, type, p, nr_segs, n);\n\t*iov = p == *iov ? NULL : p;\n\treturn 0;\n}\nEXPORT_SYMBOL(import_iovec);\n\n#ifdef CONFIG_COMPAT\n#include <linux/compat.h>\n\nint compat_import_iovec(int type, const struct compat_iovec __user * uvector,\n\t\t unsigned nr_segs, unsigned fast_segs,\n\t\t struct iovec **iov, struct iov_iter *i)\n{\n\tssize_t n;\n\tstruct iovec *p;\n\tn = compat_rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,\n\t\t\t\t  *iov, &p);\n\tif (n < 0) {\n\t\tif (p != *iov)\n\t\t\tkfree(p);\n\t\t*iov = NULL;\n\t\treturn n;\n\t}\n\tiov_iter_init(i, type, p, nr_segs, n);\n\t*iov = p == *iov ? NULL : p;\n\treturn 0;\n}\n#endif\n\nint import_single_range(int rw, void __user *buf, size_t len,\n\t\t struct iovec *iov, struct iov_iter *i)\n{\n\tif (len > MAX_RW_COUNT)\n\t\tlen = MAX_RW_COUNT;\n\tif (unlikely(!access_ok(!rw, buf, len)))\n\t\treturn -EFAULT;\n\n\tiov->iov_base = buf;\n\tiov->iov_len = len;\n\tiov_iter_init(i, rw, iov, 1, len);\n\treturn 0;\n}\nEXPORT_SYMBOL(import_single_range);\n"], "fixing_code": ["#include <linux/export.h>\n#include <linux/bvec.h>\n#include <linux/uio.h>\n#include <linux/pagemap.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/splice.h>\n#include <net/checksum.h>\n\n#define PIPE_PARANOIA /* for now */\n\n#define iterate_iovec(i, n, __v, __p, skip, STEP) {\t\\\n\tsize_t left;\t\t\t\t\t\\\n\tsize_t wanted = n;\t\t\t\t\\\n\t__p = i->iov;\t\t\t\t\t\\\n\t__v.iov_len = min(n, __p->iov_len - skip);\t\\\n\tif (likely(__v.iov_len)) {\t\t\t\\\n\t\t__v.iov_base = __p->iov_base + skip;\t\\\n\t\tleft = (STEP);\t\t\t\t\\\n\t\t__v.iov_len -= left;\t\t\t\\\n\t\tskip += __v.iov_len;\t\t\t\\\n\t\tn -= __v.iov_len;\t\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\tleft = 0;\t\t\t\t\\\n\t}\t\t\t\t\t\t\\\n\twhile (unlikely(!left && n)) {\t\t\t\\\n\t\t__p++;\t\t\t\t\t\\\n\t\t__v.iov_len = min(n, __p->iov_len);\t\\\n\t\tif (unlikely(!__v.iov_len))\t\t\\\n\t\t\tcontinue;\t\t\t\\\n\t\t__v.iov_base = __p->iov_base;\t\t\\\n\t\tleft = (STEP);\t\t\t\t\\\n\t\t__v.iov_len -= left;\t\t\t\\\n\t\tskip = __v.iov_len;\t\t\t\\\n\t\tn -= __v.iov_len;\t\t\t\\\n\t}\t\t\t\t\t\t\\\n\tn = wanted - n;\t\t\t\t\t\\\n}\n\n#define iterate_kvec(i, n, __v, __p, skip, STEP) {\t\\\n\tsize_t wanted = n;\t\t\t\t\\\n\t__p = i->kvec;\t\t\t\t\t\\\n\t__v.iov_len = min(n, __p->iov_len - skip);\t\\\n\tif (likely(__v.iov_len)) {\t\t\t\\\n\t\t__v.iov_base = __p->iov_base + skip;\t\\\n\t\t(void)(STEP);\t\t\t\t\\\n\t\tskip += __v.iov_len;\t\t\t\\\n\t\tn -= __v.iov_len;\t\t\t\\\n\t}\t\t\t\t\t\t\\\n\twhile (unlikely(n)) {\t\t\t\t\\\n\t\t__p++;\t\t\t\t\t\\\n\t\t__v.iov_len = min(n, __p->iov_len);\t\\\n\t\tif (unlikely(!__v.iov_len))\t\t\\\n\t\t\tcontinue;\t\t\t\\\n\t\t__v.iov_base = __p->iov_base;\t\t\\\n\t\t(void)(STEP);\t\t\t\t\\\n\t\tskip = __v.iov_len;\t\t\t\\\n\t\tn -= __v.iov_len;\t\t\t\\\n\t}\t\t\t\t\t\t\\\n\tn = wanted;\t\t\t\t\t\\\n}\n\n#define iterate_bvec(i, n, __v, __bi, skip, STEP) {\t\\\n\tstruct bvec_iter __start;\t\t\t\\\n\t__start.bi_size = n;\t\t\t\t\\\n\t__start.bi_bvec_done = skip;\t\t\t\\\n\t__start.bi_idx = 0;\t\t\t\t\\\n\tfor_each_bvec(__v, i->bvec, __bi, __start) {\t\\\n\t\tif (!__v.bv_len)\t\t\t\\\n\t\t\tcontinue;\t\t\t\\\n\t\t(void)(STEP);\t\t\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define iterate_all_kinds(i, n, v, I, B, K) {\t\t\t\\\n\tif (likely(n)) {\t\t\t\t\t\\\n\t\tsize_t skip = i->iov_offset;\t\t\t\\\n\t\tif (unlikely(i->type & ITER_BVEC)) {\t\t\\\n\t\t\tstruct bio_vec v;\t\t\t\\\n\t\t\tstruct bvec_iter __bi;\t\t\t\\\n\t\t\titerate_bvec(i, n, v, __bi, skip, (B))\t\\\n\t\t} else if (unlikely(i->type & ITER_KVEC)) {\t\\\n\t\t\tconst struct kvec *kvec;\t\t\\\n\t\t\tstruct kvec v;\t\t\t\t\\\n\t\t\titerate_kvec(i, n, v, kvec, skip, (K))\t\\\n\t\t} else {\t\t\t\t\t\\\n\t\t\tconst struct iovec *iov;\t\t\\\n\t\t\tstruct iovec v;\t\t\t\t\\\n\t\t\titerate_iovec(i, n, v, iov, skip, (I))\t\\\n\t\t}\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n}\n\n#define iterate_and_advance(i, n, v, I, B, K) {\t\t\t\\\n\tif (unlikely(i->count < n))\t\t\t\t\\\n\t\tn = i->count;\t\t\t\t\t\\\n\tif (i->count) {\t\t\t\t\t\t\\\n\t\tsize_t skip = i->iov_offset;\t\t\t\\\n\t\tif (unlikely(i->type & ITER_BVEC)) {\t\t\\\n\t\t\tconst struct bio_vec *bvec = i->bvec;\t\\\n\t\t\tstruct bio_vec v;\t\t\t\\\n\t\t\tstruct bvec_iter __bi;\t\t\t\\\n\t\t\titerate_bvec(i, n, v, __bi, skip, (B))\t\\\n\t\t\ti->bvec = __bvec_iter_bvec(i->bvec, __bi);\t\\\n\t\t\ti->nr_segs -= i->bvec - bvec;\t\t\\\n\t\t\tskip = __bi.bi_bvec_done;\t\t\\\n\t\t} else if (unlikely(i->type & ITER_KVEC)) {\t\\\n\t\t\tconst struct kvec *kvec;\t\t\\\n\t\t\tstruct kvec v;\t\t\t\t\\\n\t\t\titerate_kvec(i, n, v, kvec, skip, (K))\t\\\n\t\t\tif (skip == kvec->iov_len) {\t\t\\\n\t\t\t\tkvec++;\t\t\t\t\\\n\t\t\t\tskip = 0;\t\t\t\\\n\t\t\t}\t\t\t\t\t\\\n\t\t\ti->nr_segs -= kvec - i->kvec;\t\t\\\n\t\t\ti->kvec = kvec;\t\t\t\t\\\n\t\t} else {\t\t\t\t\t\\\n\t\t\tconst struct iovec *iov;\t\t\\\n\t\t\tstruct iovec v;\t\t\t\t\\\n\t\t\titerate_iovec(i, n, v, iov, skip, (I))\t\\\n\t\t\tif (skip == iov->iov_len) {\t\t\\\n\t\t\t\tiov++;\t\t\t\t\\\n\t\t\t\tskip = 0;\t\t\t\\\n\t\t\t}\t\t\t\t\t\\\n\t\t\ti->nr_segs -= iov - i->iov;\t\t\\\n\t\t\ti->iov = iov;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\ti->count -= n;\t\t\t\t\t\\\n\t\ti->iov_offset = skip;\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n}\n\nstatic size_t copy_page_to_iter_iovec(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tsize_t skip, copy, left, wanted;\n\tconst struct iovec *iov;\n\tchar __user *buf;\n\tvoid *kaddr, *from;\n\n\tif (unlikely(bytes > i->count))\n\t\tbytes = i->count;\n\n\tif (unlikely(!bytes))\n\t\treturn 0;\n\n\twanted = bytes;\n\tiov = i->iov;\n\tskip = i->iov_offset;\n\tbuf = iov->iov_base + skip;\n\tcopy = min(bytes, iov->iov_len - skip);\n\n\tif (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_pages_writeable(buf, copy)) {\n\t\tkaddr = kmap_atomic(page);\n\t\tfrom = kaddr + offset;\n\n\t\t/* first chunk, usually the only one */\n\t\tleft = __copy_to_user_inatomic(buf, from, copy);\n\t\tcopy -= left;\n\t\tskip += copy;\n\t\tfrom += copy;\n\t\tbytes -= copy;\n\n\t\twhile (unlikely(!left && bytes)) {\n\t\t\tiov++;\n\t\t\tbuf = iov->iov_base;\n\t\t\tcopy = min(bytes, iov->iov_len);\n\t\t\tleft = __copy_to_user_inatomic(buf, from, copy);\n\t\t\tcopy -= left;\n\t\t\tskip = copy;\n\t\t\tfrom += copy;\n\t\t\tbytes -= copy;\n\t\t}\n\t\tif (likely(!bytes)) {\n\t\t\tkunmap_atomic(kaddr);\n\t\t\tgoto done;\n\t\t}\n\t\toffset = from - kaddr;\n\t\tbuf += copy;\n\t\tkunmap_atomic(kaddr);\n\t\tcopy = min(bytes, iov->iov_len - skip);\n\t}\n\t/* Too bad - revert to non-atomic kmap */\n\n\tkaddr = kmap(page);\n\tfrom = kaddr + offset;\n\tleft = __copy_to_user(buf, from, copy);\n\tcopy -= left;\n\tskip += copy;\n\tfrom += copy;\n\tbytes -= copy;\n\twhile (unlikely(!left && bytes)) {\n\t\tiov++;\n\t\tbuf = iov->iov_base;\n\t\tcopy = min(bytes, iov->iov_len);\n\t\tleft = __copy_to_user(buf, from, copy);\n\t\tcopy -= left;\n\t\tskip = copy;\n\t\tfrom += copy;\n\t\tbytes -= copy;\n\t}\n\tkunmap(page);\n\ndone:\n\tif (skip == iov->iov_len) {\n\t\tiov++;\n\t\tskip = 0;\n\t}\n\ti->count -= wanted - bytes;\n\ti->nr_segs -= iov - i->iov;\n\ti->iov = iov;\n\ti->iov_offset = skip;\n\treturn wanted - bytes;\n}\n\nstatic size_t copy_page_from_iter_iovec(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tsize_t skip, copy, left, wanted;\n\tconst struct iovec *iov;\n\tchar __user *buf;\n\tvoid *kaddr, *to;\n\n\tif (unlikely(bytes > i->count))\n\t\tbytes = i->count;\n\n\tif (unlikely(!bytes))\n\t\treturn 0;\n\n\twanted = bytes;\n\tiov = i->iov;\n\tskip = i->iov_offset;\n\tbuf = iov->iov_base + skip;\n\tcopy = min(bytes, iov->iov_len - skip);\n\n\tif (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_pages_readable(buf, copy)) {\n\t\tkaddr = kmap_atomic(page);\n\t\tto = kaddr + offset;\n\n\t\t/* first chunk, usually the only one */\n\t\tleft = __copy_from_user_inatomic(to, buf, copy);\n\t\tcopy -= left;\n\t\tskip += copy;\n\t\tto += copy;\n\t\tbytes -= copy;\n\n\t\twhile (unlikely(!left && bytes)) {\n\t\t\tiov++;\n\t\t\tbuf = iov->iov_base;\n\t\t\tcopy = min(bytes, iov->iov_len);\n\t\t\tleft = __copy_from_user_inatomic(to, buf, copy);\n\t\t\tcopy -= left;\n\t\t\tskip = copy;\n\t\t\tto += copy;\n\t\t\tbytes -= copy;\n\t\t}\n\t\tif (likely(!bytes)) {\n\t\t\tkunmap_atomic(kaddr);\n\t\t\tgoto done;\n\t\t}\n\t\toffset = to - kaddr;\n\t\tbuf += copy;\n\t\tkunmap_atomic(kaddr);\n\t\tcopy = min(bytes, iov->iov_len - skip);\n\t}\n\t/* Too bad - revert to non-atomic kmap */\n\n\tkaddr = kmap(page);\n\tto = kaddr + offset;\n\tleft = __copy_from_user(to, buf, copy);\n\tcopy -= left;\n\tskip += copy;\n\tto += copy;\n\tbytes -= copy;\n\twhile (unlikely(!left && bytes)) {\n\t\tiov++;\n\t\tbuf = iov->iov_base;\n\t\tcopy = min(bytes, iov->iov_len);\n\t\tleft = __copy_from_user(to, buf, copy);\n\t\tcopy -= left;\n\t\tskip = copy;\n\t\tto += copy;\n\t\tbytes -= copy;\n\t}\n\tkunmap(page);\n\ndone:\n\tif (skip == iov->iov_len) {\n\t\tiov++;\n\t\tskip = 0;\n\t}\n\ti->count -= wanted - bytes;\n\ti->nr_segs -= iov - i->iov;\n\ti->iov = iov;\n\ti->iov_offset = skip;\n\treturn wanted - bytes;\n}\n\n#ifdef PIPE_PARANOIA\nstatic bool sanity(const struct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tint idx = i->idx;\n\tint next = pipe->curbuf + pipe->nrbufs;\n\tif (i->iov_offset) {\n\t\tstruct pipe_buffer *p;\n\t\tif (unlikely(!pipe->nrbufs))\n\t\t\tgoto Bad;\t// pipe must be non-empty\n\t\tif (unlikely(idx != ((next - 1) & (pipe->buffers - 1))))\n\t\t\tgoto Bad;\t// must be at the last buffer...\n\n\t\tp = &pipe->bufs[idx];\n\t\tif (unlikely(p->offset + p->len != i->iov_offset))\n\t\t\tgoto Bad;\t// ... at the end of segment\n\t} else {\n\t\tif (idx != (next & (pipe->buffers - 1)))\n\t\t\tgoto Bad;\t// must be right after the last buffer\n\t}\n\treturn true;\nBad:\n\tprintk(KERN_ERR \"idx = %d, offset = %zd\\n\", i->idx, i->iov_offset);\n\tprintk(KERN_ERR \"curbuf = %d, nrbufs = %d, buffers = %d\\n\",\n\t\t\tpipe->curbuf, pipe->nrbufs, pipe->buffers);\n\tfor (idx = 0; idx < pipe->buffers; idx++)\n\t\tprintk(KERN_ERR \"[%p %p %d %d]\\n\",\n\t\t\tpipe->bufs[idx].ops,\n\t\t\tpipe->bufs[idx].page,\n\t\t\tpipe->bufs[idx].offset,\n\t\t\tpipe->bufs[idx].len);\n\tWARN_ON(1);\n\treturn false;\n}\n#else\n#define sanity(i) true\n#endif\n\nstatic inline int next_idx(int idx, struct pipe_inode_info *pipe)\n{\n\treturn (idx + 1) & (pipe->buffers - 1);\n}\n\nstatic size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tstruct pipe_buffer *buf;\n\tsize_t off;\n\tint idx;\n\n\tif (unlikely(bytes > i->count))\n\t\tbytes = i->count;\n\n\tif (unlikely(!bytes))\n\t\treturn 0;\n\n\tif (!sanity(i))\n\t\treturn 0;\n\n\toff = i->iov_offset;\n\tidx = i->idx;\n\tbuf = &pipe->bufs[idx];\n\tif (off) {\n\t\tif (offset == off && buf->page == page) {\n\t\t\t/* merge with the last one */\n\t\t\tbuf->len += bytes;\n\t\t\ti->iov_offset += bytes;\n\t\t\tgoto out;\n\t\t}\n\t\tidx = next_idx(idx, pipe);\n\t\tbuf = &pipe->bufs[idx];\n\t}\n\tif (idx == pipe->curbuf && pipe->nrbufs)\n\t\treturn 0;\n\tpipe->nrbufs++;\n\tbuf->ops = &page_cache_pipe_buf_ops;\n\tget_page(buf->page = page);\n\tbuf->offset = offset;\n\tbuf->len = bytes;\n\ti->iov_offset = offset + bytes;\n\ti->idx = idx;\nout:\n\ti->count -= bytes;\n\treturn bytes;\n}\n\n/*\n * Fault in one or more iovecs of the given iov_iter, to a maximum length of\n * bytes.  For each iovec, fault in each page that constitutes the iovec.\n *\n * Return 0 on success, or non-zero if the memory could not be accessed (i.e.\n * because it is an invalid address).\n */\nint iov_iter_fault_in_readable(struct iov_iter *i, size_t bytes)\n{\n\tsize_t skip = i->iov_offset;\n\tconst struct iovec *iov;\n\tint err;\n\tstruct iovec v;\n\n\tif (!(i->type & (ITER_BVEC|ITER_KVEC))) {\n\t\titerate_iovec(i, bytes, v, iov, skip, ({\n\t\t\terr = fault_in_pages_readable(v.iov_base, v.iov_len);\n\t\t\tif (unlikely(err))\n\t\t\treturn err;\n\t\t0;}))\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(iov_iter_fault_in_readable);\n\nvoid iov_iter_init(struct iov_iter *i, int direction,\n\t\t\tconst struct iovec *iov, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\t/* It will get better.  Eventually... */\n\tif (segment_eq(get_fs(), KERNEL_DS)) {\n\t\tdirection |= ITER_KVEC;\n\t\ti->type = direction;\n\t\ti->kvec = (struct kvec *)iov;\n\t} else {\n\t\ti->type = direction;\n\t\ti->iov = iov;\n\t}\n\ti->nr_segs = nr_segs;\n\ti->iov_offset = 0;\n\ti->count = count;\n}\nEXPORT_SYMBOL(iov_iter_init);\n\nstatic void memcpy_from_page(char *to, struct page *page, size_t offset, size_t len)\n{\n\tchar *from = kmap_atomic(page);\n\tmemcpy(to, from + offset, len);\n\tkunmap_atomic(from);\n}\n\nstatic void memcpy_to_page(struct page *page, size_t offset, const char *from, size_t len)\n{\n\tchar *to = kmap_atomic(page);\n\tmemcpy(to + offset, from, len);\n\tkunmap_atomic(to);\n}\n\nstatic void memzero_page(struct page *page, size_t offset, size_t len)\n{\n\tchar *addr = kmap_atomic(page);\n\tmemset(addr + offset, 0, len);\n\tkunmap_atomic(addr);\n}\n\nstatic inline bool allocated(struct pipe_buffer *buf)\n{\n\treturn buf->ops == &default_pipe_buf_ops;\n}\n\nstatic inline void data_start(const struct iov_iter *i, int *idxp, size_t *offp)\n{\n\tsize_t off = i->iov_offset;\n\tint idx = i->idx;\n\tif (off && (!allocated(&i->pipe->bufs[idx]) || off == PAGE_SIZE)) {\n\t\tidx = next_idx(idx, i->pipe);\n\t\toff = 0;\n\t}\n\t*idxp = idx;\n\t*offp = off;\n}\n\nstatic size_t push_pipe(struct iov_iter *i, size_t size,\n\t\t\tint *idxp, size_t *offp)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tsize_t off;\n\tint idx;\n\tssize_t left;\n\n\tif (unlikely(size > i->count))\n\t\tsize = i->count;\n\tif (unlikely(!size))\n\t\treturn 0;\n\n\tleft = size;\n\tdata_start(i, &idx, &off);\n\t*idxp = idx;\n\t*offp = off;\n\tif (off) {\n\t\tleft -= PAGE_SIZE - off;\n\t\tif (left <= 0) {\n\t\t\tpipe->bufs[idx].len += size;\n\t\t\treturn size;\n\t\t}\n\t\tpipe->bufs[idx].len = PAGE_SIZE;\n\t\tidx = next_idx(idx, pipe);\n\t}\n\twhile (idx != pipe->curbuf || !pipe->nrbufs) {\n\t\tstruct page *page = alloc_page(GFP_USER);\n\t\tif (!page)\n\t\t\tbreak;\n\t\tpipe->nrbufs++;\n\t\tpipe->bufs[idx].ops = &default_pipe_buf_ops;\n\t\tpipe->bufs[idx].page = page;\n\t\tpipe->bufs[idx].offset = 0;\n\t\tif (left <= PAGE_SIZE) {\n\t\t\tpipe->bufs[idx].len = left;\n\t\t\treturn size;\n\t\t}\n\t\tpipe->bufs[idx].len = PAGE_SIZE;\n\t\tleft -= PAGE_SIZE;\n\t\tidx = next_idx(idx, pipe);\n\t}\n\treturn size - left;\n}\n\nstatic size_t copy_pipe_to_iter(const void *addr, size_t bytes,\n\t\t\t\tstruct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tsize_t n, off;\n\tint idx;\n\n\tif (!sanity(i))\n\t\treturn 0;\n\n\tbytes = n = push_pipe(i, bytes, &idx, &off);\n\tif (unlikely(!n))\n\t\treturn 0;\n\tfor ( ; n; idx = next_idx(idx, pipe), off = 0) {\n\t\tsize_t chunk = min_t(size_t, n, PAGE_SIZE - off);\n\t\tmemcpy_to_page(pipe->bufs[idx].page, off, addr, chunk);\n\t\ti->idx = idx;\n\t\ti->iov_offset = off + chunk;\n\t\tn -= chunk;\n\t\taddr += chunk;\n\t}\n\ti->count -= bytes;\n\treturn bytes;\n}\n\nsize_t copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i)\n{\n\tconst char *from = addr;\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn copy_pipe_to_iter(addr, bytes, i);\n\titerate_and_advance(i, bytes, v,\n\t\t__copy_to_user(v.iov_base, (from += v.iov_len) - v.iov_len,\n\t\t\t       v.iov_len),\n\t\tmemcpy_to_page(v.bv_page, v.bv_offset,\n\t\t\t       (from += v.bv_len) - v.bv_len, v.bv_len),\n\t\tmemcpy(v.iov_base, (from += v.iov_len) - v.iov_len, v.iov_len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(copy_to_iter);\n\nsize_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tchar *to = addr;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\titerate_and_advance(i, bytes, v,\n\t\t__copy_from_user((to += v.iov_len) - v.iov_len, v.iov_base,\n\t\t\t\t v.iov_len),\n\t\tmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(copy_from_iter);\n\nbool copy_from_iter_full(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tchar *to = addr;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn false;\n\t}\n\tif (unlikely(i->count < bytes))\n\t\treturn false;\n\n\titerate_all_kinds(i, bytes, v, ({\n\t\tif (__copy_from_user((to += v.iov_len) - v.iov_len,\n\t\t\t\t      v.iov_base, v.iov_len))\n\t\t\treturn false;\n\t\t0;}),\n\t\tmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\n\tiov_iter_advance(i, bytes);\n\treturn true;\n}\nEXPORT_SYMBOL(copy_from_iter_full);\n\nsize_t copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tchar *to = addr;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\titerate_and_advance(i, bytes, v,\n\t\t__copy_from_user_nocache((to += v.iov_len) - v.iov_len,\n\t\t\t\t\t v.iov_base, v.iov_len),\n\t\tmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(copy_from_iter_nocache);\n\nbool copy_from_iter_full_nocache(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tchar *to = addr;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn false;\n\t}\n\tif (unlikely(i->count < bytes))\n\t\treturn false;\n\titerate_all_kinds(i, bytes, v, ({\n\t\tif (__copy_from_user_nocache((to += v.iov_len) - v.iov_len,\n\t\t\t\t\t     v.iov_base, v.iov_len))\n\t\t\treturn false;\n\t\t0;}),\n\t\tmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\n\tiov_iter_advance(i, bytes);\n\treturn true;\n}\nEXPORT_SYMBOL(copy_from_iter_full_nocache);\n\nsize_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tif (i->type & (ITER_BVEC|ITER_KVEC)) {\n\t\tvoid *kaddr = kmap_atomic(page);\n\t\tsize_t wanted = copy_to_iter(kaddr + offset, bytes, i);\n\t\tkunmap_atomic(kaddr);\n\t\treturn wanted;\n\t} else if (likely(!(i->type & ITER_PIPE)))\n\t\treturn copy_page_to_iter_iovec(page, offset, bytes, i);\n\telse\n\t\treturn copy_page_to_iter_pipe(page, offset, bytes, i);\n}\nEXPORT_SYMBOL(copy_page_to_iter);\n\nsize_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\tif (i->type & (ITER_BVEC|ITER_KVEC)) {\n\t\tvoid *kaddr = kmap_atomic(page);\n\t\tsize_t wanted = copy_from_iter(kaddr + offset, bytes, i);\n\t\tkunmap_atomic(kaddr);\n\t\treturn wanted;\n\t} else\n\t\treturn copy_page_from_iter_iovec(page, offset, bytes, i);\n}\nEXPORT_SYMBOL(copy_page_from_iter);\n\nstatic size_t pipe_zero(size_t bytes, struct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tsize_t n, off;\n\tint idx;\n\n\tif (!sanity(i))\n\t\treturn 0;\n\n\tbytes = n = push_pipe(i, bytes, &idx, &off);\n\tif (unlikely(!n))\n\t\treturn 0;\n\n\tfor ( ; n; idx = next_idx(idx, pipe), off = 0) {\n\t\tsize_t chunk = min_t(size_t, n, PAGE_SIZE - off);\n\t\tmemzero_page(pipe->bufs[idx].page, off, chunk);\n\t\ti->idx = idx;\n\t\ti->iov_offset = off + chunk;\n\t\tn -= chunk;\n\t}\n\ti->count -= bytes;\n\treturn bytes;\n}\n\nsize_t iov_iter_zero(size_t bytes, struct iov_iter *i)\n{\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn pipe_zero(bytes, i);\n\titerate_and_advance(i, bytes, v,\n\t\t__clear_user(v.iov_base, v.iov_len),\n\t\tmemzero_page(v.bv_page, v.bv_offset, v.bv_len),\n\t\tmemset(v.iov_base, 0, v.iov_len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(iov_iter_zero);\n\nsize_t iov_iter_copy_from_user_atomic(struct page *page,\n\t\tstruct iov_iter *i, unsigned long offset, size_t bytes)\n{\n\tchar *kaddr = kmap_atomic(page), *p = kaddr + offset;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tkunmap_atomic(kaddr);\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\titerate_all_kinds(i, bytes, v,\n\t\t__copy_from_user_inatomic((p += v.iov_len) - v.iov_len,\n\t\t\t\t\t  v.iov_base, v.iov_len),\n\t\tmemcpy_from_page((p += v.bv_len) - v.bv_len, v.bv_page,\n\t\t\t\t v.bv_offset, v.bv_len),\n\t\tmemcpy((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\n\t)\n\tkunmap_atomic(kaddr);\n\treturn bytes;\n}\nEXPORT_SYMBOL(iov_iter_copy_from_user_atomic);\n\nstatic inline void pipe_truncate(struct iov_iter *i)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tif (pipe->nrbufs) {\n\t\tsize_t off = i->iov_offset;\n\t\tint idx = i->idx;\n\t\tint nrbufs = (idx - pipe->curbuf) & (pipe->buffers - 1);\n\t\tif (off) {\n\t\t\tpipe->bufs[idx].len = off - pipe->bufs[idx].offset;\n\t\t\tidx = next_idx(idx, pipe);\n\t\t\tnrbufs++;\n\t\t}\n\t\twhile (pipe->nrbufs > nrbufs) {\n\t\t\tpipe_buf_release(pipe, &pipe->bufs[idx]);\n\t\t\tidx = next_idx(idx, pipe);\n\t\t\tpipe->nrbufs--;\n\t\t}\n\t}\n}\n\nstatic void pipe_advance(struct iov_iter *i, size_t size)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tif (unlikely(i->count < size))\n\t\tsize = i->count;\n\tif (size) {\n\t\tstruct pipe_buffer *buf;\n\t\tsize_t off = i->iov_offset, left = size;\n\t\tint idx = i->idx;\n\t\tif (off) /* make it relative to the beginning of buffer */\n\t\t\tleft += off - pipe->bufs[idx].offset;\n\t\twhile (1) {\n\t\t\tbuf = &pipe->bufs[idx];\n\t\t\tif (left <= buf->len)\n\t\t\t\tbreak;\n\t\t\tleft -= buf->len;\n\t\t\tidx = next_idx(idx, pipe);\n\t\t}\n\t\ti->idx = idx;\n\t\ti->iov_offset = buf->offset + left;\n\t}\n\ti->count -= size;\n\t/* ... and discard everything past that point */\n\tpipe_truncate(i);\n}\n\nvoid iov_iter_advance(struct iov_iter *i, size_t size)\n{\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tpipe_advance(i, size);\n\t\treturn;\n\t}\n\titerate_and_advance(i, size, v, 0, 0, 0)\n}\nEXPORT_SYMBOL(iov_iter_advance);\n\n/*\n * Return the count of just the current iov_iter segment.\n */\nsize_t iov_iter_single_seg_count(const struct iov_iter *i)\n{\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn i->count;\t// it is a silly place, anyway\n\tif (i->nr_segs == 1)\n\t\treturn i->count;\n\telse if (i->type & ITER_BVEC)\n\t\treturn min(i->count, i->bvec->bv_len - i->iov_offset);\n\telse\n\t\treturn min(i->count, i->iov->iov_len - i->iov_offset);\n}\nEXPORT_SYMBOL(iov_iter_single_seg_count);\n\nvoid iov_iter_kvec(struct iov_iter *i, int direction,\n\t\t\tconst struct kvec *kvec, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\tBUG_ON(!(direction & ITER_KVEC));\n\ti->type = direction;\n\ti->kvec = kvec;\n\ti->nr_segs = nr_segs;\n\ti->iov_offset = 0;\n\ti->count = count;\n}\nEXPORT_SYMBOL(iov_iter_kvec);\n\nvoid iov_iter_bvec(struct iov_iter *i, int direction,\n\t\t\tconst struct bio_vec *bvec, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\tBUG_ON(!(direction & ITER_BVEC));\n\ti->type = direction;\n\ti->bvec = bvec;\n\ti->nr_segs = nr_segs;\n\ti->iov_offset = 0;\n\ti->count = count;\n}\nEXPORT_SYMBOL(iov_iter_bvec);\n\nvoid iov_iter_pipe(struct iov_iter *i, int direction,\n\t\t\tstruct pipe_inode_info *pipe,\n\t\t\tsize_t count)\n{\n\tBUG_ON(direction != ITER_PIPE);\n\tWARN_ON(pipe->nrbufs == pipe->buffers);\n\ti->type = direction;\n\ti->pipe = pipe;\n\ti->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\ti->iov_offset = 0;\n\ti->count = count;\n}\nEXPORT_SYMBOL(iov_iter_pipe);\n\nunsigned long iov_iter_alignment(const struct iov_iter *i)\n{\n\tunsigned long res = 0;\n\tsize_t size = i->count;\n\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tif (size && i->iov_offset && allocated(&i->pipe->bufs[i->idx]))\n\t\t\treturn size | i->iov_offset;\n\t\treturn size;\n\t}\n\titerate_all_kinds(i, size, v,\n\t\t(res |= (unsigned long)v.iov_base | v.iov_len, 0),\n\t\tres |= v.bv_offset | v.bv_len,\n\t\tres |= (unsigned long)v.iov_base | v.iov_len\n\t)\n\treturn res;\n}\nEXPORT_SYMBOL(iov_iter_alignment);\n\nunsigned long iov_iter_gap_alignment(const struct iov_iter *i)\n{\n\tunsigned long res = 0;\n\tsize_t size = i->count;\n\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn ~0U;\n\t}\n\n\titerate_all_kinds(i, size, v,\n\t\t(res |= (!res ? 0 : (unsigned long)v.iov_base) |\n\t\t\t(size != v.iov_len ? size : 0), 0),\n\t\t(res |= (!res ? 0 : (unsigned long)v.bv_offset) |\n\t\t\t(size != v.bv_len ? size : 0)),\n\t\t(res |= (!res ? 0 : (unsigned long)v.iov_base) |\n\t\t\t(size != v.iov_len ? size : 0))\n\t\t);\n\treturn res;\n}\nEXPORT_SYMBOL(iov_iter_gap_alignment);\n\nstatic inline size_t __pipe_get_pages(struct iov_iter *i,\n\t\t\t\tsize_t maxsize,\n\t\t\t\tstruct page **pages,\n\t\t\t\tint idx,\n\t\t\t\tsize_t *start)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tssize_t n = push_pipe(i, maxsize, &idx, start);\n\tif (!n)\n\t\treturn -EFAULT;\n\n\tmaxsize = n;\n\tn += *start;\n\twhile (n > 0) {\n\t\tget_page(*pages++ = pipe->bufs[idx].page);\n\t\tidx = next_idx(idx, pipe);\n\t\tn -= PAGE_SIZE;\n\t}\n\n\treturn maxsize;\n}\n\nstatic ssize_t pipe_get_pages(struct iov_iter *i,\n\t\t   struct page **pages, size_t maxsize, unsigned maxpages,\n\t\t   size_t *start)\n{\n\tunsigned npages;\n\tsize_t capacity;\n\tint idx;\n\n\tif (!maxsize)\n\t\treturn 0;\n\n\tif (!sanity(i))\n\t\treturn -EFAULT;\n\n\tdata_start(i, &idx, start);\n\t/* some of this one + all after this one */\n\tnpages = ((i->pipe->curbuf - idx - 1) & (i->pipe->buffers - 1)) + 1;\n\tcapacity = min(npages,maxpages) * PAGE_SIZE - *start;\n\n\treturn __pipe_get_pages(i, min(maxsize, capacity), pages, idx, start);\n}\n\nssize_t iov_iter_get_pages(struct iov_iter *i,\n\t\t   struct page **pages, size_t maxsize, unsigned maxpages,\n\t\t   size_t *start)\n{\n\tif (maxsize > i->count)\n\t\tmaxsize = i->count;\n\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn pipe_get_pages(i, pages, maxsize, maxpages, start);\n\titerate_all_kinds(i, maxsize, v, ({\n\t\tunsigned long addr = (unsigned long)v.iov_base;\n\t\tsize_t len = v.iov_len + (*start = addr & (PAGE_SIZE - 1));\n\t\tint n;\n\t\tint res;\n\n\t\tif (len > maxpages * PAGE_SIZE)\n\t\t\tlen = maxpages * PAGE_SIZE;\n\t\taddr &= ~(PAGE_SIZE - 1);\n\t\tn = DIV_ROUND_UP(len, PAGE_SIZE);\n\t\tres = get_user_pages_fast(addr, n, (i->type & WRITE) != WRITE, pages);\n\t\tif (unlikely(res < 0))\n\t\t\treturn res;\n\t\treturn (res == n ? len : res * PAGE_SIZE) - *start;\n\t0;}),({\n\t\t/* can't be more than PAGE_SIZE */\n\t\t*start = v.bv_offset;\n\t\tget_page(*pages = v.bv_page);\n\t\treturn v.bv_len;\n\t}),({\n\t\treturn -EFAULT;\n\t})\n\t)\n\treturn 0;\n}\nEXPORT_SYMBOL(iov_iter_get_pages);\n\nstatic struct page **get_pages_array(size_t n)\n{\n\tstruct page **p = kmalloc(n * sizeof(struct page *), GFP_KERNEL);\n\tif (!p)\n\t\tp = vmalloc(n * sizeof(struct page *));\n\treturn p;\n}\n\nstatic ssize_t pipe_get_pages_alloc(struct iov_iter *i,\n\t\t   struct page ***pages, size_t maxsize,\n\t\t   size_t *start)\n{\n\tstruct page **p;\n\tsize_t n;\n\tint idx;\n\tint npages;\n\n\tif (!maxsize)\n\t\treturn 0;\n\n\tif (!sanity(i))\n\t\treturn -EFAULT;\n\n\tdata_start(i, &idx, start);\n\t/* some of this one + all after this one */\n\tnpages = ((i->pipe->curbuf - idx - 1) & (i->pipe->buffers - 1)) + 1;\n\tn = npages * PAGE_SIZE - *start;\n\tif (maxsize > n)\n\t\tmaxsize = n;\n\telse\n\t\tnpages = DIV_ROUND_UP(maxsize + *start, PAGE_SIZE);\n\tp = get_pages_array(npages);\n\tif (!p)\n\t\treturn -ENOMEM;\n\tn = __pipe_get_pages(i, maxsize, p, idx, start);\n\tif (n > 0)\n\t\t*pages = p;\n\telse\n\t\tkvfree(p);\n\treturn n;\n}\n\nssize_t iov_iter_get_pages_alloc(struct iov_iter *i,\n\t\t   struct page ***pages, size_t maxsize,\n\t\t   size_t *start)\n{\n\tstruct page **p;\n\n\tif (maxsize > i->count)\n\t\tmaxsize = i->count;\n\n\tif (unlikely(i->type & ITER_PIPE))\n\t\treturn pipe_get_pages_alloc(i, pages, maxsize, start);\n\titerate_all_kinds(i, maxsize, v, ({\n\t\tunsigned long addr = (unsigned long)v.iov_base;\n\t\tsize_t len = v.iov_len + (*start = addr & (PAGE_SIZE - 1));\n\t\tint n;\n\t\tint res;\n\n\t\taddr &= ~(PAGE_SIZE - 1);\n\t\tn = DIV_ROUND_UP(len, PAGE_SIZE);\n\t\tp = get_pages_array(n);\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\t\tres = get_user_pages_fast(addr, n, (i->type & WRITE) != WRITE, p);\n\t\tif (unlikely(res < 0)) {\n\t\t\tkvfree(p);\n\t\t\treturn res;\n\t\t}\n\t\t*pages = p;\n\t\treturn (res == n ? len : res * PAGE_SIZE) - *start;\n\t0;}),({\n\t\t/* can't be more than PAGE_SIZE */\n\t\t*start = v.bv_offset;\n\t\t*pages = p = get_pages_array(1);\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\t\tget_page(*p = v.bv_page);\n\t\treturn v.bv_len;\n\t}),({\n\t\treturn -EFAULT;\n\t})\n\t)\n\treturn 0;\n}\nEXPORT_SYMBOL(iov_iter_get_pages_alloc);\n\nsize_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum,\n\t\t\t       struct iov_iter *i)\n{\n\tchar *to = addr;\n\t__wsum sum, next;\n\tsize_t off = 0;\n\tsum = *csum;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\titerate_and_advance(i, bytes, v, ({\n\t\tint err = 0;\n\t\tnext = csum_and_copy_from_user(v.iov_base,\n\t\t\t\t\t       (to += v.iov_len) - v.iov_len,\n\t\t\t\t\t       v.iov_len, 0, &err);\n\t\tif (!err) {\n\t\t\tsum = csum_block_add(sum, next, off);\n\t\t\toff += v.iov_len;\n\t\t}\n\t\terr ? v.iov_len : 0;\n\t}), ({\n\t\tchar *p = kmap_atomic(v.bv_page);\n\t\tnext = csum_partial_copy_nocheck(p + v.bv_offset,\n\t\t\t\t\t\t (to += v.bv_len) - v.bv_len,\n\t\t\t\t\t\t v.bv_len, 0);\n\t\tkunmap_atomic(p);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.bv_len;\n\t}),({\n\t\tnext = csum_partial_copy_nocheck(v.iov_base,\n\t\t\t\t\t\t (to += v.iov_len) - v.iov_len,\n\t\t\t\t\t\t v.iov_len, 0);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.iov_len;\n\t})\n\t)\n\t*csum = sum;\n\treturn bytes;\n}\nEXPORT_SYMBOL(csum_and_copy_from_iter);\n\nbool csum_and_copy_from_iter_full(void *addr, size_t bytes, __wsum *csum,\n\t\t\t       struct iov_iter *i)\n{\n\tchar *to = addr;\n\t__wsum sum, next;\n\tsize_t off = 0;\n\tsum = *csum;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn false;\n\t}\n\tif (unlikely(i->count < bytes))\n\t\treturn false;\n\titerate_all_kinds(i, bytes, v, ({\n\t\tint err = 0;\n\t\tnext = csum_and_copy_from_user(v.iov_base,\n\t\t\t\t\t       (to += v.iov_len) - v.iov_len,\n\t\t\t\t\t       v.iov_len, 0, &err);\n\t\tif (err)\n\t\t\treturn false;\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.iov_len;\n\t\t0;\n\t}), ({\n\t\tchar *p = kmap_atomic(v.bv_page);\n\t\tnext = csum_partial_copy_nocheck(p + v.bv_offset,\n\t\t\t\t\t\t (to += v.bv_len) - v.bv_len,\n\t\t\t\t\t\t v.bv_len, 0);\n\t\tkunmap_atomic(p);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.bv_len;\n\t}),({\n\t\tnext = csum_partial_copy_nocheck(v.iov_base,\n\t\t\t\t\t\t (to += v.iov_len) - v.iov_len,\n\t\t\t\t\t\t v.iov_len, 0);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.iov_len;\n\t})\n\t)\n\t*csum = sum;\n\tiov_iter_advance(i, bytes);\n\treturn true;\n}\nEXPORT_SYMBOL(csum_and_copy_from_iter_full);\n\nsize_t csum_and_copy_to_iter(const void *addr, size_t bytes, __wsum *csum,\n\t\t\t     struct iov_iter *i)\n{\n\tconst char *from = addr;\n\t__wsum sum, next;\n\tsize_t off = 0;\n\tsum = *csum;\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\t/* for now */\n\t\treturn 0;\n\t}\n\titerate_and_advance(i, bytes, v, ({\n\t\tint err = 0;\n\t\tnext = csum_and_copy_to_user((from += v.iov_len) - v.iov_len,\n\t\t\t\t\t     v.iov_base,\n\t\t\t\t\t     v.iov_len, 0, &err);\n\t\tif (!err) {\n\t\t\tsum = csum_block_add(sum, next, off);\n\t\t\toff += v.iov_len;\n\t\t}\n\t\terr ? v.iov_len : 0;\n\t}), ({\n\t\tchar *p = kmap_atomic(v.bv_page);\n\t\tnext = csum_partial_copy_nocheck((from += v.bv_len) - v.bv_len,\n\t\t\t\t\t\t p + v.bv_offset,\n\t\t\t\t\t\t v.bv_len, 0);\n\t\tkunmap_atomic(p);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.bv_len;\n\t}),({\n\t\tnext = csum_partial_copy_nocheck((from += v.iov_len) - v.iov_len,\n\t\t\t\t\t\t v.iov_base,\n\t\t\t\t\t\t v.iov_len, 0);\n\t\tsum = csum_block_add(sum, next, off);\n\t\toff += v.iov_len;\n\t})\n\t)\n\t*csum = sum;\n\treturn bytes;\n}\nEXPORT_SYMBOL(csum_and_copy_to_iter);\n\nint iov_iter_npages(const struct iov_iter *i, int maxpages)\n{\n\tsize_t size = i->count;\n\tint npages = 0;\n\n\tif (!size)\n\t\treturn 0;\n\n\tif (unlikely(i->type & ITER_PIPE)) {\n\t\tstruct pipe_inode_info *pipe = i->pipe;\n\t\tsize_t off;\n\t\tint idx;\n\n\t\tif (!sanity(i))\n\t\t\treturn 0;\n\n\t\tdata_start(i, &idx, &off);\n\t\t/* some of this one + all after this one */\n\t\tnpages = ((pipe->curbuf - idx - 1) & (pipe->buffers - 1)) + 1;\n\t\tif (npages >= maxpages)\n\t\t\treturn maxpages;\n\t} else iterate_all_kinds(i, size, v, ({\n\t\tunsigned long p = (unsigned long)v.iov_base;\n\t\tnpages += DIV_ROUND_UP(p + v.iov_len, PAGE_SIZE)\n\t\t\t- p / PAGE_SIZE;\n\t\tif (npages >= maxpages)\n\t\t\treturn maxpages;\n\t0;}),({\n\t\tnpages++;\n\t\tif (npages >= maxpages)\n\t\t\treturn maxpages;\n\t}),({\n\t\tunsigned long p = (unsigned long)v.iov_base;\n\t\tnpages += DIV_ROUND_UP(p + v.iov_len, PAGE_SIZE)\n\t\t\t- p / PAGE_SIZE;\n\t\tif (npages >= maxpages)\n\t\t\treturn maxpages;\n\t})\n\t)\n\treturn npages;\n}\nEXPORT_SYMBOL(iov_iter_npages);\n\nconst void *dup_iter(struct iov_iter *new, struct iov_iter *old, gfp_t flags)\n{\n\t*new = *old;\n\tif (unlikely(new->type & ITER_PIPE)) {\n\t\tWARN_ON(1);\n\t\treturn NULL;\n\t}\n\tif (new->type & ITER_BVEC)\n\t\treturn new->bvec = kmemdup(new->bvec,\n\t\t\t\t    new->nr_segs * sizeof(struct bio_vec),\n\t\t\t\t    flags);\n\telse\n\t\t/* iovec and kvec have identical layout */\n\t\treturn new->iov = kmemdup(new->iov,\n\t\t\t\t   new->nr_segs * sizeof(struct iovec),\n\t\t\t\t   flags);\n}\nEXPORT_SYMBOL(dup_iter);\n\n/**\n * import_iovec() - Copy an array of &struct iovec from userspace\n *     into the kernel, check that it is valid, and initialize a new\n *     &struct iov_iter iterator to access it.\n *\n * @type: One of %READ or %WRITE.\n * @uvector: Pointer to the userspace array.\n * @nr_segs: Number of elements in userspace array.\n * @fast_segs: Number of elements in @iov.\n * @iov: (input and output parameter) Pointer to pointer to (usually small\n *     on-stack) kernel array.\n * @i: Pointer to iterator that will be initialized on success.\n *\n * If the array pointed to by *@iov is large enough to hold all @nr_segs,\n * then this function places %NULL in *@iov on return. Otherwise, a new\n * array will be allocated and the result placed in *@iov. This means that\n * the caller may call kfree() on *@iov regardless of whether the small\n * on-stack array was used or not (and regardless of whether this function\n * returns an error or not).\n *\n * Return: 0 on success or negative error code on error.\n */\nint import_iovec(int type, const struct iovec __user * uvector,\n\t\t unsigned nr_segs, unsigned fast_segs,\n\t\t struct iovec **iov, struct iov_iter *i)\n{\n\tssize_t n;\n\tstruct iovec *p;\n\tn = rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,\n\t\t\t\t  *iov, &p);\n\tif (n < 0) {\n\t\tif (p != *iov)\n\t\t\tkfree(p);\n\t\t*iov = NULL;\n\t\treturn n;\n\t}\n\tiov_iter_init(i, type, p, nr_segs, n);\n\t*iov = p == *iov ? NULL : p;\n\treturn 0;\n}\nEXPORT_SYMBOL(import_iovec);\n\n#ifdef CONFIG_COMPAT\n#include <linux/compat.h>\n\nint compat_import_iovec(int type, const struct compat_iovec __user * uvector,\n\t\t unsigned nr_segs, unsigned fast_segs,\n\t\t struct iovec **iov, struct iov_iter *i)\n{\n\tssize_t n;\n\tstruct iovec *p;\n\tn = compat_rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,\n\t\t\t\t  *iov, &p);\n\tif (n < 0) {\n\t\tif (p != *iov)\n\t\t\tkfree(p);\n\t\t*iov = NULL;\n\t\treturn n;\n\t}\n\tiov_iter_init(i, type, p, nr_segs, n);\n\t*iov = p == *iov ? NULL : p;\n\treturn 0;\n}\n#endif\n\nint import_single_range(int rw, void __user *buf, size_t len,\n\t\t struct iovec *iov, struct iov_iter *i)\n{\n\tif (len > MAX_RW_COUNT)\n\t\tlen = MAX_RW_COUNT;\n\tif (unlikely(!access_ok(!rw, buf, len)))\n\t\treturn -EFAULT;\n\n\tiov->iov_base = buf;\n\tiov->iov_len = len;\n\tiov_iter_init(i, rw, iov, 1, len);\n\treturn 0;\n}\nEXPORT_SYMBOL(import_single_range);\n"], "filenames": ["lib/iov_iter.c"], "buggy_code_start_loc": [733], "buggy_code_end_loc": [828], "fixing_code_start_loc": [733], "fixing_code_end_loc": [837], "type": "CWE-200", "message": "Off-by-one error in the pipe_advance function in lib/iov_iter.c in the Linux kernel before 4.9.5 allows local users to obtain sensitive information from uninitialized heap-memory locations in opportunistic circumstances by reading from a pipe after an incorrect buffer-release decision.", "other": {"cve": {"id": "CVE-2017-5550", "sourceIdentifier": "cve@mitre.org", "published": "2017-02-06T06:59:00.667", "lastModified": "2017-02-09T19:15:53.033", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Off-by-one error in the pipe_advance function in lib/iov_iter.c in the Linux kernel before 4.9.5 allows local users to obtain sensitive information from uninitialized heap-memory locations in opportunistic circumstances by reading from a pipe after an incorrect buffer-release decision."}, {"lang": "es", "value": "Error por un paso en la funci\u00f3n pipe_advance en lib/iov_iter.c en el kernel de Linux en versiones anteriores a 4.9.5 permite a usuarios locales obtener informaci\u00f3n sensible de posiciones de memoria din\u00e1mica no inicializadas en circunstancias oportunistas leyendo desde una tuber\u00eda despu\u00e9s una decisi\u00f3n de liberaci\u00f3n de b\u00fafer incorrecta."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.9.4", "matchCriteriaId": "7C19DB2D-DE85-4140-817A-D010708EB355"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b9dc6f65bc5e232d1c05fe34b5daadc7e8bbf1fb", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.9.5", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2017/01/21/3", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/95716", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1416116", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/b9dc6f65bc5e232d1c05fe34b5daadc7e8bbf1fb", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b9dc6f65bc5e232d1c05fe34b5daadc7e8bbf1fb"}}