{"buggy_code": ["/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/compiler/jit/xla_platform_info.h\"\n\n#include \"tensorflow/compiler/xla/client/client_library.h\"\n\nnamespace tensorflow {\n\nxla::StatusOr<absl::optional<std::set<int>>> ParseVisibleDeviceList(\n    absl::string_view visible_device_list) {\n  std::set<int> gpu_ids;\n  if (visible_device_list.empty()) {\n    return {{absl::nullopt}};\n  }\n  const std::vector<string> visible_devices =\n      absl::StrSplit(visible_device_list, ',');\n  for (const string& platform_device_id_str : visible_devices) {\n    int32_t platform_device_id;\n    if (!absl::SimpleAtoi(platform_device_id_str, &platform_device_id)) {\n      return errors::InvalidArgument(\n          \"Could not parse entry in 'visible_device_list': '\",\n          platform_device_id_str,\n          \"'. visible_device_list = \", visible_device_list);\n    }\n    gpu_ids.insert(platform_device_id);\n  }\n  return {{gpu_ids}};\n}\n\nStatus BuildXlaCompilationCache(DeviceBase* device, FunctionLibraryRuntime* flr,\n                                const XlaPlatformInfo& platform_info,\n                                XlaCompilationCache** cache) {\n  if (platform_info.xla_device_metadata()) {\n    *cache = new XlaCompilationCache(\n        platform_info.xla_device_metadata()->client(),\n        platform_info.xla_device_metadata()->jit_device_type());\n    return Status::OK();\n  }\n\n  auto platform =\n      se::MultiPlatformManager::PlatformWithId(platform_info.platform_id());\n  if (!platform.ok()) {\n    return platform.status();\n  }\n\n  StatusOr<xla::Compiler*> compiler_for_platform =\n      xla::Compiler::GetForPlatform(platform.ValueOrDie());\n  if (!compiler_for_platform.ok()) {\n    // In some rare cases (usually in unit tests with very small clusters) we\n    // may end up transforming an XLA cluster with at least one GPU operation\n    // (which would normally force the cluster to be compiled using XLA:GPU)\n    // into an XLA cluster with no GPU operations (i.e. containing only CPU\n    // operations).  Such a cluster can fail compilation (in way that\n    // MarkForCompilation could not have detected) if the CPU JIT is not linked\n    // in.\n    //\n    // So bail out of _XlaCompile in this case, and let the executor handle the\n    // situation for us.\n    const Status& status = compiler_for_platform.status();\n    if (status.code() == error::NOT_FOUND) {\n      return errors::Unimplemented(\"Could not find compiler for platform \",\n                                   platform.ValueOrDie()->Name(), \": \",\n                                   status.ToString());\n    }\n  }\n\n  xla::LocalClientOptions client_options;\n  client_options.set_platform(platform.ValueOrDie());\n  client_options.set_intra_op_parallelism_threads(\n      device->tensorflow_cpu_worker_threads()->num_threads);\n\n  string allowed_gpus =\n      flr->config_proto()->gpu_options().visible_device_list();\n  TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,\n                      ParseVisibleDeviceList(allowed_gpus));\n  client_options.set_allowed_devices(gpu_ids);\n\n  auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);\n  if (!client.ok()) {\n    return client.status();\n  }\n  const XlaOpRegistry::DeviceRegistration* registration;\n  if (!XlaOpRegistry::GetCompilationDevice(platform_info.device_type().type(),\n                                           &registration)) {\n    return errors::InvalidArgument(\"No JIT device registered for \",\n                                   platform_info.device_type().type());\n  }\n  *cache = new XlaCompilationCache(\n      client.ValueOrDie(), DeviceType(registration->compilation_device_name));\n  return Status::OK();\n}\n\nXlaPlatformInfo XlaPlatformInfoFromDevice(DeviceBase* device_base) {\n  auto device = static_cast<Device*>(device_base);\n  se::Platform::Id platform_id = nullptr;\n  const XlaDevice::Metadata* xla_device_metadata = nullptr;\n  std::shared_ptr<se::DeviceMemoryAllocator> custom_allocator;\n\n  if (device->device_type() == DEVICE_CPU) {\n    platform_id = se::host::kHostPlatformId;\n  } else if (device->device_type() == DEVICE_GPU) {\n    platform_id = device->tensorflow_gpu_device_info()\n                      ->stream->parent()\n                      ->platform()\n                      ->id();\n  } else if (XlaDevice::GetMetadataFromDevice(device, &xla_device_metadata)\n                 .ok()) {\n    // If we are on an XlaDevice, use the underlying XLA platform's allocator\n    // directly. We could use the StreamExecutor's allocator which may\n    // theoretically be more correct, but XLA returns a nice OOM message in a\n    // Status and StreamExecutor does not.\n    //\n    // Importantly we can't use ctx->device()->GetAllocator() as the allocator\n    // (which xla_allocator above uses) as on an XlaDevice, this is a dummy\n    // allocator that returns XlaTensor objects. The XlaCompiler needs a real\n    // allocator to allocate real buffers.\n    platform_id = xla_device_metadata->platform()->id();\n    custom_allocator =\n        xla_device_metadata->client()->backend().shared_memory_allocator();\n  }\n\n  return XlaPlatformInfo(DeviceType(device->device_type()), platform_id,\n                         xla_device_metadata, custom_allocator);\n}\n\nstd::shared_ptr<se::DeviceMemoryAllocator> GetAllocator(\n    DeviceBase* device, se::Stream* stream,\n    const XlaPlatformInfo& platform_info) {\n  if (platform_info.custom_allocator()) {\n    return platform_info.custom_allocator();\n  }\n  auto* alloc = device->GetAllocator({});\n  if (!stream) {\n    // Stream is not set for the host platform.\n    se::Platform* platform =\n        se::MultiPlatformManager::PlatformWithId(platform_info.platform_id())\n            .ValueOrDie();\n    return std::make_shared<se::TfAllocatorAdapter>(alloc, platform);\n  }\n  return std::make_shared<se::TfAllocatorAdapter>(alloc, stream);\n}\n\nXlaCompiler::Options GenerateCompilerOptions(\n    const XlaCompilationCache& cache,\n    const FunctionLibraryRuntime& function_library, DeviceBase* device,\n    se::Stream* stream, const XlaPlatformInfo& platform_info,\n    bool has_ref_vars) {\n  XlaCompiler::Options options;\n  options.client = static_cast<xla::LocalClient*>(cache.client());\n  if (stream != nullptr) {\n    options.device_ordinal = stream->parent()->device_ordinal();\n  }\n  options.device_type = cache.device_type();\n  options.flib_def = function_library.GetFunctionLibraryDefinition();\n  options.graph_def_version = function_library.graph_def_version();\n  options.allow_cpu_custom_calls =\n      (platform_info.platform_id() == se::host::kHostPlatformId);\n  options.device_allocator = GetAllocator(device, stream, platform_info);\n  if (platform_info.xla_device_metadata()) {\n    options.shape_determination_fns =\n        platform_info.xla_device_metadata()->default_shape_determination_fns();\n  }\n  // If reference variables are not present in the graph, we can safely alias\n  // passthrough parameters without performing a copy.\n  options.alias_passthrough_params =\n      !has_ref_vars && !platform_info.is_on_xla_device();\n  return options;\n}\n\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/compiler/jit/xla_platform_info.h\"\n\n#include \"tensorflow/compiler/xla/client/client_library.h\"\n\nnamespace tensorflow {\n\nxla::StatusOr<absl::optional<std::set<int>>> ParseVisibleDeviceList(\n    absl::string_view visible_device_list) {\n  std::set<int> gpu_ids;\n  if (visible_device_list.empty()) {\n    return {{absl::nullopt}};\n  }\n  const std::vector<string> visible_devices =\n      absl::StrSplit(visible_device_list, ',');\n  for (const string& platform_device_id_str : visible_devices) {\n    int32_t platform_device_id;\n    if (!absl::SimpleAtoi(platform_device_id_str, &platform_device_id)) {\n      return errors::InvalidArgument(\n          \"Could not parse entry in 'visible_device_list': '\",\n          platform_device_id_str,\n          \"'. visible_device_list = \", visible_device_list);\n    }\n    gpu_ids.insert(platform_device_id);\n  }\n  return {{gpu_ids}};\n}\n\nStatus BuildXlaCompilationCache(DeviceBase* device, FunctionLibraryRuntime* flr,\n                                const XlaPlatformInfo& platform_info,\n                                XlaCompilationCache** cache) {\n  if (platform_info.xla_device_metadata()) {\n    *cache = new XlaCompilationCache(\n        platform_info.xla_device_metadata()->client(),\n        platform_info.xla_device_metadata()->jit_device_type());\n    return Status::OK();\n  }\n\n  auto platform =\n      se::MultiPlatformManager::PlatformWithId(platform_info.platform_id());\n  if (!platform.ok()) {\n    return platform.status();\n  }\n\n  StatusOr<xla::Compiler*> compiler_for_platform =\n      xla::Compiler::GetForPlatform(platform.ValueOrDie());\n  if (!compiler_for_platform.ok()) {\n    // In some rare cases (usually in unit tests with very small clusters) we\n    // may end up transforming an XLA cluster with at least one GPU operation\n    // (which would normally force the cluster to be compiled using XLA:GPU)\n    // into an XLA cluster with no GPU operations (i.e. containing only CPU\n    // operations).  Such a cluster can fail compilation (in way that\n    // MarkForCompilation could not have detected) if the CPU JIT is not linked\n    // in.\n    //\n    // So bail out of _XlaCompile in this case, and let the executor handle the\n    // situation for us.\n    const Status& status = compiler_for_platform.status();\n    if (status.code() == error::NOT_FOUND) {\n      return errors::Unimplemented(\"Could not find compiler for platform \",\n                                   platform.ValueOrDie()->Name(), \": \",\n                                   status.ToString());\n    }\n  }\n\n  xla::LocalClientOptions client_options;\n  client_options.set_platform(platform.ValueOrDie());\n  client_options.set_intra_op_parallelism_threads(\n      device->tensorflow_cpu_worker_threads()->num_threads);\n\n  if (flr->config_proto()) {\n    string allowed_gpus =\n        flr->config_proto()->gpu_options().visible_device_list();\n    TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,\n                        ParseVisibleDeviceList(allowed_gpus));\n    client_options.set_allowed_devices(gpu_ids);\n  }\n\n  auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);\n  if (!client.ok()) {\n    return client.status();\n  }\n  const XlaOpRegistry::DeviceRegistration* registration;\n  if (!XlaOpRegistry::GetCompilationDevice(platform_info.device_type().type(),\n                                           &registration)) {\n    return errors::InvalidArgument(\"No JIT device registered for \",\n                                   platform_info.device_type().type());\n  }\n  *cache = new XlaCompilationCache(\n      client.ValueOrDie(), DeviceType(registration->compilation_device_name));\n  return Status::OK();\n}\n\nXlaPlatformInfo XlaPlatformInfoFromDevice(DeviceBase* device_base) {\n  auto device = static_cast<Device*>(device_base);\n  se::Platform::Id platform_id = nullptr;\n  const XlaDevice::Metadata* xla_device_metadata = nullptr;\n  std::shared_ptr<se::DeviceMemoryAllocator> custom_allocator;\n\n  if (device->device_type() == DEVICE_CPU) {\n    platform_id = se::host::kHostPlatformId;\n  } else if (device->device_type() == DEVICE_GPU) {\n    platform_id = device->tensorflow_gpu_device_info()\n                      ->stream->parent()\n                      ->platform()\n                      ->id();\n  } else if (XlaDevice::GetMetadataFromDevice(device, &xla_device_metadata)\n                 .ok()) {\n    // If we are on an XlaDevice, use the underlying XLA platform's allocator\n    // directly. We could use the StreamExecutor's allocator which may\n    // theoretically be more correct, but XLA returns a nice OOM message in a\n    // Status and StreamExecutor does not.\n    //\n    // Importantly we can't use ctx->device()->GetAllocator() as the allocator\n    // (which xla_allocator above uses) as on an XlaDevice, this is a dummy\n    // allocator that returns XlaTensor objects. The XlaCompiler needs a real\n    // allocator to allocate real buffers.\n    platform_id = xla_device_metadata->platform()->id();\n    custom_allocator =\n        xla_device_metadata->client()->backend().shared_memory_allocator();\n  }\n\n  return XlaPlatformInfo(DeviceType(device->device_type()), platform_id,\n                         xla_device_metadata, custom_allocator);\n}\n\nstd::shared_ptr<se::DeviceMemoryAllocator> GetAllocator(\n    DeviceBase* device, se::Stream* stream,\n    const XlaPlatformInfo& platform_info) {\n  if (platform_info.custom_allocator()) {\n    return platform_info.custom_allocator();\n  }\n  auto* alloc = device->GetAllocator({});\n  if (!stream) {\n    // Stream is not set for the host platform.\n    se::Platform* platform =\n        se::MultiPlatformManager::PlatformWithId(platform_info.platform_id())\n            .ValueOrDie();\n    return std::make_shared<se::TfAllocatorAdapter>(alloc, platform);\n  }\n  return std::make_shared<se::TfAllocatorAdapter>(alloc, stream);\n}\n\nXlaCompiler::Options GenerateCompilerOptions(\n    const XlaCompilationCache& cache,\n    const FunctionLibraryRuntime& function_library, DeviceBase* device,\n    se::Stream* stream, const XlaPlatformInfo& platform_info,\n    bool has_ref_vars) {\n  XlaCompiler::Options options;\n  options.client = static_cast<xla::LocalClient*>(cache.client());\n  if (stream != nullptr) {\n    options.device_ordinal = stream->parent()->device_ordinal();\n  }\n  options.device_type = cache.device_type();\n  options.flib_def = function_library.GetFunctionLibraryDefinition();\n  options.graph_def_version = function_library.graph_def_version();\n  options.allow_cpu_custom_calls =\n      (platform_info.platform_id() == se::host::kHostPlatformId);\n  options.device_allocator = GetAllocator(device, stream, platform_info);\n  if (platform_info.xla_device_metadata()) {\n    options.shape_determination_fns =\n        platform_info.xla_device_metadata()->default_shape_determination_fns();\n  }\n  // If reference variables are not present in the graph, we can safely alias\n  // passthrough parameters without performing a copy.\n  options.alias_passthrough_params =\n      !has_ref_vars && !platform_info.is_on_xla_device();\n  return options;\n}\n\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/compiler/jit/xla_platform_info.cc"], "buggy_code_start_loc": [85], "buggy_code_end_loc": [90], "fixing_code_start_loc": [85], "fixing_code_end_loc": [92], "type": "CWE-476", "message": "Tensorflow is an Open Source Machine Learning Framework. When building an XLA compilation cache, if default settings are used, TensorFlow triggers a null pointer dereference. In the default scenario, all devices are allowed, so `flr->config_proto` is `nullptr`. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-23595", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-04T23:15:15.460", "lastModified": "2022-02-10T02:10:55.000", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Tensorflow is an Open Source Machine Learning Framework. When building an XLA compilation cache, if default settings are used, TensorFlow triggers a null pointer dereference. In the default scenario, all devices are allowed, so `flr->config_proto` is `nullptr`. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range."}, {"lang": "es", "value": "Tensorflow es un Marco de Aprendizaje Autom\u00e1tico de C\u00f3digo Abierto. Cuando se construye una cach\u00e9 de compilaci\u00f3n de XLA, si es usada la configuraci\u00f3n predeterminada, TensorFlow desencadena una desreferencia de puntero null. En el escenario por defecto, son permitidos todos los dispositivos, por lo que \"flr-)config_proto\" es \"nullptr\". La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.8.0. Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.7.1, TensorFlow versi\u00f3n 2.6.3, y TensorFlow versi\u00f3n 2.5.3, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.6, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.5.2", "matchCriteriaId": "688150BF-477C-48FC-9AEF-A79AC57A6DDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndIncluding": "2.6.2", "matchCriteriaId": "C9E69B60-8C97-47E2-9027-9598B8392E5D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:*:*:*:*:*:*:*", "matchCriteriaId": "2EDFAAB8-799C-4259-9102-944D4760DA2C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/274df9b02330b790aa8de1cee164b70f72b9b244/tensorflow/compiler/jit/xla_platform_info.cc#L43-L104", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/e21af685e1828f7ca65038307df5cc06de4479e8", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-fpcp-9h7m-ffpx", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/e21af685e1828f7ca65038307df5cc06de4479e8"}}