{"buggy_code": ["# Zulip's main markdown implementation.  See docs/subsystems/markdown.md for\n# detailed documentation on our markdown syntax.\nfrom typing import (Any, Callable, Dict, Iterable, List, NamedTuple,\n                    Optional, Set, Tuple, TypeVar, Union, cast)\nfrom mypy_extensions import TypedDict\nfrom typing.re import Match, Pattern\n\nimport markdown\nimport logging\nimport traceback\nimport urllib\nimport re\nimport os\nimport html\nimport time\nimport functools\nimport ujson\nimport xml.etree.cElementTree as etree\nfrom xml.etree.cElementTree import Element\n\nfrom collections import deque, defaultdict\n\nimport requests\n\nfrom django.conf import settings\nfrom django.db.models import Q\n\nfrom markdown.extensions import codehilite, nl2br, tables\nfrom zerver.lib.bugdown import fenced_code\nfrom zerver.lib.bugdown.fenced_code import FENCE_RE\nfrom zerver.lib.camo import get_camo_url\nfrom zerver.lib.emoji import translate_emoticons, emoticon_regex\nfrom zerver.lib.mention import possible_mentions, \\\n    possible_user_group_mentions, extract_user_group\nfrom zerver.lib.url_encoding import encode_stream\nfrom zerver.lib.thumbnail import user_uploads_or_external\nfrom zerver.lib.timeout import timeout, TimeoutExpired\nfrom zerver.lib.cache import cache_with_key, NotFoundInCache\nfrom zerver.lib.url_preview import preview as link_preview\nfrom zerver.models import (\n    all_realm_filters,\n    get_active_streams,\n    MAX_MESSAGE_LENGTH,\n    Message,\n    Realm,\n    realm_filters_for_realm,\n    UserProfile,\n    UserGroup,\n    UserGroupMembership,\n)\nimport zerver.lib.mention as mention\nfrom zerver.lib.tex import render_tex\nfrom zerver.lib.exceptions import BugdownRenderingException\n\nReturnT = TypeVar('ReturnT')\n\ndef one_time(method: Callable[[], ReturnT]) -> Callable[[], ReturnT]:\n    '''\n        Use this decorator with extreme caution.\n        The function you wrap should have no dependency\n        on any arguments (no args, no kwargs) nor should\n        it depend on any global state.\n    '''\n    val = None\n\n    def cache_wrapper() -> ReturnT:\n        nonlocal val\n        if val is None:\n            val = method()\n        return val\n    return cache_wrapper\n\nFullNameInfo = TypedDict('FullNameInfo', {\n    'id': int,\n    'email': str,\n    'full_name': str,\n})\n\nDbData = Dict[str, Any]\n\n# Format version of the bugdown rendering; stored along with rendered\n# messages so that we can efficiently determine what needs to be re-rendered\nversion = 1\n\n_T = TypeVar('_T')\nElementStringNone = Union[Element, Optional[str]]\n\nAVATAR_REGEX = r'!avatar\\((?P<email>[^)]*)\\)'\nGRAVATAR_REGEX = r'!gravatar\\((?P<email>[^)]*)\\)'\nEMOJI_REGEX = r'(?P<syntax>:[\\w\\-\\+]+:)'\n\ndef verbose_compile(pattern: str) -> Any:\n    return re.compile(\n        \"^(.*?)%s(.*?)$\" % pattern,\n        re.DOTALL | re.UNICODE | re.VERBOSE\n    )\n\ndef normal_compile(pattern: str) -> Any:\n    return re.compile(\n        r\"^(.*?)%s(.*)$\" % pattern,\n        re.DOTALL | re.UNICODE\n    )\n\nSTREAM_LINK_REGEX = r\"\"\"\n                     (?<![^\\s'\"\\(,:<])            # Start after whitespace or specified chars\n                     \\#\\*\\*                       # and after hash sign followed by double asterisks\n                         (?P<stream_name>[^\\*]+)  # stream name can contain anything\n                     \\*\\*                         # ends by double asterisks\n                    \"\"\"\n\n@one_time\ndef get_compiled_stream_link_regex() -> Pattern:\n    return verbose_compile(STREAM_LINK_REGEX)\n\nLINK_REGEX = None  # type: Pattern\n\ndef get_web_link_regex() -> str:\n    # We create this one time, but not at startup.  So the\n    # first message rendered in any process will have some\n    # extra costs.  It's roughly 75ms to run this code, so\n    # caching the value in LINK_REGEX is super important here.\n    global LINK_REGEX\n    if LINK_REGEX is not None:\n        return LINK_REGEX\n\n    tlds = '|'.join(list_of_tlds())\n\n    # A link starts at a word boundary, and ends at space, punctuation, or end-of-input.\n    #\n    # We detect a url either by the `https?://` or by building around the TLD.\n\n    # In lieu of having a recursive regex (which python doesn't support) to match\n    # arbitrary numbers of nested matching parenthesis, we manually build a regexp that\n    # can match up to six\n    # The inner_paren_contents chunk matches the innermore non-parenthesis-holding text,\n    # and the paren_group matches text with, optionally, a matching set of parens\n    inner_paren_contents = r\"[^\\s()\\\"]*\"\n    paren_group = r\"\"\"\n                    [^\\s()\\\"]*?            # Containing characters that won't end the URL\n                    (?: \\( %s \\)           # and more characters in matched parens\n                        [^\\s()\\\"]*?        # followed by more characters\n                    )*                     # zero-or-more sets of paired parens\n                   \"\"\"\n    nested_paren_chunk = paren_group\n    for i in range(6):\n        nested_paren_chunk = nested_paren_chunk % (paren_group,)\n    nested_paren_chunk = nested_paren_chunk % (inner_paren_contents,)\n\n    file_links = r\"| (?:file://(/[^/ ]*)+/?)\" if settings.ENABLE_FILE_LINKS else r\"\"\n    REGEX = r\"\"\"\n        (?<![^\\s'\"\\(,:<])    # Start after whitespace or specified chars\n                             # (Double-negative lookbehind to allow start-of-string)\n        (?P<url>             # Main group\n            (?:(?:           # Domain part\n                https?://[\\w.:@-]+?   # If it has a protocol, anything goes.\n               |(?:                   # Or, if not, be more strict to avoid false-positives\n                    (?:[\\w-]+\\.)+     # One or more domain components, separated by dots\n                    (?:%s)            # TLDs (filled in via format from tlds-alpha-by-domain.txt)\n                )\n            )\n            (?:/             # A path, beginning with /\n                %s           # zero-to-6 sets of paired parens\n            )?)              # Path is optional\n            | (?:[\\w.-]+\\@[\\w.-]+\\.[\\w]+) # Email is separate, since it can't have a path\n            %s               # File path start with file:///, enable by setting ENABLE_FILE_LINKS=True\n            | (?:bitcoin:[13][a-km-zA-HJ-NP-Z1-9]{25,34})  # Bitcoin address pattern, see https://mokagio.github.io/tech-journal/2014/11/21/regex-bitcoin.html\n        )\n        (?=                            # URL must be followed by (not included in group)\n            [!:;\\?\\),\\.\\'\\\"\\>]*         # Optional punctuation characters\n            (?:\\Z|\\s)                  # followed by whitespace or end of string\n        )\n        \"\"\" % (tlds, nested_paren_chunk, file_links)\n    LINK_REGEX = verbose_compile(REGEX)\n    return LINK_REGEX\n\ndef clear_state_for_testing() -> None:\n    # The link regex never changes in production, but our tests\n    # try out both sides of ENABLE_FILE_LINKS, so we need\n    # a way to clear it.\n    global LINK_REGEX\n    LINK_REGEX = None\n\nbugdown_logger = logging.getLogger()\n\ndef rewrite_local_links_to_relative(db_data: Optional[DbData], link: str) -> str:\n    \"\"\" If the link points to a local destination we can just switch to that\n    instead of opening a new tab. \"\"\"\n\n    if db_data:\n        realm_uri_prefix = db_data['realm_uri'] + \"/\"\n        if link.startswith(realm_uri_prefix):\n            # +1 to skip the `/` before the hash link.\n            return link[len(realm_uri_prefix):]\n\n    return link\n\ndef url_embed_preview_enabled(message: Optional[Message]=None,\n                              realm: Optional[Realm]=None,\n                              no_previews: Optional[bool]=False) -> bool:\n    if not settings.INLINE_URL_EMBED_PREVIEW:\n        return False\n\n    if no_previews:\n        return False\n\n    if realm is None:\n        if message is not None:\n            realm = message.get_realm()\n\n    if realm is None:\n        # realm can be None for odd use cases\n        # like generating documentation or running\n        # test code\n        return True\n\n    return realm.inline_url_embed_preview\n\ndef image_preview_enabled(message: Optional[Message]=None,\n                          realm: Optional[Realm]=None,\n                          no_previews: Optional[bool]=False) -> bool:\n    if not settings.INLINE_IMAGE_PREVIEW:\n        return False\n\n    if no_previews:\n        return False\n\n    if realm is None:\n        if message is not None:\n            realm = message.get_realm()\n\n    if realm is None:\n        # realm can be None for odd use cases\n        # like generating documentation or running\n        # test code\n        return True\n\n    return realm.inline_image_preview\n\ndef list_of_tlds() -> List[str]:\n    # HACK we manually blacklist a few domains\n    blacklist = ['PY\\n', \"MD\\n\"]\n\n    # tlds-alpha-by-domain.txt comes from http://data.iana.org/TLD/tlds-alpha-by-domain.txt\n    tlds_file = os.path.join(os.path.dirname(__file__), 'tlds-alpha-by-domain.txt')\n    tlds = [tld.lower().strip() for tld in open(tlds_file, 'r')\n            if tld not in blacklist and not tld[0].startswith('#')]\n    tlds.sort(key=len, reverse=True)\n    return tlds\n\ndef walk_tree(root: Element,\n              processor: Callable[[Element], Optional[_T]],\n              stop_after_first: bool=False) -> List[_T]:\n    results = []\n    queue = deque([root])\n\n    while queue:\n        currElement = queue.popleft()\n        for child in currElement.getchildren():\n            if child.getchildren():\n                queue.append(child)\n\n            result = processor(child)\n            if result is not None:\n                results.append(result)\n                if stop_after_first:\n                    return results\n\n    return results\n\nElementFamily = NamedTuple('ElementFamily', [\n    ('grandparent', Optional[Element]),\n    ('parent', Element),\n    ('child', Element)\n])\n\nResultWithFamily = NamedTuple('ResultWithFamily', [\n    ('family', ElementFamily),\n    ('result', Any)\n])\n\nElementPair = NamedTuple('ElementPair', [\n    ('parent', Optional[Element]),\n    ('value', Element)\n])\n\ndef walk_tree_with_family(root: Element,\n                          processor: Callable[[Element], Optional[_T]]\n                          ) -> List[ResultWithFamily]:\n    results = []\n\n    queue = deque([ElementPair(parent=None, value=root)])\n    while queue:\n        currElementPair = queue.popleft()\n        for child in currElementPair.value.getchildren():\n            if child.getchildren():\n                queue.append(ElementPair(parent=currElementPair, value=child))  # type: ignore  # Lack of Deque support in typing module for Python 3.4.3\n            result = processor(child)\n            if result is not None:\n                if currElementPair.parent is not None:\n                    grandparent_element = cast(ElementPair, currElementPair.parent)\n                    grandparent = grandparent_element.value\n                else:\n                    grandparent = None\n                family = ElementFamily(\n                    grandparent=grandparent,\n                    parent=currElementPair.value,\n                    child=child\n                )\n\n                results.append(ResultWithFamily(\n                    family=family,\n                    result=result\n                ))\n\n    return results\n\n# height is not actually used\ndef add_a(\n        root: Element,\n        url: str,\n        link: str,\n        title: Optional[str]=None,\n        desc: Optional[str]=None,\n        class_attr: str=\"message_inline_image\",\n        data_id: Optional[str]=None,\n        insertion_index: Optional[int]=None,\n        already_thumbnailed: Optional[bool]=False\n) -> None:\n    title = title if title is not None else url_filename(link)\n    title = title if title else \"\"\n    desc = desc if desc is not None else \"\"\n\n    if insertion_index is not None:\n        div = markdown.util.etree.Element(\"div\")\n        root.insert(insertion_index, div)\n    else:\n        div = markdown.util.etree.SubElement(root, \"div\")\n\n    div.set(\"class\", class_attr)\n    a = markdown.util.etree.SubElement(div, \"a\")\n    a.set(\"href\", link)\n    a.set(\"target\", \"_blank\")\n    a.set(\"title\", title)\n    if data_id is not None:\n        a.set(\"data-id\", data_id)\n    img = markdown.util.etree.SubElement(a, \"img\")\n    if settings.THUMBNAIL_IMAGES and (not already_thumbnailed) and user_uploads_or_external(url):\n        # See docs/thumbnailing.md for some high-level documentation.\n        #\n        # We strip leading '/' from relative URLs here to ensure\n        # consistency in what gets passed to /thumbnail\n        url = url.lstrip('/')\n        img.set(\"src\", \"/thumbnail?url={0}&size=thumbnail\".format(\n            urllib.parse.quote(url, safe='')\n        ))\n        img.set('data-src-fullsize', \"/thumbnail?url={0}&size=full\".format(\n            urllib.parse.quote(url, safe='')\n        ))\n    else:\n        img.set(\"src\", url)\n\n    if class_attr == \"message_inline_ref\":\n        summary_div = markdown.util.etree.SubElement(div, \"div\")\n        title_div = markdown.util.etree.SubElement(summary_div, \"div\")\n        title_div.set(\"class\", \"message_inline_image_title\")\n        title_div.text = title\n        desc_div = markdown.util.etree.SubElement(summary_div, \"desc\")\n        desc_div.set(\"class\", \"message_inline_image_desc\")\n\ndef add_embed(root: Element, link: str, extracted_data: Dict[str, Any]) -> None:\n    container = markdown.util.etree.SubElement(root, \"div\")\n    container.set(\"class\", \"message_embed\")\n\n    img_link = extracted_data.get('image')\n    if img_link:\n        parsed_img_link = urllib.parse.urlparse(img_link)\n        # Append domain where relative img_link url is given\n        if not parsed_img_link.netloc:\n            parsed_url = urllib.parse.urlparse(link)\n            domain = '{url.scheme}://{url.netloc}/'.format(url=parsed_url)\n            img_link = urllib.parse.urljoin(domain, img_link)\n        img = markdown.util.etree.SubElement(container, \"a\")\n        img.set(\"style\", \"background-image: url(\" + img_link + \")\")\n        img.set(\"href\", link)\n        img.set(\"target\", \"_blank\")\n        img.set(\"class\", \"message_embed_image\")\n\n    data_container = markdown.util.etree.SubElement(container, \"div\")\n    data_container.set(\"class\", \"data-container\")\n\n    title = extracted_data.get('title')\n    if title:\n        title_elm = markdown.util.etree.SubElement(data_container, \"div\")\n        title_elm.set(\"class\", \"message_embed_title\")\n        a = markdown.util.etree.SubElement(title_elm, \"a\")\n        a.set(\"href\", link)\n        a.set(\"target\", \"_blank\")\n        a.set(\"title\", title)\n        a.text = title\n    description = extracted_data.get('description')\n    if description:\n        description_elm = markdown.util.etree.SubElement(data_container, \"div\")\n        description_elm.set(\"class\", \"message_embed_description\")\n        description_elm.text = description\n\n@cache_with_key(lambda tweet_id: tweet_id, cache_name=\"database\", with_statsd_key=\"tweet_data\")\ndef fetch_tweet_data(tweet_id: str) -> Optional[Dict[str, Any]]:\n    if settings.TEST_SUITE:\n        from . import testing_mocks\n        res = testing_mocks.twitter(tweet_id)\n    else:\n        creds = {\n            'consumer_key': settings.TWITTER_CONSUMER_KEY,\n            'consumer_secret': settings.TWITTER_CONSUMER_SECRET,\n            'access_token_key': settings.TWITTER_ACCESS_TOKEN_KEY,\n            'access_token_secret': settings.TWITTER_ACCESS_TOKEN_SECRET,\n        }\n        if not all(creds.values()):\n            return None\n\n        # We lazily import twitter here because its import process is\n        # surprisingly slow, and doing so has a significant impact on\n        # the startup performance of `manage.py` commands.\n        import twitter\n\n        try:\n            api = twitter.Api(tweet_mode='extended', **creds)\n            # Sometimes Twitter hangs on responses.  Timing out here\n            # will cause the Tweet to go through as-is with no inline\n            # preview, rather than having the message be rejected\n            # entirely. This timeout needs to be less than our overall\n            # formatting timeout.\n            tweet = timeout(3, api.GetStatus, tweet_id)\n            res = tweet.AsDict()\n        except AttributeError:\n            bugdown_logger.error('Unable to load twitter api, you may have the wrong '\n                                 'library installed, see https://github.com/zulip/zulip/issues/86')\n            return None\n        except TimeoutExpired:\n            # We'd like to try again later and not cache the bad result,\n            # so we need to re-raise the exception (just as though\n            # we were being rate-limited)\n            raise\n        except twitter.TwitterError as e:\n            t = e.args[0]\n            if len(t) == 1 and ('code' in t[0]) and (t[0]['code'] == 34):\n                # Code 34 means that the message doesn't exist; return\n                # None so that we will cache the error\n                return None\n            elif len(t) == 1 and ('code' in t[0]) and (t[0]['code'] == 88 or\n                                                       t[0]['code'] == 130):\n                # Code 88 means that we were rate-limited and 130\n                # means Twitter is having capacity issues; either way\n                # just raise the error so we don't cache None and will\n                # try again later.\n                raise\n            else:\n                # It's not clear what to do in cases of other errors,\n                # but for now it seems reasonable to log at error\n                # level (so that we get notified), but then cache the\n                # failure to proceed with our usual work\n                bugdown_logger.error(traceback.format_exc())\n                return None\n    return res\n\nHEAD_START_RE = re.compile('^head[ >]')\nHEAD_END_RE = re.compile('^/head[ >]')\nMETA_START_RE = re.compile('^meta[ >]')\nMETA_END_RE = re.compile('^/meta[ >]')\n\ndef fetch_open_graph_image(url: str) -> Optional[Dict[str, Any]]:\n    in_head = False\n    # HTML will auto close meta tags, when we start the next tag add\n    # a closing tag if it has not been closed yet.\n    last_closed = True\n    head = []\n    # TODO: What if response content is huge? Should we get headers first?\n    try:\n        content = requests.get(url, timeout=1).text\n    except Exception:\n        return None\n    # Extract the head and meta tags\n    # All meta tags are self closing, have no children or are closed\n    # automatically.\n    for part in content.split('<'):\n        if not in_head and HEAD_START_RE.match(part):\n            # Started the head node output it to have a document root\n            in_head = True\n            head.append('<head>')\n        elif in_head and HEAD_END_RE.match(part):\n            # Found the end of the head close any remaining tag then stop\n            # processing\n            in_head = False\n            if not last_closed:\n                last_closed = True\n                head.append('</meta>')\n            head.append('</head>')\n            break\n\n        elif in_head and META_START_RE.match(part):\n            # Found a meta node copy it\n            if not last_closed:\n                head.append('</meta>')\n                last_closed = True\n            head.append('<')\n            head.append(part)\n            if '/>' not in part:\n                last_closed = False\n\n        elif in_head and META_END_RE.match(part):\n            # End of a meta node just copy it to close the tag\n            head.append('<')\n            head.append(part)\n            last_closed = True\n\n    try:\n        doc = etree.fromstring(''.join(head))\n    except etree.ParseError:\n        return None\n    og_image = doc.find('meta[@property=\"og:image\"]')\n    og_title = doc.find('meta[@property=\"og:title\"]')\n    og_desc = doc.find('meta[@property=\"og:description\"]')\n    title = None\n    desc = None\n    if og_image is not None:\n        image = og_image.get('content')\n    else:\n        return None\n    if og_title is not None:\n        title = og_title.get('content')\n    if og_desc is not None:\n        desc = og_desc.get('content')\n    return {'image': image, 'title': title, 'desc': desc}\n\ndef get_tweet_id(url: str) -> Optional[str]:\n    parsed_url = urllib.parse.urlparse(url)\n    if not (parsed_url.netloc == 'twitter.com' or parsed_url.netloc.endswith('.twitter.com')):\n        return None\n    to_match = parsed_url.path\n    # In old-style twitter.com/#!/wdaher/status/1231241234-style URLs,\n    # we need to look at the fragment instead\n    if parsed_url.path == '/' and len(parsed_url.fragment) > 5:\n        to_match = parsed_url.fragment\n\n    tweet_id_match = re.match(r'^!?/.*?/status(es)?/(?P<tweetid>\\d{10,30})(/photo/[0-9])?/?$', to_match)\n    if not tweet_id_match:\n        return None\n    return tweet_id_match.group(\"tweetid\")\n\nclass InlineHttpsProcessor(markdown.treeprocessors.Treeprocessor):\n    def run(self, root: Element) -> None:\n        # Get all URLs from the blob\n        found_imgs = walk_tree(root, lambda e: e if e.tag == \"img\" else None)\n        for img in found_imgs:\n            url = img.get(\"src\")\n            if not url.startswith(\"http://\"):\n                # Don't rewrite images on our own site (e.g. emoji).\n                continue\n            img.set(\"src\", get_camo_url(url))\n\nclass BacktickPattern(markdown.inlinepatterns.Pattern):\n    \"\"\" Return a `<code>` element containing the matching text. \"\"\"\n    def __init__(self, pattern: str) -> None:\n        markdown.inlinepatterns.Pattern.__init__(self, pattern)\n        self.ESCAPED_BSLASH = '%s%s%s' % (markdown.util.STX, ord('\\\\'), markdown.util.ETX)\n        self.tag = 'code'\n\n    def handleMatch(self, m: Match[str]) -> Union[str, Element]:\n        if m.group(4):\n            el = markdown.util.etree.Element(self.tag)\n            # Modified to not strip whitespace\n            el.text = markdown.util.AtomicString(m.group(4))\n            return el\n        else:\n            return m.group(2).replace('\\\\\\\\', self.ESCAPED_BSLASH)\n\nclass InlineInterestingLinkProcessor(markdown.treeprocessors.Treeprocessor):\n    TWITTER_MAX_IMAGE_HEIGHT = 400\n    TWITTER_MAX_TO_PREVIEW = 3\n    INLINE_PREVIEW_LIMIT_PER_MESSAGE = 5\n\n    def __init__(self, md: markdown.Markdown) -> None:\n        markdown.treeprocessors.Treeprocessor.__init__(self, md)\n\n    def get_actual_image_url(self, url: str) -> str:\n        # Add specific per-site cases to convert image-preview urls to image urls.\n        # See https://github.com/zulip/zulip/issues/4658 for more information\n        parsed_url = urllib.parse.urlparse(url)\n        if (parsed_url.netloc == 'github.com' or parsed_url.netloc.endswith('.github.com')):\n            # https://github.com/zulip/zulip/blob/master/static/images/logo/zulip-icon-128x128.png ->\n            # https://raw.githubusercontent.com/zulip/zulip/master/static/images/logo/zulip-icon-128x128.png\n            split_path = parsed_url.path.split('/')\n            if len(split_path) > 3 and split_path[3] == \"blob\":\n                return urllib.parse.urljoin('https://raw.githubusercontent.com',\n                                            '/'.join(split_path[0:3] + split_path[4:]))\n\n        return url\n\n    def is_image(self, url: str) -> bool:\n        if not self.markdown.image_preview_enabled:\n            return False\n        parsed_url = urllib.parse.urlparse(url)\n        # List from http://support.google.com/chromeos/bin/answer.py?hl=en&answer=183093\n        for ext in [\".bmp\", \".gif\", \".jpg\", \"jpeg\", \".png\", \".webp\"]:\n            if parsed_url.path.lower().endswith(ext):\n                return True\n        return False\n\n    def dropbox_image(self, url: str) -> Optional[Dict[str, Any]]:\n        # TODO: The returned Dict could possibly be a TypedDict in future.\n        parsed_url = urllib.parse.urlparse(url)\n        if (parsed_url.netloc == 'dropbox.com' or parsed_url.netloc.endswith('.dropbox.com')):\n            is_album = parsed_url.path.startswith('/sc/') or parsed_url.path.startswith('/photos/')\n            # Only allow preview Dropbox shared links\n            if not (parsed_url.path.startswith('/s/') or\n                    parsed_url.path.startswith('/sh/') or\n                    is_album):\n                return None\n\n            # Try to retrieve open graph protocol info for a preview\n            # This might be redundant right now for shared links for images.\n            # However, we might want to make use of title and description\n            # in the future. If the actual image is too big, we might also\n            # want to use the open graph image.\n            image_info = fetch_open_graph_image(url)\n\n            is_image = is_album or self.is_image(url)\n\n            # If it is from an album or not an actual image file,\n            # just use open graph image.\n            if is_album or not is_image:\n                # Failed to follow link to find an image preview so\n                # use placeholder image and guess filename\n                if image_info is None:\n                    return None\n\n                image_info[\"is_image\"] = is_image\n                return image_info\n\n            # Otherwise, try to retrieve the actual image.\n            # This is because open graph image from Dropbox may have padding\n            # and gifs do not work.\n            # TODO: What if image is huge? Should we get headers first?\n            if image_info is None:\n                image_info = dict()\n            image_info['is_image'] = True\n            parsed_url_list = list(parsed_url)\n            parsed_url_list[4] = \"dl=1\"  # Replaces query\n            image_info[\"image\"] = urllib.parse.urlunparse(parsed_url_list)\n\n            return image_info\n        return None\n\n    def youtube_id(self, url: str) -> Optional[str]:\n        if not self.markdown.image_preview_enabled:\n            return None\n        # Youtube video id extraction regular expression from http://pastebin.com/KyKAFv1s\n        # Slightly modified to support URLs of the form youtu.be/<id>\n        # If it matches, match.group(2) is the video id.\n        schema_re = r'(?:https?://)'\n        host_re = r'(?:youtu\\.be/|(?:\\w+\\.)?youtube(?:-nocookie)?\\.com/)'\n        param_re = r'(?:(?:(?:v|embed)/)|(?:(?:watch(?:_popup)?(?:\\.php)?)?(?:\\?|#!?)(?:.+&)?v=))'\n        id_re = r'([0-9A-Za-z_-]+)'\n        youtube_re = r'^({schema_re}?{host_re}{param_re}?)?{id_re}(?(1).+)?$'\n        youtube_re = youtube_re.format(schema_re=schema_re, host_re=host_re, id_re=id_re, param_re=param_re)\n        match = re.match(youtube_re, url)\n        if match is None:\n            return None\n        return match.group(2)\n\n    def youtube_image(self, url: str) -> Optional[str]:\n        yt_id = self.youtube_id(url)\n\n        if yt_id is not None:\n            return \"https://i.ytimg.com/vi/%s/default.jpg\" % (yt_id,)\n        return None\n\n    def vimeo_id(self, url: str) -> Optional[str]:\n        if not self.markdown.image_preview_enabled:\n            return None\n        #(http|https)?:\\/\\/(www\\.)?vimeo.com\\/(?:channels\\/(?:\\w+\\/)?|groups\\/([^\\/]*)\\/videos\\/|)(\\d+)(?:|\\/\\?)\n        # If it matches, match.group('id') is the video id.\n\n        vimeo_re = r'^((http|https)?:\\/\\/(www\\.)?vimeo.com\\/' + \\\n                   r'(?:channels\\/(?:\\w+\\/)?|groups\\/' + \\\n                   r'([^\\/]*)\\/videos\\/|)(\\d+)(?:|\\/\\?))$'\n        match = re.match(vimeo_re, url)\n        if match is None:\n            return None\n        return match.group(5)\n\n    def vimeo_title(self, extracted_data: Dict[str, Any]) -> Optional[str]:\n        title = extracted_data.get(\"title\")\n        if title is not None:\n            return \"Vimeo - {}\".format(title)\n        return None\n\n    def twitter_text(self, text: str,\n                     urls: List[Dict[str, str]],\n                     user_mentions: List[Dict[str, Any]],\n                     media: List[Dict[str, Any]]) -> Element:\n        \"\"\"\n        Use data from the twitter API to turn links, mentions and media into A\n        tags. Also convert unicode emojis to images.\n\n        This works by using the urls, user_mentions and media data from\n        the twitter API and searching for unicode emojis in the text using\n        `unicode_emoji_regex`.\n\n        The first step is finding the locations of the URLs, mentions, media and\n        emoji in the text. For each match we build a dictionary with type, the start\n        location, end location, the URL to link to, and the text(codepoint and title\n        in case of emojis) to be used in the link(image in case of emojis).\n\n        Next we sort the matches by start location. And for each we add the\n        text from the end of the last link to the start of the current link to\n        the output. The text needs to added to the text attribute of the first\n        node (the P tag) or the tail the last link created.\n\n        Finally we add any remaining text to the last node.\n        \"\"\"\n\n        to_process = []  # type: List[Dict[str, Any]]\n        # Build dicts for URLs\n        for url_data in urls:\n            short_url = url_data[\"url\"]\n            full_url = url_data[\"expanded_url\"]\n            for match in re.finditer(re.escape(short_url), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'url',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': short_url,\n                    'text': full_url,\n                })\n        # Build dicts for mentions\n        for user_mention in user_mentions:\n            screen_name = user_mention['screen_name']\n            mention_string = '@' + screen_name\n            for match in re.finditer(re.escape(mention_string), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'mention',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': 'https://twitter.com/' + urllib.parse.quote(screen_name),\n                    'text': mention_string,\n                })\n        # Build dicts for media\n        for media_item in media:\n            short_url = media_item['url']\n            expanded_url = media_item['expanded_url']\n            for match in re.finditer(re.escape(short_url), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'media',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': short_url,\n                    'text': expanded_url,\n                })\n        # Build dicts for emojis\n        for match in re.finditer(unicode_emoji_regex, text, re.IGNORECASE):\n            orig_syntax = match.group('syntax')\n            codepoint = unicode_emoji_to_codepoint(orig_syntax)\n            if codepoint in codepoint_to_name:\n                display_string = ':' + codepoint_to_name[codepoint] + ':'\n                to_process.append({\n                    'type': 'emoji',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'codepoint': codepoint,\n                    'title': display_string,\n                })\n\n        to_process.sort(key=lambda x: x['start'])\n        p = current_node = markdown.util.etree.Element('p')\n\n        def set_text(text: str) -> None:\n            \"\"\"\n            Helper to set the text or the tail of the current_node\n            \"\"\"\n            if current_node == p:\n                current_node.text = text\n            else:\n                current_node.tail = text\n\n        db_data = self.markdown.zulip_db_data\n        current_index = 0\n        for item in to_process:\n            # The text we want to link starts in already linked text skip it\n            if item['start'] < current_index:\n                continue\n            # Add text from the end of last link to the start of the current\n            # link\n            set_text(text[current_index:item['start']])\n            current_index = item['end']\n            if item['type'] != 'emoji':\n                current_node = elem = url_to_a(db_data, item['url'], item['text'])\n            else:\n                current_node = elem = make_emoji(item['codepoint'], item['title'])\n            p.append(elem)\n\n        # Add any unused text\n        set_text(text[current_index:])\n        return p\n\n    def twitter_link(self, url: str) -> Optional[Element]:\n        tweet_id = get_tweet_id(url)\n\n        if tweet_id is None:\n            return None\n\n        try:\n            res = fetch_tweet_data(tweet_id)\n            if res is None:\n                return None\n            user = res['user']  # type: Dict[str, Any]\n            tweet = markdown.util.etree.Element(\"div\")\n            tweet.set(\"class\", \"twitter-tweet\")\n            img_a = markdown.util.etree.SubElement(tweet, 'a')\n            img_a.set(\"href\", url)\n            img_a.set(\"target\", \"_blank\")\n            profile_img = markdown.util.etree.SubElement(img_a, 'img')\n            profile_img.set('class', 'twitter-avatar')\n            # For some reason, for, e.g. tweet 285072525413724161,\n            # python-twitter does not give us a\n            # profile_image_url_https, but instead puts that URL in\n            # profile_image_url. So use _https if available, but fall\n            # back gracefully.\n            image_url = user.get('profile_image_url_https', user['profile_image_url'])\n            profile_img.set('src', image_url)\n\n            text = html.unescape(res['full_text'])\n            urls = res.get('urls', [])\n            user_mentions = res.get('user_mentions', [])\n            media = res.get('media', [])  # type: List[Dict[str, Any]]\n            p = self.twitter_text(text, urls, user_mentions, media)\n            tweet.append(p)\n\n            span = markdown.util.etree.SubElement(tweet, 'span')\n            span.text = \"- %s (@%s)\" % (user['name'], user['screen_name'])\n\n            # Add image previews\n            for media_item in media:\n                # Only photos have a preview image\n                if media_item['type'] != 'photo':\n                    continue\n\n                # Find the image size that is smaller than\n                # TWITTER_MAX_IMAGE_HEIGHT px tall or the smallest\n                size_name_tuples = list(media_item['sizes'].items())\n                size_name_tuples.sort(reverse=True,\n                                      key=lambda x: x[1]['h'])\n                for size_name, size in size_name_tuples:\n                    if size['h'] < self.TWITTER_MAX_IMAGE_HEIGHT:\n                        break\n\n                media_url = '%s:%s' % (media_item['media_url_https'], size_name)\n                img_div = markdown.util.etree.SubElement(tweet, 'div')\n                img_div.set('class', 'twitter-image')\n                img_a = markdown.util.etree.SubElement(img_div, 'a')\n                img_a.set('href', media_item['url'])\n                img_a.set('target', '_blank')\n                img_a.set('title', media_item['url'])\n                img = markdown.util.etree.SubElement(img_a, 'img')\n                img.set('src', media_url)\n\n            return tweet\n        except Exception:\n            # We put this in its own try-except because it requires external\n            # connectivity. If Twitter flakes out, we don't want to not-render\n            # the entire message; we just want to not show the Twitter preview.\n            bugdown_logger.warning(traceback.format_exc())\n            return None\n\n    def get_url_data(self, e: Element) -> Optional[Tuple[str, str]]:\n        if e.tag == \"a\":\n            if e.text is not None:\n                return (e.get(\"href\"), e.text)\n            return (e.get(\"href\"), e.get(\"href\"))\n        return None\n\n    def handle_image_inlining(self, root: Element, found_url: ResultWithFamily) -> None:\n        grandparent = found_url.family.grandparent\n        parent = found_url.family.parent\n        ahref_element = found_url.family.child\n        (url, text) = found_url.result\n        actual_url = self.get_actual_image_url(url)\n\n        # url != text usually implies a named link, which we opt not to remove\n        url_eq_text = (url == text)\n\n        if parent.tag == 'li':\n            add_a(parent, self.get_actual_image_url(url), url, title=text)\n            if not parent.text and not ahref_element.tail and url_eq_text:\n                parent.remove(ahref_element)\n\n        elif parent.tag == 'p':\n            parent_index = None\n            for index, uncle in enumerate(grandparent.getchildren()):\n                if uncle is parent:\n                    parent_index = index\n                    break\n\n            if parent_index is not None:\n                ins_index = self.find_proper_insertion_index(grandparent, parent, parent_index)\n                add_a(grandparent, actual_url, url, title=text, insertion_index=ins_index)\n\n            else:\n                # We're not inserting after parent, since parent not found.\n                # Append to end of list of grandparent's children as normal\n                add_a(grandparent, actual_url, url, title=text)\n\n            # If link is alone in a paragraph, delete paragraph containing it\n            if (len(parent.getchildren()) == 1 and\n                    (not parent.text or parent.text == \"\\n\") and\n                    not ahref_element.tail and\n                    url_eq_text):\n                grandparent.remove(parent)\n\n        else:\n            # If none of the above criteria match, fall back to old behavior\n            add_a(root, actual_url, url, title=text)\n\n    def find_proper_insertion_index(self, grandparent: Element, parent: Element,\n                                    parent_index_in_grandparent: int) -> int:\n        # If there are several inline images from same paragraph, ensure that\n        # they are in correct (and not opposite) order by inserting after last\n        # inline image from paragraph 'parent'\n\n        uncles = grandparent.getchildren()\n        parent_links = [ele.attrib['href'] for ele in parent.iter(tag=\"a\")]\n        insertion_index = parent_index_in_grandparent\n\n        while True:\n            insertion_index += 1\n            if insertion_index >= len(uncles):\n                return insertion_index\n\n            uncle = uncles[insertion_index]\n            inline_image_classes = ['message_inline_image', 'message_inline_ref']\n            if (\n                uncle.tag != 'div' or\n                'class' not in uncle.keys() or\n                uncle.attrib['class'] not in inline_image_classes\n            ):\n                return insertion_index\n\n            uncle_link = list(uncle.iter(tag=\"a\"))[0].attrib['href']\n            if uncle_link not in parent_links:\n                return insertion_index\n\n    def is_absolute_url(self, url: str) -> bool:\n        return bool(urllib.parse.urlparse(url).netloc)\n\n    def run(self, root: Element) -> None:\n        # Get all URLs from the blob\n        found_urls = walk_tree_with_family(root, self.get_url_data)\n        if len(found_urls) == 0 or len(found_urls) > self.INLINE_PREVIEW_LIMIT_PER_MESSAGE:\n            return\n\n        rendered_tweet_count = 0\n\n        for found_url in found_urls:\n            (url, text) = found_url.result\n            if not self.is_absolute_url(url):\n                if self.is_image(url):\n                    self.handle_image_inlining(root, found_url)\n                # We don't have a strong use case for doing url preview for relative links.\n                continue\n\n            dropbox_image = self.dropbox_image(url)\n            if dropbox_image is not None:\n                class_attr = \"message_inline_ref\"\n                is_image = dropbox_image[\"is_image\"]\n                if is_image:\n                    class_attr = \"message_inline_image\"\n                    # Not making use of title and description of images\n                add_a(root, dropbox_image['image'], url,\n                      title=dropbox_image.get('title', \"\"),\n                      desc=dropbox_image.get('desc', \"\"),\n                      class_attr=class_attr,\n                      already_thumbnailed=True)\n                continue\n            if self.is_image(url):\n                self.handle_image_inlining(root, found_url)\n                continue\n            if get_tweet_id(url) is not None:\n                if rendered_tweet_count >= self.TWITTER_MAX_TO_PREVIEW:\n                    # Only render at most one tweet per message\n                    continue\n                twitter_data = self.twitter_link(url)\n                if twitter_data is None:\n                    # This link is not actually a tweet known to twitter\n                    continue\n                rendered_tweet_count += 1\n                div = markdown.util.etree.SubElement(root, \"div\")\n                div.set(\"class\", \"inline-preview-twitter\")\n                div.insert(0, twitter_data)\n                continue\n            youtube = self.youtube_image(url)\n            if youtube is not None:\n                yt_id = self.youtube_id(url)\n                add_a(root, youtube, url, None, None,\n                      \"youtube-video message_inline_image\",\n                      yt_id, already_thumbnailed=True)\n                continue\n\n            db_data = self.markdown.zulip_db_data\n            if db_data and db_data['sent_by_bot']:\n                continue\n\n            if not self.markdown.url_embed_preview_enabled:\n                continue\n\n            try:\n                extracted_data = link_preview.link_embed_data_from_cache(url)\n            except NotFoundInCache:\n                self.markdown.zulip_message.links_for_preview.add(url)\n                continue\n            if extracted_data:\n                vm_id = self.vimeo_id(url)\n                if vm_id is not None:\n                    vimeo_image = extracted_data.get('image')\n                    vimeo_title = self.vimeo_title(extracted_data)\n                    if vimeo_image is not None:\n                        add_a(root, vimeo_image, url, vimeo_title,\n                              None, \"vimeo-video message_inline_image\", vm_id,\n                              already_thumbnailed=True)\n                    if vimeo_title is not None:\n                        found_url.family.child.text = vimeo_title\n                else:\n                    add_embed(root, url, extracted_data)\n\nclass Avatar(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        img = markdown.util.etree.Element('img')\n        email_address = match.group('email')\n        email = email_address.strip().lower()\n        profile_id = None\n\n        db_data = self.markdown.zulip_db_data\n        if db_data is not None:\n            user_dict = db_data['email_info'].get(email)\n            if user_dict is not None:\n                profile_id = user_dict['id']\n\n        img.set('class', 'message_body_gravatar')\n        img.set('src', '/avatar/{0}?s=30'.format(profile_id or email))\n        img.set('title', email)\n        img.set('alt', email)\n        return img\n\ndef possible_avatar_emails(content: str) -> Set[str]:\n    emails = set()\n    for REGEX in [AVATAR_REGEX, GRAVATAR_REGEX]:\n        matches = re.findall(REGEX, content)\n        for email in matches:\n            if email:\n                emails.add(email)\n\n    return emails\n\npath_to_name_to_codepoint = os.path.join(settings.STATIC_ROOT,\n                                         \"generated\", \"emoji\", \"name_to_codepoint.json\")\nwith open(path_to_name_to_codepoint) as name_to_codepoint_file:\n    name_to_codepoint = ujson.load(name_to_codepoint_file)\n\npath_to_codepoint_to_name = os.path.join(settings.STATIC_ROOT,\n                                         \"generated\", \"emoji\", \"codepoint_to_name.json\")\nwith open(path_to_codepoint_to_name) as codepoint_to_name_file:\n    codepoint_to_name = ujson.load(codepoint_to_name_file)\n\n# All of our emojis(non ZWJ sequences) belong to one of these unicode blocks:\n# \\U0001f100-\\U0001f1ff - Enclosed Alphanumeric Supplement\n# \\U0001f200-\\U0001f2ff - Enclosed Ideographic Supplement\n# \\U0001f300-\\U0001f5ff - Miscellaneous Symbols and Pictographs\n# \\U0001f600-\\U0001f64f - Emoticons (Emoji)\n# \\U0001f680-\\U0001f6ff - Transport and Map Symbols\n# \\U0001f900-\\U0001f9ff - Supplemental Symbols and Pictographs\n# \\u2000-\\u206f         - General Punctuation\n# \\u2300-\\u23ff         - Miscellaneous Technical\n# \\u2400-\\u243f         - Control Pictures\n# \\u2440-\\u245f         - Optical Character Recognition\n# \\u2460-\\u24ff         - Enclosed Alphanumerics\n# \\u2500-\\u257f         - Box Drawing\n# \\u2580-\\u259f         - Block Elements\n# \\u25a0-\\u25ff         - Geometric Shapes\n# \\u2600-\\u26ff         - Miscellaneous Symbols\n# \\u2700-\\u27bf         - Dingbats\n# \\u2900-\\u297f         - Supplemental Arrows-B\n# \\u2b00-\\u2bff         - Miscellaneous Symbols and Arrows\n# \\u3000-\\u303f         - CJK Symbols and Punctuation\n# \\u3200-\\u32ff         - Enclosed CJK Letters and Months\nunicode_emoji_regex = '(?P<syntax>['\\\n    '\\U0001F100-\\U0001F64F'    \\\n    '\\U0001F680-\\U0001F6FF'    \\\n    '\\U0001F900-\\U0001F9FF'    \\\n    '\\u2000-\\u206F'            \\\n    '\\u2300-\\u27BF'            \\\n    '\\u2900-\\u297F'            \\\n    '\\u2B00-\\u2BFF'            \\\n    '\\u3000-\\u303F'            \\\n    '\\u3200-\\u32FF'            \\\n    '])'\n# The equivalent JS regex is \\ud83c[\\udd00-\\udfff]|\\ud83d[\\udc00-\\ude4f]|\\ud83d[\\ude80-\\udeff]|\n# \\ud83e[\\udd00-\\uddff]|[\\u2000-\\u206f]|[\\u2300-\\u27bf]|[\\u2b00-\\u2bff]|[\\u3000-\\u303f]|\n# [\\u3200-\\u32ff]. See below comments for explanation. The JS regex is used by marked.js for\n# frontend unicode emoji processing.\n# The JS regex \\ud83c[\\udd00-\\udfff]|\\ud83d[\\udc00-\\ude4f] represents U0001f100-\\U0001f64f\n# The JS regex \\ud83d[\\ude80-\\udeff] represents \\U0001f680-\\U0001f6ff\n# The JS regex \\ud83e[\\udd00-\\uddff] represents \\U0001f900-\\U0001f9ff\n# The JS regex [\\u2000-\\u206f] represents \\u2000-\\u206f\n# The JS regex [\\u2300-\\u27bf] represents \\u2300-\\u27bf\n# Similarly other JS regexes can be mapped to the respective unicode blocks.\n# For more information, please refer to the following article:\n# http://crocodillon.com/blog/parsing-emoji-unicode-in-javascript\n\ndef make_emoji(codepoint: str, display_string: str) -> Element:\n    # Replace underscore in emoji's title with space\n    title = display_string[1:-1].replace(\"_\", \" \")\n    span = markdown.util.etree.Element('span')\n    span.set('class', 'emoji emoji-%s' % (codepoint,))\n    span.set('title', title)\n    span.set('role', 'img')\n    span.set('aria-label', title)\n    span.text = display_string\n    return span\n\ndef make_realm_emoji(src: str, display_string: str) -> Element:\n    elt = markdown.util.etree.Element('img')\n    elt.set('src', src)\n    elt.set('class', 'emoji')\n    elt.set(\"alt\", display_string)\n    elt.set(\"title\", display_string[1:-1].replace(\"_\", \" \"))\n    return elt\n\ndef unicode_emoji_to_codepoint(unicode_emoji: str) -> str:\n    codepoint = hex(ord(unicode_emoji))[2:]\n    # Unicode codepoints are minimum of length 4, padded\n    # with zeroes if the length is less than zero.\n    while len(codepoint) < 4:\n        codepoint = '0' + codepoint\n    return codepoint\n\nclass EmoticonTranslation(markdown.inlinepatterns.Pattern):\n    \"\"\" Translates emoticons like `:)` into emoji like `:smile:`. \"\"\"\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        db_data = self.markdown.zulip_db_data\n        if db_data is None or not db_data['translate_emoticons']:\n            return None\n\n        emoticon = match.group('emoticon')\n        translated = translate_emoticons(emoticon)\n        name = translated[1:-1]\n        return make_emoji(name_to_codepoint[name], translated)\n\nclass UnicodeEmoji(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        orig_syntax = match.group('syntax')\n        codepoint = unicode_emoji_to_codepoint(orig_syntax)\n        if codepoint in codepoint_to_name:\n            display_string = ':' + codepoint_to_name[codepoint] + ':'\n            return make_emoji(codepoint, display_string)\n        else:\n            return None\n\nclass Emoji(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        orig_syntax = match.group(\"syntax\")\n        name = orig_syntax[1:-1]\n\n        active_realm_emoji = {}  # type: Dict[str, Dict[str, str]]\n        db_data = self.markdown.zulip_db_data\n        if db_data is not None:\n            active_realm_emoji = db_data['active_realm_emoji']\n\n        if self.markdown.zulip_message and name in active_realm_emoji:\n            return make_realm_emoji(active_realm_emoji[name]['source_url'], orig_syntax)\n        elif name == 'zulip':\n            return make_realm_emoji('/static/generated/emoji/images/emoji/unicode/zulip.png', orig_syntax)\n        elif name in name_to_codepoint:\n            return make_emoji(name_to_codepoint[name], orig_syntax)\n        else:\n            return None\n\ndef content_has_emoji_syntax(content: str) -> bool:\n    return re.search(EMOJI_REGEX, content) is not None\n\nclass ModalLink(markdown.inlinepatterns.Pattern):\n    \"\"\"\n    A pattern that allows including in-app modal links in messages.\n    \"\"\"\n\n    def handleMatch(self, match: Match[str]) -> Element:\n        relative_url = match.group('relative_url')\n        text = match.group('text')\n\n        a_tag = markdown.util.etree.Element(\"a\")\n        a_tag.set(\"href\", relative_url)\n        a_tag.set(\"title\", relative_url)\n        a_tag.text = text\n\n        return a_tag\n\nclass Tex(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Element:\n        rendered = render_tex(match.group('body'), is_inline=True)\n        if rendered is not None:\n            return etree.fromstring(rendered.encode('utf-8'))\n        else:  # Something went wrong while rendering\n            span = markdown.util.etree.Element('span')\n            span.set('class', 'tex-error')\n            span.text = '$$' + match.group('body') + '$$'\n            return span\n\nupload_title_re = re.compile(\"^(https?://[^/]*)?(/user_uploads/\\\\d+)(/[^/]*)?/[^/]*/(?P<filename>[^/]*)$\")\ndef url_filename(url: str) -> str:\n    \"\"\"Extract the filename if a URL is an uploaded file, or return the original URL\"\"\"\n    match = upload_title_re.match(url)\n    if match:\n        return match.group('filename')\n    else:\n        return url\n\ndef fixup_link(link: markdown.util.etree.Element, target_blank: bool=True) -> None:\n    \"\"\"Set certain attributes we want on every link.\"\"\"\n    if target_blank:\n        link.set('target', '_blank')\n    link.set('title', url_filename(link.get('href')))\n\n\ndef sanitize_url(url: str) -> Optional[str]:\n    \"\"\"\n    Sanitize a url against xss attacks.\n    See the docstring on markdown.inlinepatterns.LinkPattern.sanitize_url.\n    \"\"\"\n    try:\n        parts = urllib.parse.urlparse(url.replace(' ', '%20'))\n        scheme, netloc, path, params, query, fragment = parts\n    except ValueError:\n        # Bad url - so bad it couldn't be parsed.\n        return ''\n\n    # If there is no scheme or netloc and there is a '@' in the path,\n    # treat it as a mailto: and set the appropriate scheme\n    if scheme == '' and netloc == '' and '@' in path:\n        scheme = 'mailto'\n    elif scheme == '' and netloc == '' and len(path) > 0 and path[0] == '/':\n        # Allow domain-relative links\n        return urllib.parse.urlunparse(('', '', path, params, query, fragment))\n    elif (scheme, netloc, path, params, query) == ('', '', '', '', '') and len(fragment) > 0:\n        # Allow fragment links\n        return urllib.parse.urlunparse(('', '', '', '', '', fragment))\n\n    # Zulip modification: If scheme is not specified, assume http://\n    # We re-enter sanitize_url because netloc etc. need to be re-parsed.\n    if not scheme:\n        return sanitize_url('http://' + url)\n\n    locless_schemes = ['mailto', 'news', 'file', 'bitcoin']\n    if netloc == '' and scheme not in locless_schemes:\n        # This fails regardless of anything else.\n        # Return immediately to save additional processing\n        return None\n\n    # Upstream code will accept a URL like javascript://foo because it\n    # appears to have a netloc.  Additionally there are plenty of other\n    # schemes that do weird things like launch external programs.  To be\n    # on the safe side, we whitelist the scheme.\n    if scheme not in ('http', 'https', 'ftp', 'mailto', 'file', 'bitcoin'):\n        return None\n\n    # Upstream code scans path, parameters, and query for colon characters\n    # because\n    #\n    #    some aliases [for javascript:] will appear to urllib.parse to have\n    #    no scheme. On top of that relative links (i.e.: \"foo/bar.html\")\n    #    have no scheme.\n    #\n    # We already converted an empty scheme to http:// above, so we skip\n    # the colon check, which would also forbid a lot of legitimate URLs.\n\n    # Url passes all tests. Return url as-is.\n    return urllib.parse.urlunparse((scheme, netloc, path, params, query, fragment))\n\ndef url_to_a(db_data: Optional[DbData], url: str, text: Optional[str]=None) -> Union[Element, str]:\n    a = markdown.util.etree.Element('a')\n\n    href = sanitize_url(url)\n    target_blank = True\n    if href is None:\n        # Rejected by sanitize_url; render it as plain text.\n        return url\n    if text is None:\n        text = markdown.util.AtomicString(url)\n\n    href = rewrite_local_links_to_relative(db_data, href)\n    target_blank = not href.startswith(\"#narrow\") and not href.startswith('mailto:')\n\n    a.set('href', href)\n    a.text = text\n    fixup_link(a, target_blank)\n    return a\n\nclass CompiledPattern(markdown.inlinepatterns.Pattern):\n    def __init__(self, compiled_re: Pattern, md: markdown.Markdown) -> None:\n        # This is similar to the superclass's small __init__ function,\n        # but we skip the compilation step and let the caller give us\n        # a compiled regex.\n        self.compiled_re = compiled_re\n        self.md = md\n\nclass AutoLink(CompiledPattern):\n    def handleMatch(self, match: Match[str]) -> ElementStringNone:\n        url = match.group('url')\n        db_data = self.markdown.zulip_db_data\n        return url_to_a(db_data, url)\n\nclass UListProcessor(markdown.blockprocessors.UListProcessor):\n    \"\"\" Process unordered list blocks.\n\n        Based on markdown.blockprocessors.UListProcessor, but does not accept\n        '+' or '-' as a bullet character.\"\"\"\n\n    TAG = 'ul'\n    RE = re.compile('^[ ]{0,3}[*][ ]+(.*)')\n\n    def __init__(self, parser: Any) -> None:\n\n        # HACK: Set the tab length to 2 just for the initialization of\n        # this class, so that bulleted lists (and only bulleted lists)\n        # work off 2-space indentation.\n        parser.markdown.tab_length = 2\n        super().__init__(parser)\n        parser.markdown.tab_length = 4\n\nclass ListIndentProcessor(markdown.blockprocessors.ListIndentProcessor):\n    \"\"\" Process unordered list blocks.\n\n        Based on markdown.blockprocessors.ListIndentProcessor, but with 2-space indent\n    \"\"\"\n\n    def __init__(self, parser: Any) -> None:\n\n        # HACK: Set the tab length to 2 just for the initialization of\n        # this class, so that bulleted lists (and only bulleted lists)\n        # work off 2-space indentation.\n        parser.markdown.tab_length = 2\n        super().__init__(parser)\n        parser.markdown.tab_length = 4\n\nclass BlockQuoteProcessor(markdown.blockprocessors.BlockQuoteProcessor):\n    \"\"\" Process BlockQuotes.\n\n        Based on markdown.blockprocessors.BlockQuoteProcessor, but with 2-space indent\n    \"\"\"\n\n    # Original regex for blockquote is RE = re.compile(r'(^|\\n)[ ]{0,3}>[ ]?(.*)')\n    RE = re.compile(r'(^|\\n)(?!(?:[ ]{0,3}>\\s*(?:$|\\n))*(?:$|\\n))'\n                    r'[ ]{0,3}>[ ]?(.*)')\n    mention_re = re.compile(mention.find_mentions)\n\n    def clean(self, line: str) -> str:\n        # Silence all the mentions inside blockquotes\n        line = re.sub(self.mention_re, lambda m: \"@_{}\".format(m.group('match')), line)\n\n        # And then run the upstream processor's code for removing the '>'\n        return super().clean(line)\n\nclass BugdownUListPreprocessor(markdown.preprocessors.Preprocessor):\n    \"\"\" Allows unordered list blocks that come directly after a\n        paragraph to be rendered as an unordered list\n\n        Detects paragraphs that have a matching list item that comes\n        directly after a line of text, and inserts a newline between\n        to satisfy Markdown\"\"\"\n\n    LI_RE = re.compile('^[ ]{0,3}[*][ ]+(.*)', re.MULTILINE)\n    HANGING_ULIST_RE = re.compile('^.+\\\\n([ ]{0,3}[*][ ]+.*)', re.MULTILINE)\n\n    def run(self, lines: List[str]) -> List[str]:\n        \"\"\" Insert a newline between a paragraph and ulist if missing \"\"\"\n        inserts = 0\n        fence = None\n        copy = lines[:]\n        for i in range(len(lines) - 1):\n            # Ignore anything that is inside a fenced code block\n            m = FENCE_RE.match(lines[i])\n            if not fence and m:\n                fence = m.group('fence')\n            elif fence and m and fence == m.group('fence'):\n                fence = None\n\n            # If we're not in a fenced block and we detect an upcoming list\n            #  hanging off a paragraph, add a newline\n            if (not fence and lines[i] and\n                self.LI_RE.match(lines[i+1]) and\n                    not self.LI_RE.match(lines[i])):\n\n                copy.insert(i+inserts+1, '')\n                inserts += 1\n        return copy\n\nclass AutoNumberOListPreprocessor(markdown.preprocessors.Preprocessor):\n    \"\"\" Finds a sequence of lines numbered by the same number\"\"\"\n    RE = re.compile(r'^([ ]*)(\\d+)\\.[ ]+(.*)')\n    TAB_LENGTH = 2\n\n    def run(self, lines: List[str]) -> List[str]:\n        new_lines = []  # type: List[str]\n        current_list = []  # type: List[Match[str]]\n        current_indent = 0\n\n        for line in lines:\n            m = self.RE.match(line)\n\n            # Remember if this line is a continuation of already started list\n            is_next_item = (m and current_list\n                            and current_indent == len(m.group(1)) // self.TAB_LENGTH)\n\n            if not is_next_item:\n                # There is no more items in the list we were processing\n                new_lines.extend(self.renumber(current_list))\n                current_list = []\n\n            if not m:\n                # Ordinary line\n                new_lines.append(line)\n            elif is_next_item:\n                # Another list item\n                current_list.append(m)\n            else:\n                # First list item\n                current_list = [m]\n                current_indent = len(m.group(1)) // self.TAB_LENGTH\n\n        new_lines.extend(self.renumber(current_list))\n\n        return new_lines\n\n    def renumber(self, mlist: List[Match[str]]) -> List[str]:\n        if not mlist:\n            return []\n\n        start_number = int(mlist[0].group(2))\n\n        # Change numbers only if every one is the same\n        change_numbers = True\n        for m in mlist:\n            if int(m.group(2)) != start_number:\n                change_numbers = False\n                break\n\n        lines = []  # type: List[str]\n        counter = start_number\n\n        for m in mlist:\n            number = str(counter) if change_numbers else m.group(2)\n            lines.append('%s%s. %s' % (m.group(1), number, m.group(3)))\n            counter += 1\n\n        return lines\n\n# We need the following since upgrade from py-markdown 2.6.11 to 3.0.1\n# modifies the link handling significantly. The following is taken from\n# py-markdown 2.6.11 markdown/inlinepatterns.py.\n@one_time\ndef get_link_re() -> str:\n    '''\n    Very important--if you need to change this code to depend on\n    any arguments, you must eliminate the \"one_time\" decorator\n    and consider performance implications.  We only want to compute\n    this value once.\n    '''\n\n    NOBRACKET = r'[^\\]\\[]*'\n    BRK = (\n        r'\\[(' +\n        (NOBRACKET + r'(\\[')*6 +\n        (NOBRACKET + r'\\])*')*6 +\n        NOBRACKET + r')\\]'\n    )\n    NOIMG = r'(?<!\\!)'\n\n    # [text](url) or [text](<url>) or [text](url \"title\")\n    LINK_RE = NOIMG + BRK + \\\n        r'''\\(\\s*(<(?:[^<>\\\\]|\\\\.)*>|(\\([^()]*\\)|[^()])*?)\\s*(('(?:[^'\\\\]|\\\\.)*'|\"(?:[^\"\\\\]|\\\\.)*\")\\s*)?\\)'''\n    return normal_compile(LINK_RE)\n\ndef prepare_realm_pattern(source: str) -> str:\n    \"\"\" Augment a realm filter so it only matches after start-of-string,\n    whitespace, or opening delimiters, won't match if there are word\n    characters directly after, and saves what was matched as \"name\". \"\"\"\n    return r\"\"\"(?<![^\\s'\"\\(,:<])(?P<name>\"\"\" + source + r')(?!\\w)'\n\n# Given a regular expression pattern, linkifies groups that match it\n# using the provided format string to construct the URL.\nclass RealmFilterPattern(markdown.inlinepatterns.Pattern):\n    \"\"\" Applied a given realm filter to the input \"\"\"\n\n    def __init__(self, source_pattern: str,\n                 format_string: str,\n                 markdown_instance: Optional[markdown.Markdown]=None) -> None:\n        self.pattern = prepare_realm_pattern(source_pattern)\n        self.format_string = format_string\n        markdown.inlinepatterns.Pattern.__init__(self, self.pattern, markdown_instance)\n\n    def handleMatch(self, m: Match[str]) -> Union[Element, str]:\n        db_data = self.markdown.zulip_db_data\n        return url_to_a(db_data,\n                        self.format_string % m.groupdict(),\n                        m.group(\"name\"))\n\nclass UserMentionPattern(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        match = m.group('match')\n        silent = m.group('silent') == '_'\n\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            if match.startswith(\"**\") and match.endswith(\"**\"):\n                name = match[2:-2]\n            else:\n                return None\n\n            wildcard = mention.user_mention_matches_wildcard(name)\n\n            id_syntax_match = re.match(r'.+\\|(?P<user_id>\\d+)$', name)\n            if id_syntax_match:\n                id = id_syntax_match.group(\"user_id\")\n                user = db_data['mention_data'].get_user_by_id(id)\n            else:\n                user = db_data['mention_data'].get_user_by_name(name)\n\n            if wildcard:\n                self.markdown.zulip_message.mentions_wildcard = True\n                user_id = \"*\"\n            elif user:\n                if not silent:\n                    self.markdown.zulip_message.mentions_user_ids.add(user['id'])\n                name = user['full_name']\n                user_id = str(user['id'])\n            else:\n                # Don't highlight @mentions that don't refer to a valid user\n                return None\n\n            el = markdown.util.etree.Element(\"span\")\n            el.set('data-user-id', user_id)\n            if silent:\n                el.set('class', 'user-mention silent')\n                el.text = \"%s\" % (name,)\n            else:\n                el.set('class', 'user-mention')\n                el.text = \"@%s\" % (name,)\n            return el\n        return None\n\nclass UserGroupMentionPattern(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        match = m.group(2)\n\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            name = extract_user_group(match)\n            user_group = db_data['mention_data'].get_user_group(name)\n            if user_group:\n                self.markdown.zulip_message.mentions_user_group_ids.add(user_group.id)\n                name = user_group.name\n                user_group_id = str(user_group.id)\n            else:\n                # Don't highlight @-mentions that don't refer to a valid user\n                # group.\n                return None\n\n            el = markdown.util.etree.Element(\"span\")\n            el.set('class', 'user-group-mention')\n            el.set('data-user-group-id', user_group_id)\n            el.text = \"@%s\" % (name,)\n            return el\n        return None\n\nclass StreamPattern(CompiledPattern):\n    def find_stream_by_name(self, name: Match[str]) -> Optional[Dict[str, Any]]:\n        db_data = self.markdown.zulip_db_data\n        if db_data is None:\n            return None\n        stream = db_data['stream_names'].get(name)\n        return stream\n\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        name = m.group('stream_name')\n\n        if self.markdown.zulip_message:\n            stream = self.find_stream_by_name(name)\n            if stream is None:\n                return None\n            el = markdown.util.etree.Element('a')\n            el.set('class', 'stream')\n            el.set('data-stream-id', str(stream['id']))\n            # TODO: We should quite possibly not be specifying the\n            # href here and instead having the browser auto-add the\n            # href when it processes a message with one of these, to\n            # provide more clarity to API clients.\n            stream_url = encode_stream(stream['id'], name)\n            el.set('href', '/#narrow/stream/{stream_url}'.format(stream_url=stream_url))\n            el.text = '#{stream_name}'.format(stream_name=name)\n            return el\n        return None\n\ndef possible_linked_stream_names(content: str) -> Set[str]:\n    matches = re.findall(STREAM_LINK_REGEX, content, re.VERBOSE)\n    return set(matches)\n\nclass AlertWordsNotificationProcessor(markdown.preprocessors.Preprocessor):\n    def run(self, lines: Iterable[str]) -> Iterable[str]:\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            # We check for alert words here, the set of which are\n            # dependent on which users may see this message.\n            #\n            # Our caller passes in the list of possible_words.  We\n            # don't do any special rendering; we just append the alert words\n            # we find to the set self.markdown.zulip_message.alert_words.\n\n            realm_words = db_data['possible_words']\n\n            content = '\\n'.join(lines).lower()\n\n            allowed_before_punctuation = \"|\".join([r'\\s', '^', r'[\\(\\\".,\\';\\[\\*`>]'])\n            allowed_after_punctuation = \"|\".join([r'\\s', '$', r'[\\)\\\"\\?:.,\\';\\]!\\*`]'])\n\n            for word in realm_words:\n                escaped = re.escape(word.lower())\n                match_re = re.compile('(?:%s)%s(?:%s)' %\n                                      (allowed_before_punctuation,\n                                       escaped,\n                                       allowed_after_punctuation))\n                if re.search(match_re, content):\n                    self.markdown.zulip_message.alert_words.add(word)\n\n        return lines\n\n# This prevents realm_filters from running on the content of a\n# Markdown link, breaking up the link.  This is a monkey-patch, but it\n# might be worth sending a version of this change upstream.\nclass AtomicLinkPattern(CompiledPattern):\n    def get_element(self, m: Match[str]) -> Optional[Element]:\n        href = m.group(9)\n        if not href:\n            return None\n\n        if href[0] == \"<\":\n            href = href[1:-1]\n        href = sanitize_url(self.unescape(href.strip()))\n        if href is None:\n            return None\n\n        db_data = self.markdown.zulip_db_data\n        href = rewrite_local_links_to_relative(db_data, href)\n\n        el = markdown.util.etree.Element('a')\n        el.text = m.group(2)\n        el.set('href', href)\n        fixup_link(el, target_blank=(href[:1] != '#'))\n        return el\n\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        ret = self.get_element(m)\n        if ret is None:\n            return None\n        if not isinstance(ret, str):\n            ret.text = markdown.util.AtomicString(ret.text)\n        return ret\n\ndef get_sub_registry(r: markdown.util.Registry, keys: List[str]) -> markdown.util.Registry:\n    # Registry is a new class added by py-markdown to replace Ordered List.\n    # Since Registry doesn't support .keys(), it is easier to make a new\n    # object instead of removing keys from the existing object.\n    new_r = markdown.util.Registry()\n    for k in keys:\n        new_r.register(r[k], k, r.get_index_for_name(k))\n    return new_r\n\n# These are used as keys (\"realm_filters_keys\") to md_engines and the respective\n# realm filter caches\nDEFAULT_BUGDOWN_KEY = -1\nZEPHYR_MIRROR_BUGDOWN_KEY = -2\n\nclass Bugdown(markdown.Markdown):\n    def __init__(self, *args: Any, **kwargs: Union[bool, int, List[Any]]) -> None:\n        # define default configs\n        self.config = {\n            \"realm_filters\": [kwargs['realm_filters'],\n                              \"Realm-specific filters for realm_filters_key %s\" % (kwargs['realm'],)],\n            \"realm\": [kwargs['realm'], \"Realm id\"],\n            \"code_block_processor_disabled\": [kwargs['code_block_processor_disabled'],\n                                              \"Disabled for email gateway\"]\n        }\n\n        super().__init__(*args, **kwargs)\n        self.set_output_format('html')\n\n    def build_parser(self) -> markdown.Markdown:\n        # Build the parser using selected default features from py-markdown.\n        # The complete list of all available processors can be found in the\n        # super().build_parser() function.\n        #\n        # Note: for any py-markdown updates, manually check if we want any\n        # of the new features added upstream or not; they wouldn't get\n        # included by default.\n        self.preprocessors = self.build_preprocessors()\n        self.parser = self.build_block_parser()\n        self.inlinePatterns = self.build_inlinepatterns()\n        self.treeprocessors = self.build_treeprocessors()\n        self.postprocessors = self.build_postprocessors()\n        self.handle_zephyr_mirror()\n        return self\n\n    def build_preprocessors(self) -> markdown.util.Registry:\n        # We disable the following preprocessors from upstream:\n        #\n        # html_block - insecure\n        # reference - references don't make sense in a chat context.\n        preprocessors = markdown.util.Registry()\n        preprocessors.register(AutoNumberOListPreprocessor(self), 'auto_number_olist', 40)\n        preprocessors.register(BugdownUListPreprocessor(self), 'hanging_ulists', 35)\n        preprocessors.register(markdown.preprocessors.NormalizeWhitespace(self), 'normalize_whitespace', 30)\n        preprocessors.register(fenced_code.FencedBlockPreprocessor(self), 'fenced_code_block', 25)\n        preprocessors.register(AlertWordsNotificationProcessor(self), 'custom_text_notifications', 20)\n        return preprocessors\n\n    def build_block_parser(self) -> markdown.util.Registry:\n        # We disable the following blockparsers from upstream:\n        #\n        # indent - replaced by ours\n        # hashheader - disabled, since headers look bad and don't make sense in a chat context.\n        # setextheader - disabled, since headers look bad and don't make sense in a chat context.\n        # olist - replaced by ours\n        # ulist - replaced by ours\n        # quote - replaced by ours\n        parser = markdown.blockprocessors.BlockParser(self)\n        parser.blockprocessors.register(markdown.blockprocessors.EmptyBlockProcessor(parser), 'empty', 85)\n        if not self.getConfig('code_block_processor_disabled'):\n            parser.blockprocessors.register(markdown.blockprocessors.CodeBlockProcessor(parser), 'code', 80)\n        # We get priority 75 from 'table' extension\n        parser.blockprocessors.register(markdown.blockprocessors.HRProcessor(parser), 'hr', 70)\n        parser.blockprocessors.register(UListProcessor(parser), 'ulist', 65)\n        parser.blockprocessors.register(ListIndentProcessor(parser), 'indent', 60)\n        parser.blockprocessors.register(BlockQuoteProcessor(parser), 'quote', 55)\n        parser.blockprocessors.register(markdown.blockprocessors.ParagraphProcessor(parser), 'paragraph', 50)\n        return parser\n\n    def build_inlinepatterns(self) -> markdown.util.Registry:\n        # We disable the following upstream inline patterns:\n        #\n        # backtick -        replaced by ours\n        # escape -          probably will re-add at some point.\n        # link -            replaced by ours\n        # image_link -      replaced by ours\n        # autolink -        replaced by ours\n        # automail -        replaced by ours\n        # linebreak -       we use nl2br and consider that good enough\n        # html -            insecure\n        # reference -       references not useful\n        # image_reference - references not useful\n        # short_reference - references not useful\n        # ---------------------------------------------------\n        # strong_em -       for these three patterns,\n        # strong2 -         we have our own versions where\n        # emphasis2 -       we disable _ for bold and emphasis\n\n        # Declare regexes for clean single line calls to .register().\n        NOT_STRONG_RE = markdown.inlinepatterns.NOT_STRONG_RE\n        # Custom strikethrough syntax: ~~foo~~\n        DEL_RE = r'(?<!~)(\\~\\~)([^~\\n]+?)(\\~\\~)(?!~)'\n        # Custom bold syntax: **foo** but not __foo__\n        # str inside ** must start and end with a word character\n        # it need for things like \"const char *x = (char *)y\"\n        EMPHASIS_RE = r'(\\*)(?!\\s+)([^\\*^\\n]+)(?<!\\s)\\*'\n        ENTITY_RE = markdown.inlinepatterns.ENTITY_RE\n        STRONG_EM_RE = r'(\\*\\*\\*)(?!\\s+)([^\\*^\\n]+)(?<!\\s)\\*\\*\\*'\n        # Inline code block without whitespace stripping\n        BACKTICK_RE = r'(?:(?<!\\\\)((?:\\\\{2})+)(?=`+)|(?<!\\\\)(`+)(.+?)(?<!`)\\3(?!`))'\n\n        # Add Inline Patterns.  We use a custom numbering of the\n        # rules, that preserves the order from upstream but leaves\n        # space for us to add our own.\n        reg = markdown.util.Registry()\n        reg.register(BacktickPattern(BACKTICK_RE), 'backtick', 105)\n        reg.register(markdown.inlinepatterns.DoubleTagPattern(STRONG_EM_RE, 'strong,em'), 'strong_em', 100)\n        reg.register(UserMentionPattern(mention.find_mentions, self), 'usermention', 95)\n        reg.register(Tex(r'\\B(?<!\\$)\\$\\$(?P<body>[^\\n_$](\\\\\\$|[^$\\n])*)\\$\\$(?!\\$)\\B'), 'tex', 90)\n        reg.register(StreamPattern(get_compiled_stream_link_regex(), self), 'stream', 85)\n        reg.register(Avatar(AVATAR_REGEX, self), 'avatar', 80)\n        reg.register(ModalLink(r'!modal_link\\((?P<relative_url>[^)]*), (?P<text>[^)]*)\\)'), 'modal_link', 75)\n        # Note that !gravatar syntax should be deprecated long term.\n        reg.register(Avatar(GRAVATAR_REGEX, self), 'gravatar', 70)\n        reg.register(UserGroupMentionPattern(mention.user_group_mentions, self), 'usergroupmention', 65)\n        reg.register(AtomicLinkPattern(get_link_re(), self), 'link', 60)\n        reg.register(AutoLink(get_web_link_regex(), self), 'autolink', 55)\n        # Reserve priority 45-54 for Realm Filters\n        reg = self.register_realm_filters(reg)\n        reg.register(markdown.inlinepatterns.HtmlInlineProcessor(ENTITY_RE, self), 'entity', 40)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(r'(\\*\\*)([^\\n]+?)\\2', 'strong'), 'strong', 35)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(EMPHASIS_RE, 'em'), 'emphasis', 30)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(DEL_RE, 'del'), 'del', 25)\n        reg.register(markdown.inlinepatterns.SimpleTextInlineProcessor(NOT_STRONG_RE), 'not_strong', 20)\n        reg.register(Emoji(EMOJI_REGEX, self), 'emoji', 15)\n        reg.register(EmoticonTranslation(emoticon_regex, self), 'translate_emoticons', 10)\n        # We get priority 5 from 'nl2br' extension\n        reg.register(UnicodeEmoji(unicode_emoji_regex), 'unicodeemoji', 0)\n        return reg\n\n    def register_realm_filters(self, inlinePatterns: markdown.util.Registry) -> markdown.util.Registry:\n        for (pattern, format_string, id) in self.getConfig(\"realm_filters\"):\n            inlinePatterns.register(RealmFilterPattern(pattern, format_string, self),\n                                    'realm_filters/%s' % (pattern), 45)\n        return inlinePatterns\n\n    def build_treeprocessors(self) -> markdown.util.Registry:\n        # Here we build all the processors from upstream, plus a few of our own.\n        treeprocessors = markdown.util.Registry()\n        # We get priority 30 from 'hilite' extension\n        treeprocessors.register(markdown.treeprocessors.InlineProcessor(self), 'inline', 25)\n        treeprocessors.register(markdown.treeprocessors.PrettifyTreeprocessor(self), 'prettify', 20)\n        treeprocessors.register(InlineInterestingLinkProcessor(self), 'inline_interesting_links', 15)\n        if settings.CAMO_URI:\n            treeprocessors.register(InlineHttpsProcessor(self), 'rewrite_to_https', 10)\n        return treeprocessors\n\n    def build_postprocessors(self) -> markdown.util.Registry:\n        # These are the default python-markdown processors, unmodified.\n        postprocessors = markdown.util.Registry()\n        postprocessors.register(markdown.postprocessors.RawHtmlPostprocessor(self), 'raw_html', 20)\n        postprocessors.register(markdown.postprocessors.AndSubstitutePostprocessor(), 'amp_substitute', 15)\n        postprocessors.register(markdown.postprocessors.UnescapePostprocessor(), 'unescape', 10)\n        return postprocessors\n\n    def getConfig(self, key: str, default: str='') -> Any:\n        \"\"\" Return a setting for the given key or an empty string. \"\"\"\n        if key in self.config:\n            return self.config[key][0]\n        else:\n            return default\n\n    def handle_zephyr_mirror(self) -> None:\n        if self.getConfig(\"realm\") == ZEPHYR_MIRROR_BUGDOWN_KEY:\n            # Disable almost all inline patterns for zephyr mirror\n            # users' traffic that is mirrored.  Note that\n            # inline_interesting_links is a treeprocessor and thus is\n            # not removed\n            self.inlinePatterns = get_sub_registry(self.inlinePatterns, ['autolink'])\n            self.treeprocessors = get_sub_registry(self.treeprocessors, ['inline_interesting_links',\n                                                                         'rewrite_to_https'])\n            # insert new 'inline' processor because we have changed self.inlinePatterns\n            # but InlineProcessor copies md as self.md in __init__.\n            self.treeprocessors.register(markdown.treeprocessors.InlineProcessor(self), 'inline', 25)\n            self.preprocessors = get_sub_registry(self.preprocessors, ['custom_text_notifications'])\n            self.parser.blockprocessors = get_sub_registry(self.parser.blockprocessors, ['paragraph'])\n\nmd_engines = {}  # type: Dict[Tuple[int, bool], markdown.Markdown]\nrealm_filter_data = {}  # type: Dict[int, List[Tuple[str, str, int]]]\n\ndef make_md_engine(realm_filters_key: int, email_gateway: bool) -> None:\n    md_engine_key = (realm_filters_key, email_gateway)\n    if md_engine_key in md_engines:\n        del md_engines[md_engine_key]\n\n    realm_filters = realm_filter_data[realm_filters_key]\n    md_engines[md_engine_key] = build_engine(\n        realm_filters=realm_filters,\n        realm_filters_key=realm_filters_key,\n        email_gateway=email_gateway,\n    )\n\ndef build_engine(realm_filters: List[Tuple[str, str, int]],\n                 realm_filters_key: int,\n                 email_gateway: bool) -> markdown.Markdown:\n    engine = Bugdown(\n        realm_filters=realm_filters,\n        realm=realm_filters_key,\n        code_block_processor_disabled=email_gateway,\n        extensions = [\n            nl2br.makeExtension(),\n            tables.makeExtension(),\n            codehilite.makeExtension(\n                linenums=False,\n                guess_lang=False\n            ),\n        ])\n    return engine\n\ndef topic_links(realm_filters_key: int, topic_name: str) -> List[str]:\n    matches = []  # type: List[str]\n\n    realm_filters = realm_filters_for_realm(realm_filters_key)\n\n    for realm_filter in realm_filters:\n        pattern = prepare_realm_pattern(realm_filter[0])\n        for m in re.finditer(pattern, topic_name):\n            matches += [realm_filter[1] % m.groupdict()]\n    return matches\n\ndef maybe_update_markdown_engines(realm_filters_key: Optional[int], email_gateway: bool) -> None:\n    # If realm_filters_key is None, load all filters\n    global realm_filter_data\n    if realm_filters_key is None:\n        all_filters = all_realm_filters()\n        all_filters[DEFAULT_BUGDOWN_KEY] = []\n        for realm_filters_key, filters in all_filters.items():\n            realm_filter_data[realm_filters_key] = filters\n            make_md_engine(realm_filters_key, email_gateway)\n        # Hack to ensure that getConfig(\"realm\") is right for mirrored Zephyrs\n        realm_filter_data[ZEPHYR_MIRROR_BUGDOWN_KEY] = []\n        make_md_engine(ZEPHYR_MIRROR_BUGDOWN_KEY, False)\n    else:\n        realm_filters = realm_filters_for_realm(realm_filters_key)\n        if realm_filters_key not in realm_filter_data or    \\\n                realm_filter_data[realm_filters_key] != realm_filters:\n            # Realm filters data has changed, update `realm_filter_data` and any\n            # of the existing markdown engines using this set of realm filters.\n            realm_filter_data[realm_filters_key] = realm_filters\n            for email_gateway_flag in [True, False]:\n                if (realm_filters_key, email_gateway_flag) in md_engines:\n                    # Update only existing engines(if any), don't create new one.\n                    make_md_engine(realm_filters_key, email_gateway_flag)\n\n        if (realm_filters_key, email_gateway) not in md_engines:\n            # Markdown engine corresponding to this key doesn't exists so create one.\n            make_md_engine(realm_filters_key, email_gateway)\n\n# We want to log Markdown parser failures, but shouldn't log the actual input\n# message for privacy reasons.  The compromise is to replace all alphanumeric\n# characters with 'x'.\n#\n# We also use repr() to improve reproducibility, and to escape terminal control\n# codes, which can do surprisingly nasty things.\n_privacy_re = re.compile('\\\\w', flags=re.UNICODE)\ndef privacy_clean_markdown(content: str) -> str:\n    return repr(_privacy_re.sub('x', content))\n\ndef log_bugdown_error(msg: str) -> None:\n    \"\"\"We use this unusual logging approach to log the bugdown error, in\n    order to prevent AdminNotifyHandler from sending the santized\n    original markdown formatting into another Zulip message, which\n    could cause an infinite exception loop.\"\"\"\n    bugdown_logger.error(msg)\n\ndef get_email_info(realm_id: int, emails: Set[str]) -> Dict[str, FullNameInfo]:\n    if not emails:\n        return dict()\n\n    q_list = {\n        Q(email__iexact=email.strip().lower())\n        for email in emails\n    }\n\n    rows = UserProfile.objects.filter(\n        realm_id=realm_id\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'email',\n    )\n\n    dct = {\n        row['email'].strip().lower(): row\n        for row in rows\n    }\n    return dct\n\ndef get_possible_mentions_info(realm_id: int, mention_texts: Set[str]) -> List[FullNameInfo]:\n    if not mention_texts:\n        return list()\n\n    # Remove the trailing part of the `name|id` mention syntax,\n    # thus storing only full names in full_names.\n    full_names = set()\n    name_re = r'(?P<full_name>.+)\\|\\d+$'\n    for mention_text in mention_texts:\n        name_syntax_match = re.match(name_re, mention_text)\n        if name_syntax_match:\n            full_names.add(name_syntax_match.group(\"full_name\"))\n        else:\n            full_names.add(mention_text)\n\n    q_list = {\n        Q(full_name__iexact=full_name)\n        for full_name in full_names\n    }\n\n    rows = UserProfile.objects.filter(\n        realm_id=realm_id,\n        is_active=True,\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'full_name',\n        'email',\n    )\n    return list(rows)\n\nclass MentionData:\n    def __init__(self, realm_id: int, content: str) -> None:\n        mention_texts = possible_mentions(content)\n        possible_mentions_info = get_possible_mentions_info(realm_id, mention_texts)\n        self.full_name_info = {\n            row['full_name'].lower(): row\n            for row in possible_mentions_info\n        }\n        self.user_id_info = {\n            row['id']: row\n            for row in possible_mentions_info\n        }\n        self.init_user_group_data(realm_id=realm_id, content=content)\n\n    def init_user_group_data(self,\n                             realm_id: int,\n                             content: str) -> None:\n        user_group_names = possible_user_group_mentions(content)\n        self.user_group_name_info = get_user_group_name_info(realm_id, user_group_names)\n        self.user_group_members = defaultdict(list)  # type: Dict[int, List[int]]\n        group_ids = [group.id for group in self.user_group_name_info.values()]\n\n        if not group_ids:\n            # Early-return to avoid the cost of hitting the ORM,\n            # which shows up in profiles.\n            return\n\n        membership = UserGroupMembership.objects.filter(user_group_id__in=group_ids)\n        for info in membership.values('user_group_id', 'user_profile_id'):\n            group_id = info['user_group_id']\n            user_profile_id = info['user_profile_id']\n            self.user_group_members[group_id].append(user_profile_id)\n\n    def get_user_by_name(self, name: str) -> Optional[FullNameInfo]:\n        # warning: get_user_by_name is not dependable if two\n        # users of the same full name are mentioned. Use\n        # get_user_by_id where possible.\n        return self.full_name_info.get(name.lower(), None)\n\n    def get_user_by_id(self, id: str) -> Optional[FullNameInfo]:\n        return self.user_id_info.get(int(id), None)\n\n    def get_user_ids(self) -> Set[int]:\n        \"\"\"\n        Returns the user IDs that might have been mentioned by this\n        content.  Note that because this data structure has not parsed\n        the message and does not know about escaping/code blocks, this\n        will overestimate the list of user ids.\n        \"\"\"\n        return set(self.user_id_info.keys())\n\n    def get_user_group(self, name: str) -> Optional[UserGroup]:\n        return self.user_group_name_info.get(name.lower(), None)\n\n    def get_group_members(self, user_group_id: int) -> List[int]:\n        return self.user_group_members.get(user_group_id, [])\n\ndef get_user_group_name_info(realm_id: int, user_group_names: Set[str]) -> Dict[str, UserGroup]:\n    if not user_group_names:\n        return dict()\n\n    rows = UserGroup.objects.filter(realm_id=realm_id,\n                                    name__in=user_group_names)\n    dct = {row.name.lower(): row for row in rows}\n    return dct\n\ndef get_stream_name_info(realm: Realm, stream_names: Set[str]) -> Dict[str, FullNameInfo]:\n    if not stream_names:\n        return dict()\n\n    q_list = {\n        Q(name=name)\n        for name in stream_names\n    }\n\n    rows = get_active_streams(\n        realm=realm,\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'name',\n    )\n\n    dct = {\n        row['name']: row\n        for row in rows\n    }\n    return dct\n\n\ndef do_convert(content: str,\n               message: Optional[Message]=None,\n               message_realm: Optional[Realm]=None,\n               possible_words: Optional[Set[str]]=None,\n               sent_by_bot: Optional[bool]=False,\n               translate_emoticons: Optional[bool]=False,\n               mention_data: Optional[MentionData]=None,\n               email_gateway: Optional[bool]=False,\n               no_previews: Optional[bool]=False) -> str:\n    \"\"\"Convert Markdown to HTML, with Zulip-specific settings and hacks.\"\"\"\n    # This logic is a bit convoluted, but the overall goal is to support a range of use cases:\n    # * Nothing is passed in other than content -> just run default options (e.g. for docs)\n    # * message is passed, but no realm is -> look up realm from message\n    # * message_realm is passed -> use that realm for bugdown purposes\n    if message is not None:\n        if message_realm is None:\n            message_realm = message.get_realm()\n    if message_realm is None:\n        realm_filters_key = DEFAULT_BUGDOWN_KEY\n    else:\n        realm_filters_key = message_realm.id\n\n    if message and hasattr(message, 'id') and message.id:\n        logging_message_id = 'id# ' + str(message.id)\n    else:\n        logging_message_id = 'unknown'\n\n    if message is not None and message_realm is not None:\n        if message_realm.is_zephyr_mirror_realm:\n            if message.sending_client.name == \"zephyr_mirror\":\n                # Use slightly customized Markdown processor for content\n                # delivered via zephyr_mirror\n                realm_filters_key = ZEPHYR_MIRROR_BUGDOWN_KEY\n\n    maybe_update_markdown_engines(realm_filters_key, email_gateway)\n    md_engine_key = (realm_filters_key, email_gateway)\n\n    if md_engine_key in md_engines:\n        _md_engine = md_engines[md_engine_key]\n    else:\n        if DEFAULT_BUGDOWN_KEY not in md_engines:\n            maybe_update_markdown_engines(realm_filters_key=None, email_gateway=False)\n\n        _md_engine = md_engines[(DEFAULT_BUGDOWN_KEY, email_gateway)]\n    # Reset the parser; otherwise it will get slower over time.\n    _md_engine.reset()\n\n    # Filters such as UserMentionPattern need a message.\n    _md_engine.zulip_message = message\n    _md_engine.zulip_realm = message_realm\n    _md_engine.zulip_db_data = None  # for now\n    _md_engine.image_preview_enabled = image_preview_enabled(\n        message, message_realm, no_previews)\n    _md_engine.url_embed_preview_enabled = url_embed_preview_enabled(\n        message, message_realm, no_previews)\n\n    # Pre-fetch data from the DB that is used in the bugdown thread\n    if message is not None:\n        assert message_realm is not None  # ensured above if message is not None\n        if possible_words is None:\n            possible_words = set()  # Set[str]\n\n        # Here we fetch the data structures needed to render\n        # mentions/avatars/stream mentions from the database, but only\n        # if there is syntax in the message that might use them, since\n        # the fetches are somewhat expensive and these types of syntax\n        # are uncommon enough that it's a useful optimization.\n\n        if mention_data is None:\n            mention_data = MentionData(message_realm.id, content)\n\n        emails = possible_avatar_emails(content)\n        email_info = get_email_info(message_realm.id, emails)\n\n        stream_names = possible_linked_stream_names(content)\n        stream_name_info = get_stream_name_info(message_realm, stream_names)\n\n        if content_has_emoji_syntax(content):\n            active_realm_emoji = message_realm.get_active_emoji()\n        else:\n            active_realm_emoji = dict()\n\n        _md_engine.zulip_db_data = {\n            'possible_words': possible_words,\n            'email_info': email_info,\n            'mention_data': mention_data,\n            'active_realm_emoji': active_realm_emoji,\n            'realm_uri': message_realm.uri,\n            'sent_by_bot': sent_by_bot,\n            'stream_names': stream_name_info,\n            'translate_emoticons': translate_emoticons,\n        }\n\n    try:\n        # Spend at most 5 seconds rendering; this protects the backend\n        # from being overloaded by bugs (e.g. markdown logic that is\n        # extremely inefficient in corner cases) as well as user\n        # errors (e.g. a realm filter that makes some syntax\n        # infinite-loop).\n        rendered_content = timeout(5, _md_engine.convert, content)\n\n        # Throw an exception if the content is huge; this protects the\n        # rest of the codebase from any bugs where we end up rendering\n        # something huge.\n        if len(rendered_content) > MAX_MESSAGE_LENGTH * 10:\n            raise BugdownRenderingException('Rendered content exceeds %s characters (message %s)' %\n                                            (MAX_MESSAGE_LENGTH * 10, logging_message_id))\n        return rendered_content\n    except Exception:\n        cleaned = privacy_clean_markdown(content)\n        # NOTE: Don't change this message without also changing the\n        # logic in logging_handlers.py or we can create recursive\n        # exceptions.\n        exception_message = ('Exception in Markdown parser: %sInput (sanitized) was: %s\\n (message %s)'\n                             % (traceback.format_exc(), cleaned, logging_message_id))\n        bugdown_logger.exception(exception_message)\n\n        raise BugdownRenderingException()\n    finally:\n        # These next three lines are slightly paranoid, since\n        # we always set these right before actually using the\n        # engine, but better safe then sorry.\n        _md_engine.zulip_message = None\n        _md_engine.zulip_realm = None\n        _md_engine.zulip_db_data = None\n\nbugdown_time_start = 0.0\nbugdown_total_time = 0.0\nbugdown_total_requests = 0\n\ndef get_bugdown_time() -> float:\n    return bugdown_total_time\n\ndef get_bugdown_requests() -> int:\n    return bugdown_total_requests\n\ndef bugdown_stats_start() -> None:\n    global bugdown_time_start\n    bugdown_time_start = time.time()\n\ndef bugdown_stats_finish() -> None:\n    global bugdown_total_time\n    global bugdown_total_requests\n    global bugdown_time_start\n    bugdown_total_requests += 1\n    bugdown_total_time += (time.time() - bugdown_time_start)\n\ndef convert(content: str,\n            message: Optional[Message]=None,\n            message_realm: Optional[Realm]=None,\n            possible_words: Optional[Set[str]]=None,\n            sent_by_bot: Optional[bool]=False,\n            translate_emoticons: Optional[bool]=False,\n            mention_data: Optional[MentionData]=None,\n            email_gateway: Optional[bool]=False,\n            no_previews: Optional[bool]=False) -> str:\n    bugdown_stats_start()\n    ret = do_convert(content, message, message_realm,\n                     possible_words, sent_by_bot, translate_emoticons,\n                     mention_data, email_gateway, no_previews=no_previews)\n    bugdown_stats_finish()\n    return ret\n", "# -*- coding: utf-8 -*-\n# See https://zulip.readthedocs.io/en/latest/subsystems/thumbnailing.html\nimport base64\nimport os\nimport sys\nimport urllib\nfrom django.conf import settings\nfrom libthumbor import CryptoURL\n\nZULIP_PATH = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\nsys.path.append(ZULIP_PATH)\n\nfrom zthumbor.loaders.helpers import (\n    THUMBOR_S3_TYPE, THUMBOR_LOCAL_FILE_TYPE, THUMBOR_EXTERNAL_TYPE\n)\nfrom zerver.lib.camo import get_camo_url\n\ndef is_thumbor_enabled() -> bool:\n    return settings.THUMBOR_URL != ''\n\ndef user_uploads_or_external(url: str) -> bool:\n    return url.startswith('http') or url.lstrip('/').startswith('user_uploads/')\n\ndef get_source_type(url: str) -> str:\n    if not url.startswith('/user_uploads/'):\n        return THUMBOR_EXTERNAL_TYPE\n\n    local_uploads_dir = settings.LOCAL_UPLOADS_DIR\n    if local_uploads_dir:\n        return THUMBOR_LOCAL_FILE_TYPE\n    return THUMBOR_S3_TYPE\n\ndef generate_thumbnail_url(path: str,\n                           size: str='0x0',\n                           is_camo_url: bool=False) -> str:\n    if not (path.startswith('https://') or path.startswith('http://')):\n        path = '/' + path\n\n    if not is_thumbor_enabled():\n        if path.startswith('http://'):\n            return get_camo_url(path)\n        return path\n\n    if not user_uploads_or_external(path):\n        return path\n\n    source_type = get_source_type(path)\n    safe_url = base64.urlsafe_b64encode(path.encode()).decode('utf-8')\n    image_url = '%s/source_type/%s' % (safe_url, source_type)\n    width, height = map(int, size.split('x'))\n    crypto = CryptoURL(key=settings.THUMBOR_KEY)\n\n    smart_crop_enabled = True\n    apply_filters = ['no_upscale()']\n    if is_camo_url:\n        smart_crop_enabled = False\n        apply_filters.append('quality(100)')\n    if size != '0x0':\n        apply_filters.append('sharpen(0.5,0.2,true)')\n\n    encrypted_url = crypto.generate(\n        width=width,\n        height=height,\n        smart=smart_crop_enabled,\n        filters=apply_filters,\n        image_url=image_url\n    )\n\n    if settings.THUMBOR_URL == 'http://127.0.0.1:9995':\n        # If THUMBOR_URL is the default then thumbor is hosted on same machine\n        # as the Zulip server and we should serve a relative URL.\n        # We add a /thumbor in front of the relative url because we make\n        # use of a proxy pass to redirect request internally in Nginx to 9995\n        # port where thumbor is running.\n        thumbnail_url = '/thumbor' + encrypted_url\n    else:\n        thumbnail_url = urllib.parse.urljoin(settings.THUMBOR_URL, encrypted_url)\n    return thumbnail_url\n", "# -*- coding: utf-8 -*-\nfrom django.conf import settings\n\nfrom zerver.lib.test_classes import ZulipTestCase\nfrom zerver.lib.test_helpers import (\n    use_s3_backend,\n    create_s3_buckets,\n    override_settings,\n    get_test_image_file\n)\nfrom zerver.lib.upload import upload_backend, upload_emoji_image\nfrom zerver.lib.users import get_api_key\n\nfrom io import StringIO\nimport ujson\nimport urllib\nimport base64\n\nclass ThumbnailTest(ZulipTestCase):\n\n    @use_s3_backend\n    def test_s3_source_type(self) -> None:\n        def get_file_path_urlpart(uri: str, size: str='') -> str:\n            url_in_result = 'smart/filters:no_upscale()%s/%s/source_type/s3'\n            sharpen_filter = ''\n            if size:\n                url_in_result = '/%s/%s' % (size, url_in_result)\n                sharpen_filter = ':sharpen(0.5,0.2,true)'\n            hex_uri = base64.urlsafe_b64encode(uri.encode()).decode('utf-8')\n            return url_in_result % (sharpen_filter, hex_uri)\n\n        create_s3_buckets(\n            settings.S3_AUTH_UPLOADS_BUCKET,\n            settings.S3_AVATAR_BUCKET)\n\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n\n        # Test full size image.\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test thumbnail size.\n        result = self.client_get(\"/thumbnail?url=%s&size=thumbnail\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri, '0x300')\n        self.assertIn(expected_part_url, result.url)\n\n        # Test custom emoji urls in Zulip messages.\n        user_profile = self.example_user(\"hamlet\")\n        image_file = get_test_image_file(\"img.png\")\n        file_name = \"emoji.png\"\n\n        upload_emoji_image(image_file, file_name, user_profile)\n        custom_emoji_url = upload_backend.get_emoji_url(file_name, user_profile.realm_id)\n        emoji_url_base = '/user_avatars/'\n        self.assertEqual(emoji_url_base, custom_emoji_url[:len(emoji_url_base)])\n\n        quoted_emoji_url = urllib.parse.quote(custom_emoji_url[1:], safe='')\n\n        # Test full size custom emoji image (for emoji link in messages case).\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_emoji_url))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertIn(custom_emoji_url, result.url)\n\n        # Tests the /api/v1/thumbnail api endpoint with standard API auth\n        self.logout()\n        result = self.api_get(\n            self.example_email(\"hamlet\"),\n            '/thumbnail?url=%s&size=full' %\n            (quoted_uri,))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test with another user trying to access image using thumbor.\n        self.login(self.example_email(\"iago\"))\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 403, result)\n        self.assert_in_response(\"You are not authorized to view this file.\", result)\n\n    def test_external_source_type(self) -> None:\n        def run_test_with_image_url(image_url: str) -> None:\n            # Test full size image.\n            self.login(self.example_email(\"hamlet\"))\n            quoted_url = urllib.parse.quote(image_url, safe='')\n            encoded_url = base64.urlsafe_b64encode(image_url.encode()).decode('utf-8')\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_url))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/smart/filters:no_upscale()/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test thumbnail size.\n            result = self.client_get(\"/thumbnail?url=%s&size=thumbnail\" % (quoted_url))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/0x300/smart/filters:no_upscale():sharpen(0.5,0.2,true)/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test api endpoint with standard API authentication.\n            self.logout()\n            user_profile = self.example_user(\"hamlet\")\n            result = self.api_get(user_profile.email,\n                                  \"/thumbnail?url=%s&size=thumbnail\" % (quoted_url,))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/0x300/smart/filters:no_upscale():sharpen(0.5,0.2,true)/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test api endpoint with legacy API authentication.\n            user_profile = self.example_user(\"hamlet\")\n            result = self.client_get(\"/thumbnail?url=%s&size=thumbnail&api_key=%s\" % (\n                quoted_url, get_api_key(user_profile)))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/0x300/smart/filters:no_upscale():sharpen(0.5,0.2,true)/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test a second logged-in user; they should also be able to access it\n            user_profile = self.example_user(\"iago\")\n            result = self.client_get(\"/thumbnail?url=%s&size=thumbnail&api_key=%s\" % (\n                quoted_url, get_api_key(user_profile)))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/0x300/smart/filters:no_upscale():sharpen(0.5,0.2,true)/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test with another user trying to access image using thumbor.\n            # File should be always accessible to user in case of external source\n            self.login(self.example_email(\"iago\"))\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_url))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/smart/filters:no_upscale()/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n        image_url = 'https://images.foobar.com/12345'\n        run_test_with_image_url(image_url)\n\n        image_url = 'http://images.foobar.com/12345'\n        run_test_with_image_url(image_url)\n\n    def test_local_file_type(self) -> None:\n        def get_file_path_urlpart(uri: str, size: str='') -> str:\n            url_in_result = 'smart/filters:no_upscale()%s/%s/source_type/local_file'\n            sharpen_filter = ''\n            if size:\n                url_in_result = '/%s/%s' % (size, url_in_result)\n                sharpen_filter = ':sharpen(0.5,0.2,true)'\n            hex_uri = base64.urlsafe_b64encode(uri.encode()).decode('utf-8')\n            return url_in_result % (sharpen_filter, hex_uri)\n\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        # Test full size image.\n        # We remove the forward slash infront of the `/user_uploads/` to match\n        # bugdown behaviour.\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test thumbnail size.\n        result = self.client_get(\"/thumbnail?url=%s&size=thumbnail\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri, '0x300')\n        self.assertIn(expected_part_url, result.url)\n\n        # Test with a unicode filename.\n        fp = StringIO(\"zulip!\")\n        fp.name = \"\u03bc\u03ad\u03bd\u03b5\u03b9.jpg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n\n        # We remove the forward slash infront of the `/user_uploads/` to match\n        # bugdown behaviour.\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test custom emoji urls in Zulip messages.\n        user_profile = self.example_user(\"hamlet\")\n        image_file = get_test_image_file(\"img.png\")\n        file_name = \"emoji.png\"\n\n        upload_emoji_image(image_file, file_name, user_profile)\n        custom_emoji_url = upload_backend.get_emoji_url(file_name, user_profile.realm_id)\n        emoji_url_base = '/user_avatars/'\n        self.assertEqual(emoji_url_base, custom_emoji_url[:len(emoji_url_base)])\n\n        quoted_emoji_url = urllib.parse.quote(custom_emoji_url[1:], safe='')\n\n        # Test full size custom emoji image (for emoji link in messages case).\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_emoji_url))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertIn(custom_emoji_url, result.url)\n\n        # Tests the /api/v1/thumbnail api endpoint with HTTP basic auth.\n        self.logout()\n        user_profile = self.example_user(\"hamlet\")\n        result = self.api_get(\n            self.example_email(\"hamlet\"),\n            '/thumbnail?url=%s&size=full' %\n            (quoted_uri,))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Tests the /api/v1/thumbnail api endpoint with ?api_key\n        # auth.\n        user_profile = self.example_user(\"hamlet\")\n        result = self.client_get(\n            '/thumbnail?url=%s&size=full&api_key=%s' %\n            (quoted_uri, get_api_key(user_profile)))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test with another user trying to access image using thumbor.\n        self.login(self.example_email(\"iago\"))\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 403, result)\n        self.assert_in_response(\"You are not authorized to view this file.\", result)\n\n    @override_settings(THUMBOR_URL='127.0.0.1:9995')\n    def test_with_static_files(self) -> None:\n        self.login(self.example_email(\"hamlet\"))\n        uri = '/static/images/cute/turtle.png'\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertEqual(uri, result.url)\n\n    def test_with_thumbor_disabled(self) -> None:\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n\n        with self.settings(THUMBOR_URL=''):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertEqual(uri, result.url)\n\n        uri = 'https://www.google.com/images/srpr/logo4w.png'\n        quoted_uri = urllib.parse.quote(uri, safe='')\n        with self.settings(THUMBOR_URL=''):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertEqual(uri, result.url)\n\n        uri = 'http://www.google.com/images/srpr/logo4w.png'\n        quoted_uri = urllib.parse.quote(uri, safe='')\n        with self.settings(THUMBOR_URL=''):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        base = 'https://external-content.zulipcdn.net/external_content/7b6552b60c635e41e8f6daeb36d88afc4eabde79/687474703a2f2f7777772e676f6f676c652e636f6d2f696d616765732f737270722f6c6f676f34772e706e67'\n        self.assertEqual(base, result.url)\n\n    def test_with_different_THUMBOR_URL(self) -> None:\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        hex_uri = base64.urlsafe_b64encode(uri.encode()).decode('utf-8')\n        with self.settings(THUMBOR_URL='http://test-thumborhost.com'):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        base = 'http://test-thumborhost.com/'\n        self.assertEqual(base, result.url[:len(base)])\n        expected_part_url = '/smart/filters:no_upscale()/' + hex_uri + '/source_type/local_file'\n        self.assertIn(expected_part_url, result.url)\n\n    def test_with_different_sizes(self) -> None:\n        def get_file_path_urlpart(uri: str, size: str='') -> str:\n            url_in_result = 'smart/filters:no_upscale()%s/%s/source_type/local_file'\n            sharpen_filter = ''\n            if size:\n                url_in_result = '/%s/%s' % (size, url_in_result)\n                sharpen_filter = ':sharpen(0.5,0.2,true)'\n            hex_uri = base64.urlsafe_b64encode(uri.encode()).decode('utf-8')\n            return url_in_result % (sharpen_filter, hex_uri)\n\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        # Test with size supplied as a query parameter.\n        # size=thumbnail should return a 0x300 sized image.\n        # size=full should return the original resolution image.\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        result = self.client_get(\"/thumbnail?url=%s&size=thumbnail\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri, '0x300')\n        self.assertIn(expected_part_url, result.url)\n\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test with size supplied as a query parameter where size is anything\n        # else than 'full' or 'thumbnail'. Result should be an error message.\n        result = self.client_get(\"/thumbnail?url=%s&size=480x360\" % (quoted_uri))\n        self.assertEqual(result.status_code, 403, result)\n        self.assert_in_response(\"Invalid size.\", result)\n\n        # Test with no size param supplied. In this case as well we show an\n        # error message.\n        result = self.client_get(\"/thumbnail?url=%s\" % (quoted_uri))\n        self.assertEqual(result.status_code, 400, \"Missing 'size' argument\")\n"], "fixing_code": ["# Zulip's main markdown implementation.  See docs/subsystems/markdown.md for\n# detailed documentation on our markdown syntax.\nfrom typing import (Any, Callable, Dict, Iterable, List, NamedTuple,\n                    Optional, Set, Tuple, TypeVar, Union, cast)\nfrom mypy_extensions import TypedDict\nfrom typing.re import Match, Pattern\n\nimport markdown\nimport logging\nimport traceback\nimport urllib\nimport urllib.parse\nimport re\nimport os\nimport html\nimport time\nimport functools\nimport ujson\nimport xml.etree.cElementTree as etree\nfrom xml.etree.cElementTree import Element\n\nfrom collections import deque, defaultdict\n\nimport requests\n\nfrom django.conf import settings\nfrom django.db.models import Q\n\nfrom markdown.extensions import codehilite, nl2br, tables\nfrom zerver.lib.bugdown import fenced_code\nfrom zerver.lib.bugdown.fenced_code import FENCE_RE\nfrom zerver.lib.camo import get_camo_url\nfrom zerver.lib.emoji import translate_emoticons, emoticon_regex\nfrom zerver.lib.mention import possible_mentions, \\\n    possible_user_group_mentions, extract_user_group\nfrom zerver.lib.url_encoding import encode_stream\nfrom zerver.lib.thumbnail import user_uploads_or_external\nfrom zerver.lib.timeout import timeout, TimeoutExpired\nfrom zerver.lib.cache import cache_with_key, NotFoundInCache\nfrom zerver.lib.url_preview import preview as link_preview\nfrom zerver.models import (\n    all_realm_filters,\n    get_active_streams,\n    MAX_MESSAGE_LENGTH,\n    Message,\n    Realm,\n    realm_filters_for_realm,\n    UserProfile,\n    UserGroup,\n    UserGroupMembership,\n)\nimport zerver.lib.mention as mention\nfrom zerver.lib.tex import render_tex\nfrom zerver.lib.exceptions import BugdownRenderingException\n\nReturnT = TypeVar('ReturnT')\n\ndef one_time(method: Callable[[], ReturnT]) -> Callable[[], ReturnT]:\n    '''\n        Use this decorator with extreme caution.\n        The function you wrap should have no dependency\n        on any arguments (no args, no kwargs) nor should\n        it depend on any global state.\n    '''\n    val = None\n\n    def cache_wrapper() -> ReturnT:\n        nonlocal val\n        if val is None:\n            val = method()\n        return val\n    return cache_wrapper\n\nFullNameInfo = TypedDict('FullNameInfo', {\n    'id': int,\n    'email': str,\n    'full_name': str,\n})\n\nDbData = Dict[str, Any]\n\n# Format version of the bugdown rendering; stored along with rendered\n# messages so that we can efficiently determine what needs to be re-rendered\nversion = 1\n\n_T = TypeVar('_T')\nElementStringNone = Union[Element, Optional[str]]\n\nAVATAR_REGEX = r'!avatar\\((?P<email>[^)]*)\\)'\nGRAVATAR_REGEX = r'!gravatar\\((?P<email>[^)]*)\\)'\nEMOJI_REGEX = r'(?P<syntax>:[\\w\\-\\+]+:)'\n\ndef verbose_compile(pattern: str) -> Any:\n    return re.compile(\n        \"^(.*?)%s(.*?)$\" % pattern,\n        re.DOTALL | re.UNICODE | re.VERBOSE\n    )\n\ndef normal_compile(pattern: str) -> Any:\n    return re.compile(\n        r\"^(.*?)%s(.*)$\" % pattern,\n        re.DOTALL | re.UNICODE\n    )\n\nSTREAM_LINK_REGEX = r\"\"\"\n                     (?<![^\\s'\"\\(,:<])            # Start after whitespace or specified chars\n                     \\#\\*\\*                       # and after hash sign followed by double asterisks\n                         (?P<stream_name>[^\\*]+)  # stream name can contain anything\n                     \\*\\*                         # ends by double asterisks\n                    \"\"\"\n\n@one_time\ndef get_compiled_stream_link_regex() -> Pattern:\n    return verbose_compile(STREAM_LINK_REGEX)\n\nLINK_REGEX = None  # type: Pattern\n\ndef get_web_link_regex() -> str:\n    # We create this one time, but not at startup.  So the\n    # first message rendered in any process will have some\n    # extra costs.  It's roughly 75ms to run this code, so\n    # caching the value in LINK_REGEX is super important here.\n    global LINK_REGEX\n    if LINK_REGEX is not None:\n        return LINK_REGEX\n\n    tlds = '|'.join(list_of_tlds())\n\n    # A link starts at a word boundary, and ends at space, punctuation, or end-of-input.\n    #\n    # We detect a url either by the `https?://` or by building around the TLD.\n\n    # In lieu of having a recursive regex (which python doesn't support) to match\n    # arbitrary numbers of nested matching parenthesis, we manually build a regexp that\n    # can match up to six\n    # The inner_paren_contents chunk matches the innermore non-parenthesis-holding text,\n    # and the paren_group matches text with, optionally, a matching set of parens\n    inner_paren_contents = r\"[^\\s()\\\"]*\"\n    paren_group = r\"\"\"\n                    [^\\s()\\\"]*?            # Containing characters that won't end the URL\n                    (?: \\( %s \\)           # and more characters in matched parens\n                        [^\\s()\\\"]*?        # followed by more characters\n                    )*                     # zero-or-more sets of paired parens\n                   \"\"\"\n    nested_paren_chunk = paren_group\n    for i in range(6):\n        nested_paren_chunk = nested_paren_chunk % (paren_group,)\n    nested_paren_chunk = nested_paren_chunk % (inner_paren_contents,)\n\n    file_links = r\"| (?:file://(/[^/ ]*)+/?)\" if settings.ENABLE_FILE_LINKS else r\"\"\n    REGEX = r\"\"\"\n        (?<![^\\s'\"\\(,:<])    # Start after whitespace or specified chars\n                             # (Double-negative lookbehind to allow start-of-string)\n        (?P<url>             # Main group\n            (?:(?:           # Domain part\n                https?://[\\w.:@-]+?   # If it has a protocol, anything goes.\n               |(?:                   # Or, if not, be more strict to avoid false-positives\n                    (?:[\\w-]+\\.)+     # One or more domain components, separated by dots\n                    (?:%s)            # TLDs (filled in via format from tlds-alpha-by-domain.txt)\n                )\n            )\n            (?:/             # A path, beginning with /\n                %s           # zero-to-6 sets of paired parens\n            )?)              # Path is optional\n            | (?:[\\w.-]+\\@[\\w.-]+\\.[\\w]+) # Email is separate, since it can't have a path\n            %s               # File path start with file:///, enable by setting ENABLE_FILE_LINKS=True\n            | (?:bitcoin:[13][a-km-zA-HJ-NP-Z1-9]{25,34})  # Bitcoin address pattern, see https://mokagio.github.io/tech-journal/2014/11/21/regex-bitcoin.html\n        )\n        (?=                            # URL must be followed by (not included in group)\n            [!:;\\?\\),\\.\\'\\\"\\>]*         # Optional punctuation characters\n            (?:\\Z|\\s)                  # followed by whitespace or end of string\n        )\n        \"\"\" % (tlds, nested_paren_chunk, file_links)\n    LINK_REGEX = verbose_compile(REGEX)\n    return LINK_REGEX\n\ndef clear_state_for_testing() -> None:\n    # The link regex never changes in production, but our tests\n    # try out both sides of ENABLE_FILE_LINKS, so we need\n    # a way to clear it.\n    global LINK_REGEX\n    LINK_REGEX = None\n\nbugdown_logger = logging.getLogger()\n\ndef rewrite_local_links_to_relative(db_data: Optional[DbData], link: str) -> str:\n    \"\"\" If the link points to a local destination we can just switch to that\n    instead of opening a new tab. \"\"\"\n\n    if db_data:\n        realm_uri_prefix = db_data['realm_uri'] + \"/\"\n        if link.startswith(realm_uri_prefix):\n            # +1 to skip the `/` before the hash link.\n            return link[len(realm_uri_prefix):]\n\n    return link\n\ndef url_embed_preview_enabled(message: Optional[Message]=None,\n                              realm: Optional[Realm]=None,\n                              no_previews: Optional[bool]=False) -> bool:\n    if not settings.INLINE_URL_EMBED_PREVIEW:\n        return False\n\n    if no_previews:\n        return False\n\n    if realm is None:\n        if message is not None:\n            realm = message.get_realm()\n\n    if realm is None:\n        # realm can be None for odd use cases\n        # like generating documentation or running\n        # test code\n        return True\n\n    return realm.inline_url_embed_preview\n\ndef image_preview_enabled(message: Optional[Message]=None,\n                          realm: Optional[Realm]=None,\n                          no_previews: Optional[bool]=False) -> bool:\n    if not settings.INLINE_IMAGE_PREVIEW:\n        return False\n\n    if no_previews:\n        return False\n\n    if realm is None:\n        if message is not None:\n            realm = message.get_realm()\n\n    if realm is None:\n        # realm can be None for odd use cases\n        # like generating documentation or running\n        # test code\n        return True\n\n    return realm.inline_image_preview\n\ndef list_of_tlds() -> List[str]:\n    # HACK we manually blacklist a few domains\n    blacklist = ['PY\\n', \"MD\\n\"]\n\n    # tlds-alpha-by-domain.txt comes from http://data.iana.org/TLD/tlds-alpha-by-domain.txt\n    tlds_file = os.path.join(os.path.dirname(__file__), 'tlds-alpha-by-domain.txt')\n    tlds = [tld.lower().strip() for tld in open(tlds_file, 'r')\n            if tld not in blacklist and not tld[0].startswith('#')]\n    tlds.sort(key=len, reverse=True)\n    return tlds\n\ndef walk_tree(root: Element,\n              processor: Callable[[Element], Optional[_T]],\n              stop_after_first: bool=False) -> List[_T]:\n    results = []\n    queue = deque([root])\n\n    while queue:\n        currElement = queue.popleft()\n        for child in currElement.getchildren():\n            if child.getchildren():\n                queue.append(child)\n\n            result = processor(child)\n            if result is not None:\n                results.append(result)\n                if stop_after_first:\n                    return results\n\n    return results\n\nElementFamily = NamedTuple('ElementFamily', [\n    ('grandparent', Optional[Element]),\n    ('parent', Element),\n    ('child', Element)\n])\n\nResultWithFamily = NamedTuple('ResultWithFamily', [\n    ('family', ElementFamily),\n    ('result', Any)\n])\n\nElementPair = NamedTuple('ElementPair', [\n    ('parent', Optional[Element]),\n    ('value', Element)\n])\n\ndef walk_tree_with_family(root: Element,\n                          processor: Callable[[Element], Optional[_T]]\n                          ) -> List[ResultWithFamily]:\n    results = []\n\n    queue = deque([ElementPair(parent=None, value=root)])\n    while queue:\n        currElementPair = queue.popleft()\n        for child in currElementPair.value.getchildren():\n            if child.getchildren():\n                queue.append(ElementPair(parent=currElementPair, value=child))  # type: ignore  # Lack of Deque support in typing module for Python 3.4.3\n            result = processor(child)\n            if result is not None:\n                if currElementPair.parent is not None:\n                    grandparent_element = cast(ElementPair, currElementPair.parent)\n                    grandparent = grandparent_element.value\n                else:\n                    grandparent = None\n                family = ElementFamily(\n                    grandparent=grandparent,\n                    parent=currElementPair.value,\n                    child=child\n                )\n\n                results.append(ResultWithFamily(\n                    family=family,\n                    result=result\n                ))\n\n    return results\n\n# height is not actually used\ndef add_a(\n        root: Element,\n        url: str,\n        link: str,\n        title: Optional[str]=None,\n        desc: Optional[str]=None,\n        class_attr: str=\"message_inline_image\",\n        data_id: Optional[str]=None,\n        insertion_index: Optional[int]=None,\n        already_thumbnailed: Optional[bool]=False\n) -> None:\n    title = title if title is not None else url_filename(link)\n    title = title if title else \"\"\n    desc = desc if desc is not None else \"\"\n\n    if insertion_index is not None:\n        div = markdown.util.etree.Element(\"div\")\n        root.insert(insertion_index, div)\n    else:\n        div = markdown.util.etree.SubElement(root, \"div\")\n\n    div.set(\"class\", class_attr)\n    a = markdown.util.etree.SubElement(div, \"a\")\n    a.set(\"href\", link)\n    a.set(\"target\", \"_blank\")\n    a.set(\"title\", title)\n    if data_id is not None:\n        a.set(\"data-id\", data_id)\n    img = markdown.util.etree.SubElement(a, \"img\")\n    if settings.THUMBNAIL_IMAGES and (not already_thumbnailed) and user_uploads_or_external(url):\n        # See docs/thumbnailing.md for some high-level documentation.\n        #\n        # We strip leading '/' from relative URLs here to ensure\n        # consistency in what gets passed to /thumbnail\n        url = url.lstrip('/')\n        img.set(\"src\", \"/thumbnail?url={0}&size=thumbnail\".format(\n            urllib.parse.quote(url, safe='')\n        ))\n        img.set('data-src-fullsize', \"/thumbnail?url={0}&size=full\".format(\n            urllib.parse.quote(url, safe='')\n        ))\n    else:\n        img.set(\"src\", url)\n\n    if class_attr == \"message_inline_ref\":\n        summary_div = markdown.util.etree.SubElement(div, \"div\")\n        title_div = markdown.util.etree.SubElement(summary_div, \"div\")\n        title_div.set(\"class\", \"message_inline_image_title\")\n        title_div.text = title\n        desc_div = markdown.util.etree.SubElement(summary_div, \"desc\")\n        desc_div.set(\"class\", \"message_inline_image_desc\")\n\ndef add_embed(root: Element, link: str, extracted_data: Dict[str, Any]) -> None:\n    container = markdown.util.etree.SubElement(root, \"div\")\n    container.set(\"class\", \"message_embed\")\n\n    img_link = extracted_data.get('image')\n    if img_link:\n        parsed_img_link = urllib.parse.urlparse(img_link)\n        # Append domain where relative img_link url is given\n        if not parsed_img_link.netloc:\n            parsed_url = urllib.parse.urlparse(link)\n            domain = '{url.scheme}://{url.netloc}/'.format(url=parsed_url)\n            img_link = urllib.parse.urljoin(domain, img_link)\n        img = markdown.util.etree.SubElement(container, \"a\")\n        img.set(\"style\", \"background-image: url(\" + img_link + \")\")\n        img.set(\"href\", link)\n        img.set(\"target\", \"_blank\")\n        img.set(\"class\", \"message_embed_image\")\n\n    data_container = markdown.util.etree.SubElement(container, \"div\")\n    data_container.set(\"class\", \"data-container\")\n\n    title = extracted_data.get('title')\n    if title:\n        title_elm = markdown.util.etree.SubElement(data_container, \"div\")\n        title_elm.set(\"class\", \"message_embed_title\")\n        a = markdown.util.etree.SubElement(title_elm, \"a\")\n        a.set(\"href\", link)\n        a.set(\"target\", \"_blank\")\n        a.set(\"title\", title)\n        a.text = title\n    description = extracted_data.get('description')\n    if description:\n        description_elm = markdown.util.etree.SubElement(data_container, \"div\")\n        description_elm.set(\"class\", \"message_embed_description\")\n        description_elm.text = description\n\n@cache_with_key(lambda tweet_id: tweet_id, cache_name=\"database\", with_statsd_key=\"tweet_data\")\ndef fetch_tweet_data(tweet_id: str) -> Optional[Dict[str, Any]]:\n    if settings.TEST_SUITE:\n        from . import testing_mocks\n        res = testing_mocks.twitter(tweet_id)\n    else:\n        creds = {\n            'consumer_key': settings.TWITTER_CONSUMER_KEY,\n            'consumer_secret': settings.TWITTER_CONSUMER_SECRET,\n            'access_token_key': settings.TWITTER_ACCESS_TOKEN_KEY,\n            'access_token_secret': settings.TWITTER_ACCESS_TOKEN_SECRET,\n        }\n        if not all(creds.values()):\n            return None\n\n        # We lazily import twitter here because its import process is\n        # surprisingly slow, and doing so has a significant impact on\n        # the startup performance of `manage.py` commands.\n        import twitter\n\n        try:\n            api = twitter.Api(tweet_mode='extended', **creds)\n            # Sometimes Twitter hangs on responses.  Timing out here\n            # will cause the Tweet to go through as-is with no inline\n            # preview, rather than having the message be rejected\n            # entirely. This timeout needs to be less than our overall\n            # formatting timeout.\n            tweet = timeout(3, api.GetStatus, tweet_id)\n            res = tweet.AsDict()\n        except AttributeError:\n            bugdown_logger.error('Unable to load twitter api, you may have the wrong '\n                                 'library installed, see https://github.com/zulip/zulip/issues/86')\n            return None\n        except TimeoutExpired:\n            # We'd like to try again later and not cache the bad result,\n            # so we need to re-raise the exception (just as though\n            # we were being rate-limited)\n            raise\n        except twitter.TwitterError as e:\n            t = e.args[0]\n            if len(t) == 1 and ('code' in t[0]) and (t[0]['code'] == 34):\n                # Code 34 means that the message doesn't exist; return\n                # None so that we will cache the error\n                return None\n            elif len(t) == 1 and ('code' in t[0]) and (t[0]['code'] == 88 or\n                                                       t[0]['code'] == 130):\n                # Code 88 means that we were rate-limited and 130\n                # means Twitter is having capacity issues; either way\n                # just raise the error so we don't cache None and will\n                # try again later.\n                raise\n            else:\n                # It's not clear what to do in cases of other errors,\n                # but for now it seems reasonable to log at error\n                # level (so that we get notified), but then cache the\n                # failure to proceed with our usual work\n                bugdown_logger.error(traceback.format_exc())\n                return None\n    return res\n\nHEAD_START_RE = re.compile('^head[ >]')\nHEAD_END_RE = re.compile('^/head[ >]')\nMETA_START_RE = re.compile('^meta[ >]')\nMETA_END_RE = re.compile('^/meta[ >]')\n\ndef fetch_open_graph_image(url: str) -> Optional[Dict[str, Any]]:\n    in_head = False\n    # HTML will auto close meta tags, when we start the next tag add\n    # a closing tag if it has not been closed yet.\n    last_closed = True\n    head = []\n    # TODO: What if response content is huge? Should we get headers first?\n    try:\n        content = requests.get(url, timeout=1).text\n    except Exception:\n        return None\n    # Extract the head and meta tags\n    # All meta tags are self closing, have no children or are closed\n    # automatically.\n    for part in content.split('<'):\n        if not in_head and HEAD_START_RE.match(part):\n            # Started the head node output it to have a document root\n            in_head = True\n            head.append('<head>')\n        elif in_head and HEAD_END_RE.match(part):\n            # Found the end of the head close any remaining tag then stop\n            # processing\n            in_head = False\n            if not last_closed:\n                last_closed = True\n                head.append('</meta>')\n            head.append('</head>')\n            break\n\n        elif in_head and META_START_RE.match(part):\n            # Found a meta node copy it\n            if not last_closed:\n                head.append('</meta>')\n                last_closed = True\n            head.append('<')\n            head.append(part)\n            if '/>' not in part:\n                last_closed = False\n\n        elif in_head and META_END_RE.match(part):\n            # End of a meta node just copy it to close the tag\n            head.append('<')\n            head.append(part)\n            last_closed = True\n\n    try:\n        doc = etree.fromstring(''.join(head))\n    except etree.ParseError:\n        return None\n    og_image = doc.find('meta[@property=\"og:image\"]')\n    og_title = doc.find('meta[@property=\"og:title\"]')\n    og_desc = doc.find('meta[@property=\"og:description\"]')\n    title = None\n    desc = None\n    if og_image is not None:\n        image = og_image.get('content')\n    else:\n        return None\n    if og_title is not None:\n        title = og_title.get('content')\n    if og_desc is not None:\n        desc = og_desc.get('content')\n    return {'image': image, 'title': title, 'desc': desc}\n\ndef get_tweet_id(url: str) -> Optional[str]:\n    parsed_url = urllib.parse.urlparse(url)\n    if not (parsed_url.netloc == 'twitter.com' or parsed_url.netloc.endswith('.twitter.com')):\n        return None\n    to_match = parsed_url.path\n    # In old-style twitter.com/#!/wdaher/status/1231241234-style URLs,\n    # we need to look at the fragment instead\n    if parsed_url.path == '/' and len(parsed_url.fragment) > 5:\n        to_match = parsed_url.fragment\n\n    tweet_id_match = re.match(r'^!?/.*?/status(es)?/(?P<tweetid>\\d{10,30})(/photo/[0-9])?/?$', to_match)\n    if not tweet_id_match:\n        return None\n    return tweet_id_match.group(\"tweetid\")\n\nclass InlineHttpsProcessor(markdown.treeprocessors.Treeprocessor):\n    def run(self, root: Element) -> None:\n        # Get all URLs from the blob\n        found_imgs = walk_tree(root, lambda e: e if e.tag == \"img\" else None)\n        for img in found_imgs:\n            url = img.get(\"src\")\n            if urllib.parse.urlsplit(url).scheme != \"http\":\n                # Don't rewrite images on our own site (e.g. emoji).\n                continue\n            img.set(\"src\", get_camo_url(url))\n\nclass BacktickPattern(markdown.inlinepatterns.Pattern):\n    \"\"\" Return a `<code>` element containing the matching text. \"\"\"\n    def __init__(self, pattern: str) -> None:\n        markdown.inlinepatterns.Pattern.__init__(self, pattern)\n        self.ESCAPED_BSLASH = '%s%s%s' % (markdown.util.STX, ord('\\\\'), markdown.util.ETX)\n        self.tag = 'code'\n\n    def handleMatch(self, m: Match[str]) -> Union[str, Element]:\n        if m.group(4):\n            el = markdown.util.etree.Element(self.tag)\n            # Modified to not strip whitespace\n            el.text = markdown.util.AtomicString(m.group(4))\n            return el\n        else:\n            return m.group(2).replace('\\\\\\\\', self.ESCAPED_BSLASH)\n\nclass InlineInterestingLinkProcessor(markdown.treeprocessors.Treeprocessor):\n    TWITTER_MAX_IMAGE_HEIGHT = 400\n    TWITTER_MAX_TO_PREVIEW = 3\n    INLINE_PREVIEW_LIMIT_PER_MESSAGE = 5\n\n    def __init__(self, md: markdown.Markdown) -> None:\n        markdown.treeprocessors.Treeprocessor.__init__(self, md)\n\n    def get_actual_image_url(self, url: str) -> str:\n        # Add specific per-site cases to convert image-preview urls to image urls.\n        # See https://github.com/zulip/zulip/issues/4658 for more information\n        parsed_url = urllib.parse.urlparse(url)\n        if (parsed_url.netloc == 'github.com' or parsed_url.netloc.endswith('.github.com')):\n            # https://github.com/zulip/zulip/blob/master/static/images/logo/zulip-icon-128x128.png ->\n            # https://raw.githubusercontent.com/zulip/zulip/master/static/images/logo/zulip-icon-128x128.png\n            split_path = parsed_url.path.split('/')\n            if len(split_path) > 3 and split_path[3] == \"blob\":\n                return urllib.parse.urljoin('https://raw.githubusercontent.com',\n                                            '/'.join(split_path[0:3] + split_path[4:]))\n\n        return url\n\n    def is_image(self, url: str) -> bool:\n        if not self.markdown.image_preview_enabled:\n            return False\n        parsed_url = urllib.parse.urlparse(url)\n        # List from http://support.google.com/chromeos/bin/answer.py?hl=en&answer=183093\n        for ext in [\".bmp\", \".gif\", \".jpg\", \"jpeg\", \".png\", \".webp\"]:\n            if parsed_url.path.lower().endswith(ext):\n                return True\n        return False\n\n    def dropbox_image(self, url: str) -> Optional[Dict[str, Any]]:\n        # TODO: The returned Dict could possibly be a TypedDict in future.\n        parsed_url = urllib.parse.urlparse(url)\n        if (parsed_url.netloc == 'dropbox.com' or parsed_url.netloc.endswith('.dropbox.com')):\n            is_album = parsed_url.path.startswith('/sc/') or parsed_url.path.startswith('/photos/')\n            # Only allow preview Dropbox shared links\n            if not (parsed_url.path.startswith('/s/') or\n                    parsed_url.path.startswith('/sh/') or\n                    is_album):\n                return None\n\n            # Try to retrieve open graph protocol info for a preview\n            # This might be redundant right now for shared links for images.\n            # However, we might want to make use of title and description\n            # in the future. If the actual image is too big, we might also\n            # want to use the open graph image.\n            image_info = fetch_open_graph_image(url)\n\n            is_image = is_album or self.is_image(url)\n\n            # If it is from an album or not an actual image file,\n            # just use open graph image.\n            if is_album or not is_image:\n                # Failed to follow link to find an image preview so\n                # use placeholder image and guess filename\n                if image_info is None:\n                    return None\n\n                image_info[\"is_image\"] = is_image\n                return image_info\n\n            # Otherwise, try to retrieve the actual image.\n            # This is because open graph image from Dropbox may have padding\n            # and gifs do not work.\n            # TODO: What if image is huge? Should we get headers first?\n            if image_info is None:\n                image_info = dict()\n            image_info['is_image'] = True\n            parsed_url_list = list(parsed_url)\n            parsed_url_list[4] = \"dl=1\"  # Replaces query\n            image_info[\"image\"] = urllib.parse.urlunparse(parsed_url_list)\n\n            return image_info\n        return None\n\n    def youtube_id(self, url: str) -> Optional[str]:\n        if not self.markdown.image_preview_enabled:\n            return None\n        # Youtube video id extraction regular expression from http://pastebin.com/KyKAFv1s\n        # Slightly modified to support URLs of the form youtu.be/<id>\n        # If it matches, match.group(2) is the video id.\n        schema_re = r'(?:https?://)'\n        host_re = r'(?:youtu\\.be/|(?:\\w+\\.)?youtube(?:-nocookie)?\\.com/)'\n        param_re = r'(?:(?:(?:v|embed)/)|(?:(?:watch(?:_popup)?(?:\\.php)?)?(?:\\?|#!?)(?:.+&)?v=))'\n        id_re = r'([0-9A-Za-z_-]+)'\n        youtube_re = r'^({schema_re}?{host_re}{param_re}?)?{id_re}(?(1).+)?$'\n        youtube_re = youtube_re.format(schema_re=schema_re, host_re=host_re, id_re=id_re, param_re=param_re)\n        match = re.match(youtube_re, url)\n        if match is None:\n            return None\n        return match.group(2)\n\n    def youtube_image(self, url: str) -> Optional[str]:\n        yt_id = self.youtube_id(url)\n\n        if yt_id is not None:\n            return \"https://i.ytimg.com/vi/%s/default.jpg\" % (yt_id,)\n        return None\n\n    def vimeo_id(self, url: str) -> Optional[str]:\n        if not self.markdown.image_preview_enabled:\n            return None\n        #(http|https)?:\\/\\/(www\\.)?vimeo.com\\/(?:channels\\/(?:\\w+\\/)?|groups\\/([^\\/]*)\\/videos\\/|)(\\d+)(?:|\\/\\?)\n        # If it matches, match.group('id') is the video id.\n\n        vimeo_re = r'^((http|https)?:\\/\\/(www\\.)?vimeo.com\\/' + \\\n                   r'(?:channels\\/(?:\\w+\\/)?|groups\\/' + \\\n                   r'([^\\/]*)\\/videos\\/|)(\\d+)(?:|\\/\\?))$'\n        match = re.match(vimeo_re, url)\n        if match is None:\n            return None\n        return match.group(5)\n\n    def vimeo_title(self, extracted_data: Dict[str, Any]) -> Optional[str]:\n        title = extracted_data.get(\"title\")\n        if title is not None:\n            return \"Vimeo - {}\".format(title)\n        return None\n\n    def twitter_text(self, text: str,\n                     urls: List[Dict[str, str]],\n                     user_mentions: List[Dict[str, Any]],\n                     media: List[Dict[str, Any]]) -> Element:\n        \"\"\"\n        Use data from the twitter API to turn links, mentions and media into A\n        tags. Also convert unicode emojis to images.\n\n        This works by using the urls, user_mentions and media data from\n        the twitter API and searching for unicode emojis in the text using\n        `unicode_emoji_regex`.\n\n        The first step is finding the locations of the URLs, mentions, media and\n        emoji in the text. For each match we build a dictionary with type, the start\n        location, end location, the URL to link to, and the text(codepoint and title\n        in case of emojis) to be used in the link(image in case of emojis).\n\n        Next we sort the matches by start location. And for each we add the\n        text from the end of the last link to the start of the current link to\n        the output. The text needs to added to the text attribute of the first\n        node (the P tag) or the tail the last link created.\n\n        Finally we add any remaining text to the last node.\n        \"\"\"\n\n        to_process = []  # type: List[Dict[str, Any]]\n        # Build dicts for URLs\n        for url_data in urls:\n            short_url = url_data[\"url\"]\n            full_url = url_data[\"expanded_url\"]\n            for match in re.finditer(re.escape(short_url), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'url',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': short_url,\n                    'text': full_url,\n                })\n        # Build dicts for mentions\n        for user_mention in user_mentions:\n            screen_name = user_mention['screen_name']\n            mention_string = '@' + screen_name\n            for match in re.finditer(re.escape(mention_string), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'mention',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': 'https://twitter.com/' + urllib.parse.quote(screen_name),\n                    'text': mention_string,\n                })\n        # Build dicts for media\n        for media_item in media:\n            short_url = media_item['url']\n            expanded_url = media_item['expanded_url']\n            for match in re.finditer(re.escape(short_url), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'media',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': short_url,\n                    'text': expanded_url,\n                })\n        # Build dicts for emojis\n        for match in re.finditer(unicode_emoji_regex, text, re.IGNORECASE):\n            orig_syntax = match.group('syntax')\n            codepoint = unicode_emoji_to_codepoint(orig_syntax)\n            if codepoint in codepoint_to_name:\n                display_string = ':' + codepoint_to_name[codepoint] + ':'\n                to_process.append({\n                    'type': 'emoji',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'codepoint': codepoint,\n                    'title': display_string,\n                })\n\n        to_process.sort(key=lambda x: x['start'])\n        p = current_node = markdown.util.etree.Element('p')\n\n        def set_text(text: str) -> None:\n            \"\"\"\n            Helper to set the text or the tail of the current_node\n            \"\"\"\n            if current_node == p:\n                current_node.text = text\n            else:\n                current_node.tail = text\n\n        db_data = self.markdown.zulip_db_data\n        current_index = 0\n        for item in to_process:\n            # The text we want to link starts in already linked text skip it\n            if item['start'] < current_index:\n                continue\n            # Add text from the end of last link to the start of the current\n            # link\n            set_text(text[current_index:item['start']])\n            current_index = item['end']\n            if item['type'] != 'emoji':\n                current_node = elem = url_to_a(db_data, item['url'], item['text'])\n            else:\n                current_node = elem = make_emoji(item['codepoint'], item['title'])\n            p.append(elem)\n\n        # Add any unused text\n        set_text(text[current_index:])\n        return p\n\n    def twitter_link(self, url: str) -> Optional[Element]:\n        tweet_id = get_tweet_id(url)\n\n        if tweet_id is None:\n            return None\n\n        try:\n            res = fetch_tweet_data(tweet_id)\n            if res is None:\n                return None\n            user = res['user']  # type: Dict[str, Any]\n            tweet = markdown.util.etree.Element(\"div\")\n            tweet.set(\"class\", \"twitter-tweet\")\n            img_a = markdown.util.etree.SubElement(tweet, 'a')\n            img_a.set(\"href\", url)\n            img_a.set(\"target\", \"_blank\")\n            profile_img = markdown.util.etree.SubElement(img_a, 'img')\n            profile_img.set('class', 'twitter-avatar')\n            # For some reason, for, e.g. tweet 285072525413724161,\n            # python-twitter does not give us a\n            # profile_image_url_https, but instead puts that URL in\n            # profile_image_url. So use _https if available, but fall\n            # back gracefully.\n            image_url = user.get('profile_image_url_https', user['profile_image_url'])\n            profile_img.set('src', image_url)\n\n            text = html.unescape(res['full_text'])\n            urls = res.get('urls', [])\n            user_mentions = res.get('user_mentions', [])\n            media = res.get('media', [])  # type: List[Dict[str, Any]]\n            p = self.twitter_text(text, urls, user_mentions, media)\n            tweet.append(p)\n\n            span = markdown.util.etree.SubElement(tweet, 'span')\n            span.text = \"- %s (@%s)\" % (user['name'], user['screen_name'])\n\n            # Add image previews\n            for media_item in media:\n                # Only photos have a preview image\n                if media_item['type'] != 'photo':\n                    continue\n\n                # Find the image size that is smaller than\n                # TWITTER_MAX_IMAGE_HEIGHT px tall or the smallest\n                size_name_tuples = list(media_item['sizes'].items())\n                size_name_tuples.sort(reverse=True,\n                                      key=lambda x: x[1]['h'])\n                for size_name, size in size_name_tuples:\n                    if size['h'] < self.TWITTER_MAX_IMAGE_HEIGHT:\n                        break\n\n                media_url = '%s:%s' % (media_item['media_url_https'], size_name)\n                img_div = markdown.util.etree.SubElement(tweet, 'div')\n                img_div.set('class', 'twitter-image')\n                img_a = markdown.util.etree.SubElement(img_div, 'a')\n                img_a.set('href', media_item['url'])\n                img_a.set('target', '_blank')\n                img_a.set('title', media_item['url'])\n                img = markdown.util.etree.SubElement(img_a, 'img')\n                img.set('src', media_url)\n\n            return tweet\n        except Exception:\n            # We put this in its own try-except because it requires external\n            # connectivity. If Twitter flakes out, we don't want to not-render\n            # the entire message; we just want to not show the Twitter preview.\n            bugdown_logger.warning(traceback.format_exc())\n            return None\n\n    def get_url_data(self, e: Element) -> Optional[Tuple[str, str]]:\n        if e.tag == \"a\":\n            if e.text is not None:\n                return (e.get(\"href\"), e.text)\n            return (e.get(\"href\"), e.get(\"href\"))\n        return None\n\n    def handle_image_inlining(self, root: Element, found_url: ResultWithFamily) -> None:\n        grandparent = found_url.family.grandparent\n        parent = found_url.family.parent\n        ahref_element = found_url.family.child\n        (url, text) = found_url.result\n        actual_url = self.get_actual_image_url(url)\n\n        # url != text usually implies a named link, which we opt not to remove\n        url_eq_text = (url == text)\n\n        if parent.tag == 'li':\n            add_a(parent, self.get_actual_image_url(url), url, title=text)\n            if not parent.text and not ahref_element.tail and url_eq_text:\n                parent.remove(ahref_element)\n\n        elif parent.tag == 'p':\n            parent_index = None\n            for index, uncle in enumerate(grandparent.getchildren()):\n                if uncle is parent:\n                    parent_index = index\n                    break\n\n            if parent_index is not None:\n                ins_index = self.find_proper_insertion_index(grandparent, parent, parent_index)\n                add_a(grandparent, actual_url, url, title=text, insertion_index=ins_index)\n\n            else:\n                # We're not inserting after parent, since parent not found.\n                # Append to end of list of grandparent's children as normal\n                add_a(grandparent, actual_url, url, title=text)\n\n            # If link is alone in a paragraph, delete paragraph containing it\n            if (len(parent.getchildren()) == 1 and\n                    (not parent.text or parent.text == \"\\n\") and\n                    not ahref_element.tail and\n                    url_eq_text):\n                grandparent.remove(parent)\n\n        else:\n            # If none of the above criteria match, fall back to old behavior\n            add_a(root, actual_url, url, title=text)\n\n    def find_proper_insertion_index(self, grandparent: Element, parent: Element,\n                                    parent_index_in_grandparent: int) -> int:\n        # If there are several inline images from same paragraph, ensure that\n        # they are in correct (and not opposite) order by inserting after last\n        # inline image from paragraph 'parent'\n\n        uncles = grandparent.getchildren()\n        parent_links = [ele.attrib['href'] for ele in parent.iter(tag=\"a\")]\n        insertion_index = parent_index_in_grandparent\n\n        while True:\n            insertion_index += 1\n            if insertion_index >= len(uncles):\n                return insertion_index\n\n            uncle = uncles[insertion_index]\n            inline_image_classes = ['message_inline_image', 'message_inline_ref']\n            if (\n                uncle.tag != 'div' or\n                'class' not in uncle.keys() or\n                uncle.attrib['class'] not in inline_image_classes\n            ):\n                return insertion_index\n\n            uncle_link = list(uncle.iter(tag=\"a\"))[0].attrib['href']\n            if uncle_link not in parent_links:\n                return insertion_index\n\n    def is_absolute_url(self, url: str) -> bool:\n        return bool(urllib.parse.urlparse(url).netloc)\n\n    def run(self, root: Element) -> None:\n        # Get all URLs from the blob\n        found_urls = walk_tree_with_family(root, self.get_url_data)\n        if len(found_urls) == 0 or len(found_urls) > self.INLINE_PREVIEW_LIMIT_PER_MESSAGE:\n            return\n\n        rendered_tweet_count = 0\n\n        for found_url in found_urls:\n            (url, text) = found_url.result\n            if not self.is_absolute_url(url):\n                if self.is_image(url):\n                    self.handle_image_inlining(root, found_url)\n                # We don't have a strong use case for doing url preview for relative links.\n                continue\n\n            dropbox_image = self.dropbox_image(url)\n            if dropbox_image is not None:\n                class_attr = \"message_inline_ref\"\n                is_image = dropbox_image[\"is_image\"]\n                if is_image:\n                    class_attr = \"message_inline_image\"\n                    # Not making use of title and description of images\n                add_a(root, dropbox_image['image'], url,\n                      title=dropbox_image.get('title', \"\"),\n                      desc=dropbox_image.get('desc', \"\"),\n                      class_attr=class_attr,\n                      already_thumbnailed=True)\n                continue\n            if self.is_image(url):\n                self.handle_image_inlining(root, found_url)\n                continue\n            if get_tweet_id(url) is not None:\n                if rendered_tweet_count >= self.TWITTER_MAX_TO_PREVIEW:\n                    # Only render at most one tweet per message\n                    continue\n                twitter_data = self.twitter_link(url)\n                if twitter_data is None:\n                    # This link is not actually a tweet known to twitter\n                    continue\n                rendered_tweet_count += 1\n                div = markdown.util.etree.SubElement(root, \"div\")\n                div.set(\"class\", \"inline-preview-twitter\")\n                div.insert(0, twitter_data)\n                continue\n            youtube = self.youtube_image(url)\n            if youtube is not None:\n                yt_id = self.youtube_id(url)\n                add_a(root, youtube, url, None, None,\n                      \"youtube-video message_inline_image\",\n                      yt_id, already_thumbnailed=True)\n                continue\n\n            db_data = self.markdown.zulip_db_data\n            if db_data and db_data['sent_by_bot']:\n                continue\n\n            if not self.markdown.url_embed_preview_enabled:\n                continue\n\n            try:\n                extracted_data = link_preview.link_embed_data_from_cache(url)\n            except NotFoundInCache:\n                self.markdown.zulip_message.links_for_preview.add(url)\n                continue\n            if extracted_data:\n                vm_id = self.vimeo_id(url)\n                if vm_id is not None:\n                    vimeo_image = extracted_data.get('image')\n                    vimeo_title = self.vimeo_title(extracted_data)\n                    if vimeo_image is not None:\n                        add_a(root, vimeo_image, url, vimeo_title,\n                              None, \"vimeo-video message_inline_image\", vm_id,\n                              already_thumbnailed=True)\n                    if vimeo_title is not None:\n                        found_url.family.child.text = vimeo_title\n                else:\n                    add_embed(root, url, extracted_data)\n\nclass Avatar(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        img = markdown.util.etree.Element('img')\n        email_address = match.group('email')\n        email = email_address.strip().lower()\n        profile_id = None\n\n        db_data = self.markdown.zulip_db_data\n        if db_data is not None:\n            user_dict = db_data['email_info'].get(email)\n            if user_dict is not None:\n                profile_id = user_dict['id']\n\n        img.set('class', 'message_body_gravatar')\n        img.set('src', '/avatar/{0}?s=30'.format(profile_id or email))\n        img.set('title', email)\n        img.set('alt', email)\n        return img\n\ndef possible_avatar_emails(content: str) -> Set[str]:\n    emails = set()\n    for REGEX in [AVATAR_REGEX, GRAVATAR_REGEX]:\n        matches = re.findall(REGEX, content)\n        for email in matches:\n            if email:\n                emails.add(email)\n\n    return emails\n\npath_to_name_to_codepoint = os.path.join(settings.STATIC_ROOT,\n                                         \"generated\", \"emoji\", \"name_to_codepoint.json\")\nwith open(path_to_name_to_codepoint) as name_to_codepoint_file:\n    name_to_codepoint = ujson.load(name_to_codepoint_file)\n\npath_to_codepoint_to_name = os.path.join(settings.STATIC_ROOT,\n                                         \"generated\", \"emoji\", \"codepoint_to_name.json\")\nwith open(path_to_codepoint_to_name) as codepoint_to_name_file:\n    codepoint_to_name = ujson.load(codepoint_to_name_file)\n\n# All of our emojis(non ZWJ sequences) belong to one of these unicode blocks:\n# \\U0001f100-\\U0001f1ff - Enclosed Alphanumeric Supplement\n# \\U0001f200-\\U0001f2ff - Enclosed Ideographic Supplement\n# \\U0001f300-\\U0001f5ff - Miscellaneous Symbols and Pictographs\n# \\U0001f600-\\U0001f64f - Emoticons (Emoji)\n# \\U0001f680-\\U0001f6ff - Transport and Map Symbols\n# \\U0001f900-\\U0001f9ff - Supplemental Symbols and Pictographs\n# \\u2000-\\u206f         - General Punctuation\n# \\u2300-\\u23ff         - Miscellaneous Technical\n# \\u2400-\\u243f         - Control Pictures\n# \\u2440-\\u245f         - Optical Character Recognition\n# \\u2460-\\u24ff         - Enclosed Alphanumerics\n# \\u2500-\\u257f         - Box Drawing\n# \\u2580-\\u259f         - Block Elements\n# \\u25a0-\\u25ff         - Geometric Shapes\n# \\u2600-\\u26ff         - Miscellaneous Symbols\n# \\u2700-\\u27bf         - Dingbats\n# \\u2900-\\u297f         - Supplemental Arrows-B\n# \\u2b00-\\u2bff         - Miscellaneous Symbols and Arrows\n# \\u3000-\\u303f         - CJK Symbols and Punctuation\n# \\u3200-\\u32ff         - Enclosed CJK Letters and Months\nunicode_emoji_regex = '(?P<syntax>['\\\n    '\\U0001F100-\\U0001F64F'    \\\n    '\\U0001F680-\\U0001F6FF'    \\\n    '\\U0001F900-\\U0001F9FF'    \\\n    '\\u2000-\\u206F'            \\\n    '\\u2300-\\u27BF'            \\\n    '\\u2900-\\u297F'            \\\n    '\\u2B00-\\u2BFF'            \\\n    '\\u3000-\\u303F'            \\\n    '\\u3200-\\u32FF'            \\\n    '])'\n# The equivalent JS regex is \\ud83c[\\udd00-\\udfff]|\\ud83d[\\udc00-\\ude4f]|\\ud83d[\\ude80-\\udeff]|\n# \\ud83e[\\udd00-\\uddff]|[\\u2000-\\u206f]|[\\u2300-\\u27bf]|[\\u2b00-\\u2bff]|[\\u3000-\\u303f]|\n# [\\u3200-\\u32ff]. See below comments for explanation. The JS regex is used by marked.js for\n# frontend unicode emoji processing.\n# The JS regex \\ud83c[\\udd00-\\udfff]|\\ud83d[\\udc00-\\ude4f] represents U0001f100-\\U0001f64f\n# The JS regex \\ud83d[\\ude80-\\udeff] represents \\U0001f680-\\U0001f6ff\n# The JS regex \\ud83e[\\udd00-\\uddff] represents \\U0001f900-\\U0001f9ff\n# The JS regex [\\u2000-\\u206f] represents \\u2000-\\u206f\n# The JS regex [\\u2300-\\u27bf] represents \\u2300-\\u27bf\n# Similarly other JS regexes can be mapped to the respective unicode blocks.\n# For more information, please refer to the following article:\n# http://crocodillon.com/blog/parsing-emoji-unicode-in-javascript\n\ndef make_emoji(codepoint: str, display_string: str) -> Element:\n    # Replace underscore in emoji's title with space\n    title = display_string[1:-1].replace(\"_\", \" \")\n    span = markdown.util.etree.Element('span')\n    span.set('class', 'emoji emoji-%s' % (codepoint,))\n    span.set('title', title)\n    span.set('role', 'img')\n    span.set('aria-label', title)\n    span.text = display_string\n    return span\n\ndef make_realm_emoji(src: str, display_string: str) -> Element:\n    elt = markdown.util.etree.Element('img')\n    elt.set('src', src)\n    elt.set('class', 'emoji')\n    elt.set(\"alt\", display_string)\n    elt.set(\"title\", display_string[1:-1].replace(\"_\", \" \"))\n    return elt\n\ndef unicode_emoji_to_codepoint(unicode_emoji: str) -> str:\n    codepoint = hex(ord(unicode_emoji))[2:]\n    # Unicode codepoints are minimum of length 4, padded\n    # with zeroes if the length is less than zero.\n    while len(codepoint) < 4:\n        codepoint = '0' + codepoint\n    return codepoint\n\nclass EmoticonTranslation(markdown.inlinepatterns.Pattern):\n    \"\"\" Translates emoticons like `:)` into emoji like `:smile:`. \"\"\"\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        db_data = self.markdown.zulip_db_data\n        if db_data is None or not db_data['translate_emoticons']:\n            return None\n\n        emoticon = match.group('emoticon')\n        translated = translate_emoticons(emoticon)\n        name = translated[1:-1]\n        return make_emoji(name_to_codepoint[name], translated)\n\nclass UnicodeEmoji(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        orig_syntax = match.group('syntax')\n        codepoint = unicode_emoji_to_codepoint(orig_syntax)\n        if codepoint in codepoint_to_name:\n            display_string = ':' + codepoint_to_name[codepoint] + ':'\n            return make_emoji(codepoint, display_string)\n        else:\n            return None\n\nclass Emoji(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        orig_syntax = match.group(\"syntax\")\n        name = orig_syntax[1:-1]\n\n        active_realm_emoji = {}  # type: Dict[str, Dict[str, str]]\n        db_data = self.markdown.zulip_db_data\n        if db_data is not None:\n            active_realm_emoji = db_data['active_realm_emoji']\n\n        if self.markdown.zulip_message and name in active_realm_emoji:\n            return make_realm_emoji(active_realm_emoji[name]['source_url'], orig_syntax)\n        elif name == 'zulip':\n            return make_realm_emoji('/static/generated/emoji/images/emoji/unicode/zulip.png', orig_syntax)\n        elif name in name_to_codepoint:\n            return make_emoji(name_to_codepoint[name], orig_syntax)\n        else:\n            return None\n\ndef content_has_emoji_syntax(content: str) -> bool:\n    return re.search(EMOJI_REGEX, content) is not None\n\nclass ModalLink(markdown.inlinepatterns.Pattern):\n    \"\"\"\n    A pattern that allows including in-app modal links in messages.\n    \"\"\"\n\n    def handleMatch(self, match: Match[str]) -> Element:\n        relative_url = match.group('relative_url')\n        text = match.group('text')\n\n        a_tag = markdown.util.etree.Element(\"a\")\n        a_tag.set(\"href\", relative_url)\n        a_tag.set(\"title\", relative_url)\n        a_tag.text = text\n\n        return a_tag\n\nclass Tex(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Element:\n        rendered = render_tex(match.group('body'), is_inline=True)\n        if rendered is not None:\n            return etree.fromstring(rendered.encode('utf-8'))\n        else:  # Something went wrong while rendering\n            span = markdown.util.etree.Element('span')\n            span.set('class', 'tex-error')\n            span.text = '$$' + match.group('body') + '$$'\n            return span\n\nupload_title_re = re.compile(\"^(https?://[^/]*)?(/user_uploads/\\\\d+)(/[^/]*)?/[^/]*/(?P<filename>[^/]*)$\")\ndef url_filename(url: str) -> str:\n    \"\"\"Extract the filename if a URL is an uploaded file, or return the original URL\"\"\"\n    match = upload_title_re.match(url)\n    if match:\n        return match.group('filename')\n    else:\n        return url\n\ndef fixup_link(link: markdown.util.etree.Element, target_blank: bool=True) -> None:\n    \"\"\"Set certain attributes we want on every link.\"\"\"\n    if target_blank:\n        link.set('target', '_blank')\n    link.set('title', url_filename(link.get('href')))\n\n\ndef sanitize_url(url: str) -> Optional[str]:\n    \"\"\"\n    Sanitize a url against xss attacks.\n    See the docstring on markdown.inlinepatterns.LinkPattern.sanitize_url.\n    \"\"\"\n    try:\n        parts = urllib.parse.urlparse(url.replace(' ', '%20'))\n        scheme, netloc, path, params, query, fragment = parts\n    except ValueError:\n        # Bad url - so bad it couldn't be parsed.\n        return ''\n\n    # If there is no scheme or netloc and there is a '@' in the path,\n    # treat it as a mailto: and set the appropriate scheme\n    if scheme == '' and netloc == '' and '@' in path:\n        scheme = 'mailto'\n    elif scheme == '' and netloc == '' and len(path) > 0 and path[0] == '/':\n        # Allow domain-relative links\n        return urllib.parse.urlunparse(('', '', path, params, query, fragment))\n    elif (scheme, netloc, path, params, query) == ('', '', '', '', '') and len(fragment) > 0:\n        # Allow fragment links\n        return urllib.parse.urlunparse(('', '', '', '', '', fragment))\n\n    # Zulip modification: If scheme is not specified, assume http://\n    # We re-enter sanitize_url because netloc etc. need to be re-parsed.\n    if not scheme:\n        return sanitize_url('http://' + url)\n\n    locless_schemes = ['mailto', 'news', 'file', 'bitcoin']\n    if netloc == '' and scheme not in locless_schemes:\n        # This fails regardless of anything else.\n        # Return immediately to save additional processing\n        return None\n\n    # Upstream code will accept a URL like javascript://foo because it\n    # appears to have a netloc.  Additionally there are plenty of other\n    # schemes that do weird things like launch external programs.  To be\n    # on the safe side, we whitelist the scheme.\n    if scheme not in ('http', 'https', 'ftp', 'mailto', 'file', 'bitcoin'):\n        return None\n\n    # Upstream code scans path, parameters, and query for colon characters\n    # because\n    #\n    #    some aliases [for javascript:] will appear to urllib.parse to have\n    #    no scheme. On top of that relative links (i.e.: \"foo/bar.html\")\n    #    have no scheme.\n    #\n    # We already converted an empty scheme to http:// above, so we skip\n    # the colon check, which would also forbid a lot of legitimate URLs.\n\n    # Url passes all tests. Return url as-is.\n    return urllib.parse.urlunparse((scheme, netloc, path, params, query, fragment))\n\ndef url_to_a(db_data: Optional[DbData], url: str, text: Optional[str]=None) -> Union[Element, str]:\n    a = markdown.util.etree.Element('a')\n\n    href = sanitize_url(url)\n    target_blank = True\n    if href is None:\n        # Rejected by sanitize_url; render it as plain text.\n        return url\n    if text is None:\n        text = markdown.util.AtomicString(url)\n\n    href = rewrite_local_links_to_relative(db_data, href)\n    target_blank = not href.startswith(\"#narrow\") and not href.startswith('mailto:')\n\n    a.set('href', href)\n    a.text = text\n    fixup_link(a, target_blank)\n    return a\n\nclass CompiledPattern(markdown.inlinepatterns.Pattern):\n    def __init__(self, compiled_re: Pattern, md: markdown.Markdown) -> None:\n        # This is similar to the superclass's small __init__ function,\n        # but we skip the compilation step and let the caller give us\n        # a compiled regex.\n        self.compiled_re = compiled_re\n        self.md = md\n\nclass AutoLink(CompiledPattern):\n    def handleMatch(self, match: Match[str]) -> ElementStringNone:\n        url = match.group('url')\n        db_data = self.markdown.zulip_db_data\n        return url_to_a(db_data, url)\n\nclass UListProcessor(markdown.blockprocessors.UListProcessor):\n    \"\"\" Process unordered list blocks.\n\n        Based on markdown.blockprocessors.UListProcessor, but does not accept\n        '+' or '-' as a bullet character.\"\"\"\n\n    TAG = 'ul'\n    RE = re.compile('^[ ]{0,3}[*][ ]+(.*)')\n\n    def __init__(self, parser: Any) -> None:\n\n        # HACK: Set the tab length to 2 just for the initialization of\n        # this class, so that bulleted lists (and only bulleted lists)\n        # work off 2-space indentation.\n        parser.markdown.tab_length = 2\n        super().__init__(parser)\n        parser.markdown.tab_length = 4\n\nclass ListIndentProcessor(markdown.blockprocessors.ListIndentProcessor):\n    \"\"\" Process unordered list blocks.\n\n        Based on markdown.blockprocessors.ListIndentProcessor, but with 2-space indent\n    \"\"\"\n\n    def __init__(self, parser: Any) -> None:\n\n        # HACK: Set the tab length to 2 just for the initialization of\n        # this class, so that bulleted lists (and only bulleted lists)\n        # work off 2-space indentation.\n        parser.markdown.tab_length = 2\n        super().__init__(parser)\n        parser.markdown.tab_length = 4\n\nclass BlockQuoteProcessor(markdown.blockprocessors.BlockQuoteProcessor):\n    \"\"\" Process BlockQuotes.\n\n        Based on markdown.blockprocessors.BlockQuoteProcessor, but with 2-space indent\n    \"\"\"\n\n    # Original regex for blockquote is RE = re.compile(r'(^|\\n)[ ]{0,3}>[ ]?(.*)')\n    RE = re.compile(r'(^|\\n)(?!(?:[ ]{0,3}>\\s*(?:$|\\n))*(?:$|\\n))'\n                    r'[ ]{0,3}>[ ]?(.*)')\n    mention_re = re.compile(mention.find_mentions)\n\n    def clean(self, line: str) -> str:\n        # Silence all the mentions inside blockquotes\n        line = re.sub(self.mention_re, lambda m: \"@_{}\".format(m.group('match')), line)\n\n        # And then run the upstream processor's code for removing the '>'\n        return super().clean(line)\n\nclass BugdownUListPreprocessor(markdown.preprocessors.Preprocessor):\n    \"\"\" Allows unordered list blocks that come directly after a\n        paragraph to be rendered as an unordered list\n\n        Detects paragraphs that have a matching list item that comes\n        directly after a line of text, and inserts a newline between\n        to satisfy Markdown\"\"\"\n\n    LI_RE = re.compile('^[ ]{0,3}[*][ ]+(.*)', re.MULTILINE)\n    HANGING_ULIST_RE = re.compile('^.+\\\\n([ ]{0,3}[*][ ]+.*)', re.MULTILINE)\n\n    def run(self, lines: List[str]) -> List[str]:\n        \"\"\" Insert a newline between a paragraph and ulist if missing \"\"\"\n        inserts = 0\n        fence = None\n        copy = lines[:]\n        for i in range(len(lines) - 1):\n            # Ignore anything that is inside a fenced code block\n            m = FENCE_RE.match(lines[i])\n            if not fence and m:\n                fence = m.group('fence')\n            elif fence and m and fence == m.group('fence'):\n                fence = None\n\n            # If we're not in a fenced block and we detect an upcoming list\n            #  hanging off a paragraph, add a newline\n            if (not fence and lines[i] and\n                self.LI_RE.match(lines[i+1]) and\n                    not self.LI_RE.match(lines[i])):\n\n                copy.insert(i+inserts+1, '')\n                inserts += 1\n        return copy\n\nclass AutoNumberOListPreprocessor(markdown.preprocessors.Preprocessor):\n    \"\"\" Finds a sequence of lines numbered by the same number\"\"\"\n    RE = re.compile(r'^([ ]*)(\\d+)\\.[ ]+(.*)')\n    TAB_LENGTH = 2\n\n    def run(self, lines: List[str]) -> List[str]:\n        new_lines = []  # type: List[str]\n        current_list = []  # type: List[Match[str]]\n        current_indent = 0\n\n        for line in lines:\n            m = self.RE.match(line)\n\n            # Remember if this line is a continuation of already started list\n            is_next_item = (m and current_list\n                            and current_indent == len(m.group(1)) // self.TAB_LENGTH)\n\n            if not is_next_item:\n                # There is no more items in the list we were processing\n                new_lines.extend(self.renumber(current_list))\n                current_list = []\n\n            if not m:\n                # Ordinary line\n                new_lines.append(line)\n            elif is_next_item:\n                # Another list item\n                current_list.append(m)\n            else:\n                # First list item\n                current_list = [m]\n                current_indent = len(m.group(1)) // self.TAB_LENGTH\n\n        new_lines.extend(self.renumber(current_list))\n\n        return new_lines\n\n    def renumber(self, mlist: List[Match[str]]) -> List[str]:\n        if not mlist:\n            return []\n\n        start_number = int(mlist[0].group(2))\n\n        # Change numbers only if every one is the same\n        change_numbers = True\n        for m in mlist:\n            if int(m.group(2)) != start_number:\n                change_numbers = False\n                break\n\n        lines = []  # type: List[str]\n        counter = start_number\n\n        for m in mlist:\n            number = str(counter) if change_numbers else m.group(2)\n            lines.append('%s%s. %s' % (m.group(1), number, m.group(3)))\n            counter += 1\n\n        return lines\n\n# We need the following since upgrade from py-markdown 2.6.11 to 3.0.1\n# modifies the link handling significantly. The following is taken from\n# py-markdown 2.6.11 markdown/inlinepatterns.py.\n@one_time\ndef get_link_re() -> str:\n    '''\n    Very important--if you need to change this code to depend on\n    any arguments, you must eliminate the \"one_time\" decorator\n    and consider performance implications.  We only want to compute\n    this value once.\n    '''\n\n    NOBRACKET = r'[^\\]\\[]*'\n    BRK = (\n        r'\\[(' +\n        (NOBRACKET + r'(\\[')*6 +\n        (NOBRACKET + r'\\])*')*6 +\n        NOBRACKET + r')\\]'\n    )\n    NOIMG = r'(?<!\\!)'\n\n    # [text](url) or [text](<url>) or [text](url \"title\")\n    LINK_RE = NOIMG + BRK + \\\n        r'''\\(\\s*(<(?:[^<>\\\\]|\\\\.)*>|(\\([^()]*\\)|[^()])*?)\\s*(('(?:[^'\\\\]|\\\\.)*'|\"(?:[^\"\\\\]|\\\\.)*\")\\s*)?\\)'''\n    return normal_compile(LINK_RE)\n\ndef prepare_realm_pattern(source: str) -> str:\n    \"\"\" Augment a realm filter so it only matches after start-of-string,\n    whitespace, or opening delimiters, won't match if there are word\n    characters directly after, and saves what was matched as \"name\". \"\"\"\n    return r\"\"\"(?<![^\\s'\"\\(,:<])(?P<name>\"\"\" + source + r')(?!\\w)'\n\n# Given a regular expression pattern, linkifies groups that match it\n# using the provided format string to construct the URL.\nclass RealmFilterPattern(markdown.inlinepatterns.Pattern):\n    \"\"\" Applied a given realm filter to the input \"\"\"\n\n    def __init__(self, source_pattern: str,\n                 format_string: str,\n                 markdown_instance: Optional[markdown.Markdown]=None) -> None:\n        self.pattern = prepare_realm_pattern(source_pattern)\n        self.format_string = format_string\n        markdown.inlinepatterns.Pattern.__init__(self, self.pattern, markdown_instance)\n\n    def handleMatch(self, m: Match[str]) -> Union[Element, str]:\n        db_data = self.markdown.zulip_db_data\n        return url_to_a(db_data,\n                        self.format_string % m.groupdict(),\n                        m.group(\"name\"))\n\nclass UserMentionPattern(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        match = m.group('match')\n        silent = m.group('silent') == '_'\n\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            if match.startswith(\"**\") and match.endswith(\"**\"):\n                name = match[2:-2]\n            else:\n                return None\n\n            wildcard = mention.user_mention_matches_wildcard(name)\n\n            id_syntax_match = re.match(r'.+\\|(?P<user_id>\\d+)$', name)\n            if id_syntax_match:\n                id = id_syntax_match.group(\"user_id\")\n                user = db_data['mention_data'].get_user_by_id(id)\n            else:\n                user = db_data['mention_data'].get_user_by_name(name)\n\n            if wildcard:\n                self.markdown.zulip_message.mentions_wildcard = True\n                user_id = \"*\"\n            elif user:\n                if not silent:\n                    self.markdown.zulip_message.mentions_user_ids.add(user['id'])\n                name = user['full_name']\n                user_id = str(user['id'])\n            else:\n                # Don't highlight @mentions that don't refer to a valid user\n                return None\n\n            el = markdown.util.etree.Element(\"span\")\n            el.set('data-user-id', user_id)\n            if silent:\n                el.set('class', 'user-mention silent')\n                el.text = \"%s\" % (name,)\n            else:\n                el.set('class', 'user-mention')\n                el.text = \"@%s\" % (name,)\n            return el\n        return None\n\nclass UserGroupMentionPattern(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        match = m.group(2)\n\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            name = extract_user_group(match)\n            user_group = db_data['mention_data'].get_user_group(name)\n            if user_group:\n                self.markdown.zulip_message.mentions_user_group_ids.add(user_group.id)\n                name = user_group.name\n                user_group_id = str(user_group.id)\n            else:\n                # Don't highlight @-mentions that don't refer to a valid user\n                # group.\n                return None\n\n            el = markdown.util.etree.Element(\"span\")\n            el.set('class', 'user-group-mention')\n            el.set('data-user-group-id', user_group_id)\n            el.text = \"@%s\" % (name,)\n            return el\n        return None\n\nclass StreamPattern(CompiledPattern):\n    def find_stream_by_name(self, name: Match[str]) -> Optional[Dict[str, Any]]:\n        db_data = self.markdown.zulip_db_data\n        if db_data is None:\n            return None\n        stream = db_data['stream_names'].get(name)\n        return stream\n\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        name = m.group('stream_name')\n\n        if self.markdown.zulip_message:\n            stream = self.find_stream_by_name(name)\n            if stream is None:\n                return None\n            el = markdown.util.etree.Element('a')\n            el.set('class', 'stream')\n            el.set('data-stream-id', str(stream['id']))\n            # TODO: We should quite possibly not be specifying the\n            # href here and instead having the browser auto-add the\n            # href when it processes a message with one of these, to\n            # provide more clarity to API clients.\n            stream_url = encode_stream(stream['id'], name)\n            el.set('href', '/#narrow/stream/{stream_url}'.format(stream_url=stream_url))\n            el.text = '#{stream_name}'.format(stream_name=name)\n            return el\n        return None\n\ndef possible_linked_stream_names(content: str) -> Set[str]:\n    matches = re.findall(STREAM_LINK_REGEX, content, re.VERBOSE)\n    return set(matches)\n\nclass AlertWordsNotificationProcessor(markdown.preprocessors.Preprocessor):\n    def run(self, lines: Iterable[str]) -> Iterable[str]:\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            # We check for alert words here, the set of which are\n            # dependent on which users may see this message.\n            #\n            # Our caller passes in the list of possible_words.  We\n            # don't do any special rendering; we just append the alert words\n            # we find to the set self.markdown.zulip_message.alert_words.\n\n            realm_words = db_data['possible_words']\n\n            content = '\\n'.join(lines).lower()\n\n            allowed_before_punctuation = \"|\".join([r'\\s', '^', r'[\\(\\\".,\\';\\[\\*`>]'])\n            allowed_after_punctuation = \"|\".join([r'\\s', '$', r'[\\)\\\"\\?:.,\\';\\]!\\*`]'])\n\n            for word in realm_words:\n                escaped = re.escape(word.lower())\n                match_re = re.compile('(?:%s)%s(?:%s)' %\n                                      (allowed_before_punctuation,\n                                       escaped,\n                                       allowed_after_punctuation))\n                if re.search(match_re, content):\n                    self.markdown.zulip_message.alert_words.add(word)\n\n        return lines\n\n# This prevents realm_filters from running on the content of a\n# Markdown link, breaking up the link.  This is a monkey-patch, but it\n# might be worth sending a version of this change upstream.\nclass AtomicLinkPattern(CompiledPattern):\n    def get_element(self, m: Match[str]) -> Optional[Element]:\n        href = m.group(9)\n        if not href:\n            return None\n\n        if href[0] == \"<\":\n            href = href[1:-1]\n        href = sanitize_url(self.unescape(href.strip()))\n        if href is None:\n            return None\n\n        db_data = self.markdown.zulip_db_data\n        href = rewrite_local_links_to_relative(db_data, href)\n\n        el = markdown.util.etree.Element('a')\n        el.text = m.group(2)\n        el.set('href', href)\n        fixup_link(el, target_blank=(href[:1] != '#'))\n        return el\n\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        ret = self.get_element(m)\n        if ret is None:\n            return None\n        if not isinstance(ret, str):\n            ret.text = markdown.util.AtomicString(ret.text)\n        return ret\n\ndef get_sub_registry(r: markdown.util.Registry, keys: List[str]) -> markdown.util.Registry:\n    # Registry is a new class added by py-markdown to replace Ordered List.\n    # Since Registry doesn't support .keys(), it is easier to make a new\n    # object instead of removing keys from the existing object.\n    new_r = markdown.util.Registry()\n    for k in keys:\n        new_r.register(r[k], k, r.get_index_for_name(k))\n    return new_r\n\n# These are used as keys (\"realm_filters_keys\") to md_engines and the respective\n# realm filter caches\nDEFAULT_BUGDOWN_KEY = -1\nZEPHYR_MIRROR_BUGDOWN_KEY = -2\n\nclass Bugdown(markdown.Markdown):\n    def __init__(self, *args: Any, **kwargs: Union[bool, int, List[Any]]) -> None:\n        # define default configs\n        self.config = {\n            \"realm_filters\": [kwargs['realm_filters'],\n                              \"Realm-specific filters for realm_filters_key %s\" % (kwargs['realm'],)],\n            \"realm\": [kwargs['realm'], \"Realm id\"],\n            \"code_block_processor_disabled\": [kwargs['code_block_processor_disabled'],\n                                              \"Disabled for email gateway\"]\n        }\n\n        super().__init__(*args, **kwargs)\n        self.set_output_format('html')\n\n    def build_parser(self) -> markdown.Markdown:\n        # Build the parser using selected default features from py-markdown.\n        # The complete list of all available processors can be found in the\n        # super().build_parser() function.\n        #\n        # Note: for any py-markdown updates, manually check if we want any\n        # of the new features added upstream or not; they wouldn't get\n        # included by default.\n        self.preprocessors = self.build_preprocessors()\n        self.parser = self.build_block_parser()\n        self.inlinePatterns = self.build_inlinepatterns()\n        self.treeprocessors = self.build_treeprocessors()\n        self.postprocessors = self.build_postprocessors()\n        self.handle_zephyr_mirror()\n        return self\n\n    def build_preprocessors(self) -> markdown.util.Registry:\n        # We disable the following preprocessors from upstream:\n        #\n        # html_block - insecure\n        # reference - references don't make sense in a chat context.\n        preprocessors = markdown.util.Registry()\n        preprocessors.register(AutoNumberOListPreprocessor(self), 'auto_number_olist', 40)\n        preprocessors.register(BugdownUListPreprocessor(self), 'hanging_ulists', 35)\n        preprocessors.register(markdown.preprocessors.NormalizeWhitespace(self), 'normalize_whitespace', 30)\n        preprocessors.register(fenced_code.FencedBlockPreprocessor(self), 'fenced_code_block', 25)\n        preprocessors.register(AlertWordsNotificationProcessor(self), 'custom_text_notifications', 20)\n        return preprocessors\n\n    def build_block_parser(self) -> markdown.util.Registry:\n        # We disable the following blockparsers from upstream:\n        #\n        # indent - replaced by ours\n        # hashheader - disabled, since headers look bad and don't make sense in a chat context.\n        # setextheader - disabled, since headers look bad and don't make sense in a chat context.\n        # olist - replaced by ours\n        # ulist - replaced by ours\n        # quote - replaced by ours\n        parser = markdown.blockprocessors.BlockParser(self)\n        parser.blockprocessors.register(markdown.blockprocessors.EmptyBlockProcessor(parser), 'empty', 85)\n        if not self.getConfig('code_block_processor_disabled'):\n            parser.blockprocessors.register(markdown.blockprocessors.CodeBlockProcessor(parser), 'code', 80)\n        # We get priority 75 from 'table' extension\n        parser.blockprocessors.register(markdown.blockprocessors.HRProcessor(parser), 'hr', 70)\n        parser.blockprocessors.register(UListProcessor(parser), 'ulist', 65)\n        parser.blockprocessors.register(ListIndentProcessor(parser), 'indent', 60)\n        parser.blockprocessors.register(BlockQuoteProcessor(parser), 'quote', 55)\n        parser.blockprocessors.register(markdown.blockprocessors.ParagraphProcessor(parser), 'paragraph', 50)\n        return parser\n\n    def build_inlinepatterns(self) -> markdown.util.Registry:\n        # We disable the following upstream inline patterns:\n        #\n        # backtick -        replaced by ours\n        # escape -          probably will re-add at some point.\n        # link -            replaced by ours\n        # image_link -      replaced by ours\n        # autolink -        replaced by ours\n        # automail -        replaced by ours\n        # linebreak -       we use nl2br and consider that good enough\n        # html -            insecure\n        # reference -       references not useful\n        # image_reference - references not useful\n        # short_reference - references not useful\n        # ---------------------------------------------------\n        # strong_em -       for these three patterns,\n        # strong2 -         we have our own versions where\n        # emphasis2 -       we disable _ for bold and emphasis\n\n        # Declare regexes for clean single line calls to .register().\n        NOT_STRONG_RE = markdown.inlinepatterns.NOT_STRONG_RE\n        # Custom strikethrough syntax: ~~foo~~\n        DEL_RE = r'(?<!~)(\\~\\~)([^~\\n]+?)(\\~\\~)(?!~)'\n        # Custom bold syntax: **foo** but not __foo__\n        # str inside ** must start and end with a word character\n        # it need for things like \"const char *x = (char *)y\"\n        EMPHASIS_RE = r'(\\*)(?!\\s+)([^\\*^\\n]+)(?<!\\s)\\*'\n        ENTITY_RE = markdown.inlinepatterns.ENTITY_RE\n        STRONG_EM_RE = r'(\\*\\*\\*)(?!\\s+)([^\\*^\\n]+)(?<!\\s)\\*\\*\\*'\n        # Inline code block without whitespace stripping\n        BACKTICK_RE = r'(?:(?<!\\\\)((?:\\\\{2})+)(?=`+)|(?<!\\\\)(`+)(.+?)(?<!`)\\3(?!`))'\n\n        # Add Inline Patterns.  We use a custom numbering of the\n        # rules, that preserves the order from upstream but leaves\n        # space for us to add our own.\n        reg = markdown.util.Registry()\n        reg.register(BacktickPattern(BACKTICK_RE), 'backtick', 105)\n        reg.register(markdown.inlinepatterns.DoubleTagPattern(STRONG_EM_RE, 'strong,em'), 'strong_em', 100)\n        reg.register(UserMentionPattern(mention.find_mentions, self), 'usermention', 95)\n        reg.register(Tex(r'\\B(?<!\\$)\\$\\$(?P<body>[^\\n_$](\\\\\\$|[^$\\n])*)\\$\\$(?!\\$)\\B'), 'tex', 90)\n        reg.register(StreamPattern(get_compiled_stream_link_regex(), self), 'stream', 85)\n        reg.register(Avatar(AVATAR_REGEX, self), 'avatar', 80)\n        reg.register(ModalLink(r'!modal_link\\((?P<relative_url>[^)]*), (?P<text>[^)]*)\\)'), 'modal_link', 75)\n        # Note that !gravatar syntax should be deprecated long term.\n        reg.register(Avatar(GRAVATAR_REGEX, self), 'gravatar', 70)\n        reg.register(UserGroupMentionPattern(mention.user_group_mentions, self), 'usergroupmention', 65)\n        reg.register(AtomicLinkPattern(get_link_re(), self), 'link', 60)\n        reg.register(AutoLink(get_web_link_regex(), self), 'autolink', 55)\n        # Reserve priority 45-54 for Realm Filters\n        reg = self.register_realm_filters(reg)\n        reg.register(markdown.inlinepatterns.HtmlInlineProcessor(ENTITY_RE, self), 'entity', 40)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(r'(\\*\\*)([^\\n]+?)\\2', 'strong'), 'strong', 35)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(EMPHASIS_RE, 'em'), 'emphasis', 30)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(DEL_RE, 'del'), 'del', 25)\n        reg.register(markdown.inlinepatterns.SimpleTextInlineProcessor(NOT_STRONG_RE), 'not_strong', 20)\n        reg.register(Emoji(EMOJI_REGEX, self), 'emoji', 15)\n        reg.register(EmoticonTranslation(emoticon_regex, self), 'translate_emoticons', 10)\n        # We get priority 5 from 'nl2br' extension\n        reg.register(UnicodeEmoji(unicode_emoji_regex), 'unicodeemoji', 0)\n        return reg\n\n    def register_realm_filters(self, inlinePatterns: markdown.util.Registry) -> markdown.util.Registry:\n        for (pattern, format_string, id) in self.getConfig(\"realm_filters\"):\n            inlinePatterns.register(RealmFilterPattern(pattern, format_string, self),\n                                    'realm_filters/%s' % (pattern), 45)\n        return inlinePatterns\n\n    def build_treeprocessors(self) -> markdown.util.Registry:\n        # Here we build all the processors from upstream, plus a few of our own.\n        treeprocessors = markdown.util.Registry()\n        # We get priority 30 from 'hilite' extension\n        treeprocessors.register(markdown.treeprocessors.InlineProcessor(self), 'inline', 25)\n        treeprocessors.register(markdown.treeprocessors.PrettifyTreeprocessor(self), 'prettify', 20)\n        treeprocessors.register(InlineInterestingLinkProcessor(self), 'inline_interesting_links', 15)\n        if settings.CAMO_URI:\n            treeprocessors.register(InlineHttpsProcessor(self), 'rewrite_to_https', 10)\n        return treeprocessors\n\n    def build_postprocessors(self) -> markdown.util.Registry:\n        # These are the default python-markdown processors, unmodified.\n        postprocessors = markdown.util.Registry()\n        postprocessors.register(markdown.postprocessors.RawHtmlPostprocessor(self), 'raw_html', 20)\n        postprocessors.register(markdown.postprocessors.AndSubstitutePostprocessor(), 'amp_substitute', 15)\n        postprocessors.register(markdown.postprocessors.UnescapePostprocessor(), 'unescape', 10)\n        return postprocessors\n\n    def getConfig(self, key: str, default: str='') -> Any:\n        \"\"\" Return a setting for the given key or an empty string. \"\"\"\n        if key in self.config:\n            return self.config[key][0]\n        else:\n            return default\n\n    def handle_zephyr_mirror(self) -> None:\n        if self.getConfig(\"realm\") == ZEPHYR_MIRROR_BUGDOWN_KEY:\n            # Disable almost all inline patterns for zephyr mirror\n            # users' traffic that is mirrored.  Note that\n            # inline_interesting_links is a treeprocessor and thus is\n            # not removed\n            self.inlinePatterns = get_sub_registry(self.inlinePatterns, ['autolink'])\n            self.treeprocessors = get_sub_registry(self.treeprocessors, ['inline_interesting_links',\n                                                                         'rewrite_to_https'])\n            # insert new 'inline' processor because we have changed self.inlinePatterns\n            # but InlineProcessor copies md as self.md in __init__.\n            self.treeprocessors.register(markdown.treeprocessors.InlineProcessor(self), 'inline', 25)\n            self.preprocessors = get_sub_registry(self.preprocessors, ['custom_text_notifications'])\n            self.parser.blockprocessors = get_sub_registry(self.parser.blockprocessors, ['paragraph'])\n\nmd_engines = {}  # type: Dict[Tuple[int, bool], markdown.Markdown]\nrealm_filter_data = {}  # type: Dict[int, List[Tuple[str, str, int]]]\n\ndef make_md_engine(realm_filters_key: int, email_gateway: bool) -> None:\n    md_engine_key = (realm_filters_key, email_gateway)\n    if md_engine_key in md_engines:\n        del md_engines[md_engine_key]\n\n    realm_filters = realm_filter_data[realm_filters_key]\n    md_engines[md_engine_key] = build_engine(\n        realm_filters=realm_filters,\n        realm_filters_key=realm_filters_key,\n        email_gateway=email_gateway,\n    )\n\ndef build_engine(realm_filters: List[Tuple[str, str, int]],\n                 realm_filters_key: int,\n                 email_gateway: bool) -> markdown.Markdown:\n    engine = Bugdown(\n        realm_filters=realm_filters,\n        realm=realm_filters_key,\n        code_block_processor_disabled=email_gateway,\n        extensions = [\n            nl2br.makeExtension(),\n            tables.makeExtension(),\n            codehilite.makeExtension(\n                linenums=False,\n                guess_lang=False\n            ),\n        ])\n    return engine\n\ndef topic_links(realm_filters_key: int, topic_name: str) -> List[str]:\n    matches = []  # type: List[str]\n\n    realm_filters = realm_filters_for_realm(realm_filters_key)\n\n    for realm_filter in realm_filters:\n        pattern = prepare_realm_pattern(realm_filter[0])\n        for m in re.finditer(pattern, topic_name):\n            matches += [realm_filter[1] % m.groupdict()]\n    return matches\n\ndef maybe_update_markdown_engines(realm_filters_key: Optional[int], email_gateway: bool) -> None:\n    # If realm_filters_key is None, load all filters\n    global realm_filter_data\n    if realm_filters_key is None:\n        all_filters = all_realm_filters()\n        all_filters[DEFAULT_BUGDOWN_KEY] = []\n        for realm_filters_key, filters in all_filters.items():\n            realm_filter_data[realm_filters_key] = filters\n            make_md_engine(realm_filters_key, email_gateway)\n        # Hack to ensure that getConfig(\"realm\") is right for mirrored Zephyrs\n        realm_filter_data[ZEPHYR_MIRROR_BUGDOWN_KEY] = []\n        make_md_engine(ZEPHYR_MIRROR_BUGDOWN_KEY, False)\n    else:\n        realm_filters = realm_filters_for_realm(realm_filters_key)\n        if realm_filters_key not in realm_filter_data or    \\\n                realm_filter_data[realm_filters_key] != realm_filters:\n            # Realm filters data has changed, update `realm_filter_data` and any\n            # of the existing markdown engines using this set of realm filters.\n            realm_filter_data[realm_filters_key] = realm_filters\n            for email_gateway_flag in [True, False]:\n                if (realm_filters_key, email_gateway_flag) in md_engines:\n                    # Update only existing engines(if any), don't create new one.\n                    make_md_engine(realm_filters_key, email_gateway_flag)\n\n        if (realm_filters_key, email_gateway) not in md_engines:\n            # Markdown engine corresponding to this key doesn't exists so create one.\n            make_md_engine(realm_filters_key, email_gateway)\n\n# We want to log Markdown parser failures, but shouldn't log the actual input\n# message for privacy reasons.  The compromise is to replace all alphanumeric\n# characters with 'x'.\n#\n# We also use repr() to improve reproducibility, and to escape terminal control\n# codes, which can do surprisingly nasty things.\n_privacy_re = re.compile('\\\\w', flags=re.UNICODE)\ndef privacy_clean_markdown(content: str) -> str:\n    return repr(_privacy_re.sub('x', content))\n\ndef log_bugdown_error(msg: str) -> None:\n    \"\"\"We use this unusual logging approach to log the bugdown error, in\n    order to prevent AdminNotifyHandler from sending the santized\n    original markdown formatting into another Zulip message, which\n    could cause an infinite exception loop.\"\"\"\n    bugdown_logger.error(msg)\n\ndef get_email_info(realm_id: int, emails: Set[str]) -> Dict[str, FullNameInfo]:\n    if not emails:\n        return dict()\n\n    q_list = {\n        Q(email__iexact=email.strip().lower())\n        for email in emails\n    }\n\n    rows = UserProfile.objects.filter(\n        realm_id=realm_id\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'email',\n    )\n\n    dct = {\n        row['email'].strip().lower(): row\n        for row in rows\n    }\n    return dct\n\ndef get_possible_mentions_info(realm_id: int, mention_texts: Set[str]) -> List[FullNameInfo]:\n    if not mention_texts:\n        return list()\n\n    # Remove the trailing part of the `name|id` mention syntax,\n    # thus storing only full names in full_names.\n    full_names = set()\n    name_re = r'(?P<full_name>.+)\\|\\d+$'\n    for mention_text in mention_texts:\n        name_syntax_match = re.match(name_re, mention_text)\n        if name_syntax_match:\n            full_names.add(name_syntax_match.group(\"full_name\"))\n        else:\n            full_names.add(mention_text)\n\n    q_list = {\n        Q(full_name__iexact=full_name)\n        for full_name in full_names\n    }\n\n    rows = UserProfile.objects.filter(\n        realm_id=realm_id,\n        is_active=True,\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'full_name',\n        'email',\n    )\n    return list(rows)\n\nclass MentionData:\n    def __init__(self, realm_id: int, content: str) -> None:\n        mention_texts = possible_mentions(content)\n        possible_mentions_info = get_possible_mentions_info(realm_id, mention_texts)\n        self.full_name_info = {\n            row['full_name'].lower(): row\n            for row in possible_mentions_info\n        }\n        self.user_id_info = {\n            row['id']: row\n            for row in possible_mentions_info\n        }\n        self.init_user_group_data(realm_id=realm_id, content=content)\n\n    def init_user_group_data(self,\n                             realm_id: int,\n                             content: str) -> None:\n        user_group_names = possible_user_group_mentions(content)\n        self.user_group_name_info = get_user_group_name_info(realm_id, user_group_names)\n        self.user_group_members = defaultdict(list)  # type: Dict[int, List[int]]\n        group_ids = [group.id for group in self.user_group_name_info.values()]\n\n        if not group_ids:\n            # Early-return to avoid the cost of hitting the ORM,\n            # which shows up in profiles.\n            return\n\n        membership = UserGroupMembership.objects.filter(user_group_id__in=group_ids)\n        for info in membership.values('user_group_id', 'user_profile_id'):\n            group_id = info['user_group_id']\n            user_profile_id = info['user_profile_id']\n            self.user_group_members[group_id].append(user_profile_id)\n\n    def get_user_by_name(self, name: str) -> Optional[FullNameInfo]:\n        # warning: get_user_by_name is not dependable if two\n        # users of the same full name are mentioned. Use\n        # get_user_by_id where possible.\n        return self.full_name_info.get(name.lower(), None)\n\n    def get_user_by_id(self, id: str) -> Optional[FullNameInfo]:\n        return self.user_id_info.get(int(id), None)\n\n    def get_user_ids(self) -> Set[int]:\n        \"\"\"\n        Returns the user IDs that might have been mentioned by this\n        content.  Note that because this data structure has not parsed\n        the message and does not know about escaping/code blocks, this\n        will overestimate the list of user ids.\n        \"\"\"\n        return set(self.user_id_info.keys())\n\n    def get_user_group(self, name: str) -> Optional[UserGroup]:\n        return self.user_group_name_info.get(name.lower(), None)\n\n    def get_group_members(self, user_group_id: int) -> List[int]:\n        return self.user_group_members.get(user_group_id, [])\n\ndef get_user_group_name_info(realm_id: int, user_group_names: Set[str]) -> Dict[str, UserGroup]:\n    if not user_group_names:\n        return dict()\n\n    rows = UserGroup.objects.filter(realm_id=realm_id,\n                                    name__in=user_group_names)\n    dct = {row.name.lower(): row for row in rows}\n    return dct\n\ndef get_stream_name_info(realm: Realm, stream_names: Set[str]) -> Dict[str, FullNameInfo]:\n    if not stream_names:\n        return dict()\n\n    q_list = {\n        Q(name=name)\n        for name in stream_names\n    }\n\n    rows = get_active_streams(\n        realm=realm,\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'name',\n    )\n\n    dct = {\n        row['name']: row\n        for row in rows\n    }\n    return dct\n\n\ndef do_convert(content: str,\n               message: Optional[Message]=None,\n               message_realm: Optional[Realm]=None,\n               possible_words: Optional[Set[str]]=None,\n               sent_by_bot: Optional[bool]=False,\n               translate_emoticons: Optional[bool]=False,\n               mention_data: Optional[MentionData]=None,\n               email_gateway: Optional[bool]=False,\n               no_previews: Optional[bool]=False) -> str:\n    \"\"\"Convert Markdown to HTML, with Zulip-specific settings and hacks.\"\"\"\n    # This logic is a bit convoluted, but the overall goal is to support a range of use cases:\n    # * Nothing is passed in other than content -> just run default options (e.g. for docs)\n    # * message is passed, but no realm is -> look up realm from message\n    # * message_realm is passed -> use that realm for bugdown purposes\n    if message is not None:\n        if message_realm is None:\n            message_realm = message.get_realm()\n    if message_realm is None:\n        realm_filters_key = DEFAULT_BUGDOWN_KEY\n    else:\n        realm_filters_key = message_realm.id\n\n    if message and hasattr(message, 'id') and message.id:\n        logging_message_id = 'id# ' + str(message.id)\n    else:\n        logging_message_id = 'unknown'\n\n    if message is not None and message_realm is not None:\n        if message_realm.is_zephyr_mirror_realm:\n            if message.sending_client.name == \"zephyr_mirror\":\n                # Use slightly customized Markdown processor for content\n                # delivered via zephyr_mirror\n                realm_filters_key = ZEPHYR_MIRROR_BUGDOWN_KEY\n\n    maybe_update_markdown_engines(realm_filters_key, email_gateway)\n    md_engine_key = (realm_filters_key, email_gateway)\n\n    if md_engine_key in md_engines:\n        _md_engine = md_engines[md_engine_key]\n    else:\n        if DEFAULT_BUGDOWN_KEY not in md_engines:\n            maybe_update_markdown_engines(realm_filters_key=None, email_gateway=False)\n\n        _md_engine = md_engines[(DEFAULT_BUGDOWN_KEY, email_gateway)]\n    # Reset the parser; otherwise it will get slower over time.\n    _md_engine.reset()\n\n    # Filters such as UserMentionPattern need a message.\n    _md_engine.zulip_message = message\n    _md_engine.zulip_realm = message_realm\n    _md_engine.zulip_db_data = None  # for now\n    _md_engine.image_preview_enabled = image_preview_enabled(\n        message, message_realm, no_previews)\n    _md_engine.url_embed_preview_enabled = url_embed_preview_enabled(\n        message, message_realm, no_previews)\n\n    # Pre-fetch data from the DB that is used in the bugdown thread\n    if message is not None:\n        assert message_realm is not None  # ensured above if message is not None\n        if possible_words is None:\n            possible_words = set()  # Set[str]\n\n        # Here we fetch the data structures needed to render\n        # mentions/avatars/stream mentions from the database, but only\n        # if there is syntax in the message that might use them, since\n        # the fetches are somewhat expensive and these types of syntax\n        # are uncommon enough that it's a useful optimization.\n\n        if mention_data is None:\n            mention_data = MentionData(message_realm.id, content)\n\n        emails = possible_avatar_emails(content)\n        email_info = get_email_info(message_realm.id, emails)\n\n        stream_names = possible_linked_stream_names(content)\n        stream_name_info = get_stream_name_info(message_realm, stream_names)\n\n        if content_has_emoji_syntax(content):\n            active_realm_emoji = message_realm.get_active_emoji()\n        else:\n            active_realm_emoji = dict()\n\n        _md_engine.zulip_db_data = {\n            'possible_words': possible_words,\n            'email_info': email_info,\n            'mention_data': mention_data,\n            'active_realm_emoji': active_realm_emoji,\n            'realm_uri': message_realm.uri,\n            'sent_by_bot': sent_by_bot,\n            'stream_names': stream_name_info,\n            'translate_emoticons': translate_emoticons,\n        }\n\n    try:\n        # Spend at most 5 seconds rendering; this protects the backend\n        # from being overloaded by bugs (e.g. markdown logic that is\n        # extremely inefficient in corner cases) as well as user\n        # errors (e.g. a realm filter that makes some syntax\n        # infinite-loop).\n        rendered_content = timeout(5, _md_engine.convert, content)\n\n        # Throw an exception if the content is huge; this protects the\n        # rest of the codebase from any bugs where we end up rendering\n        # something huge.\n        if len(rendered_content) > MAX_MESSAGE_LENGTH * 10:\n            raise BugdownRenderingException('Rendered content exceeds %s characters (message %s)' %\n                                            (MAX_MESSAGE_LENGTH * 10, logging_message_id))\n        return rendered_content\n    except Exception:\n        cleaned = privacy_clean_markdown(content)\n        # NOTE: Don't change this message without also changing the\n        # logic in logging_handlers.py or we can create recursive\n        # exceptions.\n        exception_message = ('Exception in Markdown parser: %sInput (sanitized) was: %s\\n (message %s)'\n                             % (traceback.format_exc(), cleaned, logging_message_id))\n        bugdown_logger.exception(exception_message)\n\n        raise BugdownRenderingException()\n    finally:\n        # These next three lines are slightly paranoid, since\n        # we always set these right before actually using the\n        # engine, but better safe then sorry.\n        _md_engine.zulip_message = None\n        _md_engine.zulip_realm = None\n        _md_engine.zulip_db_data = None\n\nbugdown_time_start = 0.0\nbugdown_total_time = 0.0\nbugdown_total_requests = 0\n\ndef get_bugdown_time() -> float:\n    return bugdown_total_time\n\ndef get_bugdown_requests() -> int:\n    return bugdown_total_requests\n\ndef bugdown_stats_start() -> None:\n    global bugdown_time_start\n    bugdown_time_start = time.time()\n\ndef bugdown_stats_finish() -> None:\n    global bugdown_total_time\n    global bugdown_total_requests\n    global bugdown_time_start\n    bugdown_total_requests += 1\n    bugdown_total_time += (time.time() - bugdown_time_start)\n\ndef convert(content: str,\n            message: Optional[Message]=None,\n            message_realm: Optional[Realm]=None,\n            possible_words: Optional[Set[str]]=None,\n            sent_by_bot: Optional[bool]=False,\n            translate_emoticons: Optional[bool]=False,\n            mention_data: Optional[MentionData]=None,\n            email_gateway: Optional[bool]=False,\n            no_previews: Optional[bool]=False) -> str:\n    bugdown_stats_start()\n    ret = do_convert(content, message, message_realm,\n                     possible_words, sent_by_bot, translate_emoticons,\n                     mention_data, email_gateway, no_previews=no_previews)\n    bugdown_stats_finish()\n    return ret\n", "# -*- coding: utf-8 -*-\n# See https://zulip.readthedocs.io/en/latest/subsystems/thumbnailing.html\nimport base64\nimport os\nimport sys\nimport urllib\nfrom urllib.parse import urljoin, urlsplit, urlunsplit\nfrom django.conf import settings\nfrom libthumbor import CryptoURL\n\nZULIP_PATH = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\nsys.path.append(ZULIP_PATH)\n\nfrom zthumbor.loaders.helpers import (\n    THUMBOR_S3_TYPE, THUMBOR_LOCAL_FILE_TYPE, THUMBOR_EXTERNAL_TYPE\n)\nfrom zerver.lib.camo import get_camo_url\n\ndef is_thumbor_enabled() -> bool:\n    return settings.THUMBOR_URL != ''\n\ndef user_uploads_or_external(url: str) -> bool:\n    u = urlsplit(url)\n    return u.scheme != \"\" or u.netloc != \"\" or u.path.startswith(\"/user_uploads/\")\n\ndef get_source_type(url: str) -> str:\n    if not url.startswith('/user_uploads/'):\n        return THUMBOR_EXTERNAL_TYPE\n\n    local_uploads_dir = settings.LOCAL_UPLOADS_DIR\n    if local_uploads_dir:\n        return THUMBOR_LOCAL_FILE_TYPE\n    return THUMBOR_S3_TYPE\n\ndef generate_thumbnail_url(path: str,\n                           size: str='0x0',\n                           is_camo_url: bool=False) -> str:\n    path = urljoin(\"/\", path)\n    u = urlsplit(path)\n\n    if not is_thumbor_enabled():\n        if u.scheme == \"\" and u.netloc == \"\":\n            return urlunsplit(u)\n        return get_camo_url(path)\n\n    if u.scheme == \"\" and u.netloc == \"\" and not u.path.startswith(\"/user_uploads/\"):\n        return urlunsplit(u)\n\n    source_type = get_source_type(path)\n    safe_url = base64.urlsafe_b64encode(path.encode()).decode('utf-8')\n    image_url = '%s/source_type/%s' % (safe_url, source_type)\n    width, height = map(int, size.split('x'))\n    crypto = CryptoURL(key=settings.THUMBOR_KEY)\n\n    smart_crop_enabled = True\n    apply_filters = ['no_upscale()']\n    if is_camo_url:\n        smart_crop_enabled = False\n        apply_filters.append('quality(100)')\n    if size != '0x0':\n        apply_filters.append('sharpen(0.5,0.2,true)')\n\n    encrypted_url = crypto.generate(\n        width=width,\n        height=height,\n        smart=smart_crop_enabled,\n        filters=apply_filters,\n        image_url=image_url\n    )\n\n    if settings.THUMBOR_URL == 'http://127.0.0.1:9995':\n        # If THUMBOR_URL is the default then thumbor is hosted on same machine\n        # as the Zulip server and we should serve a relative URL.\n        # We add a /thumbor in front of the relative url because we make\n        # use of a proxy pass to redirect request internally in Nginx to 9995\n        # port where thumbor is running.\n        thumbnail_url = '/thumbor' + encrypted_url\n    else:\n        thumbnail_url = urllib.parse.urljoin(settings.THUMBOR_URL, encrypted_url)\n    return thumbnail_url\n", "# -*- coding: utf-8 -*-\nfrom django.conf import settings\n\nfrom zerver.lib.test_classes import ZulipTestCase\nfrom zerver.lib.test_helpers import (\n    use_s3_backend,\n    create_s3_buckets,\n    override_settings,\n    get_test_image_file\n)\nfrom zerver.lib.upload import upload_backend, upload_emoji_image\nfrom zerver.lib.users import get_api_key\n\nfrom io import StringIO\nimport ujson\nimport urllib\nimport base64\n\nclass ThumbnailTest(ZulipTestCase):\n\n    @use_s3_backend\n    def test_s3_source_type(self) -> None:\n        def get_file_path_urlpart(uri: str, size: str='') -> str:\n            url_in_result = 'smart/filters:no_upscale()%s/%s/source_type/s3'\n            sharpen_filter = ''\n            if size:\n                url_in_result = '/%s/%s' % (size, url_in_result)\n                sharpen_filter = ':sharpen(0.5,0.2,true)'\n            hex_uri = base64.urlsafe_b64encode(uri.encode()).decode('utf-8')\n            return url_in_result % (sharpen_filter, hex_uri)\n\n        create_s3_buckets(\n            settings.S3_AUTH_UPLOADS_BUCKET,\n            settings.S3_AVATAR_BUCKET)\n\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n\n        # Test full size image.\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test thumbnail size.\n        result = self.client_get(\"/thumbnail?url=%s&size=thumbnail\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri, '0x300')\n        self.assertIn(expected_part_url, result.url)\n\n        # Test custom emoji urls in Zulip messages.\n        user_profile = self.example_user(\"hamlet\")\n        image_file = get_test_image_file(\"img.png\")\n        file_name = \"emoji.png\"\n\n        upload_emoji_image(image_file, file_name, user_profile)\n        custom_emoji_url = upload_backend.get_emoji_url(file_name, user_profile.realm_id)\n        emoji_url_base = '/user_avatars/'\n        self.assertEqual(emoji_url_base, custom_emoji_url[:len(emoji_url_base)])\n\n        quoted_emoji_url = urllib.parse.quote(custom_emoji_url[1:], safe='')\n\n        # Test full size custom emoji image (for emoji link in messages case).\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_emoji_url))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertIn(custom_emoji_url, result.url)\n\n        # Tests the /api/v1/thumbnail api endpoint with standard API auth\n        self.logout()\n        result = self.api_get(\n            self.example_email(\"hamlet\"),\n            '/thumbnail?url=%s&size=full' %\n            (quoted_uri,))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test with another user trying to access image using thumbor.\n        self.login(self.example_email(\"iago\"))\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 403, result)\n        self.assert_in_response(\"You are not authorized to view this file.\", result)\n\n    def test_external_source_type(self) -> None:\n        def run_test_with_image_url(image_url: str) -> None:\n            # Test full size image.\n            self.login(self.example_email(\"hamlet\"))\n            quoted_url = urllib.parse.quote(image_url, safe='')\n            encoded_url = base64.urlsafe_b64encode(image_url.encode()).decode('utf-8')\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_url))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/smart/filters:no_upscale()/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test thumbnail size.\n            result = self.client_get(\"/thumbnail?url=%s&size=thumbnail\" % (quoted_url))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/0x300/smart/filters:no_upscale():sharpen(0.5,0.2,true)/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test api endpoint with standard API authentication.\n            self.logout()\n            user_profile = self.example_user(\"hamlet\")\n            result = self.api_get(user_profile.email,\n                                  \"/thumbnail?url=%s&size=thumbnail\" % (quoted_url,))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/0x300/smart/filters:no_upscale():sharpen(0.5,0.2,true)/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test api endpoint with legacy API authentication.\n            user_profile = self.example_user(\"hamlet\")\n            result = self.client_get(\"/thumbnail?url=%s&size=thumbnail&api_key=%s\" % (\n                quoted_url, get_api_key(user_profile)))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/0x300/smart/filters:no_upscale():sharpen(0.5,0.2,true)/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test a second logged-in user; they should also be able to access it\n            user_profile = self.example_user(\"iago\")\n            result = self.client_get(\"/thumbnail?url=%s&size=thumbnail&api_key=%s\" % (\n                quoted_url, get_api_key(user_profile)))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/0x300/smart/filters:no_upscale():sharpen(0.5,0.2,true)/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n            # Test with another user trying to access image using thumbor.\n            # File should be always accessible to user in case of external source\n            self.login(self.example_email(\"iago\"))\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_url))\n            self.assertEqual(result.status_code, 302, result)\n            expected_part_url = '/smart/filters:no_upscale()/' + encoded_url + '/source_type/external'\n            self.assertIn(expected_part_url, result.url)\n\n        image_url = 'https://images.foobar.com/12345'\n        run_test_with_image_url(image_url)\n\n        image_url = 'http://images.foobar.com/12345'\n        run_test_with_image_url(image_url)\n\n        image_url = '//images.foobar.com/12345'\n        run_test_with_image_url(image_url)\n\n    def test_local_file_type(self) -> None:\n        def get_file_path_urlpart(uri: str, size: str='') -> str:\n            url_in_result = 'smart/filters:no_upscale()%s/%s/source_type/local_file'\n            sharpen_filter = ''\n            if size:\n                url_in_result = '/%s/%s' % (size, url_in_result)\n                sharpen_filter = ':sharpen(0.5,0.2,true)'\n            hex_uri = base64.urlsafe_b64encode(uri.encode()).decode('utf-8')\n            return url_in_result % (sharpen_filter, hex_uri)\n\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        # Test full size image.\n        # We remove the forward slash infront of the `/user_uploads/` to match\n        # bugdown behaviour.\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test thumbnail size.\n        result = self.client_get(\"/thumbnail?url=%s&size=thumbnail\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri, '0x300')\n        self.assertIn(expected_part_url, result.url)\n\n        # Test with a unicode filename.\n        fp = StringIO(\"zulip!\")\n        fp.name = \"\u03bc\u03ad\u03bd\u03b5\u03b9.jpg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n\n        # We remove the forward slash infront of the `/user_uploads/` to match\n        # bugdown behaviour.\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test custom emoji urls in Zulip messages.\n        user_profile = self.example_user(\"hamlet\")\n        image_file = get_test_image_file(\"img.png\")\n        file_name = \"emoji.png\"\n\n        upload_emoji_image(image_file, file_name, user_profile)\n        custom_emoji_url = upload_backend.get_emoji_url(file_name, user_profile.realm_id)\n        emoji_url_base = '/user_avatars/'\n        self.assertEqual(emoji_url_base, custom_emoji_url[:len(emoji_url_base)])\n\n        quoted_emoji_url = urllib.parse.quote(custom_emoji_url[1:], safe='')\n\n        # Test full size custom emoji image (for emoji link in messages case).\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_emoji_url))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertIn(custom_emoji_url, result.url)\n\n        # Tests the /api/v1/thumbnail api endpoint with HTTP basic auth.\n        self.logout()\n        user_profile = self.example_user(\"hamlet\")\n        result = self.api_get(\n            self.example_email(\"hamlet\"),\n            '/thumbnail?url=%s&size=full' %\n            (quoted_uri,))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Tests the /api/v1/thumbnail api endpoint with ?api_key\n        # auth.\n        user_profile = self.example_user(\"hamlet\")\n        result = self.client_get(\n            '/thumbnail?url=%s&size=full&api_key=%s' %\n            (quoted_uri, get_api_key(user_profile)))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test with another user trying to access image using thumbor.\n        self.login(self.example_email(\"iago\"))\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 403, result)\n        self.assert_in_response(\"You are not authorized to view this file.\", result)\n\n    @override_settings(THUMBOR_URL='127.0.0.1:9995')\n    def test_with_static_files(self) -> None:\n        self.login(self.example_email(\"hamlet\"))\n        uri = '/static/images/cute/turtle.png'\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertEqual(uri, result.url)\n\n    def test_with_thumbor_disabled(self) -> None:\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n\n        with self.settings(THUMBOR_URL=''):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        self.assertEqual(uri, result.url)\n\n        uri = 'https://www.google.com/images/srpr/logo4w.png'\n        quoted_uri = urllib.parse.quote(uri, safe='')\n        with self.settings(THUMBOR_URL=''):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        base = 'https://external-content.zulipcdn.net/external_content/56c362a24201593891955ff526b3b412c0f9fcd2/68747470733a2f2f7777772e676f6f676c652e636f6d2f696d616765732f737270722f6c6f676f34772e706e67'\n        self.assertEqual(base, result.url)\n\n        uri = 'http://www.google.com/images/srpr/logo4w.png'\n        quoted_uri = urllib.parse.quote(uri, safe='')\n        with self.settings(THUMBOR_URL=''):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        base = 'https://external-content.zulipcdn.net/external_content/7b6552b60c635e41e8f6daeb36d88afc4eabde79/687474703a2f2f7777772e676f6f676c652e636f6d2f696d616765732f737270722f6c6f676f34772e706e67'\n        self.assertEqual(base, result.url)\n\n        uri = '//www.google.com/images/srpr/logo4w.png'\n        quoted_uri = urllib.parse.quote(uri, safe='')\n        with self.settings(THUMBOR_URL=''):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri,))\n        self.assertEqual(result.status_code, 302, result)\n        base = 'https://external-content.zulipcdn.net/external_content/676530cf4b101d56f56cc4a37c6ef4d4fd9b0c03/2f2f7777772e676f6f676c652e636f6d2f696d616765732f737270722f6c6f676f34772e706e67'\n        self.assertEqual(base, result.url)\n\n    def test_with_different_THUMBOR_URL(self) -> None:\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        hex_uri = base64.urlsafe_b64encode(uri.encode()).decode('utf-8')\n        with self.settings(THUMBOR_URL='http://test-thumborhost.com'):\n            result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        base = 'http://test-thumborhost.com/'\n        self.assertEqual(base, result.url[:len(base)])\n        expected_part_url = '/smart/filters:no_upscale()/' + hex_uri + '/source_type/local_file'\n        self.assertIn(expected_part_url, result.url)\n\n    def test_with_different_sizes(self) -> None:\n        def get_file_path_urlpart(uri: str, size: str='') -> str:\n            url_in_result = 'smart/filters:no_upscale()%s/%s/source_type/local_file'\n            sharpen_filter = ''\n            if size:\n                url_in_result = '/%s/%s' % (size, url_in_result)\n                sharpen_filter = ':sharpen(0.5,0.2,true)'\n            hex_uri = base64.urlsafe_b64encode(uri.encode()).decode('utf-8')\n            return url_in_result % (sharpen_filter, hex_uri)\n\n        self.login(self.example_email(\"hamlet\"))\n        fp = StringIO(\"zulip!\")\n        fp.name = \"zulip.jpeg\"\n\n        result = self.client_post(\"/json/user_uploads\", {'file': fp})\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n        self.assertIn(\"uri\", json)\n        uri = json[\"uri\"]\n        base = '/user_uploads/'\n        self.assertEqual(base, uri[:len(base)])\n\n        # Test with size supplied as a query parameter.\n        # size=thumbnail should return a 0x300 sized image.\n        # size=full should return the original resolution image.\n        quoted_uri = urllib.parse.quote(uri[1:], safe='')\n        result = self.client_get(\"/thumbnail?url=%s&size=thumbnail\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri, '0x300')\n        self.assertIn(expected_part_url, result.url)\n\n        result = self.client_get(\"/thumbnail?url=%s&size=full\" % (quoted_uri))\n        self.assertEqual(result.status_code, 302, result)\n        expected_part_url = get_file_path_urlpart(uri)\n        self.assertIn(expected_part_url, result.url)\n\n        # Test with size supplied as a query parameter where size is anything\n        # else than 'full' or 'thumbnail'. Result should be an error message.\n        result = self.client_get(\"/thumbnail?url=%s&size=480x360\" % (quoted_uri))\n        self.assertEqual(result.status_code, 403, result)\n        self.assert_in_response(\"Invalid size.\", result)\n\n        # Test with no size param supplied. In this case as well we show an\n        # error message.\n        result = self.client_get(\"/thumbnail?url=%s\" % (quoted_uri))\n        self.assertEqual(result.status_code, 400, \"Missing 'size' argument\")\n"], "filenames": ["zerver/lib/bugdown/__init__.py", "zerver/lib/thumbnail.py", "zerver/tests/test_thumbnail.py"], "buggy_code_start_loc": [11, 6, 150], "buggy_code_end_loc": [557, 46, 291], "fixing_code_start_loc": [12, 7, 151], "fixing_code_end_loc": [558, 48, 304], "type": "CWE-601", "message": "The image thumbnailing handler in Zulip Server versions 1.9.0 to before 2.0.8 allowed an open redirect that was visible to logged-in users.", "other": {"cve": {"id": "CVE-2019-19775", "sourceIdentifier": "cve@mitre.org", "published": "2019-12-18T04:15:15.383", "lastModified": "2019-12-18T20:29:25.407", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The image thumbnailing handler in Zulip Server versions 1.9.0 to before 2.0.8 allowed an open redirect that was visible to logged-in users."}, {"lang": "es", "value": "El controlador del proceso de im\u00e1genes miniaturas en el servidor Zulip versiones 1.9.0 anteriores a la versi\u00f3n 2.0.8, permiti\u00f3 un redireccionamiento abierto que era visible para usuarios registrados."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:C/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 6.1, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 2.7}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:P/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 5.8}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-601"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:zulip:zulip_server:*:*:*:*:*:*:*:*", "versionStartIncluding": "1.9.0", "versionEndExcluding": "2.0.8", "matchCriteriaId": "4E722ED7-3E20-4689-BE75-9608477BB7E2"}]}]}], "references": [{"url": "https://blog.zulip.org/2019/12/13/zulip-server-2-0-8-security-release/", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://github.com/zulip/zulip/commit/b7c87a4d82397a5e6ac169b6098bed0b1ae7a583", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/zulip/zulip/commit/b7c87a4d82397a5e6ac169b6098bed0b1ae7a583"}}