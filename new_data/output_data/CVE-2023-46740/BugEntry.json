{"buggy_code": ["// Copyright 2018 The CubeFS Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n// implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage proto\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"github.com/cubefs/cubefs/util/log\"\n\t\"io\"\n\t\"net\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/cubefs/cubefs/util\"\n\t\"github.com/cubefs/cubefs/util/buf\"\n)\n\nvar (\n\tGRequestID = int64(1)\n\tBuffers    *buf.BufferPool\n)\n\n// GenerateRequestID generates the request ID.\nfunc GenerateRequestID() int64 {\n\treturn atomic.AddInt64(&GRequestID, 1)\n}\n\nconst (\n\tAddrSplit = \"/\"\n)\n\n// Operations\nconst (\n\tProtoMagic           uint8 = 0xFF\n\tOpInitResultCode     uint8 = 0x00\n\tOpCreateExtent       uint8 = 0x01\n\tOpMarkDelete         uint8 = 0x02\n\tOpWrite              uint8 = 0x03\n\tOpRead               uint8 = 0x04\n\tOpStreamRead         uint8 = 0x05\n\tOpStreamFollowerRead uint8 = 0x06\n\tOpGetAllWatermarks   uint8 = 0x07\n\n\tOpNotifyReplicasToRepair         uint8 = 0x08\n\tOpExtentRepairRead               uint8 = 0x09\n\tOpBroadcastMinAppliedID          uint8 = 0x0A\n\tOpRandomWrite                    uint8 = 0x0F\n\tOpGetAppliedId                   uint8 = 0x10\n\tOpGetPartitionSize               uint8 = 0x11\n\tOpSyncRandomWrite                uint8 = 0x12\n\tOpSyncWrite                      uint8 = 0x13\n\tOpReadTinyDeleteRecord           uint8 = 0x14\n\tOpTinyExtentRepairRead           uint8 = 0x15\n\tOpGetMaxExtentIDAndPartitionSize uint8 = 0x16\n\n\t// Operations: Client -> MetaNode.\n\tOpMetaCreateInode   uint8 = 0x20\n\tOpMetaUnlinkInode   uint8 = 0x21\n\tOpMetaCreateDentry  uint8 = 0x22\n\tOpMetaDeleteDentry  uint8 = 0x23\n\tOpMetaOpen          uint8 = 0x24\n\tOpMetaLookup        uint8 = 0x25\n\tOpMetaReadDir       uint8 = 0x26\n\tOpMetaInodeGet      uint8 = 0x27\n\tOpMetaBatchInodeGet uint8 = 0x28\n\tOpMetaExtentsAdd    uint8 = 0x29\n\tOpMetaExtentsDel    uint8 = 0x2A\n\tOpMetaExtentsList   uint8 = 0x2B\n\tOpMetaUpdateDentry  uint8 = 0x2C\n\tOpMetaTruncate      uint8 = 0x2D\n\tOpMetaLinkInode     uint8 = 0x2E\n\tOpMetaEvictInode    uint8 = 0x2F\n\tOpMetaSetattr       uint8 = 0x30\n\tOpMetaReleaseOpen   uint8 = 0x31\n\n\t//Operations: MetaNode Leader -> MetaNode Follower\n\tOpMetaFreeInodesOnRaftFollower uint8 = 0x32\n\n\tOpMetaDeleteInode        uint8 = 0x33 // delete specified inode immediately and do not remove data.\n\tOpMetaBatchExtentsAdd    uint8 = 0x34 // for extents batch attachment\n\tOpMetaSetXAttr           uint8 = 0x35\n\tOpMetaGetXAttr           uint8 = 0x36\n\tOpMetaRemoveXAttr        uint8 = 0x37\n\tOpMetaListXAttr          uint8 = 0x38\n\tOpMetaBatchGetXAttr      uint8 = 0x39\n\tOpMetaExtentAddWithCheck uint8 = 0x3A // Append extent key with discard extents check\n\tOpMetaReadDirLimit       uint8 = 0x3D\n\n\t// Operations: Master -> MetaNode\n\tOpCreateMetaPartition           uint8 = 0x40\n\tOpMetaNodeHeartbeat             uint8 = 0x41\n\tOpDeleteMetaPartition           uint8 = 0x42\n\tOpUpdateMetaPartition           uint8 = 0x43\n\tOpLoadMetaPartition             uint8 = 0x44\n\tOpDecommissionMetaPartition     uint8 = 0x45\n\tOpAddMetaPartitionRaftMember    uint8 = 0x46\n\tOpRemoveMetaPartitionRaftMember uint8 = 0x47\n\tOpMetaPartitionTryToLeader      uint8 = 0x48\n\n\t// Quota\n\tOpMetaBatchSetInodeQuota    uint8 = 0x50\n\tOpMetaBatchDeleteInodeQuota uint8 = 0x51\n\tOpMetaGetInodeQuota         uint8 = 0x52\n\tOpQuotaCreateInode          uint8 = 0x53\n\tOpQuotaCreateDentry         uint8 = 0x54\n\n\t// Operations: Master -> LcNode\n\n\tOpLcNodeHeartbeat      uint8 = 0x55\n\tOpLcNodeScan           uint8 = 0x56\n\tOpLcNodeSnapshotVerDel uint8 = 0x57\n\n\t// Operations: Master -> DataNode\n\tOpCreateDataPartition           uint8 = 0x60\n\tOpDeleteDataPartition           uint8 = 0x61\n\tOpLoadDataPartition             uint8 = 0x62\n\tOpDataNodeHeartbeat             uint8 = 0x63\n\tOpReplicateFile                 uint8 = 0x64\n\tOpDeleteFile                    uint8 = 0x65\n\tOpDecommissionDataPartition     uint8 = 0x66\n\tOpAddDataPartitionRaftMember    uint8 = 0x67\n\tOpRemoveDataPartitionRaftMember uint8 = 0x68\n\tOpDataPartitionTryToLeader      uint8 = 0x69\n\tOpQos                           uint8 = 0x6A\n\tOpStopDataPartitionRepair       uint8 = 0x6B\n\n\t// Operations: MultipartInfo\n\tOpCreateMultipart  uint8 = 0x70\n\tOpGetMultipart     uint8 = 0x71\n\tOpAddMultipartPart uint8 = 0x72\n\tOpRemoveMultipart  uint8 = 0x73\n\tOpListMultiparts   uint8 = 0x74\n\n\tOpBatchDeleteExtent   uint8 = 0x75 // SDK to MetaNode\n\tOpGetExpiredMultipart uint8 = 0x76\n\n\t//Operations: MetaNode Leader -> MetaNode Follower\n\tOpMetaBatchDeleteInode  uint8 = 0x90\n\tOpMetaBatchDeleteDentry uint8 = 0x91\n\tOpMetaBatchUnlinkInode  uint8 = 0x92\n\tOpMetaBatchEvictInode   uint8 = 0x93\n\n\t//Transaction Operations: Client -> MetaNode.\n\tOpMetaTxCreate       uint8 = 0xA0\n\tOpMetaTxCreateInode  uint8 = 0xA1\n\tOpMetaTxUnlinkInode  uint8 = 0xA2\n\tOpMetaTxCreateDentry uint8 = 0xA3\n\tOpTxCommit           uint8 = 0xA4\n\tOpTxRollback         uint8 = 0xA5\n\tOpTxCommitRM         uint8 = 0xA6\n\tOpTxRollbackRM       uint8 = 0xA7\n\tOpMetaTxDeleteDentry uint8 = 0xA8\n\tOpMetaTxUpdateDentry uint8 = 0xA9\n\tOpMetaTxLinkInode    uint8 = 0xAA\n\tOpMetaTxGet          uint8 = 0xAB\n\n\t//Operations: Client -> MetaNode.\n\tOpMetaGetUniqID uint8 = 0xAC\n\n\t//Multi version snapshot\n\tOpRandomWriteAppend     uint8 = 0xB1\n\tOpSyncRandomWriteAppend uint8 = 0xB2\n\tOpRandomWriteVer        uint8 = 0xB3\n\tOpSyncRandomWriteVer    uint8 = 0xB4\n\tOpSyncRandomWriteVerRsp uint8 = 0xB5\n\tOpTryWriteAppend        uint8 = 0xB6\n\tOpSyncTryWriteAppend    uint8 = 0xB7\n\n\t// Commons\n\tOpNoSpaceErr uint8 = 0xEE\n\tOpDirQuota   uint8 = 0xF1\n\n\t// Commons\n\n\tOpConflictExtentsErr uint8 = 0xF2\n\tOpIntraGroupNetErr   uint8 = 0xF3\n\tOpArgMismatchErr     uint8 = 0xF4\n\tOpNotExistErr        uint8 = 0xF5\n\tOpDiskNoSpaceErr     uint8 = 0xF6\n\tOpDiskErr            uint8 = 0xF7\n\tOpErr                uint8 = 0xF8\n\tOpAgain              uint8 = 0xF9\n\tOpExistErr           uint8 = 0xFA\n\tOpInodeFullErr       uint8 = 0xFB\n\tOpTryOtherAddr       uint8 = 0xFC\n\tOpNotPerm            uint8 = 0xFD\n\tOpNotEmpty           uint8 = 0xFE\n\tOpOk                 uint8 = 0xF0\n\tOpTryOtherExtent     uint8 = 0xE0\n\tOpAgainVerionList    uint8 = 0xEF\n\n\tOpPing                  uint8 = 0xFF\n\tOpMetaUpdateXAttr       uint8 = 0x3B\n\tOpMetaReadDirOnly       uint8 = 0x3C\n\tOpUploadPartConflictErr uint8 = 0x3D\n\n\t// ebs obj meta\n\tOpMetaObjExtentAdd       uint8 = 0xDD\n\tOpMetaObjExtentsList     uint8 = 0xDE\n\tOpMetaExtentsEmpty       uint8 = 0xDF\n\tOpMetaBatchObjExtentsAdd uint8 = 0xD0\n\tOpMetaClearInodeCache    uint8 = 0xD1\n\n\tOpMetaBatchSetXAttr uint8 = 0xD2\n\tOpMetaGetAllXAttr   uint8 = 0xD3\n\n\t//transaction error\n\n\tOpTxInodeInfoNotExistErr  uint8 = 0xE0\n\tOpTxConflictErr           uint8 = 0xE1\n\tOpTxDentryInfoNotExistErr uint8 = 0xE2\n\tOpTxRbInodeNotExistErr    uint8 = 0xE3\n\tOpTxRbDentryNotExistErr   uint8 = 0xE4\n\tOpTxInfoNotExistErr       uint8 = 0xE5\n\tOpTxInternalErr           uint8 = 0xE6\n\tOpTxCommitItemErr         uint8 = 0xE7\n\tOpTxRollbackItemErr       uint8 = 0xE8\n\tOpTxRollbackUnknownRbType uint8 = 0xE9\n\tOpTxTimeoutErr            uint8 = 0xEA\n\tOpTxSetStateErr           uint8 = 0xEB\n\tOpTxCommitErr             uint8 = 0xEC\n\tOpTxRollbackErr           uint8 = 0xED\n\tOpTxUnknownOp             uint8 = 0xEE\n\n\t// multiVersion to dp/mp\n\tOpVersionOperation uint8 = 0xD5\n\tOpSplitMarkDelete  uint8 = 0xD6\n)\n\nconst (\n\tWriteDeadlineTime                         = 5\n\tReadDeadlineTime                          = 5\n\tSyncSendTaskDeadlineTime                  = 30\n\tNoReadDeadlineTime                        = -1\n\tBatchDeleteExtentReadDeadLineTime         = 120\n\tGetAllWatermarksDeadLineTime              = 60\n\tDefaultClusterLoadFactor          float64 = 10\n\tMultiVersionFlag                          = 0x80\n\tVersionListFlag                           = 0x40\n)\n\n// multi version operation\nconst (\n\tCreateVersion        = 1\n\tDeleteVersion        = 2\n\tCreateVersionPrepare = 3\n\tCreateVersionCommit  = 4\n\tSyncAllVersionList   = 5\n)\n\n// stage of version building\nconst (\n\tVersionInit            = 0\n\tVersionWorking         = 1\n\tVersionWorkingTimeOut  = 2\n\tVersionWorkingAbnormal = 3\n\tVersionWorkingFinished = 4\n)\n\n// status of version\nconst (\n\tVersionNormal         = 1\n\tVersionDeleted        = 2\n\tVersionDeleting       = 3\n\tVersionDeleteAbnormal = 4\n\tVersionPrepare        = 5\n)\n\nconst (\n\tTinyExtentType   = 0\n\tNormalExtentType = 1\n)\n\nconst (\n\tNormalCreateDataPartition         = 0\n\tDecommissionedCreateDataPartition = 1\n)\n\n// Packet defines the packet structure.\ntype Packet struct {\n\tMagic              uint8\n\tExtentType         uint8 // the highest bit be set while rsp to client if version not consistent then Verseq be valid\n\tOpcode             uint8\n\tResultCode         uint8\n\tRemainingFollowers uint8\n\tCRC                uint32\n\tSize               uint32\n\tArgLen             uint32\n\tKernelOffset       uint64\n\tPartitionID        uint64\n\tExtentID           uint64\n\tExtentOffset       int64\n\tReqID              int64\n\tArg                []byte // for create or append ops, the data contains the address\n\tData               []byte\n\tStartT             int64\n\tmesg               string\n\tHasPrepare         bool\n\tVerSeq             uint64 // only used in mod request to datanode\n\tVerList            []*VolVersionInfo\n}\n\nfunc IsTinyExtentType(extentType uint8) bool {\n\treturn extentType&NormalExtentType != NormalExtentType\n}\n\nfunc IsNormalExtentType(extentType uint8) bool {\n\treturn extentType&NormalExtentType == NormalExtentType\n}\n\n// NewPacket returns a new packet.\nfunc NewPacket() *Packet {\n\tp := new(Packet)\n\tp.Magic = ProtoMagic\n\tp.StartT = time.Now().UnixNano()\n\treturn p\n}\n\n// NewPacketReqID returns a new packet with ReqID assigned.\nfunc NewPacketReqID() *Packet {\n\tp := NewPacket()\n\tp.ReqID = GenerateRequestID()\n\treturn p\n}\n\nfunc (p *Packet) GetCopy() *Packet {\n\tnewPacket := NewPacket()\n\tnewPacket.ReqID = p.ReqID\n\tnewPacket.Opcode = p.Opcode\n\tnewPacket.PartitionID = p.PartitionID\n\n\tnewPacket.Data = make([]byte, p.Size)\n\tcopy(newPacket.Data[:p.Size], p.Data)\n\n\tnewPacket.Size = p.Size\n\treturn newPacket\n}\n\nfunc (p *Packet) String() string {\n\treturn fmt.Sprintf(\"ReqID(%v)Op(%v)PartitionID(%v)ResultCode(%v)ExID(%v)ExtOffset(%v)KernelOff(%v)Type(%v)Seq(%v)\",\n\t\tp.ReqID, p.GetOpMsg(), p.PartitionID, p.GetResultMsg(), p.ExtentID, p.ExtentOffset, p.KernelOffset, p.ExtentType, p.VerSeq)\n}\n\n// GetStoreType returns the store type.\nfunc (p *Packet) GetStoreType() (m string) {\n\tif IsNormalExtentType(p.ExtentType) {\n\t\treturn \"NormalExtent\"\n\t} else if IsTinyExtentType(p.ExtentType) {\n\t\treturn \"TinyExtent\"\n\t} else {\n\t\treturn \"Unknown\"\n\t}\n}\n\nfunc (p *Packet) GetOpMsgWithReqAndResult() (m string) {\n\treturn fmt.Sprintf(\"Req(%v)_(%v)_Result(%v)\", p.ReqID, p.GetOpMsg(), p.GetResultMsg())\n}\n\n// GetOpMsg returns the operation type.\nfunc (p *Packet) GetOpMsg() (m string) {\n\tswitch p.Opcode {\n\tcase OpCreateExtent:\n\t\tm = \"OpCreateExtent\"\n\tcase OpMarkDelete:\n\t\tm = \"OpMarkDelete\"\n\tcase OpSplitMarkDelete:\n\t\tm = \"OpMarkDelete\"\n\tcase OpWrite:\n\t\tm = \"OpWrite\"\n\tcase OpTryWriteAppend:\n\t\tm = \"OpTryWriteAppend\"\n\tcase OpRandomWrite:\n\t\tm = \"OpRandomWrite\"\n\tcase OpRandomWriteAppend:\n\t\tm = \"OpRandomWriteAppend\"\n\tcase OpRandomWriteVer:\n\t\tm = \"OpRandomWriteVer\"\n\tcase OpRead:\n\t\tm = \"Read\"\n\tcase OpStreamRead:\n\t\tm = \"OpStreamRead\"\n\tcase OpStreamFollowerRead:\n\t\tm = \"OpStreamFollowerRead\"\n\tcase OpGetAllWatermarks:\n\t\tm = \"OpGetAllWatermarks\"\n\tcase OpNotifyReplicasToRepair:\n\t\tm = \"OpNotifyReplicasToRepair\"\n\tcase OpExtentRepairRead:\n\t\tm = \"OpExtentRepairRead\"\n\tcase OpConflictExtentsErr:\n\t\tm = \"ConflictExtentsErr\"\n\tcase OpIntraGroupNetErr:\n\t\tm = \"IntraGroupNetErr\"\n\tcase OpMetaCreateInode:\n\t\tm = \"OpMetaCreateInode\"\n\tcase OpQuotaCreateInode:\n\t\tm = \"OpQuotaCreateInode\"\n\tcase OpMetaUnlinkInode:\n\t\tm = \"OpMetaUnlinkInode\"\n\tcase OpMetaBatchUnlinkInode:\n\t\tm = \"OpMetaBatchUnlinkInode\"\n\tcase OpMetaCreateDentry:\n\t\tm = \"OpMetaCreateDentry\"\n\tcase OpQuotaCreateDentry:\n\t\tm = \"OpQuotaCreateDentry\"\n\tcase OpMetaDeleteDentry:\n\t\tm = \"OpMetaDeleteDentry\"\n\tcase OpMetaBatchDeleteDentry:\n\t\tm = \"OpMetaBatchDeleteDentry\"\n\tcase OpMetaOpen:\n\t\tm = \"OpMetaOpen\"\n\tcase OpMetaReleaseOpen:\n\t\tm = \"OpMetaReleaseOpen\"\n\tcase OpMetaLookup:\n\t\tm = \"OpMetaLookup\"\n\tcase OpMetaReadDir:\n\t\tm = \"OpMetaReadDir\"\n\tcase OpMetaReadDirLimit:\n\t\tm = \"OpMetaReadDirLimit\"\n\tcase OpMetaInodeGet:\n\t\tm = \"OpMetaInodeGet\"\n\tcase OpMetaBatchInodeGet:\n\t\tm = \"OpMetaBatchInodeGet\"\n\tcase OpMetaExtentsAdd:\n\t\tm = \"OpMetaExtentsAdd\"\n\tcase OpMetaExtentAddWithCheck:\n\t\tm = \"OpMetaExtentAddWithCheck\"\n\tcase OpMetaObjExtentAdd:\n\t\tm = \"OpMetaObjExtentAdd\"\n\tcase OpMetaExtentsDel:\n\t\tm = \"OpMetaExtentsDel\"\n\tcase OpMetaExtentsList:\n\t\tm = \"OpMetaExtentsList\"\n\tcase OpMetaObjExtentsList:\n\t\tm = \"OpMetaObjExtentsList\"\n\tcase OpMetaUpdateDentry:\n\t\tm = \"OpMetaUpdateDentry\"\n\tcase OpMetaTruncate:\n\t\tm = \"OpMetaTruncate\"\n\tcase OpMetaLinkInode:\n\t\tm = \"OpMetaLinkInode\"\n\tcase OpMetaEvictInode:\n\t\tm = \"OpMetaEvictInode\"\n\tcase OpMetaBatchEvictInode:\n\t\tm = \"OpMetaBatchEvictInode\"\n\tcase OpMetaSetattr:\n\t\tm = \"OpMetaSetattr\"\n\tcase OpCreateMetaPartition:\n\t\tm = \"OpCreateMetaPartition\"\n\tcase OpMetaNodeHeartbeat:\n\t\tm = \"OpMetaNodeHeartbeat\"\n\tcase OpDeleteMetaPartition:\n\t\tm = \"OpDeleteMetaPartition\"\n\tcase OpUpdateMetaPartition:\n\t\tm = \"OpUpdateMetaPartition\"\n\tcase OpLoadMetaPartition:\n\t\tm = \"OpLoadMetaPartition\"\n\tcase OpDecommissionMetaPartition:\n\t\tm = \"OpDecommissionMetaPartition\"\n\tcase OpCreateDataPartition:\n\t\tm = \"OpCreateDataPartition\"\n\tcase OpDeleteDataPartition:\n\t\tm = \"OpDeleteDataPartition\"\n\tcase OpLoadDataPartition:\n\t\tm = \"OpLoadDataPartition\"\n\tcase OpDecommissionDataPartition:\n\t\tm = \"OpDecommissionDataPartition\"\n\tcase OpDataNodeHeartbeat:\n\t\tm = \"OpDataNodeHeartbeat\"\n\tcase OpReplicateFile:\n\t\tm = \"OpReplicateFile\"\n\tcase OpDeleteFile:\n\t\tm = \"OpDeleteFile\"\n\tcase OpGetAppliedId:\n\t\tm = \"OpGetAppliedId\"\n\tcase OpGetPartitionSize:\n\t\tm = \"OpGetPartitionSize\"\n\tcase OpSyncWrite:\n\t\tm = \"OpSyncWrite\"\n\tcase OpSyncTryWriteAppend:\n\t\tm = \"OpSyncTryWriteAppend\"\n\tcase OpSyncRandomWrite:\n\t\tm = \"OpSyncRandomWrite\"\n\tcase OpSyncRandomWriteVer:\n\t\tm = \"OpSyncRandomWriteVer\"\n\tcase OpSyncRandomWriteAppend:\n\t\tm = \"OpSyncRandomWriteAppend\"\n\tcase OpReadTinyDeleteRecord:\n\t\tm = \"OpReadTinyDeleteRecord\"\n\tcase OpPing:\n\t\tm = \"OpPing\"\n\tcase OpTinyExtentRepairRead:\n\t\tm = \"OpTinyExtentRepairRead\"\n\tcase OpGetMaxExtentIDAndPartitionSize:\n\t\tm = \"OpGetMaxExtentIDAndPartitionSize\"\n\tcase OpBroadcastMinAppliedID:\n\t\tm = \"OpBroadcastMinAppliedID\"\n\tcase OpRemoveDataPartitionRaftMember:\n\t\tm = \"OpRemoveDataPartitionRaftMember\"\n\tcase OpAddDataPartitionRaftMember:\n\t\tm = \"OpAddDataPartitionRaftMember\"\n\tcase OpAddMetaPartitionRaftMember:\n\t\tm = \"OpAddMetaPartitionRaftMember\"\n\tcase OpRemoveMetaPartitionRaftMember:\n\t\tm = \"OpRemoveMetaPartitionRaftMember\"\n\tcase OpMetaPartitionTryToLeader:\n\t\tm = \"OpMetaPartitionTryToLeader\"\n\tcase OpDataPartitionTryToLeader:\n\t\tm = \"OpDataPartitionTryToLeader\"\n\tcase OpMetaDeleteInode:\n\t\tm = \"OpMetaDeleteInode\"\n\tcase OpMetaBatchDeleteInode:\n\t\tm = \"OpMetaBatchDeleteInode\"\n\tcase OpMetaBatchExtentsAdd:\n\t\tm = \"OpMetaBatchExtentsAdd\"\n\tcase OpMetaBatchObjExtentsAdd:\n\t\tm = \"OpMetaBatchObjExtentsAdd\"\n\tcase OpMetaSetXAttr:\n\t\tm = \"OpMetaSetXAttr\"\n\tcase OpMetaGetXAttr:\n\t\tm = \"OpMetaGetXAttr\"\n\tcase OpMetaRemoveXAttr:\n\t\tm = \"OpMetaRemoveXAttr\"\n\tcase OpMetaListXAttr:\n\t\tm = \"OpMetaListXAttr\"\n\tcase OpMetaBatchGetXAttr:\n\t\tm = \"OpMetaBatchGetXAttr\"\n\tcase OpMetaUpdateXAttr:\n\t\tm = \"OpMetaUpdateXAttr\"\n\tcase OpCreateMultipart:\n\t\tm = \"OpCreateMultipart\"\n\tcase OpGetMultipart:\n\t\tm = \"OpGetMultipart\"\n\tcase OpAddMultipartPart:\n\t\tm = \"OpAddMultipartPart\"\n\tcase OpRemoveMultipart:\n\t\tm = \"OpRemoveMultipart\"\n\tcase OpListMultiparts:\n\t\tm = \"OpListMultiparts\"\n\tcase OpBatchDeleteExtent:\n\t\tm = \"OpBatchDeleteExtent\"\n\tcase OpMetaClearInodeCache:\n\t\tm = \"OpMetaClearInodeCache\"\n\tcase OpMetaTxCreateInode:\n\t\tm = \"OpMetaTxCreateInode\"\n\tcase OpMetaTxCreateDentry:\n\t\tm = \"OpMetaTxCreateDentry\"\n\tcase OpTxCommit:\n\t\tm = \"OpTxCommit\"\n\tcase OpMetaTxCreate:\n\t\tm = \"OpMetaTxCreate\"\n\tcase OpTxRollback:\n\t\tm = \"OpTxRollback\"\n\tcase OpTxCommitRM:\n\t\tm = \"OpTxCommitRM\"\n\tcase OpTxRollbackRM:\n\t\tm = \"OpTxRollbackRM\"\n\tcase OpMetaTxDeleteDentry:\n\t\tm = \"OpMetaTxDeleteDentry\"\n\tcase OpMetaTxUnlinkInode:\n\t\tm = \"OpMetaTxUnlinkInode\"\n\tcase OpMetaTxUpdateDentry:\n\t\tm = \"OpMetaTxUpdateDentry\"\n\tcase OpMetaTxLinkInode:\n\t\tm = \"OpMetaTxLinkInode\"\n\tcase OpMetaTxGet:\n\t\tm = \"OpMetaTxGet\"\n\tcase OpMetaBatchSetInodeQuota:\n\t\tm = \"OpMetaBatchSetInodeQuota\"\n\tcase OpMetaBatchDeleteInodeQuota:\n\t\tm = \"OpMetaBatchDeleteInodeQuota\"\n\tcase OpMetaGetInodeQuota:\n\t\tm = \"OpMetaGetInodeQuota\"\n\tcase OpStopDataPartitionRepair:\n\t\tm = \"OpStopDataPartitionRepair\"\n\tcase OpLcNodeHeartbeat:\n\t\tm = \"OpLcNodeHeartbeat\"\n\tcase OpLcNodeScan:\n\t\tm = \"OpLcNodeScan\"\n\tcase OpLcNodeSnapshotVerDel:\n\t\tm = \"OpLcNodeSnapshotVerDel\"\n\tdefault:\n\t\tm = fmt.Sprintf(\"op:%v not found\", p.Opcode)\n\t}\n\treturn\n}\n\nfunc GetStatusStr(status uint8) string {\n\tpkt := &Packet{}\n\tpkt.ResultCode = status\n\treturn pkt.GetResultMsg()\n}\n\n// GetResultMsg returns the result message.\nfunc (p *Packet) GetResultMsg() (m string) {\n\tif p == nil {\n\t\treturn \"\"\n\t}\n\n\tswitch p.ResultCode {\n\tcase OpConflictExtentsErr:\n\t\tm = \"ConflictExtentsErr\"\n\tcase OpIntraGroupNetErr:\n\t\tm = \"IntraGroupNetErr\"\n\tcase OpDiskNoSpaceErr:\n\t\tm = \"DiskNoSpaceErr\"\n\tcase OpDiskErr:\n\t\tm = \"DiskErr\"\n\tcase OpErr:\n\t\tm = \"Err: \" + string(p.Data)\n\tcase OpAgain:\n\t\tm = \"Again: \" + string(p.Data)\n\tcase OpOk:\n\t\tm = \"Ok\"\n\tcase OpExistErr:\n\t\tm = \"ExistErr\"\n\tcase OpInodeFullErr:\n\t\tm = \"InodeFullErr\"\n\tcase OpArgMismatchErr:\n\t\tm = \"ArgUnmatchErr\"\n\tcase OpNotExistErr:\n\t\tm = \"NotExistErr\"\n\tcase OpTryOtherAddr:\n\t\tm = \"TryOtherAddr\"\n\tcase OpNotPerm:\n\t\tm = \"NotPerm\"\n\tcase OpNotEmpty:\n\t\tm = \"DirNotEmpty\"\n\tcase OpDirQuota:\n\t\tm = \"OpDirQuota\"\n\tcase OpNoSpaceErr:\n\t\tm = \"NoSpaceErr\"\n\tcase OpTxInodeInfoNotExistErr:\n\t\tm = \"OpTxInodeInfoNotExistErr\"\n\tcase OpTxConflictErr:\n\t\tm = \"TransactionConflict\"\n\tcase OpTxDentryInfoNotExistErr:\n\t\tm = \"OpTxDentryInfoNotExistErr\"\n\tcase OpTxRbInodeNotExistErr:\n\t\tm = \"OpTxRbInodeNotExistEr\"\n\tcase OpTxRbDentryNotExistErr:\n\t\tm = \"OpTxRbDentryNotExistEr\"\n\tcase OpTxInfoNotExistErr:\n\t\tm = \"OpTxInfoNotExistErr\"\n\tcase OpTxInternalErr:\n\t\tm = \"OpTxInternalErr\"\n\tcase OpTxCommitItemErr:\n\t\tm = \"OpTxCommitItemErr\"\n\tcase OpTxRollbackItemErr:\n\t\tm = \"OpTxRollbackItemErr\"\n\tcase OpTxRollbackUnknownRbType:\n\t\tm = \"OpTxRollbackUnknownRbType\"\n\tcase OpTxTimeoutErr:\n\t\tm = \"OpTxTimeoutErr\"\n\tcase OpTxSetStateErr:\n\t\tm = \"OpTxSetStateErr\"\n\tcase OpTxCommitErr:\n\t\tm = \"OpTxCommitErr\"\n\tcase OpTxRollbackErr:\n\t\tm = \"OpTxRollbackErr\"\n\tcase OpUploadPartConflictErr:\n\t\tm = \"OpUploadPartConflictErr\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"Unknown ResultCode(%v)\", p.ResultCode)\n\t}\n\treturn\n}\n\nfunc (p *Packet) GetReqID() int64 {\n\treturn p.ReqID\n}\n\n// MarshalHeader marshals the packet header.\nfunc (p *Packet) MarshalHeader(out []byte) {\n\tout[0] = p.Magic\n\tout[1] = p.ExtentType\n\tout[2] = p.Opcode\n\tout[3] = p.ResultCode\n\tout[4] = p.RemainingFollowers\n\tbinary.BigEndian.PutUint32(out[5:9], p.CRC)\n\tbinary.BigEndian.PutUint32(out[9:13], p.Size)\n\tbinary.BigEndian.PutUint32(out[13:17], p.ArgLen)\n\tbinary.BigEndian.PutUint64(out[17:25], p.PartitionID)\n\tbinary.BigEndian.PutUint64(out[25:33], p.ExtentID)\n\tbinary.BigEndian.PutUint64(out[33:41], uint64(p.ExtentOffset))\n\tbinary.BigEndian.PutUint64(out[41:49], uint64(p.ReqID))\n\tbinary.BigEndian.PutUint64(out[49:util.PacketHeaderSize], p.KernelOffset)\n\tif p.Opcode == OpRandomWriteVer || p.ExtentType&MultiVersionFlag > 0 {\n\t\tbinary.BigEndian.PutUint64(out[util.PacketHeaderSize:util.PacketHeaderSize+8], p.VerSeq)\n\t}\n\treturn\n}\n\nfunc (p *Packet) IsVersionList() bool {\n\tif p.ExtentType&VersionListFlag == VersionListFlag {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// UnmarshalHeader unmarshals the packet header.\nfunc (p *Packet) UnmarshalHeader(in []byte) error {\n\tp.Magic = in[0]\n\tif p.Magic != ProtoMagic {\n\t\treturn errors.New(\"Bad Magic \" + strconv.Itoa(int(p.Magic)))\n\t}\n\n\tp.ExtentType = in[1]\n\tp.Opcode = in[2]\n\tp.ResultCode = in[3]\n\tp.RemainingFollowers = in[4]\n\tp.CRC = binary.BigEndian.Uint32(in[5:9])\n\tp.Size = binary.BigEndian.Uint32(in[9:13])\n\tp.ArgLen = binary.BigEndian.Uint32(in[13:17])\n\tp.PartitionID = binary.BigEndian.Uint64(in[17:25])\n\tp.ExtentID = binary.BigEndian.Uint64(in[25:33])\n\tp.ExtentOffset = int64(binary.BigEndian.Uint64(in[33:41]))\n\tp.ReqID = int64(binary.BigEndian.Uint64(in[41:49]))\n\tp.KernelOffset = binary.BigEndian.Uint64(in[49:util.PacketHeaderSize])\n\n\t// header opcode OpRandomWriteVer should not unmarshal here due to header size is const\n\t// the ver param should read at the higher level directly\n\t//if p.Opcode ==OpRandomWriteVer {\n\n\treturn nil\n}\n\nconst verInfoCnt = 17\n\nfunc (p *Packet) MarshalVersionSlice() (data []byte, err error) {\n\titems := p.VerList\n\tcnt := len(items)\n\tbuff := bytes.NewBuffer(make([]byte, 0, 2*cnt*verInfoCnt))\n\tif err := binary.Write(buff, binary.BigEndian, uint16(cnt)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, v := range items {\n\t\tif err := binary.Write(buff, binary.BigEndian, v.Ver); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := binary.Write(buff, binary.BigEndian, v.DelTime); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := binary.Write(buff, binary.BigEndian, v.Status); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn buff.Bytes(), nil\n}\n\nfunc (p *Packet) UnmarshalVersionSlice(cnt int, d []byte) error {\n\titems := make([]*VolVersionInfo, 0)\n\tbuf := bytes.NewBuffer(d)\n\tvar err error\n\n\tfor idx := 0; idx < cnt; idx++ {\n\t\te := &VolVersionInfo{}\n\t\terr = binary.Read(buf, binary.BigEndian, &e.Ver)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = binary.Read(buf, binary.BigEndian, &e.DelTime)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = binary.Read(buf, binary.BigEndian, &e.Status)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\titems = append(items, e)\n\t}\n\tp.VerList = items\n\treturn nil\n}\n\n// MarshalData marshals the packet data.\nfunc (p *Packet) MarshalData(v interface{}) error {\n\tdata, err := json.Marshal(v)\n\tif err == nil {\n\t\tp.Data = data\n\t\tp.Size = uint32(len(p.Data))\n\t}\n\treturn err\n}\n\n// UnmarshalData unmarshals the packet data.\nfunc (p *Packet) UnmarshalData(v interface{}) error {\n\treturn json.Unmarshal(p.Data, v)\n}\n\n// WriteToNoDeadLineConn writes through the connection without deadline.\nfunc (p *Packet) WriteToNoDeadLineConn(c net.Conn) (err error) {\n\theader, err := Buffers.Get(util.PacketHeaderSize)\n\tif err != nil {\n\t\theader = make([]byte, util.PacketHeaderSize)\n\t}\n\tdefer Buffers.Put(header)\n\n\tp.MarshalHeader(header)\n\tif _, err = c.Write(header); err == nil {\n\t\tif _, err = c.Write(p.Arg[:int(p.ArgLen)]); err == nil {\n\t\t\tif p.Data != nil {\n\t\t\t\t_, err = c.Write(p.Data[:p.Size])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\n// WriteToConn writes through the given connection.\nfunc (p *Packet) WriteToConn(c net.Conn) (err error) {\n\theadSize := util.PacketHeaderSize\n\tif p.Opcode == OpRandomWriteVer || p.ExtentType&MultiVersionFlag > 0 {\n\t\theadSize = util.PacketHeaderVerSize\n\t}\n\t//log.LogDebugf(\"packet opcode %v header size %v extentype %v conn %v\", p.Opcode, headSize, p.ExtentType, c)\n\theader, err := Buffers.Get(headSize)\n\tif err != nil {\n\t\theader = make([]byte, headSize)\n\t}\n\t// log.LogErrorf(\"action[WriteToConn] buffer get nil,opcode %v head len [%v]\", p.Opcode, len(header))\n\tdefer Buffers.Put(header)\n\tc.SetWriteDeadline(time.Now().Add(WriteDeadlineTime * time.Second))\n\tp.MarshalHeader(header)\n\tif _, err = c.Write(header); err == nil {\n\t\t// write dir version info.\n\t\tif p.IsVersionList() {\n\t\t\td, err1 := p.MarshalVersionSlice()\n\t\t\tif err1 != nil {\n\t\t\t\tlog.LogErrorf(\"MarshalVersionSlice: marshal version ifo failed, err %s\", err1.Error())\n\t\t\t\treturn err1\n\t\t\t}\n\n\t\t\t_, err = c.Write(d)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif _, err = c.Write(p.Arg[:int(p.ArgLen)]); err == nil {\n\t\t\tif p.Data != nil && p.Size != 0 {\n\t\t\t\t_, err = c.Write(p.Data[:p.Size])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\n// ReadFull is a wrapper function of io.ReadFull.\nfunc ReadFull(c net.Conn, buf *[]byte, readSize int) (err error) {\n\t*buf = make([]byte, readSize)\n\t_, err = io.ReadFull(c, (*buf)[:readSize])\n\treturn\n}\n\n// ReadFromConn reads the data from the given connection.\n// Recognize the version bit and parse out version,\n// to avoid version field rsp back , the rsp of random write from datanode with replace OpRandomWriteVer to OpRandomWriteVerRsp\nfunc (p *Packet) ReadFromConnWithVer(c net.Conn, timeoutSec int) (err error) {\n\tif timeoutSec != NoReadDeadlineTime {\n\t\tc.SetReadDeadline(time.Now().Add(time.Second * time.Duration(timeoutSec)))\n\t} else {\n\t\tc.SetReadDeadline(time.Time{})\n\t}\n\n\theader, err := Buffers.Get(util.PacketHeaderSize)\n\tif err != nil {\n\t\theader = make([]byte, util.PacketHeaderSize)\n\t}\n\tdefer Buffers.Put(header)\n\tvar n int\n\tif n, err = io.ReadFull(c, header); err != nil {\n\t\treturn\n\t}\n\tif n != util.PacketHeaderSize {\n\t\treturn syscall.EBADMSG\n\t}\n\tif err = p.UnmarshalHeader(header); err != nil {\n\t\treturn\n\t}\n\n\tif p.ExtentType&MultiVersionFlag > 0 {\n\t\tver := make([]byte, 8)\n\t\tif _, err = io.ReadFull(c, ver); err != nil {\n\t\t\treturn\n\t\t}\n\t\tp.VerSeq = binary.BigEndian.Uint64(ver)\n\t}\n\n\tif p.IsVersionList() {\n\t\tcntByte := make([]byte, 2)\n\t\tif _, err = io.ReadFull(c, cntByte); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcnt := binary.BigEndian.Uint16(cntByte)\n\t\tlog.LogDebugf(\"action[ReadFromConnWithVer] op %s verseq %v, extType %d, cnt %d\",\n\t\t\tp.GetOpMsg(), p.VerSeq, p.ExtentType, cnt)\n\t\tverData := make([]byte, cnt*verInfoCnt)\n\t\tif _, err = io.ReadFull(c, verData); err != nil {\n\t\t\tlog.LogWarnf(\"ReadFromConnWithVer: read ver slice from conn failed, err %s\", err.Error())\n\t\t\treturn err\n\t\t}\n\n\t\terr = p.UnmarshalVersionSlice(int(cnt), verData)\n\t\tif err != nil {\n\t\t\tlog.LogWarnf(\"ReadFromConnWithVer: unmarshal ver slice failed, err %s\", err.Error())\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif p.ArgLen > 0 {\n\t\tp.Arg = make([]byte, int(p.ArgLen))\n\t\tif _, err = io.ReadFull(c, p.Arg[:int(p.ArgLen)]); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif p.Size < 0 {\n\t\treturn syscall.EBADMSG\n\t}\n\tsize := p.Size\n\tif (p.Opcode == OpRead || p.Opcode == OpStreamRead || p.Opcode == OpExtentRepairRead || p.Opcode == OpStreamFollowerRead) && p.ResultCode == OpInitResultCode {\n\t\tsize = 0\n\t}\n\tp.Data = make([]byte, size)\n\tif n, err = io.ReadFull(c, p.Data[:size]); err != nil {\n\t\treturn err\n\t}\n\tif n != int(size) {\n\t\treturn syscall.EBADMSG\n\t}\n\treturn nil\n}\n\n// ReadFromConn reads the data from the given connection.\nfunc (p *Packet) ReadFromConn(c net.Conn, timeoutSec int) (err error) {\n\tif timeoutSec != NoReadDeadlineTime {\n\t\tc.SetReadDeadline(time.Now().Add(time.Second * time.Duration(timeoutSec)))\n\t} else {\n\t\tc.SetReadDeadline(time.Time{})\n\t}\n\theader, err := Buffers.Get(util.PacketHeaderSize)\n\tif err != nil {\n\t\theader = make([]byte, util.PacketHeaderSize)\n\t}\n\tdefer Buffers.Put(header)\n\tvar n int\n\tif n, err = io.ReadFull(c, header); err != nil {\n\t\treturn\n\t}\n\tif n != util.PacketHeaderSize {\n\t\treturn syscall.EBADMSG\n\t}\n\tif err = p.UnmarshalHeader(header); err != nil {\n\t\treturn\n\t}\n\n\tif p.ArgLen > 0 {\n\t\tp.Arg = make([]byte, int(p.ArgLen))\n\t\tif _, err = io.ReadFull(c, p.Arg[:int(p.ArgLen)]); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif p.Size < 0 {\n\t\treturn syscall.EBADMSG\n\t}\n\tsize := p.Size\n\tif (p.Opcode == OpRead || p.Opcode == OpStreamRead || p.Opcode == OpExtentRepairRead || p.Opcode == OpStreamFollowerRead) && p.ResultCode == OpInitResultCode {\n\t\tsize = 0\n\t}\n\tp.Data = make([]byte, size)\n\tif n, err = io.ReadFull(c, p.Data[:size]); err != nil {\n\t\treturn err\n\t}\n\tif n != int(size) {\n\t\treturn syscall.EBADMSG\n\t}\n\treturn nil\n}\n\n// PacketOkReply sets the result code as OpOk, and sets the body as empty.\nfunc (p *Packet) PacketOkReply() {\n\tp.ResultCode = OpOk\n\tp.Size = 0\n\tp.Data = nil\n\tp.ArgLen = 0\n}\n\n// PacketOkWithBody sets the result code as OpOk, and sets the body with the give data.\nfunc (p *Packet) PacketOkWithBody(reply []byte) {\n\tp.Size = uint32(len(reply))\n\tp.Data = make([]byte, p.Size)\n\tcopy(p.Data[:p.Size], reply)\n\tp.ResultCode = OpOk\n\tp.ArgLen = 0\n}\n\n// attention use for tmp byte arr, eg: json marshal data\nfunc (p *Packet) PacketOkWithByte(reply []byte) {\n\tp.Size = uint32(len(reply))\n\tp.Data = reply\n\tp.ResultCode = OpOk\n\tp.ArgLen = 0\n}\n\n// PacketErrorWithBody sets the packet with error code whose body is filled with the given data.\nfunc (p *Packet) PacketErrorWithBody(code uint8, reply []byte) {\n\tp.Size = uint32(len(reply))\n\tp.Data = make([]byte, p.Size)\n\tcopy(p.Data[:p.Size], reply)\n\tp.ResultCode = code\n\tp.ArgLen = 0\n}\n\nfunc (p *Packet) SetPacketHasPrepare() {\n\tp.setPacketPrefix()\n\tp.HasPrepare = true\n}\n\nfunc (p *Packet) SetPacketRePrepare() {\n\tp.HasPrepare = false\n}\n\nfunc (p *Packet) AddMesgLog(m string) {\n\tp.mesg += m\n}\n\n// GetUniqueLogId returns the unique log ID.\nfunc (p *Packet) GetUniqueLogId() (m string) {\n\tdefer func() {\n\t\tm = m + fmt.Sprintf(\"_ResultMesg(%v)\", p.GetResultMsg())\n\t}()\n\tif p.HasPrepare {\n\t\tm = p.mesg\n\t\treturn\n\t}\n\tm = fmt.Sprintf(\"Req(%v)_Partition(%v)_\", p.ReqID, p.PartitionID)\n\tif p.Opcode == OpSplitMarkDelete || (IsTinyExtentType(p.ExtentType) && p.Opcode == OpMarkDelete) && len(p.Data) > 0 {\n\t\text := new(TinyExtentDeleteRecord)\n\t\terr := json.Unmarshal(p.Data, ext)\n\t\tif err == nil {\n\t\t\tm += fmt.Sprintf(\"Extent(%v)_ExtentOffset(%v)_Size(%v)_Opcode(%v)\",\n\t\t\t\text.ExtentId, ext.ExtentOffset, ext.Size, p.GetOpMsg())\n\t\t\treturn m\n\t\t}\n\t} else if p.Opcode == OpReadTinyDeleteRecord || p.Opcode == OpNotifyReplicasToRepair || p.Opcode == OpDataNodeHeartbeat ||\n\t\tp.Opcode == OpLoadDataPartition || p.Opcode == OpBatchDeleteExtent {\n\t\tp.mesg += fmt.Sprintf(\"Opcode(%v)\", p.GetOpMsg())\n\t\treturn\n\t} else if p.Opcode == OpBroadcastMinAppliedID || p.Opcode == OpGetAppliedId {\n\t\tif p.Size > 0 {\n\t\t\tapplyID := binary.BigEndian.Uint64(p.Data)\n\t\t\tm += fmt.Sprintf(\"Opcode(%v)_AppliedID(%v)\", p.GetOpMsg(), applyID)\n\t\t} else {\n\t\t\tm += fmt.Sprintf(\"Opcode(%v)\", p.GetOpMsg())\n\t\t}\n\t\treturn m\n\t}\n\tm = fmt.Sprintf(\"Req(%v)_Partition(%v)_Extent(%v)_ExtentOffset(%v)_KernelOffset(%v)_\"+\n\t\t\"Size(%v)_Opcode(%v)_CRC(%v)\",\n\t\tp.ReqID, p.PartitionID, p.ExtentID, p.ExtentOffset,\n\t\tp.KernelOffset, p.Size, p.GetOpMsg(), p.CRC)\n\n\treturn\n}\n\nfunc (p *Packet) setPacketPrefix() {\n\tp.mesg = fmt.Sprintf(\"Req(%v)_Partition(%v)_\", p.ReqID, p.PartitionID)\n\tif (p.Opcode == OpSplitMarkDelete || (IsTinyExtentType(p.ExtentType) && p.Opcode == OpMarkDelete)) && len(p.Data) > 0 {\n\t\text := new(TinyExtentDeleteRecord)\n\t\terr := json.Unmarshal(p.Data, ext)\n\t\tif err == nil {\n\t\t\tp.mesg += fmt.Sprintf(\"Extent(%v)_ExtentOffset(%v)_Size(%v)_Opcode(%v)\",\n\t\t\t\text.ExtentId, ext.ExtentOffset, ext.Size, p.GetOpMsg())\n\t\t\treturn\n\t\t}\n\t} else if p.Opcode == OpReadTinyDeleteRecord || p.Opcode == OpNotifyReplicasToRepair || p.Opcode == OpDataNodeHeartbeat ||\n\t\tp.Opcode == OpLoadDataPartition || p.Opcode == OpBatchDeleteExtent {\n\t\tp.mesg += fmt.Sprintf(\"Opcode(%v)\", p.GetOpMsg())\n\t\treturn\n\t} else if p.Opcode == OpBroadcastMinAppliedID || p.Opcode == OpGetAppliedId {\n\t\tif p.Size > 0 {\n\t\t\tapplyID := binary.BigEndian.Uint64(p.Data)\n\t\t\tp.mesg += fmt.Sprintf(\"Opcode(%v)_AppliedID(%v)\", p.GetOpMsg(), applyID)\n\t\t} else {\n\t\t\tp.mesg += fmt.Sprintf(\"Opcode(%v)\", p.GetOpMsg())\n\t\t}\n\t\treturn\n\t}\n\tp.mesg = fmt.Sprintf(\"Req(%v)_Partition(%v)_Extent(%v)_ExtentOffset(%v)_KernelOffset(%v)_\"+\n\t\t\"Size(%v)_Opcode(%v)_CRC(%v)\",\n\t\tp.ReqID, p.PartitionID, p.ExtentID, p.ExtentOffset,\n\t\tp.KernelOffset, p.Size, p.GetOpMsg(), p.CRC)\n\n}\n\n// IsForwardPkt returns if the packet is the forward packet (a packet that will be forwarded to the followers).\nfunc (p *Packet) IsForwardPkt() bool {\n\treturn p.RemainingFollowers > 0\n}\n\n// LogMessage logs the given message.\nfunc (p *Packet) LogMessage(action, remote string, start int64, err error) (m string) {\n\tif err == nil {\n\t\tm = fmt.Sprintf(\"id[%v] isPrimaryBackReplLeader[%v] remote[%v] \"+\n\t\t\t\" cost[%v] \", p.GetUniqueLogId(), p.IsForwardPkt(), remote, (time.Now().UnixNano()-start)/1e6)\n\n\t} else {\n\t\tm = fmt.Sprintf(\"id[%v] isPrimaryBackReplLeader[%v] remote[%v]\"+\n\t\t\t\", err[%v]\", p.GetUniqueLogId(), p.IsForwardPkt(), remote, err.Error())\n\t}\n\n\treturn\n}\n\n// ShallRetry returns if we should retry the packet.\nfunc (p *Packet) ShouldRetryWithVersionList() bool {\n\treturn p.ResultCode == OpAgainVerionList\n}\n\n// ShallRetry returns if we should retry the packet.\nfunc (p *Packet) ShouldRetry() bool {\n\treturn p.ResultCode == OpAgain || p.ResultCode == OpErr\n}\n\nfunc (p *Packet) IsBatchDeleteExtents() bool {\n\treturn p.Opcode == OpBatchDeleteExtent\n}\n\nfunc InitBufferPool(bufLimit int64) {\n\tbuf.NormalBuffersTotalLimit = bufLimit\n\tbuf.HeadBuffersTotalLimit = bufLimit\n\tbuf.HeadVerBuffersTotalLimit = bufLimit\n\n\tBuffers = buf.NewBufferPool()\n}\n", "// Copyright 2018 The CubeFS Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n// implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage stream\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"net\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/cubefs/cubefs/proto\"\n\t\"github.com/cubefs/cubefs/sdk/data/wrapper\"\n\t\"github.com/cubefs/cubefs/storage\"\n\t\"github.com/cubefs/cubefs/util\"\n\t\"github.com/cubefs/cubefs/util/errors\"\n\t\"github.com/cubefs/cubefs/util/log\"\n)\n\nconst (\n\tMaxSelectDataPartitionForWrite = 32\n\tMaxNewHandlerRetry             = 3\n\tMaxPacketErrorCount            = 128\n\tMaxDirtyListLen                = 0\n)\n\nconst (\n\tStreamerNormal int32 = iota\n\tStreamerError\n\tLastEKVersionNotEqual\n)\n\nconst (\n\tstreamWriterFlushPeriod       = 3\n\tstreamWriterIdleTimeoutPeriod = 10\n)\n\n// VerUpdateRequest defines an verseq update request.\ntype VerUpdateRequest struct {\n\terr    error\n\tverSeq uint64\n\tdone   chan struct{}\n}\n\n// OpenRequest defines an open request.\ntype OpenRequest struct {\n\tdone chan struct{}\n}\n\n// WriteRequest defines a write request.\ntype WriteRequest struct {\n\tfileOffset int\n\tsize       int\n\tdata       []byte\n\tflags      int\n\twriteBytes int\n\terr        error\n\tdone       chan struct{}\n\tcheckFunc  func() error\n}\n\n// FlushRequest defines a flush request.\ntype FlushRequest struct {\n\terr  error\n\tdone chan struct{}\n}\n\n// ReleaseRequest defines a release request.\ntype ReleaseRequest struct {\n\terr  error\n\tdone chan struct{}\n}\n\n// TruncRequest defines a truncate request.\ntype TruncRequest struct {\n\tsize     int\n\terr      error\n\tfullPath string\n\tdone     chan struct{}\n}\n\n// EvictRequest defines an evict request.\ntype EvictRequest struct {\n\terr  error\n\tdone chan struct{}\n}\n\n// Open request shall grab the lock until request is sent to the request channel\nfunc (s *Streamer) IssueOpenRequest() error {\n\trequest := openRequestPool.Get().(*OpenRequest)\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\ts.client.streamerLock.Unlock()\n\t<-request.done\n\topenRequestPool.Put(request)\n\treturn nil\n}\n\nfunc (s *Streamer) IssueWriteRequest(offset int, data []byte, flags int, checkFunc func() error) (write int, err error) {\n\tif atomic.LoadInt32(&s.status) >= StreamerError {\n\t\treturn 0, errors.New(fmt.Sprintf(\"IssueWriteRequest: stream writer in error status, ino(%v)\", s.inode))\n\t}\n\n\ts.writeLock.Lock()\n\trequest := writeRequestPool.Get().(*WriteRequest)\n\trequest.data = data\n\trequest.fileOffset = offset\n\trequest.size = len(data)\n\trequest.flags = flags\n\trequest.done = make(chan struct{}, 1)\n\trequest.checkFunc = checkFunc\n\n\ts.request <- request\n\ts.writeLock.Unlock()\n\n\t<-request.done\n\terr = request.err\n\twrite = request.writeBytes\n\twriteRequestPool.Put(request)\n\treturn\n}\n\nfunc (s *Streamer) IssueFlushRequest() error {\n\trequest := flushRequestPool.Get().(*FlushRequest)\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\t<-request.done\n\terr := request.err\n\tflushRequestPool.Put(request)\n\treturn err\n}\n\nfunc (s *Streamer) IssueReleaseRequest() error {\n\trequest := releaseRequestPool.Get().(*ReleaseRequest)\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\ts.client.streamerLock.Unlock()\n\t<-request.done\n\terr := request.err\n\treleaseRequestPool.Put(request)\n\treturn err\n}\n\nfunc (s *Streamer) IssueTruncRequest(size int, fullPath string) error {\n\trequest := truncRequestPool.Get().(*TruncRequest)\n\trequest.size = size\n\trequest.fullPath = fullPath\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\t<-request.done\n\terr := request.err\n\ttruncRequestPool.Put(request)\n\treturn err\n}\n\nfunc (s *Streamer) IssueEvictRequest() error {\n\trequest := evictRequestPool.Get().(*EvictRequest)\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\ts.client.streamerLock.Unlock()\n\t<-request.done\n\terr := request.err\n\tevictRequestPool.Put(request)\n\treturn err\n}\n\nfunc (s *Streamer) GetStoreMod(offset int, size int) (storeMode int) {\n\t// Small files are usually written in a single write, so use tiny extent\n\t// store only for the first write operation.\n\tif offset > 0 || offset+size > s.tinySizeLimit() {\n\t\tstoreMode = proto.NormalExtentType\n\t} else {\n\t\tstoreMode = proto.TinyExtentType\n\t}\n\treturn\n}\n\nfunc (s *Streamer) server() {\n\tt := time.NewTicker(2 * time.Second)\n\tdefer t.Stop()\n\t//defer func() {\n\t//\tif !s.client.disableMetaCache && s.needBCache {\n\t//\t\tclose(s.request)\n\t//\t\ts.request = nil\n\t//\t}\n\t//}()\n\n\tfor {\n\t\tselect {\n\t\tcase request := <-s.request:\n\t\t\ts.handleRequest(request)\n\t\t\ts.idle = 0\n\t\t\ts.traversed = 0\n\t\tcase <-s.done:\n\t\t\ts.abort()\n\t\t\tlog.LogDebugf(\"done server: evict, ino(%v)\", s.inode)\n\t\t\treturn\n\t\tcase <-t.C:\n\t\t\ts.traverse()\n\t\t\tif s.refcnt <= 0 {\n\n\t\t\t\ts.client.streamerLock.Lock()\n\t\t\t\tif s.idle >= streamWriterIdleTimeoutPeriod && len(s.request) == 0 {\n\t\t\t\t\tif s.client.disableMetaCache || !s.needBCache {\n\t\t\t\t\t\tdelete(s.client.streamers, s.inode)\n\t\t\t\t\t\tif s.client.evictIcache != nil {\n\t\t\t\t\t\t\ts.client.evictIcache(s.inode)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\ts.isOpen = false\n\t\t\t\t\t// fail the remaining requests in such case\n\t\t\t\t\ts.clearRequests()\n\t\t\t\t\ts.client.streamerLock.Unlock()\n\n\t\t\t\t\tlog.LogDebugf(\"done server: no requests for a long time, ino(%v)\", s.inode)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ts.client.streamerLock.Unlock()\n\n\t\t\t\ts.idle++\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (s *Streamer) clearRequests() {\n\tfor {\n\t\tselect {\n\t\tcase request := <-s.request:\n\t\t\ts.abortRequest(request)\n\t\tdefault:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (s *Streamer) abortRequest(request interface{}) {\n\tswitch request := request.(type) {\n\tcase *OpenRequest:\n\t\trequest.done <- struct{}{}\n\tcase *WriteRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tcase *TruncRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tcase *FlushRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tcase *ReleaseRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tcase *EvictRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tdefault:\n\t}\n}\n\nfunc (s *Streamer) handleRequest(request interface{}) {\n\tif atomic.LoadInt32(&s.needUpdateVer) == 1 {\n\t\ts.closeOpenHandler()\n\t\tatomic.StoreInt32(&s.needUpdateVer, 0)\n\t}\n\n\tswitch request := request.(type) {\n\tcase *OpenRequest:\n\t\ts.open()\n\t\trequest.done <- struct{}{}\n\tcase *WriteRequest:\n\t\trequest.writeBytes, request.err = s.write(request.data, request.fileOffset, request.size, request.flags, request.checkFunc)\n\t\trequest.done <- struct{}{}\n\tcase *TruncRequest:\n\t\trequest.err = s.truncate(request.size, request.fullPath)\n\t\trequest.done <- struct{}{}\n\tcase *FlushRequest:\n\t\trequest.err = s.flush()\n\t\trequest.done <- struct{}{}\n\tcase *ReleaseRequest:\n\t\trequest.err = s.release()\n\t\trequest.done <- struct{}{}\n\tcase *EvictRequest:\n\t\trequest.err = s.evict()\n\t\trequest.done <- struct{}{}\n\tcase *VerUpdateRequest:\n\t\trequest.err = s.updateVer(request.verSeq)\n\t\trequest.done <- struct{}{}\n\tdefault:\n\t}\n\n}\n\nfunc (s *Streamer) write(data []byte, offset, size, flags int, checkFunc func() error) (total int, err error) {\n\tvar (\n\t\tdirect     bool\n\t\tretryTimes int8\n\t)\n\n\tif flags&proto.FlagsSyncWrite != 0 {\n\t\tdirect = true\n\t}\nbegin:\n\tif flags&proto.FlagsAppend != 0 {\n\t\tfilesize, _ := s.extents.Size()\n\t\toffset = filesize\n\t}\n\n\tlog.LogDebugf(\"Streamer write enter: ino(%v) offset(%v) size(%v)\", s.inode, offset, size)\n\n\tctx := context.Background()\n\ts.client.writeLimiter.Wait(ctx)\n\n\trequests := s.extents.PrepareWriteRequests(offset, size, data)\n\tlog.LogDebugf(\"Streamer write: ino(%v) prepared requests(%v)\", s.inode, requests)\n\n\tisChecked := false\n\t// Must flush before doing overwrite\n\tfor _, req := range requests {\n\t\tif req.ExtentKey == nil {\n\t\t\tcontinue\n\t\t}\n\t\terr = s.flush()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\t// some extent key in requests with partition id 0 means it's append operation and on flight.\n\t\t// need to flush and get the right key then used to make modification\n\t\trequests = s.extents.PrepareWriteRequests(offset, size, data)\n\t\tlog.LogDebugf(\"Streamer write: ino(%v) prepared requests after flush(%v)\", s.inode, requests)\n\t\tbreak\n\t}\n\n\tfor _, req := range requests {\n\t\tvar writeSize int\n\t\tif req.ExtentKey != nil {\n\t\t\tif s.client.bcacheEnable {\n\t\t\t\tcacheKey := util.GenerateRepVolKey(s.client.volumeName, s.inode, req.ExtentKey.PartitionId, req.ExtentKey.ExtentId, uint64(req.FileOffset))\n\t\t\t\tif _, ok := s.inflightEvictL1cache.Load(cacheKey); !ok {\n\t\t\t\t\tgo func(cacheKey string) {\n\t\t\t\t\t\ts.inflightEvictL1cache.Store(cacheKey, true)\n\t\t\t\t\t\ts.client.evictBcache(cacheKey)\n\t\t\t\t\t\ts.inflightEvictL1cache.Delete(cacheKey)\n\t\t\t\t\t}(cacheKey)\n\t\t\t\t}\n\t\t\t}\n\t\t\tlog.LogDebugf(\"action[streamer.write] inode [%v] latest seq [%v] extentkey seq [%v]  info [%v]\",\n\t\t\t\ts.inode, s.verSeq, req.ExtentKey.GetSeq(), req.ExtentKey)\n\t\t\tif req.ExtentKey.GetSeq() == s.verSeq {\n\t\t\t\twriteSize, err = s.doOverwrite(req, direct)\n\t\t\t\tif err == proto.ErrCodeVersionOp {\n\t\t\t\t\tlog.LogDebugf(\"action[streamer.write] write need version update\")\n\t\t\t\t\tif err = s.GetExtents(); err != nil {\n\t\t\t\t\t\tlog.LogErrorf(\"action[streamer.write] err %v\", err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif retryTimes > 3 {\n\t\t\t\t\t\terr = proto.ErrCodeVersionOp\n\t\t\t\t\t\tlog.LogWarnf(\"action[streamer.write] err %v\", err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\ttime.Sleep(time.Millisecond * 100)\n\t\t\t\t\tretryTimes++\n\t\t\t\t\tlog.LogDebugf(\"action[streamer.write] err %v retryTimes %v\", err, retryTimes)\n\t\t\t\t\tgoto begin\n\t\t\t\t}\n\t\t\t\tlog.LogDebugf(\"action[streamer.write] err %v retryTimes %v\", err, retryTimes)\n\t\t\t} else {\n\t\t\t\tlog.LogDebugf(\"action[streamer.write] ino %v doOverWriteByAppend extent key (%v)\", s.inode, req.ExtentKey)\n\t\t\t\twriteSize, _, err, _ = s.doOverWriteByAppend(req, direct)\n\t\t\t}\n\t\t\tif s.client.bcacheEnable {\n\t\t\t\tcacheKey := util.GenerateKey(s.client.volumeName, s.inode, uint64(req.FileOffset))\n\t\t\t\tgo s.client.evictBcache(cacheKey)\n\t\t\t}\n\t\t} else {\n\t\t\tif !isChecked && checkFunc != nil {\n\t\t\t\tisChecked = true\n\t\t\t\tif err = checkFunc(); err != nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\t// try append write, get response\n\t\t\tlog.LogDebugf(\"action[streamer.write] doAppendWrite req %v FileOffset %v size %v\", req.ExtentKey, req.FileOffset, req.Size)\n\t\t\tvar status int32\n\t\t\t// First, attempt sequential writes using neighboring extent keys. If the last extent has a different version,\n\t\t\t// it indicates that the extent may have been fully utilized by the previous version.\n\t\t\t// Next, try writing and directly checking the extent at the datanode. If the extent cannot be reused, create a new extent for writing.\n\t\t\tif writeSize, err, status = s.doAppendWrite(req.Data, req.FileOffset, req.Size, direct, true); status == LastEKVersionNotEqual {\n\t\t\t\tlog.LogDebugf(\"action[streamer.write] tryDirectAppendWrite req %v FileOffset %v size %v\", req.ExtentKey, req.FileOffset, req.Size)\n\t\t\t\tif writeSize, _, err, status = s.tryDirectAppendWrite(req, direct); status == int32(proto.OpTryOtherExtent) {\n\t\t\t\t\tlog.LogDebugf(\"action[streamer.write] doAppendWrite again req %v FileOffset %v size %v\", req.ExtentKey, req.FileOffset, req.Size)\n\t\t\t\t\twriteSize, err, _ = s.doAppendWrite(req.Data, req.FileOffset, req.Size, direct, false)\n\t\t\t\t}\n\t\t\t}\n\t\t\tlog.LogDebugf(\"action[streamer.write] doAppendWrite status %v err %v\", status, err)\n\t\t}\n\t\tif err != nil {\n\t\t\tlog.LogErrorf(\"Streamer write: ino(%v) err(%v)\", s.inode, err)\n\t\t\tbreak\n\t\t}\n\t\ttotal += writeSize\n\t}\n\tif filesize, _ := s.extents.Size(); offset+total > filesize {\n\t\ts.extents.SetSize(uint64(offset+total), false)\n\t\tlog.LogDebugf(\"Streamer write: ino(%v) filesize changed to (%v)\", s.inode, offset+total)\n\t}\n\tlog.LogDebugf(\"Streamer write exit: ino(%v) offset(%v) size(%v) done total(%v) err(%v)\", s.inode, offset, size, total, err)\n\treturn\n}\n\nfunc (s *Streamer) doOverWriteByAppend(req *ExtentRequest, direct bool) (total int, extKey *proto.ExtentKey, err error, status int32) {\n\t// the extent key needs to be updated because when preparing the requests,\n\t// the obtained extent key could be a local key which can be inconsistent with the remote key.\n\t// the OpTryWriteAppend is a special case, ignore it\n\treq.ExtentKey = s.extents.Get(uint64(req.FileOffset))\n\treturn s.doDirectWriteByAppend(req, direct, proto.OpRandomWriteAppend)\n}\n\nfunc (s *Streamer) tryDirectAppendWrite(req *ExtentRequest, direct bool) (total int, extKey *proto.ExtentKey, err error, status int32) {\n\n\treq.ExtentKey = s.handler.key\n\treturn s.doDirectWriteByAppend(req, direct, proto.OpTryWriteAppend)\n}\n\nfunc (s *Streamer) doDirectWriteByAppend(req *ExtentRequest, direct bool, op uint8) (total int, extKey *proto.ExtentKey, err error, status int32) {\n\tvar (\n\t\tdp        *wrapper.DataPartition\n\t\treqPacket *Packet\n\t)\n\n\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v enter in req %v\", s.inode, req)\n\terr = s.flush()\n\tif err != nil {\n\t\treturn\n\t}\n\n\tif req.ExtentKey == nil {\n\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: extent key not exist, ino(%v) ekFileOffset(%v) ek(%v)\", s.inode, req.FileOffset, req.ExtentKey))\n\t\treturn\n\t}\n\n\tif dp, err = s.client.dataWrapper.GetDataPartition(req.ExtentKey.PartitionId); err != nil {\n\t\t// TODO unhandled error\n\t\terrors.Trace(err, \"doDirectWriteByAppend: ino(%v) failed to get datapartition, ek(%v)\", s.inode, req.ExtentKey)\n\t\treturn\n\t}\n\n\tretry := true\n\tif proto.IsCold(s.client.volumeType) {\n\t\tretry = false\n\t}\n\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v  data process\", s.inode)\n\n\taddr := dp.LeaderAddr\n\tif storage.IsTinyExtent(req.ExtentKey.ExtentId) {\n\t\taddr = dp.Hosts[0]\n\t\treqPacket = NewWriteTinyDirectly(s.inode, req.ExtentKey.PartitionId, req.FileOffset, dp)\n\t} else {\n\t\treqPacket = NewOverwriteByAppendPacket(dp, req.ExtentKey.ExtentId, int(req.ExtentKey.ExtentOffset)+int(req.ExtentKey.Size),\n\t\t\ts.inode, req.FileOffset, direct, op)\n\t}\n\n\tsc := &StreamConn{\n\t\tdp:       dp,\n\t\tcurrAddr: addr,\n\t}\n\n\treplyPacket := new(Packet)\n\tif req.Size > util.BlockSize {\n\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] inode %v size too large %v\", s.inode, req.Size)\n\t\tpanic(nil)\n\t}\n\tfor total < req.Size { // normally should only run once due to key exist in the system must be less than BlockSize\n\t\t// right position in extent:offset-ek4FileOffset+total+ekExtOffset .\n\t\t// ekExtOffset will be set by replay packet at addExtentInfo(datanode)\n\n\t\tif direct {\n\t\t\treqPacket.Opcode = op\n\t\t}\n\t\tif req.ExtentKey.ExtentId <= storage.TinyExtentCount {\n\t\t\treqPacket.ExtentType = proto.TinyExtentType\n\t\t}\n\n\t\tpackSize := util.Min(req.Size-total, util.BlockSize)\n\t\tcopy(reqPacket.Data[:packSize], req.Data[total:total+packSize])\n\t\treqPacket.Size = uint32(packSize)\n\t\treqPacket.CRC = crc32.ChecksumIEEE(reqPacket.Data[:packSize])\n\n\t\terr = sc.Send(&retry, reqPacket, func(conn *net.TCPConn) (error, bool) {\n\t\t\te := replyPacket.ReadFromConnWithVer(conn, proto.ReadDeadlineTime)\n\t\t\tif e != nil {\n\t\t\t\tlog.LogWarnf(\"doDirectWriteByAppend.Stream Writer doOverwrite: ino(%v) failed to read from connect, req(%v) err(%v)\", s.inode, reqPacket, e)\n\t\t\t\t// Upon receiving TryOtherAddrError, other hosts will be retried.\n\t\t\t\treturn TryOtherAddrError, false\n\t\t\t}\n\t\t\tlog.LogDebugf(\"action[doDirectWriteByAppend] ino(%v) get replyPacket opcode %v resultCode %v\", s.inode, replyPacket.Opcode, replyPacket.ResultCode)\n\t\t\tif replyPacket.ResultCode == proto.OpAgain {\n\t\t\t\treturn nil, true\n\t\t\t}\n\n\t\t\tif replyPacket.ResultCode == proto.OpTryOtherExtent {\n\t\t\t\tstatus = int32(proto.OpTryOtherExtent)\n\t\t\t\treturn nil, false\n\t\t\t}\n\n\t\t\tif replyPacket.ResultCode == proto.OpTryOtherAddr {\n\t\t\t\te = TryOtherAddrError\n\t\t\t\tlog.LogDebugf(\"action[doDirectWriteByAppend] data process err %v\", e)\n\t\t\t}\n\t\t\treturn e, false\n\t\t})\n\n\t\tproto.Buffers.Put(reqPacket.Data)\n\t\treqPacket.Data = nil\n\t\tlog.LogDebugf(\"doDirectWriteByAppend: ino(%v) req(%v) reqPacket(%v) err(%v) replyPacket(%v)\", s.inode, req, reqPacket, err, replyPacket)\n\n\t\tif err != nil || replyPacket.ResultCode != proto.OpOk {\n\t\t\tstatus = int32(replyPacket.ResultCode)\n\t\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: failed or reply NOK: err(%v) ino(%v) req(%v) replyPacket(%v)\", err, s.inode, req, replyPacket))\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] data process err %v\", err)\n\t\t\tbreak\n\t\t}\n\n\t\tif !reqPacket.isValidWriteReply(replyPacket) || reqPacket.CRC != replyPacket.CRC {\n\t\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: is not the corresponding reply, ino(%v) req(%v) replyPacket(%v)\", s.inode, req, replyPacket))\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] data process err %v\", err)\n\t\t\tbreak\n\t\t}\n\n\t\ttotal += packSize\n\t\tbreak\n\t}\n\tif err != nil {\n\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] data process err %v\", err)\n\t\treturn\n\t}\n\textKey = &proto.ExtentKey{\n\t\tFileOffset:   uint64(req.FileOffset),\n\t\tPartitionId:  req.ExtentKey.PartitionId,\n\t\tExtentId:     replyPacket.ExtentID,\n\t\tExtentOffset: uint64(replyPacket.ExtentOffset),\n\t\tSize:         uint32(total),\n\t\tSnapInfo: &proto.ExtSnapInfo{\n\t\t\tVerSeq: s.verSeq,\n\t\t},\n\t}\n\tif op == proto.OpRandomWriteAppend || op == proto.OpSyncRandomWriteAppend {\n\t\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v local cache process start extKey %v\", s.inode, extKey)\n\t\tif err = s.extents.SplitExtentKey(s.inode, extKey); err != nil {\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] inode %v llocal cache process err %v\", s.inode, err)\n\t\t\treturn\n\t\t}\n\t\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v meta extent split with ek (%v)\", s.inode, extKey)\n\t\tif err = s.client.splitExtentKey(s.parentInode, s.inode, *extKey); err != nil {\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] inode %v meta extent split process err %v\", s.inode, err)\n\t\t\treturn\n\t\t}\n\t} else {\n\t\tdiscards := s.extents.Append(extKey, true)\n\t\tif err = s.client.appendExtentKey(s.parentInode, s.inode, *extKey, discards); err != nil {\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] inode %v meta extent split process err %v\", s.inode, err)\n\t\t\treturn\n\t\t}\n\t}\n\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v process over!\", s.inode)\n\treturn\n}\n\nfunc (s *Streamer) doOverwrite(req *ExtentRequest, direct bool) (total int, err error) {\n\tvar dp *wrapper.DataPartition\n\n\terr = s.flush()\n\tif err != nil {\n\t\treturn\n\t}\n\n\toffset := req.FileOffset\n\tsize := req.Size\n\n\t// the extent key needs to be updated because when preparing the requests,\n\t// the obtained extent key could be a local key which can be inconsistent with the remote key.\n\treq.ExtentKey = s.extents.Get(uint64(offset))\n\tekFileOffset := int(req.ExtentKey.FileOffset)\n\tekExtOffset := int(req.ExtentKey.ExtentOffset)\n\tif req.ExtentKey == nil {\n\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: extent key not exist, ino(%v) ekFileOffset(%v) ek(%v)\", s.inode, ekFileOffset, req.ExtentKey))\n\t\treturn\n\t}\n\n\tif dp, err = s.client.dataWrapper.GetDataPartition(req.ExtentKey.PartitionId); err != nil {\n\t\t// TODO unhandled error\n\t\terrors.Trace(err, \"doOverwrite: ino(%v) failed to get datapartition, ek(%v)\", s.inode, req.ExtentKey)\n\t\treturn\n\t}\n\n\tretry := true\n\tif proto.IsCold(s.client.volumeType) {\n\t\tretry = false\n\t}\n\n\tsc := NewStreamConn(dp, false)\n\n\tfor total < size {\n\t\treqPacket := NewOverwritePacket(dp, req.ExtentKey.ExtentId, offset-ekFileOffset+total+ekExtOffset, s.inode, offset)\n\t\treqPacket.VerSeq = s.client.multiVerMgr.latestVerSeq\n\t\treqPacket.VerList = s.client.multiVerMgr.verList.VerList\n\t\treqPacket.ExtentType |= proto.MultiVersionFlag\n\t\treqPacket.ExtentType |= proto.VersionListFlag\n\n\t\tlog.LogDebugf(\"action[doOverwrite] inode %v extentid %v,extentOffset %v(%v,%v,%v,%v) offset %v, streamer seq %v\", s.inode, req.ExtentKey.ExtentId, reqPacket.ExtentOffset,\n\t\t\toffset, ekFileOffset, total, ekExtOffset, offset, s.verSeq)\n\t\tif direct {\n\t\t\treqPacket.Opcode = proto.OpSyncRandomWrite\n\t\t}\n\t\tpackSize := util.Min(size-total, util.BlockSize)\n\t\tcopy(reqPacket.Data[:packSize], req.Data[total:total+packSize])\n\t\treqPacket.Size = uint32(packSize)\n\t\treqPacket.CRC = crc32.ChecksumIEEE(reqPacket.Data[:packSize])\n\t\treqPacket.VerSeq = s.verSeq\n\n\t\treplyPacket := new(Packet)\n\t\terr = sc.Send(&retry, reqPacket, func(conn *net.TCPConn) (error, bool) {\n\t\t\te := replyPacket.ReadFromConnWithVer(conn, proto.ReadDeadlineTime)\n\t\t\tif e != nil {\n\t\t\t\tlog.LogWarnf(\"Stream Writer doOverwrite: ino(%v) failed to read from connect, req(%v) err(%v)\", s.inode, reqPacket, e)\n\t\t\t\t// Upon receiving TryOtherAddrError, other hosts will be retried.\n\t\t\t\treturn TryOtherAddrError, false\n\t\t\t}\n\t\t\tlog.LogDebugf(\"action[doOverwrite] streamer verseq (%v) datanode rsp seq (%v) code(%v)\", s.verSeq, replyPacket.VerSeq, replyPacket.ResultCode)\n\t\t\tif replyPacket.ResultCode == proto.OpAgain {\n\t\t\t\treturn nil, true\n\t\t\t}\n\n\t\t\tif replyPacket.ResultCode == proto.OpTryOtherAddr {\n\t\t\t\te = TryOtherAddrError\n\t\t\t}\n\n\t\t\tif replyPacket.ResultCode == proto.ErrCodeVersionOpError {\n\t\t\t\te = proto.ErrCodeVersionOp\n\t\t\t\tlog.LogWarnf(\"action[doOverwrite] verseq (%v) be updated to (%v) by datanode rsp\", s.verSeq, replyPacket.VerSeq)\n\t\t\t\ts.verSeq = replyPacket.VerSeq\n\t\t\t\ts.extents.verSeq = s.verSeq\n\t\t\t\ts.client.UpdateLatestVer(&proto.VolVersionInfoList{VerList: replyPacket.VerList})\n\t\t\t\treturn e, false\n\t\t\t}\n\n\t\t\treturn e, false\n\t\t})\n\n\t\tproto.Buffers.Put(reqPacket.Data)\n\t\treqPacket.Data = nil\n\t\tlog.LogDebugf(\"doOverwrite: ino(%v) req(%v) reqPacket(%v) err(%v) replyPacket(%v)\", s.inode, req, reqPacket, err, replyPacket)\n\n\t\tif err != nil || replyPacket.ResultCode != proto.OpOk {\n\t\t\tif replyPacket.ResultCode == proto.ErrCodeVersionOpError {\n\t\t\t\terr = proto.ErrCodeVersionOp\n\t\t\t\tlog.LogWarnf(\"doOverwrite: need retry.ino(%v) req(%v) reqPacket(%v) err(%v) replyPacket(%v)\", s.inode, req, reqPacket, err, replyPacket)\n\t\t\t\treturn\n\t\t\t}\n\t\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: failed or reply NOK: err(%v) ino(%v) req(%v) replyPacket(%v)\", err, s.inode, req, replyPacket))\n\t\t\tbreak\n\t\t}\n\n\t\tif !reqPacket.isValidWriteReply(replyPacket) || reqPacket.CRC != replyPacket.CRC {\n\t\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: is not the corresponding reply, ino(%v) req(%v) replyPacket(%v)\", s.inode, req, replyPacket))\n\t\t\tbreak\n\t\t}\n\n\t\ttotal += packSize\n\t}\n\treturn\n}\n\nfunc (s *Streamer) tryInitExtentHandlerByLastEk(offset, size int) (isLastEkVerNotEqual bool) {\n\tstoreMode := s.GetStoreMod(offset, size)\n\n\t// && (s.handler == nil || s.handler != nil && s.handler.fileOffset+s.handler.size != offset)  delete ??\n\tif storeMode == proto.NormalExtentType && (s.handler == nil || s.handler != nil && s.handler.fileOffset+s.handler.size != offset) {\n\t\tif currentEK := s.extents.GetEndForAppendWrite(uint64(offset), s.verSeq, false); currentEK != nil && !storage.IsTinyExtent(currentEK.ExtentId) {\n\t\t\tif currentEK.GetSeq() != s.verSeq {\n\t\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk. exist ek seq %v vs request seq %v\", currentEK.GetSeq(), s.verSeq)\n\t\t\t\tisLastEkVerNotEqual = true\n\t\t\t}\n\n\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk: found ek in ExtentCache, extent_id(%v) offset(%v) size(%v), ekoffset(%v) eksize(%v) exist ek seq %v vs request seq %v\",\n\t\t\t\tcurrentEK.ExtentId, offset, size, currentEK.FileOffset, currentEK.Size, currentEK.GetSeq(), s.verSeq)\n\t\t\t_, pidErr := s.client.dataWrapper.GetDataPartition(currentEK.PartitionId)\n\t\t\tif pidErr == nil {\n\t\t\t\tseq := currentEK.GetSeq()\n\t\t\t\tif isLastEkVerNotEqual {\n\t\t\t\t\tseq = s.verSeq\n\t\t\t\t}\n\t\t\t\thandler := NewExtentHandler(s, int(currentEK.FileOffset), storeMode, int(currentEK.Size))\n\t\t\t\thandler.key = &proto.ExtentKey{\n\t\t\t\t\tFileOffset:   currentEK.FileOffset,\n\t\t\t\t\tPartitionId:  currentEK.PartitionId,\n\t\t\t\t\tExtentId:     currentEK.ExtentId,\n\t\t\t\t\tExtentOffset: currentEK.ExtentOffset,\n\t\t\t\t\tSize:         currentEK.Size,\n\t\t\t\t\tSnapInfo: &proto.ExtSnapInfo{\n\t\t\t\t\t\tVerSeq: seq,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\ts.handler = handler\n\t\t\t\ts.dirty = false\n\t\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk: currentEK.PartitionId(%v) found\", currentEK.PartitionId)\n\t\t\t} else {\n\t\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk: currentEK.PartitionId(%v) not found\", currentEK.PartitionId)\n\t\t\t}\n\n\t\t} else {\n\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk: not found ek in ExtentCache, offset(%v) size(%v)\", offset, size)\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (s *Streamer) doAppendWrite(data []byte, offset, size int, direct bool, reUseEk bool) (total int, err error, status int32) {\n\tvar (\n\t\tek        *proto.ExtentKey\n\t\tstoreMode int\n\t)\n\n\t// Small files are usually written in a single write, so use tiny extent\n\t// store only for the first write operation.\n\tstoreMode = s.GetStoreMod(offset, size)\n\n\tlog.LogDebugf(\"doAppendWrite enter: ino(%v) offset(%v) size(%v) storeMode(%v)\", s.inode, offset, size, storeMode)\n\tif proto.IsHot(s.client.volumeType) {\n\t\tif reUseEk {\n\t\t\tif isLastEkVerNotEqual := s.tryInitExtentHandlerByLastEk(offset, size); isLastEkVerNotEqual {\n\t\t\t\tlog.LogDebugf(\"doAppendWrite enter: ino(%v) tryInitExtentHandlerByLastEk worked\", s.inode)\n\t\t\t\tstatus = LastEKVersionNotEqual\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tfor i := 0; i < MaxNewHandlerRetry; i++ {\n\t\t\tif s.handler == nil {\n\t\t\t\ts.handler = NewExtentHandler(s, offset, storeMode, 0)\n\t\t\t\ts.dirty = false\n\t\t\t} else if s.handler.storeMode != storeMode {\n\t\t\t\t// store mode changed, so close open handler and start a new one\n\t\t\t\ts.closeOpenHandler()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tek, err = s.handler.write(data, offset, size, direct)\n\t\t\tif err == nil && ek != nil {\n\t\t\t\tif !s.dirty {\n\t\t\t\t\ts.dirtylist.Put(s.handler)\n\t\t\t\t\ts.dirty = true\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ts.closeOpenHandler()\n\t\t}\n\t} else {\n\t\ts.handler = NewExtentHandler(s, offset, storeMode, 0)\n\t\ts.dirty = false\n\t\tek, err = s.handler.write(data, offset, size, direct)\n\t\tif err == nil && ek != nil {\n\t\t\tif !s.dirty {\n\t\t\t\ts.dirtylist.Put(s.handler)\n\t\t\t\ts.dirty = true\n\t\t\t}\n\t\t}\n\n\t\terr = s.closeOpenHandler()\n\t}\n\n\tif err != nil || ek == nil {\n\t\tlog.LogErrorf(\"doAppendWrite error: ino(%v) offset(%v) size(%v) err(%v) ek(%v)\", s.inode, offset, size, err, ek)\n\t\treturn\n\t}\n\n\t// This ek is just a local cache for PrepareWriteRequest, so ignore discard eks here.\n\t_ = s.extents.Append(ek, false)\n\ttotal = size\n\n\treturn\n}\n\nfunc (s *Streamer) flush() (err error) {\n\tfor {\n\t\telement := s.dirtylist.Get()\n\t\tif element == nil {\n\t\t\tbreak\n\t\t}\n\t\teh := element.Value.(*ExtentHandler)\n\n\t\tlog.LogDebugf(\"Streamer flush begin: eh(%v)\", eh)\n\t\terr = eh.flush()\n\t\tif err != nil {\n\t\t\tlog.LogErrorf(\"Streamer flush failed: eh(%v)\", eh)\n\t\t\treturn\n\t\t}\n\t\teh.stream.dirtylist.Remove(element)\n\t\tif eh.getStatus() == ExtentStatusOpen {\n\t\t\ts.dirty = false\n\t\t\tlog.LogDebugf(\"Streamer flush handler open: eh(%v)\", eh)\n\t\t} else {\n\t\t\t// TODO unhandled error\n\t\t\teh.cleanup()\n\t\t\tlog.LogDebugf(\"Streamer flush handler cleaned up: eh(%v)\", eh)\n\t\t}\n\t\tlog.LogDebugf(\"Streamer flush end: eh(%v)\", eh)\n\t}\n\treturn\n}\n\nfunc (s *Streamer) traverse() (err error) {\n\ts.traversed++\n\tlength := s.dirtylist.Len()\n\tfor i := 0; i < length; i++ {\n\t\telement := s.dirtylist.Get()\n\t\tif element == nil {\n\t\t\tbreak\n\t\t}\n\t\teh := element.Value.(*ExtentHandler)\n\n\t\tlog.LogDebugf(\"Streamer traverse begin: eh(%v)\", eh)\n\t\tif eh.getStatus() >= ExtentStatusClosed {\n\t\t\t// handler can be in different status such as close, recovery, and error,\n\t\t\t// and therefore there can be packet that has not been flushed yet.\n\t\t\teh.flushPacket()\n\t\t\tif atomic.LoadInt32(&eh.inflight) > 0 {\n\t\t\t\tlog.LogDebugf(\"Streamer traverse skipped: non-zero inflight, eh(%v)\", eh)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\terr = eh.appendExtentKey()\n\t\t\tif err != nil {\n\t\t\t\tlog.LogWarnf(\"Streamer traverse abort: appendExtentKey failed, eh(%v) err(%v)\", eh, err)\n\t\t\t\t// set the streamer to error status to avoid further writes\n\t\t\t\tif err == syscall.EIO {\n\t\t\t\t\tatomic.StoreInt32(&eh.stream.status, StreamerError)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\ts.dirtylist.Remove(element)\n\t\t\teh.cleanup()\n\t\t} else {\n\t\t\tif s.traversed < streamWriterFlushPeriod {\n\t\t\t\tlog.LogDebugf(\"Streamer traverse skipped: traversed(%v) eh(%v)\", s.traversed, eh)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err = eh.flush(); err != nil {\n\t\t\t\tlog.LogWarnf(\"Streamer traverse flush: eh(%v) err(%v)\", eh, err)\n\t\t\t}\n\t\t}\n\t\tlog.LogDebugf(\"Streamer traverse end: eh(%v)\", eh)\n\t}\n\treturn\n}\n\nfunc (s *Streamer) closeOpenHandler() (err error) {\n\t// just in case to avoid infinite loop\n\tvar cnt int = 2 * MaxPacketErrorCount\n\n\thandler := s.handler\n\tfor handler != nil && cnt >= 0 {\n\t\thandler.setClosed()\n\t\tif s.dirtylist.Len() < MaxDirtyListLen {\n\t\t\thandler.flushPacket()\n\t\t} else {\n\t\t\t// TODO unhandled error\n\t\t\terr = s.handler.flush()\n\t\t}\n\t\thandler = handler.recoverHandler\n\t\tcnt--\n\t}\n\n\tif s.handler != nil {\n\t\tif !s.dirty {\n\t\t\t// in case the current handler is not on the dirty list and will not get cleaned up\n\t\t\t// TODO unhandled error\n\t\t\tlog.LogDebugf(\"action[Streamer.closeOpenHandler]\")\n\t\t\ts.handler.cleanup()\n\t\t}\n\t\ts.handler = nil\n\t}\n\treturn err\n}\n\nfunc (s *Streamer) open() {\n\ts.refcnt++\n\tlog.LogDebugf(\"open: streamer(%v) refcnt(%v)\", s, s.refcnt)\n}\n\nfunc (s *Streamer) release() error {\n\ts.refcnt--\n\ts.closeOpenHandler()\n\terr := s.flush()\n\tif err != nil {\n\t\ts.abort()\n\t}\n\tlog.LogDebugf(\"release: streamer(%v) refcnt(%v)\", s, s.refcnt)\n\treturn err\n}\n\nfunc (s *Streamer) evict() error {\n\ts.client.streamerLock.Lock()\n\tif s.refcnt > 0 || len(s.request) != 0 {\n\t\ts.client.streamerLock.Unlock()\n\t\treturn errors.New(fmt.Sprintf(\"evict: streamer(%v) refcnt(%v)\", s, s.refcnt))\n\t}\n\tif s.client.disableMetaCache || !s.needBCache {\n\t\tdelete(s.client.streamers, s.inode)\n\t}\n\ts.client.streamerLock.Unlock()\n\treturn nil\n}\n\nfunc (s *Streamer) abort() {\n\tfor {\n\t\telement := s.dirtylist.Get()\n\t\tif element == nil {\n\t\t\tbreak\n\t\t}\n\t\teh := element.Value.(*ExtentHandler)\n\t\ts.dirtylist.Remove(element)\n\t\t// TODO unhandled error\n\t\teh.cleanup()\n\t}\n}\n\nfunc (s *Streamer) truncate(size int, fullPath string) error {\n\ts.closeOpenHandler()\n\terr := s.flush()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = s.client.truncate(s.inode, uint64(size), fullPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\toldsize, _ := s.extents.Size()\n\tif oldsize <= size {\n\t\ts.extents.SetSize(uint64(size), true)\n\t\treturn nil\n\t}\n\n\ts.extents.TruncDiscard(uint64(size))\n\treturn s.GetExtentsForce()\n}\n\nfunc (s *Streamer) updateVer(verSeq uint64) (err error) {\n\tlog.LogInfof(\"action[stream.updateVer] ver %v update to %v\", s.verSeq, verSeq)\n\tif s.verSeq != verSeq {\n\t\t//log.LogInfof(\"action[stream.updateVer] ver %v update to %v\", s.verSeq, verSeq)\n\t\t//if s.handler != nil {\n\t\t//\ts.handler.verUpdate<-verSeq\n\t\t//} else {\n\t\t//\tlog.LogInfof(\"action[stream.updateVer] ver %v update to %v\", s.verSeq, verSeq)\n\t\t//}\n\t\tlog.LogInfof(\"action[stream.updateVer] ver %v update to %v\", s.verSeq, verSeq)\n\t\ts.verSeq = verSeq\n\t\ts.extents.verSeq = verSeq\n\t}\n\treturn\n}\n\nfunc (s *Streamer) tinySizeLimit() int {\n\treturn util.DefaultTinySizeLimit\n}\n", "// Copyright 2018 The CubeFS Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n// implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage util\n\nimport (\n\t\"math/rand\"\n\t\"strings\"\n\t\"time\"\n)\n\nfunc SubString(sourceString string, begin, end int) string {\n\tbytes := []byte(sourceString)\n\tstringLength := len(bytes)\n\n\tif begin < 0 {\n\t\tbegin = 0\n\t}\n\tif end > stringLength {\n\t\tend = stringLength\n\t}\n\treturn string(bytes[begin:end])\n}\n\ntype RandomSeed byte\n\nfunc (s RandomSeed) Runes() []rune {\n\tsourceBuilder := strings.Builder{}\n\tif s&Numeric > 0 {\n\t\tsourceBuilder.WriteString(\"0123456789\")\n\t}\n\tif s&LowerLetter > 0 {\n\t\tsourceBuilder.WriteString(\"abcdefghijklmnopqrstuvwxyz\")\n\t}\n\tif s&UpperLetter > 0 {\n\t\tsourceBuilder.WriteString(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\t}\n\treturn []rune(sourceBuilder.String())\n}\n\nconst (\n\tNumeric RandomSeed = 1 << iota\n\tLowerLetter\n\tUpperLetter\n)\n\nfunc RandomString(length int, seed RandomSeed) string {\n\truns := seed.Runes()\n\tresult := \"\"\n\tfor i := 0; i < length; i++ {\n\t\trand.Seed(time.Now().UnixNano())\n\t\trandNumber := rand.Intn(len(runs))\n\t\tresult += string(runs[randNumber])\n\t}\n\treturn result\n}\n"], "fixing_code": ["// Copyright 2018 The CubeFS Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n// implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage proto\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"github.com/cubefs/cubefs/util/log\"\n\t\"io\"\n\t\"net\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/cubefs/cubefs/util\"\n\t\"github.com/cubefs/cubefs/util/buf\"\n)\n\nvar (\n\tGRequestID = int64(1)\n\tBuffers    *buf.BufferPool\n)\n\n// GenerateRequestID generates the request ID.\nfunc GenerateRequestID() int64 {\n\treturn atomic.AddInt64(&GRequestID, 1)\n}\n\nconst (\n\tAddrSplit = \"/\"\n)\n\n// Operations\nconst (\n\tProtoMagic           uint8 = 0xFF\n\tOpInitResultCode     uint8 = 0x00\n\tOpCreateExtent       uint8 = 0x01\n\tOpMarkDelete         uint8 = 0x02\n\tOpWrite              uint8 = 0x03\n\tOpRead               uint8 = 0x04\n\tOpStreamRead         uint8 = 0x05\n\tOpStreamFollowerRead uint8 = 0x06\n\tOpGetAllWatermarks   uint8 = 0x07\n\n\tOpNotifyReplicasToRepair         uint8 = 0x08\n\tOpExtentRepairRead               uint8 = 0x09\n\tOpBroadcastMinAppliedID          uint8 = 0x0A\n\tOpRandomWrite                    uint8 = 0x0F\n\tOpGetAppliedId                   uint8 = 0x10\n\tOpGetPartitionSize               uint8 = 0x11\n\tOpSyncRandomWrite                uint8 = 0x12\n\tOpSyncWrite                      uint8 = 0x13\n\tOpReadTinyDeleteRecord           uint8 = 0x14\n\tOpTinyExtentRepairRead           uint8 = 0x15\n\tOpGetMaxExtentIDAndPartitionSize uint8 = 0x16\n\n\t// Operations: Client -> MetaNode.\n\tOpMetaCreateInode   uint8 = 0x20\n\tOpMetaUnlinkInode   uint8 = 0x21\n\tOpMetaCreateDentry  uint8 = 0x22\n\tOpMetaDeleteDentry  uint8 = 0x23\n\tOpMetaOpen          uint8 = 0x24\n\tOpMetaLookup        uint8 = 0x25\n\tOpMetaReadDir       uint8 = 0x26\n\tOpMetaInodeGet      uint8 = 0x27\n\tOpMetaBatchInodeGet uint8 = 0x28\n\tOpMetaExtentsAdd    uint8 = 0x29\n\tOpMetaExtentsDel    uint8 = 0x2A\n\tOpMetaExtentsList   uint8 = 0x2B\n\tOpMetaUpdateDentry  uint8 = 0x2C\n\tOpMetaTruncate      uint8 = 0x2D\n\tOpMetaLinkInode     uint8 = 0x2E\n\tOpMetaEvictInode    uint8 = 0x2F\n\tOpMetaSetattr       uint8 = 0x30\n\tOpMetaReleaseOpen   uint8 = 0x31\n\n\t//Operations: MetaNode Leader -> MetaNode Follower\n\tOpMetaFreeInodesOnRaftFollower uint8 = 0x32\n\n\tOpMetaDeleteInode        uint8 = 0x33 // delete specified inode immediately and do not remove data.\n\tOpMetaBatchExtentsAdd    uint8 = 0x34 // for extents batch attachment\n\tOpMetaSetXAttr           uint8 = 0x35\n\tOpMetaGetXAttr           uint8 = 0x36\n\tOpMetaRemoveXAttr        uint8 = 0x37\n\tOpMetaListXAttr          uint8 = 0x38\n\tOpMetaBatchGetXAttr      uint8 = 0x39\n\tOpMetaExtentAddWithCheck uint8 = 0x3A // Append extent key with discard extents check\n\tOpMetaReadDirLimit       uint8 = 0x3D\n\n\t// Operations: Master -> MetaNode\n\tOpCreateMetaPartition           uint8 = 0x40\n\tOpMetaNodeHeartbeat             uint8 = 0x41\n\tOpDeleteMetaPartition           uint8 = 0x42\n\tOpUpdateMetaPartition           uint8 = 0x43\n\tOpLoadMetaPartition             uint8 = 0x44\n\tOpDecommissionMetaPartition     uint8 = 0x45\n\tOpAddMetaPartitionRaftMember    uint8 = 0x46\n\tOpRemoveMetaPartitionRaftMember uint8 = 0x47\n\tOpMetaPartitionTryToLeader      uint8 = 0x48\n\n\t// Quota\n\tOpMetaBatchSetInodeQuota    uint8 = 0x50\n\tOpMetaBatchDeleteInodeQuota uint8 = 0x51\n\tOpMetaGetInodeQuota         uint8 = 0x52\n\tOpQuotaCreateInode          uint8 = 0x53\n\tOpQuotaCreateDentry         uint8 = 0x54\n\n\t// Operations: Master -> LcNode\n\n\tOpLcNodeHeartbeat      uint8 = 0x55\n\tOpLcNodeScan           uint8 = 0x56\n\tOpLcNodeSnapshotVerDel uint8 = 0x57\n\n\t// Operations: Master -> DataNode\n\tOpCreateDataPartition           uint8 = 0x60\n\tOpDeleteDataPartition           uint8 = 0x61\n\tOpLoadDataPartition             uint8 = 0x62\n\tOpDataNodeHeartbeat             uint8 = 0x63\n\tOpReplicateFile                 uint8 = 0x64\n\tOpDeleteFile                    uint8 = 0x65\n\tOpDecommissionDataPartition     uint8 = 0x66\n\tOpAddDataPartitionRaftMember    uint8 = 0x67\n\tOpRemoveDataPartitionRaftMember uint8 = 0x68\n\tOpDataPartitionTryToLeader      uint8 = 0x69\n\tOpQos                           uint8 = 0x6A\n\tOpStopDataPartitionRepair       uint8 = 0x6B\n\n\t// Operations: MultipartInfo\n\tOpCreateMultipart  uint8 = 0x70\n\tOpGetMultipart     uint8 = 0x71\n\tOpAddMultipartPart uint8 = 0x72\n\tOpRemoveMultipart  uint8 = 0x73\n\tOpListMultiparts   uint8 = 0x74\n\n\tOpBatchDeleteExtent   uint8 = 0x75 // SDK to MetaNode\n\tOpGetExpiredMultipart uint8 = 0x76\n\n\t//Operations: MetaNode Leader -> MetaNode Follower\n\tOpMetaBatchDeleteInode  uint8 = 0x90\n\tOpMetaBatchDeleteDentry uint8 = 0x91\n\tOpMetaBatchUnlinkInode  uint8 = 0x92\n\tOpMetaBatchEvictInode   uint8 = 0x93\n\n\t//Transaction Operations: Client -> MetaNode.\n\tOpMetaTxCreate       uint8 = 0xA0\n\tOpMetaTxCreateInode  uint8 = 0xA1\n\tOpMetaTxUnlinkInode  uint8 = 0xA2\n\tOpMetaTxCreateDentry uint8 = 0xA3\n\tOpTxCommit           uint8 = 0xA4\n\tOpTxRollback         uint8 = 0xA5\n\tOpTxCommitRM         uint8 = 0xA6\n\tOpTxRollbackRM       uint8 = 0xA7\n\tOpMetaTxDeleteDentry uint8 = 0xA8\n\tOpMetaTxUpdateDentry uint8 = 0xA9\n\tOpMetaTxLinkInode    uint8 = 0xAA\n\tOpMetaTxGet          uint8 = 0xAB\n\n\t//Operations: Client -> MetaNode.\n\tOpMetaGetUniqID uint8 = 0xAC\n\n\t//Multi version snapshot\n\tOpRandomWriteAppend     uint8 = 0xB1\n\tOpSyncRandomWriteAppend uint8 = 0xB2\n\tOpRandomWriteVer        uint8 = 0xB3\n\tOpSyncRandomWriteVer    uint8 = 0xB4\n\tOpSyncRandomWriteVerRsp uint8 = 0xB5\n\tOpTryWriteAppend        uint8 = 0xB6\n\tOpSyncTryWriteAppend    uint8 = 0xB7\n\n\t// Commons\n\tOpNoSpaceErr uint8 = 0xEE\n\tOpDirQuota   uint8 = 0xF1\n\n\t// Commons\n\n\tOpConflictExtentsErr uint8 = 0xF2\n\tOpIntraGroupNetErr   uint8 = 0xF3\n\tOpArgMismatchErr     uint8 = 0xF4\n\tOpNotExistErr        uint8 = 0xF5\n\tOpDiskNoSpaceErr     uint8 = 0xF6\n\tOpDiskErr            uint8 = 0xF7\n\tOpErr                uint8 = 0xF8\n\tOpAgain              uint8 = 0xF9\n\tOpExistErr           uint8 = 0xFA\n\tOpInodeFullErr       uint8 = 0xFB\n\tOpTryOtherAddr       uint8 = 0xFC\n\tOpNotPerm            uint8 = 0xFD\n\tOpNotEmpty           uint8 = 0xFE\n\tOpOk                 uint8 = 0xF0\n\tOpTryOtherExtent     uint8 = 0xE0\n\tOpAgainVerionList    uint8 = 0xEF\n\n\tOpPing                  uint8 = 0xFF\n\tOpMetaUpdateXAttr       uint8 = 0x3B\n\tOpMetaReadDirOnly       uint8 = 0x3C\n\tOpUploadPartConflictErr uint8 = 0x3D\n\n\t// ebs obj meta\n\tOpMetaObjExtentAdd       uint8 = 0xDD\n\tOpMetaObjExtentsList     uint8 = 0xDE\n\tOpMetaExtentsEmpty       uint8 = 0xDF\n\tOpMetaBatchObjExtentsAdd uint8 = 0xD0\n\tOpMetaClearInodeCache    uint8 = 0xD1\n\n\tOpMetaBatchSetXAttr uint8 = 0xD2\n\tOpMetaGetAllXAttr   uint8 = 0xD3\n\n\t//transaction error\n\n\tOpTxInodeInfoNotExistErr  uint8 = 0xE0\n\tOpTxConflictErr           uint8 = 0xE1\n\tOpTxDentryInfoNotExistErr uint8 = 0xE2\n\tOpTxRbInodeNotExistErr    uint8 = 0xE3\n\tOpTxRbDentryNotExistErr   uint8 = 0xE4\n\tOpTxInfoNotExistErr       uint8 = 0xE5\n\tOpTxInternalErr           uint8 = 0xE6\n\tOpTxCommitItemErr         uint8 = 0xE7\n\tOpTxRollbackItemErr       uint8 = 0xE8\n\tOpTxRollbackUnknownRbType uint8 = 0xE9\n\tOpTxTimeoutErr            uint8 = 0xEA\n\tOpTxSetStateErr           uint8 = 0xEB\n\tOpTxCommitErr             uint8 = 0xEC\n\tOpTxRollbackErr           uint8 = 0xED\n\tOpTxUnknownOp             uint8 = 0xEE\n\n\t// multiVersion to dp/mp\n\tOpVersionOperation uint8 = 0xD5\n\tOpSplitMarkDelete  uint8 = 0xD6\n)\n\nconst (\n\tWriteDeadlineTime                         = 5\n\tReadDeadlineTime                          = 5\n\tSyncSendTaskDeadlineTime                  = 30\n\tNoReadDeadlineTime                        = -1\n\tBatchDeleteExtentReadDeadLineTime         = 120\n\tGetAllWatermarksDeadLineTime              = 60\n\tDefaultClusterLoadFactor          float64 = 10\n\tMultiVersionFlag                          = 0x80\n\tVersionListFlag                           = 0x40\n)\n\n// multi version operation\nconst (\n\tCreateVersion        = 1\n\tDeleteVersion        = 2\n\tCreateVersionPrepare = 3\n\tCreateVersionCommit  = 4\n\tSyncAllVersionList   = 5\n)\n\n// stage of version building\nconst (\n\tVersionInit            = 0\n\tVersionWorking         = 1\n\tVersionWorkingTimeOut  = 2\n\tVersionWorkingAbnormal = 3\n\tVersionWorkingFinished = 4\n)\n\n// status of version\nconst (\n\tVersionNormal         = 1\n\tVersionDeleted        = 2\n\tVersionDeleting       = 3\n\tVersionDeleteAbnormal = 4\n\tVersionPrepare        = 5\n)\n\nconst (\n\tTinyExtentType   = 0\n\tNormalExtentType = 1\n)\n\nconst (\n\tNormalCreateDataPartition         = 0\n\tDecommissionedCreateDataPartition = 1\n)\n\n// Packet defines the packet structure.\ntype Packet struct {\n\tMagic              uint8\n\tExtentType         uint8 // the highest bit be set while rsp to client if version not consistent then Verseq be valid\n\tOpcode             uint8\n\tResultCode         uint8\n\tRemainingFollowers uint8\n\tCRC                uint32\n\tSize               uint32\n\tArgLen             uint32\n\tKernelOffset       uint64\n\tPartitionID        uint64\n\tExtentID           uint64\n\tExtentOffset       int64\n\tReqID              int64\n\tArg                []byte // for create or append ops, the data contains the address\n\tData               []byte\n\tStartT             int64\n\tmesg               string\n\tHasPrepare         bool\n\tVerSeq             uint64 // only used in mod request to datanode\n\tVerList            []*VolVersionInfo\n}\n\nfunc IsTinyExtentType(extentType uint8) bool {\n\treturn extentType&NormalExtentType != NormalExtentType\n}\n\nfunc IsNormalExtentType(extentType uint8) bool {\n\treturn extentType&NormalExtentType == NormalExtentType\n}\n\n// NewPacket returns a new packet.\nfunc NewPacket() *Packet {\n\tp := new(Packet)\n\tp.Magic = ProtoMagic\n\tp.StartT = time.Now().UnixNano()\n\treturn p\n}\n\n// NewPacketReqID returns a new packet with ReqID assigned.\nfunc NewPacketReqID() *Packet {\n\tp := NewPacket()\n\tp.ReqID = GenerateRequestID()\n\treturn p\n}\n\nfunc (p *Packet) GetCopy() *Packet {\n\tnewPacket := NewPacket()\n\tnewPacket.ReqID = p.ReqID\n\tnewPacket.Opcode = p.Opcode\n\tnewPacket.PartitionID = p.PartitionID\n\n\tnewPacket.Data = make([]byte, p.Size)\n\tcopy(newPacket.Data[:p.Size], p.Data)\n\n\tnewPacket.Size = p.Size\n\treturn newPacket\n}\n\nfunc (p *Packet) String() string {\n\treturn fmt.Sprintf(\"ReqID(%v)Op(%v)PartitionID(%v)ResultCode(%v)ExID(%v)ExtOffset(%v)KernelOff(%v)Type(%v)Seq(%v)\",\n\t\tp.ReqID, p.GetOpMsg(), p.PartitionID, p.GetResultMsg(), p.ExtentID, p.ExtentOffset, p.KernelOffset, p.ExtentType, p.VerSeq)\n}\n\n// GetStoreType returns the store type.\nfunc (p *Packet) GetStoreType() (m string) {\n\tif IsNormalExtentType(p.ExtentType) {\n\t\treturn \"NormalExtent\"\n\t} else if IsTinyExtentType(p.ExtentType) {\n\t\treturn \"TinyExtent\"\n\t} else {\n\t\treturn \"Unknown\"\n\t}\n}\n\nfunc (p *Packet) GetOpMsgWithReqAndResult() (m string) {\n\treturn fmt.Sprintf(\"Req(%v)_(%v)_Result(%v)\", p.ReqID, p.GetOpMsg(), p.GetResultMsg())\n}\n\n// GetOpMsg returns the operation type.\nfunc (p *Packet) GetOpMsg() (m string) {\n\tswitch p.Opcode {\n\tcase OpCreateExtent:\n\t\tm = \"OpCreateExtent\"\n\tcase OpMarkDelete:\n\t\tm = \"OpMarkDelete\"\n\tcase OpSplitMarkDelete:\n\t\tm = \"OpMarkDelete\"\n\tcase OpWrite:\n\t\tm = \"OpWrite\"\n\tcase OpTryWriteAppend:\n\t\tm = \"OpTryWriteAppend\"\n\tcase OpRandomWrite:\n\t\tm = \"OpRandomWrite\"\n\tcase OpRandomWriteAppend:\n\t\tm = \"OpRandomWriteAppend\"\n\tcase OpRandomWriteVer:\n\t\tm = \"OpRandomWriteVer\"\n\tcase OpRead:\n\t\tm = \"Read\"\n\tcase OpStreamRead:\n\t\tm = \"OpStreamRead\"\n\tcase OpStreamFollowerRead:\n\t\tm = \"OpStreamFollowerRead\"\n\tcase OpGetAllWatermarks:\n\t\tm = \"OpGetAllWatermarks\"\n\tcase OpNotifyReplicasToRepair:\n\t\tm = \"OpNotifyReplicasToRepair\"\n\tcase OpExtentRepairRead:\n\t\tm = \"OpExtentRepairRead\"\n\tcase OpConflictExtentsErr:\n\t\tm = \"ConflictExtentsErr\"\n\tcase OpIntraGroupNetErr:\n\t\tm = \"IntraGroupNetErr\"\n\tcase OpMetaCreateInode:\n\t\tm = \"OpMetaCreateInode\"\n\tcase OpQuotaCreateInode:\n\t\tm = \"OpQuotaCreateInode\"\n\tcase OpMetaUnlinkInode:\n\t\tm = \"OpMetaUnlinkInode\"\n\tcase OpMetaBatchUnlinkInode:\n\t\tm = \"OpMetaBatchUnlinkInode\"\n\tcase OpMetaCreateDentry:\n\t\tm = \"OpMetaCreateDentry\"\n\tcase OpQuotaCreateDentry:\n\t\tm = \"OpQuotaCreateDentry\"\n\tcase OpMetaDeleteDentry:\n\t\tm = \"OpMetaDeleteDentry\"\n\tcase OpMetaBatchDeleteDentry:\n\t\tm = \"OpMetaBatchDeleteDentry\"\n\tcase OpMetaOpen:\n\t\tm = \"OpMetaOpen\"\n\tcase OpMetaReleaseOpen:\n\t\tm = \"OpMetaReleaseOpen\"\n\tcase OpMetaLookup:\n\t\tm = \"OpMetaLookup\"\n\tcase OpMetaReadDir:\n\t\tm = \"OpMetaReadDir\"\n\tcase OpMetaReadDirLimit:\n\t\tm = \"OpMetaReadDirLimit\"\n\tcase OpMetaInodeGet:\n\t\tm = \"OpMetaInodeGet\"\n\tcase OpMetaBatchInodeGet:\n\t\tm = \"OpMetaBatchInodeGet\"\n\tcase OpMetaExtentsAdd:\n\t\tm = \"OpMetaExtentsAdd\"\n\tcase OpMetaExtentAddWithCheck:\n\t\tm = \"OpMetaExtentAddWithCheck\"\n\tcase OpMetaObjExtentAdd:\n\t\tm = \"OpMetaObjExtentAdd\"\n\tcase OpMetaExtentsDel:\n\t\tm = \"OpMetaExtentsDel\"\n\tcase OpMetaExtentsList:\n\t\tm = \"OpMetaExtentsList\"\n\tcase OpMetaObjExtentsList:\n\t\tm = \"OpMetaObjExtentsList\"\n\tcase OpMetaUpdateDentry:\n\t\tm = \"OpMetaUpdateDentry\"\n\tcase OpMetaTruncate:\n\t\tm = \"OpMetaTruncate\"\n\tcase OpMetaLinkInode:\n\t\tm = \"OpMetaLinkInode\"\n\tcase OpMetaEvictInode:\n\t\tm = \"OpMetaEvictInode\"\n\tcase OpMetaBatchEvictInode:\n\t\tm = \"OpMetaBatchEvictInode\"\n\tcase OpMetaSetattr:\n\t\tm = \"OpMetaSetattr\"\n\tcase OpCreateMetaPartition:\n\t\tm = \"OpCreateMetaPartition\"\n\tcase OpMetaNodeHeartbeat:\n\t\tm = \"OpMetaNodeHeartbeat\"\n\tcase OpDeleteMetaPartition:\n\t\tm = \"OpDeleteMetaPartition\"\n\tcase OpUpdateMetaPartition:\n\t\tm = \"OpUpdateMetaPartition\"\n\tcase OpLoadMetaPartition:\n\t\tm = \"OpLoadMetaPartition\"\n\tcase OpDecommissionMetaPartition:\n\t\tm = \"OpDecommissionMetaPartition\"\n\tcase OpCreateDataPartition:\n\t\tm = \"OpCreateDataPartition\"\n\tcase OpDeleteDataPartition:\n\t\tm = \"OpDeleteDataPartition\"\n\tcase OpLoadDataPartition:\n\t\tm = \"OpLoadDataPartition\"\n\tcase OpDecommissionDataPartition:\n\t\tm = \"OpDecommissionDataPartition\"\n\tcase OpDataNodeHeartbeat:\n\t\tm = \"OpDataNodeHeartbeat\"\n\tcase OpReplicateFile:\n\t\tm = \"OpReplicateFile\"\n\tcase OpDeleteFile:\n\t\tm = \"OpDeleteFile\"\n\tcase OpGetAppliedId:\n\t\tm = \"OpGetAppliedId\"\n\tcase OpGetPartitionSize:\n\t\tm = \"OpGetPartitionSize\"\n\tcase OpSyncWrite:\n\t\tm = \"OpSyncWrite\"\n\tcase OpSyncTryWriteAppend:\n\t\tm = \"OpSyncTryWriteAppend\"\n\tcase OpSyncRandomWrite:\n\t\tm = \"OpSyncRandomWrite\"\n\tcase OpSyncRandomWriteVer:\n\t\tm = \"OpSyncRandomWriteVer\"\n\tcase OpSyncRandomWriteAppend:\n\t\tm = \"OpSyncRandomWriteAppend\"\n\tcase OpReadTinyDeleteRecord:\n\t\tm = \"OpReadTinyDeleteRecord\"\n\tcase OpPing:\n\t\tm = \"OpPing\"\n\tcase OpTinyExtentRepairRead:\n\t\tm = \"OpTinyExtentRepairRead\"\n\tcase OpGetMaxExtentIDAndPartitionSize:\n\t\tm = \"OpGetMaxExtentIDAndPartitionSize\"\n\tcase OpBroadcastMinAppliedID:\n\t\tm = \"OpBroadcastMinAppliedID\"\n\tcase OpRemoveDataPartitionRaftMember:\n\t\tm = \"OpRemoveDataPartitionRaftMember\"\n\tcase OpAddDataPartitionRaftMember:\n\t\tm = \"OpAddDataPartitionRaftMember\"\n\tcase OpAddMetaPartitionRaftMember:\n\t\tm = \"OpAddMetaPartitionRaftMember\"\n\tcase OpRemoveMetaPartitionRaftMember:\n\t\tm = \"OpRemoveMetaPartitionRaftMember\"\n\tcase OpMetaPartitionTryToLeader:\n\t\tm = \"OpMetaPartitionTryToLeader\"\n\tcase OpDataPartitionTryToLeader:\n\t\tm = \"OpDataPartitionTryToLeader\"\n\tcase OpMetaDeleteInode:\n\t\tm = \"OpMetaDeleteInode\"\n\tcase OpMetaBatchDeleteInode:\n\t\tm = \"OpMetaBatchDeleteInode\"\n\tcase OpMetaBatchExtentsAdd:\n\t\tm = \"OpMetaBatchExtentsAdd\"\n\tcase OpMetaBatchObjExtentsAdd:\n\t\tm = \"OpMetaBatchObjExtentsAdd\"\n\tcase OpMetaSetXAttr:\n\t\tm = \"OpMetaSetXAttr\"\n\tcase OpMetaGetXAttr:\n\t\tm = \"OpMetaGetXAttr\"\n\tcase OpMetaRemoveXAttr:\n\t\tm = \"OpMetaRemoveXAttr\"\n\tcase OpMetaListXAttr:\n\t\tm = \"OpMetaListXAttr\"\n\tcase OpMetaBatchGetXAttr:\n\t\tm = \"OpMetaBatchGetXAttr\"\n\tcase OpMetaUpdateXAttr:\n\t\tm = \"OpMetaUpdateXAttr\"\n\tcase OpCreateMultipart:\n\t\tm = \"OpCreateMultipart\"\n\tcase OpGetMultipart:\n\t\tm = \"OpGetMultipart\"\n\tcase OpAddMultipartPart:\n\t\tm = \"OpAddMultipartPart\"\n\tcase OpRemoveMultipart:\n\t\tm = \"OpRemoveMultipart\"\n\tcase OpListMultiparts:\n\t\tm = \"OpListMultiparts\"\n\tcase OpBatchDeleteExtent:\n\t\tm = \"OpBatchDeleteExtent\"\n\tcase OpMetaClearInodeCache:\n\t\tm = \"OpMetaClearInodeCache\"\n\tcase OpMetaTxCreateInode:\n\t\tm = \"OpMetaTxCreateInode\"\n\tcase OpMetaTxCreateDentry:\n\t\tm = \"OpMetaTxCreateDentry\"\n\tcase OpTxCommit:\n\t\tm = \"OpTxCommit\"\n\tcase OpMetaTxCreate:\n\t\tm = \"OpMetaTxCreate\"\n\tcase OpTxRollback:\n\t\tm = \"OpTxRollback\"\n\tcase OpTxCommitRM:\n\t\tm = \"OpTxCommitRM\"\n\tcase OpTxRollbackRM:\n\t\tm = \"OpTxRollbackRM\"\n\tcase OpMetaTxDeleteDentry:\n\t\tm = \"OpMetaTxDeleteDentry\"\n\tcase OpMetaTxUnlinkInode:\n\t\tm = \"OpMetaTxUnlinkInode\"\n\tcase OpMetaTxUpdateDentry:\n\t\tm = \"OpMetaTxUpdateDentry\"\n\tcase OpMetaTxLinkInode:\n\t\tm = \"OpMetaTxLinkInode\"\n\tcase OpMetaTxGet:\n\t\tm = \"OpMetaTxGet\"\n\tcase OpMetaBatchSetInodeQuota:\n\t\tm = \"OpMetaBatchSetInodeQuota\"\n\tcase OpMetaBatchDeleteInodeQuota:\n\t\tm = \"OpMetaBatchDeleteInodeQuota\"\n\tcase OpMetaGetInodeQuota:\n\t\tm = \"OpMetaGetInodeQuota\"\n\tcase OpStopDataPartitionRepair:\n\t\tm = \"OpStopDataPartitionRepair\"\n\tcase OpLcNodeHeartbeat:\n\t\tm = \"OpLcNodeHeartbeat\"\n\tcase OpLcNodeScan:\n\t\tm = \"OpLcNodeScan\"\n\tcase OpLcNodeSnapshotVerDel:\n\t\tm = \"OpLcNodeSnapshotVerDel\"\n\tcase OpMetaReadDirOnly:\n\t\tm = \"OpMetaReadDirOnly\"\n\tdefault:\n\t\tm = fmt.Sprintf(\"op:%v not found\", p.Opcode)\n\t}\n\treturn\n}\n\nfunc GetStatusStr(status uint8) string {\n\tpkt := &Packet{}\n\tpkt.ResultCode = status\n\treturn pkt.GetResultMsg()\n}\n\n// GetResultMsg returns the result message.\nfunc (p *Packet) GetResultMsg() (m string) {\n\tif p == nil {\n\t\treturn \"\"\n\t}\n\n\tswitch p.ResultCode {\n\tcase OpConflictExtentsErr:\n\t\tm = \"ConflictExtentsErr\"\n\tcase OpIntraGroupNetErr:\n\t\tm = \"IntraGroupNetErr\"\n\tcase OpDiskNoSpaceErr:\n\t\tm = \"DiskNoSpaceErr\"\n\tcase OpDiskErr:\n\t\tm = \"DiskErr\"\n\tcase OpErr:\n\t\tm = \"Err: \" + string(p.Data)\n\tcase OpAgain:\n\t\tm = \"Again: \" + string(p.Data)\n\tcase OpOk:\n\t\tm = \"Ok\"\n\tcase OpExistErr:\n\t\tm = \"ExistErr\"\n\tcase OpInodeFullErr:\n\t\tm = \"InodeFullErr\"\n\tcase OpArgMismatchErr:\n\t\tm = \"ArgUnmatchErr\"\n\tcase OpNotExistErr:\n\t\tm = \"NotExistErr\"\n\tcase OpTryOtherAddr:\n\t\tm = \"TryOtherAddr\"\n\tcase OpNotPerm:\n\t\tm = \"NotPerm\"\n\tcase OpNotEmpty:\n\t\tm = \"DirNotEmpty\"\n\tcase OpDirQuota:\n\t\tm = \"OpDirQuota\"\n\tcase OpNoSpaceErr:\n\t\tm = \"NoSpaceErr\"\n\tcase OpTxInodeInfoNotExistErr:\n\t\tm = \"OpTxInodeInfoNotExistErr\"\n\tcase OpTxConflictErr:\n\t\tm = \"TransactionConflict\"\n\tcase OpTxDentryInfoNotExistErr:\n\t\tm = \"OpTxDentryInfoNotExistErr\"\n\tcase OpTxRbInodeNotExistErr:\n\t\tm = \"OpTxRbInodeNotExistEr\"\n\tcase OpTxRbDentryNotExistErr:\n\t\tm = \"OpTxRbDentryNotExistEr\"\n\tcase OpTxInfoNotExistErr:\n\t\tm = \"OpTxInfoNotExistErr\"\n\tcase OpTxInternalErr:\n\t\tm = \"OpTxInternalErr\"\n\tcase OpTxCommitItemErr:\n\t\tm = \"OpTxCommitItemErr\"\n\tcase OpTxRollbackItemErr:\n\t\tm = \"OpTxRollbackItemErr\"\n\tcase OpTxRollbackUnknownRbType:\n\t\tm = \"OpTxRollbackUnknownRbType\"\n\tcase OpTxTimeoutErr:\n\t\tm = \"OpTxTimeoutErr\"\n\tcase OpTxSetStateErr:\n\t\tm = \"OpTxSetStateErr\"\n\tcase OpTxCommitErr:\n\t\tm = \"OpTxCommitErr\"\n\tcase OpTxRollbackErr:\n\t\tm = \"OpTxRollbackErr\"\n\tcase OpUploadPartConflictErr:\n\t\tm = \"OpUploadPartConflictErr\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"Unknown ResultCode(%v)\", p.ResultCode)\n\t}\n\treturn\n}\n\nfunc (p *Packet) GetReqID() int64 {\n\treturn p.ReqID\n}\n\n// MarshalHeader marshals the packet header.\nfunc (p *Packet) MarshalHeader(out []byte) {\n\tout[0] = p.Magic\n\tout[1] = p.ExtentType\n\tout[2] = p.Opcode\n\tout[3] = p.ResultCode\n\tout[4] = p.RemainingFollowers\n\tbinary.BigEndian.PutUint32(out[5:9], p.CRC)\n\tbinary.BigEndian.PutUint32(out[9:13], p.Size)\n\tbinary.BigEndian.PutUint32(out[13:17], p.ArgLen)\n\tbinary.BigEndian.PutUint64(out[17:25], p.PartitionID)\n\tbinary.BigEndian.PutUint64(out[25:33], p.ExtentID)\n\tbinary.BigEndian.PutUint64(out[33:41], uint64(p.ExtentOffset))\n\tbinary.BigEndian.PutUint64(out[41:49], uint64(p.ReqID))\n\tbinary.BigEndian.PutUint64(out[49:util.PacketHeaderSize], p.KernelOffset)\n\tif p.Opcode == OpRandomWriteVer || p.ExtentType&MultiVersionFlag > 0 {\n\t\tbinary.BigEndian.PutUint64(out[util.PacketHeaderSize:util.PacketHeaderSize+8], p.VerSeq)\n\t}\n\treturn\n}\n\nfunc (p *Packet) IsVersionList() bool {\n\tif p.ExtentType&VersionListFlag == VersionListFlag {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// UnmarshalHeader unmarshals the packet header.\nfunc (p *Packet) UnmarshalHeader(in []byte) error {\n\tp.Magic = in[0]\n\tif p.Magic != ProtoMagic {\n\t\treturn errors.New(\"Bad Magic \" + strconv.Itoa(int(p.Magic)))\n\t}\n\n\tp.ExtentType = in[1]\n\tp.Opcode = in[2]\n\tp.ResultCode = in[3]\n\tp.RemainingFollowers = in[4]\n\tp.CRC = binary.BigEndian.Uint32(in[5:9])\n\tp.Size = binary.BigEndian.Uint32(in[9:13])\n\tp.ArgLen = binary.BigEndian.Uint32(in[13:17])\n\tp.PartitionID = binary.BigEndian.Uint64(in[17:25])\n\tp.ExtentID = binary.BigEndian.Uint64(in[25:33])\n\tp.ExtentOffset = int64(binary.BigEndian.Uint64(in[33:41]))\n\tp.ReqID = int64(binary.BigEndian.Uint64(in[41:49]))\n\tp.KernelOffset = binary.BigEndian.Uint64(in[49:util.PacketHeaderSize])\n\n\t// header opcode OpRandomWriteVer should not unmarshal here due to header size is const\n\t// the ver param should read at the higher level directly\n\t//if p.Opcode ==OpRandomWriteVer {\n\n\treturn nil\n}\n\nconst verInfoCnt = 17\n\nfunc (p *Packet) MarshalVersionSlice() (data []byte, err error) {\n\titems := p.VerList\n\tcnt := len(items)\n\tbuff := bytes.NewBuffer(make([]byte, 0, 2*cnt*verInfoCnt))\n\tif err := binary.Write(buff, binary.BigEndian, uint16(cnt)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, v := range items {\n\t\tif err := binary.Write(buff, binary.BigEndian, v.Ver); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := binary.Write(buff, binary.BigEndian, v.DelTime); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := binary.Write(buff, binary.BigEndian, v.Status); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn buff.Bytes(), nil\n}\n\nfunc (p *Packet) UnmarshalVersionSlice(cnt int, d []byte) error {\n\titems := make([]*VolVersionInfo, 0)\n\tbuf := bytes.NewBuffer(d)\n\tvar err error\n\n\tfor idx := 0; idx < cnt; idx++ {\n\t\te := &VolVersionInfo{}\n\t\terr = binary.Read(buf, binary.BigEndian, &e.Ver)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = binary.Read(buf, binary.BigEndian, &e.DelTime)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = binary.Read(buf, binary.BigEndian, &e.Status)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\titems = append(items, e)\n\t}\n\tp.VerList = items\n\treturn nil\n}\n\n// MarshalData marshals the packet data.\nfunc (p *Packet) MarshalData(v interface{}) error {\n\tdata, err := json.Marshal(v)\n\tif err == nil {\n\t\tp.Data = data\n\t\tp.Size = uint32(len(p.Data))\n\t}\n\treturn err\n}\n\n// UnmarshalData unmarshals the packet data.\nfunc (p *Packet) UnmarshalData(v interface{}) error {\n\treturn json.Unmarshal(p.Data, v)\n}\n\n// WriteToNoDeadLineConn writes through the connection without deadline.\nfunc (p *Packet) WriteToNoDeadLineConn(c net.Conn) (err error) {\n\theader, err := Buffers.Get(util.PacketHeaderSize)\n\tif err != nil {\n\t\theader = make([]byte, util.PacketHeaderSize)\n\t}\n\tdefer Buffers.Put(header)\n\n\tp.MarshalHeader(header)\n\tif _, err = c.Write(header); err == nil {\n\t\tif _, err = c.Write(p.Arg[:int(p.ArgLen)]); err == nil {\n\t\t\tif p.Data != nil {\n\t\t\t\t_, err = c.Write(p.Data[:p.Size])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\n// WriteToConn writes through the given connection.\nfunc (p *Packet) WriteToConn(c net.Conn) (err error) {\n\theadSize := util.PacketHeaderSize\n\tif p.Opcode == OpRandomWriteVer || p.ExtentType&MultiVersionFlag > 0 {\n\t\theadSize = util.PacketHeaderVerSize\n\t}\n\t//log.LogDebugf(\"packet opcode %v header size %v extentype %v conn %v\", p.Opcode, headSize, p.ExtentType, c)\n\theader, err := Buffers.Get(headSize)\n\tif err != nil {\n\t\theader = make([]byte, headSize)\n\t}\n\t// log.LogErrorf(\"action[WriteToConn] buffer get nil,opcode %v head len [%v]\", p.Opcode, len(header))\n\tdefer Buffers.Put(header)\n\tc.SetWriteDeadline(time.Now().Add(WriteDeadlineTime * time.Second))\n\tp.MarshalHeader(header)\n\tif _, err = c.Write(header); err == nil {\n\t\t// write dir version info.\n\t\tif p.IsVersionList() {\n\t\t\td, err1 := p.MarshalVersionSlice()\n\t\t\tif err1 != nil {\n\t\t\t\tlog.LogErrorf(\"MarshalVersionSlice: marshal version ifo failed, err %s\", err1.Error())\n\t\t\t\treturn err1\n\t\t\t}\n\n\t\t\t_, err = c.Write(d)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif _, err = c.Write(p.Arg[:int(p.ArgLen)]); err == nil {\n\t\t\tif p.Data != nil && p.Size != 0 {\n\t\t\t\t_, err = c.Write(p.Data[:p.Size])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\n// ReadFull is a wrapper function of io.ReadFull.\nfunc ReadFull(c net.Conn, buf *[]byte, readSize int) (err error) {\n\t*buf = make([]byte, readSize)\n\t_, err = io.ReadFull(c, (*buf)[:readSize])\n\treturn\n}\n\n// ReadFromConn reads the data from the given connection.\n// Recognize the version bit and parse out version,\n// to avoid version field rsp back , the rsp of random write from datanode with replace OpRandomWriteVer to OpRandomWriteVerRsp\nfunc (p *Packet) ReadFromConnWithVer(c net.Conn, timeoutSec int) (err error) {\n\tif timeoutSec != NoReadDeadlineTime {\n\t\tc.SetReadDeadline(time.Now().Add(time.Second * time.Duration(timeoutSec)))\n\t} else {\n\t\tc.SetReadDeadline(time.Time{})\n\t}\n\n\theader, err := Buffers.Get(util.PacketHeaderSize)\n\tif err != nil {\n\t\theader = make([]byte, util.PacketHeaderSize)\n\t}\n\tdefer Buffers.Put(header)\n\tvar n int\n\tif n, err = io.ReadFull(c, header); err != nil {\n\t\treturn\n\t}\n\tif n != util.PacketHeaderSize {\n\t\treturn syscall.EBADMSG\n\t}\n\tif err = p.UnmarshalHeader(header); err != nil {\n\t\treturn\n\t}\n\n\tif p.ExtentType&MultiVersionFlag > 0 {\n\t\tver := make([]byte, 8)\n\t\tif _, err = io.ReadFull(c, ver); err != nil {\n\t\t\treturn\n\t\t}\n\t\tp.VerSeq = binary.BigEndian.Uint64(ver)\n\t}\n\n\tif p.IsVersionList() {\n\t\tcntByte := make([]byte, 2)\n\t\tif _, err = io.ReadFull(c, cntByte); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcnt := binary.BigEndian.Uint16(cntByte)\n\t\tlog.LogDebugf(\"action[ReadFromConnWithVer] op %s verseq %v, extType %d, cnt %d\",\n\t\t\tp.GetOpMsg(), p.VerSeq, p.ExtentType, cnt)\n\t\tverData := make([]byte, cnt*verInfoCnt)\n\t\tif _, err = io.ReadFull(c, verData); err != nil {\n\t\t\tlog.LogWarnf(\"ReadFromConnWithVer: read ver slice from conn failed, err %s\", err.Error())\n\t\t\treturn err\n\t\t}\n\n\t\terr = p.UnmarshalVersionSlice(int(cnt), verData)\n\t\tif err != nil {\n\t\t\tlog.LogWarnf(\"ReadFromConnWithVer: unmarshal ver slice failed, err %s\", err.Error())\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif p.ArgLen > 0 {\n\t\tp.Arg = make([]byte, int(p.ArgLen))\n\t\tif _, err = io.ReadFull(c, p.Arg[:int(p.ArgLen)]); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif p.Size < 0 {\n\t\treturn syscall.EBADMSG\n\t}\n\tsize := p.Size\n\tif (p.Opcode == OpRead || p.Opcode == OpStreamRead || p.Opcode == OpExtentRepairRead || p.Opcode == OpStreamFollowerRead) && p.ResultCode == OpInitResultCode {\n\t\tsize = 0\n\t}\n\tp.Data = make([]byte, size)\n\tif n, err = io.ReadFull(c, p.Data[:size]); err != nil {\n\t\treturn err\n\t}\n\tif n != int(size) {\n\t\treturn syscall.EBADMSG\n\t}\n\treturn nil\n}\n\n// ReadFromConn reads the data from the given connection.\nfunc (p *Packet) ReadFromConn(c net.Conn, timeoutSec int) (err error) {\n\tif timeoutSec != NoReadDeadlineTime {\n\t\tc.SetReadDeadline(time.Now().Add(time.Second * time.Duration(timeoutSec)))\n\t} else {\n\t\tc.SetReadDeadline(time.Time{})\n\t}\n\theader, err := Buffers.Get(util.PacketHeaderSize)\n\tif err != nil {\n\t\theader = make([]byte, util.PacketHeaderSize)\n\t}\n\tdefer Buffers.Put(header)\n\tvar n int\n\tif n, err = io.ReadFull(c, header); err != nil {\n\t\treturn\n\t}\n\tif n != util.PacketHeaderSize {\n\t\treturn syscall.EBADMSG\n\t}\n\tif err = p.UnmarshalHeader(header); err != nil {\n\t\treturn\n\t}\n\n\tif p.ArgLen > 0 {\n\t\tp.Arg = make([]byte, int(p.ArgLen))\n\t\tif _, err = io.ReadFull(c, p.Arg[:int(p.ArgLen)]); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif p.Size < 0 {\n\t\treturn syscall.EBADMSG\n\t}\n\tsize := p.Size\n\tif (p.Opcode == OpRead || p.Opcode == OpStreamRead || p.Opcode == OpExtentRepairRead || p.Opcode == OpStreamFollowerRead) && p.ResultCode == OpInitResultCode {\n\t\tsize = 0\n\t}\n\tp.Data = make([]byte, size)\n\tif n, err = io.ReadFull(c, p.Data[:size]); err != nil {\n\t\treturn err\n\t}\n\tif n != int(size) {\n\t\treturn syscall.EBADMSG\n\t}\n\treturn nil\n}\n\n// PacketOkReply sets the result code as OpOk, and sets the body as empty.\nfunc (p *Packet) PacketOkReply() {\n\tp.ResultCode = OpOk\n\tp.Size = 0\n\tp.Data = nil\n\tp.ArgLen = 0\n}\n\n// PacketOkWithBody sets the result code as OpOk, and sets the body with the give data.\nfunc (p *Packet) PacketOkWithBody(reply []byte) {\n\tp.Size = uint32(len(reply))\n\tp.Data = make([]byte, p.Size)\n\tcopy(p.Data[:p.Size], reply)\n\tp.ResultCode = OpOk\n\tp.ArgLen = 0\n}\n\n// attention use for tmp byte arr, eg: json marshal data\nfunc (p *Packet) PacketOkWithByte(reply []byte) {\n\tp.Size = uint32(len(reply))\n\tp.Data = reply\n\tp.ResultCode = OpOk\n\tp.ArgLen = 0\n}\n\n// PacketErrorWithBody sets the packet with error code whose body is filled with the given data.\nfunc (p *Packet) PacketErrorWithBody(code uint8, reply []byte) {\n\tp.Size = uint32(len(reply))\n\tp.Data = make([]byte, p.Size)\n\tcopy(p.Data[:p.Size], reply)\n\tp.ResultCode = code\n\tp.ArgLen = 0\n}\n\nfunc (p *Packet) SetPacketHasPrepare() {\n\tp.setPacketPrefix()\n\tp.HasPrepare = true\n}\n\nfunc (p *Packet) SetPacketRePrepare() {\n\tp.HasPrepare = false\n}\n\nfunc (p *Packet) AddMesgLog(m string) {\n\tp.mesg += m\n}\n\n// GetUniqueLogId returns the unique log ID.\nfunc (p *Packet) GetUniqueLogId() (m string) {\n\tdefer func() {\n\t\tm = m + fmt.Sprintf(\"_ResultMesg(%v)\", p.GetResultMsg())\n\t}()\n\tif p.HasPrepare {\n\t\tm = p.mesg\n\t\treturn\n\t}\n\tm = fmt.Sprintf(\"Req(%v)_Partition(%v)_\", p.ReqID, p.PartitionID)\n\tif p.Opcode == OpSplitMarkDelete || (IsTinyExtentType(p.ExtentType) && p.Opcode == OpMarkDelete) && len(p.Data) > 0 {\n\t\text := new(TinyExtentDeleteRecord)\n\t\terr := json.Unmarshal(p.Data, ext)\n\t\tif err == nil {\n\t\t\tm += fmt.Sprintf(\"Extent(%v)_ExtentOffset(%v)_Size(%v)_Opcode(%v)\",\n\t\t\t\text.ExtentId, ext.ExtentOffset, ext.Size, p.GetOpMsg())\n\t\t\treturn m\n\t\t}\n\t} else if p.Opcode == OpReadTinyDeleteRecord || p.Opcode == OpNotifyReplicasToRepair || p.Opcode == OpDataNodeHeartbeat ||\n\t\tp.Opcode == OpLoadDataPartition || p.Opcode == OpBatchDeleteExtent {\n\t\tp.mesg += fmt.Sprintf(\"Opcode(%v)\", p.GetOpMsg())\n\t\treturn\n\t} else if p.Opcode == OpBroadcastMinAppliedID || p.Opcode == OpGetAppliedId {\n\t\tif p.Size > 0 {\n\t\t\tapplyID := binary.BigEndian.Uint64(p.Data)\n\t\t\tm += fmt.Sprintf(\"Opcode(%v)_AppliedID(%v)\", p.GetOpMsg(), applyID)\n\t\t} else {\n\t\t\tm += fmt.Sprintf(\"Opcode(%v)\", p.GetOpMsg())\n\t\t}\n\t\treturn m\n\t}\n\tm = fmt.Sprintf(\"Req(%v)_Partition(%v)_Extent(%v)_ExtentOffset(%v)_KernelOffset(%v)_\"+\n\t\t\"Size(%v)_Opcode(%v)_CRC(%v)\",\n\t\tp.ReqID, p.PartitionID, p.ExtentID, p.ExtentOffset,\n\t\tp.KernelOffset, p.Size, p.GetOpMsg(), p.CRC)\n\n\treturn\n}\n\nfunc (p *Packet) setPacketPrefix() {\n\tp.mesg = fmt.Sprintf(\"Req(%v)_Partition(%v)_\", p.ReqID, p.PartitionID)\n\tif (p.Opcode == OpSplitMarkDelete || (IsTinyExtentType(p.ExtentType) && p.Opcode == OpMarkDelete)) && len(p.Data) > 0 {\n\t\text := new(TinyExtentDeleteRecord)\n\t\terr := json.Unmarshal(p.Data, ext)\n\t\tif err == nil {\n\t\t\tp.mesg += fmt.Sprintf(\"Extent(%v)_ExtentOffset(%v)_Size(%v)_Opcode(%v)\",\n\t\t\t\text.ExtentId, ext.ExtentOffset, ext.Size, p.GetOpMsg())\n\t\t\treturn\n\t\t}\n\t} else if p.Opcode == OpReadTinyDeleteRecord || p.Opcode == OpNotifyReplicasToRepair || p.Opcode == OpDataNodeHeartbeat ||\n\t\tp.Opcode == OpLoadDataPartition || p.Opcode == OpBatchDeleteExtent {\n\t\tp.mesg += fmt.Sprintf(\"Opcode(%v)\", p.GetOpMsg())\n\t\treturn\n\t} else if p.Opcode == OpBroadcastMinAppliedID || p.Opcode == OpGetAppliedId {\n\t\tif p.Size > 0 {\n\t\t\tapplyID := binary.BigEndian.Uint64(p.Data)\n\t\t\tp.mesg += fmt.Sprintf(\"Opcode(%v)_AppliedID(%v)\", p.GetOpMsg(), applyID)\n\t\t} else {\n\t\t\tp.mesg += fmt.Sprintf(\"Opcode(%v)\", p.GetOpMsg())\n\t\t}\n\t\treturn\n\t}\n\tp.mesg = fmt.Sprintf(\"Req(%v)_Partition(%v)_Extent(%v)_ExtentOffset(%v)_KernelOffset(%v)_\"+\n\t\t\"Size(%v)_Opcode(%v)_CRC(%v)\",\n\t\tp.ReqID, p.PartitionID, p.ExtentID, p.ExtentOffset,\n\t\tp.KernelOffset, p.Size, p.GetOpMsg(), p.CRC)\n\n}\n\n// IsForwardPkt returns if the packet is the forward packet (a packet that will be forwarded to the followers).\nfunc (p *Packet) IsForwardPkt() bool {\n\treturn p.RemainingFollowers > 0\n}\n\n// LogMessage logs the given message.\nfunc (p *Packet) LogMessage(action, remote string, start int64, err error) (m string) {\n\tif err == nil {\n\t\tm = fmt.Sprintf(\"id[%v] isPrimaryBackReplLeader[%v] remote[%v] \"+\n\t\t\t\" cost[%v] \", p.GetUniqueLogId(), p.IsForwardPkt(), remote, (time.Now().UnixNano()-start)/1e6)\n\n\t} else {\n\t\tm = fmt.Sprintf(\"id[%v] isPrimaryBackReplLeader[%v] remote[%v]\"+\n\t\t\t\", err[%v]\", p.GetUniqueLogId(), p.IsForwardPkt(), remote, err.Error())\n\t}\n\n\treturn\n}\n\n// ShallRetry returns if we should retry the packet.\nfunc (p *Packet) ShouldRetryWithVersionList() bool {\n\treturn p.ResultCode == OpAgainVerionList\n}\n\n// ShallRetry returns if we should retry the packet.\nfunc (p *Packet) ShouldRetry() bool {\n\treturn p.ResultCode == OpAgain || p.ResultCode == OpErr\n}\n\nfunc (p *Packet) IsBatchDeleteExtents() bool {\n\treturn p.Opcode == OpBatchDeleteExtent\n}\n\nfunc InitBufferPool(bufLimit int64) {\n\tbuf.NormalBuffersTotalLimit = bufLimit\n\tbuf.HeadBuffersTotalLimit = bufLimit\n\tbuf.HeadVerBuffersTotalLimit = bufLimit\n\n\tBuffers = buf.NewBufferPool()\n}\n", "// Copyright 2018 The CubeFS Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n// implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage stream\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"net\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/cubefs/cubefs/proto\"\n\t\"github.com/cubefs/cubefs/sdk/data/wrapper\"\n\t\"github.com/cubefs/cubefs/storage\"\n\t\"github.com/cubefs/cubefs/util\"\n\t\"github.com/cubefs/cubefs/util/errors\"\n\t\"github.com/cubefs/cubefs/util/log\"\n)\n\nconst (\n\tMaxSelectDataPartitionForWrite = 32\n\tMaxNewHandlerRetry             = 3\n\tMaxPacketErrorCount            = 128\n\tMaxDirtyListLen                = 0\n)\n\nconst (\n\tStreamerNormal int32 = iota\n\tStreamerError\n\tLastEKVersionNotEqual\n)\n\nconst (\n\tstreamWriterFlushPeriod       = 3\n\tstreamWriterIdleTimeoutPeriod = 10\n)\n\n// VerUpdateRequest defines an verseq update request.\ntype VerUpdateRequest struct {\n\terr    error\n\tverSeq uint64\n\tdone   chan struct{}\n}\n\n// OpenRequest defines an open request.\ntype OpenRequest struct {\n\tdone chan struct{}\n}\n\n// WriteRequest defines a write request.\ntype WriteRequest struct {\n\tfileOffset int\n\tsize       int\n\tdata       []byte\n\tflags      int\n\twriteBytes int\n\terr        error\n\tdone       chan struct{}\n\tcheckFunc  func() error\n}\n\n// FlushRequest defines a flush request.\ntype FlushRequest struct {\n\terr  error\n\tdone chan struct{}\n}\n\n// ReleaseRequest defines a release request.\ntype ReleaseRequest struct {\n\terr  error\n\tdone chan struct{}\n}\n\n// TruncRequest defines a truncate request.\ntype TruncRequest struct {\n\tsize     int\n\terr      error\n\tfullPath string\n\tdone     chan struct{}\n}\n\n// EvictRequest defines an evict request.\ntype EvictRequest struct {\n\terr  error\n\tdone chan struct{}\n}\n\n// Open request shall grab the lock until request is sent to the request channel\nfunc (s *Streamer) IssueOpenRequest() error {\n\trequest := openRequestPool.Get().(*OpenRequest)\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\ts.client.streamerLock.Unlock()\n\t<-request.done\n\topenRequestPool.Put(request)\n\treturn nil\n}\n\nfunc (s *Streamer) IssueWriteRequest(offset int, data []byte, flags int, checkFunc func() error) (write int, err error) {\n\tif atomic.LoadInt32(&s.status) >= StreamerError {\n\t\treturn 0, errors.New(fmt.Sprintf(\"IssueWriteRequest: stream writer in error status, ino(%v)\", s.inode))\n\t}\n\n\ts.writeLock.Lock()\n\trequest := writeRequestPool.Get().(*WriteRequest)\n\trequest.data = data\n\trequest.fileOffset = offset\n\trequest.size = len(data)\n\trequest.flags = flags\n\trequest.done = make(chan struct{}, 1)\n\trequest.checkFunc = checkFunc\n\n\ts.request <- request\n\ts.writeLock.Unlock()\n\n\t<-request.done\n\terr = request.err\n\twrite = request.writeBytes\n\twriteRequestPool.Put(request)\n\treturn\n}\n\nfunc (s *Streamer) IssueFlushRequest() error {\n\trequest := flushRequestPool.Get().(*FlushRequest)\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\t<-request.done\n\terr := request.err\n\tflushRequestPool.Put(request)\n\treturn err\n}\n\nfunc (s *Streamer) IssueReleaseRequest() error {\n\trequest := releaseRequestPool.Get().(*ReleaseRequest)\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\ts.client.streamerLock.Unlock()\n\t<-request.done\n\terr := request.err\n\treleaseRequestPool.Put(request)\n\treturn err\n}\n\nfunc (s *Streamer) IssueTruncRequest(size int, fullPath string) error {\n\trequest := truncRequestPool.Get().(*TruncRequest)\n\trequest.size = size\n\trequest.fullPath = fullPath\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\t<-request.done\n\terr := request.err\n\ttruncRequestPool.Put(request)\n\treturn err\n}\n\nfunc (s *Streamer) IssueEvictRequest() error {\n\trequest := evictRequestPool.Get().(*EvictRequest)\n\trequest.done = make(chan struct{}, 1)\n\ts.request <- request\n\ts.client.streamerLock.Unlock()\n\t<-request.done\n\terr := request.err\n\tevictRequestPool.Put(request)\n\treturn err\n}\n\nfunc (s *Streamer) GetStoreMod(offset int, size int) (storeMode int) {\n\t// Small files are usually written in a single write, so use tiny extent\n\t// store only for the first write operation.\n\tif offset > 0 || offset+size > s.tinySizeLimit() {\n\t\tstoreMode = proto.NormalExtentType\n\t} else {\n\t\tstoreMode = proto.TinyExtentType\n\t}\n\treturn\n}\n\nfunc (s *Streamer) server() {\n\tt := time.NewTicker(2 * time.Second)\n\tdefer t.Stop()\n\t//defer func() {\n\t//\tif !s.client.disableMetaCache && s.needBCache {\n\t//\t\tclose(s.request)\n\t//\t\ts.request = nil\n\t//\t}\n\t//}()\n\n\tfor {\n\t\tselect {\n\t\tcase request := <-s.request:\n\t\t\ts.handleRequest(request)\n\t\t\ts.idle = 0\n\t\t\ts.traversed = 0\n\t\tcase <-s.done:\n\t\t\ts.abort()\n\t\t\tlog.LogDebugf(\"done server: evict, ino(%v)\", s.inode)\n\t\t\treturn\n\t\tcase <-t.C:\n\t\t\ts.traverse()\n\t\t\tif s.refcnt <= 0 {\n\n\t\t\t\ts.client.streamerLock.Lock()\n\t\t\t\tif s.idle >= streamWriterIdleTimeoutPeriod && len(s.request) == 0 {\n\t\t\t\t\tif s.client.disableMetaCache || !s.needBCache {\n\t\t\t\t\t\tdelete(s.client.streamers, s.inode)\n\t\t\t\t\t\tif s.client.evictIcache != nil {\n\t\t\t\t\t\t\ts.client.evictIcache(s.inode)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\ts.isOpen = false\n\t\t\t\t\t// fail the remaining requests in such case\n\t\t\t\t\ts.clearRequests()\n\t\t\t\t\ts.client.streamerLock.Unlock()\n\n\t\t\t\t\tlog.LogDebugf(\"done server: no requests for a long time, ino(%v)\", s.inode)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ts.client.streamerLock.Unlock()\n\n\t\t\t\ts.idle++\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (s *Streamer) clearRequests() {\n\tfor {\n\t\tselect {\n\t\tcase request := <-s.request:\n\t\t\ts.abortRequest(request)\n\t\tdefault:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (s *Streamer) abortRequest(request interface{}) {\n\tswitch request := request.(type) {\n\tcase *OpenRequest:\n\t\trequest.done <- struct{}{}\n\tcase *WriteRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tcase *TruncRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tcase *FlushRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tcase *ReleaseRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tcase *EvictRequest:\n\t\trequest.err = syscall.EAGAIN\n\t\trequest.done <- struct{}{}\n\tdefault:\n\t}\n}\n\nfunc (s *Streamer) handleRequest(request interface{}) {\n\tif atomic.LoadInt32(&s.needUpdateVer) == 1 {\n\t\ts.closeOpenHandler()\n\t\tatomic.StoreInt32(&s.needUpdateVer, 0)\n\t}\n\n\tswitch request := request.(type) {\n\tcase *OpenRequest:\n\t\ts.open()\n\t\trequest.done <- struct{}{}\n\tcase *WriteRequest:\n\t\trequest.writeBytes, request.err = s.write(request.data, request.fileOffset, request.size, request.flags, request.checkFunc)\n\t\trequest.done <- struct{}{}\n\tcase *TruncRequest:\n\t\trequest.err = s.truncate(request.size, request.fullPath)\n\t\trequest.done <- struct{}{}\n\tcase *FlushRequest:\n\t\trequest.err = s.flush()\n\t\trequest.done <- struct{}{}\n\tcase *ReleaseRequest:\n\t\trequest.err = s.release()\n\t\trequest.done <- struct{}{}\n\tcase *EvictRequest:\n\t\trequest.err = s.evict()\n\t\trequest.done <- struct{}{}\n\tcase *VerUpdateRequest:\n\t\trequest.err = s.updateVer(request.verSeq)\n\t\trequest.done <- struct{}{}\n\tdefault:\n\t}\n\n}\n\nfunc (s *Streamer) write(data []byte, offset, size, flags int, checkFunc func() error) (total int, err error) {\n\tvar (\n\t\tdirect     bool\n\t\tretryTimes int8\n\t)\n\n\tif flags&proto.FlagsSyncWrite != 0 {\n\t\tdirect = true\n\t}\nbegin:\n\tif flags&proto.FlagsAppend != 0 {\n\t\tfilesize, _ := s.extents.Size()\n\t\toffset = filesize\n\t}\n\n\tlog.LogDebugf(\"Streamer write enter: ino(%v) offset(%v) size(%v)\", s.inode, offset, size)\n\n\tctx := context.Background()\n\ts.client.writeLimiter.Wait(ctx)\n\n\trequests := s.extents.PrepareWriteRequests(offset, size, data)\n\tlog.LogDebugf(\"Streamer write: ino(%v) prepared requests(%v)\", s.inode, requests)\n\n\tisChecked := false\n\t// Must flush before doing overwrite\n\tfor _, req := range requests {\n\t\tif req.ExtentKey == nil {\n\t\t\tcontinue\n\t\t}\n\t\terr = s.flush()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\t// some extent key in requests with partition id 0 means it's append operation and on flight.\n\t\t// need to flush and get the right key then used to make modification\n\t\trequests = s.extents.PrepareWriteRequests(offset, size, data)\n\t\tlog.LogDebugf(\"Streamer write: ino(%v) prepared requests after flush(%v)\", s.inode, requests)\n\t\tbreak\n\t}\n\n\tfor _, req := range requests {\n\t\tvar writeSize int\n\t\tif req.ExtentKey != nil {\n\t\t\tif s.client.bcacheEnable {\n\t\t\t\tcacheKey := util.GenerateRepVolKey(s.client.volumeName, s.inode, req.ExtentKey.PartitionId, req.ExtentKey.ExtentId, uint64(req.FileOffset))\n\t\t\t\tif _, ok := s.inflightEvictL1cache.Load(cacheKey); !ok {\n\t\t\t\t\tgo func(cacheKey string) {\n\t\t\t\t\t\ts.inflightEvictL1cache.Store(cacheKey, true)\n\t\t\t\t\t\ts.client.evictBcache(cacheKey)\n\t\t\t\t\t\ts.inflightEvictL1cache.Delete(cacheKey)\n\t\t\t\t\t}(cacheKey)\n\t\t\t\t}\n\t\t\t}\n\t\t\tlog.LogDebugf(\"action[streamer.write] inode [%v] latest seq [%v] extentkey seq [%v]  info [%v]\",\n\t\t\t\ts.inode, s.verSeq, req.ExtentKey.GetSeq(), req.ExtentKey)\n\t\t\tif req.ExtentKey.GetSeq() == s.verSeq {\n\t\t\t\twriteSize, err = s.doOverwrite(req, direct)\n\t\t\t\tif err == proto.ErrCodeVersionOp {\n\t\t\t\t\tlog.LogDebugf(\"action[streamer.write] write need version update\")\n\t\t\t\t\tif err = s.GetExtents(); err != nil {\n\t\t\t\t\t\tlog.LogErrorf(\"action[streamer.write] err %v\", err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif retryTimes > 3 {\n\t\t\t\t\t\terr = proto.ErrCodeVersionOp\n\t\t\t\t\t\tlog.LogWarnf(\"action[streamer.write] err %v\", err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\ttime.Sleep(time.Millisecond * 100)\n\t\t\t\t\tretryTimes++\n\t\t\t\t\tlog.LogDebugf(\"action[streamer.write] err %v retryTimes %v\", err, retryTimes)\n\t\t\t\t\tgoto begin\n\t\t\t\t}\n\t\t\t\tlog.LogDebugf(\"action[streamer.write] err %v retryTimes %v\", err, retryTimes)\n\t\t\t} else {\n\t\t\t\tlog.LogDebugf(\"action[streamer.write] ino %v doOverWriteByAppend extent key (%v)\", s.inode, req.ExtentKey)\n\t\t\t\twriteSize, _, err, _ = s.doOverWriteByAppend(req, direct)\n\t\t\t}\n\t\t\tif s.client.bcacheEnable {\n\t\t\t\tcacheKey := util.GenerateKey(s.client.volumeName, s.inode, uint64(req.FileOffset))\n\t\t\t\tgo s.client.evictBcache(cacheKey)\n\t\t\t}\n\t\t} else {\n\t\t\tif !isChecked && checkFunc != nil {\n\t\t\t\tisChecked = true\n\t\t\t\tif err = checkFunc(); err != nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\t// try append write, get response\n\t\t\tlog.LogDebugf(\"action[streamer.write] doAppendWrite req: ExtentKey(%v) FileOffset(%v) size(%v)\",\n\t\t\t\treq.ExtentKey, req.FileOffset, req.Size)\n\t\t\tvar status int32\n\t\t\t// First, attempt sequential writes using neighboring extent keys. If the last extent has a different version,\n\t\t\t// it indicates that the extent may have been fully utilized by the previous version.\n\t\t\t// Next, try writing and directly checking the extent at the datanode. If the extent cannot be reused, create a new extent for writing.\n\t\t\tif writeSize, err, status = s.doAppendWrite(req.Data, req.FileOffset, req.Size, direct, true); status == LastEKVersionNotEqual {\n\t\t\t\tlog.LogDebugf(\"action[streamer.write] tryDirectAppendWrite req %v FileOffset %v size %v\", req.ExtentKey, req.FileOffset, req.Size)\n\t\t\t\tif writeSize, _, err, status = s.tryDirectAppendWrite(req, direct); status == int32(proto.OpTryOtherExtent) {\n\t\t\t\t\tlog.LogDebugf(\"action[streamer.write] doAppendWrite again req %v FileOffset %v size %v\", req.ExtentKey, req.FileOffset, req.Size)\n\t\t\t\t\twriteSize, err, _ = s.doAppendWrite(req.Data, req.FileOffset, req.Size, direct, false)\n\t\t\t\t}\n\t\t\t}\n\t\t\tlog.LogDebugf(\"action[streamer.write] doAppendWrite status %v err %v\", status, err)\n\t\t}\n\t\tif err != nil {\n\t\t\tlog.LogErrorf(\"Streamer write: ino(%v) err(%v)\", s.inode, err)\n\t\t\tbreak\n\t\t}\n\t\ttotal += writeSize\n\t}\n\tif filesize, _ := s.extents.Size(); offset+total > filesize {\n\t\ts.extents.SetSize(uint64(offset+total), false)\n\t\tlog.LogDebugf(\"Streamer write: ino(%v) filesize changed to (%v)\", s.inode, offset+total)\n\t}\n\tlog.LogDebugf(\"Streamer write exit: ino(%v) offset(%v) size(%v) done total(%v) err(%v)\", s.inode, offset, size, total, err)\n\treturn\n}\n\nfunc (s *Streamer) doOverWriteByAppend(req *ExtentRequest, direct bool) (total int, extKey *proto.ExtentKey, err error, status int32) {\n\t// the extent key needs to be updated because when preparing the requests,\n\t// the obtained extent key could be a local key which can be inconsistent with the remote key.\n\t// the OpTryWriteAppend is a special case, ignore it\n\treq.ExtentKey = s.extents.Get(uint64(req.FileOffset))\n\treturn s.doDirectWriteByAppend(req, direct, proto.OpRandomWriteAppend)\n}\n\nfunc (s *Streamer) tryDirectAppendWrite(req *ExtentRequest, direct bool) (total int, extKey *proto.ExtentKey, err error, status int32) {\n\n\treq.ExtentKey = s.handler.key\n\treturn s.doDirectWriteByAppend(req, direct, proto.OpTryWriteAppend)\n}\n\nfunc (s *Streamer) doDirectWriteByAppend(req *ExtentRequest, direct bool, op uint8) (total int, extKey *proto.ExtentKey, err error, status int32) {\n\tvar (\n\t\tdp        *wrapper.DataPartition\n\t\treqPacket *Packet\n\t)\n\n\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v enter in req %v\", s.inode, req)\n\terr = s.flush()\n\tif err != nil {\n\t\treturn\n\t}\n\n\tif req.ExtentKey == nil {\n\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: extent key not exist, ino(%v) ekFileOffset(%v) ek(%v)\", s.inode, req.FileOffset, req.ExtentKey))\n\t\treturn\n\t}\n\n\tif dp, err = s.client.dataWrapper.GetDataPartition(req.ExtentKey.PartitionId); err != nil {\n\t\t// TODO unhandled error\n\t\terrors.Trace(err, \"doDirectWriteByAppend: ino(%v) failed to get datapartition, ek(%v)\", s.inode, req.ExtentKey)\n\t\treturn\n\t}\n\n\tretry := true\n\tif proto.IsCold(s.client.volumeType) {\n\t\tretry = false\n\t}\n\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v  data process\", s.inode)\n\n\taddr := dp.LeaderAddr\n\tif storage.IsTinyExtent(req.ExtentKey.ExtentId) {\n\t\taddr = dp.Hosts[0]\n\t\treqPacket = NewWriteTinyDirectly(s.inode, req.ExtentKey.PartitionId, req.FileOffset, dp)\n\t} else {\n\t\treqPacket = NewOverwriteByAppendPacket(dp, req.ExtentKey.ExtentId, int(req.ExtentKey.ExtentOffset)+int(req.ExtentKey.Size),\n\t\t\ts.inode, req.FileOffset, direct, op)\n\t}\n\n\tsc := &StreamConn{\n\t\tdp:       dp,\n\t\tcurrAddr: addr,\n\t}\n\n\treplyPacket := new(Packet)\n\tif req.Size > util.BlockSize {\n\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] inode %v size too large %v\", s.inode, req.Size)\n\t\tpanic(nil)\n\t}\n\tfor total < req.Size { // normally should only run once due to key exist in the system must be less than BlockSize\n\t\t// right position in extent:offset-ek4FileOffset+total+ekExtOffset .\n\t\t// ekExtOffset will be set by replay packet at addExtentInfo(datanode)\n\n\t\tif direct {\n\t\t\treqPacket.Opcode = op\n\t\t}\n\t\tif req.ExtentKey.ExtentId <= storage.TinyExtentCount {\n\t\t\treqPacket.ExtentType = proto.TinyExtentType\n\t\t}\n\n\t\tpackSize := util.Min(req.Size-total, util.BlockSize)\n\t\tcopy(reqPacket.Data[:packSize], req.Data[total:total+packSize])\n\t\treqPacket.Size = uint32(packSize)\n\t\treqPacket.CRC = crc32.ChecksumIEEE(reqPacket.Data[:packSize])\n\n\t\terr = sc.Send(&retry, reqPacket, func(conn *net.TCPConn) (error, bool) {\n\t\t\te := replyPacket.ReadFromConnWithVer(conn, proto.ReadDeadlineTime)\n\t\t\tif e != nil {\n\t\t\t\tlog.LogWarnf(\"doDirectWriteByAppend.Stream Writer doOverwrite: ino(%v) failed to read from connect, req(%v) err(%v)\", s.inode, reqPacket, e)\n\t\t\t\t// Upon receiving TryOtherAddrError, other hosts will be retried.\n\t\t\t\treturn TryOtherAddrError, false\n\t\t\t}\n\t\t\tlog.LogDebugf(\"action[doDirectWriteByAppend] ino(%v) get replyPacket opcode %v resultCode %v\", s.inode, replyPacket.Opcode, replyPacket.ResultCode)\n\t\t\tif replyPacket.ResultCode == proto.OpAgain {\n\t\t\t\treturn nil, true\n\t\t\t}\n\n\t\t\tif replyPacket.ResultCode == proto.OpTryOtherExtent {\n\t\t\t\tstatus = int32(proto.OpTryOtherExtent)\n\t\t\t\treturn nil, false\n\t\t\t}\n\n\t\t\tif replyPacket.ResultCode == proto.OpTryOtherAddr {\n\t\t\t\te = TryOtherAddrError\n\t\t\t\tlog.LogDebugf(\"action[doDirectWriteByAppend] data process err %v\", e)\n\t\t\t}\n\t\t\treturn e, false\n\t\t})\n\n\t\tproto.Buffers.Put(reqPacket.Data)\n\t\treqPacket.Data = nil\n\t\tlog.LogDebugf(\"doDirectWriteByAppend: ino(%v) req(%v) reqPacket(%v) err(%v) replyPacket(%v)\", s.inode, req, reqPacket, err, replyPacket)\n\n\t\tif err != nil || replyPacket.ResultCode != proto.OpOk {\n\t\t\tstatus = int32(replyPacket.ResultCode)\n\t\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: failed or reply NOK: err(%v) ino(%v) req(%v) replyPacket(%v)\", err, s.inode, req, replyPacket))\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] data process err %v\", err)\n\t\t\tbreak\n\t\t}\n\n\t\tif !reqPacket.isValidWriteReply(replyPacket) || reqPacket.CRC != replyPacket.CRC {\n\t\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: is not the corresponding reply, ino(%v) req(%v) replyPacket(%v)\", s.inode, req, replyPacket))\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] data process err %v\", err)\n\t\t\tbreak\n\t\t}\n\n\t\ttotal += packSize\n\t\tbreak\n\t}\n\tif err != nil {\n\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] data process err %v\", err)\n\t\treturn\n\t}\n\textKey = &proto.ExtentKey{\n\t\tFileOffset:   uint64(req.FileOffset),\n\t\tPartitionId:  req.ExtentKey.PartitionId,\n\t\tExtentId:     replyPacket.ExtentID,\n\t\tExtentOffset: uint64(replyPacket.ExtentOffset),\n\t\tSize:         uint32(total),\n\t\tSnapInfo: &proto.ExtSnapInfo{\n\t\t\tVerSeq: s.verSeq,\n\t\t},\n\t}\n\tif op == proto.OpRandomWriteAppend || op == proto.OpSyncRandomWriteAppend {\n\t\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v local cache process start extKey %v\", s.inode, extKey)\n\t\tif err = s.extents.SplitExtentKey(s.inode, extKey); err != nil {\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] inode %v llocal cache process err %v\", s.inode, err)\n\t\t\treturn\n\t\t}\n\t\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v meta extent split with ek (%v)\", s.inode, extKey)\n\t\tif err = s.client.splitExtentKey(s.parentInode, s.inode, *extKey); err != nil {\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] inode %v meta extent split process err %v\", s.inode, err)\n\t\t\treturn\n\t\t}\n\t} else {\n\t\tdiscards := s.extents.Append(extKey, true)\n\t\tif err = s.client.appendExtentKey(s.parentInode, s.inode, *extKey, discards); err != nil {\n\t\t\tlog.LogErrorf(\"action[doDirectWriteByAppend] inode %v meta extent split process err %v\", s.inode, err)\n\t\t\treturn\n\t\t}\n\t}\n\tlog.LogDebugf(\"action[doDirectWriteByAppend] inode %v process over!\", s.inode)\n\treturn\n}\n\nfunc (s *Streamer) doOverwrite(req *ExtentRequest, direct bool) (total int, err error) {\n\tvar dp *wrapper.DataPartition\n\n\terr = s.flush()\n\tif err != nil {\n\t\treturn\n\t}\n\n\toffset := req.FileOffset\n\tsize := req.Size\n\n\t// the extent key needs to be updated because when preparing the requests,\n\t// the obtained extent key could be a local key which can be inconsistent with the remote key.\n\treq.ExtentKey = s.extents.Get(uint64(offset))\n\tekFileOffset := int(req.ExtentKey.FileOffset)\n\tekExtOffset := int(req.ExtentKey.ExtentOffset)\n\tif req.ExtentKey == nil {\n\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: extent key not exist, ino(%v) ekFileOffset(%v) ek(%v)\", s.inode, ekFileOffset, req.ExtentKey))\n\t\treturn\n\t}\n\n\tif dp, err = s.client.dataWrapper.GetDataPartition(req.ExtentKey.PartitionId); err != nil {\n\t\t// TODO unhandled error\n\t\terrors.Trace(err, \"doOverwrite: ino(%v) failed to get datapartition, ek(%v)\", s.inode, req.ExtentKey)\n\t\treturn\n\t}\n\n\tretry := true\n\tif proto.IsCold(s.client.volumeType) {\n\t\tretry = false\n\t}\n\n\tsc := NewStreamConn(dp, false)\n\n\tfor total < size {\n\t\treqPacket := NewOverwritePacket(dp, req.ExtentKey.ExtentId, offset-ekFileOffset+total+ekExtOffset, s.inode, offset)\n\t\treqPacket.VerSeq = s.client.multiVerMgr.latestVerSeq\n\t\treqPacket.VerList = s.client.multiVerMgr.verList.VerList\n\t\treqPacket.ExtentType |= proto.MultiVersionFlag\n\t\treqPacket.ExtentType |= proto.VersionListFlag\n\n\t\tlog.LogDebugf(\"action[doOverwrite] inode %v extentid %v,extentOffset %v(%v,%v,%v,%v) offset %v, streamer seq %v\", s.inode, req.ExtentKey.ExtentId, reqPacket.ExtentOffset,\n\t\t\toffset, ekFileOffset, total, ekExtOffset, offset, s.verSeq)\n\t\tif direct {\n\t\t\treqPacket.Opcode = proto.OpSyncRandomWrite\n\t\t}\n\t\tpackSize := util.Min(size-total, util.BlockSize)\n\t\tcopy(reqPacket.Data[:packSize], req.Data[total:total+packSize])\n\t\treqPacket.Size = uint32(packSize)\n\t\treqPacket.CRC = crc32.ChecksumIEEE(reqPacket.Data[:packSize])\n\t\treqPacket.VerSeq = s.verSeq\n\n\t\treplyPacket := new(Packet)\n\t\terr = sc.Send(&retry, reqPacket, func(conn *net.TCPConn) (error, bool) {\n\t\t\te := replyPacket.ReadFromConnWithVer(conn, proto.ReadDeadlineTime)\n\t\t\tif e != nil {\n\t\t\t\tlog.LogWarnf(\"Stream Writer doOverwrite: ino(%v) failed to read from connect, req(%v) err(%v)\", s.inode, reqPacket, e)\n\t\t\t\t// Upon receiving TryOtherAddrError, other hosts will be retried.\n\t\t\t\treturn TryOtherAddrError, false\n\t\t\t}\n\t\t\tlog.LogDebugf(\"action[doOverwrite] streamer verseq (%v) datanode rsp seq (%v) code(%v)\", s.verSeq, replyPacket.VerSeq, replyPacket.ResultCode)\n\t\t\tif replyPacket.ResultCode == proto.OpAgain {\n\t\t\t\treturn nil, true\n\t\t\t}\n\n\t\t\tif replyPacket.ResultCode == proto.OpTryOtherAddr {\n\t\t\t\te = TryOtherAddrError\n\t\t\t}\n\n\t\t\tif replyPacket.ResultCode == proto.ErrCodeVersionOpError {\n\t\t\t\te = proto.ErrCodeVersionOp\n\t\t\t\tlog.LogWarnf(\"action[doOverwrite] verseq (%v) be updated to (%v) by datanode rsp\", s.verSeq, replyPacket.VerSeq)\n\t\t\t\ts.verSeq = replyPacket.VerSeq\n\t\t\t\ts.extents.verSeq = s.verSeq\n\t\t\t\ts.client.UpdateLatestVer(&proto.VolVersionInfoList{VerList: replyPacket.VerList})\n\t\t\t\treturn e, false\n\t\t\t}\n\n\t\t\treturn e, false\n\t\t})\n\n\t\tproto.Buffers.Put(reqPacket.Data)\n\t\treqPacket.Data = nil\n\t\tlog.LogDebugf(\"doOverwrite: ino(%v) req(%v) reqPacket(%v) err(%v) replyPacket(%v)\", s.inode, req, reqPacket, err, replyPacket)\n\n\t\tif err != nil || replyPacket.ResultCode != proto.OpOk {\n\t\t\tif replyPacket.ResultCode == proto.ErrCodeVersionOpError {\n\t\t\t\terr = proto.ErrCodeVersionOp\n\t\t\t\tlog.LogWarnf(\"doOverwrite: need retry.ino(%v) req(%v) reqPacket(%v) err(%v) replyPacket(%v)\", s.inode, req, reqPacket, err, replyPacket)\n\t\t\t\treturn\n\t\t\t}\n\t\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: failed or reply NOK: err(%v) ino(%v) req(%v) replyPacket(%v)\", err, s.inode, req, replyPacket))\n\t\t\tbreak\n\t\t}\n\n\t\tif !reqPacket.isValidWriteReply(replyPacket) || reqPacket.CRC != replyPacket.CRC {\n\t\t\terr = errors.New(fmt.Sprintf(\"doOverwrite: is not the corresponding reply, ino(%v) req(%v) replyPacket(%v)\", s.inode, req, replyPacket))\n\t\t\tbreak\n\t\t}\n\n\t\ttotal += packSize\n\t}\n\treturn\n}\n\nfunc (s *Streamer) tryInitExtentHandlerByLastEk(offset, size int) (isLastEkVerNotEqual bool) {\n\tstoreMode := s.GetStoreMod(offset, size)\n\n\t// && (s.handler == nil || s.handler != nil && s.handler.fileOffset+s.handler.size != offset)  delete ??\n\tif storeMode == proto.NormalExtentType && (s.handler == nil || s.handler != nil && s.handler.fileOffset+s.handler.size != offset) {\n\t\tif currentEK := s.extents.GetEndForAppendWrite(uint64(offset), s.verSeq, false); currentEK != nil && !storage.IsTinyExtent(currentEK.ExtentId) {\n\t\t\tif currentEK.GetSeq() != s.verSeq {\n\t\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk. exist ek seq %v vs request seq %v\", currentEK.GetSeq(), s.verSeq)\n\t\t\t\tisLastEkVerNotEqual = true\n\t\t\t}\n\n\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk: found ek in ExtentCache, extent_id(%v) offset(%v) size(%v), ekoffset(%v) eksize(%v) exist ek seq %v vs request seq %v\",\n\t\t\t\tcurrentEK.ExtentId, offset, size, currentEK.FileOffset, currentEK.Size, currentEK.GetSeq(), s.verSeq)\n\t\t\t_, pidErr := s.client.dataWrapper.GetDataPartition(currentEK.PartitionId)\n\t\t\tif pidErr == nil {\n\t\t\t\tseq := currentEK.GetSeq()\n\t\t\t\tif isLastEkVerNotEqual {\n\t\t\t\t\tseq = s.verSeq\n\t\t\t\t}\n\t\t\t\thandler := NewExtentHandler(s, int(currentEK.FileOffset), storeMode, int(currentEK.Size))\n\t\t\t\thandler.key = &proto.ExtentKey{\n\t\t\t\t\tFileOffset:   currentEK.FileOffset,\n\t\t\t\t\tPartitionId:  currentEK.PartitionId,\n\t\t\t\t\tExtentId:     currentEK.ExtentId,\n\t\t\t\t\tExtentOffset: currentEK.ExtentOffset,\n\t\t\t\t\tSize:         currentEK.Size,\n\t\t\t\t\tSnapInfo: &proto.ExtSnapInfo{\n\t\t\t\t\t\tVerSeq: seq,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\ts.handler = handler\n\t\t\t\ts.dirty = false\n\t\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk: currentEK.PartitionId(%v) found\", currentEK.PartitionId)\n\t\t\t} else {\n\t\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk: currentEK.PartitionId(%v) not found\", currentEK.PartitionId)\n\t\t\t}\n\n\t\t} else {\n\t\t\tlog.LogDebugf(\"tryInitExtentHandlerByLastEk: not found ek in ExtentCache, offset(%v) size(%v)\", offset, size)\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (s *Streamer) doAppendWrite(data []byte, offset, size int, direct bool, reUseEk bool) (total int, err error, status int32) {\n\tvar (\n\t\tek        *proto.ExtentKey\n\t\tstoreMode int\n\t)\n\n\t// Small files are usually written in a single write, so use tiny extent\n\t// store only for the first write operation.\n\tstoreMode = s.GetStoreMod(offset, size)\n\n\tlog.LogDebugf(\"doAppendWrite enter: ino(%v) offset(%v) size(%v) storeMode(%v)\", s.inode, offset, size, storeMode)\n\tif proto.IsHot(s.client.volumeType) {\n\t\tif reUseEk {\n\t\t\tif isLastEkVerNotEqual := s.tryInitExtentHandlerByLastEk(offset, size); isLastEkVerNotEqual {\n\t\t\t\tlog.LogDebugf(\"doAppendWrite enter: ino(%v) tryInitExtentHandlerByLastEk worked\", s.inode)\n\t\t\t\tstatus = LastEKVersionNotEqual\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tfor i := 0; i < MaxNewHandlerRetry; i++ {\n\t\t\tif s.handler == nil {\n\t\t\t\ts.handler = NewExtentHandler(s, offset, storeMode, 0)\n\t\t\t\ts.dirty = false\n\t\t\t} else if s.handler.storeMode != storeMode {\n\t\t\t\t// store mode changed, so close open handler and start a new one\n\t\t\t\ts.closeOpenHandler()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tek, err = s.handler.write(data, offset, size, direct)\n\t\t\tif err == nil && ek != nil {\n\t\t\t\tif !s.dirty {\n\t\t\t\t\ts.dirtylist.Put(s.handler)\n\t\t\t\t\ts.dirty = true\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ts.closeOpenHandler()\n\t\t}\n\t} else {\n\t\ts.handler = NewExtentHandler(s, offset, storeMode, 0)\n\t\ts.dirty = false\n\t\tek, err = s.handler.write(data, offset, size, direct)\n\t\tif err == nil && ek != nil {\n\t\t\tif !s.dirty {\n\t\t\t\ts.dirtylist.Put(s.handler)\n\t\t\t\ts.dirty = true\n\t\t\t}\n\t\t}\n\n\t\terr = s.closeOpenHandler()\n\t}\n\n\tif err != nil || ek == nil {\n\t\tlog.LogErrorf(\"doAppendWrite error: ino(%v) offset(%v) size(%v) err(%v) ek(%v)\", s.inode, offset, size, err, ek)\n\t\treturn\n\t}\n\n\t// This ek is just a local cache for PrepareWriteRequest, so ignore discard eks here.\n\t_ = s.extents.Append(ek, false)\n\ttotal = size\n\n\treturn\n}\n\nfunc (s *Streamer) flush() (err error) {\n\tfor {\n\t\telement := s.dirtylist.Get()\n\t\tif element == nil {\n\t\t\tbreak\n\t\t}\n\t\teh := element.Value.(*ExtentHandler)\n\n\t\tlog.LogDebugf(\"Streamer flush begin: eh(%v)\", eh)\n\t\terr = eh.flush()\n\t\tif err != nil {\n\t\t\tlog.LogErrorf(\"Streamer flush failed: eh(%v)\", eh)\n\t\t\treturn\n\t\t}\n\t\teh.stream.dirtylist.Remove(element)\n\t\tif eh.getStatus() == ExtentStatusOpen {\n\t\t\ts.dirty = false\n\t\t\tlog.LogDebugf(\"Streamer flush handler open: eh(%v)\", eh)\n\t\t} else {\n\t\t\t// TODO unhandled error\n\t\t\teh.cleanup()\n\t\t\tlog.LogDebugf(\"Streamer flush handler cleaned up: eh(%v)\", eh)\n\t\t}\n\t\tlog.LogDebugf(\"Streamer flush end: eh(%v)\", eh)\n\t}\n\treturn\n}\n\nfunc (s *Streamer) traverse() (err error) {\n\ts.traversed++\n\tlength := s.dirtylist.Len()\n\tfor i := 0; i < length; i++ {\n\t\telement := s.dirtylist.Get()\n\t\tif element == nil {\n\t\t\tbreak\n\t\t}\n\t\teh := element.Value.(*ExtentHandler)\n\n\t\tlog.LogDebugf(\"Streamer traverse begin: eh(%v)\", eh)\n\t\tif eh.getStatus() >= ExtentStatusClosed {\n\t\t\t// handler can be in different status such as close, recovery, and error,\n\t\t\t// and therefore there can be packet that has not been flushed yet.\n\t\t\teh.flushPacket()\n\t\t\tif atomic.LoadInt32(&eh.inflight) > 0 {\n\t\t\t\tlog.LogDebugf(\"Streamer traverse skipped: non-zero inflight, eh(%v)\", eh)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\terr = eh.appendExtentKey()\n\t\t\tif err != nil {\n\t\t\t\tlog.LogWarnf(\"Streamer traverse abort: appendExtentKey failed, eh(%v) err(%v)\", eh, err)\n\t\t\t\t// set the streamer to error status to avoid further writes\n\t\t\t\tif err == syscall.EIO {\n\t\t\t\t\tatomic.StoreInt32(&eh.stream.status, StreamerError)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\ts.dirtylist.Remove(element)\n\t\t\teh.cleanup()\n\t\t} else {\n\t\t\tif s.traversed < streamWriterFlushPeriod {\n\t\t\t\tlog.LogDebugf(\"Streamer traverse skipped: traversed(%v) eh(%v)\", s.traversed, eh)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err = eh.flush(); err != nil {\n\t\t\t\tlog.LogWarnf(\"Streamer traverse flush: eh(%v) err(%v)\", eh, err)\n\t\t\t}\n\t\t}\n\t\tlog.LogDebugf(\"Streamer traverse end: eh(%v)\", eh)\n\t}\n\treturn\n}\n\nfunc (s *Streamer) closeOpenHandler() (err error) {\n\t// just in case to avoid infinite loop\n\tvar cnt int = 2 * MaxPacketErrorCount\n\n\thandler := s.handler\n\tfor handler != nil && cnt >= 0 {\n\t\thandler.setClosed()\n\t\tif s.dirtylist.Len() < MaxDirtyListLen {\n\t\t\thandler.flushPacket()\n\t\t} else {\n\t\t\t// TODO unhandled error\n\t\t\terr = s.handler.flush()\n\t\t}\n\t\thandler = handler.recoverHandler\n\t\tcnt--\n\t}\n\n\tif s.handler != nil {\n\t\tif !s.dirty {\n\t\t\t// in case the current handler is not on the dirty list and will not get cleaned up\n\t\t\t// TODO unhandled error\n\t\t\tlog.LogDebugf(\"action[Streamer.closeOpenHandler]\")\n\t\t\ts.handler.cleanup()\n\t\t}\n\t\ts.handler = nil\n\t}\n\treturn err\n}\n\nfunc (s *Streamer) open() {\n\ts.refcnt++\n\tlog.LogDebugf(\"open: streamer(%v) refcnt(%v)\", s, s.refcnt)\n}\n\nfunc (s *Streamer) release() error {\n\ts.refcnt--\n\ts.closeOpenHandler()\n\terr := s.flush()\n\tif err != nil {\n\t\ts.abort()\n\t}\n\tlog.LogDebugf(\"release: streamer(%v) refcnt(%v)\", s, s.refcnt)\n\treturn err\n}\n\nfunc (s *Streamer) evict() error {\n\ts.client.streamerLock.Lock()\n\tif s.refcnt > 0 || len(s.request) != 0 {\n\t\ts.client.streamerLock.Unlock()\n\t\treturn errors.New(fmt.Sprintf(\"evict: streamer(%v) refcnt(%v)\", s, s.refcnt))\n\t}\n\tif s.client.disableMetaCache || !s.needBCache {\n\t\tdelete(s.client.streamers, s.inode)\n\t}\n\ts.client.streamerLock.Unlock()\n\treturn nil\n}\n\nfunc (s *Streamer) abort() {\n\tfor {\n\t\telement := s.dirtylist.Get()\n\t\tif element == nil {\n\t\t\tbreak\n\t\t}\n\t\teh := element.Value.(*ExtentHandler)\n\t\ts.dirtylist.Remove(element)\n\t\t// TODO unhandled error\n\t\teh.cleanup()\n\t}\n}\n\nfunc (s *Streamer) truncate(size int, fullPath string) error {\n\ts.closeOpenHandler()\n\terr := s.flush()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = s.client.truncate(s.inode, uint64(size), fullPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\toldsize, _ := s.extents.Size()\n\tif oldsize <= size {\n\t\ts.extents.SetSize(uint64(size), true)\n\t\treturn nil\n\t}\n\n\ts.extents.TruncDiscard(uint64(size))\n\treturn s.GetExtentsForce()\n}\n\nfunc (s *Streamer) updateVer(verSeq uint64) (err error) {\n\tlog.LogInfof(\"action[stream.updateVer] ver %v update to %v\", s.verSeq, verSeq)\n\tif s.verSeq != verSeq {\n\t\t//log.LogInfof(\"action[stream.updateVer] ver %v update to %v\", s.verSeq, verSeq)\n\t\t//if s.handler != nil {\n\t\t//\ts.handler.verUpdate<-verSeq\n\t\t//} else {\n\t\t//\tlog.LogInfof(\"action[stream.updateVer] ver %v update to %v\", s.verSeq, verSeq)\n\t\t//}\n\t\tlog.LogInfof(\"action[stream.updateVer] ver %v update to %v\", s.verSeq, verSeq)\n\t\ts.verSeq = verSeq\n\t\ts.extents.verSeq = verSeq\n\t}\n\treturn\n}\n\nfunc (s *Streamer) tinySizeLimit() int {\n\treturn util.DefaultTinySizeLimit\n}\n", "// Copyright 2018 The CubeFS Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n// implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage util\n\nimport (\n\t\"crypto/rand\"\n\t\"math/big\"\n\t\"strings\"\n)\n\nfunc SubString(sourceString string, begin, end int) string {\n\tbytes := []byte(sourceString)\n\tstringLength := len(bytes)\n\n\tif begin < 0 {\n\t\tbegin = 0\n\t}\n\tif end > stringLength {\n\t\tend = stringLength\n\t}\n\treturn string(bytes[begin:end])\n}\n\ntype RandomSeed byte\n\nfunc (s RandomSeed) Runes() []rune {\n\tsourceBuilder := strings.Builder{}\n\tif s&Numeric > 0 {\n\t\tsourceBuilder.WriteString(\"0123456789\")\n\t}\n\tif s&LowerLetter > 0 {\n\t\tsourceBuilder.WriteString(\"abcdefghijklmnopqrstuvwxyz\")\n\t}\n\tif s&UpperLetter > 0 {\n\t\tsourceBuilder.WriteString(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\t}\n\treturn []rune(sourceBuilder.String())\n}\n\nconst (\n\tNumeric RandomSeed = 1 << iota\n\tLowerLetter\n\tUpperLetter\n)\n\nfunc RandomString(length int, seed RandomSeed) string {\n\truns := seed.Runes()\n\tresult := \"\"\n\tfor i := 0; i < length; i++ {\n\t\tlenInt64 := int64(len(runs))\n\t\trandNumber, _ := rand.Int(rand.Reader, big.NewInt(lenInt64))\n\t\tresult += string(runs[randNumber.Uint64()])\n\t}\n\treturn result\n}\n"], "filenames": ["proto/packet.go", "sdk/data/stream/stream_writer.go", "util/string.go"], "buggy_code_start_loc": [598, 398, 18], "buggy_code_end_loc": [598, 399, 65], "fixing_code_start_loc": [599, 398, 18], "fixing_code_end_loc": [601, 400, 65], "type": "CWE-330", "message": "CubeFS is an open-source cloud-native file storage system. Prior to version 3.3.1, CubeFS used an insecure random string generator to generate user-specific, sensitive keys used to authenticate users in a CubeFS deployment. This could allow an attacker to predict and/or guess the generated string and impersonate a user thereby obtaining higher privileges. When CubeFS creates new users, it creates a piece of sensitive information for the user called the \u201caccessKey\u201d. To create the \"accesKey\", CubeFS uses an insecure string generator which makes it easy to guess and thereby impersonate the created user. An attacker could leverage the predictable random string generator and guess a users access key and impersonate the user to obtain higher privileges. The issue has been fixed in v3.3.1. There is no other mitigation than to upgrade.", "other": {"cve": {"id": "CVE-2023-46740", "sourceIdentifier": "security-advisories@github.com", "published": "2024-01-03T17:15:10.590", "lastModified": "2024-01-10T17:45:07.017", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "CubeFS is an open-source cloud-native file storage system. Prior to version 3.3.1, CubeFS used an insecure random string generator to generate user-specific, sensitive keys used to authenticate users in a CubeFS deployment. This could allow an attacker to predict and/or guess the generated string and impersonate a user thereby obtaining higher privileges. When CubeFS creates new users, it creates a piece of sensitive information for the user called the \u201caccessKey\u201d. To create the \"accesKey\", CubeFS uses an insecure string generator which makes it easy to guess and thereby impersonate the created user. An attacker could leverage the predictable random string generator and guess a users access key and impersonate the user to obtain higher privileges. The issue has been fixed in v3.3.1. There is no other mitigation than to upgrade."}, {"lang": "es", "value": "CubeFS es un sistema de almacenamiento de archivos nativo de la nube de c\u00f3digo abierto. Antes de la versi\u00f3n 3.3.1, CubeFS usaba un generador de cadenas aleatorias inseguras para generar claves confidenciales espec\u00edficas del usuario utilizadas para autenticar a los usuarios en una implementaci\u00f3n de CubeFS. Esto podr\u00eda permitir a un atacante predecir y/o adivinar la cadena generada y hacerse pasar por un usuario, obteniendo as\u00ed mayores privilegios. Cuando CubeFS crea nuevos usuarios, crea una informaci\u00f3n confidencial para el usuario llamada \"clave de acceso\". Para crear la \"clave de acceso\", CubeFS utiliza un generador de cadenas inseguro que hace que sea f\u00e1cil de adivinar y, por lo tanto, suplantar al usuario creado. Un atacante podr\u00eda aprovechar el predecible generador de cadenas aleatorias y adivinar la clave de acceso de un usuario y hacerse pasar por el usuario para obtener mayores privilegios. El problema se solucion\u00f3 en v3.3.1. No hay otra mitigaci\u00f3n que actualizar."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:C/C:L/I:L/A:L", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.7}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-330"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:linuxfoundation:cubefs:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.3.1", "matchCriteriaId": "6E8D59D8-6863-4398-9D77-2442BAF81108"}]}]}], "references": [{"url": "https://github.com/cubefs/cubefs/commit/8555c6402794cabdf2cc025c8bea1576122c07ba", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/cubefs/cubefs/security/advisories/GHSA-4248-p65p-hcrm", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/cubefs/cubefs/commit/8555c6402794cabdf2cc025c8bea1576122c07ba"}}