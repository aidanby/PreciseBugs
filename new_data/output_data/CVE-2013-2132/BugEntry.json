{"buggy_code": ["# Copyright 2009-2012 10gen, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"BSON (Binary JSON) encoding and decoding.\n\"\"\"\n\nimport calendar\nimport datetime\nimport re\nimport struct\nimport sys\n\nfrom bson.binary import (Binary, OLD_UUID_SUBTYPE,\n                         JAVA_LEGACY, CSHARP_LEGACY)\nfrom bson.code import Code\nfrom bson.dbref import DBRef\nfrom bson.errors import (InvalidBSON,\n                         InvalidDocument,\n                         InvalidStringData)\nfrom bson.max_key import MaxKey\nfrom bson.min_key import MinKey\nfrom bson.objectid import ObjectId\nfrom bson.py3compat import b, binary_type\nfrom bson.son import SON, RE_TYPE\nfrom bson.timestamp import Timestamp\nfrom bson.tz_util import utc\n\n\ntry:\n    from bson import _cbson\n    _use_c = True\nexcept ImportError:\n    _use_c = False\n\ntry:\n    import uuid\n    _use_uuid = True\nexcept ImportError:\n    _use_uuid = False\n\nPY3 = sys.version_info[0] == 3\n\n\nMAX_INT32 = 2147483647\nMIN_INT32 = -2147483648\nMAX_INT64 = 9223372036854775807\nMIN_INT64 = -9223372036854775808\n\nEPOCH_AWARE = datetime.datetime.fromtimestamp(0, utc)\nEPOCH_NAIVE = datetime.datetime.utcfromtimestamp(0)\n\n# Create constants compatible with all versions of\n# python from 2.4 forward. In 2.x b(\"foo\") is just\n# \"foo\". In 3.x it becomes b\"foo\".\nEMPTY = b(\"\")\nZERO  = b(\"\\x00\")\nONE   = b(\"\\x01\")\n\nBSONNUM = b(\"\\x01\") # Floating point\nBSONSTR = b(\"\\x02\") # UTF-8 string\nBSONOBJ = b(\"\\x03\") # Embedded document\nBSONARR = b(\"\\x04\") # Array\nBSONBIN = b(\"\\x05\") # Binary\nBSONUND = b(\"\\x06\") # Undefined\nBSONOID = b(\"\\x07\") # ObjectId\nBSONBOO = b(\"\\x08\") # Boolean\nBSONDAT = b(\"\\x09\") # UTC Datetime\nBSONNUL = b(\"\\x0A\") # Null\nBSONRGX = b(\"\\x0B\") # Regex\nBSONREF = b(\"\\x0C\") # DBRef\nBSONCOD = b(\"\\x0D\") # Javascript code\nBSONSYM = b(\"\\x0E\") # Symbol\nBSONCWS = b(\"\\x0F\") # Javascript code with scope\nBSONINT = b(\"\\x10\") # 32bit int\nBSONTIM = b(\"\\x11\") # Timestamp\nBSONLON = b(\"\\x12\") # 64bit int\nBSONMIN = b(\"\\xFF\") # Min key\nBSONMAX = b(\"\\x7F\") # Max key\n\n\ndef _get_int(data, position, as_class=None,\n             tz_aware=False, uuid_subtype=OLD_UUID_SUBTYPE, unsigned=False):\n    format = unsigned and \"I\" or \"i\"\n    try:\n        value = struct.unpack(\"<%s\" % format, data[position:position + 4])[0]\n    except struct.error:\n        raise InvalidBSON()\n    position += 4\n    return value, position\n\n\ndef _get_c_string(data, position, length=None):\n    if length is None:\n        try:\n            end = data.index(ZERO, position)\n        except ValueError:\n            raise InvalidBSON()\n    else:\n        end = position + length\n    value = data[position:end].decode(\"utf-8\")\n    position = end + 1\n\n    return value, position\n\n\ndef _make_c_string(string, check_null=False):\n    if isinstance(string, unicode):\n        if check_null and \"\\x00\" in string:\n            raise InvalidDocument(\"BSON keys / regex patterns must not \"\n                                  \"contain a NULL character\")\n        return string.encode(\"utf-8\") + ZERO\n    else:\n        if check_null and ZERO in string:\n            raise InvalidDocument(\"BSON keys / regex patterns must not \"\n                                  \"contain a NULL character\")\n        try:\n            string.decode(\"utf-8\")\n            return string + ZERO\n        except UnicodeError:\n            raise InvalidStringData(\"strings in documents must be valid \"\n                                    \"UTF-8: %r\" % string)\n\n\ndef _get_number(data, position, as_class, tz_aware, uuid_subtype):\n    num = struct.unpack(\"<d\", data[position:position + 8])[0]\n    position += 8\n    return num, position\n\n\ndef _get_string(data, position, as_class, tz_aware, uuid_subtype):\n    length = struct.unpack(\"<i\", data[position:position + 4])[0] - 1\n    position += 4\n    return _get_c_string(data, position, length)\n\n\ndef _get_object(data, position, as_class, tz_aware, uuid_subtype):\n    obj_size = struct.unpack(\"<i\", data[position:position + 4])[0]\n    encoded = data[position + 4:position + obj_size - 1]\n    object = _elements_to_dict(encoded, as_class, tz_aware, uuid_subtype)\n    position += obj_size\n    if \"$ref\" in object:\n        return (DBRef(object.pop(\"$ref\"), object.pop(\"$id\"),\n                      object.pop(\"$db\", None), object), position)\n    return object, position\n\n\ndef _get_array(data, position, as_class, tz_aware, uuid_subtype):\n    obj, position = _get_object(data, position,\n                                as_class, tz_aware, uuid_subtype)\n    result = []\n    i = 0\n    while True:\n        try:\n            result.append(obj[str(i)])\n            i += 1\n        except KeyError:\n            break\n    return result, position\n\n\ndef _get_binary(data, position, as_class, tz_aware, uuid_subtype):\n    length, position = _get_int(data, position)\n    subtype = ord(data[position:position + 1])\n    position += 1\n    if subtype == 2:\n        length2, position = _get_int(data, position)\n        if length2 != length - 4:\n            raise InvalidBSON(\"invalid binary (st 2) - lengths don't match!\")\n        length = length2\n    if subtype in (3, 4) and _use_uuid:\n        # Java Legacy\n        if uuid_subtype == JAVA_LEGACY:\n            java = data[position:position + length]\n            value = uuid.UUID(bytes=java[0:8][::-1] + java[8:16][::-1])\n        # C# legacy\n        elif uuid_subtype == CSHARP_LEGACY:\n            value = uuid.UUID(bytes_le=data[position:position + length])\n        # Python\n        else:\n            value = uuid.UUID(bytes=data[position:position + length])\n        position += length\n        return (value, position)\n    # Python3 special case. Decode subtype 0 to 'bytes'.\n    if PY3 and subtype == 0:\n        value = data[position:position + length]\n    else:\n        value = Binary(data[position:position + length], subtype)\n    position += length\n    return value, position\n\n\ndef _get_oid(data, position, as_class=None,\n             tz_aware=False, uuid_subtype=OLD_UUID_SUBTYPE):\n    value = ObjectId(data[position:position + 12])\n    position += 12\n    return value, position\n\n\ndef _get_boolean(data, position, as_class, tz_aware, uuid_subtype):\n    value = data[position:position + 1] == ONE\n    position += 1\n    return value, position\n\n\ndef _get_date(data, position, as_class, tz_aware, uuid_subtype):\n    millis = struct.unpack(\"<q\", data[position:position + 8])[0]\n    diff = millis % 1000\n    seconds = (millis - diff) / 1000\n    position += 8\n    if tz_aware:\n        dt = EPOCH_AWARE + datetime.timedelta(seconds=seconds)\n    else:\n        dt = EPOCH_NAIVE + datetime.timedelta(seconds=seconds)\n    return dt.replace(microsecond=diff * 1000), position\n\n\ndef _get_code(data, position, as_class, tz_aware, uuid_subtype):\n    code, position = _get_string(data, position,\n                                 as_class, tz_aware, uuid_subtype)\n    return Code(code), position\n\n\ndef _get_code_w_scope(data, position, as_class, tz_aware, uuid_subtype):\n    _, position = _get_int(data, position)\n    code, position = _get_string(data, position,\n                                 as_class, tz_aware, uuid_subtype)\n    scope, position = _get_object(data, position,\n                                  as_class, tz_aware, uuid_subtype)\n    return Code(code, scope), position\n\n\ndef _get_null(data, position, as_class, tz_aware, uuid_subtype):\n    return None, position\n\n\ndef _get_regex(data, position, as_class, tz_aware, uuid_subtype):\n    pattern, position = _get_c_string(data, position)\n    bson_flags, position = _get_c_string(data, position)\n    flags = 0\n    if \"i\" in bson_flags:\n        flags |= re.IGNORECASE\n    if \"l\" in bson_flags:\n        flags |= re.LOCALE\n    if \"m\" in bson_flags:\n        flags |= re.MULTILINE\n    if \"s\" in bson_flags:\n        flags |= re.DOTALL\n    if \"u\" in bson_flags:\n        flags |= re.UNICODE\n    if \"x\" in bson_flags:\n        flags |= re.VERBOSE\n    return re.compile(pattern, flags), position\n\n\ndef _get_ref(data, position, as_class, tz_aware, uuid_subtype):\n    position += 4\n    collection, position = _get_c_string(data, position)\n    oid, position = _get_oid(data, position)\n    return DBRef(collection, oid), position\n\n\ndef _get_timestamp(data, position, as_class, tz_aware, uuid_subtype):\n    inc, position = _get_int(data, position, unsigned=True)\n    timestamp, position = _get_int(data, position, unsigned=True)\n    return Timestamp(timestamp, inc), position\n\n\ndef _get_long(data, position, as_class, tz_aware, uuid_subtype):\n    # Have to cast to long; on 32-bit unpack may return an int.\n    # 2to3 will change long to int. That's fine since long doesn't\n    # exist in python3.\n    value = long(struct.unpack(\"<q\", data[position:position + 8])[0])\n    position += 8\n    return value, position\n\n\n_element_getter = {\n    BSONNUM: _get_number,\n    BSONSTR: _get_string,\n    BSONOBJ: _get_object,\n    BSONARR: _get_array,\n    BSONBIN: _get_binary,\n    BSONUND: _get_null,  # undefined\n    BSONOID: _get_oid,\n    BSONBOO: _get_boolean,\n    BSONDAT: _get_date,\n    BSONNUL: _get_null,\n    BSONRGX: _get_regex,\n    BSONREF: _get_ref,\n    BSONCOD: _get_code,  # code\n    BSONSYM: _get_string,  # symbol\n    BSONCWS: _get_code_w_scope,\n    BSONINT: _get_int,  # number_int\n    BSONTIM: _get_timestamp,\n    BSONLON: _get_long, # Same as _get_int after 2to3 runs.\n    BSONMIN: lambda v, w, x, y, z: (MinKey(), w),\n    BSONMAX: lambda v, w, x, y, z: (MaxKey(), w)}\n\n\ndef _element_to_dict(data, position, as_class, tz_aware, uuid_subtype):\n    element_type = data[position:position + 1]\n    position += 1\n    element_name, position = _get_c_string(data, position)\n    value, position = _element_getter[element_type](data, position, as_class,\n                                                    tz_aware, uuid_subtype)\n    return element_name, value, position\n\n\ndef _elements_to_dict(data, as_class, tz_aware, uuid_subtype):\n    result = as_class()\n    position = 0\n    end = len(data) - 1\n    while position < end:\n        (key, value, position) = _element_to_dict(data, position, as_class,\n                                                  tz_aware, uuid_subtype)\n        result[key] = value\n    return result\n\ndef _bson_to_dict(data, as_class, tz_aware, uuid_subtype):\n    obj_size = struct.unpack(\"<i\", data[:4])[0]\n    length = len(data)\n    if length < obj_size:\n        raise InvalidBSON(\"objsize too large\")\n    if obj_size != length or data[obj_size - 1:obj_size] != ZERO:\n        raise InvalidBSON(\"bad eoo\")\n    elements = data[4:obj_size - 1]\n    return (_elements_to_dict(elements, as_class,\n                              tz_aware, uuid_subtype), data[obj_size:])\nif _use_c:\n    _bson_to_dict = _cbson._bson_to_dict\n\n\ndef _element_to_bson(key, value, check_keys, uuid_subtype):\n    if not isinstance(key, basestring):\n        raise InvalidDocument(\"documents must have only string keys, \"\n                              \"key was %r\" % key)\n\n    if check_keys:\n        if key.startswith(\"$\"):\n            raise InvalidDocument(\"key %r must not start with '$'\" % key)\n        if \".\" in key:\n            raise InvalidDocument(\"key %r must not contain '.'\" % key)\n\n    name = _make_c_string(key, True)\n    if isinstance(value, float):\n        return BSONNUM + name + struct.pack(\"<d\", value)\n\n    if _use_uuid:\n        if isinstance(value, uuid.UUID):\n            # Java Legacy\n            if uuid_subtype == JAVA_LEGACY:\n                # Python 3.0(.1) returns a bytearray instance for bytes (3.1\n                # and newer just return a bytes instance). Convert that to\n                # binary_type (here and below) for compatibility.\n                from_uuid = binary_type(value.bytes)\n                as_legacy_java = from_uuid[0:8][::-1] + from_uuid[8:16][::-1]\n                value = Binary(as_legacy_java, subtype=OLD_UUID_SUBTYPE)\n            # C# legacy\n            elif uuid_subtype == CSHARP_LEGACY:\n                # Microsoft GUID representation.\n                value = Binary(binary_type(value.bytes_le),\n                               subtype=OLD_UUID_SUBTYPE)\n            # Python\n            else:\n                value = Binary(binary_type(value.bytes), subtype=uuid_subtype)\n\n    if isinstance(value, Binary):\n        subtype = value.subtype\n        if subtype == 2:\n            value = struct.pack(\"<i\", len(value)) + value\n        return (BSONBIN + name +\n                struct.pack(\"<i\", len(value)) + b(chr(subtype)) + value)\n    if isinstance(value, Code):\n        cstring = _make_c_string(value)\n        if not value.scope:\n            length = struct.pack(\"<i\", len(cstring))\n            return BSONCOD + name + length + cstring\n        scope = _dict_to_bson(value.scope, False, uuid_subtype, False)\n        full_length = struct.pack(\"<i\", 8 + len(cstring) + len(scope))\n        length = struct.pack(\"<i\", len(cstring))\n        return BSONCWS + name + full_length + length + cstring + scope\n    if isinstance(value, binary_type):\n        if PY3:\n            # Python3 special case. Store 'bytes' as BSON binary subtype 0.\n            return (BSONBIN + name +\n                    struct.pack(\"<i\", len(value)) + ZERO + value)\n        cstring = _make_c_string(value)\n        length = struct.pack(\"<i\", len(cstring))\n        return BSONSTR + name + length + cstring\n    if isinstance(value, unicode):\n        cstring = _make_c_string(value)\n        length = struct.pack(\"<i\", len(cstring))\n        return BSONSTR + name + length + cstring\n    if isinstance(value, dict):\n        return BSONOBJ + name + _dict_to_bson(value, check_keys, uuid_subtype, False)\n    if isinstance(value, (list, tuple)):\n        as_dict = SON(zip([str(i) for i in range(len(value))], value))\n        return BSONARR + name + _dict_to_bson(as_dict, check_keys, uuid_subtype, False)\n    if isinstance(value, ObjectId):\n        return BSONOID + name + value.binary\n    if value is True:\n        return BSONBOO + name + ONE\n    if value is False:\n        return BSONBOO + name + ZERO\n    if isinstance(value, int):\n        # TODO this is an ugly way to check for this...\n        if value > MAX_INT64 or value < MIN_INT64:\n            raise OverflowError(\"BSON can only handle up to 8-byte ints\")\n        if value > MAX_INT32 or value < MIN_INT32:\n            return BSONLON + name + struct.pack(\"<q\", value)\n        return BSONINT + name + struct.pack(\"<i\", value)\n    # 2to3 will convert long to int here since there is no long in python3.\n    # That's OK. The previous if block will match instead.\n    if isinstance(value, long):\n        if value > MAX_INT64 or value < MIN_INT64:\n            raise OverflowError(\"BSON can only handle up to 8-byte ints\")\n        return BSONLON + name + struct.pack(\"<q\", value)\n    if isinstance(value, datetime.datetime):\n        if value.utcoffset() is not None:\n            value = value - value.utcoffset()\n        millis = int(calendar.timegm(value.timetuple()) * 1000 +\n                     value.microsecond / 1000)\n        return BSONDAT + name + struct.pack(\"<q\", millis)\n    if isinstance(value, Timestamp):\n        time = struct.pack(\"<I\", value.time)\n        inc = struct.pack(\"<I\", value.inc)\n        return BSONTIM + name + inc + time\n    if value is None:\n        return BSONNUL + name\n    if isinstance(value, RE_TYPE):\n        pattern = value.pattern\n        flags = \"\"\n        if value.flags & re.IGNORECASE:\n            flags += \"i\"\n        if value.flags & re.LOCALE:\n            flags += \"l\"\n        if value.flags & re.MULTILINE:\n            flags += \"m\"\n        if value.flags & re.DOTALL:\n            flags += \"s\"\n        if value.flags & re.UNICODE:\n            flags += \"u\"\n        if value.flags & re.VERBOSE:\n            flags += \"x\"\n        return BSONRGX + name + _make_c_string(pattern, True) + \\\n            _make_c_string(flags)\n    if isinstance(value, DBRef):\n        return _element_to_bson(key, value.as_doc(), False, uuid_subtype)\n    if isinstance(value, MinKey):\n        return BSONMIN + name\n    if isinstance(value, MaxKey):\n        return BSONMAX + name\n\n    raise InvalidDocument(\"cannot convert value of type %s to bson\" %\n                          type(value))\n\n\ndef _dict_to_bson(dict, check_keys, uuid_subtype, top_level=True):\n    try:\n        elements = []\n        if top_level and \"_id\" in dict:\n            elements.append(_element_to_bson(\"_id\", dict[\"_id\"], False, uuid_subtype))\n        for (key, value) in dict.iteritems():\n            if not top_level or key != \"_id\":\n                elements.append(_element_to_bson(key, value, check_keys, uuid_subtype))\n    except AttributeError:\n        raise TypeError(\"encoder expected a mapping type but got: %r\" % dict)\n\n    encoded = EMPTY.join(elements)\n    length = len(encoded) + 5\n    return struct.pack(\"<i\", length) + encoded + ZERO\nif _use_c:\n    _dict_to_bson = _cbson._dict_to_bson\n\n\n\ndef decode_all(data, as_class=dict,\n               tz_aware=True, uuid_subtype=OLD_UUID_SUBTYPE):\n    \"\"\"Decode BSON data to multiple documents.\n\n    `data` must be a string of concatenated, valid, BSON-encoded\n    documents.\n\n    :Parameters:\n      - `data`: BSON data\n      - `as_class` (optional): the class to use for the resulting\n        documents\n      - `tz_aware` (optional): if ``True``, return timezone-aware\n        :class:`~datetime.datetime` instances\n\n    .. versionadded:: 1.9\n    \"\"\"\n    docs = []\n    position = 0\n    end = len(data) - 1\n    while position < end:\n        obj_size = struct.unpack(\"<i\", data[position:position + 4])[0]\n        if len(data) - position < obj_size:\n            raise InvalidBSON(\"objsize too large\")\n        if data[position + obj_size - 1:position + obj_size] != ZERO:\n            raise InvalidBSON(\"bad eoo\")\n        elements = data[position + 4:position + obj_size - 1]\n        position += obj_size\n        docs.append(_elements_to_dict(elements, as_class,\n                                      tz_aware, uuid_subtype))\n    return docs\nif _use_c:\n    decode_all = _cbson.decode_all\n\n\ndef is_valid(bson):\n    \"\"\"Check that the given string represents valid :class:`BSON` data.\n\n    Raises :class:`TypeError` if `bson` is not an instance of\n    :class:`str` (:class:`bytes` in python 3). Returns ``True``\n    if `bson` is valid :class:`BSON`, ``False`` otherwise.\n\n    :Parameters:\n      - `bson`: the data to be validated\n    \"\"\"\n    if not isinstance(bson, binary_type):\n        raise TypeError(\"BSON data must be an instance \"\n                        \"of a subclass of %s\" % (binary_type.__name__,))\n\n    try:\n        (_, remainder) = _bson_to_dict(bson, dict, True, OLD_UUID_SUBTYPE)\n        return remainder == EMPTY\n    except:\n        return False\n\n\nclass BSON(binary_type):\n    \"\"\"BSON (Binary JSON) data.\n    \"\"\"\n\n    @classmethod\n    def encode(cls, document, check_keys=False, uuid_subtype=OLD_UUID_SUBTYPE):\n        \"\"\"Encode a document to a new :class:`BSON` instance.\n\n        A document can be any mapping type (like :class:`dict`).\n\n        Raises :class:`TypeError` if `document` is not a mapping type,\n        or contains keys that are not instances of\n        :class:`basestring` (:class:`str` in python 3). Raises\n        :class:`~bson.errors.InvalidDocument` if `document` cannot be\n        converted to :class:`BSON`.\n\n        :Parameters:\n          - `document`: mapping type representing a document\n          - `check_keys` (optional): check if keys start with '$' or\n            contain '.', raising :class:`~bson.errors.InvalidDocument` in\n            either case\n\n        .. versionadded:: 1.9\n        \"\"\"\n        return cls(_dict_to_bson(document, check_keys, uuid_subtype))\n\n    def decode(self, as_class=dict,\n               tz_aware=False, uuid_subtype=OLD_UUID_SUBTYPE):\n        \"\"\"Decode this BSON data.\n\n        The default type to use for the resultant document is\n        :class:`dict`. Any other class that supports\n        :meth:`__setitem__` can be used instead by passing it as the\n        `as_class` parameter.\n\n        If `tz_aware` is ``True`` (recommended), any\n        :class:`~datetime.datetime` instances returned will be\n        timezone-aware, with their timezone set to\n        :attr:`bson.tz_util.utc`. Otherwise (default), all\n        :class:`~datetime.datetime` instances will be naive (but\n        contain UTC).\n\n        :Parameters:\n          - `as_class` (optional): the class to use for the resulting\n            document\n          - `tz_aware` (optional): if ``True``, return timezone-aware\n            :class:`~datetime.datetime` instances\n\n        .. versionadded:: 1.9\n        \"\"\"\n        (document, _) = _bson_to_dict(self, as_class, tz_aware, uuid_subtype)\n        return document\n\n\ndef has_c():\n    \"\"\"Is the C extension installed?\n\n    .. versionadded:: 1.9\n    \"\"\"\n    return _use_c\n\n\ndef has_uuid():\n    \"\"\"Is the uuid module available?\n\n    .. versionadded:: 2.3\n    \"\"\"\n    return _use_uuid\n", "/*\n * Copyright 2009-2012 10gen, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n/*\n * This file contains C implementations of some of the functions\n * needed by the bson module. If possible, these implementations\n * should be used to speed up BSON encoding and decoding.\n */\n\n#include \"Python.h\"\n#include \"datetime.h\"\n\n#include \"buffer.h\"\n#include \"time64.h\"\n#include \"encoding_helpers.h\"\n\n#define _CBSON_MODULE\n#include \"_cbsonmodule.h\"\n\n/* New module state and initialization code.\n * See the module-initialization-and-state\n * section in the following doc:\n * http://docs.python.org/release/3.1.3/howto/cporting.html\n * which references the following pep:\n * http://www.python.org/dev/peps/pep-3121/\n * */\nstruct module_state {\n    PyObject* Binary;\n    PyObject* Code;\n    PyObject* ObjectId;\n    PyObject* DBRef;\n    PyObject* RECompile;\n    PyObject* UUID;\n    PyObject* Timestamp;\n    PyObject* MinKey;\n    PyObject* MaxKey;\n    PyObject* UTC;\n    PyTypeObject* REType;\n};\n\n#if PY_MAJOR_VERSION >= 3\n#define GETSTATE(m) ((struct module_state*)PyModule_GetState(m))\n#else\n#define GETSTATE(m) (&_state)\nstatic struct module_state _state;\n#endif\n\n#if PY_VERSION_HEX < 0x02050000\n#define WARN(category, message)                 \\\n    PyErr_Warn((category), (message))\n#else\n#define WARN(category, message)                 \\\n    PyErr_WarnEx((category), (message), 1)\n#endif\n\n/* Maximum number of regex flags */\n#define FLAGS_SIZE 7\n\n#if defined(WIN32) || defined(_MSC_VER)\n/* This macro is basically an implementation of asprintf for win32\n * We get the length of the int as string and malloc a buffer for it,\n * returning -1 if that malloc fails. We then actually print to the\n * buffer to get the string value as an int. Like asprintf, the result\n * must be explicitly free'd when done being used.\n */\n#if defined(_MSC_VER) && (_MSC_VER >= 1400)\n#define INT2STRING(buffer, i)                                           \\\n    *(buffer) = malloc(_scprintf(\"%d\", (i)) + 1),                       \\\n        (!(buffer) ?                                                    \\\n         -1 :                                                           \\\n         _snprintf_s(*(buffer),                                         \\\n                     _scprintf(\"%d\", (i)) + 1,                          \\\n                     _scprintf(\"%d\", (i)) + 1,                          \\\n                     \"%d\",                                              \\\n                     (i)))\n#define STRCAT(dest, n, src) strcat_s((dest), (n), (src))\n#else\n#define INT2STRING(buffer, i)                                           \\\n    *(buffer) = malloc(_scprintf(\"%d\", (i)) + 1),                       \\\n        (!(buffer) ?                                                    \\\n         -1 :                                                           \\\n         _snprintf(*(buffer),                                           \\\n                     _scprintf(\"%d\", (i)) + 1,                          \\\n                     \"%d\",                                              \\\n                     (i)))\n#define STRCAT(dest, n, src) strcat((dest), (src))\n#endif\n#else\n#define INT2STRING(buffer, i) asprintf((buffer), \"%d\", (i))\n#define STRCAT(dest, n, src) strcat((dest), (src))\n#endif\n\n#define JAVA_LEGACY   5\n#define CSHARP_LEGACY 6\n\n\nstatic PyObject* elements_to_dict(PyObject* self, const char* string, int max,\n                                  PyObject* as_class, unsigned char tz_aware,\n                                  unsigned char uuid_subtype);\n\nstatic int _write_element_to_buffer(PyObject* self, buffer_t buffer, int type_byte,\n                                    PyObject* value, unsigned char check_keys,\n                                    unsigned char uuid_subtype, unsigned char first_attempt);\n\n/* Date stuff */\nstatic PyObject* datetime_from_millis(long long millis) {\n    /* To encode a datetime instance like datetime(9999, 12, 31, 23, 59, 59, 999999)\n     * we follow these steps:\n     * 1. Calculate a timestamp in seconds:       253402300799\n     * 2. Multiply that by 1000:                  253402300799000\n     * 3. Add in microseconds divided by 1000     253402300799999\n     *\n     * (Note: BSON doesn't support microsecond accuracy, hence the rounding.)\n     *\n     * To decode we could do:\n     * 1. Get seconds: timestamp / 1000:          253402300799\n     * 2. Get micros: (timestamp % 1000) * 1000:  999000\n     * Resulting in datetime(9999, 12, 31, 23, 59, 59, 999000) -- the expected result\n     *\n     * Now what if the we encode (1, 1, 1, 1, 1, 1, 111111)?\n     * 1. and 2. gives:                           -62135593139000\n     * 3. Gives us:                               -62135593138889\n     *\n     * Now decode:\n     * 1. Gives us:                               -62135593138\n     * 2. Gives us:                               -889000\n     * Resulting in datetime(1, 1, 1, 1, 1, 2, 15888216) -- an invalid result\n     *\n     * If instead to decode we do:\n     * diff = ((millis % 1000) + 1000) % 1000:    111\n     * seconds = (millis - diff) / 1000:          -62135593139\n     * micros = diff * 1000                       111000\n     * Resulting in datetime(1, 1, 1, 1, 1, 1, 111000) -- the expected result\n     */\n    int diff = (int)(((millis % 1000) + 1000) % 1000);\n    int microseconds = diff * 1000;\n    Time64_T seconds = (millis - diff) / 1000;\n    struct TM timeinfo;\n    gmtime64_r(&seconds, &timeinfo);\n\n    return PyDateTime_FromDateAndTime(timeinfo.tm_year + 1900,\n                                      timeinfo.tm_mon + 1,\n                                      timeinfo.tm_mday,\n                                      timeinfo.tm_hour,\n                                      timeinfo.tm_min,\n                                      timeinfo.tm_sec,\n                                      microseconds);\n}\n\nstatic long long millis_from_datetime(PyObject* datetime) {\n    struct TM timeinfo;\n    long long millis;\n\n    timeinfo.tm_year = PyDateTime_GET_YEAR(datetime) - 1900;\n    timeinfo.tm_mon = PyDateTime_GET_MONTH(datetime) - 1;\n    timeinfo.tm_mday = PyDateTime_GET_DAY(datetime);\n    timeinfo.tm_hour = PyDateTime_DATE_GET_HOUR(datetime);\n    timeinfo.tm_min = PyDateTime_DATE_GET_MINUTE(datetime);\n    timeinfo.tm_sec = PyDateTime_DATE_GET_SECOND(datetime);\n\n    millis = timegm64(&timeinfo) * 1000;\n    millis += PyDateTime_DATE_GET_MICROSECOND(datetime) / 1000;\n    return millis;\n}\n\n/* Just make this compatible w/ the old API. */\nint buffer_write_bytes(buffer_t buffer, const char* data, int size) {\n    if (buffer_write(buffer, data, size)) {\n        PyErr_NoMemory();\n        return 0;\n    }\n    return 1;\n}\n\n#if PY_MAJOR_VERSION >= 3\nstatic int write_unicode(buffer_t buffer, PyObject* py_string) {\n    Py_ssize_t string_length;\n    const char* string;\n    PyObject* encoded = PyUnicode_AsUTF8String(py_string);\n    if (!encoded) {\n        return 0;\n    }\n    string = PyBytes_AsString(encoded);\n    if (!string) {\n        Py_DECREF(encoded);\n        return 0;\n    }\n    string_length = PyBytes_Size(encoded) + 1;\n    if (!buffer_write_bytes(buffer, (const char*)&string_length, 4)) {\n        Py_DECREF(encoded);\n        return 0;\n    }\n    if (!buffer_write_bytes(buffer, string, string_length)) {\n        Py_DECREF(encoded);\n        return 0;\n    }\n    Py_DECREF(encoded);\n    return 1;\n}\n#endif\n\n/* returns 0 on failure */\nstatic int write_string(buffer_t buffer, PyObject* py_string) {\n    Py_ssize_t string_length;\n    const char* string;\n#if PY_MAJOR_VERSION >= 3\n    if (PyUnicode_Check(py_string)){\n        return write_unicode(buffer, py_string);\n    }\n    string = PyBytes_AsString(py_string);\n#else\n    string = PyString_AsString(py_string);\n#endif\n    if (!string) {\n        return 0;\n    }\n\n#if PY_MAJOR_VERSION >= 3\n    string_length = PyBytes_Size(py_string) + 1;\n#else\n    string_length = PyString_Size(py_string) + 1;\n#endif\n\n    if (!buffer_write_bytes(buffer, (const char*)&string_length, 4)) {\n        return 0;\n    }\n    if (!buffer_write_bytes(buffer, string, string_length)) {\n        return 0;\n    }\n    return 1;\n}\n\n/* Get an error class from the bson.errors module.\n *\n * Returns a new ref */\nstatic PyObject* _error(char* name) {\n    PyObject* error;\n    PyObject* errors = PyImport_ImportModule(\"bson.errors\");\n    if (!errors) {\n        return NULL;\n    }\n    error = PyObject_GetAttrString(errors, name);\n    Py_DECREF(errors);\n    return error;\n}\n\n/* Reload a cached Python object.\n *\n * Returns non-zero on failure. */\nstatic int _reload_object(PyObject** object, char* module_name, char* object_name) {\n    PyObject* module;\n\n    module = PyImport_ImportModule(module_name);\n    if (!module) {\n        return 1;\n    }\n\n    *object = PyObject_GetAttrString(module, object_name);\n    Py_DECREF(module);\n\n    return (*object) ? 0 : 2;\n}\n\n/* Reload all cached Python objects.\n *\n * Returns non-zero on failure. */\nstatic int _reload_python_objects(PyObject* module) {\n    struct module_state *state = GETSTATE(module);\n\n    if (_reload_object(&state->Binary, \"bson.binary\", \"Binary\") ||\n        _reload_object(&state->Code, \"bson.code\", \"Code\") ||\n        _reload_object(&state->ObjectId, \"bson.objectid\", \"ObjectId\") ||\n        _reload_object(&state->DBRef, \"bson.dbref\", \"DBRef\") ||\n        _reload_object(&state->Timestamp, \"bson.timestamp\", \"Timestamp\") ||\n        _reload_object(&state->MinKey, \"bson.min_key\", \"MinKey\") ||\n        _reload_object(&state->MaxKey, \"bson.max_key\", \"MaxKey\") ||\n        _reload_object(&state->UTC, \"bson.tz_util\", \"utc\") ||\n        _reload_object(&state->RECompile, \"re\", \"compile\")) {\n        return 1;\n    }\n    /* If we couldn't import uuid then we must be on 2.4. Just ignore. */\n    if (_reload_object(&state->UUID, \"uuid\", \"UUID\") == 1) {\n        state->UUID = NULL;\n        PyErr_Clear();\n    }\n    /* Reload our REType hack too. */\n    state->REType = PyObject_CallFunction(state->RECompile, \"O\",\n#if PY_MAJOR_VERSION >= 3\n                                   PyBytes_FromString(\"\"))->ob_type;\n#else\n                                   PyString_FromString(\"\"))->ob_type;\n#endif\n    return 0;\n}\n\nstatic int write_element_to_buffer(PyObject* self, buffer_t buffer, int type_byte,\n                                   PyObject* value, unsigned char check_keys,\n                                   unsigned char uuid_subtype,\n                                   unsigned char first_attempt) {\n    int result;\n    if(Py_EnterRecursiveCall(\" while encoding an object to BSON \"))\n        return 0;\n    result = _write_element_to_buffer(self, buffer, type_byte, value,\n                                      check_keys, uuid_subtype, first_attempt);\n    Py_LeaveRecursiveCall();\n    return result;\n}\n\nstatic int _fix_java(const char* in, char* out) {\n    int i, j;\n    for (i = 0, j = 7; i < j; i++, j--) {\n        out[i] = in[j];\n        out[j] = in[i];\n    }\n    for (i = 8, j = 15; i < j; i++, j--) {\n        out[i] = in[j];\n        out[j] = in[i];\n    }\n    return 0;\n}\n\n/* TODO our platform better be little-endian w/ 4-byte ints! */\n/* Write a single value to the buffer (also write it's type_byte, for which\n * space has already been reserved.\n *\n * returns 0 on failure */\nstatic int _write_element_to_buffer(PyObject* self, buffer_t buffer, int type_byte,\n                                    PyObject* value, unsigned char check_keys,\n                                    unsigned char uuid_subtype, unsigned char first_attempt) {\n    struct module_state *state = GETSTATE(self);\n\n    if (PyBool_Check(value)) {\n#if PY_MAJOR_VERSION >= 3\n        const long bool = PyLong_AsLong(value);\n#else\n        const long bool = PyInt_AsLong(value);\n#endif\n        const char c = bool ? 0x01 : 0x00;\n        *(buffer_get_buffer(buffer) + type_byte) = 0x08;\n        return buffer_write_bytes(buffer, &c, 1);\n    }\n#if PY_MAJOR_VERSION >= 3\n    else if (PyLong_Check(value)) {\n        const long long_value = PyLong_AsLong(value);\n#else\n    else if (PyInt_Check(value)) {\n        const long long_value = PyInt_AsLong(value);\n#endif\n\n        const int int_value = (int)long_value;\n        if (PyErr_Occurred() || long_value != int_value) { /* Overflow */\n            long long long_long_value;\n            PyErr_Clear();\n            long_long_value = PyLong_AsLongLong(value);\n            if (PyErr_Occurred()) { /* Overflow AGAIN */\n                PyErr_SetString(PyExc_OverflowError,\n                                \"MongoDB can only handle up to 8-byte ints\");\n                return 0;\n            }\n            *(buffer_get_buffer(buffer) + type_byte) = 0x12;\n            return buffer_write_bytes(buffer, (const char*)&long_long_value, 8);\n        }\n        *(buffer_get_buffer(buffer) + type_byte) = 0x10;\n        return buffer_write_bytes(buffer, (const char*)&int_value, 4);\n#if PY_MAJOR_VERSION < 3\n    } else if (PyLong_Check(value)) {\n        const long long long_long_value = PyLong_AsLongLong(value);\n        if (PyErr_Occurred()) { /* Overflow */\n            PyErr_SetString(PyExc_OverflowError,\n                            \"MongoDB can only handle up to 8-byte ints\");\n            return 0;\n        }\n        *(buffer_get_buffer(buffer) + type_byte) = 0x12;\n        return buffer_write_bytes(buffer, (const char*)&long_long_value, 8);\n#endif\n    } else if (PyFloat_Check(value)) {\n        const double d = PyFloat_AsDouble(value);\n        *(buffer_get_buffer(buffer) + type_byte) = 0x01;\n        return buffer_write_bytes(buffer, (const char*)&d, 8);\n    } else if (value == Py_None) {\n        *(buffer_get_buffer(buffer) + type_byte) = 0x0A;\n        return 1;\n    } else if (PyDict_Check(value)) {\n        *(buffer_get_buffer(buffer) + type_byte) = 0x03;\n        return write_dict(self, buffer, value, check_keys, uuid_subtype, 0);\n    } else if (PyList_Check(value) || PyTuple_Check(value)) {\n        int start_position,\n            length_location,\n            items,\n            length,\n            i;\n        char zero = 0;\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x04;\n        start_position = buffer_get_position(buffer);\n\n        /* save space for length */\n        length_location = buffer_save_space(buffer, 4);\n        if (length_location == -1) {\n            PyErr_NoMemory();\n            return 0;\n        }\n\n        items = PySequence_Size(value);\n        for(i = 0; i < items; i++) {\n            int list_type_byte = buffer_save_space(buffer, 1);\n            char* name;\n            PyObject* item_value;\n\n            if (list_type_byte == -1) {\n                PyErr_NoMemory();\n                return 0;\n            }\n            if (INT2STRING(&name, i) < 0 || !name) {\n                PyErr_NoMemory();\n                return 0;\n            }\n            if (!buffer_write_bytes(buffer, name, strlen(name) + 1)) {\n                free(name);\n                return 0;\n            }\n            free(name);\n\n            item_value = PySequence_GetItem(value, i);\n            if (!write_element_to_buffer(self, buffer, list_type_byte,\n                                         item_value, check_keys, uuid_subtype, 1)) {\n                Py_DECREF(item_value);\n                return 0;\n            }\n            Py_DECREF(item_value);\n        }\n\n        /* write null byte and fill in length */\n        if (!buffer_write_bytes(buffer, &zero, 1)) {\n            return 0;\n        }\n        length = buffer_get_position(buffer) - start_position;\n        memcpy(buffer_get_buffer(buffer) + length_location, &length, 4);\n        return 1;\n    } else if (PyObject_IsInstance(value, state->Binary)) {\n        PyObject* subtype_object;\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x05;\n        subtype_object = PyObject_GetAttrString(value, \"subtype\");\n        if (!subtype_object) {\n            return 0;\n        }\n        {\n#if PY_MAJOR_VERSION >= 3\n            const long long_subtype = PyLong_AsLong(subtype_object);\n            const char subtype = (const char)long_subtype;\n            const int length = PyBytes_Size(value);\n#else\n            const long long_subtype = PyInt_AsLong(subtype_object);\n            const char subtype = (const char)long_subtype;\n            const int length = PyString_Size(value);\n#endif\n\n            Py_DECREF(subtype_object);\n            if (subtype == 2) {\n                const int other_length = length + 4;\n                if (!buffer_write_bytes(buffer, (const char*)&other_length, 4)) {\n                    return 0;\n                }\n                if (!buffer_write_bytes(buffer, &subtype, 1)) {\n                    return 0;\n                }\n            }\n            if (!buffer_write_bytes(buffer, (const char*)&length, 4)) {\n                return 0;\n            }\n            if (subtype != 2) {\n                if (!buffer_write_bytes(buffer, &subtype, 1)) {\n                    return 0;\n                }\n            }\n            {\n#if PY_MAJOR_VERSION >= 3\n                const char* string = PyBytes_AsString(value);\n#else\n                const char* string = PyString_AsString(value);\n#endif\n                if (!string) {\n                    return 0;\n                }\n                if (!buffer_write_bytes(buffer, string, length)) {\n                    return 0;\n                }\n            }\n        }\n        return 1;\n    } else if (state->UUID && PyObject_IsInstance(value, state->UUID)) {\n        // Just a special case of Binary above, but simpler to do as a separate case\n\n        PyObject* bytes;\n\n        // Could be bytes, bytearray, str...\n        const char* binarr;\n        // UUID is always 16 bytes\n        int length = 16;\n        char subtype;\n        if (uuid_subtype == JAVA_LEGACY || uuid_subtype == CSHARP_LEGACY) {\n            subtype = 3;\n        }\n        else {\n            subtype = (char)uuid_subtype;\n        }\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x05;\n        if (!buffer_write_bytes(buffer, (const char*)&length, 4)) {\n            return 0;\n        }\n        if (!buffer_write_bytes(buffer, &subtype, 1)) {\n            return 0;\n        }\n\n        if (uuid_subtype == CSHARP_LEGACY) {\n           /* Legacy C# byte order */\n            bytes = PyObject_GetAttrString(value, \"bytes_le\");\n        }\n        else {\n            bytes = PyObject_GetAttrString(value, \"bytes\");\n        }\n        if (!bytes) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        /* Work around http://bugs.python.org/issue7380 */\n        if (PyByteArray_Check(bytes)) {\n            binarr = PyByteArray_AsString(bytes);\n        }\n        else {\n            binarr = PyBytes_AsString(bytes);\n        }\n#else\n        binarr = PyString_AsString(bytes);\n#endif\n        if (uuid_subtype == JAVA_LEGACY) {\n            /* Store in legacy java byte order. */\n            char as_legacy_java[16];\n            _fix_java(binarr, as_legacy_java);\n            if (!buffer_write_bytes(buffer, as_legacy_java, length)) {\n                Py_DECREF(bytes);\n                return 0;\n            }\n        }\n        else {\n            if (!buffer_write_bytes(buffer, binarr, length)) {\n                Py_DECREF(bytes);\n                return 0;\n            }\n        }\n        Py_DECREF(bytes);\n        return 1;\n    } else if (PyObject_IsInstance(value, state->Code)) {\n        int start_position,\n            length_location,\n            length;\n\n        PyObject* scope = PyObject_GetAttrString(value, \"scope\");\n        if (!scope) {\n            return 0;\n        }\n\n        if (!PyDict_Size(scope)) {\n            Py_DECREF(scope);\n            *(buffer_get_buffer(buffer) + type_byte) = 0x0D;\n            return write_string(buffer, value);\n        }\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x0F;\n\n        start_position = buffer_get_position(buffer);\n        /* save space for length */\n        length_location = buffer_save_space(buffer, 4);\n        if (length_location == -1) {\n            PyErr_NoMemory();\n            Py_DECREF(scope);\n            return 0;\n        }\n\n        if (!write_string(buffer, value)) {\n            Py_DECREF(scope);\n            return 0;\n        }\n\n        if (!write_dict(self, buffer, scope, 0, uuid_subtype, 0)) {\n            Py_DECREF(scope);\n            return 0;\n        }\n        Py_DECREF(scope);\n\n        length = buffer_get_position(buffer) - start_position;\n        memcpy(buffer_get_buffer(buffer) + length_location, &length, 4);\n        return 1;\n#if PY_MAJOR_VERSION >= 3\n    /* Python3 special case. Store bytes as BSON binary subtype 0. */\n    } else if (PyBytes_Check(value)) {\n        Py_ssize_t length = PyBytes_Size(value);\n        const char subtype = 0;\n        *(buffer_get_buffer(buffer) + type_byte) = 0x05;\n        if (!buffer_write_bytes(buffer, (const char*)&length, 4)) {\n            return 0;\n        }\n        if (!buffer_write_bytes(buffer, &subtype, 1)) {\n            return 0;\n        }\n        if (!buffer_write_bytes(buffer, PyBytes_AsString(value), length)) {\n            return 0;\n        }\n        return 1;\n#else\n    /* PyString_Check only works in Python 2.x. */\n    } else if (PyString_Check(value)) {\n        int result;\n        result_t status;\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x02;\n        status = check_string((const unsigned char*)PyString_AsString(value),\n                              PyString_Size(value), 1, 0);\n\n        if (status == NOT_UTF_8) {\n            PyObject* InvalidStringData = _error(\"InvalidStringData\");\n            PyErr_SetString(InvalidStringData,\n                            \"strings in documents must be valid UTF-8\");\n            Py_DECREF(InvalidStringData);\n            return 0;\n        }\n        result = write_string(buffer, value);\n        return result;\n#endif\n    } else if (PyUnicode_Check(value)) {\n        PyObject* encoded;\n        int result;\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x02;\n        encoded = PyUnicode_AsUTF8String(value);\n        if (!encoded) {\n            return 0;\n        }\n        result = write_string(buffer, encoded);\n        Py_DECREF(encoded);\n        return result;\n    } else if (PyDateTime_Check(value)) {\n        long long millis;\n        PyObject* utcoffset = PyObject_CallMethod(value, \"utcoffset\", NULL);\n        if (utcoffset != Py_None) {\n            PyObject* result = PyNumber_Subtract(value, utcoffset);\n            Py_DECREF(utcoffset);\n            if (!result) {\n                return 0;\n            }\n            millis = millis_from_datetime(result);\n            Py_DECREF(result);\n        } else {\n            millis = millis_from_datetime(value);\n        }\n        *(buffer_get_buffer(buffer) + type_byte) = 0x09;\n        return buffer_write_bytes(buffer, (const char*)&millis, 8);\n    } else if (PyObject_IsInstance(value, state->ObjectId)) {\n        PyObject* pystring = PyObject_GetAttrString(value, \"_ObjectId__id\");\n        if (!pystring) {\n            return 0;\n        }\n        {\n#if PY_MAJOR_VERSION >= 3\n            const char* as_string = PyBytes_AsString(pystring);\n#else\n            const char* as_string = PyString_AsString(pystring);\n#endif\n            if (!as_string) {\n                Py_DECREF(pystring);\n                return 0;\n            }\n            if (!buffer_write_bytes(buffer, as_string, 12)) {\n                Py_DECREF(pystring);\n                return 0;\n            }\n            Py_DECREF(pystring);\n            *(buffer_get_buffer(buffer) + type_byte) = 0x07;\n        }\n        return 1;\n    } else if (PyObject_IsInstance(value, state->DBRef)) {\n        PyObject* as_doc = PyObject_CallMethod(value, \"as_doc\", NULL);\n        if (!as_doc) {\n            return 0;\n        }\n        if (!write_dict(self, buffer, as_doc, 0, uuid_subtype, 0)) {\n            Py_DECREF(as_doc);\n            return 0;\n        }\n        Py_DECREF(as_doc);\n        *(buffer_get_buffer(buffer) + type_byte) = 0x03;\n        return 1;\n    } else if (PyObject_IsInstance(value, state->Timestamp)) {\n        PyObject* obj;\n        long i;\n\n        obj = PyObject_GetAttrString(value, \"inc\");\n        if (!obj) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        i = PyLong_AsLong(obj);\n#else\n        i = PyInt_AsLong(obj);\n#endif\n        Py_DECREF(obj);\n        if (!buffer_write_bytes(buffer, (const char*)&i, 4)) {\n            return 0;\n        }\n\n        obj = PyObject_GetAttrString(value, \"time\");\n        if (!obj) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        i = PyLong_AsLong(obj);\n#else\n        i = PyInt_AsLong(obj);\n#endif\n        Py_DECREF(obj);\n        if (!buffer_write_bytes(buffer, (const char*)&i, 4)) {\n            return 0;\n        }\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x11;\n        return 1;\n    }\n    else if (PyObject_TypeCheck(value, state->REType)) {\n        PyObject* py_flags = PyObject_GetAttrString(value, \"flags\");\n        PyObject* py_pattern;\n        PyObject* encoded_pattern;\n        long int_flags;\n        char flags[FLAGS_SIZE];\n        char check_utf8 = 0;\n        int pattern_length,\n            flags_length;\n        result_t status;\n\n        if (!py_flags) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        int_flags = PyLong_AsLong(py_flags);\n#else\n        int_flags = PyInt_AsLong(py_flags);\n#endif\n        Py_DECREF(py_flags);\n        py_pattern = PyObject_GetAttrString(value, \"pattern\");\n        if (!py_pattern) {\n            return 0;\n        }\n\n        if (PyUnicode_Check(py_pattern)) {\n            encoded_pattern = PyUnicode_AsUTF8String(py_pattern);\n            Py_DECREF(py_pattern);\n            if (!encoded_pattern) {\n                return 0;\n            }\n        } else {\n            encoded_pattern = py_pattern;\n            check_utf8 = 1;\n        }\n\n#if PY_MAJOR_VERSION >= 3\n        status = check_string((const unsigned char*)PyBytes_AsString(encoded_pattern),\n                              PyBytes_Size(encoded_pattern), check_utf8, 1);\n#else\n        status = check_string((const unsigned char*)PyString_AsString(encoded_pattern),\n                              PyString_Size(encoded_pattern), check_utf8, 1);\n#endif\n        if (status == NOT_UTF_8) {\n            PyObject* InvalidStringData = _error(\"InvalidStringData\");\n            PyErr_SetString(InvalidStringData,\n                            \"regex patterns must be valid UTF-8\");\n            Py_DECREF(InvalidStringData);\n            Py_DECREF(encoded_pattern);\n            return 0;\n        } else if (status == HAS_NULL) {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n            PyErr_SetString(InvalidDocument,\n                            \"regex patterns must not contain the NULL byte\");\n            Py_DECREF(InvalidDocument);\n            Py_DECREF(encoded_pattern);\n            return 0;\n        }\n\n        {\n#if PY_MAJOR_VERSION >= 3\n            const char* pattern =  PyBytes_AsString(encoded_pattern);\n#else\n            const char* pattern =  PyString_AsString(encoded_pattern);\n#endif\n            pattern_length = strlen(pattern) + 1;\n\n            if (!buffer_write_bytes(buffer, pattern, pattern_length)) {\n                Py_DECREF(encoded_pattern);\n                return 0;\n            }\n        }\n        Py_DECREF(encoded_pattern);\n\n        flags[0] = 0;\n        /* TODO don't hardcode these */\n        if (int_flags & 2) {\n            STRCAT(flags, FLAGS_SIZE, \"i\");\n        }\n        if (int_flags & 4) {\n            STRCAT(flags, FLAGS_SIZE, \"l\");\n        }\n        if (int_flags & 8) {\n            STRCAT(flags, FLAGS_SIZE, \"m\");\n        }\n        if (int_flags & 16) {\n            STRCAT(flags, FLAGS_SIZE, \"s\");\n        }\n        if (int_flags & 32) {\n            STRCAT(flags, FLAGS_SIZE, \"u\");\n        }\n        if (int_flags & 64) {\n            STRCAT(flags, FLAGS_SIZE, \"x\");\n        }\n        flags_length = strlen(flags) + 1;\n        if (!buffer_write_bytes(buffer, flags, flags_length)) {\n            return 0;\n        }\n        *(buffer_get_buffer(buffer) + type_byte) = 0x0B;\n        return 1;\n    } else if (PyObject_IsInstance(value, state->MinKey)) {\n        *(buffer_get_buffer(buffer) + type_byte) = 0xFF;\n        return 1;\n    } else if (PyObject_IsInstance(value, state->MaxKey)) {\n        *(buffer_get_buffer(buffer) + type_byte) = 0x7F;\n        return 1;\n    } else if (first_attempt) {\n        /* Try reloading the modules and having one more go at it. */\n        if (WARN(PyExc_RuntimeWarning, \"couldn't encode - reloading python \"\n                 \"modules and trying again. if you see this without getting \"\n                 \"an InvalidDocument exception please see http://api.mongodb\"\n                 \".org/python/current/faq.html#does-pymongo-work-with-mod-\"\n                 \"wsgi\") == -1) {\n            return 0;\n        }\n        if (_reload_python_objects(self)) {\n            return 0;\n        }\n        return write_element_to_buffer(self, buffer, type_byte, value, check_keys, uuid_subtype, 0);\n    }\n    {\n        PyObject* repr = PyObject_Repr(value);\n        PyObject* InvalidDocument = _error(\"InvalidDocument\");\n#if PY_MAJOR_VERSION >= 3\n        PyObject* errmsg = PyUnicode_FromString(\"Cannot encode object: \");\n        PyObject* error = PyUnicode_Concat(errmsg, repr);\n        PyErr_SetObject(InvalidDocument, error);\n        Py_DECREF(error);\n        Py_DECREF(repr);\n#else\n        PyObject* errmsg = PyString_FromString(\"Cannot encode object: \");\n        PyString_ConcatAndDel(&errmsg, repr);\n        PyErr_SetString(InvalidDocument, PyString_AsString(errmsg));\n#endif\n        Py_DECREF(errmsg);\n        Py_DECREF(InvalidDocument);\n        return 0;\n    }\n}\n\nstatic int check_key_name(const char* name,\n                          const Py_ssize_t name_length) {\n    int i;\n    if (name_length > 0 && name[0] == '$') {\n        PyObject* InvalidDocument = _error(\"InvalidDocument\");\n#if PY_MAJOR_VERSION >= 3\n        PyObject* errmsg = PyUnicode_FromFormat(\"key '%s' must not start with '$'\", name);\n        PyErr_SetObject(InvalidDocument, errmsg);\n#else\n        PyObject* errmsg = PyString_FromFormat(\"key '%s' must not start with '$'\", name);\n        PyErr_SetString(InvalidDocument, PyString_AsString(errmsg));\n#endif\n        Py_DECREF(errmsg);\n        Py_DECREF(InvalidDocument);\n        return 0;\n    }\n    for (i = 0; i < name_length; i++) {\n        if (name[i] == '.') {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n#if PY_MAJOR_VERSION >= 3\n            PyObject* errmsg = PyUnicode_FromFormat(\"key '%s' must not contain '.'\", name);\n            PyErr_SetObject(InvalidDocument, errmsg);\n#else\n            PyObject* errmsg = PyString_FromFormat(\"key '%s' must not contain '.'\", name);\n            PyErr_SetString(InvalidDocument, PyString_AsString(errmsg));\n#endif\n            Py_DECREF(errmsg);\n            Py_DECREF(InvalidDocument);\n            return 0;\n        }\n    }\n    return 1;\n}\n\n/* Write a (key, value) pair to the buffer.\n *\n * Returns 0 on failure */\nint write_pair(PyObject* self, buffer_t buffer, const char* name, Py_ssize_t name_length,\n               PyObject* value, unsigned char check_keys,\n               unsigned char uuid_subtype, unsigned char allow_id) {\n    int type_byte;\n\n    /* Don't write any _id elements unless we're explicitly told to -\n     * _id has to be written first so we do so, but don't bother\n     * deleting it from the dictionary being written. */\n    if (!allow_id && strcmp(name, \"_id\") == 0) {\n        return 1;\n    }\n\n    type_byte = buffer_save_space(buffer, 1);\n    if (type_byte == -1) {\n        PyErr_NoMemory();\n        return 0;\n    }\n    if (check_keys && !check_key_name(name, name_length)) {\n        return 0;\n    }\n    if (!buffer_write_bytes(buffer, name, name_length + 1)) {\n        return 0;\n    }\n    if (!write_element_to_buffer(self, buffer, type_byte, value,\n                                 check_keys, uuid_subtype, 1)) {\n        return 0;\n    }\n    return 1;\n}\n\nint decode_and_write_pair(PyObject* self, buffer_t buffer,\n                          PyObject* key, PyObject* value,\n                          unsigned char check_keys,\n                          unsigned char uuid_subtype, unsigned char top_level) {\n    PyObject* encoded;\n    if (PyUnicode_Check(key)) {\n        result_t status;\n        encoded = PyUnicode_AsUTF8String(key);\n        if (!encoded) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        status = check_string((const unsigned char*)PyBytes_AsString(encoded),\n                              PyBytes_Size(encoded), 0, 1);\n#else\n        status = check_string((const unsigned char*)PyString_AsString(encoded),\n                              PyString_Size(encoded), 0, 1);\n#endif\n\n        if (status == HAS_NULL) {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n            PyErr_SetString(InvalidDocument,\n                            \"Key names must not contain the NULL byte\");\n            Py_DECREF(InvalidDocument);\n            return 0;\n        }\n#if PY_MAJOR_VERSION < 3\n    } else if (PyString_Check(key)) {\n        result_t status;\n        encoded = key;\n        Py_INCREF(encoded);\n\n        status = check_string((const unsigned char*)PyString_AsString(encoded),\n                                       PyString_Size(encoded), 1, 1);\n\n        if (status == NOT_UTF_8) {\n            PyObject* InvalidStringData = _error(\"InvalidStringData\");\n            PyErr_SetString(InvalidStringData,\n                            \"strings in documents must be valid UTF-8\");\n            Py_DECREF(InvalidStringData);\n            return 0;\n        } else if (status == HAS_NULL) {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n            PyErr_SetString(InvalidDocument,\n                            \"Key names must not contain the NULL byte\");\n            Py_DECREF(InvalidDocument);\n            return 0;\n        }\n#endif\n    } else {\n        PyObject* InvalidDocument = _error(\"InvalidDocument\");\n        PyObject* repr = PyObject_Repr(key);\n#if PY_MAJOR_VERSION >= 3\n        PyObject* errmsg = PyUnicode_FromString(\"documents must have only string keys, key was \");\n        PyObject* error = PyUnicode_Concat(errmsg, repr);\n        PyErr_SetObject(InvalidDocument, error);\n        Py_DECREF(error);\n#else\n        PyObject* errmsg = PyString_FromString(\"documents must have only string keys, key was \");\n        PyString_ConcatAndDel(&errmsg, repr);\n        PyErr_SetString(InvalidDocument, PyString_AsString(errmsg));\n#endif\n        Py_DECREF(InvalidDocument);\n        Py_DECREF(errmsg);\n        return 0;\n    }\n\n    /* If top_level is True, don't allow writing _id here - it was already written. */\n#if PY_MAJOR_VERSION >= 3\n    if (!write_pair(self, buffer, PyBytes_AsString(encoded),\n                    PyBytes_Size(encoded), value,\n                    check_keys, uuid_subtype, !top_level)) {\n#else\n    if (!write_pair(self, buffer, PyString_AsString(encoded),\n                    PyString_Size(encoded), value,\n                    check_keys, uuid_subtype, !top_level)) {\n#endif\n        Py_DECREF(encoded);\n        return 0;\n    }\n\n    Py_DECREF(encoded);\n    return 1;\n}\n\n/* returns 0 on failure */\nint write_dict(PyObject* self, buffer_t buffer, PyObject* dict,\n               unsigned char check_keys, unsigned char uuid_subtype, unsigned char top_level) {\n    PyObject* key;\n    PyObject* iter;\n    char zero = 0;\n    int length;\n    int length_location;\n\n    if (!PyDict_Check(dict)) {\n        PyObject* repr = PyObject_Repr(dict);\n#if PY_MAJOR_VERSION >= 3\n        PyObject* errmsg = PyUnicode_FromString(\"encoder expected a mapping type but got: \");\n        PyObject* error = PyUnicode_Concat(errmsg, repr);\n        PyErr_SetObject(PyExc_TypeError, error);\n        Py_DECREF(error);\n        Py_DECREF(repr);\n#else\n        PyObject* errmsg = PyString_FromString(\"encoder expected a mapping type but got: \");\n        PyString_ConcatAndDel(&errmsg, repr);\n        PyErr_SetString(PyExc_TypeError, PyString_AsString(errmsg));\n#endif\n        Py_DECREF(errmsg);\n        return 0;\n    }\n\n    length_location = buffer_save_space(buffer, 4);\n    if (length_location == -1) {\n        PyErr_NoMemory();\n        return 0;\n    }\n\n    /* Write _id first if this is a top level doc. */\n    if (top_level) {\n        PyObject* _id = PyDict_GetItemString(dict, \"_id\");\n        if (_id) {\n            /* Don't bother checking keys, but do make sure we're allowed to\n             * write _id */\n            if (!write_pair(self, buffer, \"_id\", 3, _id, 0, uuid_subtype, 1)) {\n                return 0;\n            }\n        }\n    }\n\n    iter = PyObject_GetIter(dict);\n    if (iter == NULL) {\n        return 0;\n    }\n    while ((key = PyIter_Next(iter)) != NULL) {\n        PyObject* value = PyDict_GetItem(dict, key);\n        if (!value) {\n            PyErr_SetObject(PyExc_KeyError, key);\n            Py_DECREF(key);\n            Py_DECREF(iter);\n            return 0;\n        }\n        if (!decode_and_write_pair(self, buffer, key, value,\n                                   check_keys, uuid_subtype, top_level)) {\n            Py_DECREF(key);\n            Py_DECREF(iter);\n            return 0;\n        }\n        Py_DECREF(key);\n    }\n    Py_DECREF(iter);\n\n    /* write null byte and fill in length */\n    if (!buffer_write_bytes(buffer, &zero, 1)) {\n        return 0;\n    }\n    length = buffer_get_position(buffer) - length_location;\n    memcpy(buffer_get_buffer(buffer) + length_location, &length, 4);\n    return 1;\n}\n\nstatic PyObject* _cbson_dict_to_bson(PyObject* self, PyObject* args) {\n    PyObject* dict;\n    PyObject* result;\n    unsigned char check_keys;\n    unsigned char uuid_subtype;\n    unsigned char top_level = 1;\n    buffer_t buffer;\n\n    if (!PyArg_ParseTuple(args, \"Obb|b\", &dict,\n                          &check_keys, &uuid_subtype, &top_level)) {\n        return NULL;\n    }\n\n    buffer = buffer_new();\n    if (!buffer) {\n        PyErr_NoMemory();\n        return NULL;\n    }\n\n    if (!write_dict(self, buffer, dict, check_keys, uuid_subtype, top_level)) {\n        buffer_free(buffer);\n        return NULL;\n    }\n\n    /* objectify buffer */\n#if PY_MAJOR_VERSION >= 3\n    result = Py_BuildValue(\"y#\", buffer_get_buffer(buffer),\n                           buffer_get_position(buffer));\n#else\n    result = Py_BuildValue(\"s#\", buffer_get_buffer(buffer),\n                           buffer_get_position(buffer));\n#endif\n    buffer_free(buffer);\n    return result;\n}\n\nstatic PyObject* get_value(PyObject* self, const char* buffer, int* position,\n                           int type, int max, PyObject* as_class,\n                           unsigned char tz_aware, unsigned char uuid_subtype) {\n    struct module_state *state = GETSTATE(self);\n\n    PyObject* value;\n    PyObject* error;\n    switch (type) {\n    case 1:\n        {\n            double d;\n            if (max < 8) {\n                goto invalid;\n            }\n            memcpy(&d, buffer + *position, 8);\n            value = PyFloat_FromDouble(d);\n            if (!value) {\n                return NULL;\n            }\n            *position += 8;\n            break;\n        }\n    case 2:\n    case 14:\n        {\n            int value_length = ((int*)(buffer + *position))[0] - 1;\n            if (max < value_length) {\n                goto invalid;\n            }\n            *position += 4;\n            value = PyUnicode_DecodeUTF8(buffer + *position, value_length, \"strict\");\n            if (!value) {\n                return NULL;\n            }\n            *position += value_length + 1;\n            break;\n        }\n    case 3:\n        {\n            int size;\n            memcpy(&size, buffer + *position, 4);\n            if (max < size) {\n                goto invalid;\n            }\n            value = elements_to_dict(self, buffer + *position + 4,\n                                     size - 5, as_class, tz_aware, uuid_subtype);\n            if (!value) {\n                return NULL;\n            }\n\n            /* Decoding for DBRefs */\n            if (strcmp(buffer + *position + 5, \"$ref\") == 0) { /* DBRef */\n                PyObject* dbref;\n                PyObject* collection = PyDict_GetItemString(value, \"$ref\");\n                PyObject* id = PyDict_GetItemString(value, \"$id\");\n                PyObject* database = PyDict_GetItemString(value, \"$db\");\n\n                Py_INCREF(collection);\n                PyDict_DelItemString(value, \"$ref\");\n                Py_INCREF(id);\n                PyDict_DelItemString(value, \"$id\");\n\n                if (database == NULL) {\n                    database = Py_None;\n                    Py_INCREF(database);\n                } else {\n                    Py_INCREF(database);\n                    PyDict_DelItemString(value, \"$db\");\n                }\n\n                dbref = PyObject_CallFunctionObjArgs(state->DBRef, collection, id, database, value, NULL);\n                Py_DECREF(value);\n                value = dbref;\n\n                Py_DECREF(id);\n                Py_DECREF(collection);\n                Py_DECREF(database);\n                if (!value) {\n                    return NULL;\n                }\n            }\n\n            *position += size;\n            break;\n        }\n    case 4:\n        {\n            int size,\n                end;\n\n            memcpy(&size, buffer + *position, 4);\n            if (max < size) {\n                goto invalid;\n            }\n            end = *position + size - 1;\n            *position += 4;\n\n            value = PyList_New(0);\n            if (!value) {\n                return NULL;\n            }\n            while (*position < end) {\n                PyObject* to_append;\n\n                int type = (int)buffer[(*position)++];\n                int key_size = strlen(buffer + *position);\n                *position += key_size + 1; /* just skip the key, they're in order. */\n                to_append = get_value(self, buffer, position, type,\n                                      max - key_size, as_class, tz_aware, uuid_subtype);\n                if (!to_append) {\n                    Py_DECREF(value);\n                    return NULL;\n                }\n                PyList_Append(value, to_append);\n                Py_DECREF(to_append);\n            }\n            (*position)++;\n            break;\n        }\n    case 5:\n        {\n            PyObject* data;\n            PyObject* st;\n            int length, subtype;\n\n            memcpy(&length, buffer + *position, 4);\n            if (max < length) {\n                goto invalid;\n            }\n            subtype = (unsigned char)buffer[*position + 4];\n#if PY_MAJOR_VERSION >= 3\n            /* Python3 special case. Decode BSON binary subtype 0 to bytes. */\n            if (subtype == 0) {\n                value = PyBytes_FromStringAndSize(buffer + *position + 5, length);\n                *position += length + 5;\n                break;\n            }\n            if (subtype == 2) {\n                data = PyBytes_FromStringAndSize(buffer + *position + 9, length - 4);\n            } else {\n                data = PyBytes_FromStringAndSize(buffer + *position + 5, length);\n            }\n#else\n            if (subtype == 2) {\n                data = PyString_FromStringAndSize(buffer + *position + 9, length - 4);\n            } else {\n                data = PyString_FromStringAndSize(buffer + *position + 5, length);\n            }\n#endif\n            if (!data) {\n                return NULL;\n            }\n            if ((subtype == 3 || subtype == 4) && state->UUID) { // Encode as UUID, not Binary\n                PyObject* kwargs;\n                PyObject* args = PyTuple_New(0);\n                if (!args) {\n                    Py_DECREF(data);\n                    return NULL;\n                }\n                kwargs = PyDict_New();\n                if (!kwargs) {\n                    Py_DECREF(data);\n                    Py_DECREF(args);\n                    return NULL;\n                }\n\n                assert(length == 16); // UUID should always be 16 bytes\n\n                if (uuid_subtype == CSHARP_LEGACY) {\n                    /* Legacy C# byte order */\n                    PyDict_SetItemString(kwargs, \"bytes_le\", data);\n                }\n                else {\n                    if (uuid_subtype == JAVA_LEGACY) {\n                        /* Convert from legacy java byte order */\n                        char big_endian[16];\n                        _fix_java(buffer + *position + 5, big_endian);\n                        /* Free the previously created PyString object */\n                        Py_DECREF(data);\n#if PY_MAJOR_VERSION >= 3\n                        data = PyBytes_FromStringAndSize(big_endian, length);\n#else\n                        data = PyString_FromStringAndSize(big_endian, length);\n#endif\n                    }\n                    PyDict_SetItemString(kwargs, \"bytes\", data);\n                }\n                value = PyObject_Call(state->UUID, args, kwargs);\n\n                Py_DECREF(args);\n                Py_DECREF(kwargs);\n                Py_DECREF(data);\n                if (!value) {\n                    return NULL;\n                }\n\n                *position += length + 5;\n                break;\n            }\n\n#if PY_MAJOR_VERSION >= 3\n            st = PyLong_FromLong(subtype);\n#else\n            st = PyInt_FromLong(subtype);\n#endif\n            if (!st) {\n                Py_DECREF(data);\n                return NULL;\n            }\n            value = PyObject_CallFunctionObjArgs(state->Binary, data, st, NULL);\n            Py_DECREF(st);\n            Py_DECREF(data);\n            if (!value) {\n                return NULL;\n            }\n            *position += length + 5;\n            break;\n        }\n    case 6:\n    case 10:\n        {\n            value = Py_None;\n            Py_INCREF(value);\n            break;\n        }\n    case 7:\n        {\n            if (max < 12) {\n                goto invalid;\n            }\n#if PY_MAJOR_VERSION >= 3\n            value = PyObject_CallFunction(state->ObjectId, \"y#\", buffer + *position, 12);\n#else\n            value = PyObject_CallFunction(state->ObjectId, \"s#\", buffer + *position, 12);\n#endif\n            if (!value) {\n                return NULL;\n            }\n            *position += 12;\n            break;\n        }\n    case 8:\n        {\n            value = buffer[(*position)++] ? Py_True : Py_False;\n            Py_INCREF(value);\n            break;\n        }\n    case 9:\n        {\n            PyObject* naive;\n            PyObject* replace;\n            PyObject* args;\n            PyObject* kwargs;\n            if (max < 8) {\n                goto invalid;\n            }\n            naive = datetime_from_millis(*(long long*)(buffer + *position));\n            *position += 8;\n            if (!tz_aware) { /* In the naive case, we're done here. */\n                value = naive;\n                break;\n            }\n\n            if (!naive) {\n                return NULL;\n            }\n            replace = PyObject_GetAttrString(naive, \"replace\");\n            Py_DECREF(naive);\n            if (!replace) {\n                return NULL;\n            }\n            args = PyTuple_New(0);\n            if (!args) {\n                Py_DECREF(replace);\n                return NULL;\n            }\n            kwargs = PyDict_New();\n            if (!kwargs) {\n                Py_DECREF(replace);\n                Py_DECREF(args);\n                return NULL;\n            }\n            if (PyDict_SetItemString(kwargs, \"tzinfo\", state->UTC) == -1) {\n                Py_DECREF(replace);\n                Py_DECREF(args);\n                Py_DECREF(kwargs);\n                return NULL;\n            }\n            value = PyObject_Call(replace, args, kwargs);\n            Py_DECREF(replace);\n            Py_DECREF(args);\n            Py_DECREF(kwargs);\n            break;\n        }\n    case 11:\n        {\n            PyObject* pattern;\n            int flags_length,\n                flags,\n                i;\n            int pattern_length = strlen(buffer + *position);\n            if (max < pattern_length) {\n                goto invalid;\n            }\n            pattern = PyUnicode_DecodeUTF8(buffer + *position, pattern_length, \"strict\");\n            if (!pattern) {\n                return NULL;\n            }\n            *position += pattern_length + 1;\n            flags_length = strlen(buffer + *position);\n            if (max < pattern_length + flags_length) {\n                Py_DECREF(pattern);\n                goto invalid;\n            }\n            flags = 0;\n            for (i = 0; i < flags_length; i++) {\n                if (buffer[*position + i] == 'i') {\n                    flags |= 2;\n                } else if (buffer[*position + i] == 'l') {\n                    flags |= 4;\n                } else if (buffer[*position + i] == 'm') {\n                    flags |= 8;\n                } else if (buffer[*position + i] == 's') {\n                    flags |= 16;\n                } else if (buffer[*position + i] == 'u') {\n                    flags |= 32;\n                } else if (buffer[*position + i] == 'x') {\n                    flags |= 64;\n                }\n            }\n            *position += flags_length + 1;\n            value = PyObject_CallFunction(state->RECompile, \"Oi\", pattern, flags);\n            Py_DECREF(pattern);\n            break;\n        }\n    case 12:\n        {\n            int collection_length;\n            PyObject* collection;\n            PyObject* id;\n\n            *position += 4;\n            collection_length = strlen(buffer + *position);\n            if (max < collection_length) {\n                goto invalid;\n            }\n            collection = PyUnicode_DecodeUTF8(buffer + *position, collection_length, \"strict\");\n            if (!collection) {\n                return NULL;\n            }\n            *position += collection_length + 1;\n            if (max < collection_length + 12) {\n                Py_DECREF(collection);\n                goto invalid;\n            }\n            id = PyObject_CallFunction(state->ObjectId, \"s#\", buffer + *position, 12);\n            if (!id) {\n                Py_DECREF(collection);\n                return NULL;\n            }\n            *position += 12;\n            value = PyObject_CallFunctionObjArgs(state->DBRef, collection, id, NULL);\n            Py_DECREF(collection);\n            Py_DECREF(id);\n            break;\n        }\n    case 13:\n        {\n            PyObject* code;\n            int value_length = ((int*)(buffer + *position))[0] - 1;\n            if (max < value_length) {\n                goto invalid;\n            }\n            *position += 4;\n            code = PyUnicode_DecodeUTF8(buffer + *position, value_length, \"strict\");\n            if (!code) {\n                return NULL;\n            }\n            *position += value_length + 1;\n            value = PyObject_CallFunctionObjArgs(state->Code, code, NULL, NULL);\n            Py_DECREF(code);\n            break;\n        }\n    case 15:\n        {\n            int code_length,\n                scope_size;\n            PyObject* code;\n            PyObject* scope;\n\n            *position += 8;\n            code_length = strlen(buffer + *position);\n            if (max < 8 + code_length) {\n                goto invalid;\n            }\n            code = PyUnicode_DecodeUTF8(buffer + *position, code_length, \"strict\");\n            if (!code) {\n                return NULL;\n            }\n            *position += code_length + 1;\n\n            memcpy(&scope_size, buffer + *position, 4);\n            scope = elements_to_dict(self, buffer + *position + 4, scope_size - 5,\n                                     (PyObject*)&PyDict_Type, tz_aware, uuid_subtype);\n            if (!scope) {\n                Py_DECREF(code);\n                return NULL;\n            }\n            *position += scope_size;\n\n            value = PyObject_CallFunctionObjArgs(state->Code, code, scope, NULL);\n            Py_DECREF(code);\n            Py_DECREF(scope);\n            break;\n        }\n    case 16:\n        {\n            int i;\n            if (max < 4) {\n                goto invalid;\n            }\n            memcpy(&i, buffer + *position, 4);\n#if PY_MAJOR_VERSION >= 3\n            value = PyLong_FromLong(i);\n#else\n            value = PyInt_FromLong(i);\n#endif\n            if (!value) {\n                return NULL;\n            }\n            *position += 4;\n            break;\n        }\n    case 17:\n        {\n            unsigned int time, inc;\n            if (max < 8) {\n                goto invalid;\n            }\n            memcpy(&inc, buffer + *position, 4);\n            memcpy(&time, buffer + *position + 4, 4);\n            value = PyObject_CallFunction(state->Timestamp, \"II\", time, inc);\n            if (!value) {\n                return NULL;\n            }\n            *position += 8;\n            break;\n        }\n    case 18:\n        {\n            long long ll;\n            if (max < 8) {\n                goto invalid;\n            }\n            memcpy(&ll, buffer + *position, 8);\n            value = PyLong_FromLongLong(ll);\n            if (!value) {\n                return NULL;\n            }\n            *position += 8;\n            break;\n        }\n    case -1:\n        {\n            value = PyObject_CallFunctionObjArgs(state->MinKey, NULL);\n            break;\n        }\n    case 127:\n        {\n            value = PyObject_CallFunctionObjArgs(state->MaxKey, NULL);\n            break;\n        }\n    default:\n        {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n            PyErr_SetString(InvalidDocument, \"no c decoder for this type yet\");\n            Py_DECREF(InvalidDocument);\n            return NULL;\n        }\n    }\n    return value;\n\n    invalid:\n\n    error = _error(\"InvalidBSON\");\n    PyErr_SetNone(error);\n    Py_DECREF(error);\n    return NULL;\n}\n\nstatic PyObject* elements_to_dict(PyObject* self, const char* string, int max,\n                                  PyObject* as_class, unsigned char tz_aware,\n                                  unsigned char uuid_subtype) {\n    int position = 0;\n    PyObject* dict = PyObject_CallObject(as_class, NULL);\n    if (!dict) {\n        return NULL;\n    }\n    while (position < max) {\n        PyObject* name;\n        PyObject* value;\n        int type = (int)string[position++];\n        int name_length = strlen(string + position);\n        if (position + name_length >= max) {\n            PyObject* InvalidBSON = _error(\"InvalidBSON\");\n            PyErr_SetNone(InvalidBSON);\n            Py_DECREF(InvalidBSON);\n            Py_DECREF(dict);\n            return NULL;\n        }\n        name = PyUnicode_DecodeUTF8(string + position, name_length, \"strict\");\n        if (!name) {\n            Py_DECREF(dict);\n            return NULL;\n        }\n        position += name_length + 1;\n        value = get_value(self, string, &position, type,\n                          max - position, as_class, tz_aware, uuid_subtype);\n        if (!value) {\n            Py_DECREF(name);\n            Py_DECREF(dict);\n            return NULL;\n        }\n\n        PyObject_SetItem(dict, name, value);\n        Py_DECREF(name);\n        Py_DECREF(value);\n    }\n    return dict;\n}\n\nstatic PyObject* _cbson_bson_to_dict(PyObject* self, PyObject* args) {\n    unsigned int size;\n    Py_ssize_t total_size;\n    const char* string;\n    PyObject* bson;\n    PyObject* as_class;\n    unsigned char tz_aware;\n    unsigned char uuid_subtype;\n    PyObject* dict;\n    PyObject* remainder;\n    PyObject* result;\n\n    if (!PyArg_ParseTuple(args, \"OObb\", &bson, &as_class, &tz_aware, &uuid_subtype)) {\n        return NULL;\n    }\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyBytes_Check(bson)) {\n        PyErr_SetString(PyExc_TypeError, \"argument to _bson_to_dict must be a bytes object\");\n#else\n    if (!PyString_Check(bson)) {\n        PyErr_SetString(PyExc_TypeError, \"argument to _bson_to_dict must be a string\");\n#endif\n        return NULL;\n    }\n#if PY_MAJOR_VERSION >= 3\n    total_size = PyBytes_Size(bson);\n#else\n    total_size = PyString_Size(bson);\n#endif\n    if (total_size < 5) {\n        PyObject* InvalidBSON = _error(\"InvalidBSON\");\n        PyErr_SetString(InvalidBSON,\n                        \"not enough data for a BSON document\");\n        Py_DECREF(InvalidBSON);\n        return NULL;\n    }\n\n#if PY_MAJOR_VERSION >= 3\n    string = PyBytes_AsString(bson);\n#else\n    string = PyString_AsString(bson);\n#endif\n    if (!string) {\n        return NULL;\n    }\n    memcpy(&size, string, 4);\n\n    if (total_size < size) {\n        PyObject* InvalidBSON = _error(\"InvalidBSON\");\n        PyErr_SetString(InvalidBSON,\n                        \"objsize too large\");\n        Py_DECREF(InvalidBSON);\n        return NULL;\n    }\n\n    if (size != total_size || string[size - 1]) {\n        PyObject* InvalidBSON = _error(\"InvalidBSON\");\n        PyErr_SetString(InvalidBSON,\n                        \"bad eoo\");\n        Py_DECREF(InvalidBSON);\n        return NULL;\n    }\n\n    dict = elements_to_dict(self, string + 4, size - 5, as_class, tz_aware, uuid_subtype);\n    if (!dict) {\n        return NULL;\n    }\n#if PY_MAJOR_VERSION >= 3\n    remainder = PyBytes_FromStringAndSize(string + size, total_size - size);\n#else\n    remainder = PyString_FromStringAndSize(string + size, total_size - size);\n#endif\n    if (!remainder) {\n        Py_DECREF(dict);\n        return NULL;\n    }\n    result = Py_BuildValue(\"OO\", dict, remainder);\n    Py_DECREF(dict);\n    Py_DECREF(remainder);\n    return result;\n}\n\nstatic PyObject* _cbson_decode_all(PyObject* self, PyObject* args) {\n    unsigned int size;\n    Py_ssize_t total_size;\n    const char* string;\n    PyObject* bson;\n    PyObject* dict;\n    PyObject* result;\n    PyObject* as_class = (PyObject*)&PyDict_Type;\n    unsigned char tz_aware = 1;\n    unsigned char uuid_subtype = 3;\n\n    if (!PyArg_ParseTuple(args, \"O|Obb\", &bson, &as_class, &tz_aware, &uuid_subtype)) {\n        return NULL;\n    }\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyBytes_Check(bson)) {\n        PyErr_SetString(PyExc_TypeError, \"argument to decode_all must be a bytes object\");\n#else\n    if (!PyString_Check(bson)) {\n        PyErr_SetString(PyExc_TypeError, \"argument to decode_all must be a string\");\n#endif\n        return NULL;\n    }\n#if PY_MAJOR_VERSION >= 3\n    total_size = PyBytes_Size(bson);\n    string = PyBytes_AsString(bson);\n#else\n    total_size = PyString_Size(bson);\n    string = PyString_AsString(bson);\n#endif\n    if (!string) {\n        return NULL;\n    }\n\n    result = PyList_New(0);\n\n    while (total_size > 0) {\n        if (total_size < 5) {\n            PyObject* InvalidBSON = _error(\"InvalidBSON\");\n            PyErr_SetString(InvalidBSON,\n                            \"not enough data for a BSON document\");\n            Py_DECREF(InvalidBSON);\n            Py_DECREF(result);\n            return NULL;\n        }\n\n        memcpy(&size, string, 4);\n\n        if (total_size < size) {\n            PyObject* InvalidBSON = _error(\"InvalidBSON\");\n            PyErr_SetString(InvalidBSON,\n                            \"objsize too large\");\n            Py_DECREF(InvalidBSON);\n            Py_DECREF(result);\n            return NULL;\n        }\n\n        if (string[size - 1]) {\n            PyObject* InvalidBSON = _error(\"InvalidBSON\");\n            PyErr_SetString(InvalidBSON,\n                            \"bad eoo\");\n            Py_DECREF(InvalidBSON);\n            Py_DECREF(result);\n            return NULL;\n        }\n\n        dict = elements_to_dict(self, string + 4, size - 5,\n                                as_class, tz_aware, uuid_subtype);\n        if (!dict) {\n            Py_DECREF(result);\n            return NULL;\n        }\n        PyList_Append(result, dict);\n        Py_DECREF(dict);\n        string += size;\n        total_size -= size;\n    }\n\n    return result;\n}\n\nstatic PyMethodDef _CBSONMethods[] = {\n    {\"_dict_to_bson\", _cbson_dict_to_bson, METH_VARARGS,\n     \"convert a dictionary to a string containing its BSON representation.\"},\n    {\"_bson_to_dict\", _cbson_bson_to_dict, METH_VARARGS,\n     \"convert a BSON string to a SON object.\"},\n    {\"decode_all\", _cbson_decode_all, METH_VARARGS,\n     \"convert binary data to a sequence of documents.\"},\n    {NULL, NULL, 0, NULL}\n};\n\n#if PY_MAJOR_VERSION >= 3\n#define INITERROR return NULL\nstatic int _cbson_traverse(PyObject *m, visitproc visit, void *arg) {\n    Py_VISIT(GETSTATE(m)->Binary);\n    Py_VISIT(GETSTATE(m)->Code);\n    Py_VISIT(GETSTATE(m)->ObjectId);\n    Py_VISIT(GETSTATE(m)->DBRef);\n    Py_VISIT(GETSTATE(m)->RECompile);\n    Py_VISIT(GETSTATE(m)->UUID);\n    Py_VISIT(GETSTATE(m)->Timestamp);\n    Py_VISIT(GETSTATE(m)->MinKey);\n    Py_VISIT(GETSTATE(m)->MaxKey);\n    Py_VISIT(GETSTATE(m)->UTC);\n    Py_VISIT(GETSTATE(m)->REType);\n    return 0;\n}\n\nstatic int _cbson_clear(PyObject *m) {\n    Py_CLEAR(GETSTATE(m)->Binary);\n    Py_CLEAR(GETSTATE(m)->Code);\n    Py_CLEAR(GETSTATE(m)->ObjectId);\n    Py_CLEAR(GETSTATE(m)->DBRef);\n    Py_CLEAR(GETSTATE(m)->RECompile);\n    Py_CLEAR(GETSTATE(m)->UUID);\n    Py_CLEAR(GETSTATE(m)->Timestamp);\n    Py_CLEAR(GETSTATE(m)->MinKey);\n    Py_CLEAR(GETSTATE(m)->MaxKey);\n    Py_CLEAR(GETSTATE(m)->UTC);\n    Py_CLEAR(GETSTATE(m)->REType);\n    return 0;\n}\n\nstatic struct PyModuleDef moduledef = {\n    PyModuleDef_HEAD_INIT,\n    \"_cbson\",\n    NULL,\n    sizeof(struct module_state),\n    _CBSONMethods,\n    NULL,\n    _cbson_traverse,\n    _cbson_clear,\n    NULL\n};\n\nPyMODINIT_FUNC\nPyInit__cbson(void)\n#else\n#define INITERROR return\nPyMODINIT_FUNC\ninit_cbson(void)\n#endif\n{\n    PyObject *m;\n    PyObject *c_api_object;\n    static void *_cbson_API[_cbson_API_POINTER_COUNT];\n\n#if PY_MAJOR_VERSION >= 3\n    m = PyModule_Create(&moduledef);\n#else\n    m = Py_InitModule(\"_cbson\", _CBSONMethods);\n#endif\n    if (m == NULL) {\n        INITERROR;\n    }\n\n    PyDateTime_IMPORT;\n    if (PyDateTimeAPI == NULL) {\n        Py_DECREF(m);\n        INITERROR;\n    }\n\n    /* Import several python objects */\n    if (_reload_python_objects(m)) {\n        Py_DECREF(m);\n        INITERROR;\n    }\n\n    /* Export C API */\n    _cbson_API[_cbson_buffer_write_bytes_INDEX] = (void *) buffer_write_bytes;\n    _cbson_API[_cbson_write_dict_INDEX] = (void *) write_dict;\n    _cbson_API[_cbson_write_pair_INDEX] = (void *) write_pair;\n    _cbson_API[_cbson_decode_and_write_pair_INDEX] = (void *) decode_and_write_pair;\n\n#if PY_VERSION_HEX >= 0x03010000\n    /* PyCapsule is new in python 3.1 */\n    c_api_object = PyCapsule_New((void *) _cbson_API, \"_cbson._C_API\", NULL);\n#else\n    c_api_object = PyCObject_FromVoidPtr((void *) _cbson_API, NULL);\n#endif\n\n    if (c_api_object != NULL) {\n        PyModule_AddObject(m, \"_C_API\", c_api_object);\n    }\n#if PY_MAJOR_VERSION >= 3\n    return m;\n#endif\n}\n", "# -*- coding: utf-8 -*-\n\n# Copyright 2009-2012 10gen, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Test the collection module.\"\"\"\n\nimport itertools\nimport re\nimport sys\nimport threading\nimport time\nimport unittest\nimport warnings\n\nfrom nose.plugins.skip import SkipTest\n\nsys.path[0:0] = [\"\"]\n\nfrom bson.binary import Binary, UUIDLegacy, OLD_UUID_SUBTYPE, UUID_SUBTYPE\nfrom bson.code import Code\nfrom bson.objectid import ObjectId\nfrom bson.py3compat import b\nfrom bson.son import SON\nfrom pymongo import (ASCENDING, DESCENDING, GEO2D,\n                     GEOHAYSTACK, GEOSPHERE, HASHED)\nfrom pymongo.collection import Collection\nfrom pymongo.son_manipulator import SONManipulator\nfrom pymongo.errors import (ConfigurationError,\n                            DuplicateKeyError,\n                            InvalidDocument,\n                            InvalidName,\n                            InvalidOperation,\n                            OperationFailure,\n                            TimeoutError)\nfrom test.test_client import get_client\nfrom test.utils import is_mongos, joinall\nfrom test import (qcheck,\n                  version)\n\nhave_uuid = True\ntry:\n    import uuid\nexcept ImportError:\n    have_uuid = False\n\n\nclass TestCollection(unittest.TestCase):\n\n    def setUp(self):\n        self.client = get_client()\n        self.db = self.client.pymongo_test\n\n    def tearDown(self):\n        self.db.drop_collection(\"test_large_limit\")\n        self.db = None\n        self.client = None\n\n    def test_collection(self):\n        self.assertRaises(TypeError, Collection, self.db, 5)\n\n        def make_col(base, name):\n            return base[name]\n\n        self.assertRaises(InvalidName, make_col, self.db, \"\")\n        self.assertRaises(InvalidName, make_col, self.db, \"te$t\")\n        self.assertRaises(InvalidName, make_col, self.db, \".test\")\n        self.assertRaises(InvalidName, make_col, self.db, \"test.\")\n        self.assertRaises(InvalidName, make_col, self.db, \"tes..t\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"te$t\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \".test\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"test.\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"tes..t\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"tes\\x00t\")\n\n        self.assertTrue(isinstance(self.db.test, Collection))\n        self.assertEqual(self.db.test, self.db[\"test\"])\n        self.assertEqual(self.db.test, Collection(self.db, \"test\"))\n        self.assertEqual(self.db.test.mike, self.db[\"test.mike\"])\n        self.assertEqual(self.db.test[\"mike\"], self.db[\"test.mike\"])\n\n        self.db.drop_collection('test')\n        self.assertFalse('test' in self.db.collection_names())\n\n        # No exception\n        self.db.drop_collection('test')\n\n    def test_create_index(self):\n        db = self.db\n\n        self.assertRaises(TypeError, db.test.create_index, 5)\n        self.assertRaises(TypeError, db.test.create_index, {\"hello\": 1})\n        self.assertRaises(ValueError, db.test.create_index, [])\n\n        db.test.drop_indexes()\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 1)\n\n        db.test.create_index(\"hello\")\n        db.test.create_index([(\"hello\", DESCENDING), (\"world\", ASCENDING)])\n\n        count = 0\n        for _ in db.system.indexes.find({\"ns\": u\"pymongo_test.test\"}):\n            count += 1\n        self.assertEqual(count, 3)\n\n        db.test.drop_indexes()\n        ix = db.test.create_index([(\"hello\", DESCENDING),\n                                   (\"world\", ASCENDING)], name=\"hello_world\")\n        self.assertEqual(ix, \"hello_world\")\n\n        db.test.drop_indexes()\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 1)\n        db.test.create_index(\"hello\")\n        self.assertTrue(u\"hello_1\" in\n                        [a[\"name\"] for a in db.system.indexes\n                         .find({\"ns\": u\"pymongo_test.test\"})])\n\n        db.test.drop_indexes()\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 1)\n        db.test.create_index([(\"hello\", DESCENDING), (\"world\", ASCENDING)])\n        self.assertTrue(u\"hello_-1_world_1\" in\n                        [a[\"name\"] for a in db.system.indexes\n                         .find({\"ns\": u\"pymongo_test.test\"})])\n\n        db.test.drop()\n        db.test.insert({'a': 1})\n        db.test.insert({'a': 1})\n        self.assertRaises(DuplicateKeyError, db.test.create_index,\n                                                    'a', unique=True)\n\n    def test_ensure_index(self):\n        db = self.db\n\n        self.assertRaises(TypeError, db.test.ensure_index, {\"hello\": 1})\n\n        db.test.drop_indexes()\n        self.assertEqual(\"hello_1\", db.test.create_index(\"hello\"))\n        self.assertEqual(\"hello_1\", db.test.create_index(\"hello\"))\n\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_indexes()\n        self.assertEqual(\"foo\",\n                         db.test.ensure_index(\"goodbye\", name=\"foo\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\", name=\"foo\"))\n\n        db.test.drop_indexes()\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_index(\"goodbye_1\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.drop_collection(\"test\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_index(\"goodbye_1\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.create_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_index(\"goodbye_1\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\", cache_for=1))\n        time.sleep(1.2)\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_index(\"goodbye_1\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.create_index(\"goodbye\", cache_for=1))\n        time.sleep(1.2)\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        # Make sure the expiration time is updated.\n        self.assertEqual(None,\n                         db.test.ensure_index(\"goodbye\"))\n\n        # Clean up indexes for later tests\n        db.test.drop_indexes()\n\n    def test_deprecated_ttl_index_kwarg(self):\n        db = self.db\n\n        # In Python 2.6+ we could use the catch_warnings context\n        # manager to test this warning nicely. As we can't do that\n        # we must test raising errors before the ignore filter is applied.\n        warnings.simplefilter(\"error\", DeprecationWarning)\n        self.assertRaises(DeprecationWarning, lambda:\n                        db.test.ensure_index(\"goodbye\", ttl=10))\n        warnings.resetwarnings()\n        warnings.simplefilter(\"ignore\")\n\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\", ttl=10))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n    def test_ensure_unique_index_threaded(self):\n        coll = self.db.test_unique_threaded\n        coll.drop()\n        coll.insert(({'foo': i} for i in xrange(10000)))\n\n        class Indexer(threading.Thread):\n            def run(self):\n                try:\n                    coll.ensure_index('foo', unique=True)\n                    coll.insert({'foo': 'bar'})\n                    coll.insert({'foo': 'bar'})\n                except OperationFailure:\n                    pass\n\n        threads = []\n        for _ in xrange(10):\n            t = Indexer()\n            t.setDaemon(True)\n            threads.append(t)\n\n        for i in xrange(10):\n            threads[i].start()\n\n        joinall(threads)\n\n        self.assertEqual(10001, coll.count())\n        coll.drop()\n\n    def test_index_on_binary(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.save({\"bin\": Binary(b(\"def\"))})\n        db.test.save({\"bin\": Binary(b(\"abc\"))})\n        db.test.save({\"bin\": Binary(b(\"ghi\"))})\n\n        self.assertEqual(db.test.find({\"bin\": Binary(b(\"abc\"))})\n                         .explain()[\"nscanned\"], 3)\n\n        db.test.create_index(\"bin\")\n        self.assertEqual(db.test.find({\"bin\": Binary(b(\"abc\"))})\n                         .explain()[\"nscanned\"], 1)\n\n    def test_drop_index(self):\n        db = self.db\n        db.test.drop_indexes()\n        db.test.create_index(\"hello\")\n        name = db.test.create_index(\"goodbye\")\n\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 3)\n        self.assertEqual(name, \"goodbye_1\")\n        db.test.drop_index(name)\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 2)\n        self.assertTrue(u\"hello_1\" in\n                        [a[\"name\"] for a in db.system.indexes\n                         .find({\"ns\": u\"pymongo_test.test\"})])\n\n        db.test.drop_indexes()\n        db.test.create_index(\"hello\")\n        name = db.test.create_index(\"goodbye\")\n\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 3)\n        self.assertEqual(name, \"goodbye_1\")\n        db.test.drop_index([(\"goodbye\", ASCENDING)])\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 2)\n        self.assertTrue(u\"hello_1\" in\n                        [a[\"name\"] for a in db.system.indexes\n                         .find({\"ns\": u\"pymongo_test.test\"})])\n\n    def test_reindex(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.insert({\"foo\": \"bar\", \"who\": \"what\", \"when\": \"how\"})\n        db.test.create_index(\"foo\")\n        db.test.create_index(\"who\")\n        db.test.create_index(\"when\")\n        info = db.test.index_information()\n\n        def check_result(result):\n            self.assertEqual(4, result['nIndexes'])\n            self.assertEqual(4, result['nIndexesWas'])\n            indexes = result['indexes']\n            names = [idx['name'] for idx in indexes]\n            for name in names:\n                self.assertTrue(name in info)\n            for key in info:\n                self.assertTrue(key in names)\n\n        reindexed = db.test.reindex()\n        if 'raw' in reindexed:\n            # mongos\n            for result in reindexed['raw'].itervalues():\n                check_result(result)\n        else:\n            check_result(reindexed)\n\n    def test_index_info(self):\n        db = self.db\n        db.test.drop_indexes()\n        db.test.remove({})\n        db.test.save({})  # create collection\n        self.assertEqual(len(db.test.index_information()), 1)\n        self.assertTrue(\"_id_\" in db.test.index_information())\n\n        db.test.create_index(\"hello\")\n        self.assertEqual(len(db.test.index_information()), 2)\n        self.assertEqual(db.test.index_information()[\"hello_1\"][\"key\"],\n                         [(\"hello\", ASCENDING)])\n\n        db.test.create_index([(\"hello\", DESCENDING), (\"world\", ASCENDING)],\n                             unique=True)\n        self.assertEqual(db.test.index_information()[\"hello_1\"][\"key\"],\n                         [(\"hello\", ASCENDING)])\n        self.assertEqual(len(db.test.index_information()), 3)\n        self.assertEqual([(\"hello\", DESCENDING), (\"world\", ASCENDING)],\n                         db.test.index_information()[\"hello_-1_world_1\"][\"key\"]\n                        )\n        self.assertEqual(True,\n                     db.test.index_information()[\"hello_-1_world_1\"][\"unique\"])\n\n    def test_index_geo2d(self):\n        db = self.db\n        db.test.drop_indexes()\n        self.assertEqual('loc_2d', db.test.create_index([(\"loc\", GEO2D)]))\n        index_info = db.test.index_information()['loc_2d']\n        self.assertEqual([('loc', '2d')], index_info['key'])\n\n    def test_index_haystack(self):\n        if is_mongos(self.db.connection):\n            raise SkipTest(\"geoSearch is not supported by mongos\")\n        db = self.db\n        db.test.drop_indexes()\n        db.test.remove()\n        _id = db.test.insert({\n            \"pos\": {\"long\": 34.2, \"lat\": 33.3},\n            \"type\": \"restaurant\"\n        })\n        db.test.insert({\n            \"pos\": {\"long\": 34.2, \"lat\": 37.3}, \"type\": \"restaurant\"\n        })\n        db.test.insert({\n            \"pos\": {\"long\": 59.1, \"lat\": 87.2}, \"type\": \"office\"\n        })\n        db.test.create_index(\n            [(\"pos\", GEOHAYSTACK), (\"type\", ASCENDING)],\n            bucket_size=1\n        )\n\n        results = db.command(SON([\n            (\"geoSearch\", \"test\"),\n            (\"near\", [33, 33]),\n            (\"maxDistance\", 6),\n            (\"search\", {\"type\": \"restaurant\"}),\n            (\"limit\", 30),\n        ]))['results']\n\n        self.assertEqual(2, len(results))\n        self.assertEqual({\n            \"_id\": _id,\n            \"pos\": {\"long\": 34.2, \"lat\": 33.3},\n            \"type\": \"restaurant\"\n        }, results[0])\n\n    def test_index_text(self):\n        if not version.at_least(self.client, (2, 3, 2)):\n            raise SkipTest(\"Text search requires server >=2.3.2.\")\n\n        if is_mongos(self.client):\n            raise SkipTest(\"setParameter does not work through mongos\")\n\n        self.client.admin.command('setParameter', '*',\n                                  textSearchEnabled=True)\n\n        db = self.db\n        db.test.drop_indexes()\n        self.assertEqual(\"t_text\", db.test.create_index([(\"t\", \"text\")]))\n        index_info = db.test.index_information()[\"t_text\"]\n        self.assertTrue(\"weights\" in index_info)\n        db.test.drop_indexes()\n\n        self.client.admin.command('setParameter', '*',\n                                  textSearchEnabled=False)\n\n    def test_index_2dsphere(self):\n        if not version.at_least(self.client, (2, 3, 2)):\n            raise SkipTest(\"2dsphere indexing requires server >=2.3.2.\")\n\n        db = self.db\n        db.test.drop_indexes()\n        self.assertEqual(\"geo_2dsphere\",\n                         db.test.create_index([(\"geo\", GEOSPHERE)]))\n\n        poly = {\"type\": \"Polygon\",\n                \"coordinates\": [[[40,5], [40,6], [41,6], [41,5], [40,5]]]}\n        query = {\"geo\": {\"$within\": {\"$geometry\": poly}}}\n\n        self.assertTrue(\n            db.test.find(query).explain()['cursor'].startswith('S2Cursor'))\n\n        db.test.drop_indexes()\n\n    def test_index_hashed(self):\n        if not version.at_least(self.client, (2, 3, 2)):\n            raise SkipTest(\"hashed indexing requires server >=2.3.2.\")\n\n        db = self.db\n        db.test.drop_indexes()\n        self.assertEqual(\"a_hashed\",\n                         db.test.create_index([(\"a\", HASHED)]))\n\n        self.assertEqual(\"BtreeCursor a_hashed\",\n                db.test.find({'a': 1}).explain()['cursor'])\n        db.test.drop_indexes()\n\n    def test_index_sparse(self):\n        db = self.db\n        db.test.drop_indexes()\n        db.test.create_index([('key', ASCENDING)], sparse=True)\n        self.assertTrue(db.test.index_information()['key_1']['sparse'])\n\n    def test_index_background(self):\n        db = self.db\n        db.test.drop_indexes()\n        db.test.create_index([('keya', ASCENDING)])\n        db.test.create_index([('keyb', ASCENDING)], background=False)\n        db.test.create_index([('keyc', ASCENDING)], background=True)\n        self.assertFalse('background' in db.test.index_information()['keya_1'])\n        self.assertFalse(db.test.index_information()['keyb_1']['background'])\n        self.assertTrue(db.test.index_information()['keyc_1']['background'])\n\n    def _drop_dups_setup(self, db):\n        db.drop_collection('test')\n        db.test.insert({'i': 1})\n        db.test.insert({'i': 2})\n        db.test.insert({'i': 2})  # duplicate\n        db.test.insert({'i': 3})\n\n    def test_index_drop_dups(self):\n        # Try dropping duplicates\n        db = self.db\n        self._drop_dups_setup(db)\n\n        if version.at_least(db.connection, (1, 9, 2)):\n            # No error, just drop the duplicate\n            db.test.create_index(\n                [('i', ASCENDING)],\n                unique=True,\n                drop_dups=True\n            )\n        else:\n            # https://jira.mongodb.org/browse/SERVER-2054 \"Creating an index\n            # with dropDups shouldn't assert\". On Mongo < 1.9.2, the duplicate\n            # is dropped & the index created, but an error is thrown.\n            def test_create():\n                db.test.create_index(\n                    [('i', ASCENDING)],\n                    unique=True,\n                    drop_dups=True\n                )\n            self.assertRaises(DuplicateKeyError, test_create)\n\n        # Duplicate was dropped\n        self.assertEqual(3, db.test.count())\n\n        # Index was created, plus the index on _id\n        self.assertEqual(2, len(db.test.index_information()))\n\n    def test_index_dont_drop_dups(self):\n        # Try *not* dropping duplicates\n        db = self.db\n        self._drop_dups_setup(db)\n\n        # There's a duplicate\n        def test_create():\n            db.test.create_index(\n                [('i', ASCENDING)],\n                unique=True,\n                drop_dups=False\n            )\n        self.assertRaises(DuplicateKeyError, test_create)\n\n        # Duplicate wasn't dropped\n        self.assertEqual(4, db.test.count())\n\n        # Index wasn't created, only the default index on _id\n        self.assertEqual(1, len(db.test.index_information()))\n\n    def test_field_selection(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        doc = {\"a\": 1, \"b\": 5, \"c\": {\"d\": 5, \"e\": 10}}\n        db.test.insert(doc)\n\n        # Test field inclusion\n        doc = db.test.find({}, [\"_id\"]).next()\n        self.assertEqual(doc.keys(), [\"_id\"])\n        doc = db.test.find({}, [\"a\"]).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"a\"])\n        doc = db.test.find({}, [\"b\"]).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"b\"])\n        doc = db.test.find({}, [\"c\"]).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"c\"])\n        doc = db.test.find({}, [\"a\"]).next()\n        self.assertEqual(doc[\"a\"], 1)\n        doc = db.test.find({}, [\"b\"]).next()\n        self.assertEqual(doc[\"b\"], 5)\n        doc = db.test.find({}, [\"c\"]).next()\n        self.assertEqual(doc[\"c\"], {\"d\": 5, \"e\": 10})\n\n        # Test inclusion of fields with dots\n        doc = db.test.find({}, [\"c.d\"]).next()\n        self.assertEqual(doc[\"c\"], {\"d\": 5})\n        doc = db.test.find({}, [\"c.e\"]).next()\n        self.assertEqual(doc[\"c\"], {\"e\": 10})\n        doc = db.test.find({}, [\"b\", \"c.e\"]).next()\n        self.assertEqual(doc[\"c\"], {\"e\": 10})\n\n        doc = db.test.find({}, [\"b\", \"c.e\"]).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"b\", \"c\"])\n        doc = db.test.find({}, [\"b\", \"c.e\"]).next()\n        self.assertEqual(doc[\"b\"], 5)\n\n        # Test field exclusion\n        doc = db.test.find({}, {\"a\": False, \"b\": 0}).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"c\"])\n\n        doc = db.test.find({}, {\"_id\": False}).next()\n        l = doc.keys()\n        self.assertFalse(\"_id\" in l)\n\n    def test_options(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.save({})\n        self.assertEqual(db.test.options(), {})\n        self.assertEqual(db.test.doesnotexist.options(), {})\n\n        db.drop_collection(\"test\")\n        if version.at_least(db.connection, (1, 9)):\n            db.create_collection(\"test\", capped=True, size=1000)\n            self.assertEqual(db.test.options(), {\"capped\": True, 'size': 1000})\n        else:\n            db.create_collection(\"test\", capped=True)\n            self.assertEqual(db.test.options(), {\"capped\": True})\n        db.drop_collection(\"test\")\n\n    def test_insert_find_one(self):\n        db = self.db\n        db.test.remove({})\n        self.assertEqual(0, len(list(db.test.find())))\n        doc = {\"hello\": u\"world\"}\n        id = db.test.insert(doc)\n        self.assertEqual(1, len(list(db.test.find())))\n        self.assertEqual(doc, db.test.find_one())\n        self.assertEqual(doc[\"_id\"], id)\n        self.assertTrue(isinstance(id, ObjectId))\n\n        doc_class = None\n        # Work around http://bugs.jython.org/issue1728\n        if (sys.platform.startswith('java') and\n            sys.version_info[:3] >= (2, 5, 2)):\n            doc_class = SON\n\n        def remove_insert_find_one(doc):\n            db.test.remove({})\n            db.test.insert(doc)\n            # SON equality is order sensitive.\n            return db.test.find_one(as_class=doc_class) == doc.to_dict()\n\n        qcheck.check_unittest(self, remove_insert_find_one,\n                              qcheck.gen_mongo_dict(3))\n\n    def test_generator_insert(self):\n        db = self.db\n        db.test.remove({})\n        self.assertEqual(db.test.find().count(), 0)\n        db.test.insert(({'a': i} for i in xrange(5)), manipulate=False)\n        self.assertEqual(5, db.test.count())\n        db.test.remove({})\n\n    def test_remove_all(self):\n        self.db.test.remove()\n        self.assertEqual(0, self.db.test.count())\n\n        self.db.test.insert({\"x\": 1})\n        self.db.test.insert({\"y\": 1})\n        self.assertEqual(2, self.db.test.count())\n\n        self.db.test.remove()\n        self.assertEqual(0, self.db.test.count())\n\n    def test_find_w_fields(self):\n        db = self.db\n        db.test.remove({})\n\n        db.test.insert({\"x\": 1, \"mike\": \"awesome\",\n                        \"extra thing\": \"abcdefghijklmnopqrstuvwxyz\"})\n        self.assertEqual(1, db.test.count())\n        doc = db.test.find({}).next()\n        self.assertTrue(\"x\" in doc)\n        doc = db.test.find({}).next()\n        self.assertTrue(\"mike\" in doc)\n        doc = db.test.find({}).next()\n        self.assertTrue(\"extra thing\" in doc)\n        doc = db.test.find({}, [\"x\", \"mike\"]).next()\n        self.assertTrue(\"x\" in doc)\n        doc = db.test.find({}, [\"x\", \"mike\"]).next()\n        self.assertTrue(\"mike\" in doc)\n        doc = db.test.find({}, [\"x\", \"mike\"]).next()\n        self.assertFalse(\"extra thing\" in doc)\n        doc = db.test.find({}, [\"mike\"]).next()\n        self.assertFalse(\"x\" in doc)\n        doc = db.test.find({}, [\"mike\"]).next()\n        self.assertTrue(\"mike\" in doc)\n        doc = db.test.find({}, [\"mike\"]).next()\n        self.assertFalse(\"extra thing\" in doc)\n\n    def test_fields_specifier_as_dict(self):\n        db = self.db\n        db.test.remove({})\n\n        db.test.insert({\"x\": [1, 2, 3], \"mike\": \"awesome\"})\n\n        self.assertEqual([1, 2, 3], db.test.find_one()[\"x\"])\n        if version.at_least(db.connection, (1, 5, 1)):\n            self.assertEqual([2, 3],\n                             db.test.find_one(fields={\"x\": {\"$slice\":\n                                                            -2}})[\"x\"])\n        self.assertTrue(\"x\" not in db.test.find_one(fields={\"x\": 0}))\n        self.assertTrue(\"mike\" in db.test.find_one(fields={\"x\": 0}))\n\n    def test_find_w_regex(self):\n        db = self.db\n        db.test.remove({})\n\n        db.test.insert({\"x\": \"hello_world\"})\n        db.test.insert({\"x\": \"hello_mike\"})\n        db.test.insert({\"x\": \"hello_mikey\"})\n        db.test.insert({\"x\": \"hello_test\"})\n\n        self.assertEqual(db.test.find().count(), 4)\n        self.assertEqual(db.test.find({\"x\":\n                                       re.compile(\"^hello.*\")}).count(), 4)\n        self.assertEqual(db.test.find({\"x\":\n                                       re.compile(\"ello\")}).count(), 4)\n        self.assertEqual(db.test.find({\"x\":\n                                       re.compile(\"^hello$\")}).count(), 0)\n        self.assertEqual(db.test.find({\"x\":\n                                       re.compile(\"^hello_mi.*$\")}).count(), 2)\n\n    def test_id_can_be_anything(self):\n        db = self.db\n\n        db.test.remove({})\n        auto_id = {\"hello\": \"world\"}\n        db.test.insert(auto_id)\n        self.assertTrue(isinstance(auto_id[\"_id\"], ObjectId))\n\n        numeric = {\"_id\": 240, \"hello\": \"world\"}\n        db.test.insert(numeric)\n        self.assertEqual(numeric[\"_id\"], 240)\n\n        object = {\"_id\": numeric, \"hello\": \"world\"}\n        db.test.insert(object)\n        self.assertEqual(object[\"_id\"], numeric)\n\n        for x in db.test.find():\n            self.assertEqual(x[\"hello\"], u\"world\")\n            self.assertTrue(\"_id\" in x)\n\n    def test_iteration(self):\n        db = self.db\n\n        def iterate():\n            [a for a in db.test]\n\n        self.assertRaises(TypeError, iterate)\n\n    def test_invalid_key_names(self):\n        db = self.db\n        db.test.drop()\n\n        db.test.insert({\"hello\": \"world\"})\n        db.test.insert({\"hello\": {\"hello\": \"world\"}})\n\n        self.assertRaises(InvalidDocument, db.test.insert, {\"$hello\": \"world\"})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello\": {\"$hello\": \"world\"}})\n\n        db.test.insert({\"he$llo\": \"world\"})\n        db.test.insert({\"hello\": {\"hello$\": \"world\"}})\n\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\".hello\": \"world\"})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello\": {\".hello\": \"world\"}})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello.\": \"world\"})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello\": {\"hello.\": \"world\"}})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hel.lo\": \"world\"})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello\": {\"hel.lo\": \"world\"}})\n\n    def test_insert_multiple(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        doc1 = {\"hello\": u\"world\"}\n        doc2 = {\"hello\": u\"mike\"}\n        self.assertEqual(db.test.find().count(), 0)\n        ids = db.test.insert([doc1, doc2])\n        self.assertEqual(db.test.find().count(), 2)\n        self.assertEqual(doc1, db.test.find_one({\"hello\": u\"world\"}))\n        self.assertEqual(doc2, db.test.find_one({\"hello\": u\"mike\"}))\n\n        self.assertEqual(2, len(ids))\n        self.assertEqual(doc1[\"_id\"], ids[0])\n        self.assertEqual(doc2[\"_id\"], ids[1])\n\n        id = db.test.insert([{\"hello\": 1}])\n        self.assertTrue(isinstance(id, list))\n        self.assertEqual(1, len(id))\n\n        self.assertRaises(InvalidOperation, db.test.insert, [])\n\n    def test_insert_multiple_with_duplicate(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.ensure_index([('i', ASCENDING)], unique=True)\n\n        # No error\n        db.test.insert([{'i': i} for i in range(5, 10)], w=0)\n        db.test.remove()\n\n        # No error\n        db.test.insert([{'i': 1}] * 2, w=0)\n        self.assertEqual(1, db.test.count())\n\n        self.assertRaises(\n            DuplicateKeyError,\n            lambda: db.test.insert([{'i': 2}] * 2),\n        )\n\n        db.drop_collection(\"test\")\n        db.write_concern['w'] = 0\n        db.test.ensure_index([('i', ASCENDING)], unique=True)\n\n        # No error\n        db.test.insert([{'i': 1}] * 2)\n        self.assertEqual(1, db.test.count())\n\n        # Implied safe\n        self.assertRaises(\n            DuplicateKeyError,\n            lambda: db.test.insert([{'i': 2}] * 2, j=True),\n        )\n\n        # Explicit safe\n        self.assertRaises(\n            DuplicateKeyError,\n            lambda: db.test.insert([{'i': 2}] * 2, w=1),\n        )\n\n        # Misconfigured value for safe\n        self.assertRaises(\n            TypeError,\n            lambda: db.test.insert([{'i': 2}] * 2, safe=1),\n        )\n\n    def test_insert_iterables(self):\n        db = self.db\n\n        self.assertRaises(TypeError, db.test.insert, 4)\n        self.assertRaises(TypeError, db.test.insert, None)\n        self.assertRaises(TypeError, db.test.insert, True)\n\n        db.drop_collection(\"test\")\n        self.assertEqual(db.test.find().count(), 0)\n        ids = db.test.insert(({\"hello\": u\"world\"}, {\"hello\": u\"world\"}))\n        self.assertEqual(db.test.find().count(), 2)\n\n        db.drop_collection(\"test\")\n        self.assertEqual(db.test.find().count(), 0)\n        ids = db.test.insert(itertools.imap(lambda x: {\"hello\": \"world\"},\n                                            itertools.repeat(None, 10)))\n        self.assertEqual(db.test.find().count(), 10)\n\n    def test_save(self):\n        self.db.drop_collection(\"test\")\n\n        # Save a doc with autogenerated id\n        id = self.db.test.save({\"hello\": \"world\"})\n        self.assertEqual(self.db.test.find_one()[\"_id\"], id)\n        self.assertTrue(isinstance(id, ObjectId))\n\n        # Save a doc with explicit id\n        self.db.test.save({\"_id\": \"explicit_id\", \"hello\": \"bar\"})\n        doc = self.db.test.find_one({\"_id\": \"explicit_id\"})\n        self.assertEqual(doc['_id'], 'explicit_id')\n        self.assertEqual(doc['hello'], 'bar')\n\n        # Save docs with _id field already present (shouldn't create new docs)\n        self.assertEqual(2, self.db.test.count())\n        self.db.test.save({'_id': id, 'hello': 'world'})\n        self.assertEqual(2, self.db.test.count())\n        self.db.test.save({'_id': 'explicit_id', 'hello': 'baz'})\n        self.assertEqual(2, self.db.test.count())\n        self.assertEqual(\n            'baz',\n            self.db.test.find_one({'_id': 'explicit_id'})['hello']\n        )\n\n        # Safe mode\n        self.db.test.create_index(\"hello\", unique=True)\n        # No exception, even though we duplicate the first doc's \"hello\" value\n        self.db.test.save({'_id': 'explicit_id', 'hello': 'world'}, w=0)\n\n        self.assertRaises(\n            DuplicateKeyError,\n            self.db.test.save,\n            {'_id': 'explicit_id', 'hello': 'world'})\n\n    def test_save_with_invalid_key(self):\n        self.db.drop_collection(\"test\")\n        self.assertTrue(self.db.test.insert({\"hello\": \"world\"}))\n        doc = self.db.test.find_one()\n        doc['a.b'] = 'c'\n        self.assertRaises(InvalidDocument, self.db.test.save, doc)\n\n    def test_unique_index(self):\n        db = self.db\n\n        db.drop_collection(\"test\")\n        db.test.create_index(\"hello\")\n\n        db.test.save({\"hello\": \"world\"})\n        db.test.save({\"hello\": \"mike\"})\n        db.test.save({\"hello\": \"world\"})\n        self.assertFalse(db.error())\n\n        db.drop_collection(\"test\")\n        db.test.create_index(\"hello\", unique=True)\n\n        db.test.save({\"hello\": \"world\"})\n        db.test.save({\"hello\": \"mike\"})\n        db.test.save({\"hello\": \"world\"}, w=0)\n        self.assertTrue(db.error())\n\n    def test_duplicate_key_error(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.create_index(\"x\", unique=True)\n\n        db.test.insert({\"_id\": 1, \"x\": 1})\n        db.test.insert({\"_id\": 2, \"x\": 2})\n\n        # No error\n        db.test.insert({\"_id\": 1, \"x\": 1}, safe=False)\n        db.test.save({\"_id\": 1, \"x\": 1}, safe=False)\n        db.test.insert({\"_id\": 2, \"x\": 2}, safe=False)\n        db.test.save({\"_id\": 2, \"x\": 2}, safe=False)\n        db.test.insert({\"_id\": 1, \"x\": 1}, w=0)\n        db.test.save({\"_id\": 1, \"x\": 1}, w=0)\n        db.test.insert({\"_id\": 2, \"x\": 2}, w=0)\n        db.test.save({\"_id\": 2, \"x\": 2}, w=0)\n\n        # But all those statements didn't do anything\n        self.assertEqual(2, db.test.count())\n\n        expected_error = OperationFailure\n        if version.at_least(db.connection, (1, 3)):\n            expected_error = DuplicateKeyError\n\n        self.assertRaises(expected_error,\n                          db.test.insert, {\"_id\": 1})\n        self.assertRaises(expected_error,\n                          db.test.insert, {\"x\": 1})\n\n        self.assertRaises(expected_error,\n                          db.test.save, {\"x\": 2})\n        self.assertRaises(expected_error,\n                          db.test.update, {\"x\": 1},\n                          {\"$inc\": {\"x\": 1}})\n\n    def test_continue_on_error(self):\n        db = self.db\n        if not version.at_least(db.connection, (1, 9, 1)):\n            raise SkipTest(\"continue_on_error requires MongoDB >= 1.9.1\")\n\n        db.drop_collection(\"test\")\n        oid = db.test.insert({\"one\": 1})\n        self.assertEqual(1, db.test.count())\n\n        docs = []\n        docs.append({\"_id\": oid, \"two\": 2})\n        docs.append({\"three\": 3})\n        docs.append({\"four\": 4})\n        docs.append({\"five\": 5})\n\n        db.test.insert(docs, manipulate=False, w=0)\n        self.assertEqual(11000, db.error()['code'])\n        self.assertEqual(1, db.test.count())\n\n        db.test.insert(docs, manipulate=False, continue_on_error=True, w=0)\n        self.assertEqual(11000, db.error()['code'])\n        self.assertEqual(4, db.test.count())\n\n        db.drop_collection(\"test\")\n        oid = db.test.insert({\"_id\": oid, \"one\": 1}, w=0)\n        self.assertEqual(1, db.test.count())\n        docs[0].pop(\"_id\")\n        docs[2][\"_id\"] = oid\n\n        db.test.insert(docs, manipulate=False, w=0)\n        self.assertEqual(11000, db.error()['code'])\n        self.assertEqual(3, db.test.count())\n\n        db.test.insert(docs, manipulate=False, continue_on_error=True, w=0)\n        self.assertEqual(11000, db.error()['code'])\n        self.assertEqual(6, db.test.count())\n\n    def test_error_code(self):\n        try:\n            self.db.test.update({}, {\"$thismodifierdoesntexist\": 1})\n            self.fail()\n        except OperationFailure, e:\n            if version.at_least(self.db.connection, (1, 3)):\n                self.assertEqual(10147, e.code)\n\n    def test_index_on_subfield(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.insert({\"hello\": {\"a\": 4, \"b\": 5}})\n        db.test.insert({\"hello\": {\"a\": 7, \"b\": 2}})\n        db.test.insert({\"hello\": {\"a\": 4, \"b\": 10}})\n\n        db.drop_collection(\"test\")\n        db.test.create_index(\"hello.a\", unique=True)\n\n        db.test.insert({\"hello\": {\"a\": 4, \"b\": 5}})\n        db.test.insert({\"hello\": {\"a\": 7, \"b\": 2}})\n        self.assertRaises(DuplicateKeyError,\n            db.test.insert, {\"hello\": {\"a\": 4, \"b\": 10}})\n\n    def test_safe_insert(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        a = {\"hello\": \"world\"}\n        db.test.insert(a)\n        db.test.insert(a, w=0)\n        self.assertTrue(\"E11000\" in db.error()[\"err\"])\n\n        self.assertRaises(OperationFailure, db.test.insert, a)\n\n    def test_update(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        id1 = db.test.save({\"x\": 5})\n        db.test.update({}, {\"$inc\": {\"x\": 1}})\n        self.assertEqual(db.test.find_one(id1)[\"x\"], 6)\n\n        id2 = db.test.save({\"x\": 1})\n        db.test.update({\"x\": 6}, {\"$inc\": {\"x\": 1}})\n        self.assertEqual(db.test.find_one(id1)[\"x\"], 7)\n        self.assertEqual(db.test.find_one(id2)[\"x\"], 1)\n\n    def test_multi_update(self):\n        db = self.db\n        if not version.at_least(db.connection, (1, 1, 3, -1)):\n            raise SkipTest(\"multi-update requires MongoDB >= 1.1.3\")\n\n        db.drop_collection(\"test\")\n\n        db.test.save({\"x\": 4, \"y\": 3})\n        db.test.save({\"x\": 5, \"y\": 5})\n        db.test.save({\"x\": 4, \"y\": 4})\n\n        db.test.update({\"x\": 4}, {\"$set\": {\"y\": 5}}, multi=True)\n\n        self.assertEqual(3, db.test.count())\n        for doc in db.test.find():\n            self.assertEqual(5, doc[\"y\"])\n\n        self.assertEqual(2, db.test.update({\"x\": 4}, {\"$set\": {\"y\": 6}},\n                                           multi=True)[\"n\"])\n\n    def test_upsert(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.update({\"page\": \"/\"}, {\"$inc\": {\"count\": 1}}, upsert=True)\n        db.test.update({\"page\": \"/\"}, {\"$inc\": {\"count\": 1}}, upsert=True)\n\n        self.assertEqual(1, db.test.count())\n        self.assertEqual(2, db.test.find_one()[\"count\"])\n\n    def test_safe_update(self):\n        db = self.db\n        v113minus = version.at_least(db.connection, (1, 1, 3, -1))\n        v19 = version.at_least(db.connection, (1, 9))\n\n        db.drop_collection(\"test\")\n        db.test.create_index(\"x\", unique=True)\n\n        db.test.insert({\"x\": 5})\n        id = db.test.insert({\"x\": 4})\n\n        self.assertEqual(\n            None, db.test.update({\"_id\": id}, {\"$inc\": {\"x\": 1}}, w=0))\n\n        if v19:\n            self.assertTrue(\"E11000\" in db.error()[\"err\"])\n        elif v113minus:\n            self.assertTrue(db.error()[\"err\"].startswith(\"E11001\"))\n        else:\n            self.assertTrue(db.error()[\"err\"].startswith(\"E12011\"))\n\n        self.assertRaises(OperationFailure, db.test.update,\n                          {\"_id\": id}, {\"$inc\": {\"x\": 1}})\n\n        self.assertEqual(1, db.test.update({\"_id\": id},\n                                           {\"$inc\": {\"x\": 2}})[\"n\"])\n\n        self.assertEqual(0, db.test.update({\"_id\": \"foo\"},\n                                           {\"$inc\": {\"x\": 2}})[\"n\"])\n\n    def test_update_with_invalid_keys(self):\n        self.db.drop_collection(\"test\")\n        self.assertTrue(self.db.test.insert({\"hello\": \"world\"}))\n        doc = self.db.test.find_one()\n        doc['a.b'] = 'c'\n\n        # Replace\n        self.assertRaises(InvalidDocument,\n                          self.db.test.update, {\"hello\": \"world\"}, doc)\n        # Upsert\n        self.assertRaises(InvalidDocument,\n                          self.db.test.update, {\"foo\": \"bar\"}, doc, upsert=True)\n\n        # Check that the last two ops didn't actually modify anything\n        self.assertTrue('a.b' not in self.db.test.find_one())\n\n        # Modify shouldn't check keys...\n        self.assertTrue(self.db.test.update({\"hello\": \"world\"},\n                                            {\"$set\": {\"foo.bar\": \"baz\"}},\n                                            upsert=True))\n\n        # I know this seems like testing the server but I'd like to be notified\n        # by CI if the server's behavior changes here.\n        doc = SON([(\"$set\", {\"foo.bar\": \"bim\"}), (\"hello\", \"world\")])\n        self.assertRaises(OperationFailure, self.db.test.update,\n                          {\"hello\": \"world\"}, doc, upsert=True)\n\n        # This is going to cause keys to be checked and raise InvalidDocument.\n        # That's OK assuming the server's behavior in the previous assert\n        # doesn't change. If the behavior changes checking the first key for\n        # '$' in update won't be good enough anymore.\n        doc = SON([(\"hello\", \"world\"), (\"$set\", {\"foo.bar\": \"bim\"})])\n        self.assertRaises(InvalidDocument, self.db.test.update,\n                          {\"hello\": \"world\"}, doc, upsert=True)\n\n        # Replace with empty document\n        self.assertNotEqual(0, self.db.test.update({\"hello\": \"world\"},\n                            {})['n'])\n\n    def test_safe_save(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.create_index(\"hello\", unique=True)\n\n        db.test.save({\"hello\": \"world\"})\n        db.test.save({\"hello\": \"world\"}, w=0)\n        self.assertTrue(\"E11000\" in db.error()[\"err\"])\n\n        self.assertRaises(OperationFailure, db.test.save,\n                          {\"hello\": \"world\"})\n\n    def test_safe_remove(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.create_collection(\"test\", capped=True, size=1000)\n\n        db.test.insert({\"x\": 1})\n        self.assertEqual(1, db.test.count())\n\n        self.assertEqual(None, db.test.remove({\"x\": 1}, w=0))\n        self.assertEqual(1, db.test.count())\n\n        if version.at_least(db.connection, (1, 1, 3, -1)):\n            self.assertRaises(OperationFailure, db.test.remove,\n                              {\"x\": 1})\n        else:  # Just test that it doesn't blow up\n            db.test.remove({\"x\": 1})\n\n        db.drop_collection(\"test\")\n        db.test.insert({\"x\": 1})\n        db.test.insert({\"x\": 1})\n        self.assertEqual(2, db.test.remove({})[\"n\"])\n        self.assertEqual(0, db.test.remove({})[\"n\"])\n\n    def test_last_error_options(self):\n        if not version.at_least(self.client, (1, 5, 1)):\n            raise SkipTest(\"getLastError options require MongoDB >= 1.5.1\")\n\n        # XXX: Fix this if we ever have a replica set unittest env.\n        # mongo >=1.7.6 errors with 'norepl' when w=2+\n        # and we aren't replicated.\n        if not version.at_least(self.client, (1, 7, 6)):\n            self.assertRaises(TimeoutError, self.db.test.save,\n                              {\"x\": 1}, w=2, wtimeout=1)\n            self.assertRaises(TimeoutError, self.db.test.insert,\n                              {\"x\": 1}, w=2, wtimeout=1)\n            self.assertRaises(TimeoutError, self.db.test.update,\n                              {\"x\": 1}, {\"y\": 2}, w=2, wtimeout=1)\n            self.assertRaises(TimeoutError, self.db.test.remove,\n                              {\"x\": 1}, w=2, wtimeout=1)\n\n        self.db.test.save({\"x\": 1}, w=1, wtimeout=1)\n        self.db.test.insert({\"x\": 1}, w=1, wtimeout=1)\n        self.db.test.remove({\"x\": 1}, w=1, wtimeout=1)\n        self.db.test.update({\"x\": 1}, {\"y\": 2}, w=1, wtimeout=1)\n\n    def test_manual_last_error(self):\n        self.db.test.save({\"x\": 1}, w=0)\n        # XXX: Fix this if we ever have a replica set unittest env.\n        # mongo >=1.7.6 errors with 'norepl' when w=2+\n        # and we aren't replicated\n        if not version.at_least(self.client, (1, 7, 6)):\n            self.assertRaises(TimeoutError, self.db.command,\n                              \"getlasterror\", w=2, wtimeout=1)\n        self.db.command(\"getlasterror\", w=1, wtimeout=1)\n\n    def test_count(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        self.assertEqual(db.test.count(), 0)\n        db.test.save({})\n        db.test.save({})\n        self.assertEqual(db.test.count(), 2)\n        db.test.save({'foo': 'bar'})\n        db.test.save({'foo': 'baz'})\n        self.assertEqual(db.test.find({'foo': 'bar'}).count(), 1)\n        self.assertEqual(db.test.find({'foo': re.compile(r'ba.*')}).count(), 2)\n\n    def test_aggregate(self):\n        if not version.at_least(self.db.connection, (2, 1, 0)):\n            raise SkipTest(\"The aggregate command requires MongoDB >= 2.1.0\")\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.save({'foo': [1, 2]})\n\n        self.assertRaises(TypeError, db.test.aggregate, \"wow\")\n\n        pipeline = {\"$project\": {\"_id\": False, \"foo\": True}}\n        expected = {'ok': 1.0, 'result': [{'foo': [1, 2]}]}\n        self.assertEqual(expected, db.test.aggregate(pipeline))\n        self.assertEqual(expected, db.test.aggregate([pipeline]))\n        self.assertEqual(expected, db.test.aggregate((pipeline,)))\n\n    def test_group(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        def group_checker(args, expected):\n            eval = db.test.group(*args)\n            self.assertEqual(eval, expected)\n\n        self.assertEqual([],\n                         db.test.group([], {}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                       ))\n\n        db.test.save({\"a\": 2})\n        db.test.save({\"b\": 5})\n        db.test.save({\"a\": 1})\n\n        self.assertEqual([{\"count\": 3}],\n                         db.test.group([], {}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                      ))\n\n        self.assertEqual([{\"count\": 1}],\n                         db.test.group([], {\"a\": {\"$gt\": 1}}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                      ))\n\n        db.test.save({\"a\": 2, \"b\": 3})\n\n        self.assertEqual([{\"a\": 2, \"count\": 2},\n                          {\"a\": None, \"count\": 1},\n                          {\"a\": 1, \"count\": 1}],\n                         db.test.group([\"a\"], {}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                      ))\n\n        # modifying finalize\n        self.assertEqual([{\"a\": 2, \"count\": 3},\n                          {\"a\": None, \"count\": 2},\n                          {\"a\": 1, \"count\": 2}],\n                         db.test.group([\"a\"], {}, {\"count\": 0},\n                                       \"function (obj, prev) \"\n                                       \"{ prev.count++; }\",\n                                       \"function (obj) { obj.count++; }\"))\n\n        # returning finalize\n        self.assertEqual([2, 1, 1],\n                         db.test.group([\"a\"], {}, {\"count\": 0},\n                                       \"function (obj, prev) \"\n                                       \"{ prev.count++; }\",\n                                       \"function (obj) { return obj.count; }\"))\n\n        # keyf\n        self.assertEqual([2, 2],\n                         db.test.group(\"function (obj) { if (obj.a == 2) \"\n                                       \"{ return {a: true} }; \"\n                                       \"return {b: true}; }\", {}, {\"count\": 0},\n                                       \"function (obj, prev) \"\n                                       \"{ prev.count++; }\",\n                                       \"function (obj) { return obj.count; }\"))\n\n        # no key\n        self.assertEqual([{\"count\": 4}],\n                         db.test.group(None, {}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                      ))\n\n        self.assertRaises(OperationFailure, db.test.group,\n                          [], {}, {}, \"5 ++ 5\")\n\n    def test_group_with_scope(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.save({\"a\": 1})\n        db.test.save({\"b\": 1})\n\n        reduce_function = \"function (obj, prev) { prev.count += inc_value; }\"\n\n        self.assertEqual(2, db.test.group([], {}, {\"count\": 0},\n                                          Code(reduce_function,\n                                               {\"inc_value\": 1}))[0]['count'])\n        self.assertEqual(4, db.test.group([], {}, {\"count\": 0},\n                                          Code(reduce_function,\n                                               {\"inc_value\": 2}))[0]['count'])\n\n        self.assertEqual(1,\n                         db.test.group([], {}, {\"count\": 0},\n                                       Code(reduce_function,\n                                            {\"inc_value\": 0.5}))[0]['count'])\n\n        if version.at_least(db.connection, (1, 1)):\n            self.assertEqual(2, db.test.group([], {}, {\"count\": 0},\n                                              Code(reduce_function,\n                                                   {\"inc_value\": 1}),\n                                             )[0]['count'])\n\n            self.assertEqual(4, db.test.group([], {}, {\"count\": 0},\n                                              Code(reduce_function,\n                                                   {\"inc_value\": 2}),\n                                             )[0]['count'])\n\n            self.assertEqual(1, db.test.group([], {}, {\"count\": 0},\n                                              Code(reduce_function,\n                                                   {\"inc_value\": 0.5}),\n                                             )[0]['count'])\n\n    def test_large_limit(self):\n        db = self.db\n        db.drop_collection(\"test_large_limit\")\n        db.test_large_limit.create_index([('x', 1)])\n\n        for i in range(2000):\n            doc = {\"x\": i, \"y\": \"mongomongo\" * 1000}\n            db.test_large_limit.insert(doc)\n\n        # Wait for insert to complete; often mysteriously failing in Jenkins\n        st = time.time()\n        while (\n            len(list(db.test_large_limit.find())) < 2000\n            and time.time() - st < 30\n        ):\n            time.sleep(1)\n\n        self.assertEqual(2000, len(list(db.test_large_limit.find())))\n\n        i = 0\n        y = 0\n        for doc in db.test_large_limit.find(limit=1900).sort([('x', 1)]):\n            i += 1\n            y += doc[\"x\"]\n\n        self.assertEqual(1900, i)\n        self.assertEqual((1900 * 1899) / 2, y)\n\n    def test_find_kwargs(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        for i in range(10):\n            db.test.insert({\"x\": i})\n\n        self.assertEqual(10, db.test.count())\n\n        sum = 0\n        for x in db.test.find({}, skip=4, limit=2):\n            sum += x[\"x\"]\n\n        self.assertEqual(9, sum)\n\n    def test_rename(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.drop_collection(\"foo\")\n\n        self.assertRaises(TypeError, db.test.rename, 5)\n        self.assertRaises(InvalidName, db.test.rename, \"\")\n        self.assertRaises(InvalidName, db.test.rename, \"te$t\")\n        self.assertRaises(InvalidName, db.test.rename, \".test\")\n        self.assertRaises(InvalidName, db.test.rename, \"test.\")\n        self.assertRaises(InvalidName, db.test.rename, \"tes..t\")\n\n        self.assertEqual(0, db.test.count())\n        self.assertEqual(0, db.foo.count())\n\n        for i in range(10):\n            db.test.insert({\"x\": i})\n\n        self.assertEqual(10, db.test.count())\n\n        db.test.rename(\"foo\")\n\n        self.assertEqual(0, db.test.count())\n        self.assertEqual(10, db.foo.count())\n\n        x = 0\n        for doc in db.foo.find():\n            self.assertEqual(x, doc[\"x\"])\n            x += 1\n\n        db.test.insert({})\n        self.assertRaises(OperationFailure, db.foo.rename, \"test\")\n        db.foo.rename(\"test\", dropTarget=True)\n\n    # doesn't really test functionality, just that the option is set correctly\n    def test_snapshot(self):\n        db = self.db\n\n        self.assertRaises(TypeError, db.test.find, snapshot=5)\n\n        list(db.test.find(snapshot=True))\n        self.assertRaises(OperationFailure, list,\n                          db.test.find(snapshot=True).sort(\"foo\", 1))\n\n    def test_find_one(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        id = db.test.save({\"hello\": \"world\", \"foo\": \"bar\"})\n\n        self.assertEqual(\"world\", db.test.find_one()[\"hello\"])\n        self.assertEqual(db.test.find_one(id), db.test.find_one())\n        self.assertEqual(db.test.find_one(None), db.test.find_one())\n        self.assertEqual(db.test.find_one({}), db.test.find_one())\n        self.assertEqual(db.test.find_one({\"hello\": \"world\"}),\n                                          db.test.find_one())\n\n        self.assertTrue(\"hello\" in db.test.find_one(fields=[\"hello\"]))\n        self.assertTrue(\"hello\" not in db.test.find_one(fields=[\"foo\"]))\n        self.assertEqual([\"_id\"], db.test.find_one(fields=[]).keys())\n\n        self.assertEqual(None, db.test.find_one({\"hello\": \"foo\"}))\n        self.assertEqual(None, db.test.find_one(ObjectId()))\n\n    def test_find_one_non_objectid(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.save({\"_id\": 5})\n\n        self.assertTrue(db.test.find_one(5))\n        self.assertFalse(db.test.find_one(6))\n\n    def test_remove_non_objectid(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.save({\"_id\": 5})\n\n        self.assertEqual(1, db.test.count())\n        db.test.remove(5)\n        self.assertEqual(0, db.test.count())\n\n    def test_find_one_with_find_args(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.save({\"x\": 1})\n        db.test.save({\"x\": 2})\n        db.test.save({\"x\": 3})\n\n        self.assertEqual(1, db.test.find_one()[\"x\"])\n        self.assertEqual(2, db.test.find_one(skip=1, limit=2)[\"x\"])\n\n    def test_find_with_sort(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.save({\"x\": 2})\n        db.test.save({\"x\": 1})\n        db.test.save({\"x\": 3})\n\n        self.assertEqual(2, db.test.find_one()[\"x\"])\n        self.assertEqual(1, db.test.find_one(sort=[(\"x\", 1)])[\"x\"])\n        self.assertEqual(3, db.test.find_one(sort=[(\"x\", -1)])[\"x\"])\n\n        def to_list(foo):\n            return [bar[\"x\"] for bar in foo]\n\n        self.assertEqual([2, 1, 3], to_list(db.test.find()))\n        self.assertEqual([1, 2, 3], to_list(db.test.find(sort=[(\"x\", 1)])))\n        self.assertEqual([3, 2, 1], to_list(db.test.find(sort=[(\"x\", -1)])))\n\n        self.assertRaises(TypeError, db.test.find, sort=5)\n        self.assertRaises(TypeError, db.test.find, sort=\"hello\")\n        self.assertRaises(ValueError, db.test.find, sort=[\"hello\", 1])\n\n    def test_insert_adds_id(self):\n        doc = {\"hello\": \"world\"}\n        self.db.test.insert(doc)\n        self.assertTrue(\"_id\" in doc)\n\n        docs = [{\"hello\": \"world\"}, {\"hello\": \"world\"}]\n        self.db.test.insert(docs)\n        for doc in docs:\n            self.assertTrue(\"_id\" in doc)\n\n    def test_save_adds_id(self):\n        doc = {\"hello\": \"jesse\"}\n        self.db.test.save(doc)\n        self.assertTrue(\"_id\" in doc)\n\n    # TODO doesn't actually test functionality, just that it doesn't blow up\n    def test_cursor_timeout(self):\n        list(self.db.test.find(timeout=False))\n        list(self.db.test.find(timeout=True))\n\n    def test_distinct(self):\n        if not version.at_least(self.db.connection, (1, 1)):\n            raise SkipTest(\"distinct command requires MongoDB >= 1.1\")\n\n        self.db.drop_collection(\"test\")\n\n        test = self.db.test\n        test.save({\"a\": 1})\n        test.save({\"a\": 2})\n        test.save({\"a\": 2})\n        test.save({\"a\": 2})\n        test.save({\"a\": 3})\n\n        distinct = test.distinct(\"a\")\n        distinct.sort()\n\n        self.assertEqual([1, 2, 3], distinct)\n\n        distinct = test.find({'a': {'$gt': 1}}).distinct(\"a\")\n        distinct.sort()\n\n        self.assertEqual([2, 3], distinct)\n\n        self.db.drop_collection(\"test\")\n\n        test.save({\"a\": {\"b\": \"a\"}, \"c\": 12})\n        test.save({\"a\": {\"b\": \"b\"}, \"c\": 12})\n        test.save({\"a\": {\"b\": \"c\"}, \"c\": 12})\n        test.save({\"a\": {\"b\": \"c\"}, \"c\": 12})\n\n        distinct = test.distinct(\"a.b\")\n        distinct.sort()\n\n        self.assertEqual([\"a\", \"b\", \"c\"], distinct)\n\n    def test_query_on_query_field(self):\n        self.db.drop_collection(\"test\")\n        self.db.test.save({\"query\": \"foo\"})\n        self.db.test.save({\"bar\": \"foo\"})\n\n        self.assertEqual(1,\n                         self.db.test.find({\"query\": {\"$ne\": None}}).count())\n        self.assertEqual(1,\n                         len(list(self.db.test.find({\"query\": {\"$ne\": None}})))\n                        )\n\n    def test_min_query(self):\n        self.db.drop_collection(\"test\")\n        self.db.test.save({\"x\": 1})\n        self.db.test.save({\"x\": 2})\n        self.db.test.create_index(\"x\")\n\n        self.assertEqual(1, len(list(self.db.test.find({\"$min\": {\"x\": 2},\n                                                        \"$query\": {}}))))\n        self.assertEqual(2, self.db.test.find({\"$min\": {\"x\": 2},\n                                               \"$query\": {}})[0][\"x\"])\n\n    def test_insert_large_document(self):\n        max_size = self.db.connection.max_bson_size\n        half_size = int(max_size / 2)\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            self.assertEqual(max_size, 16777216)\n        self.assertRaises(InvalidDocument, self.db.test.insert,\n                          {\"foo\": \"x\" * max_size})\n        self.assertRaises(InvalidDocument, self.db.test.save,\n                          {\"foo\": \"x\" * max_size})\n        self.assertRaises(InvalidDocument, self.db.test.insert,\n                          [{\"x\": 1}, {\"foo\": \"x\" * max_size}])\n        self.db.test.insert([{\"foo\": \"x\" * half_size},\n                             {\"foo\": \"x\" * half_size}])\n\n        self.db.test.insert({\"bar\": \"x\"})\n        self.assertRaises(InvalidDocument, self.db.test.update,\n                          {\"bar\": \"x\"}, {\"bar\": \"x\" * (max_size - 14)})\n        self.db.test.update({\"bar\": \"x\"}, {\"bar\": \"x\" * (max_size - 15)})\n\n    def test_map_reduce(self):\n        if not version.at_least(self.db.connection, (1, 1, 1)):\n            raise SkipTest(\"mapReduce command requires MongoDB >= 1.1.1\")\n\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.insert({\"id\": 1, \"tags\": [\"dog\", \"cat\"]})\n        db.test.insert({\"id\": 2, \"tags\": [\"cat\"]})\n        db.test.insert({\"id\": 3, \"tags\": [\"mouse\", \"cat\", \"dog\"]})\n        db.test.insert({\"id\": 4, \"tags\": []})\n\n        map = Code(\"function () {\"\n                   \"  this.tags.forEach(function(z) {\"\n                   \"    emit(z, 1);\"\n                   \"  });\"\n                   \"}\")\n        reduce = Code(\"function (key, values) {\"\n                      \"  var total = 0;\"\n                      \"  for (var i = 0; i < values.length; i++) {\"\n                      \"    total += values[i];\"\n                      \"  }\"\n                      \"  return total;\"\n                      \"}\")\n        result = db.test.map_reduce(map, reduce, out='mrunittests')\n        self.assertEqual(3, result.find_one({\"_id\": \"cat\"})[\"value\"])\n        self.assertEqual(2, result.find_one({\"_id\": \"dog\"})[\"value\"])\n        self.assertEqual(1, result.find_one({\"_id\": \"mouse\"})[\"value\"])\n\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            db.test.insert({\"id\": 5, \"tags\": [\"hampster\"]})\n            result = db.test.map_reduce(map, reduce, out='mrunittests')\n            self.assertEqual(1, result.find_one({\"_id\": \"hampster\"})[\"value\"])\n            db.test.remove({\"id\": 5})\n\n            result = db.test.map_reduce(map, reduce,\n                                        out={'merge': 'mrunittests'})\n            self.assertEqual(3, result.find_one({\"_id\": \"cat\"})[\"value\"])\n            self.assertEqual(1, result.find_one({\"_id\": \"hampster\"})[\"value\"])\n\n            result = db.test.map_reduce(map, reduce,\n                                        out={'reduce': 'mrunittests'})\n\n            self.assertEqual(6, result.find_one({\"_id\": \"cat\"})[\"value\"])\n            self.assertEqual(4, result.find_one({\"_id\": \"dog\"})[\"value\"])\n            self.assertEqual(2, result.find_one({\"_id\": \"mouse\"})[\"value\"])\n            self.assertEqual(1, result.find_one({\"_id\": \"hampster\"})[\"value\"])\n\n            result = db.test.map_reduce(\n                map,\n                reduce,\n                out={'replace': 'mrunittests'}\n            )\n            self.assertEqual(3, result.find_one({\"_id\": \"cat\"})[\"value\"])\n            self.assertEqual(2, result.find_one({\"_id\": \"dog\"})[\"value\"])\n            self.assertEqual(1, result.find_one({\"_id\": \"mouse\"})[\"value\"])\n\n            if (is_mongos(self.db.connection)\n                and not version.at_least(self.db.connection, (2, 1, 2))):\n                pass\n            else:\n                result = db.test.map_reduce(map, reduce,\n                                            out=SON([('replace', 'mrunittests'),\n                                                     ('db', 'mrtestdb')\n                                                    ]))\n                self.assertEqual(3, result.find_one({\"_id\": \"cat\"})[\"value\"])\n                self.assertEqual(2, result.find_one({\"_id\": \"dog\"})[\"value\"])\n                self.assertEqual(1, result.find_one({\"_id\": \"mouse\"})[\"value\"])\n                self.client.drop_database('mrtestdb')\n\n        full_result = db.test.map_reduce(map, reduce,\n                                         out='mrunittests', full_response=True)\n        self.assertEqual(6, full_result[\"counts\"][\"emit\"])\n\n        result = db.test.map_reduce(map, reduce, out='mrunittests', limit=2)\n        self.assertEqual(2, result.find_one({\"_id\": \"cat\"})[\"value\"])\n        self.assertEqual(1, result.find_one({\"_id\": \"dog\"})[\"value\"])\n        self.assertEqual(None, result.find_one({\"_id\": \"mouse\"}))\n\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            result = db.test.map_reduce(map, reduce, out={'inline': 1})\n            self.assertTrue(isinstance(result, dict))\n            self.assertTrue('results' in result)\n            self.assertTrue(result['results'][1][\"_id\"] in (\"cat\",\n                                                            \"dog\",\n                                                            \"mouse\"))\n\n            result = db.test.inline_map_reduce(map, reduce)\n            self.assertTrue(isinstance(result, list))\n            self.assertEqual(3, len(result))\n            self.assertTrue(result[1][\"_id\"] in (\"cat\", \"dog\", \"mouse\"))\n\n            full_result = db.test.inline_map_reduce(map, reduce,\n                                                    full_response=True)\n            self.assertEqual(6, full_result[\"counts\"][\"emit\"])\n\n    def test_messages_with_unicode_collection_names(self):\n        db = self.db\n\n        db[u\"Employ\u00e9s\"].insert({\"x\": 1})\n        db[u\"Employ\u00e9s\"].update({\"x\": 1}, {\"x\": 2})\n        db[u\"Employ\u00e9s\"].remove({})\n        db[u\"Employ\u00e9s\"].find_one()\n        list(db[u\"Employ\u00e9s\"].find())\n\n    def test_drop_indexes_non_existant(self):\n        self.db.drop_collection(\"test\")\n        self.db.test.drop_indexes()\n\n    # This is really a bson test but easier to just reproduce it here...\n    # (Shame on me)\n    def test_bad_encode(self):\n        c = self.db.test\n        warnings.simplefilter(\"ignore\")\n        self.assertRaises(InvalidDocument, c.save, {\"x\": c})\n        warnings.simplefilter(\"default\")\n\n    def test_as_class(self):\n        c = self.db.test\n        c.drop()\n        c.insert({\"x\": 1})\n\n        doc = c.find().next()\n        self.assertTrue(isinstance(doc, dict))\n        doc = c.find().next()\n        self.assertFalse(isinstance(doc, SON))\n        doc = c.find(as_class=SON).next()\n        self.assertTrue(isinstance(doc, SON))\n\n        self.assertTrue(isinstance(c.find_one(), dict))\n        self.assertFalse(isinstance(c.find_one(), SON))\n        self.assertTrue(isinstance(c.find_one(as_class=SON), SON))\n\n        self.assertEqual(1, c.find_one(as_class=SON)[\"x\"])\n        doc = c.find(as_class=SON).next()\n        self.assertEqual(1, doc[\"x\"])\n\n    def test_find_and_modify(self):\n        c = self.db.test\n        c.drop()\n        c.insert({'_id': 1, 'i': 1})\n\n        # Test that we raise DuplicateKeyError when appropriate.\n        c.ensure_index('i', unique=True)\n        self.assertRaises(DuplicateKeyError,\n                          c.find_and_modify, query={'i': 1, 'j': 1},\n                          update={'$set': {'k': 1}}, upsert=True)\n        c.drop_indexes()\n\n        # Test correct findAndModify\n        self.assertEqual({'_id': 1, 'i': 1},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}}))\n        self.assertEqual({'_id': 1, 'i': 3},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           new=True))\n\n        self.assertEqual({'_id': 1, 'i': 3},\n                         c.find_and_modify({'_id': 1}, remove=True))\n\n        self.assertEqual(None, c.find_one({'_id': 1}))\n\n        self.assertEqual(None,\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}}))\n        # The return value changed in 2.1.2. See SERVER-6226.\n        if version.at_least(self.db.connection, (2, 1, 2)):\n            self.assertEqual(None, c.find_and_modify({'_id': 1},\n                                                     {'$inc': {'i': 1}},\n                                                     upsert=True))\n        else:\n            self.assertEqual({}, c.find_and_modify({'_id': 1},\n                                                   {'$inc': {'i': 1}},\n                                                   upsert=True))\n        self.assertEqual({'_id': 1, 'i': 2},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           upsert=True, new=True))\n\n        self.assertEqual({'_id': 1, 'i': 2},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           fields=['i']))\n        self.assertEqual({'_id': 1, 'i': 4},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           new=True, fields={'i': 1}))\n\n        # Test with full_response=True (version > 2.4.2)\n        result = c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           new=True, upsert=True, \n                                           full_response=True,\n                                           fields={'i': 1})\n        self.assertEqual({'_id': 1, 'i': 5}, result[\"value\"])\n        self.assertEqual(True, result[\"lastErrorObject\"][\"updatedExisting\"])\n        \n        result = c.find_and_modify({'_id': 2}, {'$inc': {'i': 1}},\n                                           new=True, upsert=True, \n                                           full_response=True,\n                                           fields={'i': 1})\n        self.assertEqual({'_id': 2, 'i': 1}, result[\"value\"])\n        self.assertEqual(False, result[\"lastErrorObject\"][\"updatedExisting\"])\n\n        class ExtendedDict(dict):\n            pass\n\n        result = c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                    new=True, fields={'i': 1})\n        self.assertFalse(isinstance(result, ExtendedDict))\n        result = c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                    new=True, fields={'i': 1},\n                                    as_class=ExtendedDict)\n        self.assertTrue(isinstance(result, ExtendedDict))\n\n    def test_find_and_modify_with_sort(self):\n        c = self.db.test\n        c.drop()\n        for j in xrange(5):\n            c.insert({'j': j, 'i': 0})\n\n        sort={'j': DESCENDING}\n        self.assertEqual(4, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort={'j': ASCENDING}\n        self.assertEqual(0, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort=[('j', DESCENDING)]\n        self.assertEqual(4, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort=[('j', ASCENDING)]\n        self.assertEqual(0, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort=SON([('j', DESCENDING)])\n        self.assertEqual(4, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort=SON([('j', ASCENDING)])\n        self.assertEqual(0, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        try:\n            from collections import OrderedDict\n            sort=OrderedDict([('j', DESCENDING)])\n            self.assertEqual(4, c.find_and_modify({},\n                                                  {'$inc': {'i': 1}},\n                                                  sort=sort)['j'])\n            sort=OrderedDict([('j', ASCENDING)])\n            self.assertEqual(0, c.find_and_modify({},\n                                                  {'$inc': {'i': 1}},\n                                                  sort=sort)['j'])\n        except ImportError:\n            pass\n        # Test that a standard dict with two keys is rejected.\n        sort={'j': DESCENDING, 'foo': DESCENDING}\n        self.assertRaises(TypeError, c.find_and_modify, {},\n                                                         {'$inc': {'i': 1}},\n                                                         sort=sort)\n\n    def test_find_with_nested(self):\n        if not version.at_least(self.db.connection, (2, 0, 0)):\n            raise SkipTest(\"nested $and and $or requires MongoDB >= 2.0\")\n        c = self.db.test\n        c.drop()\n        c.insert([{'i': i} for i in range(5)])  # [0, 1, 2, 3, 4]\n        self.assertEqual(\n            [2],\n            [i['i'] for i in c.find({\n                '$and': [\n                    {\n                        # This clause gives us [1,2,4]\n                        '$or': [\n                            {'i': {'$lte': 2}},\n                            {'i': {'$gt': 3}},\n                        ],\n                    },\n                    {\n                        # This clause gives us [2,3]\n                        '$or': [\n                            {'i': 2},\n                            {'i': 3},\n                        ]\n                    },\n                ]\n            })]\n        )\n\n        self.assertEqual(\n            [0, 1, 2],\n            [i['i'] for i in c.find({\n                '$or': [\n                    {\n                        # This clause gives us [2]\n                        '$and': [\n                            {'i': {'$gte': 2}},\n                            {'i': {'$lt': 3}},\n                        ],\n                    },\n                    {\n                        # This clause gives us [0,1]\n                        '$and': [\n                            {'i': {'$gt': -100}},\n                            {'i': {'$lt': 2}},\n                        ]\n                    },\n                ]\n            })]\n        )\n\n    def test_disabling_manipulators(self):\n\n        class IncByTwo(SONManipulator):\n            def transform_outgoing(self, son, collection):\n                if 'foo' in son:\n                    son['foo'] += 2\n                return son\n\n        db = self.client.pymongo_test\n        db.add_son_manipulator(IncByTwo())\n        c = db.test\n        c.drop()\n        c.insert({'foo': 0})\n        self.assertEqual(2, c.find_one()['foo'])\n        self.assertEqual(0, c.find_one(manipulate=False)['foo'])\n        self.assertEqual(2, c.find_one(manipulate=True)['foo'])\n        c.remove({})\n\n    def test_uuid_subtype(self):\n        if not have_uuid:\n            raise SkipTest(\"No uuid module\")\n\n        coll = self.client.pymongo_test.uuid\n        coll.drop()\n\n        def change_subtype(collection, subtype):\n            collection.uuid_subtype = subtype\n\n        # Test property\n        self.assertEqual(OLD_UUID_SUBTYPE, coll.uuid_subtype)\n        self.assertRaises(ConfigurationError, change_subtype, coll, 7)\n        self.assertRaises(ConfigurationError, change_subtype, coll, 2)\n\n        # Test basic query\n        uu = uuid.uuid4()\n        # Insert as binary subtype 3\n        coll.insert({'uu': uu})\n        self.assertEqual(uu, coll.find_one({'uu': uu})['uu'])\n        coll.uuid_subtype = UUID_SUBTYPE\n        self.assertEqual(UUID_SUBTYPE, coll.uuid_subtype)\n        self.assertEqual(None, coll.find_one({'uu': uu}))\n        self.assertEqual(uu, coll.find_one({'uu': UUIDLegacy(uu)})['uu'])\n\n        # Test Cursor.count\n        self.assertEqual(0, coll.find({'uu': uu}).count())\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        self.assertEqual(1, coll.find({'uu': uu}).count())\n\n        # Test remove\n        coll.uuid_subtype = UUID_SUBTYPE\n        coll.remove({'uu': uu})\n        self.assertEqual(1, coll.count())\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        coll.remove({'uu': uu})\n        self.assertEqual(0, coll.count())\n\n        # Test save\n        coll.insert({'_id': uu, 'i': 0})\n        self.assertEqual(1, coll.count())\n        self.assertEqual(1, coll.find({'_id': uu}).count())\n        self.assertEqual(0, coll.find_one({'_id': uu})['i'])\n        doc = coll.find_one({'_id': uu})\n        doc['i'] = 1\n        coll.save(doc)\n        self.assertEqual(1, coll.find_one({'_id': uu})['i'])\n\n        # Test update\n        coll.uuid_subtype = UUID_SUBTYPE\n        coll.update({'_id': uu}, {'$set': {'i': 2}})\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        self.assertEqual(1, coll.find_one({'_id': uu})['i'])\n        coll.update({'_id': uu}, {'$set': {'i': 2}})\n        self.assertEqual(2, coll.find_one({'_id': uu})['i'])\n\n        # Test Cursor.distinct\n        self.assertEqual([2], coll.find({'_id': uu}).distinct('i'))\n        coll.uuid_subtype = UUID_SUBTYPE\n        self.assertEqual([], coll.find({'_id': uu}).distinct('i'))\n\n        # Test find_and_modify\n        self.assertEqual(None, coll.find_and_modify({'_id': uu},\n                                                     {'$set': {'i': 5}}))\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        self.assertEqual(2, coll.find_and_modify({'_id': uu},\n                                                  {'$set': {'i': 5}})['i'])\n        self.assertEqual(5, coll.find_one({'_id': uu})['i'])\n\n        # Test command\n        db = self.client.pymongo_test\n        no_obj_error = \"No matching object found\"\n        result = db.command('findAndModify', 'uuid',\n                            allowable_errors=[no_obj_error],\n                            uuid_subtype=UUID_SUBTYPE,\n                            query={'_id': uu},\n                            update={'$set': {'i': 6}})\n        self.assertEqual(None, result.get('value'))\n        self.assertEqual(5, db.command('findAndModify', 'uuid',\n                                       update={'$set': {'i': 6}},\n                                       query={'_id': uu})['value']['i'])\n        self.assertEqual(6, db.command('findAndModify', 'uuid',\n                                       update={'$set': {'i': 7}},\n                                       query={'_id': UUIDLegacy(uu)}\n                                      )['value']['i'])\n\n        # Test (inline)_map_reduce\n        coll.drop()\n        coll.insert({\"_id\": uu, \"x\": 1, \"tags\": [\"dog\", \"cat\"]})\n        coll.insert({\"_id\": uuid.uuid4(), \"x\": 3,\n                     \"tags\": [\"mouse\", \"cat\", \"dog\"]})\n\n        map = Code(\"function () {\"\n                   \"  this.tags.forEach(function(z) {\"\n                   \"    emit(z, 1);\"\n                   \"  });\"\n                   \"}\")\n\n        reduce = Code(\"function (key, values) {\"\n                      \"  var total = 0;\"\n                      \"  for (var i = 0; i < values.length; i++) {\"\n                      \"    total += values[i];\"\n                      \"  }\"\n                      \"  return total;\"\n                      \"}\")\n\n        coll.uuid_subtype = UUID_SUBTYPE\n        q = {\"_id\": uu}\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            result = coll.inline_map_reduce(map, reduce, query=q)\n            self.assertEqual([], result)\n\n        result = coll.map_reduce(map, reduce, \"results\", query=q)\n        self.assertEqual(0, db.results.count())\n\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        q = {\"_id\": uu}\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            result = coll.inline_map_reduce(map, reduce, query=q)\n            self.assertEqual(2, len(result))\n\n        result = coll.map_reduce(map, reduce, \"results\", query=q)\n        self.assertEqual(2, db.results.count())\n\n        db.drop_collection(\"result\")\n        coll.drop()\n\n        # Test group\n        coll.insert({\"_id\": uu, \"a\": 2})\n        coll.insert({\"_id\": uuid.uuid4(), \"a\": 1})\n\n        reduce = \"function (obj, prev) { prev.count++; }\"\n        coll.uuid_subtype = UUID_SUBTYPE\n        self.assertEqual([],\n                         coll.group([], {\"_id\": uu},\n                                     {\"count\": 0}, reduce))\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        self.assertEqual([{\"count\": 1}],\n                         coll.group([], {\"_id\": uu},\n                                    {\"count\": 0}, reduce))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"], "fixing_code": ["# Copyright 2009-2012 10gen, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"BSON (Binary JSON) encoding and decoding.\n\"\"\"\n\nimport calendar\nimport datetime\nimport re\nimport struct\nimport sys\n\nfrom bson.binary import (Binary, OLD_UUID_SUBTYPE,\n                         JAVA_LEGACY, CSHARP_LEGACY)\nfrom bson.code import Code\nfrom bson.dbref import DBRef\nfrom bson.errors import (InvalidBSON,\n                         InvalidDocument,\n                         InvalidStringData)\nfrom bson.max_key import MaxKey\nfrom bson.min_key import MinKey\nfrom bson.objectid import ObjectId\nfrom bson.py3compat import b, binary_type\nfrom bson.son import SON, RE_TYPE\nfrom bson.timestamp import Timestamp\nfrom bson.tz_util import utc\n\n\ntry:\n    from bson import _cbson\n    _use_c = True\nexcept ImportError:\n    _use_c = False\n\ntry:\n    import uuid\n    _use_uuid = True\nexcept ImportError:\n    _use_uuid = False\n\nPY3 = sys.version_info[0] == 3\n\n\nMAX_INT32 = 2147483647\nMIN_INT32 = -2147483648\nMAX_INT64 = 9223372036854775807\nMIN_INT64 = -9223372036854775808\n\nEPOCH_AWARE = datetime.datetime.fromtimestamp(0, utc)\nEPOCH_NAIVE = datetime.datetime.utcfromtimestamp(0)\n\n# Create constants compatible with all versions of\n# python from 2.4 forward. In 2.x b(\"foo\") is just\n# \"foo\". In 3.x it becomes b\"foo\".\nEMPTY = b(\"\")\nZERO  = b(\"\\x00\")\nONE   = b(\"\\x01\")\n\nBSONNUM = b(\"\\x01\") # Floating point\nBSONSTR = b(\"\\x02\") # UTF-8 string\nBSONOBJ = b(\"\\x03\") # Embedded document\nBSONARR = b(\"\\x04\") # Array\nBSONBIN = b(\"\\x05\") # Binary\nBSONUND = b(\"\\x06\") # Undefined\nBSONOID = b(\"\\x07\") # ObjectId\nBSONBOO = b(\"\\x08\") # Boolean\nBSONDAT = b(\"\\x09\") # UTC Datetime\nBSONNUL = b(\"\\x0A\") # Null\nBSONRGX = b(\"\\x0B\") # Regex\nBSONREF = b(\"\\x0C\") # DBRef\nBSONCOD = b(\"\\x0D\") # Javascript code\nBSONSYM = b(\"\\x0E\") # Symbol\nBSONCWS = b(\"\\x0F\") # Javascript code with scope\nBSONINT = b(\"\\x10\") # 32bit int\nBSONTIM = b(\"\\x11\") # Timestamp\nBSONLON = b(\"\\x12\") # 64bit int\nBSONMIN = b(\"\\xFF\") # Min key\nBSONMAX = b(\"\\x7F\") # Max key\n\n\ndef _get_int(data, position, as_class=None,\n             tz_aware=False, uuid_subtype=OLD_UUID_SUBTYPE, unsigned=False):\n    format = unsigned and \"I\" or \"i\"\n    try:\n        value = struct.unpack(\"<%s\" % format, data[position:position + 4])[0]\n    except struct.error:\n        raise InvalidBSON()\n    position += 4\n    return value, position\n\n\ndef _get_c_string(data, position, length=None):\n    if length is None:\n        try:\n            end = data.index(ZERO, position)\n        except ValueError:\n            raise InvalidBSON()\n    else:\n        end = position + length\n    value = data[position:end].decode(\"utf-8\")\n    position = end + 1\n\n    return value, position\n\n\ndef _make_c_string(string, check_null=False):\n    if isinstance(string, unicode):\n        if check_null and \"\\x00\" in string:\n            raise InvalidDocument(\"BSON keys / regex patterns must not \"\n                                  \"contain a NULL character\")\n        return string.encode(\"utf-8\") + ZERO\n    else:\n        if check_null and ZERO in string:\n            raise InvalidDocument(\"BSON keys / regex patterns must not \"\n                                  \"contain a NULL character\")\n        try:\n            string.decode(\"utf-8\")\n            return string + ZERO\n        except UnicodeError:\n            raise InvalidStringData(\"strings in documents must be valid \"\n                                    \"UTF-8: %r\" % string)\n\n\ndef _get_number(data, position, as_class, tz_aware, uuid_subtype):\n    num = struct.unpack(\"<d\", data[position:position + 8])[0]\n    position += 8\n    return num, position\n\n\ndef _get_string(data, position, as_class, tz_aware, uuid_subtype):\n    length = struct.unpack(\"<i\", data[position:position + 4])[0] - 1\n    position += 4\n    return _get_c_string(data, position, length)\n\n\ndef _get_object(data, position, as_class, tz_aware, uuid_subtype):\n    obj_size = struct.unpack(\"<i\", data[position:position + 4])[0]\n    encoded = data[position + 4:position + obj_size - 1]\n    object = _elements_to_dict(encoded, as_class, tz_aware, uuid_subtype)\n    position += obj_size\n    if \"$ref\" in object:\n        return (DBRef(object.pop(\"$ref\"), object.pop(\"$id\", None),\n                      object.pop(\"$db\", None), object), position)\n    return object, position\n\n\ndef _get_array(data, position, as_class, tz_aware, uuid_subtype):\n    obj, position = _get_object(data, position,\n                                as_class, tz_aware, uuid_subtype)\n    result = []\n    i = 0\n    while True:\n        try:\n            result.append(obj[str(i)])\n            i += 1\n        except KeyError:\n            break\n    return result, position\n\n\ndef _get_binary(data, position, as_class, tz_aware, uuid_subtype):\n    length, position = _get_int(data, position)\n    subtype = ord(data[position:position + 1])\n    position += 1\n    if subtype == 2:\n        length2, position = _get_int(data, position)\n        if length2 != length - 4:\n            raise InvalidBSON(\"invalid binary (st 2) - lengths don't match!\")\n        length = length2\n    if subtype in (3, 4) and _use_uuid:\n        # Java Legacy\n        if uuid_subtype == JAVA_LEGACY:\n            java = data[position:position + length]\n            value = uuid.UUID(bytes=java[0:8][::-1] + java[8:16][::-1])\n        # C# legacy\n        elif uuid_subtype == CSHARP_LEGACY:\n            value = uuid.UUID(bytes_le=data[position:position + length])\n        # Python\n        else:\n            value = uuid.UUID(bytes=data[position:position + length])\n        position += length\n        return (value, position)\n    # Python3 special case. Decode subtype 0 to 'bytes'.\n    if PY3 and subtype == 0:\n        value = data[position:position + length]\n    else:\n        value = Binary(data[position:position + length], subtype)\n    position += length\n    return value, position\n\n\ndef _get_oid(data, position, as_class=None,\n             tz_aware=False, uuid_subtype=OLD_UUID_SUBTYPE):\n    value = ObjectId(data[position:position + 12])\n    position += 12\n    return value, position\n\n\ndef _get_boolean(data, position, as_class, tz_aware, uuid_subtype):\n    value = data[position:position + 1] == ONE\n    position += 1\n    return value, position\n\n\ndef _get_date(data, position, as_class, tz_aware, uuid_subtype):\n    millis = struct.unpack(\"<q\", data[position:position + 8])[0]\n    diff = millis % 1000\n    seconds = (millis - diff) / 1000\n    position += 8\n    if tz_aware:\n        dt = EPOCH_AWARE + datetime.timedelta(seconds=seconds)\n    else:\n        dt = EPOCH_NAIVE + datetime.timedelta(seconds=seconds)\n    return dt.replace(microsecond=diff * 1000), position\n\n\ndef _get_code(data, position, as_class, tz_aware, uuid_subtype):\n    code, position = _get_string(data, position,\n                                 as_class, tz_aware, uuid_subtype)\n    return Code(code), position\n\n\ndef _get_code_w_scope(data, position, as_class, tz_aware, uuid_subtype):\n    _, position = _get_int(data, position)\n    code, position = _get_string(data, position,\n                                 as_class, tz_aware, uuid_subtype)\n    scope, position = _get_object(data, position,\n                                  as_class, tz_aware, uuid_subtype)\n    return Code(code, scope), position\n\n\ndef _get_null(data, position, as_class, tz_aware, uuid_subtype):\n    return None, position\n\n\ndef _get_regex(data, position, as_class, tz_aware, uuid_subtype):\n    pattern, position = _get_c_string(data, position)\n    bson_flags, position = _get_c_string(data, position)\n    flags = 0\n    if \"i\" in bson_flags:\n        flags |= re.IGNORECASE\n    if \"l\" in bson_flags:\n        flags |= re.LOCALE\n    if \"m\" in bson_flags:\n        flags |= re.MULTILINE\n    if \"s\" in bson_flags:\n        flags |= re.DOTALL\n    if \"u\" in bson_flags:\n        flags |= re.UNICODE\n    if \"x\" in bson_flags:\n        flags |= re.VERBOSE\n    return re.compile(pattern, flags), position\n\n\ndef _get_ref(data, position, as_class, tz_aware, uuid_subtype):\n    position += 4\n    collection, position = _get_c_string(data, position)\n    oid, position = _get_oid(data, position)\n    return DBRef(collection, oid), position\n\n\ndef _get_timestamp(data, position, as_class, tz_aware, uuid_subtype):\n    inc, position = _get_int(data, position, unsigned=True)\n    timestamp, position = _get_int(data, position, unsigned=True)\n    return Timestamp(timestamp, inc), position\n\n\ndef _get_long(data, position, as_class, tz_aware, uuid_subtype):\n    # Have to cast to long; on 32-bit unpack may return an int.\n    # 2to3 will change long to int. That's fine since long doesn't\n    # exist in python3.\n    value = long(struct.unpack(\"<q\", data[position:position + 8])[0])\n    position += 8\n    return value, position\n\n\n_element_getter = {\n    BSONNUM: _get_number,\n    BSONSTR: _get_string,\n    BSONOBJ: _get_object,\n    BSONARR: _get_array,\n    BSONBIN: _get_binary,\n    BSONUND: _get_null,  # undefined\n    BSONOID: _get_oid,\n    BSONBOO: _get_boolean,\n    BSONDAT: _get_date,\n    BSONNUL: _get_null,\n    BSONRGX: _get_regex,\n    BSONREF: _get_ref,\n    BSONCOD: _get_code,  # code\n    BSONSYM: _get_string,  # symbol\n    BSONCWS: _get_code_w_scope,\n    BSONINT: _get_int,  # number_int\n    BSONTIM: _get_timestamp,\n    BSONLON: _get_long, # Same as _get_int after 2to3 runs.\n    BSONMIN: lambda v, w, x, y, z: (MinKey(), w),\n    BSONMAX: lambda v, w, x, y, z: (MaxKey(), w)}\n\n\ndef _element_to_dict(data, position, as_class, tz_aware, uuid_subtype):\n    element_type = data[position:position + 1]\n    position += 1\n    element_name, position = _get_c_string(data, position)\n    value, position = _element_getter[element_type](data, position, as_class,\n                                                    tz_aware, uuid_subtype)\n    return element_name, value, position\n\n\ndef _elements_to_dict(data, as_class, tz_aware, uuid_subtype):\n    result = as_class()\n    position = 0\n    end = len(data) - 1\n    while position < end:\n        (key, value, position) = _element_to_dict(data, position, as_class,\n                                                  tz_aware, uuid_subtype)\n        result[key] = value\n    return result\n\ndef _bson_to_dict(data, as_class, tz_aware, uuid_subtype):\n    obj_size = struct.unpack(\"<i\", data[:4])[0]\n    length = len(data)\n    if length < obj_size:\n        raise InvalidBSON(\"objsize too large\")\n    if obj_size != length or data[obj_size - 1:obj_size] != ZERO:\n        raise InvalidBSON(\"bad eoo\")\n    elements = data[4:obj_size - 1]\n    return (_elements_to_dict(elements, as_class,\n                              tz_aware, uuid_subtype), data[obj_size:])\nif _use_c:\n    _bson_to_dict = _cbson._bson_to_dict\n\n\ndef _element_to_bson(key, value, check_keys, uuid_subtype):\n    if not isinstance(key, basestring):\n        raise InvalidDocument(\"documents must have only string keys, \"\n                              \"key was %r\" % key)\n\n    if check_keys:\n        if key.startswith(\"$\"):\n            raise InvalidDocument(\"key %r must not start with '$'\" % key)\n        if \".\" in key:\n            raise InvalidDocument(\"key %r must not contain '.'\" % key)\n\n    name = _make_c_string(key, True)\n    if isinstance(value, float):\n        return BSONNUM + name + struct.pack(\"<d\", value)\n\n    if _use_uuid:\n        if isinstance(value, uuid.UUID):\n            # Java Legacy\n            if uuid_subtype == JAVA_LEGACY:\n                # Python 3.0(.1) returns a bytearray instance for bytes (3.1\n                # and newer just return a bytes instance). Convert that to\n                # binary_type (here and below) for compatibility.\n                from_uuid = binary_type(value.bytes)\n                as_legacy_java = from_uuid[0:8][::-1] + from_uuid[8:16][::-1]\n                value = Binary(as_legacy_java, subtype=OLD_UUID_SUBTYPE)\n            # C# legacy\n            elif uuid_subtype == CSHARP_LEGACY:\n                # Microsoft GUID representation.\n                value = Binary(binary_type(value.bytes_le),\n                               subtype=OLD_UUID_SUBTYPE)\n            # Python\n            else:\n                value = Binary(binary_type(value.bytes), subtype=uuid_subtype)\n\n    if isinstance(value, Binary):\n        subtype = value.subtype\n        if subtype == 2:\n            value = struct.pack(\"<i\", len(value)) + value\n        return (BSONBIN + name +\n                struct.pack(\"<i\", len(value)) + b(chr(subtype)) + value)\n    if isinstance(value, Code):\n        cstring = _make_c_string(value)\n        if not value.scope:\n            length = struct.pack(\"<i\", len(cstring))\n            return BSONCOD + name + length + cstring\n        scope = _dict_to_bson(value.scope, False, uuid_subtype, False)\n        full_length = struct.pack(\"<i\", 8 + len(cstring) + len(scope))\n        length = struct.pack(\"<i\", len(cstring))\n        return BSONCWS + name + full_length + length + cstring + scope\n    if isinstance(value, binary_type):\n        if PY3:\n            # Python3 special case. Store 'bytes' as BSON binary subtype 0.\n            return (BSONBIN + name +\n                    struct.pack(\"<i\", len(value)) + ZERO + value)\n        cstring = _make_c_string(value)\n        length = struct.pack(\"<i\", len(cstring))\n        return BSONSTR + name + length + cstring\n    if isinstance(value, unicode):\n        cstring = _make_c_string(value)\n        length = struct.pack(\"<i\", len(cstring))\n        return BSONSTR + name + length + cstring\n    if isinstance(value, dict):\n        return BSONOBJ + name + _dict_to_bson(value, check_keys, uuid_subtype, False)\n    if isinstance(value, (list, tuple)):\n        as_dict = SON(zip([str(i) for i in range(len(value))], value))\n        return BSONARR + name + _dict_to_bson(as_dict, check_keys, uuid_subtype, False)\n    if isinstance(value, ObjectId):\n        return BSONOID + name + value.binary\n    if value is True:\n        return BSONBOO + name + ONE\n    if value is False:\n        return BSONBOO + name + ZERO\n    if isinstance(value, int):\n        # TODO this is an ugly way to check for this...\n        if value > MAX_INT64 or value < MIN_INT64:\n            raise OverflowError(\"BSON can only handle up to 8-byte ints\")\n        if value > MAX_INT32 or value < MIN_INT32:\n            return BSONLON + name + struct.pack(\"<q\", value)\n        return BSONINT + name + struct.pack(\"<i\", value)\n    # 2to3 will convert long to int here since there is no long in python3.\n    # That's OK. The previous if block will match instead.\n    if isinstance(value, long):\n        if value > MAX_INT64 or value < MIN_INT64:\n            raise OverflowError(\"BSON can only handle up to 8-byte ints\")\n        return BSONLON + name + struct.pack(\"<q\", value)\n    if isinstance(value, datetime.datetime):\n        if value.utcoffset() is not None:\n            value = value - value.utcoffset()\n        millis = int(calendar.timegm(value.timetuple()) * 1000 +\n                     value.microsecond / 1000)\n        return BSONDAT + name + struct.pack(\"<q\", millis)\n    if isinstance(value, Timestamp):\n        time = struct.pack(\"<I\", value.time)\n        inc = struct.pack(\"<I\", value.inc)\n        return BSONTIM + name + inc + time\n    if value is None:\n        return BSONNUL + name\n    if isinstance(value, RE_TYPE):\n        pattern = value.pattern\n        flags = \"\"\n        if value.flags & re.IGNORECASE:\n            flags += \"i\"\n        if value.flags & re.LOCALE:\n            flags += \"l\"\n        if value.flags & re.MULTILINE:\n            flags += \"m\"\n        if value.flags & re.DOTALL:\n            flags += \"s\"\n        if value.flags & re.UNICODE:\n            flags += \"u\"\n        if value.flags & re.VERBOSE:\n            flags += \"x\"\n        return BSONRGX + name + _make_c_string(pattern, True) + \\\n            _make_c_string(flags)\n    if isinstance(value, DBRef):\n        return _element_to_bson(key, value.as_doc(), False, uuid_subtype)\n    if isinstance(value, MinKey):\n        return BSONMIN + name\n    if isinstance(value, MaxKey):\n        return BSONMAX + name\n\n    raise InvalidDocument(\"cannot convert value of type %s to bson\" %\n                          type(value))\n\n\ndef _dict_to_bson(dict, check_keys, uuid_subtype, top_level=True):\n    try:\n        elements = []\n        if top_level and \"_id\" in dict:\n            elements.append(_element_to_bson(\"_id\", dict[\"_id\"], False, uuid_subtype))\n        for (key, value) in dict.iteritems():\n            if not top_level or key != \"_id\":\n                elements.append(_element_to_bson(key, value, check_keys, uuid_subtype))\n    except AttributeError:\n        raise TypeError(\"encoder expected a mapping type but got: %r\" % dict)\n\n    encoded = EMPTY.join(elements)\n    length = len(encoded) + 5\n    return struct.pack(\"<i\", length) + encoded + ZERO\nif _use_c:\n    _dict_to_bson = _cbson._dict_to_bson\n\n\n\ndef decode_all(data, as_class=dict,\n               tz_aware=True, uuid_subtype=OLD_UUID_SUBTYPE):\n    \"\"\"Decode BSON data to multiple documents.\n\n    `data` must be a string of concatenated, valid, BSON-encoded\n    documents.\n\n    :Parameters:\n      - `data`: BSON data\n      - `as_class` (optional): the class to use for the resulting\n        documents\n      - `tz_aware` (optional): if ``True``, return timezone-aware\n        :class:`~datetime.datetime` instances\n\n    .. versionadded:: 1.9\n    \"\"\"\n    docs = []\n    position = 0\n    end = len(data) - 1\n    while position < end:\n        obj_size = struct.unpack(\"<i\", data[position:position + 4])[0]\n        if len(data) - position < obj_size:\n            raise InvalidBSON(\"objsize too large\")\n        if data[position + obj_size - 1:position + obj_size] != ZERO:\n            raise InvalidBSON(\"bad eoo\")\n        elements = data[position + 4:position + obj_size - 1]\n        position += obj_size\n        docs.append(_elements_to_dict(elements, as_class,\n                                      tz_aware, uuid_subtype))\n    return docs\nif _use_c:\n    decode_all = _cbson.decode_all\n\n\ndef is_valid(bson):\n    \"\"\"Check that the given string represents valid :class:`BSON` data.\n\n    Raises :class:`TypeError` if `bson` is not an instance of\n    :class:`str` (:class:`bytes` in python 3). Returns ``True``\n    if `bson` is valid :class:`BSON`, ``False`` otherwise.\n\n    :Parameters:\n      - `bson`: the data to be validated\n    \"\"\"\n    if not isinstance(bson, binary_type):\n        raise TypeError(\"BSON data must be an instance \"\n                        \"of a subclass of %s\" % (binary_type.__name__,))\n\n    try:\n        (_, remainder) = _bson_to_dict(bson, dict, True, OLD_UUID_SUBTYPE)\n        return remainder == EMPTY\n    except:\n        return False\n\n\nclass BSON(binary_type):\n    \"\"\"BSON (Binary JSON) data.\n    \"\"\"\n\n    @classmethod\n    def encode(cls, document, check_keys=False, uuid_subtype=OLD_UUID_SUBTYPE):\n        \"\"\"Encode a document to a new :class:`BSON` instance.\n\n        A document can be any mapping type (like :class:`dict`).\n\n        Raises :class:`TypeError` if `document` is not a mapping type,\n        or contains keys that are not instances of\n        :class:`basestring` (:class:`str` in python 3). Raises\n        :class:`~bson.errors.InvalidDocument` if `document` cannot be\n        converted to :class:`BSON`.\n\n        :Parameters:\n          - `document`: mapping type representing a document\n          - `check_keys` (optional): check if keys start with '$' or\n            contain '.', raising :class:`~bson.errors.InvalidDocument` in\n            either case\n\n        .. versionadded:: 1.9\n        \"\"\"\n        return cls(_dict_to_bson(document, check_keys, uuid_subtype))\n\n    def decode(self, as_class=dict,\n               tz_aware=False, uuid_subtype=OLD_UUID_SUBTYPE):\n        \"\"\"Decode this BSON data.\n\n        The default type to use for the resultant document is\n        :class:`dict`. Any other class that supports\n        :meth:`__setitem__` can be used instead by passing it as the\n        `as_class` parameter.\n\n        If `tz_aware` is ``True`` (recommended), any\n        :class:`~datetime.datetime` instances returned will be\n        timezone-aware, with their timezone set to\n        :attr:`bson.tz_util.utc`. Otherwise (default), all\n        :class:`~datetime.datetime` instances will be naive (but\n        contain UTC).\n\n        :Parameters:\n          - `as_class` (optional): the class to use for the resulting\n            document\n          - `tz_aware` (optional): if ``True``, return timezone-aware\n            :class:`~datetime.datetime` instances\n\n        .. versionadded:: 1.9\n        \"\"\"\n        (document, _) = _bson_to_dict(self, as_class, tz_aware, uuid_subtype)\n        return document\n\n\ndef has_c():\n    \"\"\"Is the C extension installed?\n\n    .. versionadded:: 1.9\n    \"\"\"\n    return _use_c\n\n\ndef has_uuid():\n    \"\"\"Is the uuid module available?\n\n    .. versionadded:: 2.3\n    \"\"\"\n    return _use_uuid\n", "/*\n * Copyright 2009-2012 10gen, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n/*\n * This file contains C implementations of some of the functions\n * needed by the bson module. If possible, these implementations\n * should be used to speed up BSON encoding and decoding.\n */\n\n#include \"Python.h\"\n#include \"datetime.h\"\n\n#include \"buffer.h\"\n#include \"time64.h\"\n#include \"encoding_helpers.h\"\n\n#define _CBSON_MODULE\n#include \"_cbsonmodule.h\"\n\n/* New module state and initialization code.\n * See the module-initialization-and-state\n * section in the following doc:\n * http://docs.python.org/release/3.1.3/howto/cporting.html\n * which references the following pep:\n * http://www.python.org/dev/peps/pep-3121/\n * */\nstruct module_state {\n    PyObject* Binary;\n    PyObject* Code;\n    PyObject* ObjectId;\n    PyObject* DBRef;\n    PyObject* RECompile;\n    PyObject* UUID;\n    PyObject* Timestamp;\n    PyObject* MinKey;\n    PyObject* MaxKey;\n    PyObject* UTC;\n    PyTypeObject* REType;\n};\n\n#if PY_MAJOR_VERSION >= 3\n#define GETSTATE(m) ((struct module_state*)PyModule_GetState(m))\n#else\n#define GETSTATE(m) (&_state)\nstatic struct module_state _state;\n#endif\n\n#if PY_VERSION_HEX < 0x02050000\n#define WARN(category, message)                 \\\n    PyErr_Warn((category), (message))\n#else\n#define WARN(category, message)                 \\\n    PyErr_WarnEx((category), (message), 1)\n#endif\n\n/* Maximum number of regex flags */\n#define FLAGS_SIZE 7\n\n#if defined(WIN32) || defined(_MSC_VER)\n/* This macro is basically an implementation of asprintf for win32\n * We get the length of the int as string and malloc a buffer for it,\n * returning -1 if that malloc fails. We then actually print to the\n * buffer to get the string value as an int. Like asprintf, the result\n * must be explicitly free'd when done being used.\n */\n#if defined(_MSC_VER) && (_MSC_VER >= 1400)\n#define INT2STRING(buffer, i)                                           \\\n    *(buffer) = malloc(_scprintf(\"%d\", (i)) + 1),                       \\\n        (!(buffer) ?                                                    \\\n         -1 :                                                           \\\n         _snprintf_s(*(buffer),                                         \\\n                     _scprintf(\"%d\", (i)) + 1,                          \\\n                     _scprintf(\"%d\", (i)) + 1,                          \\\n                     \"%d\",                                              \\\n                     (i)))\n#define STRCAT(dest, n, src) strcat_s((dest), (n), (src))\n#else\n#define INT2STRING(buffer, i)                                           \\\n    *(buffer) = malloc(_scprintf(\"%d\", (i)) + 1),                       \\\n        (!(buffer) ?                                                    \\\n         -1 :                                                           \\\n         _snprintf(*(buffer),                                           \\\n                     _scprintf(\"%d\", (i)) + 1,                          \\\n                     \"%d\",                                              \\\n                     (i)))\n#define STRCAT(dest, n, src) strcat((dest), (src))\n#endif\n#else\n#define INT2STRING(buffer, i) asprintf((buffer), \"%d\", (i))\n#define STRCAT(dest, n, src) strcat((dest), (src))\n#endif\n\n#define JAVA_LEGACY   5\n#define CSHARP_LEGACY 6\n\n\nstatic PyObject* elements_to_dict(PyObject* self, const char* string, int max,\n                                  PyObject* as_class, unsigned char tz_aware,\n                                  unsigned char uuid_subtype);\n\nstatic int _write_element_to_buffer(PyObject* self, buffer_t buffer, int type_byte,\n                                    PyObject* value, unsigned char check_keys,\n                                    unsigned char uuid_subtype, unsigned char first_attempt);\n\n/* Date stuff */\nstatic PyObject* datetime_from_millis(long long millis) {\n    /* To encode a datetime instance like datetime(9999, 12, 31, 23, 59, 59, 999999)\n     * we follow these steps:\n     * 1. Calculate a timestamp in seconds:       253402300799\n     * 2. Multiply that by 1000:                  253402300799000\n     * 3. Add in microseconds divided by 1000     253402300799999\n     *\n     * (Note: BSON doesn't support microsecond accuracy, hence the rounding.)\n     *\n     * To decode we could do:\n     * 1. Get seconds: timestamp / 1000:          253402300799\n     * 2. Get micros: (timestamp % 1000) * 1000:  999000\n     * Resulting in datetime(9999, 12, 31, 23, 59, 59, 999000) -- the expected result\n     *\n     * Now what if the we encode (1, 1, 1, 1, 1, 1, 111111)?\n     * 1. and 2. gives:                           -62135593139000\n     * 3. Gives us:                               -62135593138889\n     *\n     * Now decode:\n     * 1. Gives us:                               -62135593138\n     * 2. Gives us:                               -889000\n     * Resulting in datetime(1, 1, 1, 1, 1, 2, 15888216) -- an invalid result\n     *\n     * If instead to decode we do:\n     * diff = ((millis % 1000) + 1000) % 1000:    111\n     * seconds = (millis - diff) / 1000:          -62135593139\n     * micros = diff * 1000                       111000\n     * Resulting in datetime(1, 1, 1, 1, 1, 1, 111000) -- the expected result\n     */\n    int diff = (int)(((millis % 1000) + 1000) % 1000);\n    int microseconds = diff * 1000;\n    Time64_T seconds = (millis - diff) / 1000;\n    struct TM timeinfo;\n    gmtime64_r(&seconds, &timeinfo);\n\n    return PyDateTime_FromDateAndTime(timeinfo.tm_year + 1900,\n                                      timeinfo.tm_mon + 1,\n                                      timeinfo.tm_mday,\n                                      timeinfo.tm_hour,\n                                      timeinfo.tm_min,\n                                      timeinfo.tm_sec,\n                                      microseconds);\n}\n\nstatic long long millis_from_datetime(PyObject* datetime) {\n    struct TM timeinfo;\n    long long millis;\n\n    timeinfo.tm_year = PyDateTime_GET_YEAR(datetime) - 1900;\n    timeinfo.tm_mon = PyDateTime_GET_MONTH(datetime) - 1;\n    timeinfo.tm_mday = PyDateTime_GET_DAY(datetime);\n    timeinfo.tm_hour = PyDateTime_DATE_GET_HOUR(datetime);\n    timeinfo.tm_min = PyDateTime_DATE_GET_MINUTE(datetime);\n    timeinfo.tm_sec = PyDateTime_DATE_GET_SECOND(datetime);\n\n    millis = timegm64(&timeinfo) * 1000;\n    millis += PyDateTime_DATE_GET_MICROSECOND(datetime) / 1000;\n    return millis;\n}\n\n/* Just make this compatible w/ the old API. */\nint buffer_write_bytes(buffer_t buffer, const char* data, int size) {\n    if (buffer_write(buffer, data, size)) {\n        PyErr_NoMemory();\n        return 0;\n    }\n    return 1;\n}\n\n#if PY_MAJOR_VERSION >= 3\nstatic int write_unicode(buffer_t buffer, PyObject* py_string) {\n    Py_ssize_t string_length;\n    const char* string;\n    PyObject* encoded = PyUnicode_AsUTF8String(py_string);\n    if (!encoded) {\n        return 0;\n    }\n    string = PyBytes_AsString(encoded);\n    if (!string) {\n        Py_DECREF(encoded);\n        return 0;\n    }\n    string_length = PyBytes_Size(encoded) + 1;\n    if (!buffer_write_bytes(buffer, (const char*)&string_length, 4)) {\n        Py_DECREF(encoded);\n        return 0;\n    }\n    if (!buffer_write_bytes(buffer, string, string_length)) {\n        Py_DECREF(encoded);\n        return 0;\n    }\n    Py_DECREF(encoded);\n    return 1;\n}\n#endif\n\n/* returns 0 on failure */\nstatic int write_string(buffer_t buffer, PyObject* py_string) {\n    Py_ssize_t string_length;\n    const char* string;\n#if PY_MAJOR_VERSION >= 3\n    if (PyUnicode_Check(py_string)){\n        return write_unicode(buffer, py_string);\n    }\n    string = PyBytes_AsString(py_string);\n#else\n    string = PyString_AsString(py_string);\n#endif\n    if (!string) {\n        return 0;\n    }\n\n#if PY_MAJOR_VERSION >= 3\n    string_length = PyBytes_Size(py_string) + 1;\n#else\n    string_length = PyString_Size(py_string) + 1;\n#endif\n\n    if (!buffer_write_bytes(buffer, (const char*)&string_length, 4)) {\n        return 0;\n    }\n    if (!buffer_write_bytes(buffer, string, string_length)) {\n        return 0;\n    }\n    return 1;\n}\n\n/* Get an error class from the bson.errors module.\n *\n * Returns a new ref */\nstatic PyObject* _error(char* name) {\n    PyObject* error;\n    PyObject* errors = PyImport_ImportModule(\"bson.errors\");\n    if (!errors) {\n        return NULL;\n    }\n    error = PyObject_GetAttrString(errors, name);\n    Py_DECREF(errors);\n    return error;\n}\n\n/* Reload a cached Python object.\n *\n * Returns non-zero on failure. */\nstatic int _reload_object(PyObject** object, char* module_name, char* object_name) {\n    PyObject* module;\n\n    module = PyImport_ImportModule(module_name);\n    if (!module) {\n        return 1;\n    }\n\n    *object = PyObject_GetAttrString(module, object_name);\n    Py_DECREF(module);\n\n    return (*object) ? 0 : 2;\n}\n\n/* Reload all cached Python objects.\n *\n * Returns non-zero on failure. */\nstatic int _reload_python_objects(PyObject* module) {\n    struct module_state *state = GETSTATE(module);\n\n    if (_reload_object(&state->Binary, \"bson.binary\", \"Binary\") ||\n        _reload_object(&state->Code, \"bson.code\", \"Code\") ||\n        _reload_object(&state->ObjectId, \"bson.objectid\", \"ObjectId\") ||\n        _reload_object(&state->DBRef, \"bson.dbref\", \"DBRef\") ||\n        _reload_object(&state->Timestamp, \"bson.timestamp\", \"Timestamp\") ||\n        _reload_object(&state->MinKey, \"bson.min_key\", \"MinKey\") ||\n        _reload_object(&state->MaxKey, \"bson.max_key\", \"MaxKey\") ||\n        _reload_object(&state->UTC, \"bson.tz_util\", \"utc\") ||\n        _reload_object(&state->RECompile, \"re\", \"compile\")) {\n        return 1;\n    }\n    /* If we couldn't import uuid then we must be on 2.4. Just ignore. */\n    if (_reload_object(&state->UUID, \"uuid\", \"UUID\") == 1) {\n        state->UUID = NULL;\n        PyErr_Clear();\n    }\n    /* Reload our REType hack too. */\n    state->REType = PyObject_CallFunction(state->RECompile, \"O\",\n#if PY_MAJOR_VERSION >= 3\n                                   PyBytes_FromString(\"\"))->ob_type;\n#else\n                                   PyString_FromString(\"\"))->ob_type;\n#endif\n    return 0;\n}\n\nstatic int write_element_to_buffer(PyObject* self, buffer_t buffer, int type_byte,\n                                   PyObject* value, unsigned char check_keys,\n                                   unsigned char uuid_subtype,\n                                   unsigned char first_attempt) {\n    int result;\n    if(Py_EnterRecursiveCall(\" while encoding an object to BSON \"))\n        return 0;\n    result = _write_element_to_buffer(self, buffer, type_byte, value,\n                                      check_keys, uuid_subtype, first_attempt);\n    Py_LeaveRecursiveCall();\n    return result;\n}\n\nstatic int _fix_java(const char* in, char* out) {\n    int i, j;\n    for (i = 0, j = 7; i < j; i++, j--) {\n        out[i] = in[j];\n        out[j] = in[i];\n    }\n    for (i = 8, j = 15; i < j; i++, j--) {\n        out[i] = in[j];\n        out[j] = in[i];\n    }\n    return 0;\n}\n\n/* TODO our platform better be little-endian w/ 4-byte ints! */\n/* Write a single value to the buffer (also write it's type_byte, for which\n * space has already been reserved.\n *\n * returns 0 on failure */\nstatic int _write_element_to_buffer(PyObject* self, buffer_t buffer, int type_byte,\n                                    PyObject* value, unsigned char check_keys,\n                                    unsigned char uuid_subtype, unsigned char first_attempt) {\n    struct module_state *state = GETSTATE(self);\n\n    if (PyBool_Check(value)) {\n#if PY_MAJOR_VERSION >= 3\n        const long bool = PyLong_AsLong(value);\n#else\n        const long bool = PyInt_AsLong(value);\n#endif\n        const char c = bool ? 0x01 : 0x00;\n        *(buffer_get_buffer(buffer) + type_byte) = 0x08;\n        return buffer_write_bytes(buffer, &c, 1);\n    }\n#if PY_MAJOR_VERSION >= 3\n    else if (PyLong_Check(value)) {\n        const long long_value = PyLong_AsLong(value);\n#else\n    else if (PyInt_Check(value)) {\n        const long long_value = PyInt_AsLong(value);\n#endif\n\n        const int int_value = (int)long_value;\n        if (PyErr_Occurred() || long_value != int_value) { /* Overflow */\n            long long long_long_value;\n            PyErr_Clear();\n            long_long_value = PyLong_AsLongLong(value);\n            if (PyErr_Occurred()) { /* Overflow AGAIN */\n                PyErr_SetString(PyExc_OverflowError,\n                                \"MongoDB can only handle up to 8-byte ints\");\n                return 0;\n            }\n            *(buffer_get_buffer(buffer) + type_byte) = 0x12;\n            return buffer_write_bytes(buffer, (const char*)&long_long_value, 8);\n        }\n        *(buffer_get_buffer(buffer) + type_byte) = 0x10;\n        return buffer_write_bytes(buffer, (const char*)&int_value, 4);\n#if PY_MAJOR_VERSION < 3\n    } else if (PyLong_Check(value)) {\n        const long long long_long_value = PyLong_AsLongLong(value);\n        if (PyErr_Occurred()) { /* Overflow */\n            PyErr_SetString(PyExc_OverflowError,\n                            \"MongoDB can only handle up to 8-byte ints\");\n            return 0;\n        }\n        *(buffer_get_buffer(buffer) + type_byte) = 0x12;\n        return buffer_write_bytes(buffer, (const char*)&long_long_value, 8);\n#endif\n    } else if (PyFloat_Check(value)) {\n        const double d = PyFloat_AsDouble(value);\n        *(buffer_get_buffer(buffer) + type_byte) = 0x01;\n        return buffer_write_bytes(buffer, (const char*)&d, 8);\n    } else if (value == Py_None) {\n        *(buffer_get_buffer(buffer) + type_byte) = 0x0A;\n        return 1;\n    } else if (PyDict_Check(value)) {\n        *(buffer_get_buffer(buffer) + type_byte) = 0x03;\n        return write_dict(self, buffer, value, check_keys, uuid_subtype, 0);\n    } else if (PyList_Check(value) || PyTuple_Check(value)) {\n        int start_position,\n            length_location,\n            items,\n            length,\n            i;\n        char zero = 0;\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x04;\n        start_position = buffer_get_position(buffer);\n\n        /* save space for length */\n        length_location = buffer_save_space(buffer, 4);\n        if (length_location == -1) {\n            PyErr_NoMemory();\n            return 0;\n        }\n\n        items = PySequence_Size(value);\n        for(i = 0; i < items; i++) {\n            int list_type_byte = buffer_save_space(buffer, 1);\n            char* name;\n            PyObject* item_value;\n\n            if (list_type_byte == -1) {\n                PyErr_NoMemory();\n                return 0;\n            }\n            if (INT2STRING(&name, i) < 0 || !name) {\n                PyErr_NoMemory();\n                return 0;\n            }\n            if (!buffer_write_bytes(buffer, name, strlen(name) + 1)) {\n                free(name);\n                return 0;\n            }\n            free(name);\n\n            item_value = PySequence_GetItem(value, i);\n            if (!write_element_to_buffer(self, buffer, list_type_byte,\n                                         item_value, check_keys, uuid_subtype, 1)) {\n                Py_DECREF(item_value);\n                return 0;\n            }\n            Py_DECREF(item_value);\n        }\n\n        /* write null byte and fill in length */\n        if (!buffer_write_bytes(buffer, &zero, 1)) {\n            return 0;\n        }\n        length = buffer_get_position(buffer) - start_position;\n        memcpy(buffer_get_buffer(buffer) + length_location, &length, 4);\n        return 1;\n    } else if (PyObject_IsInstance(value, state->Binary)) {\n        PyObject* subtype_object;\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x05;\n        subtype_object = PyObject_GetAttrString(value, \"subtype\");\n        if (!subtype_object) {\n            return 0;\n        }\n        {\n#if PY_MAJOR_VERSION >= 3\n            const long long_subtype = PyLong_AsLong(subtype_object);\n            const char subtype = (const char)long_subtype;\n            const int length = PyBytes_Size(value);\n#else\n            const long long_subtype = PyInt_AsLong(subtype_object);\n            const char subtype = (const char)long_subtype;\n            const int length = PyString_Size(value);\n#endif\n\n            Py_DECREF(subtype_object);\n            if (subtype == 2) {\n                const int other_length = length + 4;\n                if (!buffer_write_bytes(buffer, (const char*)&other_length, 4)) {\n                    return 0;\n                }\n                if (!buffer_write_bytes(buffer, &subtype, 1)) {\n                    return 0;\n                }\n            }\n            if (!buffer_write_bytes(buffer, (const char*)&length, 4)) {\n                return 0;\n            }\n            if (subtype != 2) {\n                if (!buffer_write_bytes(buffer, &subtype, 1)) {\n                    return 0;\n                }\n            }\n            {\n#if PY_MAJOR_VERSION >= 3\n                const char* string = PyBytes_AsString(value);\n#else\n                const char* string = PyString_AsString(value);\n#endif\n                if (!string) {\n                    return 0;\n                }\n                if (!buffer_write_bytes(buffer, string, length)) {\n                    return 0;\n                }\n            }\n        }\n        return 1;\n    } else if (state->UUID && PyObject_IsInstance(value, state->UUID)) {\n        // Just a special case of Binary above, but simpler to do as a separate case\n\n        PyObject* bytes;\n\n        // Could be bytes, bytearray, str...\n        const char* binarr;\n        // UUID is always 16 bytes\n        int length = 16;\n        char subtype;\n        if (uuid_subtype == JAVA_LEGACY || uuid_subtype == CSHARP_LEGACY) {\n            subtype = 3;\n        }\n        else {\n            subtype = (char)uuid_subtype;\n        }\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x05;\n        if (!buffer_write_bytes(buffer, (const char*)&length, 4)) {\n            return 0;\n        }\n        if (!buffer_write_bytes(buffer, &subtype, 1)) {\n            return 0;\n        }\n\n        if (uuid_subtype == CSHARP_LEGACY) {\n           /* Legacy C# byte order */\n            bytes = PyObject_GetAttrString(value, \"bytes_le\");\n        }\n        else {\n            bytes = PyObject_GetAttrString(value, \"bytes\");\n        }\n        if (!bytes) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        /* Work around http://bugs.python.org/issue7380 */\n        if (PyByteArray_Check(bytes)) {\n            binarr = PyByteArray_AsString(bytes);\n        }\n        else {\n            binarr = PyBytes_AsString(bytes);\n        }\n#else\n        binarr = PyString_AsString(bytes);\n#endif\n        if (uuid_subtype == JAVA_LEGACY) {\n            /* Store in legacy java byte order. */\n            char as_legacy_java[16];\n            _fix_java(binarr, as_legacy_java);\n            if (!buffer_write_bytes(buffer, as_legacy_java, length)) {\n                Py_DECREF(bytes);\n                return 0;\n            }\n        }\n        else {\n            if (!buffer_write_bytes(buffer, binarr, length)) {\n                Py_DECREF(bytes);\n                return 0;\n            }\n        }\n        Py_DECREF(bytes);\n        return 1;\n    } else if (PyObject_IsInstance(value, state->Code)) {\n        int start_position,\n            length_location,\n            length;\n\n        PyObject* scope = PyObject_GetAttrString(value, \"scope\");\n        if (!scope) {\n            return 0;\n        }\n\n        if (!PyDict_Size(scope)) {\n            Py_DECREF(scope);\n            *(buffer_get_buffer(buffer) + type_byte) = 0x0D;\n            return write_string(buffer, value);\n        }\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x0F;\n\n        start_position = buffer_get_position(buffer);\n        /* save space for length */\n        length_location = buffer_save_space(buffer, 4);\n        if (length_location == -1) {\n            PyErr_NoMemory();\n            Py_DECREF(scope);\n            return 0;\n        }\n\n        if (!write_string(buffer, value)) {\n            Py_DECREF(scope);\n            return 0;\n        }\n\n        if (!write_dict(self, buffer, scope, 0, uuid_subtype, 0)) {\n            Py_DECREF(scope);\n            return 0;\n        }\n        Py_DECREF(scope);\n\n        length = buffer_get_position(buffer) - start_position;\n        memcpy(buffer_get_buffer(buffer) + length_location, &length, 4);\n        return 1;\n#if PY_MAJOR_VERSION >= 3\n    /* Python3 special case. Store bytes as BSON binary subtype 0. */\n    } else if (PyBytes_Check(value)) {\n        Py_ssize_t length = PyBytes_Size(value);\n        const char subtype = 0;\n        *(buffer_get_buffer(buffer) + type_byte) = 0x05;\n        if (!buffer_write_bytes(buffer, (const char*)&length, 4)) {\n            return 0;\n        }\n        if (!buffer_write_bytes(buffer, &subtype, 1)) {\n            return 0;\n        }\n        if (!buffer_write_bytes(buffer, PyBytes_AsString(value), length)) {\n            return 0;\n        }\n        return 1;\n#else\n    /* PyString_Check only works in Python 2.x. */\n    } else if (PyString_Check(value)) {\n        int result;\n        result_t status;\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x02;\n        status = check_string((const unsigned char*)PyString_AsString(value),\n                              PyString_Size(value), 1, 0);\n\n        if (status == NOT_UTF_8) {\n            PyObject* InvalidStringData = _error(\"InvalidStringData\");\n            PyErr_SetString(InvalidStringData,\n                            \"strings in documents must be valid UTF-8\");\n            Py_DECREF(InvalidStringData);\n            return 0;\n        }\n        result = write_string(buffer, value);\n        return result;\n#endif\n    } else if (PyUnicode_Check(value)) {\n        PyObject* encoded;\n        int result;\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x02;\n        encoded = PyUnicode_AsUTF8String(value);\n        if (!encoded) {\n            return 0;\n        }\n        result = write_string(buffer, encoded);\n        Py_DECREF(encoded);\n        return result;\n    } else if (PyDateTime_Check(value)) {\n        long long millis;\n        PyObject* utcoffset = PyObject_CallMethod(value, \"utcoffset\", NULL);\n        if (utcoffset != Py_None) {\n            PyObject* result = PyNumber_Subtract(value, utcoffset);\n            Py_DECREF(utcoffset);\n            if (!result) {\n                return 0;\n            }\n            millis = millis_from_datetime(result);\n            Py_DECREF(result);\n        } else {\n            millis = millis_from_datetime(value);\n        }\n        *(buffer_get_buffer(buffer) + type_byte) = 0x09;\n        return buffer_write_bytes(buffer, (const char*)&millis, 8);\n    } else if (PyObject_IsInstance(value, state->ObjectId)) {\n        PyObject* pystring = PyObject_GetAttrString(value, \"_ObjectId__id\");\n        if (!pystring) {\n            return 0;\n        }\n        {\n#if PY_MAJOR_VERSION >= 3\n            const char* as_string = PyBytes_AsString(pystring);\n#else\n            const char* as_string = PyString_AsString(pystring);\n#endif\n            if (!as_string) {\n                Py_DECREF(pystring);\n                return 0;\n            }\n            if (!buffer_write_bytes(buffer, as_string, 12)) {\n                Py_DECREF(pystring);\n                return 0;\n            }\n            Py_DECREF(pystring);\n            *(buffer_get_buffer(buffer) + type_byte) = 0x07;\n        }\n        return 1;\n    } else if (PyObject_IsInstance(value, state->DBRef)) {\n        PyObject* as_doc = PyObject_CallMethod(value, \"as_doc\", NULL);\n        if (!as_doc) {\n            return 0;\n        }\n        if (!write_dict(self, buffer, as_doc, 0, uuid_subtype, 0)) {\n            Py_DECREF(as_doc);\n            return 0;\n        }\n        Py_DECREF(as_doc);\n        *(buffer_get_buffer(buffer) + type_byte) = 0x03;\n        return 1;\n    } else if (PyObject_IsInstance(value, state->Timestamp)) {\n        PyObject* obj;\n        long i;\n\n        obj = PyObject_GetAttrString(value, \"inc\");\n        if (!obj) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        i = PyLong_AsLong(obj);\n#else\n        i = PyInt_AsLong(obj);\n#endif\n        Py_DECREF(obj);\n        if (!buffer_write_bytes(buffer, (const char*)&i, 4)) {\n            return 0;\n        }\n\n        obj = PyObject_GetAttrString(value, \"time\");\n        if (!obj) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        i = PyLong_AsLong(obj);\n#else\n        i = PyInt_AsLong(obj);\n#endif\n        Py_DECREF(obj);\n        if (!buffer_write_bytes(buffer, (const char*)&i, 4)) {\n            return 0;\n        }\n\n        *(buffer_get_buffer(buffer) + type_byte) = 0x11;\n        return 1;\n    }\n    else if (PyObject_TypeCheck(value, state->REType)) {\n        PyObject* py_flags = PyObject_GetAttrString(value, \"flags\");\n        PyObject* py_pattern;\n        PyObject* encoded_pattern;\n        long int_flags;\n        char flags[FLAGS_SIZE];\n        char check_utf8 = 0;\n        int pattern_length,\n            flags_length;\n        result_t status;\n\n        if (!py_flags) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        int_flags = PyLong_AsLong(py_flags);\n#else\n        int_flags = PyInt_AsLong(py_flags);\n#endif\n        Py_DECREF(py_flags);\n        py_pattern = PyObject_GetAttrString(value, \"pattern\");\n        if (!py_pattern) {\n            return 0;\n        }\n\n        if (PyUnicode_Check(py_pattern)) {\n            encoded_pattern = PyUnicode_AsUTF8String(py_pattern);\n            Py_DECREF(py_pattern);\n            if (!encoded_pattern) {\n                return 0;\n            }\n        } else {\n            encoded_pattern = py_pattern;\n            check_utf8 = 1;\n        }\n\n#if PY_MAJOR_VERSION >= 3\n        status = check_string((const unsigned char*)PyBytes_AsString(encoded_pattern),\n                              PyBytes_Size(encoded_pattern), check_utf8, 1);\n#else\n        status = check_string((const unsigned char*)PyString_AsString(encoded_pattern),\n                              PyString_Size(encoded_pattern), check_utf8, 1);\n#endif\n        if (status == NOT_UTF_8) {\n            PyObject* InvalidStringData = _error(\"InvalidStringData\");\n            PyErr_SetString(InvalidStringData,\n                            \"regex patterns must be valid UTF-8\");\n            Py_DECREF(InvalidStringData);\n            Py_DECREF(encoded_pattern);\n            return 0;\n        } else if (status == HAS_NULL) {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n            PyErr_SetString(InvalidDocument,\n                            \"regex patterns must not contain the NULL byte\");\n            Py_DECREF(InvalidDocument);\n            Py_DECREF(encoded_pattern);\n            return 0;\n        }\n\n        {\n#if PY_MAJOR_VERSION >= 3\n            const char* pattern =  PyBytes_AsString(encoded_pattern);\n#else\n            const char* pattern =  PyString_AsString(encoded_pattern);\n#endif\n            pattern_length = strlen(pattern) + 1;\n\n            if (!buffer_write_bytes(buffer, pattern, pattern_length)) {\n                Py_DECREF(encoded_pattern);\n                return 0;\n            }\n        }\n        Py_DECREF(encoded_pattern);\n\n        flags[0] = 0;\n        /* TODO don't hardcode these */\n        if (int_flags & 2) {\n            STRCAT(flags, FLAGS_SIZE, \"i\");\n        }\n        if (int_flags & 4) {\n            STRCAT(flags, FLAGS_SIZE, \"l\");\n        }\n        if (int_flags & 8) {\n            STRCAT(flags, FLAGS_SIZE, \"m\");\n        }\n        if (int_flags & 16) {\n            STRCAT(flags, FLAGS_SIZE, \"s\");\n        }\n        if (int_flags & 32) {\n            STRCAT(flags, FLAGS_SIZE, \"u\");\n        }\n        if (int_flags & 64) {\n            STRCAT(flags, FLAGS_SIZE, \"x\");\n        }\n        flags_length = strlen(flags) + 1;\n        if (!buffer_write_bytes(buffer, flags, flags_length)) {\n            return 0;\n        }\n        *(buffer_get_buffer(buffer) + type_byte) = 0x0B;\n        return 1;\n    } else if (PyObject_IsInstance(value, state->MinKey)) {\n        *(buffer_get_buffer(buffer) + type_byte) = 0xFF;\n        return 1;\n    } else if (PyObject_IsInstance(value, state->MaxKey)) {\n        *(buffer_get_buffer(buffer) + type_byte) = 0x7F;\n        return 1;\n    } else if (first_attempt) {\n        /* Try reloading the modules and having one more go at it. */\n        if (WARN(PyExc_RuntimeWarning, \"couldn't encode - reloading python \"\n                 \"modules and trying again. if you see this without getting \"\n                 \"an InvalidDocument exception please see http://api.mongodb\"\n                 \".org/python/current/faq.html#does-pymongo-work-with-mod-\"\n                 \"wsgi\") == -1) {\n            return 0;\n        }\n        if (_reload_python_objects(self)) {\n            return 0;\n        }\n        return write_element_to_buffer(self, buffer, type_byte, value, check_keys, uuid_subtype, 0);\n    }\n    {\n        PyObject* repr = PyObject_Repr(value);\n        PyObject* InvalidDocument = _error(\"InvalidDocument\");\n#if PY_MAJOR_VERSION >= 3\n        PyObject* errmsg = PyUnicode_FromString(\"Cannot encode object: \");\n        PyObject* error = PyUnicode_Concat(errmsg, repr);\n        PyErr_SetObject(InvalidDocument, error);\n        Py_DECREF(error);\n        Py_DECREF(repr);\n#else\n        PyObject* errmsg = PyString_FromString(\"Cannot encode object: \");\n        PyString_ConcatAndDel(&errmsg, repr);\n        PyErr_SetString(InvalidDocument, PyString_AsString(errmsg));\n#endif\n        Py_DECREF(errmsg);\n        Py_DECREF(InvalidDocument);\n        return 0;\n    }\n}\n\nstatic int check_key_name(const char* name,\n                          const Py_ssize_t name_length) {\n    int i;\n    if (name_length > 0 && name[0] == '$') {\n        PyObject* InvalidDocument = _error(\"InvalidDocument\");\n#if PY_MAJOR_VERSION >= 3\n        PyObject* errmsg = PyUnicode_FromFormat(\"key '%s' must not start with '$'\", name);\n        PyErr_SetObject(InvalidDocument, errmsg);\n#else\n        PyObject* errmsg = PyString_FromFormat(\"key '%s' must not start with '$'\", name);\n        PyErr_SetString(InvalidDocument, PyString_AsString(errmsg));\n#endif\n        Py_DECREF(errmsg);\n        Py_DECREF(InvalidDocument);\n        return 0;\n    }\n    for (i = 0; i < name_length; i++) {\n        if (name[i] == '.') {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n#if PY_MAJOR_VERSION >= 3\n            PyObject* errmsg = PyUnicode_FromFormat(\"key '%s' must not contain '.'\", name);\n            PyErr_SetObject(InvalidDocument, errmsg);\n#else\n            PyObject* errmsg = PyString_FromFormat(\"key '%s' must not contain '.'\", name);\n            PyErr_SetString(InvalidDocument, PyString_AsString(errmsg));\n#endif\n            Py_DECREF(errmsg);\n            Py_DECREF(InvalidDocument);\n            return 0;\n        }\n    }\n    return 1;\n}\n\n/* Write a (key, value) pair to the buffer.\n *\n * Returns 0 on failure */\nint write_pair(PyObject* self, buffer_t buffer, const char* name, Py_ssize_t name_length,\n               PyObject* value, unsigned char check_keys,\n               unsigned char uuid_subtype, unsigned char allow_id) {\n    int type_byte;\n\n    /* Don't write any _id elements unless we're explicitly told to -\n     * _id has to be written first so we do so, but don't bother\n     * deleting it from the dictionary being written. */\n    if (!allow_id && strcmp(name, \"_id\") == 0) {\n        return 1;\n    }\n\n    type_byte = buffer_save_space(buffer, 1);\n    if (type_byte == -1) {\n        PyErr_NoMemory();\n        return 0;\n    }\n    if (check_keys && !check_key_name(name, name_length)) {\n        return 0;\n    }\n    if (!buffer_write_bytes(buffer, name, name_length + 1)) {\n        return 0;\n    }\n    if (!write_element_to_buffer(self, buffer, type_byte, value,\n                                 check_keys, uuid_subtype, 1)) {\n        return 0;\n    }\n    return 1;\n}\n\nint decode_and_write_pair(PyObject* self, buffer_t buffer,\n                          PyObject* key, PyObject* value,\n                          unsigned char check_keys,\n                          unsigned char uuid_subtype, unsigned char top_level) {\n    PyObject* encoded;\n    if (PyUnicode_Check(key)) {\n        result_t status;\n        encoded = PyUnicode_AsUTF8String(key);\n        if (!encoded) {\n            return 0;\n        }\n#if PY_MAJOR_VERSION >= 3\n        status = check_string((const unsigned char*)PyBytes_AsString(encoded),\n                              PyBytes_Size(encoded), 0, 1);\n#else\n        status = check_string((const unsigned char*)PyString_AsString(encoded),\n                              PyString_Size(encoded), 0, 1);\n#endif\n\n        if (status == HAS_NULL) {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n            PyErr_SetString(InvalidDocument,\n                            \"Key names must not contain the NULL byte\");\n            Py_DECREF(InvalidDocument);\n            return 0;\n        }\n#if PY_MAJOR_VERSION < 3\n    } else if (PyString_Check(key)) {\n        result_t status;\n        encoded = key;\n        Py_INCREF(encoded);\n\n        status = check_string((const unsigned char*)PyString_AsString(encoded),\n                                       PyString_Size(encoded), 1, 1);\n\n        if (status == NOT_UTF_8) {\n            PyObject* InvalidStringData = _error(\"InvalidStringData\");\n            PyErr_SetString(InvalidStringData,\n                            \"strings in documents must be valid UTF-8\");\n            Py_DECREF(InvalidStringData);\n            return 0;\n        } else if (status == HAS_NULL) {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n            PyErr_SetString(InvalidDocument,\n                            \"Key names must not contain the NULL byte\");\n            Py_DECREF(InvalidDocument);\n            return 0;\n        }\n#endif\n    } else {\n        PyObject* InvalidDocument = _error(\"InvalidDocument\");\n        PyObject* repr = PyObject_Repr(key);\n#if PY_MAJOR_VERSION >= 3\n        PyObject* errmsg = PyUnicode_FromString(\"documents must have only string keys, key was \");\n        PyObject* error = PyUnicode_Concat(errmsg, repr);\n        PyErr_SetObject(InvalidDocument, error);\n        Py_DECREF(error);\n#else\n        PyObject* errmsg = PyString_FromString(\"documents must have only string keys, key was \");\n        PyString_ConcatAndDel(&errmsg, repr);\n        PyErr_SetString(InvalidDocument, PyString_AsString(errmsg));\n#endif\n        Py_DECREF(InvalidDocument);\n        Py_DECREF(errmsg);\n        return 0;\n    }\n\n    /* If top_level is True, don't allow writing _id here - it was already written. */\n#if PY_MAJOR_VERSION >= 3\n    if (!write_pair(self, buffer, PyBytes_AsString(encoded),\n                    PyBytes_Size(encoded), value,\n                    check_keys, uuid_subtype, !top_level)) {\n#else\n    if (!write_pair(self, buffer, PyString_AsString(encoded),\n                    PyString_Size(encoded), value,\n                    check_keys, uuid_subtype, !top_level)) {\n#endif\n        Py_DECREF(encoded);\n        return 0;\n    }\n\n    Py_DECREF(encoded);\n    return 1;\n}\n\n/* returns 0 on failure */\nint write_dict(PyObject* self, buffer_t buffer, PyObject* dict,\n               unsigned char check_keys, unsigned char uuid_subtype, unsigned char top_level) {\n    PyObject* key;\n    PyObject* iter;\n    char zero = 0;\n    int length;\n    int length_location;\n\n    if (!PyDict_Check(dict)) {\n        PyObject* repr = PyObject_Repr(dict);\n#if PY_MAJOR_VERSION >= 3\n        PyObject* errmsg = PyUnicode_FromString(\"encoder expected a mapping type but got: \");\n        PyObject* error = PyUnicode_Concat(errmsg, repr);\n        PyErr_SetObject(PyExc_TypeError, error);\n        Py_DECREF(error);\n        Py_DECREF(repr);\n#else\n        PyObject* errmsg = PyString_FromString(\"encoder expected a mapping type but got: \");\n        PyString_ConcatAndDel(&errmsg, repr);\n        PyErr_SetString(PyExc_TypeError, PyString_AsString(errmsg));\n#endif\n        Py_DECREF(errmsg);\n        return 0;\n    }\n\n    length_location = buffer_save_space(buffer, 4);\n    if (length_location == -1) {\n        PyErr_NoMemory();\n        return 0;\n    }\n\n    /* Write _id first if this is a top level doc. */\n    if (top_level) {\n        PyObject* _id = PyDict_GetItemString(dict, \"_id\");\n        if (_id) {\n            /* Don't bother checking keys, but do make sure we're allowed to\n             * write _id */\n            if (!write_pair(self, buffer, \"_id\", 3, _id, 0, uuid_subtype, 1)) {\n                return 0;\n            }\n        }\n    }\n\n    iter = PyObject_GetIter(dict);\n    if (iter == NULL) {\n        return 0;\n    }\n    while ((key = PyIter_Next(iter)) != NULL) {\n        PyObject* value = PyDict_GetItem(dict, key);\n        if (!value) {\n            PyErr_SetObject(PyExc_KeyError, key);\n            Py_DECREF(key);\n            Py_DECREF(iter);\n            return 0;\n        }\n        if (!decode_and_write_pair(self, buffer, key, value,\n                                   check_keys, uuid_subtype, top_level)) {\n            Py_DECREF(key);\n            Py_DECREF(iter);\n            return 0;\n        }\n        Py_DECREF(key);\n    }\n    Py_DECREF(iter);\n\n    /* write null byte and fill in length */\n    if (!buffer_write_bytes(buffer, &zero, 1)) {\n        return 0;\n    }\n    length = buffer_get_position(buffer) - length_location;\n    memcpy(buffer_get_buffer(buffer) + length_location, &length, 4);\n    return 1;\n}\n\nstatic PyObject* _cbson_dict_to_bson(PyObject* self, PyObject* args) {\n    PyObject* dict;\n    PyObject* result;\n    unsigned char check_keys;\n    unsigned char uuid_subtype;\n    unsigned char top_level = 1;\n    buffer_t buffer;\n\n    if (!PyArg_ParseTuple(args, \"Obb|b\", &dict,\n                          &check_keys, &uuid_subtype, &top_level)) {\n        return NULL;\n    }\n\n    buffer = buffer_new();\n    if (!buffer) {\n        PyErr_NoMemory();\n        return NULL;\n    }\n\n    if (!write_dict(self, buffer, dict, check_keys, uuid_subtype, top_level)) {\n        buffer_free(buffer);\n        return NULL;\n    }\n\n    /* objectify buffer */\n#if PY_MAJOR_VERSION >= 3\n    result = Py_BuildValue(\"y#\", buffer_get_buffer(buffer),\n                           buffer_get_position(buffer));\n#else\n    result = Py_BuildValue(\"s#\", buffer_get_buffer(buffer),\n                           buffer_get_position(buffer));\n#endif\n    buffer_free(buffer);\n    return result;\n}\n\nstatic PyObject* get_value(PyObject* self, const char* buffer, int* position,\n                           int type, int max, PyObject* as_class,\n                           unsigned char tz_aware, unsigned char uuid_subtype) {\n    struct module_state *state = GETSTATE(self);\n\n    PyObject* value;\n    PyObject* error;\n    switch (type) {\n    case 1:\n        {\n            double d;\n            if (max < 8) {\n                goto invalid;\n            }\n            memcpy(&d, buffer + *position, 8);\n            value = PyFloat_FromDouble(d);\n            if (!value) {\n                return NULL;\n            }\n            *position += 8;\n            break;\n        }\n    case 2:\n    case 14:\n        {\n            int value_length = ((int*)(buffer + *position))[0] - 1;\n            if (max < value_length) {\n                goto invalid;\n            }\n            *position += 4;\n            value = PyUnicode_DecodeUTF8(buffer + *position, value_length, \"strict\");\n            if (!value) {\n                return NULL;\n            }\n            *position += value_length + 1;\n            break;\n        }\n    case 3:\n        {\n            int size;\n            memcpy(&size, buffer + *position, 4);\n            if (max < size) {\n                goto invalid;\n            }\n            value = elements_to_dict(self, buffer + *position + 4,\n                                     size - 5, as_class, tz_aware, uuid_subtype);\n            if (!value) {\n                return NULL;\n            }\n\n            /* Decoding for DBRefs */\n            if (strcmp(buffer + *position + 5, \"$ref\") == 0) { /* DBRef */\n                PyObject* dbref;\n                PyObject* collection = PyDict_GetItemString(value, \"$ref\");\n                PyObject* id = PyDict_GetItemString(value, \"$id\");\n                PyObject* database = PyDict_GetItemString(value, \"$db\");\n\n                Py_INCREF(collection);\n                PyDict_DelItemString(value, \"$ref\");\n\n                if (id == NULL) {\n                    id = Py_None;\n                    Py_INCREF(id);\n                } else {\n                    Py_INCREF(id);\n                    PyDict_DelItemString(value, \"$id\");\n                }\n\n                if (database == NULL) {\n                    database = Py_None;\n                    Py_INCREF(database);\n                } else {\n                    Py_INCREF(database);\n                    PyDict_DelItemString(value, \"$db\");\n                }\n\n                dbref = PyObject_CallFunctionObjArgs(state->DBRef, collection, id, database, value, NULL);\n                Py_DECREF(value);\n                value = dbref;\n\n                Py_DECREF(id);\n                Py_DECREF(collection);\n                Py_DECREF(database);\n                if (!value) {\n                    return NULL;\n                }\n            }\n\n            *position += size;\n            break;\n        }\n    case 4:\n        {\n            int size,\n                end;\n\n            memcpy(&size, buffer + *position, 4);\n            if (max < size) {\n                goto invalid;\n            }\n            end = *position + size - 1;\n            *position += 4;\n\n            value = PyList_New(0);\n            if (!value) {\n                return NULL;\n            }\n            while (*position < end) {\n                PyObject* to_append;\n\n                int type = (int)buffer[(*position)++];\n                int key_size = strlen(buffer + *position);\n                *position += key_size + 1; /* just skip the key, they're in order. */\n                to_append = get_value(self, buffer, position, type,\n                                      max - key_size, as_class, tz_aware, uuid_subtype);\n                if (!to_append) {\n                    Py_DECREF(value);\n                    return NULL;\n                }\n                PyList_Append(value, to_append);\n                Py_DECREF(to_append);\n            }\n            (*position)++;\n            break;\n        }\n    case 5:\n        {\n            PyObject* data;\n            PyObject* st;\n            int length, subtype;\n\n            memcpy(&length, buffer + *position, 4);\n            if (max < length) {\n                goto invalid;\n            }\n            subtype = (unsigned char)buffer[*position + 4];\n#if PY_MAJOR_VERSION >= 3\n            /* Python3 special case. Decode BSON binary subtype 0 to bytes. */\n            if (subtype == 0) {\n                value = PyBytes_FromStringAndSize(buffer + *position + 5, length);\n                *position += length + 5;\n                break;\n            }\n            if (subtype == 2) {\n                data = PyBytes_FromStringAndSize(buffer + *position + 9, length - 4);\n            } else {\n                data = PyBytes_FromStringAndSize(buffer + *position + 5, length);\n            }\n#else\n            if (subtype == 2) {\n                data = PyString_FromStringAndSize(buffer + *position + 9, length - 4);\n            } else {\n                data = PyString_FromStringAndSize(buffer + *position + 5, length);\n            }\n#endif\n            if (!data) {\n                return NULL;\n            }\n            if ((subtype == 3 || subtype == 4) && state->UUID) { // Encode as UUID, not Binary\n                PyObject* kwargs;\n                PyObject* args = PyTuple_New(0);\n                if (!args) {\n                    Py_DECREF(data);\n                    return NULL;\n                }\n                kwargs = PyDict_New();\n                if (!kwargs) {\n                    Py_DECREF(data);\n                    Py_DECREF(args);\n                    return NULL;\n                }\n\n                assert(length == 16); // UUID should always be 16 bytes\n\n                if (uuid_subtype == CSHARP_LEGACY) {\n                    /* Legacy C# byte order */\n                    PyDict_SetItemString(kwargs, \"bytes_le\", data);\n                }\n                else {\n                    if (uuid_subtype == JAVA_LEGACY) {\n                        /* Convert from legacy java byte order */\n                        char big_endian[16];\n                        _fix_java(buffer + *position + 5, big_endian);\n                        /* Free the previously created PyString object */\n                        Py_DECREF(data);\n#if PY_MAJOR_VERSION >= 3\n                        data = PyBytes_FromStringAndSize(big_endian, length);\n#else\n                        data = PyString_FromStringAndSize(big_endian, length);\n#endif\n                    }\n                    PyDict_SetItemString(kwargs, \"bytes\", data);\n                }\n                value = PyObject_Call(state->UUID, args, kwargs);\n\n                Py_DECREF(args);\n                Py_DECREF(kwargs);\n                Py_DECREF(data);\n                if (!value) {\n                    return NULL;\n                }\n\n                *position += length + 5;\n                break;\n            }\n\n#if PY_MAJOR_VERSION >= 3\n            st = PyLong_FromLong(subtype);\n#else\n            st = PyInt_FromLong(subtype);\n#endif\n            if (!st) {\n                Py_DECREF(data);\n                return NULL;\n            }\n            value = PyObject_CallFunctionObjArgs(state->Binary, data, st, NULL);\n            Py_DECREF(st);\n            Py_DECREF(data);\n            if (!value) {\n                return NULL;\n            }\n            *position += length + 5;\n            break;\n        }\n    case 6:\n    case 10:\n        {\n            value = Py_None;\n            Py_INCREF(value);\n            break;\n        }\n    case 7:\n        {\n            if (max < 12) {\n                goto invalid;\n            }\n#if PY_MAJOR_VERSION >= 3\n            value = PyObject_CallFunction(state->ObjectId, \"y#\", buffer + *position, 12);\n#else\n            value = PyObject_CallFunction(state->ObjectId, \"s#\", buffer + *position, 12);\n#endif\n            if (!value) {\n                return NULL;\n            }\n            *position += 12;\n            break;\n        }\n    case 8:\n        {\n            value = buffer[(*position)++] ? Py_True : Py_False;\n            Py_INCREF(value);\n            break;\n        }\n    case 9:\n        {\n            PyObject* naive;\n            PyObject* replace;\n            PyObject* args;\n            PyObject* kwargs;\n            if (max < 8) {\n                goto invalid;\n            }\n            naive = datetime_from_millis(*(long long*)(buffer + *position));\n            *position += 8;\n            if (!tz_aware) { /* In the naive case, we're done here. */\n                value = naive;\n                break;\n            }\n\n            if (!naive) {\n                return NULL;\n            }\n            replace = PyObject_GetAttrString(naive, \"replace\");\n            Py_DECREF(naive);\n            if (!replace) {\n                return NULL;\n            }\n            args = PyTuple_New(0);\n            if (!args) {\n                Py_DECREF(replace);\n                return NULL;\n            }\n            kwargs = PyDict_New();\n            if (!kwargs) {\n                Py_DECREF(replace);\n                Py_DECREF(args);\n                return NULL;\n            }\n            if (PyDict_SetItemString(kwargs, \"tzinfo\", state->UTC) == -1) {\n                Py_DECREF(replace);\n                Py_DECREF(args);\n                Py_DECREF(kwargs);\n                return NULL;\n            }\n            value = PyObject_Call(replace, args, kwargs);\n            Py_DECREF(replace);\n            Py_DECREF(args);\n            Py_DECREF(kwargs);\n            break;\n        }\n    case 11:\n        {\n            PyObject* pattern;\n            int flags_length,\n                flags,\n                i;\n            int pattern_length = strlen(buffer + *position);\n            if (max < pattern_length) {\n                goto invalid;\n            }\n            pattern = PyUnicode_DecodeUTF8(buffer + *position, pattern_length, \"strict\");\n            if (!pattern) {\n                return NULL;\n            }\n            *position += pattern_length + 1;\n            flags_length = strlen(buffer + *position);\n            if (max < pattern_length + flags_length) {\n                Py_DECREF(pattern);\n                goto invalid;\n            }\n            flags = 0;\n            for (i = 0; i < flags_length; i++) {\n                if (buffer[*position + i] == 'i') {\n                    flags |= 2;\n                } else if (buffer[*position + i] == 'l') {\n                    flags |= 4;\n                } else if (buffer[*position + i] == 'm') {\n                    flags |= 8;\n                } else if (buffer[*position + i] == 's') {\n                    flags |= 16;\n                } else if (buffer[*position + i] == 'u') {\n                    flags |= 32;\n                } else if (buffer[*position + i] == 'x') {\n                    flags |= 64;\n                }\n            }\n            *position += flags_length + 1;\n            value = PyObject_CallFunction(state->RECompile, \"Oi\", pattern, flags);\n            Py_DECREF(pattern);\n            break;\n        }\n    case 12:\n        {\n            int collection_length;\n            PyObject* collection;\n            PyObject* id;\n\n            *position += 4;\n            collection_length = strlen(buffer + *position);\n            if (max < collection_length) {\n                goto invalid;\n            }\n            collection = PyUnicode_DecodeUTF8(buffer + *position, collection_length, \"strict\");\n            if (!collection) {\n                return NULL;\n            }\n            *position += collection_length + 1;\n            if (max < collection_length + 12) {\n                Py_DECREF(collection);\n                goto invalid;\n            }\n            id = PyObject_CallFunction(state->ObjectId, \"s#\", buffer + *position, 12);\n            if (!id) {\n                Py_DECREF(collection);\n                return NULL;\n            }\n            *position += 12;\n            value = PyObject_CallFunctionObjArgs(state->DBRef, collection, id, NULL);\n            Py_DECREF(collection);\n            Py_DECREF(id);\n            break;\n        }\n    case 13:\n        {\n            PyObject* code;\n            int value_length = ((int*)(buffer + *position))[0] - 1;\n            if (max < value_length) {\n                goto invalid;\n            }\n            *position += 4;\n            code = PyUnicode_DecodeUTF8(buffer + *position, value_length, \"strict\");\n            if (!code) {\n                return NULL;\n            }\n            *position += value_length + 1;\n            value = PyObject_CallFunctionObjArgs(state->Code, code, NULL, NULL);\n            Py_DECREF(code);\n            break;\n        }\n    case 15:\n        {\n            int code_length,\n                scope_size;\n            PyObject* code;\n            PyObject* scope;\n\n            *position += 8;\n            code_length = strlen(buffer + *position);\n            if (max < 8 + code_length) {\n                goto invalid;\n            }\n            code = PyUnicode_DecodeUTF8(buffer + *position, code_length, \"strict\");\n            if (!code) {\n                return NULL;\n            }\n            *position += code_length + 1;\n\n            memcpy(&scope_size, buffer + *position, 4);\n            scope = elements_to_dict(self, buffer + *position + 4, scope_size - 5,\n                                     (PyObject*)&PyDict_Type, tz_aware, uuid_subtype);\n            if (!scope) {\n                Py_DECREF(code);\n                return NULL;\n            }\n            *position += scope_size;\n\n            value = PyObject_CallFunctionObjArgs(state->Code, code, scope, NULL);\n            Py_DECREF(code);\n            Py_DECREF(scope);\n            break;\n        }\n    case 16:\n        {\n            int i;\n            if (max < 4) {\n                goto invalid;\n            }\n            memcpy(&i, buffer + *position, 4);\n#if PY_MAJOR_VERSION >= 3\n            value = PyLong_FromLong(i);\n#else\n            value = PyInt_FromLong(i);\n#endif\n            if (!value) {\n                return NULL;\n            }\n            *position += 4;\n            break;\n        }\n    case 17:\n        {\n            unsigned int time, inc;\n            if (max < 8) {\n                goto invalid;\n            }\n            memcpy(&inc, buffer + *position, 4);\n            memcpy(&time, buffer + *position + 4, 4);\n            value = PyObject_CallFunction(state->Timestamp, \"II\", time, inc);\n            if (!value) {\n                return NULL;\n            }\n            *position += 8;\n            break;\n        }\n    case 18:\n        {\n            long long ll;\n            if (max < 8) {\n                goto invalid;\n            }\n            memcpy(&ll, buffer + *position, 8);\n            value = PyLong_FromLongLong(ll);\n            if (!value) {\n                return NULL;\n            }\n            *position += 8;\n            break;\n        }\n    case -1:\n        {\n            value = PyObject_CallFunctionObjArgs(state->MinKey, NULL);\n            break;\n        }\n    case 127:\n        {\n            value = PyObject_CallFunctionObjArgs(state->MaxKey, NULL);\n            break;\n        }\n    default:\n        {\n            PyObject* InvalidDocument = _error(\"InvalidDocument\");\n            PyErr_SetString(InvalidDocument, \"no c decoder for this type yet\");\n            Py_DECREF(InvalidDocument);\n            return NULL;\n        }\n    }\n    return value;\n\n    invalid:\n\n    error = _error(\"InvalidBSON\");\n    PyErr_SetNone(error);\n    Py_DECREF(error);\n    return NULL;\n}\n\nstatic PyObject* elements_to_dict(PyObject* self, const char* string, int max,\n                                  PyObject* as_class, unsigned char tz_aware,\n                                  unsigned char uuid_subtype) {\n    int position = 0;\n    PyObject* dict = PyObject_CallObject(as_class, NULL);\n    if (!dict) {\n        return NULL;\n    }\n    while (position < max) {\n        PyObject* name;\n        PyObject* value;\n        int type = (int)string[position++];\n        int name_length = strlen(string + position);\n        if (position + name_length >= max) {\n            PyObject* InvalidBSON = _error(\"InvalidBSON\");\n            PyErr_SetNone(InvalidBSON);\n            Py_DECREF(InvalidBSON);\n            Py_DECREF(dict);\n            return NULL;\n        }\n        name = PyUnicode_DecodeUTF8(string + position, name_length, \"strict\");\n        if (!name) {\n            Py_DECREF(dict);\n            return NULL;\n        }\n        position += name_length + 1;\n        value = get_value(self, string, &position, type,\n                          max - position, as_class, tz_aware, uuid_subtype);\n        if (!value) {\n            Py_DECREF(name);\n            Py_DECREF(dict);\n            return NULL;\n        }\n\n        PyObject_SetItem(dict, name, value);\n        Py_DECREF(name);\n        Py_DECREF(value);\n    }\n    return dict;\n}\n\nstatic PyObject* _cbson_bson_to_dict(PyObject* self, PyObject* args) {\n    unsigned int size;\n    Py_ssize_t total_size;\n    const char* string;\n    PyObject* bson;\n    PyObject* as_class;\n    unsigned char tz_aware;\n    unsigned char uuid_subtype;\n    PyObject* dict;\n    PyObject* remainder;\n    PyObject* result;\n\n    if (!PyArg_ParseTuple(args, \"OObb\", &bson, &as_class, &tz_aware, &uuid_subtype)) {\n        return NULL;\n    }\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyBytes_Check(bson)) {\n        PyErr_SetString(PyExc_TypeError, \"argument to _bson_to_dict must be a bytes object\");\n#else\n    if (!PyString_Check(bson)) {\n        PyErr_SetString(PyExc_TypeError, \"argument to _bson_to_dict must be a string\");\n#endif\n        return NULL;\n    }\n#if PY_MAJOR_VERSION >= 3\n    total_size = PyBytes_Size(bson);\n#else\n    total_size = PyString_Size(bson);\n#endif\n    if (total_size < 5) {\n        PyObject* InvalidBSON = _error(\"InvalidBSON\");\n        PyErr_SetString(InvalidBSON,\n                        \"not enough data for a BSON document\");\n        Py_DECREF(InvalidBSON);\n        return NULL;\n    }\n\n#if PY_MAJOR_VERSION >= 3\n    string = PyBytes_AsString(bson);\n#else\n    string = PyString_AsString(bson);\n#endif\n    if (!string) {\n        return NULL;\n    }\n    memcpy(&size, string, 4);\n\n    if (total_size < size) {\n        PyObject* InvalidBSON = _error(\"InvalidBSON\");\n        PyErr_SetString(InvalidBSON,\n                        \"objsize too large\");\n        Py_DECREF(InvalidBSON);\n        return NULL;\n    }\n\n    if (size != total_size || string[size - 1]) {\n        PyObject* InvalidBSON = _error(\"InvalidBSON\");\n        PyErr_SetString(InvalidBSON,\n                        \"bad eoo\");\n        Py_DECREF(InvalidBSON);\n        return NULL;\n    }\n\n    dict = elements_to_dict(self, string + 4, size - 5, as_class, tz_aware, uuid_subtype);\n    if (!dict) {\n        return NULL;\n    }\n#if PY_MAJOR_VERSION >= 3\n    remainder = PyBytes_FromStringAndSize(string + size, total_size - size);\n#else\n    remainder = PyString_FromStringAndSize(string + size, total_size - size);\n#endif\n    if (!remainder) {\n        Py_DECREF(dict);\n        return NULL;\n    }\n    result = Py_BuildValue(\"OO\", dict, remainder);\n    Py_DECREF(dict);\n    Py_DECREF(remainder);\n    return result;\n}\n\nstatic PyObject* _cbson_decode_all(PyObject* self, PyObject* args) {\n    unsigned int size;\n    Py_ssize_t total_size;\n    const char* string;\n    PyObject* bson;\n    PyObject* dict;\n    PyObject* result;\n    PyObject* as_class = (PyObject*)&PyDict_Type;\n    unsigned char tz_aware = 1;\n    unsigned char uuid_subtype = 3;\n\n    if (!PyArg_ParseTuple(args, \"O|Obb\", &bson, &as_class, &tz_aware, &uuid_subtype)) {\n        return NULL;\n    }\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyBytes_Check(bson)) {\n        PyErr_SetString(PyExc_TypeError, \"argument to decode_all must be a bytes object\");\n#else\n    if (!PyString_Check(bson)) {\n        PyErr_SetString(PyExc_TypeError, \"argument to decode_all must be a string\");\n#endif\n        return NULL;\n    }\n#if PY_MAJOR_VERSION >= 3\n    total_size = PyBytes_Size(bson);\n    string = PyBytes_AsString(bson);\n#else\n    total_size = PyString_Size(bson);\n    string = PyString_AsString(bson);\n#endif\n    if (!string) {\n        return NULL;\n    }\n\n    result = PyList_New(0);\n\n    while (total_size > 0) {\n        if (total_size < 5) {\n            PyObject* InvalidBSON = _error(\"InvalidBSON\");\n            PyErr_SetString(InvalidBSON,\n                            \"not enough data for a BSON document\");\n            Py_DECREF(InvalidBSON);\n            Py_DECREF(result);\n            return NULL;\n        }\n\n        memcpy(&size, string, 4);\n\n        if (total_size < size) {\n            PyObject* InvalidBSON = _error(\"InvalidBSON\");\n            PyErr_SetString(InvalidBSON,\n                            \"objsize too large\");\n            Py_DECREF(InvalidBSON);\n            Py_DECREF(result);\n            return NULL;\n        }\n\n        if (string[size - 1]) {\n            PyObject* InvalidBSON = _error(\"InvalidBSON\");\n            PyErr_SetString(InvalidBSON,\n                            \"bad eoo\");\n            Py_DECREF(InvalidBSON);\n            Py_DECREF(result);\n            return NULL;\n        }\n\n        dict = elements_to_dict(self, string + 4, size - 5,\n                                as_class, tz_aware, uuid_subtype);\n        if (!dict) {\n            Py_DECREF(result);\n            return NULL;\n        }\n        PyList_Append(result, dict);\n        Py_DECREF(dict);\n        string += size;\n        total_size -= size;\n    }\n\n    return result;\n}\n\nstatic PyMethodDef _CBSONMethods[] = {\n    {\"_dict_to_bson\", _cbson_dict_to_bson, METH_VARARGS,\n     \"convert a dictionary to a string containing its BSON representation.\"},\n    {\"_bson_to_dict\", _cbson_bson_to_dict, METH_VARARGS,\n     \"convert a BSON string to a SON object.\"},\n    {\"decode_all\", _cbson_decode_all, METH_VARARGS,\n     \"convert binary data to a sequence of documents.\"},\n    {NULL, NULL, 0, NULL}\n};\n\n#if PY_MAJOR_VERSION >= 3\n#define INITERROR return NULL\nstatic int _cbson_traverse(PyObject *m, visitproc visit, void *arg) {\n    Py_VISIT(GETSTATE(m)->Binary);\n    Py_VISIT(GETSTATE(m)->Code);\n    Py_VISIT(GETSTATE(m)->ObjectId);\n    Py_VISIT(GETSTATE(m)->DBRef);\n    Py_VISIT(GETSTATE(m)->RECompile);\n    Py_VISIT(GETSTATE(m)->UUID);\n    Py_VISIT(GETSTATE(m)->Timestamp);\n    Py_VISIT(GETSTATE(m)->MinKey);\n    Py_VISIT(GETSTATE(m)->MaxKey);\n    Py_VISIT(GETSTATE(m)->UTC);\n    Py_VISIT(GETSTATE(m)->REType);\n    return 0;\n}\n\nstatic int _cbson_clear(PyObject *m) {\n    Py_CLEAR(GETSTATE(m)->Binary);\n    Py_CLEAR(GETSTATE(m)->Code);\n    Py_CLEAR(GETSTATE(m)->ObjectId);\n    Py_CLEAR(GETSTATE(m)->DBRef);\n    Py_CLEAR(GETSTATE(m)->RECompile);\n    Py_CLEAR(GETSTATE(m)->UUID);\n    Py_CLEAR(GETSTATE(m)->Timestamp);\n    Py_CLEAR(GETSTATE(m)->MinKey);\n    Py_CLEAR(GETSTATE(m)->MaxKey);\n    Py_CLEAR(GETSTATE(m)->UTC);\n    Py_CLEAR(GETSTATE(m)->REType);\n    return 0;\n}\n\nstatic struct PyModuleDef moduledef = {\n    PyModuleDef_HEAD_INIT,\n    \"_cbson\",\n    NULL,\n    sizeof(struct module_state),\n    _CBSONMethods,\n    NULL,\n    _cbson_traverse,\n    _cbson_clear,\n    NULL\n};\n\nPyMODINIT_FUNC\nPyInit__cbson(void)\n#else\n#define INITERROR return\nPyMODINIT_FUNC\ninit_cbson(void)\n#endif\n{\n    PyObject *m;\n    PyObject *c_api_object;\n    static void *_cbson_API[_cbson_API_POINTER_COUNT];\n\n#if PY_MAJOR_VERSION >= 3\n    m = PyModule_Create(&moduledef);\n#else\n    m = Py_InitModule(\"_cbson\", _CBSONMethods);\n#endif\n    if (m == NULL) {\n        INITERROR;\n    }\n\n    PyDateTime_IMPORT;\n    if (PyDateTimeAPI == NULL) {\n        Py_DECREF(m);\n        INITERROR;\n    }\n\n    /* Import several python objects */\n    if (_reload_python_objects(m)) {\n        Py_DECREF(m);\n        INITERROR;\n    }\n\n    /* Export C API */\n    _cbson_API[_cbson_buffer_write_bytes_INDEX] = (void *) buffer_write_bytes;\n    _cbson_API[_cbson_write_dict_INDEX] = (void *) write_dict;\n    _cbson_API[_cbson_write_pair_INDEX] = (void *) write_pair;\n    _cbson_API[_cbson_decode_and_write_pair_INDEX] = (void *) decode_and_write_pair;\n\n#if PY_VERSION_HEX >= 0x03010000\n    /* PyCapsule is new in python 3.1 */\n    c_api_object = PyCapsule_New((void *) _cbson_API, \"_cbson._C_API\", NULL);\n#else\n    c_api_object = PyCObject_FromVoidPtr((void *) _cbson_API, NULL);\n#endif\n\n    if (c_api_object != NULL) {\n        PyModule_AddObject(m, \"_C_API\", c_api_object);\n    }\n#if PY_MAJOR_VERSION >= 3\n    return m;\n#endif\n}\n", "# -*- coding: utf-8 -*-\n\n# Copyright 2009-2012 10gen, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Test the collection module.\"\"\"\n\nimport itertools\nimport re\nimport sys\nimport threading\nimport time\nimport unittest\nimport warnings\n\nfrom nose.plugins.skip import SkipTest\n\nsys.path[0:0] = [\"\"]\n\nfrom bson.binary import Binary, UUIDLegacy, OLD_UUID_SUBTYPE, UUID_SUBTYPE\nfrom bson.code import Code\nfrom bson.dbref import DBRef\nfrom bson.objectid import ObjectId\nfrom bson.py3compat import b\nfrom bson.son import SON\nfrom pymongo import (ASCENDING, DESCENDING, GEO2D,\n                     GEOHAYSTACK, GEOSPHERE, HASHED)\nfrom pymongo.collection import Collection\nfrom pymongo.son_manipulator import SONManipulator\nfrom pymongo.errors import (ConfigurationError,\n                            DuplicateKeyError,\n                            InvalidDocument,\n                            InvalidName,\n                            InvalidOperation,\n                            OperationFailure,\n                            TimeoutError)\nfrom test.test_client import get_client\nfrom test.utils import is_mongos, joinall\nfrom test import (qcheck,\n                  version)\n\nhave_uuid = True\ntry:\n    import uuid\nexcept ImportError:\n    have_uuid = False\n\n\nclass TestCollection(unittest.TestCase):\n\n    def setUp(self):\n        self.client = get_client()\n        self.db = self.client.pymongo_test\n\n    def tearDown(self):\n        self.db.drop_collection(\"test_large_limit\")\n        self.db = None\n        self.client = None\n\n    def test_collection(self):\n        self.assertRaises(TypeError, Collection, self.db, 5)\n\n        def make_col(base, name):\n            return base[name]\n\n        self.assertRaises(InvalidName, make_col, self.db, \"\")\n        self.assertRaises(InvalidName, make_col, self.db, \"te$t\")\n        self.assertRaises(InvalidName, make_col, self.db, \".test\")\n        self.assertRaises(InvalidName, make_col, self.db, \"test.\")\n        self.assertRaises(InvalidName, make_col, self.db, \"tes..t\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"te$t\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \".test\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"test.\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"tes..t\")\n        self.assertRaises(InvalidName, make_col, self.db.test, \"tes\\x00t\")\n\n        self.assertTrue(isinstance(self.db.test, Collection))\n        self.assertEqual(self.db.test, self.db[\"test\"])\n        self.assertEqual(self.db.test, Collection(self.db, \"test\"))\n        self.assertEqual(self.db.test.mike, self.db[\"test.mike\"])\n        self.assertEqual(self.db.test[\"mike\"], self.db[\"test.mike\"])\n\n        self.db.drop_collection('test')\n        self.assertFalse('test' in self.db.collection_names())\n\n        # No exception\n        self.db.drop_collection('test')\n\n    def test_create_index(self):\n        db = self.db\n\n        self.assertRaises(TypeError, db.test.create_index, 5)\n        self.assertRaises(TypeError, db.test.create_index, {\"hello\": 1})\n        self.assertRaises(ValueError, db.test.create_index, [])\n\n        db.test.drop_indexes()\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 1)\n\n        db.test.create_index(\"hello\")\n        db.test.create_index([(\"hello\", DESCENDING), (\"world\", ASCENDING)])\n\n        count = 0\n        for _ in db.system.indexes.find({\"ns\": u\"pymongo_test.test\"}):\n            count += 1\n        self.assertEqual(count, 3)\n\n        db.test.drop_indexes()\n        ix = db.test.create_index([(\"hello\", DESCENDING),\n                                   (\"world\", ASCENDING)], name=\"hello_world\")\n        self.assertEqual(ix, \"hello_world\")\n\n        db.test.drop_indexes()\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 1)\n        db.test.create_index(\"hello\")\n        self.assertTrue(u\"hello_1\" in\n                        [a[\"name\"] for a in db.system.indexes\n                         .find({\"ns\": u\"pymongo_test.test\"})])\n\n        db.test.drop_indexes()\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 1)\n        db.test.create_index([(\"hello\", DESCENDING), (\"world\", ASCENDING)])\n        self.assertTrue(u\"hello_-1_world_1\" in\n                        [a[\"name\"] for a in db.system.indexes\n                         .find({\"ns\": u\"pymongo_test.test\"})])\n\n        db.test.drop()\n        db.test.insert({'a': 1})\n        db.test.insert({'a': 1})\n        self.assertRaises(DuplicateKeyError, db.test.create_index,\n                                                    'a', unique=True)\n\n    def test_ensure_index(self):\n        db = self.db\n\n        self.assertRaises(TypeError, db.test.ensure_index, {\"hello\": 1})\n\n        db.test.drop_indexes()\n        self.assertEqual(\"hello_1\", db.test.create_index(\"hello\"))\n        self.assertEqual(\"hello_1\", db.test.create_index(\"hello\"))\n\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_indexes()\n        self.assertEqual(\"foo\",\n                         db.test.ensure_index(\"goodbye\", name=\"foo\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\", name=\"foo\"))\n\n        db.test.drop_indexes()\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_index(\"goodbye_1\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.drop_collection(\"test\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_index(\"goodbye_1\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.create_index(\"goodbye\"))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_index(\"goodbye_1\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\", cache_for=1))\n        time.sleep(1.2)\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n\n        db.test.drop_index(\"goodbye_1\")\n        self.assertEqual(\"goodbye_1\",\n                         db.test.create_index(\"goodbye\", cache_for=1))\n        time.sleep(1.2)\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\"))\n        # Make sure the expiration time is updated.\n        self.assertEqual(None,\n                         db.test.ensure_index(\"goodbye\"))\n\n        # Clean up indexes for later tests\n        db.test.drop_indexes()\n\n    def test_deprecated_ttl_index_kwarg(self):\n        db = self.db\n\n        # In Python 2.6+ we could use the catch_warnings context\n        # manager to test this warning nicely. As we can't do that\n        # we must test raising errors before the ignore filter is applied.\n        warnings.simplefilter(\"error\", DeprecationWarning)\n        self.assertRaises(DeprecationWarning, lambda:\n                        db.test.ensure_index(\"goodbye\", ttl=10))\n        warnings.resetwarnings()\n        warnings.simplefilter(\"ignore\")\n\n        self.assertEqual(\"goodbye_1\",\n                         db.test.ensure_index(\"goodbye\", ttl=10))\n        self.assertEqual(None, db.test.ensure_index(\"goodbye\"))\n\n    def test_ensure_unique_index_threaded(self):\n        coll = self.db.test_unique_threaded\n        coll.drop()\n        coll.insert(({'foo': i} for i in xrange(10000)))\n\n        class Indexer(threading.Thread):\n            def run(self):\n                try:\n                    coll.ensure_index('foo', unique=True)\n                    coll.insert({'foo': 'bar'})\n                    coll.insert({'foo': 'bar'})\n                except OperationFailure:\n                    pass\n\n        threads = []\n        for _ in xrange(10):\n            t = Indexer()\n            t.setDaemon(True)\n            threads.append(t)\n\n        for i in xrange(10):\n            threads[i].start()\n\n        joinall(threads)\n\n        self.assertEqual(10001, coll.count())\n        coll.drop()\n\n    def test_index_on_binary(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.save({\"bin\": Binary(b(\"def\"))})\n        db.test.save({\"bin\": Binary(b(\"abc\"))})\n        db.test.save({\"bin\": Binary(b(\"ghi\"))})\n\n        self.assertEqual(db.test.find({\"bin\": Binary(b(\"abc\"))})\n                         .explain()[\"nscanned\"], 3)\n\n        db.test.create_index(\"bin\")\n        self.assertEqual(db.test.find({\"bin\": Binary(b(\"abc\"))})\n                         .explain()[\"nscanned\"], 1)\n\n    def test_drop_index(self):\n        db = self.db\n        db.test.drop_indexes()\n        db.test.create_index(\"hello\")\n        name = db.test.create_index(\"goodbye\")\n\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 3)\n        self.assertEqual(name, \"goodbye_1\")\n        db.test.drop_index(name)\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 2)\n        self.assertTrue(u\"hello_1\" in\n                        [a[\"name\"] for a in db.system.indexes\n                         .find({\"ns\": u\"pymongo_test.test\"})])\n\n        db.test.drop_indexes()\n        db.test.create_index(\"hello\")\n        name = db.test.create_index(\"goodbye\")\n\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 3)\n        self.assertEqual(name, \"goodbye_1\")\n        db.test.drop_index([(\"goodbye\", ASCENDING)])\n        self.assertEqual(db.system.indexes.find({\"ns\": u\"pymongo_test.test\"})\n                         .count(), 2)\n        self.assertTrue(u\"hello_1\" in\n                        [a[\"name\"] for a in db.system.indexes\n                         .find({\"ns\": u\"pymongo_test.test\"})])\n\n    def test_reindex(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.insert({\"foo\": \"bar\", \"who\": \"what\", \"when\": \"how\"})\n        db.test.create_index(\"foo\")\n        db.test.create_index(\"who\")\n        db.test.create_index(\"when\")\n        info = db.test.index_information()\n\n        def check_result(result):\n            self.assertEqual(4, result['nIndexes'])\n            self.assertEqual(4, result['nIndexesWas'])\n            indexes = result['indexes']\n            names = [idx['name'] for idx in indexes]\n            for name in names:\n                self.assertTrue(name in info)\n            for key in info:\n                self.assertTrue(key in names)\n\n        reindexed = db.test.reindex()\n        if 'raw' in reindexed:\n            # mongos\n            for result in reindexed['raw'].itervalues():\n                check_result(result)\n        else:\n            check_result(reindexed)\n\n    def test_index_info(self):\n        db = self.db\n        db.test.drop_indexes()\n        db.test.remove({})\n        db.test.save({})  # create collection\n        self.assertEqual(len(db.test.index_information()), 1)\n        self.assertTrue(\"_id_\" in db.test.index_information())\n\n        db.test.create_index(\"hello\")\n        self.assertEqual(len(db.test.index_information()), 2)\n        self.assertEqual(db.test.index_information()[\"hello_1\"][\"key\"],\n                         [(\"hello\", ASCENDING)])\n\n        db.test.create_index([(\"hello\", DESCENDING), (\"world\", ASCENDING)],\n                             unique=True)\n        self.assertEqual(db.test.index_information()[\"hello_1\"][\"key\"],\n                         [(\"hello\", ASCENDING)])\n        self.assertEqual(len(db.test.index_information()), 3)\n        self.assertEqual([(\"hello\", DESCENDING), (\"world\", ASCENDING)],\n                         db.test.index_information()[\"hello_-1_world_1\"][\"key\"]\n                        )\n        self.assertEqual(True,\n                     db.test.index_information()[\"hello_-1_world_1\"][\"unique\"])\n\n    def test_index_geo2d(self):\n        db = self.db\n        db.test.drop_indexes()\n        self.assertEqual('loc_2d', db.test.create_index([(\"loc\", GEO2D)]))\n        index_info = db.test.index_information()['loc_2d']\n        self.assertEqual([('loc', '2d')], index_info['key'])\n\n    def test_index_haystack(self):\n        if is_mongos(self.db.connection):\n            raise SkipTest(\"geoSearch is not supported by mongos\")\n        db = self.db\n        db.test.drop_indexes()\n        db.test.remove()\n        _id = db.test.insert({\n            \"pos\": {\"long\": 34.2, \"lat\": 33.3},\n            \"type\": \"restaurant\"\n        })\n        db.test.insert({\n            \"pos\": {\"long\": 34.2, \"lat\": 37.3}, \"type\": \"restaurant\"\n        })\n        db.test.insert({\n            \"pos\": {\"long\": 59.1, \"lat\": 87.2}, \"type\": \"office\"\n        })\n        db.test.create_index(\n            [(\"pos\", GEOHAYSTACK), (\"type\", ASCENDING)],\n            bucket_size=1\n        )\n\n        results = db.command(SON([\n            (\"geoSearch\", \"test\"),\n            (\"near\", [33, 33]),\n            (\"maxDistance\", 6),\n            (\"search\", {\"type\": \"restaurant\"}),\n            (\"limit\", 30),\n        ]))['results']\n\n        self.assertEqual(2, len(results))\n        self.assertEqual({\n            \"_id\": _id,\n            \"pos\": {\"long\": 34.2, \"lat\": 33.3},\n            \"type\": \"restaurant\"\n        }, results[0])\n\n    def test_index_text(self):\n        if not version.at_least(self.client, (2, 3, 2)):\n            raise SkipTest(\"Text search requires server >=2.3.2.\")\n\n        if is_mongos(self.client):\n            raise SkipTest(\"setParameter does not work through mongos\")\n\n        self.client.admin.command('setParameter', '*',\n                                  textSearchEnabled=True)\n\n        db = self.db\n        db.test.drop_indexes()\n        self.assertEqual(\"t_text\", db.test.create_index([(\"t\", \"text\")]))\n        index_info = db.test.index_information()[\"t_text\"]\n        self.assertTrue(\"weights\" in index_info)\n        db.test.drop_indexes()\n\n        self.client.admin.command('setParameter', '*',\n                                  textSearchEnabled=False)\n\n    def test_index_2dsphere(self):\n        if not version.at_least(self.client, (2, 3, 2)):\n            raise SkipTest(\"2dsphere indexing requires server >=2.3.2.\")\n\n        db = self.db\n        db.test.drop_indexes()\n        self.assertEqual(\"geo_2dsphere\",\n                         db.test.create_index([(\"geo\", GEOSPHERE)]))\n\n        poly = {\"type\": \"Polygon\",\n                \"coordinates\": [[[40,5], [40,6], [41,6], [41,5], [40,5]]]}\n        query = {\"geo\": {\"$within\": {\"$geometry\": poly}}}\n\n        self.assertTrue(\n            db.test.find(query).explain()['cursor'].startswith('S2Cursor'))\n\n        db.test.drop_indexes()\n\n    def test_index_hashed(self):\n        if not version.at_least(self.client, (2, 3, 2)):\n            raise SkipTest(\"hashed indexing requires server >=2.3.2.\")\n\n        db = self.db\n        db.test.drop_indexes()\n        self.assertEqual(\"a_hashed\",\n                         db.test.create_index([(\"a\", HASHED)]))\n\n        self.assertEqual(\"BtreeCursor a_hashed\",\n                db.test.find({'a': 1}).explain()['cursor'])\n        db.test.drop_indexes()\n\n    def test_index_sparse(self):\n        db = self.db\n        db.test.drop_indexes()\n        db.test.create_index([('key', ASCENDING)], sparse=True)\n        self.assertTrue(db.test.index_information()['key_1']['sparse'])\n\n    def test_index_background(self):\n        db = self.db\n        db.test.drop_indexes()\n        db.test.create_index([('keya', ASCENDING)])\n        db.test.create_index([('keyb', ASCENDING)], background=False)\n        db.test.create_index([('keyc', ASCENDING)], background=True)\n        self.assertFalse('background' in db.test.index_information()['keya_1'])\n        self.assertFalse(db.test.index_information()['keyb_1']['background'])\n        self.assertTrue(db.test.index_information()['keyc_1']['background'])\n\n    def _drop_dups_setup(self, db):\n        db.drop_collection('test')\n        db.test.insert({'i': 1})\n        db.test.insert({'i': 2})\n        db.test.insert({'i': 2})  # duplicate\n        db.test.insert({'i': 3})\n\n    def test_index_drop_dups(self):\n        # Try dropping duplicates\n        db = self.db\n        self._drop_dups_setup(db)\n\n        if version.at_least(db.connection, (1, 9, 2)):\n            # No error, just drop the duplicate\n            db.test.create_index(\n                [('i', ASCENDING)],\n                unique=True,\n                drop_dups=True\n            )\n        else:\n            # https://jira.mongodb.org/browse/SERVER-2054 \"Creating an index\n            # with dropDups shouldn't assert\". On Mongo < 1.9.2, the duplicate\n            # is dropped & the index created, but an error is thrown.\n            def test_create():\n                db.test.create_index(\n                    [('i', ASCENDING)],\n                    unique=True,\n                    drop_dups=True\n                )\n            self.assertRaises(DuplicateKeyError, test_create)\n\n        # Duplicate was dropped\n        self.assertEqual(3, db.test.count())\n\n        # Index was created, plus the index on _id\n        self.assertEqual(2, len(db.test.index_information()))\n\n    def test_index_dont_drop_dups(self):\n        # Try *not* dropping duplicates\n        db = self.db\n        self._drop_dups_setup(db)\n\n        # There's a duplicate\n        def test_create():\n            db.test.create_index(\n                [('i', ASCENDING)],\n                unique=True,\n                drop_dups=False\n            )\n        self.assertRaises(DuplicateKeyError, test_create)\n\n        # Duplicate wasn't dropped\n        self.assertEqual(4, db.test.count())\n\n        # Index wasn't created, only the default index on _id\n        self.assertEqual(1, len(db.test.index_information()))\n\n    def test_field_selection(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        doc = {\"a\": 1, \"b\": 5, \"c\": {\"d\": 5, \"e\": 10}}\n        db.test.insert(doc)\n\n        # Test field inclusion\n        doc = db.test.find({}, [\"_id\"]).next()\n        self.assertEqual(doc.keys(), [\"_id\"])\n        doc = db.test.find({}, [\"a\"]).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"a\"])\n        doc = db.test.find({}, [\"b\"]).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"b\"])\n        doc = db.test.find({}, [\"c\"]).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"c\"])\n        doc = db.test.find({}, [\"a\"]).next()\n        self.assertEqual(doc[\"a\"], 1)\n        doc = db.test.find({}, [\"b\"]).next()\n        self.assertEqual(doc[\"b\"], 5)\n        doc = db.test.find({}, [\"c\"]).next()\n        self.assertEqual(doc[\"c\"], {\"d\": 5, \"e\": 10})\n\n        # Test inclusion of fields with dots\n        doc = db.test.find({}, [\"c.d\"]).next()\n        self.assertEqual(doc[\"c\"], {\"d\": 5})\n        doc = db.test.find({}, [\"c.e\"]).next()\n        self.assertEqual(doc[\"c\"], {\"e\": 10})\n        doc = db.test.find({}, [\"b\", \"c.e\"]).next()\n        self.assertEqual(doc[\"c\"], {\"e\": 10})\n\n        doc = db.test.find({}, [\"b\", \"c.e\"]).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"b\", \"c\"])\n        doc = db.test.find({}, [\"b\", \"c.e\"]).next()\n        self.assertEqual(doc[\"b\"], 5)\n\n        # Test field exclusion\n        doc = db.test.find({}, {\"a\": False, \"b\": 0}).next()\n        l = doc.keys()\n        l.sort()\n        self.assertEqual(l, [\"_id\", \"c\"])\n\n        doc = db.test.find({}, {\"_id\": False}).next()\n        l = doc.keys()\n        self.assertFalse(\"_id\" in l)\n\n    def test_options(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.save({})\n        self.assertEqual(db.test.options(), {})\n        self.assertEqual(db.test.doesnotexist.options(), {})\n\n        db.drop_collection(\"test\")\n        if version.at_least(db.connection, (1, 9)):\n            db.create_collection(\"test\", capped=True, size=1000)\n            self.assertEqual(db.test.options(), {\"capped\": True, 'size': 1000})\n        else:\n            db.create_collection(\"test\", capped=True)\n            self.assertEqual(db.test.options(), {\"capped\": True})\n        db.drop_collection(\"test\")\n\n    def test_insert_find_one(self):\n        db = self.db\n        db.test.remove({})\n        self.assertEqual(0, len(list(db.test.find())))\n        doc = {\"hello\": u\"world\"}\n        id = db.test.insert(doc)\n        self.assertEqual(1, len(list(db.test.find())))\n        self.assertEqual(doc, db.test.find_one())\n        self.assertEqual(doc[\"_id\"], id)\n        self.assertTrue(isinstance(id, ObjectId))\n\n        doc_class = None\n        # Work around http://bugs.jython.org/issue1728\n        if (sys.platform.startswith('java') and\n            sys.version_info[:3] >= (2, 5, 2)):\n            doc_class = SON\n\n        def remove_insert_find_one(doc):\n            db.test.remove({})\n            db.test.insert(doc)\n            # SON equality is order sensitive.\n            return db.test.find_one(as_class=doc_class) == doc.to_dict()\n\n        qcheck.check_unittest(self, remove_insert_find_one,\n                              qcheck.gen_mongo_dict(3))\n\n    def test_generator_insert(self):\n        db = self.db\n        db.test.remove({})\n        self.assertEqual(db.test.find().count(), 0)\n        db.test.insert(({'a': i} for i in xrange(5)), manipulate=False)\n        self.assertEqual(5, db.test.count())\n        db.test.remove({})\n\n    def test_remove_all(self):\n        self.db.test.remove()\n        self.assertEqual(0, self.db.test.count())\n\n        self.db.test.insert({\"x\": 1})\n        self.db.test.insert({\"y\": 1})\n        self.assertEqual(2, self.db.test.count())\n\n        self.db.test.remove()\n        self.assertEqual(0, self.db.test.count())\n\n    def test_find_w_fields(self):\n        db = self.db\n        db.test.remove({})\n\n        db.test.insert({\"x\": 1, \"mike\": \"awesome\",\n                        \"extra thing\": \"abcdefghijklmnopqrstuvwxyz\"})\n        self.assertEqual(1, db.test.count())\n        doc = db.test.find({}).next()\n        self.assertTrue(\"x\" in doc)\n        doc = db.test.find({}).next()\n        self.assertTrue(\"mike\" in doc)\n        doc = db.test.find({}).next()\n        self.assertTrue(\"extra thing\" in doc)\n        doc = db.test.find({}, [\"x\", \"mike\"]).next()\n        self.assertTrue(\"x\" in doc)\n        doc = db.test.find({}, [\"x\", \"mike\"]).next()\n        self.assertTrue(\"mike\" in doc)\n        doc = db.test.find({}, [\"x\", \"mike\"]).next()\n        self.assertFalse(\"extra thing\" in doc)\n        doc = db.test.find({}, [\"mike\"]).next()\n        self.assertFalse(\"x\" in doc)\n        doc = db.test.find({}, [\"mike\"]).next()\n        self.assertTrue(\"mike\" in doc)\n        doc = db.test.find({}, [\"mike\"]).next()\n        self.assertFalse(\"extra thing\" in doc)\n\n    def test_fields_specifier_as_dict(self):\n        db = self.db\n        db.test.remove({})\n\n        db.test.insert({\"x\": [1, 2, 3], \"mike\": \"awesome\"})\n\n        self.assertEqual([1, 2, 3], db.test.find_one()[\"x\"])\n        if version.at_least(db.connection, (1, 5, 1)):\n            self.assertEqual([2, 3],\n                             db.test.find_one(fields={\"x\": {\"$slice\":\n                                                            -2}})[\"x\"])\n        self.assertTrue(\"x\" not in db.test.find_one(fields={\"x\": 0}))\n        self.assertTrue(\"mike\" in db.test.find_one(fields={\"x\": 0}))\n\n    def test_find_w_regex(self):\n        db = self.db\n        db.test.remove({})\n\n        db.test.insert({\"x\": \"hello_world\"})\n        db.test.insert({\"x\": \"hello_mike\"})\n        db.test.insert({\"x\": \"hello_mikey\"})\n        db.test.insert({\"x\": \"hello_test\"})\n\n        self.assertEqual(db.test.find().count(), 4)\n        self.assertEqual(db.test.find({\"x\":\n                                       re.compile(\"^hello.*\")}).count(), 4)\n        self.assertEqual(db.test.find({\"x\":\n                                       re.compile(\"ello\")}).count(), 4)\n        self.assertEqual(db.test.find({\"x\":\n                                       re.compile(\"^hello$\")}).count(), 0)\n        self.assertEqual(db.test.find({\"x\":\n                                       re.compile(\"^hello_mi.*$\")}).count(), 2)\n\n    def test_id_can_be_anything(self):\n        db = self.db\n\n        db.test.remove({})\n        auto_id = {\"hello\": \"world\"}\n        db.test.insert(auto_id)\n        self.assertTrue(isinstance(auto_id[\"_id\"], ObjectId))\n\n        numeric = {\"_id\": 240, \"hello\": \"world\"}\n        db.test.insert(numeric)\n        self.assertEqual(numeric[\"_id\"], 240)\n\n        object = {\"_id\": numeric, \"hello\": \"world\"}\n        db.test.insert(object)\n        self.assertEqual(object[\"_id\"], numeric)\n\n        for x in db.test.find():\n            self.assertEqual(x[\"hello\"], u\"world\")\n            self.assertTrue(\"_id\" in x)\n\n    def test_iteration(self):\n        db = self.db\n\n        def iterate():\n            [a for a in db.test]\n\n        self.assertRaises(TypeError, iterate)\n\n    def test_invalid_key_names(self):\n        db = self.db\n        db.test.drop()\n\n        db.test.insert({\"hello\": \"world\"})\n        db.test.insert({\"hello\": {\"hello\": \"world\"}})\n\n        self.assertRaises(InvalidDocument, db.test.insert, {\"$hello\": \"world\"})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello\": {\"$hello\": \"world\"}})\n\n        db.test.insert({\"he$llo\": \"world\"})\n        db.test.insert({\"hello\": {\"hello$\": \"world\"}})\n\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\".hello\": \"world\"})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello\": {\".hello\": \"world\"}})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello.\": \"world\"})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello\": {\"hello.\": \"world\"}})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hel.lo\": \"world\"})\n        self.assertRaises(InvalidDocument, db.test.insert,\n                          {\"hello\": {\"hel.lo\": \"world\"}})\n\n    def test_insert_multiple(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        doc1 = {\"hello\": u\"world\"}\n        doc2 = {\"hello\": u\"mike\"}\n        self.assertEqual(db.test.find().count(), 0)\n        ids = db.test.insert([doc1, doc2])\n        self.assertEqual(db.test.find().count(), 2)\n        self.assertEqual(doc1, db.test.find_one({\"hello\": u\"world\"}))\n        self.assertEqual(doc2, db.test.find_one({\"hello\": u\"mike\"}))\n\n        self.assertEqual(2, len(ids))\n        self.assertEqual(doc1[\"_id\"], ids[0])\n        self.assertEqual(doc2[\"_id\"], ids[1])\n\n        id = db.test.insert([{\"hello\": 1}])\n        self.assertTrue(isinstance(id, list))\n        self.assertEqual(1, len(id))\n\n        self.assertRaises(InvalidOperation, db.test.insert, [])\n\n    def test_insert_multiple_with_duplicate(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.ensure_index([('i', ASCENDING)], unique=True)\n\n        # No error\n        db.test.insert([{'i': i} for i in range(5, 10)], w=0)\n        db.test.remove()\n\n        # No error\n        db.test.insert([{'i': 1}] * 2, w=0)\n        self.assertEqual(1, db.test.count())\n\n        self.assertRaises(\n            DuplicateKeyError,\n            lambda: db.test.insert([{'i': 2}] * 2),\n        )\n\n        db.drop_collection(\"test\")\n        db.write_concern['w'] = 0\n        db.test.ensure_index([('i', ASCENDING)], unique=True)\n\n        # No error\n        db.test.insert([{'i': 1}] * 2)\n        self.assertEqual(1, db.test.count())\n\n        # Implied safe\n        self.assertRaises(\n            DuplicateKeyError,\n            lambda: db.test.insert([{'i': 2}] * 2, j=True),\n        )\n\n        # Explicit safe\n        self.assertRaises(\n            DuplicateKeyError,\n            lambda: db.test.insert([{'i': 2}] * 2, w=1),\n        )\n\n        # Misconfigured value for safe\n        self.assertRaises(\n            TypeError,\n            lambda: db.test.insert([{'i': 2}] * 2, safe=1),\n        )\n\n    def test_insert_iterables(self):\n        db = self.db\n\n        self.assertRaises(TypeError, db.test.insert, 4)\n        self.assertRaises(TypeError, db.test.insert, None)\n        self.assertRaises(TypeError, db.test.insert, True)\n\n        db.drop_collection(\"test\")\n        self.assertEqual(db.test.find().count(), 0)\n        ids = db.test.insert(({\"hello\": u\"world\"}, {\"hello\": u\"world\"}))\n        self.assertEqual(db.test.find().count(), 2)\n\n        db.drop_collection(\"test\")\n        self.assertEqual(db.test.find().count(), 0)\n        ids = db.test.insert(itertools.imap(lambda x: {\"hello\": \"world\"},\n                                            itertools.repeat(None, 10)))\n        self.assertEqual(db.test.find().count(), 10)\n\n    def test_save(self):\n        self.db.drop_collection(\"test\")\n\n        # Save a doc with autogenerated id\n        id = self.db.test.save({\"hello\": \"world\"})\n        self.assertEqual(self.db.test.find_one()[\"_id\"], id)\n        self.assertTrue(isinstance(id, ObjectId))\n\n        # Save a doc with explicit id\n        self.db.test.save({\"_id\": \"explicit_id\", \"hello\": \"bar\"})\n        doc = self.db.test.find_one({\"_id\": \"explicit_id\"})\n        self.assertEqual(doc['_id'], 'explicit_id')\n        self.assertEqual(doc['hello'], 'bar')\n\n        # Save docs with _id field already present (shouldn't create new docs)\n        self.assertEqual(2, self.db.test.count())\n        self.db.test.save({'_id': id, 'hello': 'world'})\n        self.assertEqual(2, self.db.test.count())\n        self.db.test.save({'_id': 'explicit_id', 'hello': 'baz'})\n        self.assertEqual(2, self.db.test.count())\n        self.assertEqual(\n            'baz',\n            self.db.test.find_one({'_id': 'explicit_id'})['hello']\n        )\n\n        # Safe mode\n        self.db.test.create_index(\"hello\", unique=True)\n        # No exception, even though we duplicate the first doc's \"hello\" value\n        self.db.test.save({'_id': 'explicit_id', 'hello': 'world'}, w=0)\n\n        self.assertRaises(\n            DuplicateKeyError,\n            self.db.test.save,\n            {'_id': 'explicit_id', 'hello': 'world'})\n\n    def test_save_with_invalid_key(self):\n        self.db.drop_collection(\"test\")\n        self.assertTrue(self.db.test.insert({\"hello\": \"world\"}))\n        doc = self.db.test.find_one()\n        doc['a.b'] = 'c'\n        self.assertRaises(InvalidDocument, self.db.test.save, doc)\n\n    def test_unique_index(self):\n        db = self.db\n\n        db.drop_collection(\"test\")\n        db.test.create_index(\"hello\")\n\n        db.test.save({\"hello\": \"world\"})\n        db.test.save({\"hello\": \"mike\"})\n        db.test.save({\"hello\": \"world\"})\n        self.assertFalse(db.error())\n\n        db.drop_collection(\"test\")\n        db.test.create_index(\"hello\", unique=True)\n\n        db.test.save({\"hello\": \"world\"})\n        db.test.save({\"hello\": \"mike\"})\n        db.test.save({\"hello\": \"world\"}, w=0)\n        self.assertTrue(db.error())\n\n    def test_duplicate_key_error(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.create_index(\"x\", unique=True)\n\n        db.test.insert({\"_id\": 1, \"x\": 1})\n        db.test.insert({\"_id\": 2, \"x\": 2})\n\n        # No error\n        db.test.insert({\"_id\": 1, \"x\": 1}, safe=False)\n        db.test.save({\"_id\": 1, \"x\": 1}, safe=False)\n        db.test.insert({\"_id\": 2, \"x\": 2}, safe=False)\n        db.test.save({\"_id\": 2, \"x\": 2}, safe=False)\n        db.test.insert({\"_id\": 1, \"x\": 1}, w=0)\n        db.test.save({\"_id\": 1, \"x\": 1}, w=0)\n        db.test.insert({\"_id\": 2, \"x\": 2}, w=0)\n        db.test.save({\"_id\": 2, \"x\": 2}, w=0)\n\n        # But all those statements didn't do anything\n        self.assertEqual(2, db.test.count())\n\n        expected_error = OperationFailure\n        if version.at_least(db.connection, (1, 3)):\n            expected_error = DuplicateKeyError\n\n        self.assertRaises(expected_error,\n                          db.test.insert, {\"_id\": 1})\n        self.assertRaises(expected_error,\n                          db.test.insert, {\"x\": 1})\n\n        self.assertRaises(expected_error,\n                          db.test.save, {\"x\": 2})\n        self.assertRaises(expected_error,\n                          db.test.update, {\"x\": 1},\n                          {\"$inc\": {\"x\": 1}})\n\n    def test_continue_on_error(self):\n        db = self.db\n        if not version.at_least(db.connection, (1, 9, 1)):\n            raise SkipTest(\"continue_on_error requires MongoDB >= 1.9.1\")\n\n        db.drop_collection(\"test\")\n        oid = db.test.insert({\"one\": 1})\n        self.assertEqual(1, db.test.count())\n\n        docs = []\n        docs.append({\"_id\": oid, \"two\": 2})\n        docs.append({\"three\": 3})\n        docs.append({\"four\": 4})\n        docs.append({\"five\": 5})\n\n        db.test.insert(docs, manipulate=False, w=0)\n        self.assertEqual(11000, db.error()['code'])\n        self.assertEqual(1, db.test.count())\n\n        db.test.insert(docs, manipulate=False, continue_on_error=True, w=0)\n        self.assertEqual(11000, db.error()['code'])\n        self.assertEqual(4, db.test.count())\n\n        db.drop_collection(\"test\")\n        oid = db.test.insert({\"_id\": oid, \"one\": 1}, w=0)\n        self.assertEqual(1, db.test.count())\n        docs[0].pop(\"_id\")\n        docs[2][\"_id\"] = oid\n\n        db.test.insert(docs, manipulate=False, w=0)\n        self.assertEqual(11000, db.error()['code'])\n        self.assertEqual(3, db.test.count())\n\n        db.test.insert(docs, manipulate=False, continue_on_error=True, w=0)\n        self.assertEqual(11000, db.error()['code'])\n        self.assertEqual(6, db.test.count())\n\n    def test_error_code(self):\n        try:\n            self.db.test.update({}, {\"$thismodifierdoesntexist\": 1})\n            self.fail()\n        except OperationFailure, e:\n            if version.at_least(self.db.connection, (1, 3)):\n                self.assertEqual(10147, e.code)\n\n    def test_index_on_subfield(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.insert({\"hello\": {\"a\": 4, \"b\": 5}})\n        db.test.insert({\"hello\": {\"a\": 7, \"b\": 2}})\n        db.test.insert({\"hello\": {\"a\": 4, \"b\": 10}})\n\n        db.drop_collection(\"test\")\n        db.test.create_index(\"hello.a\", unique=True)\n\n        db.test.insert({\"hello\": {\"a\": 4, \"b\": 5}})\n        db.test.insert({\"hello\": {\"a\": 7, \"b\": 2}})\n        self.assertRaises(DuplicateKeyError,\n            db.test.insert, {\"hello\": {\"a\": 4, \"b\": 10}})\n\n    def test_safe_insert(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        a = {\"hello\": \"world\"}\n        db.test.insert(a)\n        db.test.insert(a, w=0)\n        self.assertTrue(\"E11000\" in db.error()[\"err\"])\n\n        self.assertRaises(OperationFailure, db.test.insert, a)\n\n    def test_update(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        id1 = db.test.save({\"x\": 5})\n        db.test.update({}, {\"$inc\": {\"x\": 1}})\n        self.assertEqual(db.test.find_one(id1)[\"x\"], 6)\n\n        id2 = db.test.save({\"x\": 1})\n        db.test.update({\"x\": 6}, {\"$inc\": {\"x\": 1}})\n        self.assertEqual(db.test.find_one(id1)[\"x\"], 7)\n        self.assertEqual(db.test.find_one(id2)[\"x\"], 1)\n\n    def test_multi_update(self):\n        db = self.db\n        if not version.at_least(db.connection, (1, 1, 3, -1)):\n            raise SkipTest(\"multi-update requires MongoDB >= 1.1.3\")\n\n        db.drop_collection(\"test\")\n\n        db.test.save({\"x\": 4, \"y\": 3})\n        db.test.save({\"x\": 5, \"y\": 5})\n        db.test.save({\"x\": 4, \"y\": 4})\n\n        db.test.update({\"x\": 4}, {\"$set\": {\"y\": 5}}, multi=True)\n\n        self.assertEqual(3, db.test.count())\n        for doc in db.test.find():\n            self.assertEqual(5, doc[\"y\"])\n\n        self.assertEqual(2, db.test.update({\"x\": 4}, {\"$set\": {\"y\": 6}},\n                                           multi=True)[\"n\"])\n\n    def test_upsert(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.update({\"page\": \"/\"}, {\"$inc\": {\"count\": 1}}, upsert=True)\n        db.test.update({\"page\": \"/\"}, {\"$inc\": {\"count\": 1}}, upsert=True)\n\n        self.assertEqual(1, db.test.count())\n        self.assertEqual(2, db.test.find_one()[\"count\"])\n\n    def test_safe_update(self):\n        db = self.db\n        v113minus = version.at_least(db.connection, (1, 1, 3, -1))\n        v19 = version.at_least(db.connection, (1, 9))\n\n        db.drop_collection(\"test\")\n        db.test.create_index(\"x\", unique=True)\n\n        db.test.insert({\"x\": 5})\n        id = db.test.insert({\"x\": 4})\n\n        self.assertEqual(\n            None, db.test.update({\"_id\": id}, {\"$inc\": {\"x\": 1}}, w=0))\n\n        if v19:\n            self.assertTrue(\"E11000\" in db.error()[\"err\"])\n        elif v113minus:\n            self.assertTrue(db.error()[\"err\"].startswith(\"E11001\"))\n        else:\n            self.assertTrue(db.error()[\"err\"].startswith(\"E12011\"))\n\n        self.assertRaises(OperationFailure, db.test.update,\n                          {\"_id\": id}, {\"$inc\": {\"x\": 1}})\n\n        self.assertEqual(1, db.test.update({\"_id\": id},\n                                           {\"$inc\": {\"x\": 2}})[\"n\"])\n\n        self.assertEqual(0, db.test.update({\"_id\": \"foo\"},\n                                           {\"$inc\": {\"x\": 2}})[\"n\"])\n\n    def test_update_with_invalid_keys(self):\n        self.db.drop_collection(\"test\")\n        self.assertTrue(self.db.test.insert({\"hello\": \"world\"}))\n        doc = self.db.test.find_one()\n        doc['a.b'] = 'c'\n\n        # Replace\n        self.assertRaises(InvalidDocument,\n                          self.db.test.update, {\"hello\": \"world\"}, doc)\n        # Upsert\n        self.assertRaises(InvalidDocument,\n                          self.db.test.update, {\"foo\": \"bar\"}, doc, upsert=True)\n\n        # Check that the last two ops didn't actually modify anything\n        self.assertTrue('a.b' not in self.db.test.find_one())\n\n        # Modify shouldn't check keys...\n        self.assertTrue(self.db.test.update({\"hello\": \"world\"},\n                                            {\"$set\": {\"foo.bar\": \"baz\"}},\n                                            upsert=True))\n\n        # I know this seems like testing the server but I'd like to be notified\n        # by CI if the server's behavior changes here.\n        doc = SON([(\"$set\", {\"foo.bar\": \"bim\"}), (\"hello\", \"world\")])\n        self.assertRaises(OperationFailure, self.db.test.update,\n                          {\"hello\": \"world\"}, doc, upsert=True)\n\n        # This is going to cause keys to be checked and raise InvalidDocument.\n        # That's OK assuming the server's behavior in the previous assert\n        # doesn't change. If the behavior changes checking the first key for\n        # '$' in update won't be good enough anymore.\n        doc = SON([(\"hello\", \"world\"), (\"$set\", {\"foo.bar\": \"bim\"})])\n        self.assertRaises(InvalidDocument, self.db.test.update,\n                          {\"hello\": \"world\"}, doc, upsert=True)\n\n        # Replace with empty document\n        self.assertNotEqual(0, self.db.test.update({\"hello\": \"world\"},\n                            {})['n'])\n\n    def test_safe_save(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.create_index(\"hello\", unique=True)\n\n        db.test.save({\"hello\": \"world\"})\n        db.test.save({\"hello\": \"world\"}, w=0)\n        self.assertTrue(\"E11000\" in db.error()[\"err\"])\n\n        self.assertRaises(OperationFailure, db.test.save,\n                          {\"hello\": \"world\"})\n\n    def test_safe_remove(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.create_collection(\"test\", capped=True, size=1000)\n\n        db.test.insert({\"x\": 1})\n        self.assertEqual(1, db.test.count())\n\n        self.assertEqual(None, db.test.remove({\"x\": 1}, w=0))\n        self.assertEqual(1, db.test.count())\n\n        if version.at_least(db.connection, (1, 1, 3, -1)):\n            self.assertRaises(OperationFailure, db.test.remove,\n                              {\"x\": 1})\n        else:  # Just test that it doesn't blow up\n            db.test.remove({\"x\": 1})\n\n        db.drop_collection(\"test\")\n        db.test.insert({\"x\": 1})\n        db.test.insert({\"x\": 1})\n        self.assertEqual(2, db.test.remove({})[\"n\"])\n        self.assertEqual(0, db.test.remove({})[\"n\"])\n\n    def test_last_error_options(self):\n        if not version.at_least(self.client, (1, 5, 1)):\n            raise SkipTest(\"getLastError options require MongoDB >= 1.5.1\")\n\n        # XXX: Fix this if we ever have a replica set unittest env.\n        # mongo >=1.7.6 errors with 'norepl' when w=2+\n        # and we aren't replicated.\n        if not version.at_least(self.client, (1, 7, 6)):\n            self.assertRaises(TimeoutError, self.db.test.save,\n                              {\"x\": 1}, w=2, wtimeout=1)\n            self.assertRaises(TimeoutError, self.db.test.insert,\n                              {\"x\": 1}, w=2, wtimeout=1)\n            self.assertRaises(TimeoutError, self.db.test.update,\n                              {\"x\": 1}, {\"y\": 2}, w=2, wtimeout=1)\n            self.assertRaises(TimeoutError, self.db.test.remove,\n                              {\"x\": 1}, w=2, wtimeout=1)\n\n        self.db.test.save({\"x\": 1}, w=1, wtimeout=1)\n        self.db.test.insert({\"x\": 1}, w=1, wtimeout=1)\n        self.db.test.remove({\"x\": 1}, w=1, wtimeout=1)\n        self.db.test.update({\"x\": 1}, {\"y\": 2}, w=1, wtimeout=1)\n\n    def test_manual_last_error(self):\n        self.db.test.save({\"x\": 1}, w=0)\n        # XXX: Fix this if we ever have a replica set unittest env.\n        # mongo >=1.7.6 errors with 'norepl' when w=2+\n        # and we aren't replicated\n        if not version.at_least(self.client, (1, 7, 6)):\n            self.assertRaises(TimeoutError, self.db.command,\n                              \"getlasterror\", w=2, wtimeout=1)\n        self.db.command(\"getlasterror\", w=1, wtimeout=1)\n\n    def test_count(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        self.assertEqual(db.test.count(), 0)\n        db.test.save({})\n        db.test.save({})\n        self.assertEqual(db.test.count(), 2)\n        db.test.save({'foo': 'bar'})\n        db.test.save({'foo': 'baz'})\n        self.assertEqual(db.test.find({'foo': 'bar'}).count(), 1)\n        self.assertEqual(db.test.find({'foo': re.compile(r'ba.*')}).count(), 2)\n\n    def test_aggregate(self):\n        if not version.at_least(self.db.connection, (2, 1, 0)):\n            raise SkipTest(\"The aggregate command requires MongoDB >= 2.1.0\")\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.save({'foo': [1, 2]})\n\n        self.assertRaises(TypeError, db.test.aggregate, \"wow\")\n\n        pipeline = {\"$project\": {\"_id\": False, \"foo\": True}}\n        expected = {'ok': 1.0, 'result': [{'foo': [1, 2]}]}\n        self.assertEqual(expected, db.test.aggregate(pipeline))\n        self.assertEqual(expected, db.test.aggregate([pipeline]))\n        self.assertEqual(expected, db.test.aggregate((pipeline,)))\n\n    def test_group(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        def group_checker(args, expected):\n            eval = db.test.group(*args)\n            self.assertEqual(eval, expected)\n\n        self.assertEqual([],\n                         db.test.group([], {}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                       ))\n\n        db.test.save({\"a\": 2})\n        db.test.save({\"b\": 5})\n        db.test.save({\"a\": 1})\n\n        self.assertEqual([{\"count\": 3}],\n                         db.test.group([], {}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                      ))\n\n        self.assertEqual([{\"count\": 1}],\n                         db.test.group([], {\"a\": {\"$gt\": 1}}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                      ))\n\n        db.test.save({\"a\": 2, \"b\": 3})\n\n        self.assertEqual([{\"a\": 2, \"count\": 2},\n                          {\"a\": None, \"count\": 1},\n                          {\"a\": 1, \"count\": 1}],\n                         db.test.group([\"a\"], {}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                      ))\n\n        # modifying finalize\n        self.assertEqual([{\"a\": 2, \"count\": 3},\n                          {\"a\": None, \"count\": 2},\n                          {\"a\": 1, \"count\": 2}],\n                         db.test.group([\"a\"], {}, {\"count\": 0},\n                                       \"function (obj, prev) \"\n                                       \"{ prev.count++; }\",\n                                       \"function (obj) { obj.count++; }\"))\n\n        # returning finalize\n        self.assertEqual([2, 1, 1],\n                         db.test.group([\"a\"], {}, {\"count\": 0},\n                                       \"function (obj, prev) \"\n                                       \"{ prev.count++; }\",\n                                       \"function (obj) { return obj.count; }\"))\n\n        # keyf\n        self.assertEqual([2, 2],\n                         db.test.group(\"function (obj) { if (obj.a == 2) \"\n                                       \"{ return {a: true} }; \"\n                                       \"return {b: true}; }\", {}, {\"count\": 0},\n                                       \"function (obj, prev) \"\n                                       \"{ prev.count++; }\",\n                                       \"function (obj) { return obj.count; }\"))\n\n        # no key\n        self.assertEqual([{\"count\": 4}],\n                         db.test.group(None, {}, {\"count\": 0},\n                                       \"function (obj, prev) { prev.count++; }\"\n                                      ))\n\n        self.assertRaises(OperationFailure, db.test.group,\n                          [], {}, {}, \"5 ++ 5\")\n\n    def test_group_with_scope(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.test.save({\"a\": 1})\n        db.test.save({\"b\": 1})\n\n        reduce_function = \"function (obj, prev) { prev.count += inc_value; }\"\n\n        self.assertEqual(2, db.test.group([], {}, {\"count\": 0},\n                                          Code(reduce_function,\n                                               {\"inc_value\": 1}))[0]['count'])\n        self.assertEqual(4, db.test.group([], {}, {\"count\": 0},\n                                          Code(reduce_function,\n                                               {\"inc_value\": 2}))[0]['count'])\n\n        self.assertEqual(1,\n                         db.test.group([], {}, {\"count\": 0},\n                                       Code(reduce_function,\n                                            {\"inc_value\": 0.5}))[0]['count'])\n\n        if version.at_least(db.connection, (1, 1)):\n            self.assertEqual(2, db.test.group([], {}, {\"count\": 0},\n                                              Code(reduce_function,\n                                                   {\"inc_value\": 1}),\n                                             )[0]['count'])\n\n            self.assertEqual(4, db.test.group([], {}, {\"count\": 0},\n                                              Code(reduce_function,\n                                                   {\"inc_value\": 2}),\n                                             )[0]['count'])\n\n            self.assertEqual(1, db.test.group([], {}, {\"count\": 0},\n                                              Code(reduce_function,\n                                                   {\"inc_value\": 0.5}),\n                                             )[0]['count'])\n\n    def test_large_limit(self):\n        db = self.db\n        db.drop_collection(\"test_large_limit\")\n        db.test_large_limit.create_index([('x', 1)])\n\n        for i in range(2000):\n            doc = {\"x\": i, \"y\": \"mongomongo\" * 1000}\n            db.test_large_limit.insert(doc)\n\n        # Wait for insert to complete; often mysteriously failing in Jenkins\n        st = time.time()\n        while (\n            len(list(db.test_large_limit.find())) < 2000\n            and time.time() - st < 30\n        ):\n            time.sleep(1)\n\n        self.assertEqual(2000, len(list(db.test_large_limit.find())))\n\n        i = 0\n        y = 0\n        for doc in db.test_large_limit.find(limit=1900).sort([('x', 1)]):\n            i += 1\n            y += doc[\"x\"]\n\n        self.assertEqual(1900, i)\n        self.assertEqual((1900 * 1899) / 2, y)\n\n    def test_find_kwargs(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        for i in range(10):\n            db.test.insert({\"x\": i})\n\n        self.assertEqual(10, db.test.count())\n\n        sum = 0\n        for x in db.test.find({}, skip=4, limit=2):\n            sum += x[\"x\"]\n\n        self.assertEqual(9, sum)\n\n    def test_rename(self):\n        db = self.db\n        db.drop_collection(\"test\")\n        db.drop_collection(\"foo\")\n\n        self.assertRaises(TypeError, db.test.rename, 5)\n        self.assertRaises(InvalidName, db.test.rename, \"\")\n        self.assertRaises(InvalidName, db.test.rename, \"te$t\")\n        self.assertRaises(InvalidName, db.test.rename, \".test\")\n        self.assertRaises(InvalidName, db.test.rename, \"test.\")\n        self.assertRaises(InvalidName, db.test.rename, \"tes..t\")\n\n        self.assertEqual(0, db.test.count())\n        self.assertEqual(0, db.foo.count())\n\n        for i in range(10):\n            db.test.insert({\"x\": i})\n\n        self.assertEqual(10, db.test.count())\n\n        db.test.rename(\"foo\")\n\n        self.assertEqual(0, db.test.count())\n        self.assertEqual(10, db.foo.count())\n\n        x = 0\n        for doc in db.foo.find():\n            self.assertEqual(x, doc[\"x\"])\n            x += 1\n\n        db.test.insert({})\n        self.assertRaises(OperationFailure, db.foo.rename, \"test\")\n        db.foo.rename(\"test\", dropTarget=True)\n\n    # doesn't really test functionality, just that the option is set correctly\n    def test_snapshot(self):\n        db = self.db\n\n        self.assertRaises(TypeError, db.test.find, snapshot=5)\n\n        list(db.test.find(snapshot=True))\n        self.assertRaises(OperationFailure, list,\n                          db.test.find(snapshot=True).sort(\"foo\", 1))\n\n    def test_find_one(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        id = db.test.save({\"hello\": \"world\", \"foo\": \"bar\"})\n\n        self.assertEqual(\"world\", db.test.find_one()[\"hello\"])\n        self.assertEqual(db.test.find_one(id), db.test.find_one())\n        self.assertEqual(db.test.find_one(None), db.test.find_one())\n        self.assertEqual(db.test.find_one({}), db.test.find_one())\n        self.assertEqual(db.test.find_one({\"hello\": \"world\"}),\n                                          db.test.find_one())\n\n        self.assertTrue(\"hello\" in db.test.find_one(fields=[\"hello\"]))\n        self.assertTrue(\"hello\" not in db.test.find_one(fields=[\"foo\"]))\n        self.assertEqual([\"_id\"], db.test.find_one(fields=[]).keys())\n\n        self.assertEqual(None, db.test.find_one({\"hello\": \"foo\"}))\n        self.assertEqual(None, db.test.find_one(ObjectId()))\n\n    def test_find_one_non_objectid(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.save({\"_id\": 5})\n\n        self.assertTrue(db.test.find_one(5))\n        self.assertFalse(db.test.find_one(6))\n\n    def test_remove_non_objectid(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.save({\"_id\": 5})\n\n        self.assertEqual(1, db.test.count())\n        db.test.remove(5)\n        self.assertEqual(0, db.test.count())\n\n    def test_find_one_with_find_args(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.save({\"x\": 1})\n        db.test.save({\"x\": 2})\n        db.test.save({\"x\": 3})\n\n        self.assertEqual(1, db.test.find_one()[\"x\"])\n        self.assertEqual(2, db.test.find_one(skip=1, limit=2)[\"x\"])\n\n    def test_find_with_sort(self):\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.save({\"x\": 2})\n        db.test.save({\"x\": 1})\n        db.test.save({\"x\": 3})\n\n        self.assertEqual(2, db.test.find_one()[\"x\"])\n        self.assertEqual(1, db.test.find_one(sort=[(\"x\", 1)])[\"x\"])\n        self.assertEqual(3, db.test.find_one(sort=[(\"x\", -1)])[\"x\"])\n\n        def to_list(foo):\n            return [bar[\"x\"] for bar in foo]\n\n        self.assertEqual([2, 1, 3], to_list(db.test.find()))\n        self.assertEqual([1, 2, 3], to_list(db.test.find(sort=[(\"x\", 1)])))\n        self.assertEqual([3, 2, 1], to_list(db.test.find(sort=[(\"x\", -1)])))\n\n        self.assertRaises(TypeError, db.test.find, sort=5)\n        self.assertRaises(TypeError, db.test.find, sort=\"hello\")\n        self.assertRaises(ValueError, db.test.find, sort=[\"hello\", 1])\n\n    def test_insert_adds_id(self):\n        doc = {\"hello\": \"world\"}\n        self.db.test.insert(doc)\n        self.assertTrue(\"_id\" in doc)\n\n        docs = [{\"hello\": \"world\"}, {\"hello\": \"world\"}]\n        self.db.test.insert(docs)\n        for doc in docs:\n            self.assertTrue(\"_id\" in doc)\n\n    def test_save_adds_id(self):\n        doc = {\"hello\": \"jesse\"}\n        self.db.test.save(doc)\n        self.assertTrue(\"_id\" in doc)\n\n    # TODO doesn't actually test functionality, just that it doesn't blow up\n    def test_cursor_timeout(self):\n        list(self.db.test.find(timeout=False))\n        list(self.db.test.find(timeout=True))\n\n    def test_distinct(self):\n        if not version.at_least(self.db.connection, (1, 1)):\n            raise SkipTest(\"distinct command requires MongoDB >= 1.1\")\n\n        self.db.drop_collection(\"test\")\n\n        test = self.db.test\n        test.save({\"a\": 1})\n        test.save({\"a\": 2})\n        test.save({\"a\": 2})\n        test.save({\"a\": 2})\n        test.save({\"a\": 3})\n\n        distinct = test.distinct(\"a\")\n        distinct.sort()\n\n        self.assertEqual([1, 2, 3], distinct)\n\n        distinct = test.find({'a': {'$gt': 1}}).distinct(\"a\")\n        distinct.sort()\n\n        self.assertEqual([2, 3], distinct)\n\n        self.db.drop_collection(\"test\")\n\n        test.save({\"a\": {\"b\": \"a\"}, \"c\": 12})\n        test.save({\"a\": {\"b\": \"b\"}, \"c\": 12})\n        test.save({\"a\": {\"b\": \"c\"}, \"c\": 12})\n        test.save({\"a\": {\"b\": \"c\"}, \"c\": 12})\n\n        distinct = test.distinct(\"a.b\")\n        distinct.sort()\n\n        self.assertEqual([\"a\", \"b\", \"c\"], distinct)\n\n    def test_query_on_query_field(self):\n        self.db.drop_collection(\"test\")\n        self.db.test.save({\"query\": \"foo\"})\n        self.db.test.save({\"bar\": \"foo\"})\n\n        self.assertEqual(1,\n                         self.db.test.find({\"query\": {\"$ne\": None}}).count())\n        self.assertEqual(1,\n                         len(list(self.db.test.find({\"query\": {\"$ne\": None}})))\n                        )\n\n    def test_min_query(self):\n        self.db.drop_collection(\"test\")\n        self.db.test.save({\"x\": 1})\n        self.db.test.save({\"x\": 2})\n        self.db.test.create_index(\"x\")\n\n        self.assertEqual(1, len(list(self.db.test.find({\"$min\": {\"x\": 2},\n                                                        \"$query\": {}}))))\n        self.assertEqual(2, self.db.test.find({\"$min\": {\"x\": 2},\n                                               \"$query\": {}})[0][\"x\"])\n\n    def test_insert_large_document(self):\n        max_size = self.db.connection.max_bson_size\n        half_size = int(max_size / 2)\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            self.assertEqual(max_size, 16777216)\n        self.assertRaises(InvalidDocument, self.db.test.insert,\n                          {\"foo\": \"x\" * max_size})\n        self.assertRaises(InvalidDocument, self.db.test.save,\n                          {\"foo\": \"x\" * max_size})\n        self.assertRaises(InvalidDocument, self.db.test.insert,\n                          [{\"x\": 1}, {\"foo\": \"x\" * max_size}])\n        self.db.test.insert([{\"foo\": \"x\" * half_size},\n                             {\"foo\": \"x\" * half_size}])\n\n        self.db.test.insert({\"bar\": \"x\"})\n        self.assertRaises(InvalidDocument, self.db.test.update,\n                          {\"bar\": \"x\"}, {\"bar\": \"x\" * (max_size - 14)})\n        self.db.test.update({\"bar\": \"x\"}, {\"bar\": \"x\" * (max_size - 15)})\n\n    def test_map_reduce(self):\n        if not version.at_least(self.db.connection, (1, 1, 1)):\n            raise SkipTest(\"mapReduce command requires MongoDB >= 1.1.1\")\n\n        db = self.db\n        db.drop_collection(\"test\")\n\n        db.test.insert({\"id\": 1, \"tags\": [\"dog\", \"cat\"]})\n        db.test.insert({\"id\": 2, \"tags\": [\"cat\"]})\n        db.test.insert({\"id\": 3, \"tags\": [\"mouse\", \"cat\", \"dog\"]})\n        db.test.insert({\"id\": 4, \"tags\": []})\n\n        map = Code(\"function () {\"\n                   \"  this.tags.forEach(function(z) {\"\n                   \"    emit(z, 1);\"\n                   \"  });\"\n                   \"}\")\n        reduce = Code(\"function (key, values) {\"\n                      \"  var total = 0;\"\n                      \"  for (var i = 0; i < values.length; i++) {\"\n                      \"    total += values[i];\"\n                      \"  }\"\n                      \"  return total;\"\n                      \"}\")\n        result = db.test.map_reduce(map, reduce, out='mrunittests')\n        self.assertEqual(3, result.find_one({\"_id\": \"cat\"})[\"value\"])\n        self.assertEqual(2, result.find_one({\"_id\": \"dog\"})[\"value\"])\n        self.assertEqual(1, result.find_one({\"_id\": \"mouse\"})[\"value\"])\n\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            db.test.insert({\"id\": 5, \"tags\": [\"hampster\"]})\n            result = db.test.map_reduce(map, reduce, out='mrunittests')\n            self.assertEqual(1, result.find_one({\"_id\": \"hampster\"})[\"value\"])\n            db.test.remove({\"id\": 5})\n\n            result = db.test.map_reduce(map, reduce,\n                                        out={'merge': 'mrunittests'})\n            self.assertEqual(3, result.find_one({\"_id\": \"cat\"})[\"value\"])\n            self.assertEqual(1, result.find_one({\"_id\": \"hampster\"})[\"value\"])\n\n            result = db.test.map_reduce(map, reduce,\n                                        out={'reduce': 'mrunittests'})\n\n            self.assertEqual(6, result.find_one({\"_id\": \"cat\"})[\"value\"])\n            self.assertEqual(4, result.find_one({\"_id\": \"dog\"})[\"value\"])\n            self.assertEqual(2, result.find_one({\"_id\": \"mouse\"})[\"value\"])\n            self.assertEqual(1, result.find_one({\"_id\": \"hampster\"})[\"value\"])\n\n            result = db.test.map_reduce(\n                map,\n                reduce,\n                out={'replace': 'mrunittests'}\n            )\n            self.assertEqual(3, result.find_one({\"_id\": \"cat\"})[\"value\"])\n            self.assertEqual(2, result.find_one({\"_id\": \"dog\"})[\"value\"])\n            self.assertEqual(1, result.find_one({\"_id\": \"mouse\"})[\"value\"])\n\n            if (is_mongos(self.db.connection)\n                and not version.at_least(self.db.connection, (2, 1, 2))):\n                pass\n            else:\n                result = db.test.map_reduce(map, reduce,\n                                            out=SON([('replace', 'mrunittests'),\n                                                     ('db', 'mrtestdb')\n                                                    ]))\n                self.assertEqual(3, result.find_one({\"_id\": \"cat\"})[\"value\"])\n                self.assertEqual(2, result.find_one({\"_id\": \"dog\"})[\"value\"])\n                self.assertEqual(1, result.find_one({\"_id\": \"mouse\"})[\"value\"])\n                self.client.drop_database('mrtestdb')\n\n        full_result = db.test.map_reduce(map, reduce,\n                                         out='mrunittests', full_response=True)\n        self.assertEqual(6, full_result[\"counts\"][\"emit\"])\n\n        result = db.test.map_reduce(map, reduce, out='mrunittests', limit=2)\n        self.assertEqual(2, result.find_one({\"_id\": \"cat\"})[\"value\"])\n        self.assertEqual(1, result.find_one({\"_id\": \"dog\"})[\"value\"])\n        self.assertEqual(None, result.find_one({\"_id\": \"mouse\"}))\n\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            result = db.test.map_reduce(map, reduce, out={'inline': 1})\n            self.assertTrue(isinstance(result, dict))\n            self.assertTrue('results' in result)\n            self.assertTrue(result['results'][1][\"_id\"] in (\"cat\",\n                                                            \"dog\",\n                                                            \"mouse\"))\n\n            result = db.test.inline_map_reduce(map, reduce)\n            self.assertTrue(isinstance(result, list))\n            self.assertEqual(3, len(result))\n            self.assertTrue(result[1][\"_id\"] in (\"cat\", \"dog\", \"mouse\"))\n\n            full_result = db.test.inline_map_reduce(map, reduce,\n                                                    full_response=True)\n            self.assertEqual(6, full_result[\"counts\"][\"emit\"])\n\n    def test_messages_with_unicode_collection_names(self):\n        db = self.db\n\n        db[u\"Employ\u00e9s\"].insert({\"x\": 1})\n        db[u\"Employ\u00e9s\"].update({\"x\": 1}, {\"x\": 2})\n        db[u\"Employ\u00e9s\"].remove({})\n        db[u\"Employ\u00e9s\"].find_one()\n        list(db[u\"Employ\u00e9s\"].find())\n\n    def test_drop_indexes_non_existant(self):\n        self.db.drop_collection(\"test\")\n        self.db.test.drop_indexes()\n\n    # This is really a bson test but easier to just reproduce it here...\n    # (Shame on me)\n    def test_bad_encode(self):\n        c = self.db.test\n        warnings.simplefilter(\"ignore\")\n        self.assertRaises(InvalidDocument, c.save, {\"x\": c})\n        warnings.simplefilter(\"default\")\n\n    def test_bad_dbref(self):\n        c = self.db.test\n        c.drop()\n\n        # Incomplete DBRefs.\n        self.assertRaises(\n            InvalidDocument,\n            c.insert, {'ref': {'$ref': 'collection'}})\n\n        self.assertRaises(\n            InvalidDocument,\n            c.insert, {'ref': {'$id': ObjectId()}})\n\n        ref_only = {'ref': {'$ref': 'collection'}}\n        id_only = {'ref': {'$id': ObjectId()}}\n\n        # Force insert of ref without $id.\n        c.insert(ref_only, check_keys=False)\n        self.assertEqual(DBRef('collection', id=None), c.find_one()['ref'])\n        c.drop()\n\n        # DBRef without $ref is decoded as normal subdocument.\n        c.insert(id_only, check_keys=False)\n        self.assertEqual(id_only, c.find_one())\n\n    def test_as_class(self):\n        c = self.db.test\n        c.drop()\n        c.insert({\"x\": 1})\n\n        doc = c.find().next()\n        self.assertTrue(isinstance(doc, dict))\n        doc = c.find().next()\n        self.assertFalse(isinstance(doc, SON))\n        doc = c.find(as_class=SON).next()\n        self.assertTrue(isinstance(doc, SON))\n\n        self.assertTrue(isinstance(c.find_one(), dict))\n        self.assertFalse(isinstance(c.find_one(), SON))\n        self.assertTrue(isinstance(c.find_one(as_class=SON), SON))\n\n        self.assertEqual(1, c.find_one(as_class=SON)[\"x\"])\n        doc = c.find(as_class=SON).next()\n        self.assertEqual(1, doc[\"x\"])\n\n    def test_find_and_modify(self):\n        c = self.db.test\n        c.drop()\n        c.insert({'_id': 1, 'i': 1})\n\n        # Test that we raise DuplicateKeyError when appropriate.\n        c.ensure_index('i', unique=True)\n        self.assertRaises(DuplicateKeyError,\n                          c.find_and_modify, query={'i': 1, 'j': 1},\n                          update={'$set': {'k': 1}}, upsert=True)\n        c.drop_indexes()\n\n        # Test correct findAndModify\n        self.assertEqual({'_id': 1, 'i': 1},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}}))\n        self.assertEqual({'_id': 1, 'i': 3},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           new=True))\n\n        self.assertEqual({'_id': 1, 'i': 3},\n                         c.find_and_modify({'_id': 1}, remove=True))\n\n        self.assertEqual(None, c.find_one({'_id': 1}))\n\n        self.assertEqual(None,\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}}))\n        # The return value changed in 2.1.2. See SERVER-6226.\n        if version.at_least(self.db.connection, (2, 1, 2)):\n            self.assertEqual(None, c.find_and_modify({'_id': 1},\n                                                     {'$inc': {'i': 1}},\n                                                     upsert=True))\n        else:\n            self.assertEqual({}, c.find_and_modify({'_id': 1},\n                                                   {'$inc': {'i': 1}},\n                                                   upsert=True))\n        self.assertEqual({'_id': 1, 'i': 2},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           upsert=True, new=True))\n\n        self.assertEqual({'_id': 1, 'i': 2},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           fields=['i']))\n        self.assertEqual({'_id': 1, 'i': 4},\n                         c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           new=True, fields={'i': 1}))\n\n        # Test with full_response=True (version > 2.4.2)\n        result = c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                           new=True, upsert=True, \n                                           full_response=True,\n                                           fields={'i': 1})\n        self.assertEqual({'_id': 1, 'i': 5}, result[\"value\"])\n        self.assertEqual(True, result[\"lastErrorObject\"][\"updatedExisting\"])\n        \n        result = c.find_and_modify({'_id': 2}, {'$inc': {'i': 1}},\n                                           new=True, upsert=True, \n                                           full_response=True,\n                                           fields={'i': 1})\n        self.assertEqual({'_id': 2, 'i': 1}, result[\"value\"])\n        self.assertEqual(False, result[\"lastErrorObject\"][\"updatedExisting\"])\n\n        class ExtendedDict(dict):\n            pass\n\n        result = c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                    new=True, fields={'i': 1})\n        self.assertFalse(isinstance(result, ExtendedDict))\n        result = c.find_and_modify({'_id': 1}, {'$inc': {'i': 1}},\n                                    new=True, fields={'i': 1},\n                                    as_class=ExtendedDict)\n        self.assertTrue(isinstance(result, ExtendedDict))\n\n    def test_find_and_modify_with_sort(self):\n        c = self.db.test\n        c.drop()\n        for j in xrange(5):\n            c.insert({'j': j, 'i': 0})\n\n        sort={'j': DESCENDING}\n        self.assertEqual(4, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort={'j': ASCENDING}\n        self.assertEqual(0, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort=[('j', DESCENDING)]\n        self.assertEqual(4, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort=[('j', ASCENDING)]\n        self.assertEqual(0, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort=SON([('j', DESCENDING)])\n        self.assertEqual(4, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        sort=SON([('j', ASCENDING)])\n        self.assertEqual(0, c.find_and_modify({},\n                                              {'$inc': {'i': 1}},\n                                              sort=sort)['j'])\n        try:\n            from collections import OrderedDict\n            sort=OrderedDict([('j', DESCENDING)])\n            self.assertEqual(4, c.find_and_modify({},\n                                                  {'$inc': {'i': 1}},\n                                                  sort=sort)['j'])\n            sort=OrderedDict([('j', ASCENDING)])\n            self.assertEqual(0, c.find_and_modify({},\n                                                  {'$inc': {'i': 1}},\n                                                  sort=sort)['j'])\n        except ImportError:\n            pass\n        # Test that a standard dict with two keys is rejected.\n        sort={'j': DESCENDING, 'foo': DESCENDING}\n        self.assertRaises(TypeError, c.find_and_modify, {},\n                                                         {'$inc': {'i': 1}},\n                                                         sort=sort)\n\n    def test_find_with_nested(self):\n        if not version.at_least(self.db.connection, (2, 0, 0)):\n            raise SkipTest(\"nested $and and $or requires MongoDB >= 2.0\")\n        c = self.db.test\n        c.drop()\n        c.insert([{'i': i} for i in range(5)])  # [0, 1, 2, 3, 4]\n        self.assertEqual(\n            [2],\n            [i['i'] for i in c.find({\n                '$and': [\n                    {\n                        # This clause gives us [1,2,4]\n                        '$or': [\n                            {'i': {'$lte': 2}},\n                            {'i': {'$gt': 3}},\n                        ],\n                    },\n                    {\n                        # This clause gives us [2,3]\n                        '$or': [\n                            {'i': 2},\n                            {'i': 3},\n                        ]\n                    },\n                ]\n            })]\n        )\n\n        self.assertEqual(\n            [0, 1, 2],\n            [i['i'] for i in c.find({\n                '$or': [\n                    {\n                        # This clause gives us [2]\n                        '$and': [\n                            {'i': {'$gte': 2}},\n                            {'i': {'$lt': 3}},\n                        ],\n                    },\n                    {\n                        # This clause gives us [0,1]\n                        '$and': [\n                            {'i': {'$gt': -100}},\n                            {'i': {'$lt': 2}},\n                        ]\n                    },\n                ]\n            })]\n        )\n\n    def test_disabling_manipulators(self):\n\n        class IncByTwo(SONManipulator):\n            def transform_outgoing(self, son, collection):\n                if 'foo' in son:\n                    son['foo'] += 2\n                return son\n\n        db = self.client.pymongo_test\n        db.add_son_manipulator(IncByTwo())\n        c = db.test\n        c.drop()\n        c.insert({'foo': 0})\n        self.assertEqual(2, c.find_one()['foo'])\n        self.assertEqual(0, c.find_one(manipulate=False)['foo'])\n        self.assertEqual(2, c.find_one(manipulate=True)['foo'])\n        c.remove({})\n\n    def test_uuid_subtype(self):\n        if not have_uuid:\n            raise SkipTest(\"No uuid module\")\n\n        coll = self.client.pymongo_test.uuid\n        coll.drop()\n\n        def change_subtype(collection, subtype):\n            collection.uuid_subtype = subtype\n\n        # Test property\n        self.assertEqual(OLD_UUID_SUBTYPE, coll.uuid_subtype)\n        self.assertRaises(ConfigurationError, change_subtype, coll, 7)\n        self.assertRaises(ConfigurationError, change_subtype, coll, 2)\n\n        # Test basic query\n        uu = uuid.uuid4()\n        # Insert as binary subtype 3\n        coll.insert({'uu': uu})\n        self.assertEqual(uu, coll.find_one({'uu': uu})['uu'])\n        coll.uuid_subtype = UUID_SUBTYPE\n        self.assertEqual(UUID_SUBTYPE, coll.uuid_subtype)\n        self.assertEqual(None, coll.find_one({'uu': uu}))\n        self.assertEqual(uu, coll.find_one({'uu': UUIDLegacy(uu)})['uu'])\n\n        # Test Cursor.count\n        self.assertEqual(0, coll.find({'uu': uu}).count())\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        self.assertEqual(1, coll.find({'uu': uu}).count())\n\n        # Test remove\n        coll.uuid_subtype = UUID_SUBTYPE\n        coll.remove({'uu': uu})\n        self.assertEqual(1, coll.count())\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        coll.remove({'uu': uu})\n        self.assertEqual(0, coll.count())\n\n        # Test save\n        coll.insert({'_id': uu, 'i': 0})\n        self.assertEqual(1, coll.count())\n        self.assertEqual(1, coll.find({'_id': uu}).count())\n        self.assertEqual(0, coll.find_one({'_id': uu})['i'])\n        doc = coll.find_one({'_id': uu})\n        doc['i'] = 1\n        coll.save(doc)\n        self.assertEqual(1, coll.find_one({'_id': uu})['i'])\n\n        # Test update\n        coll.uuid_subtype = UUID_SUBTYPE\n        coll.update({'_id': uu}, {'$set': {'i': 2}})\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        self.assertEqual(1, coll.find_one({'_id': uu})['i'])\n        coll.update({'_id': uu}, {'$set': {'i': 2}})\n        self.assertEqual(2, coll.find_one({'_id': uu})['i'])\n\n        # Test Cursor.distinct\n        self.assertEqual([2], coll.find({'_id': uu}).distinct('i'))\n        coll.uuid_subtype = UUID_SUBTYPE\n        self.assertEqual([], coll.find({'_id': uu}).distinct('i'))\n\n        # Test find_and_modify\n        self.assertEqual(None, coll.find_and_modify({'_id': uu},\n                                                     {'$set': {'i': 5}}))\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        self.assertEqual(2, coll.find_and_modify({'_id': uu},\n                                                  {'$set': {'i': 5}})['i'])\n        self.assertEqual(5, coll.find_one({'_id': uu})['i'])\n\n        # Test command\n        db = self.client.pymongo_test\n        no_obj_error = \"No matching object found\"\n        result = db.command('findAndModify', 'uuid',\n                            allowable_errors=[no_obj_error],\n                            uuid_subtype=UUID_SUBTYPE,\n                            query={'_id': uu},\n                            update={'$set': {'i': 6}})\n        self.assertEqual(None, result.get('value'))\n        self.assertEqual(5, db.command('findAndModify', 'uuid',\n                                       update={'$set': {'i': 6}},\n                                       query={'_id': uu})['value']['i'])\n        self.assertEqual(6, db.command('findAndModify', 'uuid',\n                                       update={'$set': {'i': 7}},\n                                       query={'_id': UUIDLegacy(uu)}\n                                      )['value']['i'])\n\n        # Test (inline)_map_reduce\n        coll.drop()\n        coll.insert({\"_id\": uu, \"x\": 1, \"tags\": [\"dog\", \"cat\"]})\n        coll.insert({\"_id\": uuid.uuid4(), \"x\": 3,\n                     \"tags\": [\"mouse\", \"cat\", \"dog\"]})\n\n        map = Code(\"function () {\"\n                   \"  this.tags.forEach(function(z) {\"\n                   \"    emit(z, 1);\"\n                   \"  });\"\n                   \"}\")\n\n        reduce = Code(\"function (key, values) {\"\n                      \"  var total = 0;\"\n                      \"  for (var i = 0; i < values.length; i++) {\"\n                      \"    total += values[i];\"\n                      \"  }\"\n                      \"  return total;\"\n                      \"}\")\n\n        coll.uuid_subtype = UUID_SUBTYPE\n        q = {\"_id\": uu}\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            result = coll.inline_map_reduce(map, reduce, query=q)\n            self.assertEqual([], result)\n\n        result = coll.map_reduce(map, reduce, \"results\", query=q)\n        self.assertEqual(0, db.results.count())\n\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        q = {\"_id\": uu}\n        if version.at_least(self.db.connection, (1, 7, 4)):\n            result = coll.inline_map_reduce(map, reduce, query=q)\n            self.assertEqual(2, len(result))\n\n        result = coll.map_reduce(map, reduce, \"results\", query=q)\n        self.assertEqual(2, db.results.count())\n\n        db.drop_collection(\"result\")\n        coll.drop()\n\n        # Test group\n        coll.insert({\"_id\": uu, \"a\": 2})\n        coll.insert({\"_id\": uuid.uuid4(), \"a\": 1})\n\n        reduce = \"function (obj, prev) { prev.count++; }\"\n        coll.uuid_subtype = UUID_SUBTYPE\n        self.assertEqual([],\n                         coll.group([], {\"_id\": uu},\n                                     {\"count\": 0}, reduce))\n        coll.uuid_subtype = OLD_UUID_SUBTYPE\n        self.assertEqual([{\"count\": 1}],\n                         coll.group([], {\"_id\": uu},\n                                    {\"count\": 0}, reduce))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"], "filenames": ["bson/__init__.py", "bson/_cbsonmodule.c", "test/test_collection.py"], "buggy_code_start_loc": [153, 1205, 32], "buggy_code_end_loc": [154, 1207, 1676], "fixing_code_start_loc": [153, 1205, 33], "fixing_code_end_loc": [154, 1213, 1703], "type": "NVD-CWE-Other", "message": "bson/_cbsonmodule.c in the mongo-python-driver (aka. pymongo) before 2.5.2, as used in MongoDB, allows context-dependent attackers to cause a denial of service (NULL pointer dereference and crash) via vectors related to decoding of an \"invalid DBRef.\"", "other": {"cve": {"id": "CVE-2013-2132", "sourceIdentifier": "secalert@redhat.com", "published": "2013-08-15T17:55:24.500", "lastModified": "2023-02-13T04:42:55.380", "vulnStatus": "Modified", "evaluatorComment": "Per: http://cwe.mitre.org/data/definitions/476.html\n\n'CWE-476: NULL Pointer Dereference'", "descriptions": [{"lang": "en", "value": "bson/_cbsonmodule.c in the mongo-python-driver (aka. pymongo) before 2.5.2, as used in MongoDB, allows context-dependent attackers to cause a denial of service (NULL pointer dereference and crash) via vectors related to decoding of an \"invalid DBRef.\""}, {"lang": "es", "value": "bson/_cbsonmodule.c en el mongo-python-driver (tambi\u00e9n conocido como pymongo) anterior a v2.5.2, como es usado en MongoDB, permite a atacantes dependientes del contexto causar una denegaci\u00f3n de servicio (referencia NULL y ca\u00edda de la aplicaci\u00f3n) a trav\u00e9s de vectores relacionados con la decodificaci\u00f3n de un \"invalid DBRef\"."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.5.1", "matchCriteriaId": "B7FEFBDC-9423-4194-B7F1-B928F80FA004"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:1.2.0:*:*:*:*:*:*:*", "matchCriteriaId": "2FBA2303-8E19-415F-BD7C-B348DE0EC478"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:1.4.0:*:*:*:*:*:*:*", "matchCriteriaId": "92948672-3F39-4675-BFB1-4ACACD078CC6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:1.6.0:*:*:*:*:*:*:*", "matchCriteriaId": "A7590AB3-4433-4589-9575-647015A5DA24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:1.8.0:*:*:*:*:*:*:*", "matchCriteriaId": "EC4562CE-83D2-422C-AB94-CB0EFABC2CF8"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.0.0:*:*:*:*:*:*:*", "matchCriteriaId": "F1B3AD0E-11F2-477E-9D18-B6A609A4C652"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.2.0:*:*:*:*:*:*:*", "matchCriteriaId": "2E0F23FE-3AAF-4E29-AF89-C6365E839119"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.4.0:*:*:*:*:*:*:*", "matchCriteriaId": "FB2A001B-D790-4DEA-9420-6BD18F1D0BFF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "AD7C03C0-5B7B-4808-8F50-1D5517CEF61F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "C37483C2-DA83-4BE4-9960-5A691661670B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "039B2CC3-CB74-4ADE-BEEF-65A5A0C4F6E1"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "86DB7435-C47E-426E-B3A6-216089F4CCAD"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F3E07CB0-95EC-4024-B059-4E2180F025A3"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mongodb:mongodb:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D142852D-DF64-4AE5-97A4-FAA0EF3A8AB8"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:-:lts:*:*:*:*:*", "matchCriteriaId": "F5D324C4-97C7-49D3-A809-9EAD4B690C69"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.10:*:*:*:*:*:*:*", "matchCriteriaId": "E2076871-2E80-4605-A470-A41C1A8EC7EE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:13.04:*:*:*:*:*:*:*", "matchCriteriaId": "EFAA48D9-BEB4-4E49-AD50-325C262D46D9"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:opensuse:12.3:*:*:*:*:*:*:*", "matchCriteriaId": "DFBF430B-0832-44B0-AA0E-BA9E467F7668"}]}]}], "references": [{"url": "http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=710597", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-updates/2013-06/msg00180.html", "source": "secalert@redhat.com"}, {"url": "http://seclists.org/oss-sec/2013/q2/447", "source": "secalert@redhat.com"}, {"url": "http://ubuntu.com/usn/usn-1897-1", "source": "secalert@redhat.com", "tags": ["Vendor Advisory"]}, {"url": "http://www.debian.org/security/2013/dsa-2705", "source": "secalert@redhat.com"}, {"url": "http://www.securityfocus.com/bid/60252", "source": "secalert@redhat.com"}, {"url": "https://github.com/mongodb/mongo-python-driver/commit/a060c15ef87e0f0e72974c7c0e57fe811bbd06a2", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://jira.mongodb.org/browse/PYTHON-532", "source": "secalert@redhat.com"}]}, "github_commit_url": "https://github.com/mongodb/mongo-python-driver/commit/a060c15ef87e0f0e72974c7c0e57fe811bbd06a2"}}