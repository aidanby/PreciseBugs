{"buggy_code": ["\t\t\t     Event Histograms\n\n\t\t    Documentation written by Tom Zanussi\n\n1. Introduction\n===============\n\n  Histogram triggers are special event triggers that can be used to\n  aggregate trace event data into histograms.  For information on\n  trace events and event triggers, see Documentation/trace/events.rst.\n\n\n2. Histogram Trigger Command\n============================\n\n  A histogram trigger command is an event trigger command that\n  aggregates event hits into a hash table keyed on one or more trace\n  event format fields (or stacktrace) and a set of running totals\n  derived from one or more trace event format fields and/or event\n  counts (hitcount).\n\n  The format of a hist trigger is as follows:\n\n        hist:keys=<field1[,field2,...]>[:values=<field1[,field2,...]>]\n          [:sort=<field1[,field2,...]>][:size=#entries][:pause][:continue]\n          [:clear][:name=histname1] [if <filter>]\n\n  When a matching event is hit, an entry is added to a hash table\n  using the key(s) and value(s) named.  Keys and values correspond to\n  fields in the event's format description.  Values must correspond to\n  numeric fields - on an event hit, the value(s) will be added to a\n  sum kept for that field.  The special string 'hitcount' can be used\n  in place of an explicit value field - this is simply a count of\n  event hits.  If 'values' isn't specified, an implicit 'hitcount'\n  value will be automatically created and used as the only value.\n  Keys can be any field, or the special string 'stacktrace', which\n  will use the event's kernel stacktrace as the key.  The keywords\n  'keys' or 'key' can be used to specify keys, and the keywords\n  'values', 'vals', or 'val' can be used to specify values.  Compound\n  keys consisting of up to two fields can be specified by the 'keys'\n  keyword.  Hashing a compound key produces a unique entry in the\n  table for each unique combination of component keys, and can be\n  useful for providing more fine-grained summaries of event data.\n  Additionally, sort keys consisting of up to two fields can be\n  specified by the 'sort' keyword.  If more than one field is\n  specified, the result will be a 'sort within a sort': the first key\n  is taken to be the primary sort key and the second the secondary\n  key.  If a hist trigger is given a name using the 'name' parameter,\n  its histogram data will be shared with other triggers of the same\n  name, and trigger hits will update this common data.  Only triggers\n  with 'compatible' fields can be combined in this way; triggers are\n  'compatible' if the fields named in the trigger share the same\n  number and type of fields and those fields also have the same names.\n  Note that any two events always share the compatible 'hitcount' and\n  'stacktrace' fields and can therefore be combined using those\n  fields, however pointless that may be.\n\n  'hist' triggers add a 'hist' file to each event's subdirectory.\n  Reading the 'hist' file for the event will dump the hash table in\n  its entirety to stdout.  If there are multiple hist triggers\n  attached to an event, there will be a table for each trigger in the\n  output.  The table displayed for a named trigger will be the same as\n  any other instance having the same name. Each printed hash table\n  entry is a simple list of the keys and values comprising the entry;\n  keys are printed first and are delineated by curly braces, and are\n  followed by the set of value fields for the entry.  By default,\n  numeric fields are displayed as base-10 integers.  This can be\n  modified by appending any of the following modifiers to the field\n  name:\n\n        .hex        display a number as a hex value\n\t.sym        display an address as a symbol\n\t.sym-offset display an address as a symbol and offset\n\t.syscall    display a syscall id as a system call name\n\t.execname   display a common_pid as a program name\n\t.log2       display log2 value rather than raw number\n\t.usecs      display a common_timestamp in microseconds\n\n  Note that in general the semantics of a given field aren't\n  interpreted when applying a modifier to it, but there are some\n  restrictions to be aware of in this regard:\n\n    - only the 'hex' modifier can be used for values (because values\n      are essentially sums, and the other modifiers don't make sense\n      in that context).\n    - the 'execname' modifier can only be used on a 'common_pid'.  The\n      reason for this is that the execname is simply the 'comm' value\n      saved for the 'current' process when an event was triggered,\n      which is the same as the common_pid value saved by the event\n      tracing code.  Trying to apply that comm value to other pid\n      values wouldn't be correct, and typically events that care save\n      pid-specific comm fields in the event itself.\n\n  A typical usage scenario would be the following to enable a hist\n  trigger, read its current contents, and then turn it off:\n\n  # echo 'hist:keys=skbaddr.hex:vals=len' > \\\n    /sys/kernel/debug/tracing/events/net/netif_rx/trigger\n\n  # cat /sys/kernel/debug/tracing/events/net/netif_rx/hist\n\n  # echo '!hist:keys=skbaddr.hex:vals=len' > \\\n    /sys/kernel/debug/tracing/events/net/netif_rx/trigger\n\n  The trigger file itself can be read to show the details of the\n  currently attached hist trigger.  This information is also displayed\n  at the top of the 'hist' file when read.\n\n  By default, the size of the hash table is 2048 entries.  The 'size'\n  parameter can be used to specify more or fewer than that.  The units\n  are in terms of hashtable entries - if a run uses more entries than\n  specified, the results will show the number of 'drops', the number\n  of hits that were ignored.  The size should be a power of 2 between\n  128 and 131072 (any non- power-of-2 number specified will be rounded\n  up).\n\n  The 'sort' parameter can be used to specify a value field to sort\n  on.  The default if unspecified is 'hitcount' and the default sort\n  order is 'ascending'.  To sort in the opposite direction, append\n  .descending' to the sort key.\n\n  The 'pause' parameter can be used to pause an existing hist trigger\n  or to start a hist trigger but not log any events until told to do\n  so.  'continue' or 'cont' can be used to start or restart a paused\n  hist trigger.\n\n  The 'clear' parameter will clear the contents of a running hist\n  trigger and leave its current paused/active state.\n\n  Note that the 'pause', 'cont', and 'clear' parameters should be\n  applied using 'append' shell operator ('>>') if applied to an\n  existing trigger, rather than via the '>' operator, which will cause\n  the trigger to be removed through truncation.\n\n- enable_hist/disable_hist\n\n  The enable_hist and disable_hist triggers can be used to have one\n  event conditionally start and stop another event's already-attached\n  hist trigger.  Any number of enable_hist and disable_hist triggers\n  can be attached to a given event, allowing that event to kick off\n  and stop aggregations on a host of other events.\n\n  The format is very similar to the enable/disable_event triggers:\n\n      enable_hist:<system>:<event>[:count]\n      disable_hist:<system>:<event>[:count]\n\n  Instead of enabling or disabling the tracing of the target event\n  into the trace buffer as the enable/disable_event triggers do, the\n  enable/disable_hist triggers enable or disable the aggregation of\n  the target event into a hash table.\n\n  A typical usage scenario for the enable_hist/disable_hist triggers\n  would be to first set up a paused hist trigger on some event,\n  followed by an enable_hist/disable_hist pair that turns the hist\n  aggregation on and off when conditions of interest are hit:\n\n  # echo 'hist:keys=skbaddr.hex:vals=len:pause' > \\\n    /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n\n  # echo 'enable_hist:net:netif_receive_skb if filename==/usr/bin/wget' > \\\n    /sys/kernel/debug/tracing/events/sched/sched_process_exec/trigger\n\n  # echo 'disable_hist:net:netif_receive_skb if comm==wget' > \\\n    /sys/kernel/debug/tracing/events/sched/sched_process_exit/trigger\n\n  The above sets up an initially paused hist trigger which is unpaused\n  and starts aggregating events when a given program is executed, and\n  which stops aggregating when the process exits and the hist trigger\n  is paused again.\n\n  The examples below provide a more concrete illustration of the\n  concepts and typical usage patterns discussed above.\n\n  'special' event fields\n  ------------------------\n\n  There are a number of 'special event fields' available for use as\n  keys or values in a hist trigger.  These look like and behave as if\n  they were actual event fields, but aren't really part of the event's\n  field definition or format file.  They are however available for any\n  event, and can be used anywhere an actual event field could be.\n  They are:\n\n    common_timestamp       u64 - timestamp (from ring buffer) associated\n                                 with the event, in nanoseconds.  May be\n\t\t\t\t modified by .usecs to have timestamps\n\t\t\t\t interpreted as microseconds.\n    cpu                    int - the cpu on which the event occurred.\n\n  Extended error information\n  --------------------------\n\n  For some error conditions encountered when invoking a hist trigger\n  command, extended error information is available via the\n  corresponding event's 'hist' file.  Reading the hist file after an\n  error will display more detailed information about what went wrong,\n  if information is available.  This extended error information will\n  be available until the next hist trigger command for that event.\n\n  If available for a given error condition, the extended error\n  information and usage takes the following form:\n\n    # echo xxx > /sys/kernel/debug/tracing/events/sched/sched_wakeup/trigger\n    echo: write error: Invalid argument\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_wakeup/hist\n    ERROR: Couldn't yyy: zzz\n      Last command: xxx\n\n6.2 'hist' trigger examples\n---------------------------\n\n  The first set of examples creates aggregations using the kmalloc\n  event.  The fields that can be used for the hist trigger are listed\n  in the kmalloc event's format file:\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/format\n    name: kmalloc\n    ID: 374\n    format:\n\tfield:unsigned short common_type;\toffset:0;\tsize:2;\tsigned:0;\n\tfield:unsigned char common_flags;\toffset:2;\tsize:1;\tsigned:0;\n\tfield:unsigned char common_preempt_count;\t\toffset:3;\tsize:1;\tsigned:0;\n\tfield:int common_pid;\t\t\t\t\toffset:4;\tsize:4;\tsigned:1;\n\n\tfield:unsigned long call_site;\t\t\t\toffset:8;\tsize:8;\tsigned:0;\n\tfield:const void * ptr;\t\t\t\t\toffset:16;\tsize:8;\tsigned:0;\n\tfield:size_t bytes_req;\t\t\t\t\toffset:24;\tsize:8;\tsigned:0;\n\tfield:size_t bytes_alloc;\t\t\t\toffset:32;\tsize:8;\tsigned:0;\n\tfield:gfp_t gfp_flags;\t\t\t\t\toffset:40;\tsize:4;\tsigned:0;\n\n  We'll start by creating a hist trigger that generates a simple table\n  that lists the total number of bytes requested for each function in\n  the kernel that made one or more calls to kmalloc:\n\n    # echo 'hist:key=call_site:val=bytes_req' > \\\n            /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n  This tells the tracing system to create a 'hist' trigger using the\n  call_site field of the kmalloc event as the key for the table, which\n  just means that each unique call_site address will have an entry\n  created for it in the table.  The 'val=bytes_req' parameter tells\n  the hist trigger that for each unique entry (call_site) in the\n  table, it should keep a running total of the number of bytes\n  requested by that call_site.\n\n  We'll let it run for awhile and then dump the contents of the 'hist'\n  file in the kmalloc event's subdirectory (for readability, a number\n  of entries have been omitted):\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site:vals=bytes_req:sort=hitcount:size=2048 [active]\n\n    { call_site: 18446744072106379007 } hitcount:          1  bytes_req:        176\n    { call_site: 18446744071579557049 } hitcount:          1  bytes_req:       1024\n    { call_site: 18446744071580608289 } hitcount:          1  bytes_req:      16384\n    { call_site: 18446744071581827654 } hitcount:          1  bytes_req:         24\n    { call_site: 18446744071580700980 } hitcount:          1  bytes_req:          8\n    { call_site: 18446744071579359876 } hitcount:          1  bytes_req:        152\n    { call_site: 18446744071580795365 } hitcount:          3  bytes_req:        144\n    { call_site: 18446744071581303129 } hitcount:          3  bytes_req:        144\n    { call_site: 18446744071580713234 } hitcount:          4  bytes_req:       2560\n    { call_site: 18446744071580933750 } hitcount:          4  bytes_req:        736\n    .\n    .\n    .\n    { call_site: 18446744072106047046 } hitcount:         69  bytes_req:       5576\n    { call_site: 18446744071582116407 } hitcount:         73  bytes_req:       2336\n    { call_site: 18446744072106054684 } hitcount:        136  bytes_req:     140504\n    { call_site: 18446744072106224230 } hitcount:        136  bytes_req:      19584\n    { call_site: 18446744072106078074 } hitcount:        153  bytes_req:       2448\n    { call_site: 18446744072106062406 } hitcount:        153  bytes_req:      36720\n    { call_site: 18446744071582507929 } hitcount:        153  bytes_req:      37088\n    { call_site: 18446744072102520590 } hitcount:        273  bytes_req:      10920\n    { call_site: 18446744071582143559 } hitcount:        358  bytes_req:        716\n    { call_site: 18446744072106465852 } hitcount:        417  bytes_req:      56712\n    { call_site: 18446744072102523378 } hitcount:        485  bytes_req:      27160\n    { call_site: 18446744072099568646 } hitcount:       1676  bytes_req:      33520\n\n    Totals:\n        Hits: 4610\n        Entries: 45\n        Dropped: 0\n\n  The output displays a line for each entry, beginning with the key\n  specified in the trigger, followed by the value(s) also specified in\n  the trigger.  At the beginning of the output is a line that displays\n  the trigger info, which can also be displayed by reading the\n  'trigger' file:\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n    hist:keys=call_site:vals=bytes_req:sort=hitcount:size=2048 [active]\n\n  At the end of the output are a few lines that display the overall\n  totals for the run.  The 'Hits' field shows the total number of\n  times the event trigger was hit, the 'Entries' field shows the total\n  number of used entries in the hash table, and the 'Dropped' field\n  shows the number of hits that were dropped because the number of\n  used entries for the run exceeded the maximum number of entries\n  allowed for the table (normally 0, but if not a hint that you may\n  want to increase the size of the table using the 'size' parameter).\n\n  Notice in the above output that there's an extra field, 'hitcount',\n  which wasn't specified in the trigger.  Also notice that in the\n  trigger info output, there's a parameter, 'sort=hitcount', which\n  wasn't specified in the trigger either.  The reason for that is that\n  every trigger implicitly keeps a count of the total number of hits\n  attributed to a given entry, called the 'hitcount'.  That hitcount\n  information is explicitly displayed in the output, and in the\n  absence of a user-specified sort parameter, is used as the default\n  sort field.\n\n  The value 'hitcount' can be used in place of an explicit value in\n  the 'values' parameter if you don't really need to have any\n  particular field summed and are mainly interested in hit\n  frequencies.\n\n  To turn the hist trigger off, simply call up the trigger in the\n  command history and re-execute it with a '!' prepended:\n\n    # echo '!hist:key=call_site:val=bytes_req' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n  Finally, notice that the call_site as displayed in the output above\n  isn't really very useful.  It's an address, but normally addresses\n  are displayed in hex.  To have a numeric field displayed as a hex\n  value, simply append '.hex' to the field name in the trigger:\n\n    # echo 'hist:key=call_site.hex:val=bytes_req' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.hex:vals=bytes_req:sort=hitcount:size=2048 [active]\n\n    { call_site: ffffffffa026b291 } hitcount:          1  bytes_req:        433\n    { call_site: ffffffffa07186ff } hitcount:          1  bytes_req:        176\n    { call_site: ffffffff811ae721 } hitcount:          1  bytes_req:      16384\n    { call_site: ffffffff811c5134 } hitcount:          1  bytes_req:          8\n    { call_site: ffffffffa04a9ebb } hitcount:          1  bytes_req:        511\n    { call_site: ffffffff8122e0a6 } hitcount:          1  bytes_req:         12\n    { call_site: ffffffff8107da84 } hitcount:          1  bytes_req:        152\n    { call_site: ffffffff812d8246 } hitcount:          1  bytes_req:         24\n    { call_site: ffffffff811dc1e5 } hitcount:          3  bytes_req:        144\n    { call_site: ffffffffa02515e8 } hitcount:          3  bytes_req:        648\n    { call_site: ffffffff81258159 } hitcount:          3  bytes_req:        144\n    { call_site: ffffffff811c80f4 } hitcount:          4  bytes_req:        544\n    .\n    .\n    .\n    { call_site: ffffffffa06c7646 } hitcount:        106  bytes_req:       8024\n    { call_site: ffffffffa06cb246 } hitcount:        132  bytes_req:      31680\n    { call_site: ffffffffa06cef7a } hitcount:        132  bytes_req:       2112\n    { call_site: ffffffff8137e399 } hitcount:        132  bytes_req:      23232\n    { call_site: ffffffffa06c941c } hitcount:        185  bytes_req:     171360\n    { call_site: ffffffffa06f2a66 } hitcount:        185  bytes_req:      26640\n    { call_site: ffffffffa036a70e } hitcount:        265  bytes_req:      10600\n    { call_site: ffffffff81325447 } hitcount:        292  bytes_req:        584\n    { call_site: ffffffffa072da3c } hitcount:        446  bytes_req:      60656\n    { call_site: ffffffffa036b1f2 } hitcount:        526  bytes_req:      29456\n    { call_site: ffffffffa0099c06 } hitcount:       1780  bytes_req:      35600\n\n    Totals:\n        Hits: 4775\n        Entries: 46\n        Dropped: 0\n\n  Even that's only marginally more useful - while hex values do look\n  more like addresses, what users are typically more interested in\n  when looking at text addresses are the corresponding symbols\n  instead.  To have an address displayed as symbolic value instead,\n  simply append '.sym' or '.sym-offset' to the field name in the\n  trigger:\n\n    # echo 'hist:key=call_site.sym:val=bytes_req' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.sym:vals=bytes_req:sort=hitcount:size=2048 [active]\n\n    { call_site: [ffffffff810adcb9] syslog_print_all                              } hitcount:          1  bytes_req:       1024\n    { call_site: [ffffffff8154bc62] usb_control_msg                               } hitcount:          1  bytes_req:          8\n    { call_site: [ffffffffa00bf6fe] hidraw_send_report [hid]                      } hitcount:          1  bytes_req:          7\n    { call_site: [ffffffff8154acbe] usb_alloc_urb                                 } hitcount:          1  bytes_req:        192\n    { call_site: [ffffffffa00bf1ca] hidraw_report_event [hid]                     } hitcount:          1  bytes_req:          7\n    { call_site: [ffffffff811e3a25] __seq_open_private                            } hitcount:          1  bytes_req:         40\n    { call_site: [ffffffff8109524a] alloc_fair_sched_group                        } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffff811febd5] fsnotify_alloc_group                          } hitcount:          2  bytes_req:        528\n    { call_site: [ffffffff81440f58] __tty_buffer_request_room                     } hitcount:          2  bytes_req:       2624\n    { call_site: [ffffffff81200ba6] inotify_new_group                             } hitcount:          2  bytes_req:         96\n    { call_site: [ffffffffa05e19af] ieee80211_start_tx_ba_session [mac80211]      } hitcount:          2  bytes_req:        464\n    { call_site: [ffffffff81672406] tcp_get_metrics                               } hitcount:          2  bytes_req:        304\n    { call_site: [ffffffff81097ec2] alloc_rt_sched_group                          } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffff81089b05] sched_create_group                            } hitcount:          2  bytes_req:       1424\n    .\n    .\n    .\n    { call_site: [ffffffffa04a580c] intel_crtc_page_flip [i915]                   } hitcount:       1185  bytes_req:     123240\n    { call_site: [ffffffffa0287592] drm_mode_page_flip_ioctl [drm]                } hitcount:       1185  bytes_req:     104280\n    { call_site: [ffffffffa04c4a3c] intel_plane_duplicate_state [i915]            } hitcount:       1402  bytes_req:     190672\n    { call_site: [ffffffff812891ca] ext4_find_extent                              } hitcount:       1518  bytes_req:     146208\n    { call_site: [ffffffffa029070e] drm_vma_node_allow [drm]                      } hitcount:       1746  bytes_req:      69840\n    { call_site: [ffffffffa045e7c4] i915_gem_do_execbuffer.isra.23 [i915]         } hitcount:       2021  bytes_req:     792312\n    { call_site: [ffffffffa02911f2] drm_modeset_lock_crtc [drm]                   } hitcount:       2592  bytes_req:     145152\n    { call_site: [ffffffffa0489a66] intel_ring_begin [i915]                       } hitcount:       2629  bytes_req:     378576\n    { call_site: [ffffffffa046041c] i915_gem_execbuffer2 [i915]                   } hitcount:       2629  bytes_req:    3783248\n    { call_site: [ffffffff81325607] apparmor_file_alloc_security                  } hitcount:       5192  bytes_req:      10384\n    { call_site: [ffffffffa00b7c06] hid_report_raw_event [hid]                    } hitcount:       5529  bytes_req:     110584\n    { call_site: [ffffffff8131ebf7] aa_alloc_task_context                         } hitcount:      21943  bytes_req:     702176\n    { call_site: [ffffffff8125847d] ext4_htree_store_dirent                       } hitcount:      55759  bytes_req:    5074265\n\n    Totals:\n        Hits: 109928\n        Entries: 71\n        Dropped: 0\n\n  Because the default sort key above is 'hitcount', the above shows a\n  the list of call_sites by increasing hitcount, so that at the bottom\n  we see the functions that made the most kmalloc calls during the\n  run.  If instead we we wanted to see the top kmalloc callers in\n  terms of the number of bytes requested rather than the number of\n  calls, and we wanted the top caller to appear at the top, we can use\n  the 'sort' parameter, along with the 'descending' modifier:\n\n    # echo 'hist:key=call_site.sym:val=bytes_req:sort=bytes_req.descending' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.sym:vals=bytes_req:sort=bytes_req.descending:size=2048 [active]\n\n    { call_site: [ffffffffa046041c] i915_gem_execbuffer2 [i915]                   } hitcount:       2186  bytes_req:    3397464\n    { call_site: [ffffffffa045e7c4] i915_gem_do_execbuffer.isra.23 [i915]         } hitcount:       1790  bytes_req:     712176\n    { call_site: [ffffffff8125847d] ext4_htree_store_dirent                       } hitcount:       8132  bytes_req:     513135\n    { call_site: [ffffffff811e2a1b] seq_buf_alloc                                 } hitcount:        106  bytes_req:     440128\n    { call_site: [ffffffffa0489a66] intel_ring_begin [i915]                       } hitcount:       2186  bytes_req:     314784\n    { call_site: [ffffffff812891ca] ext4_find_extent                              } hitcount:       2174  bytes_req:     208992\n    { call_site: [ffffffff811ae8e1] __kmalloc                                     } hitcount:          8  bytes_req:     131072\n    { call_site: [ffffffffa04c4a3c] intel_plane_duplicate_state [i915]            } hitcount:        859  bytes_req:     116824\n    { call_site: [ffffffffa02911f2] drm_modeset_lock_crtc [drm]                   } hitcount:       1834  bytes_req:     102704\n    { call_site: [ffffffffa04a580c] intel_crtc_page_flip [i915]                   } hitcount:        972  bytes_req:     101088\n    { call_site: [ffffffffa0287592] drm_mode_page_flip_ioctl [drm]                } hitcount:        972  bytes_req:      85536\n    { call_site: [ffffffffa00b7c06] hid_report_raw_event [hid]                    } hitcount:       3333  bytes_req:      66664\n    { call_site: [ffffffff8137e559] sg_kmalloc                                    } hitcount:        209  bytes_req:      61632\n    .\n    .\n    .\n    { call_site: [ffffffff81095225] alloc_fair_sched_group                        } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffff81097ec2] alloc_rt_sched_group                          } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffff812d8406] copy_semundo                                  } hitcount:          2  bytes_req:         48\n    { call_site: [ffffffff81200ba6] inotify_new_group                             } hitcount:          1  bytes_req:         48\n    { call_site: [ffffffffa027121a] drm_getmagic [drm]                            } hitcount:          1  bytes_req:         48\n    { call_site: [ffffffff811e3a25] __seq_open_private                            } hitcount:          1  bytes_req:         40\n    { call_site: [ffffffff811c52f4] bprm_change_interp                            } hitcount:          2  bytes_req:         16\n    { call_site: [ffffffff8154bc62] usb_control_msg                               } hitcount:          1  bytes_req:          8\n    { call_site: [ffffffffa00bf1ca] hidraw_report_event [hid]                     } hitcount:          1  bytes_req:          7\n    { call_site: [ffffffffa00bf6fe] hidraw_send_report [hid]                      } hitcount:          1  bytes_req:          7\n\n    Totals:\n        Hits: 32133\n        Entries: 81\n        Dropped: 0\n\n  To display the offset and size information in addition to the symbol\n  name, just use 'sym-offset' instead:\n\n    # echo 'hist:key=call_site.sym-offset:val=bytes_req:sort=bytes_req.descending' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.sym-offset:vals=bytes_req:sort=bytes_req.descending:size=2048 [active]\n\n    { call_site: [ffffffffa046041c] i915_gem_execbuffer2+0x6c/0x2c0 [i915]                  } hitcount:       4569  bytes_req:    3163720\n    { call_site: [ffffffffa0489a66] intel_ring_begin+0xc6/0x1f0 [i915]                      } hitcount:       4569  bytes_req:     657936\n    { call_site: [ffffffffa045e7c4] i915_gem_do_execbuffer.isra.23+0x694/0x1020 [i915]      } hitcount:       1519  bytes_req:     472936\n    { call_site: [ffffffffa045e646] i915_gem_do_execbuffer.isra.23+0x516/0x1020 [i915]      } hitcount:       3050  bytes_req:     211832\n    { call_site: [ffffffff811e2a1b] seq_buf_alloc+0x1b/0x50                                 } hitcount:         34  bytes_req:     148384\n    { call_site: [ffffffffa04a580c] intel_crtc_page_flip+0xbc/0x870 [i915]                  } hitcount:       1385  bytes_req:     144040\n    { call_site: [ffffffff811ae8e1] __kmalloc+0x191/0x1b0                                   } hitcount:          8  bytes_req:     131072\n    { call_site: [ffffffffa0287592] drm_mode_page_flip_ioctl+0x282/0x360 [drm]              } hitcount:       1385  bytes_req:     121880\n    { call_site: [ffffffffa02911f2] drm_modeset_lock_crtc+0x32/0x100 [drm]                  } hitcount:       1848  bytes_req:     103488\n    { call_site: [ffffffffa04c4a3c] intel_plane_duplicate_state+0x2c/0xa0 [i915]            } hitcount:        461  bytes_req:      62696\n    { call_site: [ffffffffa029070e] drm_vma_node_allow+0x2e/0xd0 [drm]                      } hitcount:       1541  bytes_req:      61640\n    { call_site: [ffffffff815f8d7b] sk_prot_alloc+0xcb/0x1b0                                } hitcount:         57  bytes_req:      57456\n    .\n    .\n    .\n    { call_site: [ffffffff8109524a] alloc_fair_sched_group+0x5a/0x1a0                       } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffffa027b921] drm_vm_open_locked+0x31/0xa0 [drm]                      } hitcount:          3  bytes_req:         96\n    { call_site: [ffffffff8122e266] proc_self_follow_link+0x76/0xb0                         } hitcount:          8  bytes_req:         96\n    { call_site: [ffffffff81213e80] load_elf_binary+0x240/0x1650                            } hitcount:          3  bytes_req:         84\n    { call_site: [ffffffff8154bc62] usb_control_msg+0x42/0x110                              } hitcount:          1  bytes_req:          8\n    { call_site: [ffffffffa00bf6fe] hidraw_send_report+0x7e/0x1a0 [hid]                     } hitcount:          1  bytes_req:          7\n    { call_site: [ffffffffa00bf1ca] hidraw_report_event+0x8a/0x120 [hid]                    } hitcount:          1  bytes_req:          7\n\n    Totals:\n        Hits: 26098\n        Entries: 64\n        Dropped: 0\n\n  We can also add multiple fields to the 'values' parameter.  For\n  example, we might want to see the total number of bytes allocated\n  alongside bytes requested, and display the result sorted by bytes\n  allocated in a descending order:\n\n    # echo 'hist:keys=call_site.sym:values=bytes_req,bytes_alloc:sort=bytes_alloc.descending' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.sym:vals=bytes_req,bytes_alloc:sort=bytes_alloc.descending:size=2048 [active]\n\n    { call_site: [ffffffffa046041c] i915_gem_execbuffer2 [i915]                   } hitcount:       7403  bytes_req:    4084360  bytes_alloc:    5958016\n    { call_site: [ffffffff811e2a1b] seq_buf_alloc                                 } hitcount:        541  bytes_req:    2213968  bytes_alloc:    2228224\n    { call_site: [ffffffffa0489a66] intel_ring_begin [i915]                       } hitcount:       7404  bytes_req:    1066176  bytes_alloc:    1421568\n    { call_site: [ffffffffa045e7c4] i915_gem_do_execbuffer.isra.23 [i915]         } hitcount:       1565  bytes_req:     557368  bytes_alloc:    1037760\n    { call_site: [ffffffff8125847d] ext4_htree_store_dirent                       } hitcount:       9557  bytes_req:     595778  bytes_alloc:     695744\n    { call_site: [ffffffffa045e646] i915_gem_do_execbuffer.isra.23 [i915]         } hitcount:       5839  bytes_req:     430680  bytes_alloc:     470400\n    { call_site: [ffffffffa04c4a3c] intel_plane_duplicate_state [i915]            } hitcount:       2388  bytes_req:     324768  bytes_alloc:     458496\n    { call_site: [ffffffffa02911f2] drm_modeset_lock_crtc [drm]                   } hitcount:       3911  bytes_req:     219016  bytes_alloc:     250304\n    { call_site: [ffffffff815f8d7b] sk_prot_alloc                                 } hitcount:        235  bytes_req:     236880  bytes_alloc:     240640\n    { call_site: [ffffffff8137e559] sg_kmalloc                                    } hitcount:        557  bytes_req:     169024  bytes_alloc:     221760\n    { call_site: [ffffffffa00b7c06] hid_report_raw_event [hid]                    } hitcount:       9378  bytes_req:     187548  bytes_alloc:     206312\n    { call_site: [ffffffffa04a580c] intel_crtc_page_flip [i915]                   } hitcount:       1519  bytes_req:     157976  bytes_alloc:     194432\n    .\n    .\n    .\n    { call_site: [ffffffff8109bd3b] sched_autogroup_create_attach                 } hitcount:          2  bytes_req:        144  bytes_alloc:        192\n    { call_site: [ffffffff81097ee8] alloc_rt_sched_group                          } hitcount:          2  bytes_req:        128  bytes_alloc:        128\n    { call_site: [ffffffff8109524a] alloc_fair_sched_group                        } hitcount:          2  bytes_req:        128  bytes_alloc:        128\n    { call_site: [ffffffff81095225] alloc_fair_sched_group                        } hitcount:          2  bytes_req:        128  bytes_alloc:        128\n    { call_site: [ffffffff81097ec2] alloc_rt_sched_group                          } hitcount:          2  bytes_req:        128  bytes_alloc:        128\n    { call_site: [ffffffff81213e80] load_elf_binary                               } hitcount:          3  bytes_req:         84  bytes_alloc:         96\n    { call_site: [ffffffff81079a2e] kthread_create_on_node                        } hitcount:          1  bytes_req:         56  bytes_alloc:         64\n    { call_site: [ffffffffa00bf6fe] hidraw_send_report [hid]                      } hitcount:          1  bytes_req:          7  bytes_alloc:          8\n    { call_site: [ffffffff8154bc62] usb_control_msg                               } hitcount:          1  bytes_req:          8  bytes_alloc:          8\n    { call_site: [ffffffffa00bf1ca] hidraw_report_event [hid]                     } hitcount:          1  bytes_req:          7  bytes_alloc:          8\n\n    Totals:\n        Hits: 66598\n        Entries: 65\n        Dropped: 0\n\n  Finally, to finish off our kmalloc example, instead of simply having\n  the hist trigger display symbolic call_sites, we can have the hist\n  trigger additionally display the complete set of kernel stack traces\n  that led to each call_site.  To do that, we simply use the special\n  value 'stacktrace' for the key parameter:\n\n    # echo 'hist:keys=stacktrace:values=bytes_req,bytes_alloc:sort=bytes_alloc' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n  The above trigger will use the kernel stack trace in effect when an\n  event is triggered as the key for the hash table.  This allows the\n  enumeration of every kernel callpath that led up to a particular\n  event, along with a running total of any of the event fields for\n  that event.  Here we tally bytes requested and bytes allocated for\n  every callpath in the system that led up to a kmalloc (in this case\n  every callpath to a kmalloc for a kernel compile):\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=stacktrace:vals=bytes_req,bytes_alloc:sort=bytes_alloc:size=2048 [active]\n\n    { stacktrace:\n         __kmalloc_track_caller+0x10b/0x1a0\n         kmemdup+0x20/0x50\n         hidraw_report_event+0x8a/0x120 [hid]\n         hid_report_raw_event+0x3ea/0x440 [hid]\n         hid_input_report+0x112/0x190 [hid]\n         hid_irq_in+0xc2/0x260 [usbhid]\n         __usb_hcd_giveback_urb+0x72/0x120\n         usb_giveback_urb_bh+0x9e/0xe0\n         tasklet_hi_action+0xf8/0x100\n         __do_softirq+0x114/0x2c0\n         irq_exit+0xa5/0xb0\n         do_IRQ+0x5a/0xf0\n         ret_from_intr+0x0/0x30\n         cpuidle_enter+0x17/0x20\n         cpu_startup_entry+0x315/0x3e0\n         rest_init+0x7c/0x80\n    } hitcount:          3  bytes_req:         21  bytes_alloc:         24\n    { stacktrace:\n         __kmalloc_track_caller+0x10b/0x1a0\n         kmemdup+0x20/0x50\n         hidraw_report_event+0x8a/0x120 [hid]\n         hid_report_raw_event+0x3ea/0x440 [hid]\n         hid_input_report+0x112/0x190 [hid]\n         hid_irq_in+0xc2/0x260 [usbhid]\n         __usb_hcd_giveback_urb+0x72/0x120\n         usb_giveback_urb_bh+0x9e/0xe0\n         tasklet_hi_action+0xf8/0x100\n         __do_softirq+0x114/0x2c0\n         irq_exit+0xa5/0xb0\n         do_IRQ+0x5a/0xf0\n         ret_from_intr+0x0/0x30\n    } hitcount:          3  bytes_req:         21  bytes_alloc:         24\n    { stacktrace:\n         kmem_cache_alloc_trace+0xeb/0x150\n         aa_alloc_task_context+0x27/0x40\n         apparmor_cred_prepare+0x1f/0x50\n         security_prepare_creds+0x16/0x20\n         prepare_creds+0xdf/0x1a0\n         SyS_capset+0xb5/0x200\n         system_call_fastpath+0x12/0x6a\n    } hitcount:          1  bytes_req:         32  bytes_alloc:         32\n    .\n    .\n    .\n    { stacktrace:\n         __kmalloc+0x11b/0x1b0\n         i915_gem_execbuffer2+0x6c/0x2c0 [i915]\n         drm_ioctl+0x349/0x670 [drm]\n         do_vfs_ioctl+0x2f0/0x4f0\n         SyS_ioctl+0x81/0xa0\n         system_call_fastpath+0x12/0x6a\n    } hitcount:      17726  bytes_req:   13944120  bytes_alloc:   19593808\n    { stacktrace:\n         __kmalloc+0x11b/0x1b0\n         load_elf_phdrs+0x76/0xa0\n         load_elf_binary+0x102/0x1650\n         search_binary_handler+0x97/0x1d0\n         do_execveat_common.isra.34+0x551/0x6e0\n         SyS_execve+0x3a/0x50\n         return_from_execve+0x0/0x23\n    } hitcount:      33348  bytes_req:   17152128  bytes_alloc:   20226048\n    { stacktrace:\n         kmem_cache_alloc_trace+0xeb/0x150\n         apparmor_file_alloc_security+0x27/0x40\n         security_file_alloc+0x16/0x20\n         get_empty_filp+0x93/0x1c0\n         path_openat+0x31/0x5f0\n         do_filp_open+0x3a/0x90\n         do_sys_open+0x128/0x220\n         SyS_open+0x1e/0x20\n         system_call_fastpath+0x12/0x6a\n    } hitcount:    4766422  bytes_req:    9532844  bytes_alloc:   38131376\n    { stacktrace:\n         __kmalloc+0x11b/0x1b0\n         seq_buf_alloc+0x1b/0x50\n         seq_read+0x2cc/0x370\n         proc_reg_read+0x3d/0x80\n         __vfs_read+0x28/0xe0\n         vfs_read+0x86/0x140\n         SyS_read+0x46/0xb0\n         system_call_fastpath+0x12/0x6a\n    } hitcount:      19133  bytes_req:   78368768  bytes_alloc:   78368768\n\n    Totals:\n        Hits: 6085872\n        Entries: 253\n        Dropped: 0\n\n  If you key a hist trigger on common_pid, in order for example to\n  gather and display sorted totals for each process, you can use the\n  special .execname modifier to display the executable names for the\n  processes in the table rather than raw pids.  The example below\n  keeps a per-process sum of total bytes read:\n\n    # echo 'hist:key=common_pid.execname:val=count:sort=count.descending' > \\\n           /sys/kernel/debug/tracing/events/syscalls/sys_enter_read/trigger\n\n    # cat /sys/kernel/debug/tracing/events/syscalls/sys_enter_read/hist\n    # trigger info: hist:keys=common_pid.execname:vals=count:sort=count.descending:size=2048 [active]\n\n    { common_pid: gnome-terminal  [      3196] } hitcount:        280  count:    1093512\n    { common_pid: Xorg            [      1309] } hitcount:        525  count:     256640\n    { common_pid: compiz          [      2889] } hitcount:         59  count:     254400\n    { common_pid: bash            [      8710] } hitcount:          3  count:      66369\n    { common_pid: dbus-daemon-lau [      8703] } hitcount:         49  count:      47739\n    { common_pid: irqbalance      [      1252] } hitcount:         27  count:      27648\n    { common_pid: 01ifupdown      [      8705] } hitcount:          3  count:      17216\n    { common_pid: dbus-daemon     [       772] } hitcount:         10  count:      12396\n    { common_pid: Socket Thread   [      8342] } hitcount:         11  count:      11264\n    { common_pid: nm-dhcp-client. [      8701] } hitcount:          6  count:       7424\n    { common_pid: gmain           [      1315] } hitcount:         18  count:       6336\n    .\n    .\n    .\n    { common_pid: postgres        [      1892] } hitcount:          2  count:         32\n    { common_pid: postgres        [      1891] } hitcount:          2  count:         32\n    { common_pid: gmain           [      8704] } hitcount:          2  count:         32\n    { common_pid: upstart-dbus-br [      2740] } hitcount:         21  count:         21\n    { common_pid: nm-dispatcher.a [      8696] } hitcount:          1  count:         16\n    { common_pid: indicator-datet [      2904] } hitcount:          1  count:         16\n    { common_pid: gdbus           [      2998] } hitcount:          1  count:         16\n    { common_pid: rtkit-daemon    [      2052] } hitcount:          1  count:          8\n    { common_pid: init            [         1] } hitcount:          2  count:          2\n\n    Totals:\n        Hits: 2116\n        Entries: 51\n        Dropped: 0\n\n  Similarly, if you key a hist trigger on syscall id, for example to\n  gather and display a list of systemwide syscall hits, you can use\n  the special .syscall modifier to display the syscall names rather\n  than raw ids.  The example below keeps a running total of syscall\n  counts for the system during the run:\n\n    # echo 'hist:key=id.syscall:val=hitcount' > \\\n           /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/trigger\n\n    # cat /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/hist\n    # trigger info: hist:keys=id.syscall:vals=hitcount:sort=hitcount:size=2048 [active]\n\n    { id: sys_fsync                     [ 74] } hitcount:          1\n    { id: sys_newuname                  [ 63] } hitcount:          1\n    { id: sys_prctl                     [157] } hitcount:          1\n    { id: sys_statfs                    [137] } hitcount:          1\n    { id: sys_symlink                   [ 88] } hitcount:          1\n    { id: sys_sendmmsg                  [307] } hitcount:          1\n    { id: sys_semctl                    [ 66] } hitcount:          1\n    { id: sys_readlink                  [ 89] } hitcount:          3\n    { id: sys_bind                      [ 49] } hitcount:          3\n    { id: sys_getsockname               [ 51] } hitcount:          3\n    { id: sys_unlink                    [ 87] } hitcount:          3\n    { id: sys_rename                    [ 82] } hitcount:          4\n    { id: unknown_syscall               [ 58] } hitcount:          4\n    { id: sys_connect                   [ 42] } hitcount:          4\n    { id: sys_getpid                    [ 39] } hitcount:          4\n    .\n    .\n    .\n    { id: sys_rt_sigprocmask            [ 14] } hitcount:        952\n    { id: sys_futex                     [202] } hitcount:       1534\n    { id: sys_write                     [  1] } hitcount:       2689\n    { id: sys_setitimer                 [ 38] } hitcount:       2797\n    { id: sys_read                      [  0] } hitcount:       3202\n    { id: sys_select                    [ 23] } hitcount:       3773\n    { id: sys_writev                    [ 20] } hitcount:       4531\n    { id: sys_poll                      [  7] } hitcount:       8314\n    { id: sys_recvmsg                   [ 47] } hitcount:      13738\n    { id: sys_ioctl                     [ 16] } hitcount:      21843\n\n    Totals:\n        Hits: 67612\n        Entries: 72\n        Dropped: 0\n\n    The syscall counts above provide a rough overall picture of system\n    call activity on the system; we can see for example that the most\n    popular system call on this system was the 'sys_ioctl' system call.\n\n    We can use 'compound' keys to refine that number and provide some\n    further insight as to which processes exactly contribute to the\n    overall ioctl count.\n\n    The command below keeps a hitcount for every unique combination of\n    system call id and pid - the end result is essentially a table\n    that keeps a per-pid sum of system call hits.  The results are\n    sorted using the system call id as the primary key, and the\n    hitcount sum as the secondary key:\n\n    # echo 'hist:key=id.syscall,common_pid.execname:val=hitcount:sort=id,hitcount' > \\\n           /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/trigger\n\n    # cat /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/hist\n    # trigger info: hist:keys=id.syscall,common_pid.execname:vals=hitcount:sort=id.syscall,hitcount:size=2048 [active]\n\n    { id: sys_read                      [  0], common_pid: rtkit-daemon    [      1877] } hitcount:          1\n    { id: sys_read                      [  0], common_pid: gdbus           [      2976] } hitcount:          1\n    { id: sys_read                      [  0], common_pid: console-kit-dae [      3400] } hitcount:          1\n    { id: sys_read                      [  0], common_pid: postgres        [      1865] } hitcount:          1\n    { id: sys_read                      [  0], common_pid: deja-dup-monito [      3543] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: NetworkManager  [       890] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: evolution-calen [      3048] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: postgres        [      1864] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: nm-applet       [      3022] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: whoopsie        [      1212] } hitcount:          2\n    .\n    .\n    .\n    { id: sys_ioctl                     [ 16], common_pid: bash            [      8479] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: bash            [      3472] } hitcount:         12\n    { id: sys_ioctl                     [ 16], common_pid: gnome-terminal  [      3199] } hitcount:         16\n    { id: sys_ioctl                     [ 16], common_pid: Xorg            [      1267] } hitcount:       1808\n    { id: sys_ioctl                     [ 16], common_pid: compiz          [      2994] } hitcount:       5580\n    .\n    .\n    .\n    { id: sys_waitid                    [247], common_pid: upstart-dbus-br [      2690] } hitcount:          3\n    { id: sys_waitid                    [247], common_pid: upstart-dbus-br [      2688] } hitcount:         16\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [       975] } hitcount:          2\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      3204] } hitcount:          4\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      2888] } hitcount:          4\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      3003] } hitcount:          4\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      2873] } hitcount:          4\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      3196] } hitcount:          6\n    { id: sys_openat                    [257], common_pid: java            [      2623] } hitcount:          2\n    { id: sys_eventfd2                  [290], common_pid: ibus-ui-gtk3    [      2760] } hitcount:          4\n    { id: sys_eventfd2                  [290], common_pid: compiz          [      2994] } hitcount:          6\n\n    Totals:\n        Hits: 31536\n        Entries: 323\n        Dropped: 0\n\n    The above list does give us a breakdown of the ioctl syscall by\n    pid, but it also gives us quite a bit more than that, which we\n    don't really care about at the moment.  Since we know the syscall\n    id for sys_ioctl (16, displayed next to the sys_ioctl name), we\n    can use that to filter out all the other syscalls:\n\n    # echo 'hist:key=id.syscall,common_pid.execname:val=hitcount:sort=id,hitcount if id == 16' > \\\n           /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/trigger\n\n    # cat /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/hist\n    # trigger info: hist:keys=id.syscall,common_pid.execname:vals=hitcount:sort=id.syscall,hitcount:size=2048 if id == 16 [active]\n\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2769] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: evolution-addre [      8571] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      3003] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2781] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2829] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: bash            [      8726] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: bash            [      8508] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2970] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2768] } hitcount:          1\n    .\n    .\n    .\n    { id: sys_ioctl                     [ 16], common_pid: pool            [      8559] } hitcount:         45\n    { id: sys_ioctl                     [ 16], common_pid: pool            [      8555] } hitcount:         48\n    { id: sys_ioctl                     [ 16], common_pid: pool            [      8551] } hitcount:         48\n    { id: sys_ioctl                     [ 16], common_pid: avahi-daemon    [       896] } hitcount:         66\n    { id: sys_ioctl                     [ 16], common_pid: Xorg            [      1267] } hitcount:      26674\n    { id: sys_ioctl                     [ 16], common_pid: compiz          [      2994] } hitcount:      73443\n\n    Totals:\n        Hits: 101162\n        Entries: 103\n        Dropped: 0\n\n    The above output shows that 'compiz' and 'Xorg' are far and away\n    the heaviest ioctl callers (which might lead to questions about\n    whether they really need to be making all those calls and to\n    possible avenues for further investigation.)\n\n    The compound key examples used a key and a sum value (hitcount) to\n    sort the output, but we can just as easily use two keys instead.\n    Here's an example where we use a compound key composed of the the\n    common_pid and size event fields.  Sorting with pid as the primary\n    key and 'size' as the secondary key allows us to display an\n    ordered summary of the recvfrom sizes, with counts, received by\n    each process:\n\n    # echo 'hist:key=common_pid.execname,size:val=hitcount:sort=common_pid,size' > \\\n           /sys/kernel/debug/tracing/events/syscalls/sys_enter_recvfrom/trigger\n\n    # cat /sys/kernel/debug/tracing/events/syscalls/sys_enter_recvfrom/hist\n    # trigger info: hist:keys=common_pid.execname,size:vals=hitcount:sort=common_pid.execname,size:size=2048 [active]\n\n    { common_pid: smbd            [       784], size:          4 } hitcount:          1\n    { common_pid: dnsmasq         [      1412], size:       4096 } hitcount:        672\n    { common_pid: postgres        [      1796], size:       1000 } hitcount:          6\n    { common_pid: postgres        [      1867], size:       1000 } hitcount:         10\n    { common_pid: bamfdaemon      [      2787], size:         28 } hitcount:          2\n    { common_pid: bamfdaemon      [      2787], size:      14360 } hitcount:          1\n    { common_pid: compiz          [      2994], size:          8 } hitcount:          1\n    { common_pid: compiz          [      2994], size:         20 } hitcount:         11\n    { common_pid: gnome-terminal  [      3199], size:          4 } hitcount:          2\n    { common_pid: firefox         [      8817], size:          4 } hitcount:          1\n    { common_pid: firefox         [      8817], size:          8 } hitcount:          5\n    { common_pid: firefox         [      8817], size:        588 } hitcount:          2\n    { common_pid: firefox         [      8817], size:        628 } hitcount:          1\n    { common_pid: firefox         [      8817], size:       6944 } hitcount:          1\n    { common_pid: firefox         [      8817], size:     408880 } hitcount:          2\n    { common_pid: firefox         [      8822], size:          8 } hitcount:          2\n    { common_pid: firefox         [      8822], size:        160 } hitcount:          2\n    { common_pid: firefox         [      8822], size:        320 } hitcount:          2\n    { common_pid: firefox         [      8822], size:        352 } hitcount:          1\n    .\n    .\n    .\n    { common_pid: pool            [      8923], size:       1960 } hitcount:         10\n    { common_pid: pool            [      8923], size:       2048 } hitcount:         10\n    { common_pid: pool            [      8924], size:       1960 } hitcount:         10\n    { common_pid: pool            [      8924], size:       2048 } hitcount:         10\n    { common_pid: pool            [      8928], size:       1964 } hitcount:          4\n    { common_pid: pool            [      8928], size:       1965 } hitcount:          2\n    { common_pid: pool            [      8928], size:       2048 } hitcount:          6\n    { common_pid: pool            [      8929], size:       1982 } hitcount:          1\n    { common_pid: pool            [      8929], size:       2048 } hitcount:          1\n\n    Totals:\n        Hits: 2016\n        Entries: 224\n        Dropped: 0\n\n  The above example also illustrates the fact that although a compound\n  key is treated as a single entity for hashing purposes, the sub-keys\n  it's composed of can be accessed independently.\n\n  The next example uses a string field as the hash key and\n  demonstrates how you can manually pause and continue a hist trigger.\n  In this example, we'll aggregate fork counts and don't expect a\n  large number of entries in the hash table, so we'll drop it to a\n  much smaller number, say 256:\n\n    # echo 'hist:key=child_comm:val=hitcount:size=256' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_fork/trigger\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/hist\n    # trigger info: hist:keys=child_comm:vals=hitcount:sort=hitcount:size=256 [active]\n\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: ibus-daemon                         } hitcount:          1\n    { child_comm: whoopsie                            } hitcount:          1\n    { child_comm: smbd                                } hitcount:          1\n    { child_comm: gdbus                               } hitcount:          1\n    { child_comm: kthreadd                            } hitcount:          1\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: evolution-alarm                     } hitcount:          2\n    { child_comm: Socket Thread                       } hitcount:          2\n    { child_comm: postgres                            } hitcount:          2\n    { child_comm: bash                                } hitcount:          3\n    { child_comm: compiz                              } hitcount:          3\n    { child_comm: evolution-sourc                     } hitcount:          4\n    { child_comm: dhclient                            } hitcount:          4\n    { child_comm: pool                                } hitcount:          5\n    { child_comm: nm-dispatcher.a                     } hitcount:          8\n    { child_comm: firefox                             } hitcount:          8\n    { child_comm: dbus-daemon                         } hitcount:          8\n    { child_comm: glib-pacrunner                      } hitcount:         10\n    { child_comm: evolution                           } hitcount:         23\n\n    Totals:\n        Hits: 89\n        Entries: 20\n        Dropped: 0\n\n  If we want to pause the hist trigger, we can simply append :pause to\n  the command that started the trigger.  Notice that the trigger info\n  displays as [paused]:\n\n    # echo 'hist:key=child_comm:val=hitcount:size=256:pause' >> \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_fork/trigger\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/hist\n    # trigger info: hist:keys=child_comm:vals=hitcount:sort=hitcount:size=256 [paused]\n\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: kthreadd                            } hitcount:          1\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: gdbus                               } hitcount:          1\n    { child_comm: ibus-daemon                         } hitcount:          1\n    { child_comm: Socket Thread                       } hitcount:          2\n    { child_comm: evolution-alarm                     } hitcount:          2\n    { child_comm: smbd                                } hitcount:          2\n    { child_comm: bash                                } hitcount:          3\n    { child_comm: whoopsie                            } hitcount:          3\n    { child_comm: compiz                              } hitcount:          3\n    { child_comm: evolution-sourc                     } hitcount:          4\n    { child_comm: pool                                } hitcount:          5\n    { child_comm: postgres                            } hitcount:          6\n    { child_comm: firefox                             } hitcount:          8\n    { child_comm: dhclient                            } hitcount:         10\n    { child_comm: emacs                               } hitcount:         12\n    { child_comm: dbus-daemon                         } hitcount:         20\n    { child_comm: nm-dispatcher.a                     } hitcount:         20\n    { child_comm: evolution                           } hitcount:         35\n    { child_comm: glib-pacrunner                      } hitcount:         59\n\n    Totals:\n        Hits: 199\n        Entries: 21\n        Dropped: 0\n\n  To manually continue having the trigger aggregate events, append\n  :cont instead.  Notice that the trigger info displays as [active]\n  again, and the data has changed:\n\n    # echo 'hist:key=child_comm:val=hitcount:size=256:cont' >> \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_fork/trigger\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/hist\n    # trigger info: hist:keys=child_comm:vals=hitcount:sort=hitcount:size=256 [active]\n\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: kthreadd                            } hitcount:          1\n    { child_comm: gdbus                               } hitcount:          1\n    { child_comm: ibus-daemon                         } hitcount:          1\n    { child_comm: Socket Thread                       } hitcount:          2\n    { child_comm: evolution-alarm                     } hitcount:          2\n    { child_comm: smbd                                } hitcount:          2\n    { child_comm: whoopsie                            } hitcount:          3\n    { child_comm: compiz                              } hitcount:          3\n    { child_comm: evolution-sourc                     } hitcount:          4\n    { child_comm: bash                                } hitcount:          5\n    { child_comm: pool                                } hitcount:          5\n    { child_comm: postgres                            } hitcount:          6\n    { child_comm: firefox                             } hitcount:          8\n    { child_comm: dhclient                            } hitcount:         11\n    { child_comm: emacs                               } hitcount:         12\n    { child_comm: dbus-daemon                         } hitcount:         22\n    { child_comm: nm-dispatcher.a                     } hitcount:         22\n    { child_comm: evolution                           } hitcount:         35\n    { child_comm: glib-pacrunner                      } hitcount:         59\n\n    Totals:\n        Hits: 206\n        Entries: 21\n        Dropped: 0\n\n  The previous example showed how to start and stop a hist trigger by\n  appending 'pause' and 'continue' to the hist trigger command.  A\n  hist trigger can also be started in a paused state by initially\n  starting the trigger with ':pause' appended.  This allows you to\n  start the trigger only when you're ready to start collecting data\n  and not before.  For example, you could start the trigger in a\n  paused state, then unpause it and do something you want to measure,\n  then pause the trigger again when done.\n\n  Of course, doing this manually can be difficult and error-prone, but\n  it is possible to automatically start and stop a hist trigger based\n  on some condition, via the enable_hist and disable_hist triggers.\n\n  For example, suppose we wanted to take a look at the relative\n  weights in terms of skb length for each callpath that leads to a\n  netif_receieve_skb event when downloading a decent-sized file using\n  wget.\n\n  First we set up an initially paused stacktrace trigger on the\n  netif_receive_skb event:\n\n    # echo 'hist:key=stacktrace:vals=len:pause' > \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n\n  Next, we set up an 'enable_hist' trigger on the sched_process_exec\n  event, with an 'if filename==/usr/bin/wget' filter.  The effect of\n  this new trigger is that it will 'unpause' the hist trigger we just\n  set up on netif_receive_skb if and only if it sees a\n  sched_process_exec event with a filename of '/usr/bin/wget'.  When\n  that happens, all netif_receive_skb events are aggregated into a\n  hash table keyed on stacktrace:\n\n    # echo 'enable_hist:net:netif_receive_skb if filename==/usr/bin/wget' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_exec/trigger\n\n  The aggregation continues until the netif_receive_skb is paused\n  again, which is what the following disable_hist event does by\n  creating a similar setup on the sched_process_exit event, using the\n  filter 'comm==wget':\n\n    # echo 'disable_hist:net:netif_receive_skb if comm==wget' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_exit/trigger\n\n  Whenever a process exits and the comm field of the disable_hist\n  trigger filter matches 'comm==wget', the netif_receive_skb hist\n  trigger is disabled.\n\n  The overall effect is that netif_receive_skb events are aggregated\n  into the hash table for only the duration of the wget.  Executing a\n  wget command and then listing the 'hist' file will display the\n  output generated by the wget command:\n\n    $ wget https://www.kernel.org/pub/linux/kernel/v3.x/patch-3.19.xz\n\n    # cat /sys/kernel/debug/tracing/events/net/netif_receive_skb/hist\n    # trigger info: hist:keys=stacktrace:vals=len:sort=hitcount:size=2048 [paused]\n\n    { stacktrace:\n         __netif_receive_skb_core+0x46d/0x990\n         __netif_receive_skb+0x18/0x60\n         netif_receive_skb_internal+0x23/0x90\n         napi_gro_receive+0xc8/0x100\n         ieee80211_deliver_skb+0xd6/0x270 [mac80211]\n         ieee80211_rx_handlers+0xccf/0x22f0 [mac80211]\n         ieee80211_prepare_and_rx_handle+0x4e7/0xc40 [mac80211]\n         ieee80211_rx+0x31d/0x900 [mac80211]\n         iwlagn_rx_reply_rx+0x3db/0x6f0 [iwldvm]\n         iwl_rx_dispatch+0x8e/0xf0 [iwldvm]\n         iwl_pcie_irq_handler+0xe3c/0x12f0 [iwlwifi]\n         irq_thread_fn+0x20/0x50\n         irq_thread+0x11f/0x150\n         kthread+0xd2/0xf0\n         ret_from_fork+0x42/0x70\n    } hitcount:         85  len:      28884\n    { stacktrace:\n         __netif_receive_skb_core+0x46d/0x990\n         __netif_receive_skb+0x18/0x60\n         netif_receive_skb_internal+0x23/0x90\n         napi_gro_complete+0xa4/0xe0\n         dev_gro_receive+0x23a/0x360\n         napi_gro_receive+0x30/0x100\n         ieee80211_deliver_skb+0xd6/0x270 [mac80211]\n         ieee80211_rx_handlers+0xccf/0x22f0 [mac80211]\n         ieee80211_prepare_and_rx_handle+0x4e7/0xc40 [mac80211]\n         ieee80211_rx+0x31d/0x900 [mac80211]\n         iwlagn_rx_reply_rx+0x3db/0x6f0 [iwldvm]\n         iwl_rx_dispatch+0x8e/0xf0 [iwldvm]\n         iwl_pcie_irq_handler+0xe3c/0x12f0 [iwlwifi]\n         irq_thread_fn+0x20/0x50\n         irq_thread+0x11f/0x150\n         kthread+0xd2/0xf0\n    } hitcount:         98  len:     664329\n    { stacktrace:\n         __netif_receive_skb_core+0x46d/0x990\n         __netif_receive_skb+0x18/0x60\n         process_backlog+0xa8/0x150\n         net_rx_action+0x15d/0x340\n         __do_softirq+0x114/0x2c0\n         do_softirq_own_stack+0x1c/0x30\n         do_softirq+0x65/0x70\n         __local_bh_enable_ip+0xb5/0xc0\n         ip_finish_output+0x1f4/0x840\n         ip_output+0x6b/0xc0\n         ip_local_out_sk+0x31/0x40\n         ip_send_skb+0x1a/0x50\n         udp_send_skb+0x173/0x2a0\n         udp_sendmsg+0x2bf/0x9f0\n         inet_sendmsg+0x64/0xa0\n         sock_sendmsg+0x3d/0x50\n    } hitcount:        115  len:      13030\n    { stacktrace:\n         __netif_receive_skb_core+0x46d/0x990\n         __netif_receive_skb+0x18/0x60\n         netif_receive_skb_internal+0x23/0x90\n         napi_gro_complete+0xa4/0xe0\n         napi_gro_flush+0x6d/0x90\n         iwl_pcie_irq_handler+0x92a/0x12f0 [iwlwifi]\n         irq_thread_fn+0x20/0x50\n         irq_thread+0x11f/0x150\n         kthread+0xd2/0xf0\n         ret_from_fork+0x42/0x70\n    } hitcount:        934  len:    5512212\n\n    Totals:\n        Hits: 1232\n        Entries: 4\n        Dropped: 0\n\n  The above shows all the netif_receive_skb callpaths and their total\n  lengths for the duration of the wget command.\n\n  The 'clear' hist trigger param can be used to clear the hash table.\n  Suppose we wanted to try another run of the previous example but\n  this time also wanted to see the complete list of events that went\n  into the histogram.  In order to avoid having to set everything up\n  again, we can just clear the histogram first:\n\n    # echo 'hist:key=stacktrace:vals=len:clear' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n\n  Just to verify that it is in fact cleared, here's what we now see in\n  the hist file:\n\n    # cat /sys/kernel/debug/tracing/events/net/netif_receive_skb/hist\n    # trigger info: hist:keys=stacktrace:vals=len:sort=hitcount:size=2048 [paused]\n\n    Totals:\n        Hits: 0\n        Entries: 0\n        Dropped: 0\n\n  Since we want to see the detailed list of every netif_receive_skb\n  event occurring during the new run, which are in fact the same\n  events being aggregated into the hash table, we add some additional\n  'enable_event' events to the triggering sched_process_exec and\n  sched_process_exit events as such:\n\n    # echo 'enable_event:net:netif_receive_skb if filename==/usr/bin/wget' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_exec/trigger\n\n    # echo 'disable_event:net:netif_receive_skb if comm==wget' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_exit/trigger\n\n  If you read the trigger files for the sched_process_exec and\n  sched_process_exit triggers, you should see two triggers for each:\n  one enabling/disabling the hist aggregation and the other\n  enabling/disabling the logging of events:\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_exec/trigger\n    enable_event:net:netif_receive_skb:unlimited if filename==/usr/bin/wget\n    enable_hist:net:netif_receive_skb:unlimited if filename==/usr/bin/wget\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_exit/trigger\n    enable_event:net:netif_receive_skb:unlimited if comm==wget\n    disable_hist:net:netif_receive_skb:unlimited if comm==wget\n\n  In other words, whenever either of the sched_process_exec or\n  sched_process_exit events is hit and matches 'wget', it enables or\n  disables both the histogram and the event log, and what you end up\n  with is a hash table and set of events just covering the specified\n  duration.  Run the wget command again:\n\n    $ wget https://www.kernel.org/pub/linux/kernel/v3.x/patch-3.19.xz\n\n  Displaying the 'hist' file should show something similar to what you\n  saw in the last run, but this time you should also see the\n  individual events in the trace file:\n\n    # cat /sys/kernel/debug/tracing/trace\n\n    # tracer: nop\n    #\n    # entries-in-buffer/entries-written: 183/1426   #P:4\n    #\n    #                              _-----=> irqs-off\n    #                             / _----=> need-resched\n    #                            | / _---=> hardirq/softirq\n    #                            || / _--=> preempt-depth\n    #                            ||| /     delay\n    #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION\n    #              | |       |   ||||       |         |\n                wget-15108 [000] ..s1 31769.606929: netif_receive_skb: dev=lo skbaddr=ffff88009c353100 len=60\n                wget-15108 [000] ..s1 31769.606999: netif_receive_skb: dev=lo skbaddr=ffff88009c353200 len=60\n             dnsmasq-1382  [000] ..s1 31769.677652: netif_receive_skb: dev=lo skbaddr=ffff88009c352b00 len=130\n             dnsmasq-1382  [000] ..s1 31769.685917: netif_receive_skb: dev=lo skbaddr=ffff88009c352200 len=138\n    ##### CPU 2 buffer started ####\n      irq/29-iwlwifi-559   [002] ..s. 31772.031529: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d433d00 len=2948\n      irq/29-iwlwifi-559   [002] ..s. 31772.031572: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d432200 len=1500\n      irq/29-iwlwifi-559   [002] ..s. 31772.032196: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d433100 len=2948\n      irq/29-iwlwifi-559   [002] ..s. 31772.032761: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d433000 len=2948\n      irq/29-iwlwifi-559   [002] ..s. 31772.033220: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d432e00 len=1500\n    .\n    .\n    .\n\n  The following example demonstrates how multiple hist triggers can be\n  attached to a given event.  This capability can be useful for\n  creating a set of different summaries derived from the same set of\n  events, or for comparing the effects of different filters, among\n  other things.\n\n    # echo 'hist:keys=skbaddr.hex:vals=len if len < 0' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:keys=skbaddr.hex:vals=len if len > 4096' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:keys=skbaddr.hex:vals=len if len == 256' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:keys=skbaddr.hex:vals=len' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:keys=len:vals=common_preempt_count' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n\n  The above set of commands create four triggers differing only in\n  their filters, along with a completely different though fairly\n  nonsensical trigger.  Note that in order to append multiple hist\n  triggers to the same file, you should use the '>>' operator to\n  append them ('>' will also add the new hist trigger, but will remove\n  any existing hist triggers beforehand).\n\n  Displaying the contents of the 'hist' file for the event shows the\n  contents of all five histograms:\n\n    # cat /sys/kernel/debug/tracing/events/net/netif_receive_skb/hist\n\n    # event histogram\n    #\n    # trigger info: hist:keys=len:vals=hitcount,common_preempt_count:sort=hitcount:size=2048 [active]\n    #\n\n    { len:        176 } hitcount:          1  common_preempt_count:          0\n    { len:        223 } hitcount:          1  common_preempt_count:          0\n    { len:       4854 } hitcount:          1  common_preempt_count:          0\n    { len:        395 } hitcount:          1  common_preempt_count:          0\n    { len:        177 } hitcount:          1  common_preempt_count:          0\n    { len:        446 } hitcount:          1  common_preempt_count:          0\n    { len:       1601 } hitcount:          1  common_preempt_count:          0\n    .\n    .\n    .\n    { len:       1280 } hitcount:         66  common_preempt_count:          0\n    { len:        116 } hitcount:         81  common_preempt_count:         40\n    { len:        708 } hitcount:        112  common_preempt_count:          0\n    { len:         46 } hitcount:        221  common_preempt_count:          0\n    { len:       1264 } hitcount:        458  common_preempt_count:          0\n\n    Totals:\n        Hits: 1428\n        Entries: 147\n        Dropped: 0\n\n\n    # event histogram\n    #\n    # trigger info: hist:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 [active]\n    #\n\n    { skbaddr: ffff8800baee5e00 } hitcount:          1  len:        130\n    { skbaddr: ffff88005f3d5600 } hitcount:          1  len:       1280\n    { skbaddr: ffff88005f3d4900 } hitcount:          1  len:       1280\n    { skbaddr: ffff88009fed6300 } hitcount:          1  len:        115\n    { skbaddr: ffff88009fe0ad00 } hitcount:          1  len:        115\n    { skbaddr: ffff88008cdb1900 } hitcount:          1  len:         46\n    { skbaddr: ffff880064b5ef00 } hitcount:          1  len:        118\n    { skbaddr: ffff880044e3c700 } hitcount:          1  len:         60\n    { skbaddr: ffff880100065900 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d46bd500 } hitcount:          1  len:        116\n    { skbaddr: ffff88005f3d5f00 } hitcount:          1  len:       1280\n    { skbaddr: ffff880100064700 } hitcount:          1  len:        365\n    { skbaddr: ffff8800badb6f00 } hitcount:          1  len:         60\n    .\n    .\n    .\n    { skbaddr: ffff88009fe0be00 } hitcount:         27  len:      24677\n    { skbaddr: ffff88009fe0a400 } hitcount:         27  len:      23052\n    { skbaddr: ffff88009fe0b700 } hitcount:         31  len:      25589\n    { skbaddr: ffff88009fe0b600 } hitcount:         32  len:      27326\n    { skbaddr: ffff88006a462800 } hitcount:         68  len:      71678\n    { skbaddr: ffff88006a463700 } hitcount:         70  len:      72678\n    { skbaddr: ffff88006a462b00 } hitcount:         71  len:      77589\n    { skbaddr: ffff88006a463600 } hitcount:         73  len:      71307\n    { skbaddr: ffff88006a462200 } hitcount:         81  len:      81032\n\n    Totals:\n        Hits: 1451\n        Entries: 318\n        Dropped: 0\n\n\n    # event histogram\n    #\n    # trigger info: hist:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 if len == 256 [active]\n    #\n\n\n    Totals:\n        Hits: 0\n        Entries: 0\n        Dropped: 0\n\n\n    # event histogram\n    #\n    # trigger info: hist:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 if len > 4096 [active]\n    #\n\n    { skbaddr: ffff88009fd2c300 } hitcount:          1  len:       7212\n    { skbaddr: ffff8800d2bcce00 } hitcount:          1  len:       7212\n    { skbaddr: ffff8800d2bcd700 } hitcount:          1  len:       7212\n    { skbaddr: ffff8800d2bcda00 } hitcount:          1  len:      21492\n    { skbaddr: ffff8800ae2e2d00 } hitcount:          1  len:       7212\n    { skbaddr: ffff8800d2bcdb00 } hitcount:          1  len:       7212\n    { skbaddr: ffff88006a4df500 } hitcount:          1  len:       4854\n    { skbaddr: ffff88008ce47b00 } hitcount:          1  len:      18636\n    { skbaddr: ffff8800ae2e2200 } hitcount:          1  len:      12924\n    { skbaddr: ffff88005f3e1000 } hitcount:          1  len:       4356\n    { skbaddr: ffff8800d2bcdc00 } hitcount:          2  len:      24420\n    { skbaddr: ffff8800d2bcc200 } hitcount:          2  len:      12996\n\n    Totals:\n        Hits: 14\n        Entries: 12\n        Dropped: 0\n\n\n    # event histogram\n    #\n    # trigger info: hist:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 if len < 0 [active]\n    #\n\n\n    Totals:\n        Hits: 0\n        Entries: 0\n        Dropped: 0\n\n  Named triggers can be used to have triggers share a common set of\n  histogram data.  This capability is mostly useful for combining the\n  output of events generated by tracepoints contained inside inline\n  functions, but names can be used in a hist trigger on any event.\n  For example, these two triggers when hit will update the same 'len'\n  field in the shared 'foo' histogram data:\n\n    # echo 'hist:name=foo:keys=skbaddr.hex:vals=len' > \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:name=foo:keys=skbaddr.hex:vals=len' > \\\n           /sys/kernel/debug/tracing/events/net/netif_rx/trigger\n\n  You can see that they're updating common histogram data by reading\n  each event's hist files at the same time:\n\n    # cat /sys/kernel/debug/tracing/events/net/netif_receive_skb/hist;\n      cat /sys/kernel/debug/tracing/events/net/netif_rx/hist\n\n    # event histogram\n    #\n    # trigger info: hist:name=foo:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 [active]\n    #\n\n    { skbaddr: ffff88000ad53500 } hitcount:          1  len:         46\n    { skbaddr: ffff8800af5a1500 } hitcount:          1  len:         76\n    { skbaddr: ffff8800d62a1900 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bccb00 } hitcount:          1  len:        468\n    { skbaddr: ffff8800d3c69900 } hitcount:          1  len:         46\n    { skbaddr: ffff88009ff09100 } hitcount:          1  len:         52\n    { skbaddr: ffff88010f13ab00 } hitcount:          1  len:        168\n    { skbaddr: ffff88006a54f400 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcc500 } hitcount:          1  len:        260\n    { skbaddr: ffff880064505000 } hitcount:          1  len:         46\n    { skbaddr: ffff8800baf24e00 } hitcount:          1  len:         32\n    { skbaddr: ffff88009fe0ad00 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d3edff00 } hitcount:          1  len:         44\n    { skbaddr: ffff88009fe0b400 } hitcount:          1  len:        168\n    { skbaddr: ffff8800a1c55a00 } hitcount:          1  len:         40\n    { skbaddr: ffff8800d2bcd100 } hitcount:          1  len:         40\n    { skbaddr: ffff880064505f00 } hitcount:          1  len:        174\n    { skbaddr: ffff8800a8bff200 } hitcount:          1  len:        160\n    { skbaddr: ffff880044e3cc00 } hitcount:          1  len:         76\n    { skbaddr: ffff8800a8bfe700 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcdc00 } hitcount:          1  len:         32\n    { skbaddr: ffff8800a1f64800 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcde00 } hitcount:          1  len:        988\n    { skbaddr: ffff88006a5dea00 } hitcount:          1  len:         46\n    { skbaddr: ffff88002e37a200 } hitcount:          1  len:         44\n    { skbaddr: ffff8800a1f32c00 } hitcount:          2  len:        676\n    { skbaddr: ffff88000ad52600 } hitcount:          2  len:        107\n    { skbaddr: ffff8800a1f91e00 } hitcount:          2  len:         92\n    { skbaddr: ffff8800af5a0200 } hitcount:          2  len:        142\n    { skbaddr: ffff8800d2bcc600 } hitcount:          2  len:        220\n    { skbaddr: ffff8800ba36f500 } hitcount:          2  len:         92\n    { skbaddr: ffff8800d021f800 } hitcount:          2  len:         92\n    { skbaddr: ffff8800a1f33600 } hitcount:          2  len:        675\n    { skbaddr: ffff8800a8bfff00 } hitcount:          3  len:        138\n    { skbaddr: ffff8800d62a1300 } hitcount:          3  len:        138\n    { skbaddr: ffff88002e37a100 } hitcount:          4  len:        184\n    { skbaddr: ffff880064504400 } hitcount:          4  len:        184\n    { skbaddr: ffff8800a8bfec00 } hitcount:          4  len:        184\n    { skbaddr: ffff88000ad53700 } hitcount:          5  len:        230\n    { skbaddr: ffff8800d2bcdb00 } hitcount:          5  len:        196\n    { skbaddr: ffff8800a1f90000 } hitcount:          6  len:        276\n    { skbaddr: ffff88006a54f900 } hitcount:          6  len:        276\n\n    Totals:\n        Hits: 81\n        Entries: 42\n        Dropped: 0\n    # event histogram\n    #\n    # trigger info: hist:name=foo:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 [active]\n    #\n\n    { skbaddr: ffff88000ad53500 } hitcount:          1  len:         46\n    { skbaddr: ffff8800af5a1500 } hitcount:          1  len:         76\n    { skbaddr: ffff8800d62a1900 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bccb00 } hitcount:          1  len:        468\n    { skbaddr: ffff8800d3c69900 } hitcount:          1  len:         46\n    { skbaddr: ffff88009ff09100 } hitcount:          1  len:         52\n    { skbaddr: ffff88010f13ab00 } hitcount:          1  len:        168\n    { skbaddr: ffff88006a54f400 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcc500 } hitcount:          1  len:        260\n    { skbaddr: ffff880064505000 } hitcount:          1  len:         46\n    { skbaddr: ffff8800baf24e00 } hitcount:          1  len:         32\n    { skbaddr: ffff88009fe0ad00 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d3edff00 } hitcount:          1  len:         44\n    { skbaddr: ffff88009fe0b400 } hitcount:          1  len:        168\n    { skbaddr: ffff8800a1c55a00 } hitcount:          1  len:         40\n    { skbaddr: ffff8800d2bcd100 } hitcount:          1  len:         40\n    { skbaddr: ffff880064505f00 } hitcount:          1  len:        174\n    { skbaddr: ffff8800a8bff200 } hitcount:          1  len:        160\n    { skbaddr: ffff880044e3cc00 } hitcount:          1  len:         76\n    { skbaddr: ffff8800a8bfe700 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcdc00 } hitcount:          1  len:         32\n    { skbaddr: ffff8800a1f64800 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcde00 } hitcount:          1  len:        988\n    { skbaddr: ffff88006a5dea00 } hitcount:          1  len:         46\n    { skbaddr: ffff88002e37a200 } hitcount:          1  len:         44\n    { skbaddr: ffff8800a1f32c00 } hitcount:          2  len:        676\n    { skbaddr: ffff88000ad52600 } hitcount:          2  len:        107\n    { skbaddr: ffff8800a1f91e00 } hitcount:          2  len:         92\n    { skbaddr: ffff8800af5a0200 } hitcount:          2  len:        142\n    { skbaddr: ffff8800d2bcc600 } hitcount:          2  len:        220\n    { skbaddr: ffff8800ba36f500 } hitcount:          2  len:         92\n    { skbaddr: ffff8800d021f800 } hitcount:          2  len:         92\n    { skbaddr: ffff8800a1f33600 } hitcount:          2  len:        675\n    { skbaddr: ffff8800a8bfff00 } hitcount:          3  len:        138\n    { skbaddr: ffff8800d62a1300 } hitcount:          3  len:        138\n    { skbaddr: ffff88002e37a100 } hitcount:          4  len:        184\n    { skbaddr: ffff880064504400 } hitcount:          4  len:        184\n    { skbaddr: ffff8800a8bfec00 } hitcount:          4  len:        184\n    { skbaddr: ffff88000ad53700 } hitcount:          5  len:        230\n    { skbaddr: ffff8800d2bcdb00 } hitcount:          5  len:        196\n    { skbaddr: ffff8800a1f90000 } hitcount:          6  len:        276\n    { skbaddr: ffff88006a54f900 } hitcount:          6  len:        276\n\n    Totals:\n        Hits: 81\n        Entries: 42\n        Dropped: 0\n\n  And here's an example that shows how to combine histogram data from\n  any two events even if they don't share any 'compatible' fields\n  other than 'hitcount' and 'stacktrace'.  These commands create a\n  couple of triggers named 'bar' using those fields:\n\n    # echo 'hist:name=bar:key=stacktrace:val=hitcount' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_fork/trigger\n    # echo 'hist:name=bar:key=stacktrace:val=hitcount' > \\\n          /sys/kernel/debug/tracing/events/net/netif_rx/trigger\n\n  And displaying the output of either shows some interesting if\n  somewhat confusing output:\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/hist\n    # cat /sys/kernel/debug/tracing/events/net/netif_rx/hist\n\n    # event histogram\n    #\n    # trigger info: hist:name=bar:keys=stacktrace:vals=hitcount:sort=hitcount:size=2048 [active]\n    #\n\n    { stacktrace:\n             _do_fork+0x18e/0x330\n             kernel_thread+0x29/0x30\n             kthreadd+0x154/0x1b0\n             ret_from_fork+0x3f/0x70\n    } hitcount:          1\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx_ni+0x20/0x70\n             dev_loopback_xmit+0xaa/0xd0\n             ip_mc_output+0x126/0x240\n             ip_local_out_sk+0x31/0x40\n             igmp_send_report+0x1e9/0x230\n             igmp_timer_expire+0xe9/0x120\n             call_timer_fn+0x39/0xf0\n             run_timer_softirq+0x1e1/0x290\n             __do_softirq+0xfd/0x290\n             irq_exit+0x98/0xb0\n             smp_apic_timer_interrupt+0x4a/0x60\n             apic_timer_interrupt+0x6d/0x80\n             cpuidle_enter+0x17/0x20\n             call_cpuidle+0x3b/0x60\n             cpu_startup_entry+0x22d/0x310\n    } hitcount:          1\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx_ni+0x20/0x70\n             dev_loopback_xmit+0xaa/0xd0\n             ip_mc_output+0x17f/0x240\n             ip_local_out_sk+0x31/0x40\n             ip_send_skb+0x1a/0x50\n             udp_send_skb+0x13e/0x270\n             udp_sendmsg+0x2bf/0x980\n             inet_sendmsg+0x67/0xa0\n             sock_sendmsg+0x38/0x50\n             SYSC_sendto+0xef/0x170\n             SyS_sendto+0xe/0x10\n             entry_SYSCALL_64_fastpath+0x12/0x6a\n    } hitcount:          2\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx+0x1c/0x60\n             loopback_xmit+0x6c/0xb0\n             dev_hard_start_xmit+0x219/0x3a0\n             __dev_queue_xmit+0x415/0x4f0\n             dev_queue_xmit_sk+0x13/0x20\n             ip_finish_output2+0x237/0x340\n             ip_finish_output+0x113/0x1d0\n             ip_output+0x66/0xc0\n             ip_local_out_sk+0x31/0x40\n             ip_send_skb+0x1a/0x50\n             udp_send_skb+0x16d/0x270\n             udp_sendmsg+0x2bf/0x980\n             inet_sendmsg+0x67/0xa0\n             sock_sendmsg+0x38/0x50\n             ___sys_sendmsg+0x14e/0x270\n    } hitcount:         76\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx+0x1c/0x60\n             loopback_xmit+0x6c/0xb0\n             dev_hard_start_xmit+0x219/0x3a0\n             __dev_queue_xmit+0x415/0x4f0\n             dev_queue_xmit_sk+0x13/0x20\n             ip_finish_output2+0x237/0x340\n             ip_finish_output+0x113/0x1d0\n             ip_output+0x66/0xc0\n             ip_local_out_sk+0x31/0x40\n             ip_send_skb+0x1a/0x50\n             udp_send_skb+0x16d/0x270\n             udp_sendmsg+0x2bf/0x980\n             inet_sendmsg+0x67/0xa0\n             sock_sendmsg+0x38/0x50\n             ___sys_sendmsg+0x269/0x270\n    } hitcount:         77\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx+0x1c/0x60\n             loopback_xmit+0x6c/0xb0\n             dev_hard_start_xmit+0x219/0x3a0\n             __dev_queue_xmit+0x415/0x4f0\n             dev_queue_xmit_sk+0x13/0x20\n             ip_finish_output2+0x237/0x340\n             ip_finish_output+0x113/0x1d0\n             ip_output+0x66/0xc0\n             ip_local_out_sk+0x31/0x40\n             ip_send_skb+0x1a/0x50\n             udp_send_skb+0x16d/0x270\n             udp_sendmsg+0x2bf/0x980\n             inet_sendmsg+0x67/0xa0\n             sock_sendmsg+0x38/0x50\n             SYSC_sendto+0xef/0x170\n    } hitcount:         88\n    { stacktrace:\n             _do_fork+0x18e/0x330\n             SyS_clone+0x19/0x20\n             entry_SYSCALL_64_fastpath+0x12/0x6a\n    } hitcount:        244\n\n    Totals:\n        Hits: 489\n        Entries: 7\n        Dropped: 0\n\n2.2 Inter-event hist triggers\n-----------------------------\n\nInter-event hist triggers are hist triggers that combine values from\none or more other events and create a histogram using that data.  Data\nfrom an inter-event histogram can in turn become the source for\nfurther combined histograms, thus providing a chain of related\nhistograms, which is important for some applications.\n\nThe most important example of an inter-event quantity that can be used\nin this manner is latency, which is simply a difference in timestamps\nbetween two events.  Although latency is the most important\ninter-event quantity, note that because the support is completely\ngeneral across the trace event subsystem, any event field can be used\nin an inter-event quantity.\n\nAn example of a histogram that combines data from other histograms\ninto a useful chain would be a 'wakeupswitch latency' histogram that\ncombines a 'wakeup latency' histogram and a 'switch latency'\nhistogram.\n\nNormally, a hist trigger specification consists of a (possibly\ncompound) key along with one or more numeric values, which are\ncontinually updated sums associated with that key.  A histogram\nspecification in this case consists of individual key and value\nspecifications that refer to trace event fields associated with a\nsingle event type.\n\nThe inter-event hist trigger extension allows fields from multiple\nevents to be referenced and combined into a multi-event histogram\nspecification.  In support of this overall goal, a few enabling\nfeatures have been added to the hist trigger support:\n\n  - In order to compute an inter-event quantity, a value from one\n    event needs to saved and then referenced from another event.  This\n    requires the introduction of support for histogram 'variables'.\n\n  - The computation of inter-event quantities and their combination\n    require some minimal amount of support for applying simple\n    expressions to variables (+ and -).\n\n  - A histogram consisting of inter-event quantities isn't logically a\n    histogram on either event (so having the 'hist' file for either\n    event host the histogram output doesn't really make sense).  To\n    address the idea that the histogram is associated with a\n    combination of events, support is added allowing the creation of\n    'synthetic' events that are events derived from other events.\n    These synthetic events are full-fledged events just like any other\n    and can be used as such, as for instance to create the\n    'combination' histograms mentioned previously.\n\n  - A set of 'actions' can be associated with histogram entries -\n    these can be used to generate the previously mentioned synthetic\n    events, but can also be used for other purposes, such as for\n    example saving context when a 'max' latency has been hit.\n\n  - Trace events don't have a 'timestamp' associated with them, but\n    there is an implicit timestamp saved along with an event in the\n    underlying ftrace ring buffer.  This timestamp is now exposed as a\n    a synthetic field named 'common_timestamp' which can be used in\n    histograms as if it were any other event field; it isn't an actual\n    field in the trace format but rather is a synthesized value that\n    nonetheless can be used as if it were an actual field.  By default\n    it is in units of nanoseconds; appending '.usecs' to a\n    common_timestamp field changes the units to microseconds.\n\nA note on inter-event timestamps: If common_timestamp is used in a\nhistogram, the trace buffer is automatically switched over to using\nabsolute timestamps and the \"global\" trace clock, in order to avoid\nbogus timestamp differences with other clocks that aren't coherent\nacross CPUs.  This can be overridden by specifying one of the other\ntrace clocks instead, using the \"clock=XXX\" hist trigger attribute,\nwhere XXX is any of the clocks listed in the tracing/trace_clock\npseudo-file.\n\nThese features are described in more detail in the following sections.\n\n2.2.1 Histogram Variables\n-------------------------\n\nVariables are simply named locations used for saving and retrieving\nvalues between matching events.  A 'matching' event is defined as an\nevent that has a matching key - if a variable is saved for a histogram\nentry corresponding to that key, any subsequent event with a matching\nkey can access that variable.\n\nA variable's value is normally available to any subsequent event until\nit is set to something else by a subsequent event.  The one exception\nto that rule is that any variable used in an expression is essentially\n'read-once' - once it's used by an expression in a subsequent event,\nit's reset to its 'unset' state, which means it can't be used again\nunless it's set again.  This ensures not only that an event doesn't\nuse an uninitialized variable in a calculation, but that that variable\nis used only once and not for any unrelated subsequent match.\n\nThe basic syntax for saving a variable is to simply prefix a unique\nvariable name not corresponding to any keyword along with an '=' sign\nto any event field.\n\nEither keys or values can be saved and retrieved in this way.  This\ncreates a variable named 'ts0' for a histogram entry with the key\n'next_pid':\n\n  # echo 'hist:keys=next_pid:vals=$ts0:ts0=common_timestamp ... >> \\\n\tevent/trigger\n\nThe ts0 variable can be accessed by any subsequent event having the\nsame pid as 'next_pid'.\n\nVariable references are formed by prepending the variable name with\nthe '$' sign.  Thus for example, the ts0 variable above would be\nreferenced as '$ts0' in expressions.\n\nBecause 'vals=' is used, the common_timestamp variable value above\nwill also be summed as a normal histogram value would (though for a\ntimestamp it makes little sense).\n\nThe below shows that a key value can also be saved in the same way:\n\n  # echo 'hist:timer_pid=common_pid:key=timer_pid ...' >> event/trigger\n\nIf a variable isn't a key variable or prefixed with 'vals=', the\nassociated event field will be saved in a variable but won't be summed\nas a value:\n\n  # echo 'hist:keys=next_pid:ts1=common_timestamp ... >> event/trigger\n\nMultiple variables can be assigned at the same time.  The below would\nresult in both ts0 and b being created as variables, with both\ncommon_timestamp and field1 additionally being summed as values:\n\n  # echo 'hist:keys=pid:vals=$ts0,$b:ts0=common_timestamp,b=field1 ... >> \\\n\tevent/trigger\n\nNote that variable assignments can appear either preceding or\nfollowing their use.  The command below behaves identically to the\ncommand above:\n\n  # echo 'hist:keys=pid:ts0=common_timestamp,b=field1:vals=$ts0,$b ... >> \\\n\tevent/trigger\n\nAny number of variables not bound to a 'vals=' prefix can also be\nassigned by simply separating them with colons.  Below is the same\nthing but without the values being summed in the histogram:\n\n  # echo 'hist:keys=pid:ts0=common_timestamp:b=field1 ... >> event/trigger\n\nVariables set as above can be referenced and used in expressions on\nanother event.\n\nFor example, here's how a latency can be calculated:\n\n  # echo 'hist:keys=pid,prio:ts0=common_timestamp ... >> event1/trigger\n  # echo 'hist:keys=next_pid:wakeup_lat=common_timestamp-$ts0 ... >> event2/trigger\n\nIn the first line above, the event's timetamp is saved into the\nvariable ts0.  In the next line, ts0 is subtracted from the second\nevent's timestamp to produce the latency, which is then assigned into\nyet another variable, 'wakeup_lat'.  The hist trigger below in turn\nmakes use of the wakeup_lat variable to compute a combined latency\nusing the same key and variable from yet another event:\n\n  # echo 'hist:key=pid:wakeupswitch_lat=$wakeup_lat+$switchtime_lat ... >> event3/trigger\n\n2.2.2 Synthetic Events\n----------------------\n\nSynthetic events are user-defined events generated from hist trigger\nvariables or fields associated with one or more other events.  Their\npurpose is to provide a mechanism for displaying data spanning\nmultiple events consistent with the existing and already familiar\nusage for normal events.\n\nTo define a synthetic event, the user writes a simple specification\nconsisting of the name of the new event along with one or more\nvariables and their types, which can be any valid field type,\nseparated by semicolons, to the tracing/synthetic_events file.\n\nFor instance, the following creates a new event named 'wakeup_latency'\nwith 3 fields: lat, pid, and prio.  Each of those fields is simply a\nvariable reference to a variable on another event:\n\n  # echo 'wakeup_latency \\\n          u64 lat; \\\n          pid_t pid; \\\n\t  int prio' >> \\\n\t  /sys/kernel/debug/tracing/synthetic_events\n\nReading the tracing/synthetic_events file lists all the currently\ndefined synthetic events, in this case the event defined above:\n\n  # cat /sys/kernel/debug/tracing/synthetic_events\n    wakeup_latency u64 lat; pid_t pid; int prio\n\nAn existing synthetic event definition can be removed by prepending\nthe command that defined it with a '!':\n\n  # echo '!wakeup_latency u64 lat pid_t pid int prio' >> \\\n    /sys/kernel/debug/tracing/synthetic_events\n\nAt this point, there isn't yet an actual 'wakeup_latency' event\ninstantiated in the event subsytem - for this to happen, a 'hist\ntrigger action' needs to be instantiated and bound to actual fields\nand variables defined on other events (see Section 6.3.3 below).\n\nOnce that is done, an event instance is created, and a histogram can\nbe defined using it:\n\n  # echo 'hist:keys=pid,prio,lat.log2:sort=pid,lat' >> \\\n        /sys/kernel/debug/tracing/events/synthetic/wakeup_latency/trigger\n\nThe new event is created under the tracing/events/synthetic/ directory\nand looks and behaves just like any other event:\n\n  # ls /sys/kernel/debug/tracing/events/synthetic/wakeup_latency\n        enable  filter  format  hist  id  trigger\n\nLike any other event, once a histogram is enabled for the event, the\noutput can be displayed by reading the event's 'hist' file.\n\n2.2.3 Hist trigger 'actions'\n----------------------------\n\nA hist trigger 'action' is a function that's executed whenever a\nhistogram entry is added or updated.\n\nThe default 'action' if no special function is explicity specified is\nas it always has been, to simply update the set of values associated\nwith an entry.  Some applications, however, may want to perform\nadditional actions at that point, such as generate another event, or\ncompare and save a maximum.\n\nThe following additional actions are available.  To specify an action\nfor a given event, simply specify the action between colons in the\nhist trigger specification.\n\n  - onmatch(matching.event).<synthetic_event_name>(param list)\n\n    The 'onmatch(matching.event).<synthetic_event_name>(params)' hist\n    trigger action is invoked whenever an event matches and the\n    histogram entry would be added or updated.  It causes the named\n    synthetic event to be generated with the values given in the\n    'param list'.  The result is the generation of a synthetic event\n    that consists of the values contained in those variables at the\n    time the invoking event was hit.\n\n    The 'param list' consists of one or more parameters which may be\n    either variables or fields defined on either the 'matching.event'\n    or the target event.  The variables or fields specified in the\n    param list may be either fully-qualified or unqualified.  If a\n    variable is specified as unqualified, it must be unique between\n    the two events.  A field name used as a param can be unqualified\n    if it refers to the target event, but must be fully qualified if\n    it refers to the matching event.  A fully-qualified name is of the\n    form 'system.event_name.$var_name' or 'system.event_name.field'.\n\n    The 'matching.event' specification is simply the fully qualified\n    event name of the event that matches the target event for the\n    onmatch() functionality, in the form 'system.event_name'.\n\n    Finally, the number and type of variables/fields in the 'param\n    list' must match the number and types of the fields in the\n    synthetic event being generated.\n\n    As an example the below defines a simple synthetic event and uses\n    a variable defined on the sched_wakeup_new event as a parameter\n    when invoking the synthetic event.  Here we define the synthetic\n    event:\n\n    # echo 'wakeup_new_test pid_t pid' >> \\\n           /sys/kernel/debug/tracing/synthetic_events\n\n    # cat /sys/kernel/debug/tracing/synthetic_events\n          wakeup_new_test pid_t pid\n\n    The following hist trigger both defines the missing testpid\n    variable and specifies an onmatch() action that generates a\n    wakeup_new_test synthetic event whenever a sched_wakeup_new event\n    occurs, which because of the 'if comm == \"cyclictest\"' filter only\n    happens when the executable is cyclictest:\n\n    # echo 'hist:keys=$testpid:testpid=pid:onmatch(sched.sched_wakeup_new).\\\n            wakeup_new_test($testpid) if comm==\"cyclictest\"' >> \\\n            /sys/kernel/debug/tracing/events/sched/sched_wakeup_new/trigger\n\n    Creating and displaying a histogram based on those events is now\n    just a matter of using the fields and new synthetic event in the\n    tracing/events/synthetic directory, as usual:\n\n    # echo 'hist:keys=pid:sort=pid' >> \\\n           /sys/kernel/debug/tracing/events/synthetic/wakeup_new_test/trigger\n\n    Running 'cyclictest' should cause wakeup_new events to generate\n    wakeup_new_test synthetic events which should result in histogram\n    output in the wakeup_new_test event's hist file:\n\n    # cat /sys/kernel/debug/tracing/events/synthetic/wakeup_new_test/hist\n\n    A more typical usage would be to use two events to calculate a\n    latency.  The following example uses a set of hist triggers to\n    produce a 'wakeup_latency' histogram:\n\n    First, we define a 'wakeup_latency' synthetic event:\n\n    # echo 'wakeup_latency u64 lat; pid_t pid; int prio' >> \\\n            /sys/kernel/debug/tracing/synthetic_events\n\n    Next, we specify that whenever we see a sched_waking event for a\n    cyclictest thread, save the timestamp in a 'ts0' variable:\n\n    # echo 'hist:keys=$saved_pid:saved_pid=pid:ts0=common_timestamp.usecs \\\n            if comm==\"cyclictest\"' >> \\\n\t    /sys/kernel/debug/tracing/events/sched/sched_waking/trigger\n\n    Then, when the corresponding thread is actually scheduled onto the\n    CPU by a sched_switch event, calculate the latency and use that\n    along with another variable and an event field to generate a\n    wakeup_latency synthetic event:\n\n    # echo 'hist:keys=next_pid:wakeup_lat=common_timestamp.usecs-$ts0:\\\n            onmatch(sched.sched_waking).wakeup_latency($wakeup_lat,\\\n\t            $saved_pid,next_prio) if next_comm==\"cyclictest\"' >> \\\n\t    /sys/kernel/debug/tracing/events/sched/sched_switch/trigger\n\n    We also need to create a histogram on the wakeup_latency synthetic\n    event in order to aggregate the generated synthetic event data:\n\n    # echo 'hist:keys=pid,prio,lat:sort=pid,lat' >> \\\n            /sys/kernel/debug/tracing/events/synthetic/wakeup_latency/trigger\n\n    Finally, once we've run cyclictest to actually generate some\n    events, we can see the output by looking at the wakeup_latency\n    synthetic event's hist file:\n\n    # cat /sys/kernel/debug/tracing/events/synthetic/wakeup_latency/hist\n\n  - onmax(var).save(field,..\t.)\n\n    The 'onmax(var).save(field,...)' hist trigger action is invoked\n    whenever the value of 'var' associated with a histogram entry\n    exceeds the current maximum contained in that variable.\n\n    The end result is that the trace event fields specified as the\n    onmax.save() params will be saved if 'var' exceeds the current\n    maximum for that hist trigger entry.  This allows context from the\n    event that exhibited the new maximum to be saved for later\n    reference.  When the histogram is displayed, additional fields\n    displaying the saved values will be printed.\n\n    As an example the below defines a couple of hist triggers, one for\n    sched_waking and another for sched_switch, keyed on pid.  Whenever\n    a sched_waking occurs, the timestamp is saved in the entry\n    corresponding to the current pid, and when the scheduler switches\n    back to that pid, the timestamp difference is calculated.  If the\n    resulting latency, stored in wakeup_lat, exceeds the current\n    maximum latency, the values specified in the save() fields are\n    recoreded:\n\n    # echo 'hist:keys=pid:ts0=common_timestamp.usecs \\\n            if comm==\"cyclictest\"' >> \\\n            /sys/kernel/debug/tracing/events/sched/sched_waking/trigger\n\n    # echo 'hist:keys=next_pid:\\\n            wakeup_lat=common_timestamp.usecs-$ts0:\\\n            onmax($wakeup_lat).save(next_comm,prev_pid,prev_prio,prev_comm) \\\n            if next_comm==\"cyclictest\"' >> \\\n            /sys/kernel/debug/tracing/events/sched/sched_switch/trigger\n\n    When the histogram is displayed, the max value and the saved\n    values corresponding to the max are displayed following the rest\n    of the fields:\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_switch/hist\n      { next_pid:       2255 } hitcount:        239\n        common_timestamp-ts0:          0\n        max:         27\n\tnext_comm: cyclictest\n        prev_pid:          0  prev_prio:        120  prev_comm: swapper/1\n\n      { next_pid:       2256 } hitcount:       2355\n        common_timestamp-ts0: 0\n        max:         49  next_comm: cyclictest\n        prev_pid:          0  prev_prio:        120  prev_comm: swapper/0\n\n      Totals:\n          Hits: 12970\n          Entries: 2\n          Dropped: 0\n\n3. User space creating a trigger\n--------------------------------\n\nWriting into /sys/kernel/tracing/trace_marker writes into the ftrace\nring buffer. This can also act like an event, by writing into the trigger\nfile located in /sys/kernel/tracing/events/ftrace/print/\n\nModifying cyclictest to write into the trace_marker file before it sleeps\nand after it wakes up, something like this:\n\nstatic void traceputs(char *str)\n{\n\t/* tracemark_fd is the trace_marker file descriptor */\n\tif (tracemark_fd < 0)\n\t\treturn;\n\t/* write the tracemark message */\n\twrite(tracemark_fd, str, strlen(str));\n}\n\nAnd later add something like:\n\n\ttraceputs(\"start\");\n\tclock_nanosleep(...);\n\ttraceputs(\"end\");\n\nWe can make a histogram from this:\n\n # cd /sys/kernel/tracing\n # echo 'latency u64 lat' > synthetic_events\n # echo 'hist:keys=common_pid:ts0=common_timestamp.usecs if buf == \"start\"' > events/ftrace/print/trigger\n # echo 'hist:keys=common_pid:lat=common_timestamp.usecs-$ts0:onmatch(ftrace.print).latency($lat) if buf == \"end\"' >> events/ftrace/print/trigger\n # echo 'hist:keys=lat,common_pid:sort=lat' > events/synthetic/latency/trigger\n\nThe above created a synthetic event called \"latency\" and two histograms\nagainst the trace_marker, one gets triggered when \"start\" is written into the\ntrace_marker file and the other when \"end\" is written. If the pids match, then\nit will call the \"latency\" synthetic event with the calculated latency as its\nparameter. Finally, a histogram is added to the latency synthetic event to\nrecord the calculated latency along with the pid.\n\nNow running cyclictest with:\n\n # ./cyclictest -p80 -d0 -i250 -n -a -t --tracemark -b 1000\n\n -p80  : run threads at priority 80\n -d0   : have all threads run at the same interval\n -i250 : start the interval at 250 microseconds (all threads will do this)\n -n    : sleep with nanosleep\n -a    : affine all threads to a separate CPU\n -t    : one thread per available CPU\n --tracemark : enable trace mark writing\n -b 1000 : stop if any latency is greater than 1000 microseconds\n\nNote, the -b 1000 is used just to make --tracemark available.\n\nThen we can see the histogram created by this with:\n\n # cat events/synthetic/latency/hist\n# event histogram\n#\n# trigger info: hist:keys=lat,common_pid:vals=hitcount:sort=lat:size=2048 [active]\n#\n\n{ lat:        107, common_pid:       2039 } hitcount:          1\n{ lat:        122, common_pid:       2041 } hitcount:          1\n{ lat:        166, common_pid:       2039 } hitcount:          1\n{ lat:        174, common_pid:       2039 } hitcount:          1\n{ lat:        194, common_pid:       2041 } hitcount:          1\n{ lat:        196, common_pid:       2036 } hitcount:          1\n{ lat:        197, common_pid:       2038 } hitcount:          1\n{ lat:        198, common_pid:       2039 } hitcount:          1\n{ lat:        199, common_pid:       2039 } hitcount:          1\n{ lat:        200, common_pid:       2041 } hitcount:          1\n{ lat:        201, common_pid:       2039 } hitcount:          2\n{ lat:        202, common_pid:       2038 } hitcount:          1\n{ lat:        202, common_pid:       2043 } hitcount:          1\n{ lat:        203, common_pid:       2039 } hitcount:          1\n{ lat:        203, common_pid:       2036 } hitcount:          1\n{ lat:        203, common_pid:       2041 } hitcount:          1\n{ lat:        206, common_pid:       2038 } hitcount:          2\n{ lat:        207, common_pid:       2039 } hitcount:          1\n{ lat:        207, common_pid:       2036 } hitcount:          1\n{ lat:        208, common_pid:       2040 } hitcount:          1\n{ lat:        209, common_pid:       2043 } hitcount:          1\n{ lat:        210, common_pid:       2039 } hitcount:          1\n{ lat:        211, common_pid:       2039 } hitcount:          4\n{ lat:        212, common_pid:       2043 } hitcount:          1\n{ lat:        212, common_pid:       2039 } hitcount:          2\n{ lat:        213, common_pid:       2039 } hitcount:          1\n{ lat:        214, common_pid:       2038 } hitcount:          1\n{ lat:        214, common_pid:       2039 } hitcount:          2\n{ lat:        214, common_pid:       2042 } hitcount:          1\n{ lat:        215, common_pid:       2039 } hitcount:          1\n{ lat:        217, common_pid:       2036 } hitcount:          1\n{ lat:        217, common_pid:       2040 } hitcount:          1\n{ lat:        217, common_pid:       2039 } hitcount:          1\n{ lat:        218, common_pid:       2039 } hitcount:          6\n{ lat:        219, common_pid:       2039 } hitcount:          9\n{ lat:        220, common_pid:       2039 } hitcount:         11\n{ lat:        221, common_pid:       2039 } hitcount:          5\n{ lat:        221, common_pid:       2042 } hitcount:          1\n{ lat:        222, common_pid:       2039 } hitcount:          7\n{ lat:        223, common_pid:       2036 } hitcount:          1\n{ lat:        223, common_pid:       2039 } hitcount:          3\n{ lat:        224, common_pid:       2039 } hitcount:          4\n{ lat:        224, common_pid:       2037 } hitcount:          1\n{ lat:        224, common_pid:       2036 } hitcount:          2\n{ lat:        225, common_pid:       2039 } hitcount:          5\n{ lat:        225, common_pid:       2042 } hitcount:          1\n{ lat:        226, common_pid:       2039 } hitcount:          7\n{ lat:        226, common_pid:       2036 } hitcount:          4\n{ lat:        227, common_pid:       2039 } hitcount:          6\n{ lat:        227, common_pid:       2036 } hitcount:         12\n{ lat:        227, common_pid:       2043 } hitcount:          1\n{ lat:        228, common_pid:       2039 } hitcount:          7\n{ lat:        228, common_pid:       2036 } hitcount:         14\n{ lat:        229, common_pid:       2039 } hitcount:          9\n{ lat:        229, common_pid:       2036 } hitcount:          8\n{ lat:        229, common_pid:       2038 } hitcount:          1\n{ lat:        230, common_pid:       2039 } hitcount:         11\n{ lat:        230, common_pid:       2036 } hitcount:          6\n{ lat:        230, common_pid:       2043 } hitcount:          1\n{ lat:        230, common_pid:       2042 } hitcount:          2\n{ lat:        231, common_pid:       2041 } hitcount:          1\n{ lat:        231, common_pid:       2036 } hitcount:          6\n{ lat:        231, common_pid:       2043 } hitcount:          1\n{ lat:        231, common_pid:       2039 } hitcount:          8\n{ lat:        232, common_pid:       2037 } hitcount:          1\n{ lat:        232, common_pid:       2039 } hitcount:          6\n{ lat:        232, common_pid:       2040 } hitcount:          2\n{ lat:        232, common_pid:       2036 } hitcount:          5\n{ lat:        232, common_pid:       2043 } hitcount:          1\n{ lat:        233, common_pid:       2036 } hitcount:          5\n{ lat:        233, common_pid:       2039 } hitcount:         11\n{ lat:        234, common_pid:       2039 } hitcount:          4\n{ lat:        234, common_pid:       2038 } hitcount:          2\n{ lat:        234, common_pid:       2043 } hitcount:          2\n{ lat:        234, common_pid:       2036 } hitcount:         11\n{ lat:        234, common_pid:       2040 } hitcount:          1\n{ lat:        235, common_pid:       2037 } hitcount:          2\n{ lat:        235, common_pid:       2036 } hitcount:          8\n{ lat:        235, common_pid:       2043 } hitcount:          2\n{ lat:        235, common_pid:       2039 } hitcount:          5\n{ lat:        235, common_pid:       2042 } hitcount:          2\n{ lat:        235, common_pid:       2040 } hitcount:          4\n{ lat:        235, common_pid:       2041 } hitcount:          1\n{ lat:        236, common_pid:       2036 } hitcount:          7\n{ lat:        236, common_pid:       2037 } hitcount:          1\n{ lat:        236, common_pid:       2041 } hitcount:          5\n{ lat:        236, common_pid:       2039 } hitcount:          3\n{ lat:        236, common_pid:       2043 } hitcount:          9\n{ lat:        236, common_pid:       2040 } hitcount:          7\n{ lat:        237, common_pid:       2037 } hitcount:          1\n{ lat:        237, common_pid:       2040 } hitcount:          1\n{ lat:        237, common_pid:       2036 } hitcount:          9\n{ lat:        237, common_pid:       2039 } hitcount:          3\n{ lat:        237, common_pid:       2043 } hitcount:          8\n{ lat:        237, common_pid:       2042 } hitcount:          2\n{ lat:        237, common_pid:       2041 } hitcount:          2\n{ lat:        238, common_pid:       2043 } hitcount:         10\n{ lat:        238, common_pid:       2040 } hitcount:          1\n{ lat:        238, common_pid:       2037 } hitcount:          9\n{ lat:        238, common_pid:       2038 } hitcount:          1\n{ lat:        238, common_pid:       2039 } hitcount:          1\n{ lat:        238, common_pid:       2042 } hitcount:          3\n{ lat:        238, common_pid:       2036 } hitcount:          7\n{ lat:        239, common_pid:       2041 } hitcount:          1\n{ lat:        239, common_pid:       2043 } hitcount:         11\n{ lat:        239, common_pid:       2037 } hitcount:         11\n{ lat:        239, common_pid:       2038 } hitcount:          6\n{ lat:        239, common_pid:       2036 } hitcount:          7\n{ lat:        239, common_pid:       2040 } hitcount:          1\n{ lat:        239, common_pid:       2042 } hitcount:          9\n{ lat:        240, common_pid:       2037 } hitcount:         29\n{ lat:        240, common_pid:       2043 } hitcount:         15\n{ lat:        240, common_pid:       2040 } hitcount:         44\n{ lat:        240, common_pid:       2039 } hitcount:          1\n{ lat:        240, common_pid:       2041 } hitcount:          2\n{ lat:        240, common_pid:       2038 } hitcount:          1\n{ lat:        240, common_pid:       2036 } hitcount:         10\n{ lat:        240, common_pid:       2042 } hitcount:         13\n{ lat:        241, common_pid:       2036 } hitcount:         21\n{ lat:        241, common_pid:       2041 } hitcount:         36\n{ lat:        241, common_pid:       2037 } hitcount:         34\n{ lat:        241, common_pid:       2042 } hitcount:         14\n{ lat:        241, common_pid:       2040 } hitcount:         94\n{ lat:        241, common_pid:       2039 } hitcount:         12\n{ lat:        241, common_pid:       2038 } hitcount:          2\n{ lat:        241, common_pid:       2043 } hitcount:         28\n{ lat:        242, common_pid:       2040 } hitcount:        109\n{ lat:        242, common_pid:       2041 } hitcount:        506\n{ lat:        242, common_pid:       2039 } hitcount:        155\n{ lat:        242, common_pid:       2042 } hitcount:         21\n{ lat:        242, common_pid:       2037 } hitcount:         52\n{ lat:        242, common_pid:       2043 } hitcount:         21\n{ lat:        242, common_pid:       2036 } hitcount:         16\n{ lat:        242, common_pid:       2038 } hitcount:        156\n{ lat:        243, common_pid:       2037 } hitcount:         46\n{ lat:        243, common_pid:       2039 } hitcount:         40\n{ lat:        243, common_pid:       2042 } hitcount:        119\n{ lat:        243, common_pid:       2041 } hitcount:        611\n{ lat:        243, common_pid:       2036 } hitcount:         69\n{ lat:        243, common_pid:       2038 } hitcount:        784\n{ lat:        243, common_pid:       2040 } hitcount:        323\n{ lat:        243, common_pid:       2043 } hitcount:         14\n{ lat:        244, common_pid:       2043 } hitcount:         35\n{ lat:        244, common_pid:       2042 } hitcount:        305\n{ lat:        244, common_pid:       2039 } hitcount:          8\n{ lat:        244, common_pid:       2040 } hitcount:       4515\n{ lat:        244, common_pid:       2038 } hitcount:        371\n{ lat:        244, common_pid:       2037 } hitcount:         31\n{ lat:        244, common_pid:       2036 } hitcount:        114\n{ lat:        244, common_pid:       2041 } hitcount:       3396\n{ lat:        245, common_pid:       2036 } hitcount:        700\n{ lat:        245, common_pid:       2041 } hitcount:       2772\n{ lat:        245, common_pid:       2037 } hitcount:        268\n{ lat:        245, common_pid:       2039 } hitcount:        472\n{ lat:        245, common_pid:       2038 } hitcount:       2758\n{ lat:        245, common_pid:       2042 } hitcount:       3833\n{ lat:        245, common_pid:       2040 } hitcount:       3105\n{ lat:        245, common_pid:       2043 } hitcount:        645\n{ lat:        246, common_pid:       2038 } hitcount:       3451\n{ lat:        246, common_pid:       2041 } hitcount:        142\n{ lat:        246, common_pid:       2037 } hitcount:       5101\n{ lat:        246, common_pid:       2040 } hitcount:         68\n{ lat:        246, common_pid:       2043 } hitcount:       5099\n{ lat:        246, common_pid:       2039 } hitcount:       5608\n{ lat:        246, common_pid:       2042 } hitcount:       3723\n{ lat:        246, common_pid:       2036 } hitcount:       4738\n{ lat:        247, common_pid:       2042 } hitcount:        312\n{ lat:        247, common_pid:       2043 } hitcount:       2385\n{ lat:        247, common_pid:       2041 } hitcount:        452\n{ lat:        247, common_pid:       2038 } hitcount:        792\n{ lat:        247, common_pid:       2040 } hitcount:         78\n{ lat:        247, common_pid:       2036 } hitcount:       2375\n{ lat:        247, common_pid:       2039 } hitcount:       1834\n{ lat:        247, common_pid:       2037 } hitcount:       2655\n{ lat:        248, common_pid:       2037 } hitcount:         36\n{ lat:        248, common_pid:       2042 } hitcount:         11\n{ lat:        248, common_pid:       2038 } hitcount:        122\n{ lat:        248, common_pid:       2036 } hitcount:        135\n{ lat:        248, common_pid:       2039 } hitcount:         26\n{ lat:        248, common_pid:       2041 } hitcount:        503\n{ lat:        248, common_pid:       2043 } hitcount:         66\n{ lat:        248, common_pid:       2040 } hitcount:         46\n{ lat:        249, common_pid:       2037 } hitcount:         29\n{ lat:        249, common_pid:       2038 } hitcount:          1\n{ lat:        249, common_pid:       2043 } hitcount:         29\n{ lat:        249, common_pid:       2039 } hitcount:          8\n{ lat:        249, common_pid:       2042 } hitcount:         56\n{ lat:        249, common_pid:       2040 } hitcount:         27\n{ lat:        249, common_pid:       2041 } hitcount:         11\n{ lat:        249, common_pid:       2036 } hitcount:         27\n{ lat:        250, common_pid:       2038 } hitcount:          1\n{ lat:        250, common_pid:       2036 } hitcount:         30\n{ lat:        250, common_pid:       2040 } hitcount:         19\n{ lat:        250, common_pid:       2043 } hitcount:         22\n{ lat:        250, common_pid:       2042 } hitcount:         20\n{ lat:        250, common_pid:       2041 } hitcount:          1\n{ lat:        250, common_pid:       2039 } hitcount:          6\n{ lat:        250, common_pid:       2037 } hitcount:         48\n{ lat:        251, common_pid:       2037 } hitcount:         43\n{ lat:        251, common_pid:       2039 } hitcount:          1\n{ lat:        251, common_pid:       2036 } hitcount:         12\n{ lat:        251, common_pid:       2042 } hitcount:          2\n{ lat:        251, common_pid:       2041 } hitcount:          1\n{ lat:        251, common_pid:       2043 } hitcount:         15\n{ lat:        251, common_pid:       2040 } hitcount:          3\n{ lat:        252, common_pid:       2040 } hitcount:          1\n{ lat:        252, common_pid:       2036 } hitcount:         12\n{ lat:        252, common_pid:       2037 } hitcount:         21\n{ lat:        252, common_pid:       2043 } hitcount:         14\n{ lat:        253, common_pid:       2037 } hitcount:         21\n{ lat:        253, common_pid:       2039 } hitcount:          2\n{ lat:        253, common_pid:       2036 } hitcount:          9\n{ lat:        253, common_pid:       2043 } hitcount:          6\n{ lat:        253, common_pid:       2040 } hitcount:          1\n{ lat:        254, common_pid:       2036 } hitcount:          8\n{ lat:        254, common_pid:       2043 } hitcount:          3\n{ lat:        254, common_pid:       2041 } hitcount:          1\n{ lat:        254, common_pid:       2042 } hitcount:          1\n{ lat:        254, common_pid:       2039 } hitcount:          1\n{ lat:        254, common_pid:       2037 } hitcount:         12\n{ lat:        255, common_pid:       2043 } hitcount:          1\n{ lat:        255, common_pid:       2037 } hitcount:          2\n{ lat:        255, common_pid:       2036 } hitcount:          2\n{ lat:        255, common_pid:       2039 } hitcount:          8\n{ lat:        256, common_pid:       2043 } hitcount:          1\n{ lat:        256, common_pid:       2036 } hitcount:          4\n{ lat:        256, common_pid:       2039 } hitcount:          6\n{ lat:        257, common_pid:       2039 } hitcount:          5\n{ lat:        257, common_pid:       2036 } hitcount:          4\n{ lat:        258, common_pid:       2039 } hitcount:          5\n{ lat:        258, common_pid:       2036 } hitcount:          2\n{ lat:        259, common_pid:       2036 } hitcount:          7\n{ lat:        259, common_pid:       2039 } hitcount:          7\n{ lat:        260, common_pid:       2036 } hitcount:          8\n{ lat:        260, common_pid:       2039 } hitcount:          6\n{ lat:        261, common_pid:       2036 } hitcount:          5\n{ lat:        261, common_pid:       2039 } hitcount:          7\n{ lat:        262, common_pid:       2039 } hitcount:          5\n{ lat:        262, common_pid:       2036 } hitcount:          5\n{ lat:        263, common_pid:       2039 } hitcount:          7\n{ lat:        263, common_pid:       2036 } hitcount:          7\n{ lat:        264, common_pid:       2039 } hitcount:          9\n{ lat:        264, common_pid:       2036 } hitcount:          9\n{ lat:        265, common_pid:       2036 } hitcount:          5\n{ lat:        265, common_pid:       2039 } hitcount:          1\n{ lat:        266, common_pid:       2036 } hitcount:          1\n{ lat:        266, common_pid:       2039 } hitcount:          3\n{ lat:        267, common_pid:       2036 } hitcount:          1\n{ lat:        267, common_pid:       2039 } hitcount:          3\n{ lat:        268, common_pid:       2036 } hitcount:          1\n{ lat:        268, common_pid:       2039 } hitcount:          6\n{ lat:        269, common_pid:       2036 } hitcount:          1\n{ lat:        269, common_pid:       2043 } hitcount:          1\n{ lat:        269, common_pid:       2039 } hitcount:          2\n{ lat:        270, common_pid:       2040 } hitcount:          1\n{ lat:        270, common_pid:       2039 } hitcount:          6\n{ lat:        271, common_pid:       2041 } hitcount:          1\n{ lat:        271, common_pid:       2039 } hitcount:          5\n{ lat:        272, common_pid:       2039 } hitcount:         10\n{ lat:        273, common_pid:       2039 } hitcount:          8\n{ lat:        274, common_pid:       2039 } hitcount:          2\n{ lat:        275, common_pid:       2039 } hitcount:          1\n{ lat:        276, common_pid:       2039 } hitcount:          2\n{ lat:        276, common_pid:       2037 } hitcount:          1\n{ lat:        276, common_pid:       2038 } hitcount:          1\n{ lat:        277, common_pid:       2039 } hitcount:          1\n{ lat:        277, common_pid:       2042 } hitcount:          1\n{ lat:        278, common_pid:       2039 } hitcount:          1\n{ lat:        279, common_pid:       2039 } hitcount:          4\n{ lat:        279, common_pid:       2043 } hitcount:          1\n{ lat:        280, common_pid:       2039 } hitcount:          3\n{ lat:        283, common_pid:       2036 } hitcount:          2\n{ lat:        284, common_pid:       2039 } hitcount:          1\n{ lat:        284, common_pid:       2043 } hitcount:          1\n{ lat:        288, common_pid:       2039 } hitcount:          1\n{ lat:        289, common_pid:       2039 } hitcount:          1\n{ lat:        300, common_pid:       2039 } hitcount:          1\n{ lat:        384, common_pid:       2039 } hitcount:          1\n\nTotals:\n    Hits: 67625\n    Entries: 278\n    Dropped: 0\n\nNote, the writes are around the sleep, so ideally they will all be of 250\nmicroseconds. If you are wondering how there are several that are under\n250 microseconds, that is because the way cyclictest works, is if one\niteration comes in late, the next one will set the timer to wake up less that\n250. That is, if an iteration came in 50 microseconds late, the next wake up\nwill be at 200 microseconds.\n\nBut this could easily be done in userspace. To make this even more\ninteresting, we can mix the histogram between events that happened in the\nkernel with trace_marker.\n\n # cd /sys/kernel/tracing\n # echo 'latency u64 lat' > synthetic_events\n # echo 'hist:keys=pid:ts0=common_timestamp.usecs' > events/sched/sched_waking/trigger\n # echo 'hist:keys=common_pid:lat=common_timestamp.usecs-$ts0:onmatch(sched.sched_waking).latency($lat) if buf == \"end\"' > events/ftrace/print/trigger\n # echo 'hist:keys=lat,common_pid:sort=lat' > events/synthetic/latency/trigger\n\nThe difference this time is that instead of using the trace_marker to start\nthe latency, the sched_waking event is used, matching the common_pid for the\ntrace_marker write with the pid that is being woken by sched_waking.\n\nAfter running cyclictest again with the same parameters, we now have:\n\n # cat events/synthetic/latency/hist\n# event histogram\n#\n# trigger info: hist:keys=lat,common_pid:vals=hitcount:sort=lat:size=2048 [active]\n#\n\n{ lat:          7, common_pid:       2302 } hitcount:        640\n{ lat:          7, common_pid:       2299 } hitcount:         42\n{ lat:          7, common_pid:       2303 } hitcount:         18\n{ lat:          7, common_pid:       2305 } hitcount:        166\n{ lat:          7, common_pid:       2306 } hitcount:          1\n{ lat:          7, common_pid:       2301 } hitcount:         91\n{ lat:          7, common_pid:       2300 } hitcount:         17\n{ lat:          8, common_pid:       2303 } hitcount:       8296\n{ lat:          8, common_pid:       2304 } hitcount:       6864\n{ lat:          8, common_pid:       2305 } hitcount:       9464\n{ lat:          8, common_pid:       2301 } hitcount:       9213\n{ lat:          8, common_pid:       2306 } hitcount:       6246\n{ lat:          8, common_pid:       2302 } hitcount:       8797\n{ lat:          8, common_pid:       2299 } hitcount:       8771\n{ lat:          8, common_pid:       2300 } hitcount:       8119\n{ lat:          9, common_pid:       2305 } hitcount:       1519\n{ lat:          9, common_pid:       2299 } hitcount:       2346\n{ lat:          9, common_pid:       2303 } hitcount:       2841\n{ lat:          9, common_pid:       2301 } hitcount:       1846\n{ lat:          9, common_pid:       2304 } hitcount:       3861\n{ lat:          9, common_pid:       2302 } hitcount:       1210\n{ lat:          9, common_pid:       2300 } hitcount:       2762\n{ lat:          9, common_pid:       2306 } hitcount:       4247\n{ lat:         10, common_pid:       2299 } hitcount:         16\n{ lat:         10, common_pid:       2306 } hitcount:        333\n{ lat:         10, common_pid:       2303 } hitcount:         16\n{ lat:         10, common_pid:       2304 } hitcount:        168\n{ lat:         10, common_pid:       2302 } hitcount:        240\n{ lat:         10, common_pid:       2301 } hitcount:         28\n{ lat:         10, common_pid:       2300 } hitcount:         95\n{ lat:         10, common_pid:       2305 } hitcount:         18\n{ lat:         11, common_pid:       2303 } hitcount:          5\n{ lat:         11, common_pid:       2305 } hitcount:          8\n{ lat:         11, common_pid:       2306 } hitcount:        221\n{ lat:         11, common_pid:       2302 } hitcount:         76\n{ lat:         11, common_pid:       2304 } hitcount:         26\n{ lat:         11, common_pid:       2300 } hitcount:        125\n{ lat:         11, common_pid:       2299 } hitcount:          2\n{ lat:         12, common_pid:       2305 } hitcount:          3\n{ lat:         12, common_pid:       2300 } hitcount:          6\n{ lat:         12, common_pid:       2306 } hitcount:         90\n{ lat:         12, common_pid:       2302 } hitcount:          4\n{ lat:         12, common_pid:       2303 } hitcount:          1\n{ lat:         12, common_pid:       2304 } hitcount:        122\n{ lat:         13, common_pid:       2300 } hitcount:         12\n{ lat:         13, common_pid:       2301 } hitcount:          1\n{ lat:         13, common_pid:       2306 } hitcount:         32\n{ lat:         13, common_pid:       2302 } hitcount:          5\n{ lat:         13, common_pid:       2305 } hitcount:          1\n{ lat:         13, common_pid:       2303 } hitcount:          1\n{ lat:         13, common_pid:       2304 } hitcount:         61\n{ lat:         14, common_pid:       2303 } hitcount:          4\n{ lat:         14, common_pid:       2306 } hitcount:          5\n{ lat:         14, common_pid:       2305 } hitcount:          4\n{ lat:         14, common_pid:       2304 } hitcount:         62\n{ lat:         14, common_pid:       2302 } hitcount:         19\n{ lat:         14, common_pid:       2300 } hitcount:         33\n{ lat:         14, common_pid:       2299 } hitcount:          1\n{ lat:         14, common_pid:       2301 } hitcount:          4\n{ lat:         15, common_pid:       2305 } hitcount:          1\n{ lat:         15, common_pid:       2302 } hitcount:         25\n{ lat:         15, common_pid:       2300 } hitcount:         11\n{ lat:         15, common_pid:       2299 } hitcount:          5\n{ lat:         15, common_pid:       2301 } hitcount:          1\n{ lat:         15, common_pid:       2304 } hitcount:          8\n{ lat:         15, common_pid:       2303 } hitcount:          1\n{ lat:         15, common_pid:       2306 } hitcount:          6\n{ lat:         16, common_pid:       2302 } hitcount:         31\n{ lat:         16, common_pid:       2306 } hitcount:          3\n{ lat:         16, common_pid:       2300 } hitcount:          5\n{ lat:         17, common_pid:       2302 } hitcount:          6\n{ lat:         17, common_pid:       2303 } hitcount:          1\n{ lat:         18, common_pid:       2304 } hitcount:          1\n{ lat:         18, common_pid:       2302 } hitcount:          8\n{ lat:         18, common_pid:       2299 } hitcount:          1\n{ lat:         18, common_pid:       2301 } hitcount:          1\n{ lat:         19, common_pid:       2303 } hitcount:          4\n{ lat:         19, common_pid:       2304 } hitcount:          5\n{ lat:         19, common_pid:       2302 } hitcount:          4\n{ lat:         19, common_pid:       2299 } hitcount:          3\n{ lat:         19, common_pid:       2306 } hitcount:          1\n{ lat:         19, common_pid:       2300 } hitcount:          4\n{ lat:         19, common_pid:       2305 } hitcount:          5\n{ lat:         20, common_pid:       2299 } hitcount:          2\n{ lat:         20, common_pid:       2302 } hitcount:          3\n{ lat:         20, common_pid:       2305 } hitcount:          1\n{ lat:         20, common_pid:       2300 } hitcount:          2\n{ lat:         20, common_pid:       2301 } hitcount:          2\n{ lat:         20, common_pid:       2303 } hitcount:          3\n{ lat:         21, common_pid:       2305 } hitcount:          1\n{ lat:         21, common_pid:       2299 } hitcount:          5\n{ lat:         21, common_pid:       2303 } hitcount:          4\n{ lat:         21, common_pid:       2302 } hitcount:          7\n{ lat:         21, common_pid:       2300 } hitcount:          1\n{ lat:         21, common_pid:       2301 } hitcount:          5\n{ lat:         21, common_pid:       2304 } hitcount:          2\n{ lat:         22, common_pid:       2302 } hitcount:          5\n{ lat:         22, common_pid:       2303 } hitcount:          1\n{ lat:         22, common_pid:       2306 } hitcount:          3\n{ lat:         22, common_pid:       2301 } hitcount:          2\n{ lat:         22, common_pid:       2300 } hitcount:          1\n{ lat:         22, common_pid:       2299 } hitcount:          1\n{ lat:         22, common_pid:       2305 } hitcount:          1\n{ lat:         22, common_pid:       2304 } hitcount:          1\n{ lat:         23, common_pid:       2299 } hitcount:          1\n{ lat:         23, common_pid:       2306 } hitcount:          2\n{ lat:         23, common_pid:       2302 } hitcount:          6\n{ lat:         24, common_pid:       2302 } hitcount:          3\n{ lat:         24, common_pid:       2300 } hitcount:          1\n{ lat:         24, common_pid:       2306 } hitcount:          2\n{ lat:         24, common_pid:       2305 } hitcount:          1\n{ lat:         24, common_pid:       2299 } hitcount:          1\n{ lat:         25, common_pid:       2300 } hitcount:          1\n{ lat:         25, common_pid:       2302 } hitcount:          4\n{ lat:         26, common_pid:       2302 } hitcount:          2\n{ lat:         27, common_pid:       2305 } hitcount:          1\n{ lat:         27, common_pid:       2300 } hitcount:          1\n{ lat:         27, common_pid:       2302 } hitcount:          3\n{ lat:         28, common_pid:       2306 } hitcount:          1\n{ lat:         28, common_pid:       2302 } hitcount:          4\n{ lat:         29, common_pid:       2302 } hitcount:          1\n{ lat:         29, common_pid:       2300 } hitcount:          2\n{ lat:         29, common_pid:       2306 } hitcount:          1\n{ lat:         29, common_pid:       2304 } hitcount:          1\n{ lat:         30, common_pid:       2302 } hitcount:          4\n{ lat:         31, common_pid:       2302 } hitcount:          6\n{ lat:         32, common_pid:       2302 } hitcount:          1\n{ lat:         33, common_pid:       2299 } hitcount:          1\n{ lat:         33, common_pid:       2302 } hitcount:          3\n{ lat:         34, common_pid:       2302 } hitcount:          2\n{ lat:         35, common_pid:       2302 } hitcount:          1\n{ lat:         35, common_pid:       2304 } hitcount:          1\n{ lat:         36, common_pid:       2302 } hitcount:          4\n{ lat:         37, common_pid:       2302 } hitcount:          6\n{ lat:         38, common_pid:       2302 } hitcount:          2\n{ lat:         39, common_pid:       2302 } hitcount:          2\n{ lat:         39, common_pid:       2304 } hitcount:          1\n{ lat:         40, common_pid:       2304 } hitcount:          2\n{ lat:         40, common_pid:       2302 } hitcount:          5\n{ lat:         41, common_pid:       2304 } hitcount:          1\n{ lat:         41, common_pid:       2302 } hitcount:          8\n{ lat:         42, common_pid:       2302 } hitcount:          6\n{ lat:         42, common_pid:       2304 } hitcount:          1\n{ lat:         43, common_pid:       2302 } hitcount:          3\n{ lat:         43, common_pid:       2304 } hitcount:          4\n{ lat:         44, common_pid:       2302 } hitcount:          6\n{ lat:         45, common_pid:       2302 } hitcount:          5\n{ lat:         46, common_pid:       2302 } hitcount:          5\n{ lat:         47, common_pid:       2302 } hitcount:          7\n{ lat:         48, common_pid:       2301 } hitcount:          1\n{ lat:         48, common_pid:       2302 } hitcount:          9\n{ lat:         49, common_pid:       2302 } hitcount:          3\n{ lat:         50, common_pid:       2302 } hitcount:          1\n{ lat:         50, common_pid:       2301 } hitcount:          1\n{ lat:         51, common_pid:       2302 } hitcount:          2\n{ lat:         51, common_pid:       2301 } hitcount:          1\n{ lat:         61, common_pid:       2302 } hitcount:          1\n{ lat:        110, common_pid:       2302 } hitcount:          1\n\nTotals:\n    Hits: 89565\n    Entries: 158\n    Dropped: 0\n\nThis doesn't tell us any information about how late cyclictest may have\nwoken up, but it does show us a nice histogram of how long it took from\nthe time that cyclictest was woken to the time it made it into user space.\n", "/*\n *\tlinux/kernel/softirq.c\n *\n *\tCopyright (C) 1992 Linus Torvalds\n *\n *\tDistribute under GPLv2.\n *\n *\tRewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/export.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/cpu.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include <linux/rcupdate.h>\n#include <linux/ftrace.h>\n#include <linux/smp.h>\n#include <linux/smpboot.h>\n#include <linux/tick.h>\n#include <linux/irq.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/irq.h>\n\n/*\n   - No shared variables, all the data are CPU local.\n   - If a softirq needs serialization, let it serialize itself\n     by its own spinlocks.\n   - Even if softirq is serialized, only local cpu is marked for\n     execution. Hence, we get something sort of weak cpu binding.\n     Though it is still not clear, will it result in better locality\n     or will not.\n\n   Examples:\n   - NET RX softirq. It is multithreaded and does not require\n     any global serialization.\n   - NET TX softirq. It kicks software netdevice queues, hence\n     it is logically serialized per device, but this serialization\n     is invisible to common code.\n   - Tasklets: serialized wrt itself.\n */\n\n#ifndef __ARCH_IRQ_STAT\nDEFINE_PER_CPU_ALIGNED(irq_cpustat_t, irq_stat);\nEXPORT_PER_CPU_SYMBOL(irq_stat);\n#endif\n\nstatic struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;\n\nDEFINE_PER_CPU(struct task_struct *, ksoftirqd);\n\nconst char * const softirq_to_name[NR_SOFTIRQS] = {\n\t\"HI\", \"TIMER\", \"NET_TX\", \"NET_RX\", \"BLOCK\", \"IRQ_POLL\",\n\t\"TASKLET\", \"SCHED\", \"HRTIMER\", \"RCU\"\n};\n\n/*\n * we cannot loop indefinitely here to avoid userspace starvation,\n * but we also don't want to introduce a worst case 1/HZ latency\n * to the pending events, so lets the scheduler to balance\n * the softirq load for us.\n */\nstatic void wakeup_softirqd(void)\n{\n\t/* Interrupts are disabled: no need to stop preemption */\n\tstruct task_struct *tsk = __this_cpu_read(ksoftirqd);\n\n\tif (tsk && tsk->state != TASK_RUNNING)\n\t\twake_up_process(tsk);\n}\n\n/*\n * If ksoftirqd is scheduled, we do not want to process pending softirqs\n * right now. Let ksoftirqd handle this at its own rate, to get fairness.\n */\nstatic bool ksoftirqd_running(void)\n{\n\tstruct task_struct *tsk = __this_cpu_read(ksoftirqd);\n\n\treturn tsk && (tsk->state == TASK_RUNNING);\n}\n\n/*\n * preempt_count and SOFTIRQ_OFFSET usage:\n * - preempt_count is changed by SOFTIRQ_OFFSET on entering or leaving\n *   softirq processing.\n * - preempt_count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)\n *   on local_bh_disable or local_bh_enable.\n * This lets us distinguish between whether we are currently processing\n * softirq and whether we just have bh disabled.\n */\n\n/*\n * This one is for softirq.c-internal use,\n * where hardirqs are disabled legitimately:\n */\n#ifdef CONFIG_TRACE_IRQFLAGS\nvoid __local_bh_disable_ip(unsigned long ip, unsigned int cnt)\n{\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(in_irq());\n\n\traw_local_irq_save(flags);\n\t/*\n\t * The preempt tracer hooks into preempt_count_add and will break\n\t * lockdep because it calls back into lockdep after SOFTIRQ_OFFSET\n\t * is set and before current->softirq_enabled is cleared.\n\t * We must manually increment preempt_count here and manually\n\t * call the trace_preempt_off later.\n\t */\n\t__preempt_count_add(cnt);\n\t/*\n\t * Were softirqs turned off above:\n\t */\n\tif (softirq_count() == (cnt & SOFTIRQ_MASK))\n\t\ttrace_softirqs_off(ip);\n\traw_local_irq_restore(flags);\n\n\tif (preempt_count() == cnt) {\n#ifdef CONFIG_DEBUG_PREEMPT\n\t\tcurrent->preempt_disable_ip = get_lock_parent_ip();\n#endif\n\t\ttrace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());\n\t}\n}\nEXPORT_SYMBOL(__local_bh_disable_ip);\n#endif /* CONFIG_TRACE_IRQFLAGS */\n\nstatic void __local_bh_enable(unsigned int cnt)\n{\n\tlockdep_assert_irqs_disabled();\n\n\tif (softirq_count() == (cnt & SOFTIRQ_MASK))\n\t\ttrace_softirqs_on(_RET_IP_);\n\tpreempt_count_sub(cnt);\n}\n\n/*\n * Special-case - softirqs can safely be enabled by __do_softirq(),\n * without processing still-pending softirqs:\n */\nvoid _local_bh_enable(void)\n{\n\tWARN_ON_ONCE(in_irq());\n\t__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);\n}\nEXPORT_SYMBOL(_local_bh_enable);\n\nvoid __local_bh_enable_ip(unsigned long ip, unsigned int cnt)\n{\n\tWARN_ON_ONCE(in_irq());\n\tlockdep_assert_irqs_enabled();\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tlocal_irq_disable();\n#endif\n\t/*\n\t * Are softirqs going to be turned on now:\n\t */\n\tif (softirq_count() == SOFTIRQ_DISABLE_OFFSET)\n\t\ttrace_softirqs_on(ip);\n\t/*\n\t * Keep preemption disabled until we are done with\n\t * softirq processing:\n\t */\n\tpreempt_count_sub(cnt - 1);\n\n\tif (unlikely(!in_interrupt() && local_softirq_pending())) {\n\t\t/*\n\t\t * Run softirq if any pending. And do it in its own stack\n\t\t * as we may be calling this deep in a task call stack already.\n\t\t */\n\t\tdo_softirq();\n\t}\n\n\tpreempt_count_dec();\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tlocal_irq_enable();\n#endif\n\tpreempt_check_resched();\n}\nEXPORT_SYMBOL(__local_bh_enable_ip);\n\n/*\n * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,\n * but break the loop if need_resched() is set or after 2 ms.\n * The MAX_SOFTIRQ_TIME provides a nice upper bound in most cases, but in\n * certain cases, such as stop_machine(), jiffies may cease to\n * increment and so we need the MAX_SOFTIRQ_RESTART limit as\n * well to make sure we eventually return from this method.\n *\n * These limits have been established via experimentation.\n * The two things to balance is latency against fairness -\n * we want to handle softirqs as soon as possible, but they\n * should not be able to lock up the box.\n */\n#define MAX_SOFTIRQ_TIME  msecs_to_jiffies(2)\n#define MAX_SOFTIRQ_RESTART 10\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n/*\n * When we run softirqs from irq_exit() and thus on the hardirq stack we need\n * to keep the lockdep irq context tracking as tight as possible in order to\n * not miss-qualify lock contexts and miss possible deadlocks.\n */\n\nstatic inline bool lockdep_softirq_start(void)\n{\n\tbool in_hardirq = false;\n\n\tif (trace_hardirq_context(current)) {\n\t\tin_hardirq = true;\n\t\ttrace_hardirq_exit();\n\t}\n\n\tlockdep_softirq_enter();\n\n\treturn in_hardirq;\n}\n\nstatic inline void lockdep_softirq_end(bool in_hardirq)\n{\n\tlockdep_softirq_exit();\n\n\tif (in_hardirq)\n\t\ttrace_hardirq_enter();\n}\n#else\nstatic inline bool lockdep_softirq_start(void) { return false; }\nstatic inline void lockdep_softirq_end(bool in_hardirq) { }\n#endif\n\nasmlinkage __visible void __softirq_entry __do_softirq(void)\n{\n\tunsigned long end = jiffies + MAX_SOFTIRQ_TIME;\n\tunsigned long old_flags = current->flags;\n\tint max_restart = MAX_SOFTIRQ_RESTART;\n\tstruct softirq_action *h;\n\tbool in_hardirq;\n\t__u32 pending;\n\tint softirq_bit;\n\n\t/*\n\t * Mask out PF_MEMALLOC s current task context is borrowed for the\n\t * softirq. A softirq handled such as network RX might set PF_MEMALLOC\n\t * again if the socket is related to swap\n\t */\n\tcurrent->flags &= ~PF_MEMALLOC;\n\n\tpending = local_softirq_pending();\n\taccount_irq_enter_time(current);\n\n\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);\n\tin_hardirq = lockdep_softirq_start();\n\nrestart:\n\t/* Reset the pending bitmask before enabling irqs */\n\tset_softirq_pending(0);\n\n\tlocal_irq_enable();\n\n\th = softirq_vec;\n\n\twhile ((softirq_bit = ffs(pending))) {\n\t\tunsigned int vec_nr;\n\t\tint prev_count;\n\n\t\th += softirq_bit - 1;\n\n\t\tvec_nr = h - softirq_vec;\n\t\tprev_count = preempt_count();\n\n\t\tkstat_incr_softirqs_this_cpu(vec_nr);\n\n\t\ttrace_softirq_entry(vec_nr);\n\t\th->action(h);\n\t\ttrace_softirq_exit(vec_nr);\n\t\tif (unlikely(prev_count != preempt_count())) {\n\t\t\tpr_err(\"huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\\n\",\n\t\t\t       vec_nr, softirq_to_name[vec_nr], h->action,\n\t\t\t       prev_count, preempt_count());\n\t\t\tpreempt_count_set(prev_count);\n\t\t}\n\t\th++;\n\t\tpending >>= softirq_bit;\n\t}\n\n\trcu_bh_qs();\n\tlocal_irq_disable();\n\n\tpending = local_softirq_pending();\n\tif (pending) {\n\t\tif (time_before(jiffies, end) && !need_resched() &&\n\t\t    --max_restart)\n\t\t\tgoto restart;\n\n\t\twakeup_softirqd();\n\t}\n\n\tlockdep_softirq_end(in_hardirq);\n\taccount_irq_exit_time(current);\n\t__local_bh_enable(SOFTIRQ_OFFSET);\n\tWARN_ON_ONCE(in_interrupt());\n\tcurrent_restore_flags(old_flags, PF_MEMALLOC);\n}\n\nasmlinkage __visible void do_softirq(void)\n{\n\t__u32 pending;\n\tunsigned long flags;\n\n\tif (in_interrupt())\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tpending = local_softirq_pending();\n\n\tif (pending && !ksoftirqd_running())\n\t\tdo_softirq_own_stack();\n\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Enter an interrupt context.\n */\nvoid irq_enter(void)\n{\n\trcu_irq_enter();\n\tif (is_idle_task(current) && !in_interrupt()) {\n\t\t/*\n\t\t * Prevent raise_softirq from needlessly waking up ksoftirqd\n\t\t * here, as softirq will be serviced on return from interrupt.\n\t\t */\n\t\tlocal_bh_disable();\n\t\ttick_irq_enter();\n\t\t_local_bh_enable();\n\t}\n\n\t__irq_enter();\n}\n\nstatic inline void invoke_softirq(void)\n{\n\tif (ksoftirqd_running())\n\t\treturn;\n\n\tif (!force_irqthreads) {\n#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK\n\t\t/*\n\t\t * We can safely execute softirq on the current stack if\n\t\t * it is the irq stack, because it should be near empty\n\t\t * at this stage.\n\t\t */\n\t\t__do_softirq();\n#else\n\t\t/*\n\t\t * Otherwise, irq_exit() is called on the task stack that can\n\t\t * be potentially deep already. So call softirq in its own stack\n\t\t * to prevent from any overrun.\n\t\t */\n\t\tdo_softirq_own_stack();\n#endif\n\t} else {\n\t\twakeup_softirqd();\n\t}\n}\n\nstatic inline void tick_irq_exit(void)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tint cpu = smp_processor_id();\n\n\t/* Make sure that timer wheel updates are propagated */\n\tif ((idle_cpu(cpu) && !need_resched()) || tick_nohz_full_cpu(cpu)) {\n\t\tif (!in_interrupt())\n\t\t\ttick_nohz_irq_exit();\n\t}\n#endif\n}\n\n/*\n * Exit an interrupt context. Process softirqs if needed and possible:\n */\nvoid irq_exit(void)\n{\n#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED\n\tlocal_irq_disable();\n#else\n\tlockdep_assert_irqs_disabled();\n#endif\n\taccount_irq_exit_time(current);\n\tpreempt_count_sub(HARDIRQ_OFFSET);\n\tif (!in_interrupt() && local_softirq_pending())\n\t\tinvoke_softirq();\n\n\ttick_irq_exit();\n\trcu_irq_exit();\n\ttrace_hardirq_exit(); /* must be last! */\n}\n\n/*\n * This function must run with irqs disabled!\n */\ninline void raise_softirq_irqoff(unsigned int nr)\n{\n\t__raise_softirq_irqoff(nr);\n\n\t/*\n\t * If we're in an interrupt or softirq, we're done\n\t * (this also catches softirq-disabled code). We will\n\t * actually run the softirq once we return from\n\t * the irq or softirq.\n\t *\n\t * Otherwise we wake up ksoftirqd to make sure we\n\t * schedule the softirq soon.\n\t */\n\tif (!in_interrupt())\n\t\twakeup_softirqd();\n}\n\nvoid raise_softirq(unsigned int nr)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\traise_softirq_irqoff(nr);\n\tlocal_irq_restore(flags);\n}\n\nvoid __raise_softirq_irqoff(unsigned int nr)\n{\n\ttrace_softirq_raise(nr);\n\tor_softirq_pending(1UL << nr);\n}\n\nvoid open_softirq(int nr, void (*action)(struct softirq_action *))\n{\n\tsoftirq_vec[nr].action = action;\n}\n\n/*\n * Tasklets\n */\nstruct tasklet_head {\n\tstruct tasklet_struct *head;\n\tstruct tasklet_struct **tail;\n};\n\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);\n\nstatic void __tasklet_schedule_common(struct tasklet_struct *t,\n\t\t\t\t      struct tasklet_head __percpu *headp,\n\t\t\t\t      unsigned int softirq_nr)\n{\n\tstruct tasklet_head *head;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\thead = this_cpu_ptr(headp);\n\tt->next = NULL;\n\t*head->tail = t;\n\thead->tail = &(t->next);\n\traise_softirq_irqoff(softirq_nr);\n\tlocal_irq_restore(flags);\n}\n\nvoid __tasklet_schedule(struct tasklet_struct *t)\n{\n\t__tasklet_schedule_common(t, &tasklet_vec,\n\t\t\t\t  TASKLET_SOFTIRQ);\n}\nEXPORT_SYMBOL(__tasklet_schedule);\n\nvoid __tasklet_hi_schedule(struct tasklet_struct *t)\n{\n\t__tasklet_schedule_common(t, &tasklet_hi_vec,\n\t\t\t\t  HI_SOFTIRQ);\n}\nEXPORT_SYMBOL(__tasklet_hi_schedule);\n\nstatic void tasklet_action_common(struct softirq_action *a,\n\t\t\t\t  struct tasklet_head *tl_head,\n\t\t\t\t  unsigned int softirq_nr)\n{\n\tstruct tasklet_struct *list;\n\n\tlocal_irq_disable();\n\tlist = tl_head->head;\n\ttl_head->head = NULL;\n\ttl_head->tail = &tl_head->head;\n\tlocal_irq_enable();\n\n\twhile (list) {\n\t\tstruct tasklet_struct *t = list;\n\n\t\tlist = list->next;\n\n\t\tif (tasklet_trylock(t)) {\n\t\t\tif (!atomic_read(&t->count)) {\n\t\t\t\tif (!test_and_clear_bit(TASKLET_STATE_SCHED,\n\t\t\t\t\t\t\t&t->state))\n\t\t\t\t\tBUG();\n\t\t\t\tt->func(t->data);\n\t\t\t\ttasklet_unlock(t);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttasklet_unlock(t);\n\t\t}\n\n\t\tlocal_irq_disable();\n\t\tt->next = NULL;\n\t\t*tl_head->tail = t;\n\t\ttl_head->tail = &t->next;\n\t\t__raise_softirq_irqoff(softirq_nr);\n\t\tlocal_irq_enable();\n\t}\n}\n\nstatic __latent_entropy void tasklet_action(struct softirq_action *a)\n{\n\ttasklet_action_common(a, this_cpu_ptr(&tasklet_vec), TASKLET_SOFTIRQ);\n}\n\nstatic __latent_entropy void tasklet_hi_action(struct softirq_action *a)\n{\n\ttasklet_action_common(a, this_cpu_ptr(&tasklet_hi_vec), HI_SOFTIRQ);\n}\n\nvoid tasklet_init(struct tasklet_struct *t,\n\t\t  void (*func)(unsigned long), unsigned long data)\n{\n\tt->next = NULL;\n\tt->state = 0;\n\tatomic_set(&t->count, 0);\n\tt->func = func;\n\tt->data = data;\n}\nEXPORT_SYMBOL(tasklet_init);\n\nvoid tasklet_kill(struct tasklet_struct *t)\n{\n\tif (in_interrupt())\n\t\tpr_notice(\"Attempt to kill tasklet from interrupt\\n\");\n\n\twhile (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {\n\t\tdo {\n\t\t\tyield();\n\t\t} while (test_bit(TASKLET_STATE_SCHED, &t->state));\n\t}\n\ttasklet_unlock_wait(t);\n\tclear_bit(TASKLET_STATE_SCHED, &t->state);\n}\nEXPORT_SYMBOL(tasklet_kill);\n\n/*\n * tasklet_hrtimer\n */\n\n/*\n * The trampoline is called when the hrtimer expires. It schedules a tasklet\n * to run __tasklet_hrtimer_trampoline() which in turn will call the intended\n * hrtimer callback, but from softirq context.\n */\nstatic enum hrtimer_restart __hrtimer_tasklet_trampoline(struct hrtimer *timer)\n{\n\tstruct tasklet_hrtimer *ttimer =\n\t\tcontainer_of(timer, struct tasklet_hrtimer, timer);\n\n\ttasklet_hi_schedule(&ttimer->tasklet);\n\treturn HRTIMER_NORESTART;\n}\n\n/*\n * Helper function which calls the hrtimer callback from\n * tasklet/softirq context\n */\nstatic void __tasklet_hrtimer_trampoline(unsigned long data)\n{\n\tstruct tasklet_hrtimer *ttimer = (void *)data;\n\tenum hrtimer_restart restart;\n\n\trestart = ttimer->function(&ttimer->timer);\n\tif (restart != HRTIMER_NORESTART)\n\t\thrtimer_restart(&ttimer->timer);\n}\n\n/**\n * tasklet_hrtimer_init - Init a tasklet/hrtimer combo for softirq callbacks\n * @ttimer:\t tasklet_hrtimer which is initialized\n * @function:\t hrtimer callback function which gets called from softirq context\n * @which_clock: clock id (CLOCK_MONOTONIC/CLOCK_REALTIME)\n * @mode:\t hrtimer mode (HRTIMER_MODE_ABS/HRTIMER_MODE_REL)\n */\nvoid tasklet_hrtimer_init(struct tasklet_hrtimer *ttimer,\n\t\t\t  enum hrtimer_restart (*function)(struct hrtimer *),\n\t\t\t  clockid_t which_clock, enum hrtimer_mode mode)\n{\n\thrtimer_init(&ttimer->timer, which_clock, mode);\n\tttimer->timer.function = __hrtimer_tasklet_trampoline;\n\ttasklet_init(&ttimer->tasklet, __tasklet_hrtimer_trampoline,\n\t\t     (unsigned long)ttimer);\n\tttimer->function = function;\n}\nEXPORT_SYMBOL_GPL(tasklet_hrtimer_init);\n\nvoid __init softirq_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tper_cpu(tasklet_vec, cpu).tail =\n\t\t\t&per_cpu(tasklet_vec, cpu).head;\n\t\tper_cpu(tasklet_hi_vec, cpu).tail =\n\t\t\t&per_cpu(tasklet_hi_vec, cpu).head;\n\t}\n\n\topen_softirq(TASKLET_SOFTIRQ, tasklet_action);\n\topen_softirq(HI_SOFTIRQ, tasklet_hi_action);\n}\n\nstatic int ksoftirqd_should_run(unsigned int cpu)\n{\n\treturn local_softirq_pending();\n}\n\nstatic void run_ksoftirqd(unsigned int cpu)\n{\n\tlocal_irq_disable();\n\tif (local_softirq_pending()) {\n\t\t/*\n\t\t * We can safely run softirq on inline stack, as we are not deep\n\t\t * in the task stack here.\n\t\t */\n\t\t__do_softirq();\n\t\tlocal_irq_enable();\n\t\tcond_resched();\n\t\treturn;\n\t}\n\tlocal_irq_enable();\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n/*\n * tasklet_kill_immediate is called to remove a tasklet which can already be\n * scheduled for execution on @cpu.\n *\n * Unlike tasklet_kill, this function removes the tasklet\n * _immediately_, even if the tasklet is in TASKLET_STATE_SCHED state.\n *\n * When this function is called, @cpu must be in the CPU_DEAD state.\n */\nvoid tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu)\n{\n\tstruct tasklet_struct **i;\n\n\tBUG_ON(cpu_online(cpu));\n\tBUG_ON(test_bit(TASKLET_STATE_RUN, &t->state));\n\n\tif (!test_bit(TASKLET_STATE_SCHED, &t->state))\n\t\treturn;\n\n\t/* CPU is dead, so no lock needed. */\n\tfor (i = &per_cpu(tasklet_vec, cpu).head; *i; i = &(*i)->next) {\n\t\tif (*i == t) {\n\t\t\t*i = t->next;\n\t\t\t/* If this was the tail element, move the tail ptr */\n\t\t\tif (*i == NULL)\n\t\t\t\tper_cpu(tasklet_vec, cpu).tail = i;\n\t\t\treturn;\n\t\t}\n\t}\n\tBUG();\n}\n\nstatic int takeover_tasklets(unsigned int cpu)\n{\n\t/* CPU is dead, so no lock needed. */\n\tlocal_irq_disable();\n\n\t/* Find end, append list for that CPU. */\n\tif (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {\n\t\t*__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;\n\t\tthis_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);\n\t\tper_cpu(tasklet_vec, cpu).head = NULL;\n\t\tper_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;\n\t}\n\traise_softirq_irqoff(TASKLET_SOFTIRQ);\n\n\tif (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {\n\t\t*__this_cpu_read(tasklet_hi_vec.tail) = per_cpu(tasklet_hi_vec, cpu).head;\n\t\t__this_cpu_write(tasklet_hi_vec.tail, per_cpu(tasklet_hi_vec, cpu).tail);\n\t\tper_cpu(tasklet_hi_vec, cpu).head = NULL;\n\t\tper_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;\n\t}\n\traise_softirq_irqoff(HI_SOFTIRQ);\n\n\tlocal_irq_enable();\n\treturn 0;\n}\n#else\n#define takeover_tasklets\tNULL\n#endif /* CONFIG_HOTPLUG_CPU */\n\nstatic struct smp_hotplug_thread softirq_threads = {\n\t.store\t\t\t= &ksoftirqd,\n\t.thread_should_run\t= ksoftirqd_should_run,\n\t.thread_fn\t\t= run_ksoftirqd,\n\t.thread_comm\t\t= \"ksoftirqd/%u\",\n};\n\nstatic __init int spawn_ksoftirqd(void)\n{\n\tcpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, \"softirq:dead\", NULL,\n\t\t\t\t  takeover_tasklets);\n\tBUG_ON(smpboot_register_percpu_thread(&softirq_threads));\n\n\treturn 0;\n}\nearly_initcall(spawn_ksoftirqd);\n\n/*\n * [ These __weak aliases are kept in a separate compilation unit, so that\n *   GCC does not inline them incorrectly. ]\n */\n\nint __init __weak early_irq_init(void)\n{\n\treturn 0;\n}\n\nint __init __weak arch_probe_nr_irqs(void)\n{\n\treturn NR_IRQS_LEGACY;\n}\n\nint __init __weak arch_early_irq_init(void)\n{\n\treturn 0;\n}\n\nunsigned int __weak arch_dynirq_lower_bound(unsigned int from)\n{\n\treturn from;\n}\n", "/*\n * ring buffer based function tracer\n *\n * Copyright (C) 2007-2012 Steven Rostedt <srostedt@redhat.com>\n * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>\n *\n * Originally taken from the RT patch by:\n *    Arnaldo Carvalho de Melo <acme@redhat.com>\n *\n * Based on code from the latency_tracer, that is:\n *  Copyright (C) 2004-2006 Ingo Molnar\n *  Copyright (C) 2004 Nadia Yvette Chambers\n */\n#include <linux/ring_buffer.h>\n#include <generated/utsrelease.h>\n#include <linux/stacktrace.h>\n#include <linux/writeback.h>\n#include <linux/kallsyms.h>\n#include <linux/seq_file.h>\n#include <linux/notifier.h>\n#include <linux/irqflags.h>\n#include <linux/debugfs.h>\n#include <linux/tracefs.h>\n#include <linux/pagemap.h>\n#include <linux/hardirq.h>\n#include <linux/linkage.h>\n#include <linux/uaccess.h>\n#include <linux/vmalloc.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/percpu.h>\n#include <linux/splice.h>\n#include <linux/kdebug.h>\n#include <linux/string.h>\n#include <linux/mount.h>\n#include <linux/rwsem.h>\n#include <linux/slab.h>\n#include <linux/ctype.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/nmi.h>\n#include <linux/fs.h>\n#include <linux/trace.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/rt.h>\n\n#include \"trace.h\"\n#include \"trace_output.h\"\n\n/*\n * On boot up, the ring buffer is set to the minimum size, so that\n * we do not waste memory on systems that are not using tracing.\n */\nbool ring_buffer_expanded;\n\n/*\n * We need to change this state when a selftest is running.\n * A selftest will lurk into the ring-buffer to count the\n * entries inserted during the selftest although some concurrent\n * insertions into the ring-buffer such as trace_printk could occurred\n * at the same time, giving false positive or negative results.\n */\nstatic bool __read_mostly tracing_selftest_running;\n\n/*\n * If a tracer is running, we do not want to run SELFTEST.\n */\nbool __read_mostly tracing_selftest_disabled;\n\n/* Pipe tracepoints to printk */\nstruct trace_iterator *tracepoint_print_iter;\nint tracepoint_printk;\nstatic DEFINE_STATIC_KEY_FALSE(tracepoint_printk_key);\n\n/* For tracers that don't implement custom flags */\nstatic struct tracer_opt dummy_tracer_opt[] = {\n\t{ }\n};\n\nstatic int\ndummy_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\n{\n\treturn 0;\n}\n\n/*\n * To prevent the comm cache from being overwritten when no\n * tracing is active, only save the comm when a trace event\n * occurred.\n */\nstatic DEFINE_PER_CPU(bool, trace_taskinfo_save);\n\n/*\n * Kill all tracing for good (never come back).\n * It is initialized to 1 but will turn to zero if the initialization\n * of the tracer is successful. But that is the only place that sets\n * this back to zero.\n */\nstatic int tracing_disabled = 1;\n\ncpumask_var_t __read_mostly\ttracing_buffer_mask;\n\n/*\n * ftrace_dump_on_oops - variable to dump ftrace buffer on oops\n *\n * If there is an oops (or kernel panic) and the ftrace_dump_on_oops\n * is set, then ftrace_dump is called. This will output the contents\n * of the ftrace buffers to the console.  This is very useful for\n * capturing traces that lead to crashes and outputing it to a\n * serial console.\n *\n * It is default off, but you can enable it with either specifying\n * \"ftrace_dump_on_oops\" in the kernel command line, or setting\n * /proc/sys/kernel/ftrace_dump_on_oops\n * Set 1 if you want to dump buffers of all CPUs\n * Set 2 if you want to dump the buffer of the CPU that triggered oops\n */\n\nenum ftrace_dump_mode ftrace_dump_on_oops;\n\n/* When set, tracing will stop when a WARN*() is hit */\nint __disable_trace_on_warning;\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\n/* Map of enums to their values, for \"eval_map\" file */\nstruct trace_eval_map_head {\n\tstruct module\t\t\t*mod;\n\tunsigned long\t\t\tlength;\n};\n\nunion trace_eval_map_item;\n\nstruct trace_eval_map_tail {\n\t/*\n\t * \"end\" is first and points to NULL as it must be different\n\t * than \"mod\" or \"eval_string\"\n\t */\n\tunion trace_eval_map_item\t*next;\n\tconst char\t\t\t*end;\t/* points to NULL */\n};\n\nstatic DEFINE_MUTEX(trace_eval_mutex);\n\n/*\n * The trace_eval_maps are saved in an array with two extra elements,\n * one at the beginning, and one at the end. The beginning item contains\n * the count of the saved maps (head.length), and the module they\n * belong to if not built in (head.mod). The ending item contains a\n * pointer to the next array of saved eval_map items.\n */\nunion trace_eval_map_item {\n\tstruct trace_eval_map\t\tmap;\n\tstruct trace_eval_map_head\thead;\n\tstruct trace_eval_map_tail\ttail;\n};\n\nstatic union trace_eval_map_item *trace_eval_maps;\n#endif /* CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic int tracing_set_tracer(struct trace_array *tr, const char *buf);\n\n#define MAX_TRACER_SIZE\t\t100\nstatic char bootup_tracer_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *default_bootup_tracer;\n\nstatic bool allocate_snapshot;\n\nstatic int __init set_cmdline_ftrace(char *str)\n{\n\tstrlcpy(bootup_tracer_buf, str, MAX_TRACER_SIZE);\n\tdefault_bootup_tracer = bootup_tracer_buf;\n\t/* We are using ftrace early, expand it */\n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"ftrace=\", set_cmdline_ftrace);\n\nstatic int __init set_ftrace_dump_on_oops(char *str)\n{\n\tif (*str++ != '=' || !*str) {\n\t\tftrace_dump_on_oops = DUMP_ALL;\n\t\treturn 1;\n\t}\n\n\tif (!strcmp(\"orig_cpu\", str)) {\n\t\tftrace_dump_on_oops = DUMP_ORIG;\n                return 1;\n        }\n\n        return 0;\n}\n__setup(\"ftrace_dump_on_oops\", set_ftrace_dump_on_oops);\n\nstatic int __init stop_trace_on_warning(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\t__disable_trace_on_warning = 1;\n\treturn 1;\n}\n__setup(\"traceoff_on_warning\", stop_trace_on_warning);\n\nstatic int __init boot_alloc_snapshot(char *str)\n{\n\tallocate_snapshot = true;\n\t/* We also need the main ring buffer expanded */\n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"alloc_snapshot\", boot_alloc_snapshot);\n\n\nstatic char trace_boot_options_buf[MAX_TRACER_SIZE] __initdata;\n\nstatic int __init set_trace_boot_options(char *str)\n{\n\tstrlcpy(trace_boot_options_buf, str, MAX_TRACER_SIZE);\n\treturn 0;\n}\n__setup(\"trace_options=\", set_trace_boot_options);\n\nstatic char trace_boot_clock_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *trace_boot_clock __initdata;\n\nstatic int __init set_trace_boot_clock(char *str)\n{\n\tstrlcpy(trace_boot_clock_buf, str, MAX_TRACER_SIZE);\n\ttrace_boot_clock = trace_boot_clock_buf;\n\treturn 0;\n}\n__setup(\"trace_clock=\", set_trace_boot_clock);\n\nstatic int __init set_tracepoint_printk(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\ttracepoint_printk = 1;\n\treturn 1;\n}\n__setup(\"tp_printk\", set_tracepoint_printk);\n\nunsigned long long ns2usecs(u64 nsec)\n{\n\tnsec += 500;\n\tdo_div(nsec, 1000);\n\treturn nsec;\n}\n\n/* trace_flags holds trace_options default values */\n#define TRACE_DEFAULT_FLAGS\t\t\t\t\t\t\\\n\t(FUNCTION_DEFAULT_FLAGS |\t\t\t\t\t\\\n\t TRACE_ITER_PRINT_PARENT | TRACE_ITER_PRINTK |\t\t\t\\\n\t TRACE_ITER_ANNOTATE | TRACE_ITER_CONTEXT_INFO |\t\t\\\n\t TRACE_ITER_RECORD_CMD | TRACE_ITER_OVERWRITE |\t\t\t\\\n\t TRACE_ITER_IRQ_INFO | TRACE_ITER_MARKERS)\n\n/* trace_options that are only supported by global_trace */\n#define TOP_LEVEL_TRACE_FLAGS (TRACE_ITER_PRINTK |\t\t\t\\\n\t       TRACE_ITER_PRINTK_MSGONLY | TRACE_ITER_RECORD_CMD)\n\n/* trace_flags that are default zero for instances */\n#define ZEROED_TRACE_FLAGS \\\n\t(TRACE_ITER_EVENT_FORK | TRACE_ITER_FUNC_FORK)\n\n/*\n * The global_trace is the descriptor that holds the top-level tracing\n * buffers for the live tracing.\n */\nstatic struct trace_array global_trace = {\n\t.trace_flags = TRACE_DEFAULT_FLAGS,\n};\n\nLIST_HEAD(ftrace_trace_arrays);\n\nint trace_array_get(struct trace_array *this_tr)\n{\n\tstruct trace_array *tr;\n\tint ret = -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr == this_tr) {\n\t\t\ttr->ref++;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic void __trace_array_put(struct trace_array *this_tr)\n{\n\tWARN_ON(!this_tr->ref);\n\tthis_tr->ref--;\n}\n\nvoid trace_array_put(struct trace_array *this_tr)\n{\n\tmutex_lock(&trace_types_lock);\n\t__trace_array_put(this_tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\nint call_filter_check_discard(struct trace_event_call *call, void *rec,\n\t\t\t      struct ring_buffer *buffer,\n\t\t\t      struct ring_buffer_event *event)\n{\n\tif (unlikely(call->flags & TRACE_EVENT_FL_FILTERED) &&\n\t    !filter_match_preds(call->filter, rec)) {\n\t\t__trace_event_discard_commit(buffer, event);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nvoid trace_free_pid_list(struct trace_pid_list *pid_list)\n{\n\tvfree(pid_list->pids);\n\tkfree(pid_list);\n}\n\n/**\n * trace_find_filtered_pid - check if a pid exists in a filtered_pid list\n * @filtered_pids: The list of pids to check\n * @search_pid: The PID to find in @filtered_pids\n *\n * Returns true if @search_pid is fonud in @filtered_pids, and false otherwis.\n */\nbool\ntrace_find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)\n{\n\t/*\n\t * If pid_max changed after filtered_pids was created, we\n\t * by default ignore all pids greater than the previous pid_max.\n\t */\n\tif (search_pid >= filtered_pids->pid_max)\n\t\treturn false;\n\n\treturn test_bit(search_pid, filtered_pids->pids);\n}\n\n/**\n * trace_ignore_this_task - should a task be ignored for tracing\n * @filtered_pids: The list of pids to check\n * @task: The task that should be ignored if not filtered\n *\n * Checks if @task should be traced or not from @filtered_pids.\n * Returns true if @task should *NOT* be traced.\n * Returns false if @task should be traced.\n */\nbool\ntrace_ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)\n{\n\t/*\n\t * Return false, because if filtered_pids does not exist,\n\t * all pids are good to trace.\n\t */\n\tif (!filtered_pids)\n\t\treturn false;\n\n\treturn !trace_find_filtered_pid(filtered_pids, task->pid);\n}\n\n/**\n * trace_pid_filter_add_remove_task - Add or remove a task from a pid_list\n * @pid_list: The list to modify\n * @self: The current task for fork or NULL for exit\n * @task: The task to add or remove\n *\n * If adding a task, if @self is defined, the task is only added if @self\n * is also included in @pid_list. This happens on fork and tasks should\n * only be added when the parent is listed. If @self is NULL, then the\n * @task pid will be removed from the list, which would happen on exit\n * of a task.\n */\nvoid trace_filter_add_remove_task(struct trace_pid_list *pid_list,\n\t\t\t\t  struct task_struct *self,\n\t\t\t\t  struct task_struct *task)\n{\n\tif (!pid_list)\n\t\treturn;\n\n\t/* For forks, we only add if the forking task is listed */\n\tif (self) {\n\t\tif (!trace_find_filtered_pid(pid_list, self->pid))\n\t\t\treturn;\n\t}\n\n\t/* Sorry, but we don't support pid_max changing after setting */\n\tif (task->pid >= pid_list->pid_max)\n\t\treturn;\n\n\t/* \"self\" is set for forks, and NULL for exits */\n\tif (self)\n\t\tset_bit(task->pid, pid_list->pids);\n\telse\n\t\tclear_bit(task->pid, pid_list->pids);\n}\n\n/**\n * trace_pid_next - Used for seq_file to get to the next pid of a pid_list\n * @pid_list: The pid list to show\n * @v: The last pid that was shown (+1 the actual pid to let zero be displayed)\n * @pos: The position of the file\n *\n * This is used by the seq_file \"next\" operation to iterate the pids\n * listed in a trace_pid_list structure.\n *\n * Returns the pid+1 as we want to display pid of zero, but NULL would\n * stop the iteration.\n */\nvoid *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos)\n{\n\tunsigned long pid = (unsigned long)v;\n\n\t(*pos)++;\n\n\t/* pid already is +1 of the actual prevous bit */\n\tpid = find_next_bit(pid_list->pids, pid_list->pid_max, pid);\n\n\t/* Return pid + 1 to allow zero to be represented */\n\tif (pid < pid_list->pid_max)\n\t\treturn (void *)(pid + 1);\n\n\treturn NULL;\n}\n\n/**\n * trace_pid_start - Used for seq_file to start reading pid lists\n * @pid_list: The pid list to show\n * @pos: The position of the file\n *\n * This is used by seq_file \"start\" operation to start the iteration\n * of listing pids.\n *\n * Returns the pid+1 as we want to display pid of zero, but NULL would\n * stop the iteration.\n */\nvoid *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos)\n{\n\tunsigned long pid;\n\tloff_t l = 0;\n\n\tpid = find_first_bit(pid_list->pids, pid_list->pid_max);\n\tif (pid >= pid_list->pid_max)\n\t\treturn NULL;\n\n\t/* Return pid + 1 so that zero can be the exit value */\n\tfor (pid++; pid && l < *pos;\n\t     pid = (unsigned long)trace_pid_next(pid_list, (void *)pid, &l))\n\t\t;\n\treturn (void *)pid;\n}\n\n/**\n * trace_pid_show - show the current pid in seq_file processing\n * @m: The seq_file structure to write into\n * @v: A void pointer of the pid (+1) value to display\n *\n * Can be directly used by seq_file operations to display the current\n * pid value.\n */\nint trace_pid_show(struct seq_file *m, void *v)\n{\n\tunsigned long pid = (unsigned long)v - 1;\n\n\tseq_printf(m, \"%lu\\n\", pid);\n\treturn 0;\n}\n\n/* 128 should be much more than enough */\n#define PID_BUF_SIZE\t\t127\n\nint trace_pid_write(struct trace_pid_list *filtered_pids,\n\t\t    struct trace_pid_list **new_pid_list,\n\t\t    const char __user *ubuf, size_t cnt)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_parser parser;\n\tunsigned long val;\n\tint nr_pids = 0;\n\tssize_t read = 0;\n\tssize_t ret = 0;\n\tloff_t pos;\n\tpid_t pid;\n\n\tif (trace_parser_get_init(&parser, PID_BUF_SIZE + 1))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Always recreate a new array. The write is an all or nothing\n\t * operation. Always create a new array when adding new pids by\n\t * the user. If the operation fails, then the current list is\n\t * not modified.\n\t */\n\tpid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);\n\tif (!pid_list)\n\t\treturn -ENOMEM;\n\n\tpid_list->pid_max = READ_ONCE(pid_max);\n\n\t/* Only truncating will shrink pid_max */\n\tif (filtered_pids && filtered_pids->pid_max > pid_list->pid_max)\n\t\tpid_list->pid_max = filtered_pids->pid_max;\n\n\tpid_list->pids = vzalloc((pid_list->pid_max + 7) >> 3);\n\tif (!pid_list->pids) {\n\t\tkfree(pid_list);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (filtered_pids) {\n\t\t/* copy the current bits to the new max */\n\t\tfor_each_set_bit(pid, filtered_pids->pids,\n\t\t\t\t filtered_pids->pid_max) {\n\t\t\tset_bit(pid, pid_list->pids);\n\t\t\tnr_pids++;\n\t\t}\n\t}\n\n\twhile (cnt > 0) {\n\n\t\tpos = 0;\n\n\t\tret = trace_get_user(&parser, ubuf, cnt, &pos);\n\t\tif (ret < 0 || !trace_parser_loaded(&parser))\n\t\t\tbreak;\n\n\t\tread += ret;\n\t\tubuf += ret;\n\t\tcnt -= ret;\n\n\t\tret = -EINVAL;\n\t\tif (kstrtoul(parser.buffer, 0, &val))\n\t\t\tbreak;\n\t\tif (val >= pid_list->pid_max)\n\t\t\tbreak;\n\n\t\tpid = (pid_t)val;\n\n\t\tset_bit(pid, pid_list->pids);\n\t\tnr_pids++;\n\n\t\ttrace_parser_clear(&parser);\n\t\tret = 0;\n\t}\n\ttrace_parser_put(&parser);\n\n\tif (ret < 0) {\n\t\ttrace_free_pid_list(pid_list);\n\t\treturn ret;\n\t}\n\n\tif (!nr_pids) {\n\t\t/* Cleared the list of pids */\n\t\ttrace_free_pid_list(pid_list);\n\t\tread = ret;\n\t\tpid_list = NULL;\n\t}\n\n\t*new_pid_list = pid_list;\n\n\treturn read;\n}\n\nstatic u64 buffer_ftrace_now(struct trace_buffer *buf, int cpu)\n{\n\tu64 ts;\n\n\t/* Early boot up does not have a buffer yet */\n\tif (!buf->buffer)\n\t\treturn trace_clock_local();\n\n\tts = ring_buffer_time_stamp(buf->buffer, cpu);\n\tring_buffer_normalize_time_stamp(buf->buffer, cpu, &ts);\n\n\treturn ts;\n}\n\nu64 ftrace_now(int cpu)\n{\n\treturn buffer_ftrace_now(&global_trace.trace_buffer, cpu);\n}\n\n/**\n * tracing_is_enabled - Show if global_trace has been disabled\n *\n * Shows if the global trace has been enabled or not. It uses the\n * mirror flag \"buffer_disabled\" to be used in fast paths such as for\n * the irqsoff tracer. But it may be inaccurate due to races. If you\n * need to know the accurate state, use tracing_is_on() which is a little\n * slower, but accurate.\n */\nint tracing_is_enabled(void)\n{\n\t/*\n\t * For quick access (irqsoff uses this in fast path), just\n\t * return the mirror variable of the state of the ring buffer.\n\t * It's a little racy, but we don't really care.\n\t */\n\tsmp_rmb();\n\treturn !global_trace.buffer_disabled;\n}\n\n/*\n * trace_buf_size is the size in bytes that is allocated\n * for a buffer. Note, the number of bytes is always rounded\n * to page size.\n *\n * This number is purposely set to a low number of 16384.\n * If the dump on oops happens, it will be much appreciated\n * to not have to wait for all that output. Anyway this can be\n * boot time and run time configurable.\n */\n#define TRACE_BUF_SIZE_DEFAULT\t1441792UL /* 16384 * 88 (sizeof(entry)) */\n\nstatic unsigned long\t\ttrace_buf_size = TRACE_BUF_SIZE_DEFAULT;\n\n/* trace_types holds a link list of available tracers. */\nstatic struct tracer\t\t*trace_types __read_mostly;\n\n/*\n * trace_types_lock is used to protect the trace_types list.\n */\nDEFINE_MUTEX(trace_types_lock);\n\n/*\n * serialize the access of the ring buffer\n *\n * ring buffer serializes readers, but it is low level protection.\n * The validity of the events (which returns by ring_buffer_peek() ..etc)\n * are not protected by ring buffer.\n *\n * The content of events may become garbage if we allow other process consumes\n * these events concurrently:\n *   A) the page of the consumed events may become a normal page\n *      (not reader page) in ring buffer, and this page will be rewrited\n *      by events producer.\n *   B) The page of the consumed events may become a page for splice_read,\n *      and this page will be returned to system.\n *\n * These primitives allow multi process access to different cpu ring buffer\n * concurrently.\n *\n * These primitives don't distinguish read-only and read-consume access.\n * Multi read-only access are also serialized.\n */\n\n#ifdef CONFIG_SMP\nstatic DECLARE_RWSEM(all_cpu_access_lock);\nstatic DEFINE_PER_CPU(struct mutex, cpu_access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\t/* gain it for accessing the whole ring buffer. */\n\t\tdown_write(&all_cpu_access_lock);\n\t} else {\n\t\t/* gain it for accessing a cpu ring buffer. */\n\n\t\t/* Firstly block other trace_access_lock(RING_BUFFER_ALL_CPUS). */\n\t\tdown_read(&all_cpu_access_lock);\n\n\t\t/* Secondly block other access to this @cpu ring buffer. */\n\t\tmutex_lock(&per_cpu(cpu_access_lock, cpu));\n\t}\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tup_write(&all_cpu_access_lock);\n\t} else {\n\t\tmutex_unlock(&per_cpu(cpu_access_lock, cpu));\n\t\tup_read(&all_cpu_access_lock);\n\t}\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmutex_init(&per_cpu(cpu_access_lock, cpu));\n}\n\n#else\n\nstatic DEFINE_MUTEX(access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\t(void)cpu;\n\tmutex_lock(&access_lock);\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\t(void)cpu;\n\tmutex_unlock(&access_lock);\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n}\n\n#endif\n\n#ifdef CONFIG_STACKTRACE\nstatic void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t unsigned long flags,\n\t\t\t\t int skip, int pc, struct pt_regs *regs);\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs);\n\n#else\nstatic inline void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tint skip, int pc, struct pt_regs *regs)\n{\n}\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs)\n{\n}\n\n#endif\n\nstatic __always_inline void\ntrace_event_setup(struct ring_buffer_event *event,\n\t\t  int type, unsigned long flags, int pc)\n{\n\tstruct trace_entry *ent = ring_buffer_event_data(event);\n\n\ttracing_generic_entry_update(ent, flags, pc);\n\tent->type = type;\n}\n\nstatic __always_inline struct ring_buffer_event *\n__trace_buffer_lock_reserve(struct ring_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\tstruct ring_buffer_event *event;\n\n\tevent = ring_buffer_lock_reserve(buffer, len);\n\tif (event != NULL)\n\t\ttrace_event_setup(event, type, flags, pc);\n\n\treturn event;\n}\n\nvoid tracer_tracing_on(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\tring_buffer_record_on(tr->trace_buffer.buffer);\n\t/*\n\t * This flag is looked at when buffers haven't been allocated\n\t * yet, or by some tracers (like irqsoff), that just want to\n\t * know if the ring buffer has been disabled, but it can handle\n\t * races of where it gets disabled but we still do a record.\n\t * As the check is in the fast path of the tracers, it is more\n\t * important to be fast than accurate.\n\t */\n\ttr->buffer_disabled = 0;\n\t/* Make the flag seen by readers */\n\tsmp_wmb();\n}\n\n/**\n * tracing_on - enable tracing buffers\n *\n * This function enables tracing buffers that may have been\n * disabled with tracing_off.\n */\nvoid tracing_on(void)\n{\n\ttracer_tracing_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_on);\n\n\nstatic __always_inline void\n__buffer_unlock_commit(struct ring_buffer *buffer, struct ring_buffer_event *event)\n{\n\t__this_cpu_write(trace_taskinfo_save, true);\n\n\t/* If this is the temp buffer, we need to commit fully */\n\tif (this_cpu_read(trace_buffered_event) == event) {\n\t\t/* Length is in event->array[0] */\n\t\tring_buffer_write(buffer, event->array[0], &event->array[1]);\n\t\t/* Release the temp buffer */\n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t} else\n\t\tring_buffer_unlock_commit(buffer, event);\n}\n\n/**\n * __trace_puts - write a constant string into the trace buffer.\n * @ip:\t   The address of the caller\n * @str:   The constant string to write\n * @size:  The size of the string.\n */\nint __trace_puts(unsigned long ip, const char *str, int size)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tint alloc;\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\talloc = sizeof(*entry) + size + 2; /* possible \\n added */\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc, \n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, str, size);\n\n\t/* Add a newline if necessary */\n\tif (entry->buf[size - 1] != '\\n') {\n\t\tentry->buf[size] = '\\n';\n\t\tentry->buf[size + 1] = '\\0';\n\t} else\n\t\tentry->buf[size] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn size;\n}\nEXPORT_SYMBOL_GPL(__trace_puts);\n\n/**\n * __trace_bputs - write the pointer to a constant string into trace buffer\n * @ip:\t   The address of the caller\n * @str:   The constant string to write to the buffer to\n */\nint __trace_bputs(unsigned long ip, const char *str)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct bputs_entry *entry;\n\tunsigned long irq_flags;\n\tint size = sizeof(struct bputs_entry);\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPUTS, size,\n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->str\t\t\t= str;\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(__trace_bputs);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nvoid tracing_snapshot_instance(struct trace_array *tr)\n{\n\tstruct tracer *tracer = tr->current_trace;\n\tunsigned long flags;\n\n\tif (in_nmi()) {\n\t\tinternal_trace_puts(\"*** SNAPSHOT CALLED FROM NMI CONTEXT ***\\n\");\n\t\tinternal_trace_puts(\"*** snapshot is being ignored        ***\\n\");\n\t\treturn;\n\t}\n\n\tif (!tr->allocated_snapshot) {\n\t\tinternal_trace_puts(\"*** SNAPSHOT NOT ALLOCATED ***\\n\");\n\t\tinternal_trace_puts(\"*** stopping trace here!   ***\\n\");\n\t\ttracing_off();\n\t\treturn;\n\t}\n\n\t/* Note, snapshot can not be used when the tracer uses it */\n\tif (tracer->use_max_tr) {\n\t\tinternal_trace_puts(\"*** LATENCY TRACER ACTIVE ***\\n\");\n\t\tinternal_trace_puts(\"*** Can not use snapshot (sorry) ***\\n\");\n\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\tupdate_max_tr(tr, current, smp_processor_id());\n\tlocal_irq_restore(flags);\n}\n\n/**\n * tracing_snapshot - take a snapshot of the current buffer.\n *\n * This causes a swap between the snapshot buffer and the current live\n * tracing buffer. You can use this to take snapshots of the live\n * trace when some condition is triggered, but continue to trace.\n *\n * Note, make sure to allocate the snapshot with either\n * a tracing_snapshot_alloc(), or by doing it manually\n * with: echo 1 > /sys/kernel/debug/tracing/snapshot\n *\n * If the snapshot buffer is not allocated, it will stop tracing.\n * Basically making a permanent snapshot.\n */\nvoid tracing_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\ttracing_snapshot_instance(tr);\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\n\nstatic int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,\n\t\t\t\t\tstruct trace_buffer *size_buf, int cpu_id);\nstatic void set_buffer_entries(struct trace_buffer *buf, unsigned long val);\n\nint tracing_alloc_snapshot_instance(struct trace_array *tr)\n{\n\tint ret;\n\n\tif (!tr->allocated_snapshot) {\n\n\t\t/* allocate spare buffer */\n\t\tret = resize_buffer_duplicate_size(&tr->max_buffer,\n\t\t\t\t   &tr->trace_buffer, RING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\ttr->allocated_snapshot = true;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_snapshot(struct trace_array *tr)\n{\n\t/*\n\t * We don't free the ring buffer. instead, resize it because\n\t * The max_tr ring buffer has some state (e.g. ring->clock) and\n\t * we want preserve it.\n\t */\n\tring_buffer_resize(tr->max_buffer.buffer, 1, RING_BUFFER_ALL_CPUS);\n\tset_buffer_entries(&tr->max_buffer, 1);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n\ttr->allocated_snapshot = false;\n}\n\n/**\n * tracing_alloc_snapshot - allocate snapshot buffer.\n *\n * This only allocates the snapshot buffer if it isn't already\n * allocated - it doesn't also take a snapshot.\n *\n * This is meant to be used in cases where the snapshot buffer needs\n * to be set up for events that can't sleep but need to be able to\n * trigger a snapshot.\n */\nint tracing_alloc_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\tint ret;\n\n\tret = tracing_alloc_snapshot_instance(tr);\n\tWARN_ON(ret < 0);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\n\n/**\n * tracing_snapshot_alloc - allocate and take a snapshot of the current buffer.\n *\n * This is similar to tracing_snapshot(), but it will allocate the\n * snapshot buffer if it isn't already allocated. Use this only\n * where it is safe to sleep, as the allocation may sleep.\n *\n * This causes a swap between the snapshot buffer and the current live\n * tracing buffer. You can use this to take snapshots of the live\n * trace when some condition is triggered, but continue to trace.\n */\nvoid tracing_snapshot_alloc(void)\n{\n\tint ret;\n\n\tret = tracing_alloc_snapshot();\n\tif (ret < 0)\n\t\treturn;\n\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n#else\nvoid tracing_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but internal snapshot used\");\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\nint tracing_alloc_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but snapshot allocation used\");\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\nvoid tracing_snapshot_alloc(void)\n{\n\t/* Give warning */\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\nvoid tracer_tracing_off(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\tring_buffer_record_off(tr->trace_buffer.buffer);\n\t/*\n\t * This flag is looked at when buffers haven't been allocated\n\t * yet, or by some tracers (like irqsoff), that just want to\n\t * know if the ring buffer has been disabled, but it can handle\n\t * races of where it gets disabled but we still do a record.\n\t * As the check is in the fast path of the tracers, it is more\n\t * important to be fast than accurate.\n\t */\n\ttr->buffer_disabled = 1;\n\t/* Make the flag seen by readers */\n\tsmp_wmb();\n}\n\n/**\n * tracing_off - turn off tracing buffers\n *\n * This function stops the tracing buffers from recording data.\n * It does not disable any overhead the tracers themselves may\n * be causing. This function simply causes all recording to\n * the ring buffers to fail.\n */\nvoid tracing_off(void)\n{\n\ttracer_tracing_off(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_off);\n\nvoid disable_trace_on_warning(void)\n{\n\tif (__disable_trace_on_warning)\n\t\ttracing_off();\n}\n\n/**\n * tracer_tracing_is_on - show real state of ring buffer enabled\n * @tr : the trace array to know if ring buffer is enabled\n *\n * Shows real state of the ring buffer if it is enabled or not.\n */\nint tracer_tracing_is_on(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\treturn ring_buffer_record_is_on(tr->trace_buffer.buffer);\n\treturn !tr->buffer_disabled;\n}\n\n/**\n * tracing_is_on - show state of ring buffers enabled\n */\nint tracing_is_on(void)\n{\n\treturn tracer_tracing_is_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_is_on);\n\nstatic int __init set_buf_size(char *str)\n{\n\tunsigned long buf_size;\n\n\tif (!str)\n\t\treturn 0;\n\tbuf_size = memparse(str, &str);\n\t/* nr_entries can not be zero */\n\tif (buf_size == 0)\n\t\treturn 0;\n\ttrace_buf_size = buf_size;\n\treturn 1;\n}\n__setup(\"trace_buf_size=\", set_buf_size);\n\nstatic int __init set_tracing_thresh(char *str)\n{\n\tunsigned long threshold;\n\tint ret;\n\n\tif (!str)\n\t\treturn 0;\n\tret = kstrtoul(str, 0, &threshold);\n\tif (ret < 0)\n\t\treturn 0;\n\ttracing_thresh = threshold * 1000;\n\treturn 1;\n}\n__setup(\"tracing_thresh=\", set_tracing_thresh);\n\nunsigned long nsecs_to_usecs(unsigned long nsecs)\n{\n\treturn nsecs / 1000;\n}\n\n/*\n * TRACE_FLAGS is defined as a tuple matching bit masks with strings.\n * It uses C(a, b) where 'a' is the eval (enum) name and 'b' is the string that\n * matches it. By defining \"C(a, b) b\", TRACE_FLAGS becomes a list\n * of strings in the order that the evals (enum) were defined.\n */\n#undef C\n#define C(a, b) b\n\n/* These must match the bit postions in trace_iterator_flags */\nstatic const char *trace_options[] = {\n\tTRACE_FLAGS\n\tNULL\n};\n\nstatic struct {\n\tu64 (*func)(void);\n\tconst char *name;\n\tint in_ns;\t\t/* is this clock in nanoseconds? */\n} trace_clocks[] = {\n\t{ trace_clock_local,\t\t\"local\",\t1 },\n\t{ trace_clock_global,\t\t\"global\",\t1 },\n\t{ trace_clock_counter,\t\t\"counter\",\t0 },\n\t{ trace_clock_jiffies,\t\t\"uptime\",\t0 },\n\t{ trace_clock,\t\t\t\"perf\",\t\t1 },\n\t{ ktime_get_mono_fast_ns,\t\"mono\",\t\t1 },\n\t{ ktime_get_raw_fast_ns,\t\"mono_raw\",\t1 },\n\t{ ktime_get_boot_fast_ns,\t\"boot\",\t\t1 },\n\tARCH_TRACE_CLOCKS\n};\n\nbool trace_clock_in_ns(struct trace_array *tr)\n{\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * trace_parser_get_init - gets the buffer for trace parser\n */\nint trace_parser_get_init(struct trace_parser *parser, int size)\n{\n\tmemset(parser, 0, sizeof(*parser));\n\n\tparser->buffer = kmalloc(size, GFP_KERNEL);\n\tif (!parser->buffer)\n\t\treturn 1;\n\n\tparser->size = size;\n\treturn 0;\n}\n\n/*\n * trace_parser_put - frees the buffer for trace parser\n */\nvoid trace_parser_put(struct trace_parser *parser)\n{\n\tkfree(parser->buffer);\n\tparser->buffer = NULL;\n}\n\n/*\n * trace_get_user - reads the user input string separated by  space\n * (matched by isspace(ch))\n *\n * For each string found the 'struct trace_parser' is updated,\n * and the function returns.\n *\n * Returns number of bytes read.\n *\n * See kernel/trace/trace.h for 'struct trace_parser' details.\n */\nint trace_get_user(struct trace_parser *parser, const char __user *ubuf,\n\tsize_t cnt, loff_t *ppos)\n{\n\tchar ch;\n\tsize_t read = 0;\n\tssize_t ret;\n\n\tif (!*ppos)\n\t\ttrace_parser_clear(parser);\n\n\tret = get_user(ch, ubuf++);\n\tif (ret)\n\t\tgoto out;\n\n\tread++;\n\tcnt--;\n\n\t/*\n\t * The parser is not finished with the last write,\n\t * continue reading the user input without skipping spaces.\n\t */\n\tif (!parser->cont) {\n\t\t/* skip white space */\n\t\twhile (cnt && isspace(ch)) {\n\t\t\tret = get_user(ch, ubuf++);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tread++;\n\t\t\tcnt--;\n\t\t}\n\n\t\tparser->idx = 0;\n\n\t\t/* only spaces were written */\n\t\tif (isspace(ch) || !ch) {\n\t\t\t*ppos += read;\n\t\t\tret = read;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* read the non-space input */\n\twhile (cnt && !isspace(ch) && ch) {\n\t\tif (parser->idx < parser->size - 1)\n\t\t\tparser->buffer[parser->idx++] = ch;\n\t\telse {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = get_user(ch, ubuf++);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tread++;\n\t\tcnt--;\n\t}\n\n\t/* We either got finished input or we have to wait for another call. */\n\tif (isspace(ch) || !ch) {\n\t\tparser->buffer[parser->idx] = 0;\n\t\tparser->cont = false;\n\t} else if (parser->idx < parser->size - 1) {\n\t\tparser->cont = true;\n\t\tparser->buffer[parser->idx++] = ch;\n\t\t/* Make sure the parsed string always terminates with '\\0'. */\n\t\tparser->buffer[parser->idx] = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*ppos += read;\n\tret = read;\n\nout:\n\treturn ret;\n}\n\n/* TODO add a seq_buf_to_buffer() */\nstatic ssize_t trace_seq_to_buffer(struct trace_seq *s, void *buf, size_t cnt)\n{\n\tint len;\n\n\tif (trace_seq_used(s) <= s->seq.readpos)\n\t\treturn -EBUSY;\n\n\tlen = trace_seq_used(s) - s->seq.readpos;\n\tif (cnt > len)\n\t\tcnt = len;\n\tmemcpy(buf, s->buffer + s->seq.readpos, cnt);\n\n\ts->seq.readpos += cnt;\n\treturn cnt;\n}\n\nunsigned long __read_mostly\ttracing_thresh;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n/*\n * Copy the new maximum trace into the separate maximum-trace\n * structure. (this way the maximum trace is permanently saved,\n * for later retrieval via /sys/kernel/tracing/tracing_max_latency)\n */\nstatic void\n__update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\tstruct trace_buffer *max_buf = &tr->max_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(trace_buf->data, cpu);\n\tstruct trace_array_cpu *max_data = per_cpu_ptr(max_buf->data, cpu);\n\n\tmax_buf->cpu = cpu;\n\tmax_buf->time_start = data->preempt_timestamp;\n\n\tmax_data->saved_latency = tr->max_latency;\n\tmax_data->critical_start = data->critical_start;\n\tmax_data->critical_end = data->critical_end;\n\n\tmemcpy(max_data->comm, tsk->comm, TASK_COMM_LEN);\n\tmax_data->pid = tsk->pid;\n\t/*\n\t * If tsk == current, then use current_uid(), as that does not use\n\t * RCU. The irq tracer can be called out of RCU scope.\n\t */\n\tif (tsk == current)\n\t\tmax_data->uid = current_uid();\n\telse\n\t\tmax_data->uid = task_uid(tsk);\n\n\tmax_data->nice = tsk->static_prio - 20 - MAX_RT_PRIO;\n\tmax_data->policy = tsk->policy;\n\tmax_data->rt_priority = tsk->rt_priority;\n\n\t/* record this tasks comm */\n\ttracing_record_cmdline(tsk);\n}\n\n/**\n * update_max_tr - snapshot all trace buffers from global_trace to max_tr\n * @tr: tracer\n * @tsk: the task with the latency\n * @cpu: The cpu that initiated the trace.\n *\n * Flip the buffers between the @tr and the max_tr and record information\n * about which task was the cause of this latency.\n */\nvoid\nupdate_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tstruct ring_buffer *buf;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tbuf = tr->trace_buffer.buffer;\n\ttr->trace_buffer.buffer = tr->max_buffer.buffer;\n\ttr->max_buffer.buffer = buf;\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n\n/**\n * update_max_tr_single - only copy one trace over, and reset the rest\n * @tr - tracer\n * @tsk - task with the latency\n * @cpu - the cpu of the buffer to copy.\n *\n * Flip the trace of a single CPU buffer between the @tr and the max_tr.\n */\nvoid\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\nstatic int wait_on_pipe(struct trace_iterator *iter, bool full)\n{\n\t/* Iterators are static, they should be filled or empty */\n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn 0;\n\n\treturn ring_buffer_wait(iter->trace_buffer->buffer, iter->cpu_file,\n\t\t\t\tfull);\n}\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\nstatic bool selftests_can_run;\n\nstruct trace_selftests {\n\tstruct list_head\t\tlist;\n\tstruct tracer\t\t\t*type;\n};\n\nstatic LIST_HEAD(postponed_selftests);\n\nstatic int save_selftest(struct tracer *type)\n{\n\tstruct trace_selftests *selftest;\n\n\tselftest = kmalloc(sizeof(*selftest), GFP_KERNEL);\n\tif (!selftest)\n\t\treturn -ENOMEM;\n\n\tselftest->type = type;\n\tlist_add(&selftest->list, &postponed_selftests);\n\treturn 0;\n}\n\nstatic int run_tracer_selftest(struct tracer *type)\n{\n\tstruct trace_array *tr = &global_trace;\n\tstruct tracer *saved_tracer = tr->current_trace;\n\tint ret;\n\n\tif (!type->selftest || tracing_selftest_disabled)\n\t\treturn 0;\n\n\t/*\n\t * If a tracer registers early in boot up (before scheduling is\n\t * initialized and such), then do not run its selftests yet.\n\t * Instead, run it a little later in the boot process.\n\t */\n\tif (!selftests_can_run)\n\t\treturn save_selftest(type);\n\n\t/*\n\t * Run a selftest on this tracer.\n\t * Here we reset the trace buffer, and set the current\n\t * tracer to be this tracer. The tracer can then run some\n\t * internal tracing to verify that everything is in order.\n\t * If we fail, we do not register this tracer.\n\t */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n\ttr->current_trace = type;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\t/* If we expanded the buffers, make sure the max is expanded too */\n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, trace_buf_size,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t\ttr->allocated_snapshot = true;\n\t}\n#endif\n\n\t/* the test is responsible for initializing and enabling */\n\tpr_info(\"Testing tracer %s: \", type->name);\n\tret = type->selftest(type, tr);\n\t/* the test is responsible for resetting too */\n\ttr->current_trace = saved_tracer;\n\tif (ret) {\n\t\tprintk(KERN_CONT \"FAILED!\\n\");\n\t\t/* Add the warning after printing 'FAILED' */\n\t\tWARN_ON(1);\n\t\treturn -1;\n\t}\n\t/* Only reset on passing, to avoid touching corrupted buffers */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\ttr->allocated_snapshot = false;\n\n\t\t/* Shrink the max buffer again */\n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, 1,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t}\n#endif\n\n\tprintk(KERN_CONT \"PASSED\\n\");\n\treturn 0;\n}\n\nstatic __init int init_trace_selftests(void)\n{\n\tstruct trace_selftests *p, *n;\n\tstruct tracer *t, **last;\n\tint ret;\n\n\tselftests_can_run = true;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (list_empty(&postponed_selftests))\n\t\tgoto out;\n\n\tpr_info(\"Running postponed tracer tests:\\n\");\n\n\tlist_for_each_entry_safe(p, n, &postponed_selftests, list) {\n\t\tret = run_tracer_selftest(p->type);\n\t\t/* If the test fails, then warn and remove from available_tracers */\n\t\tif (ret < 0) {\n\t\t\tWARN(1, \"tracer: %s failed selftest, disabling\\n\",\n\t\t\t     p->type->name);\n\t\t\tlast = &trace_types;\n\t\t\tfor (t = trace_types; t; t = t->next) {\n\t\t\t\tif (t == p->type) {\n\t\t\t\t\t*last = t->next;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlast = &t->next;\n\t\t\t}\n\t\t}\n\t\tlist_del(&p->list);\n\t\tkfree(p);\n\t}\n\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\ncore_initcall(init_trace_selftests);\n#else\nstatic inline int run_tracer_selftest(struct tracer *type)\n{\n\treturn 0;\n}\n#endif /* CONFIG_FTRACE_STARTUP_TEST */\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t);\n\nstatic void __init apply_trace_boot_options(void);\n\n/**\n * register_tracer - register a tracer with the ftrace system.\n * @type - the plugin for the tracer\n *\n * Register a new plugin tracer.\n */\nint __init register_tracer(struct tracer *type)\n{\n\tstruct tracer *t;\n\tint ret = 0;\n\n\tif (!type->name) {\n\t\tpr_info(\"Tracer must have a name\\n\");\n\t\treturn -1;\n\t}\n\n\tif (strlen(type->name) >= MAX_TRACER_SIZE) {\n\t\tpr_info(\"Tracer has a name longer than %d\\n\", MAX_TRACER_SIZE);\n\t\treturn -1;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\ttracing_selftest_running = true;\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(type->name, t->name) == 0) {\n\t\t\t/* already found */\n\t\t\tpr_info(\"Tracer %s already registered\\n\",\n\t\t\t\ttype->name);\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!type->set_flag)\n\t\ttype->set_flag = &dummy_set_flag;\n\tif (!type->flags) {\n\t\t/*allocate a dummy tracer_flags*/\n\t\ttype->flags = kmalloc(sizeof(*type->flags), GFP_KERNEL);\n\t\tif (!type->flags) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttype->flags->val = 0;\n\t\ttype->flags->opts = dummy_tracer_opt;\n\t} else\n\t\tif (!type->flags->opts)\n\t\t\ttype->flags->opts = dummy_tracer_opt;\n\n\t/* store the tracer for __set_tracer_option */\n\ttype->flags->trace = type;\n\n\tret = run_tracer_selftest(type);\n\tif (ret < 0)\n\t\tgoto out;\n\n\ttype->next = trace_types;\n\ttrace_types = type;\n\tadd_tracer_options(&global_trace, type);\n\n out:\n\ttracing_selftest_running = false;\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret || !default_bootup_tracer)\n\t\tgoto out_unlock;\n\n\tif (strncmp(default_bootup_tracer, type->name, MAX_TRACER_SIZE))\n\t\tgoto out_unlock;\n\n\tprintk(KERN_INFO \"Starting tracer '%s'\\n\", type->name);\n\t/* Do we want this tracer to start on bootup? */\n\ttracing_set_tracer(&global_trace, type->name);\n\tdefault_bootup_tracer = NULL;\n\n\tapply_trace_boot_options();\n\n\t/* disable other selftests, since this will break it. */\n\ttracing_selftest_disabled = true;\n#ifdef CONFIG_FTRACE_STARTUP_TEST\n\tprintk(KERN_INFO \"Disabling FTRACE selftests due to running tracer '%s'\\n\",\n\t       type->name);\n#endif\n\n out_unlock:\n\treturn ret;\n}\n\nvoid tracing_reset(struct trace_buffer *buf, int cpu)\n{\n\tstruct ring_buffer *buffer = buf->buffer;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_sched();\n\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\nvoid tracing_reset_online_cpus(struct trace_buffer *buf)\n{\n\tstruct ring_buffer *buffer = buf->buffer;\n\tint cpu;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_sched();\n\n\tbuf->time_start = buffer_ftrace_now(buf, buf->cpu);\n\n\tfor_each_online_cpu(cpu)\n\t\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\n/* Must have trace_types_lock held */\nvoid tracing_reset_all_online_cpus(void)\n{\n\tstruct trace_array *tr;\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->clear_trace)\n\t\t\tcontinue;\n\t\ttr->clear_trace = false;\n\t\ttracing_reset_online_cpus(&tr->trace_buffer);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\t}\n}\n\nstatic int *tgid_map;\n\n#define SAVED_CMDLINES_DEFAULT 128\n#define NO_CMDLINE_MAP UINT_MAX\nstatic arch_spinlock_t trace_cmdline_lock = __ARCH_SPIN_LOCK_UNLOCKED;\nstruct saved_cmdlines_buffer {\n\tunsigned map_pid_to_cmdline[PID_MAX_DEFAULT+1];\n\tunsigned *map_cmdline_to_pid;\n\tunsigned cmdline_num;\n\tint cmdline_idx;\n\tchar *saved_cmdlines;\n};\nstatic struct saved_cmdlines_buffer *savedcmd;\n\n/* temporary disable recording */\nstatic atomic_t trace_record_taskinfo_disabled __read_mostly;\n\nstatic inline char *get_saved_cmdlines(int idx)\n{\n\treturn &savedcmd->saved_cmdlines[idx * TASK_COMM_LEN];\n}\n\nstatic inline void set_cmdline(int idx, const char *cmdline)\n{\n\tmemcpy(get_saved_cmdlines(idx), cmdline, TASK_COMM_LEN);\n}\n\nstatic int allocate_cmdlines_buffer(unsigned int val,\n\t\t\t\t    struct saved_cmdlines_buffer *s)\n{\n\ts->map_cmdline_to_pid = kmalloc_array(val,\n\t\t\t\t\t      sizeof(*s->map_cmdline_to_pid),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!s->map_cmdline_to_pid)\n\t\treturn -ENOMEM;\n\n\ts->saved_cmdlines = kmalloc_array(TASK_COMM_LEN, val, GFP_KERNEL);\n\tif (!s->saved_cmdlines) {\n\t\tkfree(s->map_cmdline_to_pid);\n\t\treturn -ENOMEM;\n\t}\n\n\ts->cmdline_idx = 0;\n\ts->cmdline_num = val;\n\tmemset(&s->map_pid_to_cmdline, NO_CMDLINE_MAP,\n\t       sizeof(s->map_pid_to_cmdline));\n\tmemset(s->map_cmdline_to_pid, NO_CMDLINE_MAP,\n\t       val * sizeof(*s->map_cmdline_to_pid));\n\n\treturn 0;\n}\n\nstatic int trace_create_savedcmd(void)\n{\n\tint ret;\n\n\tsavedcmd = kmalloc(sizeof(*savedcmd), GFP_KERNEL);\n\tif (!savedcmd)\n\t\treturn -ENOMEM;\n\n\tret = allocate_cmdlines_buffer(SAVED_CMDLINES_DEFAULT, savedcmd);\n\tif (ret < 0) {\n\t\tkfree(savedcmd);\n\t\tsavedcmd = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint is_tracing_stopped(void)\n{\n\treturn global_trace.stop_count;\n}\n\n/**\n * tracing_start - quick start of the tracer\n *\n * If tracing is enabled but was stopped by tracing_stop,\n * this will start the tracer back up.\n */\nvoid tracing_start(void)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&global_trace.start_lock, flags);\n\tif (--global_trace.stop_count) {\n\t\tif (global_trace.stop_count < 0) {\n\t\t\t/* Someone screwed up their debugging */\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tglobal_trace.stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Prevent the buffers from switching */\n\tarch_spin_lock(&global_trace.max_lock);\n\n\tbuffer = global_trace.trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = global_trace.max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n#endif\n\n\tarch_spin_unlock(&global_trace.max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&global_trace.start_lock, flags);\n}\n\nstatic void tracing_start_tr(struct trace_array *tr)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\t/* If global, we need to also start the max tracer */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn tracing_start();\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\n\tif (--tr->stop_count) {\n\t\tif (tr->stop_count < 0) {\n\t\t\t/* Someone screwed up their debugging */\n\t\t\tWARN_ON_ONCE(1);\n\t\t\ttr->stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tbuffer = tr->trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\n/**\n * tracing_stop - quick stop of the tracer\n *\n * Light weight way to stop tracing. Use in conjunction with\n * tracing_start.\n */\nvoid tracing_stop(void)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&global_trace.start_lock, flags);\n\tif (global_trace.stop_count++)\n\t\tgoto out;\n\n\t/* Prevent the buffers from switching */\n\tarch_spin_lock(&global_trace.max_lock);\n\n\tbuffer = global_trace.trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = global_trace.max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n#endif\n\n\tarch_spin_unlock(&global_trace.max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&global_trace.start_lock, flags);\n}\n\nstatic void tracing_stop_tr(struct trace_array *tr)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\t/* If global, we need to also stop the max tracer */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn tracing_stop();\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\tif (tr->stop_count++)\n\t\tgoto out;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\nstatic int trace_save_cmdline(struct task_struct *tsk)\n{\n\tunsigned pid, idx;\n\n\t/* treat recording of idle task as a success */\n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tif (unlikely(tsk->pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\t/*\n\t * It's not the end of the world if we don't get\n\t * the lock, but we also don't want to spin\n\t * nor do we want to disable interrupts,\n\t * so if we miss here, then better luck next time.\n\t */\n\tif (!arch_spin_trylock(&trace_cmdline_lock))\n\t\treturn 0;\n\n\tidx = savedcmd->map_pid_to_cmdline[tsk->pid];\n\tif (idx == NO_CMDLINE_MAP) {\n\t\tidx = (savedcmd->cmdline_idx + 1) % savedcmd->cmdline_num;\n\n\t\t/*\n\t\t * Check whether the cmdline buffer at idx has a pid\n\t\t * mapped. We are going to overwrite that entry so we\n\t\t * need to clear the map_pid_to_cmdline. Otherwise we\n\t\t * would read the new comm for the old pid.\n\t\t */\n\t\tpid = savedcmd->map_cmdline_to_pid[idx];\n\t\tif (pid != NO_CMDLINE_MAP)\n\t\t\tsavedcmd->map_pid_to_cmdline[pid] = NO_CMDLINE_MAP;\n\n\t\tsavedcmd->map_cmdline_to_pid[idx] = tsk->pid;\n\t\tsavedcmd->map_pid_to_cmdline[tsk->pid] = idx;\n\n\t\tsavedcmd->cmdline_idx = idx;\n\t}\n\n\tset_cmdline(idx, tsk->comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn 1;\n}\n\nstatic void __trace_find_cmdline(int pid, char comm[])\n{\n\tunsigned map;\n\n\tif (!pid) {\n\t\tstrcpy(comm, \"<idle>\");\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(pid < 0)) {\n\t\tstrcpy(comm, \"<XXX>\");\n\t\treturn;\n\t}\n\n\tif (pid > PID_MAX_DEFAULT) {\n\t\tstrcpy(comm, \"<...>\");\n\t\treturn;\n\t}\n\n\tmap = savedcmd->map_pid_to_cmdline[pid];\n\tif (map != NO_CMDLINE_MAP)\n\t\tstrlcpy(comm, get_saved_cmdlines(map), TASK_COMM_LEN);\n\telse\n\t\tstrcpy(comm, \"<...>\");\n}\n\nvoid trace_find_cmdline(int pid, char comm[])\n{\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\t__trace_find_cmdline(pid, comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nint trace_find_tgid(int pid)\n{\n\tif (unlikely(!tgid_map || !pid || pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\treturn tgid_map[pid];\n}\n\nstatic int trace_save_tgid(struct task_struct *tsk)\n{\n\t/* treat recording of idle task as a success */\n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tif (unlikely(!tgid_map || tsk->pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\ttgid_map[tsk->pid] = tsk->tgid;\n\treturn 1;\n}\n\nstatic bool tracing_record_taskinfo_skip(int flags)\n{\n\tif (unlikely(!(flags & (TRACE_RECORD_CMDLINE | TRACE_RECORD_TGID))))\n\t\treturn true;\n\tif (atomic_read(&trace_record_taskinfo_disabled) || !tracing_is_on())\n\t\treturn true;\n\tif (!__this_cpu_read(trace_taskinfo_save))\n\t\treturn true;\n\treturn false;\n}\n\n/**\n * tracing_record_taskinfo - record the task info of a task\n *\n * @task  - task to record\n * @flags - TRACE_RECORD_CMDLINE for recording comm\n *        - TRACE_RECORD_TGID for recording tgid\n */\nvoid tracing_record_taskinfo(struct task_struct *task, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t/*\n\t * Record as much task information as possible. If some fail, continue\n\t * to try to record the others.\n\t */\n\tdone = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(task);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(task);\n\n\t/* If recording any information failed, retry again soon. */\n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n/**\n * tracing_record_taskinfo_sched_switch - record task info for sched_switch\n *\n * @prev - previous task during sched_switch\n * @next - next task during sched_switch\n * @flags - TRACE_RECORD_CMDLINE for recording comm\n *          TRACE_RECORD_TGID for recording tgid\n */\nvoid tracing_record_taskinfo_sched_switch(struct task_struct *prev,\n\t\t\t\t\t  struct task_struct *next, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t/*\n\t * Record as much task information as possible. If some fail, continue\n\t * to try to record the others.\n\t */\n\tdone  = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(prev);\n\tdone &= !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(next);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(prev);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(next);\n\n\t/* If recording any information failed, retry again soon. */\n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n/* Helpers to record a specific task information */\nvoid tracing_record_cmdline(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_CMDLINE);\n}\n\nvoid tracing_record_tgid(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_TGID);\n}\n\n/*\n * Several functions return TRACE_TYPE_PARTIAL_LINE if the trace_seq\n * overflowed, and TRACE_TYPE_HANDLED otherwise. This helper function\n * simplifies those functions and keeps them in sync.\n */\nenum print_line_t trace_handle_return(struct trace_seq *s)\n{\n\treturn trace_seq_has_overflowed(s) ?\n\t\tTRACE_TYPE_PARTIAL_LINE : TRACE_TYPE_HANDLED;\n}\nEXPORT_SYMBOL_GPL(trace_handle_return);\n\nvoid\ntracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,\n\t\t\t     int pc)\n{\n\tstruct task_struct *tsk = current;\n\n\tentry->preempt_count\t\t= pc & 0xff;\n\tentry->pid\t\t\t= (tsk) ? tsk->pid : 0;\n\tentry->flags =\n#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT\n\t\t(irqs_disabled_flags(flags) ? TRACE_FLAG_IRQS_OFF : 0) |\n#else\n\t\tTRACE_FLAG_IRQS_NOSUPPORT |\n#endif\n\t\t((pc & NMI_MASK    ) ? TRACE_FLAG_NMI     : 0) |\n\t\t((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |\n\t\t((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |\n\t\t(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |\n\t\t(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);\n}\nEXPORT_SYMBOL_GPL(tracing_generic_entry_update);\n\nstruct ring_buffer_event *\ntrace_buffer_lock_reserve(struct ring_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\treturn __trace_buffer_lock_reserve(buffer, type, len, flags, pc);\n}\n\nDEFINE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);\nDEFINE_PER_CPU(int, trace_buffered_event_cnt);\nstatic int trace_buffered_event_ref;\n\n/**\n * trace_buffered_event_enable - enable buffering events\n *\n * When events are being filtered, it is quicker to use a temporary\n * buffer to write the event data into if there's a likely chance\n * that it will not be committed. The discard of the ring buffer\n * is not as fast as committing, and is much slower than copying\n * a commit.\n *\n * When an event is to be filtered, allocate per cpu buffers to\n * write the event data into, and if the event is filtered and discarded\n * it is simply dropped, otherwise, the entire data is to be committed\n * in one shot.\n */\nvoid trace_buffered_event_enable(void)\n{\n\tstruct ring_buffer_event *event;\n\tstruct page *page;\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (trace_buffered_event_ref++)\n\t\treturn;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\tif (!page)\n\t\t\tgoto failed;\n\n\t\tevent = page_address(page);\n\t\tmemset(event, 0, sizeof(*event));\n\n\t\tper_cpu(trace_buffered_event, cpu) = event;\n\n\t\tpreempt_disable();\n\t\tif (cpu == smp_processor_id() &&\n\t\t    this_cpu_read(trace_buffered_event) !=\n\t\t    per_cpu(trace_buffered_event, cpu))\n\t\t\tWARN_ON_ONCE(1);\n\t\tpreempt_enable();\n\t}\n\n\treturn;\n failed:\n\ttrace_buffered_event_disable();\n}\n\nstatic void enable_trace_buffered_event(void *data)\n{\n\t/* Probably not needed, but do it anyway */\n\tsmp_rmb();\n\tthis_cpu_dec(trace_buffered_event_cnt);\n}\n\nstatic void disable_trace_buffered_event(void *data)\n{\n\tthis_cpu_inc(trace_buffered_event_cnt);\n}\n\n/**\n * trace_buffered_event_disable - disable buffering events\n *\n * When a filter is removed, it is faster to not use the buffered\n * events, and to commit directly into the ring buffer. Free up\n * the temp buffers when there are no more users. This requires\n * special synchronization with current events.\n */\nvoid trace_buffered_event_disable(void)\n{\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (WARN_ON_ONCE(!trace_buffered_event_ref))\n\t\treturn;\n\n\tif (--trace_buffered_event_ref)\n\t\treturn;\n\n\tpreempt_disable();\n\t/* For each CPU, set the buffer as used. */\n\tsmp_call_function_many(tracing_buffer_mask,\n\t\t\t       disable_trace_buffered_event, NULL, 1);\n\tpreempt_enable();\n\n\t/* Wait for all current users to finish */\n\tsynchronize_sched();\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tfree_page((unsigned long)per_cpu(trace_buffered_event, cpu));\n\t\tper_cpu(trace_buffered_event, cpu) = NULL;\n\t}\n\t/*\n\t * Make sure trace_buffered_event is NULL before clearing\n\t * trace_buffered_event_cnt.\n\t */\n\tsmp_wmb();\n\n\tpreempt_disable();\n\t/* Do the work on each cpu */\n\tsmp_call_function_many(tracing_buffer_mask,\n\t\t\t       enable_trace_buffered_event, NULL, 1);\n\tpreempt_enable();\n}\n\nstatic struct ring_buffer *temp_buffer;\n\nstruct ring_buffer_event *\ntrace_event_buffer_lock_reserve(struct ring_buffer **current_rb,\n\t\t\t  struct trace_event_file *trace_file,\n\t\t\t  int type, unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\tstruct ring_buffer_event *entry;\n\tint val;\n\n\t*current_rb = trace_file->tr->trace_buffer.buffer;\n\n\tif (!ring_buffer_time_stamp_abs(*current_rb) && (trace_file->flags &\n\t     (EVENT_FILE_FL_SOFT_DISABLED | EVENT_FILE_FL_FILTERED)) &&\n\t    (entry = this_cpu_read(trace_buffered_event))) {\n\t\t/* Try to use the per cpu buffer first */\n\t\tval = this_cpu_inc_return(trace_buffered_event_cnt);\n\t\tif (val == 1) {\n\t\t\ttrace_event_setup(entry, type, flags, pc);\n\t\t\tentry->array[0] = len;\n\t\t\treturn entry;\n\t\t}\n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t}\n\n\tentry = __trace_buffer_lock_reserve(*current_rb,\n\t\t\t\t\t    type, len, flags, pc);\n\t/*\n\t * If tracing is off, but we have triggers enabled\n\t * we still need to look at the event data. Use the temp_buffer\n\t * to store the trace event for the tigger to use. It's recusive\n\t * safe and will not be recorded anywhere.\n\t */\n\tif (!entry && trace_file->flags & EVENT_FILE_FL_TRIGGER_COND) {\n\t\t*current_rb = temp_buffer;\n\t\tentry = __trace_buffer_lock_reserve(*current_rb,\n\t\t\t\t\t\t    type, len, flags, pc);\n\t}\n\treturn entry;\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_lock_reserve);\n\nstatic DEFINE_SPINLOCK(tracepoint_iter_lock);\nstatic DEFINE_MUTEX(tracepoint_printk_mutex);\n\nstatic void output_printk(struct trace_event_buffer *fbuffer)\n{\n\tstruct trace_event_call *event_call;\n\tstruct trace_event *event;\n\tunsigned long flags;\n\tstruct trace_iterator *iter = tracepoint_print_iter;\n\n\t/* We should never get here if iter is NULL */\n\tif (WARN_ON_ONCE(!iter))\n\t\treturn;\n\n\tevent_call = fbuffer->trace_file->event_call;\n\tif (!event_call || !event_call->event.funcs ||\n\t    !event_call->event.funcs->trace)\n\t\treturn;\n\n\tevent = &fbuffer->trace_file->event_call->event;\n\n\tspin_lock_irqsave(&tracepoint_iter_lock, flags);\n\ttrace_seq_init(&iter->seq);\n\titer->ent = fbuffer->entry;\n\tevent_call->event.funcs->trace(iter, 0, event);\n\ttrace_seq_putc(&iter->seq, 0);\n\tprintk(\"%s\", iter->seq.buffer);\n\n\tspin_unlock_irqrestore(&tracepoint_iter_lock, flags);\n}\n\nint tracepoint_printk_sysctl(struct ctl_table *table, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos)\n{\n\tint save_tracepoint_printk;\n\tint ret;\n\n\tmutex_lock(&tracepoint_printk_mutex);\n\tsave_tracepoint_printk = tracepoint_printk;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\t/*\n\t * This will force exiting early, as tracepoint_printk\n\t * is always zero when tracepoint_printk_iter is not allocated\n\t */\n\tif (!tracepoint_print_iter)\n\t\ttracepoint_printk = 0;\n\n\tif (save_tracepoint_printk == tracepoint_printk)\n\t\tgoto out;\n\n\tif (tracepoint_printk)\n\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\telse\n\t\tstatic_key_disable(&tracepoint_printk_key.key);\n\n out:\n\tmutex_unlock(&tracepoint_printk_mutex);\n\n\treturn ret;\n}\n\nvoid trace_event_buffer_commit(struct trace_event_buffer *fbuffer)\n{\n\tif (static_key_false(&tracepoint_printk_key.key))\n\t\toutput_printk(fbuffer);\n\n\tevent_trigger_unlock_commit(fbuffer->trace_file, fbuffer->buffer,\n\t\t\t\t    fbuffer->event, fbuffer->entry,\n\t\t\t\t    fbuffer->flags, fbuffer->pc);\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_commit);\n\n/*\n * Skip 3:\n *\n *   trace_buffer_unlock_commit_regs()\n *   trace_event_buffer_commit()\n *   trace_event_raw_event_xxx()\n */\n# define STACK_SKIP 3\n\nvoid trace_buffer_unlock_commit_regs(struct trace_array *tr,\n\t\t\t\t     struct ring_buffer *buffer,\n\t\t\t\t     struct ring_buffer_event *event,\n\t\t\t\t     unsigned long flags, int pc,\n\t\t\t\t     struct pt_regs *regs)\n{\n\t__buffer_unlock_commit(buffer, event);\n\n\t/*\n\t * If regs is not set, then skip the necessary functions.\n\t * Note, we can still get here via blktrace, wakeup tracer\n\t * and mmiotrace, but that's ok if they lose a function or\n\t * two. They are not that meaningful.\n\t */\n\tftrace_trace_stack(tr, buffer, flags, regs ? 0 : STACK_SKIP, pc, regs);\n\tftrace_trace_userstack(buffer, flags, pc);\n}\n\n/*\n * Similar to trace_buffer_unlock_commit_regs() but do not dump stack.\n */\nvoid\ntrace_buffer_unlock_commit_nostack(struct ring_buffer *buffer,\n\t\t\t\t   struct ring_buffer_event *event)\n{\n\t__buffer_unlock_commit(buffer, event);\n}\n\nstatic void\ntrace_process_export(struct trace_export *export,\n\t       struct ring_buffer_event *event)\n{\n\tstruct trace_entry *entry;\n\tunsigned int size = 0;\n\n\tentry = ring_buffer_event_data(event);\n\tsize = ring_buffer_event_length(event);\n\texport->write(export, entry, size);\n}\n\nstatic DEFINE_MUTEX(ftrace_export_lock);\n\nstatic struct trace_export __rcu *ftrace_exports_list __read_mostly;\n\nstatic DEFINE_STATIC_KEY_FALSE(ftrace_exports_enabled);\n\nstatic inline void ftrace_exports_enable(void)\n{\n\tstatic_branch_enable(&ftrace_exports_enabled);\n}\n\nstatic inline void ftrace_exports_disable(void)\n{\n\tstatic_branch_disable(&ftrace_exports_enabled);\n}\n\nvoid ftrace_exports(struct ring_buffer_event *event)\n{\n\tstruct trace_export *export;\n\n\tpreempt_disable_notrace();\n\n\texport = rcu_dereference_raw_notrace(ftrace_exports_list);\n\twhile (export) {\n\t\ttrace_process_export(export, event);\n\t\texport = rcu_dereference_raw_notrace(export->next);\n\t}\n\n\tpreempt_enable_notrace();\n}\n\nstatic inline void\nadd_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\trcu_assign_pointer(export->next, *list);\n\t/*\n\t * We are entering export into the list but another\n\t * CPU might be walking that list. We need to make sure\n\t * the export->next pointer is valid before another CPU sees\n\t * the export pointer included into the list.\n\t */\n\trcu_assign_pointer(*list, export);\n}\n\nstatic inline int\nrm_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\tstruct trace_export **p;\n\n\tfor (p = list; *p != NULL; p = &(*p)->next)\n\t\tif (*p == export)\n\t\t\tbreak;\n\n\tif (*p != export)\n\t\treturn -1;\n\n\trcu_assign_pointer(*p, (*p)->next);\n\n\treturn 0;\n}\n\nstatic inline void\nadd_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tif (*list == NULL)\n\t\tftrace_exports_enable();\n\n\tadd_trace_export(list, export);\n}\n\nstatic inline int\nrm_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tint ret;\n\n\tret = rm_trace_export(list, export);\n\tif (*list == NULL)\n\t\tftrace_exports_disable();\n\n\treturn ret;\n}\n\nint register_ftrace_export(struct trace_export *export)\n{\n\tif (WARN_ON_ONCE(!export->write))\n\t\treturn -1;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tadd_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(register_ftrace_export);\n\nint unregister_ftrace_export(struct trace_export *export)\n{\n\tint ret;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tret = rm_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(unregister_ftrace_export);\n\nvoid\ntrace_function(struct trace_array *tr,\n\t       unsigned long ip, unsigned long parent_ip, unsigned long flags,\n\t       int pc)\n{\n\tstruct trace_event_call *call = &event_function;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_entry *entry;\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->parent_ip\t\t= parent_ip;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\tif (static_branch_unlikely(&ftrace_exports_enabled))\n\t\t\tftrace_exports(event);\n\t\t__buffer_unlock_commit(buffer, event);\n\t}\n}\n\n#ifdef CONFIG_STACKTRACE\n\n#define FTRACE_STACK_MAX_ENTRIES (PAGE_SIZE / sizeof(unsigned long))\nstruct ftrace_stack {\n\tunsigned long\t\tcalls[FTRACE_STACK_MAX_ENTRIES];\n};\n\nstatic DEFINE_PER_CPU(struct ftrace_stack, ftrace_stack);\nstatic DEFINE_PER_CPU(int, ftrace_stack_reserve);\n\nstatic void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t unsigned long flags,\n\t\t\t\t int skip, int pc, struct pt_regs *regs)\n{\n\tstruct trace_event_call *call = &event_kernel_stack;\n\tstruct ring_buffer_event *event;\n\tstruct stack_entry *entry;\n\tstruct stack_trace trace;\n\tint use_stack;\n\tint size = FTRACE_STACK_ENTRIES;\n\n\ttrace.nr_entries\t= 0;\n\ttrace.skip\t\t= skip;\n\n\t/*\n\t * Add one, for this function and the call to save_stack_trace()\n\t * If regs is set, then these functions will not be in the way.\n\t */\n#ifndef CONFIG_UNWINDER_ORC\n\tif (!regs)\n\t\ttrace.skip++;\n#endif\n\n\t/*\n\t * Since events can happen in NMIs there's no safe way to\n\t * use the per cpu ftrace_stacks. We reserve it and if an interrupt\n\t * or NMI comes in, it will just have to use the default\n\t * FTRACE_STACK_SIZE.\n\t */\n\tpreempt_disable_notrace();\n\n\tuse_stack = __this_cpu_inc_return(ftrace_stack_reserve);\n\t/*\n\t * We don't need any atomic variables, just a barrier.\n\t * If an interrupt comes in, we don't care, because it would\n\t * have exited and put the counter back to what we want.\n\t * We just need a barrier to keep gcc from moving things\n\t * around.\n\t */\n\tbarrier();\n\tif (use_stack == 1) {\n\t\ttrace.entries\t\t= this_cpu_ptr(ftrace_stack.calls);\n\t\ttrace.max_entries\t= FTRACE_STACK_MAX_ENTRIES;\n\n\t\tif (regs)\n\t\t\tsave_stack_trace_regs(regs, &trace);\n\t\telse\n\t\t\tsave_stack_trace(&trace);\n\n\t\tif (trace.nr_entries > size)\n\t\t\tsize = trace.nr_entries;\n\t} else\n\t\t/* From now on, use_stack is a boolean */\n\t\tuse_stack = 0;\n\n\tsize *= sizeof(unsigned long);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_STACK,\n\t\t\t\t\t    sizeof(*entry) + size, flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\n\tmemset(&entry->caller, 0, size);\n\n\tif (use_stack)\n\t\tmemcpy(&entry->caller, trace.entries,\n\t\t       trace.nr_entries * sizeof(unsigned long));\n\telse {\n\t\ttrace.max_entries\t= FTRACE_STACK_ENTRIES;\n\t\ttrace.entries\t\t= entry->caller;\n\t\tif (regs)\n\t\t\tsave_stack_trace_regs(regs, &trace);\n\t\telse\n\t\t\tsave_stack_trace(&trace);\n\t}\n\n\tentry->size = trace.nr_entries;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out:\n\t/* Again, don't let gcc optimize things here */\n\tbarrier();\n\t__this_cpu_dec(ftrace_stack_reserve);\n\tpreempt_enable_notrace();\n\n}\n\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs)\n{\n\tif (!(tr->trace_flags & TRACE_ITER_STACKTRACE))\n\t\treturn;\n\n\t__ftrace_trace_stack(buffer, flags, skip, pc, regs);\n}\n\nvoid __trace_stack(struct trace_array *tr, unsigned long flags, int skip,\n\t\t   int pc)\n{\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\n\tif (rcu_is_watching()) {\n\t\t__ftrace_trace_stack(buffer, flags, skip, pc, NULL);\n\t\treturn;\n\t}\n\n\t/*\n\t * When an NMI triggers, RCU is enabled via rcu_nmi_enter(),\n\t * but if the above rcu_is_watching() failed, then the NMI\n\t * triggered someplace critical, and rcu_irq_enter() should\n\t * not be called from NMI.\n\t */\n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\trcu_irq_enter_irqson();\n\t__ftrace_trace_stack(buffer, flags, skip, pc, NULL);\n\trcu_irq_exit_irqson();\n}\n\n/**\n * trace_dump_stack - record a stack back trace in the trace buffer\n * @skip: Number of functions to skip (helper handlers)\n */\nvoid trace_dump_stack(int skip)\n{\n\tunsigned long flags;\n\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn;\n\n\tlocal_save_flags(flags);\n\n#ifndef CONFIG_UNWINDER_ORC\n\t/* Skip 1 to skip this function. */\n\tskip++;\n#endif\n\t__ftrace_trace_stack(global_trace.trace_buffer.buffer,\n\t\t\t     flags, skip, preempt_count(), NULL);\n}\n\nstatic DEFINE_PER_CPU(int, user_stack_count);\n\nvoid\nftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags, int pc)\n{\n\tstruct trace_event_call *call = &event_user_stack;\n\tstruct ring_buffer_event *event;\n\tstruct userstack_entry *entry;\n\tstruct stack_trace trace;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_USERSTACKTRACE))\n\t\treturn;\n\n\t/*\n\t * NMIs can not handle page faults, even with fix ups.\n\t * The save user stack can (and often does) fault.\n\t */\n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\t/*\n\t * prevent recursion, since the user stack tracing may\n\t * trigger other kernel events.\n\t */\n\tpreempt_disable();\n\tif (__this_cpu_read(user_stack_count))\n\t\tgoto out;\n\n\t__this_cpu_inc(user_stack_count);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_USER_STACK,\n\t\t\t\t\t    sizeof(*entry), flags, pc);\n\tif (!event)\n\t\tgoto out_drop_count;\n\tentry\t= ring_buffer_event_data(event);\n\n\tentry->tgid\t\t= current->tgid;\n\tmemset(&entry->caller, 0, sizeof(entry->caller));\n\n\ttrace.nr_entries\t= 0;\n\ttrace.max_entries\t= FTRACE_STACK_ENTRIES;\n\ttrace.skip\t\t= 0;\n\ttrace.entries\t\t= entry->caller;\n\n\tsave_stack_trace_user(&trace);\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out_drop_count:\n\t__this_cpu_dec(user_stack_count);\n out:\n\tpreempt_enable();\n}\n\n#ifdef UNUSED\nstatic void __trace_userstack(struct trace_array *tr, unsigned long flags)\n{\n\tftrace_trace_userstack(tr, flags, preempt_count());\n}\n#endif /* UNUSED */\n\n#endif /* CONFIG_STACKTRACE */\n\n/* created for use with alloc_percpu */\nstruct trace_buffer_struct {\n\tint nesting;\n\tchar buffer[4][TRACE_BUF_SIZE];\n};\n\nstatic struct trace_buffer_struct *trace_percpu_buffer;\n\n/*\n * Thise allows for lockless recording.  If we're nested too deeply, then\n * this returns NULL.\n */\nstatic char *get_trace_buf(void)\n{\n\tstruct trace_buffer_struct *buffer = this_cpu_ptr(trace_percpu_buffer);\n\n\tif (!buffer || buffer->nesting >= 4)\n\t\treturn NULL;\n\n\tbuffer->nesting++;\n\n\t/* Interrupts must see nesting incremented before we use the buffer */\n\tbarrier();\n\treturn &buffer->buffer[buffer->nesting][0];\n}\n\nstatic void put_trace_buf(void)\n{\n\t/* Don't let the decrement of nesting leak before this */\n\tbarrier();\n\tthis_cpu_dec(trace_percpu_buffer->nesting);\n}\n\nstatic int alloc_percpu_trace_buffer(void)\n{\n\tstruct trace_buffer_struct *buffers;\n\n\tbuffers = alloc_percpu(struct trace_buffer_struct);\n\tif (WARN(!buffers, \"Could not allocate percpu trace_printk buffer\"))\n\t\treturn -ENOMEM;\n\n\ttrace_percpu_buffer = buffers;\n\treturn 0;\n}\n\nstatic int buffers_allocated;\n\nvoid trace_printk_init_buffers(void)\n{\n\tif (buffers_allocated)\n\t\treturn;\n\n\tif (alloc_percpu_trace_buffer())\n\t\treturn;\n\n\t/* trace_printk() is for debug use only. Don't use it in production. */\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** trace_printk() being used. Allocating extra memory.  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** This means that this is a DEBUG kernel and it is     **\\n\");\n\tpr_warn(\"** unsafe for production use.                           **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** If you see this message and you are not debugging    **\\n\");\n\tpr_warn(\"** the kernel, report this immediately to your vendor!  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\n\t/* Expand the buffers to set size */\n\ttracing_update_buffers();\n\n\tbuffers_allocated = 1;\n\n\t/*\n\t * trace_printk_init_buffers() can be called by modules.\n\t * If that happens, then we need to start cmdline recording\n\t * directly here. If the global_trace.buffer is already\n\t * allocated here, then this was called by module code.\n\t */\n\tif (global_trace.trace_buffer.buffer)\n\t\ttracing_start_cmdline_record();\n}\n\nvoid trace_printk_start_comm(void)\n{\n\t/* Start tracing comms if trace printk is set */\n\tif (!buffers_allocated)\n\t\treturn;\n\ttracing_start_cmdline_record();\n}\n\nstatic void trace_printk_start_stop_comm(int enabled)\n{\n\tif (!buffers_allocated)\n\t\treturn;\n\n\tif (enabled)\n\t\ttracing_start_cmdline_record();\n\telse\n\t\ttracing_stop_cmdline_record();\n}\n\n/**\n * trace_vbprintk - write binary msg to tracing buffer\n *\n */\nint trace_vbprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_bprint;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct trace_array *tr = &global_trace;\n\tstruct bprint_entry *entry;\n\tunsigned long flags;\n\tchar *tbuffer;\n\tint len = 0, size, pc;\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\t/* Don't pollute graph traces with trace_vprintk internals */\n\tpause_graph_tracing();\n\n\tpc = preempt_count();\n\tpreempt_disable_notrace();\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vbin_printf((u32 *)tbuffer, TRACE_BUF_SIZE/sizeof(int), fmt, args);\n\n\tif (len > TRACE_BUF_SIZE/sizeof(int) || len < 0)\n\t\tgoto out;\n\n\tlocal_save_flags(flags);\n\tsize = sizeof(*entry) + sizeof(u32) * len;\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPRINT, size,\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->fmt\t\t\t= fmt;\n\n\tmemcpy(entry->buf, tbuffer, sizeof(u32) * len);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(tr, buffer, flags, 6, pc, NULL);\n\t}\n\nout:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(trace_vbprintk);\n\nstatic int\n__trace_array_vprintk(struct ring_buffer *buffer,\n\t\t      unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_print;\n\tstruct ring_buffer_event *event;\n\tint len = 0, size, pc;\n\tstruct print_entry *entry;\n\tunsigned long flags;\n\tchar *tbuffer;\n\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn 0;\n\n\t/* Don't pollute graph traces with trace_vprintk internals */\n\tpause_graph_tracing();\n\n\tpc = preempt_count();\n\tpreempt_disable_notrace();\n\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vscnprintf(tbuffer, TRACE_BUF_SIZE, fmt, args);\n\n\tlocal_save_flags(flags);\n\tsize = sizeof(*entry) + len + 1;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, tbuffer, len + 1);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(&global_trace, buffer, flags, 6, pc, NULL);\n\t}\n\nout:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\n\nint trace_array_vprintk(struct trace_array *tr,\n\t\t\tunsigned long ip, const char *fmt, va_list args)\n{\n\treturn __trace_array_vprintk(tr->trace_buffer.buffer, ip, fmt, args);\n}\n\nint trace_array_printk(struct trace_array *tr,\n\t\t       unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = trace_array_vprintk(tr, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\nint trace_array_printk_buf(struct ring_buffer *buffer,\n\t\t\t   unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = __trace_array_vprintk(buffer, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\nint trace_vprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\treturn trace_array_vprintk(&global_trace, ip, fmt, args);\n}\nEXPORT_SYMBOL_GPL(trace_vprintk);\n\nstatic void trace_iterator_increment(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, iter->cpu);\n\n\titer->idx++;\n\tif (buf_iter)\n\t\tring_buffer_read(buf_iter, NULL);\n}\n\nstatic struct trace_entry *\npeek_next_entry(struct trace_iterator *iter, int cpu, u64 *ts,\n\t\tunsigned long *lost_events)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, cpu);\n\n\tif (buf_iter)\n\t\tevent = ring_buffer_iter_peek(buf_iter, ts);\n\telse\n\t\tevent = ring_buffer_peek(iter->trace_buffer->buffer, cpu, ts,\n\t\t\t\t\t lost_events);\n\n\tif (event) {\n\t\titer->ent_size = ring_buffer_event_length(event);\n\t\treturn ring_buffer_event_data(event);\n\t}\n\titer->ent_size = 0;\n\treturn NULL;\n}\n\nstatic struct trace_entry *\n__find_next_entry(struct trace_iterator *iter, int *ent_cpu,\n\t\t  unsigned long *missing_events, u64 *ent_ts)\n{\n\tstruct ring_buffer *buffer = iter->trace_buffer->buffer;\n\tstruct trace_entry *ent, *next = NULL;\n\tunsigned long lost_events = 0, next_lost = 0;\n\tint cpu_file = iter->cpu_file;\n\tu64 next_ts = 0, ts;\n\tint next_cpu = -1;\n\tint next_size = 0;\n\tint cpu;\n\n\t/*\n\t * If we are in a per_cpu trace file, don't bother by iterating over\n\t * all cpu and peek directly.\n\t */\n\tif (cpu_file > RING_BUFFER_ALL_CPUS) {\n\t\tif (ring_buffer_empty_cpu(buffer, cpu_file))\n\t\t\treturn NULL;\n\t\tent = peek_next_entry(iter, cpu_file, ent_ts, missing_events);\n\t\tif (ent_cpu)\n\t\t\t*ent_cpu = cpu_file;\n\n\t\treturn ent;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\n\t\tif (ring_buffer_empty_cpu(buffer, cpu))\n\t\t\tcontinue;\n\n\t\tent = peek_next_entry(iter, cpu, &ts, &lost_events);\n\n\t\t/*\n\t\t * Pick the entry with the smallest timestamp:\n\t\t */\n\t\tif (ent && (!next || ts < next_ts)) {\n\t\t\tnext = ent;\n\t\t\tnext_cpu = cpu;\n\t\t\tnext_ts = ts;\n\t\t\tnext_lost = lost_events;\n\t\t\tnext_size = iter->ent_size;\n\t\t}\n\t}\n\n\titer->ent_size = next_size;\n\n\tif (ent_cpu)\n\t\t*ent_cpu = next_cpu;\n\n\tif (ent_ts)\n\t\t*ent_ts = next_ts;\n\n\tif (missing_events)\n\t\t*missing_events = next_lost;\n\n\treturn next;\n}\n\n/* Find the next real entry, without updating the iterator itself */\nstruct trace_entry *trace_find_next_entry(struct trace_iterator *iter,\n\t\t\t\t\t  int *ent_cpu, u64 *ent_ts)\n{\n\treturn __find_next_entry(iter, ent_cpu, NULL, ent_ts);\n}\n\n/* Find the next real entry, and increment the iterator to the next entry */\nvoid *trace_find_next_entry_inc(struct trace_iterator *iter)\n{\n\titer->ent = __find_next_entry(iter, &iter->cpu,\n\t\t\t\t      &iter->lost_events, &iter->ts);\n\n\tif (iter->ent)\n\t\ttrace_iterator_increment(iter);\n\n\treturn iter->ent ? iter : NULL;\n}\n\nstatic void trace_consume(struct trace_iterator *iter)\n{\n\tring_buffer_consume(iter->trace_buffer->buffer, iter->cpu, &iter->ts,\n\t\t\t    &iter->lost_events);\n}\n\nstatic void *s_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tint i = (int)*pos;\n\tvoid *ent;\n\n\tWARN_ON_ONCE(iter->leftover);\n\n\t(*pos)++;\n\n\t/* can't go backwards */\n\tif (iter->idx > i)\n\t\treturn NULL;\n\n\tif (iter->idx < 0)\n\t\tent = trace_find_next_entry_inc(iter);\n\telse\n\t\tent = iter;\n\n\twhile (ent && iter->idx < i)\n\t\tent = trace_find_next_entry_inc(iter);\n\n\titer->pos = *pos;\n\n\treturn ent;\n}\n\nvoid tracing_iter_reset(struct trace_iterator *iter, int cpu)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter;\n\tunsigned long entries = 0;\n\tu64 ts;\n\n\tper_cpu_ptr(iter->trace_buffer->data, cpu)->skipped_entries = 0;\n\n\tbuf_iter = trace_buffer_iter(iter, cpu);\n\tif (!buf_iter)\n\t\treturn;\n\n\tring_buffer_iter_reset(buf_iter);\n\n\t/*\n\t * We could have the case with the max latency tracers\n\t * that a reset never took place on a cpu. This is evident\n\t * by the timestamp being before the start of the buffer.\n\t */\n\twhile ((event = ring_buffer_iter_peek(buf_iter, &ts))) {\n\t\tif (ts >= iter->trace_buffer->time_start)\n\t\t\tbreak;\n\t\tentries++;\n\t\tring_buffer_read(buf_iter, NULL);\n\t}\n\n\tper_cpu_ptr(iter->trace_buffer->data, cpu)->skipped_entries = entries;\n}\n\n/*\n * The current tracer is copied to avoid a global locking\n * all around.\n */\nstatic void *s_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tint cpu_file = iter->cpu_file;\n\tvoid *p = NULL;\n\tloff_t l = 0;\n\tint cpu;\n\n\t/*\n\t * copy the tracer to avoid using a global lock all around.\n\t * iter->trace is a copy of current_trace, the pointer to the\n\t * name may be used instead of a strcmp(), as iter->trace->name\n\t * will point to the same string as current_trace->name.\n\t */\n\tmutex_lock(&trace_types_lock);\n\tif (unlikely(tr->current_trace && iter->trace->name != tr->current_trace->name))\n\t\t*iter->trace = *tr->current_trace;\n\tmutex_unlock(&trace_types_lock);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn ERR_PTR(-EBUSY);\n#endif\n\n\tif (!iter->snapshot)\n\t\tatomic_inc(&trace_record_taskinfo_disabled);\n\n\tif (*pos != iter->pos) {\n\t\titer->ent = NULL;\n\t\titer->cpu = 0;\n\t\titer->idx = -1;\n\n\t\tif (cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\t\tfor_each_tracing_cpu(cpu)\n\t\t\t\ttracing_iter_reset(iter, cpu);\n\t\t} else\n\t\t\ttracing_iter_reset(iter, cpu_file);\n\n\t\titer->leftover = 0;\n\t\tfor (p = iter; p && l < *pos; p = s_next(m, p, &l))\n\t\t\t;\n\n\t} else {\n\t\t/*\n\t\t * If we overflowed the seq_file before, then we want\n\t\t * to just reuse the trace_seq buffer again.\n\t\t */\n\t\tif (iter->leftover)\n\t\t\tp = iter;\n\t\telse {\n\t\t\tl = *pos - 1;\n\t\t\tp = s_next(m, p, &l);\n\t\t}\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(cpu_file);\n\treturn p;\n}\n\nstatic void s_stop(struct seq_file *m, void *p)\n{\n\tstruct trace_iterator *iter = m->private;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn;\n#endif\n\n\tif (!iter->snapshot)\n\t\tatomic_dec(&trace_record_taskinfo_disabled);\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n}\n\nstatic void\nget_total_entries(struct trace_buffer *buf,\n\t\t  unsigned long *total, unsigned long *entries)\n{\n\tunsigned long count;\n\tint cpu;\n\n\t*total = 0;\n\t*entries = 0;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tcount = ring_buffer_entries_cpu(buf->buffer, cpu);\n\t\t/*\n\t\t * If this buffer has skipped entries, then we hold all\n\t\t * entries for the trace and we need to ignore the\n\t\t * ones before the time stamp.\n\t\t */\n\t\tif (per_cpu_ptr(buf->data, cpu)->skipped_entries) {\n\t\t\tcount -= per_cpu_ptr(buf->data, cpu)->skipped_entries;\n\t\t\t/* total is the same as the entries */\n\t\t\t*total += count;\n\t\t} else\n\t\t\t*total += count +\n\t\t\t\tring_buffer_overrun_cpu(buf->buffer, cpu);\n\t\t*entries += count;\n\t}\n}\n\nstatic void print_lat_help_header(struct seq_file *m)\n{\n\tseq_puts(m, \"#                  _------=> CPU#            \\n\"\n\t\t    \"#                 / _-----=> irqs-off        \\n\"\n\t\t    \"#                | / _----=> need-resched    \\n\"\n\t\t    \"#                || / _---=> hardirq/softirq \\n\"\n\t\t    \"#                ||| / _--=> preempt-depth   \\n\"\n\t\t    \"#                |||| /     delay            \\n\"\n\t\t    \"#  cmd     pid   ||||| time  |   caller      \\n\"\n\t\t    \"#     \\\\   /      |||||  \\\\    |   /         \\n\");\n}\n\nstatic void print_event_info(struct trace_buffer *buf, struct seq_file *m)\n{\n\tunsigned long total;\n\tunsigned long entries;\n\n\tget_total_entries(buf, &total, &entries);\n\tseq_printf(m, \"# entries-in-buffer/entries-written: %lu/%lu   #P:%d\\n\",\n\t\t   entries, total, num_online_cpus());\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void print_func_help_header(struct trace_buffer *buf, struct seq_file *m,\n\t\t\t\t   unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\n\tprint_event_info(buf, m);\n\n\tseq_printf(m, \"#           TASK-PID   CPU#   %s  TIMESTAMP  FUNCTION\\n\", tgid ? \"TGID     \" : \"\");\n\tseq_printf(m, \"#              | |       |    %s     |         |\\n\",\t tgid ? \"  |      \" : \"\");\n}\n\nstatic void print_func_help_header_irq(struct trace_buffer *buf, struct seq_file *m,\n\t\t\t\t       unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\tconst char tgid_space[] = \"          \";\n\tconst char space[] = \"  \";\n\n\tseq_printf(m, \"#                          %s  _-----=> irqs-off\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s / _----=> need-resched\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s| / _---=> hardirq/softirq\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s|| / _--=> preempt-depth\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s||| /     delay\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#           TASK-PID   CPU#%s||||    TIMESTAMP  FUNCTION\\n\",\n\t\t   tgid ? \"   TGID   \" : space);\n\tseq_printf(m, \"#              | |       | %s||||       |         |\\n\",\n\t\t   tgid ? \"     |    \" : space);\n}\n\nvoid\nprint_trace_header(struct seq_file *m, struct trace_iterator *iter)\n{\n\tunsigned long sym_flags = (global_trace.trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_buffer *buf = iter->trace_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(buf->data, buf->cpu);\n\tstruct tracer *type = iter->trace;\n\tunsigned long entries;\n\tunsigned long total;\n\tconst char *name = \"preemption\";\n\n\tname = type->name;\n\n\tget_total_entries(buf, &total, &entries);\n\n\tseq_printf(m, \"# %s latency trace v1.1.5 on %s\\n\",\n\t\t   name, UTS_RELEASE);\n\tseq_puts(m, \"# -----------------------------------\"\n\t\t \"---------------------------------\\n\");\n\tseq_printf(m, \"# latency: %lu us, #%lu/%lu, CPU#%d |\"\n\t\t   \" (M:%s VP:%d, KP:%d, SP:%d HP:%d\",\n\t\t   nsecs_to_usecs(data->saved_latency),\n\t\t   entries,\n\t\t   total,\n\t\t   buf->cpu,\n#if defined(CONFIG_PREEMPT_NONE)\n\t\t   \"server\",\n#elif defined(CONFIG_PREEMPT_VOLUNTARY)\n\t\t   \"desktop\",\n#elif defined(CONFIG_PREEMPT)\n\t\t   \"preempt\",\n#else\n\t\t   \"unknown\",\n#endif\n\t\t   /* These are reserved for later use */\n\t\t   0, 0, 0, 0);\n#ifdef CONFIG_SMP\n\tseq_printf(m, \" #P:%d)\\n\", num_online_cpus());\n#else\n\tseq_puts(m, \")\\n\");\n#endif\n\tseq_puts(m, \"#    -----------------\\n\");\n\tseq_printf(m, \"#    | task: %.16s-%d \"\n\t\t   \"(uid:%d nice:%ld policy:%ld rt_prio:%ld)\\n\",\n\t\t   data->comm, data->pid,\n\t\t   from_kuid_munged(seq_user_ns(m), data->uid), data->nice,\n\t\t   data->policy, data->rt_priority);\n\tseq_puts(m, \"#    -----------------\\n\");\n\n\tif (data->critical_start) {\n\t\tseq_puts(m, \"#  => started at: \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_start, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#  => ended at:   \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_end, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#\\n\");\n\t}\n\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void test_cpu_buff_start(struct trace_iterator *iter)\n{\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (!(tr->trace_flags & TRACE_ITER_ANNOTATE))\n\t\treturn;\n\n\tif (!(iter->iter_flags & TRACE_FILE_ANNOTATE))\n\t\treturn;\n\n\tif (cpumask_available(iter->started) &&\n\t    cpumask_test_cpu(iter->cpu, iter->started))\n\t\treturn;\n\n\tif (per_cpu_ptr(iter->trace_buffer->data, iter->cpu)->skipped_entries)\n\t\treturn;\n\n\tif (cpumask_available(iter->started))\n\t\tcpumask_set_cpu(iter->cpu, iter->started);\n\n\t/* Don't print started cpu buffer for the first entry of the trace */\n\tif (iter->idx > 1)\n\t\ttrace_seq_printf(s, \"##### CPU %u buffer started ####\\n\",\n\t\t\t\titer->cpu);\n}\n\nstatic enum print_line_t print_trace_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned long sym_flags = (tr->trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\ttest_cpu_buff_start(iter);\n\n\tevent = ftrace_find_event(entry->type);\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\t\ttrace_print_lat_context(iter);\n\t\telse\n\t\t\ttrace_print_context(iter);\n\t}\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tif (event)\n\t\treturn event->funcs->trace(iter, sym_flags, event);\n\n\ttrace_seq_printf(s, \"Unknown type %d\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_raw_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO)\n\t\ttrace_seq_printf(s, \"%d %d %llu \",\n\t\t\t\t entry->pid, iter->cpu, iter->ts);\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event)\n\t\treturn event->funcs->raw(iter, 0, event);\n\n\ttrace_seq_printf(s, \"%d ?\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_hex_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned char newline = '\\n';\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_HEX_FIELD(s, entry->pid);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event) {\n\t\tenum print_line_t ret = event->funcs->hex(iter, 0, event);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t}\n\n\tSEQ_PUT_FIELD(s, newline);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_bin_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_FIELD(s, entry->pid);\n\t\tSEQ_PUT_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\treturn event ? event->funcs->binary(iter, 0, event) :\n\t\tTRACE_TYPE_HANDLED;\n}\n\nint trace_empty(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter;\n\tint cpu;\n\n\t/* If we are looking at one CPU buffer, only check that one */\n\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\tcpu = iter->cpu_file;\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\n/*  Called with trace_event_read_lock() held. */\nenum print_line_t print_trace_line(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\tenum print_line_t ret;\n\n\tif (iter->lost_events) {\n\t\ttrace_seq_printf(&iter->seq, \"CPU:%d [LOST %lu EVENTS]\\n\",\n\t\t\t\t iter->cpu, iter->lost_events);\n\t\tif (trace_seq_has_overflowed(&iter->seq))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tif (iter->trace && iter->trace->print_line) {\n\t\tret = iter->trace->print_line(iter);\n\t\tif (ret != TRACE_TYPE_UNHANDLED)\n\t\t\treturn ret;\n\t}\n\n\tif (iter->ent->type == TRACE_BPUTS &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bputs_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_BPRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bprintk_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_PRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_printk_msg_only(iter);\n\n\tif (trace_flags & TRACE_ITER_BIN)\n\t\treturn print_bin_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_HEX)\n\t\treturn print_hex_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_RAW)\n\t\treturn print_raw_fmt(iter);\n\n\treturn print_trace_fmt(iter);\n}\n\nvoid trace_latency_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\n\t/* print nothing if the buffers are empty */\n\tif (trace_empty(iter))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\tprint_trace_header(m, iter);\n\n\tif (!(tr->trace_flags & TRACE_ITER_VERBOSE))\n\t\tprint_lat_help_header(m);\n}\n\nvoid trace_default_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\n\tif (!(trace_flags & TRACE_ITER_CONTEXT_INFO))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT) {\n\t\t/* print nothing if the buffers are empty */\n\t\tif (trace_empty(iter))\n\t\t\treturn;\n\t\tprint_trace_header(m, iter);\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE))\n\t\t\tprint_lat_help_header(m);\n\t} else {\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE)) {\n\t\t\tif (trace_flags & TRACE_ITER_IRQ_INFO)\n\t\t\t\tprint_func_help_header_irq(iter->trace_buffer,\n\t\t\t\t\t\t\t   m, trace_flags);\n\t\t\telse\n\t\t\t\tprint_func_help_header(iter->trace_buffer, m,\n\t\t\t\t\t\t       trace_flags);\n\t\t}\n\t}\n}\n\nstatic void test_ftrace_alive(struct seq_file *m)\n{\n\tif (!ftrace_is_dead())\n\t\treturn;\n\tseq_puts(m, \"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\"\n\t\t    \"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\nstatic void show_snapshot_main_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Clears and frees snapshot buffer\\n\"\n\t\t    \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer.\\n\"\n\t\t    \"# echo 2 > snapshot : Clears snapshot buffer (but does not allocate or free)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void show_snapshot_percpu_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Invalid for per_cpu snapshot file.\\n\");\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n\tseq_puts(m, \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer for this cpu.\\n\");\n#else\n\tseq_puts(m, \"# echo 1 > snapshot : Not supported with this kernel.\\n\"\n\t\t    \"#                     Must use main snapshot file to allocate.\\n\");\n#endif\n\tseq_puts(m, \"# echo 2 > snapshot : Clears this cpu's snapshot buffer (but does not allocate)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter)\n{\n\tif (iter->tr->allocated_snapshot)\n\t\tseq_puts(m, \"#\\n# * Snapshot is allocated *\\n#\\n\");\n\telse\n\t\tseq_puts(m, \"#\\n# * Snapshot is freed *\\n#\\n\");\n\n\tseq_puts(m, \"# Snapshot commands:\\n\");\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\tshow_snapshot_main_help(m);\n\telse\n\t\tshow_snapshot_percpu_help(m);\n}\n#else\n/* Should never be called */\nstatic inline void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter) { }\n#endif\n\nstatic int s_show(struct seq_file *m, void *v)\n{\n\tstruct trace_iterator *iter = v;\n\tint ret;\n\n\tif (iter->ent == NULL) {\n\t\tif (iter->tr) {\n\t\t\tseq_printf(m, \"# tracer: %s\\n\", iter->trace->name);\n\t\t\tseq_puts(m, \"#\\n\");\n\t\t\ttest_ftrace_alive(m);\n\t\t}\n\t\tif (iter->snapshot && trace_empty(iter))\n\t\t\tprint_snapshot_help(m, iter);\n\t\telse if (iter->trace && iter->trace->print_header)\n\t\t\titer->trace->print_header(m);\n\t\telse\n\t\t\ttrace_default_header(m);\n\n\t} else if (iter->leftover) {\n\t\t/*\n\t\t * If we filled the seq_file buffer earlier, we\n\t\t * want to just show it now.\n\t\t */\n\t\tret = trace_print_seq(m, &iter->seq);\n\n\t\t/* ret should this time be zero, but you never know */\n\t\titer->leftover = ret;\n\n\t} else {\n\t\tprint_trace_line(iter);\n\t\tret = trace_print_seq(m, &iter->seq);\n\t\t/*\n\t\t * If we overflow the seq_file buffer, then it will\n\t\t * ask us for this data again at start up.\n\t\t * Use that instead.\n\t\t *  ret is 0 if seq_file write succeeded.\n\t\t *        -1 otherwise.\n\t\t */\n\t\titer->leftover = ret;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Should be used after trace_array_get(), trace_types_lock\n * ensures that i_cdev was already initialized.\n */\nstatic inline int tracing_get_cpu(struct inode *inode)\n{\n\tif (inode->i_cdev) /* See trace_create_cpu_file() */\n\t\treturn (long)inode->i_cdev - 1;\n\treturn RING_BUFFER_ALL_CPUS;\n}\n\nstatic const struct seq_operations tracer_seq_ops = {\n\t.start\t\t= s_start,\n\t.next\t\t= s_next,\n\t.stop\t\t= s_stop,\n\t.show\t\t= s_show,\n};\n\nstatic struct trace_iterator *\n__tracing_open(struct inode *inode, struct file *file, bool snapshot)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (tracing_disabled)\n\t\treturn ERR_PTR(-ENODEV);\n\n\titer = __seq_open_private(file, &tracer_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\titer->buffer_iter = kcalloc(nr_cpu_ids, sizeof(*iter->buffer_iter),\n\t\t\t\t    GFP_KERNEL);\n\tif (!iter->buffer_iter)\n\t\tgoto release;\n\n\t/*\n\t * We make a copy of the current tracer to avoid concurrent\n\t * changes on it while we are reading.\n\t */\n\tmutex_lock(&trace_types_lock);\n\titer->trace = kzalloc(sizeof(*iter->trace), GFP_KERNEL);\n\tif (!iter->trace)\n\t\tgoto fail;\n\n\t*iter->trace = *tr->current_trace;\n\n\tif (!zalloc_cpumask_var(&iter->started, GFP_KERNEL))\n\t\tgoto fail;\n\n\titer->tr = tr;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t/* Currently only the top directory has a snapshot */\n\tif (tr->current_trace->print_max || snapshot)\n\t\titer->trace_buffer = &tr->max_buffer;\n\telse\n#endif\n\t\titer->trace_buffer = &tr->trace_buffer;\n\titer->snapshot = snapshot;\n\titer->pos = -1;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\n\t/* Notify the tracer early; before we stop tracing. */\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t/* Annotate start of buffers if we had overruns */\n\tif (ring_buffer_overruns(iter->trace_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\t/* stop the trace while dumping if we are not opening \"snapshot\" */\n\tif (!iter->snapshot)\n\t\ttracing_stop_tr(tr);\n\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\titer->buffer_iter[cpu] =\n\t\t\t\tring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);\n\t\t}\n\t\tring_buffer_read_prepare_sync();\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\t\ttracing_iter_reset(iter, cpu);\n\t\t}\n\t} else {\n\t\tcpu = iter->cpu_file;\n\t\titer->buffer_iter[cpu] =\n\t\t\tring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);\n\t\tring_buffer_read_prepare_sync();\n\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\ttracing_iter_reset(iter, cpu);\n\t}\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn iter;\n\n fail:\n\tmutex_unlock(&trace_types_lock);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\nrelease:\n\tseq_release_private(inode, file);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nint tracing_open_generic(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tfilp->private_data = inode->i_private;\n\treturn 0;\n}\n\nbool tracing_is_disabled(void)\n{\n\treturn (tracing_disabled) ? true: false;\n}\n\n/*\n * Open and update trace_array ref count.\n * Must have the current trace_array passed to it.\n */\nstatic int tracing_open_generic_tr(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tfilp->private_data = inode->i_private;\n\n\treturn 0;\n}\n\nstatic int tracing_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m = file->private_data;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (!(file->f_mode & FMODE_READ)) {\n\t\ttrace_array_put(tr);\n\t\treturn 0;\n\t}\n\n\t/* Writes do not use seq_file */\n\titer = m->private;\n\tmutex_lock(&trace_types_lock);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tif (iter->buffer_iter[cpu])\n\t\t\tring_buffer_read_finish(iter->buffer_iter[cpu]);\n\t}\n\n\tif (iter->trace && iter->trace->close)\n\t\titer->trace->close(iter);\n\n\tif (!iter->snapshot)\n\t\t/* reenable tracing if it was previously enabled */\n\t\ttracing_start_tr(tr);\n\n\t__trace_array_put(tr);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tmutex_destroy(&iter->mutex);\n\tfree_cpumask_var(iter->started);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\n\tseq_release_private(inode, file);\n\n\treturn 0;\n}\n\nstatic int tracing_release_generic_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\treturn 0;\n}\n\nstatic int tracing_single_release_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\n\treturn single_release(inode, file);\n}\n\nstatic int tracing_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret = 0;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\t/* If this file was open for write, then erase contents */\n\tif ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC)) {\n\t\tint cpu = tracing_get_cpu(inode);\n\t\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tif (tr->current_trace->print_max)\n\t\t\ttrace_buf = &tr->max_buffer;\n#endif\n\n\t\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\t\ttracing_reset_online_cpus(trace_buf);\n\t\telse\n\t\t\ttracing_reset(trace_buf, cpu);\n\t}\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, false);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t\telse if (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\t}\n\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\n/*\n * Some tracers are not suitable for instance buffers.\n * A tracer is always available for the global array (toplevel)\n * or if it explicitly states that it is.\n */\nstatic bool\ntrace_ok_for_array(struct tracer *t, struct trace_array *tr)\n{\n\treturn (tr->flags & TRACE_ARRAY_FL_GLOBAL) || t->allow_instances;\n}\n\n/* Find the next tracer that this trace array may use */\nstatic struct tracer *\nget_tracer_for_array(struct trace_array *tr, struct tracer *t)\n{\n\twhile (t && !trace_ok_for_array(t, tr))\n\t\tt = t->next;\n\n\treturn t;\n}\n\nstatic void *\nt_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t = v;\n\n\t(*pos)++;\n\n\tif (t)\n\t\tt = get_tracer_for_array(tr, t->next);\n\n\treturn t;\n}\n\nstatic void *t_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tt = get_tracer_for_array(tr, trace_types);\n\tfor (; t && l < *pos; t = t_next(m, t, &l))\n\t\t\t;\n\n\treturn t;\n}\n\nstatic void t_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int t_show(struct seq_file *m, void *v)\n{\n\tstruct tracer *t = v;\n\n\tif (!t)\n\t\treturn 0;\n\n\tseq_puts(m, t->name);\n\tif (t->next)\n\t\tseq_putc(m, ' ');\n\telse\n\t\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nstatic const struct seq_operations show_traces_seq_ops = {\n\t.start\t\t= t_start,\n\t.next\t\t= t_next,\n\t.stop\t\t= t_stop,\n\t.show\t\t= t_show,\n};\n\nstatic int show_traces_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tret = seq_open(file, &show_traces_seq_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tm = file->private_data;\n\tm->private = tr;\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_write_stub(struct file *filp, const char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\treturn count;\n}\n\nloff_t tracing_lseek(struct file *file, loff_t offset, int whence)\n{\n\tint ret;\n\n\tif (file->f_mode & FMODE_READ)\n\t\tret = seq_lseek(file, offset, whence);\n\telse\n\t\tfile->f_pos = ret = 0;\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_fops = {\n\t.open\t\t= tracing_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_write_stub,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_release,\n};\n\nstatic const struct file_operations show_traces_fops = {\n\t.open\t\t= show_traces_open,\n\t.read\t\t= seq_read,\n\t.release\t= seq_release,\n\t.llseek\t\t= seq_lseek,\n};\n\nstatic ssize_t\ntracing_cpumask_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tchar *mask_str;\n\tint len;\n\n\tlen = snprintf(NULL, 0, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask)) + 1;\n\tmask_str = kmalloc(len, GFP_KERNEL);\n\tif (!mask_str)\n\t\treturn -ENOMEM;\n\n\tlen = snprintf(mask_str, len, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask));\n\tif (len >= count) {\n\t\tcount = -EINVAL;\n\t\tgoto out_err;\n\t}\n\tcount = simple_read_from_buffer(ubuf, count, ppos, mask_str, len);\n\nout_err:\n\tkfree(mask_str);\n\n\treturn count;\n}\n\nstatic ssize_t\ntracing_cpumask_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tcpumask_var_t tracing_cpumask_new;\n\tint err, cpu;\n\n\tif (!alloc_cpumask_var(&tracing_cpumask_new, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\terr = cpumask_parse_user(ubuf, count, tracing_cpumask_new);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\t/*\n\t\t * Increase/decrease the disabled counter if we are\n\t\t * about to flip a bit in the cpumask:\n\t\t */\n\t\tif (cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\t!cpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_inc(&per_cpu_ptr(tr->trace_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_disable_cpu(tr->trace_buffer.buffer, cpu);\n\t\t}\n\t\tif (!cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\tcpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_dec(&per_cpu_ptr(tr->trace_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_enable_cpu(tr->trace_buffer.buffer, cpu);\n\t\t}\n\t}\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\n\tcpumask_copy(tr->tracing_cpumask, tracing_cpumask_new);\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn count;\n\nerr_unlock:\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn err;\n}\n\nstatic const struct file_operations tracing_cpumask_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_cpumask_read,\n\t.write\t\t= tracing_cpumask_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic int tracing_trace_options_show(struct seq_file *m, void *v)\n{\n\tstruct tracer_opt *trace_opts;\n\tstruct trace_array *tr = m->private;\n\tu32 tracer_flags;\n\tint i;\n\n\tmutex_lock(&trace_types_lock);\n\ttracer_flags = tr->current_trace->flags->val;\n\ttrace_opts = tr->current_trace->flags->opts;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (tr->trace_flags & (1 << i))\n\t\t\tseq_printf(m, \"%s\\n\", trace_options[i]);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_options[i]);\n\t}\n\n\tfor (i = 0; trace_opts[i].name; i++) {\n\t\tif (tracer_flags & trace_opts[i].bit)\n\t\t\tseq_printf(m, \"%s\\n\", trace_opts[i].name);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_opts[i].name);\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int __set_tracer_option(struct trace_array *tr,\n\t\t\t       struct tracer_flags *tracer_flags,\n\t\t\t       struct tracer_opt *opts, int neg)\n{\n\tstruct tracer *trace = tracer_flags->trace;\n\tint ret;\n\n\tret = trace->set_flag(tr, tracer_flags->val, opts->bit, !neg);\n\tif (ret)\n\t\treturn ret;\n\n\tif (neg)\n\t\ttracer_flags->val &= ~opts->bit;\n\telse\n\t\ttracer_flags->val |= opts->bit;\n\treturn 0;\n}\n\n/* Try to assign a tracer specific option */\nstatic int set_tracer_option(struct trace_array *tr, char *cmp, int neg)\n{\n\tstruct tracer *trace = tr->current_trace;\n\tstruct tracer_flags *tracer_flags = trace->flags;\n\tstruct tracer_opt *opts = NULL;\n\tint i;\n\n\tfor (i = 0; tracer_flags->opts[i].name; i++) {\n\t\topts = &tracer_flags->opts[i];\n\n\t\tif (strcmp(cmp, opts->name) == 0)\n\t\t\treturn __set_tracer_option(tr, trace->flags, opts, neg);\n\t}\n\n\treturn -EINVAL;\n}\n\n/* Some tracers require overwrite to stay enabled */\nint trace_keep_overwrite(struct tracer *tracer, u32 mask, int set)\n{\n\tif (tracer->enabled && (mask & TRACE_ITER_OVERWRITE) && !set)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nint set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled)\n{\n\t/* do nothing if flag is already set */\n\tif (!!(tr->trace_flags & mask) == !!enabled)\n\t\treturn 0;\n\n\t/* Give the tracer a chance to approve the change */\n\tif (tr->current_trace->flag_changed)\n\t\tif (tr->current_trace->flag_changed(tr, mask, !!enabled))\n\t\t\treturn -EINVAL;\n\n\tif (enabled)\n\t\ttr->trace_flags |= mask;\n\telse\n\t\ttr->trace_flags &= ~mask;\n\n\tif (mask == TRACE_ITER_RECORD_CMD)\n\t\ttrace_event_enable_cmd_record(enabled);\n\n\tif (mask == TRACE_ITER_RECORD_TGID) {\n\t\tif (!tgid_map)\n\t\t\ttgid_map = kcalloc(PID_MAX_DEFAULT + 1,\n\t\t\t\t\t   sizeof(*tgid_map),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!tgid_map) {\n\t\t\ttr->trace_flags &= ~TRACE_ITER_RECORD_TGID;\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttrace_event_enable_tgid_record(enabled);\n\t}\n\n\tif (mask == TRACE_ITER_EVENT_FORK)\n\t\ttrace_event_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_FUNC_FORK)\n\t\tftrace_pid_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_OVERWRITE) {\n\t\tring_buffer_change_overwrite(tr->trace_buffer.buffer, enabled);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tring_buffer_change_overwrite(tr->max_buffer.buffer, enabled);\n#endif\n\t}\n\n\tif (mask == TRACE_ITER_PRINTK) {\n\t\ttrace_printk_start_stop_comm(enabled);\n\t\ttrace_printk_control(enabled);\n\t}\n\n\treturn 0;\n}\n\nstatic int trace_set_options(struct trace_array *tr, char *option)\n{\n\tchar *cmp;\n\tint neg = 0;\n\tint ret;\n\tsize_t orig_len = strlen(option);\n\n\tcmp = strstrip(option);\n\n\tif (strncmp(cmp, \"no\", 2) == 0) {\n\t\tneg = 1;\n\t\tcmp += 2;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\tret = match_string(trace_options, -1, cmp);\n\t/* If no option could be set, test the specific tracer options */\n\tif (ret < 0)\n\t\tret = set_tracer_option(tr, cmp, neg);\n\telse\n\t\tret = set_tracer_flag(tr, 1 << ret, !neg);\n\n\tmutex_unlock(&trace_types_lock);\n\n\t/*\n\t * If the first trailing whitespace is replaced with '\\0' by strstrip,\n\t * turn it back into a space.\n\t */\n\tif (orig_len > strlen(option))\n\t\toption[strlen(option)] = ' ';\n\n\treturn ret;\n}\n\nstatic void __init apply_trace_boot_options(void)\n{\n\tchar *buf = trace_boot_options_buf;\n\tchar *option;\n\n\twhile (true) {\n\t\toption = strsep(&buf, \",\");\n\n\t\tif (!option)\n\t\t\tbreak;\n\n\t\tif (*option)\n\t\t\ttrace_set_options(&global_trace, option);\n\n\t\t/* Put back the comma to allow this to be called again */\n\t\tif (buf)\n\t\t\t*(buf - 1) = ',';\n\t}\n}\n\nstatic ssize_t\ntracing_trace_options_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tret = trace_set_options(tr, buf);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_trace_options_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_trace_options_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_iter_fops = {\n\t.open\t\t= tracing_trace_options_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_trace_options_write,\n};\n\nstatic const char readme_msg[] =\n\t\"tracing mini-HOWTO:\\n\\n\"\n\t\"# echo 0 > tracing_on : quick way to disable tracing\\n\"\n\t\"# echo 1 > tracing_on : quick way to re-enable tracing\\n\\n\"\n\t\" Important files:\\n\"\n\t\"  trace\\t\\t\\t- The static contents of the buffer\\n\"\n\t\"\\t\\t\\t  To clear the buffer write into this file: echo > trace\\n\"\n\t\"  trace_pipe\\t\\t- A consuming read to see the contents of the buffer\\n\"\n\t\"  current_tracer\\t- function and latency tracers\\n\"\n\t\"  available_tracers\\t- list of configured tracers for current_tracer\\n\"\n\t\"  buffer_size_kb\\t- view and modify size of per cpu buffer\\n\"\n\t\"  buffer_total_size_kb  - view total size of all cpu buffers\\n\\n\"\n\t\"  trace_clock\\t\\t-change the clock used to order events\\n\"\n\t\"       local:   Per cpu clock but may not be synced across CPUs\\n\"\n\t\"      global:   Synced across CPUs but slows tracing down.\\n\"\n\t\"     counter:   Not a clock, but just an increment\\n\"\n\t\"      uptime:   Jiffy counter from time of boot\\n\"\n\t\"        perf:   Same clock that perf events use\\n\"\n#ifdef CONFIG_X86_64\n\t\"     x86-tsc:   TSC cycle counter\\n\"\n#endif\n\t\"\\n  timestamp_mode\\t-view the mode used to timestamp events\\n\"\n\t\"       delta:   Delta difference against a buffer-wide timestamp\\n\"\n\t\"    absolute:   Absolute (standalone) timestamp\\n\"\n\t\"\\n  trace_marker\\t\\t- Writes into this file writes into the kernel buffer\\n\"\n\t\"\\n  trace_marker_raw\\t\\t- Writes into this file writes binary data into the kernel buffer\\n\"\n\t\"  tracing_cpumask\\t- Limit which CPUs to trace\\n\"\n\t\"  instances\\t\\t- Make sub-buffers with: mkdir instances/foo\\n\"\n\t\"\\t\\t\\t  Remove sub-buffer with rmdir\\n\"\n\t\"  trace_options\\t\\t- Set format or modify how tracing happens\\n\"\n\t\"\\t\\t\\t  Disable an option by adding a suffix 'no' to the\\n\"\n\t\"\\t\\t\\t  option name\\n\"\n\t\"  saved_cmdlines_size\\t- echo command number in here to store comm-pid list\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"\\n  available_filter_functions - list of functions that can be filtered on\\n\"\n\t\"  set_ftrace_filter\\t- echo function name in here to only trace these\\n\"\n\t\"\\t\\t\\t  functions\\n\"\n\t\"\\t     accepts: func_full_name or glob-matching-pattern\\n\"\n\t\"\\t     modules: Can select a group via module\\n\"\n\t\"\\t      Format: :mod:<module-name>\\n\"\n\t\"\\t     example: echo :mod:ext3 > set_ftrace_filter\\n\"\n\t\"\\t    triggers: a command to perform when function is hit\\n\"\n\t\"\\t      Format: <function>:<trigger>[:count]\\n\"\n\t\"\\t     trigger: traceon, traceoff\\n\"\n\t\"\\t\\t      enable_event:<system>:<event>\\n\"\n\t\"\\t\\t      disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t      stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t      snapshot\\n\"\n#endif\n\t\"\\t\\t      dump\\n\"\n\t\"\\t\\t      cpudump\\n\"\n\t\"\\t     example: echo do_fault:traceoff > set_ftrace_filter\\n\"\n\t\"\\t              echo do_trap:traceoff:3 > set_ftrace_filter\\n\"\n\t\"\\t     The first one will disable tracing every time do_fault is hit\\n\"\n\t\"\\t     The second will disable tracing at most 3 times when do_trap is hit\\n\"\n\t\"\\t       The first time do trap is hit and it disables tracing, the\\n\"\n\t\"\\t       counter will decrement to 2. If tracing is already disabled,\\n\"\n\t\"\\t       the counter will not decrement. It only decrements when the\\n\"\n\t\"\\t       trigger did work\\n\"\n\t\"\\t     To remove trigger without count:\\n\"\n\t\"\\t       echo '!<function>:<trigger> > set_ftrace_filter\\n\"\n\t\"\\t     To remove trigger with a count:\\n\"\n\t\"\\t       echo '!<function>:<trigger>:0 > set_ftrace_filter\\n\"\n\t\"  set_ftrace_notrace\\t- echo function name in here to never trace.\\n\"\n\t\"\\t    accepts: func_full_name, *func_end, func_begin*, *func_middle*\\n\"\n\t\"\\t    modules: Can select a group via module command :mod:\\n\"\n\t\"\\t    Does not accept triggers\\n\"\n#endif /* CONFIG_DYNAMIC_FTRACE */\n#ifdef CONFIG_FUNCTION_TRACER\n\t\"  set_ftrace_pid\\t- Write pid(s) to only function trace those pids\\n\"\n\t\"\\t\\t    (function)\\n\"\n#endif\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\"  set_graph_function\\t- Trace the nested calls of a function (function_graph)\\n\"\n\t\"  set_graph_notrace\\t- Do not trace the nested calls of a function (function_graph)\\n\"\n\t\"  max_graph_depth\\t- Trace a limited depth of nested calls (0 is unlimited)\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\n  snapshot\\t\\t- Like 'trace' but shows the content of the static\\n\"\n\t\"\\t\\t\\t  snapshot buffer. Read the contents for more\\n\"\n\t\"\\t\\t\\t  information\\n\"\n#endif\n#ifdef CONFIG_STACK_TRACER\n\t\"  stack_trace\\t\\t- Shows the max stack trace when active\\n\"\n\t\"  stack_max_size\\t- Shows current max stack size that was traced\\n\"\n\t\"\\t\\t\\t  Write into this file to reset the max size (trigger a\\n\"\n\t\"\\t\\t\\t  new trace)\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"  stack_trace_filter\\t- Like set_ftrace_filter but limits what stack_trace\\n\"\n\t\"\\t\\t\\t  traces\\n\"\n#endif\n#endif /* CONFIG_STACK_TRACER */\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"  kprobe_events\\t\\t- Add/remove/show the kernel dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\t\"  uprobe_events\\t\\t- Add/remove/show the userspace dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS)\n\t\"\\t  accepts: event-definitions (one definition per line)\\n\"\n\t\"\\t   Format: p[:[<group>/]<event>] <place> [<args>]\\n\"\n\t\"\\t           r[maxactive][:[<group>/]<event>] <place> [<args>]\\n\"\n\t\"\\t           -:[<group>/]<event>\\n\"\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"\\t    place: [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n  \"place (kretprobe): [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\t\"\\t    place: <path>:<offset>\\n\"\n#endif\n\t\"\\t     args: <name>=fetcharg[:type]\\n\"\n\t\"\\t fetcharg: %<register>, @<address>, @<symbol>[+|-<offset>],\\n\"\n\t\"\\t           $stack<index>, $stack, $retval, $comm\\n\"\n\t\"\\t     type: s8/16/32/64, u8/16/32/64, x8/16/32/64, string,\\n\"\n\t\"\\t           b<bit-width>@<bit-offset>/<container-size>\\n\"\n#endif\n\t\"  events/\\t\\t- Directory containing all trace event subsystems:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all events\\n\"\n\t\"  events/<system>/\\t- Directory containing all trace events for <system>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all <system>\\n\"\n\t\"\\t\\t\\t  events\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"  events/<system>/<event>/\\t- Directory containing control files for\\n\"\n\t\"\\t\\t\\t  <event>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of <event>\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"      trigger\\t\\t- If set, a command to perform when event is hit\\n\"\n\t\"\\t    Format: <trigger>[:count][if <filter>]\\n\"\n\t\"\\t   trigger: traceon, traceoff\\n\"\n\t\"\\t            enable_event:<system>:<event>\\n\"\n\t\"\\t            disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t            enable_hist:<system>:<event>\\n\"\n\t\"\\t            disable_hist:<system>:<event>\\n\"\n#endif\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t    stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t    snapshot\\n\"\n#endif\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t\\t    hist (see below)\\n\"\n#endif\n\t\"\\t   example: echo traceoff > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo traceoff:3 > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo 'enable_event:kmem:kmalloc:3 if nr_rq > 1' > \\\\\\n\"\n\t\"\\t                  events/block/block_unplug/trigger\\n\"\n\t\"\\t   The first disables tracing every time block_unplug is hit.\\n\"\n\t\"\\t   The second disables tracing the first 3 times block_unplug is hit.\\n\"\n\t\"\\t   The third enables the kmalloc event the first 3 times block_unplug\\n\"\n\t\"\\t     is hit and has value of greater than 1 for the 'nr_rq' event field.\\n\"\n\t\"\\t   Like function triggers, the counter is only decremented if it\\n\"\n\t\"\\t    enabled or disabled tracing.\\n\"\n\t\"\\t   To remove a trigger without a count:\\n\"\n\t\"\\t     echo '!<trigger> > <system>/<event>/trigger\\n\"\n\t\"\\t   To remove a trigger with a count:\\n\"\n\t\"\\t     echo '!<trigger>:0 > <system>/<event>/trigger\\n\"\n\t\"\\t   Filters can be ignored when removing a trigger.\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"      hist trigger\\t- If set, event hits are aggregated into a hash table\\n\"\n\t\"\\t    Format: hist:keys=<field1[,field2,...]>\\n\"\n\t\"\\t            [:values=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:sort=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:size=#entries]\\n\"\n\t\"\\t            [:pause][:continue][:clear]\\n\"\n\t\"\\t            [:name=histname1]\\n\"\n\t\"\\t            [if <filter>]\\n\\n\"\n\t\"\\t    When a matching event is hit, an entry is added to a hash\\n\"\n\t\"\\t    table using the key(s) and value(s) named, and the value of a\\n\"\n\t\"\\t    sum called 'hitcount' is incremented.  Keys and values\\n\"\n\t\"\\t    correspond to fields in the event's format description.  Keys\\n\"\n\t\"\\t    can be any field, or the special string 'stacktrace'.\\n\"\n\t\"\\t    Compound keys consisting of up to two fields can be specified\\n\"\n\t\"\\t    by the 'keys' keyword.  Values must correspond to numeric\\n\"\n\t\"\\t    fields.  Sort keys consisting of up to two fields can be\\n\"\n\t\"\\t    specified using the 'sort' keyword.  The sort direction can\\n\"\n\t\"\\t    be modified by appending '.descending' or '.ascending' to a\\n\"\n\t\"\\t    sort field.  The 'size' parameter can be used to specify more\\n\"\n\t\"\\t    or fewer than the default 2048 entries for the hashtable size.\\n\"\n\t\"\\t    If a hist trigger is given a name using the 'name' parameter,\\n\"\n\t\"\\t    its histogram data will be shared with other triggers of the\\n\"\n\t\"\\t    same name, and trigger hits will update this common data.\\n\\n\"\n\t\"\\t    Reading the 'hist' file for the event will dump the hash\\n\"\n\t\"\\t    table in its entirety to stdout.  If there are multiple hist\\n\"\n\t\"\\t    triggers attached to an event, there will be a table for each\\n\"\n\t\"\\t    trigger in the output.  The table displayed for a named\\n\"\n\t\"\\t    trigger will be the same as any other instance having the\\n\"\n\t\"\\t    same name.  The default format used to display a given field\\n\"\n\t\"\\t    can be modified by appending any of the following modifiers\\n\"\n\t\"\\t    to the field name, as applicable:\\n\\n\"\n\t\"\\t            .hex        display a number as a hex value\\n\"\n\t\"\\t            .sym        display an address as a symbol\\n\"\n\t\"\\t            .sym-offset display an address as a symbol and offset\\n\"\n\t\"\\t            .execname   display a common_pid as a program name\\n\"\n\t\"\\t            .syscall    display a syscall id as a syscall name\\n\"\n\t\"\\t            .log2       display log2 value rather than raw number\\n\"\n\t\"\\t            .usecs      display a common_timestamp in microseconds\\n\\n\"\n\t\"\\t    The 'pause' parameter can be used to pause an existing hist\\n\"\n\t\"\\t    trigger or to start a hist trigger but not log any events\\n\"\n\t\"\\t    until told to do so.  'continue' can be used to start or\\n\"\n\t\"\\t    restart a paused hist trigger.\\n\\n\"\n\t\"\\t    The 'clear' parameter will clear the contents of a running\\n\"\n\t\"\\t    hist trigger and leave its current paused/active state\\n\"\n\t\"\\t    unchanged.\\n\\n\"\n\t\"\\t    The enable_hist and disable_hist triggers can be used to\\n\"\n\t\"\\t    have one event conditionally start and stop another event's\\n\"\n\t\"\\t    already-attached hist trigger.  The syntax is analagous to\\n\"\n\t\"\\t    the enable_event and disable_event triggers.\\n\"\n#endif\n;\n\nstatic ssize_t\ntracing_readme_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\treturn simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t\treadme_msg, strlen(readme_msg));\n}\n\nstatic const struct file_operations tracing_readme_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_readme_read,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic void *saved_tgids_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tint *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr <= &tgid_map[PID_MAX_DEFAULT]; ptr++) {\n\t\tif (trace_find_tgid(*ptr))\n\t\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_tgids_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tif (!tgid_map)\n\t\treturn NULL;\n\n\tv = &tgid_map[0];\n\twhile (l <= *pos) {\n\t\tv = saved_tgids_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_tgids_stop(struct seq_file *m, void *v)\n{\n}\n\nstatic int saved_tgids_show(struct seq_file *m, void *v)\n{\n\tint pid = (int *)v - tgid_map;\n\n\tseq_printf(m, \"%d %d\\n\", pid, trace_find_tgid(pid));\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_tgids_seq_ops = {\n\t.start\t\t= saved_tgids_start,\n\t.stop\t\t= saved_tgids_stop,\n\t.next\t\t= saved_tgids_next,\n\t.show\t\t= saved_tgids_show,\n};\n\nstatic int tracing_saved_tgids_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_saved_tgids_seq_ops);\n}\n\n\nstatic const struct file_operations tracing_saved_tgids_fops = {\n\t.open\t\t= tracing_saved_tgids_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic void *saved_cmdlines_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunsigned int *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr < &savedcmd->map_cmdline_to_pid[savedcmd->cmdline_num];\n\t     ptr++) {\n\t\tif (*ptr == -1 || *ptr == NO_CMDLINE_MAP)\n\t\t\tcontinue;\n\n\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_cmdlines_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\tv = &savedcmd->map_cmdline_to_pid[0];\n\twhile (l <= *pos) {\n\t\tv = saved_cmdlines_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_cmdlines_stop(struct seq_file *m, void *v)\n{\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nstatic int saved_cmdlines_show(struct seq_file *m, void *v)\n{\n\tchar buf[TASK_COMM_LEN];\n\tunsigned int *pid = v;\n\n\t__trace_find_cmdline(*pid, buf);\n\tseq_printf(m, \"%d %s\\n\", *pid, buf);\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_cmdlines_seq_ops = {\n\t.start\t\t= saved_cmdlines_start,\n\t.next\t\t= saved_cmdlines_next,\n\t.stop\t\t= saved_cmdlines_stop,\n\t.show\t\t= saved_cmdlines_show,\n};\n\nstatic int tracing_saved_cmdlines_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_saved_cmdlines_seq_ops);\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_fops = {\n\t.open\t\t= tracing_saved_cmdlines_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic ssize_t\ntracing_saved_cmdlines_size_read(struct file *filp, char __user *ubuf,\n\t\t\t\t size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tr = scnprintf(buf, sizeof(buf), \"%u\\n\", savedcmd->cmdline_num);\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic void free_saved_cmdlines_buffer(struct saved_cmdlines_buffer *s)\n{\n\tkfree(s->saved_cmdlines);\n\tkfree(s->map_cmdline_to_pid);\n\tkfree(s);\n}\n\nstatic int tracing_resize_saved_cmdlines(unsigned int val)\n{\n\tstruct saved_cmdlines_buffer *s, *savedcmd_temp;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tif (allocate_cmdlines_buffer(val, s) < 0) {\n\t\tkfree(s);\n\t\treturn -ENOMEM;\n\t}\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tsavedcmd_temp = savedcmd;\n\tsavedcmd = s;\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tfree_saved_cmdlines_buffer(savedcmd_temp);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_saved_cmdlines_size_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t/* must have at least 1 entry or less than PID_MAX_DEFAULT */\n\tif (!val || val > PID_MAX_DEFAULT)\n\t\treturn -EINVAL;\n\n\tret = tracing_resize_saved_cmdlines((unsigned int)val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_size_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_saved_cmdlines_size_read,\n\t.write\t\t= tracing_saved_cmdlines_size_write,\n};\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic union trace_eval_map_item *\nupdate_eval_map(union trace_eval_map_item *ptr)\n{\n\tif (!ptr->map.eval_string) {\n\t\tif (ptr->tail.next) {\n\t\t\tptr = ptr->tail.next;\n\t\t\t/* Set ptr to the next real item (skip head) */\n\t\t\tptr++;\n\t\t} else\n\t\t\treturn NULL;\n\t}\n\treturn ptr;\n}\n\nstatic void *eval_map_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\t/*\n\t * Paranoid! If ptr points to end, we don't want to increment past it.\n\t * This really should never happen.\n\t */\n\tptr = update_eval_map(ptr);\n\tif (WARN_ON_ONCE(!ptr))\n\t\treturn NULL;\n\n\tptr++;\n\n\t(*pos)++;\n\n\tptr = update_eval_map(ptr);\n\n\treturn ptr;\n}\n\nstatic void *eval_map_start(struct seq_file *m, loff_t *pos)\n{\n\tunion trace_eval_map_item *v;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tv = trace_eval_maps;\n\tif (v)\n\t\tv++;\n\n\twhile (v && l < *pos) {\n\t\tv = eval_map_next(m, v, &l);\n\t}\n\n\treturn v;\n}\n\nstatic void eval_map_stop(struct seq_file *m, void *v)\n{\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic int eval_map_show(struct seq_file *m, void *v)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\tseq_printf(m, \"%s %ld (%s)\\n\",\n\t\t   ptr->map.eval_string, ptr->map.eval_value,\n\t\t   ptr->map.system);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_eval_map_seq_ops = {\n\t.start\t\t= eval_map_start,\n\t.next\t\t= eval_map_next,\n\t.stop\t\t= eval_map_stop,\n\t.show\t\t= eval_map_show,\n};\n\nstatic int tracing_eval_map_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_eval_map_seq_ops);\n}\n\nstatic const struct file_operations tracing_eval_map_fops = {\n\t.open\t\t= tracing_eval_map_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic inline union trace_eval_map_item *\ntrace_eval_jmp_to_tail(union trace_eval_map_item *ptr)\n{\n\t/* Return tail of array given the head */\n\treturn ptr + ptr->head.length + 1;\n}\n\nstatic void\ntrace_insert_eval_map_file(struct module *mod, struct trace_eval_map **start,\n\t\t\t   int len)\n{\n\tstruct trace_eval_map **stop;\n\tstruct trace_eval_map **map;\n\tunion trace_eval_map_item *map_array;\n\tunion trace_eval_map_item *ptr;\n\n\tstop = start + len;\n\n\t/*\n\t * The trace_eval_maps contains the map plus a head and tail item,\n\t * where the head holds the module and length of array, and the\n\t * tail holds a pointer to the next list.\n\t */\n\tmap_array = kmalloc_array(len + 2, sizeof(*map_array), GFP_KERNEL);\n\tif (!map_array) {\n\t\tpr_warn(\"Unable to allocate trace eval mapping\\n\");\n\t\treturn;\n\t}\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tif (!trace_eval_maps)\n\t\ttrace_eval_maps = map_array;\n\telse {\n\t\tptr = trace_eval_maps;\n\t\tfor (;;) {\n\t\t\tptr = trace_eval_jmp_to_tail(ptr);\n\t\t\tif (!ptr->tail.next)\n\t\t\t\tbreak;\n\t\t\tptr = ptr->tail.next;\n\n\t\t}\n\t\tptr->tail.next = map_array;\n\t}\n\tmap_array->head.mod = mod;\n\tmap_array->head.length = len;\n\tmap_array++;\n\n\tfor (map = start; (unsigned long)map < (unsigned long)stop; map++) {\n\t\tmap_array->map = **map;\n\t\tmap_array++;\n\t}\n\tmemset(map_array, 0, sizeof(*map_array));\n\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic void trace_create_eval_file(struct dentry *d_tracer)\n{\n\ttrace_create_file(\"eval_map\", 0444, d_tracer,\n\t\t\t  NULL, &tracing_eval_map_fops);\n}\n\n#else /* CONFIG_TRACE_EVAL_MAP_FILE */\nstatic inline void trace_create_eval_file(struct dentry *d_tracer) { }\nstatic inline void trace_insert_eval_map_file(struct module *mod,\n\t\t\t      struct trace_eval_map **start, int len) { }\n#endif /* !CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic void trace_insert_eval_map(struct module *mod,\n\t\t\t\t  struct trace_eval_map **start, int len)\n{\n\tstruct trace_eval_map **map;\n\n\tif (len <= 0)\n\t\treturn;\n\n\tmap = start;\n\n\ttrace_event_eval_update(map, len);\n\n\ttrace_insert_eval_map_file(mod, start, len);\n}\n\nstatic ssize_t\ntracing_set_trace_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+2];\n\tint r;\n\n\tmutex_lock(&trace_types_lock);\n\tr = sprintf(buf, \"%s\\n\", tr->current_trace->name);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nint tracer_init(struct tracer *t, struct trace_array *tr)\n{\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\treturn t->init(tr);\n}\n\nstatic void set_buffer_entries(struct trace_buffer *buf, unsigned long val)\n{\n\tint cpu;\n\n\tfor_each_tracing_cpu(cpu)\n\t\tper_cpu_ptr(buf->data, cpu)->entries = val;\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n/* resize @tr's buffer to the size of @size_tr's entries */\nstatic int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,\n\t\t\t\t\tstruct trace_buffer *size_buf, int cpu_id)\n{\n\tint cpu, ret = 0;\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu)->entries, cpu);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tper_cpu_ptr(trace_buf->data, cpu)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu)->entries;\n\t\t}\n\t} else {\n\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu_id)->entries, cpu_id);\n\t\tif (ret == 0)\n\t\t\tper_cpu_ptr(trace_buf->data, cpu_id)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu_id)->entries;\n\t}\n\n\treturn ret;\n}\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\nstatic int __tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\tunsigned long size, int cpu)\n{\n\tint ret;\n\n\t/*\n\t * If kernel or user changes the size of the ring buffer\n\t * we use the size that was given, and we can forget about\n\t * expanding it later.\n\t */\n\tring_buffer_expanded = true;\n\n\t/* May be called before buffers are initialized */\n\tif (!tr->trace_buffer.buffer)\n\t\treturn 0;\n\n\tret = ring_buffer_resize(tr->trace_buffer.buffer, size, cpu);\n\tif (ret < 0)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (!(tr->flags & TRACE_ARRAY_FL_GLOBAL) ||\n\t    !tr->current_trace->use_max_tr)\n\t\tgoto out;\n\n\tret = ring_buffer_resize(tr->max_buffer.buffer, size, cpu);\n\tif (ret < 0) {\n\t\tint r = resize_buffer_duplicate_size(&tr->trace_buffer,\n\t\t\t\t\t\t     &tr->trace_buffer, cpu);\n\t\tif (r < 0) {\n\t\t\t/*\n\t\t\t * AARGH! We are left with different\n\t\t\t * size max buffer!!!!\n\t\t\t * The max buffer is our \"snapshot\" buffer.\n\t\t\t * When a tracer needs a snapshot (one of the\n\t\t\t * latency tracers), it swaps the max buffer\n\t\t\t * with the saved snap shot. We succeeded to\n\t\t\t * update the size of the main buffer, but failed to\n\t\t\t * update the size of the max buffer. But when we tried\n\t\t\t * to reset the main buffer to the original size, we\n\t\t\t * failed there too. This is very unlikely to\n\t\t\t * happen, but if it does, warn and kill all\n\t\t\t * tracing.\n\t\t\t */\n\t\t\tWARN_ON(1);\n\t\t\ttracing_disabled = 1;\n\t\t}\n\t\treturn ret;\n\t}\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\tset_buffer_entries(&tr->max_buffer, size);\n\telse\n\t\tper_cpu_ptr(tr->max_buffer.data, cpu)->entries = size;\n\n out:\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\tset_buffer_entries(&tr->trace_buffer, size);\n\telse\n\t\tper_cpu_ptr(tr->trace_buffer.data, cpu)->entries = size;\n\n\treturn ret;\n}\n\nstatic ssize_t tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\t  unsigned long size, int cpu_id)\n{\n\tint ret = size;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu_id != RING_BUFFER_ALL_CPUS) {\n\t\t/* make sure, this cpu is enabled in the mask */\n\t\tif (!cpumask_test_cpu(cpu_id, tracing_buffer_mask)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = __tracing_resize_ring_buffer(tr, size, cpu_id);\n\tif (ret < 0)\n\t\tret = -ENOMEM;\n\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n\n/**\n * tracing_update_buffers - used by tracing facility to expand ring buffers\n *\n * To save on memory when the tracing is never used on a system with it\n * configured in. The ring buffers are set to a minimum size. But once\n * a user starts to use the tracing facility, then they need to grow\n * to their default size.\n *\n * This function is to be called when a tracer is about to be used.\n */\nint tracing_update_buffers(void)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tif (!ring_buffer_expanded)\n\t\tret = __tracing_resize_ring_buffer(&global_trace, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct trace_option_dentry;\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer);\n\n/*\n * Used to clear out the tracer before deletion of an instance.\n * Must have trace_types_lock held.\n */\nstatic void tracing_set_nop(struct trace_array *tr)\n{\n\tif (tr->current_trace == &nop_trace)\n\t\treturn;\n\t\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\ttr->current_trace = &nop_trace;\n}\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t)\n{\n\t/* Only enable if the directory has been created already. */\n\tif (!tr->dir)\n\t\treturn;\n\n\tcreate_trace_option_files(tr, t);\n}\n\nstatic int tracing_set_tracer(struct trace_array *tr, const char *buf)\n{\n\tstruct tracer *t;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbool had_max_tr;\n#endif\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (!ring_buffer_expanded) {\n\t\tret = __tracing_resize_ring_buffer(tr, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = 0;\n\t}\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(t->name, buf) == 0)\n\t\t\tbreak;\n\t}\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (t == tr->current_trace)\n\t\tgoto out;\n\n\t/* Some tracers won't work on kernel command line */\n\tif (system_state < SYSTEM_RUNNING && t->noboot) {\n\t\tpr_warn(\"Tracer '%s' is not allowed on command line, ignored\\n\",\n\t\t\tt->name);\n\t\tgoto out;\n\t}\n\n\t/* Some tracers are only allowed for the top level buffer */\n\tif (!trace_ok_for_array(t, tr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If trace pipe files are being read, we can't change the tracer */\n\tif (tr->current_trace->ref) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\ttrace_branch_disable();\n\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\t/* Current trace needs to be nop_trace before synchronize_sched */\n\ttr->current_trace = &nop_trace;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\thad_max_tr = tr->allocated_snapshot;\n\n\tif (had_max_tr && !t->use_max_tr) {\n\t\t/*\n\t\t * We need to make sure that the update_max_tr sees that\n\t\t * current_trace changed to nop_trace to keep it from\n\t\t * swapping the buffers after we resize it.\n\t\t * The update_max_tr is called from interrupts disabled\n\t\t * so a synchronized_sched() is sufficient.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfree_snapshot(tr);\n\t}\n#endif\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (t->use_max_tr && !had_max_tr) {\n\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\n\tif (t->init) {\n\t\tret = tracer_init(t, tr);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\ttr->current_trace = t;\n\ttr->current_trace->enabled++;\n\ttrace_branch_enable(tr);\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_set_trace_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+1];\n\tint i;\n\tsize_t ret;\n\tint err;\n\n\tret = cnt;\n\n\tif (cnt > MAX_TRACER_SIZE)\n\t\tcnt = MAX_TRACER_SIZE;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\t/* strip ending whitespace. */\n\tfor (i = cnt - 1; i > 0 && isspace(buf[i]); i--)\n\t\tbuf[i] = 0;\n\n\terr = tracing_set_tracer(tr, buf);\n\tif (err)\n\t\treturn err;\n\n\t*ppos += ret;\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_nsecs_read(unsigned long *ptr, char __user *ubuf,\n\t\t   size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tr = snprintf(buf, sizeof(buf), \"%ld\\n\",\n\t\t     *ptr == (unsigned long)-1 ? -1 : nsecs_to_usecs(*ptr));\n\tif (r > sizeof(buf))\n\t\tr = sizeof(buf);\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_nsecs_write(unsigned long *ptr, const char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t*ptr = val * 1000;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_thresh_read(struct file *filp, char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(&tracing_thresh, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_thresh_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tint ret;\n\n\tmutex_lock(&trace_types_lock);\n\tret = tracing_nsecs_write(&tracing_thresh, ubuf, cnt, ppos);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (tr->current_trace->update_thresh) {\n\t\tret = tr->current_trace->update_thresh(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = cnt;\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\n\nstatic ssize_t\ntracing_max_lat_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(filp->private_data, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_max_lat_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_write(filp->private_data, ubuf, cnt, ppos);\n}\n\n#endif\n\nstatic int tracing_open_pipe(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret = 0;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\n\t/* create a buffer to store the information to pass to userspace */\n\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter) {\n\t\tret = -ENOMEM;\n\t\t__trace_array_put(tr);\n\t\tgoto out;\n\t}\n\n\ttrace_seq_init(&iter->seq);\n\titer->trace = tr->current_trace;\n\n\tif (!alloc_cpumask_var(&iter->started, GFP_KERNEL)) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t/* trace pipe does not show start of buffer */\n\tcpumask_setall(iter->started);\n\n\tif (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\titer->tr = tr;\n\titer->trace_buffer = &tr->trace_buffer;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\tfilp->private_data = iter;\n\n\tif (iter->trace->pipe_open)\n\t\titer->trace->pipe_open(iter);\n\n\tnonseekable_open(inode, filp);\n\n\ttr->current_trace->ref++;\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n\nfail:\n\tkfree(iter->trace);\n\tkfree(iter);\n\t__trace_array_put(tr);\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_release_pipe(struct inode *inode, struct file *file)\n{\n\tstruct trace_iterator *iter = file->private_data;\n\tstruct trace_array *tr = inode->i_private;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->current_trace->ref--;\n\n\tif (iter->trace->pipe_close)\n\t\titer->trace->pipe_close(iter);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tfree_cpumask_var(iter->started);\n\tmutex_destroy(&iter->mutex);\n\tkfree(iter);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic __poll_t\ntrace_poll(struct trace_iterator *iter, struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_array *tr = iter->tr;\n\n\t/* Iterators are static, they should be filled or empty */\n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\n\tif (tr->trace_flags & TRACE_ITER_BLOCK)\n\t\t/*\n\t\t * Always select as readable when in blocking mode\n\t\t */\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\telse\n\t\treturn ring_buffer_poll_wait(iter->trace_buffer->buffer, iter->cpu_file,\n\t\t\t\t\t     filp, poll_table);\n}\n\nstatic __poll_t\ntracing_poll_pipe(struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\n/* Must be called with iter->mutex held. */\nstatic int tracing_wait_pipe(struct file *filp)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tint ret;\n\n\twhile (trace_empty(iter)) {\n\n\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t/*\n\t\t * We block until we read something and tracing is disabled.\n\t\t * We still block if tracing is disabled, but we have never\n\t\t * read anything. This allows a user to cat this file, and\n\t\t * then enable tracing. But after we have read something,\n\t\t * we give an EOF when tracing is again disabled.\n\t\t *\n\t\t * iter->pos will be 0 if we haven't read anything.\n\t\t */\n\t\tif (!tracer_tracing_is_on(iter->tr) && iter->pos)\n\t\t\tbreak;\n\n\t\tmutex_unlock(&iter->mutex);\n\n\t\tret = wait_on_pipe(iter, false);\n\n\t\tmutex_lock(&iter->mutex);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 1;\n}\n\n/*\n * Consumer reader.\n */\nstatic ssize_t\ntracing_read_pipe(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tssize_t sret;\n\n\t/*\n\t * Avoid more than one consumer on a single file descriptor\n\t * This is just a matter of traces coherency, the ring buffer itself\n\t * is protected.\n\t */\n\tmutex_lock(&iter->mutex);\n\n\t/* return any leftover data */\n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (sret != -EBUSY)\n\t\tgoto out;\n\n\ttrace_seq_init(&iter->seq);\n\n\tif (iter->trace->read) {\n\t\tsret = iter->trace->read(iter, filp, ubuf, cnt, ppos);\n\t\tif (sret)\n\t\t\tgoto out;\n\t}\n\nwaitagain:\n\tsret = tracing_wait_pipe(filp);\n\tif (sret <= 0)\n\t\tgoto out;\n\n\t/* stop when tracing is finished */\n\tif (trace_empty(iter)) {\n\t\tsret = 0;\n\t\tgoto out;\n\t}\n\n\tif (cnt >= PAGE_SIZE)\n\t\tcnt = PAGE_SIZE - 1;\n\n\t/* reset all but tr, trace, and overruns */\n\tmemset(&iter->seq, 0,\n\t       sizeof(struct trace_iterator) -\n\t       offsetof(struct trace_iterator, seq));\n\tcpumask_clear(iter->started);\n\titer->pos = -1;\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\twhile (trace_find_next_entry_inc(iter) != NULL) {\n\t\tenum print_line_t ret;\n\t\tint save_len = iter->seq.seq.len;\n\n\t\tret = print_trace_line(iter);\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\t/* don't print partial lines */\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\n\t\tif (trace_seq_used(&iter->seq) >= cnt)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Setting the full flag means we reached the trace_seq buffer\n\t\t * size and we should leave by partial output condition above.\n\t\t * One of the trace_seq_* functions is not used properly.\n\t\t */\n\t\tWARN_ONCE(iter->seq.full, \"full flag set for trace type %d\",\n\t\t\t  iter->ent->type);\n\t}\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\n\t/* Now copy what we have to the user */\n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (iter->seq.seq.readpos >= trace_seq_used(&iter->seq))\n\t\ttrace_seq_init(&iter->seq);\n\n\t/*\n\t * If there was nothing to send to user, in spite of consuming trace\n\t * entries, go back to wait for more entries.\n\t */\n\tif (sret == -EBUSY)\n\t\tgoto waitagain;\n\nout:\n\tmutex_unlock(&iter->mutex);\n\n\treturn sret;\n}\n\nstatic void tracing_spd_release_pipe(struct splice_pipe_desc *spd,\n\t\t\t\t     unsigned int idx)\n{\n\t__free_page(spd->pages[idx]);\n}\n\nstatic const struct pipe_buf_operations tracing_pipe_buf_ops = {\n\t.can_merge\t\t= 0,\n\t.confirm\t\t= generic_pipe_buf_confirm,\n\t.release\t\t= generic_pipe_buf_release,\n\t.steal\t\t\t= generic_pipe_buf_steal,\n\t.get\t\t\t= generic_pipe_buf_get,\n};\n\nstatic size_t\ntracing_fill_pipe_page(size_t rem, struct trace_iterator *iter)\n{\n\tsize_t count;\n\tint save_len;\n\tint ret;\n\n\t/* Seq buffer is page-sized, exactly what we need. */\n\tfor (;;) {\n\t\tsave_len = iter->seq.seq.len;\n\t\tret = print_trace_line(iter);\n\n\t\tif (trace_seq_has_overflowed(&iter->seq)) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * This should not be hit, because it should only\n\t\t * be set if the iter->seq overflowed. But check it\n\t\t * anyway to be safe.\n\t\t */\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tcount = trace_seq_used(&iter->seq) - save_len;\n\t\tif (rem < count) {\n\t\t\trem = 0;\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\t\trem -= count;\n\t\tif (!trace_find_next_entry_inc(iter))\t{\n\t\t\trem = 0;\n\t\t\titer->ent = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rem;\n}\n\nstatic ssize_t tracing_splice_read_pipe(struct file *filp,\n\t\t\t\t\tloff_t *ppos,\n\t\t\t\t\tstruct pipe_inode_info *pipe,\n\t\t\t\t\tsize_t len,\n\t\t\t\t\tunsigned int flags)\n{\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct trace_iterator *iter = filp->private_data;\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages\t= 0, /* This gets updated below. */\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &tracing_pipe_buf_ops,\n\t\t.spd_release\t= tracing_spd_release_pipe,\n\t};\n\tssize_t ret;\n\tsize_t rem;\n\tunsigned int i;\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&iter->mutex);\n\n\tif (iter->trace->splice_read) {\n\t\tret = iter->trace->splice_read(iter, filp,\n\t\t\t\t\t       ppos, pipe, len, flags);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t}\n\n\tret = tracing_wait_pipe(filp);\n\tif (ret <= 0)\n\t\tgoto out_err;\n\n\tif (!iter->ent && !trace_find_next_entry_inc(iter)) {\n\t\tret = -EFAULT;\n\t\tgoto out_err;\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\n\t/* Fill as many pages as possible. */\n\tfor (i = 0, rem = len; i < spd.nr_pages_max && rem; i++) {\n\t\tspd.pages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!spd.pages[i])\n\t\t\tbreak;\n\n\t\trem = tracing_fill_pipe_page(rem, iter);\n\n\t\t/* Copy the data into the page, so we can start over. */\n\t\tret = trace_seq_to_buffer(&iter->seq,\n\t\t\t\t\t  page_address(spd.pages[i]),\n\t\t\t\t\t  trace_seq_used(&iter->seq));\n\t\tif (ret < 0) {\n\t\t\t__free_page(spd.pages[i]);\n\t\t\tbreak;\n\t\t}\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].len = trace_seq_used(&iter->seq);\n\n\t\ttrace_seq_init(&iter->seq);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\tmutex_unlock(&iter->mutex);\n\n\tspd.nr_pages = i;\n\n\tif (i)\n\t\tret = splice_to_pipe(pipe, &spd);\n\telse\n\t\tret = 0;\nout:\n\tsplice_shrink_spd(&spd);\n\treturn ret;\n\nout_err:\n\tmutex_unlock(&iter->mutex);\n\tgoto out;\n}\n\nstatic ssize_t\ntracing_entries_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tint cpu = tracing_get_cpu(inode);\n\tchar buf[64];\n\tint r = 0;\n\tssize_t ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tint cpu, buf_size_same;\n\t\tunsigned long size;\n\n\t\tsize = 0;\n\t\tbuf_size_same = 1;\n\t\t/* check if all cpu sizes are same */\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\t/* fill in the size from first enabled cpu */\n\t\t\tif (size == 0)\n\t\t\t\tsize = per_cpu_ptr(tr->trace_buffer.data, cpu)->entries;\n\t\t\tif (size != per_cpu_ptr(tr->trace_buffer.data, cpu)->entries) {\n\t\t\t\tbuf_size_same = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (buf_size_same) {\n\t\t\tif (!ring_buffer_expanded)\n\t\t\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\",\n\t\t\t\t\t    size >> 10,\n\t\t\t\t\t    trace_buf_size >> 10);\n\t\t\telse\n\t\t\t\tr = sprintf(buf, \"%lu\\n\", size >> 10);\n\t\t} else\n\t\t\tr = sprintf(buf, \"X\\n\");\n\t} else\n\t\tr = sprintf(buf, \"%lu\\n\", per_cpu_ptr(tr->trace_buffer.data, cpu)->entries >> 10);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_entries_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t/* must have at least 1 entry */\n\tif (!val)\n\t\treturn -EINVAL;\n\n\t/* value is in KB */\n\tval <<= 10;\n\tret = tracing_resize_ring_buffer(tr, val, tracing_get_cpu(inode));\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_total_entries_read(struct file *filp, char __user *ubuf,\n\t\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r, cpu;\n\tunsigned long size = 0, expanded_size = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\tsize += per_cpu_ptr(tr->trace_buffer.data, cpu)->entries >> 10;\n\t\tif (!ring_buffer_expanded)\n\t\t\texpanded_size += trace_buf_size >> 10;\n\t}\n\tif (ring_buffer_expanded)\n\t\tr = sprintf(buf, \"%lu\\n\", size);\n\telse\n\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\", size, expanded_size);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_free_buffer_write(struct file *filp, const char __user *ubuf,\n\t\t\t  size_t cnt, loff_t *ppos)\n{\n\t/*\n\t * There is no need to read what the user has written, this function\n\t * is just to make sure that there is no error when \"echo\" is used\n\t */\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int\ntracing_free_buffer_release(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\t/* disable tracing ? */\n\tif (tr->trace_flags & TRACE_ITER_STOP_ON_FREE)\n\t\ttracer_tracing_off(tr);\n\t/* resize the ring buffer to 0 */\n\ttracing_resize_ring_buffer(tr, 0, RING_BUFFER_ALL_CPUS);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_mark_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tenum event_trigger_type tt = ETT_NONE;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tconst char faulted[] = \"<faulted>\";\n\tssize_t written;\n\tint size;\n\tint len;\n\n/* Used in tracing_mark_raw_write() as well */\n#define FAULTED_SIZE (sizeof(faulted) - 1) /* '\\0' is already accounted for */\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tlocal_save_flags(irq_flags);\n\tsize = sizeof(*entry) + cnt + 2; /* add '\\0' and possible '\\n' */\n\n\t/* If less than \"<faulted>\", then make sure we can still add that */\n\tif (cnt < FAULTED_SIZE)\n\t\tsize += FAULTED_SIZE - cnt;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    irq_flags, preempt_count());\n\tif (unlikely(!event))\n\t\t/* Ring buffer disabled, return as if not open for write */\n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = _THIS_IP_;\n\n\tlen = __copy_from_user_inatomic(&entry->buf, ubuf, cnt);\n\tif (len) {\n\t\tmemcpy(&entry->buf, faulted, FAULTED_SIZE);\n\t\tcnt = FAULTED_SIZE;\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\tlen = cnt;\n\n\tif (tr->trace_marker_file && !list_empty(&tr->trace_marker_file->triggers)) {\n\t\t/* do not add \\n before testing triggers, but add \\0 */\n\t\tentry->buf[cnt] = '\\0';\n\t\ttt = event_triggers_call(tr->trace_marker_file, entry, event);\n\t}\n\n\tif (entry->buf[cnt - 1] != '\\n') {\n\t\tentry->buf[cnt] = '\\n';\n\t\tentry->buf[cnt + 1] = '\\0';\n\t} else\n\t\tentry->buf[cnt] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (tt)\n\t\tevent_triggers_post_call(tr->trace_marker_file, tt);\n\n\tif (written > 0)\n\t\t*fpos += written;\n\n\treturn written;\n}\n\n/* Limit it for now to 3K (including tag) */\n#define RAW_DATA_MAX_SIZE (1024*3)\n\nstatic ssize_t\ntracing_mark_raw_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct raw_data_entry *entry;\n\tconst char faulted[] = \"<faulted>\";\n\tunsigned long irq_flags;\n\tssize_t written;\n\tint size;\n\tint len;\n\n#define FAULT_SIZE_ID (FAULTED_SIZE + sizeof(int))\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\t/* The marker must at least have a tag id */\n\tif (cnt < sizeof(unsigned int) || cnt > RAW_DATA_MAX_SIZE)\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tlocal_save_flags(irq_flags);\n\tsize = sizeof(*entry) + cnt;\n\tif (cnt < FAULT_SIZE_ID)\n\t\tsize += FAULT_SIZE_ID - cnt;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_RAW_DATA, size,\n\t\t\t\t\t    irq_flags, preempt_count());\n\tif (!event)\n\t\t/* Ring buffer disabled, return as if not open for write */\n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\n\tlen = __copy_from_user_inatomic(&entry->id, ubuf, cnt);\n\tif (len) {\n\t\tentry->id = -1;\n\t\tmemcpy(&entry->buf, faulted, FAULTED_SIZE);\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (written > 0)\n\t\t*fpos += written;\n\n\treturn written;\n}\n\nstatic int tracing_clock_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++)\n\t\tseq_printf(m,\n\t\t\t\"%s%s%s%s\", i ? \" \" : \"\",\n\t\t\ti == tr->clock_id ? \"[\" : \"\", trace_clocks[i].name,\n\t\t\ti == tr->clock_id ? \"]\" : \"\");\n\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nint tracing_set_clock(struct trace_array *tr, const char *clockstr)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++) {\n\t\tif (strcmp(trace_clocks[i].name, clockstr) == 0)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(trace_clocks))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->clock_id = i;\n\n\tring_buffer_set_clock(tr->trace_buffer.buffer, trace_clocks[i].func);\n\n\t/*\n\t * New clock may not be consistent with the previous clock.\n\t * Reset the buffer so that it doesn't have incomparable timestamps.\n\t */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_clock(tr->max_buffer.buffer, trace_clocks[i].func);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic ssize_t tracing_clock_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t   size_t cnt, loff_t *fpos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tconst char *clockstr;\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tclockstr = strstrip(buf);\n\n\tret = tracing_set_clock(tr, clockstr);\n\tif (ret)\n\t\treturn ret;\n\n\t*fpos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_clock_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr))\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_clock_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic int tracing_time_stamp_mode_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (ring_buffer_time_stamp_abs(tr->trace_buffer.buffer))\n\t\tseq_puts(m, \"delta [absolute]\\n\");\n\telse\n\t\tseq_puts(m, \"[delta] absolute\\n\");\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int tracing_time_stamp_mode_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr))\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_time_stamp_mode_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nint tracing_set_time_stamp_abs(struct trace_array *tr, bool abs)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (abs && tr->time_stamp_abs_ref++)\n\t\tgoto out;\n\n\tif (!abs) {\n\t\tif (WARN_ON_ONCE(!tr->time_stamp_abs_ref)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (--tr->time_stamp_abs_ref)\n\t\t\tgoto out;\n\t}\n\n\tring_buffer_set_time_stamp_abs(tr->trace_buffer.buffer, abs);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_time_stamp_abs(tr->max_buffer.buffer, abs);\n#endif\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct ftrace_buffer_info {\n\tstruct trace_iterator\titer;\n\tvoid\t\t\t*spare;\n\tunsigned int\t\tspare_cpu;\n\tunsigned int\t\tread;\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic int tracing_snapshot_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tstruct seq_file *m;\n\tint ret = 0;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, true);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t} else {\n\t\t/* Writes still need the seq_file to hold the private data */\n\t\tret = -ENOMEM;\n\t\tm = kzalloc(sizeof(*m), GFP_KERNEL);\n\t\tif (!m)\n\t\t\tgoto out;\n\t\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\t\tif (!iter) {\n\t\t\tkfree(m);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 0;\n\n\t\titer->tr = tr;\n\t\titer->trace_buffer = &tr->max_buffer;\n\t\titer->cpu_file = tracing_get_cpu(inode);\n\t\tm->private = iter;\n\t\tfile->private_data = m;\n\t}\nout:\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_snapshot_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t       loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long val;\n\tint ret;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (tr->current_trace->use_max_tr) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tswitch (val) {\n\tcase 0:\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (tr->allocated_snapshot)\n\t\t\tfree_snapshot(tr);\n\t\tbreak;\n\tcase 1:\n/* Only allow per-cpu swap if the ring buffer supports it */\n#ifndef CONFIG_RING_BUFFER_ALLOW_SWAP\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tif (!tr->allocated_snapshot) {\n\t\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tlocal_irq_disable();\n\t\t/* Now, we're going to swap */\n\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\tupdate_max_tr(tr, current, smp_processor_id());\n\t\telse\n\t\t\tupdate_max_tr_single(tr, current, iter->cpu_file);\n\t\tlocal_irq_enable();\n\t\tbreak;\n\tdefault:\n\t\tif (tr->allocated_snapshot) {\n\t\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\t\ttracing_reset_online_cpus(&tr->max_buffer);\n\t\t\telse\n\t\t\t\ttracing_reset(&tr->max_buffer, iter->cpu_file);\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (ret >= 0) {\n\t\t*ppos += cnt;\n\t\tret = cnt;\n\t}\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_snapshot_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *m = file->private_data;\n\tint ret;\n\n\tret = tracing_release(inode, file);\n\n\tif (file->f_mode & FMODE_READ)\n\t\treturn ret;\n\n\t/* If write only, the seq_file is just a stub */\n\tif (m)\n\t\tkfree(m->private);\n\tkfree(m);\n\n\treturn 0;\n}\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp);\nstatic ssize_t tracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t\t\t    size_t count, loff_t *ppos);\nstatic int tracing_buffers_release(struct inode *inode, struct file *file);\nstatic ssize_t tracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t   struct pipe_inode_info *pipe, size_t len, unsigned int flags);\n\nstatic int snapshot_raw_open(struct inode *inode, struct file *filp)\n{\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tret = tracing_buffers_open(inode, filp);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tinfo = filp->private_data;\n\n\tif (info->iter.trace->use_max_tr) {\n\t\ttracing_buffers_release(inode, filp);\n\t\treturn -EBUSY;\n\t}\n\n\tinfo->iter.snapshot = true;\n\tinfo->iter.trace_buffer = &info->iter.tr->max_buffer;\n\n\treturn ret;\n}\n\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\n\nstatic const struct file_operations tracing_thresh_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_thresh_read,\n\t.write\t\t= tracing_thresh_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\nstatic const struct file_operations tracing_max_lat_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_max_lat_read,\n\t.write\t\t= tracing_max_lat_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif\n\nstatic const struct file_operations set_tracer_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_set_trace_read,\n\t.write\t\t= tracing_set_trace_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic const struct file_operations tracing_pipe_fops = {\n\t.open\t\t= tracing_open_pipe,\n\t.poll\t\t= tracing_poll_pipe,\n\t.read\t\t= tracing_read_pipe,\n\t.splice_read\t= tracing_splice_read_pipe,\n\t.release\t= tracing_release_pipe,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic const struct file_operations tracing_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_entries_read,\n\t.write\t\t= tracing_entries_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_total_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_total_entries_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_free_buffer_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_free_buffer_write,\n\t.release\t= tracing_free_buffer_release,\n};\n\nstatic const struct file_operations tracing_mark_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_mark_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_mark_raw_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_mark_raw_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations trace_clock_fops = {\n\t.open\t\t= tracing_clock_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_clock_write,\n};\n\nstatic const struct file_operations trace_time_stamp_mode_fops = {\n\t.open\t\t= tracing_time_stamp_mode_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic const struct file_operations snapshot_fops = {\n\t.open\t\t= tracing_snapshot_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_snapshot_write,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_snapshot_release,\n};\n\nstatic const struct file_operations snapshot_raw_fops = {\n\t.open\t\t= snapshot_raw_open,\n\t.read\t\t= tracing_buffers_read,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info) {\n\t\ttrace_array_put(tr);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\tinfo->iter.tr\t\t= tr;\n\tinfo->iter.cpu_file\t= tracing_get_cpu(inode);\n\tinfo->iter.trace\t= tr->current_trace;\n\tinfo->iter.trace_buffer = &tr->trace_buffer;\n\tinfo->spare\t\t= NULL;\n\t/* Force reading ring buffer for first read */\n\tinfo->read\t\t= (unsigned int)-1;\n\n\tfilp->private_data = info;\n\n\ttr->current_trace->ref++;\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = nonseekable_open(inode, filp);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic __poll_t\ntracing_buffers_poll(struct file *filp, poll_table *poll_table)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\nstatic ssize_t\ntracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tssize_t ret = 0;\n\tssize_t size;\n\n\tif (!count)\n\t\treturn 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (!info->spare) {\n\t\tinfo->spare = ring_buffer_alloc_read_page(iter->trace_buffer->buffer,\n\t\t\t\t\t\t\t  iter->cpu_file);\n\t\tif (IS_ERR(info->spare)) {\n\t\t\tret = PTR_ERR(info->spare);\n\t\t\tinfo->spare = NULL;\n\t\t} else {\n\t\t\tinfo->spare_cpu = iter->cpu_file;\n\t\t}\n\t}\n\tif (!info->spare)\n\t\treturn ret;\n\n\t/* Do we have previous read data to read? */\n\tif (info->read < PAGE_SIZE)\n\t\tgoto read;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tret = ring_buffer_read_page(iter->trace_buffer->buffer,\n\t\t\t\t    &info->spare,\n\t\t\t\t    count,\n\t\t\t\t    iter->cpu_file, 0);\n\ttrace_access_unlock(iter->cpu_file);\n\n\tif (ret < 0) {\n\t\tif (trace_empty(iter)) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tret = wait_on_pipe(iter, false);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tgoto again;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tinfo->read = 0;\n read:\n\tsize = PAGE_SIZE - info->read;\n\tif (size > count)\n\t\tsize = count;\n\n\tret = copy_to_user(ubuf, info->spare + info->read, size);\n\tif (ret == size)\n\t\treturn -EFAULT;\n\n\tsize -= ret;\n\n\t*ppos += size;\n\tinfo->read += size;\n\n\treturn size;\n}\n\nstatic int tracing_buffers_release(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\tmutex_lock(&trace_types_lock);\n\n\titer->tr->current_trace->ref--;\n\n\t__trace_array_put(iter->tr);\n\n\tif (info->spare)\n\t\tring_buffer_free_read_page(iter->trace_buffer->buffer,\n\t\t\t\t\t   info->spare_cpu, info->spare);\n\tkfree(info);\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstruct buffer_ref {\n\tstruct ring_buffer\t*buffer;\n\tvoid\t\t\t*page;\n\tint\t\t\tcpu;\n\tint\t\t\tref;\n};\n\nstatic void buffer_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tif (--ref->ref)\n\t\treturn;\n\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n\tbuf->private = 0;\n}\n\nstatic void buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tref->ref++;\n}\n\n/* Pipe buffer operations for a buffer. */\nstatic const struct pipe_buf_operations buffer_pipe_buf_ops = {\n\t.can_merge\t\t= 0,\n\t.confirm\t\t= generic_pipe_buf_confirm,\n\t.release\t\t= buffer_pipe_buf_release,\n\t.steal\t\t\t= generic_pipe_buf_steal,\n\t.get\t\t\t= buffer_pipe_buf_get,\n};\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void buffer_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tstruct buffer_ref *ref =\n\t\t(struct buffer_ref *)spd->partial[i].private;\n\n\tif (--ref->ref)\n\t\treturn;\n\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n\tspd->partial[i].private = 0;\n}\n\nstatic ssize_t\ntracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t\t    struct pipe_inode_info *pipe, size_t len,\n\t\t\t    unsigned int flags)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &buffer_pipe_buf_ops,\n\t\t.spd_release\t= buffer_spd_release,\n\t};\n\tstruct buffer_ref *ref;\n\tint entries, i;\n\tssize_t ret = 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (*ppos & (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tif (len & (PAGE_SIZE - 1)) {\n\t\tif (len < PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\t\tlen &= PAGE_MASK;\n\t}\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tentries = ring_buffer_entries_cpu(iter->trace_buffer->buffer, iter->cpu_file);\n\n\tfor (i = 0; i < spd.nr_pages_max && len && entries; i++, len -= PAGE_SIZE) {\n\t\tstruct page *page;\n\t\tint r;\n\n\t\tref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!ref) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tref->ref = 1;\n\t\tref->buffer = iter->trace_buffer->buffer;\n\t\tref->page = ring_buffer_alloc_read_page(ref->buffer, iter->cpu_file);\n\t\tif (IS_ERR(ref->page)) {\n\t\t\tret = PTR_ERR(ref->page);\n\t\t\tref->page = NULL;\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\t\tref->cpu = iter->cpu_file;\n\n\t\tr = ring_buffer_read_page(ref->buffer, &ref->page,\n\t\t\t\t\t  len, iter->cpu_file, 1);\n\t\tif (r < 0) {\n\t\t\tring_buffer_free_read_page(ref->buffer, ref->cpu,\n\t\t\t\t\t\t   ref->page);\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\n\t\tpage = virt_to_page(ref->page);\n\n\t\tspd.pages[i] = page;\n\t\tspd.partial[i].len = PAGE_SIZE;\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].private = (unsigned long)ref;\n\t\tspd.nr_pages++;\n\t\t*ppos += PAGE_SIZE;\n\n\t\tentries = ring_buffer_entries_cpu(iter->trace_buffer->buffer, iter->cpu_file);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\tspd.nr_pages = i;\n\n\t/* did we read anything? */\n\tif (!spd.nr_pages) {\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = -EAGAIN;\n\t\tif ((file->f_flags & O_NONBLOCK) || (flags & SPLICE_F_NONBLOCK))\n\t\t\tgoto out;\n\n\t\tret = wait_on_pipe(iter, true);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\n\tret = splice_to_pipe(pipe, &spd);\nout:\n\tsplice_shrink_spd(&spd);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_buffers_fops = {\n\t.open\t\t= tracing_buffers_open,\n\t.read\t\t= tracing_buffers_read,\n\t.poll\t\t= tracing_buffers_poll,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic ssize_t\ntracing_stats_read(struct file *filp, char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\tint cpu = tracing_get_cpu(inode);\n\tstruct trace_seq *s;\n\tunsigned long cnt;\n\tunsigned long long t;\n\tunsigned long usec_rem;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ttrace_seq_init(s);\n\n\tcnt = ring_buffer_entries_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"entries: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_commit_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"commit overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_bytes_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"bytes: %ld\\n\", cnt);\n\n\tif (trace_clocks[tr->clock_id].in_ns) {\n\t\t/* local or global for trace_clock */\n\t\tt = ns2usecs(ring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"oldest event ts: %5llu.%06lu\\n\",\n\t\t\t\t\t\t\t\tt, usec_rem);\n\n\t\tt = ns2usecs(ring_buffer_time_stamp(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"now ts: %5llu.%06lu\\n\", t, usec_rem);\n\t} else {\n\t\t/* counter or tsc mode for trace_clock */\n\t\ttrace_seq_printf(s, \"oldest event ts: %llu\\n\",\n\t\t\t\tring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\n\t\ttrace_seq_printf(s, \"now ts: %llu\\n\",\n\t\t\t\tring_buffer_time_stamp(trace_buf->buffer, cpu));\n\t}\n\n\tcnt = ring_buffer_dropped_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"dropped events: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_read_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"read events: %ld\\n\", cnt);\n\n\tcount = simple_read_from_buffer(ubuf, count, ppos,\n\t\t\t\t\ts->buffer, trace_seq_used(s));\n\n\tkfree(s);\n\n\treturn count;\n}\n\nstatic const struct file_operations tracing_stats_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_stats_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\nstatic ssize_t\ntracing_read_dyn_info(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long *p = filp->private_data;\n\tchar buf[64]; /* Not too big for a shallow stack */\n\tint r;\n\n\tr = scnprintf(buf, 63, \"%ld\", *p);\n\tbuf[r++] = '\\n';\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic const struct file_operations tracing_dyn_info_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_read_dyn_info,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif /* CONFIG_DYNAMIC_FTRACE */\n\n#if defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE)\nstatic void\nftrace_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\tstruct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\tvoid *data)\n{\n\ttracing_snapshot_instance(tr);\n}\n\nstatic void\nftrace_count_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\t      struct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\t      void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count) {\n\n\t\tif (*count <= 0)\n\t\t\treturn;\n\n\t\t(*count)--;\n\t}\n\n\ttracing_snapshot_instance(tr);\n}\n\nstatic int\nftrace_snapshot_print(struct seq_file *m, unsigned long ip,\n\t\t      struct ftrace_probe_ops *ops, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tseq_printf(m, \"%ps:\", (void *)ip);\n\n\tseq_puts(m, \"snapshot\");\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count)\n\t\tseq_printf(m, \":count=%ld\\n\", *count);\n\telse\n\t\tseq_puts(m, \":unlimited\\n\");\n\n\treturn 0;\n}\n\nstatic int\nftrace_snapshot_init(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *init_data, void **data)\n{\n\tstruct ftrace_func_mapper *mapper = *data;\n\n\tif (!mapper) {\n\t\tmapper = allocate_ftrace_func_mapper();\n\t\tif (!mapper)\n\t\t\treturn -ENOMEM;\n\t\t*data = mapper;\n\t}\n\n\treturn ftrace_func_mapper_add_ip(mapper, ip, init_data);\n}\n\nstatic void\nftrace_snapshot_free(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\n\tif (!ip) {\n\t\tif (!mapper)\n\t\t\treturn;\n\t\tfree_ftrace_func_mapper(mapper, NULL);\n\t\treturn;\n\t}\n\n\tftrace_func_mapper_remove_ip(mapper, ip);\n}\n\nstatic struct ftrace_probe_ops snapshot_probe_ops = {\n\t.func\t\t\t= ftrace_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n};\n\nstatic struct ftrace_probe_ops snapshot_count_probe_ops = {\n\t.func\t\t\t= ftrace_count_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n\t.init\t\t\t= ftrace_snapshot_init,\n\t.free\t\t\t= ftrace_snapshot_free,\n};\n\nstatic int\nftrace_trace_snapshot_callback(struct trace_array *tr, struct ftrace_hash *hash,\n\t\t\t       char *glob, char *cmd, char *param, int enable)\n{\n\tstruct ftrace_probe_ops *ops;\n\tvoid *count = (void *)-1;\n\tchar *number;\n\tint ret;\n\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\t/* hash funcs only work with set_ftrace_filter */\n\tif (!enable)\n\t\treturn -EINVAL;\n\n\tops = param ? &snapshot_count_probe_ops :  &snapshot_probe_ops;\n\n\tif (glob[0] == '!')\n\t\treturn unregister_ftrace_function_probe_func(glob+1, tr, ops);\n\n\tif (!param)\n\t\tgoto out_reg;\n\n\tnumber = strsep(&param, \":\");\n\n\tif (!strlen(number))\n\t\tgoto out_reg;\n\n\t/*\n\t * We use the callback data field (which is a pointer)\n\t * as our counter.\n\t */\n\tret = kstrtoul(number, 0, (unsigned long *)&count);\n\tif (ret)\n\t\treturn ret;\n\n out_reg:\n\tret = tracing_alloc_snapshot_instance(tr);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = register_ftrace_function_probe(glob, tr, ops, count);\n\n out:\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic struct ftrace_func_command ftrace_snapshot_cmd = {\n\t.name\t\t\t= \"snapshot\",\n\t.func\t\t\t= ftrace_trace_snapshot_callback,\n};\n\nstatic __init int register_snapshot_cmd(void)\n{\n\treturn register_ftrace_command(&ftrace_snapshot_cmd);\n}\n#else\nstatic inline __init int register_snapshot_cmd(void) { return 0; }\n#endif /* defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE) */\n\nstatic struct dentry *tracing_get_dentry(struct trace_array *tr)\n{\n\tif (WARN_ON(!tr->dir))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t/* Top directory uses NULL as the parent */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn NULL;\n\n\t/* All sub buffers have a descriptor */\n\treturn tr->dir;\n}\n\nstatic struct dentry *tracing_dentry_percpu(struct trace_array *tr, int cpu)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->percpu_dir)\n\t\treturn tr->percpu_dir;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->percpu_dir = tracefs_create_dir(\"per_cpu\", d_tracer);\n\n\tWARN_ONCE(!tr->percpu_dir,\n\t\t  \"Could not create tracefs directory 'per_cpu/%d'\\n\", cpu);\n\n\treturn tr->percpu_dir;\n}\n\nstatic struct dentry *\ntrace_create_cpu_file(const char *name, umode_t mode, struct dentry *parent,\n\t\t      void *data, long cpu, const struct file_operations *fops)\n{\n\tstruct dentry *ret = trace_create_file(name, mode, parent, data, fops);\n\n\tif (ret) /* See tracing_get_cpu() */\n\t\td_inode(ret)->i_cdev = (void *)(cpu + 1);\n\treturn ret;\n}\n\nstatic void\ntracing_init_tracefs_percpu(struct trace_array *tr, long cpu)\n{\n\tstruct dentry *d_percpu = tracing_dentry_percpu(tr, cpu);\n\tstruct dentry *d_cpu;\n\tchar cpu_dir[30]; /* 30 characters should be more than enough */\n\n\tif (!d_percpu)\n\t\treturn;\n\n\tsnprintf(cpu_dir, 30, \"cpu%ld\", cpu);\n\td_cpu = tracefs_create_dir(cpu_dir, d_percpu);\n\tif (!d_cpu) {\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", cpu_dir);\n\t\treturn;\n\t}\n\n\t/* per cpu trace_pipe */\n\ttrace_create_cpu_file(\"trace_pipe\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_pipe_fops);\n\n\t/* per cpu trace */\n\ttrace_create_cpu_file(\"trace\", 0644, d_cpu,\n\t\t\t\ttr, cpu, &tracing_fops);\n\n\ttrace_create_cpu_file(\"trace_pipe_raw\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_buffers_fops);\n\n\ttrace_create_cpu_file(\"stats\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_stats_fops);\n\n\ttrace_create_cpu_file(\"buffer_size_kb\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_entries_fops);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_cpu_file(\"snapshot\", 0644, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_fops);\n\n\ttrace_create_cpu_file(\"snapshot_raw\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_raw_fops);\n#endif\n}\n\n#ifdef CONFIG_FTRACE_SELFTEST\n/* Let selftest have access to static functions in this file */\n#include \"trace_selftest.c\"\n#endif\n\nstatic ssize_t\ntrace_options_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tchar *buf;\n\n\tif (topt->flags->val & topt->opt->bit)\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tif (!!(topt->flags->val & topt->opt->bit) != val) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tret = __set_tracer_option(topt->tr, topt->flags,\n\t\t\t\t\t  topt->opt, !val);\n\t\tmutex_unlock(&trace_types_lock);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\n\nstatic const struct file_operations trace_options_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_read,\n\t.write = trace_options_write,\n\t.llseek\t= generic_file_llseek,\n};\n\n/*\n * In order to pass in both the trace_array descriptor as well as the index\n * to the flag that the trace option file represents, the trace_array\n * has a character array of trace_flags_index[], which holds the index\n * of the bit for the flag it represents. index[0] == 0, index[1] == 1, etc.\n * The address of this character array is passed to the flag option file\n * read/write callbacks.\n *\n * In order to extract both the index and the trace_array descriptor,\n * get_tr_index() uses the following algorithm.\n *\n *   idx = *ptr;\n *\n * As the pointer itself contains the address of the index (remember\n * index[1] == 1).\n *\n * Then to get the trace_array descriptor, by subtracting that index\n * from the ptr, we get to the start of the index itself.\n *\n *   ptr - idx == &index[0]\n *\n * Then a simple container_of() from that pointer gets us to the\n * trace_array descriptor.\n */\nstatic void get_tr_index(void *data, struct trace_array **ptr,\n\t\t\t unsigned int *pindex)\n{\n\t*pindex = *(unsigned char *)data;\n\n\t*ptr = container_of(data - *pindex, struct trace_array,\n\t\t\t    trace_flags_index);\n}\n\nstatic ssize_t\ntrace_options_core_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tchar *buf;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tif (tr->trace_flags & (1 << index))\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_core_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tunsigned long val;\n\tint ret;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\tret = set_tracer_flag(tr, 1 << index, val);\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations trace_options_core_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_core_read,\n\t.write = trace_options_core_write,\n\t.llseek = generic_file_llseek,\n};\n\nstruct dentry *trace_create_file(const char *name,\n\t\t\t\t umode_t mode,\n\t\t\t\t struct dentry *parent,\n\t\t\t\t void *data,\n\t\t\t\t const struct file_operations *fops)\n{\n\tstruct dentry *ret;\n\n\tret = tracefs_create_file(name, mode, parent, data, fops);\n\tif (!ret)\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", name);\n\n\treturn ret;\n}\n\n\nstatic struct dentry *trace_options_init_dentry(struct trace_array *tr)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->options)\n\t\treturn tr->options;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->options = tracefs_create_dir(\"options\", d_tracer);\n\tif (!tr->options) {\n\t\tpr_warn(\"Could not create tracefs directory 'options'\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn tr->options;\n}\n\nstatic void\ncreate_trace_option_file(struct trace_array *tr,\n\t\t\t struct trace_option_dentry *topt,\n\t\t\t struct tracer_flags *flags,\n\t\t\t struct tracer_opt *opt)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\ttopt->flags = flags;\n\ttopt->opt = opt;\n\ttopt->tr = tr;\n\n\ttopt->entry = trace_create_file(opt->name, 0644, t_options, topt,\n\t\t\t\t    &trace_options_fops);\n\n}\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer)\n{\n\tstruct trace_option_dentry *topts;\n\tstruct trace_options *tr_topts;\n\tstruct tracer_flags *flags;\n\tstruct tracer_opt *opts;\n\tint cnt;\n\tint i;\n\n\tif (!tracer)\n\t\treturn;\n\n\tflags = tracer->flags;\n\n\tif (!flags || !flags->opts)\n\t\treturn;\n\n\t/*\n\t * If this is an instance, only create flags for tracers\n\t * the instance may have.\n\t */\n\tif (!trace_ok_for_array(tracer, tr))\n\t\treturn;\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\t/* Make sure there's no duplicate flags. */\n\t\tif (WARN_ON_ONCE(tr->topts[i].tracer->flags == tracer->flags))\n\t\t\treturn;\n\t}\n\n\topts = flags->opts;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++)\n\t\t;\n\n\ttopts = kcalloc(cnt + 1, sizeof(*topts), GFP_KERNEL);\n\tif (!topts)\n\t\treturn;\n\n\ttr_topts = krealloc(tr->topts, sizeof(*tr->topts) * (tr->nr_topts + 1),\n\t\t\t    GFP_KERNEL);\n\tif (!tr_topts) {\n\t\tkfree(topts);\n\t\treturn;\n\t}\n\n\ttr->topts = tr_topts;\n\ttr->topts[tr->nr_topts].tracer = tracer;\n\ttr->topts[tr->nr_topts].topts = topts;\n\ttr->nr_topts++;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++) {\n\t\tcreate_trace_option_file(tr, &topts[cnt], flags,\n\t\t\t\t\t &opts[cnt]);\n\t\tWARN_ONCE(topts[cnt].entry == NULL,\n\t\t\t  \"Failed to create trace option: %s\",\n\t\t\t  opts[cnt].name);\n\t}\n}\n\nstatic struct dentry *\ncreate_trace_option_core_file(struct trace_array *tr,\n\t\t\t      const char *option, long index)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn NULL;\n\n\treturn trace_create_file(option, 0644, t_options,\n\t\t\t\t (void *)&tr->trace_flags_index[index],\n\t\t\t\t &trace_options_core_fops);\n}\n\nstatic void create_trace_options_dir(struct trace_array *tr)\n{\n\tstruct dentry *t_options;\n\tbool top_level = tr == &global_trace;\n\tint i;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (top_level ||\n\t\t    !((1 << i) & TOP_LEVEL_TRACE_FLAGS))\n\t\t\tcreate_trace_option_core_file(tr, trace_options[i], i);\n\t}\n}\n\nstatic ssize_t\nrb_simple_read(struct file *filp, char __user *ubuf,\n\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r;\n\n\tr = tracer_tracing_is_on(tr);\n\tr = sprintf(buf, \"%d\\n\", r);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\nrb_simple_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (buffer) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tif (val) {\n\t\t\ttracer_tracing_on(tr);\n\t\t\tif (tr->current_trace->start)\n\t\t\t\ttr->current_trace->start(tr);\n\t\t} else {\n\t\t\ttracer_tracing_off(tr);\n\t\t\tif (tr->current_trace->stop)\n\t\t\t\ttr->current_trace->stop(tr);\n\t\t}\n\t\tmutex_unlock(&trace_types_lock);\n\t}\n\n\t(*ppos)++;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations rb_simple_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= rb_simple_read,\n\t.write\t\t= rb_simple_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= default_llseek,\n};\n\nstruct dentry *trace_instance_dir;\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer);\n\nstatic int\nallocate_trace_buffer(struct trace_array *tr, struct trace_buffer *buf, int size)\n{\n\tenum ring_buffer_flags rb_flags;\n\n\trb_flags = tr->trace_flags & TRACE_ITER_OVERWRITE ? RB_FL_OVERWRITE : 0;\n\n\tbuf->tr = tr;\n\n\tbuf->buffer = ring_buffer_alloc(size, rb_flags);\n\tif (!buf->buffer)\n\t\treturn -ENOMEM;\n\n\tbuf->data = alloc_percpu(struct trace_array_cpu);\n\tif (!buf->data) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Allocate the first page for all buffers */\n\tset_buffer_entries(&tr->trace_buffer,\n\t\t\t   ring_buffer_size(tr->trace_buffer.buffer, 0));\n\n\treturn 0;\n}\n\nstatic int allocate_trace_buffers(struct trace_array *tr, int size)\n{\n\tint ret;\n\n\tret = allocate_trace_buffer(tr, &tr->trace_buffer, size);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tret = allocate_trace_buffer(tr, &tr->max_buffer,\n\t\t\t\t    allocate_snapshot ? size : 1);\n\tif (WARN_ON(ret)) {\n\t\tring_buffer_free(tr->trace_buffer.buffer);\n\t\ttr->trace_buffer.buffer = NULL;\n\t\tfree_percpu(tr->trace_buffer.data);\n\t\ttr->trace_buffer.data = NULL;\n\t\treturn -ENOMEM;\n\t}\n\ttr->allocated_snapshot = allocate_snapshot;\n\n\t/*\n\t * Only the top level trace array gets its snapshot allocated\n\t * from the kernel command line.\n\t */\n\tallocate_snapshot = false;\n#endif\n\treturn 0;\n}\n\nstatic void free_trace_buffer(struct trace_buffer *buf)\n{\n\tif (buf->buffer) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\tfree_percpu(buf->data);\n\t\tbuf->data = NULL;\n\t}\n}\n\nstatic void free_trace_buffers(struct trace_array *tr)\n{\n\tif (!tr)\n\t\treturn;\n\n\tfree_trace_buffer(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tfree_trace_buffer(&tr->max_buffer);\n#endif\n}\n\nstatic void init_trace_flags_index(struct trace_array *tr)\n{\n\tint i;\n\n\t/* Used by the trace options files */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++)\n\t\ttr->trace_flags_index[i] = i;\n}\n\nstatic void __update_tracer_options(struct trace_array *tr)\n{\n\tstruct tracer *t;\n\n\tfor (t = trace_types; t; t = t->next)\n\t\tadd_tracer_options(tr, t);\n}\n\nstatic void update_tracer_options(struct trace_array *tr)\n{\n\tmutex_lock(&trace_types_lock);\n\t__update_tracer_options(tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int instance_mkdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -EEXIST;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\tret = -ENOMEM;\n\ttr = kzalloc(sizeof(*tr), GFP_KERNEL);\n\tif (!tr)\n\t\tgoto out_unlock;\n\n\ttr->name = kstrdup(name, GFP_KERNEL);\n\tif (!tr->name)\n\t\tgoto out_free_tr;\n\n\tif (!alloc_cpumask_var(&tr->tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_tr;\n\n\ttr->trace_flags = global_trace.trace_flags & ~ZEROED_TRACE_FLAGS;\n\n\tcpumask_copy(tr->tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&tr->start_lock);\n\n\ttr->max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\ttr->current_trace = &nop_trace;\n\n\tINIT_LIST_HEAD(&tr->systems);\n\tINIT_LIST_HEAD(&tr->events);\n\tINIT_LIST_HEAD(&tr->hist_vars);\n\n\tif (allocate_trace_buffers(tr, trace_buf_size) < 0)\n\t\tgoto out_free_tr;\n\n\ttr->dir = tracefs_create_dir(name, trace_instance_dir);\n\tif (!tr->dir)\n\t\tgoto out_free_tr;\n\n\tret = event_trace_add_tracer(tr->dir, tr);\n\tif (ret) {\n\t\ttracefs_remove_recursive(tr->dir);\n\t\tgoto out_free_tr;\n\t}\n\n\tftrace_init_trace_array(tr);\n\n\tinit_tracer_tracefs(tr, tr->dir);\n\tinit_trace_flags_index(tr);\n\t__update_tracer_options(tr);\n\n\tlist_add(&tr->list, &ftrace_trace_arrays);\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn 0;\n\n out_free_tr:\n\tfree_trace_buffers(tr);\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n\n}\n\nstatic int instance_rmdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint found = 0;\n\tint ret;\n\tint i;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -ENODEV;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\tgoto out_unlock;\n\n\tret = -EBUSY;\n\tif (tr->ref || (tr->current_trace && tr->current_trace->ref))\n\t\tgoto out_unlock;\n\n\tlist_del(&tr->list);\n\n\t/* Disable all the flags that were enabled coming in */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++) {\n\t\tif ((1 << i) & ZEROED_TRACE_FLAGS)\n\t\t\tset_tracer_flag(tr, 1 << i, 0);\n\t}\n\n\ttracing_set_nop(tr);\n\tclear_ftrace_function_probes(tr);\n\tevent_trace_del_tracer(tr);\n\tftrace_clear_pids(tr);\n\tftrace_destroy_function_files(tr);\n\ttracefs_remove_recursive(tr->dir);\n\tfree_trace_buffers(tr);\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\tkfree(tr->topts[i].topts);\n\t}\n\tkfree(tr->topts);\n\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n\tret = 0;\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\nstatic __init void create_trace_instances(struct dentry *d_tracer)\n{\n\ttrace_instance_dir = tracefs_create_instance_dir(\"instances\", d_tracer,\n\t\t\t\t\t\t\t instance_mkdir,\n\t\t\t\t\t\t\t instance_rmdir);\n\tif (WARN_ON(!trace_instance_dir))\n\t\treturn;\n}\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer)\n{\n\tstruct trace_event_file *file;\n\tint cpu;\n\n\ttrace_create_file(\"available_tracers\", 0444, d_tracer,\n\t\t\ttr, &show_traces_fops);\n\n\ttrace_create_file(\"current_tracer\", 0644, d_tracer,\n\t\t\ttr, &set_tracer_fops);\n\n\ttrace_create_file(\"tracing_cpumask\", 0644, d_tracer,\n\t\t\t  tr, &tracing_cpumask_fops);\n\n\ttrace_create_file(\"trace_options\", 0644, d_tracer,\n\t\t\t  tr, &tracing_iter_fops);\n\n\ttrace_create_file(\"trace\", 0644, d_tracer,\n\t\t\t  tr, &tracing_fops);\n\n\ttrace_create_file(\"trace_pipe\", 0444, d_tracer,\n\t\t\t  tr, &tracing_pipe_fops);\n\n\ttrace_create_file(\"buffer_size_kb\", 0644, d_tracer,\n\t\t\t  tr, &tracing_entries_fops);\n\n\ttrace_create_file(\"buffer_total_size_kb\", 0444, d_tracer,\n\t\t\t  tr, &tracing_total_entries_fops);\n\n\ttrace_create_file(\"free_buffer\", 0200, d_tracer,\n\t\t\t  tr, &tracing_free_buffer_fops);\n\n\ttrace_create_file(\"trace_marker\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_fops);\n\n\tfile = __find_event_file(tr, \"ftrace\", \"print\");\n\tif (file && file->dir)\n\t\ttrace_create_file(\"trigger\", 0644, file->dir, file,\n\t\t\t\t  &event_trigger_fops);\n\ttr->trace_marker_file = file;\n\n\ttrace_create_file(\"trace_marker_raw\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_raw_fops);\n\n\ttrace_create_file(\"trace_clock\", 0644, d_tracer, tr,\n\t\t\t  &trace_clock_fops);\n\n\ttrace_create_file(\"tracing_on\", 0644, d_tracer,\n\t\t\t  tr, &rb_simple_fops);\n\n\ttrace_create_file(\"timestamp_mode\", 0444, d_tracer, tr,\n\t\t\t  &trace_time_stamp_mode_fops);\n\n\tcreate_trace_options_dir(tr);\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\n\ttrace_create_file(\"tracing_max_latency\", 0644, d_tracer,\n\t\t\t&tr->max_latency, &tracing_max_lat_fops);\n#endif\n\n\tif (ftrace_create_function_files(tr, d_tracer))\n\t\tWARN(1, \"Could not allocate function filter files\");\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_file(\"snapshot\", 0644, d_tracer,\n\t\t\t  tr, &snapshot_fops);\n#endif\n\n\tfor_each_tracing_cpu(cpu)\n\t\ttracing_init_tracefs_percpu(tr, cpu);\n\n\tftrace_init_tracefs(tr, d_tracer);\n}\n\nstatic struct vfsmount *trace_automount(struct dentry *mntpt, void *ingore)\n{\n\tstruct vfsmount *mnt;\n\tstruct file_system_type *type;\n\n\t/*\n\t * To maintain backward compatibility for tools that mount\n\t * debugfs to get to the tracing facility, tracefs is automatically\n\t * mounted to the debugfs/tracing directory.\n\t */\n\ttype = get_fs_type(\"tracefs\");\n\tif (!type)\n\t\treturn NULL;\n\tmnt = vfs_submount(mntpt, type, \"tracefs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn NULL;\n\tmntget(mnt);\n\n\treturn mnt;\n}\n\n/**\n * tracing_init_dentry - initialize top level trace array\n *\n * This is called when creating files or directories in the tracing\n * directory. It is called via fs_initcall() by any of the boot up code\n * and expects to return the dentry of the top level tracing directory.\n */\nstruct dentry *tracing_init_dentry(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\t/* The top level trace array uses  NULL as parent */\n\tif (tr->dir)\n\t\treturn NULL;\n\n\tif (WARN_ON(!tracefs_initialized()) ||\n\t\t(IS_ENABLED(CONFIG_DEBUG_FS) &&\n\t\t WARN_ON(!debugfs_initialized())))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t/*\n\t * As there may still be users that expect the tracing\n\t * files to exist in debugfs/tracing, we must automount\n\t * the tracefs file system there, so older tools still\n\t * work with the newer kerenl.\n\t */\n\ttr->dir = debugfs_create_automount(\"tracing\", NULL,\n\t\t\t\t\t   trace_automount, NULL);\n\tif (!tr->dir) {\n\t\tpr_warn_once(\"Could not create debugfs directory 'tracing'\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn NULL;\n}\n\nextern struct trace_eval_map *__start_ftrace_eval_maps[];\nextern struct trace_eval_map *__stop_ftrace_eval_maps[];\n\nstatic void __init trace_eval_init(void)\n{\n\tint len;\n\n\tlen = __stop_ftrace_eval_maps - __start_ftrace_eval_maps;\n\ttrace_insert_eval_map(NULL, __start_ftrace_eval_maps, len);\n}\n\n#ifdef CONFIG_MODULES\nstatic void trace_module_add_evals(struct module *mod)\n{\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\t/*\n\t * Modules with bad taint do not have events created, do\n\t * not bother with enums either.\n\t */\n\tif (trace_module_has_bad_taint(mod))\n\t\treturn;\n\n\ttrace_insert_eval_map(mod, mod->trace_evals, mod->num_trace_evals);\n}\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic void trace_module_remove_evals(struct module *mod)\n{\n\tunion trace_eval_map_item *map;\n\tunion trace_eval_map_item **last = &trace_eval_maps;\n\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tmap = trace_eval_maps;\n\n\twhile (map) {\n\t\tif (map->head.mod == mod)\n\t\t\tbreak;\n\t\tmap = trace_eval_jmp_to_tail(map);\n\t\tlast = &map->tail.next;\n\t\tmap = map->tail.next;\n\t}\n\tif (!map)\n\t\tgoto out;\n\n\t*last = trace_eval_jmp_to_tail(map)->tail.next;\n\tkfree(map);\n out:\n\tmutex_unlock(&trace_eval_mutex);\n}\n#else\nstatic inline void trace_module_remove_evals(struct module *mod) { }\n#endif /* CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic int trace_module_notify(struct notifier_block *self,\n\t\t\t       unsigned long val, void *data)\n{\n\tstruct module *mod = data;\n\n\tswitch (val) {\n\tcase MODULE_STATE_COMING:\n\t\ttrace_module_add_evals(mod);\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\ttrace_module_remove_evals(mod);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic struct notifier_block trace_module_nb = {\n\t.notifier_call = trace_module_notify,\n\t.priority = 0,\n};\n#endif /* CONFIG_MODULES */\n\nstatic __init int tracer_init_tracefs(void)\n{\n\tstruct dentry *d_tracer;\n\n\ttrace_access_lock_init();\n\n\td_tracer = tracing_init_dentry();\n\tif (IS_ERR(d_tracer))\n\t\treturn 0;\n\n\tevent_trace_init();\n\n\tinit_tracer_tracefs(&global_trace, d_tracer);\n\tftrace_init_tracefs_toplevel(&global_trace, d_tracer);\n\n\ttrace_create_file(\"tracing_thresh\", 0644, d_tracer,\n\t\t\t&global_trace, &tracing_thresh_fops);\n\n\ttrace_create_file(\"README\", 0444, d_tracer,\n\t\t\tNULL, &tracing_readme_fops);\n\n\ttrace_create_file(\"saved_cmdlines\", 0444, d_tracer,\n\t\t\tNULL, &tracing_saved_cmdlines_fops);\n\n\ttrace_create_file(\"saved_cmdlines_size\", 0644, d_tracer,\n\t\t\t  NULL, &tracing_saved_cmdlines_size_fops);\n\n\ttrace_create_file(\"saved_tgids\", 0444, d_tracer,\n\t\t\tNULL, &tracing_saved_tgids_fops);\n\n\ttrace_eval_init();\n\n\ttrace_create_eval_file(d_tracer);\n\n#ifdef CONFIG_MODULES\n\tregister_module_notifier(&trace_module_nb);\n#endif\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\ttrace_create_file(\"dyn_ftrace_total_info\", 0444, d_tracer,\n\t\t\t&ftrace_update_tot_cnt, &tracing_dyn_info_fops);\n#endif\n\n\tcreate_trace_instances(d_tracer);\n\n\tupdate_tracer_options(&global_trace);\n\n\treturn 0;\n}\n\nstatic int trace_panic_handler(struct notifier_block *this,\n\t\t\t       unsigned long event, void *unused)\n{\n\tif (ftrace_dump_on_oops)\n\t\tftrace_dump(ftrace_dump_on_oops);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_panic_notifier = {\n\t.notifier_call  = trace_panic_handler,\n\t.next           = NULL,\n\t.priority       = 150   /* priority: INT_MAX >= x >= 0 */\n};\n\nstatic int trace_die_handler(struct notifier_block *self,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tswitch (val) {\n\tcase DIE_OOPS:\n\t\tif (ftrace_dump_on_oops)\n\t\t\tftrace_dump(ftrace_dump_on_oops);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_die_notifier = {\n\t.notifier_call = trace_die_handler,\n\t.priority = 200\n};\n\n/*\n * printk is set to max of 1024, we really don't need it that big.\n * Nothing should be printing 1000 characters anyway.\n */\n#define TRACE_MAX_PRINT\t\t1000\n\n/*\n * Define here KERN_TRACE so that we have one place to modify\n * it if we decide to change what log level the ftrace dump\n * should be at.\n */\n#define KERN_TRACE\t\tKERN_EMERG\n\nvoid\ntrace_printk_seq(struct trace_seq *s)\n{\n\t/* Probably should print a warning here. */\n\tif (s->seq.len >= TRACE_MAX_PRINT)\n\t\ts->seq.len = TRACE_MAX_PRINT;\n\n\t/*\n\t * More paranoid code. Although the buffer size is set to\n\t * PAGE_SIZE, and TRACE_MAX_PRINT is 1000, this is just\n\t * an extra layer of protection.\n\t */\n\tif (WARN_ON_ONCE(s->seq.len >= s->seq.size))\n\t\ts->seq.len = s->seq.size - 1;\n\n\t/* should be zero ended, but we are paranoid. */\n\ts->buffer[s->seq.len] = 0;\n\n\tprintk(KERN_TRACE \"%s\", s->buffer);\n\n\ttrace_seq_init(s);\n}\n\nvoid trace_init_global_iter(struct trace_iterator *iter)\n{\n\titer->tr = &global_trace;\n\titer->trace = iter->tr->current_trace;\n\titer->cpu_file = RING_BUFFER_ALL_CPUS;\n\titer->trace_buffer = &global_trace.trace_buffer;\n\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t/* Annotate start of buffers if we had overruns */\n\tif (ring_buffer_overruns(iter->trace_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[iter->tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n}\n\nvoid ftrace_dump(enum ftrace_dump_mode oops_dump_mode)\n{\n\t/* use static because iter can be a bit big for the stack */\n\tstatic struct trace_iterator iter;\n\tstatic atomic_t dump_running;\n\tstruct trace_array *tr = &global_trace;\n\tunsigned int old_userobj;\n\tunsigned long flags;\n\tint cnt = 0, cpu;\n\n\t/* Only allow one dump user at a time. */\n\tif (atomic_inc_return(&dump_running) != 1) {\n\t\tatomic_dec(&dump_running);\n\t\treturn;\n\t}\n\n\t/*\n\t * Always turn off tracing when we dump.\n\t * We don't need to show trace output of what happens\n\t * between multiple crashes.\n\t *\n\t * If the user does a sysrq-z, then they can re-enable\n\t * tracing with echo 1 > tracing_on.\n\t */\n\ttracing_off();\n\n\tlocal_irq_save(flags);\n\n\t/* Simulate the iterator */\n\ttrace_init_global_iter(&iter);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_inc(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);\n\t}\n\n\told_userobj = tr->trace_flags & TRACE_ITER_SYM_USEROBJ;\n\n\t/* don't look at user memory in panic mode */\n\ttr->trace_flags &= ~TRACE_ITER_SYM_USEROBJ;\n\n\tswitch (oops_dump_mode) {\n\tcase DUMP_ALL:\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t\tbreak;\n\tcase DUMP_ORIG:\n\t\titer.cpu_file = raw_smp_processor_id();\n\t\tbreak;\n\tcase DUMP_NONE:\n\t\tgoto out_enable;\n\tdefault:\n\t\tprintk(KERN_TRACE \"Bad dumping mode, switching to all CPUs dump\\n\");\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t}\n\n\tprintk(KERN_TRACE \"Dumping ftrace buffer:\\n\");\n\n\t/* Did function tracer already get disabled? */\n\tif (ftrace_is_dead()) {\n\t\tprintk(\"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\");\n\t\tprintk(\"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n\t}\n\n\t/*\n\t * We need to stop all tracing on all CPUS to read the\n\t * the next buffer. This is a bit expensive, but is\n\t * not done often. We fill all what we can read,\n\t * and then release the locks again.\n\t */\n\n\twhile (!trace_empty(&iter)) {\n\n\t\tif (!cnt)\n\t\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n\t\tcnt++;\n\n\t\t/* reset all but tr, trace, and overruns */\n\t\tmemset(&iter.seq, 0,\n\t\t       sizeof(struct trace_iterator) -\n\t\t       offsetof(struct trace_iterator, seq));\n\t\titer.iter_flags |= TRACE_FILE_LAT_FMT;\n\t\titer.pos = -1;\n\n\t\tif (trace_find_next_entry_inc(&iter) != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = print_trace_line(&iter);\n\t\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\t\ttrace_consume(&iter);\n\t\t}\n\t\ttouch_nmi_watchdog();\n\n\t\ttrace_printk_seq(&iter.seq);\n\t}\n\n\tif (!cnt)\n\t\tprintk(KERN_TRACE \"   (ftrace buffer empty)\\n\");\n\telse\n\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n out_enable:\n\ttr->trace_flags |= old_userobj;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_dec(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);\n\t}\n \tatomic_dec(&dump_running);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(ftrace_dump);\n\nint trace_run_command(const char *buf, int (*createfn)(int, char **))\n{\n\tchar **argv;\n\tint argc, ret;\n\n\targc = 0;\n\tret = 0;\n\targv = argv_split(GFP_KERNEL, buf, &argc);\n\tif (!argv)\n\t\treturn -ENOMEM;\n\n\tif (argc)\n\t\tret = createfn(argc, argv);\n\n\targv_free(argv);\n\n\treturn ret;\n}\n\n#define WRITE_BUFSIZE  4096\n\nssize_t trace_parse_run_command(struct file *file, const char __user *buffer,\n\t\t\t\tsize_t count, loff_t *ppos,\n\t\t\t\tint (*createfn)(int, char **))\n{\n\tchar *kbuf, *buf, *tmp;\n\tint ret = 0;\n\tsize_t done = 0;\n\tsize_t size;\n\n\tkbuf = kmalloc(WRITE_BUFSIZE, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\twhile (done < count) {\n\t\tsize = count - done;\n\n\t\tif (size >= WRITE_BUFSIZE)\n\t\t\tsize = WRITE_BUFSIZE - 1;\n\n\t\tif (copy_from_user(kbuf, buffer + done, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkbuf[size] = '\\0';\n\t\tbuf = kbuf;\n\t\tdo {\n\t\t\ttmp = strchr(buf, '\\n');\n\t\t\tif (tmp) {\n\t\t\t\t*tmp = '\\0';\n\t\t\t\tsize = tmp - buf + 1;\n\t\t\t} else {\n\t\t\t\tsize = strlen(buf);\n\t\t\t\tif (done + size < count) {\n\t\t\t\t\tif (buf != kbuf)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t/* This can accept WRITE_BUFSIZE - 2 ('\\n' + '\\0') */\n\t\t\t\t\tpr_warn(\"Line length is too long: Should be less than %d\\n\",\n\t\t\t\t\t\tWRITE_BUFSIZE - 2);\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tdone += size;\n\n\t\t\t/* Remove comments */\n\t\t\ttmp = strchr(buf, '#');\n\n\t\t\tif (tmp)\n\t\t\t\t*tmp = '\\0';\n\n\t\t\tret = trace_run_command(buf, createfn);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbuf += size;\n\n\t\t} while (done < count);\n\t}\n\tret = done;\n\nout:\n\tkfree(kbuf);\n\n\treturn ret;\n}\n\n__init static int tracer_alloc_buffers(void)\n{\n\tint ring_buf_size;\n\tint ret = -ENOMEM;\n\n\t/*\n\t * Make sure we don't accidently add more trace options\n\t * than we have bits for.\n\t */\n\tBUILD_BUG_ON(TRACE_ITER_LAST_BIT > TRACE_FLAGS_MAX_SIZE);\n\n\tif (!alloc_cpumask_var(&tracing_buffer_mask, GFP_KERNEL))\n\t\tgoto out;\n\n\tif (!alloc_cpumask_var(&global_trace.tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_buffer_mask;\n\n\t/* Only allocate trace_printk buffers if a trace_printk exists */\n\tif (__stop___trace_bprintk_fmt != __start___trace_bprintk_fmt)\n\t\t/* Must be called before global_trace.buffer is allocated */\n\t\ttrace_printk_init_buffers();\n\n\t/* To save memory, keep the ring buffer size to its minimum */\n\tif (ring_buffer_expanded)\n\t\tring_buf_size = trace_buf_size;\n\telse\n\t\tring_buf_size = 1;\n\n\tcpumask_copy(tracing_buffer_mask, cpu_possible_mask);\n\tcpumask_copy(global_trace.tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&global_trace.start_lock);\n\n\t/*\n\t * The prepare callbacks allocates some memory for the ring buffer. We\n\t * don't free the buffer if the if the CPU goes down. If we were to free\n\t * the buffer, then the user would lose any trace that was in the\n\t * buffer. The memory will be removed once the \"instance\" is removed.\n\t */\n\tret = cpuhp_setup_state_multi(CPUHP_TRACE_RB_PREPARE,\n\t\t\t\t      \"trace/RB:preapre\", trace_rb_cpu_prepare,\n\t\t\t\t      NULL);\n\tif (ret < 0)\n\t\tgoto out_free_cpumask;\n\t/* Used for event triggers */\n\tret = -ENOMEM;\n\ttemp_buffer = ring_buffer_alloc(PAGE_SIZE, RB_FL_OVERWRITE);\n\tif (!temp_buffer)\n\t\tgoto out_rm_hp_state;\n\n\tif (trace_create_savedcmd() < 0)\n\t\tgoto out_free_temp_buffer;\n\n\t/* TODO: make the number of buffers hot pluggable with CPUS */\n\tif (allocate_trace_buffers(&global_trace, ring_buf_size) < 0) {\n\t\tprintk(KERN_ERR \"tracer: failed to allocate ring buffer!\\n\");\n\t\tWARN_ON(1);\n\t\tgoto out_free_savedcmd;\n\t}\n\n\tif (global_trace.buffer_disabled)\n\t\ttracing_off();\n\n\tif (trace_boot_clock) {\n\t\tret = tracing_set_clock(&global_trace, trace_boot_clock);\n\t\tif (ret < 0)\n\t\t\tpr_warn(\"Trace clock %s not defined, going back to default\\n\",\n\t\t\t\ttrace_boot_clock);\n\t}\n\n\t/*\n\t * register_tracer() might reference current_trace, so it\n\t * needs to be set before we register anything. This is\n\t * just a bootstrap of current_trace anyway.\n\t */\n\tglobal_trace.current_trace = &nop_trace;\n\n\tglobal_trace.max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\tftrace_init_global_array_ops(&global_trace);\n\n\tinit_trace_flags_index(&global_trace);\n\n\tregister_tracer(&nop_trace);\n\n\t/* Function tracing may start here (via kernel command line) */\n\tinit_function_trace();\n\n\t/* All seems OK, enable tracing */\n\ttracing_disabled = 0;\n\n\tatomic_notifier_chain_register(&panic_notifier_list,\n\t\t\t\t       &trace_panic_notifier);\n\n\tregister_die_notifier(&trace_die_notifier);\n\n\tglobal_trace.flags = TRACE_ARRAY_FL_GLOBAL;\n\n\tINIT_LIST_HEAD(&global_trace.systems);\n\tINIT_LIST_HEAD(&global_trace.events);\n\tINIT_LIST_HEAD(&global_trace.hist_vars);\n\tlist_add(&global_trace.list, &ftrace_trace_arrays);\n\n\tapply_trace_boot_options();\n\n\tregister_snapshot_cmd();\n\n\treturn 0;\n\nout_free_savedcmd:\n\tfree_saved_cmdlines_buffer(savedcmd);\nout_free_temp_buffer:\n\tring_buffer_free(temp_buffer);\nout_rm_hp_state:\n\tcpuhp_remove_multi_state(CPUHP_TRACE_RB_PREPARE);\nout_free_cpumask:\n\tfree_cpumask_var(global_trace.tracing_cpumask);\nout_free_buffer_mask:\n\tfree_cpumask_var(tracing_buffer_mask);\nout:\n\treturn ret;\n}\n\nvoid __init early_trace_init(void)\n{\n\tif (tracepoint_printk) {\n\t\ttracepoint_print_iter =\n\t\t\tkmalloc(sizeof(*tracepoint_print_iter), GFP_KERNEL);\n\t\tif (WARN_ON(!tracepoint_print_iter))\n\t\t\ttracepoint_printk = 0;\n\t\telse\n\t\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\t}\n\ttracer_alloc_buffers();\n}\n\nvoid __init trace_init(void)\n{\n\ttrace_event_init();\n}\n\n__init static int clear_boot_tracer(void)\n{\n\t/*\n\t * The default tracer at boot buffer is an init section.\n\t * This function is called in lateinit. If we did not\n\t * find the boot tracer, then clear it out, to prevent\n\t * later registration from accessing the buffer that is\n\t * about to be freed.\n\t */\n\tif (!default_bootup_tracer)\n\t\treturn 0;\n\n\tprintk(KERN_INFO \"ftrace bootup tracer '%s' not registered.\\n\",\n\t       default_bootup_tracer);\n\tdefault_bootup_tracer = NULL;\n\n\treturn 0;\n}\n\nfs_initcall(tracer_init_tracefs);\nlate_initcall_sync(clear_boot_tracer);\n\n#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\n__init static int tracing_set_default_clock(void)\n{\n\t/* sched_clock_stable() is determined in late_initcall */\n\tif (!trace_boot_clock && !sched_clock_stable()) {\n\t\tprintk(KERN_WARNING\n\t\t       \"Unstable clock detected, switching default tracing clock to \\\"global\\\"\\n\"\n\t\t       \"If you want to keep using the local clock, then add:\\n\"\n\t\t       \"  \\\"trace_clock=local\\\"\\n\"\n\t\t       \"on the kernel command line\\n\");\n\t\ttracing_set_clock(&global_trace, \"global\");\n\t}\n\n\treturn 0;\n}\nlate_initcall_sync(tracing_set_default_clock);\n#endif\n", "/*\n * trace_events_filter - generic event filtering\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\n *\n * Copyright (C) 2009 Tom Zanussi <tzanussi@gmail.com>\n */\n\n#include <linux/module.h>\n#include <linux/ctype.h>\n#include <linux/mutex.h>\n#include <linux/perf_event.h>\n#include <linux/slab.h>\n\n#include \"trace.h\"\n#include \"trace_output.h\"\n\n#define DEFAULT_SYS_FILTER_MESSAGE\t\t\t\t\t\\\n\t\"### global filter ###\\n\"\t\t\t\t\t\\\n\t\"# Use this to set filters for multiple events.\\n\"\t\t\\\n\t\"# Only events with the given fields will be affected.\\n\"\t\\\n\t\"# If no events are modified, an error message will be displayed here\"\n\n/* Due to token parsing '<=' must be before '<' and '>=' must be before '>' */\n#define OPS\t\t\t\t\t\\\n\tC( OP_GLOB,\t\"~\"  ),\t\t\t\\\n\tC( OP_NE,\t\"!=\" ),\t\t\t\\\n\tC( OP_EQ,\t\"==\" ),\t\t\t\\\n\tC( OP_LE,\t\"<=\" ),\t\t\t\\\n\tC( OP_LT,\t\"<\"  ),\t\t\t\\\n\tC( OP_GE,\t\">=\" ),\t\t\t\\\n\tC( OP_GT,\t\">\"  ),\t\t\t\\\n\tC( OP_BAND,\t\"&\"  ),\t\t\t\\\n\tC( OP_MAX,\tNULL )\n\n#undef C\n#define C(a, b)\ta\n\nenum filter_op_ids { OPS };\n\n#undef C\n#define C(a, b)\tb\n\nstatic const char * ops[] = { OPS };\n\n/*\n * pred functions are OP_LE, OP_LT, OP_GE, OP_GT, and OP_BAND\n * pred_funcs_##type below must match the order of them above.\n */\n#define PRED_FUNC_START\t\t\tOP_LE\n#define PRED_FUNC_MAX\t\t\t(OP_BAND - PRED_FUNC_START)\n\n#define ERRORS\t\t\t\t\t\t\t\t\\\n\tC(NONE,\t\t\t\"No error\"),\t\t\t\t\\\n\tC(INVALID_OP,\t\t\"Invalid operator\"),\t\t\t\\\n\tC(TOO_MANY_OPEN,\t\"Too many '('\"),\t\t\t\\\n\tC(TOO_MANY_CLOSE,\t\"Too few '('\"),\t\t\t\t\\\n\tC(MISSING_QUOTE,\t\"Missing matching quote\"),\t\t\\\n\tC(OPERAND_TOO_LONG,\t\"Operand too long\"),\t\t\t\\\n\tC(EXPECT_STRING,\t\"Expecting string field\"),\t\t\\\n\tC(EXPECT_DIGIT,\t\t\"Expecting numeric field\"),\t\t\\\n\tC(ILLEGAL_FIELD_OP,\t\"Illegal operation for field type\"),\t\\\n\tC(FIELD_NOT_FOUND,\t\"Field not found\"),\t\t\t\\\n\tC(ILLEGAL_INTVAL,\t\"Illegal integer value\"),\t\t\\\n\tC(BAD_SUBSYS_FILTER,\t\"Couldn't find or set field in one of a subsystem's events\"), \\\n\tC(TOO_MANY_PREDS,\t\"Too many terms in predicate expression\"), \\\n\tC(INVALID_FILTER,\t\"Meaningless filter expression\"),\t\\\n\tC(IP_FIELD_ONLY,\t\"Only 'ip' field is supported for function trace\"), \\\n\tC(INVALID_VALUE,\t\"Invalid value (did you forget quotes)?\"),\n\n#undef C\n#define C(a, b)\t\tFILT_ERR_##a\n\nenum { ERRORS };\n\n#undef C\n#define C(a, b)\t\tb\n\nstatic char *err_text[] = { ERRORS };\n\n/* Called after a '!' character but \"!=\" and \"!~\" are not \"not\"s */\nstatic bool is_not(const char *str)\n{\n\tswitch (str[1]) {\n\tcase '=':\n\tcase '~':\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * prog_entry - a singe entry in the filter program\n * @target:\t     Index to jump to on a branch (actually one minus the index)\n * @when_to_branch:  The value of the result of the predicate to do a branch\n * @pred:\t     The predicate to execute.\n */\nstruct prog_entry {\n\tint\t\t\ttarget;\n\tint\t\t\twhen_to_branch;\n\tstruct filter_pred\t*pred;\n};\n\n/**\n * update_preds- assign a program entry a label target\n * @prog: The program array\n * @N: The index of the current entry in @prog\n * @when_to_branch: What to assign a program entry for its branch condition\n *\n * The program entry at @N has a target that points to the index of a program\n * entry that can have its target and when_to_branch fields updated.\n * Update the current program entry denoted by index @N target field to be\n * that of the updated entry. This will denote the entry to update if\n * we are processing an \"||\" after an \"&&\"\n */\nstatic void update_preds(struct prog_entry *prog, int N, int invert)\n{\n\tint t, s;\n\n\tt = prog[N].target;\n\ts = prog[t].target;\n\tprog[t].when_to_branch = invert;\n\tprog[t].target = N;\n\tprog[N].target = s;\n}\n\nstruct filter_parse_error {\n\tint lasterr;\n\tint lasterr_pos;\n};\n\nstatic void parse_error(struct filter_parse_error *pe, int err, int pos)\n{\n\tpe->lasterr = err;\n\tpe->lasterr_pos = pos;\n}\n\ntypedef int (*parse_pred_fn)(const char *str, void *data, int pos,\n\t\t\t     struct filter_parse_error *pe,\n\t\t\t     struct filter_pred **pred);\n\nenum {\n\tINVERT\t\t= 1,\n\tPROCESS_AND\t= 2,\n\tPROCESS_OR\t= 4,\n};\n\n/*\n * Without going into a formal proof, this explains the method that is used in\n * parsing the logical expressions.\n *\n * For example, if we have: \"a && !(!b || (c && g)) || d || e && !f\"\n * The first pass will convert it into the following program:\n *\n * n1: r=a;       l1: if (!r) goto l4;\n * n2: r=b;       l2: if (!r) goto l4;\n * n3: r=c; r=!r; l3: if (r) goto l4;\n * n4: r=g; r=!r; l4: if (r) goto l5;\n * n5: r=d;       l5: if (r) goto T\n * n6: r=e;       l6: if (!r) goto l7;\n * n7: r=f; r=!r; l7: if (!r) goto F\n * T: return TRUE\n * F: return FALSE\n *\n * To do this, we use a data structure to represent each of the above\n * predicate and conditions that has:\n *\n *  predicate, when_to_branch, invert, target\n *\n * The \"predicate\" will hold the function to determine the result \"r\".\n * The \"when_to_branch\" denotes what \"r\" should be if a branch is to be taken\n * \"&&\" would contain \"!r\" or (0) and \"||\" would contain \"r\" or (1).\n * The \"invert\" holds whether the value should be reversed before testing.\n * The \"target\" contains the label \"l#\" to jump to.\n *\n * A stack is created to hold values when parentheses are used.\n *\n * To simplify the logic, the labels will start at 0 and not 1.\n *\n * The possible invert values are 1 and 0. The number of \"!\"s that are in scope\n * before the predicate determines the invert value, if the number is odd then\n * the invert value is 1 and 0 otherwise. This means the invert value only\n * needs to be toggled when a new \"!\" is introduced compared to what is stored\n * on the stack, where parentheses were used.\n *\n * The top of the stack and \"invert\" are initialized to zero.\n *\n * ** FIRST PASS **\n *\n * #1 A loop through all the tokens is done:\n *\n * #2 If the token is an \"(\", the stack is push, and the current stack value\n *    gets the current invert value, and the loop continues to the next token.\n *    The top of the stack saves the \"invert\" value to keep track of what\n *    the current inversion is. As \"!(a && !b || c)\" would require all\n *    predicates being affected separately by the \"!\" before the parentheses.\n *    And that would end up being equivalent to \"(!a || b) && !c\"\n *\n * #3 If the token is an \"!\", the current \"invert\" value gets inverted, and\n *    the loop continues. Note, if the next token is a predicate, then\n *    this \"invert\" value is only valid for the current program entry,\n *    and does not affect other predicates later on.\n *\n * The only other acceptable token is the predicate string.\n *\n * #4 A new entry into the program is added saving: the predicate and the\n *    current value of \"invert\". The target is currently assigned to the\n *    previous program index (this will not be its final value).\n *\n * #5 We now enter another loop and look at the next token. The only valid\n *    tokens are \")\", \"&&\", \"||\" or end of the input string \"\\0\".\n *\n * #6 The invert variable is reset to the current value saved on the top of\n *    the stack.\n *\n * #7 The top of the stack holds not only the current invert value, but also\n *    if a \"&&\" or \"||\" needs to be processed. Note, the \"&&\" takes higher\n *    precedence than \"||\". That is \"a && b || c && d\" is equivalent to\n *    \"(a && b) || (c && d)\". Thus the first thing to do is to see if \"&&\" needs\n *    to be processed. This is the case if an \"&&\" was the last token. If it was\n *    then we call update_preds(). This takes the program, the current index in\n *    the program, and the current value of \"invert\".  More will be described\n *    below about this function.\n *\n * #8 If the next token is \"&&\" then we set a flag in the top of the stack\n *    that denotes that \"&&\" needs to be processed, break out of this loop\n *    and continue with the outer loop.\n *\n * #9 Otherwise, if a \"||\" needs to be processed then update_preds() is called.\n *    This is called with the program, the current index in the program, but\n *    this time with an inverted value of \"invert\" (that is !invert). This is\n *    because the value taken will become the \"when_to_branch\" value of the\n *    program.\n *    Note, this is called when the next token is not an \"&&\". As stated before,\n *    \"&&\" takes higher precedence, and \"||\" should not be processed yet if the\n *    next logical operation is \"&&\".\n *\n * #10 If the next token is \"||\" then we set a flag in the top of the stack\n *     that denotes that \"||\" needs to be processed, break out of this loop\n *     and continue with the outer loop.\n *\n * #11 If this is the end of the input string \"\\0\" then we break out of both\n *     loops.\n *\n * #12 Otherwise, the next token is \")\", where we pop the stack and continue\n *     this inner loop.\n *\n * Now to discuss the update_pred() function, as that is key to the setting up\n * of the program. Remember the \"target\" of the program is initialized to the\n * previous index and not the \"l\" label. The target holds the index into the\n * program that gets affected by the operand. Thus if we have something like\n *  \"a || b && c\", when we process \"a\" the target will be \"-1\" (undefined).\n * When we process \"b\", its target is \"0\", which is the index of \"a\", as that's\n * the predicate that is affected by \"||\". But because the next token after \"b\"\n * is \"&&\" we don't call update_preds(). Instead continue to \"c\". As the\n * next token after \"c\" is not \"&&\" but the end of input, we first process the\n * \"&&\" by calling update_preds() for the \"&&\" then we process the \"||\" by\n * callin updates_preds() with the values for processing \"||\".\n *\n * What does that mean? What update_preds() does is to first save the \"target\"\n * of the program entry indexed by the current program entry's \"target\"\n * (remember the \"target\" is initialized to previous program entry), and then\n * sets that \"target\" to the current index which represents the label \"l#\".\n * That entry's \"when_to_branch\" is set to the value passed in (the \"invert\"\n * or \"!invert\"). Then it sets the current program entry's target to the saved\n * \"target\" value (the old value of the program that had its \"target\" updated\n * to the label).\n *\n * Looking back at \"a || b && c\", we have the following steps:\n *  \"a\"  - prog[0] = { \"a\", X, -1 } // pred, when_to_branch, target\n *  \"||\" - flag that we need to process \"||\"; continue outer loop\n *  \"b\"  - prog[1] = { \"b\", X, 0 }\n *  \"&&\" - flag that we need to process \"&&\"; continue outer loop\n * (Notice we did not process \"||\")\n *  \"c\"  - prog[2] = { \"c\", X, 1 }\n *  update_preds(prog, 2, 0); // invert = 0 as we are processing \"&&\"\n *    t = prog[2].target; // t = 1\n *    s = prog[t].target; // s = 0\n *    prog[t].target = 2; // Set target to \"l2\"\n *    prog[t].when_to_branch = 0;\n *    prog[2].target = s;\n * update_preds(prog, 2, 1); // invert = 1 as we are now processing \"||\"\n *    t = prog[2].target; // t = 0\n *    s = prog[t].target; // s = -1\n *    prog[t].target = 2; // Set target to \"l2\"\n *    prog[t].when_to_branch = 1;\n *    prog[2].target = s;\n *\n * #13 Which brings us to the final step of the first pass, which is to set\n *     the last program entry's when_to_branch and target, which will be\n *     when_to_branch = 0; target = N; ( the label after the program entry after\n *     the last program entry processed above).\n *\n * If we denote \"TRUE\" to be the entry after the last program entry processed,\n * and \"FALSE\" the program entry after that, we are now done with the first\n * pass.\n *\n * Making the above \"a || b && c\" have a progam of:\n *  prog[0] = { \"a\", 1, 2 }\n *  prog[1] = { \"b\", 0, 2 }\n *  prog[2] = { \"c\", 0, 3 }\n *\n * Which translates into:\n * n0: r = a; l0: if (r) goto l2;\n * n1: r = b; l1: if (!r) goto l2;\n * n2: r = c; l2: if (!r) goto l3;  // Which is the same as \"goto F;\"\n * T: return TRUE; l3:\n * F: return FALSE\n *\n * Although, after the first pass, the program is correct, it is\n * inefficient. The simple sample of \"a || b && c\" could be easily been\n * converted into:\n * n0: r = a; if (r) goto T\n * n1: r = b; if (!r) goto F\n * n2: r = c; if (!r) goto F\n * T: return TRUE;\n * F: return FALSE;\n *\n * The First Pass is over the input string. The next too passes are over\n * the program itself.\n *\n * ** SECOND PASS **\n *\n * Which brings us to the second pass. If a jump to a label has the\n * same condition as that label, it can instead jump to its target.\n * The original example of \"a && !(!b || (c && g)) || d || e && !f\"\n * where the first pass gives us:\n *\n * n1: r=a;       l1: if (!r) goto l4;\n * n2: r=b;       l2: if (!r) goto l4;\n * n3: r=c; r=!r; l3: if (r) goto l4;\n * n4: r=g; r=!r; l4: if (r) goto l5;\n * n5: r=d;       l5: if (r) goto T\n * n6: r=e;       l6: if (!r) goto l7;\n * n7: r=f; r=!r; l7: if (!r) goto F:\n * T: return TRUE;\n * F: return FALSE\n *\n * We can see that \"l3: if (r) goto l4;\" and at l4, we have \"if (r) goto l5;\".\n * And \"l5: if (r) goto T\", we could optimize this by converting l3 and l4\n * to go directly to T. To accomplish this, we start from the last\n * entry in the program and work our way back. If the target of the entry\n * has the same \"when_to_branch\" then we could use that entry's target.\n * Doing this, the above would end up as:\n *\n * n1: r=a;       l1: if (!r) goto l4;\n * n2: r=b;       l2: if (!r) goto l4;\n * n3: r=c; r=!r; l3: if (r) goto T;\n * n4: r=g; r=!r; l4: if (r) goto T;\n * n5: r=d;       l5: if (r) goto T;\n * n6: r=e;       l6: if (!r) goto F;\n * n7: r=f; r=!r; l7: if (!r) goto F;\n * T: return TRUE\n * F: return FALSE\n *\n * In that same pass, if the \"when_to_branch\" doesn't match, we can simply\n * go to the program entry after the label. That is, \"l2: if (!r) goto l4;\"\n * where \"l4: if (r) goto T;\", then we can convert l2 to be:\n * \"l2: if (!r) goto n5;\".\n *\n * This will have the second pass give us:\n * n1: r=a;       l1: if (!r) goto n5;\n * n2: r=b;       l2: if (!r) goto n5;\n * n3: r=c; r=!r; l3: if (r) goto T;\n * n4: r=g; r=!r; l4: if (r) goto T;\n * n5: r=d;       l5: if (r) goto T\n * n6: r=e;       l6: if (!r) goto F;\n * n7: r=f; r=!r; l7: if (!r) goto F\n * T: return TRUE\n * F: return FALSE\n *\n * Notice, all the \"l#\" labels are no longer used, and they can now\n * be discarded.\n *\n * ** THIRD PASS **\n *\n * For the third pass we deal with the inverts. As they simply just\n * make the \"when_to_branch\" get inverted, a simple loop over the\n * program to that does: \"when_to_branch ^= invert;\" will do the\n * job, leaving us with:\n * n1: r=a; if (!r) goto n5;\n * n2: r=b; if (!r) goto n5;\n * n3: r=c: if (!r) goto T;\n * n4: r=g; if (!r) goto T;\n * n5: r=d; if (r) goto T\n * n6: r=e; if (!r) goto F;\n * n7: r=f; if (r) goto F\n * T: return TRUE\n * F: return FALSE\n *\n * As \"r = a; if (!r) goto n5;\" is obviously the same as\n * \"if (!a) goto n5;\" without doing anything we can interperate the\n * program as:\n * n1: if (!a) goto n5;\n * n2: if (!b) goto n5;\n * n3: if (!c) goto T;\n * n4: if (!g) goto T;\n * n5: if (d) goto T\n * n6: if (!e) goto F;\n * n7: if (f) goto F\n * T: return TRUE\n * F: return FALSE\n *\n * Since the inverts are discarded at the end, there's no reason to store\n * them in the program array (and waste memory). A separate array to hold\n * the inverts is used and freed at the end.\n */\nstatic struct prog_entry *\npredicate_parse(const char *str, int nr_parens, int nr_preds,\n\t\tparse_pred_fn parse_pred, void *data,\n\t\tstruct filter_parse_error *pe)\n{\n\tstruct prog_entry *prog_stack;\n\tstruct prog_entry *prog;\n\tconst char *ptr = str;\n\tchar *inverts = NULL;\n\tint *op_stack;\n\tint *top;\n\tint invert = 0;\n\tint ret = -ENOMEM;\n\tint len;\n\tint N = 0;\n\tint i;\n\n\tnr_preds += 2; /* For TRUE and FALSE */\n\n\top_stack = kmalloc_array(nr_parens, sizeof(*op_stack), GFP_KERNEL);\n\tif (!op_stack)\n\t\treturn ERR_PTR(-ENOMEM);\n\tprog_stack = kmalloc_array(nr_preds, sizeof(*prog_stack), GFP_KERNEL);\n\tif (!prog_stack) {\n\t\tparse_error(pe, -ENOMEM, 0);\n\t\tgoto out_free;\n\t}\n\tinverts = kmalloc_array(nr_preds, sizeof(*inverts), GFP_KERNEL);\n\tif (!inverts) {\n\t\tparse_error(pe, -ENOMEM, 0);\n\t\tgoto out_free;\n\t}\n\n\ttop = op_stack;\n\tprog = prog_stack;\n\t*top = 0;\n\n\t/* First pass */\n\twhile (*ptr) {\t\t\t\t\t\t/* #1 */\n\t\tconst char *next = ptr++;\n\n\t\tif (isspace(*next))\n\t\t\tcontinue;\n\n\t\tswitch (*next) {\n\t\tcase '(':\t\t\t\t\t/* #2 */\n\t\t\tif (top - op_stack > nr_parens)\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t*(++top) = invert;\n\t\t\tcontinue;\n\t\tcase '!':\t\t\t\t\t/* #3 */\n\t\t\tif (!is_not(next))\n\t\t\t\tbreak;\n\t\t\tinvert = !invert;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (N >= nr_preds) {\n\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_PREDS, next - str);\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tinverts[N] = invert;\t\t\t\t/* #4 */\n\t\tprog[N].target = N-1;\n\n\t\tlen = parse_pred(next, data, ptr - str, pe, &prog[N].pred);\n\t\tif (len < 0) {\n\t\t\tret = len;\n\t\t\tgoto out_free;\n\t\t}\n\t\tptr = next + len;\n\n\t\tN++;\n\n\t\tret = -1;\n\t\twhile (1) {\t\t\t\t\t/* #5 */\n\t\t\tnext = ptr++;\n\t\t\tif (isspace(*next))\n\t\t\t\tcontinue;\n\n\t\t\tswitch (*next) {\n\t\t\tcase ')':\n\t\t\tcase '\\0':\n\t\t\t\tbreak;\n\t\t\tcase '&':\n\t\t\tcase '|':\n\t\t\t\tif (next[1] == next[0]) {\n\t\t\t\t\tptr++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_PREDS,\n\t\t\t\t\t    next - str);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\tinvert = *top & INVERT;\n\n\t\t\tif (*top & PROCESS_AND) {\t\t/* #7 */\n\t\t\t\tupdate_preds(prog, N - 1, invert);\n\t\t\t\t*top &= ~PROCESS_AND;\n\t\t\t}\n\t\t\tif (*next == '&') {\t\t\t/* #8 */\n\t\t\t\t*top |= PROCESS_AND;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (*top & PROCESS_OR) {\t\t/* #9 */\n\t\t\t\tupdate_preds(prog, N - 1, !invert);\n\t\t\t\t*top &= ~PROCESS_OR;\n\t\t\t}\n\t\t\tif (*next == '|') {\t\t\t/* #10 */\n\t\t\t\t*top |= PROCESS_OR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!*next)\t\t\t\t/* #11 */\n\t\t\t\tgoto out;\n\n\t\t\tif (top == op_stack) {\n\t\t\t\tret = -1;\n\t\t\t\t/* Too few '(' */\n\t\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_CLOSE, ptr - str);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\ttop--;\t\t\t\t\t/* #12 */\n\t\t}\n\t}\n out:\n\tif (top != op_stack) {\n\t\t/* Too many '(' */\n\t\tparse_error(pe, FILT_ERR_TOO_MANY_OPEN, ptr - str);\n\t\tgoto out_free;\n\t}\n\n\tprog[N].pred = NULL;\t\t\t\t\t/* #13 */\n\tprog[N].target = 1;\t\t/* TRUE */\n\tprog[N+1].pred = NULL;\n\tprog[N+1].target = 0;\t\t/* FALSE */\n\tprog[N-1].target = N;\n\tprog[N-1].when_to_branch = false;\n\n\t/* Second Pass */\n\tfor (i = N-1 ; i--; ) {\n\t\tint target = prog[i].target;\n\t\tif (prog[i].when_to_branch == prog[target].when_to_branch)\n\t\t\tprog[i].target = prog[target].target;\n\t}\n\n\t/* Third Pass */\n\tfor (i = 0; i < N; i++) {\n\t\tinvert = inverts[i] ^ prog[i].when_to_branch;\n\t\tprog[i].when_to_branch = invert;\n\t\t/* Make sure the program always moves forward */\n\t\tif (WARN_ON(prog[i].target <= i)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\treturn prog;\nout_free:\n\tkfree(op_stack);\n\tkfree(prog_stack);\n\tkfree(inverts);\n\treturn ERR_PTR(ret);\n}\n\n#define DEFINE_COMPARISON_PRED(type)\t\t\t\t\t\\\nstatic int filter_pred_LT_##type(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn *addr < val;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic int filter_pred_LE_##type(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn *addr <= val;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic int filter_pred_GT_##type(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn *addr > val;\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic int filter_pred_GE_##type(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn *addr >= val;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic int filter_pred_BAND_##type(struct filter_pred *pred, void *event) \\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn !!(*addr & val);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic const filter_pred_fn_t pred_funcs_##type[] = {\t\t\t\\\n\tfilter_pred_LE_##type,\t\t\t\t\t\t\\\n\tfilter_pred_LT_##type,\t\t\t\t\t\t\\\n\tfilter_pred_GE_##type,\t\t\t\t\t\t\\\n\tfilter_pred_GT_##type,\t\t\t\t\t\t\\\n\tfilter_pred_BAND_##type,\t\t\t\t\t\\\n};\n\n#define DEFINE_EQUALITY_PRED(size)\t\t\t\t\t\\\nstatic int filter_pred_##size(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu##size *addr = (u##size *)(event + pred->offset);\t\t\\\n\tu##size val = (u##size)pred->val;\t\t\t\t\\\n\tint match;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tmatch = (val == *addr) ^ pred->not;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn match;\t\t\t\t\t\t\t\\\n}\n\nDEFINE_COMPARISON_PRED(s64);\nDEFINE_COMPARISON_PRED(u64);\nDEFINE_COMPARISON_PRED(s32);\nDEFINE_COMPARISON_PRED(u32);\nDEFINE_COMPARISON_PRED(s16);\nDEFINE_COMPARISON_PRED(u16);\nDEFINE_COMPARISON_PRED(s8);\nDEFINE_COMPARISON_PRED(u8);\n\nDEFINE_EQUALITY_PRED(64);\nDEFINE_EQUALITY_PRED(32);\nDEFINE_EQUALITY_PRED(16);\nDEFINE_EQUALITY_PRED(8);\n\n/* Filter predicate for fixed sized arrays of characters */\nstatic int filter_pred_string(struct filter_pred *pred, void *event)\n{\n\tchar *addr = (char *)(event + pred->offset);\n\tint cmp, match;\n\n\tcmp = pred->regex.match(addr, &pred->regex, pred->regex.field_len);\n\n\tmatch = cmp ^ pred->not;\n\n\treturn match;\n}\n\n/* Filter predicate for char * pointers */\nstatic int filter_pred_pchar(struct filter_pred *pred, void *event)\n{\n\tchar **addr = (char **)(event + pred->offset);\n\tint cmp, match;\n\tint len = strlen(*addr) + 1;\t/* including tailing '\\0' */\n\n\tcmp = pred->regex.match(*addr, &pred->regex, len);\n\n\tmatch = cmp ^ pred->not;\n\n\treturn match;\n}\n\n/*\n * Filter predicate for dynamic sized arrays of characters.\n * These are implemented through a list of strings at the end\n * of the entry.\n * Also each of these strings have a field in the entry which\n * contains its offset from the beginning of the entry.\n * We have then first to get this field, dereference it\n * and add it to the address of the entry, and at last we have\n * the address of the string.\n */\nstatic int filter_pred_strloc(struct filter_pred *pred, void *event)\n{\n\tu32 str_item = *(u32 *)(event + pred->offset);\n\tint str_loc = str_item & 0xffff;\n\tint str_len = str_item >> 16;\n\tchar *addr = (char *)(event + str_loc);\n\tint cmp, match;\n\n\tcmp = pred->regex.match(addr, &pred->regex, str_len);\n\n\tmatch = cmp ^ pred->not;\n\n\treturn match;\n}\n\n/* Filter predicate for CPUs. */\nstatic int filter_pred_cpu(struct filter_pred *pred, void *event)\n{\n\tint cpu, cmp;\n\n\tcpu = raw_smp_processor_id();\n\tcmp = pred->val;\n\n\tswitch (pred->op) {\n\tcase OP_EQ:\n\t\treturn cpu == cmp;\n\tcase OP_NE:\n\t\treturn cpu != cmp;\n\tcase OP_LT:\n\t\treturn cpu < cmp;\n\tcase OP_LE:\n\t\treturn cpu <= cmp;\n\tcase OP_GT:\n\t\treturn cpu > cmp;\n\tcase OP_GE:\n\t\treturn cpu >= cmp;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n/* Filter predicate for COMM. */\nstatic int filter_pred_comm(struct filter_pred *pred, void *event)\n{\n\tint cmp;\n\n\tcmp = pred->regex.match(current->comm, &pred->regex,\n\t\t\t\tTASK_COMM_LEN);\n\treturn cmp ^ pred->not;\n}\n\nstatic int filter_pred_none(struct filter_pred *pred, void *event)\n{\n\treturn 0;\n}\n\n/*\n * regex_match_foo - Basic regex callbacks\n *\n * @str: the string to be searched\n * @r:   the regex structure containing the pattern string\n * @len: the length of the string to be searched (including '\\0')\n *\n * Note:\n * - @str might not be NULL-terminated if it's of type DYN_STRING\n *   or STATIC_STRING, unless @len is zero.\n */\n\nstatic int regex_match_full(char *str, struct regex *r, int len)\n{\n\t/* len of zero means str is dynamic and ends with '\\0' */\n\tif (!len)\n\t\treturn strcmp(str, r->pattern) == 0;\n\n\treturn strncmp(str, r->pattern, len) == 0;\n}\n\nstatic int regex_match_front(char *str, struct regex *r, int len)\n{\n\tif (len && len < r->len)\n\t\treturn 0;\n\n\treturn strncmp(str, r->pattern, r->len) == 0;\n}\n\nstatic int regex_match_middle(char *str, struct regex *r, int len)\n{\n\tif (!len)\n\t\treturn strstr(str, r->pattern) != NULL;\n\n\treturn strnstr(str, r->pattern, len) != NULL;\n}\n\nstatic int regex_match_end(char *str, struct regex *r, int len)\n{\n\tint strlen = len - 1;\n\n\tif (strlen >= r->len &&\n\t    memcmp(str + strlen - r->len, r->pattern, r->len) == 0)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int regex_match_glob(char *str, struct regex *r, int len __maybe_unused)\n{\n\tif (glob_match(r->pattern, str))\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n * filter_parse_regex - parse a basic regex\n * @buff:   the raw regex\n * @len:    length of the regex\n * @search: will point to the beginning of the string to compare\n * @not:    tell whether the match will have to be inverted\n *\n * This passes in a buffer containing a regex and this function will\n * set search to point to the search part of the buffer and\n * return the type of search it is (see enum above).\n * This does modify buff.\n *\n * Returns enum type.\n *  search returns the pointer to use for comparison.\n *  not returns 1 if buff started with a '!'\n *     0 otherwise.\n */\nenum regex_type filter_parse_regex(char *buff, int len, char **search, int *not)\n{\n\tint type = MATCH_FULL;\n\tint i;\n\n\tif (buff[0] == '!') {\n\t\t*not = 1;\n\t\tbuff++;\n\t\tlen--;\n\t} else\n\t\t*not = 0;\n\n\t*search = buff;\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (buff[i] == '*') {\n\t\t\tif (!i) {\n\t\t\t\ttype = MATCH_END_ONLY;\n\t\t\t} else if (i == len - 1) {\n\t\t\t\tif (type == MATCH_END_ONLY)\n\t\t\t\t\ttype = MATCH_MIDDLE_ONLY;\n\t\t\t\telse\n\t\t\t\t\ttype = MATCH_FRONT_ONLY;\n\t\t\t\tbuff[i] = 0;\n\t\t\t\tbreak;\n\t\t\t} else {\t/* pattern continues, use full glob */\n\t\t\t\treturn MATCH_GLOB;\n\t\t\t}\n\t\t} else if (strchr(\"[?\\\\\", buff[i])) {\n\t\t\treturn MATCH_GLOB;\n\t\t}\n\t}\n\tif (buff[0] == '*')\n\t\t*search = buff + 1;\n\n\treturn type;\n}\n\nstatic void filter_build_regex(struct filter_pred *pred)\n{\n\tstruct regex *r = &pred->regex;\n\tchar *search;\n\tenum regex_type type = MATCH_FULL;\n\n\tif (pred->op == OP_GLOB) {\n\t\ttype = filter_parse_regex(r->pattern, r->len, &search, &pred->not);\n\t\tr->len = strlen(search);\n\t\tmemmove(r->pattern, search, r->len+1);\n\t}\n\n\tswitch (type) {\n\tcase MATCH_FULL:\n\t\tr->match = regex_match_full;\n\t\tbreak;\n\tcase MATCH_FRONT_ONLY:\n\t\tr->match = regex_match_front;\n\t\tbreak;\n\tcase MATCH_MIDDLE_ONLY:\n\t\tr->match = regex_match_middle;\n\t\tbreak;\n\tcase MATCH_END_ONLY:\n\t\tr->match = regex_match_end;\n\t\tbreak;\n\tcase MATCH_GLOB:\n\t\tr->match = regex_match_glob;\n\t\tbreak;\n\t}\n}\n\n/* return 1 if event matches, 0 otherwise (discard) */\nint filter_match_preds(struct event_filter *filter, void *rec)\n{\n\tstruct prog_entry *prog;\n\tint i;\n\n\t/* no filter is considered a match */\n\tif (!filter)\n\t\treturn 1;\n\n\tprog = rcu_dereference_sched(filter->prog);\n\tif (!prog)\n\t\treturn 1;\n\n\tfor (i = 0; prog[i].pred; i++) {\n\t\tstruct filter_pred *pred = prog[i].pred;\n\t\tint match = pred->fn(pred, rec);\n\t\tif (match == prog[i].when_to_branch)\n\t\t\ti = prog[i].target;\n\t}\n\treturn prog[i].target;\n}\nEXPORT_SYMBOL_GPL(filter_match_preds);\n\nstatic void remove_filter_string(struct event_filter *filter)\n{\n\tif (!filter)\n\t\treturn;\n\n\tkfree(filter->filter_string);\n\tfilter->filter_string = NULL;\n}\n\nstatic void append_filter_err(struct filter_parse_error *pe,\n\t\t\t      struct event_filter *filter)\n{\n\tstruct trace_seq *s;\n\tint pos = pe->lasterr_pos;\n\tchar *buf;\n\tint len;\n\n\tif (WARN_ON(!filter->filter_string))\n\t\treturn;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn;\n\ttrace_seq_init(s);\n\n\tlen = strlen(filter->filter_string);\n\tif (pos > len)\n\t\tpos = len;\n\n\t/* indexing is off by one */\n\tif (pos)\n\t\tpos++;\n\n\ttrace_seq_puts(s, filter->filter_string);\n\tif (pe->lasterr > 0) {\n\t\ttrace_seq_printf(s, \"\\n%*s\", pos, \"^\");\n\t\ttrace_seq_printf(s, \"\\nparse_error: %s\\n\", err_text[pe->lasterr]);\n\t} else {\n\t\ttrace_seq_printf(s, \"\\nError: (%d)\\n\", pe->lasterr);\n\t}\n\ttrace_seq_putc(s, 0);\n\tbuf = kmemdup_nul(s->buffer, s->seq.len, GFP_KERNEL);\n\tif (buf) {\n\t\tkfree(filter->filter_string);\n\t\tfilter->filter_string = buf;\n\t}\n\tkfree(s);\n}\n\nstatic inline struct event_filter *event_filter(struct trace_event_file *file)\n{\n\treturn file->filter;\n}\n\n/* caller must hold event_mutex */\nvoid print_event_filter(struct trace_event_file *file, struct trace_seq *s)\n{\n\tstruct event_filter *filter = event_filter(file);\n\n\tif (filter && filter->filter_string)\n\t\ttrace_seq_printf(s, \"%s\\n\", filter->filter_string);\n\telse\n\t\ttrace_seq_puts(s, \"none\\n\");\n}\n\nvoid print_subsystem_event_filter(struct event_subsystem *system,\n\t\t\t\t  struct trace_seq *s)\n{\n\tstruct event_filter *filter;\n\n\tmutex_lock(&event_mutex);\n\tfilter = system->filter;\n\tif (filter && filter->filter_string)\n\t\ttrace_seq_printf(s, \"%s\\n\", filter->filter_string);\n\telse\n\t\ttrace_seq_puts(s, DEFAULT_SYS_FILTER_MESSAGE \"\\n\");\n\tmutex_unlock(&event_mutex);\n}\n\nstatic void free_prog(struct event_filter *filter)\n{\n\tstruct prog_entry *prog;\n\tint i;\n\n\tprog = rcu_access_pointer(filter->prog);\n\tif (!prog)\n\t\treturn;\n\n\tfor (i = 0; prog[i].pred; i++)\n\t\tkfree(prog[i].pred);\n\tkfree(prog);\n}\n\nstatic void filter_disable(struct trace_event_file *file)\n{\n\tunsigned long old_flags = file->flags;\n\n\tfile->flags &= ~EVENT_FILE_FL_FILTERED;\n\n\tif (old_flags != file->flags)\n\t\ttrace_buffered_event_disable();\n}\n\nstatic void __free_filter(struct event_filter *filter)\n{\n\tif (!filter)\n\t\treturn;\n\n\tfree_prog(filter);\n\tkfree(filter->filter_string);\n\tkfree(filter);\n}\n\nvoid free_event_filter(struct event_filter *filter)\n{\n\t__free_filter(filter);\n}\n\nstatic inline void __remove_filter(struct trace_event_file *file)\n{\n\tfilter_disable(file);\n\tremove_filter_string(file->filter);\n}\n\nstatic void filter_free_subsystem_preds(struct trace_subsystem_dir *dir,\n\t\t\t\t\tstruct trace_array *tr)\n{\n\tstruct trace_event_file *file;\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\t\tif (file->system != dir)\n\t\t\tcontinue;\n\t\t__remove_filter(file);\n\t}\n}\n\nstatic inline void __free_subsystem_filter(struct trace_event_file *file)\n{\n\t__free_filter(file->filter);\n\tfile->filter = NULL;\n}\n\nstatic void filter_free_subsystem_filters(struct trace_subsystem_dir *dir,\n\t\t\t\t\t  struct trace_array *tr)\n{\n\tstruct trace_event_file *file;\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\t\tif (file->system != dir)\n\t\t\tcontinue;\n\t\t__free_subsystem_filter(file);\n\t}\n}\n\nint filter_assign_type(const char *type)\n{\n\tif (strstr(type, \"__data_loc\") && strstr(type, \"char\"))\n\t\treturn FILTER_DYN_STRING;\n\n\tif (strchr(type, '[') && strstr(type, \"char\"))\n\t\treturn FILTER_STATIC_STRING;\n\n\treturn FILTER_OTHER;\n}\n\nstatic filter_pred_fn_t select_comparison_fn(enum filter_op_ids op,\n\t\t\t\t\t    int field_size, int field_is_signed)\n{\n\tfilter_pred_fn_t fn = NULL;\n\tint pred_func_index = -1;\n\n\tswitch (op) {\n\tcase OP_EQ:\n\tcase OP_NE:\n\t\tbreak;\n\tdefault:\n\t\tif (WARN_ON_ONCE(op < PRED_FUNC_START))\n\t\t\treturn NULL;\n\t\tpred_func_index = op - PRED_FUNC_START;\n\t\tif (WARN_ON_ONCE(pred_func_index > PRED_FUNC_MAX))\n\t\t\treturn NULL;\n\t}\n\n\tswitch (field_size) {\n\tcase 8:\n\t\tif (pred_func_index < 0)\n\t\t\tfn = filter_pred_64;\n\t\telse if (field_is_signed)\n\t\t\tfn = pred_funcs_s64[pred_func_index];\n\t\telse\n\t\t\tfn = pred_funcs_u64[pred_func_index];\n\t\tbreak;\n\tcase 4:\n\t\tif (pred_func_index < 0)\n\t\t\tfn = filter_pred_32;\n\t\telse if (field_is_signed)\n\t\t\tfn = pred_funcs_s32[pred_func_index];\n\t\telse\n\t\t\tfn = pred_funcs_u32[pred_func_index];\n\t\tbreak;\n\tcase 2:\n\t\tif (pred_func_index < 0)\n\t\t\tfn = filter_pred_16;\n\t\telse if (field_is_signed)\n\t\t\tfn = pred_funcs_s16[pred_func_index];\n\t\telse\n\t\t\tfn = pred_funcs_u16[pred_func_index];\n\t\tbreak;\n\tcase 1:\n\t\tif (pred_func_index < 0)\n\t\t\tfn = filter_pred_8;\n\t\telse if (field_is_signed)\n\t\t\tfn = pred_funcs_s8[pred_func_index];\n\t\telse\n\t\t\tfn = pred_funcs_u8[pred_func_index];\n\t\tbreak;\n\t}\n\n\treturn fn;\n}\n\n/* Called when a predicate is encountered by predicate_parse() */\nstatic int parse_pred(const char *str, void *data,\n\t\t      int pos, struct filter_parse_error *pe,\n\t\t      struct filter_pred **pred_ptr)\n{\n\tstruct trace_event_call *call = data;\n\tstruct ftrace_event_field *field;\n\tstruct filter_pred *pred = NULL;\n\tchar num_buf[24];\t/* Big enough to hold an address */\n\tchar *field_name;\n\tchar q;\n\tu64 val;\n\tint len;\n\tint ret;\n\tint op;\n\tint s;\n\tint i = 0;\n\n\t/* First find the field to associate to */\n\twhile (isspace(str[i]))\n\t\ti++;\n\ts = i;\n\n\twhile (isalnum(str[i]) || str[i] == '_')\n\t\ti++;\n\n\tlen = i - s;\n\n\tif (!len)\n\t\treturn -1;\n\n\tfield_name = kmemdup_nul(str + s, len, GFP_KERNEL);\n\tif (!field_name)\n\t\treturn -ENOMEM;\n\n\t/* Make sure that the field exists */\n\n\tfield = trace_find_event_field(call, field_name);\n\tkfree(field_name);\n\tif (!field) {\n\t\tparse_error(pe, FILT_ERR_FIELD_NOT_FOUND, pos + i);\n\t\treturn -EINVAL;\n\t}\n\n\twhile (isspace(str[i]))\n\t\ti++;\n\n\t/* Make sure this op is supported */\n\tfor (op = 0; ops[op]; op++) {\n\t\t/* This is why '<=' must come before '<' in ops[] */\n\t\tif (strncmp(str + i, ops[op], strlen(ops[op])) == 0)\n\t\t\tbreak;\n\t}\n\n\tif (!ops[op]) {\n\t\tparse_error(pe, FILT_ERR_INVALID_OP, pos + i);\n\t\tgoto err_free;\n\t}\n\n\ti += strlen(ops[op]);\n\n\twhile (isspace(str[i]))\n\t\ti++;\n\n\ts = i;\n\n\tpred = kzalloc(sizeof(*pred), GFP_KERNEL);\n\tif (!pred)\n\t\treturn -ENOMEM;\n\n\tpred->field = field;\n\tpred->offset = field->offset;\n\tpred->op = op;\n\n\tif (ftrace_event_is_function(call)) {\n\t\t/*\n\t\t * Perf does things different with function events.\n\t\t * It only allows an \"ip\" field, and expects a string.\n\t\t * But the string does not need to be surrounded by quotes.\n\t\t * If it is a string, the assigned function as a nop,\n\t\t * (perf doesn't use it) and grab everything.\n\t\t */\n\t\tif (strcmp(field->name, \"ip\") != 0) {\n\t\t\t parse_error(pe, FILT_ERR_IP_FIELD_ONLY, pos + i);\n\t\t\t goto err_free;\n\t\t }\n\t\t pred->fn = filter_pred_none;\n\n\t\t /*\n\t\t  * Quotes are not required, but if they exist then we need\n\t\t  * to read them till we hit a matching one.\n\t\t  */\n\t\t if (str[i] == '\\'' || str[i] == '\"')\n\t\t\t q = str[i];\n\t\t else\n\t\t\t q = 0;\n\n\t\t for (i++; str[i]; i++) {\n\t\t\t if (q && str[i] == q)\n\t\t\t\t break;\n\t\t\t if (!q && (str[i] == ')' || str[i] == '&' ||\n\t\t\t\t    str[i] == '|'))\n\t\t\t\t break;\n\t\t }\n\t\t /* Skip quotes */\n\t\t if (q)\n\t\t\t s++;\n\t\tlen = i - s;\n\t\tif (len >= MAX_FILTER_STR_VAL) {\n\t\t\tparse_error(pe, FILT_ERR_OPERAND_TOO_LONG, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tpred->regex.len = len;\n\t\tstrncpy(pred->regex.pattern, str + s, len);\n\t\tpred->regex.pattern[len] = 0;\n\n\t/* This is either a string, or an integer */\n\t} else if (str[i] == '\\'' || str[i] == '\"') {\n\t\tchar q = str[i];\n\n\t\t/* Make sure the op is OK for strings */\n\t\tswitch (op) {\n\t\tcase OP_NE:\n\t\t\tpred->not = 1;\n\t\t\t/* Fall through */\n\t\tcase OP_GLOB:\n\t\tcase OP_EQ:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tparse_error(pe, FILT_ERR_ILLEGAL_FIELD_OP, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* Make sure the field is OK for strings */\n\t\tif (!is_string_field(field)) {\n\t\t\tparse_error(pe, FILT_ERR_EXPECT_DIGIT, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tfor (i++; str[i]; i++) {\n\t\t\tif (str[i] == q)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!str[i]) {\n\t\t\tparse_error(pe, FILT_ERR_MISSING_QUOTE, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* Skip quotes */\n\t\ts++;\n\t\tlen = i - s;\n\t\tif (len >= MAX_FILTER_STR_VAL) {\n\t\t\tparse_error(pe, FILT_ERR_OPERAND_TOO_LONG, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tpred->regex.len = len;\n\t\tstrncpy(pred->regex.pattern, str + s, len);\n\t\tpred->regex.pattern[len] = 0;\n\n\t\tfilter_build_regex(pred);\n\n\t\tif (field->filter_type == FILTER_COMM) {\n\t\t\tpred->fn = filter_pred_comm;\n\n\t\t} else if (field->filter_type == FILTER_STATIC_STRING) {\n\t\t\tpred->fn = filter_pred_string;\n\t\t\tpred->regex.field_len = field->size;\n\n\t\t} else if (field->filter_type == FILTER_DYN_STRING)\n\t\t\tpred->fn = filter_pred_strloc;\n\t\telse\n\t\t\tpred->fn = filter_pred_pchar;\n\t\t/* go past the last quote */\n\t\ti++;\n\n\t} else if (isdigit(str[i])) {\n\n\t\t/* Make sure the field is not a string */\n\t\tif (is_string_field(field)) {\n\t\t\tparse_error(pe, FILT_ERR_EXPECT_STRING, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (op == OP_GLOB) {\n\t\t\tparse_error(pe, FILT_ERR_ILLEGAL_FIELD_OP, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* We allow 0xDEADBEEF */\n\t\twhile (isalnum(str[i]))\n\t\t\ti++;\n\n\t\tlen = i - s;\n\t\t/* 0xfeedfacedeadbeef is 18 chars max */\n\t\tif (len >= sizeof(num_buf)) {\n\t\t\tparse_error(pe, FILT_ERR_OPERAND_TOO_LONG, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tstrncpy(num_buf, str + s, len);\n\t\tnum_buf[len] = 0;\n\n\t\t/* Make sure it is a value */\n\t\tif (field->is_signed)\n\t\t\tret = kstrtoll(num_buf, 0, &val);\n\t\telse\n\t\t\tret = kstrtoull(num_buf, 0, &val);\n\t\tif (ret) {\n\t\t\tparse_error(pe, FILT_ERR_ILLEGAL_INTVAL, pos + s);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tpred->val = val;\n\n\t\tif (field->filter_type == FILTER_CPU)\n\t\t\tpred->fn = filter_pred_cpu;\n\t\telse {\n\t\t\tpred->fn = select_comparison_fn(pred->op, field->size,\n\t\t\t\t\t\t\tfield->is_signed);\n\t\t\tif (pred->op == OP_NE)\n\t\t\t\tpred->not = 1;\n\t\t}\n\n\t} else {\n\t\tparse_error(pe, FILT_ERR_INVALID_VALUE, pos + i);\n\t\tgoto err_free;\n\t}\n\n\t*pred_ptr = pred;\n\treturn i;\n\nerr_free:\n\tkfree(pred);\n\treturn -EINVAL;\n}\n\nenum {\n\tTOO_MANY_CLOSE\t\t= -1,\n\tTOO_MANY_OPEN\t\t= -2,\n\tMISSING_QUOTE\t\t= -3,\n};\n\n/*\n * Read the filter string once to calculate the number of predicates\n * as well as how deep the parentheses go.\n *\n * Returns:\n *   0 - everything is fine (err is undefined)\n *  -1 - too many ')'\n *  -2 - too many '('\n *  -3 - No matching quote\n */\nstatic int calc_stack(const char *str, int *parens, int *preds, int *err)\n{\n\tbool is_pred = false;\n\tint nr_preds = 0;\n\tint open = 1; /* Count the expression as \"(E)\" */\n\tint last_quote = 0;\n\tint max_open = 1;\n\tint quote = 0;\n\tint i;\n\n\t*err = 0;\n\n\tfor (i = 0; str[i]; i++) {\n\t\tif (isspace(str[i]))\n\t\t\tcontinue;\n\t\tif (quote) {\n\t\t\tif (str[i] == quote)\n\t\t\t       quote = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (str[i]) {\n\t\tcase '\\'':\n\t\tcase '\"':\n\t\t\tquote = str[i];\n\t\t\tlast_quote = i;\n\t\t\tbreak;\n\t\tcase '|':\n\t\tcase '&':\n\t\t\tif (str[i+1] != str[i])\n\t\t\t\tbreak;\n\t\t\tis_pred = false;\n\t\t\tcontinue;\n\t\tcase '(':\n\t\t\tis_pred = false;\n\t\t\topen++;\n\t\t\tif (open > max_open)\n\t\t\t\tmax_open = open;\n\t\t\tcontinue;\n\t\tcase ')':\n\t\t\tis_pred = false;\n\t\t\tif (open == 1) {\n\t\t\t\t*err = i;\n\t\t\t\treturn TOO_MANY_CLOSE;\n\t\t\t}\n\t\t\topen--;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!is_pred) {\n\t\t\tnr_preds++;\n\t\t\tis_pred = true;\n\t\t}\n\t}\n\n\tif (quote) {\n\t\t*err = last_quote;\n\t\treturn MISSING_QUOTE;\n\t}\n\n\tif (open != 1) {\n\t\tint level = open;\n\n\t\t/* find the bad open */\n\t\tfor (i--; i; i--) {\n\t\t\tif (quote) {\n\t\t\t\tif (str[i] == quote)\n\t\t\t\t\tquote = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tswitch (str[i]) {\n\t\t\tcase '(':\n\t\t\t\tif (level == open) {\n\t\t\t\t\t*err = i;\n\t\t\t\t\treturn TOO_MANY_OPEN;\n\t\t\t\t}\n\t\t\t\tlevel--;\n\t\t\t\tbreak;\n\t\t\tcase ')':\n\t\t\t\tlevel++;\n\t\t\t\tbreak;\n\t\t\tcase '\\'':\n\t\t\tcase '\"':\n\t\t\t\tquote = str[i];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t/* First character is the '(' with missing ')' */\n\t\t*err = 0;\n\t\treturn TOO_MANY_OPEN;\n\t}\n\n\t/* Set the size of the required stacks */\n\t*parens = max_open;\n\t*preds = nr_preds;\n\treturn 0;\n}\n\nstatic int process_preds(struct trace_event_call *call,\n\t\t\t const char *filter_string,\n\t\t\t struct event_filter *filter,\n\t\t\t struct filter_parse_error *pe)\n{\n\tstruct prog_entry *prog;\n\tint nr_parens;\n\tint nr_preds;\n\tint index;\n\tint ret;\n\n\tret = calc_stack(filter_string, &nr_parens, &nr_preds, &index);\n\tif (ret < 0) {\n\t\tswitch (ret) {\n\t\tcase MISSING_QUOTE:\n\t\t\tparse_error(pe, FILT_ERR_MISSING_QUOTE, index);\n\t\t\tbreak;\n\t\tcase TOO_MANY_OPEN:\n\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_OPEN, index);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_CLOSE, index);\n\t\t}\n\t\treturn ret;\n\t}\n\n\tif (!nr_preds)\n\t\treturn -EINVAL;\n\n\tprog = predicate_parse(filter_string, nr_parens, nr_preds,\n\t\t\t       parse_pred, call, pe);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\trcu_assign_pointer(filter->prog, prog);\n\treturn 0;\n}\n\nstatic inline void event_set_filtered_flag(struct trace_event_file *file)\n{\n\tunsigned long old_flags = file->flags;\n\n\tfile->flags |= EVENT_FILE_FL_FILTERED;\n\n\tif (old_flags != file->flags)\n\t\ttrace_buffered_event_enable();\n}\n\nstatic inline void event_set_filter(struct trace_event_file *file,\n\t\t\t\t    struct event_filter *filter)\n{\n\trcu_assign_pointer(file->filter, filter);\n}\n\nstatic inline void event_clear_filter(struct trace_event_file *file)\n{\n\tRCU_INIT_POINTER(file->filter, NULL);\n}\n\nstatic inline void\nevent_set_no_set_filter_flag(struct trace_event_file *file)\n{\n\tfile->flags |= EVENT_FILE_FL_NO_SET_FILTER;\n}\n\nstatic inline void\nevent_clear_no_set_filter_flag(struct trace_event_file *file)\n{\n\tfile->flags &= ~EVENT_FILE_FL_NO_SET_FILTER;\n}\n\nstatic inline bool\nevent_no_set_filter_flag(struct trace_event_file *file)\n{\n\tif (file->flags & EVENT_FILE_FL_NO_SET_FILTER)\n\t\treturn true;\n\n\treturn false;\n}\n\nstruct filter_list {\n\tstruct list_head\tlist;\n\tstruct event_filter\t*filter;\n};\n\nstatic int process_system_preds(struct trace_subsystem_dir *dir,\n\t\t\t\tstruct trace_array *tr,\n\t\t\t\tstruct filter_parse_error *pe,\n\t\t\t\tchar *filter_string)\n{\n\tstruct trace_event_file *file;\n\tstruct filter_list *filter_item;\n\tstruct event_filter *filter = NULL;\n\tstruct filter_list *tmp;\n\tLIST_HEAD(filter_list);\n\tbool fail = true;\n\tint err;\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\n\t\tif (file->system != dir)\n\t\t\tcontinue;\n\n\t\tfilter = kzalloc(sizeof(*filter), GFP_KERNEL);\n\t\tif (!filter)\n\t\t\tgoto fail_mem;\n\n\t\tfilter->filter_string = kstrdup(filter_string, GFP_KERNEL);\n\t\tif (!filter->filter_string)\n\t\t\tgoto fail_mem;\n\n\t\terr = process_preds(file->event_call, filter_string, filter, pe);\n\t\tif (err) {\n\t\t\tfilter_disable(file);\n\t\t\tparse_error(pe, FILT_ERR_BAD_SUBSYS_FILTER, 0);\n\t\t\tappend_filter_err(pe, filter);\n\t\t} else\n\t\t\tevent_set_filtered_flag(file);\n\n\n\t\tfilter_item = kzalloc(sizeof(*filter_item), GFP_KERNEL);\n\t\tif (!filter_item)\n\t\t\tgoto fail_mem;\n\n\t\tlist_add_tail(&filter_item->list, &filter_list);\n\t\t/*\n\t\t * Regardless of if this returned an error, we still\n\t\t * replace the filter for the call.\n\t\t */\n\t\tfilter_item->filter = event_filter(file);\n\t\tevent_set_filter(file, filter);\n\t\tfilter = NULL;\n\n\t\tfail = false;\n\t}\n\n\tif (fail)\n\t\tgoto fail;\n\n\t/*\n\t * The calls can still be using the old filters.\n\t * Do a synchronize_sched() to ensure all calls are\n\t * done with them before we free them.\n\t */\n\tsynchronize_sched();\n\tlist_for_each_entry_safe(filter_item, tmp, &filter_list, list) {\n\t\t__free_filter(filter_item->filter);\n\t\tlist_del(&filter_item->list);\n\t\tkfree(filter_item);\n\t}\n\treturn 0;\n fail:\n\t/* No call succeeded */\n\tlist_for_each_entry_safe(filter_item, tmp, &filter_list, list) {\n\t\tlist_del(&filter_item->list);\n\t\tkfree(filter_item);\n\t}\n\tparse_error(pe, FILT_ERR_BAD_SUBSYS_FILTER, 0);\n\treturn -EINVAL;\n fail_mem:\n\tkfree(filter);\n\t/* If any call succeeded, we still need to sync */\n\tif (!fail)\n\t\tsynchronize_sched();\n\tlist_for_each_entry_safe(filter_item, tmp, &filter_list, list) {\n\t\t__free_filter(filter_item->filter);\n\t\tlist_del(&filter_item->list);\n\t\tkfree(filter_item);\n\t}\n\treturn -ENOMEM;\n}\n\nstatic int create_filter_start(char *filter_string, bool set_str,\n\t\t\t       struct filter_parse_error **pse,\n\t\t\t       struct event_filter **filterp)\n{\n\tstruct event_filter *filter;\n\tstruct filter_parse_error *pe = NULL;\n\tint err = 0;\n\n\tif (WARN_ON_ONCE(*pse || *filterp))\n\t\treturn -EINVAL;\n\n\tfilter = kzalloc(sizeof(*filter), GFP_KERNEL);\n\tif (filter && set_str) {\n\t\tfilter->filter_string = kstrdup(filter_string, GFP_KERNEL);\n\t\tif (!filter->filter_string)\n\t\t\terr = -ENOMEM;\n\t}\n\n\tpe = kzalloc(sizeof(*pe), GFP_KERNEL);\n\n\tif (!filter || !pe || err) {\n\t\tkfree(pe);\n\t\t__free_filter(filter);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* we're committed to creating a new filter */\n\t*filterp = filter;\n\t*pse = pe;\n\n\treturn 0;\n}\n\nstatic void create_filter_finish(struct filter_parse_error *pe)\n{\n\tkfree(pe);\n}\n\n/**\n * create_filter - create a filter for a trace_event_call\n * @call: trace_event_call to create a filter for\n * @filter_str: filter string\n * @set_str: remember @filter_str and enable detailed error in filter\n * @filterp: out param for created filter (always updated on return)\n *\n * Creates a filter for @call with @filter_str.  If @set_str is %true,\n * @filter_str is copied and recorded in the new filter.\n *\n * On success, returns 0 and *@filterp points to the new filter.  On\n * failure, returns -errno and *@filterp may point to %NULL or to a new\n * filter.  In the latter case, the returned filter contains error\n * information if @set_str is %true and the caller is responsible for\n * freeing it.\n */\nstatic int create_filter(struct trace_event_call *call,\n\t\t\t char *filter_string, bool set_str,\n\t\t\t struct event_filter **filterp)\n{\n\tstruct filter_parse_error *pe = NULL;\n\tint err;\n\n\terr = create_filter_start(filter_string, set_str, &pe, filterp);\n\tif (err)\n\t\treturn err;\n\n\terr = process_preds(call, filter_string, *filterp, pe);\n\tif (err && set_str)\n\t\tappend_filter_err(pe, *filterp);\n\n\treturn err;\n}\n\nint create_event_filter(struct trace_event_call *call,\n\t\t\tchar *filter_str, bool set_str,\n\t\t\tstruct event_filter **filterp)\n{\n\treturn create_filter(call, filter_str, set_str, filterp);\n}\n\n/**\n * create_system_filter - create a filter for an event_subsystem\n * @system: event_subsystem to create a filter for\n * @filter_str: filter string\n * @filterp: out param for created filter (always updated on return)\n *\n * Identical to create_filter() except that it creates a subsystem filter\n * and always remembers @filter_str.\n */\nstatic int create_system_filter(struct trace_subsystem_dir *dir,\n\t\t\t\tstruct trace_array *tr,\n\t\t\t\tchar *filter_str, struct event_filter **filterp)\n{\n\tstruct filter_parse_error *pe = NULL;\n\tint err;\n\n\terr = create_filter_start(filter_str, true, &pe, filterp);\n\tif (!err) {\n\t\terr = process_system_preds(dir, tr, pe, filter_str);\n\t\tif (!err) {\n\t\t\t/* System filters just show a default message */\n\t\t\tkfree((*filterp)->filter_string);\n\t\t\t(*filterp)->filter_string = NULL;\n\t\t} else {\n\t\t\tappend_filter_err(pe, *filterp);\n\t\t}\n\t}\n\tcreate_filter_finish(pe);\n\n\treturn err;\n}\n\n/* caller must hold event_mutex */\nint apply_event_filter(struct trace_event_file *file, char *filter_string)\n{\n\tstruct trace_event_call *call = file->event_call;\n\tstruct event_filter *filter = NULL;\n\tint err;\n\n\tif (!strcmp(strstrip(filter_string), \"0\")) {\n\t\tfilter_disable(file);\n\t\tfilter = event_filter(file);\n\n\t\tif (!filter)\n\t\t\treturn 0;\n\n\t\tevent_clear_filter(file);\n\n\t\t/* Make sure the filter is not being used */\n\t\tsynchronize_sched();\n\t\t__free_filter(filter);\n\n\t\treturn 0;\n\t}\n\n\terr = create_filter(call, filter_string, true, &filter);\n\n\t/*\n\t * Always swap the call filter with the new filter\n\t * even if there was an error. If there was an error\n\t * in the filter, we disable the filter and show the error\n\t * string\n\t */\n\tif (filter) {\n\t\tstruct event_filter *tmp;\n\n\t\ttmp = event_filter(file);\n\t\tif (!err)\n\t\t\tevent_set_filtered_flag(file);\n\t\telse\n\t\t\tfilter_disable(file);\n\n\t\tevent_set_filter(file, filter);\n\n\t\tif (tmp) {\n\t\t\t/* Make sure the call is done with the filter */\n\t\t\tsynchronize_sched();\n\t\t\t__free_filter(tmp);\n\t\t}\n\t}\n\n\treturn err;\n}\n\nint apply_subsystem_event_filter(struct trace_subsystem_dir *dir,\n\t\t\t\t char *filter_string)\n{\n\tstruct event_subsystem *system = dir->subsystem;\n\tstruct trace_array *tr = dir->tr;\n\tstruct event_filter *filter = NULL;\n\tint err = 0;\n\n\tmutex_lock(&event_mutex);\n\n\t/* Make sure the system still has events */\n\tif (!dir->nr_events) {\n\t\terr = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!strcmp(strstrip(filter_string), \"0\")) {\n\t\tfilter_free_subsystem_preds(dir, tr);\n\t\tremove_filter_string(system->filter);\n\t\tfilter = system->filter;\n\t\tsystem->filter = NULL;\n\t\t/* Ensure all filters are no longer used */\n\t\tsynchronize_sched();\n\t\tfilter_free_subsystem_filters(dir, tr);\n\t\t__free_filter(filter);\n\t\tgoto out_unlock;\n\t}\n\n\terr = create_system_filter(dir, tr, filter_string, &filter);\n\tif (filter) {\n\t\t/*\n\t\t * No event actually uses the system filter\n\t\t * we can free it without synchronize_sched().\n\t\t */\n\t\t__free_filter(system->filter);\n\t\tsystem->filter = filter;\n\t}\nout_unlock:\n\tmutex_unlock(&event_mutex);\n\n\treturn err;\n}\n\n#ifdef CONFIG_PERF_EVENTS\n\nvoid ftrace_profile_free_filter(struct perf_event *event)\n{\n\tstruct event_filter *filter = event->filter;\n\n\tevent->filter = NULL;\n\t__free_filter(filter);\n}\n\nstruct function_filter_data {\n\tstruct ftrace_ops *ops;\n\tint first_filter;\n\tint first_notrace;\n};\n\n#ifdef CONFIG_FUNCTION_TRACER\nstatic char **\nftrace_function_filter_re(char *buf, int len, int *count)\n{\n\tchar *str, **re;\n\n\tstr = kstrndup(buf, len, GFP_KERNEL);\n\tif (!str)\n\t\treturn NULL;\n\n\t/*\n\t * The argv_split function takes white space\n\t * as a separator, so convert ',' into spaces.\n\t */\n\tstrreplace(str, ',', ' ');\n\n\tre = argv_split(GFP_KERNEL, str, count);\n\tkfree(str);\n\treturn re;\n}\n\nstatic int ftrace_function_set_regexp(struct ftrace_ops *ops, int filter,\n\t\t\t\t      int reset, char *re, int len)\n{\n\tint ret;\n\n\tif (filter)\n\t\tret = ftrace_set_filter(ops, re, len, reset);\n\telse\n\t\tret = ftrace_set_notrace(ops, re, len, reset);\n\n\treturn ret;\n}\n\nstatic int __ftrace_function_set_filter(int filter, char *buf, int len,\n\t\t\t\t\tstruct function_filter_data *data)\n{\n\tint i, re_cnt, ret = -EINVAL;\n\tint *reset;\n\tchar **re;\n\n\treset = filter ? &data->first_filter : &data->first_notrace;\n\n\t/*\n\t * The 'ip' field could have multiple filters set, separated\n\t * either by space or comma. We first cut the filter and apply\n\t * all pieces separatelly.\n\t */\n\tre = ftrace_function_filter_re(buf, len, &re_cnt);\n\tif (!re)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < re_cnt; i++) {\n\t\tret = ftrace_function_set_regexp(data->ops, filter, *reset,\n\t\t\t\t\t\t re[i], strlen(re[i]));\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (*reset)\n\t\t\t*reset = 0;\n\t}\n\n\targv_free(re);\n\treturn ret;\n}\n\nstatic int ftrace_function_check_pred(struct filter_pred *pred)\n{\n\tstruct ftrace_event_field *field = pred->field;\n\n\t/*\n\t * Check the predicate for function trace, verify:\n\t *  - only '==' and '!=' is used\n\t *  - the 'ip' field is used\n\t */\n\tif ((pred->op != OP_EQ) && (pred->op != OP_NE))\n\t\treturn -EINVAL;\n\n\tif (strcmp(field->name, \"ip\"))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int ftrace_function_set_filter_pred(struct filter_pred *pred,\n\t\t\t\t\t   struct function_filter_data *data)\n{\n\tint ret;\n\n\t/* Checking the node is valid for function trace. */\n\tret = ftrace_function_check_pred(pred);\n\tif (ret)\n\t\treturn ret;\n\n\treturn __ftrace_function_set_filter(pred->op == OP_EQ,\n\t\t\t\t\t    pred->regex.pattern,\n\t\t\t\t\t    pred->regex.len,\n\t\t\t\t\t    data);\n}\n\nstatic bool is_or(struct prog_entry *prog, int i)\n{\n\tint target;\n\n\t/*\n\t * Only \"||\" is allowed for function events, thus,\n\t * all true branches should jump to true, and any\n\t * false branch should jump to false.\n\t */\n\ttarget = prog[i].target + 1;\n\t/* True and false have NULL preds (all prog entries should jump to one */\n\tif (prog[target].pred)\n\t\treturn false;\n\n\t/* prog[target].target is 1 for TRUE, 0 for FALSE */\n\treturn prog[i].when_to_branch == prog[target].target;\n}\n\nstatic int ftrace_function_set_filter(struct perf_event *event,\n\t\t\t\t      struct event_filter *filter)\n{\n\tstruct prog_entry *prog = rcu_dereference_protected(filter->prog,\n\t\t\t\t\t\tlockdep_is_held(&event_mutex));\n\tstruct function_filter_data data = {\n\t\t.first_filter  = 1,\n\t\t.first_notrace = 1,\n\t\t.ops           = &event->ftrace_ops,\n\t};\n\tint i;\n\n\tfor (i = 0; prog[i].pred; i++) {\n\t\tstruct filter_pred *pred = prog[i].pred;\n\n\t\tif (!is_or(prog, i))\n\t\t\treturn -EINVAL;\n\n\t\tif (ftrace_function_set_filter_pred(pred, &data) < 0)\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n#else\nstatic int ftrace_function_set_filter(struct perf_event *event,\n\t\t\t\t      struct event_filter *filter)\n{\n\treturn -ENODEV;\n}\n#endif /* CONFIG_FUNCTION_TRACER */\n\nint ftrace_profile_set_filter(struct perf_event *event, int event_id,\n\t\t\t      char *filter_str)\n{\n\tint err;\n\tstruct event_filter *filter = NULL;\n\tstruct trace_event_call *call;\n\n\tmutex_lock(&event_mutex);\n\n\tcall = event->tp_event;\n\n\terr = -EINVAL;\n\tif (!call)\n\t\tgoto out_unlock;\n\n\terr = -EEXIST;\n\tif (event->filter)\n\t\tgoto out_unlock;\n\n\terr = create_filter(call, filter_str, false, &filter);\n\tif (err)\n\t\tgoto free_filter;\n\n\tif (ftrace_event_is_function(call))\n\t\terr = ftrace_function_set_filter(event, filter);\n\telse\n\t\tevent->filter = filter;\n\nfree_filter:\n\tif (err || ftrace_event_is_function(call))\n\t\t__free_filter(filter);\n\nout_unlock:\n\tmutex_unlock(&event_mutex);\n\n\treturn err;\n}\n\n#endif /* CONFIG_PERF_EVENTS */\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\n\n#include <linux/types.h>\n#include <linux/tracepoint.h>\n\n#define CREATE_TRACE_POINTS\n#include \"trace_events_filter_test.h\"\n\n#define DATA_REC(m, va, vb, vc, vd, ve, vf, vg, vh, nvisit) \\\n{ \\\n\t.filter = FILTER, \\\n\t.rec    = { .a = va, .b = vb, .c = vc, .d = vd, \\\n\t\t    .e = ve, .f = vf, .g = vg, .h = vh }, \\\n\t.match  = m, \\\n\t.not_visited = nvisit, \\\n}\n#define YES 1\n#define NO  0\n\nstatic struct test_filter_data_t {\n\tchar *filter;\n\tstruct trace_event_raw_ftrace_test_filter rec;\n\tint match;\n\tchar *not_visited;\n} test_filter_data[] = {\n#define FILTER \"a == 1 && b == 1 && c == 1 && d == 1 && \" \\\n\t       \"e == 1 && f == 1 && g == 1 && h == 1\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 1, 1, \"\"),\n\tDATA_REC(NO,  0, 1, 1, 1, 1, 1, 1, 1, \"bcdefgh\"),\n\tDATA_REC(NO,  1, 1, 1, 1, 1, 1, 1, 0, \"\"),\n#undef FILTER\n#define FILTER \"a == 1 || b == 1 || c == 1 || d == 1 || \" \\\n\t       \"e == 1 || f == 1 || g == 1 || h == 1\"\n\tDATA_REC(NO,  0, 0, 0, 0, 0, 0, 0, 0, \"\"),\n\tDATA_REC(YES, 0, 0, 0, 0, 0, 0, 0, 1, \"\"),\n\tDATA_REC(YES, 1, 0, 0, 0, 0, 0, 0, 0, \"bcdefgh\"),\n#undef FILTER\n#define FILTER \"(a == 1 || b == 1) && (c == 1 || d == 1) && \" \\\n\t       \"(e == 1 || f == 1) && (g == 1 || h == 1)\"\n\tDATA_REC(NO,  0, 0, 1, 1, 1, 1, 1, 1, \"dfh\"),\n\tDATA_REC(YES, 0, 1, 0, 1, 0, 1, 0, 1, \"\"),\n\tDATA_REC(YES, 1, 0, 1, 0, 0, 1, 0, 1, \"bd\"),\n\tDATA_REC(NO,  1, 0, 1, 0, 0, 1, 0, 0, \"bd\"),\n#undef FILTER\n#define FILTER \"(a == 1 && b == 1) || (c == 1 && d == 1) || \" \\\n\t       \"(e == 1 && f == 1) || (g == 1 && h == 1)\"\n\tDATA_REC(YES, 1, 0, 1, 1, 1, 1, 1, 1, \"efgh\"),\n\tDATA_REC(YES, 0, 0, 0, 0, 0, 0, 1, 1, \"\"),\n\tDATA_REC(NO,  0, 0, 0, 0, 0, 0, 0, 1, \"\"),\n#undef FILTER\n#define FILTER \"(a == 1 && b == 1) && (c == 1 && d == 1) && \" \\\n\t       \"(e == 1 && f == 1) || (g == 1 && h == 1)\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 0, 0, \"gh\"),\n\tDATA_REC(NO,  0, 0, 0, 0, 0, 0, 0, 1, \"\"),\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 0, 1, 1, \"\"),\n#undef FILTER\n#define FILTER \"((a == 1 || b == 1) || (c == 1 || d == 1) || \" \\\n\t       \"(e == 1 || f == 1)) && (g == 1 || h == 1)\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 0, 1, \"bcdef\"),\n\tDATA_REC(NO,  0, 0, 0, 0, 0, 0, 0, 0, \"\"),\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 0, 1, 1, \"h\"),\n#undef FILTER\n#define FILTER \"((((((((a == 1) && (b == 1)) || (c == 1)) && (d == 1)) || \" \\\n\t       \"(e == 1)) && (f == 1)) || (g == 1)) && (h == 1))\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 1, 1, \"ceg\"),\n\tDATA_REC(NO,  0, 1, 0, 1, 0, 1, 0, 1, \"\"),\n\tDATA_REC(NO,  1, 0, 1, 0, 1, 0, 1, 0, \"\"),\n#undef FILTER\n#define FILTER \"((((((((a == 1) || (b == 1)) && (c == 1)) || (d == 1)) && \" \\\n\t       \"(e == 1)) || (f == 1)) && (g == 1)) || (h == 1))\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 1, 1, \"bdfh\"),\n\tDATA_REC(YES, 0, 1, 0, 1, 0, 1, 0, 1, \"\"),\n\tDATA_REC(YES, 1, 0, 1, 0, 1, 0, 1, 0, \"bdfh\"),\n};\n\n#undef DATA_REC\n#undef FILTER\n#undef YES\n#undef NO\n\n#define DATA_CNT ARRAY_SIZE(test_filter_data)\n\nstatic int test_pred_visited;\n\nstatic int test_pred_visited_fn(struct filter_pred *pred, void *event)\n{\n\tstruct ftrace_event_field *field = pred->field;\n\n\ttest_pred_visited = 1;\n\tprintk(KERN_INFO \"\\npred visited %s\\n\", field->name);\n\treturn 1;\n}\n\nstatic void update_pred_fn(struct event_filter *filter, char *fields)\n{\n\tstruct prog_entry *prog = rcu_dereference_protected(filter->prog,\n\t\t\t\t\t\tlockdep_is_held(&event_mutex));\n\tint i;\n\n\tfor (i = 0; prog[i].pred; i++) {\n\t\tstruct filter_pred *pred = prog[i].pred;\n\t\tstruct ftrace_event_field *field = pred->field;\n\n\t\tWARN_ON_ONCE(!pred->fn);\n\n\t\tif (!field) {\n\t\t\tWARN_ONCE(1, \"all leafs should have field defined %d\", i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strchr(fields, *field->name))\n\t\t\tcontinue;\n\n\t\tpred->fn = test_pred_visited_fn;\n\t}\n}\n\nstatic __init int ftrace_test_event_filter(void)\n{\n\tint i;\n\n\tprintk(KERN_INFO \"Testing ftrace filter: \");\n\n\tfor (i = 0; i < DATA_CNT; i++) {\n\t\tstruct event_filter *filter = NULL;\n\t\tstruct test_filter_data_t *d = &test_filter_data[i];\n\t\tint err;\n\n\t\terr = create_filter(&event_ftrace_test_filter, d->filter,\n\t\t\t\t    false, &filter);\n\t\tif (err) {\n\t\t\tprintk(KERN_INFO\n\t\t\t       \"Failed to get filter for '%s', err %d\\n\",\n\t\t\t       d->filter, err);\n\t\t\t__free_filter(filter);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Needed to dereference filter->prog */\n\t\tmutex_lock(&event_mutex);\n\t\t/*\n\t\t * The preemption disabling is not really needed for self\n\t\t * tests, but the rcu dereference will complain without it.\n\t\t */\n\t\tpreempt_disable();\n\t\tif (*d->not_visited)\n\t\t\tupdate_pred_fn(filter, d->not_visited);\n\n\t\ttest_pred_visited = 0;\n\t\terr = filter_match_preds(filter, &d->rec);\n\t\tpreempt_enable();\n\n\t\tmutex_unlock(&event_mutex);\n\n\t\t__free_filter(filter);\n\n\t\tif (test_pred_visited) {\n\t\t\tprintk(KERN_INFO\n\t\t\t       \"Failed, unwanted pred visited for filter %s\\n\",\n\t\t\t       d->filter);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (err != d->match) {\n\t\t\tprintk(KERN_INFO\n\t\t\t       \"Failed to match filter '%s', expected %d\\n\",\n\t\t\t       d->filter, d->match);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == DATA_CNT)\n\t\tprintk(KERN_CONT \"OK\\n\");\n\n\treturn 0;\n}\n\nlate_initcall(ftrace_test_event_filter);\n\n#endif /* CONFIG_FTRACE_STARTUP_TEST */\n", "# SPDX-License-Identifier: GPL-2.0\n# ==========================================================================\n# Building\n# ==========================================================================\n\nsrc := $(obj)\n\nPHONY := __build\n__build:\n\n# Init all relevant variables used in kbuild files so\n# 1) they have correct type\n# 2) they do not inherit any value from the environment\nobj-y :=\nobj-m :=\nlib-y :=\nlib-m :=\nalways :=\ntargets :=\nsubdir-y :=\nsubdir-m :=\nEXTRA_AFLAGS   :=\nEXTRA_CFLAGS   :=\nEXTRA_CPPFLAGS :=\nEXTRA_LDFLAGS  :=\nasflags-y  :=\nccflags-y  :=\ncppflags-y :=\nldflags-y  :=\n\nsubdir-asflags-y :=\nsubdir-ccflags-y :=\n\n# Read auto.conf if it exists, otherwise ignore\n-include include/config/auto.conf\n\ninclude scripts/Kbuild.include\n\n# For backward compatibility check that these variables do not change\nsave-cflags := $(CFLAGS)\n\n# The filename Kbuild has precedence over Makefile\nkbuild-dir := $(if $(filter /%,$(src)),$(src),$(srctree)/$(src))\nkbuild-file := $(if $(wildcard $(kbuild-dir)/Kbuild),$(kbuild-dir)/Kbuild,$(kbuild-dir)/Makefile)\ninclude $(kbuild-file)\n\n# If the save-* variables changed error out\nifeq ($(KBUILD_NOPEDANTIC),)\n        ifneq (\"$(save-cflags)\",\"$(CFLAGS)\")\n                $(error CFLAGS was changed in \"$(kbuild-file)\". Fix it to use ccflags-y)\n        endif\nendif\n\ninclude scripts/Makefile.lib\n\nifdef host-progs\nifneq ($(hostprogs-y),$(host-progs))\n$(warning kbuild: $(obj)/Makefile - Usage of host-progs is deprecated. Please replace with hostprogs-y!)\nhostprogs-y += $(host-progs)\nendif\nendif\n\n# Do not include host rules unless needed\nifneq ($(hostprogs-y)$(hostprogs-m)$(hostlibs-y)$(hostlibs-m)$(hostcxxlibs-y)$(hostcxxlibs-m),)\ninclude scripts/Makefile.host\nendif\n\nifndef obj\n$(warning kbuild: Makefile.build is included improperly)\nendif\n\n# ===========================================================================\n\nifneq ($(strip $(lib-y) $(lib-m) $(lib-)),)\nlib-target := $(obj)/lib.a\nreal-obj-y += $(obj)/lib-ksyms.o\nendif\n\nifneq ($(strip $(real-obj-y) $(need-builtin)),)\nbuiltin-target := $(obj)/built-in.a\nendif\n\nmodorder-target := $(obj)/modules.order\n\n# We keep a list of all modules in $(MODVERDIR)\n\n__build: $(if $(KBUILD_BUILTIN),$(builtin-target) $(lib-target) $(extra-y)) \\\n\t $(if $(KBUILD_MODULES),$(obj-m) $(modorder-target)) \\\n\t $(subdir-ym) $(always)\n\t@:\n\n# Linus' kernel sanity checking tool\nifneq ($(KBUILD_CHECKSRC),0)\n  ifeq ($(KBUILD_CHECKSRC),2)\n    quiet_cmd_force_checksrc = CHECK   $<\n          cmd_force_checksrc = $(CHECK) $(CHECKFLAGS) $(c_flags) $< ;\n  else\n      quiet_cmd_checksrc     = CHECK   $<\n            cmd_checksrc     = $(CHECK) $(CHECKFLAGS) $(c_flags) $< ;\n  endif\nendif\n\nifneq ($(KBUILD_ENABLE_EXTRA_GCC_CHECKS),)\n  cmd_checkdoc = $(srctree)/scripts/kernel-doc -none $< ;\nendif\n\n# Do section mismatch analysis for each module/built-in.a\nifdef CONFIG_DEBUG_SECTION_MISMATCH\n  cmd_secanalysis = ; scripts/mod/modpost $@\nendif\n\n# Compile C sources (.c)\n# ---------------------------------------------------------------------------\n\n# Default is built-in, unless we know otherwise\nmodkern_cflags =                                          \\\n\t$(if $(part-of-module),                           \\\n\t\t$(KBUILD_CFLAGS_MODULE) $(CFLAGS_MODULE), \\\n\t\t$(KBUILD_CFLAGS_KERNEL) $(CFLAGS_KERNEL))\nquiet_modtag := $(empty)   $(empty)\n\n$(real-obj-m)        : part-of-module := y\n$(real-obj-m:.o=.i)  : part-of-module := y\n$(real-obj-m:.o=.s)  : part-of-module := y\n$(real-obj-m:.o=.lst): part-of-module := y\n\n$(real-obj-m)        : quiet_modtag := [M]\n$(real-obj-m:.o=.i)  : quiet_modtag := [M]\n$(real-obj-m:.o=.s)  : quiet_modtag := [M]\n$(real-obj-m:.o=.lst): quiet_modtag := [M]\n\n$(obj-m)             : quiet_modtag := [M]\n\nquiet_cmd_cc_s_c = CC $(quiet_modtag)  $@\ncmd_cc_s_c       = $(CC) $(c_flags) $(DISABLE_LTO) -fverbose-asm -S -o $@ $<\n\n$(obj)/%.s: $(src)/%.c FORCE\n\t$(call if_changed_dep,cc_s_c)\n\nquiet_cmd_cpp_i_c = CPP $(quiet_modtag) $@\ncmd_cpp_i_c       = $(CPP) $(c_flags) -o $@ $<\n\n$(obj)/%.i: $(src)/%.c FORCE\n\t$(call if_changed_dep,cpp_i_c)\n\n# These mirror gensymtypes_S and co below, keep them in synch.\ncmd_gensymtypes_c =                                                         \\\n    $(CPP) -D__GENKSYMS__ $(c_flags) $< |                                   \\\n    $(GENKSYMS) $(if $(1), -T $(2))                                         \\\n     $(patsubst y,-R,$(CONFIG_MODULE_REL_CRCS))                             \\\n     $(if $(KBUILD_PRESERVE),-p)                                            \\\n     -r $(firstword $(wildcard $(2:.symtypes=.symref) /dev/null))\n\nquiet_cmd_cc_symtypes_c = SYM $(quiet_modtag) $@\ncmd_cc_symtypes_c =                                                         \\\n    set -e;                                                                 \\\n    $(call cmd_gensymtypes_c,true,$@) >/dev/null;                           \\\n    test -s $@ || rm -f $@\n\n$(obj)/%.symtypes : $(src)/%.c FORCE\n\t$(call cmd,cc_symtypes_c)\n\n# LLVM assembly\n# Generate .ll files from .c\nquiet_cmd_cc_ll_c = CC $(quiet_modtag)  $@\n      cmd_cc_ll_c = $(CC) $(c_flags) -emit-llvm -S -o $@ $<\n\n$(obj)/%.ll: $(src)/%.c FORCE\n\t$(call if_changed_dep,cc_ll_c)\n\n# C (.c) files\n# The C file is compiled and updated dependency information is generated.\n# (See cmd_cc_o_c + relevant part of rule_cc_o_c)\n\nquiet_cmd_cc_o_c = CC $(quiet_modtag)  $@\n\nifndef CONFIG_MODVERSIONS\ncmd_cc_o_c = $(CC) $(c_flags) -c -o $@ $<\n\nelse\n# When module versioning is enabled the following steps are executed:\n# o compile a .tmp_<file>.o from <file>.c\n# o if .tmp_<file>.o doesn't contain a __ksymtab version, i.e. does\n#   not export symbols, we just rename .tmp_<file>.o to <file>.o and\n#   are done.\n# o otherwise, we calculate symbol versions using the good old\n#   genksyms on the preprocessed source and postprocess them in a way\n#   that they are usable as a linker script\n# o generate <file>.o from .tmp_<file>.o using the linker to\n#   replace the unresolved symbols __crc_exported_symbol with\n#   the actual value of the checksum generated by genksyms\n\ncmd_cc_o_c = $(CC) $(c_flags) -c -o $(@D)/.tmp_$(@F) $<\n\ncmd_modversions_c =\t\t\t\t\t\t\t\t\\\n\tif $(OBJDUMP) -h $(@D)/.tmp_$(@F) | grep -q __ksymtab; then\t\t\\\n\t\t$(call cmd_gensymtypes_c,$(KBUILD_SYMTYPES),$(@:.o=.symtypes))\t\\\n\t\t    > $(@D)/.tmp_$(@F:.o=.ver);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\t\t$(LD) $(LDFLAGS) -r -o $@ $(@D)/.tmp_$(@F) \t\t\t\\\n\t\t\t-T $(@D)/.tmp_$(@F:.o=.ver);\t\t\t\t\\\n\t\trm -f $(@D)/.tmp_$(@F) $(@D)/.tmp_$(@F:.o=.ver);\t\t\\\n\telse\t\t\t\t\t\t\t\t\t\\\n\t\tmv -f $(@D)/.tmp_$(@F) $@;\t\t\t\t\t\\\n\tfi;\nendif\n\nifdef CONFIG_FTRACE_MCOUNT_RECORD\n# gcc 5 supports generating the mcount tables directly\nifneq ($(call cc-option,-mrecord-mcount,y),y)\nKBUILD_CFLAGS += -mrecord-mcount\nelse\n# else do it all manually\nifdef BUILD_C_RECORDMCOUNT\nifeq (\"$(origin RECORDMCOUNT_WARN)\", \"command line\")\n  RECORDMCOUNT_FLAGS = -w\nendif\n# Due to recursion, we must skip empty.o.\n# The empty.o file is created in the make process in order to determine\n# the target endianness and word size. It is made before all other C\n# files, including recordmcount.\nsub_cmd_record_mcount =\t\t\t\t\t\\\n\tif [ $(@) != \"scripts/mod/empty.o\" ]; then\t\\\n\t\t$(objtree)/scripts/recordmcount $(RECORDMCOUNT_FLAGS) \"$(@)\";\t\\\n\tfi;\nrecordmcount_source := $(srctree)/scripts/recordmcount.c \\\n\t\t    $(srctree)/scripts/recordmcount.h\nelse\nsub_cmd_record_mcount = set -e ; perl $(srctree)/scripts/recordmcount.pl \"$(ARCH)\" \\\n\t\"$(if $(CONFIG_CPU_BIG_ENDIAN),big,little)\" \\\n\t\"$(if $(CONFIG_64BIT),64,32)\" \\\n\t\"$(OBJDUMP)\" \"$(OBJCOPY)\" \"$(CC) $(KBUILD_CFLAGS)\" \\\n\t\"$(LD)\" \"$(NM)\" \"$(RM)\" \"$(MV)\" \\\n\t\"$(if $(part-of-module),1,0)\" \"$(@)\";\nrecordmcount_source := $(srctree)/scripts/recordmcount.pl\nendif # BUILD_C_RECORDMCOUNT\ncmd_record_mcount =\t\t\t\t\t\t\\\n\tif [ \"$(findstring $(CC_FLAGS_FTRACE),$(_c_flags))\" =\t\\\n\t     \"$(CC_FLAGS_FTRACE)\" ]; then\t\t\t\\\n\t\t$(sub_cmd_record_mcount)\t\t\t\\\n\tfi;\nendif # CONFIG_FTRACE_MCOUNT_RECORD\n\nifdef CONFIG_STACK_VALIDATION\nifneq ($(SKIP_STACK_VALIDATION),1)\n\n__objtool_obj := $(objtree)/tools/objtool/objtool\n\nobjtool_args = $(if $(CONFIG_UNWINDER_ORC),orc generate,check)\n\nobjtool_args += $(if $(part-of-module), --module,)\n\nifndef CONFIG_FRAME_POINTER\nobjtool_args += --no-fp\nendif\nifdef CONFIG_GCOV_KERNEL\nobjtool_args += --no-unreachable\nelse\nobjtool_args += $(call cc-ifversion, -lt, 0405, --no-unreachable)\nendif\nifdef CONFIG_RETPOLINE\nifneq ($(RETPOLINE_CFLAGS),)\n  objtool_args += --retpoline\nendif\nendif\nendif\n\n\nifdef CONFIG_MODVERSIONS\nobjtool_o = $(@D)/.tmp_$(@F)\nelse\nobjtool_o = $(@)\nendif\n\n# 'OBJECT_FILES_NON_STANDARD := y': skip objtool checking for a directory\n# 'OBJECT_FILES_NON_STANDARD_foo.o := 'y': skip objtool checking for a file\n# 'OBJECT_FILES_NON_STANDARD_foo.o := 'n': override directory skip for a file\ncmd_objtool = $(if $(patsubst y%,, \\\n\t$(OBJECT_FILES_NON_STANDARD_$(basetarget).o)$(OBJECT_FILES_NON_STANDARD)n), \\\n\t$(__objtool_obj) $(objtool_args) \"$(objtool_o)\";)\nobjtool_obj = $(if $(patsubst y%,, \\\n\t$(OBJECT_FILES_NON_STANDARD_$(basetarget).o)$(OBJECT_FILES_NON_STANDARD)n), \\\n\t$(__objtool_obj))\n\nendif # SKIP_STACK_VALIDATION\nendif # CONFIG_STACK_VALIDATION\n\n# Rebuild all objects when objtool changes, or is enabled/disabled.\nobjtool_dep = $(objtool_obj)\t\t\t\t\t\\\n\t      $(wildcard include/config/orc/unwinder.h\t\t\\\n\t\t\t include/config/stack/validation.h)\n\ndefine rule_cc_o_c\n\t$(call echo-cmd,checksrc) $(cmd_checksrc)\t\t\t  \\\n\t$(call cmd_and_fixdep,cc_o_c)\t\t\t\t\t  \\\n\t$(cmd_checkdoc)\t\t\t\t\t\t\t  \\\n\t$(call echo-cmd,objtool) $(cmd_objtool)\t\t\t\t  \\\n\t$(cmd_modversions_c)\t\t\t\t\t\t  \\\n\t$(call echo-cmd,record_mcount) $(cmd_record_mcount)\nendef\n\ndefine rule_as_o_S\n\t$(call cmd_and_fixdep,as_o_S)\t\t\t\t\t  \\\n\t$(call echo-cmd,objtool) $(cmd_objtool)\t\t\t\t  \\\n\t$(cmd_modversions_S)\nendef\n\n# List module undefined symbols (or empty line if not enabled)\nifdef CONFIG_TRIM_UNUSED_KSYMS\ncmd_undef_syms = $(NM) $@ | sed -n 's/^  *U //p' | xargs echo\nelse\ncmd_undef_syms = echo\nendif\n\n# Built-in and composite module parts\n$(obj)/%.o: $(src)/%.c $(recordmcount_source) $(objtool_dep) FORCE\n\t$(call cmd,force_checksrc)\n\t$(call if_changed_rule,cc_o_c)\n\n# Single-part modules are special since we need to mark them in $(MODVERDIR)\n\n$(single-used-m): $(obj)/%.o: $(src)/%.c $(recordmcount_source) $(objtool_dep) FORCE\n\t$(call cmd,force_checksrc)\n\t$(call if_changed_rule,cc_o_c)\n\t@{ echo $(@:.o=.ko); echo $@; \\\n\t   $(cmd_undef_syms); } > $(MODVERDIR)/$(@F:.o=.mod)\n\nquiet_cmd_cc_lst_c = MKLST   $@\n      cmd_cc_lst_c = $(CC) $(c_flags) -g -c -o $*.o $< && \\\n\t\t     $(CONFIG_SHELL) $(srctree)/scripts/makelst $*.o \\\n\t\t\t\t     System.map $(OBJDUMP) > $@\n\n$(obj)/%.lst: $(src)/%.c FORCE\n\t$(call if_changed_dep,cc_lst_c)\n\n# Compile assembler sources (.S)\n# ---------------------------------------------------------------------------\n\nmodkern_aflags := $(KBUILD_AFLAGS_KERNEL) $(AFLAGS_KERNEL)\n\n$(real-obj-m)      : modkern_aflags := $(KBUILD_AFLAGS_MODULE) $(AFLAGS_MODULE)\n$(real-obj-m:.o=.s): modkern_aflags := $(KBUILD_AFLAGS_MODULE) $(AFLAGS_MODULE)\n\n# .S file exports must have their C prototypes defined in asm/asm-prototypes.h\n# or a file that it includes, in order to get versioned symbols. We build a\n# dummy C file that includes asm-prototypes and the EXPORT_SYMBOL lines from\n# the .S file (with trailing ';'), and run genksyms on that, to extract vers.\n#\n# This is convoluted. The .S file must first be preprocessed to run guards and\n# expand names, then the resulting exports must be constructed into plain\n# EXPORT_SYMBOL(symbol); to build our dummy C file, and that gets preprocessed\n# to make the genksyms input.\n#\n# These mirror gensymtypes_c and co above, keep them in synch.\ncmd_gensymtypes_S =                                                         \\\n    (echo \"\\#include <linux/kernel.h>\" ;                                    \\\n     echo \"\\#include <asm/asm-prototypes.h>\" ;                              \\\n    $(CPP) $(a_flags) $< |                                                  \\\n     grep \"\\<___EXPORT_SYMBOL\\>\" |                                          \\\n     sed 's/.*___EXPORT_SYMBOL[[:space:]]*\\([a-zA-Z0-9_]*\\)[[:space:]]*,.*/EXPORT_SYMBOL(\\1);/' ) | \\\n    $(CPP) -D__GENKSYMS__ $(c_flags) -xc - |                                \\\n    $(GENKSYMS) $(if $(1), -T $(2))                                         \\\n     $(patsubst y,-R,$(CONFIG_MODULE_REL_CRCS))                             \\\n     $(if $(KBUILD_PRESERVE),-p)                                            \\\n     -r $(firstword $(wildcard $(2:.symtypes=.symref) /dev/null))\n\nquiet_cmd_cc_symtypes_S = SYM $(quiet_modtag) $@\ncmd_cc_symtypes_S =                                                         \\\n    set -e;                                                                 \\\n    $(call cmd_gensymtypes_S,true,$@) >/dev/null;                           \\\n    test -s $@ || rm -f $@\n\n$(obj)/%.symtypes : $(src)/%.S FORCE\n\t$(call cmd,cc_symtypes_S)\n\n\nquiet_cmd_cpp_s_S = CPP $(quiet_modtag) $@\ncmd_cpp_s_S       = $(CPP) $(a_flags) -o $@ $<\n\n$(obj)/%.s: $(src)/%.S FORCE\n\t$(call if_changed_dep,cpp_s_S)\n\nquiet_cmd_as_o_S = AS $(quiet_modtag)  $@\n\nifndef CONFIG_MODVERSIONS\ncmd_as_o_S = $(CC) $(a_flags) -c -o $@ $<\n\nelse\n\nASM_PROTOTYPES := $(wildcard $(srctree)/arch/$(SRCARCH)/include/asm/asm-prototypes.h)\n\nifeq ($(ASM_PROTOTYPES),)\ncmd_as_o_S = $(CC) $(a_flags) -c -o $@ $<\n\nelse\n\n# versioning matches the C process described above, with difference that\n# we parse asm-prototypes.h C header to get function definitions.\n\ncmd_as_o_S = $(CC) $(a_flags) -c -o $(@D)/.tmp_$(@F) $<\n\ncmd_modversions_S =\t\t\t\t\t\t\t\t\\\n\tif $(OBJDUMP) -h $(@D)/.tmp_$(@F) | grep -q __ksymtab; then\t\t\\\n\t\t$(call cmd_gensymtypes_S,$(KBUILD_SYMTYPES),$(@:.o=.symtypes))\t\\\n\t\t    > $(@D)/.tmp_$(@F:.o=.ver);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\t\t$(LD) $(LDFLAGS) -r -o $@ $(@D)/.tmp_$(@F) \t\t\t\\\n\t\t\t-T $(@D)/.tmp_$(@F:.o=.ver);\t\t\t\t\\\n\t\trm -f $(@D)/.tmp_$(@F) $(@D)/.tmp_$(@F:.o=.ver);\t\t\\\n\telse\t\t\t\t\t\t\t\t\t\\\n\t\tmv -f $(@D)/.tmp_$(@F) $@;\t\t\t\t\t\\\n\tfi;\nendif\nendif\n\n$(obj)/%.o: $(src)/%.S $(objtool_dep) FORCE\n\t$(call if_changed_rule,as_o_S)\n\ntargets += $(filter-out $(subdir-obj-y), $(real-obj-y)) $(real-obj-m) $(lib-y)\ntargets += $(extra-y) $(MAKECMDGOALS) $(always)\n\n# Linker scripts preprocessor (.lds.S -> .lds)\n# ---------------------------------------------------------------------------\nquiet_cmd_cpp_lds_S = LDS     $@\n      cmd_cpp_lds_S = $(CPP) $(cpp_flags) -P -U$(ARCH) \\\n\t                     -D__ASSEMBLY__ -DLINKER_SCRIPT -o $@ $<\n\n$(obj)/%.lds: $(src)/%.lds.S FORCE\n\t$(call if_changed_dep,cpp_lds_S)\n\n# ASN.1 grammar\n# ---------------------------------------------------------------------------\nquiet_cmd_asn1_compiler = ASN.1   $@\n      cmd_asn1_compiler = $(objtree)/scripts/asn1_compiler $< \\\n\t\t\t\t$(subst .h,.c,$@) $(subst .c,.h,$@)\n\n$(obj)/%.asn1.c $(obj)/%.asn1.h: $(src)/%.asn1 $(objtree)/scripts/asn1_compiler\n\t$(call cmd,asn1_compiler)\n\n# Build the compiled-in targets\n# ---------------------------------------------------------------------------\n\n# To build objects in subdirs, we need to descend into the directories\n$(sort $(subdir-obj-y)): $(subdir-ym) ;\n\n#\n# Rule to compile a set of .o files into one .o file\n#\nifdef builtin-target\n\n# built-in.a archives are made with no symbol table or index which\n# makes them small and fast, but unable to be used by the linker.\n# scripts/link-vmlinux.sh builds an aggregate built-in.a with a symbol\n# table and index.\nquiet_cmd_ar_builtin = AR      $@\n      cmd_ar_builtin = rm -f $@; \\\n                     $(AR) rcSTP$(KBUILD_ARFLAGS) $@ $(filter $(real-obj-y), $^)\n\n$(builtin-target): $(real-obj-y) FORCE\n\t$(call if_changed,ar_builtin)\n\ntargets += $(builtin-target)\nendif # builtin-target\n\n#\n# Rule to create modules.order file\n#\n# Create commands to either record .ko file or cat modules.order from\n# a subdirectory\nmodorder-cmds =\t\t\t\t\t\t\\\n\t$(foreach m, $(modorder),\t\t\t\\\n\t\t$(if $(filter %/modules.order, $m),\t\\\n\t\t\tcat $m;, echo kernel/$m;))\n\n$(modorder-target): $(subdir-ym) FORCE\n\t$(Q)(cat /dev/null; $(modorder-cmds)) > $@\n\n#\n# Rule to compile a set of .o files into one .a file\n#\nifdef lib-target\nquiet_cmd_link_l_target = AR      $@\n\n# lib target archives do get a symbol table and index\ncmd_link_l_target = rm -f $@; $(AR) rcsTP$(KBUILD_ARFLAGS) $@ $(lib-y)\n\n$(lib-target): $(lib-y) FORCE\n\t$(call if_changed,link_l_target)\n\ntargets += $(lib-target)\n\ndummy-object = $(obj)/.lib_exports.o\nksyms-lds = $(dot-target).lds\n\nquiet_cmd_export_list = EXPORTS $@\ncmd_export_list = $(OBJDUMP) -h $< | \\\n\tsed -ne '/___ksymtab/s/.*+\\([^ ]*\\).*/EXTERN(\\1)/p' >$(ksyms-lds);\\\n\trm -f $(dummy-object);\\\n\techo | $(CC) $(a_flags) -c -o $(dummy-object) -x assembler -;\\\n\t$(LD) $(ld_flags) -r -o $@ -T $(ksyms-lds) $(dummy-object);\\\n\trm $(dummy-object) $(ksyms-lds)\n\n$(obj)/lib-ksyms.o: $(lib-target) FORCE\n\t$(call if_changed,export_list)\n\ntargets += $(obj)/lib-ksyms.o\n\nendif\n\n#\n# Rule to link composite objects\n#\n#  Composite objects are specified in kbuild makefile as follows:\n#    <composite-object>-objs := <list of .o files>\n#  or\n#    <composite-object>-y    := <list of .o files>\n#  or\n#    <composite-object>-m    := <list of .o files>\n#  The -m syntax only works if <composite object> is a module\nlink_multi_deps =                     \\\n$(filter $(addprefix $(obj)/,         \\\n$($(subst $(obj)/,,$(@:.o=-objs)))    \\\n$($(subst $(obj)/,,$(@:.o=-y)))       \\\n$($(subst $(obj)/,,$(@:.o=-m)))), $^)\n\nquiet_cmd_link_multi-m = LD [M]  $@\ncmd_link_multi-m = $(LD) $(ld_flags) -r -o $@ $(link_multi_deps) $(cmd_secanalysis)\n\n$(multi-used-m): FORCE\n\t$(call if_changed,link_multi-m)\n\t@{ echo $(@:.o=.ko); echo $(link_multi_deps); \\\n\t   $(cmd_undef_syms); } > $(MODVERDIR)/$(@F:.o=.mod)\n$(call multi_depend, $(multi-used-m), .o, -objs -y -m)\n\ntargets += $(multi-used-m)\ntargets := $(filter-out $(PHONY), $(targets))\n\n# Add intermediate targets:\n# When building objects with specific suffix patterns, add intermediate\n# targets that the final targets are derived from.\nintermediate_targets = $(foreach sfx, $(2), \\\n\t\t\t\t$(patsubst %$(strip $(1)),%$(sfx), \\\n\t\t\t\t\t$(filter %$(strip $(1)), $(targets))))\n# %.asn1.o <- %.asn1.[ch] <- %.asn1\n# %.dtb.o <- %.dtb.S <- %.dtb <- %.dts\n# %.lex.o <- %.lex.c <- %.l\n# %.tab.o <- %.tab.[ch] <- %.y\ntargets += $(call intermediate_targets, .asn1.o, .asn1.c .asn1.h) \\\n\t   $(call intermediate_targets, .dtb.o, .dtb.S .dtb) \\\n\t   $(call intermediate_targets, .lex.o, .lex.c) \\\n\t   $(call intermediate_targets, .tab.o, .tab.c .tab.h)\n\n# Descending\n# ---------------------------------------------------------------------------\n\nPHONY += $(subdir-ym)\n$(subdir-ym):\n\t$(Q)$(MAKE) $(build)=$@ need-builtin=$(if $(findstring $@,$(subdir-obj-y)),1)\n\n# Add FORCE to the prequisites of a target to force it to be always rebuilt.\n# ---------------------------------------------------------------------------\n\nPHONY += FORCE\n\nFORCE:\n\n# Read all saved command lines and dependencies for the $(targets) we\n# may be building above, using $(if_changed{,_dep}). As an\n# optimization, we don't need to read them if the target does not\n# exist, we will rebuild anyway in that case.\n\ncmd_files := $(wildcard $(foreach f,$(sort $(targets)),$(dir $(f)).$(notdir $(f)).cmd))\n\nifneq ($(cmd_files),)\n  include $(cmd_files)\nendif\n\nifneq ($(KBUILD_SRC),)\n# Create directories for object files if they do not exist\nobj-dirs := $(sort $(obj) $(patsubst %/,%, $(dir $(targets))))\n# If cmd_files exist, their directories apparently exist.  Skip mkdir.\nexist-dirs := $(sort $(patsubst %/,%, $(dir $(cmd_files))))\nobj-dirs := $(strip $(filter-out $(exist-dirs), $(obj-dirs)))\nifneq ($(obj-dirs),)\n$(shell mkdir -p $(obj-dirs))\nendif\nendif\n\n# Some files contained in $(targets) are intermediate artifacts.\n# We never want them to be removed automatically.\n.SECONDARY: $(targets)\n\n# Declare the contents of the .PHONY variable as phony.  We keep that\n# information in a variable se we can use it in if_changed and friends.\n\n.PHONY: $(PHONY)\n"], "fixing_code": ["\t\t\t     Event Histograms\n\n\t\t    Documentation written by Tom Zanussi\n\n1. Introduction\n===============\n\n  Histogram triggers are special event triggers that can be used to\n  aggregate trace event data into histograms.  For information on\n  trace events and event triggers, see Documentation/trace/events.rst.\n\n\n2. Histogram Trigger Command\n============================\n\n  A histogram trigger command is an event trigger command that\n  aggregates event hits into a hash table keyed on one or more trace\n  event format fields (or stacktrace) and a set of running totals\n  derived from one or more trace event format fields and/or event\n  counts (hitcount).\n\n  The format of a hist trigger is as follows:\n\n        hist:keys=<field1[,field2,...]>[:values=<field1[,field2,...]>]\n          [:sort=<field1[,field2,...]>][:size=#entries][:pause][:continue]\n          [:clear][:name=histname1] [if <filter>]\n\n  When a matching event is hit, an entry is added to a hash table\n  using the key(s) and value(s) named.  Keys and values correspond to\n  fields in the event's format description.  Values must correspond to\n  numeric fields - on an event hit, the value(s) will be added to a\n  sum kept for that field.  The special string 'hitcount' can be used\n  in place of an explicit value field - this is simply a count of\n  event hits.  If 'values' isn't specified, an implicit 'hitcount'\n  value will be automatically created and used as the only value.\n  Keys can be any field, or the special string 'stacktrace', which\n  will use the event's kernel stacktrace as the key.  The keywords\n  'keys' or 'key' can be used to specify keys, and the keywords\n  'values', 'vals', or 'val' can be used to specify values.  Compound\n  keys consisting of up to two fields can be specified by the 'keys'\n  keyword.  Hashing a compound key produces a unique entry in the\n  table for each unique combination of component keys, and can be\n  useful for providing more fine-grained summaries of event data.\n  Additionally, sort keys consisting of up to two fields can be\n  specified by the 'sort' keyword.  If more than one field is\n  specified, the result will be a 'sort within a sort': the first key\n  is taken to be the primary sort key and the second the secondary\n  key.  If a hist trigger is given a name using the 'name' parameter,\n  its histogram data will be shared with other triggers of the same\n  name, and trigger hits will update this common data.  Only triggers\n  with 'compatible' fields can be combined in this way; triggers are\n  'compatible' if the fields named in the trigger share the same\n  number and type of fields and those fields also have the same names.\n  Note that any two events always share the compatible 'hitcount' and\n  'stacktrace' fields and can therefore be combined using those\n  fields, however pointless that may be.\n\n  'hist' triggers add a 'hist' file to each event's subdirectory.\n  Reading the 'hist' file for the event will dump the hash table in\n  its entirety to stdout.  If there are multiple hist triggers\n  attached to an event, there will be a table for each trigger in the\n  output.  The table displayed for a named trigger will be the same as\n  any other instance having the same name. Each printed hash table\n  entry is a simple list of the keys and values comprising the entry;\n  keys are printed first and are delineated by curly braces, and are\n  followed by the set of value fields for the entry.  By default,\n  numeric fields are displayed as base-10 integers.  This can be\n  modified by appending any of the following modifiers to the field\n  name:\n\n        .hex        display a number as a hex value\n\t.sym        display an address as a symbol\n\t.sym-offset display an address as a symbol and offset\n\t.syscall    display a syscall id as a system call name\n\t.execname   display a common_pid as a program name\n\t.log2       display log2 value rather than raw number\n\t.usecs      display a common_timestamp in microseconds\n\n  Note that in general the semantics of a given field aren't\n  interpreted when applying a modifier to it, but there are some\n  restrictions to be aware of in this regard:\n\n    - only the 'hex' modifier can be used for values (because values\n      are essentially sums, and the other modifiers don't make sense\n      in that context).\n    - the 'execname' modifier can only be used on a 'common_pid'.  The\n      reason for this is that the execname is simply the 'comm' value\n      saved for the 'current' process when an event was triggered,\n      which is the same as the common_pid value saved by the event\n      tracing code.  Trying to apply that comm value to other pid\n      values wouldn't be correct, and typically events that care save\n      pid-specific comm fields in the event itself.\n\n  A typical usage scenario would be the following to enable a hist\n  trigger, read its current contents, and then turn it off:\n\n  # echo 'hist:keys=skbaddr.hex:vals=len' > \\\n    /sys/kernel/debug/tracing/events/net/netif_rx/trigger\n\n  # cat /sys/kernel/debug/tracing/events/net/netif_rx/hist\n\n  # echo '!hist:keys=skbaddr.hex:vals=len' > \\\n    /sys/kernel/debug/tracing/events/net/netif_rx/trigger\n\n  The trigger file itself can be read to show the details of the\n  currently attached hist trigger.  This information is also displayed\n  at the top of the 'hist' file when read.\n\n  By default, the size of the hash table is 2048 entries.  The 'size'\n  parameter can be used to specify more or fewer than that.  The units\n  are in terms of hashtable entries - if a run uses more entries than\n  specified, the results will show the number of 'drops', the number\n  of hits that were ignored.  The size should be a power of 2 between\n  128 and 131072 (any non- power-of-2 number specified will be rounded\n  up).\n\n  The 'sort' parameter can be used to specify a value field to sort\n  on.  The default if unspecified is 'hitcount' and the default sort\n  order is 'ascending'.  To sort in the opposite direction, append\n  .descending' to the sort key.\n\n  The 'pause' parameter can be used to pause an existing hist trigger\n  or to start a hist trigger but not log any events until told to do\n  so.  'continue' or 'cont' can be used to start or restart a paused\n  hist trigger.\n\n  The 'clear' parameter will clear the contents of a running hist\n  trigger and leave its current paused/active state.\n\n  Note that the 'pause', 'cont', and 'clear' parameters should be\n  applied using 'append' shell operator ('>>') if applied to an\n  existing trigger, rather than via the '>' operator, which will cause\n  the trigger to be removed through truncation.\n\n- enable_hist/disable_hist\n\n  The enable_hist and disable_hist triggers can be used to have one\n  event conditionally start and stop another event's already-attached\n  hist trigger.  Any number of enable_hist and disable_hist triggers\n  can be attached to a given event, allowing that event to kick off\n  and stop aggregations on a host of other events.\n\n  The format is very similar to the enable/disable_event triggers:\n\n      enable_hist:<system>:<event>[:count]\n      disable_hist:<system>:<event>[:count]\n\n  Instead of enabling or disabling the tracing of the target event\n  into the trace buffer as the enable/disable_event triggers do, the\n  enable/disable_hist triggers enable or disable the aggregation of\n  the target event into a hash table.\n\n  A typical usage scenario for the enable_hist/disable_hist triggers\n  would be to first set up a paused hist trigger on some event,\n  followed by an enable_hist/disable_hist pair that turns the hist\n  aggregation on and off when conditions of interest are hit:\n\n  # echo 'hist:keys=skbaddr.hex:vals=len:pause' > \\\n    /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n\n  # echo 'enable_hist:net:netif_receive_skb if filename==/usr/bin/wget' > \\\n    /sys/kernel/debug/tracing/events/sched/sched_process_exec/trigger\n\n  # echo 'disable_hist:net:netif_receive_skb if comm==wget' > \\\n    /sys/kernel/debug/tracing/events/sched/sched_process_exit/trigger\n\n  The above sets up an initially paused hist trigger which is unpaused\n  and starts aggregating events when a given program is executed, and\n  which stops aggregating when the process exits and the hist trigger\n  is paused again.\n\n  The examples below provide a more concrete illustration of the\n  concepts and typical usage patterns discussed above.\n\n  'special' event fields\n  ------------------------\n\n  There are a number of 'special event fields' available for use as\n  keys or values in a hist trigger.  These look like and behave as if\n  they were actual event fields, but aren't really part of the event's\n  field definition or format file.  They are however available for any\n  event, and can be used anywhere an actual event field could be.\n  They are:\n\n    common_timestamp       u64 - timestamp (from ring buffer) associated\n                                 with the event, in nanoseconds.  May be\n\t\t\t\t modified by .usecs to have timestamps\n\t\t\t\t interpreted as microseconds.\n    cpu                    int - the cpu on which the event occurred.\n\n  Extended error information\n  --------------------------\n\n  For some error conditions encountered when invoking a hist trigger\n  command, extended error information is available via the\n  corresponding event's 'hist' file.  Reading the hist file after an\n  error will display more detailed information about what went wrong,\n  if information is available.  This extended error information will\n  be available until the next hist trigger command for that event.\n\n  If available for a given error condition, the extended error\n  information and usage takes the following form:\n\n    # echo xxx > /sys/kernel/debug/tracing/events/sched/sched_wakeup/trigger\n    echo: write error: Invalid argument\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_wakeup/hist\n    ERROR: Couldn't yyy: zzz\n      Last command: xxx\n\n6.2 'hist' trigger examples\n---------------------------\n\n  The first set of examples creates aggregations using the kmalloc\n  event.  The fields that can be used for the hist trigger are listed\n  in the kmalloc event's format file:\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/format\n    name: kmalloc\n    ID: 374\n    format:\n\tfield:unsigned short common_type;\toffset:0;\tsize:2;\tsigned:0;\n\tfield:unsigned char common_flags;\toffset:2;\tsize:1;\tsigned:0;\n\tfield:unsigned char common_preempt_count;\t\toffset:3;\tsize:1;\tsigned:0;\n\tfield:int common_pid;\t\t\t\t\toffset:4;\tsize:4;\tsigned:1;\n\n\tfield:unsigned long call_site;\t\t\t\toffset:8;\tsize:8;\tsigned:0;\n\tfield:const void * ptr;\t\t\t\t\toffset:16;\tsize:8;\tsigned:0;\n\tfield:size_t bytes_req;\t\t\t\t\toffset:24;\tsize:8;\tsigned:0;\n\tfield:size_t bytes_alloc;\t\t\t\toffset:32;\tsize:8;\tsigned:0;\n\tfield:gfp_t gfp_flags;\t\t\t\t\toffset:40;\tsize:4;\tsigned:0;\n\n  We'll start by creating a hist trigger that generates a simple table\n  that lists the total number of bytes requested for each function in\n  the kernel that made one or more calls to kmalloc:\n\n    # echo 'hist:key=call_site:val=bytes_req' > \\\n            /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n  This tells the tracing system to create a 'hist' trigger using the\n  call_site field of the kmalloc event as the key for the table, which\n  just means that each unique call_site address will have an entry\n  created for it in the table.  The 'val=bytes_req' parameter tells\n  the hist trigger that for each unique entry (call_site) in the\n  table, it should keep a running total of the number of bytes\n  requested by that call_site.\n\n  We'll let it run for awhile and then dump the contents of the 'hist'\n  file in the kmalloc event's subdirectory (for readability, a number\n  of entries have been omitted):\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site:vals=bytes_req:sort=hitcount:size=2048 [active]\n\n    { call_site: 18446744072106379007 } hitcount:          1  bytes_req:        176\n    { call_site: 18446744071579557049 } hitcount:          1  bytes_req:       1024\n    { call_site: 18446744071580608289 } hitcount:          1  bytes_req:      16384\n    { call_site: 18446744071581827654 } hitcount:          1  bytes_req:         24\n    { call_site: 18446744071580700980 } hitcount:          1  bytes_req:          8\n    { call_site: 18446744071579359876 } hitcount:          1  bytes_req:        152\n    { call_site: 18446744071580795365 } hitcount:          3  bytes_req:        144\n    { call_site: 18446744071581303129 } hitcount:          3  bytes_req:        144\n    { call_site: 18446744071580713234 } hitcount:          4  bytes_req:       2560\n    { call_site: 18446744071580933750 } hitcount:          4  bytes_req:        736\n    .\n    .\n    .\n    { call_site: 18446744072106047046 } hitcount:         69  bytes_req:       5576\n    { call_site: 18446744071582116407 } hitcount:         73  bytes_req:       2336\n    { call_site: 18446744072106054684 } hitcount:        136  bytes_req:     140504\n    { call_site: 18446744072106224230 } hitcount:        136  bytes_req:      19584\n    { call_site: 18446744072106078074 } hitcount:        153  bytes_req:       2448\n    { call_site: 18446744072106062406 } hitcount:        153  bytes_req:      36720\n    { call_site: 18446744071582507929 } hitcount:        153  bytes_req:      37088\n    { call_site: 18446744072102520590 } hitcount:        273  bytes_req:      10920\n    { call_site: 18446744071582143559 } hitcount:        358  bytes_req:        716\n    { call_site: 18446744072106465852 } hitcount:        417  bytes_req:      56712\n    { call_site: 18446744072102523378 } hitcount:        485  bytes_req:      27160\n    { call_site: 18446744072099568646 } hitcount:       1676  bytes_req:      33520\n\n    Totals:\n        Hits: 4610\n        Entries: 45\n        Dropped: 0\n\n  The output displays a line for each entry, beginning with the key\n  specified in the trigger, followed by the value(s) also specified in\n  the trigger.  At the beginning of the output is a line that displays\n  the trigger info, which can also be displayed by reading the\n  'trigger' file:\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n    hist:keys=call_site:vals=bytes_req:sort=hitcount:size=2048 [active]\n\n  At the end of the output are a few lines that display the overall\n  totals for the run.  The 'Hits' field shows the total number of\n  times the event trigger was hit, the 'Entries' field shows the total\n  number of used entries in the hash table, and the 'Dropped' field\n  shows the number of hits that were dropped because the number of\n  used entries for the run exceeded the maximum number of entries\n  allowed for the table (normally 0, but if not a hint that you may\n  want to increase the size of the table using the 'size' parameter).\n\n  Notice in the above output that there's an extra field, 'hitcount',\n  which wasn't specified in the trigger.  Also notice that in the\n  trigger info output, there's a parameter, 'sort=hitcount', which\n  wasn't specified in the trigger either.  The reason for that is that\n  every trigger implicitly keeps a count of the total number of hits\n  attributed to a given entry, called the 'hitcount'.  That hitcount\n  information is explicitly displayed in the output, and in the\n  absence of a user-specified sort parameter, is used as the default\n  sort field.\n\n  The value 'hitcount' can be used in place of an explicit value in\n  the 'values' parameter if you don't really need to have any\n  particular field summed and are mainly interested in hit\n  frequencies.\n\n  To turn the hist trigger off, simply call up the trigger in the\n  command history and re-execute it with a '!' prepended:\n\n    # echo '!hist:key=call_site:val=bytes_req' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n  Finally, notice that the call_site as displayed in the output above\n  isn't really very useful.  It's an address, but normally addresses\n  are displayed in hex.  To have a numeric field displayed as a hex\n  value, simply append '.hex' to the field name in the trigger:\n\n    # echo 'hist:key=call_site.hex:val=bytes_req' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.hex:vals=bytes_req:sort=hitcount:size=2048 [active]\n\n    { call_site: ffffffffa026b291 } hitcount:          1  bytes_req:        433\n    { call_site: ffffffffa07186ff } hitcount:          1  bytes_req:        176\n    { call_site: ffffffff811ae721 } hitcount:          1  bytes_req:      16384\n    { call_site: ffffffff811c5134 } hitcount:          1  bytes_req:          8\n    { call_site: ffffffffa04a9ebb } hitcount:          1  bytes_req:        511\n    { call_site: ffffffff8122e0a6 } hitcount:          1  bytes_req:         12\n    { call_site: ffffffff8107da84 } hitcount:          1  bytes_req:        152\n    { call_site: ffffffff812d8246 } hitcount:          1  bytes_req:         24\n    { call_site: ffffffff811dc1e5 } hitcount:          3  bytes_req:        144\n    { call_site: ffffffffa02515e8 } hitcount:          3  bytes_req:        648\n    { call_site: ffffffff81258159 } hitcount:          3  bytes_req:        144\n    { call_site: ffffffff811c80f4 } hitcount:          4  bytes_req:        544\n    .\n    .\n    .\n    { call_site: ffffffffa06c7646 } hitcount:        106  bytes_req:       8024\n    { call_site: ffffffffa06cb246 } hitcount:        132  bytes_req:      31680\n    { call_site: ffffffffa06cef7a } hitcount:        132  bytes_req:       2112\n    { call_site: ffffffff8137e399 } hitcount:        132  bytes_req:      23232\n    { call_site: ffffffffa06c941c } hitcount:        185  bytes_req:     171360\n    { call_site: ffffffffa06f2a66 } hitcount:        185  bytes_req:      26640\n    { call_site: ffffffffa036a70e } hitcount:        265  bytes_req:      10600\n    { call_site: ffffffff81325447 } hitcount:        292  bytes_req:        584\n    { call_site: ffffffffa072da3c } hitcount:        446  bytes_req:      60656\n    { call_site: ffffffffa036b1f2 } hitcount:        526  bytes_req:      29456\n    { call_site: ffffffffa0099c06 } hitcount:       1780  bytes_req:      35600\n\n    Totals:\n        Hits: 4775\n        Entries: 46\n        Dropped: 0\n\n  Even that's only marginally more useful - while hex values do look\n  more like addresses, what users are typically more interested in\n  when looking at text addresses are the corresponding symbols\n  instead.  To have an address displayed as symbolic value instead,\n  simply append '.sym' or '.sym-offset' to the field name in the\n  trigger:\n\n    # echo 'hist:key=call_site.sym:val=bytes_req' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.sym:vals=bytes_req:sort=hitcount:size=2048 [active]\n\n    { call_site: [ffffffff810adcb9] syslog_print_all                              } hitcount:          1  bytes_req:       1024\n    { call_site: [ffffffff8154bc62] usb_control_msg                               } hitcount:          1  bytes_req:          8\n    { call_site: [ffffffffa00bf6fe] hidraw_send_report [hid]                      } hitcount:          1  bytes_req:          7\n    { call_site: [ffffffff8154acbe] usb_alloc_urb                                 } hitcount:          1  bytes_req:        192\n    { call_site: [ffffffffa00bf1ca] hidraw_report_event [hid]                     } hitcount:          1  bytes_req:          7\n    { call_site: [ffffffff811e3a25] __seq_open_private                            } hitcount:          1  bytes_req:         40\n    { call_site: [ffffffff8109524a] alloc_fair_sched_group                        } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffff811febd5] fsnotify_alloc_group                          } hitcount:          2  bytes_req:        528\n    { call_site: [ffffffff81440f58] __tty_buffer_request_room                     } hitcount:          2  bytes_req:       2624\n    { call_site: [ffffffff81200ba6] inotify_new_group                             } hitcount:          2  bytes_req:         96\n    { call_site: [ffffffffa05e19af] ieee80211_start_tx_ba_session [mac80211]      } hitcount:          2  bytes_req:        464\n    { call_site: [ffffffff81672406] tcp_get_metrics                               } hitcount:          2  bytes_req:        304\n    { call_site: [ffffffff81097ec2] alloc_rt_sched_group                          } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffff81089b05] sched_create_group                            } hitcount:          2  bytes_req:       1424\n    .\n    .\n    .\n    { call_site: [ffffffffa04a580c] intel_crtc_page_flip [i915]                   } hitcount:       1185  bytes_req:     123240\n    { call_site: [ffffffffa0287592] drm_mode_page_flip_ioctl [drm]                } hitcount:       1185  bytes_req:     104280\n    { call_site: [ffffffffa04c4a3c] intel_plane_duplicate_state [i915]            } hitcount:       1402  bytes_req:     190672\n    { call_site: [ffffffff812891ca] ext4_find_extent                              } hitcount:       1518  bytes_req:     146208\n    { call_site: [ffffffffa029070e] drm_vma_node_allow [drm]                      } hitcount:       1746  bytes_req:      69840\n    { call_site: [ffffffffa045e7c4] i915_gem_do_execbuffer.isra.23 [i915]         } hitcount:       2021  bytes_req:     792312\n    { call_site: [ffffffffa02911f2] drm_modeset_lock_crtc [drm]                   } hitcount:       2592  bytes_req:     145152\n    { call_site: [ffffffffa0489a66] intel_ring_begin [i915]                       } hitcount:       2629  bytes_req:     378576\n    { call_site: [ffffffffa046041c] i915_gem_execbuffer2 [i915]                   } hitcount:       2629  bytes_req:    3783248\n    { call_site: [ffffffff81325607] apparmor_file_alloc_security                  } hitcount:       5192  bytes_req:      10384\n    { call_site: [ffffffffa00b7c06] hid_report_raw_event [hid]                    } hitcount:       5529  bytes_req:     110584\n    { call_site: [ffffffff8131ebf7] aa_alloc_task_context                         } hitcount:      21943  bytes_req:     702176\n    { call_site: [ffffffff8125847d] ext4_htree_store_dirent                       } hitcount:      55759  bytes_req:    5074265\n\n    Totals:\n        Hits: 109928\n        Entries: 71\n        Dropped: 0\n\n  Because the default sort key above is 'hitcount', the above shows a\n  the list of call_sites by increasing hitcount, so that at the bottom\n  we see the functions that made the most kmalloc calls during the\n  run.  If instead we we wanted to see the top kmalloc callers in\n  terms of the number of bytes requested rather than the number of\n  calls, and we wanted the top caller to appear at the top, we can use\n  the 'sort' parameter, along with the 'descending' modifier:\n\n    # echo 'hist:key=call_site.sym:val=bytes_req:sort=bytes_req.descending' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.sym:vals=bytes_req:sort=bytes_req.descending:size=2048 [active]\n\n    { call_site: [ffffffffa046041c] i915_gem_execbuffer2 [i915]                   } hitcount:       2186  bytes_req:    3397464\n    { call_site: [ffffffffa045e7c4] i915_gem_do_execbuffer.isra.23 [i915]         } hitcount:       1790  bytes_req:     712176\n    { call_site: [ffffffff8125847d] ext4_htree_store_dirent                       } hitcount:       8132  bytes_req:     513135\n    { call_site: [ffffffff811e2a1b] seq_buf_alloc                                 } hitcount:        106  bytes_req:     440128\n    { call_site: [ffffffffa0489a66] intel_ring_begin [i915]                       } hitcount:       2186  bytes_req:     314784\n    { call_site: [ffffffff812891ca] ext4_find_extent                              } hitcount:       2174  bytes_req:     208992\n    { call_site: [ffffffff811ae8e1] __kmalloc                                     } hitcount:          8  bytes_req:     131072\n    { call_site: [ffffffffa04c4a3c] intel_plane_duplicate_state [i915]            } hitcount:        859  bytes_req:     116824\n    { call_site: [ffffffffa02911f2] drm_modeset_lock_crtc [drm]                   } hitcount:       1834  bytes_req:     102704\n    { call_site: [ffffffffa04a580c] intel_crtc_page_flip [i915]                   } hitcount:        972  bytes_req:     101088\n    { call_site: [ffffffffa0287592] drm_mode_page_flip_ioctl [drm]                } hitcount:        972  bytes_req:      85536\n    { call_site: [ffffffffa00b7c06] hid_report_raw_event [hid]                    } hitcount:       3333  bytes_req:      66664\n    { call_site: [ffffffff8137e559] sg_kmalloc                                    } hitcount:        209  bytes_req:      61632\n    .\n    .\n    .\n    { call_site: [ffffffff81095225] alloc_fair_sched_group                        } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffff81097ec2] alloc_rt_sched_group                          } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffff812d8406] copy_semundo                                  } hitcount:          2  bytes_req:         48\n    { call_site: [ffffffff81200ba6] inotify_new_group                             } hitcount:          1  bytes_req:         48\n    { call_site: [ffffffffa027121a] drm_getmagic [drm]                            } hitcount:          1  bytes_req:         48\n    { call_site: [ffffffff811e3a25] __seq_open_private                            } hitcount:          1  bytes_req:         40\n    { call_site: [ffffffff811c52f4] bprm_change_interp                            } hitcount:          2  bytes_req:         16\n    { call_site: [ffffffff8154bc62] usb_control_msg                               } hitcount:          1  bytes_req:          8\n    { call_site: [ffffffffa00bf1ca] hidraw_report_event [hid]                     } hitcount:          1  bytes_req:          7\n    { call_site: [ffffffffa00bf6fe] hidraw_send_report [hid]                      } hitcount:          1  bytes_req:          7\n\n    Totals:\n        Hits: 32133\n        Entries: 81\n        Dropped: 0\n\n  To display the offset and size information in addition to the symbol\n  name, just use 'sym-offset' instead:\n\n    # echo 'hist:key=call_site.sym-offset:val=bytes_req:sort=bytes_req.descending' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.sym-offset:vals=bytes_req:sort=bytes_req.descending:size=2048 [active]\n\n    { call_site: [ffffffffa046041c] i915_gem_execbuffer2+0x6c/0x2c0 [i915]                  } hitcount:       4569  bytes_req:    3163720\n    { call_site: [ffffffffa0489a66] intel_ring_begin+0xc6/0x1f0 [i915]                      } hitcount:       4569  bytes_req:     657936\n    { call_site: [ffffffffa045e7c4] i915_gem_do_execbuffer.isra.23+0x694/0x1020 [i915]      } hitcount:       1519  bytes_req:     472936\n    { call_site: [ffffffffa045e646] i915_gem_do_execbuffer.isra.23+0x516/0x1020 [i915]      } hitcount:       3050  bytes_req:     211832\n    { call_site: [ffffffff811e2a1b] seq_buf_alloc+0x1b/0x50                                 } hitcount:         34  bytes_req:     148384\n    { call_site: [ffffffffa04a580c] intel_crtc_page_flip+0xbc/0x870 [i915]                  } hitcount:       1385  bytes_req:     144040\n    { call_site: [ffffffff811ae8e1] __kmalloc+0x191/0x1b0                                   } hitcount:          8  bytes_req:     131072\n    { call_site: [ffffffffa0287592] drm_mode_page_flip_ioctl+0x282/0x360 [drm]              } hitcount:       1385  bytes_req:     121880\n    { call_site: [ffffffffa02911f2] drm_modeset_lock_crtc+0x32/0x100 [drm]                  } hitcount:       1848  bytes_req:     103488\n    { call_site: [ffffffffa04c4a3c] intel_plane_duplicate_state+0x2c/0xa0 [i915]            } hitcount:        461  bytes_req:      62696\n    { call_site: [ffffffffa029070e] drm_vma_node_allow+0x2e/0xd0 [drm]                      } hitcount:       1541  bytes_req:      61640\n    { call_site: [ffffffff815f8d7b] sk_prot_alloc+0xcb/0x1b0                                } hitcount:         57  bytes_req:      57456\n    .\n    .\n    .\n    { call_site: [ffffffff8109524a] alloc_fair_sched_group+0x5a/0x1a0                       } hitcount:          2  bytes_req:        128\n    { call_site: [ffffffffa027b921] drm_vm_open_locked+0x31/0xa0 [drm]                      } hitcount:          3  bytes_req:         96\n    { call_site: [ffffffff8122e266] proc_self_follow_link+0x76/0xb0                         } hitcount:          8  bytes_req:         96\n    { call_site: [ffffffff81213e80] load_elf_binary+0x240/0x1650                            } hitcount:          3  bytes_req:         84\n    { call_site: [ffffffff8154bc62] usb_control_msg+0x42/0x110                              } hitcount:          1  bytes_req:          8\n    { call_site: [ffffffffa00bf6fe] hidraw_send_report+0x7e/0x1a0 [hid]                     } hitcount:          1  bytes_req:          7\n    { call_site: [ffffffffa00bf1ca] hidraw_report_event+0x8a/0x120 [hid]                    } hitcount:          1  bytes_req:          7\n\n    Totals:\n        Hits: 26098\n        Entries: 64\n        Dropped: 0\n\n  We can also add multiple fields to the 'values' parameter.  For\n  example, we might want to see the total number of bytes allocated\n  alongside bytes requested, and display the result sorted by bytes\n  allocated in a descending order:\n\n    # echo 'hist:keys=call_site.sym:values=bytes_req,bytes_alloc:sort=bytes_alloc.descending' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=call_site.sym:vals=bytes_req,bytes_alloc:sort=bytes_alloc.descending:size=2048 [active]\n\n    { call_site: [ffffffffa046041c] i915_gem_execbuffer2 [i915]                   } hitcount:       7403  bytes_req:    4084360  bytes_alloc:    5958016\n    { call_site: [ffffffff811e2a1b] seq_buf_alloc                                 } hitcount:        541  bytes_req:    2213968  bytes_alloc:    2228224\n    { call_site: [ffffffffa0489a66] intel_ring_begin [i915]                       } hitcount:       7404  bytes_req:    1066176  bytes_alloc:    1421568\n    { call_site: [ffffffffa045e7c4] i915_gem_do_execbuffer.isra.23 [i915]         } hitcount:       1565  bytes_req:     557368  bytes_alloc:    1037760\n    { call_site: [ffffffff8125847d] ext4_htree_store_dirent                       } hitcount:       9557  bytes_req:     595778  bytes_alloc:     695744\n    { call_site: [ffffffffa045e646] i915_gem_do_execbuffer.isra.23 [i915]         } hitcount:       5839  bytes_req:     430680  bytes_alloc:     470400\n    { call_site: [ffffffffa04c4a3c] intel_plane_duplicate_state [i915]            } hitcount:       2388  bytes_req:     324768  bytes_alloc:     458496\n    { call_site: [ffffffffa02911f2] drm_modeset_lock_crtc [drm]                   } hitcount:       3911  bytes_req:     219016  bytes_alloc:     250304\n    { call_site: [ffffffff815f8d7b] sk_prot_alloc                                 } hitcount:        235  bytes_req:     236880  bytes_alloc:     240640\n    { call_site: [ffffffff8137e559] sg_kmalloc                                    } hitcount:        557  bytes_req:     169024  bytes_alloc:     221760\n    { call_site: [ffffffffa00b7c06] hid_report_raw_event [hid]                    } hitcount:       9378  bytes_req:     187548  bytes_alloc:     206312\n    { call_site: [ffffffffa04a580c] intel_crtc_page_flip [i915]                   } hitcount:       1519  bytes_req:     157976  bytes_alloc:     194432\n    .\n    .\n    .\n    { call_site: [ffffffff8109bd3b] sched_autogroup_create_attach                 } hitcount:          2  bytes_req:        144  bytes_alloc:        192\n    { call_site: [ffffffff81097ee8] alloc_rt_sched_group                          } hitcount:          2  bytes_req:        128  bytes_alloc:        128\n    { call_site: [ffffffff8109524a] alloc_fair_sched_group                        } hitcount:          2  bytes_req:        128  bytes_alloc:        128\n    { call_site: [ffffffff81095225] alloc_fair_sched_group                        } hitcount:          2  bytes_req:        128  bytes_alloc:        128\n    { call_site: [ffffffff81097ec2] alloc_rt_sched_group                          } hitcount:          2  bytes_req:        128  bytes_alloc:        128\n    { call_site: [ffffffff81213e80] load_elf_binary                               } hitcount:          3  bytes_req:         84  bytes_alloc:         96\n    { call_site: [ffffffff81079a2e] kthread_create_on_node                        } hitcount:          1  bytes_req:         56  bytes_alloc:         64\n    { call_site: [ffffffffa00bf6fe] hidraw_send_report [hid]                      } hitcount:          1  bytes_req:          7  bytes_alloc:          8\n    { call_site: [ffffffff8154bc62] usb_control_msg                               } hitcount:          1  bytes_req:          8  bytes_alloc:          8\n    { call_site: [ffffffffa00bf1ca] hidraw_report_event [hid]                     } hitcount:          1  bytes_req:          7  bytes_alloc:          8\n\n    Totals:\n        Hits: 66598\n        Entries: 65\n        Dropped: 0\n\n  Finally, to finish off our kmalloc example, instead of simply having\n  the hist trigger display symbolic call_sites, we can have the hist\n  trigger additionally display the complete set of kernel stack traces\n  that led to each call_site.  To do that, we simply use the special\n  value 'stacktrace' for the key parameter:\n\n    # echo 'hist:keys=stacktrace:values=bytes_req,bytes_alloc:sort=bytes_alloc' > \\\n           /sys/kernel/debug/tracing/events/kmem/kmalloc/trigger\n\n  The above trigger will use the kernel stack trace in effect when an\n  event is triggered as the key for the hash table.  This allows the\n  enumeration of every kernel callpath that led up to a particular\n  event, along with a running total of any of the event fields for\n  that event.  Here we tally bytes requested and bytes allocated for\n  every callpath in the system that led up to a kmalloc (in this case\n  every callpath to a kmalloc for a kernel compile):\n\n    # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/hist\n    # trigger info: hist:keys=stacktrace:vals=bytes_req,bytes_alloc:sort=bytes_alloc:size=2048 [active]\n\n    { stacktrace:\n         __kmalloc_track_caller+0x10b/0x1a0\n         kmemdup+0x20/0x50\n         hidraw_report_event+0x8a/0x120 [hid]\n         hid_report_raw_event+0x3ea/0x440 [hid]\n         hid_input_report+0x112/0x190 [hid]\n         hid_irq_in+0xc2/0x260 [usbhid]\n         __usb_hcd_giveback_urb+0x72/0x120\n         usb_giveback_urb_bh+0x9e/0xe0\n         tasklet_hi_action+0xf8/0x100\n         __do_softirq+0x114/0x2c0\n         irq_exit+0xa5/0xb0\n         do_IRQ+0x5a/0xf0\n         ret_from_intr+0x0/0x30\n         cpuidle_enter+0x17/0x20\n         cpu_startup_entry+0x315/0x3e0\n         rest_init+0x7c/0x80\n    } hitcount:          3  bytes_req:         21  bytes_alloc:         24\n    { stacktrace:\n         __kmalloc_track_caller+0x10b/0x1a0\n         kmemdup+0x20/0x50\n         hidraw_report_event+0x8a/0x120 [hid]\n         hid_report_raw_event+0x3ea/0x440 [hid]\n         hid_input_report+0x112/0x190 [hid]\n         hid_irq_in+0xc2/0x260 [usbhid]\n         __usb_hcd_giveback_urb+0x72/0x120\n         usb_giveback_urb_bh+0x9e/0xe0\n         tasklet_hi_action+0xf8/0x100\n         __do_softirq+0x114/0x2c0\n         irq_exit+0xa5/0xb0\n         do_IRQ+0x5a/0xf0\n         ret_from_intr+0x0/0x30\n    } hitcount:          3  bytes_req:         21  bytes_alloc:         24\n    { stacktrace:\n         kmem_cache_alloc_trace+0xeb/0x150\n         aa_alloc_task_context+0x27/0x40\n         apparmor_cred_prepare+0x1f/0x50\n         security_prepare_creds+0x16/0x20\n         prepare_creds+0xdf/0x1a0\n         SyS_capset+0xb5/0x200\n         system_call_fastpath+0x12/0x6a\n    } hitcount:          1  bytes_req:         32  bytes_alloc:         32\n    .\n    .\n    .\n    { stacktrace:\n         __kmalloc+0x11b/0x1b0\n         i915_gem_execbuffer2+0x6c/0x2c0 [i915]\n         drm_ioctl+0x349/0x670 [drm]\n         do_vfs_ioctl+0x2f0/0x4f0\n         SyS_ioctl+0x81/0xa0\n         system_call_fastpath+0x12/0x6a\n    } hitcount:      17726  bytes_req:   13944120  bytes_alloc:   19593808\n    { stacktrace:\n         __kmalloc+0x11b/0x1b0\n         load_elf_phdrs+0x76/0xa0\n         load_elf_binary+0x102/0x1650\n         search_binary_handler+0x97/0x1d0\n         do_execveat_common.isra.34+0x551/0x6e0\n         SyS_execve+0x3a/0x50\n         return_from_execve+0x0/0x23\n    } hitcount:      33348  bytes_req:   17152128  bytes_alloc:   20226048\n    { stacktrace:\n         kmem_cache_alloc_trace+0xeb/0x150\n         apparmor_file_alloc_security+0x27/0x40\n         security_file_alloc+0x16/0x20\n         get_empty_filp+0x93/0x1c0\n         path_openat+0x31/0x5f0\n         do_filp_open+0x3a/0x90\n         do_sys_open+0x128/0x220\n         SyS_open+0x1e/0x20\n         system_call_fastpath+0x12/0x6a\n    } hitcount:    4766422  bytes_req:    9532844  bytes_alloc:   38131376\n    { stacktrace:\n         __kmalloc+0x11b/0x1b0\n         seq_buf_alloc+0x1b/0x50\n         seq_read+0x2cc/0x370\n         proc_reg_read+0x3d/0x80\n         __vfs_read+0x28/0xe0\n         vfs_read+0x86/0x140\n         SyS_read+0x46/0xb0\n         system_call_fastpath+0x12/0x6a\n    } hitcount:      19133  bytes_req:   78368768  bytes_alloc:   78368768\n\n    Totals:\n        Hits: 6085872\n        Entries: 253\n        Dropped: 0\n\n  If you key a hist trigger on common_pid, in order for example to\n  gather and display sorted totals for each process, you can use the\n  special .execname modifier to display the executable names for the\n  processes in the table rather than raw pids.  The example below\n  keeps a per-process sum of total bytes read:\n\n    # echo 'hist:key=common_pid.execname:val=count:sort=count.descending' > \\\n           /sys/kernel/debug/tracing/events/syscalls/sys_enter_read/trigger\n\n    # cat /sys/kernel/debug/tracing/events/syscalls/sys_enter_read/hist\n    # trigger info: hist:keys=common_pid.execname:vals=count:sort=count.descending:size=2048 [active]\n\n    { common_pid: gnome-terminal  [      3196] } hitcount:        280  count:    1093512\n    { common_pid: Xorg            [      1309] } hitcount:        525  count:     256640\n    { common_pid: compiz          [      2889] } hitcount:         59  count:     254400\n    { common_pid: bash            [      8710] } hitcount:          3  count:      66369\n    { common_pid: dbus-daemon-lau [      8703] } hitcount:         49  count:      47739\n    { common_pid: irqbalance      [      1252] } hitcount:         27  count:      27648\n    { common_pid: 01ifupdown      [      8705] } hitcount:          3  count:      17216\n    { common_pid: dbus-daemon     [       772] } hitcount:         10  count:      12396\n    { common_pid: Socket Thread   [      8342] } hitcount:         11  count:      11264\n    { common_pid: nm-dhcp-client. [      8701] } hitcount:          6  count:       7424\n    { common_pid: gmain           [      1315] } hitcount:         18  count:       6336\n    .\n    .\n    .\n    { common_pid: postgres        [      1892] } hitcount:          2  count:         32\n    { common_pid: postgres        [      1891] } hitcount:          2  count:         32\n    { common_pid: gmain           [      8704] } hitcount:          2  count:         32\n    { common_pid: upstart-dbus-br [      2740] } hitcount:         21  count:         21\n    { common_pid: nm-dispatcher.a [      8696] } hitcount:          1  count:         16\n    { common_pid: indicator-datet [      2904] } hitcount:          1  count:         16\n    { common_pid: gdbus           [      2998] } hitcount:          1  count:         16\n    { common_pid: rtkit-daemon    [      2052] } hitcount:          1  count:          8\n    { common_pid: init            [         1] } hitcount:          2  count:          2\n\n    Totals:\n        Hits: 2116\n        Entries: 51\n        Dropped: 0\n\n  Similarly, if you key a hist trigger on syscall id, for example to\n  gather and display a list of systemwide syscall hits, you can use\n  the special .syscall modifier to display the syscall names rather\n  than raw ids.  The example below keeps a running total of syscall\n  counts for the system during the run:\n\n    # echo 'hist:key=id.syscall:val=hitcount' > \\\n           /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/trigger\n\n    # cat /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/hist\n    # trigger info: hist:keys=id.syscall:vals=hitcount:sort=hitcount:size=2048 [active]\n\n    { id: sys_fsync                     [ 74] } hitcount:          1\n    { id: sys_newuname                  [ 63] } hitcount:          1\n    { id: sys_prctl                     [157] } hitcount:          1\n    { id: sys_statfs                    [137] } hitcount:          1\n    { id: sys_symlink                   [ 88] } hitcount:          1\n    { id: sys_sendmmsg                  [307] } hitcount:          1\n    { id: sys_semctl                    [ 66] } hitcount:          1\n    { id: sys_readlink                  [ 89] } hitcount:          3\n    { id: sys_bind                      [ 49] } hitcount:          3\n    { id: sys_getsockname               [ 51] } hitcount:          3\n    { id: sys_unlink                    [ 87] } hitcount:          3\n    { id: sys_rename                    [ 82] } hitcount:          4\n    { id: unknown_syscall               [ 58] } hitcount:          4\n    { id: sys_connect                   [ 42] } hitcount:          4\n    { id: sys_getpid                    [ 39] } hitcount:          4\n    .\n    .\n    .\n    { id: sys_rt_sigprocmask            [ 14] } hitcount:        952\n    { id: sys_futex                     [202] } hitcount:       1534\n    { id: sys_write                     [  1] } hitcount:       2689\n    { id: sys_setitimer                 [ 38] } hitcount:       2797\n    { id: sys_read                      [  0] } hitcount:       3202\n    { id: sys_select                    [ 23] } hitcount:       3773\n    { id: sys_writev                    [ 20] } hitcount:       4531\n    { id: sys_poll                      [  7] } hitcount:       8314\n    { id: sys_recvmsg                   [ 47] } hitcount:      13738\n    { id: sys_ioctl                     [ 16] } hitcount:      21843\n\n    Totals:\n        Hits: 67612\n        Entries: 72\n        Dropped: 0\n\n    The syscall counts above provide a rough overall picture of system\n    call activity on the system; we can see for example that the most\n    popular system call on this system was the 'sys_ioctl' system call.\n\n    We can use 'compound' keys to refine that number and provide some\n    further insight as to which processes exactly contribute to the\n    overall ioctl count.\n\n    The command below keeps a hitcount for every unique combination of\n    system call id and pid - the end result is essentially a table\n    that keeps a per-pid sum of system call hits.  The results are\n    sorted using the system call id as the primary key, and the\n    hitcount sum as the secondary key:\n\n    # echo 'hist:key=id.syscall,common_pid.execname:val=hitcount:sort=id,hitcount' > \\\n           /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/trigger\n\n    # cat /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/hist\n    # trigger info: hist:keys=id.syscall,common_pid.execname:vals=hitcount:sort=id.syscall,hitcount:size=2048 [active]\n\n    { id: sys_read                      [  0], common_pid: rtkit-daemon    [      1877] } hitcount:          1\n    { id: sys_read                      [  0], common_pid: gdbus           [      2976] } hitcount:          1\n    { id: sys_read                      [  0], common_pid: console-kit-dae [      3400] } hitcount:          1\n    { id: sys_read                      [  0], common_pid: postgres        [      1865] } hitcount:          1\n    { id: sys_read                      [  0], common_pid: deja-dup-monito [      3543] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: NetworkManager  [       890] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: evolution-calen [      3048] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: postgres        [      1864] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: nm-applet       [      3022] } hitcount:          2\n    { id: sys_read                      [  0], common_pid: whoopsie        [      1212] } hitcount:          2\n    .\n    .\n    .\n    { id: sys_ioctl                     [ 16], common_pid: bash            [      8479] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: bash            [      3472] } hitcount:         12\n    { id: sys_ioctl                     [ 16], common_pid: gnome-terminal  [      3199] } hitcount:         16\n    { id: sys_ioctl                     [ 16], common_pid: Xorg            [      1267] } hitcount:       1808\n    { id: sys_ioctl                     [ 16], common_pid: compiz          [      2994] } hitcount:       5580\n    .\n    .\n    .\n    { id: sys_waitid                    [247], common_pid: upstart-dbus-br [      2690] } hitcount:          3\n    { id: sys_waitid                    [247], common_pid: upstart-dbus-br [      2688] } hitcount:         16\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [       975] } hitcount:          2\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      3204] } hitcount:          4\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      2888] } hitcount:          4\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      3003] } hitcount:          4\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      2873] } hitcount:          4\n    { id: sys_inotify_add_watch         [254], common_pid: gmain           [      3196] } hitcount:          6\n    { id: sys_openat                    [257], common_pid: java            [      2623] } hitcount:          2\n    { id: sys_eventfd2                  [290], common_pid: ibus-ui-gtk3    [      2760] } hitcount:          4\n    { id: sys_eventfd2                  [290], common_pid: compiz          [      2994] } hitcount:          6\n\n    Totals:\n        Hits: 31536\n        Entries: 323\n        Dropped: 0\n\n    The above list does give us a breakdown of the ioctl syscall by\n    pid, but it also gives us quite a bit more than that, which we\n    don't really care about at the moment.  Since we know the syscall\n    id for sys_ioctl (16, displayed next to the sys_ioctl name), we\n    can use that to filter out all the other syscalls:\n\n    # echo 'hist:key=id.syscall,common_pid.execname:val=hitcount:sort=id,hitcount if id == 16' > \\\n           /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/trigger\n\n    # cat /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/hist\n    # trigger info: hist:keys=id.syscall,common_pid.execname:vals=hitcount:sort=id.syscall,hitcount:size=2048 if id == 16 [active]\n\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2769] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: evolution-addre [      8571] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      3003] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2781] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2829] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: bash            [      8726] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: bash            [      8508] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2970] } hitcount:          1\n    { id: sys_ioctl                     [ 16], common_pid: gmain           [      2768] } hitcount:          1\n    .\n    .\n    .\n    { id: sys_ioctl                     [ 16], common_pid: pool            [      8559] } hitcount:         45\n    { id: sys_ioctl                     [ 16], common_pid: pool            [      8555] } hitcount:         48\n    { id: sys_ioctl                     [ 16], common_pid: pool            [      8551] } hitcount:         48\n    { id: sys_ioctl                     [ 16], common_pid: avahi-daemon    [       896] } hitcount:         66\n    { id: sys_ioctl                     [ 16], common_pid: Xorg            [      1267] } hitcount:      26674\n    { id: sys_ioctl                     [ 16], common_pid: compiz          [      2994] } hitcount:      73443\n\n    Totals:\n        Hits: 101162\n        Entries: 103\n        Dropped: 0\n\n    The above output shows that 'compiz' and 'Xorg' are far and away\n    the heaviest ioctl callers (which might lead to questions about\n    whether they really need to be making all those calls and to\n    possible avenues for further investigation.)\n\n    The compound key examples used a key and a sum value (hitcount) to\n    sort the output, but we can just as easily use two keys instead.\n    Here's an example where we use a compound key composed of the the\n    common_pid and size event fields.  Sorting with pid as the primary\n    key and 'size' as the secondary key allows us to display an\n    ordered summary of the recvfrom sizes, with counts, received by\n    each process:\n\n    # echo 'hist:key=common_pid.execname,size:val=hitcount:sort=common_pid,size' > \\\n           /sys/kernel/debug/tracing/events/syscalls/sys_enter_recvfrom/trigger\n\n    # cat /sys/kernel/debug/tracing/events/syscalls/sys_enter_recvfrom/hist\n    # trigger info: hist:keys=common_pid.execname,size:vals=hitcount:sort=common_pid.execname,size:size=2048 [active]\n\n    { common_pid: smbd            [       784], size:          4 } hitcount:          1\n    { common_pid: dnsmasq         [      1412], size:       4096 } hitcount:        672\n    { common_pid: postgres        [      1796], size:       1000 } hitcount:          6\n    { common_pid: postgres        [      1867], size:       1000 } hitcount:         10\n    { common_pid: bamfdaemon      [      2787], size:         28 } hitcount:          2\n    { common_pid: bamfdaemon      [      2787], size:      14360 } hitcount:          1\n    { common_pid: compiz          [      2994], size:          8 } hitcount:          1\n    { common_pid: compiz          [      2994], size:         20 } hitcount:         11\n    { common_pid: gnome-terminal  [      3199], size:          4 } hitcount:          2\n    { common_pid: firefox         [      8817], size:          4 } hitcount:          1\n    { common_pid: firefox         [      8817], size:          8 } hitcount:          5\n    { common_pid: firefox         [      8817], size:        588 } hitcount:          2\n    { common_pid: firefox         [      8817], size:        628 } hitcount:          1\n    { common_pid: firefox         [      8817], size:       6944 } hitcount:          1\n    { common_pid: firefox         [      8817], size:     408880 } hitcount:          2\n    { common_pid: firefox         [      8822], size:          8 } hitcount:          2\n    { common_pid: firefox         [      8822], size:        160 } hitcount:          2\n    { common_pid: firefox         [      8822], size:        320 } hitcount:          2\n    { common_pid: firefox         [      8822], size:        352 } hitcount:          1\n    .\n    .\n    .\n    { common_pid: pool            [      8923], size:       1960 } hitcount:         10\n    { common_pid: pool            [      8923], size:       2048 } hitcount:         10\n    { common_pid: pool            [      8924], size:       1960 } hitcount:         10\n    { common_pid: pool            [      8924], size:       2048 } hitcount:         10\n    { common_pid: pool            [      8928], size:       1964 } hitcount:          4\n    { common_pid: pool            [      8928], size:       1965 } hitcount:          2\n    { common_pid: pool            [      8928], size:       2048 } hitcount:          6\n    { common_pid: pool            [      8929], size:       1982 } hitcount:          1\n    { common_pid: pool            [      8929], size:       2048 } hitcount:          1\n\n    Totals:\n        Hits: 2016\n        Entries: 224\n        Dropped: 0\n\n  The above example also illustrates the fact that although a compound\n  key is treated as a single entity for hashing purposes, the sub-keys\n  it's composed of can be accessed independently.\n\n  The next example uses a string field as the hash key and\n  demonstrates how you can manually pause and continue a hist trigger.\n  In this example, we'll aggregate fork counts and don't expect a\n  large number of entries in the hash table, so we'll drop it to a\n  much smaller number, say 256:\n\n    # echo 'hist:key=child_comm:val=hitcount:size=256' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_fork/trigger\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/hist\n    # trigger info: hist:keys=child_comm:vals=hitcount:sort=hitcount:size=256 [active]\n\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: ibus-daemon                         } hitcount:          1\n    { child_comm: whoopsie                            } hitcount:          1\n    { child_comm: smbd                                } hitcount:          1\n    { child_comm: gdbus                               } hitcount:          1\n    { child_comm: kthreadd                            } hitcount:          1\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: evolution-alarm                     } hitcount:          2\n    { child_comm: Socket Thread                       } hitcount:          2\n    { child_comm: postgres                            } hitcount:          2\n    { child_comm: bash                                } hitcount:          3\n    { child_comm: compiz                              } hitcount:          3\n    { child_comm: evolution-sourc                     } hitcount:          4\n    { child_comm: dhclient                            } hitcount:          4\n    { child_comm: pool                                } hitcount:          5\n    { child_comm: nm-dispatcher.a                     } hitcount:          8\n    { child_comm: firefox                             } hitcount:          8\n    { child_comm: dbus-daemon                         } hitcount:          8\n    { child_comm: glib-pacrunner                      } hitcount:         10\n    { child_comm: evolution                           } hitcount:         23\n\n    Totals:\n        Hits: 89\n        Entries: 20\n        Dropped: 0\n\n  If we want to pause the hist trigger, we can simply append :pause to\n  the command that started the trigger.  Notice that the trigger info\n  displays as [paused]:\n\n    # echo 'hist:key=child_comm:val=hitcount:size=256:pause' >> \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_fork/trigger\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/hist\n    # trigger info: hist:keys=child_comm:vals=hitcount:sort=hitcount:size=256 [paused]\n\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: kthreadd                            } hitcount:          1\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: gdbus                               } hitcount:          1\n    { child_comm: ibus-daemon                         } hitcount:          1\n    { child_comm: Socket Thread                       } hitcount:          2\n    { child_comm: evolution-alarm                     } hitcount:          2\n    { child_comm: smbd                                } hitcount:          2\n    { child_comm: bash                                } hitcount:          3\n    { child_comm: whoopsie                            } hitcount:          3\n    { child_comm: compiz                              } hitcount:          3\n    { child_comm: evolution-sourc                     } hitcount:          4\n    { child_comm: pool                                } hitcount:          5\n    { child_comm: postgres                            } hitcount:          6\n    { child_comm: firefox                             } hitcount:          8\n    { child_comm: dhclient                            } hitcount:         10\n    { child_comm: emacs                               } hitcount:         12\n    { child_comm: dbus-daemon                         } hitcount:         20\n    { child_comm: nm-dispatcher.a                     } hitcount:         20\n    { child_comm: evolution                           } hitcount:         35\n    { child_comm: glib-pacrunner                      } hitcount:         59\n\n    Totals:\n        Hits: 199\n        Entries: 21\n        Dropped: 0\n\n  To manually continue having the trigger aggregate events, append\n  :cont instead.  Notice that the trigger info displays as [active]\n  again, and the data has changed:\n\n    # echo 'hist:key=child_comm:val=hitcount:size=256:cont' >> \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_fork/trigger\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/hist\n    # trigger info: hist:keys=child_comm:vals=hitcount:sort=hitcount:size=256 [active]\n\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: dconf worker                        } hitcount:          1\n    { child_comm: kthreadd                            } hitcount:          1\n    { child_comm: gdbus                               } hitcount:          1\n    { child_comm: ibus-daemon                         } hitcount:          1\n    { child_comm: Socket Thread                       } hitcount:          2\n    { child_comm: evolution-alarm                     } hitcount:          2\n    { child_comm: smbd                                } hitcount:          2\n    { child_comm: whoopsie                            } hitcount:          3\n    { child_comm: compiz                              } hitcount:          3\n    { child_comm: evolution-sourc                     } hitcount:          4\n    { child_comm: bash                                } hitcount:          5\n    { child_comm: pool                                } hitcount:          5\n    { child_comm: postgres                            } hitcount:          6\n    { child_comm: firefox                             } hitcount:          8\n    { child_comm: dhclient                            } hitcount:         11\n    { child_comm: emacs                               } hitcount:         12\n    { child_comm: dbus-daemon                         } hitcount:         22\n    { child_comm: nm-dispatcher.a                     } hitcount:         22\n    { child_comm: evolution                           } hitcount:         35\n    { child_comm: glib-pacrunner                      } hitcount:         59\n\n    Totals:\n        Hits: 206\n        Entries: 21\n        Dropped: 0\n\n  The previous example showed how to start and stop a hist trigger by\n  appending 'pause' and 'continue' to the hist trigger command.  A\n  hist trigger can also be started in a paused state by initially\n  starting the trigger with ':pause' appended.  This allows you to\n  start the trigger only when you're ready to start collecting data\n  and not before.  For example, you could start the trigger in a\n  paused state, then unpause it and do something you want to measure,\n  then pause the trigger again when done.\n\n  Of course, doing this manually can be difficult and error-prone, but\n  it is possible to automatically start and stop a hist trigger based\n  on some condition, via the enable_hist and disable_hist triggers.\n\n  For example, suppose we wanted to take a look at the relative\n  weights in terms of skb length for each callpath that leads to a\n  netif_receieve_skb event when downloading a decent-sized file using\n  wget.\n\n  First we set up an initially paused stacktrace trigger on the\n  netif_receive_skb event:\n\n    # echo 'hist:key=stacktrace:vals=len:pause' > \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n\n  Next, we set up an 'enable_hist' trigger on the sched_process_exec\n  event, with an 'if filename==/usr/bin/wget' filter.  The effect of\n  this new trigger is that it will 'unpause' the hist trigger we just\n  set up on netif_receive_skb if and only if it sees a\n  sched_process_exec event with a filename of '/usr/bin/wget'.  When\n  that happens, all netif_receive_skb events are aggregated into a\n  hash table keyed on stacktrace:\n\n    # echo 'enable_hist:net:netif_receive_skb if filename==/usr/bin/wget' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_exec/trigger\n\n  The aggregation continues until the netif_receive_skb is paused\n  again, which is what the following disable_hist event does by\n  creating a similar setup on the sched_process_exit event, using the\n  filter 'comm==wget':\n\n    # echo 'disable_hist:net:netif_receive_skb if comm==wget' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_exit/trigger\n\n  Whenever a process exits and the comm field of the disable_hist\n  trigger filter matches 'comm==wget', the netif_receive_skb hist\n  trigger is disabled.\n\n  The overall effect is that netif_receive_skb events are aggregated\n  into the hash table for only the duration of the wget.  Executing a\n  wget command and then listing the 'hist' file will display the\n  output generated by the wget command:\n\n    $ wget https://www.kernel.org/pub/linux/kernel/v3.x/patch-3.19.xz\n\n    # cat /sys/kernel/debug/tracing/events/net/netif_receive_skb/hist\n    # trigger info: hist:keys=stacktrace:vals=len:sort=hitcount:size=2048 [paused]\n\n    { stacktrace:\n         __netif_receive_skb_core+0x46d/0x990\n         __netif_receive_skb+0x18/0x60\n         netif_receive_skb_internal+0x23/0x90\n         napi_gro_receive+0xc8/0x100\n         ieee80211_deliver_skb+0xd6/0x270 [mac80211]\n         ieee80211_rx_handlers+0xccf/0x22f0 [mac80211]\n         ieee80211_prepare_and_rx_handle+0x4e7/0xc40 [mac80211]\n         ieee80211_rx+0x31d/0x900 [mac80211]\n         iwlagn_rx_reply_rx+0x3db/0x6f0 [iwldvm]\n         iwl_rx_dispatch+0x8e/0xf0 [iwldvm]\n         iwl_pcie_irq_handler+0xe3c/0x12f0 [iwlwifi]\n         irq_thread_fn+0x20/0x50\n         irq_thread+0x11f/0x150\n         kthread+0xd2/0xf0\n         ret_from_fork+0x42/0x70\n    } hitcount:         85  len:      28884\n    { stacktrace:\n         __netif_receive_skb_core+0x46d/0x990\n         __netif_receive_skb+0x18/0x60\n         netif_receive_skb_internal+0x23/0x90\n         napi_gro_complete+0xa4/0xe0\n         dev_gro_receive+0x23a/0x360\n         napi_gro_receive+0x30/0x100\n         ieee80211_deliver_skb+0xd6/0x270 [mac80211]\n         ieee80211_rx_handlers+0xccf/0x22f0 [mac80211]\n         ieee80211_prepare_and_rx_handle+0x4e7/0xc40 [mac80211]\n         ieee80211_rx+0x31d/0x900 [mac80211]\n         iwlagn_rx_reply_rx+0x3db/0x6f0 [iwldvm]\n         iwl_rx_dispatch+0x8e/0xf0 [iwldvm]\n         iwl_pcie_irq_handler+0xe3c/0x12f0 [iwlwifi]\n         irq_thread_fn+0x20/0x50\n         irq_thread+0x11f/0x150\n         kthread+0xd2/0xf0\n    } hitcount:         98  len:     664329\n    { stacktrace:\n         __netif_receive_skb_core+0x46d/0x990\n         __netif_receive_skb+0x18/0x60\n         process_backlog+0xa8/0x150\n         net_rx_action+0x15d/0x340\n         __do_softirq+0x114/0x2c0\n         do_softirq_own_stack+0x1c/0x30\n         do_softirq+0x65/0x70\n         __local_bh_enable_ip+0xb5/0xc0\n         ip_finish_output+0x1f4/0x840\n         ip_output+0x6b/0xc0\n         ip_local_out_sk+0x31/0x40\n         ip_send_skb+0x1a/0x50\n         udp_send_skb+0x173/0x2a0\n         udp_sendmsg+0x2bf/0x9f0\n         inet_sendmsg+0x64/0xa0\n         sock_sendmsg+0x3d/0x50\n    } hitcount:        115  len:      13030\n    { stacktrace:\n         __netif_receive_skb_core+0x46d/0x990\n         __netif_receive_skb+0x18/0x60\n         netif_receive_skb_internal+0x23/0x90\n         napi_gro_complete+0xa4/0xe0\n         napi_gro_flush+0x6d/0x90\n         iwl_pcie_irq_handler+0x92a/0x12f0 [iwlwifi]\n         irq_thread_fn+0x20/0x50\n         irq_thread+0x11f/0x150\n         kthread+0xd2/0xf0\n         ret_from_fork+0x42/0x70\n    } hitcount:        934  len:    5512212\n\n    Totals:\n        Hits: 1232\n        Entries: 4\n        Dropped: 0\n\n  The above shows all the netif_receive_skb callpaths and their total\n  lengths for the duration of the wget command.\n\n  The 'clear' hist trigger param can be used to clear the hash table.\n  Suppose we wanted to try another run of the previous example but\n  this time also wanted to see the complete list of events that went\n  into the histogram.  In order to avoid having to set everything up\n  again, we can just clear the histogram first:\n\n    # echo 'hist:key=stacktrace:vals=len:clear' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n\n  Just to verify that it is in fact cleared, here's what we now see in\n  the hist file:\n\n    # cat /sys/kernel/debug/tracing/events/net/netif_receive_skb/hist\n    # trigger info: hist:keys=stacktrace:vals=len:sort=hitcount:size=2048 [paused]\n\n    Totals:\n        Hits: 0\n        Entries: 0\n        Dropped: 0\n\n  Since we want to see the detailed list of every netif_receive_skb\n  event occurring during the new run, which are in fact the same\n  events being aggregated into the hash table, we add some additional\n  'enable_event' events to the triggering sched_process_exec and\n  sched_process_exit events as such:\n\n    # echo 'enable_event:net:netif_receive_skb if filename==/usr/bin/wget' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_exec/trigger\n\n    # echo 'disable_event:net:netif_receive_skb if comm==wget' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_exit/trigger\n\n  If you read the trigger files for the sched_process_exec and\n  sched_process_exit triggers, you should see two triggers for each:\n  one enabling/disabling the hist aggregation and the other\n  enabling/disabling the logging of events:\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_exec/trigger\n    enable_event:net:netif_receive_skb:unlimited if filename==/usr/bin/wget\n    enable_hist:net:netif_receive_skb:unlimited if filename==/usr/bin/wget\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_exit/trigger\n    enable_event:net:netif_receive_skb:unlimited if comm==wget\n    disable_hist:net:netif_receive_skb:unlimited if comm==wget\n\n  In other words, whenever either of the sched_process_exec or\n  sched_process_exit events is hit and matches 'wget', it enables or\n  disables both the histogram and the event log, and what you end up\n  with is a hash table and set of events just covering the specified\n  duration.  Run the wget command again:\n\n    $ wget https://www.kernel.org/pub/linux/kernel/v3.x/patch-3.19.xz\n\n  Displaying the 'hist' file should show something similar to what you\n  saw in the last run, but this time you should also see the\n  individual events in the trace file:\n\n    # cat /sys/kernel/debug/tracing/trace\n\n    # tracer: nop\n    #\n    # entries-in-buffer/entries-written: 183/1426   #P:4\n    #\n    #                              _-----=> irqs-off\n    #                             / _----=> need-resched\n    #                            | / _---=> hardirq/softirq\n    #                            || / _--=> preempt-depth\n    #                            ||| /     delay\n    #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION\n    #              | |       |   ||||       |         |\n                wget-15108 [000] ..s1 31769.606929: netif_receive_skb: dev=lo skbaddr=ffff88009c353100 len=60\n                wget-15108 [000] ..s1 31769.606999: netif_receive_skb: dev=lo skbaddr=ffff88009c353200 len=60\n             dnsmasq-1382  [000] ..s1 31769.677652: netif_receive_skb: dev=lo skbaddr=ffff88009c352b00 len=130\n             dnsmasq-1382  [000] ..s1 31769.685917: netif_receive_skb: dev=lo skbaddr=ffff88009c352200 len=138\n    ##### CPU 2 buffer started ####\n      irq/29-iwlwifi-559   [002] ..s. 31772.031529: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d433d00 len=2948\n      irq/29-iwlwifi-559   [002] ..s. 31772.031572: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d432200 len=1500\n      irq/29-iwlwifi-559   [002] ..s. 31772.032196: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d433100 len=2948\n      irq/29-iwlwifi-559   [002] ..s. 31772.032761: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d433000 len=2948\n      irq/29-iwlwifi-559   [002] ..s. 31772.033220: netif_receive_skb: dev=wlan0 skbaddr=ffff88009d432e00 len=1500\n    .\n    .\n    .\n\n  The following example demonstrates how multiple hist triggers can be\n  attached to a given event.  This capability can be useful for\n  creating a set of different summaries derived from the same set of\n  events, or for comparing the effects of different filters, among\n  other things.\n\n    # echo 'hist:keys=skbaddr.hex:vals=len if len < 0' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:keys=skbaddr.hex:vals=len if len > 4096' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:keys=skbaddr.hex:vals=len if len == 256' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:keys=skbaddr.hex:vals=len' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:keys=len:vals=common_preempt_count' >> \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n\n  The above set of commands create four triggers differing only in\n  their filters, along with a completely different though fairly\n  nonsensical trigger.  Note that in order to append multiple hist\n  triggers to the same file, you should use the '>>' operator to\n  append them ('>' will also add the new hist trigger, but will remove\n  any existing hist triggers beforehand).\n\n  Displaying the contents of the 'hist' file for the event shows the\n  contents of all five histograms:\n\n    # cat /sys/kernel/debug/tracing/events/net/netif_receive_skb/hist\n\n    # event histogram\n    #\n    # trigger info: hist:keys=len:vals=hitcount,common_preempt_count:sort=hitcount:size=2048 [active]\n    #\n\n    { len:        176 } hitcount:          1  common_preempt_count:          0\n    { len:        223 } hitcount:          1  common_preempt_count:          0\n    { len:       4854 } hitcount:          1  common_preempt_count:          0\n    { len:        395 } hitcount:          1  common_preempt_count:          0\n    { len:        177 } hitcount:          1  common_preempt_count:          0\n    { len:        446 } hitcount:          1  common_preempt_count:          0\n    { len:       1601 } hitcount:          1  common_preempt_count:          0\n    .\n    .\n    .\n    { len:       1280 } hitcount:         66  common_preempt_count:          0\n    { len:        116 } hitcount:         81  common_preempt_count:         40\n    { len:        708 } hitcount:        112  common_preempt_count:          0\n    { len:         46 } hitcount:        221  common_preempt_count:          0\n    { len:       1264 } hitcount:        458  common_preempt_count:          0\n\n    Totals:\n        Hits: 1428\n        Entries: 147\n        Dropped: 0\n\n\n    # event histogram\n    #\n    # trigger info: hist:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 [active]\n    #\n\n    { skbaddr: ffff8800baee5e00 } hitcount:          1  len:        130\n    { skbaddr: ffff88005f3d5600 } hitcount:          1  len:       1280\n    { skbaddr: ffff88005f3d4900 } hitcount:          1  len:       1280\n    { skbaddr: ffff88009fed6300 } hitcount:          1  len:        115\n    { skbaddr: ffff88009fe0ad00 } hitcount:          1  len:        115\n    { skbaddr: ffff88008cdb1900 } hitcount:          1  len:         46\n    { skbaddr: ffff880064b5ef00 } hitcount:          1  len:        118\n    { skbaddr: ffff880044e3c700 } hitcount:          1  len:         60\n    { skbaddr: ffff880100065900 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d46bd500 } hitcount:          1  len:        116\n    { skbaddr: ffff88005f3d5f00 } hitcount:          1  len:       1280\n    { skbaddr: ffff880100064700 } hitcount:          1  len:        365\n    { skbaddr: ffff8800badb6f00 } hitcount:          1  len:         60\n    .\n    .\n    .\n    { skbaddr: ffff88009fe0be00 } hitcount:         27  len:      24677\n    { skbaddr: ffff88009fe0a400 } hitcount:         27  len:      23052\n    { skbaddr: ffff88009fe0b700 } hitcount:         31  len:      25589\n    { skbaddr: ffff88009fe0b600 } hitcount:         32  len:      27326\n    { skbaddr: ffff88006a462800 } hitcount:         68  len:      71678\n    { skbaddr: ffff88006a463700 } hitcount:         70  len:      72678\n    { skbaddr: ffff88006a462b00 } hitcount:         71  len:      77589\n    { skbaddr: ffff88006a463600 } hitcount:         73  len:      71307\n    { skbaddr: ffff88006a462200 } hitcount:         81  len:      81032\n\n    Totals:\n        Hits: 1451\n        Entries: 318\n        Dropped: 0\n\n\n    # event histogram\n    #\n    # trigger info: hist:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 if len == 256 [active]\n    #\n\n\n    Totals:\n        Hits: 0\n        Entries: 0\n        Dropped: 0\n\n\n    # event histogram\n    #\n    # trigger info: hist:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 if len > 4096 [active]\n    #\n\n    { skbaddr: ffff88009fd2c300 } hitcount:          1  len:       7212\n    { skbaddr: ffff8800d2bcce00 } hitcount:          1  len:       7212\n    { skbaddr: ffff8800d2bcd700 } hitcount:          1  len:       7212\n    { skbaddr: ffff8800d2bcda00 } hitcount:          1  len:      21492\n    { skbaddr: ffff8800ae2e2d00 } hitcount:          1  len:       7212\n    { skbaddr: ffff8800d2bcdb00 } hitcount:          1  len:       7212\n    { skbaddr: ffff88006a4df500 } hitcount:          1  len:       4854\n    { skbaddr: ffff88008ce47b00 } hitcount:          1  len:      18636\n    { skbaddr: ffff8800ae2e2200 } hitcount:          1  len:      12924\n    { skbaddr: ffff88005f3e1000 } hitcount:          1  len:       4356\n    { skbaddr: ffff8800d2bcdc00 } hitcount:          2  len:      24420\n    { skbaddr: ffff8800d2bcc200 } hitcount:          2  len:      12996\n\n    Totals:\n        Hits: 14\n        Entries: 12\n        Dropped: 0\n\n\n    # event histogram\n    #\n    # trigger info: hist:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 if len < 0 [active]\n    #\n\n\n    Totals:\n        Hits: 0\n        Entries: 0\n        Dropped: 0\n\n  Named triggers can be used to have triggers share a common set of\n  histogram data.  This capability is mostly useful for combining the\n  output of events generated by tracepoints contained inside inline\n  functions, but names can be used in a hist trigger on any event.\n  For example, these two triggers when hit will update the same 'len'\n  field in the shared 'foo' histogram data:\n\n    # echo 'hist:name=foo:keys=skbaddr.hex:vals=len' > \\\n           /sys/kernel/debug/tracing/events/net/netif_receive_skb/trigger\n    # echo 'hist:name=foo:keys=skbaddr.hex:vals=len' > \\\n           /sys/kernel/debug/tracing/events/net/netif_rx/trigger\n\n  You can see that they're updating common histogram data by reading\n  each event's hist files at the same time:\n\n    # cat /sys/kernel/debug/tracing/events/net/netif_receive_skb/hist;\n      cat /sys/kernel/debug/tracing/events/net/netif_rx/hist\n\n    # event histogram\n    #\n    # trigger info: hist:name=foo:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 [active]\n    #\n\n    { skbaddr: ffff88000ad53500 } hitcount:          1  len:         46\n    { skbaddr: ffff8800af5a1500 } hitcount:          1  len:         76\n    { skbaddr: ffff8800d62a1900 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bccb00 } hitcount:          1  len:        468\n    { skbaddr: ffff8800d3c69900 } hitcount:          1  len:         46\n    { skbaddr: ffff88009ff09100 } hitcount:          1  len:         52\n    { skbaddr: ffff88010f13ab00 } hitcount:          1  len:        168\n    { skbaddr: ffff88006a54f400 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcc500 } hitcount:          1  len:        260\n    { skbaddr: ffff880064505000 } hitcount:          1  len:         46\n    { skbaddr: ffff8800baf24e00 } hitcount:          1  len:         32\n    { skbaddr: ffff88009fe0ad00 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d3edff00 } hitcount:          1  len:         44\n    { skbaddr: ffff88009fe0b400 } hitcount:          1  len:        168\n    { skbaddr: ffff8800a1c55a00 } hitcount:          1  len:         40\n    { skbaddr: ffff8800d2bcd100 } hitcount:          1  len:         40\n    { skbaddr: ffff880064505f00 } hitcount:          1  len:        174\n    { skbaddr: ffff8800a8bff200 } hitcount:          1  len:        160\n    { skbaddr: ffff880044e3cc00 } hitcount:          1  len:         76\n    { skbaddr: ffff8800a8bfe700 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcdc00 } hitcount:          1  len:         32\n    { skbaddr: ffff8800a1f64800 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcde00 } hitcount:          1  len:        988\n    { skbaddr: ffff88006a5dea00 } hitcount:          1  len:         46\n    { skbaddr: ffff88002e37a200 } hitcount:          1  len:         44\n    { skbaddr: ffff8800a1f32c00 } hitcount:          2  len:        676\n    { skbaddr: ffff88000ad52600 } hitcount:          2  len:        107\n    { skbaddr: ffff8800a1f91e00 } hitcount:          2  len:         92\n    { skbaddr: ffff8800af5a0200 } hitcount:          2  len:        142\n    { skbaddr: ffff8800d2bcc600 } hitcount:          2  len:        220\n    { skbaddr: ffff8800ba36f500 } hitcount:          2  len:         92\n    { skbaddr: ffff8800d021f800 } hitcount:          2  len:         92\n    { skbaddr: ffff8800a1f33600 } hitcount:          2  len:        675\n    { skbaddr: ffff8800a8bfff00 } hitcount:          3  len:        138\n    { skbaddr: ffff8800d62a1300 } hitcount:          3  len:        138\n    { skbaddr: ffff88002e37a100 } hitcount:          4  len:        184\n    { skbaddr: ffff880064504400 } hitcount:          4  len:        184\n    { skbaddr: ffff8800a8bfec00 } hitcount:          4  len:        184\n    { skbaddr: ffff88000ad53700 } hitcount:          5  len:        230\n    { skbaddr: ffff8800d2bcdb00 } hitcount:          5  len:        196\n    { skbaddr: ffff8800a1f90000 } hitcount:          6  len:        276\n    { skbaddr: ffff88006a54f900 } hitcount:          6  len:        276\n\n    Totals:\n        Hits: 81\n        Entries: 42\n        Dropped: 0\n    # event histogram\n    #\n    # trigger info: hist:name=foo:keys=skbaddr.hex:vals=hitcount,len:sort=hitcount:size=2048 [active]\n    #\n\n    { skbaddr: ffff88000ad53500 } hitcount:          1  len:         46\n    { skbaddr: ffff8800af5a1500 } hitcount:          1  len:         76\n    { skbaddr: ffff8800d62a1900 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bccb00 } hitcount:          1  len:        468\n    { skbaddr: ffff8800d3c69900 } hitcount:          1  len:         46\n    { skbaddr: ffff88009ff09100 } hitcount:          1  len:         52\n    { skbaddr: ffff88010f13ab00 } hitcount:          1  len:        168\n    { skbaddr: ffff88006a54f400 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcc500 } hitcount:          1  len:        260\n    { skbaddr: ffff880064505000 } hitcount:          1  len:         46\n    { skbaddr: ffff8800baf24e00 } hitcount:          1  len:         32\n    { skbaddr: ffff88009fe0ad00 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d3edff00 } hitcount:          1  len:         44\n    { skbaddr: ffff88009fe0b400 } hitcount:          1  len:        168\n    { skbaddr: ffff8800a1c55a00 } hitcount:          1  len:         40\n    { skbaddr: ffff8800d2bcd100 } hitcount:          1  len:         40\n    { skbaddr: ffff880064505f00 } hitcount:          1  len:        174\n    { skbaddr: ffff8800a8bff200 } hitcount:          1  len:        160\n    { skbaddr: ffff880044e3cc00 } hitcount:          1  len:         76\n    { skbaddr: ffff8800a8bfe700 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcdc00 } hitcount:          1  len:         32\n    { skbaddr: ffff8800a1f64800 } hitcount:          1  len:         46\n    { skbaddr: ffff8800d2bcde00 } hitcount:          1  len:        988\n    { skbaddr: ffff88006a5dea00 } hitcount:          1  len:         46\n    { skbaddr: ffff88002e37a200 } hitcount:          1  len:         44\n    { skbaddr: ffff8800a1f32c00 } hitcount:          2  len:        676\n    { skbaddr: ffff88000ad52600 } hitcount:          2  len:        107\n    { skbaddr: ffff8800a1f91e00 } hitcount:          2  len:         92\n    { skbaddr: ffff8800af5a0200 } hitcount:          2  len:        142\n    { skbaddr: ffff8800d2bcc600 } hitcount:          2  len:        220\n    { skbaddr: ffff8800ba36f500 } hitcount:          2  len:         92\n    { skbaddr: ffff8800d021f800 } hitcount:          2  len:         92\n    { skbaddr: ffff8800a1f33600 } hitcount:          2  len:        675\n    { skbaddr: ffff8800a8bfff00 } hitcount:          3  len:        138\n    { skbaddr: ffff8800d62a1300 } hitcount:          3  len:        138\n    { skbaddr: ffff88002e37a100 } hitcount:          4  len:        184\n    { skbaddr: ffff880064504400 } hitcount:          4  len:        184\n    { skbaddr: ffff8800a8bfec00 } hitcount:          4  len:        184\n    { skbaddr: ffff88000ad53700 } hitcount:          5  len:        230\n    { skbaddr: ffff8800d2bcdb00 } hitcount:          5  len:        196\n    { skbaddr: ffff8800a1f90000 } hitcount:          6  len:        276\n    { skbaddr: ffff88006a54f900 } hitcount:          6  len:        276\n\n    Totals:\n        Hits: 81\n        Entries: 42\n        Dropped: 0\n\n  And here's an example that shows how to combine histogram data from\n  any two events even if they don't share any 'compatible' fields\n  other than 'hitcount' and 'stacktrace'.  These commands create a\n  couple of triggers named 'bar' using those fields:\n\n    # echo 'hist:name=bar:key=stacktrace:val=hitcount' > \\\n           /sys/kernel/debug/tracing/events/sched/sched_process_fork/trigger\n    # echo 'hist:name=bar:key=stacktrace:val=hitcount' > \\\n          /sys/kernel/debug/tracing/events/net/netif_rx/trigger\n\n  And displaying the output of either shows some interesting if\n  somewhat confusing output:\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/hist\n    # cat /sys/kernel/debug/tracing/events/net/netif_rx/hist\n\n    # event histogram\n    #\n    # trigger info: hist:name=bar:keys=stacktrace:vals=hitcount:sort=hitcount:size=2048 [active]\n    #\n\n    { stacktrace:\n             _do_fork+0x18e/0x330\n             kernel_thread+0x29/0x30\n             kthreadd+0x154/0x1b0\n             ret_from_fork+0x3f/0x70\n    } hitcount:          1\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx_ni+0x20/0x70\n             dev_loopback_xmit+0xaa/0xd0\n             ip_mc_output+0x126/0x240\n             ip_local_out_sk+0x31/0x40\n             igmp_send_report+0x1e9/0x230\n             igmp_timer_expire+0xe9/0x120\n             call_timer_fn+0x39/0xf0\n             run_timer_softirq+0x1e1/0x290\n             __do_softirq+0xfd/0x290\n             irq_exit+0x98/0xb0\n             smp_apic_timer_interrupt+0x4a/0x60\n             apic_timer_interrupt+0x6d/0x80\n             cpuidle_enter+0x17/0x20\n             call_cpuidle+0x3b/0x60\n             cpu_startup_entry+0x22d/0x310\n    } hitcount:          1\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx_ni+0x20/0x70\n             dev_loopback_xmit+0xaa/0xd0\n             ip_mc_output+0x17f/0x240\n             ip_local_out_sk+0x31/0x40\n             ip_send_skb+0x1a/0x50\n             udp_send_skb+0x13e/0x270\n             udp_sendmsg+0x2bf/0x980\n             inet_sendmsg+0x67/0xa0\n             sock_sendmsg+0x38/0x50\n             SYSC_sendto+0xef/0x170\n             SyS_sendto+0xe/0x10\n             entry_SYSCALL_64_fastpath+0x12/0x6a\n    } hitcount:          2\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx+0x1c/0x60\n             loopback_xmit+0x6c/0xb0\n             dev_hard_start_xmit+0x219/0x3a0\n             __dev_queue_xmit+0x415/0x4f0\n             dev_queue_xmit_sk+0x13/0x20\n             ip_finish_output2+0x237/0x340\n             ip_finish_output+0x113/0x1d0\n             ip_output+0x66/0xc0\n             ip_local_out_sk+0x31/0x40\n             ip_send_skb+0x1a/0x50\n             udp_send_skb+0x16d/0x270\n             udp_sendmsg+0x2bf/0x980\n             inet_sendmsg+0x67/0xa0\n             sock_sendmsg+0x38/0x50\n             ___sys_sendmsg+0x14e/0x270\n    } hitcount:         76\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx+0x1c/0x60\n             loopback_xmit+0x6c/0xb0\n             dev_hard_start_xmit+0x219/0x3a0\n             __dev_queue_xmit+0x415/0x4f0\n             dev_queue_xmit_sk+0x13/0x20\n             ip_finish_output2+0x237/0x340\n             ip_finish_output+0x113/0x1d0\n             ip_output+0x66/0xc0\n             ip_local_out_sk+0x31/0x40\n             ip_send_skb+0x1a/0x50\n             udp_send_skb+0x16d/0x270\n             udp_sendmsg+0x2bf/0x980\n             inet_sendmsg+0x67/0xa0\n             sock_sendmsg+0x38/0x50\n             ___sys_sendmsg+0x269/0x270\n    } hitcount:         77\n    { stacktrace:\n             netif_rx_internal+0xb2/0xd0\n             netif_rx+0x1c/0x60\n             loopback_xmit+0x6c/0xb0\n             dev_hard_start_xmit+0x219/0x3a0\n             __dev_queue_xmit+0x415/0x4f0\n             dev_queue_xmit_sk+0x13/0x20\n             ip_finish_output2+0x237/0x340\n             ip_finish_output+0x113/0x1d0\n             ip_output+0x66/0xc0\n             ip_local_out_sk+0x31/0x40\n             ip_send_skb+0x1a/0x50\n             udp_send_skb+0x16d/0x270\n             udp_sendmsg+0x2bf/0x980\n             inet_sendmsg+0x67/0xa0\n             sock_sendmsg+0x38/0x50\n             SYSC_sendto+0xef/0x170\n    } hitcount:         88\n    { stacktrace:\n             _do_fork+0x18e/0x330\n             SyS_clone+0x19/0x20\n             entry_SYSCALL_64_fastpath+0x12/0x6a\n    } hitcount:        244\n\n    Totals:\n        Hits: 489\n        Entries: 7\n        Dropped: 0\n\n2.2 Inter-event hist triggers\n-----------------------------\n\nInter-event hist triggers are hist triggers that combine values from\none or more other events and create a histogram using that data.  Data\nfrom an inter-event histogram can in turn become the source for\nfurther combined histograms, thus providing a chain of related\nhistograms, which is important for some applications.\n\nThe most important example of an inter-event quantity that can be used\nin this manner is latency, which is simply a difference in timestamps\nbetween two events.  Although latency is the most important\ninter-event quantity, note that because the support is completely\ngeneral across the trace event subsystem, any event field can be used\nin an inter-event quantity.\n\nAn example of a histogram that combines data from other histograms\ninto a useful chain would be a 'wakeupswitch latency' histogram that\ncombines a 'wakeup latency' histogram and a 'switch latency'\nhistogram.\n\nNormally, a hist trigger specification consists of a (possibly\ncompound) key along with one or more numeric values, which are\ncontinually updated sums associated with that key.  A histogram\nspecification in this case consists of individual key and value\nspecifications that refer to trace event fields associated with a\nsingle event type.\n\nThe inter-event hist trigger extension allows fields from multiple\nevents to be referenced and combined into a multi-event histogram\nspecification.  In support of this overall goal, a few enabling\nfeatures have been added to the hist trigger support:\n\n  - In order to compute an inter-event quantity, a value from one\n    event needs to saved and then referenced from another event.  This\n    requires the introduction of support for histogram 'variables'.\n\n  - The computation of inter-event quantities and their combination\n    require some minimal amount of support for applying simple\n    expressions to variables (+ and -).\n\n  - A histogram consisting of inter-event quantities isn't logically a\n    histogram on either event (so having the 'hist' file for either\n    event host the histogram output doesn't really make sense).  To\n    address the idea that the histogram is associated with a\n    combination of events, support is added allowing the creation of\n    'synthetic' events that are events derived from other events.\n    These synthetic events are full-fledged events just like any other\n    and can be used as such, as for instance to create the\n    'combination' histograms mentioned previously.\n\n  - A set of 'actions' can be associated with histogram entries -\n    these can be used to generate the previously mentioned synthetic\n    events, but can also be used for other purposes, such as for\n    example saving context when a 'max' latency has been hit.\n\n  - Trace events don't have a 'timestamp' associated with them, but\n    there is an implicit timestamp saved along with an event in the\n    underlying ftrace ring buffer.  This timestamp is now exposed as a\n    a synthetic field named 'common_timestamp' which can be used in\n    histograms as if it were any other event field; it isn't an actual\n    field in the trace format but rather is a synthesized value that\n    nonetheless can be used as if it were an actual field.  By default\n    it is in units of nanoseconds; appending '.usecs' to a\n    common_timestamp field changes the units to microseconds.\n\nA note on inter-event timestamps: If common_timestamp is used in a\nhistogram, the trace buffer is automatically switched over to using\nabsolute timestamps and the \"global\" trace clock, in order to avoid\nbogus timestamp differences with other clocks that aren't coherent\nacross CPUs.  This can be overridden by specifying one of the other\ntrace clocks instead, using the \"clock=XXX\" hist trigger attribute,\nwhere XXX is any of the clocks listed in the tracing/trace_clock\npseudo-file.\n\nThese features are described in more detail in the following sections.\n\n2.2.1 Histogram Variables\n-------------------------\n\nVariables are simply named locations used for saving and retrieving\nvalues between matching events.  A 'matching' event is defined as an\nevent that has a matching key - if a variable is saved for a histogram\nentry corresponding to that key, any subsequent event with a matching\nkey can access that variable.\n\nA variable's value is normally available to any subsequent event until\nit is set to something else by a subsequent event.  The one exception\nto that rule is that any variable used in an expression is essentially\n'read-once' - once it's used by an expression in a subsequent event,\nit's reset to its 'unset' state, which means it can't be used again\nunless it's set again.  This ensures not only that an event doesn't\nuse an uninitialized variable in a calculation, but that that variable\nis used only once and not for any unrelated subsequent match.\n\nThe basic syntax for saving a variable is to simply prefix a unique\nvariable name not corresponding to any keyword along with an '=' sign\nto any event field.\n\nEither keys or values can be saved and retrieved in this way.  This\ncreates a variable named 'ts0' for a histogram entry with the key\n'next_pid':\n\n  # echo 'hist:keys=next_pid:vals=$ts0:ts0=common_timestamp ... >> \\\n\tevent/trigger\n\nThe ts0 variable can be accessed by any subsequent event having the\nsame pid as 'next_pid'.\n\nVariable references are formed by prepending the variable name with\nthe '$' sign.  Thus for example, the ts0 variable above would be\nreferenced as '$ts0' in expressions.\n\nBecause 'vals=' is used, the common_timestamp variable value above\nwill also be summed as a normal histogram value would (though for a\ntimestamp it makes little sense).\n\nThe below shows that a key value can also be saved in the same way:\n\n  # echo 'hist:timer_pid=common_pid:key=timer_pid ...' >> event/trigger\n\nIf a variable isn't a key variable or prefixed with 'vals=', the\nassociated event field will be saved in a variable but won't be summed\nas a value:\n\n  # echo 'hist:keys=next_pid:ts1=common_timestamp ...' >> event/trigger\n\nMultiple variables can be assigned at the same time.  The below would\nresult in both ts0 and b being created as variables, with both\ncommon_timestamp and field1 additionally being summed as values:\n\n  # echo 'hist:keys=pid:vals=$ts0,$b:ts0=common_timestamp,b=field1 ...' >> \\\n\tevent/trigger\n\nNote that variable assignments can appear either preceding or\nfollowing their use.  The command below behaves identically to the\ncommand above:\n\n  # echo 'hist:keys=pid:ts0=common_timestamp,b=field1:vals=$ts0,$b ...' >> \\\n\tevent/trigger\n\nAny number of variables not bound to a 'vals=' prefix can also be\nassigned by simply separating them with colons.  Below is the same\nthing but without the values being summed in the histogram:\n\n  # echo 'hist:keys=pid:ts0=common_timestamp:b=field1 ...' >> event/trigger\n\nVariables set as above can be referenced and used in expressions on\nanother event.\n\nFor example, here's how a latency can be calculated:\n\n  # echo 'hist:keys=pid,prio:ts0=common_timestamp ...' >> event1/trigger\n  # echo 'hist:keys=next_pid:wakeup_lat=common_timestamp-$ts0 ...' >> event2/trigger\n\nIn the first line above, the event's timetamp is saved into the\nvariable ts0.  In the next line, ts0 is subtracted from the second\nevent's timestamp to produce the latency, which is then assigned into\nyet another variable, 'wakeup_lat'.  The hist trigger below in turn\nmakes use of the wakeup_lat variable to compute a combined latency\nusing the same key and variable from yet another event:\n\n  # echo 'hist:key=pid:wakeupswitch_lat=$wakeup_lat+$switchtime_lat ...' >> event3/trigger\n\n2.2.2 Synthetic Events\n----------------------\n\nSynthetic events are user-defined events generated from hist trigger\nvariables or fields associated with one or more other events.  Their\npurpose is to provide a mechanism for displaying data spanning\nmultiple events consistent with the existing and already familiar\nusage for normal events.\n\nTo define a synthetic event, the user writes a simple specification\nconsisting of the name of the new event along with one or more\nvariables and their types, which can be any valid field type,\nseparated by semicolons, to the tracing/synthetic_events file.\n\nFor instance, the following creates a new event named 'wakeup_latency'\nwith 3 fields: lat, pid, and prio.  Each of those fields is simply a\nvariable reference to a variable on another event:\n\n  # echo 'wakeup_latency \\\n          u64 lat; \\\n          pid_t pid; \\\n\t  int prio' >> \\\n\t  /sys/kernel/debug/tracing/synthetic_events\n\nReading the tracing/synthetic_events file lists all the currently\ndefined synthetic events, in this case the event defined above:\n\n  # cat /sys/kernel/debug/tracing/synthetic_events\n    wakeup_latency u64 lat; pid_t pid; int prio\n\nAn existing synthetic event definition can be removed by prepending\nthe command that defined it with a '!':\n\n  # echo '!wakeup_latency u64 lat pid_t pid int prio' >> \\\n    /sys/kernel/debug/tracing/synthetic_events\n\nAt this point, there isn't yet an actual 'wakeup_latency' event\ninstantiated in the event subsytem - for this to happen, a 'hist\ntrigger action' needs to be instantiated and bound to actual fields\nand variables defined on other events (see Section 2.2.3 below on\nhow that is done using hist trigger 'onmatch' action). Once that is\ndone, the 'wakeup_latency' synthetic event instance is created.\n\nA histogram can now be defined for the new synthetic event:\n\n  # echo 'hist:keys=pid,prio,lat.log2:sort=pid,lat' >> \\\n        /sys/kernel/debug/tracing/events/synthetic/wakeup_latency/trigger\n\nThe new event is created under the tracing/events/synthetic/ directory\nand looks and behaves just like any other event:\n\n  # ls /sys/kernel/debug/tracing/events/synthetic/wakeup_latency\n        enable  filter  format  hist  id  trigger\n\nLike any other event, once a histogram is enabled for the event, the\noutput can be displayed by reading the event's 'hist' file.\n\n2.2.3 Hist trigger 'actions'\n----------------------------\n\nA hist trigger 'action' is a function that's executed whenever a\nhistogram entry is added or updated.\n\nThe default 'action' if no special function is explicity specified is\nas it always has been, to simply update the set of values associated\nwith an entry.  Some applications, however, may want to perform\nadditional actions at that point, such as generate another event, or\ncompare and save a maximum.\n\nThe following additional actions are available.  To specify an action\nfor a given event, simply specify the action between colons in the\nhist trigger specification.\n\n  - onmatch(matching.event).<synthetic_event_name>(param list)\n\n    The 'onmatch(matching.event).<synthetic_event_name>(params)' hist\n    trigger action is invoked whenever an event matches and the\n    histogram entry would be added or updated.  It causes the named\n    synthetic event to be generated with the values given in the\n    'param list'.  The result is the generation of a synthetic event\n    that consists of the values contained in those variables at the\n    time the invoking event was hit.\n\n    The 'param list' consists of one or more parameters which may be\n    either variables or fields defined on either the 'matching.event'\n    or the target event.  The variables or fields specified in the\n    param list may be either fully-qualified or unqualified.  If a\n    variable is specified as unqualified, it must be unique between\n    the two events.  A field name used as a param can be unqualified\n    if it refers to the target event, but must be fully qualified if\n    it refers to the matching event.  A fully-qualified name is of the\n    form 'system.event_name.$var_name' or 'system.event_name.field'.\n\n    The 'matching.event' specification is simply the fully qualified\n    event name of the event that matches the target event for the\n    onmatch() functionality, in the form 'system.event_name'.\n\n    Finally, the number and type of variables/fields in the 'param\n    list' must match the number and types of the fields in the\n    synthetic event being generated.\n\n    As an example the below defines a simple synthetic event and uses\n    a variable defined on the sched_wakeup_new event as a parameter\n    when invoking the synthetic event.  Here we define the synthetic\n    event:\n\n    # echo 'wakeup_new_test pid_t pid' >> \\\n           /sys/kernel/debug/tracing/synthetic_events\n\n    # cat /sys/kernel/debug/tracing/synthetic_events\n          wakeup_new_test pid_t pid\n\n    The following hist trigger both defines the missing testpid\n    variable and specifies an onmatch() action that generates a\n    wakeup_new_test synthetic event whenever a sched_wakeup_new event\n    occurs, which because of the 'if comm == \"cyclictest\"' filter only\n    happens when the executable is cyclictest:\n\n    # echo 'hist:keys=$testpid:testpid=pid:onmatch(sched.sched_wakeup_new).\\\n            wakeup_new_test($testpid) if comm==\"cyclictest\"' >> \\\n            /sys/kernel/debug/tracing/events/sched/sched_wakeup_new/trigger\n\n    Creating and displaying a histogram based on those events is now\n    just a matter of using the fields and new synthetic event in the\n    tracing/events/synthetic directory, as usual:\n\n    # echo 'hist:keys=pid:sort=pid' >> \\\n           /sys/kernel/debug/tracing/events/synthetic/wakeup_new_test/trigger\n\n    Running 'cyclictest' should cause wakeup_new events to generate\n    wakeup_new_test synthetic events which should result in histogram\n    output in the wakeup_new_test event's hist file:\n\n    # cat /sys/kernel/debug/tracing/events/synthetic/wakeup_new_test/hist\n\n    A more typical usage would be to use two events to calculate a\n    latency.  The following example uses a set of hist triggers to\n    produce a 'wakeup_latency' histogram:\n\n    First, we define a 'wakeup_latency' synthetic event:\n\n    # echo 'wakeup_latency u64 lat; pid_t pid; int prio' >> \\\n            /sys/kernel/debug/tracing/synthetic_events\n\n    Next, we specify that whenever we see a sched_waking event for a\n    cyclictest thread, save the timestamp in a 'ts0' variable:\n\n    # echo 'hist:keys=$saved_pid:saved_pid=pid:ts0=common_timestamp.usecs \\\n            if comm==\"cyclictest\"' >> \\\n\t    /sys/kernel/debug/tracing/events/sched/sched_waking/trigger\n\n    Then, when the corresponding thread is actually scheduled onto the\n    CPU by a sched_switch event, calculate the latency and use that\n    along with another variable and an event field to generate a\n    wakeup_latency synthetic event:\n\n    # echo 'hist:keys=next_pid:wakeup_lat=common_timestamp.usecs-$ts0:\\\n            onmatch(sched.sched_waking).wakeup_latency($wakeup_lat,\\\n\t            $saved_pid,next_prio) if next_comm==\"cyclictest\"' >> \\\n\t    /sys/kernel/debug/tracing/events/sched/sched_switch/trigger\n\n    We also need to create a histogram on the wakeup_latency synthetic\n    event in order to aggregate the generated synthetic event data:\n\n    # echo 'hist:keys=pid,prio,lat:sort=pid,lat' >> \\\n            /sys/kernel/debug/tracing/events/synthetic/wakeup_latency/trigger\n\n    Finally, once we've run cyclictest to actually generate some\n    events, we can see the output by looking at the wakeup_latency\n    synthetic event's hist file:\n\n    # cat /sys/kernel/debug/tracing/events/synthetic/wakeup_latency/hist\n\n  - onmax(var).save(field,..\t.)\n\n    The 'onmax(var).save(field,...)' hist trigger action is invoked\n    whenever the value of 'var' associated with a histogram entry\n    exceeds the current maximum contained in that variable.\n\n    The end result is that the trace event fields specified as the\n    onmax.save() params will be saved if 'var' exceeds the current\n    maximum for that hist trigger entry.  This allows context from the\n    event that exhibited the new maximum to be saved for later\n    reference.  When the histogram is displayed, additional fields\n    displaying the saved values will be printed.\n\n    As an example the below defines a couple of hist triggers, one for\n    sched_waking and another for sched_switch, keyed on pid.  Whenever\n    a sched_waking occurs, the timestamp is saved in the entry\n    corresponding to the current pid, and when the scheduler switches\n    back to that pid, the timestamp difference is calculated.  If the\n    resulting latency, stored in wakeup_lat, exceeds the current\n    maximum latency, the values specified in the save() fields are\n    recorded:\n\n    # echo 'hist:keys=pid:ts0=common_timestamp.usecs \\\n            if comm==\"cyclictest\"' >> \\\n            /sys/kernel/debug/tracing/events/sched/sched_waking/trigger\n\n    # echo 'hist:keys=next_pid:\\\n            wakeup_lat=common_timestamp.usecs-$ts0:\\\n            onmax($wakeup_lat).save(next_comm,prev_pid,prev_prio,prev_comm) \\\n            if next_comm==\"cyclictest\"' >> \\\n            /sys/kernel/debug/tracing/events/sched/sched_switch/trigger\n\n    When the histogram is displayed, the max value and the saved\n    values corresponding to the max are displayed following the rest\n    of the fields:\n\n    # cat /sys/kernel/debug/tracing/events/sched/sched_switch/hist\n      { next_pid:       2255 } hitcount:        239\n        common_timestamp-ts0:          0\n        max:         27\n\tnext_comm: cyclictest\n        prev_pid:          0  prev_prio:        120  prev_comm: swapper/1\n\n      { next_pid:       2256 } hitcount:       2355\n        common_timestamp-ts0: 0\n        max:         49  next_comm: cyclictest\n        prev_pid:          0  prev_prio:        120  prev_comm: swapper/0\n\n      Totals:\n          Hits: 12970\n          Entries: 2\n          Dropped: 0\n\n3. User space creating a trigger\n--------------------------------\n\nWriting into /sys/kernel/tracing/trace_marker writes into the ftrace\nring buffer. This can also act like an event, by writing into the trigger\nfile located in /sys/kernel/tracing/events/ftrace/print/\n\nModifying cyclictest to write into the trace_marker file before it sleeps\nand after it wakes up, something like this:\n\nstatic void traceputs(char *str)\n{\n\t/* tracemark_fd is the trace_marker file descriptor */\n\tif (tracemark_fd < 0)\n\t\treturn;\n\t/* write the tracemark message */\n\twrite(tracemark_fd, str, strlen(str));\n}\n\nAnd later add something like:\n\n\ttraceputs(\"start\");\n\tclock_nanosleep(...);\n\ttraceputs(\"end\");\n\nWe can make a histogram from this:\n\n # cd /sys/kernel/tracing\n # echo 'latency u64 lat' > synthetic_events\n # echo 'hist:keys=common_pid:ts0=common_timestamp.usecs if buf == \"start\"' > events/ftrace/print/trigger\n # echo 'hist:keys=common_pid:lat=common_timestamp.usecs-$ts0:onmatch(ftrace.print).latency($lat) if buf == \"end\"' >> events/ftrace/print/trigger\n # echo 'hist:keys=lat,common_pid:sort=lat' > events/synthetic/latency/trigger\n\nThe above created a synthetic event called \"latency\" and two histograms\nagainst the trace_marker, one gets triggered when \"start\" is written into the\ntrace_marker file and the other when \"end\" is written. If the pids match, then\nit will call the \"latency\" synthetic event with the calculated latency as its\nparameter. Finally, a histogram is added to the latency synthetic event to\nrecord the calculated latency along with the pid.\n\nNow running cyclictest with:\n\n # ./cyclictest -p80 -d0 -i250 -n -a -t --tracemark -b 1000\n\n -p80  : run threads at priority 80\n -d0   : have all threads run at the same interval\n -i250 : start the interval at 250 microseconds (all threads will do this)\n -n    : sleep with nanosleep\n -a    : affine all threads to a separate CPU\n -t    : one thread per available CPU\n --tracemark : enable trace mark writing\n -b 1000 : stop if any latency is greater than 1000 microseconds\n\nNote, the -b 1000 is used just to make --tracemark available.\n\nThen we can see the histogram created by this with:\n\n # cat events/synthetic/latency/hist\n# event histogram\n#\n# trigger info: hist:keys=lat,common_pid:vals=hitcount:sort=lat:size=2048 [active]\n#\n\n{ lat:        107, common_pid:       2039 } hitcount:          1\n{ lat:        122, common_pid:       2041 } hitcount:          1\n{ lat:        166, common_pid:       2039 } hitcount:          1\n{ lat:        174, common_pid:       2039 } hitcount:          1\n{ lat:        194, common_pid:       2041 } hitcount:          1\n{ lat:        196, common_pid:       2036 } hitcount:          1\n{ lat:        197, common_pid:       2038 } hitcount:          1\n{ lat:        198, common_pid:       2039 } hitcount:          1\n{ lat:        199, common_pid:       2039 } hitcount:          1\n{ lat:        200, common_pid:       2041 } hitcount:          1\n{ lat:        201, common_pid:       2039 } hitcount:          2\n{ lat:        202, common_pid:       2038 } hitcount:          1\n{ lat:        202, common_pid:       2043 } hitcount:          1\n{ lat:        203, common_pid:       2039 } hitcount:          1\n{ lat:        203, common_pid:       2036 } hitcount:          1\n{ lat:        203, common_pid:       2041 } hitcount:          1\n{ lat:        206, common_pid:       2038 } hitcount:          2\n{ lat:        207, common_pid:       2039 } hitcount:          1\n{ lat:        207, common_pid:       2036 } hitcount:          1\n{ lat:        208, common_pid:       2040 } hitcount:          1\n{ lat:        209, common_pid:       2043 } hitcount:          1\n{ lat:        210, common_pid:       2039 } hitcount:          1\n{ lat:        211, common_pid:       2039 } hitcount:          4\n{ lat:        212, common_pid:       2043 } hitcount:          1\n{ lat:        212, common_pid:       2039 } hitcount:          2\n{ lat:        213, common_pid:       2039 } hitcount:          1\n{ lat:        214, common_pid:       2038 } hitcount:          1\n{ lat:        214, common_pid:       2039 } hitcount:          2\n{ lat:        214, common_pid:       2042 } hitcount:          1\n{ lat:        215, common_pid:       2039 } hitcount:          1\n{ lat:        217, common_pid:       2036 } hitcount:          1\n{ lat:        217, common_pid:       2040 } hitcount:          1\n{ lat:        217, common_pid:       2039 } hitcount:          1\n{ lat:        218, common_pid:       2039 } hitcount:          6\n{ lat:        219, common_pid:       2039 } hitcount:          9\n{ lat:        220, common_pid:       2039 } hitcount:         11\n{ lat:        221, common_pid:       2039 } hitcount:          5\n{ lat:        221, common_pid:       2042 } hitcount:          1\n{ lat:        222, common_pid:       2039 } hitcount:          7\n{ lat:        223, common_pid:       2036 } hitcount:          1\n{ lat:        223, common_pid:       2039 } hitcount:          3\n{ lat:        224, common_pid:       2039 } hitcount:          4\n{ lat:        224, common_pid:       2037 } hitcount:          1\n{ lat:        224, common_pid:       2036 } hitcount:          2\n{ lat:        225, common_pid:       2039 } hitcount:          5\n{ lat:        225, common_pid:       2042 } hitcount:          1\n{ lat:        226, common_pid:       2039 } hitcount:          7\n{ lat:        226, common_pid:       2036 } hitcount:          4\n{ lat:        227, common_pid:       2039 } hitcount:          6\n{ lat:        227, common_pid:       2036 } hitcount:         12\n{ lat:        227, common_pid:       2043 } hitcount:          1\n{ lat:        228, common_pid:       2039 } hitcount:          7\n{ lat:        228, common_pid:       2036 } hitcount:         14\n{ lat:        229, common_pid:       2039 } hitcount:          9\n{ lat:        229, common_pid:       2036 } hitcount:          8\n{ lat:        229, common_pid:       2038 } hitcount:          1\n{ lat:        230, common_pid:       2039 } hitcount:         11\n{ lat:        230, common_pid:       2036 } hitcount:          6\n{ lat:        230, common_pid:       2043 } hitcount:          1\n{ lat:        230, common_pid:       2042 } hitcount:          2\n{ lat:        231, common_pid:       2041 } hitcount:          1\n{ lat:        231, common_pid:       2036 } hitcount:          6\n{ lat:        231, common_pid:       2043 } hitcount:          1\n{ lat:        231, common_pid:       2039 } hitcount:          8\n{ lat:        232, common_pid:       2037 } hitcount:          1\n{ lat:        232, common_pid:       2039 } hitcount:          6\n{ lat:        232, common_pid:       2040 } hitcount:          2\n{ lat:        232, common_pid:       2036 } hitcount:          5\n{ lat:        232, common_pid:       2043 } hitcount:          1\n{ lat:        233, common_pid:       2036 } hitcount:          5\n{ lat:        233, common_pid:       2039 } hitcount:         11\n{ lat:        234, common_pid:       2039 } hitcount:          4\n{ lat:        234, common_pid:       2038 } hitcount:          2\n{ lat:        234, common_pid:       2043 } hitcount:          2\n{ lat:        234, common_pid:       2036 } hitcount:         11\n{ lat:        234, common_pid:       2040 } hitcount:          1\n{ lat:        235, common_pid:       2037 } hitcount:          2\n{ lat:        235, common_pid:       2036 } hitcount:          8\n{ lat:        235, common_pid:       2043 } hitcount:          2\n{ lat:        235, common_pid:       2039 } hitcount:          5\n{ lat:        235, common_pid:       2042 } hitcount:          2\n{ lat:        235, common_pid:       2040 } hitcount:          4\n{ lat:        235, common_pid:       2041 } hitcount:          1\n{ lat:        236, common_pid:       2036 } hitcount:          7\n{ lat:        236, common_pid:       2037 } hitcount:          1\n{ lat:        236, common_pid:       2041 } hitcount:          5\n{ lat:        236, common_pid:       2039 } hitcount:          3\n{ lat:        236, common_pid:       2043 } hitcount:          9\n{ lat:        236, common_pid:       2040 } hitcount:          7\n{ lat:        237, common_pid:       2037 } hitcount:          1\n{ lat:        237, common_pid:       2040 } hitcount:          1\n{ lat:        237, common_pid:       2036 } hitcount:          9\n{ lat:        237, common_pid:       2039 } hitcount:          3\n{ lat:        237, common_pid:       2043 } hitcount:          8\n{ lat:        237, common_pid:       2042 } hitcount:          2\n{ lat:        237, common_pid:       2041 } hitcount:          2\n{ lat:        238, common_pid:       2043 } hitcount:         10\n{ lat:        238, common_pid:       2040 } hitcount:          1\n{ lat:        238, common_pid:       2037 } hitcount:          9\n{ lat:        238, common_pid:       2038 } hitcount:          1\n{ lat:        238, common_pid:       2039 } hitcount:          1\n{ lat:        238, common_pid:       2042 } hitcount:          3\n{ lat:        238, common_pid:       2036 } hitcount:          7\n{ lat:        239, common_pid:       2041 } hitcount:          1\n{ lat:        239, common_pid:       2043 } hitcount:         11\n{ lat:        239, common_pid:       2037 } hitcount:         11\n{ lat:        239, common_pid:       2038 } hitcount:          6\n{ lat:        239, common_pid:       2036 } hitcount:          7\n{ lat:        239, common_pid:       2040 } hitcount:          1\n{ lat:        239, common_pid:       2042 } hitcount:          9\n{ lat:        240, common_pid:       2037 } hitcount:         29\n{ lat:        240, common_pid:       2043 } hitcount:         15\n{ lat:        240, common_pid:       2040 } hitcount:         44\n{ lat:        240, common_pid:       2039 } hitcount:          1\n{ lat:        240, common_pid:       2041 } hitcount:          2\n{ lat:        240, common_pid:       2038 } hitcount:          1\n{ lat:        240, common_pid:       2036 } hitcount:         10\n{ lat:        240, common_pid:       2042 } hitcount:         13\n{ lat:        241, common_pid:       2036 } hitcount:         21\n{ lat:        241, common_pid:       2041 } hitcount:         36\n{ lat:        241, common_pid:       2037 } hitcount:         34\n{ lat:        241, common_pid:       2042 } hitcount:         14\n{ lat:        241, common_pid:       2040 } hitcount:         94\n{ lat:        241, common_pid:       2039 } hitcount:         12\n{ lat:        241, common_pid:       2038 } hitcount:          2\n{ lat:        241, common_pid:       2043 } hitcount:         28\n{ lat:        242, common_pid:       2040 } hitcount:        109\n{ lat:        242, common_pid:       2041 } hitcount:        506\n{ lat:        242, common_pid:       2039 } hitcount:        155\n{ lat:        242, common_pid:       2042 } hitcount:         21\n{ lat:        242, common_pid:       2037 } hitcount:         52\n{ lat:        242, common_pid:       2043 } hitcount:         21\n{ lat:        242, common_pid:       2036 } hitcount:         16\n{ lat:        242, common_pid:       2038 } hitcount:        156\n{ lat:        243, common_pid:       2037 } hitcount:         46\n{ lat:        243, common_pid:       2039 } hitcount:         40\n{ lat:        243, common_pid:       2042 } hitcount:        119\n{ lat:        243, common_pid:       2041 } hitcount:        611\n{ lat:        243, common_pid:       2036 } hitcount:         69\n{ lat:        243, common_pid:       2038 } hitcount:        784\n{ lat:        243, common_pid:       2040 } hitcount:        323\n{ lat:        243, common_pid:       2043 } hitcount:         14\n{ lat:        244, common_pid:       2043 } hitcount:         35\n{ lat:        244, common_pid:       2042 } hitcount:        305\n{ lat:        244, common_pid:       2039 } hitcount:          8\n{ lat:        244, common_pid:       2040 } hitcount:       4515\n{ lat:        244, common_pid:       2038 } hitcount:        371\n{ lat:        244, common_pid:       2037 } hitcount:         31\n{ lat:        244, common_pid:       2036 } hitcount:        114\n{ lat:        244, common_pid:       2041 } hitcount:       3396\n{ lat:        245, common_pid:       2036 } hitcount:        700\n{ lat:        245, common_pid:       2041 } hitcount:       2772\n{ lat:        245, common_pid:       2037 } hitcount:        268\n{ lat:        245, common_pid:       2039 } hitcount:        472\n{ lat:        245, common_pid:       2038 } hitcount:       2758\n{ lat:        245, common_pid:       2042 } hitcount:       3833\n{ lat:        245, common_pid:       2040 } hitcount:       3105\n{ lat:        245, common_pid:       2043 } hitcount:        645\n{ lat:        246, common_pid:       2038 } hitcount:       3451\n{ lat:        246, common_pid:       2041 } hitcount:        142\n{ lat:        246, common_pid:       2037 } hitcount:       5101\n{ lat:        246, common_pid:       2040 } hitcount:         68\n{ lat:        246, common_pid:       2043 } hitcount:       5099\n{ lat:        246, common_pid:       2039 } hitcount:       5608\n{ lat:        246, common_pid:       2042 } hitcount:       3723\n{ lat:        246, common_pid:       2036 } hitcount:       4738\n{ lat:        247, common_pid:       2042 } hitcount:        312\n{ lat:        247, common_pid:       2043 } hitcount:       2385\n{ lat:        247, common_pid:       2041 } hitcount:        452\n{ lat:        247, common_pid:       2038 } hitcount:        792\n{ lat:        247, common_pid:       2040 } hitcount:         78\n{ lat:        247, common_pid:       2036 } hitcount:       2375\n{ lat:        247, common_pid:       2039 } hitcount:       1834\n{ lat:        247, common_pid:       2037 } hitcount:       2655\n{ lat:        248, common_pid:       2037 } hitcount:         36\n{ lat:        248, common_pid:       2042 } hitcount:         11\n{ lat:        248, common_pid:       2038 } hitcount:        122\n{ lat:        248, common_pid:       2036 } hitcount:        135\n{ lat:        248, common_pid:       2039 } hitcount:         26\n{ lat:        248, common_pid:       2041 } hitcount:        503\n{ lat:        248, common_pid:       2043 } hitcount:         66\n{ lat:        248, common_pid:       2040 } hitcount:         46\n{ lat:        249, common_pid:       2037 } hitcount:         29\n{ lat:        249, common_pid:       2038 } hitcount:          1\n{ lat:        249, common_pid:       2043 } hitcount:         29\n{ lat:        249, common_pid:       2039 } hitcount:          8\n{ lat:        249, common_pid:       2042 } hitcount:         56\n{ lat:        249, common_pid:       2040 } hitcount:         27\n{ lat:        249, common_pid:       2041 } hitcount:         11\n{ lat:        249, common_pid:       2036 } hitcount:         27\n{ lat:        250, common_pid:       2038 } hitcount:          1\n{ lat:        250, common_pid:       2036 } hitcount:         30\n{ lat:        250, common_pid:       2040 } hitcount:         19\n{ lat:        250, common_pid:       2043 } hitcount:         22\n{ lat:        250, common_pid:       2042 } hitcount:         20\n{ lat:        250, common_pid:       2041 } hitcount:          1\n{ lat:        250, common_pid:       2039 } hitcount:          6\n{ lat:        250, common_pid:       2037 } hitcount:         48\n{ lat:        251, common_pid:       2037 } hitcount:         43\n{ lat:        251, common_pid:       2039 } hitcount:          1\n{ lat:        251, common_pid:       2036 } hitcount:         12\n{ lat:        251, common_pid:       2042 } hitcount:          2\n{ lat:        251, common_pid:       2041 } hitcount:          1\n{ lat:        251, common_pid:       2043 } hitcount:         15\n{ lat:        251, common_pid:       2040 } hitcount:          3\n{ lat:        252, common_pid:       2040 } hitcount:          1\n{ lat:        252, common_pid:       2036 } hitcount:         12\n{ lat:        252, common_pid:       2037 } hitcount:         21\n{ lat:        252, common_pid:       2043 } hitcount:         14\n{ lat:        253, common_pid:       2037 } hitcount:         21\n{ lat:        253, common_pid:       2039 } hitcount:          2\n{ lat:        253, common_pid:       2036 } hitcount:          9\n{ lat:        253, common_pid:       2043 } hitcount:          6\n{ lat:        253, common_pid:       2040 } hitcount:          1\n{ lat:        254, common_pid:       2036 } hitcount:          8\n{ lat:        254, common_pid:       2043 } hitcount:          3\n{ lat:        254, common_pid:       2041 } hitcount:          1\n{ lat:        254, common_pid:       2042 } hitcount:          1\n{ lat:        254, common_pid:       2039 } hitcount:          1\n{ lat:        254, common_pid:       2037 } hitcount:         12\n{ lat:        255, common_pid:       2043 } hitcount:          1\n{ lat:        255, common_pid:       2037 } hitcount:          2\n{ lat:        255, common_pid:       2036 } hitcount:          2\n{ lat:        255, common_pid:       2039 } hitcount:          8\n{ lat:        256, common_pid:       2043 } hitcount:          1\n{ lat:        256, common_pid:       2036 } hitcount:          4\n{ lat:        256, common_pid:       2039 } hitcount:          6\n{ lat:        257, common_pid:       2039 } hitcount:          5\n{ lat:        257, common_pid:       2036 } hitcount:          4\n{ lat:        258, common_pid:       2039 } hitcount:          5\n{ lat:        258, common_pid:       2036 } hitcount:          2\n{ lat:        259, common_pid:       2036 } hitcount:          7\n{ lat:        259, common_pid:       2039 } hitcount:          7\n{ lat:        260, common_pid:       2036 } hitcount:          8\n{ lat:        260, common_pid:       2039 } hitcount:          6\n{ lat:        261, common_pid:       2036 } hitcount:          5\n{ lat:        261, common_pid:       2039 } hitcount:          7\n{ lat:        262, common_pid:       2039 } hitcount:          5\n{ lat:        262, common_pid:       2036 } hitcount:          5\n{ lat:        263, common_pid:       2039 } hitcount:          7\n{ lat:        263, common_pid:       2036 } hitcount:          7\n{ lat:        264, common_pid:       2039 } hitcount:          9\n{ lat:        264, common_pid:       2036 } hitcount:          9\n{ lat:        265, common_pid:       2036 } hitcount:          5\n{ lat:        265, common_pid:       2039 } hitcount:          1\n{ lat:        266, common_pid:       2036 } hitcount:          1\n{ lat:        266, common_pid:       2039 } hitcount:          3\n{ lat:        267, common_pid:       2036 } hitcount:          1\n{ lat:        267, common_pid:       2039 } hitcount:          3\n{ lat:        268, common_pid:       2036 } hitcount:          1\n{ lat:        268, common_pid:       2039 } hitcount:          6\n{ lat:        269, common_pid:       2036 } hitcount:          1\n{ lat:        269, common_pid:       2043 } hitcount:          1\n{ lat:        269, common_pid:       2039 } hitcount:          2\n{ lat:        270, common_pid:       2040 } hitcount:          1\n{ lat:        270, common_pid:       2039 } hitcount:          6\n{ lat:        271, common_pid:       2041 } hitcount:          1\n{ lat:        271, common_pid:       2039 } hitcount:          5\n{ lat:        272, common_pid:       2039 } hitcount:         10\n{ lat:        273, common_pid:       2039 } hitcount:          8\n{ lat:        274, common_pid:       2039 } hitcount:          2\n{ lat:        275, common_pid:       2039 } hitcount:          1\n{ lat:        276, common_pid:       2039 } hitcount:          2\n{ lat:        276, common_pid:       2037 } hitcount:          1\n{ lat:        276, common_pid:       2038 } hitcount:          1\n{ lat:        277, common_pid:       2039 } hitcount:          1\n{ lat:        277, common_pid:       2042 } hitcount:          1\n{ lat:        278, common_pid:       2039 } hitcount:          1\n{ lat:        279, common_pid:       2039 } hitcount:          4\n{ lat:        279, common_pid:       2043 } hitcount:          1\n{ lat:        280, common_pid:       2039 } hitcount:          3\n{ lat:        283, common_pid:       2036 } hitcount:          2\n{ lat:        284, common_pid:       2039 } hitcount:          1\n{ lat:        284, common_pid:       2043 } hitcount:          1\n{ lat:        288, common_pid:       2039 } hitcount:          1\n{ lat:        289, common_pid:       2039 } hitcount:          1\n{ lat:        300, common_pid:       2039 } hitcount:          1\n{ lat:        384, common_pid:       2039 } hitcount:          1\n\nTotals:\n    Hits: 67625\n    Entries: 278\n    Dropped: 0\n\nNote, the writes are around the sleep, so ideally they will all be of 250\nmicroseconds. If you are wondering how there are several that are under\n250 microseconds, that is because the way cyclictest works, is if one\niteration comes in late, the next one will set the timer to wake up less that\n250. That is, if an iteration came in 50 microseconds late, the next wake up\nwill be at 200 microseconds.\n\nBut this could easily be done in userspace. To make this even more\ninteresting, we can mix the histogram between events that happened in the\nkernel with trace_marker.\n\n # cd /sys/kernel/tracing\n # echo 'latency u64 lat' > synthetic_events\n # echo 'hist:keys=pid:ts0=common_timestamp.usecs' > events/sched/sched_waking/trigger\n # echo 'hist:keys=common_pid:lat=common_timestamp.usecs-$ts0:onmatch(sched.sched_waking).latency($lat) if buf == \"end\"' > events/ftrace/print/trigger\n # echo 'hist:keys=lat,common_pid:sort=lat' > events/synthetic/latency/trigger\n\nThe difference this time is that instead of using the trace_marker to start\nthe latency, the sched_waking event is used, matching the common_pid for the\ntrace_marker write with the pid that is being woken by sched_waking.\n\nAfter running cyclictest again with the same parameters, we now have:\n\n # cat events/synthetic/latency/hist\n# event histogram\n#\n# trigger info: hist:keys=lat,common_pid:vals=hitcount:sort=lat:size=2048 [active]\n#\n\n{ lat:          7, common_pid:       2302 } hitcount:        640\n{ lat:          7, common_pid:       2299 } hitcount:         42\n{ lat:          7, common_pid:       2303 } hitcount:         18\n{ lat:          7, common_pid:       2305 } hitcount:        166\n{ lat:          7, common_pid:       2306 } hitcount:          1\n{ lat:          7, common_pid:       2301 } hitcount:         91\n{ lat:          7, common_pid:       2300 } hitcount:         17\n{ lat:          8, common_pid:       2303 } hitcount:       8296\n{ lat:          8, common_pid:       2304 } hitcount:       6864\n{ lat:          8, common_pid:       2305 } hitcount:       9464\n{ lat:          8, common_pid:       2301 } hitcount:       9213\n{ lat:          8, common_pid:       2306 } hitcount:       6246\n{ lat:          8, common_pid:       2302 } hitcount:       8797\n{ lat:          8, common_pid:       2299 } hitcount:       8771\n{ lat:          8, common_pid:       2300 } hitcount:       8119\n{ lat:          9, common_pid:       2305 } hitcount:       1519\n{ lat:          9, common_pid:       2299 } hitcount:       2346\n{ lat:          9, common_pid:       2303 } hitcount:       2841\n{ lat:          9, common_pid:       2301 } hitcount:       1846\n{ lat:          9, common_pid:       2304 } hitcount:       3861\n{ lat:          9, common_pid:       2302 } hitcount:       1210\n{ lat:          9, common_pid:       2300 } hitcount:       2762\n{ lat:          9, common_pid:       2306 } hitcount:       4247\n{ lat:         10, common_pid:       2299 } hitcount:         16\n{ lat:         10, common_pid:       2306 } hitcount:        333\n{ lat:         10, common_pid:       2303 } hitcount:         16\n{ lat:         10, common_pid:       2304 } hitcount:        168\n{ lat:         10, common_pid:       2302 } hitcount:        240\n{ lat:         10, common_pid:       2301 } hitcount:         28\n{ lat:         10, common_pid:       2300 } hitcount:         95\n{ lat:         10, common_pid:       2305 } hitcount:         18\n{ lat:         11, common_pid:       2303 } hitcount:          5\n{ lat:         11, common_pid:       2305 } hitcount:          8\n{ lat:         11, common_pid:       2306 } hitcount:        221\n{ lat:         11, common_pid:       2302 } hitcount:         76\n{ lat:         11, common_pid:       2304 } hitcount:         26\n{ lat:         11, common_pid:       2300 } hitcount:        125\n{ lat:         11, common_pid:       2299 } hitcount:          2\n{ lat:         12, common_pid:       2305 } hitcount:          3\n{ lat:         12, common_pid:       2300 } hitcount:          6\n{ lat:         12, common_pid:       2306 } hitcount:         90\n{ lat:         12, common_pid:       2302 } hitcount:          4\n{ lat:         12, common_pid:       2303 } hitcount:          1\n{ lat:         12, common_pid:       2304 } hitcount:        122\n{ lat:         13, common_pid:       2300 } hitcount:         12\n{ lat:         13, common_pid:       2301 } hitcount:          1\n{ lat:         13, common_pid:       2306 } hitcount:         32\n{ lat:         13, common_pid:       2302 } hitcount:          5\n{ lat:         13, common_pid:       2305 } hitcount:          1\n{ lat:         13, common_pid:       2303 } hitcount:          1\n{ lat:         13, common_pid:       2304 } hitcount:         61\n{ lat:         14, common_pid:       2303 } hitcount:          4\n{ lat:         14, common_pid:       2306 } hitcount:          5\n{ lat:         14, common_pid:       2305 } hitcount:          4\n{ lat:         14, common_pid:       2304 } hitcount:         62\n{ lat:         14, common_pid:       2302 } hitcount:         19\n{ lat:         14, common_pid:       2300 } hitcount:         33\n{ lat:         14, common_pid:       2299 } hitcount:          1\n{ lat:         14, common_pid:       2301 } hitcount:          4\n{ lat:         15, common_pid:       2305 } hitcount:          1\n{ lat:         15, common_pid:       2302 } hitcount:         25\n{ lat:         15, common_pid:       2300 } hitcount:         11\n{ lat:         15, common_pid:       2299 } hitcount:          5\n{ lat:         15, common_pid:       2301 } hitcount:          1\n{ lat:         15, common_pid:       2304 } hitcount:          8\n{ lat:         15, common_pid:       2303 } hitcount:          1\n{ lat:         15, common_pid:       2306 } hitcount:          6\n{ lat:         16, common_pid:       2302 } hitcount:         31\n{ lat:         16, common_pid:       2306 } hitcount:          3\n{ lat:         16, common_pid:       2300 } hitcount:          5\n{ lat:         17, common_pid:       2302 } hitcount:          6\n{ lat:         17, common_pid:       2303 } hitcount:          1\n{ lat:         18, common_pid:       2304 } hitcount:          1\n{ lat:         18, common_pid:       2302 } hitcount:          8\n{ lat:         18, common_pid:       2299 } hitcount:          1\n{ lat:         18, common_pid:       2301 } hitcount:          1\n{ lat:         19, common_pid:       2303 } hitcount:          4\n{ lat:         19, common_pid:       2304 } hitcount:          5\n{ lat:         19, common_pid:       2302 } hitcount:          4\n{ lat:         19, common_pid:       2299 } hitcount:          3\n{ lat:         19, common_pid:       2306 } hitcount:          1\n{ lat:         19, common_pid:       2300 } hitcount:          4\n{ lat:         19, common_pid:       2305 } hitcount:          5\n{ lat:         20, common_pid:       2299 } hitcount:          2\n{ lat:         20, common_pid:       2302 } hitcount:          3\n{ lat:         20, common_pid:       2305 } hitcount:          1\n{ lat:         20, common_pid:       2300 } hitcount:          2\n{ lat:         20, common_pid:       2301 } hitcount:          2\n{ lat:         20, common_pid:       2303 } hitcount:          3\n{ lat:         21, common_pid:       2305 } hitcount:          1\n{ lat:         21, common_pid:       2299 } hitcount:          5\n{ lat:         21, common_pid:       2303 } hitcount:          4\n{ lat:         21, common_pid:       2302 } hitcount:          7\n{ lat:         21, common_pid:       2300 } hitcount:          1\n{ lat:         21, common_pid:       2301 } hitcount:          5\n{ lat:         21, common_pid:       2304 } hitcount:          2\n{ lat:         22, common_pid:       2302 } hitcount:          5\n{ lat:         22, common_pid:       2303 } hitcount:          1\n{ lat:         22, common_pid:       2306 } hitcount:          3\n{ lat:         22, common_pid:       2301 } hitcount:          2\n{ lat:         22, common_pid:       2300 } hitcount:          1\n{ lat:         22, common_pid:       2299 } hitcount:          1\n{ lat:         22, common_pid:       2305 } hitcount:          1\n{ lat:         22, common_pid:       2304 } hitcount:          1\n{ lat:         23, common_pid:       2299 } hitcount:          1\n{ lat:         23, common_pid:       2306 } hitcount:          2\n{ lat:         23, common_pid:       2302 } hitcount:          6\n{ lat:         24, common_pid:       2302 } hitcount:          3\n{ lat:         24, common_pid:       2300 } hitcount:          1\n{ lat:         24, common_pid:       2306 } hitcount:          2\n{ lat:         24, common_pid:       2305 } hitcount:          1\n{ lat:         24, common_pid:       2299 } hitcount:          1\n{ lat:         25, common_pid:       2300 } hitcount:          1\n{ lat:         25, common_pid:       2302 } hitcount:          4\n{ lat:         26, common_pid:       2302 } hitcount:          2\n{ lat:         27, common_pid:       2305 } hitcount:          1\n{ lat:         27, common_pid:       2300 } hitcount:          1\n{ lat:         27, common_pid:       2302 } hitcount:          3\n{ lat:         28, common_pid:       2306 } hitcount:          1\n{ lat:         28, common_pid:       2302 } hitcount:          4\n{ lat:         29, common_pid:       2302 } hitcount:          1\n{ lat:         29, common_pid:       2300 } hitcount:          2\n{ lat:         29, common_pid:       2306 } hitcount:          1\n{ lat:         29, common_pid:       2304 } hitcount:          1\n{ lat:         30, common_pid:       2302 } hitcount:          4\n{ lat:         31, common_pid:       2302 } hitcount:          6\n{ lat:         32, common_pid:       2302 } hitcount:          1\n{ lat:         33, common_pid:       2299 } hitcount:          1\n{ lat:         33, common_pid:       2302 } hitcount:          3\n{ lat:         34, common_pid:       2302 } hitcount:          2\n{ lat:         35, common_pid:       2302 } hitcount:          1\n{ lat:         35, common_pid:       2304 } hitcount:          1\n{ lat:         36, common_pid:       2302 } hitcount:          4\n{ lat:         37, common_pid:       2302 } hitcount:          6\n{ lat:         38, common_pid:       2302 } hitcount:          2\n{ lat:         39, common_pid:       2302 } hitcount:          2\n{ lat:         39, common_pid:       2304 } hitcount:          1\n{ lat:         40, common_pid:       2304 } hitcount:          2\n{ lat:         40, common_pid:       2302 } hitcount:          5\n{ lat:         41, common_pid:       2304 } hitcount:          1\n{ lat:         41, common_pid:       2302 } hitcount:          8\n{ lat:         42, common_pid:       2302 } hitcount:          6\n{ lat:         42, common_pid:       2304 } hitcount:          1\n{ lat:         43, common_pid:       2302 } hitcount:          3\n{ lat:         43, common_pid:       2304 } hitcount:          4\n{ lat:         44, common_pid:       2302 } hitcount:          6\n{ lat:         45, common_pid:       2302 } hitcount:          5\n{ lat:         46, common_pid:       2302 } hitcount:          5\n{ lat:         47, common_pid:       2302 } hitcount:          7\n{ lat:         48, common_pid:       2301 } hitcount:          1\n{ lat:         48, common_pid:       2302 } hitcount:          9\n{ lat:         49, common_pid:       2302 } hitcount:          3\n{ lat:         50, common_pid:       2302 } hitcount:          1\n{ lat:         50, common_pid:       2301 } hitcount:          1\n{ lat:         51, common_pid:       2302 } hitcount:          2\n{ lat:         51, common_pid:       2301 } hitcount:          1\n{ lat:         61, common_pid:       2302 } hitcount:          1\n{ lat:        110, common_pid:       2302 } hitcount:          1\n\nTotals:\n    Hits: 89565\n    Entries: 158\n    Dropped: 0\n\nThis doesn't tell us any information about how late cyclictest may have\nwoken up, but it does show us a nice histogram of how long it took from\nthe time that cyclictest was woken to the time it made it into user space.\n", "/*\n *\tlinux/kernel/softirq.c\n *\n *\tCopyright (C) 1992 Linus Torvalds\n *\n *\tDistribute under GPLv2.\n *\n *\tRewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/export.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/cpu.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include <linux/rcupdate.h>\n#include <linux/ftrace.h>\n#include <linux/smp.h>\n#include <linux/smpboot.h>\n#include <linux/tick.h>\n#include <linux/irq.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/irq.h>\n\n/*\n   - No shared variables, all the data are CPU local.\n   - If a softirq needs serialization, let it serialize itself\n     by its own spinlocks.\n   - Even if softirq is serialized, only local cpu is marked for\n     execution. Hence, we get something sort of weak cpu binding.\n     Though it is still not clear, will it result in better locality\n     or will not.\n\n   Examples:\n   - NET RX softirq. It is multithreaded and does not require\n     any global serialization.\n   - NET TX softirq. It kicks software netdevice queues, hence\n     it is logically serialized per device, but this serialization\n     is invisible to common code.\n   - Tasklets: serialized wrt itself.\n */\n\n#ifndef __ARCH_IRQ_STAT\nDEFINE_PER_CPU_ALIGNED(irq_cpustat_t, irq_stat);\nEXPORT_PER_CPU_SYMBOL(irq_stat);\n#endif\n\nstatic struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;\n\nDEFINE_PER_CPU(struct task_struct *, ksoftirqd);\n\nconst char * const softirq_to_name[NR_SOFTIRQS] = {\n\t\"HI\", \"TIMER\", \"NET_TX\", \"NET_RX\", \"BLOCK\", \"IRQ_POLL\",\n\t\"TASKLET\", \"SCHED\", \"HRTIMER\", \"RCU\"\n};\n\n/*\n * we cannot loop indefinitely here to avoid userspace starvation,\n * but we also don't want to introduce a worst case 1/HZ latency\n * to the pending events, so lets the scheduler to balance\n * the softirq load for us.\n */\nstatic void wakeup_softirqd(void)\n{\n\t/* Interrupts are disabled: no need to stop preemption */\n\tstruct task_struct *tsk = __this_cpu_read(ksoftirqd);\n\n\tif (tsk && tsk->state != TASK_RUNNING)\n\t\twake_up_process(tsk);\n}\n\n/*\n * If ksoftirqd is scheduled, we do not want to process pending softirqs\n * right now. Let ksoftirqd handle this at its own rate, to get fairness.\n */\nstatic bool ksoftirqd_running(void)\n{\n\tstruct task_struct *tsk = __this_cpu_read(ksoftirqd);\n\n\treturn tsk && (tsk->state == TASK_RUNNING);\n}\n\n/*\n * preempt_count and SOFTIRQ_OFFSET usage:\n * - preempt_count is changed by SOFTIRQ_OFFSET on entering or leaving\n *   softirq processing.\n * - preempt_count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)\n *   on local_bh_disable or local_bh_enable.\n * This lets us distinguish between whether we are currently processing\n * softirq and whether we just have bh disabled.\n */\n\n/*\n * This one is for softirq.c-internal use,\n * where hardirqs are disabled legitimately:\n */\n#ifdef CONFIG_TRACE_IRQFLAGS\nvoid __local_bh_disable_ip(unsigned long ip, unsigned int cnt)\n{\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(in_irq());\n\n\traw_local_irq_save(flags);\n\t/*\n\t * The preempt tracer hooks into preempt_count_add and will break\n\t * lockdep because it calls back into lockdep after SOFTIRQ_OFFSET\n\t * is set and before current->softirq_enabled is cleared.\n\t * We must manually increment preempt_count here and manually\n\t * call the trace_preempt_off later.\n\t */\n\t__preempt_count_add(cnt);\n\t/*\n\t * Were softirqs turned off above:\n\t */\n\tif (softirq_count() == (cnt & SOFTIRQ_MASK))\n\t\ttrace_softirqs_off(ip);\n\traw_local_irq_restore(flags);\n\n\tif (preempt_count() == cnt) {\n#ifdef CONFIG_DEBUG_PREEMPT\n\t\tcurrent->preempt_disable_ip = get_lock_parent_ip();\n#endif\n\t\ttrace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());\n\t}\n}\nEXPORT_SYMBOL(__local_bh_disable_ip);\n#endif /* CONFIG_TRACE_IRQFLAGS */\n\nstatic void __local_bh_enable(unsigned int cnt)\n{\n\tlockdep_assert_irqs_disabled();\n\n\tif (preempt_count() == cnt)\n\t\ttrace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());\n\n\tif (softirq_count() == (cnt & SOFTIRQ_MASK))\n\t\ttrace_softirqs_on(_RET_IP_);\n\n\t__preempt_count_sub(cnt);\n}\n\n/*\n * Special-case - softirqs can safely be enabled by __do_softirq(),\n * without processing still-pending softirqs:\n */\nvoid _local_bh_enable(void)\n{\n\tWARN_ON_ONCE(in_irq());\n\t__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);\n}\nEXPORT_SYMBOL(_local_bh_enable);\n\nvoid __local_bh_enable_ip(unsigned long ip, unsigned int cnt)\n{\n\tWARN_ON_ONCE(in_irq());\n\tlockdep_assert_irqs_enabled();\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tlocal_irq_disable();\n#endif\n\t/*\n\t * Are softirqs going to be turned on now:\n\t */\n\tif (softirq_count() == SOFTIRQ_DISABLE_OFFSET)\n\t\ttrace_softirqs_on(ip);\n\t/*\n\t * Keep preemption disabled until we are done with\n\t * softirq processing:\n\t */\n\tpreempt_count_sub(cnt - 1);\n\n\tif (unlikely(!in_interrupt() && local_softirq_pending())) {\n\t\t/*\n\t\t * Run softirq if any pending. And do it in its own stack\n\t\t * as we may be calling this deep in a task call stack already.\n\t\t */\n\t\tdo_softirq();\n\t}\n\n\tpreempt_count_dec();\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tlocal_irq_enable();\n#endif\n\tpreempt_check_resched();\n}\nEXPORT_SYMBOL(__local_bh_enable_ip);\n\n/*\n * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,\n * but break the loop if need_resched() is set or after 2 ms.\n * The MAX_SOFTIRQ_TIME provides a nice upper bound in most cases, but in\n * certain cases, such as stop_machine(), jiffies may cease to\n * increment and so we need the MAX_SOFTIRQ_RESTART limit as\n * well to make sure we eventually return from this method.\n *\n * These limits have been established via experimentation.\n * The two things to balance is latency against fairness -\n * we want to handle softirqs as soon as possible, but they\n * should not be able to lock up the box.\n */\n#define MAX_SOFTIRQ_TIME  msecs_to_jiffies(2)\n#define MAX_SOFTIRQ_RESTART 10\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n/*\n * When we run softirqs from irq_exit() and thus on the hardirq stack we need\n * to keep the lockdep irq context tracking as tight as possible in order to\n * not miss-qualify lock contexts and miss possible deadlocks.\n */\n\nstatic inline bool lockdep_softirq_start(void)\n{\n\tbool in_hardirq = false;\n\n\tif (trace_hardirq_context(current)) {\n\t\tin_hardirq = true;\n\t\ttrace_hardirq_exit();\n\t}\n\n\tlockdep_softirq_enter();\n\n\treturn in_hardirq;\n}\n\nstatic inline void lockdep_softirq_end(bool in_hardirq)\n{\n\tlockdep_softirq_exit();\n\n\tif (in_hardirq)\n\t\ttrace_hardirq_enter();\n}\n#else\nstatic inline bool lockdep_softirq_start(void) { return false; }\nstatic inline void lockdep_softirq_end(bool in_hardirq) { }\n#endif\n\nasmlinkage __visible void __softirq_entry __do_softirq(void)\n{\n\tunsigned long end = jiffies + MAX_SOFTIRQ_TIME;\n\tunsigned long old_flags = current->flags;\n\tint max_restart = MAX_SOFTIRQ_RESTART;\n\tstruct softirq_action *h;\n\tbool in_hardirq;\n\t__u32 pending;\n\tint softirq_bit;\n\n\t/*\n\t * Mask out PF_MEMALLOC s current task context is borrowed for the\n\t * softirq. A softirq handled such as network RX might set PF_MEMALLOC\n\t * again if the socket is related to swap\n\t */\n\tcurrent->flags &= ~PF_MEMALLOC;\n\n\tpending = local_softirq_pending();\n\taccount_irq_enter_time(current);\n\n\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);\n\tin_hardirq = lockdep_softirq_start();\n\nrestart:\n\t/* Reset the pending bitmask before enabling irqs */\n\tset_softirq_pending(0);\n\n\tlocal_irq_enable();\n\n\th = softirq_vec;\n\n\twhile ((softirq_bit = ffs(pending))) {\n\t\tunsigned int vec_nr;\n\t\tint prev_count;\n\n\t\th += softirq_bit - 1;\n\n\t\tvec_nr = h - softirq_vec;\n\t\tprev_count = preempt_count();\n\n\t\tkstat_incr_softirqs_this_cpu(vec_nr);\n\n\t\ttrace_softirq_entry(vec_nr);\n\t\th->action(h);\n\t\ttrace_softirq_exit(vec_nr);\n\t\tif (unlikely(prev_count != preempt_count())) {\n\t\t\tpr_err(\"huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\\n\",\n\t\t\t       vec_nr, softirq_to_name[vec_nr], h->action,\n\t\t\t       prev_count, preempt_count());\n\t\t\tpreempt_count_set(prev_count);\n\t\t}\n\t\th++;\n\t\tpending >>= softirq_bit;\n\t}\n\n\trcu_bh_qs();\n\tlocal_irq_disable();\n\n\tpending = local_softirq_pending();\n\tif (pending) {\n\t\tif (time_before(jiffies, end) && !need_resched() &&\n\t\t    --max_restart)\n\t\t\tgoto restart;\n\n\t\twakeup_softirqd();\n\t}\n\n\tlockdep_softirq_end(in_hardirq);\n\taccount_irq_exit_time(current);\n\t__local_bh_enable(SOFTIRQ_OFFSET);\n\tWARN_ON_ONCE(in_interrupt());\n\tcurrent_restore_flags(old_flags, PF_MEMALLOC);\n}\n\nasmlinkage __visible void do_softirq(void)\n{\n\t__u32 pending;\n\tunsigned long flags;\n\n\tif (in_interrupt())\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tpending = local_softirq_pending();\n\n\tif (pending && !ksoftirqd_running())\n\t\tdo_softirq_own_stack();\n\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Enter an interrupt context.\n */\nvoid irq_enter(void)\n{\n\trcu_irq_enter();\n\tif (is_idle_task(current) && !in_interrupt()) {\n\t\t/*\n\t\t * Prevent raise_softirq from needlessly waking up ksoftirqd\n\t\t * here, as softirq will be serviced on return from interrupt.\n\t\t */\n\t\tlocal_bh_disable();\n\t\ttick_irq_enter();\n\t\t_local_bh_enable();\n\t}\n\n\t__irq_enter();\n}\n\nstatic inline void invoke_softirq(void)\n{\n\tif (ksoftirqd_running())\n\t\treturn;\n\n\tif (!force_irqthreads) {\n#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK\n\t\t/*\n\t\t * We can safely execute softirq on the current stack if\n\t\t * it is the irq stack, because it should be near empty\n\t\t * at this stage.\n\t\t */\n\t\t__do_softirq();\n#else\n\t\t/*\n\t\t * Otherwise, irq_exit() is called on the task stack that can\n\t\t * be potentially deep already. So call softirq in its own stack\n\t\t * to prevent from any overrun.\n\t\t */\n\t\tdo_softirq_own_stack();\n#endif\n\t} else {\n\t\twakeup_softirqd();\n\t}\n}\n\nstatic inline void tick_irq_exit(void)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tint cpu = smp_processor_id();\n\n\t/* Make sure that timer wheel updates are propagated */\n\tif ((idle_cpu(cpu) && !need_resched()) || tick_nohz_full_cpu(cpu)) {\n\t\tif (!in_interrupt())\n\t\t\ttick_nohz_irq_exit();\n\t}\n#endif\n}\n\n/*\n * Exit an interrupt context. Process softirqs if needed and possible:\n */\nvoid irq_exit(void)\n{\n#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED\n\tlocal_irq_disable();\n#else\n\tlockdep_assert_irqs_disabled();\n#endif\n\taccount_irq_exit_time(current);\n\tpreempt_count_sub(HARDIRQ_OFFSET);\n\tif (!in_interrupt() && local_softirq_pending())\n\t\tinvoke_softirq();\n\n\ttick_irq_exit();\n\trcu_irq_exit();\n\ttrace_hardirq_exit(); /* must be last! */\n}\n\n/*\n * This function must run with irqs disabled!\n */\ninline void raise_softirq_irqoff(unsigned int nr)\n{\n\t__raise_softirq_irqoff(nr);\n\n\t/*\n\t * If we're in an interrupt or softirq, we're done\n\t * (this also catches softirq-disabled code). We will\n\t * actually run the softirq once we return from\n\t * the irq or softirq.\n\t *\n\t * Otherwise we wake up ksoftirqd to make sure we\n\t * schedule the softirq soon.\n\t */\n\tif (!in_interrupt())\n\t\twakeup_softirqd();\n}\n\nvoid raise_softirq(unsigned int nr)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\traise_softirq_irqoff(nr);\n\tlocal_irq_restore(flags);\n}\n\nvoid __raise_softirq_irqoff(unsigned int nr)\n{\n\ttrace_softirq_raise(nr);\n\tor_softirq_pending(1UL << nr);\n}\n\nvoid open_softirq(int nr, void (*action)(struct softirq_action *))\n{\n\tsoftirq_vec[nr].action = action;\n}\n\n/*\n * Tasklets\n */\nstruct tasklet_head {\n\tstruct tasklet_struct *head;\n\tstruct tasklet_struct **tail;\n};\n\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);\n\nstatic void __tasklet_schedule_common(struct tasklet_struct *t,\n\t\t\t\t      struct tasklet_head __percpu *headp,\n\t\t\t\t      unsigned int softirq_nr)\n{\n\tstruct tasklet_head *head;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\thead = this_cpu_ptr(headp);\n\tt->next = NULL;\n\t*head->tail = t;\n\thead->tail = &(t->next);\n\traise_softirq_irqoff(softirq_nr);\n\tlocal_irq_restore(flags);\n}\n\nvoid __tasklet_schedule(struct tasklet_struct *t)\n{\n\t__tasklet_schedule_common(t, &tasklet_vec,\n\t\t\t\t  TASKLET_SOFTIRQ);\n}\nEXPORT_SYMBOL(__tasklet_schedule);\n\nvoid __tasklet_hi_schedule(struct tasklet_struct *t)\n{\n\t__tasklet_schedule_common(t, &tasklet_hi_vec,\n\t\t\t\t  HI_SOFTIRQ);\n}\nEXPORT_SYMBOL(__tasklet_hi_schedule);\n\nstatic void tasklet_action_common(struct softirq_action *a,\n\t\t\t\t  struct tasklet_head *tl_head,\n\t\t\t\t  unsigned int softirq_nr)\n{\n\tstruct tasklet_struct *list;\n\n\tlocal_irq_disable();\n\tlist = tl_head->head;\n\ttl_head->head = NULL;\n\ttl_head->tail = &tl_head->head;\n\tlocal_irq_enable();\n\n\twhile (list) {\n\t\tstruct tasklet_struct *t = list;\n\n\t\tlist = list->next;\n\n\t\tif (tasklet_trylock(t)) {\n\t\t\tif (!atomic_read(&t->count)) {\n\t\t\t\tif (!test_and_clear_bit(TASKLET_STATE_SCHED,\n\t\t\t\t\t\t\t&t->state))\n\t\t\t\t\tBUG();\n\t\t\t\tt->func(t->data);\n\t\t\t\ttasklet_unlock(t);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttasklet_unlock(t);\n\t\t}\n\n\t\tlocal_irq_disable();\n\t\tt->next = NULL;\n\t\t*tl_head->tail = t;\n\t\ttl_head->tail = &t->next;\n\t\t__raise_softirq_irqoff(softirq_nr);\n\t\tlocal_irq_enable();\n\t}\n}\n\nstatic __latent_entropy void tasklet_action(struct softirq_action *a)\n{\n\ttasklet_action_common(a, this_cpu_ptr(&tasklet_vec), TASKLET_SOFTIRQ);\n}\n\nstatic __latent_entropy void tasklet_hi_action(struct softirq_action *a)\n{\n\ttasklet_action_common(a, this_cpu_ptr(&tasklet_hi_vec), HI_SOFTIRQ);\n}\n\nvoid tasklet_init(struct tasklet_struct *t,\n\t\t  void (*func)(unsigned long), unsigned long data)\n{\n\tt->next = NULL;\n\tt->state = 0;\n\tatomic_set(&t->count, 0);\n\tt->func = func;\n\tt->data = data;\n}\nEXPORT_SYMBOL(tasklet_init);\n\nvoid tasklet_kill(struct tasklet_struct *t)\n{\n\tif (in_interrupt())\n\t\tpr_notice(\"Attempt to kill tasklet from interrupt\\n\");\n\n\twhile (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {\n\t\tdo {\n\t\t\tyield();\n\t\t} while (test_bit(TASKLET_STATE_SCHED, &t->state));\n\t}\n\ttasklet_unlock_wait(t);\n\tclear_bit(TASKLET_STATE_SCHED, &t->state);\n}\nEXPORT_SYMBOL(tasklet_kill);\n\n/*\n * tasklet_hrtimer\n */\n\n/*\n * The trampoline is called when the hrtimer expires. It schedules a tasklet\n * to run __tasklet_hrtimer_trampoline() which in turn will call the intended\n * hrtimer callback, but from softirq context.\n */\nstatic enum hrtimer_restart __hrtimer_tasklet_trampoline(struct hrtimer *timer)\n{\n\tstruct tasklet_hrtimer *ttimer =\n\t\tcontainer_of(timer, struct tasklet_hrtimer, timer);\n\n\ttasklet_hi_schedule(&ttimer->tasklet);\n\treturn HRTIMER_NORESTART;\n}\n\n/*\n * Helper function which calls the hrtimer callback from\n * tasklet/softirq context\n */\nstatic void __tasklet_hrtimer_trampoline(unsigned long data)\n{\n\tstruct tasklet_hrtimer *ttimer = (void *)data;\n\tenum hrtimer_restart restart;\n\n\trestart = ttimer->function(&ttimer->timer);\n\tif (restart != HRTIMER_NORESTART)\n\t\thrtimer_restart(&ttimer->timer);\n}\n\n/**\n * tasklet_hrtimer_init - Init a tasklet/hrtimer combo for softirq callbacks\n * @ttimer:\t tasklet_hrtimer which is initialized\n * @function:\t hrtimer callback function which gets called from softirq context\n * @which_clock: clock id (CLOCK_MONOTONIC/CLOCK_REALTIME)\n * @mode:\t hrtimer mode (HRTIMER_MODE_ABS/HRTIMER_MODE_REL)\n */\nvoid tasklet_hrtimer_init(struct tasklet_hrtimer *ttimer,\n\t\t\t  enum hrtimer_restart (*function)(struct hrtimer *),\n\t\t\t  clockid_t which_clock, enum hrtimer_mode mode)\n{\n\thrtimer_init(&ttimer->timer, which_clock, mode);\n\tttimer->timer.function = __hrtimer_tasklet_trampoline;\n\ttasklet_init(&ttimer->tasklet, __tasklet_hrtimer_trampoline,\n\t\t     (unsigned long)ttimer);\n\tttimer->function = function;\n}\nEXPORT_SYMBOL_GPL(tasklet_hrtimer_init);\n\nvoid __init softirq_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tper_cpu(tasklet_vec, cpu).tail =\n\t\t\t&per_cpu(tasklet_vec, cpu).head;\n\t\tper_cpu(tasklet_hi_vec, cpu).tail =\n\t\t\t&per_cpu(tasklet_hi_vec, cpu).head;\n\t}\n\n\topen_softirq(TASKLET_SOFTIRQ, tasklet_action);\n\topen_softirq(HI_SOFTIRQ, tasklet_hi_action);\n}\n\nstatic int ksoftirqd_should_run(unsigned int cpu)\n{\n\treturn local_softirq_pending();\n}\n\nstatic void run_ksoftirqd(unsigned int cpu)\n{\n\tlocal_irq_disable();\n\tif (local_softirq_pending()) {\n\t\t/*\n\t\t * We can safely run softirq on inline stack, as we are not deep\n\t\t * in the task stack here.\n\t\t */\n\t\t__do_softirq();\n\t\tlocal_irq_enable();\n\t\tcond_resched();\n\t\treturn;\n\t}\n\tlocal_irq_enable();\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n/*\n * tasklet_kill_immediate is called to remove a tasklet which can already be\n * scheduled for execution on @cpu.\n *\n * Unlike tasklet_kill, this function removes the tasklet\n * _immediately_, even if the tasklet is in TASKLET_STATE_SCHED state.\n *\n * When this function is called, @cpu must be in the CPU_DEAD state.\n */\nvoid tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu)\n{\n\tstruct tasklet_struct **i;\n\n\tBUG_ON(cpu_online(cpu));\n\tBUG_ON(test_bit(TASKLET_STATE_RUN, &t->state));\n\n\tif (!test_bit(TASKLET_STATE_SCHED, &t->state))\n\t\treturn;\n\n\t/* CPU is dead, so no lock needed. */\n\tfor (i = &per_cpu(tasklet_vec, cpu).head; *i; i = &(*i)->next) {\n\t\tif (*i == t) {\n\t\t\t*i = t->next;\n\t\t\t/* If this was the tail element, move the tail ptr */\n\t\t\tif (*i == NULL)\n\t\t\t\tper_cpu(tasklet_vec, cpu).tail = i;\n\t\t\treturn;\n\t\t}\n\t}\n\tBUG();\n}\n\nstatic int takeover_tasklets(unsigned int cpu)\n{\n\t/* CPU is dead, so no lock needed. */\n\tlocal_irq_disable();\n\n\t/* Find end, append list for that CPU. */\n\tif (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {\n\t\t*__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;\n\t\tthis_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);\n\t\tper_cpu(tasklet_vec, cpu).head = NULL;\n\t\tper_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;\n\t}\n\traise_softirq_irqoff(TASKLET_SOFTIRQ);\n\n\tif (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {\n\t\t*__this_cpu_read(tasklet_hi_vec.tail) = per_cpu(tasklet_hi_vec, cpu).head;\n\t\t__this_cpu_write(tasklet_hi_vec.tail, per_cpu(tasklet_hi_vec, cpu).tail);\n\t\tper_cpu(tasklet_hi_vec, cpu).head = NULL;\n\t\tper_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;\n\t}\n\traise_softirq_irqoff(HI_SOFTIRQ);\n\n\tlocal_irq_enable();\n\treturn 0;\n}\n#else\n#define takeover_tasklets\tNULL\n#endif /* CONFIG_HOTPLUG_CPU */\n\nstatic struct smp_hotplug_thread softirq_threads = {\n\t.store\t\t\t= &ksoftirqd,\n\t.thread_should_run\t= ksoftirqd_should_run,\n\t.thread_fn\t\t= run_ksoftirqd,\n\t.thread_comm\t\t= \"ksoftirqd/%u\",\n};\n\nstatic __init int spawn_ksoftirqd(void)\n{\n\tcpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, \"softirq:dead\", NULL,\n\t\t\t\t  takeover_tasklets);\n\tBUG_ON(smpboot_register_percpu_thread(&softirq_threads));\n\n\treturn 0;\n}\nearly_initcall(spawn_ksoftirqd);\n\n/*\n * [ These __weak aliases are kept in a separate compilation unit, so that\n *   GCC does not inline them incorrectly. ]\n */\n\nint __init __weak early_irq_init(void)\n{\n\treturn 0;\n}\n\nint __init __weak arch_probe_nr_irqs(void)\n{\n\treturn NR_IRQS_LEGACY;\n}\n\nint __init __weak arch_early_irq_init(void)\n{\n\treturn 0;\n}\n\nunsigned int __weak arch_dynirq_lower_bound(unsigned int from)\n{\n\treturn from;\n}\n", "/*\n * ring buffer based function tracer\n *\n * Copyright (C) 2007-2012 Steven Rostedt <srostedt@redhat.com>\n * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>\n *\n * Originally taken from the RT patch by:\n *    Arnaldo Carvalho de Melo <acme@redhat.com>\n *\n * Based on code from the latency_tracer, that is:\n *  Copyright (C) 2004-2006 Ingo Molnar\n *  Copyright (C) 2004 Nadia Yvette Chambers\n */\n#include <linux/ring_buffer.h>\n#include <generated/utsrelease.h>\n#include <linux/stacktrace.h>\n#include <linux/writeback.h>\n#include <linux/kallsyms.h>\n#include <linux/seq_file.h>\n#include <linux/notifier.h>\n#include <linux/irqflags.h>\n#include <linux/debugfs.h>\n#include <linux/tracefs.h>\n#include <linux/pagemap.h>\n#include <linux/hardirq.h>\n#include <linux/linkage.h>\n#include <linux/uaccess.h>\n#include <linux/vmalloc.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/percpu.h>\n#include <linux/splice.h>\n#include <linux/kdebug.h>\n#include <linux/string.h>\n#include <linux/mount.h>\n#include <linux/rwsem.h>\n#include <linux/slab.h>\n#include <linux/ctype.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/nmi.h>\n#include <linux/fs.h>\n#include <linux/trace.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/rt.h>\n\n#include \"trace.h\"\n#include \"trace_output.h\"\n\n/*\n * On boot up, the ring buffer is set to the minimum size, so that\n * we do not waste memory on systems that are not using tracing.\n */\nbool ring_buffer_expanded;\n\n/*\n * We need to change this state when a selftest is running.\n * A selftest will lurk into the ring-buffer to count the\n * entries inserted during the selftest although some concurrent\n * insertions into the ring-buffer such as trace_printk could occurred\n * at the same time, giving false positive or negative results.\n */\nstatic bool __read_mostly tracing_selftest_running;\n\n/*\n * If a tracer is running, we do not want to run SELFTEST.\n */\nbool __read_mostly tracing_selftest_disabled;\n\n/* Pipe tracepoints to printk */\nstruct trace_iterator *tracepoint_print_iter;\nint tracepoint_printk;\nstatic DEFINE_STATIC_KEY_FALSE(tracepoint_printk_key);\n\n/* For tracers that don't implement custom flags */\nstatic struct tracer_opt dummy_tracer_opt[] = {\n\t{ }\n};\n\nstatic int\ndummy_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\n{\n\treturn 0;\n}\n\n/*\n * To prevent the comm cache from being overwritten when no\n * tracing is active, only save the comm when a trace event\n * occurred.\n */\nstatic DEFINE_PER_CPU(bool, trace_taskinfo_save);\n\n/*\n * Kill all tracing for good (never come back).\n * It is initialized to 1 but will turn to zero if the initialization\n * of the tracer is successful. But that is the only place that sets\n * this back to zero.\n */\nstatic int tracing_disabled = 1;\n\ncpumask_var_t __read_mostly\ttracing_buffer_mask;\n\n/*\n * ftrace_dump_on_oops - variable to dump ftrace buffer on oops\n *\n * If there is an oops (or kernel panic) and the ftrace_dump_on_oops\n * is set, then ftrace_dump is called. This will output the contents\n * of the ftrace buffers to the console.  This is very useful for\n * capturing traces that lead to crashes and outputing it to a\n * serial console.\n *\n * It is default off, but you can enable it with either specifying\n * \"ftrace_dump_on_oops\" in the kernel command line, or setting\n * /proc/sys/kernel/ftrace_dump_on_oops\n * Set 1 if you want to dump buffers of all CPUs\n * Set 2 if you want to dump the buffer of the CPU that triggered oops\n */\n\nenum ftrace_dump_mode ftrace_dump_on_oops;\n\n/* When set, tracing will stop when a WARN*() is hit */\nint __disable_trace_on_warning;\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\n/* Map of enums to their values, for \"eval_map\" file */\nstruct trace_eval_map_head {\n\tstruct module\t\t\t*mod;\n\tunsigned long\t\t\tlength;\n};\n\nunion trace_eval_map_item;\n\nstruct trace_eval_map_tail {\n\t/*\n\t * \"end\" is first and points to NULL as it must be different\n\t * than \"mod\" or \"eval_string\"\n\t */\n\tunion trace_eval_map_item\t*next;\n\tconst char\t\t\t*end;\t/* points to NULL */\n};\n\nstatic DEFINE_MUTEX(trace_eval_mutex);\n\n/*\n * The trace_eval_maps are saved in an array with two extra elements,\n * one at the beginning, and one at the end. The beginning item contains\n * the count of the saved maps (head.length), and the module they\n * belong to if not built in (head.mod). The ending item contains a\n * pointer to the next array of saved eval_map items.\n */\nunion trace_eval_map_item {\n\tstruct trace_eval_map\t\tmap;\n\tstruct trace_eval_map_head\thead;\n\tstruct trace_eval_map_tail\ttail;\n};\n\nstatic union trace_eval_map_item *trace_eval_maps;\n#endif /* CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic int tracing_set_tracer(struct trace_array *tr, const char *buf);\n\n#define MAX_TRACER_SIZE\t\t100\nstatic char bootup_tracer_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *default_bootup_tracer;\n\nstatic bool allocate_snapshot;\n\nstatic int __init set_cmdline_ftrace(char *str)\n{\n\tstrlcpy(bootup_tracer_buf, str, MAX_TRACER_SIZE);\n\tdefault_bootup_tracer = bootup_tracer_buf;\n\t/* We are using ftrace early, expand it */\n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"ftrace=\", set_cmdline_ftrace);\n\nstatic int __init set_ftrace_dump_on_oops(char *str)\n{\n\tif (*str++ != '=' || !*str) {\n\t\tftrace_dump_on_oops = DUMP_ALL;\n\t\treturn 1;\n\t}\n\n\tif (!strcmp(\"orig_cpu\", str)) {\n\t\tftrace_dump_on_oops = DUMP_ORIG;\n                return 1;\n        }\n\n        return 0;\n}\n__setup(\"ftrace_dump_on_oops\", set_ftrace_dump_on_oops);\n\nstatic int __init stop_trace_on_warning(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\t__disable_trace_on_warning = 1;\n\treturn 1;\n}\n__setup(\"traceoff_on_warning\", stop_trace_on_warning);\n\nstatic int __init boot_alloc_snapshot(char *str)\n{\n\tallocate_snapshot = true;\n\t/* We also need the main ring buffer expanded */\n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"alloc_snapshot\", boot_alloc_snapshot);\n\n\nstatic char trace_boot_options_buf[MAX_TRACER_SIZE] __initdata;\n\nstatic int __init set_trace_boot_options(char *str)\n{\n\tstrlcpy(trace_boot_options_buf, str, MAX_TRACER_SIZE);\n\treturn 0;\n}\n__setup(\"trace_options=\", set_trace_boot_options);\n\nstatic char trace_boot_clock_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *trace_boot_clock __initdata;\n\nstatic int __init set_trace_boot_clock(char *str)\n{\n\tstrlcpy(trace_boot_clock_buf, str, MAX_TRACER_SIZE);\n\ttrace_boot_clock = trace_boot_clock_buf;\n\treturn 0;\n}\n__setup(\"trace_clock=\", set_trace_boot_clock);\n\nstatic int __init set_tracepoint_printk(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\ttracepoint_printk = 1;\n\treturn 1;\n}\n__setup(\"tp_printk\", set_tracepoint_printk);\n\nunsigned long long ns2usecs(u64 nsec)\n{\n\tnsec += 500;\n\tdo_div(nsec, 1000);\n\treturn nsec;\n}\n\n/* trace_flags holds trace_options default values */\n#define TRACE_DEFAULT_FLAGS\t\t\t\t\t\t\\\n\t(FUNCTION_DEFAULT_FLAGS |\t\t\t\t\t\\\n\t TRACE_ITER_PRINT_PARENT | TRACE_ITER_PRINTK |\t\t\t\\\n\t TRACE_ITER_ANNOTATE | TRACE_ITER_CONTEXT_INFO |\t\t\\\n\t TRACE_ITER_RECORD_CMD | TRACE_ITER_OVERWRITE |\t\t\t\\\n\t TRACE_ITER_IRQ_INFO | TRACE_ITER_MARKERS)\n\n/* trace_options that are only supported by global_trace */\n#define TOP_LEVEL_TRACE_FLAGS (TRACE_ITER_PRINTK |\t\t\t\\\n\t       TRACE_ITER_PRINTK_MSGONLY | TRACE_ITER_RECORD_CMD)\n\n/* trace_flags that are default zero for instances */\n#define ZEROED_TRACE_FLAGS \\\n\t(TRACE_ITER_EVENT_FORK | TRACE_ITER_FUNC_FORK)\n\n/*\n * The global_trace is the descriptor that holds the top-level tracing\n * buffers for the live tracing.\n */\nstatic struct trace_array global_trace = {\n\t.trace_flags = TRACE_DEFAULT_FLAGS,\n};\n\nLIST_HEAD(ftrace_trace_arrays);\n\nint trace_array_get(struct trace_array *this_tr)\n{\n\tstruct trace_array *tr;\n\tint ret = -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr == this_tr) {\n\t\t\ttr->ref++;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic void __trace_array_put(struct trace_array *this_tr)\n{\n\tWARN_ON(!this_tr->ref);\n\tthis_tr->ref--;\n}\n\nvoid trace_array_put(struct trace_array *this_tr)\n{\n\tmutex_lock(&trace_types_lock);\n\t__trace_array_put(this_tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\nint call_filter_check_discard(struct trace_event_call *call, void *rec,\n\t\t\t      struct ring_buffer *buffer,\n\t\t\t      struct ring_buffer_event *event)\n{\n\tif (unlikely(call->flags & TRACE_EVENT_FL_FILTERED) &&\n\t    !filter_match_preds(call->filter, rec)) {\n\t\t__trace_event_discard_commit(buffer, event);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nvoid trace_free_pid_list(struct trace_pid_list *pid_list)\n{\n\tvfree(pid_list->pids);\n\tkfree(pid_list);\n}\n\n/**\n * trace_find_filtered_pid - check if a pid exists in a filtered_pid list\n * @filtered_pids: The list of pids to check\n * @search_pid: The PID to find in @filtered_pids\n *\n * Returns true if @search_pid is fonud in @filtered_pids, and false otherwis.\n */\nbool\ntrace_find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)\n{\n\t/*\n\t * If pid_max changed after filtered_pids was created, we\n\t * by default ignore all pids greater than the previous pid_max.\n\t */\n\tif (search_pid >= filtered_pids->pid_max)\n\t\treturn false;\n\n\treturn test_bit(search_pid, filtered_pids->pids);\n}\n\n/**\n * trace_ignore_this_task - should a task be ignored for tracing\n * @filtered_pids: The list of pids to check\n * @task: The task that should be ignored if not filtered\n *\n * Checks if @task should be traced or not from @filtered_pids.\n * Returns true if @task should *NOT* be traced.\n * Returns false if @task should be traced.\n */\nbool\ntrace_ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)\n{\n\t/*\n\t * Return false, because if filtered_pids does not exist,\n\t * all pids are good to trace.\n\t */\n\tif (!filtered_pids)\n\t\treturn false;\n\n\treturn !trace_find_filtered_pid(filtered_pids, task->pid);\n}\n\n/**\n * trace_pid_filter_add_remove_task - Add or remove a task from a pid_list\n * @pid_list: The list to modify\n * @self: The current task for fork or NULL for exit\n * @task: The task to add or remove\n *\n * If adding a task, if @self is defined, the task is only added if @self\n * is also included in @pid_list. This happens on fork and tasks should\n * only be added when the parent is listed. If @self is NULL, then the\n * @task pid will be removed from the list, which would happen on exit\n * of a task.\n */\nvoid trace_filter_add_remove_task(struct trace_pid_list *pid_list,\n\t\t\t\t  struct task_struct *self,\n\t\t\t\t  struct task_struct *task)\n{\n\tif (!pid_list)\n\t\treturn;\n\n\t/* For forks, we only add if the forking task is listed */\n\tif (self) {\n\t\tif (!trace_find_filtered_pid(pid_list, self->pid))\n\t\t\treturn;\n\t}\n\n\t/* Sorry, but we don't support pid_max changing after setting */\n\tif (task->pid >= pid_list->pid_max)\n\t\treturn;\n\n\t/* \"self\" is set for forks, and NULL for exits */\n\tif (self)\n\t\tset_bit(task->pid, pid_list->pids);\n\telse\n\t\tclear_bit(task->pid, pid_list->pids);\n}\n\n/**\n * trace_pid_next - Used for seq_file to get to the next pid of a pid_list\n * @pid_list: The pid list to show\n * @v: The last pid that was shown (+1 the actual pid to let zero be displayed)\n * @pos: The position of the file\n *\n * This is used by the seq_file \"next\" operation to iterate the pids\n * listed in a trace_pid_list structure.\n *\n * Returns the pid+1 as we want to display pid of zero, but NULL would\n * stop the iteration.\n */\nvoid *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos)\n{\n\tunsigned long pid = (unsigned long)v;\n\n\t(*pos)++;\n\n\t/* pid already is +1 of the actual prevous bit */\n\tpid = find_next_bit(pid_list->pids, pid_list->pid_max, pid);\n\n\t/* Return pid + 1 to allow zero to be represented */\n\tif (pid < pid_list->pid_max)\n\t\treturn (void *)(pid + 1);\n\n\treturn NULL;\n}\n\n/**\n * trace_pid_start - Used for seq_file to start reading pid lists\n * @pid_list: The pid list to show\n * @pos: The position of the file\n *\n * This is used by seq_file \"start\" operation to start the iteration\n * of listing pids.\n *\n * Returns the pid+1 as we want to display pid of zero, but NULL would\n * stop the iteration.\n */\nvoid *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos)\n{\n\tunsigned long pid;\n\tloff_t l = 0;\n\n\tpid = find_first_bit(pid_list->pids, pid_list->pid_max);\n\tif (pid >= pid_list->pid_max)\n\t\treturn NULL;\n\n\t/* Return pid + 1 so that zero can be the exit value */\n\tfor (pid++; pid && l < *pos;\n\t     pid = (unsigned long)trace_pid_next(pid_list, (void *)pid, &l))\n\t\t;\n\treturn (void *)pid;\n}\n\n/**\n * trace_pid_show - show the current pid in seq_file processing\n * @m: The seq_file structure to write into\n * @v: A void pointer of the pid (+1) value to display\n *\n * Can be directly used by seq_file operations to display the current\n * pid value.\n */\nint trace_pid_show(struct seq_file *m, void *v)\n{\n\tunsigned long pid = (unsigned long)v - 1;\n\n\tseq_printf(m, \"%lu\\n\", pid);\n\treturn 0;\n}\n\n/* 128 should be much more than enough */\n#define PID_BUF_SIZE\t\t127\n\nint trace_pid_write(struct trace_pid_list *filtered_pids,\n\t\t    struct trace_pid_list **new_pid_list,\n\t\t    const char __user *ubuf, size_t cnt)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_parser parser;\n\tunsigned long val;\n\tint nr_pids = 0;\n\tssize_t read = 0;\n\tssize_t ret = 0;\n\tloff_t pos;\n\tpid_t pid;\n\n\tif (trace_parser_get_init(&parser, PID_BUF_SIZE + 1))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Always recreate a new array. The write is an all or nothing\n\t * operation. Always create a new array when adding new pids by\n\t * the user. If the operation fails, then the current list is\n\t * not modified.\n\t */\n\tpid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);\n\tif (!pid_list)\n\t\treturn -ENOMEM;\n\n\tpid_list->pid_max = READ_ONCE(pid_max);\n\n\t/* Only truncating will shrink pid_max */\n\tif (filtered_pids && filtered_pids->pid_max > pid_list->pid_max)\n\t\tpid_list->pid_max = filtered_pids->pid_max;\n\n\tpid_list->pids = vzalloc((pid_list->pid_max + 7) >> 3);\n\tif (!pid_list->pids) {\n\t\tkfree(pid_list);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (filtered_pids) {\n\t\t/* copy the current bits to the new max */\n\t\tfor_each_set_bit(pid, filtered_pids->pids,\n\t\t\t\t filtered_pids->pid_max) {\n\t\t\tset_bit(pid, pid_list->pids);\n\t\t\tnr_pids++;\n\t\t}\n\t}\n\n\twhile (cnt > 0) {\n\n\t\tpos = 0;\n\n\t\tret = trace_get_user(&parser, ubuf, cnt, &pos);\n\t\tif (ret < 0 || !trace_parser_loaded(&parser))\n\t\t\tbreak;\n\n\t\tread += ret;\n\t\tubuf += ret;\n\t\tcnt -= ret;\n\n\t\tret = -EINVAL;\n\t\tif (kstrtoul(parser.buffer, 0, &val))\n\t\t\tbreak;\n\t\tif (val >= pid_list->pid_max)\n\t\t\tbreak;\n\n\t\tpid = (pid_t)val;\n\n\t\tset_bit(pid, pid_list->pids);\n\t\tnr_pids++;\n\n\t\ttrace_parser_clear(&parser);\n\t\tret = 0;\n\t}\n\ttrace_parser_put(&parser);\n\n\tif (ret < 0) {\n\t\ttrace_free_pid_list(pid_list);\n\t\treturn ret;\n\t}\n\n\tif (!nr_pids) {\n\t\t/* Cleared the list of pids */\n\t\ttrace_free_pid_list(pid_list);\n\t\tread = ret;\n\t\tpid_list = NULL;\n\t}\n\n\t*new_pid_list = pid_list;\n\n\treturn read;\n}\n\nstatic u64 buffer_ftrace_now(struct trace_buffer *buf, int cpu)\n{\n\tu64 ts;\n\n\t/* Early boot up does not have a buffer yet */\n\tif (!buf->buffer)\n\t\treturn trace_clock_local();\n\n\tts = ring_buffer_time_stamp(buf->buffer, cpu);\n\tring_buffer_normalize_time_stamp(buf->buffer, cpu, &ts);\n\n\treturn ts;\n}\n\nu64 ftrace_now(int cpu)\n{\n\treturn buffer_ftrace_now(&global_trace.trace_buffer, cpu);\n}\n\n/**\n * tracing_is_enabled - Show if global_trace has been disabled\n *\n * Shows if the global trace has been enabled or not. It uses the\n * mirror flag \"buffer_disabled\" to be used in fast paths such as for\n * the irqsoff tracer. But it may be inaccurate due to races. If you\n * need to know the accurate state, use tracing_is_on() which is a little\n * slower, but accurate.\n */\nint tracing_is_enabled(void)\n{\n\t/*\n\t * For quick access (irqsoff uses this in fast path), just\n\t * return the mirror variable of the state of the ring buffer.\n\t * It's a little racy, but we don't really care.\n\t */\n\tsmp_rmb();\n\treturn !global_trace.buffer_disabled;\n}\n\n/*\n * trace_buf_size is the size in bytes that is allocated\n * for a buffer. Note, the number of bytes is always rounded\n * to page size.\n *\n * This number is purposely set to a low number of 16384.\n * If the dump on oops happens, it will be much appreciated\n * to not have to wait for all that output. Anyway this can be\n * boot time and run time configurable.\n */\n#define TRACE_BUF_SIZE_DEFAULT\t1441792UL /* 16384 * 88 (sizeof(entry)) */\n\nstatic unsigned long\t\ttrace_buf_size = TRACE_BUF_SIZE_DEFAULT;\n\n/* trace_types holds a link list of available tracers. */\nstatic struct tracer\t\t*trace_types __read_mostly;\n\n/*\n * trace_types_lock is used to protect the trace_types list.\n */\nDEFINE_MUTEX(trace_types_lock);\n\n/*\n * serialize the access of the ring buffer\n *\n * ring buffer serializes readers, but it is low level protection.\n * The validity of the events (which returns by ring_buffer_peek() ..etc)\n * are not protected by ring buffer.\n *\n * The content of events may become garbage if we allow other process consumes\n * these events concurrently:\n *   A) the page of the consumed events may become a normal page\n *      (not reader page) in ring buffer, and this page will be rewrited\n *      by events producer.\n *   B) The page of the consumed events may become a page for splice_read,\n *      and this page will be returned to system.\n *\n * These primitives allow multi process access to different cpu ring buffer\n * concurrently.\n *\n * These primitives don't distinguish read-only and read-consume access.\n * Multi read-only access are also serialized.\n */\n\n#ifdef CONFIG_SMP\nstatic DECLARE_RWSEM(all_cpu_access_lock);\nstatic DEFINE_PER_CPU(struct mutex, cpu_access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\t/* gain it for accessing the whole ring buffer. */\n\t\tdown_write(&all_cpu_access_lock);\n\t} else {\n\t\t/* gain it for accessing a cpu ring buffer. */\n\n\t\t/* Firstly block other trace_access_lock(RING_BUFFER_ALL_CPUS). */\n\t\tdown_read(&all_cpu_access_lock);\n\n\t\t/* Secondly block other access to this @cpu ring buffer. */\n\t\tmutex_lock(&per_cpu(cpu_access_lock, cpu));\n\t}\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tup_write(&all_cpu_access_lock);\n\t} else {\n\t\tmutex_unlock(&per_cpu(cpu_access_lock, cpu));\n\t\tup_read(&all_cpu_access_lock);\n\t}\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmutex_init(&per_cpu(cpu_access_lock, cpu));\n}\n\n#else\n\nstatic DEFINE_MUTEX(access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\t(void)cpu;\n\tmutex_lock(&access_lock);\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\t(void)cpu;\n\tmutex_unlock(&access_lock);\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n}\n\n#endif\n\n#ifdef CONFIG_STACKTRACE\nstatic void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t unsigned long flags,\n\t\t\t\t int skip, int pc, struct pt_regs *regs);\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs);\n\n#else\nstatic inline void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tint skip, int pc, struct pt_regs *regs)\n{\n}\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs)\n{\n}\n\n#endif\n\nstatic __always_inline void\ntrace_event_setup(struct ring_buffer_event *event,\n\t\t  int type, unsigned long flags, int pc)\n{\n\tstruct trace_entry *ent = ring_buffer_event_data(event);\n\n\ttracing_generic_entry_update(ent, flags, pc);\n\tent->type = type;\n}\n\nstatic __always_inline struct ring_buffer_event *\n__trace_buffer_lock_reserve(struct ring_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\tstruct ring_buffer_event *event;\n\n\tevent = ring_buffer_lock_reserve(buffer, len);\n\tif (event != NULL)\n\t\ttrace_event_setup(event, type, flags, pc);\n\n\treturn event;\n}\n\nvoid tracer_tracing_on(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\tring_buffer_record_on(tr->trace_buffer.buffer);\n\t/*\n\t * This flag is looked at when buffers haven't been allocated\n\t * yet, or by some tracers (like irqsoff), that just want to\n\t * know if the ring buffer has been disabled, but it can handle\n\t * races of where it gets disabled but we still do a record.\n\t * As the check is in the fast path of the tracers, it is more\n\t * important to be fast than accurate.\n\t */\n\ttr->buffer_disabled = 0;\n\t/* Make the flag seen by readers */\n\tsmp_wmb();\n}\n\n/**\n * tracing_on - enable tracing buffers\n *\n * This function enables tracing buffers that may have been\n * disabled with tracing_off.\n */\nvoid tracing_on(void)\n{\n\ttracer_tracing_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_on);\n\n\nstatic __always_inline void\n__buffer_unlock_commit(struct ring_buffer *buffer, struct ring_buffer_event *event)\n{\n\t__this_cpu_write(trace_taskinfo_save, true);\n\n\t/* If this is the temp buffer, we need to commit fully */\n\tif (this_cpu_read(trace_buffered_event) == event) {\n\t\t/* Length is in event->array[0] */\n\t\tring_buffer_write(buffer, event->array[0], &event->array[1]);\n\t\t/* Release the temp buffer */\n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t} else\n\t\tring_buffer_unlock_commit(buffer, event);\n}\n\n/**\n * __trace_puts - write a constant string into the trace buffer.\n * @ip:\t   The address of the caller\n * @str:   The constant string to write\n * @size:  The size of the string.\n */\nint __trace_puts(unsigned long ip, const char *str, int size)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tint alloc;\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\talloc = sizeof(*entry) + size + 2; /* possible \\n added */\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc, \n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, str, size);\n\n\t/* Add a newline if necessary */\n\tif (entry->buf[size - 1] != '\\n') {\n\t\tentry->buf[size] = '\\n';\n\t\tentry->buf[size + 1] = '\\0';\n\t} else\n\t\tentry->buf[size] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn size;\n}\nEXPORT_SYMBOL_GPL(__trace_puts);\n\n/**\n * __trace_bputs - write the pointer to a constant string into trace buffer\n * @ip:\t   The address of the caller\n * @str:   The constant string to write to the buffer to\n */\nint __trace_bputs(unsigned long ip, const char *str)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct bputs_entry *entry;\n\tunsigned long irq_flags;\n\tint size = sizeof(struct bputs_entry);\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPUTS, size,\n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->str\t\t\t= str;\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(__trace_bputs);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nvoid tracing_snapshot_instance(struct trace_array *tr)\n{\n\tstruct tracer *tracer = tr->current_trace;\n\tunsigned long flags;\n\n\tif (in_nmi()) {\n\t\tinternal_trace_puts(\"*** SNAPSHOT CALLED FROM NMI CONTEXT ***\\n\");\n\t\tinternal_trace_puts(\"*** snapshot is being ignored        ***\\n\");\n\t\treturn;\n\t}\n\n\tif (!tr->allocated_snapshot) {\n\t\tinternal_trace_puts(\"*** SNAPSHOT NOT ALLOCATED ***\\n\");\n\t\tinternal_trace_puts(\"*** stopping trace here!   ***\\n\");\n\t\ttracing_off();\n\t\treturn;\n\t}\n\n\t/* Note, snapshot can not be used when the tracer uses it */\n\tif (tracer->use_max_tr) {\n\t\tinternal_trace_puts(\"*** LATENCY TRACER ACTIVE ***\\n\");\n\t\tinternal_trace_puts(\"*** Can not use snapshot (sorry) ***\\n\");\n\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\tupdate_max_tr(tr, current, smp_processor_id());\n\tlocal_irq_restore(flags);\n}\n\n/**\n * tracing_snapshot - take a snapshot of the current buffer.\n *\n * This causes a swap between the snapshot buffer and the current live\n * tracing buffer. You can use this to take snapshots of the live\n * trace when some condition is triggered, but continue to trace.\n *\n * Note, make sure to allocate the snapshot with either\n * a tracing_snapshot_alloc(), or by doing it manually\n * with: echo 1 > /sys/kernel/debug/tracing/snapshot\n *\n * If the snapshot buffer is not allocated, it will stop tracing.\n * Basically making a permanent snapshot.\n */\nvoid tracing_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\ttracing_snapshot_instance(tr);\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\n\nstatic int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,\n\t\t\t\t\tstruct trace_buffer *size_buf, int cpu_id);\nstatic void set_buffer_entries(struct trace_buffer *buf, unsigned long val);\n\nint tracing_alloc_snapshot_instance(struct trace_array *tr)\n{\n\tint ret;\n\n\tif (!tr->allocated_snapshot) {\n\n\t\t/* allocate spare buffer */\n\t\tret = resize_buffer_duplicate_size(&tr->max_buffer,\n\t\t\t\t   &tr->trace_buffer, RING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\ttr->allocated_snapshot = true;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_snapshot(struct trace_array *tr)\n{\n\t/*\n\t * We don't free the ring buffer. instead, resize it because\n\t * The max_tr ring buffer has some state (e.g. ring->clock) and\n\t * we want preserve it.\n\t */\n\tring_buffer_resize(tr->max_buffer.buffer, 1, RING_BUFFER_ALL_CPUS);\n\tset_buffer_entries(&tr->max_buffer, 1);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n\ttr->allocated_snapshot = false;\n}\n\n/**\n * tracing_alloc_snapshot - allocate snapshot buffer.\n *\n * This only allocates the snapshot buffer if it isn't already\n * allocated - it doesn't also take a snapshot.\n *\n * This is meant to be used in cases where the snapshot buffer needs\n * to be set up for events that can't sleep but need to be able to\n * trigger a snapshot.\n */\nint tracing_alloc_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\tint ret;\n\n\tret = tracing_alloc_snapshot_instance(tr);\n\tWARN_ON(ret < 0);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\n\n/**\n * tracing_snapshot_alloc - allocate and take a snapshot of the current buffer.\n *\n * This is similar to tracing_snapshot(), but it will allocate the\n * snapshot buffer if it isn't already allocated. Use this only\n * where it is safe to sleep, as the allocation may sleep.\n *\n * This causes a swap between the snapshot buffer and the current live\n * tracing buffer. You can use this to take snapshots of the live\n * trace when some condition is triggered, but continue to trace.\n */\nvoid tracing_snapshot_alloc(void)\n{\n\tint ret;\n\n\tret = tracing_alloc_snapshot();\n\tif (ret < 0)\n\t\treturn;\n\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n#else\nvoid tracing_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but internal snapshot used\");\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\nint tracing_alloc_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but snapshot allocation used\");\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\nvoid tracing_snapshot_alloc(void)\n{\n\t/* Give warning */\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\nvoid tracer_tracing_off(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\tring_buffer_record_off(tr->trace_buffer.buffer);\n\t/*\n\t * This flag is looked at when buffers haven't been allocated\n\t * yet, or by some tracers (like irqsoff), that just want to\n\t * know if the ring buffer has been disabled, but it can handle\n\t * races of where it gets disabled but we still do a record.\n\t * As the check is in the fast path of the tracers, it is more\n\t * important to be fast than accurate.\n\t */\n\ttr->buffer_disabled = 1;\n\t/* Make the flag seen by readers */\n\tsmp_wmb();\n}\n\n/**\n * tracing_off - turn off tracing buffers\n *\n * This function stops the tracing buffers from recording data.\n * It does not disable any overhead the tracers themselves may\n * be causing. This function simply causes all recording to\n * the ring buffers to fail.\n */\nvoid tracing_off(void)\n{\n\ttracer_tracing_off(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_off);\n\nvoid disable_trace_on_warning(void)\n{\n\tif (__disable_trace_on_warning)\n\t\ttracing_off();\n}\n\n/**\n * tracer_tracing_is_on - show real state of ring buffer enabled\n * @tr : the trace array to know if ring buffer is enabled\n *\n * Shows real state of the ring buffer if it is enabled or not.\n */\nint tracer_tracing_is_on(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\treturn ring_buffer_record_is_on(tr->trace_buffer.buffer);\n\treturn !tr->buffer_disabled;\n}\n\n/**\n * tracing_is_on - show state of ring buffers enabled\n */\nint tracing_is_on(void)\n{\n\treturn tracer_tracing_is_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_is_on);\n\nstatic int __init set_buf_size(char *str)\n{\n\tunsigned long buf_size;\n\n\tif (!str)\n\t\treturn 0;\n\tbuf_size = memparse(str, &str);\n\t/* nr_entries can not be zero */\n\tif (buf_size == 0)\n\t\treturn 0;\n\ttrace_buf_size = buf_size;\n\treturn 1;\n}\n__setup(\"trace_buf_size=\", set_buf_size);\n\nstatic int __init set_tracing_thresh(char *str)\n{\n\tunsigned long threshold;\n\tint ret;\n\n\tif (!str)\n\t\treturn 0;\n\tret = kstrtoul(str, 0, &threshold);\n\tif (ret < 0)\n\t\treturn 0;\n\ttracing_thresh = threshold * 1000;\n\treturn 1;\n}\n__setup(\"tracing_thresh=\", set_tracing_thresh);\n\nunsigned long nsecs_to_usecs(unsigned long nsecs)\n{\n\treturn nsecs / 1000;\n}\n\n/*\n * TRACE_FLAGS is defined as a tuple matching bit masks with strings.\n * It uses C(a, b) where 'a' is the eval (enum) name and 'b' is the string that\n * matches it. By defining \"C(a, b) b\", TRACE_FLAGS becomes a list\n * of strings in the order that the evals (enum) were defined.\n */\n#undef C\n#define C(a, b) b\n\n/* These must match the bit postions in trace_iterator_flags */\nstatic const char *trace_options[] = {\n\tTRACE_FLAGS\n\tNULL\n};\n\nstatic struct {\n\tu64 (*func)(void);\n\tconst char *name;\n\tint in_ns;\t\t/* is this clock in nanoseconds? */\n} trace_clocks[] = {\n\t{ trace_clock_local,\t\t\"local\",\t1 },\n\t{ trace_clock_global,\t\t\"global\",\t1 },\n\t{ trace_clock_counter,\t\t\"counter\",\t0 },\n\t{ trace_clock_jiffies,\t\t\"uptime\",\t0 },\n\t{ trace_clock,\t\t\t\"perf\",\t\t1 },\n\t{ ktime_get_mono_fast_ns,\t\"mono\",\t\t1 },\n\t{ ktime_get_raw_fast_ns,\t\"mono_raw\",\t1 },\n\t{ ktime_get_boot_fast_ns,\t\"boot\",\t\t1 },\n\tARCH_TRACE_CLOCKS\n};\n\nbool trace_clock_in_ns(struct trace_array *tr)\n{\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * trace_parser_get_init - gets the buffer for trace parser\n */\nint trace_parser_get_init(struct trace_parser *parser, int size)\n{\n\tmemset(parser, 0, sizeof(*parser));\n\n\tparser->buffer = kmalloc(size, GFP_KERNEL);\n\tif (!parser->buffer)\n\t\treturn 1;\n\n\tparser->size = size;\n\treturn 0;\n}\n\n/*\n * trace_parser_put - frees the buffer for trace parser\n */\nvoid trace_parser_put(struct trace_parser *parser)\n{\n\tkfree(parser->buffer);\n\tparser->buffer = NULL;\n}\n\n/*\n * trace_get_user - reads the user input string separated by  space\n * (matched by isspace(ch))\n *\n * For each string found the 'struct trace_parser' is updated,\n * and the function returns.\n *\n * Returns number of bytes read.\n *\n * See kernel/trace/trace.h for 'struct trace_parser' details.\n */\nint trace_get_user(struct trace_parser *parser, const char __user *ubuf,\n\tsize_t cnt, loff_t *ppos)\n{\n\tchar ch;\n\tsize_t read = 0;\n\tssize_t ret;\n\n\tif (!*ppos)\n\t\ttrace_parser_clear(parser);\n\n\tret = get_user(ch, ubuf++);\n\tif (ret)\n\t\tgoto out;\n\n\tread++;\n\tcnt--;\n\n\t/*\n\t * The parser is not finished with the last write,\n\t * continue reading the user input without skipping spaces.\n\t */\n\tif (!parser->cont) {\n\t\t/* skip white space */\n\t\twhile (cnt && isspace(ch)) {\n\t\t\tret = get_user(ch, ubuf++);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tread++;\n\t\t\tcnt--;\n\t\t}\n\n\t\tparser->idx = 0;\n\n\t\t/* only spaces were written */\n\t\tif (isspace(ch) || !ch) {\n\t\t\t*ppos += read;\n\t\t\tret = read;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* read the non-space input */\n\twhile (cnt && !isspace(ch) && ch) {\n\t\tif (parser->idx < parser->size - 1)\n\t\t\tparser->buffer[parser->idx++] = ch;\n\t\telse {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = get_user(ch, ubuf++);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tread++;\n\t\tcnt--;\n\t}\n\n\t/* We either got finished input or we have to wait for another call. */\n\tif (isspace(ch) || !ch) {\n\t\tparser->buffer[parser->idx] = 0;\n\t\tparser->cont = false;\n\t} else if (parser->idx < parser->size - 1) {\n\t\tparser->cont = true;\n\t\tparser->buffer[parser->idx++] = ch;\n\t\t/* Make sure the parsed string always terminates with '\\0'. */\n\t\tparser->buffer[parser->idx] = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*ppos += read;\n\tret = read;\n\nout:\n\treturn ret;\n}\n\n/* TODO add a seq_buf_to_buffer() */\nstatic ssize_t trace_seq_to_buffer(struct trace_seq *s, void *buf, size_t cnt)\n{\n\tint len;\n\n\tif (trace_seq_used(s) <= s->seq.readpos)\n\t\treturn -EBUSY;\n\n\tlen = trace_seq_used(s) - s->seq.readpos;\n\tif (cnt > len)\n\t\tcnt = len;\n\tmemcpy(buf, s->buffer + s->seq.readpos, cnt);\n\n\ts->seq.readpos += cnt;\n\treturn cnt;\n}\n\nunsigned long __read_mostly\ttracing_thresh;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n/*\n * Copy the new maximum trace into the separate maximum-trace\n * structure. (this way the maximum trace is permanently saved,\n * for later retrieval via /sys/kernel/tracing/tracing_max_latency)\n */\nstatic void\n__update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\tstruct trace_buffer *max_buf = &tr->max_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(trace_buf->data, cpu);\n\tstruct trace_array_cpu *max_data = per_cpu_ptr(max_buf->data, cpu);\n\n\tmax_buf->cpu = cpu;\n\tmax_buf->time_start = data->preempt_timestamp;\n\n\tmax_data->saved_latency = tr->max_latency;\n\tmax_data->critical_start = data->critical_start;\n\tmax_data->critical_end = data->critical_end;\n\n\tmemcpy(max_data->comm, tsk->comm, TASK_COMM_LEN);\n\tmax_data->pid = tsk->pid;\n\t/*\n\t * If tsk == current, then use current_uid(), as that does not use\n\t * RCU. The irq tracer can be called out of RCU scope.\n\t */\n\tif (tsk == current)\n\t\tmax_data->uid = current_uid();\n\telse\n\t\tmax_data->uid = task_uid(tsk);\n\n\tmax_data->nice = tsk->static_prio - 20 - MAX_RT_PRIO;\n\tmax_data->policy = tsk->policy;\n\tmax_data->rt_priority = tsk->rt_priority;\n\n\t/* record this tasks comm */\n\ttracing_record_cmdline(tsk);\n}\n\n/**\n * update_max_tr - snapshot all trace buffers from global_trace to max_tr\n * @tr: tracer\n * @tsk: the task with the latency\n * @cpu: The cpu that initiated the trace.\n *\n * Flip the buffers between the @tr and the max_tr and record information\n * about which task was the cause of this latency.\n */\nvoid\nupdate_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tswap(tr->trace_buffer.buffer, tr->max_buffer.buffer);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n\n/**\n * update_max_tr_single - only copy one trace over, and reset the rest\n * @tr - tracer\n * @tsk - task with the latency\n * @cpu - the cpu of the buffer to copy.\n *\n * Flip the trace of a single CPU buffer between the @tr and the max_tr.\n */\nvoid\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\nstatic int wait_on_pipe(struct trace_iterator *iter, bool full)\n{\n\t/* Iterators are static, they should be filled or empty */\n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn 0;\n\n\treturn ring_buffer_wait(iter->trace_buffer->buffer, iter->cpu_file,\n\t\t\t\tfull);\n}\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\nstatic bool selftests_can_run;\n\nstruct trace_selftests {\n\tstruct list_head\t\tlist;\n\tstruct tracer\t\t\t*type;\n};\n\nstatic LIST_HEAD(postponed_selftests);\n\nstatic int save_selftest(struct tracer *type)\n{\n\tstruct trace_selftests *selftest;\n\n\tselftest = kmalloc(sizeof(*selftest), GFP_KERNEL);\n\tif (!selftest)\n\t\treturn -ENOMEM;\n\n\tselftest->type = type;\n\tlist_add(&selftest->list, &postponed_selftests);\n\treturn 0;\n}\n\nstatic int run_tracer_selftest(struct tracer *type)\n{\n\tstruct trace_array *tr = &global_trace;\n\tstruct tracer *saved_tracer = tr->current_trace;\n\tint ret;\n\n\tif (!type->selftest || tracing_selftest_disabled)\n\t\treturn 0;\n\n\t/*\n\t * If a tracer registers early in boot up (before scheduling is\n\t * initialized and such), then do not run its selftests yet.\n\t * Instead, run it a little later in the boot process.\n\t */\n\tif (!selftests_can_run)\n\t\treturn save_selftest(type);\n\n\t/*\n\t * Run a selftest on this tracer.\n\t * Here we reset the trace buffer, and set the current\n\t * tracer to be this tracer. The tracer can then run some\n\t * internal tracing to verify that everything is in order.\n\t * If we fail, we do not register this tracer.\n\t */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n\ttr->current_trace = type;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\t/* If we expanded the buffers, make sure the max is expanded too */\n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, trace_buf_size,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t\ttr->allocated_snapshot = true;\n\t}\n#endif\n\n\t/* the test is responsible for initializing and enabling */\n\tpr_info(\"Testing tracer %s: \", type->name);\n\tret = type->selftest(type, tr);\n\t/* the test is responsible for resetting too */\n\ttr->current_trace = saved_tracer;\n\tif (ret) {\n\t\tprintk(KERN_CONT \"FAILED!\\n\");\n\t\t/* Add the warning after printing 'FAILED' */\n\t\tWARN_ON(1);\n\t\treturn -1;\n\t}\n\t/* Only reset on passing, to avoid touching corrupted buffers */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\ttr->allocated_snapshot = false;\n\n\t\t/* Shrink the max buffer again */\n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, 1,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t}\n#endif\n\n\tprintk(KERN_CONT \"PASSED\\n\");\n\treturn 0;\n}\n\nstatic __init int init_trace_selftests(void)\n{\n\tstruct trace_selftests *p, *n;\n\tstruct tracer *t, **last;\n\tint ret;\n\n\tselftests_can_run = true;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (list_empty(&postponed_selftests))\n\t\tgoto out;\n\n\tpr_info(\"Running postponed tracer tests:\\n\");\n\n\tlist_for_each_entry_safe(p, n, &postponed_selftests, list) {\n\t\tret = run_tracer_selftest(p->type);\n\t\t/* If the test fails, then warn and remove from available_tracers */\n\t\tif (ret < 0) {\n\t\t\tWARN(1, \"tracer: %s failed selftest, disabling\\n\",\n\t\t\t     p->type->name);\n\t\t\tlast = &trace_types;\n\t\t\tfor (t = trace_types; t; t = t->next) {\n\t\t\t\tif (t == p->type) {\n\t\t\t\t\t*last = t->next;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlast = &t->next;\n\t\t\t}\n\t\t}\n\t\tlist_del(&p->list);\n\t\tkfree(p);\n\t}\n\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\ncore_initcall(init_trace_selftests);\n#else\nstatic inline int run_tracer_selftest(struct tracer *type)\n{\n\treturn 0;\n}\n#endif /* CONFIG_FTRACE_STARTUP_TEST */\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t);\n\nstatic void __init apply_trace_boot_options(void);\n\n/**\n * register_tracer - register a tracer with the ftrace system.\n * @type - the plugin for the tracer\n *\n * Register a new plugin tracer.\n */\nint __init register_tracer(struct tracer *type)\n{\n\tstruct tracer *t;\n\tint ret = 0;\n\n\tif (!type->name) {\n\t\tpr_info(\"Tracer must have a name\\n\");\n\t\treturn -1;\n\t}\n\n\tif (strlen(type->name) >= MAX_TRACER_SIZE) {\n\t\tpr_info(\"Tracer has a name longer than %d\\n\", MAX_TRACER_SIZE);\n\t\treturn -1;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\ttracing_selftest_running = true;\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(type->name, t->name) == 0) {\n\t\t\t/* already found */\n\t\t\tpr_info(\"Tracer %s already registered\\n\",\n\t\t\t\ttype->name);\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!type->set_flag)\n\t\ttype->set_flag = &dummy_set_flag;\n\tif (!type->flags) {\n\t\t/*allocate a dummy tracer_flags*/\n\t\ttype->flags = kmalloc(sizeof(*type->flags), GFP_KERNEL);\n\t\tif (!type->flags) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttype->flags->val = 0;\n\t\ttype->flags->opts = dummy_tracer_opt;\n\t} else\n\t\tif (!type->flags->opts)\n\t\t\ttype->flags->opts = dummy_tracer_opt;\n\n\t/* store the tracer for __set_tracer_option */\n\ttype->flags->trace = type;\n\n\tret = run_tracer_selftest(type);\n\tif (ret < 0)\n\t\tgoto out;\n\n\ttype->next = trace_types;\n\ttrace_types = type;\n\tadd_tracer_options(&global_trace, type);\n\n out:\n\ttracing_selftest_running = false;\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret || !default_bootup_tracer)\n\t\tgoto out_unlock;\n\n\tif (strncmp(default_bootup_tracer, type->name, MAX_TRACER_SIZE))\n\t\tgoto out_unlock;\n\n\tprintk(KERN_INFO \"Starting tracer '%s'\\n\", type->name);\n\t/* Do we want this tracer to start on bootup? */\n\ttracing_set_tracer(&global_trace, type->name);\n\tdefault_bootup_tracer = NULL;\n\n\tapply_trace_boot_options();\n\n\t/* disable other selftests, since this will break it. */\n\ttracing_selftest_disabled = true;\n#ifdef CONFIG_FTRACE_STARTUP_TEST\n\tprintk(KERN_INFO \"Disabling FTRACE selftests due to running tracer '%s'\\n\",\n\t       type->name);\n#endif\n\n out_unlock:\n\treturn ret;\n}\n\nvoid tracing_reset(struct trace_buffer *buf, int cpu)\n{\n\tstruct ring_buffer *buffer = buf->buffer;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_sched();\n\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\nvoid tracing_reset_online_cpus(struct trace_buffer *buf)\n{\n\tstruct ring_buffer *buffer = buf->buffer;\n\tint cpu;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_sched();\n\n\tbuf->time_start = buffer_ftrace_now(buf, buf->cpu);\n\n\tfor_each_online_cpu(cpu)\n\t\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\n/* Must have trace_types_lock held */\nvoid tracing_reset_all_online_cpus(void)\n{\n\tstruct trace_array *tr;\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->clear_trace)\n\t\t\tcontinue;\n\t\ttr->clear_trace = false;\n\t\ttracing_reset_online_cpus(&tr->trace_buffer);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\t}\n}\n\nstatic int *tgid_map;\n\n#define SAVED_CMDLINES_DEFAULT 128\n#define NO_CMDLINE_MAP UINT_MAX\nstatic arch_spinlock_t trace_cmdline_lock = __ARCH_SPIN_LOCK_UNLOCKED;\nstruct saved_cmdlines_buffer {\n\tunsigned map_pid_to_cmdline[PID_MAX_DEFAULT+1];\n\tunsigned *map_cmdline_to_pid;\n\tunsigned cmdline_num;\n\tint cmdline_idx;\n\tchar *saved_cmdlines;\n};\nstatic struct saved_cmdlines_buffer *savedcmd;\n\n/* temporary disable recording */\nstatic atomic_t trace_record_taskinfo_disabled __read_mostly;\n\nstatic inline char *get_saved_cmdlines(int idx)\n{\n\treturn &savedcmd->saved_cmdlines[idx * TASK_COMM_LEN];\n}\n\nstatic inline void set_cmdline(int idx, const char *cmdline)\n{\n\tmemcpy(get_saved_cmdlines(idx), cmdline, TASK_COMM_LEN);\n}\n\nstatic int allocate_cmdlines_buffer(unsigned int val,\n\t\t\t\t    struct saved_cmdlines_buffer *s)\n{\n\ts->map_cmdline_to_pid = kmalloc_array(val,\n\t\t\t\t\t      sizeof(*s->map_cmdline_to_pid),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!s->map_cmdline_to_pid)\n\t\treturn -ENOMEM;\n\n\ts->saved_cmdlines = kmalloc_array(TASK_COMM_LEN, val, GFP_KERNEL);\n\tif (!s->saved_cmdlines) {\n\t\tkfree(s->map_cmdline_to_pid);\n\t\treturn -ENOMEM;\n\t}\n\n\ts->cmdline_idx = 0;\n\ts->cmdline_num = val;\n\tmemset(&s->map_pid_to_cmdline, NO_CMDLINE_MAP,\n\t       sizeof(s->map_pid_to_cmdline));\n\tmemset(s->map_cmdline_to_pid, NO_CMDLINE_MAP,\n\t       val * sizeof(*s->map_cmdline_to_pid));\n\n\treturn 0;\n}\n\nstatic int trace_create_savedcmd(void)\n{\n\tint ret;\n\n\tsavedcmd = kmalloc(sizeof(*savedcmd), GFP_KERNEL);\n\tif (!savedcmd)\n\t\treturn -ENOMEM;\n\n\tret = allocate_cmdlines_buffer(SAVED_CMDLINES_DEFAULT, savedcmd);\n\tif (ret < 0) {\n\t\tkfree(savedcmd);\n\t\tsavedcmd = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint is_tracing_stopped(void)\n{\n\treturn global_trace.stop_count;\n}\n\n/**\n * tracing_start - quick start of the tracer\n *\n * If tracing is enabled but was stopped by tracing_stop,\n * this will start the tracer back up.\n */\nvoid tracing_start(void)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&global_trace.start_lock, flags);\n\tif (--global_trace.stop_count) {\n\t\tif (global_trace.stop_count < 0) {\n\t\t\t/* Someone screwed up their debugging */\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tglobal_trace.stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Prevent the buffers from switching */\n\tarch_spin_lock(&global_trace.max_lock);\n\n\tbuffer = global_trace.trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = global_trace.max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n#endif\n\n\tarch_spin_unlock(&global_trace.max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&global_trace.start_lock, flags);\n}\n\nstatic void tracing_start_tr(struct trace_array *tr)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\t/* If global, we need to also start the max tracer */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn tracing_start();\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\n\tif (--tr->stop_count) {\n\t\tif (tr->stop_count < 0) {\n\t\t\t/* Someone screwed up their debugging */\n\t\t\tWARN_ON_ONCE(1);\n\t\t\ttr->stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tbuffer = tr->trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\n/**\n * tracing_stop - quick stop of the tracer\n *\n * Light weight way to stop tracing. Use in conjunction with\n * tracing_start.\n */\nvoid tracing_stop(void)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&global_trace.start_lock, flags);\n\tif (global_trace.stop_count++)\n\t\tgoto out;\n\n\t/* Prevent the buffers from switching */\n\tarch_spin_lock(&global_trace.max_lock);\n\n\tbuffer = global_trace.trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = global_trace.max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n#endif\n\n\tarch_spin_unlock(&global_trace.max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&global_trace.start_lock, flags);\n}\n\nstatic void tracing_stop_tr(struct trace_array *tr)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\t/* If global, we need to also stop the max tracer */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn tracing_stop();\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\tif (tr->stop_count++)\n\t\tgoto out;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\nstatic int trace_save_cmdline(struct task_struct *tsk)\n{\n\tunsigned pid, idx;\n\n\t/* treat recording of idle task as a success */\n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tif (unlikely(tsk->pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\t/*\n\t * It's not the end of the world if we don't get\n\t * the lock, but we also don't want to spin\n\t * nor do we want to disable interrupts,\n\t * so if we miss here, then better luck next time.\n\t */\n\tif (!arch_spin_trylock(&trace_cmdline_lock))\n\t\treturn 0;\n\n\tidx = savedcmd->map_pid_to_cmdline[tsk->pid];\n\tif (idx == NO_CMDLINE_MAP) {\n\t\tidx = (savedcmd->cmdline_idx + 1) % savedcmd->cmdline_num;\n\n\t\t/*\n\t\t * Check whether the cmdline buffer at idx has a pid\n\t\t * mapped. We are going to overwrite that entry so we\n\t\t * need to clear the map_pid_to_cmdline. Otherwise we\n\t\t * would read the new comm for the old pid.\n\t\t */\n\t\tpid = savedcmd->map_cmdline_to_pid[idx];\n\t\tif (pid != NO_CMDLINE_MAP)\n\t\t\tsavedcmd->map_pid_to_cmdline[pid] = NO_CMDLINE_MAP;\n\n\t\tsavedcmd->map_cmdline_to_pid[idx] = tsk->pid;\n\t\tsavedcmd->map_pid_to_cmdline[tsk->pid] = idx;\n\n\t\tsavedcmd->cmdline_idx = idx;\n\t}\n\n\tset_cmdline(idx, tsk->comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn 1;\n}\n\nstatic void __trace_find_cmdline(int pid, char comm[])\n{\n\tunsigned map;\n\n\tif (!pid) {\n\t\tstrcpy(comm, \"<idle>\");\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(pid < 0)) {\n\t\tstrcpy(comm, \"<XXX>\");\n\t\treturn;\n\t}\n\n\tif (pid > PID_MAX_DEFAULT) {\n\t\tstrcpy(comm, \"<...>\");\n\t\treturn;\n\t}\n\n\tmap = savedcmd->map_pid_to_cmdline[pid];\n\tif (map != NO_CMDLINE_MAP)\n\t\tstrlcpy(comm, get_saved_cmdlines(map), TASK_COMM_LEN);\n\telse\n\t\tstrcpy(comm, \"<...>\");\n}\n\nvoid trace_find_cmdline(int pid, char comm[])\n{\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\t__trace_find_cmdline(pid, comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nint trace_find_tgid(int pid)\n{\n\tif (unlikely(!tgid_map || !pid || pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\treturn tgid_map[pid];\n}\n\nstatic int trace_save_tgid(struct task_struct *tsk)\n{\n\t/* treat recording of idle task as a success */\n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tif (unlikely(!tgid_map || tsk->pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\ttgid_map[tsk->pid] = tsk->tgid;\n\treturn 1;\n}\n\nstatic bool tracing_record_taskinfo_skip(int flags)\n{\n\tif (unlikely(!(flags & (TRACE_RECORD_CMDLINE | TRACE_RECORD_TGID))))\n\t\treturn true;\n\tif (atomic_read(&trace_record_taskinfo_disabled) || !tracing_is_on())\n\t\treturn true;\n\tif (!__this_cpu_read(trace_taskinfo_save))\n\t\treturn true;\n\treturn false;\n}\n\n/**\n * tracing_record_taskinfo - record the task info of a task\n *\n * @task  - task to record\n * @flags - TRACE_RECORD_CMDLINE for recording comm\n *        - TRACE_RECORD_TGID for recording tgid\n */\nvoid tracing_record_taskinfo(struct task_struct *task, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t/*\n\t * Record as much task information as possible. If some fail, continue\n\t * to try to record the others.\n\t */\n\tdone = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(task);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(task);\n\n\t/* If recording any information failed, retry again soon. */\n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n/**\n * tracing_record_taskinfo_sched_switch - record task info for sched_switch\n *\n * @prev - previous task during sched_switch\n * @next - next task during sched_switch\n * @flags - TRACE_RECORD_CMDLINE for recording comm\n *          TRACE_RECORD_TGID for recording tgid\n */\nvoid tracing_record_taskinfo_sched_switch(struct task_struct *prev,\n\t\t\t\t\t  struct task_struct *next, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t/*\n\t * Record as much task information as possible. If some fail, continue\n\t * to try to record the others.\n\t */\n\tdone  = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(prev);\n\tdone &= !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(next);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(prev);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(next);\n\n\t/* If recording any information failed, retry again soon. */\n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n/* Helpers to record a specific task information */\nvoid tracing_record_cmdline(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_CMDLINE);\n}\n\nvoid tracing_record_tgid(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_TGID);\n}\n\n/*\n * Several functions return TRACE_TYPE_PARTIAL_LINE if the trace_seq\n * overflowed, and TRACE_TYPE_HANDLED otherwise. This helper function\n * simplifies those functions and keeps them in sync.\n */\nenum print_line_t trace_handle_return(struct trace_seq *s)\n{\n\treturn trace_seq_has_overflowed(s) ?\n\t\tTRACE_TYPE_PARTIAL_LINE : TRACE_TYPE_HANDLED;\n}\nEXPORT_SYMBOL_GPL(trace_handle_return);\n\nvoid\ntracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,\n\t\t\t     int pc)\n{\n\tstruct task_struct *tsk = current;\n\n\tentry->preempt_count\t\t= pc & 0xff;\n\tentry->pid\t\t\t= (tsk) ? tsk->pid : 0;\n\tentry->flags =\n#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT\n\t\t(irqs_disabled_flags(flags) ? TRACE_FLAG_IRQS_OFF : 0) |\n#else\n\t\tTRACE_FLAG_IRQS_NOSUPPORT |\n#endif\n\t\t((pc & NMI_MASK    ) ? TRACE_FLAG_NMI     : 0) |\n\t\t((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |\n\t\t((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |\n\t\t(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |\n\t\t(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);\n}\nEXPORT_SYMBOL_GPL(tracing_generic_entry_update);\n\nstruct ring_buffer_event *\ntrace_buffer_lock_reserve(struct ring_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\treturn __trace_buffer_lock_reserve(buffer, type, len, flags, pc);\n}\n\nDEFINE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);\nDEFINE_PER_CPU(int, trace_buffered_event_cnt);\nstatic int trace_buffered_event_ref;\n\n/**\n * trace_buffered_event_enable - enable buffering events\n *\n * When events are being filtered, it is quicker to use a temporary\n * buffer to write the event data into if there's a likely chance\n * that it will not be committed. The discard of the ring buffer\n * is not as fast as committing, and is much slower than copying\n * a commit.\n *\n * When an event is to be filtered, allocate per cpu buffers to\n * write the event data into, and if the event is filtered and discarded\n * it is simply dropped, otherwise, the entire data is to be committed\n * in one shot.\n */\nvoid trace_buffered_event_enable(void)\n{\n\tstruct ring_buffer_event *event;\n\tstruct page *page;\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (trace_buffered_event_ref++)\n\t\treturn;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\tif (!page)\n\t\t\tgoto failed;\n\n\t\tevent = page_address(page);\n\t\tmemset(event, 0, sizeof(*event));\n\n\t\tper_cpu(trace_buffered_event, cpu) = event;\n\n\t\tpreempt_disable();\n\t\tif (cpu == smp_processor_id() &&\n\t\t    this_cpu_read(trace_buffered_event) !=\n\t\t    per_cpu(trace_buffered_event, cpu))\n\t\t\tWARN_ON_ONCE(1);\n\t\tpreempt_enable();\n\t}\n\n\treturn;\n failed:\n\ttrace_buffered_event_disable();\n}\n\nstatic void enable_trace_buffered_event(void *data)\n{\n\t/* Probably not needed, but do it anyway */\n\tsmp_rmb();\n\tthis_cpu_dec(trace_buffered_event_cnt);\n}\n\nstatic void disable_trace_buffered_event(void *data)\n{\n\tthis_cpu_inc(trace_buffered_event_cnt);\n}\n\n/**\n * trace_buffered_event_disable - disable buffering events\n *\n * When a filter is removed, it is faster to not use the buffered\n * events, and to commit directly into the ring buffer. Free up\n * the temp buffers when there are no more users. This requires\n * special synchronization with current events.\n */\nvoid trace_buffered_event_disable(void)\n{\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (WARN_ON_ONCE(!trace_buffered_event_ref))\n\t\treturn;\n\n\tif (--trace_buffered_event_ref)\n\t\treturn;\n\n\tpreempt_disable();\n\t/* For each CPU, set the buffer as used. */\n\tsmp_call_function_many(tracing_buffer_mask,\n\t\t\t       disable_trace_buffered_event, NULL, 1);\n\tpreempt_enable();\n\n\t/* Wait for all current users to finish */\n\tsynchronize_sched();\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tfree_page((unsigned long)per_cpu(trace_buffered_event, cpu));\n\t\tper_cpu(trace_buffered_event, cpu) = NULL;\n\t}\n\t/*\n\t * Make sure trace_buffered_event is NULL before clearing\n\t * trace_buffered_event_cnt.\n\t */\n\tsmp_wmb();\n\n\tpreempt_disable();\n\t/* Do the work on each cpu */\n\tsmp_call_function_many(tracing_buffer_mask,\n\t\t\t       enable_trace_buffered_event, NULL, 1);\n\tpreempt_enable();\n}\n\nstatic struct ring_buffer *temp_buffer;\n\nstruct ring_buffer_event *\ntrace_event_buffer_lock_reserve(struct ring_buffer **current_rb,\n\t\t\t  struct trace_event_file *trace_file,\n\t\t\t  int type, unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\tstruct ring_buffer_event *entry;\n\tint val;\n\n\t*current_rb = trace_file->tr->trace_buffer.buffer;\n\n\tif (!ring_buffer_time_stamp_abs(*current_rb) && (trace_file->flags &\n\t     (EVENT_FILE_FL_SOFT_DISABLED | EVENT_FILE_FL_FILTERED)) &&\n\t    (entry = this_cpu_read(trace_buffered_event))) {\n\t\t/* Try to use the per cpu buffer first */\n\t\tval = this_cpu_inc_return(trace_buffered_event_cnt);\n\t\tif (val == 1) {\n\t\t\ttrace_event_setup(entry, type, flags, pc);\n\t\t\tentry->array[0] = len;\n\t\t\treturn entry;\n\t\t}\n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t}\n\n\tentry = __trace_buffer_lock_reserve(*current_rb,\n\t\t\t\t\t    type, len, flags, pc);\n\t/*\n\t * If tracing is off, but we have triggers enabled\n\t * we still need to look at the event data. Use the temp_buffer\n\t * to store the trace event for the tigger to use. It's recusive\n\t * safe and will not be recorded anywhere.\n\t */\n\tif (!entry && trace_file->flags & EVENT_FILE_FL_TRIGGER_COND) {\n\t\t*current_rb = temp_buffer;\n\t\tentry = __trace_buffer_lock_reserve(*current_rb,\n\t\t\t\t\t\t    type, len, flags, pc);\n\t}\n\treturn entry;\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_lock_reserve);\n\nstatic DEFINE_SPINLOCK(tracepoint_iter_lock);\nstatic DEFINE_MUTEX(tracepoint_printk_mutex);\n\nstatic void output_printk(struct trace_event_buffer *fbuffer)\n{\n\tstruct trace_event_call *event_call;\n\tstruct trace_event *event;\n\tunsigned long flags;\n\tstruct trace_iterator *iter = tracepoint_print_iter;\n\n\t/* We should never get here if iter is NULL */\n\tif (WARN_ON_ONCE(!iter))\n\t\treturn;\n\n\tevent_call = fbuffer->trace_file->event_call;\n\tif (!event_call || !event_call->event.funcs ||\n\t    !event_call->event.funcs->trace)\n\t\treturn;\n\n\tevent = &fbuffer->trace_file->event_call->event;\n\n\tspin_lock_irqsave(&tracepoint_iter_lock, flags);\n\ttrace_seq_init(&iter->seq);\n\titer->ent = fbuffer->entry;\n\tevent_call->event.funcs->trace(iter, 0, event);\n\ttrace_seq_putc(&iter->seq, 0);\n\tprintk(\"%s\", iter->seq.buffer);\n\n\tspin_unlock_irqrestore(&tracepoint_iter_lock, flags);\n}\n\nint tracepoint_printk_sysctl(struct ctl_table *table, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos)\n{\n\tint save_tracepoint_printk;\n\tint ret;\n\n\tmutex_lock(&tracepoint_printk_mutex);\n\tsave_tracepoint_printk = tracepoint_printk;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\t/*\n\t * This will force exiting early, as tracepoint_printk\n\t * is always zero when tracepoint_printk_iter is not allocated\n\t */\n\tif (!tracepoint_print_iter)\n\t\ttracepoint_printk = 0;\n\n\tif (save_tracepoint_printk == tracepoint_printk)\n\t\tgoto out;\n\n\tif (tracepoint_printk)\n\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\telse\n\t\tstatic_key_disable(&tracepoint_printk_key.key);\n\n out:\n\tmutex_unlock(&tracepoint_printk_mutex);\n\n\treturn ret;\n}\n\nvoid trace_event_buffer_commit(struct trace_event_buffer *fbuffer)\n{\n\tif (static_key_false(&tracepoint_printk_key.key))\n\t\toutput_printk(fbuffer);\n\n\tevent_trigger_unlock_commit(fbuffer->trace_file, fbuffer->buffer,\n\t\t\t\t    fbuffer->event, fbuffer->entry,\n\t\t\t\t    fbuffer->flags, fbuffer->pc);\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_commit);\n\n/*\n * Skip 3:\n *\n *   trace_buffer_unlock_commit_regs()\n *   trace_event_buffer_commit()\n *   trace_event_raw_event_xxx()\n */\n# define STACK_SKIP 3\n\nvoid trace_buffer_unlock_commit_regs(struct trace_array *tr,\n\t\t\t\t     struct ring_buffer *buffer,\n\t\t\t\t     struct ring_buffer_event *event,\n\t\t\t\t     unsigned long flags, int pc,\n\t\t\t\t     struct pt_regs *regs)\n{\n\t__buffer_unlock_commit(buffer, event);\n\n\t/*\n\t * If regs is not set, then skip the necessary functions.\n\t * Note, we can still get here via blktrace, wakeup tracer\n\t * and mmiotrace, but that's ok if they lose a function or\n\t * two. They are not that meaningful.\n\t */\n\tftrace_trace_stack(tr, buffer, flags, regs ? 0 : STACK_SKIP, pc, regs);\n\tftrace_trace_userstack(buffer, flags, pc);\n}\n\n/*\n * Similar to trace_buffer_unlock_commit_regs() but do not dump stack.\n */\nvoid\ntrace_buffer_unlock_commit_nostack(struct ring_buffer *buffer,\n\t\t\t\t   struct ring_buffer_event *event)\n{\n\t__buffer_unlock_commit(buffer, event);\n}\n\nstatic void\ntrace_process_export(struct trace_export *export,\n\t       struct ring_buffer_event *event)\n{\n\tstruct trace_entry *entry;\n\tunsigned int size = 0;\n\n\tentry = ring_buffer_event_data(event);\n\tsize = ring_buffer_event_length(event);\n\texport->write(export, entry, size);\n}\n\nstatic DEFINE_MUTEX(ftrace_export_lock);\n\nstatic struct trace_export __rcu *ftrace_exports_list __read_mostly;\n\nstatic DEFINE_STATIC_KEY_FALSE(ftrace_exports_enabled);\n\nstatic inline void ftrace_exports_enable(void)\n{\n\tstatic_branch_enable(&ftrace_exports_enabled);\n}\n\nstatic inline void ftrace_exports_disable(void)\n{\n\tstatic_branch_disable(&ftrace_exports_enabled);\n}\n\nvoid ftrace_exports(struct ring_buffer_event *event)\n{\n\tstruct trace_export *export;\n\n\tpreempt_disable_notrace();\n\n\texport = rcu_dereference_raw_notrace(ftrace_exports_list);\n\twhile (export) {\n\t\ttrace_process_export(export, event);\n\t\texport = rcu_dereference_raw_notrace(export->next);\n\t}\n\n\tpreempt_enable_notrace();\n}\n\nstatic inline void\nadd_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\trcu_assign_pointer(export->next, *list);\n\t/*\n\t * We are entering export into the list but another\n\t * CPU might be walking that list. We need to make sure\n\t * the export->next pointer is valid before another CPU sees\n\t * the export pointer included into the list.\n\t */\n\trcu_assign_pointer(*list, export);\n}\n\nstatic inline int\nrm_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\tstruct trace_export **p;\n\n\tfor (p = list; *p != NULL; p = &(*p)->next)\n\t\tif (*p == export)\n\t\t\tbreak;\n\n\tif (*p != export)\n\t\treturn -1;\n\n\trcu_assign_pointer(*p, (*p)->next);\n\n\treturn 0;\n}\n\nstatic inline void\nadd_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tif (*list == NULL)\n\t\tftrace_exports_enable();\n\n\tadd_trace_export(list, export);\n}\n\nstatic inline int\nrm_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tint ret;\n\n\tret = rm_trace_export(list, export);\n\tif (*list == NULL)\n\t\tftrace_exports_disable();\n\n\treturn ret;\n}\n\nint register_ftrace_export(struct trace_export *export)\n{\n\tif (WARN_ON_ONCE(!export->write))\n\t\treturn -1;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tadd_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(register_ftrace_export);\n\nint unregister_ftrace_export(struct trace_export *export)\n{\n\tint ret;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tret = rm_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(unregister_ftrace_export);\n\nvoid\ntrace_function(struct trace_array *tr,\n\t       unsigned long ip, unsigned long parent_ip, unsigned long flags,\n\t       int pc)\n{\n\tstruct trace_event_call *call = &event_function;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_entry *entry;\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->parent_ip\t\t= parent_ip;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\tif (static_branch_unlikely(&ftrace_exports_enabled))\n\t\t\tftrace_exports(event);\n\t\t__buffer_unlock_commit(buffer, event);\n\t}\n}\n\n#ifdef CONFIG_STACKTRACE\n\n#define FTRACE_STACK_MAX_ENTRIES (PAGE_SIZE / sizeof(unsigned long))\nstruct ftrace_stack {\n\tunsigned long\t\tcalls[FTRACE_STACK_MAX_ENTRIES];\n};\n\nstatic DEFINE_PER_CPU(struct ftrace_stack, ftrace_stack);\nstatic DEFINE_PER_CPU(int, ftrace_stack_reserve);\n\nstatic void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t unsigned long flags,\n\t\t\t\t int skip, int pc, struct pt_regs *regs)\n{\n\tstruct trace_event_call *call = &event_kernel_stack;\n\tstruct ring_buffer_event *event;\n\tstruct stack_entry *entry;\n\tstruct stack_trace trace;\n\tint use_stack;\n\tint size = FTRACE_STACK_ENTRIES;\n\n\ttrace.nr_entries\t= 0;\n\ttrace.skip\t\t= skip;\n\n\t/*\n\t * Add one, for this function and the call to save_stack_trace()\n\t * If regs is set, then these functions will not be in the way.\n\t */\n#ifndef CONFIG_UNWINDER_ORC\n\tif (!regs)\n\t\ttrace.skip++;\n#endif\n\n\t/*\n\t * Since events can happen in NMIs there's no safe way to\n\t * use the per cpu ftrace_stacks. We reserve it and if an interrupt\n\t * or NMI comes in, it will just have to use the default\n\t * FTRACE_STACK_SIZE.\n\t */\n\tpreempt_disable_notrace();\n\n\tuse_stack = __this_cpu_inc_return(ftrace_stack_reserve);\n\t/*\n\t * We don't need any atomic variables, just a barrier.\n\t * If an interrupt comes in, we don't care, because it would\n\t * have exited and put the counter back to what we want.\n\t * We just need a barrier to keep gcc from moving things\n\t * around.\n\t */\n\tbarrier();\n\tif (use_stack == 1) {\n\t\ttrace.entries\t\t= this_cpu_ptr(ftrace_stack.calls);\n\t\ttrace.max_entries\t= FTRACE_STACK_MAX_ENTRIES;\n\n\t\tif (regs)\n\t\t\tsave_stack_trace_regs(regs, &trace);\n\t\telse\n\t\t\tsave_stack_trace(&trace);\n\n\t\tif (trace.nr_entries > size)\n\t\t\tsize = trace.nr_entries;\n\t} else\n\t\t/* From now on, use_stack is a boolean */\n\t\tuse_stack = 0;\n\n\tsize *= sizeof(unsigned long);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_STACK,\n\t\t\t\t\t    sizeof(*entry) + size, flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\n\tmemset(&entry->caller, 0, size);\n\n\tif (use_stack)\n\t\tmemcpy(&entry->caller, trace.entries,\n\t\t       trace.nr_entries * sizeof(unsigned long));\n\telse {\n\t\ttrace.max_entries\t= FTRACE_STACK_ENTRIES;\n\t\ttrace.entries\t\t= entry->caller;\n\t\tif (regs)\n\t\t\tsave_stack_trace_regs(regs, &trace);\n\t\telse\n\t\t\tsave_stack_trace(&trace);\n\t}\n\n\tentry->size = trace.nr_entries;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out:\n\t/* Again, don't let gcc optimize things here */\n\tbarrier();\n\t__this_cpu_dec(ftrace_stack_reserve);\n\tpreempt_enable_notrace();\n\n}\n\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs)\n{\n\tif (!(tr->trace_flags & TRACE_ITER_STACKTRACE))\n\t\treturn;\n\n\t__ftrace_trace_stack(buffer, flags, skip, pc, regs);\n}\n\nvoid __trace_stack(struct trace_array *tr, unsigned long flags, int skip,\n\t\t   int pc)\n{\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\n\tif (rcu_is_watching()) {\n\t\t__ftrace_trace_stack(buffer, flags, skip, pc, NULL);\n\t\treturn;\n\t}\n\n\t/*\n\t * When an NMI triggers, RCU is enabled via rcu_nmi_enter(),\n\t * but if the above rcu_is_watching() failed, then the NMI\n\t * triggered someplace critical, and rcu_irq_enter() should\n\t * not be called from NMI.\n\t */\n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\trcu_irq_enter_irqson();\n\t__ftrace_trace_stack(buffer, flags, skip, pc, NULL);\n\trcu_irq_exit_irqson();\n}\n\n/**\n * trace_dump_stack - record a stack back trace in the trace buffer\n * @skip: Number of functions to skip (helper handlers)\n */\nvoid trace_dump_stack(int skip)\n{\n\tunsigned long flags;\n\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn;\n\n\tlocal_save_flags(flags);\n\n#ifndef CONFIG_UNWINDER_ORC\n\t/* Skip 1 to skip this function. */\n\tskip++;\n#endif\n\t__ftrace_trace_stack(global_trace.trace_buffer.buffer,\n\t\t\t     flags, skip, preempt_count(), NULL);\n}\n\nstatic DEFINE_PER_CPU(int, user_stack_count);\n\nvoid\nftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags, int pc)\n{\n\tstruct trace_event_call *call = &event_user_stack;\n\tstruct ring_buffer_event *event;\n\tstruct userstack_entry *entry;\n\tstruct stack_trace trace;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_USERSTACKTRACE))\n\t\treturn;\n\n\t/*\n\t * NMIs can not handle page faults, even with fix ups.\n\t * The save user stack can (and often does) fault.\n\t */\n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\t/*\n\t * prevent recursion, since the user stack tracing may\n\t * trigger other kernel events.\n\t */\n\tpreempt_disable();\n\tif (__this_cpu_read(user_stack_count))\n\t\tgoto out;\n\n\t__this_cpu_inc(user_stack_count);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_USER_STACK,\n\t\t\t\t\t    sizeof(*entry), flags, pc);\n\tif (!event)\n\t\tgoto out_drop_count;\n\tentry\t= ring_buffer_event_data(event);\n\n\tentry->tgid\t\t= current->tgid;\n\tmemset(&entry->caller, 0, sizeof(entry->caller));\n\n\ttrace.nr_entries\t= 0;\n\ttrace.max_entries\t= FTRACE_STACK_ENTRIES;\n\ttrace.skip\t\t= 0;\n\ttrace.entries\t\t= entry->caller;\n\n\tsave_stack_trace_user(&trace);\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out_drop_count:\n\t__this_cpu_dec(user_stack_count);\n out:\n\tpreempt_enable();\n}\n\n#ifdef UNUSED\nstatic void __trace_userstack(struct trace_array *tr, unsigned long flags)\n{\n\tftrace_trace_userstack(tr, flags, preempt_count());\n}\n#endif /* UNUSED */\n\n#endif /* CONFIG_STACKTRACE */\n\n/* created for use with alloc_percpu */\nstruct trace_buffer_struct {\n\tint nesting;\n\tchar buffer[4][TRACE_BUF_SIZE];\n};\n\nstatic struct trace_buffer_struct *trace_percpu_buffer;\n\n/*\n * Thise allows for lockless recording.  If we're nested too deeply, then\n * this returns NULL.\n */\nstatic char *get_trace_buf(void)\n{\n\tstruct trace_buffer_struct *buffer = this_cpu_ptr(trace_percpu_buffer);\n\n\tif (!buffer || buffer->nesting >= 4)\n\t\treturn NULL;\n\n\tbuffer->nesting++;\n\n\t/* Interrupts must see nesting incremented before we use the buffer */\n\tbarrier();\n\treturn &buffer->buffer[buffer->nesting][0];\n}\n\nstatic void put_trace_buf(void)\n{\n\t/* Don't let the decrement of nesting leak before this */\n\tbarrier();\n\tthis_cpu_dec(trace_percpu_buffer->nesting);\n}\n\nstatic int alloc_percpu_trace_buffer(void)\n{\n\tstruct trace_buffer_struct *buffers;\n\n\tbuffers = alloc_percpu(struct trace_buffer_struct);\n\tif (WARN(!buffers, \"Could not allocate percpu trace_printk buffer\"))\n\t\treturn -ENOMEM;\n\n\ttrace_percpu_buffer = buffers;\n\treturn 0;\n}\n\nstatic int buffers_allocated;\n\nvoid trace_printk_init_buffers(void)\n{\n\tif (buffers_allocated)\n\t\treturn;\n\n\tif (alloc_percpu_trace_buffer())\n\t\treturn;\n\n\t/* trace_printk() is for debug use only. Don't use it in production. */\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** trace_printk() being used. Allocating extra memory.  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** This means that this is a DEBUG kernel and it is     **\\n\");\n\tpr_warn(\"** unsafe for production use.                           **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** If you see this message and you are not debugging    **\\n\");\n\tpr_warn(\"** the kernel, report this immediately to your vendor!  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\n\t/* Expand the buffers to set size */\n\ttracing_update_buffers();\n\n\tbuffers_allocated = 1;\n\n\t/*\n\t * trace_printk_init_buffers() can be called by modules.\n\t * If that happens, then we need to start cmdline recording\n\t * directly here. If the global_trace.buffer is already\n\t * allocated here, then this was called by module code.\n\t */\n\tif (global_trace.trace_buffer.buffer)\n\t\ttracing_start_cmdline_record();\n}\n\nvoid trace_printk_start_comm(void)\n{\n\t/* Start tracing comms if trace printk is set */\n\tif (!buffers_allocated)\n\t\treturn;\n\ttracing_start_cmdline_record();\n}\n\nstatic void trace_printk_start_stop_comm(int enabled)\n{\n\tif (!buffers_allocated)\n\t\treturn;\n\n\tif (enabled)\n\t\ttracing_start_cmdline_record();\n\telse\n\t\ttracing_stop_cmdline_record();\n}\n\n/**\n * trace_vbprintk - write binary msg to tracing buffer\n *\n */\nint trace_vbprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_bprint;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct trace_array *tr = &global_trace;\n\tstruct bprint_entry *entry;\n\tunsigned long flags;\n\tchar *tbuffer;\n\tint len = 0, size, pc;\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\t/* Don't pollute graph traces with trace_vprintk internals */\n\tpause_graph_tracing();\n\n\tpc = preempt_count();\n\tpreempt_disable_notrace();\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vbin_printf((u32 *)tbuffer, TRACE_BUF_SIZE/sizeof(int), fmt, args);\n\n\tif (len > TRACE_BUF_SIZE/sizeof(int) || len < 0)\n\t\tgoto out;\n\n\tlocal_save_flags(flags);\n\tsize = sizeof(*entry) + sizeof(u32) * len;\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPRINT, size,\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->fmt\t\t\t= fmt;\n\n\tmemcpy(entry->buf, tbuffer, sizeof(u32) * len);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(tr, buffer, flags, 6, pc, NULL);\n\t}\n\nout:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(trace_vbprintk);\n\nstatic int\n__trace_array_vprintk(struct ring_buffer *buffer,\n\t\t      unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_print;\n\tstruct ring_buffer_event *event;\n\tint len = 0, size, pc;\n\tstruct print_entry *entry;\n\tunsigned long flags;\n\tchar *tbuffer;\n\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn 0;\n\n\t/* Don't pollute graph traces with trace_vprintk internals */\n\tpause_graph_tracing();\n\n\tpc = preempt_count();\n\tpreempt_disable_notrace();\n\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vscnprintf(tbuffer, TRACE_BUF_SIZE, fmt, args);\n\n\tlocal_save_flags(flags);\n\tsize = sizeof(*entry) + len + 1;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, tbuffer, len + 1);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(&global_trace, buffer, flags, 6, pc, NULL);\n\t}\n\nout:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\n\nint trace_array_vprintk(struct trace_array *tr,\n\t\t\tunsigned long ip, const char *fmt, va_list args)\n{\n\treturn __trace_array_vprintk(tr->trace_buffer.buffer, ip, fmt, args);\n}\n\nint trace_array_printk(struct trace_array *tr,\n\t\t       unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = trace_array_vprintk(tr, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\nint trace_array_printk_buf(struct ring_buffer *buffer,\n\t\t\t   unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = __trace_array_vprintk(buffer, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\nint trace_vprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\treturn trace_array_vprintk(&global_trace, ip, fmt, args);\n}\nEXPORT_SYMBOL_GPL(trace_vprintk);\n\nstatic void trace_iterator_increment(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, iter->cpu);\n\n\titer->idx++;\n\tif (buf_iter)\n\t\tring_buffer_read(buf_iter, NULL);\n}\n\nstatic struct trace_entry *\npeek_next_entry(struct trace_iterator *iter, int cpu, u64 *ts,\n\t\tunsigned long *lost_events)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, cpu);\n\n\tif (buf_iter)\n\t\tevent = ring_buffer_iter_peek(buf_iter, ts);\n\telse\n\t\tevent = ring_buffer_peek(iter->trace_buffer->buffer, cpu, ts,\n\t\t\t\t\t lost_events);\n\n\tif (event) {\n\t\titer->ent_size = ring_buffer_event_length(event);\n\t\treturn ring_buffer_event_data(event);\n\t}\n\titer->ent_size = 0;\n\treturn NULL;\n}\n\nstatic struct trace_entry *\n__find_next_entry(struct trace_iterator *iter, int *ent_cpu,\n\t\t  unsigned long *missing_events, u64 *ent_ts)\n{\n\tstruct ring_buffer *buffer = iter->trace_buffer->buffer;\n\tstruct trace_entry *ent, *next = NULL;\n\tunsigned long lost_events = 0, next_lost = 0;\n\tint cpu_file = iter->cpu_file;\n\tu64 next_ts = 0, ts;\n\tint next_cpu = -1;\n\tint next_size = 0;\n\tint cpu;\n\n\t/*\n\t * If we are in a per_cpu trace file, don't bother by iterating over\n\t * all cpu and peek directly.\n\t */\n\tif (cpu_file > RING_BUFFER_ALL_CPUS) {\n\t\tif (ring_buffer_empty_cpu(buffer, cpu_file))\n\t\t\treturn NULL;\n\t\tent = peek_next_entry(iter, cpu_file, ent_ts, missing_events);\n\t\tif (ent_cpu)\n\t\t\t*ent_cpu = cpu_file;\n\n\t\treturn ent;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\n\t\tif (ring_buffer_empty_cpu(buffer, cpu))\n\t\t\tcontinue;\n\n\t\tent = peek_next_entry(iter, cpu, &ts, &lost_events);\n\n\t\t/*\n\t\t * Pick the entry with the smallest timestamp:\n\t\t */\n\t\tif (ent && (!next || ts < next_ts)) {\n\t\t\tnext = ent;\n\t\t\tnext_cpu = cpu;\n\t\t\tnext_ts = ts;\n\t\t\tnext_lost = lost_events;\n\t\t\tnext_size = iter->ent_size;\n\t\t}\n\t}\n\n\titer->ent_size = next_size;\n\n\tif (ent_cpu)\n\t\t*ent_cpu = next_cpu;\n\n\tif (ent_ts)\n\t\t*ent_ts = next_ts;\n\n\tif (missing_events)\n\t\t*missing_events = next_lost;\n\n\treturn next;\n}\n\n/* Find the next real entry, without updating the iterator itself */\nstruct trace_entry *trace_find_next_entry(struct trace_iterator *iter,\n\t\t\t\t\t  int *ent_cpu, u64 *ent_ts)\n{\n\treturn __find_next_entry(iter, ent_cpu, NULL, ent_ts);\n}\n\n/* Find the next real entry, and increment the iterator to the next entry */\nvoid *trace_find_next_entry_inc(struct trace_iterator *iter)\n{\n\titer->ent = __find_next_entry(iter, &iter->cpu,\n\t\t\t\t      &iter->lost_events, &iter->ts);\n\n\tif (iter->ent)\n\t\ttrace_iterator_increment(iter);\n\n\treturn iter->ent ? iter : NULL;\n}\n\nstatic void trace_consume(struct trace_iterator *iter)\n{\n\tring_buffer_consume(iter->trace_buffer->buffer, iter->cpu, &iter->ts,\n\t\t\t    &iter->lost_events);\n}\n\nstatic void *s_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tint i = (int)*pos;\n\tvoid *ent;\n\n\tWARN_ON_ONCE(iter->leftover);\n\n\t(*pos)++;\n\n\t/* can't go backwards */\n\tif (iter->idx > i)\n\t\treturn NULL;\n\n\tif (iter->idx < 0)\n\t\tent = trace_find_next_entry_inc(iter);\n\telse\n\t\tent = iter;\n\n\twhile (ent && iter->idx < i)\n\t\tent = trace_find_next_entry_inc(iter);\n\n\titer->pos = *pos;\n\n\treturn ent;\n}\n\nvoid tracing_iter_reset(struct trace_iterator *iter, int cpu)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter;\n\tunsigned long entries = 0;\n\tu64 ts;\n\n\tper_cpu_ptr(iter->trace_buffer->data, cpu)->skipped_entries = 0;\n\n\tbuf_iter = trace_buffer_iter(iter, cpu);\n\tif (!buf_iter)\n\t\treturn;\n\n\tring_buffer_iter_reset(buf_iter);\n\n\t/*\n\t * We could have the case with the max latency tracers\n\t * that a reset never took place on a cpu. This is evident\n\t * by the timestamp being before the start of the buffer.\n\t */\n\twhile ((event = ring_buffer_iter_peek(buf_iter, &ts))) {\n\t\tif (ts >= iter->trace_buffer->time_start)\n\t\t\tbreak;\n\t\tentries++;\n\t\tring_buffer_read(buf_iter, NULL);\n\t}\n\n\tper_cpu_ptr(iter->trace_buffer->data, cpu)->skipped_entries = entries;\n}\n\n/*\n * The current tracer is copied to avoid a global locking\n * all around.\n */\nstatic void *s_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tint cpu_file = iter->cpu_file;\n\tvoid *p = NULL;\n\tloff_t l = 0;\n\tint cpu;\n\n\t/*\n\t * copy the tracer to avoid using a global lock all around.\n\t * iter->trace is a copy of current_trace, the pointer to the\n\t * name may be used instead of a strcmp(), as iter->trace->name\n\t * will point to the same string as current_trace->name.\n\t */\n\tmutex_lock(&trace_types_lock);\n\tif (unlikely(tr->current_trace && iter->trace->name != tr->current_trace->name))\n\t\t*iter->trace = *tr->current_trace;\n\tmutex_unlock(&trace_types_lock);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn ERR_PTR(-EBUSY);\n#endif\n\n\tif (!iter->snapshot)\n\t\tatomic_inc(&trace_record_taskinfo_disabled);\n\n\tif (*pos != iter->pos) {\n\t\titer->ent = NULL;\n\t\titer->cpu = 0;\n\t\titer->idx = -1;\n\n\t\tif (cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\t\tfor_each_tracing_cpu(cpu)\n\t\t\t\ttracing_iter_reset(iter, cpu);\n\t\t} else\n\t\t\ttracing_iter_reset(iter, cpu_file);\n\n\t\titer->leftover = 0;\n\t\tfor (p = iter; p && l < *pos; p = s_next(m, p, &l))\n\t\t\t;\n\n\t} else {\n\t\t/*\n\t\t * If we overflowed the seq_file before, then we want\n\t\t * to just reuse the trace_seq buffer again.\n\t\t */\n\t\tif (iter->leftover)\n\t\t\tp = iter;\n\t\telse {\n\t\t\tl = *pos - 1;\n\t\t\tp = s_next(m, p, &l);\n\t\t}\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(cpu_file);\n\treturn p;\n}\n\nstatic void s_stop(struct seq_file *m, void *p)\n{\n\tstruct trace_iterator *iter = m->private;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn;\n#endif\n\n\tif (!iter->snapshot)\n\t\tatomic_dec(&trace_record_taskinfo_disabled);\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n}\n\nstatic void\nget_total_entries(struct trace_buffer *buf,\n\t\t  unsigned long *total, unsigned long *entries)\n{\n\tunsigned long count;\n\tint cpu;\n\n\t*total = 0;\n\t*entries = 0;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tcount = ring_buffer_entries_cpu(buf->buffer, cpu);\n\t\t/*\n\t\t * If this buffer has skipped entries, then we hold all\n\t\t * entries for the trace and we need to ignore the\n\t\t * ones before the time stamp.\n\t\t */\n\t\tif (per_cpu_ptr(buf->data, cpu)->skipped_entries) {\n\t\t\tcount -= per_cpu_ptr(buf->data, cpu)->skipped_entries;\n\t\t\t/* total is the same as the entries */\n\t\t\t*total += count;\n\t\t} else\n\t\t\t*total += count +\n\t\t\t\tring_buffer_overrun_cpu(buf->buffer, cpu);\n\t\t*entries += count;\n\t}\n}\n\nstatic void print_lat_help_header(struct seq_file *m)\n{\n\tseq_puts(m, \"#                  _------=> CPU#            \\n\"\n\t\t    \"#                 / _-----=> irqs-off        \\n\"\n\t\t    \"#                | / _----=> need-resched    \\n\"\n\t\t    \"#                || / _---=> hardirq/softirq \\n\"\n\t\t    \"#                ||| / _--=> preempt-depth   \\n\"\n\t\t    \"#                |||| /     delay            \\n\"\n\t\t    \"#  cmd     pid   ||||| time  |   caller      \\n\"\n\t\t    \"#     \\\\   /      |||||  \\\\    |   /         \\n\");\n}\n\nstatic void print_event_info(struct trace_buffer *buf, struct seq_file *m)\n{\n\tunsigned long total;\n\tunsigned long entries;\n\n\tget_total_entries(buf, &total, &entries);\n\tseq_printf(m, \"# entries-in-buffer/entries-written: %lu/%lu   #P:%d\\n\",\n\t\t   entries, total, num_online_cpus());\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void print_func_help_header(struct trace_buffer *buf, struct seq_file *m,\n\t\t\t\t   unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\n\tprint_event_info(buf, m);\n\n\tseq_printf(m, \"#           TASK-PID   CPU#   %s  TIMESTAMP  FUNCTION\\n\", tgid ? \"TGID     \" : \"\");\n\tseq_printf(m, \"#              | |       |    %s     |         |\\n\",\t tgid ? \"  |      \" : \"\");\n}\n\nstatic void print_func_help_header_irq(struct trace_buffer *buf, struct seq_file *m,\n\t\t\t\t       unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\tconst char tgid_space[] = \"          \";\n\tconst char space[] = \"  \";\n\n\tseq_printf(m, \"#                          %s  _-----=> irqs-off\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s / _----=> need-resched\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s| / _---=> hardirq/softirq\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s|| / _--=> preempt-depth\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s||| /     delay\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#           TASK-PID   CPU#%s||||    TIMESTAMP  FUNCTION\\n\",\n\t\t   tgid ? \"   TGID   \" : space);\n\tseq_printf(m, \"#              | |       | %s||||       |         |\\n\",\n\t\t   tgid ? \"     |    \" : space);\n}\n\nvoid\nprint_trace_header(struct seq_file *m, struct trace_iterator *iter)\n{\n\tunsigned long sym_flags = (global_trace.trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_buffer *buf = iter->trace_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(buf->data, buf->cpu);\n\tstruct tracer *type = iter->trace;\n\tunsigned long entries;\n\tunsigned long total;\n\tconst char *name = \"preemption\";\n\n\tname = type->name;\n\n\tget_total_entries(buf, &total, &entries);\n\n\tseq_printf(m, \"# %s latency trace v1.1.5 on %s\\n\",\n\t\t   name, UTS_RELEASE);\n\tseq_puts(m, \"# -----------------------------------\"\n\t\t \"---------------------------------\\n\");\n\tseq_printf(m, \"# latency: %lu us, #%lu/%lu, CPU#%d |\"\n\t\t   \" (M:%s VP:%d, KP:%d, SP:%d HP:%d\",\n\t\t   nsecs_to_usecs(data->saved_latency),\n\t\t   entries,\n\t\t   total,\n\t\t   buf->cpu,\n#if defined(CONFIG_PREEMPT_NONE)\n\t\t   \"server\",\n#elif defined(CONFIG_PREEMPT_VOLUNTARY)\n\t\t   \"desktop\",\n#elif defined(CONFIG_PREEMPT)\n\t\t   \"preempt\",\n#else\n\t\t   \"unknown\",\n#endif\n\t\t   /* These are reserved for later use */\n\t\t   0, 0, 0, 0);\n#ifdef CONFIG_SMP\n\tseq_printf(m, \" #P:%d)\\n\", num_online_cpus());\n#else\n\tseq_puts(m, \")\\n\");\n#endif\n\tseq_puts(m, \"#    -----------------\\n\");\n\tseq_printf(m, \"#    | task: %.16s-%d \"\n\t\t   \"(uid:%d nice:%ld policy:%ld rt_prio:%ld)\\n\",\n\t\t   data->comm, data->pid,\n\t\t   from_kuid_munged(seq_user_ns(m), data->uid), data->nice,\n\t\t   data->policy, data->rt_priority);\n\tseq_puts(m, \"#    -----------------\\n\");\n\n\tif (data->critical_start) {\n\t\tseq_puts(m, \"#  => started at: \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_start, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#  => ended at:   \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_end, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#\\n\");\n\t}\n\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void test_cpu_buff_start(struct trace_iterator *iter)\n{\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (!(tr->trace_flags & TRACE_ITER_ANNOTATE))\n\t\treturn;\n\n\tif (!(iter->iter_flags & TRACE_FILE_ANNOTATE))\n\t\treturn;\n\n\tif (cpumask_available(iter->started) &&\n\t    cpumask_test_cpu(iter->cpu, iter->started))\n\t\treturn;\n\n\tif (per_cpu_ptr(iter->trace_buffer->data, iter->cpu)->skipped_entries)\n\t\treturn;\n\n\tif (cpumask_available(iter->started))\n\t\tcpumask_set_cpu(iter->cpu, iter->started);\n\n\t/* Don't print started cpu buffer for the first entry of the trace */\n\tif (iter->idx > 1)\n\t\ttrace_seq_printf(s, \"##### CPU %u buffer started ####\\n\",\n\t\t\t\titer->cpu);\n}\n\nstatic enum print_line_t print_trace_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned long sym_flags = (tr->trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\ttest_cpu_buff_start(iter);\n\n\tevent = ftrace_find_event(entry->type);\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\t\ttrace_print_lat_context(iter);\n\t\telse\n\t\t\ttrace_print_context(iter);\n\t}\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tif (event)\n\t\treturn event->funcs->trace(iter, sym_flags, event);\n\n\ttrace_seq_printf(s, \"Unknown type %d\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_raw_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO)\n\t\ttrace_seq_printf(s, \"%d %d %llu \",\n\t\t\t\t entry->pid, iter->cpu, iter->ts);\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event)\n\t\treturn event->funcs->raw(iter, 0, event);\n\n\ttrace_seq_printf(s, \"%d ?\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_hex_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned char newline = '\\n';\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_HEX_FIELD(s, entry->pid);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event) {\n\t\tenum print_line_t ret = event->funcs->hex(iter, 0, event);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t}\n\n\tSEQ_PUT_FIELD(s, newline);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_bin_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_FIELD(s, entry->pid);\n\t\tSEQ_PUT_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\treturn event ? event->funcs->binary(iter, 0, event) :\n\t\tTRACE_TYPE_HANDLED;\n}\n\nint trace_empty(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter;\n\tint cpu;\n\n\t/* If we are looking at one CPU buffer, only check that one */\n\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\tcpu = iter->cpu_file;\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\n/*  Called with trace_event_read_lock() held. */\nenum print_line_t print_trace_line(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\tenum print_line_t ret;\n\n\tif (iter->lost_events) {\n\t\ttrace_seq_printf(&iter->seq, \"CPU:%d [LOST %lu EVENTS]\\n\",\n\t\t\t\t iter->cpu, iter->lost_events);\n\t\tif (trace_seq_has_overflowed(&iter->seq))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tif (iter->trace && iter->trace->print_line) {\n\t\tret = iter->trace->print_line(iter);\n\t\tif (ret != TRACE_TYPE_UNHANDLED)\n\t\t\treturn ret;\n\t}\n\n\tif (iter->ent->type == TRACE_BPUTS &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bputs_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_BPRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bprintk_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_PRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_printk_msg_only(iter);\n\n\tif (trace_flags & TRACE_ITER_BIN)\n\t\treturn print_bin_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_HEX)\n\t\treturn print_hex_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_RAW)\n\t\treturn print_raw_fmt(iter);\n\n\treturn print_trace_fmt(iter);\n}\n\nvoid trace_latency_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\n\t/* print nothing if the buffers are empty */\n\tif (trace_empty(iter))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\tprint_trace_header(m, iter);\n\n\tif (!(tr->trace_flags & TRACE_ITER_VERBOSE))\n\t\tprint_lat_help_header(m);\n}\n\nvoid trace_default_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\n\tif (!(trace_flags & TRACE_ITER_CONTEXT_INFO))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT) {\n\t\t/* print nothing if the buffers are empty */\n\t\tif (trace_empty(iter))\n\t\t\treturn;\n\t\tprint_trace_header(m, iter);\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE))\n\t\t\tprint_lat_help_header(m);\n\t} else {\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE)) {\n\t\t\tif (trace_flags & TRACE_ITER_IRQ_INFO)\n\t\t\t\tprint_func_help_header_irq(iter->trace_buffer,\n\t\t\t\t\t\t\t   m, trace_flags);\n\t\t\telse\n\t\t\t\tprint_func_help_header(iter->trace_buffer, m,\n\t\t\t\t\t\t       trace_flags);\n\t\t}\n\t}\n}\n\nstatic void test_ftrace_alive(struct seq_file *m)\n{\n\tif (!ftrace_is_dead())\n\t\treturn;\n\tseq_puts(m, \"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\"\n\t\t    \"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\nstatic void show_snapshot_main_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Clears and frees snapshot buffer\\n\"\n\t\t    \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer.\\n\"\n\t\t    \"# echo 2 > snapshot : Clears snapshot buffer (but does not allocate or free)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void show_snapshot_percpu_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Invalid for per_cpu snapshot file.\\n\");\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n\tseq_puts(m, \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer for this cpu.\\n\");\n#else\n\tseq_puts(m, \"# echo 1 > snapshot : Not supported with this kernel.\\n\"\n\t\t    \"#                     Must use main snapshot file to allocate.\\n\");\n#endif\n\tseq_puts(m, \"# echo 2 > snapshot : Clears this cpu's snapshot buffer (but does not allocate)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter)\n{\n\tif (iter->tr->allocated_snapshot)\n\t\tseq_puts(m, \"#\\n# * Snapshot is allocated *\\n#\\n\");\n\telse\n\t\tseq_puts(m, \"#\\n# * Snapshot is freed *\\n#\\n\");\n\n\tseq_puts(m, \"# Snapshot commands:\\n\");\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\tshow_snapshot_main_help(m);\n\telse\n\t\tshow_snapshot_percpu_help(m);\n}\n#else\n/* Should never be called */\nstatic inline void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter) { }\n#endif\n\nstatic int s_show(struct seq_file *m, void *v)\n{\n\tstruct trace_iterator *iter = v;\n\tint ret;\n\n\tif (iter->ent == NULL) {\n\t\tif (iter->tr) {\n\t\t\tseq_printf(m, \"# tracer: %s\\n\", iter->trace->name);\n\t\t\tseq_puts(m, \"#\\n\");\n\t\t\ttest_ftrace_alive(m);\n\t\t}\n\t\tif (iter->snapshot && trace_empty(iter))\n\t\t\tprint_snapshot_help(m, iter);\n\t\telse if (iter->trace && iter->trace->print_header)\n\t\t\titer->trace->print_header(m);\n\t\telse\n\t\t\ttrace_default_header(m);\n\n\t} else if (iter->leftover) {\n\t\t/*\n\t\t * If we filled the seq_file buffer earlier, we\n\t\t * want to just show it now.\n\t\t */\n\t\tret = trace_print_seq(m, &iter->seq);\n\n\t\t/* ret should this time be zero, but you never know */\n\t\titer->leftover = ret;\n\n\t} else {\n\t\tprint_trace_line(iter);\n\t\tret = trace_print_seq(m, &iter->seq);\n\t\t/*\n\t\t * If we overflow the seq_file buffer, then it will\n\t\t * ask us for this data again at start up.\n\t\t * Use that instead.\n\t\t *  ret is 0 if seq_file write succeeded.\n\t\t *        -1 otherwise.\n\t\t */\n\t\titer->leftover = ret;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Should be used after trace_array_get(), trace_types_lock\n * ensures that i_cdev was already initialized.\n */\nstatic inline int tracing_get_cpu(struct inode *inode)\n{\n\tif (inode->i_cdev) /* See trace_create_cpu_file() */\n\t\treturn (long)inode->i_cdev - 1;\n\treturn RING_BUFFER_ALL_CPUS;\n}\n\nstatic const struct seq_operations tracer_seq_ops = {\n\t.start\t\t= s_start,\n\t.next\t\t= s_next,\n\t.stop\t\t= s_stop,\n\t.show\t\t= s_show,\n};\n\nstatic struct trace_iterator *\n__tracing_open(struct inode *inode, struct file *file, bool snapshot)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (tracing_disabled)\n\t\treturn ERR_PTR(-ENODEV);\n\n\titer = __seq_open_private(file, &tracer_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\titer->buffer_iter = kcalloc(nr_cpu_ids, sizeof(*iter->buffer_iter),\n\t\t\t\t    GFP_KERNEL);\n\tif (!iter->buffer_iter)\n\t\tgoto release;\n\n\t/*\n\t * We make a copy of the current tracer to avoid concurrent\n\t * changes on it while we are reading.\n\t */\n\tmutex_lock(&trace_types_lock);\n\titer->trace = kzalloc(sizeof(*iter->trace), GFP_KERNEL);\n\tif (!iter->trace)\n\t\tgoto fail;\n\n\t*iter->trace = *tr->current_trace;\n\n\tif (!zalloc_cpumask_var(&iter->started, GFP_KERNEL))\n\t\tgoto fail;\n\n\titer->tr = tr;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t/* Currently only the top directory has a snapshot */\n\tif (tr->current_trace->print_max || snapshot)\n\t\titer->trace_buffer = &tr->max_buffer;\n\telse\n#endif\n\t\titer->trace_buffer = &tr->trace_buffer;\n\titer->snapshot = snapshot;\n\titer->pos = -1;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\n\t/* Notify the tracer early; before we stop tracing. */\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t/* Annotate start of buffers if we had overruns */\n\tif (ring_buffer_overruns(iter->trace_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\t/* stop the trace while dumping if we are not opening \"snapshot\" */\n\tif (!iter->snapshot)\n\t\ttracing_stop_tr(tr);\n\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\titer->buffer_iter[cpu] =\n\t\t\t\tring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);\n\t\t}\n\t\tring_buffer_read_prepare_sync();\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\t\ttracing_iter_reset(iter, cpu);\n\t\t}\n\t} else {\n\t\tcpu = iter->cpu_file;\n\t\titer->buffer_iter[cpu] =\n\t\t\tring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);\n\t\tring_buffer_read_prepare_sync();\n\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\ttracing_iter_reset(iter, cpu);\n\t}\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn iter;\n\n fail:\n\tmutex_unlock(&trace_types_lock);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\nrelease:\n\tseq_release_private(inode, file);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nint tracing_open_generic(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tfilp->private_data = inode->i_private;\n\treturn 0;\n}\n\nbool tracing_is_disabled(void)\n{\n\treturn (tracing_disabled) ? true: false;\n}\n\n/*\n * Open and update trace_array ref count.\n * Must have the current trace_array passed to it.\n */\nstatic int tracing_open_generic_tr(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tfilp->private_data = inode->i_private;\n\n\treturn 0;\n}\n\nstatic int tracing_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m = file->private_data;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (!(file->f_mode & FMODE_READ)) {\n\t\ttrace_array_put(tr);\n\t\treturn 0;\n\t}\n\n\t/* Writes do not use seq_file */\n\titer = m->private;\n\tmutex_lock(&trace_types_lock);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tif (iter->buffer_iter[cpu])\n\t\t\tring_buffer_read_finish(iter->buffer_iter[cpu]);\n\t}\n\n\tif (iter->trace && iter->trace->close)\n\t\titer->trace->close(iter);\n\n\tif (!iter->snapshot)\n\t\t/* reenable tracing if it was previously enabled */\n\t\ttracing_start_tr(tr);\n\n\t__trace_array_put(tr);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tmutex_destroy(&iter->mutex);\n\tfree_cpumask_var(iter->started);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\n\tseq_release_private(inode, file);\n\n\treturn 0;\n}\n\nstatic int tracing_release_generic_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\treturn 0;\n}\n\nstatic int tracing_single_release_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\n\treturn single_release(inode, file);\n}\n\nstatic int tracing_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret = 0;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\t/* If this file was open for write, then erase contents */\n\tif ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC)) {\n\t\tint cpu = tracing_get_cpu(inode);\n\t\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tif (tr->current_trace->print_max)\n\t\t\ttrace_buf = &tr->max_buffer;\n#endif\n\n\t\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\t\ttracing_reset_online_cpus(trace_buf);\n\t\telse\n\t\t\ttracing_reset(trace_buf, cpu);\n\t}\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, false);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t\telse if (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\t}\n\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\n/*\n * Some tracers are not suitable for instance buffers.\n * A tracer is always available for the global array (toplevel)\n * or if it explicitly states that it is.\n */\nstatic bool\ntrace_ok_for_array(struct tracer *t, struct trace_array *tr)\n{\n\treturn (tr->flags & TRACE_ARRAY_FL_GLOBAL) || t->allow_instances;\n}\n\n/* Find the next tracer that this trace array may use */\nstatic struct tracer *\nget_tracer_for_array(struct trace_array *tr, struct tracer *t)\n{\n\twhile (t && !trace_ok_for_array(t, tr))\n\t\tt = t->next;\n\n\treturn t;\n}\n\nstatic void *\nt_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t = v;\n\n\t(*pos)++;\n\n\tif (t)\n\t\tt = get_tracer_for_array(tr, t->next);\n\n\treturn t;\n}\n\nstatic void *t_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tt = get_tracer_for_array(tr, trace_types);\n\tfor (; t && l < *pos; t = t_next(m, t, &l))\n\t\t\t;\n\n\treturn t;\n}\n\nstatic void t_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int t_show(struct seq_file *m, void *v)\n{\n\tstruct tracer *t = v;\n\n\tif (!t)\n\t\treturn 0;\n\n\tseq_puts(m, t->name);\n\tif (t->next)\n\t\tseq_putc(m, ' ');\n\telse\n\t\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nstatic const struct seq_operations show_traces_seq_ops = {\n\t.start\t\t= t_start,\n\t.next\t\t= t_next,\n\t.stop\t\t= t_stop,\n\t.show\t\t= t_show,\n};\n\nstatic int show_traces_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tret = seq_open(file, &show_traces_seq_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tm = file->private_data;\n\tm->private = tr;\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_write_stub(struct file *filp, const char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\treturn count;\n}\n\nloff_t tracing_lseek(struct file *file, loff_t offset, int whence)\n{\n\tint ret;\n\n\tif (file->f_mode & FMODE_READ)\n\t\tret = seq_lseek(file, offset, whence);\n\telse\n\t\tfile->f_pos = ret = 0;\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_fops = {\n\t.open\t\t= tracing_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_write_stub,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_release,\n};\n\nstatic const struct file_operations show_traces_fops = {\n\t.open\t\t= show_traces_open,\n\t.read\t\t= seq_read,\n\t.release\t= seq_release,\n\t.llseek\t\t= seq_lseek,\n};\n\nstatic ssize_t\ntracing_cpumask_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tchar *mask_str;\n\tint len;\n\n\tlen = snprintf(NULL, 0, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask)) + 1;\n\tmask_str = kmalloc(len, GFP_KERNEL);\n\tif (!mask_str)\n\t\treturn -ENOMEM;\n\n\tlen = snprintf(mask_str, len, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask));\n\tif (len >= count) {\n\t\tcount = -EINVAL;\n\t\tgoto out_err;\n\t}\n\tcount = simple_read_from_buffer(ubuf, count, ppos, mask_str, len);\n\nout_err:\n\tkfree(mask_str);\n\n\treturn count;\n}\n\nstatic ssize_t\ntracing_cpumask_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tcpumask_var_t tracing_cpumask_new;\n\tint err, cpu;\n\n\tif (!alloc_cpumask_var(&tracing_cpumask_new, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\terr = cpumask_parse_user(ubuf, count, tracing_cpumask_new);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\t/*\n\t\t * Increase/decrease the disabled counter if we are\n\t\t * about to flip a bit in the cpumask:\n\t\t */\n\t\tif (cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\t!cpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_inc(&per_cpu_ptr(tr->trace_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_disable_cpu(tr->trace_buffer.buffer, cpu);\n\t\t}\n\t\tif (!cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\tcpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_dec(&per_cpu_ptr(tr->trace_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_enable_cpu(tr->trace_buffer.buffer, cpu);\n\t\t}\n\t}\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\n\tcpumask_copy(tr->tracing_cpumask, tracing_cpumask_new);\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn count;\n\nerr_unlock:\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn err;\n}\n\nstatic const struct file_operations tracing_cpumask_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_cpumask_read,\n\t.write\t\t= tracing_cpumask_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic int tracing_trace_options_show(struct seq_file *m, void *v)\n{\n\tstruct tracer_opt *trace_opts;\n\tstruct trace_array *tr = m->private;\n\tu32 tracer_flags;\n\tint i;\n\n\tmutex_lock(&trace_types_lock);\n\ttracer_flags = tr->current_trace->flags->val;\n\ttrace_opts = tr->current_trace->flags->opts;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (tr->trace_flags & (1 << i))\n\t\t\tseq_printf(m, \"%s\\n\", trace_options[i]);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_options[i]);\n\t}\n\n\tfor (i = 0; trace_opts[i].name; i++) {\n\t\tif (tracer_flags & trace_opts[i].bit)\n\t\t\tseq_printf(m, \"%s\\n\", trace_opts[i].name);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_opts[i].name);\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int __set_tracer_option(struct trace_array *tr,\n\t\t\t       struct tracer_flags *tracer_flags,\n\t\t\t       struct tracer_opt *opts, int neg)\n{\n\tstruct tracer *trace = tracer_flags->trace;\n\tint ret;\n\n\tret = trace->set_flag(tr, tracer_flags->val, opts->bit, !neg);\n\tif (ret)\n\t\treturn ret;\n\n\tif (neg)\n\t\ttracer_flags->val &= ~opts->bit;\n\telse\n\t\ttracer_flags->val |= opts->bit;\n\treturn 0;\n}\n\n/* Try to assign a tracer specific option */\nstatic int set_tracer_option(struct trace_array *tr, char *cmp, int neg)\n{\n\tstruct tracer *trace = tr->current_trace;\n\tstruct tracer_flags *tracer_flags = trace->flags;\n\tstruct tracer_opt *opts = NULL;\n\tint i;\n\n\tfor (i = 0; tracer_flags->opts[i].name; i++) {\n\t\topts = &tracer_flags->opts[i];\n\n\t\tif (strcmp(cmp, opts->name) == 0)\n\t\t\treturn __set_tracer_option(tr, trace->flags, opts, neg);\n\t}\n\n\treturn -EINVAL;\n}\n\n/* Some tracers require overwrite to stay enabled */\nint trace_keep_overwrite(struct tracer *tracer, u32 mask, int set)\n{\n\tif (tracer->enabled && (mask & TRACE_ITER_OVERWRITE) && !set)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nint set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled)\n{\n\t/* do nothing if flag is already set */\n\tif (!!(tr->trace_flags & mask) == !!enabled)\n\t\treturn 0;\n\n\t/* Give the tracer a chance to approve the change */\n\tif (tr->current_trace->flag_changed)\n\t\tif (tr->current_trace->flag_changed(tr, mask, !!enabled))\n\t\t\treturn -EINVAL;\n\n\tif (enabled)\n\t\ttr->trace_flags |= mask;\n\telse\n\t\ttr->trace_flags &= ~mask;\n\n\tif (mask == TRACE_ITER_RECORD_CMD)\n\t\ttrace_event_enable_cmd_record(enabled);\n\n\tif (mask == TRACE_ITER_RECORD_TGID) {\n\t\tif (!tgid_map)\n\t\t\ttgid_map = kcalloc(PID_MAX_DEFAULT + 1,\n\t\t\t\t\t   sizeof(*tgid_map),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!tgid_map) {\n\t\t\ttr->trace_flags &= ~TRACE_ITER_RECORD_TGID;\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttrace_event_enable_tgid_record(enabled);\n\t}\n\n\tif (mask == TRACE_ITER_EVENT_FORK)\n\t\ttrace_event_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_FUNC_FORK)\n\t\tftrace_pid_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_OVERWRITE) {\n\t\tring_buffer_change_overwrite(tr->trace_buffer.buffer, enabled);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tring_buffer_change_overwrite(tr->max_buffer.buffer, enabled);\n#endif\n\t}\n\n\tif (mask == TRACE_ITER_PRINTK) {\n\t\ttrace_printk_start_stop_comm(enabled);\n\t\ttrace_printk_control(enabled);\n\t}\n\n\treturn 0;\n}\n\nstatic int trace_set_options(struct trace_array *tr, char *option)\n{\n\tchar *cmp;\n\tint neg = 0;\n\tint ret;\n\tsize_t orig_len = strlen(option);\n\n\tcmp = strstrip(option);\n\n\tif (strncmp(cmp, \"no\", 2) == 0) {\n\t\tneg = 1;\n\t\tcmp += 2;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\tret = match_string(trace_options, -1, cmp);\n\t/* If no option could be set, test the specific tracer options */\n\tif (ret < 0)\n\t\tret = set_tracer_option(tr, cmp, neg);\n\telse\n\t\tret = set_tracer_flag(tr, 1 << ret, !neg);\n\n\tmutex_unlock(&trace_types_lock);\n\n\t/*\n\t * If the first trailing whitespace is replaced with '\\0' by strstrip,\n\t * turn it back into a space.\n\t */\n\tif (orig_len > strlen(option))\n\t\toption[strlen(option)] = ' ';\n\n\treturn ret;\n}\n\nstatic void __init apply_trace_boot_options(void)\n{\n\tchar *buf = trace_boot_options_buf;\n\tchar *option;\n\n\twhile (true) {\n\t\toption = strsep(&buf, \",\");\n\n\t\tif (!option)\n\t\t\tbreak;\n\n\t\tif (*option)\n\t\t\ttrace_set_options(&global_trace, option);\n\n\t\t/* Put back the comma to allow this to be called again */\n\t\tif (buf)\n\t\t\t*(buf - 1) = ',';\n\t}\n}\n\nstatic ssize_t\ntracing_trace_options_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tret = trace_set_options(tr, buf);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_trace_options_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_trace_options_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_iter_fops = {\n\t.open\t\t= tracing_trace_options_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_trace_options_write,\n};\n\nstatic const char readme_msg[] =\n\t\"tracing mini-HOWTO:\\n\\n\"\n\t\"# echo 0 > tracing_on : quick way to disable tracing\\n\"\n\t\"# echo 1 > tracing_on : quick way to re-enable tracing\\n\\n\"\n\t\" Important files:\\n\"\n\t\"  trace\\t\\t\\t- The static contents of the buffer\\n\"\n\t\"\\t\\t\\t  To clear the buffer write into this file: echo > trace\\n\"\n\t\"  trace_pipe\\t\\t- A consuming read to see the contents of the buffer\\n\"\n\t\"  current_tracer\\t- function and latency tracers\\n\"\n\t\"  available_tracers\\t- list of configured tracers for current_tracer\\n\"\n\t\"  buffer_size_kb\\t- view and modify size of per cpu buffer\\n\"\n\t\"  buffer_total_size_kb  - view total size of all cpu buffers\\n\\n\"\n\t\"  trace_clock\\t\\t-change the clock used to order events\\n\"\n\t\"       local:   Per cpu clock but may not be synced across CPUs\\n\"\n\t\"      global:   Synced across CPUs but slows tracing down.\\n\"\n\t\"     counter:   Not a clock, but just an increment\\n\"\n\t\"      uptime:   Jiffy counter from time of boot\\n\"\n\t\"        perf:   Same clock that perf events use\\n\"\n#ifdef CONFIG_X86_64\n\t\"     x86-tsc:   TSC cycle counter\\n\"\n#endif\n\t\"\\n  timestamp_mode\\t-view the mode used to timestamp events\\n\"\n\t\"       delta:   Delta difference against a buffer-wide timestamp\\n\"\n\t\"    absolute:   Absolute (standalone) timestamp\\n\"\n\t\"\\n  trace_marker\\t\\t- Writes into this file writes into the kernel buffer\\n\"\n\t\"\\n  trace_marker_raw\\t\\t- Writes into this file writes binary data into the kernel buffer\\n\"\n\t\"  tracing_cpumask\\t- Limit which CPUs to trace\\n\"\n\t\"  instances\\t\\t- Make sub-buffers with: mkdir instances/foo\\n\"\n\t\"\\t\\t\\t  Remove sub-buffer with rmdir\\n\"\n\t\"  trace_options\\t\\t- Set format or modify how tracing happens\\n\"\n\t\"\\t\\t\\t  Disable an option by adding a suffix 'no' to the\\n\"\n\t\"\\t\\t\\t  option name\\n\"\n\t\"  saved_cmdlines_size\\t- echo command number in here to store comm-pid list\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"\\n  available_filter_functions - list of functions that can be filtered on\\n\"\n\t\"  set_ftrace_filter\\t- echo function name in here to only trace these\\n\"\n\t\"\\t\\t\\t  functions\\n\"\n\t\"\\t     accepts: func_full_name or glob-matching-pattern\\n\"\n\t\"\\t     modules: Can select a group via module\\n\"\n\t\"\\t      Format: :mod:<module-name>\\n\"\n\t\"\\t     example: echo :mod:ext3 > set_ftrace_filter\\n\"\n\t\"\\t    triggers: a command to perform when function is hit\\n\"\n\t\"\\t      Format: <function>:<trigger>[:count]\\n\"\n\t\"\\t     trigger: traceon, traceoff\\n\"\n\t\"\\t\\t      enable_event:<system>:<event>\\n\"\n\t\"\\t\\t      disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t      stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t      snapshot\\n\"\n#endif\n\t\"\\t\\t      dump\\n\"\n\t\"\\t\\t      cpudump\\n\"\n\t\"\\t     example: echo do_fault:traceoff > set_ftrace_filter\\n\"\n\t\"\\t              echo do_trap:traceoff:3 > set_ftrace_filter\\n\"\n\t\"\\t     The first one will disable tracing every time do_fault is hit\\n\"\n\t\"\\t     The second will disable tracing at most 3 times when do_trap is hit\\n\"\n\t\"\\t       The first time do trap is hit and it disables tracing, the\\n\"\n\t\"\\t       counter will decrement to 2. If tracing is already disabled,\\n\"\n\t\"\\t       the counter will not decrement. It only decrements when the\\n\"\n\t\"\\t       trigger did work\\n\"\n\t\"\\t     To remove trigger without count:\\n\"\n\t\"\\t       echo '!<function>:<trigger> > set_ftrace_filter\\n\"\n\t\"\\t     To remove trigger with a count:\\n\"\n\t\"\\t       echo '!<function>:<trigger>:0 > set_ftrace_filter\\n\"\n\t\"  set_ftrace_notrace\\t- echo function name in here to never trace.\\n\"\n\t\"\\t    accepts: func_full_name, *func_end, func_begin*, *func_middle*\\n\"\n\t\"\\t    modules: Can select a group via module command :mod:\\n\"\n\t\"\\t    Does not accept triggers\\n\"\n#endif /* CONFIG_DYNAMIC_FTRACE */\n#ifdef CONFIG_FUNCTION_TRACER\n\t\"  set_ftrace_pid\\t- Write pid(s) to only function trace those pids\\n\"\n\t\"\\t\\t    (function)\\n\"\n#endif\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\"  set_graph_function\\t- Trace the nested calls of a function (function_graph)\\n\"\n\t\"  set_graph_notrace\\t- Do not trace the nested calls of a function (function_graph)\\n\"\n\t\"  max_graph_depth\\t- Trace a limited depth of nested calls (0 is unlimited)\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\n  snapshot\\t\\t- Like 'trace' but shows the content of the static\\n\"\n\t\"\\t\\t\\t  snapshot buffer. Read the contents for more\\n\"\n\t\"\\t\\t\\t  information\\n\"\n#endif\n#ifdef CONFIG_STACK_TRACER\n\t\"  stack_trace\\t\\t- Shows the max stack trace when active\\n\"\n\t\"  stack_max_size\\t- Shows current max stack size that was traced\\n\"\n\t\"\\t\\t\\t  Write into this file to reset the max size (trigger a\\n\"\n\t\"\\t\\t\\t  new trace)\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"  stack_trace_filter\\t- Like set_ftrace_filter but limits what stack_trace\\n\"\n\t\"\\t\\t\\t  traces\\n\"\n#endif\n#endif /* CONFIG_STACK_TRACER */\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"  kprobe_events\\t\\t- Add/remove/show the kernel dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\t\"  uprobe_events\\t\\t- Add/remove/show the userspace dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS)\n\t\"\\t  accepts: event-definitions (one definition per line)\\n\"\n\t\"\\t   Format: p[:[<group>/]<event>] <place> [<args>]\\n\"\n\t\"\\t           r[maxactive][:[<group>/]<event>] <place> [<args>]\\n\"\n\t\"\\t           -:[<group>/]<event>\\n\"\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"\\t    place: [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n  \"place (kretprobe): [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\t\"\\t    place: <path>:<offset>\\n\"\n#endif\n\t\"\\t     args: <name>=fetcharg[:type]\\n\"\n\t\"\\t fetcharg: %<register>, @<address>, @<symbol>[+|-<offset>],\\n\"\n\t\"\\t           $stack<index>, $stack, $retval, $comm\\n\"\n\t\"\\t     type: s8/16/32/64, u8/16/32/64, x8/16/32/64, string,\\n\"\n\t\"\\t           b<bit-width>@<bit-offset>/<container-size>\\n\"\n#endif\n\t\"  events/\\t\\t- Directory containing all trace event subsystems:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all events\\n\"\n\t\"  events/<system>/\\t- Directory containing all trace events for <system>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all <system>\\n\"\n\t\"\\t\\t\\t  events\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"  events/<system>/<event>/\\t- Directory containing control files for\\n\"\n\t\"\\t\\t\\t  <event>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of <event>\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"      trigger\\t\\t- If set, a command to perform when event is hit\\n\"\n\t\"\\t    Format: <trigger>[:count][if <filter>]\\n\"\n\t\"\\t   trigger: traceon, traceoff\\n\"\n\t\"\\t            enable_event:<system>:<event>\\n\"\n\t\"\\t            disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t            enable_hist:<system>:<event>\\n\"\n\t\"\\t            disable_hist:<system>:<event>\\n\"\n#endif\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t    stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t    snapshot\\n\"\n#endif\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t\\t    hist (see below)\\n\"\n#endif\n\t\"\\t   example: echo traceoff > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo traceoff:3 > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo 'enable_event:kmem:kmalloc:3 if nr_rq > 1' > \\\\\\n\"\n\t\"\\t                  events/block/block_unplug/trigger\\n\"\n\t\"\\t   The first disables tracing every time block_unplug is hit.\\n\"\n\t\"\\t   The second disables tracing the first 3 times block_unplug is hit.\\n\"\n\t\"\\t   The third enables the kmalloc event the first 3 times block_unplug\\n\"\n\t\"\\t     is hit and has value of greater than 1 for the 'nr_rq' event field.\\n\"\n\t\"\\t   Like function triggers, the counter is only decremented if it\\n\"\n\t\"\\t    enabled or disabled tracing.\\n\"\n\t\"\\t   To remove a trigger without a count:\\n\"\n\t\"\\t     echo '!<trigger> > <system>/<event>/trigger\\n\"\n\t\"\\t   To remove a trigger with a count:\\n\"\n\t\"\\t     echo '!<trigger>:0 > <system>/<event>/trigger\\n\"\n\t\"\\t   Filters can be ignored when removing a trigger.\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"      hist trigger\\t- If set, event hits are aggregated into a hash table\\n\"\n\t\"\\t    Format: hist:keys=<field1[,field2,...]>\\n\"\n\t\"\\t            [:values=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:sort=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:size=#entries]\\n\"\n\t\"\\t            [:pause][:continue][:clear]\\n\"\n\t\"\\t            [:name=histname1]\\n\"\n\t\"\\t            [if <filter>]\\n\\n\"\n\t\"\\t    When a matching event is hit, an entry is added to a hash\\n\"\n\t\"\\t    table using the key(s) and value(s) named, and the value of a\\n\"\n\t\"\\t    sum called 'hitcount' is incremented.  Keys and values\\n\"\n\t\"\\t    correspond to fields in the event's format description.  Keys\\n\"\n\t\"\\t    can be any field, or the special string 'stacktrace'.\\n\"\n\t\"\\t    Compound keys consisting of up to two fields can be specified\\n\"\n\t\"\\t    by the 'keys' keyword.  Values must correspond to numeric\\n\"\n\t\"\\t    fields.  Sort keys consisting of up to two fields can be\\n\"\n\t\"\\t    specified using the 'sort' keyword.  The sort direction can\\n\"\n\t\"\\t    be modified by appending '.descending' or '.ascending' to a\\n\"\n\t\"\\t    sort field.  The 'size' parameter can be used to specify more\\n\"\n\t\"\\t    or fewer than the default 2048 entries for the hashtable size.\\n\"\n\t\"\\t    If a hist trigger is given a name using the 'name' parameter,\\n\"\n\t\"\\t    its histogram data will be shared with other triggers of the\\n\"\n\t\"\\t    same name, and trigger hits will update this common data.\\n\\n\"\n\t\"\\t    Reading the 'hist' file for the event will dump the hash\\n\"\n\t\"\\t    table in its entirety to stdout.  If there are multiple hist\\n\"\n\t\"\\t    triggers attached to an event, there will be a table for each\\n\"\n\t\"\\t    trigger in the output.  The table displayed for a named\\n\"\n\t\"\\t    trigger will be the same as any other instance having the\\n\"\n\t\"\\t    same name.  The default format used to display a given field\\n\"\n\t\"\\t    can be modified by appending any of the following modifiers\\n\"\n\t\"\\t    to the field name, as applicable:\\n\\n\"\n\t\"\\t            .hex        display a number as a hex value\\n\"\n\t\"\\t            .sym        display an address as a symbol\\n\"\n\t\"\\t            .sym-offset display an address as a symbol and offset\\n\"\n\t\"\\t            .execname   display a common_pid as a program name\\n\"\n\t\"\\t            .syscall    display a syscall id as a syscall name\\n\"\n\t\"\\t            .log2       display log2 value rather than raw number\\n\"\n\t\"\\t            .usecs      display a common_timestamp in microseconds\\n\\n\"\n\t\"\\t    The 'pause' parameter can be used to pause an existing hist\\n\"\n\t\"\\t    trigger or to start a hist trigger but not log any events\\n\"\n\t\"\\t    until told to do so.  'continue' can be used to start or\\n\"\n\t\"\\t    restart a paused hist trigger.\\n\\n\"\n\t\"\\t    The 'clear' parameter will clear the contents of a running\\n\"\n\t\"\\t    hist trigger and leave its current paused/active state\\n\"\n\t\"\\t    unchanged.\\n\\n\"\n\t\"\\t    The enable_hist and disable_hist triggers can be used to\\n\"\n\t\"\\t    have one event conditionally start and stop another event's\\n\"\n\t\"\\t    already-attached hist trigger.  The syntax is analagous to\\n\"\n\t\"\\t    the enable_event and disable_event triggers.\\n\"\n#endif\n;\n\nstatic ssize_t\ntracing_readme_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\treturn simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t\treadme_msg, strlen(readme_msg));\n}\n\nstatic const struct file_operations tracing_readme_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_readme_read,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic void *saved_tgids_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tint *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr <= &tgid_map[PID_MAX_DEFAULT]; ptr++) {\n\t\tif (trace_find_tgid(*ptr))\n\t\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_tgids_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tif (!tgid_map)\n\t\treturn NULL;\n\n\tv = &tgid_map[0];\n\twhile (l <= *pos) {\n\t\tv = saved_tgids_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_tgids_stop(struct seq_file *m, void *v)\n{\n}\n\nstatic int saved_tgids_show(struct seq_file *m, void *v)\n{\n\tint pid = (int *)v - tgid_map;\n\n\tseq_printf(m, \"%d %d\\n\", pid, trace_find_tgid(pid));\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_tgids_seq_ops = {\n\t.start\t\t= saved_tgids_start,\n\t.stop\t\t= saved_tgids_stop,\n\t.next\t\t= saved_tgids_next,\n\t.show\t\t= saved_tgids_show,\n};\n\nstatic int tracing_saved_tgids_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_saved_tgids_seq_ops);\n}\n\n\nstatic const struct file_operations tracing_saved_tgids_fops = {\n\t.open\t\t= tracing_saved_tgids_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic void *saved_cmdlines_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunsigned int *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr < &savedcmd->map_cmdline_to_pid[savedcmd->cmdline_num];\n\t     ptr++) {\n\t\tif (*ptr == -1 || *ptr == NO_CMDLINE_MAP)\n\t\t\tcontinue;\n\n\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_cmdlines_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\tv = &savedcmd->map_cmdline_to_pid[0];\n\twhile (l <= *pos) {\n\t\tv = saved_cmdlines_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_cmdlines_stop(struct seq_file *m, void *v)\n{\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nstatic int saved_cmdlines_show(struct seq_file *m, void *v)\n{\n\tchar buf[TASK_COMM_LEN];\n\tunsigned int *pid = v;\n\n\t__trace_find_cmdline(*pid, buf);\n\tseq_printf(m, \"%d %s\\n\", *pid, buf);\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_cmdlines_seq_ops = {\n\t.start\t\t= saved_cmdlines_start,\n\t.next\t\t= saved_cmdlines_next,\n\t.stop\t\t= saved_cmdlines_stop,\n\t.show\t\t= saved_cmdlines_show,\n};\n\nstatic int tracing_saved_cmdlines_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_saved_cmdlines_seq_ops);\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_fops = {\n\t.open\t\t= tracing_saved_cmdlines_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic ssize_t\ntracing_saved_cmdlines_size_read(struct file *filp, char __user *ubuf,\n\t\t\t\t size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tr = scnprintf(buf, sizeof(buf), \"%u\\n\", savedcmd->cmdline_num);\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic void free_saved_cmdlines_buffer(struct saved_cmdlines_buffer *s)\n{\n\tkfree(s->saved_cmdlines);\n\tkfree(s->map_cmdline_to_pid);\n\tkfree(s);\n}\n\nstatic int tracing_resize_saved_cmdlines(unsigned int val)\n{\n\tstruct saved_cmdlines_buffer *s, *savedcmd_temp;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tif (allocate_cmdlines_buffer(val, s) < 0) {\n\t\tkfree(s);\n\t\treturn -ENOMEM;\n\t}\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tsavedcmd_temp = savedcmd;\n\tsavedcmd = s;\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tfree_saved_cmdlines_buffer(savedcmd_temp);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_saved_cmdlines_size_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t/* must have at least 1 entry or less than PID_MAX_DEFAULT */\n\tif (!val || val > PID_MAX_DEFAULT)\n\t\treturn -EINVAL;\n\n\tret = tracing_resize_saved_cmdlines((unsigned int)val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_size_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_saved_cmdlines_size_read,\n\t.write\t\t= tracing_saved_cmdlines_size_write,\n};\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic union trace_eval_map_item *\nupdate_eval_map(union trace_eval_map_item *ptr)\n{\n\tif (!ptr->map.eval_string) {\n\t\tif (ptr->tail.next) {\n\t\t\tptr = ptr->tail.next;\n\t\t\t/* Set ptr to the next real item (skip head) */\n\t\t\tptr++;\n\t\t} else\n\t\t\treturn NULL;\n\t}\n\treturn ptr;\n}\n\nstatic void *eval_map_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\t/*\n\t * Paranoid! If ptr points to end, we don't want to increment past it.\n\t * This really should never happen.\n\t */\n\tptr = update_eval_map(ptr);\n\tif (WARN_ON_ONCE(!ptr))\n\t\treturn NULL;\n\n\tptr++;\n\n\t(*pos)++;\n\n\tptr = update_eval_map(ptr);\n\n\treturn ptr;\n}\n\nstatic void *eval_map_start(struct seq_file *m, loff_t *pos)\n{\n\tunion trace_eval_map_item *v;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tv = trace_eval_maps;\n\tif (v)\n\t\tv++;\n\n\twhile (v && l < *pos) {\n\t\tv = eval_map_next(m, v, &l);\n\t}\n\n\treturn v;\n}\n\nstatic void eval_map_stop(struct seq_file *m, void *v)\n{\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic int eval_map_show(struct seq_file *m, void *v)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\tseq_printf(m, \"%s %ld (%s)\\n\",\n\t\t   ptr->map.eval_string, ptr->map.eval_value,\n\t\t   ptr->map.system);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_eval_map_seq_ops = {\n\t.start\t\t= eval_map_start,\n\t.next\t\t= eval_map_next,\n\t.stop\t\t= eval_map_stop,\n\t.show\t\t= eval_map_show,\n};\n\nstatic int tracing_eval_map_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_eval_map_seq_ops);\n}\n\nstatic const struct file_operations tracing_eval_map_fops = {\n\t.open\t\t= tracing_eval_map_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic inline union trace_eval_map_item *\ntrace_eval_jmp_to_tail(union trace_eval_map_item *ptr)\n{\n\t/* Return tail of array given the head */\n\treturn ptr + ptr->head.length + 1;\n}\n\nstatic void\ntrace_insert_eval_map_file(struct module *mod, struct trace_eval_map **start,\n\t\t\t   int len)\n{\n\tstruct trace_eval_map **stop;\n\tstruct trace_eval_map **map;\n\tunion trace_eval_map_item *map_array;\n\tunion trace_eval_map_item *ptr;\n\n\tstop = start + len;\n\n\t/*\n\t * The trace_eval_maps contains the map plus a head and tail item,\n\t * where the head holds the module and length of array, and the\n\t * tail holds a pointer to the next list.\n\t */\n\tmap_array = kmalloc_array(len + 2, sizeof(*map_array), GFP_KERNEL);\n\tif (!map_array) {\n\t\tpr_warn(\"Unable to allocate trace eval mapping\\n\");\n\t\treturn;\n\t}\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tif (!trace_eval_maps)\n\t\ttrace_eval_maps = map_array;\n\telse {\n\t\tptr = trace_eval_maps;\n\t\tfor (;;) {\n\t\t\tptr = trace_eval_jmp_to_tail(ptr);\n\t\t\tif (!ptr->tail.next)\n\t\t\t\tbreak;\n\t\t\tptr = ptr->tail.next;\n\n\t\t}\n\t\tptr->tail.next = map_array;\n\t}\n\tmap_array->head.mod = mod;\n\tmap_array->head.length = len;\n\tmap_array++;\n\n\tfor (map = start; (unsigned long)map < (unsigned long)stop; map++) {\n\t\tmap_array->map = **map;\n\t\tmap_array++;\n\t}\n\tmemset(map_array, 0, sizeof(*map_array));\n\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic void trace_create_eval_file(struct dentry *d_tracer)\n{\n\ttrace_create_file(\"eval_map\", 0444, d_tracer,\n\t\t\t  NULL, &tracing_eval_map_fops);\n}\n\n#else /* CONFIG_TRACE_EVAL_MAP_FILE */\nstatic inline void trace_create_eval_file(struct dentry *d_tracer) { }\nstatic inline void trace_insert_eval_map_file(struct module *mod,\n\t\t\t      struct trace_eval_map **start, int len) { }\n#endif /* !CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic void trace_insert_eval_map(struct module *mod,\n\t\t\t\t  struct trace_eval_map **start, int len)\n{\n\tstruct trace_eval_map **map;\n\n\tif (len <= 0)\n\t\treturn;\n\n\tmap = start;\n\n\ttrace_event_eval_update(map, len);\n\n\ttrace_insert_eval_map_file(mod, start, len);\n}\n\nstatic ssize_t\ntracing_set_trace_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+2];\n\tint r;\n\n\tmutex_lock(&trace_types_lock);\n\tr = sprintf(buf, \"%s\\n\", tr->current_trace->name);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nint tracer_init(struct tracer *t, struct trace_array *tr)\n{\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\treturn t->init(tr);\n}\n\nstatic void set_buffer_entries(struct trace_buffer *buf, unsigned long val)\n{\n\tint cpu;\n\n\tfor_each_tracing_cpu(cpu)\n\t\tper_cpu_ptr(buf->data, cpu)->entries = val;\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n/* resize @tr's buffer to the size of @size_tr's entries */\nstatic int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,\n\t\t\t\t\tstruct trace_buffer *size_buf, int cpu_id)\n{\n\tint cpu, ret = 0;\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu)->entries, cpu);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tper_cpu_ptr(trace_buf->data, cpu)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu)->entries;\n\t\t}\n\t} else {\n\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu_id)->entries, cpu_id);\n\t\tif (ret == 0)\n\t\t\tper_cpu_ptr(trace_buf->data, cpu_id)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu_id)->entries;\n\t}\n\n\treturn ret;\n}\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\nstatic int __tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\tunsigned long size, int cpu)\n{\n\tint ret;\n\n\t/*\n\t * If kernel or user changes the size of the ring buffer\n\t * we use the size that was given, and we can forget about\n\t * expanding it later.\n\t */\n\tring_buffer_expanded = true;\n\n\t/* May be called before buffers are initialized */\n\tif (!tr->trace_buffer.buffer)\n\t\treturn 0;\n\n\tret = ring_buffer_resize(tr->trace_buffer.buffer, size, cpu);\n\tif (ret < 0)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (!(tr->flags & TRACE_ARRAY_FL_GLOBAL) ||\n\t    !tr->current_trace->use_max_tr)\n\t\tgoto out;\n\n\tret = ring_buffer_resize(tr->max_buffer.buffer, size, cpu);\n\tif (ret < 0) {\n\t\tint r = resize_buffer_duplicate_size(&tr->trace_buffer,\n\t\t\t\t\t\t     &tr->trace_buffer, cpu);\n\t\tif (r < 0) {\n\t\t\t/*\n\t\t\t * AARGH! We are left with different\n\t\t\t * size max buffer!!!!\n\t\t\t * The max buffer is our \"snapshot\" buffer.\n\t\t\t * When a tracer needs a snapshot (one of the\n\t\t\t * latency tracers), it swaps the max buffer\n\t\t\t * with the saved snap shot. We succeeded to\n\t\t\t * update the size of the main buffer, but failed to\n\t\t\t * update the size of the max buffer. But when we tried\n\t\t\t * to reset the main buffer to the original size, we\n\t\t\t * failed there too. This is very unlikely to\n\t\t\t * happen, but if it does, warn and kill all\n\t\t\t * tracing.\n\t\t\t */\n\t\t\tWARN_ON(1);\n\t\t\ttracing_disabled = 1;\n\t\t}\n\t\treturn ret;\n\t}\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\tset_buffer_entries(&tr->max_buffer, size);\n\telse\n\t\tper_cpu_ptr(tr->max_buffer.data, cpu)->entries = size;\n\n out:\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\tset_buffer_entries(&tr->trace_buffer, size);\n\telse\n\t\tper_cpu_ptr(tr->trace_buffer.data, cpu)->entries = size;\n\n\treturn ret;\n}\n\nstatic ssize_t tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\t  unsigned long size, int cpu_id)\n{\n\tint ret = size;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu_id != RING_BUFFER_ALL_CPUS) {\n\t\t/* make sure, this cpu is enabled in the mask */\n\t\tif (!cpumask_test_cpu(cpu_id, tracing_buffer_mask)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = __tracing_resize_ring_buffer(tr, size, cpu_id);\n\tif (ret < 0)\n\t\tret = -ENOMEM;\n\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n\n/**\n * tracing_update_buffers - used by tracing facility to expand ring buffers\n *\n * To save on memory when the tracing is never used on a system with it\n * configured in. The ring buffers are set to a minimum size. But once\n * a user starts to use the tracing facility, then they need to grow\n * to their default size.\n *\n * This function is to be called when a tracer is about to be used.\n */\nint tracing_update_buffers(void)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tif (!ring_buffer_expanded)\n\t\tret = __tracing_resize_ring_buffer(&global_trace, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct trace_option_dentry;\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer);\n\n/*\n * Used to clear out the tracer before deletion of an instance.\n * Must have trace_types_lock held.\n */\nstatic void tracing_set_nop(struct trace_array *tr)\n{\n\tif (tr->current_trace == &nop_trace)\n\t\treturn;\n\t\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\ttr->current_trace = &nop_trace;\n}\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t)\n{\n\t/* Only enable if the directory has been created already. */\n\tif (!tr->dir)\n\t\treturn;\n\n\tcreate_trace_option_files(tr, t);\n}\n\nstatic int tracing_set_tracer(struct trace_array *tr, const char *buf)\n{\n\tstruct tracer *t;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbool had_max_tr;\n#endif\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (!ring_buffer_expanded) {\n\t\tret = __tracing_resize_ring_buffer(tr, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = 0;\n\t}\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(t->name, buf) == 0)\n\t\t\tbreak;\n\t}\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (t == tr->current_trace)\n\t\tgoto out;\n\n\t/* Some tracers won't work on kernel command line */\n\tif (system_state < SYSTEM_RUNNING && t->noboot) {\n\t\tpr_warn(\"Tracer '%s' is not allowed on command line, ignored\\n\",\n\t\t\tt->name);\n\t\tgoto out;\n\t}\n\n\t/* Some tracers are only allowed for the top level buffer */\n\tif (!trace_ok_for_array(t, tr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If trace pipe files are being read, we can't change the tracer */\n\tif (tr->current_trace->ref) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\ttrace_branch_disable();\n\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\t/* Current trace needs to be nop_trace before synchronize_sched */\n\ttr->current_trace = &nop_trace;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\thad_max_tr = tr->allocated_snapshot;\n\n\tif (had_max_tr && !t->use_max_tr) {\n\t\t/*\n\t\t * We need to make sure that the update_max_tr sees that\n\t\t * current_trace changed to nop_trace to keep it from\n\t\t * swapping the buffers after we resize it.\n\t\t * The update_max_tr is called from interrupts disabled\n\t\t * so a synchronized_sched() is sufficient.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfree_snapshot(tr);\n\t}\n#endif\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (t->use_max_tr && !had_max_tr) {\n\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\n\tif (t->init) {\n\t\tret = tracer_init(t, tr);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\ttr->current_trace = t;\n\ttr->current_trace->enabled++;\n\ttrace_branch_enable(tr);\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_set_trace_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+1];\n\tint i;\n\tsize_t ret;\n\tint err;\n\n\tret = cnt;\n\n\tif (cnt > MAX_TRACER_SIZE)\n\t\tcnt = MAX_TRACER_SIZE;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\t/* strip ending whitespace. */\n\tfor (i = cnt - 1; i > 0 && isspace(buf[i]); i--)\n\t\tbuf[i] = 0;\n\n\terr = tracing_set_tracer(tr, buf);\n\tif (err)\n\t\treturn err;\n\n\t*ppos += ret;\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_nsecs_read(unsigned long *ptr, char __user *ubuf,\n\t\t   size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tr = snprintf(buf, sizeof(buf), \"%ld\\n\",\n\t\t     *ptr == (unsigned long)-1 ? -1 : nsecs_to_usecs(*ptr));\n\tif (r > sizeof(buf))\n\t\tr = sizeof(buf);\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_nsecs_write(unsigned long *ptr, const char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t*ptr = val * 1000;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_thresh_read(struct file *filp, char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(&tracing_thresh, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_thresh_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tint ret;\n\n\tmutex_lock(&trace_types_lock);\n\tret = tracing_nsecs_write(&tracing_thresh, ubuf, cnt, ppos);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (tr->current_trace->update_thresh) {\n\t\tret = tr->current_trace->update_thresh(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = cnt;\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\n\nstatic ssize_t\ntracing_max_lat_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(filp->private_data, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_max_lat_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_write(filp->private_data, ubuf, cnt, ppos);\n}\n\n#endif\n\nstatic int tracing_open_pipe(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret = 0;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\n\t/* create a buffer to store the information to pass to userspace */\n\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter) {\n\t\tret = -ENOMEM;\n\t\t__trace_array_put(tr);\n\t\tgoto out;\n\t}\n\n\ttrace_seq_init(&iter->seq);\n\titer->trace = tr->current_trace;\n\n\tif (!alloc_cpumask_var(&iter->started, GFP_KERNEL)) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t/* trace pipe does not show start of buffer */\n\tcpumask_setall(iter->started);\n\n\tif (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\titer->tr = tr;\n\titer->trace_buffer = &tr->trace_buffer;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\tfilp->private_data = iter;\n\n\tif (iter->trace->pipe_open)\n\t\titer->trace->pipe_open(iter);\n\n\tnonseekable_open(inode, filp);\n\n\ttr->current_trace->ref++;\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n\nfail:\n\tkfree(iter->trace);\n\tkfree(iter);\n\t__trace_array_put(tr);\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_release_pipe(struct inode *inode, struct file *file)\n{\n\tstruct trace_iterator *iter = file->private_data;\n\tstruct trace_array *tr = inode->i_private;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->current_trace->ref--;\n\n\tif (iter->trace->pipe_close)\n\t\titer->trace->pipe_close(iter);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tfree_cpumask_var(iter->started);\n\tmutex_destroy(&iter->mutex);\n\tkfree(iter);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic __poll_t\ntrace_poll(struct trace_iterator *iter, struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_array *tr = iter->tr;\n\n\t/* Iterators are static, they should be filled or empty */\n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\n\tif (tr->trace_flags & TRACE_ITER_BLOCK)\n\t\t/*\n\t\t * Always select as readable when in blocking mode\n\t\t */\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\telse\n\t\treturn ring_buffer_poll_wait(iter->trace_buffer->buffer, iter->cpu_file,\n\t\t\t\t\t     filp, poll_table);\n}\n\nstatic __poll_t\ntracing_poll_pipe(struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\n/* Must be called with iter->mutex held. */\nstatic int tracing_wait_pipe(struct file *filp)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tint ret;\n\n\twhile (trace_empty(iter)) {\n\n\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t/*\n\t\t * We block until we read something and tracing is disabled.\n\t\t * We still block if tracing is disabled, but we have never\n\t\t * read anything. This allows a user to cat this file, and\n\t\t * then enable tracing. But after we have read something,\n\t\t * we give an EOF when tracing is again disabled.\n\t\t *\n\t\t * iter->pos will be 0 if we haven't read anything.\n\t\t */\n\t\tif (!tracer_tracing_is_on(iter->tr) && iter->pos)\n\t\t\tbreak;\n\n\t\tmutex_unlock(&iter->mutex);\n\n\t\tret = wait_on_pipe(iter, false);\n\n\t\tmutex_lock(&iter->mutex);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 1;\n}\n\n/*\n * Consumer reader.\n */\nstatic ssize_t\ntracing_read_pipe(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tssize_t sret;\n\n\t/*\n\t * Avoid more than one consumer on a single file descriptor\n\t * This is just a matter of traces coherency, the ring buffer itself\n\t * is protected.\n\t */\n\tmutex_lock(&iter->mutex);\n\n\t/* return any leftover data */\n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (sret != -EBUSY)\n\t\tgoto out;\n\n\ttrace_seq_init(&iter->seq);\n\n\tif (iter->trace->read) {\n\t\tsret = iter->trace->read(iter, filp, ubuf, cnt, ppos);\n\t\tif (sret)\n\t\t\tgoto out;\n\t}\n\nwaitagain:\n\tsret = tracing_wait_pipe(filp);\n\tif (sret <= 0)\n\t\tgoto out;\n\n\t/* stop when tracing is finished */\n\tif (trace_empty(iter)) {\n\t\tsret = 0;\n\t\tgoto out;\n\t}\n\n\tif (cnt >= PAGE_SIZE)\n\t\tcnt = PAGE_SIZE - 1;\n\n\t/* reset all but tr, trace, and overruns */\n\tmemset(&iter->seq, 0,\n\t       sizeof(struct trace_iterator) -\n\t       offsetof(struct trace_iterator, seq));\n\tcpumask_clear(iter->started);\n\titer->pos = -1;\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\twhile (trace_find_next_entry_inc(iter) != NULL) {\n\t\tenum print_line_t ret;\n\t\tint save_len = iter->seq.seq.len;\n\n\t\tret = print_trace_line(iter);\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\t/* don't print partial lines */\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\n\t\tif (trace_seq_used(&iter->seq) >= cnt)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Setting the full flag means we reached the trace_seq buffer\n\t\t * size and we should leave by partial output condition above.\n\t\t * One of the trace_seq_* functions is not used properly.\n\t\t */\n\t\tWARN_ONCE(iter->seq.full, \"full flag set for trace type %d\",\n\t\t\t  iter->ent->type);\n\t}\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\n\t/* Now copy what we have to the user */\n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (iter->seq.seq.readpos >= trace_seq_used(&iter->seq))\n\t\ttrace_seq_init(&iter->seq);\n\n\t/*\n\t * If there was nothing to send to user, in spite of consuming trace\n\t * entries, go back to wait for more entries.\n\t */\n\tif (sret == -EBUSY)\n\t\tgoto waitagain;\n\nout:\n\tmutex_unlock(&iter->mutex);\n\n\treturn sret;\n}\n\nstatic void tracing_spd_release_pipe(struct splice_pipe_desc *spd,\n\t\t\t\t     unsigned int idx)\n{\n\t__free_page(spd->pages[idx]);\n}\n\nstatic const struct pipe_buf_operations tracing_pipe_buf_ops = {\n\t.can_merge\t\t= 0,\n\t.confirm\t\t= generic_pipe_buf_confirm,\n\t.release\t\t= generic_pipe_buf_release,\n\t.steal\t\t\t= generic_pipe_buf_steal,\n\t.get\t\t\t= generic_pipe_buf_get,\n};\n\nstatic size_t\ntracing_fill_pipe_page(size_t rem, struct trace_iterator *iter)\n{\n\tsize_t count;\n\tint save_len;\n\tint ret;\n\n\t/* Seq buffer is page-sized, exactly what we need. */\n\tfor (;;) {\n\t\tsave_len = iter->seq.seq.len;\n\t\tret = print_trace_line(iter);\n\n\t\tif (trace_seq_has_overflowed(&iter->seq)) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * This should not be hit, because it should only\n\t\t * be set if the iter->seq overflowed. But check it\n\t\t * anyway to be safe.\n\t\t */\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tcount = trace_seq_used(&iter->seq) - save_len;\n\t\tif (rem < count) {\n\t\t\trem = 0;\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\t\trem -= count;\n\t\tif (!trace_find_next_entry_inc(iter))\t{\n\t\t\trem = 0;\n\t\t\titer->ent = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rem;\n}\n\nstatic ssize_t tracing_splice_read_pipe(struct file *filp,\n\t\t\t\t\tloff_t *ppos,\n\t\t\t\t\tstruct pipe_inode_info *pipe,\n\t\t\t\t\tsize_t len,\n\t\t\t\t\tunsigned int flags)\n{\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct trace_iterator *iter = filp->private_data;\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages\t= 0, /* This gets updated below. */\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &tracing_pipe_buf_ops,\n\t\t.spd_release\t= tracing_spd_release_pipe,\n\t};\n\tssize_t ret;\n\tsize_t rem;\n\tunsigned int i;\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&iter->mutex);\n\n\tif (iter->trace->splice_read) {\n\t\tret = iter->trace->splice_read(iter, filp,\n\t\t\t\t\t       ppos, pipe, len, flags);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t}\n\n\tret = tracing_wait_pipe(filp);\n\tif (ret <= 0)\n\t\tgoto out_err;\n\n\tif (!iter->ent && !trace_find_next_entry_inc(iter)) {\n\t\tret = -EFAULT;\n\t\tgoto out_err;\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\n\t/* Fill as many pages as possible. */\n\tfor (i = 0, rem = len; i < spd.nr_pages_max && rem; i++) {\n\t\tspd.pages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!spd.pages[i])\n\t\t\tbreak;\n\n\t\trem = tracing_fill_pipe_page(rem, iter);\n\n\t\t/* Copy the data into the page, so we can start over. */\n\t\tret = trace_seq_to_buffer(&iter->seq,\n\t\t\t\t\t  page_address(spd.pages[i]),\n\t\t\t\t\t  trace_seq_used(&iter->seq));\n\t\tif (ret < 0) {\n\t\t\t__free_page(spd.pages[i]);\n\t\t\tbreak;\n\t\t}\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].len = trace_seq_used(&iter->seq);\n\n\t\ttrace_seq_init(&iter->seq);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\tmutex_unlock(&iter->mutex);\n\n\tspd.nr_pages = i;\n\n\tif (i)\n\t\tret = splice_to_pipe(pipe, &spd);\n\telse\n\t\tret = 0;\nout:\n\tsplice_shrink_spd(&spd);\n\treturn ret;\n\nout_err:\n\tmutex_unlock(&iter->mutex);\n\tgoto out;\n}\n\nstatic ssize_t\ntracing_entries_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tint cpu = tracing_get_cpu(inode);\n\tchar buf[64];\n\tint r = 0;\n\tssize_t ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tint cpu, buf_size_same;\n\t\tunsigned long size;\n\n\t\tsize = 0;\n\t\tbuf_size_same = 1;\n\t\t/* check if all cpu sizes are same */\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\t/* fill in the size from first enabled cpu */\n\t\t\tif (size == 0)\n\t\t\t\tsize = per_cpu_ptr(tr->trace_buffer.data, cpu)->entries;\n\t\t\tif (size != per_cpu_ptr(tr->trace_buffer.data, cpu)->entries) {\n\t\t\t\tbuf_size_same = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (buf_size_same) {\n\t\t\tif (!ring_buffer_expanded)\n\t\t\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\",\n\t\t\t\t\t    size >> 10,\n\t\t\t\t\t    trace_buf_size >> 10);\n\t\t\telse\n\t\t\t\tr = sprintf(buf, \"%lu\\n\", size >> 10);\n\t\t} else\n\t\t\tr = sprintf(buf, \"X\\n\");\n\t} else\n\t\tr = sprintf(buf, \"%lu\\n\", per_cpu_ptr(tr->trace_buffer.data, cpu)->entries >> 10);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_entries_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t/* must have at least 1 entry */\n\tif (!val)\n\t\treturn -EINVAL;\n\n\t/* value is in KB */\n\tval <<= 10;\n\tret = tracing_resize_ring_buffer(tr, val, tracing_get_cpu(inode));\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_total_entries_read(struct file *filp, char __user *ubuf,\n\t\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r, cpu;\n\tunsigned long size = 0, expanded_size = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\tsize += per_cpu_ptr(tr->trace_buffer.data, cpu)->entries >> 10;\n\t\tif (!ring_buffer_expanded)\n\t\t\texpanded_size += trace_buf_size >> 10;\n\t}\n\tif (ring_buffer_expanded)\n\t\tr = sprintf(buf, \"%lu\\n\", size);\n\telse\n\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\", size, expanded_size);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_free_buffer_write(struct file *filp, const char __user *ubuf,\n\t\t\t  size_t cnt, loff_t *ppos)\n{\n\t/*\n\t * There is no need to read what the user has written, this function\n\t * is just to make sure that there is no error when \"echo\" is used\n\t */\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int\ntracing_free_buffer_release(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\t/* disable tracing ? */\n\tif (tr->trace_flags & TRACE_ITER_STOP_ON_FREE)\n\t\ttracer_tracing_off(tr);\n\t/* resize the ring buffer to 0 */\n\ttracing_resize_ring_buffer(tr, 0, RING_BUFFER_ALL_CPUS);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_mark_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tenum event_trigger_type tt = ETT_NONE;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tconst char faulted[] = \"<faulted>\";\n\tssize_t written;\n\tint size;\n\tint len;\n\n/* Used in tracing_mark_raw_write() as well */\n#define FAULTED_SIZE (sizeof(faulted) - 1) /* '\\0' is already accounted for */\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tlocal_save_flags(irq_flags);\n\tsize = sizeof(*entry) + cnt + 2; /* add '\\0' and possible '\\n' */\n\n\t/* If less than \"<faulted>\", then make sure we can still add that */\n\tif (cnt < FAULTED_SIZE)\n\t\tsize += FAULTED_SIZE - cnt;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    irq_flags, preempt_count());\n\tif (unlikely(!event))\n\t\t/* Ring buffer disabled, return as if not open for write */\n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = _THIS_IP_;\n\n\tlen = __copy_from_user_inatomic(&entry->buf, ubuf, cnt);\n\tif (len) {\n\t\tmemcpy(&entry->buf, faulted, FAULTED_SIZE);\n\t\tcnt = FAULTED_SIZE;\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\tlen = cnt;\n\n\tif (tr->trace_marker_file && !list_empty(&tr->trace_marker_file->triggers)) {\n\t\t/* do not add \\n before testing triggers, but add \\0 */\n\t\tentry->buf[cnt] = '\\0';\n\t\ttt = event_triggers_call(tr->trace_marker_file, entry, event);\n\t}\n\n\tif (entry->buf[cnt - 1] != '\\n') {\n\t\tentry->buf[cnt] = '\\n';\n\t\tentry->buf[cnt + 1] = '\\0';\n\t} else\n\t\tentry->buf[cnt] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (tt)\n\t\tevent_triggers_post_call(tr->trace_marker_file, tt);\n\n\tif (written > 0)\n\t\t*fpos += written;\n\n\treturn written;\n}\n\n/* Limit it for now to 3K (including tag) */\n#define RAW_DATA_MAX_SIZE (1024*3)\n\nstatic ssize_t\ntracing_mark_raw_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct raw_data_entry *entry;\n\tconst char faulted[] = \"<faulted>\";\n\tunsigned long irq_flags;\n\tssize_t written;\n\tint size;\n\tint len;\n\n#define FAULT_SIZE_ID (FAULTED_SIZE + sizeof(int))\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\t/* The marker must at least have a tag id */\n\tif (cnt < sizeof(unsigned int) || cnt > RAW_DATA_MAX_SIZE)\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tlocal_save_flags(irq_flags);\n\tsize = sizeof(*entry) + cnt;\n\tif (cnt < FAULT_SIZE_ID)\n\t\tsize += FAULT_SIZE_ID - cnt;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_RAW_DATA, size,\n\t\t\t\t\t    irq_flags, preempt_count());\n\tif (!event)\n\t\t/* Ring buffer disabled, return as if not open for write */\n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\n\tlen = __copy_from_user_inatomic(&entry->id, ubuf, cnt);\n\tif (len) {\n\t\tentry->id = -1;\n\t\tmemcpy(&entry->buf, faulted, FAULTED_SIZE);\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (written > 0)\n\t\t*fpos += written;\n\n\treturn written;\n}\n\nstatic int tracing_clock_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++)\n\t\tseq_printf(m,\n\t\t\t\"%s%s%s%s\", i ? \" \" : \"\",\n\t\t\ti == tr->clock_id ? \"[\" : \"\", trace_clocks[i].name,\n\t\t\ti == tr->clock_id ? \"]\" : \"\");\n\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nint tracing_set_clock(struct trace_array *tr, const char *clockstr)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++) {\n\t\tif (strcmp(trace_clocks[i].name, clockstr) == 0)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(trace_clocks))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->clock_id = i;\n\n\tring_buffer_set_clock(tr->trace_buffer.buffer, trace_clocks[i].func);\n\n\t/*\n\t * New clock may not be consistent with the previous clock.\n\t * Reset the buffer so that it doesn't have incomparable timestamps.\n\t */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_clock(tr->max_buffer.buffer, trace_clocks[i].func);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic ssize_t tracing_clock_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t   size_t cnt, loff_t *fpos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tconst char *clockstr;\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tclockstr = strstrip(buf);\n\n\tret = tracing_set_clock(tr, clockstr);\n\tif (ret)\n\t\treturn ret;\n\n\t*fpos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_clock_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr))\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_clock_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic int tracing_time_stamp_mode_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (ring_buffer_time_stamp_abs(tr->trace_buffer.buffer))\n\t\tseq_puts(m, \"delta [absolute]\\n\");\n\telse\n\t\tseq_puts(m, \"[delta] absolute\\n\");\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int tracing_time_stamp_mode_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr))\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_time_stamp_mode_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nint tracing_set_time_stamp_abs(struct trace_array *tr, bool abs)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (abs && tr->time_stamp_abs_ref++)\n\t\tgoto out;\n\n\tif (!abs) {\n\t\tif (WARN_ON_ONCE(!tr->time_stamp_abs_ref)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (--tr->time_stamp_abs_ref)\n\t\t\tgoto out;\n\t}\n\n\tring_buffer_set_time_stamp_abs(tr->trace_buffer.buffer, abs);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_time_stamp_abs(tr->max_buffer.buffer, abs);\n#endif\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct ftrace_buffer_info {\n\tstruct trace_iterator\titer;\n\tvoid\t\t\t*spare;\n\tunsigned int\t\tspare_cpu;\n\tunsigned int\t\tread;\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic int tracing_snapshot_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tstruct seq_file *m;\n\tint ret = 0;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, true);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t} else {\n\t\t/* Writes still need the seq_file to hold the private data */\n\t\tret = -ENOMEM;\n\t\tm = kzalloc(sizeof(*m), GFP_KERNEL);\n\t\tif (!m)\n\t\t\tgoto out;\n\t\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\t\tif (!iter) {\n\t\t\tkfree(m);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 0;\n\n\t\titer->tr = tr;\n\t\titer->trace_buffer = &tr->max_buffer;\n\t\titer->cpu_file = tracing_get_cpu(inode);\n\t\tm->private = iter;\n\t\tfile->private_data = m;\n\t}\nout:\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_snapshot_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t       loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long val;\n\tint ret;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (tr->current_trace->use_max_tr) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tswitch (val) {\n\tcase 0:\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (tr->allocated_snapshot)\n\t\t\tfree_snapshot(tr);\n\t\tbreak;\n\tcase 1:\n/* Only allow per-cpu swap if the ring buffer supports it */\n#ifndef CONFIG_RING_BUFFER_ALLOW_SWAP\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tif (!tr->allocated_snapshot) {\n\t\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tlocal_irq_disable();\n\t\t/* Now, we're going to swap */\n\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\tupdate_max_tr(tr, current, smp_processor_id());\n\t\telse\n\t\t\tupdate_max_tr_single(tr, current, iter->cpu_file);\n\t\tlocal_irq_enable();\n\t\tbreak;\n\tdefault:\n\t\tif (tr->allocated_snapshot) {\n\t\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\t\ttracing_reset_online_cpus(&tr->max_buffer);\n\t\t\telse\n\t\t\t\ttracing_reset(&tr->max_buffer, iter->cpu_file);\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (ret >= 0) {\n\t\t*ppos += cnt;\n\t\tret = cnt;\n\t}\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_snapshot_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *m = file->private_data;\n\tint ret;\n\n\tret = tracing_release(inode, file);\n\n\tif (file->f_mode & FMODE_READ)\n\t\treturn ret;\n\n\t/* If write only, the seq_file is just a stub */\n\tif (m)\n\t\tkfree(m->private);\n\tkfree(m);\n\n\treturn 0;\n}\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp);\nstatic ssize_t tracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t\t\t    size_t count, loff_t *ppos);\nstatic int tracing_buffers_release(struct inode *inode, struct file *file);\nstatic ssize_t tracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t   struct pipe_inode_info *pipe, size_t len, unsigned int flags);\n\nstatic int snapshot_raw_open(struct inode *inode, struct file *filp)\n{\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tret = tracing_buffers_open(inode, filp);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tinfo = filp->private_data;\n\n\tif (info->iter.trace->use_max_tr) {\n\t\ttracing_buffers_release(inode, filp);\n\t\treturn -EBUSY;\n\t}\n\n\tinfo->iter.snapshot = true;\n\tinfo->iter.trace_buffer = &info->iter.tr->max_buffer;\n\n\treturn ret;\n}\n\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\n\nstatic const struct file_operations tracing_thresh_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_thresh_read,\n\t.write\t\t= tracing_thresh_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\nstatic const struct file_operations tracing_max_lat_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_max_lat_read,\n\t.write\t\t= tracing_max_lat_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif\n\nstatic const struct file_operations set_tracer_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_set_trace_read,\n\t.write\t\t= tracing_set_trace_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic const struct file_operations tracing_pipe_fops = {\n\t.open\t\t= tracing_open_pipe,\n\t.poll\t\t= tracing_poll_pipe,\n\t.read\t\t= tracing_read_pipe,\n\t.splice_read\t= tracing_splice_read_pipe,\n\t.release\t= tracing_release_pipe,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic const struct file_operations tracing_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_entries_read,\n\t.write\t\t= tracing_entries_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_total_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_total_entries_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_free_buffer_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_free_buffer_write,\n\t.release\t= tracing_free_buffer_release,\n};\n\nstatic const struct file_operations tracing_mark_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_mark_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_mark_raw_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_mark_raw_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations trace_clock_fops = {\n\t.open\t\t= tracing_clock_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_clock_write,\n};\n\nstatic const struct file_operations trace_time_stamp_mode_fops = {\n\t.open\t\t= tracing_time_stamp_mode_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic const struct file_operations snapshot_fops = {\n\t.open\t\t= tracing_snapshot_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_snapshot_write,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_snapshot_release,\n};\n\nstatic const struct file_operations snapshot_raw_fops = {\n\t.open\t\t= snapshot_raw_open,\n\t.read\t\t= tracing_buffers_read,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info) {\n\t\ttrace_array_put(tr);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\tinfo->iter.tr\t\t= tr;\n\tinfo->iter.cpu_file\t= tracing_get_cpu(inode);\n\tinfo->iter.trace\t= tr->current_trace;\n\tinfo->iter.trace_buffer = &tr->trace_buffer;\n\tinfo->spare\t\t= NULL;\n\t/* Force reading ring buffer for first read */\n\tinfo->read\t\t= (unsigned int)-1;\n\n\tfilp->private_data = info;\n\n\ttr->current_trace->ref++;\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = nonseekable_open(inode, filp);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic __poll_t\ntracing_buffers_poll(struct file *filp, poll_table *poll_table)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\nstatic ssize_t\ntracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tssize_t ret = 0;\n\tssize_t size;\n\n\tif (!count)\n\t\treturn 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (!info->spare) {\n\t\tinfo->spare = ring_buffer_alloc_read_page(iter->trace_buffer->buffer,\n\t\t\t\t\t\t\t  iter->cpu_file);\n\t\tif (IS_ERR(info->spare)) {\n\t\t\tret = PTR_ERR(info->spare);\n\t\t\tinfo->spare = NULL;\n\t\t} else {\n\t\t\tinfo->spare_cpu = iter->cpu_file;\n\t\t}\n\t}\n\tif (!info->spare)\n\t\treturn ret;\n\n\t/* Do we have previous read data to read? */\n\tif (info->read < PAGE_SIZE)\n\t\tgoto read;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tret = ring_buffer_read_page(iter->trace_buffer->buffer,\n\t\t\t\t    &info->spare,\n\t\t\t\t    count,\n\t\t\t\t    iter->cpu_file, 0);\n\ttrace_access_unlock(iter->cpu_file);\n\n\tif (ret < 0) {\n\t\tif (trace_empty(iter)) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tret = wait_on_pipe(iter, false);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tgoto again;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tinfo->read = 0;\n read:\n\tsize = PAGE_SIZE - info->read;\n\tif (size > count)\n\t\tsize = count;\n\n\tret = copy_to_user(ubuf, info->spare + info->read, size);\n\tif (ret == size)\n\t\treturn -EFAULT;\n\n\tsize -= ret;\n\n\t*ppos += size;\n\tinfo->read += size;\n\n\treturn size;\n}\n\nstatic int tracing_buffers_release(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\tmutex_lock(&trace_types_lock);\n\n\titer->tr->current_trace->ref--;\n\n\t__trace_array_put(iter->tr);\n\n\tif (info->spare)\n\t\tring_buffer_free_read_page(iter->trace_buffer->buffer,\n\t\t\t\t\t   info->spare_cpu, info->spare);\n\tkfree(info);\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstruct buffer_ref {\n\tstruct ring_buffer\t*buffer;\n\tvoid\t\t\t*page;\n\tint\t\t\tcpu;\n\tint\t\t\tref;\n};\n\nstatic void buffer_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tif (--ref->ref)\n\t\treturn;\n\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n\tbuf->private = 0;\n}\n\nstatic void buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tref->ref++;\n}\n\n/* Pipe buffer operations for a buffer. */\nstatic const struct pipe_buf_operations buffer_pipe_buf_ops = {\n\t.can_merge\t\t= 0,\n\t.confirm\t\t= generic_pipe_buf_confirm,\n\t.release\t\t= buffer_pipe_buf_release,\n\t.steal\t\t\t= generic_pipe_buf_steal,\n\t.get\t\t\t= buffer_pipe_buf_get,\n};\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void buffer_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tstruct buffer_ref *ref =\n\t\t(struct buffer_ref *)spd->partial[i].private;\n\n\tif (--ref->ref)\n\t\treturn;\n\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n\tspd->partial[i].private = 0;\n}\n\nstatic ssize_t\ntracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t\t    struct pipe_inode_info *pipe, size_t len,\n\t\t\t    unsigned int flags)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &buffer_pipe_buf_ops,\n\t\t.spd_release\t= buffer_spd_release,\n\t};\n\tstruct buffer_ref *ref;\n\tint entries, i;\n\tssize_t ret = 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (*ppos & (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tif (len & (PAGE_SIZE - 1)) {\n\t\tif (len < PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\t\tlen &= PAGE_MASK;\n\t}\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tentries = ring_buffer_entries_cpu(iter->trace_buffer->buffer, iter->cpu_file);\n\n\tfor (i = 0; i < spd.nr_pages_max && len && entries; i++, len -= PAGE_SIZE) {\n\t\tstruct page *page;\n\t\tint r;\n\n\t\tref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!ref) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tref->ref = 1;\n\t\tref->buffer = iter->trace_buffer->buffer;\n\t\tref->page = ring_buffer_alloc_read_page(ref->buffer, iter->cpu_file);\n\t\tif (IS_ERR(ref->page)) {\n\t\t\tret = PTR_ERR(ref->page);\n\t\t\tref->page = NULL;\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\t\tref->cpu = iter->cpu_file;\n\n\t\tr = ring_buffer_read_page(ref->buffer, &ref->page,\n\t\t\t\t\t  len, iter->cpu_file, 1);\n\t\tif (r < 0) {\n\t\t\tring_buffer_free_read_page(ref->buffer, ref->cpu,\n\t\t\t\t\t\t   ref->page);\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\n\t\tpage = virt_to_page(ref->page);\n\n\t\tspd.pages[i] = page;\n\t\tspd.partial[i].len = PAGE_SIZE;\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].private = (unsigned long)ref;\n\t\tspd.nr_pages++;\n\t\t*ppos += PAGE_SIZE;\n\n\t\tentries = ring_buffer_entries_cpu(iter->trace_buffer->buffer, iter->cpu_file);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\tspd.nr_pages = i;\n\n\t/* did we read anything? */\n\tif (!spd.nr_pages) {\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = -EAGAIN;\n\t\tif ((file->f_flags & O_NONBLOCK) || (flags & SPLICE_F_NONBLOCK))\n\t\t\tgoto out;\n\n\t\tret = wait_on_pipe(iter, true);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\n\tret = splice_to_pipe(pipe, &spd);\nout:\n\tsplice_shrink_spd(&spd);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_buffers_fops = {\n\t.open\t\t= tracing_buffers_open,\n\t.read\t\t= tracing_buffers_read,\n\t.poll\t\t= tracing_buffers_poll,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic ssize_t\ntracing_stats_read(struct file *filp, char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\tint cpu = tracing_get_cpu(inode);\n\tstruct trace_seq *s;\n\tunsigned long cnt;\n\tunsigned long long t;\n\tunsigned long usec_rem;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ttrace_seq_init(s);\n\n\tcnt = ring_buffer_entries_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"entries: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_commit_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"commit overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_bytes_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"bytes: %ld\\n\", cnt);\n\n\tif (trace_clocks[tr->clock_id].in_ns) {\n\t\t/* local or global for trace_clock */\n\t\tt = ns2usecs(ring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"oldest event ts: %5llu.%06lu\\n\",\n\t\t\t\t\t\t\t\tt, usec_rem);\n\n\t\tt = ns2usecs(ring_buffer_time_stamp(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"now ts: %5llu.%06lu\\n\", t, usec_rem);\n\t} else {\n\t\t/* counter or tsc mode for trace_clock */\n\t\ttrace_seq_printf(s, \"oldest event ts: %llu\\n\",\n\t\t\t\tring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\n\t\ttrace_seq_printf(s, \"now ts: %llu\\n\",\n\t\t\t\tring_buffer_time_stamp(trace_buf->buffer, cpu));\n\t}\n\n\tcnt = ring_buffer_dropped_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"dropped events: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_read_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"read events: %ld\\n\", cnt);\n\n\tcount = simple_read_from_buffer(ubuf, count, ppos,\n\t\t\t\t\ts->buffer, trace_seq_used(s));\n\n\tkfree(s);\n\n\treturn count;\n}\n\nstatic const struct file_operations tracing_stats_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_stats_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\nstatic ssize_t\ntracing_read_dyn_info(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long *p = filp->private_data;\n\tchar buf[64]; /* Not too big for a shallow stack */\n\tint r;\n\n\tr = scnprintf(buf, 63, \"%ld\", *p);\n\tbuf[r++] = '\\n';\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic const struct file_operations tracing_dyn_info_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_read_dyn_info,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif /* CONFIG_DYNAMIC_FTRACE */\n\n#if defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE)\nstatic void\nftrace_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\tstruct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\tvoid *data)\n{\n\ttracing_snapshot_instance(tr);\n}\n\nstatic void\nftrace_count_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\t      struct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\t      void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count) {\n\n\t\tif (*count <= 0)\n\t\t\treturn;\n\n\t\t(*count)--;\n\t}\n\n\ttracing_snapshot_instance(tr);\n}\n\nstatic int\nftrace_snapshot_print(struct seq_file *m, unsigned long ip,\n\t\t      struct ftrace_probe_ops *ops, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tseq_printf(m, \"%ps:\", (void *)ip);\n\n\tseq_puts(m, \"snapshot\");\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count)\n\t\tseq_printf(m, \":count=%ld\\n\", *count);\n\telse\n\t\tseq_puts(m, \":unlimited\\n\");\n\n\treturn 0;\n}\n\nstatic int\nftrace_snapshot_init(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *init_data, void **data)\n{\n\tstruct ftrace_func_mapper *mapper = *data;\n\n\tif (!mapper) {\n\t\tmapper = allocate_ftrace_func_mapper();\n\t\tif (!mapper)\n\t\t\treturn -ENOMEM;\n\t\t*data = mapper;\n\t}\n\n\treturn ftrace_func_mapper_add_ip(mapper, ip, init_data);\n}\n\nstatic void\nftrace_snapshot_free(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\n\tif (!ip) {\n\t\tif (!mapper)\n\t\t\treturn;\n\t\tfree_ftrace_func_mapper(mapper, NULL);\n\t\treturn;\n\t}\n\n\tftrace_func_mapper_remove_ip(mapper, ip);\n}\n\nstatic struct ftrace_probe_ops snapshot_probe_ops = {\n\t.func\t\t\t= ftrace_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n};\n\nstatic struct ftrace_probe_ops snapshot_count_probe_ops = {\n\t.func\t\t\t= ftrace_count_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n\t.init\t\t\t= ftrace_snapshot_init,\n\t.free\t\t\t= ftrace_snapshot_free,\n};\n\nstatic int\nftrace_trace_snapshot_callback(struct trace_array *tr, struct ftrace_hash *hash,\n\t\t\t       char *glob, char *cmd, char *param, int enable)\n{\n\tstruct ftrace_probe_ops *ops;\n\tvoid *count = (void *)-1;\n\tchar *number;\n\tint ret;\n\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\t/* hash funcs only work with set_ftrace_filter */\n\tif (!enable)\n\t\treturn -EINVAL;\n\n\tops = param ? &snapshot_count_probe_ops :  &snapshot_probe_ops;\n\n\tif (glob[0] == '!')\n\t\treturn unregister_ftrace_function_probe_func(glob+1, tr, ops);\n\n\tif (!param)\n\t\tgoto out_reg;\n\n\tnumber = strsep(&param, \":\");\n\n\tif (!strlen(number))\n\t\tgoto out_reg;\n\n\t/*\n\t * We use the callback data field (which is a pointer)\n\t * as our counter.\n\t */\n\tret = kstrtoul(number, 0, (unsigned long *)&count);\n\tif (ret)\n\t\treturn ret;\n\n out_reg:\n\tret = tracing_alloc_snapshot_instance(tr);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = register_ftrace_function_probe(glob, tr, ops, count);\n\n out:\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic struct ftrace_func_command ftrace_snapshot_cmd = {\n\t.name\t\t\t= \"snapshot\",\n\t.func\t\t\t= ftrace_trace_snapshot_callback,\n};\n\nstatic __init int register_snapshot_cmd(void)\n{\n\treturn register_ftrace_command(&ftrace_snapshot_cmd);\n}\n#else\nstatic inline __init int register_snapshot_cmd(void) { return 0; }\n#endif /* defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE) */\n\nstatic struct dentry *tracing_get_dentry(struct trace_array *tr)\n{\n\tif (WARN_ON(!tr->dir))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t/* Top directory uses NULL as the parent */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn NULL;\n\n\t/* All sub buffers have a descriptor */\n\treturn tr->dir;\n}\n\nstatic struct dentry *tracing_dentry_percpu(struct trace_array *tr, int cpu)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->percpu_dir)\n\t\treturn tr->percpu_dir;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->percpu_dir = tracefs_create_dir(\"per_cpu\", d_tracer);\n\n\tWARN_ONCE(!tr->percpu_dir,\n\t\t  \"Could not create tracefs directory 'per_cpu/%d'\\n\", cpu);\n\n\treturn tr->percpu_dir;\n}\n\nstatic struct dentry *\ntrace_create_cpu_file(const char *name, umode_t mode, struct dentry *parent,\n\t\t      void *data, long cpu, const struct file_operations *fops)\n{\n\tstruct dentry *ret = trace_create_file(name, mode, parent, data, fops);\n\n\tif (ret) /* See tracing_get_cpu() */\n\t\td_inode(ret)->i_cdev = (void *)(cpu + 1);\n\treturn ret;\n}\n\nstatic void\ntracing_init_tracefs_percpu(struct trace_array *tr, long cpu)\n{\n\tstruct dentry *d_percpu = tracing_dentry_percpu(tr, cpu);\n\tstruct dentry *d_cpu;\n\tchar cpu_dir[30]; /* 30 characters should be more than enough */\n\n\tif (!d_percpu)\n\t\treturn;\n\n\tsnprintf(cpu_dir, 30, \"cpu%ld\", cpu);\n\td_cpu = tracefs_create_dir(cpu_dir, d_percpu);\n\tif (!d_cpu) {\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", cpu_dir);\n\t\treturn;\n\t}\n\n\t/* per cpu trace_pipe */\n\ttrace_create_cpu_file(\"trace_pipe\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_pipe_fops);\n\n\t/* per cpu trace */\n\ttrace_create_cpu_file(\"trace\", 0644, d_cpu,\n\t\t\t\ttr, cpu, &tracing_fops);\n\n\ttrace_create_cpu_file(\"trace_pipe_raw\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_buffers_fops);\n\n\ttrace_create_cpu_file(\"stats\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_stats_fops);\n\n\ttrace_create_cpu_file(\"buffer_size_kb\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_entries_fops);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_cpu_file(\"snapshot\", 0644, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_fops);\n\n\ttrace_create_cpu_file(\"snapshot_raw\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_raw_fops);\n#endif\n}\n\n#ifdef CONFIG_FTRACE_SELFTEST\n/* Let selftest have access to static functions in this file */\n#include \"trace_selftest.c\"\n#endif\n\nstatic ssize_t\ntrace_options_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tchar *buf;\n\n\tif (topt->flags->val & topt->opt->bit)\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tif (!!(topt->flags->val & topt->opt->bit) != val) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tret = __set_tracer_option(topt->tr, topt->flags,\n\t\t\t\t\t  topt->opt, !val);\n\t\tmutex_unlock(&trace_types_lock);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\n\nstatic const struct file_operations trace_options_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_read,\n\t.write = trace_options_write,\n\t.llseek\t= generic_file_llseek,\n};\n\n/*\n * In order to pass in both the trace_array descriptor as well as the index\n * to the flag that the trace option file represents, the trace_array\n * has a character array of trace_flags_index[], which holds the index\n * of the bit for the flag it represents. index[0] == 0, index[1] == 1, etc.\n * The address of this character array is passed to the flag option file\n * read/write callbacks.\n *\n * In order to extract both the index and the trace_array descriptor,\n * get_tr_index() uses the following algorithm.\n *\n *   idx = *ptr;\n *\n * As the pointer itself contains the address of the index (remember\n * index[1] == 1).\n *\n * Then to get the trace_array descriptor, by subtracting that index\n * from the ptr, we get to the start of the index itself.\n *\n *   ptr - idx == &index[0]\n *\n * Then a simple container_of() from that pointer gets us to the\n * trace_array descriptor.\n */\nstatic void get_tr_index(void *data, struct trace_array **ptr,\n\t\t\t unsigned int *pindex)\n{\n\t*pindex = *(unsigned char *)data;\n\n\t*ptr = container_of(data - *pindex, struct trace_array,\n\t\t\t    trace_flags_index);\n}\n\nstatic ssize_t\ntrace_options_core_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tchar *buf;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tif (tr->trace_flags & (1 << index))\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_core_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tunsigned long val;\n\tint ret;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\tret = set_tracer_flag(tr, 1 << index, val);\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations trace_options_core_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_core_read,\n\t.write = trace_options_core_write,\n\t.llseek = generic_file_llseek,\n};\n\nstruct dentry *trace_create_file(const char *name,\n\t\t\t\t umode_t mode,\n\t\t\t\t struct dentry *parent,\n\t\t\t\t void *data,\n\t\t\t\t const struct file_operations *fops)\n{\n\tstruct dentry *ret;\n\n\tret = tracefs_create_file(name, mode, parent, data, fops);\n\tif (!ret)\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", name);\n\n\treturn ret;\n}\n\n\nstatic struct dentry *trace_options_init_dentry(struct trace_array *tr)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->options)\n\t\treturn tr->options;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->options = tracefs_create_dir(\"options\", d_tracer);\n\tif (!tr->options) {\n\t\tpr_warn(\"Could not create tracefs directory 'options'\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn tr->options;\n}\n\nstatic void\ncreate_trace_option_file(struct trace_array *tr,\n\t\t\t struct trace_option_dentry *topt,\n\t\t\t struct tracer_flags *flags,\n\t\t\t struct tracer_opt *opt)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\ttopt->flags = flags;\n\ttopt->opt = opt;\n\ttopt->tr = tr;\n\n\ttopt->entry = trace_create_file(opt->name, 0644, t_options, topt,\n\t\t\t\t    &trace_options_fops);\n\n}\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer)\n{\n\tstruct trace_option_dentry *topts;\n\tstruct trace_options *tr_topts;\n\tstruct tracer_flags *flags;\n\tstruct tracer_opt *opts;\n\tint cnt;\n\tint i;\n\n\tif (!tracer)\n\t\treturn;\n\n\tflags = tracer->flags;\n\n\tif (!flags || !flags->opts)\n\t\treturn;\n\n\t/*\n\t * If this is an instance, only create flags for tracers\n\t * the instance may have.\n\t */\n\tif (!trace_ok_for_array(tracer, tr))\n\t\treturn;\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\t/* Make sure there's no duplicate flags. */\n\t\tif (WARN_ON_ONCE(tr->topts[i].tracer->flags == tracer->flags))\n\t\t\treturn;\n\t}\n\n\topts = flags->opts;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++)\n\t\t;\n\n\ttopts = kcalloc(cnt + 1, sizeof(*topts), GFP_KERNEL);\n\tif (!topts)\n\t\treturn;\n\n\ttr_topts = krealloc(tr->topts, sizeof(*tr->topts) * (tr->nr_topts + 1),\n\t\t\t    GFP_KERNEL);\n\tif (!tr_topts) {\n\t\tkfree(topts);\n\t\treturn;\n\t}\n\n\ttr->topts = tr_topts;\n\ttr->topts[tr->nr_topts].tracer = tracer;\n\ttr->topts[tr->nr_topts].topts = topts;\n\ttr->nr_topts++;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++) {\n\t\tcreate_trace_option_file(tr, &topts[cnt], flags,\n\t\t\t\t\t &opts[cnt]);\n\t\tWARN_ONCE(topts[cnt].entry == NULL,\n\t\t\t  \"Failed to create trace option: %s\",\n\t\t\t  opts[cnt].name);\n\t}\n}\n\nstatic struct dentry *\ncreate_trace_option_core_file(struct trace_array *tr,\n\t\t\t      const char *option, long index)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn NULL;\n\n\treturn trace_create_file(option, 0644, t_options,\n\t\t\t\t (void *)&tr->trace_flags_index[index],\n\t\t\t\t &trace_options_core_fops);\n}\n\nstatic void create_trace_options_dir(struct trace_array *tr)\n{\n\tstruct dentry *t_options;\n\tbool top_level = tr == &global_trace;\n\tint i;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (top_level ||\n\t\t    !((1 << i) & TOP_LEVEL_TRACE_FLAGS))\n\t\t\tcreate_trace_option_core_file(tr, trace_options[i], i);\n\t}\n}\n\nstatic ssize_t\nrb_simple_read(struct file *filp, char __user *ubuf,\n\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r;\n\n\tr = tracer_tracing_is_on(tr);\n\tr = sprintf(buf, \"%d\\n\", r);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\nrb_simple_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (buffer) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tif (val) {\n\t\t\ttracer_tracing_on(tr);\n\t\t\tif (tr->current_trace->start)\n\t\t\t\ttr->current_trace->start(tr);\n\t\t} else {\n\t\t\ttracer_tracing_off(tr);\n\t\t\tif (tr->current_trace->stop)\n\t\t\t\ttr->current_trace->stop(tr);\n\t\t}\n\t\tmutex_unlock(&trace_types_lock);\n\t}\n\n\t(*ppos)++;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations rb_simple_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= rb_simple_read,\n\t.write\t\t= rb_simple_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= default_llseek,\n};\n\nstruct dentry *trace_instance_dir;\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer);\n\nstatic int\nallocate_trace_buffer(struct trace_array *tr, struct trace_buffer *buf, int size)\n{\n\tenum ring_buffer_flags rb_flags;\n\n\trb_flags = tr->trace_flags & TRACE_ITER_OVERWRITE ? RB_FL_OVERWRITE : 0;\n\n\tbuf->tr = tr;\n\n\tbuf->buffer = ring_buffer_alloc(size, rb_flags);\n\tif (!buf->buffer)\n\t\treturn -ENOMEM;\n\n\tbuf->data = alloc_percpu(struct trace_array_cpu);\n\tif (!buf->data) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Allocate the first page for all buffers */\n\tset_buffer_entries(&tr->trace_buffer,\n\t\t\t   ring_buffer_size(tr->trace_buffer.buffer, 0));\n\n\treturn 0;\n}\n\nstatic int allocate_trace_buffers(struct trace_array *tr, int size)\n{\n\tint ret;\n\n\tret = allocate_trace_buffer(tr, &tr->trace_buffer, size);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tret = allocate_trace_buffer(tr, &tr->max_buffer,\n\t\t\t\t    allocate_snapshot ? size : 1);\n\tif (WARN_ON(ret)) {\n\t\tring_buffer_free(tr->trace_buffer.buffer);\n\t\ttr->trace_buffer.buffer = NULL;\n\t\tfree_percpu(tr->trace_buffer.data);\n\t\ttr->trace_buffer.data = NULL;\n\t\treturn -ENOMEM;\n\t}\n\ttr->allocated_snapshot = allocate_snapshot;\n\n\t/*\n\t * Only the top level trace array gets its snapshot allocated\n\t * from the kernel command line.\n\t */\n\tallocate_snapshot = false;\n#endif\n\treturn 0;\n}\n\nstatic void free_trace_buffer(struct trace_buffer *buf)\n{\n\tif (buf->buffer) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\tfree_percpu(buf->data);\n\t\tbuf->data = NULL;\n\t}\n}\n\nstatic void free_trace_buffers(struct trace_array *tr)\n{\n\tif (!tr)\n\t\treturn;\n\n\tfree_trace_buffer(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tfree_trace_buffer(&tr->max_buffer);\n#endif\n}\n\nstatic void init_trace_flags_index(struct trace_array *tr)\n{\n\tint i;\n\n\t/* Used by the trace options files */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++)\n\t\ttr->trace_flags_index[i] = i;\n}\n\nstatic void __update_tracer_options(struct trace_array *tr)\n{\n\tstruct tracer *t;\n\n\tfor (t = trace_types; t; t = t->next)\n\t\tadd_tracer_options(tr, t);\n}\n\nstatic void update_tracer_options(struct trace_array *tr)\n{\n\tmutex_lock(&trace_types_lock);\n\t__update_tracer_options(tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int instance_mkdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -EEXIST;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\tret = -ENOMEM;\n\ttr = kzalloc(sizeof(*tr), GFP_KERNEL);\n\tif (!tr)\n\t\tgoto out_unlock;\n\n\ttr->name = kstrdup(name, GFP_KERNEL);\n\tif (!tr->name)\n\t\tgoto out_free_tr;\n\n\tif (!alloc_cpumask_var(&tr->tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_tr;\n\n\ttr->trace_flags = global_trace.trace_flags & ~ZEROED_TRACE_FLAGS;\n\n\tcpumask_copy(tr->tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&tr->start_lock);\n\n\ttr->max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\ttr->current_trace = &nop_trace;\n\n\tINIT_LIST_HEAD(&tr->systems);\n\tINIT_LIST_HEAD(&tr->events);\n\tINIT_LIST_HEAD(&tr->hist_vars);\n\n\tif (allocate_trace_buffers(tr, trace_buf_size) < 0)\n\t\tgoto out_free_tr;\n\n\ttr->dir = tracefs_create_dir(name, trace_instance_dir);\n\tif (!tr->dir)\n\t\tgoto out_free_tr;\n\n\tret = event_trace_add_tracer(tr->dir, tr);\n\tif (ret) {\n\t\ttracefs_remove_recursive(tr->dir);\n\t\tgoto out_free_tr;\n\t}\n\n\tftrace_init_trace_array(tr);\n\n\tinit_tracer_tracefs(tr, tr->dir);\n\tinit_trace_flags_index(tr);\n\t__update_tracer_options(tr);\n\n\tlist_add(&tr->list, &ftrace_trace_arrays);\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn 0;\n\n out_free_tr:\n\tfree_trace_buffers(tr);\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n\n}\n\nstatic int instance_rmdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint found = 0;\n\tint ret;\n\tint i;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -ENODEV;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\tgoto out_unlock;\n\n\tret = -EBUSY;\n\tif (tr->ref || (tr->current_trace && tr->current_trace->ref))\n\t\tgoto out_unlock;\n\n\tlist_del(&tr->list);\n\n\t/* Disable all the flags that were enabled coming in */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++) {\n\t\tif ((1 << i) & ZEROED_TRACE_FLAGS)\n\t\t\tset_tracer_flag(tr, 1 << i, 0);\n\t}\n\n\ttracing_set_nop(tr);\n\tclear_ftrace_function_probes(tr);\n\tevent_trace_del_tracer(tr);\n\tftrace_clear_pids(tr);\n\tftrace_destroy_function_files(tr);\n\ttracefs_remove_recursive(tr->dir);\n\tfree_trace_buffers(tr);\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\tkfree(tr->topts[i].topts);\n\t}\n\tkfree(tr->topts);\n\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n\tret = 0;\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\nstatic __init void create_trace_instances(struct dentry *d_tracer)\n{\n\ttrace_instance_dir = tracefs_create_instance_dir(\"instances\", d_tracer,\n\t\t\t\t\t\t\t instance_mkdir,\n\t\t\t\t\t\t\t instance_rmdir);\n\tif (WARN_ON(!trace_instance_dir))\n\t\treturn;\n}\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer)\n{\n\tstruct trace_event_file *file;\n\tint cpu;\n\n\ttrace_create_file(\"available_tracers\", 0444, d_tracer,\n\t\t\ttr, &show_traces_fops);\n\n\ttrace_create_file(\"current_tracer\", 0644, d_tracer,\n\t\t\ttr, &set_tracer_fops);\n\n\ttrace_create_file(\"tracing_cpumask\", 0644, d_tracer,\n\t\t\t  tr, &tracing_cpumask_fops);\n\n\ttrace_create_file(\"trace_options\", 0644, d_tracer,\n\t\t\t  tr, &tracing_iter_fops);\n\n\ttrace_create_file(\"trace\", 0644, d_tracer,\n\t\t\t  tr, &tracing_fops);\n\n\ttrace_create_file(\"trace_pipe\", 0444, d_tracer,\n\t\t\t  tr, &tracing_pipe_fops);\n\n\ttrace_create_file(\"buffer_size_kb\", 0644, d_tracer,\n\t\t\t  tr, &tracing_entries_fops);\n\n\ttrace_create_file(\"buffer_total_size_kb\", 0444, d_tracer,\n\t\t\t  tr, &tracing_total_entries_fops);\n\n\ttrace_create_file(\"free_buffer\", 0200, d_tracer,\n\t\t\t  tr, &tracing_free_buffer_fops);\n\n\ttrace_create_file(\"trace_marker\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_fops);\n\n\tfile = __find_event_file(tr, \"ftrace\", \"print\");\n\tif (file && file->dir)\n\t\ttrace_create_file(\"trigger\", 0644, file->dir, file,\n\t\t\t\t  &event_trigger_fops);\n\ttr->trace_marker_file = file;\n\n\ttrace_create_file(\"trace_marker_raw\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_raw_fops);\n\n\ttrace_create_file(\"trace_clock\", 0644, d_tracer, tr,\n\t\t\t  &trace_clock_fops);\n\n\ttrace_create_file(\"tracing_on\", 0644, d_tracer,\n\t\t\t  tr, &rb_simple_fops);\n\n\ttrace_create_file(\"timestamp_mode\", 0444, d_tracer, tr,\n\t\t\t  &trace_time_stamp_mode_fops);\n\n\tcreate_trace_options_dir(tr);\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\n\ttrace_create_file(\"tracing_max_latency\", 0644, d_tracer,\n\t\t\t&tr->max_latency, &tracing_max_lat_fops);\n#endif\n\n\tif (ftrace_create_function_files(tr, d_tracer))\n\t\tWARN(1, \"Could not allocate function filter files\");\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_file(\"snapshot\", 0644, d_tracer,\n\t\t\t  tr, &snapshot_fops);\n#endif\n\n\tfor_each_tracing_cpu(cpu)\n\t\ttracing_init_tracefs_percpu(tr, cpu);\n\n\tftrace_init_tracefs(tr, d_tracer);\n}\n\nstatic struct vfsmount *trace_automount(struct dentry *mntpt, void *ingore)\n{\n\tstruct vfsmount *mnt;\n\tstruct file_system_type *type;\n\n\t/*\n\t * To maintain backward compatibility for tools that mount\n\t * debugfs to get to the tracing facility, tracefs is automatically\n\t * mounted to the debugfs/tracing directory.\n\t */\n\ttype = get_fs_type(\"tracefs\");\n\tif (!type)\n\t\treturn NULL;\n\tmnt = vfs_submount(mntpt, type, \"tracefs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn NULL;\n\tmntget(mnt);\n\n\treturn mnt;\n}\n\n/**\n * tracing_init_dentry - initialize top level trace array\n *\n * This is called when creating files or directories in the tracing\n * directory. It is called via fs_initcall() by any of the boot up code\n * and expects to return the dentry of the top level tracing directory.\n */\nstruct dentry *tracing_init_dentry(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\t/* The top level trace array uses  NULL as parent */\n\tif (tr->dir)\n\t\treturn NULL;\n\n\tif (WARN_ON(!tracefs_initialized()) ||\n\t\t(IS_ENABLED(CONFIG_DEBUG_FS) &&\n\t\t WARN_ON(!debugfs_initialized())))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t/*\n\t * As there may still be users that expect the tracing\n\t * files to exist in debugfs/tracing, we must automount\n\t * the tracefs file system there, so older tools still\n\t * work with the newer kerenl.\n\t */\n\ttr->dir = debugfs_create_automount(\"tracing\", NULL,\n\t\t\t\t\t   trace_automount, NULL);\n\tif (!tr->dir) {\n\t\tpr_warn_once(\"Could not create debugfs directory 'tracing'\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn NULL;\n}\n\nextern struct trace_eval_map *__start_ftrace_eval_maps[];\nextern struct trace_eval_map *__stop_ftrace_eval_maps[];\n\nstatic void __init trace_eval_init(void)\n{\n\tint len;\n\n\tlen = __stop_ftrace_eval_maps - __start_ftrace_eval_maps;\n\ttrace_insert_eval_map(NULL, __start_ftrace_eval_maps, len);\n}\n\n#ifdef CONFIG_MODULES\nstatic void trace_module_add_evals(struct module *mod)\n{\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\t/*\n\t * Modules with bad taint do not have events created, do\n\t * not bother with enums either.\n\t */\n\tif (trace_module_has_bad_taint(mod))\n\t\treturn;\n\n\ttrace_insert_eval_map(mod, mod->trace_evals, mod->num_trace_evals);\n}\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic void trace_module_remove_evals(struct module *mod)\n{\n\tunion trace_eval_map_item *map;\n\tunion trace_eval_map_item **last = &trace_eval_maps;\n\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tmap = trace_eval_maps;\n\n\twhile (map) {\n\t\tif (map->head.mod == mod)\n\t\t\tbreak;\n\t\tmap = trace_eval_jmp_to_tail(map);\n\t\tlast = &map->tail.next;\n\t\tmap = map->tail.next;\n\t}\n\tif (!map)\n\t\tgoto out;\n\n\t*last = trace_eval_jmp_to_tail(map)->tail.next;\n\tkfree(map);\n out:\n\tmutex_unlock(&trace_eval_mutex);\n}\n#else\nstatic inline void trace_module_remove_evals(struct module *mod) { }\n#endif /* CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic int trace_module_notify(struct notifier_block *self,\n\t\t\t       unsigned long val, void *data)\n{\n\tstruct module *mod = data;\n\n\tswitch (val) {\n\tcase MODULE_STATE_COMING:\n\t\ttrace_module_add_evals(mod);\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\ttrace_module_remove_evals(mod);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic struct notifier_block trace_module_nb = {\n\t.notifier_call = trace_module_notify,\n\t.priority = 0,\n};\n#endif /* CONFIG_MODULES */\n\nstatic __init int tracer_init_tracefs(void)\n{\n\tstruct dentry *d_tracer;\n\n\ttrace_access_lock_init();\n\n\td_tracer = tracing_init_dentry();\n\tif (IS_ERR(d_tracer))\n\t\treturn 0;\n\n\tevent_trace_init();\n\n\tinit_tracer_tracefs(&global_trace, d_tracer);\n\tftrace_init_tracefs_toplevel(&global_trace, d_tracer);\n\n\ttrace_create_file(\"tracing_thresh\", 0644, d_tracer,\n\t\t\t&global_trace, &tracing_thresh_fops);\n\n\ttrace_create_file(\"README\", 0444, d_tracer,\n\t\t\tNULL, &tracing_readme_fops);\n\n\ttrace_create_file(\"saved_cmdlines\", 0444, d_tracer,\n\t\t\tNULL, &tracing_saved_cmdlines_fops);\n\n\ttrace_create_file(\"saved_cmdlines_size\", 0644, d_tracer,\n\t\t\t  NULL, &tracing_saved_cmdlines_size_fops);\n\n\ttrace_create_file(\"saved_tgids\", 0444, d_tracer,\n\t\t\tNULL, &tracing_saved_tgids_fops);\n\n\ttrace_eval_init();\n\n\ttrace_create_eval_file(d_tracer);\n\n#ifdef CONFIG_MODULES\n\tregister_module_notifier(&trace_module_nb);\n#endif\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\ttrace_create_file(\"dyn_ftrace_total_info\", 0444, d_tracer,\n\t\t\t&ftrace_update_tot_cnt, &tracing_dyn_info_fops);\n#endif\n\n\tcreate_trace_instances(d_tracer);\n\n\tupdate_tracer_options(&global_trace);\n\n\treturn 0;\n}\n\nstatic int trace_panic_handler(struct notifier_block *this,\n\t\t\t       unsigned long event, void *unused)\n{\n\tif (ftrace_dump_on_oops)\n\t\tftrace_dump(ftrace_dump_on_oops);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_panic_notifier = {\n\t.notifier_call  = trace_panic_handler,\n\t.next           = NULL,\n\t.priority       = 150   /* priority: INT_MAX >= x >= 0 */\n};\n\nstatic int trace_die_handler(struct notifier_block *self,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tswitch (val) {\n\tcase DIE_OOPS:\n\t\tif (ftrace_dump_on_oops)\n\t\t\tftrace_dump(ftrace_dump_on_oops);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_die_notifier = {\n\t.notifier_call = trace_die_handler,\n\t.priority = 200\n};\n\n/*\n * printk is set to max of 1024, we really don't need it that big.\n * Nothing should be printing 1000 characters anyway.\n */\n#define TRACE_MAX_PRINT\t\t1000\n\n/*\n * Define here KERN_TRACE so that we have one place to modify\n * it if we decide to change what log level the ftrace dump\n * should be at.\n */\n#define KERN_TRACE\t\tKERN_EMERG\n\nvoid\ntrace_printk_seq(struct trace_seq *s)\n{\n\t/* Probably should print a warning here. */\n\tif (s->seq.len >= TRACE_MAX_PRINT)\n\t\ts->seq.len = TRACE_MAX_PRINT;\n\n\t/*\n\t * More paranoid code. Although the buffer size is set to\n\t * PAGE_SIZE, and TRACE_MAX_PRINT is 1000, this is just\n\t * an extra layer of protection.\n\t */\n\tif (WARN_ON_ONCE(s->seq.len >= s->seq.size))\n\t\ts->seq.len = s->seq.size - 1;\n\n\t/* should be zero ended, but we are paranoid. */\n\ts->buffer[s->seq.len] = 0;\n\n\tprintk(KERN_TRACE \"%s\", s->buffer);\n\n\ttrace_seq_init(s);\n}\n\nvoid trace_init_global_iter(struct trace_iterator *iter)\n{\n\titer->tr = &global_trace;\n\titer->trace = iter->tr->current_trace;\n\titer->cpu_file = RING_BUFFER_ALL_CPUS;\n\titer->trace_buffer = &global_trace.trace_buffer;\n\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t/* Annotate start of buffers if we had overruns */\n\tif (ring_buffer_overruns(iter->trace_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[iter->tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n}\n\nvoid ftrace_dump(enum ftrace_dump_mode oops_dump_mode)\n{\n\t/* use static because iter can be a bit big for the stack */\n\tstatic struct trace_iterator iter;\n\tstatic atomic_t dump_running;\n\tstruct trace_array *tr = &global_trace;\n\tunsigned int old_userobj;\n\tunsigned long flags;\n\tint cnt = 0, cpu;\n\n\t/* Only allow one dump user at a time. */\n\tif (atomic_inc_return(&dump_running) != 1) {\n\t\tatomic_dec(&dump_running);\n\t\treturn;\n\t}\n\n\t/*\n\t * Always turn off tracing when we dump.\n\t * We don't need to show trace output of what happens\n\t * between multiple crashes.\n\t *\n\t * If the user does a sysrq-z, then they can re-enable\n\t * tracing with echo 1 > tracing_on.\n\t */\n\ttracing_off();\n\n\tlocal_irq_save(flags);\n\n\t/* Simulate the iterator */\n\ttrace_init_global_iter(&iter);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_inc(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);\n\t}\n\n\told_userobj = tr->trace_flags & TRACE_ITER_SYM_USEROBJ;\n\n\t/* don't look at user memory in panic mode */\n\ttr->trace_flags &= ~TRACE_ITER_SYM_USEROBJ;\n\n\tswitch (oops_dump_mode) {\n\tcase DUMP_ALL:\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t\tbreak;\n\tcase DUMP_ORIG:\n\t\titer.cpu_file = raw_smp_processor_id();\n\t\tbreak;\n\tcase DUMP_NONE:\n\t\tgoto out_enable;\n\tdefault:\n\t\tprintk(KERN_TRACE \"Bad dumping mode, switching to all CPUs dump\\n\");\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t}\n\n\tprintk(KERN_TRACE \"Dumping ftrace buffer:\\n\");\n\n\t/* Did function tracer already get disabled? */\n\tif (ftrace_is_dead()) {\n\t\tprintk(\"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\");\n\t\tprintk(\"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n\t}\n\n\t/*\n\t * We need to stop all tracing on all CPUS to read the\n\t * the next buffer. This is a bit expensive, but is\n\t * not done often. We fill all what we can read,\n\t * and then release the locks again.\n\t */\n\n\twhile (!trace_empty(&iter)) {\n\n\t\tif (!cnt)\n\t\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n\t\tcnt++;\n\n\t\t/* reset all but tr, trace, and overruns */\n\t\tmemset(&iter.seq, 0,\n\t\t       sizeof(struct trace_iterator) -\n\t\t       offsetof(struct trace_iterator, seq));\n\t\titer.iter_flags |= TRACE_FILE_LAT_FMT;\n\t\titer.pos = -1;\n\n\t\tif (trace_find_next_entry_inc(&iter) != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = print_trace_line(&iter);\n\t\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\t\ttrace_consume(&iter);\n\t\t}\n\t\ttouch_nmi_watchdog();\n\n\t\ttrace_printk_seq(&iter.seq);\n\t}\n\n\tif (!cnt)\n\t\tprintk(KERN_TRACE \"   (ftrace buffer empty)\\n\");\n\telse\n\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n out_enable:\n\ttr->trace_flags |= old_userobj;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_dec(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);\n\t}\n \tatomic_dec(&dump_running);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(ftrace_dump);\n\nint trace_run_command(const char *buf, int (*createfn)(int, char **))\n{\n\tchar **argv;\n\tint argc, ret;\n\n\targc = 0;\n\tret = 0;\n\targv = argv_split(GFP_KERNEL, buf, &argc);\n\tif (!argv)\n\t\treturn -ENOMEM;\n\n\tif (argc)\n\t\tret = createfn(argc, argv);\n\n\targv_free(argv);\n\n\treturn ret;\n}\n\n#define WRITE_BUFSIZE  4096\n\nssize_t trace_parse_run_command(struct file *file, const char __user *buffer,\n\t\t\t\tsize_t count, loff_t *ppos,\n\t\t\t\tint (*createfn)(int, char **))\n{\n\tchar *kbuf, *buf, *tmp;\n\tint ret = 0;\n\tsize_t done = 0;\n\tsize_t size;\n\n\tkbuf = kmalloc(WRITE_BUFSIZE, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\twhile (done < count) {\n\t\tsize = count - done;\n\n\t\tif (size >= WRITE_BUFSIZE)\n\t\t\tsize = WRITE_BUFSIZE - 1;\n\n\t\tif (copy_from_user(kbuf, buffer + done, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkbuf[size] = '\\0';\n\t\tbuf = kbuf;\n\t\tdo {\n\t\t\ttmp = strchr(buf, '\\n');\n\t\t\tif (tmp) {\n\t\t\t\t*tmp = '\\0';\n\t\t\t\tsize = tmp - buf + 1;\n\t\t\t} else {\n\t\t\t\tsize = strlen(buf);\n\t\t\t\tif (done + size < count) {\n\t\t\t\t\tif (buf != kbuf)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t/* This can accept WRITE_BUFSIZE - 2 ('\\n' + '\\0') */\n\t\t\t\t\tpr_warn(\"Line length is too long: Should be less than %d\\n\",\n\t\t\t\t\t\tWRITE_BUFSIZE - 2);\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tdone += size;\n\n\t\t\t/* Remove comments */\n\t\t\ttmp = strchr(buf, '#');\n\n\t\t\tif (tmp)\n\t\t\t\t*tmp = '\\0';\n\n\t\t\tret = trace_run_command(buf, createfn);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbuf += size;\n\n\t\t} while (done < count);\n\t}\n\tret = done;\n\nout:\n\tkfree(kbuf);\n\n\treturn ret;\n}\n\n__init static int tracer_alloc_buffers(void)\n{\n\tint ring_buf_size;\n\tint ret = -ENOMEM;\n\n\t/*\n\t * Make sure we don't accidently add more trace options\n\t * than we have bits for.\n\t */\n\tBUILD_BUG_ON(TRACE_ITER_LAST_BIT > TRACE_FLAGS_MAX_SIZE);\n\n\tif (!alloc_cpumask_var(&tracing_buffer_mask, GFP_KERNEL))\n\t\tgoto out;\n\n\tif (!alloc_cpumask_var(&global_trace.tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_buffer_mask;\n\n\t/* Only allocate trace_printk buffers if a trace_printk exists */\n\tif (__stop___trace_bprintk_fmt != __start___trace_bprintk_fmt)\n\t\t/* Must be called before global_trace.buffer is allocated */\n\t\ttrace_printk_init_buffers();\n\n\t/* To save memory, keep the ring buffer size to its minimum */\n\tif (ring_buffer_expanded)\n\t\tring_buf_size = trace_buf_size;\n\telse\n\t\tring_buf_size = 1;\n\n\tcpumask_copy(tracing_buffer_mask, cpu_possible_mask);\n\tcpumask_copy(global_trace.tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&global_trace.start_lock);\n\n\t/*\n\t * The prepare callbacks allocates some memory for the ring buffer. We\n\t * don't free the buffer if the if the CPU goes down. If we were to free\n\t * the buffer, then the user would lose any trace that was in the\n\t * buffer. The memory will be removed once the \"instance\" is removed.\n\t */\n\tret = cpuhp_setup_state_multi(CPUHP_TRACE_RB_PREPARE,\n\t\t\t\t      \"trace/RB:preapre\", trace_rb_cpu_prepare,\n\t\t\t\t      NULL);\n\tif (ret < 0)\n\t\tgoto out_free_cpumask;\n\t/* Used for event triggers */\n\tret = -ENOMEM;\n\ttemp_buffer = ring_buffer_alloc(PAGE_SIZE, RB_FL_OVERWRITE);\n\tif (!temp_buffer)\n\t\tgoto out_rm_hp_state;\n\n\tif (trace_create_savedcmd() < 0)\n\t\tgoto out_free_temp_buffer;\n\n\t/* TODO: make the number of buffers hot pluggable with CPUS */\n\tif (allocate_trace_buffers(&global_trace, ring_buf_size) < 0) {\n\t\tprintk(KERN_ERR \"tracer: failed to allocate ring buffer!\\n\");\n\t\tWARN_ON(1);\n\t\tgoto out_free_savedcmd;\n\t}\n\n\tif (global_trace.buffer_disabled)\n\t\ttracing_off();\n\n\tif (trace_boot_clock) {\n\t\tret = tracing_set_clock(&global_trace, trace_boot_clock);\n\t\tif (ret < 0)\n\t\t\tpr_warn(\"Trace clock %s not defined, going back to default\\n\",\n\t\t\t\ttrace_boot_clock);\n\t}\n\n\t/*\n\t * register_tracer() might reference current_trace, so it\n\t * needs to be set before we register anything. This is\n\t * just a bootstrap of current_trace anyway.\n\t */\n\tglobal_trace.current_trace = &nop_trace;\n\n\tglobal_trace.max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\tftrace_init_global_array_ops(&global_trace);\n\n\tinit_trace_flags_index(&global_trace);\n\n\tregister_tracer(&nop_trace);\n\n\t/* Function tracing may start here (via kernel command line) */\n\tinit_function_trace();\n\n\t/* All seems OK, enable tracing */\n\ttracing_disabled = 0;\n\n\tatomic_notifier_chain_register(&panic_notifier_list,\n\t\t\t\t       &trace_panic_notifier);\n\n\tregister_die_notifier(&trace_die_notifier);\n\n\tglobal_trace.flags = TRACE_ARRAY_FL_GLOBAL;\n\n\tINIT_LIST_HEAD(&global_trace.systems);\n\tINIT_LIST_HEAD(&global_trace.events);\n\tINIT_LIST_HEAD(&global_trace.hist_vars);\n\tlist_add(&global_trace.list, &ftrace_trace_arrays);\n\n\tapply_trace_boot_options();\n\n\tregister_snapshot_cmd();\n\n\treturn 0;\n\nout_free_savedcmd:\n\tfree_saved_cmdlines_buffer(savedcmd);\nout_free_temp_buffer:\n\tring_buffer_free(temp_buffer);\nout_rm_hp_state:\n\tcpuhp_remove_multi_state(CPUHP_TRACE_RB_PREPARE);\nout_free_cpumask:\n\tfree_cpumask_var(global_trace.tracing_cpumask);\nout_free_buffer_mask:\n\tfree_cpumask_var(tracing_buffer_mask);\nout:\n\treturn ret;\n}\n\nvoid __init early_trace_init(void)\n{\n\tif (tracepoint_printk) {\n\t\ttracepoint_print_iter =\n\t\t\tkmalloc(sizeof(*tracepoint_print_iter), GFP_KERNEL);\n\t\tif (WARN_ON(!tracepoint_print_iter))\n\t\t\ttracepoint_printk = 0;\n\t\telse\n\t\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\t}\n\ttracer_alloc_buffers();\n}\n\nvoid __init trace_init(void)\n{\n\ttrace_event_init();\n}\n\n__init static int clear_boot_tracer(void)\n{\n\t/*\n\t * The default tracer at boot buffer is an init section.\n\t * This function is called in lateinit. If we did not\n\t * find the boot tracer, then clear it out, to prevent\n\t * later registration from accessing the buffer that is\n\t * about to be freed.\n\t */\n\tif (!default_bootup_tracer)\n\t\treturn 0;\n\n\tprintk(KERN_INFO \"ftrace bootup tracer '%s' not registered.\\n\",\n\t       default_bootup_tracer);\n\tdefault_bootup_tracer = NULL;\n\n\treturn 0;\n}\n\nfs_initcall(tracer_init_tracefs);\nlate_initcall_sync(clear_boot_tracer);\n\n#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\n__init static int tracing_set_default_clock(void)\n{\n\t/* sched_clock_stable() is determined in late_initcall */\n\tif (!trace_boot_clock && !sched_clock_stable()) {\n\t\tprintk(KERN_WARNING\n\t\t       \"Unstable clock detected, switching default tracing clock to \\\"global\\\"\\n\"\n\t\t       \"If you want to keep using the local clock, then add:\\n\"\n\t\t       \"  \\\"trace_clock=local\\\"\\n\"\n\t\t       \"on the kernel command line\\n\");\n\t\ttracing_set_clock(&global_trace, \"global\");\n\t}\n\n\treturn 0;\n}\nlate_initcall_sync(tracing_set_default_clock);\n#endif\n", "/*\n * trace_events_filter - generic event filtering\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\n *\n * Copyright (C) 2009 Tom Zanussi <tzanussi@gmail.com>\n */\n\n#include <linux/module.h>\n#include <linux/ctype.h>\n#include <linux/mutex.h>\n#include <linux/perf_event.h>\n#include <linux/slab.h>\n\n#include \"trace.h\"\n#include \"trace_output.h\"\n\n#define DEFAULT_SYS_FILTER_MESSAGE\t\t\t\t\t\\\n\t\"### global filter ###\\n\"\t\t\t\t\t\\\n\t\"# Use this to set filters for multiple events.\\n\"\t\t\\\n\t\"# Only events with the given fields will be affected.\\n\"\t\\\n\t\"# If no events are modified, an error message will be displayed here\"\n\n/* Due to token parsing '<=' must be before '<' and '>=' must be before '>' */\n#define OPS\t\t\t\t\t\\\n\tC( OP_GLOB,\t\"~\"  ),\t\t\t\\\n\tC( OP_NE,\t\"!=\" ),\t\t\t\\\n\tC( OP_EQ,\t\"==\" ),\t\t\t\\\n\tC( OP_LE,\t\"<=\" ),\t\t\t\\\n\tC( OP_LT,\t\"<\"  ),\t\t\t\\\n\tC( OP_GE,\t\">=\" ),\t\t\t\\\n\tC( OP_GT,\t\">\"  ),\t\t\t\\\n\tC( OP_BAND,\t\"&\"  ),\t\t\t\\\n\tC( OP_MAX,\tNULL )\n\n#undef C\n#define C(a, b)\ta\n\nenum filter_op_ids { OPS };\n\n#undef C\n#define C(a, b)\tb\n\nstatic const char * ops[] = { OPS };\n\n/*\n * pred functions are OP_LE, OP_LT, OP_GE, OP_GT, and OP_BAND\n * pred_funcs_##type below must match the order of them above.\n */\n#define PRED_FUNC_START\t\t\tOP_LE\n#define PRED_FUNC_MAX\t\t\t(OP_BAND - PRED_FUNC_START)\n\n#define ERRORS\t\t\t\t\t\t\t\t\\\n\tC(NONE,\t\t\t\"No error\"),\t\t\t\t\\\n\tC(INVALID_OP,\t\t\"Invalid operator\"),\t\t\t\\\n\tC(TOO_MANY_OPEN,\t\"Too many '('\"),\t\t\t\\\n\tC(TOO_MANY_CLOSE,\t\"Too few '('\"),\t\t\t\t\\\n\tC(MISSING_QUOTE,\t\"Missing matching quote\"),\t\t\\\n\tC(OPERAND_TOO_LONG,\t\"Operand too long\"),\t\t\t\\\n\tC(EXPECT_STRING,\t\"Expecting string field\"),\t\t\\\n\tC(EXPECT_DIGIT,\t\t\"Expecting numeric field\"),\t\t\\\n\tC(ILLEGAL_FIELD_OP,\t\"Illegal operation for field type\"),\t\\\n\tC(FIELD_NOT_FOUND,\t\"Field not found\"),\t\t\t\\\n\tC(ILLEGAL_INTVAL,\t\"Illegal integer value\"),\t\t\\\n\tC(BAD_SUBSYS_FILTER,\t\"Couldn't find or set field in one of a subsystem's events\"), \\\n\tC(TOO_MANY_PREDS,\t\"Too many terms in predicate expression\"), \\\n\tC(INVALID_FILTER,\t\"Meaningless filter expression\"),\t\\\n\tC(IP_FIELD_ONLY,\t\"Only 'ip' field is supported for function trace\"), \\\n\tC(INVALID_VALUE,\t\"Invalid value (did you forget quotes)?\"), \\\n\tC(NO_FILTER,\t\t\"No filter found\"),\n\n#undef C\n#define C(a, b)\t\tFILT_ERR_##a\n\nenum { ERRORS };\n\n#undef C\n#define C(a, b)\t\tb\n\nstatic char *err_text[] = { ERRORS };\n\n/* Called after a '!' character but \"!=\" and \"!~\" are not \"not\"s */\nstatic bool is_not(const char *str)\n{\n\tswitch (str[1]) {\n\tcase '=':\n\tcase '~':\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * prog_entry - a singe entry in the filter program\n * @target:\t     Index to jump to on a branch (actually one minus the index)\n * @when_to_branch:  The value of the result of the predicate to do a branch\n * @pred:\t     The predicate to execute.\n */\nstruct prog_entry {\n\tint\t\t\ttarget;\n\tint\t\t\twhen_to_branch;\n\tstruct filter_pred\t*pred;\n};\n\n/**\n * update_preds- assign a program entry a label target\n * @prog: The program array\n * @N: The index of the current entry in @prog\n * @when_to_branch: What to assign a program entry for its branch condition\n *\n * The program entry at @N has a target that points to the index of a program\n * entry that can have its target and when_to_branch fields updated.\n * Update the current program entry denoted by index @N target field to be\n * that of the updated entry. This will denote the entry to update if\n * we are processing an \"||\" after an \"&&\"\n */\nstatic void update_preds(struct prog_entry *prog, int N, int invert)\n{\n\tint t, s;\n\n\tt = prog[N].target;\n\ts = prog[t].target;\n\tprog[t].when_to_branch = invert;\n\tprog[t].target = N;\n\tprog[N].target = s;\n}\n\nstruct filter_parse_error {\n\tint lasterr;\n\tint lasterr_pos;\n};\n\nstatic void parse_error(struct filter_parse_error *pe, int err, int pos)\n{\n\tpe->lasterr = err;\n\tpe->lasterr_pos = pos;\n}\n\ntypedef int (*parse_pred_fn)(const char *str, void *data, int pos,\n\t\t\t     struct filter_parse_error *pe,\n\t\t\t     struct filter_pred **pred);\n\nenum {\n\tINVERT\t\t= 1,\n\tPROCESS_AND\t= 2,\n\tPROCESS_OR\t= 4,\n};\n\n/*\n * Without going into a formal proof, this explains the method that is used in\n * parsing the logical expressions.\n *\n * For example, if we have: \"a && !(!b || (c && g)) || d || e && !f\"\n * The first pass will convert it into the following program:\n *\n * n1: r=a;       l1: if (!r) goto l4;\n * n2: r=b;       l2: if (!r) goto l4;\n * n3: r=c; r=!r; l3: if (r) goto l4;\n * n4: r=g; r=!r; l4: if (r) goto l5;\n * n5: r=d;       l5: if (r) goto T\n * n6: r=e;       l6: if (!r) goto l7;\n * n7: r=f; r=!r; l7: if (!r) goto F\n * T: return TRUE\n * F: return FALSE\n *\n * To do this, we use a data structure to represent each of the above\n * predicate and conditions that has:\n *\n *  predicate, when_to_branch, invert, target\n *\n * The \"predicate\" will hold the function to determine the result \"r\".\n * The \"when_to_branch\" denotes what \"r\" should be if a branch is to be taken\n * \"&&\" would contain \"!r\" or (0) and \"||\" would contain \"r\" or (1).\n * The \"invert\" holds whether the value should be reversed before testing.\n * The \"target\" contains the label \"l#\" to jump to.\n *\n * A stack is created to hold values when parentheses are used.\n *\n * To simplify the logic, the labels will start at 0 and not 1.\n *\n * The possible invert values are 1 and 0. The number of \"!\"s that are in scope\n * before the predicate determines the invert value, if the number is odd then\n * the invert value is 1 and 0 otherwise. This means the invert value only\n * needs to be toggled when a new \"!\" is introduced compared to what is stored\n * on the stack, where parentheses were used.\n *\n * The top of the stack and \"invert\" are initialized to zero.\n *\n * ** FIRST PASS **\n *\n * #1 A loop through all the tokens is done:\n *\n * #2 If the token is an \"(\", the stack is push, and the current stack value\n *    gets the current invert value, and the loop continues to the next token.\n *    The top of the stack saves the \"invert\" value to keep track of what\n *    the current inversion is. As \"!(a && !b || c)\" would require all\n *    predicates being affected separately by the \"!\" before the parentheses.\n *    And that would end up being equivalent to \"(!a || b) && !c\"\n *\n * #3 If the token is an \"!\", the current \"invert\" value gets inverted, and\n *    the loop continues. Note, if the next token is a predicate, then\n *    this \"invert\" value is only valid for the current program entry,\n *    and does not affect other predicates later on.\n *\n * The only other acceptable token is the predicate string.\n *\n * #4 A new entry into the program is added saving: the predicate and the\n *    current value of \"invert\". The target is currently assigned to the\n *    previous program index (this will not be its final value).\n *\n * #5 We now enter another loop and look at the next token. The only valid\n *    tokens are \")\", \"&&\", \"||\" or end of the input string \"\\0\".\n *\n * #6 The invert variable is reset to the current value saved on the top of\n *    the stack.\n *\n * #7 The top of the stack holds not only the current invert value, but also\n *    if a \"&&\" or \"||\" needs to be processed. Note, the \"&&\" takes higher\n *    precedence than \"||\". That is \"a && b || c && d\" is equivalent to\n *    \"(a && b) || (c && d)\". Thus the first thing to do is to see if \"&&\" needs\n *    to be processed. This is the case if an \"&&\" was the last token. If it was\n *    then we call update_preds(). This takes the program, the current index in\n *    the program, and the current value of \"invert\".  More will be described\n *    below about this function.\n *\n * #8 If the next token is \"&&\" then we set a flag in the top of the stack\n *    that denotes that \"&&\" needs to be processed, break out of this loop\n *    and continue with the outer loop.\n *\n * #9 Otherwise, if a \"||\" needs to be processed then update_preds() is called.\n *    This is called with the program, the current index in the program, but\n *    this time with an inverted value of \"invert\" (that is !invert). This is\n *    because the value taken will become the \"when_to_branch\" value of the\n *    program.\n *    Note, this is called when the next token is not an \"&&\". As stated before,\n *    \"&&\" takes higher precedence, and \"||\" should not be processed yet if the\n *    next logical operation is \"&&\".\n *\n * #10 If the next token is \"||\" then we set a flag in the top of the stack\n *     that denotes that \"||\" needs to be processed, break out of this loop\n *     and continue with the outer loop.\n *\n * #11 If this is the end of the input string \"\\0\" then we break out of both\n *     loops.\n *\n * #12 Otherwise, the next token is \")\", where we pop the stack and continue\n *     this inner loop.\n *\n * Now to discuss the update_pred() function, as that is key to the setting up\n * of the program. Remember the \"target\" of the program is initialized to the\n * previous index and not the \"l\" label. The target holds the index into the\n * program that gets affected by the operand. Thus if we have something like\n *  \"a || b && c\", when we process \"a\" the target will be \"-1\" (undefined).\n * When we process \"b\", its target is \"0\", which is the index of \"a\", as that's\n * the predicate that is affected by \"||\". But because the next token after \"b\"\n * is \"&&\" we don't call update_preds(). Instead continue to \"c\". As the\n * next token after \"c\" is not \"&&\" but the end of input, we first process the\n * \"&&\" by calling update_preds() for the \"&&\" then we process the \"||\" by\n * callin updates_preds() with the values for processing \"||\".\n *\n * What does that mean? What update_preds() does is to first save the \"target\"\n * of the program entry indexed by the current program entry's \"target\"\n * (remember the \"target\" is initialized to previous program entry), and then\n * sets that \"target\" to the current index which represents the label \"l#\".\n * That entry's \"when_to_branch\" is set to the value passed in (the \"invert\"\n * or \"!invert\"). Then it sets the current program entry's target to the saved\n * \"target\" value (the old value of the program that had its \"target\" updated\n * to the label).\n *\n * Looking back at \"a || b && c\", we have the following steps:\n *  \"a\"  - prog[0] = { \"a\", X, -1 } // pred, when_to_branch, target\n *  \"||\" - flag that we need to process \"||\"; continue outer loop\n *  \"b\"  - prog[1] = { \"b\", X, 0 }\n *  \"&&\" - flag that we need to process \"&&\"; continue outer loop\n * (Notice we did not process \"||\")\n *  \"c\"  - prog[2] = { \"c\", X, 1 }\n *  update_preds(prog, 2, 0); // invert = 0 as we are processing \"&&\"\n *    t = prog[2].target; // t = 1\n *    s = prog[t].target; // s = 0\n *    prog[t].target = 2; // Set target to \"l2\"\n *    prog[t].when_to_branch = 0;\n *    prog[2].target = s;\n * update_preds(prog, 2, 1); // invert = 1 as we are now processing \"||\"\n *    t = prog[2].target; // t = 0\n *    s = prog[t].target; // s = -1\n *    prog[t].target = 2; // Set target to \"l2\"\n *    prog[t].when_to_branch = 1;\n *    prog[2].target = s;\n *\n * #13 Which brings us to the final step of the first pass, which is to set\n *     the last program entry's when_to_branch and target, which will be\n *     when_to_branch = 0; target = N; ( the label after the program entry after\n *     the last program entry processed above).\n *\n * If we denote \"TRUE\" to be the entry after the last program entry processed,\n * and \"FALSE\" the program entry after that, we are now done with the first\n * pass.\n *\n * Making the above \"a || b && c\" have a progam of:\n *  prog[0] = { \"a\", 1, 2 }\n *  prog[1] = { \"b\", 0, 2 }\n *  prog[2] = { \"c\", 0, 3 }\n *\n * Which translates into:\n * n0: r = a; l0: if (r) goto l2;\n * n1: r = b; l1: if (!r) goto l2;\n * n2: r = c; l2: if (!r) goto l3;  // Which is the same as \"goto F;\"\n * T: return TRUE; l3:\n * F: return FALSE\n *\n * Although, after the first pass, the program is correct, it is\n * inefficient. The simple sample of \"a || b && c\" could be easily been\n * converted into:\n * n0: r = a; if (r) goto T\n * n1: r = b; if (!r) goto F\n * n2: r = c; if (!r) goto F\n * T: return TRUE;\n * F: return FALSE;\n *\n * The First Pass is over the input string. The next too passes are over\n * the program itself.\n *\n * ** SECOND PASS **\n *\n * Which brings us to the second pass. If a jump to a label has the\n * same condition as that label, it can instead jump to its target.\n * The original example of \"a && !(!b || (c && g)) || d || e && !f\"\n * where the first pass gives us:\n *\n * n1: r=a;       l1: if (!r) goto l4;\n * n2: r=b;       l2: if (!r) goto l4;\n * n3: r=c; r=!r; l3: if (r) goto l4;\n * n4: r=g; r=!r; l4: if (r) goto l5;\n * n5: r=d;       l5: if (r) goto T\n * n6: r=e;       l6: if (!r) goto l7;\n * n7: r=f; r=!r; l7: if (!r) goto F:\n * T: return TRUE;\n * F: return FALSE\n *\n * We can see that \"l3: if (r) goto l4;\" and at l4, we have \"if (r) goto l5;\".\n * And \"l5: if (r) goto T\", we could optimize this by converting l3 and l4\n * to go directly to T. To accomplish this, we start from the last\n * entry in the program and work our way back. If the target of the entry\n * has the same \"when_to_branch\" then we could use that entry's target.\n * Doing this, the above would end up as:\n *\n * n1: r=a;       l1: if (!r) goto l4;\n * n2: r=b;       l2: if (!r) goto l4;\n * n3: r=c; r=!r; l3: if (r) goto T;\n * n4: r=g; r=!r; l4: if (r) goto T;\n * n5: r=d;       l5: if (r) goto T;\n * n6: r=e;       l6: if (!r) goto F;\n * n7: r=f; r=!r; l7: if (!r) goto F;\n * T: return TRUE\n * F: return FALSE\n *\n * In that same pass, if the \"when_to_branch\" doesn't match, we can simply\n * go to the program entry after the label. That is, \"l2: if (!r) goto l4;\"\n * where \"l4: if (r) goto T;\", then we can convert l2 to be:\n * \"l2: if (!r) goto n5;\".\n *\n * This will have the second pass give us:\n * n1: r=a;       l1: if (!r) goto n5;\n * n2: r=b;       l2: if (!r) goto n5;\n * n3: r=c; r=!r; l3: if (r) goto T;\n * n4: r=g; r=!r; l4: if (r) goto T;\n * n5: r=d;       l5: if (r) goto T\n * n6: r=e;       l6: if (!r) goto F;\n * n7: r=f; r=!r; l7: if (!r) goto F\n * T: return TRUE\n * F: return FALSE\n *\n * Notice, all the \"l#\" labels are no longer used, and they can now\n * be discarded.\n *\n * ** THIRD PASS **\n *\n * For the third pass we deal with the inverts. As they simply just\n * make the \"when_to_branch\" get inverted, a simple loop over the\n * program to that does: \"when_to_branch ^= invert;\" will do the\n * job, leaving us with:\n * n1: r=a; if (!r) goto n5;\n * n2: r=b; if (!r) goto n5;\n * n3: r=c: if (!r) goto T;\n * n4: r=g; if (!r) goto T;\n * n5: r=d; if (r) goto T\n * n6: r=e; if (!r) goto F;\n * n7: r=f; if (r) goto F\n * T: return TRUE\n * F: return FALSE\n *\n * As \"r = a; if (!r) goto n5;\" is obviously the same as\n * \"if (!a) goto n5;\" without doing anything we can interperate the\n * program as:\n * n1: if (!a) goto n5;\n * n2: if (!b) goto n5;\n * n3: if (!c) goto T;\n * n4: if (!g) goto T;\n * n5: if (d) goto T\n * n6: if (!e) goto F;\n * n7: if (f) goto F\n * T: return TRUE\n * F: return FALSE\n *\n * Since the inverts are discarded at the end, there's no reason to store\n * them in the program array (and waste memory). A separate array to hold\n * the inverts is used and freed at the end.\n */\nstatic struct prog_entry *\npredicate_parse(const char *str, int nr_parens, int nr_preds,\n\t\tparse_pred_fn parse_pred, void *data,\n\t\tstruct filter_parse_error *pe)\n{\n\tstruct prog_entry *prog_stack;\n\tstruct prog_entry *prog;\n\tconst char *ptr = str;\n\tchar *inverts = NULL;\n\tint *op_stack;\n\tint *top;\n\tint invert = 0;\n\tint ret = -ENOMEM;\n\tint len;\n\tint N = 0;\n\tint i;\n\n\tnr_preds += 2; /* For TRUE and FALSE */\n\n\top_stack = kmalloc_array(nr_parens, sizeof(*op_stack), GFP_KERNEL);\n\tif (!op_stack)\n\t\treturn ERR_PTR(-ENOMEM);\n\tprog_stack = kmalloc_array(nr_preds, sizeof(*prog_stack), GFP_KERNEL);\n\tif (!prog_stack) {\n\t\tparse_error(pe, -ENOMEM, 0);\n\t\tgoto out_free;\n\t}\n\tinverts = kmalloc_array(nr_preds, sizeof(*inverts), GFP_KERNEL);\n\tif (!inverts) {\n\t\tparse_error(pe, -ENOMEM, 0);\n\t\tgoto out_free;\n\t}\n\n\ttop = op_stack;\n\tprog = prog_stack;\n\t*top = 0;\n\n\t/* First pass */\n\twhile (*ptr) {\t\t\t\t\t\t/* #1 */\n\t\tconst char *next = ptr++;\n\n\t\tif (isspace(*next))\n\t\t\tcontinue;\n\n\t\tswitch (*next) {\n\t\tcase '(':\t\t\t\t\t/* #2 */\n\t\t\tif (top - op_stack > nr_parens)\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t*(++top) = invert;\n\t\t\tcontinue;\n\t\tcase '!':\t\t\t\t\t/* #3 */\n\t\t\tif (!is_not(next))\n\t\t\t\tbreak;\n\t\t\tinvert = !invert;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (N >= nr_preds) {\n\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_PREDS, next - str);\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tinverts[N] = invert;\t\t\t\t/* #4 */\n\t\tprog[N].target = N-1;\n\n\t\tlen = parse_pred(next, data, ptr - str, pe, &prog[N].pred);\n\t\tif (len < 0) {\n\t\t\tret = len;\n\t\t\tgoto out_free;\n\t\t}\n\t\tptr = next + len;\n\n\t\tN++;\n\n\t\tret = -1;\n\t\twhile (1) {\t\t\t\t\t/* #5 */\n\t\t\tnext = ptr++;\n\t\t\tif (isspace(*next))\n\t\t\t\tcontinue;\n\n\t\t\tswitch (*next) {\n\t\t\tcase ')':\n\t\t\tcase '\\0':\n\t\t\t\tbreak;\n\t\t\tcase '&':\n\t\t\tcase '|':\n\t\t\t\tif (next[1] == next[0]) {\n\t\t\t\t\tptr++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_PREDS,\n\t\t\t\t\t    next - str);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\tinvert = *top & INVERT;\n\n\t\t\tif (*top & PROCESS_AND) {\t\t/* #7 */\n\t\t\t\tupdate_preds(prog, N - 1, invert);\n\t\t\t\t*top &= ~PROCESS_AND;\n\t\t\t}\n\t\t\tif (*next == '&') {\t\t\t/* #8 */\n\t\t\t\t*top |= PROCESS_AND;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (*top & PROCESS_OR) {\t\t/* #9 */\n\t\t\t\tupdate_preds(prog, N - 1, !invert);\n\t\t\t\t*top &= ~PROCESS_OR;\n\t\t\t}\n\t\t\tif (*next == '|') {\t\t\t/* #10 */\n\t\t\t\t*top |= PROCESS_OR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!*next)\t\t\t\t/* #11 */\n\t\t\t\tgoto out;\n\n\t\t\tif (top == op_stack) {\n\t\t\t\tret = -1;\n\t\t\t\t/* Too few '(' */\n\t\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_CLOSE, ptr - str);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\ttop--;\t\t\t\t\t/* #12 */\n\t\t}\n\t}\n out:\n\tif (top != op_stack) {\n\t\t/* Too many '(' */\n\t\tparse_error(pe, FILT_ERR_TOO_MANY_OPEN, ptr - str);\n\t\tgoto out_free;\n\t}\n\n\tif (!N) {\n\t\t/* No program? */\n\t\tret = -EINVAL;\n\t\tparse_error(pe, FILT_ERR_NO_FILTER, ptr - str);\n\t\tgoto out_free;\n\t}\n\n\tprog[N].pred = NULL;\t\t\t\t\t/* #13 */\n\tprog[N].target = 1;\t\t/* TRUE */\n\tprog[N+1].pred = NULL;\n\tprog[N+1].target = 0;\t\t/* FALSE */\n\tprog[N-1].target = N;\n\tprog[N-1].when_to_branch = false;\n\n\t/* Second Pass */\n\tfor (i = N-1 ; i--; ) {\n\t\tint target = prog[i].target;\n\t\tif (prog[i].when_to_branch == prog[target].when_to_branch)\n\t\t\tprog[i].target = prog[target].target;\n\t}\n\n\t/* Third Pass */\n\tfor (i = 0; i < N; i++) {\n\t\tinvert = inverts[i] ^ prog[i].when_to_branch;\n\t\tprog[i].when_to_branch = invert;\n\t\t/* Make sure the program always moves forward */\n\t\tif (WARN_ON(prog[i].target <= i)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\treturn prog;\nout_free:\n\tkfree(op_stack);\n\tkfree(prog_stack);\n\tkfree(inverts);\n\treturn ERR_PTR(ret);\n}\n\n#define DEFINE_COMPARISON_PRED(type)\t\t\t\t\t\\\nstatic int filter_pred_LT_##type(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn *addr < val;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic int filter_pred_LE_##type(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn *addr <= val;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic int filter_pred_GT_##type(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn *addr > val;\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic int filter_pred_GE_##type(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn *addr >= val;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic int filter_pred_BAND_##type(struct filter_pred *pred, void *event) \\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *addr = (type *)(event + pred->offset);\t\t\t\\\n\ttype val = (type)pred->val;\t\t\t\t\t\\\n\treturn !!(*addr & val);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic const filter_pred_fn_t pred_funcs_##type[] = {\t\t\t\\\n\tfilter_pred_LE_##type,\t\t\t\t\t\t\\\n\tfilter_pred_LT_##type,\t\t\t\t\t\t\\\n\tfilter_pred_GE_##type,\t\t\t\t\t\t\\\n\tfilter_pred_GT_##type,\t\t\t\t\t\t\\\n\tfilter_pred_BAND_##type,\t\t\t\t\t\\\n};\n\n#define DEFINE_EQUALITY_PRED(size)\t\t\t\t\t\\\nstatic int filter_pred_##size(struct filter_pred *pred, void *event)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu##size *addr = (u##size *)(event + pred->offset);\t\t\\\n\tu##size val = (u##size)pred->val;\t\t\t\t\\\n\tint match;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tmatch = (val == *addr) ^ pred->not;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn match;\t\t\t\t\t\t\t\\\n}\n\nDEFINE_COMPARISON_PRED(s64);\nDEFINE_COMPARISON_PRED(u64);\nDEFINE_COMPARISON_PRED(s32);\nDEFINE_COMPARISON_PRED(u32);\nDEFINE_COMPARISON_PRED(s16);\nDEFINE_COMPARISON_PRED(u16);\nDEFINE_COMPARISON_PRED(s8);\nDEFINE_COMPARISON_PRED(u8);\n\nDEFINE_EQUALITY_PRED(64);\nDEFINE_EQUALITY_PRED(32);\nDEFINE_EQUALITY_PRED(16);\nDEFINE_EQUALITY_PRED(8);\n\n/* Filter predicate for fixed sized arrays of characters */\nstatic int filter_pred_string(struct filter_pred *pred, void *event)\n{\n\tchar *addr = (char *)(event + pred->offset);\n\tint cmp, match;\n\n\tcmp = pred->regex.match(addr, &pred->regex, pred->regex.field_len);\n\n\tmatch = cmp ^ pred->not;\n\n\treturn match;\n}\n\n/* Filter predicate for char * pointers */\nstatic int filter_pred_pchar(struct filter_pred *pred, void *event)\n{\n\tchar **addr = (char **)(event + pred->offset);\n\tint cmp, match;\n\tint len = strlen(*addr) + 1;\t/* including tailing '\\0' */\n\n\tcmp = pred->regex.match(*addr, &pred->regex, len);\n\n\tmatch = cmp ^ pred->not;\n\n\treturn match;\n}\n\n/*\n * Filter predicate for dynamic sized arrays of characters.\n * These are implemented through a list of strings at the end\n * of the entry.\n * Also each of these strings have a field in the entry which\n * contains its offset from the beginning of the entry.\n * We have then first to get this field, dereference it\n * and add it to the address of the entry, and at last we have\n * the address of the string.\n */\nstatic int filter_pred_strloc(struct filter_pred *pred, void *event)\n{\n\tu32 str_item = *(u32 *)(event + pred->offset);\n\tint str_loc = str_item & 0xffff;\n\tint str_len = str_item >> 16;\n\tchar *addr = (char *)(event + str_loc);\n\tint cmp, match;\n\n\tcmp = pred->regex.match(addr, &pred->regex, str_len);\n\n\tmatch = cmp ^ pred->not;\n\n\treturn match;\n}\n\n/* Filter predicate for CPUs. */\nstatic int filter_pred_cpu(struct filter_pred *pred, void *event)\n{\n\tint cpu, cmp;\n\n\tcpu = raw_smp_processor_id();\n\tcmp = pred->val;\n\n\tswitch (pred->op) {\n\tcase OP_EQ:\n\t\treturn cpu == cmp;\n\tcase OP_NE:\n\t\treturn cpu != cmp;\n\tcase OP_LT:\n\t\treturn cpu < cmp;\n\tcase OP_LE:\n\t\treturn cpu <= cmp;\n\tcase OP_GT:\n\t\treturn cpu > cmp;\n\tcase OP_GE:\n\t\treturn cpu >= cmp;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n/* Filter predicate for COMM. */\nstatic int filter_pred_comm(struct filter_pred *pred, void *event)\n{\n\tint cmp;\n\n\tcmp = pred->regex.match(current->comm, &pred->regex,\n\t\t\t\tTASK_COMM_LEN);\n\treturn cmp ^ pred->not;\n}\n\nstatic int filter_pred_none(struct filter_pred *pred, void *event)\n{\n\treturn 0;\n}\n\n/*\n * regex_match_foo - Basic regex callbacks\n *\n * @str: the string to be searched\n * @r:   the regex structure containing the pattern string\n * @len: the length of the string to be searched (including '\\0')\n *\n * Note:\n * - @str might not be NULL-terminated if it's of type DYN_STRING\n *   or STATIC_STRING, unless @len is zero.\n */\n\nstatic int regex_match_full(char *str, struct regex *r, int len)\n{\n\t/* len of zero means str is dynamic and ends with '\\0' */\n\tif (!len)\n\t\treturn strcmp(str, r->pattern) == 0;\n\n\treturn strncmp(str, r->pattern, len) == 0;\n}\n\nstatic int regex_match_front(char *str, struct regex *r, int len)\n{\n\tif (len && len < r->len)\n\t\treturn 0;\n\n\treturn strncmp(str, r->pattern, r->len) == 0;\n}\n\nstatic int regex_match_middle(char *str, struct regex *r, int len)\n{\n\tif (!len)\n\t\treturn strstr(str, r->pattern) != NULL;\n\n\treturn strnstr(str, r->pattern, len) != NULL;\n}\n\nstatic int regex_match_end(char *str, struct regex *r, int len)\n{\n\tint strlen = len - 1;\n\n\tif (strlen >= r->len &&\n\t    memcmp(str + strlen - r->len, r->pattern, r->len) == 0)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int regex_match_glob(char *str, struct regex *r, int len __maybe_unused)\n{\n\tif (glob_match(r->pattern, str))\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n * filter_parse_regex - parse a basic regex\n * @buff:   the raw regex\n * @len:    length of the regex\n * @search: will point to the beginning of the string to compare\n * @not:    tell whether the match will have to be inverted\n *\n * This passes in a buffer containing a regex and this function will\n * set search to point to the search part of the buffer and\n * return the type of search it is (see enum above).\n * This does modify buff.\n *\n * Returns enum type.\n *  search returns the pointer to use for comparison.\n *  not returns 1 if buff started with a '!'\n *     0 otherwise.\n */\nenum regex_type filter_parse_regex(char *buff, int len, char **search, int *not)\n{\n\tint type = MATCH_FULL;\n\tint i;\n\n\tif (buff[0] == '!') {\n\t\t*not = 1;\n\t\tbuff++;\n\t\tlen--;\n\t} else\n\t\t*not = 0;\n\n\t*search = buff;\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (buff[i] == '*') {\n\t\t\tif (!i) {\n\t\t\t\ttype = MATCH_END_ONLY;\n\t\t\t} else if (i == len - 1) {\n\t\t\t\tif (type == MATCH_END_ONLY)\n\t\t\t\t\ttype = MATCH_MIDDLE_ONLY;\n\t\t\t\telse\n\t\t\t\t\ttype = MATCH_FRONT_ONLY;\n\t\t\t\tbuff[i] = 0;\n\t\t\t\tbreak;\n\t\t\t} else {\t/* pattern continues, use full glob */\n\t\t\t\treturn MATCH_GLOB;\n\t\t\t}\n\t\t} else if (strchr(\"[?\\\\\", buff[i])) {\n\t\t\treturn MATCH_GLOB;\n\t\t}\n\t}\n\tif (buff[0] == '*')\n\t\t*search = buff + 1;\n\n\treturn type;\n}\n\nstatic void filter_build_regex(struct filter_pred *pred)\n{\n\tstruct regex *r = &pred->regex;\n\tchar *search;\n\tenum regex_type type = MATCH_FULL;\n\n\tif (pred->op == OP_GLOB) {\n\t\ttype = filter_parse_regex(r->pattern, r->len, &search, &pred->not);\n\t\tr->len = strlen(search);\n\t\tmemmove(r->pattern, search, r->len+1);\n\t}\n\n\tswitch (type) {\n\tcase MATCH_FULL:\n\t\tr->match = regex_match_full;\n\t\tbreak;\n\tcase MATCH_FRONT_ONLY:\n\t\tr->match = regex_match_front;\n\t\tbreak;\n\tcase MATCH_MIDDLE_ONLY:\n\t\tr->match = regex_match_middle;\n\t\tbreak;\n\tcase MATCH_END_ONLY:\n\t\tr->match = regex_match_end;\n\t\tbreak;\n\tcase MATCH_GLOB:\n\t\tr->match = regex_match_glob;\n\t\tbreak;\n\t}\n}\n\n/* return 1 if event matches, 0 otherwise (discard) */\nint filter_match_preds(struct event_filter *filter, void *rec)\n{\n\tstruct prog_entry *prog;\n\tint i;\n\n\t/* no filter is considered a match */\n\tif (!filter)\n\t\treturn 1;\n\n\tprog = rcu_dereference_sched(filter->prog);\n\tif (!prog)\n\t\treturn 1;\n\n\tfor (i = 0; prog[i].pred; i++) {\n\t\tstruct filter_pred *pred = prog[i].pred;\n\t\tint match = pred->fn(pred, rec);\n\t\tif (match == prog[i].when_to_branch)\n\t\t\ti = prog[i].target;\n\t}\n\treturn prog[i].target;\n}\nEXPORT_SYMBOL_GPL(filter_match_preds);\n\nstatic void remove_filter_string(struct event_filter *filter)\n{\n\tif (!filter)\n\t\treturn;\n\n\tkfree(filter->filter_string);\n\tfilter->filter_string = NULL;\n}\n\nstatic void append_filter_err(struct filter_parse_error *pe,\n\t\t\t      struct event_filter *filter)\n{\n\tstruct trace_seq *s;\n\tint pos = pe->lasterr_pos;\n\tchar *buf;\n\tint len;\n\n\tif (WARN_ON(!filter->filter_string))\n\t\treturn;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn;\n\ttrace_seq_init(s);\n\n\tlen = strlen(filter->filter_string);\n\tif (pos > len)\n\t\tpos = len;\n\n\t/* indexing is off by one */\n\tif (pos)\n\t\tpos++;\n\n\ttrace_seq_puts(s, filter->filter_string);\n\tif (pe->lasterr > 0) {\n\t\ttrace_seq_printf(s, \"\\n%*s\", pos, \"^\");\n\t\ttrace_seq_printf(s, \"\\nparse_error: %s\\n\", err_text[pe->lasterr]);\n\t} else {\n\t\ttrace_seq_printf(s, \"\\nError: (%d)\\n\", pe->lasterr);\n\t}\n\ttrace_seq_putc(s, 0);\n\tbuf = kmemdup_nul(s->buffer, s->seq.len, GFP_KERNEL);\n\tif (buf) {\n\t\tkfree(filter->filter_string);\n\t\tfilter->filter_string = buf;\n\t}\n\tkfree(s);\n}\n\nstatic inline struct event_filter *event_filter(struct trace_event_file *file)\n{\n\treturn file->filter;\n}\n\n/* caller must hold event_mutex */\nvoid print_event_filter(struct trace_event_file *file, struct trace_seq *s)\n{\n\tstruct event_filter *filter = event_filter(file);\n\n\tif (filter && filter->filter_string)\n\t\ttrace_seq_printf(s, \"%s\\n\", filter->filter_string);\n\telse\n\t\ttrace_seq_puts(s, \"none\\n\");\n}\n\nvoid print_subsystem_event_filter(struct event_subsystem *system,\n\t\t\t\t  struct trace_seq *s)\n{\n\tstruct event_filter *filter;\n\n\tmutex_lock(&event_mutex);\n\tfilter = system->filter;\n\tif (filter && filter->filter_string)\n\t\ttrace_seq_printf(s, \"%s\\n\", filter->filter_string);\n\telse\n\t\ttrace_seq_puts(s, DEFAULT_SYS_FILTER_MESSAGE \"\\n\");\n\tmutex_unlock(&event_mutex);\n}\n\nstatic void free_prog(struct event_filter *filter)\n{\n\tstruct prog_entry *prog;\n\tint i;\n\n\tprog = rcu_access_pointer(filter->prog);\n\tif (!prog)\n\t\treturn;\n\n\tfor (i = 0; prog[i].pred; i++)\n\t\tkfree(prog[i].pred);\n\tkfree(prog);\n}\n\nstatic void filter_disable(struct trace_event_file *file)\n{\n\tunsigned long old_flags = file->flags;\n\n\tfile->flags &= ~EVENT_FILE_FL_FILTERED;\n\n\tif (old_flags != file->flags)\n\t\ttrace_buffered_event_disable();\n}\n\nstatic void __free_filter(struct event_filter *filter)\n{\n\tif (!filter)\n\t\treturn;\n\n\tfree_prog(filter);\n\tkfree(filter->filter_string);\n\tkfree(filter);\n}\n\nvoid free_event_filter(struct event_filter *filter)\n{\n\t__free_filter(filter);\n}\n\nstatic inline void __remove_filter(struct trace_event_file *file)\n{\n\tfilter_disable(file);\n\tremove_filter_string(file->filter);\n}\n\nstatic void filter_free_subsystem_preds(struct trace_subsystem_dir *dir,\n\t\t\t\t\tstruct trace_array *tr)\n{\n\tstruct trace_event_file *file;\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\t\tif (file->system != dir)\n\t\t\tcontinue;\n\t\t__remove_filter(file);\n\t}\n}\n\nstatic inline void __free_subsystem_filter(struct trace_event_file *file)\n{\n\t__free_filter(file->filter);\n\tfile->filter = NULL;\n}\n\nstatic void filter_free_subsystem_filters(struct trace_subsystem_dir *dir,\n\t\t\t\t\t  struct trace_array *tr)\n{\n\tstruct trace_event_file *file;\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\t\tif (file->system != dir)\n\t\t\tcontinue;\n\t\t__free_subsystem_filter(file);\n\t}\n}\n\nint filter_assign_type(const char *type)\n{\n\tif (strstr(type, \"__data_loc\") && strstr(type, \"char\"))\n\t\treturn FILTER_DYN_STRING;\n\n\tif (strchr(type, '[') && strstr(type, \"char\"))\n\t\treturn FILTER_STATIC_STRING;\n\n\treturn FILTER_OTHER;\n}\n\nstatic filter_pred_fn_t select_comparison_fn(enum filter_op_ids op,\n\t\t\t\t\t    int field_size, int field_is_signed)\n{\n\tfilter_pred_fn_t fn = NULL;\n\tint pred_func_index = -1;\n\n\tswitch (op) {\n\tcase OP_EQ:\n\tcase OP_NE:\n\t\tbreak;\n\tdefault:\n\t\tif (WARN_ON_ONCE(op < PRED_FUNC_START))\n\t\t\treturn NULL;\n\t\tpred_func_index = op - PRED_FUNC_START;\n\t\tif (WARN_ON_ONCE(pred_func_index > PRED_FUNC_MAX))\n\t\t\treturn NULL;\n\t}\n\n\tswitch (field_size) {\n\tcase 8:\n\t\tif (pred_func_index < 0)\n\t\t\tfn = filter_pred_64;\n\t\telse if (field_is_signed)\n\t\t\tfn = pred_funcs_s64[pred_func_index];\n\t\telse\n\t\t\tfn = pred_funcs_u64[pred_func_index];\n\t\tbreak;\n\tcase 4:\n\t\tif (pred_func_index < 0)\n\t\t\tfn = filter_pred_32;\n\t\telse if (field_is_signed)\n\t\t\tfn = pred_funcs_s32[pred_func_index];\n\t\telse\n\t\t\tfn = pred_funcs_u32[pred_func_index];\n\t\tbreak;\n\tcase 2:\n\t\tif (pred_func_index < 0)\n\t\t\tfn = filter_pred_16;\n\t\telse if (field_is_signed)\n\t\t\tfn = pred_funcs_s16[pred_func_index];\n\t\telse\n\t\t\tfn = pred_funcs_u16[pred_func_index];\n\t\tbreak;\n\tcase 1:\n\t\tif (pred_func_index < 0)\n\t\t\tfn = filter_pred_8;\n\t\telse if (field_is_signed)\n\t\t\tfn = pred_funcs_s8[pred_func_index];\n\t\telse\n\t\t\tfn = pred_funcs_u8[pred_func_index];\n\t\tbreak;\n\t}\n\n\treturn fn;\n}\n\n/* Called when a predicate is encountered by predicate_parse() */\nstatic int parse_pred(const char *str, void *data,\n\t\t      int pos, struct filter_parse_error *pe,\n\t\t      struct filter_pred **pred_ptr)\n{\n\tstruct trace_event_call *call = data;\n\tstruct ftrace_event_field *field;\n\tstruct filter_pred *pred = NULL;\n\tchar num_buf[24];\t/* Big enough to hold an address */\n\tchar *field_name;\n\tchar q;\n\tu64 val;\n\tint len;\n\tint ret;\n\tint op;\n\tint s;\n\tint i = 0;\n\n\t/* First find the field to associate to */\n\twhile (isspace(str[i]))\n\t\ti++;\n\ts = i;\n\n\twhile (isalnum(str[i]) || str[i] == '_')\n\t\ti++;\n\n\tlen = i - s;\n\n\tif (!len)\n\t\treturn -1;\n\n\tfield_name = kmemdup_nul(str + s, len, GFP_KERNEL);\n\tif (!field_name)\n\t\treturn -ENOMEM;\n\n\t/* Make sure that the field exists */\n\n\tfield = trace_find_event_field(call, field_name);\n\tkfree(field_name);\n\tif (!field) {\n\t\tparse_error(pe, FILT_ERR_FIELD_NOT_FOUND, pos + i);\n\t\treturn -EINVAL;\n\t}\n\n\twhile (isspace(str[i]))\n\t\ti++;\n\n\t/* Make sure this op is supported */\n\tfor (op = 0; ops[op]; op++) {\n\t\t/* This is why '<=' must come before '<' in ops[] */\n\t\tif (strncmp(str + i, ops[op], strlen(ops[op])) == 0)\n\t\t\tbreak;\n\t}\n\n\tif (!ops[op]) {\n\t\tparse_error(pe, FILT_ERR_INVALID_OP, pos + i);\n\t\tgoto err_free;\n\t}\n\n\ti += strlen(ops[op]);\n\n\twhile (isspace(str[i]))\n\t\ti++;\n\n\ts = i;\n\n\tpred = kzalloc(sizeof(*pred), GFP_KERNEL);\n\tif (!pred)\n\t\treturn -ENOMEM;\n\n\tpred->field = field;\n\tpred->offset = field->offset;\n\tpred->op = op;\n\n\tif (ftrace_event_is_function(call)) {\n\t\t/*\n\t\t * Perf does things different with function events.\n\t\t * It only allows an \"ip\" field, and expects a string.\n\t\t * But the string does not need to be surrounded by quotes.\n\t\t * If it is a string, the assigned function as a nop,\n\t\t * (perf doesn't use it) and grab everything.\n\t\t */\n\t\tif (strcmp(field->name, \"ip\") != 0) {\n\t\t\t parse_error(pe, FILT_ERR_IP_FIELD_ONLY, pos + i);\n\t\t\t goto err_free;\n\t\t }\n\t\t pred->fn = filter_pred_none;\n\n\t\t /*\n\t\t  * Quotes are not required, but if they exist then we need\n\t\t  * to read them till we hit a matching one.\n\t\t  */\n\t\t if (str[i] == '\\'' || str[i] == '\"')\n\t\t\t q = str[i];\n\t\t else\n\t\t\t q = 0;\n\n\t\t for (i++; str[i]; i++) {\n\t\t\t if (q && str[i] == q)\n\t\t\t\t break;\n\t\t\t if (!q && (str[i] == ')' || str[i] == '&' ||\n\t\t\t\t    str[i] == '|'))\n\t\t\t\t break;\n\t\t }\n\t\t /* Skip quotes */\n\t\t if (q)\n\t\t\t s++;\n\t\tlen = i - s;\n\t\tif (len >= MAX_FILTER_STR_VAL) {\n\t\t\tparse_error(pe, FILT_ERR_OPERAND_TOO_LONG, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tpred->regex.len = len;\n\t\tstrncpy(pred->regex.pattern, str + s, len);\n\t\tpred->regex.pattern[len] = 0;\n\n\t/* This is either a string, or an integer */\n\t} else if (str[i] == '\\'' || str[i] == '\"') {\n\t\tchar q = str[i];\n\n\t\t/* Make sure the op is OK for strings */\n\t\tswitch (op) {\n\t\tcase OP_NE:\n\t\t\tpred->not = 1;\n\t\t\t/* Fall through */\n\t\tcase OP_GLOB:\n\t\tcase OP_EQ:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tparse_error(pe, FILT_ERR_ILLEGAL_FIELD_OP, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* Make sure the field is OK for strings */\n\t\tif (!is_string_field(field)) {\n\t\t\tparse_error(pe, FILT_ERR_EXPECT_DIGIT, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tfor (i++; str[i]; i++) {\n\t\t\tif (str[i] == q)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!str[i]) {\n\t\t\tparse_error(pe, FILT_ERR_MISSING_QUOTE, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* Skip quotes */\n\t\ts++;\n\t\tlen = i - s;\n\t\tif (len >= MAX_FILTER_STR_VAL) {\n\t\t\tparse_error(pe, FILT_ERR_OPERAND_TOO_LONG, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tpred->regex.len = len;\n\t\tstrncpy(pred->regex.pattern, str + s, len);\n\t\tpred->regex.pattern[len] = 0;\n\n\t\tfilter_build_regex(pred);\n\n\t\tif (field->filter_type == FILTER_COMM) {\n\t\t\tpred->fn = filter_pred_comm;\n\n\t\t} else if (field->filter_type == FILTER_STATIC_STRING) {\n\t\t\tpred->fn = filter_pred_string;\n\t\t\tpred->regex.field_len = field->size;\n\n\t\t} else if (field->filter_type == FILTER_DYN_STRING)\n\t\t\tpred->fn = filter_pred_strloc;\n\t\telse\n\t\t\tpred->fn = filter_pred_pchar;\n\t\t/* go past the last quote */\n\t\ti++;\n\n\t} else if (isdigit(str[i])) {\n\n\t\t/* Make sure the field is not a string */\n\t\tif (is_string_field(field)) {\n\t\t\tparse_error(pe, FILT_ERR_EXPECT_STRING, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (op == OP_GLOB) {\n\t\t\tparse_error(pe, FILT_ERR_ILLEGAL_FIELD_OP, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* We allow 0xDEADBEEF */\n\t\twhile (isalnum(str[i]))\n\t\t\ti++;\n\n\t\tlen = i - s;\n\t\t/* 0xfeedfacedeadbeef is 18 chars max */\n\t\tif (len >= sizeof(num_buf)) {\n\t\t\tparse_error(pe, FILT_ERR_OPERAND_TOO_LONG, pos + i);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tstrncpy(num_buf, str + s, len);\n\t\tnum_buf[len] = 0;\n\n\t\t/* Make sure it is a value */\n\t\tif (field->is_signed)\n\t\t\tret = kstrtoll(num_buf, 0, &val);\n\t\telse\n\t\t\tret = kstrtoull(num_buf, 0, &val);\n\t\tif (ret) {\n\t\t\tparse_error(pe, FILT_ERR_ILLEGAL_INTVAL, pos + s);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tpred->val = val;\n\n\t\tif (field->filter_type == FILTER_CPU)\n\t\t\tpred->fn = filter_pred_cpu;\n\t\telse {\n\t\t\tpred->fn = select_comparison_fn(pred->op, field->size,\n\t\t\t\t\t\t\tfield->is_signed);\n\t\t\tif (pred->op == OP_NE)\n\t\t\t\tpred->not = 1;\n\t\t}\n\n\t} else {\n\t\tparse_error(pe, FILT_ERR_INVALID_VALUE, pos + i);\n\t\tgoto err_free;\n\t}\n\n\t*pred_ptr = pred;\n\treturn i;\n\nerr_free:\n\tkfree(pred);\n\treturn -EINVAL;\n}\n\nenum {\n\tTOO_MANY_CLOSE\t\t= -1,\n\tTOO_MANY_OPEN\t\t= -2,\n\tMISSING_QUOTE\t\t= -3,\n};\n\n/*\n * Read the filter string once to calculate the number of predicates\n * as well as how deep the parentheses go.\n *\n * Returns:\n *   0 - everything is fine (err is undefined)\n *  -1 - too many ')'\n *  -2 - too many '('\n *  -3 - No matching quote\n */\nstatic int calc_stack(const char *str, int *parens, int *preds, int *err)\n{\n\tbool is_pred = false;\n\tint nr_preds = 0;\n\tint open = 1; /* Count the expression as \"(E)\" */\n\tint last_quote = 0;\n\tint max_open = 1;\n\tint quote = 0;\n\tint i;\n\n\t*err = 0;\n\n\tfor (i = 0; str[i]; i++) {\n\t\tif (isspace(str[i]))\n\t\t\tcontinue;\n\t\tif (quote) {\n\t\t\tif (str[i] == quote)\n\t\t\t       quote = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (str[i]) {\n\t\tcase '\\'':\n\t\tcase '\"':\n\t\t\tquote = str[i];\n\t\t\tlast_quote = i;\n\t\t\tbreak;\n\t\tcase '|':\n\t\tcase '&':\n\t\t\tif (str[i+1] != str[i])\n\t\t\t\tbreak;\n\t\t\tis_pred = false;\n\t\t\tcontinue;\n\t\tcase '(':\n\t\t\tis_pred = false;\n\t\t\topen++;\n\t\t\tif (open > max_open)\n\t\t\t\tmax_open = open;\n\t\t\tcontinue;\n\t\tcase ')':\n\t\t\tis_pred = false;\n\t\t\tif (open == 1) {\n\t\t\t\t*err = i;\n\t\t\t\treturn TOO_MANY_CLOSE;\n\t\t\t}\n\t\t\topen--;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!is_pred) {\n\t\t\tnr_preds++;\n\t\t\tis_pred = true;\n\t\t}\n\t}\n\n\tif (quote) {\n\t\t*err = last_quote;\n\t\treturn MISSING_QUOTE;\n\t}\n\n\tif (open != 1) {\n\t\tint level = open;\n\n\t\t/* find the bad open */\n\t\tfor (i--; i; i--) {\n\t\t\tif (quote) {\n\t\t\t\tif (str[i] == quote)\n\t\t\t\t\tquote = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tswitch (str[i]) {\n\t\t\tcase '(':\n\t\t\t\tif (level == open) {\n\t\t\t\t\t*err = i;\n\t\t\t\t\treturn TOO_MANY_OPEN;\n\t\t\t\t}\n\t\t\t\tlevel--;\n\t\t\t\tbreak;\n\t\t\tcase ')':\n\t\t\t\tlevel++;\n\t\t\t\tbreak;\n\t\t\tcase '\\'':\n\t\t\tcase '\"':\n\t\t\t\tquote = str[i];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t/* First character is the '(' with missing ')' */\n\t\t*err = 0;\n\t\treturn TOO_MANY_OPEN;\n\t}\n\n\t/* Set the size of the required stacks */\n\t*parens = max_open;\n\t*preds = nr_preds;\n\treturn 0;\n}\n\nstatic int process_preds(struct trace_event_call *call,\n\t\t\t const char *filter_string,\n\t\t\t struct event_filter *filter,\n\t\t\t struct filter_parse_error *pe)\n{\n\tstruct prog_entry *prog;\n\tint nr_parens;\n\tint nr_preds;\n\tint index;\n\tint ret;\n\n\tret = calc_stack(filter_string, &nr_parens, &nr_preds, &index);\n\tif (ret < 0) {\n\t\tswitch (ret) {\n\t\tcase MISSING_QUOTE:\n\t\t\tparse_error(pe, FILT_ERR_MISSING_QUOTE, index);\n\t\t\tbreak;\n\t\tcase TOO_MANY_OPEN:\n\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_OPEN, index);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_CLOSE, index);\n\t\t}\n\t\treturn ret;\n\t}\n\n\tif (!nr_preds)\n\t\treturn -EINVAL;\n\n\tprog = predicate_parse(filter_string, nr_parens, nr_preds,\n\t\t\t       parse_pred, call, pe);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\trcu_assign_pointer(filter->prog, prog);\n\treturn 0;\n}\n\nstatic inline void event_set_filtered_flag(struct trace_event_file *file)\n{\n\tunsigned long old_flags = file->flags;\n\n\tfile->flags |= EVENT_FILE_FL_FILTERED;\n\n\tif (old_flags != file->flags)\n\t\ttrace_buffered_event_enable();\n}\n\nstatic inline void event_set_filter(struct trace_event_file *file,\n\t\t\t\t    struct event_filter *filter)\n{\n\trcu_assign_pointer(file->filter, filter);\n}\n\nstatic inline void event_clear_filter(struct trace_event_file *file)\n{\n\tRCU_INIT_POINTER(file->filter, NULL);\n}\n\nstatic inline void\nevent_set_no_set_filter_flag(struct trace_event_file *file)\n{\n\tfile->flags |= EVENT_FILE_FL_NO_SET_FILTER;\n}\n\nstatic inline void\nevent_clear_no_set_filter_flag(struct trace_event_file *file)\n{\n\tfile->flags &= ~EVENT_FILE_FL_NO_SET_FILTER;\n}\n\nstatic inline bool\nevent_no_set_filter_flag(struct trace_event_file *file)\n{\n\tif (file->flags & EVENT_FILE_FL_NO_SET_FILTER)\n\t\treturn true;\n\n\treturn false;\n}\n\nstruct filter_list {\n\tstruct list_head\tlist;\n\tstruct event_filter\t*filter;\n};\n\nstatic int process_system_preds(struct trace_subsystem_dir *dir,\n\t\t\t\tstruct trace_array *tr,\n\t\t\t\tstruct filter_parse_error *pe,\n\t\t\t\tchar *filter_string)\n{\n\tstruct trace_event_file *file;\n\tstruct filter_list *filter_item;\n\tstruct event_filter *filter = NULL;\n\tstruct filter_list *tmp;\n\tLIST_HEAD(filter_list);\n\tbool fail = true;\n\tint err;\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\n\t\tif (file->system != dir)\n\t\t\tcontinue;\n\n\t\tfilter = kzalloc(sizeof(*filter), GFP_KERNEL);\n\t\tif (!filter)\n\t\t\tgoto fail_mem;\n\n\t\tfilter->filter_string = kstrdup(filter_string, GFP_KERNEL);\n\t\tif (!filter->filter_string)\n\t\t\tgoto fail_mem;\n\n\t\terr = process_preds(file->event_call, filter_string, filter, pe);\n\t\tif (err) {\n\t\t\tfilter_disable(file);\n\t\t\tparse_error(pe, FILT_ERR_BAD_SUBSYS_FILTER, 0);\n\t\t\tappend_filter_err(pe, filter);\n\t\t} else\n\t\t\tevent_set_filtered_flag(file);\n\n\n\t\tfilter_item = kzalloc(sizeof(*filter_item), GFP_KERNEL);\n\t\tif (!filter_item)\n\t\t\tgoto fail_mem;\n\n\t\tlist_add_tail(&filter_item->list, &filter_list);\n\t\t/*\n\t\t * Regardless of if this returned an error, we still\n\t\t * replace the filter for the call.\n\t\t */\n\t\tfilter_item->filter = event_filter(file);\n\t\tevent_set_filter(file, filter);\n\t\tfilter = NULL;\n\n\t\tfail = false;\n\t}\n\n\tif (fail)\n\t\tgoto fail;\n\n\t/*\n\t * The calls can still be using the old filters.\n\t * Do a synchronize_sched() to ensure all calls are\n\t * done with them before we free them.\n\t */\n\tsynchronize_sched();\n\tlist_for_each_entry_safe(filter_item, tmp, &filter_list, list) {\n\t\t__free_filter(filter_item->filter);\n\t\tlist_del(&filter_item->list);\n\t\tkfree(filter_item);\n\t}\n\treturn 0;\n fail:\n\t/* No call succeeded */\n\tlist_for_each_entry_safe(filter_item, tmp, &filter_list, list) {\n\t\tlist_del(&filter_item->list);\n\t\tkfree(filter_item);\n\t}\n\tparse_error(pe, FILT_ERR_BAD_SUBSYS_FILTER, 0);\n\treturn -EINVAL;\n fail_mem:\n\tkfree(filter);\n\t/* If any call succeeded, we still need to sync */\n\tif (!fail)\n\t\tsynchronize_sched();\n\tlist_for_each_entry_safe(filter_item, tmp, &filter_list, list) {\n\t\t__free_filter(filter_item->filter);\n\t\tlist_del(&filter_item->list);\n\t\tkfree(filter_item);\n\t}\n\treturn -ENOMEM;\n}\n\nstatic int create_filter_start(char *filter_string, bool set_str,\n\t\t\t       struct filter_parse_error **pse,\n\t\t\t       struct event_filter **filterp)\n{\n\tstruct event_filter *filter;\n\tstruct filter_parse_error *pe = NULL;\n\tint err = 0;\n\n\tif (WARN_ON_ONCE(*pse || *filterp))\n\t\treturn -EINVAL;\n\n\tfilter = kzalloc(sizeof(*filter), GFP_KERNEL);\n\tif (filter && set_str) {\n\t\tfilter->filter_string = kstrdup(filter_string, GFP_KERNEL);\n\t\tif (!filter->filter_string)\n\t\t\terr = -ENOMEM;\n\t}\n\n\tpe = kzalloc(sizeof(*pe), GFP_KERNEL);\n\n\tif (!filter || !pe || err) {\n\t\tkfree(pe);\n\t\t__free_filter(filter);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* we're committed to creating a new filter */\n\t*filterp = filter;\n\t*pse = pe;\n\n\treturn 0;\n}\n\nstatic void create_filter_finish(struct filter_parse_error *pe)\n{\n\tkfree(pe);\n}\n\n/**\n * create_filter - create a filter for a trace_event_call\n * @call: trace_event_call to create a filter for\n * @filter_str: filter string\n * @set_str: remember @filter_str and enable detailed error in filter\n * @filterp: out param for created filter (always updated on return)\n *\n * Creates a filter for @call with @filter_str.  If @set_str is %true,\n * @filter_str is copied and recorded in the new filter.\n *\n * On success, returns 0 and *@filterp points to the new filter.  On\n * failure, returns -errno and *@filterp may point to %NULL or to a new\n * filter.  In the latter case, the returned filter contains error\n * information if @set_str is %true and the caller is responsible for\n * freeing it.\n */\nstatic int create_filter(struct trace_event_call *call,\n\t\t\t char *filter_string, bool set_str,\n\t\t\t struct event_filter **filterp)\n{\n\tstruct filter_parse_error *pe = NULL;\n\tint err;\n\n\terr = create_filter_start(filter_string, set_str, &pe, filterp);\n\tif (err)\n\t\treturn err;\n\n\terr = process_preds(call, filter_string, *filterp, pe);\n\tif (err && set_str)\n\t\tappend_filter_err(pe, *filterp);\n\n\treturn err;\n}\n\nint create_event_filter(struct trace_event_call *call,\n\t\t\tchar *filter_str, bool set_str,\n\t\t\tstruct event_filter **filterp)\n{\n\treturn create_filter(call, filter_str, set_str, filterp);\n}\n\n/**\n * create_system_filter - create a filter for an event_subsystem\n * @system: event_subsystem to create a filter for\n * @filter_str: filter string\n * @filterp: out param for created filter (always updated on return)\n *\n * Identical to create_filter() except that it creates a subsystem filter\n * and always remembers @filter_str.\n */\nstatic int create_system_filter(struct trace_subsystem_dir *dir,\n\t\t\t\tstruct trace_array *tr,\n\t\t\t\tchar *filter_str, struct event_filter **filterp)\n{\n\tstruct filter_parse_error *pe = NULL;\n\tint err;\n\n\terr = create_filter_start(filter_str, true, &pe, filterp);\n\tif (!err) {\n\t\terr = process_system_preds(dir, tr, pe, filter_str);\n\t\tif (!err) {\n\t\t\t/* System filters just show a default message */\n\t\t\tkfree((*filterp)->filter_string);\n\t\t\t(*filterp)->filter_string = NULL;\n\t\t} else {\n\t\t\tappend_filter_err(pe, *filterp);\n\t\t}\n\t}\n\tcreate_filter_finish(pe);\n\n\treturn err;\n}\n\n/* caller must hold event_mutex */\nint apply_event_filter(struct trace_event_file *file, char *filter_string)\n{\n\tstruct trace_event_call *call = file->event_call;\n\tstruct event_filter *filter = NULL;\n\tint err;\n\n\tif (!strcmp(strstrip(filter_string), \"0\")) {\n\t\tfilter_disable(file);\n\t\tfilter = event_filter(file);\n\n\t\tif (!filter)\n\t\t\treturn 0;\n\n\t\tevent_clear_filter(file);\n\n\t\t/* Make sure the filter is not being used */\n\t\tsynchronize_sched();\n\t\t__free_filter(filter);\n\n\t\treturn 0;\n\t}\n\n\terr = create_filter(call, filter_string, true, &filter);\n\n\t/*\n\t * Always swap the call filter with the new filter\n\t * even if there was an error. If there was an error\n\t * in the filter, we disable the filter and show the error\n\t * string\n\t */\n\tif (filter) {\n\t\tstruct event_filter *tmp;\n\n\t\ttmp = event_filter(file);\n\t\tif (!err)\n\t\t\tevent_set_filtered_flag(file);\n\t\telse\n\t\t\tfilter_disable(file);\n\n\t\tevent_set_filter(file, filter);\n\n\t\tif (tmp) {\n\t\t\t/* Make sure the call is done with the filter */\n\t\t\tsynchronize_sched();\n\t\t\t__free_filter(tmp);\n\t\t}\n\t}\n\n\treturn err;\n}\n\nint apply_subsystem_event_filter(struct trace_subsystem_dir *dir,\n\t\t\t\t char *filter_string)\n{\n\tstruct event_subsystem *system = dir->subsystem;\n\tstruct trace_array *tr = dir->tr;\n\tstruct event_filter *filter = NULL;\n\tint err = 0;\n\n\tmutex_lock(&event_mutex);\n\n\t/* Make sure the system still has events */\n\tif (!dir->nr_events) {\n\t\terr = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!strcmp(strstrip(filter_string), \"0\")) {\n\t\tfilter_free_subsystem_preds(dir, tr);\n\t\tremove_filter_string(system->filter);\n\t\tfilter = system->filter;\n\t\tsystem->filter = NULL;\n\t\t/* Ensure all filters are no longer used */\n\t\tsynchronize_sched();\n\t\tfilter_free_subsystem_filters(dir, tr);\n\t\t__free_filter(filter);\n\t\tgoto out_unlock;\n\t}\n\n\terr = create_system_filter(dir, tr, filter_string, &filter);\n\tif (filter) {\n\t\t/*\n\t\t * No event actually uses the system filter\n\t\t * we can free it without synchronize_sched().\n\t\t */\n\t\t__free_filter(system->filter);\n\t\tsystem->filter = filter;\n\t}\nout_unlock:\n\tmutex_unlock(&event_mutex);\n\n\treturn err;\n}\n\n#ifdef CONFIG_PERF_EVENTS\n\nvoid ftrace_profile_free_filter(struct perf_event *event)\n{\n\tstruct event_filter *filter = event->filter;\n\n\tevent->filter = NULL;\n\t__free_filter(filter);\n}\n\nstruct function_filter_data {\n\tstruct ftrace_ops *ops;\n\tint first_filter;\n\tint first_notrace;\n};\n\n#ifdef CONFIG_FUNCTION_TRACER\nstatic char **\nftrace_function_filter_re(char *buf, int len, int *count)\n{\n\tchar *str, **re;\n\n\tstr = kstrndup(buf, len, GFP_KERNEL);\n\tif (!str)\n\t\treturn NULL;\n\n\t/*\n\t * The argv_split function takes white space\n\t * as a separator, so convert ',' into spaces.\n\t */\n\tstrreplace(str, ',', ' ');\n\n\tre = argv_split(GFP_KERNEL, str, count);\n\tkfree(str);\n\treturn re;\n}\n\nstatic int ftrace_function_set_regexp(struct ftrace_ops *ops, int filter,\n\t\t\t\t      int reset, char *re, int len)\n{\n\tint ret;\n\n\tif (filter)\n\t\tret = ftrace_set_filter(ops, re, len, reset);\n\telse\n\t\tret = ftrace_set_notrace(ops, re, len, reset);\n\n\treturn ret;\n}\n\nstatic int __ftrace_function_set_filter(int filter, char *buf, int len,\n\t\t\t\t\tstruct function_filter_data *data)\n{\n\tint i, re_cnt, ret = -EINVAL;\n\tint *reset;\n\tchar **re;\n\n\treset = filter ? &data->first_filter : &data->first_notrace;\n\n\t/*\n\t * The 'ip' field could have multiple filters set, separated\n\t * either by space or comma. We first cut the filter and apply\n\t * all pieces separatelly.\n\t */\n\tre = ftrace_function_filter_re(buf, len, &re_cnt);\n\tif (!re)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < re_cnt; i++) {\n\t\tret = ftrace_function_set_regexp(data->ops, filter, *reset,\n\t\t\t\t\t\t re[i], strlen(re[i]));\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (*reset)\n\t\t\t*reset = 0;\n\t}\n\n\targv_free(re);\n\treturn ret;\n}\n\nstatic int ftrace_function_check_pred(struct filter_pred *pred)\n{\n\tstruct ftrace_event_field *field = pred->field;\n\n\t/*\n\t * Check the predicate for function trace, verify:\n\t *  - only '==' and '!=' is used\n\t *  - the 'ip' field is used\n\t */\n\tif ((pred->op != OP_EQ) && (pred->op != OP_NE))\n\t\treturn -EINVAL;\n\n\tif (strcmp(field->name, \"ip\"))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int ftrace_function_set_filter_pred(struct filter_pred *pred,\n\t\t\t\t\t   struct function_filter_data *data)\n{\n\tint ret;\n\n\t/* Checking the node is valid for function trace. */\n\tret = ftrace_function_check_pred(pred);\n\tif (ret)\n\t\treturn ret;\n\n\treturn __ftrace_function_set_filter(pred->op == OP_EQ,\n\t\t\t\t\t    pred->regex.pattern,\n\t\t\t\t\t    pred->regex.len,\n\t\t\t\t\t    data);\n}\n\nstatic bool is_or(struct prog_entry *prog, int i)\n{\n\tint target;\n\n\t/*\n\t * Only \"||\" is allowed for function events, thus,\n\t * all true branches should jump to true, and any\n\t * false branch should jump to false.\n\t */\n\ttarget = prog[i].target + 1;\n\t/* True and false have NULL preds (all prog entries should jump to one */\n\tif (prog[target].pred)\n\t\treturn false;\n\n\t/* prog[target].target is 1 for TRUE, 0 for FALSE */\n\treturn prog[i].when_to_branch == prog[target].target;\n}\n\nstatic int ftrace_function_set_filter(struct perf_event *event,\n\t\t\t\t      struct event_filter *filter)\n{\n\tstruct prog_entry *prog = rcu_dereference_protected(filter->prog,\n\t\t\t\t\t\tlockdep_is_held(&event_mutex));\n\tstruct function_filter_data data = {\n\t\t.first_filter  = 1,\n\t\t.first_notrace = 1,\n\t\t.ops           = &event->ftrace_ops,\n\t};\n\tint i;\n\n\tfor (i = 0; prog[i].pred; i++) {\n\t\tstruct filter_pred *pred = prog[i].pred;\n\n\t\tif (!is_or(prog, i))\n\t\t\treturn -EINVAL;\n\n\t\tif (ftrace_function_set_filter_pred(pred, &data) < 0)\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n#else\nstatic int ftrace_function_set_filter(struct perf_event *event,\n\t\t\t\t      struct event_filter *filter)\n{\n\treturn -ENODEV;\n}\n#endif /* CONFIG_FUNCTION_TRACER */\n\nint ftrace_profile_set_filter(struct perf_event *event, int event_id,\n\t\t\t      char *filter_str)\n{\n\tint err;\n\tstruct event_filter *filter = NULL;\n\tstruct trace_event_call *call;\n\n\tmutex_lock(&event_mutex);\n\n\tcall = event->tp_event;\n\n\terr = -EINVAL;\n\tif (!call)\n\t\tgoto out_unlock;\n\n\terr = -EEXIST;\n\tif (event->filter)\n\t\tgoto out_unlock;\n\n\terr = create_filter(call, filter_str, false, &filter);\n\tif (err)\n\t\tgoto free_filter;\n\n\tif (ftrace_event_is_function(call))\n\t\terr = ftrace_function_set_filter(event, filter);\n\telse\n\t\tevent->filter = filter;\n\nfree_filter:\n\tif (err || ftrace_event_is_function(call))\n\t\t__free_filter(filter);\n\nout_unlock:\n\tmutex_unlock(&event_mutex);\n\n\treturn err;\n}\n\n#endif /* CONFIG_PERF_EVENTS */\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\n\n#include <linux/types.h>\n#include <linux/tracepoint.h>\n\n#define CREATE_TRACE_POINTS\n#include \"trace_events_filter_test.h\"\n\n#define DATA_REC(m, va, vb, vc, vd, ve, vf, vg, vh, nvisit) \\\n{ \\\n\t.filter = FILTER, \\\n\t.rec    = { .a = va, .b = vb, .c = vc, .d = vd, \\\n\t\t    .e = ve, .f = vf, .g = vg, .h = vh }, \\\n\t.match  = m, \\\n\t.not_visited = nvisit, \\\n}\n#define YES 1\n#define NO  0\n\nstatic struct test_filter_data_t {\n\tchar *filter;\n\tstruct trace_event_raw_ftrace_test_filter rec;\n\tint match;\n\tchar *not_visited;\n} test_filter_data[] = {\n#define FILTER \"a == 1 && b == 1 && c == 1 && d == 1 && \" \\\n\t       \"e == 1 && f == 1 && g == 1 && h == 1\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 1, 1, \"\"),\n\tDATA_REC(NO,  0, 1, 1, 1, 1, 1, 1, 1, \"bcdefgh\"),\n\tDATA_REC(NO,  1, 1, 1, 1, 1, 1, 1, 0, \"\"),\n#undef FILTER\n#define FILTER \"a == 1 || b == 1 || c == 1 || d == 1 || \" \\\n\t       \"e == 1 || f == 1 || g == 1 || h == 1\"\n\tDATA_REC(NO,  0, 0, 0, 0, 0, 0, 0, 0, \"\"),\n\tDATA_REC(YES, 0, 0, 0, 0, 0, 0, 0, 1, \"\"),\n\tDATA_REC(YES, 1, 0, 0, 0, 0, 0, 0, 0, \"bcdefgh\"),\n#undef FILTER\n#define FILTER \"(a == 1 || b == 1) && (c == 1 || d == 1) && \" \\\n\t       \"(e == 1 || f == 1) && (g == 1 || h == 1)\"\n\tDATA_REC(NO,  0, 0, 1, 1, 1, 1, 1, 1, \"dfh\"),\n\tDATA_REC(YES, 0, 1, 0, 1, 0, 1, 0, 1, \"\"),\n\tDATA_REC(YES, 1, 0, 1, 0, 0, 1, 0, 1, \"bd\"),\n\tDATA_REC(NO,  1, 0, 1, 0, 0, 1, 0, 0, \"bd\"),\n#undef FILTER\n#define FILTER \"(a == 1 && b == 1) || (c == 1 && d == 1) || \" \\\n\t       \"(e == 1 && f == 1) || (g == 1 && h == 1)\"\n\tDATA_REC(YES, 1, 0, 1, 1, 1, 1, 1, 1, \"efgh\"),\n\tDATA_REC(YES, 0, 0, 0, 0, 0, 0, 1, 1, \"\"),\n\tDATA_REC(NO,  0, 0, 0, 0, 0, 0, 0, 1, \"\"),\n#undef FILTER\n#define FILTER \"(a == 1 && b == 1) && (c == 1 && d == 1) && \" \\\n\t       \"(e == 1 && f == 1) || (g == 1 && h == 1)\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 0, 0, \"gh\"),\n\tDATA_REC(NO,  0, 0, 0, 0, 0, 0, 0, 1, \"\"),\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 0, 1, 1, \"\"),\n#undef FILTER\n#define FILTER \"((a == 1 || b == 1) || (c == 1 || d == 1) || \" \\\n\t       \"(e == 1 || f == 1)) && (g == 1 || h == 1)\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 0, 1, \"bcdef\"),\n\tDATA_REC(NO,  0, 0, 0, 0, 0, 0, 0, 0, \"\"),\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 0, 1, 1, \"h\"),\n#undef FILTER\n#define FILTER \"((((((((a == 1) && (b == 1)) || (c == 1)) && (d == 1)) || \" \\\n\t       \"(e == 1)) && (f == 1)) || (g == 1)) && (h == 1))\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 1, 1, \"ceg\"),\n\tDATA_REC(NO,  0, 1, 0, 1, 0, 1, 0, 1, \"\"),\n\tDATA_REC(NO,  1, 0, 1, 0, 1, 0, 1, 0, \"\"),\n#undef FILTER\n#define FILTER \"((((((((a == 1) || (b == 1)) && (c == 1)) || (d == 1)) && \" \\\n\t       \"(e == 1)) || (f == 1)) && (g == 1)) || (h == 1))\"\n\tDATA_REC(YES, 1, 1, 1, 1, 1, 1, 1, 1, \"bdfh\"),\n\tDATA_REC(YES, 0, 1, 0, 1, 0, 1, 0, 1, \"\"),\n\tDATA_REC(YES, 1, 0, 1, 0, 1, 0, 1, 0, \"bdfh\"),\n};\n\n#undef DATA_REC\n#undef FILTER\n#undef YES\n#undef NO\n\n#define DATA_CNT ARRAY_SIZE(test_filter_data)\n\nstatic int test_pred_visited;\n\nstatic int test_pred_visited_fn(struct filter_pred *pred, void *event)\n{\n\tstruct ftrace_event_field *field = pred->field;\n\n\ttest_pred_visited = 1;\n\tprintk(KERN_INFO \"\\npred visited %s\\n\", field->name);\n\treturn 1;\n}\n\nstatic void update_pred_fn(struct event_filter *filter, char *fields)\n{\n\tstruct prog_entry *prog = rcu_dereference_protected(filter->prog,\n\t\t\t\t\t\tlockdep_is_held(&event_mutex));\n\tint i;\n\n\tfor (i = 0; prog[i].pred; i++) {\n\t\tstruct filter_pred *pred = prog[i].pred;\n\t\tstruct ftrace_event_field *field = pred->field;\n\n\t\tWARN_ON_ONCE(!pred->fn);\n\n\t\tif (!field) {\n\t\t\tWARN_ONCE(1, \"all leafs should have field defined %d\", i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strchr(fields, *field->name))\n\t\t\tcontinue;\n\n\t\tpred->fn = test_pred_visited_fn;\n\t}\n}\n\nstatic __init int ftrace_test_event_filter(void)\n{\n\tint i;\n\n\tprintk(KERN_INFO \"Testing ftrace filter: \");\n\n\tfor (i = 0; i < DATA_CNT; i++) {\n\t\tstruct event_filter *filter = NULL;\n\t\tstruct test_filter_data_t *d = &test_filter_data[i];\n\t\tint err;\n\n\t\terr = create_filter(&event_ftrace_test_filter, d->filter,\n\t\t\t\t    false, &filter);\n\t\tif (err) {\n\t\t\tprintk(KERN_INFO\n\t\t\t       \"Failed to get filter for '%s', err %d\\n\",\n\t\t\t       d->filter, err);\n\t\t\t__free_filter(filter);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Needed to dereference filter->prog */\n\t\tmutex_lock(&event_mutex);\n\t\t/*\n\t\t * The preemption disabling is not really needed for self\n\t\t * tests, but the rcu dereference will complain without it.\n\t\t */\n\t\tpreempt_disable();\n\t\tif (*d->not_visited)\n\t\t\tupdate_pred_fn(filter, d->not_visited);\n\n\t\ttest_pred_visited = 0;\n\t\terr = filter_match_preds(filter, &d->rec);\n\t\tpreempt_enable();\n\n\t\tmutex_unlock(&event_mutex);\n\n\t\t__free_filter(filter);\n\n\t\tif (test_pred_visited) {\n\t\t\tprintk(KERN_INFO\n\t\t\t       \"Failed, unwanted pred visited for filter %s\\n\",\n\t\t\t       d->filter);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (err != d->match) {\n\t\t\tprintk(KERN_INFO\n\t\t\t       \"Failed to match filter '%s', expected %d\\n\",\n\t\t\t       d->filter, d->match);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == DATA_CNT)\n\t\tprintk(KERN_CONT \"OK\\n\");\n\n\treturn 0;\n}\n\nlate_initcall(ftrace_test_event_filter);\n\n#endif /* CONFIG_FTRACE_STARTUP_TEST */\n", "# SPDX-License-Identifier: GPL-2.0\n# ==========================================================================\n# Building\n# ==========================================================================\n\nsrc := $(obj)\n\nPHONY := __build\n__build:\n\n# Init all relevant variables used in kbuild files so\n# 1) they have correct type\n# 2) they do not inherit any value from the environment\nobj-y :=\nobj-m :=\nlib-y :=\nlib-m :=\nalways :=\ntargets :=\nsubdir-y :=\nsubdir-m :=\nEXTRA_AFLAGS   :=\nEXTRA_CFLAGS   :=\nEXTRA_CPPFLAGS :=\nEXTRA_LDFLAGS  :=\nasflags-y  :=\nccflags-y  :=\ncppflags-y :=\nldflags-y  :=\n\nsubdir-asflags-y :=\nsubdir-ccflags-y :=\n\n# Read auto.conf if it exists, otherwise ignore\n-include include/config/auto.conf\n\ninclude scripts/Kbuild.include\n\n# For backward compatibility check that these variables do not change\nsave-cflags := $(CFLAGS)\n\n# The filename Kbuild has precedence over Makefile\nkbuild-dir := $(if $(filter /%,$(src)),$(src),$(srctree)/$(src))\nkbuild-file := $(if $(wildcard $(kbuild-dir)/Kbuild),$(kbuild-dir)/Kbuild,$(kbuild-dir)/Makefile)\ninclude $(kbuild-file)\n\n# If the save-* variables changed error out\nifeq ($(KBUILD_NOPEDANTIC),)\n        ifneq (\"$(save-cflags)\",\"$(CFLAGS)\")\n                $(error CFLAGS was changed in \"$(kbuild-file)\". Fix it to use ccflags-y)\n        endif\nendif\n\ninclude scripts/Makefile.lib\n\nifdef host-progs\nifneq ($(hostprogs-y),$(host-progs))\n$(warning kbuild: $(obj)/Makefile - Usage of host-progs is deprecated. Please replace with hostprogs-y!)\nhostprogs-y += $(host-progs)\nendif\nendif\n\n# Do not include host rules unless needed\nifneq ($(hostprogs-y)$(hostprogs-m)$(hostlibs-y)$(hostlibs-m)$(hostcxxlibs-y)$(hostcxxlibs-m),)\ninclude scripts/Makefile.host\nendif\n\nifndef obj\n$(warning kbuild: Makefile.build is included improperly)\nendif\n\n# ===========================================================================\n\nifneq ($(strip $(lib-y) $(lib-m) $(lib-)),)\nlib-target := $(obj)/lib.a\nreal-obj-y += $(obj)/lib-ksyms.o\nendif\n\nifneq ($(strip $(real-obj-y) $(need-builtin)),)\nbuiltin-target := $(obj)/built-in.a\nendif\n\nmodorder-target := $(obj)/modules.order\n\n# We keep a list of all modules in $(MODVERDIR)\n\n__build: $(if $(KBUILD_BUILTIN),$(builtin-target) $(lib-target) $(extra-y)) \\\n\t $(if $(KBUILD_MODULES),$(obj-m) $(modorder-target)) \\\n\t $(subdir-ym) $(always)\n\t@:\n\n# Linus' kernel sanity checking tool\nifneq ($(KBUILD_CHECKSRC),0)\n  ifeq ($(KBUILD_CHECKSRC),2)\n    quiet_cmd_force_checksrc = CHECK   $<\n          cmd_force_checksrc = $(CHECK) $(CHECKFLAGS) $(c_flags) $< ;\n  else\n      quiet_cmd_checksrc     = CHECK   $<\n            cmd_checksrc     = $(CHECK) $(CHECKFLAGS) $(c_flags) $< ;\n  endif\nendif\n\nifneq ($(KBUILD_ENABLE_EXTRA_GCC_CHECKS),)\n  cmd_checkdoc = $(srctree)/scripts/kernel-doc -none $< ;\nendif\n\n# Do section mismatch analysis for each module/built-in.a\nifdef CONFIG_DEBUG_SECTION_MISMATCH\n  cmd_secanalysis = ; scripts/mod/modpost $@\nendif\n\n# Compile C sources (.c)\n# ---------------------------------------------------------------------------\n\n# Default is built-in, unless we know otherwise\nmodkern_cflags =                                          \\\n\t$(if $(part-of-module),                           \\\n\t\t$(KBUILD_CFLAGS_MODULE) $(CFLAGS_MODULE), \\\n\t\t$(KBUILD_CFLAGS_KERNEL) $(CFLAGS_KERNEL))\nquiet_modtag := $(empty)   $(empty)\n\n$(real-obj-m)        : part-of-module := y\n$(real-obj-m:.o=.i)  : part-of-module := y\n$(real-obj-m:.o=.s)  : part-of-module := y\n$(real-obj-m:.o=.lst): part-of-module := y\n\n$(real-obj-m)        : quiet_modtag := [M]\n$(real-obj-m:.o=.i)  : quiet_modtag := [M]\n$(real-obj-m:.o=.s)  : quiet_modtag := [M]\n$(real-obj-m:.o=.lst): quiet_modtag := [M]\n\n$(obj-m)             : quiet_modtag := [M]\n\nquiet_cmd_cc_s_c = CC $(quiet_modtag)  $@\ncmd_cc_s_c       = $(CC) $(c_flags) $(DISABLE_LTO) -fverbose-asm -S -o $@ $<\n\n$(obj)/%.s: $(src)/%.c FORCE\n\t$(call if_changed_dep,cc_s_c)\n\nquiet_cmd_cpp_i_c = CPP $(quiet_modtag) $@\ncmd_cpp_i_c       = $(CPP) $(c_flags) -o $@ $<\n\n$(obj)/%.i: $(src)/%.c FORCE\n\t$(call if_changed_dep,cpp_i_c)\n\n# These mirror gensymtypes_S and co below, keep them in synch.\ncmd_gensymtypes_c =                                                         \\\n    $(CPP) -D__GENKSYMS__ $(c_flags) $< |                                   \\\n    $(GENKSYMS) $(if $(1), -T $(2))                                         \\\n     $(patsubst y,-R,$(CONFIG_MODULE_REL_CRCS))                             \\\n     $(if $(KBUILD_PRESERVE),-p)                                            \\\n     -r $(firstword $(wildcard $(2:.symtypes=.symref) /dev/null))\n\nquiet_cmd_cc_symtypes_c = SYM $(quiet_modtag) $@\ncmd_cc_symtypes_c =                                                         \\\n    set -e;                                                                 \\\n    $(call cmd_gensymtypes_c,true,$@) >/dev/null;                           \\\n    test -s $@ || rm -f $@\n\n$(obj)/%.symtypes : $(src)/%.c FORCE\n\t$(call cmd,cc_symtypes_c)\n\n# LLVM assembly\n# Generate .ll files from .c\nquiet_cmd_cc_ll_c = CC $(quiet_modtag)  $@\n      cmd_cc_ll_c = $(CC) $(c_flags) -emit-llvm -S -o $@ $<\n\n$(obj)/%.ll: $(src)/%.c FORCE\n\t$(call if_changed_dep,cc_ll_c)\n\n# C (.c) files\n# The C file is compiled and updated dependency information is generated.\n# (See cmd_cc_o_c + relevant part of rule_cc_o_c)\n\nquiet_cmd_cc_o_c = CC $(quiet_modtag)  $@\n\nifndef CONFIG_MODVERSIONS\ncmd_cc_o_c = $(CC) $(c_flags) -c -o $@ $<\n\nelse\n# When module versioning is enabled the following steps are executed:\n# o compile a .tmp_<file>.o from <file>.c\n# o if .tmp_<file>.o doesn't contain a __ksymtab version, i.e. does\n#   not export symbols, we just rename .tmp_<file>.o to <file>.o and\n#   are done.\n# o otherwise, we calculate symbol versions using the good old\n#   genksyms on the preprocessed source and postprocess them in a way\n#   that they are usable as a linker script\n# o generate <file>.o from .tmp_<file>.o using the linker to\n#   replace the unresolved symbols __crc_exported_symbol with\n#   the actual value of the checksum generated by genksyms\n\ncmd_cc_o_c = $(CC) $(c_flags) -c -o $(@D)/.tmp_$(@F) $<\n\ncmd_modversions_c =\t\t\t\t\t\t\t\t\\\n\tif $(OBJDUMP) -h $(@D)/.tmp_$(@F) | grep -q __ksymtab; then\t\t\\\n\t\t$(call cmd_gensymtypes_c,$(KBUILD_SYMTYPES),$(@:.o=.symtypes))\t\\\n\t\t    > $(@D)/.tmp_$(@F:.o=.ver);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\t\t$(LD) $(LDFLAGS) -r -o $@ $(@D)/.tmp_$(@F) \t\t\t\\\n\t\t\t-T $(@D)/.tmp_$(@F:.o=.ver);\t\t\t\t\\\n\t\trm -f $(@D)/.tmp_$(@F) $(@D)/.tmp_$(@F:.o=.ver);\t\t\\\n\telse\t\t\t\t\t\t\t\t\t\\\n\t\tmv -f $(@D)/.tmp_$(@F) $@;\t\t\t\t\t\\\n\tfi;\nendif\n\nifdef CONFIG_FTRACE_MCOUNT_RECORD\n# gcc 5 supports generating the mcount tables directly\nifneq ($(call cc-option,-mrecord-mcount,y),y)\nKBUILD_CFLAGS += -mrecord-mcount\nelse\n# else do it all manually\nifdef BUILD_C_RECORDMCOUNT\nifeq (\"$(origin RECORDMCOUNT_WARN)\", \"command line\")\n  RECORDMCOUNT_FLAGS = -w\nendif\n# Due to recursion, we must skip empty.o.\n# The empty.o file is created in the make process in order to determine\n# the target endianness and word size. It is made before all other C\n# files, including recordmcount.\nsub_cmd_record_mcount =\t\t\t\t\t\\\n\tif [ $(@) != \"scripts/mod/empty.o\" ]; then\t\\\n\t\t$(objtree)/scripts/recordmcount $(RECORDMCOUNT_FLAGS) \"$(@)\";\t\\\n\tfi;\nrecordmcount_source := $(srctree)/scripts/recordmcount.c \\\n\t\t    $(srctree)/scripts/recordmcount.h\nelse\nsub_cmd_record_mcount = set -e ; perl $(srctree)/scripts/recordmcount.pl \"$(ARCH)\" \\\n\t\"$(if $(CONFIG_CPU_BIG_ENDIAN),big,little)\" \\\n\t\"$(if $(CONFIG_64BIT),64,32)\" \\\n\t\"$(OBJDUMP)\" \"$(OBJCOPY)\" \"$(CC) $(KBUILD_CFLAGS)\" \\\n\t\"$(LD)\" \"$(NM)\" \"$(RM)\" \"$(MV)\" \\\n\t\"$(if $(part-of-module),1,0)\" \"$(@)\";\nrecordmcount_source := $(srctree)/scripts/recordmcount.pl\nendif # BUILD_C_RECORDMCOUNT\ncmd_record_mcount =\t\t\t\t\t\t\\\n\tif [ \"$(findstring $(CC_FLAGS_FTRACE),$(_c_flags))\" =\t\\\n\t     \"$(CC_FLAGS_FTRACE)\" ]; then\t\t\t\\\n\t\t$(sub_cmd_record_mcount)\t\t\t\\\n\tfi;\nendif # -record-mcount\nendif # CONFIG_FTRACE_MCOUNT_RECORD\n\nifdef CONFIG_STACK_VALIDATION\nifneq ($(SKIP_STACK_VALIDATION),1)\n\n__objtool_obj := $(objtree)/tools/objtool/objtool\n\nobjtool_args = $(if $(CONFIG_UNWINDER_ORC),orc generate,check)\n\nobjtool_args += $(if $(part-of-module), --module,)\n\nifndef CONFIG_FRAME_POINTER\nobjtool_args += --no-fp\nendif\nifdef CONFIG_GCOV_KERNEL\nobjtool_args += --no-unreachable\nelse\nobjtool_args += $(call cc-ifversion, -lt, 0405, --no-unreachable)\nendif\nifdef CONFIG_RETPOLINE\nifneq ($(RETPOLINE_CFLAGS),)\n  objtool_args += --retpoline\nendif\nendif\n\n\nifdef CONFIG_MODVERSIONS\nobjtool_o = $(@D)/.tmp_$(@F)\nelse\nobjtool_o = $(@)\nendif\n\n# 'OBJECT_FILES_NON_STANDARD := y': skip objtool checking for a directory\n# 'OBJECT_FILES_NON_STANDARD_foo.o := 'y': skip objtool checking for a file\n# 'OBJECT_FILES_NON_STANDARD_foo.o := 'n': override directory skip for a file\ncmd_objtool = $(if $(patsubst y%,, \\\n\t$(OBJECT_FILES_NON_STANDARD_$(basetarget).o)$(OBJECT_FILES_NON_STANDARD)n), \\\n\t$(__objtool_obj) $(objtool_args) \"$(objtool_o)\";)\nobjtool_obj = $(if $(patsubst y%,, \\\n\t$(OBJECT_FILES_NON_STANDARD_$(basetarget).o)$(OBJECT_FILES_NON_STANDARD)n), \\\n\t$(__objtool_obj))\n\nendif # SKIP_STACK_VALIDATION\nendif # CONFIG_STACK_VALIDATION\n\n# Rebuild all objects when objtool changes, or is enabled/disabled.\nobjtool_dep = $(objtool_obj)\t\t\t\t\t\\\n\t      $(wildcard include/config/orc/unwinder.h\t\t\\\n\t\t\t include/config/stack/validation.h)\n\ndefine rule_cc_o_c\n\t$(call echo-cmd,checksrc) $(cmd_checksrc)\t\t\t  \\\n\t$(call cmd_and_fixdep,cc_o_c)\t\t\t\t\t  \\\n\t$(cmd_checkdoc)\t\t\t\t\t\t\t  \\\n\t$(call echo-cmd,objtool) $(cmd_objtool)\t\t\t\t  \\\n\t$(cmd_modversions_c)\t\t\t\t\t\t  \\\n\t$(call echo-cmd,record_mcount) $(cmd_record_mcount)\nendef\n\ndefine rule_as_o_S\n\t$(call cmd_and_fixdep,as_o_S)\t\t\t\t\t  \\\n\t$(call echo-cmd,objtool) $(cmd_objtool)\t\t\t\t  \\\n\t$(cmd_modversions_S)\nendef\n\n# List module undefined symbols (or empty line if not enabled)\nifdef CONFIG_TRIM_UNUSED_KSYMS\ncmd_undef_syms = $(NM) $@ | sed -n 's/^  *U //p' | xargs echo\nelse\ncmd_undef_syms = echo\nendif\n\n# Built-in and composite module parts\n$(obj)/%.o: $(src)/%.c $(recordmcount_source) $(objtool_dep) FORCE\n\t$(call cmd,force_checksrc)\n\t$(call if_changed_rule,cc_o_c)\n\n# Single-part modules are special since we need to mark them in $(MODVERDIR)\n\n$(single-used-m): $(obj)/%.o: $(src)/%.c $(recordmcount_source) $(objtool_dep) FORCE\n\t$(call cmd,force_checksrc)\n\t$(call if_changed_rule,cc_o_c)\n\t@{ echo $(@:.o=.ko); echo $@; \\\n\t   $(cmd_undef_syms); } > $(MODVERDIR)/$(@F:.o=.mod)\n\nquiet_cmd_cc_lst_c = MKLST   $@\n      cmd_cc_lst_c = $(CC) $(c_flags) -g -c -o $*.o $< && \\\n\t\t     $(CONFIG_SHELL) $(srctree)/scripts/makelst $*.o \\\n\t\t\t\t     System.map $(OBJDUMP) > $@\n\n$(obj)/%.lst: $(src)/%.c FORCE\n\t$(call if_changed_dep,cc_lst_c)\n\n# Compile assembler sources (.S)\n# ---------------------------------------------------------------------------\n\nmodkern_aflags := $(KBUILD_AFLAGS_KERNEL) $(AFLAGS_KERNEL)\n\n$(real-obj-m)      : modkern_aflags := $(KBUILD_AFLAGS_MODULE) $(AFLAGS_MODULE)\n$(real-obj-m:.o=.s): modkern_aflags := $(KBUILD_AFLAGS_MODULE) $(AFLAGS_MODULE)\n\n# .S file exports must have their C prototypes defined in asm/asm-prototypes.h\n# or a file that it includes, in order to get versioned symbols. We build a\n# dummy C file that includes asm-prototypes and the EXPORT_SYMBOL lines from\n# the .S file (with trailing ';'), and run genksyms on that, to extract vers.\n#\n# This is convoluted. The .S file must first be preprocessed to run guards and\n# expand names, then the resulting exports must be constructed into plain\n# EXPORT_SYMBOL(symbol); to build our dummy C file, and that gets preprocessed\n# to make the genksyms input.\n#\n# These mirror gensymtypes_c and co above, keep them in synch.\ncmd_gensymtypes_S =                                                         \\\n    (echo \"\\#include <linux/kernel.h>\" ;                                    \\\n     echo \"\\#include <asm/asm-prototypes.h>\" ;                              \\\n    $(CPP) $(a_flags) $< |                                                  \\\n     grep \"\\<___EXPORT_SYMBOL\\>\" |                                          \\\n     sed 's/.*___EXPORT_SYMBOL[[:space:]]*\\([a-zA-Z0-9_]*\\)[[:space:]]*,.*/EXPORT_SYMBOL(\\1);/' ) | \\\n    $(CPP) -D__GENKSYMS__ $(c_flags) -xc - |                                \\\n    $(GENKSYMS) $(if $(1), -T $(2))                                         \\\n     $(patsubst y,-R,$(CONFIG_MODULE_REL_CRCS))                             \\\n     $(if $(KBUILD_PRESERVE),-p)                                            \\\n     -r $(firstword $(wildcard $(2:.symtypes=.symref) /dev/null))\n\nquiet_cmd_cc_symtypes_S = SYM $(quiet_modtag) $@\ncmd_cc_symtypes_S =                                                         \\\n    set -e;                                                                 \\\n    $(call cmd_gensymtypes_S,true,$@) >/dev/null;                           \\\n    test -s $@ || rm -f $@\n\n$(obj)/%.symtypes : $(src)/%.S FORCE\n\t$(call cmd,cc_symtypes_S)\n\n\nquiet_cmd_cpp_s_S = CPP $(quiet_modtag) $@\ncmd_cpp_s_S       = $(CPP) $(a_flags) -o $@ $<\n\n$(obj)/%.s: $(src)/%.S FORCE\n\t$(call if_changed_dep,cpp_s_S)\n\nquiet_cmd_as_o_S = AS $(quiet_modtag)  $@\n\nifndef CONFIG_MODVERSIONS\ncmd_as_o_S = $(CC) $(a_flags) -c -o $@ $<\n\nelse\n\nASM_PROTOTYPES := $(wildcard $(srctree)/arch/$(SRCARCH)/include/asm/asm-prototypes.h)\n\nifeq ($(ASM_PROTOTYPES),)\ncmd_as_o_S = $(CC) $(a_flags) -c -o $@ $<\n\nelse\n\n# versioning matches the C process described above, with difference that\n# we parse asm-prototypes.h C header to get function definitions.\n\ncmd_as_o_S = $(CC) $(a_flags) -c -o $(@D)/.tmp_$(@F) $<\n\ncmd_modversions_S =\t\t\t\t\t\t\t\t\\\n\tif $(OBJDUMP) -h $(@D)/.tmp_$(@F) | grep -q __ksymtab; then\t\t\\\n\t\t$(call cmd_gensymtypes_S,$(KBUILD_SYMTYPES),$(@:.o=.symtypes))\t\\\n\t\t    > $(@D)/.tmp_$(@F:.o=.ver);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\t\t$(LD) $(LDFLAGS) -r -o $@ $(@D)/.tmp_$(@F) \t\t\t\\\n\t\t\t-T $(@D)/.tmp_$(@F:.o=.ver);\t\t\t\t\\\n\t\trm -f $(@D)/.tmp_$(@F) $(@D)/.tmp_$(@F:.o=.ver);\t\t\\\n\telse\t\t\t\t\t\t\t\t\t\\\n\t\tmv -f $(@D)/.tmp_$(@F) $@;\t\t\t\t\t\\\n\tfi;\nendif\nendif\n\n$(obj)/%.o: $(src)/%.S $(objtool_dep) FORCE\n\t$(call if_changed_rule,as_o_S)\n\ntargets += $(filter-out $(subdir-obj-y), $(real-obj-y)) $(real-obj-m) $(lib-y)\ntargets += $(extra-y) $(MAKECMDGOALS) $(always)\n\n# Linker scripts preprocessor (.lds.S -> .lds)\n# ---------------------------------------------------------------------------\nquiet_cmd_cpp_lds_S = LDS     $@\n      cmd_cpp_lds_S = $(CPP) $(cpp_flags) -P -U$(ARCH) \\\n\t                     -D__ASSEMBLY__ -DLINKER_SCRIPT -o $@ $<\n\n$(obj)/%.lds: $(src)/%.lds.S FORCE\n\t$(call if_changed_dep,cpp_lds_S)\n\n# ASN.1 grammar\n# ---------------------------------------------------------------------------\nquiet_cmd_asn1_compiler = ASN.1   $@\n      cmd_asn1_compiler = $(objtree)/scripts/asn1_compiler $< \\\n\t\t\t\t$(subst .h,.c,$@) $(subst .c,.h,$@)\n\n$(obj)/%.asn1.c $(obj)/%.asn1.h: $(src)/%.asn1 $(objtree)/scripts/asn1_compiler\n\t$(call cmd,asn1_compiler)\n\n# Build the compiled-in targets\n# ---------------------------------------------------------------------------\n\n# To build objects in subdirs, we need to descend into the directories\n$(sort $(subdir-obj-y)): $(subdir-ym) ;\n\n#\n# Rule to compile a set of .o files into one .o file\n#\nifdef builtin-target\n\n# built-in.a archives are made with no symbol table or index which\n# makes them small and fast, but unable to be used by the linker.\n# scripts/link-vmlinux.sh builds an aggregate built-in.a with a symbol\n# table and index.\nquiet_cmd_ar_builtin = AR      $@\n      cmd_ar_builtin = rm -f $@; \\\n                     $(AR) rcSTP$(KBUILD_ARFLAGS) $@ $(filter $(real-obj-y), $^)\n\n$(builtin-target): $(real-obj-y) FORCE\n\t$(call if_changed,ar_builtin)\n\ntargets += $(builtin-target)\nendif # builtin-target\n\n#\n# Rule to create modules.order file\n#\n# Create commands to either record .ko file or cat modules.order from\n# a subdirectory\nmodorder-cmds =\t\t\t\t\t\t\\\n\t$(foreach m, $(modorder),\t\t\t\\\n\t\t$(if $(filter %/modules.order, $m),\t\\\n\t\t\tcat $m;, echo kernel/$m;))\n\n$(modorder-target): $(subdir-ym) FORCE\n\t$(Q)(cat /dev/null; $(modorder-cmds)) > $@\n\n#\n# Rule to compile a set of .o files into one .a file\n#\nifdef lib-target\nquiet_cmd_link_l_target = AR      $@\n\n# lib target archives do get a symbol table and index\ncmd_link_l_target = rm -f $@; $(AR) rcsTP$(KBUILD_ARFLAGS) $@ $(lib-y)\n\n$(lib-target): $(lib-y) FORCE\n\t$(call if_changed,link_l_target)\n\ntargets += $(lib-target)\n\ndummy-object = $(obj)/.lib_exports.o\nksyms-lds = $(dot-target).lds\n\nquiet_cmd_export_list = EXPORTS $@\ncmd_export_list = $(OBJDUMP) -h $< | \\\n\tsed -ne '/___ksymtab/s/.*+\\([^ ]*\\).*/EXTERN(\\1)/p' >$(ksyms-lds);\\\n\trm -f $(dummy-object);\\\n\techo | $(CC) $(a_flags) -c -o $(dummy-object) -x assembler -;\\\n\t$(LD) $(ld_flags) -r -o $@ -T $(ksyms-lds) $(dummy-object);\\\n\trm $(dummy-object) $(ksyms-lds)\n\n$(obj)/lib-ksyms.o: $(lib-target) FORCE\n\t$(call if_changed,export_list)\n\ntargets += $(obj)/lib-ksyms.o\n\nendif\n\n#\n# Rule to link composite objects\n#\n#  Composite objects are specified in kbuild makefile as follows:\n#    <composite-object>-objs := <list of .o files>\n#  or\n#    <composite-object>-y    := <list of .o files>\n#  or\n#    <composite-object>-m    := <list of .o files>\n#  The -m syntax only works if <composite object> is a module\nlink_multi_deps =                     \\\n$(filter $(addprefix $(obj)/,         \\\n$($(subst $(obj)/,,$(@:.o=-objs)))    \\\n$($(subst $(obj)/,,$(@:.o=-y)))       \\\n$($(subst $(obj)/,,$(@:.o=-m)))), $^)\n\nquiet_cmd_link_multi-m = LD [M]  $@\ncmd_link_multi-m = $(LD) $(ld_flags) -r -o $@ $(link_multi_deps) $(cmd_secanalysis)\n\n$(multi-used-m): FORCE\n\t$(call if_changed,link_multi-m)\n\t@{ echo $(@:.o=.ko); echo $(link_multi_deps); \\\n\t   $(cmd_undef_syms); } > $(MODVERDIR)/$(@F:.o=.mod)\n$(call multi_depend, $(multi-used-m), .o, -objs -y -m)\n\ntargets += $(multi-used-m)\ntargets := $(filter-out $(PHONY), $(targets))\n\n# Add intermediate targets:\n# When building objects with specific suffix patterns, add intermediate\n# targets that the final targets are derived from.\nintermediate_targets = $(foreach sfx, $(2), \\\n\t\t\t\t$(patsubst %$(strip $(1)),%$(sfx), \\\n\t\t\t\t\t$(filter %$(strip $(1)), $(targets))))\n# %.asn1.o <- %.asn1.[ch] <- %.asn1\n# %.dtb.o <- %.dtb.S <- %.dtb <- %.dts\n# %.lex.o <- %.lex.c <- %.l\n# %.tab.o <- %.tab.[ch] <- %.y\ntargets += $(call intermediate_targets, .asn1.o, .asn1.c .asn1.h) \\\n\t   $(call intermediate_targets, .dtb.o, .dtb.S .dtb) \\\n\t   $(call intermediate_targets, .lex.o, .lex.c) \\\n\t   $(call intermediate_targets, .tab.o, .tab.c .tab.h)\n\n# Descending\n# ---------------------------------------------------------------------------\n\nPHONY += $(subdir-ym)\n$(subdir-ym):\n\t$(Q)$(MAKE) $(build)=$@ need-builtin=$(if $(findstring $@,$(subdir-obj-y)),1)\n\n# Add FORCE to the prequisites of a target to force it to be always rebuilt.\n# ---------------------------------------------------------------------------\n\nPHONY += FORCE\n\nFORCE:\n\n# Read all saved command lines and dependencies for the $(targets) we\n# may be building above, using $(if_changed{,_dep}). As an\n# optimization, we don't need to read them if the target does not\n# exist, we will rebuild anyway in that case.\n\ncmd_files := $(wildcard $(foreach f,$(sort $(targets)),$(dir $(f)).$(notdir $(f)).cmd))\n\nifneq ($(cmd_files),)\n  include $(cmd_files)\nendif\n\nifneq ($(KBUILD_SRC),)\n# Create directories for object files if they do not exist\nobj-dirs := $(sort $(obj) $(patsubst %/,%, $(dir $(targets))))\n# If cmd_files exist, their directories apparently exist.  Skip mkdir.\nexist-dirs := $(sort $(patsubst %/,%, $(dir $(cmd_files))))\nobj-dirs := $(strip $(filter-out $(exist-dirs), $(obj-dirs)))\nifneq ($(obj-dirs),)\n$(shell mkdir -p $(obj-dirs))\nendif\nendif\n\n# Some files contained in $(targets) are intermediate artifacts.\n# We never want them to be removed automatically.\n.SECONDARY: $(targets)\n\n# Declare the contents of the .PHONY variable as phony.  We keep that\n# information in a variable se we can use it in if_changed and friends.\n\n.PHONY: $(PHONY)\n"], "filenames": ["Documentation/trace/histogram.txt", "kernel/softirq.c", "kernel/trace/trace.c", "kernel/trace/trace_events_filter.c", "scripts/Makefile.build"], "buggy_code_start_loc": [1732, 141, 1363, 81, 241], "buggy_code_end_loc": [1964, 145, 1381, 549, 265], "fixing_code_start_loc": [1732, 142, 1362, 81, 242], "fixing_code_end_loc": [1965, 149, 1377, 558, 264], "type": "CWE-787", "message": "An issue was discovered in the Linux kernel through 4.17.2. The filter parsing in kernel/trace/trace_events_filter.c could be called with no filter, which is an N=0 case when it expected at least one line to have been read, thus making the N-1 index invalid. This allows attackers to cause a denial of service (slab out-of-bounds write) or possibly have unspecified other impact via crafted perf_event_open and mmap system calls.", "other": {"cve": {"id": "CVE-2018-12714", "sourceIdentifier": "cve@mitre.org", "published": "2018-06-24T23:29:00.333", "lastModified": "2023-01-19T16:13:17.070", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel through 4.17.2. The filter parsing in kernel/trace/trace_events_filter.c could be called with no filter, which is an N=0 case when it expected at least one line to have been read, thus making the N-1 index invalid. This allows attackers to cause a denial of service (slab out-of-bounds write) or possibly have unspecified other impact via crafted perf_event_open and mmap system calls."}, {"lang": "es", "value": "Se ha descubierto un problema en el kernel de Linux hasta la versi\u00f3n 4.17.2. El an\u00e1lisis del filtros en kernel/trace/trace_events_filter.c podr\u00eda ser llamado sin un filtro, que es un caso N=0 cuando espera que, por lo menos, se haya le\u00eddo una l\u00ednea, lo que hace que el \u00edndice N-1 no sea v\u00e1lido. Esto permite que los atacantes provoquen una denegaci\u00f3n de servicio (escritura fuera de l\u00edmites del slab) u otro tipo de impacto mediante llamadas del sistema perf_event_open y mmap."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 10.0}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.17", "versionEndExcluding": "4.17.4", "matchCriteriaId": "F5CFAAEA-2412-461B-AA38-9031C852E11E"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=81f9c4e4177d31ced6f52a89bb70e93bfb77ca03", "source": "cve@mitre.org", "tags": ["Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/104544", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.kernel.org/show_bug.cgi?id=200019", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Vendor Advisory"]}, {"url": "https://github.com/lcytxw/bug_repro/tree/master/bug_200019", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/81f9c4e4177d31ced6f52a89bb70e93bfb77ca03", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/81f9c4e4177d31ced6f52a89bb70e93bfb77ca03"}}