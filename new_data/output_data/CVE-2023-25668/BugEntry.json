{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <ostream>\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/strided_slice_op.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\nusing shape_inference::UnchangedShape;\n\nnamespace {\n\nStatus GetAxisForPackAndUnpack(InferenceContext* c, int32_t rank_after_pack,\n                               int32* axis) {\n  TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", axis));\n  if (*axis < -1 * rank_after_pack || *axis >= rank_after_pack) {\n    return errors::InvalidArgument(\"Invalid axis: \", *axis, \"; must be in [\",\n                                   -1 * rank_after_pack, \",\", rank_after_pack,\n                                   \")\");\n  }\n  if (*axis < 0) *axis = (rank_after_pack + *axis);\n  return OkStatus();\n}\n\ntemplate <typename T>\nstd::vector<int64_t> AsInt64(const Tensor* tensor, int64_t num_elements) {\n  std::vector<int64_t> ret(num_elements);\n  auto data = tensor->vec<T>();\n  for (int64_t i = 0; i < num_elements; ++i) {\n    ret[i] = data(i);\n  }\n  return ret;\n}\n\ntemplate <typename T>\nStatus PadKnown(InferenceContext* c, ShapeHandle input,\n                const Tensor* paddings_t, int64_t num_dims) {\n  // paddings_t is known.\n  std::vector<DimensionHandle> dims(num_dims);\n  auto paddings_data = paddings_t->matrix<T>();\n  for (int64_t i = 0; i < num_dims; ++i) {\n    const T pad0 = paddings_data(i, 0);\n    const T pad1 = paddings_data(i, 1);\n    if (pad0 < 0 || pad1 < 0) {\n      return errors::InvalidArgument(\"Paddings must be non-negative\");\n    }\n    TF_RETURN_IF_ERROR(c->Add(c->Dim(input, i), pad0 + pad1, &dims[i]));\n  }\n  c->set_output(0, c->MakeShape(dims));\n  return OkStatus();\n}\n\nStatus PadShapeFn(InferenceContext* c) {\n  // Paddings is a matrix of [input_rank, 2].\n  ShapeHandle paddings;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 2, &paddings));\n  DimensionHandle unused;\n  TF_RETURN_IF_ERROR(c->WithValue(c->Dim(paddings, 1), 2, &unused));\n\n  // n_dim and input.rank are equivalent.\n  ShapeHandle input = c->input(0);\n  DimensionHandle n_dim = c->Dim(paddings, 0);\n  if (c->ValueKnown(n_dim)) {\n    TF_RETURN_IF_ERROR(c->WithRank(input, c->Value(n_dim), &input));\n  } else if (c->RankKnown(input)) {\n    TF_RETURN_IF_ERROR(c->WithValue(n_dim, c->Rank(input), &n_dim));\n  }\n\n  const Tensor* paddings_t = c->input_tensor(1);\n\n  // paddings_t is unknown\n  if (paddings_t == nullptr) {\n    if (c->ValueKnown(n_dim)) {\n      // Make output with n_dim unknown dims.\n      c->set_output(0, c->UnknownShapeOfRank(c->Value(n_dim)));\n    } else {\n      c->set_output(0, c->UnknownShape());\n    }\n    return OkStatus();\n  }\n\n  const int64_t num_dims = paddings_t->shape().dim_size(0);\n  TF_RETURN_IF_ERROR(c->WithRank(input, num_dims, &input));\n  TF_RETURN_IF_ERROR(c->WithValue(n_dim, num_dims, &n_dim));\n\n  if (paddings_t->dtype() == DT_INT32) {\n    return PadKnown<int32>(c, input, paddings_t, num_dims);\n  } else {\n    return PadKnown<int64_t>(c, input, paddings_t, num_dims);\n  }\n}\n\nStatus TransposeShapeFn(InferenceContext* c) {\n  ShapeHandle input = c->input(0);\n  ShapeHandle perm_shape = c->input(1);\n  const Tensor* perm = c->input_tensor(1);\n  DimensionHandle perm_elems = c->NumElements(perm_shape);\n  // If we don't have rank information on the input or value information on\n  // perm we can't return any shape information, otherwise we have enough\n  // information to at least find the rank of the output.\n  if (!c->RankKnown(input) && !c->ValueKnown(perm_elems) && perm == nullptr) {\n    c->set_output(0, c->UnknownShape());\n    return OkStatus();\n  }\n\n  // Find our value of the rank.\n  int64_t rank;\n  if (c->RankKnown(input)) {\n    rank = c->Rank(input);\n  } else if (c->ValueKnown(perm_elems)) {\n    rank = c->Value(perm_elems);\n  } else {\n    rank = perm->NumElements();\n  }\n  if (!c->RankKnown(input) && rank < 2) {\n    // A permutation array containing a single element is ambiguous. It could\n    // indicate either a scalar or a 1-dimensional array, both of which the\n    // transpose op returns unchanged.\n    c->set_output(0, input);\n    return OkStatus();\n  }\n\n  std::vector<DimensionHandle> dims;\n  dims.resize(rank);\n  TF_RETURN_IF_ERROR(c->WithRank(input, rank, &input));\n  // Ensure that perm is a vector and has rank elements.\n  TF_RETURN_IF_ERROR(c->WithRank(perm_shape, 1, &perm_shape));\n  TF_RETURN_IF_ERROR(c->WithValue(perm_elems, rank, &perm_elems));\n\n  // If we know the rank of the input and the value of perm, we can return\n  // all shape information, otherwise we can only return rank information,\n  // but no information for the dimensions.\n  if (perm != nullptr) {\n    std::vector<int64_t> data;\n    if (perm->dtype() == DT_INT32) {\n      data = AsInt64<int32>(perm, rank);\n    } else {\n      data = AsInt64<int64_t>(perm, rank);\n    }\n\n    for (int32_t i = 0; i < rank; ++i) {\n      int64_t in_idx = data[i];\n      if (in_idx >= rank || in_idx <= -rank) {\n        return errors::InvalidArgument(\"perm dim \", in_idx,\n                                       \" is out of range of input rank \", rank);\n      }\n      dims[i] = c->Dim(input, in_idx);\n    }\n  } else {\n    for (int i = 0; i < rank; ++i) {\n      dims[i] = c->UnknownDim();\n    }\n  }\n\n  c->set_output(0, c->MakeShape(dims));\n  return OkStatus();\n}\n\nStatus SetOutputShapeForReshape(InferenceContext* c) {\n  ShapeHandle in = c->input(0);\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &out));\n\n  if (!c->RankKnown(out)) {\n    // We have no information about the shape of the output.\n    c->set_output(0, out);\n    return OkStatus();\n  }\n  if (c->RankKnown(in)) {\n    // We don't know the number of output elements, but we can try to infer\n    // the missing dimension.\n    bool too_many_unknown = false;\n    int32_t out_unknown_idx = -1;\n\n    DimensionHandle known_out_elems = c->NumElements(out);\n    if (!c->ValueKnown(known_out_elems)) {\n      known_out_elems = c->MakeDim(1);\n      for (int32_t i = 0; i < c->Rank(out); ++i) {\n        DimensionHandle dim = c->Dim(out, i);\n        if (!c->ValueKnown(dim)) {\n          if (out_unknown_idx >= 0) {\n            too_many_unknown = true;\n            break;\n          }\n          out_unknown_idx = i;\n        } else {\n          TF_RETURN_IF_ERROR(\n              c->Multiply(known_out_elems, dim, &known_out_elems));\n        }\n      }\n    }\n    int32_t in_unknown_idx = -1;\n    DimensionHandle known_in_elems = c->NumElements(in);\n    if (!c->ValueKnown(known_in_elems)) {\n      known_in_elems = c->MakeDim(1);\n      for (int32_t i = 0; i < c->Rank(in); ++i) {\n        DimensionHandle dim = c->Dim(in, i);\n        if (!c->ValueKnown(dim)) {\n          if (in_unknown_idx >= 0) {\n            too_many_unknown = true;\n            break;\n          }\n          in_unknown_idx = i;\n        } else {\n          TF_RETURN_IF_ERROR(c->Multiply(known_in_elems, dim, &known_in_elems));\n        }\n      }\n    }\n\n    if (!too_many_unknown) {\n      if (in_unknown_idx < 0 && out_unknown_idx < 0) {\n        // Just check that the dimensions match.\n        if (c->Value(known_in_elems) != c->Value(known_out_elems)) {\n          return errors::InvalidArgument(\n              \"Cannot reshape a tensor with \", c->DebugString(known_in_elems),\n              \" elements to shape \", c->DebugString(out), \" (\",\n              c->DebugString(known_out_elems), \" elements)\");\n        }\n      } else if (in_unknown_idx < 0 && out_unknown_idx >= 0 &&\n                 c->Value(known_out_elems) > 0) {\n        // Input fully known, infer the one missing output dim\n        DimensionHandle inferred_dim;\n        TF_RETURN_IF_ERROR(c->Divide(known_in_elems, c->Value(known_out_elems),\n                                     true /* evenly_divisible */,\n                                     &inferred_dim));\n        TF_RETURN_IF_ERROR(\n            c->ReplaceDim(out, out_unknown_idx, inferred_dim, &out));\n\n      } else if (in_unknown_idx >= 0 && out_unknown_idx < 0 &&\n                 c->Value(known_in_elems) != 0) {\n        // Output fully known, infer the one missing input dim\n        DimensionHandle inferred_dim;\n        TF_RETURN_IF_ERROR(c->Divide(known_out_elems, c->Value(known_in_elems),\n                                     true /* evenly_divisible */,\n                                     &inferred_dim));\n        DimensionHandle unknown_in_dim = c->Dim(in, in_unknown_idx);\n        TF_RETURN_IF_ERROR(\n            c->Merge(unknown_in_dim, inferred_dim, &unknown_in_dim));\n      } else if (in_unknown_idx >= 0 && out_unknown_idx >= 0) {\n        // Exactly one unknown dimension in both input and output. These 2 are\n        // equal iff the known elements are equal.\n        if (c->Value(known_in_elems) == c->Value(known_out_elems)) {\n          DimensionHandle unknown_in_dim = c->Dim(in, in_unknown_idx);\n          TF_RETURN_IF_ERROR(\n              c->ReplaceDim(out, out_unknown_idx, unknown_in_dim, &out));\n        }\n      }\n    }\n  }\n  c->set_output(0, out);\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"ParallelConcat\")\n    .Input(\"values: N * T\")\n    .Output(\"output: T\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: type\")\n    .Attr(\"shape: shape\")\n    .SetShapeFn([](InferenceContext* c) {\n      // Validate that the shape attr is correct.\n      PartialTensorShape shape;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shape\", &shape));\n      ShapeHandle passed_shape;\n      TF_RETURN_IF_ERROR(\n          c->MakeShapeFromPartialTensorShape(shape, &passed_shape));\n      if (!c->FullyDefined(passed_shape)) {\n        return errors::InvalidArgument(\"shape attr must be fully defined.\");\n      }\n      ShapeHandle cur;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(\n          passed_shape, 0, c->MakeDim(shape_inference::DimensionOrConstant(1)),\n          &cur));\n      for (int i = 0; i < c->num_inputs(); ++i) {\n        if (!c->FullyDefined(c->input(i))) {\n          return errors::InvalidArgument(\n              \"All input shapes must be fully defined.\");\n        }\n        DimensionHandle unused;\n        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {\n          return errors::InvalidArgument(\"Size of first dimension must be 1.\");\n        }\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\n                                        \"From merging shape \", i,\n                                        \" with other shapes.\");\n      }\n\n      c->set_output(0, passed_shape);\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Pack\")\n    .Input(\"values: N * T\")\n    .Output(\"output: T\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: type\")\n    .Attr(\"axis: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      // Validate shapes of all inputs are compatible\n      ShapeHandle cur = c->input(c->num_inputs() - 1);\n      for (int i = c->num_inputs() - 2; i >= 0; --i) {\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\n                                        \"From merging shape \", i,\n                                        \" with other shapes.\");\n      }\n      if (!c->RankKnown(cur)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n      // Determine the axis that will be added, converting from negative\n      // axes to a positive point per negative indexing rules.\n      int32_t rank = c->Rank(cur);\n      int32_t axis;\n      TF_RETURN_IF_ERROR(GetAxisForPackAndUnpack(c, rank + 1, &axis));\n\n      // Copy all dimensions over, inserting a dimension of value #inputs\n      // at <axis>.\n      std::vector<DimensionHandle> dims;\n      int index = 0;\n      while (index < axis) dims.push_back(c->Dim(cur, index++));\n      dims.push_back(c->MakeDim(c->num_inputs()));\n      while (index < rank) dims.push_back(c->Dim(cur, index++));\n\n      c->set_output(0, c->MakeShape(dims));\n      for (int i = 0; i < c->num_inputs(); ++i) {\n        auto* shape_and_type = c->input_handle_shapes_and_types(i);\n        if (shape_and_type) {\n          if (!c->RelaxOutputHandleShapesAndMergeTypes(0, *shape_and_type)) {\n            c->set_output_handle_shapes_and_types(\n                0, std::vector<shape_inference::ShapeAndType>({}));\n            break;\n          }\n        }\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"DeepCopy\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetIsStateful()\n    .SetShapeFn(UnchangedShape);\n\nREGISTER_OP(\"InplaceUpdate\")\n    .Input(\"x: T\")\n    .Input(\"i: int32\")\n    .Input(\"v: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(UnchangedShape);\n\nREGISTER_OP(\"InplaceAdd\")\n    .Input(\"x: T\")\n    .Input(\"i: int32\")\n    .Input(\"v: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(UnchangedShape);\n\nREGISTER_OP(\"InplaceSub\")\n    .Input(\"x: T\")\n    .Input(\"i: int32\")\n    .Input(\"v: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(UnchangedShape);\n\nREGISTER_OP(\"Empty\")\n    .Input(\"shape: int32\")\n    .Output(\"output: dtype\")\n    .Attr(\"dtype: type\")\n    .Attr(\"init: bool = false\")\n    .SetDoNotOptimize()\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Unpack\")\n    .Input(\"value: T\")\n    .Output(\"output: num * T\")\n    .Attr(\"num: int >= 0\")\n    .Attr(\"T: type\")\n    .Attr(\"axis: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s = c->input(0);\n      ShapeHandle out;\n      if (c->RankKnown(s)) {\n        // Determine the axis that will be removed, converting from negative\n        // axes to a positive point per negative indexing rules.\n        int32_t rank = c->Rank(s);\n        int32_t axis;\n        TF_RETURN_IF_ERROR(GetAxisForPackAndUnpack(c, rank, &axis));\n\n        // The axis dim matches the number of outputs.\n        DimensionHandle unused;\n        TF_RETURN_IF_ERROR(\n            c->WithValue(c->Dim(s, axis), c->num_outputs(), &unused));\n\n        // Copy all dimensions, removing the <axis> dimension.\n        std::vector<DimensionHandle> dims;\n        for (int i = 0; i < rank; ++i) {\n          if (i != axis) dims.push_back(c->Dim(s, i));\n        }\n        out = c->MakeShape(dims);\n      } else {\n        // All outputs are the same shape, but it's not known.\n        out = c->UnknownShape();\n      }\n      for (int i = 0; i < c->num_outputs(); ++i) c->set_output(i, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"UnravelIndex\")\n    .Input(\"indices: Tidx\")\n    .Input(\"dims: Tidx\")\n    .Output(\"output: Tidx\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle indices = c->input(0);\n      ShapeHandle dims;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &dims));\n      if (c->RankKnown(indices) && c->Rank(indices) == 0) {\n        c->set_output(0, c->Vector(c->Dim(dims, 0)));\n      } else if (c->RankKnown(indices)) {\n        c->set_output(0, c->Matrix(c->Dim(dims, 0), c->NumElements(indices)));\n      } else {\n        c->set_output(0, c->UnknownShape());\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"BroadcastTo\")\n    .Input(\"input: T\")\n    .Input(\"shape: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle shape_in = c->input(1);\n      TF_RETURN_IF_ERROR(c->WithRank(shape_in, 1, &shape_in));\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &out));\n      if (!c->RankKnown(out)) {\n        // We have no information about the shape of the output.\n        c->set_output(0, out);\n        return OkStatus();\n      }\n\n      ShapeHandle in = c->input(0);\n      if (!c->RankKnown(in)) {\n        // We have no information about the shape of the input,\n        // nothing to do here.\n        c->set_output(0, out);\n        return OkStatus();\n      }\n      int out_rank = c->Rank(out);\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(in, out_rank, &in));\n      int in_rank = c->Rank(in);\n      for (int i = 0; i < in_rank; ++i) {\n        auto in_dim = c->Dim(in, in_rank - i - 1);\n        if (c->Value(in_dim) > 1) {\n          // If the input dimension is greater than 1 then the output dimension\n          // must be equal to it, since we only broadcast \"from left to right\".\n          auto out_dim = c->Dim(out, out_rank - i - 1);\n          TF_RETURN_IF_ERROR(c->Merge(in_dim, out_dim, &out_dim));\n          TF_RETURN_IF_ERROR(\n              c->ReplaceDim(out, out_rank - i - 1, out_dim, &out));\n        }\n      }\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n// TODO(josh11b): Remove the >= 2 constraint, once we can rewrite the graph\n// in the N == 1 case to remove the node.\nREGISTER_OP(\"Concat\")\n    .Input(\"concat_dim: int32\")\n    .Input(\"values: N * T\")\n    .Output(\"output: T\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::ConcatShape(c, c->num_inputs() - 1);\n    });\n\nREGISTER_OP(\"ConcatV2\")\n    .Input(\"values: N * T\")\n    .Input(\"axis: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ConcatV2Shape);\n\n// TODO(vivek.v.rane@intel.com): Prefix the op names with underscore if the ops\n// are not to be made user-accessible.\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklConcatV2\")\n    .Input(\"values: N * T\")\n    .Input(\"axis: Tidx\")\n    .Input(\"mkl_values: N * uint8\")\n    .Input(\"mkl_axis: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ConcatV2Shape)\n    .Doc(R\"doc(\nMKL version of ConcatV2 operator. Uses MKL DNN APIs to perform concatenation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\nREGISTER_OP(\"ConcatOffset\")\n    .Input(\"concat_dim: int32\")\n    .Input(\"shape: N * int32\")\n    .Output(\"offset: N * int32\")\n    .Attr(\"N: int >= 2\")\n    .SetShapeFn([](InferenceContext* c) {\n      for (int i = 1; i < c->num_inputs(); ++i) {\n        c->set_output(i - 1, c->input(i));\n      }\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Split\")\n    .Input(\"split_dim: int32\")\n    .Input(\"value: T\")\n    .Output(\"output: num_split * T\")\n    .Attr(\"num_split: int >= 1\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      DimensionHandle split_dimension;\n      ShapeHandle input = c->input(1);\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInputWithNegativeIndexing(\n          0, c->Rank(input), &split_dimension));\n      int num_split = c->num_outputs();\n      ShapeHandle out;\n      if (!c->ValueKnown(split_dimension)) {\n        if (c->RankKnown(input)) {\n          out = c->UnknownShapeOfRank(c->Rank(input));\n        } else {\n          out = c->UnknownShape();\n        }\n      } else {\n        int64_t split_dim = c->Value(split_dimension);\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(input, split_dim + 1, &input));\n        DimensionHandle split_dim_size;\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(\n            c->Divide(c->Dim(input, split_dim), num_split,\n                      true /* evenly_divisible */, &split_dim_size),\n            \"Number of ways to split should evenly divide the split dimension\");\n        TF_RETURN_IF_ERROR(\n            c->ReplaceDim(input, split_dim, split_dim_size, &out));\n      }\n      for (int i = 0; i < num_split; ++i) c->set_output(i, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"SplitV\")\n    .Input(\"value: T\")\n    .Input(\"size_splits: Tlen\")\n    .Input(\"split_dim: int32\")\n    .Output(\"output: num_split * T\")\n    .Attr(\"num_split: int >= 1\")\n    .Attr(\"T: type\")\n    .Attr(\"Tlen: {int8, int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      DimensionHandle split_dimension;\n      ShapeHandle input = c->input(0);\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInputWithNegativeIndexing(\n          2, c->Rank(input), &split_dimension));\n      int32_t num_outputs = c->num_outputs();\n      int32_t rank = c->Rank(input);\n      ShapeHandle output_shape;\n      const Tensor* size_splits = c->input_tensor(1);\n      if (rank == InferenceContext::kUnknownRank) {\n        // If the rank of input tensor is unknown, then return unknown shapes.\n        // Note that the shape of each output can be different.\n        for (int i = 0; i < num_outputs; ++i) {\n          c->set_output(i, c->UnknownShape());\n        }\n      } else if (rank == 0) {\n        // Throw error if input is a scalar.\n        return errors::InvalidArgument(\"Can't split scalars\");\n      } else if (size_splits == nullptr && c->ValueKnown(split_dimension)) {\n        // If split dimension is known, but the sizes are unknown, then\n        // only the split dimension is unknown\n        output_shape = input;\n        for (int i = 0; i < num_outputs; ++i) {\n          TF_RETURN_IF_ERROR(c->ReplaceDim(output_shape,\n                                           c->Value(split_dimension),\n                                           c->UnknownDim(), &output_shape));\n          c->set_output(i, output_shape);\n        }\n      } else if (size_splits == nullptr && !c->ValueKnown(split_dimension)) {\n        // If split dimension or tensor containing the split sizes is unknown,\n        // then return unknown shapes of same rank as input. Note that each\n        // output shape can be different since splitv doesn't always split\n        // tensors evenly.\n        for (int i = 0; i < num_outputs; ++i) {\n          c->set_output(i, c->UnknownShapeOfRank(rank));\n        }\n      } else {\n        // Determine the output shape if split dimension and split sizes are\n        // known.\n        int64_t split_dim = c->Value(split_dimension);\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(input, split_dim + 1, &input));\n        std::vector<int64_t> data;\n        if (size_splits->dtype() == DT_INT32) {\n          data = AsInt64<int32>(size_splits, size_splits->shape().dim_size(0));\n        } else {\n          data =\n              AsInt64<int64_t>(size_splits, size_splits->shape().dim_size(0));\n        }\n        if (num_outputs != data.size()) {\n          return errors::InvalidArgument(\n              \"Length of size_splits should be equal to num_outputs\");\n        }\n        int64_t total_size = 0;\n        bool has_neg_one = false;\n        for (const auto size : data) {\n          if (size == -1) {\n            if (has_neg_one) {\n              return errors::InvalidArgument(\n                  \"size_splits can only have one -1\");\n            }\n            has_neg_one = true;\n          } else {\n            total_size += size;\n          }\n        }\n        auto split_dim_size = c->Value(c->Dim(input, split_dim));\n        // If the sizes of the splits are known, then\n        // make sure that the sizes add up to the expected\n        // dimension size, with the possibility of a -1.\n        // Specify the full output shapes.\n        for (int i = 0; i < num_outputs; ++i) {\n          auto size = data[i];\n          if (data[i] == -1 && c->ValueKnown(split_dim_size)) {\n            size = split_dim_size - total_size;\n          }\n          // If we have a negative known size (either explicit, or computed\n          // via -1), then the split sizes are invalid.\n          if (size < -1 || (size == -1 && c->ValueKnown(split_dim_size))) {\n            return errors::InvalidArgument(\"Split size at index \", i,\n                                           \" must be >= 0. Got: \", size);\n          }\n          TF_RETURN_IF_ERROR(\n              c->ReplaceDim(input, split_dim, c->MakeDim(size), &output_shape));\n          c->set_output(i, output_shape);\n        }\n        if (c->ValueKnown(split_dim_size)) {\n          if (has_neg_one ? total_size > split_dim_size\n                          : total_size != split_dim_size) {\n            return errors::InvalidArgument(\n                \"can't split axis of size \", split_dim_size,\n                \" into pieces of size [\", absl::StrJoin(data, \",\"), \"]\");\n          }\n        }\n      }\n\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Const\")\n    .Output(\"output: dtype\")\n    .Attr(\"value: tensor\")\n    .Attr(\"dtype: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      const TensorProto* proto = nullptr;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"value\", &proto));\n      TF_RETURN_IF_ERROR(TensorShape::IsValidShape(proto->tensor_shape()));\n      TensorShape shape(proto->tensor_shape());\n      std::vector<DimensionHandle> dims;\n      dims.reserve(shape.dims());\n      for (int i = 0; i < shape.dims(); ++i) {\n        dims.push_back(c->MakeDim(shape.dim_size(i)));\n      }\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\n// Returns a constant tensor on the host.  Useful for writing C++ tests\n// and benchmarks which run on GPU but require arguments pinned to the host.\n// Used by test::graph::HostConstant.\n// value: Attr `value` is the tensor to return.\nREGISTER_OP(\"HostConst\")\n    .Output(\"output: dtype\")\n    .Attr(\"value: tensor\")\n    .Attr(\"dtype: type\")\n    .SetShapeFn(shape_inference::UnknownShape);\n\n// Used executing op-by-op to copy constants to the current device without\n// serializing tensors as TensorProtos, after a host tensor has been\n// created. Same behavior as Identity, but no gradient and potentially relaxed\n// copy semantics.\nREGISTER_OP(\"_EagerConst\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\n// TODO(mgubin): Update the doc when the freeze_graph script supports converting\n// into memmapped format.\nREGISTER_OP(\"ImmutableConst\")\n    .Attr(\"dtype: type\")\n    .Attr(\"shape: shape\")\n    .Attr(\"memory_region_name: string\")\n    .Output(\"tensor: dtype\")\n    .SetShapeFn(shape_inference::ExplicitShape);\n\nREGISTER_OP(\"GuaranteeConst\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      return UnchangedShape(c);\n    })\n    // We don't want this to be optimized away.\n    .SetDoNotOptimize();\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ZerosLike\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"OnesLike\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, uint8, int16, uint16, int32, \"\n        \"uint32, int64, uint64, complex64, complex128, bool}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Diag\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int32, int64, complex64, \"\n        \"complex128}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle in = c->input(0);\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(in, 1, &in));\n      // Output shape is original concatenated with itself.\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->Concatenate(in, in, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"DiagPart\")\n    .Input(\"input: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int32, int64, complex64, \"\n        \"complex128}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle in = c->input(0);\n      if (!c->RankKnown(in)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n      // Rank must be even, and result will have rank <rank/2>.\n      const int32_t rank = c->Rank(in);\n      if ((rank % 2) != 0 || rank <= 0) {\n        return errors::InvalidArgument(\n            \"Input must have even and non-zero rank, input rank is \", rank);\n      }\n      const int32_t mid = rank / 2;\n\n      // output dim[i] is the merge of in.dim[i] and in.dim[i+mid].\n      std::vector<DimensionHandle> dims(mid);\n      for (int i = 0; i < mid; ++i) {\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(in, i), c->Dim(in, i + mid), &dims[i]));\n      }\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MatrixDiag\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle in;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &in));\n      if (!c->RankKnown(in)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n      const int32_t rank = c->Rank(in);\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(\n          c->Concatenate(in, c->Vector(c->Dim(in, rank - 1)), &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MatrixDiagV2\")\n    .Input(\"diagonal: T\")\n    .Input(\"k: int32\")\n    .Input(\"num_rows: int32\")\n    .Input(\"num_cols: int32\")\n    .Input(\"padding_value: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::MatrixDiagV2Shape);\n\nREGISTER_OP(\"MatrixDiagV3\")\n    .Input(\"diagonal: T\")\n    .Input(\"k: int32\")\n    .Input(\"num_rows: int32\")\n    .Input(\"num_cols: int32\")\n    .Input(\"padding_value: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\n        \"align: {'LEFT_RIGHT', 'RIGHT_LEFT', 'LEFT_LEFT', 'RIGHT_RIGHT'} = \"\n        \"'RIGHT_LEFT'\")\n    .SetShapeFn(shape_inference::MatrixDiagV2Shape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MatrixSetDiag\")\n    .Input(\"input: T\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      ShapeHandle diag;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 2, &input));\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &diag));\n      if (c->RankKnown(input)) {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(1), c->Rank(input) - 1, &diag));\n      }\n      DimensionHandle smallest_dim;\n      TF_RETURN_IF_ERROR(\n          c->Min(c->Dim(input, -2), c->Dim(input, -1), &smallest_dim));\n      TF_RETURN_IF_ERROR(\n          c->Merge(smallest_dim, c->Dim(diag, -1), &smallest_dim));\n\n      ShapeHandle output = input;\n      if (c->RankKnown(diag) && !c->FullyDefined(input)) {\n        // Try to infer parts of shape from diag.\n        ShapeHandle diag_batch_shape;\n        TF_RETURN_IF_ERROR(c->Subshape(diag, 0, -1, &diag_batch_shape));\n        TF_RETURN_IF_ERROR(\n            c->Concatenate(diag_batch_shape, c->UnknownShapeOfRank(2), &diag));\n        TF_RETURN_IF_ERROR(c->Merge(input, diag, &output));\n      }\n      c->set_output(0, output);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MatrixSetDiagV2\")\n    .Input(\"input: T\")\n    .Input(\"diagonal: T\")\n    .Input(\"k: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::MatrixSetDiagV2Shape);\n\nREGISTER_OP(\"MatrixSetDiagV3\")\n    .Input(\"input: T\")\n    .Input(\"diagonal: T\")\n    .Input(\"k: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\n        \"align: {'LEFT_RIGHT', 'RIGHT_LEFT', 'LEFT_LEFT', 'RIGHT_RIGHT'} = \"\n        \"'RIGHT_LEFT'\")\n    .SetShapeFn(shape_inference::MatrixSetDiagV2Shape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MatrixDiagPart\")\n    .Input(\"input: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle in;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 2, &in));\n      if (!c->RankKnown(in)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n      const int32_t rank = c->Rank(in);\n      std::vector<DimensionHandle> dims;\n      dims.reserve(rank - 2);\n      for (int i = 0; i < rank - 2; ++i) dims.push_back(c->Dim(in, i));\n\n      DimensionHandle min_dim;\n      TF_RETURN_IF_ERROR(\n          c->Min(c->Dim(in, rank - 2), c->Dim(in, rank - 1), &min_dim));\n      dims.push_back(min_dim);\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MatrixDiagPartV2\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Input(\"padding_value: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::MatrixDiagPartV2Shape);\n\nREGISTER_OP(\"MatrixDiagPartV3\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Input(\"padding_value: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\"T: type\")\n    .Attr(\n        \"align: {'LEFT_RIGHT', 'RIGHT_LEFT', 'LEFT_LEFT', 'RIGHT_RIGHT'} = \"\n        \"'RIGHT_LEFT'\")\n    .SetShapeFn(shape_inference::MatrixDiagPartV2Shape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MatrixBandPart\")\n    .Input(\"input: T\")\n    .Input(\"num_lower: Tindex\")\n    .Input(\"num_upper: Tindex\")\n    .Output(\"band: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindex: {int32, int64} = DT_INT64\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Reverse\")\n    .Input(\"tensor: T\")\n    .Input(\"dims: bool\")\n    .Output(\"output: T\")\n    .Attr(\n        \"T: {uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, \"\n        \"bfloat16, half, float, double, complex64, complex128, string}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle dims;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &dims));\n      DimensionHandle dims_dim = c->Dim(dims, 0);\n      if (c->ValueKnown(dims_dim)) {\n        TF_RETURN_IF_ERROR(c->WithRank(input, c->Value(dims_dim), &input));\n      }\n      if (c->Rank(input) > 8) {\n        return errors::InvalidArgument(\n            \"reverse does not work on tensors with more than 8 dimensions\");\n      }\n      c->set_output(0, input);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ReverseV2\")\n    .Input(\"tensor: T\")\n    .Input(\"axis: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\n        \"T: {uint8, int8, uint16, int16, int32, uint32, int64, uint64, bool, \"\n        \"bfloat16, half, float, double, complex64, complex128, string}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle axis;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &axis));\n      if (c->Rank(input) > 8) {\n        return errors::InvalidArgument(\n            \"reverse does not work on tensors with more than 8 dimensions\");\n      }\n      const Tensor* axis_tensor = c->input_tensor(1);\n      if (axis_tensor != nullptr && c->RankKnown(input)) {\n        int32_t rank = c->Rank(input);\n        std::vector<int64_t> axis_value;\n        if (axis_tensor->dtype() == DT_INT32) {\n          axis_value = AsInt64<int32>(axis_tensor, axis_tensor->NumElements());\n        } else {\n          axis_value =\n              AsInt64<int64_t>(axis_tensor, axis_tensor->NumElements());\n        }\n        std::vector<bool> axes_dense(c->Rank(input), false);\n        for (int i = 0; i < axis_value.size(); i++) {\n          int64_t canonical_axis =\n              axis_value[i] < 0 ? rank + axis_value[i] : axis_value[i];\n          if (canonical_axis < 0 || canonical_axis >= rank) {\n            return errors::InvalidArgument(\"'axis'[\", i, \"] = \", axis_value[i],\n                                           \" is out of valid range [\", 0, \", \",\n                                           rank - 1);\n          }\n          if (axes_dense[canonical_axis]) {\n            return errors::InvalidArgument(\"axis \", canonical_axis,\n                                           \" specified more than once.\");\n          }\n          axes_dense[canonical_axis] = true;\n        }\n      }\n      c->set_output(0, input);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"EditDistance\")\n    .Input(\"hypothesis_indices: int64\")\n    .Input(\"hypothesis_values: T\")\n    .Input(\"hypothesis_shape: int64\")\n    .Input(\"truth_indices: int64\")\n    .Input(\"truth_values: T\")\n    .Input(\"truth_shape: int64\")\n    .Attr(\"normalize: bool = true\")\n    .Attr(\"T: type\")\n    .Output(\"output: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::ValidateSparseTensor(\n          c, c->input(0), c->input(1), c->input(2)));\n      TF_RETURN_IF_ERROR(shape_inference::ValidateSparseTensor(\n          c, c->input(3), c->input(4), c->input(5)));\n      const Tensor* hypothesis_shape_t = c->input_tensor(2);\n      const Tensor* truth_shape_t = c->input_tensor(5);\n      if (hypothesis_shape_t == nullptr || truth_shape_t == nullptr) {\n        // We need to know the runtime shape of the two tensors,\n        // or else the output shape is unknown.\n        return shape_inference::UnknownShape(c);\n      }\n\n      if (hypothesis_shape_t->NumElements() != truth_shape_t->NumElements()) {\n        return errors::InvalidArgument(\n            \"Num elements of hypothesis_shape does not match truth_shape: \",\n            hypothesis_shape_t->NumElements(), \" vs. \",\n            truth_shape_t->NumElements());\n      }\n\n      auto h_values = hypothesis_shape_t->flat<int64_t>();\n      auto t_values = truth_shape_t->flat<int64_t>();\n      std::vector<DimensionHandle> dims(hypothesis_shape_t->NumElements() - 1);\n      for (int i = 0; i < dims.size(); ++i) {\n        dims[i] = c->MakeDim(std::max(h_values(i), t_values(i)));\n      }\n\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Fill\")\n    .Input(\"dims: index_type\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"index_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      DataType index_type = DT_INT32;\n      Status s = c->GetAttr(\"index_type\", &index_type);\n      if (!s.ok() && s.code() != error::NOT_FOUND) {\n        return s;\n      }\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n\n      const Tensor* t = c->input_tensor(0);\n      if (t != nullptr) {\n        for (int i = 0; i < t->NumElements(); ++i) {\n          if ((index_type == DT_INT32 && t->vec<int32>()(i) < 0) ||\n              (index_type == DT_INT64 && t->vec<int64_t>()(i) < 0)) {\n            return errors::InvalidArgument(\"Fill dimensions must be >= 0\");\n          }\n        }\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n      c->set_output(0, out);\n\n      auto* shape_and_type = c->input_handle_shapes_and_types(1);\n      if (shape_and_type) {\n        c->set_output_handle_shapes_and_types(0, *shape_and_type);\n      }\n\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"_ParallelConcatStart\")\n    .Output(\"output: dtype\")\n    .Attr(\"shape: shape\")\n    .Attr(\"dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::ExplicitShape)\n    .Doc(R\"doc(\nCreates an empty Tensor with shape `shape` and type `dtype`.\n\nThe memory can optionally be initialized. This is usually useful in\nconjunction with inplace operations.\n\nshape: 1-D `Tensor` indicating the shape of the output.\ndtype: The element type of the returned tensor.\noutput: An empty Tensor of the specified type.\n)doc\");\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"_ParallelConcatUpdate\")\n    .Input(\"value: T\")\n    .Input(\"update: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"loc: int\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nUpdates input `value` at `loc` with `update`.\n\nIf you use this function you will almost certainly want to add\na control dependency as done in the implementation of parallel_stack to\navoid race conditions.\n\nvalue: A `Tensor` object that will be updated in-place.\nloc: A scalar indicating the index of the first dimension such that\n         value[loc, :] is updated.\nupdate: A `Tensor` of rank one less than `value` if `loc` is a scalar,\n        otherwise of rank equal to `value` that contains the new values\n        for `value`.\noutput: `value` that has been updated accordingly.\n)doc\");\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Gather\")\n    .Input(\"params: Tparams\")\n    .Input(\"indices: Tindices\")\n    .Attr(\"validate_indices: bool = true\")\n    .Output(\"output: Tparams\")\n    .Attr(\"Tparams: type\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n      ShapeHandle params_subshape;\n      TF_RETURN_IF_ERROR(c->Subshape(c->input(0), 1, &params_subshape));\n      ShapeHandle indices_shape = c->input(1);\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->Concatenate(indices_shape, params_subshape, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"GatherV2\")\n    .Input(\"params: Tparams\")\n    .Input(\"indices: Tindices\")\n    .Input(\"axis: Taxis\")\n    .Attr(\"batch_dims: int = 0\")\n    .Output(\"output: Tparams\")\n    .Attr(\"Tparams: type\")\n    .Attr(\"Tindices: {int16, int32,int64}\")\n    .Attr(\"Taxis: {int32,int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle params_shape;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &params_shape));\n\n      ShapeHandle indices_shape = c->input(1);\n      ShapeHandle unused_axis_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused_axis_shape));\n      const Tensor* axis_t = c->input_tensor(2);\n\n      // If axis is unknown, we can only infer that the result is params_rank +\n      // indices_rank - 1.\n      if (axis_t == nullptr) {\n        if (c->RankKnown(params_shape) && c->RankKnown(indices_shape)) {\n          int32_t batch_dims;\n          TF_RETURN_IF_ERROR(c->GetAttr(\"batch_dims\", &batch_dims));\n          c->set_output(0, c->UnknownShapeOfRank(c->Rank(params_shape) +\n                                                 c->Rank(indices_shape) - 1 -\n                                                 batch_dims));\n        } else {\n          c->set_output(0, c->UnknownShape());\n        }\n        return OkStatus();\n      }\n\n      // Note, axis can be negative.\n      int64_t axis = 0;\n      if (axis_t->dtype() == DT_INT32) {\n        axis = axis_t->scalar<int32>()();\n      } else {\n        axis = axis_t->scalar<int64_t>()();\n      }\n\n      // Check that params has rank of at least axis + 1.\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(\n          params_shape, axis < 0 ? -axis : axis + 1, &unused));\n\n      // Note, batch_dims can be negative.\n      int32_t batch_dims;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"batch_dims\", &batch_dims));\n      // -rank(indices) <= batch_dims <= rank(indices)\n      TF_RETURN_IF_ERROR(\n          c->WithRankAtLeast(indices_shape, std::abs(batch_dims), &unused));\n      if (batch_dims < 0) {\n        batch_dims += c->Rank(indices_shape);\n      }\n      // rank(params) > batch_dims\n      TF_RETURN_IF_ERROR(\n          c->WithRankAtLeast(params_shape, batch_dims + 1, &unused));\n\n      ShapeHandle params_outer_subshape;\n      TF_RETURN_IF_ERROR(\n          c->Subshape(params_shape, 0, axis, &params_outer_subshape));\n\n      ShapeHandle indices_inner_subshape;\n      TF_RETURN_IF_ERROR(\n          c->Subshape(indices_shape, batch_dims, &indices_inner_subshape));\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(\n          c->Concatenate(params_outer_subshape, indices_inner_subshape, &out));\n\n      // Slice from axis + 1 to the end of params_shape to collect the inner\n      // dimensions of the result. Special case -1 here since -1 + 1 wraps, and\n      // we slice from 0 to the end of shape. Subshape() handles all other\n      // out-of-bounds checking.\n      if (axis != -1) {\n        ShapeHandle params_inner_subshape;\n        TF_RETURN_IF_ERROR(\n            c->Subshape(params_shape, axis + 1, &params_inner_subshape));\n        TF_RETURN_IF_ERROR(c->Concatenate(out, params_inner_subshape, &out));\n      }\n\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"GatherNd\")\n    .Input(\"params: Tparams\")\n    .Input(\"indices: Tindices\")\n    .Output(\"output: Tparams\")\n    .Attr(\"Tparams: type\")\n    .Attr(\"Tindices: {int16, int32,int64}\")\n    .SetShapeFn(shape_inference::GatherNdShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Identity\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetForwardTypeFn(full_type::ReplicateInput())\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Snapshot\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklIdentity\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"Doc( Mkl implementation of IdentityOp\n)Doc\");\n#endif\n\nREGISTER_OP(\"IdentityN\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: list(type)\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      std::vector<ShapeHandle> input;\n      TF_RETURN_IF_ERROR(c->input(\"input\", &input));\n      TF_RETURN_IF_ERROR(c->set_output(\"output\", input));\n      // If any of the input shapes are not known, we should return error.\n      for (int i = 0; i < input.size(); i++) {\n        if (!input[i].Handle()) {\n          return errors::InvalidArgument(absl::StrCat(\n              \"Cannot infer output shape #\", i,\n              \" for IdentityN node because input shape #\", i, \" is unknown.\"));\n        }\n      }\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"RefIdentity\")\n    .Input(\"input: Ref(T)\")\n    .Output(\"output: Ref(T)\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .SetAllowsUninitializedInput();\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"DebugGradientIdentity\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .SetAllowsUninitializedInput();\n\nREGISTER_OP(\"DebugGradientRefIdentity\")\n    .Input(\"input: Ref(T)\")\n    .Output(\"output: Ref(T)\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .SetAllowsUninitializedInput();\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"StopGradient\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"PreventGradient\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"message: string = ''\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"CheckNumerics\")\n    .Input(\"tensor: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"message: string\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"CheckNumericsV2\")\n    .Input(\"tensor: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"message: string\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Reshape\")\n    .Input(\"tensor: T\")\n    .Input(\"shape: Tshape\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      return SetOutputShapeForReshape(c);\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklReshape\")\n    .Input(\"tensor: T\")\n    .Input(\"shape: Tshape\")\n    .Input(\"mkl_tensor: uint8\")\n    .Input(\"mkl_shape: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: type\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) { return SetOutputShapeForReshape(c); })\n    .Doc(R\"Doc( MKL implementation of ReshapeOp.\n)Doc\");\n#endif  // INTEL_MKL\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"InvertPermutation\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle x;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &x));\n      c->set_output(0, x);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Transpose\")\n    .Input(\"x: T\")\n    .Input(\"perm: Tperm\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\n    .SetShapeFn(TransposeShapeFn);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklTranspose\")\n    .Input(\"x: T\")\n    .Input(\"perm: Tperm\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\n    .SetShapeFn(TransposeShapeFn);\n#endif  // INTEL_MKL\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ConjugateTranspose\")\n    .Input(\"x: T\")\n    .Input(\"perm: Tperm\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\n    .SetShapeFn(TransposeShapeFn);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklConjugateTranspose\")\n    .Input(\"x: T\")\n    .Input(\"perm: Tperm\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\n    .SetShapeFn(TransposeShapeFn);\n#endif  // INTEL_MKL\n\n// --------------------------------------------------------------------------\nnamespace {\nStatus UniqueIdxShapeFn(InferenceContext* c) {\n  ShapeHandle input = c->input(0);\n  const Tensor* axis_t = c->input_tensor(1);\n  if (axis_t == nullptr || !c->RankKnown(input)) {\n    c->set_output(1, c->Vector(InferenceContext::kUnknownDim));\n    return OkStatus();\n  }\n\n  if (c->Rank(c->input(1)) != 1) {\n    return errors::InvalidArgument(\"axis expects a 1D vector.\");\n  }\n\n  int32_t n = axis_t->NumElements();\n  if (n == 0) {\n    if (c->Rank(input) != 1) {\n      return errors::InvalidArgument(\"x expects a 1D vector.\");\n    }\n    c->set_output(1, input);\n    return OkStatus();\n  } else if (n == 1) {\n    int64_t axis;\n    if (axis_t->dtype() == DT_INT32) {\n      axis = static_cast<int64_t>(axis_t->flat<int32>()(0));\n    } else {\n      axis = axis_t->flat<int64_t>()(0);\n    }\n\n    int64_t input_rank = c->Rank(input);\n    if (axis < -input_rank || axis >= input_rank) {\n      return errors::InvalidArgument(\"axis expects to be in the range [\",\n                                     -input_rank, \", \", input_rank, \")\");\n    }\n    if (axis < 0) {\n      axis += input_rank;\n    }\n    c->set_output(1, c->Vector(c->Dim(input, axis)));\n    return OkStatus();\n  }\n  return errors::InvalidArgument(\n      \"axis does not support input tensors larger than 1 elements.\");\n}\n}  // namespace\n\nREGISTER_OP(\"Unique\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Output(\"idx: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n      c->set_output(1, c->input(0));\n      // Assert that the input rank is 1.\n      ShapeHandle dummy;\n      return c->WithRank(c->input(0), 1, &dummy);\n    });\n\nREGISTER_OP(\"UniqueV2\")\n    .Input(\"x: T\")\n    .Input(\"axis: Taxis\")\n    .Output(\"y: T\")\n    .Output(\"idx: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"Taxis: {int32,int64} = DT_INT64\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->UnknownShapeOfRank(c->Rank(c->input(0))));\n      TF_RETURN_IF_ERROR(UniqueIdxShapeFn(c));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"UniqueWithCounts\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Output(\"idx: out_idx\")\n    .Output(\"count: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto uniq = c->Vector(InferenceContext::kUnknownDim);\n      c->set_output(0, uniq);\n      c->set_output(1, c->input(0));\n      c->set_output(2, uniq);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"UniqueWithCountsV2\")\n    .Input(\"x: T\")\n    .Input(\"axis: Taxis\")\n    .Output(\"y: T\")\n    .Output(\"idx: out_idx\")\n    .Output(\"count: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"Taxis: {int32,int64} = DT_INT64\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->UnknownShapeOfRank(c->Rank(c->input(0))));\n      TF_RETURN_IF_ERROR(UniqueIdxShapeFn(c));\n      c->set_output(2, c->Vector(InferenceContext::kUnknownDim));\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus ShapeShapeFn(InferenceContext* c) {\n  for (int i = 0; i < c->num_inputs(); ++i) {\n    DimensionHandle dim;\n    if (c->RankKnown(c->input(i))) {\n      dim = c->MakeDim(c->Rank(c->input(i)));\n    } else {\n      dim = c->UnknownDim();\n    }\n    c->set_output(i, c->Vector(dim));\n  }\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Shape\")\n    .Input(\"input: T\")\n    .Output(\"output: out_type\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn(ShapeShapeFn);\n\nREGISTER_OP(\"ShapeN\")\n    .Input(\"input: N * T\")\n    .Output(\"output: N * out_type\")\n    .Attr(\"N: int\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn(ShapeShapeFn);\n\nREGISTER_OP(\"EnsureShape\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"shape: shape\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      // Merges desired shape and statically known shape of input\n      PartialTensorShape desired_shape;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shape\", &desired_shape));\n\n      int rank = desired_shape.dims();\n      ShapeHandle input_shape_handle;\n      ShapeHandle desired_shape_handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), rank, &input_shape_handle));\n      TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(\n          desired_shape, &desired_shape_handle));\n\n      ShapeHandle merged_shape;\n      TF_RETURN_IF_ERROR(\n          c->Merge(desired_shape_handle, input_shape_handle, &merged_shape));\n      c->set_output(0, merged_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ReverseSequence\")\n    .Input(\"input: T\")\n    .Input(\"seq_lengths: Tlen\")\n    .Output(\"output: T\")\n    .Attr(\"seq_dim: int\")\n    .Attr(\"batch_dim: int = 0\")\n    .Attr(\"T: type\")\n    .Attr(\"Tlen: {int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle seq_lens_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &seq_lens_shape));\n\n      int64_t seq_dim;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"seq_dim\", &seq_dim));\n      int64_t batch_dim;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"batch_dim\", &batch_dim));\n\n      if (!c->RankKnown(input)) {\n        return shape_inference::UnknownShape(c);\n      }\n\n      // Validate batch_dim and seq_dim against input.\n      const int32_t input_rank = c->Rank(input);\n      if (batch_dim >= input_rank) {\n        return errors::InvalidArgument(\n            \"batch_dim must be < input rank: \", batch_dim, \" vs. \", input_rank);\n      }\n\n      if (seq_dim >= input_rank) {\n        return errors::InvalidArgument(\n            \"seq_dim must be < input rank: \", seq_dim, \" vs. \", input_rank);\n      }\n\n      // To prevent out of bound access when calling c->Dim(input, batch_dim),\n      // batch_dim range [-1 * input rank, input rank) is allowed. However,\n      // the op implementation has a stricter bound for batch_dim requiring >= 0\n      // value. Thus, perform strict check here.\n      if (batch_dim < 0) {\n        return errors::InvalidArgument(\"batch_dim must be >=0, got \",\n                                       batch_dim);\n      }\n\n      DimensionHandle batch_dim_dim = c->Dim(input, batch_dim);\n      TF_RETURN_IF_ERROR(\n          c->Merge(batch_dim_dim, c->Dim(seq_lens_shape, 0), &batch_dim_dim));\n\n      // Replace batch_dim of input with batch_size\n      ShapeHandle output_shape;\n      TF_RETURN_IF_ERROR(\n          c->ReplaceDim(input, batch_dim, batch_dim_dim, &output_shape));\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Rank\")\n    .Input(\"input: T\")\n    .Output(\"output: int32\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Size\")\n    .Input(\"input: T\")\n    .Output(\"output: out_type\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Slice\")\n    .Input(\"input: T\")\n    .Input(\"begin: Index\")\n    .Input(\"size: Index\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32,int64}\")\n    .SetShapeFn(shape_inference::SliceShape);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklSlice\")\n    .Input(\"input: T\")\n    .Input(\"begin: Index\")\n    .Input(\"size: Index\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_begin: uint8\")\n    .Input(\"mkl_size: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32,int64}\")\n    .SetShapeFn(shape_inference::SliceShape);\n#endif\n\nREGISTER_OP(\"StridedSlice\")\n    .Input(\"input: T\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int16, int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle begin_shape, end_shape, strides_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &begin_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &end_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &strides_shape));\n      TF_RETURN_IF_ERROR(c->Merge(begin_shape, end_shape, &begin_shape));\n      TF_RETURN_IF_ERROR(c->Merge(begin_shape, strides_shape, &begin_shape));\n      DimensionHandle sparse_dims_dim = c->Dim(begin_shape, 0);\n\n      const Tensor* strides_value = c->input_tensor(3);\n      // TODO(aselle,allenl): If we had a stride_mask it would be possible to do\n      // more shape inference here (e.g. for x[3, ::T]).\n      if (!c->RankKnown(input) || !c->ValueKnown(sparse_dims_dim) ||\n          strides_value == nullptr) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n\n      PartialTensorShape input_shape({});\n      for (int i = 0; i < c->Rank(input); ++i) {\n        auto dim = c->Dim(input, i);\n        input_shape.AddDim(c->ValueKnown(dim) ? c->Value(dim) : -1);\n      }\n\n      int32_t begin_mask, end_mask, ellipsis_mask, new_axis_mask,\n          shrink_axis_mask;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"begin_mask\", &begin_mask));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"end_mask\", &end_mask));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"ellipsis_mask\", &ellipsis_mask));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"new_axis_mask\", &new_axis_mask));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shrink_axis_mask\", &shrink_axis_mask));\n\n      const Tensor* begin_value = c->input_tensor(1);\n      const Tensor* end_value = c->input_tensor(2);\n\n      PartialTensorShape processing_shape, final_shape;\n      bool is_identity, is_simple_slice, slice_dim0;\n      gtl::InlinedVector<int64, 4> begin, end, strides;\n      TF_RETURN_IF_ERROR(ValidateStridedSliceOp(\n          begin_value, end_value, *strides_value, input_shape, begin_mask,\n          end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask,\n          &processing_shape, &final_shape, &is_identity, &is_simple_slice,\n          &slice_dim0, &begin, &end, &strides));\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(final_shape, &out));\n      c->set_output(0, out);\n\n      auto* shape_and_type = c->input_handle_shapes_and_types(0);\n      if (shape_and_type) {\n        c->set_output_handle_shapes_and_types(0, *shape_and_type);\n      }\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"StridedSliceGrad\")\n    .Input(\"shape: Index\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Input(\"dy: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"StridedSliceAssign\")\n    .Input(\"ref: Ref(T)\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Input(\"value: T\")\n    .Output(\"output_ref: Ref(T)\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n// TODO(aselle): Fix this documentation once StridedSliceAssign Supports\n// broadcasting.\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"ResourceStridedSliceAssign\")\n    .Input(\"ref: resource\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Input(\"value: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn(shape_inference::NoOutputs);\n\nREGISTER_OP(\"TensorStridedSliceUpdate\")\n    .Input(\"input: T\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Tile\")\n    .Input(\"input: T\")\n    .Input(\"multiples: Tmultiples\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tmultiples: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      // NOTE(mrry): Represent `multiples` as a `TensorShape` because (i)\n      // it is a vector of non-negative integers, and (ii) doing so allows\n      // us to handle partially-known multiples.\n      ShapeHandle multiples;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &multiples));\n      if (c->RankKnown(input)) {\n        TF_RETURN_IF_ERROR(c->WithRank(multiples, c->Rank(input), &multiples));\n        ShapeHandle dummy;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->input(1), c->Vector(c->Rank(input)), &dummy));\n      }\n\n      if (!c->RankKnown(multiples)) {\n        return shape_inference::UnknownShape(c);\n      }\n\n      int32_t rank = c->Rank(multiples);\n      TF_RETURN_IF_ERROR(c->WithRank(input, rank, &input));\n      std::vector<DimensionHandle> dims(rank);\n      for (int i = 0; i < rank; ++i) {\n        TF_RETURN_IF_ERROR(\n            c->Multiply(c->Dim(input, i), c->Dim(multiples, i), &dims[i]));\n      }\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"TileGrad\")\n    .Input(\"input: T\")\n    .Input(\"multiples: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Deprecated(3, \"TileGrad has been replaced with reduce_sum\")\n    .SetShapeFn(tensorflow::shape_inference::UnknownShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Where\")\n    .Input(\"input: T\")\n    .Attr(\"T: {numbertype, bool} = DT_BOOL\")\n    .Output(\"index: int64\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Matrix(c->UnknownDim(), c->Rank(c->input(0))));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"BroadcastArgs\")\n    .Input(\"s0: T\")\n    .Input(\"s1: T\")\n    .Output(\"r0: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      ShapeHandle shape_x = c->input(0);\n      ShapeHandle shape_y = c->input(1);\n      TF_RETURN_IF_ERROR(c->WithRank(shape_x, 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(shape_y, 1, &unused));\n\n      if (!c->ValueKnown(c->Dim(shape_x, 0)) ||\n          !c->ValueKnown(c->Dim(shape_y, 0))) {\n        c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n        return OkStatus();\n      }\n\n      int64_t x_dim = c->Value(c->Dim(shape_x, 0));\n      int64_t y_dim = c->Value(c->Dim(shape_y, 0));\n\n      // Broadcasted shape is going to be as large as the largest dimension.\n      c->set_output(0, c->Vector(std::max(x_dim, y_dim)));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"BroadcastGradientArgs\")\n    .Input(\"s0: T\")\n    .Input(\"s1: T\")\n    .Output(\"r0: T\")\n    .Output(\"r1: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      // TODO(mrry): Implement constant_value for BroadcastGradientArgs?\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));\n      c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n      c->set_output(1, c->Vector(InferenceContext::kUnknownDim));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Pad\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(PadShapeFn);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"PadV2\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Input(\"constant_values: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(PadShapeFn);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MirrorPad\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .Attr(GetMirrorPadModeAttrString())\n    .SetShapeFn(PadShapeFn);\n\n// --------------------------------------------------------------------------\nnamespace {\ntemplate <typename T>\nStatus MirrorPadKnown(InferenceContext* c, ShapeHandle input,\n                      const Tensor* paddings_t, int64_t input_rank) {\n  auto paddings_data = paddings_t->matrix<T>();\n  std::vector<DimensionHandle> dims(input_rank);\n  for (int64_t i = 0; i < input_rank; ++i) {\n    const int64_t pad0 = static_cast<int64_t>(paddings_data(i, 0));\n    const int64_t pad1 = static_cast<int64_t>(paddings_data(i, 1));\n    if (pad0 < 0 || pad1 < 0) {\n      return errors::InvalidArgument(\"Paddings must be non-negative\");\n    }\n\n    TF_RETURN_IF_ERROR(c->Subtract(c->Dim(input, i), pad0 + pad1, &dims[i]));\n  }\n  c->set_output(0, c->MakeShape(dims));\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"MirrorPadGrad\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .Attr(GetMirrorPadModeAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle paddings;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 2, &paddings));\n      DimensionHandle pad_0 = c->Dim(paddings, 0);\n      if (!c->ValueKnown(pad_0)) {\n        // We don't know the rank of the output since the first\n        // padding dimension is unknown.\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n\n      int64_t input_rank = c->Value(pad_0);\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), input_rank, &input));\n      TF_RETURN_IF_ERROR(\n          c->Merge(paddings, c->Matrix(input_rank, 2), &paddings));\n\n      const Tensor* paddings_t = c->input_tensor(1);\n      if (paddings_t == nullptr) {\n        // Values of 'paddings' is not available, but we know the\n        // input rank, so return the rank of the output with unknown\n        // dimensions.\n        c->set_output(0, c->UnknownShapeOfRank(input_rank));\n        return OkStatus();\n      }\n\n      if (paddings_t->dtype() == DT_INT32) {\n        return MirrorPadKnown<int32>(c, input, paddings_t, input_rank);\n      } else {\n        return MirrorPadKnown<int64_t>(c, input, paddings_t, input_rank);\n      }\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Placeholder\")\n    .Output(\"output: dtype\")\n    .Attr(\"dtype: type\")\n    .Attr(\"shape: shape = { unknown_rank: true }\")\n    .SetShapeFn([](InferenceContext* c) {\n      PartialTensorShape shape;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shape\", &shape));\n\n      // Placeholder has legacy behavior where we cannot tell the difference\n      // between a scalar shape attribute and 'unknown shape'.  So if the shape\n      // is a scalar, we return an unknown shape.\n      if (c->graph_def_version() <= 21 && shape.dims() <= 0) {\n        return shape_inference::UnknownShape(c);\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(shape, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// Placeholder was modified in a backwards compatible way to do what\n// PlaceholderV2 did, so we have deprecated V2 (no one was really\n// using it).\nREGISTER_OP(\"PlaceholderV2\")\n    .Output(\"output: dtype\")\n    .Attr(\"dtype: type\")\n    .Attr(\"shape: shape\")\n    .SetShapeFn(shape_inference::ExplicitShape)\n    .Deprecated(23, \"Placeholder now behaves the same as PlaceholderV2.\");\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"PlaceholderWithDefault\")\n    .Input(\"input: dtype\")\n    .Output(\"output: dtype\")\n    .Attr(\"dtype: type\")\n    .Attr(\"shape: shape\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      PartialTensorShape shape;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shape\", &shape));\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(shape, &out));\n\n      // We merge for compatibility checking, but return the output,\n      // since output_shape may be less precise than input_shape.\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->Merge(input, out, &unused));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ExpandDims\")\n    .Input(\"input: T\")\n    .Input(\"dim: Tdim\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tdim: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n\n      const Tensor* dim_t = c->input_tensor(1);\n      if (dim_t != nullptr && dim_t->NumElements() != 1) {\n        return errors::InvalidArgument(\n            \"'dim' input must be a tensor with a single value\");\n      }\n      if (dim_t == nullptr || !c->RankKnown(input)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n\n      int64_t dim;\n      if (dim_t->dtype() == DT_INT32) {\n        dim = static_cast<int64_t>(dim_t->flat<int32>()(0));\n      } else {\n        dim = dim_t->flat<int64_t>()(0);\n      }\n\n      const int32_t rank = c->Rank(input);\n      const int32_t min_dim = -1 * rank - 1;\n      if (dim < min_dim || dim > rank) {\n        return errors::InvalidArgument(\"dim \", dim, \" not in the interval [\",\n                                       min_dim, \", \", rank, \"].\");\n      }\n\n      if (dim < 0) {\n        dim += rank + 1;\n      }\n\n      ShapeHandle end;\n      TF_RETURN_IF_ERROR(c->Subshape(input, dim, &end));\n\n      // Build output as start + 1 + end.\n      ShapeHandle output;\n      TF_RETURN_IF_ERROR(c->Subshape(input, 0, dim, &output));\n      TF_RETURN_IF_ERROR(c->Concatenate(output, c->Vector(1), &output));\n      TF_RETURN_IF_ERROR(c->Concatenate(output, end, &output));\n      c->set_output(0, output);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Squeeze\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"squeeze_dims: list(int) >= 0 = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      if (!c->RankKnown(input)) {\n        // Input shape unknown.\n        return shape_inference::UnknownShape(c);\n      }\n\n      const int32_t input_rank = c->Rank(input);\n\n      // Validate and wrap squeeze dimensions.\n      std::vector<int32> squeeze_dims;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"squeeze_dims\", &squeeze_dims));\n      for (int i = 0; i < squeeze_dims.size(); ++i) {\n        if (squeeze_dims[i] < -input_rank || squeeze_dims[i] >= input_rank) {\n          return errors::InvalidArgument(\"squeeze_dims[\", i, \"] not in [\",\n                                         -input_rank, \",\", input_rank, \").\");\n        }\n\n        if (squeeze_dims[i] < 0) {\n          squeeze_dims[i] += input_rank;\n        }\n      }\n\n      std::vector<DimensionHandle> result_shape;\n      for (int i = 0; i < input_rank; ++i) {\n        // True if squeeze_dims contains an entry to squeeze this\n        // dimension.\n        bool is_explicit_match =\n            std::find(squeeze_dims.begin(), squeeze_dims.end(), i) !=\n            squeeze_dims.end();\n\n        DimensionHandle dim = c->Dim(input, i);\n\n        if (!c->ValueKnown(dim)) {\n          // Assume that the squeezed dimension will be 1 at runtime.\n          if (is_explicit_match) continue;\n\n          // If squeezing all 1 dimensions, and we see an unknown value,\n          // give up and return Unknown Shape.\n          if (squeeze_dims.empty()) {\n            c->set_output(0, c->UnknownShape());\n            return OkStatus();\n          }\n        } else if (c->Value(dim) == 1) {\n          if (is_explicit_match || squeeze_dims.empty()) {\n            // If explicitly squeezing, or squeezing all 1s, remove\n            // this dimension.\n            continue;\n          }\n        } else if (is_explicit_match) {\n          return errors::InvalidArgument(\"Can not squeeze dim[\", i,\n                                         \"], expected a dimension of 1, got \",\n                                         c->Value(c->Dim(input, i)));\n        }\n\n        result_shape.emplace_back(dim);\n      }\n\n      c->set_output(0, c->MakeShape(result_shape));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ListDiff\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"out: T\")\n    .Output(\"idx: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));\n      // TODO(mrry): Indicate that the length falls within an interval?\n      ShapeHandle out = c->Vector(InferenceContext::kUnknownDim);\n      c->set_output(0, out);\n      c->set_output(1, out);\n      return OkStatus();\n    });\n\nnamespace {\n\n// Converts Tensor to flat std::vector<int64_t>.\ntemplate <typename InputType>\nstd::vector<int64_t> GetFlatInt64(const Tensor& t) {\n  std::vector<int64_t> output(t.shape().num_elements());\n  if (t.shape().num_elements() > 0) {\n    auto eigen_vec = t.flat<InputType>();\n    std::copy_n(&eigen_vec(0), output.size(), output.begin());\n  }\n  return output;\n}\n\n// Converts int32 or int64 Tensor to flat std::vector<int64_t>.\nstd::vector<int64_t> GetFlatInt64(const Tensor& t) {\n  if (t.dtype() == DT_INT32) {\n    return GetFlatInt64<int32>(t);\n  } else {\n    return GetFlatInt64<int64_t>(t);\n  }\n}\n\nStatus SpaceToBatchShapeHelper(InferenceContext* c, ShapeHandle input_shape,\n                               ShapeHandle block_shape_shape,\n                               const Tensor* block_shape_t,\n                               ShapeHandle paddings_shape,\n                               const Tensor* paddings_t) {\n  if (c->Rank(block_shape_shape) != 1) {\n    return errors::InvalidArgument(\"block_shape must have rank 1.\");\n  }\n\n  const DimensionHandle num_block_dims_handle = c->Dim(block_shape_shape, 0);\n  if (!c->ValueKnown(num_block_dims_handle)) {\n    return errors::InvalidArgument(\"block_shape must have known size.\");\n  }\n\n  const int64_t num_block_dims = c->Value(num_block_dims_handle);\n\n  TF_RETURN_IF_ERROR(\n      c->WithRankAtLeast(input_shape, num_block_dims + 1, &input_shape));\n\n  TF_RETURN_IF_ERROR(\n      c->Merge(paddings_shape, c->Matrix(num_block_dims, 2), &paddings_shape));\n\n  DimensionHandle batch_size = c->Dim(input_shape, 0);\n  std::vector<int64_t> block_shape_vec;\n  if (block_shape_t && (block_shape_t->NumElements() > 0)) {\n    block_shape_vec = GetFlatInt64(*block_shape_t);\n    for (int64_t dim = 0; dim < num_block_dims; ++dim) {\n      const int64_t block_shape_value = block_shape_vec[dim];\n      if (block_shape_value < 1) {\n        return errors::InvalidArgument(\"block_shape must be positive\");\n      }\n      if (c->ValueKnown(batch_size)) {\n        TF_RETURN_IF_ERROR(\n            c->Multiply(batch_size, block_shape_value, &batch_size));\n      } else {\n        batch_size = c->UnknownDim();\n      }\n    }\n  } else if (num_block_dims > 0) {\n    batch_size = c->UnknownDim();\n  }\n\n  std::vector<DimensionHandle> output_dims{batch_size};\n  output_dims.resize(num_block_dims + 1, c->UnknownDim());\n\n  if (paddings_t && (paddings_t->NumElements() > 0)) {\n    const std::vector<int64_t> paddings_vec = GetFlatInt64(*paddings_t);\n    for (int64_t dim = 0; dim < num_block_dims; ++dim) {\n      const int64_t pad_start = paddings_vec[dim * 2],\n                    pad_end = paddings_vec[dim * 2 + 1];\n      if (pad_start < 0 || pad_end < 0) {\n        return errors::InvalidArgument(\"paddings cannot be negative\");\n      }\n      if (block_shape_t) {\n        DimensionHandle padded_size;\n        TF_RETURN_IF_ERROR(\n            c->Add(c->Dim(input_shape, dim + 1), pad_start, &padded_size));\n        TF_RETURN_IF_ERROR(c->Add(padded_size, pad_end, &padded_size));\n        TF_RETURN_IF_ERROR(c->Divide(padded_size, block_shape_vec[dim],\n                                     /*evenly_divisible=*/true,\n                                     &output_dims[dim + 1]));\n      }\n    }\n  }\n\n  ShapeHandle remaining_input_shape;\n  TF_RETURN_IF_ERROR(\n      c->Subshape(input_shape, 1 + num_block_dims, &remaining_input_shape));\n\n  ShapeHandle result;\n  TF_RETURN_IF_ERROR(c->Concatenate(c->MakeShape(output_dims),\n                                    remaining_input_shape, &result));\n  c->set_output(0, result);\n  return OkStatus();\n}\n\nStatus BatchToSpaceShapeHelper(InferenceContext* c, ShapeHandle input_shape,\n                               ShapeHandle block_shape_shape,\n                               const Tensor* block_shape_t,\n                               ShapeHandle crops_shape, const Tensor* crops_t) {\n  if (c->Rank(block_shape_shape) != 1) {\n    return errors::InvalidArgument(\"block_shape must have rank 1.\");\n  }\n\n  const DimensionHandle num_block_dims_handle = c->Dim(block_shape_shape, 0);\n  if (!c->ValueKnown(num_block_dims_handle)) {\n    return errors::InvalidArgument(\"block_shape must have known size.\");\n  }\n\n  const int64_t num_block_dims = c->Value(num_block_dims_handle);\n\n  TF_RETURN_IF_ERROR(\n      c->WithRankAtLeast(input_shape, num_block_dims + 1, &input_shape));\n\n  TF_RETURN_IF_ERROR(\n      c->Merge(crops_shape, c->Matrix(num_block_dims, 2), &crops_shape));\n\n  DimensionHandle batch_size = c->Dim(input_shape, 0);\n  std::vector<int64_t> block_shape_vec;\n  if (block_shape_t) {\n    block_shape_vec = GetFlatInt64(*block_shape_t);\n    for (int64_t dim = 0; dim < num_block_dims; ++dim) {\n      const int64_t block_shape_value = block_shape_vec[dim];\n      if (block_shape_value < 1) {\n        return errors::InvalidArgument(\"block_shape must be positive\");\n      }\n      if (c->ValueKnown(batch_size)) {\n        TF_RETURN_IF_ERROR(c->Divide(batch_size, block_shape_value,\n                                     /*evenly_divisible=*/true, &batch_size));\n      } else {\n        batch_size = c->UnknownDim();\n      }\n    }\n  } else if (num_block_dims > 0) {\n    batch_size = c->UnknownDim();\n  }\n\n  std::vector<DimensionHandle> output_dims{batch_size};\n  output_dims.resize(num_block_dims + 1, c->UnknownDim());\n\n  if (crops_t) {\n    const std::vector<int64_t> crops_vec = GetFlatInt64(*crops_t);\n    for (int64_t dim = 0; dim < num_block_dims; ++dim) {\n      const int64_t crop_start = crops_vec[dim * 2],\n                    crop_end = crops_vec[dim * 2 + 1];\n      if (crop_start < 0 || crop_end < 0) {\n        return errors::InvalidArgument(\"crops cannot be negative\");\n      }\n      if (block_shape_t) {\n        DimensionHandle cropped_size;\n        TF_RETURN_IF_ERROR(c->Multiply(c->Dim(input_shape, dim + 1),\n                                       block_shape_vec[dim], &cropped_size));\n        TF_RETURN_IF_ERROR(\n            c->Subtract(cropped_size, crop_start, &cropped_size));\n        TF_RETURN_IF_ERROR(\n            c->Subtract(cropped_size, crop_end, &output_dims[dim + 1]));\n      }\n    }\n  }\n\n  ShapeHandle remaining_input_shape;\n  TF_RETURN_IF_ERROR(\n      c->Subshape(input_shape, 1 + num_block_dims, &remaining_input_shape));\n\n  ShapeHandle result;\n  TF_RETURN_IF_ERROR(c->Concatenate(c->MakeShape(output_dims),\n                                    remaining_input_shape, &result));\n  c->set_output(0, result);\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"SpaceToBatchND\")\n    .Input(\"input: T\")\n    .Input(\"block_shape: Tblock_shape\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tblock_shape: {int32, int64} = DT_INT32\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      return SpaceToBatchShapeHelper(c, c->input(0), c->input(1),\n                                     c->input_tensor(1), c->input(2),\n                                     c->input_tensor(2));\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"SpaceToBatch\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .Attr(\"block_size: int >= 2\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n\n      int32_t block_size;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"block_size\", &block_size));\n\n      Tensor block_shape(tensorflow::DT_INT64, TensorShape({2}));\n      auto block_shape_vec = block_shape.vec<int64_t>();\n      block_shape_vec(0) = block_size;\n      block_shape_vec(1) = block_size;\n\n      return SpaceToBatchShapeHelper(c, input_shape, c->MakeShape({2}),\n                                     &block_shape, c->input(1),\n                                     c->input_tensor(1));\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"BatchToSpaceND\")\n    .Input(\"input: T\")\n    .Input(\"block_shape: Tblock_shape\")\n    .Input(\"crops: Tcrops\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tblock_shape: {int32, int64} = DT_INT32\")\n    .Attr(\"Tcrops: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      return BatchToSpaceShapeHelper(c, c->input(0), c->input(1),\n                                     c->input_tensor(1), c->input(2),\n                                     c->input_tensor(2));\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"BatchToSpace\")\n    .Input(\"input: T\")\n    .Input(\"crops: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"block_size: int >= 2\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n\n      int32_t block_size;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"block_size\", &block_size));\n\n      Tensor block_shape(tensorflow::DT_INT64, TensorShape({2}));\n      auto block_shape_vec = block_shape.vec<int64_t>();\n      block_shape_vec(0) = block_size;\n      block_shape_vec(1) = block_size;\n\n      return BatchToSpaceShapeHelper(c, input_shape, c->MakeShape({2}),\n                                     &block_shape, c->input(1),\n                                     c->input_tensor(1));\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"SpaceToDepth\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"block_size: int >= 2\")\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    // TODO(pauldonnelly): Implement GPU kernels for NCHW_VECT_C.\n    .SetShapeFn([](InferenceContext* c) {\n      string data_format_str;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n      TensorFormat data_format;\n      FormatFromString(data_format_str, &data_format);\n\n      constexpr int num_spatial_dims = 2;\n      const int dims =\n          GetTensorDimsFromSpatialDims(num_spatial_dims, data_format);\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), dims, &input));\n\n      int32_t block_size;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"block_size\", &block_size));\n\n      DimensionHandle batch_size =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'N'));\n      DimensionHandle input_height =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'H'));\n      DimensionHandle input_width =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'W'));\n      DimensionHandle input_depth =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'C'));\n\n      DimensionHandle output_height;\n      DimensionHandle output_width;\n      DimensionHandle output_depth;\n      // Will return an error if input height or width are not evenly divisible.\n      TF_RETURN_IF_ERROR(c->Divide(input_height, block_size,\n                                   true /* evenly_divisible */,\n                                   &output_height));\n      TF_RETURN_IF_ERROR(c->Divide(input_width, block_size,\n                                   true /* evenly_divisible */, &output_width));\n\n      TF_RETURN_IF_ERROR(\n          c->Multiply(input_depth, block_size * block_size, &output_depth));\n\n      ShapeHandle output_shape;\n      TF_RETURN_IF_ERROR(MakeShapeFromFormat(data_format, batch_size,\n                                             {output_height, output_width},\n                                             output_depth, &output_shape, c));\n\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"DepthToSpace\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"block_size: int >= 2\")\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    // TODO(pauldonnelly): Implement GPU kernels for NCHW and NCHW_VECT_C.\n    .SetShapeFn([](InferenceContext* c) {\n      string data_format_str;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n      TensorFormat data_format;\n      FormatFromString(data_format_str, &data_format);\n\n      constexpr int num_spatial_dims = 2;\n      const int dims =\n          GetTensorDimsFromSpatialDims(num_spatial_dims, data_format);\n\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), dims, &input));\n\n      int32_t block_size;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"block_size\", &block_size));\n\n      DimensionHandle batch_size =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'N'));\n      DimensionHandle input_height =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'H'));\n      DimensionHandle input_width =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'W'));\n      DimensionHandle input_depth =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'C'));\n\n      DimensionHandle output_height;\n      DimensionHandle output_width;\n      DimensionHandle output_depth;\n      TF_RETURN_IF_ERROR(c->Multiply(input_height, block_size, &output_height));\n      TF_RETURN_IF_ERROR(c->Multiply(input_width, block_size, &output_width));\n\n      // Will return an error if input_depth is not evenly divisible.\n      TF_RETURN_IF_ERROR(c->Divide(input_depth, block_size * block_size,\n                                   true /* evenly_divisible */, &output_depth));\n\n      ShapeHandle output_shape;\n      TF_RETURN_IF_ERROR(MakeShapeFromFormat(data_format, batch_size,\n                                             {output_height, output_width},\n                                             output_depth, &output_shape, c));\n\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"ExtractImagePatches\")\n    .Input(\"images: T\")\n    .Output(\"patches: T\")\n    .Attr(\"ksizes: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, int16, int32, int64, \"\n        \"uint8, uint16, uint32, uint64, complex64, complex128, bool}\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n\n      std::vector<int32> ksizes;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"ksizes\", &ksizes));\n      if (ksizes.size() != 4) {\n        return errors::InvalidArgument(\n            \"ExtractImagePatches requires the ksizes attribute to contain 4 \"\n            \"values, but got: \",\n            ksizes.size());\n      }\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 4) {\n        return errors::InvalidArgument(\n            \"ExtractImagePatches requires the stride attribute to contain 4 \"\n            \"values, but got: \",\n            strides.size());\n      }\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 4) {\n        return errors::InvalidArgument(\n            \"ExtractImagePatches requires the rates attribute to contain 4 \"\n            \"values, but got: \",\n            rates.size());\n      }\n\n      int32_t ksize_rows = ksizes[1];\n      int32_t ksize_cols = ksizes[2];\n\n      int32_t stride_rows = strides[1];\n      int32_t stride_cols = strides[2];\n\n      int32_t rate_rows = rates[1];\n      int32_t rate_cols = rates[2];\n\n      int32_t ksize_rows_eff = ksize_rows + (ksize_rows - 1) * (rate_rows - 1);\n      int32_t ksize_cols_eff = ksize_cols + (ksize_cols - 1) * (rate_cols - 1);\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 2);\n      DimensionHandle output_depth_dim;\n      TF_RETURN_IF_ERROR(c->Multiply(\n          c->Dim(input_shape, 3), ksize_rows * ksize_cols, &output_depth_dim));\n\n      if (!c->ValueKnown(in_rows_dim) || !c->ValueKnown(in_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, ksize_rows_eff, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, ksize_cols_eff, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n      ShapeHandle output_shape = c->MakeShape(\n          {batch_size_dim, output_rows, output_cols, output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\n// To enable rates, uncomment all lines commented below and use ksize_*_eff\n// as the second parameter of all GetWindowedOutputSizeVerbose calls instead\n// of ksize_*.\nREGISTER_OP(\"ExtractVolumePatches\")\n    .Input(\"input: T\")\n    .Output(\"patches: T\")\n    .Attr(\"ksizes: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    /* .Attr(\"rates: list(int) >= 5\") */\n    .Attr(\"T: realnumbertype\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 5, &input_shape));\n\n      std::vector<int32> ksizes;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"ksizes\", &ksizes));\n      if (ksizes.size() != 5) {\n        return errors::InvalidArgument(\n            \"ExtractVolumePatches requires the ksizes attribute to contain 5 \"\n            \"values, but got: \",\n            ksizes.size());\n      }\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 5) {\n        return errors::InvalidArgument(\n            \"ExtractVolumePatches requires the stride attribute to contain 5 \"\n            \"values, but got: \",\n            strides.size());\n      }\n\n      /*\n      // TODO(hsgkim): Enable rates.\n      // See extract_volume_patches_op.cc for why rates are disabled now.\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 5) {\n        return errors::InvalidArgument(\n            \"ExtractVolumePatches requires the rates attribute to contain 5 \"\n            \"values, but got: \",\n            rates.size());\n      }\n      */\n\n      int32_t ksize_planes = ksizes[1];\n      int32_t ksize_rows = ksizes[2];\n      int32_t ksize_cols = ksizes[3];\n\n      int32_t stride_planes = strides[1];\n      int32_t stride_rows = strides[2];\n      int32_t stride_cols = strides[3];\n\n      /*\n      int32 rate_planes = rates[1];\n      int32 rate_rows = rates[2];\n      int32 rate_cols = rates[3];\n\n      int32 ksize_planes_eff = ksize_planes +\n                               (ksize_planes - 1) * (rate_planes - 1);\n      int32 ksize_rows_eff = ksize_rows + (ksize_rows - 1) * (rate_rows - 1);\n      int32 ksize_cols_eff = ksize_cols + (ksize_cols - 1) * (rate_cols - 1);\n      */\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_planes_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 2);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 3);\n      DimensionHandle output_depth_dim;\n      TF_RETURN_IF_ERROR(c->Multiply(c->Dim(input_shape, 4),\n                                     ksize_planes * ksize_rows * ksize_cols,\n                                     &output_depth_dim));\n\n      if (!c->ValueKnown(in_planes_dim) || !c->ValueKnown(in_rows_dim) ||\n          !c->ValueKnown(in_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      auto in_planes = c->Value(in_planes_dim);\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_planes, output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_planes, ksize_planes, stride_planes, padding, &output_planes,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, ksize_rows, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, ksize_cols, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n      ShapeHandle output_shape =\n          c->MakeShape({batch_size_dim, output_planes, output_rows, output_cols,\n                        output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"OneHot\")\n    .Input(\"indices: TI\")\n    .Input(\"depth: int32\")\n    .Input(\"on_value: T\")\n    .Input(\"off_value: T\")\n    .Attr(\"axis: int = -1\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"TI: {uint8, int8, int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      int32_t axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      if (axis < -1) return errors::InvalidArgument(\"axis must be >= -1\");\n\n      DimensionHandle depth;\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &depth));\n\n      ShapeHandle indices = c->input(0);\n      if (!c->RankKnown(indices)) return shape_inference::UnknownShape(c);\n\n      int32_t new_rank = c->Rank(indices) + 1;\n      // We need to add new_rank to axis in the case the axis is -1 because\n      // C++ returns negative values from % if the dividend is negative.\n      int32_t depth_index = (axis + new_rank) % new_rank;\n      // Out shape is indices[0:depth_index] + [depth] + indices[depth_index:].\n      ShapeHandle front;\n      ShapeHandle back;\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->Subshape(indices, 0, depth_index, &front));\n      TF_RETURN_IF_ERROR(c->Subshape(indices, depth_index, &back));\n      TF_RETURN_IF_ERROR(c->Concatenate(front, c->Vector(depth), &front));\n      TF_RETURN_IF_ERROR(c->Concatenate(front, back, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// EXPERIMENTAL. DO NOT USE OR DEPEND ON THIS YET.\nREGISTER_OP(\"QuantizeAndDequantize\")\n    .Input(\"input: T\")\n    .Attr(\"signed_input: bool = true\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"range_given: bool = false\")\n    .Attr(\"input_min: float = 0\")\n    .Attr(\"input_max: float = 0\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Deprecated(22, \"Replaced by QuantizeAndDequantizeV2\");\n\n// TODO(suharshs): Deprecate QuantizeAndDequantizeV2.\nREGISTER_OP(\"QuantizeAndDequantizeV2\")\n    .Input(\"input: T\")\n    .Input(\"input_min: T\")\n    .Input(\"input_max: T\")\n    .Attr(\"signed_input: bool = true\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"range_given: bool = false\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\n        \"round_mode: {'HALF_TO_EVEN', 'HALF_UP'} = \"\n        \"'HALF_TO_EVEN'\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      } else if (axis != -1) {\n        ShapeHandle input;\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      c->set_output(0, c->input(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizeAndDequantizeV4\")\n    .Input(\"input: T\")\n    .Input(\"input_min: T\")\n    .Input(\"input_max: T\")\n    .Attr(\"signed_input: bool = true\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"range_given: bool = false\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\n        \"round_mode: {'HALF_TO_EVEN', 'HALF_UP'} = \"\n        \"'HALF_TO_EVEN'\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      } else if (axis != -1) {\n        ShapeHandle input;\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      c->set_output(0, c->input(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizeAndDequantizeV4Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"input: T\")\n    .Input(\"input_min: T\")\n    .Input(\"input_max: T\")\n    .Output(\"input_backprop: T\")\n    .Output(\"input_min_backprop: T\")\n    .Output(\"input_max_backprop: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"axis: int = -1\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(3), minmax, &minmax));\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      } else if (axis != -1) {\n        ShapeHandle input;\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      ShapeHandle inputs;\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &inputs));\n      c->set_output(0, inputs);\n      c->set_output(1, minmax);\n      c->set_output(2, minmax);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizeAndDequantizeV3\")\n    .Input(\"input: T\")\n    .Input(\"input_min: T\")\n    .Input(\"input_max: T\")\n    .Input(\"num_bits: int32\")\n    .Attr(\"signed_input: bool = true\")\n    .Attr(\"range_given: bool = true\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      } else if (axis != -1) {\n        ShapeHandle input;\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(0, c->input(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizeV2\")\n    .Input(\"input: float\")\n    .Input(\"min_range: float\")\n    .Input(\"max_range: float\")\n    .Output(\"output: T\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"mode: {'MIN_COMBINED', 'MIN_FIRST', 'SCALED'} = 'MIN_COMBINED'\")\n    .Attr(\n        \"round_mode: {'HALF_AWAY_FROM_ZERO', 'HALF_TO_EVEN'} = \"\n        \"'HALF_AWAY_FROM_ZERO'\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .Attr(\"ensure_minimum_range: float = 0.01\")\n    .SetShapeFn(shape_inference::QuantizeV2Shape);\n\nREGISTER_OP(\"Dequantize\")\n    .Input(\"input: T\")\n    .Input(\"min_range: float\")\n    .Input(\"max_range: float\")\n    .Output(\"output: dtype\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"mode: {'MIN_COMBINED', 'MIN_FIRST', 'SCALED'} = 'MIN_COMBINED'\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .Attr(\"dtype: {bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis = -1;\n      Status s = c->GetAttr(\"axis\", &axis);\n      if (!s.ok() && s.code() != error::NOT_FOUND) {\n        return s;\n      }\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      }\n      auto input_dims = c->Rank(c->input(0));\n      if (axis > input_dims) {\n        return errors::InvalidArgument(\n            \"Axis must be less than input dimension(\", input_dims, \"), got \",\n            axis);\n      }\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), minmax_rank, &minmax));\n      if (axis != -1) {\n        ShapeHandle input;\n        if (axis >= kint32max) {\n          // Check int32 max bound for a corner case to prevent integer flow\n          // when input actually has kint32max rank and above bound check is not\n          // triggered.\n          return errors::InvalidArgument(\n              \"Axis cannot be >= kint32max value, got \", axis);\n        }\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConcat\")\n    .Input(\"concat_dim: int32\")\n    .Input(\"values: N * T\")\n    .Input(\"input_mins: N * float32\")\n    .Input(\"input_maxes: N * float32\")\n    .Output(\"output: T\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      const int n = (c->num_inputs() - 1) / 3;\n      TF_RETURN_IF_ERROR(shape_inference::ConcatShape(c, n));\n      ShapeHandle unused;\n      for (int i = n + 1; i < c->num_inputs(); ++i) {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 0, &unused));\n      }\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedReshape\")\n    .Input(\"tensor: T\")\n    .Input(\"shape: Tshape\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Output(\"output: T\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"T: type\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(SetOutputShapeForReshape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedInstanceNorm\")\n    .Input(\"x: T\")\n    .Input(\"x_min: float\")\n    .Input(\"x_max: float\")\n    .Output(\"y: T\")\n    .Output(\"y_min: float\")\n    .Output(\"y_max: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"output_range_given: bool = false\")\n    .Attr(\"given_y_min: float = 0\")\n    .Attr(\"given_y_max: float = 0\")\n    .Attr(\"variance_epsilon: float = 1e-5\")\n    .Attr(\"min_separation: float = 1e-3\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // x should be a rank 4 tensor.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &unused));\n      // Assert x_min and x_max are scalars (rank 0).\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      // y has the same shape as x.\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      // y_min and y_max are scalars.\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus ScatterNdTensorShape(InferenceContext* c) {\n  ShapeHandle output_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &output_shape));\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &indices_shape));\n  ShapeHandle updates_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 0, &updates_shape));\n  return shape_inference::ScatterNdShapeHelper(c, indices_shape, updates_shape,\n                                               output_shape);\n}\n\n}  // namespace\n\nREGISTER_OP(\"UpperBound\")\n    .Input(\"sorted_inputs: T\")\n    .Input(\"values: T\")\n    .Output(\"output: out_type\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &unused_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 2, &unused_shape));\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LowerBound\")\n    .Input(\"sorted_inputs: T\")\n    .Input(\"values: T\")\n    .Output(\"output: out_type\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &unused_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 2, &unused_shape));\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"ScatterNd\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Input(\"shape: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int16, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle indices_shape;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &indices_shape));\n      ShapeHandle updates_shape;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &updates_shape));\n      ShapeHandle output_shape;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(2, &output_shape));\n      return shape_inference::ScatterNdShapeHelper(c, indices_shape,\n                                                   updates_shape, output_shape);\n    });\n\nREGISTER_OP(\"TensorScatterUpdate\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int16, int32, int64, uint16}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"TensorScatterAdd\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"TensorScatterSub\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"TensorScatterMin\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"TensorScatterMax\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"ScatterNdNonAliasingAdd\")\n    .Input(\"input: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {numbertype, bool}\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"FakeQuantWithMinMaxArgs\")\n    .Attr(\"min: float = -6.0\")\n    .Attr(\"max: float = 6.0\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"inputs: float\")\n    .Output(\"outputs: float\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FakeQuantWithMinMaxArgsGradient\")\n    .Attr(\"min: float = -6.0\")\n    .Attr(\"max: float = 6.0\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"gradients: float\")\n    .Input(\"inputs: float\")\n    .Output(\"backprops: float\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FakeQuantWithMinMaxVars\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"inputs: float\")\n    .Input(\"min: float\")\n    .Input(\"max: float\")\n    .Output(\"outputs: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"FakeQuantWithMinMaxVarsGradient\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"gradients: float\")\n    .Input(\"inputs: float\")\n    .Input(\"min: float\")\n    .Input(\"max: float\")\n    .Output(\"backprops_wrt_input: float\")\n    .Output(\"backprop_wrt_min: float\")\n    .Output(\"backprop_wrt_max: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      // gradients and inputs are same size.\n      ShapeHandle inputs;\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &inputs));\n\n      // min and max are scalars\n      ShapeHandle min_max;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &min_max));\n      TF_RETURN_IF_ERROR(c->Merge(min_max, c->input(3), &min_max));\n\n      c->set_output(0, inputs);\n      c->set_output(1, min_max);\n      c->set_output(2, min_max);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"FakeQuantWithMinMaxVarsPerChannel\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"inputs: float\")\n    .Input(\"min: float\")\n    .Input(\"max: float\")\n    .Output(\"outputs: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input, min, max;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &min));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &max));\n\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(c->Merge(c->Dim(input, -1), c->Dim(min, 0), &unused));\n      TF_RETURN_IF_ERROR(c->Merge(c->Dim(input, -1), c->Dim(max, 0), &unused));\n      TF_RETURN_IF_ERROR(c->Merge(c->Dim(min, 0), c->Dim(max, 0), &unused));\n\n      c->set_output(0, input);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"FakeQuantWithMinMaxVarsPerChannelGradient\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"gradients: float\")\n    .Input(\"inputs: float\")\n    .Input(\"min: float\")\n    .Input(\"max: float\")\n    .Output(\"backprops_wrt_input: float\")\n    .Output(\"backprop_wrt_min: float\")\n    .Output(\"backprop_wrt_max: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle inputs;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &inputs));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(inputs, 4, &inputs));\n      TF_RETURN_IF_ERROR(c->Merge(inputs, c->input(1), &inputs));\n\n      ShapeHandle last_dim = c->Vector(c->Dim(inputs, -1));\n\n      ShapeHandle min_max;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &min_max));\n      TF_RETURN_IF_ERROR(c->Merge(min_max, last_dim, &min_max));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(3), min_max, &min_max));\n\n      c->set_output(0, inputs);\n      c->set_output(1, min_max);\n      c->set_output(2, min_max);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Fingerprint\")\n    .Input(\"data: T\")\n    .Input(\"method: string\")\n    .Output(\"fingerprint: uint8\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n\n      DimensionHandle fingerprint_size;\n      const Tensor* method = c->input_tensor(1);\n      if (method == nullptr) {\n        fingerprint_size = c->UnknownDim();\n      } else {\n        if (method->dims() != 0) {\n          return errors::InvalidArgument(\"`method` must be rank 0: \",\n                                         method->shape());\n        }\n        const string& method_string = method->scalar<tstring>()();\n        if (method_string != \"farmhash64\") {\n          return errors::InvalidArgument(\"Unsupported method: \", method_string);\n        }\n        fingerprint_size = c->MakeDim(sizeof(uint64));\n      }\n\n      DimensionHandle batch = c->Dim(c->input(0), 0);\n      c->set_output(0, c->MakeShape({batch, fingerprint_size}));\n      return OkStatus();\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklConcat\")\n    .Input(\"concat_dim: int32\")\n    .Input(\"values: N * T\")\n    .Input(\"mkl_concat_dim: uint8\")\n    .Input(\"mkl_values: N * uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::ConcatShape(c, c->num_inputs() - 3);\n    })\n    .Doc(R\"doc(\nMKL version of Concat operator. Uses MKL DNN APIs to perform concatenation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\n// Deprecated op registrations:\n\n// The following can be deleted after 10mar2017.\nREGISTER_OP(\"BatchMatrixDiag\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Deprecated(14, \"Use MatrixDiag\")\n    .SetShapeFn(shape_inference::UnknownShape);\nREGISTER_OP(\"BatchMatrixSetDiag\")\n    .Input(\"input: T\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Deprecated(14, \"Use MatrixSetDiag\")\n    .SetShapeFn(shape_inference::UnknownShape);\nREGISTER_OP(\"BatchMatrixDiagPart\")\n    .Input(\"input: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\"T: type\")\n    .Deprecated(14, \"Use MatrixDiagPart\")\n    .SetShapeFn(shape_inference::UnknownShape);\nREGISTER_OP(\"BatchMatrixBandPart\")\n    .Input(\"input: T\")\n    .Input(\"num_lower: int64\")\n    .Input(\"num_upper: int64\")\n    .Output(\"band: T\")\n    .Attr(\"T: type\")\n    .Deprecated(14, \"Use MatrixBandPart\")\n    .SetShapeFn(shape_inference::UnknownShape);\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for array_ops.\"\"\"\nimport re\nimport time\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import config\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import list_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensor\nfrom tensorflow.python.platform import test as test_lib\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BatchMatrixTransposeTest(test_util.TensorFlowTestCase):\n\n  def testNonBatchMatrix(self):\n    matrix = [[1, 2, 3], [4, 5, 6]]  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n    transposed = array_ops.matrix_transpose(matrix)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testConjugate(self):\n    m = [[1 + 1j, 2 + 2j, 3 + 3j], [4 + 4j, 5 + 5j, 6 + 6j]]\n    expected_transposed = [[1 - 1j, 4 - 4j], [2 - 2j, 5 - 5j], [3 - 3j, 6 - 6j]]\n    matrix = ops.convert_to_tensor(m)\n    transposed = array_ops.matrix_transpose(matrix, conjugate=True)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testBatchMatrix(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    batch_matrix = [matrix_0, matrix_1]  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n    transposed = array_ops.matrix_transpose(batch_matrix)\n    self.assertEqual((2, 3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testNonBatchMatrixDynamicallyDefined(self):\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    matrix = constant_op.constant([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(matrix))\n\n  def testBatchMatrixDynamicallyDefined(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    batch_matrix = constant_op.constant([matrix_0, matrix_1])  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(batch_matrix))\n\n  def testTensorWithStaticRankLessThanTwoRaisesBecauseNotAMatrix(self):\n    vector = [1, 2, 3]\n    with self.assertRaisesRegex(ValueError, \"should be a \"):\n      array_ops.matrix_transpose(vector)\n\n  def testNarrowMatrixConjugateTranspose(self):\n    for dtype in (dtypes.float32, dtypes.float64):\n      for conjugate in (True, False):\n        with self.subTest(complex_type=dtype, conjugate=conjugate):\n          vector = math_ops.complex(\n              constant_op.constant(0, dtype=dtype),\n              math_ops.range(96, dtype=dtype))\n          column_vector = array_ops.expand_dims(vector, axis=-1)\n          row_vector = array_ops.expand_dims(vector, axis=0)\n          narrow_matrix = array_ops.tile(column_vector, [1, 2])  # [96, 2]\n          expected_transposed = array_ops.tile(row_vector, [2, 1])  # [2, 96]\n          if conjugate:\n            expected_transposed = -expected_transposed\n\n          transposed = array_ops.matrix_transpose(\n              narrow_matrix, conjugate=conjugate)\n\n          self.assertEqual((2, 96), transposed.get_shape())\n          self.assertAllEqual(expected_transposed, transposed)\n\n\nclass BooleanMaskTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.rng = np.random.RandomState(42)\n\n  def CheckVersusNumpy(self, ndims_mask, arr_shape, make_mask=None, axis=None):\n    \"\"\"Check equivalence between boolean_mask and numpy masking.\"\"\"\n    if make_mask is None:\n      make_mask = lambda shape: self.rng.randint(0, 2, size=shape).astype(bool)\n    arr = np.random.rand(*arr_shape)\n    mask = make_mask(arr_shape[:ndims_mask])\n    if axis is not None:\n      mask = make_mask(arr_shape[axis:ndims_mask + axis])\n    if axis is None or axis == 0:\n      masked_arr = arr[mask]\n    elif axis == 1:\n      masked_arr = arr[:, mask]\n    elif axis == 2:\n      masked_arr = arr[:, :, mask]\n    masked_tensor = array_ops.boolean_mask(arr, mask, axis=axis)\n\n    # Leading dimension size of masked_tensor is always unknown until runtime\n    # since we don't how many elements will be kept.\n    leading = 1 if axis is None else axis + 1\n    self.assertAllEqual(masked_tensor.get_shape()[leading:],\n                        masked_arr.shape[leading:])\n\n    self.assertAllClose(masked_arr, masked_tensor)\n\n  def testMaskDim1ArrDim2Axis1(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim2ArrDim2Axis1(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim1ArrDim1(self):\n    ndims_mask = 1\n    for arr_shape in [(1,), (2,), (3,), (10,)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim1ArrDim2(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim2(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim3(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1, 1), (1, 2, 2), (2, 2, 1)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testEmptyInput2D(self):\n    mask = np.array([True, False])\n    arr = np.array([[], []]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyInput1D(self):\n    mask = np.array([]).astype(bool)\n    arr = np.array([]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyOutput(self):\n    make_mask = lambda shape: np.zeros(shape, dtype=bool)\n    for ndims_mask in range(1, 4):\n      for ndims_arr in range(ndims_mask, ndims_mask + 3):\n        for _ in range(3):\n          with self.subTest(ndims_mask=ndims_mask, ndims_arr=ndims_arr, _=_):\n            arr_shape = np.random.randint(1, 5, size=ndims_arr)\n            self.CheckVersusNumpy(ndims_mask, arr_shape, make_mask=make_mask)\n\n  def testWorksWithDimensionsEqualToNoneDuringGraphBuild(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(ph_tensor, ph_mask):\n      return array_ops.boolean_mask(ph_tensor, ph_mask)\n\n    f = func.get_concrete_function(\n        tensor_spec.TensorSpec(None, dtypes.int32),\n        tensor_spec.TensorSpec([None], dtypes.bool))\n    arr = np.array([[1, 2], [3, 4]], np.int32)\n    mask = np.array([False, True])\n    masked_tensor = f(arr, mask)\n    self.assertAllEqual(masked_tensor, arr[mask])\n\n  def testMaskDimensionsSetToNoneRaises(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(tensor, mask):\n      return array_ops.boolean_mask(tensor, mask)\n\n    with self.assertRaisesRegex(ValueError, \"dimensions must be specified\"):\n      _ = func.get_concrete_function(\n          tensor_spec.TensorSpec([None, 2], dtypes.int32),\n          tensor_spec.TensorSpec(None, dtypes.bool))\n\n  def testMaskHasMoreDimsThanTensorRaises(self):\n    mask = [[True, True], [False, False]]\n    tensor = [1, 2, 3, 4]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskIsScalarRaises(self):\n    mask = True\n    tensor = 1\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"mask.*scalar\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskShapeDifferentThanFirstPartOfTensorShapeRaises(self):\n    mask = [True, True, True]\n    tensor = [[1, 2], [3, 4]]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testStringMask(self):\n    # Reproduces b/111171330, where the optimized boolean_mask graph would\n    # be incorrectly placed on GPU.\n    config.set_optimizer_experimental_options({\"shape_optimization\": True})\n\n    @def_function.function\n    def func(tile_input):\n      string_tensor = array_ops.tile([[\"hello\"]], tile_input)\n      bool_tensor = array_ops.tile([[True]], tile_input)\n      masked_tensor = array_ops.boolean_mask(string_tensor, bool_tensor)\n      return masked_tensor\n\n    result = func([2, 2])\n    self.assertAllEqual([b\"hello\", b\"hello\", b\"hello\", b\"hello\"], result)\n\n  def testMaskWithAxisTensor(self):\n\n    @def_function.function(autograph=False)\n    def f():\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True],\n                                    axis=constant_op.constant(\n                                        0, dtype=dtypes.int32))\n\n    self.assertAllEqual(self.evaluate(f()), [1, 3])\n\n  def testMaskWithAxisNonConstTensor(self):\n\n    @def_function.function(\n        autograph=False,\n        input_signature=[\n            tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n        ])\n    def f(axis):\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True], axis=axis)\n\n    self.assertAllEqual(\n        self.evaluate(f(constant_op.constant(0, dtype=dtypes.int32))), [1, 3])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass OperatorShapeTest(test_util.TensorFlowTestCase):\n\n  def testExpandScalar(self):\n    scalar = \"hello\"\n    scalar_expanded = array_ops.expand_dims(scalar, [0])\n    self.assertEqual(scalar_expanded.get_shape(), (1,))\n\n  def testSqueezeScalar(self):\n    scalar = \"hello\"\n    scalar_squeezed = array_ops.squeeze(scalar, ())\n    self.assertEqual(scalar_squeezed.get_shape(), ())\n\n  def testSqueezeMatrix(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, [0])\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n    with self.assertRaisesRegex(\n        Exception, \"Can not squeeze dim.1., expected a dimension of 1, got 3\"):\n      matrix_squeezed = array_ops.squeeze(matrix, [1])\n\n  def testSqueezeScalarDim(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, 0)\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n  def testExpandDimsWithNonScalarDim(self):\n    with self.assertRaisesRegex(Exception,\n                                \"must be a tensor with a single value\"):\n      array_ops.expand_dims(1, axis=[0, 1])\n\n  def testReshapeWithManyDims(self):\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"too many dimensions\"):\n      self.evaluate(\n          array_ops.reshape(\n              tensor=[[1]],\n              shape=constant_op.constant([1 for i in range(254)],\n                                         dtype=dtypes.int64)))\n\n\n@test_util.with_eager_op_as_function\nclass ReverseV2Test(test_util.TensorFlowTestCase):\n\n  def testReverse0DimAuto(self):\n    x_np = 4\n    for use_gpu in [False, True]:\n      with self.subTest(use_gpu=use_gpu):\n        with self.cached_session(use_gpu=use_gpu):\n          x_tf = self.evaluate(array_ops.reverse_v2(x_np, []))\n          self.assertAllEqual(x_tf, x_np)\n\n  def _reverse1DimAuto(self, np_dtype):\n    x_np = np.array([1, 200, 3, 40, 5], dtype=np_dtype)\n\n    for use_gpu in [False, True]:\n      for axis_dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(use_gpu=use_gpu, axis_dtype=axis_dtype):\n          x_tf = self.evaluate(\n              array_ops.reverse_v2(x_np,\n                                   constant_op.constant([0], dtype=axis_dtype)))\n          self.assertAllEqual(x_tf, np.asarray(x_np)[::-1])\n\n  def _reverse2DimAuto(self, np_dtype):\n    x_np = np.array([[1, 200, 3], [4, 5, 60]], dtype=np_dtype)\n\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for use_gpu in [False, True]:\n        for axis_dtype in [dtypes.int32, dtypes.int64]:\n          with self.subTest(\n              reverse_f=reverse_f, use_gpu=use_gpu, axis_dtype=axis_dtype):\n            x_tf_1 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([0], dtype=axis_dtype)))\n            x_tf_2 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-2], dtype=axis_dtype)))\n            x_tf_3 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1], dtype=axis_dtype)))\n            x_tf_4 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-1], dtype=axis_dtype)))\n            x_tf_5 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1, 0], dtype=axis_dtype)))\n            self.assertAllEqual(x_tf_1, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_2, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_3, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_4, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_5, np.asarray(x_np)[::-1, ::-1])\n\n  # This test covers the axis validation in the shape function\n  # (no eval())\n  def testInvalidAxis(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [-30])\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [2])\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        r\"axis 0 specified more than once|axis 0 was repeated\"):\n      array_ops.reverse_v2(x_np, [0, -2])\n\n  # This is the version of reverse that uses axis indices rather than\n  # bool tensors\n  # TODO(b/32254538): Change this test to use array_ops.reverse\n  #\n  # Note: this test passes placeholder as constant axis is validated\n  # in shape function (see testInvalidAxis)\n  def testInvalid(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n\n    @def_function.function\n    def func(ax):\n      return array_ops.reverse_v2(x_np, ax)\n\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([-30])\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([2])\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"(axis 0 specified more than once|canonicalized axis 0 was repeated.)\"):\n      func([0, -2])\n\n  def testReverse1DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse1DimAuto(dtype)\n\n  def testReverse2DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse2DimAuto(dtype)\n\n  def testReverseRowsOf3Channels(self):\n    \"\"\"Tests optimized code for reversing rows with last dim size = 3.\"\"\"\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseRowsOf4Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 4, dtype=np.float32),\n                newshape=(outer_size, middle_size, 4))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseColumnsOf3Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in list(range(50)) + [100000]:\n        for middle_size in (1, 2):\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [0]))\n            np_answer = x_np[::-1, :, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseInvalidShape(self):\n    x = np.ndarray(shape=[0, 1, 1])\n    v = array_ops.reverse_v2(x, axis=[1])\n    self.assertAllEqual(self.evaluate(v), v)\n\n\nclass MeshgridTest(test_util.TensorFlowTestCase):\n\n  def _compareDiff(self, x, y, use_gpu):\n    for index in (\"ij\", \"xy\"):\n      numpy_out = np.meshgrid(x, y, indexing=index)\n      tf_out = array_ops.meshgrid(x, y, indexing=index)\n      with self.cached_session(use_gpu=use_gpu):\n        for xx, yy in zip(numpy_out, tf_out):\n          self.assertAllEqual(xx, yy)\n\n  def _compareDiffType(self, n, np_dtype, use_gpu):\n    inputs = []\n    for index in (\"ij\", \"xy\"):\n      for _ in range(n):\n        x = np.linspace(-10, 10, 5).astype(np_dtype)\n        if np_dtype in (np.complex64, np.complex128):\n          x += 1j\n        inputs.append(x)\n      numpy_out = np.meshgrid(*inputs, indexing=index)\n      with test_util.device(use_gpu=use_gpu):\n        tf_out = array_ops.meshgrid(*inputs, indexing=index)\n        for x_np, x_tf in zip(numpy_out, tf_out):\n          self.assertAllEqual(x_np, x_tf)\n\n  def testCompare(self):\n    for t in (np.float16, np.float32, np.float64, np.int32, np.int64,\n              np.complex64, np.complex128):\n      with self.subTest(t=t):\n        self._compareDiffType(2, t, False)\n        self._compareDiffType(3, t, False)\n\n        x = [1, 2, 3]\n        y = [4, 5]\n\n        a = [[1, 1], [1, 1]]\n\n        self._compareDiff(x, y, False)\n        self._compareDiff(x, a, False)\n\n\nclass StridedSliceChecker(object):\n  \"\"\"Check a given tensor against the numpy result.\"\"\"\n\n  REF_TENSOR = np.arange(1, 19, dtype=np.float32).reshape(3, 2, 3)\n  REF_TENSOR_ALIGNED = np.arange(1, 97, dtype=np.float32).reshape(3, 4, 8)\n\n  def __init__(self, test, x, tensor_type=dtypes.int32, check_type_infer=True):\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    if tensor_type.is_bool:\n      self.x_np = np.array(x % 3).astype(np.bool_)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.test = test\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n    self.check_type_infer = check_type_infer\n\n  def __getitem__(self, spec):\n    op = self.x.__getitem__(spec)\n\n    def eval_if_tensor(x):\n      try:\n        return self.test.evaluate(x)\n      except (AttributeError, TypeError, ValueError):\n        return x\n\n    def casts_to_bool_nparray(x):\n      try:\n        return np.asarray(x).dtype == bool\n      except NotImplementedError:\n        return False\n\n    if isinstance(spec, bool) or \\\n      (isinstance(spec, ops.Tensor) and spec.dtype == dtypes.bool) or \\\n      (isinstance(spec, np.ndarray) and spec.dtype == bool) or \\\n      (isinstance(spec, (list, tuple)) and casts_to_bool_nparray(spec)):\n      tensor = self.test.evaluate(op)\n      np_spec = eval_if_tensor(spec)\n      self.test.assertAllEqual(self.x_np[np_spec], tensor)\n      return tensor\n\n    if not isinstance(spec, (list, tuple)):\n      spec = [spec]\n\n    tensor = self.test.evaluate(op)\n\n    # Make a numpy spec that pre-evals the tensors\n    np_specs = []\n\n    for s in spec:\n      if isinstance(s, slice):\n        start = eval_if_tensor(s.start)\n        stop = eval_if_tensor(s.stop)\n        step = eval_if_tensor(s.step)\n        np_specs.append(slice(start, stop, step))\n      else:\n        np_specs.append(eval_if_tensor(s))\n\n    self.test.assertAllEqual(self.x_np[tuple(np_specs)], tensor)\n    if self.check_type_infer:\n      self.test.assertAllEqual(tensor.shape, op.get_shape())\n    return tensor\n\n\nSTRIDED_SLICE_TYPES = [\n    dtypes.int32, dtypes.int64, dtypes.int16, dtypes.int8, dtypes.uint8,\n    dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128,\n    dtypes.bool, dtypes.bfloat16\n]\n\n\nclass StridedSliceTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the strided slice operation with variants of slices.\"\"\"\n\n  def test_basic_slice(self):\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n        _ = checker[:, :, :]\n        # Various ways of representing identity slice\n        _ = checker[:, :, :]\n        _ = checker[::, ::, ::]\n        _ = checker[::1, ::1, ::1]\n        # Not zero slice\n        _ = checker[::1, ::5, ::2]\n        # Reverse in each dimension independently\n        _ = checker[::-1, :, :]\n        _ = checker[:, ::-1, :]\n        _ = checker[:, :, ::-1]\n        ## negative index tests i.e. n-2 in first component\n        _ = checker[-2::-1, :, ::1]\n        # negative index tests i.e. n-2 in first component, non-unit stride\n        _ = checker[-2::-1, :, ::2]\n\n        # Check rank-0 examples\n        checker2 = StridedSliceChecker(self, 5, tensor_type=tensor_type)\n        _ = checker2[None]\n        _ = checker2[...]\n        _ = checker2[tuple()]\n\n  def testInt64GPU(self):\n    if not test_util.is_gpu_available():\n      self.skipTest(\"No GPU available\")\n\n    with test_util.force_gpu():\n      x = constant_op.constant([1., 2., 3.])\n      begin = constant_op.constant([2], dtype=dtypes.int64)\n      end = constant_op.constant([3], dtype=dtypes.int64)\n      strides = constant_op.constant([1], dtype=dtypes.int64)\n      s = array_ops.strided_slice(x, begin, end, strides)\n      self.assertAllEqual([3.], self.evaluate(s))\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testTensorSliceEagerMemory(self):\n    with context.eager_mode():\n      inputs = constant_op.constant([[[1], [2], [3], [4]]],\n                                    dtype=dtypes.float32)\n      # Tests that slicing an EagerTensor doesn't leak memory\n      inputs[0]  # pylint: disable=pointless-statement\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testVariableSliceEagerMemory(self):\n    with context.eager_mode():\n      v = variables.Variable([1., 2.])\n      v[0]  # pylint: disable=pointless-statement\n\n  def testDegenerateSlices(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      # degenerate by offering a forward interval with a negative stride\n      _ = checker[0:-1:-1, :, :]\n      # degenerate with a reverse interval with a positive stride\n      _ = checker[-1:0, :, :]\n      # empty interval in every dimension\n      _ = checker[-1:0, 2:2, 2:3:-1]\n      # empty first dimension only (used to break for aligned tensors).\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      _ = checker[1:0]\n\n  def testSliceWithUndefinedDimension(self):\n    t = constant_op.constant([1, 2, 3])\n    d = tensor_shape.Dimension(None)\n    self.assertAllEqual(t[d:d:d], t)\n\n  def testEllipsis(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2], [3, 4], [5, 6]]], [[[7, 8], [9, 10], [11, 12]]]]]\n      checker = StridedSliceChecker(self, raw)\n\n      _ = checker[0:]\n      # implicit ellipsis\n      _ = checker[0:, ...]\n      # ellipsis alone\n      _ = checker[...]\n      # ellipsis at end\n      _ = checker[0:1, ...]\n      # ellipsis at begin\n      _ = checker[..., 0:1]\n      # ellipsis at middle\n      _ = checker[0:1, ..., 0:1]\n      # multiple ellipses not allowed\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"Multiple ellipses\"):\n        _ = checker[..., :, ...].eval()\n\n  def testShrink(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      _ = checker[:, :, :, :, 3]\n      _ = checker[..., 3]\n      _ = checker[:, 0]\n      _ = checker[:, :, 0]\n\n  def testBothNewAxisAndShrink(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def func(inp):\n        return inp[array_ops.newaxis, :, 0]\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec([2, 2], dtypes.int16))\n\n      # TODO(b/190416665): Allow the constant to be eagerly copied/created on\n      # the GPU.\n      with ops.device(\"CPU\"):\n        ones = constant_op.constant([[1, 1], [1, 1]], dtypes.int16)\n      self.assertAllEqual([[1, 1]], self.evaluate(f(ones)))\n\n  def testTensorIndexing(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw, check_type_infer=False)\n      bar = constant_op.constant(2)\n      bar2 = constant_op.constant(3)\n      _ = checker[..., bar:bar2]\n      _ = checker[..., bar]\n      _ = checker[..., 3]\n      _ = checker[..., 2**64 // 2**63]  # Test longs in Python 2\n\n  def testTensorIndexingTypeError(self):\n    with self.session():\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      expected = re.escape(array_ops._SLICE_TYPE_ERROR)\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[\"foo\"]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(\"foo\")]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[0.0]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(0.0)]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant([1, 2, 3])]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[[2.1, -0.7, 1.5]]\n\n  def testExpand(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      # new axis (followed by implicit ellipsis)\n      _ = checker[np.newaxis]\n      # newaxis after ellipsis\n      _ = checker[..., np.newaxis]\n      # newaxis in between ellipsis and explicit range\n      _ = checker[..., np.newaxis, :]\n      _ = checker[:, ..., np.newaxis, :, :]\n      # Reverse final dimension with new axis\n      _ = checker[:, :, np.newaxis, :, 2::-1]\n      # Ellipsis in middle of two newaxis\n      _ = checker[np.newaxis, ..., np.newaxis]\n\n  def testExpandVariable(self):\n    with test_util.device(use_gpu=True):\n      x = variables.Variable(7, dtype=dtypes.int32)\n      self.evaluate(x.initializer)\n      y = self.evaluate(x[None])\n      self.assertEqual(y.shape, (1,))\n      self.assertAllEqual(y, (7,))\n\n  def testOptimizedCases(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      # Identity\n      _ = checker[:]\n      # Identity\n      _ = checker[...]\n      # Identity\n      _ = checker[np.newaxis, ..., np.newaxis]\n      # First axis slice\n      _ = checker[1:]\n      # First axis slice\n      _ = checker[np.newaxis, 1:]\n\n  def testMasks(self):\n    with test_util.device(use_gpu=True):\n      scalar = np.array(0)\n      # Test tensor type mask\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      _ = checker[checker.x > 2]\n      _ = checker[checker.x <= 5]\n      _ = checker[ops.convert_to_tensor(scalar)]\n\n      # Test numpy array type mask\n      raw = np.array([[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n                       [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23,\n                                                              24]]]]])\n      checker1 = StridedSliceChecker(self, raw)\n      _ = checker1[raw >= 4]\n      _ = checker1[raw < 19]\n      _ = checker1[scalar]\n\n      # Test boolean and non boolean cases\n      mask = np.array([True, False, True])\n      raw1 = np.array([[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]])\n      checker2 = StridedSliceChecker(self, raw1)\n      _ = checker2[mask]\n      _ = checker2[ops.convert_to_tensor(mask)]\n\n  def test_int16_indices(self):\n\n    def _int16(i):\n      return constant_op.constant(i, dtype=dtypes.int16)\n\n    def _int64(i):\n      return constant_op.constant(i, dtype=dtypes.int64)\n\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n\n        _ = checker[_int16(1)]\n\n        with self.assertRaises(Exception):\n          _ = checker[_int16(1)::1, :, 1:_int64(3):2]\n        with self.assertRaises(Exception):\n          _ = checker[:, _int16(1):_int16(5):-1, :]\n        with self.assertRaises(Exception):\n          _ = checker[::_int64(1), _int64(1):10:_int16(3), ::_int64(2)]\n\n        _ = checker[::_int16(1), _int16(1)::_int16(5), ::2]\n        _ = checker[_int16(1):_int16(5):_int16(2), 1:2, :]\n\n\nclass StridedSliceShapeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the shape inference of StridedSliceShapes.\"\"\"\n\n  def testUnknown(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f(x):\n        y = x[...]\n        self.assertAllEqual(y.get_shape().ndims, None)\n\n      _ = f.get_concrete_function(tensor_spec.TensorSpec(None, dtypes.float32))\n\n  def tensorShapeEqual(self, x, y):\n    self.assertTrue(x is not None and y is not None or x is None and y is None)\n    self.assertEqual(x.as_list(), y.as_list())\n\n  def testTensorShapeUncertain(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f1(x):\n        y = x[3:5]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f2(x):\n        y = x[3:5, :, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f3(x):\n        y = x[3:5, 3:4, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f4(x):\n        y = x[3:5, :, 5:10]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 2]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f5(x):\n        y = x[3:5, :, 50:3]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 0]))\n\n      _ = f5.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f6(x):\n        y = x[3:5, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f6.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f7(x):\n        y = x[1:5:2, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f7.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f8(x):\n        y = x[:5:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f8.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f9(x):\n        y = x[:2:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([1, None, 1, 0]))\n\n      _ = f9.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f10(x):\n        y = x[::-1, :, array_ops.newaxis, ::-2]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([5, None, 1, 4]))\n\n      _ = f10.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n  def testTensorValuedIndexShape(self):\n    with self.session():\n\n      @def_function.function\n      def f1(x, y):\n        z = x[y]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f2(x, y):\n        z = x[y, ::-1]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f3(x, y):\n        z = x[y, ::-2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([2, 7]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f4(x, y, s):\n        z = x[y, s:2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([None,\n                                                                       7]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n\nclass GradSliceChecker(object):\n  \"\"\"Tests that we can compute a gradient for var^2.\"\"\"\n\n  def __init__(self, test, var, varnp, use_tape):\n    self.test = test\n    self.var = var\n    self.varnp = varnp\n    self.use_tape = use_tape\n\n  def __getitem__(self, spec):\n    with test_util.AbstractGradientTape(\n        use_tape=self.use_tape, persistent=True) as tape:\n      tape.watch(self.var)\n      val = self.var * self.var\n      slice_var = self.var[spec]\n      slice_val = val[spec]\n\n      # compute analytic 2nd derivative\n      analytic_grad2 = 2 * slice_val\n\n      dy = variables.Variable(\n          array_ops.ones_like(slice_var, dtype=dtypes.float32))\n      assign = dy.assign(slice_var)\n\n      slice_val_grad = tape.gradient(slice_val, self.var, [dy])\n      slice_val_grad2 = tape.gradient(slice_val_grad, dy, [self.var])\n    self.test.evaluate(assign)\n    slice_val_grad_evaled, slice_val_grad2_evaled = (\n        self.test.evaluate([slice_val_grad, slice_val_grad2]))\n    analytic_grad2_evaled = self.test.evaluate(analytic_grad2)\n    self.test.assertAllEqual(slice_val_grad2_evaled, analytic_grad2_evaled)\n\n    # compute analytic gradient for slice\n    np_val_grad = (2 * self.varnp * self.varnp)\n    np_sliceval_grad = np.zeros(self.var.get_shape())\n    if isinstance(spec, ops.Tensor):\n      spec = self.test.evaluate(spec)\n    np_sliceval_grad[spec] = np_val_grad[spec]\n    # verify gradient\n    self.test.assertAllEqual(slice_val_grad_evaled, np_sliceval_grad)\n\n\nclass StridedSliceGradTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n  \"\"\"Test that strided slice's custom gradient produces correct gradients.\"\"\"\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  @test_util.disable_xla(\n      \"b/210077724: Auto-clustering with where op isn't supported. Has loose \"\n      \"output shape bounds\")\n  def testGradient(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(\n          array_ops.reshape(\n              math_ops.range(1, 97, 1, dtype=dtypes.float32), shape=(6, 4, 4)))\n      self.evaluate(var.initializer)\n\n      raw = np.array(range(1, 97, 1)).reshape((6, 4, 4))\n      grad = GradSliceChecker(self, var, raw, use_tape)\n      _ = grad[2:6:2, 1:3, 1:3]\n      _ = grad[3:0:-2, 1:3, 1:3]\n      _ = grad[3:0:-2, array_ops.newaxis, 1:3, 2, array_ops.newaxis]\n      _ = grad[3:0:-2, 1:3, 2]\n      _ = grad[:, -1, :]\n      _ = grad[:, -2, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, -200, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, 200, :]\n\n      # Test numpy array type mask\n      _ = grad[raw > 51]\n      # Test tensor type mask\n      _ = grad[ops.convert_to_tensor(raw) <= 76]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testGradientZero(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(8.)\n      self.evaluate(var.initializer)\n      grad = GradSliceChecker(self, var, np.array(8), use_tape)\n      _ = grad[tuple()]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testInt64Indices(self, use_tape):\n    with test_util.AbstractGradientTape(use_tape=use_tape) as tape:\n      a = math_ops.range(3, dtype=dtypes.float32)\n      tape.watch(a)\n      index = constant_op.constant(1, dtype=dtypes.int64)\n      b = 2. * a[index]\n    grad = tape.gradient(b, a)\n    self.assertAllEqual(self.evaluate(grad), [0., 2., 0.])\n\n\nclass StridedSliceGradTypeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test varied index types and host located memory.\"\"\"\n\n  def testHostVsDevice(self):\n    var2 = variables.Variable(\n        array_ops.reshape(\n            math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32),\n            shape=(4, 1, 1)))\n    varshape = variables.Variable([6, 4, 4], dtype=dtypes.int32)\n    begin = constant_op.constant([0, 0, 0])\n    end = constant_op.constant([4, 1, 1])\n    strides = constant_op.constant([1, 1, 1])\n    foo = array_ops.strided_slice_grad(varshape, begin, end, strides, var2)\n    self.evaluate(var2.initializer)\n    self.evaluate(varshape.initializer)\n    self.evaluate(foo)\n\n  def testInt64Shape(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int64)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                      original_dy)\n    self.evaluate(dx)\n\n  def testMixedIndexTypes(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int32)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    with self.assertRaises((TypeError, errors_impl.InvalidArgumentError)):\n      dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                        original_dy)\n      self.evaluate(dx)\n\n\nclass BenchmarkSlice(object):\n\n  def __init__(self, tensor):\n    self.tensor = tensor\n\n  def __getitem__(self, x):\n    return self.tensor[x]\n\n\nclass StridedSliceBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark new strided slice operation on non-trivial case.\"\"\"\n\n  def run_and_time(self, slice_op):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(10):\n      _ = self.evaluate(slice_op)\n    iters = 1000\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(slice_op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / 1000.0)\n\n  def make_variable(self):\n    n = 256\n    shape = (n, n, n)\n    items = n**3\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtypes.float32)\n    return var\n\n  def benchmark_strided_slice_skip(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[::2, ::1, ::2]\n      self.run_and_time(slice_op)\n\n  def benchmark_strided_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n  def benchmark_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      slice_op = var[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n\nclass StridedSliceAssignChecker(object):\n\n  def __init__(self, test, x, tensor_type=dtypes.float32, use_resource=False):\n    self.tensor_type = tensor_type\n    self.test = test\n    self._use_resource = use_resource\n\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n\n  def __setitem__(self, index, value):\n    value = np.array(value).astype(self.tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if self.tensor_type.is_complex:\n      value -= 1j * value\n\n    with test_util.device(use_gpu=True):\n      if self._use_resource:\n        var = resource_variable_ops.ResourceVariable(self.x)\n      else:\n        var = variables.Variable(self.x)\n      self.test.evaluate(var.initializer)\n      val = self.test.evaluate(var[index].assign(value))\n      # val_copy is used to check that tf.compat.v1.assign works equivalently\n      # to the assign method above.\n      val_copy = self.test.evaluate(state_ops.assign(var[index], value))\n      valnp = np.copy(self.x_np)\n      valnp[index] = np.array(value)\n      self.test.assertAllEqual(val, valnp)\n      self.test.assertAllEqual(val_copy, valnp)\n\n\nclass SliceAssignTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  def testInvalidSlice(self):\n    foo = constant_op.constant([1, 2, 3])\n    with self.assertRaisesRegex(AttributeError, \"no attribute 'assign'\"):\n      bar = foo[:2].assign(constant_op.constant([1, 2]))\n      self.evaluate(bar)\n\n  def doTestSliceAssign(self, use_resource):\n    for dtype in STRIDED_SLICE_TYPES:\n      with self.subTest(dtype=dtype):\n        checker = StridedSliceAssignChecker(\n            self, [[1, 2, 3], [4, 5, 6]],\n            use_resource=use_resource,\n            tensor_type=dtype)\n        # Check if equal\n        checker[:] = [[10, 20, 30], [40, 50, 60]]\n        # Check trivial (1,1) shape tensor\n        checker[1:2, 1:2] = [[66]]\n        # shrinks shape changes\n        checker[1:2, 1] = [66]\n        checker[1, 1:2] = [66]\n        checker[1, 1] = 66\n        # newaxis shape changes\n        checker[:, None, :] = [[[10, 20, 30]], [[40, 50, 50]]]\n        # shrink and newaxis\n        checker[None, None, 0, 0:1] = [[[99]]]\n        # Non unit strides\n        checker[::1, ::-2] = [[3, 33], [4, 44]]\n        # degenerate interval\n        checker[8:10, 0] = []\n        checker[8:10, 8:10] = [[]]\n    # Assign vector to scalar (rank-0) using newaxis\n    checker2 = StridedSliceAssignChecker(self, 222)\n    checker2[()] = 6  # no indices\n    checker2[...] = 6  # ellipsis\n    checker2[None] = [6]  # new axis\n\n  def doTestSliceAssignWithBroadcasting(self, use_resource):\n    for dtype in STRIDED_SLICE_TYPES:\n      with self.subTest(dtype=dtype):\n        checker = StridedSliceAssignChecker(\n            self, [[1, 2, 3], [4, 5, 6]],\n            use_resource=use_resource,\n            tensor_type=dtype)\n        # Broadcast to full LHS.\n        checker[:] = [[40, 50, 60]]\n        # Assign a trivial (1,1) tensor.\n        checker[1:2, 1:2] = 66\n        # Broadcast with shrink axis shape changes.\n        checker[1:2, 1] = 66\n        checker[1, 1:2] = 66\n        # Broadcast with newaxis shape changes.\n        checker[:, None, :] = [10, 20, 30]\n        # Broadcast with both shrink and newaxis.\n        checker[None, None, 0, 0:1] = 99\n        # Broadcast with non-unit strides.\n        checker[::1, ::-2] = [[4, 44]]\n        # Broadcast a degenerate interval.\n        checker[8:10, 8:10] = []\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssign(self):\n    self.doTestSliceAssign(use_resource=False)\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssignWithBroadcasting(self):\n    self.doTestSliceAssignWithBroadcasting(use_resource=False)\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssignResource(self):\n    self.doTestSliceAssign(use_resource=True)\n\n  def testTypeError(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = variables.VariableV1(init_val)\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_small_val))\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_large_val))\n\n  def testTypeErrorResource(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = resource_variable_ops.ResourceVariable(init_val)\n    self.evaluate(v.initializer)\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_large_val))\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_small_val))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateWithInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with input-forwarding taking effect.\"\"\"\n    @def_function.function\n    def assign(x):\n      y = x + 1\n      return gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0])\n    self.assertAllEqual([0, 1], self.evaluate(assign(array_ops.zeros([2]))))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateWithInputForwardInt32(self):\n    \"\"\"Tests tensor_strided_slice_update with int32.\"\"\"\n    @def_function.function\n    def assign(x):\n      y = x + 1\n      return gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0])\n    self.assertAllEqual(\n        [0, 1], self.evaluate(assign(array_ops.zeros([2], dtype=dtypes.int32))))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateNoInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with no input-forwarding.\"\"\"\n    x = constant_op.constant([0.2, 0.3])\n    y = x + 1\n    # y's buffer won't be forwarded to z because y and z will be alive at the\n    # same time later.\n    z = gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0.4])\n    ans = y + z\n    self.assertAllClose([1.6, 2.6], self.evaluate(ans))\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGradSimple(self):\n    original = constant_op.constant([0.2, 0.3])\n    updates = constant_op.constant([0.4])\n    with backprop.GradientTape() as tape:\n      tape.watch([original, updates])\n      updated = gen_array_ops.tensor_strided_slice_update(\n          original, [0], [1], [1], updates)\n    d1, d2 = tape.gradient(updated, [original, updates],\n                           output_gradients=constant_op.constant([2.0, 3.0]))\n    self.assertAllClose([0.0, 3.0], d1)\n    self.assertAllClose([2.0], d2)\n\n  @parameterized.named_parameters(\n      (\"_%s\" % i, *args) for i, args in enumerate([  # pylint:disable=g-complex-comprehension\n          ([2, 5], [0, 1], [1, 0], [1, 2], [2], 0, 2, 0, 0, 1),\n          ([4], [5], [3], [1], [3], 1, 0, 0, 0, 0),\n          ([2, 2, 3, 2], [0, 0, 1], [1, 0, 2], [1, 0, 1], [2, 3], 0, 0, 2, 0, 5)\n      ]))\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGrad(\n      self, shape, begin, end, strides, updates_shape, *args):\n    with self.cached_session():\n      def f(a, b):\n        return gen_array_ops.tensor_strided_slice_update(\n            a, begin, end, strides, b, *args)\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [array_ops.zeros(shape), array_ops.ones(updates_shape)], delta=1.0)\n      self.assertAllClose(theoretical, numerical)\n\n  @parameterized.named_parameters((\"_%s\" % i, *args) for i, args in enumerate([  # pylint:disable=g-complex-comprehension\n      ([2, 5], [0, 1], [1, 0], [1, 2], [1], 0, 2, 0, 0,\n       1), ([4], [5], [3], [1], [], 1, 0, 0, 0, 0),\n      ([2, 2, 3, 2], [0, 0, 1], [1, 0, 2], [1, 0, 1], [2, 1], 0, 0, 2, 0, 5)\n  ]))\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateWithBroadcastingGrad(self, shape, begin, end,\n                                                       strides, updates_shape,\n                                                       *args):\n    with self.cached_session():\n\n      def f(a, b):\n        return gen_array_ops.tensor_strided_slice_update(\n            a, begin, end, strides, b, *args)\n\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [array_ops.zeros(shape),\n              array_ops.ones(updates_shape)], delta=1.0)\n      self.assertAllClose(theoretical, numerical)\n\n\nclass ShapeSizeRankTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseShape(self):\n    t_value = [[0, 42], [24, 0]]\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t_value)))\n\n    t = constant_op.constant(t_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseShape(self):\n    sp_value = sparse_tensor.SparseTensorValue(\n        indices=((0, 1), (1, 0)), values=(42, 24), dense_shape=(2, 2))\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp_value)))\n\n    sp = sparse_tensor.SparseTensor.from_value(sp_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSizeDtype(self):\n    tensor = [1]\n    self.assertEqual(dtypes.int32, self.evaluate(array_ops.size(tensor)).dtype)\n    self.assertEqual(\n        dtypes.int64,\n        self.evaluate(array_ops.size(tensor, out_type=dtypes.int64)).dtype)\n\n\nclass SequenceMaskTest(test_util.TensorFlowTestCase):\n\n  def testExceptions(self):\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"`maxlen` must be scalar\"):\n        array_ops.sequence_mask([10, 20], [10, 20])\n\n  def testOneDimensionalWithMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([1, 3, 2]), 5)\n    self.assertAllEqual(res.get_shape(), [3, 5])\n    self.assertAllEqual(\n        res,\n        [[True, False, False, False, False], [True, True, True, False, False],\n         [True, True, False, False, False]])\n\n  def testOneDimensionalDtypeWithoutMaxlen(self):\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([0, 1, 4]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(\n        res, [[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]])\n\n  def testOneDimensionalWithoutMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([0, 1, 4]))\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(res,\n                        [[False, False, False, False],\n                         [True, False, False, False], [True, True, True, True]])\n\n  def testTwoDimensional(self):\n    res = array_ops.sequence_mask(constant_op.constant([[1, 3, 2]]), 5)\n    self.assertAllEqual(res.get_shape(), [1, 3, 5])\n    self.assertAllEqual(\n        res,\n        [[[True, False, False, False, False], [True, True, True, False, False],\n          [True, True, False, False, False]]])\n\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([[0, 1, 4], [1, 2, 3]]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [2, 3, 4])\n    self.assertAllEqual(\n        res,\n        [[[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]],\n         [[1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0]]])\n\n  def testDtypes(self):\n\n    def check_dtypes(lengths_dtype, maxlen_dtype):\n      res = array_ops.sequence_mask(\n          constant_op.constant([1, 3, 2], dtype=lengths_dtype),\n          constant_op.constant(5, dtype=maxlen_dtype))\n      self.assertAllEqual(res.get_shape(), [3, 5])\n      self.assertAllEqual(\n          res,\n          [[True, False, False, False, False], [True, True, True, False, False],\n           [True, True, False, False, False]])\n\n    check_dtypes(dtypes.int32, dtypes.int32)\n    check_dtypes(dtypes.int32, dtypes.int64)\n    check_dtypes(dtypes.int64, dtypes.int32)\n    check_dtypes(dtypes.int64, dtypes.int64)\n\n  def testOutputDtype(self):\n\n    def check_output_dtype(output_dtype):\n      res = self.evaluate(\n          array_ops.sequence_mask(\n              constant_op.constant([1, 3, 2], dtype=dtypes.int32),\n              constant_op.constant(5, dtype=dtypes.int32),\n              dtype=output_dtype))\n      self.assertAllEqual(\n          res,\n          self.evaluate(\n              math_ops.cast([[True, False, False, False, False],\n                             [True, True, True, False, False],\n                             [True, True, False, False, False]], output_dtype)))\n\n    check_output_dtype(dtypes.bool)\n    check_output_dtype(\"bool\")\n    check_output_dtype(np.bool_)\n    check_output_dtype(dtypes.int32)\n    check_output_dtype(\"int32\")\n    check_output_dtype(np.int32)\n    check_output_dtype(dtypes.float32)\n    check_output_dtype(\"float32\")\n    check_output_dtype(np.float32)\n    check_output_dtype(dtypes.int64)\n    check_output_dtype(\"float64\")\n    check_output_dtype(np.float64)\n\n\nclass ConcatSliceResourceTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConcatSlice(self):\n    r1 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"b\")\n    r2 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"c\")\n    c = array_ops.stack([r1, r2])\n    s = array_ops.strided_slice(c, [1], [2])\n    self.evaluate(test_ops.resource_create_op(s))\n    with self.assertRaises(errors.AlreadyExistsError):\n      self.evaluate(test_ops.resource_create_op(r2))\n\n\nclass IdentityTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_gpu_only\n  def testEagerIdentity(self):\n    with context.eager_mode():\n\n      def _test(x, y, device):\n        self.assertAllEqual(x.numpy(), y.numpy())\n        self.assertTrue(device in y.device.lower())\n\n      with test_util.force_gpu():\n        a = constant_op.constant([[2], [3]], dtype=dtypes.float32)\n      with test_util.force_gpu():\n        b = array_ops.identity(a)\n        _test(a, b, \"gpu\")\n      with test_util.force_cpu():\n        c = array_ops.identity(b)\n        _test(b, c, \"cpu\")\n      with test_util.force_cpu():\n        d = array_ops.identity(c)\n        _test(c, d, \"cpu\")\n      with test_util.force_gpu():\n        e = array_ops.identity(d)\n        _test(d, e, \"gpu\")\n\n  def testIdentityVariable(self):\n    v = resource_variable_ops.ResourceVariable(1.0)\n    self.evaluate(v.initializer)\n    result = array_ops.identity(v)\n    self.assertIsInstance(result, ops.Tensor)\n    self.assertAllEqual(result, v)\n\n\nclass PadTest(test_util.TensorFlowTestCase):\n\n  def testEager(self):\n    with context.eager_mode():\n      t = constant_op.constant([[1, 2, 3], [4, 5, 6]])\n      paddings = constant_op.constant([[\n          1,\n          1,\n      ], [2, 2]])\n      padded = array_ops.pad(t, paddings, \"CONSTANT\")\n      self.assertAllEqual(padded.numpy(),\n                          [[0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 3, 0, 0],\n                           [0, 0, 4, 5, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0]])\n\n  # b/246325518: Bad shape size. Explicitly testing different execution paths.\n  def testInvalidMirrorPadGradEagerMode(self):\n    with context.eager_mode():\n      with self.assertRaises(Exception):\n        gen_array_ops.MirrorPadGrad(\n            input=[1], paddings=[[0x77f00000, 0xa000000]], mode=\"REFLECT\")\n\n  # b/246325518: Bad shape size. Explicitly testing different execution paths.\n  def testInvalidMirrorPadGradGraphMode(self):\n    with context.graph_mode():\n      with self.assertRaises(Exception):\n        result = gen_array_ops.MirrorPadGrad(\n            input=[1], paddings=[[0x77f00000, 0xa000000]], mode=\"REFLECT\")\n        self.evaluate(result)\n\n  def testSymmetricMirrorPadGrad(self):\n    t = np.broadcast_to(np.arange(0, 7), (3, 2, 1, 7))\n    paddings = constant_op.constant([\n        [1, 1],\n        [0, 0],\n        [0, 0],\n        [2, 2],\n    ])\n    expected = np.broadcast_to(np.array([9, 27, 27]), (1, 2, 1, 3))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"SYMMETRIC\")\n    self.assertAllEqual(result, expected)\n\n  def testReflectMirrorPadGrad(self):\n    t = np.broadcast_to(np.reshape(np.arange(0, 7), (7, 1)), (1, 4, 7, 1))\n    paddings = constant_op.constant([\n        [0, 0],\n        [1, 1],\n        [2, 2],\n        [0, 0],\n    ])\n    expected = np.broadcast_to(\n        np.reshape(np.array([16, 18, 8]), (3, 1)), (1, 2, 3, 1))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"REFLECT\")\n    self.assertAllEqual(result, expected)\n\n\nclass InvertPermutationTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([3, 4, 0, 2, 1], dtype=dtype)\n        y = array_ops.invert_permutation(x)\n        self.assertAllEqual(y.get_shape(), [5])\n        self.assertAllEqual(y, [2, 4, 3, 0, 1])\n\n\nclass UnravelIndexTest(test_util.TensorFlowTestCase):\n\n  # TODO(b/73086570): Reenable test.\n  @unittest.skip(\"Test does not pass internally.\")\n  def testUnravelIndex(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(dtype=dtype):\n          indices_1 = constant_op.constant(1621, dtype=dtype)\n          dims_1 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_1 = array_ops.unravel_index(indices_1, dims_1)\n          self.assertAllEqual(out_1, [3, 1, 4, 1])\n\n          indices_2 = constant_op.constant([1621], dtype=dtype)\n          dims_2 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_2 = array_ops.unravel_index(indices_2, dims_2)\n          self.assertAllEqual(out_2, [[3], [1], [4], [1]])\n\n          indices_3 = constant_op.constant([22, 41, 37], dtype=dtype)\n          dims_3 = constant_op.constant([7, 6], dtype=dtype)\n          out_3 = array_ops.unravel_index(indices_3, dims_3)\n          self.assertAllEqual(out_3, [[3, 6, 6], [4, 5, 1]])\n\n  # Test case for GitHub issue 40204.\n  def testUnravelIndexZeroDim(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    \"dims cannot contain a dim of zero\"):\n          indices = constant_op.constant([2, 5, 7], dtype=dtype)\n          dims = constant_op.constant([3, 0], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n  def testUnravelIndexIntegerOverflow(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(\n            errors.InvalidArgumentError,\n            r\"Input dims product is causing integer overflow\"):\n          indices = constant_op.constant(-0x100000, dtype=dtype)\n          if dtype == dtypes.int32:\n            value = 0x10000000\n          else:\n            value = 0x7FFFFFFFFFFFFFFF\n          dims = constant_op.constant([value, value], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n\nclass GuaranteeConstOpTest(test_util.TensorFlowTestCase):\n\n  def testSimple(self):\n    a = array_ops.constant(10)\n    guarantee_a = array_ops.guarantee_const(a)\n    self.assertEqual(10, self.evaluate(guarantee_a))\n\n  def testVariables(self):\n    for use_resource in [False, True]:\n      with self.subTest(use_resource=use_resource):\n        a = variable_scope.get_variable(\n            \"var_{}\".format(use_resource), [],\n            initializer=init_ops.constant_initializer(10.0),\n            use_resource=use_resource)\n        guarantee_a = array_ops.guarantee_const(a)\n        self.evaluate(a.initializer)\n        self.assertEqual(10.0, self.evaluate(guarantee_a))\n\n  def testResourceRejection(self):\n    with ops.device(\"/cpu:0\"):\n      a = variable_scope.get_variable(\n          \"resource_var\", [],\n          initializer=init_ops.constant_initializer(10.0),\n          use_resource=True)\n    with self.assertRaisesWithPredicateMatch(errors.InvalidArgumentError,\n                                             \"cannot be a resource variable\"):\n      guarantee_a = array_ops.guarantee_const(a.handle)\n      self.evaluate(a.initializer)\n      self.evaluate(guarantee_a)\n\n\nclass SnapshotOpTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([0, 1, 2, 3], dtype=dtype)\n        y = gen_array_ops.snapshot(x)\n        self.assertAllEqual(y, [0, 1, 2, 3])\n\n\n@test_util.with_eager_op_as_function\n@test_util.run_all_in_graph_and_eager_modes\nclass QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n\n  # Generates a tensor of the specified `shape` using values from `values`\n  # scaled by (slice_idx + 1) along `axis` dimension.\n  def _scale_per_slice(self, shape, axis, values):\n    # Note: repeats the values if the shape is larger than values.\n    out = np.take(values, np.remainder(np.arange(np.prod(shape)),\n                                       len(values))).reshape(shape)\n    if axis is not None:\n      scale_shape = [1] * len(shape)\n      scale_shape[axis] = shape[axis]\n      out *= np.arange(1, shape[axis] + 1).reshape(scale_shape)\n    return out\n\n  def testAxis(self):\n    shape = np.array([2, 3, 4, 5])\n    values = np.array([-1, -0.5, 0, 0.3, 0.8, 0.555, 0.5], dtype=np.float32)\n    quant_values = np.array(\n        [-1, -0.5, 0, 38.0 / 128, 102.0 / 128, 71.0 / 128, 0.5],\n        dtype=np.float32)\n    for axis in [None, 0, 1, 2, 3]:\n      with self.subTest(axis=axis):\n        inputs = constant_op.constant(\n            self._scale_per_slice(shape, axis, values))\n        expected = self._scale_per_slice(shape, axis, quant_values)\n        unused_minmax_value = 0 if axis is None else [0] * shape[axis]\n        fake_quantized = self.evaluate(\n            array_ops.quantize_and_dequantize_v2(\n                inputs,\n                unused_minmax_value,\n                unused_minmax_value,\n                range_given=False,\n                round_mode=\"HALF_UP\",\n                axis=axis))\n        self.assertAllEqual(fake_quantized, expected)\n        if axis is not None:\n          fake_quantized = self.evaluate(\n              array_ops.quantize_and_dequantize_v2(\n                  inputs,\n                  unused_minmax_value,\n                  unused_minmax_value,\n                  range_given=False,\n                  axis=(axis - 4)))\n          self.assertAllClose(fake_quantized, expected)\n\n  def testBadAxis(self):\n    input_tensor = [2.5, 2.5]\n    input_min = [0, 0]\n    input_max = [1, 1]\n    # When eager_op_as_function mode is enabled XLA auto-clustering kicks in.\n    # XLA raises an UnimplementedError on invalid axis.\n    error_message_pattern = (r\"Shape must be at least rank 11 but is rank \"\n                             r\"1|invalid axis\")\n    # TODO(b/171260356): Eager mode and graph mode throw different error types\n    error = (errors.InvalidArgumentError, ValueError, errors.UnimplementedError)\n    with self.assertRaisesRegex(error, error_message_pattern):\n      self.evaluate(\n          array_ops.quantize_and_dequantize_v2(\n              input=input_tensor,\n              input_min=input_min,\n              input_max=input_max,\n              axis=10))\n\n  def testQuantizeDequantizeGrad(self):\n    shape = (2, 2)\n    max_threshold = 0\n    min_threshold = -10\n    input_value = np.random.rand(2, 2) * 40.0 - 20.0\n    input_tensor = constant_op.constant(input_value, shape=shape,\n                                        name=\"input_tensor\")\n    with self.cached_session():\n      def f(a):\n        return array_ops.quantize_and_dequantize_v2(\n            a,\n            input_min=min_threshold,\n            input_max=max_threshold,\n            range_given=True)\n      output_grad = gradient_checker_v2.compute_gradient(f, [input_tensor])\n      self.assertAllClose(output_grad[0], np.zeros([1, 4, 4]))\n\n  def testOutOfBoundAxis(self):\n    input_tensor = constant_op.constant([1., 1.])\n    input_min = [0]\n    input_max = [1]\n    q_input, _, _ = array_ops.quantize(input_tensor, 0, 1, dtypes.qint32)\n    error = (errors.InvalidArgumentError, ValueError)\n    with self.assertRaisesRegex(error,\n                                r\".*Axis must be less than input dimension.*\"):\n      self.evaluate(\n          gen_array_ops.dequantize(\n              input=q_input,\n              min_range=input_min,\n              max_range=input_max,\n              axis=2**31 - 1))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SortedSearchTest(test_util.TensorFlowTestCase):\n\n  def testUpperBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testZeroSequenceSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 0]),\n                array_ops.ones([2, 3]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 3], dtype))\n\n  def testZeroValueSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 3]),\n                array_ops.ones([2, 0]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 0], dtype))\n\n  def testZeroInputSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 0]),\n                array_ops.ones([2, 3]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 3], dtype))\n\n  def testInt64(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y, out_type=dtypes.int64)\n\n    _ = g.get_concrete_function()\n\n  def testInt64UnspecifiedOutType(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y)\n\n    _ = g.get_concrete_function()\n\n\nclass BatchGatherNdTest(test_util.TensorFlowTestCase):\n\n  def testShapesMatch(self):\n    \"\"\"Tests for various different shape combinations.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 2), (2, 3), 0),)\n    shapes.append(((2, 2, 2), (3,), 0),)\n    shapes.append(((2, 2, 2), (1,), 0),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(1.0, shape=(params_shape))\n        indices = constant_op.constant(\n            1, shape=(indices_shape), dtype=dtypes.int32)\n        out = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        ndims_params = len(params_shape) - batch_dims\n        ndims_rows = ndims_params - indices_shape[-1]\n        expected_out_shape = indices_shape[:-1]\n        if ndims_rows > 0:\n          expected_out_shape += params_shape[-ndims_rows:]\n        self.assertSequenceEqual(out.shape, expected_out_shape)\n\n  def testReducesToGatherNDWhenBatchDimIsZero(self):\n    \"\"\"Confirms setting batch_dims to zero reduces to tf.gather_nd.\"\"\"\n    params = constant_op.constant(np.random.uniform(0.0, 1.0, size=(7, 8, 9)))\n    indices_shapes = []\n    indices_shapes.append((1,))\n    indices_shapes.append((3, 1))\n    indices_shapes.append((3, 3, 1))\n    indices_shapes.append((2,))\n    indices_shapes.append((3, 2))\n    indices_shapes.append((3, 3, 2))\n    indices_shapes.append((3,))\n    indices_shapes.append((3, 3))\n    indices_shapes.append((3, 3, 3))\n\n    for indices_shape in indices_shapes:\n      with self.subTest(indices_shape=indices_shape):\n        indices = np.random.randint(0, 7, size=indices_shape)\n        gather_nd_result = gen_array_ops.gather_nd(params, indices)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=0)\n        self.assertAllEqual(gather_nd_result, batch_gather_nd_result)\n\n  def testSameResultAsMapFn(self):\n    \"\"\"Compares results with gather_nd called on every element with map_fn.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n\n        if batch_dims > 1:\n          params = array_ops.reshape(\n              params, shape=[-1] + list(params_shape[batch_dims:]))\n          indices = array_ops.reshape(\n              indices, shape=[-1] + list(indices_shape[batch_dims:]))\n\n        map_fn_gather_nd_result = map_fn.map_fn(\n            fn=self._map_fn_body, elems=(params, indices), dtype=dtypes.float64)\n\n        if batch_dims > 1:\n          out_shape = map_fn_gather_nd_result.shape.as_list()\n          out_shape = list(params_shape[:batch_dims]) + out_shape[1:]\n          map_fn_gather_nd_result = array_ops.reshape(\n              map_fn_gather_nd_result, shape=out_shape)\n\n        self.assertAllEqual(map_fn_gather_nd_result, batch_gather_nd_result)\n\n  def _map_fn_body(self, elems):\n    return gen_array_ops.gather_nd(elems[0], elems[1])\n\n  def testBatchDimsAsTensor(self):\n    \"\"\"Tests Tensor batch_dims as input works as intended.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 0),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        batch_dims_tensor = constant_op.constant([batch_dims])\n        batch_gather_nd_tensor_batch_dims_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims_tensor)\n\n        self.assertAllEqual(batch_gather_nd_tensor_batch_dims_result,\n                            batch_gather_nd_result)\n\n  def testInvalidBatchDimsRaisesException(self):\n    \"\"\"Tests whether invalid batch_dims raise expected exceptions.\"\"\"\n    params = constant_op.constant(\n        np.random.uniform(0.0, 1.0, size=(3, 2, 2, 3, 4)))\n    indices = np.random.randint(0, 2, size=(3, 2, 3))\n\n    with self.assertRaises(TypeError):\n      array_ops.batch_gather_nd(\n          params=params,\n          indices=indices,\n          batch_dims=constant_op.constant((0, 1)))\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=-1)\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=4)\n\n  def testNoneBatchDimensions(self):\n    \"\"\"Tests gather_nd works with None dimensions.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      params_ph_shape = list(params_shape)\n      indices_ph_shape = list(indices_shape)\n      for i in range(batch_dims):\n        params_ph_shape[i] = None\n        indices_ph_shape[i] = None\n\n      @def_function.function\n      def func(params, indices):\n        return array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)  # pylint: disable=cell-var-from-loop\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec(params_ph_shape, dtypes.float32),\n          tensor_spec.TensorSpec(indices_ph_shape, dtypes.int32))\n\n      params_val = np.ones(dtype=np.float32, shape=params_shape)\n      indices_val = np.ones(dtype=np.int32, shape=indices_shape)\n      res = f(params_val, indices_val)\n      row_ndims = len(params_shape) - batch_dims - indices_shape[-1]\n      expected_out_shape = indices_shape[:-1]\n      if row_ndims > 0:\n        expected_out_shape += params_shape[-row_ndims:]\n\n      self.assertSequenceEqual(res.shape, expected_out_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RepeatTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (3, 4, None),\n      ([[1, 2], [3, 4]], 2, None),\n      ([[1, 2], [3, 4]], [1, 2], 0),\n      ([[1, 2], [3, 4]], [1, 2], 1),\n      ([[1, 2], [3, 4]], 3, 1),\n      ([[1, 2], [3, 4]], [1, 2, 3, 4], None),\n      (np.ones([0, 4]), 0, 1),\n      (np.ones([1, 2]), [2], None),\n  )\n  @test_util.with_forward_compatibility_horizons(None, [2052, 2, 7])\n  def testRepeat(self, array, repeats, axis):\n    array = np.array(array)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.int32)] * 2)\n    def repeat_fn(array, repeats):\n      return array_ops.repeat(array, repeats, axis)\n\n    v_tf = array_ops.repeat(constant_op.constant(array), repeats, axis)\n    v_tf_fn = repeat_fn(\n        constant_op.constant(array, dtype=dtypes.int32), repeats)\n    v_np = np.repeat(array, repeats, axis)\n    self.assertAllEqual(v_tf, v_np)\n    self.assertAllEqual(v_tf_fn, v_np)\n\n\nclass RepeatBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark the repeat implementation.\"\"\"\n\n  def run_and_time(self, op, iters=100, warmup_iters=10):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(warmup_iters):\n      _ = self.evaluate(op)\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / float(iters))\n\n  def make_variable(self, shape, dtype=dtypes.float32):\n    items = 1\n    for dim in shape:\n      items *= dim\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtype)\n    return var\n\n  def run_benchmark(self, shape, max_repeats, axis=None):\n    with session.Session():\n      var = self.make_variable(shape)\n      if axis is None:\n        axis_size = 1\n        for dim in shape:\n          axis_size *= dim\n      else:\n        axis_size = shape[axis]\n      repeats = constant_op.constant(\n          np.random.randint(max_repeats, size=[axis_size]), dtype=dtypes.int64)\n      repeat_op = array_ops.repeat(var, repeats, axis=axis)\n      # Return a scalar to reduce the device-to-host memcopy overhead.\n      repeat_op = repeat_op[(0,) * len(shape)]\n      self.run_and_time(repeat_op)\n\n  def benchmark_repeat_few_1d(self):\n    self.run_benchmark(shape=[1024 * 1024], max_repeats=8, axis=0)\n\n  def benchmark_repeat_many_1d(self):\n    self.run_benchmark(shape=[8 * 1024], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_few_2d_axis0(self):\n    self.run_benchmark(shape=[8, 128 * 1024], max_repeats=8, axis=0)\n\n  def benchmark_repeat_many_2d_axis0(self):\n    self.run_benchmark(shape=[8, 1024], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_many_2d_axis0_big(self):\n    self.run_benchmark(shape=[1024, 32], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_few_2d_axis1(self):\n    self.run_benchmark(shape=[8, 128 * 1024], max_repeats=8, axis=1)\n\n  def benchmark_repeat_many_2d_axis1(self):\n    self.run_benchmark(shape=[8, 1024], max_repeats=1024, axis=1)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass TileVariantTest(test_util.TensorFlowTestCase):\n\n  def test_tile_tensor_list(self):\n    t = constant_op.constant(np.random.uniform(size=[2, 3, 4]))\n    handle = list_ops.tensor_list_from_tensor(t, element_shape=None)\n    with ops.device(\"CPU:0\"):\n      tiled_handles = array_ops.tile(array_ops.reshape(handle, [1]), [2])\n    tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                [3, 4])\n    tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n    # Now mutate some of the lists and make sure the changes are not reflected\n    # in the tiled handles.\n    with ops.control_dependencies([\n        list_ops.tensor_list_scatter([t[0] + 1], [0], input_handle=handle),\n        list_ops.tensor_list_set_item(tiled_handles[0], 0, t[0] + 2)]):\n      tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                  [3, 4])\n      tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                  [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n\n\nclass StopGradientTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  def testStopGradient(self):\n    x = array_ops.zeros(3)\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientGradientTape(self):\n    x = array_ops.zeros(3)\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n  def testStopGradientGradientTapeRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n  @parameterized.named_parameters([\n      (\"TFFunction\", def_function.function),\n      (\"PythonFunction\", lambda f: f),\n  ])\n  def test_stop_gradient_resource_variable(self, decorator):\n    x = resource_variable_ops.ResourceVariable([1.0])\n    self.evaluate(x.initializer)\n\n    @decorator\n    def stop_gradient_f(x):\n      return array_ops.stop_gradient(x)\n\n    with backprop.GradientTape() as tape:\n      y = stop_gradient_f(x)\n    self.assertIsNone(tape.gradient(y, x))\n    # stop_gradient converts ResourceVariable to Tensor\n    self.assertIsInstance(y, ops.Tensor)\n    self.assertAllEqual(y, x)\n\nif __name__ == \"__main__\":\n  test_lib.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <ostream>\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/strided_slice_op.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\nusing shape_inference::UnchangedShape;\n\nnamespace {\n\nStatus GetAxisForPackAndUnpack(InferenceContext* c, int32_t rank_after_pack,\n                               int32* axis) {\n  TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", axis));\n  if (*axis < -1 * rank_after_pack || *axis >= rank_after_pack) {\n    return errors::InvalidArgument(\"Invalid axis: \", *axis, \"; must be in [\",\n                                   -1 * rank_after_pack, \",\", rank_after_pack,\n                                   \")\");\n  }\n  if (*axis < 0) *axis = (rank_after_pack + *axis);\n  return OkStatus();\n}\n\ntemplate <typename T>\nstd::vector<int64_t> AsInt64(const Tensor* tensor, int64_t num_elements) {\n  std::vector<int64_t> ret(num_elements);\n  auto data = tensor->vec<T>();\n  for (int64_t i = 0; i < num_elements; ++i) {\n    ret[i] = data(i);\n  }\n  return ret;\n}\n\ntemplate <typename T>\nStatus PadKnown(InferenceContext* c, ShapeHandle input,\n                const Tensor* paddings_t, int64_t num_dims) {\n  // paddings_t is known.\n  std::vector<DimensionHandle> dims(num_dims);\n  auto paddings_data = paddings_t->matrix<T>();\n  for (int64_t i = 0; i < num_dims; ++i) {\n    const T pad0 = paddings_data(i, 0);\n    const T pad1 = paddings_data(i, 1);\n    if (pad0 < 0 || pad1 < 0) {\n      return errors::InvalidArgument(\"Paddings must be non-negative\");\n    }\n    TF_RETURN_IF_ERROR(c->Add(c->Dim(input, i), pad0 + pad1, &dims[i]));\n  }\n  c->set_output(0, c->MakeShape(dims));\n  return OkStatus();\n}\n\nStatus PadShapeFn(InferenceContext* c) {\n  // Paddings is a matrix of [input_rank, 2].\n  ShapeHandle paddings;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 2, &paddings));\n  DimensionHandle unused;\n  TF_RETURN_IF_ERROR(c->WithValue(c->Dim(paddings, 1), 2, &unused));\n\n  // n_dim and input.rank are equivalent.\n  ShapeHandle input = c->input(0);\n  DimensionHandle n_dim = c->Dim(paddings, 0);\n  if (c->ValueKnown(n_dim)) {\n    TF_RETURN_IF_ERROR(c->WithRank(input, c->Value(n_dim), &input));\n  } else if (c->RankKnown(input)) {\n    TF_RETURN_IF_ERROR(c->WithValue(n_dim, c->Rank(input), &n_dim));\n  }\n\n  const Tensor* paddings_t = c->input_tensor(1);\n\n  // paddings_t is unknown\n  if (paddings_t == nullptr) {\n    if (c->ValueKnown(n_dim)) {\n      // Make output with n_dim unknown dims.\n      c->set_output(0, c->UnknownShapeOfRank(c->Value(n_dim)));\n    } else {\n      c->set_output(0, c->UnknownShape());\n    }\n    return OkStatus();\n  }\n\n  const int64_t num_dims = paddings_t->shape().dim_size(0);\n  TF_RETURN_IF_ERROR(c->WithRank(input, num_dims, &input));\n  TF_RETURN_IF_ERROR(c->WithValue(n_dim, num_dims, &n_dim));\n\n  if (paddings_t->dtype() == DT_INT32) {\n    return PadKnown<int32>(c, input, paddings_t, num_dims);\n  } else {\n    return PadKnown<int64_t>(c, input, paddings_t, num_dims);\n  }\n}\n\nStatus TransposeShapeFn(InferenceContext* c) {\n  ShapeHandle input = c->input(0);\n  ShapeHandle perm_shape = c->input(1);\n  const Tensor* perm = c->input_tensor(1);\n  DimensionHandle perm_elems = c->NumElements(perm_shape);\n  // If we don't have rank information on the input or value information on\n  // perm we can't return any shape information, otherwise we have enough\n  // information to at least find the rank of the output.\n  if (!c->RankKnown(input) && !c->ValueKnown(perm_elems) && perm == nullptr) {\n    c->set_output(0, c->UnknownShape());\n    return OkStatus();\n  }\n\n  // Find our value of the rank.\n  int64_t rank;\n  if (c->RankKnown(input)) {\n    rank = c->Rank(input);\n  } else if (c->ValueKnown(perm_elems)) {\n    rank = c->Value(perm_elems);\n  } else {\n    rank = perm->NumElements();\n  }\n  if (!c->RankKnown(input) && rank < 2) {\n    // A permutation array containing a single element is ambiguous. It could\n    // indicate either a scalar or a 1-dimensional array, both of which the\n    // transpose op returns unchanged.\n    c->set_output(0, input);\n    return OkStatus();\n  }\n\n  std::vector<DimensionHandle> dims;\n  dims.resize(rank);\n  TF_RETURN_IF_ERROR(c->WithRank(input, rank, &input));\n  // Ensure that perm is a vector and has rank elements.\n  TF_RETURN_IF_ERROR(c->WithRank(perm_shape, 1, &perm_shape));\n  TF_RETURN_IF_ERROR(c->WithValue(perm_elems, rank, &perm_elems));\n\n  // If we know the rank of the input and the value of perm, we can return\n  // all shape information, otherwise we can only return rank information,\n  // but no information for the dimensions.\n  if (perm != nullptr) {\n    std::vector<int64_t> data;\n    if (perm->dtype() == DT_INT32) {\n      data = AsInt64<int32>(perm, rank);\n    } else {\n      data = AsInt64<int64_t>(perm, rank);\n    }\n\n    for (int32_t i = 0; i < rank; ++i) {\n      int64_t in_idx = data[i];\n      if (in_idx >= rank || in_idx <= -rank) {\n        return errors::InvalidArgument(\"perm dim \", in_idx,\n                                       \" is out of range of input rank \", rank);\n      }\n      dims[i] = c->Dim(input, in_idx);\n    }\n  } else {\n    for (int i = 0; i < rank; ++i) {\n      dims[i] = c->UnknownDim();\n    }\n  }\n\n  c->set_output(0, c->MakeShape(dims));\n  return OkStatus();\n}\n\nStatus SetOutputShapeForReshape(InferenceContext* c) {\n  ShapeHandle in = c->input(0);\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &out));\n\n  if (!c->RankKnown(out)) {\n    // We have no information about the shape of the output.\n    c->set_output(0, out);\n    return OkStatus();\n  }\n  if (c->RankKnown(in)) {\n    // We don't know the number of output elements, but we can try to infer\n    // the missing dimension.\n    bool too_many_unknown = false;\n    int32_t out_unknown_idx = -1;\n\n    DimensionHandle known_out_elems = c->NumElements(out);\n    if (!c->ValueKnown(known_out_elems)) {\n      known_out_elems = c->MakeDim(1);\n      for (int32_t i = 0; i < c->Rank(out); ++i) {\n        DimensionHandle dim = c->Dim(out, i);\n        if (!c->ValueKnown(dim)) {\n          if (out_unknown_idx >= 0) {\n            too_many_unknown = true;\n            break;\n          }\n          out_unknown_idx = i;\n        } else {\n          TF_RETURN_IF_ERROR(\n              c->Multiply(known_out_elems, dim, &known_out_elems));\n        }\n      }\n    }\n    int32_t in_unknown_idx = -1;\n    DimensionHandle known_in_elems = c->NumElements(in);\n    if (!c->ValueKnown(known_in_elems)) {\n      known_in_elems = c->MakeDim(1);\n      for (int32_t i = 0; i < c->Rank(in); ++i) {\n        DimensionHandle dim = c->Dim(in, i);\n        if (!c->ValueKnown(dim)) {\n          if (in_unknown_idx >= 0) {\n            too_many_unknown = true;\n            break;\n          }\n          in_unknown_idx = i;\n        } else {\n          TF_RETURN_IF_ERROR(c->Multiply(known_in_elems, dim, &known_in_elems));\n        }\n      }\n    }\n\n    if (!too_many_unknown) {\n      if (in_unknown_idx < 0 && out_unknown_idx < 0) {\n        // Just check that the dimensions match.\n        if (c->Value(known_in_elems) != c->Value(known_out_elems)) {\n          return errors::InvalidArgument(\n              \"Cannot reshape a tensor with \", c->DebugString(known_in_elems),\n              \" elements to shape \", c->DebugString(out), \" (\",\n              c->DebugString(known_out_elems), \" elements)\");\n        }\n      } else if (in_unknown_idx < 0 && out_unknown_idx >= 0 &&\n                 c->Value(known_out_elems) > 0) {\n        // Input fully known, infer the one missing output dim\n        DimensionHandle inferred_dim;\n        TF_RETURN_IF_ERROR(c->Divide(known_in_elems, c->Value(known_out_elems),\n                                     true /* evenly_divisible */,\n                                     &inferred_dim));\n        TF_RETURN_IF_ERROR(\n            c->ReplaceDim(out, out_unknown_idx, inferred_dim, &out));\n\n      } else if (in_unknown_idx >= 0 && out_unknown_idx < 0 &&\n                 c->Value(known_in_elems) != 0) {\n        // Output fully known, infer the one missing input dim\n        DimensionHandle inferred_dim;\n        TF_RETURN_IF_ERROR(c->Divide(known_out_elems, c->Value(known_in_elems),\n                                     true /* evenly_divisible */,\n                                     &inferred_dim));\n        DimensionHandle unknown_in_dim = c->Dim(in, in_unknown_idx);\n        TF_RETURN_IF_ERROR(\n            c->Merge(unknown_in_dim, inferred_dim, &unknown_in_dim));\n      } else if (in_unknown_idx >= 0 && out_unknown_idx >= 0) {\n        // Exactly one unknown dimension in both input and output. These 2 are\n        // equal iff the known elements are equal.\n        if (c->Value(known_in_elems) == c->Value(known_out_elems)) {\n          DimensionHandle unknown_in_dim = c->Dim(in, in_unknown_idx);\n          TF_RETURN_IF_ERROR(\n              c->ReplaceDim(out, out_unknown_idx, unknown_in_dim, &out));\n        }\n      }\n    }\n  }\n  c->set_output(0, out);\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"ParallelConcat\")\n    .Input(\"values: N * T\")\n    .Output(\"output: T\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: type\")\n    .Attr(\"shape: shape\")\n    .SetShapeFn([](InferenceContext* c) {\n      // Validate that the shape attr is correct.\n      PartialTensorShape shape;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shape\", &shape));\n      ShapeHandle passed_shape;\n      TF_RETURN_IF_ERROR(\n          c->MakeShapeFromPartialTensorShape(shape, &passed_shape));\n      if (!c->FullyDefined(passed_shape)) {\n        return errors::InvalidArgument(\"shape attr must be fully defined.\");\n      }\n      ShapeHandle cur;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(\n          passed_shape, 0, c->MakeDim(shape_inference::DimensionOrConstant(1)),\n          &cur));\n      for (int i = 0; i < c->num_inputs(); ++i) {\n        if (!c->FullyDefined(c->input(i))) {\n          return errors::InvalidArgument(\n              \"All input shapes must be fully defined.\");\n        }\n        DimensionHandle unused;\n        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {\n          return errors::InvalidArgument(\"Size of first dimension must be 1.\");\n        }\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\n                                        \"From merging shape \", i,\n                                        \" with other shapes.\");\n      }\n\n      c->set_output(0, passed_shape);\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Pack\")\n    .Input(\"values: N * T\")\n    .Output(\"output: T\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: type\")\n    .Attr(\"axis: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      // Validate shapes of all inputs are compatible\n      ShapeHandle cur = c->input(c->num_inputs() - 1);\n      for (int i = c->num_inputs() - 2; i >= 0; --i) {\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\n                                        \"From merging shape \", i,\n                                        \" with other shapes.\");\n      }\n      if (!c->RankKnown(cur)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n      // Determine the axis that will be added, converting from negative\n      // axes to a positive point per negative indexing rules.\n      int32_t rank = c->Rank(cur);\n      int32_t axis;\n      TF_RETURN_IF_ERROR(GetAxisForPackAndUnpack(c, rank + 1, &axis));\n\n      // Copy all dimensions over, inserting a dimension of value #inputs\n      // at <axis>.\n      std::vector<DimensionHandle> dims;\n      int index = 0;\n      while (index < axis) dims.push_back(c->Dim(cur, index++));\n      dims.push_back(c->MakeDim(c->num_inputs()));\n      while (index < rank) dims.push_back(c->Dim(cur, index++));\n\n      c->set_output(0, c->MakeShape(dims));\n      for (int i = 0; i < c->num_inputs(); ++i) {\n        auto* shape_and_type = c->input_handle_shapes_and_types(i);\n        if (shape_and_type) {\n          if (!c->RelaxOutputHandleShapesAndMergeTypes(0, *shape_and_type)) {\n            c->set_output_handle_shapes_and_types(\n                0, std::vector<shape_inference::ShapeAndType>({}));\n            break;\n          }\n        }\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"DeepCopy\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetIsStateful()\n    .SetShapeFn(UnchangedShape);\n\nREGISTER_OP(\"InplaceUpdate\")\n    .Input(\"x: T\")\n    .Input(\"i: int32\")\n    .Input(\"v: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(UnchangedShape);\n\nREGISTER_OP(\"InplaceAdd\")\n    .Input(\"x: T\")\n    .Input(\"i: int32\")\n    .Input(\"v: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(UnchangedShape);\n\nREGISTER_OP(\"InplaceSub\")\n    .Input(\"x: T\")\n    .Input(\"i: int32\")\n    .Input(\"v: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(UnchangedShape);\n\nREGISTER_OP(\"Empty\")\n    .Input(\"shape: int32\")\n    .Output(\"output: dtype\")\n    .Attr(\"dtype: type\")\n    .Attr(\"init: bool = false\")\n    .SetDoNotOptimize()\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Unpack\")\n    .Input(\"value: T\")\n    .Output(\"output: num * T\")\n    .Attr(\"num: int >= 0\")\n    .Attr(\"T: type\")\n    .Attr(\"axis: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s = c->input(0);\n      ShapeHandle out;\n      if (c->RankKnown(s)) {\n        // Determine the axis that will be removed, converting from negative\n        // axes to a positive point per negative indexing rules.\n        int32_t rank = c->Rank(s);\n        int32_t axis;\n        TF_RETURN_IF_ERROR(GetAxisForPackAndUnpack(c, rank, &axis));\n\n        // The axis dim matches the number of outputs.\n        DimensionHandle unused;\n        TF_RETURN_IF_ERROR(\n            c->WithValue(c->Dim(s, axis), c->num_outputs(), &unused));\n\n        // Copy all dimensions, removing the <axis> dimension.\n        std::vector<DimensionHandle> dims;\n        for (int i = 0; i < rank; ++i) {\n          if (i != axis) dims.push_back(c->Dim(s, i));\n        }\n        out = c->MakeShape(dims);\n      } else {\n        // All outputs are the same shape, but it's not known.\n        out = c->UnknownShape();\n      }\n      for (int i = 0; i < c->num_outputs(); ++i) c->set_output(i, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"UnravelIndex\")\n    .Input(\"indices: Tidx\")\n    .Input(\"dims: Tidx\")\n    .Output(\"output: Tidx\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle indices = c->input(0);\n      ShapeHandle dims;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &dims));\n      if (c->RankKnown(indices) && c->Rank(indices) == 0) {\n        c->set_output(0, c->Vector(c->Dim(dims, 0)));\n      } else if (c->RankKnown(indices)) {\n        c->set_output(0, c->Matrix(c->Dim(dims, 0), c->NumElements(indices)));\n      } else {\n        c->set_output(0, c->UnknownShape());\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"BroadcastTo\")\n    .Input(\"input: T\")\n    .Input(\"shape: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle shape_in = c->input(1);\n      TF_RETURN_IF_ERROR(c->WithRank(shape_in, 1, &shape_in));\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &out));\n      if (!c->RankKnown(out)) {\n        // We have no information about the shape of the output.\n        c->set_output(0, out);\n        return OkStatus();\n      }\n\n      ShapeHandle in = c->input(0);\n      if (!c->RankKnown(in)) {\n        // We have no information about the shape of the input,\n        // nothing to do here.\n        c->set_output(0, out);\n        return OkStatus();\n      }\n      int out_rank = c->Rank(out);\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(in, out_rank, &in));\n      int in_rank = c->Rank(in);\n      for (int i = 0; i < in_rank; ++i) {\n        auto in_dim = c->Dim(in, in_rank - i - 1);\n        if (c->Value(in_dim) > 1) {\n          // If the input dimension is greater than 1 then the output dimension\n          // must be equal to it, since we only broadcast \"from left to right\".\n          auto out_dim = c->Dim(out, out_rank - i - 1);\n          TF_RETURN_IF_ERROR(c->Merge(in_dim, out_dim, &out_dim));\n          TF_RETURN_IF_ERROR(\n              c->ReplaceDim(out, out_rank - i - 1, out_dim, &out));\n        }\n      }\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n// TODO(josh11b): Remove the >= 2 constraint, once we can rewrite the graph\n// in the N == 1 case to remove the node.\nREGISTER_OP(\"Concat\")\n    .Input(\"concat_dim: int32\")\n    .Input(\"values: N * T\")\n    .Output(\"output: T\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::ConcatShape(c, c->num_inputs() - 1);\n    });\n\nREGISTER_OP(\"ConcatV2\")\n    .Input(\"values: N * T\")\n    .Input(\"axis: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ConcatV2Shape);\n\n// TODO(vivek.v.rane@intel.com): Prefix the op names with underscore if the ops\n// are not to be made user-accessible.\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklConcatV2\")\n    .Input(\"values: N * T\")\n    .Input(\"axis: Tidx\")\n    .Input(\"mkl_values: N * uint8\")\n    .Input(\"mkl_axis: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ConcatV2Shape)\n    .Doc(R\"doc(\nMKL version of ConcatV2 operator. Uses MKL DNN APIs to perform concatenation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\nREGISTER_OP(\"ConcatOffset\")\n    .Input(\"concat_dim: int32\")\n    .Input(\"shape: N * int32\")\n    .Output(\"offset: N * int32\")\n    .Attr(\"N: int >= 2\")\n    .SetShapeFn([](InferenceContext* c) {\n      for (int i = 1; i < c->num_inputs(); ++i) {\n        c->set_output(i - 1, c->input(i));\n      }\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Split\")\n    .Input(\"split_dim: int32\")\n    .Input(\"value: T\")\n    .Output(\"output: num_split * T\")\n    .Attr(\"num_split: int >= 1\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      DimensionHandle split_dimension;\n      ShapeHandle input = c->input(1);\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInputWithNegativeIndexing(\n          0, c->Rank(input), &split_dimension));\n      int num_split = c->num_outputs();\n      ShapeHandle out;\n      if (!c->ValueKnown(split_dimension)) {\n        if (c->RankKnown(input)) {\n          out = c->UnknownShapeOfRank(c->Rank(input));\n        } else {\n          out = c->UnknownShape();\n        }\n      } else {\n        int64_t split_dim = c->Value(split_dimension);\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(input, split_dim + 1, &input));\n        DimensionHandle split_dim_size;\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(\n            c->Divide(c->Dim(input, split_dim), num_split,\n                      true /* evenly_divisible */, &split_dim_size),\n            \"Number of ways to split should evenly divide the split dimension\");\n        TF_RETURN_IF_ERROR(\n            c->ReplaceDim(input, split_dim, split_dim_size, &out));\n      }\n      for (int i = 0; i < num_split; ++i) c->set_output(i, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"SplitV\")\n    .Input(\"value: T\")\n    .Input(\"size_splits: Tlen\")\n    .Input(\"split_dim: int32\")\n    .Output(\"output: num_split * T\")\n    .Attr(\"num_split: int >= 1\")\n    .Attr(\"T: type\")\n    .Attr(\"Tlen: {int8, int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      DimensionHandle split_dimension;\n      ShapeHandle input = c->input(0);\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInputWithNegativeIndexing(\n          2, c->Rank(input), &split_dimension));\n      int32_t num_outputs = c->num_outputs();\n      int32_t rank = c->Rank(input);\n      ShapeHandle output_shape;\n      const Tensor* size_splits = c->input_tensor(1);\n      if (rank == InferenceContext::kUnknownRank) {\n        // If the rank of input tensor is unknown, then return unknown shapes.\n        // Note that the shape of each output can be different.\n        for (int i = 0; i < num_outputs; ++i) {\n          c->set_output(i, c->UnknownShape());\n        }\n      } else if (rank == 0) {\n        // Throw error if input is a scalar.\n        return errors::InvalidArgument(\"Can't split scalars\");\n      } else if (size_splits == nullptr && c->ValueKnown(split_dimension)) {\n        // If split dimension is known, but the sizes are unknown, then\n        // only the split dimension is unknown\n        output_shape = input;\n        for (int i = 0; i < num_outputs; ++i) {\n          TF_RETURN_IF_ERROR(c->ReplaceDim(output_shape,\n                                           c->Value(split_dimension),\n                                           c->UnknownDim(), &output_shape));\n          c->set_output(i, output_shape);\n        }\n      } else if (size_splits == nullptr && !c->ValueKnown(split_dimension)) {\n        // If split dimension or tensor containing the split sizes is unknown,\n        // then return unknown shapes of same rank as input. Note that each\n        // output shape can be different since splitv doesn't always split\n        // tensors evenly.\n        for (int i = 0; i < num_outputs; ++i) {\n          c->set_output(i, c->UnknownShapeOfRank(rank));\n        }\n      } else {\n        // Determine the output shape if split dimension and split sizes are\n        // known.\n        int64_t split_dim = c->Value(split_dimension);\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(input, split_dim + 1, &input));\n        std::vector<int64_t> data;\n        if (size_splits->dtype() == DT_INT32) {\n          data = AsInt64<int32>(size_splits, size_splits->shape().dim_size(0));\n        } else {\n          data =\n              AsInt64<int64_t>(size_splits, size_splits->shape().dim_size(0));\n        }\n        if (num_outputs != data.size()) {\n          return errors::InvalidArgument(\n              \"Length of size_splits should be equal to num_outputs\");\n        }\n        int64_t total_size = 0;\n        bool has_neg_one = false;\n        for (const auto size : data) {\n          if (size == -1) {\n            if (has_neg_one) {\n              return errors::InvalidArgument(\n                  \"size_splits can only have one -1\");\n            }\n            has_neg_one = true;\n          } else {\n            total_size += size;\n          }\n        }\n        auto split_dim_size = c->Value(c->Dim(input, split_dim));\n        // If the sizes of the splits are known, then\n        // make sure that the sizes add up to the expected\n        // dimension size, with the possibility of a -1.\n        // Specify the full output shapes.\n        for (int i = 0; i < num_outputs; ++i) {\n          auto size = data[i];\n          if (data[i] == -1 && c->ValueKnown(split_dim_size)) {\n            size = split_dim_size - total_size;\n          }\n          // If we have a negative known size (either explicit, or computed\n          // via -1), then the split sizes are invalid.\n          if (size < -1 || (size == -1 && c->ValueKnown(split_dim_size))) {\n            return errors::InvalidArgument(\"Split size at index \", i,\n                                           \" must be >= 0. Got: \", size);\n          }\n          TF_RETURN_IF_ERROR(\n              c->ReplaceDim(input, split_dim, c->MakeDim(size), &output_shape));\n          c->set_output(i, output_shape);\n        }\n        if (c->ValueKnown(split_dim_size)) {\n          if (has_neg_one ? total_size > split_dim_size\n                          : total_size != split_dim_size) {\n            return errors::InvalidArgument(\n                \"can't split axis of size \", split_dim_size,\n                \" into pieces of size [\", absl::StrJoin(data, \",\"), \"]\");\n          }\n        }\n      }\n\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Const\")\n    .Output(\"output: dtype\")\n    .Attr(\"value: tensor\")\n    .Attr(\"dtype: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      const TensorProto* proto = nullptr;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"value\", &proto));\n      TF_RETURN_IF_ERROR(TensorShape::IsValidShape(proto->tensor_shape()));\n      TensorShape shape(proto->tensor_shape());\n      std::vector<DimensionHandle> dims;\n      dims.reserve(shape.dims());\n      for (int i = 0; i < shape.dims(); ++i) {\n        dims.push_back(c->MakeDim(shape.dim_size(i)));\n      }\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\n// Returns a constant tensor on the host.  Useful for writing C++ tests\n// and benchmarks which run on GPU but require arguments pinned to the host.\n// Used by test::graph::HostConstant.\n// value: Attr `value` is the tensor to return.\nREGISTER_OP(\"HostConst\")\n    .Output(\"output: dtype\")\n    .Attr(\"value: tensor\")\n    .Attr(\"dtype: type\")\n    .SetShapeFn(shape_inference::UnknownShape);\n\n// Used executing op-by-op to copy constants to the current device without\n// serializing tensors as TensorProtos, after a host tensor has been\n// created. Same behavior as Identity, but no gradient and potentially relaxed\n// copy semantics.\nREGISTER_OP(\"_EagerConst\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\n// TODO(mgubin): Update the doc when the freeze_graph script supports converting\n// into memmapped format.\nREGISTER_OP(\"ImmutableConst\")\n    .Attr(\"dtype: type\")\n    .Attr(\"shape: shape\")\n    .Attr(\"memory_region_name: string\")\n    .Output(\"tensor: dtype\")\n    .SetShapeFn(shape_inference::ExplicitShape);\n\nREGISTER_OP(\"GuaranteeConst\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      return UnchangedShape(c);\n    })\n    // We don't want this to be optimized away.\n    .SetDoNotOptimize();\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ZerosLike\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"OnesLike\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, uint8, int16, uint16, int32, \"\n        \"uint32, int64, uint64, complex64, complex128, bool}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Diag\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int32, int64, complex64, \"\n        \"complex128}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle in = c->input(0);\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(in, 1, &in));\n      // Output shape is original concatenated with itself.\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->Concatenate(in, in, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"DiagPart\")\n    .Input(\"input: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int32, int64, complex64, \"\n        \"complex128}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle in = c->input(0);\n      if (!c->RankKnown(in)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n      // Rank must be even, and result will have rank <rank/2>.\n      const int32_t rank = c->Rank(in);\n      if ((rank % 2) != 0 || rank <= 0) {\n        return errors::InvalidArgument(\n            \"Input must have even and non-zero rank, input rank is \", rank);\n      }\n      const int32_t mid = rank / 2;\n\n      // output dim[i] is the merge of in.dim[i] and in.dim[i+mid].\n      std::vector<DimensionHandle> dims(mid);\n      for (int i = 0; i < mid; ++i) {\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(in, i), c->Dim(in, i + mid), &dims[i]));\n      }\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MatrixDiag\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle in;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &in));\n      if (!c->RankKnown(in)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n      const int32_t rank = c->Rank(in);\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(\n          c->Concatenate(in, c->Vector(c->Dim(in, rank - 1)), &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MatrixDiagV2\")\n    .Input(\"diagonal: T\")\n    .Input(\"k: int32\")\n    .Input(\"num_rows: int32\")\n    .Input(\"num_cols: int32\")\n    .Input(\"padding_value: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::MatrixDiagV2Shape);\n\nREGISTER_OP(\"MatrixDiagV3\")\n    .Input(\"diagonal: T\")\n    .Input(\"k: int32\")\n    .Input(\"num_rows: int32\")\n    .Input(\"num_cols: int32\")\n    .Input(\"padding_value: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\n        \"align: {'LEFT_RIGHT', 'RIGHT_LEFT', 'LEFT_LEFT', 'RIGHT_RIGHT'} = \"\n        \"'RIGHT_LEFT'\")\n    .SetShapeFn(shape_inference::MatrixDiagV2Shape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MatrixSetDiag\")\n    .Input(\"input: T\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      ShapeHandle diag;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 2, &input));\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &diag));\n      if (c->RankKnown(input)) {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(1), c->Rank(input) - 1, &diag));\n      }\n      DimensionHandle smallest_dim;\n      TF_RETURN_IF_ERROR(\n          c->Min(c->Dim(input, -2), c->Dim(input, -1), &smallest_dim));\n      TF_RETURN_IF_ERROR(\n          c->Merge(smallest_dim, c->Dim(diag, -1), &smallest_dim));\n\n      ShapeHandle output = input;\n      if (c->RankKnown(diag) && !c->FullyDefined(input)) {\n        // Try to infer parts of shape from diag.\n        ShapeHandle diag_batch_shape;\n        TF_RETURN_IF_ERROR(c->Subshape(diag, 0, -1, &diag_batch_shape));\n        TF_RETURN_IF_ERROR(\n            c->Concatenate(diag_batch_shape, c->UnknownShapeOfRank(2), &diag));\n        TF_RETURN_IF_ERROR(c->Merge(input, diag, &output));\n      }\n      c->set_output(0, output);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MatrixSetDiagV2\")\n    .Input(\"input: T\")\n    .Input(\"diagonal: T\")\n    .Input(\"k: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::MatrixSetDiagV2Shape);\n\nREGISTER_OP(\"MatrixSetDiagV3\")\n    .Input(\"input: T\")\n    .Input(\"diagonal: T\")\n    .Input(\"k: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\n        \"align: {'LEFT_RIGHT', 'RIGHT_LEFT', 'LEFT_LEFT', 'RIGHT_RIGHT'} = \"\n        \"'RIGHT_LEFT'\")\n    .SetShapeFn(shape_inference::MatrixSetDiagV2Shape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MatrixDiagPart\")\n    .Input(\"input: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle in;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 2, &in));\n      if (!c->RankKnown(in)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n      const int32_t rank = c->Rank(in);\n      std::vector<DimensionHandle> dims;\n      dims.reserve(rank - 2);\n      for (int i = 0; i < rank - 2; ++i) dims.push_back(c->Dim(in, i));\n\n      DimensionHandle min_dim;\n      TF_RETURN_IF_ERROR(\n          c->Min(c->Dim(in, rank - 2), c->Dim(in, rank - 1), &min_dim));\n      dims.push_back(min_dim);\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MatrixDiagPartV2\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Input(\"padding_value: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::MatrixDiagPartV2Shape);\n\nREGISTER_OP(\"MatrixDiagPartV3\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Input(\"padding_value: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\"T: type\")\n    .Attr(\n        \"align: {'LEFT_RIGHT', 'RIGHT_LEFT', 'LEFT_LEFT', 'RIGHT_RIGHT'} = \"\n        \"'RIGHT_LEFT'\")\n    .SetShapeFn(shape_inference::MatrixDiagPartV2Shape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MatrixBandPart\")\n    .Input(\"input: T\")\n    .Input(\"num_lower: Tindex\")\n    .Input(\"num_upper: Tindex\")\n    .Output(\"band: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindex: {int32, int64} = DT_INT64\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Reverse\")\n    .Input(\"tensor: T\")\n    .Input(\"dims: bool\")\n    .Output(\"output: T\")\n    .Attr(\n        \"T: {uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, \"\n        \"bfloat16, half, float, double, complex64, complex128, string}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle dims;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &dims));\n      DimensionHandle dims_dim = c->Dim(dims, 0);\n      if (c->ValueKnown(dims_dim)) {\n        TF_RETURN_IF_ERROR(c->WithRank(input, c->Value(dims_dim), &input));\n      }\n      if (c->Rank(input) > 8) {\n        return errors::InvalidArgument(\n            \"reverse does not work on tensors with more than 8 dimensions\");\n      }\n      c->set_output(0, input);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ReverseV2\")\n    .Input(\"tensor: T\")\n    .Input(\"axis: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\n        \"T: {uint8, int8, uint16, int16, int32, uint32, int64, uint64, bool, \"\n        \"bfloat16, half, float, double, complex64, complex128, string}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle axis;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &axis));\n      if (c->Rank(input) > 8) {\n        return errors::InvalidArgument(\n            \"reverse does not work on tensors with more than 8 dimensions\");\n      }\n      const Tensor* axis_tensor = c->input_tensor(1);\n      if (axis_tensor != nullptr && c->RankKnown(input)) {\n        int32_t rank = c->Rank(input);\n        std::vector<int64_t> axis_value;\n        if (axis_tensor->dtype() == DT_INT32) {\n          axis_value = AsInt64<int32>(axis_tensor, axis_tensor->NumElements());\n        } else {\n          axis_value =\n              AsInt64<int64_t>(axis_tensor, axis_tensor->NumElements());\n        }\n        std::vector<bool> axes_dense(c->Rank(input), false);\n        for (int i = 0; i < axis_value.size(); i++) {\n          int64_t canonical_axis =\n              axis_value[i] < 0 ? rank + axis_value[i] : axis_value[i];\n          if (canonical_axis < 0 || canonical_axis >= rank) {\n            return errors::InvalidArgument(\"'axis'[\", i, \"] = \", axis_value[i],\n                                           \" is out of valid range [\", 0, \", \",\n                                           rank - 1);\n          }\n          if (axes_dense[canonical_axis]) {\n            return errors::InvalidArgument(\"axis \", canonical_axis,\n                                           \" specified more than once.\");\n          }\n          axes_dense[canonical_axis] = true;\n        }\n      }\n      c->set_output(0, input);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"EditDistance\")\n    .Input(\"hypothesis_indices: int64\")\n    .Input(\"hypothesis_values: T\")\n    .Input(\"hypothesis_shape: int64\")\n    .Input(\"truth_indices: int64\")\n    .Input(\"truth_values: T\")\n    .Input(\"truth_shape: int64\")\n    .Attr(\"normalize: bool = true\")\n    .Attr(\"T: type\")\n    .Output(\"output: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::ValidateSparseTensor(\n          c, c->input(0), c->input(1), c->input(2)));\n      TF_RETURN_IF_ERROR(shape_inference::ValidateSparseTensor(\n          c, c->input(3), c->input(4), c->input(5)));\n      const Tensor* hypothesis_shape_t = c->input_tensor(2);\n      const Tensor* truth_shape_t = c->input_tensor(5);\n      if (hypothesis_shape_t == nullptr || truth_shape_t == nullptr) {\n        // We need to know the runtime shape of the two tensors,\n        // or else the output shape is unknown.\n        return shape_inference::UnknownShape(c);\n      }\n\n      if (hypothesis_shape_t->NumElements() != truth_shape_t->NumElements()) {\n        return errors::InvalidArgument(\n            \"Num elements of hypothesis_shape does not match truth_shape: \",\n            hypothesis_shape_t->NumElements(), \" vs. \",\n            truth_shape_t->NumElements());\n      }\n\n      auto h_values = hypothesis_shape_t->flat<int64_t>();\n      auto t_values = truth_shape_t->flat<int64_t>();\n      std::vector<DimensionHandle> dims(hypothesis_shape_t->NumElements() - 1);\n      for (int i = 0; i < dims.size(); ++i) {\n        dims[i] = c->MakeDim(std::max(h_values(i), t_values(i)));\n      }\n\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Fill\")\n    .Input(\"dims: index_type\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"index_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      DataType index_type = DT_INT32;\n      Status s = c->GetAttr(\"index_type\", &index_type);\n      if (!s.ok() && s.code() != error::NOT_FOUND) {\n        return s;\n      }\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n\n      const Tensor* t = c->input_tensor(0);\n      if (t != nullptr) {\n        for (int i = 0; i < t->NumElements(); ++i) {\n          if ((index_type == DT_INT32 && t->vec<int32>()(i) < 0) ||\n              (index_type == DT_INT64 && t->vec<int64_t>()(i) < 0)) {\n            return errors::InvalidArgument(\"Fill dimensions must be >= 0\");\n          }\n        }\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n      c->set_output(0, out);\n\n      auto* shape_and_type = c->input_handle_shapes_and_types(1);\n      if (shape_and_type) {\n        c->set_output_handle_shapes_and_types(0, *shape_and_type);\n      }\n\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"_ParallelConcatStart\")\n    .Output(\"output: dtype\")\n    .Attr(\"shape: shape\")\n    .Attr(\"dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::ExplicitShape)\n    .Doc(R\"doc(\nCreates an empty Tensor with shape `shape` and type `dtype`.\n\nThe memory can optionally be initialized. This is usually useful in\nconjunction with inplace operations.\n\nshape: 1-D `Tensor` indicating the shape of the output.\ndtype: The element type of the returned tensor.\noutput: An empty Tensor of the specified type.\n)doc\");\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"_ParallelConcatUpdate\")\n    .Input(\"value: T\")\n    .Input(\"update: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"loc: int\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nUpdates input `value` at `loc` with `update`.\n\nIf you use this function you will almost certainly want to add\na control dependency as done in the implementation of parallel_stack to\navoid race conditions.\n\nvalue: A `Tensor` object that will be updated in-place.\nloc: A scalar indicating the index of the first dimension such that\n         value[loc, :] is updated.\nupdate: A `Tensor` of rank one less than `value` if `loc` is a scalar,\n        otherwise of rank equal to `value` that contains the new values\n        for `value`.\noutput: `value` that has been updated accordingly.\n)doc\");\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Gather\")\n    .Input(\"params: Tparams\")\n    .Input(\"indices: Tindices\")\n    .Attr(\"validate_indices: bool = true\")\n    .Output(\"output: Tparams\")\n    .Attr(\"Tparams: type\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n      ShapeHandle params_subshape;\n      TF_RETURN_IF_ERROR(c->Subshape(c->input(0), 1, &params_subshape));\n      ShapeHandle indices_shape = c->input(1);\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->Concatenate(indices_shape, params_subshape, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"GatherV2\")\n    .Input(\"params: Tparams\")\n    .Input(\"indices: Tindices\")\n    .Input(\"axis: Taxis\")\n    .Attr(\"batch_dims: int = 0\")\n    .Output(\"output: Tparams\")\n    .Attr(\"Tparams: type\")\n    .Attr(\"Tindices: {int16, int32,int64}\")\n    .Attr(\"Taxis: {int32,int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle params_shape;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &params_shape));\n\n      ShapeHandle indices_shape = c->input(1);\n      ShapeHandle unused_axis_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused_axis_shape));\n      const Tensor* axis_t = c->input_tensor(2);\n\n      // If axis is unknown, we can only infer that the result is params_rank +\n      // indices_rank - 1.\n      if (axis_t == nullptr) {\n        if (c->RankKnown(params_shape) && c->RankKnown(indices_shape)) {\n          int32_t batch_dims;\n          TF_RETURN_IF_ERROR(c->GetAttr(\"batch_dims\", &batch_dims));\n          c->set_output(0, c->UnknownShapeOfRank(c->Rank(params_shape) +\n                                                 c->Rank(indices_shape) - 1 -\n                                                 batch_dims));\n        } else {\n          c->set_output(0, c->UnknownShape());\n        }\n        return OkStatus();\n      }\n\n      // Note, axis can be negative.\n      int64_t axis = 0;\n      if (axis_t->dtype() == DT_INT32) {\n        axis = axis_t->scalar<int32>()();\n      } else {\n        axis = axis_t->scalar<int64_t>()();\n      }\n\n      // Check that params has rank of at least axis + 1.\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(\n          params_shape, axis < 0 ? -axis : axis + 1, &unused));\n\n      // Note, batch_dims can be negative.\n      int32_t batch_dims;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"batch_dims\", &batch_dims));\n      // -rank(indices) <= batch_dims <= rank(indices)\n      TF_RETURN_IF_ERROR(\n          c->WithRankAtLeast(indices_shape, std::abs(batch_dims), &unused));\n      if (batch_dims < 0) {\n        batch_dims += c->Rank(indices_shape);\n      }\n      // rank(params) > batch_dims\n      TF_RETURN_IF_ERROR(\n          c->WithRankAtLeast(params_shape, batch_dims + 1, &unused));\n\n      ShapeHandle params_outer_subshape;\n      TF_RETURN_IF_ERROR(\n          c->Subshape(params_shape, 0, axis, &params_outer_subshape));\n\n      ShapeHandle indices_inner_subshape;\n      TF_RETURN_IF_ERROR(\n          c->Subshape(indices_shape, batch_dims, &indices_inner_subshape));\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(\n          c->Concatenate(params_outer_subshape, indices_inner_subshape, &out));\n\n      // Slice from axis + 1 to the end of params_shape to collect the inner\n      // dimensions of the result. Special case -1 here since -1 + 1 wraps, and\n      // we slice from 0 to the end of shape. Subshape() handles all other\n      // out-of-bounds checking.\n      if (axis != -1) {\n        ShapeHandle params_inner_subshape;\n        TF_RETURN_IF_ERROR(\n            c->Subshape(params_shape, axis + 1, &params_inner_subshape));\n        TF_RETURN_IF_ERROR(c->Concatenate(out, params_inner_subshape, &out));\n      }\n\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"GatherNd\")\n    .Input(\"params: Tparams\")\n    .Input(\"indices: Tindices\")\n    .Output(\"output: Tparams\")\n    .Attr(\"Tparams: type\")\n    .Attr(\"Tindices: {int16, int32,int64}\")\n    .SetShapeFn(shape_inference::GatherNdShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Identity\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetForwardTypeFn(full_type::ReplicateInput())\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Snapshot\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklIdentity\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"Doc( Mkl implementation of IdentityOp\n)Doc\");\n#endif\n\nREGISTER_OP(\"IdentityN\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: list(type)\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      std::vector<ShapeHandle> input;\n      TF_RETURN_IF_ERROR(c->input(\"input\", &input));\n      TF_RETURN_IF_ERROR(c->set_output(\"output\", input));\n      // If any of the input shapes are not known, we should return error.\n      for (int i = 0; i < input.size(); i++) {\n        if (!input[i].Handle()) {\n          return errors::InvalidArgument(absl::StrCat(\n              \"Cannot infer output shape #\", i,\n              \" for IdentityN node because input shape #\", i, \" is unknown.\"));\n        }\n      }\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"RefIdentity\")\n    .Input(\"input: Ref(T)\")\n    .Output(\"output: Ref(T)\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .SetAllowsUninitializedInput();\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"DebugGradientIdentity\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .SetAllowsUninitializedInput();\n\nREGISTER_OP(\"DebugGradientRefIdentity\")\n    .Input(\"input: Ref(T)\")\n    .Output(\"output: Ref(T)\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .SetAllowsUninitializedInput();\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"StopGradient\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"PreventGradient\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"message: string = ''\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"CheckNumerics\")\n    .Input(\"tensor: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"message: string\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"CheckNumericsV2\")\n    .Input(\"tensor: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"message: string\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Reshape\")\n    .Input(\"tensor: T\")\n    .Input(\"shape: Tshape\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      return SetOutputShapeForReshape(c);\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklReshape\")\n    .Input(\"tensor: T\")\n    .Input(\"shape: Tshape\")\n    .Input(\"mkl_tensor: uint8\")\n    .Input(\"mkl_shape: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: type\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) { return SetOutputShapeForReshape(c); })\n    .Doc(R\"Doc( MKL implementation of ReshapeOp.\n)Doc\");\n#endif  // INTEL_MKL\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"InvertPermutation\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle x;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &x));\n      c->set_output(0, x);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Transpose\")\n    .Input(\"x: T\")\n    .Input(\"perm: Tperm\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\n    .SetShapeFn(TransposeShapeFn);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklTranspose\")\n    .Input(\"x: T\")\n    .Input(\"perm: Tperm\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\n    .SetShapeFn(TransposeShapeFn);\n#endif  // INTEL_MKL\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ConjugateTranspose\")\n    .Input(\"x: T\")\n    .Input(\"perm: Tperm\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\n    .SetShapeFn(TransposeShapeFn);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklConjugateTranspose\")\n    .Input(\"x: T\")\n    .Input(\"perm: Tperm\")\n    .Output(\"y: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\n    .SetShapeFn(TransposeShapeFn);\n#endif  // INTEL_MKL\n\n// --------------------------------------------------------------------------\nnamespace {\nStatus UniqueIdxShapeFn(InferenceContext* c) {\n  ShapeHandle input = c->input(0);\n  const Tensor* axis_t = c->input_tensor(1);\n  if (axis_t == nullptr || !c->RankKnown(input)) {\n    c->set_output(1, c->Vector(InferenceContext::kUnknownDim));\n    return OkStatus();\n  }\n\n  if (c->Rank(c->input(1)) != 1) {\n    return errors::InvalidArgument(\"axis expects a 1D vector.\");\n  }\n\n  int32_t n = axis_t->NumElements();\n  if (n == 0) {\n    if (c->Rank(input) != 1) {\n      return errors::InvalidArgument(\"x expects a 1D vector.\");\n    }\n    c->set_output(1, input);\n    return OkStatus();\n  } else if (n == 1) {\n    int64_t axis;\n    if (axis_t->dtype() == DT_INT32) {\n      axis = static_cast<int64_t>(axis_t->flat<int32>()(0));\n    } else {\n      axis = axis_t->flat<int64_t>()(0);\n    }\n\n    int64_t input_rank = c->Rank(input);\n    if (axis < -input_rank || axis >= input_rank) {\n      return errors::InvalidArgument(\"axis expects to be in the range [\",\n                                     -input_rank, \", \", input_rank, \")\");\n    }\n    if (axis < 0) {\n      axis += input_rank;\n    }\n    c->set_output(1, c->Vector(c->Dim(input, axis)));\n    return OkStatus();\n  }\n  return errors::InvalidArgument(\n      \"axis does not support input tensors larger than 1 elements.\");\n}\n}  // namespace\n\nREGISTER_OP(\"Unique\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Output(\"idx: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n      c->set_output(1, c->input(0));\n      // Assert that the input rank is 1.\n      ShapeHandle dummy;\n      return c->WithRank(c->input(0), 1, &dummy);\n    });\n\nREGISTER_OP(\"UniqueV2\")\n    .Input(\"x: T\")\n    .Input(\"axis: Taxis\")\n    .Output(\"y: T\")\n    .Output(\"idx: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"Taxis: {int32,int64} = DT_INT64\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->UnknownShapeOfRank(c->Rank(c->input(0))));\n      TF_RETURN_IF_ERROR(UniqueIdxShapeFn(c));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"UniqueWithCounts\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Output(\"idx: out_idx\")\n    .Output(\"count: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto uniq = c->Vector(InferenceContext::kUnknownDim);\n      c->set_output(0, uniq);\n      c->set_output(1, c->input(0));\n      c->set_output(2, uniq);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"UniqueWithCountsV2\")\n    .Input(\"x: T\")\n    .Input(\"axis: Taxis\")\n    .Output(\"y: T\")\n    .Output(\"idx: out_idx\")\n    .Output(\"count: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"Taxis: {int32,int64} = DT_INT64\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->UnknownShapeOfRank(c->Rank(c->input(0))));\n      TF_RETURN_IF_ERROR(UniqueIdxShapeFn(c));\n      c->set_output(2, c->Vector(InferenceContext::kUnknownDim));\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus ShapeShapeFn(InferenceContext* c) {\n  for (int i = 0; i < c->num_inputs(); ++i) {\n    DimensionHandle dim;\n    if (c->RankKnown(c->input(i))) {\n      dim = c->MakeDim(c->Rank(c->input(i)));\n    } else {\n      dim = c->UnknownDim();\n    }\n    c->set_output(i, c->Vector(dim));\n  }\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Shape\")\n    .Input(\"input: T\")\n    .Output(\"output: out_type\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn(ShapeShapeFn);\n\nREGISTER_OP(\"ShapeN\")\n    .Input(\"input: N * T\")\n    .Output(\"output: N * out_type\")\n    .Attr(\"N: int\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn(ShapeShapeFn);\n\nREGISTER_OP(\"EnsureShape\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"shape: shape\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      // Merges desired shape and statically known shape of input\n      PartialTensorShape desired_shape;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shape\", &desired_shape));\n\n      int rank = desired_shape.dims();\n      ShapeHandle input_shape_handle;\n      ShapeHandle desired_shape_handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), rank, &input_shape_handle));\n      TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(\n          desired_shape, &desired_shape_handle));\n\n      ShapeHandle merged_shape;\n      TF_RETURN_IF_ERROR(\n          c->Merge(desired_shape_handle, input_shape_handle, &merged_shape));\n      c->set_output(0, merged_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ReverseSequence\")\n    .Input(\"input: T\")\n    .Input(\"seq_lengths: Tlen\")\n    .Output(\"output: T\")\n    .Attr(\"seq_dim: int\")\n    .Attr(\"batch_dim: int = 0\")\n    .Attr(\"T: type\")\n    .Attr(\"Tlen: {int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle seq_lens_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &seq_lens_shape));\n\n      int64_t seq_dim;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"seq_dim\", &seq_dim));\n      int64_t batch_dim;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"batch_dim\", &batch_dim));\n\n      if (!c->RankKnown(input)) {\n        return shape_inference::UnknownShape(c);\n      }\n\n      // Validate batch_dim and seq_dim against input.\n      const int32_t input_rank = c->Rank(input);\n      if (batch_dim >= input_rank) {\n        return errors::InvalidArgument(\n            \"batch_dim must be < input rank: \", batch_dim, \" vs. \", input_rank);\n      }\n\n      if (seq_dim >= input_rank) {\n        return errors::InvalidArgument(\n            \"seq_dim must be < input rank: \", seq_dim, \" vs. \", input_rank);\n      }\n\n      // To prevent out of bound access when calling c->Dim(input, batch_dim),\n      // batch_dim range [-1 * input rank, input rank) is allowed. However,\n      // the op implementation has a stricter bound for batch_dim requiring >= 0\n      // value. Thus, perform strict check here.\n      if (batch_dim < 0) {\n        return errors::InvalidArgument(\"batch_dim must be >=0, got \",\n                                       batch_dim);\n      }\n\n      DimensionHandle batch_dim_dim = c->Dim(input, batch_dim);\n      TF_RETURN_IF_ERROR(\n          c->Merge(batch_dim_dim, c->Dim(seq_lens_shape, 0), &batch_dim_dim));\n\n      // Replace batch_dim of input with batch_size\n      ShapeHandle output_shape;\n      TF_RETURN_IF_ERROR(\n          c->ReplaceDim(input, batch_dim, batch_dim_dim, &output_shape));\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Rank\")\n    .Input(\"input: T\")\n    .Output(\"output: int32\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Size\")\n    .Input(\"input: T\")\n    .Output(\"output: out_type\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Slice\")\n    .Input(\"input: T\")\n    .Input(\"begin: Index\")\n    .Input(\"size: Index\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32,int64}\")\n    .SetShapeFn(shape_inference::SliceShape);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklSlice\")\n    .Input(\"input: T\")\n    .Input(\"begin: Index\")\n    .Input(\"size: Index\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_begin: uint8\")\n    .Input(\"mkl_size: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32,int64}\")\n    .SetShapeFn(shape_inference::SliceShape);\n#endif\n\nREGISTER_OP(\"StridedSlice\")\n    .Input(\"input: T\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int16, int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle begin_shape, end_shape, strides_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &begin_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &end_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &strides_shape));\n      TF_RETURN_IF_ERROR(c->Merge(begin_shape, end_shape, &begin_shape));\n      TF_RETURN_IF_ERROR(c->Merge(begin_shape, strides_shape, &begin_shape));\n      DimensionHandle sparse_dims_dim = c->Dim(begin_shape, 0);\n\n      const Tensor* strides_value = c->input_tensor(3);\n      // TODO(aselle,allenl): If we had a stride_mask it would be possible to do\n      // more shape inference here (e.g. for x[3, ::T]).\n      if (!c->RankKnown(input) || !c->ValueKnown(sparse_dims_dim) ||\n          strides_value == nullptr) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n\n      PartialTensorShape input_shape({});\n      for (int i = 0; i < c->Rank(input); ++i) {\n        auto dim = c->Dim(input, i);\n        input_shape.AddDim(c->ValueKnown(dim) ? c->Value(dim) : -1);\n      }\n\n      int32_t begin_mask, end_mask, ellipsis_mask, new_axis_mask,\n          shrink_axis_mask;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"begin_mask\", &begin_mask));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"end_mask\", &end_mask));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"ellipsis_mask\", &ellipsis_mask));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"new_axis_mask\", &new_axis_mask));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shrink_axis_mask\", &shrink_axis_mask));\n\n      const Tensor* begin_value = c->input_tensor(1);\n      const Tensor* end_value = c->input_tensor(2);\n\n      PartialTensorShape processing_shape, final_shape;\n      bool is_identity, is_simple_slice, slice_dim0;\n      gtl::InlinedVector<int64, 4> begin, end, strides;\n      TF_RETURN_IF_ERROR(ValidateStridedSliceOp(\n          begin_value, end_value, *strides_value, input_shape, begin_mask,\n          end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask,\n          &processing_shape, &final_shape, &is_identity, &is_simple_slice,\n          &slice_dim0, &begin, &end, &strides));\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(final_shape, &out));\n      c->set_output(0, out);\n\n      auto* shape_and_type = c->input_handle_shapes_and_types(0);\n      if (shape_and_type) {\n        c->set_output_handle_shapes_and_types(0, *shape_and_type);\n      }\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"StridedSliceGrad\")\n    .Input(\"shape: Index\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Input(\"dy: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"StridedSliceAssign\")\n    .Input(\"ref: Ref(T)\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Input(\"value: T\")\n    .Output(\"output_ref: Ref(T)\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n// TODO(aselle): Fix this documentation once StridedSliceAssign Supports\n// broadcasting.\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"ResourceStridedSliceAssign\")\n    .Input(\"ref: resource\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Input(\"value: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn(shape_inference::NoOutputs);\n\nREGISTER_OP(\"TensorStridedSliceUpdate\")\n    .Input(\"input: T\")\n    .Input(\"begin: Index\")\n    .Input(\"end: Index\")\n    .Input(\"strides: Index\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Index: {int32, int64}\")\n    .Attr(\"begin_mask: int = 0\")\n    .Attr(\"end_mask: int = 0\")\n    .Attr(\"ellipsis_mask: int = 0\")\n    .Attr(\"new_axis_mask: int = 0\")\n    .Attr(\"shrink_axis_mask: int = 0\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Tile\")\n    .Input(\"input: T\")\n    .Input(\"multiples: Tmultiples\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tmultiples: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      // NOTE(mrry): Represent `multiples` as a `TensorShape` because (i)\n      // it is a vector of non-negative integers, and (ii) doing so allows\n      // us to handle partially-known multiples.\n      ShapeHandle multiples;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &multiples));\n      if (c->RankKnown(input)) {\n        TF_RETURN_IF_ERROR(c->WithRank(multiples, c->Rank(input), &multiples));\n        ShapeHandle dummy;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->input(1), c->Vector(c->Rank(input)), &dummy));\n      }\n\n      if (!c->RankKnown(multiples)) {\n        return shape_inference::UnknownShape(c);\n      }\n\n      int32_t rank = c->Rank(multiples);\n      TF_RETURN_IF_ERROR(c->WithRank(input, rank, &input));\n      std::vector<DimensionHandle> dims(rank);\n      for (int i = 0; i < rank; ++i) {\n        TF_RETURN_IF_ERROR(\n            c->Multiply(c->Dim(input, i), c->Dim(multiples, i), &dims[i]));\n      }\n      c->set_output(0, c->MakeShape(dims));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"TileGrad\")\n    .Input(\"input: T\")\n    .Input(\"multiples: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Deprecated(3, \"TileGrad has been replaced with reduce_sum\")\n    .SetShapeFn(tensorflow::shape_inference::UnknownShape);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Where\")\n    .Input(\"input: T\")\n    .Attr(\"T: {numbertype, bool} = DT_BOOL\")\n    .Output(\"index: int64\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Matrix(c->UnknownDim(), c->Rank(c->input(0))));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"BroadcastArgs\")\n    .Input(\"s0: T\")\n    .Input(\"s1: T\")\n    .Output(\"r0: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      ShapeHandle shape_x = c->input(0);\n      ShapeHandle shape_y = c->input(1);\n      TF_RETURN_IF_ERROR(c->WithRank(shape_x, 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(shape_y, 1, &unused));\n\n      if (!c->ValueKnown(c->Dim(shape_x, 0)) ||\n          !c->ValueKnown(c->Dim(shape_y, 0))) {\n        c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n        return OkStatus();\n      }\n\n      int64_t x_dim = c->Value(c->Dim(shape_x, 0));\n      int64_t y_dim = c->Value(c->Dim(shape_y, 0));\n\n      // Broadcasted shape is going to be as large as the largest dimension.\n      c->set_output(0, c->Vector(std::max(x_dim, y_dim)));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"BroadcastGradientArgs\")\n    .Input(\"s0: T\")\n    .Input(\"s1: T\")\n    .Output(\"r0: T\")\n    .Output(\"r1: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      // TODO(mrry): Implement constant_value for BroadcastGradientArgs?\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));\n      c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n      c->set_output(1, c->Vector(InferenceContext::kUnknownDim));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Pad\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(PadShapeFn);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"PadV2\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Input(\"constant_values: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(PadShapeFn);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"MirrorPad\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .Attr(GetMirrorPadModeAttrString())\n    .SetShapeFn(PadShapeFn);\n\n// --------------------------------------------------------------------------\nnamespace {\ntemplate <typename T>\nStatus MirrorPadKnown(InferenceContext* c, ShapeHandle input,\n                      const Tensor* paddings_t, int64_t input_rank) {\n  auto paddings_data = paddings_t->matrix<T>();\n  std::vector<DimensionHandle> dims(input_rank);\n  for (int64_t i = 0; i < input_rank; ++i) {\n    const int64_t pad0 = static_cast<int64_t>(paddings_data(i, 0));\n    const int64_t pad1 = static_cast<int64_t>(paddings_data(i, 1));\n    if (pad0 < 0 || pad1 < 0) {\n      return errors::InvalidArgument(\"Paddings must be non-negative\");\n    }\n\n    TF_RETURN_IF_ERROR(c->Subtract(c->Dim(input, i), pad0 + pad1, &dims[i]));\n  }\n  c->set_output(0, c->MakeShape(dims));\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"MirrorPadGrad\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .Attr(GetMirrorPadModeAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle paddings;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 2, &paddings));\n      DimensionHandle pad_0 = c->Dim(paddings, 0);\n      if (!c->ValueKnown(pad_0)) {\n        // We don't know the rank of the output since the first\n        // padding dimension is unknown.\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n\n      int64_t input_rank = c->Value(pad_0);\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), input_rank, &input));\n      TF_RETURN_IF_ERROR(\n          c->Merge(paddings, c->Matrix(input_rank, 2), &paddings));\n\n      const Tensor* paddings_t = c->input_tensor(1);\n      if (paddings_t == nullptr) {\n        // Values of 'paddings' is not available, but we know the\n        // input rank, so return the rank of the output with unknown\n        // dimensions.\n        c->set_output(0, c->UnknownShapeOfRank(input_rank));\n        return OkStatus();\n      }\n\n      if (paddings_t->dtype() == DT_INT32) {\n        return MirrorPadKnown<int32>(c, input, paddings_t, input_rank);\n      } else {\n        return MirrorPadKnown<int64_t>(c, input, paddings_t, input_rank);\n      }\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Placeholder\")\n    .Output(\"output: dtype\")\n    .Attr(\"dtype: type\")\n    .Attr(\"shape: shape = { unknown_rank: true }\")\n    .SetShapeFn([](InferenceContext* c) {\n      PartialTensorShape shape;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shape\", &shape));\n\n      // Placeholder has legacy behavior where we cannot tell the difference\n      // between a scalar shape attribute and 'unknown shape'.  So if the shape\n      // is a scalar, we return an unknown shape.\n      if (c->graph_def_version() <= 21 && shape.dims() <= 0) {\n        return shape_inference::UnknownShape(c);\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(shape, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// Placeholder was modified in a backwards compatible way to do what\n// PlaceholderV2 did, so we have deprecated V2 (no one was really\n// using it).\nREGISTER_OP(\"PlaceholderV2\")\n    .Output(\"output: dtype\")\n    .Attr(\"dtype: type\")\n    .Attr(\"shape: shape\")\n    .SetShapeFn(shape_inference::ExplicitShape)\n    .Deprecated(23, \"Placeholder now behaves the same as PlaceholderV2.\");\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"PlaceholderWithDefault\")\n    .Input(\"input: dtype\")\n    .Output(\"output: dtype\")\n    .Attr(\"dtype: type\")\n    .Attr(\"shape: shape\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      PartialTensorShape shape;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"shape\", &shape));\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(shape, &out));\n\n      // We merge for compatibility checking, but return the output,\n      // since output_shape may be less precise than input_shape.\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->Merge(input, out, &unused));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ExpandDims\")\n    .Input(\"input: T\")\n    .Input(\"dim: Tdim\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tdim: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n\n      const Tensor* dim_t = c->input_tensor(1);\n      if (dim_t != nullptr && dim_t->NumElements() != 1) {\n        return errors::InvalidArgument(\n            \"'dim' input must be a tensor with a single value\");\n      }\n      if (dim_t == nullptr || !c->RankKnown(input)) {\n        c->set_output(0, c->UnknownShape());\n        return OkStatus();\n      }\n\n      int64_t dim;\n      if (dim_t->dtype() == DT_INT32) {\n        dim = static_cast<int64_t>(dim_t->flat<int32>()(0));\n      } else {\n        dim = dim_t->flat<int64_t>()(0);\n      }\n\n      const int32_t rank = c->Rank(input);\n      const int32_t min_dim = -1 * rank - 1;\n      if (dim < min_dim || dim > rank) {\n        return errors::InvalidArgument(\"dim \", dim, \" not in the interval [\",\n                                       min_dim, \", \", rank, \"].\");\n      }\n\n      if (dim < 0) {\n        dim += rank + 1;\n      }\n\n      ShapeHandle end;\n      TF_RETURN_IF_ERROR(c->Subshape(input, dim, &end));\n\n      // Build output as start + 1 + end.\n      ShapeHandle output;\n      TF_RETURN_IF_ERROR(c->Subshape(input, 0, dim, &output));\n      TF_RETURN_IF_ERROR(c->Concatenate(output, c->Vector(1), &output));\n      TF_RETURN_IF_ERROR(c->Concatenate(output, end, &output));\n      c->set_output(0, output);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Squeeze\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"squeeze_dims: list(int) >= 0 = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      if (!c->RankKnown(input)) {\n        // Input shape unknown.\n        return shape_inference::UnknownShape(c);\n      }\n\n      const int32_t input_rank = c->Rank(input);\n\n      // Validate and wrap squeeze dimensions.\n      std::vector<int32> squeeze_dims;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"squeeze_dims\", &squeeze_dims));\n      for (int i = 0; i < squeeze_dims.size(); ++i) {\n        if (squeeze_dims[i] < -input_rank || squeeze_dims[i] >= input_rank) {\n          return errors::InvalidArgument(\"squeeze_dims[\", i, \"] not in [\",\n                                         -input_rank, \",\", input_rank, \").\");\n        }\n\n        if (squeeze_dims[i] < 0) {\n          squeeze_dims[i] += input_rank;\n        }\n      }\n\n      std::vector<DimensionHandle> result_shape;\n      for (int i = 0; i < input_rank; ++i) {\n        // True if squeeze_dims contains an entry to squeeze this\n        // dimension.\n        bool is_explicit_match =\n            std::find(squeeze_dims.begin(), squeeze_dims.end(), i) !=\n            squeeze_dims.end();\n\n        DimensionHandle dim = c->Dim(input, i);\n\n        if (!c->ValueKnown(dim)) {\n          // Assume that the squeezed dimension will be 1 at runtime.\n          if (is_explicit_match) continue;\n\n          // If squeezing all 1 dimensions, and we see an unknown value,\n          // give up and return Unknown Shape.\n          if (squeeze_dims.empty()) {\n            c->set_output(0, c->UnknownShape());\n            return OkStatus();\n          }\n        } else if (c->Value(dim) == 1) {\n          if (is_explicit_match || squeeze_dims.empty()) {\n            // If explicitly squeezing, or squeezing all 1s, remove\n            // this dimension.\n            continue;\n          }\n        } else if (is_explicit_match) {\n          return errors::InvalidArgument(\"Can not squeeze dim[\", i,\n                                         \"], expected a dimension of 1, got \",\n                                         c->Value(c->Dim(input, i)));\n        }\n\n        result_shape.emplace_back(dim);\n      }\n\n      c->set_output(0, c->MakeShape(result_shape));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"ListDiff\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"out: T\")\n    .Output(\"idx: out_idx\")\n    .Attr(\"T: type\")\n    .Attr(\"out_idx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));\n      // TODO(mrry): Indicate that the length falls within an interval?\n      ShapeHandle out = c->Vector(InferenceContext::kUnknownDim);\n      c->set_output(0, out);\n      c->set_output(1, out);\n      return OkStatus();\n    });\n\nnamespace {\n\n// Converts Tensor to flat std::vector<int64_t>.\ntemplate <typename InputType>\nstd::vector<int64_t> GetFlatInt64(const Tensor& t) {\n  std::vector<int64_t> output(t.shape().num_elements());\n  if (t.shape().num_elements() > 0) {\n    auto eigen_vec = t.flat<InputType>();\n    std::copy_n(&eigen_vec(0), output.size(), output.begin());\n  }\n  return output;\n}\n\n// Converts int32 or int64 Tensor to flat std::vector<int64_t>.\nstd::vector<int64_t> GetFlatInt64(const Tensor& t) {\n  if (t.dtype() == DT_INT32) {\n    return GetFlatInt64<int32>(t);\n  } else {\n    return GetFlatInt64<int64_t>(t);\n  }\n}\n\nStatus SpaceToBatchShapeHelper(InferenceContext* c, ShapeHandle input_shape,\n                               ShapeHandle block_shape_shape,\n                               const Tensor* block_shape_t,\n                               ShapeHandle paddings_shape,\n                               const Tensor* paddings_t) {\n  if (c->Rank(block_shape_shape) != 1) {\n    return errors::InvalidArgument(\"block_shape must have rank 1.\");\n  }\n\n  const DimensionHandle num_block_dims_handle = c->Dim(block_shape_shape, 0);\n  if (!c->ValueKnown(num_block_dims_handle)) {\n    return errors::InvalidArgument(\"block_shape must have known size.\");\n  }\n\n  const int64_t num_block_dims = c->Value(num_block_dims_handle);\n\n  TF_RETURN_IF_ERROR(\n      c->WithRankAtLeast(input_shape, num_block_dims + 1, &input_shape));\n\n  TF_RETURN_IF_ERROR(\n      c->Merge(paddings_shape, c->Matrix(num_block_dims, 2), &paddings_shape));\n\n  DimensionHandle batch_size = c->Dim(input_shape, 0);\n  std::vector<int64_t> block_shape_vec;\n  if (block_shape_t && (block_shape_t->NumElements() > 0)) {\n    block_shape_vec = GetFlatInt64(*block_shape_t);\n    for (int64_t dim = 0; dim < num_block_dims; ++dim) {\n      const int64_t block_shape_value = block_shape_vec[dim];\n      if (block_shape_value < 1) {\n        return errors::InvalidArgument(\"block_shape must be positive\");\n      }\n      if (c->ValueKnown(batch_size)) {\n        TF_RETURN_IF_ERROR(\n            c->Multiply(batch_size, block_shape_value, &batch_size));\n      } else {\n        batch_size = c->UnknownDim();\n      }\n    }\n  } else if (num_block_dims > 0) {\n    batch_size = c->UnknownDim();\n  }\n\n  std::vector<DimensionHandle> output_dims{batch_size};\n  output_dims.resize(num_block_dims + 1, c->UnknownDim());\n\n  if (paddings_t && (paddings_t->NumElements() > 0)) {\n    const std::vector<int64_t> paddings_vec = GetFlatInt64(*paddings_t);\n    for (int64_t dim = 0; dim < num_block_dims; ++dim) {\n      const int64_t pad_start = paddings_vec[dim * 2],\n                    pad_end = paddings_vec[dim * 2 + 1];\n      if (pad_start < 0 || pad_end < 0) {\n        return errors::InvalidArgument(\"paddings cannot be negative\");\n      }\n      if (block_shape_t) {\n        DimensionHandle padded_size;\n        TF_RETURN_IF_ERROR(\n            c->Add(c->Dim(input_shape, dim + 1), pad_start, &padded_size));\n        TF_RETURN_IF_ERROR(c->Add(padded_size, pad_end, &padded_size));\n        TF_RETURN_IF_ERROR(c->Divide(padded_size, block_shape_vec[dim],\n                                     /*evenly_divisible=*/true,\n                                     &output_dims[dim + 1]));\n      }\n    }\n  }\n\n  ShapeHandle remaining_input_shape;\n  TF_RETURN_IF_ERROR(\n      c->Subshape(input_shape, 1 + num_block_dims, &remaining_input_shape));\n\n  ShapeHandle result;\n  TF_RETURN_IF_ERROR(c->Concatenate(c->MakeShape(output_dims),\n                                    remaining_input_shape, &result));\n  c->set_output(0, result);\n  return OkStatus();\n}\n\nStatus BatchToSpaceShapeHelper(InferenceContext* c, ShapeHandle input_shape,\n                               ShapeHandle block_shape_shape,\n                               const Tensor* block_shape_t,\n                               ShapeHandle crops_shape, const Tensor* crops_t) {\n  if (c->Rank(block_shape_shape) != 1) {\n    return errors::InvalidArgument(\"block_shape must have rank 1.\");\n  }\n\n  const DimensionHandle num_block_dims_handle = c->Dim(block_shape_shape, 0);\n  if (!c->ValueKnown(num_block_dims_handle)) {\n    return errors::InvalidArgument(\"block_shape must have known size.\");\n  }\n\n  const int64_t num_block_dims = c->Value(num_block_dims_handle);\n\n  TF_RETURN_IF_ERROR(\n      c->WithRankAtLeast(input_shape, num_block_dims + 1, &input_shape));\n\n  TF_RETURN_IF_ERROR(\n      c->Merge(crops_shape, c->Matrix(num_block_dims, 2), &crops_shape));\n\n  DimensionHandle batch_size = c->Dim(input_shape, 0);\n  std::vector<int64_t> block_shape_vec;\n  if (block_shape_t) {\n    block_shape_vec = GetFlatInt64(*block_shape_t);\n    for (int64_t dim = 0; dim < num_block_dims; ++dim) {\n      const int64_t block_shape_value = block_shape_vec[dim];\n      if (block_shape_value < 1) {\n        return errors::InvalidArgument(\"block_shape must be positive\");\n      }\n      if (c->ValueKnown(batch_size)) {\n        TF_RETURN_IF_ERROR(c->Divide(batch_size, block_shape_value,\n                                     /*evenly_divisible=*/true, &batch_size));\n      } else {\n        batch_size = c->UnknownDim();\n      }\n    }\n  } else if (num_block_dims > 0) {\n    batch_size = c->UnknownDim();\n  }\n\n  std::vector<DimensionHandle> output_dims{batch_size};\n  output_dims.resize(num_block_dims + 1, c->UnknownDim());\n\n  if (crops_t) {\n    const std::vector<int64_t> crops_vec = GetFlatInt64(*crops_t);\n    for (int64_t dim = 0; dim < num_block_dims; ++dim) {\n      const int64_t crop_start = crops_vec[dim * 2],\n                    crop_end = crops_vec[dim * 2 + 1];\n      if (crop_start < 0 || crop_end < 0) {\n        return errors::InvalidArgument(\"crops cannot be negative\");\n      }\n      if (block_shape_t) {\n        DimensionHandle cropped_size;\n        TF_RETURN_IF_ERROR(c->Multiply(c->Dim(input_shape, dim + 1),\n                                       block_shape_vec[dim], &cropped_size));\n        TF_RETURN_IF_ERROR(\n            c->Subtract(cropped_size, crop_start, &cropped_size));\n        TF_RETURN_IF_ERROR(\n            c->Subtract(cropped_size, crop_end, &output_dims[dim + 1]));\n      }\n    }\n  }\n\n  ShapeHandle remaining_input_shape;\n  TF_RETURN_IF_ERROR(\n      c->Subshape(input_shape, 1 + num_block_dims, &remaining_input_shape));\n\n  ShapeHandle result;\n  TF_RETURN_IF_ERROR(c->Concatenate(c->MakeShape(output_dims),\n                                    remaining_input_shape, &result));\n  c->set_output(0, result);\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"SpaceToBatchND\")\n    .Input(\"input: T\")\n    .Input(\"block_shape: Tblock_shape\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tblock_shape: {int32, int64} = DT_INT32\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      return SpaceToBatchShapeHelper(c, c->input(0), c->input(1),\n                                     c->input_tensor(1), c->input(2),\n                                     c->input_tensor(2));\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"SpaceToBatch\")\n    .Input(\"input: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .Attr(\"block_size: int >= 2\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n\n      int32_t block_size;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"block_size\", &block_size));\n\n      Tensor block_shape(tensorflow::DT_INT64, TensorShape({2}));\n      auto block_shape_vec = block_shape.vec<int64_t>();\n      block_shape_vec(0) = block_size;\n      block_shape_vec(1) = block_size;\n\n      return SpaceToBatchShapeHelper(c, input_shape, c->MakeShape({2}),\n                                     &block_shape, c->input(1),\n                                     c->input_tensor(1));\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"BatchToSpaceND\")\n    .Input(\"input: T\")\n    .Input(\"block_shape: Tblock_shape\")\n    .Input(\"crops: Tcrops\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tblock_shape: {int32, int64} = DT_INT32\")\n    .Attr(\"Tcrops: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      return BatchToSpaceShapeHelper(c, c->input(0), c->input(1),\n                                     c->input_tensor(1), c->input(2),\n                                     c->input_tensor(2));\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"BatchToSpace\")\n    .Input(\"input: T\")\n    .Input(\"crops: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"block_size: int >= 2\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n\n      int32_t block_size;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"block_size\", &block_size));\n\n      Tensor block_shape(tensorflow::DT_INT64, TensorShape({2}));\n      auto block_shape_vec = block_shape.vec<int64_t>();\n      block_shape_vec(0) = block_size;\n      block_shape_vec(1) = block_size;\n\n      return BatchToSpaceShapeHelper(c, input_shape, c->MakeShape({2}),\n                                     &block_shape, c->input(1),\n                                     c->input_tensor(1));\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"SpaceToDepth\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"block_size: int >= 2\")\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    // TODO(pauldonnelly): Implement GPU kernels for NCHW_VECT_C.\n    .SetShapeFn([](InferenceContext* c) {\n      string data_format_str;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n      TensorFormat data_format;\n      FormatFromString(data_format_str, &data_format);\n\n      constexpr int num_spatial_dims = 2;\n      const int dims =\n          GetTensorDimsFromSpatialDims(num_spatial_dims, data_format);\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), dims, &input));\n\n      int32_t block_size;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"block_size\", &block_size));\n\n      DimensionHandle batch_size =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'N'));\n      DimensionHandle input_height =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'H'));\n      DimensionHandle input_width =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'W'));\n      DimensionHandle input_depth =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'C'));\n\n      DimensionHandle output_height;\n      DimensionHandle output_width;\n      DimensionHandle output_depth;\n      // Will return an error if input height or width are not evenly divisible.\n      TF_RETURN_IF_ERROR(c->Divide(input_height, block_size,\n                                   true /* evenly_divisible */,\n                                   &output_height));\n      TF_RETURN_IF_ERROR(c->Divide(input_width, block_size,\n                                   true /* evenly_divisible */, &output_width));\n\n      TF_RETURN_IF_ERROR(\n          c->Multiply(input_depth, block_size * block_size, &output_depth));\n\n      ShapeHandle output_shape;\n      TF_RETURN_IF_ERROR(MakeShapeFromFormat(data_format, batch_size,\n                                             {output_height, output_width},\n                                             output_depth, &output_shape, c));\n\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"DepthToSpace\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"block_size: int >= 2\")\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    // TODO(pauldonnelly): Implement GPU kernels for NCHW and NCHW_VECT_C.\n    .SetShapeFn([](InferenceContext* c) {\n      string data_format_str;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n      TensorFormat data_format;\n      FormatFromString(data_format_str, &data_format);\n\n      constexpr int num_spatial_dims = 2;\n      const int dims =\n          GetTensorDimsFromSpatialDims(num_spatial_dims, data_format);\n\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), dims, &input));\n\n      int32_t block_size;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"block_size\", &block_size));\n\n      DimensionHandle batch_size =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'N'));\n      DimensionHandle input_height =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'H'));\n      DimensionHandle input_width =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'W'));\n      DimensionHandle input_depth =\n          c->Dim(input, GetTensorDimIndex<num_spatial_dims>(data_format, 'C'));\n\n      DimensionHandle output_height;\n      DimensionHandle output_width;\n      DimensionHandle output_depth;\n      TF_RETURN_IF_ERROR(c->Multiply(input_height, block_size, &output_height));\n      TF_RETURN_IF_ERROR(c->Multiply(input_width, block_size, &output_width));\n\n      // Will return an error if input_depth is not evenly divisible.\n      TF_RETURN_IF_ERROR(c->Divide(input_depth, block_size * block_size,\n                                   true /* evenly_divisible */, &output_depth));\n\n      ShapeHandle output_shape;\n      TF_RETURN_IF_ERROR(MakeShapeFromFormat(data_format, batch_size,\n                                             {output_height, output_width},\n                                             output_depth, &output_shape, c));\n\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"ExtractImagePatches\")\n    .Input(\"images: T\")\n    .Output(\"patches: T\")\n    .Attr(\"ksizes: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, int16, int32, int64, \"\n        \"uint8, uint16, uint32, uint64, complex64, complex128, bool}\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n\n      std::vector<int32> ksizes;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"ksizes\", &ksizes));\n      if (ksizes.size() != 4) {\n        return errors::InvalidArgument(\n            \"ExtractImagePatches requires the ksizes attribute to contain 4 \"\n            \"values, but got: \",\n            ksizes.size());\n      }\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 4) {\n        return errors::InvalidArgument(\n            \"ExtractImagePatches requires the stride attribute to contain 4 \"\n            \"values, but got: \",\n            strides.size());\n      }\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 4) {\n        return errors::InvalidArgument(\n            \"ExtractImagePatches requires the rates attribute to contain 4 \"\n            \"values, but got: \",\n            rates.size());\n      }\n\n      int32_t ksize_rows = ksizes[1];\n      int32_t ksize_cols = ksizes[2];\n\n      int32_t stride_rows = strides[1];\n      int32_t stride_cols = strides[2];\n\n      int32_t rate_rows = rates[1];\n      int32_t rate_cols = rates[2];\n\n      int32_t ksize_rows_eff = ksize_rows + (ksize_rows - 1) * (rate_rows - 1);\n      int32_t ksize_cols_eff = ksize_cols + (ksize_cols - 1) * (rate_cols - 1);\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 2);\n      DimensionHandle output_depth_dim;\n      TF_RETURN_IF_ERROR(c->Multiply(\n          c->Dim(input_shape, 3), ksize_rows * ksize_cols, &output_depth_dim));\n\n      if (!c->ValueKnown(in_rows_dim) || !c->ValueKnown(in_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, ksize_rows_eff, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, ksize_cols_eff, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n      ShapeHandle output_shape = c->MakeShape(\n          {batch_size_dim, output_rows, output_cols, output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\n// To enable rates, uncomment all lines commented below and use ksize_*_eff\n// as the second parameter of all GetWindowedOutputSizeVerbose calls instead\n// of ksize_*.\nREGISTER_OP(\"ExtractVolumePatches\")\n    .Input(\"input: T\")\n    .Output(\"patches: T\")\n    .Attr(\"ksizes: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    /* .Attr(\"rates: list(int) >= 5\") */\n    .Attr(\"T: realnumbertype\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 5, &input_shape));\n\n      std::vector<int32> ksizes;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"ksizes\", &ksizes));\n      if (ksizes.size() != 5) {\n        return errors::InvalidArgument(\n            \"ExtractVolumePatches requires the ksizes attribute to contain 5 \"\n            \"values, but got: \",\n            ksizes.size());\n      }\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 5) {\n        return errors::InvalidArgument(\n            \"ExtractVolumePatches requires the stride attribute to contain 5 \"\n            \"values, but got: \",\n            strides.size());\n      }\n\n      /*\n      // TODO(hsgkim): Enable rates.\n      // See extract_volume_patches_op.cc for why rates are disabled now.\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 5) {\n        return errors::InvalidArgument(\n            \"ExtractVolumePatches requires the rates attribute to contain 5 \"\n            \"values, but got: \",\n            rates.size());\n      }\n      */\n\n      int32_t ksize_planes = ksizes[1];\n      int32_t ksize_rows = ksizes[2];\n      int32_t ksize_cols = ksizes[3];\n\n      int32_t stride_planes = strides[1];\n      int32_t stride_rows = strides[2];\n      int32_t stride_cols = strides[3];\n\n      /*\n      int32 rate_planes = rates[1];\n      int32 rate_rows = rates[2];\n      int32 rate_cols = rates[3];\n\n      int32 ksize_planes_eff = ksize_planes +\n                               (ksize_planes - 1) * (rate_planes - 1);\n      int32 ksize_rows_eff = ksize_rows + (ksize_rows - 1) * (rate_rows - 1);\n      int32 ksize_cols_eff = ksize_cols + (ksize_cols - 1) * (rate_cols - 1);\n      */\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_planes_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 2);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 3);\n      DimensionHandle output_depth_dim;\n      TF_RETURN_IF_ERROR(c->Multiply(c->Dim(input_shape, 4),\n                                     ksize_planes * ksize_rows * ksize_cols,\n                                     &output_depth_dim));\n\n      if (!c->ValueKnown(in_planes_dim) || !c->ValueKnown(in_rows_dim) ||\n          !c->ValueKnown(in_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      auto in_planes = c->Value(in_planes_dim);\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_planes, output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_planes, ksize_planes, stride_planes, padding, &output_planes,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, ksize_rows, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, ksize_cols, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n      ShapeHandle output_shape =\n          c->MakeShape({batch_size_dim, output_planes, output_rows, output_cols,\n                        output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"OneHot\")\n    .Input(\"indices: TI\")\n    .Input(\"depth: int32\")\n    .Input(\"on_value: T\")\n    .Input(\"off_value: T\")\n    .Attr(\"axis: int = -1\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"TI: {uint8, int8, int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      int32_t axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      if (axis < -1) return errors::InvalidArgument(\"axis must be >= -1\");\n\n      DimensionHandle depth;\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &depth));\n\n      ShapeHandle indices = c->input(0);\n      if (!c->RankKnown(indices)) return shape_inference::UnknownShape(c);\n\n      int32_t new_rank = c->Rank(indices) + 1;\n      // We need to add new_rank to axis in the case the axis is -1 because\n      // C++ returns negative values from % if the dividend is negative.\n      int32_t depth_index = (axis + new_rank) % new_rank;\n      // Out shape is indices[0:depth_index] + [depth] + indices[depth_index:].\n      ShapeHandle front;\n      ShapeHandle back;\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->Subshape(indices, 0, depth_index, &front));\n      TF_RETURN_IF_ERROR(c->Subshape(indices, depth_index, &back));\n      TF_RETURN_IF_ERROR(c->Concatenate(front, c->Vector(depth), &front));\n      TF_RETURN_IF_ERROR(c->Concatenate(front, back, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\n// EXPERIMENTAL. DO NOT USE OR DEPEND ON THIS YET.\nREGISTER_OP(\"QuantizeAndDequantize\")\n    .Input(\"input: T\")\n    .Attr(\"signed_input: bool = true\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"range_given: bool = false\")\n    .Attr(\"input_min: float = 0\")\n    .Attr(\"input_max: float = 0\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Deprecated(22, \"Replaced by QuantizeAndDequantizeV2\");\n\n// TODO(suharshs): Deprecate QuantizeAndDequantizeV2.\nREGISTER_OP(\"QuantizeAndDequantizeV2\")\n    .Input(\"input: T\")\n    .Input(\"input_min: T\")\n    .Input(\"input_max: T\")\n    .Attr(\"signed_input: bool = true\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"range_given: bool = false\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\n        \"round_mode: {'HALF_TO_EVEN', 'HALF_UP'} = \"\n        \"'HALF_TO_EVEN'\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      } else if (axis != -1) {\n        ShapeHandle input;\n        if (axis >= kint32max) {\n          return errors::InvalidArgument(\n              \"Axis cannot be >= kint32max value, got \", axis);\n        }\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      c->set_output(0, c->input(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizeAndDequantizeV4\")\n    .Input(\"input: T\")\n    .Input(\"input_min: T\")\n    .Input(\"input_max: T\")\n    .Attr(\"signed_input: bool = true\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"range_given: bool = false\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\n        \"round_mode: {'HALF_TO_EVEN', 'HALF_UP'} = \"\n        \"'HALF_TO_EVEN'\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      } else if (axis != -1) {\n        ShapeHandle input;\n        if (axis >= kint32max) {\n          return errors::InvalidArgument(\n              \"Axis cannot be >= kint32max value, got \", axis);\n        }\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      c->set_output(0, c->input(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizeAndDequantizeV4Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"input: T\")\n    .Input(\"input_min: T\")\n    .Input(\"input_max: T\")\n    .Output(\"input_backprop: T\")\n    .Output(\"input_min_backprop: T\")\n    .Output(\"input_max_backprop: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"axis: int = -1\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(3), minmax, &minmax));\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      } else if (axis != -1) {\n        ShapeHandle input;\n        if (axis >= kint32max) {\n          return errors::InvalidArgument(\n              \"Axis cannot be >= kint32max value, got \", axis);\n        }\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      ShapeHandle inputs;\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &inputs));\n      c->set_output(0, inputs);\n      c->set_output(1, minmax);\n      c->set_output(2, minmax);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizeAndDequantizeV3\")\n    .Input(\"input: T\")\n    .Input(\"input_min: T\")\n    .Input(\"input_max: T\")\n    .Input(\"num_bits: int32\")\n    .Attr(\"signed_input: bool = true\")\n    .Attr(\"range_given: bool = true\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      } else if (axis != -1) {\n        ShapeHandle input;\n        if (axis >= kint32max) {\n          return errors::InvalidArgument(\n              \"Axis cannot be >= kint32max value, got \", axis);\n        }\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(0, c->input(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizeV2\")\n    .Input(\"input: float\")\n    .Input(\"min_range: float\")\n    .Input(\"max_range: float\")\n    .Output(\"output: T\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"mode: {'MIN_COMBINED', 'MIN_FIRST', 'SCALED'} = 'MIN_COMBINED'\")\n    .Attr(\n        \"round_mode: {'HALF_AWAY_FROM_ZERO', 'HALF_TO_EVEN'} = \"\n        \"'HALF_AWAY_FROM_ZERO'\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .Attr(\"ensure_minimum_range: float = 0.01\")\n    .SetShapeFn(shape_inference::QuantizeV2Shape);\n\nREGISTER_OP(\"Dequantize\")\n    .Input(\"input: T\")\n    .Input(\"min_range: float\")\n    .Input(\"max_range: float\")\n    .Output(\"output: dtype\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"mode: {'MIN_COMBINED', 'MIN_FIRST', 'SCALED'} = 'MIN_COMBINED'\")\n    .Attr(\"narrow_range: bool = false\")\n    .Attr(\"axis: int = -1\")\n    .Attr(\"dtype: {bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      int axis = -1;\n      Status s = c->GetAttr(\"axis\", &axis);\n      if (!s.ok() && s.code() != error::NOT_FOUND) {\n        return s;\n      }\n      if (axis < -1) {\n        return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                       axis);\n      }\n      auto input_dims = c->Rank(c->input(0));\n      if (axis > input_dims) {\n        return errors::InvalidArgument(\n            \"Axis must be less than input dimension(\", input_dims, \"), got \",\n            axis);\n      }\n      const int minmax_rank = (axis == -1) ? 0 : 1;\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle minmax;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), minmax_rank, &minmax));\n      if (axis != -1) {\n        ShapeHandle input;\n        if (axis >= kint32max) {\n          // Check int32 max bound for a corner case to prevent integer flow\n          // when input actually has kint32max rank and above bound check is not\n          // triggered.\n          return errors::InvalidArgument(\n              \"Axis cannot be >= kint32max value, got \", axis);\n        }\n        TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n        DimensionHandle depth;\n        TF_RETURN_IF_ERROR(\n            c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConcat\")\n    .Input(\"concat_dim: int32\")\n    .Input(\"values: N * T\")\n    .Input(\"input_mins: N * float32\")\n    .Input(\"input_maxes: N * float32\")\n    .Output(\"output: T\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      const int n = (c->num_inputs() - 1) / 3;\n      TF_RETURN_IF_ERROR(shape_inference::ConcatShape(c, n));\n      ShapeHandle unused;\n      for (int i = n + 1; i < c->num_inputs(); ++i) {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 0, &unused));\n      }\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedReshape\")\n    .Input(\"tensor: T\")\n    .Input(\"shape: Tshape\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Output(\"output: T\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"T: type\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(SetOutputShapeForReshape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedInstanceNorm\")\n    .Input(\"x: T\")\n    .Input(\"x_min: float\")\n    .Input(\"x_max: float\")\n    .Output(\"y: T\")\n    .Output(\"y_min: float\")\n    .Output(\"y_max: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"output_range_given: bool = false\")\n    .Attr(\"given_y_min: float = 0\")\n    .Attr(\"given_y_max: float = 0\")\n    .Attr(\"variance_epsilon: float = 1e-5\")\n    .Attr(\"min_separation: float = 1e-3\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // x should be a rank 4 tensor.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &unused));\n      // Assert x_min and x_max are scalars (rank 0).\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      // y has the same shape as x.\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      // y_min and y_max are scalars.\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus ScatterNdTensorShape(InferenceContext* c) {\n  ShapeHandle output_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &output_shape));\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &indices_shape));\n  ShapeHandle updates_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 0, &updates_shape));\n  return shape_inference::ScatterNdShapeHelper(c, indices_shape, updates_shape,\n                                               output_shape);\n}\n\n}  // namespace\n\nREGISTER_OP(\"UpperBound\")\n    .Input(\"sorted_inputs: T\")\n    .Input(\"values: T\")\n    .Output(\"output: out_type\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &unused_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 2, &unused_shape));\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LowerBound\")\n    .Input(\"sorted_inputs: T\")\n    .Input(\"values: T\")\n    .Output(\"output: out_type\")\n    .Attr(\"T: type\")\n    .Attr(\"out_type: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &unused_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 2, &unused_shape));\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"ScatterNd\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Input(\"shape: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int16, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle indices_shape;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &indices_shape));\n      ShapeHandle updates_shape;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &updates_shape));\n      ShapeHandle output_shape;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(2, &output_shape));\n      return shape_inference::ScatterNdShapeHelper(c, indices_shape,\n                                                   updates_shape, output_shape);\n    });\n\nREGISTER_OP(\"TensorScatterUpdate\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int16, int32, int64, uint16}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"TensorScatterAdd\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"TensorScatterSub\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"TensorScatterMin\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"TensorScatterMax\")\n    .Input(\"tensor: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"ScatterNdNonAliasingAdd\")\n    .Input(\"input: T\")\n    .Input(\"indices: Tindices\")\n    .Input(\"updates: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {numbertype, bool}\")\n    .Attr(\"Tindices: {int32, int64}\")\n    .SetShapeFn(ScatterNdTensorShape);\n\nREGISTER_OP(\"FakeQuantWithMinMaxArgs\")\n    .Attr(\"min: float = -6.0\")\n    .Attr(\"max: float = 6.0\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"inputs: float\")\n    .Output(\"outputs: float\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FakeQuantWithMinMaxArgsGradient\")\n    .Attr(\"min: float = -6.0\")\n    .Attr(\"max: float = 6.0\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"gradients: float\")\n    .Input(\"inputs: float\")\n    .Output(\"backprops: float\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FakeQuantWithMinMaxVars\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"inputs: float\")\n    .Input(\"min: float\")\n    .Input(\"max: float\")\n    .Output(\"outputs: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"FakeQuantWithMinMaxVarsGradient\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"gradients: float\")\n    .Input(\"inputs: float\")\n    .Input(\"min: float\")\n    .Input(\"max: float\")\n    .Output(\"backprops_wrt_input: float\")\n    .Output(\"backprop_wrt_min: float\")\n    .Output(\"backprop_wrt_max: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      // gradients and inputs are same size.\n      ShapeHandle inputs;\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &inputs));\n\n      // min and max are scalars\n      ShapeHandle min_max;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &min_max));\n      TF_RETURN_IF_ERROR(c->Merge(min_max, c->input(3), &min_max));\n\n      c->set_output(0, inputs);\n      c->set_output(1, min_max);\n      c->set_output(2, min_max);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"FakeQuantWithMinMaxVarsPerChannel\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"inputs: float\")\n    .Input(\"min: float\")\n    .Input(\"max: float\")\n    .Output(\"outputs: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input, min, max;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &min));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &max));\n\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(c->Merge(c->Dim(input, -1), c->Dim(min, 0), &unused));\n      TF_RETURN_IF_ERROR(c->Merge(c->Dim(input, -1), c->Dim(max, 0), &unused));\n      TF_RETURN_IF_ERROR(c->Merge(c->Dim(min, 0), c->Dim(max, 0), &unused));\n\n      c->set_output(0, input);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"FakeQuantWithMinMaxVarsPerChannelGradient\")\n    .Attr(\"num_bits: int = 8\")\n    .Attr(\"narrow_range: bool = false\")\n    .Input(\"gradients: float\")\n    .Input(\"inputs: float\")\n    .Input(\"min: float\")\n    .Input(\"max: float\")\n    .Output(\"backprops_wrt_input: float\")\n    .Output(\"backprop_wrt_min: float\")\n    .Output(\"backprop_wrt_max: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle inputs;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &inputs));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(inputs, 4, &inputs));\n      TF_RETURN_IF_ERROR(c->Merge(inputs, c->input(1), &inputs));\n\n      ShapeHandle last_dim = c->Vector(c->Dim(inputs, -1));\n\n      ShapeHandle min_max;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &min_max));\n      TF_RETURN_IF_ERROR(c->Merge(min_max, last_dim, &min_max));\n      TF_RETURN_IF_ERROR(c->Merge(c->input(3), min_max, &min_max));\n\n      c->set_output(0, inputs);\n      c->set_output(1, min_max);\n      c->set_output(2, min_max);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Fingerprint\")\n    .Input(\"data: T\")\n    .Input(\"method: string\")\n    .Output(\"fingerprint: uint8\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n\n      DimensionHandle fingerprint_size;\n      const Tensor* method = c->input_tensor(1);\n      if (method == nullptr) {\n        fingerprint_size = c->UnknownDim();\n      } else {\n        if (method->dims() != 0) {\n          return errors::InvalidArgument(\"`method` must be rank 0: \",\n                                         method->shape());\n        }\n        const string& method_string = method->scalar<tstring>()();\n        if (method_string != \"farmhash64\") {\n          return errors::InvalidArgument(\"Unsupported method: \", method_string);\n        }\n        fingerprint_size = c->MakeDim(sizeof(uint64));\n      }\n\n      DimensionHandle batch = c->Dim(c->input(0), 0);\n      c->set_output(0, c->MakeShape({batch, fingerprint_size}));\n      return OkStatus();\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklConcat\")\n    .Input(\"concat_dim: int32\")\n    .Input(\"values: N * T\")\n    .Input(\"mkl_concat_dim: uint8\")\n    .Input(\"mkl_values: N * uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"N: int >= 2\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::ConcatShape(c, c->num_inputs() - 3);\n    })\n    .Doc(R\"doc(\nMKL version of Concat operator. Uses MKL DNN APIs to perform concatenation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\n// Deprecated op registrations:\n\n// The following can be deleted after 10mar2017.\nREGISTER_OP(\"BatchMatrixDiag\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Deprecated(14, \"Use MatrixDiag\")\n    .SetShapeFn(shape_inference::UnknownShape);\nREGISTER_OP(\"BatchMatrixSetDiag\")\n    .Input(\"input: T\")\n    .Input(\"diagonal: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Deprecated(14, \"Use MatrixSetDiag\")\n    .SetShapeFn(shape_inference::UnknownShape);\nREGISTER_OP(\"BatchMatrixDiagPart\")\n    .Input(\"input: T\")\n    .Output(\"diagonal: T\")\n    .Attr(\"T: type\")\n    .Deprecated(14, \"Use MatrixDiagPart\")\n    .SetShapeFn(shape_inference::UnknownShape);\nREGISTER_OP(\"BatchMatrixBandPart\")\n    .Input(\"input: T\")\n    .Input(\"num_lower: int64\")\n    .Input(\"num_upper: int64\")\n    .Output(\"band: T\")\n    .Attr(\"T: type\")\n    .Deprecated(14, \"Use MatrixBandPart\")\n    .SetShapeFn(shape_inference::UnknownShape);\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for array_ops.\"\"\"\nimport re\nimport time\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import config\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import list_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensor\nfrom tensorflow.python.platform import test as test_lib\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BatchMatrixTransposeTest(test_util.TensorFlowTestCase):\n\n  def testNonBatchMatrix(self):\n    matrix = [[1, 2, 3], [4, 5, 6]]  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n    transposed = array_ops.matrix_transpose(matrix)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testConjugate(self):\n    m = [[1 + 1j, 2 + 2j, 3 + 3j], [4 + 4j, 5 + 5j, 6 + 6j]]\n    expected_transposed = [[1 - 1j, 4 - 4j], [2 - 2j, 5 - 5j], [3 - 3j, 6 - 6j]]\n    matrix = ops.convert_to_tensor(m)\n    transposed = array_ops.matrix_transpose(matrix, conjugate=True)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testBatchMatrix(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    batch_matrix = [matrix_0, matrix_1]  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n    transposed = array_ops.matrix_transpose(batch_matrix)\n    self.assertEqual((2, 3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testNonBatchMatrixDynamicallyDefined(self):\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    matrix = constant_op.constant([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(matrix))\n\n  def testBatchMatrixDynamicallyDefined(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    batch_matrix = constant_op.constant([matrix_0, matrix_1])  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(batch_matrix))\n\n  def testTensorWithStaticRankLessThanTwoRaisesBecauseNotAMatrix(self):\n    vector = [1, 2, 3]\n    with self.assertRaisesRegex(ValueError, \"should be a \"):\n      array_ops.matrix_transpose(vector)\n\n  def testNarrowMatrixConjugateTranspose(self):\n    for dtype in (dtypes.float32, dtypes.float64):\n      for conjugate in (True, False):\n        with self.subTest(complex_type=dtype, conjugate=conjugate):\n          vector = math_ops.complex(\n              constant_op.constant(0, dtype=dtype),\n              math_ops.range(96, dtype=dtype))\n          column_vector = array_ops.expand_dims(vector, axis=-1)\n          row_vector = array_ops.expand_dims(vector, axis=0)\n          narrow_matrix = array_ops.tile(column_vector, [1, 2])  # [96, 2]\n          expected_transposed = array_ops.tile(row_vector, [2, 1])  # [2, 96]\n          if conjugate:\n            expected_transposed = -expected_transposed\n\n          transposed = array_ops.matrix_transpose(\n              narrow_matrix, conjugate=conjugate)\n\n          self.assertEqual((2, 96), transposed.get_shape())\n          self.assertAllEqual(expected_transposed, transposed)\n\n\nclass BooleanMaskTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.rng = np.random.RandomState(42)\n\n  def CheckVersusNumpy(self, ndims_mask, arr_shape, make_mask=None, axis=None):\n    \"\"\"Check equivalence between boolean_mask and numpy masking.\"\"\"\n    if make_mask is None:\n      make_mask = lambda shape: self.rng.randint(0, 2, size=shape).astype(bool)\n    arr = np.random.rand(*arr_shape)\n    mask = make_mask(arr_shape[:ndims_mask])\n    if axis is not None:\n      mask = make_mask(arr_shape[axis:ndims_mask + axis])\n    if axis is None or axis == 0:\n      masked_arr = arr[mask]\n    elif axis == 1:\n      masked_arr = arr[:, mask]\n    elif axis == 2:\n      masked_arr = arr[:, :, mask]\n    masked_tensor = array_ops.boolean_mask(arr, mask, axis=axis)\n\n    # Leading dimension size of masked_tensor is always unknown until runtime\n    # since we don't how many elements will be kept.\n    leading = 1 if axis is None else axis + 1\n    self.assertAllEqual(masked_tensor.get_shape()[leading:],\n                        masked_arr.shape[leading:])\n\n    self.assertAllClose(masked_arr, masked_tensor)\n\n  def testMaskDim1ArrDim2Axis1(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim2ArrDim2Axis1(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim1ArrDim1(self):\n    ndims_mask = 1\n    for arr_shape in [(1,), (2,), (3,), (10,)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim1ArrDim2(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim2(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim3(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1, 1), (1, 2, 2), (2, 2, 1)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testEmptyInput2D(self):\n    mask = np.array([True, False])\n    arr = np.array([[], []]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyInput1D(self):\n    mask = np.array([]).astype(bool)\n    arr = np.array([]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyOutput(self):\n    make_mask = lambda shape: np.zeros(shape, dtype=bool)\n    for ndims_mask in range(1, 4):\n      for ndims_arr in range(ndims_mask, ndims_mask + 3):\n        for _ in range(3):\n          with self.subTest(ndims_mask=ndims_mask, ndims_arr=ndims_arr, _=_):\n            arr_shape = np.random.randint(1, 5, size=ndims_arr)\n            self.CheckVersusNumpy(ndims_mask, arr_shape, make_mask=make_mask)\n\n  def testWorksWithDimensionsEqualToNoneDuringGraphBuild(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(ph_tensor, ph_mask):\n      return array_ops.boolean_mask(ph_tensor, ph_mask)\n\n    f = func.get_concrete_function(\n        tensor_spec.TensorSpec(None, dtypes.int32),\n        tensor_spec.TensorSpec([None], dtypes.bool))\n    arr = np.array([[1, 2], [3, 4]], np.int32)\n    mask = np.array([False, True])\n    masked_tensor = f(arr, mask)\n    self.assertAllEqual(masked_tensor, arr[mask])\n\n  def testMaskDimensionsSetToNoneRaises(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(tensor, mask):\n      return array_ops.boolean_mask(tensor, mask)\n\n    with self.assertRaisesRegex(ValueError, \"dimensions must be specified\"):\n      _ = func.get_concrete_function(\n          tensor_spec.TensorSpec([None, 2], dtypes.int32),\n          tensor_spec.TensorSpec(None, dtypes.bool))\n\n  def testMaskHasMoreDimsThanTensorRaises(self):\n    mask = [[True, True], [False, False]]\n    tensor = [1, 2, 3, 4]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskIsScalarRaises(self):\n    mask = True\n    tensor = 1\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"mask.*scalar\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskShapeDifferentThanFirstPartOfTensorShapeRaises(self):\n    mask = [True, True, True]\n    tensor = [[1, 2], [3, 4]]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testStringMask(self):\n    # Reproduces b/111171330, where the optimized boolean_mask graph would\n    # be incorrectly placed on GPU.\n    config.set_optimizer_experimental_options({\"shape_optimization\": True})\n\n    @def_function.function\n    def func(tile_input):\n      string_tensor = array_ops.tile([[\"hello\"]], tile_input)\n      bool_tensor = array_ops.tile([[True]], tile_input)\n      masked_tensor = array_ops.boolean_mask(string_tensor, bool_tensor)\n      return masked_tensor\n\n    result = func([2, 2])\n    self.assertAllEqual([b\"hello\", b\"hello\", b\"hello\", b\"hello\"], result)\n\n  def testMaskWithAxisTensor(self):\n\n    @def_function.function(autograph=False)\n    def f():\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True],\n                                    axis=constant_op.constant(\n                                        0, dtype=dtypes.int32))\n\n    self.assertAllEqual(self.evaluate(f()), [1, 3])\n\n  def testMaskWithAxisNonConstTensor(self):\n\n    @def_function.function(\n        autograph=False,\n        input_signature=[\n            tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n        ])\n    def f(axis):\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True], axis=axis)\n\n    self.assertAllEqual(\n        self.evaluate(f(constant_op.constant(0, dtype=dtypes.int32))), [1, 3])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass OperatorShapeTest(test_util.TensorFlowTestCase):\n\n  def testExpandScalar(self):\n    scalar = \"hello\"\n    scalar_expanded = array_ops.expand_dims(scalar, [0])\n    self.assertEqual(scalar_expanded.get_shape(), (1,))\n\n  def testSqueezeScalar(self):\n    scalar = \"hello\"\n    scalar_squeezed = array_ops.squeeze(scalar, ())\n    self.assertEqual(scalar_squeezed.get_shape(), ())\n\n  def testSqueezeMatrix(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, [0])\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n    with self.assertRaisesRegex(\n        Exception, \"Can not squeeze dim.1., expected a dimension of 1, got 3\"):\n      matrix_squeezed = array_ops.squeeze(matrix, [1])\n\n  def testSqueezeScalarDim(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, 0)\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n  def testExpandDimsWithNonScalarDim(self):\n    with self.assertRaisesRegex(Exception,\n                                \"must be a tensor with a single value\"):\n      array_ops.expand_dims(1, axis=[0, 1])\n\n  def testReshapeWithManyDims(self):\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"too many dimensions\"):\n      self.evaluate(\n          array_ops.reshape(\n              tensor=[[1]],\n              shape=constant_op.constant([1 for i in range(254)],\n                                         dtype=dtypes.int64)))\n\n\n@test_util.with_eager_op_as_function\nclass ReverseV2Test(test_util.TensorFlowTestCase):\n\n  def testReverse0DimAuto(self):\n    x_np = 4\n    for use_gpu in [False, True]:\n      with self.subTest(use_gpu=use_gpu):\n        with self.cached_session(use_gpu=use_gpu):\n          x_tf = self.evaluate(array_ops.reverse_v2(x_np, []))\n          self.assertAllEqual(x_tf, x_np)\n\n  def _reverse1DimAuto(self, np_dtype):\n    x_np = np.array([1, 200, 3, 40, 5], dtype=np_dtype)\n\n    for use_gpu in [False, True]:\n      for axis_dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(use_gpu=use_gpu, axis_dtype=axis_dtype):\n          x_tf = self.evaluate(\n              array_ops.reverse_v2(x_np,\n                                   constant_op.constant([0], dtype=axis_dtype)))\n          self.assertAllEqual(x_tf, np.asarray(x_np)[::-1])\n\n  def _reverse2DimAuto(self, np_dtype):\n    x_np = np.array([[1, 200, 3], [4, 5, 60]], dtype=np_dtype)\n\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for use_gpu in [False, True]:\n        for axis_dtype in [dtypes.int32, dtypes.int64]:\n          with self.subTest(\n              reverse_f=reverse_f, use_gpu=use_gpu, axis_dtype=axis_dtype):\n            x_tf_1 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([0], dtype=axis_dtype)))\n            x_tf_2 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-2], dtype=axis_dtype)))\n            x_tf_3 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1], dtype=axis_dtype)))\n            x_tf_4 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-1], dtype=axis_dtype)))\n            x_tf_5 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1, 0], dtype=axis_dtype)))\n            self.assertAllEqual(x_tf_1, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_2, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_3, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_4, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_5, np.asarray(x_np)[::-1, ::-1])\n\n  # This test covers the axis validation in the shape function\n  # (no eval())\n  def testInvalidAxis(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [-30])\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [2])\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        r\"axis 0 specified more than once|axis 0 was repeated\"):\n      array_ops.reverse_v2(x_np, [0, -2])\n\n  # This is the version of reverse that uses axis indices rather than\n  # bool tensors\n  # TODO(b/32254538): Change this test to use array_ops.reverse\n  #\n  # Note: this test passes placeholder as constant axis is validated\n  # in shape function (see testInvalidAxis)\n  def testInvalid(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n\n    @def_function.function\n    def func(ax):\n      return array_ops.reverse_v2(x_np, ax)\n\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([-30])\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([2])\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"(axis 0 specified more than once|canonicalized axis 0 was repeated.)\"):\n      func([0, -2])\n\n  def testReverse1DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse1DimAuto(dtype)\n\n  def testReverse2DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse2DimAuto(dtype)\n\n  def testReverseRowsOf3Channels(self):\n    \"\"\"Tests optimized code for reversing rows with last dim size = 3.\"\"\"\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseRowsOf4Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 4, dtype=np.float32),\n                newshape=(outer_size, middle_size, 4))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseColumnsOf3Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in list(range(50)) + [100000]:\n        for middle_size in (1, 2):\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [0]))\n            np_answer = x_np[::-1, :, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseInvalidShape(self):\n    x = np.ndarray(shape=[0, 1, 1])\n    v = array_ops.reverse_v2(x, axis=[1])\n    self.assertAllEqual(self.evaluate(v), v)\n\n\nclass MeshgridTest(test_util.TensorFlowTestCase):\n\n  def _compareDiff(self, x, y, use_gpu):\n    for index in (\"ij\", \"xy\"):\n      numpy_out = np.meshgrid(x, y, indexing=index)\n      tf_out = array_ops.meshgrid(x, y, indexing=index)\n      with self.cached_session(use_gpu=use_gpu):\n        for xx, yy in zip(numpy_out, tf_out):\n          self.assertAllEqual(xx, yy)\n\n  def _compareDiffType(self, n, np_dtype, use_gpu):\n    inputs = []\n    for index in (\"ij\", \"xy\"):\n      for _ in range(n):\n        x = np.linspace(-10, 10, 5).astype(np_dtype)\n        if np_dtype in (np.complex64, np.complex128):\n          x += 1j\n        inputs.append(x)\n      numpy_out = np.meshgrid(*inputs, indexing=index)\n      with test_util.device(use_gpu=use_gpu):\n        tf_out = array_ops.meshgrid(*inputs, indexing=index)\n        for x_np, x_tf in zip(numpy_out, tf_out):\n          self.assertAllEqual(x_np, x_tf)\n\n  def testCompare(self):\n    for t in (np.float16, np.float32, np.float64, np.int32, np.int64,\n              np.complex64, np.complex128):\n      with self.subTest(t=t):\n        self._compareDiffType(2, t, False)\n        self._compareDiffType(3, t, False)\n\n        x = [1, 2, 3]\n        y = [4, 5]\n\n        a = [[1, 1], [1, 1]]\n\n        self._compareDiff(x, y, False)\n        self._compareDiff(x, a, False)\n\n\nclass StridedSliceChecker(object):\n  \"\"\"Check a given tensor against the numpy result.\"\"\"\n\n  REF_TENSOR = np.arange(1, 19, dtype=np.float32).reshape(3, 2, 3)\n  REF_TENSOR_ALIGNED = np.arange(1, 97, dtype=np.float32).reshape(3, 4, 8)\n\n  def __init__(self, test, x, tensor_type=dtypes.int32, check_type_infer=True):\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    if tensor_type.is_bool:\n      self.x_np = np.array(x % 3).astype(np.bool_)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.test = test\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n    self.check_type_infer = check_type_infer\n\n  def __getitem__(self, spec):\n    op = self.x.__getitem__(spec)\n\n    def eval_if_tensor(x):\n      try:\n        return self.test.evaluate(x)\n      except (AttributeError, TypeError, ValueError):\n        return x\n\n    def casts_to_bool_nparray(x):\n      try:\n        return np.asarray(x).dtype == bool\n      except NotImplementedError:\n        return False\n\n    if isinstance(spec, bool) or \\\n      (isinstance(spec, ops.Tensor) and spec.dtype == dtypes.bool) or \\\n      (isinstance(spec, np.ndarray) and spec.dtype == bool) or \\\n      (isinstance(spec, (list, tuple)) and casts_to_bool_nparray(spec)):\n      tensor = self.test.evaluate(op)\n      np_spec = eval_if_tensor(spec)\n      self.test.assertAllEqual(self.x_np[np_spec], tensor)\n      return tensor\n\n    if not isinstance(spec, (list, tuple)):\n      spec = [spec]\n\n    tensor = self.test.evaluate(op)\n\n    # Make a numpy spec that pre-evals the tensors\n    np_specs = []\n\n    for s in spec:\n      if isinstance(s, slice):\n        start = eval_if_tensor(s.start)\n        stop = eval_if_tensor(s.stop)\n        step = eval_if_tensor(s.step)\n        np_specs.append(slice(start, stop, step))\n      else:\n        np_specs.append(eval_if_tensor(s))\n\n    self.test.assertAllEqual(self.x_np[tuple(np_specs)], tensor)\n    if self.check_type_infer:\n      self.test.assertAllEqual(tensor.shape, op.get_shape())\n    return tensor\n\n\nSTRIDED_SLICE_TYPES = [\n    dtypes.int32, dtypes.int64, dtypes.int16, dtypes.int8, dtypes.uint8,\n    dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128,\n    dtypes.bool, dtypes.bfloat16\n]\n\n\nclass StridedSliceTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the strided slice operation with variants of slices.\"\"\"\n\n  def test_basic_slice(self):\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n        _ = checker[:, :, :]\n        # Various ways of representing identity slice\n        _ = checker[:, :, :]\n        _ = checker[::, ::, ::]\n        _ = checker[::1, ::1, ::1]\n        # Not zero slice\n        _ = checker[::1, ::5, ::2]\n        # Reverse in each dimension independently\n        _ = checker[::-1, :, :]\n        _ = checker[:, ::-1, :]\n        _ = checker[:, :, ::-1]\n        ## negative index tests i.e. n-2 in first component\n        _ = checker[-2::-1, :, ::1]\n        # negative index tests i.e. n-2 in first component, non-unit stride\n        _ = checker[-2::-1, :, ::2]\n\n        # Check rank-0 examples\n        checker2 = StridedSliceChecker(self, 5, tensor_type=tensor_type)\n        _ = checker2[None]\n        _ = checker2[...]\n        _ = checker2[tuple()]\n\n  def testInt64GPU(self):\n    if not test_util.is_gpu_available():\n      self.skipTest(\"No GPU available\")\n\n    with test_util.force_gpu():\n      x = constant_op.constant([1., 2., 3.])\n      begin = constant_op.constant([2], dtype=dtypes.int64)\n      end = constant_op.constant([3], dtype=dtypes.int64)\n      strides = constant_op.constant([1], dtype=dtypes.int64)\n      s = array_ops.strided_slice(x, begin, end, strides)\n      self.assertAllEqual([3.], self.evaluate(s))\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testTensorSliceEagerMemory(self):\n    with context.eager_mode():\n      inputs = constant_op.constant([[[1], [2], [3], [4]]],\n                                    dtype=dtypes.float32)\n      # Tests that slicing an EagerTensor doesn't leak memory\n      inputs[0]  # pylint: disable=pointless-statement\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testVariableSliceEagerMemory(self):\n    with context.eager_mode():\n      v = variables.Variable([1., 2.])\n      v[0]  # pylint: disable=pointless-statement\n\n  def testDegenerateSlices(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      # degenerate by offering a forward interval with a negative stride\n      _ = checker[0:-1:-1, :, :]\n      # degenerate with a reverse interval with a positive stride\n      _ = checker[-1:0, :, :]\n      # empty interval in every dimension\n      _ = checker[-1:0, 2:2, 2:3:-1]\n      # empty first dimension only (used to break for aligned tensors).\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      _ = checker[1:0]\n\n  def testSliceWithUndefinedDimension(self):\n    t = constant_op.constant([1, 2, 3])\n    d = tensor_shape.Dimension(None)\n    self.assertAllEqual(t[d:d:d], t)\n\n  def testEllipsis(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2], [3, 4], [5, 6]]], [[[7, 8], [9, 10], [11, 12]]]]]\n      checker = StridedSliceChecker(self, raw)\n\n      _ = checker[0:]\n      # implicit ellipsis\n      _ = checker[0:, ...]\n      # ellipsis alone\n      _ = checker[...]\n      # ellipsis at end\n      _ = checker[0:1, ...]\n      # ellipsis at begin\n      _ = checker[..., 0:1]\n      # ellipsis at middle\n      _ = checker[0:1, ..., 0:1]\n      # multiple ellipses not allowed\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"Multiple ellipses\"):\n        _ = checker[..., :, ...].eval()\n\n  def testShrink(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      _ = checker[:, :, :, :, 3]\n      _ = checker[..., 3]\n      _ = checker[:, 0]\n      _ = checker[:, :, 0]\n\n  def testBothNewAxisAndShrink(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def func(inp):\n        return inp[array_ops.newaxis, :, 0]\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec([2, 2], dtypes.int16))\n\n      # TODO(b/190416665): Allow the constant to be eagerly copied/created on\n      # the GPU.\n      with ops.device(\"CPU\"):\n        ones = constant_op.constant([[1, 1], [1, 1]], dtypes.int16)\n      self.assertAllEqual([[1, 1]], self.evaluate(f(ones)))\n\n  def testTensorIndexing(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw, check_type_infer=False)\n      bar = constant_op.constant(2)\n      bar2 = constant_op.constant(3)\n      _ = checker[..., bar:bar2]\n      _ = checker[..., bar]\n      _ = checker[..., 3]\n      _ = checker[..., 2**64 // 2**63]  # Test longs in Python 2\n\n  def testTensorIndexingTypeError(self):\n    with self.session():\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      expected = re.escape(array_ops._SLICE_TYPE_ERROR)\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[\"foo\"]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(\"foo\")]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[0.0]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(0.0)]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant([1, 2, 3])]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[[2.1, -0.7, 1.5]]\n\n  def testExpand(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      # new axis (followed by implicit ellipsis)\n      _ = checker[np.newaxis]\n      # newaxis after ellipsis\n      _ = checker[..., np.newaxis]\n      # newaxis in between ellipsis and explicit range\n      _ = checker[..., np.newaxis, :]\n      _ = checker[:, ..., np.newaxis, :, :]\n      # Reverse final dimension with new axis\n      _ = checker[:, :, np.newaxis, :, 2::-1]\n      # Ellipsis in middle of two newaxis\n      _ = checker[np.newaxis, ..., np.newaxis]\n\n  def testExpandVariable(self):\n    with test_util.device(use_gpu=True):\n      x = variables.Variable(7, dtype=dtypes.int32)\n      self.evaluate(x.initializer)\n      y = self.evaluate(x[None])\n      self.assertEqual(y.shape, (1,))\n      self.assertAllEqual(y, (7,))\n\n  def testOptimizedCases(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      # Identity\n      _ = checker[:]\n      # Identity\n      _ = checker[...]\n      # Identity\n      _ = checker[np.newaxis, ..., np.newaxis]\n      # First axis slice\n      _ = checker[1:]\n      # First axis slice\n      _ = checker[np.newaxis, 1:]\n\n  def testMasks(self):\n    with test_util.device(use_gpu=True):\n      scalar = np.array(0)\n      # Test tensor type mask\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      _ = checker[checker.x > 2]\n      _ = checker[checker.x <= 5]\n      _ = checker[ops.convert_to_tensor(scalar)]\n\n      # Test numpy array type mask\n      raw = np.array([[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n                       [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23,\n                                                              24]]]]])\n      checker1 = StridedSliceChecker(self, raw)\n      _ = checker1[raw >= 4]\n      _ = checker1[raw < 19]\n      _ = checker1[scalar]\n\n      # Test boolean and non boolean cases\n      mask = np.array([True, False, True])\n      raw1 = np.array([[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]])\n      checker2 = StridedSliceChecker(self, raw1)\n      _ = checker2[mask]\n      _ = checker2[ops.convert_to_tensor(mask)]\n\n  def test_int16_indices(self):\n\n    def _int16(i):\n      return constant_op.constant(i, dtype=dtypes.int16)\n\n    def _int64(i):\n      return constant_op.constant(i, dtype=dtypes.int64)\n\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n\n        _ = checker[_int16(1)]\n\n        with self.assertRaises(Exception):\n          _ = checker[_int16(1)::1, :, 1:_int64(3):2]\n        with self.assertRaises(Exception):\n          _ = checker[:, _int16(1):_int16(5):-1, :]\n        with self.assertRaises(Exception):\n          _ = checker[::_int64(1), _int64(1):10:_int16(3), ::_int64(2)]\n\n        _ = checker[::_int16(1), _int16(1)::_int16(5), ::2]\n        _ = checker[_int16(1):_int16(5):_int16(2), 1:2, :]\n\n\nclass StridedSliceShapeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the shape inference of StridedSliceShapes.\"\"\"\n\n  def testUnknown(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f(x):\n        y = x[...]\n        self.assertAllEqual(y.get_shape().ndims, None)\n\n      _ = f.get_concrete_function(tensor_spec.TensorSpec(None, dtypes.float32))\n\n  def tensorShapeEqual(self, x, y):\n    self.assertTrue(x is not None and y is not None or x is None and y is None)\n    self.assertEqual(x.as_list(), y.as_list())\n\n  def testTensorShapeUncertain(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f1(x):\n        y = x[3:5]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f2(x):\n        y = x[3:5, :, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f3(x):\n        y = x[3:5, 3:4, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f4(x):\n        y = x[3:5, :, 5:10]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 2]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f5(x):\n        y = x[3:5, :, 50:3]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 0]))\n\n      _ = f5.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f6(x):\n        y = x[3:5, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f6.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f7(x):\n        y = x[1:5:2, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f7.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f8(x):\n        y = x[:5:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f8.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f9(x):\n        y = x[:2:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([1, None, 1, 0]))\n\n      _ = f9.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f10(x):\n        y = x[::-1, :, array_ops.newaxis, ::-2]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([5, None, 1, 4]))\n\n      _ = f10.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n  def testTensorValuedIndexShape(self):\n    with self.session():\n\n      @def_function.function\n      def f1(x, y):\n        z = x[y]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f2(x, y):\n        z = x[y, ::-1]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f3(x, y):\n        z = x[y, ::-2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([2, 7]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f4(x, y, s):\n        z = x[y, s:2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([None,\n                                                                       7]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n\nclass GradSliceChecker(object):\n  \"\"\"Tests that we can compute a gradient for var^2.\"\"\"\n\n  def __init__(self, test, var, varnp, use_tape):\n    self.test = test\n    self.var = var\n    self.varnp = varnp\n    self.use_tape = use_tape\n\n  def __getitem__(self, spec):\n    with test_util.AbstractGradientTape(\n        use_tape=self.use_tape, persistent=True) as tape:\n      tape.watch(self.var)\n      val = self.var * self.var\n      slice_var = self.var[spec]\n      slice_val = val[spec]\n\n      # compute analytic 2nd derivative\n      analytic_grad2 = 2 * slice_val\n\n      dy = variables.Variable(\n          array_ops.ones_like(slice_var, dtype=dtypes.float32))\n      assign = dy.assign(slice_var)\n\n      slice_val_grad = tape.gradient(slice_val, self.var, [dy])\n      slice_val_grad2 = tape.gradient(slice_val_grad, dy, [self.var])\n    self.test.evaluate(assign)\n    slice_val_grad_evaled, slice_val_grad2_evaled = (\n        self.test.evaluate([slice_val_grad, slice_val_grad2]))\n    analytic_grad2_evaled = self.test.evaluate(analytic_grad2)\n    self.test.assertAllEqual(slice_val_grad2_evaled, analytic_grad2_evaled)\n\n    # compute analytic gradient for slice\n    np_val_grad = (2 * self.varnp * self.varnp)\n    np_sliceval_grad = np.zeros(self.var.get_shape())\n    if isinstance(spec, ops.Tensor):\n      spec = self.test.evaluate(spec)\n    np_sliceval_grad[spec] = np_val_grad[spec]\n    # verify gradient\n    self.test.assertAllEqual(slice_val_grad_evaled, np_sliceval_grad)\n\n\nclass StridedSliceGradTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n  \"\"\"Test that strided slice's custom gradient produces correct gradients.\"\"\"\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  @test_util.disable_xla(\n      \"b/210077724: Auto-clustering with where op isn't supported. Has loose \"\n      \"output shape bounds\")\n  def testGradient(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(\n          array_ops.reshape(\n              math_ops.range(1, 97, 1, dtype=dtypes.float32), shape=(6, 4, 4)))\n      self.evaluate(var.initializer)\n\n      raw = np.array(range(1, 97, 1)).reshape((6, 4, 4))\n      grad = GradSliceChecker(self, var, raw, use_tape)\n      _ = grad[2:6:2, 1:3, 1:3]\n      _ = grad[3:0:-2, 1:3, 1:3]\n      _ = grad[3:0:-2, array_ops.newaxis, 1:3, 2, array_ops.newaxis]\n      _ = grad[3:0:-2, 1:3, 2]\n      _ = grad[:, -1, :]\n      _ = grad[:, -2, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, -200, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, 200, :]\n\n      # Test numpy array type mask\n      _ = grad[raw > 51]\n      # Test tensor type mask\n      _ = grad[ops.convert_to_tensor(raw) <= 76]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testGradientZero(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(8.)\n      self.evaluate(var.initializer)\n      grad = GradSliceChecker(self, var, np.array(8), use_tape)\n      _ = grad[tuple()]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testInt64Indices(self, use_tape):\n    with test_util.AbstractGradientTape(use_tape=use_tape) as tape:\n      a = math_ops.range(3, dtype=dtypes.float32)\n      tape.watch(a)\n      index = constant_op.constant(1, dtype=dtypes.int64)\n      b = 2. * a[index]\n    grad = tape.gradient(b, a)\n    self.assertAllEqual(self.evaluate(grad), [0., 2., 0.])\n\n\nclass StridedSliceGradTypeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test varied index types and host located memory.\"\"\"\n\n  def testHostVsDevice(self):\n    var2 = variables.Variable(\n        array_ops.reshape(\n            math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32),\n            shape=(4, 1, 1)))\n    varshape = variables.Variable([6, 4, 4], dtype=dtypes.int32)\n    begin = constant_op.constant([0, 0, 0])\n    end = constant_op.constant([4, 1, 1])\n    strides = constant_op.constant([1, 1, 1])\n    foo = array_ops.strided_slice_grad(varshape, begin, end, strides, var2)\n    self.evaluate(var2.initializer)\n    self.evaluate(varshape.initializer)\n    self.evaluate(foo)\n\n  def testInt64Shape(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int64)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                      original_dy)\n    self.evaluate(dx)\n\n  def testMixedIndexTypes(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int32)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    with self.assertRaises((TypeError, errors_impl.InvalidArgumentError)):\n      dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                        original_dy)\n      self.evaluate(dx)\n\n\nclass BenchmarkSlice(object):\n\n  def __init__(self, tensor):\n    self.tensor = tensor\n\n  def __getitem__(self, x):\n    return self.tensor[x]\n\n\nclass StridedSliceBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark new strided slice operation on non-trivial case.\"\"\"\n\n  def run_and_time(self, slice_op):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(10):\n      _ = self.evaluate(slice_op)\n    iters = 1000\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(slice_op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / 1000.0)\n\n  def make_variable(self):\n    n = 256\n    shape = (n, n, n)\n    items = n**3\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtypes.float32)\n    return var\n\n  def benchmark_strided_slice_skip(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[::2, ::1, ::2]\n      self.run_and_time(slice_op)\n\n  def benchmark_strided_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n  def benchmark_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      slice_op = var[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n\nclass StridedSliceAssignChecker(object):\n\n  def __init__(self, test, x, tensor_type=dtypes.float32, use_resource=False):\n    self.tensor_type = tensor_type\n    self.test = test\n    self._use_resource = use_resource\n\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n\n  def __setitem__(self, index, value):\n    value = np.array(value).astype(self.tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if self.tensor_type.is_complex:\n      value -= 1j * value\n\n    with test_util.device(use_gpu=True):\n      if self._use_resource:\n        var = resource_variable_ops.ResourceVariable(self.x)\n      else:\n        var = variables.Variable(self.x)\n      self.test.evaluate(var.initializer)\n      val = self.test.evaluate(var[index].assign(value))\n      # val_copy is used to check that tf.compat.v1.assign works equivalently\n      # to the assign method above.\n      val_copy = self.test.evaluate(state_ops.assign(var[index], value))\n      valnp = np.copy(self.x_np)\n      valnp[index] = np.array(value)\n      self.test.assertAllEqual(val, valnp)\n      self.test.assertAllEqual(val_copy, valnp)\n\n\nclass SliceAssignTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  def testInvalidSlice(self):\n    foo = constant_op.constant([1, 2, 3])\n    with self.assertRaisesRegex(AttributeError, \"no attribute 'assign'\"):\n      bar = foo[:2].assign(constant_op.constant([1, 2]))\n      self.evaluate(bar)\n\n  def doTestSliceAssign(self, use_resource):\n    for dtype in STRIDED_SLICE_TYPES:\n      with self.subTest(dtype=dtype):\n        checker = StridedSliceAssignChecker(\n            self, [[1, 2, 3], [4, 5, 6]],\n            use_resource=use_resource,\n            tensor_type=dtype)\n        # Check if equal\n        checker[:] = [[10, 20, 30], [40, 50, 60]]\n        # Check trivial (1,1) shape tensor\n        checker[1:2, 1:2] = [[66]]\n        # shrinks shape changes\n        checker[1:2, 1] = [66]\n        checker[1, 1:2] = [66]\n        checker[1, 1] = 66\n        # newaxis shape changes\n        checker[:, None, :] = [[[10, 20, 30]], [[40, 50, 50]]]\n        # shrink and newaxis\n        checker[None, None, 0, 0:1] = [[[99]]]\n        # Non unit strides\n        checker[::1, ::-2] = [[3, 33], [4, 44]]\n        # degenerate interval\n        checker[8:10, 0] = []\n        checker[8:10, 8:10] = [[]]\n    # Assign vector to scalar (rank-0) using newaxis\n    checker2 = StridedSliceAssignChecker(self, 222)\n    checker2[()] = 6  # no indices\n    checker2[...] = 6  # ellipsis\n    checker2[None] = [6]  # new axis\n\n  def doTestSliceAssignWithBroadcasting(self, use_resource):\n    for dtype in STRIDED_SLICE_TYPES:\n      with self.subTest(dtype=dtype):\n        checker = StridedSliceAssignChecker(\n            self, [[1, 2, 3], [4, 5, 6]],\n            use_resource=use_resource,\n            tensor_type=dtype)\n        # Broadcast to full LHS.\n        checker[:] = [[40, 50, 60]]\n        # Assign a trivial (1,1) tensor.\n        checker[1:2, 1:2] = 66\n        # Broadcast with shrink axis shape changes.\n        checker[1:2, 1] = 66\n        checker[1, 1:2] = 66\n        # Broadcast with newaxis shape changes.\n        checker[:, None, :] = [10, 20, 30]\n        # Broadcast with both shrink and newaxis.\n        checker[None, None, 0, 0:1] = 99\n        # Broadcast with non-unit strides.\n        checker[::1, ::-2] = [[4, 44]]\n        # Broadcast a degenerate interval.\n        checker[8:10, 8:10] = []\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssign(self):\n    self.doTestSliceAssign(use_resource=False)\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssignWithBroadcasting(self):\n    self.doTestSliceAssignWithBroadcasting(use_resource=False)\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssignResource(self):\n    self.doTestSliceAssign(use_resource=True)\n\n  def testTypeError(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = variables.VariableV1(init_val)\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_small_val))\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_large_val))\n\n  def testTypeErrorResource(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = resource_variable_ops.ResourceVariable(init_val)\n    self.evaluate(v.initializer)\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_large_val))\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_small_val))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateWithInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with input-forwarding taking effect.\"\"\"\n    @def_function.function\n    def assign(x):\n      y = x + 1\n      return gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0])\n    self.assertAllEqual([0, 1], self.evaluate(assign(array_ops.zeros([2]))))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateWithInputForwardInt32(self):\n    \"\"\"Tests tensor_strided_slice_update with int32.\"\"\"\n    @def_function.function\n    def assign(x):\n      y = x + 1\n      return gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0])\n    self.assertAllEqual(\n        [0, 1], self.evaluate(assign(array_ops.zeros([2], dtype=dtypes.int32))))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateNoInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with no input-forwarding.\"\"\"\n    x = constant_op.constant([0.2, 0.3])\n    y = x + 1\n    # y's buffer won't be forwarded to z because y and z will be alive at the\n    # same time later.\n    z = gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0.4])\n    ans = y + z\n    self.assertAllClose([1.6, 2.6], self.evaluate(ans))\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGradSimple(self):\n    original = constant_op.constant([0.2, 0.3])\n    updates = constant_op.constant([0.4])\n    with backprop.GradientTape() as tape:\n      tape.watch([original, updates])\n      updated = gen_array_ops.tensor_strided_slice_update(\n          original, [0], [1], [1], updates)\n    d1, d2 = tape.gradient(updated, [original, updates],\n                           output_gradients=constant_op.constant([2.0, 3.0]))\n    self.assertAllClose([0.0, 3.0], d1)\n    self.assertAllClose([2.0], d2)\n\n  @parameterized.named_parameters(\n      (\"_%s\" % i, *args) for i, args in enumerate([  # pylint:disable=g-complex-comprehension\n          ([2, 5], [0, 1], [1, 0], [1, 2], [2], 0, 2, 0, 0, 1),\n          ([4], [5], [3], [1], [3], 1, 0, 0, 0, 0),\n          ([2, 2, 3, 2], [0, 0, 1], [1, 0, 2], [1, 0, 1], [2, 3], 0, 0, 2, 0, 5)\n      ]))\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGrad(\n      self, shape, begin, end, strides, updates_shape, *args):\n    with self.cached_session():\n      def f(a, b):\n        return gen_array_ops.tensor_strided_slice_update(\n            a, begin, end, strides, b, *args)\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [array_ops.zeros(shape), array_ops.ones(updates_shape)], delta=1.0)\n      self.assertAllClose(theoretical, numerical)\n\n  @parameterized.named_parameters((\"_%s\" % i, *args) for i, args in enumerate([  # pylint:disable=g-complex-comprehension\n      ([2, 5], [0, 1], [1, 0], [1, 2], [1], 0, 2, 0, 0,\n       1), ([4], [5], [3], [1], [], 1, 0, 0, 0, 0),\n      ([2, 2, 3, 2], [0, 0, 1], [1, 0, 2], [1, 0, 1], [2, 1], 0, 0, 2, 0, 5)\n  ]))\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateWithBroadcastingGrad(self, shape, begin, end,\n                                                       strides, updates_shape,\n                                                       *args):\n    with self.cached_session():\n\n      def f(a, b):\n        return gen_array_ops.tensor_strided_slice_update(\n            a, begin, end, strides, b, *args)\n\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [array_ops.zeros(shape),\n              array_ops.ones(updates_shape)], delta=1.0)\n      self.assertAllClose(theoretical, numerical)\n\n\nclass ShapeSizeRankTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseShape(self):\n    t_value = [[0, 42], [24, 0]]\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t_value)))\n\n    t = constant_op.constant(t_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseShape(self):\n    sp_value = sparse_tensor.SparseTensorValue(\n        indices=((0, 1), (1, 0)), values=(42, 24), dense_shape=(2, 2))\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp_value)))\n\n    sp = sparse_tensor.SparseTensor.from_value(sp_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSizeDtype(self):\n    tensor = [1]\n    self.assertEqual(dtypes.int32, self.evaluate(array_ops.size(tensor)).dtype)\n    self.assertEqual(\n        dtypes.int64,\n        self.evaluate(array_ops.size(tensor, out_type=dtypes.int64)).dtype)\n\n\nclass SequenceMaskTest(test_util.TensorFlowTestCase):\n\n  def testExceptions(self):\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"`maxlen` must be scalar\"):\n        array_ops.sequence_mask([10, 20], [10, 20])\n\n  def testOneDimensionalWithMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([1, 3, 2]), 5)\n    self.assertAllEqual(res.get_shape(), [3, 5])\n    self.assertAllEqual(\n        res,\n        [[True, False, False, False, False], [True, True, True, False, False],\n         [True, True, False, False, False]])\n\n  def testOneDimensionalDtypeWithoutMaxlen(self):\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([0, 1, 4]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(\n        res, [[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]])\n\n  def testOneDimensionalWithoutMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([0, 1, 4]))\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(res,\n                        [[False, False, False, False],\n                         [True, False, False, False], [True, True, True, True]])\n\n  def testTwoDimensional(self):\n    res = array_ops.sequence_mask(constant_op.constant([[1, 3, 2]]), 5)\n    self.assertAllEqual(res.get_shape(), [1, 3, 5])\n    self.assertAllEqual(\n        res,\n        [[[True, False, False, False, False], [True, True, True, False, False],\n          [True, True, False, False, False]]])\n\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([[0, 1, 4], [1, 2, 3]]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [2, 3, 4])\n    self.assertAllEqual(\n        res,\n        [[[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]],\n         [[1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0]]])\n\n  def testDtypes(self):\n\n    def check_dtypes(lengths_dtype, maxlen_dtype):\n      res = array_ops.sequence_mask(\n          constant_op.constant([1, 3, 2], dtype=lengths_dtype),\n          constant_op.constant(5, dtype=maxlen_dtype))\n      self.assertAllEqual(res.get_shape(), [3, 5])\n      self.assertAllEqual(\n          res,\n          [[True, False, False, False, False], [True, True, True, False, False],\n           [True, True, False, False, False]])\n\n    check_dtypes(dtypes.int32, dtypes.int32)\n    check_dtypes(dtypes.int32, dtypes.int64)\n    check_dtypes(dtypes.int64, dtypes.int32)\n    check_dtypes(dtypes.int64, dtypes.int64)\n\n  def testOutputDtype(self):\n\n    def check_output_dtype(output_dtype):\n      res = self.evaluate(\n          array_ops.sequence_mask(\n              constant_op.constant([1, 3, 2], dtype=dtypes.int32),\n              constant_op.constant(5, dtype=dtypes.int32),\n              dtype=output_dtype))\n      self.assertAllEqual(\n          res,\n          self.evaluate(\n              math_ops.cast([[True, False, False, False, False],\n                             [True, True, True, False, False],\n                             [True, True, False, False, False]], output_dtype)))\n\n    check_output_dtype(dtypes.bool)\n    check_output_dtype(\"bool\")\n    check_output_dtype(np.bool_)\n    check_output_dtype(dtypes.int32)\n    check_output_dtype(\"int32\")\n    check_output_dtype(np.int32)\n    check_output_dtype(dtypes.float32)\n    check_output_dtype(\"float32\")\n    check_output_dtype(np.float32)\n    check_output_dtype(dtypes.int64)\n    check_output_dtype(\"float64\")\n    check_output_dtype(np.float64)\n\n\nclass ConcatSliceResourceTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConcatSlice(self):\n    r1 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"b\")\n    r2 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"c\")\n    c = array_ops.stack([r1, r2])\n    s = array_ops.strided_slice(c, [1], [2])\n    self.evaluate(test_ops.resource_create_op(s))\n    with self.assertRaises(errors.AlreadyExistsError):\n      self.evaluate(test_ops.resource_create_op(r2))\n\n\nclass IdentityTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_gpu_only\n  def testEagerIdentity(self):\n    with context.eager_mode():\n\n      def _test(x, y, device):\n        self.assertAllEqual(x.numpy(), y.numpy())\n        self.assertTrue(device in y.device.lower())\n\n      with test_util.force_gpu():\n        a = constant_op.constant([[2], [3]], dtype=dtypes.float32)\n      with test_util.force_gpu():\n        b = array_ops.identity(a)\n        _test(a, b, \"gpu\")\n      with test_util.force_cpu():\n        c = array_ops.identity(b)\n        _test(b, c, \"cpu\")\n      with test_util.force_cpu():\n        d = array_ops.identity(c)\n        _test(c, d, \"cpu\")\n      with test_util.force_gpu():\n        e = array_ops.identity(d)\n        _test(d, e, \"gpu\")\n\n  def testIdentityVariable(self):\n    v = resource_variable_ops.ResourceVariable(1.0)\n    self.evaluate(v.initializer)\n    result = array_ops.identity(v)\n    self.assertIsInstance(result, ops.Tensor)\n    self.assertAllEqual(result, v)\n\n\nclass PadTest(test_util.TensorFlowTestCase):\n\n  def testEager(self):\n    with context.eager_mode():\n      t = constant_op.constant([[1, 2, 3], [4, 5, 6]])\n      paddings = constant_op.constant([[\n          1,\n          1,\n      ], [2, 2]])\n      padded = array_ops.pad(t, paddings, \"CONSTANT\")\n      self.assertAllEqual(padded.numpy(),\n                          [[0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 3, 0, 0],\n                           [0, 0, 4, 5, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0]])\n\n  # b/246325518: Bad shape size. Explicitly testing different execution paths.\n  def testInvalidMirrorPadGradEagerMode(self):\n    with context.eager_mode():\n      with self.assertRaises(Exception):\n        gen_array_ops.MirrorPadGrad(\n            input=[1], paddings=[[0x77f00000, 0xa000000]], mode=\"REFLECT\")\n\n  # b/246325518: Bad shape size. Explicitly testing different execution paths.\n  def testInvalidMirrorPadGradGraphMode(self):\n    with context.graph_mode():\n      with self.assertRaises(Exception):\n        result = gen_array_ops.MirrorPadGrad(\n            input=[1], paddings=[[0x77f00000, 0xa000000]], mode=\"REFLECT\")\n        self.evaluate(result)\n\n  def testSymmetricMirrorPadGrad(self):\n    t = np.broadcast_to(np.arange(0, 7), (3, 2, 1, 7))\n    paddings = constant_op.constant([\n        [1, 1],\n        [0, 0],\n        [0, 0],\n        [2, 2],\n    ])\n    expected = np.broadcast_to(np.array([9, 27, 27]), (1, 2, 1, 3))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"SYMMETRIC\")\n    self.assertAllEqual(result, expected)\n\n  def testReflectMirrorPadGrad(self):\n    t = np.broadcast_to(np.reshape(np.arange(0, 7), (7, 1)), (1, 4, 7, 1))\n    paddings = constant_op.constant([\n        [0, 0],\n        [1, 1],\n        [2, 2],\n        [0, 0],\n    ])\n    expected = np.broadcast_to(\n        np.reshape(np.array([16, 18, 8]), (3, 1)), (1, 2, 3, 1))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"REFLECT\")\n    self.assertAllEqual(result, expected)\n\n\nclass InvertPermutationTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([3, 4, 0, 2, 1], dtype=dtype)\n        y = array_ops.invert_permutation(x)\n        self.assertAllEqual(y.get_shape(), [5])\n        self.assertAllEqual(y, [2, 4, 3, 0, 1])\n\n\nclass UnravelIndexTest(test_util.TensorFlowTestCase):\n\n  # TODO(b/73086570): Reenable test.\n  @unittest.skip(\"Test does not pass internally.\")\n  def testUnravelIndex(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(dtype=dtype):\n          indices_1 = constant_op.constant(1621, dtype=dtype)\n          dims_1 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_1 = array_ops.unravel_index(indices_1, dims_1)\n          self.assertAllEqual(out_1, [3, 1, 4, 1])\n\n          indices_2 = constant_op.constant([1621], dtype=dtype)\n          dims_2 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_2 = array_ops.unravel_index(indices_2, dims_2)\n          self.assertAllEqual(out_2, [[3], [1], [4], [1]])\n\n          indices_3 = constant_op.constant([22, 41, 37], dtype=dtype)\n          dims_3 = constant_op.constant([7, 6], dtype=dtype)\n          out_3 = array_ops.unravel_index(indices_3, dims_3)\n          self.assertAllEqual(out_3, [[3, 6, 6], [4, 5, 1]])\n\n  # Test case for GitHub issue 40204.\n  def testUnravelIndexZeroDim(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    \"dims cannot contain a dim of zero\"):\n          indices = constant_op.constant([2, 5, 7], dtype=dtype)\n          dims = constant_op.constant([3, 0], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n  def testUnravelIndexIntegerOverflow(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(\n            errors.InvalidArgumentError,\n            r\"Input dims product is causing integer overflow\"):\n          indices = constant_op.constant(-0x100000, dtype=dtype)\n          if dtype == dtypes.int32:\n            value = 0x10000000\n          else:\n            value = 0x7FFFFFFFFFFFFFFF\n          dims = constant_op.constant([value, value], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n\nclass GuaranteeConstOpTest(test_util.TensorFlowTestCase):\n\n  def testSimple(self):\n    a = array_ops.constant(10)\n    guarantee_a = array_ops.guarantee_const(a)\n    self.assertEqual(10, self.evaluate(guarantee_a))\n\n  def testVariables(self):\n    for use_resource in [False, True]:\n      with self.subTest(use_resource=use_resource):\n        a = variable_scope.get_variable(\n            \"var_{}\".format(use_resource), [],\n            initializer=init_ops.constant_initializer(10.0),\n            use_resource=use_resource)\n        guarantee_a = array_ops.guarantee_const(a)\n        self.evaluate(a.initializer)\n        self.assertEqual(10.0, self.evaluate(guarantee_a))\n\n  def testResourceRejection(self):\n    with ops.device(\"/cpu:0\"):\n      a = variable_scope.get_variable(\n          \"resource_var\", [],\n          initializer=init_ops.constant_initializer(10.0),\n          use_resource=True)\n    with self.assertRaisesWithPredicateMatch(errors.InvalidArgumentError,\n                                             \"cannot be a resource variable\"):\n      guarantee_a = array_ops.guarantee_const(a.handle)\n      self.evaluate(a.initializer)\n      self.evaluate(guarantee_a)\n\n\nclass SnapshotOpTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([0, 1, 2, 3], dtype=dtype)\n        y = gen_array_ops.snapshot(x)\n        self.assertAllEqual(y, [0, 1, 2, 3])\n\n\n@test_util.with_eager_op_as_function\n@test_util.run_all_in_graph_and_eager_modes\nclass QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n\n  # Generates a tensor of the specified `shape` using values from `values`\n  # scaled by (slice_idx + 1) along `axis` dimension.\n  def _scale_per_slice(self, shape, axis, values):\n    # Note: repeats the values if the shape is larger than values.\n    out = np.take(values, np.remainder(np.arange(np.prod(shape)),\n                                       len(values))).reshape(shape)\n    if axis is not None:\n      scale_shape = [1] * len(shape)\n      scale_shape[axis] = shape[axis]\n      out *= np.arange(1, shape[axis] + 1).reshape(scale_shape)\n    return out\n\n  def testAxis(self):\n    shape = np.array([2, 3, 4, 5])\n    values = np.array([-1, -0.5, 0, 0.3, 0.8, 0.555, 0.5], dtype=np.float32)\n    quant_values = np.array(\n        [-1, -0.5, 0, 38.0 / 128, 102.0 / 128, 71.0 / 128, 0.5],\n        dtype=np.float32)\n    for axis in [None, 0, 1, 2, 3]:\n      with self.subTest(axis=axis):\n        inputs = constant_op.constant(\n            self._scale_per_slice(shape, axis, values))\n        expected = self._scale_per_slice(shape, axis, quant_values)\n        unused_minmax_value = 0 if axis is None else [0] * shape[axis]\n        fake_quantized = self.evaluate(\n            array_ops.quantize_and_dequantize_v2(\n                inputs,\n                unused_minmax_value,\n                unused_minmax_value,\n                range_given=False,\n                round_mode=\"HALF_UP\",\n                axis=axis))\n        self.assertAllEqual(fake_quantized, expected)\n        if axis is not None:\n          fake_quantized = self.evaluate(\n              array_ops.quantize_and_dequantize_v2(\n                  inputs,\n                  unused_minmax_value,\n                  unused_minmax_value,\n                  range_given=False,\n                  axis=(axis - 4)))\n          self.assertAllClose(fake_quantized, expected)\n\n  def testBadAxis(self):\n    input_tensor = [2.5, 2.5]\n    input_min = [0, 0]\n    input_max = [1, 1]\n    # When eager_op_as_function mode is enabled XLA auto-clustering kicks in.\n    # XLA raises an UnimplementedError on invalid axis.\n    error_message_pattern = (r\"Shape must be at least rank 11 but is rank \"\n                             r\"1|invalid axis\")\n    # TODO(b/171260356): Eager mode and graph mode throw different error types\n    error = (errors.InvalidArgumentError, ValueError, errors.UnimplementedError)\n    with self.assertRaisesRegex(error, error_message_pattern):\n      self.evaluate(\n          array_ops.quantize_and_dequantize_v2(\n              input=input_tensor,\n              input_min=input_min,\n              input_max=input_max,\n              axis=10))\n\n  def testQuantizeDequantizeGrad(self):\n    shape = (2, 2)\n    max_threshold = 0\n    min_threshold = -10\n    input_value = np.random.rand(2, 2) * 40.0 - 20.0\n    input_tensor = constant_op.constant(input_value, shape=shape,\n                                        name=\"input_tensor\")\n    with self.cached_session():\n      def f(a):\n        return array_ops.quantize_and_dequantize_v2(\n            a,\n            input_min=min_threshold,\n            input_max=max_threshold,\n            range_given=True)\n      output_grad = gradient_checker_v2.compute_gradient(f, [input_tensor])\n      self.assertAllClose(output_grad[0], np.zeros([1, 4, 4]))\n\n  def testOutOfBoundAxis(self):\n    input_tensor = constant_op.constant([1., 1.])\n    input_min = [0]\n    input_max = [1]\n    q_input, _, _ = array_ops.quantize(input_tensor, 0, 1, dtypes.qint32)\n    error = (errors.InvalidArgumentError, ValueError)\n    with self.assertRaisesRegex(error,\n                                r\".*Axis must be less than input dimension.*\"):\n      self.evaluate(\n          gen_array_ops.dequantize(\n              input=q_input,\n              min_range=input_min,\n              max_range=input_max,\n              axis=2**31 - 1))\n\n  @test_util.run_v2_only\n  def testInvalidAxis(self):\n\n    @def_function.function\n    def test_quantize_and_dequantize_v2():\n      gen_array_ops.quantize_and_dequantize_v2(\n          input=[2.5],\n          input_min=[1.0],\n          input_max=[10.0],\n          signed_input=True,\n          num_bits=1,\n          range_given=True,\n          round_mode=\"HALF_TO_EVEN\",\n          narrow_range=True,\n          axis=0x7fffffff)\n\n    @def_function.function\n    def test_quantize_and_dequantize_v3():\n      gen_array_ops.quantize_and_dequantize_v3(\n          input=[2.5],\n          input_min=[1.0],\n          input_max=[10.0],\n          num_bits=1,\n          signed_input=True,\n          range_given=True,\n          narrow_range=True,\n          axis=0x7fffffff)\n\n    @def_function.function\n    def test_quantize_and_dequantize_v4():\n      gen_array_ops.quantize_and_dequantize_v4(\n          input=[2.5],\n          input_min=[1.0],\n          input_max=[10.0],\n          signed_input=True,\n          num_bits=1,\n          range_given=True,\n          round_mode=\"HALF_TO_EVEN\",\n          narrow_range=True,\n          axis=0x7fffffff)\n\n    @def_function.function\n    def test_quantize_and_dequantize_v4_grad():\n      gen_array_ops.quantize_and_dequantize_v4_grad(\n          gradients=[2.5],\n          input=[2.5],\n          input_min=[1.0],\n          input_max=[10.0],\n          axis=0x7fffffff)\n\n    with self.assertRaisesRegex(\n        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n      test_quantize_and_dequantize_v2()\n\n    with self.assertRaisesRegex(\n        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n      test_quantize_and_dequantize_v3()\n\n    with self.assertRaisesRegex(\n        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n      test_quantize_and_dequantize_v4()\n\n    with self.assertRaisesRegex(\n        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n      test_quantize_and_dequantize_v4_grad()\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SortedSearchTest(test_util.TensorFlowTestCase):\n\n  def testUpperBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testZeroSequenceSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 0]),\n                array_ops.ones([2, 3]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 3], dtype))\n\n  def testZeroValueSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 3]),\n                array_ops.ones([2, 0]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 0], dtype))\n\n  def testZeroInputSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 0]),\n                array_ops.ones([2, 3]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 3], dtype))\n\n  def testInt64(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y, out_type=dtypes.int64)\n\n    _ = g.get_concrete_function()\n\n  def testInt64UnspecifiedOutType(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y)\n\n    _ = g.get_concrete_function()\n\n\nclass BatchGatherNdTest(test_util.TensorFlowTestCase):\n\n  def testShapesMatch(self):\n    \"\"\"Tests for various different shape combinations.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 2), (2, 3), 0),)\n    shapes.append(((2, 2, 2), (3,), 0),)\n    shapes.append(((2, 2, 2), (1,), 0),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(1.0, shape=(params_shape))\n        indices = constant_op.constant(\n            1, shape=(indices_shape), dtype=dtypes.int32)\n        out = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        ndims_params = len(params_shape) - batch_dims\n        ndims_rows = ndims_params - indices_shape[-1]\n        expected_out_shape = indices_shape[:-1]\n        if ndims_rows > 0:\n          expected_out_shape += params_shape[-ndims_rows:]\n        self.assertSequenceEqual(out.shape, expected_out_shape)\n\n  def testReducesToGatherNDWhenBatchDimIsZero(self):\n    \"\"\"Confirms setting batch_dims to zero reduces to tf.gather_nd.\"\"\"\n    params = constant_op.constant(np.random.uniform(0.0, 1.0, size=(7, 8, 9)))\n    indices_shapes = []\n    indices_shapes.append((1,))\n    indices_shapes.append((3, 1))\n    indices_shapes.append((3, 3, 1))\n    indices_shapes.append((2,))\n    indices_shapes.append((3, 2))\n    indices_shapes.append((3, 3, 2))\n    indices_shapes.append((3,))\n    indices_shapes.append((3, 3))\n    indices_shapes.append((3, 3, 3))\n\n    for indices_shape in indices_shapes:\n      with self.subTest(indices_shape=indices_shape):\n        indices = np.random.randint(0, 7, size=indices_shape)\n        gather_nd_result = gen_array_ops.gather_nd(params, indices)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=0)\n        self.assertAllEqual(gather_nd_result, batch_gather_nd_result)\n\n  def testSameResultAsMapFn(self):\n    \"\"\"Compares results with gather_nd called on every element with map_fn.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n\n        if batch_dims > 1:\n          params = array_ops.reshape(\n              params, shape=[-1] + list(params_shape[batch_dims:]))\n          indices = array_ops.reshape(\n              indices, shape=[-1] + list(indices_shape[batch_dims:]))\n\n        map_fn_gather_nd_result = map_fn.map_fn(\n            fn=self._map_fn_body, elems=(params, indices), dtype=dtypes.float64)\n\n        if batch_dims > 1:\n          out_shape = map_fn_gather_nd_result.shape.as_list()\n          out_shape = list(params_shape[:batch_dims]) + out_shape[1:]\n          map_fn_gather_nd_result = array_ops.reshape(\n              map_fn_gather_nd_result, shape=out_shape)\n\n        self.assertAllEqual(map_fn_gather_nd_result, batch_gather_nd_result)\n\n  def _map_fn_body(self, elems):\n    return gen_array_ops.gather_nd(elems[0], elems[1])\n\n  def testBatchDimsAsTensor(self):\n    \"\"\"Tests Tensor batch_dims as input works as intended.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 0),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        batch_dims_tensor = constant_op.constant([batch_dims])\n        batch_gather_nd_tensor_batch_dims_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims_tensor)\n\n        self.assertAllEqual(batch_gather_nd_tensor_batch_dims_result,\n                            batch_gather_nd_result)\n\n  def testInvalidBatchDimsRaisesException(self):\n    \"\"\"Tests whether invalid batch_dims raise expected exceptions.\"\"\"\n    params = constant_op.constant(\n        np.random.uniform(0.0, 1.0, size=(3, 2, 2, 3, 4)))\n    indices = np.random.randint(0, 2, size=(3, 2, 3))\n\n    with self.assertRaises(TypeError):\n      array_ops.batch_gather_nd(\n          params=params,\n          indices=indices,\n          batch_dims=constant_op.constant((0, 1)))\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=-1)\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=4)\n\n  def testNoneBatchDimensions(self):\n    \"\"\"Tests gather_nd works with None dimensions.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      params_ph_shape = list(params_shape)\n      indices_ph_shape = list(indices_shape)\n      for i in range(batch_dims):\n        params_ph_shape[i] = None\n        indices_ph_shape[i] = None\n\n      @def_function.function\n      def func(params, indices):\n        return array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)  # pylint: disable=cell-var-from-loop\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec(params_ph_shape, dtypes.float32),\n          tensor_spec.TensorSpec(indices_ph_shape, dtypes.int32))\n\n      params_val = np.ones(dtype=np.float32, shape=params_shape)\n      indices_val = np.ones(dtype=np.int32, shape=indices_shape)\n      res = f(params_val, indices_val)\n      row_ndims = len(params_shape) - batch_dims - indices_shape[-1]\n      expected_out_shape = indices_shape[:-1]\n      if row_ndims > 0:\n        expected_out_shape += params_shape[-row_ndims:]\n\n      self.assertSequenceEqual(res.shape, expected_out_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RepeatTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (3, 4, None),\n      ([[1, 2], [3, 4]], 2, None),\n      ([[1, 2], [3, 4]], [1, 2], 0),\n      ([[1, 2], [3, 4]], [1, 2], 1),\n      ([[1, 2], [3, 4]], 3, 1),\n      ([[1, 2], [3, 4]], [1, 2, 3, 4], None),\n      (np.ones([0, 4]), 0, 1),\n      (np.ones([1, 2]), [2], None),\n  )\n  @test_util.with_forward_compatibility_horizons(None, [2052, 2, 7])\n  def testRepeat(self, array, repeats, axis):\n    array = np.array(array)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.int32)] * 2)\n    def repeat_fn(array, repeats):\n      return array_ops.repeat(array, repeats, axis)\n\n    v_tf = array_ops.repeat(constant_op.constant(array), repeats, axis)\n    v_tf_fn = repeat_fn(\n        constant_op.constant(array, dtype=dtypes.int32), repeats)\n    v_np = np.repeat(array, repeats, axis)\n    self.assertAllEqual(v_tf, v_np)\n    self.assertAllEqual(v_tf_fn, v_np)\n\n\nclass RepeatBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark the repeat implementation.\"\"\"\n\n  def run_and_time(self, op, iters=100, warmup_iters=10):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(warmup_iters):\n      _ = self.evaluate(op)\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / float(iters))\n\n  def make_variable(self, shape, dtype=dtypes.float32):\n    items = 1\n    for dim in shape:\n      items *= dim\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtype)\n    return var\n\n  def run_benchmark(self, shape, max_repeats, axis=None):\n    with session.Session():\n      var = self.make_variable(shape)\n      if axis is None:\n        axis_size = 1\n        for dim in shape:\n          axis_size *= dim\n      else:\n        axis_size = shape[axis]\n      repeats = constant_op.constant(\n          np.random.randint(max_repeats, size=[axis_size]), dtype=dtypes.int64)\n      repeat_op = array_ops.repeat(var, repeats, axis=axis)\n      # Return a scalar to reduce the device-to-host memcopy overhead.\n      repeat_op = repeat_op[(0,) * len(shape)]\n      self.run_and_time(repeat_op)\n\n  def benchmark_repeat_few_1d(self):\n    self.run_benchmark(shape=[1024 * 1024], max_repeats=8, axis=0)\n\n  def benchmark_repeat_many_1d(self):\n    self.run_benchmark(shape=[8 * 1024], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_few_2d_axis0(self):\n    self.run_benchmark(shape=[8, 128 * 1024], max_repeats=8, axis=0)\n\n  def benchmark_repeat_many_2d_axis0(self):\n    self.run_benchmark(shape=[8, 1024], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_many_2d_axis0_big(self):\n    self.run_benchmark(shape=[1024, 32], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_few_2d_axis1(self):\n    self.run_benchmark(shape=[8, 128 * 1024], max_repeats=8, axis=1)\n\n  def benchmark_repeat_many_2d_axis1(self):\n    self.run_benchmark(shape=[8, 1024], max_repeats=1024, axis=1)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass TileVariantTest(test_util.TensorFlowTestCase):\n\n  def test_tile_tensor_list(self):\n    t = constant_op.constant(np.random.uniform(size=[2, 3, 4]))\n    handle = list_ops.tensor_list_from_tensor(t, element_shape=None)\n    with ops.device(\"CPU:0\"):\n      tiled_handles = array_ops.tile(array_ops.reshape(handle, [1]), [2])\n    tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                [3, 4])\n    tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n    # Now mutate some of the lists and make sure the changes are not reflected\n    # in the tiled handles.\n    with ops.control_dependencies([\n        list_ops.tensor_list_scatter([t[0] + 1], [0], input_handle=handle),\n        list_ops.tensor_list_set_item(tiled_handles[0], 0, t[0] + 2)]):\n      tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                  [3, 4])\n      tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                  [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n\n\nclass StopGradientTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  def testStopGradient(self):\n    x = array_ops.zeros(3)\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientGradientTape(self):\n    x = array_ops.zeros(3)\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n  def testStopGradientGradientTapeRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n  @parameterized.named_parameters([\n      (\"TFFunction\", def_function.function),\n      (\"PythonFunction\", lambda f: f),\n  ])\n  def test_stop_gradient_resource_variable(self, decorator):\n    x = resource_variable_ops.ResourceVariable([1.0])\n    self.evaluate(x.initializer)\n\n    @decorator\n    def stop_gradient_f(x):\n      return array_ops.stop_gradient(x)\n\n    with backprop.GradientTape() as tape:\n      y = stop_gradient_f(x)\n    self.assertIsNone(tape.gradient(y, x))\n    # stop_gradient converts ResourceVariable to Tensor\n    self.assertIsInstance(y, ops.Tensor)\n    self.assertAllEqual(y, x)\n\nif __name__ == \"__main__\":\n  test_lib.main()\n"], "filenames": ["tensorflow/core/ops/array_ops.cc", "tensorflow/python/kernel_tests/array_ops/array_ops_test.py"], "buggy_code_start_loc": [2881, 1858], "buggy_code_end_loc": [2983, 1858], "fixing_code_start_loc": [2882, 1859], "fixing_code_end_loc": [3000, 1925], "type": "CWE-122", "message": "TensorFlow is an open source platform for machine learning. Attackers using Tensorflow prior to 2.12.0 or 2.11.1 can access heap memory which is not in the control of user, leading to a crash or remote code execution. The fix will be included in TensorFlow version 2.12.0 and will also cherrypick this commit on TensorFlow version 2.11.1.", "other": {"cve": {"id": "CVE-2023-25668", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-25T00:15:07.593", "lastModified": "2023-03-31T14:20:18.307", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. Attackers using Tensorflow prior to 2.12.0 or 2.11.1 can access heap memory which is not in the control of user, leading to a crash or remote code execution. The fix will be included in TensorFlow version 2.12.0 and will also cherrypick this commit on TensorFlow version 2.11.1."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-122"}, {"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.12.0", "matchCriteriaId": "FAC3DE54-93B4-4D6C-9648-B9D416B9770F"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/7b174a0f2e40ff3f3aa957aecddfd5aaae35eccb", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-gw97-ff7c-9v96", "source": "security-advisories@github.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/7b174a0f2e40ff3f3aa957aecddfd5aaae35eccb"}}