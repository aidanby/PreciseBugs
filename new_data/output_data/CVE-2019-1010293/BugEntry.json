{"buggy_code": ["// SPDX-License-Identifier: BSD-2-Clause\n/*\n * Copyright (c) 2016, Linaro Limited\n * Copyright (c) 2014, STMicroelectronics International N.V.\n */\n\n#include <arm.h>\n#include <assert.h>\n#include <kernel/panic.h>\n#include <kernel/spinlock.h>\n#include <kernel/tee_common.h>\n#include <kernel/tee_misc.h>\n#include <kernel/tlb_helpers.h>\n#include <mm/core_memprot.h>\n#include <mm/core_mmu.h>\n#include <mm/mobj.h>\n#include <mm/pgt_cache.h>\n#include <mm/tee_mm.h>\n#include <mm/tee_mmu.h>\n#include <mm/tee_mmu_types.h>\n#include <mm/tee_pager.h>\n#include <sm/optee_smc.h>\n#include <stdlib.h>\n#include <tee_api_defines_extensions.h>\n#include <tee_api_types.h>\n#include <trace.h>\n#include <types_ext.h>\n#include <user_ta_header.h>\n#include <util.h>\n\n#ifdef CFG_PL310\n#include <kernel/tee_l2cc_mutex.h>\n#endif\n\n#define TEE_MMU_UDATA_ATTR\t\t(TEE_MATTR_VALID_BLOCK | \\\n\t\t\t\t\t TEE_MATTR_PRW | TEE_MATTR_URW | \\\n\t\t\t\t\t TEE_MATTR_SECURE)\n#define TEE_MMU_UCODE_ATTR\t\t(TEE_MATTR_VALID_BLOCK | \\\n\t\t\t\t\t TEE_MATTR_PRW | TEE_MATTR_URWX | \\\n\t\t\t\t\t TEE_MATTR_SECURE)\n\n#define TEE_MMU_UCACHE_DEFAULT_ATTR\t(TEE_MATTR_CACHE_CACHED << \\\n\t\t\t\t\t TEE_MATTR_CACHE_SHIFT)\n\nstatic vaddr_t select_va_in_range(vaddr_t prev_end, uint32_t prev_attr,\n\t\t\t\t  vaddr_t next_begin, uint32_t next_attr,\n\t\t\t\t  const struct vm_region *reg)\n{\n\tsize_t granul;\n\tconst uint32_t a = TEE_MATTR_EPHEMERAL | TEE_MATTR_PERMANENT;\n\tsize_t pad;\n\tvaddr_t begin_va;\n\tvaddr_t end_va;\n\n\t/*\n\t * Insert an unmapped entry to separate regions with differing\n\t * TEE_MATTR_EPHEMERAL TEE_MATTR_PERMANENT bits as they never are\n\t * to be contiguous with another region.\n\t */\n\tif (prev_attr && (prev_attr & a) != (reg->attr & a))\n\t\tpad = SMALL_PAGE_SIZE;\n\telse\n\t\tpad = 0;\n\n\tgranul = SMALL_PAGE_SIZE;\n#ifndef CFG_WITH_LPAE\n\tif ((prev_attr & TEE_MATTR_SECURE) != (reg->attr & TEE_MATTR_SECURE))\n\t\tgranul = CORE_MMU_PGDIR_SIZE;\n#endif\n\tbegin_va = ROUNDUP(prev_end + pad, granul);\n\tif (reg->va) {\n\t\tif (reg->va < begin_va)\n\t\t\treturn 0;\n\t\tbegin_va = reg->va;\n\t}\n\n\tif (next_attr && (next_attr & a) != (reg->attr & a))\n\t\tpad = SMALL_PAGE_SIZE;\n\telse\n\t\tpad = 0;\n\n\tgranul = SMALL_PAGE_SIZE;\n#ifndef CFG_WITH_LPAE\n\tif ((next_attr & TEE_MATTR_SECURE) != (reg->attr & TEE_MATTR_SECURE))\n\t\tgranul = CORE_MMU_PGDIR_SIZE;\n#endif\n\tend_va = ROUNDUP(begin_va + reg->size + pad, granul);\n\n\tif (end_va <= next_begin) {\n\t\tassert(!reg->va || reg->va == begin_va);\n\t\treturn begin_va;\n\t}\n\n\treturn 0;\n}\n\nstatic size_t get_num_req_pgts(struct user_ta_ctx *utc, vaddr_t *begin,\n\t\t\t       vaddr_t *end)\n{\n\tvaddr_t b;\n\tvaddr_t e;\n\n\tif (TAILQ_EMPTY(&utc->vm_info->regions)) {\n\t\tcore_mmu_get_user_va_range(&b, NULL);\n\t\te = b;\n\t} else {\n\t\tstruct vm_region *r;\n\n\t\tb = TAILQ_FIRST(&utc->vm_info->regions)->va;\n\t\tr = TAILQ_LAST(&utc->vm_info->regions, vm_region_head);\n\t\te = r->va + r->size;\n\t\tb = ROUNDDOWN(b, CORE_MMU_PGDIR_SIZE);\n\t\te = ROUNDUP(e, CORE_MMU_PGDIR_SIZE);\n\t}\n\n\tif (begin)\n\t\t*begin = b;\n\tif (end)\n\t\t*end = e;\n\treturn (e - b) >> CORE_MMU_PGDIR_SHIFT;\n}\n\nstatic TEE_Result alloc_pgt(struct user_ta_ctx *utc)\n{\n\tstruct thread_specific_data *tsd __maybe_unused;\n\tvaddr_t b;\n\tvaddr_t e;\n\tsize_t ntbl;\n\n\tntbl = get_num_req_pgts(utc, &b, &e);\n\tif (!pgt_check_avail(ntbl)) {\n\t\tEMSG(\"%zu page tables not available\", ntbl);\n\t\treturn TEE_ERROR_OUT_OF_MEMORY;\n\t}\n\n#ifdef CFG_PAGED_USER_TA\n\ttsd = thread_get_tsd();\n\tif (&utc->ctx == tsd->ctx) {\n\t\t/*\n\t\t * The supplied utc is the current active utc, allocate the\n\t\t * page tables too as the pager needs to use them soon.\n\t\t */\n\t\tpgt_alloc(&tsd->pgt_cache, &utc->ctx, b, e - 1);\n\t}\n#endif\n\n\treturn TEE_SUCCESS;\n}\n\nstatic void free_pgt(struct user_ta_ctx *utc, vaddr_t base, size_t size)\n{\n\tstruct thread_specific_data *tsd = thread_get_tsd();\n\tstruct pgt_cache *pgt_cache = NULL;\n\n\tif (&utc->ctx == tsd->ctx)\n\t\tpgt_cache = &tsd->pgt_cache;\n\n\tpgt_flush_ctx_range(pgt_cache, &utc->ctx, base, base + size);\n}\n\nstatic TEE_Result umap_add_region(struct vm_info *vmi, struct vm_region *reg)\n{\n\tstruct vm_region *r;\n\tstruct vm_region *prev_r;\n\tvaddr_t va_range_base;\n\tsize_t va_range_size;\n\tvaddr_t va;\n\n\tcore_mmu_get_user_va_range(&va_range_base, &va_range_size);\n\n\t/* Check alignment, it has to be at least SMALL_PAGE based */\n\tif ((reg->va | reg->size) & SMALL_PAGE_MASK)\n\t\treturn TEE_ERROR_ACCESS_CONFLICT;\n\n\t/* Check that the mobj is defined for the entire range */\n\tif ((reg->offset + reg->size) >\n\t     ROUNDUP(reg->mobj->size, SMALL_PAGE_SIZE))\n\t\treturn TEE_ERROR_BAD_PARAMETERS;\n\n\tprev_r = NULL;\n\tTAILQ_FOREACH(r, &vmi->regions, link) {\n\t\tif (TAILQ_FIRST(&vmi->regions) == r) {\n\t\t\tva = select_va_in_range(va_range_base, 0,\n\t\t\t\t\t\tr->va, r->attr, reg);\n\t\t\tif (va) {\n\t\t\t\treg->va = va;\n\t\t\t\tTAILQ_INSERT_HEAD(&vmi->regions, reg, link);\n\t\t\t\treturn TEE_SUCCESS;\n\t\t\t}\n\t\t} else {\n\t\t\tva = select_va_in_range(prev_r->va + prev_r->size,\n\t\t\t\t\t\tprev_r->attr, r->va, r->attr,\n\t\t\t\t\t\treg);\n\t\t\tif (va) {\n\t\t\t\treg->va = va;\n\t\t\t\tTAILQ_INSERT_BEFORE(r, reg, link);\n\t\t\t\treturn TEE_SUCCESS;\n\t\t\t}\n\t\t}\n\t\tprev_r = r;\n\t}\n\n\tr = TAILQ_LAST(&vmi->regions, vm_region_head);\n\tif (r) {\n\t\tva = select_va_in_range(r->va + r->size, r->attr,\n\t\t\t\t\tva_range_base + va_range_size, 0, reg);\n\t\tif (va) {\n\t\t\treg->va = va;\n\t\t\tTAILQ_INSERT_TAIL(&vmi->regions, reg, link);\n\t\t\treturn TEE_SUCCESS;\n\t\t}\n\t} else {\n\t\tva = select_va_in_range(va_range_base, 0,\n\t\t\t\t\tva_range_base + va_range_size, 0, reg);\n\t\tif (va) {\n\t\t\treg->va = va;\n\t\t\tTAILQ_INSERT_HEAD(&vmi->regions, reg, link);\n\t\t\treturn TEE_SUCCESS;\n\t\t}\n\t}\n\n\treturn TEE_ERROR_ACCESS_CONFLICT;\n}\n\nTEE_Result vm_map(struct user_ta_ctx *utc, vaddr_t *va, size_t len,\n\t\t  uint32_t prot, struct mobj *mobj, size_t offs)\n{\n\tTEE_Result res;\n\tstruct vm_region *reg = calloc(1, sizeof(*reg));\n\tuint32_t attr = 0;\n\tconst uint32_t prot_mask = TEE_MATTR_PROT_MASK | TEE_MATTR_PERMANENT |\n\t\t\t\t   TEE_MATTR_EPHEMERAL;\n\n\tif (!reg)\n\t\treturn TEE_ERROR_OUT_OF_MEMORY;\n\n\tif (prot & ~prot_mask) {\n\t\tres = TEE_ERROR_BAD_PARAMETERS;\n\t\tgoto err_free_reg;\n\t}\n\n\tif (!mobj_is_paged(mobj)) {\n\t\tuint32_t cattr;\n\n\t\tres = mobj_get_cattr(mobj, &cattr);\n\t\tif (res)\n\t\t\tgoto err_free_reg;\n\t\tattr |= cattr << TEE_MATTR_CACHE_SHIFT;\n\t}\n\tattr |= TEE_MATTR_VALID_BLOCK;\n\tif (mobj_is_secure(mobj))\n\t\tattr |= TEE_MATTR_SECURE;\n\n\treg->mobj = mobj;\n\treg->offset = offs;\n\treg->va = *va;\n\treg->size = ROUNDUP(len, SMALL_PAGE_SIZE);\n\treg->attr = attr | prot;\n\n\tres = umap_add_region(utc->vm_info, reg);\n\tif (res)\n\t\tgoto err_free_reg;\n\n\tres = alloc_pgt(utc);\n\tif (res)\n\t\tgoto err_rem_reg;\n\n\tif (!(reg->attr & (TEE_MATTR_EPHEMERAL | TEE_MATTR_PERMANENT)) &&\n\t    mobj_is_paged(mobj)) {\n\t\tif (!tee_pager_add_uta_area(utc, reg->va, reg->size)) {\n\t\t\tres = TEE_ERROR_GENERIC;\n\t\t\tgoto err_rem_reg;\n\t\t}\n\t}\n\n\t/*\n\t * If the context currently is active set it again to update\n\t * the mapping.\n\t */\n\tif (thread_get_tsd()->ctx == &utc->ctx)\n\t\ttee_mmu_set_ctx(&utc->ctx);\n\n\t*va = reg->va;\n\n\treturn TEE_SUCCESS;\n\nerr_rem_reg:\n\tTAILQ_REMOVE(&utc->vm_info->regions, reg, link);\nerr_free_reg:\n\tfree(reg);\n\treturn res;\n}\n\nTEE_Result vm_set_prot(struct user_ta_ctx *utc, vaddr_t va, size_t len,\n\t\t       uint32_t prot)\n{\n\tstruct vm_region *r;\n\n\t/*\n\t * To keep thing simple: specified va and len has to match exactly\n\t * with an already registered region.\n\t */\n\tTAILQ_FOREACH(r, &utc->vm_info->regions, link) {\n\t\tif (core_is_buffer_intersect(r->va, r->size, va, len)) {\n\t\t\tif (r->va != va || r->size != len)\n\t\t\t\treturn TEE_ERROR_BAD_PARAMETERS;\n\t\t\tif (mobj_is_paged(r->mobj)) {\n\t\t\t\tif (!tee_pager_set_uta_area_attr(utc, va, len,\n\t\t\t\t\t\t\t\t prot))\n\t\t\t\t\treturn TEE_ERROR_GENERIC;\n\t\t\t} else if ((prot & TEE_MATTR_UX) &&\n\t\t\t\t   (r->attr & (TEE_MATTR_UW | TEE_MATTR_PW))) {\n\t\t\t\tcache_op_inner(DCACHE_AREA_CLEAN,\n\t\t\t\t\t       (void *)va, len);\n\t\t\t\tcache_op_inner(ICACHE_AREA_INVALIDATE,\n\t\t\t\t\t       (void *)va, len);\n\t\t\t}\n\t\t\tr->attr &= ~TEE_MATTR_PROT_MASK;\n\t\t\tr->attr |= prot & TEE_MATTR_PROT_MASK;\n\t\t\treturn TEE_SUCCESS;\n\t\t}\n\t}\n\n\treturn TEE_ERROR_ITEM_NOT_FOUND;\n}\n\n\nstatic TEE_Result map_kinit(struct user_ta_ctx *utc __maybe_unused)\n{\n\tTEE_Result res;\n\tstruct mobj *mobj;\n\tsize_t offs;\n\tvaddr_t va;\n\tsize_t sz;\n\n\tthread_get_user_kcode(&mobj, &offs, &va, &sz);\n\tif (sz) {\n\t\tres = vm_map(utc, &va, sz, TEE_MATTR_PRX | TEE_MATTR_PERMANENT,\n\t\t\t     mobj, offs);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\n\tthread_get_user_kdata(&mobj, &offs, &va, &sz);\n\tif (sz)\n\t\treturn vm_map(utc, &va, sz, TEE_MATTR_PRW | TEE_MATTR_PERMANENT,\n\t\t\t      mobj, offs);\n\n\treturn TEE_SUCCESS;\n}\n\nTEE_Result vm_info_init(struct user_ta_ctx *utc)\n{\n\tTEE_Result res;\n\tuint32_t asid = asid_alloc();\n\n\tif (!asid) {\n\t\tDMSG(\"Failed to allocate ASID\");\n\t\treturn TEE_ERROR_GENERIC;\n\t}\n\n\tutc->vm_info = calloc(1, sizeof(*utc->vm_info));\n\tif (!utc->vm_info) {\n\t\tasid_free(asid);\n\t\treturn TEE_ERROR_OUT_OF_MEMORY;\n\t}\n\tTAILQ_INIT(&utc->vm_info->regions);\n\tutc->vm_info->asid = asid;\n\n\tres = map_kinit(utc);\n\tif (res)\n\t\tvm_info_final(utc);\n\treturn res;\n}\n\nstatic void umap_remove_region(struct vm_info *vmi, struct vm_region *reg)\n{\n\tTAILQ_REMOVE(&vmi->regions, reg, link);\n\tfree(reg);\n}\n\nstatic void clear_param_map(struct user_ta_ctx *utc)\n{\n\tstruct vm_region *next_r;\n\tstruct vm_region *r;\n\n\tTAILQ_FOREACH_SAFE(r, &utc->vm_info->regions, link, next_r)\n\t\tif (r->attr & TEE_MATTR_EPHEMERAL)\n\t\t\tumap_remove_region(utc->vm_info, r);\n}\n\nstatic TEE_Result param_mem_to_user_va(struct user_ta_ctx *utc,\n\t\t\t\t       struct param_mem *mem, void **user_va)\n{\n\tstruct vm_region *region;\n\n\tTAILQ_FOREACH(region, &utc->vm_info->regions, link) {\n\t\tvaddr_t va;\n\t\tsize_t phys_offs;\n\n\t\tif (!(region->attr & TEE_MATTR_EPHEMERAL))\n\t\t\tcontinue;\n\t\tif (mem->mobj != region->mobj)\n\t\t\tcontinue;\n\t\tif (mem->offs < region->offset)\n\t\t\tcontinue;\n\t\tif (mem->offs >= (region->offset + region->size))\n\t\t\tcontinue;\n\t\tphys_offs = mobj_get_phys_offs(mem->mobj,\n\t\t\t\t\t       CORE_MMU_USER_PARAM_SIZE);\n\t\tva = region->va + mem->offs + phys_offs - region->offset;\n\t\t*user_va = (void *)va;\n\t\treturn TEE_SUCCESS;\n\t}\n\treturn TEE_ERROR_GENERIC;\n}\n\nstatic int cmp_param_mem(const void *a0, const void *a1)\n{\n\tconst struct param_mem *m1 = a1;\n\tconst struct param_mem *m0 = a0;\n\tint ret;\n\n\t/* Make sure that invalid param_mem are placed last in the array */\n\tif (!m0->size && !m1->size)\n\t\treturn 0;\n\tif (!m0->size)\n\t\treturn 1;\n\tif (!m1->size)\n\t\treturn -1;\n\n\tret = CMP_TRILEAN(mobj_is_secure(m0->mobj), mobj_is_secure(m1->mobj));\n\tif (ret)\n\t\treturn ret;\n\n\tret = CMP_TRILEAN((vaddr_t)m0->mobj, (vaddr_t)m1->mobj);\n\tif (ret)\n\t\treturn ret;\n\n\tret = CMP_TRILEAN(m0->offs, m1->offs);\n\tif (ret)\n\t\treturn ret;\n\n\treturn CMP_TRILEAN(m0->size, m1->size);\n}\n\nTEE_Result tee_mmu_map_param(struct user_ta_ctx *utc,\n\t\tstruct tee_ta_param *param, void *param_va[TEE_NUM_PARAMS])\n{\n\tTEE_Result res = TEE_SUCCESS;\n\tsize_t n;\n\tsize_t m;\n\tstruct param_mem mem[TEE_NUM_PARAMS];\n\n\tmemset(mem, 0, sizeof(mem));\n\tfor (n = 0; n < TEE_NUM_PARAMS; n++) {\n\t\tuint32_t param_type = TEE_PARAM_TYPE_GET(param->types, n);\n\t\tsize_t phys_offs;\n\n\t\tif (param_type != TEE_PARAM_TYPE_MEMREF_INPUT &&\n\t\t    param_type != TEE_PARAM_TYPE_MEMREF_OUTPUT &&\n\t\t    param_type != TEE_PARAM_TYPE_MEMREF_INOUT)\n\t\t\tcontinue;\n\t\tphys_offs = mobj_get_phys_offs(param->u[n].mem.mobj,\n\t\t\t\t\t       CORE_MMU_USER_PARAM_SIZE);\n\t\tmem[n].mobj = param->u[n].mem.mobj;\n\t\tmem[n].offs = ROUNDDOWN(phys_offs + param->u[n].mem.offs,\n\t\t\t\t\tCORE_MMU_USER_PARAM_SIZE);\n\t\tmem[n].size = ROUNDUP(phys_offs + param->u[n].mem.offs -\n\t\t\t\t      mem[n].offs + param->u[n].mem.size,\n\t\t\t\t      CORE_MMU_USER_PARAM_SIZE);\n\t}\n\n\t/*\n\t * Sort arguments so size = 0 is last, secure mobjs first, then by\n\t * mobj pointer value since those entries can't be merged either,\n\t * finally by offset.\n\t *\n\t * This should result in a list where all mergeable entries are\n\t * next to each other and unused/invalid entries are at the end.\n\t */\n\tqsort(mem, TEE_NUM_PARAMS, sizeof(struct param_mem), cmp_param_mem);\n\n\tfor (n = 1, m = 0; n < TEE_NUM_PARAMS && mem[n].size; n++) {\n\t\tif (mem[n].mobj == mem[m].mobj &&\n\t\t    (mem[n].offs == (mem[m].offs + mem[m].size) ||\n\t\t     core_is_buffer_intersect(mem[m].offs, mem[m].size,\n\t\t\t\t\t      mem[n].offs, mem[n].size))) {\n\t\t\tmem[m].size = mem[n].offs + mem[n].size - mem[m].offs;\n\t\t\tcontinue;\n\t\t}\n\t\tm++;\n\t\tif (n != m)\n\t\t\tmem[m] = mem[n];\n\t}\n\t/*\n\t * We'd like 'm' to be the number of valid entries. Here 'm' is the\n\t * index of the last valid entry if the first entry is valid, else\n\t * 0.\n\t */\n\tif (mem[0].size)\n\t\tm++;\n\n\t/* Clear all the param entries as they can hold old information */\n\tclear_param_map(utc);\n\n\tfor (n = 0; n < m; n++) {\n\t\tvaddr_t va = 0;\n\t\tconst uint32_t prot = TEE_MATTR_PRW | TEE_MATTR_URW |\n\t\t\t\t      TEE_MATTR_EPHEMERAL;\n\n\t\tres = vm_map(utc, &va, mem[n].size, prot, mem[n].mobj,\n\t\t\t     mem[n].offs);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\n\tfor (n = 0; n < TEE_NUM_PARAMS; n++) {\n\t\tuint32_t param_type = TEE_PARAM_TYPE_GET(param->types, n);\n\n\t\tif (param_type != TEE_PARAM_TYPE_MEMREF_INPUT &&\n\t\t    param_type != TEE_PARAM_TYPE_MEMREF_OUTPUT &&\n\t\t    param_type != TEE_PARAM_TYPE_MEMREF_INOUT)\n\t\t\tcontinue;\n\t\tif (param->u[n].mem.size == 0)\n\t\t\tcontinue;\n\n\t\tres = param_mem_to_user_va(utc, &param->u[n].mem, param_va + n);\n\t\tif (res != TEE_SUCCESS)\n\t\t\treturn res;\n\t}\n\n\treturn alloc_pgt(utc);\n}\n\nTEE_Result tee_mmu_add_rwmem(struct user_ta_ctx *utc, struct mobj *mobj,\n\t\t\t     vaddr_t *va)\n{\n\tTEE_Result res;\n\tstruct vm_region *reg = calloc(1, sizeof(*reg));\n\n\tif (!reg)\n\t\treturn TEE_ERROR_OUT_OF_MEMORY;\n\n\treg->mobj = mobj;\n\treg->offset = 0;\n\treg->va = 0;\n\treg->size = ROUNDUP(mobj->size, SMALL_PAGE_SIZE);\n\tif (mobj_is_secure(mobj))\n\t\treg->attr = TEE_MATTR_SECURE;\n\telse\n\t\treg->attr = 0;\n\n\tres = umap_add_region(utc->vm_info, reg);\n\tif (res) {\n\t\tfree(reg);\n\t\treturn res;\n\t}\n\n\tres = alloc_pgt(utc);\n\tif (res)\n\t\tumap_remove_region(utc->vm_info, reg);\n\telse\n\t\t*va = reg->va;\n\n\treturn res;\n}\n\nvoid tee_mmu_rem_rwmem(struct user_ta_ctx *utc, struct mobj *mobj, vaddr_t va)\n{\n\tstruct vm_region *reg;\n\n\tTAILQ_FOREACH(reg, &utc->vm_info->regions, link) {\n\t\tif (reg->mobj == mobj && reg->va == va) {\n\t\t\tfree_pgt(utc, reg->va, reg->size);\n\t\t\tumap_remove_region(utc->vm_info, reg);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nvoid vm_info_final(struct user_ta_ctx *utc)\n{\n\tif (!utc->vm_info)\n\t\treturn;\n\n\t/* clear MMU entries to avoid clash when asid is reused */\n\ttlbi_asid(utc->vm_info->asid);\n\n\tasid_free(utc->vm_info->asid);\n\twhile (!TAILQ_EMPTY(&utc->vm_info->regions))\n\t\tumap_remove_region(utc->vm_info,\n\t\t\t\t   TAILQ_FIRST(&utc->vm_info->regions));\n\tfree(utc->vm_info);\n\tutc->vm_info = NULL;\n}\n\n/* return true only if buffer fits inside TA private memory */\nbool tee_mmu_is_vbuf_inside_ta_private(const struct user_ta_ctx *utc,\n\t\t\t\t  const void *va, size_t size)\n{\n\tstruct vm_region *r;\n\n\tTAILQ_FOREACH(r, &utc->vm_info->regions, link) {\n\t\tif (r->attr & (TEE_MATTR_EPHEMERAL | TEE_MATTR_PERMANENT))\n\t\t\tcontinue;\n\t\tif (core_is_buffer_inside(va, size, r->va, r->size))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* return true only if buffer intersects TA private memory */\nbool tee_mmu_is_vbuf_intersect_ta_private(const struct user_ta_ctx *utc,\n\t\t\t\t\t  const void *va, size_t size)\n{\n\tstruct vm_region *r;\n\n\tTAILQ_FOREACH(r, &utc->vm_info->regions, link) {\n\t\tif (r->attr & (TEE_MATTR_EPHEMERAL | TEE_MATTR_PERMANENT))\n\t\t\tcontinue;\n\t\tif (core_is_buffer_intersect(va, size, r->va, r->size))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nTEE_Result tee_mmu_vbuf_to_mobj_offs(const struct user_ta_ctx *utc,\n\t\t\t\t     const void *va, size_t size,\n\t\t\t\t     struct mobj **mobj, size_t *offs)\n{\n\tstruct vm_region *r;\n\n\tTAILQ_FOREACH(r, &utc->vm_info->regions, link) {\n\t\tif (!r->mobj)\n\t\t\tcontinue;\n\t\tif (core_is_buffer_inside(va, size, r->va, r->size)) {\n\t\t\tsize_t poffs;\n\n\t\t\tpoffs = mobj_get_phys_offs(r->mobj,\n\t\t\t\t\t\t   CORE_MMU_USER_PARAM_SIZE);\n\t\t\t*mobj = r->mobj;\n\t\t\t*offs = (vaddr_t)va - r->va + r->offset - poffs;\n\t\t\treturn TEE_SUCCESS;\n\t\t}\n\t}\n\n\treturn TEE_ERROR_BAD_PARAMETERS;\n}\n\nstatic TEE_Result tee_mmu_user_va2pa_attr(const struct user_ta_ctx *utc,\n\t\t\tvoid *ua, paddr_t *pa, uint32_t *attr)\n{\n\tstruct vm_region *region;\n\n\tTAILQ_FOREACH(region, &utc->vm_info->regions, link) {\n\t\tif (!core_is_buffer_inside(ua, 1, region->va, region->size))\n\t\t\tcontinue;\n\n\t\tif (pa) {\n\t\t\tTEE_Result res;\n\t\t\tpaddr_t p;\n\t\t\tsize_t offset;\n\t\t\tsize_t granule;\n\n\t\t\t/*\n\t\t\t * mobj and input user address may each include\n\t\t\t * a specific offset-in-granule position.\n\t\t\t * Drop both to get target physical page base\n\t\t\t * address then apply only user address\n\t\t\t * offset-in-granule.\n\t\t\t * Mapping lowest granule is the small page.\n\t\t\t */\n\t\t\tgranule = MAX(region->mobj->phys_granule,\n\t\t\t\t      (size_t)SMALL_PAGE_SIZE);\n\t\t\tassert(!granule || IS_POWER_OF_TWO(granule));\n\n\t\t\toffset = region->offset +\n\t\t\t\t ROUNDDOWN((vaddr_t)ua - region->va, granule);\n\n\t\t\tres = mobj_get_pa(region->mobj, offset, granule, &p);\n\t\t\tif (res != TEE_SUCCESS)\n\t\t\t\treturn res;\n\n\t\t\t*pa = p | ((vaddr_t)ua & (granule - 1));\n\t\t}\n\t\tif (attr)\n\t\t\t*attr = region->attr;\n\n\t\treturn TEE_SUCCESS;\n\t}\n\n\treturn TEE_ERROR_ACCESS_DENIED;\n}\n\nTEE_Result tee_mmu_user_va2pa_helper(const struct user_ta_ctx *utc, void *ua,\n\t\t\t\t     paddr_t *pa)\n{\n\treturn tee_mmu_user_va2pa_attr(utc, ua, pa, NULL);\n}\n\n/* */\nTEE_Result tee_mmu_user_pa2va_helper(const struct user_ta_ctx *utc,\n\t\t\t\t      paddr_t pa, void **va)\n{\n\tTEE_Result res;\n\tpaddr_t p;\n\tstruct vm_region *region;\n\n\tTAILQ_FOREACH(region, &utc->vm_info->regions, link) {\n\t\tsize_t granule;\n\t\tsize_t size;\n\t\tsize_t ofs;\n\n\t\t/* pa2va is expected only for memory tracked through mobj */\n\t\tif (!region->mobj)\n\t\t\tcontinue;\n\n\t\t/* Physically granulated memory object must be scanned */\n\t\tgranule = region->mobj->phys_granule;\n\t\tassert(!granule || IS_POWER_OF_TWO(granule));\n\n\t\tfor (ofs = region->offset; ofs < region->size; ofs += size) {\n\n\t\t\tif (granule) {\n\t\t\t\t/* From current offset to buffer/granule end */\n\t\t\t\tsize = granule - (ofs & (granule - 1));\n\n\t\t\t\tif (size > (region->size - ofs))\n\t\t\t\t\tsize = region->size - ofs;\n\t\t\t} else\n\t\t\t\tsize = region->size;\n\n\t\t\tres = mobj_get_pa(region->mobj, ofs, granule, &p);\n\t\t\tif (res != TEE_SUCCESS)\n\t\t\t\treturn res;\n\n\t\t\tif (core_is_buffer_inside(pa, 1, p, size)) {\n\t\t\t\t/* Remove region offset (mobj phys offset) */\n\t\t\t\tofs -= region->offset;\n\t\t\t\t/* Get offset-in-granule */\n\t\t\t\tp = pa - p;\n\n\t\t\t\t*va = (void *)(region->va + ofs + (vaddr_t)p);\n\t\t\t\treturn TEE_SUCCESS;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn TEE_ERROR_ACCESS_DENIED;\n}\n\nTEE_Result tee_mmu_check_access_rights(const struct user_ta_ctx *utc,\n\t\t\t\t       uint32_t flags, uaddr_t uaddr,\n\t\t\t\t       size_t len)\n{\n\tuaddr_t a;\n\tsize_t addr_incr = MIN(CORE_MMU_USER_CODE_SIZE,\n\t\t\t       CORE_MMU_USER_PARAM_SIZE);\n\n\tif (ADD_OVERFLOW(uaddr, len, &a))\n\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\tif ((flags & TEE_MEMORY_ACCESS_NONSECURE) &&\n\t    (flags & TEE_MEMORY_ACCESS_SECURE))\n\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\t/*\n\t * Rely on TA private memory test to check if address range is private\n\t * to TA or not.\n\t */\n\tif (!(flags & TEE_MEMORY_ACCESS_ANY_OWNER) &&\n\t   !tee_mmu_is_vbuf_inside_ta_private(utc, (void *)uaddr, len))\n\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\tfor (a = uaddr; a < (uaddr + len); a += addr_incr) {\n\t\tuint32_t attr;\n\t\tTEE_Result res;\n\n\t\tres = tee_mmu_user_va2pa_attr(utc, (void *)a, NULL, &attr);\n\t\tif (res != TEE_SUCCESS)\n\t\t\treturn res;\n\n\t\tif ((flags & TEE_MEMORY_ACCESS_NONSECURE) &&\n\t\t    (attr & TEE_MATTR_SECURE))\n\t\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\t\tif ((flags & TEE_MEMORY_ACCESS_SECURE) &&\n\t\t    !(attr & TEE_MATTR_SECURE))\n\t\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\t\tif ((flags & TEE_MEMORY_ACCESS_WRITE) && !(attr & TEE_MATTR_UW))\n\t\t\treturn TEE_ERROR_ACCESS_DENIED;\n\t\tif ((flags & TEE_MEMORY_ACCESS_READ) && !(attr & TEE_MATTR_UR))\n\t\t\treturn TEE_ERROR_ACCESS_DENIED;\n\t}\n\n\treturn TEE_SUCCESS;\n}\n\nvoid tee_mmu_set_ctx(struct tee_ta_ctx *ctx)\n{\n\tstruct thread_specific_data *tsd = thread_get_tsd();\n\n\tcore_mmu_set_user_map(NULL);\n\t/*\n\t * No matter what happens below, the current user TA will not be\n\t * current any longer. Make sure pager is in sync with that.\n\t * This function has to be called before there's a chance that\n\t * pgt_free_unlocked() is called.\n\t *\n\t * Save translation tables in a cache if it's a user TA.\n\t */\n\tpgt_free(&tsd->pgt_cache, tsd->ctx && is_user_ta_ctx(tsd->ctx));\n\n\tif (ctx && is_user_ta_ctx(ctx)) {\n\t\tstruct core_mmu_user_map map;\n\t\tstruct user_ta_ctx *utc = to_user_ta_ctx(ctx);\n\n\t\tcore_mmu_create_user_map(utc, &map);\n\t\tcore_mmu_set_user_map(&map);\n\t\ttee_pager_assign_uta_tables(utc);\n\t}\n\ttsd->ctx = ctx;\n}\n\nstruct tee_ta_ctx *tee_mmu_get_ctx(void)\n{\n\treturn thread_get_tsd()->ctx;\n}\n\nvoid teecore_init_ta_ram(void)\n{\n\tvaddr_t s;\n\tvaddr_t e;\n\tpaddr_t ps;\n\tpaddr_t pe;\n\n\t/* get virtual addr/size of RAM where TA are loaded/executedNSec\n\t * shared mem allcated from teecore */\n\tcore_mmu_get_mem_by_type(MEM_AREA_TA_RAM, &s, &e);\n\tps = virt_to_phys((void *)s);\n\tpe = virt_to_phys((void *)(e - 1)) + 1;\n\n\tif (!ps || (ps & CORE_MMU_USER_CODE_MASK) ||\n\t    !pe || (pe & CORE_MMU_USER_CODE_MASK))\n\t\tpanic(\"invalid TA RAM\");\n\n\t/* extra check: we could rely on  core_mmu_get_mem_by_type() */\n\tif (!tee_pbuf_is_sec(ps, pe - ps))\n\t\tpanic(\"TA RAM is not secure\");\n\n\tif (!tee_mm_is_empty(&tee_mm_sec_ddr))\n\t\tpanic(\"TA RAM pool is not empty\");\n\n\t/* remove previous config and init TA ddr memory pool */\n\ttee_mm_final(&tee_mm_sec_ddr);\n\ttee_mm_init(&tee_mm_sec_ddr, ps, pe, CORE_MMU_USER_CODE_SHIFT,\n\t\t    TEE_MM_POOL_NO_FLAGS);\n}\n\nvoid teecore_init_pub_ram(void)\n{\n\tvaddr_t s;\n\tvaddr_t e;\n\n\t/* get virtual addr/size of NSec shared mem allcated from teecore */\n\tcore_mmu_get_mem_by_type(MEM_AREA_NSEC_SHM, &s, &e);\n\n\tif (s >= e || s & SMALL_PAGE_MASK || e & SMALL_PAGE_MASK)\n\t\tpanic(\"invalid PUB RAM\");\n\n\t/* extra check: we could rely on  core_mmu_get_mem_by_type() */\n\tif (!tee_vbuf_is_non_sec(s, e - s))\n\t\tpanic(\"PUB RAM is not non-secure\");\n\n#ifdef CFG_PL310\n\t/* Allocate statically the l2cc mutex */\n\ttee_l2cc_store_mutex_boot_pa(virt_to_phys((void *)s));\n\ts += sizeof(uint32_t);\t\t\t/* size of a pl310 mutex */\n\ts =  ROUNDUP(s, SMALL_PAGE_SIZE);\t/* keep required alignment */\n#endif\n\n\tdefault_nsec_shm_paddr = virt_to_phys((void *)s);\n\tdefault_nsec_shm_size = e - s;\n}\n\nuint32_t tee_mmu_user_get_cache_attr(struct user_ta_ctx *utc, void *va)\n{\n\tuint32_t attr;\n\n\tif (tee_mmu_user_va2pa_attr(utc, va, NULL, &attr) != TEE_SUCCESS)\n\t\tpanic(\"cannot get attr\");\n\n\treturn (attr >> TEE_MATTR_CACHE_SHIFT) & TEE_MATTR_CACHE_MASK;\n}\n"], "fixing_code": ["// SPDX-License-Identifier: BSD-2-Clause\n/*\n * Copyright (c) 2016, Linaro Limited\n * Copyright (c) 2014, STMicroelectronics International N.V.\n */\n\n#include <arm.h>\n#include <assert.h>\n#include <kernel/panic.h>\n#include <kernel/spinlock.h>\n#include <kernel/tee_common.h>\n#include <kernel/tee_misc.h>\n#include <kernel/tlb_helpers.h>\n#include <mm/core_memprot.h>\n#include <mm/core_mmu.h>\n#include <mm/mobj.h>\n#include <mm/pgt_cache.h>\n#include <mm/tee_mm.h>\n#include <mm/tee_mmu.h>\n#include <mm/tee_mmu_types.h>\n#include <mm/tee_pager.h>\n#include <sm/optee_smc.h>\n#include <stdlib.h>\n#include <tee_api_defines_extensions.h>\n#include <tee_api_types.h>\n#include <trace.h>\n#include <types_ext.h>\n#include <user_ta_header.h>\n#include <util.h>\n\n#ifdef CFG_PL310\n#include <kernel/tee_l2cc_mutex.h>\n#endif\n\n#define TEE_MMU_UDATA_ATTR\t\t(TEE_MATTR_VALID_BLOCK | \\\n\t\t\t\t\t TEE_MATTR_PRW | TEE_MATTR_URW | \\\n\t\t\t\t\t TEE_MATTR_SECURE)\n#define TEE_MMU_UCODE_ATTR\t\t(TEE_MATTR_VALID_BLOCK | \\\n\t\t\t\t\t TEE_MATTR_PRW | TEE_MATTR_URWX | \\\n\t\t\t\t\t TEE_MATTR_SECURE)\n\n#define TEE_MMU_UCACHE_DEFAULT_ATTR\t(TEE_MATTR_CACHE_CACHED << \\\n\t\t\t\t\t TEE_MATTR_CACHE_SHIFT)\n\nstatic vaddr_t select_va_in_range(vaddr_t prev_end, uint32_t prev_attr,\n\t\t\t\t  vaddr_t next_begin, uint32_t next_attr,\n\t\t\t\t  const struct vm_region *reg)\n{\n\tsize_t granul;\n\tconst uint32_t a = TEE_MATTR_EPHEMERAL | TEE_MATTR_PERMANENT;\n\tsize_t pad;\n\tvaddr_t begin_va;\n\tvaddr_t end_va;\n\n\t/*\n\t * Insert an unmapped entry to separate regions with differing\n\t * TEE_MATTR_EPHEMERAL TEE_MATTR_PERMANENT bits as they never are\n\t * to be contiguous with another region.\n\t */\n\tif (prev_attr && (prev_attr & a) != (reg->attr & a))\n\t\tpad = SMALL_PAGE_SIZE;\n\telse\n\t\tpad = 0;\n\n\tgranul = SMALL_PAGE_SIZE;\n#ifndef CFG_WITH_LPAE\n\tif ((prev_attr & TEE_MATTR_SECURE) != (reg->attr & TEE_MATTR_SECURE))\n\t\tgranul = CORE_MMU_PGDIR_SIZE;\n#endif\n\tbegin_va = ROUNDUP(prev_end + pad, granul);\n\tif (reg->va) {\n\t\tif (reg->va < begin_va)\n\t\t\treturn 0;\n\t\tbegin_va = reg->va;\n\t}\n\n\tif (next_attr && (next_attr & a) != (reg->attr & a))\n\t\tpad = SMALL_PAGE_SIZE;\n\telse\n\t\tpad = 0;\n\n\tgranul = SMALL_PAGE_SIZE;\n#ifndef CFG_WITH_LPAE\n\tif ((next_attr & TEE_MATTR_SECURE) != (reg->attr & TEE_MATTR_SECURE))\n\t\tgranul = CORE_MMU_PGDIR_SIZE;\n#endif\n\tend_va = ROUNDUP(begin_va + reg->size + pad, granul);\n\n\tif (end_va <= next_begin) {\n\t\tassert(!reg->va || reg->va == begin_va);\n\t\treturn begin_va;\n\t}\n\n\treturn 0;\n}\n\nstatic size_t get_num_req_pgts(struct user_ta_ctx *utc, vaddr_t *begin,\n\t\t\t       vaddr_t *end)\n{\n\tvaddr_t b;\n\tvaddr_t e;\n\n\tif (TAILQ_EMPTY(&utc->vm_info->regions)) {\n\t\tcore_mmu_get_user_va_range(&b, NULL);\n\t\te = b;\n\t} else {\n\t\tstruct vm_region *r;\n\n\t\tb = TAILQ_FIRST(&utc->vm_info->regions)->va;\n\t\tr = TAILQ_LAST(&utc->vm_info->regions, vm_region_head);\n\t\te = r->va + r->size;\n\t\tb = ROUNDDOWN(b, CORE_MMU_PGDIR_SIZE);\n\t\te = ROUNDUP(e, CORE_MMU_PGDIR_SIZE);\n\t}\n\n\tif (begin)\n\t\t*begin = b;\n\tif (end)\n\t\t*end = e;\n\treturn (e - b) >> CORE_MMU_PGDIR_SHIFT;\n}\n\nstatic TEE_Result alloc_pgt(struct user_ta_ctx *utc)\n{\n\tstruct thread_specific_data *tsd __maybe_unused;\n\tvaddr_t b;\n\tvaddr_t e;\n\tsize_t ntbl;\n\n\tntbl = get_num_req_pgts(utc, &b, &e);\n\tif (!pgt_check_avail(ntbl)) {\n\t\tEMSG(\"%zu page tables not available\", ntbl);\n\t\treturn TEE_ERROR_OUT_OF_MEMORY;\n\t}\n\n#ifdef CFG_PAGED_USER_TA\n\ttsd = thread_get_tsd();\n\tif (&utc->ctx == tsd->ctx) {\n\t\t/*\n\t\t * The supplied utc is the current active utc, allocate the\n\t\t * page tables too as the pager needs to use them soon.\n\t\t */\n\t\tpgt_alloc(&tsd->pgt_cache, &utc->ctx, b, e - 1);\n\t}\n#endif\n\n\treturn TEE_SUCCESS;\n}\n\nstatic void free_pgt(struct user_ta_ctx *utc, vaddr_t base, size_t size)\n{\n\tstruct thread_specific_data *tsd = thread_get_tsd();\n\tstruct pgt_cache *pgt_cache = NULL;\n\n\tif (&utc->ctx == tsd->ctx)\n\t\tpgt_cache = &tsd->pgt_cache;\n\n\tpgt_flush_ctx_range(pgt_cache, &utc->ctx, base, base + size);\n}\n\nstatic TEE_Result umap_add_region(struct vm_info *vmi, struct vm_region *reg)\n{\n\tstruct vm_region *r;\n\tstruct vm_region *prev_r;\n\tvaddr_t va_range_base;\n\tsize_t va_range_size;\n\tvaddr_t va;\n\n\tcore_mmu_get_user_va_range(&va_range_base, &va_range_size);\n\n\t/* Check alignment, it has to be at least SMALL_PAGE based */\n\tif ((reg->va | reg->size) & SMALL_PAGE_MASK)\n\t\treturn TEE_ERROR_ACCESS_CONFLICT;\n\n\t/* Check that the mobj is defined for the entire range */\n\tif ((reg->offset + reg->size) >\n\t     ROUNDUP(reg->mobj->size, SMALL_PAGE_SIZE))\n\t\treturn TEE_ERROR_BAD_PARAMETERS;\n\n\tprev_r = NULL;\n\tTAILQ_FOREACH(r, &vmi->regions, link) {\n\t\tif (TAILQ_FIRST(&vmi->regions) == r) {\n\t\t\tva = select_va_in_range(va_range_base, 0,\n\t\t\t\t\t\tr->va, r->attr, reg);\n\t\t\tif (va) {\n\t\t\t\treg->va = va;\n\t\t\t\tTAILQ_INSERT_HEAD(&vmi->regions, reg, link);\n\t\t\t\treturn TEE_SUCCESS;\n\t\t\t}\n\t\t} else {\n\t\t\tva = select_va_in_range(prev_r->va + prev_r->size,\n\t\t\t\t\t\tprev_r->attr, r->va, r->attr,\n\t\t\t\t\t\treg);\n\t\t\tif (va) {\n\t\t\t\treg->va = va;\n\t\t\t\tTAILQ_INSERT_BEFORE(r, reg, link);\n\t\t\t\treturn TEE_SUCCESS;\n\t\t\t}\n\t\t}\n\t\tprev_r = r;\n\t}\n\n\tr = TAILQ_LAST(&vmi->regions, vm_region_head);\n\tif (r) {\n\t\tva = select_va_in_range(r->va + r->size, r->attr,\n\t\t\t\t\tva_range_base + va_range_size, 0, reg);\n\t\tif (va) {\n\t\t\treg->va = va;\n\t\t\tTAILQ_INSERT_TAIL(&vmi->regions, reg, link);\n\t\t\treturn TEE_SUCCESS;\n\t\t}\n\t} else {\n\t\tva = select_va_in_range(va_range_base, 0,\n\t\t\t\t\tva_range_base + va_range_size, 0, reg);\n\t\tif (va) {\n\t\t\treg->va = va;\n\t\t\tTAILQ_INSERT_HEAD(&vmi->regions, reg, link);\n\t\t\treturn TEE_SUCCESS;\n\t\t}\n\t}\n\n\treturn TEE_ERROR_ACCESS_CONFLICT;\n}\n\nTEE_Result vm_map(struct user_ta_ctx *utc, vaddr_t *va, size_t len,\n\t\t  uint32_t prot, struct mobj *mobj, size_t offs)\n{\n\tTEE_Result res;\n\tstruct vm_region *reg = calloc(1, sizeof(*reg));\n\tuint32_t attr = 0;\n\tconst uint32_t prot_mask = TEE_MATTR_PROT_MASK | TEE_MATTR_PERMANENT |\n\t\t\t\t   TEE_MATTR_EPHEMERAL;\n\n\tif (!reg)\n\t\treturn TEE_ERROR_OUT_OF_MEMORY;\n\n\tif (prot & ~prot_mask) {\n\t\tres = TEE_ERROR_BAD_PARAMETERS;\n\t\tgoto err_free_reg;\n\t}\n\n\tif (!mobj_is_paged(mobj)) {\n\t\tuint32_t cattr;\n\n\t\tres = mobj_get_cattr(mobj, &cattr);\n\t\tif (res)\n\t\t\tgoto err_free_reg;\n\t\tattr |= cattr << TEE_MATTR_CACHE_SHIFT;\n\t}\n\tattr |= TEE_MATTR_VALID_BLOCK;\n\tif (mobj_is_secure(mobj))\n\t\tattr |= TEE_MATTR_SECURE;\n\n\treg->mobj = mobj;\n\treg->offset = offs;\n\treg->va = *va;\n\treg->size = ROUNDUP(len, SMALL_PAGE_SIZE);\n\treg->attr = attr | prot;\n\n\tres = umap_add_region(utc->vm_info, reg);\n\tif (res)\n\t\tgoto err_free_reg;\n\n\tres = alloc_pgt(utc);\n\tif (res)\n\t\tgoto err_rem_reg;\n\n\tif (!(reg->attr & (TEE_MATTR_EPHEMERAL | TEE_MATTR_PERMANENT)) &&\n\t    mobj_is_paged(mobj)) {\n\t\tif (!tee_pager_add_uta_area(utc, reg->va, reg->size)) {\n\t\t\tres = TEE_ERROR_GENERIC;\n\t\t\tgoto err_rem_reg;\n\t\t}\n\t}\n\n\t/*\n\t * If the context currently is active set it again to update\n\t * the mapping.\n\t */\n\tif (thread_get_tsd()->ctx == &utc->ctx)\n\t\ttee_mmu_set_ctx(&utc->ctx);\n\n\t*va = reg->va;\n\n\treturn TEE_SUCCESS;\n\nerr_rem_reg:\n\tTAILQ_REMOVE(&utc->vm_info->regions, reg, link);\nerr_free_reg:\n\tfree(reg);\n\treturn res;\n}\n\nTEE_Result vm_set_prot(struct user_ta_ctx *utc, vaddr_t va, size_t len,\n\t\t       uint32_t prot)\n{\n\tstruct vm_region *r;\n\n\t/*\n\t * To keep thing simple: specified va and len has to match exactly\n\t * with an already registered region.\n\t */\n\tTAILQ_FOREACH(r, &utc->vm_info->regions, link) {\n\t\tif (core_is_buffer_intersect(r->va, r->size, va, len)) {\n\t\t\tif (r->va != va || r->size != len)\n\t\t\t\treturn TEE_ERROR_BAD_PARAMETERS;\n\t\t\tif (mobj_is_paged(r->mobj)) {\n\t\t\t\tif (!tee_pager_set_uta_area_attr(utc, va, len,\n\t\t\t\t\t\t\t\t prot))\n\t\t\t\t\treturn TEE_ERROR_GENERIC;\n\t\t\t} else if ((prot & TEE_MATTR_UX) &&\n\t\t\t\t   (r->attr & (TEE_MATTR_UW | TEE_MATTR_PW))) {\n\t\t\t\tcache_op_inner(DCACHE_AREA_CLEAN,\n\t\t\t\t\t       (void *)va, len);\n\t\t\t\tcache_op_inner(ICACHE_AREA_INVALIDATE,\n\t\t\t\t\t       (void *)va, len);\n\t\t\t}\n\t\t\tr->attr &= ~TEE_MATTR_PROT_MASK;\n\t\t\tr->attr |= prot & TEE_MATTR_PROT_MASK;\n\t\t\treturn TEE_SUCCESS;\n\t\t}\n\t}\n\n\treturn TEE_ERROR_ITEM_NOT_FOUND;\n}\n\n\nstatic TEE_Result map_kinit(struct user_ta_ctx *utc __maybe_unused)\n{\n\tTEE_Result res;\n\tstruct mobj *mobj;\n\tsize_t offs;\n\tvaddr_t va;\n\tsize_t sz;\n\n\tthread_get_user_kcode(&mobj, &offs, &va, &sz);\n\tif (sz) {\n\t\tres = vm_map(utc, &va, sz, TEE_MATTR_PRX | TEE_MATTR_PERMANENT,\n\t\t\t     mobj, offs);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\n\tthread_get_user_kdata(&mobj, &offs, &va, &sz);\n\tif (sz)\n\t\treturn vm_map(utc, &va, sz, TEE_MATTR_PRW | TEE_MATTR_PERMANENT,\n\t\t\t      mobj, offs);\n\n\treturn TEE_SUCCESS;\n}\n\nTEE_Result vm_info_init(struct user_ta_ctx *utc)\n{\n\tTEE_Result res;\n\tuint32_t asid = asid_alloc();\n\n\tif (!asid) {\n\t\tDMSG(\"Failed to allocate ASID\");\n\t\treturn TEE_ERROR_GENERIC;\n\t}\n\n\tutc->vm_info = calloc(1, sizeof(*utc->vm_info));\n\tif (!utc->vm_info) {\n\t\tasid_free(asid);\n\t\treturn TEE_ERROR_OUT_OF_MEMORY;\n\t}\n\tTAILQ_INIT(&utc->vm_info->regions);\n\tutc->vm_info->asid = asid;\n\n\tres = map_kinit(utc);\n\tif (res)\n\t\tvm_info_final(utc);\n\treturn res;\n}\n\nstatic void umap_remove_region(struct vm_info *vmi, struct vm_region *reg)\n{\n\tTAILQ_REMOVE(&vmi->regions, reg, link);\n\tfree(reg);\n}\n\nstatic void clear_param_map(struct user_ta_ctx *utc)\n{\n\tstruct vm_region *next_r;\n\tstruct vm_region *r;\n\n\tTAILQ_FOREACH_SAFE(r, &utc->vm_info->regions, link, next_r)\n\t\tif (r->attr & TEE_MATTR_EPHEMERAL)\n\t\t\tumap_remove_region(utc->vm_info, r);\n}\n\nstatic TEE_Result param_mem_to_user_va(struct user_ta_ctx *utc,\n\t\t\t\t       struct param_mem *mem, void **user_va)\n{\n\tstruct vm_region *region;\n\n\tTAILQ_FOREACH(region, &utc->vm_info->regions, link) {\n\t\tvaddr_t va;\n\t\tsize_t phys_offs;\n\n\t\tif (!(region->attr & TEE_MATTR_EPHEMERAL))\n\t\t\tcontinue;\n\t\tif (mem->mobj != region->mobj)\n\t\t\tcontinue;\n\t\tif (mem->offs < region->offset)\n\t\t\tcontinue;\n\t\tif (mem->offs >= (region->offset + region->size))\n\t\t\tcontinue;\n\t\tphys_offs = mobj_get_phys_offs(mem->mobj,\n\t\t\t\t\t       CORE_MMU_USER_PARAM_SIZE);\n\t\tva = region->va + mem->offs + phys_offs - region->offset;\n\t\t*user_va = (void *)va;\n\t\treturn TEE_SUCCESS;\n\t}\n\treturn TEE_ERROR_GENERIC;\n}\n\nstatic int cmp_param_mem(const void *a0, const void *a1)\n{\n\tconst struct param_mem *m1 = a1;\n\tconst struct param_mem *m0 = a0;\n\tint ret;\n\n\t/* Make sure that invalid param_mem are placed last in the array */\n\tif (!m0->size && !m1->size)\n\t\treturn 0;\n\tif (!m0->size)\n\t\treturn 1;\n\tif (!m1->size)\n\t\treturn -1;\n\n\tret = CMP_TRILEAN(mobj_is_secure(m0->mobj), mobj_is_secure(m1->mobj));\n\tif (ret)\n\t\treturn ret;\n\n\tret = CMP_TRILEAN((vaddr_t)m0->mobj, (vaddr_t)m1->mobj);\n\tif (ret)\n\t\treturn ret;\n\n\tret = CMP_TRILEAN(m0->offs, m1->offs);\n\tif (ret)\n\t\treturn ret;\n\n\treturn CMP_TRILEAN(m0->size, m1->size);\n}\n\nTEE_Result tee_mmu_map_param(struct user_ta_ctx *utc,\n\t\tstruct tee_ta_param *param, void *param_va[TEE_NUM_PARAMS])\n{\n\tTEE_Result res = TEE_SUCCESS;\n\tsize_t n;\n\tsize_t m;\n\tstruct param_mem mem[TEE_NUM_PARAMS];\n\n\tmemset(mem, 0, sizeof(mem));\n\tfor (n = 0; n < TEE_NUM_PARAMS; n++) {\n\t\tuint32_t param_type = TEE_PARAM_TYPE_GET(param->types, n);\n\t\tsize_t phys_offs;\n\n\t\tif (param_type != TEE_PARAM_TYPE_MEMREF_INPUT &&\n\t\t    param_type != TEE_PARAM_TYPE_MEMREF_OUTPUT &&\n\t\t    param_type != TEE_PARAM_TYPE_MEMREF_INOUT)\n\t\t\tcontinue;\n\t\tphys_offs = mobj_get_phys_offs(param->u[n].mem.mobj,\n\t\t\t\t\t       CORE_MMU_USER_PARAM_SIZE);\n\t\tmem[n].mobj = param->u[n].mem.mobj;\n\t\tmem[n].offs = ROUNDDOWN(phys_offs + param->u[n].mem.offs,\n\t\t\t\t\tCORE_MMU_USER_PARAM_SIZE);\n\t\tmem[n].size = ROUNDUP(phys_offs + param->u[n].mem.offs -\n\t\t\t\t      mem[n].offs + param->u[n].mem.size,\n\t\t\t\t      CORE_MMU_USER_PARAM_SIZE);\n\t}\n\n\t/*\n\t * Sort arguments so size = 0 is last, secure mobjs first, then by\n\t * mobj pointer value since those entries can't be merged either,\n\t * finally by offset.\n\t *\n\t * This should result in a list where all mergeable entries are\n\t * next to each other and unused/invalid entries are at the end.\n\t */\n\tqsort(mem, TEE_NUM_PARAMS, sizeof(struct param_mem), cmp_param_mem);\n\n\tfor (n = 1, m = 0; n < TEE_NUM_PARAMS && mem[n].size; n++) {\n\t\tif (mem[n].mobj == mem[m].mobj &&\n\t\t    (mem[n].offs == (mem[m].offs + mem[m].size) ||\n\t\t     core_is_buffer_intersect(mem[m].offs, mem[m].size,\n\t\t\t\t\t      mem[n].offs, mem[n].size))) {\n\t\t\tmem[m].size = mem[n].offs + mem[n].size - mem[m].offs;\n\t\t\tcontinue;\n\t\t}\n\t\tm++;\n\t\tif (n != m)\n\t\t\tmem[m] = mem[n];\n\t}\n\t/*\n\t * We'd like 'm' to be the number of valid entries. Here 'm' is the\n\t * index of the last valid entry if the first entry is valid, else\n\t * 0.\n\t */\n\tif (mem[0].size)\n\t\tm++;\n\n\t/* Clear all the param entries as they can hold old information */\n\tclear_param_map(utc);\n\n\tfor (n = 0; n < m; n++) {\n\t\tvaddr_t va = 0;\n\t\tconst uint32_t prot = TEE_MATTR_PRW | TEE_MATTR_URW |\n\t\t\t\t      TEE_MATTR_EPHEMERAL;\n\n\t\tres = vm_map(utc, &va, mem[n].size, prot, mem[n].mobj,\n\t\t\t     mem[n].offs);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\n\tfor (n = 0; n < TEE_NUM_PARAMS; n++) {\n\t\tuint32_t param_type = TEE_PARAM_TYPE_GET(param->types, n);\n\n\t\tif (param_type != TEE_PARAM_TYPE_MEMREF_INPUT &&\n\t\t    param_type != TEE_PARAM_TYPE_MEMREF_OUTPUT &&\n\t\t    param_type != TEE_PARAM_TYPE_MEMREF_INOUT)\n\t\t\tcontinue;\n\t\tif (param->u[n].mem.size == 0)\n\t\t\tcontinue;\n\n\t\tres = param_mem_to_user_va(utc, &param->u[n].mem, param_va + n);\n\t\tif (res != TEE_SUCCESS)\n\t\t\treturn res;\n\t}\n\n\treturn alloc_pgt(utc);\n}\n\nTEE_Result tee_mmu_add_rwmem(struct user_ta_ctx *utc, struct mobj *mobj,\n\t\t\t     vaddr_t *va)\n{\n\tTEE_Result res;\n\tstruct vm_region *reg = calloc(1, sizeof(*reg));\n\n\tif (!reg)\n\t\treturn TEE_ERROR_OUT_OF_MEMORY;\n\n\treg->mobj = mobj;\n\treg->offset = 0;\n\treg->va = 0;\n\treg->size = ROUNDUP(mobj->size, SMALL_PAGE_SIZE);\n\tif (mobj_is_secure(mobj))\n\t\treg->attr = TEE_MATTR_SECURE;\n\telse\n\t\treg->attr = 0;\n\n\tres = umap_add_region(utc->vm_info, reg);\n\tif (res) {\n\t\tfree(reg);\n\t\treturn res;\n\t}\n\n\tres = alloc_pgt(utc);\n\tif (res)\n\t\tumap_remove_region(utc->vm_info, reg);\n\telse\n\t\t*va = reg->va;\n\n\treturn res;\n}\n\nvoid tee_mmu_rem_rwmem(struct user_ta_ctx *utc, struct mobj *mobj, vaddr_t va)\n{\n\tstruct vm_region *reg;\n\n\tTAILQ_FOREACH(reg, &utc->vm_info->regions, link) {\n\t\tif (reg->mobj == mobj && reg->va == va) {\n\t\t\tfree_pgt(utc, reg->va, reg->size);\n\t\t\tumap_remove_region(utc->vm_info, reg);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nvoid vm_info_final(struct user_ta_ctx *utc)\n{\n\tif (!utc->vm_info)\n\t\treturn;\n\n\t/* clear MMU entries to avoid clash when asid is reused */\n\ttlbi_asid(utc->vm_info->asid);\n\n\tasid_free(utc->vm_info->asid);\n\twhile (!TAILQ_EMPTY(&utc->vm_info->regions))\n\t\tumap_remove_region(utc->vm_info,\n\t\t\t\t   TAILQ_FIRST(&utc->vm_info->regions));\n\tfree(utc->vm_info);\n\tutc->vm_info = NULL;\n}\n\n/* return true only if buffer fits inside TA private memory */\nbool tee_mmu_is_vbuf_inside_ta_private(const struct user_ta_ctx *utc,\n\t\t\t\t  const void *va, size_t size)\n{\n\tstruct vm_region *r;\n\n\tTAILQ_FOREACH(r, &utc->vm_info->regions, link) {\n\t\tif (r->attr & (TEE_MATTR_EPHEMERAL | TEE_MATTR_PERMANENT))\n\t\t\tcontinue;\n\t\tif (core_is_buffer_inside(va, size, r->va, r->size))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* return true only if buffer intersects TA private memory */\nbool tee_mmu_is_vbuf_intersect_ta_private(const struct user_ta_ctx *utc,\n\t\t\t\t\t  const void *va, size_t size)\n{\n\tstruct vm_region *r;\n\n\tTAILQ_FOREACH(r, &utc->vm_info->regions, link) {\n\t\tif (r->attr & (TEE_MATTR_EPHEMERAL | TEE_MATTR_PERMANENT))\n\t\t\tcontinue;\n\t\tif (core_is_buffer_intersect(va, size, r->va, r->size))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nTEE_Result tee_mmu_vbuf_to_mobj_offs(const struct user_ta_ctx *utc,\n\t\t\t\t     const void *va, size_t size,\n\t\t\t\t     struct mobj **mobj, size_t *offs)\n{\n\tstruct vm_region *r;\n\n\tTAILQ_FOREACH(r, &utc->vm_info->regions, link) {\n\t\tif (!r->mobj)\n\t\t\tcontinue;\n\t\tif (core_is_buffer_inside(va, size, r->va, r->size)) {\n\t\t\tsize_t poffs;\n\n\t\t\tpoffs = mobj_get_phys_offs(r->mobj,\n\t\t\t\t\t\t   CORE_MMU_USER_PARAM_SIZE);\n\t\t\t*mobj = r->mobj;\n\t\t\t*offs = (vaddr_t)va - r->va + r->offset - poffs;\n\t\t\treturn TEE_SUCCESS;\n\t\t}\n\t}\n\n\treturn TEE_ERROR_BAD_PARAMETERS;\n}\n\nstatic TEE_Result tee_mmu_user_va2pa_attr(const struct user_ta_ctx *utc,\n\t\t\tvoid *ua, paddr_t *pa, uint32_t *attr)\n{\n\tstruct vm_region *region;\n\n\tTAILQ_FOREACH(region, &utc->vm_info->regions, link) {\n\t\tif (!core_is_buffer_inside(ua, 1, region->va, region->size))\n\t\t\tcontinue;\n\n\t\tif (pa) {\n\t\t\tTEE_Result res;\n\t\t\tpaddr_t p;\n\t\t\tsize_t offset;\n\t\t\tsize_t granule;\n\n\t\t\t/*\n\t\t\t * mobj and input user address may each include\n\t\t\t * a specific offset-in-granule position.\n\t\t\t * Drop both to get target physical page base\n\t\t\t * address then apply only user address\n\t\t\t * offset-in-granule.\n\t\t\t * Mapping lowest granule is the small page.\n\t\t\t */\n\t\t\tgranule = MAX(region->mobj->phys_granule,\n\t\t\t\t      (size_t)SMALL_PAGE_SIZE);\n\t\t\tassert(!granule || IS_POWER_OF_TWO(granule));\n\n\t\t\toffset = region->offset +\n\t\t\t\t ROUNDDOWN((vaddr_t)ua - region->va, granule);\n\n\t\t\tres = mobj_get_pa(region->mobj, offset, granule, &p);\n\t\t\tif (res != TEE_SUCCESS)\n\t\t\t\treturn res;\n\n\t\t\t*pa = p | ((vaddr_t)ua & (granule - 1));\n\t\t}\n\t\tif (attr)\n\t\t\t*attr = region->attr;\n\n\t\treturn TEE_SUCCESS;\n\t}\n\n\treturn TEE_ERROR_ACCESS_DENIED;\n}\n\nTEE_Result tee_mmu_user_va2pa_helper(const struct user_ta_ctx *utc, void *ua,\n\t\t\t\t     paddr_t *pa)\n{\n\treturn tee_mmu_user_va2pa_attr(utc, ua, pa, NULL);\n}\n\n/* */\nTEE_Result tee_mmu_user_pa2va_helper(const struct user_ta_ctx *utc,\n\t\t\t\t      paddr_t pa, void **va)\n{\n\tTEE_Result res;\n\tpaddr_t p;\n\tstruct vm_region *region;\n\n\tTAILQ_FOREACH(region, &utc->vm_info->regions, link) {\n\t\tsize_t granule;\n\t\tsize_t size;\n\t\tsize_t ofs;\n\n\t\t/* pa2va is expected only for memory tracked through mobj */\n\t\tif (!region->mobj)\n\t\t\tcontinue;\n\n\t\t/* Physically granulated memory object must be scanned */\n\t\tgranule = region->mobj->phys_granule;\n\t\tassert(!granule || IS_POWER_OF_TWO(granule));\n\n\t\tfor (ofs = region->offset; ofs < region->size; ofs += size) {\n\n\t\t\tif (granule) {\n\t\t\t\t/* From current offset to buffer/granule end */\n\t\t\t\tsize = granule - (ofs & (granule - 1));\n\n\t\t\t\tif (size > (region->size - ofs))\n\t\t\t\t\tsize = region->size - ofs;\n\t\t\t} else\n\t\t\t\tsize = region->size;\n\n\t\t\tres = mobj_get_pa(region->mobj, ofs, granule, &p);\n\t\t\tif (res != TEE_SUCCESS)\n\t\t\t\treturn res;\n\n\t\t\tif (core_is_buffer_inside(pa, 1, p, size)) {\n\t\t\t\t/* Remove region offset (mobj phys offset) */\n\t\t\t\tofs -= region->offset;\n\t\t\t\t/* Get offset-in-granule */\n\t\t\t\tp = pa - p;\n\n\t\t\t\t*va = (void *)(region->va + ofs + (vaddr_t)p);\n\t\t\t\treturn TEE_SUCCESS;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn TEE_ERROR_ACCESS_DENIED;\n}\n\nTEE_Result tee_mmu_check_access_rights(const struct user_ta_ctx *utc,\n\t\t\t\t       uint32_t flags, uaddr_t uaddr,\n\t\t\t\t       size_t len)\n{\n\tuaddr_t a;\n\tuaddr_t end_addr = 0;\n\tsize_t addr_incr = MIN(CORE_MMU_USER_CODE_SIZE,\n\t\t\t       CORE_MMU_USER_PARAM_SIZE);\n\n\tif (ADD_OVERFLOW(uaddr, len, &end_addr))\n\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\tif ((flags & TEE_MEMORY_ACCESS_NONSECURE) &&\n\t    (flags & TEE_MEMORY_ACCESS_SECURE))\n\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\t/*\n\t * Rely on TA private memory test to check if address range is private\n\t * to TA or not.\n\t */\n\tif (!(flags & TEE_MEMORY_ACCESS_ANY_OWNER) &&\n\t   !tee_mmu_is_vbuf_inside_ta_private(utc, (void *)uaddr, len))\n\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\tfor (a = ROUNDDOWN(uaddr, addr_incr); a < end_addr; a += addr_incr) {\n\t\tuint32_t attr;\n\t\tTEE_Result res;\n\n\t\tres = tee_mmu_user_va2pa_attr(utc, (void *)a, NULL, &attr);\n\t\tif (res != TEE_SUCCESS)\n\t\t\treturn res;\n\n\t\tif ((flags & TEE_MEMORY_ACCESS_NONSECURE) &&\n\t\t    (attr & TEE_MATTR_SECURE))\n\t\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\t\tif ((flags & TEE_MEMORY_ACCESS_SECURE) &&\n\t\t    !(attr & TEE_MATTR_SECURE))\n\t\t\treturn TEE_ERROR_ACCESS_DENIED;\n\n\t\tif ((flags & TEE_MEMORY_ACCESS_WRITE) && !(attr & TEE_MATTR_UW))\n\t\t\treturn TEE_ERROR_ACCESS_DENIED;\n\t\tif ((flags & TEE_MEMORY_ACCESS_READ) && !(attr & TEE_MATTR_UR))\n\t\t\treturn TEE_ERROR_ACCESS_DENIED;\n\t}\n\n\treturn TEE_SUCCESS;\n}\n\nvoid tee_mmu_set_ctx(struct tee_ta_ctx *ctx)\n{\n\tstruct thread_specific_data *tsd = thread_get_tsd();\n\n\tcore_mmu_set_user_map(NULL);\n\t/*\n\t * No matter what happens below, the current user TA will not be\n\t * current any longer. Make sure pager is in sync with that.\n\t * This function has to be called before there's a chance that\n\t * pgt_free_unlocked() is called.\n\t *\n\t * Save translation tables in a cache if it's a user TA.\n\t */\n\tpgt_free(&tsd->pgt_cache, tsd->ctx && is_user_ta_ctx(tsd->ctx));\n\n\tif (ctx && is_user_ta_ctx(ctx)) {\n\t\tstruct core_mmu_user_map map;\n\t\tstruct user_ta_ctx *utc = to_user_ta_ctx(ctx);\n\n\t\tcore_mmu_create_user_map(utc, &map);\n\t\tcore_mmu_set_user_map(&map);\n\t\ttee_pager_assign_uta_tables(utc);\n\t}\n\ttsd->ctx = ctx;\n}\n\nstruct tee_ta_ctx *tee_mmu_get_ctx(void)\n{\n\treturn thread_get_tsd()->ctx;\n}\n\nvoid teecore_init_ta_ram(void)\n{\n\tvaddr_t s;\n\tvaddr_t e;\n\tpaddr_t ps;\n\tpaddr_t pe;\n\n\t/* get virtual addr/size of RAM where TA are loaded/executedNSec\n\t * shared mem allcated from teecore */\n\tcore_mmu_get_mem_by_type(MEM_AREA_TA_RAM, &s, &e);\n\tps = virt_to_phys((void *)s);\n\tpe = virt_to_phys((void *)(e - 1)) + 1;\n\n\tif (!ps || (ps & CORE_MMU_USER_CODE_MASK) ||\n\t    !pe || (pe & CORE_MMU_USER_CODE_MASK))\n\t\tpanic(\"invalid TA RAM\");\n\n\t/* extra check: we could rely on  core_mmu_get_mem_by_type() */\n\tif (!tee_pbuf_is_sec(ps, pe - ps))\n\t\tpanic(\"TA RAM is not secure\");\n\n\tif (!tee_mm_is_empty(&tee_mm_sec_ddr))\n\t\tpanic(\"TA RAM pool is not empty\");\n\n\t/* remove previous config and init TA ddr memory pool */\n\ttee_mm_final(&tee_mm_sec_ddr);\n\ttee_mm_init(&tee_mm_sec_ddr, ps, pe, CORE_MMU_USER_CODE_SHIFT,\n\t\t    TEE_MM_POOL_NO_FLAGS);\n}\n\nvoid teecore_init_pub_ram(void)\n{\n\tvaddr_t s;\n\tvaddr_t e;\n\n\t/* get virtual addr/size of NSec shared mem allcated from teecore */\n\tcore_mmu_get_mem_by_type(MEM_AREA_NSEC_SHM, &s, &e);\n\n\tif (s >= e || s & SMALL_PAGE_MASK || e & SMALL_PAGE_MASK)\n\t\tpanic(\"invalid PUB RAM\");\n\n\t/* extra check: we could rely on  core_mmu_get_mem_by_type() */\n\tif (!tee_vbuf_is_non_sec(s, e - s))\n\t\tpanic(\"PUB RAM is not non-secure\");\n\n#ifdef CFG_PL310\n\t/* Allocate statically the l2cc mutex */\n\ttee_l2cc_store_mutex_boot_pa(virt_to_phys((void *)s));\n\ts += sizeof(uint32_t);\t\t\t/* size of a pl310 mutex */\n\ts =  ROUNDUP(s, SMALL_PAGE_SIZE);\t/* keep required alignment */\n#endif\n\n\tdefault_nsec_shm_paddr = virt_to_phys((void *)s);\n\tdefault_nsec_shm_size = e - s;\n}\n\nuint32_t tee_mmu_user_get_cache_attr(struct user_ta_ctx *utc, void *va)\n{\n\tuint32_t attr;\n\n\tif (tee_mmu_user_va2pa_attr(utc, va, NULL, &attr) != TEE_SUCCESS)\n\t\tpanic(\"cannot get attr\");\n\n\treturn (attr >> TEE_MATTR_CACHE_SHIFT) & TEE_MATTR_CACHE_MASK;\n}\n"], "filenames": ["core/arch/arm/mm/tee_mmu.c"], "buggy_code_start_loc": [759], "buggy_code_end_loc": [779], "fixing_code_start_loc": [760], "fixing_code_end_loc": [780], "type": "CWE-787", "message": "Linaro/OP-TEE OP-TEE 3.3.0 and earlier is affected by: Boundary crossing. The impact is: Memory corruption of the TEE itself. The component is: optee_os. The fixed version is: 3.4.0 and later.", "other": {"cve": {"id": "CVE-2019-1010293", "sourceIdentifier": "josh@bress.net", "published": "2019-07-15T18:15:11.303", "lastModified": "2020-08-24T17:37:01.140", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Linaro/OP-TEE OP-TEE 3.3.0 and earlier is affected by: Boundary crossing. The impact is: Memory corruption of the TEE itself. The component is: optee_os. The fixed version is: 3.4.0 and later."}, {"lang": "es", "value": "Linaro / OP-TEE OP-TEE 3.3.0 y versiones anteriores se ven afectados por: Cruce de l\u00edmites. El impacto es: Corrupci\u00f3n de memoria del propio ETE. El componente es: optee_os. La versi\u00f3n solucionada es: 3.4.0 y posteriores."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 7.5}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linaro:op-tee:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.3.0", "matchCriteriaId": "BE2AA919-55C6-4AE4-B611-EBEB412B2370"}]}]}], "references": [{"url": "https://github.com/OP-TEE/optee_os/commit/95f36d661f2b75887772ea28baaad904bde96970", "source": "josh@bress.net", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/OP-TEE/optee_os/commit/95f36d661f2b75887772ea28baaad904bde96970"}}