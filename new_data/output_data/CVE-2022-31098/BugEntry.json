{"buggy_code": ["package clustersmngr\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cheshir/ttlcache\"\n\t\"github.com/go-logr/logr\"\n\t\"github.com/weaveworks/weave-gitops/core/nsaccess\"\n\t\"github.com/weaveworks/weave-gitops/pkg/server/auth\"\n\tv1 \"k8s.io/api/core/v1\"\n\tapiruntime \"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/util/wait\"\n\t\"k8s.io/client-go/rest\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n)\n\n//go:generate go run github.com/maxbrunsfeld/counterfeiter/v6 -generate\n\nconst (\n\tuserNamespaceTTL = 30 * time.Second\n\t// How often we need to stop the world and remove outdated records.\n\tuserNamespaceResolution = 30 * time.Second\n\twatchClustersFrequency  = 30 * time.Second\n\twatchNamespaceFrequency = 30 * time.Second\n)\n\n// ClientsFactory is a factory for creating clients for clusters\n//counterfeiter:generate . ClientsFactory\ntype ClientsFactory interface {\n\t// GetImpersonatedClient returns the clusters client for the given user\n\tGetImpersonatedClient(ctx context.Context, user *auth.UserPrincipal) (Client, error)\n\t// UpdateClusters updates the clusters list\n\tUpdateClusters(ctx context.Context) error\n\t// UpdateNamespaces updates the namespaces all namespaces for all clusters\n\tUpdateNamespaces(ctx context.Context) error\n\t// UpdateUserNamespaces updates the cache of accessible namespaces for the user\n\tUpdateUserNamespaces(ctx context.Context, user *auth.UserPrincipal)\n\t// GetServerClient returns the cluster client with gitops server permissions\n\tGetServerClient(ctx context.Context) (Client, error)\n\t// GetClustersNamespaces returns the namespaces for all clusters\n\tGetClustersNamespaces() map[string][]v1.Namespace\n\t// GetUserNamespaces returns the accessible namespaces for the user\n\tGetUserNamespaces(user *auth.UserPrincipal) map[string][]v1.Namespace\n\t// Start starts go routines to keep clusters and namespaces lists up to date\n\tStart(ctx context.Context)\n}\n\ntype clientsFactory struct {\n\tclustersFetcher ClusterFetcher\n\tnsChecker       nsaccess.Checker\n\tlog             logr.Logger\n\n\t// list of clusters returned by the clusters fetcher\n\tclusters *Clusters\n\t// string containing ordered list of cluster names, used to refresh dependent caches\n\tclustersHash string\n\t// the lists of all namespaces of each cluster\n\tclustersNamespaces *ClustersNamespaces\n\t// lists of namespaces accessible by the user on every cluster\n\tusersNamespaces *UsersNamespaces\n\n\tinitialClustersLoad chan bool\n\tscheme              *apiruntime.Scheme\n}\n\nfunc NewClientFactory(fetcher ClusterFetcher, nsChecker nsaccess.Checker, logger logr.Logger, scheme *apiruntime.Scheme) ClientsFactory {\n\treturn &clientsFactory{\n\t\tclustersFetcher:     fetcher,\n\t\tnsChecker:           nsChecker,\n\t\tclusters:            &Clusters{},\n\t\tclustersNamespaces:  &ClustersNamespaces{},\n\t\tusersNamespaces:     &UsersNamespaces{Cache: ttlcache.New(userNamespaceResolution)},\n\t\tlog:                 logger,\n\t\tinitialClustersLoad: make(chan bool),\n\t\tscheme:              scheme,\n\t}\n}\n\nfunc (cf *clientsFactory) Start(ctx context.Context) {\n\tgo cf.watchClusters(ctx)\n\tgo cf.watchNamespaces(ctx)\n}\n\nfunc (cf *clientsFactory) watchClusters(ctx context.Context) {\n\tif err := cf.UpdateClusters(ctx); err != nil {\n\t\tcf.log.Error(err, \"failed updating clusters\")\n\t}\n\n\tcf.initialClustersLoad <- true\n\n\tif err := wait.PollImmediateInfinite(watchClustersFrequency, func() (bool, error) {\n\t\tif err := cf.UpdateClusters(ctx); err != nil {\n\t\t\tcf.log.Error(err, \"Failed to update clusters\")\n\t\t}\n\n\t\treturn false, nil\n\t}); err != nil {\n\t\tcf.log.Error(err, \"failed polling clusters\")\n\t}\n}\n\nfunc (cf *clientsFactory) UpdateClusters(ctx context.Context) error {\n\tclusters, err := cf.clustersFetcher.Fetch(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to fetch clusters: %w\", err)\n\t}\n\n\tcf.clusters.Set(clusters)\n\n\treturn nil\n}\n\nfunc (cf *clientsFactory) watchNamespaces(ctx context.Context) {\n\t// waits the first load of cluster to start watching namespaces\n\t<-cf.initialClustersLoad\n\n\tif err := wait.PollImmediateInfinite(watchNamespaceFrequency, func() (bool, error) {\n\t\tif err := cf.UpdateNamespaces(ctx); err != nil {\n\t\t\tcf.log.Error(err, \"Failed to update namespaces\")\n\t\t}\n\n\t\treturn false, nil\n\t}); err != nil {\n\t\tcf.log.Error(err, \"failed polling namespaces\")\n\t}\n}\n\nfunc (cf *clientsFactory) UpdateNamespaces(ctx context.Context) error {\n\tclients, err := clientsForClusters(cf.clusters.Get())\n\tif err != nil {\n\t\tcf.log.Error(err, \"failed to create clients for\", \"clusters\", cf.clusters.Get())\n\t\treturn err\n\t}\n\n\tcf.syncCaches()\n\n\twg := sync.WaitGroup{}\n\n\tfor clusterName, c := range clients {\n\t\twg.Add(1)\n\n\t\tgo func(clusterName string, c client.Client) {\n\t\t\tdefer wg.Done()\n\n\t\t\tnsList := &v1.NamespaceList{}\n\n\t\t\tif err := c.List(ctx, nsList); err != nil {\n\t\t\t\tcf.log.Error(err, \"failed listing namespaces\", \"cluster\", clusterName)\n\t\t\t}\n\n\t\t\tcf.clustersNamespaces.Set(clusterName, nsList.Items)\n\t\t}(clusterName, c)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\nfunc (cf *clientsFactory) GetClustersNamespaces() map[string][]v1.Namespace {\n\treturn cf.clustersNamespaces.namespaces\n}\n\nfunc (cf *clientsFactory) syncCaches() {\n\tnewHash := cf.clusters.Hash()\n\n\tif newHash != cf.clustersHash {\n\t\tcf.clustersNamespaces.Clear()\n\t\tcf.usersNamespaces.Clear()\n\t\tcf.clustersHash = newHash\n\t}\n}\n\nfunc (cf *clientsFactory) GetImpersonatedClient(ctx context.Context, user *auth.UserPrincipal) (Client, error) {\n\tpool := NewClustersClientsPool(cf.scheme)\n\n\tfor _, cluster := range cf.clusters.Get() {\n\t\tif err := pool.Add(ClientConfigWithUser(user), cluster); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed adding cluster client to pool: %w\", err)\n\t\t}\n\t}\n\n\treturn NewClient(pool, cf.userNsList(ctx, user)), nil\n}\n\nfunc (cf *clientsFactory) GetServerClient(ctx context.Context) (Client, error) {\n\tpool := NewClustersClientsPool(cf.scheme)\n\n\tfor _, cluster := range cf.clusters.Get() {\n\t\tif err := pool.Add(restConfigFromCluster, cluster); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed adding cluster client to pool: %w\", err)\n\t\t}\n\t}\n\n\treturn NewClient(pool, cf.clustersNamespaces.namespaces), nil\n}\n\nfunc (cf *clientsFactory) UpdateUserNamespaces(ctx context.Context, user *auth.UserPrincipal) {\n\twg := sync.WaitGroup{}\n\n\tfor _, cluster := range cf.clusters.Get() {\n\t\twg.Add(1)\n\n\t\tgo func(cluster Cluster) {\n\t\t\tdefer wg.Done()\n\n\t\t\tclusterNs := cf.clustersNamespaces.Get(cluster.Name)\n\n\t\t\tfilteredNs, err := cf.nsChecker.FilterAccessibleNamespaces(ctx, impersonatedConfig(cluster, user), clusterNs)\n\t\t\tif err != nil {\n\t\t\t\tcf.log.Error(err, \"failed filtering namespaces\", \"cluster\", cluster.Name, \"user\", user.ID)\n\t\t\t}\n\n\t\t\tcf.usersNamespaces.Set(user, cluster.Name, filteredNs)\n\t\t}(cluster)\n\t}\n\n\twg.Wait()\n}\n\nfunc (cf *clientsFactory) GetUserNamespaces(user *auth.UserPrincipal) map[string][]v1.Namespace {\n\treturn cf.usersNamespaces.GetAll(user, cf.clusters.Get())\n}\n\nfunc (cf *clientsFactory) userNsList(ctx context.Context, user *auth.UserPrincipal) map[string][]v1.Namespace {\n\tuserNamespaces := cf.GetUserNamespaces(user)\n\tif len(userNamespaces) > 0 {\n\t\treturn userNamespaces\n\t}\n\n\tcf.UpdateUserNamespaces(ctx, user)\n\n\treturn cf.GetUserNamespaces(user)\n}\n\nfunc impersonatedConfig(cluster Cluster, user *auth.UserPrincipal) *rest.Config {\n\tshallowCopy := *restConfigFromCluster(cluster)\n\n\tshallowCopy.Impersonate = rest.ImpersonationConfig{\n\t\tUserName: user.ID,\n\t\tGroups:   user.Groups,\n\t}\n\n\treturn &shallowCopy\n}\n\nfunc clientsForClusters(clusters []Cluster) (map[string]client.Client, error) {\n\tclients := map[string]client.Client{}\n\n\tfor _, cluster := range clusters {\n\t\tc, err := client.New(restConfigFromCluster(cluster), client.Options{})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed creating client for cluster %s: %w\", cluster.Name, err)\n\t\t}\n\n\t\tclients[cluster.Name] = c\n\t}\n\n\treturn clients, nil\n}\n\nfunc restConfigFromCluster(cluster Cluster) *rest.Config {\n\treturn &rest.Config{\n\t\tHost:            cluster.Server,\n\t\tBearerToken:     cluster.BearerToken,\n\t\tTLSClientConfig: cluster.TLSConfig,\n\t\tQPS:             ClientQPS,\n\t\tBurst:           ClientBurst,\n\t}\n}\n"], "fixing_code": ["package clustersmngr\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cheshir/ttlcache\"\n\t\"github.com/go-logr/logr\"\n\t\"github.com/weaveworks/weave-gitops/core/nsaccess\"\n\t\"github.com/weaveworks/weave-gitops/pkg/server/auth\"\n\tv1 \"k8s.io/api/core/v1\"\n\tapiruntime \"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/util/wait\"\n\t\"k8s.io/client-go/rest\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n)\n\n//go:generate go run github.com/maxbrunsfeld/counterfeiter/v6 -generate\n\nconst (\n\tuserNamespaceTTL = 30 * time.Second\n\t// How often we need to stop the world and remove outdated records.\n\tuserNamespaceResolution = 30 * time.Second\n\twatchClustersFrequency  = 30 * time.Second\n\twatchNamespaceFrequency = 30 * time.Second\n)\n\n// ClientsFactory is a factory for creating clients for clusters\n//counterfeiter:generate . ClientsFactory\ntype ClientsFactory interface {\n\t// GetImpersonatedClient returns the clusters client for the given user\n\tGetImpersonatedClient(ctx context.Context, user *auth.UserPrincipal) (Client, error)\n\t// UpdateClusters updates the clusters list\n\tUpdateClusters(ctx context.Context) error\n\t// UpdateNamespaces updates the namespaces all namespaces for all clusters\n\tUpdateNamespaces(ctx context.Context) error\n\t// UpdateUserNamespaces updates the cache of accessible namespaces for the user\n\tUpdateUserNamespaces(ctx context.Context, user *auth.UserPrincipal)\n\t// GetServerClient returns the cluster client with gitops server permissions\n\tGetServerClient(ctx context.Context) (Client, error)\n\t// GetClustersNamespaces returns the namespaces for all clusters\n\tGetClustersNamespaces() map[string][]v1.Namespace\n\t// GetUserNamespaces returns the accessible namespaces for the user\n\tGetUserNamespaces(user *auth.UserPrincipal) map[string][]v1.Namespace\n\t// Start starts go routines to keep clusters and namespaces lists up to date\n\tStart(ctx context.Context)\n}\n\ntype clientsFactory struct {\n\tclustersFetcher ClusterFetcher\n\tnsChecker       nsaccess.Checker\n\tlog             logr.Logger\n\n\t// list of clusters returned by the clusters fetcher\n\tclusters *Clusters\n\t// string containing ordered list of cluster names, used to refresh dependent caches\n\tclustersHash string\n\t// the lists of all namespaces of each cluster\n\tclustersNamespaces *ClustersNamespaces\n\t// lists of namespaces accessible by the user on every cluster\n\tusersNamespaces *UsersNamespaces\n\n\tinitialClustersLoad chan bool\n\tscheme              *apiruntime.Scheme\n}\n\nfunc NewClientFactory(fetcher ClusterFetcher, nsChecker nsaccess.Checker, logger logr.Logger, scheme *apiruntime.Scheme) ClientsFactory {\n\treturn &clientsFactory{\n\t\tclustersFetcher:     fetcher,\n\t\tnsChecker:           nsChecker,\n\t\tclusters:            &Clusters{},\n\t\tclustersNamespaces:  &ClustersNamespaces{},\n\t\tusersNamespaces:     &UsersNamespaces{Cache: ttlcache.New(userNamespaceResolution)},\n\t\tlog:                 logger,\n\t\tinitialClustersLoad: make(chan bool),\n\t\tscheme:              scheme,\n\t}\n}\n\nfunc (cf *clientsFactory) Start(ctx context.Context) {\n\tgo cf.watchClusters(ctx)\n\tgo cf.watchNamespaces(ctx)\n}\n\nfunc (cf *clientsFactory) watchClusters(ctx context.Context) {\n\tif err := cf.UpdateClusters(ctx); err != nil {\n\t\tcf.log.Error(err, \"failed updating clusters\")\n\t}\n\n\tcf.initialClustersLoad <- true\n\n\tif err := wait.PollImmediateInfinite(watchClustersFrequency, func() (bool, error) {\n\t\tif err := cf.UpdateClusters(ctx); err != nil {\n\t\t\tcf.log.Error(err, \"Failed to update clusters\")\n\t\t}\n\n\t\treturn false, nil\n\t}); err != nil {\n\t\tcf.log.Error(err, \"failed polling clusters\")\n\t}\n}\n\nfunc (cf *clientsFactory) UpdateClusters(ctx context.Context) error {\n\tclusters, err := cf.clustersFetcher.Fetch(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to fetch clusters: %w\", err)\n\t}\n\n\tcf.clusters.Set(clusters)\n\n\treturn nil\n}\n\nfunc (cf *clientsFactory) watchNamespaces(ctx context.Context) {\n\t// waits the first load of cluster to start watching namespaces\n\t<-cf.initialClustersLoad\n\n\tif err := wait.PollImmediateInfinite(watchNamespaceFrequency, func() (bool, error) {\n\t\tif err := cf.UpdateNamespaces(ctx); err != nil {\n\t\t\tcf.log.Error(err, \"Failed to update namespaces\")\n\t\t}\n\n\t\treturn false, nil\n\t}); err != nil {\n\t\tcf.log.Error(err, \"failed polling namespaces\")\n\t}\n}\n\nfunc (cf *clientsFactory) UpdateNamespaces(ctx context.Context) error {\n\tclients, err := clientsForClusters(cf.clusters.Get())\n\tif err != nil {\n\t\tcf.log.Error(err, \"failed to create client\")\n\t\treturn err\n\t}\n\n\tcf.syncCaches()\n\n\twg := sync.WaitGroup{}\n\n\tfor clusterName, c := range clients {\n\t\twg.Add(1)\n\n\t\tgo func(clusterName string, c client.Client) {\n\t\t\tdefer wg.Done()\n\n\t\t\tnsList := &v1.NamespaceList{}\n\n\t\t\tif err := c.List(ctx, nsList); err != nil {\n\t\t\t\tcf.log.Error(err, \"failed listing namespaces\", \"cluster\", clusterName)\n\t\t\t}\n\n\t\t\tcf.clustersNamespaces.Set(clusterName, nsList.Items)\n\t\t}(clusterName, c)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\nfunc (cf *clientsFactory) GetClustersNamespaces() map[string][]v1.Namespace {\n\treturn cf.clustersNamespaces.namespaces\n}\n\nfunc (cf *clientsFactory) syncCaches() {\n\tnewHash := cf.clusters.Hash()\n\n\tif newHash != cf.clustersHash {\n\t\tcf.clustersNamespaces.Clear()\n\t\tcf.usersNamespaces.Clear()\n\t\tcf.clustersHash = newHash\n\t}\n}\n\nfunc (cf *clientsFactory) GetImpersonatedClient(ctx context.Context, user *auth.UserPrincipal) (Client, error) {\n\tpool := NewClustersClientsPool(cf.scheme)\n\n\tfor _, cluster := range cf.clusters.Get() {\n\t\tif err := pool.Add(ClientConfigWithUser(user), cluster); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed adding cluster client to pool: %w\", err)\n\t\t}\n\t}\n\n\treturn NewClient(pool, cf.userNsList(ctx, user)), nil\n}\n\nfunc (cf *clientsFactory) GetServerClient(ctx context.Context) (Client, error) {\n\tpool := NewClustersClientsPool(cf.scheme)\n\n\tfor _, cluster := range cf.clusters.Get() {\n\t\tif err := pool.Add(restConfigFromCluster, cluster); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed adding cluster client to pool: %w\", err)\n\t\t}\n\t}\n\n\treturn NewClient(pool, cf.clustersNamespaces.namespaces), nil\n}\n\nfunc (cf *clientsFactory) UpdateUserNamespaces(ctx context.Context, user *auth.UserPrincipal) {\n\twg := sync.WaitGroup{}\n\n\tfor _, cluster := range cf.clusters.Get() {\n\t\twg.Add(1)\n\n\t\tgo func(cluster Cluster) {\n\t\t\tdefer wg.Done()\n\n\t\t\tclusterNs := cf.clustersNamespaces.Get(cluster.Name)\n\n\t\t\tfilteredNs, err := cf.nsChecker.FilterAccessibleNamespaces(ctx, impersonatedConfig(cluster, user), clusterNs)\n\t\t\tif err != nil {\n\t\t\t\tcf.log.Error(err, \"failed filtering namespaces\", \"cluster\", cluster.Name, \"user\", user.ID)\n\t\t\t}\n\n\t\t\tcf.usersNamespaces.Set(user, cluster.Name, filteredNs)\n\t\t}(cluster)\n\t}\n\n\twg.Wait()\n}\n\nfunc (cf *clientsFactory) GetUserNamespaces(user *auth.UserPrincipal) map[string][]v1.Namespace {\n\treturn cf.usersNamespaces.GetAll(user, cf.clusters.Get())\n}\n\nfunc (cf *clientsFactory) userNsList(ctx context.Context, user *auth.UserPrincipal) map[string][]v1.Namespace {\n\tuserNamespaces := cf.GetUserNamespaces(user)\n\tif len(userNamespaces) > 0 {\n\t\treturn userNamespaces\n\t}\n\n\tcf.UpdateUserNamespaces(ctx, user)\n\n\treturn cf.GetUserNamespaces(user)\n}\n\nfunc impersonatedConfig(cluster Cluster, user *auth.UserPrincipal) *rest.Config {\n\tshallowCopy := *restConfigFromCluster(cluster)\n\n\tshallowCopy.Impersonate = rest.ImpersonationConfig{\n\t\tUserName: user.ID,\n\t\tGroups:   user.Groups,\n\t}\n\n\treturn &shallowCopy\n}\n\nfunc clientsForClusters(clusters []Cluster) (map[string]client.Client, error) {\n\tclients := map[string]client.Client{}\n\n\tfor _, cluster := range clusters {\n\t\tc, err := client.New(restConfigFromCluster(cluster), client.Options{})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed creating client for cluster %s: %w\", cluster.Name, err)\n\t\t}\n\n\t\tclients[cluster.Name] = c\n\t}\n\n\treturn clients, nil\n}\n\nfunc restConfigFromCluster(cluster Cluster) *rest.Config {\n\treturn &rest.Config{\n\t\tHost:            cluster.Server,\n\t\tBearerToken:     cluster.BearerToken,\n\t\tTLSClientConfig: cluster.TLSConfig,\n\t\tQPS:             ClientQPS,\n\t\tBurst:           ClientBurst,\n\t}\n}\n"], "filenames": ["core/clustersmngr/factory.go"], "buggy_code_start_loc": [134], "buggy_code_end_loc": [135], "fixing_code_start_loc": [134], "fixing_code_end_loc": [135], "type": "CWE-532", "message": "Weave GitOps is a simple open source developer platform for people who want cloud native applications, without needing Kubernetes expertise. A vulnerability in the logging of Weave GitOps could allow an authenticated remote attacker to view sensitive cluster configurations, aka KubeConfg, of registered Kubernetes clusters, including the service account tokens in plain text from Weave GitOps's pod logs on the management cluster. An unauthorized remote attacker can also view these sensitive configurations from external log storage if enabled by the management cluster. This vulnerability is due to the client factory dumping cluster configurations and their service account tokens when the cluster manager tries to connect to an API server of a registered cluster, and a connection error occurs. An attacker could exploit this vulnerability by either accessing logs of a pod of Weave GitOps, or from external log storage and obtaining all cluster configurations of registered clusters. A successful exploit could allow the attacker to use those cluster configurations to manage the registered Kubernetes clusters. This vulnerability has been fixed by commit 567356f471353fb5c676c77f5abc2a04631d50ca. Users should upgrade to Weave GitOps core version v0.8.1-rc.6 or newer. There is no known workaround for this vulnerability.", "other": {"cve": {"id": "CVE-2022-31098", "sourceIdentifier": "security-advisories@github.com", "published": "2022-06-27T22:15:09.180", "lastModified": "2022-07-11T13:54:03.967", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Weave GitOps is a simple open source developer platform for people who want cloud native applications, without needing Kubernetes expertise. A vulnerability in the logging of Weave GitOps could allow an authenticated remote attacker to view sensitive cluster configurations, aka KubeConfg, of registered Kubernetes clusters, including the service account tokens in plain text from Weave GitOps's pod logs on the management cluster. An unauthorized remote attacker can also view these sensitive configurations from external log storage if enabled by the management cluster. This vulnerability is due to the client factory dumping cluster configurations and their service account tokens when the cluster manager tries to connect to an API server of a registered cluster, and a connection error occurs. An attacker could exploit this vulnerability by either accessing logs of a pod of Weave GitOps, or from external log storage and obtaining all cluster configurations of registered clusters. A successful exploit could allow the attacker to use those cluster configurations to manage the registered Kubernetes clusters. This vulnerability has been fixed by commit 567356f471353fb5c676c77f5abc2a04631d50ca. Users should upgrade to Weave GitOps core version v0.8.1-rc.6 or newer. There is no known workaround for this vulnerability."}, {"lang": "es", "value": "Weave GitOps es una sencilla plataforma de c\u00f3digo abierto para desarrolladores que quieren aplicaciones nativas en la nube, sin necesidad de tener conocimientos de Kubernetes. Una vulnerabilidad en el registro de Weave GitOps podr\u00eda permitir a un atacante remoto autenticado visualizar las configuraciones confidenciales del cl\u00faster, tambi\u00e9n conocido como KubeConfg, de los cl\u00fasteres Kubernetes registrados, incluyendo los tokens de la cuenta de servicio en texto plano desde los registros de pods de Weave GitOps en el cl\u00faster de administraci\u00f3n. Un atacante remoto no autorizado tambi\u00e9n puede visualizar estas configuraciones confidenciales desde el almacenamiento de registros externos si el cl\u00faster de administraci\u00f3n lo permite. Esta vulnerabilidad es debido a que la f\u00e1brica de clientes vuelca las configuraciones del cl\u00faster y sus tokens de cuentas de servicio cuando el administrador del cl\u00faster intenta conectarse a un servidor API de un cl\u00faster registrado, y es producido un error de conexi\u00f3n. Un atacante podr\u00eda explotar esta vulnerabilidad al acceder a los logs de un pod de Weave GitOps, o desde un almacenamiento de logs externo y obteniendo todas las configuraciones de cluster de los clusters registrados. Una explotaci\u00f3n con \u00e9xito podr\u00eda permitir al atacante usar esas configuraciones de cl\u00faster para administrar los cl\u00fasteres Kubernetes registrados. Esta vulnerabilidad ha sido corregida por el commit 567356f471353fb5c676c77f5abc2a04631d50ca. Los usuarios deben actualizar a versi\u00f3n core de Weave GitOps versiones v0.8.1-rc.6 o m\u00e1s reciente. No se conoce ninguna mitigaci\u00f3n para esta vulnerabilidad"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:R/S:C/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.0, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 2.3, "impactScore": 6.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-532"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:weave:weave_gitops:*:*:*:*:*:*:*:*", "versionEndExcluding": "0.8.1", "matchCriteriaId": "53DDEDAD-3D68-43B9-8604-4AE247861015"}, {"vulnerable": true, "criteria": "cpe:2.3:a:weave:weave_gitops:0.8.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "7D046128-FF45-4245-A062-09F00A62C09B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:weave:weave_gitops:0.8.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "884C62D8-3161-4C42-8C1D-F7CBEFFF95A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:weave:weave_gitops:0.8.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "CAE586FB-BC82-4EEF-A738-0CECCF151243"}, {"vulnerable": true, "criteria": "cpe:2.3:a:weave:weave_gitops:0.8.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "AF888D0E-B8E2-4D18-B97B-D33EDC018941"}, {"vulnerable": true, "criteria": "cpe:2.3:a:weave:weave_gitops:0.8.1:rc5:*:*:*:*:*:*", "matchCriteriaId": "3D054881-C362-4511-BE51-34393E2A9AD5"}]}]}], "references": [{"url": "https://github.com/weaveworks/weave-gitops/commit/567356f471353fb5c676c77f5abc2a04631d50ca", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/weaveworks/weave-gitops/security/advisories/GHSA-xggc-qprg-x6mw", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/weaveworks/weave-gitops/commit/567356f471353fb5c676c77f5abc2a04631d50ca"}}