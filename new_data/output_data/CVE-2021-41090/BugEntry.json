{"buggy_code": ["# Main (unreleased)\n\n- [FEATURE] (beta) Enable experimental config urls for fetching remote configs. Currently,\n   only HTTP/S is supported. Use `-experiment.config-urls.enable` flag to turn this on. (@rlankfo)\n\n- [ENHANCEMENT] Traces: Improved pod association in PromSD processor (@mapno)\n\n# v0.21.1 (2021-11-18)\n\n- [BUGFIX] Fix panic when using postgres_exporter integration (@saputradharma)\n\n- [BUGFIX] Fix panic when dnsamsq_exporter integration tried to log a warning (@rfratto)\n\n- [BUGFIX] Statsd Integration: Adding logger instance to the statsd mapper instantiation. (@gaantunes)\n\n- [BUGFIX] Statsd Integration: Fix issue where mapped metrics weren't exposed to the integration. (@mattdurham)\n\n- [BUGFIX] Operator: fix bug where version was a required field (@rfratto)\n\n- [BUGFIX] Metrics: Only run WAL cleaner when metrics are being used and a WAL is configured. (@rfratto)\n\n# v0.21.0 (2021-11-17)\n\n- [ENHANCEMENT] Update Cortex dependency to v1.10.0-92-g85c378182. (@rlankfo)\n\n- [ENHANCEMENT] Update Loki dependency to v2.1.0-656-g0ae0d4da1. (@rlankfo)\n\n- [ENHANCEMENT] Update Prometheus dependency to v2.31.0 (@rlankfo)\n\n- [ENHANCEMENT] Add Agent Operator Helm quickstart guide (@hjet)\n\n- [ENHANCEMENT] Reorg Agent Operator quickstart guides (@hjet)\n\n- [BUGFIX] Packaging: Use correct user/group env variables in RPM %post script (@simonc6372)\n\n- [BUGFIX] Validate logs config when using logs_instance with automatic logging processor (@mapno)\n\n- [BUGFIX] Operator: Fix MetricsInstance Service port (@hjet)\n\n- [BUGFIX] Operator: Create govern service per Grafana Agent (@shturman)\n\n- [BUGFIX] Operator: Fix relabel_config directive for PodLogs resource (@hjet)\n\n- [BUGFIX] Traces: Fix `success_logic` code in service graphs processor (@mapno)\n\n- [CHANGE] Self-scraped integrations will now use an SUO-specific value for the `instance` label. (@rfratto)\n\n- [CHANGE] Traces: Changed service graphs store implementation to improve CPU performance (@mapno)\n\n# v0.20.0 (2021-10-28)\n\n- [FEATURE] Operator: The Grafana Agent Operator can now generate a Kubelet\n  service to allow a ServiceMonitor to collect Kubelet and cAdvisor metrics.\n  This requires passing a `--kubelet-service` flag to the Operator in\n  `namespace/name` format (like `kube-system/kubelet`). (@rfratto)\n\n- [FEATURE] Service graphs processor (@mapno)\n\n- [ENHANCEMENT] Updated mysqld_exporter to v0.13.0 (@gaantunes)\n\n- [ENHANCEMENT] Updated postgres_exporter to v0.10.0 (@gaantunes)\n\n- [ENHANCEMENT] Updated redis_exporter to v1.27.1 (@gaantunes)\n\n- [ENHANCEMENT] Updated memcached_exporter to v0.9.0 (@gaantunes)\n\n- [ENHANCEMENT] Updated statsd_exporter to v0.22.2 (@gaantunes)\n\n- [ENHANCEMENT] Updated elasticsearch_exporter to v1.2.1 (@gaantunes)\n\n- [ENHANCEMENT] Add remote write to silent Windows Installer  (@mattdurham)\n\n- [ENHANCEMENT] Updated mongodb_exporter to v0.20.7 (@rfratto)\n\n- [ENHANCEMENT] Updated OTel to v0.36 (@mapno)\n\n- [ENHANCEMENT] Updated statsd_exporter to v0.22.2 (@mattdurham)\n\n- [ENHANCEMENT] Update windows_exporter to v0.16.0 (@rfratto, @mattdurham)\n\n- [ENHANCEMENT] Add send latency to agent dashboard (@bboreham)\n\n- [BUGFIX] Do not immediately cancel context when creating a new trace\n  processor. This was preventing scrape_configs in traces from\n  functioning. (@lheinlen)\n\n- [BUGFIX] Sanitize autologged Loki labels by replacing invalid characters with underscores (@mapno)\n\n- [BUGFIX] Traces: remove extra line feed/spaces/tabs when reading password_file content (@nicoche)\n\n- [BUGFIX] Updated envsubst to v2.0.0-20210730161058-179042472c46. This version has a fix needed for escaping values\n  outside of variable substitutions. (@rlankfo)\n\n- [BUGFIX] Grafana Agent Operator should no longer delete resources matching\n  the names of the resources it manages. (@rfratto)\n\n- [BUGFIX] Grafana Agent Operator will now appropriately assign an\n  `app.kubernetes.io/managed-by=grafana-agent-operator` to all created\n  resources.\n\n- [CHANGE] Configuration API now returns 404 instead of 400 when attempting to get or delete a config\n  which does not exist. (@kgeckhart)\n\n- [CHANGE] The windows_exporter now disables the textfile collector by default. (@rfratto)\n\n- [CHANGE] **Breaking change** push_config is no longer supported in trace's config (@mapno)\n\n# v0.19.0 (2021-09-29)\n\nThis release has breaking changes. Please read [CHANGE] entries carefully and\nconsult the\n[upgrade guide](https://github.com/grafana/agent/blob/main/docs/upgrade-guide/_index.md)\nfor specific instructions.\n\n\n- [FEATURE] Added [Github exporter](https://github.com/infinityworks/github-exporter) integration. (@rgeyer)\n\n- [FEATURE] Add TLS config options for tempo `remote_write`s. (@mapno)\n\n- [FEATURE] Support autologging span attributes as log labels (@mapno)\n\n- [FEATURE] Put Tests requiring Network Access behind a -online flag (@flokli)\n\n- [FEATURE] Add logging support to the Grafana Agent Operator. (@rfratto)\n\n- [FEATURE] Add `operator-detach` command to agentctl to allow zero-downtime\n  upgrades when removing an Operator CRD. (@rfratto)\n\n- [ENHANCEMENT] The Grafana Agent Operator will now default to deploying\n  the matching release version of the Grafana Agent instead of v0.14.0.\n  (@rfratto)\n\n- [ENHANCEMENT] Update OTel dependency to v0.30.0 (@mapno)\n\n- [ENHANCEMENT] Allow reloading configuration using `SIGHUP` signal. (@tharun208)\n\n- [ENHANCEMENT] Add HOSTNAME environment variable to service file to allow for expanding\n  the $HOSTNAME variable in agent config.  (@dfrankel33)\n\n- [ENHANCEMENT] Update jsonnet-libs to 1.21 for Kubernetes 1.21+ compatability. (@MurzNN)\n\n- [ENHANCEMENT] Make method used to add k/v to spans in prom_sd processor\n  configurable. (@mapno)\n\n- [BUGFIX] Regex capture groups like `${1}` will now be kept intact when\n  using `-config.expand-env`. (@rfratto)\n\n- [BUGFIX] The directory of the logs positions file will now properly be created\n  on startup for all instances. (@rfratto)\n\n- [BUGFIX] The Linux system packages will now configure the grafana-agent user\n  to be a member of the adm and systemd-journal groups. This will allow logs to\n  read from journald and /var/log by default. (@rfratto)\n\n- [BUGFIX] Fix collecting filesystem metrics on Mac OS (darwin) in the\n  `node_exporter` integration default config. (@eamonryan)\n\n- [BUGFIX] Remove v0.0.0 flags during build with no explicit release tag (@mattdurham)\n\n- [BUGFIX] Fix issue with global scrape_interval changes not reloading integrations (@kgeckhart)\n\n- [BUGFIX] Grafana Agent Operator will now detect changes to referenced\n  ConfigMaps and Secrets and reload the Agent properly. (@rfratto)\n\n- [BUGFIX] Grafana Agent Operator's object label selectors will now use\n  Kubernetes defaults when undefined (i.e., default to nothing). (@rfratto)\n\n- [BUGFIX] Fix yaml marshalling tag for cert_file in kafka exporter agent config. (@rgeyer)\n\n- [BUGFIX] Fix warn-level logging of dropped targets. (@james-callahan)\n\n- [BUGFIX] Standardize scrape_interval to 1m in examples. (@mattdurham)\n\n- [CHANGE] Breaking change: reduced verbosity of tracing autologging\n  by not logging `STATUS_CODE_UNSET` status codes. (@mapno)\n\n- [CHANGE] Breaking change: Operator: rename Prometheus* CRDs to Metrics* and\n  Prometheus* fields to Metrics*. (@rfratto)\n\n- [CHANGE] Breaking change: Operator: CRDs are no longer referenced using a\n  hyphen in the name to be consistent with how Kubernetes refers to resources.\n  (@rfratto)\n\n- [CHANGE] Breaking change: `prom_instance` in the spanmetrics config is now\n  named `metrics_instance`. (@rfratto)\n\n- [DEPRECATION] The `loki` key at the root of the config file has been\n  deprecated in favor of `logs`. `loki`-named fields in `automatic_logging`\n  have been renamed accordinly: `loki_name` is now `logs_instance_name`,\n  `loki_tag` is now `logs_instance_tag`, and `backend: loki` is now\n  `backend: logs_instance`. (@rfratto)\n\n- [DEPRECATION] The `prometheus` key at the root of the config file has been\n  deprecated in favor of `metrics`. Flag names starting with `prometheus.` have\n  also been deprecated in favor of the same flags with the `metrics.` prefix.\n  Metrics prefixed with `agent_prometheus_` are now prefixed with\n  `agent_metrics_`. (@rfratto)\n\n- [DEPRECATION] The `tempo` key at the root of the config file has been\n  deprecated in favor of `traces`. (@mattdurham)\n\n# v0.18.4 (2021-09-14)\n\n- [BUGFIX] Fix info logging on windows. (@mattdurham)\n\n- [BUGFIX] Scraping service: Ensure that a reshard is scheduled every reshard\n  interval. (@rfratto)\n\n- [CHANGE] Add `agent_prometheus_configs_changed_total` metric to track instance\n  config events. (@rfratto)\n\n# v0.18.3 (2021-09-08)\n\n- [BUGFIX] Register missing metric for configstore consul request duration.\n  (@rfratto)\n\n- [BUGFIX] Logs should contain a caller field with file and line numbers again\n  (@kgeckhart)\n\n- [BUGFIX] In scraping service mode, the polling configuration refresh should\n  honor timeout. (@mattdurham)\n\n- [BUGFIX] In scraping service mode, the lifecycle reshard should happen using a\n  goroutine. (@mattdurham)\n\n- [BUGFIX] In scraping service mode, scraping service can deadlock when\n  reloading during join. (@mattdurham)\n\n- [BUGFIX] Scraping service: prevent more than one refresh from being queued at\n  a time. (@rfratto)\n\n# v0.18.2 (2021-08-12)\n\n- [BUGFIX] Honor the prefix and remove prefix from consul list results (@mattdurham)\n\n# v0.18.1 (2021-08-09)\n\n- [BUGFIX] Reduce number of consul calls when ran in scrape service mode (@mattdurham)\n\n# v0.18.0 (2021-07-29)\n\n- [FEATURE] Added [Github exporter](https://github.com/infinityworks/github-exporter) integration. (@rgeyer)\n\n- [FEATURE] Add support for OTLP HTTP trace exporting. (@mapno)\n\n- [ENHANCEMENT] Switch to drone for releases. (@mattdurham)\n\n- [ENHANCEMENT] Update postgres_exporter to a [branch of](https://github.com/grafana/postgres_exporter/tree/exporter-package-v0.10.0) v0.10.0\n\n- [BUGFIX]  Enabled flag is not being honored. (@mattdurham)\n\n# v0.17.0 (2021-07-15)\n\n- [FEATURE] Added [Kafka Lag exporter](https://github.com/davidmparrott/kafka_exporter)\n  integration. (@gaantunes)\n\n- [BUGFIX] Fix race condition that may occur and result in a panic when\n  initializing scraping service cluster. (@rfratto)\n\n# v0.16.1 (2021-06-22)\n\n- [BUGFIX] Fix issue where replaying a WAL caused incorrect metrics to be sent\n  over remote write. (@rfratto)\n\n# v0.16.0 (2021-06-17)\n\n- [FEATURE] (beta) A Grafana Agent Operator is now available. (@rfratto)\n\n- [ENHANCEMENT] Error messages when installing the Grafana Agent for Grafana\n  Cloud will now be shown. (@rfratto)\n\n- [BUGFIX] Fix a leak in the shared string interner introduced in v0.14.0.\n  This fix was made to a [dependency](https://github.com/grafana/prometheus/pull/21).\n  (@rfratto)\n\n- [BUGFIX] Fix issue where a target will fail to be scraped for the process lifetime\n  if that target had gone down for long enough that its series were removed from\n  the in-memory cache (2 GC cycles). (@rfratto)\n\n# v0.15.0 (2021-06-03)\n\nBREAKING CHANGE: Configuration of Tempo Autologging changed in this release.\nPlease review the [migration\nguide](./docs/migration-guide.md) for details.\n\n- [FEATURE] Add support for exemplars. (@mapno)\n\n- [ENHANCEMENT] Add the option to log to stdout instead of a Loki instance. (@joe-elliott)\n\n- [ENHANCEMENT] Update Cortex dependency to v1.8.0.\n\n- [ENHANCEMENT] Running the Agent as a DaemonSet with host_filter and role: pod\n  should no longer cause unnecessary load against the Kubernetes SD API.\n  (@rfratto)\n\n- [ENHANCEMENT] Update Prometheus to v2.27.0. (@mapno)\n\n- [ENHANCEMENT] Update Loki dependency to d88f3996eaa2. This is a non-release\n  build, and was needed to support exemplars. (@mapno)\n\n- [ENHANCEMENT] Update Cortex dependency to to d382e1d80eaf. This is a\n  non-release build, and was needed to support exemplars. (@mapno)\n\n- [BUGFIX] Host filter relabeling rules should now work. (@rfratto)\n\n- [BUGFIX] Fixed issue where span metrics where being reported with wrong time unit. (@mapno)\n\n- [CHANGE] Intentionally order tracing processors. (@joe-elliott)\n\n# v0.14.0 (2021-05-24)\n\nBREAKING CHANGE: This release has a breaking change for SigV4 support. Please\nread the release notes carefully and our [migration\nguide](./docs/migration-guide.md) to help migrate your configuration files to\nthe new format.\n\nBREAKING CHANGE: For security, the scraping service config API will reject\nconfigs that read credentials from disk to prevent malicious users from reading\nartbirary files and sending their contents over the network. The old behavior\ncan be achieved by enabling `dangerous_allow_reading_files` in the scraping\nservice config.\n\nAs of this release, functionality that is not recommended for production use\nand is expected to change will be tagged interchangably as \"experimental\" or\n\"beta.\"\n\n- [FEATURE] (beta) New integration: windows_exporter (@mattdurham)\n\n- [FEATURE] (beta) Grafana Agent Windows Installer is now included as a release\n  artifact. (@mattdurham)\n\n- [FEATURE] Official M1 Mac release builds will now be generated! Look for\n  `agent-darwin-arm64` and `agentctl-darwin-arm64` in the release assets.\n  (@rfratto)\n\n- [FEATURE] Add support for running as a Windows service (@mattdurham)\n\n- [FEATURE] (beta) Add /-/reload support. It is not recommended to invoke\n  `/-/reload` against the main HTTP server. Instead, two new command-line flags\n  have been added: `--reload-addr` and `--reload-port`. These will launch a\n  `/-/reload`-only HTTP server that can be used to safely reload the Agent's\n  state.  (@rfratto)\n\n- [FEATURE] Add a /-/config endpoint. This endpoint will return the current\n  configuration file with defaults applied that the Agent has loaded from disk.\n  (@rfratto)\n\n- [FEATURE] (beta) Support generating metrics and exposing them via a Prometheus exporter\n  from span data. (@yeya24)\n\n- [FEATURE] Tail-based sampling for tracing pipelines (@mapno)\n\n- [FEATURE] Added Automatic Logging feature for Tempo (@joe-elliott)\n\n- [FEATURE] Disallow reading files from within scraping service configs by\n  default. (@rfratto)\n\n- [FEATURE] Add remote write for span metrics (@mapno)\n\n- [ENHANCEMENT] Support compression for trace export. (@mdisibio)\n\n- [ENHANCEMENT] Add global remote_write configuration that is shared between all\n  instances and integrations. (@mattdurham)\n\n- [ENHANCEMENT] Go 1.16 is now used for all builds of the Agent. (@rfratto)\n\n- [ENHANCEMENT] Update Prometheus dependency to v2.26.0. (@rfratto)\n\n- [ENHANCEMENT] Upgrade `go.opentelemetry.io/collector` to v0.21.0 (@mapno)\n\n- [ENHANCEMENT] Add kafka trace receiver (@mapno)\n\n- [ENHANCEMENT] Support mirroring a trace pipeline to multiple backends (@mapno)\n\n- [ENHANCEMENT] Add  `headers` field in `remote_write` config for Tempo. `headers`\n  specifies HTTP headers to forward to the remote endpoint. (@alexbiehl)\n\n- [ENHANCEMENT] Add silent uninstall to Windows Uninstaller. (@mattdurham)\n\n- [BUGFIX] Native Darwin arm64 builds will no longer crash when writing metrics\n  to the WAL. (@rfratto)\n\n- [BUGFIX] Remote write endpoints that never function across the lifetime of the\n  Agent will no longer prevent the WAL from being truncated. (@rfratto)\n\n- [BUGFIX] Bring back FreeBSD support. (@rfratto)\n\n- [BUGFIX] agentctl will no longer leak WAL resources when retrieving WAL stats. (@rfratto)\n\n- [BUGFIX] Ensure defaults are applied to undefined sections in config file.\n  This fixes a problem where integrations didn't work if `prometheus:` wasn't\n  configured. (@rfratto)\n\n- [BUGFIX] Fixed issue where automatic logging double logged \"svc\". (@joe-elliott)\n\n- [CHANGE] The Grafana Cloud Agent has been renamed to the Grafana Agent.\n  (@rfratto)\n\n- [CHANGE] Instance configs uploaded to the Config Store API will no longer be\n  stored along with the global Prometheus defaults. This is done to allow\n  globals to be updated and re-apply the new global defaults to the configs from\n  the Config Store. (@rfratto)\n\n- [CHANGE] The User-Agent header sent for logs will now be\n  `GrafanaAgent/<version>` (@rfratto)\n\n- [CHANGE] Add `tempo_spanmetrics` namespace in spanmetrics (@mapno)\n\n- [DEPRECATION] `push_config` is now supplanted by `remote_block` and `batch`.\n  `push_config` will be removed in a future version (@mapno)\n\n# v0.13.1 (2021-04-09)\n\n- [BUGFIX] Validate that incoming scraped metrics do not have an empty label\n  set or a label set with duplicate labels, mirroring the behavior of\n  Prometheus. (@rfratto)\n\n# v0.13.0 (2021-02-25)\n\nThe primary branch name has changed from `master` to `main`. You may have to\nupdate your local checkouts of the repository to point at the new branch name.\n\n- [FEATURE] postgres_exporter: Support query_path and disable_default_metrics. (@rfratto)\n\n- [ENHANCEMENT] Support other architectures in installation script. (@rfratto)\n\n- [ENHANCEMENT] Allow specifying custom wal_truncate_frequency per integration.\n  (@rfratto)\n\n- [ENHANCEMENT] The SigV4 region can now be inferred using the shared config\n  (at `$HOME/.aws/config`) or environment variables (via `AWS_CONFIG`).\n  (@rfratto)\n\n- [ENHANCEMENT] Update Prometheus dependency to v2.25.0. (@rfratto)\n\n- [BUGFIX] Not providing an `-addr` flag for `agentctl config-sync` will no\n  longer report an error and will instead use the pre-existing default value.\n  (@rfratto)\n\n- [BUGFIX] Fixed a bug from v0.12.0 where the Loki installation script failed\n  because positions_directory was not set. (@rfratto)\n\n- [BUGFIX] (#400) Reduce the likelihood of dataloss during a remote_write-side\n  outage by increasing the default wal_truncation_frequency to 60m and preventing\n  the WAL from being truncated if the last truncation timestamp hasn't changed.\n  This change increases the size of the WAL on average, and users may configure\n  a lower wal_truncation_frequency to deliberately choose a smaller WAL over\n  write guarantees. (@rfratto)\n\n- [BUGFIX] (#368) Add the ability to read and serve HTTPS integration metrics when\n  given a set certificates (@mattdurham)\n\n# v0.12.0 (2021-02-05)\n\nBREAKING CHANGES: This release has two breaking changes in the configuration\nfile. Please read the release notes carefully and our\n[migration guide](./docs/migration-guide.md) to help migrate your configuration\nfiles to the new format.\n\n- [FEATURE] BREAKING CHANGE: Support for multiple Loki Promtail instances has\n  been added, using the same `configs` array used by the Prometheus subsystem.\n  (@rfratto)\n\n- [FEATURE] BREAKING CHANGE: Support for multiple Tempo instances has\n  been added, using the same `configs` array used by the Prometheus subsystem.\n  (@rfratto)\n\n- [FEATURE] Added [ElasticSearch exporter](https://github.com/justwatchcom/elasticsearch_exporter)\n  integration. (@colega)\n\n- [ENHANCEMENT] `.deb` and `.rpm` packages are now generated for all supported\n  architectures. The architecture of the AMD64 package in the filename has\n  been renamed to `amd64` to stay synchronized with the architecture name\n  presented from other release assets. (@rfratto)\n\n- [ENHANCEMENT] The `/agent/api/v1/targets` API will now include discovered labels\n  on the target pre-relabeling in a `discovered_labels` field. (@rfratto)\n\n- [ENHANCEMENT] Update Loki to 59a34f9867ce. This is a non-release build, and was needed\n  to support multiple Loki instances. (@rfratto)\n\n- [ENHANCEMENT] Scraping service: Unhealthy Agents in the ring will no longer\n  cause job distribution to fail. (@rfratto)\n\n- [ENHANCEMENT] Scraping service: Cortex ring metrics (prefixed with\n  cortex_ring_) will now be registered for tracking the state of the hash\n  ring. (@rfratto)\n\n- [ENHANCEMENT] Scraping service: instance config ownership is now determined by\n  the hash of the instance config name instead of the entire config. This means\n  that updating a config is guaranteed to always hash to the same Agent,\n  reducing the number of metrics gaps. (@rfratto)\n\n- [ENHANCEMENT] Only keep a handful of K8s API server metrics by default to reduce\n  default active series usage. (@hjet)\n\n- [ENHANCEMENT] Go 1.15.8 is now used for all distributions of the Agent.\n  (@rfratto)\n\n- [BUGFIX] `agentctl config-check` will now work correctly when the supplied\n  config file contains integrations. (@hoenn)\n\n# v0.11.0 (2021-01-20)\n\n- [FEATURE] ARMv6 builds of `agent` and `agentctl` will now be included in\n  releases to expand Agent support to cover all models of Raspberry Pis.\n  ARMv6 docker builds are also now available.\n  (@rfratto)\n\n- [FEATURE] Added `config-check` subcommand for `agentctl` that can be used\n  to validate Agent configuration files before attempting to load them in the\n  `agent` itself. (@56quarters)\n\n- [ENHANCEMENT] A sigv4 install script for Prometheus has been added. (@rfratto)\n\n- [ENHANCEMENT] NAMESPACE may be passed as an environment variable to the\n  Kubernetes install scripts to specify an installation namespace. (@rfratto)\n\n- [BUGFIX] The K8s API server scrape job will use the API server Service name\n  when resolving IP addresses for Prometheus service discovery using the\n  \"Endpoints\" role. (@hjet)\n\n- [BUGFIX] The K8s manifests will no longer include the `default/kubernetes` job\n  twice in both the DaemonSet and the Deployment. (@rfratto)\n\n# v0.10.0 (2021-01-13)\n\n- [FEATURE] Prometheus `remote_write` now supports SigV4 authentication using\n  the [AWS default credentials\n  chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html).\n  This enables the Agent to send metrics to Amazon Managed Prometheus without\n  needing the [SigV4 Proxy](https://github.com/awslabs/aws-sigv4-proxy).\n  (@rfratto)\n\n- [ENHANCEMENT] Update `redis_exporter` to v1.15.0. (@rfratto)\n\n- [ENHANCEMENT] `memcached_exporter` has been updated to v0.8.0. (@rfratto)\n\n- [ENHANCEMENT] `process-exporter` has been updated to v0.7.5. (@rfratto)\n\n- [ENHANCEMENT] `wal_cleanup_age` and `wal_cleanup_period` have been added to the\n  top-level Prometheus configuration section. These settings control how Write Ahead\n  Logs (WALs) that are not associated with any instances are cleaned up. By default,\n  WALs not associated with an instance that have not been written in the last 12 hours\n  are eligible to be cleaned up. This cleanup can be disabled by setting `wal_cleanup_period`\n  to `0`. (#304) (@56quarters)\n\n- [ENHANCEMENT] Configuring logs to read from the systemd journal should now\n  work on journals that use +ZSTD compression. (@rfratto)\n\n- [BUGFIX] Integrations will now function if the HTTP listen address was set to\n  a value other than the default. ([#206](https://github.com/grafana/agent/issues/206)) (@mattdurham)\n\n- [BUGFIX] The default Loki installation will now be able to write its positions\n  file. This was prevented by accidentally writing to a readonly volume mount.\n  (@rfratto)\n\n# v0.9.1 (2021-01-04)\n\n- [ENHANCEMENT] agentctl will now be installed by the rpm and deb packages as\n  `grafana-agentctl`. (@rfratto)\n\n# v0.9.0 (2020-12-10)\n\n- [FEATURE] Add support to configure TLS config for the Tempo exporter to use\n  insecure_skip_verify to disable TLS chain verification. (@bombsimon)\n\n- [FEATURE] Add `sample-stats` to `agentctl` to search the WAL and return a\n  summary of samples of series matching the given label selector. (@simonswine)\n\n- [FEATURE] New integration:\n  [postgres_exporter](https://github.com/wrouesnel/postgres_exporter) (@rfratto)\n\n- [FEATURE] New integration:\n  [statsd_exporter](https://github.com/prometheus/statsd_exporter) (@rfratto)\n\n- [FEATURE] New integration:\n  [consul_exporter](https://github.com/prometheus/consul_exporter) (@rfratto)\n\n- [FEATURE] Add optional environment variable substitution of configuration\n  file. (@dcseifert)\n\n- [ENHANCEMENT] `min_wal_time` and `max_wal_time` have been added to the\n  instance config settings, guaranteeing that data in the WAL will exist for at\n  least `min_wal_time` and will not exist for longer than `max_wal_time`. This\n  change will increase the size of the WAL slightly but will prevent certain\n  scenarios where data is deleted before it is sent. To revert back to the old\n  behavior, set `min_wal_time` to `0s`. (@rfratto)\n\n- [ENHANCEMENT] Update `redis_exporter` to v1.13.1. (@rfratto)\n\n- [ENHANCEMENT] Bump OpenTelemetry-collector dependency to v0.16.0. (@bombsimon)\n\n- [BUGFIX] Fix issue where the Tempo example manifest could not be applied\n  because the port names were too long. (@rfratto)\n\n- [BUGFIX] Fix issue where the Agent Kubernetes manifests may not load properly\n  on AKS. (#279) (@rfratto)\n\n- [CHANGE] The User-Agent header sent for logs will now be\n  `GrafanaCloudAgent/<version>` (@rfratto)\n\n# v0.8.0 (2020-11-06)\n\n- [FEATURE] New integration: [dnsamsq_exporter](https://github.com/google/dnsamsq_exporter)\n  (@rfratto).\n\n- [FEATURE] New integration: [memcached_exporter](https://github.com/prometheus/memcached_exporter)\n  (@rfratto).\n\n- [ENHANCEMENT] Add `<integration name>_build_info` metric to all integrations.\n  The build info displayed will match the build information of the Agent and\n  *not* the embedded exporter. This metric is used by community dashboards, so\n  adding it to the Agent increases compatibility with existing dashboards that\n  depend on it existing. (@rfratto)\n\n- [ENHANCEMENT] Bump OpenTelemetry-collector dependency to 0.14.0 (@joe-elliott)\n\n- [BUGFIX] Error messages when retrieving configs from the KV store will\n  now be logged, rather than just logging a generic message saying that\n  retrieving the config has failed. (@rfratto)\n\n# v0.7.2 (2020-10-29)\n\n- [ENHANCEMENT] Bump Prometheus dependency to 2.21. (@rfratto)\n\n- [ENHANCEMENT] Bump OpenTelemetry-collector dependency to 0.13.0 (@rfratto)\n\n- [ENHANCEMENT] Bump Promtail dependency to 2.0. (@rfratto)\n\n- [ENHANCEMENT] Enhance host_filtering mode to support targets from Docker Swarm\n  and Consul. Also, add a `host_filter_relabel_configs` to that will apply relabeling\n  rules for determining if a target should be dropped. Add a documentation\n  section explaining all of this in detail. (@rfratto)\n\n- [BUGFIX] Fix deb package prerm script so that it stops the agent on package removal. (@jdbaldry)\n\n- [BUGFIX] Fix issue where the `push_config` for Tempo field was expected to be\n  `remote_write`. `push_config` now works as expected. (@rfratto)\n\n# v0.7.1 (2020-10-23)\n\n- [BUGFIX] Fix issue where ARM binaries were not published with the GitHub\n  release.\n\n# v0.7.0 (2020-10-23)\n\n- [FEATURE] Added Tracing Support. (@joe-elliott)\n\n- [FEATURE] Add RPM and deb packaging. (@jdbaldry) (@simon6372)\n\n- [FEATURE] arm64 and arm/v7 Docker containers and release builds are now\n  available for `agent` and `agentctl`. (@rfratto)\n\n- [FEATURE] Add `wal-stats` and `target-stats` tooling to `agentctl` to discover\n  WAL and cardinality issues. (@rfratto)\n\n- [FEATURE] [mysqld_exporter](https://github.com/prometheus/mysqld_exporter) is\n  now embedded and available as an integration. (@rfratto)\n\n- [FEATURE] [redis_exporter](https://github.com/oliver006/redis_exporter) is\n  now embedded and available as an integration. (@dafydd-t)\n\n- [ENHANCEMENT] Resharding the cluster when using the scraping service mode now\n  supports timeouts through `reshard_timeout`. The default value is `30s.` This\n  timeout applies to cluster-wide reshards (performed when joining and leaving\n  the cluster) and local reshards (done on the `reshard_interval`). (@rfratto)\n\n- [BUGFIX] Fix issue where integrations crashed with instance_mode was set to\n  `distinct` (@rfratto)\n\n- [BUGFIX] Fix issue where the `agent` integration did not work on Windows\n  (@rfratto).\n\n- [BUGFIX] Support URL-encoded paths in the scraping service API. (@rfratto)\n\n- [BUGFIX] The instance label written from replace_instance_label can now be\n  overwritten with relabel_configs. This bugfix slightly modifies the behavior\n  of what data is stored. The final instance label will now be stored in the WAL\n  rather than computed by remote_write. This change should not negatively effect\n  existing users. (@rfratto)\n\n# v0.6.1 (2020-04-11)\n\n- [BUGFIX] Fix issue where build information was empty when running the Agent\n  with --version. (@rfratto)\n\n- [BUGFIX] Fix issue where updating a config in the scraping service may fail to\n  pick up new targets. (@rfratto)\n\n- [BUGFIX] Fix deadlock that slowly prevents the Agent from scraping targets at\n  a high scrape volume. (@rfratto)\n\n# v0.6.0 (2020-09-04)\n\n- [FEATURE] The Grafana Agent can now collect logs and send to Loki. This\n  is done by embedding Promtail, the official Loki log collection client.\n  (@rfratto)\n\n- [FEATURE] Integrations can now be enabled without scraping. Set\n  scrape_integrations to `false` at the `integrations` key or within the\n  specific integration you don't want to scrape. This is useful when another\n  Agent or Prometheus server will scrape the integration. (@rfratto)\n\n- [FEATURE] [process-exporter](https://github.com/ncabatoff/process-exporter) is\n  now embedded as `process_exporter`. The hypen has been changed to an\n  underscore in the config file to retain consistency with `node_exporter`.\n  (@rfratto)\n\n- [ENHANCEMENT] A new config option, `replace_instance_label`, is now available\n  for use with integrations. When this is true, the instance label for all\n  metrics coming from an integration will be replaced with the machine's\n  hostname rather than 127.0.0.1. (@rfratto)\n\n- [EHANCEMENT] The embedded Prometheus version has been updated to 2.20.1.\n  (@rfratto, @gotjosh)\n\n- [ENHANCEMENT] The User-Agent header written by the Agent when remote_writing\n  will now be `GrafanaCloudAgent/<Version>` instead of `Prometheus/<Prometheus Version>`.\n  (@rfratto)\n\n- [ENHANCEMENT] The subsystems of the Agent (`prometheus`, `loki`) are now made\n  optional. Enabling integrations also implicitly enables the associated\n  subsystem. For example, enabling the `agent` or `node_exporter` integration will\n  force the `prometheus` subsystem to be enabled.  (@rfratto)\n\n- [BUGFIX] The documentation for Tanka configs is now correct. (@amckinley)\n\n- [BUGFIX] Minor corrections and spelling issues have been fixed in the Overview\n  documentation. (@amckinley)\n\n- [BUGFIX] The new default of `shared` instances mode broke the metric value for\n  `agent_prometheus_active_configs`, which was tracking the number of combined\n  configs (i.e., number of launched instances). This metric has been fixed and\n  a new metric, `agent_prometheus_active_instances`, has been added to track\n  the numbger of launched instances. If instance sharing is not enabled, both\n  metrics will share the same value. (@rfratto)\n\n- [BUGFIX] The Configs API will now disallow two instance configs having\n  multiple `scrape_configs` with the same `job_name`. THIS IS A BREAKING CHANGE.\n  This was needed for the instance sharing mode, where combined instances may\n  have duplicate `job_names` across their `scrape_configs`. This brings the\n  scraping service more in line with Prometheus, where `job_names` must globally\n  be unique. This change also disallows concurrent requests to the put/apply\n  config API endpoint to prevent a race condition of two conflicting configs\n  being applied at the same time. (@rfratto)\n\n- [BUGFIX] `remote_write` names in a group will no longer be copied from the\n  remote_write names of the first instance in the group. Rather, all\n  remote_write names will be generated based on the first 6 characters of the\n  group hash and the first six characters of the remote_write hash. (@rfratto)\n\n- [BUGFIX] Fix a panic that may occur during shutdown if the WAL is closed in\n  the middle of the WAL being truncated. (@rfratto)\n\n- [DEPRECATION] `use_hostname_label` is now supplanted by\n  `replace_instance_label`. `use_hostname_label` will be removed in a future\n  version. (@rfratto)\n\n# v0.5.0 (2020-08-12)\n\n- [FEATURE] A [scrape targets API](https://github.com/grafana/agent/blob/main/docs/api.md#list-current-scrape-targets)\n  has been added to show every target the Agent is currently scraping, when it\n  was last scraped, how long it took to scrape, and errors from the last scrape,\n  if any. (@rfratto)\n\n- [FEATURE]  \"Shared Instance Mode\" is the new default mode for spawning\n  Prometheus instances, and will improve CPU and memory usage for users of\n  integrations and the scraping service. (@rfratto)\n\n- [ENHANCEMENT] Memory stability and utilization of the WAL has been improved,\n  and the reported number of active series in the WAL will stop double-counting\n  recently churned series. (@rfratto)\n\n- [ENHANCEMENT] Changing scrape_configs and remote_write configs for an instance\n  will now be dynamically applied without restarting the instance. This will\n  result in less missing metrics for users of the scraping service that change a\n  config. (@rfratto)\n\n- [ENHANCEMENT] The Tanka configuration now uses k8s-alpha. (@duologic)\n\n- [BUGFIX] The Tanka configuration will now also deploy a single-replica\n  deployment specifically for scraping the Kubernetes API. This deployment acts\n  together with the Daemonset to scrape the full cluster and the control plane.\n  (@gotjosh)\n\n- [BUGFIX] The node_exporter filesystem collector will now work on Linux systems\n  without needing to manually set the blocklist and allowlist of filesystems.\n  (@rfratto)\n\n# v0.4.0 (2020-06-18)\n\n- [FEATURE] Support for integrations has been added. Integrations can be any\n  embedded tool, but are currently used for embedding exporters and generating\n  scrape configs. (@rfratto)\n\n- [FEATURE] node_exporter has been added as an integration. This is the full\n  version of node_exporter with the same configuration options. (@rfratto)\n\n- [FEATURE] An Agent integration that makes the Agent automatically scrape\n  itself has been added. (@rfratto)\n\n- [ENHANCEMENT] The WAL can now be truncated if running the Agent without any\n  remote_write endpoints. (@rfratto)\n\n- [ENHANCEMENT] Clarify server_config description in documentation. (@rfratto)\n\n- [ENHANCEMENT] Clarify wal_truncate_frequency and remote_flush_deadline in\n  documentation. (@rfratto)\n\n- [ENHANCEMENT] Document /agent/api/v1/instances endpoint (@rfratto)\n\n- [ENHANCEMENT] Be explicit about envsubst requirement for Kubernetes install\n  script. (@robx)\n\n- [BUGFIX] Prevent the Agent from crashing when a global Prometheus config\n  stanza is not provided. (@robx)\n\n- [BUGFIX] Enable agent host_filter in the Tanka configs, which was disabled by\n  default by mistake. (@rfratto)\n\n# v0.3.2 (2020-05-29)\n\n- [FEATURE] Tanka configs that deploy the scraping service mode are now\n  available (@rfratto)\n\n- [FEATURE] A k3d example has been added as a counterpart to the docker-compose\n  example. (@rfratto)\n\n- [ENHANCEMENT] Labels provided by the default deployment of the Agent\n  (Kubernetes and Tanka) have been changed to align with the latest changes to\n  grafana/jsonnet-libs. The old `instance` label is now called `pod`, and the\n  new `instance` label is unique. A `container` label has also been added. The\n  Agent mixin has been subsequently updated to also incorporate these label\n  changes. (@rfratto)\n\n- [ENHANCEMENT] The `remote_write` and `scrape_config` sections now share the\n  same validations as Prometheus (@rfratto)\n\n- [ENHANCEMENT] Setting `wal_truncation_frequency` to less than the scrape\n  interval is now disallowed (@rfratto)\n\n- [BUGFIX] A deadlock in scraping service mode when updating a config that\n  shards to the same node has been fixed (@rfratto)\n\n- [BUGFIX] `remote_write` config stanzas will no longer ignore `password_file`\n  (@rfratto)\n\n- [BUGFIX] `scrape_config` client secrets (e.g., basic auth, bearer token,\n  `password_file`) will now be properly retained in scraping service mode\n  (@rfratto)\n\n- [BUGFIX] Labels for CPU, RX, and TX graphs in the Agent Operational dashboard\n  now correctly show the pod name of the Agent instead of the exporter name.\n  (@rfratto)\n\n# v0.3.1 (2020-05-20)\n\n- [BUGFIX] A typo in the Tanka configs and Kubernetes manifests that prevents\n  the Agent launching with v0.3.0 has been fixed (@captncraig)\n\n- [BUGFIX] Fixed a bug where Tanka mixins could not be used due to an issue with\n  the folder placement enhancement (@rfratto)\n\n- [ENHANCEMENT] `agentctl` and the config API will now validate that the YAML\n  they receive are valid instance configs. (@rfratto)\n\n- [FEATURE] The Agent has upgraded its vendored Prometheus to v2.18.1\n  (@gotjosh, @rfratto)\n\n# v0.3.0 (2020-05-13)\n\n- [FEATURE] A third operational mode called \"scraping service mode\" has been\n  added. A KV store is used to store instance configs which are distributed\n  amongst a clustered set of Agent processes, dividing the total scrape load\n  across each agent. An API is exposed on the Agents to list, create, update,\n  and delete instance configurations from the KV store. (@rfratto)\n\n- [FEATURE] An \"agentctl\" binary has been released to interact with the new\n  instance config management API created by the \"scraping service mode.\"\n  (@rfratto, @hoenn)\n\n- [FEATURE] The Agent now includes readiness and healthiness endpoints.\n  (@rfratto)\n\n- [ENHANCEMENT] The YAML files are now parsed strictly and an invalid YAML will\n  generate an error at runtime. (@hoenn)\n\n- [ENHANCEMENT] The default build mode for the Docker containers is now release,\n  not debug. (@rfratto)\n\n- [ENHANCEMENT] The Grafana Agent Tanka Mixins now are placed in an \"Agent\"\n  folder within Grafana. (@cyriltovena)\n\n# v0.2.0 (2020-04-09)\n\n- [FEATURE] The Prometheus remote write protocol will now send scraped metadata (metric name, help, type and unit). This results in almost negligent bytes sent increase as metadata is only sent every minute. It is on by default. (@gotjosh)\n\n  These metrics are available to monitor metadata being sent:\n    - `prometheus_remote_storage_succeeded_metadata_total`\n    - `prometheus_remote_storage_failed_metadata_total`\n    - `prometheus_remote_storage_retried_metadata_total`\n    - `prometheus_remote_storage_sent_batch_duration_seconds` and\n      `prometheus_remote_storage_sent_bytes_total` have a new label \u201ctype\u201d with\n      the values of `metadata` or `samples`.\n\n- [FEATURE] The Agent has upgraded its vendored Prometheus to v2.17.1 (@rfratto)\n\n- [BUGFIX] Invalid configs passed to the agent will now stop the process after they are logged as invalid; previously the Agent process would continue. (@rfratto)\n\n- [BUGFIX] Enabling host_filter will now allow metrics from node role Kubernetes service discovery to be scraped properly (e.g., cAdvisor, Kubelet). (@rfratto)\n\n# v0.1.1 (2020-03-16)\n\n- Nits in documentation (@sh0rez)\n- Fix various dashboard mixin problems from v0.1.0 (@rfratto)\n- Pass through release tag to `docker build` (@rfratto)\n\n# v0.1.0 (2020-03-16)\n\nFirst (beta) release!\n\nThis release comes with support for scraping Prometheus metrics and\nsharding the agent through the presence of a `host_filter` flag within the\nAgent configuration file.\n\nNote that enabling the `host_filter` flag currently works best when using our\npreferred Kubernetes deployment, as it deploys the agent as a DaemonSet.\n", "+++\ntitle = \"node_exporter_config\"\n+++\n\n# node_exporter_config\n\nThe `node_exporter_config` block configures the `node_exporter` integration,\nwhich is an embedded version of\n[`node_exporter`](https://github.com/prometheus/node_exporter)\nand allows for collecting metrics from the UNIX system that `node_exporter` is\nrunning on. It provides a significant amount of collectors that are responsible\nfor monitoring various aspects of the host system.\n\nNote that if running the Agent in a container, you will need to bind mount\nfolders from the host system so the integration can monitor them. You can use\nthe example below, making sure to replace `/path/to/config.yaml` with a path on\nyour host machine where an Agent configuration file is:\n\n```\ndocker run \\\n  --net=\"host\" \\\n  --pid=\"host\" \\\n  --cap-add=SYS_TIME \\\n  -v \"/:/host/root:ro,rslave\" \\\n  -v \"/sys:/host/sys:ro,rslave\" \\\n  -v \"/proc:/host/proc:ro,rslave\" \\\n  -v /tmp/agent:/etc/agent \\\n  -v /path/to/config.yaml:/etc/agent-config/agent.yaml \\\n  grafana/agent:v0.21.1 \\\n  --config.file=/etc/agent-config/agent.yaml\n```\n\nUse this configuration file for testing out `node_exporter` support, replacing\nthe `remote_write` settings with settings appropriate for you:\n\n```yaml\nserver:\n  log_level: info\n  http_listen_port: 12345\n\nmetrics:\n  wal_directory: /tmp/agent\n  global:\n    scrape_interval: 15s\n    remote_write:\n    - url: https://prometheus-us-central1.grafana.net/api/prom/push\n      basic_auth:\n        username: user-id\n        password: api-token\n\nintegrations:\n  node_exporter:\n    enabled: true\n    rootfs_path: /host/root\n    sysfs_path: /host/sys\n    procfs_path: /host/proc\n```\n\nFor running on Kubernetes, ensure to set the equivalent mounts and capabilities\nthere as well:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: agent\nspec:\n  containers:\n  - image: grafana/agent:v0.21.1\n    name: agent\n    args:\n    - --config.file=/etc/agent-config/agent.yaml\n    securityContext:\n      capabilities:\n        add: [\"SYS_TIME\"]\n      priviliged: true\n      runAsUser: 0\n    volumeMounts:\n    - name: rootfs\n      mountPath: /host/root\n      readOnly: true\n    - name: sysfs\n      mountPath: /host/sys\n      readOnly: true\n    - name: procfs\n      mountPath: /host/proc\n      readOnly: true\n  hostPID: true\n  hostNetwork: true\n  dnsPolicy: ClusterFirstWithHostNet\n  volumes:\n  - name: rootfs\n    hostPath:\n      path: /\n  - name: sysfs\n    hostPath:\n      path: /sys\n  - name: procfs\n    hostPath:\n      path: /proc\n```\n\nThe manifest and Tanka configs provided by this repository do not have the\nmounts or capabilities required for running this integration.\n\nSome collectors only work on specific operating systems, documented in the\ntable below. Enabling a collector that is not supported by the operating system\nthe Agent is running on is a no-op.\n\n| Name             | Description | OS | Enabled by default |\n| ---------------- | ----------- | -- | ------------------ |\n| arp              | Exposes ARP statistics from /proc/net/arp. | Linux | yes |\n| bcache           | Exposes bcache statistics from /sys/fs/bcache. | Linux | yes |\n| bonding          | Exposes the number of configured and active slaves of Linux bonding interfaces. | Linux | yes |\n| boottime         | Exposes system boot time derived from the kern.boottime sysctl. | Darwin, Dragonfly, FreeBSD, NetBSD, OpenBSD, Solaris | yes |\n| btrfs            | Exposes statistics on btrfs. | Linux | yes |\n| buddyinfo        | Exposes statistics of memory fragments as reported by /proc/buddyinfo. | Linux | no |\n| conntrack        | Shows conntrack statistics (does nothing if no /proc/sys/net/netfilter/ present). | Linux | yes |\n| cpu              | Exposes CPU statistics. | Darwin, Dragonfly, FreeBSD, Linux, Solaris | yes |\n| cpufreq          | Exposes CPU frequency statistics. | Linux, Solaris | yes |\n| devstat          | Exposes device statistics. | Dragonfly, FreeBSD | no |\n| diskstats        | Exposes disk I/O statistics. | Darwin, Linux, OpenBSD | yes |\n| drbd             | Exposes Distributed Replicated Block Device statistics (to version 8.4). | Linux | no |\n| edac             | Exposes error detection and correction statistics. | Linux | yes |\n| entropy          | Exposes available entropy. | Linux | yes |\n| exec             | Exposes execution statistics. | Dragonfly, FreeBSD | yes |\n| filefd           | Exposes file descriptor statistics from /proc/sys/fs/file-nr. | Linux | yes |\n| filesystem       | Exposes filesystem statistics, such as disk space used. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD | yes |\n| hwmon            | Exposes hardware monitoring and sensor data from /sys/class/hwmon. | Linux | yes |\n| infiniband       | Exposes network statistics specific to InfiniBand and Intel OmniPath configurations. | Linux | yes |\n| interrupts       | Exposes detailed interrupts statistics. | Linux, OpenBSD | no |\n| ipvs             | Exposes IPVS status from /proc/net/ip_vs and stats from /proc/net/ip_vs_stats. | Linux | yes |\n| ksmd             | Exposes kernel and system statistics from /sys/kernel/mm/ksm. | Linux | no |\n| loadavg          | Exposes load average. | Darwin, Dragonfly, FreeBSD, Linux, NetBSD, OpenBSD, Solaris | yes |\n| logind           | Exposes session counts from logind. | Linux | no |\n| mdadm            | Exposes statistics about devices in /proc/mdstat (does nothing if no /proc/mdstat present). | Linux | yes |\n| meminfo          | Exposes memory statistics. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD | yes |\n| meminfo_numa     | Exposes memory statistics from /proc/meminfo_numa. | Linux | no |\n| mountstats       | Exposes filesystem statistics from /proc/self/mountstats. Exposes detailed NFS client statistics. | Linux | no |\n| netclass         | Exposes network interface info from /sys/class/net. | Linux | yes |\n| netdev           | Exposes network interface statistics such as bytes transferred. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD | yes |\n| netstat          | Exposes network statistics from /proc/net/netstat. This is the same information as netstat -s. | Linux | yes |\n| nfs              | Exposes NFS client statistics from /proc/net/rpc/nfs. This is the same information as nfsstat -c. | Linux | yes |\n| nfsd             | Exposes NFS kernel server statistics from /proc/net/rpc/nfsd. This is the same information as nfsstat -s. | Linux | yes |\n| ntp              | Exposes local NTP daemon helath to check time. | any | no |\n| perf             | Exposes perf based metrics (Warning: Metrics are dependent on kernel configuration and settings). | Linux | no |\n| powersupplyclass | Collects information on power supplies. | any | yes |\n| pressure         | Exposes pressure stall statistics from /proc/pressure/. | Linux (kernel 4.20+ and/or CONFIG_PSI) | yes |\n| processes        | Exposes aggregate process statistics from /proc. | Linux | no |\n| qdisc            | Exposes queuing discipline statistics. | Linux | no |\n| rapl             | Exposes various statistics from /sys/class/powercap. | Linux | yes |\n| runit            | Exposes service status from runit. | any | no |\n| schedstat        | Exposes task scheduler statistics from /proc/schedstat. | Linux | yes |\n| sockstat         | Exposes various statistics from /proc/net/sockstat. | Linux | yes |\n| softnet          | Exposes statistics from /proc/net/softnet_stat. | Linux | yes |\n| stat             | Exposes various statistics from /proc/stat. This includes boot time, forks and interrupts. | Linux | yes |\n| supervisord      | Exposes service status from supervisord. | any | no |\n| systemd          | Exposes service and system status from systemd. | Linux | no |\n| tcpstat          | Exposes TCP connection status information from /proc/net/tcp and /proc/net/tcp6. (Warning: the current version has potential performance issues in high load situations). | Linux | no |\n| textfile         | Collects metrics from files in a directory matching the filename pattern *.prom. The files must be using the text format defined here: https://prometheus.io/docs/instrumenting/exposition_formats/ | any | yes |\n| thermal_zone     | Exposes thermal zone & cooling device statistics from /sys/class/thermal. | Linux | yes |\n| time             | Exposes the current system time. | any | yes |\n| timex            | Exposes selected adjtimex(2) system call stats. | Linux | yes |\n| udp_queues       | Exposes UDP total lengths of the rx_queue and tx_queue from /proc/net/udp and /proc/net/udp6. | Linux | yes |\n| uname            | Exposes system information as provided by the uname system call. | Darwin, FreeBSD, Linux, OpenBSD | yes |\n| vmstat           | Exposes statistics from /proc/vmstat. | Linux | yes |\n| wifi             | Exposes WiFi device and station statistics. | Linux | no |\n| xfs              | Exposes XFS runtime statistics. | Linux (kernel 4.4+) | yes |\n| zfs              | Exposes ZFS performance statistics. | Linux, Solaris | yes |\n\n\n```yaml\n  # Enables the node_exporter integration, allowing the Agent to automatically\n  # collect system metrics from the host UNIX system.\n  [enabled: <boolean> | default = false]\n\n  # Sets an explicit value for the instance label when the integration is\n  # self-scraped. Overrides inferred values.\n  #\n  # The default value for this integration is inferred from the agent hostname\n  # and HTTP listen port, delimited by a colon.\n  [instance: <string>]\n\n  # Automatically collect metrics from this integration. If disabled,\n  # the node_exporter integration will be run but not scraped and thus not remote-written. Metrics for the\n  # integration will be exposed at /integrations/node_exporter/metrics and can\n  # be scraped by an external process.\n  [scrape_integration: <boolean> | default = <integrations_config.scrape_integrations>]\n\n  # How often should the metrics be collected? Defaults to\n  # prometheus.global.scrape_interval.\n  [scrape_interval: <duration> | default = <global_config.scrape_interval>]\n\n  # The timtout before considering the scrape a failure. Defaults to\n  # prometheus.global.scrape_timeout.\n  [scrape_timeout: <duration> | default = <global_config.scrape_timeout>]\n\n  # Allows for relabeling labels on the target.\n  relabel_configs:\n    [- <relabel_config> ... ]\n\n  # Relabel metrics coming from the integration, allowing to drop series\n  # from the integration that you don't care about.\n  metric_relabel_configs:\n    [ - <relabel_config> ... ]\n\n  # How frequent to truncate the WAL for this integration.\n  [wal_truncate_frequency: <duration> | default = \"60m\"]\n\n  # Monitor the exporter itself and include those metrics in the results.\n  [include_exporter_metrics: <boolean> | default = false]\n\n  # Optionally defines the the list of enabled-by-default collectors.\n  # Anything not provided in the list below will be disabled by default,\n  # but requires at least one element to be treated as defined.\n  #\n  # This is useful if you have a very explicit set of collectors you wish\n  # to run.\n  set_collectors:\n    - [<string>]\n\n  # Additional collectors to enable on top of the default set of enabled\n  # collectors or on top of the list provided by set_collectors.\n  #\n  # This is useful if you have a few collectors you wish to run that are\n  # not enabled by default, but do not want to explicitly provide an entire\n  # list through set_collectors.\n  enable_collectors:\n    - [<string>]\n\n  # Additional collectors to disable on top of the default set of disabled\n  # collectors. Takes precedence over enable_collectors.\n\n  # Additional collectors to disable from the set of enabled collectors.\n  # Takes precedence over enabled_collectors.\n  #\n  # This is useful if you have a few collectors you do not want to run that\n  # are enabled by default, but do not want to explicitly provide an entire\n  # list through set_collectors.\n  disable_collectors:\n    - [<string>]\n\n  # procfs mountpoint.\n  [procfs_path: <string> | default = \"/proc\"]\n\n  # sysfs mountpoint.\n  [sysfs_path: <string> | default = \"/sys\"]\n\n  # rootfs mountpoint. If running in docker, the root filesystem of the host\n  # machine should be mounted and this value should be changed to the mount\n  # directory.\n  [rootfs_path: <string> | default = \"/\"]\n\n  # Enable the cpu_info metric for the cpu collector.\n  [enable_cpu_info_metric: <boolean> | default = true]\n\n  # Regexmp of devices to ignore for diskstats.\n  [diskstats_ignored_devices: <string> | default = \"^(ram|loop|fd|(h|s|v|xv)d[a-z]|nvme\\\\d+n\\\\d+p)\\\\d+$\"]\n\n  # Regexp of mount points to ignore for filesystem collector.\n  [filesystem_ignored_mount_points: <string> | default = \"^/(dev|proc|sys|var/lib/docker/.+)($|/)\"]\n\n  # Regexp of filesystem types to ignore for filesystem collector.\n  [filesystem_ignored_fs_types: <string> | default = \"^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"]\n\n  # NTP server to use for ntp collector\n  [ntp_server: <string> | default = \"127.0.0.1\"]\n\n  # NTP protocol version\n  [ntp_protocol_version: <int> | default = 4]\n\n  # Certify that the server address is not a public ntp server.\n  [ntp_server_is_local: <boolean> | default = false]\n\n  # IP TTL to use wile sending NTP query.\n  [ntp_ip_ttl: <int> | default = 1]\n\n  # Max accumulated distance to the root.\n  [ntp_max_distance: <duration> | default = \"3466080us\"]\n\n  # Offset between local clock and local ntpd time to tolerate.\n  [ntp_local_offset_tolerance: <duration> | default = \"1ms\"]\n\n  # Regexp of net devices to ignore for netclass collector.\n  [netclass_ignored_devices: <string> | default = \"^$\"]\n\n  # Regexp of net devices to blacklist (mutually exclusive with whitelist)\n  [netdev_device_blacklist: <string> | default = \"\"]\n\n  # Regexp of net devices to whitelist (mutually exclusive with blacklist)\n  [netdev_device_whitelist: <string> | default = \"\"]\n\n  # Regexp of fields to return for netstat collector.\n  [netstat_fields: <string> | default = \"^(.*_(InErrors|InErrs)|Ip_Forwarding|Ip(6|Ext)_(InOctets|OutOctets)|Icmp6?_(InMsgs|OutMsgs)|TcpExt_(Listen.*|Syncookies.*|TCPSynRetrans)|Tcp_(ActiveOpens|InSegs|OutSegs|PassiveOpens|RetransSegs|CurrEstab)|Udp6?_(InDatagrams|OutDatagrams|NoPorts|RcvbufErrors|SndbufErrors))$\"]\n\n  # List of CPUs from which perf metrics should be collected.\n  [perf_cpus: <string> | default = \"\"]\n\n  # Regexp of power supplies to ignore for the powersupplyclass collector.\n  [powersupply_ignored_supplies: <string> | default = \"^$\"]\n\n  # Path to runit service directory.\n  [runit_service_dir: <string> | default = \"/etc/service\"]\n\n  # XML RPC endpoint for the supervisord collector.\n  [supervisord_url: <string> | default = \"http://localhost:9001/RPC2\"]\n\n  # Regexp of systemd units to whitelist. Units must both match whitelist\n  # and not match blacklist to be included.\n  [systemd_unit_whitelist: <string> | default = \".+\"]\n\n  # Regexp of systemd units to blacklist. Units must both match whitelist\n  # and not match blacklist to be included.\n  [systemd_unit_blacklist: <string> | default = \".+\\\\.(automount|device|mount|scope|slice)\"]\n\n  # Enables service unit tasks metrics unit_tasks_current and unit_tasks_max\n  [systemd_enable_task_metrics: <boolean> | default = false]\n\n  # Enables service unit metric service_restart_total\n  [systemd_enable_restarts_metrics: <boolean> | default = false]\n\n  # Enables service unit metric unit_start_time_seconds\n  [systemd_enable_start_time_metrics: <boolean> | default = false]\n\n  # Directory to read *.prom files from for the textfile collector.\n  [textfile_directory: <string> | default = \"\"]\n\n  # Regexp of fields to return for the vmstat collector.\n  [vmstat_fields: <string> | default = \"^(oom_kill|pgpg|pswp|pg.*fault).*\"]\n```\n", "+++\ntitle = \"process_exporter_config\"\n+++\n\n# process_exporter_config\n\nThe `process_exporter_config` block configures the `process_exporter` integration,\nwhich is an embedded version of\n[`process-exporter`](https://github.com/ncabatoff/process-exporter)\nand allows for collection metrics based on the /proc filesystem on Linux\nsystems. Note that on non-Linux systems, enabling this exporter is a no-op.\n\nNote that if running the Agent in a container, you will need to bind mount\nfolders from the host system so the integration can monitor them:\n\n```\ndocker run \\\n  -v \"/proc:/proc:ro\" \\\n  -v /tmp/agent:/etc/agent \\\n  -v /path/to/config.yaml:/etc/agent-config/agent.yaml \\\n  grafana/agent:v0.21.1 \\\n  --config.file=/etc/agent-config/agent.yaml\n```\n\nReplace `/path/to/config.yaml` with the appropriate path on your host system\nwhere an Agent config file can be found.\n\nFor running on Kubernetes, ensure to set the equivalent mounts and capabilities\nthere as well:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: agent\nspec:\n  containers:\n  - image: grafana/agent:v0.21.1\n    name: agent\n    args:\n    - --config.file=/etc/agent-config/agent.yaml\n    volumeMounts:\n    - name: procfs\n      mountPath: /proc\n      readOnly: true\n  volumes:\n  - name: procfs\n    hostPath:\n      path: /proc\n```\n\nThe manifest and Tanka configs provided by this repository do not have the\nmounts or capabilities required for running this integration.\n\nAn example config for `process_exporter_config` that tracks all processes is the\nfollowing:\n\n```\nenabled: true\nprocess_names:\n- name: \"{{.Comm}}\"\n  cmdline:\n  - '.+'\n```\n\nFull reference of options:\n\n```yaml\n  # Enables the process_exporter integration, allowing the Agent to automatically\n  # collect system metrics from the host UNIX system.\n  [enabled: <boolean> | default = false]\n\n  # Sets an explicit value for the instance label when the integration is\n  # self-scraped. Overrides inferred values.\n  #\n  # The default value for this integration is inferred from the agent hostname\n  # and HTTP listen port, delimited by a colon.\n  [instance: <string>]\n\n  # Automatically collect metrics from this integration. If disabled,\n  # the process_exporter integration will be run but not scraped and thus not\n  # remote-written. Metrics for the integration will be exposed at\n  # /integrations/process_exporter/metrics and can be scraped by an external\n  # process.\n  [scrape_integration: <boolean> | default = <integrations_config.scrape_integrations>]\n\n  # How often should the metrics be collected? Defaults to\n  # prometheus.global.scrape_interval.\n  [scrape_interval: <duration> | default = <global_config.scrape_interval>]\n\n  # The timeout before considering the scrape a failure. Defaults to\n  # prometheus.global.scrape_timeout.\n  [scrape_timeout: <duration> | default = <global_config.scrape_timeout>]\n\n  # Allows for relabeling labels on the target.\n  relabel_configs:\n    [- <relabel_config> ... ]\n\n  # Relabel metrics coming from the integration, allowing to drop series\n  # from the integration that you don't care about.\n  metric_relabel_configs:\n    [ - <relabel_config> ... ]\n\n  # How frequent to truncate the WAL for this integration.\n  [wal_truncate_frequency: <duration> | default = \"60m\"]\n\n  # procfs mountpoint.\n  [procfs_path: <string> | default = \"/proc\"]\n\n  # If a proc is tracked, track with it any children that aren't a part of their\n  # own group.\n  [track_children: <boolean> | default = true]\n\n  # Report on per-threadname metrics as well.\n  [track_threads: <boolean> | default = true]\n\n  # Gather metrics from smaps file, which contains proportional resident memory\n  # size.\n  [gather_smaps: <boolean> | default = true]\n\n  # Recheck process names on each scrape.\n  [recheck_on_scrape: <boolean> | default = false]\n\n  # A collection of matching rules to use for deciding which processes to\n  # monitor. Each config can match multiple processes to be tracked as a single\n  # process \"group.\"\n  process_names:\n    [- <process_matcher_config>]\n```\n\n## process_matcher_config\n\n```yaml\n# The name to use for identifying the process group name in the metric. By\n# default, it uses the base path of the executable.\n#\n# The following template variables are available:\n#\n# - {{.Comm}}:      Basename of the original executable from /proc/<pid>/stat\n# - {{.ExeBase}}:   Basename of the executable from argv[0]\n# - {{.ExeFull}}:   Fully qualified path of the executable\n# - {{.Username}}:  Username of the effective user\n# - {{.Matches}}:   Map containing all regex capture groups resulting from\n#                   matching a process with the cmdline rule group.\n# - {{.PID}}:       PID of the process. Note that the PID is copied from the\n#                   first executable found.\n# - {{.StartTime}}: The start time of the process. This is useful when combined\n#                   with PID as PIDS get reused over time.\n[name: <string> | default = \"{{.ExeBase}}\"]\n\n# A list of strings that match the base executable name for a process, truncated\n# at 15 characters. It is derived from reading the second field of\n# /proc/<pid>/stat minus the parens.\n#\n# If any of the strings match, the process will be tracked.\ncomm:\n  [- <string>]\n\n# A list of strings that match argv[0] for a process. If there are no slashes,\n# only the basename of argv[0] needs to match. Otherwise the name must be an\n# exact match. For example, \"postgres\" may match any postgres binary but\n# \"/usr/local/bin/postgres\" can only match a postgres at that path exactly.\n#\n# If any of the strings match, the process will be tracked.\nexe:\n  [- <string>]\n\n# A list of regular expressions applied to the argv of the process. Each\n# regex here must match the corresponding argv for the process to be tracked.\n# The first element that is matched is argv[1].\n#\n# Regex Captures are added to the .Matches map for use in the name.\ncmdline:\n  [- <string>]\n```\n", "+++\ntitle = \"Getting started with Grafana Agent\"\nweight = 100\n+++\n\n# Getting started with Grafana Agent\n\nThis guide helps users get started with the Grafana Agent. For getting started\nwith the Grafana Agent Operator, please refer to the Operator-specific\n[documentation](../operator/).\n\nCurrently, there are six ways to install the agent:\n\n- Use our Docker container\n- Use the Kubernetes manifests directly\n- Use the Kubernetes manifests along with the [Grafana Cloud Kubernetes Quickstart Guides](#grafana-cloud-kubernetes-quickstart-guides)\n- Installing the static binaries locally\n- Using Grafana Labs' official Tanka configs (_recommended advanced_)\n- Using the [Windows Installer]({{< relref \"./install-agent-on-windows.md\" >}})\n\nSee the list of [Community Projects](#community-projects) for the community-driven ecosystem around the Grafana Agent.\n\n## Docker container\n\n```\ndocker run \\\n  -v /tmp/agent:/etc/agent/data \\\n  -v /path/to/config.yaml:/etc/agent/agent.yaml \\\n  grafana/agent:v0.21.1\n```\n\nReplace `/tmp/agent` with the folder you wish to store WAL data in. WAL data is\nwhere metrics are stored before they are sent to Prometheus. Old WAL data is\ncleaned up every hour, and will be used for recovering if the process happens to\ncrash.\n\nTo override the default flags passed to the container, add the following flags\nto the end of the `docker run` command:\n\n- `--config.file=path/to/agent.yaml`, replacing the argument with the full path\n  to your Agent's YAML configuration file.\n\n- `--prometheus.wal-directory=/tmp/agent/data`, replacing `/tmp/agent/data` with\n  the directory you wish to use for storing data. Note that `/tmp` may get\n  deleted by most operating systems after a reboot.\n\nNote that using paths on your host machine must be exposed to the Docker\ncontainer through a bind mount for the flags to work properly.\n\n## Kubernetes manifests\n\nIf you wish to manually modify the Kubernetes manifests before deploying them, you can do so by downloading them from the [`kubernetes` directory](../../production/kubernetes/). Note that these manifests do not include Agent configuration files. For sample configuration, please see the Grafana Cloud Kubernetes quickstarts.\n\n## Grafana Cloud kubernetes quickstart guides\n\nThese guides help you get up and running with the Agent and Grafana Cloud, and include sample ConfigMaps.\n\nYou can find them in the [Grafana Cloud documentation](https://grafana.com/docs/grafana-cloud/quickstart/agent-k8s/)\n\n## Install locally\n\nOur [Releases](https://github.com/grafana/agent/releases) page contains\ninstructions for downloading static binaries that are published with every release.\nThese releases contain the plain binary alongside system packages for Windows,\nRed Hat, and Debian.\n\n## Tanka\n\nWe provide [Tanka](https://tanka.dev) configurations in our [`production/`](https://github.com/grafana/agent/tree/main/production/tanka/grafana-agent) directory.\n\n## Community Projects\n\nBelow is a list of community lead projects for working with Grafana Agent. These projects are not maintained or supported by Grafana Labs.\n\n### Helm (Kubernetes Deployment)\n\nA publically available release of a Grafana Agent Helm chart is maintained [here](https://github.com/DandyDeveloper/charts/tree/master/charts/grafana-agent). Contributions and improvements are welcomed. Full details on rolling out and supported options can be found in the [readme](https://github.com/DandyDeveloper/charts/blob/master/charts/grafana-agent/README.md).\n\nThis *does not* require the Grafana Agent Operator to rollout / deploy.\n\n### Juju (Charmed Operator)\n\nThe [grafana-agent-k8s](https://github.com/canonical/grafana-agent-operator) charmed operator runs with [Juju](https://juju.is) the Grafana Agent on Kubernetes.\nThe Grafana Agent charmed operator is designed to work with the [Logs, Metrics and Alerts](https://juju.is/docs/lma2) observability stack.\n", "+++\ntitle = \"Custom Resource Quickstart\"\nweight = 120\n+++\n# Grafana Agent Operator Custom Resource Quickstart\n\nIn this guide you'll learn how to deploy [Agent Operator]({{< relref \"./_index.md\" >}})'s custom resources into your Kubernetes cluster.\n\nYou'll roll out the following custom resources (CRs):\n\n- A `GrafanaAgent` resource, which discovers one or more `MetricsInstance` and `LogsInstances` resources.\n- A `MetricsInstance` resource that defines where to ship collected metrics. Under the hood, this rolls out a Grafana Agent StatefulSet that will scrape and ship metrics to a `remote_write` endpoint.\n- A `ServiceMonitor` resource to collect cAdvisor and kubelet metrics. Under the hood, this configures the `MetricsInstance` / Agent StatefulSet.\n- A `LogsInstance` resource that defines where to ship collected logs. Under the hood, this rolls out a Grafana Agent DaemonSet that will tail log files on your cluster nodes.\n- A `PodLogs` resource to collect container logs from Kubernetes Pods. Under the hood, this configures the`LogsInstance` / Agent DaemonSet.\n\nTo learn more about the custom resources Operator provides and their hierarchy, please consult [Operator architecture]({{< relref \"./architecture.md\" >}}).\n\n> **Note:** Agent Operator is currently in beta and its custom resources are subject to change as the project evolves. It currently supports the metrics and logs subsystems of Grafana Agent. Integrations and traces support is coming soon.\n\nBy the end of this guide, you will be scraping and shipping cAdvisor and Kubelet metrics to a Prometheus-compatible metrics endpoint. You'll also be collecting and shipping your Pods' container logs to a Loki-compatible logs endpoint.\n\n## Prerequisites\n\nBefore you begin, make sure that you have installed Agent Operator into your cluster. You can learn how to do this in:\n- [Installing Grafana Agent Operator with Helm]({{< relref \"./helm-getting-started.md\" >}})\n- [Installing Grafana Agent Operator]({{< relref \"./getting-started.md\" >}})\n\n## Step 1: Deploy GrafanaAgent resource\n\nIn this step you'll roll out a `GrafanaAgent` resource. A `GrafanaAgent` resource discovers `MetricsInstance` and `LogsInstance` resources and defines the Grafana Agent image, Pod requests, limits, affinities, and tolerations. Pod attributes can only be defined at the GrafanaAgent level and are propagated to `MetricsInstance` and `LogsInstance` Pods. To learn more, please see the GrafanaAgent [Custom Resource Definition](https://github.com/grafana/agent/blob/main/production/operator/crds/monitoring.grafana.com_grafanaagents.yaml).\n\n> **Note:** Due to the variety of possible deployment architectures, the official Agent Operator Helm chart does not provide built-in templates for the custom resources described in this quickstart. These must be configured and deployed manually. However, you are encouraged to template and add the following manifests to your own in-house Helm charts and GitOps flows.\n\nRoll out the following manifests in your cluster:\n\n```yaml\napiVersion: monitoring.grafana.com/v1alpha1\nkind: GrafanaAgent\nmetadata:\n  name: grafana-agent\n  namespace: default\n  labels:\n    app: grafana-agent\nspec:\n  image: grafana/agent:v0.21.1\n  logLevel: info\n  serviceAccountName: grafana-agent\n  metrics:\n    instanceSelector:\n      matchLabels:\n        agent: grafana-agent-metrics\n    externalLabels:\n      cluster: cloud\n\n  logs:\n    instanceSelector:\n      matchLabels:\n        agent: grafana-agent-logs\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent\n  namespace: default\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/proxy\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  - /metrics/cadvisor\n  verbs:\n  - get\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent\n  namespace: default\n```\n\nThis creates a ServiceAccount, ClusterRole, and ClusterRoleBinding for the GrafanaAgent resource. It also creates a GrafanaAgent resource and specifies an Agent image version. Finally, the GrafanaAgent resource specifies `MetricsInstance` and `LogsInstance` selectors. These search for MetricsInstances and LogsInstances in the same namespace with labels matching `agent: grafana-agent-metrics` and `agent: grafana-agent-logs`, respectively. It also sets a `cluster: cloud` label for all metrics shipped your Prometheus-compatible endpoint. You should change this label to your desired cluster name.\n\nThe full hierarchy of custom resources is as follows:\n\n- `GrafanaAgent`\n  - `MetricsInstance`\n    - `PodMonitor`\n    - `Probe`\n    - `ServiceMonitor`\n  - `LogsInstance`\n    - `PodLogs`\n\nDeploying a GrafanaAgent resource on its own will not spin up any Agent Pods. Agent Operator will create Agent Pods once MetricsInstance and LogsIntance resources have been created. In the next step, you'll roll out a `MetricsInstance` resource to scrape cAdvisor and Kubelet metrics and ship these to your Prometheus-compatible metrics endpoint.\n\n## Step 2: Deploy a MetricsInstance resource\n\nIn this step you'll roll out a MetricsInstance resource. MetricsInstance resources define a `remote_write` sink for metrics and configure one or more selectors to watch for creation and updates to `*Monitor` objects. These objects allow you to define Agent scrape targets via K8s manifests:\n\n- [ServiceMonitors](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#servicemonitor)\n- [PodMonitors](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#podmonitor)\n- [Probes](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#probe)\n\nRoll out the following manifest into your cluster:\n\n```yaml\napiVersion: monitoring.grafana.com/v1alpha1\nkind: MetricsInstance\nmetadata:\n  name: primary\n  namespace: default\n  labels:\n    agent: grafana-agent-metrics\nspec:\n  remoteWrite:\n  - url: your_remote_write_URL\n    basicAuth:\n      username:\n        name: primary-credentials-metrics\n        key: username\n      password:\n        name: primary-credentials-metrics\n        key: password\n\n  # Supply an empty namespace selector to look in all namespaces. Remove\n  # this to only look in the same namespace as the MetricsInstance CR\n  serviceMonitorNamespaceSelector: {}\n  serviceMonitorSelector:\n    matchLabels:\n      instance: primary\n\n  # Supply an empty namespace selector to look in all namespaces. Remove\n  # this to only look in the same namespace as the MetricsInstance CR.\n  podMonitorNamespaceSelector: {}\n  podMonitorSelector:\n    matchLabels:\n      instance: primary\n\n  # Supply an empty namespace selector to look in all namespaces. Remove\n  # this to only look in the same namespace as the MetricsInstance CR.\n  probeNamespaceSelector: {}\n  probeSelector:\n    matchLabels:\n      instance: primary\n```\n\nBe sure to replace the `remote_write` URL and customize the namespace and label configuration as necessary. This will associate itself with the `agent: grafana-agent` GrafanaAgent resource deployed in the previous step, and watch for creation and updates to `*Monitors` monitors with the the `instance: primary` label.\n\nOnce you've rolled out this manifest, create the `basicAuth` credentials using a Kubernetes Secret:\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: primary-credentials-metrics\n  namespace: default\nstringData:\n  username: 'your_cloud_prometheus_username'\n  password: 'your_cloud_prometheus_API_key'\n```\n\nIf you're using Grafana Cloud, you can find your hosted Prometheus endpoint username and password in the [Grafana Cloud Portal](https://grafana.com/profile/org ). You may wish to base64-encode these values yourself. In this case, please use `data` instead of `stringData`.\n\nOnce you've rolled out the `MetricsInstance` and its Secret, you can confirm that the MetricsInstance Agent is up and running with `kubectl get pod`. Since we haven't defined any monitors yet, this Agent will not have any scrape targets defined. In the next step, we'll create scrape targets for the cAdvisor and kubelet endpoints exposed by the `kubelet` service in the cluster.\n\n## Step 3: Create ServiceMonitors for kubelet and cAdvisor endpoints\n\nIn this step, you'll create ServiceMonitors for kubelet and cAdvisor metrics exposed by the `kubelet` Service. Every node in your cluster exposes kubelet and cadvisor metrics at `/metrics` and `/metrics/cadvisor` respectively. Agent Operator creates a `kubelet` service that exposes these Node endpoints so that they can be scraped using ServiceMonitors.\n\nTo scrape these two endpoints, roll out the following two ServiceMonitors in your cluster:\n\n- Kubelet ServiceMonitor\n\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    instance: primary\n  name: kubelet-monitor\n  namespace: default\nspec:\n  endpoints:\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    honorLabels: true\n    interval: 60s\n    metricRelabelings:\n    - action: keep\n      regex: kubelet_cgroup_manager_duration_seconds_count|go_goroutines|kubelet_pod_start_duration_seconds_count|kubelet_runtime_operations_total|kubelet_pleg_relist_duration_seconds_bucket|volume_manager_total_volumes|kubelet_volume_stats_capacity_bytes|container_cpu_usage_seconds_total|container_network_transmit_bytes_total|kubelet_runtime_operations_errors_total|container_network_receive_bytes_total|container_memory_swap|container_network_receive_packets_total|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|kubelet_running_pod_count|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate|container_memory_working_set_bytes|storage_operation_errors_total|kubelet_pleg_relist_duration_seconds_count|kubelet_running_pods|rest_client_request_duration_seconds_bucket|process_resident_memory_bytes|storage_operation_duration_seconds_count|kubelet_running_containers|kubelet_runtime_operations_duration_seconds_bucket|kubelet_node_config_error|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_running_container_count|kubelet_volume_stats_available_bytes|kubelet_volume_stats_inodes|container_memory_rss|kubelet_pod_worker_duration_seconds_count|kubelet_node_name|kubelet_pleg_relist_interval_seconds_bucket|container_network_receive_packets_dropped_total|kubelet_pod_worker_duration_seconds_bucket|container_start_time_seconds|container_network_transmit_packets_dropped_total|process_cpu_seconds_total|storage_operation_duration_seconds_bucket|container_memory_cache|container_network_transmit_packets_total|kubelet_volume_stats_inodes_used|up|rest_client_requests_total\n      sourceLabels:\n      - __name__\n    - action: replace\n      targetLabel: job\n      replacement: integrations/kubernetes/kubelet\n    port: https-metrics\n    relabelings:\n    - sourceLabels:\n      - __metrics_path__\n      targetLabel: metrics_path\n    scheme: https\n    tlsConfig:\n      insecureSkipVerify: true\n  namespaceSelector:\n    matchNames:\n    - default\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kubelet\n```\n\n- cAdvsior ServiceMonitor\n\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    instance: primary\n  name: cadvisor-monitor\n  namespace: default\nspec:\n  endpoints:\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    honorLabels: true\n    honorTimestamps: false\n    interval: 60s\n    metricRelabelings:\n    - action: keep\n      regex: kubelet_cgroup_manager_duration_seconds_count|go_goroutines|kubelet_pod_start_duration_seconds_count|kubelet_runtime_operations_total|kubelet_pleg_relist_duration_seconds_bucket|volume_manager_total_volumes|kubelet_volume_stats_capacity_bytes|container_cpu_usage_seconds_total|container_network_transmit_bytes_total|kubelet_runtime_operations_errors_total|container_network_receive_bytes_total|container_memory_swap|container_network_receive_packets_total|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|kubelet_running_pod_count|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate|container_memory_working_set_bytes|storage_operation_errors_total|kubelet_pleg_relist_duration_seconds_count|kubelet_running_pods|rest_client_request_duration_seconds_bucket|process_resident_memory_bytes|storage_operation_duration_seconds_count|kubelet_running_containers|kubelet_runtime_operations_duration_seconds_bucket|kubelet_node_config_error|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_running_container_count|kubelet_volume_stats_available_bytes|kubelet_volume_stats_inodes|container_memory_rss|kubelet_pod_worker_duration_seconds_count|kubelet_node_name|kubelet_pleg_relist_interval_seconds_bucket|container_network_receive_packets_dropped_total|kubelet_pod_worker_duration_seconds_bucket|container_start_time_seconds|container_network_transmit_packets_dropped_total|process_cpu_seconds_total|storage_operation_duration_seconds_bucket|container_memory_cache|container_network_transmit_packets_total|kubelet_volume_stats_inodes_used|up|rest_client_requests_total\n      sourceLabels:\n      - __name__\n    - action: replace\n      targetLabel: job\n      replacement: integrations/kubernetes/cadvisor\n    path: /metrics/cadvisor\n    port: https-metrics\n    relabelings:\n    - sourceLabels:\n      - __metrics_path__\n      targetLabel: metrics_path\n    scheme: https\n    tlsConfig:\n      insecureSkipVerify: true\n  namespaceSelector:\n    matchNames:\n    - default\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kubelet\n```\n\nThese two ServiceMonitors configure Agent to scrape all the Kubelet and cAdvisor endpoints in your Kubernetes cluster (one of each per Node). In addition, it defines a `job` label which you may change (it is preset here for compatibility with Grafana Cloud's Kubernetes integration), and allowlists a core set of Kubernetes metrics to reduce remote metrics usage. If you don't need this allowlist, you may omit it, however note that your metrics usage will increase significantly.\n\n When you're done, Agent should now be shipping Kubelet and cAdvisor metrics to your remote Prometheus endpoint.\n\n## Step 4: Deploy LogsInstance and PodLogs resources\n\nIn this step, you'll deploy a LogsInstance resource to collect logs from your cluster nodes and ship these to your remote Loki endpoint. Under the hood, Agent Operator will deploy a DaemonSet of Agents in your cluster that will tail log files defined in PodLogs resources.\n\nDeploy the LogsInstance into your cluster:\n\n```yaml\napiVersion: monitoring.grafana.com/v1alpha1\nkind: LogsInstance\nmetadata:\n  name: primary\n  namespace: default\n  labels:\n    agent: grafana-agent-logs\nspec:\n  clients:\n  - url: your_remote_logs_URL\n    basicAuth:\n      username:\n        name: primary-credentials-logs\n        key: username\n      password:\n        name: primary-credentials-logs\n        key: password\n\n  # Supply an empty namespace selector to look in all namespaces. Remove\n  # this to only look in the same namespace as the LogsInstance CR\n  podLogsNamespaceSelector: {}\n  podLogsSelector:\n    matchLabels:\n      instance: primary\n```\n\nThis LogsInstance will pick up PodLogs resources with the `instance: primary` label. Be sure to set the Loki URL to the correct push endpoint (for Grafana Cloud, this will be something like `logs-prod-us-central1.grafana.net/loki/api/v1/push`, however you should check the Cloud Portal to confirm).\n\nAlso note that we are using the `agent: grafana-agent-logs` label here, which will associate this LogsInstance with the GrafanaAgent resource defined in Step 1. This means that it will inherit requests, limits, affinities and other properties defined in the GrafanaAgent custom resource.\n\nCreate the Secret for the LogsInstance resource:\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: primary-credentials-logs\n  namespace: default\nstringData:\n  username: 'your_username_here'\n  password: 'your_password_here'\n```\n\nIf you're using Grafana Cloud, you can find your hosted Loki endpoint username and password in the [Grafana Cloud Portal](https://grafana.com/profile/org). You may wish to base64-encode these values yourself. In this case, please use `data` instead of `stringData`.\n\nFinally, we'll roll out a PodLogs resource to define our logging targets. Under the hood, Agent Operator will turn this into Agent config for the logs subsystem, and roll it out to the DaemonSet of logging agents.\n\nThe following is a minimal working example which you should adapt to your production needs:\n\n```yaml\napiVersion: monitoring.grafana.com/v1alpha1\nkind: PodLogs\nmetadata:\n  labels:\n    instance: primary\n  name: kubernetes-pods\n  namespace: default\nspec:\n  pipelineStages:\n    - docker: {}\n  namespaceSelector:\n    matchNames:\n    - default\n  selector:\n    matchLabels: {}\n```\n\nThis tails container logs for all Pods in the `default` Namespace. You can restrict the set of Pods matched by using the `matchLabels` selector. You can also set additional `pipelineStages` and create `relabelings` to add or modify log line labels. To learn more about the PodLogs spec and available resource fields, please see the [PodLogs CRD](https://github.com/grafana/agent/blob/main/production/operator/crds/monitoring.grafana.com_podlogs.yaml).\n\nUnder the hood, the above PodLogs resource will add the following labels to log lines:\n\n- `namespace`\n- `service`\n- `pod`\n- `container`\n- `job`\n  - Set to `PodLogs_namespace/PodLogs_name`\n- `__path__` (the path to log files)\n  - Set to `/var/log/pods/*$1/*.log` where `$1` is `__meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name`\n\nTo learn more about this config format and other available labels, please see the [Promtail Scraping](https://grafana.com/docs/loki/latest/clients/promtail/scraping/#promtail-scraping-service-discovery) reference documentation. Agent Operator will load this config into the LogsInstance agents automatically.\n\nAt this point the DaemonSet of logging agents should be tailing your container logs, applying some default labels to the log lines, and shipping them to your remote Loki endpoint.\n\n## Conclusion\n\nAt this point you've rolled out the following into your cluster:\n\n- A `GrafanaAgent` resource, which discovers one or more `MetricsInstance` and `LogsInstances` resources.\n- A `MetricsInstance`  resource that defines where to ship collected metrics.\n- A `ServiceMonitor` resource to collect cAdvisor and kubelet metrics.\n- A `LogsInstance` resource that defines where to ship collected logs.\n- A `PodLogs` resource to collect container logs from Kubernetes Pods.\n\nYou can verify that everything is working correctly by navigating to your Grafana instance and querying your Loki and Prometheus datasources. Operator support for Tempo and traces is coming soon.\n", "+++\ntitle = \"Installing Grafana Agent Operator\"\nweight = 100\n+++\n\n# Installing Grafana Agent Operator\n\nIn this guide you'll learn how to deploy the [Grafana Agent Operator]({{< relref \"./_index.md\" >}}) into your Kubernetes cluster. This guide does *not* use Helm. To learn how to deploy Agent Operator using the [grafana-agent-operator Helm chart](https://github.com/grafana/helm-charts/tree/main/charts/agent-operator), please see [Installing Grafana Agent Operator with Helm]({{< relref \"./helm-getting-started.md\" >}}).\n\n> **Note:** Agent Operator is currently in beta and its custom resources are subject to change as the project evolves. It currently supports the metrics and logs subsystems of Grafana Agent. Integrations and traces support is coming soon.\n\nBy the end of this guide, you'll have deloyed Agent Operator into your cluster.\n\n## Prerequisites\n\nBefore you begin, make sure that you have the following available to you:\n\n- A Kubernetes cluster\n- The `kubectl` command-line client installed and configured on your machine\n\n## Step 1: Deploy CustomResourceDefinitions\n\nBefore you can write custom resources to describe a Grafana Agent deployment,\nyou _must_ deploy the\n[CustomResourceDefinitions](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)\nto the cluster first. These definitions describe the schema that the custom\nresources will conform to. This is also required for the operator to run; it\nwill fail if it can't find the custom resource definitions of objects it is\nlooking to use.\n\nThe current set of CustomResourceDefinitions can be found in\n[production/operator/crds](https://github.com/grafana/agent/tree/main/production/operator/crds). Apply them from the\nroot of this repository using:\n\n```\nkubectl apply -f production/operator/crds\n```\n\nThis step _must_ be done before installing the Operator, as the Operator will\nfail to start if the CRDs do not exist.\n\n### Find information on the supported values for the CustomResourceDefinitions\n\nOnce you've deployed the CustomResourceDefinitions\nto your Kubernetes cluster, use `kubectl explain <resource>` to get access to\nthe documentation for each resource. For example, `kubectl explain GrafanaAgent`\nwill describe the GrafanaAgent CRD, and `kubectl explain GrafanaAgent.spec` will\ngive you information on its spec field.\n\n## Step 2: Install Agent Operator\n\nUse the following deployment to run the Operator, changing values as desired:\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-agent-operator\n  namespace: default\n  labels:\n    app: grafana-agent-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana-agent-operator\n  template:\n    metadata:\n      labels:\n        app: grafana-agent-operator\n    spec:\n      serviceAccountName: grafana-agent-operator\n      containers:\n      - name: operator\n        image: grafana/agent-operator:v0.21.1\n        args:\n        - --kubelet-service=default/kubelet\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent-operator\n  namespace: default\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-operator\nrules:\n- apiGroups: [monitoring.grafana.com]\n  resources:\n  - grafanaagents\n  - metricsinstances\n  - logsinstances\n  - podlogs\n  verbs: [get, list, watch]\n- apiGroups: [monitoring.coreos.com]\n  resources:\n  - podmonitors\n  - probes\n  - servicemonitors\n  verbs: [get, list, watch]\n- apiGroups: [\"\"]\n  resources:\n  - namespaces\n  - nodes\n  verbs: [get, list, watch]\n- apiGroups: [\"\"]\n  resources:\n  - secrets\n  - services\n  - configmaps\n  - endpoints\n  verbs: [get, list, watch, create, update, patch, delete]\n- apiGroups: [\"apps\"]\n  resources:\n  - statefulsets\n  - daemonsets\n  verbs: [get, list, watch, create, update, patch, delete]\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent-operator\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent-operator\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent-operator\n  namespace: default\n```\n\n### Run Operator locally\n\nBefore running locally, _make sure your kubectl context is correct!_\nRunning locally uses your current kubectl context, and you probably don't want\nto accidentally deploy a new Grafana Agent to prod.\n\nCRDs should be installed on the cluster prior to running locally. If you haven't\ndone this yet, follow [deploying CustomResourceDefinitions](#deploying-customresourcedefinitions)\nfirst.\n\nAfterwards, you can run the operator using `go run`:\n\n```\ngo run ./cmd/agent-operator\n```\n\n## Conclusion\n\nWith Agent Operator up and running, you can move on to setting up a `GrafanaAgent` custom resource. This will discover `MetricsInstance` and `LogsInstance` custom resources and endow them with Pod attributes (like requests and limits) defined in the `GrafanaAgent` spec. To learn how to do this, please see [Custom Resource Quickstart]({{< relref \"./custom-resource-quickstart.md\" >}}).\n", "+++\ntitle = \"Upgrade guide\"\nweight = 200\n+++\n\n# Upgrade guide\n\nThis guide describes all breaking changes that have happened in prior\nreleases and how to migrate to newer versions.\n\n## Unreleased Changes\n\nThese changes will come in a future version.\n\n## v0.21.0\n\n### Integrations: Change in how instance labels are handled (Breaking change)\n\nIntegrations will now use a SUO-specific `instance` label value. Integrations\nthat apply to a whole machine or agent will continue to use `<agent machine\nhostname>:<agent listen port>`, but integrations that connect to an external\nsystem will now infer an appropriate value based on the config for that specific\nintegration. Please refer to the documentation for each integration for which\ndefaults are used.\n\n*Note:* In some cases, a default value for `instance` cannot be inferred. This\nis the case for mongodb_exporter and postgres_exporter if more than one SUO is\nbeing connected to. In these cases, the instance value can be manually set by\nconfiguring the `instance` field on the integration. This can also be useful if\ntwo agents infer the same value for instance for the same integration.\n\nAs part of this change, the `agent_hostname` label is permanently affixed to\nself-scraped integrations and cannot be disabled. This disambigutates multiple\nagents using the same instance label for an integration, and allows users to\nidentify which agents need to be updated with an override for `instance`.\n\nBoth `use_hostname_label` and `replace_instance_label` are now both deprecated\nand ignored from the YAML file, permanently treated as true. A future release\nwill remove these fields, causing YAML errors on load instead of being silently\nignored.\n\n## v0.20.0\n\n### Traces: Changes to receiver's TLS config (Breaking change).\n\nUpgrading to OpenTelemetry v0.36.0 contains a change in the receivers TLS config.\nTLS params have been changed from being squashed to being in its own block.\nThis affect the jaeger receiver's `remote_sampling` config.\n\nExample old config:\n\n```yaml\nreceivers:\n  jaeger:\n    protocols:\n      grpc: null,\n    remote_sampling:\n      strategy_file: <file_path>\n      insecure: true\n```\n\nExample new config:\n\n```yaml\nreceivers:\n  jaeger:\n    protocols:\n      grpc: null,\n    remote_sampling:\n      strategy_file: <file_path>\n      tls:\n        insecure: true\n```\n\n### Traces: push_config is no longer supported (Breaking change)\n\n`push_config` was deprecated in favor of `remote_write` in v0.14.0, while\nmaintaining backwards compatibility.\nRefer to the [deprecation announcement](#tempo-push_config-deprecation) for how to upgrade.\n\n## v0.19.0\n\n### Traces: Deprecation of \"tempo\" in config and metrics. (Deprecation)\n\nThe term `tempo` in the config has been deprecated of favor of `traces`. This\nchange is to make intent clearer.\n\nExample old config:\n\n```yaml\ntempo:\n  configs:\n    - name: default\n      receivers:\n        jaeger:\n          protocols:\n            thrift_http:\n```\n\nExample of new config:\n```yaml\ntraces:\n  configs:\n    - name: default\n      receivers:\n        jaeger:\n          protocols:\n            thrift_http:\n```\n\nAny tempo metrics have been renamed from `tempo_*` to `traces_*`.\n\n\n### Tempo: split grouping by trace from tail sampling config (Breaking change)\n\nLoad balancing traces between agent instances has been moved from an embedded\nfunctionality in tail sampling to its own configuration block.\nThis is done due to more processor benefiting from receiving consistently\nreceiving all spans for a trace in the same agent to be processed, such as\nservice graphs.\n\nAs a consequence, `tail_sampling.load_balancing` has been deprecated in favor of\na `load_balancing` block. Also, `port` has been renamed to `receiver_port` and\nmoved to the new `load_balancing` block.\n\nExample old config:\n\n```yaml\ntail_sampling:\n  policies:\n    - always_sample:\n  port: 4318\n  load_balancing:\n    exporter:\n      insecure: true\n    resolver:\n      dns:\n        hostname: agent\n        port: 4318\n```\n\nExample new config:\n\n```yaml\ntail_sampling:\n  policies:\n    - always_sample:\nload_balancing:\n  exporter:\n    insecure: true\n  resolver:\n    dns:\n      hostname: agent\n      port: 4318\n  receiver_port: 4318\n```\n\n### Operator: Rename of Prometheus to Metrics (Breaking change)\n\nAs a part of the deprecation of \"Prometheus,\" all Operator CRDs and fields with\n\"Prometheus\" in the name have changed to \"Metrics.\"\n\nThis includes:\n\n- The `PrometheusInstance` CRD is now `MetricsInstance` (referenced by\n  `metricsinstances` and not `metrics-instances` within ClusterRoles).\n- The `Prometheus` field of the `GrafanaAgent` resource is now `Metrics`\n- `PrometheusExternalLabelName` is now `MetricsExternalLabelName`\n\nThis is a hard breaking change, and all fields must change accordingly for the\noperator to continue working.\n\nNote that old CRDs with the old hyphenated names must be deleted (`kubectl\ndelete crds/{grafana-agents,prometheus-instances}`) for ClusterRoles to work\ncorrectly.\n\nTo do a zero-downtime upgrade of the Operator when there is a breaking change,\nrefer to the new `agentctl operator-detatch` command: this will iterate through\nall of your objects and remove any OwnerReferences to a CRD, allowing you to\ndelete your Operator CRDs or CRs.\n\n### Operator: Rename of CRD paths (Breaking change)\n\n`prometheus-instances` and `grafana-agents` have been renamed to\n`metricsinstances` and `grafanaagents` respectively. This is to remain\nconsistent with how Kubernetes names multi-word objects.\n\nAs a result, you will need to update your ClusterRoles to change the path of\nresources.\n\nTo do a zero-downtime upgrade of the Operator when there is a breaking change,\nrefer to the new `agentctl operator-detatch` command: this will iterate through\nall of your objects and remove any OwnerReferences to a CRD, allowing you to\ndelete your Operator CRDs or CRs.\n\n\nExample old ClusterRole:\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-operator\nrules:\n- apiGroups: [monitoring.grafana.com]\n  resources:\n  - grafana-agents\n  - prometheus-instances\n  verbs: [get, list, watch]\n```\n\nExample new ClusterRole:\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-operator\nrules:\n- apiGroups: [monitoring.grafana.com]\n  resources:\n  - grafanaagents\n  - metricsinstances\n  verbs: [get, list, watch]\n```\n\n### Metrics: Deprecation of \"prometheus\" in config. (Deprecation)\n\nThe term `prometheus` in the config has been deprecated of favor of `metrics`. This\nchange is to make it clearer when referring to Prometheus or another\nPrometheus-like database, and configuration of Grafana Agent to send metrics to\none of those systems.\n\nOld configs will continue to work for now, but support for the old format will\neventually be removed. To migrate your config, change the `prometheus` key to\n`metrics`.\n\nExample old config:\n\n```yaml\nprometheus:\n  configs:\n    - name: default\n      host_filter: false\n      scrape_configs:\n        - job_name: local_scrape\n          static_configs:\n            - targets: ['127.0.0.1:12345']\n              labels:\n                cluster: 'localhost'\n      remote_write:\n        - url: http://localhost:9009/api/prom/push\n```\n\nExample new config:\n\n```yaml\nmetrics:\n  configs:\n    - name: default\n      host_filter: false\n      scrape_configs:\n        - job_name: local_scrape\n          static_configs:\n            - targets: ['127.0.0.1:12345']\n              labels:\n                cluster: 'localhost'\n      remote_write:\n        - url: http://localhost:9009/api/prom/push\n```\n\n### Tempo: prom_instance rename (Breaking change)\n\nAs part of `prometheus` being renamed to `metrics`, the spanmetrics\n`prom_instance` field has been renamed to `metrics_instance`. This is a breaking\nchange, and the old name will no longer work.\n\nExample old config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    spanmetrics:\n      prom_instance: default\n```\n\nExample new config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    spanmetrics:\n      metrics_instance: default\n```\n\n### Logs: Deprecation of \"loki\" in config. (Deprecation)\n\nThe term `loki` in the config has been deprecated of favor of `logs`. This\nchange is to make it clearer when referring to Grafana Loki, and\nconfiguration of Grafana Agent to send logs to Grafana Loki.\n\nOld configs will continue to work for now, but support for the old format will\neventually be removed. To migrate your config, change the `loki` key to `logs`.\n\nExample old config:\n\n```yaml\nloki:\n  positions_directory: /tmp/loki-positions\n  configs:\n  - name: default\n    clients:\n      - url: http://localhost:3100/loki/api/v1/push\n    scrape_configs:\n    - job_name: system\n      static_configs:\n      - targets: ['localhost']\n        labels:\n          job: varlogs\n          __path__: /var/log/*log\n```\n\nExample new config:\n\n```yaml\nlogs:\n  positions_directory: /tmp/loki-positions\n  configs:\n  - name: default\n    clients:\n      - url: http://localhost:3100/loki/api/v1/push\n    scrape_configs:\n    - job_name: system\n      static_configs:\n      - targets: ['localhost']\n        labels:\n          job: varlogs\n          __path__: /var/log/*log\n```\n\n#### Tempo: Deprecation of \"loki\" in config. (Deprecation)\n\nAs part of the `loki` to `logs` rename, parts of the automatic_logging component\nin Tempo have been updated to refer to `logs_instance` instead.\n\nOld configurations using `loki_name`, `loki_tag`, or `backend: loki` will\ncontinue to work as of this version, but support for the old config format\nwill eventually be removed.\n\nExample old config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    automatic_logging:\n      backend: loki\n      loki_name: default\n      spans: true\n      processes: true\n      roots: true\n    overrides:\n      loki_tag: tempo\n```\n\nExample new config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    automatic_logging:\n      backend: logs_instance\n      logs_instance_name: default\n      spans: true\n      processes: true\n      roots: true\n    overrides:\n      logs_instance_tag: tempo\n```\n\n## v0.18.0\n\n### Tempo: Remote write TLS config\n\nTempo `remote_write` now supports configuring TLS settings in the trace\nexporter's client. `insecure_skip_verify` is moved into this setting's block.\n\nOld configurations with `insecure_skip_verify` outside `tls_config` will continue\nto work as of this version, but support will eventually be removed.\nIf both `insecure_skip_verify` and `tls_config.insecure_skip_verify` are used,\nthen the latter take precedence.\n\nExample old config:\n\n```\ntempo:\n  configs:\n    - name: default\n      remote_write:\n        - endpoint: otel-collector:55680\n          insecure: true\n          insecure_skip_verify: true\n```\n\nExample new config:\n\n```\ntempo:\n  configs:\n    - name: default\n      remote_write:\n        - endpoint: otel-collector:55680\n          insecure: true\n          tls_config:\n            insecure_skip_verify: true\n```\n\n## v0.15.0\n\n### Tempo: `automatic_logging` changes\n\nTempo automatic logging previously assumed that the operator wanted to log\nto a Loki instance. With the addition of an option to log to stdout a new\nfield is required to maintain the old behavior.\n\nExample old config:\n\n```\ntempo:\n  configs:\n  - name: default\n    automatic_logging:\n      loki_name: <some loki instance>\n```\n\nExample new config:\n\n```\ntempo:\n  configs:\n  - name: default\n    automatic_logging:\n      backend: loki\n      loki_name: <some loki instance>\n```\n\n## v0.14.0\n\n### Scraping Service security change\n\nv0.14.0 changes the default behavior of the scraping service config management\nAPI to reject all configuration files that read credentials from a file on disk.\nThis prevents malicious users from crafting an instance config file that read\narbitrary files on disk and send their contents to remote endpoints.\n\nTo revert to the old behavior, add `dangerous_allow_reading_files: true` in your\n`scraping_service` config.\n\nExample old config:\n\n```yaml\nprometheus:\n  scraping_service:\n    # ...\n```\n\nExample new config:\n\n```yaml\nprometheus:\n  scraping_service:\n    dangerous_allow_reading_files: true\n    # ...\n```\n\n### SigV4 config change\n\nv0.14.0 updates the internal Prometheus dependency to 2.26.0, which includes\nnative support for SigV4, but uses a slightly different configuration structure\nthan the Grafana Agent did.\n\nTo migrate, remove the `enabled` key from your `sigv4` configs. If `enabled` was\nthe only key, define sigv4 as an empty object: `sigv4: {}`.\n\nExample old config:\n\n```yaml\nsigv4:\n  enabled: true\n  region: us-east-1\n```\n\nExample new config:\n\n```yaml\nsigv4:\n  region: us-east-1\n```\n\n### Tempo: `push_config` deprecation\n\n`push_config` is now deprecated in favor of a `remote_write` array which allows for sending spans to multiple endpoints.\n`push_config` will be removed in a future release, and it is recommended to migrate to `remote_write` as soon as possible.\n\nTo migrate, move the batch options outside the `push_config` block.\nThen, add a `remote_write` array and move the remaining of your `push_config` block inside it.\n\nExample old config:\n\n```yaml\ntempo:\n  configs:\n    - name: default\n      receivers:\n        otlp:\n          protocols:\n            gpc:\n      push_config:\n        endpoint: otel-collector:55680\n        insecure: true\n        batch:\n          timeout: 5s\n          send_batch_size: 100\n```\n\nExample migrated config:\n\n```yaml\ntempo:\n  configs:\n    - name: default\n      receivers:\n        otlp:\n          protocols:\n            gpc:\n      remote_write:\n        - endpoint: otel-collector:55680\n          insecure: true\n      batch:\n        timeout: 5s\n        send_batch_size: 100\n```\n\n\n## v0.12.0\n\nv0.12.0 had two breaking changes: the `tempo` and `loki` sections have been changed to require a list of `tempo`/`loki` configs rather than just one.\n\n### Tempo Config Change\n\nThe Tempo config (`tempo` in the config file) has been changed to store\nconfigs within a `configs` list. This allows for defining multiple Tempo\ninstances for collecting traces and forwarding them to different OTLP\nendpoints.\n\nTo migrate, add a `configs:` array and move your existing config inside of it.\nGive the element a `name: default` field.\n\nEach config must have a unique non-empty name. `default` is recommended for users\nthat don't have other configs. The name of the config will be added as a\n`tempo_config` label for metrics.\n\nExample old config:\n\n```yaml\ntempo:\n  receivers:\n    jaeger:\n      protocols:\n        thrift_http:\n  attributes:\n    actions:\n    - action: upsert\n      key: env\n      value: prod\n  push_config:\n    endpoint: otel-collector:55680\n    insecure: true\n    batch:\n      timeout: 5s\n      send_batch_size: 100\n```\n\nExample migrated config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    receivers:\n      jaeger:\n        protocols:\n          thrift_http:\n    attributes:\n      actions:\n      - action: upsert\n        key: env\n        value: prod\n    push_config:\n      endpoint: otel-collector:55680\n      insecure: true\n      batch:\n        timeout: 5s\n        send_batch_size: 100\n```\n\n### Loki Promtail Config Change\n\nThe Loki Promtail config (`loki` in the config file) has been changed to store\nconfigs within a `configs` list. This allows for defining multiple Loki\nPromtail instances for collecting logs and forwarding them to different Loki\nservers.\n\nTo migrate, add a `configs:` array and move your existing config inside of it.\nGive the element a `name: default` field.\n\nEach config must have a unique non-empty name. `default` is recommended for users\nthat don't have other configs. The name of the config will be added as a\n`loki_config` label for Loki Promtail metrics.\n\nExample old config:\n\n```yaml\nloki:\n  positions:\n    filename: /tmp/positions.yaml\n  clients:\n    - url: http://loki:3100/loki/api/v1/push\n  scrape_configs:\n  - job_name: system\n    static_configs:\n      - targets:\n        - localhost\n        labels:\n          job: varlogs\n          __path__: /var/log/*log\n```\n\nExample migrated config:\n\n```yaml\nloki:\n  configs:\n  - name: default\n    positions:\n      filename: /tmp/positions.yaml\n    clients:\n      - url: http://loki:3100/loki/api/v1/push\n    scrape_configs:\n    - job_name: system\n      static_configs:\n        - targets:\n          - localhost\n          labels:\n            job: varlogs\n            __path__: /var/log/*log\n```\n", "package operator\n\n// Supported versions of the Grafana Agent.\nvar (\n\tAgentCompatibilityMatrix = []string{\n\t\t\"v0.14.0\",\n\t\t\"v0.15.0\",\n\t\t// \"v0.16.0\", // Pulled due to critical bug fixed in v0.16.1.\n\t\t\"v0.16.1\",\n\t\t\"v0.17.0\",\n\t\t\"v0.18.0\",\n\t\t\"v0.18.1\",\n\t\t\"v0.18.2\",\n\t\t\"v0.18.3\",\n\t\t\"v0.18.4\",\n\t\t\"v0.19.0\",\n\t\t\"v0.20.0\",\n\t\t\"v0.21.0\",\n\t\t\"v0.21.1\",\n\n\t\t// NOTE(rfratto): when performing an upgrade, add the newest version above instead of changing the existing reference.\n\t}\n\n\tDefaultAgentVersion   = AgentCompatibilityMatrix[len(AgentCompatibilityMatrix)-1]\n\tDefaultAgentBaseImage = \"grafana/agent\"\n\tDefaultAgentImage     = DefaultAgentBaseImage + \":\" + DefaultAgentVersion\n)\n", "# Running Grafana Agent\n\nHere are some resources to help you run the Grafana Agent:\n\n- [Windows Installation](#windows-installation)\n- [Run the Agent with Docker](#running-the-agent-with-docker)\n- [Run the Agent locally](#running-the-agent-locally)\n- [Use the example Kubernetes configs](#use-the-example-kubernetes-configs)\n- [Grafana Cloud Kubernetes Quickstart Guides](#grafana-cloud-kubernetes-quickstart-guides)\n- [Agent Operator Helm Quickstart](#agent-operator-helm-quickstart-guide)\n- [Build the Agent from Source](#build-the-agent-from-source)\n- [Use our production Tanka configs](#use-our-production-tanka-configs)\n\n## Windows Installation\n\nTo run the Windows Installation, download the Windows Installer executable from the [release page](https://github.com/grafana/agent/releases). Then run the installer, this will setup the Agent and run the Agent as a Windows Service. More details can be found in the [Windows Guide](../docs/getting-started/install-agent-on-windows.md)\n\n## Running the Agent with Docker\n\nTo run the Agent with Docker, you should have a configuration file on\nyour local machine ready to bind mount into the container. Then modify\nthe following command for your environment. Replace `/path/to/config.yaml` with\nthe full path to your YAML configuration, and replace `/tmp/agent` with the\ndirectory on your host that you want the agent to store its WAL.\n\n```\ndocker run \\\n  -v /tmp/agent:/etc/agent/data \\\n  -v /path/to/config.yaml:/etc/agent/agent.yaml \\\n  grafana/agent:v0.21.1\n```\n\n## Running the Agent locally\n\nCurrently, you must provide your own system configuration files to run the\nAgent as a long-living process (e.g., write your own systemd unit files).\n\n## Use the example Kubernetes configs\n\nYou can find sample deployment manifests in the [Kubernetes](./kubernetes) directory.\n\n## Grafana Cloud Kubernetes quickstart guides\n\nThese guides help you get up and running with the Agent and Grafana Cloud, and include sample ConfigMaps.\n\nYou can find them in the [Grafana Cloud documentation](https://grafana.com/docs/grafana-cloud/quickstart/agent-k8s/)\n\n## Agent Operator Helm quickstart guide\n\nThis guide will show you how to deploy the [Grafana Agent Operator](https://grafana.com/docs/agent/latest/operator/) into your Kubernetes cluster using the [grafana-agent-operator Helm chart](https://github.com/grafana/helm-charts/tree/main/charts/agent-operator).\n\nYou'll also deploy the following custom resources (CRs):\n- A `GrafanaAgent` resource, which discovers one or more `MetricsInstance` and `LogsInstances` resources.\n- A `MetricsInstance` resource that defines where to ship collected metrics.\n- A `ServiceMonitor` resource to collect cAdvisor and kubelet metrics.\n- A `LogsInstance` resource that defines where to ship collected logs.\n- A `PodLogs` resource to collect container logs from Kubernetes Pods.\n\nYou can find the guide [here](https://grafana.com/docs/agent/latest/operator/helm-getting-started/).\n\n## Build the Agent from source\n\nGo 1.14 is currently needed to build the agent from source. Run `make agent`\nfrom the root of this repository, and then the build agent binary will be placed\nat `./cmd/agent/agent`.\n\n## Use our production Tanka configs\n\nThe Tanka configs we use to deploy the agent ourselves can be found in our\n[production Tanka directory](./tanka/grafana-agent). These configs are also used\nto generate the Kubernetes configs for the install script. To get started with\nthe tanka configs, do the following:\n\n```\nmkdir tanka-agent\ncd tanka-agent\ntk init --k8s=false\njb install github.com/grafana/agent/production/tanka/grafana-agent\n\n# substitute your target k8s version for \"1.16\" in the next few commands\njb install github.com/jsonnet-libs/k8s-alpha/1.16\necho '(import \"github.com/jsonnet-libs/k8s-alpha/1.16/main.libsonnet\")' > lib/k.libsonnet\necho '+ (import \"github.com/jsonnet-libs/k8s-alpha/1.16/extensions/kausal-shim.libsonnet\")' >> lib/k.libsonnet\n```\n\nThen put this in `environments/default/main.jsonnet`:\n```\nlocal agent = import 'grafana-agent/grafana-agent.libsonnet';\n\nagent {\n  _config+:: {\n    namespace: 'grafana-agent'\n  },\n}\n```\n\nIf all these steps worked, `tk eval environments/default` should output the\ndefault JSON we use to build our Kubernetes manifests.\n", "#!/usr/bin/env sh\n# shellcheck shell=dash\n# This script should run in all POSIX environments and Dash is POSIX compliant.\n\n# grafanacloud-install.sh installs the Grafana Agent on supported\n# Linux systems for Grafana Cloud users. Those who aren't users of Grafana Cloud\n# or need to install the Agent on a different architecture or platform should\n# try another installation method.\n#\n# grafanacloud-install.sh has a hard dependency on being run on a supported\n# Linux system. Currently only systems that can install deb or rpm packages\n# are supported. The target system will try to be detected, but if it cannot,\n# PACKAGE_SYSTEM can be passed as an environment variable with either rpm or\n# deb.\nset -eu\ntrap \"exit 1\" TERM\nMY_PID=$$\n\nlog() {\n  echo \"$@\" >&2\n}\n\nfatal() {\n  log \"$@\"\n  kill -s TERM \"$MY_PID\"\n}\n\n#\n# REQUIRED environment variables.\n#\nGCLOUD_STACK_ID=${GCLOUD_STACK_ID:=} # Stack ID where integrations are installed\nGCLOUD_API_KEY=${GCLOUD_API_KEY:=}   # API key to authenticate against Grafana Cloud's API with\nGCLOUD_API_URL=${GCLOUD_API_URL:=}   # Grafana Cloud's API url\n\n[ -z \"$GCLOUD_STACK_ID\" ] && fatal \"Required environment variable \\$GCLOUD_STACK_ID not set.\"\n[ -z \"$GCLOUD_API_KEY\" ]  && fatal \"Required environment variable \\$GCLOUD_API_KEY not set.\"\n\n#\n# OPTIONAL environment variables.\n#\n\n# Architecture to install.\nARCH=${ARCH:=amd64}\n\n# Package system to install the Agent with. If not empty, MUST be either rpm or\n# deb. If empty, the script will try to detect the host OS and the appropriate\n# package system to use.\nPACKAGE_SYSTEM=${PACKAGE_SYSTEM:=}\n\n#\n# Global constants.\n#\nRELEASE_VERSION=\"0.21.1\"\n\nRELEASE_URL=\"https://github.com/grafana/agent/releases/download/v${RELEASE_VERSION}\"\nDEB_URL=\"${RELEASE_URL}/grafana-agent-${RELEASE_VERSION}-1.${ARCH}.deb\"\nRPM_URL=\"${RELEASE_URL}/grafana-agent-${RELEASE_VERSION}-1.${ARCH}.rpm\"\n\nmain() {\n  if [ -z \"$PACKAGE_SYSTEM\" ]; then\n    PACKAGE_SYSTEM=$(detect_package_system)\n  fi\n  log \"--- Using package system $PACKAGE_SYSTEM. Downloading and installing package for ${ARCH}\"\n\n  case \"$PACKAGE_SYSTEM\" in\n    deb)\n      install_deb\n      ;;\n    rpm)\n      install_rpm\n      ;;\n    *)\n      fatal \"Unsupported PACKAGE_SYSTEM value $PACKAGE_SYSTEM. Must be either rpm or deb\".\n      ;;\n  esac\n\n  log '--- Retrieving config and placing in /etc/grafana-agent.yaml'\n  retrieve_config | sudo tee /etc/grafana-agent.yaml\n\n  log '--- Enabling and starting grafana-agent.service'\n  sudo systemctl enable grafana-agent.service\n  sudo systemctl start grafana-agent.service\n\n  # Add some empty newlines to give some visual whitespace before printing the\n  # success message.\n  log ''\n  log ''\n  log 'Grafana Agent is now running! To check the status of your Agent, run:'\n  log '   sudo systemctl status grafana-agent.service'\n}\n\n# detect_package_system tries to detect the host distribution to determine if\n# deb or rpm should be used for installing the Agent. Prints out either \"deb\"\n# or \"rpm\". Calls fatal if the host OS is not supported.\ndetect_package_system() {\n  command -v dpkg >/dev/null 2>&1 && { echo \"deb\"; return; }\n  command -v rpm  >/dev/null 2>&1 && { echo \"rpm\"; return; }\n\n  case \"$(uname)\" in\n    Darwin)\n      fatal 'macOS not supported'\n      ;;\n    *)\n      fatal \"Unknown unsupported OS: $(uname)\"\n      ;;\n  esac\n}\n\n# install_deb downloads and installs the deb package of the Grafana Agent.\ninstall_deb() {\n  curl -fsL \"${DEB_URL}\" -o /tmp/grafana-agent.deb || fatal 'Failed to download package'\n  sudo dpkg -i /tmp/grafana-agent.deb\n  rm /tmp/grafana-agent.deb\n}\n\n# install_rpm downloads and installs the deb package of the Grafana Agent.\ninstall_rpm() {\n  sudo rpm --reinstall \"${RPM_URL}\"\n}\n\n# retrieve_config downloads the config file for the Agent and prints out its\n# contents to stdout.\nretrieve_config() {\n  if ! grafana-agentctl cloud-config -u \"${GCLOUD_STACK_ID}\" -p \"${GCLOUD_API_KEY}\" -e \"${GCLOUD_API_URL}\" 2>/dev/null; then\n    fatal \"Failed to retrieve config\"\n  fi\n}\n\nmain\n", "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent\n  namespace: ${NAMESPACE}\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent\n  namespace: ${NAMESPACE}\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-agent\n  namespace: ${NAMESPACE}\nspec:\n  minReadySeconds: 10\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      name: grafana-agent\n  template:\n    metadata:\n      labels:\n        name: grafana-agent\n    spec:\n      containers:\n      - args:\n        - -config.file=/etc/agent/agent.yaml\n        command:\n        - /bin/agent\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        image: grafana/agent:v0.21.1\n        imagePullPolicy: IfNotPresent\n        name: agent\n        ports:\n        - containerPort: 12345\n          name: http-metrics\n        volumeMounts:\n        - mountPath: /etc/agent\n          name: grafana-agent\n      serviceAccount: grafana-agent\n      volumes:\n      - configMap:\n          name: grafana-agent\n        name: grafana-agent\n", "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent-logs\n  namespace: YOUR_NAMESPACE\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-logs\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent-logs\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent-logs\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent-logs\n  namespace: YOUR_NAMESPACE\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: grafana-agent-logs\n  namespace: YOUR_NAMESPACE\nspec:\n  minReadySeconds: 10\n  selector:\n    matchLabels:\n      name: grafana-agent-logs\n  template:\n    metadata:\n      labels:\n        name: grafana-agent-logs\n    spec:\n      containers:\n      - args:\n        - -config.file=/etc/agent/agent.yaml\n        command:\n        - /bin/agent\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        image: grafana/agent:v0.21.1\n        imagePullPolicy: IfNotPresent\n        name: agent\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        securityContext:\n          privileged: true\n          runAsUser: 0\n        volumeMounts:\n        - mountPath: /etc/agent\n          name: grafana-agent-logs\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      serviceAccount: grafana-agent-logs\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - configMap:\n          name: grafana-agent-logs\n        name: grafana-agent-logs\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n        name: etcmachineid\n  updateStrategy:\n    type: RollingUpdate\n", "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent-traces\n  namespace: YOUR_NAMESPACE\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-traces\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent-traces\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent-traces\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent-traces\n  namespace: YOUR_NAMESPACE\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: grafana-agent-traces\n  name: grafana-agent-traces\n  namespace: YOUR_NAMESPACE\nspec:\n  ports:\n  - name: agent-http-metrics\n    port: 8080\n    targetPort: 8080\n  - name: agent-thrift-compact\n    port: 6831\n    protocol: UDP\n    targetPort: 6831\n  - name: agent-thrift-binary\n    port: 6832\n    protocol: UDP\n    targetPort: 6832\n  - name: agent-thrift-http\n    port: 14268\n    protocol: TCP\n    targetPort: 14268\n  - name: agent-thrift-grpc\n    port: 14250\n    protocol: TCP\n    targetPort: 14250\n  - name: agent-zipkin\n    port: 9411\n    protocol: TCP\n    targetPort: 9411\n  - name: agent-otlp\n    port: 55680\n    protocol: TCP\n    targetPort: 55680\n  - name: agent-opencensus\n    port: 55678\n    protocol: TCP\n    targetPort: 55678\n  selector:\n    name: grafana-agent-traces\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-agent-traces\n  namespace: YOUR_NAMESPACE\nspec:\n  minReadySeconds: 10\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      name: grafana-agent-traces\n  template:\n    metadata:\n      labels:\n        name: grafana-agent-traces\n    spec:\n      containers:\n      - args:\n        - -config.file=/etc/agent/agent.yaml\n        command:\n        - /bin/agent\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        image: grafana/agent:v0.21.1\n        imagePullPolicy: IfNotPresent\n        name: agent\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 6831\n          name: thrift-compact\n          protocol: UDP\n        - containerPort: 6832\n          name: thrift-binary\n          protocol: UDP\n        - containerPort: 14268\n          name: thrift-http\n          protocol: TCP\n        - containerPort: 14250\n          name: thrift-grpc\n          protocol: TCP\n        - containerPort: 9411\n          name: zipkin\n          protocol: TCP\n        - containerPort: 55680\n          name: otlp\n          protocol: TCP\n        - containerPort: 55678\n          name: opencensus\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /etc/agent\n          name: grafana-agent-traces\n      serviceAccount: grafana-agent-traces\n      volumes:\n      - configMap:\n          name: grafana-agent-traces\n        name: grafana-agent-traces\n", "'grafana/agent:v0.21.1'\n", "#!/usr/bin/env bash\n# shellcheck shell=bash\n\n#\n# install-bare.sh is an installer for the Agent without a ConfigMap. It is\n# used during the Grafana Cloud integrations wizard and is not recommended\n# to be used directly. Instead of calling this script directly, please\n# make a copy of ./agent-bare.yaml and modify it for your needs.\n#\n# Note that agent-bare.yaml does not have a ConfigMap, so the Grafana Agent\n# will not launch until one is created. For more information on setting up\n# a ConfigMap, please refer to:\n#\n# Metrics quickstart: https://grafana.com/docs/grafana-cloud/quickstart/agent-k8s/k8s_agent_metrics/\n# Logs quickstart: https://grafana.com/docs/grafana-cloud/quickstart/agent-k8s/k8s_agent_logs/\n#\n\ncheck_installed() {\n  if ! type \"$1\" >/dev/null 2>&1; then\n    echo \"error: $1 not installed\" >&2\n    exit 1\n  fi\n}\n\ncheck_installed curl\ncheck_installed envsubst\n\nMANIFEST_BRANCH=v0.21.1\nMANIFEST_URL=${MANIFEST_URL:-https://raw.githubusercontent.com/grafana/agent/${MANIFEST_BRANCH}/production/kubernetes/agent-bare.yaml}\nNAMESPACE=${NAMESPACE:-default}\n\nexport NAMESPACE\n\ncurl -fsSL \"$MANIFEST_URL\" | envsubst\n", "local agent = import './internal/agent.libsonnet';\nlocal utils = import './internal/utils.libsonnet';\nlocal k = import 'ksonnet-util/kausal.libsonnet';\n\nlocal container = k.core.v1.container;\nlocal configMap = k.core.v1.configMap;\nlocal service = k.core.v1.service;\n\n// Merge all of our libraries to create the final exposed library.\n(import './lib/deployment.libsonnet') +\n(import './lib/integrations.libsonnet') +\n(import './lib/metrics.libsonnet') +\n(import './lib/scraping_service.libsonnet') +\n(import './lib/logs.libsonnet') +\n(import './lib/traces.libsonnet') +\n{\n  _images:: {\n    agent: 'grafana/agent:v0.21.1',\n    agentctl: 'grafana/agentctl:v0.21.1',\n  },\n\n  // new creates a new DaemonSet deployment of the grafana-agent. By default,\n  // the deployment will do no collection. You must merge the result of this\n  // function with one or more of the following:\n  //\n  // - withMetricsConfig, withMetricsInstances (and optionally withRemoteWrite)\n  // - withLogsConfig\n  //\n  // When using withMetricsInstances, a [name]-etc deployment\n  // with one replica will be created alongside the DaemonSet. This deployment\n  // is responsible for handling scrape configs that will not work on the host\n  // machine.\n  //\n  // For example, if a scrape_config scrapes the Kubernetes API, that must be\n  // handled by the [name]-etc deployment as the Kubernetes API does not run\n  // on any node in the cluster.\n  //\n  // scrapeInstanceKubernetes provides the default\n  // MetricsInstanceConfig Grafana Labs uses in production.\n  new(name='grafana-agent', namespace='default'):: {\n    local this = self,\n\n    _mode:: 'daemonset',\n    _images:: $._images,\n    _config_hash:: true,\n\n    local has_logs_config = std.objectHasAll(self, '_logs_config'),\n    local has_trace_config = std.objectHasAll(self, '_trace_config'),\n    local has_metrics_config = std.objectHasAll(self, '_metrics_config'),\n    local has_metrics_instances = std.objectHasAll(self, '_metrics_instances'),\n    local has_integrations = std.objectHasAll(self, '_integrations'),\n    local has_sampling_strategies = std.objectHasAll(self, '_traces_sampling_strategies'),\n\n    local metrics_instances =\n      if has_metrics_instances then this._metrics_instances else [],\n    local host_filter_instances = utils.transformInstances(metrics_instances, true),\n    local etc_instances = utils.transformInstances(metrics_instances, false),\n\n    config:: {\n      server: {\n        log_level: 'info',\n        http_listen_port: 8080,\n      },\n    } + (\n      if has_metrics_config\n      then { metrics: this._metrics_config { configs: host_filter_instances } }\n      else {}\n    ) + (\n      if has_logs_config then {\n        logs: {\n          positions_directory: '/tmp/positions',\n          configs: [this._logs_config {\n            name: 'default',\n          }],\n        },\n      } else {}\n    ) + (\n      if has_trace_config then {\n        traces: {\n          configs: [this._trace_config {\n            name: 'default',\n          }],\n        },\n      }\n      else {}\n    ) + (\n      if has_integrations then { integrations: this._integrations } else {}\n    ),\n\n    etc_config:: if has_metrics_config then this.config {\n      // Hide logs and integrations from our extra configs, we just want the\n      // scrape configs that wouldn't work for the DaemonSet.\n      metrics+: {\n        configs: std.map(function(cfg) cfg { host_filter: false }, etc_instances),\n      },\n      logs:: {},\n      traces:: {},\n      integrations:: {},\n    },\n\n    agent:\n      agent.newAgent(name, namespace, self._images.agent, self.config, use_daemonset=true) +\n      agent.withConfigHash(self._config_hash) + {\n        // If sampling strategies were defined, we need to mount them as a JSON\n        // file.\n        config_map+:\n          if has_sampling_strategies\n          then configMap.withDataMixin({\n            'strategies.json': std.toString(this._traces_sampling_strategies),\n          })\n          else {},\n\n        // If we're deploying for tracing, applications will want to write to\n        // a service for load balancing span delivery.\n        service:\n          if has_trace_config\n          then k.util.serviceFor(self.agent) + service.mixin.metadata.withNamespace(namespace)\n          else {},\n      } + (\n        if has_logs_config then $.logsPermissionsMixin else {}\n      ) + (\n        if has_integrations && std.objectHas(this._integrations, 'node_exporter') then $.integrationsMixin else {}\n      ),\n\n    agent_etc: if std.length(etc_instances) > 0 then\n      agent.newAgent(name + '-etc', namespace, self._images.agent, self.etc_config, use_daemonset=false) +\n      agent.withConfigHash(self._config_hash),\n  },\n\n  // withImages sets the images used for launching the Agent.\n  // Keys supported: agent, agentctl\n  withImages(images):: { _images+: images },\n\n  // Includes or excludes the config hash annotation.\n  withConfigHash(include=true):: { _config_hash:: include },\n\n  // withPortsMixin adds extra ports to expose.\n  withPortsMixin(ports=[]):: {\n    agent+: {\n      container+:: container.withPortsMixin(ports),\n    },\n  },\n}\n", "function(name='grafana-agent', namespace='') {\n  local k = (import 'ksonnet-util/kausal.libsonnet') { _config+:: { namespace: namespace } },\n\n  local container = k.core.v1.container,\n  local configMap = k.core.v1.configMap,\n  local containerPort = k.core.v1.containerPort,\n  local policyRule = k.rbac.v1.policyRule,\n  local serviceAccount = k.core.v1.serviceAccount,\n\n  local this = self,\n\n  _images:: {\n    agent: 'grafana/agent:v0.21.1',\n    agentctl: 'grafana/agentctl:v0.21.1',\n  },\n  _config:: {\n    name: name,\n    namespace: namespace,\n    config_hash: true,\n    agent_config: '',\n  },\n\n  rbac: k.util.rbac(name, [\n    policyRule.withApiGroups(['']) +\n    policyRule.withResources(['nodes', 'nodes/proxy', 'services', 'endpoints', 'pods']) +\n    policyRule.withVerbs(['get', 'list', 'watch']),\n\n    policyRule.withNonResourceUrls('/metrics') +\n    policyRule.withVerbs(['get']),\n  ]) {\n    service_account+: serviceAccount.mixin.metadata.withNamespace(namespace),\n  },\n\n  configMap:\n    configMap.new(name) +\n    configMap.mixin.metadata.withNamespace(namespace) +\n    configMap.withData({\n      'agent.yaml': k.util.manifestYaml(this._config.agent_config),\n    }),\n\n  container::\n    container.new(name, this._images.agent) +\n    container.withPorts(containerPort.new('http-metrics', 80)) +\n    container.withCommand('/bin/agent') +\n    container.withArgsMixin(k.util.mapToFlags({\n      'config.file': '/etc/agent/agent.yaml',\n    })),\n}\n", "local k = import 'ksonnet-util/kausal.libsonnet';\n\nlocal cronJob = k.batch.v1beta1.cronJob;\nlocal configMap = k.core.v1.configMap;\nlocal container = k.core.v1.container;\nlocal deployment = k.apps.v1.deployment;\nlocal volumeMount = k.core.v1.volumeMount;\nlocal volume = k.core.v1.volume;\n\nfunction(\n  name='grafana-agent-syncer',\n  namespace='',\n  config={},\n) {\n  local _config = {\n    api: error 'api must be set',\n    image: 'grafana/agentctl:v0.21.1',\n    schedule: '*/5 * * * *',\n    configs: [],\n  } + config,\n\n  local this = self,\n  local _configs = std.foldl(\n    function(agg, cfg)\n      // Sanitize the name and remove / so every file goes into the same\n      // folder.\n      local name = std.strReplace(cfg.name, '/', '_');\n\n      agg { ['%s.yml' % name]: k.util.manifestYaml(cfg) },\n    _config.configs,\n    {},\n  ),\n\n  configMap:\n    configMap.new(name) +\n    configMap.mixin.metadata.withNamespace(namespace) +\n    configMap.withData(_configs),\n\n  container::\n    container.new(name, _config.image) +\n    container.withArgsMixin([\n      'config-sync',\n      '--addr=%s' % _config.api,\n      '/etc/configs',\n    ]) +\n    container.withVolumeMounts(volumeMount.new(name, '/etc/configs')),\n\n  job:\n    cronJob.new(name, _config.schedule, this.container) +\n    cronJob.mixin.metadata.withNamespace(namespace) +\n    cronJob.mixin.spec.withSuccessfulJobsHistoryLimit(1) +\n    cronJob.mixin.spec.withFailedJobsHistoryLimit(3) +\n    cronJob.mixin.spec.jobTemplate.spec.template.spec.withRestartPolicy('OnFailure') +\n    cronJob.mixin.spec.jobTemplate.spec.template.spec.withActiveDeadlineSeconds(600) +\n    cronJob.mixin.spec.jobTemplate.spec.withTtlSecondsAfterFinished(120) +\n    cronJob.mixin.spec.jobTemplate.spec.template.spec.withVolumes([\n      volume.fromConfigMap(\n        name=name,\n        configMapName=this.configMap.metadata.name,\n      ),\n    ]),\n}\n"], "fixing_code": ["# Main (unreleased)\n\n- [FEATURE] (beta) Enable experimental config urls for fetching remote configs. Currently,\n   only HTTP/S is supported. Use `-experiment.config-urls.enable` flag to turn this on. (@rlankfo)\n\n- [ENHANCEMENT] Traces: Improved pod association in PromSD processor (@mapno)\n\n# v0.21.2 (2021-12-08)\n\n- [SECURITY] This release contains a fix for\n  [CVE-2021-41090](https://github.com/grafana/agent/security/advisories/GHSA-9c4x-5hgq-q3wh).\n\n- [CHANGE] This release disables the existing `/-/config` and\n  `/agent/api/v1/configs/{name}` endpoitns by default. Pass the\n  `--config.enable-read-api` flag at the command line to opt in to these\n  endpoints.\n\n# v0.21.1 (2021-11-18)\n\n- [BUGFIX] Fix panic when using postgres_exporter integration (@saputradharma)\n\n- [BUGFIX] Fix panic when dnsamsq_exporter integration tried to log a warning (@rfratto)\n\n- [BUGFIX] Statsd Integration: Adding logger instance to the statsd mapper instantiation. (@gaantunes)\n\n- [BUGFIX] Statsd Integration: Fix issue where mapped metrics weren't exposed to the integration. (@mattdurham)\n\n- [BUGFIX] Operator: fix bug where version was a required field (@rfratto)\n\n- [BUGFIX] Metrics: Only run WAL cleaner when metrics are being used and a WAL is configured. (@rfratto)\n\n# v0.21.0 (2021-11-17)\n\n- [ENHANCEMENT] Update Cortex dependency to v1.10.0-92-g85c378182. (@rlankfo)\n\n- [ENHANCEMENT] Update Loki dependency to v2.1.0-656-g0ae0d4da1. (@rlankfo)\n\n- [ENHANCEMENT] Update Prometheus dependency to v2.31.0 (@rlankfo)\n\n- [ENHANCEMENT] Add Agent Operator Helm quickstart guide (@hjet)\n\n- [ENHANCEMENT] Reorg Agent Operator quickstart guides (@hjet)\n\n- [BUGFIX] Packaging: Use correct user/group env variables in RPM %post script (@simonc6372)\n\n- [BUGFIX] Validate logs config when using logs_instance with automatic logging processor (@mapno)\n\n- [BUGFIX] Operator: Fix MetricsInstance Service port (@hjet)\n\n- [BUGFIX] Operator: Create govern service per Grafana Agent (@shturman)\n\n- [BUGFIX] Operator: Fix relabel_config directive for PodLogs resource (@hjet)\n\n- [BUGFIX] Traces: Fix `success_logic` code in service graphs processor (@mapno)\n\n- [CHANGE] Self-scraped integrations will now use an SUO-specific value for the `instance` label. (@rfratto)\n\n- [CHANGE] Traces: Changed service graphs store implementation to improve CPU performance (@mapno)\n\n# v0.20.1 (2021-12-08)\n\n*NOTE*: The fixes in this patch are only present in v0.20.1 and >=v0.21.2.\n\n- [SECURITY] This release contains a fix for\n  [CVE-2021-41090](https://github.com/grafana/agent/security/advisories/GHSA-9c4x-5hgq-q3wh).\n\n- [CHANGE] This release disables the existing `/-/config` and\n  `/agent/api/v1/configs/{name}` endpoitns by default. Pass the\n  `--config.enable-read-api` flag at the command line to opt in to these\n  endpoints.\n\n# v0.20.0 (2021-10-28)\n\n- [FEATURE] Operator: The Grafana Agent Operator can now generate a Kubelet\n  service to allow a ServiceMonitor to collect Kubelet and cAdvisor metrics.\n  This requires passing a `--kubelet-service` flag to the Operator in\n  `namespace/name` format (like `kube-system/kubelet`). (@rfratto)\n\n- [FEATURE] Service graphs processor (@mapno)\n\n- [ENHANCEMENT] Updated mysqld_exporter to v0.13.0 (@gaantunes)\n\n- [ENHANCEMENT] Updated postgres_exporter to v0.10.0 (@gaantunes)\n\n- [ENHANCEMENT] Updated redis_exporter to v1.27.1 (@gaantunes)\n\n- [ENHANCEMENT] Updated memcached_exporter to v0.9.0 (@gaantunes)\n\n- [ENHANCEMENT] Updated statsd_exporter to v0.22.2 (@gaantunes)\n\n- [ENHANCEMENT] Updated elasticsearch_exporter to v1.2.1 (@gaantunes)\n\n- [ENHANCEMENT] Add remote write to silent Windows Installer  (@mattdurham)\n\n- [ENHANCEMENT] Updated mongodb_exporter to v0.20.7 (@rfratto)\n\n- [ENHANCEMENT] Updated OTel to v0.36 (@mapno)\n\n- [ENHANCEMENT] Updated statsd_exporter to v0.22.2 (@mattdurham)\n\n- [ENHANCEMENT] Update windows_exporter to v0.16.0 (@rfratto, @mattdurham)\n\n- [ENHANCEMENT] Add send latency to agent dashboard (@bboreham)\n\n- [BUGFIX] Do not immediately cancel context when creating a new trace\n  processor. This was preventing scrape_configs in traces from\n  functioning. (@lheinlen)\n\n- [BUGFIX] Sanitize autologged Loki labels by replacing invalid characters with underscores (@mapno)\n\n- [BUGFIX] Traces: remove extra line feed/spaces/tabs when reading password_file content (@nicoche)\n\n- [BUGFIX] Updated envsubst to v2.0.0-20210730161058-179042472c46. This version has a fix needed for escaping values\n  outside of variable substitutions. (@rlankfo)\n\n- [BUGFIX] Grafana Agent Operator should no longer delete resources matching\n  the names of the resources it manages. (@rfratto)\n\n- [BUGFIX] Grafana Agent Operator will now appropriately assign an\n  `app.kubernetes.io/managed-by=grafana-agent-operator` to all created\n  resources.\n\n- [CHANGE] Configuration API now returns 404 instead of 400 when attempting to get or delete a config\n  which does not exist. (@kgeckhart)\n\n- [CHANGE] The windows_exporter now disables the textfile collector by default. (@rfratto)\n\n- [CHANGE] **Breaking change** push_config is no longer supported in trace's config (@mapno)\n\n# v0.19.0 (2021-09-29)\n\nThis release has breaking changes. Please read [CHANGE] entries carefully and\nconsult the\n[upgrade guide](https://github.com/grafana/agent/blob/main/docs/upgrade-guide/_index.md)\nfor specific instructions.\n\n\n- [FEATURE] Added [Github exporter](https://github.com/infinityworks/github-exporter) integration. (@rgeyer)\n\n- [FEATURE] Add TLS config options for tempo `remote_write`s. (@mapno)\n\n- [FEATURE] Support autologging span attributes as log labels (@mapno)\n\n- [FEATURE] Put Tests requiring Network Access behind a -online flag (@flokli)\n\n- [FEATURE] Add logging support to the Grafana Agent Operator. (@rfratto)\n\n- [FEATURE] Add `operator-detach` command to agentctl to allow zero-downtime\n  upgrades when removing an Operator CRD. (@rfratto)\n\n- [ENHANCEMENT] The Grafana Agent Operator will now default to deploying\n  the matching release version of the Grafana Agent instead of v0.14.0.\n  (@rfratto)\n\n- [ENHANCEMENT] Update OTel dependency to v0.30.0 (@mapno)\n\n- [ENHANCEMENT] Allow reloading configuration using `SIGHUP` signal. (@tharun208)\n\n- [ENHANCEMENT] Add HOSTNAME environment variable to service file to allow for expanding\n  the $HOSTNAME variable in agent config.  (@dfrankel33)\n\n- [ENHANCEMENT] Update jsonnet-libs to 1.21 for Kubernetes 1.21+ compatability. (@MurzNN)\n\n- [ENHANCEMENT] Make method used to add k/v to spans in prom_sd processor\n  configurable. (@mapno)\n\n- [BUGFIX] Regex capture groups like `${1}` will now be kept intact when\n  using `-config.expand-env`. (@rfratto)\n\n- [BUGFIX] The directory of the logs positions file will now properly be created\n  on startup for all instances. (@rfratto)\n\n- [BUGFIX] The Linux system packages will now configure the grafana-agent user\n  to be a member of the adm and systemd-journal groups. This will allow logs to\n  read from journald and /var/log by default. (@rfratto)\n\n- [BUGFIX] Fix collecting filesystem metrics on Mac OS (darwin) in the\n  `node_exporter` integration default config. (@eamonryan)\n\n- [BUGFIX] Remove v0.0.0 flags during build with no explicit release tag (@mattdurham)\n\n- [BUGFIX] Fix issue with global scrape_interval changes not reloading integrations (@kgeckhart)\n\n- [BUGFIX] Grafana Agent Operator will now detect changes to referenced\n  ConfigMaps and Secrets and reload the Agent properly. (@rfratto)\n\n- [BUGFIX] Grafana Agent Operator's object label selectors will now use\n  Kubernetes defaults when undefined (i.e., default to nothing). (@rfratto)\n\n- [BUGFIX] Fix yaml marshalling tag for cert_file in kafka exporter agent config. (@rgeyer)\n\n- [BUGFIX] Fix warn-level logging of dropped targets. (@james-callahan)\n\n- [BUGFIX] Standardize scrape_interval to 1m in examples. (@mattdurham)\n\n- [CHANGE] Breaking change: reduced verbosity of tracing autologging\n  by not logging `STATUS_CODE_UNSET` status codes. (@mapno)\n\n- [CHANGE] Breaking change: Operator: rename Prometheus* CRDs to Metrics* and\n  Prometheus* fields to Metrics*. (@rfratto)\n\n- [CHANGE] Breaking change: Operator: CRDs are no longer referenced using a\n  hyphen in the name to be consistent with how Kubernetes refers to resources.\n  (@rfratto)\n\n- [CHANGE] Breaking change: `prom_instance` in the spanmetrics config is now\n  named `metrics_instance`. (@rfratto)\n\n- [DEPRECATION] The `loki` key at the root of the config file has been\n  deprecated in favor of `logs`. `loki`-named fields in `automatic_logging`\n  have been renamed accordinly: `loki_name` is now `logs_instance_name`,\n  `loki_tag` is now `logs_instance_tag`, and `backend: loki` is now\n  `backend: logs_instance`. (@rfratto)\n\n- [DEPRECATION] The `prometheus` key at the root of the config file has been\n  deprecated in favor of `metrics`. Flag names starting with `prometheus.` have\n  also been deprecated in favor of the same flags with the `metrics.` prefix.\n  Metrics prefixed with `agent_prometheus_` are now prefixed with\n  `agent_metrics_`. (@rfratto)\n\n- [DEPRECATION] The `tempo` key at the root of the config file has been\n  deprecated in favor of `traces`. (@mattdurham)\n\n# v0.18.4 (2021-09-14)\n\n- [BUGFIX] Fix info logging on windows. (@mattdurham)\n\n- [BUGFIX] Scraping service: Ensure that a reshard is scheduled every reshard\n  interval. (@rfratto)\n\n- [CHANGE] Add `agent_prometheus_configs_changed_total` metric to track instance\n  config events. (@rfratto)\n\n# v0.18.3 (2021-09-08)\n\n- [BUGFIX] Register missing metric for configstore consul request duration.\n  (@rfratto)\n\n- [BUGFIX] Logs should contain a caller field with file and line numbers again\n  (@kgeckhart)\n\n- [BUGFIX] In scraping service mode, the polling configuration refresh should\n  honor timeout. (@mattdurham)\n\n- [BUGFIX] In scraping service mode, the lifecycle reshard should happen using a\n  goroutine. (@mattdurham)\n\n- [BUGFIX] In scraping service mode, scraping service can deadlock when\n  reloading during join. (@mattdurham)\n\n- [BUGFIX] Scraping service: prevent more than one refresh from being queued at\n  a time. (@rfratto)\n\n# v0.18.2 (2021-08-12)\n\n- [BUGFIX] Honor the prefix and remove prefix from consul list results (@mattdurham)\n\n# v0.18.1 (2021-08-09)\n\n- [BUGFIX] Reduce number of consul calls when ran in scrape service mode (@mattdurham)\n\n# v0.18.0 (2021-07-29)\n\n- [FEATURE] Added [Github exporter](https://github.com/infinityworks/github-exporter) integration. (@rgeyer)\n\n- [FEATURE] Add support for OTLP HTTP trace exporting. (@mapno)\n\n- [ENHANCEMENT] Switch to drone for releases. (@mattdurham)\n\n- [ENHANCEMENT] Update postgres_exporter to a [branch of](https://github.com/grafana/postgres_exporter/tree/exporter-package-v0.10.0) v0.10.0\n\n- [BUGFIX]  Enabled flag is not being honored. (@mattdurham)\n\n# v0.17.0 (2021-07-15)\n\n- [FEATURE] Added [Kafka Lag exporter](https://github.com/davidmparrott/kafka_exporter)\n  integration. (@gaantunes)\n\n- [BUGFIX] Fix race condition that may occur and result in a panic when\n  initializing scraping service cluster. (@rfratto)\n\n# v0.16.1 (2021-06-22)\n\n- [BUGFIX] Fix issue where replaying a WAL caused incorrect metrics to be sent\n  over remote write. (@rfratto)\n\n# v0.16.0 (2021-06-17)\n\n- [FEATURE] (beta) A Grafana Agent Operator is now available. (@rfratto)\n\n- [ENHANCEMENT] Error messages when installing the Grafana Agent for Grafana\n  Cloud will now be shown. (@rfratto)\n\n- [BUGFIX] Fix a leak in the shared string interner introduced in v0.14.0.\n  This fix was made to a [dependency](https://github.com/grafana/prometheus/pull/21).\n  (@rfratto)\n\n- [BUGFIX] Fix issue where a target will fail to be scraped for the process lifetime\n  if that target had gone down for long enough that its series were removed from\n  the in-memory cache (2 GC cycles). (@rfratto)\n\n# v0.15.0 (2021-06-03)\n\nBREAKING CHANGE: Configuration of Tempo Autologging changed in this release.\nPlease review the [migration\nguide](./docs/migration-guide.md) for details.\n\n- [FEATURE] Add support for exemplars. (@mapno)\n\n- [ENHANCEMENT] Add the option to log to stdout instead of a Loki instance. (@joe-elliott)\n\n- [ENHANCEMENT] Update Cortex dependency to v1.8.0.\n\n- [ENHANCEMENT] Running the Agent as a DaemonSet with host_filter and role: pod\n  should no longer cause unnecessary load against the Kubernetes SD API.\n  (@rfratto)\n\n- [ENHANCEMENT] Update Prometheus to v2.27.0. (@mapno)\n\n- [ENHANCEMENT] Update Loki dependency to d88f3996eaa2. This is a non-release\n  build, and was needed to support exemplars. (@mapno)\n\n- [ENHANCEMENT] Update Cortex dependency to to d382e1d80eaf. This is a\n  non-release build, and was needed to support exemplars. (@mapno)\n\n- [BUGFIX] Host filter relabeling rules should now work. (@rfratto)\n\n- [BUGFIX] Fixed issue where span metrics where being reported with wrong time unit. (@mapno)\n\n- [CHANGE] Intentionally order tracing processors. (@joe-elliott)\n\n# v0.14.0 (2021-05-24)\n\nBREAKING CHANGE: This release has a breaking change for SigV4 support. Please\nread the release notes carefully and our [migration\nguide](./docs/migration-guide.md) to help migrate your configuration files to\nthe new format.\n\nBREAKING CHANGE: For security, the scraping service config API will reject\nconfigs that read credentials from disk to prevent malicious users from reading\nartbirary files and sending their contents over the network. The old behavior\ncan be achieved by enabling `dangerous_allow_reading_files` in the scraping\nservice config.\n\nAs of this release, functionality that is not recommended for production use\nand is expected to change will be tagged interchangably as \"experimental\" or\n\"beta.\"\n\n- [FEATURE] (beta) New integration: windows_exporter (@mattdurham)\n\n- [FEATURE] (beta) Grafana Agent Windows Installer is now included as a release\n  artifact. (@mattdurham)\n\n- [FEATURE] Official M1 Mac release builds will now be generated! Look for\n  `agent-darwin-arm64` and `agentctl-darwin-arm64` in the release assets.\n  (@rfratto)\n\n- [FEATURE] Add support for running as a Windows service (@mattdurham)\n\n- [FEATURE] (beta) Add /-/reload support. It is not recommended to invoke\n  `/-/reload` against the main HTTP server. Instead, two new command-line flags\n  have been added: `--reload-addr` and `--reload-port`. These will launch a\n  `/-/reload`-only HTTP server that can be used to safely reload the Agent's\n  state.  (@rfratto)\n\n- [FEATURE] Add a /-/config endpoint. This endpoint will return the current\n  configuration file with defaults applied that the Agent has loaded from disk.\n  (@rfratto)\n\n- [FEATURE] (beta) Support generating metrics and exposing them via a Prometheus exporter\n  from span data. (@yeya24)\n\n- [FEATURE] Tail-based sampling for tracing pipelines (@mapno)\n\n- [FEATURE] Added Automatic Logging feature for Tempo (@joe-elliott)\n\n- [FEATURE] Disallow reading files from within scraping service configs by\n  default. (@rfratto)\n\n- [FEATURE] Add remote write for span metrics (@mapno)\n\n- [ENHANCEMENT] Support compression for trace export. (@mdisibio)\n\n- [ENHANCEMENT] Add global remote_write configuration that is shared between all\n  instances and integrations. (@mattdurham)\n\n- [ENHANCEMENT] Go 1.16 is now used for all builds of the Agent. (@rfratto)\n\n- [ENHANCEMENT] Update Prometheus dependency to v2.26.0. (@rfratto)\n\n- [ENHANCEMENT] Upgrade `go.opentelemetry.io/collector` to v0.21.0 (@mapno)\n\n- [ENHANCEMENT] Add kafka trace receiver (@mapno)\n\n- [ENHANCEMENT] Support mirroring a trace pipeline to multiple backends (@mapno)\n\n- [ENHANCEMENT] Add  `headers` field in `remote_write` config for Tempo. `headers`\n  specifies HTTP headers to forward to the remote endpoint. (@alexbiehl)\n\n- [ENHANCEMENT] Add silent uninstall to Windows Uninstaller. (@mattdurham)\n\n- [BUGFIX] Native Darwin arm64 builds will no longer crash when writing metrics\n  to the WAL. (@rfratto)\n\n- [BUGFIX] Remote write endpoints that never function across the lifetime of the\n  Agent will no longer prevent the WAL from being truncated. (@rfratto)\n\n- [BUGFIX] Bring back FreeBSD support. (@rfratto)\n\n- [BUGFIX] agentctl will no longer leak WAL resources when retrieving WAL stats. (@rfratto)\n\n- [BUGFIX] Ensure defaults are applied to undefined sections in config file.\n  This fixes a problem where integrations didn't work if `prometheus:` wasn't\n  configured. (@rfratto)\n\n- [BUGFIX] Fixed issue where automatic logging double logged \"svc\". (@joe-elliott)\n\n- [CHANGE] The Grafana Cloud Agent has been renamed to the Grafana Agent.\n  (@rfratto)\n\n- [CHANGE] Instance configs uploaded to the Config Store API will no longer be\n  stored along with the global Prometheus defaults. This is done to allow\n  globals to be updated and re-apply the new global defaults to the configs from\n  the Config Store. (@rfratto)\n\n- [CHANGE] The User-Agent header sent for logs will now be\n  `GrafanaAgent/<version>` (@rfratto)\n\n- [CHANGE] Add `tempo_spanmetrics` namespace in spanmetrics (@mapno)\n\n- [DEPRECATION] `push_config` is now supplanted by `remote_block` and `batch`.\n  `push_config` will be removed in a future version (@mapno)\n\n# v0.13.1 (2021-04-09)\n\n- [BUGFIX] Validate that incoming scraped metrics do not have an empty label\n  set or a label set with duplicate labels, mirroring the behavior of\n  Prometheus. (@rfratto)\n\n# v0.13.0 (2021-02-25)\n\nThe primary branch name has changed from `master` to `main`. You may have to\nupdate your local checkouts of the repository to point at the new branch name.\n\n- [FEATURE] postgres_exporter: Support query_path and disable_default_metrics. (@rfratto)\n\n- [ENHANCEMENT] Support other architectures in installation script. (@rfratto)\n\n- [ENHANCEMENT] Allow specifying custom wal_truncate_frequency per integration.\n  (@rfratto)\n\n- [ENHANCEMENT] The SigV4 region can now be inferred using the shared config\n  (at `$HOME/.aws/config`) or environment variables (via `AWS_CONFIG`).\n  (@rfratto)\n\n- [ENHANCEMENT] Update Prometheus dependency to v2.25.0. (@rfratto)\n\n- [BUGFIX] Not providing an `-addr` flag for `agentctl config-sync` will no\n  longer report an error and will instead use the pre-existing default value.\n  (@rfratto)\n\n- [BUGFIX] Fixed a bug from v0.12.0 where the Loki installation script failed\n  because positions_directory was not set. (@rfratto)\n\n- [BUGFIX] (#400) Reduce the likelihood of dataloss during a remote_write-side\n  outage by increasing the default wal_truncation_frequency to 60m and preventing\n  the WAL from being truncated if the last truncation timestamp hasn't changed.\n  This change increases the size of the WAL on average, and users may configure\n  a lower wal_truncation_frequency to deliberately choose a smaller WAL over\n  write guarantees. (@rfratto)\n\n- [BUGFIX] (#368) Add the ability to read and serve HTTPS integration metrics when\n  given a set certificates (@mattdurham)\n\n# v0.12.0 (2021-02-05)\n\nBREAKING CHANGES: This release has two breaking changes in the configuration\nfile. Please read the release notes carefully and our\n[migration guide](./docs/migration-guide.md) to help migrate your configuration\nfiles to the new format.\n\n- [FEATURE] BREAKING CHANGE: Support for multiple Loki Promtail instances has\n  been added, using the same `configs` array used by the Prometheus subsystem.\n  (@rfratto)\n\n- [FEATURE] BREAKING CHANGE: Support for multiple Tempo instances has\n  been added, using the same `configs` array used by the Prometheus subsystem.\n  (@rfratto)\n\n- [FEATURE] Added [ElasticSearch exporter](https://github.com/justwatchcom/elasticsearch_exporter)\n  integration. (@colega)\n\n- [ENHANCEMENT] `.deb` and `.rpm` packages are now generated for all supported\n  architectures. The architecture of the AMD64 package in the filename has\n  been renamed to `amd64` to stay synchronized with the architecture name\n  presented from other release assets. (@rfratto)\n\n- [ENHANCEMENT] The `/agent/api/v1/targets` API will now include discovered labels\n  on the target pre-relabeling in a `discovered_labels` field. (@rfratto)\n\n- [ENHANCEMENT] Update Loki to 59a34f9867ce. This is a non-release build, and was needed\n  to support multiple Loki instances. (@rfratto)\n\n- [ENHANCEMENT] Scraping service: Unhealthy Agents in the ring will no longer\n  cause job distribution to fail. (@rfratto)\n\n- [ENHANCEMENT] Scraping service: Cortex ring metrics (prefixed with\n  cortex_ring_) will now be registered for tracking the state of the hash\n  ring. (@rfratto)\n\n- [ENHANCEMENT] Scraping service: instance config ownership is now determined by\n  the hash of the instance config name instead of the entire config. This means\n  that updating a config is guaranteed to always hash to the same Agent,\n  reducing the number of metrics gaps. (@rfratto)\n\n- [ENHANCEMENT] Only keep a handful of K8s API server metrics by default to reduce\n  default active series usage. (@hjet)\n\n- [ENHANCEMENT] Go 1.15.8 is now used for all distributions of the Agent.\n  (@rfratto)\n\n- [BUGFIX] `agentctl config-check` will now work correctly when the supplied\n  config file contains integrations. (@hoenn)\n\n# v0.11.0 (2021-01-20)\n\n- [FEATURE] ARMv6 builds of `agent` and `agentctl` will now be included in\n  releases to expand Agent support to cover all models of Raspberry Pis.\n  ARMv6 docker builds are also now available.\n  (@rfratto)\n\n- [FEATURE] Added `config-check` subcommand for `agentctl` that can be used\n  to validate Agent configuration files before attempting to load them in the\n  `agent` itself. (@56quarters)\n\n- [ENHANCEMENT] A sigv4 install script for Prometheus has been added. (@rfratto)\n\n- [ENHANCEMENT] NAMESPACE may be passed as an environment variable to the\n  Kubernetes install scripts to specify an installation namespace. (@rfratto)\n\n- [BUGFIX] The K8s API server scrape job will use the API server Service name\n  when resolving IP addresses for Prometheus service discovery using the\n  \"Endpoints\" role. (@hjet)\n\n- [BUGFIX] The K8s manifests will no longer include the `default/kubernetes` job\n  twice in both the DaemonSet and the Deployment. (@rfratto)\n\n# v0.10.0 (2021-01-13)\n\n- [FEATURE] Prometheus `remote_write` now supports SigV4 authentication using\n  the [AWS default credentials\n  chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html).\n  This enables the Agent to send metrics to Amazon Managed Prometheus without\n  needing the [SigV4 Proxy](https://github.com/awslabs/aws-sigv4-proxy).\n  (@rfratto)\n\n- [ENHANCEMENT] Update `redis_exporter` to v1.15.0. (@rfratto)\n\n- [ENHANCEMENT] `memcached_exporter` has been updated to v0.8.0. (@rfratto)\n\n- [ENHANCEMENT] `process-exporter` has been updated to v0.7.5. (@rfratto)\n\n- [ENHANCEMENT] `wal_cleanup_age` and `wal_cleanup_period` have been added to the\n  top-level Prometheus configuration section. These settings control how Write Ahead\n  Logs (WALs) that are not associated with any instances are cleaned up. By default,\n  WALs not associated with an instance that have not been written in the last 12 hours\n  are eligible to be cleaned up. This cleanup can be disabled by setting `wal_cleanup_period`\n  to `0`. (#304) (@56quarters)\n\n- [ENHANCEMENT] Configuring logs to read from the systemd journal should now\n  work on journals that use +ZSTD compression. (@rfratto)\n\n- [BUGFIX] Integrations will now function if the HTTP listen address was set to\n  a value other than the default. ([#206](https://github.com/grafana/agent/issues/206)) (@mattdurham)\n\n- [BUGFIX] The default Loki installation will now be able to write its positions\n  file. This was prevented by accidentally writing to a readonly volume mount.\n  (@rfratto)\n\n# v0.9.1 (2021-01-04)\n\n- [ENHANCEMENT] agentctl will now be installed by the rpm and deb packages as\n  `grafana-agentctl`. (@rfratto)\n\n# v0.9.0 (2020-12-10)\n\n- [FEATURE] Add support to configure TLS config for the Tempo exporter to use\n  insecure_skip_verify to disable TLS chain verification. (@bombsimon)\n\n- [FEATURE] Add `sample-stats` to `agentctl` to search the WAL and return a\n  summary of samples of series matching the given label selector. (@simonswine)\n\n- [FEATURE] New integration:\n  [postgres_exporter](https://github.com/wrouesnel/postgres_exporter) (@rfratto)\n\n- [FEATURE] New integration:\n  [statsd_exporter](https://github.com/prometheus/statsd_exporter) (@rfratto)\n\n- [FEATURE] New integration:\n  [consul_exporter](https://github.com/prometheus/consul_exporter) (@rfratto)\n\n- [FEATURE] Add optional environment variable substitution of configuration\n  file. (@dcseifert)\n\n- [ENHANCEMENT] `min_wal_time` and `max_wal_time` have been added to the\n  instance config settings, guaranteeing that data in the WAL will exist for at\n  least `min_wal_time` and will not exist for longer than `max_wal_time`. This\n  change will increase the size of the WAL slightly but will prevent certain\n  scenarios where data is deleted before it is sent. To revert back to the old\n  behavior, set `min_wal_time` to `0s`. (@rfratto)\n\n- [ENHANCEMENT] Update `redis_exporter` to v1.13.1. (@rfratto)\n\n- [ENHANCEMENT] Bump OpenTelemetry-collector dependency to v0.16.0. (@bombsimon)\n\n- [BUGFIX] Fix issue where the Tempo example manifest could not be applied\n  because the port names were too long. (@rfratto)\n\n- [BUGFIX] Fix issue where the Agent Kubernetes manifests may not load properly\n  on AKS. (#279) (@rfratto)\n\n- [CHANGE] The User-Agent header sent for logs will now be\n  `GrafanaCloudAgent/<version>` (@rfratto)\n\n# v0.8.0 (2020-11-06)\n\n- [FEATURE] New integration: [dnsamsq_exporter](https://github.com/google/dnsamsq_exporter)\n  (@rfratto).\n\n- [FEATURE] New integration: [memcached_exporter](https://github.com/prometheus/memcached_exporter)\n  (@rfratto).\n\n- [ENHANCEMENT] Add `<integration name>_build_info` metric to all integrations.\n  The build info displayed will match the build information of the Agent and\n  *not* the embedded exporter. This metric is used by community dashboards, so\n  adding it to the Agent increases compatibility with existing dashboards that\n  depend on it existing. (@rfratto)\n\n- [ENHANCEMENT] Bump OpenTelemetry-collector dependency to 0.14.0 (@joe-elliott)\n\n- [BUGFIX] Error messages when retrieving configs from the KV store will\n  now be logged, rather than just logging a generic message saying that\n  retrieving the config has failed. (@rfratto)\n\n# v0.7.2 (2020-10-29)\n\n- [ENHANCEMENT] Bump Prometheus dependency to 2.21. (@rfratto)\n\n- [ENHANCEMENT] Bump OpenTelemetry-collector dependency to 0.13.0 (@rfratto)\n\n- [ENHANCEMENT] Bump Promtail dependency to 2.0. (@rfratto)\n\n- [ENHANCEMENT] Enhance host_filtering mode to support targets from Docker Swarm\n  and Consul. Also, add a `host_filter_relabel_configs` to that will apply relabeling\n  rules for determining if a target should be dropped. Add a documentation\n  section explaining all of this in detail. (@rfratto)\n\n- [BUGFIX] Fix deb package prerm script so that it stops the agent on package removal. (@jdbaldry)\n\n- [BUGFIX] Fix issue where the `push_config` for Tempo field was expected to be\n  `remote_write`. `push_config` now works as expected. (@rfratto)\n\n# v0.7.1 (2020-10-23)\n\n- [BUGFIX] Fix issue where ARM binaries were not published with the GitHub\n  release.\n\n# v0.7.0 (2020-10-23)\n\n- [FEATURE] Added Tracing Support. (@joe-elliott)\n\n- [FEATURE] Add RPM and deb packaging. (@jdbaldry) (@simon6372)\n\n- [FEATURE] arm64 and arm/v7 Docker containers and release builds are now\n  available for `agent` and `agentctl`. (@rfratto)\n\n- [FEATURE] Add `wal-stats` and `target-stats` tooling to `agentctl` to discover\n  WAL and cardinality issues. (@rfratto)\n\n- [FEATURE] [mysqld_exporter](https://github.com/prometheus/mysqld_exporter) is\n  now embedded and available as an integration. (@rfratto)\n\n- [FEATURE] [redis_exporter](https://github.com/oliver006/redis_exporter) is\n  now embedded and available as an integration. (@dafydd-t)\n\n- [ENHANCEMENT] Resharding the cluster when using the scraping service mode now\n  supports timeouts through `reshard_timeout`. The default value is `30s.` This\n  timeout applies to cluster-wide reshards (performed when joining and leaving\n  the cluster) and local reshards (done on the `reshard_interval`). (@rfratto)\n\n- [BUGFIX] Fix issue where integrations crashed with instance_mode was set to\n  `distinct` (@rfratto)\n\n- [BUGFIX] Fix issue where the `agent` integration did not work on Windows\n  (@rfratto).\n\n- [BUGFIX] Support URL-encoded paths in the scraping service API. (@rfratto)\n\n- [BUGFIX] The instance label written from replace_instance_label can now be\n  overwritten with relabel_configs. This bugfix slightly modifies the behavior\n  of what data is stored. The final instance label will now be stored in the WAL\n  rather than computed by remote_write. This change should not negatively effect\n  existing users. (@rfratto)\n\n# v0.6.1 (2020-04-11)\n\n- [BUGFIX] Fix issue where build information was empty when running the Agent\n  with --version. (@rfratto)\n\n- [BUGFIX] Fix issue where updating a config in the scraping service may fail to\n  pick up new targets. (@rfratto)\n\n- [BUGFIX] Fix deadlock that slowly prevents the Agent from scraping targets at\n  a high scrape volume. (@rfratto)\n\n# v0.6.0 (2020-09-04)\n\n- [FEATURE] The Grafana Agent can now collect logs and send to Loki. This\n  is done by embedding Promtail, the official Loki log collection client.\n  (@rfratto)\n\n- [FEATURE] Integrations can now be enabled without scraping. Set\n  scrape_integrations to `false` at the `integrations` key or within the\n  specific integration you don't want to scrape. This is useful when another\n  Agent or Prometheus server will scrape the integration. (@rfratto)\n\n- [FEATURE] [process-exporter](https://github.com/ncabatoff/process-exporter) is\n  now embedded as `process_exporter`. The hypen has been changed to an\n  underscore in the config file to retain consistency with `node_exporter`.\n  (@rfratto)\n\n- [ENHANCEMENT] A new config option, `replace_instance_label`, is now available\n  for use with integrations. When this is true, the instance label for all\n  metrics coming from an integration will be replaced with the machine's\n  hostname rather than 127.0.0.1. (@rfratto)\n\n- [EHANCEMENT] The embedded Prometheus version has been updated to 2.20.1.\n  (@rfratto, @gotjosh)\n\n- [ENHANCEMENT] The User-Agent header written by the Agent when remote_writing\n  will now be `GrafanaCloudAgent/<Version>` instead of `Prometheus/<Prometheus Version>`.\n  (@rfratto)\n\n- [ENHANCEMENT] The subsystems of the Agent (`prometheus`, `loki`) are now made\n  optional. Enabling integrations also implicitly enables the associated\n  subsystem. For example, enabling the `agent` or `node_exporter` integration will\n  force the `prometheus` subsystem to be enabled.  (@rfratto)\n\n- [BUGFIX] The documentation for Tanka configs is now correct. (@amckinley)\n\n- [BUGFIX] Minor corrections and spelling issues have been fixed in the Overview\n  documentation. (@amckinley)\n\n- [BUGFIX] The new default of `shared` instances mode broke the metric value for\n  `agent_prometheus_active_configs`, which was tracking the number of combined\n  configs (i.e., number of launched instances). This metric has been fixed and\n  a new metric, `agent_prometheus_active_instances`, has been added to track\n  the numbger of launched instances. If instance sharing is not enabled, both\n  metrics will share the same value. (@rfratto)\n\n- [BUGFIX] The Configs API will now disallow two instance configs having\n  multiple `scrape_configs` with the same `job_name`. THIS IS A BREAKING CHANGE.\n  This was needed for the instance sharing mode, where combined instances may\n  have duplicate `job_names` across their `scrape_configs`. This brings the\n  scraping service more in line with Prometheus, where `job_names` must globally\n  be unique. This change also disallows concurrent requests to the put/apply\n  config API endpoint to prevent a race condition of two conflicting configs\n  being applied at the same time. (@rfratto)\n\n- [BUGFIX] `remote_write` names in a group will no longer be copied from the\n  remote_write names of the first instance in the group. Rather, all\n  remote_write names will be generated based on the first 6 characters of the\n  group hash and the first six characters of the remote_write hash. (@rfratto)\n\n- [BUGFIX] Fix a panic that may occur during shutdown if the WAL is closed in\n  the middle of the WAL being truncated. (@rfratto)\n\n- [DEPRECATION] `use_hostname_label` is now supplanted by\n  `replace_instance_label`. `use_hostname_label` will be removed in a future\n  version. (@rfratto)\n\n# v0.5.0 (2020-08-12)\n\n- [FEATURE] A [scrape targets API](https://github.com/grafana/agent/blob/main/docs/api.md#list-current-scrape-targets)\n  has been added to show every target the Agent is currently scraping, when it\n  was last scraped, how long it took to scrape, and errors from the last scrape,\n  if any. (@rfratto)\n\n- [FEATURE]  \"Shared Instance Mode\" is the new default mode for spawning\n  Prometheus instances, and will improve CPU and memory usage for users of\n  integrations and the scraping service. (@rfratto)\n\n- [ENHANCEMENT] Memory stability and utilization of the WAL has been improved,\n  and the reported number of active series in the WAL will stop double-counting\n  recently churned series. (@rfratto)\n\n- [ENHANCEMENT] Changing scrape_configs and remote_write configs for an instance\n  will now be dynamically applied without restarting the instance. This will\n  result in less missing metrics for users of the scraping service that change a\n  config. (@rfratto)\n\n- [ENHANCEMENT] The Tanka configuration now uses k8s-alpha. (@duologic)\n\n- [BUGFIX] The Tanka configuration will now also deploy a single-replica\n  deployment specifically for scraping the Kubernetes API. This deployment acts\n  together with the Daemonset to scrape the full cluster and the control plane.\n  (@gotjosh)\n\n- [BUGFIX] The node_exporter filesystem collector will now work on Linux systems\n  without needing to manually set the blocklist and allowlist of filesystems.\n  (@rfratto)\n\n# v0.4.0 (2020-06-18)\n\n- [FEATURE] Support for integrations has been added. Integrations can be any\n  embedded tool, but are currently used for embedding exporters and generating\n  scrape configs. (@rfratto)\n\n- [FEATURE] node_exporter has been added as an integration. This is the full\n  version of node_exporter with the same configuration options. (@rfratto)\n\n- [FEATURE] An Agent integration that makes the Agent automatically scrape\n  itself has been added. (@rfratto)\n\n- [ENHANCEMENT] The WAL can now be truncated if running the Agent without any\n  remote_write endpoints. (@rfratto)\n\n- [ENHANCEMENT] Clarify server_config description in documentation. (@rfratto)\n\n- [ENHANCEMENT] Clarify wal_truncate_frequency and remote_flush_deadline in\n  documentation. (@rfratto)\n\n- [ENHANCEMENT] Document /agent/api/v1/instances endpoint (@rfratto)\n\n- [ENHANCEMENT] Be explicit about envsubst requirement for Kubernetes install\n  script. (@robx)\n\n- [BUGFIX] Prevent the Agent from crashing when a global Prometheus config\n  stanza is not provided. (@robx)\n\n- [BUGFIX] Enable agent host_filter in the Tanka configs, which was disabled by\n  default by mistake. (@rfratto)\n\n# v0.3.2 (2020-05-29)\n\n- [FEATURE] Tanka configs that deploy the scraping service mode are now\n  available (@rfratto)\n\n- [FEATURE] A k3d example has been added as a counterpart to the docker-compose\n  example. (@rfratto)\n\n- [ENHANCEMENT] Labels provided by the default deployment of the Agent\n  (Kubernetes and Tanka) have been changed to align with the latest changes to\n  grafana/jsonnet-libs. The old `instance` label is now called `pod`, and the\n  new `instance` label is unique. A `container` label has also been added. The\n  Agent mixin has been subsequently updated to also incorporate these label\n  changes. (@rfratto)\n\n- [ENHANCEMENT] The `remote_write` and `scrape_config` sections now share the\n  same validations as Prometheus (@rfratto)\n\n- [ENHANCEMENT] Setting `wal_truncation_frequency` to less than the scrape\n  interval is now disallowed (@rfratto)\n\n- [BUGFIX] A deadlock in scraping service mode when updating a config that\n  shards to the same node has been fixed (@rfratto)\n\n- [BUGFIX] `remote_write` config stanzas will no longer ignore `password_file`\n  (@rfratto)\n\n- [BUGFIX] `scrape_config` client secrets (e.g., basic auth, bearer token,\n  `password_file`) will now be properly retained in scraping service mode\n  (@rfratto)\n\n- [BUGFIX] Labels for CPU, RX, and TX graphs in the Agent Operational dashboard\n  now correctly show the pod name of the Agent instead of the exporter name.\n  (@rfratto)\n\n# v0.3.1 (2020-05-20)\n\n- [BUGFIX] A typo in the Tanka configs and Kubernetes manifests that prevents\n  the Agent launching with v0.3.0 has been fixed (@captncraig)\n\n- [BUGFIX] Fixed a bug where Tanka mixins could not be used due to an issue with\n  the folder placement enhancement (@rfratto)\n\n- [ENHANCEMENT] `agentctl` and the config API will now validate that the YAML\n  they receive are valid instance configs. (@rfratto)\n\n- [FEATURE] The Agent has upgraded its vendored Prometheus to v2.18.1\n  (@gotjosh, @rfratto)\n\n# v0.3.0 (2020-05-13)\n\n- [FEATURE] A third operational mode called \"scraping service mode\" has been\n  added. A KV store is used to store instance configs which are distributed\n  amongst a clustered set of Agent processes, dividing the total scrape load\n  across each agent. An API is exposed on the Agents to list, create, update,\n  and delete instance configurations from the KV store. (@rfratto)\n\n- [FEATURE] An \"agentctl\" binary has been released to interact with the new\n  instance config management API created by the \"scraping service mode.\"\n  (@rfratto, @hoenn)\n\n- [FEATURE] The Agent now includes readiness and healthiness endpoints.\n  (@rfratto)\n\n- [ENHANCEMENT] The YAML files are now parsed strictly and an invalid YAML will\n  generate an error at runtime. (@hoenn)\n\n- [ENHANCEMENT] The default build mode for the Docker containers is now release,\n  not debug. (@rfratto)\n\n- [ENHANCEMENT] The Grafana Agent Tanka Mixins now are placed in an \"Agent\"\n  folder within Grafana. (@cyriltovena)\n\n# v0.2.0 (2020-04-09)\n\n- [FEATURE] The Prometheus remote write protocol will now send scraped metadata (metric name, help, type and unit). This results in almost negligent bytes sent increase as metadata is only sent every minute. It is on by default. (@gotjosh)\n\n  These metrics are available to monitor metadata being sent:\n    - `prometheus_remote_storage_succeeded_metadata_total`\n    - `prometheus_remote_storage_failed_metadata_total`\n    - `prometheus_remote_storage_retried_metadata_total`\n    - `prometheus_remote_storage_sent_batch_duration_seconds` and\n      `prometheus_remote_storage_sent_bytes_total` have a new label \u201ctype\u201d with\n      the values of `metadata` or `samples`.\n\n- [FEATURE] The Agent has upgraded its vendored Prometheus to v2.17.1 (@rfratto)\n\n- [BUGFIX] Invalid configs passed to the agent will now stop the process after they are logged as invalid; previously the Agent process would continue. (@rfratto)\n\n- [BUGFIX] Enabling host_filter will now allow metrics from node role Kubernetes service discovery to be scraped properly (e.g., cAdvisor, Kubelet). (@rfratto)\n\n# v0.1.1 (2020-03-16)\n\n- Nits in documentation (@sh0rez)\n- Fix various dashboard mixin problems from v0.1.0 (@rfratto)\n- Pass through release tag to `docker build` (@rfratto)\n\n# v0.1.0 (2020-03-16)\n\nFirst (beta) release!\n\nThis release comes with support for scraping Prometheus metrics and\nsharding the agent through the presence of a `host_filter` flag within the\nAgent configuration file.\n\nNote that enabling the `host_filter` flag currently works best when using our\npreferred Kubernetes deployment, as it deploys the agent as a DaemonSet.\n", "+++\ntitle = \"node_exporter_config\"\n+++\n\n# node_exporter_config\n\nThe `node_exporter_config` block configures the `node_exporter` integration,\nwhich is an embedded version of\n[`node_exporter`](https://github.com/prometheus/node_exporter)\nand allows for collecting metrics from the UNIX system that `node_exporter` is\nrunning on. It provides a significant amount of collectors that are responsible\nfor monitoring various aspects of the host system.\n\nNote that if running the Agent in a container, you will need to bind mount\nfolders from the host system so the integration can monitor them. You can use\nthe example below, making sure to replace `/path/to/config.yaml` with a path on\nyour host machine where an Agent configuration file is:\n\n```\ndocker run \\\n  --net=\"host\" \\\n  --pid=\"host\" \\\n  --cap-add=SYS_TIME \\\n  -v \"/:/host/root:ro,rslave\" \\\n  -v \"/sys:/host/sys:ro,rslave\" \\\n  -v \"/proc:/host/proc:ro,rslave\" \\\n  -v /tmp/agent:/etc/agent \\\n  -v /path/to/config.yaml:/etc/agent-config/agent.yaml \\\n  grafana/agent:v0.21.2 \\\n  --config.file=/etc/agent-config/agent.yaml\n```\n\nUse this configuration file for testing out `node_exporter` support, replacing\nthe `remote_write` settings with settings appropriate for you:\n\n```yaml\nserver:\n  log_level: info\n  http_listen_port: 12345\n\nmetrics:\n  wal_directory: /tmp/agent\n  global:\n    scrape_interval: 15s\n    remote_write:\n    - url: https://prometheus-us-central1.grafana.net/api/prom/push\n      basic_auth:\n        username: user-id\n        password: api-token\n\nintegrations:\n  node_exporter:\n    enabled: true\n    rootfs_path: /host/root\n    sysfs_path: /host/sys\n    procfs_path: /host/proc\n```\n\nFor running on Kubernetes, ensure to set the equivalent mounts and capabilities\nthere as well:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: agent\nspec:\n  containers:\n  - image: grafana/agent:v0.21.2\n    name: agent\n    args:\n    - --config.file=/etc/agent-config/agent.yaml\n    securityContext:\n      capabilities:\n        add: [\"SYS_TIME\"]\n      priviliged: true\n      runAsUser: 0\n    volumeMounts:\n    - name: rootfs\n      mountPath: /host/root\n      readOnly: true\n    - name: sysfs\n      mountPath: /host/sys\n      readOnly: true\n    - name: procfs\n      mountPath: /host/proc\n      readOnly: true\n  hostPID: true\n  hostNetwork: true\n  dnsPolicy: ClusterFirstWithHostNet\n  volumes:\n  - name: rootfs\n    hostPath:\n      path: /\n  - name: sysfs\n    hostPath:\n      path: /sys\n  - name: procfs\n    hostPath:\n      path: /proc\n```\n\nThe manifest and Tanka configs provided by this repository do not have the\nmounts or capabilities required for running this integration.\n\nSome collectors only work on specific operating systems, documented in the\ntable below. Enabling a collector that is not supported by the operating system\nthe Agent is running on is a no-op.\n\n| Name             | Description | OS | Enabled by default |\n| ---------------- | ----------- | -- | ------------------ |\n| arp              | Exposes ARP statistics from /proc/net/arp. | Linux | yes |\n| bcache           | Exposes bcache statistics from /sys/fs/bcache. | Linux | yes |\n| bonding          | Exposes the number of configured and active slaves of Linux bonding interfaces. | Linux | yes |\n| boottime         | Exposes system boot time derived from the kern.boottime sysctl. | Darwin, Dragonfly, FreeBSD, NetBSD, OpenBSD, Solaris | yes |\n| btrfs            | Exposes statistics on btrfs. | Linux | yes |\n| buddyinfo        | Exposes statistics of memory fragments as reported by /proc/buddyinfo. | Linux | no |\n| conntrack        | Shows conntrack statistics (does nothing if no /proc/sys/net/netfilter/ present). | Linux | yes |\n| cpu              | Exposes CPU statistics. | Darwin, Dragonfly, FreeBSD, Linux, Solaris | yes |\n| cpufreq          | Exposes CPU frequency statistics. | Linux, Solaris | yes |\n| devstat          | Exposes device statistics. | Dragonfly, FreeBSD | no |\n| diskstats        | Exposes disk I/O statistics. | Darwin, Linux, OpenBSD | yes |\n| drbd             | Exposes Distributed Replicated Block Device statistics (to version 8.4). | Linux | no |\n| edac             | Exposes error detection and correction statistics. | Linux | yes |\n| entropy          | Exposes available entropy. | Linux | yes |\n| exec             | Exposes execution statistics. | Dragonfly, FreeBSD | yes |\n| filefd           | Exposes file descriptor statistics from /proc/sys/fs/file-nr. | Linux | yes |\n| filesystem       | Exposes filesystem statistics, such as disk space used. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD | yes |\n| hwmon            | Exposes hardware monitoring and sensor data from /sys/class/hwmon. | Linux | yes |\n| infiniband       | Exposes network statistics specific to InfiniBand and Intel OmniPath configurations. | Linux | yes |\n| interrupts       | Exposes detailed interrupts statistics. | Linux, OpenBSD | no |\n| ipvs             | Exposes IPVS status from /proc/net/ip_vs and stats from /proc/net/ip_vs_stats. | Linux | yes |\n| ksmd             | Exposes kernel and system statistics from /sys/kernel/mm/ksm. | Linux | no |\n| loadavg          | Exposes load average. | Darwin, Dragonfly, FreeBSD, Linux, NetBSD, OpenBSD, Solaris | yes |\n| logind           | Exposes session counts from logind. | Linux | no |\n| mdadm            | Exposes statistics about devices in /proc/mdstat (does nothing if no /proc/mdstat present). | Linux | yes |\n| meminfo          | Exposes memory statistics. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD | yes |\n| meminfo_numa     | Exposes memory statistics from /proc/meminfo_numa. | Linux | no |\n| mountstats       | Exposes filesystem statistics from /proc/self/mountstats. Exposes detailed NFS client statistics. | Linux | no |\n| netclass         | Exposes network interface info from /sys/class/net. | Linux | yes |\n| netdev           | Exposes network interface statistics such as bytes transferred. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD | yes |\n| netstat          | Exposes network statistics from /proc/net/netstat. This is the same information as netstat -s. | Linux | yes |\n| nfs              | Exposes NFS client statistics from /proc/net/rpc/nfs. This is the same information as nfsstat -c. | Linux | yes |\n| nfsd             | Exposes NFS kernel server statistics from /proc/net/rpc/nfsd. This is the same information as nfsstat -s. | Linux | yes |\n| ntp              | Exposes local NTP daemon helath to check time. | any | no |\n| perf             | Exposes perf based metrics (Warning: Metrics are dependent on kernel configuration and settings). | Linux | no |\n| powersupplyclass | Collects information on power supplies. | any | yes |\n| pressure         | Exposes pressure stall statistics from /proc/pressure/. | Linux (kernel 4.20+ and/or CONFIG_PSI) | yes |\n| processes        | Exposes aggregate process statistics from /proc. | Linux | no |\n| qdisc            | Exposes queuing discipline statistics. | Linux | no |\n| rapl             | Exposes various statistics from /sys/class/powercap. | Linux | yes |\n| runit            | Exposes service status from runit. | any | no |\n| schedstat        | Exposes task scheduler statistics from /proc/schedstat. | Linux | yes |\n| sockstat         | Exposes various statistics from /proc/net/sockstat. | Linux | yes |\n| softnet          | Exposes statistics from /proc/net/softnet_stat. | Linux | yes |\n| stat             | Exposes various statistics from /proc/stat. This includes boot time, forks and interrupts. | Linux | yes |\n| supervisord      | Exposes service status from supervisord. | any | no |\n| systemd          | Exposes service and system status from systemd. | Linux | no |\n| tcpstat          | Exposes TCP connection status information from /proc/net/tcp and /proc/net/tcp6. (Warning: the current version has potential performance issues in high load situations). | Linux | no |\n| textfile         | Collects metrics from files in a directory matching the filename pattern *.prom. The files must be using the text format defined here: https://prometheus.io/docs/instrumenting/exposition_formats/ | any | yes |\n| thermal_zone     | Exposes thermal zone & cooling device statistics from /sys/class/thermal. | Linux | yes |\n| time             | Exposes the current system time. | any | yes |\n| timex            | Exposes selected adjtimex(2) system call stats. | Linux | yes |\n| udp_queues       | Exposes UDP total lengths of the rx_queue and tx_queue from /proc/net/udp and /proc/net/udp6. | Linux | yes |\n| uname            | Exposes system information as provided by the uname system call. | Darwin, FreeBSD, Linux, OpenBSD | yes |\n| vmstat           | Exposes statistics from /proc/vmstat. | Linux | yes |\n| wifi             | Exposes WiFi device and station statistics. | Linux | no |\n| xfs              | Exposes XFS runtime statistics. | Linux (kernel 4.4+) | yes |\n| zfs              | Exposes ZFS performance statistics. | Linux, Solaris | yes |\n\n\n```yaml\n  # Enables the node_exporter integration, allowing the Agent to automatically\n  # collect system metrics from the host UNIX system.\n  [enabled: <boolean> | default = false]\n\n  # Sets an explicit value for the instance label when the integration is\n  # self-scraped. Overrides inferred values.\n  #\n  # The default value for this integration is inferred from the agent hostname\n  # and HTTP listen port, delimited by a colon.\n  [instance: <string>]\n\n  # Automatically collect metrics from this integration. If disabled,\n  # the node_exporter integration will be run but not scraped and thus not remote-written. Metrics for the\n  # integration will be exposed at /integrations/node_exporter/metrics and can\n  # be scraped by an external process.\n  [scrape_integration: <boolean> | default = <integrations_config.scrape_integrations>]\n\n  # How often should the metrics be collected? Defaults to\n  # prometheus.global.scrape_interval.\n  [scrape_interval: <duration> | default = <global_config.scrape_interval>]\n\n  # The timtout before considering the scrape a failure. Defaults to\n  # prometheus.global.scrape_timeout.\n  [scrape_timeout: <duration> | default = <global_config.scrape_timeout>]\n\n  # Allows for relabeling labels on the target.\n  relabel_configs:\n    [- <relabel_config> ... ]\n\n  # Relabel metrics coming from the integration, allowing to drop series\n  # from the integration that you don't care about.\n  metric_relabel_configs:\n    [ - <relabel_config> ... ]\n\n  # How frequent to truncate the WAL for this integration.\n  [wal_truncate_frequency: <duration> | default = \"60m\"]\n\n  # Monitor the exporter itself and include those metrics in the results.\n  [include_exporter_metrics: <boolean> | default = false]\n\n  # Optionally defines the the list of enabled-by-default collectors.\n  # Anything not provided in the list below will be disabled by default,\n  # but requires at least one element to be treated as defined.\n  #\n  # This is useful if you have a very explicit set of collectors you wish\n  # to run.\n  set_collectors:\n    - [<string>]\n\n  # Additional collectors to enable on top of the default set of enabled\n  # collectors or on top of the list provided by set_collectors.\n  #\n  # This is useful if you have a few collectors you wish to run that are\n  # not enabled by default, but do not want to explicitly provide an entire\n  # list through set_collectors.\n  enable_collectors:\n    - [<string>]\n\n  # Additional collectors to disable on top of the default set of disabled\n  # collectors. Takes precedence over enable_collectors.\n\n  # Additional collectors to disable from the set of enabled collectors.\n  # Takes precedence over enabled_collectors.\n  #\n  # This is useful if you have a few collectors you do not want to run that\n  # are enabled by default, but do not want to explicitly provide an entire\n  # list through set_collectors.\n  disable_collectors:\n    - [<string>]\n\n  # procfs mountpoint.\n  [procfs_path: <string> | default = \"/proc\"]\n\n  # sysfs mountpoint.\n  [sysfs_path: <string> | default = \"/sys\"]\n\n  # rootfs mountpoint. If running in docker, the root filesystem of the host\n  # machine should be mounted and this value should be changed to the mount\n  # directory.\n  [rootfs_path: <string> | default = \"/\"]\n\n  # Enable the cpu_info metric for the cpu collector.\n  [enable_cpu_info_metric: <boolean> | default = true]\n\n  # Regexmp of devices to ignore for diskstats.\n  [diskstats_ignored_devices: <string> | default = \"^(ram|loop|fd|(h|s|v|xv)d[a-z]|nvme\\\\d+n\\\\d+p)\\\\d+$\"]\n\n  # Regexp of mount points to ignore for filesystem collector.\n  [filesystem_ignored_mount_points: <string> | default = \"^/(dev|proc|sys|var/lib/docker/.+)($|/)\"]\n\n  # Regexp of filesystem types to ignore for filesystem collector.\n  [filesystem_ignored_fs_types: <string> | default = \"^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"]\n\n  # NTP server to use for ntp collector\n  [ntp_server: <string> | default = \"127.0.0.1\"]\n\n  # NTP protocol version\n  [ntp_protocol_version: <int> | default = 4]\n\n  # Certify that the server address is not a public ntp server.\n  [ntp_server_is_local: <boolean> | default = false]\n\n  # IP TTL to use wile sending NTP query.\n  [ntp_ip_ttl: <int> | default = 1]\n\n  # Max accumulated distance to the root.\n  [ntp_max_distance: <duration> | default = \"3466080us\"]\n\n  # Offset between local clock and local ntpd time to tolerate.\n  [ntp_local_offset_tolerance: <duration> | default = \"1ms\"]\n\n  # Regexp of net devices to ignore for netclass collector.\n  [netclass_ignored_devices: <string> | default = \"^$\"]\n\n  # Regexp of net devices to blacklist (mutually exclusive with whitelist)\n  [netdev_device_blacklist: <string> | default = \"\"]\n\n  # Regexp of net devices to whitelist (mutually exclusive with blacklist)\n  [netdev_device_whitelist: <string> | default = \"\"]\n\n  # Regexp of fields to return for netstat collector.\n  [netstat_fields: <string> | default = \"^(.*_(InErrors|InErrs)|Ip_Forwarding|Ip(6|Ext)_(InOctets|OutOctets)|Icmp6?_(InMsgs|OutMsgs)|TcpExt_(Listen.*|Syncookies.*|TCPSynRetrans)|Tcp_(ActiveOpens|InSegs|OutSegs|PassiveOpens|RetransSegs|CurrEstab)|Udp6?_(InDatagrams|OutDatagrams|NoPorts|RcvbufErrors|SndbufErrors))$\"]\n\n  # List of CPUs from which perf metrics should be collected.\n  [perf_cpus: <string> | default = \"\"]\n\n  # Regexp of power supplies to ignore for the powersupplyclass collector.\n  [powersupply_ignored_supplies: <string> | default = \"^$\"]\n\n  # Path to runit service directory.\n  [runit_service_dir: <string> | default = \"/etc/service\"]\n\n  # XML RPC endpoint for the supervisord collector.\n  [supervisord_url: <string> | default = \"http://localhost:9001/RPC2\"]\n\n  # Regexp of systemd units to whitelist. Units must both match whitelist\n  # and not match blacklist to be included.\n  [systemd_unit_whitelist: <string> | default = \".+\"]\n\n  # Regexp of systemd units to blacklist. Units must both match whitelist\n  # and not match blacklist to be included.\n  [systemd_unit_blacklist: <string> | default = \".+\\\\.(automount|device|mount|scope|slice)\"]\n\n  # Enables service unit tasks metrics unit_tasks_current and unit_tasks_max\n  [systemd_enable_task_metrics: <boolean> | default = false]\n\n  # Enables service unit metric service_restart_total\n  [systemd_enable_restarts_metrics: <boolean> | default = false]\n\n  # Enables service unit metric unit_start_time_seconds\n  [systemd_enable_start_time_metrics: <boolean> | default = false]\n\n  # Directory to read *.prom files from for the textfile collector.\n  [textfile_directory: <string> | default = \"\"]\n\n  # Regexp of fields to return for the vmstat collector.\n  [vmstat_fields: <string> | default = \"^(oom_kill|pgpg|pswp|pg.*fault).*\"]\n```\n", "+++\ntitle = \"process_exporter_config\"\n+++\n\n# process_exporter_config\n\nThe `process_exporter_config` block configures the `process_exporter` integration,\nwhich is an embedded version of\n[`process-exporter`](https://github.com/ncabatoff/process-exporter)\nand allows for collection metrics based on the /proc filesystem on Linux\nsystems. Note that on non-Linux systems, enabling this exporter is a no-op.\n\nNote that if running the Agent in a container, you will need to bind mount\nfolders from the host system so the integration can monitor them:\n\n```\ndocker run \\\n  -v \"/proc:/proc:ro\" \\\n  -v /tmp/agent:/etc/agent \\\n  -v /path/to/config.yaml:/etc/agent-config/agent.yaml \\\n  grafana/agent:v0.21.2 \\\n  --config.file=/etc/agent-config/agent.yaml\n```\n\nReplace `/path/to/config.yaml` with the appropriate path on your host system\nwhere an Agent config file can be found.\n\nFor running on Kubernetes, ensure to set the equivalent mounts and capabilities\nthere as well:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: agent\nspec:\n  containers:\n  - image: grafana/agent:v0.21.2\n    name: agent\n    args:\n    - --config.file=/etc/agent-config/agent.yaml\n    volumeMounts:\n    - name: procfs\n      mountPath: /proc\n      readOnly: true\n  volumes:\n  - name: procfs\n    hostPath:\n      path: /proc\n```\n\nThe manifest and Tanka configs provided by this repository do not have the\nmounts or capabilities required for running this integration.\n\nAn example config for `process_exporter_config` that tracks all processes is the\nfollowing:\n\n```\nenabled: true\nprocess_names:\n- name: \"{{.Comm}}\"\n  cmdline:\n  - '.+'\n```\n\nFull reference of options:\n\n```yaml\n  # Enables the process_exporter integration, allowing the Agent to automatically\n  # collect system metrics from the host UNIX system.\n  [enabled: <boolean> | default = false]\n\n  # Sets an explicit value for the instance label when the integration is\n  # self-scraped. Overrides inferred values.\n  #\n  # The default value for this integration is inferred from the agent hostname\n  # and HTTP listen port, delimited by a colon.\n  [instance: <string>]\n\n  # Automatically collect metrics from this integration. If disabled,\n  # the process_exporter integration will be run but not scraped and thus not\n  # remote-written. Metrics for the integration will be exposed at\n  # /integrations/process_exporter/metrics and can be scraped by an external\n  # process.\n  [scrape_integration: <boolean> | default = <integrations_config.scrape_integrations>]\n\n  # How often should the metrics be collected? Defaults to\n  # prometheus.global.scrape_interval.\n  [scrape_interval: <duration> | default = <global_config.scrape_interval>]\n\n  # The timeout before considering the scrape a failure. Defaults to\n  # prometheus.global.scrape_timeout.\n  [scrape_timeout: <duration> | default = <global_config.scrape_timeout>]\n\n  # Allows for relabeling labels on the target.\n  relabel_configs:\n    [- <relabel_config> ... ]\n\n  # Relabel metrics coming from the integration, allowing to drop series\n  # from the integration that you don't care about.\n  metric_relabel_configs:\n    [ - <relabel_config> ... ]\n\n  # How frequent to truncate the WAL for this integration.\n  [wal_truncate_frequency: <duration> | default = \"60m\"]\n\n  # procfs mountpoint.\n  [procfs_path: <string> | default = \"/proc\"]\n\n  # If a proc is tracked, track with it any children that aren't a part of their\n  # own group.\n  [track_children: <boolean> | default = true]\n\n  # Report on per-threadname metrics as well.\n  [track_threads: <boolean> | default = true]\n\n  # Gather metrics from smaps file, which contains proportional resident memory\n  # size.\n  [gather_smaps: <boolean> | default = true]\n\n  # Recheck process names on each scrape.\n  [recheck_on_scrape: <boolean> | default = false]\n\n  # A collection of matching rules to use for deciding which processes to\n  # monitor. Each config can match multiple processes to be tracked as a single\n  # process \"group.\"\n  process_names:\n    [- <process_matcher_config>]\n```\n\n## process_matcher_config\n\n```yaml\n# The name to use for identifying the process group name in the metric. By\n# default, it uses the base path of the executable.\n#\n# The following template variables are available:\n#\n# - {{.Comm}}:      Basename of the original executable from /proc/<pid>/stat\n# - {{.ExeBase}}:   Basename of the executable from argv[0]\n# - {{.ExeFull}}:   Fully qualified path of the executable\n# - {{.Username}}:  Username of the effective user\n# - {{.Matches}}:   Map containing all regex capture groups resulting from\n#                   matching a process with the cmdline rule group.\n# - {{.PID}}:       PID of the process. Note that the PID is copied from the\n#                   first executable found.\n# - {{.StartTime}}: The start time of the process. This is useful when combined\n#                   with PID as PIDS get reused over time.\n[name: <string> | default = \"{{.ExeBase}}\"]\n\n# A list of strings that match the base executable name for a process, truncated\n# at 15 characters. It is derived from reading the second field of\n# /proc/<pid>/stat minus the parens.\n#\n# If any of the strings match, the process will be tracked.\ncomm:\n  [- <string>]\n\n# A list of strings that match argv[0] for a process. If there are no slashes,\n# only the basename of argv[0] needs to match. Otherwise the name must be an\n# exact match. For example, \"postgres\" may match any postgres binary but\n# \"/usr/local/bin/postgres\" can only match a postgres at that path exactly.\n#\n# If any of the strings match, the process will be tracked.\nexe:\n  [- <string>]\n\n# A list of regular expressions applied to the argv of the process. Each\n# regex here must match the corresponding argv for the process to be tracked.\n# The first element that is matched is argv[1].\n#\n# Regex Captures are added to the .Matches map for use in the name.\ncmdline:\n  [- <string>]\n```\n", "+++\ntitle = \"Getting started with Grafana Agent\"\nweight = 100\n+++\n\n# Getting started with Grafana Agent\n\nThis guide helps users get started with the Grafana Agent. For getting started\nwith the Grafana Agent Operator, please refer to the Operator-specific\n[documentation](../operator/).\n\nCurrently, there are six ways to install the agent:\n\n- Use our Docker container\n- Use the Kubernetes manifests directly\n- Use the Kubernetes manifests along with the [Grafana Cloud Kubernetes Quickstart Guides](#grafana-cloud-kubernetes-quickstart-guides)\n- Installing the static binaries locally\n- Using Grafana Labs' official Tanka configs (_recommended advanced_)\n- Using the [Windows Installer]({{< relref \"./install-agent-on-windows.md\" >}})\n\nSee the list of [Community Projects](#community-projects) for the community-driven ecosystem around the Grafana Agent.\n\n## Docker container\n\n```\ndocker run \\\n  -v /tmp/agent:/etc/agent/data \\\n  -v /path/to/config.yaml:/etc/agent/agent.yaml \\\n  grafana/agent:v0.21.2\n```\n\nReplace `/tmp/agent` with the folder you wish to store WAL data in. WAL data is\nwhere metrics are stored before they are sent to Prometheus. Old WAL data is\ncleaned up every hour, and will be used for recovering if the process happens to\ncrash.\n\nTo override the default flags passed to the container, add the following flags\nto the end of the `docker run` command:\n\n- `--config.file=path/to/agent.yaml`, replacing the argument with the full path\n  to your Agent's YAML configuration file.\n\n- `--prometheus.wal-directory=/tmp/agent/data`, replacing `/tmp/agent/data` with\n  the directory you wish to use for storing data. Note that `/tmp` may get\n  deleted by most operating systems after a reboot.\n\nNote that using paths on your host machine must be exposed to the Docker\ncontainer through a bind mount for the flags to work properly.\n\n## Kubernetes manifests\n\nIf you wish to manually modify the Kubernetes manifests before deploying them, you can do so by downloading them from the [`kubernetes` directory](../../production/kubernetes/). Note that these manifests do not include Agent configuration files. For sample configuration, please see the Grafana Cloud Kubernetes quickstarts.\n\n## Grafana Cloud kubernetes quickstart guides\n\nThese guides help you get up and running with the Agent and Grafana Cloud, and include sample ConfigMaps.\n\nYou can find them in the [Grafana Cloud documentation](https://grafana.com/docs/grafana-cloud/quickstart/agent-k8s/)\n\n## Install locally\n\nOur [Releases](https://github.com/grafana/agent/releases) page contains\ninstructions for downloading static binaries that are published with every release.\nThese releases contain the plain binary alongside system packages for Windows,\nRed Hat, and Debian.\n\n## Tanka\n\nWe provide [Tanka](https://tanka.dev) configurations in our [`production/`](https://github.com/grafana/agent/tree/main/production/tanka/grafana-agent) directory.\n\n## Community Projects\n\nBelow is a list of community lead projects for working with Grafana Agent. These projects are not maintained or supported by Grafana Labs.\n\n### Helm (Kubernetes Deployment)\n\nA publically available release of a Grafana Agent Helm chart is maintained [here](https://github.com/DandyDeveloper/charts/tree/master/charts/grafana-agent). Contributions and improvements are welcomed. Full details on rolling out and supported options can be found in the [readme](https://github.com/DandyDeveloper/charts/blob/master/charts/grafana-agent/README.md).\n\nThis *does not* require the Grafana Agent Operator to rollout / deploy.\n\n### Juju (Charmed Operator)\n\nThe [grafana-agent-k8s](https://github.com/canonical/grafana-agent-operator) charmed operator runs with [Juju](https://juju.is) the Grafana Agent on Kubernetes.\nThe Grafana Agent charmed operator is designed to work with the [Logs, Metrics and Alerts](https://juju.is/docs/lma2) observability stack.\n", "+++\ntitle = \"Custom Resource Quickstart\"\nweight = 120\n+++\n# Grafana Agent Operator Custom Resource Quickstart\n\nIn this guide you'll learn how to deploy [Agent Operator]({{< relref \"./_index.md\" >}})'s custom resources into your Kubernetes cluster.\n\nYou'll roll out the following custom resources (CRs):\n\n- A `GrafanaAgent` resource, which discovers one or more `MetricsInstance` and `LogsInstances` resources.\n- A `MetricsInstance` resource that defines where to ship collected metrics. Under the hood, this rolls out a Grafana Agent StatefulSet that will scrape and ship metrics to a `remote_write` endpoint.\n- A `ServiceMonitor` resource to collect cAdvisor and kubelet metrics. Under the hood, this configures the `MetricsInstance` / Agent StatefulSet.\n- A `LogsInstance` resource that defines where to ship collected logs. Under the hood, this rolls out a Grafana Agent DaemonSet that will tail log files on your cluster nodes.\n- A `PodLogs` resource to collect container logs from Kubernetes Pods. Under the hood, this configures the`LogsInstance` / Agent DaemonSet.\n\nTo learn more about the custom resources Operator provides and their hierarchy, please consult [Operator architecture]({{< relref \"./architecture.md\" >}}).\n\n> **Note:** Agent Operator is currently in beta and its custom resources are subject to change as the project evolves. It currently supports the metrics and logs subsystems of Grafana Agent. Integrations and traces support is coming soon.\n\nBy the end of this guide, you will be scraping and shipping cAdvisor and Kubelet metrics to a Prometheus-compatible metrics endpoint. You'll also be collecting and shipping your Pods' container logs to a Loki-compatible logs endpoint.\n\n## Prerequisites\n\nBefore you begin, make sure that you have installed Agent Operator into your cluster. You can learn how to do this in:\n- [Installing Grafana Agent Operator with Helm]({{< relref \"./helm-getting-started.md\" >}})\n- [Installing Grafana Agent Operator]({{< relref \"./getting-started.md\" >}})\n\n## Step 1: Deploy GrafanaAgent resource\n\nIn this step you'll roll out a `GrafanaAgent` resource. A `GrafanaAgent` resource discovers `MetricsInstance` and `LogsInstance` resources and defines the Grafana Agent image, Pod requests, limits, affinities, and tolerations. Pod attributes can only be defined at the GrafanaAgent level and are propagated to `MetricsInstance` and `LogsInstance` Pods. To learn more, please see the GrafanaAgent [Custom Resource Definition](https://github.com/grafana/agent/blob/main/production/operator/crds/monitoring.grafana.com_grafanaagents.yaml).\n\n> **Note:** Due to the variety of possible deployment architectures, the official Agent Operator Helm chart does not provide built-in templates for the custom resources described in this quickstart. These must be configured and deployed manually. However, you are encouraged to template and add the following manifests to your own in-house Helm charts and GitOps flows.\n\nRoll out the following manifests in your cluster:\n\n```yaml\napiVersion: monitoring.grafana.com/v1alpha1\nkind: GrafanaAgent\nmetadata:\n  name: grafana-agent\n  namespace: default\n  labels:\n    app: grafana-agent\nspec:\n  image: grafana/agent:v0.21.2\n  logLevel: info\n  serviceAccountName: grafana-agent\n  metrics:\n    instanceSelector:\n      matchLabels:\n        agent: grafana-agent-metrics\n    externalLabels:\n      cluster: cloud\n\n  logs:\n    instanceSelector:\n      matchLabels:\n        agent: grafana-agent-logs\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent\n  namespace: default\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/proxy\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  - /metrics/cadvisor\n  verbs:\n  - get\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent\n  namespace: default\n```\n\nThis creates a ServiceAccount, ClusterRole, and ClusterRoleBinding for the GrafanaAgent resource. It also creates a GrafanaAgent resource and specifies an Agent image version. Finally, the GrafanaAgent resource specifies `MetricsInstance` and `LogsInstance` selectors. These search for MetricsInstances and LogsInstances in the same namespace with labels matching `agent: grafana-agent-metrics` and `agent: grafana-agent-logs`, respectively. It also sets a `cluster: cloud` label for all metrics shipped your Prometheus-compatible endpoint. You should change this label to your desired cluster name.\n\nThe full hierarchy of custom resources is as follows:\n\n- `GrafanaAgent`\n  - `MetricsInstance`\n    - `PodMonitor`\n    - `Probe`\n    - `ServiceMonitor`\n  - `LogsInstance`\n    - `PodLogs`\n\nDeploying a GrafanaAgent resource on its own will not spin up any Agent Pods. Agent Operator will create Agent Pods once MetricsInstance and LogsIntance resources have been created. In the next step, you'll roll out a `MetricsInstance` resource to scrape cAdvisor and Kubelet metrics and ship these to your Prometheus-compatible metrics endpoint.\n\n## Step 2: Deploy a MetricsInstance resource\n\nIn this step you'll roll out a MetricsInstance resource. MetricsInstance resources define a `remote_write` sink for metrics and configure one or more selectors to watch for creation and updates to `*Monitor` objects. These objects allow you to define Agent scrape targets via K8s manifests:\n\n- [ServiceMonitors](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#servicemonitor)\n- [PodMonitors](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#podmonitor)\n- [Probes](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#probe)\n\nRoll out the following manifest into your cluster:\n\n```yaml\napiVersion: monitoring.grafana.com/v1alpha1\nkind: MetricsInstance\nmetadata:\n  name: primary\n  namespace: default\n  labels:\n    agent: grafana-agent-metrics\nspec:\n  remoteWrite:\n  - url: your_remote_write_URL\n    basicAuth:\n      username:\n        name: primary-credentials-metrics\n        key: username\n      password:\n        name: primary-credentials-metrics\n        key: password\n\n  # Supply an empty namespace selector to look in all namespaces. Remove\n  # this to only look in the same namespace as the MetricsInstance CR\n  serviceMonitorNamespaceSelector: {}\n  serviceMonitorSelector:\n    matchLabels:\n      instance: primary\n\n  # Supply an empty namespace selector to look in all namespaces. Remove\n  # this to only look in the same namespace as the MetricsInstance CR.\n  podMonitorNamespaceSelector: {}\n  podMonitorSelector:\n    matchLabels:\n      instance: primary\n\n  # Supply an empty namespace selector to look in all namespaces. Remove\n  # this to only look in the same namespace as the MetricsInstance CR.\n  probeNamespaceSelector: {}\n  probeSelector:\n    matchLabels:\n      instance: primary\n```\n\nBe sure to replace the `remote_write` URL and customize the namespace and label configuration as necessary. This will associate itself with the `agent: grafana-agent` GrafanaAgent resource deployed in the previous step, and watch for creation and updates to `*Monitors` monitors with the the `instance: primary` label.\n\nOnce you've rolled out this manifest, create the `basicAuth` credentials using a Kubernetes Secret:\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: primary-credentials-metrics\n  namespace: default\nstringData:\n  username: 'your_cloud_prometheus_username'\n  password: 'your_cloud_prometheus_API_key'\n```\n\nIf you're using Grafana Cloud, you can find your hosted Prometheus endpoint username and password in the [Grafana Cloud Portal](https://grafana.com/profile/org ). You may wish to base64-encode these values yourself. In this case, please use `data` instead of `stringData`.\n\nOnce you've rolled out the `MetricsInstance` and its Secret, you can confirm that the MetricsInstance Agent is up and running with `kubectl get pod`. Since we haven't defined any monitors yet, this Agent will not have any scrape targets defined. In the next step, we'll create scrape targets for the cAdvisor and kubelet endpoints exposed by the `kubelet` service in the cluster.\n\n## Step 3: Create ServiceMonitors for kubelet and cAdvisor endpoints\n\nIn this step, you'll create ServiceMonitors for kubelet and cAdvisor metrics exposed by the `kubelet` Service. Every node in your cluster exposes kubelet and cadvisor metrics at `/metrics` and `/metrics/cadvisor` respectively. Agent Operator creates a `kubelet` service that exposes these Node endpoints so that they can be scraped using ServiceMonitors.\n\nTo scrape these two endpoints, roll out the following two ServiceMonitors in your cluster:\n\n- Kubelet ServiceMonitor\n\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    instance: primary\n  name: kubelet-monitor\n  namespace: default\nspec:\n  endpoints:\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    honorLabels: true\n    interval: 60s\n    metricRelabelings:\n    - action: keep\n      regex: kubelet_cgroup_manager_duration_seconds_count|go_goroutines|kubelet_pod_start_duration_seconds_count|kubelet_runtime_operations_total|kubelet_pleg_relist_duration_seconds_bucket|volume_manager_total_volumes|kubelet_volume_stats_capacity_bytes|container_cpu_usage_seconds_total|container_network_transmit_bytes_total|kubelet_runtime_operations_errors_total|container_network_receive_bytes_total|container_memory_swap|container_network_receive_packets_total|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|kubelet_running_pod_count|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate|container_memory_working_set_bytes|storage_operation_errors_total|kubelet_pleg_relist_duration_seconds_count|kubelet_running_pods|rest_client_request_duration_seconds_bucket|process_resident_memory_bytes|storage_operation_duration_seconds_count|kubelet_running_containers|kubelet_runtime_operations_duration_seconds_bucket|kubelet_node_config_error|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_running_container_count|kubelet_volume_stats_available_bytes|kubelet_volume_stats_inodes|container_memory_rss|kubelet_pod_worker_duration_seconds_count|kubelet_node_name|kubelet_pleg_relist_interval_seconds_bucket|container_network_receive_packets_dropped_total|kubelet_pod_worker_duration_seconds_bucket|container_start_time_seconds|container_network_transmit_packets_dropped_total|process_cpu_seconds_total|storage_operation_duration_seconds_bucket|container_memory_cache|container_network_transmit_packets_total|kubelet_volume_stats_inodes_used|up|rest_client_requests_total\n      sourceLabels:\n      - __name__\n    - action: replace\n      targetLabel: job\n      replacement: integrations/kubernetes/kubelet\n    port: https-metrics\n    relabelings:\n    - sourceLabels:\n      - __metrics_path__\n      targetLabel: metrics_path\n    scheme: https\n    tlsConfig:\n      insecureSkipVerify: true\n  namespaceSelector:\n    matchNames:\n    - default\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kubelet\n```\n\n- cAdvsior ServiceMonitor\n\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    instance: primary\n  name: cadvisor-monitor\n  namespace: default\nspec:\n  endpoints:\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    honorLabels: true\n    honorTimestamps: false\n    interval: 60s\n    metricRelabelings:\n    - action: keep\n      regex: kubelet_cgroup_manager_duration_seconds_count|go_goroutines|kubelet_pod_start_duration_seconds_count|kubelet_runtime_operations_total|kubelet_pleg_relist_duration_seconds_bucket|volume_manager_total_volumes|kubelet_volume_stats_capacity_bytes|container_cpu_usage_seconds_total|container_network_transmit_bytes_total|kubelet_runtime_operations_errors_total|container_network_receive_bytes_total|container_memory_swap|container_network_receive_packets_total|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|kubelet_running_pod_count|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate|container_memory_working_set_bytes|storage_operation_errors_total|kubelet_pleg_relist_duration_seconds_count|kubelet_running_pods|rest_client_request_duration_seconds_bucket|process_resident_memory_bytes|storage_operation_duration_seconds_count|kubelet_running_containers|kubelet_runtime_operations_duration_seconds_bucket|kubelet_node_config_error|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_running_container_count|kubelet_volume_stats_available_bytes|kubelet_volume_stats_inodes|container_memory_rss|kubelet_pod_worker_duration_seconds_count|kubelet_node_name|kubelet_pleg_relist_interval_seconds_bucket|container_network_receive_packets_dropped_total|kubelet_pod_worker_duration_seconds_bucket|container_start_time_seconds|container_network_transmit_packets_dropped_total|process_cpu_seconds_total|storage_operation_duration_seconds_bucket|container_memory_cache|container_network_transmit_packets_total|kubelet_volume_stats_inodes_used|up|rest_client_requests_total\n      sourceLabels:\n      - __name__\n    - action: replace\n      targetLabel: job\n      replacement: integrations/kubernetes/cadvisor\n    path: /metrics/cadvisor\n    port: https-metrics\n    relabelings:\n    - sourceLabels:\n      - __metrics_path__\n      targetLabel: metrics_path\n    scheme: https\n    tlsConfig:\n      insecureSkipVerify: true\n  namespaceSelector:\n    matchNames:\n    - default\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kubelet\n```\n\nThese two ServiceMonitors configure Agent to scrape all the Kubelet and cAdvisor endpoints in your Kubernetes cluster (one of each per Node). In addition, it defines a `job` label which you may change (it is preset here for compatibility with Grafana Cloud's Kubernetes integration), and allowlists a core set of Kubernetes metrics to reduce remote metrics usage. If you don't need this allowlist, you may omit it, however note that your metrics usage will increase significantly.\n\n When you're done, Agent should now be shipping Kubelet and cAdvisor metrics to your remote Prometheus endpoint.\n\n## Step 4: Deploy LogsInstance and PodLogs resources\n\nIn this step, you'll deploy a LogsInstance resource to collect logs from your cluster nodes and ship these to your remote Loki endpoint. Under the hood, Agent Operator will deploy a DaemonSet of Agents in your cluster that will tail log files defined in PodLogs resources.\n\nDeploy the LogsInstance into your cluster:\n\n```yaml\napiVersion: monitoring.grafana.com/v1alpha1\nkind: LogsInstance\nmetadata:\n  name: primary\n  namespace: default\n  labels:\n    agent: grafana-agent-logs\nspec:\n  clients:\n  - url: your_remote_logs_URL\n    basicAuth:\n      username:\n        name: primary-credentials-logs\n        key: username\n      password:\n        name: primary-credentials-logs\n        key: password\n\n  # Supply an empty namespace selector to look in all namespaces. Remove\n  # this to only look in the same namespace as the LogsInstance CR\n  podLogsNamespaceSelector: {}\n  podLogsSelector:\n    matchLabels:\n      instance: primary\n```\n\nThis LogsInstance will pick up PodLogs resources with the `instance: primary` label. Be sure to set the Loki URL to the correct push endpoint (for Grafana Cloud, this will be something like `logs-prod-us-central1.grafana.net/loki/api/v1/push`, however you should check the Cloud Portal to confirm).\n\nAlso note that we are using the `agent: grafana-agent-logs` label here, which will associate this LogsInstance with the GrafanaAgent resource defined in Step 1. This means that it will inherit requests, limits, affinities and other properties defined in the GrafanaAgent custom resource.\n\nCreate the Secret for the LogsInstance resource:\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: primary-credentials-logs\n  namespace: default\nstringData:\n  username: 'your_username_here'\n  password: 'your_password_here'\n```\n\nIf you're using Grafana Cloud, you can find your hosted Loki endpoint username and password in the [Grafana Cloud Portal](https://grafana.com/profile/org). You may wish to base64-encode these values yourself. In this case, please use `data` instead of `stringData`.\n\nFinally, we'll roll out a PodLogs resource to define our logging targets. Under the hood, Agent Operator will turn this into Agent config for the logs subsystem, and roll it out to the DaemonSet of logging agents.\n\nThe following is a minimal working example which you should adapt to your production needs:\n\n```yaml\napiVersion: monitoring.grafana.com/v1alpha1\nkind: PodLogs\nmetadata:\n  labels:\n    instance: primary\n  name: kubernetes-pods\n  namespace: default\nspec:\n  pipelineStages:\n    - docker: {}\n  namespaceSelector:\n    matchNames:\n    - default\n  selector:\n    matchLabels: {}\n```\n\nThis tails container logs for all Pods in the `default` Namespace. You can restrict the set of Pods matched by using the `matchLabels` selector. You can also set additional `pipelineStages` and create `relabelings` to add or modify log line labels. To learn more about the PodLogs spec and available resource fields, please see the [PodLogs CRD](https://github.com/grafana/agent/blob/main/production/operator/crds/monitoring.grafana.com_podlogs.yaml).\n\nUnder the hood, the above PodLogs resource will add the following labels to log lines:\n\n- `namespace`\n- `service`\n- `pod`\n- `container`\n- `job`\n  - Set to `PodLogs_namespace/PodLogs_name`\n- `__path__` (the path to log files)\n  - Set to `/var/log/pods/*$1/*.log` where `$1` is `__meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name`\n\nTo learn more about this config format and other available labels, please see the [Promtail Scraping](https://grafana.com/docs/loki/latest/clients/promtail/scraping/#promtail-scraping-service-discovery) reference documentation. Agent Operator will load this config into the LogsInstance agents automatically.\n\nAt this point the DaemonSet of logging agents should be tailing your container logs, applying some default labels to the log lines, and shipping them to your remote Loki endpoint.\n\n## Conclusion\n\nAt this point you've rolled out the following into your cluster:\n\n- A `GrafanaAgent` resource, which discovers one or more `MetricsInstance` and `LogsInstances` resources.\n- A `MetricsInstance`  resource that defines where to ship collected metrics.\n- A `ServiceMonitor` resource to collect cAdvisor and kubelet metrics.\n- A `LogsInstance` resource that defines where to ship collected logs.\n- A `PodLogs` resource to collect container logs from Kubernetes Pods.\n\nYou can verify that everything is working correctly by navigating to your Grafana instance and querying your Loki and Prometheus datasources. Operator support for Tempo and traces is coming soon.\n", "+++\ntitle = \"Installing Grafana Agent Operator\"\nweight = 100\n+++\n\n# Installing Grafana Agent Operator\n\nIn this guide you'll learn how to deploy the [Grafana Agent Operator]({{< relref \"./_index.md\" >}}) into your Kubernetes cluster. This guide does *not* use Helm. To learn how to deploy Agent Operator using the [grafana-agent-operator Helm chart](https://github.com/grafana/helm-charts/tree/main/charts/agent-operator), please see [Installing Grafana Agent Operator with Helm]({{< relref \"./helm-getting-started.md\" >}}).\n\n> **Note:** Agent Operator is currently in beta and its custom resources are subject to change as the project evolves. It currently supports the metrics and logs subsystems of Grafana Agent. Integrations and traces support is coming soon.\n\nBy the end of this guide, you'll have deloyed Agent Operator into your cluster.\n\n## Prerequisites\n\nBefore you begin, make sure that you have the following available to you:\n\n- A Kubernetes cluster\n- The `kubectl` command-line client installed and configured on your machine\n\n## Step 1: Deploy CustomResourceDefinitions\n\nBefore you can write custom resources to describe a Grafana Agent deployment,\nyou _must_ deploy the\n[CustomResourceDefinitions](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)\nto the cluster first. These definitions describe the schema that the custom\nresources will conform to. This is also required for the operator to run; it\nwill fail if it can't find the custom resource definitions of objects it is\nlooking to use.\n\nThe current set of CustomResourceDefinitions can be found in\n[production/operator/crds](https://github.com/grafana/agent/tree/main/production/operator/crds). Apply them from the\nroot of this repository using:\n\n```\nkubectl apply -f production/operator/crds\n```\n\nThis step _must_ be done before installing the Operator, as the Operator will\nfail to start if the CRDs do not exist.\n\n### Find information on the supported values for the CustomResourceDefinitions\n\nOnce you've deployed the CustomResourceDefinitions\nto your Kubernetes cluster, use `kubectl explain <resource>` to get access to\nthe documentation for each resource. For example, `kubectl explain GrafanaAgent`\nwill describe the GrafanaAgent CRD, and `kubectl explain GrafanaAgent.spec` will\ngive you information on its spec field.\n\n## Step 2: Install Agent Operator\n\nUse the following deployment to run the Operator, changing values as desired:\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-agent-operator\n  namespace: default\n  labels:\n    app: grafana-agent-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana-agent-operator\n  template:\n    metadata:\n      labels:\n        app: grafana-agent-operator\n    spec:\n      serviceAccountName: grafana-agent-operator\n      containers:\n      - name: operator\n        image: grafana/agent-operator:v0.21.2\n        args:\n        - --kubelet-service=default/kubelet\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent-operator\n  namespace: default\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-operator\nrules:\n- apiGroups: [monitoring.grafana.com]\n  resources:\n  - grafanaagents\n  - metricsinstances\n  - logsinstances\n  - podlogs\n  verbs: [get, list, watch]\n- apiGroups: [monitoring.coreos.com]\n  resources:\n  - podmonitors\n  - probes\n  - servicemonitors\n  verbs: [get, list, watch]\n- apiGroups: [\"\"]\n  resources:\n  - namespaces\n  - nodes\n  verbs: [get, list, watch]\n- apiGroups: [\"\"]\n  resources:\n  - secrets\n  - services\n  - configmaps\n  - endpoints\n  verbs: [get, list, watch, create, update, patch, delete]\n- apiGroups: [\"apps\"]\n  resources:\n  - statefulsets\n  - daemonsets\n  verbs: [get, list, watch, create, update, patch, delete]\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent-operator\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent-operator\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent-operator\n  namespace: default\n```\n\n### Run Operator locally\n\nBefore running locally, _make sure your kubectl context is correct!_\nRunning locally uses your current kubectl context, and you probably don't want\nto accidentally deploy a new Grafana Agent to prod.\n\nCRDs should be installed on the cluster prior to running locally. If you haven't\ndone this yet, follow [deploying CustomResourceDefinitions](#deploying-customresourcedefinitions)\nfirst.\n\nAfterwards, you can run the operator using `go run`:\n\n```\ngo run ./cmd/agent-operator\n```\n\n## Conclusion\n\nWith Agent Operator up and running, you can move on to setting up a `GrafanaAgent` custom resource. This will discover `MetricsInstance` and `LogsInstance` custom resources and endow them with Pod attributes (like requests and limits) defined in the `GrafanaAgent` spec. To learn how to do this, please see [Custom Resource Quickstart]({{< relref \"./custom-resource-quickstart.md\" >}}).\n", "+++\ntitle = \"Upgrade guide\"\nweight = 200\n+++\n\n# Upgrade guide\n\nThis guide describes all breaking changes that have happened in prior\nreleases and how to migrate to newer versions.\n\n## Unreleased Changes\n\nThese changes will come in a future version.\n\n## v0.21.2, v0.20.1\n\n### Disabling of config retrieval enpoints\n\nThese two patch releases, as part of a fix for\n[CVE-2021-41090](https://github.com/grafana/agent/security/advisories/GHSA-9c4x-5hgq-q3wh),\ndisable the `/-/config` and `/agent/api/v1/configs/{name}` endpoints by\ndefault. Pass the `--config.enable-read-api` flag at the command line to\nre-enable them.\n\n## v0.21.0\n\n### Integrations: Change in how instance labels are handled (Breaking change)\n\nIntegrations will now use a SUO-specific `instance` label value. Integrations\nthat apply to a whole machine or agent will continue to use `<agent machine\nhostname>:<agent listen port>`, but integrations that connect to an external\nsystem will now infer an appropriate value based on the config for that specific\nintegration. Please refer to the documentation for each integration for which\ndefaults are used.\n\n*Note:* In some cases, a default value for `instance` cannot be inferred. This\nis the case for mongodb_exporter and postgres_exporter if more than one SUO is\nbeing connected to. In these cases, the instance value can be manually set by\nconfiguring the `instance` field on the integration. This can also be useful if\ntwo agents infer the same value for instance for the same integration.\n\nAs part of this change, the `agent_hostname` label is permanently affixed to\nself-scraped integrations and cannot be disabled. This disambigutates multiple\nagents using the same instance label for an integration, and allows users to\nidentify which agents need to be updated with an override for `instance`.\n\nBoth `use_hostname_label` and `replace_instance_label` are now both deprecated\nand ignored from the YAML file, permanently treated as true. A future release\nwill remove these fields, causing YAML errors on load instead of being silently\nignored.\n\n## v0.20.0\n\n### Traces: Changes to receiver's TLS config (Breaking change).\n\nUpgrading to OpenTelemetry v0.36.0 contains a change in the receivers TLS config.\nTLS params have been changed from being squashed to being in its own block.\nThis affect the jaeger receiver's `remote_sampling` config.\n\nExample old config:\n\n```yaml\nreceivers:\n  jaeger:\n    protocols:\n      grpc: null,\n    remote_sampling:\n      strategy_file: <file_path>\n      insecure: true\n```\n\nExample new config:\n\n```yaml\nreceivers:\n  jaeger:\n    protocols:\n      grpc: null,\n    remote_sampling:\n      strategy_file: <file_path>\n      tls:\n        insecure: true\n```\n\n### Traces: push_config is no longer supported (Breaking change)\n\n`push_config` was deprecated in favor of `remote_write` in v0.14.0, while\nmaintaining backwards compatibility.\nRefer to the [deprecation announcement](#tempo-push_config-deprecation) for how to upgrade.\n\n## v0.19.0\n\n### Traces: Deprecation of \"tempo\" in config and metrics. (Deprecation)\n\nThe term `tempo` in the config has been deprecated of favor of `traces`. This\nchange is to make intent clearer.\n\nExample old config:\n\n```yaml\ntempo:\n  configs:\n    - name: default\n      receivers:\n        jaeger:\n          protocols:\n            thrift_http:\n```\n\nExample of new config:\n```yaml\ntraces:\n  configs:\n    - name: default\n      receivers:\n        jaeger:\n          protocols:\n            thrift_http:\n```\n\nAny tempo metrics have been renamed from `tempo_*` to `traces_*`.\n\n\n### Tempo: split grouping by trace from tail sampling config (Breaking change)\n\nLoad balancing traces between agent instances has been moved from an embedded\nfunctionality in tail sampling to its own configuration block.\nThis is done due to more processor benefiting from receiving consistently\nreceiving all spans for a trace in the same agent to be processed, such as\nservice graphs.\n\nAs a consequence, `tail_sampling.load_balancing` has been deprecated in favor of\na `load_balancing` block. Also, `port` has been renamed to `receiver_port` and\nmoved to the new `load_balancing` block.\n\nExample old config:\n\n```yaml\ntail_sampling:\n  policies:\n    - always_sample:\n  port: 4318\n  load_balancing:\n    exporter:\n      insecure: true\n    resolver:\n      dns:\n        hostname: agent\n        port: 4318\n```\n\nExample new config:\n\n```yaml\ntail_sampling:\n  policies:\n    - always_sample:\nload_balancing:\n  exporter:\n    insecure: true\n  resolver:\n    dns:\n      hostname: agent\n      port: 4318\n  receiver_port: 4318\n```\n\n### Operator: Rename of Prometheus to Metrics (Breaking change)\n\nAs a part of the deprecation of \"Prometheus,\" all Operator CRDs and fields with\n\"Prometheus\" in the name have changed to \"Metrics.\"\n\nThis includes:\n\n- The `PrometheusInstance` CRD is now `MetricsInstance` (referenced by\n  `metricsinstances` and not `metrics-instances` within ClusterRoles).\n- The `Prometheus` field of the `GrafanaAgent` resource is now `Metrics`\n- `PrometheusExternalLabelName` is now `MetricsExternalLabelName`\n\nThis is a hard breaking change, and all fields must change accordingly for the\noperator to continue working.\n\nNote that old CRDs with the old hyphenated names must be deleted (`kubectl\ndelete crds/{grafana-agents,prometheus-instances}`) for ClusterRoles to work\ncorrectly.\n\nTo do a zero-downtime upgrade of the Operator when there is a breaking change,\nrefer to the new `agentctl operator-detatch` command: this will iterate through\nall of your objects and remove any OwnerReferences to a CRD, allowing you to\ndelete your Operator CRDs or CRs.\n\n### Operator: Rename of CRD paths (Breaking change)\n\n`prometheus-instances` and `grafana-agents` have been renamed to\n`metricsinstances` and `grafanaagents` respectively. This is to remain\nconsistent with how Kubernetes names multi-word objects.\n\nAs a result, you will need to update your ClusterRoles to change the path of\nresources.\n\nTo do a zero-downtime upgrade of the Operator when there is a breaking change,\nrefer to the new `agentctl operator-detatch` command: this will iterate through\nall of your objects and remove any OwnerReferences to a CRD, allowing you to\ndelete your Operator CRDs or CRs.\n\n\nExample old ClusterRole:\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-operator\nrules:\n- apiGroups: [monitoring.grafana.com]\n  resources:\n  - grafana-agents\n  - prometheus-instances\n  verbs: [get, list, watch]\n```\n\nExample new ClusterRole:\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-operator\nrules:\n- apiGroups: [monitoring.grafana.com]\n  resources:\n  - grafanaagents\n  - metricsinstances\n  verbs: [get, list, watch]\n```\n\n### Metrics: Deprecation of \"prometheus\" in config. (Deprecation)\n\nThe term `prometheus` in the config has been deprecated of favor of `metrics`. This\nchange is to make it clearer when referring to Prometheus or another\nPrometheus-like database, and configuration of Grafana Agent to send metrics to\none of those systems.\n\nOld configs will continue to work for now, but support for the old format will\neventually be removed. To migrate your config, change the `prometheus` key to\n`metrics`.\n\nExample old config:\n\n```yaml\nprometheus:\n  configs:\n    - name: default\n      host_filter: false\n      scrape_configs:\n        - job_name: local_scrape\n          static_configs:\n            - targets: ['127.0.0.1:12345']\n              labels:\n                cluster: 'localhost'\n      remote_write:\n        - url: http://localhost:9009/api/prom/push\n```\n\nExample new config:\n\n```yaml\nmetrics:\n  configs:\n    - name: default\n      host_filter: false\n      scrape_configs:\n        - job_name: local_scrape\n          static_configs:\n            - targets: ['127.0.0.1:12345']\n              labels:\n                cluster: 'localhost'\n      remote_write:\n        - url: http://localhost:9009/api/prom/push\n```\n\n### Tempo: prom_instance rename (Breaking change)\n\nAs part of `prometheus` being renamed to `metrics`, the spanmetrics\n`prom_instance` field has been renamed to `metrics_instance`. This is a breaking\nchange, and the old name will no longer work.\n\nExample old config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    spanmetrics:\n      prom_instance: default\n```\n\nExample new config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    spanmetrics:\n      metrics_instance: default\n```\n\n### Logs: Deprecation of \"loki\" in config. (Deprecation)\n\nThe term `loki` in the config has been deprecated of favor of `logs`. This\nchange is to make it clearer when referring to Grafana Loki, and\nconfiguration of Grafana Agent to send logs to Grafana Loki.\n\nOld configs will continue to work for now, but support for the old format will\neventually be removed. To migrate your config, change the `loki` key to `logs`.\n\nExample old config:\n\n```yaml\nloki:\n  positions_directory: /tmp/loki-positions\n  configs:\n  - name: default\n    clients:\n      - url: http://localhost:3100/loki/api/v1/push\n    scrape_configs:\n    - job_name: system\n      static_configs:\n      - targets: ['localhost']\n        labels:\n          job: varlogs\n          __path__: /var/log/*log\n```\n\nExample new config:\n\n```yaml\nlogs:\n  positions_directory: /tmp/loki-positions\n  configs:\n  - name: default\n    clients:\n      - url: http://localhost:3100/loki/api/v1/push\n    scrape_configs:\n    - job_name: system\n      static_configs:\n      - targets: ['localhost']\n        labels:\n          job: varlogs\n          __path__: /var/log/*log\n```\n\n#### Tempo: Deprecation of \"loki\" in config. (Deprecation)\n\nAs part of the `loki` to `logs` rename, parts of the automatic_logging component\nin Tempo have been updated to refer to `logs_instance` instead.\n\nOld configurations using `loki_name`, `loki_tag`, or `backend: loki` will\ncontinue to work as of this version, but support for the old config format\nwill eventually be removed.\n\nExample old config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    automatic_logging:\n      backend: loki\n      loki_name: default\n      spans: true\n      processes: true\n      roots: true\n    overrides:\n      loki_tag: tempo\n```\n\nExample new config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    automatic_logging:\n      backend: logs_instance\n      logs_instance_name: default\n      spans: true\n      processes: true\n      roots: true\n    overrides:\n      logs_instance_tag: tempo\n```\n\n## v0.18.0\n\n### Tempo: Remote write TLS config\n\nTempo `remote_write` now supports configuring TLS settings in the trace\nexporter's client. `insecure_skip_verify` is moved into this setting's block.\n\nOld configurations with `insecure_skip_verify` outside `tls_config` will continue\nto work as of this version, but support will eventually be removed.\nIf both `insecure_skip_verify` and `tls_config.insecure_skip_verify` are used,\nthen the latter take precedence.\n\nExample old config:\n\n```\ntempo:\n  configs:\n    - name: default\n      remote_write:\n        - endpoint: otel-collector:55680\n          insecure: true\n          insecure_skip_verify: true\n```\n\nExample new config:\n\n```\ntempo:\n  configs:\n    - name: default\n      remote_write:\n        - endpoint: otel-collector:55680\n          insecure: true\n          tls_config:\n            insecure_skip_verify: true\n```\n\n## v0.15.0\n\n### Tempo: `automatic_logging` changes\n\nTempo automatic logging previously assumed that the operator wanted to log\nto a Loki instance. With the addition of an option to log to stdout a new\nfield is required to maintain the old behavior.\n\nExample old config:\n\n```\ntempo:\n  configs:\n  - name: default\n    automatic_logging:\n      loki_name: <some loki instance>\n```\n\nExample new config:\n\n```\ntempo:\n  configs:\n  - name: default\n    automatic_logging:\n      backend: loki\n      loki_name: <some loki instance>\n```\n\n## v0.14.0\n\n### Scraping Service security change\n\nv0.14.0 changes the default behavior of the scraping service config management\nAPI to reject all configuration files that read credentials from a file on disk.\nThis prevents malicious users from crafting an instance config file that read\narbitrary files on disk and send their contents to remote endpoints.\n\nTo revert to the old behavior, add `dangerous_allow_reading_files: true` in your\n`scraping_service` config.\n\nExample old config:\n\n```yaml\nprometheus:\n  scraping_service:\n    # ...\n```\n\nExample new config:\n\n```yaml\nprometheus:\n  scraping_service:\n    dangerous_allow_reading_files: true\n    # ...\n```\n\n### SigV4 config change\n\nv0.14.0 updates the internal Prometheus dependency to 2.26.0, which includes\nnative support for SigV4, but uses a slightly different configuration structure\nthan the Grafana Agent did.\n\nTo migrate, remove the `enabled` key from your `sigv4` configs. If `enabled` was\nthe only key, define sigv4 as an empty object: `sigv4: {}`.\n\nExample old config:\n\n```yaml\nsigv4:\n  enabled: true\n  region: us-east-1\n```\n\nExample new config:\n\n```yaml\nsigv4:\n  region: us-east-1\n```\n\n### Tempo: `push_config` deprecation\n\n`push_config` is now deprecated in favor of a `remote_write` array which allows for sending spans to multiple endpoints.\n`push_config` will be removed in a future release, and it is recommended to migrate to `remote_write` as soon as possible.\n\nTo migrate, move the batch options outside the `push_config` block.\nThen, add a `remote_write` array and move the remaining of your `push_config` block inside it.\n\nExample old config:\n\n```yaml\ntempo:\n  configs:\n    - name: default\n      receivers:\n        otlp:\n          protocols:\n            gpc:\n      push_config:\n        endpoint: otel-collector:55680\n        insecure: true\n        batch:\n          timeout: 5s\n          send_batch_size: 100\n```\n\nExample migrated config:\n\n```yaml\ntempo:\n  configs:\n    - name: default\n      receivers:\n        otlp:\n          protocols:\n            gpc:\n      remote_write:\n        - endpoint: otel-collector:55680\n          insecure: true\n      batch:\n        timeout: 5s\n        send_batch_size: 100\n```\n\n\n## v0.12.0\n\nv0.12.0 had two breaking changes: the `tempo` and `loki` sections have been changed to require a list of `tempo`/`loki` configs rather than just one.\n\n### Tempo Config Change\n\nThe Tempo config (`tempo` in the config file) has been changed to store\nconfigs within a `configs` list. This allows for defining multiple Tempo\ninstances for collecting traces and forwarding them to different OTLP\nendpoints.\n\nTo migrate, add a `configs:` array and move your existing config inside of it.\nGive the element a `name: default` field.\n\nEach config must have a unique non-empty name. `default` is recommended for users\nthat don't have other configs. The name of the config will be added as a\n`tempo_config` label for metrics.\n\nExample old config:\n\n```yaml\ntempo:\n  receivers:\n    jaeger:\n      protocols:\n        thrift_http:\n  attributes:\n    actions:\n    - action: upsert\n      key: env\n      value: prod\n  push_config:\n    endpoint: otel-collector:55680\n    insecure: true\n    batch:\n      timeout: 5s\n      send_batch_size: 100\n```\n\nExample migrated config:\n\n```yaml\ntempo:\n  configs:\n  - name: default\n    receivers:\n      jaeger:\n        protocols:\n          thrift_http:\n    attributes:\n      actions:\n      - action: upsert\n        key: env\n        value: prod\n    push_config:\n      endpoint: otel-collector:55680\n      insecure: true\n      batch:\n        timeout: 5s\n        send_batch_size: 100\n```\n\n### Loki Promtail Config Change\n\nThe Loki Promtail config (`loki` in the config file) has been changed to store\nconfigs within a `configs` list. This allows for defining multiple Loki\nPromtail instances for collecting logs and forwarding them to different Loki\nservers.\n\nTo migrate, add a `configs:` array and move your existing config inside of it.\nGive the element a `name: default` field.\n\nEach config must have a unique non-empty name. `default` is recommended for users\nthat don't have other configs. The name of the config will be added as a\n`loki_config` label for Loki Promtail metrics.\n\nExample old config:\n\n```yaml\nloki:\n  positions:\n    filename: /tmp/positions.yaml\n  clients:\n    - url: http://loki:3100/loki/api/v1/push\n  scrape_configs:\n  - job_name: system\n    static_configs:\n      - targets:\n        - localhost\n        labels:\n          job: varlogs\n          __path__: /var/log/*log\n```\n\nExample migrated config:\n\n```yaml\nloki:\n  configs:\n  - name: default\n    positions:\n      filename: /tmp/positions.yaml\n    clients:\n      - url: http://loki:3100/loki/api/v1/push\n    scrape_configs:\n    - job_name: system\n      static_configs:\n        - targets:\n          - localhost\n          labels:\n            job: varlogs\n            __path__: /var/log/*log\n```\n", "package operator\n\n// Supported versions of the Grafana Agent.\nvar (\n\tAgentCompatibilityMatrix = []string{\n\t\t\"v0.14.0\",\n\t\t\"v0.15.0\",\n\t\t// \"v0.16.0\", // Pulled due to critical bug fixed in v0.16.1.\n\t\t\"v0.16.1\",\n\t\t\"v0.17.0\",\n\t\t\"v0.18.0\",\n\t\t\"v0.18.1\",\n\t\t\"v0.18.2\",\n\t\t\"v0.18.3\",\n\t\t\"v0.18.4\",\n\t\t\"v0.19.0\",\n\t\t\"v0.20.0\",\n\t\t\"v0.20.1\",\n\t\t\"v0.21.0\",\n\t\t\"v0.21.1\",\n\t\t\"v0.21.2\",\n\n\t\t// NOTE(rfratto): when performing an upgrade, add the newest version above instead of changing the existing reference.\n\t}\n\n\tDefaultAgentVersion   = AgentCompatibilityMatrix[len(AgentCompatibilityMatrix)-1]\n\tDefaultAgentBaseImage = \"grafana/agent\"\n\tDefaultAgentImage     = DefaultAgentBaseImage + \":\" + DefaultAgentVersion\n)\n", "# Running Grafana Agent\n\nHere are some resources to help you run the Grafana Agent:\n\n- [Windows Installation](#windows-installation)\n- [Run the Agent with Docker](#running-the-agent-with-docker)\n- [Run the Agent locally](#running-the-agent-locally)\n- [Use the example Kubernetes configs](#use-the-example-kubernetes-configs)\n- [Grafana Cloud Kubernetes Quickstart Guides](#grafana-cloud-kubernetes-quickstart-guides)\n- [Agent Operator Helm Quickstart](#agent-operator-helm-quickstart-guide)\n- [Build the Agent from Source](#build-the-agent-from-source)\n- [Use our production Tanka configs](#use-our-production-tanka-configs)\n\n## Windows Installation\n\nTo run the Windows Installation, download the Windows Installer executable from the [release page](https://github.com/grafana/agent/releases). Then run the installer, this will setup the Agent and run the Agent as a Windows Service. More details can be found in the [Windows Guide](../docs/getting-started/install-agent-on-windows.md)\n\n## Running the Agent with Docker\n\nTo run the Agent with Docker, you should have a configuration file on\nyour local machine ready to bind mount into the container. Then modify\nthe following command for your environment. Replace `/path/to/config.yaml` with\nthe full path to your YAML configuration, and replace `/tmp/agent` with the\ndirectory on your host that you want the agent to store its WAL.\n\n```\ndocker run \\\n  -v /tmp/agent:/etc/agent/data \\\n  -v /path/to/config.yaml:/etc/agent/agent.yaml \\\n  grafana/agent:v0.21.2\n```\n\n## Running the Agent locally\n\nCurrently, you must provide your own system configuration files to run the\nAgent as a long-living process (e.g., write your own systemd unit files).\n\n## Use the example Kubernetes configs\n\nYou can find sample deployment manifests in the [Kubernetes](./kubernetes) directory.\n\n## Grafana Cloud Kubernetes quickstart guides\n\nThese guides help you get up and running with the Agent and Grafana Cloud, and include sample ConfigMaps.\n\nYou can find them in the [Grafana Cloud documentation](https://grafana.com/docs/grafana-cloud/quickstart/agent-k8s/)\n\n## Agent Operator Helm quickstart guide\n\nThis guide will show you how to deploy the [Grafana Agent Operator](https://grafana.com/docs/agent/latest/operator/) into your Kubernetes cluster using the [grafana-agent-operator Helm chart](https://github.com/grafana/helm-charts/tree/main/charts/agent-operator).\n\nYou'll also deploy the following custom resources (CRs):\n- A `GrafanaAgent` resource, which discovers one or more `MetricsInstance` and `LogsInstances` resources.\n- A `MetricsInstance` resource that defines where to ship collected metrics.\n- A `ServiceMonitor` resource to collect cAdvisor and kubelet metrics.\n- A `LogsInstance` resource that defines where to ship collected logs.\n- A `PodLogs` resource to collect container logs from Kubernetes Pods.\n\nYou can find the guide [here](https://grafana.com/docs/agent/latest/operator/helm-getting-started/).\n\n## Build the Agent from source\n\nGo 1.14 is currently needed to build the agent from source. Run `make agent`\nfrom the root of this repository, and then the build agent binary will be placed\nat `./cmd/agent/agent`.\n\n## Use our production Tanka configs\n\nThe Tanka configs we use to deploy the agent ourselves can be found in our\n[production Tanka directory](./tanka/grafana-agent). These configs are also used\nto generate the Kubernetes configs for the install script. To get started with\nthe tanka configs, do the following:\n\n```\nmkdir tanka-agent\ncd tanka-agent\ntk init --k8s=false\njb install github.com/grafana/agent/production/tanka/grafana-agent\n\n# substitute your target k8s version for \"1.16\" in the next few commands\njb install github.com/jsonnet-libs/k8s-alpha/1.16\necho '(import \"github.com/jsonnet-libs/k8s-alpha/1.16/main.libsonnet\")' > lib/k.libsonnet\necho '+ (import \"github.com/jsonnet-libs/k8s-alpha/1.16/extensions/kausal-shim.libsonnet\")' >> lib/k.libsonnet\n```\n\nThen put this in `environments/default/main.jsonnet`:\n```\nlocal agent = import 'grafana-agent/grafana-agent.libsonnet';\n\nagent {\n  _config+:: {\n    namespace: 'grafana-agent'\n  },\n}\n```\n\nIf all these steps worked, `tk eval environments/default` should output the\ndefault JSON we use to build our Kubernetes manifests.\n", "#!/usr/bin/env sh\n# shellcheck shell=dash\n# This script should run in all POSIX environments and Dash is POSIX compliant.\n\n# grafanacloud-install.sh installs the Grafana Agent on supported\n# Linux systems for Grafana Cloud users. Those who aren't users of Grafana Cloud\n# or need to install the Agent on a different architecture or platform should\n# try another installation method.\n#\n# grafanacloud-install.sh has a hard dependency on being run on a supported\n# Linux system. Currently only systems that can install deb or rpm packages\n# are supported. The target system will try to be detected, but if it cannot,\n# PACKAGE_SYSTEM can be passed as an environment variable with either rpm or\n# deb.\nset -eu\ntrap \"exit 1\" TERM\nMY_PID=$$\n\nlog() {\n  echo \"$@\" >&2\n}\n\nfatal() {\n  log \"$@\"\n  kill -s TERM \"$MY_PID\"\n}\n\n#\n# REQUIRED environment variables.\n#\nGCLOUD_STACK_ID=${GCLOUD_STACK_ID:=} # Stack ID where integrations are installed\nGCLOUD_API_KEY=${GCLOUD_API_KEY:=}   # API key to authenticate against Grafana Cloud's API with\nGCLOUD_API_URL=${GCLOUD_API_URL:=}   # Grafana Cloud's API url\n\n[ -z \"$GCLOUD_STACK_ID\" ] && fatal \"Required environment variable \\$GCLOUD_STACK_ID not set.\"\n[ -z \"$GCLOUD_API_KEY\" ]  && fatal \"Required environment variable \\$GCLOUD_API_KEY not set.\"\n\n#\n# OPTIONAL environment variables.\n#\n\n# Architecture to install.\nARCH=${ARCH:=amd64}\n\n# Package system to install the Agent with. If not empty, MUST be either rpm or\n# deb. If empty, the script will try to detect the host OS and the appropriate\n# package system to use.\nPACKAGE_SYSTEM=${PACKAGE_SYSTEM:=}\n\n#\n# Global constants.\n#\nRELEASE_VERSION=\"0.21.2\"\n\nRELEASE_URL=\"https://github.com/grafana/agent/releases/download/v${RELEASE_VERSION}\"\nDEB_URL=\"${RELEASE_URL}/grafana-agent-${RELEASE_VERSION}-1.${ARCH}.deb\"\nRPM_URL=\"${RELEASE_URL}/grafana-agent-${RELEASE_VERSION}-1.${ARCH}.rpm\"\n\nmain() {\n  if [ -z \"$PACKAGE_SYSTEM\" ]; then\n    PACKAGE_SYSTEM=$(detect_package_system)\n  fi\n  log \"--- Using package system $PACKAGE_SYSTEM. Downloading and installing package for ${ARCH}\"\n\n  case \"$PACKAGE_SYSTEM\" in\n    deb)\n      install_deb\n      ;;\n    rpm)\n      install_rpm\n      ;;\n    *)\n      fatal \"Unsupported PACKAGE_SYSTEM value $PACKAGE_SYSTEM. Must be either rpm or deb\".\n      ;;\n  esac\n\n  log '--- Retrieving config and placing in /etc/grafana-agent.yaml'\n  retrieve_config | sudo tee /etc/grafana-agent.yaml\n\n  log '--- Enabling and starting grafana-agent.service'\n  sudo systemctl enable grafana-agent.service\n  sudo systemctl start grafana-agent.service\n\n  # Add some empty newlines to give some visual whitespace before printing the\n  # success message.\n  log ''\n  log ''\n  log 'Grafana Agent is now running! To check the status of your Agent, run:'\n  log '   sudo systemctl status grafana-agent.service'\n}\n\n# detect_package_system tries to detect the host distribution to determine if\n# deb or rpm should be used for installing the Agent. Prints out either \"deb\"\n# or \"rpm\". Calls fatal if the host OS is not supported.\ndetect_package_system() {\n  command -v dpkg >/dev/null 2>&1 && { echo \"deb\"; return; }\n  command -v rpm  >/dev/null 2>&1 && { echo \"rpm\"; return; }\n\n  case \"$(uname)\" in\n    Darwin)\n      fatal 'macOS not supported'\n      ;;\n    *)\n      fatal \"Unknown unsupported OS: $(uname)\"\n      ;;\n  esac\n}\n\n# install_deb downloads and installs the deb package of the Grafana Agent.\ninstall_deb() {\n  curl -fsL \"${DEB_URL}\" -o /tmp/grafana-agent.deb || fatal 'Failed to download package'\n  sudo dpkg -i /tmp/grafana-agent.deb\n  rm /tmp/grafana-agent.deb\n}\n\n# install_rpm downloads and installs the deb package of the Grafana Agent.\ninstall_rpm() {\n  sudo rpm --reinstall \"${RPM_URL}\"\n}\n\n# retrieve_config downloads the config file for the Agent and prints out its\n# contents to stdout.\nretrieve_config() {\n  if ! grafana-agentctl cloud-config -u \"${GCLOUD_STACK_ID}\" -p \"${GCLOUD_API_KEY}\" -e \"${GCLOUD_API_URL}\" 2>/dev/null; then\n    fatal \"Failed to retrieve config\"\n  fi\n}\n\nmain\n", "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent\n  namespace: ${NAMESPACE}\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent\n  namespace: ${NAMESPACE}\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-agent\n  namespace: ${NAMESPACE}\nspec:\n  minReadySeconds: 10\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      name: grafana-agent\n  template:\n    metadata:\n      labels:\n        name: grafana-agent\n    spec:\n      containers:\n      - args:\n        - -config.file=/etc/agent/agent.yaml\n        command:\n        - /bin/agent\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        image: grafana/agent:v0.21.2\n        imagePullPolicy: IfNotPresent\n        name: agent\n        ports:\n        - containerPort: 12345\n          name: http-metrics\n        volumeMounts:\n        - mountPath: /etc/agent\n          name: grafana-agent\n      serviceAccount: grafana-agent\n      volumes:\n      - configMap:\n          name: grafana-agent\n        name: grafana-agent\n", "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent-logs\n  namespace: YOUR_NAMESPACE\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-logs\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent-logs\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent-logs\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent-logs\n  namespace: YOUR_NAMESPACE\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: grafana-agent-logs\n  namespace: YOUR_NAMESPACE\nspec:\n  minReadySeconds: 10\n  selector:\n    matchLabels:\n      name: grafana-agent-logs\n  template:\n    metadata:\n      labels:\n        name: grafana-agent-logs\n    spec:\n      containers:\n      - args:\n        - -config.file=/etc/agent/agent.yaml\n        command:\n        - /bin/agent\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        image: grafana/agent:v0.21.2\n        imagePullPolicy: IfNotPresent\n        name: agent\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        securityContext:\n          privileged: true\n          runAsUser: 0\n        volumeMounts:\n        - mountPath: /etc/agent\n          name: grafana-agent-logs\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      serviceAccount: grafana-agent-logs\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - configMap:\n          name: grafana-agent-logs\n        name: grafana-agent-logs\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n        name: etcmachineid\n  updateStrategy:\n    type: RollingUpdate\n", "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: grafana-agent-traces\n  namespace: YOUR_NAMESPACE\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-agent-traces\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: grafana-agent-traces\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-agent-traces\nsubjects:\n- kind: ServiceAccount\n  name: grafana-agent-traces\n  namespace: YOUR_NAMESPACE\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: grafana-agent-traces\n  name: grafana-agent-traces\n  namespace: YOUR_NAMESPACE\nspec:\n  ports:\n  - name: agent-http-metrics\n    port: 8080\n    targetPort: 8080\n  - name: agent-thrift-compact\n    port: 6831\n    protocol: UDP\n    targetPort: 6831\n  - name: agent-thrift-binary\n    port: 6832\n    protocol: UDP\n    targetPort: 6832\n  - name: agent-thrift-http\n    port: 14268\n    protocol: TCP\n    targetPort: 14268\n  - name: agent-thrift-grpc\n    port: 14250\n    protocol: TCP\n    targetPort: 14250\n  - name: agent-zipkin\n    port: 9411\n    protocol: TCP\n    targetPort: 9411\n  - name: agent-otlp\n    port: 55680\n    protocol: TCP\n    targetPort: 55680\n  - name: agent-opencensus\n    port: 55678\n    protocol: TCP\n    targetPort: 55678\n  selector:\n    name: grafana-agent-traces\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-agent-traces\n  namespace: YOUR_NAMESPACE\nspec:\n  minReadySeconds: 10\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      name: grafana-agent-traces\n  template:\n    metadata:\n      labels:\n        name: grafana-agent-traces\n    spec:\n      containers:\n      - args:\n        - -config.file=/etc/agent/agent.yaml\n        command:\n        - /bin/agent\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        image: grafana/agent:v0.21.2\n        imagePullPolicy: IfNotPresent\n        name: agent\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 6831\n          name: thrift-compact\n          protocol: UDP\n        - containerPort: 6832\n          name: thrift-binary\n          protocol: UDP\n        - containerPort: 14268\n          name: thrift-http\n          protocol: TCP\n        - containerPort: 14250\n          name: thrift-grpc\n          protocol: TCP\n        - containerPort: 9411\n          name: zipkin\n          protocol: TCP\n        - containerPort: 55680\n          name: otlp\n          protocol: TCP\n        - containerPort: 55678\n          name: opencensus\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /etc/agent\n          name: grafana-agent-traces\n      serviceAccount: grafana-agent-traces\n      volumes:\n      - configMap:\n          name: grafana-agent-traces\n        name: grafana-agent-traces\n", "'grafana/agent:v0.21.2'\n", "#!/usr/bin/env bash\n# shellcheck shell=bash\n\n#\n# install-bare.sh is an installer for the Agent without a ConfigMap. It is\n# used during the Grafana Cloud integrations wizard and is not recommended\n# to be used directly. Instead of calling this script directly, please\n# make a copy of ./agent-bare.yaml and modify it for your needs.\n#\n# Note that agent-bare.yaml does not have a ConfigMap, so the Grafana Agent\n# will not launch until one is created. For more information on setting up\n# a ConfigMap, please refer to:\n#\n# Metrics quickstart: https://grafana.com/docs/grafana-cloud/quickstart/agent-k8s/k8s_agent_metrics/\n# Logs quickstart: https://grafana.com/docs/grafana-cloud/quickstart/agent-k8s/k8s_agent_logs/\n#\n\ncheck_installed() {\n  if ! type \"$1\" >/dev/null 2>&1; then\n    echo \"error: $1 not installed\" >&2\n    exit 1\n  fi\n}\n\ncheck_installed curl\ncheck_installed envsubst\n\nMANIFEST_BRANCH=v0.21.2\nMANIFEST_URL=${MANIFEST_URL:-https://raw.githubusercontent.com/grafana/agent/${MANIFEST_BRANCH}/production/kubernetes/agent-bare.yaml}\nNAMESPACE=${NAMESPACE:-default}\n\nexport NAMESPACE\n\ncurl -fsSL \"$MANIFEST_URL\" | envsubst\n", "local agent = import './internal/agent.libsonnet';\nlocal utils = import './internal/utils.libsonnet';\nlocal k = import 'ksonnet-util/kausal.libsonnet';\n\nlocal container = k.core.v1.container;\nlocal configMap = k.core.v1.configMap;\nlocal service = k.core.v1.service;\n\n// Merge all of our libraries to create the final exposed library.\n(import './lib/deployment.libsonnet') +\n(import './lib/integrations.libsonnet') +\n(import './lib/metrics.libsonnet') +\n(import './lib/scraping_service.libsonnet') +\n(import './lib/logs.libsonnet') +\n(import './lib/traces.libsonnet') +\n{\n  _images:: {\n    agent: 'grafana/agent:v0.21.2',\n    agentctl: 'grafana/agentctl:v0.21.2',\n  },\n\n  // new creates a new DaemonSet deployment of the grafana-agent. By default,\n  // the deployment will do no collection. You must merge the result of this\n  // function with one or more of the following:\n  //\n  // - withMetricsConfig, withMetricsInstances (and optionally withRemoteWrite)\n  // - withLogsConfig\n  //\n  // When using withMetricsInstances, a [name]-etc deployment\n  // with one replica will be created alongside the DaemonSet. This deployment\n  // is responsible for handling scrape configs that will not work on the host\n  // machine.\n  //\n  // For example, if a scrape_config scrapes the Kubernetes API, that must be\n  // handled by the [name]-etc deployment as the Kubernetes API does not run\n  // on any node in the cluster.\n  //\n  // scrapeInstanceKubernetes provides the default\n  // MetricsInstanceConfig Grafana Labs uses in production.\n  new(name='grafana-agent', namespace='default'):: {\n    local this = self,\n\n    _mode:: 'daemonset',\n    _images:: $._images,\n    _config_hash:: true,\n\n    local has_logs_config = std.objectHasAll(self, '_logs_config'),\n    local has_trace_config = std.objectHasAll(self, '_trace_config'),\n    local has_metrics_config = std.objectHasAll(self, '_metrics_config'),\n    local has_metrics_instances = std.objectHasAll(self, '_metrics_instances'),\n    local has_integrations = std.objectHasAll(self, '_integrations'),\n    local has_sampling_strategies = std.objectHasAll(self, '_traces_sampling_strategies'),\n\n    local metrics_instances =\n      if has_metrics_instances then this._metrics_instances else [],\n    local host_filter_instances = utils.transformInstances(metrics_instances, true),\n    local etc_instances = utils.transformInstances(metrics_instances, false),\n\n    config:: {\n      server: {\n        log_level: 'info',\n        http_listen_port: 8080,\n      },\n    } + (\n      if has_metrics_config\n      then { metrics: this._metrics_config { configs: host_filter_instances } }\n      else {}\n    ) + (\n      if has_logs_config then {\n        logs: {\n          positions_directory: '/tmp/positions',\n          configs: [this._logs_config {\n            name: 'default',\n          }],\n        },\n      } else {}\n    ) + (\n      if has_trace_config then {\n        traces: {\n          configs: [this._trace_config {\n            name: 'default',\n          }],\n        },\n      }\n      else {}\n    ) + (\n      if has_integrations then { integrations: this._integrations } else {}\n    ),\n\n    etc_config:: if has_metrics_config then this.config {\n      // Hide logs and integrations from our extra configs, we just want the\n      // scrape configs that wouldn't work for the DaemonSet.\n      metrics+: {\n        configs: std.map(function(cfg) cfg { host_filter: false }, etc_instances),\n      },\n      logs:: {},\n      traces:: {},\n      integrations:: {},\n    },\n\n    agent:\n      agent.newAgent(name, namespace, self._images.agent, self.config, use_daemonset=true) +\n      agent.withConfigHash(self._config_hash) + {\n        // If sampling strategies were defined, we need to mount them as a JSON\n        // file.\n        config_map+:\n          if has_sampling_strategies\n          then configMap.withDataMixin({\n            'strategies.json': std.toString(this._traces_sampling_strategies),\n          })\n          else {},\n\n        // If we're deploying for tracing, applications will want to write to\n        // a service for load balancing span delivery.\n        service:\n          if has_trace_config\n          then k.util.serviceFor(self.agent) + service.mixin.metadata.withNamespace(namespace)\n          else {},\n      } + (\n        if has_logs_config then $.logsPermissionsMixin else {}\n      ) + (\n        if has_integrations && std.objectHas(this._integrations, 'node_exporter') then $.integrationsMixin else {}\n      ),\n\n    agent_etc: if std.length(etc_instances) > 0 then\n      agent.newAgent(name + '-etc', namespace, self._images.agent, self.etc_config, use_daemonset=false) +\n      agent.withConfigHash(self._config_hash),\n  },\n\n  // withImages sets the images used for launching the Agent.\n  // Keys supported: agent, agentctl\n  withImages(images):: { _images+: images },\n\n  // Includes or excludes the config hash annotation.\n  withConfigHash(include=true):: { _config_hash:: include },\n\n  // withPortsMixin adds extra ports to expose.\n  withPortsMixin(ports=[]):: {\n    agent+: {\n      container+:: container.withPortsMixin(ports),\n    },\n  },\n}\n", "function(name='grafana-agent', namespace='') {\n  local k = (import 'ksonnet-util/kausal.libsonnet') { _config+:: { namespace: namespace } },\n\n  local container = k.core.v1.container,\n  local configMap = k.core.v1.configMap,\n  local containerPort = k.core.v1.containerPort,\n  local policyRule = k.rbac.v1.policyRule,\n  local serviceAccount = k.core.v1.serviceAccount,\n\n  local this = self,\n\n  _images:: {\n    agent: 'grafana/agent:v0.21.2',\n    agentctl: 'grafana/agentctl:v0.21.2',\n  },\n  _config:: {\n    name: name,\n    namespace: namespace,\n    config_hash: true,\n    agent_config: '',\n  },\n\n  rbac: k.util.rbac(name, [\n    policyRule.withApiGroups(['']) +\n    policyRule.withResources(['nodes', 'nodes/proxy', 'services', 'endpoints', 'pods']) +\n    policyRule.withVerbs(['get', 'list', 'watch']),\n\n    policyRule.withNonResourceUrls('/metrics') +\n    policyRule.withVerbs(['get']),\n  ]) {\n    service_account+: serviceAccount.mixin.metadata.withNamespace(namespace),\n  },\n\n  configMap:\n    configMap.new(name) +\n    configMap.mixin.metadata.withNamespace(namespace) +\n    configMap.withData({\n      'agent.yaml': k.util.manifestYaml(this._config.agent_config),\n    }),\n\n  container::\n    container.new(name, this._images.agent) +\n    container.withPorts(containerPort.new('http-metrics', 80)) +\n    container.withCommand('/bin/agent') +\n    container.withArgsMixin(k.util.mapToFlags({\n      'config.file': '/etc/agent/agent.yaml',\n    })),\n}\n", "local k = import 'ksonnet-util/kausal.libsonnet';\n\nlocal cronJob = k.batch.v1beta1.cronJob;\nlocal configMap = k.core.v1.configMap;\nlocal container = k.core.v1.container;\nlocal deployment = k.apps.v1.deployment;\nlocal volumeMount = k.core.v1.volumeMount;\nlocal volume = k.core.v1.volume;\n\nfunction(\n  name='grafana-agent-syncer',\n  namespace='',\n  config={},\n) {\n  local _config = {\n    api: error 'api must be set',\n    image: 'grafana/agentctl:v0.21.2',\n    schedule: '*/5 * * * *',\n    configs: [],\n  } + config,\n\n  local this = self,\n  local _configs = std.foldl(\n    function(agg, cfg)\n      // Sanitize the name and remove / so every file goes into the same\n      // folder.\n      local name = std.strReplace(cfg.name, '/', '_');\n\n      agg { ['%s.yml' % name]: k.util.manifestYaml(cfg) },\n    _config.configs,\n    {},\n  ),\n\n  configMap:\n    configMap.new(name) +\n    configMap.mixin.metadata.withNamespace(namespace) +\n    configMap.withData(_configs),\n\n  container::\n    container.new(name, _config.image) +\n    container.withArgsMixin([\n      'config-sync',\n      '--addr=%s' % _config.api,\n      '/etc/configs',\n    ]) +\n    container.withVolumeMounts(volumeMount.new(name, '/etc/configs')),\n\n  job:\n    cronJob.new(name, _config.schedule, this.container) +\n    cronJob.mixin.metadata.withNamespace(namespace) +\n    cronJob.mixin.spec.withSuccessfulJobsHistoryLimit(1) +\n    cronJob.mixin.spec.withFailedJobsHistoryLimit(3) +\n    cronJob.mixin.spec.jobTemplate.spec.template.spec.withRestartPolicy('OnFailure') +\n    cronJob.mixin.spec.jobTemplate.spec.template.spec.withActiveDeadlineSeconds(600) +\n    cronJob.mixin.spec.jobTemplate.spec.withTtlSecondsAfterFinished(120) +\n    cronJob.mixin.spec.jobTemplate.spec.template.spec.withVolumes([\n      volume.fromConfigMap(\n        name=name,\n        configMapName=this.configMap.metadata.name,\n      ),\n    ]),\n}\n"], "filenames": ["CHANGELOG.md", "docs/configuration/integrations/node-exporter-config.md", "docs/configuration/integrations/process-exporter-config.md", "docs/getting-started/_index.md", "docs/operator/custom-resource-quickstart.md", "docs/operator/getting-started.md", "docs/upgrade-guide/_index.md", "pkg/operator/defaults.go", "production/README.md", "production/grafanacloud-install.sh", "production/kubernetes/agent-bare.yaml", "production/kubernetes/agent-loki.yaml", "production/kubernetes/agent-traces.yaml", "production/kubernetes/build/lib/version.libsonnet", "production/kubernetes/install-bare.sh", "production/tanka/grafana-agent/v1/main.libsonnet", "production/tanka/grafana-agent/v2/internal/base.libsonnet", "production/tanka/grafana-agent/v2/internal/syncer.libsonnet"], "buggy_code_start_loc": [7, 29, 21, 29, 46, 75, 13, 17, 30, 53, 69, 67, 112, 1, 28, 18, 13, 17], "buggy_code_end_loc": [48, 70, 39, 30, 47, 76, 13, 19, 31, 54, 70, 68, 113, 2, 29, 20, 15, 18], "fixing_code_start_loc": [8, 29, 21, 29, 46, 75, 14, 18, 30, 53, 69, 67, 112, 1, 28, 18, 13, 17], "fixing_code_end_loc": [71, 70, 39, 30, 47, 76, 24, 22, 31, 54, 70, 68, 113, 2, 29, 20, 15, 18], "type": "CWE-312", "message": "Grafana Agent is a telemetry collector for sending metrics, logs, and trace data to the opinionated Grafana observability stack. Prior to versions 0.20.1 and 0.21.2, inline secrets defined within a metrics instance config are exposed in plaintext over two endpoints: metrics instance configs defined in the base YAML file are exposed at `/-/config` and metrics instance configs defined for the scraping service are exposed at `/agent/api/v1/configs/:key`. Inline secrets will be exposed to anyone being able to reach these endpoints. If HTTPS with client authentication is not configured, these endpoints are accessible to unauthenticated users. Secrets found in these sections are used for delivering metrics to a Prometheus Remote Write system, authenticating against a system for discovering Prometheus targets, and authenticating against a system for collecting metrics. This does not apply for non-inlined secrets, such as `*_file` based secrets. This issue is patched in Grafana Agent versions 0.20.1 and 0.21.2. A few workarounds are available. Users who cannot upgrade should use non-inline secrets where possible. Users may also desire to restrict API access to Grafana Agent with some combination of restricting the network interfaces Grafana Agent listens on through `http_listen_address` in the `server` block, configuring Grafana Agent to use HTTPS with client authentication, and/or using firewall rules to restrict external access to Grafana Agent's API.", "other": {"cve": {"id": "CVE-2021-41090", "sourceIdentifier": "security-advisories@github.com", "published": "2021-12-08T17:15:11.093", "lastModified": "2022-03-31T16:30:42.437", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Grafana Agent is a telemetry collector for sending metrics, logs, and trace data to the opinionated Grafana observability stack. Prior to versions 0.20.1 and 0.21.2, inline secrets defined within a metrics instance config are exposed in plaintext over two endpoints: metrics instance configs defined in the base YAML file are exposed at `/-/config` and metrics instance configs defined for the scraping service are exposed at `/agent/api/v1/configs/:key`. Inline secrets will be exposed to anyone being able to reach these endpoints. If HTTPS with client authentication is not configured, these endpoints are accessible to unauthenticated users. Secrets found in these sections are used for delivering metrics to a Prometheus Remote Write system, authenticating against a system for discovering Prometheus targets, and authenticating against a system for collecting metrics. This does not apply for non-inlined secrets, such as `*_file` based secrets. This issue is patched in Grafana Agent versions 0.20.1 and 0.21.2. A few workarounds are available. Users who cannot upgrade should use non-inline secrets where possible. Users may also desire to restrict API access to Grafana Agent with some combination of restricting the network interfaces Grafana Agent listens on through `http_listen_address` in the `server` block, configuring Grafana Agent to use HTTPS with client authentication, and/or using firewall rules to restrict external access to Grafana Agent's API."}, {"lang": "es", "value": "Grafana Agent es un recolector de telemetr\u00eda para enviar m\u00e9tricas, registros y datos de rastreo a la pila de observabilidad de Grafana. Antes de las versiones 0.20.1 y 0.21.2, los secretos en l\u00ednea definidos dentro de una configuraci\u00f3n de instancia de m\u00e9trica se exponen en texto plano en dos endpoints: las configuraciones de instancia de m\u00e9trica definidas en el archivo YAML base se exponen en \"/-/config\" y las configuraciones de instancia de m\u00e9trica definidas para el servicio de raspado se exponen en \"/agent/api/v1/configs/:key\". Los secretos en l\u00ednea son expuestos a cualquiera que pueda alcanzar estos endpoints. Si no se configura HTTPS con autenticaci\u00f3n de cliente, estos endpoints son accesibles a usuarios no autenticados. Los secretos que son encontrados en estas secciones son usados para entregar m\u00e9tricas a un sistema de escritura remota de Prometheus, autenticar contra un sistema para detectar objetivos de Prometheus y autenticar contra un sistema para recopilar m\u00e9tricas. Esto no es aplicado a los secretos no alineados, como los secretos basados en \"*_file\". Este problema se ha parcheado en Grafana Agent versiones 0.20.1 y 0.21.2. Se presentan algunas soluciones disponibles. Los usuarios que no puedan actualizares deber\u00edan usar secretos no basados en la l\u00ednea siempre que sea posible. Los usuarios tambi\u00e9n pueden querer restringir el acceso a la API de Grafana Agent con alguna combinaci\u00f3n de restricci\u00f3n de las interfaces de red que escucha Grafana Agent mediante \"http_listen_address\" en el bloque \"server\", configurando Grafana Agent para usar HTTPS con autenticaci\u00f3n de cliente, y/o usando reglas de firewall para restringir el acceso externo a la API de Grafana Agent"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 2.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-312"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:grafana:agent:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.14.0", "versionEndExcluding": "0.20.1", "matchCriteriaId": "753FABAF-758C-48AB-9A78-FBA2CBCC33A1"}, {"vulnerable": true, "criteria": "cpe:2.3:a:grafana:agent:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.21.0", "versionEndExcluding": "0.21.2", "matchCriteriaId": "BAC32D71-5F57-4917-A964-6FC490EE20AF"}]}]}], "references": [{"url": "https://github.com/grafana/agent/commit/af7fb01e31fe2d389e5f1c36b399ddc46b412b21", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/grafana/agent/pull/1152", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/grafana/agent/releases/tag/v0.20.1", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/grafana/agent/releases/tag/v0.21.2", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/grafana/agent/security/advisories/GHSA-9c4x-5hgq-q3wh", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20211229-0004/", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/grafana/agent/commit/af7fb01e31fe2d389e5f1c36b399ddc46b412b21"}}