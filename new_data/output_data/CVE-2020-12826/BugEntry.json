{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/fs/exec.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n/*\n * #!-checking implemented by tytso.\n */\n/*\n * Demand-loading implemented 01.12.91 - no need to read anything but\n * the header into memory. The inode of the executable is put into\n * \"current->executable\", and page faults do the actual loading. Clean.\n *\n * Once more I can proudly say that linux stood up to being changed: it\n * was less than 2 hours work to get demand-loading completely implemented.\n *\n * Demand loading changed July 1993 by Eric Youngdale.   Use mmap instead,\n * current->executable is only used by the procfs.  This allows a dispatch\n * table to check for several different types  of binary formats.  We keep\n * trying until we recognize the file or we run out of supported binary\n * formats.\n */\n\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/vmacache.h>\n#include <linux/stat.h>\n#include <linux/fcntl.h>\n#include <linux/swap.h>\n#include <linux/string.h>\n#include <linux/init.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/task.h>\n#include <linux/pagemap.h>\n#include <linux/perf_event.h>\n#include <linux/highmem.h>\n#include <linux/spinlock.h>\n#include <linux/key.h>\n#include <linux/personality.h>\n#include <linux/binfmts.h>\n#include <linux/utsname.h>\n#include <linux/pid_namespace.h>\n#include <linux/module.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/tsacct_kern.h>\n#include <linux/cn_proc.h>\n#include <linux/audit.h>\n#include <linux/tracehook.h>\n#include <linux/kmod.h>\n#include <linux/fsnotify.h>\n#include <linux/fs_struct.h>\n#include <linux/oom.h>\n#include <linux/compat.h>\n#include <linux/vmalloc.h>\n\n#include <linux/uaccess.h>\n#include <asm/mmu_context.h>\n#include <asm/tlb.h>\n\n#include <trace/events/task.h>\n#include \"internal.h\"\n\n#include <trace/events/sched.h>\n\nint suid_dumpable = 0;\n\nstatic LIST_HEAD(formats);\nstatic DEFINE_RWLOCK(binfmt_lock);\n\nvoid __register_binfmt(struct linux_binfmt * fmt, int insert)\n{\n\tBUG_ON(!fmt);\n\tif (WARN_ON(!fmt->load_binary))\n\t\treturn;\n\twrite_lock(&binfmt_lock);\n\tinsert ? list_add(&fmt->lh, &formats) :\n\t\t list_add_tail(&fmt->lh, &formats);\n\twrite_unlock(&binfmt_lock);\n}\n\nEXPORT_SYMBOL(__register_binfmt);\n\nvoid unregister_binfmt(struct linux_binfmt * fmt)\n{\n\twrite_lock(&binfmt_lock);\n\tlist_del(&fmt->lh);\n\twrite_unlock(&binfmt_lock);\n}\n\nEXPORT_SYMBOL(unregister_binfmt);\n\nstatic inline void put_binfmt(struct linux_binfmt * fmt)\n{\n\tmodule_put(fmt->module);\n}\n\nbool path_noexec(const struct path *path)\n{\n\treturn (path->mnt->mnt_flags & MNT_NOEXEC) ||\n\t       (path->mnt->mnt_sb->s_iflags & SB_I_NOEXEC);\n}\n\n#ifdef CONFIG_USELIB\n/*\n * Note that a shared library must be both readable and executable due to\n * security reasons.\n *\n * Also note that we take the address to load from from the file itself.\n */\nSYSCALL_DEFINE1(uselib, const char __user *, library)\n{\n\tstruct linux_binfmt *fmt;\n\tstruct file *file;\n\tstruct filename *tmp = getname(library);\n\tint error = PTR_ERR(tmp);\n\tstatic const struct open_flags uselib_flags = {\n\t\t.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,\n\t\t.acc_mode = MAY_READ | MAY_EXEC,\n\t\t.intent = LOOKUP_OPEN,\n\t\t.lookup_flags = LOOKUP_FOLLOW,\n\t};\n\n\tif (IS_ERR(tmp))\n\t\tgoto out;\n\n\tfile = do_filp_open(AT_FDCWD, tmp, &uselib_flags);\n\tputname(tmp);\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out;\n\n\terror = -EINVAL;\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\tgoto exit;\n\n\terror = -EACCES;\n\tif (path_noexec(&file->f_path))\n\t\tgoto exit;\n\n\tfsnotify_open(file);\n\n\terror = -ENOEXEC;\n\n\tread_lock(&binfmt_lock);\n\tlist_for_each_entry(fmt, &formats, lh) {\n\t\tif (!fmt->load_shlib)\n\t\t\tcontinue;\n\t\tif (!try_module_get(fmt->module))\n\t\t\tcontinue;\n\t\tread_unlock(&binfmt_lock);\n\t\terror = fmt->load_shlib(file);\n\t\tread_lock(&binfmt_lock);\n\t\tput_binfmt(fmt);\n\t\tif (error != -ENOEXEC)\n\t\t\tbreak;\n\t}\n\tread_unlock(&binfmt_lock);\nexit:\n\tfput(file);\nout:\n  \treturn error;\n}\n#endif /* #ifdef CONFIG_USELIB */\n\n#ifdef CONFIG_MMU\n/*\n * The nascent bprm->mm is not visible until exec_mmap() but it can\n * use a lot of memory, account these pages in current->mm temporary\n * for oom_badness()->get_mm_rss(). Once exec succeeds or fails, we\n * change the counter back via acct_arg_size(0).\n */\nstatic void acct_arg_size(struct linux_binprm *bprm, unsigned long pages)\n{\n\tstruct mm_struct *mm = current->mm;\n\tlong diff = (long)(pages - bprm->vma_pages);\n\n\tif (!mm || !diff)\n\t\treturn;\n\n\tbprm->vma_pages = pages;\n\tadd_mm_counter(mm, MM_ANONPAGES, diff);\n}\n\nstatic struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\tint ret;\n\tunsigned int gup_flags = FOLL_FORCE;\n\n#ifdef CONFIG_STACK_GROWSUP\n\tif (write) {\n\t\tret = expand_downwards(bprm->vma, pos);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t}\n#endif\n\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\t/*\n\t * We are doing an exec().  'current' is the process\n\t * doing the exec and bprm->mm is the new process's mm.\n\t */\n\tret = get_user_pages_remote(current, bprm->mm, pos, 1, gup_flags,\n\t\t\t&page, NULL, NULL);\n\tif (ret <= 0)\n\t\treturn NULL;\n\n\tif (write)\n\t\tacct_arg_size(bprm, vma_pages(bprm->vma));\n\n\treturn page;\n}\n\nstatic void put_arg_page(struct page *page)\n{\n\tput_page(page);\n}\n\nstatic void free_arg_pages(struct linux_binprm *bprm)\n{\n}\n\nstatic void flush_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tstruct page *page)\n{\n\tflush_cache_page(bprm->vma, pos, page_to_pfn(page));\n}\n\nstatic int __bprm_mm_init(struct linux_binprm *bprm)\n{\n\tint err;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = bprm->mm;\n\n\tbprm->vma = vma = vm_area_alloc(mm);\n\tif (!vma)\n\t\treturn -ENOMEM;\n\tvma_set_anonymous(vma);\n\n\tif (down_write_killable(&mm->mmap_sem)) {\n\t\terr = -EINTR;\n\t\tgoto err_free;\n\t}\n\n\t/*\n\t * Place the stack at the largest stack address the architecture\n\t * supports. Later, we'll move this to an appropriate place. We don't\n\t * use STACK_TOP because that can depend on attributes which aren't\n\t * configured yet.\n\t */\n\tBUILD_BUG_ON(VM_STACK_FLAGS & VM_STACK_INCOMPLETE_SETUP);\n\tvma->vm_end = STACK_TOP_MAX;\n\tvma->vm_start = vma->vm_end - PAGE_SIZE;\n\tvma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n\n\terr = insert_vm_struct(mm, vma);\n\tif (err)\n\t\tgoto err;\n\n\tmm->stack_vm = mm->total_vm = 1;\n\tup_write(&mm->mmap_sem);\n\tbprm->p = vma->vm_end - sizeof(void *);\n\treturn 0;\nerr:\n\tup_write(&mm->mmap_sem);\nerr_free:\n\tbprm->vma = NULL;\n\tvm_area_free(vma);\n\treturn err;\n}\n\nstatic bool valid_arg_len(struct linux_binprm *bprm, long len)\n{\n\treturn len <= MAX_ARG_STRLEN;\n}\n\n#else\n\nstatic inline void acct_arg_size(struct linux_binprm *bprm, unsigned long pages)\n{\n}\n\nstatic struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\n\tpage = bprm->page[pos / PAGE_SIZE];\n\tif (!page && write) {\n\t\tpage = alloc_page(GFP_HIGHUSER|__GFP_ZERO);\n\t\tif (!page)\n\t\t\treturn NULL;\n\t\tbprm->page[pos / PAGE_SIZE] = page;\n\t}\n\n\treturn page;\n}\n\nstatic void put_arg_page(struct page *page)\n{\n}\n\nstatic void free_arg_page(struct linux_binprm *bprm, int i)\n{\n\tif (bprm->page[i]) {\n\t\t__free_page(bprm->page[i]);\n\t\tbprm->page[i] = NULL;\n\t}\n}\n\nstatic void free_arg_pages(struct linux_binprm *bprm)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_ARG_PAGES; i++)\n\t\tfree_arg_page(bprm, i);\n}\n\nstatic void flush_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tstruct page *page)\n{\n}\n\nstatic int __bprm_mm_init(struct linux_binprm *bprm)\n{\n\tbprm->p = PAGE_SIZE * MAX_ARG_PAGES - sizeof(void *);\n\treturn 0;\n}\n\nstatic bool valid_arg_len(struct linux_binprm *bprm, long len)\n{\n\treturn len <= bprm->p;\n}\n\n#endif /* CONFIG_MMU */\n\n/*\n * Create a new mm_struct and populate it with a temporary stack\n * vm_area_struct.  We don't have enough context at this point to set the stack\n * flags, permissions, and offset, so we use temporary values.  We'll update\n * them later in setup_arg_pages().\n */\nstatic int bprm_mm_init(struct linux_binprm *bprm)\n{\n\tint err;\n\tstruct mm_struct *mm = NULL;\n\n\tbprm->mm = mm = mm_alloc();\n\terr = -ENOMEM;\n\tif (!mm)\n\t\tgoto err;\n\n\t/* Save current stack limit for all calculations made during exec. */\n\ttask_lock(current->group_leader);\n\tbprm->rlim_stack = current->signal->rlim[RLIMIT_STACK];\n\ttask_unlock(current->group_leader);\n\n\terr = __bprm_mm_init(bprm);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tif (mm) {\n\t\tbprm->mm = NULL;\n\t\tmmdrop(mm);\n\t}\n\n\treturn err;\n}\n\nstruct user_arg_ptr {\n#ifdef CONFIG_COMPAT\n\tbool is_compat;\n#endif\n\tunion {\n\t\tconst char __user *const __user *native;\n#ifdef CONFIG_COMPAT\n\t\tconst compat_uptr_t __user *compat;\n#endif\n\t} ptr;\n};\n\nstatic const char __user *get_user_arg_ptr(struct user_arg_ptr argv, int nr)\n{\n\tconst char __user *native;\n\n#ifdef CONFIG_COMPAT\n\tif (unlikely(argv.is_compat)) {\n\t\tcompat_uptr_t compat;\n\n\t\tif (get_user(compat, argv.ptr.compat + nr))\n\t\t\treturn ERR_PTR(-EFAULT);\n\n\t\treturn compat_ptr(compat);\n\t}\n#endif\n\n\tif (get_user(native, argv.ptr.native + nr))\n\t\treturn ERR_PTR(-EFAULT);\n\n\treturn native;\n}\n\n/*\n * count() counts the number of strings in array ARGV.\n */\nstatic int count(struct user_arg_ptr argv, int max)\n{\n\tint i = 0;\n\n\tif (argv.ptr.native != NULL) {\n\t\tfor (;;) {\n\t\t\tconst char __user *p = get_user_arg_ptr(argv, i);\n\n\t\t\tif (!p)\n\t\t\t\tbreak;\n\n\t\t\tif (IS_ERR(p))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tif (i >= max)\n\t\t\t\treturn -E2BIG;\n\t\t\t++i;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\treturn -ERESTARTNOHAND;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\treturn i;\n}\n\nstatic int prepare_arg_pages(struct linux_binprm *bprm,\n\t\t\tstruct user_arg_ptr argv, struct user_arg_ptr envp)\n{\n\tunsigned long limit, ptr_size;\n\n\tbprm->argc = count(argv, MAX_ARG_STRINGS);\n\tif (bprm->argc < 0)\n\t\treturn bprm->argc;\n\n\tbprm->envc = count(envp, MAX_ARG_STRINGS);\n\tif (bprm->envc < 0)\n\t\treturn bprm->envc;\n\n\t/*\n\t * Limit to 1/4 of the max stack size or 3/4 of _STK_LIM\n\t * (whichever is smaller) for the argv+env strings.\n\t * This ensures that:\n\t *  - the remaining binfmt code will not run out of stack space,\n\t *  - the program will have a reasonable amount of stack left\n\t *    to work from.\n\t */\n\tlimit = _STK_LIM / 4 * 3;\n\tlimit = min(limit, bprm->rlim_stack.rlim_cur / 4);\n\t/*\n\t * We've historically supported up to 32 pages (ARG_MAX)\n\t * of argument strings even with small stacks\n\t */\n\tlimit = max_t(unsigned long, limit, ARG_MAX);\n\t/*\n\t * We must account for the size of all the argv and envp pointers to\n\t * the argv and envp strings, since they will also take up space in\n\t * the stack. They aren't stored until much later when we can't\n\t * signal to the parent that the child has run out of stack space.\n\t * Instead, calculate it here so it's possible to fail gracefully.\n\t */\n\tptr_size = (bprm->argc + bprm->envc) * sizeof(void *);\n\tif (limit <= ptr_size)\n\t\treturn -E2BIG;\n\tlimit -= ptr_size;\n\n\tbprm->argmin = bprm->p - limit;\n\treturn 0;\n}\n\n/*\n * 'copy_strings()' copies argument/environment strings from the old\n * processes's memory to the new process's stack.  The call to get_user_pages()\n * ensures the destination page is created and not swapped out.\n */\nstatic int copy_strings(int argc, struct user_arg_ptr argv,\n\t\t\tstruct linux_binprm *bprm)\n{\n\tstruct page *kmapped_page = NULL;\n\tchar *kaddr = NULL;\n\tunsigned long kpos = 0;\n\tint ret;\n\n\twhile (argc-- > 0) {\n\t\tconst char __user *str;\n\t\tint len;\n\t\tunsigned long pos;\n\n\t\tret = -EFAULT;\n\t\tstr = get_user_arg_ptr(argv, argc);\n\t\tif (IS_ERR(str))\n\t\t\tgoto out;\n\n\t\tlen = strnlen_user(str, MAX_ARG_STRLEN);\n\t\tif (!len)\n\t\t\tgoto out;\n\n\t\tret = -E2BIG;\n\t\tif (!valid_arg_len(bprm, len))\n\t\t\tgoto out;\n\n\t\t/* We're going to work our way backwords. */\n\t\tpos = bprm->p;\n\t\tstr += len;\n\t\tbprm->p -= len;\n#ifdef CONFIG_MMU\n\t\tif (bprm->p < bprm->argmin)\n\t\t\tgoto out;\n#endif\n\n\t\twhile (len > 0) {\n\t\t\tint offset, bytes_to_copy;\n\n\t\t\tif (fatal_signal_pending(current)) {\n\t\t\t\tret = -ERESTARTNOHAND;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcond_resched();\n\n\t\t\toffset = pos % PAGE_SIZE;\n\t\t\tif (offset == 0)\n\t\t\t\toffset = PAGE_SIZE;\n\n\t\t\tbytes_to_copy = offset;\n\t\t\tif (bytes_to_copy > len)\n\t\t\t\tbytes_to_copy = len;\n\n\t\t\toffset -= bytes_to_copy;\n\t\t\tpos -= bytes_to_copy;\n\t\t\tstr -= bytes_to_copy;\n\t\t\tlen -= bytes_to_copy;\n\n\t\t\tif (!kmapped_page || kpos != (pos & PAGE_MASK)) {\n\t\t\t\tstruct page *page;\n\n\t\t\t\tpage = get_arg_page(bprm, pos, 1);\n\t\t\t\tif (!page) {\n\t\t\t\t\tret = -E2BIG;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tif (kmapped_page) {\n\t\t\t\t\tflush_kernel_dcache_page(kmapped_page);\n\t\t\t\t\tkunmap(kmapped_page);\n\t\t\t\t\tput_arg_page(kmapped_page);\n\t\t\t\t}\n\t\t\t\tkmapped_page = page;\n\t\t\t\tkaddr = kmap(kmapped_page);\n\t\t\t\tkpos = pos & PAGE_MASK;\n\t\t\t\tflush_arg_page(bprm, kpos, kmapped_page);\n\t\t\t}\n\t\t\tif (copy_from_user(kaddr+offset, str, bytes_to_copy)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tret = 0;\nout:\n\tif (kmapped_page) {\n\t\tflush_kernel_dcache_page(kmapped_page);\n\t\tkunmap(kmapped_page);\n\t\tput_arg_page(kmapped_page);\n\t}\n\treturn ret;\n}\n\n/*\n * Like copy_strings, but get argv and its values from kernel memory.\n */\nint copy_strings_kernel(int argc, const char *const *__argv,\n\t\t\tstruct linux_binprm *bprm)\n{\n\tint r;\n\tmm_segment_t oldfs = get_fs();\n\tstruct user_arg_ptr argv = {\n\t\t.ptr.native = (const char __user *const  __user *)__argv,\n\t};\n\n\tset_fs(KERNEL_DS);\n\tr = copy_strings(argc, argv, bprm);\n\tset_fs(oldfs);\n\n\treturn r;\n}\nEXPORT_SYMBOL(copy_strings_kernel);\n\n#ifdef CONFIG_MMU\n\n/*\n * During bprm_mm_init(), we create a temporary stack at STACK_TOP_MAX.  Once\n * the binfmt code determines where the new stack should reside, we shift it to\n * its final location.  The process proceeds as follows:\n *\n * 1) Use shift to calculate the new vma endpoints.\n * 2) Extend vma to cover both the old and new ranges.  This ensures the\n *    arguments passed to subsequent functions are consistent.\n * 3) Move vma's page tables to the new range.\n * 4) Free up any cleared pgd range.\n * 5) Shrink the vma to cover only the new range.\n */\nstatic int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long old_start = vma->vm_start;\n\tunsigned long old_end = vma->vm_end;\n\tunsigned long length = old_end - old_start;\n\tunsigned long new_start = old_start - shift;\n\tunsigned long new_end = old_end - shift;\n\tstruct mmu_gather tlb;\n\n\tBUG_ON(new_start > new_end);\n\n\t/*\n\t * ensure there are no vmas between where we want to go\n\t * and where we are\n\t */\n\tif (vma != find_vma(mm, new_start))\n\t\treturn -EFAULT;\n\n\t/*\n\t * cover the whole range: [new_start, old_end)\n\t */\n\tif (vma_adjust(vma, new_start, old_end, vma->vm_pgoff, NULL))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * move the page tables downwards, on failure we rely on\n\t * process cleanup to remove whatever mess we made.\n\t */\n\tif (length != move_page_tables(vma, old_start,\n\t\t\t\t       vma, new_start, length, false))\n\t\treturn -ENOMEM;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm, old_start, old_end);\n\tif (new_end > old_start) {\n\t\t/*\n\t\t * when the old and new regions overlap clear from new_end.\n\t\t */\n\t\tfree_pgd_range(&tlb, new_end, old_end, new_end,\n\t\t\tvma->vm_next ? vma->vm_next->vm_start : USER_PGTABLES_CEILING);\n\t} else {\n\t\t/*\n\t\t * otherwise, clean from old_start; this is done to not touch\n\t\t * the address space in [new_end, old_start) some architectures\n\t\t * have constraints on va-space that make this illegal (IA64) -\n\t\t * for the others its just a little faster.\n\t\t */\n\t\tfree_pgd_range(&tlb, old_start, old_end, new_end,\n\t\t\tvma->vm_next ? vma->vm_next->vm_start : USER_PGTABLES_CEILING);\n\t}\n\ttlb_finish_mmu(&tlb, old_start, old_end);\n\n\t/*\n\t * Shrink the vma to just the new range.  Always succeeds.\n\t */\n\tvma_adjust(vma, new_start, new_end, vma->vm_pgoff, NULL);\n\n\treturn 0;\n}\n\n/*\n * Finalizes the stack vm_area_struct. The flags and permissions are updated,\n * the stack is optionally relocated, and some extra space is added.\n */\nint setup_arg_pages(struct linux_binprm *bprm,\n\t\t    unsigned long stack_top,\n\t\t    int executable_stack)\n{\n\tunsigned long ret;\n\tunsigned long stack_shift;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = bprm->vma;\n\tstruct vm_area_struct *prev = NULL;\n\tunsigned long vm_flags;\n\tunsigned long stack_base;\n\tunsigned long stack_size;\n\tunsigned long stack_expand;\n\tunsigned long rlim_stack;\n\n#ifdef CONFIG_STACK_GROWSUP\n\t/* Limit stack size */\n\tstack_base = bprm->rlim_stack.rlim_max;\n\tif (stack_base > STACK_SIZE_MAX)\n\t\tstack_base = STACK_SIZE_MAX;\n\n\t/* Add space for stack randomization. */\n\tstack_base += (STACK_RND_MASK << PAGE_SHIFT);\n\n\t/* Make sure we didn't let the argument array grow too large. */\n\tif (vma->vm_end - vma->vm_start > stack_base)\n\t\treturn -ENOMEM;\n\n\tstack_base = PAGE_ALIGN(stack_top - stack_base);\n\n\tstack_shift = vma->vm_start - stack_base;\n\tmm->arg_start = bprm->p - stack_shift;\n\tbprm->p = vma->vm_end - stack_shift;\n#else\n\tstack_top = arch_align_stack(stack_top);\n\tstack_top = PAGE_ALIGN(stack_top);\n\n\tif (unlikely(stack_top < mmap_min_addr) ||\n\t    unlikely(vma->vm_end - vma->vm_start >= stack_top - mmap_min_addr))\n\t\treturn -ENOMEM;\n\n\tstack_shift = vma->vm_end - stack_top;\n\n\tbprm->p -= stack_shift;\n\tmm->arg_start = bprm->p;\n#endif\n\n\tif (bprm->loader)\n\t\tbprm->loader -= stack_shift;\n\tbprm->exec -= stack_shift;\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tvm_flags = VM_STACK_FLAGS;\n\n\t/*\n\t * Adjust stack execute permissions; explicitly enable for\n\t * EXSTACK_ENABLE_X, disable for EXSTACK_DISABLE_X and leave alone\n\t * (arch default) otherwise.\n\t */\n\tif (unlikely(executable_stack == EXSTACK_ENABLE_X))\n\t\tvm_flags |= VM_EXEC;\n\telse if (executable_stack == EXSTACK_DISABLE_X)\n\t\tvm_flags &= ~VM_EXEC;\n\tvm_flags |= mm->def_flags;\n\tvm_flags |= VM_STACK_INCOMPLETE_SETUP;\n\n\tret = mprotect_fixup(vma, &prev, vma->vm_start, vma->vm_end,\n\t\t\tvm_flags);\n\tif (ret)\n\t\tgoto out_unlock;\n\tBUG_ON(prev != vma);\n\n\tif (unlikely(vm_flags & VM_EXEC)) {\n\t\tpr_warn_once(\"process '%pD4' started with executable stack\\n\",\n\t\t\t     bprm->file);\n\t}\n\n\t/* Move stack pages down in memory. */\n\tif (stack_shift) {\n\t\tret = shift_arg_pages(vma, stack_shift);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* mprotect_fixup is overkill to remove the temporary stack flags */\n\tvma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;\n\n\tstack_expand = 131072UL; /* randomly 32*4k (or 2*64k) pages */\n\tstack_size = vma->vm_end - vma->vm_start;\n\t/*\n\t * Align this down to a page boundary as expand_stack\n\t * will align it up.\n\t */\n\trlim_stack = bprm->rlim_stack.rlim_cur & PAGE_MASK;\n#ifdef CONFIG_STACK_GROWSUP\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_start + rlim_stack;\n\telse\n\t\tstack_base = vma->vm_end + stack_expand;\n#else\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_end - rlim_stack;\n\telse\n\t\tstack_base = vma->vm_start - stack_expand;\n#endif\n\tcurrent->mm->start_stack = bprm->p;\n\tret = expand_stack(vma, stack_base);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(setup_arg_pages);\n\n#else\n\n/*\n * Transfer the program arguments and environment from the holding pages\n * onto the stack. The provided stack pointer is adjusted accordingly.\n */\nint transfer_args_to_stack(struct linux_binprm *bprm,\n\t\t\t   unsigned long *sp_location)\n{\n\tunsigned long index, stop, sp;\n\tint ret = 0;\n\n\tstop = bprm->p >> PAGE_SHIFT;\n\tsp = *sp_location;\n\n\tfor (index = MAX_ARG_PAGES - 1; index >= stop; index--) {\n\t\tunsigned int offset = index == stop ? bprm->p & ~PAGE_MASK : 0;\n\t\tchar *src = kmap(bprm->page[index]) + offset;\n\t\tsp -= PAGE_SIZE - offset;\n\t\tif (copy_to_user((void *) sp, src, PAGE_SIZE - offset) != 0)\n\t\t\tret = -EFAULT;\n\t\tkunmap(bprm->page[index]);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t*sp_location = sp;\n\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(transfer_args_to_stack);\n\n#endif /* CONFIG_MMU */\n\nstatic struct file *do_open_execat(int fd, struct filename *name, int flags)\n{\n\tstruct file *file;\n\tint err;\n\tstruct open_flags open_exec_flags = {\n\t\t.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,\n\t\t.acc_mode = MAY_EXEC,\n\t\t.intent = LOOKUP_OPEN,\n\t\t.lookup_flags = LOOKUP_FOLLOW,\n\t};\n\n\tif ((flags & ~(AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH)) != 0)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (flags & AT_SYMLINK_NOFOLLOW)\n\t\topen_exec_flags.lookup_flags &= ~LOOKUP_FOLLOW;\n\tif (flags & AT_EMPTY_PATH)\n\t\topen_exec_flags.lookup_flags |= LOOKUP_EMPTY;\n\n\tfile = do_filp_open(fd, name, &open_exec_flags);\n\tif (IS_ERR(file))\n\t\tgoto out;\n\n\terr = -EACCES;\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\tgoto exit;\n\n\tif (path_noexec(&file->f_path))\n\t\tgoto exit;\n\n\terr = deny_write_access(file);\n\tif (err)\n\t\tgoto exit;\n\n\tif (name->name[0] != '\\0')\n\t\tfsnotify_open(file);\n\nout:\n\treturn file;\n\nexit:\n\tfput(file);\n\treturn ERR_PTR(err);\n}\n\nstruct file *open_exec(const char *name)\n{\n\tstruct filename *filename = getname_kernel(name);\n\tstruct file *f = ERR_CAST(filename);\n\n\tif (!IS_ERR(filename)) {\n\t\tf = do_open_execat(AT_FDCWD, filename, 0);\n\t\tputname(filename);\n\t}\n\treturn f;\n}\nEXPORT_SYMBOL(open_exec);\n\nint kernel_read_file(struct file *file, void **buf, loff_t *size,\n\t\t     loff_t max_size, enum kernel_read_file_id id)\n{\n\tloff_t i_size, pos;\n\tssize_t bytes = 0;\n\tint ret;\n\n\tif (!S_ISREG(file_inode(file)->i_mode) || max_size < 0)\n\t\treturn -EINVAL;\n\n\tret = deny_write_access(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = security_kernel_read_file(file, id);\n\tif (ret)\n\t\tgoto out;\n\n\ti_size = i_size_read(file_inode(file));\n\tif (i_size <= 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (i_size > SIZE_MAX || (max_size > 0 && i_size > max_size)) {\n\t\tret = -EFBIG;\n\t\tgoto out;\n\t}\n\n\tif (id != READING_FIRMWARE_PREALLOC_BUFFER)\n\t\t*buf = vmalloc(i_size);\n\tif (!*buf) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpos = 0;\n\twhile (pos < i_size) {\n\t\tbytes = kernel_read(file, *buf + pos, i_size - pos, &pos);\n\t\tif (bytes < 0) {\n\t\t\tret = bytes;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tif (bytes == 0)\n\t\t\tbreak;\n\t}\n\n\tif (pos != i_size) {\n\t\tret = -EIO;\n\t\tgoto out_free;\n\t}\n\n\tret = security_kernel_post_read_file(file, *buf, i_size, id);\n\tif (!ret)\n\t\t*size = pos;\n\nout_free:\n\tif (ret < 0) {\n\t\tif (id != READING_FIRMWARE_PREALLOC_BUFFER) {\n\t\t\tvfree(*buf);\n\t\t\t*buf = NULL;\n\t\t}\n\t}\n\nout:\n\tallow_write_access(file);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kernel_read_file);\n\nint kernel_read_file_from_path(const char *path, void **buf, loff_t *size,\n\t\t\t       loff_t max_size, enum kernel_read_file_id id)\n{\n\tstruct file *file;\n\tint ret;\n\n\tif (!path || !*path)\n\t\treturn -EINVAL;\n\n\tfile = filp_open(path, O_RDONLY, 0);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tret = kernel_read_file(file, buf, size, max_size, id);\n\tfput(file);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kernel_read_file_from_path);\n\nint kernel_read_file_from_fd(int fd, void **buf, loff_t *size, loff_t max_size,\n\t\t\t     enum kernel_read_file_id id)\n{\n\tstruct fd f = fdget(fd);\n\tint ret = -EBADF;\n\n\tif (!f.file)\n\t\tgoto out;\n\n\tret = kernel_read_file(f.file, buf, size, max_size, id);\nout:\n\tfdput(f);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kernel_read_file_from_fd);\n\nssize_t read_code(struct file *file, unsigned long addr, loff_t pos, size_t len)\n{\n\tssize_t res = vfs_read(file, (void __user *)addr, len, &pos);\n\tif (res > 0)\n\t\tflush_icache_range(addr, addr + len);\n\treturn res;\n}\nEXPORT_SYMBOL(read_code);\n\nstatic int exec_mmap(struct mm_struct *mm)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *old_mm, *active_mm;\n\n\t/* Notify parent that we're no longer interested in the old VM */\n\ttsk = current;\n\told_mm = current->mm;\n\texec_mm_release(tsk, old_mm);\n\n\tif (old_mm) {\n\t\tsync_mm_rss(old_mm);\n\t\t/*\n\t\t * Make sure that if there is a core dump in progress\n\t\t * for the old mm, we get out and die instead of going\n\t\t * through with the exec.  We must hold mmap_sem around\n\t\t * checking core_state and changing tsk->mm.\n\t\t */\n\t\tdown_read(&old_mm->mmap_sem);\n\t\tif (unlikely(old_mm->core_state)) {\n\t\t\tup_read(&old_mm->mmap_sem);\n\t\t\treturn -EINTR;\n\t\t}\n\t}\n\ttask_lock(tsk);\n\tactive_mm = tsk->active_mm;\n\tmembarrier_exec_mmap(mm);\n\ttsk->mm = mm;\n\ttsk->active_mm = mm;\n\tactivate_mm(active_mm, mm);\n\ttsk->mm->vmacache_seqnum = 0;\n\tvmacache_flush(tsk);\n\ttask_unlock(tsk);\n\tif (old_mm) {\n\t\tup_read(&old_mm->mmap_sem);\n\t\tBUG_ON(active_mm != old_mm);\n\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, old_mm);\n\t\tmm_update_next_owner(old_mm);\n\t\tmmput(old_mm);\n\t\treturn 0;\n\t}\n\tmmdrop(active_mm);\n\treturn 0;\n}\n\n/*\n * This function makes sure the current process has its own signal table,\n * so that flush_signal_handlers can later reset the handlers without\n * disturbing other processes.  (Other processes might share the signal\n * table via the CLONE_SIGHAND option to clone().)\n */\nstatic int de_thread(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tstruct sighand_struct *oldsighand = tsk->sighand;\n\tspinlock_t *lock = &oldsighand->siglock;\n\n\tif (thread_group_empty(tsk))\n\t\tgoto no_thread_group;\n\n\t/*\n\t * Kill all other threads in the thread group.\n\t */\n\tspin_lock_irq(lock);\n\tif (signal_group_exit(sig)) {\n\t\t/*\n\t\t * Another group action in progress, just\n\t\t * return so that the signal is processed.\n\t\t */\n\t\tspin_unlock_irq(lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tsig->group_exit_task = tsk;\n\tsig->notify_count = zap_other_threads(tsk);\n\tif (!thread_group_leader(tsk))\n\t\tsig->notify_count--;\n\n\twhile (sig->notify_count) {\n\t\t__set_current_state(TASK_KILLABLE);\n\t\tspin_unlock_irq(lock);\n\t\tschedule();\n\t\tif (__fatal_signal_pending(tsk))\n\t\t\tgoto killed;\n\t\tspin_lock_irq(lock);\n\t}\n\tspin_unlock_irq(lock);\n\n\t/*\n\t * At this point all other threads have exited, all we have to\n\t * do is to wait for the thread group leader to become inactive,\n\t * and to assume its PID:\n\t */\n\tif (!thread_group_leader(tsk)) {\n\t\tstruct task_struct *leader = tsk->group_leader;\n\n\t\tfor (;;) {\n\t\t\tcgroup_threadgroup_change_begin(tsk);\n\t\t\twrite_lock_irq(&tasklist_lock);\n\t\t\t/*\n\t\t\t * Do this under tasklist_lock to ensure that\n\t\t\t * exit_notify() can't miss ->group_exit_task\n\t\t\t */\n\t\t\tsig->notify_count = -1;\n\t\t\tif (likely(leader->exit_state))\n\t\t\t\tbreak;\n\t\t\t__set_current_state(TASK_KILLABLE);\n\t\t\twrite_unlock_irq(&tasklist_lock);\n\t\t\tcgroup_threadgroup_change_end(tsk);\n\t\t\tschedule();\n\t\t\tif (__fatal_signal_pending(tsk))\n\t\t\t\tgoto killed;\n\t\t}\n\n\t\t/*\n\t\t * The only record we have of the real-time age of a\n\t\t * process, regardless of execs it's done, is start_time.\n\t\t * All the past CPU time is accumulated in signal_struct\n\t\t * from sister threads now dead.  But in this non-leader\n\t\t * exec, nothing survives from the original leader thread,\n\t\t * whose birth marks the true age of this process now.\n\t\t * When we take on its identity by switching to its PID, we\n\t\t * also take its birthdate (always earlier than our own).\n\t\t */\n\t\ttsk->start_time = leader->start_time;\n\t\ttsk->start_boottime = leader->start_boottime;\n\n\t\tBUG_ON(!same_thread_group(leader, tsk));\n\t\tBUG_ON(has_group_leader_pid(tsk));\n\t\t/*\n\t\t * An exec() starts a new thread group with the\n\t\t * TGID of the previous thread group. Rehash the\n\t\t * two threads with a switched PID, and release\n\t\t * the former thread group leader:\n\t\t */\n\n\t\t/* Become a process group leader with the old leader's pid.\n\t\t * The old leader becomes a thread of the this thread group.\n\t\t * Note: The old leader also uses this pid until release_task\n\t\t *       is called.  Odd but simple and correct.\n\t\t */\n\t\ttsk->pid = leader->pid;\n\t\tchange_pid(tsk, PIDTYPE_PID, task_pid(leader));\n\t\ttransfer_pid(leader, tsk, PIDTYPE_TGID);\n\t\ttransfer_pid(leader, tsk, PIDTYPE_PGID);\n\t\ttransfer_pid(leader, tsk, PIDTYPE_SID);\n\n\t\tlist_replace_rcu(&leader->tasks, &tsk->tasks);\n\t\tlist_replace_init(&leader->sibling, &tsk->sibling);\n\n\t\ttsk->group_leader = tsk;\n\t\tleader->group_leader = tsk;\n\n\t\ttsk->exit_signal = SIGCHLD;\n\t\tleader->exit_signal = -1;\n\n\t\tBUG_ON(leader->exit_state != EXIT_ZOMBIE);\n\t\tleader->exit_state = EXIT_DEAD;\n\n\t\t/*\n\t\t * We are going to release_task()->ptrace_unlink() silently,\n\t\t * the tracer can sleep in do_wait(). EXIT_DEAD guarantees\n\t\t * the tracer wont't block again waiting for this thread.\n\t\t */\n\t\tif (unlikely(leader->ptrace))\n\t\t\t__wake_up_parent(leader, leader->parent);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tcgroup_threadgroup_change_end(tsk);\n\n\t\trelease_task(leader);\n\t}\n\n\tsig->group_exit_task = NULL;\n\tsig->notify_count = 0;\n\nno_thread_group:\n\t/* we have changed execution domain */\n\ttsk->exit_signal = SIGCHLD;\n\n#ifdef CONFIG_POSIX_TIMERS\n\texit_itimers(sig);\n\tflush_itimer_signals();\n#endif\n\n\tif (refcount_read(&oldsighand->count) != 1) {\n\t\tstruct sighand_struct *newsighand;\n\t\t/*\n\t\t * This ->sighand is shared with the CLONE_SIGHAND\n\t\t * but not CLONE_THREAD task, switch to the new one.\n\t\t */\n\t\tnewsighand = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);\n\t\tif (!newsighand)\n\t\t\treturn -ENOMEM;\n\n\t\trefcount_set(&newsighand->count, 1);\n\t\tmemcpy(newsighand->action, oldsighand->action,\n\t\t       sizeof(newsighand->action));\n\n\t\twrite_lock_irq(&tasklist_lock);\n\t\tspin_lock(&oldsighand->siglock);\n\t\trcu_assign_pointer(tsk->sighand, newsighand);\n\t\tspin_unlock(&oldsighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\n\t\t__cleanup_sighand(oldsighand);\n\t}\n\n\tBUG_ON(!thread_group_leader(tsk));\n\treturn 0;\n\nkilled:\n\t/* protects against exit_notify() and __exit_signal() */\n\tread_lock(&tasklist_lock);\n\tsig->group_exit_task = NULL;\n\tsig->notify_count = 0;\n\tread_unlock(&tasklist_lock);\n\treturn -EAGAIN;\n}\n\nchar *__get_task_comm(char *buf, size_t buf_size, struct task_struct *tsk)\n{\n\ttask_lock(tsk);\n\tstrncpy(buf, tsk->comm, buf_size);\n\ttask_unlock(tsk);\n\treturn buf;\n}\nEXPORT_SYMBOL_GPL(__get_task_comm);\n\n/*\n * These functions flushes out all traces of the currently running executable\n * so that a new one can be started\n */\n\nvoid __set_task_comm(struct task_struct *tsk, const char *buf, bool exec)\n{\n\ttask_lock(tsk);\n\ttrace_task_rename(tsk, buf);\n\tstrlcpy(tsk->comm, buf, sizeof(tsk->comm));\n\ttask_unlock(tsk);\n\tperf_event_comm(tsk, exec);\n}\n\n/*\n * Calling this is the point of no return. None of the failures will be\n * seen by userspace since either the process is already taking a fatal\n * signal (via de_thread() or coredump), or will have SEGV raised\n * (after exec_mmap()) by search_binary_handlers (see below).\n */\nint flush_old_exec(struct linux_binprm * bprm)\n{\n\tint retval;\n\n\t/*\n\t * Make sure we have a private signal table and that\n\t * we are unassociated from the previous thread group.\n\t */\n\tretval = de_thread(current);\n\tif (retval)\n\t\tgoto out;\n\n\t/*\n\t * Must be called _before_ exec_mmap() as bprm->mm is\n\t * not visibile until then. This also enables the update\n\t * to be lockless.\n\t */\n\tset_mm_exe_file(bprm->mm, bprm->file);\n\n\t/*\n\t * Release all of the old mmap stuff\n\t */\n\tacct_arg_size(bprm, 0);\n\tretval = exec_mmap(bprm->mm);\n\tif (retval)\n\t\tgoto out;\n\n\t/*\n\t * After clearing bprm->mm (to mark that current is using the\n\t * prepared mm now), we have nothing left of the original\n\t * process. If anything from here on returns an error, the check\n\t * in search_binary_handler() will SEGV current.\n\t */\n\tbprm->mm = NULL;\n\n\tset_fs(USER_DS);\n\tcurrent->flags &= ~(PF_RANDOMIZE | PF_FORKNOEXEC | PF_KTHREAD |\n\t\t\t\t\tPF_NOFREEZE | PF_NO_SETAFFINITY);\n\tflush_thread();\n\tcurrent->personality &= ~bprm->per_clear;\n\n\t/*\n\t * We have to apply CLOEXEC before we change whether the process is\n\t * dumpable (in setup_new_exec) to avoid a race with a process in userspace\n\t * trying to access the should-be-closed file descriptors of a process\n\t * undergoing exec(2).\n\t */\n\tdo_close_on_exec(current->files);\n\treturn 0;\n\nout:\n\treturn retval;\n}\nEXPORT_SYMBOL(flush_old_exec);\n\nvoid would_dump(struct linux_binprm *bprm, struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\tif (inode_permission(inode, MAY_READ) < 0) {\n\t\tstruct user_namespace *old, *user_ns;\n\t\tbprm->interp_flags |= BINPRM_FLAGS_ENFORCE_NONDUMP;\n\n\t\t/* Ensure mm->user_ns contains the executable */\n\t\tuser_ns = old = bprm->mm->user_ns;\n\t\twhile ((user_ns != &init_user_ns) &&\n\t\t       !privileged_wrt_inode_uidgid(user_ns, inode))\n\t\t\tuser_ns = user_ns->parent;\n\n\t\tif (old != user_ns) {\n\t\t\tbprm->mm->user_ns = get_user_ns(user_ns);\n\t\t\tput_user_ns(old);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(would_dump);\n\nvoid setup_new_exec(struct linux_binprm * bprm)\n{\n\t/*\n\t * Once here, prepare_binrpm() will not be called any more, so\n\t * the final state of setuid/setgid/fscaps can be merged into the\n\t * secureexec flag.\n\t */\n\tbprm->secureexec |= bprm->cap_elevated;\n\n\tif (bprm->secureexec) {\n\t\t/* Make sure parent cannot signal privileged process. */\n\t\tcurrent->pdeath_signal = 0;\n\n\t\t/*\n\t\t * For secureexec, reset the stack limit to sane default to\n\t\t * avoid bad behavior from the prior rlimits. This has to\n\t\t * happen before arch_pick_mmap_layout(), which examines\n\t\t * RLIMIT_STACK, but after the point of no return to avoid\n\t\t * needing to clean up the change on failure.\n\t\t */\n\t\tif (bprm->rlim_stack.rlim_cur > _STK_LIM)\n\t\t\tbprm->rlim_stack.rlim_cur = _STK_LIM;\n\t}\n\n\tarch_pick_mmap_layout(current->mm, &bprm->rlim_stack);\n\n\tcurrent->sas_ss_sp = current->sas_ss_size = 0;\n\n\t/*\n\t * Figure out dumpability. Note that this checking only of current\n\t * is wrong, but userspace depends on it. This should be testing\n\t * bprm->secureexec instead.\n\t */\n\tif (bprm->interp_flags & BINPRM_FLAGS_ENFORCE_NONDUMP ||\n\t    !(uid_eq(current_euid(), current_uid()) &&\n\t      gid_eq(current_egid(), current_gid())))\n\t\tset_dumpable(current->mm, suid_dumpable);\n\telse\n\t\tset_dumpable(current->mm, SUID_DUMP_USER);\n\n\tarch_setup_new_exec();\n\tperf_event_exec();\n\t__set_task_comm(current, kbasename(bprm->filename), true);\n\n\t/* Set the new mm task size. We have to do that late because it may\n\t * depend on TIF_32BIT which is only updated in flush_thread() on\n\t * some architectures like powerpc\n\t */\n\tcurrent->mm->task_size = TASK_SIZE;\n\n\t/* An exec changes our domain. We are no longer part of the thread\n\t   group */\n\tcurrent->self_exec_id++;\n\tflush_signal_handlers(current, 0);\n}\nEXPORT_SYMBOL(setup_new_exec);\n\n/* Runs immediately before start_thread() takes over. */\nvoid finalize_exec(struct linux_binprm *bprm)\n{\n\t/* Store any stack rlimit changes before starting thread. */\n\ttask_lock(current->group_leader);\n\tcurrent->signal->rlim[RLIMIT_STACK] = bprm->rlim_stack;\n\ttask_unlock(current->group_leader);\n}\nEXPORT_SYMBOL(finalize_exec);\n\n/*\n * Prepare credentials and lock ->cred_guard_mutex.\n * install_exec_creds() commits the new creds and drops the lock.\n * Or, if exec fails before, free_bprm() should release ->cred and\n * and unlock.\n */\nstatic int prepare_bprm_creds(struct linux_binprm *bprm)\n{\n\tif (mutex_lock_interruptible(&current->signal->cred_guard_mutex))\n\t\treturn -ERESTARTNOINTR;\n\n\tbprm->cred = prepare_exec_creds();\n\tif (likely(bprm->cred))\n\t\treturn 0;\n\n\tmutex_unlock(&current->signal->cred_guard_mutex);\n\treturn -ENOMEM;\n}\n\nstatic void free_bprm(struct linux_binprm *bprm)\n{\n\tfree_arg_pages(bprm);\n\tif (bprm->cred) {\n\t\tmutex_unlock(&current->signal->cred_guard_mutex);\n\t\tabort_creds(bprm->cred);\n\t}\n\tif (bprm->file) {\n\t\tallow_write_access(bprm->file);\n\t\tfput(bprm->file);\n\t}\n\t/* If a binfmt changed the interp, free it. */\n\tif (bprm->interp != bprm->filename)\n\t\tkfree(bprm->interp);\n\tkfree(bprm);\n}\n\nint bprm_change_interp(const char *interp, struct linux_binprm *bprm)\n{\n\t/* If a binfmt changed the interp, free it first. */\n\tif (bprm->interp != bprm->filename)\n\t\tkfree(bprm->interp);\n\tbprm->interp = kstrdup(interp, GFP_KERNEL);\n\tif (!bprm->interp)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nEXPORT_SYMBOL(bprm_change_interp);\n\n/*\n * install the new credentials for this executable\n */\nvoid install_exec_creds(struct linux_binprm *bprm)\n{\n\tsecurity_bprm_committing_creds(bprm);\n\n\tcommit_creds(bprm->cred);\n\tbprm->cred = NULL;\n\n\t/*\n\t * Disable monitoring for regular users\n\t * when executing setuid binaries. Must\n\t * wait until new credentials are committed\n\t * by commit_creds() above\n\t */\n\tif (get_dumpable(current->mm) != SUID_DUMP_USER)\n\t\tperf_event_exit_task(current);\n\t/*\n\t * cred_guard_mutex must be held at least to this point to prevent\n\t * ptrace_attach() from altering our determination of the task's\n\t * credentials; any time after this it may be unlocked.\n\t */\n\tsecurity_bprm_committed_creds(bprm);\n\tmutex_unlock(&current->signal->cred_guard_mutex);\n}\nEXPORT_SYMBOL(install_exec_creds);\n\n/*\n * determine how safe it is to execute the proposed program\n * - the caller must hold ->cred_guard_mutex to protect against\n *   PTRACE_ATTACH or seccomp thread-sync\n */\nstatic void check_unsafe_exec(struct linux_binprm *bprm)\n{\n\tstruct task_struct *p = current, *t;\n\tunsigned n_fs;\n\n\tif (p->ptrace)\n\t\tbprm->unsafe |= LSM_UNSAFE_PTRACE;\n\n\t/*\n\t * This isn't strictly necessary, but it makes it harder for LSMs to\n\t * mess up.\n\t */\n\tif (task_no_new_privs(current))\n\t\tbprm->unsafe |= LSM_UNSAFE_NO_NEW_PRIVS;\n\n\tt = p;\n\tn_fs = 1;\n\tspin_lock(&p->fs->lock);\n\trcu_read_lock();\n\twhile_each_thread(p, t) {\n\t\tif (t->fs == p->fs)\n\t\t\tn_fs++;\n\t}\n\trcu_read_unlock();\n\n\tif (p->fs->users > n_fs)\n\t\tbprm->unsafe |= LSM_UNSAFE_SHARE;\n\telse\n\t\tp->fs->in_exec = 1;\n\tspin_unlock(&p->fs->lock);\n}\n\nstatic void bprm_fill_uid(struct linux_binprm *bprm)\n{\n\tstruct inode *inode;\n\tunsigned int mode;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\t/*\n\t * Since this can be called multiple times (via prepare_binprm),\n\t * we must clear any previous work done when setting set[ug]id\n\t * bits from any earlier bprm->file uses (for example when run\n\t * first for a setuid script then again for its interpreter).\n\t */\n\tbprm->cred->euid = current_euid();\n\tbprm->cred->egid = current_egid();\n\n\tif (!mnt_may_suid(bprm->file->f_path.mnt))\n\t\treturn;\n\n\tif (task_no_new_privs(current))\n\t\treturn;\n\n\tinode = bprm->file->f_path.dentry->d_inode;\n\tmode = READ_ONCE(inode->i_mode);\n\tif (!(mode & (S_ISUID|S_ISGID)))\n\t\treturn;\n\n\t/* Be careful if suid/sgid is set */\n\tinode_lock(inode);\n\n\t/* reload atomically mode/uid/gid now that lock held */\n\tmode = inode->i_mode;\n\tuid = inode->i_uid;\n\tgid = inode->i_gid;\n\tinode_unlock(inode);\n\n\t/* We ignore suid/sgid if there are no mappings for them in the ns */\n\tif (!kuid_has_mapping(bprm->cred->user_ns, uid) ||\n\t\t !kgid_has_mapping(bprm->cred->user_ns, gid))\n\t\treturn;\n\n\tif (mode & S_ISUID) {\n\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\tbprm->cred->euid = uid;\n\t}\n\n\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\tbprm->cred->egid = gid;\n\t}\n}\n\n/*\n * Fill the binprm structure from the inode.\n * Check permissions, then read the first BINPRM_BUF_SIZE bytes\n *\n * This may be called multiple times for binary chains (scripts for example).\n */\nint prepare_binprm(struct linux_binprm *bprm)\n{\n\tint retval;\n\tloff_t pos = 0;\n\n\tbprm_fill_uid(bprm);\n\n\t/* fill in binprm security blob */\n\tretval = security_bprm_set_creds(bprm);\n\tif (retval)\n\t\treturn retval;\n\tbprm->called_set_creds = 1;\n\n\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\treturn kernel_read(bprm->file, bprm->buf, BINPRM_BUF_SIZE, &pos);\n}\n\nEXPORT_SYMBOL(prepare_binprm);\n\n/*\n * Arguments are '\\0' separated strings found at the location bprm->p\n * points to; chop off the first by relocating brpm->p to right after\n * the first '\\0' encountered.\n */\nint remove_arg_zero(struct linux_binprm *bprm)\n{\n\tint ret = 0;\n\tunsigned long offset;\n\tchar *kaddr;\n\tstruct page *page;\n\n\tif (!bprm->argc)\n\t\treturn 0;\n\n\tdo {\n\t\toffset = bprm->p & ~PAGE_MASK;\n\t\tpage = get_arg_page(bprm, bprm->p, 0);\n\t\tif (!page) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkaddr = kmap_atomic(page);\n\n\t\tfor (; offset < PAGE_SIZE && kaddr[offset];\n\t\t\t\toffset++, bprm->p++)\n\t\t\t;\n\n\t\tkunmap_atomic(kaddr);\n\t\tput_arg_page(page);\n\t} while (offset == PAGE_SIZE);\n\n\tbprm->p++;\n\tbprm->argc--;\n\tret = 0;\n\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(remove_arg_zero);\n\n#define printable(c) (((c)=='\\t') || ((c)=='\\n') || (0x20<=(c) && (c)<=0x7e))\n/*\n * cycle the list of binary formats handler, until one recognizes the image\n */\nint search_binary_handler(struct linux_binprm *bprm)\n{\n\tbool need_retry = IS_ENABLED(CONFIG_MODULES);\n\tstruct linux_binfmt *fmt;\n\tint retval;\n\n\t/* This allows 4 levels of binfmt rewrites before failing hard. */\n\tif (bprm->recursion_depth > 5)\n\t\treturn -ELOOP;\n\n\tretval = security_bprm_check(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = -ENOENT;\n retry:\n\tread_lock(&binfmt_lock);\n\tlist_for_each_entry(fmt, &formats, lh) {\n\t\tif (!try_module_get(fmt->module))\n\t\t\tcontinue;\n\t\tread_unlock(&binfmt_lock);\n\n\t\tbprm->recursion_depth++;\n\t\tretval = fmt->load_binary(bprm);\n\t\tbprm->recursion_depth--;\n\n\t\tread_lock(&binfmt_lock);\n\t\tput_binfmt(fmt);\n\t\tif (retval < 0 && !bprm->mm) {\n\t\t\t/* we got to flush_old_exec() and failed after it */\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\tforce_sigsegv(SIGSEGV);\n\t\t\treturn retval;\n\t\t}\n\t\tif (retval != -ENOEXEC || !bprm->file) {\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\treturn retval;\n\t\t}\n\t}\n\tread_unlock(&binfmt_lock);\n\n\tif (need_retry) {\n\t\tif (printable(bprm->buf[0]) && printable(bprm->buf[1]) &&\n\t\t    printable(bprm->buf[2]) && printable(bprm->buf[3]))\n\t\t\treturn retval;\n\t\tif (request_module(\"binfmt-%04x\", *(ushort *)(bprm->buf + 2)) < 0)\n\t\t\treturn retval;\n\t\tneed_retry = false;\n\t\tgoto retry;\n\t}\n\n\treturn retval;\n}\nEXPORT_SYMBOL(search_binary_handler);\n\nstatic int exec_binprm(struct linux_binprm *bprm)\n{\n\tpid_t old_pid, old_vpid;\n\tint ret;\n\n\t/* Need to fetch pid before load_binary changes it */\n\told_pid = current->pid;\n\trcu_read_lock();\n\told_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));\n\trcu_read_unlock();\n\n\tret = search_binary_handler(bprm);\n\tif (ret >= 0) {\n\t\taudit_bprm(bprm);\n\t\ttrace_sched_process_exec(current, old_pid, bprm);\n\t\tptrace_event(PTRACE_EVENT_EXEC, old_vpid);\n\t\tproc_exec_connector(current);\n\t}\n\n\treturn ret;\n}\n\n/*\n * sys_execve() executes a new program.\n */\nstatic int __do_execve_file(int fd, struct filename *filename,\n\t\t\t    struct user_arg_ptr argv,\n\t\t\t    struct user_arg_ptr envp,\n\t\t\t    int flags, struct file *file)\n{\n\tchar *pathbuf = NULL;\n\tstruct linux_binprm *bprm;\n\tstruct files_struct *displaced;\n\tint retval;\n\n\tif (IS_ERR(filename))\n\t\treturn PTR_ERR(filename);\n\n\t/*\n\t * We move the actual failure in case of RLIMIT_NPROC excess from\n\t * set*uid() to execve() because too many poorly written programs\n\t * don't check setuid() return code.  Here we additionally recheck\n\t * whether NPROC limit is still exceeded.\n\t */\n\tif ((current->flags & PF_NPROC_EXCEEDED) &&\n\t    atomic_read(&current_user()->processes) > rlimit(RLIMIT_NPROC)) {\n\t\tretval = -EAGAIN;\n\t\tgoto out_ret;\n\t}\n\n\t/* We're below the limit (still or again), so we don't want to make\n\t * further execve() calls fail. */\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\tgoto out_ret;\n\n\tretval = -ENOMEM;\n\tbprm = kzalloc(sizeof(*bprm), GFP_KERNEL);\n\tif (!bprm)\n\t\tgoto out_files;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_free;\n\n\tcheck_unsafe_exec(bprm);\n\tcurrent->in_execve = 1;\n\n\tif (!file)\n\t\tfile = do_open_execat(fd, filename, flags);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\tif (!filename) {\n\t\tbprm->filename = \"none\";\n\t} else if (fd == AT_FDCWD || filename->name[0] == '/') {\n\t\tbprm->filename = filename->name;\n\t} else {\n\t\tif (filename->name[0] == '\\0')\n\t\t\tpathbuf = kasprintf(GFP_KERNEL, \"/dev/fd/%d\", fd);\n\t\telse\n\t\t\tpathbuf = kasprintf(GFP_KERNEL, \"/dev/fd/%d/%s\",\n\t\t\t\t\t    fd, filename->name);\n\t\tif (!pathbuf) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out_unmark;\n\t\t}\n\t\t/*\n\t\t * Record that a name derived from an O_CLOEXEC fd will be\n\t\t * inaccessible after exec. Relies on having exclusive access to\n\t\t * current->files (due to unshare_files above).\n\t\t */\n\t\tif (close_on_exec(fd, rcu_dereference_raw(current->files->fdt)))\n\t\t\tbprm->interp_flags |= BINPRM_FLAGS_PATH_INACCESSIBLE;\n\t\tbprm->filename = pathbuf;\n\t}\n\tbprm->interp = bprm->filename;\n\n\tretval = bprm_mm_init(bprm);\n\tif (retval)\n\t\tgoto out_unmark;\n\n\tretval = prepare_arg_pages(bprm, argv, envp);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tbprm->exec = bprm->p;\n\tretval = copy_strings(bprm->envc, envp, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings(bprm->argc, argv, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\twould_dump(bprm, bprm->file);\n\n\tretval = exec_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\trseq_execve(current);\n\tacct_update_integrals(current);\n\ttask_numa_free(current, false);\n\tfree_bprm(bprm);\n\tkfree(pathbuf);\n\tif (filename)\n\t\tputname(filename);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\tif (bprm->mm) {\n\t\tacct_arg_size(bprm, 0);\n\t\tmmput(bprm->mm);\n\t}\n\nout_unmark:\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_free:\n\tfree_bprm(bprm);\n\tkfree(pathbuf);\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\nout_ret:\n\tif (filename)\n\t\tputname(filename);\n\treturn retval;\n}\n\nstatic int do_execveat_common(int fd, struct filename *filename,\n\t\t\t      struct user_arg_ptr argv,\n\t\t\t      struct user_arg_ptr envp,\n\t\t\t      int flags)\n{\n\treturn __do_execve_file(fd, filename, argv, envp, flags, NULL);\n}\n\nint do_execve_file(struct file *file, void *__argv, void *__envp)\n{\n\tstruct user_arg_ptr argv = { .ptr.native = __argv };\n\tstruct user_arg_ptr envp = { .ptr.native = __envp };\n\n\treturn __do_execve_file(AT_FDCWD, NULL, argv, envp, 0, file);\n}\n\nint do_execve(struct filename *filename,\n\tconst char __user *const __user *__argv,\n\tconst char __user *const __user *__envp)\n{\n\tstruct user_arg_ptr argv = { .ptr.native = __argv };\n\tstruct user_arg_ptr envp = { .ptr.native = __envp };\n\treturn do_execveat_common(AT_FDCWD, filename, argv, envp, 0);\n}\n\nint do_execveat(int fd, struct filename *filename,\n\t\tconst char __user *const __user *__argv,\n\t\tconst char __user *const __user *__envp,\n\t\tint flags)\n{\n\tstruct user_arg_ptr argv = { .ptr.native = __argv };\n\tstruct user_arg_ptr envp = { .ptr.native = __envp };\n\n\treturn do_execveat_common(fd, filename, argv, envp, flags);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_do_execve(struct filename *filename,\n\tconst compat_uptr_t __user *__argv,\n\tconst compat_uptr_t __user *__envp)\n{\n\tstruct user_arg_ptr argv = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __argv,\n\t};\n\tstruct user_arg_ptr envp = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __envp,\n\t};\n\treturn do_execveat_common(AT_FDCWD, filename, argv, envp, 0);\n}\n\nstatic int compat_do_execveat(int fd, struct filename *filename,\n\t\t\t      const compat_uptr_t __user *__argv,\n\t\t\t      const compat_uptr_t __user *__envp,\n\t\t\t      int flags)\n{\n\tstruct user_arg_ptr argv = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __argv,\n\t};\n\tstruct user_arg_ptr envp = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __envp,\n\t};\n\treturn do_execveat_common(fd, filename, argv, envp, flags);\n}\n#endif\n\nvoid set_binfmt(struct linux_binfmt *new)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif (mm->binfmt)\n\t\tmodule_put(mm->binfmt->module);\n\n\tmm->binfmt = new;\n\tif (new)\n\t\t__module_get(new->module);\n}\nEXPORT_SYMBOL(set_binfmt);\n\n/*\n * set_dumpable stores three-value SUID_DUMP_* into mm->flags.\n */\nvoid set_dumpable(struct mm_struct *mm, int value)\n{\n\tif (WARN_ON((unsigned)value > SUID_DUMP_ROOT))\n\t\treturn;\n\n\tset_mask_bits(&mm->flags, MMF_DUMPABLE_MASK, value);\n}\n\nSYSCALL_DEFINE3(execve,\n\t\tconst char __user *, filename,\n\t\tconst char __user *const __user *, argv,\n\t\tconst char __user *const __user *, envp)\n{\n\treturn do_execve(getname(filename), argv, envp);\n}\n\nSYSCALL_DEFINE5(execveat,\n\t\tint, fd, const char __user *, filename,\n\t\tconst char __user *const __user *, argv,\n\t\tconst char __user *const __user *, envp,\n\t\tint, flags)\n{\n\tint lookup_flags = (flags & AT_EMPTY_PATH) ? LOOKUP_EMPTY : 0;\n\n\treturn do_execveat(fd,\n\t\t\t   getname_flags(filename, lookup_flags, NULL),\n\t\t\t   argv, envp, flags);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(execve, const char __user *, filename,\n\tconst compat_uptr_t __user *, argv,\n\tconst compat_uptr_t __user *, envp)\n{\n\treturn compat_do_execve(getname(filename), argv, envp);\n}\n\nCOMPAT_SYSCALL_DEFINE5(execveat, int, fd,\n\t\t       const char __user *, filename,\n\t\t       const compat_uptr_t __user *, argv,\n\t\t       const compat_uptr_t __user *, envp,\n\t\t       int,  flags)\n{\n\tint lookup_flags = (flags & AT_EMPTY_PATH) ? LOOKUP_EMPTY : 0;\n\n\treturn compat_do_execveat(fd,\n\t\t\t\t  getname_flags(filename, lookup_flags, NULL),\n\t\t\t\t  argv, envp, flags);\n}\n#endif\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_SCHED_H\n#define _LINUX_SCHED_H\n\n/*\n * Define 'struct task_struct' and provide the main scheduler\n * APIs (schedule(), wakeup variants, etc.)\n */\n\n#include <uapi/linux/sched.h>\n\n#include <asm/current.h>\n\n#include <linux/pid.h>\n#include <linux/sem.h>\n#include <linux/shm.h>\n#include <linux/kcov.h>\n#include <linux/mutex.h>\n#include <linux/plist.h>\n#include <linux/hrtimer.h>\n#include <linux/seccomp.h>\n#include <linux/nodemask.h>\n#include <linux/rcupdate.h>\n#include <linux/refcount.h>\n#include <linux/resource.h>\n#include <linux/latencytop.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/types.h>\n#include <linux/signal_types.h>\n#include <linux/mm_types_task.h>\n#include <linux/task_io_accounting.h>\n#include <linux/posix-timers.h>\n#include <linux/rseq.h>\n\n/* task_struct member predeclarations (sorted alphabetically): */\nstruct audit_context;\nstruct backing_dev_info;\nstruct bio_list;\nstruct blk_plug;\nstruct capture_control;\nstruct cfs_rq;\nstruct fs_struct;\nstruct futex_pi_state;\nstruct io_context;\nstruct mempolicy;\nstruct nameidata;\nstruct nsproxy;\nstruct perf_event_context;\nstruct pid_namespace;\nstruct pipe_inode_info;\nstruct rcu_node;\nstruct reclaim_state;\nstruct robust_list_head;\nstruct root_domain;\nstruct rq;\nstruct sched_attr;\nstruct sched_param;\nstruct seq_file;\nstruct sighand_struct;\nstruct signal_struct;\nstruct task_delay_info;\nstruct task_group;\n\n/*\n * Task state bitmask. NOTE! These bits are also\n * encoded in fs/proc/array.c: get_task_state().\n *\n * We have two separate sets of flags: task->state\n * is about runnability, while task->exit_state are\n * about the task exiting. Confusing, but this way\n * modifying one set can't modify the other one by\n * mistake.\n */\n\n/* Used in tsk->state: */\n#define TASK_RUNNING\t\t\t0x0000\n#define TASK_INTERRUPTIBLE\t\t0x0001\n#define TASK_UNINTERRUPTIBLE\t\t0x0002\n#define __TASK_STOPPED\t\t\t0x0004\n#define __TASK_TRACED\t\t\t0x0008\n/* Used in tsk->exit_state: */\n#define EXIT_DEAD\t\t\t0x0010\n#define EXIT_ZOMBIE\t\t\t0x0020\n#define EXIT_TRACE\t\t\t(EXIT_ZOMBIE | EXIT_DEAD)\n/* Used in tsk->state again: */\n#define TASK_PARKED\t\t\t0x0040\n#define TASK_DEAD\t\t\t0x0080\n#define TASK_WAKEKILL\t\t\t0x0100\n#define TASK_WAKING\t\t\t0x0200\n#define TASK_NOLOAD\t\t\t0x0400\n#define TASK_NEW\t\t\t0x0800\n#define TASK_STATE_MAX\t\t\t0x1000\n\n/* Convenience macros for the sake of set_current_state: */\n#define TASK_KILLABLE\t\t\t(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)\n#define TASK_STOPPED\t\t\t(TASK_WAKEKILL | __TASK_STOPPED)\n#define TASK_TRACED\t\t\t(TASK_WAKEKILL | __TASK_TRACED)\n\n#define TASK_IDLE\t\t\t(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)\n\n/* Convenience macros for the sake of wake_up(): */\n#define TASK_NORMAL\t\t\t(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)\n\n/* get_task_state(): */\n#define TASK_REPORT\t\t\t(TASK_RUNNING | TASK_INTERRUPTIBLE | \\\n\t\t\t\t\t TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\\n\t\t\t\t\t __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \\\n\t\t\t\t\t TASK_PARKED)\n\n#define task_is_traced(task)\t\t((task->state & __TASK_TRACED) != 0)\n\n#define task_is_stopped(task)\t\t((task->state & __TASK_STOPPED) != 0)\n\n#define task_is_stopped_or_traced(task)\t((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)\n\n#define task_contributes_to_load(task)\t((task->state & TASK_UNINTERRUPTIBLE) != 0 && \\\n\t\t\t\t\t (task->flags & PF_FROZEN) == 0 && \\\n\t\t\t\t\t (task->state & TASK_NOLOAD) == 0)\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\n/*\n * Special states are those that do not use the normal wait-loop pattern. See\n * the comment with set_special_state().\n */\n#define is_special_task_state(state)\t\t\t\t\\\n\t((state) & (__TASK_STOPPED | __TASK_TRACED | TASK_PARKED | TASK_DEAD))\n\n#define __set_current_state(state_value)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tWARN_ON_ONCE(is_special_task_state(state_value));\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\\\n\t\tcurrent->state = (state_value);\t\t\t\\\n\t} while (0)\n\n#define set_current_state(state_value)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tWARN_ON_ONCE(is_special_task_state(state_value));\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\\\n\t\tsmp_store_mb(current->state, (state_value));\t\\\n\t} while (0)\n\n#define set_special_state(state_value)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tunsigned long flags; /* may shadow */\t\t\t\\\n\t\tWARN_ON_ONCE(!is_special_task_state(state_value));\t\\\n\t\traw_spin_lock_irqsave(&current->pi_lock, flags);\t\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\t\\\n\t\tcurrent->state = (state_value);\t\t\t\t\\\n\t\traw_spin_unlock_irqrestore(&current->pi_lock, flags);\t\\\n\t} while (0)\n#else\n/*\n * set_current_state() includes a barrier so that the write of current->state\n * is correctly serialised wrt the caller's subsequent test of whether to\n * actually sleep:\n *\n *   for (;;) {\n *\tset_current_state(TASK_UNINTERRUPTIBLE);\n *\tif (!need_sleep)\n *\t\tbreak;\n *\n *\tschedule();\n *   }\n *   __set_current_state(TASK_RUNNING);\n *\n * If the caller does not need such serialisation (because, for instance, the\n * condition test and condition change and wakeup are under the same lock) then\n * use __set_current_state().\n *\n * The above is typically ordered against the wakeup, which does:\n *\n *   need_sleep = false;\n *   wake_up_state(p, TASK_UNINTERRUPTIBLE);\n *\n * where wake_up_state() executes a full memory barrier before accessing the\n * task state.\n *\n * Wakeup will do: if (@state & p->state) p->state = TASK_RUNNING, that is,\n * once it observes the TASK_UNINTERRUPTIBLE store the waking CPU can issue a\n * TASK_RUNNING store which can collide with __set_current_state(TASK_RUNNING).\n *\n * However, with slightly different timing the wakeup TASK_RUNNING store can\n * also collide with the TASK_UNINTERRUPTIBLE store. Losing that store is not\n * a problem either because that will result in one extra go around the loop\n * and our @cond test will save the day.\n *\n * Also see the comments of try_to_wake_up().\n */\n#define __set_current_state(state_value)\t\t\t\t\\\n\tcurrent->state = (state_value)\n\n#define set_current_state(state_value)\t\t\t\t\t\\\n\tsmp_store_mb(current->state, (state_value))\n\n/*\n * set_special_state() should be used for those states when the blocking task\n * can not use the regular condition based wait-loop. In that case we must\n * serialize against wakeups such that any possible in-flight TASK_RUNNING stores\n * will not collide with our state change.\n */\n#define set_special_state(state_value)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tunsigned long flags; /* may shadow */\t\t\t\\\n\t\traw_spin_lock_irqsave(&current->pi_lock, flags);\t\\\n\t\tcurrent->state = (state_value);\t\t\t\t\\\n\t\traw_spin_unlock_irqrestore(&current->pi_lock, flags);\t\\\n\t} while (0)\n\n#endif\n\n/* Task command name length: */\n#define TASK_COMM_LEN\t\t\t16\n\nextern void scheduler_tick(void);\n\n#define\tMAX_SCHEDULE_TIMEOUT\t\tLONG_MAX\n\nextern long schedule_timeout(long timeout);\nextern long schedule_timeout_interruptible(long timeout);\nextern long schedule_timeout_killable(long timeout);\nextern long schedule_timeout_uninterruptible(long timeout);\nextern long schedule_timeout_idle(long timeout);\nasmlinkage void schedule(void);\nextern void schedule_preempt_disabled(void);\nasmlinkage void preempt_schedule_irq(void);\n\nextern int __must_check io_schedule_prepare(void);\nextern void io_schedule_finish(int token);\nextern long io_schedule_timeout(long timeout);\nextern void io_schedule(void);\n\n/**\n * struct prev_cputime - snapshot of system and user cputime\n * @utime: time spent in user mode\n * @stime: time spent in system mode\n * @lock: protects the above two fields\n *\n * Stores previous user/system time values such that we can guarantee\n * monotonicity.\n */\nstruct prev_cputime {\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n\tu64\t\t\t\tutime;\n\tu64\t\t\t\tstime;\n\traw_spinlock_t\t\t\tlock;\n#endif\n};\n\nenum vtime_state {\n\t/* Task is sleeping or running in a CPU with VTIME inactive: */\n\tVTIME_INACTIVE = 0,\n\t/* Task is idle */\n\tVTIME_IDLE,\n\t/* Task runs in kernelspace in a CPU with VTIME active: */\n\tVTIME_SYS,\n\t/* Task runs in userspace in a CPU with VTIME active: */\n\tVTIME_USER,\n\t/* Task runs as guests in a CPU with VTIME active: */\n\tVTIME_GUEST,\n};\n\nstruct vtime {\n\tseqcount_t\t\tseqcount;\n\tunsigned long long\tstarttime;\n\tenum vtime_state\tstate;\n\tunsigned int\t\tcpu;\n\tu64\t\t\tutime;\n\tu64\t\t\tstime;\n\tu64\t\t\tgtime;\n};\n\n/*\n * Utilization clamp constraints.\n * @UCLAMP_MIN:\tMinimum utilization\n * @UCLAMP_MAX:\tMaximum utilization\n * @UCLAMP_CNT:\tUtilization clamp constraints count\n */\nenum uclamp_id {\n\tUCLAMP_MIN = 0,\n\tUCLAMP_MAX,\n\tUCLAMP_CNT\n};\n\n#ifdef CONFIG_SMP\nextern struct root_domain def_root_domain;\nextern struct mutex sched_domains_mutex;\n#endif\n\nstruct sched_info {\n#ifdef CONFIG_SCHED_INFO\n\t/* Cumulative counters: */\n\n\t/* # of times we have run on this CPU: */\n\tunsigned long\t\t\tpcount;\n\n\t/* Time spent waiting on a runqueue: */\n\tunsigned long long\t\trun_delay;\n\n\t/* Timestamps: */\n\n\t/* When did we last run on a CPU? */\n\tunsigned long long\t\tlast_arrival;\n\n\t/* When were we last queued to run? */\n\tunsigned long long\t\tlast_queued;\n\n#endif /* CONFIG_SCHED_INFO */\n};\n\n/*\n * Integer metrics need fixed point arithmetic, e.g., sched/fair\n * has a few: load, load_avg, util_avg, freq, and capacity.\n *\n * We define a basic fixed point arithmetic range, and then formalize\n * all these metrics based on that basic range.\n */\n# define SCHED_FIXEDPOINT_SHIFT\t\t10\n# define SCHED_FIXEDPOINT_SCALE\t\t(1L << SCHED_FIXEDPOINT_SHIFT)\n\n/* Increase resolution of cpu_capacity calculations */\n# define SCHED_CAPACITY_SHIFT\t\tSCHED_FIXEDPOINT_SHIFT\n# define SCHED_CAPACITY_SCALE\t\t(1L << SCHED_CAPACITY_SHIFT)\n\nstruct load_weight {\n\tunsigned long\t\t\tweight;\n\tu32\t\t\t\tinv_weight;\n};\n\n/**\n * struct util_est - Estimation utilization of FAIR tasks\n * @enqueued: instantaneous estimated utilization of a task/cpu\n * @ewma:     the Exponential Weighted Moving Average (EWMA)\n *            utilization of a task\n *\n * Support data structure to track an Exponential Weighted Moving Average\n * (EWMA) of a FAIR task's utilization. New samples are added to the moving\n * average each time a task completes an activation. Sample's weight is chosen\n * so that the EWMA will be relatively insensitive to transient changes to the\n * task's workload.\n *\n * The enqueued attribute has a slightly different meaning for tasks and cpus:\n * - task:   the task's util_avg at last task dequeue time\n * - cfs_rq: the sum of util_est.enqueued for each RUNNABLE task on that CPU\n * Thus, the util_est.enqueued of a task represents the contribution on the\n * estimated utilization of the CPU where that task is currently enqueued.\n *\n * Only for tasks we track a moving average of the past instantaneous\n * estimated utilization. This allows to absorb sporadic drops in utilization\n * of an otherwise almost periodic task.\n */\nstruct util_est {\n\tunsigned int\t\t\tenqueued;\n\tunsigned int\t\t\tewma;\n#define UTIL_EST_WEIGHT_SHIFT\t\t2\n} __attribute__((__aligned__(sizeof(u64))));\n\n/*\n * The load_avg/util_avg accumulates an infinite geometric series\n * (see __update_load_avg() in kernel/sched/fair.c).\n *\n * [load_avg definition]\n *\n *   load_avg = runnable% * scale_load_down(load)\n *\n * where runnable% is the time ratio that a sched_entity is runnable.\n * For cfs_rq, it is the aggregated load_avg of all runnable and\n * blocked sched_entities.\n *\n * [util_avg definition]\n *\n *   util_avg = running% * SCHED_CAPACITY_SCALE\n *\n * where running% is the time ratio that a sched_entity is running on\n * a CPU. For cfs_rq, it is the aggregated util_avg of all runnable\n * and blocked sched_entities.\n *\n * load_avg and util_avg don't direcly factor frequency scaling and CPU\n * capacity scaling. The scaling is done through the rq_clock_pelt that\n * is used for computing those signals (see update_rq_clock_pelt())\n *\n * N.B., the above ratios (runnable% and running%) themselves are in the\n * range of [0, 1]. To do fixed point arithmetics, we therefore scale them\n * to as large a range as necessary. This is for example reflected by\n * util_avg's SCHED_CAPACITY_SCALE.\n *\n * [Overflow issue]\n *\n * The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities\n * with the highest load (=88761), always runnable on a single cfs_rq,\n * and should not overflow as the number already hits PID_MAX_LIMIT.\n *\n * For all other cases (including 32-bit kernels), struct load_weight's\n * weight will overflow first before we do, because:\n *\n *    Max(load_avg) <= Max(load.weight)\n *\n * Then it is the load_weight's responsibility to consider overflow\n * issues.\n */\nstruct sched_avg {\n\tu64\t\t\t\tlast_update_time;\n\tu64\t\t\t\tload_sum;\n\tu64\t\t\t\trunnable_load_sum;\n\tu32\t\t\t\tutil_sum;\n\tu32\t\t\t\tperiod_contrib;\n\tunsigned long\t\t\tload_avg;\n\tunsigned long\t\t\trunnable_load_avg;\n\tunsigned long\t\t\tutil_avg;\n\tstruct util_est\t\t\tutil_est;\n} ____cacheline_aligned;\n\nstruct sched_statistics {\n#ifdef CONFIG_SCHEDSTATS\n\tu64\t\t\t\twait_start;\n\tu64\t\t\t\twait_max;\n\tu64\t\t\t\twait_count;\n\tu64\t\t\t\twait_sum;\n\tu64\t\t\t\tiowait_count;\n\tu64\t\t\t\tiowait_sum;\n\n\tu64\t\t\t\tsleep_start;\n\tu64\t\t\t\tsleep_max;\n\ts64\t\t\t\tsum_sleep_runtime;\n\n\tu64\t\t\t\tblock_start;\n\tu64\t\t\t\tblock_max;\n\tu64\t\t\t\texec_max;\n\tu64\t\t\t\tslice_max;\n\n\tu64\t\t\t\tnr_migrations_cold;\n\tu64\t\t\t\tnr_failed_migrations_affine;\n\tu64\t\t\t\tnr_failed_migrations_running;\n\tu64\t\t\t\tnr_failed_migrations_hot;\n\tu64\t\t\t\tnr_forced_migrations;\n\n\tu64\t\t\t\tnr_wakeups;\n\tu64\t\t\t\tnr_wakeups_sync;\n\tu64\t\t\t\tnr_wakeups_migrate;\n\tu64\t\t\t\tnr_wakeups_local;\n\tu64\t\t\t\tnr_wakeups_remote;\n\tu64\t\t\t\tnr_wakeups_affine;\n\tu64\t\t\t\tnr_wakeups_affine_attempts;\n\tu64\t\t\t\tnr_wakeups_passive;\n\tu64\t\t\t\tnr_wakeups_idle;\n#endif\n};\n\nstruct sched_entity {\n\t/* For load-balancing: */\n\tstruct load_weight\t\tload;\n\tunsigned long\t\t\trunnable_weight;\n\tstruct rb_node\t\t\trun_node;\n\tstruct list_head\t\tgroup_node;\n\tunsigned int\t\t\ton_rq;\n\n\tu64\t\t\t\texec_start;\n\tu64\t\t\t\tsum_exec_runtime;\n\tu64\t\t\t\tvruntime;\n\tu64\t\t\t\tprev_sum_exec_runtime;\n\n\tu64\t\t\t\tnr_migrations;\n\n\tstruct sched_statistics\t\tstatistics;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tint\t\t\t\tdepth;\n\tstruct sched_entity\t\t*parent;\n\t/* rq on which this entity is (to be) queued: */\n\tstruct cfs_rq\t\t\t*cfs_rq;\n\t/* rq \"owned\" by this entity/group: */\n\tstruct cfs_rq\t\t\t*my_q;\n#endif\n\n#ifdef CONFIG_SMP\n\t/*\n\t * Per entity load average tracking.\n\t *\n\t * Put into separate cache line so it does not\n\t * collide with read-mostly values above.\n\t */\n\tstruct sched_avg\t\tavg;\n#endif\n};\n\nstruct sched_rt_entity {\n\tstruct list_head\t\trun_list;\n\tunsigned long\t\t\ttimeout;\n\tunsigned long\t\t\twatchdog_stamp;\n\tunsigned int\t\t\ttime_slice;\n\tunsigned short\t\t\ton_rq;\n\tunsigned short\t\t\ton_list;\n\n\tstruct sched_rt_entity\t\t*back;\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity\t\t*parent;\n\t/* rq on which this entity is (to be) queued: */\n\tstruct rt_rq\t\t\t*rt_rq;\n\t/* rq \"owned\" by this entity/group: */\n\tstruct rt_rq\t\t\t*my_q;\n#endif\n} __randomize_layout;\n\nstruct sched_dl_entity {\n\tstruct rb_node\t\t\trb_node;\n\n\t/*\n\t * Original scheduling parameters. Copied here from sched_attr\n\t * during sched_setattr(), they will remain the same until\n\t * the next sched_setattr().\n\t */\n\tu64\t\t\t\tdl_runtime;\t/* Maximum runtime for each instance\t*/\n\tu64\t\t\t\tdl_deadline;\t/* Relative deadline of each instance\t*/\n\tu64\t\t\t\tdl_period;\t/* Separation of two instances (period) */\n\tu64\t\t\t\tdl_bw;\t\t/* dl_runtime / dl_period\t\t*/\n\tu64\t\t\t\tdl_density;\t/* dl_runtime / dl_deadline\t\t*/\n\n\t/*\n\t * Actual scheduling parameters. Initialized with the values above,\n\t * they are continuously updated during task execution. Note that\n\t * the remaining runtime could be < 0 in case we are in overrun.\n\t */\n\ts64\t\t\t\truntime;\t/* Remaining runtime for this instance\t*/\n\tu64\t\t\t\tdeadline;\t/* Absolute deadline for this instance\t*/\n\tunsigned int\t\t\tflags;\t\t/* Specifying the scheduler behaviour\t*/\n\n\t/*\n\t * Some bool flags:\n\t *\n\t * @dl_throttled tells if we exhausted the runtime. If so, the\n\t * task has to wait for a replenishment to be performed at the\n\t * next firing of dl_timer.\n\t *\n\t * @dl_boosted tells if we are boosted due to DI. If so we are\n\t * outside bandwidth enforcement mechanism (but only until we\n\t * exit the critical section);\n\t *\n\t * @dl_yielded tells if task gave up the CPU before consuming\n\t * all its available runtime during the last job.\n\t *\n\t * @dl_non_contending tells if the task is inactive while still\n\t * contributing to the active utilization. In other words, it\n\t * indicates if the inactive timer has been armed and its handler\n\t * has not been executed yet. This flag is useful to avoid race\n\t * conditions between the inactive timer handler and the wakeup\n\t * code.\n\t *\n\t * @dl_overrun tells if the task asked to be informed about runtime\n\t * overruns.\n\t */\n\tunsigned int\t\t\tdl_throttled      : 1;\n\tunsigned int\t\t\tdl_boosted        : 1;\n\tunsigned int\t\t\tdl_yielded        : 1;\n\tunsigned int\t\t\tdl_non_contending : 1;\n\tunsigned int\t\t\tdl_overrun\t  : 1;\n\n\t/*\n\t * Bandwidth enforcement timer. Each -deadline task has its\n\t * own bandwidth to be enforced, thus we need one timer per task.\n\t */\n\tstruct hrtimer\t\t\tdl_timer;\n\n\t/*\n\t * Inactive timer, responsible for decreasing the active utilization\n\t * at the \"0-lag time\". When a -deadline task blocks, it contributes\n\t * to GRUB's active utilization until the \"0-lag time\", hence a\n\t * timer is needed to decrease the active utilization at the correct\n\t * time.\n\t */\n\tstruct hrtimer inactive_timer;\n};\n\n#ifdef CONFIG_UCLAMP_TASK\n/* Number of utilization clamp buckets (shorter alias) */\n#define UCLAMP_BUCKETS CONFIG_UCLAMP_BUCKETS_COUNT\n\n/*\n * Utilization clamp for a scheduling entity\n * @value:\t\tclamp value \"assigned\" to a se\n * @bucket_id:\t\tbucket index corresponding to the \"assigned\" value\n * @active:\t\tthe se is currently refcounted in a rq's bucket\n * @user_defined:\tthe requested clamp value comes from user-space\n *\n * The bucket_id is the index of the clamp bucket matching the clamp value\n * which is pre-computed and stored to avoid expensive integer divisions from\n * the fast path.\n *\n * The active bit is set whenever a task has got an \"effective\" value assigned,\n * which can be different from the clamp value \"requested\" from user-space.\n * This allows to know a task is refcounted in the rq's bucket corresponding\n * to the \"effective\" bucket_id.\n *\n * The user_defined bit is set whenever a task has got a task-specific clamp\n * value requested from userspace, i.e. the system defaults apply to this task\n * just as a restriction. This allows to relax default clamps when a less\n * restrictive task-specific value has been requested, thus allowing to\n * implement a \"nice\" semantic. For example, a task running with a 20%\n * default boost can still drop its own boosting to 0%.\n */\nstruct uclamp_se {\n\tunsigned int value\t\t: bits_per(SCHED_CAPACITY_SCALE);\n\tunsigned int bucket_id\t\t: bits_per(UCLAMP_BUCKETS);\n\tunsigned int active\t\t: 1;\n\tunsigned int user_defined\t: 1;\n};\n#endif /* CONFIG_UCLAMP_TASK */\n\nunion rcu_special {\n\tstruct {\n\t\tu8\t\t\tblocked;\n\t\tu8\t\t\tneed_qs;\n\t\tu8\t\t\texp_hint; /* Hint for performance. */\n\t\tu8\t\t\tdeferred_qs;\n\t} b; /* Bits. */\n\tu32 s; /* Set of bits. */\n};\n\nenum perf_event_task_context {\n\tperf_invalid_context = -1,\n\tperf_hw_context = 0,\n\tperf_sw_context,\n\tperf_nr_task_contexts,\n};\n\nstruct wake_q_node {\n\tstruct wake_q_node *next;\n};\n\nstruct task_struct {\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t/*\n\t * For reasons of header soup (see current_thread_info()), this\n\t * must be the first element of task_struct.\n\t */\n\tstruct thread_info\t\tthread_info;\n#endif\n\t/* -1 unrunnable, 0 runnable, >0 stopped: */\n\tvolatile long\t\t\tstate;\n\n\t/*\n\t * This begins the randomizable portion of task_struct. Only\n\t * scheduling-critical items should be added above here.\n\t */\n\trandomized_struct_fields_start\n\n\tvoid\t\t\t\t*stack;\n\trefcount_t\t\t\tusage;\n\t/* Per task flags (PF_*), defined further below: */\n\tunsigned int\t\t\tflags;\n\tunsigned int\t\t\tptrace;\n\n#ifdef CONFIG_SMP\n\tstruct llist_node\t\twake_entry;\n\tint\t\t\t\ton_cpu;\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t/* Current CPU: */\n\tunsigned int\t\t\tcpu;\n#endif\n\tunsigned int\t\t\twakee_flips;\n\tunsigned long\t\t\twakee_flip_decay_ts;\n\tstruct task_struct\t\t*last_wakee;\n\n\t/*\n\t * recent_used_cpu is initially set as the last CPU used by a task\n\t * that wakes affine another task. Waker/wakee relationships can\n\t * push tasks around a CPU where each wakeup moves to the next one.\n\t * Tracking a recently used CPU allows a quick search for a recently\n\t * used CPU that may be idle.\n\t */\n\tint\t\t\t\trecent_used_cpu;\n\tint\t\t\t\twake_cpu;\n#endif\n\tint\t\t\t\ton_rq;\n\n\tint\t\t\t\tprio;\n\tint\t\t\t\tstatic_prio;\n\tint\t\t\t\tnormal_prio;\n\tunsigned int\t\t\trt_priority;\n\n\tconst struct sched_class\t*sched_class;\n\tstruct sched_entity\t\tse;\n\tstruct sched_rt_entity\t\trt;\n#ifdef CONFIG_CGROUP_SCHED\n\tstruct task_group\t\t*sched_task_group;\n#endif\n\tstruct sched_dl_entity\t\tdl;\n\n#ifdef CONFIG_UCLAMP_TASK\n\t/* Clamp values requested for a scheduling entity */\n\tstruct uclamp_se\t\tuclamp_req[UCLAMP_CNT];\n\t/* Effective clamp values used for a scheduling entity */\n\tstruct uclamp_se\t\tuclamp[UCLAMP_CNT];\n#endif\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\t/* List of struct preempt_notifier: */\n\tstruct hlist_head\t\tpreempt_notifiers;\n#endif\n\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tunsigned int\t\t\tbtrace_seq;\n#endif\n\n\tunsigned int\t\t\tpolicy;\n\tint\t\t\t\tnr_cpus_allowed;\n\tconst cpumask_t\t\t\t*cpus_ptr;\n\tcpumask_t\t\t\tcpus_mask;\n\n#ifdef CONFIG_PREEMPT_RCU\n\tint\t\t\t\trcu_read_lock_nesting;\n\tunion rcu_special\t\trcu_read_unlock_special;\n\tstruct list_head\t\trcu_node_entry;\n\tstruct rcu_node\t\t\t*rcu_blocked_node;\n#endif /* #ifdef CONFIG_PREEMPT_RCU */\n\n#ifdef CONFIG_TASKS_RCU\n\tunsigned long\t\t\trcu_tasks_nvcsw;\n\tu8\t\t\t\trcu_tasks_holdout;\n\tu8\t\t\t\trcu_tasks_idx;\n\tint\t\t\t\trcu_tasks_idle_cpu;\n\tstruct list_head\t\trcu_tasks_holdout_list;\n#endif /* #ifdef CONFIG_TASKS_RCU */\n\n\tstruct sched_info\t\tsched_info;\n\n\tstruct list_head\t\ttasks;\n#ifdef CONFIG_SMP\n\tstruct plist_node\t\tpushable_tasks;\n\tstruct rb_node\t\t\tpushable_dl_tasks;\n#endif\n\n\tstruct mm_struct\t\t*mm;\n\tstruct mm_struct\t\t*active_mm;\n\n\t/* Per-thread vma caching: */\n\tstruct vmacache\t\t\tvmacache;\n\n#ifdef SPLIT_RSS_COUNTING\n\tstruct task_rss_stat\t\trss_stat;\n#endif\n\tint\t\t\t\texit_state;\n\tint\t\t\t\texit_code;\n\tint\t\t\t\texit_signal;\n\t/* The signal sent when the parent dies: */\n\tint\t\t\t\tpdeath_signal;\n\t/* JOBCTL_*, siglock protected: */\n\tunsigned long\t\t\tjobctl;\n\n\t/* Used for emulating ABI behavior of previous Linux versions: */\n\tunsigned int\t\t\tpersonality;\n\n\t/* Scheduler bits, serialized by scheduler locks: */\n\tunsigned\t\t\tsched_reset_on_fork:1;\n\tunsigned\t\t\tsched_contributes_to_load:1;\n\tunsigned\t\t\tsched_migrated:1;\n\tunsigned\t\t\tsched_remote_wakeup:1;\n#ifdef CONFIG_PSI\n\tunsigned\t\t\tsched_psi_wake_requeue:1;\n#endif\n\n\t/* Force alignment to the next boundary: */\n\tunsigned\t\t\t:0;\n\n\t/* Unserialized, strictly 'current' */\n\n\t/* Bit to tell LSMs we're in execve(): */\n\tunsigned\t\t\tin_execve:1;\n\tunsigned\t\t\tin_iowait:1;\n#ifndef TIF_RESTORE_SIGMASK\n\tunsigned\t\t\trestore_sigmask:1;\n#endif\n#ifdef CONFIG_MEMCG\n\tunsigned\t\t\tin_user_fault:1;\n#endif\n#ifdef CONFIG_COMPAT_BRK\n\tunsigned\t\t\tbrk_randomized:1;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/* disallow userland-initiated cgroup migration */\n\tunsigned\t\t\tno_cgroup_migration:1;\n\t/* task is frozen/stopped (used by the cgroup freezer) */\n\tunsigned\t\t\tfrozen:1;\n#endif\n#ifdef CONFIG_BLK_CGROUP\n\t/* to be used once the psi infrastructure lands upstream. */\n\tunsigned\t\t\tuse_memdelay:1;\n#endif\n\n\tunsigned long\t\t\tatomic_flags; /* Flags requiring atomic access. */\n\n\tstruct restart_block\t\trestart_block;\n\n\tpid_t\t\t\t\tpid;\n\tpid_t\t\t\t\ttgid;\n\n#ifdef CONFIG_STACKPROTECTOR\n\t/* Canary value for the -fstack-protector GCC feature: */\n\tunsigned long\t\t\tstack_canary;\n#endif\n\t/*\n\t * Pointers to the (original) parent process, youngest child, younger sibling,\n\t * older sibling, respectively.  (p->father can be replaced with\n\t * p->real_parent->pid)\n\t */\n\n\t/* Real parent process: */\n\tstruct task_struct __rcu\t*real_parent;\n\n\t/* Recipient of SIGCHLD, wait4() reports: */\n\tstruct task_struct __rcu\t*parent;\n\n\t/*\n\t * Children/sibling form the list of natural children:\n\t */\n\tstruct list_head\t\tchildren;\n\tstruct list_head\t\tsibling;\n\tstruct task_struct\t\t*group_leader;\n\n\t/*\n\t * 'ptraced' is the list of tasks this task is using ptrace() on.\n\t *\n\t * This includes both natural children and PTRACE_ATTACH targets.\n\t * 'ptrace_entry' is this task's link on the p->parent->ptraced list.\n\t */\n\tstruct list_head\t\tptraced;\n\tstruct list_head\t\tptrace_entry;\n\n\t/* PID/PID hash table linkage. */\n\tstruct pid\t\t\t*thread_pid;\n\tstruct hlist_node\t\tpid_links[PIDTYPE_MAX];\n\tstruct list_head\t\tthread_group;\n\tstruct list_head\t\tthread_node;\n\n\tstruct completion\t\t*vfork_done;\n\n\t/* CLONE_CHILD_SETTID: */\n\tint __user\t\t\t*set_child_tid;\n\n\t/* CLONE_CHILD_CLEARTID: */\n\tint __user\t\t\t*clear_child_tid;\n\n\tu64\t\t\t\tutime;\n\tu64\t\t\t\tstime;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tu64\t\t\t\tutimescaled;\n\tu64\t\t\t\tstimescaled;\n#endif\n\tu64\t\t\t\tgtime;\n\tstruct prev_cputime\t\tprev_cputime;\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tstruct vtime\t\t\tvtime;\n#endif\n\n#ifdef CONFIG_NO_HZ_FULL\n\tatomic_t\t\t\ttick_dep_mask;\n#endif\n\t/* Context switch counts: */\n\tunsigned long\t\t\tnvcsw;\n\tunsigned long\t\t\tnivcsw;\n\n\t/* Monotonic time in nsecs: */\n\tu64\t\t\t\tstart_time;\n\n\t/* Boot based time in nsecs: */\n\tu64\t\t\t\tstart_boottime;\n\n\t/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */\n\tunsigned long\t\t\tmin_flt;\n\tunsigned long\t\t\tmaj_flt;\n\n\t/* Empty if CONFIG_POSIX_CPUTIMERS=n */\n\tstruct posix_cputimers\t\tposix_cputimers;\n\n\t/* Process credentials: */\n\n\t/* Tracer's credentials at attach: */\n\tconst struct cred __rcu\t\t*ptracer_cred;\n\n\t/* Objective and real subjective task credentials (COW): */\n\tconst struct cred __rcu\t\t*real_cred;\n\n\t/* Effective (overridable) subjective task credentials (COW): */\n\tconst struct cred __rcu\t\t*cred;\n\n#ifdef CONFIG_KEYS\n\t/* Cached requested key. */\n\tstruct key\t\t\t*cached_requested_key;\n#endif\n\n\t/*\n\t * executable name, excluding path.\n\t *\n\t * - normally initialized setup_new_exec()\n\t * - access it with [gs]et_task_comm()\n\t * - lock it with task_lock()\n\t */\n\tchar\t\t\t\tcomm[TASK_COMM_LEN];\n\n\tstruct nameidata\t\t*nameidata;\n\n#ifdef CONFIG_SYSVIPC\n\tstruct sysv_sem\t\t\tsysvsem;\n\tstruct sysv_shm\t\t\tsysvshm;\n#endif\n#ifdef CONFIG_DETECT_HUNG_TASK\n\tunsigned long\t\t\tlast_switch_count;\n\tunsigned long\t\t\tlast_switch_time;\n#endif\n\t/* Filesystem information: */\n\tstruct fs_struct\t\t*fs;\n\n\t/* Open file information: */\n\tstruct files_struct\t\t*files;\n\n\t/* Namespaces: */\n\tstruct nsproxy\t\t\t*nsproxy;\n\n\t/* Signal handlers: */\n\tstruct signal_struct\t\t*signal;\n\tstruct sighand_struct __rcu\t\t*sighand;\n\tsigset_t\t\t\tblocked;\n\tsigset_t\t\t\treal_blocked;\n\t/* Restored if set_restore_sigmask() was used: */\n\tsigset_t\t\t\tsaved_sigmask;\n\tstruct sigpending\t\tpending;\n\tunsigned long\t\t\tsas_ss_sp;\n\tsize_t\t\t\t\tsas_ss_size;\n\tunsigned int\t\t\tsas_ss_flags;\n\n\tstruct callback_head\t\t*task_works;\n\n#ifdef CONFIG_AUDIT\n#ifdef CONFIG_AUDITSYSCALL\n\tstruct audit_context\t\t*audit_context;\n#endif\n\tkuid_t\t\t\t\tloginuid;\n\tunsigned int\t\t\tsessionid;\n#endif\n\tstruct seccomp\t\t\tseccomp;\n\n\t/* Thread group tracking: */\n\tu32\t\t\t\tparent_exec_id;\n\tu32\t\t\t\tself_exec_id;\n\n\t/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */\n\tspinlock_t\t\t\talloc_lock;\n\n\t/* Protection of the PI data structures: */\n\traw_spinlock_t\t\t\tpi_lock;\n\n\tstruct wake_q_node\t\twake_q;\n\n#ifdef CONFIG_RT_MUTEXES\n\t/* PI waiters blocked on a rt_mutex held by this task: */\n\tstruct rb_root_cached\t\tpi_waiters;\n\t/* Updated under owner's pi_lock and rq lock */\n\tstruct task_struct\t\t*pi_top_task;\n\t/* Deadlock detection and priority inheritance handling: */\n\tstruct rt_mutex_waiter\t\t*pi_blocked_on;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\t/* Mutex deadlock detection: */\n\tstruct mutex_waiter\t\t*blocked_on;\n#endif\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tint\t\t\t\tnon_block_count;\n#endif\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tunsigned int\t\t\tirq_events;\n\tunsigned long\t\t\thardirq_enable_ip;\n\tunsigned long\t\t\thardirq_disable_ip;\n\tunsigned int\t\t\thardirq_enable_event;\n\tunsigned int\t\t\thardirq_disable_event;\n\tint\t\t\t\thardirqs_enabled;\n\tint\t\t\t\thardirq_context;\n\tunsigned long\t\t\tsoftirq_disable_ip;\n\tunsigned long\t\t\tsoftirq_enable_ip;\n\tunsigned int\t\t\tsoftirq_disable_event;\n\tunsigned int\t\t\tsoftirq_enable_event;\n\tint\t\t\t\tsoftirqs_enabled;\n\tint\t\t\t\tsoftirq_context;\n#endif\n\n#ifdef CONFIG_LOCKDEP\n# define MAX_LOCK_DEPTH\t\t\t48UL\n\tu64\t\t\t\tcurr_chain_key;\n\tint\t\t\t\tlockdep_depth;\n\tunsigned int\t\t\tlockdep_recursion;\n\tstruct held_lock\t\theld_locks[MAX_LOCK_DEPTH];\n#endif\n\n#ifdef CONFIG_UBSAN\n\tunsigned int\t\t\tin_ubsan;\n#endif\n\n\t/* Journalling filesystem info: */\n\tvoid\t\t\t\t*journal_info;\n\n\t/* Stacked block device info: */\n\tstruct bio_list\t\t\t*bio_list;\n\n#ifdef CONFIG_BLOCK\n\t/* Stack plugging: */\n\tstruct blk_plug\t\t\t*plug;\n#endif\n\n\t/* VM state: */\n\tstruct reclaim_state\t\t*reclaim_state;\n\n\tstruct backing_dev_info\t\t*backing_dev_info;\n\n\tstruct io_context\t\t*io_context;\n\n#ifdef CONFIG_COMPACTION\n\tstruct capture_control\t\t*capture_control;\n#endif\n\t/* Ptrace state: */\n\tunsigned long\t\t\tptrace_message;\n\tkernel_siginfo_t\t\t*last_siginfo;\n\n\tstruct task_io_accounting\tioac;\n#ifdef CONFIG_PSI\n\t/* Pressure stall state */\n\tunsigned int\t\t\tpsi_flags;\n#endif\n#ifdef CONFIG_TASK_XACCT\n\t/* Accumulated RSS usage: */\n\tu64\t\t\t\tacct_rss_mem1;\n\t/* Accumulated virtual memory usage: */\n\tu64\t\t\t\tacct_vm_mem1;\n\t/* stime + utime since last update: */\n\tu64\t\t\t\tacct_timexpd;\n#endif\n#ifdef CONFIG_CPUSETS\n\t/* Protected by ->alloc_lock: */\n\tnodemask_t\t\t\tmems_allowed;\n\t/* Seqence number to catch updates: */\n\tseqcount_t\t\t\tmems_allowed_seq;\n\tint\t\t\t\tcpuset_mem_spread_rotor;\n\tint\t\t\t\tcpuset_slab_spread_rotor;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/* Control Group info protected by css_set_lock: */\n\tstruct css_set __rcu\t\t*cgroups;\n\t/* cg_list protected by css_set_lock and tsk->alloc_lock: */\n\tstruct list_head\t\tcg_list;\n#endif\n#ifdef CONFIG_X86_CPU_RESCTRL\n\tu32\t\t\t\tclosid;\n\tu32\t\t\t\trmid;\n#endif\n#ifdef CONFIG_FUTEX\n\tstruct robust_list_head __user\t*robust_list;\n#ifdef CONFIG_COMPAT\n\tstruct compat_robust_list_head __user *compat_robust_list;\n#endif\n\tstruct list_head\t\tpi_state_list;\n\tstruct futex_pi_state\t\t*pi_state_cache;\n\tstruct mutex\t\t\tfutex_exit_mutex;\n\tunsigned int\t\t\tfutex_state;\n#endif\n#ifdef CONFIG_PERF_EVENTS\n\tstruct perf_event_context\t*perf_event_ctxp[perf_nr_task_contexts];\n\tstruct mutex\t\t\tperf_event_mutex;\n\tstruct list_head\t\tperf_event_list;\n#endif\n#ifdef CONFIG_DEBUG_PREEMPT\n\tunsigned long\t\t\tpreempt_disable_ip;\n#endif\n#ifdef CONFIG_NUMA\n\t/* Protected by alloc_lock: */\n\tstruct mempolicy\t\t*mempolicy;\n\tshort\t\t\t\til_prev;\n\tshort\t\t\t\tpref_node_fork;\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\tint\t\t\t\tnuma_scan_seq;\n\tunsigned int\t\t\tnuma_scan_period;\n\tunsigned int\t\t\tnuma_scan_period_max;\n\tint\t\t\t\tnuma_preferred_nid;\n\tunsigned long\t\t\tnuma_migrate_retry;\n\t/* Migration stamp: */\n\tu64\t\t\t\tnode_stamp;\n\tu64\t\t\t\tlast_task_numa_placement;\n\tu64\t\t\t\tlast_sum_exec_runtime;\n\tstruct callback_head\t\tnuma_work;\n\n\t/*\n\t * This pointer is only modified for current in syscall and\n\t * pagefault context (and for tasks being destroyed), so it can be read\n\t * from any of the following contexts:\n\t *  - RCU read-side critical section\n\t *  - current->numa_group from everywhere\n\t *  - task's runqueue locked, task not running\n\t */\n\tstruct numa_group __rcu\t\t*numa_group;\n\n\t/*\n\t * numa_faults is an array split into four regions:\n\t * faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer\n\t * in this precise order.\n\t *\n\t * faults_memory: Exponential decaying average of faults on a per-node\n\t * basis. Scheduling placement decisions are made based on these\n\t * counts. The values remain static for the duration of a PTE scan.\n\t * faults_cpu: Track the nodes the process was running on when a NUMA\n\t * hinting fault was incurred.\n\t * faults_memory_buffer and faults_cpu_buffer: Record faults per node\n\t * during the current scan window. When the scan completes, the counts\n\t * in faults_memory and faults_cpu decay and these values are copied.\n\t */\n\tunsigned long\t\t\t*numa_faults;\n\tunsigned long\t\t\ttotal_numa_faults;\n\n\t/*\n\t * numa_faults_locality tracks if faults recorded during the last\n\t * scan window were remote/local or failed to migrate. The task scan\n\t * period is adapted based on the locality of the faults with different\n\t * weights depending on whether they were shared or private faults\n\t */\n\tunsigned long\t\t\tnuma_faults_locality[3];\n\n\tunsigned long\t\t\tnuma_pages_migrated;\n#endif /* CONFIG_NUMA_BALANCING */\n\n#ifdef CONFIG_RSEQ\n\tstruct rseq __user *rseq;\n\tu32 rseq_sig;\n\t/*\n\t * RmW on rseq_event_mask must be performed atomically\n\t * with respect to preemption.\n\t */\n\tunsigned long rseq_event_mask;\n#endif\n\n\tstruct tlbflush_unmap_batch\ttlb_ubc;\n\n\tunion {\n\t\trefcount_t\t\trcu_users;\n\t\tstruct rcu_head\t\trcu;\n\t};\n\n\t/* Cache last used pipe for splice(): */\n\tstruct pipe_inode_info\t\t*splice_pipe;\n\n\tstruct page_frag\t\ttask_frag;\n\n#ifdef CONFIG_TASK_DELAY_ACCT\n\tstruct task_delay_info\t\t*delays;\n#endif\n\n#ifdef CONFIG_FAULT_INJECTION\n\tint\t\t\t\tmake_it_fail;\n\tunsigned int\t\t\tfail_nth;\n#endif\n\t/*\n\t * When (nr_dirtied >= nr_dirtied_pause), it's time to call\n\t * balance_dirty_pages() for a dirty throttling pause:\n\t */\n\tint\t\t\t\tnr_dirtied;\n\tint\t\t\t\tnr_dirtied_pause;\n\t/* Start of a write-and-pause period: */\n\tunsigned long\t\t\tdirty_paused_when;\n\n#ifdef CONFIG_LATENCYTOP\n\tint\t\t\t\tlatency_record_count;\n\tstruct latency_record\t\tlatency_record[LT_SAVECOUNT];\n#endif\n\t/*\n\t * Time slack values; these are used to round up poll() and\n\t * select() etc timeout values. These are in nanoseconds.\n\t */\n\tu64\t\t\t\ttimer_slack_ns;\n\tu64\t\t\t\tdefault_timer_slack_ns;\n\n#ifdef CONFIG_KASAN\n\tunsigned int\t\t\tkasan_depth;\n#endif\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t/* Index of current stored address in ret_stack: */\n\tint\t\t\t\tcurr_ret_stack;\n\tint\t\t\t\tcurr_ret_depth;\n\n\t/* Stack of return addresses for return function tracing: */\n\tstruct ftrace_ret_stack\t\t*ret_stack;\n\n\t/* Timestamp for last schedule: */\n\tunsigned long long\t\tftrace_timestamp;\n\n\t/*\n\t * Number of functions that haven't been traced\n\t * because of depth overrun:\n\t */\n\tatomic_t\t\t\ttrace_overrun;\n\n\t/* Pause tracing: */\n\tatomic_t\t\t\ttracing_graph_pause;\n#endif\n\n#ifdef CONFIG_TRACING\n\t/* State flags for use by tracers: */\n\tunsigned long\t\t\ttrace;\n\n\t/* Bitmask and counter of trace recursion: */\n\tunsigned long\t\t\ttrace_recursion;\n#endif /* CONFIG_TRACING */\n\n#ifdef CONFIG_KCOV\n\t/* See kernel/kcov.c for more details. */\n\n\t/* Coverage collection mode enabled for this task (0 if disabled): */\n\tunsigned int\t\t\tkcov_mode;\n\n\t/* Size of the kcov_area: */\n\tunsigned int\t\t\tkcov_size;\n\n\t/* Buffer for coverage collection: */\n\tvoid\t\t\t\t*kcov_area;\n\n\t/* KCOV descriptor wired with this task or NULL: */\n\tstruct kcov\t\t\t*kcov;\n\n\t/* KCOV common handle for remote coverage collection: */\n\tu64\t\t\t\tkcov_handle;\n\n\t/* KCOV sequence number: */\n\tint\t\t\t\tkcov_sequence;\n#endif\n\n#ifdef CONFIG_MEMCG\n\tstruct mem_cgroup\t\t*memcg_in_oom;\n\tgfp_t\t\t\t\tmemcg_oom_gfp_mask;\n\tint\t\t\t\tmemcg_oom_order;\n\n\t/* Number of pages to reclaim on returning to userland: */\n\tunsigned int\t\t\tmemcg_nr_pages_over_high;\n\n\t/* Used by memcontrol for targeted memcg charge: */\n\tstruct mem_cgroup\t\t*active_memcg;\n#endif\n\n#ifdef CONFIG_BLK_CGROUP\n\tstruct request_queue\t\t*throttle_queue;\n#endif\n\n#ifdef CONFIG_UPROBES\n\tstruct uprobe_task\t\t*utask;\n#endif\n#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)\n\tunsigned int\t\t\tsequential_io;\n\tunsigned int\t\t\tsequential_io_avg;\n#endif\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tunsigned long\t\t\ttask_state_change;\n#endif\n\tint\t\t\t\tpagefault_disabled;\n#ifdef CONFIG_MMU\n\tstruct task_struct\t\t*oom_reaper_list;\n#endif\n#ifdef CONFIG_VMAP_STACK\n\tstruct vm_struct\t\t*stack_vm_area;\n#endif\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t/* A live task holds one reference: */\n\trefcount_t\t\t\tstack_refcount;\n#endif\n#ifdef CONFIG_LIVEPATCH\n\tint patch_state;\n#endif\n#ifdef CONFIG_SECURITY\n\t/* Used by LSM modules for access restriction: */\n\tvoid\t\t\t\t*security;\n#endif\n\n#ifdef CONFIG_GCC_PLUGIN_STACKLEAK\n\tunsigned long\t\t\tlowest_stack;\n\tunsigned long\t\t\tprev_lowest_stack;\n#endif\n\n\t/*\n\t * New fields for task_struct should be added above here, so that\n\t * they are included in the randomized portion of task_struct.\n\t */\n\trandomized_struct_fields_end\n\n\t/* CPU-specific state of this task: */\n\tstruct thread_struct\t\tthread;\n\n\t/*\n\t * WARNING: on x86, 'thread_struct' contains a variable-sized\n\t * structure.  It *MUST* be at the end of 'task_struct'.\n\t *\n\t * Do not put anything below here!\n\t */\n};\n\nstatic inline struct pid *task_pid(struct task_struct *task)\n{\n\treturn task->thread_pid;\n}\n\n/*\n * the helpers to get the task's different pids as they are seen\n * from various namespaces\n *\n * task_xid_nr()     : global id, i.e. the id seen from the init namespace;\n * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of\n *                     current.\n * task_xid_nr_ns()  : id seen from the ns specified;\n *\n * see also pid_nr() etc in include/linux/pid.h\n */\npid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);\n\nstatic inline pid_t task_pid_nr(struct task_struct *tsk)\n{\n\treturn tsk->pid;\n}\n\nstatic inline pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);\n}\n\nstatic inline pid_t task_pid_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);\n}\n\n\nstatic inline pid_t task_tgid_nr(struct task_struct *tsk)\n{\n\treturn tsk->tgid;\n}\n\n/**\n * pid_alive - check that a task structure is not stale\n * @p: Task structure to be checked.\n *\n * Test if a process is not yet dead (at most zombie state)\n * If pid_alive fails, then pointers within the task structure\n * can be stale and must not be dereferenced.\n *\n * Return: 1 if the process is alive. 0 otherwise.\n */\nstatic inline int pid_alive(const struct task_struct *p)\n{\n\treturn p->thread_pid != NULL;\n}\n\nstatic inline pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);\n}\n\nstatic inline pid_t task_pgrp_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);\n}\n\n\nstatic inline pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);\n}\n\nstatic inline pid_t task_session_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);\n}\n\nstatic inline pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_TGID, ns);\n}\n\nstatic inline pid_t task_tgid_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_TGID, NULL);\n}\n\nstatic inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)\n{\n\tpid_t pid = 0;\n\n\trcu_read_lock();\n\tif (pid_alive(tsk))\n\t\tpid = task_tgid_nr_ns(rcu_dereference(tsk->real_parent), ns);\n\trcu_read_unlock();\n\n\treturn pid;\n}\n\nstatic inline pid_t task_ppid_nr(const struct task_struct *tsk)\n{\n\treturn task_ppid_nr_ns(tsk, &init_pid_ns);\n}\n\n/* Obsolete, do not use: */\nstatic inline pid_t task_pgrp_nr(struct task_struct *tsk)\n{\n\treturn task_pgrp_nr_ns(tsk, &init_pid_ns);\n}\n\n#define TASK_REPORT_IDLE\t(TASK_REPORT + 1)\n#define TASK_REPORT_MAX\t\t(TASK_REPORT_IDLE << 1)\n\nstatic inline unsigned int task_state_index(struct task_struct *tsk)\n{\n\tunsigned int tsk_state = READ_ONCE(tsk->state);\n\tunsigned int state = (tsk_state | tsk->exit_state) & TASK_REPORT;\n\n\tBUILD_BUG_ON_NOT_POWER_OF_2(TASK_REPORT_MAX);\n\n\tif (tsk_state == TASK_IDLE)\n\t\tstate = TASK_REPORT_IDLE;\n\n\treturn fls(state);\n}\n\nstatic inline char task_index_to_char(unsigned int state)\n{\n\tstatic const char state_char[] = \"RSDTtXZPI\";\n\n\tBUILD_BUG_ON(1 + ilog2(TASK_REPORT_MAX) != sizeof(state_char) - 1);\n\n\treturn state_char[state];\n}\n\nstatic inline char task_state_to_char(struct task_struct *tsk)\n{\n\treturn task_index_to_char(task_state_index(tsk));\n}\n\n/**\n * is_global_init - check if a task structure is init. Since init\n * is free to have sub-threads we need to check tgid.\n * @tsk: Task structure to be checked.\n *\n * Check if a task structure is the first user space task the kernel created.\n *\n * Return: 1 if the task structure is init. 0 otherwise.\n */\nstatic inline int is_global_init(struct task_struct *tsk)\n{\n\treturn task_tgid_nr(tsk) == 1;\n}\n\nextern struct pid *cad_pid;\n\n/*\n * Per process flags\n */\n#define PF_IDLE\t\t\t0x00000002\t/* I am an IDLE thread */\n#define PF_EXITING\t\t0x00000004\t/* Getting shut down */\n#define PF_VCPU\t\t\t0x00000010\t/* I'm a virtual CPU */\n#define PF_WQ_WORKER\t\t0x00000020\t/* I'm a workqueue worker */\n#define PF_FORKNOEXEC\t\t0x00000040\t/* Forked but didn't exec */\n#define PF_MCE_PROCESS\t\t0x00000080      /* Process policy on mce errors */\n#define PF_SUPERPRIV\t\t0x00000100\t/* Used super-user privileges */\n#define PF_DUMPCORE\t\t0x00000200\t/* Dumped core */\n#define PF_SIGNALED\t\t0x00000400\t/* Killed by a signal */\n#define PF_MEMALLOC\t\t0x00000800\t/* Allocating memory */\n#define PF_NPROC_EXCEEDED\t0x00001000\t/* set_user() noticed that RLIMIT_NPROC was exceeded */\n#define PF_USED_MATH\t\t0x00002000\t/* If unset the fpu must be initialized before use */\n#define PF_USED_ASYNC\t\t0x00004000\t/* Used async_schedule*(), used by module init */\n#define PF_NOFREEZE\t\t0x00008000\t/* This thread should not be frozen */\n#define PF_FROZEN\t\t0x00010000\t/* Frozen for system suspend */\n#define PF_KSWAPD\t\t0x00020000\t/* I am kswapd */\n#define PF_MEMALLOC_NOFS\t0x00040000\t/* All allocation requests will inherit GFP_NOFS */\n#define PF_MEMALLOC_NOIO\t0x00080000\t/* All allocation requests will inherit GFP_NOIO */\n#define PF_LESS_THROTTLE\t0x00100000\t/* Throttle me less: I clean memory */\n#define PF_KTHREAD\t\t0x00200000\t/* I am a kernel thread */\n#define PF_RANDOMIZE\t\t0x00400000\t/* Randomize virtual address space */\n#define PF_SWAPWRITE\t\t0x00800000\t/* Allowed to write to swap */\n#define PF_MEMSTALL\t\t0x01000000\t/* Stalled due to lack of memory */\n#define PF_UMH\t\t\t0x02000000\t/* I'm an Usermodehelper process */\n#define PF_NO_SETAFFINITY\t0x04000000\t/* Userland is not allowed to meddle with cpus_mask */\n#define PF_MCE_EARLY\t\t0x08000000      /* Early kill for mce process policy */\n#define PF_MEMALLOC_NOCMA\t0x10000000\t/* All allocation request will have _GFP_MOVABLE cleared */\n#define PF_IO_WORKER\t\t0x20000000\t/* Task is an IO worker */\n#define PF_FREEZER_SKIP\t\t0x40000000\t/* Freezer should not count it as freezable */\n#define PF_SUSPEND_TASK\t\t0x80000000      /* This thread called freeze_processes() and should not be frozen */\n\n/*\n * Only the _current_ task can read/write to tsk->flags, but other\n * tasks can access tsk->flags in readonly mode for example\n * with tsk_used_math (like during threaded core dumping).\n * There is however an exception to this rule during ptrace\n * or during fork: the ptracer task is allowed to write to the\n * child->flags of its traced child (same goes for fork, the parent\n * can write to the child->flags), because we're guaranteed the\n * child is not running and in turn not changing child->flags\n * at the same time the parent does it.\n */\n#define clear_stopped_child_used_math(child)\tdo { (child)->flags &= ~PF_USED_MATH; } while (0)\n#define set_stopped_child_used_math(child)\tdo { (child)->flags |= PF_USED_MATH; } while (0)\n#define clear_used_math()\t\t\tclear_stopped_child_used_math(current)\n#define set_used_math()\t\t\t\tset_stopped_child_used_math(current)\n\n#define conditional_stopped_child_used_math(condition, child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)\n\n#define conditional_used_math(condition)\tconditional_stopped_child_used_math(condition, current)\n\n#define copy_to_stopped_child_used_math(child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)\n\n/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */\n#define tsk_used_math(p)\t\t\t((p)->flags & PF_USED_MATH)\n#define used_math()\t\t\t\ttsk_used_math(current)\n\nstatic inline bool is_percpu_thread(void)\n{\n#ifdef CONFIG_SMP\n\treturn (current->flags & PF_NO_SETAFFINITY) &&\n\t\t(current->nr_cpus_allowed  == 1);\n#else\n\treturn true;\n#endif\n}\n\n/* Per-process atomic flags. */\n#define PFA_NO_NEW_PRIVS\t\t0\t/* May not gain new privileges. */\n#define PFA_SPREAD_PAGE\t\t\t1\t/* Spread page cache over cpuset */\n#define PFA_SPREAD_SLAB\t\t\t2\t/* Spread some slab caches over cpuset */\n#define PFA_SPEC_SSB_DISABLE\t\t3\t/* Speculative Store Bypass disabled */\n#define PFA_SPEC_SSB_FORCE_DISABLE\t4\t/* Speculative Store Bypass force disabled*/\n#define PFA_SPEC_IB_DISABLE\t\t5\t/* Indirect branch speculation restricted */\n#define PFA_SPEC_IB_FORCE_DISABLE\t6\t/* Indirect branch speculation permanently restricted */\n#define PFA_SPEC_SSB_NOEXEC\t\t7\t/* Speculative Store Bypass clear on execve() */\n\n#define TASK_PFA_TEST(name, func)\t\t\t\t\t\\\n\tstatic inline bool task_##func(struct task_struct *p)\t\t\\\n\t{ return test_bit(PFA_##name, &p->atomic_flags); }\n\n#define TASK_PFA_SET(name, func)\t\t\t\t\t\\\n\tstatic inline void task_set_##func(struct task_struct *p)\t\\\n\t{ set_bit(PFA_##name, &p->atomic_flags); }\n\n#define TASK_PFA_CLEAR(name, func)\t\t\t\t\t\\\n\tstatic inline void task_clear_##func(struct task_struct *p)\t\\\n\t{ clear_bit(PFA_##name, &p->atomic_flags); }\n\nTASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)\nTASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)\n\nTASK_PFA_TEST(SPREAD_PAGE, spread_page)\nTASK_PFA_SET(SPREAD_PAGE, spread_page)\nTASK_PFA_CLEAR(SPREAD_PAGE, spread_page)\n\nTASK_PFA_TEST(SPREAD_SLAB, spread_slab)\nTASK_PFA_SET(SPREAD_SLAB, spread_slab)\nTASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)\n\nTASK_PFA_TEST(SPEC_SSB_DISABLE, spec_ssb_disable)\nTASK_PFA_SET(SPEC_SSB_DISABLE, spec_ssb_disable)\nTASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)\n\nTASK_PFA_TEST(SPEC_SSB_NOEXEC, spec_ssb_noexec)\nTASK_PFA_SET(SPEC_SSB_NOEXEC, spec_ssb_noexec)\nTASK_PFA_CLEAR(SPEC_SSB_NOEXEC, spec_ssb_noexec)\n\nTASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)\nTASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)\n\nTASK_PFA_TEST(SPEC_IB_DISABLE, spec_ib_disable)\nTASK_PFA_SET(SPEC_IB_DISABLE, spec_ib_disable)\nTASK_PFA_CLEAR(SPEC_IB_DISABLE, spec_ib_disable)\n\nTASK_PFA_TEST(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)\nTASK_PFA_SET(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)\n\nstatic inline void\ncurrent_restore_flags(unsigned long orig_flags, unsigned long flags)\n{\n\tcurrent->flags &= ~flags;\n\tcurrent->flags |= orig_flags & flags;\n}\n\nextern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);\nextern int task_can_attach(struct task_struct *p, const struct cpumask *cs_cpus_allowed);\n#ifdef CONFIG_SMP\nextern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);\nextern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);\n#else\nstatic inline void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)\n{\n}\nstatic inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tif (!cpumask_test_cpu(0, new_mask))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n#endif\n\nextern int yield_to(struct task_struct *p, bool preempt);\nextern void set_user_nice(struct task_struct *p, long nice);\nextern int task_prio(const struct task_struct *p);\n\n/**\n * task_nice - return the nice value of a given task.\n * @p: the task in question.\n *\n * Return: The nice value [ -20 ... 0 ... 19 ].\n */\nstatic inline int task_nice(const struct task_struct *p)\n{\n\treturn PRIO_TO_NICE((p)->static_prio);\n}\n\nextern int can_nice(const struct task_struct *p, const int nice);\nextern int task_curr(const struct task_struct *p);\nextern int idle_cpu(int cpu);\nextern int available_idle_cpu(int cpu);\nextern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);\nextern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);\nextern int sched_setattr(struct task_struct *, const struct sched_attr *);\nextern int sched_setattr_nocheck(struct task_struct *, const struct sched_attr *);\nextern struct task_struct *idle_task(int cpu);\n\n/**\n * is_idle_task - is the specified task an idle task?\n * @p: the task in question.\n *\n * Return: 1 if @p is an idle task. 0 otherwise.\n */\nstatic inline bool is_idle_task(const struct task_struct *p)\n{\n\treturn !!(p->flags & PF_IDLE);\n}\n\nextern struct task_struct *curr_task(int cpu);\nextern void ia64_set_curr_task(int cpu, struct task_struct *p);\n\nvoid yield(void);\n\nunion thread_union {\n#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK\n\tstruct task_struct task;\n#endif\n#ifndef CONFIG_THREAD_INFO_IN_TASK\n\tstruct thread_info thread_info;\n#endif\n\tunsigned long stack[THREAD_SIZE/sizeof(long)];\n};\n\n#ifndef CONFIG_THREAD_INFO_IN_TASK\nextern struct thread_info init_thread_info;\n#endif\n\nextern unsigned long init_stack[THREAD_SIZE / sizeof(unsigned long)];\n\n#ifdef CONFIG_THREAD_INFO_IN_TASK\nstatic inline struct thread_info *task_thread_info(struct task_struct *task)\n{\n\treturn &task->thread_info;\n}\n#elif !defined(__HAVE_THREAD_FUNCTIONS)\n# define task_thread_info(task)\t((struct thread_info *)(task)->stack)\n#endif\n\n/*\n * find a task by one of its numerical ids\n *\n * find_task_by_pid_ns():\n *      finds a task by its pid in the specified namespace\n * find_task_by_vpid():\n *      finds a task by its virtual pid\n *\n * see also find_vpid() etc in include/linux/pid.h\n */\n\nextern struct task_struct *find_task_by_vpid(pid_t nr);\nextern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);\n\n/*\n * find a task by its virtual pid and get the task struct\n */\nextern struct task_struct *find_get_task_by_vpid(pid_t nr);\n\nextern int wake_up_state(struct task_struct *tsk, unsigned int state);\nextern int wake_up_process(struct task_struct *tsk);\nextern void wake_up_new_task(struct task_struct *tsk);\n\n#ifdef CONFIG_SMP\nextern void kick_process(struct task_struct *tsk);\n#else\nstatic inline void kick_process(struct task_struct *tsk) { }\n#endif\n\nextern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);\n\nstatic inline void set_task_comm(struct task_struct *tsk, const char *from)\n{\n\t__set_task_comm(tsk, from, false);\n}\n\nextern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);\n#define get_task_comm(buf, tsk) ({\t\t\t\\\n\tBUILD_BUG_ON(sizeof(buf) != TASK_COMM_LEN);\t\\\n\t__get_task_comm(buf, sizeof(buf), tsk);\t\t\\\n})\n\n#ifdef CONFIG_SMP\nvoid scheduler_ipi(void);\nextern unsigned long wait_task_inactive(struct task_struct *, long match_state);\n#else\nstatic inline void scheduler_ipi(void) { }\nstatic inline unsigned long wait_task_inactive(struct task_struct *p, long match_state)\n{\n\treturn 1;\n}\n#endif\n\n/*\n * Set thread flags in other task's structures.\n * See asm/thread_info.h for TIF_xxxx flags available:\n */\nstatic inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tset_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tclear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void update_tsk_thread_flag(struct task_struct *tsk, int flag,\n\t\t\t\t\t  bool value)\n{\n\tupdate_ti_thread_flag(task_thread_info(tsk), flag, value);\n}\n\nstatic inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_set_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void set_tsk_need_resched(struct task_struct *tsk)\n{\n\tset_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline void clear_tsk_need_resched(struct task_struct *tsk)\n{\n\tclear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline int test_tsk_need_resched(struct task_struct *tsk)\n{\n\treturn unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));\n}\n\n/*\n * cond_resched() and cond_resched_lock(): latency reduction via\n * explicit rescheduling in places that are safe. The return\n * value indicates whether a reschedule was done in fact.\n * cond_resched_lock() will drop the spinlock before scheduling,\n */\n#ifndef CONFIG_PREEMPTION\nextern int _cond_resched(void);\n#else\nstatic inline int _cond_resched(void) { return 0; }\n#endif\n\n#define cond_resched() ({\t\t\t\\\n\t___might_sleep(__FILE__, __LINE__, 0);\t\\\n\t_cond_resched();\t\t\t\\\n})\n\nextern int __cond_resched_lock(spinlock_t *lock);\n\n#define cond_resched_lock(lock) ({\t\t\t\t\\\n\t___might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);\\\n\t__cond_resched_lock(lock);\t\t\t\t\\\n})\n\nstatic inline void cond_resched_rcu(void)\n{\n#if defined(CONFIG_DEBUG_ATOMIC_SLEEP) || !defined(CONFIG_PREEMPT_RCU)\n\trcu_read_unlock();\n\tcond_resched();\n\trcu_read_lock();\n#endif\n}\n\n/*\n * Does a critical section need to be broken due to another\n * task waiting?: (technically does not depend on CONFIG_PREEMPTION,\n * but a general need for low latency)\n */\nstatic inline int spin_needbreak(spinlock_t *lock)\n{\n#ifdef CONFIG_PREEMPTION\n\treturn spin_is_contended(lock);\n#else\n\treturn 0;\n#endif\n}\n\nstatic __always_inline bool need_resched(void)\n{\n\treturn unlikely(tif_need_resched());\n}\n\n/*\n * Wrappers for p->thread_info->cpu access. No-op on UP.\n */\n#ifdef CONFIG_SMP\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\treturn READ_ONCE(p->cpu);\n#else\n\treturn READ_ONCE(task_thread_info(p)->cpu);\n#endif\n}\n\nextern void set_task_cpu(struct task_struct *p, unsigned int cpu);\n\n#else\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n\treturn 0;\n}\n\nstatic inline void set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n}\n\n#endif /* CONFIG_SMP */\n\n/*\n * In order to reduce various lock holder preemption latencies provide an\n * interface to see if a vCPU is currently running or not.\n *\n * This allows us to terminate optimistic spin loops and block, analogous to\n * the native optimistic spin heuristic of testing if the lock owner task is\n * running or not.\n */\n#ifndef vcpu_is_preempted\nstatic inline bool vcpu_is_preempted(int cpu)\n{\n\treturn false;\n}\n#endif\n\nextern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);\nextern long sched_getaffinity(pid_t pid, struct cpumask *mask);\n\n#ifndef TASK_SIZE_OF\n#define TASK_SIZE_OF(tsk)\tTASK_SIZE\n#endif\n\n#ifdef CONFIG_RSEQ\n\n/*\n * Map the event mask on the user-space ABI enum rseq_cs_flags\n * for direct mask checks.\n */\nenum rseq_event_mask_bits {\n\tRSEQ_EVENT_PREEMPT_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT,\n\tRSEQ_EVENT_SIGNAL_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT,\n\tRSEQ_EVENT_MIGRATE_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT,\n};\n\nenum rseq_event_mask {\n\tRSEQ_EVENT_PREEMPT\t= (1U << RSEQ_EVENT_PREEMPT_BIT),\n\tRSEQ_EVENT_SIGNAL\t= (1U << RSEQ_EVENT_SIGNAL_BIT),\n\tRSEQ_EVENT_MIGRATE\t= (1U << RSEQ_EVENT_MIGRATE_BIT),\n};\n\nstatic inline void rseq_set_notify_resume(struct task_struct *t)\n{\n\tif (t->rseq)\n\t\tset_tsk_thread_flag(t, TIF_NOTIFY_RESUME);\n}\n\nvoid __rseq_handle_notify_resume(struct ksignal *sig, struct pt_regs *regs);\n\nstatic inline void rseq_handle_notify_resume(struct ksignal *ksig,\n\t\t\t\t\t     struct pt_regs *regs)\n{\n\tif (current->rseq)\n\t\t__rseq_handle_notify_resume(ksig, regs);\n}\n\nstatic inline void rseq_signal_deliver(struct ksignal *ksig,\n\t\t\t\t       struct pt_regs *regs)\n{\n\tpreempt_disable();\n\t__set_bit(RSEQ_EVENT_SIGNAL_BIT, &current->rseq_event_mask);\n\tpreempt_enable();\n\trseq_handle_notify_resume(ksig, regs);\n}\n\n/* rseq_preempt() requires preemption to be disabled. */\nstatic inline void rseq_preempt(struct task_struct *t)\n{\n\t__set_bit(RSEQ_EVENT_PREEMPT_BIT, &t->rseq_event_mask);\n\trseq_set_notify_resume(t);\n}\n\n/* rseq_migrate() requires preemption to be disabled. */\nstatic inline void rseq_migrate(struct task_struct *t)\n{\n\t__set_bit(RSEQ_EVENT_MIGRATE_BIT, &t->rseq_event_mask);\n\trseq_set_notify_resume(t);\n}\n\n/*\n * If parent process has a registered restartable sequences area, the\n * child inherits. Unregister rseq for a clone with CLONE_VM set.\n */\nstatic inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)\n{\n\tif (clone_flags & CLONE_VM) {\n\t\tt->rseq = NULL;\n\t\tt->rseq_sig = 0;\n\t\tt->rseq_event_mask = 0;\n\t} else {\n\t\tt->rseq = current->rseq;\n\t\tt->rseq_sig = current->rseq_sig;\n\t\tt->rseq_event_mask = current->rseq_event_mask;\n\t}\n}\n\nstatic inline void rseq_execve(struct task_struct *t)\n{\n\tt->rseq = NULL;\n\tt->rseq_sig = 0;\n\tt->rseq_event_mask = 0;\n}\n\n#else\n\nstatic inline void rseq_set_notify_resume(struct task_struct *t)\n{\n}\nstatic inline void rseq_handle_notify_resume(struct ksignal *ksig,\n\t\t\t\t\t     struct pt_regs *regs)\n{\n}\nstatic inline void rseq_signal_deliver(struct ksignal *ksig,\n\t\t\t\t       struct pt_regs *regs)\n{\n}\nstatic inline void rseq_preempt(struct task_struct *t)\n{\n}\nstatic inline void rseq_migrate(struct task_struct *t)\n{\n}\nstatic inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)\n{\n}\nstatic inline void rseq_execve(struct task_struct *t)\n{\n}\n\n#endif\n\nvoid __exit_umh(struct task_struct *tsk);\n\nstatic inline void exit_umh(struct task_struct *tsk)\n{\n\tif (unlikely(tsk->flags & PF_UMH))\n\t\t__exit_umh(tsk);\n}\n\n#ifdef CONFIG_DEBUG_RSEQ\n\nvoid rseq_syscall(struct pt_regs *regs);\n\n#else\n\nstatic inline void rseq_syscall(struct pt_regs *regs)\n{\n}\n\n#endif\n\nconst struct sched_avg *sched_trace_cfs_rq_avg(struct cfs_rq *cfs_rq);\nchar *sched_trace_cfs_rq_path(struct cfs_rq *cfs_rq, char *str, int len);\nint sched_trace_cfs_rq_cpu(struct cfs_rq *cfs_rq);\n\nconst struct sched_avg *sched_trace_rq_avg_rt(struct rq *rq);\nconst struct sched_avg *sched_trace_rq_avg_dl(struct rq *rq);\nconst struct sched_avg *sched_trace_rq_avg_irq(struct rq *rq);\n\nint sched_trace_rq_cpu(struct rq *rq);\n\nconst struct cpumask *sched_trace_rd_span(struct root_domain *rd);\n\n#endif\n", "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/kernel/signal.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson\n *\n *  2003-06-02  Jim Houston - Concurrent Computer Corp.\n *\t\tChanges to use preallocated sigqueue structures\n *\t\tto allow signals to be sent reliably.\n */\n\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/user.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/proc_fs.h>\n#include <linux/tty.h>\n#include <linux/binfmts.h>\n#include <linux/coredump.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/ptrace.h>\n#include <linux/signal.h>\n#include <linux/signalfd.h>\n#include <linux/ratelimit.h>\n#include <linux/tracehook.h>\n#include <linux/capability.h>\n#include <linux/freezer.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/user_namespace.h>\n#include <linux/uprobes.h>\n#include <linux/compat.h>\n#include <linux/cn_proc.h>\n#include <linux/compiler.h>\n#include <linux/posix-timers.h>\n#include <linux/livepatch.h>\n#include <linux/cgroup.h>\n#include <linux/audit.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/signal.h>\n\n#include <asm/param.h>\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/siginfo.h>\n#include <asm/cacheflush.h>\n\n/*\n * SLAB caches for signal bits.\n */\n\nstatic struct kmem_cache *sigqueue_cachep;\n\nint print_fatal_signals __read_mostly;\n\nstatic void __user *sig_handler(struct task_struct *t, int sig)\n{\n\treturn t->sighand->action[sig - 1].sa.sa_handler;\n}\n\nstatic inline bool sig_handler_ignored(void __user *handler, int sig)\n{\n\t/* Is it explicitly or implicitly ignored? */\n\treturn handler == SIG_IGN ||\n\t       (handler == SIG_DFL && sig_kernel_ignore(sig));\n}\n\nstatic bool sig_task_ignored(struct task_struct *t, int sig, bool force)\n{\n\tvoid __user *handler;\n\n\thandler = sig_handler(t, sig);\n\n\t/* SIGKILL and SIGSTOP may not be sent to the global init */\n\tif (unlikely(is_global_init(t) && sig_kernel_only(sig)))\n\t\treturn true;\n\n\tif (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&\n\t    handler == SIG_DFL && !(force && sig_kernel_only(sig)))\n\t\treturn true;\n\n\t/* Only allow kernel generated signals to this kthread */\n\tif (unlikely((t->flags & PF_KTHREAD) &&\n\t\t     (handler == SIG_KTHREAD_KERNEL) && !force))\n\t\treturn true;\n\n\treturn sig_handler_ignored(handler, sig);\n}\n\nstatic bool sig_ignored(struct task_struct *t, int sig, bool force)\n{\n\t/*\n\t * Blocked signals are never ignored, since the\n\t * signal handler may change by the time it is\n\t * unblocked.\n\t */\n\tif (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))\n\t\treturn false;\n\n\t/*\n\t * Tracers may want to know about even ignored signal unless it\n\t * is SIGKILL which can't be reported anyway but can be ignored\n\t * by SIGNAL_UNKILLABLE task.\n\t */\n\tif (t->ptrace && sig != SIGKILL)\n\t\treturn false;\n\n\treturn sig_task_ignored(t, sig, force);\n}\n\n/*\n * Re-calculate pending state from the set of locally pending\n * signals, globally pending signals, and blocked signals.\n */\nstatic inline bool has_pending_signals(sigset_t *signal, sigset_t *blocked)\n{\n\tunsigned long ready;\n\tlong i;\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = _NSIG_WORDS, ready = 0; --i >= 0 ;)\n\t\t\tready |= signal->sig[i] &~ blocked->sig[i];\n\t\tbreak;\n\n\tcase 4: ready  = signal->sig[3] &~ blocked->sig[3];\n\t\tready |= signal->sig[2] &~ blocked->sig[2];\n\t\tready |= signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 2: ready  = signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 1: ready  = signal->sig[0] &~ blocked->sig[0];\n\t}\n\treturn ready !=\t0;\n}\n\n#define PENDING(p,b) has_pending_signals(&(p)->signal, (b))\n\nstatic bool recalc_sigpending_tsk(struct task_struct *t)\n{\n\tif ((t->jobctl & (JOBCTL_PENDING_MASK | JOBCTL_TRAP_FREEZE)) ||\n\t    PENDING(&t->pending, &t->blocked) ||\n\t    PENDING(&t->signal->shared_pending, &t->blocked) ||\n\t    cgroup_task_frozen(t)) {\n\t\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t\treturn true;\n\t}\n\n\t/*\n\t * We must never clear the flag in another thread, or in current\n\t * when it's possible the current syscall is returning -ERESTART*.\n\t * So we don't clear it here, and only callers who know they should do.\n\t */\n\treturn false;\n}\n\n/*\n * After recalculating TIF_SIGPENDING, we need to make sure the task wakes up.\n * This is superfluous when called on current, the wakeup is a harmless no-op.\n */\nvoid recalc_sigpending_and_wake(struct task_struct *t)\n{\n\tif (recalc_sigpending_tsk(t))\n\t\tsignal_wake_up(t, 0);\n}\n\nvoid recalc_sigpending(void)\n{\n\tif (!recalc_sigpending_tsk(current) && !freezing(current) &&\n\t    !klp_patch_pending(current))\n\t\tclear_thread_flag(TIF_SIGPENDING);\n\n}\nEXPORT_SYMBOL(recalc_sigpending);\n\nvoid calculate_sigpending(void)\n{\n\t/* Have any signals or users of TIF_SIGPENDING been delayed\n\t * until after fork?\n\t */\n\tspin_lock_irq(&current->sighand->siglock);\n\tset_tsk_thread_flag(current, TIF_SIGPENDING);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n}\n\n/* Given the mask, find the first available signal that should be serviced. */\n\n#define SYNCHRONOUS_MASK \\\n\t(sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \\\n\t sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))\n\nint next_signal(struct sigpending *pending, sigset_t *mask)\n{\n\tunsigned long i, *s, *m, x;\n\tint sig = 0;\n\n\ts = pending->signal.sig;\n\tm = mask->sig;\n\n\t/*\n\t * Handle the first word specially: it contains the\n\t * synchronous signals that need to be dequeued first.\n\t */\n\tx = *s &~ *m;\n\tif (x) {\n\t\tif (x & SYNCHRONOUS_MASK)\n\t\t\tx &= SYNCHRONOUS_MASK;\n\t\tsig = ffz(~x) + 1;\n\t\treturn sig;\n\t}\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = 1; i < _NSIG_WORDS; ++i) {\n\t\t\tx = *++s &~ *++m;\n\t\t\tif (!x)\n\t\t\t\tcontinue;\n\t\t\tsig = ffz(~x) + i*_NSIG_BPW + 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 2:\n\t\tx = s[1] &~ m[1];\n\t\tif (!x)\n\t\t\tbreak;\n\t\tsig = ffz(~x) + _NSIG_BPW + 1;\n\t\tbreak;\n\n\tcase 1:\n\t\t/* Nothing to do */\n\t\tbreak;\n\t}\n\n\treturn sig;\n}\n\nstatic inline void print_dropped_signal(int sig)\n{\n\tstatic DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);\n\n\tif (!print_fatal_signals)\n\t\treturn;\n\n\tif (!__ratelimit(&ratelimit_state))\n\t\treturn;\n\n\tpr_info(\"%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\\n\",\n\t\t\t\tcurrent->comm, current->pid, sig);\n}\n\n/**\n * task_set_jobctl_pending - set jobctl pending bits\n * @task: target task\n * @mask: pending bits to set\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK | %JOBCTL_STOP_CONSUME | %JOBCTL_STOP_SIGMASK |\n * %JOBCTL_TRAPPING.  If stop signo is being set, the existing signo is\n * cleared.  If @task is already being killed or exiting, this function\n * becomes noop.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if @mask is set, %false if made noop because @task was dying.\n */\nbool task_set_jobctl_pending(struct task_struct *task, unsigned long mask)\n{\n\tBUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |\n\t\t\tJOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));\n\tBUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));\n\n\tif (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))\n\t\treturn false;\n\n\tif (mask & JOBCTL_STOP_SIGMASK)\n\t\ttask->jobctl &= ~JOBCTL_STOP_SIGMASK;\n\n\ttask->jobctl |= mask;\n\treturn true;\n}\n\n/**\n * task_clear_jobctl_trapping - clear jobctl trapping bit\n * @task: target task\n *\n * If JOBCTL_TRAPPING is set, a ptracer is waiting for us to enter TRACED.\n * Clear it and wake up the ptracer.  Note that we don't need any further\n * locking.  @task->siglock guarantees that @task->parent points to the\n * ptracer.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_trapping(struct task_struct *task)\n{\n\tif (unlikely(task->jobctl & JOBCTL_TRAPPING)) {\n\t\ttask->jobctl &= ~JOBCTL_TRAPPING;\n\t\tsmp_mb();\t/* advised by wake_up_bit() */\n\t\twake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);\n\t}\n}\n\n/**\n * task_clear_jobctl_pending - clear jobctl pending bits\n * @task: target task\n * @mask: pending bits to clear\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK.  If %JOBCTL_STOP_PENDING is being cleared, other\n * STOP bits are cleared together.\n *\n * If clearing of @mask leaves no stop or trap pending, this function calls\n * task_clear_jobctl_trapping().\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_pending(struct task_struct *task, unsigned long mask)\n{\n\tBUG_ON(mask & ~JOBCTL_PENDING_MASK);\n\n\tif (mask & JOBCTL_STOP_PENDING)\n\t\tmask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;\n\n\ttask->jobctl &= ~mask;\n\n\tif (!(task->jobctl & JOBCTL_PENDING_MASK))\n\t\ttask_clear_jobctl_trapping(task);\n}\n\n/**\n * task_participate_group_stop - participate in a group stop\n * @task: task participating in a group stop\n *\n * @task has %JOBCTL_STOP_PENDING set and is participating in a group stop.\n * Group stop states are cleared and the group stop count is consumed if\n * %JOBCTL_STOP_CONSUME was set.  If the consumption completes the group\n * stop, the appropriate `SIGNAL_*` flags are set.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if group stop completion should be notified to the parent, %false\n * otherwise.\n */\nstatic bool task_participate_group_stop(struct task_struct *task)\n{\n\tstruct signal_struct *sig = task->signal;\n\tbool consume = task->jobctl & JOBCTL_STOP_CONSUME;\n\n\tWARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));\n\n\ttask_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);\n\n\tif (!consume)\n\t\treturn false;\n\n\tif (!WARN_ON_ONCE(sig->group_stop_count == 0))\n\t\tsig->group_stop_count--;\n\n\t/*\n\t * Tell the caller to notify completion iff we are entering into a\n\t * fresh group stop.  Read comment in do_signal_stop() for details.\n\t */\n\tif (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {\n\t\tsignal_set_stop_flags(sig, SIGNAL_STOP_STOPPED);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nvoid task_join_group_stop(struct task_struct *task)\n{\n\t/* Have the new thread join an on-going signal group stop */\n\tunsigned long jobctl = current->jobctl;\n\tif (jobctl & JOBCTL_STOP_PENDING) {\n\t\tstruct signal_struct *sig = current->signal;\n\t\tunsigned long signr = jobctl & JOBCTL_STOP_SIGMASK;\n\t\tunsigned long gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\n\t\tif (task_set_jobctl_pending(task, signr | gstop)) {\n\t\t\tsig->group_stop_count++;\n\t\t}\n\t}\n}\n\n/*\n * allocate a new signal queue record\n * - this may be called without locks if and only if t == current, otherwise an\n *   appropriate lock must be held to stop the target task from exiting\n */\nstatic struct sigqueue *\n__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)\n{\n\tstruct sigqueue *q = NULL;\n\tstruct user_struct *user;\n\tint sigpending;\n\n\t/*\n\t * Protect access to @t credentials. This can go away when all\n\t * callers hold rcu read lock.\n\t *\n\t * NOTE! A pending signal will hold on to the user refcount,\n\t * and we get/put the refcount only when the sigpending count\n\t * changes from/to zero.\n\t */\n\trcu_read_lock();\n\tuser = __task_cred(t)->user;\n\tsigpending = atomic_inc_return(&user->sigpending);\n\tif (sigpending == 1)\n\t\tget_uid(user);\n\trcu_read_unlock();\n\n\tif (override_rlimit || likely(sigpending <= task_rlimit(t, RLIMIT_SIGPENDING))) {\n\t\tq = kmem_cache_alloc(sigqueue_cachep, flags);\n\t} else {\n\t\tprint_dropped_signal(sig);\n\t}\n\n\tif (unlikely(q == NULL)) {\n\t\tif (atomic_dec_and_test(&user->sigpending))\n\t\t\tfree_uid(user);\n\t} else {\n\t\tINIT_LIST_HEAD(&q->list);\n\t\tq->flags = 0;\n\t\tq->user = user;\n\t}\n\n\treturn q;\n}\n\nstatic void __sigqueue_free(struct sigqueue *q)\n{\n\tif (q->flags & SIGQUEUE_PREALLOC)\n\t\treturn;\n\tif (atomic_dec_and_test(&q->user->sigpending))\n\t\tfree_uid(q->user);\n\tkmem_cache_free(sigqueue_cachep, q);\n}\n\nvoid flush_sigqueue(struct sigpending *queue)\n{\n\tstruct sigqueue *q;\n\n\tsigemptyset(&queue->signal);\n\twhile (!list_empty(&queue->list)) {\n\t\tq = list_entry(queue->list.next, struct sigqueue , list);\n\t\tlist_del_init(&q->list);\n\t\t__sigqueue_free(q);\n\t}\n}\n\n/*\n * Flush all pending signals for this kthread.\n */\nvoid flush_signals(struct task_struct *t)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\tclear_tsk_thread_flag(t, TIF_SIGPENDING);\n\tflush_sigqueue(&t->pending);\n\tflush_sigqueue(&t->signal->shared_pending);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n}\nEXPORT_SYMBOL(flush_signals);\n\n#ifdef CONFIG_POSIX_TIMERS\nstatic void __flush_itimer_signals(struct sigpending *pending)\n{\n\tsigset_t signal, retain;\n\tstruct sigqueue *q, *n;\n\n\tsignal = pending->signal;\n\tsigemptyset(&retain);\n\n\tlist_for_each_entry_safe(q, n, &pending->list, list) {\n\t\tint sig = q->info.si_signo;\n\n\t\tif (likely(q->info.si_code != SI_TIMER)) {\n\t\t\tsigaddset(&retain, sig);\n\t\t} else {\n\t\t\tsigdelset(&signal, sig);\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\n\tsigorsets(&pending->signal, &signal, &retain);\n}\n\nvoid flush_itimer_signals(void)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\n\t__flush_itimer_signals(&tsk->pending);\n\t__flush_itimer_signals(&tsk->signal->shared_pending);\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n}\n#endif\n\nvoid ignore_signals(struct task_struct *t)\n{\n\tint i;\n\n\tfor (i = 0; i < _NSIG; ++i)\n\t\tt->sighand->action[i].sa.sa_handler = SIG_IGN;\n\n\tflush_signals(t);\n}\n\n/*\n * Flush all handlers for a task.\n */\n\nvoid\nflush_signal_handlers(struct task_struct *t, int force_default)\n{\n\tint i;\n\tstruct k_sigaction *ka = &t->sighand->action[0];\n\tfor (i = _NSIG ; i != 0 ; i--) {\n\t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n\t\t\tka->sa.sa_handler = SIG_DFL;\n\t\tka->sa.sa_flags = 0;\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tka->sa.sa_restorer = NULL;\n#endif\n\t\tsigemptyset(&ka->sa.sa_mask);\n\t\tka++;\n\t}\n}\n\nbool unhandled_signal(struct task_struct *tsk, int sig)\n{\n\tvoid __user *handler = tsk->sighand->action[sig-1].sa.sa_handler;\n\tif (is_global_init(tsk))\n\t\treturn true;\n\n\tif (handler != SIG_IGN && handler != SIG_DFL)\n\t\treturn false;\n\n\t/* if ptraced, let the tracer determine */\n\treturn !tsk->ptrace;\n}\n\nstatic void collect_signal(int sig, struct sigpending *list, kernel_siginfo_t *info,\n\t\t\t   bool *resched_timer)\n{\n\tstruct sigqueue *q, *first = NULL;\n\n\t/*\n\t * Collect the siginfo appropriate to this signal.  Check if\n\t * there is another siginfo for the same signal.\n\t*/\n\tlist_for_each_entry(q, &list->list, list) {\n\t\tif (q->info.si_signo == sig) {\n\t\t\tif (first)\n\t\t\t\tgoto still_pending;\n\t\t\tfirst = q;\n\t\t}\n\t}\n\n\tsigdelset(&list->signal, sig);\n\n\tif (first) {\nstill_pending:\n\t\tlist_del_init(&first->list);\n\t\tcopy_siginfo(info, &first->info);\n\n\t\t*resched_timer =\n\t\t\t(first->flags & SIGQUEUE_PREALLOC) &&\n\t\t\t(info->si_code == SI_TIMER) &&\n\t\t\t(info->si_sys_private);\n\n\t\t__sigqueue_free(first);\n\t} else {\n\t\t/*\n\t\t * Ok, it wasn't in the queue.  This must be\n\t\t * a fast-pathed signal or we must have been\n\t\t * out of queue space.  So zero out the info.\n\t\t */\n\t\tclear_siginfo(info);\n\t\tinfo->si_signo = sig;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\tinfo->si_pid = 0;\n\t\tinfo->si_uid = 0;\n\t}\n}\n\nstatic int __dequeue_signal(struct sigpending *pending, sigset_t *mask,\n\t\t\tkernel_siginfo_t *info, bool *resched_timer)\n{\n\tint sig = next_signal(pending, mask);\n\n\tif (sig)\n\t\tcollect_signal(sig, pending, info, resched_timer);\n\treturn sig;\n}\n\n/*\n * Dequeue a signal and return the element to the caller, which is\n * expected to free it.\n *\n * All callers have to hold the siglock.\n */\nint dequeue_signal(struct task_struct *tsk, sigset_t *mask, kernel_siginfo_t *info)\n{\n\tbool resched_timer = false;\n\tint signr;\n\n\t/* We only dequeue private signals from ourselves, we don't let\n\t * signalfd steal them\n\t */\n\tsignr = __dequeue_signal(&tsk->pending, mask, info, &resched_timer);\n\tif (!signr) {\n\t\tsignr = __dequeue_signal(&tsk->signal->shared_pending,\n\t\t\t\t\t mask, info, &resched_timer);\n#ifdef CONFIG_POSIX_TIMERS\n\t\t/*\n\t\t * itimer signal ?\n\t\t *\n\t\t * itimers are process shared and we restart periodic\n\t\t * itimers in the signal delivery path to prevent DoS\n\t\t * attacks in the high resolution timer case. This is\n\t\t * compliant with the old way of self-restarting\n\t\t * itimers, as the SIGALRM is a legacy signal and only\n\t\t * queued once. Changing the restart behaviour to\n\t\t * restart the timer in the signal dequeue path is\n\t\t * reducing the timer noise on heavy loaded !highres\n\t\t * systems too.\n\t\t */\n\t\tif (unlikely(signr == SIGALRM)) {\n\t\t\tstruct hrtimer *tmr = &tsk->signal->real_timer;\n\n\t\t\tif (!hrtimer_is_queued(tmr) &&\n\t\t\t    tsk->signal->it_real_incr != 0) {\n\t\t\t\thrtimer_forward(tmr, tmr->base->get_time(),\n\t\t\t\t\t\ttsk->signal->it_real_incr);\n\t\t\t\thrtimer_restart(tmr);\n\t\t\t}\n\t\t}\n#endif\n\t}\n\n\trecalc_sigpending();\n\tif (!signr)\n\t\treturn 0;\n\n\tif (unlikely(sig_kernel_stop(signr))) {\n\t\t/*\n\t\t * Set a marker that we have dequeued a stop signal.  Our\n\t\t * caller might release the siglock and then the pending\n\t\t * stop signal it is about to process is no longer in the\n\t\t * pending bitmasks, but must still be cleared by a SIGCONT\n\t\t * (and overruled by a SIGKILL).  So those cases clear this\n\t\t * shared flag after we've set it.  Note that this flag may\n\t\t * remain set after the signal we return is ignored or\n\t\t * handled.  That doesn't matter because its only purpose\n\t\t * is to alert stop-signal processing code when another\n\t\t * processor has come along and cleared the flag.\n\t\t */\n\t\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\t}\n#ifdef CONFIG_POSIX_TIMERS\n\tif (resched_timer) {\n\t\t/*\n\t\t * Release the siglock to ensure proper locking order\n\t\t * of timer locks outside of siglocks.  Note, we leave\n\t\t * irqs disabled here, since the posix-timers code is\n\t\t * about to disable them again anyway.\n\t\t */\n\t\tspin_unlock(&tsk->sighand->siglock);\n\t\tposixtimer_rearm(info);\n\t\tspin_lock(&tsk->sighand->siglock);\n\n\t\t/* Don't expose the si_sys_private value to userspace */\n\t\tinfo->si_sys_private = 0;\n\t}\n#endif\n\treturn signr;\n}\nEXPORT_SYMBOL_GPL(dequeue_signal);\n\nstatic int dequeue_synchronous_signal(kernel_siginfo_t *info)\n{\n\tstruct task_struct *tsk = current;\n\tstruct sigpending *pending = &tsk->pending;\n\tstruct sigqueue *q, *sync = NULL;\n\n\t/*\n\t * Might a synchronous signal be in the queue?\n\t */\n\tif (!((pending->signal.sig[0] & ~tsk->blocked.sig[0]) & SYNCHRONOUS_MASK))\n\t\treturn 0;\n\n\t/*\n\t * Return the first synchronous signal in the queue.\n\t */\n\tlist_for_each_entry(q, &pending->list, list) {\n\t\t/* Synchronous signals have a postive si_code */\n\t\tif ((q->info.si_code > SI_USER) &&\n\t\t    (sigmask(q->info.si_signo) & SYNCHRONOUS_MASK)) {\n\t\t\tsync = q;\n\t\t\tgoto next;\n\t\t}\n\t}\n\treturn 0;\nnext:\n\t/*\n\t * Check if there is another siginfo for the same signal.\n\t */\n\tlist_for_each_entry_continue(q, &pending->list, list) {\n\t\tif (q->info.si_signo == sync->info.si_signo)\n\t\t\tgoto still_pending;\n\t}\n\n\tsigdelset(&pending->signal, sync->info.si_signo);\n\trecalc_sigpending();\nstill_pending:\n\tlist_del_init(&sync->list);\n\tcopy_siginfo(info, &sync->info);\n\t__sigqueue_free(sync);\n\treturn info->si_signo;\n}\n\n/*\n * Tell a process that it has a new active signal..\n *\n * NOTE! we rely on the previous spin_lock to\n * lock interrupts for us! We can only be called with\n * \"siglock\" held, and the local interrupt must\n * have been disabled when that got acquired!\n *\n * No need to set need_resched since signal event passing\n * goes through ->blocked\n */\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}\n\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n */\nstatic void flush_sigqueue_mask(sigset_t *mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\tsigset_t m;\n\n\tsigandsets(&m, mask, &s->signal);\n\tif (sigisemptyset(&m))\n\t\treturn;\n\n\tsigandnsets(&s->signal, &s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (sigismember(mask, q->info.si_signo)) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n}\n\nstatic inline int is_si_special(const struct kernel_siginfo *info)\n{\n\treturn info <= SEND_SIG_PRIV;\n}\n\nstatic inline bool si_fromuser(const struct kernel_siginfo *info)\n{\n\treturn info == SEND_SIG_NOINFO ||\n\t\t(!is_si_special(info) && SI_FROMUSER(info));\n}\n\n/*\n * called with RCU read lock from check_kill_permission()\n */\nstatic bool kill_ok_by_cred(struct task_struct *t)\n{\n\tconst struct cred *cred = current_cred();\n\tconst struct cred *tcred = __task_cred(t);\n\n\treturn uid_eq(cred->euid, tcred->suid) ||\n\t       uid_eq(cred->euid, tcred->uid) ||\n\t       uid_eq(cred->uid, tcred->suid) ||\n\t       uid_eq(cred->uid, tcred->uid) ||\n\t       ns_capable(tcred->user_ns, CAP_KILL);\n}\n\n/*\n * Bad permissions for sending the signal\n * - the caller must hold the RCU read lock\n */\nstatic int check_kill_permission(int sig, struct kernel_siginfo *info,\n\t\t\t\t struct task_struct *t)\n{\n\tstruct pid *sid;\n\tint error;\n\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\tif (!si_fromuser(info))\n\t\treturn 0;\n\n\terror = audit_signal_info(sig, t); /* Let audit system see the signal */\n\tif (error)\n\t\treturn error;\n\n\tif (!same_thread_group(current, t) &&\n\t    !kill_ok_by_cred(t)) {\n\t\tswitch (sig) {\n\t\tcase SIGCONT:\n\t\t\tsid = task_session(t);\n\t\t\t/*\n\t\t\t * We don't return the error if sid == NULL. The\n\t\t\t * task was unhashed, the caller must notice this.\n\t\t\t */\n\t\t\tif (!sid || sid == task_session(current))\n\t\t\t\tbreak;\n\t\t\t/* fall through */\n\t\tdefault:\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n\n\treturn security_task_kill(t, info, sig, NULL);\n}\n\n/**\n * ptrace_trap_notify - schedule trap to notify ptracer\n * @t: tracee wanting to notify tracer\n *\n * This function schedules sticky ptrace trap which is cleared on the next\n * TRAP_STOP to notify ptracer of an event.  @t must have been seized by\n * ptracer.\n *\n * If @t is running, STOP trap will be taken.  If trapped for STOP and\n * ptracer is listening for events, tracee is woken up so that it can\n * re-trap for the new event.  If trapped otherwise, STOP trap will be\n * eventually taken without returning to userland after the existing traps\n * are finished by PTRACE_CONT.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nstatic void ptrace_trap_notify(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n\tassert_spin_locked(&t->sighand->siglock);\n\n\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}\n\n/*\n * Handle magic process-wide effects of stop/continue signals. Unlike\n * the signal actions, these happen immediately at signal-generation\n * time regardless of blocking, ignoring, or handling.  This does the\n * actual continuing for SIGCONT, but not the actual stopping for stop\n * signals. The process stop is done as a signal action for SIG_DFL.\n *\n * Returns true if the signal should be actually delivered, otherwise\n * it should be dropped.\n */\nstatic bool prepare_signal(int sig, struct task_struct *p, bool force)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\tsigset_t flush;\n\n\tif (signal->flags & (SIGNAL_GROUP_EXIT | SIGNAL_GROUP_COREDUMP)) {\n\t\tif (!(signal->flags & SIGNAL_GROUP_EXIT))\n\t\t\treturn sig == SIGKILL;\n\t\t/*\n\t\t * The process is in the middle of dying, nothing to do.\n\t\t */\n\t} else if (sig_kernel_stop(sig)) {\n\t\t/*\n\t\t * This is a stop signal.  Remove SIGCONT from all queues.\n\t\t */\n\t\tsiginitset(&flush, sigmask(SIGCONT));\n\t\tflush_sigqueue_mask(&flush, &signal->shared_pending);\n\t\tfor_each_thread(p, t)\n\t\t\tflush_sigqueue_mask(&flush, &t->pending);\n\t} else if (sig == SIGCONT) {\n\t\tunsigned int why;\n\t\t/*\n\t\t * Remove all stop signals from all queues, wake all threads.\n\t\t */\n\t\tsiginitset(&flush, SIG_KERNEL_STOP_MASK);\n\t\tflush_sigqueue_mask(&flush, &signal->shared_pending);\n\t\tfor_each_thread(p, t) {\n\t\t\tflush_sigqueue_mask(&flush, &t->pending);\n\t\t\ttask_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);\n\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\twake_up_state(t, __TASK_STOPPED);\n\t\t\telse\n\t\t\t\tptrace_trap_notify(t);\n\t\t}\n\n\t\t/*\n\t\t * Notify the parent with CLD_CONTINUED if we were stopped.\n\t\t *\n\t\t * If we were in the middle of a group stop, we pretend it\n\t\t * was already finished, and then continued. Since SIGCHLD\n\t\t * doesn't queue we report only CLD_STOPPED, as if the next\n\t\t * CLD_CONTINUED was dropped.\n\t\t */\n\t\twhy = 0;\n\t\tif (signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\twhy |= SIGNAL_CLD_CONTINUED;\n\t\telse if (signal->group_stop_count)\n\t\t\twhy |= SIGNAL_CLD_STOPPED;\n\n\t\tif (why) {\n\t\t\t/*\n\t\t\t * The first thread which returns from do_signal_stop()\n\t\t\t * will take ->siglock, notice SIGNAL_CLD_MASK, and\n\t\t\t * notify its parent. See get_signal().\n\t\t\t */\n\t\t\tsignal_set_stop_flags(signal, why | SIGNAL_STOP_CONTINUED);\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tsignal->group_exit_code = 0;\n\t\t}\n\t}\n\n\treturn !sig_ignored(p, sig, force);\n}\n\n/*\n * Test if P wants to take SIG.  After we've checked all threads with this,\n * it's equivalent to finding no threads not blocking SIG.  Any threads not\n * blocking SIG were ruled out because they are not running and already\n * have pending signals.  Such threads will dequeue from the shared queue\n * as soon as they're available, so putting the signal on the shared queue\n * will be equivalent to sending it to one such thread.\n */\nstatic inline bool wants_signal(int sig, struct task_struct *p)\n{\n\tif (sigismember(&p->blocked, sig))\n\t\treturn false;\n\n\tif (p->flags & PF_EXITING)\n\t\treturn false;\n\n\tif (sig == SIGKILL)\n\t\treturn true;\n\n\tif (task_is_stopped_or_traced(p))\n\t\treturn false;\n\n\treturn task_curr(p) || !signal_pending(p);\n}\n\nstatic void complete_signal(int sig, struct task_struct *p, enum pid_type type)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\t/*\n\t * Now find a thread we can wake up to take the signal off the queue.\n\t *\n\t * If the main thread wants the signal, it gets first crack.\n\t * Probably the least surprising to the average bear.\n\t */\n\tif (wants_signal(sig, p))\n\t\tt = p;\n\telse if ((type == PIDTYPE_PID) || thread_group_empty(p))\n\t\t/*\n\t\t * There is just one thread and it does not need to be woken.\n\t\t * It will dequeue unblocked signals before it runs again.\n\t\t */\n\t\treturn;\n\telse {\n\t\t/*\n\t\t * Otherwise try to find a suitable thread.\n\t\t */\n\t\tt = signal->curr_target;\n\t\twhile (!wants_signal(sig, t)) {\n\t\t\tt = next_thread(t);\n\t\t\tif (t == signal->curr_target)\n\t\t\t\t/*\n\t\t\t\t * No thread needs to be woken.\n\t\t\t\t * Any eligible threads will see\n\t\t\t\t * the signal in the queue soon.\n\t\t\t\t */\n\t\t\t\treturn;\n\t\t}\n\t\tsignal->curr_target = t;\n\t}\n\n\t/*\n\t * Found a killable thread.  If the signal will be fatal,\n\t * then start taking the whole group down immediately.\n\t */\n\tif (sig_fatal(p, sig) &&\n\t    !(signal->flags & SIGNAL_GROUP_EXIT) &&\n\t    !sigismember(&t->real_blocked, sig) &&\n\t    (sig == SIGKILL || !p->ptrace)) {\n\t\t/*\n\t\t * This signal will be fatal to the whole group.\n\t\t */\n\t\tif (!sig_kernel_coredump(sig)) {\n\t\t\t/*\n\t\t\t * Start a group exit and wake everybody up.\n\t\t\t * This way we don't have other threads\n\t\t\t * running and doing things after a slower\n\t\t\t * thread has the fatal signal pending.\n\t\t\t */\n\t\t\tsignal->flags = SIGNAL_GROUP_EXIT;\n\t\t\tsignal->group_exit_code = sig;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tt = p;\n\t\t\tdo {\n\t\t\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\t\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\t\t\tsignal_wake_up(t, 1);\n\t\t\t} while_each_thread(p, t);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * The signal is already in the shared-pending queue.\n\t * Tell the chosen thread to wake up and dequeue it.\n\t */\n\tsignal_wake_up(t, sig == SIGKILL);\n\treturn;\n}\n\nstatic inline bool legacy_queue(struct sigpending *signals, int sig)\n{\n\treturn (sig < SIGRTMIN) && sigismember(&signals->signal, sig);\n}\n\nstatic int __send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t,\n\t\t\tenum pid_type type, bool force)\n{\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint override_rlimit;\n\tint ret = 0, result;\n\n\tassert_spin_locked(&t->sighand->siglock);\n\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t, force))\n\t\tgoto ret;\n\n\tpending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\n\t/*\n\t * Short-circuit ignored signals and support queuing\n\t * exactly one non-rt signal, so that we can get more\n\t * detailed information about the cause of the signal.\n\t */\n\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\tif (legacy_queue(pending, sig))\n\t\tgoto ret;\n\n\tresult = TRACE_SIGNAL_DELIVERED;\n\t/*\n\t * Skip useless siginfo allocation for SIGKILL and kernel threads.\n\t */\n\tif ((sig == SIGKILL) || (t->flags & PF_KTHREAD))\n\t\tgoto out_set;\n\n\t/*\n\t * Real-time signals must be queued if sent by sigqueue, or\n\t * some other real-time mechanism.  It is implementation\n\t * defined whether kill() does so.  We attempt to do so, on\n\t * the principle of least surprise, but since kill is not\n\t * allowed to fail with EAGAIN when low on memory we just\n\t * make sure at least one signal gets delivered and don't\n\t * pass on the info struct.\n\t */\n\tif (sig < SIGRTMIN)\n\t\toverride_rlimit = (is_si_special(info) || info->si_code >= 0);\n\telse\n\t\toverride_rlimit = 0;\n\n\tq = __sigqueue_alloc(sig, t, GFP_ATOMIC, override_rlimit);\n\tif (q) {\n\t\tlist_add_tail(&q->list, &pending->list);\n\t\tswitch ((unsigned long) info) {\n\t\tcase (unsigned long) SEND_SIG_NOINFO:\n\t\t\tclear_siginfo(&q->info);\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_USER;\n\t\t\tq->info.si_pid = task_tgid_nr_ns(current,\n\t\t\t\t\t\t\ttask_active_pid_ns(t));\n\t\t\trcu_read_lock();\n\t\t\tq->info.si_uid =\n\t\t\t\tfrom_kuid_munged(task_cred_xxx(t, user_ns),\n\t\t\t\t\t\t current_uid());\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\tcase (unsigned long) SEND_SIG_PRIV:\n\t\t\tclear_siginfo(&q->info);\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_KERNEL;\n\t\t\tq->info.si_pid = 0;\n\t\t\tq->info.si_uid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcopy_siginfo(&q->info, info);\n\t\t\tbreak;\n\t\t}\n\t} else if (!is_si_special(info) &&\n\t\t   sig >= SIGRTMIN && info->si_code != SI_USER) {\n\t\t/*\n\t\t * Queue overflow, abort.  We may abort if the\n\t\t * signal was rt and sent by user using something\n\t\t * other than kill().\n\t\t */\n\t\tresult = TRACE_SIGNAL_OVERFLOW_FAIL;\n\t\tret = -EAGAIN;\n\t\tgoto ret;\n\t} else {\n\t\t/*\n\t\t * This is a silent loss of information.  We still\n\t\t * send the signal, but the *info bits are lost.\n\t\t */\n\t\tresult = TRACE_SIGNAL_LOSE_INFO;\n\t}\n\nout_set:\n\tsignalfd_notify(t, sig);\n\tsigaddset(&pending->signal, sig);\n\n\t/* Let multiprocess signals appear after on-going forks */\n\tif (type > PIDTYPE_TGID) {\n\t\tstruct multiprocess_signals *delayed;\n\t\thlist_for_each_entry(delayed, &t->signal->multiprocess, node) {\n\t\t\tsigset_t *signal = &delayed->signal;\n\t\t\t/* Can't queue both a stop and a continue signal */\n\t\t\tif (sig == SIGCONT)\n\t\t\t\tsigdelsetmask(signal, SIG_KERNEL_STOP_MASK);\n\t\t\telse if (sig_kernel_stop(sig))\n\t\t\t\tsigdelset(signal, SIGCONT);\n\t\t\tsigaddset(signal, sig);\n\t\t}\n\t}\n\n\tcomplete_signal(sig, t, type);\nret:\n\ttrace_signal_generate(sig, info, t, type != PIDTYPE_PID, result);\n\treturn ret;\n}\n\nstatic inline bool has_si_pid_and_uid(struct kernel_siginfo *info)\n{\n\tbool ret = false;\n\tswitch (siginfo_layout(info->si_signo, info->si_code)) {\n\tcase SIL_KILL:\n\tcase SIL_CHLD:\n\tcase SIL_RT:\n\t\tret = true;\n\t\tbreak;\n\tcase SIL_TIMER:\n\tcase SIL_POLL:\n\tcase SIL_FAULT:\n\tcase SIL_FAULT_MCEERR:\n\tcase SIL_FAULT_BNDERR:\n\tcase SIL_FAULT_PKUERR:\n\tcase SIL_SYS:\n\t\tret = false;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t,\n\t\t\tenum pid_type type)\n{\n\t/* Should SIGKILL or SIGSTOP be received by a pid namespace init? */\n\tbool force = false;\n\n\tif (info == SEND_SIG_NOINFO) {\n\t\t/* Force if sent from an ancestor pid namespace */\n\t\tforce = !task_pid_nr_ns(current, task_active_pid_ns(t));\n\t} else if (info == SEND_SIG_PRIV) {\n\t\t/* Don't ignore kernel generated signals */\n\t\tforce = true;\n\t} else if (has_si_pid_and_uid(info)) {\n\t\t/* SIGKILL and SIGSTOP is special or has ids */\n\t\tstruct user_namespace *t_user_ns;\n\n\t\trcu_read_lock();\n\t\tt_user_ns = task_cred_xxx(t, user_ns);\n\t\tif (current_user_ns() != t_user_ns) {\n\t\t\tkuid_t uid = make_kuid(current_user_ns(), info->si_uid);\n\t\t\tinfo->si_uid = from_kuid_munged(t_user_ns, uid);\n\t\t}\n\t\trcu_read_unlock();\n\n\t\t/* A kernel generated signal? */\n\t\tforce = (info->si_code == SI_KERNEL);\n\n\t\t/* From an ancestor pid namespace? */\n\t\tif (!task_pid_nr_ns(current, task_active_pid_ns(t))) {\n\t\t\tinfo->si_pid = 0;\n\t\t\tforce = true;\n\t\t}\n\t}\n\treturn __send_signal(sig, info, t, type, force);\n}\n\nstatic void print_fatal_signal(int signr)\n{\n\tstruct pt_regs *regs = signal_pt_regs();\n\tpr_info(\"potentially unexpected fatal signal %d.\\n\", signr);\n\n#if defined(__i386__) && !defined(__arch_um__)\n\tpr_info(\"code at %08lx: \", regs->ip);\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < 16; i++) {\n\t\t\tunsigned char insn;\n\n\t\t\tif (get_user(insn, (unsigned char *)(regs->ip + i)))\n\t\t\t\tbreak;\n\t\t\tpr_cont(\"%02x \", insn);\n\t\t}\n\t}\n\tpr_cont(\"\\n\");\n#endif\n\tpreempt_disable();\n\tshow_regs(regs);\n\tpreempt_enable();\n}\n\nstatic int __init setup_print_fatal_signals(char *str)\n{\n\tget_option (&str, &print_fatal_signals);\n\n\treturn 1;\n}\n\n__setup(\"print-fatal-signals=\", setup_print_fatal_signals);\n\nint\n__group_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p)\n{\n\treturn send_signal(sig, info, p, PIDTYPE_TGID);\n}\n\nint do_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p,\n\t\t\tenum pid_type type)\n{\n\tunsigned long flags;\n\tint ret = -ESRCH;\n\n\tif (lock_task_sighand(p, &flags)) {\n\t\tret = send_signal(sig, info, p, type);\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Force a signal that the process can't ignore: if necessary\n * we unblock the signal and change any SIG_IGN to SIG_DFL.\n *\n * Note: If we unblock the signal, we always reset it to SIG_DFL,\n * since we do not want to have a signal handler that was blocked\n * be invoked when user space had explicitly blocked it.\n *\n * We don't want to have recursive SIGSEGV's etc, for example,\n * that is why we also clear SIGNAL_UNKILLABLE.\n */\nstatic int\nforce_sig_info_to_task(struct kernel_siginfo *info, struct task_struct *t)\n{\n\tunsigned long int flags;\n\tint ret, blocked, ignored;\n\tstruct k_sigaction *action;\n\tint sig = info->si_signo;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\taction = &t->sighand->action[sig-1];\n\tignored = action->sa.sa_handler == SIG_IGN;\n\tblocked = sigismember(&t->blocked, sig);\n\tif (blocked || ignored) {\n\t\taction->sa.sa_handler = SIG_DFL;\n\t\tif (blocked) {\n\t\t\tsigdelset(&t->blocked, sig);\n\t\t\trecalc_sigpending_and_wake(t);\n\t\t}\n\t}\n\t/*\n\t * Don't clear SIGNAL_UNKILLABLE for traced tasks, users won't expect\n\t * debugging to leave init killable.\n\t */\n\tif (action->sa.sa_handler == SIG_DFL && !t->ptrace)\n\t\tt->signal->flags &= ~SIGNAL_UNKILLABLE;\n\tret = send_signal(sig, info, t, PIDTYPE_PID);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n\n\treturn ret;\n}\n\nint force_sig_info(struct kernel_siginfo *info)\n{\n\treturn force_sig_info_to_task(info, current);\n}\n\n/*\n * Nuke all other threads in the group.\n */\nint zap_other_threads(struct task_struct *p)\n{\n\tstruct task_struct *t = p;\n\tint count = 0;\n\n\tp->signal->group_stop_count = 0;\n\n\twhile_each_thread(p, t) {\n\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\tcount++;\n\n\t\t/* Don't bother with already dead threads */\n\t\tif (t->exit_state)\n\t\t\tcontinue;\n\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\tsignal_wake_up(t, 1);\n\t}\n\n\treturn count;\n}\n\nstruct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t   unsigned long *flags)\n{\n\tstruct sighand_struct *sighand;\n\n\trcu_read_lock();\n\tfor (;;) {\n\t\tsighand = rcu_dereference(tsk->sighand);\n\t\tif (unlikely(sighand == NULL))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * This sighand can be already freed and even reused, but\n\t\t * we rely on SLAB_TYPESAFE_BY_RCU and sighand_ctor() which\n\t\t * initializes ->siglock: this slab can't go away, it has\n\t\t * the same object type, ->siglock can't be reinitialized.\n\t\t *\n\t\t * We need to ensure that tsk->sighand is still the same\n\t\t * after we take the lock, we can race with de_thread() or\n\t\t * __exit_signal(). In the latter case the next iteration\n\t\t * must see ->sighand == NULL.\n\t\t */\n\t\tspin_lock_irqsave(&sighand->siglock, *flags);\n\t\tif (likely(sighand == rcu_access_pointer(tsk->sighand)))\n\t\t\tbreak;\n\t\tspin_unlock_irqrestore(&sighand->siglock, *flags);\n\t}\n\trcu_read_unlock();\n\n\treturn sighand;\n}\n\n/*\n * send signal info to all the members of a group\n */\nint group_send_sig_info(int sig, struct kernel_siginfo *info,\n\t\t\tstruct task_struct *p, enum pid_type type)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = check_kill_permission(sig, info, p);\n\trcu_read_unlock();\n\n\tif (!ret && sig)\n\t\tret = do_send_sig_info(sig, info, p, type);\n\n\treturn ret;\n}\n\n/*\n * __kill_pgrp_info() sends a signal to a process group: this is what the tty\n * control characters do (^C, ^Z etc)\n * - the caller must hold at least a readlock on tasklist_lock\n */\nint __kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp)\n{\n\tstruct task_struct *p = NULL;\n\tint retval, success;\n\n\tsuccess = 0;\n\tretval = -ESRCH;\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tint err = group_send_sig_info(sig, info, p, PIDTYPE_PGID);\n\t\tsuccess |= !err;\n\t\tretval = err;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\treturn success ? 0 : retval;\n}\n\nint kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid)\n{\n\tint error = -ESRCH;\n\tstruct task_struct *p;\n\n\tfor (;;) {\n\t\trcu_read_lock();\n\t\tp = pid_task(pid, PIDTYPE_PID);\n\t\tif (p)\n\t\t\terror = group_send_sig_info(sig, info, p, PIDTYPE_TGID);\n\t\trcu_read_unlock();\n\t\tif (likely(!p || error != -ESRCH))\n\t\t\treturn error;\n\n\t\t/*\n\t\t * The task was unhashed in between, try again.  If it\n\t\t * is dead, pid_task() will return NULL, if we race with\n\t\t * de_thread() it will find the new leader.\n\t\t */\n\t}\n}\n\nstatic int kill_proc_info(int sig, struct kernel_siginfo *info, pid_t pid)\n{\n\tint error;\n\trcu_read_lock();\n\terror = kill_pid_info(sig, info, find_vpid(pid));\n\trcu_read_unlock();\n\treturn error;\n}\n\nstatic inline bool kill_as_cred_perm(const struct cred *cred,\n\t\t\t\t     struct task_struct *target)\n{\n\tconst struct cred *pcred = __task_cred(target);\n\n\treturn uid_eq(cred->euid, pcred->suid) ||\n\t       uid_eq(cred->euid, pcred->uid) ||\n\t       uid_eq(cred->uid, pcred->suid) ||\n\t       uid_eq(cred->uid, pcred->uid);\n}\n\n/*\n * The usb asyncio usage of siginfo is wrong.  The glibc support\n * for asyncio which uses SI_ASYNCIO assumes the layout is SIL_RT.\n * AKA after the generic fields:\n *\tkernel_pid_t\tsi_pid;\n *\tkernel_uid32_t\tsi_uid;\n *\tsigval_t\tsi_value;\n *\n * Unfortunately when usb generates SI_ASYNCIO it assumes the layout\n * after the generic fields is:\n *\tvoid __user \t*si_addr;\n *\n * This is a practical problem when there is a 64bit big endian kernel\n * and a 32bit userspace.  As the 32bit address will encoded in the low\n * 32bits of the pointer.  Those low 32bits will be stored at higher\n * address than appear in a 32 bit pointer.  So userspace will not\n * see the address it was expecting for it's completions.\n *\n * There is nothing in the encoding that can allow\n * copy_siginfo_to_user32 to detect this confusion of formats, so\n * handle this by requiring the caller of kill_pid_usb_asyncio to\n * notice when this situration takes place and to store the 32bit\n * pointer in sival_int, instead of sival_addr of the sigval_t addr\n * parameter.\n */\nint kill_pid_usb_asyncio(int sig, int errno, sigval_t addr,\n\t\t\t struct pid *pid, const struct cred *cred)\n{\n\tstruct kernel_siginfo info;\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tint ret = -EINVAL;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = errno;\n\tinfo.si_code = SI_ASYNCIO;\n\t*((sigval_t *)&info.si_pid) = addr;\n\n\tif (!valid_signal(sig))\n\t\treturn ret;\n\n\trcu_read_lock();\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (!p) {\n\t\tret = -ESRCH;\n\t\tgoto out_unlock;\n\t}\n\tif (!kill_as_cred_perm(cred, p)) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\tret = security_task_kill(p, &info, sig, cred);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tif (sig) {\n\t\tif (lock_task_sighand(p, &flags)) {\n\t\t\tret = __send_signal(sig, &info, p, PIDTYPE_TGID, false);\n\t\t\tunlock_task_sighand(p, &flags);\n\t\t} else\n\t\t\tret = -ESRCH;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kill_pid_usb_asyncio);\n\n/*\n * kill_something_info() interprets pid in interesting ways just like kill(2).\n *\n * POSIX specifies that kill(-1,sig) is unspecified, but what we have\n * is probably wrong.  Should make it like BSD or SYSV.\n */\n\nstatic int kill_something_info(int sig, struct kernel_siginfo *info, pid_t pid)\n{\n\tint ret;\n\n\tif (pid > 0) {\n\t\trcu_read_lock();\n\t\tret = kill_pid_info(sig, info, find_vpid(pid));\n\t\trcu_read_unlock();\n\t\treturn ret;\n\t}\n\n\t/* -INT_MIN is undefined.  Exclude this case to avoid a UBSAN warning */\n\tif (pid == INT_MIN)\n\t\treturn -ESRCH;\n\n\tread_lock(&tasklist_lock);\n\tif (pid != -1) {\n\t\tret = __kill_pgrp_info(sig, info,\n\t\t\t\tpid ? find_vpid(-pid) : task_pgrp(current));\n\t} else {\n\t\tint retval = 0, count = 0;\n\t\tstruct task_struct * p;\n\n\t\tfor_each_process(p) {\n\t\t\tif (task_pid_vnr(p) > 1 &&\n\t\t\t\t\t!same_thread_group(p, current)) {\n\t\t\t\tint err = group_send_sig_info(sig, info, p,\n\t\t\t\t\t\t\t      PIDTYPE_MAX);\n\t\t\t\t++count;\n\t\t\t\tif (err != -EPERM)\n\t\t\t\t\tretval = err;\n\t\t\t}\n\t\t}\n\t\tret = count ? retval : -ESRCH;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * These are for backward compatibility with the rest of the kernel source.\n */\n\nint send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p)\n{\n\t/*\n\t * Make sure legacy kernel users don't send in bad values\n\t * (normal paths check this in check_kill_permission).\n\t */\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\treturn do_send_sig_info(sig, info, p, PIDTYPE_PID);\n}\nEXPORT_SYMBOL(send_sig_info);\n\n#define __si_special(priv) \\\n\t((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)\n\nint\nsend_sig(int sig, struct task_struct *p, int priv)\n{\n\treturn send_sig_info(sig, __si_special(priv), p);\n}\nEXPORT_SYMBOL(send_sig);\n\nvoid force_sig(int sig)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_KERNEL;\n\tinfo.si_pid = 0;\n\tinfo.si_uid = 0;\n\tforce_sig_info(&info);\n}\nEXPORT_SYMBOL(force_sig);\n\n/*\n * When things go south during signal handling, we\n * will force a SIGSEGV. And if the signal that caused\n * the problem was already a SIGSEGV, we'll want to\n * make sure we don't even try to deliver the signal..\n */\nvoid force_sigsegv(int sig)\n{\n\tstruct task_struct *p = current;\n\n\tif (sig == SIGSEGV) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&p->sighand->siglock, flags);\n\t\tp->sighand->action[sig - 1].sa.sa_handler = SIG_DFL;\n\t\tspin_unlock_irqrestore(&p->sighand->siglock, flags);\n\t}\n\tforce_sig(SIGSEGV);\n}\n\nint force_sig_fault_to_task(int sig, int code, void __user *addr\n\t___ARCH_SI_TRAPNO(int trapno)\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n\t, struct task_struct *t)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = code;\n\tinfo.si_addr  = addr;\n#ifdef __ARCH_SI_TRAPNO\n\tinfo.si_trapno = trapno;\n#endif\n#ifdef __ia64__\n\tinfo.si_imm = imm;\n\tinfo.si_flags = flags;\n\tinfo.si_isr = isr;\n#endif\n\treturn force_sig_info_to_task(&info, t);\n}\n\nint force_sig_fault(int sig, int code, void __user *addr\n\t___ARCH_SI_TRAPNO(int trapno)\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr))\n{\n\treturn force_sig_fault_to_task(sig, code, addr\n\t\t\t\t       ___ARCH_SI_TRAPNO(trapno)\n\t\t\t\t       ___ARCH_SI_IA64(imm, flags, isr), current);\n}\n\nint send_sig_fault(int sig, int code, void __user *addr\n\t___ARCH_SI_TRAPNO(int trapno)\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n\t, struct task_struct *t)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = code;\n\tinfo.si_addr  = addr;\n#ifdef __ARCH_SI_TRAPNO\n\tinfo.si_trapno = trapno;\n#endif\n#ifdef __ia64__\n\tinfo.si_imm = imm;\n\tinfo.si_flags = flags;\n\tinfo.si_isr = isr;\n#endif\n\treturn send_sig_info(info.si_signo, &info, t);\n}\n\nint force_sig_mceerr(int code, void __user *addr, short lsb)\n{\n\tstruct kernel_siginfo info;\n\n\tWARN_ON((code != BUS_MCEERR_AO) && (code != BUS_MCEERR_AR));\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = code;\n\tinfo.si_addr = addr;\n\tinfo.si_addr_lsb = lsb;\n\treturn force_sig_info(&info);\n}\n\nint send_sig_mceerr(int code, void __user *addr, short lsb, struct task_struct *t)\n{\n\tstruct kernel_siginfo info;\n\n\tWARN_ON((code != BUS_MCEERR_AO) && (code != BUS_MCEERR_AR));\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = code;\n\tinfo.si_addr = addr;\n\tinfo.si_addr_lsb = lsb;\n\treturn send_sig_info(info.si_signo, &info, t);\n}\nEXPORT_SYMBOL(send_sig_mceerr);\n\nint force_sig_bnderr(void __user *addr, void __user *lower, void __user *upper)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGSEGV;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = SEGV_BNDERR;\n\tinfo.si_addr  = addr;\n\tinfo.si_lower = lower;\n\tinfo.si_upper = upper;\n\treturn force_sig_info(&info);\n}\n\n#ifdef SEGV_PKUERR\nint force_sig_pkuerr(void __user *addr, u32 pkey)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGSEGV;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = SEGV_PKUERR;\n\tinfo.si_addr  = addr;\n\tinfo.si_pkey  = pkey;\n\treturn force_sig_info(&info);\n}\n#endif\n\n/* For the crazy architectures that include trap information in\n * the errno field, instead of an actual errno value.\n */\nint force_sig_ptrace_errno_trap(int errno, void __user *addr)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = errno;\n\tinfo.si_code  = TRAP_HWBKPT;\n\tinfo.si_addr  = addr;\n\treturn force_sig_info(&info);\n}\n\nint kill_pgrp(struct pid *pid, int sig, int priv)\n{\n\tint ret;\n\n\tread_lock(&tasklist_lock);\n\tret = __kill_pgrp_info(sig, __si_special(priv), pid);\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kill_pgrp);\n\nint kill_pid(struct pid *pid, int sig, int priv)\n{\n\treturn kill_pid_info(sig, __si_special(priv), pid);\n}\nEXPORT_SYMBOL(kill_pid);\n\n/*\n * These functions support sending signals using preallocated sigqueue\n * structures.  This is needed \"because realtime applications cannot\n * afford to lose notifications of asynchronous events, like timer\n * expirations or I/O completions\".  In the case of POSIX Timers\n * we allocate the sigqueue structure from the timer_create.  If this\n * allocation fails we are able to report the failure to the application\n * with an EAGAIN error.\n */\nstruct sigqueue *sigqueue_alloc(void)\n{\n\tstruct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);\n\n\tif (q)\n\t\tq->flags |= SIGQUEUE_PREALLOC;\n\n\treturn q;\n}\n\nvoid sigqueue_free(struct sigqueue *q)\n{\n\tunsigned long flags;\n\tspinlock_t *lock = &current->sighand->siglock;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\t/*\n\t * We must hold ->siglock while testing q->list\n\t * to serialize with collect_signal() or with\n\t * __exit_signal()->flush_sigqueue().\n\t */\n\tspin_lock_irqsave(lock, flags);\n\tq->flags &= ~SIGQUEUE_PREALLOC;\n\t/*\n\t * If it is queued it will be freed when dequeued,\n\t * like the \"regular\" sigqueue.\n\t */\n\tif (!list_empty(&q->list))\n\t\tq = NULL;\n\tspin_unlock_irqrestore(lock, flags);\n\n\tif (q)\n\t\t__sigqueue_free(q);\n}\n\nint send_sigqueue(struct sigqueue *q, struct pid *pid, enum pid_type type)\n{\n\tint sig = q->info.si_signo;\n\tstruct sigpending *pending;\n\tstruct task_struct *t;\n\tunsigned long flags;\n\tint ret, result;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\n\tret = -1;\n\trcu_read_lock();\n\tt = pid_task(pid, type);\n\tif (!t || !likely(lock_task_sighand(t, &flags)))\n\t\tgoto ret;\n\n\tret = 1; /* the signal is ignored */\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t, false))\n\t\tgoto out;\n\n\tret = 0;\n\tif (unlikely(!list_empty(&q->list))) {\n\t\t/*\n\t\t * If an SI_TIMER entry is already queue just increment\n\t\t * the overrun count.\n\t\t */\n\t\tBUG_ON(q->info.si_code != SI_TIMER);\n\t\tq->info.si_overrun++;\n\t\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\t\tgoto out;\n\t}\n\tq->info.si_overrun = 0;\n\n\tsignalfd_notify(t, sig);\n\tpending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\n\tlist_add_tail(&q->list, &pending->list);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, type);\n\tresult = TRACE_SIGNAL_DELIVERED;\nout:\n\ttrace_signal_generate(sig, &q->info, t, type != PIDTYPE_PID, result);\n\tunlock_task_sighand(t, &flags);\nret:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic void do_notify_pidfd(struct task_struct *task)\n{\n\tstruct pid *pid;\n\n\tWARN_ON(task->exit_state == 0);\n\tpid = task_pid(task);\n\twake_up_all(&pid->wait_pidfd);\n}\n\n/*\n * Let a parent know about the death of a child.\n * For a stopped/continued status change, use do_notify_parent_cldstop instead.\n *\n * Returns true if our parent ignored us and so we've switched to\n * self-reaping.\n */\nbool do_notify_parent(struct task_struct *tsk, int sig)\n{\n\tstruct kernel_siginfo info;\n\tunsigned long flags;\n\tstruct sighand_struct *psig;\n\tbool autoreap = false;\n\tu64 utime, stime;\n\n\tBUG_ON(sig == -1);\n\n \t/* do_notify_parent_cldstop should have been called instead.  */\n \tBUG_ON(task_is_stopped_or_traced(tsk));\n\n\tBUG_ON(!tsk->ptrace &&\n\t       (tsk->group_leader != tsk || !thread_group_empty(tsk)));\n\n\t/* Wake up all pidfd waiters */\n\tdo_notify_pidfd(tsk);\n\n\tif (sig != SIGCHLD) {\n\t\t/*\n\t\t * This is only possible if parent == real_parent.\n\t\t * Check if it has changed security domain.\n\t\t */\n\t\tif (tsk->parent_exec_id != tsk->parent->self_exec_id)\n\t\t\tsig = SIGCHLD;\n\t}\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\t/*\n\t * We are under tasklist_lock here so our parent is tied to\n\t * us and cannot change.\n\t *\n\t * task_active_pid_ns will always return the same pid namespace\n\t * until a task passes through release_task.\n\t *\n\t * write_lock() currently calls preempt_disable() which is the\n\t * same as rcu_read_lock(), but according to Oleg, this is not\n\t * correct to rely on this\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),\n\t\t\t\t       task_uid(tsk));\n\trcu_read_unlock();\n\n\ttask_cputime(tsk, &utime, &stime);\n\tinfo.si_utime = nsec_to_clock_t(utime + tsk->signal->utime);\n\tinfo.si_stime = nsec_to_clock_t(stime + tsk->signal->stime);\n\n\tinfo.si_status = tsk->exit_code & 0x7f;\n\tif (tsk->exit_code & 0x80)\n\t\tinfo.si_code = CLD_DUMPED;\n\telse if (tsk->exit_code & 0x7f)\n\t\tinfo.si_code = CLD_KILLED;\n\telse {\n\t\tinfo.si_code = CLD_EXITED;\n\t\tinfo.si_status = tsk->exit_code >> 8;\n\t}\n\n\tpsig = tsk->parent->sighand;\n\tspin_lock_irqsave(&psig->siglock, flags);\n\tif (!tsk->ptrace && sig == SIGCHLD &&\n\t    (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||\n\t     (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {\n\t\t/*\n\t\t * We are exiting and our parent doesn't care.  POSIX.1\n\t\t * defines special semantics for setting SIGCHLD to SIG_IGN\n\t\t * or setting the SA_NOCLDWAIT flag: we should be reaped\n\t\t * automatically and not left for our parent's wait4 call.\n\t\t * Rather than having the parent do it as a magic kind of\n\t\t * signal handler, we just set this to tell do_exit that we\n\t\t * can be cleaned up without becoming a zombie.  Note that\n\t\t * we still call __wake_up_parent in this case, because a\n\t\t * blocked sys_wait4 might now return -ECHILD.\n\t\t *\n\t\t * Whether we send SIGCHLD or not for SA_NOCLDWAIT\n\t\t * is implementation-defined: we do (if you don't want\n\t\t * it, just use SIG_IGN instead).\n\t\t */\n\t\tautoreap = true;\n\t\tif (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)\n\t\t\tsig = 0;\n\t}\n\tif (valid_signal(sig) && sig)\n\t\t__group_send_sig_info(sig, &info, tsk->parent);\n\t__wake_up_parent(tsk, tsk->parent);\n\tspin_unlock_irqrestore(&psig->siglock, flags);\n\n\treturn autoreap;\n}\n\n/**\n * do_notify_parent_cldstop - notify parent of stopped/continued state change\n * @tsk: task reporting the state change\n * @for_ptracer: the notification is for ptracer\n * @why: CLD_{CONTINUED|STOPPED|TRAPPED} to report\n *\n * Notify @tsk's parent that the stopped/continued state has changed.  If\n * @for_ptracer is %false, @tsk's group leader notifies to its real parent.\n * If %true, @tsk reports to @tsk->parent which should be the ptracer.\n *\n * CONTEXT:\n * Must be called with tasklist_lock at least read locked.\n */\nstatic void do_notify_parent_cldstop(struct task_struct *tsk,\n\t\t\t\t     bool for_ptracer, int why)\n{\n\tstruct kernel_siginfo info;\n\tunsigned long flags;\n\tstruct task_struct *parent;\n\tstruct sighand_struct *sighand;\n\tu64 utime, stime;\n\n\tif (for_ptracer) {\n\t\tparent = tsk->parent;\n\t} else {\n\t\ttsk = tsk->group_leader;\n\t\tparent = tsk->real_parent;\n\t}\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGCHLD;\n\tinfo.si_errno = 0;\n\t/*\n\t * see comment in do_notify_parent() about the following 4 lines\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));\n\trcu_read_unlock();\n\n\ttask_cputime(tsk, &utime, &stime);\n\tinfo.si_utime = nsec_to_clock_t(utime);\n\tinfo.si_stime = nsec_to_clock_t(stime);\n\n \tinfo.si_code = why;\n \tswitch (why) {\n \tcase CLD_CONTINUED:\n \t\tinfo.si_status = SIGCONT;\n \t\tbreak;\n \tcase CLD_STOPPED:\n \t\tinfo.si_status = tsk->signal->group_exit_code & 0x7f;\n \t\tbreak;\n \tcase CLD_TRAPPED:\n \t\tinfo.si_status = tsk->exit_code & 0x7f;\n \t\tbreak;\n \tdefault:\n \t\tBUG();\n \t}\n\n\tsighand = parent->sighand;\n\tspin_lock_irqsave(&sighand->siglock, flags);\n\tif (sighand->action[SIGCHLD-1].sa.sa_handler != SIG_IGN &&\n\t    !(sighand->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))\n\t\t__group_send_sig_info(SIGCHLD, &info, parent);\n\t/*\n\t * Even if SIGCHLD is not generated, we must wake up wait4 calls.\n\t */\n\t__wake_up_parent(tsk, parent);\n\tspin_unlock_irqrestore(&sighand->siglock, flags);\n}\n\nstatic inline bool may_ptrace_stop(void)\n{\n\tif (!likely(current->ptrace))\n\t\treturn false;\n\t/*\n\t * Are we in the middle of do_coredump?\n\t * If so and our tracer is also part of the coredump stopping\n\t * is a deadlock situation, and pointless because our tracer\n\t * is dead so don't allow us to stop.\n\t * If SIGKILL was already sent before the caller unlocked\n\t * ->siglock we must see ->core_state != NULL. Otherwise it\n\t * is safe to enter schedule().\n\t *\n\t * This is almost outdated, a task with the pending SIGKILL can't\n\t * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported\n\t * after SIGKILL was already dequeued.\n\t */\n\tif (unlikely(current->mm->core_state) &&\n\t    unlikely(current->mm == current->parent->mm))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Return non-zero if there is a SIGKILL that should be waking us up.\n * Called with the siglock held.\n */\nstatic bool sigkill_pending(struct task_struct *tsk)\n{\n\treturn sigismember(&tsk->pending.signal, SIGKILL) ||\n\t       sigismember(&tsk->signal->shared_pending.signal, SIGKILL);\n}\n\n/*\n * This must be called with current->sighand->siglock held.\n *\n * This should be the path for all ptrace stops.\n * We always set current->last_siginfo while stopped here.\n * That makes it a way to test a stopped process for\n * being ptrace-stopped vs being job-control-stopped.\n *\n * If we actually decide not to stop at all because the tracer\n * is gone, we keep current->exit_code unless clear_code.\n */\nstatic void ptrace_stop(int exit_code, int why, int clear_code, kernel_siginfo_t *info)\n\t__releases(&current->sighand->siglock)\n\t__acquires(&current->sighand->siglock)\n{\n\tbool gstop_done = false;\n\n\tif (arch_ptrace_stop_needed(exit_code, info)) {\n\t\t/*\n\t\t * The arch code has something special to do before a\n\t\t * ptrace stop.  This is allowed to block, e.g. for faults\n\t\t * on user stack pages.  We can't keep the siglock while\n\t\t * calling arch_ptrace_stop, so we must release it now.\n\t\t * To preserve proper semantics, we must do this before\n\t\t * any signal bookkeeping like checking group_stop_count.\n\t\t * Meanwhile, a SIGKILL could come in before we retake the\n\t\t * siglock.  That must prevent us from sleeping in TASK_TRACED.\n\t\t * So after regaining the lock, we must check for SIGKILL.\n\t\t */\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tarch_ptrace_stop(exit_code, info);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\tif (sigkill_pending(current))\n\t\t\treturn;\n\t}\n\n\tset_special_state(TASK_TRACED);\n\n\t/*\n\t * We're committing to trapping.  TRACED should be visible before\n\t * TRAPPING is cleared; otherwise, the tracer might fail do_wait().\n\t * Also, transition to TRACED and updates to ->jobctl should be\n\t * atomic with respect to siglock and should be done after the arch\n\t * hook as siglock is released and regrabbed across it.\n\t *\n\t *     TRACER\t\t\t\t    TRACEE\n\t *\n\t *     ptrace_attach()\n\t * [L]   wait_on_bit(JOBCTL_TRAPPING)\t[S] set_special_state(TRACED)\n\t *     do_wait()\n\t *       set_current_state()                smp_wmb();\n\t *       ptrace_do_wait()\n\t *         wait_task_stopped()\n\t *           task_stopped_code()\n\t * [L]         task_is_traced()\t\t[S] task_clear_jobctl_trapping();\n\t */\n\tsmp_wmb();\n\n\tcurrent->last_siginfo = info;\n\tcurrent->exit_code = exit_code;\n\n\t/*\n\t * If @why is CLD_STOPPED, we're trapping to participate in a group\n\t * stop.  Do the bookkeeping.  Note that if SIGCONT was delievered\n\t * across siglock relocks since INTERRUPT was scheduled, PENDING\n\t * could be clear now.  We act as if SIGCONT is received after\n\t * TASK_TRACED is entered - ignore it.\n\t */\n\tif (why == CLD_STOPPED && (current->jobctl & JOBCTL_STOP_PENDING))\n\t\tgstop_done = task_participate_group_stop(current);\n\n\t/* any trap clears pending STOP trap, STOP trap clears NOTIFY */\n\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\tif (info && info->si_code >> 8 == PTRACE_EVENT_STOP)\n\t\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY);\n\n\t/* entering a trap, clear TRAPPING */\n\ttask_clear_jobctl_trapping(current);\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\tread_lock(&tasklist_lock);\n\tif (may_ptrace_stop()) {\n\t\t/*\n\t\t * Notify parents of the stop.\n\t\t *\n\t\t * While ptraced, there are two parents - the ptracer and\n\t\t * the real_parent of the group_leader.  The ptracer should\n\t\t * know about every stop while the real parent is only\n\t\t * interested in the completion of group stop.  The states\n\t\t * for the two don't interact with each other.  Notify\n\t\t * separately unless they're gonna be duplicates.\n\t\t */\n\t\tdo_notify_parent_cldstop(current, true, why);\n\t\tif (gstop_done && ptrace_reparented(current))\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/*\n\t\t * Don't want to allow preemption here, because\n\t\t * sys_ptrace() needs this task to be inactive.\n\t\t *\n\t\t * XXX: implement read_unlock_no_resched().\n\t\t */\n\t\tpreempt_disable();\n\t\tread_unlock(&tasklist_lock);\n\t\tcgroup_enter_frozen();\n\t\tpreempt_enable_no_resched();\n\t\tfreezable_schedule();\n\t\tcgroup_leave_frozen(true);\n\t} else {\n\t\t/*\n\t\t * By the time we got the lock, our tracer went away.\n\t\t * Don't drop the lock yet, another tracer may come.\n\t\t *\n\t\t * If @gstop_done, the ptracer went away between group stop\n\t\t * completion and here.  During detach, it would have set\n\t\t * JOBCTL_STOP_PENDING on us and we'll re-enter\n\t\t * TASK_STOPPED in do_signal_stop() on return, so notifying\n\t\t * the real parent of the group stop completion is enough.\n\t\t */\n\t\tif (gstop_done)\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/* tasklist protects us from ptrace_freeze_traced() */\n\t\t__set_current_state(TASK_RUNNING);\n\t\tif (clear_code)\n\t\t\tcurrent->exit_code = 0;\n\t\tread_unlock(&tasklist_lock);\n\t}\n\n\t/*\n\t * We are back.  Now reacquire the siglock before touching\n\t * last_siginfo, so that we are sure to have synchronized with\n\t * any signal-sending on another CPU that wants to examine it.\n\t */\n\tspin_lock_irq(&current->sighand->siglock);\n\tcurrent->last_siginfo = NULL;\n\n\t/* LISTENING can be set only during STOP traps, clear it */\n\tcurrent->jobctl &= ~JOBCTL_LISTENING;\n\n\t/*\n\t * Queued signals ignored us while we were stopped for tracing.\n\t * So check for any that we should take before resuming user mode.\n\t * This sets TIF_SIGPENDING, but never clears it.\n\t */\n\trecalc_sigpending_tsk(current);\n}\n\nstatic void ptrace_do_notify(int signr, int exit_code, int why)\n{\n\tkernel_siginfo_t info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = signr;\n\tinfo.si_code = exit_code;\n\tinfo.si_pid = task_pid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\t/* Let the debugger run.  */\n\tptrace_stop(exit_code, why, 1, &info);\n}\n\nvoid ptrace_notify(int exit_code)\n{\n\tBUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED);\n\tspin_unlock_irq(&current->sighand->siglock);\n}\n\n/**\n * do_signal_stop - handle group stop for SIGSTOP and other stop signals\n * @signr: signr causing group stop if initiating\n *\n * If %JOBCTL_STOP_PENDING is not set yet, initiate group stop with @signr\n * and participate in it.  If already set, participate in the existing\n * group stop.  If participated in a group stop (and thus slept), %true is\n * returned with siglock released.\n *\n * If ptraced, this function doesn't handle stop itself.  Instead,\n * %JOBCTL_TRAP_STOP is scheduled and %false is returned with siglock\n * untouched.  The caller must ensure that INTERRUPT trap handling takes\n * places afterwards.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which is released\n * on %true return.\n *\n * RETURNS:\n * %false if group stop is already cancelled or ptrace trap is scheduled.\n * %true if participated in group stop.\n */\nstatic bool do_signal_stop(int signr)\n\t__releases(&current->sighand->siglock)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tif (!(current->jobctl & JOBCTL_STOP_PENDING)) {\n\t\tunsigned long gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\n\t\tstruct task_struct *t;\n\n\t\t/* signr will be recorded in task->jobctl for retries */\n\t\tWARN_ON_ONCE(signr & ~JOBCTL_STOP_SIGMASK);\n\n\t\tif (!likely(current->jobctl & JOBCTL_STOP_DEQUEUED) ||\n\t\t    unlikely(signal_group_exit(sig)))\n\t\t\treturn false;\n\t\t/*\n\t\t * There is no group stop already in progress.  We must\n\t\t * initiate one now.\n\t\t *\n\t\t * While ptraced, a task may be resumed while group stop is\n\t\t * still in effect and then receive a stop signal and\n\t\t * initiate another group stop.  This deviates from the\n\t\t * usual behavior as two consecutive stop signals can't\n\t\t * cause two group stops when !ptraced.  That is why we\n\t\t * also check !task_is_stopped(t) below.\n\t\t *\n\t\t * The condition can be distinguished by testing whether\n\t\t * SIGNAL_STOP_STOPPED is already set.  Don't generate\n\t\t * group_exit_code in such case.\n\t\t *\n\t\t * This is not necessary for SIGNAL_STOP_CONTINUED because\n\t\t * an intervening stop signal is required to cause two\n\t\t * continued events regardless of ptrace.\n\t\t */\n\t\tif (!(sig->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsig->group_exit_code = signr;\n\n\t\tsig->group_stop_count = 0;\n\n\t\tif (task_set_jobctl_pending(current, signr | gstop))\n\t\t\tsig->group_stop_count++;\n\n\t\tt = current;\n\t\twhile_each_thread(current, t) {\n\t\t\t/*\n\t\t\t * Setting state to TASK_STOPPED for a group\n\t\t\t * stop is always done with the siglock held,\n\t\t\t * so this check has no races.\n\t\t\t */\n\t\t\tif (!task_is_stopped(t) &&\n\t\t\t    task_set_jobctl_pending(t, signr | gstop)) {\n\t\t\t\tsig->group_stop_count++;\n\t\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\t\tsignal_wake_up(t, 0);\n\t\t\t\telse\n\t\t\t\t\tptrace_trap_notify(t);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (likely(!current->ptrace)) {\n\t\tint notify = 0;\n\n\t\t/*\n\t\t * If there are no other threads in the group, or if there\n\t\t * is a group stop in progress and we are the last to stop,\n\t\t * report to the parent.\n\t\t */\n\t\tif (task_participate_group_stop(current))\n\t\t\tnotify = CLD_STOPPED;\n\n\t\tset_special_state(TASK_STOPPED);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent of the group stop completion.  Because\n\t\t * we're not holding either the siglock or tasklist_lock\n\t\t * here, ptracer may attach inbetween; however, this is for\n\t\t * group stop and should always be delivered to the real\n\t\t * parent of the group leader.  The new ptracer will get\n\t\t * its notification when this task transitions into\n\t\t * TASK_TRACED.\n\t\t */\n\t\tif (notify) {\n\t\t\tread_lock(&tasklist_lock);\n\t\t\tdo_notify_parent_cldstop(current, false, notify);\n\t\t\tread_unlock(&tasklist_lock);\n\t\t}\n\n\t\t/* Now we don't run again until woken by SIGCONT or SIGKILL */\n\t\tcgroup_enter_frozen();\n\t\tfreezable_schedule();\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * While ptraced, group stop is handled by STOP trap.\n\t\t * Schedule it and let the caller deal with it.\n\t\t */\n\t\ttask_set_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\t\treturn false;\n\t}\n}\n\n/**\n * do_jobctl_trap - take care of ptrace jobctl traps\n *\n * When PT_SEIZED, it's used for both group stop and explicit\n * SEIZE/INTERRUPT traps.  Both generate PTRACE_EVENT_STOP trap with\n * accompanying siginfo.  If stopped, lower eight bits of exit_code contain\n * the stop signal; otherwise, %SIGTRAP.\n *\n * When !PT_SEIZED, it's used only for group stop trap with stop signal\n * number as exit_code and no siginfo.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which may be\n * released and re-acquired before returning with intervening sleep.\n */\nstatic void do_jobctl_trap(void)\n{\n\tstruct signal_struct *signal = current->signal;\n\tint signr = current->jobctl & JOBCTL_STOP_SIGMASK;\n\n\tif (current->ptrace & PT_SEIZED) {\n\t\tif (!signal->group_stop_count &&\n\t\t    !(signal->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsignr = SIGTRAP;\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),\n\t\t\t\t CLD_STOPPED);\n\t} else {\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_stop(signr, CLD_STOPPED, 0, NULL);\n\t\tcurrent->exit_code = 0;\n\t}\n}\n\n/**\n * do_freezer_trap - handle the freezer jobctl trap\n *\n * Puts the task into frozen state, if only the task is not about to quit.\n * In this case it drops JOBCTL_TRAP_FREEZE.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held,\n * which is always released before returning.\n */\nstatic void do_freezer_trap(void)\n\t__releases(&current->sighand->siglock)\n{\n\t/*\n\t * If there are other trap bits pending except JOBCTL_TRAP_FREEZE,\n\t * let's make another loop to give it a chance to be handled.\n\t * In any case, we'll return back.\n\t */\n\tif ((current->jobctl & (JOBCTL_PENDING_MASK | JOBCTL_TRAP_FREEZE)) !=\n\t     JOBCTL_TRAP_FREEZE) {\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\treturn;\n\t}\n\n\t/*\n\t * Now we're sure that there is no pending fatal signal and no\n\t * pending traps. Clear TIF_SIGPENDING to not get out of schedule()\n\t * immediately (if there is a non-fatal signal pending), and\n\t * put the task into sleep.\n\t */\n\t__set_current_state(TASK_INTERRUPTIBLE);\n\tclear_thread_flag(TIF_SIGPENDING);\n\tspin_unlock_irq(&current->sighand->siglock);\n\tcgroup_enter_frozen();\n\tfreezable_schedule();\n}\n\nstatic int ptrace_signal(int signr, kernel_siginfo_t *info)\n{\n\t/*\n\t * We do not check sig_kernel_stop(signr) but set this marker\n\t * unconditionally because we do not know whether debugger will\n\t * change signr. This flag has no meaning unless we are going\n\t * to stop after return from ptrace_stop(). In this case it will\n\t * be checked in do_signal_stop(), we should only stop if it was\n\t * not cleared by SIGCONT while we were sleeping. See also the\n\t * comment in dequeue_signal().\n\t */\n\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\tptrace_stop(signr, CLD_TRAPPED, 0, info);\n\n\t/* We're back.  Did the debugger cancel the sig?  */\n\tsignr = current->exit_code;\n\tif (signr == 0)\n\t\treturn signr;\n\n\tcurrent->exit_code = 0;\n\n\t/*\n\t * Update the siginfo structure if the signal has\n\t * changed.  If the debugger wanted something\n\t * specific in the siginfo structure then it should\n\t * have updated *info via PTRACE_SETSIGINFO.\n\t */\n\tif (signr != info->si_signo) {\n\t\tclear_siginfo(info);\n\t\tinfo->si_signo = signr;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\trcu_read_lock();\n\t\tinfo->si_pid = task_pid_vnr(current->parent);\n\t\tinfo->si_uid = from_kuid_munged(current_user_ns(),\n\t\t\t\t\t\ttask_uid(current->parent));\n\t\trcu_read_unlock();\n\t}\n\n\t/* If the (new) signal is now blocked, requeue it.  */\n\tif (sigismember(&current->blocked, signr)) {\n\t\tsend_signal(signr, info, current, PIDTYPE_PID);\n\t\tsignr = 0;\n\t}\n\n\treturn signr;\n}\n\nbool get_signal(struct ksignal *ksig)\n{\n\tstruct sighand_struct *sighand = current->sighand;\n\tstruct signal_struct *signal = current->signal;\n\tint signr;\n\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tif (unlikely(uprobe_deny_signal()))\n\t\treturn false;\n\n\t/*\n\t * Do this once, we can't return to user-mode if freezing() == T.\n\t * do_signal_stop() and ptrace_stop() do freezable_schedule() and\n\t * thus do not need another check after return.\n\t */\n\ttry_to_freeze();\n\nrelock:\n\tspin_lock_irq(&sighand->siglock);\n\t/*\n\t * Every stopped thread goes here after wakeup. Check to see if\n\t * we should notify the parent, prepare_signal(SIGCONT) encodes\n\t * the CLD_ si_code into SIGNAL_CLD_MASK bits.\n\t */\n\tif (unlikely(signal->flags & SIGNAL_CLD_MASK)) {\n\t\tint why;\n\n\t\tif (signal->flags & SIGNAL_CLD_CONTINUED)\n\t\t\twhy = CLD_CONTINUED;\n\t\telse\n\t\t\twhy = CLD_STOPPED;\n\n\t\tsignal->flags &= ~SIGNAL_CLD_MASK;\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent that we're continuing.  This event is\n\t\t * always per-process and doesn't make whole lot of sense\n\t\t * for ptracers, who shouldn't consume the state via\n\t\t * wait(2) either, but, for backward compatibility, notify\n\t\t * the ptracer of the group leader too unless it's gonna be\n\t\t * a duplicate.\n\t\t */\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\tif (ptrace_reparented(current->group_leader))\n\t\t\tdo_notify_parent_cldstop(current->group_leader,\n\t\t\t\t\t\ttrue, why);\n\t\tread_unlock(&tasklist_lock);\n\n\t\tgoto relock;\n\t}\n\n\t/* Has this task already been marked for death? */\n\tif (signal_group_exit(signal)) {\n\t\tksig->info.si_signo = signr = SIGKILL;\n\t\tsigdelset(&current->pending.signal, SIGKILL);\n\t\ttrace_signal_deliver(SIGKILL, SEND_SIG_NOINFO,\n\t\t\t\t&sighand->action[SIGKILL - 1]);\n\t\trecalc_sigpending();\n\t\tgoto fatal;\n\t}\n\n\tfor (;;) {\n\t\tstruct k_sigaction *ka;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&\n\t\t    do_signal_stop(0))\n\t\t\tgoto relock;\n\n\t\tif (unlikely(current->jobctl &\n\t\t\t     (JOBCTL_TRAP_MASK | JOBCTL_TRAP_FREEZE))) {\n\t\t\tif (current->jobctl & JOBCTL_TRAP_MASK) {\n\t\t\t\tdo_jobctl_trap();\n\t\t\t\tspin_unlock_irq(&sighand->siglock);\n\t\t\t} else if (current->jobctl & JOBCTL_TRAP_FREEZE)\n\t\t\t\tdo_freezer_trap();\n\n\t\t\tgoto relock;\n\t\t}\n\n\t\t/*\n\t\t * If the task is leaving the frozen state, let's update\n\t\t * cgroup counters and reset the frozen bit.\n\t\t */\n\t\tif (unlikely(cgroup_task_frozen(current))) {\n\t\t\tspin_unlock_irq(&sighand->siglock);\n\t\t\tcgroup_leave_frozen(false);\n\t\t\tgoto relock;\n\t\t}\n\n\t\t/*\n\t\t * Signals generated by the execution of an instruction\n\t\t * need to be delivered before any other pending signals\n\t\t * so that the instruction pointer in the signal stack\n\t\t * frame points to the faulting instruction.\n\t\t */\n\t\tsignr = dequeue_synchronous_signal(&ksig->info);\n\t\tif (!signr)\n\t\t\tsignr = dequeue_signal(current, &current->blocked, &ksig->info);\n\n\t\tif (!signr)\n\t\t\tbreak; /* will return 0 */\n\n\t\tif (unlikely(current->ptrace) && signr != SIGKILL) {\n\t\t\tsignr = ptrace_signal(signr, &ksig->info);\n\t\t\tif (!signr)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tka = &sighand->action[signr-1];\n\n\t\t/* Trace actually delivered signals. */\n\t\ttrace_signal_deliver(signr, &ksig->info, ka);\n\n\t\tif (ka->sa.sa_handler == SIG_IGN) /* Do nothing.  */\n\t\t\tcontinue;\n\t\tif (ka->sa.sa_handler != SIG_DFL) {\n\t\t\t/* Run the handler.  */\n\t\t\tksig->ka = *ka;\n\n\t\t\tif (ka->sa.sa_flags & SA_ONESHOT)\n\t\t\t\tka->sa.sa_handler = SIG_DFL;\n\n\t\t\tbreak; /* will return non-zero \"signr\" value */\n\t\t}\n\n\t\t/*\n\t\t * Now we are doing the default action for this signal.\n\t\t */\n\t\tif (sig_kernel_ignore(signr)) /* Default is nothing. */\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Global init gets no signals it doesn't want.\n\t\t * Container-init gets no signals it doesn't want from same\n\t\t * container.\n\t\t *\n\t\t * Note that if global/container-init sees a sig_kernel_only()\n\t\t * signal here, the signal must have been generated internally\n\t\t * or must have come from an ancestor namespace. In either\n\t\t * case, the signal cannot be dropped.\n\t\t */\n\t\tif (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\t\t!sig_kernel_only(signr))\n\t\t\tcontinue;\n\n\t\tif (sig_kernel_stop(signr)) {\n\t\t\t/*\n\t\t\t * The default action is to stop all threads in\n\t\t\t * the thread group.  The job control signals\n\t\t\t * do nothing in an orphaned pgrp, but SIGSTOP\n\t\t\t * always works.  Note that siglock needs to be\n\t\t\t * dropped during the call to is_orphaned_pgrp()\n\t\t\t * because of lock ordering with tasklist_lock.\n\t\t\t * This allows an intervening SIGCONT to be posted.\n\t\t\t * We need to check for that and bail out if necessary.\n\t\t\t */\n\t\t\tif (signr != SIGSTOP) {\n\t\t\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t\t\t/* signals can be posted during this window */\n\n\t\t\t\tif (is_current_pgrp_orphaned())\n\t\t\t\t\tgoto relock;\n\n\t\t\t\tspin_lock_irq(&sighand->siglock);\n\t\t\t}\n\n\t\t\tif (likely(do_signal_stop(ksig->info.si_signo))) {\n\t\t\t\t/* It released the siglock.  */\n\t\t\t\tgoto relock;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We didn't actually stop, due to a race\n\t\t\t * with SIGCONT or something like that.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\tfatal:\n\t\tspin_unlock_irq(&sighand->siglock);\n\t\tif (unlikely(cgroup_task_frozen(current)))\n\t\t\tcgroup_leave_frozen(true);\n\n\t\t/*\n\t\t * Anything else is fatal, maybe with a core dump.\n\t\t */\n\t\tcurrent->flags |= PF_SIGNALED;\n\n\t\tif (sig_kernel_coredump(signr)) {\n\t\t\tif (print_fatal_signals)\n\t\t\t\tprint_fatal_signal(ksig->info.si_signo);\n\t\t\tproc_coredump_connector(current);\n\t\t\t/*\n\t\t\t * If it was able to dump core, this kills all\n\t\t\t * other threads in the group and synchronizes with\n\t\t\t * their demise.  If we lost the race with another\n\t\t\t * thread getting here, it set group_exit_code\n\t\t\t * first and our do_group_exit call below will use\n\t\t\t * that value and ignore the one we pass it.\n\t\t\t */\n\t\t\tdo_coredump(&ksig->info);\n\t\t}\n\n\t\t/*\n\t\t * Death signals, no core dump.\n\t\t */\n\t\tdo_group_exit(ksig->info.si_signo);\n\t\t/* NOTREACHED */\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n\n\tksig->sig = signr;\n\treturn ksig->sig > 0;\n}\n\n/**\n * signal_delivered - \n * @ksig:\t\tkernel signal struct\n * @stepping:\t\tnonzero if debugger single-step or block-step in use\n *\n * This function should be called when a signal has successfully been\n * delivered. It updates the blocked signals accordingly (@ksig->ka.sa.sa_mask\n * is always blocked, and the signal itself is blocked unless %SA_NODEFER\n * is set in @ksig->ka.sa.sa_flags.  Tracing is notified.\n */\nstatic void signal_delivered(struct ksignal *ksig, int stepping)\n{\n\tsigset_t blocked;\n\n\t/* A signal was successfully delivered, and the\n\t   saved sigmask was stored on the signal frame,\n\t   and will be restored by sigreturn.  So we can\n\t   simply clear the restore sigmask flag.  */\n\tclear_restore_sigmask();\n\n\tsigorsets(&blocked, &current->blocked, &ksig->ka.sa.sa_mask);\n\tif (!(ksig->ka.sa.sa_flags & SA_NODEFER))\n\t\tsigaddset(&blocked, ksig->sig);\n\tset_current_blocked(&blocked);\n\ttracehook_signal_handler(stepping);\n}\n\nvoid signal_setup_done(int failed, struct ksignal *ksig, int stepping)\n{\n\tif (failed)\n\t\tforce_sigsegv(ksig->sig);\n\telse\n\t\tsignal_delivered(ksig, stepping);\n}\n\n/*\n * It could be that complete_signal() picked us to notify about the\n * group-wide signal. Other threads should be notified now to take\n * the shared signals in @which since we will not.\n */\nstatic void retarget_shared_pending(struct task_struct *tsk, sigset_t *which)\n{\n\tsigset_t retarget;\n\tstruct task_struct *t;\n\n\tsigandsets(&retarget, &tsk->signal->shared_pending.signal, which);\n\tif (sigisemptyset(&retarget))\n\t\treturn;\n\n\tt = tsk;\n\twhile_each_thread(tsk, t) {\n\t\tif (t->flags & PF_EXITING)\n\t\t\tcontinue;\n\n\t\tif (!has_pending_signals(&retarget, &t->blocked))\n\t\t\tcontinue;\n\t\t/* Remove the signals this thread can handle. */\n\t\tsigandsets(&retarget, &retarget, &t->blocked);\n\n\t\tif (!signal_pending(t))\n\t\t\tsignal_wake_up(t, 0);\n\n\t\tif (sigisemptyset(&retarget))\n\t\t\tbreak;\n\t}\n}\n\nvoid exit_signals(struct task_struct *tsk)\n{\n\tint group_stop = 0;\n\tsigset_t unblocked;\n\n\t/*\n\t * @tsk is about to have PF_EXITING set - lock out users which\n\t * expect stable threadgroup.\n\t */\n\tcgroup_threadgroup_change_begin(tsk);\n\n\tif (thread_group_empty(tsk) || signal_group_exit(tsk->signal)) {\n\t\ttsk->flags |= PF_EXITING;\n\t\tcgroup_threadgroup_change_end(tsk);\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t/*\n\t * From now this task is not visible for group-wide signals,\n\t * see wants_signal(), do_signal_stop().\n\t */\n\ttsk->flags |= PF_EXITING;\n\n\tcgroup_threadgroup_change_end(tsk);\n\n\tif (!signal_pending(tsk))\n\t\tgoto out;\n\n\tunblocked = tsk->blocked;\n\tsignotset(&unblocked);\n\tretarget_shared_pending(tsk, &unblocked);\n\n\tif (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&\n\t    task_participate_group_stop(tsk))\n\t\tgroup_stop = CLD_STOPPED;\nout:\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t/*\n\t * If group stop has completed, deliver the notification.  This\n\t * should always go to the real parent of the group leader.\n\t */\n\tif (unlikely(group_stop)) {\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(tsk, false, group_stop);\n\t\tread_unlock(&tasklist_lock);\n\t}\n}\n\n/*\n * System call entry points.\n */\n\n/**\n *  sys_restart_syscall - restart a system call\n */\nSYSCALL_DEFINE0(restart_syscall)\n{\n\tstruct restart_block *restart = &current->restart_block;\n\treturn restart->fn(restart);\n}\n\nlong do_no_restart_syscall(struct restart_block *param)\n{\n\treturn -EINTR;\n}\n\nstatic void __set_task_blocked(struct task_struct *tsk, const sigset_t *newset)\n{\n\tif (signal_pending(tsk) && !thread_group_empty(tsk)) {\n\t\tsigset_t newblocked;\n\t\t/* A set of now blocked but previously unblocked signals. */\n\t\tsigandnsets(&newblocked, newset, &current->blocked);\n\t\tretarget_shared_pending(tsk, &newblocked);\n\t}\n\ttsk->blocked = *newset;\n\trecalc_sigpending();\n}\n\n/**\n * set_current_blocked - change current->blocked mask\n * @newset: new mask\n *\n * It is wrong to change ->blocked directly, this helper should be used\n * to ensure the process can't miss a shared signal we are going to block.\n */\nvoid set_current_blocked(sigset_t *newset)\n{\n\tsigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t__set_current_blocked(newset);\n}\n\nvoid __set_current_blocked(const sigset_t *newset)\n{\n\tstruct task_struct *tsk = current;\n\n\t/*\n\t * In case the signal mask hasn't changed, there is nothing we need\n\t * to do. The current->blocked shouldn't be modified by other task.\n\t */\n\tif (sigequalsets(&tsk->blocked, newset))\n\t\treturn;\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t__set_task_blocked(tsk, newset);\n\tspin_unlock_irq(&tsk->sighand->siglock);\n}\n\n/*\n * This is also useful for kernel threads that want to temporarily\n * (or permanently) block certain signals.\n *\n * NOTE! Unlike the user-mode sys_sigprocmask(), the kernel\n * interface happily blocks \"unblockable\" signals like SIGKILL\n * and friends.\n */\nint sigprocmask(int how, sigset_t *set, sigset_t *oldset)\n{\n\tstruct task_struct *tsk = current;\n\tsigset_t newset;\n\n\t/* Lockless, only current can change ->blocked, never from irq */\n\tif (oldset)\n\t\t*oldset = tsk->blocked;\n\n\tswitch (how) {\n\tcase SIG_BLOCK:\n\t\tsigorsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_UNBLOCK:\n\t\tsigandnsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_SETMASK:\n\t\tnewset = *set;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t__set_current_blocked(&newset);\n\treturn 0;\n}\nEXPORT_SYMBOL(sigprocmask);\n\n/*\n * The api helps set app-provided sigmasks.\n *\n * This is useful for syscalls such as ppoll, pselect, io_pgetevents and\n * epoll_pwait where a new sigmask is passed from userland for the syscalls.\n *\n * Note that it does set_restore_sigmask() in advance, so it must be always\n * paired with restore_saved_sigmask_unless() before return from syscall.\n */\nint set_user_sigmask(const sigset_t __user *umask, size_t sigsetsize)\n{\n\tsigset_t kmask;\n\n\tif (!umask)\n\t\treturn 0;\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&kmask, umask, sizeof(sigset_t)))\n\t\treturn -EFAULT;\n\n\tset_restore_sigmask();\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(&kmask);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nint set_compat_user_sigmask(const compat_sigset_t __user *umask,\n\t\t\t    size_t sigsetsize)\n{\n\tsigset_t kmask;\n\n\tif (!umask)\n\t\treturn 0;\n\tif (sigsetsize != sizeof(compat_sigset_t))\n\t\treturn -EINVAL;\n\tif (get_compat_sigset(&kmask, umask))\n\t\treturn -EFAULT;\n\n\tset_restore_sigmask();\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(&kmask);\n\n\treturn 0;\n}\n#endif\n\n/**\n *  sys_rt_sigprocmask - change the list of currently blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: stores pending signals\n *  @oset: previous value of signal mask if non-null\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigprocmask, int, how, sigset_t __user *, nset,\n\t\tsigset_t __user *, oset, size_t, sigsetsize)\n{\n\tsigset_t old_set, new_set;\n\tint error;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\told_set = current->blocked;\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigprocmask, int, how, compat_sigset_t __user *, nset,\n\t\tcompat_sigset_t __user *, oset, compat_size_t, sigsetsize)\n{\n\tsigset_t old_set = current->blocked;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (nset) {\n\t\tsigset_t new_set;\n\t\tint error;\n\t\tif (get_compat_sigset(&new_set, nset))\n\t\t\treturn -EFAULT;\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\treturn oset ? put_compat_sigset(oset, &old_set, sizeof(*oset)) : 0;\n}\n#endif\n\nstatic void do_sigpending(sigset_t *set)\n{\n\tspin_lock_irq(&current->sighand->siglock);\n\tsigorsets(set, &current->pending.signal,\n\t\t  &current->signal->shared_pending.signal);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\t/* Outside the lock because only this thread touches it.  */\n\tsigandsets(set, &current->blocked, set);\n}\n\n/**\n *  sys_rt_sigpending - examine a pending signal that has been raised\n *\t\t\twhile blocked\n *  @uset: stores pending signals\n *  @sigsetsize: size of sigset_t type or larger\n */\nSYSCALL_DEFINE2(rt_sigpending, sigset_t __user *, uset, size_t, sigsetsize)\n{\n\tsigset_t set;\n\n\tif (sigsetsize > sizeof(*uset))\n\t\treturn -EINVAL;\n\n\tdo_sigpending(&set);\n\n\tif (copy_to_user(uset, &set, sigsetsize))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigpending, compat_sigset_t __user *, uset,\n\t\tcompat_size_t, sigsetsize)\n{\n\tsigset_t set;\n\n\tif (sigsetsize > sizeof(*uset))\n\t\treturn -EINVAL;\n\n\tdo_sigpending(&set);\n\n\treturn put_compat_sigset(uset, &set, sigsetsize);\n}\n#endif\n\nstatic const struct {\n\tunsigned char limit, layout;\n} sig_sicodes[] = {\n\t[SIGILL]  = { NSIGILL,  SIL_FAULT },\n\t[SIGFPE]  = { NSIGFPE,  SIL_FAULT },\n\t[SIGSEGV] = { NSIGSEGV, SIL_FAULT },\n\t[SIGBUS]  = { NSIGBUS,  SIL_FAULT },\n\t[SIGTRAP] = { NSIGTRAP, SIL_FAULT },\n#if defined(SIGEMT)\n\t[SIGEMT]  = { NSIGEMT,  SIL_FAULT },\n#endif\n\t[SIGCHLD] = { NSIGCHLD, SIL_CHLD },\n\t[SIGPOLL] = { NSIGPOLL, SIL_POLL },\n\t[SIGSYS]  = { NSIGSYS,  SIL_SYS },\n};\n\nstatic bool known_siginfo_layout(unsigned sig, int si_code)\n{\n\tif (si_code == SI_KERNEL)\n\t\treturn true;\n\telse if ((si_code > SI_USER)) {\n\t\tif (sig_specific_sicodes(sig)) {\n\t\t\tif (si_code <= sig_sicodes[sig].limit)\n\t\t\t\treturn true;\n\t\t}\n\t\telse if (si_code <= NSIGPOLL)\n\t\t\treturn true;\n\t}\n\telse if (si_code >= SI_DETHREAD)\n\t\treturn true;\n\telse if (si_code == SI_ASYNCNL)\n\t\treturn true;\n\treturn false;\n}\n\nenum siginfo_layout siginfo_layout(unsigned sig, int si_code)\n{\n\tenum siginfo_layout layout = SIL_KILL;\n\tif ((si_code > SI_USER) && (si_code < SI_KERNEL)) {\n\t\tif ((sig < ARRAY_SIZE(sig_sicodes)) &&\n\t\t    (si_code <= sig_sicodes[sig].limit)) {\n\t\t\tlayout = sig_sicodes[sig].layout;\n\t\t\t/* Handle the exceptions */\n\t\t\tif ((sig == SIGBUS) &&\n\t\t\t    (si_code >= BUS_MCEERR_AR) && (si_code <= BUS_MCEERR_AO))\n\t\t\t\tlayout = SIL_FAULT_MCEERR;\n\t\t\telse if ((sig == SIGSEGV) && (si_code == SEGV_BNDERR))\n\t\t\t\tlayout = SIL_FAULT_BNDERR;\n#ifdef SEGV_PKUERR\n\t\t\telse if ((sig == SIGSEGV) && (si_code == SEGV_PKUERR))\n\t\t\t\tlayout = SIL_FAULT_PKUERR;\n#endif\n\t\t}\n\t\telse if (si_code <= NSIGPOLL)\n\t\t\tlayout = SIL_POLL;\n\t} else {\n\t\tif (si_code == SI_TIMER)\n\t\t\tlayout = SIL_TIMER;\n\t\telse if (si_code == SI_SIGIO)\n\t\t\tlayout = SIL_POLL;\n\t\telse if (si_code < 0)\n\t\t\tlayout = SIL_RT;\n\t}\n\treturn layout;\n}\n\nstatic inline char __user *si_expansion(const siginfo_t __user *info)\n{\n\treturn ((char __user *)info) + sizeof(struct kernel_siginfo);\n}\n\nint copy_siginfo_to_user(siginfo_t __user *to, const kernel_siginfo_t *from)\n{\n\tchar __user *expansion = si_expansion(to);\n\tif (copy_to_user(to, from , sizeof(struct kernel_siginfo)))\n\t\treturn -EFAULT;\n\tif (clear_user(expansion, SI_EXPANSION_SIZE))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int post_copy_siginfo_from_user(kernel_siginfo_t *info,\n\t\t\t\t       const siginfo_t __user *from)\n{\n\tif (unlikely(!known_siginfo_layout(info->si_signo, info->si_code))) {\n\t\tchar __user *expansion = si_expansion(from);\n\t\tchar buf[SI_EXPANSION_SIZE];\n\t\tint i;\n\t\t/*\n\t\t * An unknown si_code might need more than\n\t\t * sizeof(struct kernel_siginfo) bytes.  Verify all of the\n\t\t * extra bytes are 0.  This guarantees copy_siginfo_to_user\n\t\t * will return this data to userspace exactly.\n\t\t */\n\t\tif (copy_from_user(&buf, expansion, SI_EXPANSION_SIZE))\n\t\t\treturn -EFAULT;\n\t\tfor (i = 0; i < SI_EXPANSION_SIZE; i++) {\n\t\t\tif (buf[i] != 0)\n\t\t\t\treturn -E2BIG;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int __copy_siginfo_from_user(int signo, kernel_siginfo_t *to,\n\t\t\t\t    const siginfo_t __user *from)\n{\n\tif (copy_from_user(to, from, sizeof(struct kernel_siginfo)))\n\t\treturn -EFAULT;\n\tto->si_signo = signo;\n\treturn post_copy_siginfo_from_user(to, from);\n}\n\nint copy_siginfo_from_user(kernel_siginfo_t *to, const siginfo_t __user *from)\n{\n\tif (copy_from_user(to, from, sizeof(struct kernel_siginfo)))\n\t\treturn -EFAULT;\n\treturn post_copy_siginfo_from_user(to, from);\n}\n\n#ifdef CONFIG_COMPAT\nint copy_siginfo_to_user32(struct compat_siginfo __user *to,\n\t\t\t   const struct kernel_siginfo *from)\n#if defined(CONFIG_X86_X32_ABI) || defined(CONFIG_IA32_EMULATION)\n{\n\treturn __copy_siginfo_to_user32(to, from, in_x32_syscall());\n}\nint __copy_siginfo_to_user32(struct compat_siginfo __user *to,\n\t\t\t     const struct kernel_siginfo *from, bool x32_ABI)\n#endif\n{\n\tstruct compat_siginfo new;\n\tmemset(&new, 0, sizeof(new));\n\n\tnew.si_signo = from->si_signo;\n\tnew.si_errno = from->si_errno;\n\tnew.si_code  = from->si_code;\n\tswitch(siginfo_layout(from->si_signo, from->si_code)) {\n\tcase SIL_KILL:\n\t\tnew.si_pid = from->si_pid;\n\t\tnew.si_uid = from->si_uid;\n\t\tbreak;\n\tcase SIL_TIMER:\n\t\tnew.si_tid     = from->si_tid;\n\t\tnew.si_overrun = from->si_overrun;\n\t\tnew.si_int     = from->si_int;\n\t\tbreak;\n\tcase SIL_POLL:\n\t\tnew.si_band = from->si_band;\n\t\tnew.si_fd   = from->si_fd;\n\t\tbreak;\n\tcase SIL_FAULT:\n\t\tnew.si_addr = ptr_to_compat(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tnew.si_trapno = from->si_trapno;\n#endif\n\t\tbreak;\n\tcase SIL_FAULT_MCEERR:\n\t\tnew.si_addr = ptr_to_compat(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tnew.si_trapno = from->si_trapno;\n#endif\n\t\tnew.si_addr_lsb = from->si_addr_lsb;\n\t\tbreak;\n\tcase SIL_FAULT_BNDERR:\n\t\tnew.si_addr = ptr_to_compat(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tnew.si_trapno = from->si_trapno;\n#endif\n\t\tnew.si_lower = ptr_to_compat(from->si_lower);\n\t\tnew.si_upper = ptr_to_compat(from->si_upper);\n\t\tbreak;\n\tcase SIL_FAULT_PKUERR:\n\t\tnew.si_addr = ptr_to_compat(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tnew.si_trapno = from->si_trapno;\n#endif\n\t\tnew.si_pkey = from->si_pkey;\n\t\tbreak;\n\tcase SIL_CHLD:\n\t\tnew.si_pid    = from->si_pid;\n\t\tnew.si_uid    = from->si_uid;\n\t\tnew.si_status = from->si_status;\n#ifdef CONFIG_X86_X32_ABI\n\t\tif (x32_ABI) {\n\t\t\tnew._sifields._sigchld_x32._utime = from->si_utime;\n\t\t\tnew._sifields._sigchld_x32._stime = from->si_stime;\n\t\t} else\n#endif\n\t\t{\n\t\t\tnew.si_utime = from->si_utime;\n\t\t\tnew.si_stime = from->si_stime;\n\t\t}\n\t\tbreak;\n\tcase SIL_RT:\n\t\tnew.si_pid = from->si_pid;\n\t\tnew.si_uid = from->si_uid;\n\t\tnew.si_int = from->si_int;\n\t\tbreak;\n\tcase SIL_SYS:\n\t\tnew.si_call_addr = ptr_to_compat(from->si_call_addr);\n\t\tnew.si_syscall   = from->si_syscall;\n\t\tnew.si_arch      = from->si_arch;\n\t\tbreak;\n\t}\n\n\tif (copy_to_user(to, &new, sizeof(struct compat_siginfo)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int post_copy_siginfo_from_user32(kernel_siginfo_t *to,\n\t\t\t\t\t const struct compat_siginfo *from)\n{\n\tclear_siginfo(to);\n\tto->si_signo = from->si_signo;\n\tto->si_errno = from->si_errno;\n\tto->si_code  = from->si_code;\n\tswitch(siginfo_layout(from->si_signo, from->si_code)) {\n\tcase SIL_KILL:\n\t\tto->si_pid = from->si_pid;\n\t\tto->si_uid = from->si_uid;\n\t\tbreak;\n\tcase SIL_TIMER:\n\t\tto->si_tid     = from->si_tid;\n\t\tto->si_overrun = from->si_overrun;\n\t\tto->si_int     = from->si_int;\n\t\tbreak;\n\tcase SIL_POLL:\n\t\tto->si_band = from->si_band;\n\t\tto->si_fd   = from->si_fd;\n\t\tbreak;\n\tcase SIL_FAULT:\n\t\tto->si_addr = compat_ptr(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tto->si_trapno = from->si_trapno;\n#endif\n\t\tbreak;\n\tcase SIL_FAULT_MCEERR:\n\t\tto->si_addr = compat_ptr(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tto->si_trapno = from->si_trapno;\n#endif\n\t\tto->si_addr_lsb = from->si_addr_lsb;\n\t\tbreak;\n\tcase SIL_FAULT_BNDERR:\n\t\tto->si_addr = compat_ptr(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tto->si_trapno = from->si_trapno;\n#endif\n\t\tto->si_lower = compat_ptr(from->si_lower);\n\t\tto->si_upper = compat_ptr(from->si_upper);\n\t\tbreak;\n\tcase SIL_FAULT_PKUERR:\n\t\tto->si_addr = compat_ptr(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tto->si_trapno = from->si_trapno;\n#endif\n\t\tto->si_pkey = from->si_pkey;\n\t\tbreak;\n\tcase SIL_CHLD:\n\t\tto->si_pid    = from->si_pid;\n\t\tto->si_uid    = from->si_uid;\n\t\tto->si_status = from->si_status;\n#ifdef CONFIG_X86_X32_ABI\n\t\tif (in_x32_syscall()) {\n\t\t\tto->si_utime = from->_sifields._sigchld_x32._utime;\n\t\t\tto->si_stime = from->_sifields._sigchld_x32._stime;\n\t\t} else\n#endif\n\t\t{\n\t\t\tto->si_utime = from->si_utime;\n\t\t\tto->si_stime = from->si_stime;\n\t\t}\n\t\tbreak;\n\tcase SIL_RT:\n\t\tto->si_pid = from->si_pid;\n\t\tto->si_uid = from->si_uid;\n\t\tto->si_int = from->si_int;\n\t\tbreak;\n\tcase SIL_SYS:\n\t\tto->si_call_addr = compat_ptr(from->si_call_addr);\n\t\tto->si_syscall   = from->si_syscall;\n\t\tto->si_arch      = from->si_arch;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int __copy_siginfo_from_user32(int signo, struct kernel_siginfo *to,\n\t\t\t\t      const struct compat_siginfo __user *ufrom)\n{\n\tstruct compat_siginfo from;\n\n\tif (copy_from_user(&from, ufrom, sizeof(struct compat_siginfo)))\n\t\treturn -EFAULT;\n\n\tfrom.si_signo = signo;\n\treturn post_copy_siginfo_from_user32(to, &from);\n}\n\nint copy_siginfo_from_user32(struct kernel_siginfo *to,\n\t\t\t     const struct compat_siginfo __user *ufrom)\n{\n\tstruct compat_siginfo from;\n\n\tif (copy_from_user(&from, ufrom, sizeof(struct compat_siginfo)))\n\t\treturn -EFAULT;\n\n\treturn post_copy_siginfo_from_user32(to, &from);\n}\n#endif /* CONFIG_COMPAT */\n\n/**\n *  do_sigtimedwait - wait for queued signals specified in @which\n *  @which: queued signals to wait for\n *  @info: if non-null, the signal's siginfo is returned here\n *  @ts: upper bound on process time suspension\n */\nstatic int do_sigtimedwait(const sigset_t *which, kernel_siginfo_t *info,\n\t\t    const struct timespec64 *ts)\n{\n\tktime_t *to = NULL, timeout = KTIME_MAX;\n\tstruct task_struct *tsk = current;\n\tsigset_t mask = *which;\n\tint sig, ret = 0;\n\n\tif (ts) {\n\t\tif (!timespec64_valid(ts))\n\t\t\treturn -EINVAL;\n\t\ttimeout = timespec64_to_ktime(*ts);\n\t\tto = &timeout;\n\t}\n\n\t/*\n\t * Invert the set of allowed signals to get those we want to block.\n\t */\n\tsigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\tsignotset(&mask);\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\tsig = dequeue_signal(tsk, &mask, info);\n\tif (!sig && timeout) {\n\t\t/*\n\t\t * None ready, temporarily unblock those we're interested\n\t\t * while we are sleeping in so that we'll be awakened when\n\t\t * they arrive. Unblocking is always fine, we can avoid\n\t\t * set_current_blocked().\n\t\t */\n\t\ttsk->real_blocked = tsk->blocked;\n\t\tsigandsets(&tsk->blocked, &tsk->blocked, &mask);\n\t\trecalc_sigpending();\n\t\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tret = freezable_schedule_hrtimeout_range(to, tsk->timer_slack_ns,\n\t\t\t\t\t\t\t HRTIMER_MODE_REL);\n\t\tspin_lock_irq(&tsk->sighand->siglock);\n\t\t__set_task_blocked(tsk, &tsk->real_blocked);\n\t\tsigemptyset(&tsk->real_blocked);\n\t\tsig = dequeue_signal(tsk, &mask, info);\n\t}\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\tif (sig)\n\t\treturn sig;\n\treturn ret ? -EINTR : -EAGAIN;\n}\n\n/**\n *  sys_rt_sigtimedwait - synchronously wait for queued signals specified\n *\t\t\tin @uthese\n *  @uthese: queued signals to wait for\n *  @uinfo: if non-null, the signal's siginfo is returned here\n *  @uts: upper bound on process time suspension\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigtimedwait, const sigset_t __user *, uthese,\n\t\tsiginfo_t __user *, uinfo,\n\t\tconst struct __kernel_timespec __user *, uts,\n\t\tsize_t, sigsetsize)\n{\n\tsigset_t these;\n\tstruct timespec64 ts;\n\tkernel_siginfo_t info;\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&these, uthese, sizeof(these)))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (get_timespec64(&ts, uts))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nSYSCALL_DEFINE4(rt_sigtimedwait_time32, const sigset_t __user *, uthese,\n\t\tsiginfo_t __user *, uinfo,\n\t\tconst struct old_timespec32 __user *, uts,\n\t\tsize_t, sigsetsize)\n{\n\tsigset_t these;\n\tstruct timespec64 ts;\n\tkernel_siginfo_t info;\n\tint ret;\n\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&these, uthese, sizeof(these)))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (get_old_timespec32(&ts, uts))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n#endif\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigtimedwait_time64, compat_sigset_t __user *, uthese,\n\t\tstruct compat_siginfo __user *, uinfo,\n\t\tstruct __kernel_timespec __user *, uts, compat_size_t, sigsetsize)\n{\n\tsigset_t s;\n\tstruct timespec64 t;\n\tkernel_siginfo_t info;\n\tlong ret;\n\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (get_compat_sigset(&s, uthese))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (get_timespec64(&t, uts))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&s, &info, uts ? &t : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user32(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nCOMPAT_SYSCALL_DEFINE4(rt_sigtimedwait_time32, compat_sigset_t __user *, uthese,\n\t\tstruct compat_siginfo __user *, uinfo,\n\t\tstruct old_timespec32 __user *, uts, compat_size_t, sigsetsize)\n{\n\tsigset_t s;\n\tstruct timespec64 t;\n\tkernel_siginfo_t info;\n\tlong ret;\n\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (get_compat_sigset(&s, uthese))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (get_old_timespec32(&t, uts))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&s, &info, uts ? &t : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user32(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n#endif\n#endif\n\nstatic inline void prepare_kill_siginfo(int sig, struct kernel_siginfo *info)\n{\n\tclear_siginfo(info);\n\tinfo->si_signo = sig;\n\tinfo->si_errno = 0;\n\tinfo->si_code = SI_USER;\n\tinfo->si_pid = task_tgid_vnr(current);\n\tinfo->si_uid = from_kuid_munged(current_user_ns(), current_uid());\n}\n\n/**\n *  sys_kill - send a signal to a process\n *  @pid: the PID of the process\n *  @sig: signal to be sent\n */\nSYSCALL_DEFINE2(kill, pid_t, pid, int, sig)\n{\n\tstruct kernel_siginfo info;\n\n\tprepare_kill_siginfo(sig, &info);\n\n\treturn kill_something_info(sig, &info, pid);\n}\n\n/*\n * Verify that the signaler and signalee either are in the same pid namespace\n * or that the signaler's pid namespace is an ancestor of the signalee's pid\n * namespace.\n */\nstatic bool access_pidfd_pidns(struct pid *pid)\n{\n\tstruct pid_namespace *active = task_active_pid_ns(current);\n\tstruct pid_namespace *p = ns_of_pid(pid);\n\n\tfor (;;) {\n\t\tif (!p)\n\t\t\treturn false;\n\t\tif (p == active)\n\t\t\tbreak;\n\t\tp = p->parent;\n\t}\n\n\treturn true;\n}\n\nstatic int copy_siginfo_from_user_any(kernel_siginfo_t *kinfo, siginfo_t *info)\n{\n#ifdef CONFIG_COMPAT\n\t/*\n\t * Avoid hooking up compat syscalls and instead handle necessary\n\t * conversions here. Note, this is a stop-gap measure and should not be\n\t * considered a generic solution.\n\t */\n\tif (in_compat_syscall())\n\t\treturn copy_siginfo_from_user32(\n\t\t\tkinfo, (struct compat_siginfo __user *)info);\n#endif\n\treturn copy_siginfo_from_user(kinfo, info);\n}\n\nstatic struct pid *pidfd_to_pid(const struct file *file)\n{\n\tstruct pid *pid;\n\n\tpid = pidfd_pid(file);\n\tif (!IS_ERR(pid))\n\t\treturn pid;\n\n\treturn tgid_pidfd_to_pid(file);\n}\n\n/**\n * sys_pidfd_send_signal - Signal a process through a pidfd\n * @pidfd:  file descriptor of the process\n * @sig:    signal to send\n * @info:   signal info\n * @flags:  future flags\n *\n * The syscall currently only signals via PIDTYPE_PID which covers\n * kill(<positive-pid>, <signal>. It does not signal threads or process\n * groups.\n * In order to extend the syscall to threads and process groups the @flags\n * argument should be used. In essence, the @flags argument will determine\n * what is signaled and not the file descriptor itself. Put in other words,\n * grouping is a property of the flags argument not a property of the file\n * descriptor.\n *\n * Return: 0 on success, negative errno on failure\n */\nSYSCALL_DEFINE4(pidfd_send_signal, int, pidfd, int, sig,\n\t\tsiginfo_t __user *, info, unsigned int, flags)\n{\n\tint ret;\n\tstruct fd f;\n\tstruct pid *pid;\n\tkernel_siginfo_t kinfo;\n\n\t/* Enforce flags be set to 0 until we add an extension. */\n\tif (flags)\n\t\treturn -EINVAL;\n\n\tf = fdget(pidfd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\t/* Is this a pidfd? */\n\tpid = pidfd_to_pid(f.file);\n\tif (IS_ERR(pid)) {\n\t\tret = PTR_ERR(pid);\n\t\tgoto err;\n\t}\n\n\tret = -EINVAL;\n\tif (!access_pidfd_pidns(pid))\n\t\tgoto err;\n\n\tif (info) {\n\t\tret = copy_siginfo_from_user_any(&kinfo, info);\n\t\tif (unlikely(ret))\n\t\t\tgoto err;\n\n\t\tret = -EINVAL;\n\t\tif (unlikely(sig != kinfo.si_signo))\n\t\t\tgoto err;\n\n\t\t/* Only allow sending arbitrary signals to yourself. */\n\t\tret = -EPERM;\n\t\tif ((task_pid(current) != pid) &&\n\t\t    (kinfo.si_code >= 0 || kinfo.si_code == SI_TKILL))\n\t\t\tgoto err;\n\t} else {\n\t\tprepare_kill_siginfo(sig, &kinfo);\n\t}\n\n\tret = kill_pid_info(sig, &kinfo, pid);\n\nerr:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int\ndo_send_specific(pid_t tgid, pid_t pid, int sig, struct kernel_siginfo *info)\n{\n\tstruct task_struct *p;\n\tint error = -ESRCH;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p && (tgid <= 0 || task_tgid_vnr(p) == tgid)) {\n\t\terror = check_kill_permission(sig, info, p);\n\t\t/*\n\t\t * The null signal is a permissions and process existence\n\t\t * probe.  No signal is actually delivered.\n\t\t */\n\t\tif (!error && sig) {\n\t\t\terror = do_send_sig_info(sig, info, p, PIDTYPE_PID);\n\t\t\t/*\n\t\t\t * If lock_task_sighand() failed we pretend the task\n\t\t\t * dies after receiving the signal. The window is tiny,\n\t\t\t * and the signal is private anyway.\n\t\t\t */\n\t\t\tif (unlikely(error == -ESRCH))\n\t\t\t\terror = 0;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nstatic int do_tkill(pid_t tgid, pid_t pid, int sig)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_TKILL;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn do_send_specific(tgid, pid, sig, &info);\n}\n\n/**\n *  sys_tgkill - send signal to one specific thread\n *  @tgid: the thread group ID of the thread\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *\n *  This syscall also checks the @tgid and returns -ESRCH even if the PID\n *  exists but it's not belonging to the target process anymore. This\n *  method solves the problem of threads exiting and PIDs getting reused.\n */\nSYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(tgid, pid, sig);\n}\n\n/**\n *  sys_tkill - send signal to one specific task\n *  @pid: the PID of the task\n *  @sig: signal to be sent\n *\n *  Send a signal to only one task, even if it's a CLONE_THREAD task.\n */\nSYSCALL_DEFINE2(tkill, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(0, pid, sig);\n}\n\nstatic int do_rt_sigqueueinfo(pid_t pid, int sig, kernel_siginfo_t *info)\n{\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\n\t    (task_pid_vnr(current) != pid))\n\t\treturn -EPERM;\n\n\t/* POSIX.1b doesn't mention process groups.  */\n\treturn kill_proc_info(sig, info, pid);\n}\n\n/**\n *  sys_rt_sigqueueinfo - send signal information to a signal\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *  @uinfo: signal info to be sent\n */\nSYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tkernel_siginfo_t info;\n\tint ret = __copy_siginfo_from_user(sig, &info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_sigqueueinfo(pid, sig, &info);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(rt_sigqueueinfo,\n\t\t\tcompat_pid_t, pid,\n\t\t\tint, sig,\n\t\t\tstruct compat_siginfo __user *, uinfo)\n{\n\tkernel_siginfo_t info;\n\tint ret = __copy_siginfo_from_user32(sig, &info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_sigqueueinfo(pid, sig, &info);\n}\n#endif\n\nstatic int do_rt_tgsigqueueinfo(pid_t tgid, pid_t pid, int sig, kernel_siginfo_t *info)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\n\t    (task_pid_vnr(current) != pid))\n\t\treturn -EPERM;\n\n\treturn do_send_specific(tgid, pid, sig, info);\n}\n\nSYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tkernel_siginfo_t info;\n\tint ret = __copy_siginfo_from_user(sig, &info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_tgsigqueueinfo,\n\t\t\tcompat_pid_t, tgid,\n\t\t\tcompat_pid_t, pid,\n\t\t\tint, sig,\n\t\t\tstruct compat_siginfo __user *, uinfo)\n{\n\tkernel_siginfo_t info;\n\tint ret = __copy_siginfo_from_user32(sig, &info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n#endif\n\n/*\n * For kthreads only, must not be used if cloned with CLONE_SIGHAND\n */\nvoid kernel_sigaction(int sig, __sighandler_t action)\n{\n\tspin_lock_irq(&current->sighand->siglock);\n\tcurrent->sighand->action[sig - 1].sa.sa_handler = action;\n\tif (action == SIG_IGN) {\n\t\tsigset_t mask;\n\n\t\tsigemptyset(&mask);\n\t\tsigaddset(&mask, sig);\n\n\t\tflush_sigqueue_mask(&mask, &current->signal->shared_pending);\n\t\tflush_sigqueue_mask(&mask, &current->pending);\n\t\trecalc_sigpending();\n\t}\n\tspin_unlock_irq(&current->sighand->siglock);\n}\nEXPORT_SYMBOL(kernel_sigaction);\n\nvoid __weak sigaction_compat_abi(struct k_sigaction *act,\n\t\tstruct k_sigaction *oact)\n{\n}\n\nint do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)\n{\n\tstruct task_struct *p = current, *t;\n\tstruct k_sigaction *k;\n\tsigset_t mask;\n\n\tif (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))\n\t\treturn -EINVAL;\n\n\tk = &p->sighand->action[sig-1];\n\n\tspin_lock_irq(&p->sighand->siglock);\n\tif (oact)\n\t\t*oact = *k;\n\n\tsigaction_compat_abi(act, oact);\n\n\tif (act) {\n\t\tsigdelsetmask(&act->sa.sa_mask,\n\t\t\t      sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t\t*k = *act;\n\t\t/*\n\t\t * POSIX 3.3.1.3:\n\t\t *  \"Setting a signal action to SIG_IGN for a signal that is\n\t\t *   pending shall cause the pending signal to be discarded,\n\t\t *   whether or not it is blocked.\"\n\t\t *\n\t\t *  \"Setting a signal action to SIG_DFL for a signal that is\n\t\t *   pending and whose default action is to ignore the signal\n\t\t *   (for example, SIGCHLD), shall cause the pending signal to\n\t\t *   be discarded, whether or not it is blocked\"\n\t\t */\n\t\tif (sig_handler_ignored(sig_handler(p, sig), sig)) {\n\t\t\tsigemptyset(&mask);\n\t\t\tsigaddset(&mask, sig);\n\t\t\tflush_sigqueue_mask(&mask, &p->signal->shared_pending);\n\t\t\tfor_each_thread(p, t)\n\t\t\t\tflush_sigqueue_mask(&mask, &t->pending);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&p->sighand->siglock);\n\treturn 0;\n}\n\nstatic int\ndo_sigaltstack (const stack_t *ss, stack_t *oss, unsigned long sp,\n\t\tsize_t min_ss_size)\n{\n\tstruct task_struct *t = current;\n\n\tif (oss) {\n\t\tmemset(oss, 0, sizeof(stack_t));\n\t\toss->ss_sp = (void __user *) t->sas_ss_sp;\n\t\toss->ss_size = t->sas_ss_size;\n\t\toss->ss_flags = sas_ss_flags(sp) |\n\t\t\t(current->sas_ss_flags & SS_FLAG_BITS);\n\t}\n\n\tif (ss) {\n\t\tvoid __user *ss_sp = ss->ss_sp;\n\t\tsize_t ss_size = ss->ss_size;\n\t\tunsigned ss_flags = ss->ss_flags;\n\t\tint ss_mode;\n\n\t\tif (unlikely(on_sig_stack(sp)))\n\t\t\treturn -EPERM;\n\n\t\tss_mode = ss_flags & ~SS_FLAG_BITS;\n\t\tif (unlikely(ss_mode != SS_DISABLE && ss_mode != SS_ONSTACK &&\n\t\t\t\tss_mode != 0))\n\t\t\treturn -EINVAL;\n\n\t\tif (ss_mode == SS_DISABLE) {\n\t\t\tss_size = 0;\n\t\t\tss_sp = NULL;\n\t\t} else {\n\t\t\tif (unlikely(ss_size < min_ss_size))\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tt->sas_ss_sp = (unsigned long) ss_sp;\n\t\tt->sas_ss_size = ss_size;\n\t\tt->sas_ss_flags = ss_flags;\n\t}\n\treturn 0;\n}\n\nSYSCALL_DEFINE2(sigaltstack,const stack_t __user *,uss, stack_t __user *,uoss)\n{\n\tstack_t new, old;\n\tint err;\n\tif (uss && copy_from_user(&new, uss, sizeof(stack_t)))\n\t\treturn -EFAULT;\n\terr = do_sigaltstack(uss ? &new : NULL, uoss ? &old : NULL,\n\t\t\t      current_user_stack_pointer(),\n\t\t\t      MINSIGSTKSZ);\n\tif (!err && uoss && copy_to_user(uoss, &old, sizeof(stack_t)))\n\t\terr = -EFAULT;\n\treturn err;\n}\n\nint restore_altstack(const stack_t __user *uss)\n{\n\tstack_t new;\n\tif (copy_from_user(&new, uss, sizeof(stack_t)))\n\t\treturn -EFAULT;\n\t(void)do_sigaltstack(&new, NULL, current_user_stack_pointer(),\n\t\t\t     MINSIGSTKSZ);\n\t/* squash all but EFAULT for now */\n\treturn 0;\n}\n\nint __save_altstack(stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\tint err = __put_user((void __user *)t->sas_ss_sp, &uss->ss_sp) |\n\t\t__put_user(t->sas_ss_flags, &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n\tif (err)\n\t\treturn err;\n\tif (t->sas_ss_flags & SS_AUTODISARM)\n\t\tsas_ss_reset(t);\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nstatic int do_compat_sigaltstack(const compat_stack_t __user *uss_ptr,\n\t\t\t\t compat_stack_t __user *uoss_ptr)\n{\n\tstack_t uss, uoss;\n\tint ret;\n\n\tif (uss_ptr) {\n\t\tcompat_stack_t uss32;\n\t\tif (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))\n\t\t\treturn -EFAULT;\n\t\tuss.ss_sp = compat_ptr(uss32.ss_sp);\n\t\tuss.ss_flags = uss32.ss_flags;\n\t\tuss.ss_size = uss32.ss_size;\n\t}\n\tret = do_sigaltstack(uss_ptr ? &uss : NULL, &uoss,\n\t\t\t     compat_user_stack_pointer(),\n\t\t\t     COMPAT_MINSIGSTKSZ);\n\tif (ret >= 0 && uoss_ptr)  {\n\t\tcompat_stack_t old;\n\t\tmemset(&old, 0, sizeof(old));\n\t\told.ss_sp = ptr_to_compat(uoss.ss_sp);\n\t\told.ss_flags = uoss.ss_flags;\n\t\told.ss_size = uoss.ss_size;\n\t\tif (copy_to_user(uoss_ptr, &old, sizeof(compat_stack_t)))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\nCOMPAT_SYSCALL_DEFINE2(sigaltstack,\n\t\t\tconst compat_stack_t __user *, uss_ptr,\n\t\t\tcompat_stack_t __user *, uoss_ptr)\n{\n\treturn do_compat_sigaltstack(uss_ptr, uoss_ptr);\n}\n\nint compat_restore_altstack(const compat_stack_t __user *uss)\n{\n\tint err = do_compat_sigaltstack(uss, NULL);\n\t/* squash all but -EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)\n{\n\tint err;\n\tstruct task_struct *t = current;\n\terr = __put_user(ptr_to_compat((void __user *)t->sas_ss_sp),\n\t\t\t &uss->ss_sp) |\n\t\t__put_user(t->sas_ss_flags, &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n\tif (err)\n\t\treturn err;\n\tif (t->sas_ss_flags & SS_AUTODISARM)\n\t\tsas_ss_reset(t);\n\treturn 0;\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPENDING\n\n/**\n *  sys_sigpending - examine pending signals\n *  @uset: where mask of pending signal is returned\n */\nSYSCALL_DEFINE1(sigpending, old_sigset_t __user *, uset)\n{\n\tsigset_t set;\n\n\tif (sizeof(old_sigset_t) > sizeof(*uset))\n\t\treturn -EINVAL;\n\n\tdo_sigpending(&set);\n\n\tif (copy_to_user(uset, &set, sizeof(old_sigset_t)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE1(sigpending, compat_old_sigset_t __user *, set32)\n{\n\tsigset_t set;\n\n\tdo_sigpending(&set);\n\n\treturn put_user(set.sig[0], set32);\n}\n#endif\n\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\n/**\n *  sys_sigprocmask - examine and change blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: signals to add or remove (if non-null)\n *  @oset: previous value of signal mask if non-null\n *\n * Some platforms have their own version with special arguments;\n * others support only sys_rt_sigprocmask.\n */\n\nSYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,\n\t\told_sigset_t __user *, oset)\n{\n\told_sigset_t old_set, new_set;\n\tsigset_t new_blocked;\n\n\told_set = current->blocked.sig[0];\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(*nset)))\n\t\t\treturn -EFAULT;\n\n\t\tnew_blocked = current->blocked;\n\n\t\tswitch (how) {\n\t\tcase SIG_BLOCK:\n\t\t\tsigaddsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_UNBLOCK:\n\t\t\tsigdelsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_SETMASK:\n\t\t\tnew_blocked.sig[0] = new_set;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tset_current_blocked(&new_blocked);\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(*oset)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n#endif /* __ARCH_WANT_SYS_SIGPROCMASK */\n\n#ifndef CONFIG_ODD_RT_SIGACTION\n/**\n *  sys_rt_sigaction - alter an action taken by a process\n *  @sig: signal to be sent\n *  @act: new sigaction\n *  @oact: used to save the previous sigaction\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct sigaction __user *, act,\n\t\tstruct sigaction __user *, oact,\n\t\tsize_t, sigsetsize)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (act && copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))\n\t\treturn -EFAULT;\n\n\tret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tif (oact && copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct compat_sigaction __user *, act,\n\t\tstruct compat_sigaction __user *, oact,\n\t\tcompat_size_t, sigsetsize)\n{\n\tstruct k_sigaction new_ka, old_ka;\n#ifdef __ARCH_HAS_SA_RESTORER\n\tcompat_uptr_t restorer;\n#endif\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(compat_sigset_t))\n\t\treturn -EINVAL;\n\n\tif (act) {\n\t\tcompat_uptr_t handler;\n\t\tret = get_user(handler, &act->sa_handler);\n\t\tnew_ka.sa.sa_handler = compat_ptr(handler);\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tret |= get_user(restorer, &act->sa_restorer);\n\t\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\n#endif\n\t\tret |= get_compat_sigset(&new_ka.sa.sa_mask, &act->sa_mask);\n\t\tret |= get_user(new_ka.sa.sa_flags, &act->sa_flags);\n\t\tif (ret)\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\tif (!ret && oact) {\n\t\tret = put_user(ptr_to_compat(old_ka.sa.sa_handler), \n\t\t\t       &oact->sa_handler);\n\t\tret |= put_compat_sigset(&oact->sa_mask, &old_ka.sa.sa_mask,\n\t\t\t\t\t sizeof(oact->sa_mask));\n\t\tret |= put_user(old_ka.sa.sa_flags, &oact->sa_flags);\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tret |= put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n\t\t\t\t&oact->sa_restorer);\n#endif\n\t}\n\treturn ret;\n}\n#endif\n#endif /* !CONFIG_ODD_RT_SIGACTION */\n\n#ifdef CONFIG_OLD_SIGACTION\nSYSCALL_DEFINE3(sigaction, int, sig,\n\t\tconst struct old_sigaction __user *, act,\n\t        struct old_sigaction __user *, oact)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tint ret;\n\n\tif (act) {\n\t\told_sigset_t mask;\n\t\tif (!access_ok(act, sizeof(*act)) ||\n\t\t    __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||\n\t\t    __get_user(new_ka.sa.sa_restorer, &act->sa_restorer) ||\n\t\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n\t\t    __get_user(mask, &act->sa_mask))\n\t\t\treturn -EFAULT;\n#ifdef __ARCH_HAS_KA_RESTORER\n\t\tnew_ka.ka_restorer = NULL;\n#endif\n\t\tsiginitset(&new_ka.sa.sa_mask, mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n\tif (!ret && oact) {\n\t\tif (!access_ok(oact, sizeof(*oact)) ||\n\t\t    __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||\n\t\t    __put_user(old_ka.sa.sa_restorer, &oact->sa_restorer) ||\n\t\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n\t\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn ret;\n}\n#endif\n#ifdef CONFIG_COMPAT_OLD_SIGACTION\nCOMPAT_SYSCALL_DEFINE3(sigaction, int, sig,\n\t\tconst struct compat_old_sigaction __user *, act,\n\t        struct compat_old_sigaction __user *, oact)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tint ret;\n\tcompat_old_sigset_t mask;\n\tcompat_uptr_t handler, restorer;\n\n\tif (act) {\n\t\tif (!access_ok(act, sizeof(*act)) ||\n\t\t    __get_user(handler, &act->sa_handler) ||\n\t\t    __get_user(restorer, &act->sa_restorer) ||\n\t\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n\t\t    __get_user(mask, &act->sa_mask))\n\t\t\treturn -EFAULT;\n\n#ifdef __ARCH_HAS_KA_RESTORER\n\t\tnew_ka.ka_restorer = NULL;\n#endif\n\t\tnew_ka.sa.sa_handler = compat_ptr(handler);\n\t\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\n\t\tsiginitset(&new_ka.sa.sa_mask, mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n\tif (!ret && oact) {\n\t\tif (!access_ok(oact, sizeof(*oact)) ||\n\t\t    __put_user(ptr_to_compat(old_ka.sa.sa_handler),\n\t\t\t       &oact->sa_handler) ||\n\t\t    __put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n\t\t\t       &oact->sa_restorer) ||\n\t\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n\t\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n\t\t\treturn -EFAULT;\n\t}\n\treturn ret;\n}\n#endif\n\n#ifdef CONFIG_SGETMASK_SYSCALL\n\n/*\n * For backwards compatibility.  Functionality superseded by sigprocmask.\n */\nSYSCALL_DEFINE0(sgetmask)\n{\n\t/* SMP safe */\n\treturn current->blocked.sig[0];\n}\n\nSYSCALL_DEFINE1(ssetmask, int, newmask)\n{\n\tint old = current->blocked.sig[0];\n\tsigset_t newset;\n\n\tsiginitset(&newset, newmask);\n\tset_current_blocked(&newset);\n\n\treturn old;\n}\n#endif /* CONFIG_SGETMASK_SYSCALL */\n\n#ifdef __ARCH_WANT_SYS_SIGNAL\n/*\n * For backwards compatibility.  Functionality superseded by sigaction.\n */\nSYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret;\n\n\tnew_sa.sa.sa_handler = handler;\n\tnew_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;\n\tsigemptyset(&new_sa.sa.sa_mask);\n\n\tret = do_sigaction(sig, &new_sa, &old_sa);\n\n\treturn ret ? ret : (unsigned long)old_sa.sa.sa_handler;\n}\n#endif /* __ARCH_WANT_SYS_SIGNAL */\n\n#ifdef __ARCH_WANT_SYS_PAUSE\n\nSYSCALL_DEFINE0(pause)\n{\n\twhile (!signal_pending(current)) {\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n\treturn -ERESTARTNOHAND;\n}\n\n#endif\n\nstatic int sigsuspend(sigset_t *set)\n{\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(set);\n\n\twhile (!signal_pending(current)) {\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n\tset_restore_sigmask();\n\treturn -ERESTARTNOHAND;\n}\n\n/**\n *  sys_rt_sigsuspend - replace the signal mask for a value with the\n *\t@unewset value until a signal is received\n *  @unewset: new signal mask value\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)\n{\n\tsigset_t newset;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&newset, unewset, sizeof(newset)))\n\t\treturn -EFAULT;\n\treturn sigsuspend(&newset);\n}\n \n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigsuspend, compat_sigset_t __user *, unewset, compat_size_t, sigsetsize)\n{\n\tsigset_t newset;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (get_compat_sigset(&newset, unewset))\n\t\treturn -EFAULT;\n\treturn sigsuspend(&newset);\n}\n#endif\n\n#ifdef CONFIG_OLD_SIGSUSPEND\nSYSCALL_DEFINE1(sigsuspend, old_sigset_t, mask)\n{\n\tsigset_t blocked;\n\tsiginitset(&blocked, mask);\n\treturn sigsuspend(&blocked);\n}\n#endif\n#ifdef CONFIG_OLD_SIGSUSPEND3\nSYSCALL_DEFINE3(sigsuspend, int, unused1, int, unused2, old_sigset_t, mask)\n{\n\tsigset_t blocked;\n\tsiginitset(&blocked, mask);\n\treturn sigsuspend(&blocked);\n}\n#endif\n\n__weak const char *arch_vma_name(struct vm_area_struct *vma)\n{\n\treturn NULL;\n}\n\nstatic inline void siginfo_buildtime_checks(void)\n{\n\tBUILD_BUG_ON(sizeof(struct siginfo) != SI_MAX_SIZE);\n\n\t/* Verify the offsets in the two siginfos match */\n#define CHECK_OFFSET(field) \\\n\tBUILD_BUG_ON(offsetof(siginfo_t, field) != offsetof(kernel_siginfo_t, field))\n\n\t/* kill */\n\tCHECK_OFFSET(si_pid);\n\tCHECK_OFFSET(si_uid);\n\n\t/* timer */\n\tCHECK_OFFSET(si_tid);\n\tCHECK_OFFSET(si_overrun);\n\tCHECK_OFFSET(si_value);\n\n\t/* rt */\n\tCHECK_OFFSET(si_pid);\n\tCHECK_OFFSET(si_uid);\n\tCHECK_OFFSET(si_value);\n\n\t/* sigchld */\n\tCHECK_OFFSET(si_pid);\n\tCHECK_OFFSET(si_uid);\n\tCHECK_OFFSET(si_status);\n\tCHECK_OFFSET(si_utime);\n\tCHECK_OFFSET(si_stime);\n\n\t/* sigfault */\n\tCHECK_OFFSET(si_addr);\n\tCHECK_OFFSET(si_addr_lsb);\n\tCHECK_OFFSET(si_lower);\n\tCHECK_OFFSET(si_upper);\n\tCHECK_OFFSET(si_pkey);\n\n\t/* sigpoll */\n\tCHECK_OFFSET(si_band);\n\tCHECK_OFFSET(si_fd);\n\n\t/* sigsys */\n\tCHECK_OFFSET(si_call_addr);\n\tCHECK_OFFSET(si_syscall);\n\tCHECK_OFFSET(si_arch);\n#undef CHECK_OFFSET\n\n\t/* usb asyncio */\n\tBUILD_BUG_ON(offsetof(struct siginfo, si_pid) !=\n\t\t     offsetof(struct siginfo, si_addr));\n\tif (sizeof(int) == sizeof(void __user *)) {\n\t\tBUILD_BUG_ON(sizeof_field(struct siginfo, si_pid) !=\n\t\t\t     sizeof(void __user *));\n\t} else {\n\t\tBUILD_BUG_ON((sizeof_field(struct siginfo, si_pid) +\n\t\t\t      sizeof_field(struct siginfo, si_uid)) !=\n\t\t\t     sizeof(void __user *));\n\t\tBUILD_BUG_ON(offsetofend(struct siginfo, si_pid) !=\n\t\t\t     offsetof(struct siginfo, si_uid));\n\t}\n#ifdef CONFIG_COMPAT\n\tBUILD_BUG_ON(offsetof(struct compat_siginfo, si_pid) !=\n\t\t     offsetof(struct compat_siginfo, si_addr));\n\tBUILD_BUG_ON(sizeof_field(struct compat_siginfo, si_pid) !=\n\t\t     sizeof(compat_uptr_t));\n\tBUILD_BUG_ON(sizeof_field(struct compat_siginfo, si_pid) !=\n\t\t     sizeof_field(struct siginfo, si_pid));\n#endif\n}\n\nvoid __init signals_init(void)\n{\n\tsiginfo_buildtime_checks();\n\n\tsigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC);\n}\n\n#ifdef CONFIG_KGDB_KDB\n#include <linux/kdb.h>\n/*\n * kdb_send_sig - Allows kdb to send signals without exposing\n * signal internals.  This function checks if the required locks are\n * available before calling the main signal code, to avoid kdb\n * deadlocks.\n */\nvoid kdb_send_sig(struct task_struct *t, int sig)\n{\n\tstatic struct task_struct *kdb_prev_t;\n\tint new_t, ret;\n\tif (!spin_trylock(&t->sighand->siglock)) {\n\t\tkdb_printf(\"Can't do kill command now.\\n\"\n\t\t\t   \"The sigmask lock is held somewhere else in \"\n\t\t\t   \"kernel, try again later\\n\");\n\t\treturn;\n\t}\n\tnew_t = kdb_prev_t != t;\n\tkdb_prev_t = t;\n\tif (t->state != TASK_RUNNING && new_t) {\n\t\tspin_unlock(&t->sighand->siglock);\n\t\tkdb_printf(\"Process is not RUNNING, sending a signal from \"\n\t\t\t   \"kdb risks deadlock\\n\"\n\t\t\t   \"on the run queue locks. \"\n\t\t\t   \"The signal has _not_ been sent.\\n\"\n\t\t\t   \"Reissue the kill command if you want to risk \"\n\t\t\t   \"the deadlock.\\n\");\n\t\treturn;\n\t}\n\tret = send_signal(sig, SEND_SIG_PRIV, t, PIDTYPE_PID);\n\tspin_unlock(&t->sighand->siglock);\n\tif (ret)\n\t\tkdb_printf(\"Fail to deliver Signal %d to process %d.\\n\",\n\t\t\t   sig, t->pid);\n\telse\n\t\tkdb_printf(\"Signal %d is sent to process %d.\\n\", sig, t->pid);\n}\n#endif\t/* CONFIG_KGDB_KDB */\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/fs/exec.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n/*\n * #!-checking implemented by tytso.\n */\n/*\n * Demand-loading implemented 01.12.91 - no need to read anything but\n * the header into memory. The inode of the executable is put into\n * \"current->executable\", and page faults do the actual loading. Clean.\n *\n * Once more I can proudly say that linux stood up to being changed: it\n * was less than 2 hours work to get demand-loading completely implemented.\n *\n * Demand loading changed July 1993 by Eric Youngdale.   Use mmap instead,\n * current->executable is only used by the procfs.  This allows a dispatch\n * table to check for several different types  of binary formats.  We keep\n * trying until we recognize the file or we run out of supported binary\n * formats.\n */\n\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/vmacache.h>\n#include <linux/stat.h>\n#include <linux/fcntl.h>\n#include <linux/swap.h>\n#include <linux/string.h>\n#include <linux/init.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/task.h>\n#include <linux/pagemap.h>\n#include <linux/perf_event.h>\n#include <linux/highmem.h>\n#include <linux/spinlock.h>\n#include <linux/key.h>\n#include <linux/personality.h>\n#include <linux/binfmts.h>\n#include <linux/utsname.h>\n#include <linux/pid_namespace.h>\n#include <linux/module.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/tsacct_kern.h>\n#include <linux/cn_proc.h>\n#include <linux/audit.h>\n#include <linux/tracehook.h>\n#include <linux/kmod.h>\n#include <linux/fsnotify.h>\n#include <linux/fs_struct.h>\n#include <linux/oom.h>\n#include <linux/compat.h>\n#include <linux/vmalloc.h>\n\n#include <linux/uaccess.h>\n#include <asm/mmu_context.h>\n#include <asm/tlb.h>\n\n#include <trace/events/task.h>\n#include \"internal.h\"\n\n#include <trace/events/sched.h>\n\nint suid_dumpable = 0;\n\nstatic LIST_HEAD(formats);\nstatic DEFINE_RWLOCK(binfmt_lock);\n\nvoid __register_binfmt(struct linux_binfmt * fmt, int insert)\n{\n\tBUG_ON(!fmt);\n\tif (WARN_ON(!fmt->load_binary))\n\t\treturn;\n\twrite_lock(&binfmt_lock);\n\tinsert ? list_add(&fmt->lh, &formats) :\n\t\t list_add_tail(&fmt->lh, &formats);\n\twrite_unlock(&binfmt_lock);\n}\n\nEXPORT_SYMBOL(__register_binfmt);\n\nvoid unregister_binfmt(struct linux_binfmt * fmt)\n{\n\twrite_lock(&binfmt_lock);\n\tlist_del(&fmt->lh);\n\twrite_unlock(&binfmt_lock);\n}\n\nEXPORT_SYMBOL(unregister_binfmt);\n\nstatic inline void put_binfmt(struct linux_binfmt * fmt)\n{\n\tmodule_put(fmt->module);\n}\n\nbool path_noexec(const struct path *path)\n{\n\treturn (path->mnt->mnt_flags & MNT_NOEXEC) ||\n\t       (path->mnt->mnt_sb->s_iflags & SB_I_NOEXEC);\n}\n\n#ifdef CONFIG_USELIB\n/*\n * Note that a shared library must be both readable and executable due to\n * security reasons.\n *\n * Also note that we take the address to load from from the file itself.\n */\nSYSCALL_DEFINE1(uselib, const char __user *, library)\n{\n\tstruct linux_binfmt *fmt;\n\tstruct file *file;\n\tstruct filename *tmp = getname(library);\n\tint error = PTR_ERR(tmp);\n\tstatic const struct open_flags uselib_flags = {\n\t\t.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,\n\t\t.acc_mode = MAY_READ | MAY_EXEC,\n\t\t.intent = LOOKUP_OPEN,\n\t\t.lookup_flags = LOOKUP_FOLLOW,\n\t};\n\n\tif (IS_ERR(tmp))\n\t\tgoto out;\n\n\tfile = do_filp_open(AT_FDCWD, tmp, &uselib_flags);\n\tputname(tmp);\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out;\n\n\terror = -EINVAL;\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\tgoto exit;\n\n\terror = -EACCES;\n\tif (path_noexec(&file->f_path))\n\t\tgoto exit;\n\n\tfsnotify_open(file);\n\n\terror = -ENOEXEC;\n\n\tread_lock(&binfmt_lock);\n\tlist_for_each_entry(fmt, &formats, lh) {\n\t\tif (!fmt->load_shlib)\n\t\t\tcontinue;\n\t\tif (!try_module_get(fmt->module))\n\t\t\tcontinue;\n\t\tread_unlock(&binfmt_lock);\n\t\terror = fmt->load_shlib(file);\n\t\tread_lock(&binfmt_lock);\n\t\tput_binfmt(fmt);\n\t\tif (error != -ENOEXEC)\n\t\t\tbreak;\n\t}\n\tread_unlock(&binfmt_lock);\nexit:\n\tfput(file);\nout:\n  \treturn error;\n}\n#endif /* #ifdef CONFIG_USELIB */\n\n#ifdef CONFIG_MMU\n/*\n * The nascent bprm->mm is not visible until exec_mmap() but it can\n * use a lot of memory, account these pages in current->mm temporary\n * for oom_badness()->get_mm_rss(). Once exec succeeds or fails, we\n * change the counter back via acct_arg_size(0).\n */\nstatic void acct_arg_size(struct linux_binprm *bprm, unsigned long pages)\n{\n\tstruct mm_struct *mm = current->mm;\n\tlong diff = (long)(pages - bprm->vma_pages);\n\n\tif (!mm || !diff)\n\t\treturn;\n\n\tbprm->vma_pages = pages;\n\tadd_mm_counter(mm, MM_ANONPAGES, diff);\n}\n\nstatic struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\tint ret;\n\tunsigned int gup_flags = FOLL_FORCE;\n\n#ifdef CONFIG_STACK_GROWSUP\n\tif (write) {\n\t\tret = expand_downwards(bprm->vma, pos);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t}\n#endif\n\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\t/*\n\t * We are doing an exec().  'current' is the process\n\t * doing the exec and bprm->mm is the new process's mm.\n\t */\n\tret = get_user_pages_remote(current, bprm->mm, pos, 1, gup_flags,\n\t\t\t&page, NULL, NULL);\n\tif (ret <= 0)\n\t\treturn NULL;\n\n\tif (write)\n\t\tacct_arg_size(bprm, vma_pages(bprm->vma));\n\n\treturn page;\n}\n\nstatic void put_arg_page(struct page *page)\n{\n\tput_page(page);\n}\n\nstatic void free_arg_pages(struct linux_binprm *bprm)\n{\n}\n\nstatic void flush_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tstruct page *page)\n{\n\tflush_cache_page(bprm->vma, pos, page_to_pfn(page));\n}\n\nstatic int __bprm_mm_init(struct linux_binprm *bprm)\n{\n\tint err;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = bprm->mm;\n\n\tbprm->vma = vma = vm_area_alloc(mm);\n\tif (!vma)\n\t\treturn -ENOMEM;\n\tvma_set_anonymous(vma);\n\n\tif (down_write_killable(&mm->mmap_sem)) {\n\t\terr = -EINTR;\n\t\tgoto err_free;\n\t}\n\n\t/*\n\t * Place the stack at the largest stack address the architecture\n\t * supports. Later, we'll move this to an appropriate place. We don't\n\t * use STACK_TOP because that can depend on attributes which aren't\n\t * configured yet.\n\t */\n\tBUILD_BUG_ON(VM_STACK_FLAGS & VM_STACK_INCOMPLETE_SETUP);\n\tvma->vm_end = STACK_TOP_MAX;\n\tvma->vm_start = vma->vm_end - PAGE_SIZE;\n\tvma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n\n\terr = insert_vm_struct(mm, vma);\n\tif (err)\n\t\tgoto err;\n\n\tmm->stack_vm = mm->total_vm = 1;\n\tup_write(&mm->mmap_sem);\n\tbprm->p = vma->vm_end - sizeof(void *);\n\treturn 0;\nerr:\n\tup_write(&mm->mmap_sem);\nerr_free:\n\tbprm->vma = NULL;\n\tvm_area_free(vma);\n\treturn err;\n}\n\nstatic bool valid_arg_len(struct linux_binprm *bprm, long len)\n{\n\treturn len <= MAX_ARG_STRLEN;\n}\n\n#else\n\nstatic inline void acct_arg_size(struct linux_binprm *bprm, unsigned long pages)\n{\n}\n\nstatic struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\n\tpage = bprm->page[pos / PAGE_SIZE];\n\tif (!page && write) {\n\t\tpage = alloc_page(GFP_HIGHUSER|__GFP_ZERO);\n\t\tif (!page)\n\t\t\treturn NULL;\n\t\tbprm->page[pos / PAGE_SIZE] = page;\n\t}\n\n\treturn page;\n}\n\nstatic void put_arg_page(struct page *page)\n{\n}\n\nstatic void free_arg_page(struct linux_binprm *bprm, int i)\n{\n\tif (bprm->page[i]) {\n\t\t__free_page(bprm->page[i]);\n\t\tbprm->page[i] = NULL;\n\t}\n}\n\nstatic void free_arg_pages(struct linux_binprm *bprm)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_ARG_PAGES; i++)\n\t\tfree_arg_page(bprm, i);\n}\n\nstatic void flush_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tstruct page *page)\n{\n}\n\nstatic int __bprm_mm_init(struct linux_binprm *bprm)\n{\n\tbprm->p = PAGE_SIZE * MAX_ARG_PAGES - sizeof(void *);\n\treturn 0;\n}\n\nstatic bool valid_arg_len(struct linux_binprm *bprm, long len)\n{\n\treturn len <= bprm->p;\n}\n\n#endif /* CONFIG_MMU */\n\n/*\n * Create a new mm_struct and populate it with a temporary stack\n * vm_area_struct.  We don't have enough context at this point to set the stack\n * flags, permissions, and offset, so we use temporary values.  We'll update\n * them later in setup_arg_pages().\n */\nstatic int bprm_mm_init(struct linux_binprm *bprm)\n{\n\tint err;\n\tstruct mm_struct *mm = NULL;\n\n\tbprm->mm = mm = mm_alloc();\n\terr = -ENOMEM;\n\tif (!mm)\n\t\tgoto err;\n\n\t/* Save current stack limit for all calculations made during exec. */\n\ttask_lock(current->group_leader);\n\tbprm->rlim_stack = current->signal->rlim[RLIMIT_STACK];\n\ttask_unlock(current->group_leader);\n\n\terr = __bprm_mm_init(bprm);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tif (mm) {\n\t\tbprm->mm = NULL;\n\t\tmmdrop(mm);\n\t}\n\n\treturn err;\n}\n\nstruct user_arg_ptr {\n#ifdef CONFIG_COMPAT\n\tbool is_compat;\n#endif\n\tunion {\n\t\tconst char __user *const __user *native;\n#ifdef CONFIG_COMPAT\n\t\tconst compat_uptr_t __user *compat;\n#endif\n\t} ptr;\n};\n\nstatic const char __user *get_user_arg_ptr(struct user_arg_ptr argv, int nr)\n{\n\tconst char __user *native;\n\n#ifdef CONFIG_COMPAT\n\tif (unlikely(argv.is_compat)) {\n\t\tcompat_uptr_t compat;\n\n\t\tif (get_user(compat, argv.ptr.compat + nr))\n\t\t\treturn ERR_PTR(-EFAULT);\n\n\t\treturn compat_ptr(compat);\n\t}\n#endif\n\n\tif (get_user(native, argv.ptr.native + nr))\n\t\treturn ERR_PTR(-EFAULT);\n\n\treturn native;\n}\n\n/*\n * count() counts the number of strings in array ARGV.\n */\nstatic int count(struct user_arg_ptr argv, int max)\n{\n\tint i = 0;\n\n\tif (argv.ptr.native != NULL) {\n\t\tfor (;;) {\n\t\t\tconst char __user *p = get_user_arg_ptr(argv, i);\n\n\t\t\tif (!p)\n\t\t\t\tbreak;\n\n\t\t\tif (IS_ERR(p))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tif (i >= max)\n\t\t\t\treturn -E2BIG;\n\t\t\t++i;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\treturn -ERESTARTNOHAND;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\treturn i;\n}\n\nstatic int prepare_arg_pages(struct linux_binprm *bprm,\n\t\t\tstruct user_arg_ptr argv, struct user_arg_ptr envp)\n{\n\tunsigned long limit, ptr_size;\n\n\tbprm->argc = count(argv, MAX_ARG_STRINGS);\n\tif (bprm->argc < 0)\n\t\treturn bprm->argc;\n\n\tbprm->envc = count(envp, MAX_ARG_STRINGS);\n\tif (bprm->envc < 0)\n\t\treturn bprm->envc;\n\n\t/*\n\t * Limit to 1/4 of the max stack size or 3/4 of _STK_LIM\n\t * (whichever is smaller) for the argv+env strings.\n\t * This ensures that:\n\t *  - the remaining binfmt code will not run out of stack space,\n\t *  - the program will have a reasonable amount of stack left\n\t *    to work from.\n\t */\n\tlimit = _STK_LIM / 4 * 3;\n\tlimit = min(limit, bprm->rlim_stack.rlim_cur / 4);\n\t/*\n\t * We've historically supported up to 32 pages (ARG_MAX)\n\t * of argument strings even with small stacks\n\t */\n\tlimit = max_t(unsigned long, limit, ARG_MAX);\n\t/*\n\t * We must account for the size of all the argv and envp pointers to\n\t * the argv and envp strings, since they will also take up space in\n\t * the stack. They aren't stored until much later when we can't\n\t * signal to the parent that the child has run out of stack space.\n\t * Instead, calculate it here so it's possible to fail gracefully.\n\t */\n\tptr_size = (bprm->argc + bprm->envc) * sizeof(void *);\n\tif (limit <= ptr_size)\n\t\treturn -E2BIG;\n\tlimit -= ptr_size;\n\n\tbprm->argmin = bprm->p - limit;\n\treturn 0;\n}\n\n/*\n * 'copy_strings()' copies argument/environment strings from the old\n * processes's memory to the new process's stack.  The call to get_user_pages()\n * ensures the destination page is created and not swapped out.\n */\nstatic int copy_strings(int argc, struct user_arg_ptr argv,\n\t\t\tstruct linux_binprm *bprm)\n{\n\tstruct page *kmapped_page = NULL;\n\tchar *kaddr = NULL;\n\tunsigned long kpos = 0;\n\tint ret;\n\n\twhile (argc-- > 0) {\n\t\tconst char __user *str;\n\t\tint len;\n\t\tunsigned long pos;\n\n\t\tret = -EFAULT;\n\t\tstr = get_user_arg_ptr(argv, argc);\n\t\tif (IS_ERR(str))\n\t\t\tgoto out;\n\n\t\tlen = strnlen_user(str, MAX_ARG_STRLEN);\n\t\tif (!len)\n\t\t\tgoto out;\n\n\t\tret = -E2BIG;\n\t\tif (!valid_arg_len(bprm, len))\n\t\t\tgoto out;\n\n\t\t/* We're going to work our way backwords. */\n\t\tpos = bprm->p;\n\t\tstr += len;\n\t\tbprm->p -= len;\n#ifdef CONFIG_MMU\n\t\tif (bprm->p < bprm->argmin)\n\t\t\tgoto out;\n#endif\n\n\t\twhile (len > 0) {\n\t\t\tint offset, bytes_to_copy;\n\n\t\t\tif (fatal_signal_pending(current)) {\n\t\t\t\tret = -ERESTARTNOHAND;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcond_resched();\n\n\t\t\toffset = pos % PAGE_SIZE;\n\t\t\tif (offset == 0)\n\t\t\t\toffset = PAGE_SIZE;\n\n\t\t\tbytes_to_copy = offset;\n\t\t\tif (bytes_to_copy > len)\n\t\t\t\tbytes_to_copy = len;\n\n\t\t\toffset -= bytes_to_copy;\n\t\t\tpos -= bytes_to_copy;\n\t\t\tstr -= bytes_to_copy;\n\t\t\tlen -= bytes_to_copy;\n\n\t\t\tif (!kmapped_page || kpos != (pos & PAGE_MASK)) {\n\t\t\t\tstruct page *page;\n\n\t\t\t\tpage = get_arg_page(bprm, pos, 1);\n\t\t\t\tif (!page) {\n\t\t\t\t\tret = -E2BIG;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tif (kmapped_page) {\n\t\t\t\t\tflush_kernel_dcache_page(kmapped_page);\n\t\t\t\t\tkunmap(kmapped_page);\n\t\t\t\t\tput_arg_page(kmapped_page);\n\t\t\t\t}\n\t\t\t\tkmapped_page = page;\n\t\t\t\tkaddr = kmap(kmapped_page);\n\t\t\t\tkpos = pos & PAGE_MASK;\n\t\t\t\tflush_arg_page(bprm, kpos, kmapped_page);\n\t\t\t}\n\t\t\tif (copy_from_user(kaddr+offset, str, bytes_to_copy)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tret = 0;\nout:\n\tif (kmapped_page) {\n\t\tflush_kernel_dcache_page(kmapped_page);\n\t\tkunmap(kmapped_page);\n\t\tput_arg_page(kmapped_page);\n\t}\n\treturn ret;\n}\n\n/*\n * Like copy_strings, but get argv and its values from kernel memory.\n */\nint copy_strings_kernel(int argc, const char *const *__argv,\n\t\t\tstruct linux_binprm *bprm)\n{\n\tint r;\n\tmm_segment_t oldfs = get_fs();\n\tstruct user_arg_ptr argv = {\n\t\t.ptr.native = (const char __user *const  __user *)__argv,\n\t};\n\n\tset_fs(KERNEL_DS);\n\tr = copy_strings(argc, argv, bprm);\n\tset_fs(oldfs);\n\n\treturn r;\n}\nEXPORT_SYMBOL(copy_strings_kernel);\n\n#ifdef CONFIG_MMU\n\n/*\n * During bprm_mm_init(), we create a temporary stack at STACK_TOP_MAX.  Once\n * the binfmt code determines where the new stack should reside, we shift it to\n * its final location.  The process proceeds as follows:\n *\n * 1) Use shift to calculate the new vma endpoints.\n * 2) Extend vma to cover both the old and new ranges.  This ensures the\n *    arguments passed to subsequent functions are consistent.\n * 3) Move vma's page tables to the new range.\n * 4) Free up any cleared pgd range.\n * 5) Shrink the vma to cover only the new range.\n */\nstatic int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long old_start = vma->vm_start;\n\tunsigned long old_end = vma->vm_end;\n\tunsigned long length = old_end - old_start;\n\tunsigned long new_start = old_start - shift;\n\tunsigned long new_end = old_end - shift;\n\tstruct mmu_gather tlb;\n\n\tBUG_ON(new_start > new_end);\n\n\t/*\n\t * ensure there are no vmas between where we want to go\n\t * and where we are\n\t */\n\tif (vma != find_vma(mm, new_start))\n\t\treturn -EFAULT;\n\n\t/*\n\t * cover the whole range: [new_start, old_end)\n\t */\n\tif (vma_adjust(vma, new_start, old_end, vma->vm_pgoff, NULL))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * move the page tables downwards, on failure we rely on\n\t * process cleanup to remove whatever mess we made.\n\t */\n\tif (length != move_page_tables(vma, old_start,\n\t\t\t\t       vma, new_start, length, false))\n\t\treturn -ENOMEM;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm, old_start, old_end);\n\tif (new_end > old_start) {\n\t\t/*\n\t\t * when the old and new regions overlap clear from new_end.\n\t\t */\n\t\tfree_pgd_range(&tlb, new_end, old_end, new_end,\n\t\t\tvma->vm_next ? vma->vm_next->vm_start : USER_PGTABLES_CEILING);\n\t} else {\n\t\t/*\n\t\t * otherwise, clean from old_start; this is done to not touch\n\t\t * the address space in [new_end, old_start) some architectures\n\t\t * have constraints on va-space that make this illegal (IA64) -\n\t\t * for the others its just a little faster.\n\t\t */\n\t\tfree_pgd_range(&tlb, old_start, old_end, new_end,\n\t\t\tvma->vm_next ? vma->vm_next->vm_start : USER_PGTABLES_CEILING);\n\t}\n\ttlb_finish_mmu(&tlb, old_start, old_end);\n\n\t/*\n\t * Shrink the vma to just the new range.  Always succeeds.\n\t */\n\tvma_adjust(vma, new_start, new_end, vma->vm_pgoff, NULL);\n\n\treturn 0;\n}\n\n/*\n * Finalizes the stack vm_area_struct. The flags and permissions are updated,\n * the stack is optionally relocated, and some extra space is added.\n */\nint setup_arg_pages(struct linux_binprm *bprm,\n\t\t    unsigned long stack_top,\n\t\t    int executable_stack)\n{\n\tunsigned long ret;\n\tunsigned long stack_shift;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = bprm->vma;\n\tstruct vm_area_struct *prev = NULL;\n\tunsigned long vm_flags;\n\tunsigned long stack_base;\n\tunsigned long stack_size;\n\tunsigned long stack_expand;\n\tunsigned long rlim_stack;\n\n#ifdef CONFIG_STACK_GROWSUP\n\t/* Limit stack size */\n\tstack_base = bprm->rlim_stack.rlim_max;\n\tif (stack_base > STACK_SIZE_MAX)\n\t\tstack_base = STACK_SIZE_MAX;\n\n\t/* Add space for stack randomization. */\n\tstack_base += (STACK_RND_MASK << PAGE_SHIFT);\n\n\t/* Make sure we didn't let the argument array grow too large. */\n\tif (vma->vm_end - vma->vm_start > stack_base)\n\t\treturn -ENOMEM;\n\n\tstack_base = PAGE_ALIGN(stack_top - stack_base);\n\n\tstack_shift = vma->vm_start - stack_base;\n\tmm->arg_start = bprm->p - stack_shift;\n\tbprm->p = vma->vm_end - stack_shift;\n#else\n\tstack_top = arch_align_stack(stack_top);\n\tstack_top = PAGE_ALIGN(stack_top);\n\n\tif (unlikely(stack_top < mmap_min_addr) ||\n\t    unlikely(vma->vm_end - vma->vm_start >= stack_top - mmap_min_addr))\n\t\treturn -ENOMEM;\n\n\tstack_shift = vma->vm_end - stack_top;\n\n\tbprm->p -= stack_shift;\n\tmm->arg_start = bprm->p;\n#endif\n\n\tif (bprm->loader)\n\t\tbprm->loader -= stack_shift;\n\tbprm->exec -= stack_shift;\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tvm_flags = VM_STACK_FLAGS;\n\n\t/*\n\t * Adjust stack execute permissions; explicitly enable for\n\t * EXSTACK_ENABLE_X, disable for EXSTACK_DISABLE_X and leave alone\n\t * (arch default) otherwise.\n\t */\n\tif (unlikely(executable_stack == EXSTACK_ENABLE_X))\n\t\tvm_flags |= VM_EXEC;\n\telse if (executable_stack == EXSTACK_DISABLE_X)\n\t\tvm_flags &= ~VM_EXEC;\n\tvm_flags |= mm->def_flags;\n\tvm_flags |= VM_STACK_INCOMPLETE_SETUP;\n\n\tret = mprotect_fixup(vma, &prev, vma->vm_start, vma->vm_end,\n\t\t\tvm_flags);\n\tif (ret)\n\t\tgoto out_unlock;\n\tBUG_ON(prev != vma);\n\n\tif (unlikely(vm_flags & VM_EXEC)) {\n\t\tpr_warn_once(\"process '%pD4' started with executable stack\\n\",\n\t\t\t     bprm->file);\n\t}\n\n\t/* Move stack pages down in memory. */\n\tif (stack_shift) {\n\t\tret = shift_arg_pages(vma, stack_shift);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* mprotect_fixup is overkill to remove the temporary stack flags */\n\tvma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;\n\n\tstack_expand = 131072UL; /* randomly 32*4k (or 2*64k) pages */\n\tstack_size = vma->vm_end - vma->vm_start;\n\t/*\n\t * Align this down to a page boundary as expand_stack\n\t * will align it up.\n\t */\n\trlim_stack = bprm->rlim_stack.rlim_cur & PAGE_MASK;\n#ifdef CONFIG_STACK_GROWSUP\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_start + rlim_stack;\n\telse\n\t\tstack_base = vma->vm_end + stack_expand;\n#else\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_end - rlim_stack;\n\telse\n\t\tstack_base = vma->vm_start - stack_expand;\n#endif\n\tcurrent->mm->start_stack = bprm->p;\n\tret = expand_stack(vma, stack_base);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(setup_arg_pages);\n\n#else\n\n/*\n * Transfer the program arguments and environment from the holding pages\n * onto the stack. The provided stack pointer is adjusted accordingly.\n */\nint transfer_args_to_stack(struct linux_binprm *bprm,\n\t\t\t   unsigned long *sp_location)\n{\n\tunsigned long index, stop, sp;\n\tint ret = 0;\n\n\tstop = bprm->p >> PAGE_SHIFT;\n\tsp = *sp_location;\n\n\tfor (index = MAX_ARG_PAGES - 1; index >= stop; index--) {\n\t\tunsigned int offset = index == stop ? bprm->p & ~PAGE_MASK : 0;\n\t\tchar *src = kmap(bprm->page[index]) + offset;\n\t\tsp -= PAGE_SIZE - offset;\n\t\tif (copy_to_user((void *) sp, src, PAGE_SIZE - offset) != 0)\n\t\t\tret = -EFAULT;\n\t\tkunmap(bprm->page[index]);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t*sp_location = sp;\n\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(transfer_args_to_stack);\n\n#endif /* CONFIG_MMU */\n\nstatic struct file *do_open_execat(int fd, struct filename *name, int flags)\n{\n\tstruct file *file;\n\tint err;\n\tstruct open_flags open_exec_flags = {\n\t\t.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,\n\t\t.acc_mode = MAY_EXEC,\n\t\t.intent = LOOKUP_OPEN,\n\t\t.lookup_flags = LOOKUP_FOLLOW,\n\t};\n\n\tif ((flags & ~(AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH)) != 0)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (flags & AT_SYMLINK_NOFOLLOW)\n\t\topen_exec_flags.lookup_flags &= ~LOOKUP_FOLLOW;\n\tif (flags & AT_EMPTY_PATH)\n\t\topen_exec_flags.lookup_flags |= LOOKUP_EMPTY;\n\n\tfile = do_filp_open(fd, name, &open_exec_flags);\n\tif (IS_ERR(file))\n\t\tgoto out;\n\n\terr = -EACCES;\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\tgoto exit;\n\n\tif (path_noexec(&file->f_path))\n\t\tgoto exit;\n\n\terr = deny_write_access(file);\n\tif (err)\n\t\tgoto exit;\n\n\tif (name->name[0] != '\\0')\n\t\tfsnotify_open(file);\n\nout:\n\treturn file;\n\nexit:\n\tfput(file);\n\treturn ERR_PTR(err);\n}\n\nstruct file *open_exec(const char *name)\n{\n\tstruct filename *filename = getname_kernel(name);\n\tstruct file *f = ERR_CAST(filename);\n\n\tif (!IS_ERR(filename)) {\n\t\tf = do_open_execat(AT_FDCWD, filename, 0);\n\t\tputname(filename);\n\t}\n\treturn f;\n}\nEXPORT_SYMBOL(open_exec);\n\nint kernel_read_file(struct file *file, void **buf, loff_t *size,\n\t\t     loff_t max_size, enum kernel_read_file_id id)\n{\n\tloff_t i_size, pos;\n\tssize_t bytes = 0;\n\tint ret;\n\n\tif (!S_ISREG(file_inode(file)->i_mode) || max_size < 0)\n\t\treturn -EINVAL;\n\n\tret = deny_write_access(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = security_kernel_read_file(file, id);\n\tif (ret)\n\t\tgoto out;\n\n\ti_size = i_size_read(file_inode(file));\n\tif (i_size <= 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (i_size > SIZE_MAX || (max_size > 0 && i_size > max_size)) {\n\t\tret = -EFBIG;\n\t\tgoto out;\n\t}\n\n\tif (id != READING_FIRMWARE_PREALLOC_BUFFER)\n\t\t*buf = vmalloc(i_size);\n\tif (!*buf) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpos = 0;\n\twhile (pos < i_size) {\n\t\tbytes = kernel_read(file, *buf + pos, i_size - pos, &pos);\n\t\tif (bytes < 0) {\n\t\t\tret = bytes;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tif (bytes == 0)\n\t\t\tbreak;\n\t}\n\n\tif (pos != i_size) {\n\t\tret = -EIO;\n\t\tgoto out_free;\n\t}\n\n\tret = security_kernel_post_read_file(file, *buf, i_size, id);\n\tif (!ret)\n\t\t*size = pos;\n\nout_free:\n\tif (ret < 0) {\n\t\tif (id != READING_FIRMWARE_PREALLOC_BUFFER) {\n\t\t\tvfree(*buf);\n\t\t\t*buf = NULL;\n\t\t}\n\t}\n\nout:\n\tallow_write_access(file);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kernel_read_file);\n\nint kernel_read_file_from_path(const char *path, void **buf, loff_t *size,\n\t\t\t       loff_t max_size, enum kernel_read_file_id id)\n{\n\tstruct file *file;\n\tint ret;\n\n\tif (!path || !*path)\n\t\treturn -EINVAL;\n\n\tfile = filp_open(path, O_RDONLY, 0);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tret = kernel_read_file(file, buf, size, max_size, id);\n\tfput(file);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kernel_read_file_from_path);\n\nint kernel_read_file_from_fd(int fd, void **buf, loff_t *size, loff_t max_size,\n\t\t\t     enum kernel_read_file_id id)\n{\n\tstruct fd f = fdget(fd);\n\tint ret = -EBADF;\n\n\tif (!f.file)\n\t\tgoto out;\n\n\tret = kernel_read_file(f.file, buf, size, max_size, id);\nout:\n\tfdput(f);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kernel_read_file_from_fd);\n\nssize_t read_code(struct file *file, unsigned long addr, loff_t pos, size_t len)\n{\n\tssize_t res = vfs_read(file, (void __user *)addr, len, &pos);\n\tif (res > 0)\n\t\tflush_icache_range(addr, addr + len);\n\treturn res;\n}\nEXPORT_SYMBOL(read_code);\n\nstatic int exec_mmap(struct mm_struct *mm)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *old_mm, *active_mm;\n\n\t/* Notify parent that we're no longer interested in the old VM */\n\ttsk = current;\n\told_mm = current->mm;\n\texec_mm_release(tsk, old_mm);\n\n\tif (old_mm) {\n\t\tsync_mm_rss(old_mm);\n\t\t/*\n\t\t * Make sure that if there is a core dump in progress\n\t\t * for the old mm, we get out and die instead of going\n\t\t * through with the exec.  We must hold mmap_sem around\n\t\t * checking core_state and changing tsk->mm.\n\t\t */\n\t\tdown_read(&old_mm->mmap_sem);\n\t\tif (unlikely(old_mm->core_state)) {\n\t\t\tup_read(&old_mm->mmap_sem);\n\t\t\treturn -EINTR;\n\t\t}\n\t}\n\ttask_lock(tsk);\n\tactive_mm = tsk->active_mm;\n\tmembarrier_exec_mmap(mm);\n\ttsk->mm = mm;\n\ttsk->active_mm = mm;\n\tactivate_mm(active_mm, mm);\n\ttsk->mm->vmacache_seqnum = 0;\n\tvmacache_flush(tsk);\n\ttask_unlock(tsk);\n\tif (old_mm) {\n\t\tup_read(&old_mm->mmap_sem);\n\t\tBUG_ON(active_mm != old_mm);\n\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, old_mm);\n\t\tmm_update_next_owner(old_mm);\n\t\tmmput(old_mm);\n\t\treturn 0;\n\t}\n\tmmdrop(active_mm);\n\treturn 0;\n}\n\n/*\n * This function makes sure the current process has its own signal table,\n * so that flush_signal_handlers can later reset the handlers without\n * disturbing other processes.  (Other processes might share the signal\n * table via the CLONE_SIGHAND option to clone().)\n */\nstatic int de_thread(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tstruct sighand_struct *oldsighand = tsk->sighand;\n\tspinlock_t *lock = &oldsighand->siglock;\n\n\tif (thread_group_empty(tsk))\n\t\tgoto no_thread_group;\n\n\t/*\n\t * Kill all other threads in the thread group.\n\t */\n\tspin_lock_irq(lock);\n\tif (signal_group_exit(sig)) {\n\t\t/*\n\t\t * Another group action in progress, just\n\t\t * return so that the signal is processed.\n\t\t */\n\t\tspin_unlock_irq(lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tsig->group_exit_task = tsk;\n\tsig->notify_count = zap_other_threads(tsk);\n\tif (!thread_group_leader(tsk))\n\t\tsig->notify_count--;\n\n\twhile (sig->notify_count) {\n\t\t__set_current_state(TASK_KILLABLE);\n\t\tspin_unlock_irq(lock);\n\t\tschedule();\n\t\tif (__fatal_signal_pending(tsk))\n\t\t\tgoto killed;\n\t\tspin_lock_irq(lock);\n\t}\n\tspin_unlock_irq(lock);\n\n\t/*\n\t * At this point all other threads have exited, all we have to\n\t * do is to wait for the thread group leader to become inactive,\n\t * and to assume its PID:\n\t */\n\tif (!thread_group_leader(tsk)) {\n\t\tstruct task_struct *leader = tsk->group_leader;\n\n\t\tfor (;;) {\n\t\t\tcgroup_threadgroup_change_begin(tsk);\n\t\t\twrite_lock_irq(&tasklist_lock);\n\t\t\t/*\n\t\t\t * Do this under tasklist_lock to ensure that\n\t\t\t * exit_notify() can't miss ->group_exit_task\n\t\t\t */\n\t\t\tsig->notify_count = -1;\n\t\t\tif (likely(leader->exit_state))\n\t\t\t\tbreak;\n\t\t\t__set_current_state(TASK_KILLABLE);\n\t\t\twrite_unlock_irq(&tasklist_lock);\n\t\t\tcgroup_threadgroup_change_end(tsk);\n\t\t\tschedule();\n\t\t\tif (__fatal_signal_pending(tsk))\n\t\t\t\tgoto killed;\n\t\t}\n\n\t\t/*\n\t\t * The only record we have of the real-time age of a\n\t\t * process, regardless of execs it's done, is start_time.\n\t\t * All the past CPU time is accumulated in signal_struct\n\t\t * from sister threads now dead.  But in this non-leader\n\t\t * exec, nothing survives from the original leader thread,\n\t\t * whose birth marks the true age of this process now.\n\t\t * When we take on its identity by switching to its PID, we\n\t\t * also take its birthdate (always earlier than our own).\n\t\t */\n\t\ttsk->start_time = leader->start_time;\n\t\ttsk->start_boottime = leader->start_boottime;\n\n\t\tBUG_ON(!same_thread_group(leader, tsk));\n\t\tBUG_ON(has_group_leader_pid(tsk));\n\t\t/*\n\t\t * An exec() starts a new thread group with the\n\t\t * TGID of the previous thread group. Rehash the\n\t\t * two threads with a switched PID, and release\n\t\t * the former thread group leader:\n\t\t */\n\n\t\t/* Become a process group leader with the old leader's pid.\n\t\t * The old leader becomes a thread of the this thread group.\n\t\t * Note: The old leader also uses this pid until release_task\n\t\t *       is called.  Odd but simple and correct.\n\t\t */\n\t\ttsk->pid = leader->pid;\n\t\tchange_pid(tsk, PIDTYPE_PID, task_pid(leader));\n\t\ttransfer_pid(leader, tsk, PIDTYPE_TGID);\n\t\ttransfer_pid(leader, tsk, PIDTYPE_PGID);\n\t\ttransfer_pid(leader, tsk, PIDTYPE_SID);\n\n\t\tlist_replace_rcu(&leader->tasks, &tsk->tasks);\n\t\tlist_replace_init(&leader->sibling, &tsk->sibling);\n\n\t\ttsk->group_leader = tsk;\n\t\tleader->group_leader = tsk;\n\n\t\ttsk->exit_signal = SIGCHLD;\n\t\tleader->exit_signal = -1;\n\n\t\tBUG_ON(leader->exit_state != EXIT_ZOMBIE);\n\t\tleader->exit_state = EXIT_DEAD;\n\n\t\t/*\n\t\t * We are going to release_task()->ptrace_unlink() silently,\n\t\t * the tracer can sleep in do_wait(). EXIT_DEAD guarantees\n\t\t * the tracer wont't block again waiting for this thread.\n\t\t */\n\t\tif (unlikely(leader->ptrace))\n\t\t\t__wake_up_parent(leader, leader->parent);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tcgroup_threadgroup_change_end(tsk);\n\n\t\trelease_task(leader);\n\t}\n\n\tsig->group_exit_task = NULL;\n\tsig->notify_count = 0;\n\nno_thread_group:\n\t/* we have changed execution domain */\n\ttsk->exit_signal = SIGCHLD;\n\n#ifdef CONFIG_POSIX_TIMERS\n\texit_itimers(sig);\n\tflush_itimer_signals();\n#endif\n\n\tif (refcount_read(&oldsighand->count) != 1) {\n\t\tstruct sighand_struct *newsighand;\n\t\t/*\n\t\t * This ->sighand is shared with the CLONE_SIGHAND\n\t\t * but not CLONE_THREAD task, switch to the new one.\n\t\t */\n\t\tnewsighand = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);\n\t\tif (!newsighand)\n\t\t\treturn -ENOMEM;\n\n\t\trefcount_set(&newsighand->count, 1);\n\t\tmemcpy(newsighand->action, oldsighand->action,\n\t\t       sizeof(newsighand->action));\n\n\t\twrite_lock_irq(&tasklist_lock);\n\t\tspin_lock(&oldsighand->siglock);\n\t\trcu_assign_pointer(tsk->sighand, newsighand);\n\t\tspin_unlock(&oldsighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\n\t\t__cleanup_sighand(oldsighand);\n\t}\n\n\tBUG_ON(!thread_group_leader(tsk));\n\treturn 0;\n\nkilled:\n\t/* protects against exit_notify() and __exit_signal() */\n\tread_lock(&tasklist_lock);\n\tsig->group_exit_task = NULL;\n\tsig->notify_count = 0;\n\tread_unlock(&tasklist_lock);\n\treturn -EAGAIN;\n}\n\nchar *__get_task_comm(char *buf, size_t buf_size, struct task_struct *tsk)\n{\n\ttask_lock(tsk);\n\tstrncpy(buf, tsk->comm, buf_size);\n\ttask_unlock(tsk);\n\treturn buf;\n}\nEXPORT_SYMBOL_GPL(__get_task_comm);\n\n/*\n * These functions flushes out all traces of the currently running executable\n * so that a new one can be started\n */\n\nvoid __set_task_comm(struct task_struct *tsk, const char *buf, bool exec)\n{\n\ttask_lock(tsk);\n\ttrace_task_rename(tsk, buf);\n\tstrlcpy(tsk->comm, buf, sizeof(tsk->comm));\n\ttask_unlock(tsk);\n\tperf_event_comm(tsk, exec);\n}\n\n/*\n * Calling this is the point of no return. None of the failures will be\n * seen by userspace since either the process is already taking a fatal\n * signal (via de_thread() or coredump), or will have SEGV raised\n * (after exec_mmap()) by search_binary_handlers (see below).\n */\nint flush_old_exec(struct linux_binprm * bprm)\n{\n\tint retval;\n\n\t/*\n\t * Make sure we have a private signal table and that\n\t * we are unassociated from the previous thread group.\n\t */\n\tretval = de_thread(current);\n\tif (retval)\n\t\tgoto out;\n\n\t/*\n\t * Must be called _before_ exec_mmap() as bprm->mm is\n\t * not visibile until then. This also enables the update\n\t * to be lockless.\n\t */\n\tset_mm_exe_file(bprm->mm, bprm->file);\n\n\t/*\n\t * Release all of the old mmap stuff\n\t */\n\tacct_arg_size(bprm, 0);\n\tretval = exec_mmap(bprm->mm);\n\tif (retval)\n\t\tgoto out;\n\n\t/*\n\t * After clearing bprm->mm (to mark that current is using the\n\t * prepared mm now), we have nothing left of the original\n\t * process. If anything from here on returns an error, the check\n\t * in search_binary_handler() will SEGV current.\n\t */\n\tbprm->mm = NULL;\n\n\tset_fs(USER_DS);\n\tcurrent->flags &= ~(PF_RANDOMIZE | PF_FORKNOEXEC | PF_KTHREAD |\n\t\t\t\t\tPF_NOFREEZE | PF_NO_SETAFFINITY);\n\tflush_thread();\n\tcurrent->personality &= ~bprm->per_clear;\n\n\t/*\n\t * We have to apply CLOEXEC before we change whether the process is\n\t * dumpable (in setup_new_exec) to avoid a race with a process in userspace\n\t * trying to access the should-be-closed file descriptors of a process\n\t * undergoing exec(2).\n\t */\n\tdo_close_on_exec(current->files);\n\treturn 0;\n\nout:\n\treturn retval;\n}\nEXPORT_SYMBOL(flush_old_exec);\n\nvoid would_dump(struct linux_binprm *bprm, struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\tif (inode_permission(inode, MAY_READ) < 0) {\n\t\tstruct user_namespace *old, *user_ns;\n\t\tbprm->interp_flags |= BINPRM_FLAGS_ENFORCE_NONDUMP;\n\n\t\t/* Ensure mm->user_ns contains the executable */\n\t\tuser_ns = old = bprm->mm->user_ns;\n\t\twhile ((user_ns != &init_user_ns) &&\n\t\t       !privileged_wrt_inode_uidgid(user_ns, inode))\n\t\t\tuser_ns = user_ns->parent;\n\n\t\tif (old != user_ns) {\n\t\t\tbprm->mm->user_ns = get_user_ns(user_ns);\n\t\t\tput_user_ns(old);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(would_dump);\n\nvoid setup_new_exec(struct linux_binprm * bprm)\n{\n\t/*\n\t * Once here, prepare_binrpm() will not be called any more, so\n\t * the final state of setuid/setgid/fscaps can be merged into the\n\t * secureexec flag.\n\t */\n\tbprm->secureexec |= bprm->cap_elevated;\n\n\tif (bprm->secureexec) {\n\t\t/* Make sure parent cannot signal privileged process. */\n\t\tcurrent->pdeath_signal = 0;\n\n\t\t/*\n\t\t * For secureexec, reset the stack limit to sane default to\n\t\t * avoid bad behavior from the prior rlimits. This has to\n\t\t * happen before arch_pick_mmap_layout(), which examines\n\t\t * RLIMIT_STACK, but after the point of no return to avoid\n\t\t * needing to clean up the change on failure.\n\t\t */\n\t\tif (bprm->rlim_stack.rlim_cur > _STK_LIM)\n\t\t\tbprm->rlim_stack.rlim_cur = _STK_LIM;\n\t}\n\n\tarch_pick_mmap_layout(current->mm, &bprm->rlim_stack);\n\n\tcurrent->sas_ss_sp = current->sas_ss_size = 0;\n\n\t/*\n\t * Figure out dumpability. Note that this checking only of current\n\t * is wrong, but userspace depends on it. This should be testing\n\t * bprm->secureexec instead.\n\t */\n\tif (bprm->interp_flags & BINPRM_FLAGS_ENFORCE_NONDUMP ||\n\t    !(uid_eq(current_euid(), current_uid()) &&\n\t      gid_eq(current_egid(), current_gid())))\n\t\tset_dumpable(current->mm, suid_dumpable);\n\telse\n\t\tset_dumpable(current->mm, SUID_DUMP_USER);\n\n\tarch_setup_new_exec();\n\tperf_event_exec();\n\t__set_task_comm(current, kbasename(bprm->filename), true);\n\n\t/* Set the new mm task size. We have to do that late because it may\n\t * depend on TIF_32BIT which is only updated in flush_thread() on\n\t * some architectures like powerpc\n\t */\n\tcurrent->mm->task_size = TASK_SIZE;\n\n\t/* An exec changes our domain. We are no longer part of the thread\n\t   group */\n\tWRITE_ONCE(current->self_exec_id, current->self_exec_id + 1);\n\tflush_signal_handlers(current, 0);\n}\nEXPORT_SYMBOL(setup_new_exec);\n\n/* Runs immediately before start_thread() takes over. */\nvoid finalize_exec(struct linux_binprm *bprm)\n{\n\t/* Store any stack rlimit changes before starting thread. */\n\ttask_lock(current->group_leader);\n\tcurrent->signal->rlim[RLIMIT_STACK] = bprm->rlim_stack;\n\ttask_unlock(current->group_leader);\n}\nEXPORT_SYMBOL(finalize_exec);\n\n/*\n * Prepare credentials and lock ->cred_guard_mutex.\n * install_exec_creds() commits the new creds and drops the lock.\n * Or, if exec fails before, free_bprm() should release ->cred and\n * and unlock.\n */\nstatic int prepare_bprm_creds(struct linux_binprm *bprm)\n{\n\tif (mutex_lock_interruptible(&current->signal->cred_guard_mutex))\n\t\treturn -ERESTARTNOINTR;\n\n\tbprm->cred = prepare_exec_creds();\n\tif (likely(bprm->cred))\n\t\treturn 0;\n\n\tmutex_unlock(&current->signal->cred_guard_mutex);\n\treturn -ENOMEM;\n}\n\nstatic void free_bprm(struct linux_binprm *bprm)\n{\n\tfree_arg_pages(bprm);\n\tif (bprm->cred) {\n\t\tmutex_unlock(&current->signal->cred_guard_mutex);\n\t\tabort_creds(bprm->cred);\n\t}\n\tif (bprm->file) {\n\t\tallow_write_access(bprm->file);\n\t\tfput(bprm->file);\n\t}\n\t/* If a binfmt changed the interp, free it. */\n\tif (bprm->interp != bprm->filename)\n\t\tkfree(bprm->interp);\n\tkfree(bprm);\n}\n\nint bprm_change_interp(const char *interp, struct linux_binprm *bprm)\n{\n\t/* If a binfmt changed the interp, free it first. */\n\tif (bprm->interp != bprm->filename)\n\t\tkfree(bprm->interp);\n\tbprm->interp = kstrdup(interp, GFP_KERNEL);\n\tif (!bprm->interp)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nEXPORT_SYMBOL(bprm_change_interp);\n\n/*\n * install the new credentials for this executable\n */\nvoid install_exec_creds(struct linux_binprm *bprm)\n{\n\tsecurity_bprm_committing_creds(bprm);\n\n\tcommit_creds(bprm->cred);\n\tbprm->cred = NULL;\n\n\t/*\n\t * Disable monitoring for regular users\n\t * when executing setuid binaries. Must\n\t * wait until new credentials are committed\n\t * by commit_creds() above\n\t */\n\tif (get_dumpable(current->mm) != SUID_DUMP_USER)\n\t\tperf_event_exit_task(current);\n\t/*\n\t * cred_guard_mutex must be held at least to this point to prevent\n\t * ptrace_attach() from altering our determination of the task's\n\t * credentials; any time after this it may be unlocked.\n\t */\n\tsecurity_bprm_committed_creds(bprm);\n\tmutex_unlock(&current->signal->cred_guard_mutex);\n}\nEXPORT_SYMBOL(install_exec_creds);\n\n/*\n * determine how safe it is to execute the proposed program\n * - the caller must hold ->cred_guard_mutex to protect against\n *   PTRACE_ATTACH or seccomp thread-sync\n */\nstatic void check_unsafe_exec(struct linux_binprm *bprm)\n{\n\tstruct task_struct *p = current, *t;\n\tunsigned n_fs;\n\n\tif (p->ptrace)\n\t\tbprm->unsafe |= LSM_UNSAFE_PTRACE;\n\n\t/*\n\t * This isn't strictly necessary, but it makes it harder for LSMs to\n\t * mess up.\n\t */\n\tif (task_no_new_privs(current))\n\t\tbprm->unsafe |= LSM_UNSAFE_NO_NEW_PRIVS;\n\n\tt = p;\n\tn_fs = 1;\n\tspin_lock(&p->fs->lock);\n\trcu_read_lock();\n\twhile_each_thread(p, t) {\n\t\tif (t->fs == p->fs)\n\t\t\tn_fs++;\n\t}\n\trcu_read_unlock();\n\n\tif (p->fs->users > n_fs)\n\t\tbprm->unsafe |= LSM_UNSAFE_SHARE;\n\telse\n\t\tp->fs->in_exec = 1;\n\tspin_unlock(&p->fs->lock);\n}\n\nstatic void bprm_fill_uid(struct linux_binprm *bprm)\n{\n\tstruct inode *inode;\n\tunsigned int mode;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\t/*\n\t * Since this can be called multiple times (via prepare_binprm),\n\t * we must clear any previous work done when setting set[ug]id\n\t * bits from any earlier bprm->file uses (for example when run\n\t * first for a setuid script then again for its interpreter).\n\t */\n\tbprm->cred->euid = current_euid();\n\tbprm->cred->egid = current_egid();\n\n\tif (!mnt_may_suid(bprm->file->f_path.mnt))\n\t\treturn;\n\n\tif (task_no_new_privs(current))\n\t\treturn;\n\n\tinode = bprm->file->f_path.dentry->d_inode;\n\tmode = READ_ONCE(inode->i_mode);\n\tif (!(mode & (S_ISUID|S_ISGID)))\n\t\treturn;\n\n\t/* Be careful if suid/sgid is set */\n\tinode_lock(inode);\n\n\t/* reload atomically mode/uid/gid now that lock held */\n\tmode = inode->i_mode;\n\tuid = inode->i_uid;\n\tgid = inode->i_gid;\n\tinode_unlock(inode);\n\n\t/* We ignore suid/sgid if there are no mappings for them in the ns */\n\tif (!kuid_has_mapping(bprm->cred->user_ns, uid) ||\n\t\t !kgid_has_mapping(bprm->cred->user_ns, gid))\n\t\treturn;\n\n\tif (mode & S_ISUID) {\n\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\tbprm->cred->euid = uid;\n\t}\n\n\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\tbprm->cred->egid = gid;\n\t}\n}\n\n/*\n * Fill the binprm structure from the inode.\n * Check permissions, then read the first BINPRM_BUF_SIZE bytes\n *\n * This may be called multiple times for binary chains (scripts for example).\n */\nint prepare_binprm(struct linux_binprm *bprm)\n{\n\tint retval;\n\tloff_t pos = 0;\n\n\tbprm_fill_uid(bprm);\n\n\t/* fill in binprm security blob */\n\tretval = security_bprm_set_creds(bprm);\n\tif (retval)\n\t\treturn retval;\n\tbprm->called_set_creds = 1;\n\n\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\treturn kernel_read(bprm->file, bprm->buf, BINPRM_BUF_SIZE, &pos);\n}\n\nEXPORT_SYMBOL(prepare_binprm);\n\n/*\n * Arguments are '\\0' separated strings found at the location bprm->p\n * points to; chop off the first by relocating brpm->p to right after\n * the first '\\0' encountered.\n */\nint remove_arg_zero(struct linux_binprm *bprm)\n{\n\tint ret = 0;\n\tunsigned long offset;\n\tchar *kaddr;\n\tstruct page *page;\n\n\tif (!bprm->argc)\n\t\treturn 0;\n\n\tdo {\n\t\toffset = bprm->p & ~PAGE_MASK;\n\t\tpage = get_arg_page(bprm, bprm->p, 0);\n\t\tif (!page) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkaddr = kmap_atomic(page);\n\n\t\tfor (; offset < PAGE_SIZE && kaddr[offset];\n\t\t\t\toffset++, bprm->p++)\n\t\t\t;\n\n\t\tkunmap_atomic(kaddr);\n\t\tput_arg_page(page);\n\t} while (offset == PAGE_SIZE);\n\n\tbprm->p++;\n\tbprm->argc--;\n\tret = 0;\n\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(remove_arg_zero);\n\n#define printable(c) (((c)=='\\t') || ((c)=='\\n') || (0x20<=(c) && (c)<=0x7e))\n/*\n * cycle the list of binary formats handler, until one recognizes the image\n */\nint search_binary_handler(struct linux_binprm *bprm)\n{\n\tbool need_retry = IS_ENABLED(CONFIG_MODULES);\n\tstruct linux_binfmt *fmt;\n\tint retval;\n\n\t/* This allows 4 levels of binfmt rewrites before failing hard. */\n\tif (bprm->recursion_depth > 5)\n\t\treturn -ELOOP;\n\n\tretval = security_bprm_check(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = -ENOENT;\n retry:\n\tread_lock(&binfmt_lock);\n\tlist_for_each_entry(fmt, &formats, lh) {\n\t\tif (!try_module_get(fmt->module))\n\t\t\tcontinue;\n\t\tread_unlock(&binfmt_lock);\n\n\t\tbprm->recursion_depth++;\n\t\tretval = fmt->load_binary(bprm);\n\t\tbprm->recursion_depth--;\n\n\t\tread_lock(&binfmt_lock);\n\t\tput_binfmt(fmt);\n\t\tif (retval < 0 && !bprm->mm) {\n\t\t\t/* we got to flush_old_exec() and failed after it */\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\tforce_sigsegv(SIGSEGV);\n\t\t\treturn retval;\n\t\t}\n\t\tif (retval != -ENOEXEC || !bprm->file) {\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\treturn retval;\n\t\t}\n\t}\n\tread_unlock(&binfmt_lock);\n\n\tif (need_retry) {\n\t\tif (printable(bprm->buf[0]) && printable(bprm->buf[1]) &&\n\t\t    printable(bprm->buf[2]) && printable(bprm->buf[3]))\n\t\t\treturn retval;\n\t\tif (request_module(\"binfmt-%04x\", *(ushort *)(bprm->buf + 2)) < 0)\n\t\t\treturn retval;\n\t\tneed_retry = false;\n\t\tgoto retry;\n\t}\n\n\treturn retval;\n}\nEXPORT_SYMBOL(search_binary_handler);\n\nstatic int exec_binprm(struct linux_binprm *bprm)\n{\n\tpid_t old_pid, old_vpid;\n\tint ret;\n\n\t/* Need to fetch pid before load_binary changes it */\n\told_pid = current->pid;\n\trcu_read_lock();\n\told_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));\n\trcu_read_unlock();\n\n\tret = search_binary_handler(bprm);\n\tif (ret >= 0) {\n\t\taudit_bprm(bprm);\n\t\ttrace_sched_process_exec(current, old_pid, bprm);\n\t\tptrace_event(PTRACE_EVENT_EXEC, old_vpid);\n\t\tproc_exec_connector(current);\n\t}\n\n\treturn ret;\n}\n\n/*\n * sys_execve() executes a new program.\n */\nstatic int __do_execve_file(int fd, struct filename *filename,\n\t\t\t    struct user_arg_ptr argv,\n\t\t\t    struct user_arg_ptr envp,\n\t\t\t    int flags, struct file *file)\n{\n\tchar *pathbuf = NULL;\n\tstruct linux_binprm *bprm;\n\tstruct files_struct *displaced;\n\tint retval;\n\n\tif (IS_ERR(filename))\n\t\treturn PTR_ERR(filename);\n\n\t/*\n\t * We move the actual failure in case of RLIMIT_NPROC excess from\n\t * set*uid() to execve() because too many poorly written programs\n\t * don't check setuid() return code.  Here we additionally recheck\n\t * whether NPROC limit is still exceeded.\n\t */\n\tif ((current->flags & PF_NPROC_EXCEEDED) &&\n\t    atomic_read(&current_user()->processes) > rlimit(RLIMIT_NPROC)) {\n\t\tretval = -EAGAIN;\n\t\tgoto out_ret;\n\t}\n\n\t/* We're below the limit (still or again), so we don't want to make\n\t * further execve() calls fail. */\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\tgoto out_ret;\n\n\tretval = -ENOMEM;\n\tbprm = kzalloc(sizeof(*bprm), GFP_KERNEL);\n\tif (!bprm)\n\t\tgoto out_files;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_free;\n\n\tcheck_unsafe_exec(bprm);\n\tcurrent->in_execve = 1;\n\n\tif (!file)\n\t\tfile = do_open_execat(fd, filename, flags);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\tif (!filename) {\n\t\tbprm->filename = \"none\";\n\t} else if (fd == AT_FDCWD || filename->name[0] == '/') {\n\t\tbprm->filename = filename->name;\n\t} else {\n\t\tif (filename->name[0] == '\\0')\n\t\t\tpathbuf = kasprintf(GFP_KERNEL, \"/dev/fd/%d\", fd);\n\t\telse\n\t\t\tpathbuf = kasprintf(GFP_KERNEL, \"/dev/fd/%d/%s\",\n\t\t\t\t\t    fd, filename->name);\n\t\tif (!pathbuf) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out_unmark;\n\t\t}\n\t\t/*\n\t\t * Record that a name derived from an O_CLOEXEC fd will be\n\t\t * inaccessible after exec. Relies on having exclusive access to\n\t\t * current->files (due to unshare_files above).\n\t\t */\n\t\tif (close_on_exec(fd, rcu_dereference_raw(current->files->fdt)))\n\t\t\tbprm->interp_flags |= BINPRM_FLAGS_PATH_INACCESSIBLE;\n\t\tbprm->filename = pathbuf;\n\t}\n\tbprm->interp = bprm->filename;\n\n\tretval = bprm_mm_init(bprm);\n\tif (retval)\n\t\tgoto out_unmark;\n\n\tretval = prepare_arg_pages(bprm, argv, envp);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tbprm->exec = bprm->p;\n\tretval = copy_strings(bprm->envc, envp, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings(bprm->argc, argv, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\twould_dump(bprm, bprm->file);\n\n\tretval = exec_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\trseq_execve(current);\n\tacct_update_integrals(current);\n\ttask_numa_free(current, false);\n\tfree_bprm(bprm);\n\tkfree(pathbuf);\n\tif (filename)\n\t\tputname(filename);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\tif (bprm->mm) {\n\t\tacct_arg_size(bprm, 0);\n\t\tmmput(bprm->mm);\n\t}\n\nout_unmark:\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_free:\n\tfree_bprm(bprm);\n\tkfree(pathbuf);\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\nout_ret:\n\tif (filename)\n\t\tputname(filename);\n\treturn retval;\n}\n\nstatic int do_execveat_common(int fd, struct filename *filename,\n\t\t\t      struct user_arg_ptr argv,\n\t\t\t      struct user_arg_ptr envp,\n\t\t\t      int flags)\n{\n\treturn __do_execve_file(fd, filename, argv, envp, flags, NULL);\n}\n\nint do_execve_file(struct file *file, void *__argv, void *__envp)\n{\n\tstruct user_arg_ptr argv = { .ptr.native = __argv };\n\tstruct user_arg_ptr envp = { .ptr.native = __envp };\n\n\treturn __do_execve_file(AT_FDCWD, NULL, argv, envp, 0, file);\n}\n\nint do_execve(struct filename *filename,\n\tconst char __user *const __user *__argv,\n\tconst char __user *const __user *__envp)\n{\n\tstruct user_arg_ptr argv = { .ptr.native = __argv };\n\tstruct user_arg_ptr envp = { .ptr.native = __envp };\n\treturn do_execveat_common(AT_FDCWD, filename, argv, envp, 0);\n}\n\nint do_execveat(int fd, struct filename *filename,\n\t\tconst char __user *const __user *__argv,\n\t\tconst char __user *const __user *__envp,\n\t\tint flags)\n{\n\tstruct user_arg_ptr argv = { .ptr.native = __argv };\n\tstruct user_arg_ptr envp = { .ptr.native = __envp };\n\n\treturn do_execveat_common(fd, filename, argv, envp, flags);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_do_execve(struct filename *filename,\n\tconst compat_uptr_t __user *__argv,\n\tconst compat_uptr_t __user *__envp)\n{\n\tstruct user_arg_ptr argv = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __argv,\n\t};\n\tstruct user_arg_ptr envp = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __envp,\n\t};\n\treturn do_execveat_common(AT_FDCWD, filename, argv, envp, 0);\n}\n\nstatic int compat_do_execveat(int fd, struct filename *filename,\n\t\t\t      const compat_uptr_t __user *__argv,\n\t\t\t      const compat_uptr_t __user *__envp,\n\t\t\t      int flags)\n{\n\tstruct user_arg_ptr argv = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __argv,\n\t};\n\tstruct user_arg_ptr envp = {\n\t\t.is_compat = true,\n\t\t.ptr.compat = __envp,\n\t};\n\treturn do_execveat_common(fd, filename, argv, envp, flags);\n}\n#endif\n\nvoid set_binfmt(struct linux_binfmt *new)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif (mm->binfmt)\n\t\tmodule_put(mm->binfmt->module);\n\n\tmm->binfmt = new;\n\tif (new)\n\t\t__module_get(new->module);\n}\nEXPORT_SYMBOL(set_binfmt);\n\n/*\n * set_dumpable stores three-value SUID_DUMP_* into mm->flags.\n */\nvoid set_dumpable(struct mm_struct *mm, int value)\n{\n\tif (WARN_ON((unsigned)value > SUID_DUMP_ROOT))\n\t\treturn;\n\n\tset_mask_bits(&mm->flags, MMF_DUMPABLE_MASK, value);\n}\n\nSYSCALL_DEFINE3(execve,\n\t\tconst char __user *, filename,\n\t\tconst char __user *const __user *, argv,\n\t\tconst char __user *const __user *, envp)\n{\n\treturn do_execve(getname(filename), argv, envp);\n}\n\nSYSCALL_DEFINE5(execveat,\n\t\tint, fd, const char __user *, filename,\n\t\tconst char __user *const __user *, argv,\n\t\tconst char __user *const __user *, envp,\n\t\tint, flags)\n{\n\tint lookup_flags = (flags & AT_EMPTY_PATH) ? LOOKUP_EMPTY : 0;\n\n\treturn do_execveat(fd,\n\t\t\t   getname_flags(filename, lookup_flags, NULL),\n\t\t\t   argv, envp, flags);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(execve, const char __user *, filename,\n\tconst compat_uptr_t __user *, argv,\n\tconst compat_uptr_t __user *, envp)\n{\n\treturn compat_do_execve(getname(filename), argv, envp);\n}\n\nCOMPAT_SYSCALL_DEFINE5(execveat, int, fd,\n\t\t       const char __user *, filename,\n\t\t       const compat_uptr_t __user *, argv,\n\t\t       const compat_uptr_t __user *, envp,\n\t\t       int,  flags)\n{\n\tint lookup_flags = (flags & AT_EMPTY_PATH) ? LOOKUP_EMPTY : 0;\n\n\treturn compat_do_execveat(fd,\n\t\t\t\t  getname_flags(filename, lookup_flags, NULL),\n\t\t\t\t  argv, envp, flags);\n}\n#endif\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_SCHED_H\n#define _LINUX_SCHED_H\n\n/*\n * Define 'struct task_struct' and provide the main scheduler\n * APIs (schedule(), wakeup variants, etc.)\n */\n\n#include <uapi/linux/sched.h>\n\n#include <asm/current.h>\n\n#include <linux/pid.h>\n#include <linux/sem.h>\n#include <linux/shm.h>\n#include <linux/kcov.h>\n#include <linux/mutex.h>\n#include <linux/plist.h>\n#include <linux/hrtimer.h>\n#include <linux/seccomp.h>\n#include <linux/nodemask.h>\n#include <linux/rcupdate.h>\n#include <linux/refcount.h>\n#include <linux/resource.h>\n#include <linux/latencytop.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/types.h>\n#include <linux/signal_types.h>\n#include <linux/mm_types_task.h>\n#include <linux/task_io_accounting.h>\n#include <linux/posix-timers.h>\n#include <linux/rseq.h>\n\n/* task_struct member predeclarations (sorted alphabetically): */\nstruct audit_context;\nstruct backing_dev_info;\nstruct bio_list;\nstruct blk_plug;\nstruct capture_control;\nstruct cfs_rq;\nstruct fs_struct;\nstruct futex_pi_state;\nstruct io_context;\nstruct mempolicy;\nstruct nameidata;\nstruct nsproxy;\nstruct perf_event_context;\nstruct pid_namespace;\nstruct pipe_inode_info;\nstruct rcu_node;\nstruct reclaim_state;\nstruct robust_list_head;\nstruct root_domain;\nstruct rq;\nstruct sched_attr;\nstruct sched_param;\nstruct seq_file;\nstruct sighand_struct;\nstruct signal_struct;\nstruct task_delay_info;\nstruct task_group;\n\n/*\n * Task state bitmask. NOTE! These bits are also\n * encoded in fs/proc/array.c: get_task_state().\n *\n * We have two separate sets of flags: task->state\n * is about runnability, while task->exit_state are\n * about the task exiting. Confusing, but this way\n * modifying one set can't modify the other one by\n * mistake.\n */\n\n/* Used in tsk->state: */\n#define TASK_RUNNING\t\t\t0x0000\n#define TASK_INTERRUPTIBLE\t\t0x0001\n#define TASK_UNINTERRUPTIBLE\t\t0x0002\n#define __TASK_STOPPED\t\t\t0x0004\n#define __TASK_TRACED\t\t\t0x0008\n/* Used in tsk->exit_state: */\n#define EXIT_DEAD\t\t\t0x0010\n#define EXIT_ZOMBIE\t\t\t0x0020\n#define EXIT_TRACE\t\t\t(EXIT_ZOMBIE | EXIT_DEAD)\n/* Used in tsk->state again: */\n#define TASK_PARKED\t\t\t0x0040\n#define TASK_DEAD\t\t\t0x0080\n#define TASK_WAKEKILL\t\t\t0x0100\n#define TASK_WAKING\t\t\t0x0200\n#define TASK_NOLOAD\t\t\t0x0400\n#define TASK_NEW\t\t\t0x0800\n#define TASK_STATE_MAX\t\t\t0x1000\n\n/* Convenience macros for the sake of set_current_state: */\n#define TASK_KILLABLE\t\t\t(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)\n#define TASK_STOPPED\t\t\t(TASK_WAKEKILL | __TASK_STOPPED)\n#define TASK_TRACED\t\t\t(TASK_WAKEKILL | __TASK_TRACED)\n\n#define TASK_IDLE\t\t\t(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)\n\n/* Convenience macros for the sake of wake_up(): */\n#define TASK_NORMAL\t\t\t(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)\n\n/* get_task_state(): */\n#define TASK_REPORT\t\t\t(TASK_RUNNING | TASK_INTERRUPTIBLE | \\\n\t\t\t\t\t TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\\n\t\t\t\t\t __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \\\n\t\t\t\t\t TASK_PARKED)\n\n#define task_is_traced(task)\t\t((task->state & __TASK_TRACED) != 0)\n\n#define task_is_stopped(task)\t\t((task->state & __TASK_STOPPED) != 0)\n\n#define task_is_stopped_or_traced(task)\t((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)\n\n#define task_contributes_to_load(task)\t((task->state & TASK_UNINTERRUPTIBLE) != 0 && \\\n\t\t\t\t\t (task->flags & PF_FROZEN) == 0 && \\\n\t\t\t\t\t (task->state & TASK_NOLOAD) == 0)\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\n/*\n * Special states are those that do not use the normal wait-loop pattern. See\n * the comment with set_special_state().\n */\n#define is_special_task_state(state)\t\t\t\t\\\n\t((state) & (__TASK_STOPPED | __TASK_TRACED | TASK_PARKED | TASK_DEAD))\n\n#define __set_current_state(state_value)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tWARN_ON_ONCE(is_special_task_state(state_value));\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\\\n\t\tcurrent->state = (state_value);\t\t\t\\\n\t} while (0)\n\n#define set_current_state(state_value)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tWARN_ON_ONCE(is_special_task_state(state_value));\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\\\n\t\tsmp_store_mb(current->state, (state_value));\t\\\n\t} while (0)\n\n#define set_special_state(state_value)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tunsigned long flags; /* may shadow */\t\t\t\\\n\t\tWARN_ON_ONCE(!is_special_task_state(state_value));\t\\\n\t\traw_spin_lock_irqsave(&current->pi_lock, flags);\t\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\t\\\n\t\tcurrent->state = (state_value);\t\t\t\t\\\n\t\traw_spin_unlock_irqrestore(&current->pi_lock, flags);\t\\\n\t} while (0)\n#else\n/*\n * set_current_state() includes a barrier so that the write of current->state\n * is correctly serialised wrt the caller's subsequent test of whether to\n * actually sleep:\n *\n *   for (;;) {\n *\tset_current_state(TASK_UNINTERRUPTIBLE);\n *\tif (!need_sleep)\n *\t\tbreak;\n *\n *\tschedule();\n *   }\n *   __set_current_state(TASK_RUNNING);\n *\n * If the caller does not need such serialisation (because, for instance, the\n * condition test and condition change and wakeup are under the same lock) then\n * use __set_current_state().\n *\n * The above is typically ordered against the wakeup, which does:\n *\n *   need_sleep = false;\n *   wake_up_state(p, TASK_UNINTERRUPTIBLE);\n *\n * where wake_up_state() executes a full memory barrier before accessing the\n * task state.\n *\n * Wakeup will do: if (@state & p->state) p->state = TASK_RUNNING, that is,\n * once it observes the TASK_UNINTERRUPTIBLE store the waking CPU can issue a\n * TASK_RUNNING store which can collide with __set_current_state(TASK_RUNNING).\n *\n * However, with slightly different timing the wakeup TASK_RUNNING store can\n * also collide with the TASK_UNINTERRUPTIBLE store. Losing that store is not\n * a problem either because that will result in one extra go around the loop\n * and our @cond test will save the day.\n *\n * Also see the comments of try_to_wake_up().\n */\n#define __set_current_state(state_value)\t\t\t\t\\\n\tcurrent->state = (state_value)\n\n#define set_current_state(state_value)\t\t\t\t\t\\\n\tsmp_store_mb(current->state, (state_value))\n\n/*\n * set_special_state() should be used for those states when the blocking task\n * can not use the regular condition based wait-loop. In that case we must\n * serialize against wakeups such that any possible in-flight TASK_RUNNING stores\n * will not collide with our state change.\n */\n#define set_special_state(state_value)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tunsigned long flags; /* may shadow */\t\t\t\\\n\t\traw_spin_lock_irqsave(&current->pi_lock, flags);\t\\\n\t\tcurrent->state = (state_value);\t\t\t\t\\\n\t\traw_spin_unlock_irqrestore(&current->pi_lock, flags);\t\\\n\t} while (0)\n\n#endif\n\n/* Task command name length: */\n#define TASK_COMM_LEN\t\t\t16\n\nextern void scheduler_tick(void);\n\n#define\tMAX_SCHEDULE_TIMEOUT\t\tLONG_MAX\n\nextern long schedule_timeout(long timeout);\nextern long schedule_timeout_interruptible(long timeout);\nextern long schedule_timeout_killable(long timeout);\nextern long schedule_timeout_uninterruptible(long timeout);\nextern long schedule_timeout_idle(long timeout);\nasmlinkage void schedule(void);\nextern void schedule_preempt_disabled(void);\nasmlinkage void preempt_schedule_irq(void);\n\nextern int __must_check io_schedule_prepare(void);\nextern void io_schedule_finish(int token);\nextern long io_schedule_timeout(long timeout);\nextern void io_schedule(void);\n\n/**\n * struct prev_cputime - snapshot of system and user cputime\n * @utime: time spent in user mode\n * @stime: time spent in system mode\n * @lock: protects the above two fields\n *\n * Stores previous user/system time values such that we can guarantee\n * monotonicity.\n */\nstruct prev_cputime {\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n\tu64\t\t\t\tutime;\n\tu64\t\t\t\tstime;\n\traw_spinlock_t\t\t\tlock;\n#endif\n};\n\nenum vtime_state {\n\t/* Task is sleeping or running in a CPU with VTIME inactive: */\n\tVTIME_INACTIVE = 0,\n\t/* Task is idle */\n\tVTIME_IDLE,\n\t/* Task runs in kernelspace in a CPU with VTIME active: */\n\tVTIME_SYS,\n\t/* Task runs in userspace in a CPU with VTIME active: */\n\tVTIME_USER,\n\t/* Task runs as guests in a CPU with VTIME active: */\n\tVTIME_GUEST,\n};\n\nstruct vtime {\n\tseqcount_t\t\tseqcount;\n\tunsigned long long\tstarttime;\n\tenum vtime_state\tstate;\n\tunsigned int\t\tcpu;\n\tu64\t\t\tutime;\n\tu64\t\t\tstime;\n\tu64\t\t\tgtime;\n};\n\n/*\n * Utilization clamp constraints.\n * @UCLAMP_MIN:\tMinimum utilization\n * @UCLAMP_MAX:\tMaximum utilization\n * @UCLAMP_CNT:\tUtilization clamp constraints count\n */\nenum uclamp_id {\n\tUCLAMP_MIN = 0,\n\tUCLAMP_MAX,\n\tUCLAMP_CNT\n};\n\n#ifdef CONFIG_SMP\nextern struct root_domain def_root_domain;\nextern struct mutex sched_domains_mutex;\n#endif\n\nstruct sched_info {\n#ifdef CONFIG_SCHED_INFO\n\t/* Cumulative counters: */\n\n\t/* # of times we have run on this CPU: */\n\tunsigned long\t\t\tpcount;\n\n\t/* Time spent waiting on a runqueue: */\n\tunsigned long long\t\trun_delay;\n\n\t/* Timestamps: */\n\n\t/* When did we last run on a CPU? */\n\tunsigned long long\t\tlast_arrival;\n\n\t/* When were we last queued to run? */\n\tunsigned long long\t\tlast_queued;\n\n#endif /* CONFIG_SCHED_INFO */\n};\n\n/*\n * Integer metrics need fixed point arithmetic, e.g., sched/fair\n * has a few: load, load_avg, util_avg, freq, and capacity.\n *\n * We define a basic fixed point arithmetic range, and then formalize\n * all these metrics based on that basic range.\n */\n# define SCHED_FIXEDPOINT_SHIFT\t\t10\n# define SCHED_FIXEDPOINT_SCALE\t\t(1L << SCHED_FIXEDPOINT_SHIFT)\n\n/* Increase resolution of cpu_capacity calculations */\n# define SCHED_CAPACITY_SHIFT\t\tSCHED_FIXEDPOINT_SHIFT\n# define SCHED_CAPACITY_SCALE\t\t(1L << SCHED_CAPACITY_SHIFT)\n\nstruct load_weight {\n\tunsigned long\t\t\tweight;\n\tu32\t\t\t\tinv_weight;\n};\n\n/**\n * struct util_est - Estimation utilization of FAIR tasks\n * @enqueued: instantaneous estimated utilization of a task/cpu\n * @ewma:     the Exponential Weighted Moving Average (EWMA)\n *            utilization of a task\n *\n * Support data structure to track an Exponential Weighted Moving Average\n * (EWMA) of a FAIR task's utilization. New samples are added to the moving\n * average each time a task completes an activation. Sample's weight is chosen\n * so that the EWMA will be relatively insensitive to transient changes to the\n * task's workload.\n *\n * The enqueued attribute has a slightly different meaning for tasks and cpus:\n * - task:   the task's util_avg at last task dequeue time\n * - cfs_rq: the sum of util_est.enqueued for each RUNNABLE task on that CPU\n * Thus, the util_est.enqueued of a task represents the contribution on the\n * estimated utilization of the CPU where that task is currently enqueued.\n *\n * Only for tasks we track a moving average of the past instantaneous\n * estimated utilization. This allows to absorb sporadic drops in utilization\n * of an otherwise almost periodic task.\n */\nstruct util_est {\n\tunsigned int\t\t\tenqueued;\n\tunsigned int\t\t\tewma;\n#define UTIL_EST_WEIGHT_SHIFT\t\t2\n} __attribute__((__aligned__(sizeof(u64))));\n\n/*\n * The load_avg/util_avg accumulates an infinite geometric series\n * (see __update_load_avg() in kernel/sched/fair.c).\n *\n * [load_avg definition]\n *\n *   load_avg = runnable% * scale_load_down(load)\n *\n * where runnable% is the time ratio that a sched_entity is runnable.\n * For cfs_rq, it is the aggregated load_avg of all runnable and\n * blocked sched_entities.\n *\n * [util_avg definition]\n *\n *   util_avg = running% * SCHED_CAPACITY_SCALE\n *\n * where running% is the time ratio that a sched_entity is running on\n * a CPU. For cfs_rq, it is the aggregated util_avg of all runnable\n * and blocked sched_entities.\n *\n * load_avg and util_avg don't direcly factor frequency scaling and CPU\n * capacity scaling. The scaling is done through the rq_clock_pelt that\n * is used for computing those signals (see update_rq_clock_pelt())\n *\n * N.B., the above ratios (runnable% and running%) themselves are in the\n * range of [0, 1]. To do fixed point arithmetics, we therefore scale them\n * to as large a range as necessary. This is for example reflected by\n * util_avg's SCHED_CAPACITY_SCALE.\n *\n * [Overflow issue]\n *\n * The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities\n * with the highest load (=88761), always runnable on a single cfs_rq,\n * and should not overflow as the number already hits PID_MAX_LIMIT.\n *\n * For all other cases (including 32-bit kernels), struct load_weight's\n * weight will overflow first before we do, because:\n *\n *    Max(load_avg) <= Max(load.weight)\n *\n * Then it is the load_weight's responsibility to consider overflow\n * issues.\n */\nstruct sched_avg {\n\tu64\t\t\t\tlast_update_time;\n\tu64\t\t\t\tload_sum;\n\tu64\t\t\t\trunnable_load_sum;\n\tu32\t\t\t\tutil_sum;\n\tu32\t\t\t\tperiod_contrib;\n\tunsigned long\t\t\tload_avg;\n\tunsigned long\t\t\trunnable_load_avg;\n\tunsigned long\t\t\tutil_avg;\n\tstruct util_est\t\t\tutil_est;\n} ____cacheline_aligned;\n\nstruct sched_statistics {\n#ifdef CONFIG_SCHEDSTATS\n\tu64\t\t\t\twait_start;\n\tu64\t\t\t\twait_max;\n\tu64\t\t\t\twait_count;\n\tu64\t\t\t\twait_sum;\n\tu64\t\t\t\tiowait_count;\n\tu64\t\t\t\tiowait_sum;\n\n\tu64\t\t\t\tsleep_start;\n\tu64\t\t\t\tsleep_max;\n\ts64\t\t\t\tsum_sleep_runtime;\n\n\tu64\t\t\t\tblock_start;\n\tu64\t\t\t\tblock_max;\n\tu64\t\t\t\texec_max;\n\tu64\t\t\t\tslice_max;\n\n\tu64\t\t\t\tnr_migrations_cold;\n\tu64\t\t\t\tnr_failed_migrations_affine;\n\tu64\t\t\t\tnr_failed_migrations_running;\n\tu64\t\t\t\tnr_failed_migrations_hot;\n\tu64\t\t\t\tnr_forced_migrations;\n\n\tu64\t\t\t\tnr_wakeups;\n\tu64\t\t\t\tnr_wakeups_sync;\n\tu64\t\t\t\tnr_wakeups_migrate;\n\tu64\t\t\t\tnr_wakeups_local;\n\tu64\t\t\t\tnr_wakeups_remote;\n\tu64\t\t\t\tnr_wakeups_affine;\n\tu64\t\t\t\tnr_wakeups_affine_attempts;\n\tu64\t\t\t\tnr_wakeups_passive;\n\tu64\t\t\t\tnr_wakeups_idle;\n#endif\n};\n\nstruct sched_entity {\n\t/* For load-balancing: */\n\tstruct load_weight\t\tload;\n\tunsigned long\t\t\trunnable_weight;\n\tstruct rb_node\t\t\trun_node;\n\tstruct list_head\t\tgroup_node;\n\tunsigned int\t\t\ton_rq;\n\n\tu64\t\t\t\texec_start;\n\tu64\t\t\t\tsum_exec_runtime;\n\tu64\t\t\t\tvruntime;\n\tu64\t\t\t\tprev_sum_exec_runtime;\n\n\tu64\t\t\t\tnr_migrations;\n\n\tstruct sched_statistics\t\tstatistics;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tint\t\t\t\tdepth;\n\tstruct sched_entity\t\t*parent;\n\t/* rq on which this entity is (to be) queued: */\n\tstruct cfs_rq\t\t\t*cfs_rq;\n\t/* rq \"owned\" by this entity/group: */\n\tstruct cfs_rq\t\t\t*my_q;\n#endif\n\n#ifdef CONFIG_SMP\n\t/*\n\t * Per entity load average tracking.\n\t *\n\t * Put into separate cache line so it does not\n\t * collide with read-mostly values above.\n\t */\n\tstruct sched_avg\t\tavg;\n#endif\n};\n\nstruct sched_rt_entity {\n\tstruct list_head\t\trun_list;\n\tunsigned long\t\t\ttimeout;\n\tunsigned long\t\t\twatchdog_stamp;\n\tunsigned int\t\t\ttime_slice;\n\tunsigned short\t\t\ton_rq;\n\tunsigned short\t\t\ton_list;\n\n\tstruct sched_rt_entity\t\t*back;\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity\t\t*parent;\n\t/* rq on which this entity is (to be) queued: */\n\tstruct rt_rq\t\t\t*rt_rq;\n\t/* rq \"owned\" by this entity/group: */\n\tstruct rt_rq\t\t\t*my_q;\n#endif\n} __randomize_layout;\n\nstruct sched_dl_entity {\n\tstruct rb_node\t\t\trb_node;\n\n\t/*\n\t * Original scheduling parameters. Copied here from sched_attr\n\t * during sched_setattr(), they will remain the same until\n\t * the next sched_setattr().\n\t */\n\tu64\t\t\t\tdl_runtime;\t/* Maximum runtime for each instance\t*/\n\tu64\t\t\t\tdl_deadline;\t/* Relative deadline of each instance\t*/\n\tu64\t\t\t\tdl_period;\t/* Separation of two instances (period) */\n\tu64\t\t\t\tdl_bw;\t\t/* dl_runtime / dl_period\t\t*/\n\tu64\t\t\t\tdl_density;\t/* dl_runtime / dl_deadline\t\t*/\n\n\t/*\n\t * Actual scheduling parameters. Initialized with the values above,\n\t * they are continuously updated during task execution. Note that\n\t * the remaining runtime could be < 0 in case we are in overrun.\n\t */\n\ts64\t\t\t\truntime;\t/* Remaining runtime for this instance\t*/\n\tu64\t\t\t\tdeadline;\t/* Absolute deadline for this instance\t*/\n\tunsigned int\t\t\tflags;\t\t/* Specifying the scheduler behaviour\t*/\n\n\t/*\n\t * Some bool flags:\n\t *\n\t * @dl_throttled tells if we exhausted the runtime. If so, the\n\t * task has to wait for a replenishment to be performed at the\n\t * next firing of dl_timer.\n\t *\n\t * @dl_boosted tells if we are boosted due to DI. If so we are\n\t * outside bandwidth enforcement mechanism (but only until we\n\t * exit the critical section);\n\t *\n\t * @dl_yielded tells if task gave up the CPU before consuming\n\t * all its available runtime during the last job.\n\t *\n\t * @dl_non_contending tells if the task is inactive while still\n\t * contributing to the active utilization. In other words, it\n\t * indicates if the inactive timer has been armed and its handler\n\t * has not been executed yet. This flag is useful to avoid race\n\t * conditions between the inactive timer handler and the wakeup\n\t * code.\n\t *\n\t * @dl_overrun tells if the task asked to be informed about runtime\n\t * overruns.\n\t */\n\tunsigned int\t\t\tdl_throttled      : 1;\n\tunsigned int\t\t\tdl_boosted        : 1;\n\tunsigned int\t\t\tdl_yielded        : 1;\n\tunsigned int\t\t\tdl_non_contending : 1;\n\tunsigned int\t\t\tdl_overrun\t  : 1;\n\n\t/*\n\t * Bandwidth enforcement timer. Each -deadline task has its\n\t * own bandwidth to be enforced, thus we need one timer per task.\n\t */\n\tstruct hrtimer\t\t\tdl_timer;\n\n\t/*\n\t * Inactive timer, responsible for decreasing the active utilization\n\t * at the \"0-lag time\". When a -deadline task blocks, it contributes\n\t * to GRUB's active utilization until the \"0-lag time\", hence a\n\t * timer is needed to decrease the active utilization at the correct\n\t * time.\n\t */\n\tstruct hrtimer inactive_timer;\n};\n\n#ifdef CONFIG_UCLAMP_TASK\n/* Number of utilization clamp buckets (shorter alias) */\n#define UCLAMP_BUCKETS CONFIG_UCLAMP_BUCKETS_COUNT\n\n/*\n * Utilization clamp for a scheduling entity\n * @value:\t\tclamp value \"assigned\" to a se\n * @bucket_id:\t\tbucket index corresponding to the \"assigned\" value\n * @active:\t\tthe se is currently refcounted in a rq's bucket\n * @user_defined:\tthe requested clamp value comes from user-space\n *\n * The bucket_id is the index of the clamp bucket matching the clamp value\n * which is pre-computed and stored to avoid expensive integer divisions from\n * the fast path.\n *\n * The active bit is set whenever a task has got an \"effective\" value assigned,\n * which can be different from the clamp value \"requested\" from user-space.\n * This allows to know a task is refcounted in the rq's bucket corresponding\n * to the \"effective\" bucket_id.\n *\n * The user_defined bit is set whenever a task has got a task-specific clamp\n * value requested from userspace, i.e. the system defaults apply to this task\n * just as a restriction. This allows to relax default clamps when a less\n * restrictive task-specific value has been requested, thus allowing to\n * implement a \"nice\" semantic. For example, a task running with a 20%\n * default boost can still drop its own boosting to 0%.\n */\nstruct uclamp_se {\n\tunsigned int value\t\t: bits_per(SCHED_CAPACITY_SCALE);\n\tunsigned int bucket_id\t\t: bits_per(UCLAMP_BUCKETS);\n\tunsigned int active\t\t: 1;\n\tunsigned int user_defined\t: 1;\n};\n#endif /* CONFIG_UCLAMP_TASK */\n\nunion rcu_special {\n\tstruct {\n\t\tu8\t\t\tblocked;\n\t\tu8\t\t\tneed_qs;\n\t\tu8\t\t\texp_hint; /* Hint for performance. */\n\t\tu8\t\t\tdeferred_qs;\n\t} b; /* Bits. */\n\tu32 s; /* Set of bits. */\n};\n\nenum perf_event_task_context {\n\tperf_invalid_context = -1,\n\tperf_hw_context = 0,\n\tperf_sw_context,\n\tperf_nr_task_contexts,\n};\n\nstruct wake_q_node {\n\tstruct wake_q_node *next;\n};\n\nstruct task_struct {\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t/*\n\t * For reasons of header soup (see current_thread_info()), this\n\t * must be the first element of task_struct.\n\t */\n\tstruct thread_info\t\tthread_info;\n#endif\n\t/* -1 unrunnable, 0 runnable, >0 stopped: */\n\tvolatile long\t\t\tstate;\n\n\t/*\n\t * This begins the randomizable portion of task_struct. Only\n\t * scheduling-critical items should be added above here.\n\t */\n\trandomized_struct_fields_start\n\n\tvoid\t\t\t\t*stack;\n\trefcount_t\t\t\tusage;\n\t/* Per task flags (PF_*), defined further below: */\n\tunsigned int\t\t\tflags;\n\tunsigned int\t\t\tptrace;\n\n#ifdef CONFIG_SMP\n\tstruct llist_node\t\twake_entry;\n\tint\t\t\t\ton_cpu;\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t/* Current CPU: */\n\tunsigned int\t\t\tcpu;\n#endif\n\tunsigned int\t\t\twakee_flips;\n\tunsigned long\t\t\twakee_flip_decay_ts;\n\tstruct task_struct\t\t*last_wakee;\n\n\t/*\n\t * recent_used_cpu is initially set as the last CPU used by a task\n\t * that wakes affine another task. Waker/wakee relationships can\n\t * push tasks around a CPU where each wakeup moves to the next one.\n\t * Tracking a recently used CPU allows a quick search for a recently\n\t * used CPU that may be idle.\n\t */\n\tint\t\t\t\trecent_used_cpu;\n\tint\t\t\t\twake_cpu;\n#endif\n\tint\t\t\t\ton_rq;\n\n\tint\t\t\t\tprio;\n\tint\t\t\t\tstatic_prio;\n\tint\t\t\t\tnormal_prio;\n\tunsigned int\t\t\trt_priority;\n\n\tconst struct sched_class\t*sched_class;\n\tstruct sched_entity\t\tse;\n\tstruct sched_rt_entity\t\trt;\n#ifdef CONFIG_CGROUP_SCHED\n\tstruct task_group\t\t*sched_task_group;\n#endif\n\tstruct sched_dl_entity\t\tdl;\n\n#ifdef CONFIG_UCLAMP_TASK\n\t/* Clamp values requested for a scheduling entity */\n\tstruct uclamp_se\t\tuclamp_req[UCLAMP_CNT];\n\t/* Effective clamp values used for a scheduling entity */\n\tstruct uclamp_se\t\tuclamp[UCLAMP_CNT];\n#endif\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\t/* List of struct preempt_notifier: */\n\tstruct hlist_head\t\tpreempt_notifiers;\n#endif\n\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tunsigned int\t\t\tbtrace_seq;\n#endif\n\n\tunsigned int\t\t\tpolicy;\n\tint\t\t\t\tnr_cpus_allowed;\n\tconst cpumask_t\t\t\t*cpus_ptr;\n\tcpumask_t\t\t\tcpus_mask;\n\n#ifdef CONFIG_PREEMPT_RCU\n\tint\t\t\t\trcu_read_lock_nesting;\n\tunion rcu_special\t\trcu_read_unlock_special;\n\tstruct list_head\t\trcu_node_entry;\n\tstruct rcu_node\t\t\t*rcu_blocked_node;\n#endif /* #ifdef CONFIG_PREEMPT_RCU */\n\n#ifdef CONFIG_TASKS_RCU\n\tunsigned long\t\t\trcu_tasks_nvcsw;\n\tu8\t\t\t\trcu_tasks_holdout;\n\tu8\t\t\t\trcu_tasks_idx;\n\tint\t\t\t\trcu_tasks_idle_cpu;\n\tstruct list_head\t\trcu_tasks_holdout_list;\n#endif /* #ifdef CONFIG_TASKS_RCU */\n\n\tstruct sched_info\t\tsched_info;\n\n\tstruct list_head\t\ttasks;\n#ifdef CONFIG_SMP\n\tstruct plist_node\t\tpushable_tasks;\n\tstruct rb_node\t\t\tpushable_dl_tasks;\n#endif\n\n\tstruct mm_struct\t\t*mm;\n\tstruct mm_struct\t\t*active_mm;\n\n\t/* Per-thread vma caching: */\n\tstruct vmacache\t\t\tvmacache;\n\n#ifdef SPLIT_RSS_COUNTING\n\tstruct task_rss_stat\t\trss_stat;\n#endif\n\tint\t\t\t\texit_state;\n\tint\t\t\t\texit_code;\n\tint\t\t\t\texit_signal;\n\t/* The signal sent when the parent dies: */\n\tint\t\t\t\tpdeath_signal;\n\t/* JOBCTL_*, siglock protected: */\n\tunsigned long\t\t\tjobctl;\n\n\t/* Used for emulating ABI behavior of previous Linux versions: */\n\tunsigned int\t\t\tpersonality;\n\n\t/* Scheduler bits, serialized by scheduler locks: */\n\tunsigned\t\t\tsched_reset_on_fork:1;\n\tunsigned\t\t\tsched_contributes_to_load:1;\n\tunsigned\t\t\tsched_migrated:1;\n\tunsigned\t\t\tsched_remote_wakeup:1;\n#ifdef CONFIG_PSI\n\tunsigned\t\t\tsched_psi_wake_requeue:1;\n#endif\n\n\t/* Force alignment to the next boundary: */\n\tunsigned\t\t\t:0;\n\n\t/* Unserialized, strictly 'current' */\n\n\t/* Bit to tell LSMs we're in execve(): */\n\tunsigned\t\t\tin_execve:1;\n\tunsigned\t\t\tin_iowait:1;\n#ifndef TIF_RESTORE_SIGMASK\n\tunsigned\t\t\trestore_sigmask:1;\n#endif\n#ifdef CONFIG_MEMCG\n\tunsigned\t\t\tin_user_fault:1;\n#endif\n#ifdef CONFIG_COMPAT_BRK\n\tunsigned\t\t\tbrk_randomized:1;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/* disallow userland-initiated cgroup migration */\n\tunsigned\t\t\tno_cgroup_migration:1;\n\t/* task is frozen/stopped (used by the cgroup freezer) */\n\tunsigned\t\t\tfrozen:1;\n#endif\n#ifdef CONFIG_BLK_CGROUP\n\t/* to be used once the psi infrastructure lands upstream. */\n\tunsigned\t\t\tuse_memdelay:1;\n#endif\n\n\tunsigned long\t\t\tatomic_flags; /* Flags requiring atomic access. */\n\n\tstruct restart_block\t\trestart_block;\n\n\tpid_t\t\t\t\tpid;\n\tpid_t\t\t\t\ttgid;\n\n#ifdef CONFIG_STACKPROTECTOR\n\t/* Canary value for the -fstack-protector GCC feature: */\n\tunsigned long\t\t\tstack_canary;\n#endif\n\t/*\n\t * Pointers to the (original) parent process, youngest child, younger sibling,\n\t * older sibling, respectively.  (p->father can be replaced with\n\t * p->real_parent->pid)\n\t */\n\n\t/* Real parent process: */\n\tstruct task_struct __rcu\t*real_parent;\n\n\t/* Recipient of SIGCHLD, wait4() reports: */\n\tstruct task_struct __rcu\t*parent;\n\n\t/*\n\t * Children/sibling form the list of natural children:\n\t */\n\tstruct list_head\t\tchildren;\n\tstruct list_head\t\tsibling;\n\tstruct task_struct\t\t*group_leader;\n\n\t/*\n\t * 'ptraced' is the list of tasks this task is using ptrace() on.\n\t *\n\t * This includes both natural children and PTRACE_ATTACH targets.\n\t * 'ptrace_entry' is this task's link on the p->parent->ptraced list.\n\t */\n\tstruct list_head\t\tptraced;\n\tstruct list_head\t\tptrace_entry;\n\n\t/* PID/PID hash table linkage. */\n\tstruct pid\t\t\t*thread_pid;\n\tstruct hlist_node\t\tpid_links[PIDTYPE_MAX];\n\tstruct list_head\t\tthread_group;\n\tstruct list_head\t\tthread_node;\n\n\tstruct completion\t\t*vfork_done;\n\n\t/* CLONE_CHILD_SETTID: */\n\tint __user\t\t\t*set_child_tid;\n\n\t/* CLONE_CHILD_CLEARTID: */\n\tint __user\t\t\t*clear_child_tid;\n\n\tu64\t\t\t\tutime;\n\tu64\t\t\t\tstime;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tu64\t\t\t\tutimescaled;\n\tu64\t\t\t\tstimescaled;\n#endif\n\tu64\t\t\t\tgtime;\n\tstruct prev_cputime\t\tprev_cputime;\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tstruct vtime\t\t\tvtime;\n#endif\n\n#ifdef CONFIG_NO_HZ_FULL\n\tatomic_t\t\t\ttick_dep_mask;\n#endif\n\t/* Context switch counts: */\n\tunsigned long\t\t\tnvcsw;\n\tunsigned long\t\t\tnivcsw;\n\n\t/* Monotonic time in nsecs: */\n\tu64\t\t\t\tstart_time;\n\n\t/* Boot based time in nsecs: */\n\tu64\t\t\t\tstart_boottime;\n\n\t/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */\n\tunsigned long\t\t\tmin_flt;\n\tunsigned long\t\t\tmaj_flt;\n\n\t/* Empty if CONFIG_POSIX_CPUTIMERS=n */\n\tstruct posix_cputimers\t\tposix_cputimers;\n\n\t/* Process credentials: */\n\n\t/* Tracer's credentials at attach: */\n\tconst struct cred __rcu\t\t*ptracer_cred;\n\n\t/* Objective and real subjective task credentials (COW): */\n\tconst struct cred __rcu\t\t*real_cred;\n\n\t/* Effective (overridable) subjective task credentials (COW): */\n\tconst struct cred __rcu\t\t*cred;\n\n#ifdef CONFIG_KEYS\n\t/* Cached requested key. */\n\tstruct key\t\t\t*cached_requested_key;\n#endif\n\n\t/*\n\t * executable name, excluding path.\n\t *\n\t * - normally initialized setup_new_exec()\n\t * - access it with [gs]et_task_comm()\n\t * - lock it with task_lock()\n\t */\n\tchar\t\t\t\tcomm[TASK_COMM_LEN];\n\n\tstruct nameidata\t\t*nameidata;\n\n#ifdef CONFIG_SYSVIPC\n\tstruct sysv_sem\t\t\tsysvsem;\n\tstruct sysv_shm\t\t\tsysvshm;\n#endif\n#ifdef CONFIG_DETECT_HUNG_TASK\n\tunsigned long\t\t\tlast_switch_count;\n\tunsigned long\t\t\tlast_switch_time;\n#endif\n\t/* Filesystem information: */\n\tstruct fs_struct\t\t*fs;\n\n\t/* Open file information: */\n\tstruct files_struct\t\t*files;\n\n\t/* Namespaces: */\n\tstruct nsproxy\t\t\t*nsproxy;\n\n\t/* Signal handlers: */\n\tstruct signal_struct\t\t*signal;\n\tstruct sighand_struct __rcu\t\t*sighand;\n\tsigset_t\t\t\tblocked;\n\tsigset_t\t\t\treal_blocked;\n\t/* Restored if set_restore_sigmask() was used: */\n\tsigset_t\t\t\tsaved_sigmask;\n\tstruct sigpending\t\tpending;\n\tunsigned long\t\t\tsas_ss_sp;\n\tsize_t\t\t\t\tsas_ss_size;\n\tunsigned int\t\t\tsas_ss_flags;\n\n\tstruct callback_head\t\t*task_works;\n\n#ifdef CONFIG_AUDIT\n#ifdef CONFIG_AUDITSYSCALL\n\tstruct audit_context\t\t*audit_context;\n#endif\n\tkuid_t\t\t\t\tloginuid;\n\tunsigned int\t\t\tsessionid;\n#endif\n\tstruct seccomp\t\t\tseccomp;\n\n\t/* Thread group tracking: */\n\tu64\t\t\t\tparent_exec_id;\n\tu64\t\t\t\tself_exec_id;\n\n\t/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */\n\tspinlock_t\t\t\talloc_lock;\n\n\t/* Protection of the PI data structures: */\n\traw_spinlock_t\t\t\tpi_lock;\n\n\tstruct wake_q_node\t\twake_q;\n\n#ifdef CONFIG_RT_MUTEXES\n\t/* PI waiters blocked on a rt_mutex held by this task: */\n\tstruct rb_root_cached\t\tpi_waiters;\n\t/* Updated under owner's pi_lock and rq lock */\n\tstruct task_struct\t\t*pi_top_task;\n\t/* Deadlock detection and priority inheritance handling: */\n\tstruct rt_mutex_waiter\t\t*pi_blocked_on;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\t/* Mutex deadlock detection: */\n\tstruct mutex_waiter\t\t*blocked_on;\n#endif\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tint\t\t\t\tnon_block_count;\n#endif\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tunsigned int\t\t\tirq_events;\n\tunsigned long\t\t\thardirq_enable_ip;\n\tunsigned long\t\t\thardirq_disable_ip;\n\tunsigned int\t\t\thardirq_enable_event;\n\tunsigned int\t\t\thardirq_disable_event;\n\tint\t\t\t\thardirqs_enabled;\n\tint\t\t\t\thardirq_context;\n\tunsigned long\t\t\tsoftirq_disable_ip;\n\tunsigned long\t\t\tsoftirq_enable_ip;\n\tunsigned int\t\t\tsoftirq_disable_event;\n\tunsigned int\t\t\tsoftirq_enable_event;\n\tint\t\t\t\tsoftirqs_enabled;\n\tint\t\t\t\tsoftirq_context;\n#endif\n\n#ifdef CONFIG_LOCKDEP\n# define MAX_LOCK_DEPTH\t\t\t48UL\n\tu64\t\t\t\tcurr_chain_key;\n\tint\t\t\t\tlockdep_depth;\n\tunsigned int\t\t\tlockdep_recursion;\n\tstruct held_lock\t\theld_locks[MAX_LOCK_DEPTH];\n#endif\n\n#ifdef CONFIG_UBSAN\n\tunsigned int\t\t\tin_ubsan;\n#endif\n\n\t/* Journalling filesystem info: */\n\tvoid\t\t\t\t*journal_info;\n\n\t/* Stacked block device info: */\n\tstruct bio_list\t\t\t*bio_list;\n\n#ifdef CONFIG_BLOCK\n\t/* Stack plugging: */\n\tstruct blk_plug\t\t\t*plug;\n#endif\n\n\t/* VM state: */\n\tstruct reclaim_state\t\t*reclaim_state;\n\n\tstruct backing_dev_info\t\t*backing_dev_info;\n\n\tstruct io_context\t\t*io_context;\n\n#ifdef CONFIG_COMPACTION\n\tstruct capture_control\t\t*capture_control;\n#endif\n\t/* Ptrace state: */\n\tunsigned long\t\t\tptrace_message;\n\tkernel_siginfo_t\t\t*last_siginfo;\n\n\tstruct task_io_accounting\tioac;\n#ifdef CONFIG_PSI\n\t/* Pressure stall state */\n\tunsigned int\t\t\tpsi_flags;\n#endif\n#ifdef CONFIG_TASK_XACCT\n\t/* Accumulated RSS usage: */\n\tu64\t\t\t\tacct_rss_mem1;\n\t/* Accumulated virtual memory usage: */\n\tu64\t\t\t\tacct_vm_mem1;\n\t/* stime + utime since last update: */\n\tu64\t\t\t\tacct_timexpd;\n#endif\n#ifdef CONFIG_CPUSETS\n\t/* Protected by ->alloc_lock: */\n\tnodemask_t\t\t\tmems_allowed;\n\t/* Seqence number to catch updates: */\n\tseqcount_t\t\t\tmems_allowed_seq;\n\tint\t\t\t\tcpuset_mem_spread_rotor;\n\tint\t\t\t\tcpuset_slab_spread_rotor;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/* Control Group info protected by css_set_lock: */\n\tstruct css_set __rcu\t\t*cgroups;\n\t/* cg_list protected by css_set_lock and tsk->alloc_lock: */\n\tstruct list_head\t\tcg_list;\n#endif\n#ifdef CONFIG_X86_CPU_RESCTRL\n\tu32\t\t\t\tclosid;\n\tu32\t\t\t\trmid;\n#endif\n#ifdef CONFIG_FUTEX\n\tstruct robust_list_head __user\t*robust_list;\n#ifdef CONFIG_COMPAT\n\tstruct compat_robust_list_head __user *compat_robust_list;\n#endif\n\tstruct list_head\t\tpi_state_list;\n\tstruct futex_pi_state\t\t*pi_state_cache;\n\tstruct mutex\t\t\tfutex_exit_mutex;\n\tunsigned int\t\t\tfutex_state;\n#endif\n#ifdef CONFIG_PERF_EVENTS\n\tstruct perf_event_context\t*perf_event_ctxp[perf_nr_task_contexts];\n\tstruct mutex\t\t\tperf_event_mutex;\n\tstruct list_head\t\tperf_event_list;\n#endif\n#ifdef CONFIG_DEBUG_PREEMPT\n\tunsigned long\t\t\tpreempt_disable_ip;\n#endif\n#ifdef CONFIG_NUMA\n\t/* Protected by alloc_lock: */\n\tstruct mempolicy\t\t*mempolicy;\n\tshort\t\t\t\til_prev;\n\tshort\t\t\t\tpref_node_fork;\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\tint\t\t\t\tnuma_scan_seq;\n\tunsigned int\t\t\tnuma_scan_period;\n\tunsigned int\t\t\tnuma_scan_period_max;\n\tint\t\t\t\tnuma_preferred_nid;\n\tunsigned long\t\t\tnuma_migrate_retry;\n\t/* Migration stamp: */\n\tu64\t\t\t\tnode_stamp;\n\tu64\t\t\t\tlast_task_numa_placement;\n\tu64\t\t\t\tlast_sum_exec_runtime;\n\tstruct callback_head\t\tnuma_work;\n\n\t/*\n\t * This pointer is only modified for current in syscall and\n\t * pagefault context (and for tasks being destroyed), so it can be read\n\t * from any of the following contexts:\n\t *  - RCU read-side critical section\n\t *  - current->numa_group from everywhere\n\t *  - task's runqueue locked, task not running\n\t */\n\tstruct numa_group __rcu\t\t*numa_group;\n\n\t/*\n\t * numa_faults is an array split into four regions:\n\t * faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer\n\t * in this precise order.\n\t *\n\t * faults_memory: Exponential decaying average of faults on a per-node\n\t * basis. Scheduling placement decisions are made based on these\n\t * counts. The values remain static for the duration of a PTE scan.\n\t * faults_cpu: Track the nodes the process was running on when a NUMA\n\t * hinting fault was incurred.\n\t * faults_memory_buffer and faults_cpu_buffer: Record faults per node\n\t * during the current scan window. When the scan completes, the counts\n\t * in faults_memory and faults_cpu decay and these values are copied.\n\t */\n\tunsigned long\t\t\t*numa_faults;\n\tunsigned long\t\t\ttotal_numa_faults;\n\n\t/*\n\t * numa_faults_locality tracks if faults recorded during the last\n\t * scan window were remote/local or failed to migrate. The task scan\n\t * period is adapted based on the locality of the faults with different\n\t * weights depending on whether they were shared or private faults\n\t */\n\tunsigned long\t\t\tnuma_faults_locality[3];\n\n\tunsigned long\t\t\tnuma_pages_migrated;\n#endif /* CONFIG_NUMA_BALANCING */\n\n#ifdef CONFIG_RSEQ\n\tstruct rseq __user *rseq;\n\tu32 rseq_sig;\n\t/*\n\t * RmW on rseq_event_mask must be performed atomically\n\t * with respect to preemption.\n\t */\n\tunsigned long rseq_event_mask;\n#endif\n\n\tstruct tlbflush_unmap_batch\ttlb_ubc;\n\n\tunion {\n\t\trefcount_t\t\trcu_users;\n\t\tstruct rcu_head\t\trcu;\n\t};\n\n\t/* Cache last used pipe for splice(): */\n\tstruct pipe_inode_info\t\t*splice_pipe;\n\n\tstruct page_frag\t\ttask_frag;\n\n#ifdef CONFIG_TASK_DELAY_ACCT\n\tstruct task_delay_info\t\t*delays;\n#endif\n\n#ifdef CONFIG_FAULT_INJECTION\n\tint\t\t\t\tmake_it_fail;\n\tunsigned int\t\t\tfail_nth;\n#endif\n\t/*\n\t * When (nr_dirtied >= nr_dirtied_pause), it's time to call\n\t * balance_dirty_pages() for a dirty throttling pause:\n\t */\n\tint\t\t\t\tnr_dirtied;\n\tint\t\t\t\tnr_dirtied_pause;\n\t/* Start of a write-and-pause period: */\n\tunsigned long\t\t\tdirty_paused_when;\n\n#ifdef CONFIG_LATENCYTOP\n\tint\t\t\t\tlatency_record_count;\n\tstruct latency_record\t\tlatency_record[LT_SAVECOUNT];\n#endif\n\t/*\n\t * Time slack values; these are used to round up poll() and\n\t * select() etc timeout values. These are in nanoseconds.\n\t */\n\tu64\t\t\t\ttimer_slack_ns;\n\tu64\t\t\t\tdefault_timer_slack_ns;\n\n#ifdef CONFIG_KASAN\n\tunsigned int\t\t\tkasan_depth;\n#endif\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t/* Index of current stored address in ret_stack: */\n\tint\t\t\t\tcurr_ret_stack;\n\tint\t\t\t\tcurr_ret_depth;\n\n\t/* Stack of return addresses for return function tracing: */\n\tstruct ftrace_ret_stack\t\t*ret_stack;\n\n\t/* Timestamp for last schedule: */\n\tunsigned long long\t\tftrace_timestamp;\n\n\t/*\n\t * Number of functions that haven't been traced\n\t * because of depth overrun:\n\t */\n\tatomic_t\t\t\ttrace_overrun;\n\n\t/* Pause tracing: */\n\tatomic_t\t\t\ttracing_graph_pause;\n#endif\n\n#ifdef CONFIG_TRACING\n\t/* State flags for use by tracers: */\n\tunsigned long\t\t\ttrace;\n\n\t/* Bitmask and counter of trace recursion: */\n\tunsigned long\t\t\ttrace_recursion;\n#endif /* CONFIG_TRACING */\n\n#ifdef CONFIG_KCOV\n\t/* See kernel/kcov.c for more details. */\n\n\t/* Coverage collection mode enabled for this task (0 if disabled): */\n\tunsigned int\t\t\tkcov_mode;\n\n\t/* Size of the kcov_area: */\n\tunsigned int\t\t\tkcov_size;\n\n\t/* Buffer for coverage collection: */\n\tvoid\t\t\t\t*kcov_area;\n\n\t/* KCOV descriptor wired with this task or NULL: */\n\tstruct kcov\t\t\t*kcov;\n\n\t/* KCOV common handle for remote coverage collection: */\n\tu64\t\t\t\tkcov_handle;\n\n\t/* KCOV sequence number: */\n\tint\t\t\t\tkcov_sequence;\n#endif\n\n#ifdef CONFIG_MEMCG\n\tstruct mem_cgroup\t\t*memcg_in_oom;\n\tgfp_t\t\t\t\tmemcg_oom_gfp_mask;\n\tint\t\t\t\tmemcg_oom_order;\n\n\t/* Number of pages to reclaim on returning to userland: */\n\tunsigned int\t\t\tmemcg_nr_pages_over_high;\n\n\t/* Used by memcontrol for targeted memcg charge: */\n\tstruct mem_cgroup\t\t*active_memcg;\n#endif\n\n#ifdef CONFIG_BLK_CGROUP\n\tstruct request_queue\t\t*throttle_queue;\n#endif\n\n#ifdef CONFIG_UPROBES\n\tstruct uprobe_task\t\t*utask;\n#endif\n#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)\n\tunsigned int\t\t\tsequential_io;\n\tunsigned int\t\t\tsequential_io_avg;\n#endif\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tunsigned long\t\t\ttask_state_change;\n#endif\n\tint\t\t\t\tpagefault_disabled;\n#ifdef CONFIG_MMU\n\tstruct task_struct\t\t*oom_reaper_list;\n#endif\n#ifdef CONFIG_VMAP_STACK\n\tstruct vm_struct\t\t*stack_vm_area;\n#endif\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t/* A live task holds one reference: */\n\trefcount_t\t\t\tstack_refcount;\n#endif\n#ifdef CONFIG_LIVEPATCH\n\tint patch_state;\n#endif\n#ifdef CONFIG_SECURITY\n\t/* Used by LSM modules for access restriction: */\n\tvoid\t\t\t\t*security;\n#endif\n\n#ifdef CONFIG_GCC_PLUGIN_STACKLEAK\n\tunsigned long\t\t\tlowest_stack;\n\tunsigned long\t\t\tprev_lowest_stack;\n#endif\n\n\t/*\n\t * New fields for task_struct should be added above here, so that\n\t * they are included in the randomized portion of task_struct.\n\t */\n\trandomized_struct_fields_end\n\n\t/* CPU-specific state of this task: */\n\tstruct thread_struct\t\tthread;\n\n\t/*\n\t * WARNING: on x86, 'thread_struct' contains a variable-sized\n\t * structure.  It *MUST* be at the end of 'task_struct'.\n\t *\n\t * Do not put anything below here!\n\t */\n};\n\nstatic inline struct pid *task_pid(struct task_struct *task)\n{\n\treturn task->thread_pid;\n}\n\n/*\n * the helpers to get the task's different pids as they are seen\n * from various namespaces\n *\n * task_xid_nr()     : global id, i.e. the id seen from the init namespace;\n * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of\n *                     current.\n * task_xid_nr_ns()  : id seen from the ns specified;\n *\n * see also pid_nr() etc in include/linux/pid.h\n */\npid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);\n\nstatic inline pid_t task_pid_nr(struct task_struct *tsk)\n{\n\treturn tsk->pid;\n}\n\nstatic inline pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);\n}\n\nstatic inline pid_t task_pid_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);\n}\n\n\nstatic inline pid_t task_tgid_nr(struct task_struct *tsk)\n{\n\treturn tsk->tgid;\n}\n\n/**\n * pid_alive - check that a task structure is not stale\n * @p: Task structure to be checked.\n *\n * Test if a process is not yet dead (at most zombie state)\n * If pid_alive fails, then pointers within the task structure\n * can be stale and must not be dereferenced.\n *\n * Return: 1 if the process is alive. 0 otherwise.\n */\nstatic inline int pid_alive(const struct task_struct *p)\n{\n\treturn p->thread_pid != NULL;\n}\n\nstatic inline pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);\n}\n\nstatic inline pid_t task_pgrp_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);\n}\n\n\nstatic inline pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);\n}\n\nstatic inline pid_t task_session_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);\n}\n\nstatic inline pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_TGID, ns);\n}\n\nstatic inline pid_t task_tgid_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_TGID, NULL);\n}\n\nstatic inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)\n{\n\tpid_t pid = 0;\n\n\trcu_read_lock();\n\tif (pid_alive(tsk))\n\t\tpid = task_tgid_nr_ns(rcu_dereference(tsk->real_parent), ns);\n\trcu_read_unlock();\n\n\treturn pid;\n}\n\nstatic inline pid_t task_ppid_nr(const struct task_struct *tsk)\n{\n\treturn task_ppid_nr_ns(tsk, &init_pid_ns);\n}\n\n/* Obsolete, do not use: */\nstatic inline pid_t task_pgrp_nr(struct task_struct *tsk)\n{\n\treturn task_pgrp_nr_ns(tsk, &init_pid_ns);\n}\n\n#define TASK_REPORT_IDLE\t(TASK_REPORT + 1)\n#define TASK_REPORT_MAX\t\t(TASK_REPORT_IDLE << 1)\n\nstatic inline unsigned int task_state_index(struct task_struct *tsk)\n{\n\tunsigned int tsk_state = READ_ONCE(tsk->state);\n\tunsigned int state = (tsk_state | tsk->exit_state) & TASK_REPORT;\n\n\tBUILD_BUG_ON_NOT_POWER_OF_2(TASK_REPORT_MAX);\n\n\tif (tsk_state == TASK_IDLE)\n\t\tstate = TASK_REPORT_IDLE;\n\n\treturn fls(state);\n}\n\nstatic inline char task_index_to_char(unsigned int state)\n{\n\tstatic const char state_char[] = \"RSDTtXZPI\";\n\n\tBUILD_BUG_ON(1 + ilog2(TASK_REPORT_MAX) != sizeof(state_char) - 1);\n\n\treturn state_char[state];\n}\n\nstatic inline char task_state_to_char(struct task_struct *tsk)\n{\n\treturn task_index_to_char(task_state_index(tsk));\n}\n\n/**\n * is_global_init - check if a task structure is init. Since init\n * is free to have sub-threads we need to check tgid.\n * @tsk: Task structure to be checked.\n *\n * Check if a task structure is the first user space task the kernel created.\n *\n * Return: 1 if the task structure is init. 0 otherwise.\n */\nstatic inline int is_global_init(struct task_struct *tsk)\n{\n\treturn task_tgid_nr(tsk) == 1;\n}\n\nextern struct pid *cad_pid;\n\n/*\n * Per process flags\n */\n#define PF_IDLE\t\t\t0x00000002\t/* I am an IDLE thread */\n#define PF_EXITING\t\t0x00000004\t/* Getting shut down */\n#define PF_VCPU\t\t\t0x00000010\t/* I'm a virtual CPU */\n#define PF_WQ_WORKER\t\t0x00000020\t/* I'm a workqueue worker */\n#define PF_FORKNOEXEC\t\t0x00000040\t/* Forked but didn't exec */\n#define PF_MCE_PROCESS\t\t0x00000080      /* Process policy on mce errors */\n#define PF_SUPERPRIV\t\t0x00000100\t/* Used super-user privileges */\n#define PF_DUMPCORE\t\t0x00000200\t/* Dumped core */\n#define PF_SIGNALED\t\t0x00000400\t/* Killed by a signal */\n#define PF_MEMALLOC\t\t0x00000800\t/* Allocating memory */\n#define PF_NPROC_EXCEEDED\t0x00001000\t/* set_user() noticed that RLIMIT_NPROC was exceeded */\n#define PF_USED_MATH\t\t0x00002000\t/* If unset the fpu must be initialized before use */\n#define PF_USED_ASYNC\t\t0x00004000\t/* Used async_schedule*(), used by module init */\n#define PF_NOFREEZE\t\t0x00008000\t/* This thread should not be frozen */\n#define PF_FROZEN\t\t0x00010000\t/* Frozen for system suspend */\n#define PF_KSWAPD\t\t0x00020000\t/* I am kswapd */\n#define PF_MEMALLOC_NOFS\t0x00040000\t/* All allocation requests will inherit GFP_NOFS */\n#define PF_MEMALLOC_NOIO\t0x00080000\t/* All allocation requests will inherit GFP_NOIO */\n#define PF_LESS_THROTTLE\t0x00100000\t/* Throttle me less: I clean memory */\n#define PF_KTHREAD\t\t0x00200000\t/* I am a kernel thread */\n#define PF_RANDOMIZE\t\t0x00400000\t/* Randomize virtual address space */\n#define PF_SWAPWRITE\t\t0x00800000\t/* Allowed to write to swap */\n#define PF_MEMSTALL\t\t0x01000000\t/* Stalled due to lack of memory */\n#define PF_UMH\t\t\t0x02000000\t/* I'm an Usermodehelper process */\n#define PF_NO_SETAFFINITY\t0x04000000\t/* Userland is not allowed to meddle with cpus_mask */\n#define PF_MCE_EARLY\t\t0x08000000      /* Early kill for mce process policy */\n#define PF_MEMALLOC_NOCMA\t0x10000000\t/* All allocation request will have _GFP_MOVABLE cleared */\n#define PF_IO_WORKER\t\t0x20000000\t/* Task is an IO worker */\n#define PF_FREEZER_SKIP\t\t0x40000000\t/* Freezer should not count it as freezable */\n#define PF_SUSPEND_TASK\t\t0x80000000      /* This thread called freeze_processes() and should not be frozen */\n\n/*\n * Only the _current_ task can read/write to tsk->flags, but other\n * tasks can access tsk->flags in readonly mode for example\n * with tsk_used_math (like during threaded core dumping).\n * There is however an exception to this rule during ptrace\n * or during fork: the ptracer task is allowed to write to the\n * child->flags of its traced child (same goes for fork, the parent\n * can write to the child->flags), because we're guaranteed the\n * child is not running and in turn not changing child->flags\n * at the same time the parent does it.\n */\n#define clear_stopped_child_used_math(child)\tdo { (child)->flags &= ~PF_USED_MATH; } while (0)\n#define set_stopped_child_used_math(child)\tdo { (child)->flags |= PF_USED_MATH; } while (0)\n#define clear_used_math()\t\t\tclear_stopped_child_used_math(current)\n#define set_used_math()\t\t\t\tset_stopped_child_used_math(current)\n\n#define conditional_stopped_child_used_math(condition, child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)\n\n#define conditional_used_math(condition)\tconditional_stopped_child_used_math(condition, current)\n\n#define copy_to_stopped_child_used_math(child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)\n\n/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */\n#define tsk_used_math(p)\t\t\t((p)->flags & PF_USED_MATH)\n#define used_math()\t\t\t\ttsk_used_math(current)\n\nstatic inline bool is_percpu_thread(void)\n{\n#ifdef CONFIG_SMP\n\treturn (current->flags & PF_NO_SETAFFINITY) &&\n\t\t(current->nr_cpus_allowed  == 1);\n#else\n\treturn true;\n#endif\n}\n\n/* Per-process atomic flags. */\n#define PFA_NO_NEW_PRIVS\t\t0\t/* May not gain new privileges. */\n#define PFA_SPREAD_PAGE\t\t\t1\t/* Spread page cache over cpuset */\n#define PFA_SPREAD_SLAB\t\t\t2\t/* Spread some slab caches over cpuset */\n#define PFA_SPEC_SSB_DISABLE\t\t3\t/* Speculative Store Bypass disabled */\n#define PFA_SPEC_SSB_FORCE_DISABLE\t4\t/* Speculative Store Bypass force disabled*/\n#define PFA_SPEC_IB_DISABLE\t\t5\t/* Indirect branch speculation restricted */\n#define PFA_SPEC_IB_FORCE_DISABLE\t6\t/* Indirect branch speculation permanently restricted */\n#define PFA_SPEC_SSB_NOEXEC\t\t7\t/* Speculative Store Bypass clear on execve() */\n\n#define TASK_PFA_TEST(name, func)\t\t\t\t\t\\\n\tstatic inline bool task_##func(struct task_struct *p)\t\t\\\n\t{ return test_bit(PFA_##name, &p->atomic_flags); }\n\n#define TASK_PFA_SET(name, func)\t\t\t\t\t\\\n\tstatic inline void task_set_##func(struct task_struct *p)\t\\\n\t{ set_bit(PFA_##name, &p->atomic_flags); }\n\n#define TASK_PFA_CLEAR(name, func)\t\t\t\t\t\\\n\tstatic inline void task_clear_##func(struct task_struct *p)\t\\\n\t{ clear_bit(PFA_##name, &p->atomic_flags); }\n\nTASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)\nTASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)\n\nTASK_PFA_TEST(SPREAD_PAGE, spread_page)\nTASK_PFA_SET(SPREAD_PAGE, spread_page)\nTASK_PFA_CLEAR(SPREAD_PAGE, spread_page)\n\nTASK_PFA_TEST(SPREAD_SLAB, spread_slab)\nTASK_PFA_SET(SPREAD_SLAB, spread_slab)\nTASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)\n\nTASK_PFA_TEST(SPEC_SSB_DISABLE, spec_ssb_disable)\nTASK_PFA_SET(SPEC_SSB_DISABLE, spec_ssb_disable)\nTASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)\n\nTASK_PFA_TEST(SPEC_SSB_NOEXEC, spec_ssb_noexec)\nTASK_PFA_SET(SPEC_SSB_NOEXEC, spec_ssb_noexec)\nTASK_PFA_CLEAR(SPEC_SSB_NOEXEC, spec_ssb_noexec)\n\nTASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)\nTASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)\n\nTASK_PFA_TEST(SPEC_IB_DISABLE, spec_ib_disable)\nTASK_PFA_SET(SPEC_IB_DISABLE, spec_ib_disable)\nTASK_PFA_CLEAR(SPEC_IB_DISABLE, spec_ib_disable)\n\nTASK_PFA_TEST(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)\nTASK_PFA_SET(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)\n\nstatic inline void\ncurrent_restore_flags(unsigned long orig_flags, unsigned long flags)\n{\n\tcurrent->flags &= ~flags;\n\tcurrent->flags |= orig_flags & flags;\n}\n\nextern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);\nextern int task_can_attach(struct task_struct *p, const struct cpumask *cs_cpus_allowed);\n#ifdef CONFIG_SMP\nextern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);\nextern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);\n#else\nstatic inline void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)\n{\n}\nstatic inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tif (!cpumask_test_cpu(0, new_mask))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n#endif\n\nextern int yield_to(struct task_struct *p, bool preempt);\nextern void set_user_nice(struct task_struct *p, long nice);\nextern int task_prio(const struct task_struct *p);\n\n/**\n * task_nice - return the nice value of a given task.\n * @p: the task in question.\n *\n * Return: The nice value [ -20 ... 0 ... 19 ].\n */\nstatic inline int task_nice(const struct task_struct *p)\n{\n\treturn PRIO_TO_NICE((p)->static_prio);\n}\n\nextern int can_nice(const struct task_struct *p, const int nice);\nextern int task_curr(const struct task_struct *p);\nextern int idle_cpu(int cpu);\nextern int available_idle_cpu(int cpu);\nextern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);\nextern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);\nextern int sched_setattr(struct task_struct *, const struct sched_attr *);\nextern int sched_setattr_nocheck(struct task_struct *, const struct sched_attr *);\nextern struct task_struct *idle_task(int cpu);\n\n/**\n * is_idle_task - is the specified task an idle task?\n * @p: the task in question.\n *\n * Return: 1 if @p is an idle task. 0 otherwise.\n */\nstatic inline bool is_idle_task(const struct task_struct *p)\n{\n\treturn !!(p->flags & PF_IDLE);\n}\n\nextern struct task_struct *curr_task(int cpu);\nextern void ia64_set_curr_task(int cpu, struct task_struct *p);\n\nvoid yield(void);\n\nunion thread_union {\n#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK\n\tstruct task_struct task;\n#endif\n#ifndef CONFIG_THREAD_INFO_IN_TASK\n\tstruct thread_info thread_info;\n#endif\n\tunsigned long stack[THREAD_SIZE/sizeof(long)];\n};\n\n#ifndef CONFIG_THREAD_INFO_IN_TASK\nextern struct thread_info init_thread_info;\n#endif\n\nextern unsigned long init_stack[THREAD_SIZE / sizeof(unsigned long)];\n\n#ifdef CONFIG_THREAD_INFO_IN_TASK\nstatic inline struct thread_info *task_thread_info(struct task_struct *task)\n{\n\treturn &task->thread_info;\n}\n#elif !defined(__HAVE_THREAD_FUNCTIONS)\n# define task_thread_info(task)\t((struct thread_info *)(task)->stack)\n#endif\n\n/*\n * find a task by one of its numerical ids\n *\n * find_task_by_pid_ns():\n *      finds a task by its pid in the specified namespace\n * find_task_by_vpid():\n *      finds a task by its virtual pid\n *\n * see also find_vpid() etc in include/linux/pid.h\n */\n\nextern struct task_struct *find_task_by_vpid(pid_t nr);\nextern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);\n\n/*\n * find a task by its virtual pid and get the task struct\n */\nextern struct task_struct *find_get_task_by_vpid(pid_t nr);\n\nextern int wake_up_state(struct task_struct *tsk, unsigned int state);\nextern int wake_up_process(struct task_struct *tsk);\nextern void wake_up_new_task(struct task_struct *tsk);\n\n#ifdef CONFIG_SMP\nextern void kick_process(struct task_struct *tsk);\n#else\nstatic inline void kick_process(struct task_struct *tsk) { }\n#endif\n\nextern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);\n\nstatic inline void set_task_comm(struct task_struct *tsk, const char *from)\n{\n\t__set_task_comm(tsk, from, false);\n}\n\nextern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);\n#define get_task_comm(buf, tsk) ({\t\t\t\\\n\tBUILD_BUG_ON(sizeof(buf) != TASK_COMM_LEN);\t\\\n\t__get_task_comm(buf, sizeof(buf), tsk);\t\t\\\n})\n\n#ifdef CONFIG_SMP\nvoid scheduler_ipi(void);\nextern unsigned long wait_task_inactive(struct task_struct *, long match_state);\n#else\nstatic inline void scheduler_ipi(void) { }\nstatic inline unsigned long wait_task_inactive(struct task_struct *p, long match_state)\n{\n\treturn 1;\n}\n#endif\n\n/*\n * Set thread flags in other task's structures.\n * See asm/thread_info.h for TIF_xxxx flags available:\n */\nstatic inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tset_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tclear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void update_tsk_thread_flag(struct task_struct *tsk, int flag,\n\t\t\t\t\t  bool value)\n{\n\tupdate_ti_thread_flag(task_thread_info(tsk), flag, value);\n}\n\nstatic inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_set_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void set_tsk_need_resched(struct task_struct *tsk)\n{\n\tset_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline void clear_tsk_need_resched(struct task_struct *tsk)\n{\n\tclear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline int test_tsk_need_resched(struct task_struct *tsk)\n{\n\treturn unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));\n}\n\n/*\n * cond_resched() and cond_resched_lock(): latency reduction via\n * explicit rescheduling in places that are safe. The return\n * value indicates whether a reschedule was done in fact.\n * cond_resched_lock() will drop the spinlock before scheduling,\n */\n#ifndef CONFIG_PREEMPTION\nextern int _cond_resched(void);\n#else\nstatic inline int _cond_resched(void) { return 0; }\n#endif\n\n#define cond_resched() ({\t\t\t\\\n\t___might_sleep(__FILE__, __LINE__, 0);\t\\\n\t_cond_resched();\t\t\t\\\n})\n\nextern int __cond_resched_lock(spinlock_t *lock);\n\n#define cond_resched_lock(lock) ({\t\t\t\t\\\n\t___might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);\\\n\t__cond_resched_lock(lock);\t\t\t\t\\\n})\n\nstatic inline void cond_resched_rcu(void)\n{\n#if defined(CONFIG_DEBUG_ATOMIC_SLEEP) || !defined(CONFIG_PREEMPT_RCU)\n\trcu_read_unlock();\n\tcond_resched();\n\trcu_read_lock();\n#endif\n}\n\n/*\n * Does a critical section need to be broken due to another\n * task waiting?: (technically does not depend on CONFIG_PREEMPTION,\n * but a general need for low latency)\n */\nstatic inline int spin_needbreak(spinlock_t *lock)\n{\n#ifdef CONFIG_PREEMPTION\n\treturn spin_is_contended(lock);\n#else\n\treturn 0;\n#endif\n}\n\nstatic __always_inline bool need_resched(void)\n{\n\treturn unlikely(tif_need_resched());\n}\n\n/*\n * Wrappers for p->thread_info->cpu access. No-op on UP.\n */\n#ifdef CONFIG_SMP\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\treturn READ_ONCE(p->cpu);\n#else\n\treturn READ_ONCE(task_thread_info(p)->cpu);\n#endif\n}\n\nextern void set_task_cpu(struct task_struct *p, unsigned int cpu);\n\n#else\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n\treturn 0;\n}\n\nstatic inline void set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n}\n\n#endif /* CONFIG_SMP */\n\n/*\n * In order to reduce various lock holder preemption latencies provide an\n * interface to see if a vCPU is currently running or not.\n *\n * This allows us to terminate optimistic spin loops and block, analogous to\n * the native optimistic spin heuristic of testing if the lock owner task is\n * running or not.\n */\n#ifndef vcpu_is_preempted\nstatic inline bool vcpu_is_preempted(int cpu)\n{\n\treturn false;\n}\n#endif\n\nextern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);\nextern long sched_getaffinity(pid_t pid, struct cpumask *mask);\n\n#ifndef TASK_SIZE_OF\n#define TASK_SIZE_OF(tsk)\tTASK_SIZE\n#endif\n\n#ifdef CONFIG_RSEQ\n\n/*\n * Map the event mask on the user-space ABI enum rseq_cs_flags\n * for direct mask checks.\n */\nenum rseq_event_mask_bits {\n\tRSEQ_EVENT_PREEMPT_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT,\n\tRSEQ_EVENT_SIGNAL_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT,\n\tRSEQ_EVENT_MIGRATE_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT,\n};\n\nenum rseq_event_mask {\n\tRSEQ_EVENT_PREEMPT\t= (1U << RSEQ_EVENT_PREEMPT_BIT),\n\tRSEQ_EVENT_SIGNAL\t= (1U << RSEQ_EVENT_SIGNAL_BIT),\n\tRSEQ_EVENT_MIGRATE\t= (1U << RSEQ_EVENT_MIGRATE_BIT),\n};\n\nstatic inline void rseq_set_notify_resume(struct task_struct *t)\n{\n\tif (t->rseq)\n\t\tset_tsk_thread_flag(t, TIF_NOTIFY_RESUME);\n}\n\nvoid __rseq_handle_notify_resume(struct ksignal *sig, struct pt_regs *regs);\n\nstatic inline void rseq_handle_notify_resume(struct ksignal *ksig,\n\t\t\t\t\t     struct pt_regs *regs)\n{\n\tif (current->rseq)\n\t\t__rseq_handle_notify_resume(ksig, regs);\n}\n\nstatic inline void rseq_signal_deliver(struct ksignal *ksig,\n\t\t\t\t       struct pt_regs *regs)\n{\n\tpreempt_disable();\n\t__set_bit(RSEQ_EVENT_SIGNAL_BIT, &current->rseq_event_mask);\n\tpreempt_enable();\n\trseq_handle_notify_resume(ksig, regs);\n}\n\n/* rseq_preempt() requires preemption to be disabled. */\nstatic inline void rseq_preempt(struct task_struct *t)\n{\n\t__set_bit(RSEQ_EVENT_PREEMPT_BIT, &t->rseq_event_mask);\n\trseq_set_notify_resume(t);\n}\n\n/* rseq_migrate() requires preemption to be disabled. */\nstatic inline void rseq_migrate(struct task_struct *t)\n{\n\t__set_bit(RSEQ_EVENT_MIGRATE_BIT, &t->rseq_event_mask);\n\trseq_set_notify_resume(t);\n}\n\n/*\n * If parent process has a registered restartable sequences area, the\n * child inherits. Unregister rseq for a clone with CLONE_VM set.\n */\nstatic inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)\n{\n\tif (clone_flags & CLONE_VM) {\n\t\tt->rseq = NULL;\n\t\tt->rseq_sig = 0;\n\t\tt->rseq_event_mask = 0;\n\t} else {\n\t\tt->rseq = current->rseq;\n\t\tt->rseq_sig = current->rseq_sig;\n\t\tt->rseq_event_mask = current->rseq_event_mask;\n\t}\n}\n\nstatic inline void rseq_execve(struct task_struct *t)\n{\n\tt->rseq = NULL;\n\tt->rseq_sig = 0;\n\tt->rseq_event_mask = 0;\n}\n\n#else\n\nstatic inline void rseq_set_notify_resume(struct task_struct *t)\n{\n}\nstatic inline void rseq_handle_notify_resume(struct ksignal *ksig,\n\t\t\t\t\t     struct pt_regs *regs)\n{\n}\nstatic inline void rseq_signal_deliver(struct ksignal *ksig,\n\t\t\t\t       struct pt_regs *regs)\n{\n}\nstatic inline void rseq_preempt(struct task_struct *t)\n{\n}\nstatic inline void rseq_migrate(struct task_struct *t)\n{\n}\nstatic inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)\n{\n}\nstatic inline void rseq_execve(struct task_struct *t)\n{\n}\n\n#endif\n\nvoid __exit_umh(struct task_struct *tsk);\n\nstatic inline void exit_umh(struct task_struct *tsk)\n{\n\tif (unlikely(tsk->flags & PF_UMH))\n\t\t__exit_umh(tsk);\n}\n\n#ifdef CONFIG_DEBUG_RSEQ\n\nvoid rseq_syscall(struct pt_regs *regs);\n\n#else\n\nstatic inline void rseq_syscall(struct pt_regs *regs)\n{\n}\n\n#endif\n\nconst struct sched_avg *sched_trace_cfs_rq_avg(struct cfs_rq *cfs_rq);\nchar *sched_trace_cfs_rq_path(struct cfs_rq *cfs_rq, char *str, int len);\nint sched_trace_cfs_rq_cpu(struct cfs_rq *cfs_rq);\n\nconst struct sched_avg *sched_trace_rq_avg_rt(struct rq *rq);\nconst struct sched_avg *sched_trace_rq_avg_dl(struct rq *rq);\nconst struct sched_avg *sched_trace_rq_avg_irq(struct rq *rq);\n\nint sched_trace_rq_cpu(struct rq *rq);\n\nconst struct cpumask *sched_trace_rd_span(struct root_domain *rd);\n\n#endif\n", "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/kernel/signal.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson\n *\n *  2003-06-02  Jim Houston - Concurrent Computer Corp.\n *\t\tChanges to use preallocated sigqueue structures\n *\t\tto allow signals to be sent reliably.\n */\n\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/user.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/proc_fs.h>\n#include <linux/tty.h>\n#include <linux/binfmts.h>\n#include <linux/coredump.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/ptrace.h>\n#include <linux/signal.h>\n#include <linux/signalfd.h>\n#include <linux/ratelimit.h>\n#include <linux/tracehook.h>\n#include <linux/capability.h>\n#include <linux/freezer.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/user_namespace.h>\n#include <linux/uprobes.h>\n#include <linux/compat.h>\n#include <linux/cn_proc.h>\n#include <linux/compiler.h>\n#include <linux/posix-timers.h>\n#include <linux/livepatch.h>\n#include <linux/cgroup.h>\n#include <linux/audit.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/signal.h>\n\n#include <asm/param.h>\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/siginfo.h>\n#include <asm/cacheflush.h>\n\n/*\n * SLAB caches for signal bits.\n */\n\nstatic struct kmem_cache *sigqueue_cachep;\n\nint print_fatal_signals __read_mostly;\n\nstatic void __user *sig_handler(struct task_struct *t, int sig)\n{\n\treturn t->sighand->action[sig - 1].sa.sa_handler;\n}\n\nstatic inline bool sig_handler_ignored(void __user *handler, int sig)\n{\n\t/* Is it explicitly or implicitly ignored? */\n\treturn handler == SIG_IGN ||\n\t       (handler == SIG_DFL && sig_kernel_ignore(sig));\n}\n\nstatic bool sig_task_ignored(struct task_struct *t, int sig, bool force)\n{\n\tvoid __user *handler;\n\n\thandler = sig_handler(t, sig);\n\n\t/* SIGKILL and SIGSTOP may not be sent to the global init */\n\tif (unlikely(is_global_init(t) && sig_kernel_only(sig)))\n\t\treturn true;\n\n\tif (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&\n\t    handler == SIG_DFL && !(force && sig_kernel_only(sig)))\n\t\treturn true;\n\n\t/* Only allow kernel generated signals to this kthread */\n\tif (unlikely((t->flags & PF_KTHREAD) &&\n\t\t     (handler == SIG_KTHREAD_KERNEL) && !force))\n\t\treturn true;\n\n\treturn sig_handler_ignored(handler, sig);\n}\n\nstatic bool sig_ignored(struct task_struct *t, int sig, bool force)\n{\n\t/*\n\t * Blocked signals are never ignored, since the\n\t * signal handler may change by the time it is\n\t * unblocked.\n\t */\n\tif (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))\n\t\treturn false;\n\n\t/*\n\t * Tracers may want to know about even ignored signal unless it\n\t * is SIGKILL which can't be reported anyway but can be ignored\n\t * by SIGNAL_UNKILLABLE task.\n\t */\n\tif (t->ptrace && sig != SIGKILL)\n\t\treturn false;\n\n\treturn sig_task_ignored(t, sig, force);\n}\n\n/*\n * Re-calculate pending state from the set of locally pending\n * signals, globally pending signals, and blocked signals.\n */\nstatic inline bool has_pending_signals(sigset_t *signal, sigset_t *blocked)\n{\n\tunsigned long ready;\n\tlong i;\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = _NSIG_WORDS, ready = 0; --i >= 0 ;)\n\t\t\tready |= signal->sig[i] &~ blocked->sig[i];\n\t\tbreak;\n\n\tcase 4: ready  = signal->sig[3] &~ blocked->sig[3];\n\t\tready |= signal->sig[2] &~ blocked->sig[2];\n\t\tready |= signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 2: ready  = signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 1: ready  = signal->sig[0] &~ blocked->sig[0];\n\t}\n\treturn ready !=\t0;\n}\n\n#define PENDING(p,b) has_pending_signals(&(p)->signal, (b))\n\nstatic bool recalc_sigpending_tsk(struct task_struct *t)\n{\n\tif ((t->jobctl & (JOBCTL_PENDING_MASK | JOBCTL_TRAP_FREEZE)) ||\n\t    PENDING(&t->pending, &t->blocked) ||\n\t    PENDING(&t->signal->shared_pending, &t->blocked) ||\n\t    cgroup_task_frozen(t)) {\n\t\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t\treturn true;\n\t}\n\n\t/*\n\t * We must never clear the flag in another thread, or in current\n\t * when it's possible the current syscall is returning -ERESTART*.\n\t * So we don't clear it here, and only callers who know they should do.\n\t */\n\treturn false;\n}\n\n/*\n * After recalculating TIF_SIGPENDING, we need to make sure the task wakes up.\n * This is superfluous when called on current, the wakeup is a harmless no-op.\n */\nvoid recalc_sigpending_and_wake(struct task_struct *t)\n{\n\tif (recalc_sigpending_tsk(t))\n\t\tsignal_wake_up(t, 0);\n}\n\nvoid recalc_sigpending(void)\n{\n\tif (!recalc_sigpending_tsk(current) && !freezing(current) &&\n\t    !klp_patch_pending(current))\n\t\tclear_thread_flag(TIF_SIGPENDING);\n\n}\nEXPORT_SYMBOL(recalc_sigpending);\n\nvoid calculate_sigpending(void)\n{\n\t/* Have any signals or users of TIF_SIGPENDING been delayed\n\t * until after fork?\n\t */\n\tspin_lock_irq(&current->sighand->siglock);\n\tset_tsk_thread_flag(current, TIF_SIGPENDING);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n}\n\n/* Given the mask, find the first available signal that should be serviced. */\n\n#define SYNCHRONOUS_MASK \\\n\t(sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \\\n\t sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))\n\nint next_signal(struct sigpending *pending, sigset_t *mask)\n{\n\tunsigned long i, *s, *m, x;\n\tint sig = 0;\n\n\ts = pending->signal.sig;\n\tm = mask->sig;\n\n\t/*\n\t * Handle the first word specially: it contains the\n\t * synchronous signals that need to be dequeued first.\n\t */\n\tx = *s &~ *m;\n\tif (x) {\n\t\tif (x & SYNCHRONOUS_MASK)\n\t\t\tx &= SYNCHRONOUS_MASK;\n\t\tsig = ffz(~x) + 1;\n\t\treturn sig;\n\t}\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = 1; i < _NSIG_WORDS; ++i) {\n\t\t\tx = *++s &~ *++m;\n\t\t\tif (!x)\n\t\t\t\tcontinue;\n\t\t\tsig = ffz(~x) + i*_NSIG_BPW + 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 2:\n\t\tx = s[1] &~ m[1];\n\t\tif (!x)\n\t\t\tbreak;\n\t\tsig = ffz(~x) + _NSIG_BPW + 1;\n\t\tbreak;\n\n\tcase 1:\n\t\t/* Nothing to do */\n\t\tbreak;\n\t}\n\n\treturn sig;\n}\n\nstatic inline void print_dropped_signal(int sig)\n{\n\tstatic DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);\n\n\tif (!print_fatal_signals)\n\t\treturn;\n\n\tif (!__ratelimit(&ratelimit_state))\n\t\treturn;\n\n\tpr_info(\"%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\\n\",\n\t\t\t\tcurrent->comm, current->pid, sig);\n}\n\n/**\n * task_set_jobctl_pending - set jobctl pending bits\n * @task: target task\n * @mask: pending bits to set\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK | %JOBCTL_STOP_CONSUME | %JOBCTL_STOP_SIGMASK |\n * %JOBCTL_TRAPPING.  If stop signo is being set, the existing signo is\n * cleared.  If @task is already being killed or exiting, this function\n * becomes noop.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if @mask is set, %false if made noop because @task was dying.\n */\nbool task_set_jobctl_pending(struct task_struct *task, unsigned long mask)\n{\n\tBUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |\n\t\t\tJOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));\n\tBUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));\n\n\tif (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))\n\t\treturn false;\n\n\tif (mask & JOBCTL_STOP_SIGMASK)\n\t\ttask->jobctl &= ~JOBCTL_STOP_SIGMASK;\n\n\ttask->jobctl |= mask;\n\treturn true;\n}\n\n/**\n * task_clear_jobctl_trapping - clear jobctl trapping bit\n * @task: target task\n *\n * If JOBCTL_TRAPPING is set, a ptracer is waiting for us to enter TRACED.\n * Clear it and wake up the ptracer.  Note that we don't need any further\n * locking.  @task->siglock guarantees that @task->parent points to the\n * ptracer.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_trapping(struct task_struct *task)\n{\n\tif (unlikely(task->jobctl & JOBCTL_TRAPPING)) {\n\t\ttask->jobctl &= ~JOBCTL_TRAPPING;\n\t\tsmp_mb();\t/* advised by wake_up_bit() */\n\t\twake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);\n\t}\n}\n\n/**\n * task_clear_jobctl_pending - clear jobctl pending bits\n * @task: target task\n * @mask: pending bits to clear\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK.  If %JOBCTL_STOP_PENDING is being cleared, other\n * STOP bits are cleared together.\n *\n * If clearing of @mask leaves no stop or trap pending, this function calls\n * task_clear_jobctl_trapping().\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_pending(struct task_struct *task, unsigned long mask)\n{\n\tBUG_ON(mask & ~JOBCTL_PENDING_MASK);\n\n\tif (mask & JOBCTL_STOP_PENDING)\n\t\tmask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;\n\n\ttask->jobctl &= ~mask;\n\n\tif (!(task->jobctl & JOBCTL_PENDING_MASK))\n\t\ttask_clear_jobctl_trapping(task);\n}\n\n/**\n * task_participate_group_stop - participate in a group stop\n * @task: task participating in a group stop\n *\n * @task has %JOBCTL_STOP_PENDING set and is participating in a group stop.\n * Group stop states are cleared and the group stop count is consumed if\n * %JOBCTL_STOP_CONSUME was set.  If the consumption completes the group\n * stop, the appropriate `SIGNAL_*` flags are set.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if group stop completion should be notified to the parent, %false\n * otherwise.\n */\nstatic bool task_participate_group_stop(struct task_struct *task)\n{\n\tstruct signal_struct *sig = task->signal;\n\tbool consume = task->jobctl & JOBCTL_STOP_CONSUME;\n\n\tWARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));\n\n\ttask_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);\n\n\tif (!consume)\n\t\treturn false;\n\n\tif (!WARN_ON_ONCE(sig->group_stop_count == 0))\n\t\tsig->group_stop_count--;\n\n\t/*\n\t * Tell the caller to notify completion iff we are entering into a\n\t * fresh group stop.  Read comment in do_signal_stop() for details.\n\t */\n\tif (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {\n\t\tsignal_set_stop_flags(sig, SIGNAL_STOP_STOPPED);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nvoid task_join_group_stop(struct task_struct *task)\n{\n\t/* Have the new thread join an on-going signal group stop */\n\tunsigned long jobctl = current->jobctl;\n\tif (jobctl & JOBCTL_STOP_PENDING) {\n\t\tstruct signal_struct *sig = current->signal;\n\t\tunsigned long signr = jobctl & JOBCTL_STOP_SIGMASK;\n\t\tunsigned long gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\n\t\tif (task_set_jobctl_pending(task, signr | gstop)) {\n\t\t\tsig->group_stop_count++;\n\t\t}\n\t}\n}\n\n/*\n * allocate a new signal queue record\n * - this may be called without locks if and only if t == current, otherwise an\n *   appropriate lock must be held to stop the target task from exiting\n */\nstatic struct sigqueue *\n__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)\n{\n\tstruct sigqueue *q = NULL;\n\tstruct user_struct *user;\n\tint sigpending;\n\n\t/*\n\t * Protect access to @t credentials. This can go away when all\n\t * callers hold rcu read lock.\n\t *\n\t * NOTE! A pending signal will hold on to the user refcount,\n\t * and we get/put the refcount only when the sigpending count\n\t * changes from/to zero.\n\t */\n\trcu_read_lock();\n\tuser = __task_cred(t)->user;\n\tsigpending = atomic_inc_return(&user->sigpending);\n\tif (sigpending == 1)\n\t\tget_uid(user);\n\trcu_read_unlock();\n\n\tif (override_rlimit || likely(sigpending <= task_rlimit(t, RLIMIT_SIGPENDING))) {\n\t\tq = kmem_cache_alloc(sigqueue_cachep, flags);\n\t} else {\n\t\tprint_dropped_signal(sig);\n\t}\n\n\tif (unlikely(q == NULL)) {\n\t\tif (atomic_dec_and_test(&user->sigpending))\n\t\t\tfree_uid(user);\n\t} else {\n\t\tINIT_LIST_HEAD(&q->list);\n\t\tq->flags = 0;\n\t\tq->user = user;\n\t}\n\n\treturn q;\n}\n\nstatic void __sigqueue_free(struct sigqueue *q)\n{\n\tif (q->flags & SIGQUEUE_PREALLOC)\n\t\treturn;\n\tif (atomic_dec_and_test(&q->user->sigpending))\n\t\tfree_uid(q->user);\n\tkmem_cache_free(sigqueue_cachep, q);\n}\n\nvoid flush_sigqueue(struct sigpending *queue)\n{\n\tstruct sigqueue *q;\n\n\tsigemptyset(&queue->signal);\n\twhile (!list_empty(&queue->list)) {\n\t\tq = list_entry(queue->list.next, struct sigqueue , list);\n\t\tlist_del_init(&q->list);\n\t\t__sigqueue_free(q);\n\t}\n}\n\n/*\n * Flush all pending signals for this kthread.\n */\nvoid flush_signals(struct task_struct *t)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\tclear_tsk_thread_flag(t, TIF_SIGPENDING);\n\tflush_sigqueue(&t->pending);\n\tflush_sigqueue(&t->signal->shared_pending);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n}\nEXPORT_SYMBOL(flush_signals);\n\n#ifdef CONFIG_POSIX_TIMERS\nstatic void __flush_itimer_signals(struct sigpending *pending)\n{\n\tsigset_t signal, retain;\n\tstruct sigqueue *q, *n;\n\n\tsignal = pending->signal;\n\tsigemptyset(&retain);\n\n\tlist_for_each_entry_safe(q, n, &pending->list, list) {\n\t\tint sig = q->info.si_signo;\n\n\t\tif (likely(q->info.si_code != SI_TIMER)) {\n\t\t\tsigaddset(&retain, sig);\n\t\t} else {\n\t\t\tsigdelset(&signal, sig);\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\n\tsigorsets(&pending->signal, &signal, &retain);\n}\n\nvoid flush_itimer_signals(void)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\n\t__flush_itimer_signals(&tsk->pending);\n\t__flush_itimer_signals(&tsk->signal->shared_pending);\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n}\n#endif\n\nvoid ignore_signals(struct task_struct *t)\n{\n\tint i;\n\n\tfor (i = 0; i < _NSIG; ++i)\n\t\tt->sighand->action[i].sa.sa_handler = SIG_IGN;\n\n\tflush_signals(t);\n}\n\n/*\n * Flush all handlers for a task.\n */\n\nvoid\nflush_signal_handlers(struct task_struct *t, int force_default)\n{\n\tint i;\n\tstruct k_sigaction *ka = &t->sighand->action[0];\n\tfor (i = _NSIG ; i != 0 ; i--) {\n\t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n\t\t\tka->sa.sa_handler = SIG_DFL;\n\t\tka->sa.sa_flags = 0;\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tka->sa.sa_restorer = NULL;\n#endif\n\t\tsigemptyset(&ka->sa.sa_mask);\n\t\tka++;\n\t}\n}\n\nbool unhandled_signal(struct task_struct *tsk, int sig)\n{\n\tvoid __user *handler = tsk->sighand->action[sig-1].sa.sa_handler;\n\tif (is_global_init(tsk))\n\t\treturn true;\n\n\tif (handler != SIG_IGN && handler != SIG_DFL)\n\t\treturn false;\n\n\t/* if ptraced, let the tracer determine */\n\treturn !tsk->ptrace;\n}\n\nstatic void collect_signal(int sig, struct sigpending *list, kernel_siginfo_t *info,\n\t\t\t   bool *resched_timer)\n{\n\tstruct sigqueue *q, *first = NULL;\n\n\t/*\n\t * Collect the siginfo appropriate to this signal.  Check if\n\t * there is another siginfo for the same signal.\n\t*/\n\tlist_for_each_entry(q, &list->list, list) {\n\t\tif (q->info.si_signo == sig) {\n\t\t\tif (first)\n\t\t\t\tgoto still_pending;\n\t\t\tfirst = q;\n\t\t}\n\t}\n\n\tsigdelset(&list->signal, sig);\n\n\tif (first) {\nstill_pending:\n\t\tlist_del_init(&first->list);\n\t\tcopy_siginfo(info, &first->info);\n\n\t\t*resched_timer =\n\t\t\t(first->flags & SIGQUEUE_PREALLOC) &&\n\t\t\t(info->si_code == SI_TIMER) &&\n\t\t\t(info->si_sys_private);\n\n\t\t__sigqueue_free(first);\n\t} else {\n\t\t/*\n\t\t * Ok, it wasn't in the queue.  This must be\n\t\t * a fast-pathed signal or we must have been\n\t\t * out of queue space.  So zero out the info.\n\t\t */\n\t\tclear_siginfo(info);\n\t\tinfo->si_signo = sig;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\tinfo->si_pid = 0;\n\t\tinfo->si_uid = 0;\n\t}\n}\n\nstatic int __dequeue_signal(struct sigpending *pending, sigset_t *mask,\n\t\t\tkernel_siginfo_t *info, bool *resched_timer)\n{\n\tint sig = next_signal(pending, mask);\n\n\tif (sig)\n\t\tcollect_signal(sig, pending, info, resched_timer);\n\treturn sig;\n}\n\n/*\n * Dequeue a signal and return the element to the caller, which is\n * expected to free it.\n *\n * All callers have to hold the siglock.\n */\nint dequeue_signal(struct task_struct *tsk, sigset_t *mask, kernel_siginfo_t *info)\n{\n\tbool resched_timer = false;\n\tint signr;\n\n\t/* We only dequeue private signals from ourselves, we don't let\n\t * signalfd steal them\n\t */\n\tsignr = __dequeue_signal(&tsk->pending, mask, info, &resched_timer);\n\tif (!signr) {\n\t\tsignr = __dequeue_signal(&tsk->signal->shared_pending,\n\t\t\t\t\t mask, info, &resched_timer);\n#ifdef CONFIG_POSIX_TIMERS\n\t\t/*\n\t\t * itimer signal ?\n\t\t *\n\t\t * itimers are process shared and we restart periodic\n\t\t * itimers in the signal delivery path to prevent DoS\n\t\t * attacks in the high resolution timer case. This is\n\t\t * compliant with the old way of self-restarting\n\t\t * itimers, as the SIGALRM is a legacy signal and only\n\t\t * queued once. Changing the restart behaviour to\n\t\t * restart the timer in the signal dequeue path is\n\t\t * reducing the timer noise on heavy loaded !highres\n\t\t * systems too.\n\t\t */\n\t\tif (unlikely(signr == SIGALRM)) {\n\t\t\tstruct hrtimer *tmr = &tsk->signal->real_timer;\n\n\t\t\tif (!hrtimer_is_queued(tmr) &&\n\t\t\t    tsk->signal->it_real_incr != 0) {\n\t\t\t\thrtimer_forward(tmr, tmr->base->get_time(),\n\t\t\t\t\t\ttsk->signal->it_real_incr);\n\t\t\t\thrtimer_restart(tmr);\n\t\t\t}\n\t\t}\n#endif\n\t}\n\n\trecalc_sigpending();\n\tif (!signr)\n\t\treturn 0;\n\n\tif (unlikely(sig_kernel_stop(signr))) {\n\t\t/*\n\t\t * Set a marker that we have dequeued a stop signal.  Our\n\t\t * caller might release the siglock and then the pending\n\t\t * stop signal it is about to process is no longer in the\n\t\t * pending bitmasks, but must still be cleared by a SIGCONT\n\t\t * (and overruled by a SIGKILL).  So those cases clear this\n\t\t * shared flag after we've set it.  Note that this flag may\n\t\t * remain set after the signal we return is ignored or\n\t\t * handled.  That doesn't matter because its only purpose\n\t\t * is to alert stop-signal processing code when another\n\t\t * processor has come along and cleared the flag.\n\t\t */\n\t\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\t}\n#ifdef CONFIG_POSIX_TIMERS\n\tif (resched_timer) {\n\t\t/*\n\t\t * Release the siglock to ensure proper locking order\n\t\t * of timer locks outside of siglocks.  Note, we leave\n\t\t * irqs disabled here, since the posix-timers code is\n\t\t * about to disable them again anyway.\n\t\t */\n\t\tspin_unlock(&tsk->sighand->siglock);\n\t\tposixtimer_rearm(info);\n\t\tspin_lock(&tsk->sighand->siglock);\n\n\t\t/* Don't expose the si_sys_private value to userspace */\n\t\tinfo->si_sys_private = 0;\n\t}\n#endif\n\treturn signr;\n}\nEXPORT_SYMBOL_GPL(dequeue_signal);\n\nstatic int dequeue_synchronous_signal(kernel_siginfo_t *info)\n{\n\tstruct task_struct *tsk = current;\n\tstruct sigpending *pending = &tsk->pending;\n\tstruct sigqueue *q, *sync = NULL;\n\n\t/*\n\t * Might a synchronous signal be in the queue?\n\t */\n\tif (!((pending->signal.sig[0] & ~tsk->blocked.sig[0]) & SYNCHRONOUS_MASK))\n\t\treturn 0;\n\n\t/*\n\t * Return the first synchronous signal in the queue.\n\t */\n\tlist_for_each_entry(q, &pending->list, list) {\n\t\t/* Synchronous signals have a postive si_code */\n\t\tif ((q->info.si_code > SI_USER) &&\n\t\t    (sigmask(q->info.si_signo) & SYNCHRONOUS_MASK)) {\n\t\t\tsync = q;\n\t\t\tgoto next;\n\t\t}\n\t}\n\treturn 0;\nnext:\n\t/*\n\t * Check if there is another siginfo for the same signal.\n\t */\n\tlist_for_each_entry_continue(q, &pending->list, list) {\n\t\tif (q->info.si_signo == sync->info.si_signo)\n\t\t\tgoto still_pending;\n\t}\n\n\tsigdelset(&pending->signal, sync->info.si_signo);\n\trecalc_sigpending();\nstill_pending:\n\tlist_del_init(&sync->list);\n\tcopy_siginfo(info, &sync->info);\n\t__sigqueue_free(sync);\n\treturn info->si_signo;\n}\n\n/*\n * Tell a process that it has a new active signal..\n *\n * NOTE! we rely on the previous spin_lock to\n * lock interrupts for us! We can only be called with\n * \"siglock\" held, and the local interrupt must\n * have been disabled when that got acquired!\n *\n * No need to set need_resched since signal event passing\n * goes through ->blocked\n */\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}\n\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n */\nstatic void flush_sigqueue_mask(sigset_t *mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\tsigset_t m;\n\n\tsigandsets(&m, mask, &s->signal);\n\tif (sigisemptyset(&m))\n\t\treturn;\n\n\tsigandnsets(&s->signal, &s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (sigismember(mask, q->info.si_signo)) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n}\n\nstatic inline int is_si_special(const struct kernel_siginfo *info)\n{\n\treturn info <= SEND_SIG_PRIV;\n}\n\nstatic inline bool si_fromuser(const struct kernel_siginfo *info)\n{\n\treturn info == SEND_SIG_NOINFO ||\n\t\t(!is_si_special(info) && SI_FROMUSER(info));\n}\n\n/*\n * called with RCU read lock from check_kill_permission()\n */\nstatic bool kill_ok_by_cred(struct task_struct *t)\n{\n\tconst struct cred *cred = current_cred();\n\tconst struct cred *tcred = __task_cred(t);\n\n\treturn uid_eq(cred->euid, tcred->suid) ||\n\t       uid_eq(cred->euid, tcred->uid) ||\n\t       uid_eq(cred->uid, tcred->suid) ||\n\t       uid_eq(cred->uid, tcred->uid) ||\n\t       ns_capable(tcred->user_ns, CAP_KILL);\n}\n\n/*\n * Bad permissions for sending the signal\n * - the caller must hold the RCU read lock\n */\nstatic int check_kill_permission(int sig, struct kernel_siginfo *info,\n\t\t\t\t struct task_struct *t)\n{\n\tstruct pid *sid;\n\tint error;\n\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\tif (!si_fromuser(info))\n\t\treturn 0;\n\n\terror = audit_signal_info(sig, t); /* Let audit system see the signal */\n\tif (error)\n\t\treturn error;\n\n\tif (!same_thread_group(current, t) &&\n\t    !kill_ok_by_cred(t)) {\n\t\tswitch (sig) {\n\t\tcase SIGCONT:\n\t\t\tsid = task_session(t);\n\t\t\t/*\n\t\t\t * We don't return the error if sid == NULL. The\n\t\t\t * task was unhashed, the caller must notice this.\n\t\t\t */\n\t\t\tif (!sid || sid == task_session(current))\n\t\t\t\tbreak;\n\t\t\t/* fall through */\n\t\tdefault:\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n\n\treturn security_task_kill(t, info, sig, NULL);\n}\n\n/**\n * ptrace_trap_notify - schedule trap to notify ptracer\n * @t: tracee wanting to notify tracer\n *\n * This function schedules sticky ptrace trap which is cleared on the next\n * TRAP_STOP to notify ptracer of an event.  @t must have been seized by\n * ptracer.\n *\n * If @t is running, STOP trap will be taken.  If trapped for STOP and\n * ptracer is listening for events, tracee is woken up so that it can\n * re-trap for the new event.  If trapped otherwise, STOP trap will be\n * eventually taken without returning to userland after the existing traps\n * are finished by PTRACE_CONT.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nstatic void ptrace_trap_notify(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n\tassert_spin_locked(&t->sighand->siglock);\n\n\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}\n\n/*\n * Handle magic process-wide effects of stop/continue signals. Unlike\n * the signal actions, these happen immediately at signal-generation\n * time regardless of blocking, ignoring, or handling.  This does the\n * actual continuing for SIGCONT, but not the actual stopping for stop\n * signals. The process stop is done as a signal action for SIG_DFL.\n *\n * Returns true if the signal should be actually delivered, otherwise\n * it should be dropped.\n */\nstatic bool prepare_signal(int sig, struct task_struct *p, bool force)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\tsigset_t flush;\n\n\tif (signal->flags & (SIGNAL_GROUP_EXIT | SIGNAL_GROUP_COREDUMP)) {\n\t\tif (!(signal->flags & SIGNAL_GROUP_EXIT))\n\t\t\treturn sig == SIGKILL;\n\t\t/*\n\t\t * The process is in the middle of dying, nothing to do.\n\t\t */\n\t} else if (sig_kernel_stop(sig)) {\n\t\t/*\n\t\t * This is a stop signal.  Remove SIGCONT from all queues.\n\t\t */\n\t\tsiginitset(&flush, sigmask(SIGCONT));\n\t\tflush_sigqueue_mask(&flush, &signal->shared_pending);\n\t\tfor_each_thread(p, t)\n\t\t\tflush_sigqueue_mask(&flush, &t->pending);\n\t} else if (sig == SIGCONT) {\n\t\tunsigned int why;\n\t\t/*\n\t\t * Remove all stop signals from all queues, wake all threads.\n\t\t */\n\t\tsiginitset(&flush, SIG_KERNEL_STOP_MASK);\n\t\tflush_sigqueue_mask(&flush, &signal->shared_pending);\n\t\tfor_each_thread(p, t) {\n\t\t\tflush_sigqueue_mask(&flush, &t->pending);\n\t\t\ttask_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);\n\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\twake_up_state(t, __TASK_STOPPED);\n\t\t\telse\n\t\t\t\tptrace_trap_notify(t);\n\t\t}\n\n\t\t/*\n\t\t * Notify the parent with CLD_CONTINUED if we were stopped.\n\t\t *\n\t\t * If we were in the middle of a group stop, we pretend it\n\t\t * was already finished, and then continued. Since SIGCHLD\n\t\t * doesn't queue we report only CLD_STOPPED, as if the next\n\t\t * CLD_CONTINUED was dropped.\n\t\t */\n\t\twhy = 0;\n\t\tif (signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\twhy |= SIGNAL_CLD_CONTINUED;\n\t\telse if (signal->group_stop_count)\n\t\t\twhy |= SIGNAL_CLD_STOPPED;\n\n\t\tif (why) {\n\t\t\t/*\n\t\t\t * The first thread which returns from do_signal_stop()\n\t\t\t * will take ->siglock, notice SIGNAL_CLD_MASK, and\n\t\t\t * notify its parent. See get_signal().\n\t\t\t */\n\t\t\tsignal_set_stop_flags(signal, why | SIGNAL_STOP_CONTINUED);\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tsignal->group_exit_code = 0;\n\t\t}\n\t}\n\n\treturn !sig_ignored(p, sig, force);\n}\n\n/*\n * Test if P wants to take SIG.  After we've checked all threads with this,\n * it's equivalent to finding no threads not blocking SIG.  Any threads not\n * blocking SIG were ruled out because they are not running and already\n * have pending signals.  Such threads will dequeue from the shared queue\n * as soon as they're available, so putting the signal on the shared queue\n * will be equivalent to sending it to one such thread.\n */\nstatic inline bool wants_signal(int sig, struct task_struct *p)\n{\n\tif (sigismember(&p->blocked, sig))\n\t\treturn false;\n\n\tif (p->flags & PF_EXITING)\n\t\treturn false;\n\n\tif (sig == SIGKILL)\n\t\treturn true;\n\n\tif (task_is_stopped_or_traced(p))\n\t\treturn false;\n\n\treturn task_curr(p) || !signal_pending(p);\n}\n\nstatic void complete_signal(int sig, struct task_struct *p, enum pid_type type)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\t/*\n\t * Now find a thread we can wake up to take the signal off the queue.\n\t *\n\t * If the main thread wants the signal, it gets first crack.\n\t * Probably the least surprising to the average bear.\n\t */\n\tif (wants_signal(sig, p))\n\t\tt = p;\n\telse if ((type == PIDTYPE_PID) || thread_group_empty(p))\n\t\t/*\n\t\t * There is just one thread and it does not need to be woken.\n\t\t * It will dequeue unblocked signals before it runs again.\n\t\t */\n\t\treturn;\n\telse {\n\t\t/*\n\t\t * Otherwise try to find a suitable thread.\n\t\t */\n\t\tt = signal->curr_target;\n\t\twhile (!wants_signal(sig, t)) {\n\t\t\tt = next_thread(t);\n\t\t\tif (t == signal->curr_target)\n\t\t\t\t/*\n\t\t\t\t * No thread needs to be woken.\n\t\t\t\t * Any eligible threads will see\n\t\t\t\t * the signal in the queue soon.\n\t\t\t\t */\n\t\t\t\treturn;\n\t\t}\n\t\tsignal->curr_target = t;\n\t}\n\n\t/*\n\t * Found a killable thread.  If the signal will be fatal,\n\t * then start taking the whole group down immediately.\n\t */\n\tif (sig_fatal(p, sig) &&\n\t    !(signal->flags & SIGNAL_GROUP_EXIT) &&\n\t    !sigismember(&t->real_blocked, sig) &&\n\t    (sig == SIGKILL || !p->ptrace)) {\n\t\t/*\n\t\t * This signal will be fatal to the whole group.\n\t\t */\n\t\tif (!sig_kernel_coredump(sig)) {\n\t\t\t/*\n\t\t\t * Start a group exit and wake everybody up.\n\t\t\t * This way we don't have other threads\n\t\t\t * running and doing things after a slower\n\t\t\t * thread has the fatal signal pending.\n\t\t\t */\n\t\t\tsignal->flags = SIGNAL_GROUP_EXIT;\n\t\t\tsignal->group_exit_code = sig;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tt = p;\n\t\t\tdo {\n\t\t\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\t\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\t\t\tsignal_wake_up(t, 1);\n\t\t\t} while_each_thread(p, t);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * The signal is already in the shared-pending queue.\n\t * Tell the chosen thread to wake up and dequeue it.\n\t */\n\tsignal_wake_up(t, sig == SIGKILL);\n\treturn;\n}\n\nstatic inline bool legacy_queue(struct sigpending *signals, int sig)\n{\n\treturn (sig < SIGRTMIN) && sigismember(&signals->signal, sig);\n}\n\nstatic int __send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t,\n\t\t\tenum pid_type type, bool force)\n{\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint override_rlimit;\n\tint ret = 0, result;\n\n\tassert_spin_locked(&t->sighand->siglock);\n\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t, force))\n\t\tgoto ret;\n\n\tpending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\n\t/*\n\t * Short-circuit ignored signals and support queuing\n\t * exactly one non-rt signal, so that we can get more\n\t * detailed information about the cause of the signal.\n\t */\n\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\tif (legacy_queue(pending, sig))\n\t\tgoto ret;\n\n\tresult = TRACE_SIGNAL_DELIVERED;\n\t/*\n\t * Skip useless siginfo allocation for SIGKILL and kernel threads.\n\t */\n\tif ((sig == SIGKILL) || (t->flags & PF_KTHREAD))\n\t\tgoto out_set;\n\n\t/*\n\t * Real-time signals must be queued if sent by sigqueue, or\n\t * some other real-time mechanism.  It is implementation\n\t * defined whether kill() does so.  We attempt to do so, on\n\t * the principle of least surprise, but since kill is not\n\t * allowed to fail with EAGAIN when low on memory we just\n\t * make sure at least one signal gets delivered and don't\n\t * pass on the info struct.\n\t */\n\tif (sig < SIGRTMIN)\n\t\toverride_rlimit = (is_si_special(info) || info->si_code >= 0);\n\telse\n\t\toverride_rlimit = 0;\n\n\tq = __sigqueue_alloc(sig, t, GFP_ATOMIC, override_rlimit);\n\tif (q) {\n\t\tlist_add_tail(&q->list, &pending->list);\n\t\tswitch ((unsigned long) info) {\n\t\tcase (unsigned long) SEND_SIG_NOINFO:\n\t\t\tclear_siginfo(&q->info);\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_USER;\n\t\t\tq->info.si_pid = task_tgid_nr_ns(current,\n\t\t\t\t\t\t\ttask_active_pid_ns(t));\n\t\t\trcu_read_lock();\n\t\t\tq->info.si_uid =\n\t\t\t\tfrom_kuid_munged(task_cred_xxx(t, user_ns),\n\t\t\t\t\t\t current_uid());\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\tcase (unsigned long) SEND_SIG_PRIV:\n\t\t\tclear_siginfo(&q->info);\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_KERNEL;\n\t\t\tq->info.si_pid = 0;\n\t\t\tq->info.si_uid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcopy_siginfo(&q->info, info);\n\t\t\tbreak;\n\t\t}\n\t} else if (!is_si_special(info) &&\n\t\t   sig >= SIGRTMIN && info->si_code != SI_USER) {\n\t\t/*\n\t\t * Queue overflow, abort.  We may abort if the\n\t\t * signal was rt and sent by user using something\n\t\t * other than kill().\n\t\t */\n\t\tresult = TRACE_SIGNAL_OVERFLOW_FAIL;\n\t\tret = -EAGAIN;\n\t\tgoto ret;\n\t} else {\n\t\t/*\n\t\t * This is a silent loss of information.  We still\n\t\t * send the signal, but the *info bits are lost.\n\t\t */\n\t\tresult = TRACE_SIGNAL_LOSE_INFO;\n\t}\n\nout_set:\n\tsignalfd_notify(t, sig);\n\tsigaddset(&pending->signal, sig);\n\n\t/* Let multiprocess signals appear after on-going forks */\n\tif (type > PIDTYPE_TGID) {\n\t\tstruct multiprocess_signals *delayed;\n\t\thlist_for_each_entry(delayed, &t->signal->multiprocess, node) {\n\t\t\tsigset_t *signal = &delayed->signal;\n\t\t\t/* Can't queue both a stop and a continue signal */\n\t\t\tif (sig == SIGCONT)\n\t\t\t\tsigdelsetmask(signal, SIG_KERNEL_STOP_MASK);\n\t\t\telse if (sig_kernel_stop(sig))\n\t\t\t\tsigdelset(signal, SIGCONT);\n\t\t\tsigaddset(signal, sig);\n\t\t}\n\t}\n\n\tcomplete_signal(sig, t, type);\nret:\n\ttrace_signal_generate(sig, info, t, type != PIDTYPE_PID, result);\n\treturn ret;\n}\n\nstatic inline bool has_si_pid_and_uid(struct kernel_siginfo *info)\n{\n\tbool ret = false;\n\tswitch (siginfo_layout(info->si_signo, info->si_code)) {\n\tcase SIL_KILL:\n\tcase SIL_CHLD:\n\tcase SIL_RT:\n\t\tret = true;\n\t\tbreak;\n\tcase SIL_TIMER:\n\tcase SIL_POLL:\n\tcase SIL_FAULT:\n\tcase SIL_FAULT_MCEERR:\n\tcase SIL_FAULT_BNDERR:\n\tcase SIL_FAULT_PKUERR:\n\tcase SIL_SYS:\n\t\tret = false;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t,\n\t\t\tenum pid_type type)\n{\n\t/* Should SIGKILL or SIGSTOP be received by a pid namespace init? */\n\tbool force = false;\n\n\tif (info == SEND_SIG_NOINFO) {\n\t\t/* Force if sent from an ancestor pid namespace */\n\t\tforce = !task_pid_nr_ns(current, task_active_pid_ns(t));\n\t} else if (info == SEND_SIG_PRIV) {\n\t\t/* Don't ignore kernel generated signals */\n\t\tforce = true;\n\t} else if (has_si_pid_and_uid(info)) {\n\t\t/* SIGKILL and SIGSTOP is special or has ids */\n\t\tstruct user_namespace *t_user_ns;\n\n\t\trcu_read_lock();\n\t\tt_user_ns = task_cred_xxx(t, user_ns);\n\t\tif (current_user_ns() != t_user_ns) {\n\t\t\tkuid_t uid = make_kuid(current_user_ns(), info->si_uid);\n\t\t\tinfo->si_uid = from_kuid_munged(t_user_ns, uid);\n\t\t}\n\t\trcu_read_unlock();\n\n\t\t/* A kernel generated signal? */\n\t\tforce = (info->si_code == SI_KERNEL);\n\n\t\t/* From an ancestor pid namespace? */\n\t\tif (!task_pid_nr_ns(current, task_active_pid_ns(t))) {\n\t\t\tinfo->si_pid = 0;\n\t\t\tforce = true;\n\t\t}\n\t}\n\treturn __send_signal(sig, info, t, type, force);\n}\n\nstatic void print_fatal_signal(int signr)\n{\n\tstruct pt_regs *regs = signal_pt_regs();\n\tpr_info(\"potentially unexpected fatal signal %d.\\n\", signr);\n\n#if defined(__i386__) && !defined(__arch_um__)\n\tpr_info(\"code at %08lx: \", regs->ip);\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < 16; i++) {\n\t\t\tunsigned char insn;\n\n\t\t\tif (get_user(insn, (unsigned char *)(regs->ip + i)))\n\t\t\t\tbreak;\n\t\t\tpr_cont(\"%02x \", insn);\n\t\t}\n\t}\n\tpr_cont(\"\\n\");\n#endif\n\tpreempt_disable();\n\tshow_regs(regs);\n\tpreempt_enable();\n}\n\nstatic int __init setup_print_fatal_signals(char *str)\n{\n\tget_option (&str, &print_fatal_signals);\n\n\treturn 1;\n}\n\n__setup(\"print-fatal-signals=\", setup_print_fatal_signals);\n\nint\n__group_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p)\n{\n\treturn send_signal(sig, info, p, PIDTYPE_TGID);\n}\n\nint do_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p,\n\t\t\tenum pid_type type)\n{\n\tunsigned long flags;\n\tint ret = -ESRCH;\n\n\tif (lock_task_sighand(p, &flags)) {\n\t\tret = send_signal(sig, info, p, type);\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Force a signal that the process can't ignore: if necessary\n * we unblock the signal and change any SIG_IGN to SIG_DFL.\n *\n * Note: If we unblock the signal, we always reset it to SIG_DFL,\n * since we do not want to have a signal handler that was blocked\n * be invoked when user space had explicitly blocked it.\n *\n * We don't want to have recursive SIGSEGV's etc, for example,\n * that is why we also clear SIGNAL_UNKILLABLE.\n */\nstatic int\nforce_sig_info_to_task(struct kernel_siginfo *info, struct task_struct *t)\n{\n\tunsigned long int flags;\n\tint ret, blocked, ignored;\n\tstruct k_sigaction *action;\n\tint sig = info->si_signo;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\taction = &t->sighand->action[sig-1];\n\tignored = action->sa.sa_handler == SIG_IGN;\n\tblocked = sigismember(&t->blocked, sig);\n\tif (blocked || ignored) {\n\t\taction->sa.sa_handler = SIG_DFL;\n\t\tif (blocked) {\n\t\t\tsigdelset(&t->blocked, sig);\n\t\t\trecalc_sigpending_and_wake(t);\n\t\t}\n\t}\n\t/*\n\t * Don't clear SIGNAL_UNKILLABLE for traced tasks, users won't expect\n\t * debugging to leave init killable.\n\t */\n\tif (action->sa.sa_handler == SIG_DFL && !t->ptrace)\n\t\tt->signal->flags &= ~SIGNAL_UNKILLABLE;\n\tret = send_signal(sig, info, t, PIDTYPE_PID);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n\n\treturn ret;\n}\n\nint force_sig_info(struct kernel_siginfo *info)\n{\n\treturn force_sig_info_to_task(info, current);\n}\n\n/*\n * Nuke all other threads in the group.\n */\nint zap_other_threads(struct task_struct *p)\n{\n\tstruct task_struct *t = p;\n\tint count = 0;\n\n\tp->signal->group_stop_count = 0;\n\n\twhile_each_thread(p, t) {\n\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\tcount++;\n\n\t\t/* Don't bother with already dead threads */\n\t\tif (t->exit_state)\n\t\t\tcontinue;\n\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\tsignal_wake_up(t, 1);\n\t}\n\n\treturn count;\n}\n\nstruct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t   unsigned long *flags)\n{\n\tstruct sighand_struct *sighand;\n\n\trcu_read_lock();\n\tfor (;;) {\n\t\tsighand = rcu_dereference(tsk->sighand);\n\t\tif (unlikely(sighand == NULL))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * This sighand can be already freed and even reused, but\n\t\t * we rely on SLAB_TYPESAFE_BY_RCU and sighand_ctor() which\n\t\t * initializes ->siglock: this slab can't go away, it has\n\t\t * the same object type, ->siglock can't be reinitialized.\n\t\t *\n\t\t * We need to ensure that tsk->sighand is still the same\n\t\t * after we take the lock, we can race with de_thread() or\n\t\t * __exit_signal(). In the latter case the next iteration\n\t\t * must see ->sighand == NULL.\n\t\t */\n\t\tspin_lock_irqsave(&sighand->siglock, *flags);\n\t\tif (likely(sighand == rcu_access_pointer(tsk->sighand)))\n\t\t\tbreak;\n\t\tspin_unlock_irqrestore(&sighand->siglock, *flags);\n\t}\n\trcu_read_unlock();\n\n\treturn sighand;\n}\n\n/*\n * send signal info to all the members of a group\n */\nint group_send_sig_info(int sig, struct kernel_siginfo *info,\n\t\t\tstruct task_struct *p, enum pid_type type)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = check_kill_permission(sig, info, p);\n\trcu_read_unlock();\n\n\tif (!ret && sig)\n\t\tret = do_send_sig_info(sig, info, p, type);\n\n\treturn ret;\n}\n\n/*\n * __kill_pgrp_info() sends a signal to a process group: this is what the tty\n * control characters do (^C, ^Z etc)\n * - the caller must hold at least a readlock on tasklist_lock\n */\nint __kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp)\n{\n\tstruct task_struct *p = NULL;\n\tint retval, success;\n\n\tsuccess = 0;\n\tretval = -ESRCH;\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tint err = group_send_sig_info(sig, info, p, PIDTYPE_PGID);\n\t\tsuccess |= !err;\n\t\tretval = err;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\treturn success ? 0 : retval;\n}\n\nint kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid)\n{\n\tint error = -ESRCH;\n\tstruct task_struct *p;\n\n\tfor (;;) {\n\t\trcu_read_lock();\n\t\tp = pid_task(pid, PIDTYPE_PID);\n\t\tif (p)\n\t\t\terror = group_send_sig_info(sig, info, p, PIDTYPE_TGID);\n\t\trcu_read_unlock();\n\t\tif (likely(!p || error != -ESRCH))\n\t\t\treturn error;\n\n\t\t/*\n\t\t * The task was unhashed in between, try again.  If it\n\t\t * is dead, pid_task() will return NULL, if we race with\n\t\t * de_thread() it will find the new leader.\n\t\t */\n\t}\n}\n\nstatic int kill_proc_info(int sig, struct kernel_siginfo *info, pid_t pid)\n{\n\tint error;\n\trcu_read_lock();\n\terror = kill_pid_info(sig, info, find_vpid(pid));\n\trcu_read_unlock();\n\treturn error;\n}\n\nstatic inline bool kill_as_cred_perm(const struct cred *cred,\n\t\t\t\t     struct task_struct *target)\n{\n\tconst struct cred *pcred = __task_cred(target);\n\n\treturn uid_eq(cred->euid, pcred->suid) ||\n\t       uid_eq(cred->euid, pcred->uid) ||\n\t       uid_eq(cred->uid, pcred->suid) ||\n\t       uid_eq(cred->uid, pcred->uid);\n}\n\n/*\n * The usb asyncio usage of siginfo is wrong.  The glibc support\n * for asyncio which uses SI_ASYNCIO assumes the layout is SIL_RT.\n * AKA after the generic fields:\n *\tkernel_pid_t\tsi_pid;\n *\tkernel_uid32_t\tsi_uid;\n *\tsigval_t\tsi_value;\n *\n * Unfortunately when usb generates SI_ASYNCIO it assumes the layout\n * after the generic fields is:\n *\tvoid __user \t*si_addr;\n *\n * This is a practical problem when there is a 64bit big endian kernel\n * and a 32bit userspace.  As the 32bit address will encoded in the low\n * 32bits of the pointer.  Those low 32bits will be stored at higher\n * address than appear in a 32 bit pointer.  So userspace will not\n * see the address it was expecting for it's completions.\n *\n * There is nothing in the encoding that can allow\n * copy_siginfo_to_user32 to detect this confusion of formats, so\n * handle this by requiring the caller of kill_pid_usb_asyncio to\n * notice when this situration takes place and to store the 32bit\n * pointer in sival_int, instead of sival_addr of the sigval_t addr\n * parameter.\n */\nint kill_pid_usb_asyncio(int sig, int errno, sigval_t addr,\n\t\t\t struct pid *pid, const struct cred *cred)\n{\n\tstruct kernel_siginfo info;\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tint ret = -EINVAL;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = errno;\n\tinfo.si_code = SI_ASYNCIO;\n\t*((sigval_t *)&info.si_pid) = addr;\n\n\tif (!valid_signal(sig))\n\t\treturn ret;\n\n\trcu_read_lock();\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (!p) {\n\t\tret = -ESRCH;\n\t\tgoto out_unlock;\n\t}\n\tif (!kill_as_cred_perm(cred, p)) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\tret = security_task_kill(p, &info, sig, cred);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tif (sig) {\n\t\tif (lock_task_sighand(p, &flags)) {\n\t\t\tret = __send_signal(sig, &info, p, PIDTYPE_TGID, false);\n\t\t\tunlock_task_sighand(p, &flags);\n\t\t} else\n\t\t\tret = -ESRCH;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kill_pid_usb_asyncio);\n\n/*\n * kill_something_info() interprets pid in interesting ways just like kill(2).\n *\n * POSIX specifies that kill(-1,sig) is unspecified, but what we have\n * is probably wrong.  Should make it like BSD or SYSV.\n */\n\nstatic int kill_something_info(int sig, struct kernel_siginfo *info, pid_t pid)\n{\n\tint ret;\n\n\tif (pid > 0) {\n\t\trcu_read_lock();\n\t\tret = kill_pid_info(sig, info, find_vpid(pid));\n\t\trcu_read_unlock();\n\t\treturn ret;\n\t}\n\n\t/* -INT_MIN is undefined.  Exclude this case to avoid a UBSAN warning */\n\tif (pid == INT_MIN)\n\t\treturn -ESRCH;\n\n\tread_lock(&tasklist_lock);\n\tif (pid != -1) {\n\t\tret = __kill_pgrp_info(sig, info,\n\t\t\t\tpid ? find_vpid(-pid) : task_pgrp(current));\n\t} else {\n\t\tint retval = 0, count = 0;\n\t\tstruct task_struct * p;\n\n\t\tfor_each_process(p) {\n\t\t\tif (task_pid_vnr(p) > 1 &&\n\t\t\t\t\t!same_thread_group(p, current)) {\n\t\t\t\tint err = group_send_sig_info(sig, info, p,\n\t\t\t\t\t\t\t      PIDTYPE_MAX);\n\t\t\t\t++count;\n\t\t\t\tif (err != -EPERM)\n\t\t\t\t\tretval = err;\n\t\t\t}\n\t\t}\n\t\tret = count ? retval : -ESRCH;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * These are for backward compatibility with the rest of the kernel source.\n */\n\nint send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p)\n{\n\t/*\n\t * Make sure legacy kernel users don't send in bad values\n\t * (normal paths check this in check_kill_permission).\n\t */\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\treturn do_send_sig_info(sig, info, p, PIDTYPE_PID);\n}\nEXPORT_SYMBOL(send_sig_info);\n\n#define __si_special(priv) \\\n\t((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)\n\nint\nsend_sig(int sig, struct task_struct *p, int priv)\n{\n\treturn send_sig_info(sig, __si_special(priv), p);\n}\nEXPORT_SYMBOL(send_sig);\n\nvoid force_sig(int sig)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_KERNEL;\n\tinfo.si_pid = 0;\n\tinfo.si_uid = 0;\n\tforce_sig_info(&info);\n}\nEXPORT_SYMBOL(force_sig);\n\n/*\n * When things go south during signal handling, we\n * will force a SIGSEGV. And if the signal that caused\n * the problem was already a SIGSEGV, we'll want to\n * make sure we don't even try to deliver the signal..\n */\nvoid force_sigsegv(int sig)\n{\n\tstruct task_struct *p = current;\n\n\tif (sig == SIGSEGV) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&p->sighand->siglock, flags);\n\t\tp->sighand->action[sig - 1].sa.sa_handler = SIG_DFL;\n\t\tspin_unlock_irqrestore(&p->sighand->siglock, flags);\n\t}\n\tforce_sig(SIGSEGV);\n}\n\nint force_sig_fault_to_task(int sig, int code, void __user *addr\n\t___ARCH_SI_TRAPNO(int trapno)\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n\t, struct task_struct *t)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = code;\n\tinfo.si_addr  = addr;\n#ifdef __ARCH_SI_TRAPNO\n\tinfo.si_trapno = trapno;\n#endif\n#ifdef __ia64__\n\tinfo.si_imm = imm;\n\tinfo.si_flags = flags;\n\tinfo.si_isr = isr;\n#endif\n\treturn force_sig_info_to_task(&info, t);\n}\n\nint force_sig_fault(int sig, int code, void __user *addr\n\t___ARCH_SI_TRAPNO(int trapno)\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr))\n{\n\treturn force_sig_fault_to_task(sig, code, addr\n\t\t\t\t       ___ARCH_SI_TRAPNO(trapno)\n\t\t\t\t       ___ARCH_SI_IA64(imm, flags, isr), current);\n}\n\nint send_sig_fault(int sig, int code, void __user *addr\n\t___ARCH_SI_TRAPNO(int trapno)\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n\t, struct task_struct *t)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = code;\n\tinfo.si_addr  = addr;\n#ifdef __ARCH_SI_TRAPNO\n\tinfo.si_trapno = trapno;\n#endif\n#ifdef __ia64__\n\tinfo.si_imm = imm;\n\tinfo.si_flags = flags;\n\tinfo.si_isr = isr;\n#endif\n\treturn send_sig_info(info.si_signo, &info, t);\n}\n\nint force_sig_mceerr(int code, void __user *addr, short lsb)\n{\n\tstruct kernel_siginfo info;\n\n\tWARN_ON((code != BUS_MCEERR_AO) && (code != BUS_MCEERR_AR));\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = code;\n\tinfo.si_addr = addr;\n\tinfo.si_addr_lsb = lsb;\n\treturn force_sig_info(&info);\n}\n\nint send_sig_mceerr(int code, void __user *addr, short lsb, struct task_struct *t)\n{\n\tstruct kernel_siginfo info;\n\n\tWARN_ON((code != BUS_MCEERR_AO) && (code != BUS_MCEERR_AR));\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = code;\n\tinfo.si_addr = addr;\n\tinfo.si_addr_lsb = lsb;\n\treturn send_sig_info(info.si_signo, &info, t);\n}\nEXPORT_SYMBOL(send_sig_mceerr);\n\nint force_sig_bnderr(void __user *addr, void __user *lower, void __user *upper)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGSEGV;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = SEGV_BNDERR;\n\tinfo.si_addr  = addr;\n\tinfo.si_lower = lower;\n\tinfo.si_upper = upper;\n\treturn force_sig_info(&info);\n}\n\n#ifdef SEGV_PKUERR\nint force_sig_pkuerr(void __user *addr, u32 pkey)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGSEGV;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = SEGV_PKUERR;\n\tinfo.si_addr  = addr;\n\tinfo.si_pkey  = pkey;\n\treturn force_sig_info(&info);\n}\n#endif\n\n/* For the crazy architectures that include trap information in\n * the errno field, instead of an actual errno value.\n */\nint force_sig_ptrace_errno_trap(int errno, void __user *addr)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = errno;\n\tinfo.si_code  = TRAP_HWBKPT;\n\tinfo.si_addr  = addr;\n\treturn force_sig_info(&info);\n}\n\nint kill_pgrp(struct pid *pid, int sig, int priv)\n{\n\tint ret;\n\n\tread_lock(&tasklist_lock);\n\tret = __kill_pgrp_info(sig, __si_special(priv), pid);\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kill_pgrp);\n\nint kill_pid(struct pid *pid, int sig, int priv)\n{\n\treturn kill_pid_info(sig, __si_special(priv), pid);\n}\nEXPORT_SYMBOL(kill_pid);\n\n/*\n * These functions support sending signals using preallocated sigqueue\n * structures.  This is needed \"because realtime applications cannot\n * afford to lose notifications of asynchronous events, like timer\n * expirations or I/O completions\".  In the case of POSIX Timers\n * we allocate the sigqueue structure from the timer_create.  If this\n * allocation fails we are able to report the failure to the application\n * with an EAGAIN error.\n */\nstruct sigqueue *sigqueue_alloc(void)\n{\n\tstruct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);\n\n\tif (q)\n\t\tq->flags |= SIGQUEUE_PREALLOC;\n\n\treturn q;\n}\n\nvoid sigqueue_free(struct sigqueue *q)\n{\n\tunsigned long flags;\n\tspinlock_t *lock = &current->sighand->siglock;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\t/*\n\t * We must hold ->siglock while testing q->list\n\t * to serialize with collect_signal() or with\n\t * __exit_signal()->flush_sigqueue().\n\t */\n\tspin_lock_irqsave(lock, flags);\n\tq->flags &= ~SIGQUEUE_PREALLOC;\n\t/*\n\t * If it is queued it will be freed when dequeued,\n\t * like the \"regular\" sigqueue.\n\t */\n\tif (!list_empty(&q->list))\n\t\tq = NULL;\n\tspin_unlock_irqrestore(lock, flags);\n\n\tif (q)\n\t\t__sigqueue_free(q);\n}\n\nint send_sigqueue(struct sigqueue *q, struct pid *pid, enum pid_type type)\n{\n\tint sig = q->info.si_signo;\n\tstruct sigpending *pending;\n\tstruct task_struct *t;\n\tunsigned long flags;\n\tint ret, result;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\n\tret = -1;\n\trcu_read_lock();\n\tt = pid_task(pid, type);\n\tif (!t || !likely(lock_task_sighand(t, &flags)))\n\t\tgoto ret;\n\n\tret = 1; /* the signal is ignored */\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t, false))\n\t\tgoto out;\n\n\tret = 0;\n\tif (unlikely(!list_empty(&q->list))) {\n\t\t/*\n\t\t * If an SI_TIMER entry is already queue just increment\n\t\t * the overrun count.\n\t\t */\n\t\tBUG_ON(q->info.si_code != SI_TIMER);\n\t\tq->info.si_overrun++;\n\t\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\t\tgoto out;\n\t}\n\tq->info.si_overrun = 0;\n\n\tsignalfd_notify(t, sig);\n\tpending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\n\tlist_add_tail(&q->list, &pending->list);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, type);\n\tresult = TRACE_SIGNAL_DELIVERED;\nout:\n\ttrace_signal_generate(sig, &q->info, t, type != PIDTYPE_PID, result);\n\tunlock_task_sighand(t, &flags);\nret:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic void do_notify_pidfd(struct task_struct *task)\n{\n\tstruct pid *pid;\n\n\tWARN_ON(task->exit_state == 0);\n\tpid = task_pid(task);\n\twake_up_all(&pid->wait_pidfd);\n}\n\n/*\n * Let a parent know about the death of a child.\n * For a stopped/continued status change, use do_notify_parent_cldstop instead.\n *\n * Returns true if our parent ignored us and so we've switched to\n * self-reaping.\n */\nbool do_notify_parent(struct task_struct *tsk, int sig)\n{\n\tstruct kernel_siginfo info;\n\tunsigned long flags;\n\tstruct sighand_struct *psig;\n\tbool autoreap = false;\n\tu64 utime, stime;\n\n\tBUG_ON(sig == -1);\n\n \t/* do_notify_parent_cldstop should have been called instead.  */\n \tBUG_ON(task_is_stopped_or_traced(tsk));\n\n\tBUG_ON(!tsk->ptrace &&\n\t       (tsk->group_leader != tsk || !thread_group_empty(tsk)));\n\n\t/* Wake up all pidfd waiters */\n\tdo_notify_pidfd(tsk);\n\n\tif (sig != SIGCHLD) {\n\t\t/*\n\t\t * This is only possible if parent == real_parent.\n\t\t * Check if it has changed security domain.\n\t\t */\n\t\tif (tsk->parent_exec_id != READ_ONCE(tsk->parent->self_exec_id))\n\t\t\tsig = SIGCHLD;\n\t}\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\t/*\n\t * We are under tasklist_lock here so our parent is tied to\n\t * us and cannot change.\n\t *\n\t * task_active_pid_ns will always return the same pid namespace\n\t * until a task passes through release_task.\n\t *\n\t * write_lock() currently calls preempt_disable() which is the\n\t * same as rcu_read_lock(), but according to Oleg, this is not\n\t * correct to rely on this\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),\n\t\t\t\t       task_uid(tsk));\n\trcu_read_unlock();\n\n\ttask_cputime(tsk, &utime, &stime);\n\tinfo.si_utime = nsec_to_clock_t(utime + tsk->signal->utime);\n\tinfo.si_stime = nsec_to_clock_t(stime + tsk->signal->stime);\n\n\tinfo.si_status = tsk->exit_code & 0x7f;\n\tif (tsk->exit_code & 0x80)\n\t\tinfo.si_code = CLD_DUMPED;\n\telse if (tsk->exit_code & 0x7f)\n\t\tinfo.si_code = CLD_KILLED;\n\telse {\n\t\tinfo.si_code = CLD_EXITED;\n\t\tinfo.si_status = tsk->exit_code >> 8;\n\t}\n\n\tpsig = tsk->parent->sighand;\n\tspin_lock_irqsave(&psig->siglock, flags);\n\tif (!tsk->ptrace && sig == SIGCHLD &&\n\t    (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||\n\t     (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {\n\t\t/*\n\t\t * We are exiting and our parent doesn't care.  POSIX.1\n\t\t * defines special semantics for setting SIGCHLD to SIG_IGN\n\t\t * or setting the SA_NOCLDWAIT flag: we should be reaped\n\t\t * automatically and not left for our parent's wait4 call.\n\t\t * Rather than having the parent do it as a magic kind of\n\t\t * signal handler, we just set this to tell do_exit that we\n\t\t * can be cleaned up without becoming a zombie.  Note that\n\t\t * we still call __wake_up_parent in this case, because a\n\t\t * blocked sys_wait4 might now return -ECHILD.\n\t\t *\n\t\t * Whether we send SIGCHLD or not for SA_NOCLDWAIT\n\t\t * is implementation-defined: we do (if you don't want\n\t\t * it, just use SIG_IGN instead).\n\t\t */\n\t\tautoreap = true;\n\t\tif (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)\n\t\t\tsig = 0;\n\t}\n\tif (valid_signal(sig) && sig)\n\t\t__group_send_sig_info(sig, &info, tsk->parent);\n\t__wake_up_parent(tsk, tsk->parent);\n\tspin_unlock_irqrestore(&psig->siglock, flags);\n\n\treturn autoreap;\n}\n\n/**\n * do_notify_parent_cldstop - notify parent of stopped/continued state change\n * @tsk: task reporting the state change\n * @for_ptracer: the notification is for ptracer\n * @why: CLD_{CONTINUED|STOPPED|TRAPPED} to report\n *\n * Notify @tsk's parent that the stopped/continued state has changed.  If\n * @for_ptracer is %false, @tsk's group leader notifies to its real parent.\n * If %true, @tsk reports to @tsk->parent which should be the ptracer.\n *\n * CONTEXT:\n * Must be called with tasklist_lock at least read locked.\n */\nstatic void do_notify_parent_cldstop(struct task_struct *tsk,\n\t\t\t\t     bool for_ptracer, int why)\n{\n\tstruct kernel_siginfo info;\n\tunsigned long flags;\n\tstruct task_struct *parent;\n\tstruct sighand_struct *sighand;\n\tu64 utime, stime;\n\n\tif (for_ptracer) {\n\t\tparent = tsk->parent;\n\t} else {\n\t\ttsk = tsk->group_leader;\n\t\tparent = tsk->real_parent;\n\t}\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = SIGCHLD;\n\tinfo.si_errno = 0;\n\t/*\n\t * see comment in do_notify_parent() about the following 4 lines\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));\n\trcu_read_unlock();\n\n\ttask_cputime(tsk, &utime, &stime);\n\tinfo.si_utime = nsec_to_clock_t(utime);\n\tinfo.si_stime = nsec_to_clock_t(stime);\n\n \tinfo.si_code = why;\n \tswitch (why) {\n \tcase CLD_CONTINUED:\n \t\tinfo.si_status = SIGCONT;\n \t\tbreak;\n \tcase CLD_STOPPED:\n \t\tinfo.si_status = tsk->signal->group_exit_code & 0x7f;\n \t\tbreak;\n \tcase CLD_TRAPPED:\n \t\tinfo.si_status = tsk->exit_code & 0x7f;\n \t\tbreak;\n \tdefault:\n \t\tBUG();\n \t}\n\n\tsighand = parent->sighand;\n\tspin_lock_irqsave(&sighand->siglock, flags);\n\tif (sighand->action[SIGCHLD-1].sa.sa_handler != SIG_IGN &&\n\t    !(sighand->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))\n\t\t__group_send_sig_info(SIGCHLD, &info, parent);\n\t/*\n\t * Even if SIGCHLD is not generated, we must wake up wait4 calls.\n\t */\n\t__wake_up_parent(tsk, parent);\n\tspin_unlock_irqrestore(&sighand->siglock, flags);\n}\n\nstatic inline bool may_ptrace_stop(void)\n{\n\tif (!likely(current->ptrace))\n\t\treturn false;\n\t/*\n\t * Are we in the middle of do_coredump?\n\t * If so and our tracer is also part of the coredump stopping\n\t * is a deadlock situation, and pointless because our tracer\n\t * is dead so don't allow us to stop.\n\t * If SIGKILL was already sent before the caller unlocked\n\t * ->siglock we must see ->core_state != NULL. Otherwise it\n\t * is safe to enter schedule().\n\t *\n\t * This is almost outdated, a task with the pending SIGKILL can't\n\t * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported\n\t * after SIGKILL was already dequeued.\n\t */\n\tif (unlikely(current->mm->core_state) &&\n\t    unlikely(current->mm == current->parent->mm))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Return non-zero if there is a SIGKILL that should be waking us up.\n * Called with the siglock held.\n */\nstatic bool sigkill_pending(struct task_struct *tsk)\n{\n\treturn sigismember(&tsk->pending.signal, SIGKILL) ||\n\t       sigismember(&tsk->signal->shared_pending.signal, SIGKILL);\n}\n\n/*\n * This must be called with current->sighand->siglock held.\n *\n * This should be the path for all ptrace stops.\n * We always set current->last_siginfo while stopped here.\n * That makes it a way to test a stopped process for\n * being ptrace-stopped vs being job-control-stopped.\n *\n * If we actually decide not to stop at all because the tracer\n * is gone, we keep current->exit_code unless clear_code.\n */\nstatic void ptrace_stop(int exit_code, int why, int clear_code, kernel_siginfo_t *info)\n\t__releases(&current->sighand->siglock)\n\t__acquires(&current->sighand->siglock)\n{\n\tbool gstop_done = false;\n\n\tif (arch_ptrace_stop_needed(exit_code, info)) {\n\t\t/*\n\t\t * The arch code has something special to do before a\n\t\t * ptrace stop.  This is allowed to block, e.g. for faults\n\t\t * on user stack pages.  We can't keep the siglock while\n\t\t * calling arch_ptrace_stop, so we must release it now.\n\t\t * To preserve proper semantics, we must do this before\n\t\t * any signal bookkeeping like checking group_stop_count.\n\t\t * Meanwhile, a SIGKILL could come in before we retake the\n\t\t * siglock.  That must prevent us from sleeping in TASK_TRACED.\n\t\t * So after regaining the lock, we must check for SIGKILL.\n\t\t */\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tarch_ptrace_stop(exit_code, info);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\tif (sigkill_pending(current))\n\t\t\treturn;\n\t}\n\n\tset_special_state(TASK_TRACED);\n\n\t/*\n\t * We're committing to trapping.  TRACED should be visible before\n\t * TRAPPING is cleared; otherwise, the tracer might fail do_wait().\n\t * Also, transition to TRACED and updates to ->jobctl should be\n\t * atomic with respect to siglock and should be done after the arch\n\t * hook as siglock is released and regrabbed across it.\n\t *\n\t *     TRACER\t\t\t\t    TRACEE\n\t *\n\t *     ptrace_attach()\n\t * [L]   wait_on_bit(JOBCTL_TRAPPING)\t[S] set_special_state(TRACED)\n\t *     do_wait()\n\t *       set_current_state()                smp_wmb();\n\t *       ptrace_do_wait()\n\t *         wait_task_stopped()\n\t *           task_stopped_code()\n\t * [L]         task_is_traced()\t\t[S] task_clear_jobctl_trapping();\n\t */\n\tsmp_wmb();\n\n\tcurrent->last_siginfo = info;\n\tcurrent->exit_code = exit_code;\n\n\t/*\n\t * If @why is CLD_STOPPED, we're trapping to participate in a group\n\t * stop.  Do the bookkeeping.  Note that if SIGCONT was delievered\n\t * across siglock relocks since INTERRUPT was scheduled, PENDING\n\t * could be clear now.  We act as if SIGCONT is received after\n\t * TASK_TRACED is entered - ignore it.\n\t */\n\tif (why == CLD_STOPPED && (current->jobctl & JOBCTL_STOP_PENDING))\n\t\tgstop_done = task_participate_group_stop(current);\n\n\t/* any trap clears pending STOP trap, STOP trap clears NOTIFY */\n\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\tif (info && info->si_code >> 8 == PTRACE_EVENT_STOP)\n\t\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY);\n\n\t/* entering a trap, clear TRAPPING */\n\ttask_clear_jobctl_trapping(current);\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\tread_lock(&tasklist_lock);\n\tif (may_ptrace_stop()) {\n\t\t/*\n\t\t * Notify parents of the stop.\n\t\t *\n\t\t * While ptraced, there are two parents - the ptracer and\n\t\t * the real_parent of the group_leader.  The ptracer should\n\t\t * know about every stop while the real parent is only\n\t\t * interested in the completion of group stop.  The states\n\t\t * for the two don't interact with each other.  Notify\n\t\t * separately unless they're gonna be duplicates.\n\t\t */\n\t\tdo_notify_parent_cldstop(current, true, why);\n\t\tif (gstop_done && ptrace_reparented(current))\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/*\n\t\t * Don't want to allow preemption here, because\n\t\t * sys_ptrace() needs this task to be inactive.\n\t\t *\n\t\t * XXX: implement read_unlock_no_resched().\n\t\t */\n\t\tpreempt_disable();\n\t\tread_unlock(&tasklist_lock);\n\t\tcgroup_enter_frozen();\n\t\tpreempt_enable_no_resched();\n\t\tfreezable_schedule();\n\t\tcgroup_leave_frozen(true);\n\t} else {\n\t\t/*\n\t\t * By the time we got the lock, our tracer went away.\n\t\t * Don't drop the lock yet, another tracer may come.\n\t\t *\n\t\t * If @gstop_done, the ptracer went away between group stop\n\t\t * completion and here.  During detach, it would have set\n\t\t * JOBCTL_STOP_PENDING on us and we'll re-enter\n\t\t * TASK_STOPPED in do_signal_stop() on return, so notifying\n\t\t * the real parent of the group stop completion is enough.\n\t\t */\n\t\tif (gstop_done)\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/* tasklist protects us from ptrace_freeze_traced() */\n\t\t__set_current_state(TASK_RUNNING);\n\t\tif (clear_code)\n\t\t\tcurrent->exit_code = 0;\n\t\tread_unlock(&tasklist_lock);\n\t}\n\n\t/*\n\t * We are back.  Now reacquire the siglock before touching\n\t * last_siginfo, so that we are sure to have synchronized with\n\t * any signal-sending on another CPU that wants to examine it.\n\t */\n\tspin_lock_irq(&current->sighand->siglock);\n\tcurrent->last_siginfo = NULL;\n\n\t/* LISTENING can be set only during STOP traps, clear it */\n\tcurrent->jobctl &= ~JOBCTL_LISTENING;\n\n\t/*\n\t * Queued signals ignored us while we were stopped for tracing.\n\t * So check for any that we should take before resuming user mode.\n\t * This sets TIF_SIGPENDING, but never clears it.\n\t */\n\trecalc_sigpending_tsk(current);\n}\n\nstatic void ptrace_do_notify(int signr, int exit_code, int why)\n{\n\tkernel_siginfo_t info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = signr;\n\tinfo.si_code = exit_code;\n\tinfo.si_pid = task_pid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\t/* Let the debugger run.  */\n\tptrace_stop(exit_code, why, 1, &info);\n}\n\nvoid ptrace_notify(int exit_code)\n{\n\tBUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED);\n\tspin_unlock_irq(&current->sighand->siglock);\n}\n\n/**\n * do_signal_stop - handle group stop for SIGSTOP and other stop signals\n * @signr: signr causing group stop if initiating\n *\n * If %JOBCTL_STOP_PENDING is not set yet, initiate group stop with @signr\n * and participate in it.  If already set, participate in the existing\n * group stop.  If participated in a group stop (and thus slept), %true is\n * returned with siglock released.\n *\n * If ptraced, this function doesn't handle stop itself.  Instead,\n * %JOBCTL_TRAP_STOP is scheduled and %false is returned with siglock\n * untouched.  The caller must ensure that INTERRUPT trap handling takes\n * places afterwards.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which is released\n * on %true return.\n *\n * RETURNS:\n * %false if group stop is already cancelled or ptrace trap is scheduled.\n * %true if participated in group stop.\n */\nstatic bool do_signal_stop(int signr)\n\t__releases(&current->sighand->siglock)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tif (!(current->jobctl & JOBCTL_STOP_PENDING)) {\n\t\tunsigned long gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\n\t\tstruct task_struct *t;\n\n\t\t/* signr will be recorded in task->jobctl for retries */\n\t\tWARN_ON_ONCE(signr & ~JOBCTL_STOP_SIGMASK);\n\n\t\tif (!likely(current->jobctl & JOBCTL_STOP_DEQUEUED) ||\n\t\t    unlikely(signal_group_exit(sig)))\n\t\t\treturn false;\n\t\t/*\n\t\t * There is no group stop already in progress.  We must\n\t\t * initiate one now.\n\t\t *\n\t\t * While ptraced, a task may be resumed while group stop is\n\t\t * still in effect and then receive a stop signal and\n\t\t * initiate another group stop.  This deviates from the\n\t\t * usual behavior as two consecutive stop signals can't\n\t\t * cause two group stops when !ptraced.  That is why we\n\t\t * also check !task_is_stopped(t) below.\n\t\t *\n\t\t * The condition can be distinguished by testing whether\n\t\t * SIGNAL_STOP_STOPPED is already set.  Don't generate\n\t\t * group_exit_code in such case.\n\t\t *\n\t\t * This is not necessary for SIGNAL_STOP_CONTINUED because\n\t\t * an intervening stop signal is required to cause two\n\t\t * continued events regardless of ptrace.\n\t\t */\n\t\tif (!(sig->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsig->group_exit_code = signr;\n\n\t\tsig->group_stop_count = 0;\n\n\t\tif (task_set_jobctl_pending(current, signr | gstop))\n\t\t\tsig->group_stop_count++;\n\n\t\tt = current;\n\t\twhile_each_thread(current, t) {\n\t\t\t/*\n\t\t\t * Setting state to TASK_STOPPED for a group\n\t\t\t * stop is always done with the siglock held,\n\t\t\t * so this check has no races.\n\t\t\t */\n\t\t\tif (!task_is_stopped(t) &&\n\t\t\t    task_set_jobctl_pending(t, signr | gstop)) {\n\t\t\t\tsig->group_stop_count++;\n\t\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\t\tsignal_wake_up(t, 0);\n\t\t\t\telse\n\t\t\t\t\tptrace_trap_notify(t);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (likely(!current->ptrace)) {\n\t\tint notify = 0;\n\n\t\t/*\n\t\t * If there are no other threads in the group, or if there\n\t\t * is a group stop in progress and we are the last to stop,\n\t\t * report to the parent.\n\t\t */\n\t\tif (task_participate_group_stop(current))\n\t\t\tnotify = CLD_STOPPED;\n\n\t\tset_special_state(TASK_STOPPED);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent of the group stop completion.  Because\n\t\t * we're not holding either the siglock or tasklist_lock\n\t\t * here, ptracer may attach inbetween; however, this is for\n\t\t * group stop and should always be delivered to the real\n\t\t * parent of the group leader.  The new ptracer will get\n\t\t * its notification when this task transitions into\n\t\t * TASK_TRACED.\n\t\t */\n\t\tif (notify) {\n\t\t\tread_lock(&tasklist_lock);\n\t\t\tdo_notify_parent_cldstop(current, false, notify);\n\t\t\tread_unlock(&tasklist_lock);\n\t\t}\n\n\t\t/* Now we don't run again until woken by SIGCONT or SIGKILL */\n\t\tcgroup_enter_frozen();\n\t\tfreezable_schedule();\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * While ptraced, group stop is handled by STOP trap.\n\t\t * Schedule it and let the caller deal with it.\n\t\t */\n\t\ttask_set_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\t\treturn false;\n\t}\n}\n\n/**\n * do_jobctl_trap - take care of ptrace jobctl traps\n *\n * When PT_SEIZED, it's used for both group stop and explicit\n * SEIZE/INTERRUPT traps.  Both generate PTRACE_EVENT_STOP trap with\n * accompanying siginfo.  If stopped, lower eight bits of exit_code contain\n * the stop signal; otherwise, %SIGTRAP.\n *\n * When !PT_SEIZED, it's used only for group stop trap with stop signal\n * number as exit_code and no siginfo.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which may be\n * released and re-acquired before returning with intervening sleep.\n */\nstatic void do_jobctl_trap(void)\n{\n\tstruct signal_struct *signal = current->signal;\n\tint signr = current->jobctl & JOBCTL_STOP_SIGMASK;\n\n\tif (current->ptrace & PT_SEIZED) {\n\t\tif (!signal->group_stop_count &&\n\t\t    !(signal->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsignr = SIGTRAP;\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),\n\t\t\t\t CLD_STOPPED);\n\t} else {\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_stop(signr, CLD_STOPPED, 0, NULL);\n\t\tcurrent->exit_code = 0;\n\t}\n}\n\n/**\n * do_freezer_trap - handle the freezer jobctl trap\n *\n * Puts the task into frozen state, if only the task is not about to quit.\n * In this case it drops JOBCTL_TRAP_FREEZE.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held,\n * which is always released before returning.\n */\nstatic void do_freezer_trap(void)\n\t__releases(&current->sighand->siglock)\n{\n\t/*\n\t * If there are other trap bits pending except JOBCTL_TRAP_FREEZE,\n\t * let's make another loop to give it a chance to be handled.\n\t * In any case, we'll return back.\n\t */\n\tif ((current->jobctl & (JOBCTL_PENDING_MASK | JOBCTL_TRAP_FREEZE)) !=\n\t     JOBCTL_TRAP_FREEZE) {\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\treturn;\n\t}\n\n\t/*\n\t * Now we're sure that there is no pending fatal signal and no\n\t * pending traps. Clear TIF_SIGPENDING to not get out of schedule()\n\t * immediately (if there is a non-fatal signal pending), and\n\t * put the task into sleep.\n\t */\n\t__set_current_state(TASK_INTERRUPTIBLE);\n\tclear_thread_flag(TIF_SIGPENDING);\n\tspin_unlock_irq(&current->sighand->siglock);\n\tcgroup_enter_frozen();\n\tfreezable_schedule();\n}\n\nstatic int ptrace_signal(int signr, kernel_siginfo_t *info)\n{\n\t/*\n\t * We do not check sig_kernel_stop(signr) but set this marker\n\t * unconditionally because we do not know whether debugger will\n\t * change signr. This flag has no meaning unless we are going\n\t * to stop after return from ptrace_stop(). In this case it will\n\t * be checked in do_signal_stop(), we should only stop if it was\n\t * not cleared by SIGCONT while we were sleeping. See also the\n\t * comment in dequeue_signal().\n\t */\n\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\tptrace_stop(signr, CLD_TRAPPED, 0, info);\n\n\t/* We're back.  Did the debugger cancel the sig?  */\n\tsignr = current->exit_code;\n\tif (signr == 0)\n\t\treturn signr;\n\n\tcurrent->exit_code = 0;\n\n\t/*\n\t * Update the siginfo structure if the signal has\n\t * changed.  If the debugger wanted something\n\t * specific in the siginfo structure then it should\n\t * have updated *info via PTRACE_SETSIGINFO.\n\t */\n\tif (signr != info->si_signo) {\n\t\tclear_siginfo(info);\n\t\tinfo->si_signo = signr;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\trcu_read_lock();\n\t\tinfo->si_pid = task_pid_vnr(current->parent);\n\t\tinfo->si_uid = from_kuid_munged(current_user_ns(),\n\t\t\t\t\t\ttask_uid(current->parent));\n\t\trcu_read_unlock();\n\t}\n\n\t/* If the (new) signal is now blocked, requeue it.  */\n\tif (sigismember(&current->blocked, signr)) {\n\t\tsend_signal(signr, info, current, PIDTYPE_PID);\n\t\tsignr = 0;\n\t}\n\n\treturn signr;\n}\n\nbool get_signal(struct ksignal *ksig)\n{\n\tstruct sighand_struct *sighand = current->sighand;\n\tstruct signal_struct *signal = current->signal;\n\tint signr;\n\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tif (unlikely(uprobe_deny_signal()))\n\t\treturn false;\n\n\t/*\n\t * Do this once, we can't return to user-mode if freezing() == T.\n\t * do_signal_stop() and ptrace_stop() do freezable_schedule() and\n\t * thus do not need another check after return.\n\t */\n\ttry_to_freeze();\n\nrelock:\n\tspin_lock_irq(&sighand->siglock);\n\t/*\n\t * Every stopped thread goes here after wakeup. Check to see if\n\t * we should notify the parent, prepare_signal(SIGCONT) encodes\n\t * the CLD_ si_code into SIGNAL_CLD_MASK bits.\n\t */\n\tif (unlikely(signal->flags & SIGNAL_CLD_MASK)) {\n\t\tint why;\n\n\t\tif (signal->flags & SIGNAL_CLD_CONTINUED)\n\t\t\twhy = CLD_CONTINUED;\n\t\telse\n\t\t\twhy = CLD_STOPPED;\n\n\t\tsignal->flags &= ~SIGNAL_CLD_MASK;\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent that we're continuing.  This event is\n\t\t * always per-process and doesn't make whole lot of sense\n\t\t * for ptracers, who shouldn't consume the state via\n\t\t * wait(2) either, but, for backward compatibility, notify\n\t\t * the ptracer of the group leader too unless it's gonna be\n\t\t * a duplicate.\n\t\t */\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\tif (ptrace_reparented(current->group_leader))\n\t\t\tdo_notify_parent_cldstop(current->group_leader,\n\t\t\t\t\t\ttrue, why);\n\t\tread_unlock(&tasklist_lock);\n\n\t\tgoto relock;\n\t}\n\n\t/* Has this task already been marked for death? */\n\tif (signal_group_exit(signal)) {\n\t\tksig->info.si_signo = signr = SIGKILL;\n\t\tsigdelset(&current->pending.signal, SIGKILL);\n\t\ttrace_signal_deliver(SIGKILL, SEND_SIG_NOINFO,\n\t\t\t\t&sighand->action[SIGKILL - 1]);\n\t\trecalc_sigpending();\n\t\tgoto fatal;\n\t}\n\n\tfor (;;) {\n\t\tstruct k_sigaction *ka;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&\n\t\t    do_signal_stop(0))\n\t\t\tgoto relock;\n\n\t\tif (unlikely(current->jobctl &\n\t\t\t     (JOBCTL_TRAP_MASK | JOBCTL_TRAP_FREEZE))) {\n\t\t\tif (current->jobctl & JOBCTL_TRAP_MASK) {\n\t\t\t\tdo_jobctl_trap();\n\t\t\t\tspin_unlock_irq(&sighand->siglock);\n\t\t\t} else if (current->jobctl & JOBCTL_TRAP_FREEZE)\n\t\t\t\tdo_freezer_trap();\n\n\t\t\tgoto relock;\n\t\t}\n\n\t\t/*\n\t\t * If the task is leaving the frozen state, let's update\n\t\t * cgroup counters and reset the frozen bit.\n\t\t */\n\t\tif (unlikely(cgroup_task_frozen(current))) {\n\t\t\tspin_unlock_irq(&sighand->siglock);\n\t\t\tcgroup_leave_frozen(false);\n\t\t\tgoto relock;\n\t\t}\n\n\t\t/*\n\t\t * Signals generated by the execution of an instruction\n\t\t * need to be delivered before any other pending signals\n\t\t * so that the instruction pointer in the signal stack\n\t\t * frame points to the faulting instruction.\n\t\t */\n\t\tsignr = dequeue_synchronous_signal(&ksig->info);\n\t\tif (!signr)\n\t\t\tsignr = dequeue_signal(current, &current->blocked, &ksig->info);\n\n\t\tif (!signr)\n\t\t\tbreak; /* will return 0 */\n\n\t\tif (unlikely(current->ptrace) && signr != SIGKILL) {\n\t\t\tsignr = ptrace_signal(signr, &ksig->info);\n\t\t\tif (!signr)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tka = &sighand->action[signr-1];\n\n\t\t/* Trace actually delivered signals. */\n\t\ttrace_signal_deliver(signr, &ksig->info, ka);\n\n\t\tif (ka->sa.sa_handler == SIG_IGN) /* Do nothing.  */\n\t\t\tcontinue;\n\t\tif (ka->sa.sa_handler != SIG_DFL) {\n\t\t\t/* Run the handler.  */\n\t\t\tksig->ka = *ka;\n\n\t\t\tif (ka->sa.sa_flags & SA_ONESHOT)\n\t\t\t\tka->sa.sa_handler = SIG_DFL;\n\n\t\t\tbreak; /* will return non-zero \"signr\" value */\n\t\t}\n\n\t\t/*\n\t\t * Now we are doing the default action for this signal.\n\t\t */\n\t\tif (sig_kernel_ignore(signr)) /* Default is nothing. */\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Global init gets no signals it doesn't want.\n\t\t * Container-init gets no signals it doesn't want from same\n\t\t * container.\n\t\t *\n\t\t * Note that if global/container-init sees a sig_kernel_only()\n\t\t * signal here, the signal must have been generated internally\n\t\t * or must have come from an ancestor namespace. In either\n\t\t * case, the signal cannot be dropped.\n\t\t */\n\t\tif (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\t\t!sig_kernel_only(signr))\n\t\t\tcontinue;\n\n\t\tif (sig_kernel_stop(signr)) {\n\t\t\t/*\n\t\t\t * The default action is to stop all threads in\n\t\t\t * the thread group.  The job control signals\n\t\t\t * do nothing in an orphaned pgrp, but SIGSTOP\n\t\t\t * always works.  Note that siglock needs to be\n\t\t\t * dropped during the call to is_orphaned_pgrp()\n\t\t\t * because of lock ordering with tasklist_lock.\n\t\t\t * This allows an intervening SIGCONT to be posted.\n\t\t\t * We need to check for that and bail out if necessary.\n\t\t\t */\n\t\t\tif (signr != SIGSTOP) {\n\t\t\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t\t\t/* signals can be posted during this window */\n\n\t\t\t\tif (is_current_pgrp_orphaned())\n\t\t\t\t\tgoto relock;\n\n\t\t\t\tspin_lock_irq(&sighand->siglock);\n\t\t\t}\n\n\t\t\tif (likely(do_signal_stop(ksig->info.si_signo))) {\n\t\t\t\t/* It released the siglock.  */\n\t\t\t\tgoto relock;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We didn't actually stop, due to a race\n\t\t\t * with SIGCONT or something like that.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\tfatal:\n\t\tspin_unlock_irq(&sighand->siglock);\n\t\tif (unlikely(cgroup_task_frozen(current)))\n\t\t\tcgroup_leave_frozen(true);\n\n\t\t/*\n\t\t * Anything else is fatal, maybe with a core dump.\n\t\t */\n\t\tcurrent->flags |= PF_SIGNALED;\n\n\t\tif (sig_kernel_coredump(signr)) {\n\t\t\tif (print_fatal_signals)\n\t\t\t\tprint_fatal_signal(ksig->info.si_signo);\n\t\t\tproc_coredump_connector(current);\n\t\t\t/*\n\t\t\t * If it was able to dump core, this kills all\n\t\t\t * other threads in the group and synchronizes with\n\t\t\t * their demise.  If we lost the race with another\n\t\t\t * thread getting here, it set group_exit_code\n\t\t\t * first and our do_group_exit call below will use\n\t\t\t * that value and ignore the one we pass it.\n\t\t\t */\n\t\t\tdo_coredump(&ksig->info);\n\t\t}\n\n\t\t/*\n\t\t * Death signals, no core dump.\n\t\t */\n\t\tdo_group_exit(ksig->info.si_signo);\n\t\t/* NOTREACHED */\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n\n\tksig->sig = signr;\n\treturn ksig->sig > 0;\n}\n\n/**\n * signal_delivered - \n * @ksig:\t\tkernel signal struct\n * @stepping:\t\tnonzero if debugger single-step or block-step in use\n *\n * This function should be called when a signal has successfully been\n * delivered. It updates the blocked signals accordingly (@ksig->ka.sa.sa_mask\n * is always blocked, and the signal itself is blocked unless %SA_NODEFER\n * is set in @ksig->ka.sa.sa_flags.  Tracing is notified.\n */\nstatic void signal_delivered(struct ksignal *ksig, int stepping)\n{\n\tsigset_t blocked;\n\n\t/* A signal was successfully delivered, and the\n\t   saved sigmask was stored on the signal frame,\n\t   and will be restored by sigreturn.  So we can\n\t   simply clear the restore sigmask flag.  */\n\tclear_restore_sigmask();\n\n\tsigorsets(&blocked, &current->blocked, &ksig->ka.sa.sa_mask);\n\tif (!(ksig->ka.sa.sa_flags & SA_NODEFER))\n\t\tsigaddset(&blocked, ksig->sig);\n\tset_current_blocked(&blocked);\n\ttracehook_signal_handler(stepping);\n}\n\nvoid signal_setup_done(int failed, struct ksignal *ksig, int stepping)\n{\n\tif (failed)\n\t\tforce_sigsegv(ksig->sig);\n\telse\n\t\tsignal_delivered(ksig, stepping);\n}\n\n/*\n * It could be that complete_signal() picked us to notify about the\n * group-wide signal. Other threads should be notified now to take\n * the shared signals in @which since we will not.\n */\nstatic void retarget_shared_pending(struct task_struct *tsk, sigset_t *which)\n{\n\tsigset_t retarget;\n\tstruct task_struct *t;\n\n\tsigandsets(&retarget, &tsk->signal->shared_pending.signal, which);\n\tif (sigisemptyset(&retarget))\n\t\treturn;\n\n\tt = tsk;\n\twhile_each_thread(tsk, t) {\n\t\tif (t->flags & PF_EXITING)\n\t\t\tcontinue;\n\n\t\tif (!has_pending_signals(&retarget, &t->blocked))\n\t\t\tcontinue;\n\t\t/* Remove the signals this thread can handle. */\n\t\tsigandsets(&retarget, &retarget, &t->blocked);\n\n\t\tif (!signal_pending(t))\n\t\t\tsignal_wake_up(t, 0);\n\n\t\tif (sigisemptyset(&retarget))\n\t\t\tbreak;\n\t}\n}\n\nvoid exit_signals(struct task_struct *tsk)\n{\n\tint group_stop = 0;\n\tsigset_t unblocked;\n\n\t/*\n\t * @tsk is about to have PF_EXITING set - lock out users which\n\t * expect stable threadgroup.\n\t */\n\tcgroup_threadgroup_change_begin(tsk);\n\n\tif (thread_group_empty(tsk) || signal_group_exit(tsk->signal)) {\n\t\ttsk->flags |= PF_EXITING;\n\t\tcgroup_threadgroup_change_end(tsk);\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t/*\n\t * From now this task is not visible for group-wide signals,\n\t * see wants_signal(), do_signal_stop().\n\t */\n\ttsk->flags |= PF_EXITING;\n\n\tcgroup_threadgroup_change_end(tsk);\n\n\tif (!signal_pending(tsk))\n\t\tgoto out;\n\n\tunblocked = tsk->blocked;\n\tsignotset(&unblocked);\n\tretarget_shared_pending(tsk, &unblocked);\n\n\tif (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&\n\t    task_participate_group_stop(tsk))\n\t\tgroup_stop = CLD_STOPPED;\nout:\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t/*\n\t * If group stop has completed, deliver the notification.  This\n\t * should always go to the real parent of the group leader.\n\t */\n\tif (unlikely(group_stop)) {\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(tsk, false, group_stop);\n\t\tread_unlock(&tasklist_lock);\n\t}\n}\n\n/*\n * System call entry points.\n */\n\n/**\n *  sys_restart_syscall - restart a system call\n */\nSYSCALL_DEFINE0(restart_syscall)\n{\n\tstruct restart_block *restart = &current->restart_block;\n\treturn restart->fn(restart);\n}\n\nlong do_no_restart_syscall(struct restart_block *param)\n{\n\treturn -EINTR;\n}\n\nstatic void __set_task_blocked(struct task_struct *tsk, const sigset_t *newset)\n{\n\tif (signal_pending(tsk) && !thread_group_empty(tsk)) {\n\t\tsigset_t newblocked;\n\t\t/* A set of now blocked but previously unblocked signals. */\n\t\tsigandnsets(&newblocked, newset, &current->blocked);\n\t\tretarget_shared_pending(tsk, &newblocked);\n\t}\n\ttsk->blocked = *newset;\n\trecalc_sigpending();\n}\n\n/**\n * set_current_blocked - change current->blocked mask\n * @newset: new mask\n *\n * It is wrong to change ->blocked directly, this helper should be used\n * to ensure the process can't miss a shared signal we are going to block.\n */\nvoid set_current_blocked(sigset_t *newset)\n{\n\tsigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t__set_current_blocked(newset);\n}\n\nvoid __set_current_blocked(const sigset_t *newset)\n{\n\tstruct task_struct *tsk = current;\n\n\t/*\n\t * In case the signal mask hasn't changed, there is nothing we need\n\t * to do. The current->blocked shouldn't be modified by other task.\n\t */\n\tif (sigequalsets(&tsk->blocked, newset))\n\t\treturn;\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t__set_task_blocked(tsk, newset);\n\tspin_unlock_irq(&tsk->sighand->siglock);\n}\n\n/*\n * This is also useful for kernel threads that want to temporarily\n * (or permanently) block certain signals.\n *\n * NOTE! Unlike the user-mode sys_sigprocmask(), the kernel\n * interface happily blocks \"unblockable\" signals like SIGKILL\n * and friends.\n */\nint sigprocmask(int how, sigset_t *set, sigset_t *oldset)\n{\n\tstruct task_struct *tsk = current;\n\tsigset_t newset;\n\n\t/* Lockless, only current can change ->blocked, never from irq */\n\tif (oldset)\n\t\t*oldset = tsk->blocked;\n\n\tswitch (how) {\n\tcase SIG_BLOCK:\n\t\tsigorsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_UNBLOCK:\n\t\tsigandnsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_SETMASK:\n\t\tnewset = *set;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t__set_current_blocked(&newset);\n\treturn 0;\n}\nEXPORT_SYMBOL(sigprocmask);\n\n/*\n * The api helps set app-provided sigmasks.\n *\n * This is useful for syscalls such as ppoll, pselect, io_pgetevents and\n * epoll_pwait where a new sigmask is passed from userland for the syscalls.\n *\n * Note that it does set_restore_sigmask() in advance, so it must be always\n * paired with restore_saved_sigmask_unless() before return from syscall.\n */\nint set_user_sigmask(const sigset_t __user *umask, size_t sigsetsize)\n{\n\tsigset_t kmask;\n\n\tif (!umask)\n\t\treturn 0;\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&kmask, umask, sizeof(sigset_t)))\n\t\treturn -EFAULT;\n\n\tset_restore_sigmask();\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(&kmask);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nint set_compat_user_sigmask(const compat_sigset_t __user *umask,\n\t\t\t    size_t sigsetsize)\n{\n\tsigset_t kmask;\n\n\tif (!umask)\n\t\treturn 0;\n\tif (sigsetsize != sizeof(compat_sigset_t))\n\t\treturn -EINVAL;\n\tif (get_compat_sigset(&kmask, umask))\n\t\treturn -EFAULT;\n\n\tset_restore_sigmask();\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(&kmask);\n\n\treturn 0;\n}\n#endif\n\n/**\n *  sys_rt_sigprocmask - change the list of currently blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: stores pending signals\n *  @oset: previous value of signal mask if non-null\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigprocmask, int, how, sigset_t __user *, nset,\n\t\tsigset_t __user *, oset, size_t, sigsetsize)\n{\n\tsigset_t old_set, new_set;\n\tint error;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\told_set = current->blocked;\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigprocmask, int, how, compat_sigset_t __user *, nset,\n\t\tcompat_sigset_t __user *, oset, compat_size_t, sigsetsize)\n{\n\tsigset_t old_set = current->blocked;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (nset) {\n\t\tsigset_t new_set;\n\t\tint error;\n\t\tif (get_compat_sigset(&new_set, nset))\n\t\t\treturn -EFAULT;\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\treturn oset ? put_compat_sigset(oset, &old_set, sizeof(*oset)) : 0;\n}\n#endif\n\nstatic void do_sigpending(sigset_t *set)\n{\n\tspin_lock_irq(&current->sighand->siglock);\n\tsigorsets(set, &current->pending.signal,\n\t\t  &current->signal->shared_pending.signal);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\t/* Outside the lock because only this thread touches it.  */\n\tsigandsets(set, &current->blocked, set);\n}\n\n/**\n *  sys_rt_sigpending - examine a pending signal that has been raised\n *\t\t\twhile blocked\n *  @uset: stores pending signals\n *  @sigsetsize: size of sigset_t type or larger\n */\nSYSCALL_DEFINE2(rt_sigpending, sigset_t __user *, uset, size_t, sigsetsize)\n{\n\tsigset_t set;\n\n\tif (sigsetsize > sizeof(*uset))\n\t\treturn -EINVAL;\n\n\tdo_sigpending(&set);\n\n\tif (copy_to_user(uset, &set, sigsetsize))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigpending, compat_sigset_t __user *, uset,\n\t\tcompat_size_t, sigsetsize)\n{\n\tsigset_t set;\n\n\tif (sigsetsize > sizeof(*uset))\n\t\treturn -EINVAL;\n\n\tdo_sigpending(&set);\n\n\treturn put_compat_sigset(uset, &set, sigsetsize);\n}\n#endif\n\nstatic const struct {\n\tunsigned char limit, layout;\n} sig_sicodes[] = {\n\t[SIGILL]  = { NSIGILL,  SIL_FAULT },\n\t[SIGFPE]  = { NSIGFPE,  SIL_FAULT },\n\t[SIGSEGV] = { NSIGSEGV, SIL_FAULT },\n\t[SIGBUS]  = { NSIGBUS,  SIL_FAULT },\n\t[SIGTRAP] = { NSIGTRAP, SIL_FAULT },\n#if defined(SIGEMT)\n\t[SIGEMT]  = { NSIGEMT,  SIL_FAULT },\n#endif\n\t[SIGCHLD] = { NSIGCHLD, SIL_CHLD },\n\t[SIGPOLL] = { NSIGPOLL, SIL_POLL },\n\t[SIGSYS]  = { NSIGSYS,  SIL_SYS },\n};\n\nstatic bool known_siginfo_layout(unsigned sig, int si_code)\n{\n\tif (si_code == SI_KERNEL)\n\t\treturn true;\n\telse if ((si_code > SI_USER)) {\n\t\tif (sig_specific_sicodes(sig)) {\n\t\t\tif (si_code <= sig_sicodes[sig].limit)\n\t\t\t\treturn true;\n\t\t}\n\t\telse if (si_code <= NSIGPOLL)\n\t\t\treturn true;\n\t}\n\telse if (si_code >= SI_DETHREAD)\n\t\treturn true;\n\telse if (si_code == SI_ASYNCNL)\n\t\treturn true;\n\treturn false;\n}\n\nenum siginfo_layout siginfo_layout(unsigned sig, int si_code)\n{\n\tenum siginfo_layout layout = SIL_KILL;\n\tif ((si_code > SI_USER) && (si_code < SI_KERNEL)) {\n\t\tif ((sig < ARRAY_SIZE(sig_sicodes)) &&\n\t\t    (si_code <= sig_sicodes[sig].limit)) {\n\t\t\tlayout = sig_sicodes[sig].layout;\n\t\t\t/* Handle the exceptions */\n\t\t\tif ((sig == SIGBUS) &&\n\t\t\t    (si_code >= BUS_MCEERR_AR) && (si_code <= BUS_MCEERR_AO))\n\t\t\t\tlayout = SIL_FAULT_MCEERR;\n\t\t\telse if ((sig == SIGSEGV) && (si_code == SEGV_BNDERR))\n\t\t\t\tlayout = SIL_FAULT_BNDERR;\n#ifdef SEGV_PKUERR\n\t\t\telse if ((sig == SIGSEGV) && (si_code == SEGV_PKUERR))\n\t\t\t\tlayout = SIL_FAULT_PKUERR;\n#endif\n\t\t}\n\t\telse if (si_code <= NSIGPOLL)\n\t\t\tlayout = SIL_POLL;\n\t} else {\n\t\tif (si_code == SI_TIMER)\n\t\t\tlayout = SIL_TIMER;\n\t\telse if (si_code == SI_SIGIO)\n\t\t\tlayout = SIL_POLL;\n\t\telse if (si_code < 0)\n\t\t\tlayout = SIL_RT;\n\t}\n\treturn layout;\n}\n\nstatic inline char __user *si_expansion(const siginfo_t __user *info)\n{\n\treturn ((char __user *)info) + sizeof(struct kernel_siginfo);\n}\n\nint copy_siginfo_to_user(siginfo_t __user *to, const kernel_siginfo_t *from)\n{\n\tchar __user *expansion = si_expansion(to);\n\tif (copy_to_user(to, from , sizeof(struct kernel_siginfo)))\n\t\treturn -EFAULT;\n\tif (clear_user(expansion, SI_EXPANSION_SIZE))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int post_copy_siginfo_from_user(kernel_siginfo_t *info,\n\t\t\t\t       const siginfo_t __user *from)\n{\n\tif (unlikely(!known_siginfo_layout(info->si_signo, info->si_code))) {\n\t\tchar __user *expansion = si_expansion(from);\n\t\tchar buf[SI_EXPANSION_SIZE];\n\t\tint i;\n\t\t/*\n\t\t * An unknown si_code might need more than\n\t\t * sizeof(struct kernel_siginfo) bytes.  Verify all of the\n\t\t * extra bytes are 0.  This guarantees copy_siginfo_to_user\n\t\t * will return this data to userspace exactly.\n\t\t */\n\t\tif (copy_from_user(&buf, expansion, SI_EXPANSION_SIZE))\n\t\t\treturn -EFAULT;\n\t\tfor (i = 0; i < SI_EXPANSION_SIZE; i++) {\n\t\t\tif (buf[i] != 0)\n\t\t\t\treturn -E2BIG;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int __copy_siginfo_from_user(int signo, kernel_siginfo_t *to,\n\t\t\t\t    const siginfo_t __user *from)\n{\n\tif (copy_from_user(to, from, sizeof(struct kernel_siginfo)))\n\t\treturn -EFAULT;\n\tto->si_signo = signo;\n\treturn post_copy_siginfo_from_user(to, from);\n}\n\nint copy_siginfo_from_user(kernel_siginfo_t *to, const siginfo_t __user *from)\n{\n\tif (copy_from_user(to, from, sizeof(struct kernel_siginfo)))\n\t\treturn -EFAULT;\n\treturn post_copy_siginfo_from_user(to, from);\n}\n\n#ifdef CONFIG_COMPAT\nint copy_siginfo_to_user32(struct compat_siginfo __user *to,\n\t\t\t   const struct kernel_siginfo *from)\n#if defined(CONFIG_X86_X32_ABI) || defined(CONFIG_IA32_EMULATION)\n{\n\treturn __copy_siginfo_to_user32(to, from, in_x32_syscall());\n}\nint __copy_siginfo_to_user32(struct compat_siginfo __user *to,\n\t\t\t     const struct kernel_siginfo *from, bool x32_ABI)\n#endif\n{\n\tstruct compat_siginfo new;\n\tmemset(&new, 0, sizeof(new));\n\n\tnew.si_signo = from->si_signo;\n\tnew.si_errno = from->si_errno;\n\tnew.si_code  = from->si_code;\n\tswitch(siginfo_layout(from->si_signo, from->si_code)) {\n\tcase SIL_KILL:\n\t\tnew.si_pid = from->si_pid;\n\t\tnew.si_uid = from->si_uid;\n\t\tbreak;\n\tcase SIL_TIMER:\n\t\tnew.si_tid     = from->si_tid;\n\t\tnew.si_overrun = from->si_overrun;\n\t\tnew.si_int     = from->si_int;\n\t\tbreak;\n\tcase SIL_POLL:\n\t\tnew.si_band = from->si_band;\n\t\tnew.si_fd   = from->si_fd;\n\t\tbreak;\n\tcase SIL_FAULT:\n\t\tnew.si_addr = ptr_to_compat(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tnew.si_trapno = from->si_trapno;\n#endif\n\t\tbreak;\n\tcase SIL_FAULT_MCEERR:\n\t\tnew.si_addr = ptr_to_compat(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tnew.si_trapno = from->si_trapno;\n#endif\n\t\tnew.si_addr_lsb = from->si_addr_lsb;\n\t\tbreak;\n\tcase SIL_FAULT_BNDERR:\n\t\tnew.si_addr = ptr_to_compat(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tnew.si_trapno = from->si_trapno;\n#endif\n\t\tnew.si_lower = ptr_to_compat(from->si_lower);\n\t\tnew.si_upper = ptr_to_compat(from->si_upper);\n\t\tbreak;\n\tcase SIL_FAULT_PKUERR:\n\t\tnew.si_addr = ptr_to_compat(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tnew.si_trapno = from->si_trapno;\n#endif\n\t\tnew.si_pkey = from->si_pkey;\n\t\tbreak;\n\tcase SIL_CHLD:\n\t\tnew.si_pid    = from->si_pid;\n\t\tnew.si_uid    = from->si_uid;\n\t\tnew.si_status = from->si_status;\n#ifdef CONFIG_X86_X32_ABI\n\t\tif (x32_ABI) {\n\t\t\tnew._sifields._sigchld_x32._utime = from->si_utime;\n\t\t\tnew._sifields._sigchld_x32._stime = from->si_stime;\n\t\t} else\n#endif\n\t\t{\n\t\t\tnew.si_utime = from->si_utime;\n\t\t\tnew.si_stime = from->si_stime;\n\t\t}\n\t\tbreak;\n\tcase SIL_RT:\n\t\tnew.si_pid = from->si_pid;\n\t\tnew.si_uid = from->si_uid;\n\t\tnew.si_int = from->si_int;\n\t\tbreak;\n\tcase SIL_SYS:\n\t\tnew.si_call_addr = ptr_to_compat(from->si_call_addr);\n\t\tnew.si_syscall   = from->si_syscall;\n\t\tnew.si_arch      = from->si_arch;\n\t\tbreak;\n\t}\n\n\tif (copy_to_user(to, &new, sizeof(struct compat_siginfo)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int post_copy_siginfo_from_user32(kernel_siginfo_t *to,\n\t\t\t\t\t const struct compat_siginfo *from)\n{\n\tclear_siginfo(to);\n\tto->si_signo = from->si_signo;\n\tto->si_errno = from->si_errno;\n\tto->si_code  = from->si_code;\n\tswitch(siginfo_layout(from->si_signo, from->si_code)) {\n\tcase SIL_KILL:\n\t\tto->si_pid = from->si_pid;\n\t\tto->si_uid = from->si_uid;\n\t\tbreak;\n\tcase SIL_TIMER:\n\t\tto->si_tid     = from->si_tid;\n\t\tto->si_overrun = from->si_overrun;\n\t\tto->si_int     = from->si_int;\n\t\tbreak;\n\tcase SIL_POLL:\n\t\tto->si_band = from->si_band;\n\t\tto->si_fd   = from->si_fd;\n\t\tbreak;\n\tcase SIL_FAULT:\n\t\tto->si_addr = compat_ptr(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tto->si_trapno = from->si_trapno;\n#endif\n\t\tbreak;\n\tcase SIL_FAULT_MCEERR:\n\t\tto->si_addr = compat_ptr(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tto->si_trapno = from->si_trapno;\n#endif\n\t\tto->si_addr_lsb = from->si_addr_lsb;\n\t\tbreak;\n\tcase SIL_FAULT_BNDERR:\n\t\tto->si_addr = compat_ptr(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tto->si_trapno = from->si_trapno;\n#endif\n\t\tto->si_lower = compat_ptr(from->si_lower);\n\t\tto->si_upper = compat_ptr(from->si_upper);\n\t\tbreak;\n\tcase SIL_FAULT_PKUERR:\n\t\tto->si_addr = compat_ptr(from->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\tto->si_trapno = from->si_trapno;\n#endif\n\t\tto->si_pkey = from->si_pkey;\n\t\tbreak;\n\tcase SIL_CHLD:\n\t\tto->si_pid    = from->si_pid;\n\t\tto->si_uid    = from->si_uid;\n\t\tto->si_status = from->si_status;\n#ifdef CONFIG_X86_X32_ABI\n\t\tif (in_x32_syscall()) {\n\t\t\tto->si_utime = from->_sifields._sigchld_x32._utime;\n\t\t\tto->si_stime = from->_sifields._sigchld_x32._stime;\n\t\t} else\n#endif\n\t\t{\n\t\t\tto->si_utime = from->si_utime;\n\t\t\tto->si_stime = from->si_stime;\n\t\t}\n\t\tbreak;\n\tcase SIL_RT:\n\t\tto->si_pid = from->si_pid;\n\t\tto->si_uid = from->si_uid;\n\t\tto->si_int = from->si_int;\n\t\tbreak;\n\tcase SIL_SYS:\n\t\tto->si_call_addr = compat_ptr(from->si_call_addr);\n\t\tto->si_syscall   = from->si_syscall;\n\t\tto->si_arch      = from->si_arch;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int __copy_siginfo_from_user32(int signo, struct kernel_siginfo *to,\n\t\t\t\t      const struct compat_siginfo __user *ufrom)\n{\n\tstruct compat_siginfo from;\n\n\tif (copy_from_user(&from, ufrom, sizeof(struct compat_siginfo)))\n\t\treturn -EFAULT;\n\n\tfrom.si_signo = signo;\n\treturn post_copy_siginfo_from_user32(to, &from);\n}\n\nint copy_siginfo_from_user32(struct kernel_siginfo *to,\n\t\t\t     const struct compat_siginfo __user *ufrom)\n{\n\tstruct compat_siginfo from;\n\n\tif (copy_from_user(&from, ufrom, sizeof(struct compat_siginfo)))\n\t\treturn -EFAULT;\n\n\treturn post_copy_siginfo_from_user32(to, &from);\n}\n#endif /* CONFIG_COMPAT */\n\n/**\n *  do_sigtimedwait - wait for queued signals specified in @which\n *  @which: queued signals to wait for\n *  @info: if non-null, the signal's siginfo is returned here\n *  @ts: upper bound on process time suspension\n */\nstatic int do_sigtimedwait(const sigset_t *which, kernel_siginfo_t *info,\n\t\t    const struct timespec64 *ts)\n{\n\tktime_t *to = NULL, timeout = KTIME_MAX;\n\tstruct task_struct *tsk = current;\n\tsigset_t mask = *which;\n\tint sig, ret = 0;\n\n\tif (ts) {\n\t\tif (!timespec64_valid(ts))\n\t\t\treturn -EINVAL;\n\t\ttimeout = timespec64_to_ktime(*ts);\n\t\tto = &timeout;\n\t}\n\n\t/*\n\t * Invert the set of allowed signals to get those we want to block.\n\t */\n\tsigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\tsignotset(&mask);\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\tsig = dequeue_signal(tsk, &mask, info);\n\tif (!sig && timeout) {\n\t\t/*\n\t\t * None ready, temporarily unblock those we're interested\n\t\t * while we are sleeping in so that we'll be awakened when\n\t\t * they arrive. Unblocking is always fine, we can avoid\n\t\t * set_current_blocked().\n\t\t */\n\t\ttsk->real_blocked = tsk->blocked;\n\t\tsigandsets(&tsk->blocked, &tsk->blocked, &mask);\n\t\trecalc_sigpending();\n\t\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tret = freezable_schedule_hrtimeout_range(to, tsk->timer_slack_ns,\n\t\t\t\t\t\t\t HRTIMER_MODE_REL);\n\t\tspin_lock_irq(&tsk->sighand->siglock);\n\t\t__set_task_blocked(tsk, &tsk->real_blocked);\n\t\tsigemptyset(&tsk->real_blocked);\n\t\tsig = dequeue_signal(tsk, &mask, info);\n\t}\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\tif (sig)\n\t\treturn sig;\n\treturn ret ? -EINTR : -EAGAIN;\n}\n\n/**\n *  sys_rt_sigtimedwait - synchronously wait for queued signals specified\n *\t\t\tin @uthese\n *  @uthese: queued signals to wait for\n *  @uinfo: if non-null, the signal's siginfo is returned here\n *  @uts: upper bound on process time suspension\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigtimedwait, const sigset_t __user *, uthese,\n\t\tsiginfo_t __user *, uinfo,\n\t\tconst struct __kernel_timespec __user *, uts,\n\t\tsize_t, sigsetsize)\n{\n\tsigset_t these;\n\tstruct timespec64 ts;\n\tkernel_siginfo_t info;\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&these, uthese, sizeof(these)))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (get_timespec64(&ts, uts))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nSYSCALL_DEFINE4(rt_sigtimedwait_time32, const sigset_t __user *, uthese,\n\t\tsiginfo_t __user *, uinfo,\n\t\tconst struct old_timespec32 __user *, uts,\n\t\tsize_t, sigsetsize)\n{\n\tsigset_t these;\n\tstruct timespec64 ts;\n\tkernel_siginfo_t info;\n\tint ret;\n\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&these, uthese, sizeof(these)))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (get_old_timespec32(&ts, uts))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n#endif\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigtimedwait_time64, compat_sigset_t __user *, uthese,\n\t\tstruct compat_siginfo __user *, uinfo,\n\t\tstruct __kernel_timespec __user *, uts, compat_size_t, sigsetsize)\n{\n\tsigset_t s;\n\tstruct timespec64 t;\n\tkernel_siginfo_t info;\n\tlong ret;\n\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (get_compat_sigset(&s, uthese))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (get_timespec64(&t, uts))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&s, &info, uts ? &t : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user32(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nCOMPAT_SYSCALL_DEFINE4(rt_sigtimedwait_time32, compat_sigset_t __user *, uthese,\n\t\tstruct compat_siginfo __user *, uinfo,\n\t\tstruct old_timespec32 __user *, uts, compat_size_t, sigsetsize)\n{\n\tsigset_t s;\n\tstruct timespec64 t;\n\tkernel_siginfo_t info;\n\tlong ret;\n\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (get_compat_sigset(&s, uthese))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (get_old_timespec32(&t, uts))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&s, &info, uts ? &t : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user32(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n#endif\n#endif\n\nstatic inline void prepare_kill_siginfo(int sig, struct kernel_siginfo *info)\n{\n\tclear_siginfo(info);\n\tinfo->si_signo = sig;\n\tinfo->si_errno = 0;\n\tinfo->si_code = SI_USER;\n\tinfo->si_pid = task_tgid_vnr(current);\n\tinfo->si_uid = from_kuid_munged(current_user_ns(), current_uid());\n}\n\n/**\n *  sys_kill - send a signal to a process\n *  @pid: the PID of the process\n *  @sig: signal to be sent\n */\nSYSCALL_DEFINE2(kill, pid_t, pid, int, sig)\n{\n\tstruct kernel_siginfo info;\n\n\tprepare_kill_siginfo(sig, &info);\n\n\treturn kill_something_info(sig, &info, pid);\n}\n\n/*\n * Verify that the signaler and signalee either are in the same pid namespace\n * or that the signaler's pid namespace is an ancestor of the signalee's pid\n * namespace.\n */\nstatic bool access_pidfd_pidns(struct pid *pid)\n{\n\tstruct pid_namespace *active = task_active_pid_ns(current);\n\tstruct pid_namespace *p = ns_of_pid(pid);\n\n\tfor (;;) {\n\t\tif (!p)\n\t\t\treturn false;\n\t\tif (p == active)\n\t\t\tbreak;\n\t\tp = p->parent;\n\t}\n\n\treturn true;\n}\n\nstatic int copy_siginfo_from_user_any(kernel_siginfo_t *kinfo, siginfo_t *info)\n{\n#ifdef CONFIG_COMPAT\n\t/*\n\t * Avoid hooking up compat syscalls and instead handle necessary\n\t * conversions here. Note, this is a stop-gap measure and should not be\n\t * considered a generic solution.\n\t */\n\tif (in_compat_syscall())\n\t\treturn copy_siginfo_from_user32(\n\t\t\tkinfo, (struct compat_siginfo __user *)info);\n#endif\n\treturn copy_siginfo_from_user(kinfo, info);\n}\n\nstatic struct pid *pidfd_to_pid(const struct file *file)\n{\n\tstruct pid *pid;\n\n\tpid = pidfd_pid(file);\n\tif (!IS_ERR(pid))\n\t\treturn pid;\n\n\treturn tgid_pidfd_to_pid(file);\n}\n\n/**\n * sys_pidfd_send_signal - Signal a process through a pidfd\n * @pidfd:  file descriptor of the process\n * @sig:    signal to send\n * @info:   signal info\n * @flags:  future flags\n *\n * The syscall currently only signals via PIDTYPE_PID which covers\n * kill(<positive-pid>, <signal>. It does not signal threads or process\n * groups.\n * In order to extend the syscall to threads and process groups the @flags\n * argument should be used. In essence, the @flags argument will determine\n * what is signaled and not the file descriptor itself. Put in other words,\n * grouping is a property of the flags argument not a property of the file\n * descriptor.\n *\n * Return: 0 on success, negative errno on failure\n */\nSYSCALL_DEFINE4(pidfd_send_signal, int, pidfd, int, sig,\n\t\tsiginfo_t __user *, info, unsigned int, flags)\n{\n\tint ret;\n\tstruct fd f;\n\tstruct pid *pid;\n\tkernel_siginfo_t kinfo;\n\n\t/* Enforce flags be set to 0 until we add an extension. */\n\tif (flags)\n\t\treturn -EINVAL;\n\n\tf = fdget(pidfd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\t/* Is this a pidfd? */\n\tpid = pidfd_to_pid(f.file);\n\tif (IS_ERR(pid)) {\n\t\tret = PTR_ERR(pid);\n\t\tgoto err;\n\t}\n\n\tret = -EINVAL;\n\tif (!access_pidfd_pidns(pid))\n\t\tgoto err;\n\n\tif (info) {\n\t\tret = copy_siginfo_from_user_any(&kinfo, info);\n\t\tif (unlikely(ret))\n\t\t\tgoto err;\n\n\t\tret = -EINVAL;\n\t\tif (unlikely(sig != kinfo.si_signo))\n\t\t\tgoto err;\n\n\t\t/* Only allow sending arbitrary signals to yourself. */\n\t\tret = -EPERM;\n\t\tif ((task_pid(current) != pid) &&\n\t\t    (kinfo.si_code >= 0 || kinfo.si_code == SI_TKILL))\n\t\t\tgoto err;\n\t} else {\n\t\tprepare_kill_siginfo(sig, &kinfo);\n\t}\n\n\tret = kill_pid_info(sig, &kinfo, pid);\n\nerr:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int\ndo_send_specific(pid_t tgid, pid_t pid, int sig, struct kernel_siginfo *info)\n{\n\tstruct task_struct *p;\n\tint error = -ESRCH;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p && (tgid <= 0 || task_tgid_vnr(p) == tgid)) {\n\t\terror = check_kill_permission(sig, info, p);\n\t\t/*\n\t\t * The null signal is a permissions and process existence\n\t\t * probe.  No signal is actually delivered.\n\t\t */\n\t\tif (!error && sig) {\n\t\t\terror = do_send_sig_info(sig, info, p, PIDTYPE_PID);\n\t\t\t/*\n\t\t\t * If lock_task_sighand() failed we pretend the task\n\t\t\t * dies after receiving the signal. The window is tiny,\n\t\t\t * and the signal is private anyway.\n\t\t\t */\n\t\t\tif (unlikely(error == -ESRCH))\n\t\t\t\terror = 0;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nstatic int do_tkill(pid_t tgid, pid_t pid, int sig)\n{\n\tstruct kernel_siginfo info;\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_TKILL;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn do_send_specific(tgid, pid, sig, &info);\n}\n\n/**\n *  sys_tgkill - send signal to one specific thread\n *  @tgid: the thread group ID of the thread\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *\n *  This syscall also checks the @tgid and returns -ESRCH even if the PID\n *  exists but it's not belonging to the target process anymore. This\n *  method solves the problem of threads exiting and PIDs getting reused.\n */\nSYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(tgid, pid, sig);\n}\n\n/**\n *  sys_tkill - send signal to one specific task\n *  @pid: the PID of the task\n *  @sig: signal to be sent\n *\n *  Send a signal to only one task, even if it's a CLONE_THREAD task.\n */\nSYSCALL_DEFINE2(tkill, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(0, pid, sig);\n}\n\nstatic int do_rt_sigqueueinfo(pid_t pid, int sig, kernel_siginfo_t *info)\n{\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\n\t    (task_pid_vnr(current) != pid))\n\t\treturn -EPERM;\n\n\t/* POSIX.1b doesn't mention process groups.  */\n\treturn kill_proc_info(sig, info, pid);\n}\n\n/**\n *  sys_rt_sigqueueinfo - send signal information to a signal\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *  @uinfo: signal info to be sent\n */\nSYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tkernel_siginfo_t info;\n\tint ret = __copy_siginfo_from_user(sig, &info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_sigqueueinfo(pid, sig, &info);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(rt_sigqueueinfo,\n\t\t\tcompat_pid_t, pid,\n\t\t\tint, sig,\n\t\t\tstruct compat_siginfo __user *, uinfo)\n{\n\tkernel_siginfo_t info;\n\tint ret = __copy_siginfo_from_user32(sig, &info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_sigqueueinfo(pid, sig, &info);\n}\n#endif\n\nstatic int do_rt_tgsigqueueinfo(pid_t tgid, pid_t pid, int sig, kernel_siginfo_t *info)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\n\t    (task_pid_vnr(current) != pid))\n\t\treturn -EPERM;\n\n\treturn do_send_specific(tgid, pid, sig, info);\n}\n\nSYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tkernel_siginfo_t info;\n\tint ret = __copy_siginfo_from_user(sig, &info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_tgsigqueueinfo,\n\t\t\tcompat_pid_t, tgid,\n\t\t\tcompat_pid_t, pid,\n\t\t\tint, sig,\n\t\t\tstruct compat_siginfo __user *, uinfo)\n{\n\tkernel_siginfo_t info;\n\tint ret = __copy_siginfo_from_user32(sig, &info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n#endif\n\n/*\n * For kthreads only, must not be used if cloned with CLONE_SIGHAND\n */\nvoid kernel_sigaction(int sig, __sighandler_t action)\n{\n\tspin_lock_irq(&current->sighand->siglock);\n\tcurrent->sighand->action[sig - 1].sa.sa_handler = action;\n\tif (action == SIG_IGN) {\n\t\tsigset_t mask;\n\n\t\tsigemptyset(&mask);\n\t\tsigaddset(&mask, sig);\n\n\t\tflush_sigqueue_mask(&mask, &current->signal->shared_pending);\n\t\tflush_sigqueue_mask(&mask, &current->pending);\n\t\trecalc_sigpending();\n\t}\n\tspin_unlock_irq(&current->sighand->siglock);\n}\nEXPORT_SYMBOL(kernel_sigaction);\n\nvoid __weak sigaction_compat_abi(struct k_sigaction *act,\n\t\tstruct k_sigaction *oact)\n{\n}\n\nint do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)\n{\n\tstruct task_struct *p = current, *t;\n\tstruct k_sigaction *k;\n\tsigset_t mask;\n\n\tif (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))\n\t\treturn -EINVAL;\n\n\tk = &p->sighand->action[sig-1];\n\n\tspin_lock_irq(&p->sighand->siglock);\n\tif (oact)\n\t\t*oact = *k;\n\n\tsigaction_compat_abi(act, oact);\n\n\tif (act) {\n\t\tsigdelsetmask(&act->sa.sa_mask,\n\t\t\t      sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t\t*k = *act;\n\t\t/*\n\t\t * POSIX 3.3.1.3:\n\t\t *  \"Setting a signal action to SIG_IGN for a signal that is\n\t\t *   pending shall cause the pending signal to be discarded,\n\t\t *   whether or not it is blocked.\"\n\t\t *\n\t\t *  \"Setting a signal action to SIG_DFL for a signal that is\n\t\t *   pending and whose default action is to ignore the signal\n\t\t *   (for example, SIGCHLD), shall cause the pending signal to\n\t\t *   be discarded, whether or not it is blocked\"\n\t\t */\n\t\tif (sig_handler_ignored(sig_handler(p, sig), sig)) {\n\t\t\tsigemptyset(&mask);\n\t\t\tsigaddset(&mask, sig);\n\t\t\tflush_sigqueue_mask(&mask, &p->signal->shared_pending);\n\t\t\tfor_each_thread(p, t)\n\t\t\t\tflush_sigqueue_mask(&mask, &t->pending);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&p->sighand->siglock);\n\treturn 0;\n}\n\nstatic int\ndo_sigaltstack (const stack_t *ss, stack_t *oss, unsigned long sp,\n\t\tsize_t min_ss_size)\n{\n\tstruct task_struct *t = current;\n\n\tif (oss) {\n\t\tmemset(oss, 0, sizeof(stack_t));\n\t\toss->ss_sp = (void __user *) t->sas_ss_sp;\n\t\toss->ss_size = t->sas_ss_size;\n\t\toss->ss_flags = sas_ss_flags(sp) |\n\t\t\t(current->sas_ss_flags & SS_FLAG_BITS);\n\t}\n\n\tif (ss) {\n\t\tvoid __user *ss_sp = ss->ss_sp;\n\t\tsize_t ss_size = ss->ss_size;\n\t\tunsigned ss_flags = ss->ss_flags;\n\t\tint ss_mode;\n\n\t\tif (unlikely(on_sig_stack(sp)))\n\t\t\treturn -EPERM;\n\n\t\tss_mode = ss_flags & ~SS_FLAG_BITS;\n\t\tif (unlikely(ss_mode != SS_DISABLE && ss_mode != SS_ONSTACK &&\n\t\t\t\tss_mode != 0))\n\t\t\treturn -EINVAL;\n\n\t\tif (ss_mode == SS_DISABLE) {\n\t\t\tss_size = 0;\n\t\t\tss_sp = NULL;\n\t\t} else {\n\t\t\tif (unlikely(ss_size < min_ss_size))\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tt->sas_ss_sp = (unsigned long) ss_sp;\n\t\tt->sas_ss_size = ss_size;\n\t\tt->sas_ss_flags = ss_flags;\n\t}\n\treturn 0;\n}\n\nSYSCALL_DEFINE2(sigaltstack,const stack_t __user *,uss, stack_t __user *,uoss)\n{\n\tstack_t new, old;\n\tint err;\n\tif (uss && copy_from_user(&new, uss, sizeof(stack_t)))\n\t\treturn -EFAULT;\n\terr = do_sigaltstack(uss ? &new : NULL, uoss ? &old : NULL,\n\t\t\t      current_user_stack_pointer(),\n\t\t\t      MINSIGSTKSZ);\n\tif (!err && uoss && copy_to_user(uoss, &old, sizeof(stack_t)))\n\t\terr = -EFAULT;\n\treturn err;\n}\n\nint restore_altstack(const stack_t __user *uss)\n{\n\tstack_t new;\n\tif (copy_from_user(&new, uss, sizeof(stack_t)))\n\t\treturn -EFAULT;\n\t(void)do_sigaltstack(&new, NULL, current_user_stack_pointer(),\n\t\t\t     MINSIGSTKSZ);\n\t/* squash all but EFAULT for now */\n\treturn 0;\n}\n\nint __save_altstack(stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\tint err = __put_user((void __user *)t->sas_ss_sp, &uss->ss_sp) |\n\t\t__put_user(t->sas_ss_flags, &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n\tif (err)\n\t\treturn err;\n\tif (t->sas_ss_flags & SS_AUTODISARM)\n\t\tsas_ss_reset(t);\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nstatic int do_compat_sigaltstack(const compat_stack_t __user *uss_ptr,\n\t\t\t\t compat_stack_t __user *uoss_ptr)\n{\n\tstack_t uss, uoss;\n\tint ret;\n\n\tif (uss_ptr) {\n\t\tcompat_stack_t uss32;\n\t\tif (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))\n\t\t\treturn -EFAULT;\n\t\tuss.ss_sp = compat_ptr(uss32.ss_sp);\n\t\tuss.ss_flags = uss32.ss_flags;\n\t\tuss.ss_size = uss32.ss_size;\n\t}\n\tret = do_sigaltstack(uss_ptr ? &uss : NULL, &uoss,\n\t\t\t     compat_user_stack_pointer(),\n\t\t\t     COMPAT_MINSIGSTKSZ);\n\tif (ret >= 0 && uoss_ptr)  {\n\t\tcompat_stack_t old;\n\t\tmemset(&old, 0, sizeof(old));\n\t\told.ss_sp = ptr_to_compat(uoss.ss_sp);\n\t\told.ss_flags = uoss.ss_flags;\n\t\told.ss_size = uoss.ss_size;\n\t\tif (copy_to_user(uoss_ptr, &old, sizeof(compat_stack_t)))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\nCOMPAT_SYSCALL_DEFINE2(sigaltstack,\n\t\t\tconst compat_stack_t __user *, uss_ptr,\n\t\t\tcompat_stack_t __user *, uoss_ptr)\n{\n\treturn do_compat_sigaltstack(uss_ptr, uoss_ptr);\n}\n\nint compat_restore_altstack(const compat_stack_t __user *uss)\n{\n\tint err = do_compat_sigaltstack(uss, NULL);\n\t/* squash all but -EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)\n{\n\tint err;\n\tstruct task_struct *t = current;\n\terr = __put_user(ptr_to_compat((void __user *)t->sas_ss_sp),\n\t\t\t &uss->ss_sp) |\n\t\t__put_user(t->sas_ss_flags, &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n\tif (err)\n\t\treturn err;\n\tif (t->sas_ss_flags & SS_AUTODISARM)\n\t\tsas_ss_reset(t);\n\treturn 0;\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPENDING\n\n/**\n *  sys_sigpending - examine pending signals\n *  @uset: where mask of pending signal is returned\n */\nSYSCALL_DEFINE1(sigpending, old_sigset_t __user *, uset)\n{\n\tsigset_t set;\n\n\tif (sizeof(old_sigset_t) > sizeof(*uset))\n\t\treturn -EINVAL;\n\n\tdo_sigpending(&set);\n\n\tif (copy_to_user(uset, &set, sizeof(old_sigset_t)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE1(sigpending, compat_old_sigset_t __user *, set32)\n{\n\tsigset_t set;\n\n\tdo_sigpending(&set);\n\n\treturn put_user(set.sig[0], set32);\n}\n#endif\n\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\n/**\n *  sys_sigprocmask - examine and change blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: signals to add or remove (if non-null)\n *  @oset: previous value of signal mask if non-null\n *\n * Some platforms have their own version with special arguments;\n * others support only sys_rt_sigprocmask.\n */\n\nSYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,\n\t\told_sigset_t __user *, oset)\n{\n\told_sigset_t old_set, new_set;\n\tsigset_t new_blocked;\n\n\told_set = current->blocked.sig[0];\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(*nset)))\n\t\t\treturn -EFAULT;\n\n\t\tnew_blocked = current->blocked;\n\n\t\tswitch (how) {\n\t\tcase SIG_BLOCK:\n\t\t\tsigaddsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_UNBLOCK:\n\t\t\tsigdelsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_SETMASK:\n\t\t\tnew_blocked.sig[0] = new_set;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tset_current_blocked(&new_blocked);\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(*oset)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n#endif /* __ARCH_WANT_SYS_SIGPROCMASK */\n\n#ifndef CONFIG_ODD_RT_SIGACTION\n/**\n *  sys_rt_sigaction - alter an action taken by a process\n *  @sig: signal to be sent\n *  @act: new sigaction\n *  @oact: used to save the previous sigaction\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct sigaction __user *, act,\n\t\tstruct sigaction __user *, oact,\n\t\tsize_t, sigsetsize)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (act && copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))\n\t\treturn -EFAULT;\n\n\tret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tif (oact && copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct compat_sigaction __user *, act,\n\t\tstruct compat_sigaction __user *, oact,\n\t\tcompat_size_t, sigsetsize)\n{\n\tstruct k_sigaction new_ka, old_ka;\n#ifdef __ARCH_HAS_SA_RESTORER\n\tcompat_uptr_t restorer;\n#endif\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(compat_sigset_t))\n\t\treturn -EINVAL;\n\n\tif (act) {\n\t\tcompat_uptr_t handler;\n\t\tret = get_user(handler, &act->sa_handler);\n\t\tnew_ka.sa.sa_handler = compat_ptr(handler);\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tret |= get_user(restorer, &act->sa_restorer);\n\t\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\n#endif\n\t\tret |= get_compat_sigset(&new_ka.sa.sa_mask, &act->sa_mask);\n\t\tret |= get_user(new_ka.sa.sa_flags, &act->sa_flags);\n\t\tif (ret)\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\tif (!ret && oact) {\n\t\tret = put_user(ptr_to_compat(old_ka.sa.sa_handler), \n\t\t\t       &oact->sa_handler);\n\t\tret |= put_compat_sigset(&oact->sa_mask, &old_ka.sa.sa_mask,\n\t\t\t\t\t sizeof(oact->sa_mask));\n\t\tret |= put_user(old_ka.sa.sa_flags, &oact->sa_flags);\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tret |= put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n\t\t\t\t&oact->sa_restorer);\n#endif\n\t}\n\treturn ret;\n}\n#endif\n#endif /* !CONFIG_ODD_RT_SIGACTION */\n\n#ifdef CONFIG_OLD_SIGACTION\nSYSCALL_DEFINE3(sigaction, int, sig,\n\t\tconst struct old_sigaction __user *, act,\n\t        struct old_sigaction __user *, oact)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tint ret;\n\n\tif (act) {\n\t\told_sigset_t mask;\n\t\tif (!access_ok(act, sizeof(*act)) ||\n\t\t    __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||\n\t\t    __get_user(new_ka.sa.sa_restorer, &act->sa_restorer) ||\n\t\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n\t\t    __get_user(mask, &act->sa_mask))\n\t\t\treturn -EFAULT;\n#ifdef __ARCH_HAS_KA_RESTORER\n\t\tnew_ka.ka_restorer = NULL;\n#endif\n\t\tsiginitset(&new_ka.sa.sa_mask, mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n\tif (!ret && oact) {\n\t\tif (!access_ok(oact, sizeof(*oact)) ||\n\t\t    __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||\n\t\t    __put_user(old_ka.sa.sa_restorer, &oact->sa_restorer) ||\n\t\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n\t\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn ret;\n}\n#endif\n#ifdef CONFIG_COMPAT_OLD_SIGACTION\nCOMPAT_SYSCALL_DEFINE3(sigaction, int, sig,\n\t\tconst struct compat_old_sigaction __user *, act,\n\t        struct compat_old_sigaction __user *, oact)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tint ret;\n\tcompat_old_sigset_t mask;\n\tcompat_uptr_t handler, restorer;\n\n\tif (act) {\n\t\tif (!access_ok(act, sizeof(*act)) ||\n\t\t    __get_user(handler, &act->sa_handler) ||\n\t\t    __get_user(restorer, &act->sa_restorer) ||\n\t\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n\t\t    __get_user(mask, &act->sa_mask))\n\t\t\treturn -EFAULT;\n\n#ifdef __ARCH_HAS_KA_RESTORER\n\t\tnew_ka.ka_restorer = NULL;\n#endif\n\t\tnew_ka.sa.sa_handler = compat_ptr(handler);\n\t\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\n\t\tsiginitset(&new_ka.sa.sa_mask, mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n\tif (!ret && oact) {\n\t\tif (!access_ok(oact, sizeof(*oact)) ||\n\t\t    __put_user(ptr_to_compat(old_ka.sa.sa_handler),\n\t\t\t       &oact->sa_handler) ||\n\t\t    __put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n\t\t\t       &oact->sa_restorer) ||\n\t\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n\t\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n\t\t\treturn -EFAULT;\n\t}\n\treturn ret;\n}\n#endif\n\n#ifdef CONFIG_SGETMASK_SYSCALL\n\n/*\n * For backwards compatibility.  Functionality superseded by sigprocmask.\n */\nSYSCALL_DEFINE0(sgetmask)\n{\n\t/* SMP safe */\n\treturn current->blocked.sig[0];\n}\n\nSYSCALL_DEFINE1(ssetmask, int, newmask)\n{\n\tint old = current->blocked.sig[0];\n\tsigset_t newset;\n\n\tsiginitset(&newset, newmask);\n\tset_current_blocked(&newset);\n\n\treturn old;\n}\n#endif /* CONFIG_SGETMASK_SYSCALL */\n\n#ifdef __ARCH_WANT_SYS_SIGNAL\n/*\n * For backwards compatibility.  Functionality superseded by sigaction.\n */\nSYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret;\n\n\tnew_sa.sa.sa_handler = handler;\n\tnew_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;\n\tsigemptyset(&new_sa.sa.sa_mask);\n\n\tret = do_sigaction(sig, &new_sa, &old_sa);\n\n\treturn ret ? ret : (unsigned long)old_sa.sa.sa_handler;\n}\n#endif /* __ARCH_WANT_SYS_SIGNAL */\n\n#ifdef __ARCH_WANT_SYS_PAUSE\n\nSYSCALL_DEFINE0(pause)\n{\n\twhile (!signal_pending(current)) {\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n\treturn -ERESTARTNOHAND;\n}\n\n#endif\n\nstatic int sigsuspend(sigset_t *set)\n{\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(set);\n\n\twhile (!signal_pending(current)) {\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n\tset_restore_sigmask();\n\treturn -ERESTARTNOHAND;\n}\n\n/**\n *  sys_rt_sigsuspend - replace the signal mask for a value with the\n *\t@unewset value until a signal is received\n *  @unewset: new signal mask value\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)\n{\n\tsigset_t newset;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&newset, unewset, sizeof(newset)))\n\t\treturn -EFAULT;\n\treturn sigsuspend(&newset);\n}\n \n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigsuspend, compat_sigset_t __user *, unewset, compat_size_t, sigsetsize)\n{\n\tsigset_t newset;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (get_compat_sigset(&newset, unewset))\n\t\treturn -EFAULT;\n\treturn sigsuspend(&newset);\n}\n#endif\n\n#ifdef CONFIG_OLD_SIGSUSPEND\nSYSCALL_DEFINE1(sigsuspend, old_sigset_t, mask)\n{\n\tsigset_t blocked;\n\tsiginitset(&blocked, mask);\n\treturn sigsuspend(&blocked);\n}\n#endif\n#ifdef CONFIG_OLD_SIGSUSPEND3\nSYSCALL_DEFINE3(sigsuspend, int, unused1, int, unused2, old_sigset_t, mask)\n{\n\tsigset_t blocked;\n\tsiginitset(&blocked, mask);\n\treturn sigsuspend(&blocked);\n}\n#endif\n\n__weak const char *arch_vma_name(struct vm_area_struct *vma)\n{\n\treturn NULL;\n}\n\nstatic inline void siginfo_buildtime_checks(void)\n{\n\tBUILD_BUG_ON(sizeof(struct siginfo) != SI_MAX_SIZE);\n\n\t/* Verify the offsets in the two siginfos match */\n#define CHECK_OFFSET(field) \\\n\tBUILD_BUG_ON(offsetof(siginfo_t, field) != offsetof(kernel_siginfo_t, field))\n\n\t/* kill */\n\tCHECK_OFFSET(si_pid);\n\tCHECK_OFFSET(si_uid);\n\n\t/* timer */\n\tCHECK_OFFSET(si_tid);\n\tCHECK_OFFSET(si_overrun);\n\tCHECK_OFFSET(si_value);\n\n\t/* rt */\n\tCHECK_OFFSET(si_pid);\n\tCHECK_OFFSET(si_uid);\n\tCHECK_OFFSET(si_value);\n\n\t/* sigchld */\n\tCHECK_OFFSET(si_pid);\n\tCHECK_OFFSET(si_uid);\n\tCHECK_OFFSET(si_status);\n\tCHECK_OFFSET(si_utime);\n\tCHECK_OFFSET(si_stime);\n\n\t/* sigfault */\n\tCHECK_OFFSET(si_addr);\n\tCHECK_OFFSET(si_addr_lsb);\n\tCHECK_OFFSET(si_lower);\n\tCHECK_OFFSET(si_upper);\n\tCHECK_OFFSET(si_pkey);\n\n\t/* sigpoll */\n\tCHECK_OFFSET(si_band);\n\tCHECK_OFFSET(si_fd);\n\n\t/* sigsys */\n\tCHECK_OFFSET(si_call_addr);\n\tCHECK_OFFSET(si_syscall);\n\tCHECK_OFFSET(si_arch);\n#undef CHECK_OFFSET\n\n\t/* usb asyncio */\n\tBUILD_BUG_ON(offsetof(struct siginfo, si_pid) !=\n\t\t     offsetof(struct siginfo, si_addr));\n\tif (sizeof(int) == sizeof(void __user *)) {\n\t\tBUILD_BUG_ON(sizeof_field(struct siginfo, si_pid) !=\n\t\t\t     sizeof(void __user *));\n\t} else {\n\t\tBUILD_BUG_ON((sizeof_field(struct siginfo, si_pid) +\n\t\t\t      sizeof_field(struct siginfo, si_uid)) !=\n\t\t\t     sizeof(void __user *));\n\t\tBUILD_BUG_ON(offsetofend(struct siginfo, si_pid) !=\n\t\t\t     offsetof(struct siginfo, si_uid));\n\t}\n#ifdef CONFIG_COMPAT\n\tBUILD_BUG_ON(offsetof(struct compat_siginfo, si_pid) !=\n\t\t     offsetof(struct compat_siginfo, si_addr));\n\tBUILD_BUG_ON(sizeof_field(struct compat_siginfo, si_pid) !=\n\t\t     sizeof(compat_uptr_t));\n\tBUILD_BUG_ON(sizeof_field(struct compat_siginfo, si_pid) !=\n\t\t     sizeof_field(struct siginfo, si_pid));\n#endif\n}\n\nvoid __init signals_init(void)\n{\n\tsiginfo_buildtime_checks();\n\n\tsigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC);\n}\n\n#ifdef CONFIG_KGDB_KDB\n#include <linux/kdb.h>\n/*\n * kdb_send_sig - Allows kdb to send signals without exposing\n * signal internals.  This function checks if the required locks are\n * available before calling the main signal code, to avoid kdb\n * deadlocks.\n */\nvoid kdb_send_sig(struct task_struct *t, int sig)\n{\n\tstatic struct task_struct *kdb_prev_t;\n\tint new_t, ret;\n\tif (!spin_trylock(&t->sighand->siglock)) {\n\t\tkdb_printf(\"Can't do kill command now.\\n\"\n\t\t\t   \"The sigmask lock is held somewhere else in \"\n\t\t\t   \"kernel, try again later\\n\");\n\t\treturn;\n\t}\n\tnew_t = kdb_prev_t != t;\n\tkdb_prev_t = t;\n\tif (t->state != TASK_RUNNING && new_t) {\n\t\tspin_unlock(&t->sighand->siglock);\n\t\tkdb_printf(\"Process is not RUNNING, sending a signal from \"\n\t\t\t   \"kdb risks deadlock\\n\"\n\t\t\t   \"on the run queue locks. \"\n\t\t\t   \"The signal has _not_ been sent.\\n\"\n\t\t\t   \"Reissue the kill command if you want to risk \"\n\t\t\t   \"the deadlock.\\n\");\n\t\treturn;\n\t}\n\tret = send_signal(sig, SEND_SIG_PRIV, t, PIDTYPE_PID);\n\tspin_unlock(&t->sighand->siglock);\n\tif (ret)\n\t\tkdb_printf(\"Fail to deliver Signal %d to process %d.\\n\",\n\t\t\t   sig, t->pid);\n\telse\n\t\tkdb_printf(\"Signal %d is sent to process %d.\\n\", sig, t->pid);\n}\n#endif\t/* CONFIG_KGDB_KDB */\n"], "filenames": ["fs/exec.c", "include/linux/sched.h", "kernel/signal.c"], "buggy_code_start_loc": [1389, 942, 1934], "buggy_code_end_loc": [1390, 944, 1935], "fixing_code_start_loc": [1389, 942, 1934], "fixing_code_end_loc": [1390, 944, 1935], "type": "CWE-190", "message": "A signal access-control issue was discovered in the Linux kernel before 5.6.5, aka CID-7395ea4e65c2. Because exec_id in include/linux/sched.h is only 32 bits, an integer overflow can interfere with a do_notify_parent protection mechanism. A child process can send an arbitrary signal to a parent process in a different security domain. Exploitation limitations include the amount of elapsed time before an integer overflow occurs, and the lack of scenarios where signals to a parent process present a substantial operational threat.", "other": {"cve": {"id": "CVE-2020-12826", "sourceIdentifier": "cve@mitre.org", "published": "2020-05-12T19:15:11.080", "lastModified": "2021-07-15T19:16:09.750", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "A signal access-control issue was discovered in the Linux kernel before 5.6.5, aka CID-7395ea4e65c2. Because exec_id in include/linux/sched.h is only 32 bits, an integer overflow can interfere with a do_notify_parent protection mechanism. A child process can send an arbitrary signal to a parent process in a different security domain. Exploitation limitations include the amount of elapsed time before an integer overflow occurs, and the lack of scenarios where signals to a parent process present a substantial operational threat."}, {"lang": "es", "value": "Se detect\u00f3 un problema de control de acceso de se\u00f1al en el kernel de Linux versiones anteriores a 5.6.5, se conoce como CID-7395ea4e65c2. Porque la funci\u00f3n exec_id en el archivo include/linux/sched.h presenta solo 32 bits, un desbordamiento de enteros puede interferir con un mecanismo de protecci\u00f3n do_notify_parent. Un proceso secundario puede enviar una se\u00f1al arbitraria hacia un proceso primario en un dominio de seguridad diferente. Las limitaciones de explotaci\u00f3n incluyen la cantidad de tiempo transcurrido antes de que ocurra un desbordamiento de enteros y una falta de escenarios donde las se\u00f1ales en un proceso primario presenten una amenaza operacional sustancial."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:L/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.6.5", "matchCriteriaId": "74C74C8C-8070-4FB7-8E86-26641C4EE4FF"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:20.04:*:*:*:lts:*:*:*", "matchCriteriaId": "902B8056-9E37-443B-8905-8AA93E2447FB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:5.0:*:*:*:*:*:*:*", "matchCriteriaId": "1D8B549B-E57B-4DFE-8A13-CAB06B5356B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:6.0:*:*:*:*:*:*:*", "matchCriteriaId": "2F6AB192-9D7D-4A9A-8995-E53A9DE9EAFC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_mrg:2.0:*:*:*:*:*:*:*", "matchCriteriaId": "C60FA8B1-1802-4522-A088-22171DCF7A93"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1822077", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.6.5", "source": "cve@mitre.org", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/7395ea4e65c2a00d23185a3f63ad315756ba9cef", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2020/06/msg00011.html", "source": "cve@mitre.org"}, {"url": "https://lists.debian.org/debian-lts-announce/2020/06/msg00013.html", "source": "cve@mitre.org"}, {"url": "https://lists.openwall.net/linux-kernel/2020/03/24/1803", "source": "cve@mitre.org", "tags": ["Exploit", "Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20200608-0001/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4367-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4369-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4391-1/", "source": "cve@mitre.org"}, {"url": "https://www.openwall.com/lists/kernel-hardening/2020/03/25/1", "source": "cve@mitre.org", "tags": ["Exploit", "Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/7395ea4e65c2a00d23185a3f63ad315756ba9cef"}}