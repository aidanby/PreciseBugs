{"buggy_code": ["/*\n * Copyright (C) 2001 Jens Axboe <axboe@kernel.dk>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n *\n */\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/uio.h>\n#include <linux/iocontext.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/mempool.h>\n#include <linux/workqueue.h>\n#include <linux/cgroup.h>\n\n#include <trace/events/block.h>\n#include \"blk.h\"\n\n/*\n * Test patch to inline a certain number of bi_io_vec's inside the bio\n * itself, to shrink a bio data allocation from two mempool calls to one\n */\n#define BIO_INLINE_VECS\t\t4\n\n/*\n * if you change this list, also change bvec_alloc or things will\n * break badly! cannot be bigger than what you can fit into an\n * unsigned short\n */\n#define BV(x) { .nr_vecs = x, .name = \"biovec-\"__stringify(x) }\nstatic struct biovec_slab bvec_slabs[BVEC_POOL_NR] __read_mostly = {\n\tBV(1), BV(4), BV(16), BV(64), BV(128), BV(BIO_MAX_PAGES),\n};\n#undef BV\n\n/*\n * fs_bio_set is the bio_set containing bio and iovec memory pools used by\n * IO code that does not need private memory pools.\n */\nstruct bio_set *fs_bio_set;\nEXPORT_SYMBOL(fs_bio_set);\n\n/*\n * Our slab pool management\n */\nstruct bio_slab {\n\tstruct kmem_cache *slab;\n\tunsigned int slab_ref;\n\tunsigned int slab_size;\n\tchar name[8];\n};\nstatic DEFINE_MUTEX(bio_slab_lock);\nstatic struct bio_slab *bio_slabs;\nstatic unsigned int bio_slab_nr, bio_slab_max;\n\nstatic struct kmem_cache *bio_find_or_create_slab(unsigned int extra_size)\n{\n\tunsigned int sz = sizeof(struct bio) + extra_size;\n\tstruct kmem_cache *slab = NULL;\n\tstruct bio_slab *bslab, *new_bio_slabs;\n\tunsigned int new_bio_slab_max;\n\tunsigned int i, entry = -1;\n\n\tmutex_lock(&bio_slab_lock);\n\n\ti = 0;\n\twhile (i < bio_slab_nr) {\n\t\tbslab = &bio_slabs[i];\n\n\t\tif (!bslab->slab && entry == -1)\n\t\t\tentry = i;\n\t\telse if (bslab->slab_size == sz) {\n\t\t\tslab = bslab->slab;\n\t\t\tbslab->slab_ref++;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\n\tif (slab)\n\t\tgoto out_unlock;\n\n\tif (bio_slab_nr == bio_slab_max && entry == -1) {\n\t\tnew_bio_slab_max = bio_slab_max << 1;\n\t\tnew_bio_slabs = krealloc(bio_slabs,\n\t\t\t\t\t new_bio_slab_max * sizeof(struct bio_slab),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!new_bio_slabs)\n\t\t\tgoto out_unlock;\n\t\tbio_slab_max = new_bio_slab_max;\n\t\tbio_slabs = new_bio_slabs;\n\t}\n\tif (entry == -1)\n\t\tentry = bio_slab_nr++;\n\n\tbslab = &bio_slabs[entry];\n\n\tsnprintf(bslab->name, sizeof(bslab->name), \"bio-%d\", entry);\n\tslab = kmem_cache_create(bslab->name, sz, ARCH_KMALLOC_MINALIGN,\n\t\t\t\t SLAB_HWCACHE_ALIGN, NULL);\n\tif (!slab)\n\t\tgoto out_unlock;\n\n\tbslab->slab = slab;\n\tbslab->slab_ref = 1;\n\tbslab->slab_size = sz;\nout_unlock:\n\tmutex_unlock(&bio_slab_lock);\n\treturn slab;\n}\n\nstatic void bio_put_slab(struct bio_set *bs)\n{\n\tstruct bio_slab *bslab = NULL;\n\tunsigned int i;\n\n\tmutex_lock(&bio_slab_lock);\n\n\tfor (i = 0; i < bio_slab_nr; i++) {\n\t\tif (bs->bio_slab == bio_slabs[i].slab) {\n\t\t\tbslab = &bio_slabs[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (WARN(!bslab, KERN_ERR \"bio: unable to find slab!\\n\"))\n\t\tgoto out;\n\n\tWARN_ON(!bslab->slab_ref);\n\n\tif (--bslab->slab_ref)\n\t\tgoto out;\n\n\tkmem_cache_destroy(bslab->slab);\n\tbslab->slab = NULL;\n\nout:\n\tmutex_unlock(&bio_slab_lock);\n}\n\nunsigned int bvec_nr_vecs(unsigned short idx)\n{\n\treturn bvec_slabs[idx].nr_vecs;\n}\n\nvoid bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned int idx)\n{\n\tif (!idx)\n\t\treturn;\n\tidx--;\n\n\tBIO_BUG_ON(idx >= BVEC_POOL_NR);\n\n\tif (idx == BVEC_POOL_MAX) {\n\t\tmempool_free(bv, pool);\n\t} else {\n\t\tstruct biovec_slab *bvs = bvec_slabs + idx;\n\n\t\tkmem_cache_free(bvs->slab, bv);\n\t}\n}\n\nstruct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,\n\t\t\t   mempool_t *pool)\n{\n\tstruct bio_vec *bvl;\n\n\t/*\n\t * see comment near bvec_array define!\n\t */\n\tswitch (nr) {\n\tcase 1:\n\t\t*idx = 0;\n\t\tbreak;\n\tcase 2 ... 4:\n\t\t*idx = 1;\n\t\tbreak;\n\tcase 5 ... 16:\n\t\t*idx = 2;\n\t\tbreak;\n\tcase 17 ... 64:\n\t\t*idx = 3;\n\t\tbreak;\n\tcase 65 ... 128:\n\t\t*idx = 4;\n\t\tbreak;\n\tcase 129 ... BIO_MAX_PAGES:\n\t\t*idx = 5;\n\t\tbreak;\n\tdefault:\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * idx now points to the pool we want to allocate from. only the\n\t * 1-vec entry pool is mempool backed.\n\t */\n\tif (*idx == BVEC_POOL_MAX) {\nfallback:\n\t\tbvl = mempool_alloc(pool, gfp_mask);\n\t} else {\n\t\tstruct biovec_slab *bvs = bvec_slabs + *idx;\n\t\tgfp_t __gfp_mask = gfp_mask & ~(__GFP_DIRECT_RECLAIM | __GFP_IO);\n\n\t\t/*\n\t\t * Make this allocation restricted and don't dump info on\n\t\t * allocation failures, since we'll fallback to the mempool\n\t\t * in case of failure.\n\t\t */\n\t\t__gfp_mask |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;\n\n\t\t/*\n\t\t * Try a slab allocation. If this fails and __GFP_DIRECT_RECLAIM\n\t\t * is set, retry with the 1-entry mempool\n\t\t */\n\t\tbvl = kmem_cache_alloc(bvs->slab, __gfp_mask);\n\t\tif (unlikely(!bvl && (gfp_mask & __GFP_DIRECT_RECLAIM))) {\n\t\t\t*idx = BVEC_POOL_MAX;\n\t\t\tgoto fallback;\n\t\t}\n\t}\n\n\t(*idx)++;\n\treturn bvl;\n}\n\nvoid bio_uninit(struct bio *bio)\n{\n\tbio_disassociate_task(bio);\n}\nEXPORT_SYMBOL(bio_uninit);\n\nstatic void bio_free(struct bio *bio)\n{\n\tstruct bio_set *bs = bio->bi_pool;\n\tvoid *p;\n\n\tbio_uninit(bio);\n\n\tif (bs) {\n\t\tbvec_free(bs->bvec_pool, bio->bi_io_vec, BVEC_POOL_IDX(bio));\n\n\t\t/*\n\t\t * If we have front padding, adjust the bio pointer before freeing\n\t\t */\n\t\tp = bio;\n\t\tp -= bs->front_pad;\n\n\t\tmempool_free(p, bs->bio_pool);\n\t} else {\n\t\t/* Bio was allocated by bio_kmalloc() */\n\t\tkfree(bio);\n\t}\n}\n\n/*\n * Users of this function have their own bio allocation. Subsequently,\n * they must remember to pair any call to bio_init() with bio_uninit()\n * when IO has completed, or when the bio is released.\n */\nvoid bio_init(struct bio *bio, struct bio_vec *table,\n\t      unsigned short max_vecs)\n{\n\tmemset(bio, 0, sizeof(*bio));\n\tatomic_set(&bio->__bi_remaining, 1);\n\tatomic_set(&bio->__bi_cnt, 1);\n\n\tbio->bi_io_vec = table;\n\tbio->bi_max_vecs = max_vecs;\n}\nEXPORT_SYMBOL(bio_init);\n\n/**\n * bio_reset - reinitialize a bio\n * @bio:\tbio to reset\n *\n * Description:\n *   After calling bio_reset(), @bio will be in the same state as a freshly\n *   allocated bio returned bio bio_alloc_bioset() - the only fields that are\n *   preserved are the ones that are initialized by bio_alloc_bioset(). See\n *   comment in struct bio.\n */\nvoid bio_reset(struct bio *bio)\n{\n\tunsigned long flags = bio->bi_flags & (~0UL << BIO_RESET_BITS);\n\n\tbio_uninit(bio);\n\n\tmemset(bio, 0, BIO_RESET_BYTES);\n\tbio->bi_flags = flags;\n\tatomic_set(&bio->__bi_remaining, 1);\n}\nEXPORT_SYMBOL(bio_reset);\n\nstatic struct bio *__bio_chain_endio(struct bio *bio)\n{\n\tstruct bio *parent = bio->bi_private;\n\n\tif (!parent->bi_status)\n\t\tparent->bi_status = bio->bi_status;\n\tbio_put(bio);\n\treturn parent;\n}\n\nstatic void bio_chain_endio(struct bio *bio)\n{\n\tbio_endio(__bio_chain_endio(bio));\n}\n\n/**\n * bio_chain - chain bio completions\n * @bio: the target bio\n * @parent: the @bio's parent bio\n *\n * The caller won't have a bi_end_io called when @bio completes - instead,\n * @parent's bi_end_io won't be called until both @parent and @bio have\n * completed; the chained bio will also be freed when it completes.\n *\n * The caller must not set bi_private or bi_end_io in @bio.\n */\nvoid bio_chain(struct bio *bio, struct bio *parent)\n{\n\tBUG_ON(bio->bi_private || bio->bi_end_io);\n\n\tbio->bi_private = parent;\n\tbio->bi_end_io\t= bio_chain_endio;\n\tbio_inc_remaining(parent);\n}\nEXPORT_SYMBOL(bio_chain);\n\nstatic void bio_alloc_rescue(struct work_struct *work)\n{\n\tstruct bio_set *bs = container_of(work, struct bio_set, rescue_work);\n\tstruct bio *bio;\n\n\twhile (1) {\n\t\tspin_lock(&bs->rescue_lock);\n\t\tbio = bio_list_pop(&bs->rescue_list);\n\t\tspin_unlock(&bs->rescue_lock);\n\n\t\tif (!bio)\n\t\t\tbreak;\n\n\t\tgeneric_make_request(bio);\n\t}\n}\n\nstatic void punt_bios_to_rescuer(struct bio_set *bs)\n{\n\tstruct bio_list punt, nopunt;\n\tstruct bio *bio;\n\n\tif (WARN_ON_ONCE(!bs->rescue_workqueue))\n\t\treturn;\n\t/*\n\t * In order to guarantee forward progress we must punt only bios that\n\t * were allocated from this bio_set; otherwise, if there was a bio on\n\t * there for a stacking driver higher up in the stack, processing it\n\t * could require allocating bios from this bio_set, and doing that from\n\t * our own rescuer would be bad.\n\t *\n\t * Since bio lists are singly linked, pop them all instead of trying to\n\t * remove from the middle of the list:\n\t */\n\n\tbio_list_init(&punt);\n\tbio_list_init(&nopunt);\n\n\twhile ((bio = bio_list_pop(&current->bio_list[0])))\n\t\tbio_list_add(bio->bi_pool == bs ? &punt : &nopunt, bio);\n\tcurrent->bio_list[0] = nopunt;\n\n\tbio_list_init(&nopunt);\n\twhile ((bio = bio_list_pop(&current->bio_list[1])))\n\t\tbio_list_add(bio->bi_pool == bs ? &punt : &nopunt, bio);\n\tcurrent->bio_list[1] = nopunt;\n\n\tspin_lock(&bs->rescue_lock);\n\tbio_list_merge(&bs->rescue_list, &punt);\n\tspin_unlock(&bs->rescue_lock);\n\n\tqueue_work(bs->rescue_workqueue, &bs->rescue_work);\n}\n\n/**\n * bio_alloc_bioset - allocate a bio for I/O\n * @gfp_mask:   the GFP_ mask given to the slab allocator\n * @nr_iovecs:\tnumber of iovecs to pre-allocate\n * @bs:\t\tthe bio_set to allocate from.\n *\n * Description:\n *   If @bs is NULL, uses kmalloc() to allocate the bio; else the allocation is\n *   backed by the @bs's mempool.\n *\n *   When @bs is not NULL, if %__GFP_DIRECT_RECLAIM is set then bio_alloc will\n *   always be able to allocate a bio. This is due to the mempool guarantees.\n *   To make this work, callers must never allocate more than 1 bio at a time\n *   from this pool. Callers that need to allocate more than 1 bio must always\n *   submit the previously allocated bio for IO before attempting to allocate\n *   a new one. Failure to do so can cause deadlocks under memory pressure.\n *\n *   Note that when running under generic_make_request() (i.e. any block\n *   driver), bios are not submitted until after you return - see the code in\n *   generic_make_request() that converts recursion into iteration, to prevent\n *   stack overflows.\n *\n *   This would normally mean allocating multiple bios under\n *   generic_make_request() would be susceptible to deadlocks, but we have\n *   deadlock avoidance code that resubmits any blocked bios from a rescuer\n *   thread.\n *\n *   However, we do not guarantee forward progress for allocations from other\n *   mempools. Doing multiple allocations from the same mempool under\n *   generic_make_request() should be avoided - instead, use bio_set's front_pad\n *   for per bio allocations.\n *\n *   RETURNS:\n *   Pointer to new bio on success, NULL on failure.\n */\nstruct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,\n\t\t\t     struct bio_set *bs)\n{\n\tgfp_t saved_gfp = gfp_mask;\n\tunsigned front_pad;\n\tunsigned inline_vecs;\n\tstruct bio_vec *bvl = NULL;\n\tstruct bio *bio;\n\tvoid *p;\n\n\tif (!bs) {\n\t\tif (nr_iovecs > UIO_MAXIOV)\n\t\t\treturn NULL;\n\n\t\tp = kmalloc(sizeof(struct bio) +\n\t\t\t    nr_iovecs * sizeof(struct bio_vec),\n\t\t\t    gfp_mask);\n\t\tfront_pad = 0;\n\t\tinline_vecs = nr_iovecs;\n\t} else {\n\t\t/* should not use nobvec bioset for nr_iovecs > 0 */\n\t\tif (WARN_ON_ONCE(!bs->bvec_pool && nr_iovecs > 0))\n\t\t\treturn NULL;\n\t\t/*\n\t\t * generic_make_request() converts recursion to iteration; this\n\t\t * means if we're running beneath it, any bios we allocate and\n\t\t * submit will not be submitted (and thus freed) until after we\n\t\t * return.\n\t\t *\n\t\t * This exposes us to a potential deadlock if we allocate\n\t\t * multiple bios from the same bio_set() while running\n\t\t * underneath generic_make_request(). If we were to allocate\n\t\t * multiple bios (say a stacking block driver that was splitting\n\t\t * bios), we would deadlock if we exhausted the mempool's\n\t\t * reserve.\n\t\t *\n\t\t * We solve this, and guarantee forward progress, with a rescuer\n\t\t * workqueue per bio_set. If we go to allocate and there are\n\t\t * bios on current->bio_list, we first try the allocation\n\t\t * without __GFP_DIRECT_RECLAIM; if that fails, we punt those\n\t\t * bios we would be blocking to the rescuer workqueue before\n\t\t * we retry with the original gfp_flags.\n\t\t */\n\n\t\tif (current->bio_list &&\n\t\t    (!bio_list_empty(&current->bio_list[0]) ||\n\t\t     !bio_list_empty(&current->bio_list[1])) &&\n\t\t    bs->rescue_workqueue)\n\t\t\tgfp_mask &= ~__GFP_DIRECT_RECLAIM;\n\n\t\tp = mempool_alloc(bs->bio_pool, gfp_mask);\n\t\tif (!p && gfp_mask != saved_gfp) {\n\t\t\tpunt_bios_to_rescuer(bs);\n\t\t\tgfp_mask = saved_gfp;\n\t\t\tp = mempool_alloc(bs->bio_pool, gfp_mask);\n\t\t}\n\n\t\tfront_pad = bs->front_pad;\n\t\tinline_vecs = BIO_INLINE_VECS;\n\t}\n\n\tif (unlikely(!p))\n\t\treturn NULL;\n\n\tbio = p + front_pad;\n\tbio_init(bio, NULL, 0);\n\n\tif (nr_iovecs > inline_vecs) {\n\t\tunsigned long idx = 0;\n\n\t\tbvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, bs->bvec_pool);\n\t\tif (!bvl && gfp_mask != saved_gfp) {\n\t\t\tpunt_bios_to_rescuer(bs);\n\t\t\tgfp_mask = saved_gfp;\n\t\t\tbvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, bs->bvec_pool);\n\t\t}\n\n\t\tif (unlikely(!bvl))\n\t\t\tgoto err_free;\n\n\t\tbio->bi_flags |= idx << BVEC_POOL_OFFSET;\n\t} else if (nr_iovecs) {\n\t\tbvl = bio->bi_inline_vecs;\n\t}\n\n\tbio->bi_pool = bs;\n\tbio->bi_max_vecs = nr_iovecs;\n\tbio->bi_io_vec = bvl;\n\treturn bio;\n\nerr_free:\n\tmempool_free(p, bs->bio_pool);\n\treturn NULL;\n}\nEXPORT_SYMBOL(bio_alloc_bioset);\n\nvoid zero_fill_bio(struct bio *bio)\n{\n\tunsigned long flags;\n\tstruct bio_vec bv;\n\tstruct bvec_iter iter;\n\n\tbio_for_each_segment(bv, bio, iter) {\n\t\tchar *data = bvec_kmap_irq(&bv, &flags);\n\t\tmemset(data, 0, bv.bv_len);\n\t\tflush_dcache_page(bv.bv_page);\n\t\tbvec_kunmap_irq(data, &flags);\n\t}\n}\nEXPORT_SYMBOL(zero_fill_bio);\n\n/**\n * bio_put - release a reference to a bio\n * @bio:   bio to release reference to\n *\n * Description:\n *   Put a reference to a &struct bio, either one you have gotten with\n *   bio_alloc, bio_get or bio_clone_*. The last put of a bio will free it.\n **/\nvoid bio_put(struct bio *bio)\n{\n\tif (!bio_flagged(bio, BIO_REFFED))\n\t\tbio_free(bio);\n\telse {\n\t\tBIO_BUG_ON(!atomic_read(&bio->__bi_cnt));\n\n\t\t/*\n\t\t * last put frees it\n\t\t */\n\t\tif (atomic_dec_and_test(&bio->__bi_cnt))\n\t\t\tbio_free(bio);\n\t}\n}\nEXPORT_SYMBOL(bio_put);\n\ninline int bio_phys_segments(struct request_queue *q, struct bio *bio)\n{\n\tif (unlikely(!bio_flagged(bio, BIO_SEG_VALID)))\n\t\tblk_recount_segments(q, bio);\n\n\treturn bio->bi_phys_segments;\n}\nEXPORT_SYMBOL(bio_phys_segments);\n\n/**\n * \t__bio_clone_fast - clone a bio that shares the original bio's biovec\n * \t@bio: destination bio\n * \t@bio_src: bio to clone\n *\n *\tClone a &bio. Caller will own the returned bio, but not\n *\tthe actual data it points to. Reference count of returned\n * \tbio will be one.\n *\n * \tCaller must ensure that @bio_src is not freed before @bio.\n */\nvoid __bio_clone_fast(struct bio *bio, struct bio *bio_src)\n{\n\tBUG_ON(bio->bi_pool && BVEC_POOL_IDX(bio));\n\n\t/*\n\t * most users will be overriding ->bi_disk with a new target,\n\t * so we don't set nor calculate new physical/hw segment counts here\n\t */\n\tbio->bi_disk = bio_src->bi_disk;\n\tbio_set_flag(bio, BIO_CLONED);\n\tbio->bi_opf = bio_src->bi_opf;\n\tbio->bi_write_hint = bio_src->bi_write_hint;\n\tbio->bi_iter = bio_src->bi_iter;\n\tbio->bi_io_vec = bio_src->bi_io_vec;\n\n\tbio_clone_blkcg_association(bio, bio_src);\n}\nEXPORT_SYMBOL(__bio_clone_fast);\n\n/**\n *\tbio_clone_fast - clone a bio that shares the original bio's biovec\n *\t@bio: bio to clone\n *\t@gfp_mask: allocation priority\n *\t@bs: bio_set to allocate from\n *\n * \tLike __bio_clone_fast, only also allocates the returned bio\n */\nstruct bio *bio_clone_fast(struct bio *bio, gfp_t gfp_mask, struct bio_set *bs)\n{\n\tstruct bio *b;\n\n\tb = bio_alloc_bioset(gfp_mask, 0, bs);\n\tif (!b)\n\t\treturn NULL;\n\n\t__bio_clone_fast(b, bio);\n\n\tif (bio_integrity(bio)) {\n\t\tint ret;\n\n\t\tret = bio_integrity_clone(b, bio, gfp_mask);\n\n\t\tif (ret < 0) {\n\t\t\tbio_put(b);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn b;\n}\nEXPORT_SYMBOL(bio_clone_fast);\n\n/**\n * \tbio_clone_bioset - clone a bio\n * \t@bio_src: bio to clone\n *\t@gfp_mask: allocation priority\n *\t@bs: bio_set to allocate from\n *\n *\tClone bio. Caller will own the returned bio, but not the actual data it\n *\tpoints to. Reference count of returned bio will be one.\n */\nstruct bio *bio_clone_bioset(struct bio *bio_src, gfp_t gfp_mask,\n\t\t\t     struct bio_set *bs)\n{\n\tstruct bvec_iter iter;\n\tstruct bio_vec bv;\n\tstruct bio *bio;\n\n\t/*\n\t * Pre immutable biovecs, __bio_clone() used to just do a memcpy from\n\t * bio_src->bi_io_vec to bio->bi_io_vec.\n\t *\n\t * We can't do that anymore, because:\n\t *\n\t *  - The point of cloning the biovec is to produce a bio with a biovec\n\t *    the caller can modify: bi_idx and bi_bvec_done should be 0.\n\t *\n\t *  - The original bio could've had more than BIO_MAX_PAGES biovecs; if\n\t *    we tried to clone the whole thing bio_alloc_bioset() would fail.\n\t *    But the clone should succeed as long as the number of biovecs we\n\t *    actually need to allocate is fewer than BIO_MAX_PAGES.\n\t *\n\t *  - Lastly, bi_vcnt should not be looked at or relied upon by code\n\t *    that does not own the bio - reason being drivers don't use it for\n\t *    iterating over the biovec anymore, so expecting it to be kept up\n\t *    to date (i.e. for clones that share the parent biovec) is just\n\t *    asking for trouble and would force extra work on\n\t *    __bio_clone_fast() anyways.\n\t */\n\n\tbio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);\n\tif (!bio)\n\t\treturn NULL;\n\tbio->bi_disk\t\t= bio_src->bi_disk;\n\tbio->bi_opf\t\t= bio_src->bi_opf;\n\tbio->bi_write_hint\t= bio_src->bi_write_hint;\n\tbio->bi_iter.bi_sector\t= bio_src->bi_iter.bi_sector;\n\tbio->bi_iter.bi_size\t= bio_src->bi_iter.bi_size;\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tbreak;\n\tcase REQ_OP_WRITE_SAME:\n\t\tbio->bi_io_vec[bio->bi_vcnt++] = bio_src->bi_io_vec[0];\n\t\tbreak;\n\tdefault:\n\t\tbio_for_each_segment(bv, bio_src, iter)\n\t\t\tbio->bi_io_vec[bio->bi_vcnt++] = bv;\n\t\tbreak;\n\t}\n\n\tif (bio_integrity(bio_src)) {\n\t\tint ret;\n\n\t\tret = bio_integrity_clone(bio, bio_src, gfp_mask);\n\t\tif (ret < 0) {\n\t\t\tbio_put(bio);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tbio_clone_blkcg_association(bio, bio_src);\n\n\treturn bio;\n}\nEXPORT_SYMBOL(bio_clone_bioset);\n\n/**\n *\tbio_add_pc_page\t-\tattempt to add page to bio\n *\t@q: the target queue\n *\t@bio: destination bio\n *\t@page: page to add\n *\t@len: vec entry length\n *\t@offset: vec entry offset\n *\n *\tAttempt to add a page to the bio_vec maplist. This can fail for a\n *\tnumber of reasons, such as the bio being full or target block device\n *\tlimitations. The target block device must allow bio's up to PAGE_SIZE,\n *\tso it is always possible to add a single page to an empty bio.\n *\n *\tThis should only be used by REQ_PC bios.\n */\nint bio_add_pc_page(struct request_queue *q, struct bio *bio, struct page\n\t\t    *page, unsigned int len, unsigned int offset)\n{\n\tint retried_segments = 0;\n\tstruct bio_vec *bvec;\n\n\t/*\n\t * cloned bio must not modify vec list\n\t */\n\tif (unlikely(bio_flagged(bio, BIO_CLONED)))\n\t\treturn 0;\n\n\tif (((bio->bi_iter.bi_size + len) >> 9) > queue_max_hw_sectors(q))\n\t\treturn 0;\n\n\t/*\n\t * For filesystems with a blocksize smaller than the pagesize\n\t * we will often be called with the same page as last time and\n\t * a consecutive offset.  Optimize this special case.\n\t */\n\tif (bio->bi_vcnt > 0) {\n\t\tstruct bio_vec *prev = &bio->bi_io_vec[bio->bi_vcnt - 1];\n\n\t\tif (page == prev->bv_page &&\n\t\t    offset == prev->bv_offset + prev->bv_len) {\n\t\t\tprev->bv_len += len;\n\t\t\tbio->bi_iter.bi_size += len;\n\t\t\tgoto done;\n\t\t}\n\n\t\t/*\n\t\t * If the queue doesn't support SG gaps and adding this\n\t\t * offset would create a gap, disallow it.\n\t\t */\n\t\tif (bvec_gap_to_prev(q, prev, offset))\n\t\t\treturn 0;\n\t}\n\n\tif (bio->bi_vcnt >= bio->bi_max_vecs)\n\t\treturn 0;\n\n\t/*\n\t * setup the new entry, we might clear it again later if we\n\t * cannot add the page\n\t */\n\tbvec = &bio->bi_io_vec[bio->bi_vcnt];\n\tbvec->bv_page = page;\n\tbvec->bv_len = len;\n\tbvec->bv_offset = offset;\n\tbio->bi_vcnt++;\n\tbio->bi_phys_segments++;\n\tbio->bi_iter.bi_size += len;\n\n\t/*\n\t * Perform a recount if the number of segments is greater\n\t * than queue_max_segments(q).\n\t */\n\n\twhile (bio->bi_phys_segments > queue_max_segments(q)) {\n\n\t\tif (retried_segments)\n\t\t\tgoto failed;\n\n\t\tretried_segments = 1;\n\t\tblk_recount_segments(q, bio);\n\t}\n\n\t/* If we may be able to merge these biovecs, force a recount */\n\tif (bio->bi_vcnt > 1 && (BIOVEC_PHYS_MERGEABLE(bvec-1, bvec)))\n\t\tbio_clear_flag(bio, BIO_SEG_VALID);\n\n done:\n\treturn len;\n\n failed:\n\tbvec->bv_page = NULL;\n\tbvec->bv_len = 0;\n\tbvec->bv_offset = 0;\n\tbio->bi_vcnt--;\n\tbio->bi_iter.bi_size -= len;\n\tblk_recount_segments(q, bio);\n\treturn 0;\n}\nEXPORT_SYMBOL(bio_add_pc_page);\n\n/**\n *\tbio_add_page\t-\tattempt to add page to bio\n *\t@bio: destination bio\n *\t@page: page to add\n *\t@len: vec entry length\n *\t@offset: vec entry offset\n *\n *\tAttempt to add a page to the bio_vec maplist. This will only fail\n *\tif either bio->bi_vcnt == bio->bi_max_vecs or it's a cloned bio.\n */\nint bio_add_page(struct bio *bio, struct page *page,\n\t\t unsigned int len, unsigned int offset)\n{\n\tstruct bio_vec *bv;\n\n\t/*\n\t * cloned bio must not modify vec list\n\t */\n\tif (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))\n\t\treturn 0;\n\n\t/*\n\t * For filesystems with a blocksize smaller than the pagesize\n\t * we will often be called with the same page as last time and\n\t * a consecutive offset.  Optimize this special case.\n\t */\n\tif (bio->bi_vcnt > 0) {\n\t\tbv = &bio->bi_io_vec[bio->bi_vcnt - 1];\n\n\t\tif (page == bv->bv_page &&\n\t\t    offset == bv->bv_offset + bv->bv_len) {\n\t\t\tbv->bv_len += len;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (bio->bi_vcnt >= bio->bi_max_vecs)\n\t\treturn 0;\n\n\tbv\t\t= &bio->bi_io_vec[bio->bi_vcnt];\n\tbv->bv_page\t= page;\n\tbv->bv_len\t= len;\n\tbv->bv_offset\t= offset;\n\n\tbio->bi_vcnt++;\ndone:\n\tbio->bi_iter.bi_size += len;\n\treturn len;\n}\nEXPORT_SYMBOL(bio_add_page);\n\n/**\n * bio_iov_iter_get_pages - pin user or kernel pages and add them to a bio\n * @bio: bio to add pages to\n * @iter: iov iterator describing the region to be mapped\n *\n * Pins as many pages from *iter and appends them to @bio's bvec array. The\n * pages will have to be released using put_page() when done.\n */\nint bio_iov_iter_get_pages(struct bio *bio, struct iov_iter *iter)\n{\n\tunsigned short nr_pages = bio->bi_max_vecs - bio->bi_vcnt;\n\tstruct bio_vec *bv = bio->bi_io_vec + bio->bi_vcnt;\n\tstruct page **pages = (struct page **)bv;\n\tsize_t offset, diff;\n\tssize_t size;\n\n\tsize = iov_iter_get_pages(iter, pages, LONG_MAX, nr_pages, &offset);\n\tif (unlikely(size <= 0))\n\t\treturn size ? size : -EFAULT;\n\tnr_pages = (size + offset + PAGE_SIZE - 1) / PAGE_SIZE;\n\n\t/*\n\t * Deep magic below:  We need to walk the pinned pages backwards\n\t * because we are abusing the space allocated for the bio_vecs\n\t * for the page array.  Because the bio_vecs are larger than the\n\t * page pointers by definition this will always work.  But it also\n\t * means we can't use bio_add_page, so any changes to it's semantics\n\t * need to be reflected here as well.\n\t */\n\tbio->bi_iter.bi_size += size;\n\tbio->bi_vcnt += nr_pages;\n\n\tdiff = (nr_pages * PAGE_SIZE - offset) - size;\n\twhile (nr_pages--) {\n\t\tbv[nr_pages].bv_page = pages[nr_pages];\n\t\tbv[nr_pages].bv_len = PAGE_SIZE;\n\t\tbv[nr_pages].bv_offset = 0;\n\t}\n\n\tbv[0].bv_offset += offset;\n\tbv[0].bv_len -= offset;\n\tif (diff)\n\t\tbv[bio->bi_vcnt - 1].bv_len -= diff;\n\n\tiov_iter_advance(iter, size);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bio_iov_iter_get_pages);\n\nstruct submit_bio_ret {\n\tstruct completion event;\n\tint error;\n};\n\nstatic void submit_bio_wait_endio(struct bio *bio)\n{\n\tstruct submit_bio_ret *ret = bio->bi_private;\n\n\tret->error = blk_status_to_errno(bio->bi_status);\n\tcomplete(&ret->event);\n}\n\n/**\n * submit_bio_wait - submit a bio, and wait until it completes\n * @bio: The &struct bio which describes the I/O\n *\n * Simple wrapper around submit_bio(). Returns 0 on success, or the error from\n * bio_endio() on failure.\n *\n * WARNING: Unlike to how submit_bio() is usually used, this function does not\n * result in bio reference to be consumed. The caller must drop the reference\n * on his own.\n */\nint submit_bio_wait(struct bio *bio)\n{\n\tstruct submit_bio_ret ret;\n\n\tinit_completion(&ret.event);\n\tbio->bi_private = &ret;\n\tbio->bi_end_io = submit_bio_wait_endio;\n\tbio->bi_opf |= REQ_SYNC;\n\tsubmit_bio(bio);\n\twait_for_completion_io(&ret.event);\n\n\treturn ret.error;\n}\nEXPORT_SYMBOL(submit_bio_wait);\n\n/**\n * bio_advance - increment/complete a bio by some number of bytes\n * @bio:\tbio to advance\n * @bytes:\tnumber of bytes to complete\n *\n * This updates bi_sector, bi_size and bi_idx; if the number of bytes to\n * complete doesn't align with a bvec boundary, then bv_len and bv_offset will\n * be updated on the last bvec as well.\n *\n * @bio will then represent the remaining, uncompleted portion of the io.\n */\nvoid bio_advance(struct bio *bio, unsigned bytes)\n{\n\tif (bio_integrity(bio))\n\t\tbio_integrity_advance(bio, bytes);\n\n\tbio_advance_iter(bio, &bio->bi_iter, bytes);\n}\nEXPORT_SYMBOL(bio_advance);\n\n/**\n * bio_alloc_pages - allocates a single page for each bvec in a bio\n * @bio: bio to allocate pages for\n * @gfp_mask: flags for allocation\n *\n * Allocates pages up to @bio->bi_vcnt.\n *\n * Returns 0 on success, -ENOMEM on failure. On failure, any allocated pages are\n * freed.\n */\nint bio_alloc_pages(struct bio *bio, gfp_t gfp_mask)\n{\n\tint i;\n\tstruct bio_vec *bv;\n\n\tbio_for_each_segment_all(bv, bio, i) {\n\t\tbv->bv_page = alloc_page(gfp_mask);\n\t\tif (!bv->bv_page) {\n\t\t\twhile (--bv >= bio->bi_io_vec)\n\t\t\t\t__free_page(bv->bv_page);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(bio_alloc_pages);\n\n/**\n * bio_copy_data - copy contents of data buffers from one chain of bios to\n * another\n * @src: source bio list\n * @dst: destination bio list\n *\n * If @src and @dst are single bios, bi_next must be NULL - otherwise, treats\n * @src and @dst as linked lists of bios.\n *\n * Stops when it reaches the end of either @src or @dst - that is, copies\n * min(src->bi_size, dst->bi_size) bytes (or the equivalent for lists of bios).\n */\nvoid bio_copy_data(struct bio *dst, struct bio *src)\n{\n\tstruct bvec_iter src_iter, dst_iter;\n\tstruct bio_vec src_bv, dst_bv;\n\tvoid *src_p, *dst_p;\n\tunsigned bytes;\n\n\tsrc_iter = src->bi_iter;\n\tdst_iter = dst->bi_iter;\n\n\twhile (1) {\n\t\tif (!src_iter.bi_size) {\n\t\t\tsrc = src->bi_next;\n\t\t\tif (!src)\n\t\t\t\tbreak;\n\n\t\t\tsrc_iter = src->bi_iter;\n\t\t}\n\n\t\tif (!dst_iter.bi_size) {\n\t\t\tdst = dst->bi_next;\n\t\t\tif (!dst)\n\t\t\t\tbreak;\n\n\t\t\tdst_iter = dst->bi_iter;\n\t\t}\n\n\t\tsrc_bv = bio_iter_iovec(src, src_iter);\n\t\tdst_bv = bio_iter_iovec(dst, dst_iter);\n\n\t\tbytes = min(src_bv.bv_len, dst_bv.bv_len);\n\n\t\tsrc_p = kmap_atomic(src_bv.bv_page);\n\t\tdst_p = kmap_atomic(dst_bv.bv_page);\n\n\t\tmemcpy(dst_p + dst_bv.bv_offset,\n\t\t       src_p + src_bv.bv_offset,\n\t\t       bytes);\n\n\t\tkunmap_atomic(dst_p);\n\t\tkunmap_atomic(src_p);\n\n\t\tbio_advance_iter(src, &src_iter, bytes);\n\t\tbio_advance_iter(dst, &dst_iter, bytes);\n\t}\n}\nEXPORT_SYMBOL(bio_copy_data);\n\nstruct bio_map_data {\n\tint is_our_pages;\n\tstruct iov_iter iter;\n\tstruct iovec iov[];\n};\n\nstatic struct bio_map_data *bio_alloc_map_data(unsigned int iov_count,\n\t\t\t\t\t       gfp_t gfp_mask)\n{\n\tif (iov_count > UIO_MAXIOV)\n\t\treturn NULL;\n\n\treturn kmalloc(sizeof(struct bio_map_data) +\n\t\t       sizeof(struct iovec) * iov_count, gfp_mask);\n}\n\n/**\n * bio_copy_from_iter - copy all pages from iov_iter to bio\n * @bio: The &struct bio which describes the I/O as destination\n * @iter: iov_iter as source\n *\n * Copy all pages from iov_iter to bio.\n * Returns 0 on success, or error on failure.\n */\nstatic int bio_copy_from_iter(struct bio *bio, struct iov_iter iter)\n{\n\tint i;\n\tstruct bio_vec *bvec;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tssize_t ret;\n\n\t\tret = copy_page_from_iter(bvec->bv_page,\n\t\t\t\t\t  bvec->bv_offset,\n\t\t\t\t\t  bvec->bv_len,\n\t\t\t\t\t  &iter);\n\n\t\tif (!iov_iter_count(&iter))\n\t\t\tbreak;\n\n\t\tif (ret < bvec->bv_len)\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n/**\n * bio_copy_to_iter - copy all pages from bio to iov_iter\n * @bio: The &struct bio which describes the I/O as source\n * @iter: iov_iter as destination\n *\n * Copy all pages from bio to iov_iter.\n * Returns 0 on success, or error on failure.\n */\nstatic int bio_copy_to_iter(struct bio *bio, struct iov_iter iter)\n{\n\tint i;\n\tstruct bio_vec *bvec;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tssize_t ret;\n\n\t\tret = copy_page_to_iter(bvec->bv_page,\n\t\t\t\t\tbvec->bv_offset,\n\t\t\t\t\tbvec->bv_len,\n\t\t\t\t\t&iter);\n\n\t\tif (!iov_iter_count(&iter))\n\t\t\tbreak;\n\n\t\tif (ret < bvec->bv_len)\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nvoid bio_free_pages(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i)\n\t\t__free_page(bvec->bv_page);\n}\nEXPORT_SYMBOL(bio_free_pages);\n\n/**\n *\tbio_uncopy_user\t-\tfinish previously mapped bio\n *\t@bio: bio being terminated\n *\n *\tFree pages allocated from bio_copy_user_iov() and write back data\n *\tto user space in case of a read.\n */\nint bio_uncopy_user(struct bio *bio)\n{\n\tstruct bio_map_data *bmd = bio->bi_private;\n\tint ret = 0;\n\n\tif (!bio_flagged(bio, BIO_NULL_MAPPED)) {\n\t\t/*\n\t\t * if we're in a workqueue, the request is orphaned, so\n\t\t * don't copy into a random user address space, just free\n\t\t * and return -EINTR so user space doesn't expect any data.\n\t\t */\n\t\tif (!current->mm)\n\t\t\tret = -EINTR;\n\t\telse if (bio_data_dir(bio) == READ)\n\t\t\tret = bio_copy_to_iter(bio, bmd->iter);\n\t\tif (bmd->is_our_pages)\n\t\t\tbio_free_pages(bio);\n\t}\n\tkfree(bmd);\n\tbio_put(bio);\n\treturn ret;\n}\n\n/**\n *\tbio_copy_user_iov\t-\tcopy user data to bio\n *\t@q:\t\tdestination block queue\n *\t@map_data:\tpointer to the rq_map_data holding pages (if necessary)\n *\t@iter:\t\tiovec iterator\n *\t@gfp_mask:\tmemory allocation flags\n *\n *\tPrepares and returns a bio for indirect user io, bouncing data\n *\tto/from kernel pages as necessary. Must be paired with\n *\tcall bio_uncopy_user() on io completion.\n */\nstruct bio *bio_copy_user_iov(struct request_queue *q,\n\t\t\t      struct rq_map_data *map_data,\n\t\t\t      const struct iov_iter *iter,\n\t\t\t      gfp_t gfp_mask)\n{\n\tstruct bio_map_data *bmd;\n\tstruct page *page;\n\tstruct bio *bio;\n\tint i, ret;\n\tint nr_pages = 0;\n\tunsigned int len = iter->count;\n\tunsigned int offset = map_data ? offset_in_page(map_data->offset) : 0;\n\n\tfor (i = 0; i < iter->nr_segs; i++) {\n\t\tunsigned long uaddr;\n\t\tunsigned long end;\n\t\tunsigned long start;\n\n\t\tuaddr = (unsigned long) iter->iov[i].iov_base;\n\t\tend = (uaddr + iter->iov[i].iov_len + PAGE_SIZE - 1)\n\t\t\t>> PAGE_SHIFT;\n\t\tstart = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t}\n\n\tif (offset)\n\t\tnr_pages++;\n\n\tbmd = bio_alloc_map_data(iter->nr_segs, gfp_mask);\n\tif (!bmd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * We need to do a deep copy of the iov_iter including the iovecs.\n\t * The caller provided iov might point to an on-stack or otherwise\n\t * shortlived one.\n\t */\n\tbmd->is_our_pages = map_data ? 0 : 1;\n\tmemcpy(bmd->iov, iter->iov, sizeof(struct iovec) * iter->nr_segs);\n\tiov_iter_init(&bmd->iter, iter->type, bmd->iov,\n\t\t\titer->nr_segs, iter->count);\n\n\tret = -ENOMEM;\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\tgoto out_bmd;\n\n\tret = 0;\n\n\tif (map_data) {\n\t\tnr_pages = 1 << map_data->page_order;\n\t\ti = map_data->offset / PAGE_SIZE;\n\t}\n\twhile (len) {\n\t\tunsigned int bytes = PAGE_SIZE;\n\n\t\tbytes -= offset;\n\n\t\tif (bytes > len)\n\t\t\tbytes = len;\n\n\t\tif (map_data) {\n\t\t\tif (i == map_data->nr_entries * nr_pages) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tpage = map_data->pages[i / nr_pages];\n\t\t\tpage += (i % nr_pages);\n\n\t\t\ti++;\n\t\t} else {\n\t\t\tpage = alloc_page(q->bounce_gfp | gfp_mask);\n\t\t\tif (!page) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (bio_add_pc_page(q, bio, page, bytes, offset) < bytes)\n\t\t\tbreak;\n\n\t\tlen -= bytes;\n\t\toffset = 0;\n\t}\n\n\tif (ret)\n\t\tgoto cleanup;\n\n\t/*\n\t * success\n\t */\n\tif (((iter->type & WRITE) && (!map_data || !map_data->null_mapped)) ||\n\t    (map_data && map_data->from_user)) {\n\t\tret = bio_copy_from_iter(bio, *iter);\n\t\tif (ret)\n\t\t\tgoto cleanup;\n\t}\n\n\tbio->bi_private = bmd;\n\treturn bio;\ncleanup:\n\tif (!map_data)\n\t\tbio_free_pages(bio);\n\tbio_put(bio);\nout_bmd:\n\tkfree(bmd);\n\treturn ERR_PTR(ret);\n}\n\n/**\n *\tbio_map_user_iov - map user iovec into bio\n *\t@q:\t\tthe struct request_queue for the bio\n *\t@iter:\t\tiovec iterator\n *\t@gfp_mask:\tmemory allocation flags\n *\n *\tMap the user space address into a bio suitable for io to a block\n *\tdevice. Returns an error pointer in case of error.\n */\nstruct bio *bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask)\n{\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (ret < local_nr_pages) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * check if vector was merged with previous\n\t\t\t * drop page reference if needed\n\t\t\t */\n\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)\n\t\t\t\tput_page(pages[j]);\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tfor (j = 0; j < nr_pages; j++) {\n\t\tif (!pages[j])\n\t\t\tbreak;\n\t\tput_page(pages[j]);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}\n\nstatic void __bio_unmap_user(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n\t/*\n\t * make sure we dirty pages we wrote to\n\t */\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tif (bio_data_dir(bio) == READ)\n\t\t\tset_page_dirty_lock(bvec->bv_page);\n\n\t\tput_page(bvec->bv_page);\n\t}\n\n\tbio_put(bio);\n}\n\n/**\n *\tbio_unmap_user\t-\tunmap a bio\n *\t@bio:\t\tthe bio being unmapped\n *\n *\tUnmap a bio previously mapped by bio_map_user_iov(). Must be called from\n *\tprocess context.\n *\n *\tbio_unmap_user() may sleep.\n */\nvoid bio_unmap_user(struct bio *bio)\n{\n\t__bio_unmap_user(bio);\n\tbio_put(bio);\n}\n\nstatic void bio_map_kern_endio(struct bio *bio)\n{\n\tbio_put(bio);\n}\n\n/**\n *\tbio_map_kern\t-\tmap kernel address into bio\n *\t@q: the struct request_queue for the bio\n *\t@data: pointer to buffer to map\n *\t@len: length in bytes\n *\t@gfp_mask: allocation flags for bio allocation\n *\n *\tMap the kernel address into a bio suitable for io to a block\n *\tdevice. Returns an error pointer in case of error.\n */\nstruct bio *bio_map_kern(struct request_queue *q, void *data, unsigned int len,\n\t\t\t gfp_t gfp_mask)\n{\n\tunsigned long kaddr = (unsigned long)data;\n\tunsigned long end = (kaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tunsigned long start = kaddr >> PAGE_SHIFT;\n\tconst int nr_pages = end - start;\n\tint offset, i;\n\tstruct bio *bio;\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\toffset = offset_in_page(kaddr);\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned int bytes = PAGE_SIZE - offset;\n\n\t\tif (len <= 0)\n\t\t\tbreak;\n\n\t\tif (bytes > len)\n\t\t\tbytes = len;\n\n\t\tif (bio_add_pc_page(q, bio, virt_to_page(data), bytes,\n\t\t\t\t    offset) < bytes) {\n\t\t\t/* we don't support partial mappings */\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\tdata += bytes;\n\t\tlen -= bytes;\n\t\toffset = 0;\n\t}\n\n\tbio->bi_end_io = bio_map_kern_endio;\n\treturn bio;\n}\nEXPORT_SYMBOL(bio_map_kern);\n\nstatic void bio_copy_kern_endio(struct bio *bio)\n{\n\tbio_free_pages(bio);\n\tbio_put(bio);\n}\n\nstatic void bio_copy_kern_endio_read(struct bio *bio)\n{\n\tchar *p = bio->bi_private;\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tmemcpy(p, page_address(bvec->bv_page), bvec->bv_len);\n\t\tp += bvec->bv_len;\n\t}\n\n\tbio_copy_kern_endio(bio);\n}\n\n/**\n *\tbio_copy_kern\t-\tcopy kernel address into bio\n *\t@q: the struct request_queue for the bio\n *\t@data: pointer to buffer to copy\n *\t@len: length in bytes\n *\t@gfp_mask: allocation flags for bio and page allocation\n *\t@reading: data direction is READ\n *\n *\tcopy the kernel address into a bio suitable for io to a block\n *\tdevice. Returns an error pointer in case of error.\n */\nstruct bio *bio_copy_kern(struct request_queue *q, void *data, unsigned int len,\n\t\t\t  gfp_t gfp_mask, int reading)\n{\n\tunsigned long kaddr = (unsigned long)data;\n\tunsigned long end = (kaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tunsigned long start = kaddr >> PAGE_SHIFT;\n\tstruct bio *bio;\n\tvoid *p = data;\n\tint nr_pages = 0;\n\n\t/*\n\t * Overflow, abort\n\t */\n\tif (end < start)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tnr_pages = end - start;\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\twhile (len) {\n\t\tstruct page *page;\n\t\tunsigned int bytes = PAGE_SIZE;\n\n\t\tif (bytes > len)\n\t\t\tbytes = len;\n\n\t\tpage = alloc_page(q->bounce_gfp | gfp_mask);\n\t\tif (!page)\n\t\t\tgoto cleanup;\n\n\t\tif (!reading)\n\t\t\tmemcpy(page_address(page), p, bytes);\n\n\t\tif (bio_add_pc_page(q, bio, page, bytes, 0) < bytes)\n\t\t\tbreak;\n\n\t\tlen -= bytes;\n\t\tp += bytes;\n\t}\n\n\tif (reading) {\n\t\tbio->bi_end_io = bio_copy_kern_endio_read;\n\t\tbio->bi_private = data;\n\t} else {\n\t\tbio->bi_end_io = bio_copy_kern_endio;\n\t}\n\n\treturn bio;\n\ncleanup:\n\tbio_free_pages(bio);\n\tbio_put(bio);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/*\n * bio_set_pages_dirty() and bio_check_pages_dirty() are support functions\n * for performing direct-IO in BIOs.\n *\n * The problem is that we cannot run set_page_dirty() from interrupt context\n * because the required locks are not interrupt-safe.  So what we can do is to\n * mark the pages dirty _before_ performing IO.  And in interrupt context,\n * check that the pages are still dirty.   If so, fine.  If not, redirty them\n * in process context.\n *\n * We special-case compound pages here: normally this means reads into hugetlb\n * pages.  The logic in here doesn't really work right for compound pages\n * because the VM does not uniformly chase down the head page in all cases.\n * But dirtiness of compound pages is pretty meaningless anyway: the VM doesn't\n * handle them at all.  So we skip compound pages here at an early stage.\n *\n * Note that this code is very hard to test under normal circumstances because\n * direct-io pins the pages with get_user_pages().  This makes\n * is_page_cache_freeable return false, and the VM will not clean the pages.\n * But other code (eg, flusher threads) could clean the pages if they are mapped\n * pagecache.\n *\n * Simply disabling the call to bio_set_pages_dirty() is a good way to test the\n * deferred bio dirtying paths.\n */\n\n/*\n * bio_set_pages_dirty() will mark all the bio's pages as dirty.\n */\nvoid bio_set_pages_dirty(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tif (page && !PageCompound(page))\n\t\t\tset_page_dirty_lock(page);\n\t}\n}\n\nstatic void bio_release_pages(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tif (page)\n\t\t\tput_page(page);\n\t}\n}\n\n/*\n * bio_check_pages_dirty() will check that all the BIO's pages are still dirty.\n * If they are, then fine.  If, however, some pages are clean then they must\n * have been written out during the direct-IO read.  So we take another ref on\n * the BIO and the offending pages and re-dirty the pages in process context.\n *\n * It is expected that bio_check_pages_dirty() will wholly own the BIO from\n * here on.  It will run one put_page() against each page and will run one\n * bio_put() against the BIO.\n */\n\nstatic void bio_dirty_fn(struct work_struct *work);\n\nstatic DECLARE_WORK(bio_dirty_work, bio_dirty_fn);\nstatic DEFINE_SPINLOCK(bio_dirty_lock);\nstatic struct bio *bio_dirty_list;\n\n/*\n * This runs in process context\n */\nstatic void bio_dirty_fn(struct work_struct *work)\n{\n\tunsigned long flags;\n\tstruct bio *bio;\n\n\tspin_lock_irqsave(&bio_dirty_lock, flags);\n\tbio = bio_dirty_list;\n\tbio_dirty_list = NULL;\n\tspin_unlock_irqrestore(&bio_dirty_lock, flags);\n\n\twhile (bio) {\n\t\tstruct bio *next = bio->bi_private;\n\n\t\tbio_set_pages_dirty(bio);\n\t\tbio_release_pages(bio);\n\t\tbio_put(bio);\n\t\tbio = next;\n\t}\n}\n\nvoid bio_check_pages_dirty(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint nr_clean_pages = 0;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tif (PageDirty(page) || PageCompound(page)) {\n\t\t\tput_page(page);\n\t\t\tbvec->bv_page = NULL;\n\t\t} else {\n\t\t\tnr_clean_pages++;\n\t\t}\n\t}\n\n\tif (nr_clean_pages) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&bio_dirty_lock, flags);\n\t\tbio->bi_private = bio_dirty_list;\n\t\tbio_dirty_list = bio;\n\t\tspin_unlock_irqrestore(&bio_dirty_lock, flags);\n\t\tschedule_work(&bio_dirty_work);\n\t} else {\n\t\tbio_put(bio);\n\t}\n}\n\nvoid generic_start_io_acct(struct request_queue *q, int rw,\n\t\t\t   unsigned long sectors, struct hd_struct *part)\n{\n\tint cpu = part_stat_lock();\n\n\tpart_round_stats(q, cpu, part);\n\tpart_stat_inc(cpu, part, ios[rw]);\n\tpart_stat_add(cpu, part, sectors[rw], sectors);\n\tpart_inc_in_flight(q, part, rw);\n\n\tpart_stat_unlock();\n}\nEXPORT_SYMBOL(generic_start_io_acct);\n\nvoid generic_end_io_acct(struct request_queue *q, int rw,\n\t\t\t struct hd_struct *part, unsigned long start_time)\n{\n\tunsigned long duration = jiffies - start_time;\n\tint cpu = part_stat_lock();\n\n\tpart_stat_add(cpu, part, ticks[rw], duration);\n\tpart_round_stats(q, cpu, part);\n\tpart_dec_in_flight(q, part, rw);\n\n\tpart_stat_unlock();\n}\nEXPORT_SYMBOL(generic_end_io_acct);\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\nvoid bio_flush_dcache_pages(struct bio *bi)\n{\n\tstruct bio_vec bvec;\n\tstruct bvec_iter iter;\n\n\tbio_for_each_segment(bvec, bi, iter)\n\t\tflush_dcache_page(bvec.bv_page);\n}\nEXPORT_SYMBOL(bio_flush_dcache_pages);\n#endif\n\nstatic inline bool bio_remaining_done(struct bio *bio)\n{\n\t/*\n\t * If we're not chaining, then ->__bi_remaining is always 1 and\n\t * we always end io on the first invocation.\n\t */\n\tif (!bio_flagged(bio, BIO_CHAIN))\n\t\treturn true;\n\n\tBUG_ON(atomic_read(&bio->__bi_remaining) <= 0);\n\n\tif (atomic_dec_and_test(&bio->__bi_remaining)) {\n\t\tbio_clear_flag(bio, BIO_CHAIN);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/**\n * bio_endio - end I/O on a bio\n * @bio:\tbio\n *\n * Description:\n *   bio_endio() will end I/O on the whole bio. bio_endio() is the preferred\n *   way to end I/O on a bio. No one should call bi_end_io() directly on a\n *   bio unless they own it and thus know that it has an end_io function.\n *\n *   bio_endio() can be called several times on a bio that has been chained\n *   using bio_chain().  The ->bi_end_io() function will only be called the\n *   last time.  At this point the BLK_TA_COMPLETE tracing event will be\n *   generated if BIO_TRACE_COMPLETION is set.\n **/\nvoid bio_endio(struct bio *bio)\n{\nagain:\n\tif (!bio_remaining_done(bio))\n\t\treturn;\n\tif (!bio_integrity_endio(bio))\n\t\treturn;\n\n\t/*\n\t * Need to have a real endio function for chained bios, otherwise\n\t * various corner cases will break (like stacking block devices that\n\t * save/restore bi_end_io) - however, we want to avoid unbounded\n\t * recursion and blowing the stack. Tail call optimization would\n\t * handle this, but compiling with frame pointers also disables\n\t * gcc's sibling call optimization.\n\t */\n\tif (bio->bi_end_io == bio_chain_endio) {\n\t\tbio = __bio_chain_endio(bio);\n\t\tgoto again;\n\t}\n\n\tif (bio->bi_disk && bio_flagged(bio, BIO_TRACE_COMPLETION)) {\n\t\ttrace_block_bio_complete(bio->bi_disk->queue, bio,\n\t\t\t\t\t blk_status_to_errno(bio->bi_status));\n\t\tbio_clear_flag(bio, BIO_TRACE_COMPLETION);\n\t}\n\n\tblk_throtl_bio_endio(bio);\n\t/* release cgroup info */\n\tbio_uninit(bio);\n\tif (bio->bi_end_io)\n\t\tbio->bi_end_io(bio);\n}\nEXPORT_SYMBOL(bio_endio);\n\n/**\n * bio_split - split a bio\n * @bio:\tbio to split\n * @sectors:\tnumber of sectors to split from the front of @bio\n * @gfp:\tgfp mask\n * @bs:\t\tbio set to allocate from\n *\n * Allocates and returns a new bio which represents @sectors from the start of\n * @bio, and updates @bio to represent the remaining sectors.\n *\n * Unless this is a discard request the newly allocated bio will point\n * to @bio's bi_io_vec; it is the caller's responsibility to ensure that\n * @bio is not freed before the split.\n */\nstruct bio *bio_split(struct bio *bio, int sectors,\n\t\t      gfp_t gfp, struct bio_set *bs)\n{\n\tstruct bio *split = NULL;\n\n\tBUG_ON(sectors <= 0);\n\tBUG_ON(sectors >= bio_sectors(bio));\n\n\tsplit = bio_clone_fast(bio, gfp, bs);\n\tif (!split)\n\t\treturn NULL;\n\n\tsplit->bi_iter.bi_size = sectors << 9;\n\n\tif (bio_integrity(split))\n\t\tbio_integrity_trim(split);\n\n\tbio_advance(bio, split->bi_iter.bi_size);\n\n\tif (bio_flagged(bio, BIO_TRACE_COMPLETION))\n\t\tbio_set_flag(bio, BIO_TRACE_COMPLETION);\n\n\treturn split;\n}\nEXPORT_SYMBOL(bio_split);\n\n/**\n * bio_trim - trim a bio\n * @bio:\tbio to trim\n * @offset:\tnumber of sectors to trim from the front of @bio\n * @size:\tsize we want to trim @bio to, in sectors\n */\nvoid bio_trim(struct bio *bio, int offset, int size)\n{\n\t/* 'bio' is a cloned bio which we need to trim to match\n\t * the given offset and size.\n\t */\n\n\tsize <<= 9;\n\tif (offset == 0 && size == bio->bi_iter.bi_size)\n\t\treturn;\n\n\tbio_clear_flag(bio, BIO_SEG_VALID);\n\n\tbio_advance(bio, offset << 9);\n\n\tbio->bi_iter.bi_size = size;\n\n\tif (bio_integrity(bio))\n\t\tbio_integrity_trim(bio);\n\n}\nEXPORT_SYMBOL_GPL(bio_trim);\n\n/*\n * create memory pools for biovec's in a bio_set.\n * use the global biovec slabs created for general use.\n */\nmempool_t *biovec_create_pool(int pool_entries)\n{\n\tstruct biovec_slab *bp = bvec_slabs + BVEC_POOL_MAX;\n\n\treturn mempool_create_slab_pool(pool_entries, bp->slab);\n}\n\nvoid bioset_free(struct bio_set *bs)\n{\n\tif (bs->rescue_workqueue)\n\t\tdestroy_workqueue(bs->rescue_workqueue);\n\n\tif (bs->bio_pool)\n\t\tmempool_destroy(bs->bio_pool);\n\n\tif (bs->bvec_pool)\n\t\tmempool_destroy(bs->bvec_pool);\n\n\tbioset_integrity_free(bs);\n\tbio_put_slab(bs);\n\n\tkfree(bs);\n}\nEXPORT_SYMBOL(bioset_free);\n\n/**\n * bioset_create  - Create a bio_set\n * @pool_size:\tNumber of bio and bio_vecs to cache in the mempool\n * @front_pad:\tNumber of bytes to allocate in front of the returned bio\n * @flags:\tFlags to modify behavior, currently %BIOSET_NEED_BVECS\n *              and %BIOSET_NEED_RESCUER\n *\n * Description:\n *    Set up a bio_set to be used with @bio_alloc_bioset. Allows the caller\n *    to ask for a number of bytes to be allocated in front of the bio.\n *    Front pad allocation is useful for embedding the bio inside\n *    another structure, to avoid allocating extra data to go with the bio.\n *    Note that the bio must be embedded at the END of that structure always,\n *    or things will break badly.\n *    If %BIOSET_NEED_BVECS is set in @flags, a separate pool will be allocated\n *    for allocating iovecs.  This pool is not needed e.g. for bio_clone_fast().\n *    If %BIOSET_NEED_RESCUER is set, a workqueue is created which can be used to\n *    dispatch queued requests when the mempool runs out of space.\n *\n */\nstruct bio_set *bioset_create(unsigned int pool_size,\n\t\t\t      unsigned int front_pad,\n\t\t\t      int flags)\n{\n\tunsigned int back_pad = BIO_INLINE_VECS * sizeof(struct bio_vec);\n\tstruct bio_set *bs;\n\n\tbs = kzalloc(sizeof(*bs), GFP_KERNEL);\n\tif (!bs)\n\t\treturn NULL;\n\n\tbs->front_pad = front_pad;\n\n\tspin_lock_init(&bs->rescue_lock);\n\tbio_list_init(&bs->rescue_list);\n\tINIT_WORK(&bs->rescue_work, bio_alloc_rescue);\n\n\tbs->bio_slab = bio_find_or_create_slab(front_pad + back_pad);\n\tif (!bs->bio_slab) {\n\t\tkfree(bs);\n\t\treturn NULL;\n\t}\n\n\tbs->bio_pool = mempool_create_slab_pool(pool_size, bs->bio_slab);\n\tif (!bs->bio_pool)\n\t\tgoto bad;\n\n\tif (flags & BIOSET_NEED_BVECS) {\n\t\tbs->bvec_pool = biovec_create_pool(pool_size);\n\t\tif (!bs->bvec_pool)\n\t\t\tgoto bad;\n\t}\n\n\tif (!(flags & BIOSET_NEED_RESCUER))\n\t\treturn bs;\n\n\tbs->rescue_workqueue = alloc_workqueue(\"bioset\", WQ_MEM_RECLAIM, 0);\n\tif (!bs->rescue_workqueue)\n\t\tgoto bad;\n\n\treturn bs;\nbad:\n\tbioset_free(bs);\n\treturn NULL;\n}\nEXPORT_SYMBOL(bioset_create);\n\n#ifdef CONFIG_BLK_CGROUP\n\n/**\n * bio_associate_blkcg - associate a bio with the specified blkcg\n * @bio: target bio\n * @blkcg_css: css of the blkcg to associate\n *\n * Associate @bio with the blkcg specified by @blkcg_css.  Block layer will\n * treat @bio as if it were issued by a task which belongs to the blkcg.\n *\n * This function takes an extra reference of @blkcg_css which will be put\n * when @bio is released.  The caller must own @bio and is responsible for\n * synchronizing calls to this function.\n */\nint bio_associate_blkcg(struct bio *bio, struct cgroup_subsys_state *blkcg_css)\n{\n\tif (unlikely(bio->bi_css))\n\t\treturn -EBUSY;\n\tcss_get(blkcg_css);\n\tbio->bi_css = blkcg_css;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bio_associate_blkcg);\n\n/**\n * bio_associate_current - associate a bio with %current\n * @bio: target bio\n *\n * Associate @bio with %current if it hasn't been associated yet.  Block\n * layer will treat @bio as if it were issued by %current no matter which\n * task actually issues it.\n *\n * This function takes an extra reference of @task's io_context and blkcg\n * which will be put when @bio is released.  The caller must own @bio,\n * ensure %current->io_context exists, and is responsible for synchronizing\n * calls to this function.\n */\nint bio_associate_current(struct bio *bio)\n{\n\tstruct io_context *ioc;\n\n\tif (bio->bi_css)\n\t\treturn -EBUSY;\n\n\tioc = current->io_context;\n\tif (!ioc)\n\t\treturn -ENOENT;\n\n\tget_io_context_active(ioc);\n\tbio->bi_ioc = ioc;\n\tbio->bi_css = task_get_css(current, io_cgrp_id);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bio_associate_current);\n\n/**\n * bio_disassociate_task - undo bio_associate_current()\n * @bio: target bio\n */\nvoid bio_disassociate_task(struct bio *bio)\n{\n\tif (bio->bi_ioc) {\n\t\tput_io_context(bio->bi_ioc);\n\t\tbio->bi_ioc = NULL;\n\t}\n\tif (bio->bi_css) {\n\t\tcss_put(bio->bi_css);\n\t\tbio->bi_css = NULL;\n\t}\n}\n\n/**\n * bio_clone_blkcg_association - clone blkcg association from src to dst bio\n * @dst: destination bio\n * @src: source bio\n */\nvoid bio_clone_blkcg_association(struct bio *dst, struct bio *src)\n{\n\tif (src->bi_css)\n\t\tWARN_ON(bio_associate_blkcg(dst, src->bi_css));\n}\nEXPORT_SYMBOL_GPL(bio_clone_blkcg_association);\n#endif /* CONFIG_BLK_CGROUP */\n\nstatic void __init biovec_init_slabs(void)\n{\n\tint i;\n\n\tfor (i = 0; i < BVEC_POOL_NR; i++) {\n\t\tint size;\n\t\tstruct biovec_slab *bvs = bvec_slabs + i;\n\n\t\tif (bvs->nr_vecs <= BIO_INLINE_VECS) {\n\t\t\tbvs->slab = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tsize = bvs->nr_vecs * sizeof(struct bio_vec);\n\t\tbvs->slab = kmem_cache_create(bvs->name, size, 0,\n                                SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\n\t}\n}\n\nstatic int __init init_bio(void)\n{\n\tbio_slab_max = 2;\n\tbio_slab_nr = 0;\n\tbio_slabs = kzalloc(bio_slab_max * sizeof(struct bio_slab), GFP_KERNEL);\n\tif (!bio_slabs)\n\t\tpanic(\"bio: can't allocate bios\\n\");\n\n\tbio_integrity_init();\n\tbiovec_init_slabs();\n\n\tfs_bio_set = bioset_create(BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);\n\tif (!fs_bio_set)\n\t\tpanic(\"bio: can't allocate bios\\n\");\n\n\tif (bioset_integrity_create(fs_bio_set, BIO_POOL_SIZE))\n\t\tpanic(\"bio: can't create integrity pool\\n\");\n\n\treturn 0;\n}\nsubsys_initcall(init_bio);\n"], "fixing_code": ["/*\n * Copyright (C) 2001 Jens Axboe <axboe@kernel.dk>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n *\n */\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/uio.h>\n#include <linux/iocontext.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/mempool.h>\n#include <linux/workqueue.h>\n#include <linux/cgroup.h>\n\n#include <trace/events/block.h>\n#include \"blk.h\"\n\n/*\n * Test patch to inline a certain number of bi_io_vec's inside the bio\n * itself, to shrink a bio data allocation from two mempool calls to one\n */\n#define BIO_INLINE_VECS\t\t4\n\n/*\n * if you change this list, also change bvec_alloc or things will\n * break badly! cannot be bigger than what you can fit into an\n * unsigned short\n */\n#define BV(x) { .nr_vecs = x, .name = \"biovec-\"__stringify(x) }\nstatic struct biovec_slab bvec_slabs[BVEC_POOL_NR] __read_mostly = {\n\tBV(1), BV(4), BV(16), BV(64), BV(128), BV(BIO_MAX_PAGES),\n};\n#undef BV\n\n/*\n * fs_bio_set is the bio_set containing bio and iovec memory pools used by\n * IO code that does not need private memory pools.\n */\nstruct bio_set *fs_bio_set;\nEXPORT_SYMBOL(fs_bio_set);\n\n/*\n * Our slab pool management\n */\nstruct bio_slab {\n\tstruct kmem_cache *slab;\n\tunsigned int slab_ref;\n\tunsigned int slab_size;\n\tchar name[8];\n};\nstatic DEFINE_MUTEX(bio_slab_lock);\nstatic struct bio_slab *bio_slabs;\nstatic unsigned int bio_slab_nr, bio_slab_max;\n\nstatic struct kmem_cache *bio_find_or_create_slab(unsigned int extra_size)\n{\n\tunsigned int sz = sizeof(struct bio) + extra_size;\n\tstruct kmem_cache *slab = NULL;\n\tstruct bio_slab *bslab, *new_bio_slabs;\n\tunsigned int new_bio_slab_max;\n\tunsigned int i, entry = -1;\n\n\tmutex_lock(&bio_slab_lock);\n\n\ti = 0;\n\twhile (i < bio_slab_nr) {\n\t\tbslab = &bio_slabs[i];\n\n\t\tif (!bslab->slab && entry == -1)\n\t\t\tentry = i;\n\t\telse if (bslab->slab_size == sz) {\n\t\t\tslab = bslab->slab;\n\t\t\tbslab->slab_ref++;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\n\tif (slab)\n\t\tgoto out_unlock;\n\n\tif (bio_slab_nr == bio_slab_max && entry == -1) {\n\t\tnew_bio_slab_max = bio_slab_max << 1;\n\t\tnew_bio_slabs = krealloc(bio_slabs,\n\t\t\t\t\t new_bio_slab_max * sizeof(struct bio_slab),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!new_bio_slabs)\n\t\t\tgoto out_unlock;\n\t\tbio_slab_max = new_bio_slab_max;\n\t\tbio_slabs = new_bio_slabs;\n\t}\n\tif (entry == -1)\n\t\tentry = bio_slab_nr++;\n\n\tbslab = &bio_slabs[entry];\n\n\tsnprintf(bslab->name, sizeof(bslab->name), \"bio-%d\", entry);\n\tslab = kmem_cache_create(bslab->name, sz, ARCH_KMALLOC_MINALIGN,\n\t\t\t\t SLAB_HWCACHE_ALIGN, NULL);\n\tif (!slab)\n\t\tgoto out_unlock;\n\n\tbslab->slab = slab;\n\tbslab->slab_ref = 1;\n\tbslab->slab_size = sz;\nout_unlock:\n\tmutex_unlock(&bio_slab_lock);\n\treturn slab;\n}\n\nstatic void bio_put_slab(struct bio_set *bs)\n{\n\tstruct bio_slab *bslab = NULL;\n\tunsigned int i;\n\n\tmutex_lock(&bio_slab_lock);\n\n\tfor (i = 0; i < bio_slab_nr; i++) {\n\t\tif (bs->bio_slab == bio_slabs[i].slab) {\n\t\t\tbslab = &bio_slabs[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (WARN(!bslab, KERN_ERR \"bio: unable to find slab!\\n\"))\n\t\tgoto out;\n\n\tWARN_ON(!bslab->slab_ref);\n\n\tif (--bslab->slab_ref)\n\t\tgoto out;\n\n\tkmem_cache_destroy(bslab->slab);\n\tbslab->slab = NULL;\n\nout:\n\tmutex_unlock(&bio_slab_lock);\n}\n\nunsigned int bvec_nr_vecs(unsigned short idx)\n{\n\treturn bvec_slabs[idx].nr_vecs;\n}\n\nvoid bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned int idx)\n{\n\tif (!idx)\n\t\treturn;\n\tidx--;\n\n\tBIO_BUG_ON(idx >= BVEC_POOL_NR);\n\n\tif (idx == BVEC_POOL_MAX) {\n\t\tmempool_free(bv, pool);\n\t} else {\n\t\tstruct biovec_slab *bvs = bvec_slabs + idx;\n\n\t\tkmem_cache_free(bvs->slab, bv);\n\t}\n}\n\nstruct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,\n\t\t\t   mempool_t *pool)\n{\n\tstruct bio_vec *bvl;\n\n\t/*\n\t * see comment near bvec_array define!\n\t */\n\tswitch (nr) {\n\tcase 1:\n\t\t*idx = 0;\n\t\tbreak;\n\tcase 2 ... 4:\n\t\t*idx = 1;\n\t\tbreak;\n\tcase 5 ... 16:\n\t\t*idx = 2;\n\t\tbreak;\n\tcase 17 ... 64:\n\t\t*idx = 3;\n\t\tbreak;\n\tcase 65 ... 128:\n\t\t*idx = 4;\n\t\tbreak;\n\tcase 129 ... BIO_MAX_PAGES:\n\t\t*idx = 5;\n\t\tbreak;\n\tdefault:\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * idx now points to the pool we want to allocate from. only the\n\t * 1-vec entry pool is mempool backed.\n\t */\n\tif (*idx == BVEC_POOL_MAX) {\nfallback:\n\t\tbvl = mempool_alloc(pool, gfp_mask);\n\t} else {\n\t\tstruct biovec_slab *bvs = bvec_slabs + *idx;\n\t\tgfp_t __gfp_mask = gfp_mask & ~(__GFP_DIRECT_RECLAIM | __GFP_IO);\n\n\t\t/*\n\t\t * Make this allocation restricted and don't dump info on\n\t\t * allocation failures, since we'll fallback to the mempool\n\t\t * in case of failure.\n\t\t */\n\t\t__gfp_mask |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;\n\n\t\t/*\n\t\t * Try a slab allocation. If this fails and __GFP_DIRECT_RECLAIM\n\t\t * is set, retry with the 1-entry mempool\n\t\t */\n\t\tbvl = kmem_cache_alloc(bvs->slab, __gfp_mask);\n\t\tif (unlikely(!bvl && (gfp_mask & __GFP_DIRECT_RECLAIM))) {\n\t\t\t*idx = BVEC_POOL_MAX;\n\t\t\tgoto fallback;\n\t\t}\n\t}\n\n\t(*idx)++;\n\treturn bvl;\n}\n\nvoid bio_uninit(struct bio *bio)\n{\n\tbio_disassociate_task(bio);\n}\nEXPORT_SYMBOL(bio_uninit);\n\nstatic void bio_free(struct bio *bio)\n{\n\tstruct bio_set *bs = bio->bi_pool;\n\tvoid *p;\n\n\tbio_uninit(bio);\n\n\tif (bs) {\n\t\tbvec_free(bs->bvec_pool, bio->bi_io_vec, BVEC_POOL_IDX(bio));\n\n\t\t/*\n\t\t * If we have front padding, adjust the bio pointer before freeing\n\t\t */\n\t\tp = bio;\n\t\tp -= bs->front_pad;\n\n\t\tmempool_free(p, bs->bio_pool);\n\t} else {\n\t\t/* Bio was allocated by bio_kmalloc() */\n\t\tkfree(bio);\n\t}\n}\n\n/*\n * Users of this function have their own bio allocation. Subsequently,\n * they must remember to pair any call to bio_init() with bio_uninit()\n * when IO has completed, or when the bio is released.\n */\nvoid bio_init(struct bio *bio, struct bio_vec *table,\n\t      unsigned short max_vecs)\n{\n\tmemset(bio, 0, sizeof(*bio));\n\tatomic_set(&bio->__bi_remaining, 1);\n\tatomic_set(&bio->__bi_cnt, 1);\n\n\tbio->bi_io_vec = table;\n\tbio->bi_max_vecs = max_vecs;\n}\nEXPORT_SYMBOL(bio_init);\n\n/**\n * bio_reset - reinitialize a bio\n * @bio:\tbio to reset\n *\n * Description:\n *   After calling bio_reset(), @bio will be in the same state as a freshly\n *   allocated bio returned bio bio_alloc_bioset() - the only fields that are\n *   preserved are the ones that are initialized by bio_alloc_bioset(). See\n *   comment in struct bio.\n */\nvoid bio_reset(struct bio *bio)\n{\n\tunsigned long flags = bio->bi_flags & (~0UL << BIO_RESET_BITS);\n\n\tbio_uninit(bio);\n\n\tmemset(bio, 0, BIO_RESET_BYTES);\n\tbio->bi_flags = flags;\n\tatomic_set(&bio->__bi_remaining, 1);\n}\nEXPORT_SYMBOL(bio_reset);\n\nstatic struct bio *__bio_chain_endio(struct bio *bio)\n{\n\tstruct bio *parent = bio->bi_private;\n\n\tif (!parent->bi_status)\n\t\tparent->bi_status = bio->bi_status;\n\tbio_put(bio);\n\treturn parent;\n}\n\nstatic void bio_chain_endio(struct bio *bio)\n{\n\tbio_endio(__bio_chain_endio(bio));\n}\n\n/**\n * bio_chain - chain bio completions\n * @bio: the target bio\n * @parent: the @bio's parent bio\n *\n * The caller won't have a bi_end_io called when @bio completes - instead,\n * @parent's bi_end_io won't be called until both @parent and @bio have\n * completed; the chained bio will also be freed when it completes.\n *\n * The caller must not set bi_private or bi_end_io in @bio.\n */\nvoid bio_chain(struct bio *bio, struct bio *parent)\n{\n\tBUG_ON(bio->bi_private || bio->bi_end_io);\n\n\tbio->bi_private = parent;\n\tbio->bi_end_io\t= bio_chain_endio;\n\tbio_inc_remaining(parent);\n}\nEXPORT_SYMBOL(bio_chain);\n\nstatic void bio_alloc_rescue(struct work_struct *work)\n{\n\tstruct bio_set *bs = container_of(work, struct bio_set, rescue_work);\n\tstruct bio *bio;\n\n\twhile (1) {\n\t\tspin_lock(&bs->rescue_lock);\n\t\tbio = bio_list_pop(&bs->rescue_list);\n\t\tspin_unlock(&bs->rescue_lock);\n\n\t\tif (!bio)\n\t\t\tbreak;\n\n\t\tgeneric_make_request(bio);\n\t}\n}\n\nstatic void punt_bios_to_rescuer(struct bio_set *bs)\n{\n\tstruct bio_list punt, nopunt;\n\tstruct bio *bio;\n\n\tif (WARN_ON_ONCE(!bs->rescue_workqueue))\n\t\treturn;\n\t/*\n\t * In order to guarantee forward progress we must punt only bios that\n\t * were allocated from this bio_set; otherwise, if there was a bio on\n\t * there for a stacking driver higher up in the stack, processing it\n\t * could require allocating bios from this bio_set, and doing that from\n\t * our own rescuer would be bad.\n\t *\n\t * Since bio lists are singly linked, pop them all instead of trying to\n\t * remove from the middle of the list:\n\t */\n\n\tbio_list_init(&punt);\n\tbio_list_init(&nopunt);\n\n\twhile ((bio = bio_list_pop(&current->bio_list[0])))\n\t\tbio_list_add(bio->bi_pool == bs ? &punt : &nopunt, bio);\n\tcurrent->bio_list[0] = nopunt;\n\n\tbio_list_init(&nopunt);\n\twhile ((bio = bio_list_pop(&current->bio_list[1])))\n\t\tbio_list_add(bio->bi_pool == bs ? &punt : &nopunt, bio);\n\tcurrent->bio_list[1] = nopunt;\n\n\tspin_lock(&bs->rescue_lock);\n\tbio_list_merge(&bs->rescue_list, &punt);\n\tspin_unlock(&bs->rescue_lock);\n\n\tqueue_work(bs->rescue_workqueue, &bs->rescue_work);\n}\n\n/**\n * bio_alloc_bioset - allocate a bio for I/O\n * @gfp_mask:   the GFP_ mask given to the slab allocator\n * @nr_iovecs:\tnumber of iovecs to pre-allocate\n * @bs:\t\tthe bio_set to allocate from.\n *\n * Description:\n *   If @bs is NULL, uses kmalloc() to allocate the bio; else the allocation is\n *   backed by the @bs's mempool.\n *\n *   When @bs is not NULL, if %__GFP_DIRECT_RECLAIM is set then bio_alloc will\n *   always be able to allocate a bio. This is due to the mempool guarantees.\n *   To make this work, callers must never allocate more than 1 bio at a time\n *   from this pool. Callers that need to allocate more than 1 bio must always\n *   submit the previously allocated bio for IO before attempting to allocate\n *   a new one. Failure to do so can cause deadlocks under memory pressure.\n *\n *   Note that when running under generic_make_request() (i.e. any block\n *   driver), bios are not submitted until after you return - see the code in\n *   generic_make_request() that converts recursion into iteration, to prevent\n *   stack overflows.\n *\n *   This would normally mean allocating multiple bios under\n *   generic_make_request() would be susceptible to deadlocks, but we have\n *   deadlock avoidance code that resubmits any blocked bios from a rescuer\n *   thread.\n *\n *   However, we do not guarantee forward progress for allocations from other\n *   mempools. Doing multiple allocations from the same mempool under\n *   generic_make_request() should be avoided - instead, use bio_set's front_pad\n *   for per bio allocations.\n *\n *   RETURNS:\n *   Pointer to new bio on success, NULL on failure.\n */\nstruct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,\n\t\t\t     struct bio_set *bs)\n{\n\tgfp_t saved_gfp = gfp_mask;\n\tunsigned front_pad;\n\tunsigned inline_vecs;\n\tstruct bio_vec *bvl = NULL;\n\tstruct bio *bio;\n\tvoid *p;\n\n\tif (!bs) {\n\t\tif (nr_iovecs > UIO_MAXIOV)\n\t\t\treturn NULL;\n\n\t\tp = kmalloc(sizeof(struct bio) +\n\t\t\t    nr_iovecs * sizeof(struct bio_vec),\n\t\t\t    gfp_mask);\n\t\tfront_pad = 0;\n\t\tinline_vecs = nr_iovecs;\n\t} else {\n\t\t/* should not use nobvec bioset for nr_iovecs > 0 */\n\t\tif (WARN_ON_ONCE(!bs->bvec_pool && nr_iovecs > 0))\n\t\t\treturn NULL;\n\t\t/*\n\t\t * generic_make_request() converts recursion to iteration; this\n\t\t * means if we're running beneath it, any bios we allocate and\n\t\t * submit will not be submitted (and thus freed) until after we\n\t\t * return.\n\t\t *\n\t\t * This exposes us to a potential deadlock if we allocate\n\t\t * multiple bios from the same bio_set() while running\n\t\t * underneath generic_make_request(). If we were to allocate\n\t\t * multiple bios (say a stacking block driver that was splitting\n\t\t * bios), we would deadlock if we exhausted the mempool's\n\t\t * reserve.\n\t\t *\n\t\t * We solve this, and guarantee forward progress, with a rescuer\n\t\t * workqueue per bio_set. If we go to allocate and there are\n\t\t * bios on current->bio_list, we first try the allocation\n\t\t * without __GFP_DIRECT_RECLAIM; if that fails, we punt those\n\t\t * bios we would be blocking to the rescuer workqueue before\n\t\t * we retry with the original gfp_flags.\n\t\t */\n\n\t\tif (current->bio_list &&\n\t\t    (!bio_list_empty(&current->bio_list[0]) ||\n\t\t     !bio_list_empty(&current->bio_list[1])) &&\n\t\t    bs->rescue_workqueue)\n\t\t\tgfp_mask &= ~__GFP_DIRECT_RECLAIM;\n\n\t\tp = mempool_alloc(bs->bio_pool, gfp_mask);\n\t\tif (!p && gfp_mask != saved_gfp) {\n\t\t\tpunt_bios_to_rescuer(bs);\n\t\t\tgfp_mask = saved_gfp;\n\t\t\tp = mempool_alloc(bs->bio_pool, gfp_mask);\n\t\t}\n\n\t\tfront_pad = bs->front_pad;\n\t\tinline_vecs = BIO_INLINE_VECS;\n\t}\n\n\tif (unlikely(!p))\n\t\treturn NULL;\n\n\tbio = p + front_pad;\n\tbio_init(bio, NULL, 0);\n\n\tif (nr_iovecs > inline_vecs) {\n\t\tunsigned long idx = 0;\n\n\t\tbvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, bs->bvec_pool);\n\t\tif (!bvl && gfp_mask != saved_gfp) {\n\t\t\tpunt_bios_to_rescuer(bs);\n\t\t\tgfp_mask = saved_gfp;\n\t\t\tbvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, bs->bvec_pool);\n\t\t}\n\n\t\tif (unlikely(!bvl))\n\t\t\tgoto err_free;\n\n\t\tbio->bi_flags |= idx << BVEC_POOL_OFFSET;\n\t} else if (nr_iovecs) {\n\t\tbvl = bio->bi_inline_vecs;\n\t}\n\n\tbio->bi_pool = bs;\n\tbio->bi_max_vecs = nr_iovecs;\n\tbio->bi_io_vec = bvl;\n\treturn bio;\n\nerr_free:\n\tmempool_free(p, bs->bio_pool);\n\treturn NULL;\n}\nEXPORT_SYMBOL(bio_alloc_bioset);\n\nvoid zero_fill_bio(struct bio *bio)\n{\n\tunsigned long flags;\n\tstruct bio_vec bv;\n\tstruct bvec_iter iter;\n\n\tbio_for_each_segment(bv, bio, iter) {\n\t\tchar *data = bvec_kmap_irq(&bv, &flags);\n\t\tmemset(data, 0, bv.bv_len);\n\t\tflush_dcache_page(bv.bv_page);\n\t\tbvec_kunmap_irq(data, &flags);\n\t}\n}\nEXPORT_SYMBOL(zero_fill_bio);\n\n/**\n * bio_put - release a reference to a bio\n * @bio:   bio to release reference to\n *\n * Description:\n *   Put a reference to a &struct bio, either one you have gotten with\n *   bio_alloc, bio_get or bio_clone_*. The last put of a bio will free it.\n **/\nvoid bio_put(struct bio *bio)\n{\n\tif (!bio_flagged(bio, BIO_REFFED))\n\t\tbio_free(bio);\n\telse {\n\t\tBIO_BUG_ON(!atomic_read(&bio->__bi_cnt));\n\n\t\t/*\n\t\t * last put frees it\n\t\t */\n\t\tif (atomic_dec_and_test(&bio->__bi_cnt))\n\t\t\tbio_free(bio);\n\t}\n}\nEXPORT_SYMBOL(bio_put);\n\ninline int bio_phys_segments(struct request_queue *q, struct bio *bio)\n{\n\tif (unlikely(!bio_flagged(bio, BIO_SEG_VALID)))\n\t\tblk_recount_segments(q, bio);\n\n\treturn bio->bi_phys_segments;\n}\nEXPORT_SYMBOL(bio_phys_segments);\n\n/**\n * \t__bio_clone_fast - clone a bio that shares the original bio's biovec\n * \t@bio: destination bio\n * \t@bio_src: bio to clone\n *\n *\tClone a &bio. Caller will own the returned bio, but not\n *\tthe actual data it points to. Reference count of returned\n * \tbio will be one.\n *\n * \tCaller must ensure that @bio_src is not freed before @bio.\n */\nvoid __bio_clone_fast(struct bio *bio, struct bio *bio_src)\n{\n\tBUG_ON(bio->bi_pool && BVEC_POOL_IDX(bio));\n\n\t/*\n\t * most users will be overriding ->bi_disk with a new target,\n\t * so we don't set nor calculate new physical/hw segment counts here\n\t */\n\tbio->bi_disk = bio_src->bi_disk;\n\tbio_set_flag(bio, BIO_CLONED);\n\tbio->bi_opf = bio_src->bi_opf;\n\tbio->bi_write_hint = bio_src->bi_write_hint;\n\tbio->bi_iter = bio_src->bi_iter;\n\tbio->bi_io_vec = bio_src->bi_io_vec;\n\n\tbio_clone_blkcg_association(bio, bio_src);\n}\nEXPORT_SYMBOL(__bio_clone_fast);\n\n/**\n *\tbio_clone_fast - clone a bio that shares the original bio's biovec\n *\t@bio: bio to clone\n *\t@gfp_mask: allocation priority\n *\t@bs: bio_set to allocate from\n *\n * \tLike __bio_clone_fast, only also allocates the returned bio\n */\nstruct bio *bio_clone_fast(struct bio *bio, gfp_t gfp_mask, struct bio_set *bs)\n{\n\tstruct bio *b;\n\n\tb = bio_alloc_bioset(gfp_mask, 0, bs);\n\tif (!b)\n\t\treturn NULL;\n\n\t__bio_clone_fast(b, bio);\n\n\tif (bio_integrity(bio)) {\n\t\tint ret;\n\n\t\tret = bio_integrity_clone(b, bio, gfp_mask);\n\n\t\tif (ret < 0) {\n\t\t\tbio_put(b);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn b;\n}\nEXPORT_SYMBOL(bio_clone_fast);\n\n/**\n * \tbio_clone_bioset - clone a bio\n * \t@bio_src: bio to clone\n *\t@gfp_mask: allocation priority\n *\t@bs: bio_set to allocate from\n *\n *\tClone bio. Caller will own the returned bio, but not the actual data it\n *\tpoints to. Reference count of returned bio will be one.\n */\nstruct bio *bio_clone_bioset(struct bio *bio_src, gfp_t gfp_mask,\n\t\t\t     struct bio_set *bs)\n{\n\tstruct bvec_iter iter;\n\tstruct bio_vec bv;\n\tstruct bio *bio;\n\n\t/*\n\t * Pre immutable biovecs, __bio_clone() used to just do a memcpy from\n\t * bio_src->bi_io_vec to bio->bi_io_vec.\n\t *\n\t * We can't do that anymore, because:\n\t *\n\t *  - The point of cloning the biovec is to produce a bio with a biovec\n\t *    the caller can modify: bi_idx and bi_bvec_done should be 0.\n\t *\n\t *  - The original bio could've had more than BIO_MAX_PAGES biovecs; if\n\t *    we tried to clone the whole thing bio_alloc_bioset() would fail.\n\t *    But the clone should succeed as long as the number of biovecs we\n\t *    actually need to allocate is fewer than BIO_MAX_PAGES.\n\t *\n\t *  - Lastly, bi_vcnt should not be looked at or relied upon by code\n\t *    that does not own the bio - reason being drivers don't use it for\n\t *    iterating over the biovec anymore, so expecting it to be kept up\n\t *    to date (i.e. for clones that share the parent biovec) is just\n\t *    asking for trouble and would force extra work on\n\t *    __bio_clone_fast() anyways.\n\t */\n\n\tbio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);\n\tif (!bio)\n\t\treturn NULL;\n\tbio->bi_disk\t\t= bio_src->bi_disk;\n\tbio->bi_opf\t\t= bio_src->bi_opf;\n\tbio->bi_write_hint\t= bio_src->bi_write_hint;\n\tbio->bi_iter.bi_sector\t= bio_src->bi_iter.bi_sector;\n\tbio->bi_iter.bi_size\t= bio_src->bi_iter.bi_size;\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tbreak;\n\tcase REQ_OP_WRITE_SAME:\n\t\tbio->bi_io_vec[bio->bi_vcnt++] = bio_src->bi_io_vec[0];\n\t\tbreak;\n\tdefault:\n\t\tbio_for_each_segment(bv, bio_src, iter)\n\t\t\tbio->bi_io_vec[bio->bi_vcnt++] = bv;\n\t\tbreak;\n\t}\n\n\tif (bio_integrity(bio_src)) {\n\t\tint ret;\n\n\t\tret = bio_integrity_clone(bio, bio_src, gfp_mask);\n\t\tif (ret < 0) {\n\t\t\tbio_put(bio);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tbio_clone_blkcg_association(bio, bio_src);\n\n\treturn bio;\n}\nEXPORT_SYMBOL(bio_clone_bioset);\n\n/**\n *\tbio_add_pc_page\t-\tattempt to add page to bio\n *\t@q: the target queue\n *\t@bio: destination bio\n *\t@page: page to add\n *\t@len: vec entry length\n *\t@offset: vec entry offset\n *\n *\tAttempt to add a page to the bio_vec maplist. This can fail for a\n *\tnumber of reasons, such as the bio being full or target block device\n *\tlimitations. The target block device must allow bio's up to PAGE_SIZE,\n *\tso it is always possible to add a single page to an empty bio.\n *\n *\tThis should only be used by REQ_PC bios.\n */\nint bio_add_pc_page(struct request_queue *q, struct bio *bio, struct page\n\t\t    *page, unsigned int len, unsigned int offset)\n{\n\tint retried_segments = 0;\n\tstruct bio_vec *bvec;\n\n\t/*\n\t * cloned bio must not modify vec list\n\t */\n\tif (unlikely(bio_flagged(bio, BIO_CLONED)))\n\t\treturn 0;\n\n\tif (((bio->bi_iter.bi_size + len) >> 9) > queue_max_hw_sectors(q))\n\t\treturn 0;\n\n\t/*\n\t * For filesystems with a blocksize smaller than the pagesize\n\t * we will often be called with the same page as last time and\n\t * a consecutive offset.  Optimize this special case.\n\t */\n\tif (bio->bi_vcnt > 0) {\n\t\tstruct bio_vec *prev = &bio->bi_io_vec[bio->bi_vcnt - 1];\n\n\t\tif (page == prev->bv_page &&\n\t\t    offset == prev->bv_offset + prev->bv_len) {\n\t\t\tprev->bv_len += len;\n\t\t\tbio->bi_iter.bi_size += len;\n\t\t\tgoto done;\n\t\t}\n\n\t\t/*\n\t\t * If the queue doesn't support SG gaps and adding this\n\t\t * offset would create a gap, disallow it.\n\t\t */\n\t\tif (bvec_gap_to_prev(q, prev, offset))\n\t\t\treturn 0;\n\t}\n\n\tif (bio->bi_vcnt >= bio->bi_max_vecs)\n\t\treturn 0;\n\n\t/*\n\t * setup the new entry, we might clear it again later if we\n\t * cannot add the page\n\t */\n\tbvec = &bio->bi_io_vec[bio->bi_vcnt];\n\tbvec->bv_page = page;\n\tbvec->bv_len = len;\n\tbvec->bv_offset = offset;\n\tbio->bi_vcnt++;\n\tbio->bi_phys_segments++;\n\tbio->bi_iter.bi_size += len;\n\n\t/*\n\t * Perform a recount if the number of segments is greater\n\t * than queue_max_segments(q).\n\t */\n\n\twhile (bio->bi_phys_segments > queue_max_segments(q)) {\n\n\t\tif (retried_segments)\n\t\t\tgoto failed;\n\n\t\tretried_segments = 1;\n\t\tblk_recount_segments(q, bio);\n\t}\n\n\t/* If we may be able to merge these biovecs, force a recount */\n\tif (bio->bi_vcnt > 1 && (BIOVEC_PHYS_MERGEABLE(bvec-1, bvec)))\n\t\tbio_clear_flag(bio, BIO_SEG_VALID);\n\n done:\n\treturn len;\n\n failed:\n\tbvec->bv_page = NULL;\n\tbvec->bv_len = 0;\n\tbvec->bv_offset = 0;\n\tbio->bi_vcnt--;\n\tbio->bi_iter.bi_size -= len;\n\tblk_recount_segments(q, bio);\n\treturn 0;\n}\nEXPORT_SYMBOL(bio_add_pc_page);\n\n/**\n *\tbio_add_page\t-\tattempt to add page to bio\n *\t@bio: destination bio\n *\t@page: page to add\n *\t@len: vec entry length\n *\t@offset: vec entry offset\n *\n *\tAttempt to add a page to the bio_vec maplist. This will only fail\n *\tif either bio->bi_vcnt == bio->bi_max_vecs or it's a cloned bio.\n */\nint bio_add_page(struct bio *bio, struct page *page,\n\t\t unsigned int len, unsigned int offset)\n{\n\tstruct bio_vec *bv;\n\n\t/*\n\t * cloned bio must not modify vec list\n\t */\n\tif (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))\n\t\treturn 0;\n\n\t/*\n\t * For filesystems with a blocksize smaller than the pagesize\n\t * we will often be called with the same page as last time and\n\t * a consecutive offset.  Optimize this special case.\n\t */\n\tif (bio->bi_vcnt > 0) {\n\t\tbv = &bio->bi_io_vec[bio->bi_vcnt - 1];\n\n\t\tif (page == bv->bv_page &&\n\t\t    offset == bv->bv_offset + bv->bv_len) {\n\t\t\tbv->bv_len += len;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (bio->bi_vcnt >= bio->bi_max_vecs)\n\t\treturn 0;\n\n\tbv\t\t= &bio->bi_io_vec[bio->bi_vcnt];\n\tbv->bv_page\t= page;\n\tbv->bv_len\t= len;\n\tbv->bv_offset\t= offset;\n\n\tbio->bi_vcnt++;\ndone:\n\tbio->bi_iter.bi_size += len;\n\treturn len;\n}\nEXPORT_SYMBOL(bio_add_page);\n\n/**\n * bio_iov_iter_get_pages - pin user or kernel pages and add them to a bio\n * @bio: bio to add pages to\n * @iter: iov iterator describing the region to be mapped\n *\n * Pins as many pages from *iter and appends them to @bio's bvec array. The\n * pages will have to be released using put_page() when done.\n */\nint bio_iov_iter_get_pages(struct bio *bio, struct iov_iter *iter)\n{\n\tunsigned short nr_pages = bio->bi_max_vecs - bio->bi_vcnt;\n\tstruct bio_vec *bv = bio->bi_io_vec + bio->bi_vcnt;\n\tstruct page **pages = (struct page **)bv;\n\tsize_t offset, diff;\n\tssize_t size;\n\n\tsize = iov_iter_get_pages(iter, pages, LONG_MAX, nr_pages, &offset);\n\tif (unlikely(size <= 0))\n\t\treturn size ? size : -EFAULT;\n\tnr_pages = (size + offset + PAGE_SIZE - 1) / PAGE_SIZE;\n\n\t/*\n\t * Deep magic below:  We need to walk the pinned pages backwards\n\t * because we are abusing the space allocated for the bio_vecs\n\t * for the page array.  Because the bio_vecs are larger than the\n\t * page pointers by definition this will always work.  But it also\n\t * means we can't use bio_add_page, so any changes to it's semantics\n\t * need to be reflected here as well.\n\t */\n\tbio->bi_iter.bi_size += size;\n\tbio->bi_vcnt += nr_pages;\n\n\tdiff = (nr_pages * PAGE_SIZE - offset) - size;\n\twhile (nr_pages--) {\n\t\tbv[nr_pages].bv_page = pages[nr_pages];\n\t\tbv[nr_pages].bv_len = PAGE_SIZE;\n\t\tbv[nr_pages].bv_offset = 0;\n\t}\n\n\tbv[0].bv_offset += offset;\n\tbv[0].bv_len -= offset;\n\tif (diff)\n\t\tbv[bio->bi_vcnt - 1].bv_len -= diff;\n\n\tiov_iter_advance(iter, size);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bio_iov_iter_get_pages);\n\nstruct submit_bio_ret {\n\tstruct completion event;\n\tint error;\n};\n\nstatic void submit_bio_wait_endio(struct bio *bio)\n{\n\tstruct submit_bio_ret *ret = bio->bi_private;\n\n\tret->error = blk_status_to_errno(bio->bi_status);\n\tcomplete(&ret->event);\n}\n\n/**\n * submit_bio_wait - submit a bio, and wait until it completes\n * @bio: The &struct bio which describes the I/O\n *\n * Simple wrapper around submit_bio(). Returns 0 on success, or the error from\n * bio_endio() on failure.\n *\n * WARNING: Unlike to how submit_bio() is usually used, this function does not\n * result in bio reference to be consumed. The caller must drop the reference\n * on his own.\n */\nint submit_bio_wait(struct bio *bio)\n{\n\tstruct submit_bio_ret ret;\n\n\tinit_completion(&ret.event);\n\tbio->bi_private = &ret;\n\tbio->bi_end_io = submit_bio_wait_endio;\n\tbio->bi_opf |= REQ_SYNC;\n\tsubmit_bio(bio);\n\twait_for_completion_io(&ret.event);\n\n\treturn ret.error;\n}\nEXPORT_SYMBOL(submit_bio_wait);\n\n/**\n * bio_advance - increment/complete a bio by some number of bytes\n * @bio:\tbio to advance\n * @bytes:\tnumber of bytes to complete\n *\n * This updates bi_sector, bi_size and bi_idx; if the number of bytes to\n * complete doesn't align with a bvec boundary, then bv_len and bv_offset will\n * be updated on the last bvec as well.\n *\n * @bio will then represent the remaining, uncompleted portion of the io.\n */\nvoid bio_advance(struct bio *bio, unsigned bytes)\n{\n\tif (bio_integrity(bio))\n\t\tbio_integrity_advance(bio, bytes);\n\n\tbio_advance_iter(bio, &bio->bi_iter, bytes);\n}\nEXPORT_SYMBOL(bio_advance);\n\n/**\n * bio_alloc_pages - allocates a single page for each bvec in a bio\n * @bio: bio to allocate pages for\n * @gfp_mask: flags for allocation\n *\n * Allocates pages up to @bio->bi_vcnt.\n *\n * Returns 0 on success, -ENOMEM on failure. On failure, any allocated pages are\n * freed.\n */\nint bio_alloc_pages(struct bio *bio, gfp_t gfp_mask)\n{\n\tint i;\n\tstruct bio_vec *bv;\n\n\tbio_for_each_segment_all(bv, bio, i) {\n\t\tbv->bv_page = alloc_page(gfp_mask);\n\t\tif (!bv->bv_page) {\n\t\t\twhile (--bv >= bio->bi_io_vec)\n\t\t\t\t__free_page(bv->bv_page);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(bio_alloc_pages);\n\n/**\n * bio_copy_data - copy contents of data buffers from one chain of bios to\n * another\n * @src: source bio list\n * @dst: destination bio list\n *\n * If @src and @dst are single bios, bi_next must be NULL - otherwise, treats\n * @src and @dst as linked lists of bios.\n *\n * Stops when it reaches the end of either @src or @dst - that is, copies\n * min(src->bi_size, dst->bi_size) bytes (or the equivalent for lists of bios).\n */\nvoid bio_copy_data(struct bio *dst, struct bio *src)\n{\n\tstruct bvec_iter src_iter, dst_iter;\n\tstruct bio_vec src_bv, dst_bv;\n\tvoid *src_p, *dst_p;\n\tunsigned bytes;\n\n\tsrc_iter = src->bi_iter;\n\tdst_iter = dst->bi_iter;\n\n\twhile (1) {\n\t\tif (!src_iter.bi_size) {\n\t\t\tsrc = src->bi_next;\n\t\t\tif (!src)\n\t\t\t\tbreak;\n\n\t\t\tsrc_iter = src->bi_iter;\n\t\t}\n\n\t\tif (!dst_iter.bi_size) {\n\t\t\tdst = dst->bi_next;\n\t\t\tif (!dst)\n\t\t\t\tbreak;\n\n\t\t\tdst_iter = dst->bi_iter;\n\t\t}\n\n\t\tsrc_bv = bio_iter_iovec(src, src_iter);\n\t\tdst_bv = bio_iter_iovec(dst, dst_iter);\n\n\t\tbytes = min(src_bv.bv_len, dst_bv.bv_len);\n\n\t\tsrc_p = kmap_atomic(src_bv.bv_page);\n\t\tdst_p = kmap_atomic(dst_bv.bv_page);\n\n\t\tmemcpy(dst_p + dst_bv.bv_offset,\n\t\t       src_p + src_bv.bv_offset,\n\t\t       bytes);\n\n\t\tkunmap_atomic(dst_p);\n\t\tkunmap_atomic(src_p);\n\n\t\tbio_advance_iter(src, &src_iter, bytes);\n\t\tbio_advance_iter(dst, &dst_iter, bytes);\n\t}\n}\nEXPORT_SYMBOL(bio_copy_data);\n\nstruct bio_map_data {\n\tint is_our_pages;\n\tstruct iov_iter iter;\n\tstruct iovec iov[];\n};\n\nstatic struct bio_map_data *bio_alloc_map_data(unsigned int iov_count,\n\t\t\t\t\t       gfp_t gfp_mask)\n{\n\tif (iov_count > UIO_MAXIOV)\n\t\treturn NULL;\n\n\treturn kmalloc(sizeof(struct bio_map_data) +\n\t\t       sizeof(struct iovec) * iov_count, gfp_mask);\n}\n\n/**\n * bio_copy_from_iter - copy all pages from iov_iter to bio\n * @bio: The &struct bio which describes the I/O as destination\n * @iter: iov_iter as source\n *\n * Copy all pages from iov_iter to bio.\n * Returns 0 on success, or error on failure.\n */\nstatic int bio_copy_from_iter(struct bio *bio, struct iov_iter iter)\n{\n\tint i;\n\tstruct bio_vec *bvec;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tssize_t ret;\n\n\t\tret = copy_page_from_iter(bvec->bv_page,\n\t\t\t\t\t  bvec->bv_offset,\n\t\t\t\t\t  bvec->bv_len,\n\t\t\t\t\t  &iter);\n\n\t\tif (!iov_iter_count(&iter))\n\t\t\tbreak;\n\n\t\tif (ret < bvec->bv_len)\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n/**\n * bio_copy_to_iter - copy all pages from bio to iov_iter\n * @bio: The &struct bio which describes the I/O as source\n * @iter: iov_iter as destination\n *\n * Copy all pages from bio to iov_iter.\n * Returns 0 on success, or error on failure.\n */\nstatic int bio_copy_to_iter(struct bio *bio, struct iov_iter iter)\n{\n\tint i;\n\tstruct bio_vec *bvec;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tssize_t ret;\n\n\t\tret = copy_page_to_iter(bvec->bv_page,\n\t\t\t\t\tbvec->bv_offset,\n\t\t\t\t\tbvec->bv_len,\n\t\t\t\t\t&iter);\n\n\t\tif (!iov_iter_count(&iter))\n\t\t\tbreak;\n\n\t\tif (ret < bvec->bv_len)\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nvoid bio_free_pages(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i)\n\t\t__free_page(bvec->bv_page);\n}\nEXPORT_SYMBOL(bio_free_pages);\n\n/**\n *\tbio_uncopy_user\t-\tfinish previously mapped bio\n *\t@bio: bio being terminated\n *\n *\tFree pages allocated from bio_copy_user_iov() and write back data\n *\tto user space in case of a read.\n */\nint bio_uncopy_user(struct bio *bio)\n{\n\tstruct bio_map_data *bmd = bio->bi_private;\n\tint ret = 0;\n\n\tif (!bio_flagged(bio, BIO_NULL_MAPPED)) {\n\t\t/*\n\t\t * if we're in a workqueue, the request is orphaned, so\n\t\t * don't copy into a random user address space, just free\n\t\t * and return -EINTR so user space doesn't expect any data.\n\t\t */\n\t\tif (!current->mm)\n\t\t\tret = -EINTR;\n\t\telse if (bio_data_dir(bio) == READ)\n\t\t\tret = bio_copy_to_iter(bio, bmd->iter);\n\t\tif (bmd->is_our_pages)\n\t\t\tbio_free_pages(bio);\n\t}\n\tkfree(bmd);\n\tbio_put(bio);\n\treturn ret;\n}\n\n/**\n *\tbio_copy_user_iov\t-\tcopy user data to bio\n *\t@q:\t\tdestination block queue\n *\t@map_data:\tpointer to the rq_map_data holding pages (if necessary)\n *\t@iter:\t\tiovec iterator\n *\t@gfp_mask:\tmemory allocation flags\n *\n *\tPrepares and returns a bio for indirect user io, bouncing data\n *\tto/from kernel pages as necessary. Must be paired with\n *\tcall bio_uncopy_user() on io completion.\n */\nstruct bio *bio_copy_user_iov(struct request_queue *q,\n\t\t\t      struct rq_map_data *map_data,\n\t\t\t      const struct iov_iter *iter,\n\t\t\t      gfp_t gfp_mask)\n{\n\tstruct bio_map_data *bmd;\n\tstruct page *page;\n\tstruct bio *bio;\n\tint i, ret;\n\tint nr_pages = 0;\n\tunsigned int len = iter->count;\n\tunsigned int offset = map_data ? offset_in_page(map_data->offset) : 0;\n\n\tfor (i = 0; i < iter->nr_segs; i++) {\n\t\tunsigned long uaddr;\n\t\tunsigned long end;\n\t\tunsigned long start;\n\n\t\tuaddr = (unsigned long) iter->iov[i].iov_base;\n\t\tend = (uaddr + iter->iov[i].iov_len + PAGE_SIZE - 1)\n\t\t\t>> PAGE_SHIFT;\n\t\tstart = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t}\n\n\tif (offset)\n\t\tnr_pages++;\n\n\tbmd = bio_alloc_map_data(iter->nr_segs, gfp_mask);\n\tif (!bmd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * We need to do a deep copy of the iov_iter including the iovecs.\n\t * The caller provided iov might point to an on-stack or otherwise\n\t * shortlived one.\n\t */\n\tbmd->is_our_pages = map_data ? 0 : 1;\n\tmemcpy(bmd->iov, iter->iov, sizeof(struct iovec) * iter->nr_segs);\n\tiov_iter_init(&bmd->iter, iter->type, bmd->iov,\n\t\t\titer->nr_segs, iter->count);\n\n\tret = -ENOMEM;\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\tgoto out_bmd;\n\n\tret = 0;\n\n\tif (map_data) {\n\t\tnr_pages = 1 << map_data->page_order;\n\t\ti = map_data->offset / PAGE_SIZE;\n\t}\n\twhile (len) {\n\t\tunsigned int bytes = PAGE_SIZE;\n\n\t\tbytes -= offset;\n\n\t\tif (bytes > len)\n\t\t\tbytes = len;\n\n\t\tif (map_data) {\n\t\t\tif (i == map_data->nr_entries * nr_pages) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tpage = map_data->pages[i / nr_pages];\n\t\t\tpage += (i % nr_pages);\n\n\t\t\ti++;\n\t\t} else {\n\t\t\tpage = alloc_page(q->bounce_gfp | gfp_mask);\n\t\t\tif (!page) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (bio_add_pc_page(q, bio, page, bytes, offset) < bytes)\n\t\t\tbreak;\n\n\t\tlen -= bytes;\n\t\toffset = 0;\n\t}\n\n\tif (ret)\n\t\tgoto cleanup;\n\n\t/*\n\t * success\n\t */\n\tif (((iter->type & WRITE) && (!map_data || !map_data->null_mapped)) ||\n\t    (map_data && map_data->from_user)) {\n\t\tret = bio_copy_from_iter(bio, *iter);\n\t\tif (ret)\n\t\t\tgoto cleanup;\n\t}\n\n\tbio->bi_private = bmd;\n\treturn bio;\ncleanup:\n\tif (!map_data)\n\t\tbio_free_pages(bio);\n\tbio_put(bio);\nout_bmd:\n\tkfree(bmd);\n\treturn ERR_PTR(ret);\n}\n\n/**\n *\tbio_map_user_iov - map user iovec into bio\n *\t@q:\t\tthe struct request_queue for the bio\n *\t@iter:\t\tiovec iterator\n *\t@gfp_mask:\tmemory allocation flags\n *\n *\tMap the user space address into a bio suitable for io to a block\n *\tdevice. Returns an error pointer in case of error.\n */\nstruct bio *bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask)\n{\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\tstruct bio_vec *bvec;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (unlikely(ret < local_nr_pages)) {\n\t\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\t\tif (!pages[j])\n\t\t\t\t\tbreak;\n\t\t\t\tput_page(pages[j]);\n\t\t\t}\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * check if vector was merged with previous\n\t\t\t * drop page reference if needed\n\t\t\t */\n\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)\n\t\t\t\tput_page(pages[j]);\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tbio_for_each_segment_all(bvec, bio, j) {\n\t\tput_page(bvec->bv_page);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}\n\nstatic void __bio_unmap_user(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n\t/*\n\t * make sure we dirty pages we wrote to\n\t */\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tif (bio_data_dir(bio) == READ)\n\t\t\tset_page_dirty_lock(bvec->bv_page);\n\n\t\tput_page(bvec->bv_page);\n\t}\n\n\tbio_put(bio);\n}\n\n/**\n *\tbio_unmap_user\t-\tunmap a bio\n *\t@bio:\t\tthe bio being unmapped\n *\n *\tUnmap a bio previously mapped by bio_map_user_iov(). Must be called from\n *\tprocess context.\n *\n *\tbio_unmap_user() may sleep.\n */\nvoid bio_unmap_user(struct bio *bio)\n{\n\t__bio_unmap_user(bio);\n\tbio_put(bio);\n}\n\nstatic void bio_map_kern_endio(struct bio *bio)\n{\n\tbio_put(bio);\n}\n\n/**\n *\tbio_map_kern\t-\tmap kernel address into bio\n *\t@q: the struct request_queue for the bio\n *\t@data: pointer to buffer to map\n *\t@len: length in bytes\n *\t@gfp_mask: allocation flags for bio allocation\n *\n *\tMap the kernel address into a bio suitable for io to a block\n *\tdevice. Returns an error pointer in case of error.\n */\nstruct bio *bio_map_kern(struct request_queue *q, void *data, unsigned int len,\n\t\t\t gfp_t gfp_mask)\n{\n\tunsigned long kaddr = (unsigned long)data;\n\tunsigned long end = (kaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tunsigned long start = kaddr >> PAGE_SHIFT;\n\tconst int nr_pages = end - start;\n\tint offset, i;\n\tstruct bio *bio;\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\toffset = offset_in_page(kaddr);\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned int bytes = PAGE_SIZE - offset;\n\n\t\tif (len <= 0)\n\t\t\tbreak;\n\n\t\tif (bytes > len)\n\t\t\tbytes = len;\n\n\t\tif (bio_add_pc_page(q, bio, virt_to_page(data), bytes,\n\t\t\t\t    offset) < bytes) {\n\t\t\t/* we don't support partial mappings */\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\tdata += bytes;\n\t\tlen -= bytes;\n\t\toffset = 0;\n\t}\n\n\tbio->bi_end_io = bio_map_kern_endio;\n\treturn bio;\n}\nEXPORT_SYMBOL(bio_map_kern);\n\nstatic void bio_copy_kern_endio(struct bio *bio)\n{\n\tbio_free_pages(bio);\n\tbio_put(bio);\n}\n\nstatic void bio_copy_kern_endio_read(struct bio *bio)\n{\n\tchar *p = bio->bi_private;\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tmemcpy(p, page_address(bvec->bv_page), bvec->bv_len);\n\t\tp += bvec->bv_len;\n\t}\n\n\tbio_copy_kern_endio(bio);\n}\n\n/**\n *\tbio_copy_kern\t-\tcopy kernel address into bio\n *\t@q: the struct request_queue for the bio\n *\t@data: pointer to buffer to copy\n *\t@len: length in bytes\n *\t@gfp_mask: allocation flags for bio and page allocation\n *\t@reading: data direction is READ\n *\n *\tcopy the kernel address into a bio suitable for io to a block\n *\tdevice. Returns an error pointer in case of error.\n */\nstruct bio *bio_copy_kern(struct request_queue *q, void *data, unsigned int len,\n\t\t\t  gfp_t gfp_mask, int reading)\n{\n\tunsigned long kaddr = (unsigned long)data;\n\tunsigned long end = (kaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tunsigned long start = kaddr >> PAGE_SHIFT;\n\tstruct bio *bio;\n\tvoid *p = data;\n\tint nr_pages = 0;\n\n\t/*\n\t * Overflow, abort\n\t */\n\tif (end < start)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tnr_pages = end - start;\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\twhile (len) {\n\t\tstruct page *page;\n\t\tunsigned int bytes = PAGE_SIZE;\n\n\t\tif (bytes > len)\n\t\t\tbytes = len;\n\n\t\tpage = alloc_page(q->bounce_gfp | gfp_mask);\n\t\tif (!page)\n\t\t\tgoto cleanup;\n\n\t\tif (!reading)\n\t\t\tmemcpy(page_address(page), p, bytes);\n\n\t\tif (bio_add_pc_page(q, bio, page, bytes, 0) < bytes)\n\t\t\tbreak;\n\n\t\tlen -= bytes;\n\t\tp += bytes;\n\t}\n\n\tif (reading) {\n\t\tbio->bi_end_io = bio_copy_kern_endio_read;\n\t\tbio->bi_private = data;\n\t} else {\n\t\tbio->bi_end_io = bio_copy_kern_endio;\n\t}\n\n\treturn bio;\n\ncleanup:\n\tbio_free_pages(bio);\n\tbio_put(bio);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/*\n * bio_set_pages_dirty() and bio_check_pages_dirty() are support functions\n * for performing direct-IO in BIOs.\n *\n * The problem is that we cannot run set_page_dirty() from interrupt context\n * because the required locks are not interrupt-safe.  So what we can do is to\n * mark the pages dirty _before_ performing IO.  And in interrupt context,\n * check that the pages are still dirty.   If so, fine.  If not, redirty them\n * in process context.\n *\n * We special-case compound pages here: normally this means reads into hugetlb\n * pages.  The logic in here doesn't really work right for compound pages\n * because the VM does not uniformly chase down the head page in all cases.\n * But dirtiness of compound pages is pretty meaningless anyway: the VM doesn't\n * handle them at all.  So we skip compound pages here at an early stage.\n *\n * Note that this code is very hard to test under normal circumstances because\n * direct-io pins the pages with get_user_pages().  This makes\n * is_page_cache_freeable return false, and the VM will not clean the pages.\n * But other code (eg, flusher threads) could clean the pages if they are mapped\n * pagecache.\n *\n * Simply disabling the call to bio_set_pages_dirty() is a good way to test the\n * deferred bio dirtying paths.\n */\n\n/*\n * bio_set_pages_dirty() will mark all the bio's pages as dirty.\n */\nvoid bio_set_pages_dirty(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tif (page && !PageCompound(page))\n\t\t\tset_page_dirty_lock(page);\n\t}\n}\n\nstatic void bio_release_pages(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tif (page)\n\t\t\tput_page(page);\n\t}\n}\n\n/*\n * bio_check_pages_dirty() will check that all the BIO's pages are still dirty.\n * If they are, then fine.  If, however, some pages are clean then they must\n * have been written out during the direct-IO read.  So we take another ref on\n * the BIO and the offending pages and re-dirty the pages in process context.\n *\n * It is expected that bio_check_pages_dirty() will wholly own the BIO from\n * here on.  It will run one put_page() against each page and will run one\n * bio_put() against the BIO.\n */\n\nstatic void bio_dirty_fn(struct work_struct *work);\n\nstatic DECLARE_WORK(bio_dirty_work, bio_dirty_fn);\nstatic DEFINE_SPINLOCK(bio_dirty_lock);\nstatic struct bio *bio_dirty_list;\n\n/*\n * This runs in process context\n */\nstatic void bio_dirty_fn(struct work_struct *work)\n{\n\tunsigned long flags;\n\tstruct bio *bio;\n\n\tspin_lock_irqsave(&bio_dirty_lock, flags);\n\tbio = bio_dirty_list;\n\tbio_dirty_list = NULL;\n\tspin_unlock_irqrestore(&bio_dirty_lock, flags);\n\n\twhile (bio) {\n\t\tstruct bio *next = bio->bi_private;\n\n\t\tbio_set_pages_dirty(bio);\n\t\tbio_release_pages(bio);\n\t\tbio_put(bio);\n\t\tbio = next;\n\t}\n}\n\nvoid bio_check_pages_dirty(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint nr_clean_pages = 0;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tif (PageDirty(page) || PageCompound(page)) {\n\t\t\tput_page(page);\n\t\t\tbvec->bv_page = NULL;\n\t\t} else {\n\t\t\tnr_clean_pages++;\n\t\t}\n\t}\n\n\tif (nr_clean_pages) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&bio_dirty_lock, flags);\n\t\tbio->bi_private = bio_dirty_list;\n\t\tbio_dirty_list = bio;\n\t\tspin_unlock_irqrestore(&bio_dirty_lock, flags);\n\t\tschedule_work(&bio_dirty_work);\n\t} else {\n\t\tbio_put(bio);\n\t}\n}\n\nvoid generic_start_io_acct(struct request_queue *q, int rw,\n\t\t\t   unsigned long sectors, struct hd_struct *part)\n{\n\tint cpu = part_stat_lock();\n\n\tpart_round_stats(q, cpu, part);\n\tpart_stat_inc(cpu, part, ios[rw]);\n\tpart_stat_add(cpu, part, sectors[rw], sectors);\n\tpart_inc_in_flight(q, part, rw);\n\n\tpart_stat_unlock();\n}\nEXPORT_SYMBOL(generic_start_io_acct);\n\nvoid generic_end_io_acct(struct request_queue *q, int rw,\n\t\t\t struct hd_struct *part, unsigned long start_time)\n{\n\tunsigned long duration = jiffies - start_time;\n\tint cpu = part_stat_lock();\n\n\tpart_stat_add(cpu, part, ticks[rw], duration);\n\tpart_round_stats(q, cpu, part);\n\tpart_dec_in_flight(q, part, rw);\n\n\tpart_stat_unlock();\n}\nEXPORT_SYMBOL(generic_end_io_acct);\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\nvoid bio_flush_dcache_pages(struct bio *bi)\n{\n\tstruct bio_vec bvec;\n\tstruct bvec_iter iter;\n\n\tbio_for_each_segment(bvec, bi, iter)\n\t\tflush_dcache_page(bvec.bv_page);\n}\nEXPORT_SYMBOL(bio_flush_dcache_pages);\n#endif\n\nstatic inline bool bio_remaining_done(struct bio *bio)\n{\n\t/*\n\t * If we're not chaining, then ->__bi_remaining is always 1 and\n\t * we always end io on the first invocation.\n\t */\n\tif (!bio_flagged(bio, BIO_CHAIN))\n\t\treturn true;\n\n\tBUG_ON(atomic_read(&bio->__bi_remaining) <= 0);\n\n\tif (atomic_dec_and_test(&bio->__bi_remaining)) {\n\t\tbio_clear_flag(bio, BIO_CHAIN);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/**\n * bio_endio - end I/O on a bio\n * @bio:\tbio\n *\n * Description:\n *   bio_endio() will end I/O on the whole bio. bio_endio() is the preferred\n *   way to end I/O on a bio. No one should call bi_end_io() directly on a\n *   bio unless they own it and thus know that it has an end_io function.\n *\n *   bio_endio() can be called several times on a bio that has been chained\n *   using bio_chain().  The ->bi_end_io() function will only be called the\n *   last time.  At this point the BLK_TA_COMPLETE tracing event will be\n *   generated if BIO_TRACE_COMPLETION is set.\n **/\nvoid bio_endio(struct bio *bio)\n{\nagain:\n\tif (!bio_remaining_done(bio))\n\t\treturn;\n\tif (!bio_integrity_endio(bio))\n\t\treturn;\n\n\t/*\n\t * Need to have a real endio function for chained bios, otherwise\n\t * various corner cases will break (like stacking block devices that\n\t * save/restore bi_end_io) - however, we want to avoid unbounded\n\t * recursion and blowing the stack. Tail call optimization would\n\t * handle this, but compiling with frame pointers also disables\n\t * gcc's sibling call optimization.\n\t */\n\tif (bio->bi_end_io == bio_chain_endio) {\n\t\tbio = __bio_chain_endio(bio);\n\t\tgoto again;\n\t}\n\n\tif (bio->bi_disk && bio_flagged(bio, BIO_TRACE_COMPLETION)) {\n\t\ttrace_block_bio_complete(bio->bi_disk->queue, bio,\n\t\t\t\t\t blk_status_to_errno(bio->bi_status));\n\t\tbio_clear_flag(bio, BIO_TRACE_COMPLETION);\n\t}\n\n\tblk_throtl_bio_endio(bio);\n\t/* release cgroup info */\n\tbio_uninit(bio);\n\tif (bio->bi_end_io)\n\t\tbio->bi_end_io(bio);\n}\nEXPORT_SYMBOL(bio_endio);\n\n/**\n * bio_split - split a bio\n * @bio:\tbio to split\n * @sectors:\tnumber of sectors to split from the front of @bio\n * @gfp:\tgfp mask\n * @bs:\t\tbio set to allocate from\n *\n * Allocates and returns a new bio which represents @sectors from the start of\n * @bio, and updates @bio to represent the remaining sectors.\n *\n * Unless this is a discard request the newly allocated bio will point\n * to @bio's bi_io_vec; it is the caller's responsibility to ensure that\n * @bio is not freed before the split.\n */\nstruct bio *bio_split(struct bio *bio, int sectors,\n\t\t      gfp_t gfp, struct bio_set *bs)\n{\n\tstruct bio *split = NULL;\n\n\tBUG_ON(sectors <= 0);\n\tBUG_ON(sectors >= bio_sectors(bio));\n\n\tsplit = bio_clone_fast(bio, gfp, bs);\n\tif (!split)\n\t\treturn NULL;\n\n\tsplit->bi_iter.bi_size = sectors << 9;\n\n\tif (bio_integrity(split))\n\t\tbio_integrity_trim(split);\n\n\tbio_advance(bio, split->bi_iter.bi_size);\n\n\tif (bio_flagged(bio, BIO_TRACE_COMPLETION))\n\t\tbio_set_flag(bio, BIO_TRACE_COMPLETION);\n\n\treturn split;\n}\nEXPORT_SYMBOL(bio_split);\n\n/**\n * bio_trim - trim a bio\n * @bio:\tbio to trim\n * @offset:\tnumber of sectors to trim from the front of @bio\n * @size:\tsize we want to trim @bio to, in sectors\n */\nvoid bio_trim(struct bio *bio, int offset, int size)\n{\n\t/* 'bio' is a cloned bio which we need to trim to match\n\t * the given offset and size.\n\t */\n\n\tsize <<= 9;\n\tif (offset == 0 && size == bio->bi_iter.bi_size)\n\t\treturn;\n\n\tbio_clear_flag(bio, BIO_SEG_VALID);\n\n\tbio_advance(bio, offset << 9);\n\n\tbio->bi_iter.bi_size = size;\n\n\tif (bio_integrity(bio))\n\t\tbio_integrity_trim(bio);\n\n}\nEXPORT_SYMBOL_GPL(bio_trim);\n\n/*\n * create memory pools for biovec's in a bio_set.\n * use the global biovec slabs created for general use.\n */\nmempool_t *biovec_create_pool(int pool_entries)\n{\n\tstruct biovec_slab *bp = bvec_slabs + BVEC_POOL_MAX;\n\n\treturn mempool_create_slab_pool(pool_entries, bp->slab);\n}\n\nvoid bioset_free(struct bio_set *bs)\n{\n\tif (bs->rescue_workqueue)\n\t\tdestroy_workqueue(bs->rescue_workqueue);\n\n\tif (bs->bio_pool)\n\t\tmempool_destroy(bs->bio_pool);\n\n\tif (bs->bvec_pool)\n\t\tmempool_destroy(bs->bvec_pool);\n\n\tbioset_integrity_free(bs);\n\tbio_put_slab(bs);\n\n\tkfree(bs);\n}\nEXPORT_SYMBOL(bioset_free);\n\n/**\n * bioset_create  - Create a bio_set\n * @pool_size:\tNumber of bio and bio_vecs to cache in the mempool\n * @front_pad:\tNumber of bytes to allocate in front of the returned bio\n * @flags:\tFlags to modify behavior, currently %BIOSET_NEED_BVECS\n *              and %BIOSET_NEED_RESCUER\n *\n * Description:\n *    Set up a bio_set to be used with @bio_alloc_bioset. Allows the caller\n *    to ask for a number of bytes to be allocated in front of the bio.\n *    Front pad allocation is useful for embedding the bio inside\n *    another structure, to avoid allocating extra data to go with the bio.\n *    Note that the bio must be embedded at the END of that structure always,\n *    or things will break badly.\n *    If %BIOSET_NEED_BVECS is set in @flags, a separate pool will be allocated\n *    for allocating iovecs.  This pool is not needed e.g. for bio_clone_fast().\n *    If %BIOSET_NEED_RESCUER is set, a workqueue is created which can be used to\n *    dispatch queued requests when the mempool runs out of space.\n *\n */\nstruct bio_set *bioset_create(unsigned int pool_size,\n\t\t\t      unsigned int front_pad,\n\t\t\t      int flags)\n{\n\tunsigned int back_pad = BIO_INLINE_VECS * sizeof(struct bio_vec);\n\tstruct bio_set *bs;\n\n\tbs = kzalloc(sizeof(*bs), GFP_KERNEL);\n\tif (!bs)\n\t\treturn NULL;\n\n\tbs->front_pad = front_pad;\n\n\tspin_lock_init(&bs->rescue_lock);\n\tbio_list_init(&bs->rescue_list);\n\tINIT_WORK(&bs->rescue_work, bio_alloc_rescue);\n\n\tbs->bio_slab = bio_find_or_create_slab(front_pad + back_pad);\n\tif (!bs->bio_slab) {\n\t\tkfree(bs);\n\t\treturn NULL;\n\t}\n\n\tbs->bio_pool = mempool_create_slab_pool(pool_size, bs->bio_slab);\n\tif (!bs->bio_pool)\n\t\tgoto bad;\n\n\tif (flags & BIOSET_NEED_BVECS) {\n\t\tbs->bvec_pool = biovec_create_pool(pool_size);\n\t\tif (!bs->bvec_pool)\n\t\t\tgoto bad;\n\t}\n\n\tif (!(flags & BIOSET_NEED_RESCUER))\n\t\treturn bs;\n\n\tbs->rescue_workqueue = alloc_workqueue(\"bioset\", WQ_MEM_RECLAIM, 0);\n\tif (!bs->rescue_workqueue)\n\t\tgoto bad;\n\n\treturn bs;\nbad:\n\tbioset_free(bs);\n\treturn NULL;\n}\nEXPORT_SYMBOL(bioset_create);\n\n#ifdef CONFIG_BLK_CGROUP\n\n/**\n * bio_associate_blkcg - associate a bio with the specified blkcg\n * @bio: target bio\n * @blkcg_css: css of the blkcg to associate\n *\n * Associate @bio with the blkcg specified by @blkcg_css.  Block layer will\n * treat @bio as if it were issued by a task which belongs to the blkcg.\n *\n * This function takes an extra reference of @blkcg_css which will be put\n * when @bio is released.  The caller must own @bio and is responsible for\n * synchronizing calls to this function.\n */\nint bio_associate_blkcg(struct bio *bio, struct cgroup_subsys_state *blkcg_css)\n{\n\tif (unlikely(bio->bi_css))\n\t\treturn -EBUSY;\n\tcss_get(blkcg_css);\n\tbio->bi_css = blkcg_css;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bio_associate_blkcg);\n\n/**\n * bio_associate_current - associate a bio with %current\n * @bio: target bio\n *\n * Associate @bio with %current if it hasn't been associated yet.  Block\n * layer will treat @bio as if it were issued by %current no matter which\n * task actually issues it.\n *\n * This function takes an extra reference of @task's io_context and blkcg\n * which will be put when @bio is released.  The caller must own @bio,\n * ensure %current->io_context exists, and is responsible for synchronizing\n * calls to this function.\n */\nint bio_associate_current(struct bio *bio)\n{\n\tstruct io_context *ioc;\n\n\tif (bio->bi_css)\n\t\treturn -EBUSY;\n\n\tioc = current->io_context;\n\tif (!ioc)\n\t\treturn -ENOENT;\n\n\tget_io_context_active(ioc);\n\tbio->bi_ioc = ioc;\n\tbio->bi_css = task_get_css(current, io_cgrp_id);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bio_associate_current);\n\n/**\n * bio_disassociate_task - undo bio_associate_current()\n * @bio: target bio\n */\nvoid bio_disassociate_task(struct bio *bio)\n{\n\tif (bio->bi_ioc) {\n\t\tput_io_context(bio->bi_ioc);\n\t\tbio->bi_ioc = NULL;\n\t}\n\tif (bio->bi_css) {\n\t\tcss_put(bio->bi_css);\n\t\tbio->bi_css = NULL;\n\t}\n}\n\n/**\n * bio_clone_blkcg_association - clone blkcg association from src to dst bio\n * @dst: destination bio\n * @src: source bio\n */\nvoid bio_clone_blkcg_association(struct bio *dst, struct bio *src)\n{\n\tif (src->bi_css)\n\t\tWARN_ON(bio_associate_blkcg(dst, src->bi_css));\n}\nEXPORT_SYMBOL_GPL(bio_clone_blkcg_association);\n#endif /* CONFIG_BLK_CGROUP */\n\nstatic void __init biovec_init_slabs(void)\n{\n\tint i;\n\n\tfor (i = 0; i < BVEC_POOL_NR; i++) {\n\t\tint size;\n\t\tstruct biovec_slab *bvs = bvec_slabs + i;\n\n\t\tif (bvs->nr_vecs <= BIO_INLINE_VECS) {\n\t\t\tbvs->slab = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tsize = bvs->nr_vecs * sizeof(struct bio_vec);\n\t\tbvs->slab = kmem_cache_create(bvs->name, size, 0,\n                                SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\n\t}\n}\n\nstatic int __init init_bio(void)\n{\n\tbio_slab_max = 2;\n\tbio_slab_nr = 0;\n\tbio_slabs = kzalloc(bio_slab_max * sizeof(struct bio_slab), GFP_KERNEL);\n\tif (!bio_slabs)\n\t\tpanic(\"bio: can't allocate bios\\n\");\n\n\tbio_integrity_init();\n\tbiovec_init_slabs();\n\n\tfs_bio_set = bioset_create(BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);\n\tif (!fs_bio_set)\n\t\tpanic(\"bio: can't allocate bios\\n\");\n\n\tif (bioset_integrity_create(fs_bio_set, BIO_POOL_SIZE))\n\t\tpanic(\"bio: can't create integrity pool\\n\");\n\n\treturn 0;\n}\nsubsys_initcall(init_bio);\n"], "filenames": ["block/bio.c"], "buggy_code_start_loc": [1333], "buggy_code_end_loc": [1438], "fixing_code_start_loc": [1334], "fixing_code_end_loc": [1442], "type": "CWE-400", "message": "The bio_map_user_iov and bio_unmap_user functions in block/bio.c in the Linux kernel before 4.13.8 do unbalanced refcounting when a SCSI I/O vector has small consecutive buffers belonging to the same page. The bio_add_pc_page function merges them into one, but the page reference is never dropped. This causes a memory leak and possible system lockup (exploitable against the host OS by a guest OS user, if a SCSI disk is passed through to a virtual machine) due to an out-of-memory condition.", "other": {"cve": {"id": "CVE-2017-12190", "sourceIdentifier": "secalert@redhat.com", "published": "2017-11-22T18:29:00.477", "lastModified": "2023-02-12T23:27:30.660", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The bio_map_user_iov and bio_unmap_user functions in block/bio.c in the Linux kernel before 4.13.8 do unbalanced refcounting when a SCSI I/O vector has small consecutive buffers belonging to the same page. The bio_add_pc_page function merges them into one, but the page reference is never dropped. This causes a memory leak and possible system lockup (exploitable against the host OS by a guest OS user, if a SCSI disk is passed through to a virtual machine) due to an out-of-memory condition."}, {"lang": "es", "value": "Las funciones bio_map_user_iov y bio_unmap_user en block/bio.c en el kernel de Linux en versiones anteriores a la 4.13.8 realizan un refcount no equilibrado cuando un vector SCSI I/O tiene b\u00faferes peque\u00f1os consecutivos que pertenecen a la misma p\u00e1gina. La funci\u00f3n bio_add_pc_page los combina en uno solo, pero la referencia de la p\u00e1gina nunca se anula. Esto provoca una fuga de memoria y un posible bloqueo del sistema (explotable contra el host del sistema operativo por un usuario invitado del sistema operativo, si se pasa un disco SCSI a una m\u00e1quina virtual) debido a una condici\u00f3n de falta de memoria."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.0, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "secalert@redhat.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-772"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.13.7", "matchCriteriaId": "11C6B206-8716-4A16-81BD-F3B8C8ACBE19"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=2b04e8f6bbb196cab4b232af0f8d48ff2c7a8058", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Vendor Advisory"]}, {"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=95d78c28b5a85bacbc29b8dba7c04babb9b0d467", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Vendor Advisory"]}, {"url": "http://seclists.org/oss-sec/2017/q4/52", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Mailing List", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.13.8", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/101911", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:0654", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2018:0676", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2018:1062", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2018:1854", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2019:1170", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2019:1190", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1495089", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/2b04e8f6bbb196cab4b232af0f8d48ff2c7a8058", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/95d78c28b5a85bacbc29b8dba7c04babb9b0d467", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2017/12/msg00004.html", "source": "secalert@redhat.com"}, {"url": "https://support.f5.com/csp/article/K93472064?utm_source=f5support&amp%3Butm_medium=RSS", "source": "secalert@redhat.com"}, {"url": "https://usn.ubuntu.com/3582-1/", "source": "secalert@redhat.com"}, {"url": "https://usn.ubuntu.com/3582-2/", "source": "secalert@redhat.com"}, {"url": "https://usn.ubuntu.com/3583-1/", "source": "secalert@redhat.com"}, {"url": "https://usn.ubuntu.com/3583-2/", "source": "secalert@redhat.com"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/2b04e8f6bbb196cab4b232af0f8d48ff2c7a8058"}}