{"buggy_code": ["# Release 2.7.0\n\n<INSERT SMALL BLURB ABOUT RELEASE FOCUS AREA AND POTENTIAL TOOLCHAIN CHANGES>\n\n## Breaking Changes\n\n* `tf.keras`:\n  * The methods `Model.fit()`, `Model.predict()`, and `Model.evaluate()` will\n    no longer uprank input data of shape `(batch_size,)`\n    to become `(batch_size, 1)`. This enables `Model` subclasses to process\n    scalar data in their `train_step()`/`test_step()`/`predict_step()` methods.\n    Note that this change may break certain subclassed models.\n    You can revert back to the previous behavior by adding upranking yourself\n    in the `train_step()`/`test_step()`/`predict_step()` methods, e.g.\n    `if x.shape.rank == 1: x = tf.expand_dims(x, axis=-1)`.\n    Functional models as well as Sequential models built with an explicit\n    input shape are not affected.\n\n* `tf.lite`:\n  * Rename fields `SignatureDef` table in schema to maximize the parity with\n    TF SavedModel's Signature concept.\n\n* TF Core:\n    *   `tf.Graph.get_name_scope()` now always returns a string, as documented.\n        Previously, when called within `name_scope(\"\")` or `name_scope(None)`\n        contexts, it returned None; now it returns the empty string.\n    *   `tensorflow/core/ir/` contains a new MLIR-based Graph dialect that is\n        isomorphic to GraphDef and will be used to replace GraphDef-based (e.g.,\n        Grappler) optimizations.\n\n## Known Caveats\n\n*<CAVEATS REGARDING THE RELEASE (BUT NOT BREAKING CHANGES).>\n*<ADDING/BUMPING DEPENDENCIES SHOULD GO HERE>\n*<KNOWN LACK OF SUPPORT ON SOME PLATFORM, SHOULD GO HERE>\n\n## Major Features and Improvements\n\n* Improvements to the TensorFlow debugging experience:\n  * Previously, TensorFlow error stack traces involved many internal frames,\n    which could be challenging to read through,\n    while not being actionable for end users.\n    As of TF 2.7, TensorFlow filters internal frames in most errors that it\n    raises, to keep stack traces short, readable, and focused on what's\n    actionable for end users (their own code).\n\n    This behavior can be disabled by calling\n    `tf.debugging.disable_traceback_filtering()`, and can be re-enabled via\n    `tf.debugging.enable_traceback_filtering()`. If you are debugging a\n    TensorFlow-internal issue (e.g. to prepare a TensorFlow PR), make sure\n    to disable traceback filtering. You can check whether this feature is\n    currently enabled by calling\n    `tf.debugging.is_traceback_filtering_enabled()`.\n\n    Note that this feature is only available with Python 3.7 or higher.\n\n  * Improve the informativeness of error messages raised by Keras\n    `Layer.__call__()`, by adding the full list of argument values passed to the\n    layer in every exception.\n\n*   TF1 -> TF2 Migration\n    * Introduced the `tf.compat.v1.keras.utils.track_tf1_style_variables`\n      decorator, which enables using large classes of tf1-style variable_scope,\n      `get_variable`, and `compat.v1.layer`-based components from within TF2\n      models running with TF2 behavior enabled.\n\n*  `tf.data`:\n    *   tf.data service now supports auto-sharding. Users specify the sharding\n        policy with `tf.data.experimental.service.ShardingPolicy` enum. It can\n        be one of OFF (equivalent to today's `\"parallel_epochs\"` mode), DYNAMIC\n        (equivalent to today's `\"distributed_epoch\"` mode), or one of the static\n        sharding policies: FILE, DATA, FILE_OR_DATA, or HINT (corresponding to\n        values of `tf.data.experimental.AutoShardPolicy`).\n\n        Static sharding (auto-sharding) requires the number of tf.data service\n        workers be fixed. Users need to specify the worker addresses in\n        `tensorflow.data.experimental.DispatcherConfig`.\n*  Keras:\n  *  `tf.keras.layers.Conv` now includes a public `convolution_op` method.\n      This method can be used to simplify the implementation of Conv subclasses.\n      There are two primary ways to use this new method.  The first is to use the method directly in your own `call` method:\n      ```\n        class StandardizedConv2D(tf.keras.layers.Conv2D):\n          def call(self, inputs):\n            mean, var = tf.nn.moments(self.kernel, axes=[0, 1, 2], keepdims=True)\n            return self.convolution_op(inputs, (self.kernel - mean) / tf.sqrt(var + 1e-10))\n      ```\n      Alternatively, you can override `convolution_op`:\n      ```\n        class StandardizedConv2D(tf.keras.Layer):\n          def convolution_op(self, inputs, kernel):\n            mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)\n            # Author code uses std + 1e-5\n            return super().convolution_op(inputs, (kernel - mean) / tf.sqrt(var + 1e-10))\n      ```\n\n## Bug Fixes and Other Changes\n\n*<SIMILAR TO ABOVE SECTION, BUT FOR OTHER IMPORTANT CHANGES / BUG FIXES>\n*<IF A CHANGE CLOSES A GITHUB ISSUE, IT SHOULD BE DOCUMENTED HERE>\n*<NOTES SHOULD BE GROUPED PER AREA>\n*   TF Core:\n    * Random number generation (RNG) system\n        *   Added argument `alg` to `tf.random.stateless_*` functions to explicitly select the RNG algorithm.\n        *   Added `tf.nn.experimental.stateless_dropout`, a stateless version of `tf.nn.dropout`.\n        *   `tf.random.Generator` now can be created inside the scope of `tf.distribute.experimental.CentralStorageStrategy`.\n*   `tf.data`:\n    *   Promoting `tf.data.Options.experimental_deterministic` API to\n        `tf.data.Options.deterministic` and deprecating the experimental\n        endpoint.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n<INSERT>, <NAME>, <HERE>, <USING>, <GITHUB>, <HANDLE>\n\n# Release 2.6.0\n\n<INSERT SMALL BLURB ABOUT RELEASE FOCUS AREA AND POTENTIAL TOOLCHAIN CHANGES>\n\n## Breaking Changes\n\n*   `tf.train.experimental.enable_mixed_precision_graph_rewrite` is removed, as\n    the API only works in graph mode and is not customizable. The function is\n    still accessible under\n    `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite`, but it\n    is recommended to use the\n    [Keras mixed precision API](https://www.tensorflow.org/guide/mixed_precision)\n    instead.\n\n*   `tf.lite`:\n\n    *   Remove `experimental.nn.dynamic_rnn`, `experimental.nn.TfLiteRNNCell`\n        and `experimental.nn.TfLiteLSTMCell` since they're no longer supported.\n        It's recommended to just use\n        [keras lstm](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n        instead.\n\n*   Keras been split into a separate PIP package (`keras`), and its code has\n    been moved to the GitHub\n    repository[keras-team/keras](http://github.com/keras-team/keras). The API\n    endpoints for `tf.keras` stay unchanged, but are now backed by the `keras`\n    PIP package. The existing code in tensorflow/python/keras is a staled copy\n    and will be removed in future release (2.7). Please remove any imports to\n    `tensorflow.python.keras` and replace them with public tf.keras API instead.\n\n*   Modular File System Migration\n\n    *   S3 and HDFS file system supports have been migrated to modular file\n        systems and is now available in https://github.com/tensorflow/io. The\n        tensorflow-io python package should be installed for S3 and HDFS support\n        with tensorflow.\n\n*<DOCUMENT BREAKING CHANGES HERE>\n*<THIS SECTION SHOULD CONTAIN API, ABI AND BEHAVIORAL BREAKING CHANGES>\n\n## Known Caveats\n\n*<CAVEATS REGARDING THE RELEASE (BUT NOT BREAKING CHANGES).>\n*<ADDING/BUMPING DEPENDENCIES SHOULD GO HERE>\n*<KNOWN LACK OF SUPPORT ON SOME PLATFORM, SHOULD GO HERE>\n\n* TF Core:\n    * A longstanding bug in `tf.while_loop`, which caused it to execute\n      sequentially, even when `parallel_iterations>1`, has now been fixed.\n      However, the increased parallelism may result in increased memory use.\n      Users who experience unwanted regressions should reset their\n      `while_loop`'s `parallel_iterations` value to 1, which is consistent with\n      prior behavior.\n\n* `tf.keras`:\n  * The `trainable` argument when creating a Keras Layer must now be a boolean\n    (previously there was no validation and `None` values were allowed).\n\n## Major Features and Improvements\n\n*<INSERT MAJOR FEATURE HERE, USING MARKDOWN SYNTAX>\n*<IF RELEASE CONTAINS MULTIPLE FEATURES FROM SAME AREA, GROUP THEM TOGETHER>\n\n* `tf.keras`:\n    *   Keras has been split into a separate PIP package (`keras`),\n        and its code has been moved to the GitHub repository\n        [keras-team/keras](http://github.com/keras-team/keras).\n        The API endpoints for `tf.keras` stay unchanged,\n        but are now backed by the `keras` PIP package. All Keras-related\n        PRs and issues should now be directed to the GitHub repository\n        [keras-team/keras](http://github.com/keras-team/keras).\n    *   `tf.keras.utils.experimental.DatasetCreator` now takes an optional\n        `tf.distribute.InputOptions` for specific options when used with\n        distribution.\n    *   `tf.keras.experimental.SidecarEvaluator` is now available for a program\n        intended to be run on an evaluator task, which is commonly used to\n        supplement a training cluster running with\n        `tf.distribute.experimental.ParameterServerStrategy` (see\n        https://www.tensorflow.org/tutorials/distribute/parameter_server_training).\n        It can also be used with single-worker training or other strategies.\n        See docstring for more info.\n    *   Preprocessing layers moved from experimental to core.\n        *   Import paths moved from\n            `tf.keras.layers.preprocessing.experimental` to `tf.keras.layers`.\n    *   Updates to Preprocessing layers API for consistency and clarity:\n        *   `StringLookup` and `IntegerLookup` default for `mask_token` changed\n            to `None`. This matches the default masking behavior of `Hashing`\n            and `Embedding` layers. To keep existing behavior, pass\n            `mask_token=\"\"` during layer creation.\n        *   Renamed `\"binary\"` output mode to `\"multi_hot\"` for\n            `CategoryEncoding`, `StringLookup`, `IntegerLookup`, and\n            `TextVectorization`. Multi-hot encoding will no longer\n            automatically uprank rank 1 inputs, so these layers can now\n            multi-hot encode unbatched multi-dimensional samples.\n        *   Added a new output mode `\"one_hot\"` for `CategoryEncoding`,\n            `StringLookup`, `IntegerLookup`, which will encode each element in\n            an input batch individually, and automatically append a new output\n            dimension if necessary. Use this mode on rank 1 inputs for the old\n            `\"binary\"` behavior of one-hot encoding a batch of scalars.\n        *   `Normalization` will no longer automatically uprank rank 1 inputs,\n            allowing normalization of unbatched multi-dimensional samples.\n\n* `tf.lite`:\n    *   The recommended Android NDK version for building TensorFlow Lite has\n        been changed from r18b to r19c.\n    *   Supports int64 for mul.\n    *   Supports native variable builtin ops - ReadVariable, AssignVariable.\n    *   Converter:\n        *  Experimental support for variables in TFLite. To enable through\n           conversion, users need to set\n           `experimental_enable_resource_variables` on tf.lite.TFLiteConverter\n           to True.\n           Note: mutable variables is only available using from_saved_model\n           in this release, support for other methods is coming soon.\n        *  Old Converter (TOCO) is getting removed from next release.\n           It's been deprecated for few releases already.\n* `tf.saved_model`:\n    *   SavedModels can now save custom gradients. Use the option\n        `tf.saved_model.SaveOption(experimental_custom_gradients=True)` to\n        enable this feature. The documentation in [Advanced autodiff]\n        (https://www.tensorflow.org/guide/advanced_autodiff#custom_gradients)\n        has been updated.\n    *   Object metadata has now been deprecated and no longer saved to the\n        SavedModel.\n*   TF Core:\n    *   Added `tf.config.experimental.reset_memory_stats` to reset the tracked\n        peak memory returned by `tf.config.experimental.get_memory_info`.\n*  `tf.data`:\n    *   Added checkpointing support for `tf.data.experimental.save()`.\n    *   Added `target_workers` param to `data_service_ops.from_dataset_id` and\n        `data_service_ops.distribute`. Users can specify `\"AUTO\"`, `\"ANY\"`, or\n        `\"LOCAL\"` (case insensitive). If `\"AUTO\"`, tf.data service runtime\n        decides which workers to read from. If `\"ANY\"`, TF workers read from any\n        tf.data service workers. If `\"LOCAL\"`, TF workers will only read from\n        local in-processs tf.data service workers. `\"AUTO\"` works well for most\n        cases, while users can specify other targets. For example, `\"LOCAL\"`\n        would help avoid RPCs and data copy if every TF worker colocates with a\n        tf.data service worker. Currently, `\"AUTO\"` reads from any tf.data\n        service workers to preserve existing behavior. The default value is\n        `\"AUTO\"`.\n\n## Bug Fixes and Other Changes\n\n*<SIMILAR TO ABOVE SECTION, BUT FOR OTHER IMPORTANT CHANGES / BUG FIXES>\n*<IF A CHANGE CLOSES A GITHUB ISSUE, IT SHOULD BE DOCUMENTED HERE>\n*<NOTES SHOULD BE GROUPED PER AREA>\n*   TF Core:\n    *   Added `tf.lookup.experimental.MutableHashTable`, which provides a\n        generic mutable hash table implementation.\n        *   Compared to `tf.lookup.experimental.DenseHashTable` this offers\n        lower overall memory usage, and a cleaner API. It does not require\n        specifying a `delete_key` and `empty_key` that cannot be inserted into\n        the table.\n    *   Added support for specifying number of subdivisions in all reduce host\n        collective. This parallelizes work on CPU and speeds up the collective\n        performance. Default behavior is unchanged.\n    *   Add an option `perturb_singular` to `tf.linalg.tridiagonal_solve` that\n        allows solving linear systems with a numerically singular tridiagonal\n        matrix, e.g. for use in inverse iteration.\n    *   Added `tf.linalg.eigh_tridiagonal` that computes the eigenvalues of a\n        Hermitian tridiagonal matrix.\n    *   `tf.constant` now places its output on the current default device.\n    *   SavedModel\n        *   Added `tf.saved_model.experimental.TrackableResource`, which allows\n            the creation of custom wrapper objects for resource tensors.\n        *   Added a SavedModel load option to allow restoring partial\n            checkpoints into the SavedModel. See [`tf.saved_model.LoadOptions`]\n  (https://www.tensorflow.org/api_docs/python/tf/saved_model/LoadOptions)\n            for details.\n    *   Added a new op `SparseSegmentSumGrad` to match the other sparse segment\n        gradient ops and avoid an extra gather operation that was in the\n        previous gradient implementation.\n    *   Added a new session config setting `internal_fragmentation_fraction`,\n        which controls when the BFC Allocator needs to split an oversized chunk\n        to satisfy an allocation request.\n    *   Added `tf.get_current_name_scope()` which returns the current full name\n        scope string that will be prepended to op names.\n*   `tf.data`:\n    *   Promoting `tf.data.experimental.bucket_by_sequence_length` API to\n        `tf.data.Dataset.bucket_by_sequence_length` and deprecating the\n        experimental endpoint.\n    *   Promoting `tf.data.experimental.get_single_element` API to\n        `tf.data.Dataset.get_single_element` and deprecating the experimental\n        endpoint.\n    *   Promoting `tf.data.experimental.group_by_window` API to\n        `tf.data.Dataset.group_by_window` and deprecating the experimental\n        endpoint.\n    *   Promoting `tf.data.experimental.RandomDataset` API to\n        `tf.data.Dataset.random` and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.scan` API to `tf.data.Dataset.scan`\n        and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.snapshot` API to\n        `tf.data.Dataset.shapshot` and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.take_while` API to\n        `tf.data.Dataset.take_while` and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.ThreadingOptions` API to\n        `tf.data.ThreadingOptions` and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.unique` API to\n        `tf.data.Dataset.unique` and deprecating the experimental endpoint.\n    *   Added `stop_on_empty_dataset` parameter to `sample_from_datasets` and\n        `choose_from_datasets`. Setting `stop_on_empty_dataset=True` will stop\n        sampling if it encounters an empty dataset. This preserves the sampling\n        ratio throughout training. The prior behavior was to continue sampling,\n        skipping over exhausted datasets, until all datasets are exhausted. By\n        default, the original behavior (`stop_on_empty_dataset=False`) is\n        preserved.\n    *   Removed previously deprecated tf.data statistics related APIs:\n        *   `tf.data.Options.experimental_stats`\n        *   `tf.data.experimental.StatsAggregator`\n        *   `tf.data.experimental.StatsOptions.*`\n        *   `tf.data.experimental.bytes_produced_stats`\n        *   `tf.data.experimental.latency_stats`\n    *   Removed the following experimental tf.data optimization APIs:\n        *   `tf.data.experimental.MapVectorizationOptions.*`\n        *   `tf.data.experimental.OptimizationOptions.filter_with_random_uniform_fusion`\n        *   `tf.data.experimental.OptimizationOptions.hoist_random_uniform`\n        *   `tf.data.experimental.OptimizationOptions.map_vectorization`                 *   `tf.data.experimental.OptimizationOptions.reorder_data_discarding_ops`\n*   `tf.keras`:\n    *   Fix usage of `__getitem__` slicing in Keras Functional APIs when the\n        inputs are `RaggedTensor` objects.\n    *   Add `keepdims` argument to all `GlobalPooling` layers.\n    *   Add `include_preprocessing` argument to `MobileNetV3` architectures to\n        control the inclusion of `Rescaling` layer in the model.\n    *   Add optional argument (`force`) to `make_(train|test|predict)_funtion`\n        methods to skip the cached function and generate a new one. This is\n\tuseful to regenerate in a single call the compiled training function\n\twhen any `.trainable` attribute of any model's layer has changed.\n    *   Models now have a `save_spec` property which contains the `TensorSpec`\n        specs for calling the model. This spec is automatically saved when\n        the model is called for the first time.\n*   `tf.linalg`:\n    *   Add `CompositeTensor` as a base class to `LinearOperator`.\n*   `tf.lite`:\n    *   Fix mean op reference quantization rounding issue.\n    *   Added `framework_stable` BUILD target, which links in only the\n        non-experimental TF Lite APIs.\n    *   Remove deprecated Java `Interpreter` methods:\n        *    `modifyGraphWithDelegate` - Use `Interpreter.Options.addDelegate`\n        *    `setNumThreads` - Use `Interpreter.Options.setNumThreads`\n    *   Add Conv3DTranspose as a builtin op.\n*   `tf.summary`:\n    *   Fix `tf.summary.should_record_summaries()` so it correctly reflects when\n        summaries will be written, even when `tf.summary.record_if()` is not\n        in effect, by returning True tensor if default writer is present.\n*   `Grappler`:\n    *   Disable default Grappler optimization timeout to make the optimization\n        pipeline deterministic. This may lead to increased model loading time,\n        because time spent in graph optimizations is now unbounded (was 20\n        minutes).\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n<INSERT>, <NAME>, <HERE>, <USING>, <GITHUB>, <HANDLE>\n\n# Release 2.4.2\n\nThis release introduces several vulnerability fixes:\n\n*   Fixes a heap buffer overflow in `RaggedBinCount`\n    ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n*   Fixes a heap out of bounds write in `RaggedBinCount`\n    ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n*   Fixes a type confusion during tensor casts which leads to dereferencing null\n    pointers\n    ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n*   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n    ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n*   Fixes a null pointer dereference via invalid Ragged Tensors\n    ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n*   Fixes a division by zero in `Conv3D`\n    ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n*   Fixes vulnerabilities where session operations in eager mode lead to null\n    pointer dereferences\n    ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n*   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n    ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n*   Fixes a segfault in `SparseCountSparseOutput`\n    ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n*   Fixes a heap buffer overflow in `Conv3DBackprop*`\n    ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n*   Fixes a division by 0 in `Conv3DBackprop*`\n    ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n*   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n    ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n*   Fixes a division by 0 in `Conv2DBackpropFilter`\n    ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n*   Fixes a division by 0 in `Conv2DBackpropInput`\n    ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n*   Fixes a division by 0 in `Conv2D`\n    ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n*   Fixes a division by 0 in `QuantizedConv2D`\n    ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n*   Fixes a division by 0 in `QuantizedMul`\n    ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n*   Fixes vulnerabilities caused by invalid validation in\n    `SparseMatrixSparseCholesky`\n    ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n*   Fixes a heap buffer overflow caused by rounding\n    ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n    ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n*   Fixes a heap out of bounds read in `RaggedCross`\n    ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n*   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n    ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n*   Fixes a heap buffer overflow in `QuantizedMul`\n    ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n*   Fixes a `CHECK`-fail in `SparseConcat`\n    ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n*   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n    ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n*   Fixes a heap buffer overflow in `QuantizedReshape`\n    ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n*   Fixes a division by zero in `Conv2DBackpropFilter`\n    ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n*   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n    ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n*   Fixes a heap buffer overflow in `StringNGrams`\n    ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n*   Fixes a null pointer dereference in `StringNGrams`\n    ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n*   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n    ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n*   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n    ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n*   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n    ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n*   Fixes a division by 0 in `QuantizedBiasAdd`\n    ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n*   Fixes a heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n*   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n*   Fixes a division by 0 in `QuantizedAdd`\n    ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n*   Fixes a division by 0 in `FractionalAvgPool`\n    ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n*   Fixes an OOB read in `MatrixTriangularSolve`\n    ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n*   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n    ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n*   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n    ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n*   Fixes a division by 0 in `DenseCountSparseOutput`\n    ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n*   Fixes a division by 0 in `FusedBatchNorm`\n    ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n*   Fixes a division by 0 in `SparseMatMul`\n    ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n*   Fixes a division by 0 in `Reverse`\n    ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n*   Fixes a heap buffer overflow in `SparseSplit`\n    ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n*   Fixes a heap OOB access in unicode ops\n    ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n*   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n    ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n*   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n    ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n    ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n    ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n*   Fixes a null pointer dereference in `EditDistance`\n    ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n*   Fixes a null pointer dereference in `SparseFillEmptyRows`\n    ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n*   Fixes a heap OOB access in `Dilation2DBackpropInput`\n    ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n*   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n    ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n*   Fixes a set of vulnerabilities caused by lack of validation in\n    `SparseDenseCwiseMul`\n    ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n*   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n*   Fixes a heap out of bounds read in `RequantizationRange`\n    ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n*   Fixes a memory corruption in `DrawBoundingBoxesV2`\n    ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n*   Fixes a reference binding to nullptr in `SdcaOptimizer`\n    ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n*   Fixes an overflow and a denial of service in `tf.raw_ops.ReverseSequence`\n    ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n*   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n*   Fixes an undefined behavior in `MaxPool3DGradGrad`\n    ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n*   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n    ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n*   Fixes a heap buffer overflow in `AvgPool3DGrad`\n    ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n*   Fixes an undefined behavior and a `CHECK`-fail in `FractionalMaxPoolGrad`\n    ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n*   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n    ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n*   Fixes a heap buffer overflow in `MaxPoolGrad`\n    ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n*   Fixes a segfault in `CTCBeamSearchDecoder`\n    ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n*   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n    ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n*   Fixes a `CHECK`-fail due to integer overflow\n    ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n*   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n    ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n*   Fixes a division by zero in padding computation in TFLite\n    ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n*   Fixes a division by zero in optimized pooling implementations in TFLite\n    ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n    ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n*   Fixes a division by zero in TFLite's implementation of `GatherNd`\n    ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n*   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n    ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n*   Fixes a heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n    ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n*   Fixes a null pointer dereference in TFLite's `Reshape` operator\n    ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n*   Fixes a stack overflow due to looping TFLite subgraph\n    ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n*   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n    ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n*   Fixes a division by zero in TFLite's convolution code\n    ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n*   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n    ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n*   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n    ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n    ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n*   Fixes a division by zero in TFLite's implementation of `SVDF`\n    ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n*   Fixes a division by zero in TFLite's implementation of `Split`\n    ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n*   Fixes a division by zero in TFLite's implementation of `OneHot`\n    ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n*   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n    ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n*   Fixes a division by zero in TFLite's implementation of hashtable lookup\n    ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n*   Fixes a integer overflow in TFLite concatentation\n    ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n*   Fixes a integer overflow in TFLite memory allocation\n    ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n*   Fixes a heap OOB write in TFLite\n    ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n*   Fixes a heap OOB read in TFLite\n    ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n*   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n    ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n    ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `SparseSparseMinimum`\n    ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n    ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n*   Fixes vulnerabilities caused by invalid validation in\n    `QuantizeAndDequantizeV2`\n    ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n*   Fixes a heap buffer overflow in `BandedTriangularSolve`\n    ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `tf.raw_ops.CTCLoss`\n    ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n*   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n    ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n*   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n    ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n*   Fixes a null dereference in Grappler's `TrySimplify`\n    ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n*   Fixes a crash in `tf.transpose` with complex inputs\n    ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n*   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n    ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n*   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n    ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n*   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n    ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n*   Updates `curl` to `7.76.0` to handle\n    [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n    [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n    [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n    [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n    [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n    and\n    [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n# Release 2.3.3\n\nThis release introduces several vulnerability fixes:\n\n*   Fixes a heap buffer overflow in `RaggedBinCount`\n    ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n*   Fixes a heap out of bounds write in `RaggedBinCount`\n    ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n*   Fixes a type confusion during tensor casts which leads to dereferencing null\n    pointers\n    ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n*   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n    ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n*   Fixes a null pointer dereference via invalid Ragged Tensors\n    ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n*   Fixes a division by zero in `Conv3D`\n    ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n*   Fixes vulnerabilities where session operations in eager mode lead to null\n    pointer dereferences\n    ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n*   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n    ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n*   Fixes a segfault in `SparseCountSparseOutput`\n    ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n*   Fixes a heap buffer overflow in `Conv3DBackprop*`\n    ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n*   Fixes a division by 0 in `Conv3DBackprop*`\n    ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n*   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n    ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n*   Fixes a division by 0 in `Conv2DBackpropFilter`\n    ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n*   Fixes a division by 0 in `Conv2DBackpropInput`\n    ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n*   Fixes a division by 0 in `Conv2D`\n    ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n*   Fixes a division by 0 in `QuantizedConv2D`\n    ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n*   Fixes a division by 0 in `QuantizedMul`\n    ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n*   Fixes vulnerabilities caused by invalid validation in\n    `SparseMatrixSparseCholesky`\n    ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n*   Fixes a heap buffer overflow caused by rounding\n    ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n    ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n*   Fixes a heap out of bounds read in `RaggedCross`\n    ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n*   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n    ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n*   Fixes a heap buffer overflow in `QuantizedMul`\n    ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n*   Fixes a `CHECK`-fail in `SparseConcat`\n    ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n*   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n    ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n*   Fixes a heap buffer overflow in `QuantizedReshape`\n    ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n*   Fixes a division by zero in `Conv2DBackpropFilter`\n    ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n*   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n    ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n*   Fixes a heap buffer overflow in `StringNGrams`\n    ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n*   Fixes a null pointer dereference in `StringNGrams`\n    ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n*   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n    ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n*   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n    ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n*   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n    ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n*   Fixes a division by 0 in `QuantizedBiasAdd`\n    ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n*   Fixes a heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n*   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n*   Fixes a division by 0 in `QuantizedAdd`\n    ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n*   Fixes a division by 0 in `FractionalAvgPool`\n    ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n*   Fixes an OOB read in `MatrixTriangularSolve`\n    ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n*   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n    ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n*   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n    ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n*   Fixes a division by 0 in `DenseCountSparseOutput`\n    ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n*   Fixes a division by 0 in `FusedBatchNorm`\n    ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n*   Fixes a division by 0 in `SparseMatMul`\n    ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n*   Fixes a division by 0 in `Reverse`\n    ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n*   Fixes a heap buffer overflow in `SparseSplit`\n    ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n*   Fixes a heap OOB access in unicode ops\n    ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n*   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n    ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n*   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n    ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n    ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n    ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n*   Fixes a null pointer dereference in `EditDistance`\n    ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n*   Fixes a null pointer dereference in `SparseFillEmptyRows`\n    ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n*   Fixes a heap OOB access in `Dilation2DBackpropInput`\n    ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n*   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n    ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n*   Fixes a set of vulnerabilities caused by lack of validation in\n    `SparseDenseCwiseMul`\n    ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n*   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n*   Fixes a heap out of bounds read in `RequantizationRange`\n    ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n*   Fixes a memory corruption in `DrawBoundingBoxesV2`\n    ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n*   Fixes a reference binding to nullptr in `SdcaOptimizer`\n    ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n*   Fixes an overflow and a denial of service in `tf.raw_ops.ReverseSequence`\n    ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n*   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n*   Fixes an undefined behavior in `MaxPool3DGradGrad`\n    ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n*   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n    ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n*   Fixes a heap buffer overflow in `AvgPool3DGrad`\n    ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n*   Fixes an undefined behavior and a `CHECK`-fail in `FractionalMaxPoolGrad`\n    ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n*   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n    ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n*   Fixes a heap buffer overflow in `MaxPoolGrad`\n    ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n*   Fixes a segfault in `CTCBeamSearchDecoder`\n    ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n*   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n    ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n*   Fixes a `CHECK`-fail due to integer overflow\n    ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n*   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n    ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n*   Fixes a division by zero in padding computation in TFLite\n    ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n*   Fixes a division by zero in optimized pooling implementations in TFLite\n    ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n    ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n*   Fixes a division by zero in TFLite's implementation of `GatherNd`\n    ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n*   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n    ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n*   Fixes a heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n    ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n*   Fixes a null pointer dereference in TFLite's `Reshape` operator\n    ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n*   Fixes a stack overflow due to looping TFLite subgraph\n    ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n*   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n    ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n*   Fixes a division by zero in TFLite's convolution code\n    ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n*   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n    ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n*   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n    ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n    ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n*   Fixes a division by zero in TFLite's implementation of `SVDF`\n    ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n*   Fixes a division by zero in TFLite's implementation of `Split`\n    ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n*   Fixes a division by zero in TFLite's implementation of `OneHot`\n    ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n*   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n    ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n*   Fixes a division by zero in TFLite's implementation of hashtable lookup\n    ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n*   Fixes a integer overflow in TFLite concatentation\n    ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n*   Fixes a integer overflow in TFLite memory allocation\n    ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n*   Fixes a heap OOB write in TFLite\n    ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n*   Fixes a heap OOB read in TFLite\n    ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n*   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n    ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n    ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `SparseSparseMinimum`\n    ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n    ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n*   Fixes vulnerabilities caused by invalid validation in\n    `QuantizeAndDequantizeV2`\n    ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n*   Fixes a heap buffer overflow in `BandedTriangularSolve`\n    ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `tf.raw_ops.CTCLoss`\n    ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n*   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n    ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n*   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n    ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n*   Fixes a null dereference in Grappler's `TrySimplify`\n    ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n*   Fixes a crash in `tf.transpose` with complex inputs\n    ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n*   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n    ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n*   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n    ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n*   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n    ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n*   Updates `curl` to `7.76.0` to handle\n    [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n    [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n    [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n    [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n    [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n    and\n    [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n# Release 2.2.3\n\nThis release introduces several vulnerability fixes:\n\n*   Fixes a heap buffer overflow in `RaggedBinCount`\n    ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n*   Fixes a heap out of bounds write in `RaggedBinCount`\n    ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n*   Fixes a type confusion during tensor casts which leads to dereferencing null\n    pointers\n    ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n*   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n    ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n*   Fixes a null pointer dereference via invalid Ragged Tensors\n    ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n*   Fixes a division by zero in `Conv3D`\n    ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n*   Fixes vulnerabilities where session operations in eager mode lead to null\n    pointer dereferences\n    ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n*   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n    ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n*   Fixes a segfault in `SparseCountSparseOutput`\n    ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n*   Fixes a heap buffer overflow in `Conv3DBackprop*`\n    ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n*   Fixes a division by 0 in `Conv3DBackprop*`\n    ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n*   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n    ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n*   Fixes a division by 0 in `Conv2DBackpropFilter`\n    ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n*   Fixes a division by 0 in `Conv2DBackpropInput`\n    ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n*   Fixes a division by 0 in `Conv2D`\n    ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n*   Fixes a division by 0 in `QuantizedConv2D`\n    ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n*   Fixes a division by 0 in `QuantizedMul`\n    ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n*   Fixes vulnerabilities caused by invalid validation in\n    `SparseMatrixSparseCholesky`\n    ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n*   Fixes a heap buffer overflow caused by rounding\n    ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n    ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n*   Fixes a heap out of bounds read in `RaggedCross`\n    ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n*   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n    ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n*   Fixes a heap buffer overflow in `QuantizedMul`\n    ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n*   Fixes a `CHECK`-fail in `SparseConcat`\n    ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n*   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n    ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n*   Fixes a heap buffer overflow in `QuantizedReshape`\n    ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n*   Fixes a division by zero in `Conv2DBackpropFilter`\n    ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n*   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n    ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n*   Fixes a heap buffer overflow in `StringNGrams`\n    ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n*   Fixes a null pointer dereference in `StringNGrams`\n    ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n*   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n    ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n*   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n    ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n*   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n    ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n*   Fixes a division by 0 in `QuantizedBiasAdd`\n    ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n*   Fixes a heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n*   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n*   Fixes a division by 0 in `QuantizedAdd`\n    ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n*   Fixes a division by 0 in `FractionalAvgPool`\n    ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n*   Fixes an OOB read in `MatrixTriangularSolve`\n    ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n*   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n    ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n*   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n    ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n*   Fixes a division by 0 in `DenseCountSparseOutput`\n    ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n*   Fixes a division by 0 in `FusedBatchNorm`\n    ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n*   Fixes a division by 0 in `SparseMatMul`\n    ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n*   Fixes a division by 0 in `Reverse`\n    ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n*   Fixes a heap buffer overflow in `SparseSplit`\n    ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n*   Fixes a heap OOB access in unicode ops\n    ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n*   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n    ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n*   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n    ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n    ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n    ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n*   Fixes a null pointer dereference in `EditDistance`\n    ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n*   Fixes a null pointer dereference in `SparseFillEmptyRows`\n    ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n*   Fixes a heap OOB access in `Dilation2DBackpropInput`\n    ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n*   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n    ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n*   Fixes a set of vulnerabilities caused by lack of validation in\n    `SparseDenseCwiseMul`\n    ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n*   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n*   Fixes a heap out of bounds read in `RequantizationRange`\n    ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n*   Fixes a memory corruption in `DrawBoundingBoxesV2`\n    ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n*   Fixes a reference binding to nullptr in `SdcaOptimizer`\n    ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n*   Fixes an overflow and a denial of service in `tf.raw_ops.ReverseSequence`\n    ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n*   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n*   Fixes an undefined behavior in `MaxPool3DGradGrad`\n    ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n*   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n    ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n*   Fixes a heap buffer overflow in `AvgPool3DGrad`\n    ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n*   Fixes an undefined behavior and a `CHECK`-fail in `FractionalMaxPoolGrad`\n    ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n*   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n    ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n*   Fixes a heap buffer overflow in `MaxPoolGrad`\n    ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n*   Fixes a segfault in `CTCBeamSearchDecoder`\n    ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n*   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n    ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n*   Fixes a `CHECK`-fail due to integer overflow\n    ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n*   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n    ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n*   Fixes a division by zero in padding computation in TFLite\n    ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n*   Fixes a division by zero in optimized pooling implementations in TFLite\n    ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n    ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n*   Fixes a division by zero in TFLite's implementation of `GatherNd`\n    ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n*   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n    ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n*   Fixes a heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n    ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n*   Fixes a null pointer dereference in TFLite's `Reshape` operator\n    ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n*   Fixes a stack overflow due to looping TFLite subgraph\n    ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n*   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n    ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n*   Fixes a division by zero in TFLite's convolution code\n    ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n*   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n    ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n*   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n    ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n    ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n*   Fixes a division by zero in TFLite's implementation of `SVDF`\n    ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n*   Fixes a division by zero in TFLite's implementation of `Split`\n    ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n*   Fixes a division by zero in TFLite's implementation of `OneHot`\n    ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n*   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n    ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n*   Fixes a division by zero in TFLite's implementation of hashtable lookup\n    ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n*   Fixes a integer overflow in TFLite concatentation\n    ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n*   Fixes a integer overflow in TFLite memory allocation\n    ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n*   Fixes a heap OOB write in TFLite\n    ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n*   Fixes a heap OOB read in TFLite\n    ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n*   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n    ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n    ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `SparseSparseMinimum`\n    ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n    ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n*   Fixes vulnerabilities caused by invalid validation in\n    `QuantizeAndDequantizeV2`\n    ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n*   Fixes a heap buffer overflow in `BandedTriangularSolve`\n    ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `tf.raw_ops.CTCLoss`\n    ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n*   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n    ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n*   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n    ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n*   Fixes a null dereference in Grappler's `TrySimplify`\n    ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n*   Fixes a crash in `tf.transpose` with complex inputs\n    ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n*   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n    ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n*   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n    ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n*   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n    ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n*   Updates `curl` to `7.76.0` to handle\n    [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n    [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n    [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n    [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n    [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n    and\n    [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n# Release 2.1.4\n\nThis release introduces several vulnerability fixes:\n\n*   Fixes a heap buffer overflow in `RaggedBinCount`\n    ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n*   Fixes a heap out of bounds write in `RaggedBinCount`\n    ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n*   Fixes a type confusion during tensor casts which leads to dereferencing null\n    pointers\n    ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n*   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n    ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n*   Fixes a null pointer dereference via invalid Ragged Tensors\n    ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n*   Fixes a division by zero in `Conv3D`\n    ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n*   Fixes vulnerabilities where session operations in eager mode lead to null\n    pointer dereferences\n    ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n*   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n    ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n*   Fixes a segfault in `SparseCountSparseOutput`\n    ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n*   Fixes a heap buffer overflow in `Conv3DBackprop*`\n    ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n*   Fixes a division by 0 in `Conv3DBackprop*`\n    ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n*   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n    ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n*   Fixes a division by 0 in `Conv2DBackpropFilter`\n    ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n*   Fixes a division by 0 in `Conv2DBackpropInput`\n    ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n*   Fixes a division by 0 in `Conv2D`\n    ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n*   Fixes a division by 0 in `QuantizedConv2D`\n    ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n*   Fixes a division by 0 in `QuantizedMul`\n    ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n*   Fixes vulnerabilities caused by invalid validation in\n    `SparseMatrixSparseCholesky`\n    ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n*   Fixes a heap buffer overflow caused by rounding\n    ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n    ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n*   Fixes a heap out of bounds read in `RaggedCross`\n    ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n*   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n    ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n*   Fixes a heap buffer overflow in `QuantizedMul`\n    ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n*   Fixes a `CHECK`-fail in `SparseConcat`\n    ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n*   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n    ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n*   Fixes a heap buffer overflow in `QuantizedReshape`\n    ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n*   Fixes a division by zero in `Conv2DBackpropFilter`\n    ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n*   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n    ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n*   Fixes a heap buffer overflow in `StringNGrams`\n    ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n*   Fixes a null pointer dereference in `StringNGrams`\n    ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n*   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n    ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n*   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n    ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n*   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n    ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n*   Fixes a division by 0 in `QuantizedBiasAdd`\n    ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n*   Fixes a heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n*   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n*   Fixes a division by 0 in `QuantizedAdd`\n    ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n*   Fixes a division by 0 in `FractionalAvgPool`\n    ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n*   Fixes an OOB read in `MatrixTriangularSolve`\n    ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n*   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n    ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n*   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n    ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n*   Fixes a division by 0 in `DenseCountSparseOutput`\n    ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n*   Fixes a division by 0 in `FusedBatchNorm`\n    ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n*   Fixes a division by 0 in `SparseMatMul`\n    ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n*   Fixes a division by 0 in `Reverse`\n    ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n*   Fixes a heap buffer overflow in `SparseSplit`\n    ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n*   Fixes a heap OOB access in unicode ops\n    ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n*   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n    ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n*   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n    ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n    ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n    ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n*   Fixes a null pointer dereference in `EditDistance`\n    ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n*   Fixes a null pointer dereference in `SparseFillEmptyRows`\n    ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n*   Fixes a heap OOB access in `Dilation2DBackpropInput`\n    ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n*   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n    ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n*   Fixes a set of vulnerabilities caused by lack of validation in\n    `SparseDenseCwiseMul`\n    ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n*   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n*   Fixes a heap out of bounds read in `RequantizationRange`\n    ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n*   Fixes a memory corruption in `DrawBoundingBoxesV2`\n    ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n*   Fixes a reference binding to nullptr in `SdcaOptimizer`\n    ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n*   Fixes an overflow and a denial of service in `tf.raw_ops.ReverseSequence`\n    ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n*   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n*   Fixes an undefined behavior in `MaxPool3DGradGrad`\n    ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n*   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n    ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n*   Fixes a heap buffer overflow in `AvgPool3DGrad`\n    ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n*   Fixes an undefined behavior and a `CHECK`-fail in `FractionalMaxPoolGrad`\n    ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n*   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n    ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n*   Fixes a heap buffer overflow in `MaxPoolGrad`\n    ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n*   Fixes a segfault in `CTCBeamSearchDecoder`\n    ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n*   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n    ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n*   Fixes a `CHECK`-fail due to integer overflow\n    ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n*   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n    ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n*   Fixes a division by zero in padding computation in TFLite\n    ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n*   Fixes a division by zero in optimized pooling implementations in TFLite\n    ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n    ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n*   Fixes a division by zero in TFLite's implementation of `GatherNd`\n    ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n*   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n    ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n*   Fixes a heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n    ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n*   Fixes a null pointer dereference in TFLite's `Reshape` operator\n    ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n*   Fixes a stack overflow due to looping TFLite subgraph\n    ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n*   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n    ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n*   Fixes a division by zero in TFLite's convolution code\n    ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n*   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n    ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n*   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n    ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n    ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n*   Fixes a division by zero in TFLite's implementation of `SVDF`\n    ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n*   Fixes a division by zero in TFLite's implementation of `Split`\n    ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n*   Fixes a division by zero in TFLite's implementation of `OneHot`\n    ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n*   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n    ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n*   Fixes a division by zero in TFLite's implementation of hashtable lookup\n    ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n*   Fixes a integer overflow in TFLite concatentation\n    ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n*   Fixes a integer overflow in TFLite memory allocation\n    ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n*   Fixes a heap OOB write in TFLite\n    ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n*   Fixes a heap OOB read in TFLite\n    ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n*   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n    ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n    ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `SparseSparseMinimum`\n    ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n    ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n*   Fixes vulnerabilities caused by invalid validation in\n    `QuantizeAndDequantizeV2`\n    ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n*   Fixes a heap buffer overflow in `BandedTriangularSolve`\n    ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `tf.raw_ops.CTCLoss`\n    ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n*   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n    ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n*   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n    ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n*   Fixes a null dereference in Grappler's `TrySimplify`\n    ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n*   Fixes a crash in `tf.transpose` with complex inputs\n    ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n*   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n    ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n*   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n    ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n*   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n    ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n*   Updates `curl` to `7.76.0` to handle\n    [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n    [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n    [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n    [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n    [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n    and\n    [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n# Release 2.5.0\n\n## Major Features and Improvements\n\n*   Support for Python3.9 has been added.\n*   `tf.data`:\n    *   `tf.data` service now supports strict round-robin reads, which is useful\n        for synchronous training workloads where example sizes vary. With strict\n        round robin reads, users can guarantee that consumers get similar-sized\n        examples in the same step.\n    *   tf.data service now supports optional compression. Previously data would\n        always be compressed, but now you can disable compression by passing\n        `compression=None` to `tf.data.experimental.service.distribute(...)`.\n    *   `tf.data.Dataset.batch()` now supports `num_parallel_calls` and\n        `deterministic` arguments. `num_parallel_calls` is used to indicate that\n        multiple input batches should be computed in parallel. With\n        `num_parallel_calls` set, `deterministic` is used to indicate that\n        outputs can be obtained in the non-deterministic order.\n    *   Options returned by `tf.data.Dataset.options()` are no longer mutable.\n    *   tf.data input pipelines can now be executed in debug mode, which\n        disables any asynchrony, parallelism, or non-determinism and forces\n        Python execution (as opposed to trace-compiled graph execution) of\n        user-defined functions passed into transformations such as `map`. The\n        debug mode can be enabled through\n        `tf.data.experimental.enable_debug_mode()`.\n*   `tf.lite`\n    *   Enabled the new MLIR-based quantization backend by default\n        *   The new backend is used for 8 bits full integer post-training\n            quantization\n        *   The new backend removes the redundant rescales and fixes some bugs\n            (shared weight/bias, extremely small scales, etc)\n        *   Set `experimental_new_quantizer` in tf.lite.TFLiteConverter to False\n            to disable this change\n*   `tf.keras`\n    *   `tf.keras.metrics.AUC` now support logit predictions.\n    *   Enabled a new supported input type in `Model.fit`,\n        `tf.keras.utils.experimental.DatasetCreator`, which takes a callable,\n        `dataset_fn`. `DatasetCreator` is intended to work across all\n        `tf.distribute` strategies, and is the only input type supported for\n        Parameter Server strategy.\n*   `tf.distribute`\n    *   `tf.distribute.experimental.ParameterServerStrategy` now supports\n        training with Keras `Model.fit` when used with `DatasetCreator`.\n    *   Creating `tf.random.Generator` under `tf.distribute.Strategy` scopes is\n        now allowed (except for\n        `tf.distribute.experimental.CentralStorageStrategy` and\n        `tf.distribute.experimental.ParameterServerStrategy`). Different\n        replicas will get different random-number streams.\n*   TPU embedding support\n    *   Added `profile_data_directory` to `EmbeddingConfigSpec` in\n        `_tpu_estimator_embedding.py`. This allows embedding lookup statistics\n        gathered at runtime to be used in embedding layer partitioning\n        decisions.\n*   PluggableDevice\n    *   Third-party devices can now connect to TensorFlow as plug-ins through\n        [StreamExecutor C API](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md).\n        and\n        [PluggableDevice](https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md)\n        interface.\n        *   Add custom ops and kernels through\n            [kernel and op registration C API](https://github.com/tensorflow/community/blob/master/rfcs/20190814-kernel-and-op-registration.md).\n        *   Register custom graph optimization passes with\n            [graph optimization C API](https://github.com/tensorflow/community/blob/master/rfcs/20201027-modular-tensorflow-graph-c-api.md).\n*   [oneAPI Deep Neural Network Library (oneDNN)](https://github.com/oneapi-src/oneDNN)\n    CPU performance optimizations from\n    [Intel-optimized TensorFlow](https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html)\n    are now available in the official x86-64 Linux and Windows builds.\n    *   They are off by default. Enable them by setting the environment variable\n        `TF_ENABLE_ONEDNN_OPTS=1`.\n    *   We do not recommend using them in GPU systems, as they have not been\n        sufficiently tested with GPUs yet.\n*   TensorFlow pip packages are now built with CUDA11.2 and cuDNN 8.1.0\n\n## Breaking Changes\n\n*   The `TF_CPP_MIN_VLOG_LEVEL` environment variable has been renamed to to\n    `TF_CPP_MAX_VLOG_LEVEL` which correctly describes its effect.\n\n## Bug Fixes and Other Changes\n\n*   `tf.keras`:\n\n    *   Preprocessing layers API consistency changes:\n        *   `StringLookup` added `output_mode`, `sparse`, and\n            `pad_to_max_tokens` arguments with same semantics as\n            `TextVectorization`.\n        *   `IntegerLookup` added `output_mode`, `sparse`, and\n            `pad_to_max_tokens` arguments with same semantics as\n            `TextVectorization`. Renamed `max_values`, `oov_value` and\n            `mask_value` to `max_tokens`, `oov_token` and `mask_token` to align\n            with `StringLookup` and `TextVectorization`.\n        *   `TextVectorization` default for `pad_to_max_tokens` switched to\n            False.\n        *   `CategoryEncoding` no longer supports `adapt`, `IntegerLookup` now\n            supports equivalent functionality. `max_tokens` argument renamed to\n            `num_tokens`.\n        *   `Discretization` added `num_bins` argument for learning bins\n            boundaries through calling `adapt` on a dataset. Renamed `bins`\n            argument to `bin_boundaries` for specifying bins without `adapt`.\n    *   Improvements to model saving/loading:\n        *   `model.load_weights` now accepts paths to saved models.\n    *   Keras inputs can now be created directly from arbitrary `tf.TypeSpecs`.\n    *   Two new learning rate schedules added:\n        `tf.keras.optimizers.schedules.CosineDecay`\n        and`tf.keras.optimizers.schedules.CosineDecayRestarts`.\n\n*   `tf.data`:\n\n    *   Exposing `tf.data.experimental.ExternalStatePolicy`, which can be used\n        to control how external state should be handled during dataset\n        serialization or iterator checkpointing.\n    *   Changing `tf.data.experimental.save` to store the type specification of\n        the dataset elements. This avoids the need for explicitly specifying the\n        `element_spec` argument of `tf.data.experimental.load` when loading the\n        previously saved dataset.\n    *   Add `.element_spec` property to `tf.data.DatasetSpec` to access the\n        inner spec. This can be used to extract the structure of nested\n        datasets.\n    *   Add `tf.data.experimental.AutoShardingPolicy.HINT` which can be used to\n        provide hints to tf.distribute-based auto-sharding as to where in the\n        input pipeline to insert sharding transformations.\n    *   Make tf.data.Options persistent across `tf.function` and `GraphDef`\n        boundaries.\n\n*   XLA compilation:\n\n    *   `tf.function(experimental_compile=True)` has become a stable API,\n        renamed `tf.function(jit_compile=True)`.\n    *   XLA can now compile MirroredStrategy: the step function passed\n        to`strategy.run` can now be annoted with `jit_compile=True`.\n\n*   `tf.distribute`:\n\n    *   Rename `experimental_prefetch_to_device` in `tf.distribute.InputOptions`\n        to `experimental_fetch_to_device` to better reflect the purpose.\n\n*   `tf.lite`:\n\n    *   class `tflite::Subgraph`:\n        *   Removed the `tensors()` method and the non-const overload of the\n            `nodes_and_registration()` method, both of which were previously\n            documented as temporary and to be removed.\n            *   Uses of `tensors()` can be replaced by calling the existing\n                methods `tensors_size()` and `tensor(int)`.\n            *   Uses of the non-const overload of `nodes_and_registration` can\n                be replaced by calling the existing methods `nodes_size()` and\n                `context()`, and then calling the `GetNodeAndRegistration`\n                method in the `TfLiteContext` returned by `context()`.\n    *   NNAPI\n        *   Removed deprecated `Interpreter::UseNNAPI(bool)` C++ API.\n            *   Use `NnApiDelegate()` and related delegate configuration methods\n                directly.\n        *   Replaced the model cache key for models computation algorithm with\n            one guaranteed to be stable across runs.\n    *   16 bits quantization\n        *   Added int16x8 support for ABS, REDUCE_MAX and REDUCE_MIN operators.\n        *   Additional tests and fixes for ADD and SUB operators.\n    *   Added support for saved model's session initializer through\n        `TFLiteConverter.from_saved_model`.\n    *   Added DEPTH_TO_SPACE support in Post training quantization.\n    *   Added dynamic range quantization support for the BatchMatMul op.\n        *   Both symmetric and asymmetric quantized input tensor are supported.\n    *   Add `RFFT2D` as builtin op. (`RFFT2D` also supports `RFFTD`.) Currently\n        only supports float32 input.\n    *   Add 5D support to `SLICE` op.\n    *   TFLite Supports SingatureDef:\n        *   TFLiteConverter exports models with SignatureDef\n        *   Interpreter supports getting a list of signatures and getting\n            callable function for a given signaturedef.\n    *   Add int8 support for `ReshapeV2`.\n    *   Add experimental support for optimization with sparsity.\n    *   Add nominal support for unsigned 32-bit integer tensor types. Note that\n        very few TFLite kernels support this type natively, so its use in mobile\n        ML authoring is generally discouraged.\n    *   Add support for static hash tables through\n        `TFLiteConverter.from_saved_model`.\n    *   The Python TF Lite Interpreter bindings now has an option\n        `experimental_preserve_all_tensors` to aid in debugging conversion.\n    *   Quantized x86 execution defaults to Ruy GEMM library for platforms with\n        AVX support.\n    *   Deprecate\n        `tf.compat.v1.lite.experimental.get_potentially_supported_ops`. Use\n        `tf.lite.TFLiteConverter` directly to check whether a model is\n        convertible.\n    *   Add support to select one of three different built-in op resolvers\n    *   Enabled post training with calibrations for models that require user\n        provided TensorFlow Lite custom op libraries via\n        `converter.target_spec._experimental_custom_op_registerers`. used in\n        Python Interpreter API.\n\n*   TF Core:\n\n    *   Corrected higher-order gradients of control flow constructs (`tf.cond`,\n        `tf.while_loop`, and compositions like `tf.foldl`) computed with\n        `tf.GradientTape` inside a `tf.function`.\n    *   Changed the default step size in `gradient_checker_v2.compute_gradients`\n        to be exactly representable as a binary floating point numbers. This\n        avoids poluting gradient approximations needlessly, which is some cases\n        leads to false negatives in op gradient tests.\n    *   Added `tf.config.experimental.get_memory_info`, returning a dict with\n        the current and peak memory usage. Deprecated\n        `tf.config.experimental.get_memory_usage` in favor of this new function.\n    *   Extended `tf.config.experimental.enable_tensor_float_32_execution` to\n        control Tensor-Float-32 evaluation in RNNs.\n    *   Added a 'experimental_payloads' field to tf.errors.OpError and its\n        subclasses to support more detailed error reporting. This is inspired\n        from Abseil Status payloads:\n        https://github.com/abseil/abseil-cpp/blob/master/absl/status/status.h\n\n*   `tf.summary`:\n\n    *   New `tf.summary.graph` allows manual write of TensorFlow graph\n        (`tf.Graph` or `tf.compat.v1.GraphDef`) as a summary. This is not a\n        replacement for the trace-based API.\n\n*   Set `/d2ReducedOptimizeHugeFunctions` by default for Windows builds. This\n    provides a big compile-time speedup, and effectively raises the minimum\n    supported MSVC version to 16.4 (current: 16.8).\n\n    *   See:\n        https://groups.google.com/a/tensorflow.org/d/topic/build/SsW98Eo7l3o/discussion\n\n*   TensorRT\n\n    *   Removed the deprecated `session_config` parameter for the TF1-TRT\n        converter `TrtGraphConverter`. Previously, we issued a warning when the\n        value of the parameter is not None.\n    *   The TF2-TRT converter `TrtGraphConverterV2` takes an object of class\n        TrtConversionParams as a parameter. Removed three deprecated fields from\n        this class: `rewriter_config_template`, `is_dynamic_op`, and\n        `max_batch_size`. Previously, we issued a warning when the value of\n        `rewriter_config_template` is not None. We issued an error when the\n        value of `is_dynamic_op` is not True. We didn't use the value for\n        `max_batch_size` for building TensorRT engines. Add parameters\n        `use_dynamic_shape` to enable dynamic shape support. The default is to\n        disable dynamic shape support. Add `dynamic_shape_profile_strategy` for\n        selecting a dynamic shape profile strategy. The default is profile\n        strategy is `Range`.\n    *   Issue a warning when function get_tensorrt_rewriter_config is used.\n\n*   TF XLA\n\n    *   Add new enum value `MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED` to\n        `tf.config.experimental.mlir_bridge_rollout` to enable a \\\"safe\\\" mode.\n        This runs the MLIR bridge only when an analysis of the graph only when\n        an analysis of the graph determines that it is safe to run.\n    *   Add new enum value `MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED'\n        to`tf.config.experimental.mlir_bridge_rollout` to enable a fallback for\n        the MLIR bridge in a \\\"safe\\\" mode. This runs the MLIR bridge in a\n        FallbackEnabled mode when an analysis of the graph determines that the\n        graph does not have unsupported features.\n\n*   Deterministic Op Functionality:\n\n    *   Add determinism-unimplemented exception-throwing to the segment-sum ops.\n        When the environment variable `TF_DETERMINISTIC_OPS` is set to `\"true\"`\n        or `\"1\"` (when op-determinism is expected), an attempt to run the\n        following ops on a GPU will throw `tf.errors.UnimplementedError` (with\n        an understandable message) when `data` is a floating-point type,\n        including complex types (if supported): `tf.math.segment_prod`,\n        `tf.math.segment_sum`, `tf.math.unsorted_segment_mean`,\n        `tf.math.unsorted_segment_sqrt_n`, `tf.math.unsorted_segment_prod`,\n        `tf.math.unsorted_segment_sum`, and therefore also\n        `tf.convert_to_tensor` when `value` is of type `tf.IndexedSlices` (such\n        as in the back prop though `tf.gather` into a dense embedding). See\n        issue [39751](https://github.com/tensorflow/tensorflow/issues/39751)\n        which this change addresses, but does not solve. This exception-throwing\n        behavior can be disabled by setting the environment variable\n        `TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS` to `\"true\"` or\n        `\"1\"`. For more information about these changes, see the description in\n        pull request\n        [47772](https://github.com/tensorflow/tensorflow/pull/47772).\n    *   In previous versions of TensorFlow, when a GPU was available,\n        `tf.sparse.sparse_dense_matmul` introduced truly random noise in the\n        forward path for data of type `tf.float32` but not for data of type\n        `tf.float64` (for which there was no GPU implementation). In this\n        current release, GPU support for other floating-point types\n        (`tf.float16`, `tf.float64`, `tf.complex64`, and `tf.complex128`) has\n        been added for this op. If you were relying on the determinism of the\n        `tf.float64` CPU implementation being automatically selected because of\n        the absence of the `tf.float64` GPU implementation, you with either need\n        to force the op to run on the CPU or use a different data type.\n\n*   Security\n\n    *   Fixes a heap buffer overflow in `RaggedBinCount`\n        ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n    *   Fixes a heap out of bounds write in `RaggedBinCount`\n        ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n    *   Fixes a type confusion during tensor casts which leads to dereferencing\n        null pointers\n        ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n    *   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n        ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n    *   Fixes a null pointer dereference via invalid Ragged Tensors\n        ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n    *   Fixes a division by zero in `Conv3D`\n        ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n    *   Fixes vulnerabilities where session operations in eager mode lead to\n        null pointer dereferences\n        ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n    *   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n        ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n    *   Fixes a segfault in `SparseCountSparseOutput`\n        ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n    *   Fixes a heap buffer overflow in `Conv3DBackprop*`\n        ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n    *   Fixes a division by 0 in `Conv3DBackprop*`\n        ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n    *   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n        ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n    *   Fixes a division by 0 in `Conv2DBackpropFilter`\n        ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n    *   Fixes a division by 0 in `Conv2DBackpropInput`\n        ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n    *   Fixes a division by 0 in `Conv2D`\n        ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n    *   Fixes a division by 0 in `QuantizedConv2D`\n        ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n    *   Fixes a division by 0 in `QuantizedMul`\n        ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n    *   Fixes vulnerabilities caused by invalid validation in\n        `SparseMatrixSparseCholesky`\n        ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n    *   Fixes a heap buffer overflow caused by rounding\n        ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n    *   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n        ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n    *   Fixes a heap out of bounds read in `RaggedCross`\n        ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n    *   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n        ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n    *   Fixes a heap buffer overflow in `QuantizedMul`\n        ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n    *   Fixes a `CHECK`-fail in `SparseConcat`\n        ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n    *   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n        ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n    *   Fixes a heap buffer overflow in `QuantizedReshape`\n        ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n    *   Fixes a division by zero in `Conv2DBackpropFilter`\n        ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n    *   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n        ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n    *   Fixes a heap buffer overflow in `StringNGrams`\n        ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n    *   Fixes a null pointer dereference in `StringNGrams`\n        ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n    *   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n        ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n    *   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n        ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n    *   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n        ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n    *   Fixes a division by 0 in `QuantizedBiasAdd`\n        ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n    *   Fixes a heap out of bounds in\n        `QuantizedBatchNormWithGlobalNormalization`\n        ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n    *   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n        ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n    *   Fixes a division by 0 in `QuantizedAdd`\n        ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n    *   Fixes a division by 0 in `FractionalAvgPool`\n        ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n    *   Fixes an OOB read in `MatrixTriangularSolve`\n        ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n    *   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n        ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n    *   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n        ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n    *   Fixes a division by 0 in `DenseCountSparseOutput`\n        ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n    *   Fixes a division by 0 in `FusedBatchNorm`\n        ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n    *   Fixes a division by 0 in `SparseMatMul`\n        ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n    *   Fixes a division by 0 in `Reverse`\n        ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n    *   Fixes a heap buffer overflow in `SparseSplit`\n        ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n    *   Fixes a heap OOB access in unicode ops\n        ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n    *   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n        ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n    *   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n        ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n    *   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n        ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n    *   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n        ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n    *   Fixes a null pointer dereference in `EditDistance`\n        ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n    *   Fixes a null pointer dereference in `SparseFillEmptyRows`\n        ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n    *   Fixes a heap OOB access in `Dilation2DBackpropInput`\n        ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n    *   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n        ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n    *   Fixes a set of vulnerabilities caused by lack of validation in\n        `SparseDenseCwiseMul`\n        ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n    *   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n        ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n    *   Fixes a heap out of bounds read in `RequantizationRange`\n        ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n    *   Fixes a memory corruption in `DrawBoundingBoxesV2`\n        ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n    *   Fixes a reference binding to nullptr in `SdcaOptimizer`\n        ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n    *   Fixes an overflow and a denial of service in\n        `tf.raw_ops.ReverseSequence`\n        ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n    *   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n        ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n    *   Fixes an undefined behavior in `MaxPool3DGradGrad`\n        ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n    *   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n        ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n    *   Fixes a heap buffer overflow in `AvgPool3DGrad`\n        ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n    *   Fixes an undefined behavior and a `CHECK`-fail in\n        `FractionalMaxPoolGrad`\n        ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n    *   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n        ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n    *   Fixes a heap buffer overflow in `MaxPoolGrad`\n        ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n    *   Fixes a segfault in `CTCBeamSearchDecoder`\n        ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n    *   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n        ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n    *   Fixes a `CHECK`-fail due to integer overflow\n        ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n    *   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n        ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n    *   Fixes a division by zero in padding computation in TFLite\n        ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n    *   Fixes a division by zero in optimized pooling implementations in TFLite\n        ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n    *   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n        ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n    *   Fixes a division by zero in TFLite's implementation of `GatherNd`\n        ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n    *   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n        ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n    *   Fixes a heap OOB read in TFLite's implementation of `Minimum` or\n        `Maximum`\n        ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n    *   Fixes a null pointer dereference in TFLite's `Reshape` operator\n        ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n    *   Fixes a stack overflow due to looping TFLite subgraph\n        ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n    *   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n        ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n    *   Fixes a division by zero in TFLite's convolution code\n        ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n    *   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n        ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n    *   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n        ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n    *   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n        ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n    *   Fixes a division by zero in TFLite's implementation of `SVDF`\n        ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n    *   Fixes a division by zero in TFLite's implementation of `Split`\n        ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n    *   Fixes a division by zero in TFLite's implementation of `OneHot`\n        ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n    *   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n        ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n    *   Fixes a division by zero in TFLite's implementation of hashtable lookup\n        ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n    *   Fixes a integer overflow in TFLite concatentation\n        ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n    *   Fixes a integer overflow in TFLite memory allocation\n        ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n    *   Fixes a heap OOB write in TFLite\n        ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n    *   Fixes a heap OOB read in TFLite\n        ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n    *   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n        ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n    *   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n        ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n    *   Fixes vulnerabilities caused by incomplete validation in\n        `SparseSparseMinimum`\n        ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n    *   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n        ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n    *   Fixes vulnerabilities caused by invalid validation in\n        `QuantizeAndDequantizeV2`\n        ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n    *   Fixes a heap buffer overflow in `BandedTriangularSolve`\n        ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n    *   Fixes vulnerabilities caused by incomplete validation in\n        `tf.raw_ops.CTCLoss`\n        ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n    *   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n        ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n    *   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n        ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n    *   Fixes a null dereference in Grappler's `TrySimplify`\n        ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n    *   Fixes a crash in `tf.transpose` with complex inputs\n        ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n    *   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n        ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n    *   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n        ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n    *   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n        ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n    *   Updates `curl` to `7.76.0` to handle\n        [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n        [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n        [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n        [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n        [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n        and\n        [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n*   Other\n\n    *   Added `show_debug_info` to `mlir.convert_graph_def` and\n        `mlir.convert_function`.\n    *   Added\n        [Arm Compute Library (ACL)](https://github.com/ARM-software/ComputeLibrary)\n        support to `--config=mkl_aarch64` build.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n8bitmp3, Aaron S. Mondal, Abhilash Mahendrakar, Abhinav Upadhyay, Abhishek\nKulkarni, Abolfazl Shahbazi, Adam Hillier, Aditya Kane, Ag Ramesh, ahmedsabie,\nAlbert Villanova Del Moral, Aleksey Vitebskiy, Alex Hoffman, Alexander Bayandin,\nAlfie Edwards, Aman Kishore, Amogh Joshi, andreABbauer, Andrew Goodbody, Andrzej\nPomirski, Artemiy Ryabinkov, Ashish Jha, ather, Ayan Moitra, Bairen Yi, Bart\nRibbers, Bas Aarts, Behzad Abghari, Ben Arnao, Ben Barsdell, Benjamin Klimczak,\nbhack, Brendan Collins, Can Wang, Cheng Ren, Chris Leary, Chris Olivier, Clemens\nGiuliani, Cloud Han, Corey Cole, Cui, Yifeng, Cuong V. Nguyen, Daniel Moore,\nDawid Wojciechowski, Ddavis-2015, Dean Wyatte, Denisa Roberts, dependabot[bot],\nDmitry Volodin, Dominic Jack, Duncan Riach, dushuai, Elena Zhelezina, Eli\nOsherovich, Erik Smistad, ewsn1593, Felix Fent, fo40225, Fran\u00e7ois Chollet,\nFrederic Bastien, Freedom\" Koan-Sin Tan, fsx950223, ganand1, gbaned, Georgiy\nManuilov, gerbauz, Guillaume Klein, Guozhong Zhuang, Harry Slatyer, Harsh188,\nhenri, Henri Woodcock, Hiran Sarkar, Hollow Man, H\u00e5kon Sandsmark, I Wayan\nDharmana, icysapphire, Ikko Ashimine, Jab Hofmeier, Jack Hessel, Jacob Valdez,\nJakub Jatczak, James Bernardi, Jared Smolens, Jason Zaman, jedlimlx, Jenny\nPlunkett, Jens Elofsson, Jerry Shih, jgehw, Jia Fu Low, Jim Fisher, jpodivin,\nJulien Stephan, Jungsub Lim, Junha Park, Junhyuk So, justkw, Kaixi Hou,\nkashyapraval, Kasra Bigdeli, Kazuaki Ishizaki, Keith Mok, Kevin Cheng, kopytjuk,\nKristian Hartikainen, ksood12345, Kulin Seth, kushanam, latyas, Lequn Chen,\nLeslie-Fang, Long M. L\u01b0u, Lukas Geiger, machineko, Mahmoud Abuzaina, Manish, Mao\nYunfei, Maozhou, Ge, Marcin Juszkiewicz, Marcin Owsiany, Marconi Jiang, Marcos\nPereira, Maria Romanenko Vexlard, Maria Vexlard, Marius Brehler, marload, Martin\nKubov\u010d\u00edk, Matej, Mateusz Holenko, Maxiwell S. Garcia, Mazhar, mazharul,\nmbhuiyan, mdfaijul, Michael Gielda, Michael Kuchnik, Michal Szutenberg, Mikhail\nStepanov, Milan Straka, Mitchel Humpherys, Mohamed Moselhy, Mohamed Nour\nAbouelseoud, M\u00e5ns Bermell, M\u00e5ns Nilsson, Nathan Luehr, Nico Jahn, Niroop\nAmmbashankar, Oceania2018, Omri Steiner, Orivej Desh, Oskar Flordal, oujiafan,\nPatrik Laurell, Paul B. Isaac'S, Paul Klinger, Pawel Piskorski, Pedro Marques,\nPhat Tran, Piotr Zierhoffer, piyushdatta, Pnikam-Cad, Prashant Kumar, Prateek\nGupta, PratsBhatt, Pravin Karandikar, qqq.jq, QQ\u55b5, Quintin, Rama Ketineni,\nravikyram, Rehan Guha, rhdong, rmothukuru, Roger Cheng, Rohit Santhanam, rposts,\nRsanthanam-Amd, rsun, Rsun-Bdti, Ryan Kuester, ryanking13, Saduf2019, Sami Kama,\nSamuel Marks, Scott Tseng, Sean Moriarity, Sergey Popov, Sergii Khomenko, Sheng,\nYang, shwetaoj, Sidong-Wei, Simon Maurer, Simrit Kaur, Srini511, Srinivasan\nNarayanamoorthy, Stephan, Stephen Matthews, Sungmann Cho, Sunoru, Suraj Sudhir,\nSuraj Upadhyay, Taebum Kim, Takayoshi Koizumi, Tamas Bela Feher, Teng Lu,\nThibaut Goetghebuer-Planchon, Tomwildenhain-Microsoft, Tony, Traun Leyden, Trent\nLo, TVLIgnacy, Tzu-Wei Sung, vaibhav, Vignesh Kothapalli, Vikram Dattu,\nviktprog, Vinayaka Bandishti, Vincent Abriou, Vishakha Agrawal, Vivek Panyam,\nVladimir Silyaev, V\u00f5 V\u0103n Ngh\u0129a, wamuir, Wang, Yanzhang, wangsiyu, Waqar Hameed,\nwxinix, Xiao Yang, xiaohong1031, Xiaoming (Jason) Cui, Xinan Jiang, Yair\nEhrenwald, Yajush Vyas, Yasir Modak, Yimei Sun, Yong Tang, Yosshi999,\nyoushenmebutuo, yqtianust, Yuan Tang, yuanbopeng, Yuriy Chernyshov, Yuta\nFukasawa, Zachary Deane-Mayer, Zeno Gantner, Zhoulong Jiang, zhuyie, zilinzhu,\n\u5f6d\u9707\u4e1c\n\n# Release 2.4.1\n\n* This release removes the AVX2 requirement from TF 2.4.0.\n\n# Release 2.3.2\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Solves an OOM issue on TPUs when XLA contexts use fused average updates\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n\n# Release 2.2.2\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Prevents memory leaks in loading `SavedModel`s that import functions\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n\n# Release 2.1.3\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n* Newer ROCm versions are supported on the 2.1 branch.\n\n# Release 2.0.4\n\nNote that this is the last patch release for the TensorFlow 2.0.x series.\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n\n# Release 1.15.5\n\nNote that this is the last patch release for the TensorFlow 1.x series.\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n\n# Release 2.4.0\n\n ## Major Features and Improvements\n\n* `tf.distribute` introduces experimental support for asynchronous training of\n  models via the [`tf.distribute.experimental.ParameterServerStrategy`]\n  (https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy)\n  API. Please see the [tutorial](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n  to learn more.\n\n* [`MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy)\n  is now a stable API and is no longer considered experimental. Some of the\n  major improvements involve handling peer failure and many bug fixes. Please\n  check out the detailed tutorial on [Multi-worker training with Keras]\n  (https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras).\n\n* Introduces experimental support for a new module named [`tf.experimental.numpy`]\n  (https://www.tensorflow.org/api_docs/python/tf/experimental/numpy) which is a\n  NumPy-compatible API for writing TF programs. See the [detailed guide]\n  (https://www.tensorflow.org/guide/tf_numpy) to learn more. Additional details below.\n\n* Adds Support for\n  [TensorFloat-32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)\n  on Ampere based GPUs. TensorFloat-32, or TF32 for short, is a math mode for\n  NVIDIA Ampere based GPUs and is enabled by default.\n\n* A major refactoring of the internals of the Keras Functional API has been\n  completed, that should improve the reliability, stability, and performance of\n  constructing Functional models.\n\n* Keras mixed precision API [`tf.keras.mixed_precision`]\n  (https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision?version=nightly)\n  is no longer experimental and allows the use of 16-bit floating point formats\n  during training, improving performance by up to 3x on GPUs and 60% on TPUs.\n  Please see below for additional details.\n\n* TensorFlow Profiler now supports profiling `MultiWorkerMirroredStrategy` and\n  tracing multiple workers using the [sampling mode API]\n  (https://www.tensorflow.org/guide/profiler#profiling_apis).\n\n* TFLite Profiler for Android is available. See the detailed [guide]\n  (https://www.tensorflow.org/lite/performance/measurement#trace_tensorflow_lite_internals_in_android)\n  to learn more.\n\n* TensorFlow pip packages are now built with CUDA11 and cuDNN 8.0.2.\n\n## Breaking Changes\n\n* TF Core:\n  * Certain float32 ops run in lower precision on Ampere based GPUs, including\n  matmuls and convolutions, due to the use of [TensorFloat-32]\n  (https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/).\n  Specifically, inputs to such ops are rounded from 23 bits of precision to 10\n  bits of precision. This is unlikely to cause issues in practice for deep learning\n  models. In some cases, TensorFloat-32 is also used for complex64 ops.\n  TensorFloat-32 can be disabled by running `tf.config.experimental.enable_tensor_float_32_execution(False)`.\n  * The byte layout for string tensors across the C-API has been updated to match\n  TF Core/C++; i.e., a contiguous array of `tensorflow::tstring`/`TF_TString`s.\n  * C-API functions `TF_StringDecode`, `TF_StringEncode`, and `TF_StringEncodedSize`\n  are no longer relevant and have been removed; see `core/platform/ctstring.h` for\n  string access/modification in C.\n  * `tensorflow.python`, `tensorflow.core` and `tensorflow.compiler` modules are\n  now hidden. These modules are not part of TensorFlow public API.\n  * `tf.raw_ops.Max` and `tf.raw_ops.Min` no longer accept inputs of type\n  `tf.complex64` or `tf.complex128`, because the behavior of these ops is not\n  well defined for complex types.\n  * XLA:CPU and XLA:GPU devices are no longer registered by default. Use\n  `TF_XLA_FLAGS=--tf_xla_enable_xla_devices` if you really need them, but this\n  flag will eventually be removed in subsequent releases.\n\n* `tf.keras`:\n  * The `steps_per_execution` argument in `model.compile()` is no longer experimental;\n  if you were passing `experimental_steps_per_execution`, rename it to\n  `steps_per_execution` in your code. This argument controls the number of batches\n  to run during each `tf.function` call when calling `model.fit()`. Running multiple\n  batches inside a single `tf.function` call can greatly improve performance on\n  TPUs or small models with a large Python overhead.\n  * A **major refactoring** of the internals of the Keras Functional API may affect code that\n  is relying on certain internal details:\n    * Code that uses `isinstance(x, tf.Tensor)` instead of `tf.is_tensor` when\n  checking Keras symbolic inputs/outputs should switch to using `tf.is_tensor`.\n    * Code that is overly dependent on the exact names attached to symbolic tensors\n  (e.g. assumes there will be \":0\" at the end of the inputs, treats names as\n  unique identifiers instead of using `tensor.ref()`, etc.) may break.\n    * Code that uses full path for `get_concrete_function` to trace Keras symbolic\n  inputs directly should switch to building matching `tf.TensorSpec`s directly and\n  tracing the `TensorSpec` objects.\n    * Code that relies on the exact number and names of the op layers that TensorFlow\n  operations  were converted into may have changed.\n    * Code that uses `tf.map_fn`/`tf.cond`/`tf.while_loop`/control flow as op layers\n  and  happens to work before TF 2.4. These will explicitly be unsupported now.\n  Converting these ops to Functional API op layers was unreliable before TF 2.4,\n  and prone to erroring incomprehensibly  or being silently buggy.\n    * Code that directly asserts on a Keras symbolic value in cases where ops\n  like `tf.rank` used to  return a static or symbolic value depending on if the\n  input had a fully static shape or not. Now these ops always return symbolic values.\n    * Code already susceptible to leaking tensors outside of graphs becomes slightly\n  more likely to do so now.\n    * Code that tries directly getting gradients with respect to symbolic Keras\n  inputs/outputs. Use `GradientTape` on the actual Tensors passed to the already-constructed\n  model instead.\n    * Code that requires very tricky shape manipulation via converted op layers\n  in order to work, where the Keras symbolic shape inference proves insufficient.\n    * Code that tries manually walking a `tf.keras.Model` layer by layer and assumes\n  layers only ever have one positional argument. This assumption doesn't hold\n  true before TF 2.4 either, but is more likely to cause issues now.\n    * Code that manually enters `keras.backend.get_graph()` before building a\n  functional model is no longer needed.\n    * Start enforcing input shape assumptions when calling Functional API Keras\n  models. This may potentially break some users, in case there is a mismatch\n  between the shape used when creating `Input` objects in a Functional model,\n  and the shape of the data passed to that model. You can fix this mismatch by\n  either calling the model with correctly-shaped data, or by relaxing `Input` shape\n  assumptions (note that you can pass shapes with `None` entries for axes that\n  are meant to be dynamic). You can also disable the input checking entirely by\n  setting `model.input_spec = None`.\n  * Several changes have been made to `tf.keras.mixed_precision.experimental`.\n  Note that it is now recommended to use the non-experimental\n  `tf.keras.mixed_precision` API.\n   * `AutoCastVariable.dtype` now refers to the actual variable dtype, not the\n  dtype it will be casted to.\n   * When mixed precision is enabled, `tf.keras.layers.Embedding` now outputs a\n  float16 or bfloat16 tensor instead of a float32 tensor.\n   * The property `tf.keras.mixed_precision.experimental.LossScaleOptimizer.loss_scale`\n  is now a tensor, not a `LossScale` object. This means to get a loss scale\n  of a `LossScaleOptimizer` as a tensor, you must now call `opt.loss_scale`instead of `opt.loss_scale()`.\n   * The property `should_cast_variables` has been removed from `tf.keras.mixed_precision.experimental.Policy`\n   * When passing a `tf.mixed_precision.experimental.DynamicLossScale` to `tf.keras.mixed_precision.experimental.LossScaleOptimizer`,\n  the `DynamicLossScale`'s multiplier must be 2.\n   * When passing a `tf.mixed_precision.experimental.DynamicLossScale` to\n  `tf.keras.mixed_precision.experimental.LossScaleOptimizer`, the weights of\n  the `DynanmicLossScale` are copied into the `LossScaleOptimizer` instead of being reused.\n  This means modifying the weights of the `DynamicLossScale` will no longer affect the weights of the LossScaleOptimizer, and vice versa.\n   * The global policy can no longer be set to a non-floating point policy in `tf.keras.mixed_precision.experimental.set_policy`\n   * In `Layer.call`, `AutoCastVariable`s will no longer be casted within\n  `MirroredStrategy.run` or `ReplicaContext.merge_call`. This is because a thread local\n  variable is used to determine whether `AutoCastVariable`s are casted, and those\n  two functions run with a different thread. Note this only applies if one of\n  these two functions is called within `Layer.call`; if one of those two functions calls `Layer.call`, `AutoCastVariable`s will still be casted.\n\n* `tf.data`:\n  * `tf.data.experimental.service.DispatchServer` now takes a config tuple\n  instead of individual arguments. Usages should be updated to\n  `tf.data.experimental.service.DispatchServer(dispatcher_config)`.\n  * `tf.data.experimental.service.WorkerServer` now takes a config tuple instead\n  of individual arguments. Usages should be updated to  `tf.data.experimental.service.WorkerServer(worker_config)`.\n\n* `tf.distribute`:\n  * Removes `tf.distribute.Strategy.experimental_make_numpy_dataset`. Please use\n  `tf.data.Dataset.from_tensor_slices` instead.\n  * Renames `experimental_hints` in `tf.distribute.StrategyExtended.reduce_to`,\n  `tf.distribute.StrategyExtended.batch_reduce_to`, `tf.distribute.ReplicaContext.all_reduce`\n  to `options`.\n  * Renames `tf.distribute.experimental.CollectiveHints` to `tf.distribute.experimental.CommunicationOptions`.\n  * Renames `tf.distribute.experimental.CollectiveCommunication` to `tf.distribute.experimental.CommunicationImplementation`.\n  * Renames `tf.distribute.Strategy.experimental_distribute_datasets_from_function` to `distribute_datasets_from_function` as it is no longer experimental.\n  * Removes `tf.distribute.Strategy.experimental_run_v2` method, which was deprecated in TF 2.2.\n\n* `tf.lite`:\n  * `tf.quantization.quantize_and_dequantize_v2` has been introduced, which updates the gradient definition for quantization which is outside the range\n     to be 0. To simulate the V1 the behavior of `tf.quantization.quantize_and_dequantize(...)` use\n  `tf.grad_pass_through(tf.quantization.quantize_and_dequantize_v2)(...)`.\n\n* Building TensorFlow:\n  * Windows platform builds: TensorFlow on Windows under MSVC is now built with\n  `--copt=/experimental:preprocessor --host_copt=/experimental:preprocessor`\n  (see `.bazelrc` for more details). Builds including TensorFlow may fail with\n  unexpected syntax errors if these flags are absent. See also\n  [this thread on SIG Build](https://groups.google.com/a/tensorflow.org/g/build/c/LbAw8RILvTg/m/ttnuhYU2BgAJ).\n\n## Known Caveats\n  * `tf.keras.mixed_precision`\n    * When using mixed precision, calling `RMSprop.apply_gradients` or\n  `Nadam.apply_gradients` outside a `tf.function` does not work and will raise\n  the AttributeError \"Tensor.op is meaningless when eager execution is enabled\".\n  See this [issue](https://github.com/tensorflow/tensorflow/issues/45536) for details and a workaround.\n\n## Bug Fixes and Other Changes\n\n### TF Core:\n  * Introduces experimental support for a new module named [`tf.experimental.numpy`]\n  (https://www.tensorflow.org/api_docs/python/tf/experimental/numpy), which is a\n  NumPy-compatible API for writing TF programs. This module provides class\n  `ndarray`, which mimics the `ndarray` class in NumPy, and wraps an immutable\n  `tf.Tensor` under the hood. A subset of NumPy functions (e.g. `numpy.add`) are\n  provided. Their inter-operation with TF facilities is seamless in most cases.\n    See [tensorflow/python/ops/numpy_ops/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/README.md)\n    for details of what operations are supported and what are the differences\n  from NumPy.\n  * `tf.types.experimental.TensorLike` is a new `Union` type that can be used as\n  type annotation for variables representing a Tensor or a value\n    that can be converted to Tensor by `tf.convert_to_tensor`.\n  * Calling ops with a python constants or numpy values is now consistent with\n  tf.convert_to_tensor behavior. This avoids operations like\n    tf.reshape truncating inputs such as from int64 to int32.\n  * Adds `tf.sparse.map_values` to apply a function to the `.value`s of\n  `SparseTensor` arguments.\n  * The Python bitwise operators for `Tensor` (`__and__`, `__or__`, `__xor__` and `__invert__` now support non-`bool`\n  arguments and apply the corresponding bitwise ops. `bool` arguments continue\n  to be supported and dispatch to logical ops. This brings them more in line with\n  Python and NumPy behavior.\n  * Adds `tf.SparseTensor.with_values`. This returns a new SparseTensor with the same sparsity pattern, but with new provided values. It is\n    similar to the `with_values` function of `RaggedTensor`.\n  * Adds `StatelessCase` op, and uses it if none of case branches has stateful ops.\n  * Adds `tf.config.experimental.get_memory_usage` to return total memory usage of the device.\n  * Adds gradients for `RaggedTensorToVariant` and `RaggedTensorFromVariant`.\n  * Improve shape inference of nested function calls by supporting constant\n  folding across Arg nodes which makes more static values available to shape\n  inference functions.\n* `tf.debugging`:\n  * `tf.debugging.assert_shapes()` now works on `SparseTensor`s (Fixes [#36268](https://github.com/tensorflow/tensorflow/issues/36268)).\n* GPU\n  * Adds Support for [TensorFloat-32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)\n  on Ampere based GPUs.TensorFloat-32, or TF32 for short, is a math mode for\n  NVIDIA Ampere based GPUs which causes certain float32 ops, such as matrix\n  multiplications and convolutions, to run much faster on Ampere GPUs but with\n  reduced precision. This reduced precision has not been found to effect\n  convergence quality of deep learning models in practice. TensorFloat-32 is\n  enabled by default, but can be disabled with `tf.config.experimental.enable_tensor_float_32_execution`.\n* `tf.math`:\n  * Adds `tf.math.erfcinv`, the inverse to `tf.math.erfc`.\n* `tf.nn`:\n  *   `tf.nn.max_pool2d` now supports explicit padding.\n* `tf.image`:\n  * Adds deterministic `tf.image.stateless_random_*` functions for each\n  `tf.image.random_*` function. Added a new op `stateless_sample_distorted_bounding_box`\n  which is a deterministic version of `sample_distorted_bounding_box` op.\n  Given the same seed, these stateless functions/ops produce the same results\n  independent of how many times the function is called, and independent of global seed settings.\n  * Adds deterministic `tf.image.resize` backprop CUDA kernels for\n  `method=ResizeMethod.BILINEAR` (the default method). Enable by setting the environment\n  variable `TF_DETERMINISTIC_OPS` to `\"true\"` or `\"1\"`.\n* `tf.print`:\n  * Bug fix in `tf.print()` with `OrderedDict` where if an `OrderedDict`\n  didn't have the keys sorted, the keys and values were not being printed\n    in accordance with their correct mapping.\n* `tf.train.Checkpoint`:\n  * Now accepts a `root` argument in the initialization, which generates a\n  checkpoint with a root object. This allows users to create a `Checkpoint`\n  object that     is compatible with Keras `model.save_weights()` and\n  `model.load_weights`. The checkpoint is also compatible with the checkpoint\n  saved in the `variables/` folder in the SavedModel.\n  * When restoring, `save_path` can be a path to a SavedModel. The function will\n  automatically find the checkpoint in the SavedModel.\n\n### `tf.data`:\n  * Adds new `tf.data.experimental.service.register_dataset` and\n  `tf.data.experimental.service.from_dataset_id` APIs to enable one process to\n  register a dataset with the tf.data service, and another process to consume\n  data from the dataset.\n  * Adds support for dispatcher fault tolerance. To enable fault tolerance,\n  configure a `work_dir` when running your dispatcher server and set\n  `dispatcher_fault_tolerance=True`. The dispatcher will store its state to\n  `work_dir`, so that on restart it can continue from its previous state after restart.\n  * Adds support for sharing dataset graphs via shared filesystem instead of\n  over RPC. This reduces load on the dispatcher, improving performance\n    of distributing datasets. For this to work, the dispatcher's `work_dir`\n  must be accessible from workers. If the worker fails to read from the `work_dir`,\n  it falls back to using RPC for dataset graph transfer.\n  * Adds support for a new \"distributed_epoch\" processing mode.\n  This processing mode distributes a dataset across all tf.data workers,\n    instead of having each worker process the full dataset. See\n  [the tf.data service docs](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service#understand_processing_mode)\n  to learn more.\n  * Adds optional `exclude_cols` parameter to CsvDataset. This parameter is the\n  complement of `select_cols`; at most one of these should be specified.\n  * We have implemented an optimization which reorders data-discarding\n  transformations such as `take` and `shard` to happen earlier in the dataset\n  when it is safe to do so. The optimization can be disabled via the\n  `experimental_optimization.reorder_data_discarding_ops` dataset option.\n  * `tf.data.Options` were previously immutable and can now be overridden.\n  * `tf.data.Dataset.from_generator` now supports Ragged and Sparse tensors with\n  a new `output_signature` argument, which allows `from_generator` to produce any\n  type describable by a `tf.TypeSpec`.\n  * `tf.data.experimental.AUTOTUNE` is now available in the core API as `tf.data.AUTOTUNE`.\n\n### `tf.distribute`:\n  * Introduces experimental support for asynchronous training of models via\n  `tf.distribute.experimental.ParameterServerStrategy`:\n    * Replaces the existing `tf.distribute.experimental.ParameterServerStrategy`\n  symbol with a new class that is for parameter server training in TF2. Usage of\n  the old symbol, usually with Estimator API, should be **replaced** with\n  [`tf.compat.v1.distribute.experimental.ParameterServerStrategy`].\n    * Added `tf.distribute.experimental.coordinator.*` namespace, including the\n  main API `ClusterCoordinator` for coordinating the training cluster, the\n  related data structure `RemoteValue` and `PerWorkerValue`.\n  * `MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy)\n  is now a stable API and is no longer considered experimental. Some of the major\n  improvements involve handling peer failure and many bug fixes. Please check out\n  the detailed tutorial on [Multi-worer training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras).\n  * Adds `tf.distribute.Strategy.gather` and `tf.distribute.ReplicaContext.all_gather`\n  APIs to support gathering dense distributed values.\n  * Fixes various issues with saving a distributed model.\n\n### `tf.keras`:\n  * Improvements from the Functional API refactoring:\n    * Functional model construction does not need to maintain a global workspace\n  graph, removing memory leaks especially when building many models or very large models.\n    * Functional model construction should be ~8-10% faster on average.\n    * Functional models can now contain non-symbolic values in their call inputs\n  inside of the first positional argument.\n    * Several classes of TF ops that were not reliably converted to Keras layers\n  during functional API construction should now work, e.g.`tf.image.ssim_multiscale`\n    * Error messages when Functional API construction goes wrong (and when ops cannot be converted to Keras layers automatically) should be\n      clearer and easier to understand.\n  * `Optimizer.minimize` can now accept a loss `Tensor` and a `GradientTape`\n  as an alternative to accepting a `callable` loss.\n  * Adds `beta` hyperparameter to [FTRL](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl)\n  optimizer classes (Keras and others) to match [FTRL paper](https://research.google.com/pubs/archive/41159.pdf).\n  * `Optimizer.__init__` now accepts a `gradient_aggregator` to allow for customization\n  of how gradients are aggregated across devices, as well as `gradients_transformers`\n  to allow for custom gradient transformations (such as gradient clipping).\n  * Improvements to Keras preprocessing layers:\n    * TextVectorization can now accept a vocabulary list or file as an init arg.\n    * Normalization can now accept mean and variance values as init args.\n  * In `Attention` and `AdditiveAttention` layers, the `call()` method now accepts a `return_attention_scores` argument. When set to\n    True, the layer returns the attention scores as an additional output argument.\n  * Adds `tf.metrics.log_cosh` and `tf.metrics.logcosh` API entrypoints with the\n  same implementation as their `tf.losses` equivalent.\n  * For Keras model, the individual call of `Model.evaluate` uses no cached data\n  for evaluation, while `Model.fit` uses cached data when `validation_data` arg\n  is provided for better performance.\n  * Adds a `save_traces` argument to `model.save`/ `tf.keras.models.save_model`\n  which determines whether the SavedModel format stores the Keras model/layer call\n  functions. The traced functions allow Keras to revive custom models and layers\n  without the original class definition, but if this isn't required the tracing\n  can be disabled with the added option.\n  * The `tf.keras.mixed_precision` API is now non-experimental.\n  The non-experimental API differs from the experimental API in several ways.\n    * `tf.keras.mixed_precision.Policy` no longer takes in a `tf.mixed_precision.\n  experimental.LossScale` in the constructor, and no longer has a `LossScale`\n  associated with it. Instead, `Model.compile` will automatically wrap the optimizer\n  with a `LossScaleOptimizer` using dynamic loss scaling if `Policy.name`\n  is \"mixed_float16\".\n    * `tf.keras.mixed_precision.LossScaleOptimizer`'s constructor takes in different\n  arguments. In particular, it no longer takes in a `LossScale`, and there is\n  no longer a `LossScale` associated with the `LossScaleOptimizer`. Instead,\n  `LossScaleOptimizer` directly implements fixed or dynamic loss scaling. See the\n  documentation of [`tf.keras.mixed_precision.experimental.LossScaleOptimizer`]\n  (https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer?version=nightly)\n  for details on the differences between the experimental `LossScaleOptimizer`\n  and the new non-experimental `LossScaleOptimizer`.\n    * `tf.mixed_precision.experimental.LossScale` and its subclasses are\n  deprecated, as all of its functionality now exists within `tf.keras.mixed_precision.LossScaleOptimizer`\n\n### `tf.lite`:\n  * `TFLiteConverter`:\n    * Support optional flags `inference_input_type` and `inference_output_type`\n  for full integer quantized models. This allows users to modify the model input\n  and output type to integer types (`tf.int8`, `tf.uint8`) instead of defaulting\n  to float type (`tf.float32`).\n  * NNAPI\n    * Adds NNAPI Delegation support for requantization use cases by converting\n  the operation into a dequantize-quantize pair.\n    * Removes deprecated `Interpreter.setUseNNAPI(boolean)` Java API. Use\n  `Interpreter.Options.setUseNNAPI` instead.\n    * Deprecates `Interpreter::UseNNAPI(bool)` C++ API. Use `NnApiDelegate()`\n  and related delegate configuration methods directly.\n    * Deprecates `Interpreter::SetAllowFp16PrecisionForFp32(bool)` C++ API.\n  Prefer controlling this via delegate options, e.g. `tflite::StatefulNnApiDelegate::Options::allow_fp16'\n  or `TfLiteGpuDelegateOptionsV2::is_precision_loss_allowed`.\n  * GPU\n    * GPU acceleration now supports quantized models by default\n  * `DynamicBuffer::AddJoinedString()` will now add a separator if the first string to be joined is empty.\n  *  Adds support for cumulative sum (cumsum), both as builtin op and MLIR conversion.\n\n### `TensorRT`\n  * Issues a warning when the `session_config` parameter for the TF1 converter\n  is used or the `rewrite_config_template` field in the TF2 converter parameter\n  object is used.\n\n### TPU Enhancements:\n  * Adds support for the `beta` parameter of the FTRL optimizer for TPU\n  embeddings. Users of other TensorFlow platforms can implement equivalent\n  behavior by adjusting the `l2` parameter.\n\n### XLA Support:\n  * xla.experimental.compile is deprecated, use `tf.function(experimental_compile=True)` instead.\n  * Adds `tf.function.experimental_get_compiler_ir` which returns compiler IR\n  (currently 'hlo' and 'optimized_hlo') for given input for given function.\n\n### Security:\n  * Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`,\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n  * Fixes three vulnerabilities in conversion to DLPack format\n    * [CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n    * [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n    * [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193)\n  * Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n    * [CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n    * [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195)\n  * Fixes several vulnerabilities in `RaggedCountSparseOutput` and `SparseCountSparseOutput` operations\n    * [CVE-2020-15196](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15196),\n    * [CVE-2020-15197](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15197),\n    * [CVE-2020-15198](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15198),\n    * [CVE-2020-15199](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15199),\n    * [CVE-2020-15200](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15200),\n    * [CVE-2020-15201](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15201)\n  * Fixes an integer truncation vulnerability in code using the work sharder API,\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n  * Fixes a format string vulnerability in `tf.strings.as_string`,\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n  * Fixes segfault raised by calling session-only ops in eager mode,\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n  * Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`,\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n  * Fixes segfaults caused by incomplete `SavedModel` validation,\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n  * Fixes a data corruption due to a bug in negative indexing support in TFLite,\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n  * Fixes a data corruption due to dimension mismatch in TFLite,\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n  * Fixes several vulnerabilities in TFLite saved model format\n    * [CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n    * [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n    * [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211)\n  * Fixes several vulnerabilities in TFLite implementation of segment sum\n    * [CVE-2020-15212](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212),\n    * [CVE-2020-15213](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213),\n    * [CVE-2020-15214](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214)\n  * Fixes a segfault in `tf.quantization.quantize_and_dequantize`,\n  ([CVE-2020-15265](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15265))\n  * Fixes an undefined behavior float cast causing a crash,\n  ([CVE-2020-15266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15266))\n  * Fixes a lack of validation in `tf.raw_ops.DataFormatVecPermute` and\n  `tf.raw_ops.DataFormatDimMap` which can cause uninitialized memory access,\n  read outside bounds of arrays, data corruption and segmentation faults\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n  * Fixes a crash caused by writing to read only memory region\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268))\n  * Fixes a heap out of bounds access in filesystem globbing implementation\n  ([CVE-2020-26269](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26269))\n\n### Other:\n  * We have replaced uses of \"whitelist\" and \"blacklist\" with \"allowlist\" and\n  \"denylist\" where possible. Please see [this list](https://developers.google.com/style/word-list#blacklist) for more context.\n  * Adds `tf.config.experimental.mlir_bridge_rollout` which will help us rollout the new MLIR TPU bridge.\n  * Adds `tf.experimental.register_filesystem_plugin` to load modular filesystem plugins from Python\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google as well as the following external contributors:\n\n8bitmp3, aaa.jq, Abhineet Choudhary, Abolfazl Shahbazi, acxz, Adam Hillier, Adrian Garcia Badaracco, Ag Ramesh, ahmedsabie, Alan Anderson, Alexander Grund, Alexandre Lissy, Alexey Ivanov, Amedeo Cavallo, anencore94, Aniket Kumar Singh, Anthony Platanios, Ashwin Phadke, Balint Cristian, Basit Ayantunde, bbbboom, Ben Barsdell, Benjamin Chetioui, Benjamin Peterson, bhack, Bhanu Prakash Bandaru Venkata, Biagio Montaruli, Brent M. Spell, bubblebooy, bzhao, cfRod, Cheng Chen, Cheng(Kit) Chen, Chris Tessum, Christian, chuanqiw, codeadmin_peritiae, COTASPAR, CuiYifeng, danielknobe, danielyou0230, dannyfriar, daria, DarrenZhang01, Denisa Roberts, dependabot[bot], Deven Desai, Dmitry Volodin, Dmitry Zakharov, drebain, Duncan Riach, Eduard Feicho, Ehsan Toosi, Elena Zhelezina, emlaprise2358, Eugene Kuznetsov, Evaderan-Lab, Evgeniy Polyakov, Fausto Morales, Felix Johnny, fo40225, Frederic Bastien, Fredrik Knutsson, fsx950223, Gaurav Singh, Gauri1 Deshpande, George Grzegorz Pawelczak, gerbauz, Gianluca Baratti, Giorgio Arena, Gmc2, Guozhong Zhuang, Hannes Achleitner, Harirai, HarisWang, Harsh188, hedgehog91, Hemal Mamtora, Hideto Ueno, Hugh Ku, Ian Beauregard, Ilya Persky, jacco, Jakub Ber\u00e1nek, Jan Jongboom, Javier Montalt Tordera, Jens Elofsson, Jerry Shih, jerryyin, jgehw, Jinjing Zhou, jma, jmsmdy, Johan Nordstr\u00f6m, John Poole, Jonah Kohn, Jonathan Dekhtiar, jpodivin, Jung Daun, Kai Katsumata, Kaixi Hou, Kamil Rakoczy, Kaustubh Maske Patil, Kazuaki Ishizaki, Kedar Sovani, Koan-Sin Tan, Koki Ibukuro, Krzysztof Laskowski, Kushagra Sharma, Kushan Ahmadian, Lakshay Tokas, Leicong Li, levinxo, Lukas Geiger, Maderator, Mahmoud Abuzaina, Mao Yunfei, Marius Brehler, markf, Martin Hwasser, Martin Kubov\u010d\u00edk, Matt Conley, Matthias, mazharul, mdfaijul, Michael137, MichelBr, Mikhail Startsev, Milan Straka, Ml-0, Myung-Hyun Kim, M\u00e5ns Nilsson, Nathan Luehr, ngc92, nikochiko, Niranjan Hasabnis, nyagato_00, Oceania2018, Oleg Guba, Ongun Kanat, OscarVanL, Patrik Laurell, Paul Tanger, Peter Sobot, Phil Pearl, PlusPlusUltra, Poedator, Prasad Nikam, Rahul-Kamat, Rajeshwar Reddy T, redwrasse, Rickard, Robert Szczepanski, Rohan Lekhwani, Sam Holt, Sami Kama, Samuel Holt, Sandeep Giri, sboshin, Sean Settle, settle, Sharada Shiddibhavi, Shawn Presser, ShengYang1, Shi,Guangyong, Shuxiang Gao, Sicong Li, Sidong-Wei, Srihari Humbarwadi, Srinivasan Narayanamoorthy, Steenu Johnson, Steven Clarkson, stjohnso98, Tamas Bela Feher, Tamas Nyiri, Tarandeep Singh, Teng Lu, Thibaut Goetghebuer-Planchon, Tim Bradley, Tomasz Strejczek, Tongzhou Wang, Torsten Rudolf, Trent Lo, Ty Mick, Tzu-Wei Sung, Varghese, Jojimon, Vignesh Kothapalli, Vishakha Agrawal, Vividha, Vladimir Menshakov, Vladimir Silyaev, VoVAllen, V\u00f5 V\u0103n Ngh\u0129a, wondertx, xiaohong1031, Xiaoming (Jason) Cui, Xinan Jiang, Yair Ehrenwald, Yasir Modak, Yasuhiro Matsumoto, Yimei Sun, Yiwen Li, Yixing, Yoav Ramon, Yong Tang, Yong Wu, yuanbopeng, Yunmo Koo, Zhangqiang, Zhou Peng, ZhuBaohe, zilinzhu, zmx\n\n\n# Release 2.3.1\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes several vulnerabilities in `RaggedCountSparseOutput` and\n  `SparseCountSparseOutput` operations\n  ([CVE-2020-15196](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15196),\n  [CVE-2020-15197](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15197),\n  [CVE-2020-15198](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15198),\n  [CVE-2020-15199](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15199),\n  [CVE-2020-15200](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15200),\n  [CVE-2020-15201](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15201))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Fixes several vulnerabilities in TFLite implementation of segment sum\n  ([CVE-2020-15212](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212),\n  [CVE-2020-15213](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213),\n  [CVE-2020-15214](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Fixes deprecated usage of `collections` API\n* Removes `scipy` dependency from `setup.py` since TensorFlow does not need it\n  to install the pip package\n\n\n# Release 2.2.1\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Fixes several vulnerabilities in TFLite implementation of segment sum\n  ([CVE-2020-15212](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212),\n  [CVE-2020-15213](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213),\n  [CVE-2020-15214](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327),\n  [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655),\n  [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656),\n  [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434),\n  [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435),\n  [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630),\n  [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631),\n  [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871),\n  and\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Fixes deprecated usage of `collections` API\n* Removes `scipy` dependency from `setup.py` since TensorFlow does not need it\n  to install the pip package\n\n\n# Release 2.1.2\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327),\n  [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655),\n  [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656),\n  [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434),\n  [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435),\n  [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630),\n  [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631),\n  [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871),\n  and\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Removes `scipy` dependency from `setup.py` since TensorFlow does not need it\n  to install the pip package\n* Switches ROCM builds to use ROCM 3.7\n\n\n# Release 2.0.3\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327),\n  [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655),\n  [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656),\n  [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434),\n  [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435),\n  [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630),\n  [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631),\n  [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871),\n  and\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Pins `numpy` to 1.18.5 to prevent ABI breakage when compiling code that uses\n  both NumPy and TensorFlow headers.\n\n\n# Release 1.15.4\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327),\n  [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655),\n  [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656),\n  [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434),\n  [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435),\n  [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630),\n  [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631),\n  [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871),\n  and\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Fixes #41630 by including `max_seq_length` in CuDNN descriptor cache key\n* Pins `numpy` to 1.18.5 to prevent ABI breakage when compiling code that uses\n  both NumPy and TensorFlow headers.\n\n\n# Release 2.3.0\n\n## Major Features and Improvements\n\n*   `tf.data` adds two new mechanisms to solve input pipeline bottlenecks and\n    save resources:\n\n    *   [snapshot](https://www.tensorflow.org/api_docs/python/tf/data/experimental/snapshot)\n    *   [tf.data service](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service).\n\n    In addition checkout the detailed\n    [guide](https://www.tensorflow.org/guide/data_performance_analysis) for\n    analyzing input pipeline performance with TF Profiler.\n\n*   [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)\n    is now a stable API and no longer considered experimental for TensorFlow.\n    (earlier `tf.distribute.experimental.TPUStrategy`).\n\n*   [TF Profiler](https://www.tensorflow.org/guide/profiler) introduces two new\n    tools: a memory profiler to visualize your model\u2019s memory usage over time\n    and a [python tracer](https://www.tensorflow.org/guide/profiler#events)\n    which allows you to trace python function calls in your model. Usability\n    improvements include better diagnostic messages and\n    [profile options](https://tensorflow.org/guide/profiler#collect_performance_data)\n    to customize the host and device trace verbosity level.\n\n*   Introduces experimental support for Keras Preprocessing Layers API\n    ([`tf.keras.layers.experimental.preprocessing.*`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing?version=nightly))\n    to handle data preprocessing operations, with support for composite tensor\n    inputs. Please see below for additional details on these layers.\n\n*   TFLite now properly supports dynamic shapes during conversion and inference.\n    We\u2019ve also added opt-in support on Android and iOS for\n    [XNNPACK](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack),\n    a highly optimized set of CPU kernels, as well as opt-in support for\n    [executing quantized models on the GPU](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/gpu_advanced.md#running-quantized-models-experimental).\n\n*   Libtensorflow packages are available in GCS starting this release. We have\n    also started to\n    [release a nightly version of these packages](https://github.com/tensorflow/tensorflow#official-builds).\n\n*   The experimental Python API\n    [`tf.debugging.experimental.enable_dump_debug_info()`](https://www.tensorflow.org/api_docs/python/tf/debugging/experimental/enable_dump_debug_info)\n    now allows you to instrument a TensorFlow program and dump debugging\n    information to a directory on the file system. The directory can be read and\n    visualized by a new interactive dashboard in TensorBoard 2.3 called\n    [Debugger V2](https://www.tensorflow.org/tensorboard/debugger_v2), which\n    reveals the details of the TensorFlow program including graph structures,\n    history of op executions at the Python (eager) and intra-graph levels, the\n    runtime dtype, shape, and numerical composition of tensors, as well as their\n    code locations.\n\n## Breaking Changes\n\n*   Increases the **minimum bazel version** required to build TF to **3.1.0**.\n*   `tf.data`\n    *   Makes the following (breaking) changes to the `tf.data`.\n    *   C++ API: - `IteratorBase::RestoreInternal`,\n        `IteratorBase::SaveInternal`, and `DatasetBase::CheckExternalState`\n        become pure-virtual and subclasses are now expected to provide an\n        implementation.\n    *   The deprecated `DatasetBase::IsStateful` method is removed in favor of\n        `DatasetBase::CheckExternalState`.\n    *   Deprecated overrides of `DatasetBase::MakeIterator` and\n        `MakeIteratorFromInputElement` are removed.\n    *   The signature of `tensorflow::data::IteratorBase::SaveInternal` and\n        `tensorflow::data::IteratorBase::SaveInput` has been extended with\n        `SerializationContext` argument to enable overriding the default policy\n        for the handling external state during iterator checkpointing. This is\n        not a backwards compatible change and all subclasses of `IteratorBase`\n        *need to be updated* accordingly.\n*   `tf.keras`\n    *   Add a new `BackupAndRestore` callback for handling distributed training\n        failures & restarts. Please take a look at this\n        [tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)\n        for details on how to use the callback.\n*   `tf.image.extract_glimpse` has been updated to correctly process the case\n    where `centered=False` and `normalized=False`. This is a breaking change as\n    the output is different from (incorrect) previous versions. Note this\n    breaking change only impacts `tf.image.extract_glimpse` and\n    `tf.compat.v2.image.extract_glimpse` API endpoints. The behavior of\n    `tf.compat.v1.image.extract_glimpse` does not change. The behavior of\n    existing C++ kernel `ExtractGlimpse` does not change either, so saved models\n    using `tf.raw_ops.ExtractGlimpse` will not be impacted.\n\n## Known Caveats\n  * `tf.lite`\n    * Keras-based LSTM models must be converted with an explicit batch size in the input layer.\n\n## Bug Fixes and Other Changes\n\n### TF Core:\n  * Set `tf2_behavior` to 1 to enable V2 for early loading cases.\n  * Add `execute_fn_for_device function` to dynamically choose the implementation based on underlying device placement.\n  * Eager:\n    * Add `reduce_logsumexp` benchmark with experiment compile.\n    * Give `EagerTensor`s a meaningful `__array__` implementation.\n    * Add another version of defun matmul for performance analysis.\n  * `tf.function`/AutoGraph:\n    * `AutoGraph` now includes into TensorFlow loops any variables that are closed over by local functions. Previously, such variables were sometimes incorrectly ignored.\n    * functions returned by the `get_concrete_function` method of `tf.function` objects can now be called with arguments consistent with the original arguments or type specs passed to `get_concrete_function`.  This calling convention is now the preferred way to use concrete functions with nested values and composite tensors. Please check the [guide](https://www.tensorflow.org/guide/concrete_function) for more details on `concrete_ function`.\n    * Update `tf.function`'s `experimental_relax_shapes` to handle composite tensors appropriately.\n    * Optimize `tf.function` invocation, by removing redundant list converter.\n    * `tf.function` will retrace when called with a different variable instead of simply using the `dtype` & `shape`.\n    * [Improve support](https://github.com/tensorflow/tensorflow/issues/33862) for dynamically-sized TensorArray inside `tf.function`.\n  * `tf.math`:\n    * Narrow down `argmin`/`argmax` contract to always return the smallest index for ties.\n    * `tf.math.reduce_variance` and `tf.math.reduce_std` return correct computation for complex types and no longer support integer types.\n    * Add Bessel functions of order 0,1 to `tf.math.special`.\n    * `tf.divide` now always returns a tensor to be consistent with documentation and other APIs.\n  * `tf.image`:\n    * Replaced [`tf.image.non_max_suppression_padded`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/image/non_max_suppression_padded?hl=en) with a new implementation that supports batched inputs, which is considerably faster on TPUs and GPUs. Boxes with area=0 will be ignored. Existing usage with single inputs should still work as before.\n  * `tf.linalg`\n    * Add `tf.linalg.banded_triangular_solve`.\n  * `tf.random`:\n    * Add `tf.random.stateless_parameterized_truncated_normal`.\n  * `tf.ragged`:\n    * Add `tf.ragged.cross` and `tf.ragged.cross_hashed` operations.\n  * `tf.RaggedTensor`:\n    * `RaggedTensor.to_tensor()` now preserves static shape.\n    * Add `tf.strings.format()` and `tf.print()` to support RaggedTensors.\n  * `tf.saved_model`:\n    * `@tf.function` from SavedModel no longer ignores args after a `RaggedTensor` when selecting the concrete function to run.\n    * Fix save model issue for ops with a list of functions.\n    * Add `tf.saved_model.LoadOptions` with [`experimental_io_device`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/saved_model/LoadOptions?hl=en) as arg with default value `None` to choose the I/O device for loading models and weights.\n    * Update `tf.saved_model.SaveOptions` with [`experimental_io_device`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/saved_model/SaveOptions?hl=en) as arg with default value `None` to choose the I/O device for saving models and weights.\n    * Mutable tables now restore checkpointed values when loaded from SavedModel.\n    * The user object metadata field in the SavedModel proto has been deprecated as part of the updates to Keras SavedModel. Keras was the only consumer of this field prior to the update.\n  * GPU\n    * TF 2.3 includes PTX kernels only for [compute capability](https://developer.nvidia.com/cuda-gpus) 7.0 to reduce the TF pip binary size.  Earlier releases included PTX for a variety of older compute capabilities.\n    * Remove environmental variable `TF_USE_CUDNN`.\n  * Others\n    * Retain parent namescope for ops added inside `tf.while_loop`/`tf.cond`/`tf.switch_case`.\n    * Update `tf.vectorized_map` to support vectorizing `tf.while_loop` and TensorList operations.\n    * `tf.custom_gradient` can now be applied to functions that accept nested structures of `tensors` as inputs (instead of just a list of tensors). Note that Python structures such as tuples and lists now won't be treated as tensors, so if you still want them to be treated that way, you need to wrap them with `tf.convert_to_tensor`.\n    * No lowering on gradient case op when input is `DeviceIndex` op.\n    * Extend the ragged version of `tf.gather` to support `batch_dims` and `axis` args.\n    * Update `tf.map_fn` to support RaggedTensors and SparseTensors.\n    * Deprecate `tf.group`. It is not useful in eager mode.\n    * Add CPU and GPU implementation of modified variation of [`FTRL`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/raw_ops/ApplyFtrl)/[`FTRLV2`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/raw_ops/ApplyFtrlV2) that can triggerred by `multiply_linear_by_lr` allowing a learning rate of zero.\n\n### `tf.data`:\n  * `tf.data.experimental.dense_to_ragged_batch` works correctly with tuples.\n  * `tf.data.experimental.dense_to_ragged_batch` to output variable ragged rank.\n  * `tf.data.experimental.cardinality` is now a method on `tf.data.Dataset`.\n  * `tf.data.Dataset` now supports `len(Dataset)` when the cardinality is finite.\n\n### `tf.distribute`:\n  * Expose experimental [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/distribute/DistributedDataset?hl=en) and [`tf.distribute.DistributedIterator`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/distribute/DistributedIterator) to distribute input data when using `tf.distribute` to scale training on multiple devices.\n    * Added a [`get_next_as_optional`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/distribute/DistributedIterator?hl=en#get_next_as_optional) method for [`tf.distribute.DistributedIterator`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/distribute/DistributedIterator?hl=en) class to return a `tf.experimental.Optional` instance that contains the next value for all replicas or none instead of raising an out of range error. Also see *new* [guide on input distribution](https://www.tensorflow.org/tutorials/distribute/input).\n  * Allow var.assign on MirroredVariables with aggregation=NONE in replica context. Previously this would raise an error. We now allow this because many users and library writers find using `.assign` in replica context to be more convenient, instead of having to use `Strategy.extended.update` which was the previous way of updating variables in this situation.\n  * `tf.distribute.experimental.MultiWorkerMirroredStrategy` adds support for partial batches. Workers running out of data now continue to participate in the training with empty inputs, instead of raising an error. Learn more about [partial batches here](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\n  * Improve the performance of reading metrics eagerly under `tf.distribute.experimental.MultiWorkerMirroredStrategy`.\n  * Fix the issue that `strategy.reduce()` inside `tf.function` may raise exceptions when the values to reduce are from loops or if-clauses.\n  * Fix the issue that `tf.distribute.MirroredStrategy` cannot be used together with `tf.distribute.experimental.MultiWorkerMirroredStrategy`.\n  * Add a `tf.distribute.cluster_resolver.TPUClusterResolver.connect` API to simplify TPU initialization.\n  * Add `tf.distribute.Strategy.gather` and `tf.distribute.ReplicaContext.all_gather` methods to gather and concatenate `tf.distribute.DistributedValues` across workers and devices.\n\n### `tf.keras`:\n  * Introduces experimental preprocessing layers API (`tf.keras.layers.experimental.preprocessing`)  to handle data preprocessing operations such as categorical feature encoding, text vectorization, data normalization, and data discretization (binning). The newly added layers provide a replacement for the  legacy feature column API, and support composite tensor inputs.\n  * Added **categorical data** processing layers:\n    * `IntegerLookup` & `StringLookup`: build an index of categorical feature values\n    * `CategoryEncoding`: turn integer-encoded categories into one-hot, multi-hot, or tf-idf encoded representations\n    * `CategoryCrossing`: create new categorical features representing co-occurrences of previous categorical feature values\n    * `Hashing`: the hashing trick, for large-vocabulary categorical features\n    * `Discretization`: turn continuous numerical features into categorical features by binning their values\n  * Improved **image preprocessing** layers: `CenterCrop`, `Rescaling`\n  * Improved **image augmentation** layers: `RandomCrop`, `RandomFlip`, `RandomTranslation`, `RandomRotation`, `RandomHeight`, `RandomWidth`, `RandomZoom`, `RandomContrast`\n  * Improved **`TextVectorization`** layer, which handles string tokenization, n-gram generation, and token encoding\n    * The `TextVectorization` layer now accounts for the mask_token as part of the vocabulary size when output_mode='int'. This means that, if you have a max_tokens value of 5000, your output will have 5000 unique values (not 5001 as before).\n    * Change the return value of `TextVectorization.get_vocabulary()` from `byte` to `string`. Users who previously were calling 'decode' on the output of this method should no longer need to do so.\n  * Introduce new Keras dataset generation utilities :\n    * **[`image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory)** is a utility based on `tf.data.Dataset`, meant to replace the legacy `ImageDataGenerator`. It takes you from a structured directory of images to a labeled dataset, in one function call. Note that it doesn't perform image data augmentation (which is meant to be done using preprocessing layers).\n    * **[`text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory)** takes you from a structured directory of text files to a labeled dataset, in one function call.\n    * **[`timeseries_dataset_from_array`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array)** is a `tf.data.Dataset`-based replacement of the legacy `TimeseriesGenerator`. It takes you from an array of timeseries data to a dataset of shifting windows with their targets.\n  * Added [`experimental_steps_per_execution`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/Model?hl=en#compile)\n arg to `model.compile` to indicate the number of batches to run per `tf.function` call. This can speed up Keras Models on TPUs up to 3x.\n  * Extends `tf.keras.layers.Lambda` layers to support multi-argument lambdas, and keyword arguments when calling the layer.\n  * Functional models now get constructed if *any* tensor in a layer call's arguments/keyword arguments comes from a keras input. Previously the functional api would only work if all of the elements in the first argument to the layer came from a keras input.\n  * Clean up `BatchNormalization` layer's `trainable` property to act like standard python state when it's used inside `tf.functions` (frozen at tracing time), instead of acting like a pseudo-variable whose updates *kind of sometimes* get reflected in already-traced `tf.function` traces.\n  * Add the `Conv1DTranspose` layer.\n  * Refine the semantics of `SensitivitySpecificityBase` derived metrics. See the updated API docstrings for [`tf.keras.metrics.SensitivityAtSpecificity`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/metrics/SensitivityAtSpecificity) and [`tf.keras.metrics.SpecificityAtSensitivty`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/metrics/SpecificityAtSensitivity).\n\n### `tf.lite`:\n  * Converter\n      * Restored `inference_input_type` and `inference_output_type` flags in TF 2.x TFLiteConverter (backward compatible with TF 1.x) to support integer (tf.int8, tf.uint8) input and output types in post training full integer quantized models.\n      * Added support for converting and resizing models with dynamic (placeholder) dimensions. Previously, there was only limited support for dynamic batch size, and even that did not guarantee that the model could be properly resized at runtime.\n       * Enabled experimental support for a new quantization mode with 16-bit activations and 8-bit weights. See `lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8`.\n  * CPU\n      * Fix an issue w/ dynamic weights and `Conv2D` on x86.\n      * Add a runtime Android flag for enabling `XNNPACK` for optimized CPU performance.\n      * Add a runtime iOS flag for enabling `XNNPACK` for optimized CPU performance.\n      * Add a compiler flag to enable building a TFLite library that applies `XNNPACK` delegate automatically when the model has a `fp32` operation.\n  * GPU\n      * Allow GPU acceleration starting with internal graph nodes\n      * Experimental support for quantized models with the Android GPU delegate\n      * Add GPU delegate whitelist.\n      * Rename GPU whitelist -> compatibility (list).\n      * Improve GPU compatibility list entries from crash reports.\n  * NNAPI\n      * Set default value for `StatefulNnApiDelegate::Options::max_number_delegated_partitions` to 3.\n      * Add capability to disable `NNAPI` CPU and check `NNAPI` Errno.\n      * Fix crashes when using `NNAPI` with target accelerator specified with model containing Conv2d or FullyConnected or LSTM nodes with quantized weights.\n      * Fix `ANEURALNETWORKS_BAD_DATA` execution failures with `sum`/`max`/`min`/`reduce` operations with `scalar` inputs.\n  * Hexagon\n      * TFLite Hexagon Delegate out of experimental.\n      * Experimental `int8` support for most hexagon ops.\n      * Experimental per-channel quant support for `conv` in Hexagon delegate.\n      * Support dynamic batch size in C++ API.\n  * CoreML\n     * Opensource CoreML delegate\n  * Misc\n      * Enable building Android TFLite targets on Windows\n      * Add support for `BatchMatMul`.\n      * Add support for `half_pixel_centers` with `ResizeNearestNeighbor`.\n      * Add 3D support for `BatchToSpaceND`.\n      * Add 5D support for `BroadcastSub`, `Maximum`, `Minimum`, `Transpose` and `BroadcastDiv`.\n      * Rename `kTfLiteActRelu1` to `kTfLiteActReluN1To1`.\n      * Enable flex delegate on tensorflow.lite.Interpreter Python package.\n      * Add `Buckettize`, `SparseCross` and `BoostedTreesBucketize` to the flex whitelist.\n      * Add support for selective registration of flex ops.\n      * Add missing kernels for flex delegate whitelisted ops.\n      * Fix issue when using direct `ByteBuffer` inputs with graphs that have dynamic shapes.\n      * Fix error checking supported operations in a model containing `HardSwish`.\n\n### Packaging Support\n  * Added `tf.sysconfig.get_build_info()`. Returns a dict that describes the build environment of the currently installed TensorFlow package, e.g. the NVIDIA CUDA and NVIDIA CuDNN versions used when TensorFlow was built.\n\n### Profiler\n  * Fix a subtle use-after-free issue in `XStatVisitor::RefValue()`.\n\n### TPU Enhancements\n  * Adds 3D mesh support in TPU configurations ops.\n  * Added TPU code for `FTRL` with `multiply_linear_by_lr`.\n  * Silently adds a new file system registry at `gstpu`.\n  * Support `restartType` in cloud tpu client.\n  * Depend on a specific version of google-api-python-client.\n  * Fixes apiclient import.\n\n### Tracing and Debugging\n  * Add a `TFE_Py_Execute` traceme.\n\n### XLA Support\n  * Implement stable `argmin` and `argmax`\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n902449@58880@bigcat_chen@ASIC, Abdul Baseer Khan, Abhineet Choudhary, Abolfazl Shahbazi, Adam Hillier, ag.ramesh, Agoniii, Ajay P, Alex Hoffman, Alexander Bayandin, Alexander Grund, Alexandre Abadie, Alexey Rogachevskiy, amoitra, Andrew Stevens, Angus-Luo, Anshuman Tripathy, Anush Elangovan, Artem Mavrin, Ashutosh Hathidara, autoih, Ayushman Kumar, ayushmankumar7, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, bhack, Bharat Raghunathan, Biagio Montaruli, Bigcat-Himax, blueyi, Bryan Cutler, Byambaa, Carlos Hernandez-Vaquero, Chen Lei, Chris Knorowski, Christian Clauss, chuanqiw, CuiYifeng, Daniel Situnayake, Daria Zhuravleva, Dayananda-V, Deven Desai, Devi Sandeep Endluri, Dmitry Zakharov, Dominic Jack, Duncan Riach, Edgar Liberis, Ehsan Toosi, ekuznetsov139, Elena Zhelezina, Eugene Kuznetsov, Eugene Mikhantiev, Evgenii Zheltonozhskii, Fabio Di Domenico, Fausto Morales, Fei Sun, feihugis, Felix E. Klee, flyingcat, Frederic Bastien, Fredrik Knutsson, frreiss, fsx950223, ganler, Gaurav Singh, Georgios Pinitas, Gian Marco Iodice, Giorgio Arena, Giuseppe Rossini, Gregory Keith, Guozhong Zhuang, gurushantj, Hahn Anselm, Harald Husum, Harjyot Bagga, Hristo Vrigazov, Ilya Persky, Ir1d, Itamar Turner-Trauring, jacco, Jake Tae, Janosh Riebesell, Jason Zaman, jayanth, Jeff Daily, Jens Elofsson, Jinzhe Zeng, JLZ, Jonas Skog, Jonathan Dekhtiar, Josh Meyer, Joshua Chia, Judd, justkw, Kaixi Hou, Kam D Kasravi, Kamil Rakoczy, Karol Gugala, Kayou, Kazuaki Ishizaki, Keith Smiley, Khaled Besrour, Kilaru Yasaswi Sri Chandra Gandhi, Kim, Young Soo, Kristian Hartikainen, Kwabena W. Agyeman, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Geiger, Lutz Roeder, M\\U00E5Ns Nilsson, Mahmoud Abuzaina, Manish, Marcel Koester, Marcin Sielski, marload, Martin Jul, Matt Conley, mdfaijul, Meng, Peng, Meteorix, Michael K\u00e4ufl, Michael137, Milan Straka, Mitchell Vitez, Ml-0, Mokke Meguru, Mshr-H, nammbash, Nathan Luehr, naumkin, Neeraj Bhadani, ngc92, Nick Morgan, nihui, Niranjan Hasabnis, Niranjan Yadla, Nishidha Panpaliya, Oceania2018, oclyke, Ouyang Jin, OverLordGoldDragon, Owen Lyke, Patrick Hemmer, Paul Andrey, Peng Sun, periannath, Phil Pearl, Prashant Dandriyal, Prashant Kumar, Rahul Huilgol, Rajan Singh, Rajeshwar Reddy T, rangjiaheng, Rishit Dagli, Rohan Reddy, rpalakkal, rposts, Ruan Kunliang, Rushabh Vasani, Ryohei Ikegami, Semun Lee, Seo-Inyoung, Sergey Mironov, Sharada Shiddibhavi, ShengYang1, Shraiysh Vaishay, Shunya Ueta, shwetaoj, Siyavash Najafzade, Srinivasan Narayanamoorthy, Stephan Uphoff, storypku, sunchenggen, sunway513, Sven-Hendrik Haase, Swapnil Parekh, Tamas Bela Feher, Teng Lu, tigertang, tomas, Tomohiro Ubukata, tongxuan.ltx, Tony Tonev, Tzu-Wei Huang, T\u00e9o Bouvard, Uday Bondhugula, Vaibhav Jade, Vijay Tadikamalla, Vikram Dattu, Vincent Abriou, Vishnuvardhan Janapati, Vo Van Nghia, VoVAllen, Will Battel, William D. Irons, wyzhao, Xiaoming (Jason) Cui, Xiaoquan Kong, Xinan Jiang, xutianming, Yair Ehrenwald, Yasir Modak, Yasuhiro Matsumoto, Yixing Fu, Yong Tang, Yuan Tang, zhaozheng09, Zilin Zhu, zilinzhu, \u5f20\u5fd7\u8c6a\n\n# Release 2.1.1\n\n## Bug Fixes and Other Changes\n* Updates `sqlite3` to `3.31.01` to handle [CVE-2019-19880](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880), [CVE-2019-19244](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244) and [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645)\n* Updates `curl` to `7.69.1` to handle [CVE-2019-15601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601)\n* Updates `libjpeg-turbo` to `2.0.4` to handle [CVE-2018-19664](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664), [CVE-2018-20330](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330) and [CVE-2019-13960](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960)\n* Updates Apache Spark to `2.4.5` to handle [CVE-2019-10099](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099), [CVE-2018-17190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190) and [CVE-2018-11770](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770)\n* Fixes a versioning bug which causes Keras layers from TF 1.x to be used instead of those from TF 2.x\n\n# Release 2.0.2\n\n## Bug Fixes and Other Changes\n* Updates `sqlite3` to `3.31.01` to handle [CVE-2019-19880](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880), [CVE-2019-19244](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244) and [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645)\n* Updates `curl` to `7.69.1` to handle [CVE-2019-15601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601)\n* Updates `libjpeg-turbo` to `2.0.4` to handle [CVE-2018-19664](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664), [CVE-2018-20330](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330) and [CVE-2019-13960](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960)\n* Updates Apache Spark to `2.4.5` to handle [CVE-2019-10099](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099), [CVE-2018-17190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190) and [CVE-2018-11770](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770)\n\n# Release 1.15.3\n\n## Bug Fixes and Other Changes\n* Updates `sqlite3` to `3.31.01` to handle [CVE-2019-19880](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880), [CVE-2019-19244](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244) and [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645)\n* Updates `curl` to `7.69.1` to handle [CVE-2019-15601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601)\n* Updates `libjpeg-turbo` to `2.0.4` to handle [CVE-2018-19664](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664), [CVE-2018-20330](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330) and [CVE-2019-13960](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960)\n* Updates Apache Spark to `2.4.5` to handle [CVE-2019-10099](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099), [CVE-2018-17190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190) and [CVE-2018-11770](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770)\n\n# Release 2.2.0\n\nTensorFlow 2.2 discontinues support for Python 2, [previously announced](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ) as following [Python 2's EOL on January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update).\n\nCoinciding with this change, new releases of [TensorFlow's Docker images](https://hub.docker.com/r/tensorflow/tensorflow/) provide Python 3 exclusively. Because all images now use Python 3, Docker tags containing `-py3` will no longer be provided and existing `-py3` tags like `latest-py3` will not be updated.\n\n## Major Features and Improvements\n\n* Replaced the scalar type for string tensors from `std::string` to `tensorflow::tstring` which is now ABI stable.\n* A new Profiler for TF 2 for CPU/GPU/TPU. It offers both device and host performance analysis, including input pipeline and TF Ops. Optimization advisory is provided whenever possible. Please see [this tutorial](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) and [guide](https://www.tensorflow.org/guide/profiler) for usage guidelines.\n* Export C++ functions to Python using `pybind11` as opposed to `SWIG` as a part of our [deprecation of swig efforts](https://github.com/tensorflow/community/blob/master/rfcs/20190208-pybind11.md).\n* `tf.distribute`:\n  * Support added for global sync `BatchNormalization` by using the newly added `tf.keras.layers.experimental.SyncBatchNormalization` layer. This layer will sync `BatchNormalization` statistics every step across all replicas taking part in sync training.\n  * Performance improvements for GPU multi-worker distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy`\n    * Update NVIDIA `NCCL` to `2.5.7-1` for better performance and performance tuning. Please see [nccl developer guide](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html) for more information on this.\n    * Support gradient `allreduce` in `float16`. See this [example](https://github.com/tensorflow/models/blob/master/official/staging/training/grad_utils.py) usage.\n    * Experimental support of [all reduce gradient packing](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CollectiveHints) to allow overlapping gradient aggregation with backward path computation.\n    * Deprecated `experimental_run_v2` method for distribution strategies and renamed the method `run` as it is no longer experimental.\n    * Add CompositeTensor support for DistributedIterators. This should help prevent unnecessary function retracing and memory leaks.\n* `tf.keras`:\n  * `Model.fit` major improvements:\n     * You can now use custom training logic with `Model.fit` by overriding `Model.train_step`.\n     * Easily write state-of-the-art training loops without worrying about all of the features `Model.fit` handles for you (distribution strategies, callbacks, data formats, looping logic, etc)\n     * See the default [`Model.train_step`](https://github.com/tensorflow/tensorflow/blob/1381fc8e15e22402417b98e3881dfd409998daea/tensorflow/python/keras/engine/training.py#L540) for an example of what this function should look like. Same applies for validation and inference via `Model.test_step` and `Model.predict_step`.\n     * SavedModel uses its own `Model._saved_model_inputs_spec` attr now instead of\n       relying on `Model.inputs` and `Model.input_names`, which are no longer set for subclass Models.\n       This attr is set in eager, `tf.function`, and graph modes. This gets rid of the need for users to\n       manually call `Model._set_inputs` when using Custom Training Loops(CTLs).\n     * Dynamic shapes are supported for generators by calling the Model on the first batch we \"peek\" from the generator.\n       This used to happen implicitly in `Model._standardize_user_data`. Long-term, a solution where the\n       `DataAdapter` doesn't need to call the Model is probably preferable.\n  * The SavedModel format now supports all Keras built-in layers (including metrics, preprocessing layers, and stateful RNN layers)\n  * Update Keras batch normalization layer to use the running mean and average computation in the `fused_batch_norm`. You should see significant performance improvements when using `fused_batch_norm` in Eager mode.\n\n* `tf.lite`:\n  * Enable TFLite experimental new converter by default.\n* XLA\n  * XLA now builds and works on windows. All prebuilt packages come with XLA available.\n  * XLA can be [enabled for a `tf.function`](https://www.tensorflow.org/xla#explicit_compilation_with_tffunction\n) with \u201ccompile or throw exception\u201d semantics on CPU and GPU.\n\n## Breaking Changes\n* `tf.keras`:\n  * In `tf.keras.applications` the name of the \"top\" layer has been standardized to \"predictions\". This is only a problem if your code relies on the exact name of the layer.\n  * Huber loss function has been updated to be consistent with other Keras losses. It now computes mean over the last axis of per-sample losses before applying the reduction function.\n* AutoGraph no longer converts functions passed to `tf.py_function`, `tf.py_func` and `tf.numpy_function`.\n* Deprecating `XLA_CPU` and `XLA_GPU` devices with this release.\n* Increasing the minimum bazel version to build TF to 2.0.0 to use Bazel's `cc_experimental_shared_library`.\n* Keras compile/fit behavior for functional and subclassed models have been unified. Model properties such as `metrics`, `metrics_names` will now be available only after **training/evaluating the model on actual data** for functional models. `metrics` will **now include** model `loss` and output losses.`loss_functions` property has been removed from the model. This was an undocumented property that was accidentally public and has now been removed.\n\n## Known Caveats\n* The current TensorFlow release now **requires** [gast](https://pypi.org/project/gast/) version 0.3.3.\n\n## Bug Fixes and Other Changes\n\n*   `tf.data`:\n    *   Removed `autotune_algorithm` from experimental optimization options.\n*   TF Core:\n    *   `tf.constant` always creates CPU tensors irrespective of the current\n        device context.\n    *   Eager `TensorHandles` maintain a list of mirrors for any copies to local\n        or remote devices. This avoids any redundant copies due to op execution.\n    *   For `tf.Tensor` & `tf.Variable`, `.experimental_ref()` is no longer\n        experimental and is available as simply `.ref()`.\n    *   `pfor/vectorized_map`: Added support for vectorizing 56 more ops.\n        Vectorizing `tf.cond` is also supported now.\n    *   Set as much partial shape as we can infer statically within the gradient\n        impl of the gather op.\n    *   Gradient of `tf.while_loop` emits `StatelessWhile` op if `cond` and body\n        functions are stateless. This allows multiple gradients while ops to run\n        in parallel under distribution strategy.\n    *   Speed up `GradientTape` in eager mode by auto-generating list of op\n        inputs/outputs which are unused and hence not cached for gradient\n        functions.\n    *   Support `back_prop=False` in `while_v2` but mark it as deprecated.\n    *   Improve error message when attempting to use `None` in data-dependent\n        control flow.\n    *   Add `RaggedTensor.numpy()`.\n    *   Update `RaggedTensor.__getitem__` to preserve uniform dimensions & allow\n        indexing into uniform dimensions.\n    *   Update `tf.expand_dims` to always insert the new dimension as a\n        non-ragged dimension.\n    *   Update `tf.embedding_lookup` to use `partition_strategy` and `max_norm`\n        when `ids` is ragged.\n    *   Allow `batch_dims==rank(indices)` in `tf.gather`.\n    *   Add support for bfloat16 in `tf.print`.\n*   `tf.distribute`:\n    *   Support `embedding_column` with variable-length input features for\n        `MultiWorkerMirroredStrategy`.\n*   `tf.keras`:\n    *   Added `experimental_aggregate_gradients` argument to\n        `tf.keras.optimizer.Optimizer.apply_gradients`. This allows custom\n        gradient aggregation and processing aggregated gradients in custom\n        training loop.\n    *   Allow `pathlib.Path` paths for loading models via Keras API.\n*   `tf.function`/AutoGraph:\n    *   AutoGraph is now available in `ReplicaContext.merge_call`,\n        `Strategy.extended.update` and `Strategy.extended.update_non_slot`.\n    *   Experimental support for shape invariants has been enabled in\n        `tf.function`. See the API docs for\n        `tf.autograph.experimental.set_loop_options` for additional info.\n    *   AutoGraph error messages now exclude frames corresponding to APIs\n        internal to AutoGraph.\n    *   Improve shape inference for `tf.function` input arguments to unlock more\n        Grappler optimizations in TensorFlow 2.x.\n    *   Improve automatic control dependency management of resources by allowing\n        resource reads to occur in parallel and synchronizing only on writes.\n    *   Fix execution order of multiple stateful calls to `experimental_run_v2`\n        in `tf.function`.\n    *   You can now iterate over `RaggedTensors` using a for loop inside\n        `tf.function`.\n*   `tf.lite`:\n    *   Migrated the `tf.lite` C inference API out of experimental into lite/c.\n    *   Add an option to disallow `NNAPI` CPU / partial acceleration on Android\n        10\n    *   TFLite Android AARs now include the C headers and APIs are required to\n        use TFLite from native code.\n    *   Refactors the delegate and delegate kernel sources to allow usage in the\n        linter.\n    *   Limit delegated ops to actually supported ones if a device name is\n        specified or `NNAPI` CPU Fallback is disabled.\n    *   TFLite now supports `tf.math.reciprocal1` op by lowering to `tf.div op`.\n    *   TFLite's unpack op now supports boolean tensor inputs.\n    *   Microcontroller and embedded code moved from experimental to main\n        TensorFlow Lite folder\n    *   Check for large TFLite tensors.\n    *   Fix GPU delegate crash with C++17.\n    *   Add 5D support to TFLite `strided_slice`.\n    *   Fix error in delegation of `DEPTH_TO_SPACE` to `NNAPI` causing op not to\n        be accelerated.\n    *   Fix segmentation fault when running a model with LSTM nodes using\n        `NNAPI` Delegate\n    *   Fix `NNAPI` delegate failure when an operand for Maximum/Minimum\n        operation is a scalar.\n    *   Fix `NNAPI` delegate failure when Axis input for reduce operation is a\n        scalar.\n    *   Expose option to limit the number of partitions that will be delegated\n        to `NNAPI`.\n    *   If a target accelerator is specified, use its feature level to determine\n        operations to delegate instead of SDK version.\n*   `tf.random`:\n    *   Various random number generation improvements:\n    *   Add a fast path for default `random_uniform`\n    *   `random_seed` documentation improvement.\n    *   `RandomBinomial` broadcasts and appends the sample shape to the left\n        rather than the right.\n    *   Added `tf.random.stateless_binomial`, `tf.random.stateless_gamma`,\n        `tf.random.stateless_poisson`\n    *   `tf.random.stateless_uniform` now supports unbounded sampling of `int`\n        types.\n*   Math and Linear Algebra:\n    *   Add `tf.linalg.LinearOperatorTridiag`.\n    *   Add `LinearOperatorBlockLowerTriangular`\n    *   Add broadcasting support to\n        tf.linalg.triangular_solve[#26204](https://github.com/tensorflow/tensorflow/issues/26204),\n        tf.math.invert_permutation.\n    *   Add `tf.math.sobol_sample` op.\n    *   Add `tf.math.xlog1py`.\n    *   Add `tf.math.special.{dawsn,expi,fresnel_cos,fresnel_sin,spence}`.\n    *   Add a Modified Discrete Cosine Transform (MDCT) and its inverse to\n        `tf.signal`.\n*   TPU Enhancements:\n    *   Refactor `TpuClusterResolver` to move shared logic to a separate pip\n        package.\n    *   Support configuring TPU software version from cloud tpu client.\n    *   Allowed TPU embedding weight decay factor to be multiplied by learning\n        rate.\n*   XLA Support:\n    *   Add standalone XLA AOT runtime target + relevant .cc sources to pip\n        package.\n    *   Add check for memory alignment to MemoryAllocation::MemoryAllocation()\n        on 32-bit ARM. This ensures a deterministic early exit instead of a hard\n        to debug bus error later.\n    *   `saved_model_cli aot_compile_cpu` allows you to compile saved models to\n        XLA header+object files and include them in your C++ programs.\n    *   Enable `Igamma`, `Igammac` for XLA.\n*   Deterministic Op Functionality:\n    *   XLA reduction emitter is deterministic when the environment variable\n        `TF_DETERMINISTIC_OPS` is set to \"true\" or \"1\". This extends\n        deterministic `tf.nn.bias_add` back-prop functionality (and therefore\n        also deterministic back-prop of bias-addition in Keras layers) to\n        include when XLA JIT compilation is enabled.\n    *   Fix problem, when running on a CUDA GPU and when either environment\n        variable `TF_DETERMINISTIC_OPS` or environment variable\n        `TF_CUDNN_DETERMINISTIC` is set to \"true\" or \"1\", in which some layer\n        configurations led to an exception with the message \"No algorithm\n        worked!\"\n*   Tracing and Debugging:\n    *   Add source, destination name to `_send` traceme to allow easier\n        debugging.\n    *   Add traceme event to `fastpathexecute`.\n*   Other:\n    *   Fix an issue with AUC.reset_states for multi-label AUC\n        [#35852](https://github.com/tensorflow/tensorflow/issues/35852)\n    *   Fix the TF upgrade script to not delete files when there is a parsing\n        error and the output mode is `in-place`.\n    *   Move `tensorflow/core:framework/*_pyclif` rules to\n        `tensorflow/core/framework:*_pyclif`.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n372046933, 8bitmp3, aaronhma, Abin Shahab, Aditya Patwardhan, Agoniii, Ahti Kitsik, Alan Yee, Albin Joy, Alex Hoffman, Alexander Grund, Alexandre E. Eichenberger, Amit Kumar Jaiswal, amoitra, Andrew Anderson, Angus-Luo, Anthony Barbier, Anton Kachatkou, Anuj Rawat, archis, Arpan-Dhatt, Arvind Sundararajan, Ashutosh Hathidara, autoih, Bairen Yi, Balint Cristian, Bas Aarts, BashirSbaiti, Basit Ayantunde, Ben Barsdell, Benjamin Gaillard, boron, Brett Koonce, Bryan Cutler, Christian Goll, Christian Sachs, Clayne Robison, comet, Daniel Falbel, Daria Zhuravleva, darsh8200, David Truby, Dayananda-V, deepakm, Denis Khalikov, Devansh Singh, Dheeraj R Reddy, Diederik Van Liere, Diego Caballero, Dominic Jack, dothinking, Douman, Drake Gens, Duncan Riach, Ehsan Toosi, ekuznetsov139, Elena Zhelezina, elzino, Ending2015a, Eric Schweitz, Erik Zettel, Ethan Saadia, Eugene Kuznetsov, Evgeniy Zheltonozhskiy, Ewout Ter Hoeven, exfalso, FAIJUL, Fangjun Kuang, Fei Hu, Frank Laub, Frederic Bastien, Fredrik Knutsson, frreiss, Fr\u00e9d\u00e9ric Rechtenstein, fsx950223, Gaurav Singh, gbaned, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, Hans Gaiser, Hans Pabst, Haoyu Wu, Harry Slatyer, hsahovic, Hugo, Hugo Sj\u00f6berg, IrinaM21, jacco, Jake Tae, Jean-Denis Lesage, Jean-Michel Gorius, Jeff Daily, Jens Elofsson, Jerry Shih, jerryyin, Jin Mingjian, Jinjing Zhou, JKIsaacLee, jojimonv, Jonathan Dekhtiar, Jose Ignacio Gomez, Joseph-Rance, Judd, Julian Gross, Kaixi Hou, Kaustubh Maske Patil, Keunwoo Choi, Kevin Hanselman, Khor Chean Wei, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan, Koki Ibukuro, Kristian Holsheimer, kurileo, Lakshay Tokas, Lee Netherton, leike666666, Leslie-Fang-Intel, Li, Guizi, LIUJIAN435, Lukas Geiger, Lyo Nguyen, madisetti, Maher Jendoubi, Mahmoud Abuzaina, Manuel Freiberger, Marcel Koester, Marco Jacopo Ferrarotti, Markus Franke, marload, Mbah-Javis, mbhuiyan, Meng Zhang, Michael Liao, MichaelKonobeev, Michal Tarnowski, Milan Straka, minoring, Mohamed Nour Abouelseoud, MoussaMM, Mrinal Jain, mrTsjolder, M\u00e5ns Nilsson, Namrata Bhave, Nicholas Gao, Niels Ole Salscheider, nikochiko, Niranjan Hasabnis, Nishidha Panpaliya, nmostafa, Noah Trenaman, nuka137, Officium, Owen L - Sfe, Pallavi G, Paul Andrey, Peng Sun, Peng Wu, Phil Pearl, PhilipMay, pingsutw, Pooya Davoodi, PragmaTwice, pshiko, Qwerty71, R Gomathi, Rahul Huilgol, Richard Xiao, Rick Wierenga, Roberto Rosmaninho, ruchit2801, Rushabh Vasani, Sami, Sana Damani, Sarvesh Dubey, Sasan Jafarnejad, Sergii Khomenko, Shane Smiskol, Shaochen Shi, sharkdtu, Shawn Presser, ShengYang1, Shreyash Patodia, Shyam Sundar Dhanabalan, Siju Samuel, Somyajit Chakraborty Sam, Srihari Humbarwadi, srinivasan.narayanamoorthy, Srishti Yadav, Steph-En-M, Stephan Uphoff, Stephen Mugisha, SumanSudhir, Taehun Kim, Tamas Bela Feher, TengLu, Tetragramm, Thierry Herrmann, Tian Jin, tigertang, Tom Carchrae, Tom Forbes, Trent Lo, Victor Peng, vijayphoenix, Vincent Abriou, Vishal Bhola, Vishnuvardhan Janapati, vladbataev, VoVAllen, Wallyss Lima, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, William Zhang, Xiaoming (Jason) Cui, Xiaoquan Kong, Xinan Jiang, Yasir Modak, Yasuhiro Matsumoto, Yaxun (Sam) Liu, Yong Tang, Ytyt-Yt, yuan, Yuan Mingshuai, Yuan Tang, Yuki Ueda, Yusup, zhangshijin, zhuwenxi\n\n# Release 2.0.1\n\n## Bug Fixes and Other Changes\n* Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))\n* Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)\n* Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)\n\n\n# Release 1.15.2\n\n## Bug Fixes and Other Changes\n* Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))\n* Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)\n* Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)\n\n\n# Release 2.1.0\n\nTensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.\n\n## Major Features and Improvements\n\n*   The `tensorflow` pip package now includes GPU support by default (same as\n    `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and\n    without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only\n    packages can be downloaded at `tensorflow-cpu` for users who are concerned\n    about package size.\n*   **Windows users:** Officially-released `tensorflow` Pip packages are now\n    built with Visual Studio 2019 version 16.4 in order to take advantage of the\n    new `/d2ReducedOptimizeHugeFunctions` compiler flag. To use these new\n    packages, you must install \"Microsoft Visual C++ Redistributable for Visual\n    Studio 2015, 2017 and 2019\", available from Microsoft's website\n    [here](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads).\n    *   This does not change the minimum required version for building\n        TensorFlow from source on Windows, but builds enabling\n        `EIGEN_STRONG_INLINE` can take over 48 hours to compile without this\n        flag. Refer to `configure.py` for more information about\n        `EIGEN_STRONG_INLINE` and `/d2ReducedOptimizeHugeFunctions`.\n    *   If either of the required DLLs, `msvcp140.dll` (old) or `msvcp140_1.dll`\n        (new), are missing on your machine, `import tensorflow` will print a\n        warning message.\n*   The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.\n*   `tf.keras`\n    *   Experimental support for mixed precision is available on GPUs and Cloud\n        TPUs. See\n        [usage guide](https://www.tensorflow.org/guide/keras/mixed_precision).\n    *   Introduced the `TextVectorization` layer, which takes as input raw\n        strings and takes care of text standardization, tokenization, n-gram\n        generation, and vocabulary indexing. See this\n        [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3).\n    *   Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be\n        outside of the DistributionStrategy scope, as long as the model was\n        constructed inside of a scope.\n    *   Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and\n        `.predict` is available for Cloud TPUs, Cloud TPU, for all types of\n        Keras models (sequential, functional and subclassing models).\n    *   Automatic outside compilation is now enabled for Cloud TPUs. This allows\n        `tf.summary` to be used more conveniently with Cloud TPUs.\n    *   Dynamic batch sizes with DistributionStrategy and Keras are supported on\n        Cloud TPUs.\n    *   Support for `.fit`, `.evaluate`, `.predict` on TPU using numpy data, in\n        addition to `tf.data.Dataset`.\n    *   Keras reference implementations for many popular models are available in\n        the TensorFlow\n        [Model Garden](https://github.com/tensorflow/models/tree/master/official).\n*   `tf.data`\n    *   Changes rebatching for `tf.data datasets` + DistributionStrategy for\n        better performance. Note that the dataset also behaves slightly\n        differently, in that the rebatched dataset cardinality will always be a\n        multiple of the number of replicas.\n    *   `tf.data.Dataset` now supports automatic data distribution and sharding\n        in distributed environments, including on TPU pods.\n    *   Distribution policies for `tf.data.Dataset` can now be tuned with 1.\n        `tf.data.experimental.AutoShardPolicy(OFF, AUTO, FILE, DATA)` 2.\n        `tf.data.experimental.ExternalStatePolicy(WARN, IGNORE, FAIL)`\n*   `tf.debugging`\n    *   Add `tf.debugging.enable_check_numerics()` and\n        `tf.debugging.disable_check_numerics()` to help debugging the root\n        causes of issues involving infinities and `NaN`s.\n*   `tf.distribute`\n    *   Custom training loop support on TPUs and TPU pods is available through\n        `strategy.experimental_distribute_dataset`,\n        `strategy.experimental_distribute_datasets_from_function`,\n        `strategy.experimental_run_v2`, `strategy.reduce`.\n    *   Support for a global distribution strategy through\n        `tf.distribute.experimental_set_strategy(),` in addition to\n        `strategy.scope()`.\n*   `TensorRT`\n    *   [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new)\n        is now supported and enabled by default. This adds support for more\n        TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D,\n        MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the\n        TensorFlow-TensorRT python conversion API is exported as\n        `tf.experimental.tensorrt.Converter`.\n*   Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to\n    \"true\" or \"1\", this environment variable makes `tf.nn.bias_add` operate\n    deterministically (i.e. reproducibly), but currently only when XLA JIT\n    compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to \"true\" or\n    \"1\" also makes cuDNN convolution and max-pooling operate deterministically.\n    This makes Keras Conv\\*D and MaxPool\\*D layers operate deterministically in\n    both the forward and backward directions when running on a CUDA-enabled GPU.\n\n## Breaking Changes\n* Deletes `Operation.traceback_with_start_lines` for which we know of no usages.\n* Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.\n* Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\n* The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.\n* `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.\n* `tf.config.experimental_list_devices` has been removed, please use\n`tf.config.list_logical_devices`.\n\n## Bug Fixes and Other Changes\n* `tf.data`\n  * Fixes concurrency issue with `tf.data.experimental.parallel_interleave` with `sloppy=True`.\n  * Add `tf.data.experimental.dense_to_ragged_batch()`.\n  * Extend `tf.data` parsing ops to support `RaggedTensors`.\n* `tf.distribute`\n  * Fix issue where GRU would crash or give incorrect output when a `tf.distribute.Strategy` was used.\n* `tf.estimator`\n  * Added option in `tf.estimator.CheckpointSaverHook` to not save the `GraphDef`.\n  * Moving the checkpoint reader from swig to pybind11.\n* `tf.keras`\n  * Export `depthwise_conv2d` in `tf.keras.backend`.\n  * In Keras Layers and Models, Variables in `trainable_weights`, `non_trainable_weights`, and `weights` are explicitly deduplicated.\n  * Keras `model.load_weights` now accepts `skip_mismatch` as an argument. This was available in external Keras, and has now been copied over to `tf.keras`.\n  * Fix the input shape caching behavior of Keras convolutional layers.\n  * `Model.fit_generator`, `Model.evaluate_generator`, `Model.predict_generator`, `Model.train_on_batch`, `Model.test_on_batch`, and `Model.predict_on_batch` methods now respect the `run_eagerly` property, and will correctly run using `tf.function` by default. Note that `Model.fit_generator`, `Model.evaluate_generator`, and `Model.predict_generator` are deprecated endpoints. They are subsumed by `Model.fit`, `Model.evaluate`, and `Model.predict` which now support generators and Sequences.\n* `tf.lite`\n  * Legalization for `NMS` ops in TFLite.\n  * add `narrow_range` and `axis` to `quantize_v2` and `dequantize` ops.\n  * Added support for `FusedBatchNormV3` in converter.\n  * Add an `errno`-like field to `NNAPI` delegate for detecting `NNAPI` errors for fallback behaviour.\n  * Refactors `NNAPI` Delegate to support detailed reason why an operation is not accelerated.\n  * Converts hardswish subgraphs into atomic ops.\n* Other\n  * Critical stability updates for TPUs, especially in cases where the XLA compiler produces compilation errors.\n  * TPUs can now be re-initialized multiple times, using `tf.tpu.experimental.initialize_tpu_system`.\n  * Add `RaggedTensor.merge_dims()`.\n  * Added new `uniform_row_length` row-partitioning tensor to `RaggedTensor`.\n  * Add `shape` arg to `RaggedTensor.to_tensor`; Improve speed of `RaggedTensor.to_tensor`.\n  * `tf.io.parse_sequence_example` and `tf.io.parse_single_sequence_example` now support ragged features.\n  * Fix `while_v2` with variables in custom gradient.\n  * Support taking gradients of V2 `tf.cond` and `tf.while_loop` using `LookupTable`.\n  * Fix bug where `vectorized_map` failed on inputs with unknown static shape.\n  * Add preliminary support for sparse CSR matrices.\n  * Tensor equality with `None` now behaves as expected.\n  * Make calls to `tf.function(f)()`, `tf.function(f).get_concrete_function` and `tf.function(f).get_initialization_function` thread-safe.\n  * Extend `tf.identity` to work with CompositeTensors (such as SparseTensor)\n  * Added more `dtypes` and zero-sized inputs to `Einsum` Op and improved its performance\n  * Enable multi-worker `NCCL` `all-reduce` inside functions executing eagerly.\n  * Added complex128 support to `RFFT`, `RFFT2D`, `RFFT3D`, `IRFFT`, `IRFFT2D`, and `IRFFT3D`.\n  * Add `pfor` converter for `SelfAdjointEigV2`.\n  * Add `tf.math.ndtri` and `tf.math.erfinv`.\n  * Add `tf.config.experimental.enable_mlir_bridge` to allow using MLIR compiler bridge in eager model.\n  * Added support for MatrixSolve on Cloud TPU / XLA.\n  * Added `tf.autodiff.ForwardAccumulator` for forward-mode autodiff\n  * Add `LinearOperatorPermutation`.\n  * A few performance optimizations on `tf.reduce_logsumexp`.\n  * Added multilabel handling to `AUC` metric\n  * Optimization on `zeros_like`.\n  * Dimension constructor now requires `None` or types with an `__index__` method.\n  * Add `tf.random.uniform` microbenchmark.\n  * Use `_protogen` suffix for proto library targets instead of `_cc_protogen` suffix.\n  * Moving the checkpoint reader from `swig` to `pybind11`.\n  * `tf.device` & `MirroredStrategy` now supports passing in a `tf.config.LogicalDevice`\n  * If you're building Tensorflow from source, consider using [bazelisk](https://github.com/bazelbuild/bazelisk) to automatically download and use the correct Bazel version. Bazelisk reads the `.bazelversion` file at the root of the project directory.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n8bitmp3, Aaron Ma, Abd\u00fcLhamit Yilmaz, Abhai Kollara, aflc, Ag Ramesh, Albert Z. Guo, Alex Torres, amoitra, Andrii Prymostka, angeliand, Anshuman Tripathy, Anthony Barbier, Anton Kachatkou, Anubh-V, Anuja Jakhade, Artem Ryabov, autoih, Bairen Yi, Bas Aarts, Basit Ayantunde, Ben Barsdell, Bhavani Subramanian, Brett Koonce, candy.dc, Captain-Pool, caster, cathy, Chong Yan, Choong Yin Thong, Clayne Robison, Colle, Dan Ganea, David Norman, David Refaeli, dengziming, Diego Caballero, Divyanshu, djshen, Douman, Duncan Riach, EFanZh, Elena Zhelezina, Eric Schweitz, Evgenii Zheltonozhskii, Fei Hu, fo40225, Fred Reiss, Frederic Bastien, Fredrik Knutsson, fsx950223, fwcore, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, giuros01, Gomathi Ramamurthy, Guozhong Zhuang, Haifeng Jin, Haoyu Wu, HarikrishnanBalagopal, HJYOO, Huang Chen-Yi, Ilham Firdausi Putra, Imran Salam, Jared Nielsen, Jason Zaman, Jasper Vicenti, Jeff Daily, Jeff Poznanovic, Jens Elofsson, Jerry Shih, jerryyin, Jesper Dramsch, jim.meyer, Jongwon Lee, Jun Wan, Junyuan Xie, Kaixi Hou, kamalkraj, Kan Chen, Karthik Muthuraman, Keiji Ariyama, Kevin Rose, Kevin Wang, Koan-Sin Tan, kstuedem, Kwabena W. Agyeman, Lakshay Tokas, latyas, Leslie-Fang-Intel, Li, Guizi, Luciano Resende, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manuel Freiberger, Mark Ryan, Martin Mlostek, Masaki Kozuki, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Muhwan Kim, Nagy Mostafa, nammbash, Nathan Luehr, Nathan Wells, Niranjan Hasabnis, Oleksii Volkovskyi, Olivier Moindrot, olramde, Ouyang Jin, OverLordGoldDragon, Pallavi G, Paul Andrey, Paul Wais, pkanwar23, Pooya Davoodi, Prabindh Sundareson, Rajeshwar Reddy T, Ralovich, Kristof, Refraction-Ray, Richard Barnes, richardbrks, Robert Herbig, Romeo Kienzler, Ryan Mccormick, saishruthi, Saket Khandelwal, Sami Kama, Sana Damani, Satoshi Tanaka, Sergey Mironov, Sergii Khomenko, Shahid, Shawn Presser, ShengYang1, Siddhartha Bagaria, Simon Plovyt, skeydan, srinivasan.narayanamoorthy, Stephen Mugisha, sunway513, Takeshi Watanabe, Taylor Jakobson, TengLu, TheMindVirus, ThisIsIsaac, Tim Gates, Timothy Liu, Tomer Gafner, Trent Lo, Trevor Hickey, Trevor Morris, vcarpani, Wei Wang, Wen-Heng (Jack) Chung, wenshuai, Wenshuai-Xiaomi, wenxizhu, william, William D. Irons, Xinan Jiang, Yannic, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Youwei Song, Zaccharie Ramzi, Zhang, Zhenyu Guo, \u738b\u632f\u534e (Zhenhua Wang), \u97e9\u8463, \uc774\uc911\uac74 Isaac Lee\n\n# Release 1.15.0\nThis is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.\n\n## Major Features and Improvements\n* As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\n* TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.\nThis enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.\n* EagerTensor now supports numpy buffer interface for tensors.\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.\n* Adds `enable_tensor_equality()`, which switches the behavior such that:\n  * Tensors are no longer hashable.\n  * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.\n\n## Breaking Changes\n* Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\n* TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\n* `tf.keras`:\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\n  * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer \"layer-name\" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\n  * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\n\n## Bug Fixes and Other Changes\n* `tf.estimator`:\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.\n  * Fix tests in canned estimators.\n  * Expose Head as public API.\n  * Fixes critical bugs that help with `DenseFeatures` usability in TF2\n* `tf.data`:\n  * Promoting `unbatch` from experimental to core API.\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\n* `tf.keras`:\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.\n  * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\n  * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\n* `tf.lite`\n  * Add `GATHER` support to NN API delegate.\n  * tflite object detection script has a debug mode.\n  * Add delegate support for `QUANTIZE`.\n  * Added evaluation script for COCO minival.\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\n  * Converts hardswish subgraphs into atomic ops.\n* Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\n* `parallel_for`: Add converter for `MatrixDiag`.\n* Add `narrow_range` attribute to `QuantizeAndDequantizeV2` and V3.\n* Added new op: `tf.strings.unsorted_segment_join`.\n* Add HW acceleration support for `topK_v2`.\n* Add new `TypeSpec` classes.\n* CloudBigtable version updated to v0.10.0.\n* Expose `Head` as public API.\n* Update docstring for gather to properly describe the non-empty `batch_dims` case.\n* Added `tf.sparse.from_dense` utility function.\n* Improved ragged tensor support in `TensorFlowTestCase`.\n* Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\n* `ResizeInputTensor` now works for all delegates.\n* Add `EXPAND_DIMS` support to NN API delegate TEST:  expand_dims_test\n* `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\n* `tf.cond`, `tf.while` and `if` and `while` in AutoGraph now accept a nonscalar predicate if has a single element. This does not affect non-V2 control flow.\n* `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\n* Refactors code in Quant8 LSTM support to reduce TFLite binary size.\n* Add support of local soft device placement for eager op.\n* Add HW acceleration support for `LogSoftMax`.\n* Added a function `nested_value_rowids` for ragged tensors.\n* Add guard to avoid acceleration of L2 Normalization with input rank != 4\n* Add `tf.math.cumulative_logsumexp operation`.\n* Add `tf.ragged.stack`.\n* Fix memory allocation problem when calling `AddNewInputConstantTensor`.\n* Delegate application failure leaves interpreter in valid state.\n* Add check for correct memory alignment to `MemoryAllocation::MemoryAllocation()`.\n* Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\n* Added support for `FusedBatchNormV3` in converter.\n* A ragged to dense op for directly calculating tensors.\n* Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\na6802739, Aaron Ma, Abdullah Selek, Abolfazl Shahbazi, Ag Ramesh, Albert Z. Guo, Albin Joy, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Amit Srivastava, amoitra, Andrew Lihonosov, Andrii Prymostka, Anuj Rawat, Astropeak, Ayush Agrawal, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bryan Cutler, candy.dc, Cao Zongyan, Captain-Pool, Casper Da Costa-Luis, Chen Guoyin, Cheng Chang, chengchingwen, Chong Yan, Choong Yin Thong, Christopher Yeh, Clayne Robison, Coady, Patrick, Dan Ganea, David Norman, Denis Khalikov, Deven Desai, Diego Caballero, Duncan Dean, Duncan Riach, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Fangjun Kuang, Fei Hu, fo40225, formath, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, George Grzegorz Pawelczak, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, haison, Haraldur T\u00f3Mas Hallgr\u00edMsson, HarikrishnanBalagopal, H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, Jeroen B\u00e9Dorf, Jerry Shih, jerryyin, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Joon, Josh Beal, Julian Niedermeier, Jun Wan, Junqin Zhang, Junyuan Xie, Justin Tunis, Kaixi Hou, Karl Lessard, Karthik Muthuraman, Kbhute-Ibm, khanhlvg, Koock Yoon, kstuedem, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, minds, mpppk, musikisomorphie, Nagy Mostafa, Nayana Thorat, Neil, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, ocjosen, olramde, Pariksheet Pinjari, Patrick J. Lopresti, Patrik Gustavsson, per1234, PeterLee, Phan Van Nguyen Duc, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, richardbrks, robert, RonLek, Ryan Jiang, saishruthi, Saket Khandelwal, Saleem Abdulrasool, Sami Kama, Sana-Damani, Sergii Khomenko, Severen Redwood, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, Srini511, srinivasan.narayanamoorthy, Sumesh Udayakumaran, Sungmann Cho, Tae-Hwan Jung, Taehoon Lee, Takeshi Watanabe, TengLu, terryky, TheMindVirus, ThisIsIsaac, Till Hoffmann, Timothy Liu, Tomer Gafner, Tongxuan Liu, Trent Lo, Trevor Morris, Uday Bondhugula, Vasileios Lioutas, vbvg2008, Vishnuvardhan Janapati, Vivek Suryamurthy, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xinan Jiang, Xinping Wang, Yann-Yy, Yasir Modak, Yong Tang, Yongfeng Gu, Yuchen Ying, Yuxin Wu, zyeric, \u738b\u632f\u534e (Zhenhua Wang)\n\n# Release 2.0.0\n\n## Major Features and Improvements\n\nTensorFlow 2.0 focuses on **simplicity** and **ease of use**, featuring updates like:\n\n* Easy model building with Keras and eager execution.\n* Robust model deployment in production on any platform.\n* Powerful experimentation for research.\n* API simplification by reducing duplication and removing deprecated endpoints.\n\nFor details on best practices with 2.0, see [the Effective 2.0 guide](https://www.tensorflow.org/beta/guide/effective_tf2)\n\n\nFor information on upgrading your existing TensorFlow 1.x models, please refer to our [Upgrade](https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5) and [Migration](https://www.tensorflow.org/beta/guide/migration_guide) guides. We have also released a collection of [tutorials and getting started guides](https://www.tensorflow.org/beta).\n\n## Highlights\n\n*   TF 2.0 delivers Keras as the central high level API used to build and train\n    models. Keras provides several model-building APIs such as Sequential,\n    Functional, and Subclassing along with eager execution, for immediate\n    iteration and intuitive debugging, and `tf.data`, for building scalable\n    input pipelines. Checkout\n    [guide](https://www.tensorflow.org/beta/guide/keras/overview) for additional\n    details.\n*   Distribution Strategy: TF 2.0 users will be able to use the\n    [`tf.distribute.Strategy`](https://www.tensorflow.org/beta/guide/distribute_strategy)\n    API to distribute training with minimal code changes, yielding great\n    out-of-the-box performance. It supports distributed training with Keras\n    model.fit, as well as with custom training loops. Multi-GPU support is\n    available, along with experimental support for multi worker and Cloud TPUs.\n    Check out the\n    [guide](https://www.tensorflow.org/beta/guide/distribute_strategy) for more\n    details.\n*   Functions, not Sessions. The traditional declarative programming model of\n    building a graph and executing it via a `tf.Session` is discouraged, and\n    replaced with by writing regular Python functions. Using the `tf.function`\n    decorator, such functions can be turned into graphs which can be executed\n    remotely, serialized, and optimized for performance.\n*   Unification of `tf.train.Optimizers` and `tf.keras.Optimizers`. Use\n    `tf.keras.Optimizers` for TF2.0. `compute_gradients` is removed as public\n    API, use `GradientTape` to compute gradients.\n*   AutoGraph translates Python control flow into TensorFlow expressions,\n    allowing users to write regular Python inside `tf.function`-decorated\n    functions. AutoGraph is also applied in functions used with tf.data,\n    tf.distribute and tf.keras APIs.\n*   Unification of exchange formats to SavedModel. All TensorFlow ecosystem\n    projects (TensorFlow Lite, TensorFlow JS, TensorFlow Serving, TensorFlow\n    Hub) accept SavedModels. Model state should be saved to and restored from\n    SavedModels.\n*   API Changes: Many API symbols have been renamed or removed, and argument\n    names have changed. Many of these changes are motivated by consistency and\n    clarity. The 1.x API remains available in the compat.v1 module. A list of\n    all symbol changes can be found\n    [here](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0).\n    *   API clean-up, included removing `tf.app`, `tf.flags`, and `tf.logging`\n        in favor of [absl-py](https://github.com/abseil/abseil-py).\n*   No more global variables with helper methods like\n    `tf.global_variables_initializer` and `tf.get_global_step`.\n*   Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()`\n    for enabling/disabling v2 control flow.\n*   Enable v2 control flow as part of `tf.enable_v2_behavior()` and\n    `TF2_BEHAVIOR=1`.\n*   Fixes autocomplete for most TensorFlow API references by switching to use\n    relative imports in API `__init__.py` files.\n*   Auto Mixed-Precision graph optimizer simplifies converting models to\n    `float16` for acceleration on Volta and Turing Tensor Cores. This feature\n    can be enabled by wrapping an optimizer class with\n    `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.\n*   Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to \"true\" or \"1\"\n    forces the selection of deterministic cuDNN convolution and max-pooling\n    algorithms. When this is enabled, the algorithm selection procedure itself\n    is also deterministic.\n\n## Breaking Changes\n* Many backwards incompatible API changes have been made to clean up the APIs and make them more consistent.\n* Toolchains:\n  * TensorFlow 2.0.0 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\n  * Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\n  Removed the `freeze_graph` command line tool; `SavedModel` should be used in place of frozen graphs.\n\n* `tf.contrib`:\n  * `tf.contrib` has been deprecated, and functionality has been either migrated to the core TensorFlow API, to an ecosystem project such as [tensorflow/addons](https://www.github.com/tensorflow/addons) or [tensorflow/io](https://www.github.com/tensorflow/io), or removed entirely.\n  * Remove `tf.contrib.timeseries` dependency on TF distributions.\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in `early_stopping.py`.\n\n* `tf.estimator`:\n  * Premade estimators in the tf.estimator.DNN/Linear/DNNLinearCombined family have been updated to use `tf.keras.optimizers` instead of the `tf.compat.v1.train.Optimizer`s. If you do not pass in an `optimizer=` arg or if you use a string, the premade estimator will use the Keras optimizer. This is checkpoint breaking, as the optimizers have separate variables. A checkpoint converter tool for converting optimizers is included with the release,  but if you want to avoid any change, switch to the v1 version of the estimator:  `tf.compat.v1.estimator.DNN/Linear/DNNLinearCombined*`.\n  * Default aggregation for canned Estimators is now `SUM_OVER_BATCH_SIZE`. To maintain previous default behavior, please pass `SUM` as the loss aggregation method.\n  * Canned Estimators don\u2019t support `input_layer_partitioner` arg in the API. If you have this arg, you will have to switch to `tf.compat.v1 canned Estimators`.\n  * `Estimator.export_savedmodel` has been renamed to `export_saved_model`.\n  * When saving to SavedModel, Estimators will strip default op attributes. This is almost always the correct behavior, as it is more forwards compatible, but if you require that default attributes to be saved with the model, please use `tf.compat.v1.Estimator`.\n  * Feature Columns have been upgraded to be more Eager-friendly and to work with Keras. As a result, `tf.feature_column.input_layer` has been deprecated in favor of `tf.keras.layers.DenseFeatures`. v1 feature columns have direct analogues in v2 except for `shared_embedding_columns`, which are not cross-compatible with v1 and v2. Use `tf.feature_column.shared_embeddings` instead.\n\n* `tf.keras`:\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config.  To configure the number of threads, use `tf.config.threading` APIs.\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel. HDF5 files are still supported.\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\n  * Layers now default to float32, and automatically cast their inputs to the layer's dtype. If you had a model that used float64, it will probably silently use float32 in TensorFlow 2, and a warning will be issued that starts with `Layer <layer-name>` is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\n\n* `tf.lite`:\n  * Removed `lite.OpHint`, `lite.experimental`, and `lite.constant` from 2.0 API.\n* Tensors are no longer hashable, but instead compare element-wise with `==` and `!=`. Use `tf.compat.v1.disable_tensor_equality()` to return to the previous behavior.\n* Performing equality operations on Tensors or Variables with incompatible shapes an exception is no longer thrown. Instead `__eq__` returns False and `__ne__` returns True.\n* Removed `tf.string_split` from v2 API.\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\n* Add `UnifiedGRU` as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from `hard_sigmoid` to `sigmoid`, and `reset_after` to True in 2.0. Historically recurrent activation is `hard_sigmoid` since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\n* `CUDNN_INSTALL_PATH`, `TENSORRT_INSTALL_PATH`, `NCCL_INSTALL_PATH`, `NCCL_HDR_PATH` are deprecated. Use `TF_CUDA_PATHS` instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\n\nRefer to our [public project status tracker](https://github.com/orgs/tensorflow/projects/4) and [issues tagged with `2.0`](https://github.com/tensorflow/tensorflow/issues?q=is%3Aopen+is%3Aissue+label%3A2.0) on GitHub for insight into recent issues and development progress.\n\nIf you experience any snags when using TF 2.0, please let us know at the [TF 2.0 Testing User Group](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/testing). We have a support mailing list as well as weekly testing meetings, and would love to hear your migration feedback and questions.\n\n\n## Bug Fixes and Other Changes\n\n*   `tf.contrib`:\n\n    *   Expose `tf.contrib.proto.*` ops in `tf.io` (they will exist in TF2)\n\n*   `tf.data`:\n\n    *   Add support for TensorArrays to `tf.data Dataset`.\n    *   Integrate Ragged Tensors with `tf.data`.\n    *   All core and experimental tf.data transformations that input\n        user-defined functions can span multiple devices now.\n    *   Extending the TF 2.0 support for `shuffle(...,\n        reshuffle_each_iteration=True)` and `cache()` to work across different\n        Python iterators for the same dataset.\n    *   Removing the `experimental_numa_aware` option from `tf.data.Options`.\n    *   Add `num_parallel_reads` and passing in a Dataset containing filenames\n        into `TextLineDataset` and `FixedLengthRecordDataset`.\n    *   Add support for defaulting the value of `cycle_length` argument of\n        `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\n    *   Promoting `tf.data.experimental.enumerate_dataset` to core as\n        `tf.data.Dataset.enumerate`.\n    *   Promoting `tf.data.experimental.unbatch` to core as\n        `tf.data.Dataset.unbatch`.\n    *   Adds option for introducing slack in the pipeline to reduce CPU\n        contention, via `tf.data.Options().experimental_slack = True`\n    *   Added experimental support for parallel batching to `batch()` and\n        `padded_batch()`. This functionality can be enabled through\n        `tf.data.Options()`.\n    *   Support cancellation of long-running `reduce`.\n    *   Now we use `dataset` node name as prefix instead of the op name, to\n        identify the component correctly in metrics, for pipelines with repeated\n        components.\n    *   Improve the performance of datasets using `from_tensors()`.\n    *   Promoting `unbatch` from experimental to core API.\n    *   Adding support for datasets as inputs to `from_tensors` and\n        `from_tensor_slices` and batching and unbatching of nested datasets.\n\n*   `tf.distribute`:\n\n    *   Enable `tf.distribute.experimental.MultiWorkerMirroredStrategy` working\n        in eager mode.\n    *   Callbacks are supported in `MultiWorkerMirroredStrategy`.\n    *   Disable `run_eagerly` and distribution strategy if there are symbolic\n        tensors added to the model using `add_metric` or `add_loss`.\n    *   Loss and gradients should now more reliably be correctly scaled w.r.t.\n        the global batch size when using a `tf.distribute.Strategy`.\n    *   Set default loss reduction as `AUTO` for improving reliability of loss\n        scaling with distribution strategy and custom training loops. `AUTO`\n        indicates that the reduction option will be determined by the usage\n        context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`.\n        When used in distribution strategy scope, outside of built-in training\n        loops such as `tf.keras` `compile` and `fit`, we expect reduction value\n        to be 'None' or 'SUM'. Using other values will raise an error.\n    *   Support for multi-host `ncclAllReduce` in Distribution Strategy.\n\n*   `tf.estimator`:\n\n    *   Replace `tf.contrib.estimator.add_metrics` with\n        `tf.estimator.add_metrics`\n    *   Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs`\n    *   Replace contrib references with `tf.estimator.experimental.*` for apis\n        in early_s in Estimator\n    *   Canned Estimators will now use keras optimizers by default. An error\n        will be raised if tf.train.Optimizers are used, and you will have to\n        switch to tf.keras.optimizers or tf.compat.v1 canned Estimators.\n    *   A checkpoint converter for canned Estimators has been provided to\n        transition canned Estimators that are warm started from\n        `tf.train.Optimizers` to `tf.keras.optimizers`.\n    *   Losses are scaled in canned estimator v2 and not in the optimizers\n        anymore. If you are using Estimator + distribution strategy + optimikzer\n        v1 then the behavior does not change. This implies that if you are using\n        custom estimator with optimizer v2, you have to scale losses. We have\n        new utilities to help scale losses `tf.nn.compute_average_loss`,\n        `tf.nn.scale_regularization_loss`.\n\n*   `tf.keras`:\n\n    *   Premade models (including Linear and WideDeep) have been introduced for\n        the purpose of replacing Premade estimators.\n    *   Model saving changes\n    *   `model.save` and `tf.saved_model.save` may now save to the TensorFlow\n        SavedModel format. The model can be restored using\n        `tf.keras.models.load_model`. HDF5 files are still supported, and may be\n        used by specifying `save_format=\"h5\"` when saving.\n    *   Raw TensorFlow functions can now be used in conjunction with the Keras\n        Functional API during model creation. This obviates the need for users\n        to create Lambda layers in most cases when using the Functional API.\n        Like Lambda layers, TensorFlow functions that result in Variable\n        creation or assign ops are not supported.\n    *   Add support for passing list of lists to the `metrics` argument in Keras\n        `compile`.\n    *   Add `tf.keras.layers.AbstractRNNCell` as the preferred implementation\n        for RNN cells in TF v2. User can use it to implement RNN cells with\n        custom behavior.\n    *   Keras training and validation curves are shown on the same plot when\n        using the TensorBoard callback.\n    *   Switched Keras `fit/evaluate/predict` execution to use only a single\n        unified path by default unless eager execution has been explicitly\n        disabled, regardless of input type. This unified path places an\n        eager-friendly training step inside of a `tf.function`. With this\n    *   All input types are converted to `Dataset`.\n    *   The path assumes there is always a distribution strategy. when\n        distribution strategy is not specified the path uses a no-op\n        distribution strategy.\n    *   The training step is wrapped in `tf.function` unless `run_eagerly=True`\n        is set in compile. The single path execution code does not yet support\n        all use cases. We fallback to the existing v1 execution paths if your\n        model contains the following:\n        1.  `sample_weight_mode` in compile\n        2.  `weighted_metrics` in compile\n        3.  v1 optimizer\n        4.  target tensors in compile If you are experiencing any issues because\n            of this change, please inform us (file an issue) about your use case\n            and you can unblock yourself by setting\n            `experimental_run_tf_function=False` in compile meanwhile. We have\n            seen couple of use cases where the model usage pattern is not as\n            expected and would not work with this change.\n    *   output tensors of one layer is used in the constructor of another.\n    *   symbolic tensors outside the scope of the model are used in custom loss\n        functions. The flag can be disabled for these cases and ideally the\n        usage pattern will need to be fixed.\n    *   Mark Keras `set_session` as `compat.v1` only.\n    *   `tf.keras.estimator.model_to_estimator` now supports exporting to\n        `tf.train.Checkpoint format`, which allows the saved checkpoints to be\n        compatible with `model.load_weights`.\n    *   `keras.backend.resize_images` (and consequently,\n        `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing\n        implementation was fixed.\n    *   Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D`\n        and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor`\n        to store weights, allowing a dramatic speedup for large sparse models.\n    *   Raise error if `batch_size` argument is used when input is\n        dataset/generator/keras sequence.\n    *   Update TF 2.0 `keras.backend.name_scope` to use TF 2.0 `name_scope`.\n    *   Add v2 module aliases for losses, metrics, initializers and optimizers:\n        `tf.losses = tf.keras.losses` & `tf.metrics = tf.keras.metrics` &\n        `tf.initializers = tf.keras.initializers` & `tf.optimizers =\n        tf.keras.optimizers`.\n    *   Updates binary cross entropy logic in Keras when input is probabilities.\n        Instead of converting probabilities to logits, we are using the cross\n        entropy formula for probabilities.\n    *   Added public APIs for `cumsum` and `cumprod` keras backend functions.\n    *   Add support for temporal sample weight mode in subclassed models.\n    *   Raise `ValueError` if an integer is passed to the training APIs.\n    *   Added fault-tolerance support for training Keras model via `model.fit()`\n        with `MultiWorkerMirroredStrategy`, tutorial available.\n    *   Custom Callback tutorial is now available.\n    *   To train with `tf.distribute`, Keras API is recommended over estimator.\n    *   `steps_per_epoch` and `steps` arguments are supported with numpy arrays.\n    *   New error message when unexpected keys are used in\n        sample_weight/class_weight dictionaries\n    *   Losses are scaled in Keras compile/fit and not in the optimizers\n        anymore. If you are using custom training loop, we have new utilities to\n        help scale losses `tf.nn.compute_average_loss`,\n        `tf.nn.scale_regularization_loss`.\n    *   `Layer` apply and add_variable APIs are deprecated.\n    *   Added support for channels first data format in cross entropy losses\n        with logits and support for tensors with unknown ranks.\n    *   Error messages will be raised if `add_update`, `add_metric`, `add_loss`,\n        activity regularizers are used inside of a control flow branch.\n    *   New loss reduction types:\n    *   `AUTO`: Indicates that the reduction option will be determined by the\n        usage context. For almost all cases this defaults to\n        `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside\n        of built-in training loops such as `tf.keras` `compile` and `fit`, we\n        expect reduction value to be `SUM` or `NONE`. Using `AUTO` in that case\n        will raise an error.\n    *   `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis\n        specified by loss function). When this reduction type used with built-in\n        Keras training loops like `fit`/`evaluate`, the unreduced vector loss is\n        passed to the optimizer but the reported loss will be a scalar value.\n    *   `SUM`: Scalar sum of weighted losses. 4. `SUM_OVER_BATCH_SIZE`: Scalar\n        `SUM` divided by number of elements in losses. This reduction type is\n        not supported when used with `tf.distribute.Strategy` outside of\n        built-in training loops like `tf.keras` `compile`/`fit`.\n    *   Wraps losses passed to the `compile` API (strings and v1 losses) which\n        are not instances of v2 `Loss` class in `LossWrapper` class. => All\n        losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\n    *   `model.add_loss(symbolic_tensor)` should work in ambient eager.\n    *   Update metric name to always reflect what the user has given in compile.\n        Affects following cases\n    *   When name is given as 'accuracy'/'crossentropy'\n    *   When an aliased function name is used eg. 'mse'\n    *   Removing the `weighted` prefix from weighted metric names.\n    *   Allow non-Tensors through v2 losses.\n    *   Add v2 sparse categorical crossentropy metric.\n    *   Add v2 APIs for `AUCCurve` and `AUCSummationMethod` enums.\n    *   `add_update` can now be passed a zero-arg callable in order to support\n        turning off the update when setting `trainable=False` on a Layer of a\n        Model compiled with `run_eagerly=True`.\n    *   Standardize the LayerNormalization API by replacing the args `norm_axis`\n        and `params_axis` with `axis`.\n    *   Fixed critical bugs that help with DenseFeatures usability in TF2\n\n*   `tf.lite`:\n\n    *   Added evaluation script for `COCO` minival\n    *   Add delegate support for `QUANTIZE`.\n    *   Add `GATHER` support to NN API delegate.\n    *   Added support for TFLiteConverter Python API in 2.0. Contains functions\n        from_saved_model, from_keras_file, and from_concrete_functions.\n    *   Add `EXPAND_DIMS` support to NN API delegate TEST.\n    *   Add `narrow_range` attribute to QuantizeAndDequantizeV2 and V3.\n    *   Added support for `tflite_convert` command line tool in 2.0.\n    *   Post-training quantization tool supports quantizing weights shared by\n        multiple operations. The models made with versions of this tool will use\n        INT8 types for weights and will only be executable interpreters from\n        this version onwards.\n    *   Post-training quantization tool supports fp16 weights and GPU delegate\n        acceleration for fp16.\n    *   Add delegate support for `QUANTIZED_16BIT_LSTM`.\n    *   Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\n\n*   TensorRT\n\n    *   Add TensorFlow 2.0-compatible `TrtGraphConverterV2` API for TensorRT\n        conversion. TensorRT initialization arguments are now passed wrapped in\n        a named-tuple, `TrtConversionParams`, rather than as separate arguments\n        as in `TrtGraphConverter`.\n    *   Changed API to optimize TensorRT engines during graph optimization. This\n        is now done by calling `converter.build()` where previously\n        `is_dynamic_op=False` would be set.\n    *   `converter.convert()` no longer returns a `tf.function`. Now the\n        function must be accessed from the saved model.\n    *   The `converter.calibrate()` method has been removed. To trigger\n        calibration, a `calibration_input_fn` should be provided to\n        `converter.convert()`.\n\n*   Other:\n\n    *   Fix accidental quadratic graph construction cost in graph-mode\n        `tf.gradients()`.\n    *   ResourceVariable's gather op supports batch dimensions.\n    *   ResourceVariable support for `gather_nd`.\n    *   `ResourceVariable` and `Variable` no longer accepts `constraint` in the\n        constructor, nor expose it as a @property.\n    *   Added gradient for `SparseToDense` op.\n    *   Expose a flag that allows the number of threads to vary across Python\n        benchmarks.\n    *   `image.resize` in 2.0 now supports gradients for the new resize kernels.\n    *   `image.resize` now considers proper pixel centers and has new kernels\n        (incl. anti-aliasing).\n    *   Renamed `tf.image` functions to remove duplicate \"image\" where it is\n        redundant.\n    *   Variadic reduce is supported on CPU Variadic reduce is supported on CPU\n    *   Remove unused `StringViewVariantWrapper`.\n    *   Delete unused `Fingerprint64Map` op registration\n    *   Add broadcasting support to `tf.matmul`.\n    *   Add C++ Gradient for `BatchMatMulV2`.\n    *   Add `tf.math.cumulative_logsumexp` operation.\n    *   Add ellipsis (...) support for `tf.einsum()`.\n    *   Add expand_composites argument to all `nest.*` methods.\n    *   Added `strings.byte_split`.\n    *   Add a new \"result_type\" parameter to `tf.strings.split`.\n    *   Add name argument to `tf.string_split` and `tf.strings_split`.\n    *   Extend `tf.strings.split` to support inputs with any rank.\n    *   Added `tf.random.binomial`.\n    *   Added `key` and `skip` methods to `random.experimental.Generator`.\n    *   Extend `tf.function` with basic support for CompositeTensors arguments\n        (such as `SparseTensor` and `RaggedTensor`).\n    *   `parallel_for.pfor`: add converters for Softmax, LogSoftmax, IsNaN, All,\n        Any, and MatrixSetDiag.\n    *   `parallel_for`: add converters for LowerTriangularSolve and Cholesky.\n    *   `parallel_for`: add converters for `LogMatrixDeterminant` and\n        `MatrixBandPart`.\n    *   `parallel_for`: Add converter for `MatrixDiag`.\n    *   `parallel_for`: Add converters for `OneHot`, `LowerBound`, `UpperBound`.\n    *   `parallel_for`: add converter for `BroadcastTo`.\n    *   Add `pfor` converter for `Squeeze`.\n    *   Add `RaggedTensor.placeholder()`.\n    *   Add ragged tensor support to `tf.squeeze`.\n    *   Update RaggedTensors to support int32 row_splits.\n    *   Allow `LinearOperator.solve` to take a `LinearOperator`.\n    *   Allow all dtypes for `LinearOperatorCirculant`.\n    *   Introduce MaxParallelism method\n    *   Add `LinearOperatorHouseholder`.\n    *   Adds Philox support to new stateful RNG's XLA path.\n    *   Added `TensorSpec` support for CompositeTensors.\n    *   Added `tf.linalg.tridiagonal_solve` op.\n    *   Added partial_pivoting input parameter to `tf.linalg.tridiagonal_solve`.\n    *   Added gradient to `tf.linalg.tridiagonal_solve`.\n    *   Added `tf.linalg.tridiagonal_mul op`.\n    *   Added GPU implementation of `tf.linalg.tridiagonal_matmul`.\n    *   Added `LinearOperatorToeplitz`.\n    *   Upgraded LIBXSMM to version 1.11.\n    *   Uniform processing of quantized embeddings by Gather and EmbeddingLookup\n        Ops.\n    *   Correct a misstatement in the documentation of the sparse softmax cross\n        entropy logit parameter.\n    *   Add `tf.ragged.boolean_mask`.\n    *   `tf.switch_case` added, which selects a branch_fn based on a\n        branch_index.\n    *   The C++ kernel of gather op supports batch dimensions.\n    *   Fixed default value and documentation for `trainable` arg of\n        tf.Variable.\n    *   `EagerTensor` now supports numpy buffer interface for tensors.\n    *   This change bumps the version number of the `FullyConnected` Op to 5.\n    *   Added new op: `tf.strings.unsorted_segment_join`.\n    *   Added HW acceleration support for `topK_v2`.\n    *   CloudBigtable version updated to v0.10.0 BEGIN_PUBLIC CloudBigtable\n        version updated to v0.10.0.\n    *   Expose `Head` as public API.\n    *   Added `tf.sparse.from_dense` utility function.\n    *   Improved ragged tensor support in `TensorFlowTestCase`.\n    *   Added a function `nested_value_rowids` for ragged tensors.\n    *   Added `tf.ragged.stack`.\n    *   Makes the a-normal form transformation in Pyct configurable as to which\n        nodes are converted to variables and which are not.\n    *   `ResizeInputTensor` now works for all delegates.\n    *   `tf.cond` emits a StatelessIf op if the branch functions are stateless\n        and do not touch any resources.\n    *   Add support of local soft device placement for eager op.\n    *   Pass partial_pivoting to the `_TridiagonalSolveGrad`.\n    *   Add HW acceleration support for `LogSoftMax`.\n    *   Add guard to avoid acceleration of L2 Normalization with input rank != 4\n    *   Fix memory allocation problem when calling `AddNewInputConstantTensor`.\n    *   Delegate application failure leaves interpreter in valid state\n    *   `tf.while_loop` emits a StatelessWhile op if the cond and body functions\n        are stateless and do not touch any resources.\n    *   `tf.cond`, `tf.while` and if and while in AutoGraph now accept a\n        nonscalar predicate if has a single element. This does not affect non-V2\n        control flow.\n    *   Fix potential security vulnerability where decoding variant tensors from\n        proto could result in heap out of bounds memory access.\n    *   Only create a GCS directory object if the object does not already exist.\n    *   Introduce `dynamic` constructor argument in Layer and Model, which\n        should be set to `True` when using imperative control flow in the `call`\n        method.\n    *   Begin adding Go wrapper for C Eager API.\n    *   XLA HLO graphs can be inspected with interactive_graphviz tool now.\n    *   Add dataset ops to the graph (or create kernels in Eager execution)\n        during the python Dataset object creation instead doing it during\n        Iterator creation time.\n    *   Add `batch_dims` argument to `tf.gather`.\n    *   The behavior of `tf.gather` is now correct when `axis=None` and\n        `batch_dims<0`.\n    *   Update docstring for gather to properly describe the non-empty\n        `batch_dims` case.\n    *   Removing of dtype in the constructor of initializers and partition_info\n        in call.\n    *   Add `tf.math.nextafter` op.\n    *   Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically\n        dispatches the best kernel implementation based on CPU vector\n        architecture. To disable them, build with\n        `--define=tensorflow_mkldnn_contraction_kernel=0`.\n    *   `tf.linspace(start, stop, num)` now always uses \"stop\" as last value\n        (for num > 1)\n    *   Added top-k to precision and recall to keras metrics.\n    *   Add a ragged size op and register it to the op dispatcher\n    *   Transitive dependencies on :`pooling_ops` were removed. Some users may\n        need to add explicit dependencies on :`pooling_ops` if they reference\n        the operators from that library.\n    *   Add `CompositeTensor` base class.\n    *   Malformed gif images could result in an access out of bounds in the\n        color palette of the frame. This has been fixed now\n    *   Add templates and interfaces for creating lookup tables\n    *   `Tensor::UnsafeCopyFromInternal` deprecated in favor\n        `Tensor::BitcastFrom`.\n    *   In `map_vectorization` optimization, reduce the degree of parallelism in\n        the vectorized map node.\n    *   Add variant wrapper for `absl::string_view`.\n    *   Add OpKernels for some stateless maps.\n    *   DType is no longer convertible to an int. Use `dtype.as_datatype_enum`\n        instead of `int(dtype)` to get the same result.\n    *   Support both binary and -1/1 label input in v2 hinge and squared hinge\n        losses.\n    *   Added `LinearOperator.adjoint` and `LinearOperator.H` (alias).\n    *   Expose CriticalSection in core as `tf.CriticalSection`.\n    *   Enhanced graphviz output.\n    *   Add opkernel templates for common table operations.\n    *   Fix callbacks do not log values in eager mode when a deferred build\n        model is used.\n    *   `SignatureDef` util functions have been deprecated.\n    *   Update `Fingerprint64Map` to use aliases\n    *   Add legacy string flat hash map op kernels.\n    *   Add support for `add_metric` in the graph function mode.\n    *   Updating cosine similarity loss - removed the negate sign from cosine\n        similarity.\n    *   Changed default for gradient accumulation for TPU embeddings to true.\n    *   Adds summary trace API for collecting graph and profile information.\n    *   The `precision_mode` argument to `TrtGraphConverter` is now case\n        insensitive.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n1e100, a6802739, 4d55397500, a6802739, Abdullah Selek, abenmao, Abolfazl\nShahbazi, Adam Richter, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Alex, Alex\nItkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Aman Patel,\nAmit, Amit Kumar Jaiswal, Amit Srivastava, amoitra, Andreas Eberle, Andrew\nLihonosov, Andy Craze, Anshuman Tripathy, Anthony Hsu, Anthony Platanios, Anuj\nRawat, arp95, Arpit Shah, Armen Poghosov, armenpoghosov, Astropeak, Ashwin\nRamaswami, Arpit Shah, Augustina Ragwitz, Aurelien Geron, Aur\u00e9Lien Geron,\navasid, aweers, awesomealex1, Ayush Agrawal, Bas Aarts, Bastian Eichenberger,\nBairen Yi, Bayberry Z, Ben Barsdell, Benjamin Peterson, bhack, Bharat\nRaghunathan, Bhavani Subramanian, Bin Fan, blairhan, Bl\u00e9Nesi Attila, Bodin-E,\nBrandon Carter, Bryan Cutler, candy.dc, Cao Zongyan, Casper Da Costa-Luis, Chao\nLiu, Chen Guoyin, chenchc, chengchingwen, chie8842, Christian Hansen, Christoph\nBoeddeker, Christopher Yeh, Clayne Robison, Coady, Patrick, crafet, csukuangfj,\nctiijima, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Daniel Rasmussen, Daniel\nSalvadori, Dave Airlie, David Norman, Dayananda V, delock, Denis Khalikov, Deven\nDesai, Dheeraj Rajaram Reddy, Diego Caballero, dmitrievanthony, Donovan Ong,\nDrew Szurko, Duncan Dean, Duncan Riach, Dustin Neighly, Dwight J Lyle, Eamon\nIto-Fisher, eashtian3, Edward Forgacs, EFanZh, ejot, Elroy Ashtian Jr, Eric\nSchweitz, Evgeniy Polyakov, Fangjun Kuang, Federico Martinez, Fei Hu, Felix\nLemke, Filip Matzner, FlashTek, fo40225, formath, Franc\u0327Ois Chollet, frreiss,\nFred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel,\nGautam, gehring, Geoffrey Irving, George Grzegorz Pawelczak, Grzegorz Pawelczak,\nGeorge Sterpu, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein,\nGurpreet Singh, Gustavo Lima Chaves, Gyoung-Yoon Ryoo, haison, Hanton Yang,\nHanGuo97, Haraldur T\u00f3Mas Hallgr\u00edMsson, Hari Shankar, hehongliang, Heungsub Lee,\nHoeseong Kim, Huan Li (\u674e\u5353\u6853), H\u00e5Kon Sandsmark, I-Hong, I-Hong Jhuo, Ilham\nFirdausi Putra, Ilango R, Imran Salam, Innovimax, Jacky Ko, Irene Dea, Ivan\nHabernal, Jakub Lipinski, Jacky, Jason Zaman, Jason Zavaglia, jayhpark530,\njcf94, jefby, Jeff Daily, Jeff Poznanovic, Jeffrey Poznanovic, Jekyll Lai, jer,\nJeroen B\u00e9Dorf, jerryyin, jhalakp, jiakai, Jia Qingtong, Jiankang, JiangXIAO, Joe\nBowser, Joe Q, Joe Quadrino, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese,\nJonas Rauber, Jonathan Kyl, Jonathan, Joon, Joppe Geluykens, Joseph Friedman,\nJosh Beal, jtressle, Julian Niedermeier, Junqin Zhang, Justin Dujardin, Justin\nTunis, jwu, K. Hodges, kaixih, Kaixi Hou, kjopek, Karl Lessard, Karl\nWeinmeister, Karthik Muthuraman, Kashif Rasul, Kay Zhu, Kbhute-Ibm, KDR, Keno\nFischer, Kevin Mader, khanhlvg, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan,\nKoock Yoon, kouml, ktaebum, Kyuwon Kim, Lakshay Tokas, Laurent Le Brun,\nleike666666, leonard951, Leslie-Fang, Letian Kang, Li, Guizi, Loo Rong Jie,\nLucas Hendren, Lukas Folle, Lukas Geiger, Luke Han, luxupu, lvli, Ma, Guokai,\nMahmoud Abuzaina, Maksym Kysylov, Mandar Deshpande, manhyuk, Manraj Singh\nGrover, Marco Gaido, Marek Drozdowski, Margaret Maynard-Reid, Mark Ryan, mars20,\nMateusz Chudyk, Matt Conley, mbhuiyan, mdfaijul, Mei Jie, Melissa Grueter,\nmerturl, MichaelKonobeev, Michael K\u00e4Ufl, Michal W. Tarnowski, Micka\u00ebL\nSchoentgen, Miguel Morin, Mihail Salnikov, Mikalai Drabovich, Mike Arpaia, Mike\nHolcomb, minds, monklof, Moses Marin, mpppk, Mr. Metal, Mshr-H, musikisomorphie,\nnammbash, Natalia Gimelshein, Nathan Luehr, Nayana-Ibm, Nayana Thorat, neargye,\nNeeraj Pradhan, Nehal J Wani, Neil, Nick, Nick Lewycky, Niels Ole Salscheider,\nNiklas Silfverstr\u00f6M, Niranjan Hasabnis, Nuka-137, Nutti, ocjosen, olicht,\nomeir1, P Sudeepam, Paige Bailey, Palmer Lao, Pan Daoxin, Pariksheet Pinjari,\nPasquale Minervini, Patrick J. Lopresti, Patrik Gustavsson, Pavel Akhtyamov,\nPavel Samolysov, PENGWA, per1234, PeterLee, Phan Van Nguyen Duc, Philipp Jund,\nPhillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, R\nS Nikhil Krishna, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben\nMorais, robert, Rohit Gupta, Roland Zimmermann, Roman Soldatow, RonLek, Ruizhe,\nRyan Jiang, saishruthi, Saleem Abdulrasool, Samantha Andow, Sami Kama,\nSana-Damani, Saurabh Deoras, sdamani, Sean Morgan, seanshpark, Sebastien Iooss,\nServ-Inc, Severen Redwood, Shahzad Lone, Shashank Gupta, shashvat, Shashvat\nChand Shahi, Shubham Goyal, Shashi, Sigrid Keydana, Siju, Siju Samuel,\nsleighsoft, smilu97, Snease-Abq, Son Tran, Spencer Schaber, sremedios, Srini511,\nsrinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Subin, Sumesh Udayakumaran,\nSungmann Cho, sunway513, Supriya Rao, sxwang, Tae-Hwan Jung, Taehoon Lee, Takeo\nSawada, Taylor Jakobson, Taylor Thornton, Ted Chang, TengLu, terryky,\nThisIsIsaac, ThisIsPIRI, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Till\nHoffmann, Tim Zaman, tomguluson92, Tongxuan Liu, Trent Lo, Trevor Morris,\nTungJerry, Tyorden, Uday Bondhugula, v1incent, Vagif, Vasileios Lioutas,\nvbvg2008, vcarpani, Vijay Ravichandran, Vikram Tiwari,Viktor Gal, Vishwak\nSrinivasan, Vincent, Vishnuvardhan Janapati, Vitor-Alves, Vivek Suryamurthy,\nwangsiyu, wateryzephyr, WeberXie, Wei Wang, WeijieSun, Wen-Heng (Jack) Chung,\nwenxizhu, Will Battel, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui,\nXiaoquan Kong, Xin, Xinping Wang, Yan Facai (\u989c\u53d1\u624d), Yann-Yy, Yasir Modak,\nYasuhiro Matsumoto, ymodak, Yong Tang, Yongfeng Gu, Younes Khoudli, Yuan Lin,\nYuan (Terry) Tang, Yuchen Ying, Yves-Noel Weweler, zhangyujing, zjjott, zyeric,\n\u738b\u632f\u534e (Zhenhua Wang), \u9ec4\u946b\n\n# Release 1.14.0\n\n## Major Features and Improvements\n\n*   This is the first 1.x release containing the compat.v2 module. This module\n    is required to allow libraries to publish code which works in both 1.x and\n    2.x. After this release, no backwards incompatible changes are allowed in\n    the 2.0 Python API.\n*   Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically\n    dispatches the best kernel implementation based on CPU vector architecture.\n    To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\n\n## Behavioral changes\n\n*   Set default loss reduction as `AUTO` for improving reliability of loss\n    scaling with distribution strategy and custom training loops. `AUTO`\n    indicates that the reduction option will be determined by the usage context.\n    For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in\n    distribution strategy scope, outside of built-in training loops such as\n    `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or\n    'SUM'. Using other values will raise an error.\n*   Wraps losses passed to the `compile` API (strings and v1 losses) which are\n    not instances of v2 `Loss` class in `LossWrapper` class. => All losses will\n    now use `SUM_OVER_BATCH_SIZE` reduction as default.\n*   Disable `run_eagerly` and distribution strategy if there are symbolic\n    tensors added to the model using `add_metric` or `add_loss`.\n*   tf.linspace(start, stop, num) now always uses \"stop\" as last value (for\n    num > 1)\n*   `ResourceVariable` and `Variable` no longer accepts `constraint` in the\n    constructor, nor expose it as a @property.\n*   The behavior of tf.gather is now correct when axis=None and batch_dims<0.\n*   Only create a GCS directory object if the object does not already exist.\n*   In `map_vectorization` optimization, reduce the degree of parallelism in the\n    vectorized map node.\n*   Bug fix: loss and gradients should now more reliably be correctly scaled\n    w.r.t. the global batch size when using a tf.distribute.Strategy.\n*   Updating cosine similarity loss - removed the negate sign from cosine\n    similarity.\n*   DType is no longer convertible to an int. Use dtype.as_datatype_enum instead\n    of int(dtype) to get the same result.\n*   Changed default for gradient accumulation for TPU embeddings to true.\n*   Callbacks now log values in eager mode when a deferred build model is used.\n*   Transitive dependencies on :pooling_ops were removed. Some users may need to\n    add explicit dependencies on :pooling_ops if they reference the operators\n    from that library.\n*   tf.keras.optimizers default learning rate changes:\n    *   Adadelta: 1.000 to 0.001\n    *   Adagrad: 0.01 to 0.001\n    *   Adamax: 0.002 to 0.001\n    *   NAdam: 0.002 to 0.001\n\n## Bug Fixes and Other Changes\n\n*   Documentation\n*   Deprecations and Symbol renames.\n    *   Remove unused StringViewVariantWrapper\n    *   Delete unused Fingerprint64Map op registration\n    *   SignatureDef util functions have been deprecated.\n    *   Renamed tf.image functions to remove duplicate \"image\" where it is\n        redundant.\n    *   tf.keras.experimental.export renamed to\n        tf.keras.experimental.export_saved_model\n    *   Standardize the LayerNormalization API by replacing the args `norm_axis`\n        and `params_axis` with `axis`.\n    *   Tensor::UnsafeCopyFromInternal deprecated in favor Tensor::BitcastFrom\n*   Keras & Python API\n    *   Add v2 module aliases for:\n    *   tf.initializers => tf.keras.initializers\n    *   tf.losses => tf.keras.losses & tf.metrics => tf.keras.metrics\n    *   tf.optimizers => tf.keras.optimizers\n    *   Add tf.keras.layers.AbstractRNNCell as the preferred implementation of\n        RNN cell for TF v2. User can use it to implement RNN cell with custom\n        behavior.\n    *   Adding `clear_losses` API to be able to clear losses at the end of\n        forward pass in a custom training loop in eager.\n    *   Add support for passing list of lists to the `metrics` param in Keras\n        `compile`.\n    *   Added top-k to precision and recall to keras metrics.\n    *   Adding public APIs for `cumsum` and `cumprod` keras backend functions.\n    *   Fix: model.add_loss(symbolic_tensor) should work in ambient eager.\n    *   Add name argument to tf.string_split and tf.strings_split\n    *   Minor change to SavedModels exported from Keras using\n        tf.keras.experimental.export. (SignatureDef key for evaluation mode is\n        now \"eval\" instead of \"test\"). This will be reverted back to \"test\" in\n        the near future.\n    *   Updates binary cross entropy logic in Keras when input is probabilities.\n        Instead of converting probabilities to logits, we are using the cross\n        entropy formula for probabilities.\n    *   Raw TensorFlow functions can now be used in conjunction with the Keras\n        Functional API during model creation. This obviates the need for users\n        to create Lambda layers in most cases when using the Functional API.\n        Like Lambda layers, TensorFlow functions that result in Variable\n        creation or assign ops are not supported.\n    *   Keras training and validation curves are shown on the same plot.\n    *   Introduce `dynamic` constructor argument in Layer and Model, which\n        should be set to True when using imperative control flow in the `call`\n        method.\n    *   Removing of dtype in the constructor of initializers and partition_info\n        in call.\n*   New ops and improved op functionality\n    *   Add OpKernels for some stateless maps\n    *   Add v2 APIs for AUCCurve and AUCSummationMethod\n        enums. #tf-metrics-convergence\n    *   Add tf.math.nextafter op.\n    *   Add CompositeTensor base class.\n    *   Add tf.linalg.tridiagonal_solve op.\n    *   Add opkernel templates for common table operations.\n    *   Added support for TFLite in TensorFlow 2.0.\n    *   Adds summary trace API for collecting graph and profile information.\n    *   Add batch_dims argument to tf.gather.\n    *   Add support for `add_metric` in the graph function mode.\n    *   Add C++ Gradient for BatchMatMulV2.\n    *   Added tf.random.binomial\n    *   Added gradient for SparseToDense op.\n    *   Add legacy string flat hash map op kernels\n    *   Add a ragged size op and register it to the op dispatcher\n    *   Add broadcasting support to tf.matmul.\n    *   Add ellipsis (...) support for tf.einsum()\n    *   Added LinearOperator.adjoint and LinearOperator.H (alias).\n    *   Added GPU implementation of tf.linalg.tridiagonal_solve.\n    *   Added strings.byte_split\n    *   Add RaggedTensor.placeholder()\n    *   Add a new \"result_type\" parameter to tf.strings.split\n    *   `add_update` can now be passed a zero-arg callable in order to support\n        turning off the update when setting `trainable=False` on a Layer of a\n        Model compiled with `run_eagerly=True`.\n    *   Add variant wrapper for absl::string_view\n    *   Add expand_composites argument to all nest.* methods.\n    *   Add pfor converter for Squeeze.\n    *   Bug fix for tf.tile gradient\n    *   Expose CriticalSection in core as tf.CriticalSection.\n    *   Update Fingerprint64Map to use aliases\n    *   ResourceVariable support for gather_nd.\n    *   ResourceVariable's gather op supports batch dimensions.\n    *   Variadic reduce is supported on CPU\n    *   Extend tf.function with basic support for CompositeTensors arguments\n        (such as SparseTensor and RaggedTensor).\n    *   Add templates and interfaces for creating lookup tables\n    *   Post-training quantization tool supports quantizing weights shared by\n        multiple operations. The models made with versions of this tool will use\n        INT8 types for weights and will only be executable interpreters from\n        this version onwards.\n    *   Malformed gif images could result in an access out of bounds in the\n        color palette of the frame. This has been fixed now\n    *   image.resize now considers proper pixel centers and has new kernels\n        (incl. anti-aliasing).\n    *   Added an isotonic regression solver (tf.nn.isotonic_regression).\n*   Performance\n    *   Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically\n        dispatches the best kernel implementation based on CPU vector\n        architecture. To disable them, build with\n        --define=tensorflow_mkldnn_contraction_kernel=0.\n    *   Support for multi-host ncclAllReduce in Distribution Strategy.\n    *   Expose a flag that allows the number of threads to vary across Python\n        benchmarks.\n*   TensorFlow 2.0 Development\n    *   Add v2 sparse categorical crossentropy metric.\n    *   Allow non-Tensors through v2 losses.\n    *   Add UnifiedGRU as the new GRU implementation for tf2.0. Change the\n        default recurrent activation function for GRU from 'hard_sigmoid' to\n        'sigmoid', and 'reset_after' to True in 2.0. Historically recurrent\n        activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new\n        unified backend between CPU and GPU mode, since the CuDNN kernel is\n        using sigmoid, we change the default for CPU mode to sigmoid as well.\n        With that, the default GRU will be compatible with both CPU and GPU\n        kernel. This will enable user with GPU to use CuDNN kernel by default\n        and get a 10x performance boost in training. Note that this is\n        checkpoint breaking change. If user want to use their 1.x pre-trained\n        checkpoint, please construct the layer with\n        GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback\n        to 1.x behavior.\n    *   TF 2.0 - Update metric name to always reflect what the user has given in\n        compile. Affects following cases 1. When name is given as\n        'accuracy'/'crossentropy' 2. When an aliased function name is used eg.\n        'mse' 3. Removing the `weighted` prefix from weighted metric names.\n    *   Begin adding Go wrapper for C Eager API\n    *   image.resize in 2.0 now supports gradients for the new resize kernels.\n    *   removed tf.string_split from v2 API\n    *   Expose tf.contrib.proto.* ops in tf.io (they will exist in TF2)\n    *   \"Updates the TFLiteConverter API in 2.0. Changes from_concrete_function\n        to from_concrete_functions.\"\n    *   Enable tf.distribute.experimental.MultiWorkerMirroredStrategy working in\n        eager mode.\n    *   Support both binary and -1/1 label input in v2 hinge and squared hinge\n        losses.\n*   TensorFlow Lite\n    *   \"Adds support for tflite_convert in 2.0.\"\n    *   \"Remove lite.OpHint, lite.experimental, and lite.constant from 2.0 API.\"\n*   tf.contrib\n    *   Added Neural Turing Implementation as described in\n        https://arxiv.org/abs/1807.08518.\n    *   Remove tf.contrib.timeseries dependency on TF distributions.\n*   tf.data\n    *   Add num_parallel_reads and passing in a Dataset containing filenames\n        into TextLineDataset and FixedLengthRecordDataset\n    *   Going forward we operate in TF 2.0, this change is part of the effort to\n        slowly converting XYZDataset to DatasetV2 type which is the official\n        version going to be used in TF 2.0 and motivated by some compatibility\n        issue found, _BigtableXYZDataset (of type DatasetV2) does not implement\n        the _as_variant_tensor() of DatasetV1, when moving contrib.bigtable to\n        tensorflow_io. Converting into DatasetV2 removes the overheads to\n        maintain V1 while we are moving into TF 2.0.\n    *   Add dataset ops to the graph (or create kernels in Eager execution)\n        during the python Dataset object creation instead doing it during\n        Iterator creation time.\n    *   Add support for TensorArrays to tf.data Dataset.\n    *   Switching tf.data functions to use `defun`, providing an escape hatch to\n        continue using the legacy `Defun`.\n*   Toolchains\n    *   CUDNN_INSTALL_PATH, TENSORRT_INSTALL_PATH, NCCL_INSTALL_PATH,\n        NCCL_HDR_PATH are deprecated. Use TF_CUDA_PATHS instead which supports a\n        comma-separated list of base paths that are searched to find CUDA\n        libraries and headers.\n    *   TF code now resides in `tensorflow_core` and `tensorflow` is just a\n        virtual pip package. No code changes are needed for projects using\n        TensorFlow, the change is transparent\n*   XLA\n    *   XLA HLO graphs can be inspected with interactive_graphviz tool now.\n*   Estimator\n    *   Use tf.compat.v1.estimator.inputs instead of tf.estimator.inputs\n    *   Replace contrib references with tf.estimator.experimental.* for apis in\n        early_stopping.py\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n1e100, 4d55397500, a6802739, abenmao, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy,\nAlex, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, Andreas Eberle,\nAndy Craze, Anthony Platanios, Armen Poghosov, armenpoghosov, arp95, Arpit Shah,\nAshwin Ramaswami, Aurelien Geron, Aur\u00e9Lien Geron, aweers, awesomealex1, Ayush\nAgrawal, Ben Barsdell, Bharat Raghunathan, Bhavani Subramanian, blairhan,\nBl\u00e9Nesi Attila, Brandon Carter, candy.dc, Chao Liu, chenchc, chie8842, Christian\nHansen, Christian Sigg, Clayne Robison, crafet, csukuangfj, ctiijima, Dan\nJarvis, Dan Lazewatsky, Daniel Ingram, Daniel Salvadori, Dave Airlie, David\nNorman, Dayananda V, Dayananda-V, delock, Denis Khalikov, Deven Desai, Dheeraj\nRajaram Reddy, dmitrievanthony, Donovan Ong, Drew Szurko, Duncan Riach, Dustin\nNeighly, Edward Forgacs, EFanZh, Fei Hu, Felix Lemke, Filip Matzner, fo40225,\nfrreiss, Gautam, gehring, Geoffrey Irving, Grzegorz George Pawelczak, Grzegorz\nPawelczak, Gyoung-Yoon Ryoo, HanGuo97, Hanton Yang, Hari Shankar, hehongliang,\nHeungsub Lee, Hoeseong Kim, I-Hong Jhuo, Ilango R, Innovimax, Irene Dea, Jacky\nKo, Jakub Lipinski, Jason Zaman, jcf94, Jeffrey Poznanovic, Jens Elofsson,\nJeroen B\u00e9Dorf, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Joeran Beel, Jonas\nRauber, Jonathan, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu,\nK Yasaswi Sri Chandra Gandhi, K. Hodges, Kaixi Hou, Karl Lessard, Karl\nWeinmeister, Karthik Muthuraman, Kashif Rasul, KDR, Keno Fischer, Kevin Mader,\nkjopek, Koan-Sin Tan, kouml, ktaebum, Lakshay Tokas, Laurent Le Brun, Letian\nKang, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, luxupu,\nMa, Guokai, Mahmoud Abuzaina, Mandar Deshpande, manhyuk, Marco Gaido, Marek\nDrozdowski, Mark Collier, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley,\nMattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL\nSchoentgen, Miguel Morin, Mihail Salnikov, Mike Arpaia, Mike Holcomb, monklof,\nMoses Marin, Mshr-H, nammbash, Natalia Gimelshein, Nayana-Ibm, neargye, Neeraj\nPradhan, Nehal J Wani, Nick, Niels Ole Salscheider, Niranjan Hasabnis, nlewycky,\nNuka-137, Nutti, olicht, P Sudeepam, Palmer Lao, Pan Daoxin, Pariksheet Pinjari,\nPavel Samolysov, PENGWA, Pooya Davoodi, R S Nikhil Krishna, Rohit Gupta, Roman\nSoldatow, rthadur, Ruizhe, Ryan Jiang, Samantha Andow, Sami Kama, Sana-Damani,\nSaurabh Deoras, sdamani, seanshpark, Sebastien Iooss, Serv-Inc, Shahzad Lone,\nShashank Gupta, Shashi, shashvat, shashvatshahi1998, Siju, Siju Samuel,\nSnease-Abq, Spencer Schaber, sremedios, srinivasan.narayanamoorthy, Steve Lang,\nSteve Nesae, Sumesh Udayakumaran, Supriya Rao, Taylor Jakobson, Taylor Thornton,\nTed Chang, ThisIsPIRI, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Tim Zaman,\ntomguluson92, Tongxuan Liu, TungJerry, v1incent, Vagif, vcarpani, Vikram Tiwari,\nVishwak Srinivasan, Vitor-Alves, wangsiyu, wateryzephyr, WeberXie, WeijieSun,\nWen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, wyzhao, Xin,\nYasuhiro Matsumoto, ymodak, Yong Tang, Younes Khoudli, Yuan Lin, Yves-Noel\nWeweler, Zantares, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Wang Zhenhua), \u9ec4\u946b\n\n# Release 1.12.3\n\n## Bug Fixes and Other Changes\n\n*   Updates `png_archive` dependency to 1.6.37 to not be affected by\n    CVE-2019-7317, CVE-2018-13785, and CVE-2018-14048.\n*   Updates `sqlite` dependency to 3.28.0 to not be affected by CVE-2018-20506,\n    CVE-2018-20346, and CVE-2018-20505.\n\n# Release 1.12.2\n\n## Bug Fixes and Other Changes\n\n*   Fixes a potential security vulnerability where carefully crafted GIF images\n    can produce a null pointer dereference during decoding.\n\n# Release 1.13.0\n\n## Major Features and Improvements\n\n* TensorFlow Lite has moved from contrib to core. This means that Python modules are under `tf.lite` and source code is now under `tensorflow/lite` rather than `tensorflow/contrib/lite`.\n* TensorFlow GPU binaries are now built against CUDA 10 and TensorRT 5.0.\n* Support for Python3.7 on all operating systems.\n* Moved NCCL to core.\n\n## Behavioral changes\n\n* Disallow conversion of python floating types to uint32/64 (matching behavior of other integer types) in `tf.constant`.\n* Make the `gain` argument of convolutional orthogonal initializers (`convolutional_delta_orthogonal`, `convolutional_orthogonal_1D`, `convolutional_orthogonal_2D`, `convolutional_orthogonal_3D`) have consistent behavior with the `tf.initializers.orthogonal` initializer, i.e. scale the output l2-norm by `gain` and NOT by `sqrt(gain)`. (Note that these functions are currently in `tf.contrib` which is not guaranteed backward compatible).\n\n## Bug Fixes and Other Changes\n\n*   Documentation\n    *   Update the doc with the details about the rounding mode used in\n        quantize_and_dequantize_v2.\n    *   Clarify that tensorflow::port::InitMain() _should_ be called before\n        using the TensorFlow library. Programs failing to do this are not\n        portable to all platforms.\n*   Deprecations and Symbol renames.\n    *   Removing deprecations for the following endpoints: `tf.acos`,\n        `tf.acosh`, `tf.add`, `tf.as_string`, `tf.asin`, `tf.asinh`, `tf.atan`,\n        `tf.atan2`, `tf.atanh`, `tf.cos`, `tf.cosh`, `tf.equal`, `tf.exp`,\n        `tf.floor`, `tf.greater`, `tf.greater_equal`, `tf.less`,\n        `tf.less_equal`, `tf.log`, `tf.logp1`, `tf.logical_and`,\n        `tf.logical_not`, `tf.logical_or`, `tf.maximum`, `tf.minimum`,\n        `tf.not_equal`, `tf.sin`, `tf.sinh`, `tf.tan`\n    *   Deprecate `tf.data.Dataset.shard`.\n    *   Deprecate `saved_model.loader.load` which is replaced by\n        `saved_model.load` and `saved_model.main_op`, which will be replaced by\n        `saved_model.main_op` in V2.\n    *   Deprecate tf.QUANTIZED_DTYPES. The official new symbol is\n        tf.dtypes.QUANTIZED_DTYPES.\n    *   Update sklearn imports for deprecated packages.\n    *   Deprecate `Variable.count_up_to` and `tf.count_up_to` in favor of\n        `Dataset.range`.\n    *   Export `confusion_matrix` op as `tf.math.confusion_matrix` instead of\n        `tf.train.confusion_matrix`.\n    *   Add `tf.dtypes.` endpoint for every constant in dtypes.py. Moving\n        endpoints in versions.py to corresponding endpoints in `tf.sysconfig.`\n        and `tf.version.`. Moving all constants under `tf.saved_model`\n        submodules to `tf.saved_model` module. New endpoints are added in V1 and\n        V2 but existing endpoint removals are only applied in V2.\n    *   Deprecates behavior where device assignment overrides collocation\n        constraints inside a collocation context manager.\n*   Keras & Python API\n    *   Add to Keras functionality analogous to\n        `tf.register_tensor_conversion_function`.\n    *   Subclassed Keras models can now be saved through\n        `tf.contrib.saved_model.save_keras_model`.\n    *   `LinearOperator.matmul` now returns a new `LinearOperator`.\n*   New ops and improved op functionality\n    *   Add a Nearest Neighbor Resize op.\n    *   Add an `ignore_unknown` argument to `parse_values` which suppresses\n        ValueError for unknown hyperparameter types. Such * Add\n        `tf.linalg.matvec` convenience function.\n    *   `tf.einsum()`raises `ValueError` for unsupported equations like\n        `\"ii->\"`.\n    *   Add DCT-I and IDCT-I in `tf.signal.dct` and `tf.signal.idct`.\n    *   Add LU decomposition op.\n    *   Add quantile loss to gradient boosted trees in estimator.\n    *   Add `round_mode` to `QuantizeAndDequantizeV2` op to select rounding\n        algorithm.\n    *   Add `unicode_encode`, `unicode_decode`, `unicode_decode_with_offsets`,\n        `unicode_split`, `unicode_split_with_offset`, and `unicode_transcode`\n        ops. Amongst other things, this Op adds the ability to encode, decode,\n        and transcode a variety of input text encoding formats into the main\n        Unicode encodings (UTF-8, UTF-16-BE, UTF-32-BE)\n    *   Add \"unit\" attribute to the substr op, which allows obtaining the\n        substring of a string containing unicode characters.\n    *   Broadcasting support for Ragged Tensors.\n    *   `SpaceToDepth` supports uint8 data type.\n    *   Support multi-label quantile regression in estimator.\n    *   We now use \"div\" as the default partition_strategy in\n        `tf.nn.safe_embedding_lookup_sparse`, `tf.nn.sampled_softmax` and\n        `tf.nn.nce_loss`. hyperparameter are ignored.\n*   Performance\n    *   Improve performance of GPU cumsum/cumprod by up to 300x.\n    *   Added support for weight decay in most TPU embedding optimizers,\n        including AdamW and MomentumW.\n*   TensorFlow 2.0 Development\n    *   Add a command line tool to convert to TF2.0, tf_upgrade_v2\n    *   Merge `tf.spectral` into `tf.signal` for TensorFlow 2.0.\n    *   Change the default recurrent activation function for LSTM from\n        'hard_sigmoid' to 'sigmoid' in 2.0. Historically recurrent activation is\n        'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend\n        between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we\n        change the default for CPU mode to sigmoid as well. With that, the\n        default LSTM will be compatible with both CPU and GPU kernel. This will\n        enable user with GPU to use CuDNN kernel by default and get a 10x\n        performance boost in training. Note that this is checkpoint breaking\n        change. If user want to use their 1.x pre-trained checkpoint, please\n        construct the layer with LSTM(recurrent_activation='hard_sigmoid') to\n        fallback to 1.x behavior.\n*   TensorFlow Lite\n    *   Move from `tensorflow/contrib/lite` to `tensorflow/lite`.\n    *   Add experimental Java API for injecting TensorFlow Lite delegates\n    *   Add support for strings in TensorFlow Lite Java API.\n*   `tf.contrib`:\n    *   Add Apache Ignite Filesystem plugin to support accessing Apache IGFS.\n    *   Dropout now takes `rate` argument, `keep_prob` is deprecated.\n    *   Estimator occurrences references `tf.contrib.estimator` were changed to\n        `tf.estimator`:\n    *   `tf.contrib.estimator.BaselineEstimator` with\n        `tf.estimator.BaselineEstimator`\n    *   `tf.contrib.estimator.DNNLinearCombinedEstimator` with\n        `tf.estimator.DNNLinearCombinedEstimator`\n    *   `tf.contrib.estimator.DNNEstimator` with `tf.estimator.DNNEstimator`\n    *   `tf.contrib.estimator.LinearEstimator` with\n        `tf.estimator.LinearEstimator`\n    *   `tf.contrib.estimator.InMemoryEvaluatorHook` and\n        tf.estimator.experimental.InMemoryEvaluatorHook`.\n    *   `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with\n        `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`.\n    *   Expose `tf.distribute.Strategy as the new name for\n        tf.contrib.distribute.DistributionStrategy.\n    *   Migrate linear optimizer from contrib to core.\n    *   Move `tf.contrib.signal` to `tf.signal` (preserving aliases in\n        tf.contrib.signal).\n    *   Users of `tf.contrib.estimator.export_all_saved_models` and related\n        should switch to\n        `tf.estimator.Estimator.experimental_export_all_saved_models`.\n*   tf.data:\n    *   Add `tf.data.experimental.StatsOptions()`, to configure options to\n        collect statistics from `tf.data.Dataset` pipeline using\n        `StatsAggregator`. Add nested option, `experimental_stats` (which takes\n        a `tf.data.experimen tal.StatsOptions` object), to `tf.data.Options`.\n        Deprecates `tf.data.experimental.set_stats_agregator`.\n    *   Performance optimizations:\n    *   Add `tf.data.experimental.OptimizationOptions()`, to configure options\n        to enable `tf.data` performance optimizations. Add nested option,\n        `experimental_optimization` (which takes a\n        `tf.data.experimental.OptimizationOptions` object), to\n        `tf.data.Options`. Remove performance optimization options from\n        `tf.data.Options`, and add them under\n        `tf.data.experimental.OptimizationOptions` instead.\n    *   Enable `map_and_batch_fusion` and `noop_elimination` optimizations by\n        default. They can be disabled by configuring\n        `tf.data.experimental.OptimizationOptions` to set `map_and_batch =\n        False` or `noop_elimination = False` respectively. To disable all\n        default optimizations, set `apply_default_optimizations = False`.\n    *   Support parallel map in `map_and_filter_fusion`.\n    *   Disable static optimizations for input pipelines that use non-resource\n        `tf.Variable`s.\n    *   Add NUMA-aware MapAndBatch dataset.\n    *   Deprecate `tf.data.Dataset.make_one_shot_iterator()` in V1, removed it\n        from V2, and added tf.compat.v1.data.make_one_shot_iterator()`.\n    *   Deprecate `tf.data.Dataset.make_initializable_iterator()` in V1, removed\n        it from V2, and added `tf.compat.v1.data.make_initializable_iterator()`.\n    *   Enable nested dataset support in core `tf.data` transformations.\n    *   For `tf.data.Dataset` implementers: Added\n        `tf.data.Dataset._element_structured property` to replace\n        `Dataset.output_{types,shapes,classes}`.\n    *   Make `num_parallel_calls` of `tf.data.Dataset.interleave` and\n        `tf.data.Dataset.map` work in Eager mode.\n*   Toolchains\n    *   Fixed OpenSSL compatibility by avoiding `EVP_MD_CTX_destroy`.\n    *   Added bounds checking to printing deprecation warnings.\n    *   Upgraded CUDA dependency to 10.0\n    *   To build with Android NDK r14b, add \"#include <linux/compiler.h>\" to\n        android-ndk-r14b/platforms/android-14/arch-*/usr/include/linux/futex.h\n    *   Removed `:android_tensorflow_lib_selective_registration*` targets, use\n        `:android_tensorflow_lib_lite*` targets instead.\n*   XLA\n    *   Move `RoundToEven` function to xla/client/lib/math.h.\n    *   A new environment variable `TF_XLA_DEBUG_OPTIONS_PASSTHROUGH` set to \"1\"\n        or \"true\" allows the debug options passed within an XRTCompile op to be\n        passed directly to the XLA compilation backend. If such variable is not\n        set (service side), only a restricted set will be passed through.\n    *   Allow the XRTCompile op to return the ProgramShape resulted form the XLA\n        compilation as a second return argument.\n    *   XLA HLO graphs can now be rendered as SVG/HTML.\n*   Estimator\n    *   Replace all occurrences of `tf.contrib.estimator.BaselineEstimator` with\n        `tf.estimator.BaselineEstimator`\n    *   Replace all occurrences of\n        `tf.contrib.estimator.DNNLinearCombinedEstimator` with\n        `tf.estimator.DNNLinearCombinedEstimator`\n    *   Replace all occurrences of `tf.contrib.estimator.DNNEstimator` with\n        `tf.estimator.DNNEstimator`\n    *   Replace all occurrences of `tf.contrib.estimator.LinearEstimator` with\n        `tf.estimator.LinearEstimator`\n    *   Users of `tf.contrib.estimator.export_all_saved_models` and related\n        should switch to\n        `tf.estimator.Estimator.experimental_export_all_saved_models`.\n    *   Update `regression_head` to the new Head API for Canned Estimator V2.\n    *   Switch `multi_class_head` to Head API for Canned Estimator V2.\n    *   Replace all occurrences of `tf.contrib.estimator.InMemoryEvaluatorHook`\n        and `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with\n        `tf.estimator.experimental.InMemoryEvaluatorHook` and\n        `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`\n    *   Migrate linear optimizer from contrib to core.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAbhinav Upadhyay, Ag Ramesh, akikaaa, Alexis Louis, Anders Huss, Andreas Madsen, Andrew Banchich, Andy Craze, Anton Dmitriev, Artem Malykh, Avijit-Nervana, Balint Cristian, Benjamin Tan Wei Hao, Bhavani Subramanian, Brendan Finan, Brian Nemsick, Bryan Cutler, By Shen, Cao Zongyan, Castiel, Chris Antaki, Christian Goll, Cibifang, Clayne Robison, Codrut Grosu, Cong Xu, Dalmo Cirne, Daniel Hunter, Dougal J. Sutherland, Edvard Fagerholm, EFanZh, Erik Smistad, Evgeniy Polyakov, Feiyang Chen, franklin5, Fred Reiss, Gautam, gehring, Geoffrey Irving, George Sterpu, Gitea, Grzegorz George Pawelczak, Guozhong Zhuang, himkt, Hoeseong Kim, Huan Li (\u674e\u5353\u6853), HuiyangFei, hyunyoung, Isaac Burbank, jackonan, Jacky Ko, Jason Furmanek, Jason Zaman, Javier Luraschi, Jiang,Zhoulong, joaak, John Lin, Jonathan Wyatt Hoech, josephyearsley, Josh Gordon, Julian Niedermeier, Karl Lessard, Keno Fischer, lanhin, Leon Graser, leondgarse, Li, Guizi, Li, Yiqiang, lxl910915, Mahmoud Abuzaina, manhyuk, Marcela Morales Quispe, margaretmz, Matt Conley, Max Pumperla, mbhuiyan, mdfaijul, Meng, Peng, Michael, Michael Gielda, mrTsjolder, Muhammad Wildan, neargye, Nehal J Wani, NEWPLAN, Niranjan Hasabnis, Nutti, olicht, Pan Daoxin, Pedro Monreal, Peng Yu, pillarpond, Pooya Davoodi, qiezi, Rholais Lii, Richard Yu, Rin Arakaki, Roger Iyengar, sahilbadyal, Sami Kama, Sandip Giri, Scott Leishman, Serge Panev, Seunghoon Park, Shafi Dayatar, shengfuintel, Shimin Guo, Siju, silent567, Stefan Dyulgerov, steven, Tao Wei, Thor Johnsen, Tingbo Lu, tomguluson92, Tongxuan Liu, Trevor Morris, Ubuntu, Vadim Borisov, vanderliang, wangsiyu, Wen Yun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Xiaoming (Jason) Cui, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yaniv Blumenfeld, Yash Gaurkar, Yicheng Fan, Yong Tang, Yongjoon Lee, Yuan (Terry) Tang, Yuxin Wu, zldrobit\n\n# Release 1.12.0\n\n## Major Features and Improvements\n\n*   Keras models can now be directly exported to the SavedModel\n    format(`tf.contrib.saved_model.save_keras_model()`) and used with Tensorflow\n    Serving.\n*   Keras models now support evaluating with a `tf.data.Dataset`.\n*   TensorFlow binaries are built with XLA support linked in by default.\n*   Ignite Dataset added to contrib/ignite that allows to work with Apache\n    Ignite.\n\n## Bug Fixes and Other Changes\n\n*   tf.data:\n    *   tf.data users can now represent, get, and set options of TensorFlow\n        input pipelines using `tf.data.Options()`, `tf.data.Dataset.options()`,\n        and `tf.data.Dataset.with_options()` respectively.\n    *   New `tf.data.Dataset.reduce()` API allows users to reduce a finite\n        dataset to a single element using a user-provided reduce function.\n    *   New `tf.data.Dataset.window()` API allows users to create finite windows\n        of input dataset; when combined with the `tf.data.Dataset.reduce()` API,\n        this allows users to implement customized batching.\n    *   All C++ code moves to the `tensorflow::data` namespace.\n    *   Add support for `num_parallel_calls` to `tf.data.Dataset.interleave`.\n*   `tf.contrib`:\n    *   Remove `tf.contrib.linalg`. `tf.linalg` should be used instead.\n    *   Replace any calls to `tf.contrib.get_signature_def_by_key(metagraph_def,\n        signature_def_key)` with\n        `meta_graph_def.signature_def[signature_def_key]`. Catching a ValueError\n        exception thrown by `tf.contrib.get_signature_def_by_key` should be\n        replaced by catching a KeyError exception.\n*   `tf.contrib.data`\n    *   Deprecate, and replace by tf.data.experimental.\n*   Other:\n    *   Instead of jemalloc, revert back to using system malloc since it\n        simplifies build and has comparable performance.\n    *   Remove integer types from `tf.nn.softplus` and `tf.nn.softsign` OpDefs.\n        This is a bugfix; these ops were never meant to support integers.\n    *   Allow subslicing Tensors with a single dimension.\n    *   Add option to calculate string length in Unicode characters.\n    *   Add functionality to SubSlice a tensor.\n    *   Add searchsorted (ie lower/upper_bound) op.\n    *   Add model explainability to Boosted Trees.\n    *   Support negative positions for tf.substr.\n    *   There was previously a bug in the bijector_impl where the\n        _reduce_jacobian_det_over_event does not handle scalar ILDJ\n        implementations properly.\n    *   In tf eager execution, allow re-entering a GradientTape context.\n    *   Add tf_api_version flag. If --define=tf_api_version=2 flag is passed in,\n        then bazel will build TensorFlow API version 2.0. Note that TensorFlow\n        2.0 is under active development and has no guarantees at this point.\n    *   Add additional compression options to TfRecordWriter.\n    *   Performance improvements for regex full match operations.\n    *   Replace tf.GraphKeys.VARIABLES with `tf.GraphKeys.GLOBAL_VARIABLES`.\n    *   Remove unused dynamic learning rate support.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n(David) Siu-Kei Muk, Ag Ramesh, Anton Dmitriev, Artem Sobolev, Avijit-Nervana,\nBairen Yi, Bruno Goncalves, By Shen, candy.dc, Cheng Chen, Clayne Robison,\ncoder3101, Dao Zhang, Elms, Fei Hu, feiquan, Geoffrey Irving, Guozhong Zhuang,\nhellcom, Hoeseong Kim, imsheridan, Jason Furmanek, Jason Zaman, Jenny Sahng,\njiefangxuanyan, Johannes Bannhofer, Jonathan Homer, Koan-Sin Tan, kouml, Loo\nRong Jie, Lukas Geiger, manipopopo, Ming Li, Moritz Kr\u00f6Ger, Naurril, Niranjan\nHasabnis, Pan Daoxin, Peng Yu, pengwa, rasmi, Roger Xin, Roland Fernandez, Sami\nKama, Samuel Matzek, Sangjung Woo, Sergei Lebedev, Sergii Khomenko, shaohua,\nShaohua Zhang, Shujian2015, Sunitha Kambhampati, tomguluson92, Vin\u00edCius Camargo,\nwangsiyu, weidankong, Wen-Heng (Jack) Chung, William D. Irons, Xin Jin, Yan\nFacai (\u989c\u53d1\u624d), Yanbo Liang, Yash Katariya, Yong Tang, \u5728\u539f\u4f50\u4e3a\n\n# Release 1.11.0\n\n## Major Features and Improvements\n\n*   Nvidia GPU:\n    *   Prebuilt binaries are now (as of TensorFlow 1.11) built against cuDNN\n        7.2 and TensorRT 4. See updated install guides:\n        [Installing TensorFlow on Ubuntu](https://www.tensorflow.org/install/install_linux#tensorflow_gpu_support)\n*   Google Cloud TPU:\n    *   Experimental tf.data integration for Keras on Google Cloud TPUs.\n    *   Experimental / preview support for eager execution on Google Cloud TPUs.\n*   DistributionStrategy:\n    *   Add multi-GPU DistributionStrategy support in tf.keras. Users can now\n        use `fit`, `evaluate` and `predict` to distribute their model on\n        multiple GPUs.\n    *   Add multi-worker DistributionStrategy and standalone client support in\n        Estimator. See\n        [README](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute)\n        for more details.\n*   Add C, C++, and Python functions for querying kernels.\n\n## Breaking Changes\n\n* Keras:\n  * The default values for tf.keras `RandomUniform`, `RandomNormal`, and `TruncatedNormal` initializers have been changed to match those in external Keras.\n  * Breaking change: `model.get_config()` on a Sequential model now returns a config dictionary (consistent with other Model instances) instead of a list of configs for the underlying layers.\n\n## Bug Fixes and Other Changes\n\n*   C++:\n    *   Changed the signature of SessionFactory::NewSession so that it can\n        return a meaningful error message on failure.\n*   tf.data:\n    *   Remove `num_parallel_parser_calls` argument from\n        `tf.contrib.data.make_csv_dataset()`. [tf.data] Remove\n        `num_parallel_parser_calls` argument from\n        `tf.contrib.data.make_csv_dataset()`.\n    *   `tf.data.Dataset.list_files()` raises an exception at initialization\n        time if the argument matches no files.\n    *   Renamed BigTable class to BigtableTable for clarity\n    *   Document use of the Cloud Bigtable API\n    *   Add `tf.contrib.data.reduce_dataset` which can be used to reduce a\n        dataset to a single element.\n    *   Generalization of `tf.contrib.data.sliding_window_batch`.\n*   INC:\n    *   Runtime improvements to triangular solve.\n*   `tf.contrib`:\n    *   Add an `implementation` argument to `tf.keras.layers.LocallyConnected2D`\n        and `tf.keras.layers.LocallyConnected1D`. The new mode\n        (`implementation=2`) performs forward pass as a single dense matrix\n        multiplication, allowing dramatic speedups in certain scenarios (but\n        worse performance in others - see docstring). The option also allows to\n        use `padding=same`.\n    *   Add documentation clarifying the differences between tf.fill and\n        tf.constant.\n    *   Add experimental IndexedDatasets.\n    *   Add selective registration target using the lite proto runtime.\n    *   Add simple Tensor and DataType classes to TensorFlow Lite Java\n    *   Add support for bitcasting to/from uint32 and uint64.\n    *   Added a subclass of Estimator that can be created from a SavedModel\n        (SavedModelEstimator).\n    *   Adds leaf index modes as an argument.\n    *   Allow a different output shape from the input in\n        tf.contrib.image.transform.\n    *   Change the state_size order of the StackedRNNCell to be natural order.\n        To keep the existing behavior, user can add reverse_state_order=True\n        when constructing the StackedRNNCells.\n    *   Deprecate self.test_session() in favor of self.session() or\n        self.cached_session().\n    *   Directly import tensor.proto.h (the transitive import will be removed\n        from tensor.h soon).\n    *   Estimator.train() now supports tf.contrib.summary.\\* summaries out of\n        the box; each call to .train() will now create a separate tfevents file\n        rather than re-using a shared one.\n    *   Fix FTRL L2-shrinkage behavior: the gradient from the L2 shrinkage term\n        should not end up in the accumulator.\n    *   Fix toco compilation/execution on Windows.\n    *   GoogleZoneProvider class added to detect which Google Cloud Engine zone\n        tensorflow is running in.\n    *   It is now safe to call any of the C API's TF_Delete\\* functions on\n        nullptr.\n    *   Log some errors on Android to logcat.\n    *   Match FakeQuant numerics in TFLite to improve accuracy of TFLite\n        quantized inference models.\n    *   Optional bucket location check for the GCS Filesystem.\n    *   Performance enhancements for StringSplitOp & StringSplitV2Op.\n    *   Performance improvements for regex replace operations.\n    *   TFRecordWriter now raises an error if .write() fails.\n    *   TPU: More helpful error messages in TPUClusterResolvers.\n    *   The legacy_init_op argument to SavedModelBuilder methods for adding\n        MetaGraphs has been deprecated. Please use the equivalent main_op\n        argument instead. As part of this, we now explicitly check for a single\n        main_op or legacy_init_op at the time of SavedModel building, whereas\n        the check on main_op was previously only done at load time.\n    *   The protocol used for Estimator training is now configurable in\n        RunConfig.\n    *   Triangular solve performance improvements.\n    *   Unify RNN cell interface between TF and Keras. Add new\n        get_initial_state() to Keras and TF RNN cell, which will use to replace\n        the existing zero_state() method.\n    *   Update initialization of variables in Keras.\n    *   Updates to \"constrained_optimization\" in tensorflow/contrib.\n    *   boosted trees: adding pruning mode.\n    *   tf.train.Checkpoint does not delete old checkpoints by default.\n    *   tfdbg: Limit the total disk space occupied by dumped tensor data to 100\n        GBytes. Add environment variable `TFDBG_DISK_BYTES_LIMIT` to allow\n        adjustment of this upper limit.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAapeli, adoda, Ag Ramesh, Amogh Mannekote, Andrew Gibiansky, Andy Craze, Anirudh Koul, Aurelien Geron, Avijit, Avijit-Nervana, Ben, Benjamin H. Myara, bhack, Brett Koonce, Cao Zongyan, cbockman, cheerss, Chikanaga Tomoyuki, Clayne Robison, cosine0, Cui Wei, Dan J, David, David Norman, Dmitry Klimenkov, Eliel Hojman, Florian Courtial, fo40225, formath, Geoffrey Irving, gracehoney, Grzegorz Pawelczak, Guoliang Hua, Guozhong Zhuang, Herman Zvonimir Do\u0161Ilovi\u0107, HuiyangFei, Jacker, Jan H\u00fcNnemeyer, Jason Taylor, Jason Zaman, Jesse, Jiang,Zhoulong, Jiawei Zhang, Jie, Joe Yearsley, Johannes Schmitz, Jon Perl, Jon Triebenbach, Jonathan, Jonathan Hseu, Jongmin Park, Justin Shenk, karl@kubx.ca, Kate Hodesdon, Kb Sriram, Keishi Hattori, Kenneth Blomqvist, Koan-Sin Tan, Li Liangbin, Li, Yiqiang, Loo Rong Jie, Madiyar, Mahmoud Abuzaina, Mark Ryan, Matt Dodge, mbhuiyan, melvinljy96, Miguel Mota, Nafis Sadat, Nathan Luehr, naurril, Nehal J Wani, Niall Moran, Niranjan Hasabnis, Nishidha Panpaliya, npow, olicht, Pei Zhang, Peng Wang (Simpeng), Peng Yu, Philipp Jund, Pradeep Banavara, Pratik Kalshetti, qwertWZ, Rakesh Chada, Randy West, Ray Kim, Rholais Lii, Robin Richtsfeld, Rodrigo Silveira, Ruizhi, Santosh Kumar, Seb Bro, Sergei Lebedev, sfujiwara, Shaba Abhiram, Shashi, SneakyFish5, Soila Kavulya, Stefan Dyulgerov, Steven Winston, Sunitha Kambhampati, Surry Shome, Taehoon Lee, Thor Johnsen, Tristan Rice, TShapinsky, tucan, tucan9389, Vicente Reyes, Vilmar-Hillow, Vitaly Lavrukhin, wangershi, weidan.kong, weidankong, Wen-Heng (Jack) Chung, William D. Irons, Wim Glenn, XFeiF, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yong Tang, Yoshihiro Yamazaki, Yuan (Terry) Tang, Yuan, Man, zhaoyongke, \u00c1Ron\nRicardo Perez-Lopez, \u5f20\u5929\u542f, \u5f20\u6653\u98de\n\n\n# Release 1.10.1\n## Bug Fixes and Other Changes\n\n* `tf.keras`:\n  * Fixing keras on Cloud TPUs. No new binaries will be built for Windows.\n\n\n# Release 1.10.0\n\n## Major Features And Improvements\n\n* The `tf.lite` runtime now supports `complex64`.\n* Initial [Google Cloud Bigtable integration](https://github.com/tensorflow/tensorflow/tree/r1.10/tensorflow/contrib/bigtable) for `tf.data`.\n* Improved local run behavior in `tf.estimator.train_and_evaluate` which does not reload checkpoints for evaluation.\n* `RunConfig` now sets device_filters to restrict how workers and PS can communicate. This can speed up training and ensure clean shutdowns in some situations. But if you have jobs that require communication between workers, you will have to set custom session_options in your `RunConfig`.\n* Moved Distributions and Bijectors from `tf.contrib.distributions` to [Tensorflow Probability (TFP)](https://github.com/tensorflow/probability). `tf.contrib.distributions` is now deprecated and will be removed by the end of 2018.\n* Adding new endpoints for existing tensorflow symbols. These endpoints are going to be the preferred endpoints going forward and may replace some of the existing endpoints in the future. See below for the complete list. New symbols have been added to the following modules: [`tf.debugging`](https://www.tensorflow.org/versions/master/api_docs/python/tf/debugging), [`tf.dtypes`](https://www.tensorflow.org/versions/master/api_docs/python/tf/dtypes), [`tf.image`](https://www.tensorflow.org/versions/master/api_docs/python/tf/image), [`tf.io`](https://www.tensorflow.org/versions/master/api_docs/python/tf/io), [`tf.linalg`](https://www.tensorflow.org/versions/master/api_docs/python/tf/linalg), [`tf.manip`](https://www.tensorflow.org/versions/master/api_docs/python/tf/manip), [`tf.math`](https://www.tensorflow.org/versions/master/api_docs/python/tf/math), [`tf.quantization`](https://www.tensorflow.org/versions/master/api_docs/python/tf/quantization), [`tf.strings`](https://www.tensorflow.org/versions/master/api_docs/python/tf/strings)\n\n## Breaking Changes\n\n* Prebuilt binaries are now (as of TensorFlow 1.10) built against NCCL 2.2 and no longer include NCCL in the binary install. TensorFlow usage with multiple GPUs and NCCL requires upgrade to [NCCL 2.2](https://developer.nvidia.com/nccl). See updated install guides: [TensorFlow GPU support](https://www.tensorflow.org/install/gpu) and [Build TensorFlow from source](https://www.tensorflow.org/install/source).\n* Starting from TensorFlow 1.11, Windows builds will use Bazel. Therefore, we will drop official support for cmake.\n\n## Bug Fixes and Other Changes\n\n* `tf.data`:\n  * `tf.contrib.data.group_by_reducer()` is now available via the public API.\n  * `tf.contrib.data.choose_from_datasets()` is now available via the public API.\n  * Adding `drop_remainder` argument to `tf.data.Dataset.batch()` and `tf.data.Dataset.padded_batch()`, deprecating `tf.contrib.data.batch_and_drop_remainder()` and `tf.contrib.data.padded_batch_and_drop_remainder()`.\n* `tf.estimator`:\n  * `Estimator`s now use custom savers included in `EstimatorSpec` scaffolds for saving SavedModels during export.\n  * `EstimatorSpec` will now add a default prediction output for export if no `export_output` is provided, eliminating the need to explicitly include a `PredictOutput` object in the `model_fn` for simple use-cases.\n  * Support sparse_combiner in canned Linear Estimators.\n  * Added batch normalization to `DNNClassifier`, `DNNRegressor`, and `DNNEstimator`.\n  * Adding ranking support for boosted trees.\n  * Adding center bias option for boosted trees.\n* Add `synchronization` and `aggregation` args to get_variable(). These args will be used for distributed variables.\n* Add `synchronization` and `aggregation` args to the layer `add_weight()` API. These args will be used for distributed variables.\n* `tf.losses.*` do not add to the global collection when executing eagerly (to avoid leaking memory).\n* Support different summary and checkpoint directories in `tf.train.MonitoredTrainingSession()`.\n* Added IndRNN, IndyGRU, and IndyLSTM cells to `tf.contrib.rnn`.\n* Add safe static factory functions for SparseTensor and convert all CHECKs to DCHECKs. Using the constructor directly is unsafe and deprecated.\n* Make the Bigtable client connection pool configurable & increase the default # of connections for performance.\n* Added derivative of `tf.random_gamma` with respect to the alpha parameter.\n* Added derivative of `tf.igamma(a, x)` and `tf.igammac(a, x)` with respect to a.\n* Modified Bessel functions of order zero and one.\n* Add FillTriangular Bijector to create triangular matrices.\n* Added support for Type III DCT, and `tf.spectral.idct(type=2|3)`.\n* Correctly handle CuDNN RNN weight loaded when nest in `TimeDistributed`.\n* Adding per-element weight support for `WALSComputePartialLhsAndRhsOp`.\n* ZerosLike and OnesLike ops treated as constants by Graph Transform Tool.\n* Gamma distribution and the derived distributions (Beta, Dirichlet, Student's t, inverse Gamma) now fully reparameterized.\n* Java: Experimental wrapper classes to make graph generation easier. Thanks @karllessard and @kbsriram\n* Build & link in secure gRPC components (switch from the insecure grpc dependency to secure grpc dependency).\n* Adding new endpoints for existing tensorflow symbols. These endpoints are going to be the preferred endpoints going forward and may replace some of the existing endpoints in the future. List of new endpoints:\n  * New endpoints in `tf.image` namespace: `tf.image.extract_image_patches`\n  * New endpoints in `tf.debugging` namespace: `tf.debugging.check_numerics`, `tf.debugging.is_finite`, `tf.debugging.is_inf`, `tf.debugging.is_nan`.\n  * New endpoints in `tf.dtypes` namespace: `tf.dtypes.as_string`.\n  * New endpoints in `tf.io` namespace: `tf.io.decode_base64`, `tf.io.decode_compressed`, `tf.io.decode_json_example`, `tf.io.decode_raw`, `tf.io.encode_base64`, `tf.io.matching_files`, `tf.io.parse_tensor`, `tf.io.read_file, `tf.io.write_file`.\n  * New endpoints in tf.linalg namespace: `tf.linalg.cross`, `tf.linalg.tensor_diag` (corresponds to `tf.diag`), `tf.linalg.tensor_diag_part` (corresponds to `tf.diag_part`).\n  * New endpoints in tf.manip namespace: `tf.manip.batch_to_space_nd`, `tf.manip.gather_nd`, `tf.manip.reshape`, `tf.manip.reverse`, `tf.manip.scatter_nd`, `tf.manip.space_to_batch_nd`, `tf.manip.tile`\n  * New endpoints in tf.math namespace: `tf.math.acos`, `tf.math.acosh`, `tf.math.add`, `tf.math.asin`, `tf.math.asinh`, `tf.math.atan`, `tf.math.atan2`, `tf.math.atanh`, `tf.math.betainc`, `tf.math.ceil`, `tf.math.cos`, `tf.math.cosh`, `tf.math.digamma`, `tf.math.equal`, `tf.math.erfc`, `tf.math.exp`, `tf.math.expm1`, `tf.math.floor`, `tf.math.greater`, `tf.math.greater_equal`, `tf.math.igamma`, `tf.math.igammac`, `tf.math.invert_permutation`, `tf.math.less`, `tf.math.less_equal`, `tf.math.lgamma`, `tf.math.log`, `tf.math.log1p`, `tf.math.logical_and`, `tf.math.logical_not`, `tf.math.logical_or`, `tf.math.maximum`, `tf.math.minimum`, `tf.math.not_equal`, `tf.math.polygamma`, `tf.math.reciprocal`, `tf.math.rint`, `tf.math.rsqrt`, `tf.math.segment_max`, `tf.math.segment_mean`, `tf.math.segment_min`, `tf.math.segment_prod`, `tf.math.segment_sum`, `tf.math.sin`, `tf.math.sinh`, `tf.math.softplus`, `tf.math.softsign`, `tf.math.squared_difference`, `tf.math.tan`, `tf.math.unsorted_segment_max`, `tf.math.unsorted_segment_min`, `tf.math.unsorted_segment_prod`, `tf.math.unsorted_segment_sum`, `tf.math.zeta`.\n  * New endpoints in `tf.quantization` namespace: `tf.quantization.dequantize`, `tf.quantization.fake_quant_with_min_max_args`, `tf.quantization.fake_quant_with_min_max_args_gradient`, `tf.quantization.fake_quant_with_min_max_vars`,  `tf.quantization.fake_quant_with_min_max_vars_gradient`, `tf.quantization.fake_quant_with_min_max_vars_per_channel`,  `tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient`.\n  * New endpoints in tf.strings namespace: `tf.strings.join` (corresponds to `tf.string_join`), `tf.strings.regex_replace`, `tf.strings.to_number` (corresponds to `tf.string_to_number`), `tf.strings.strip` (corresponds to `tf.string_strip`), `tf.strings.substr`, `tf.strings.to_hash_bucket` (corresponds to `tf.string_to_hash_bucket`), `tf.strings.to_hash_bucket_fast` (corresponds to `tf.string_to_hash_bucket_fast`), `tf.strings.to_hash_bucket_strong` (corresponds to `tf.string_to_hash_bucket_strong`).\n\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAg Ramesh, Alex Wiltschko, Alexander Pantyukhin, Amogh Mannekote, An Jiaoyang, Andrei Nigmatulin, Andrew Ginns, Bj\u00f8Rn Moholt, Brett Koonce, Chengzhi Chen, Chinmay Das, Christian Ertler, Christoph Boeddeker, Clayne Robison, Courtial Florian, ctiijima, Dan Douthit, Dan J, Dan Ringwalt, EFanZh, Emanuele Ballarin, eqy, Evgeniy Zheltonozhskiy, Freedom\" Koan-Sin Tan, Fr\u00e9D\u00e9Ric Branchaud-Charron, G K, gracehoney, Guillaume Klein, Guozhong Zhuang, Hsien-Yang Li, hsm207, ImSheridan, Jayaram Bobba, Jiandong Ruan, Jie, Joel Shor, Jonas Rauber, Jongmin Baek, jsawruk, Karan Kaw, Karl Lessard, karl@kubx.ca, Kb Sriram, KinmanLam, leiiwang, Li, Yiqiang, Loo Rong Jie, Mahmoud Abuzaina, Mahmoud Aslan, ManHyuk, Martin Patz, Martin Zeitler, mktozk, Mohammad Ashraf Bhuiyan, mrTsjolder, Naman Bhalla, Nick Felt, Nicolas Lopez, Niranjan Hasabnis, Nishidha Panpaliya, Nitish, nrstott, Nutti, Parag Jain, PeterLee, Philipp Jund, Rach L, Rafal Wojdyla, Roland Zimmermann, Sergei Lebedev, SneakyFish5, Soila Kavulya, Sriram Veturi, Steven Schmatz, Taehoon Lee, Tang, Wenyi, Taras Sereda, Ted Chang, Tim Zaman, Tristan Rice, tucan, vchigrin, Vikram Tiwari, Vincent, WeberXie, William D. Irons, Yan Facai (\u989c\u53d1\u624d), Yong Tang, Yu Yi, Yuxin Wu, Z\u00e9 Vin\u00edCius\n\n# Release 1.9.0\n\n## Major Features And Improvements\n* Updated docs for `tf.keras`: New Keras-based [get started](http://tensorflow.org/versions/r1.9/get_started),\n  and [programmers guide page](http://tensorflow.org/versions/r1.9/programmers_guide/keras).\n* Update `tf.keras` to the Keras 2.1.6 API.\n* Added [`tf.keras.layers.CuDNNGRU`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/keras/layers/CuDNNGRU) and [`tf.keras.layers.CuDNNLSTM`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/keras/layers/CuDNNLSTM) layers. [Try it](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb?linkId=53292082).\n* Adding support of core [feature columns](https://www.tensorflow.org/get_started/feature_columns) and [losses](https://www.tensorflow.org/api_docs/python/tf/losses) to [gradient boosted trees estimators](https://github.com/tensorflow/models/tree/master/official/r1/boosted_trees).\n* The [python interface](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/lite)\n  for the [TFLite Optimizing Converter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/README.md)\n  has been expanded, and the command line interface (AKA: `toco`, `tflite_convert`) is once again\n  included in the standard `pip` installation.\n* Improved data-loading and text processing with:\n    * [`tf.decode_compressed`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/decode_compressed)\n    * [`tf.string_strip`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/string_strip)\n    * [`tf.strings.regex_full_match`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/strings/regex_full_match)\n* Added experimental support for new pre-made Estimators:\n  * [`tf.contrib.estimator.BaselineEstimator`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/estimator/BaselineEstimator)\n  * [`tf.contrib.estimator.RNNClassifier`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/estimator/RNNEstimator)\n  * [`tf.contrib.estimator.RNNEstimator`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/estimator/RNNClassifier)\n* The [distributions.Bijector](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/distributions/bijectors/Bijector)\n  API supports broadcasting for Bijectors with new API changes.\n\n## Breaking Changes\n  * If you're opening empty variable scopes; replace `variable_scope('', ...)` by\n    `variable_scope(tf.get_variable_scope(), ...)`.\n  * Headers used for building custom ops have been moved from site-packages/external into site-packages/tensorflow/include/external.\n\n## Bug Fixes and Other Changes\n\n*   `tfe.Network` is deprecated. Please inherit from `tf.keras.Model`.\n*   Layered variable names have changed in the following conditions:\n    *   Using `tf.keras.layers` with custom variable scopes.\n    *   Using `tf.layers` in a subclassed `tf.keras.Model` class. See\n        [here](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/layers)\n        for more details\n*   `tf.data`:\n    *   `Dataset.from_generator()` now accepts an `args` list, in order to\n        create nested generators.\n    *   `Dataset.list_files()` now produces deterministic results when\n        `shuffle=False` or a `seed` is passed.\n    *   `tf.contrib.data.sample_from_datasets()` and\n        `tf.contrib.data.choose_from_datasets()` make it easier to sample or\n        deterministically choose elements from multiple datasets.\n    *   `tf.contrib.data.make_csv_dataset()` now supports line breaks in quoted\n        strings, and two infrequently used arguments removed.\n    *   (C++) `DatasetBase::DebugString()` is now `const`.\n    *   (C++) `DatasetBase::MakeIterator()` has been renamed to\n        `DatasetBase::MakeIteratorInternal()`.\n    *   (C++) `IteratorBase::Initialize()` method was added to support raising\n        errors during iterator construction.\n*   Eager Execution:\n    *   Added the ability to pause recording operations for gradient computation\n        via `tf.GradientTape.stop_recording`.\n    *   Updated documentation, introductory notebooks.\n*   `tf.keras`:\n    *   Move Keras code out of _impl folder and remove API files.\n    *   `tf.keras.Model.save_weights` now saves in TensorFlow format by default.\n    *   Enable dataset iterators to be passed to `tf.keras.Model` training/eval\n        methods.\n*   TensorFlow Debugger (tfdbg) CLI: fix an issue in which the TensorBoard\n    Debugger Plugin could not handle total source file size exceeding gRPC\n    message size limit (4 MB).\n*   `tf.contrib`:\n    *   `tf.contrib.framework.zero_initializer` supports ResourceVariable.\n    *   Adding \"constrained_optimization\" to tensorflow/contrib.\n*   Other:\n    *   Add GCS Configuration Ops.\n    *   Changing signature of `MakeIterator` to enable propagating error status.\n    *   KL divergence for two Dirichlet distributions.\n    *   More consistent GcsFileSystem behavior for certain reads past EOF.\n    *   Update benchmark for tf.scan to match ranges across eager and graph\n        modes.\n    *   Fixed bug in `tf.reduce_prod gradient` for complex dtypes.\n    *   Allow the use of '.' in variables (e.g. \"hparams.parse('a.b=1.0')\"),\n        which would previously raise an error. This will correspond to an\n        attribute name with an embedded '.' symbol (e.g. 'a.b'), which can only\n        be accessed indirectly (e.g. through getattr and setattr). To set this\n        up the user will first need to explicitly add the variable to the hparam\n        object (e.g. \"hparams.add_hparam(name='a.b', value=0.0)\").\n    *   Benchmark for tf.scan in graph and eager modes.\n    *   Added complex128 support to FFT, FFT2D, FFT3D, IFFT, IFFT2D, and IFFT3D.\n    *   Making ids unique in `nn.embedding_lookup_sparse`. This helps to reduce\n        RPC calls for looking up the embeddings when there are repeated ids in\n        the batch.\n    *   Support indicator column in boosted trees.\n    *   Prevent `tf.gradients()` from backpropagating through integer tensors.\n    *   LinearOperator[1D,2D,3D]Circulant added to `tensorflow.linalg`.\n    *   Conv3D, Conv3DBackpropInput, Conv3DBackpropFilter now supports\n        arbitrary.\n    *   Added `tf.train.Checkpoint` for reading/writing object-based\n        checkpoints.\n    *   Added LinearOperatorKronecker, a dense-free implementation of the\n        Kronecker Product.\n    *   Allow LinearOperator to broadcast.\n    *   SavedModelBuilder will now deduplicate asset names that point to files\n        with the same basename and the same contents. Note that this may result\n        in new asset files included in SavedModels in cases where assets with\n        the same name but different contents were previously overwriting each\n        other.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAbdullah Alrasheed, Achal Shah, Ad-530, ADiegoCAlonso, Aditya Yogi, Ag Ramesh, akindyakov, Andy Kernahan, Anya Petrova, Aurelien Geron, Ben, Ben Barsdell, Bhavani-Subramanian, braincodercn, Brett Koonce, Brian Nemsick, Brian Zier, Bryan Heden, candy.dc, cclauss, Clayne Robison, ctiijima, Dalmo Cirne, David Norman, David T.H. Kao, DosLin, ekelsen, Elson Rodriguez, Erik Smistad, Felix Abecassis, Fergal Cotter, fo40225, foo0x29a, Freedom\" Koan-Sin Tan, Fr\u00e9D\u00e9Ric Branchaud-Charron, gdh1995, Geoffrey Irving, Giuseppe, gracehoney, Guido Zuidhof, Guillaume Klein, Guozhong Zhuang, Haggai, Harald Husum, imsheridan, Ivan Zhang, Jan Zikes, Jayaram Bobba, Jesse Benson, Jesse Gumz, Jiajia Li, Jie, jinghuangintel, Jingwen, jjsjann123, Joe Yearsley, Joel Hestness, Joel Shor, josephyearsley, Junpeng Lao, Karol M. Langner, Kb Sriram, krantideep95, Krish Ravindranath, Letian Feng, Loo Rong Jie, Lukas Geiger, Maciej, Mahmoud Abuzaina, ManHyuk, Mark Ryan, mbhuiyan, Michal Turek, Mostafa Alaa, Myungsung Kwak, Nand Dalal, Nehal J Wani, Neil Tenenholtz, ngc92, Nicholas Nadeau, P.Eng., Avs, Niranjan Hasabnis, P-Hidringer, Paul Van Eck, Peng Yu, Qing Zhao, Qingying Chen, Quanlong, Rajendra Arora, Rholais Lii, rmanyari, Robin Richtsfeld, Russell Klopfer, Sagi, Sam Sendelbach, Sandeep N Gupta, Sandip Giri, Sarah Edkins, Scott Tseng, Sdalbsoo, Sergii Khomenko, Seungwoo Choi (Biggie), Seyed Majid Azimi, Shaoning Zeng, shengfuintel, Siu Kei, Muk, Smit Shilu, soonson, Stefan Schweter, Sukhwan Kim, Sunitha Kambhampati, Taehoon Lee, tamimaddari82, Tang, Wenyi, Ted Chang, u2takey, Utkarsh Upadhyay, Vadim Markovtsev, voegtlel, Wai Hon Law, wangsiyu, Wenhao Hu, wenhao.hu, William D. Irons, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yihong Wang, Yilei (Dolee) Yang, Yong Tang, Yuan (Terry) Tang\n\n# Release 1.8.0\n\n## Major Features And Improvements\n* Can now pass `tf.contrib.distribute.MirroredStrategy()` to `tf.estimator.RunConfig()` to run an Estimator model on multiple GPUs on one machine.\n* Add `tf.contrib.data.prefetch_to_device()`, which supports prefetching to GPU memory.\n* Added Gradient Boosted Trees as pre-made Estimators: BoostedTreesClassifier, BoostedTreesRegressor.\n* Add 3rd generation pipeline config for Cloud TPUs which improves performance and usability.\n* `tf.contrib.bayesflow` is moving out to it's own repo.\n* Added `tf.contrib.{proto,rpc}` to allow generic proto parsing and RPC communication<sup>[1](#rpc-issue)</sup>.\n\n## Bug Fixes and Other Changes\n* `tf.data`:\n  * Add `tf.contrib.data.prefetch_to_device`, which enables prefetching dataset elements to GPU memory.\n  * Add `tf.contrib.data.AUTOTUNE`, which allows the tf.data runtime to automatically tune the prefetch buffer sizes based on your system and environment.\n  * Add `tf.contrib.data.make_csv_dataset` for building datasets of CSV files.\n* Eager Execution:\n  * With eager execution Datasets can now be used as standard python iterators (`for batch in dataset:`). Both `Dataset.__iter__()` and `Dataset.make_one_shot_iterator()` can now be used to create iterators when eager execution is enabled.\n  * Automatic device placement has been enabled (i.e., use a GPU if available automatically, without requiring an explicit `with tf.device(\u201c/gpu:0\u201d)`) (Fixes #14133)\n  * `tf.GradientTape` has moved out of contrib.\n* `tf.keras`:\n  * Added the fashion mnist dataset.\n  * New data preprocessing functions: `image/random_brightness`, `sequence/TimeseriesGenerator`, and `text/hashing_trick`.\n* Accelerated Linear Algebra (XLA):\n  * Select and scatter in reference util and evaluator now use lexicographical order to break ties.\n* TensorFlow Debugger (tfdbg) CLI:\n  * During tensor-filter operations, allow exclusion of nodes by regular expressions.\n  * Fix spurious background colors in some text terminals.\n* `tf.contrib`:\n  * Add meta-distribution BatchReshape which reshapes batch dimensions.\n  * `tf.contrib.layers.recompute_grad` works for explicit gradient checkpointing on TPU.\n  * Add `tf.contrib.framework.argsort`.\n  * Allow `DNNBoostedTreeCombinedEstimator` to work with core versions of feature columns and losses.\n  * Add non-linear image warping ops: `tf.contrib.image.sparse_image_warp`, `tf.contrib.image.dense_image_warp`, and `tf.contrib.image.interpolate_spline`.\n  * Fix bug in `tf.contrib.opt.MultitaskOptimizerWrapper` where types of tensors were mismatched.\n* Other:\n  * Low-level graph construction now calls the TensorFlow C API. This change should be invisible to most users, but can be disabled by setting the environment variable `TF_C_API_GRAPH_CONSTRUCTION=0` in this release. Future releases will remove the ability to disable this change. Please [file a bug](https://github.com/tensorflow/tensorflow/issues/new) if you find yourself using this escape hatch.\n  * Add description of shapes and a pointer to tutorial notebook in `tf.distributions.Distribution`.\n  * Update scatter operations:\n    * Add `tf.scatter_min` and `tf.scatter_max`\n    * Extend scatter operations to work with a scalar update parameter.\n  * Move cuDNN RNN ops to core for use in TensorFlow codebase only.\n  * Add `float64` support for `Conv2d`, `Conv2dBackpropInput`, and `Conv2dBackpropFilter`.\n  * Add `float64` support for `AvgPool`/`AvgPoolGrad`.\n  * Make graph name scope thread local so that they work correctly in multi-threaded environments.\n  * Update nsync synchronization library to avoid slow primitives on Linux.\n  * Removed need to put nsync/public on C include path when building custom ops.\n  * Add `tf.image.psnr`, `tf.image.ssim`, `tf.image.ssim_multiscale`, `tf.image.image_gradients`, `tf.image.sobel_edges`.\n  * Add links to https://js.tensorflow.org.\n  * Fix non-uniformity of orthogonal matrices.\n  * Fix bug where multi-image Estimator eval summaries were not displayed correctly.\n\n<a name=\"rpc-issue\"><sup>1</sup></a> The cancellation logic of the RPC op contains a concurrency error. A fix has been submitted to master and will be part of the next release.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4d55397500, Aghasy, Alan Du, Alan Lee, Alan Yee, Alex Wiltschko, Animesh Karnewar, Ankit Gupta, Anton Matosov, Aris L, Ben Barsdell, Brent Yi, Brett Koonce, Carl Thom\u00e9, cbockman, Chikanaga Tomoyuki, Chris Tava, C\u00e9Dric Deltheil, Dahan Gong, Dalmo Cirne, Daniel Erenrich, David Norman, DavidNorman, Edd Wilder-James, Fanjin Zeng, Felix Abecassis, fo40225, George Sterpu, Giovanni Terlingen, Gor Baghdasaryan, Guillaume Klein, Hanchen Li, Ilya Polenov, Jakub Kolodziejczyk, Jason Sadler, Jayaram Bobba, Jerry Liu, jinghuangintel, Jiongyan Zhang (\u5f20\u70af\u884d), Joel Shor, Jong Wook Kim, Julian Eisenschlos, Karl Lessard, Krish Ravindranath, Loo Rong Jie, Lukas Geiger, Luke Iwanski, Mahmoud Abuzaina, ManHyuk, Marvin Richter, Maximilian Mitchell, Mohammad Ashraf Bhuiyan, msofka, Mustafa Kasap, Nathan Burnham, Nathan Luehr, Naveen Marri, ngc92, nio1814, Oleg Zabluda, Ou Changkun, Panos Ipeirotis, Paul Van Eck, Peter Lee, Piotr Czapla, qjivy, Rholais Lii, Rodrigo Formigone, Russell Klopfer, ryantimjohn, Sang Han, Sebasti\u00e1N Ram\u00edRez, shengfuintel, Siby Jose Plathottam, Silver Chan, Stanislaw Antol, Taehoon Lee, Tarang Chugh, Ted Chang, Thomas Bastiani, Xian Xu, Xiaoming (Jason) Cui, Yan Facai (\u989c\u53d1\u624d), yaox12, Yashal Shakti Kanungo, Yong Tang, Yuan (Terry) Tang, Yuxin Wu, Ziyue(Louis) Lu\n\n# Release 1.7.0\n\n## Major Features And Improvements\n* Eager mode is moving out of contrib, try `tf.enable_eager_execution()`.\n* Graph rewrites emulating fixed-point quantization compatible with TensorFlow Lite, supported by new `tf.contrib.quantize` package.\n* Easily customize gradient computation with `tf.custom_gradient`.\n* [TensorBoard Debugger Plugin](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/debugger/README.md), the graphical user interface (GUI) of TensorFlow Debugger (tfdbg), is now in alpha.\n* Experimental support for reading a sqlite database as a `Dataset` with new `tf.contrib.data.SqlDataset`.\n* Distributed Mutex / CriticalSection added to `tf.contrib.framework.CriticalSection`.\n* Better text processing with `tf.regex_replace`.\n* Easy, efficient sequence input with `tf.contrib.data.bucket_by_sequence_length`\n* Initial support for `tf.contrib.tensorrt` that enables native TensorRT in\n  TensorFlow.\n\n## Bug Fixes and Other Changes\n* Accelerated Linear Algebra (XLA):\n  * Add `MaxPoolGradGrad` support for XLA\n  * CSE pass from Tensorflow is now disabled in XLA.\n* `tf.data`:\n  * `tf.data.Dataset`\n    * Add support for building C++ Dataset op kernels as external libraries, using the `tf.load_op_library()` mechanism.\n    * `Dataset.list_files()` now shuffles its output by default.\n    * `Dataset.shuffle(..., seed=tf.constant(0, dtype=tf.int64))` now yields the same sequence of elements as `Dataset.shuffle(..., seed=0)`.\n  * Add `num_parallel_reads` argument to `tf.data.TFRecordDataset`.\n* `tf.contrib`:\n  * `tf.contrib.bayesflow.halton_sequence` now supports randomization.\n  * Add support for scalars in `tf.contrib.all_reduce`.\n  * Add `effective_sample_size` to `tf.contrib.bayesflow.mcmc_diagnostics`.\n  * Add `potential_scale_reduction` to `tf.contrib.bayesflow.mcmc_diagnostics`.\n  * Add `BatchNormalization`, `Kumaraswamy` bijectors.\n  * Deprecate `tf.contrib.learn`. Please check contrib/learn/README.md for instructions on how to convert existing code.\n  * `tf.contrib.data`\n    * Remove deprecated `tf.contrib.data.Dataset`, `tf.contrib.data.Iterator`, `tf.contrib.data.FixedLengthRecordDataset`, `tf.contrib.data.TextLineDataset`, and `tf.contrib.data.TFRecordDataset` classes.\n    * Added `bucket_by_sequence_length`, `sliding_window_batch`, and `make_batched_features_dataset`\n  * Remove unmaintained `tf.contrib.ndlstm`. You can find it externally at https://github.com/tmbarchive/tfndlstm.\n  * Moved most of `tf.contrib.bayesflow` to its own repo: `tfp`\n* Other:\n  * tf.py_func now reports the full stack trace if an exception occurs.\n  * Integrate `TPUClusterResolver` with GKE's integration for Cloud TPUs.\n  * Add a library for statistical testing of samplers.\n  * Add Helpers to stream data from the GCE VM to a Cloud TPU.\n  * Integrate ClusterResolvers with TPUEstimator.\n  * Unify metropolis_hastings interface with HMC kernel.\n  * Move LIBXSMM convolutions to a separate --define flag so that they are disabled by default.\n  * Fix `MomentumOptimizer` lambda.\n  * Reduce `tfp.layers` boilerplate via programmable docstrings.\n  * Add `auc_with_confidence_intervals`, a method for computing the AUC and confidence interval with linearithmic time complexity.\n  * `regression_head` now accepts customized link function, to satisfy the usage that user can define their own link function if the `array_ops.identity` does not meet the requirement.\n  * Fix `initialized_value` and `initial_value` behaviors for `ResourceVariables` created from `VariableDef` protos.\n  * Add TensorSpec to represent the specification of Tensors.\n  * Constant folding pass is now deterministic.\n  * Support `float16` `dtype` in `tf.linalg.*`.\n  * Add `tf.estimator.export.TensorServingInputReceiver` that allows `tf.estimator.Estimator.export_savedmodel` to pass raw tensors to model functions.\n\n## Deprecations\n\n* TensorFlow 1.7 may be the last time we support Cuda versions below 8.0.\n  Starting with TensorFlow 1.8 release, 8.0 will be the minimum supported\n  version.\n* TensorFlow 1.7 may be the last time we support cuDNN versions below 6.0.\n  Starting with TensorFlow 1.8 release, 6.0 will be the minimum supported\n  version.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4d55397500, Abe, Alistair Low, Andy Kernahan, Appledore, Ben, Ben Barsdell, Boris Pfahringer, Brad Wannow, Brett Koonce, Carl Thom\u00e9, cclauss, Chengzhi Chen, Chris Drake, Christopher Yeh, Clayne Robison, Codrut Grosu, Daniel Trebbien, Danny Goodman, David Goodwin, David Norman, Deron Eriksson, Donggeon Lim, Donny Viszneki, DosLin, DylanDmitri, Francisco Guerrero, Fred Reiss, gdh1995, Giuseppe, Glenn Weidner, gracehoney, Guozhong Zhuang, Haichen \"Hc\" Li, Harald Husum, harumitsu.nobuta, Henry Spivey, hsm207, Jekyll Song, Jerome, Jiongyan Zhang, jjsjann123, John Sungjin Park, Johnson145, JoshVarty, Julian Wolff, Jun Wang, June-One, Kamil Sindi, Kb Sriram, Kdavis-Mozilla, Kenji, lazypanda1, Liang-Chi Hsieh, Loo Rong Jie, Mahesh Bhosale, MandarJKulkarni, ManHyuk, Marcus Ong, Marshal Hayes, Martin Pool, matthieudelaro, mdfaijul, mholzel, Michael Zhou, Ming Li, Minmin Sun, Myungjoo Ham, MyungsungKwak, Naman Kamra, Peng Yu, Penghao Cen, Phil, Raghuraman-K, resec, Rohin Mohanadas, Sandeep N Gupta, Scott Tseng, seaotterman, Seo Sanghyeon, Sergei Lebedev, Ted Chang, terrytangyuan, Tim H, tkunic, Tod, vihanjain, Yan Facai (\u989c\u53d1\u624d), Yin Li, Yong Tang, Yukun Chen, Yusuke Yamada\n\n\n\n# Release 1.6.0\n\n## Breaking Changes\n* Prebuilt binaries are now built against CUDA 9.0 and cuDNN 7.\n* Prebuilt binaries will use AVX instructions. This may break TF on older CPUs.\n\n## Major Features And Improvements\n* New Optimizer internal API for non-slot variables. Descendants of AdamOptimizer that access _beta[12]_power will need to be updated.\n* `tf.estimator.{FinalExporter,LatestExporter}` now export stripped SavedModels. This improves forward compatibility of the SavedModel.\n* FFT support added to XLA CPU/GPU.\n\n## Bug Fixes and Other Changes\n* Documentation updates:\n  * Added a second version of Getting Started, which is aimed at ML\nnewcomers.\n  * Clarified documentation on `resize_images.align_corners` parameter.\n  * Additional documentation for TPUs.\n* Google Cloud Storage (GCS):\n  * Add client-side throttle.\n  * Add a `FlushCaches()` method to the FileSystem interface, with an implementation for GcsFileSystem.\n* Other:\n  * Add `tf.contrib.distributions.Kumaraswamy`.\n  * `RetryingFileSystem::FlushCaches()` calls the base FileSystem's `FlushCaches()`.\n  * Add `auto_correlation` to distributions.\n  * Add `tf.contrib.distributions.Autoregressive`.\n  * Add SeparableConv1D layer.\n  * Add convolutional Flipout layers.\n  * When both inputs of `tf.matmul` are bfloat16, it returns bfloat16, instead of float32.\n  * Added `tf.contrib.image.connected_components`.\n  * Add `tf.contrib.framework.CriticalSection` that allows atomic variable access.\n  * Output variance over trees predictions for classifications tasks.\n  * For `pt` and `eval` commands, allow writing tensor values to filesystem as numpy files.\n  * gRPC: Propagate truncated errors (instead of returning gRPC internal error).\n  * Augment `parallel_interleave` to support 2 kinds of prefetching.\n  * Improved XLA support for C64-related ops log, pow, atan2, tanh.\n  * Add probabilistic convolutional layers.\n\n## API Changes\n* Introducing `prepare_variance` boolean with default setting to False for backward compatibility.\n* Move `layers_dense_variational_impl.py` to `layers_dense_variational.py`.\n\n## Known Bugs\n* Using XLA:GPU with CUDA 9 and CUDA 9.1 results in garbage results and/or\n  `CUDA_ILLEGAL_ADDRESS` failures.\n\n  Google discovered in mid-December 2017 that the PTX-to-SASS compiler in CUDA 9\n  and CUDA 9.1 sometimes does not properly compute the carry bit when\n  decomposing 64-bit address calculations with large offsets (e.g. `load [x +\n  large_constant]`) into 32-bit arithmetic in SASS.\n\n  As a result, these versions of `ptxas` miscompile most XLA programs which use\n  more than 4GB of temp memory.  This results in garbage results and/or\n  `CUDA_ERROR_ILLEGAL_ADDRESS` failures.\n\n  A fix in CUDA 9.1.121 is expected in late February 2018.  We do not expect a\n  fix for CUDA 9.0.x.  Until the fix is available, the only workaround is to\n  [downgrade](https://developer.nvidia.com/cuda-toolkit-archive) to CUDA 8.0.x\n  or disable XLA:GPU.\n\n  TensorFlow will print a warning if you use XLA:GPU with a known-bad version of\n  CUDA; see e00ba24c4038e7644da417ddc639169b6ea59122.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4d55397500, Ag Ramesh, Aiden Scandella, Akimasa Kimura, Alex Rothberg, Allen Goodman,\namilioto, Andrei Costinescu, Andrei Nigmatulin, Anjum Sayed, Anthony Platanios,\nAnush Elangovan, Armando Fandango, Ashish Kumar Ram, Ashwini Shukla, Ben, Bhavani Subramanian,\nBrett Koonce, Carl Thom\u00e9, cclauss, Cesc, Changming Sun, Christoph Boeddeker, Clayne Robison,\nClemens Schulz, Clint (Woonhyuk Baek), codrut3, Cole Gerdemann, Colin Raffel, Daniel Trebbien,\nDaniel Ylitalo, Daniel Zhang, Daniyar, Darjan Salaj, Dave Maclachlan, David Norman, Dong--Jian,\ndongsamb, dssgsra, Edward H, eladweiss, elilienstein, Eric Lilienstein, error.d, Eunji Jeong, fanlu,\nFlorian Courtial, fo40225, Fred, Gregg Helt, Guozhong Zhuang, Hanchen Li, hsm207, hyunyoung2,\nImSheridan, Ishant Mrinal Haloi, Jacky Ko, Jay Young, Jean Flaherty, Jerome, JerrikEph, Jesse\nKinkead, jfaath, Jian Lin, jinghuangintel, Jiongyan Zhang, Joel Hestness, Joel Shor, Johnny Chan,\nJulian Niedermeier, Julian Wolff, JxKing, K-W-W, Karl Lessard, Kasper Marstal, Keiji Ariyama,\nKoan-Sin Tan, Loki Der Quaeler, Loo Rong Jie, Luke Schaefer, Lynn Jackson, ManHyuk, Matt Basta,\nMatt Smith, Matthew Schulkind, Michael, michaelkhan3, Miguel Piedrafita, Mikalai Drabovich,\nMike Knapp, mjwen, mktozk, Mohamed Aly, Mohammad Ashraf Bhuiyan, Myungjoo Ham, Naman Bhalla,\nNamrata-Ibm, Nathan Luehr, nathansilberman, Netzeband, Niranjan Hasabnis, Omar Aflak, Ozge\nYalcinkaya, Parth P Panchal, patrickzzy, Patryk Chrabaszcz, Paul Van Eck, Pawe\u0142 Kapica, Peng Yu,\nPhilip Yang, Pierre Blondeau, Po-Hsien Chu, powderluv, Puyu Wang, Rajendra Arora, Rasmus, Renat\nIdrisov, resec, Robin Richtsfeld, Ronald Eddy Jr, Sahil Singh, Sam Matzek, Sami Kama, sandipmgiri,\nSantiago Castro, Sayed Hadi Hashemi, Scott Tseng, Sergii Khomenko, Shahid, Shengpeng Liu, Shreyash\nSharma, Shrinidhi Kl, Simone Cirillo, simsicon, Stanislav Levental, starsblinking, Stephen Lumenta,\nSteven Hickson, Su Tang, Taehoon Lee, Takuya Wakisaka, Ted Chang, Ted Ying, Tijmen Verhulsdonck,\nTimofey Kondrashov, vade, vaibhav, Valentin Khrulkov, vchigrin, Victor Costan, Viraj Navkal,\nVivek Rane, wagonhelm, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yaroslav Bulatov, yegord, Yong Tang,\nYoni Tsafir, yordun, Yuan (Terry) Tang, Yuxin Wu, zhengdi, Zhengsheng Wei, \u7530\u4f20\u6b66\n\n# Release 1.5.0\n\n## Breaking Changes\n* Prebuilt binaries are now built against CUDA 9.0 and cuDNN 7.\n* Starting from 1.6 release, our prebuilt binaries will use AVX instructions.\n  This may break TF on older CPUs.\n\n## Major Features And Improvements\n* [Eager execution](https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/eager)\n  preview version is now available.\n* [TensorFlow Lite](https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/lite)\n  dev preview is now available.\n* CUDA 9.0 and cuDNN 7 support.\n* Accelerated Linear Algebra (XLA):\n  * Add `complex64` support to XLA compiler.\n  * `bfloat` support is now added to XLA infrastructure.\n  * Make `ClusterSpec` propagation work with XLA devices.\n  * Use a deterministic executor to generate XLA graph.\n* `tf.contrib`:\n  * `tf.contrib.distributions`:\n    * Add `tf.contrib.distributions.Autoregressive`.\n    * Make `tf.contrib.distributions` QuadratureCompound classes support batch\n    * Infer `tf.contrib.distributions.RelaxedOneHotCategorical` `dtype` from arguments.\n    * Make `tf.contrib.distributions` quadrature family parameterized by\n      `quadrature_grid_and_prob` vs `quadrature_degree`.\n    * `auto_correlation` added to `tf.contrib.distributions`\n  * Add `tf.contrib.bayesflow.layers`, a collection of probabilistic (neural) layers.\n  * Add `tf.contrib.bayesflow.halton_sequence`.\n  * Add `tf.contrib.data.make_saveable_from_iterator.`\n  * Add `tf.contrib.data.shuffle_and_repeat`.\n  * Add new custom transformation: `tf.contrib.data.scan()`.\n  * `tf.contrib.distributions.bijectors`:\n    * Add `tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow`.\n    * Add `tf.contrib.distributions.bijectors.Permute`.\n    * Add `tf.contrib.distributions.bijectors.Gumbel`.\n    * Add `tf.contrib.distributions.bijectors.Reshape`.\n    * Support shape inference (i.e., shapes containing -1) in the Reshape bijector.\n* Add `streaming_precision_recall_at_equal_thresholds,` a method for computing\n  streaming precision and recall with `O(num_thresholds + size of predictions)`\n  time and space complexity.\n* Change `RunConfig` default behavior to not set a random seed, making random\n  behavior independently random on distributed workers. We expect this to\n  generally improve training performance. Models that do rely on determinism\n  should set a random seed explicitly.\n* Replaced the implementation of `tf.flags` with `absl.flags`.\n* Add support for `CUBLAS_TENSOR_OP_MATH` in fp16 GEMM\n* Add support for CUDA on NVIDIA Tegra devices\n\n## Bug Fixes and Other Changes\n* Documentation updates:\n  * Clarified that you can only install TensorFlow on 64-bit machines.\n  * Added a short doc explaining how `Estimator`s save checkpoints.\n  * Add documentation for ops supported by the `tf2xla` bridge.\n  * Fix minor typos in the doc of `SpaceToDepth` and `DepthToSpace`.\n  * Updated documentation comments in `mfcc_mel_filterbank.h` and `mfcc.h` to\n    clarify that the input domain is squared magnitude spectra and the weighting\n    is done on linear magnitude spectra (sqrt of inputs).\n  * Change `tf.contrib.distributions` docstring examples to use `tfd` alias\n    rather than `ds`, `bs`.\n  * Fix docstring typos in `tf.distributions.bijectors.Bijector`.\n  * `tf.assert_equal` no longer raises `ValueError.` It now raises\n    `InvalidArgumentError,` as documented.\n  * Update Getting Started docs and API intro.\n* Google Cloud Storage (GCS):\n  * Add userspace DNS caching for the GCS client.\n  * Customize request timeouts for the GCS filesystem.\n  * Improve GCS filesystem caching.\n* Bug Fixes:\n  * Fix bug where partitioned integer variables got their wrong shapes. Before\n  * Fix correctness bug in CPU and GPU implementations of Adadelta.\n  * Fix a bug in `import_meta_graph`'s handling of partitioned variables when\n    importing into a scope. WARNING: This may break loading checkpoints of\n    graphs with partitioned variables saved after using `import_meta_graph` with\n    a non-empty `import_scope` argument.\n  * Fix bug in offline debugger which prevented viewing events.\n  * Added the `WorkerService.DeleteWorkerSession` method to the gRPC interface,\n    to fix a memory leak. Ensure that your master and worker servers are running\n    the same version of TensorFlow to avoid compatibility issues.\n  * Fix bug in peephole implementation of BlockLSTM cell.\n  * Fix bug by casting dtype of `log_det_jacobian` to match `log_prob` in\n    `TransformedDistribution`.\n  * Fix a bug in `import_meta_graph`'s handling of partitioned variables when\n  * Ensure `tf.distributions.Multinomial` doesn't underflow in `log_prob`.\n    Before this change, all partitions of an integer variable were initialized\n    with the shape of the unpartitioned variable; after this change they are\n    initialized correctly.\n* Other:\n  * Add necessary shape util support for bfloat16.\n  * Add a way to run ops using a step function to MonitoredSession.\n  * Add `DenseFlipout` probabilistic layer.\n  * A new flag `ignore_live_threads` is available on train. If set to `True`, it\n    will ignore threads that remain running when tearing down infrastructure\n    after successfully completing training, instead of throwing a RuntimeError.\n  * Restandardize `DenseVariational` as simpler template for other probabilistic\n    layers.\n  * `tf.data` now supports `tf.SparseTensor` components in dataset elements.\n  * It is now possible to iterate over `Tensor`s.\n  * Allow `SparseSegmentReduction` ops to have missing segment IDs.\n  * Modify custom export strategy to account for multidimensional sparse float\n    splits.\n  * `Conv2D`, `Conv2DBackpropInput`, `Conv2DBackpropFilter` now supports arbitrary\n    dilations with GPU and cuDNNv6 support.\n  * `Estimator` now supports `Dataset`: `input_fn` can return a `Dataset`\n    instead of `Tensor`s.\n  * Add `RevBlock`, a memory-efficient implementation of reversible residual layers.\n  * Reduce BFCAllocator internal fragmentation.\n  * Add `cross_entropy` and `kl_divergence` to `tf.distributions.Distribution`.\n  * Add `tf.nn.softmax_cross_entropy_with_logits_v2` which enables backprop\n    w.r.t. the labels.\n  * GPU back-end now uses `ptxas` to compile generated PTX.\n  * `BufferAssignment`'s protocol buffer dump is now deterministic.\n  * Change embedding op to use parallel version of `DynamicStitch`.\n  * Add support for sparse multidimensional feature columns.\n  * Speed up the case for sparse float columns that have only 1 value.\n  * Allow sparse float splits to support multivalent feature columns.\n  * Add `quantile` to `tf.distributions.TransformedDistribution`.\n  * Add `NCHW_VECT_C` support for `tf.depth_to_space` on GPU.\n  * Add `NCHW_VECT_C` support for `tf.space_to_depth` on GPU.\n\n## API Changes\n* Rename `SqueezeDims` attribute to `Axis` in C++ API for Squeeze op.\n* `Stream::BlockHostUntilDone` now returns Status rather than bool.\n* Minor refactor: move stats files from `stochastic` to `common` and remove\n  `stochastic`.\n\n## Known Bugs\n* Using XLA:GPU with CUDA 9 and CUDA 9.1 results in garbage results and/or\n  `CUDA_ILLEGAL_ADDRESS` failures.\n\n  Google discovered in mid-December 2017 that the PTX-to-SASS compiler in CUDA 9\n  and CUDA 9.1 sometimes does not properly compute the carry bit when\n  decomposing 64-bit address calculations with large offsets (e.g. `load [x +\n  large_constant]`) into 32-bit arithmetic in SASS.\n\n  As a result, these versions of `ptxas` miscompile most XLA programs which use\n  more than 4GB of temp memory.  This results in garbage results and/or\n  `CUDA_ERROR_ILLEGAL_ADDRESS` failures.\n\n  A fix in CUDA 9.1.121 is expected in late February 2018.  We do not expect a\n  fix for CUDA 9.0.x.  Until the fix is available, the only workaround is to\n  [downgrade](https://developer.nvidia.com/cuda-toolkit-archive) to CUDA 8.0.x\n  or disable XLA:GPU.\n\n  TensorFlow will print a warning if you use XLA:GPU with a known-bad version of\n  CUDA; see e00ba24c4038e7644da417ddc639169b6ea59122.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAdam Zahran, Ag Ramesh, Alan Lee, Alan Yee, Alex Sergeev, Alexander, Amir H. Jadidinejad,\nAmy, Anastasios Doumoulakis, Andrei Costinescu, Andrei Nigmatulin, Anthony Platanios,\nAnush Elangovan, arixlin, Armen Donigian, Art\u00ebM Sobolev, Atlas7, Ben Barsdell, Bill Prin,\nBo Wang, Brett Koonce, Cameron Thomas, Carl Thom\u00e9, Cem Eteke, cglewis, Changming Sun,\nCharles Shenton, Chi-Hung, Chris Donahue, Chris Filo Gorgolewski, Chris Hoyean Song,\nChris Tava, Christian Grail, Christoph Boeddeker, cinqS, Clayne Robison, codrut3, concerttttt,\nCQY, Dan Becker, Dan Jarvis, Daniel Zhang, David Norman, dmaclach, Dmitry Trifonov,\nDonggeon Lim, dongpilYu, Dr. Kashif Rasul, Edd Wilder-James, Eric Lv, fcharras, Felix Abecassis,\nFirefoxMetzger, formath, FredZhang, Gaojin Cao, Gary Deer, Guenther Schmuelling, Hanchen Li,\nHanmin Qin, hannesa2, hyunyoung2, Ilya Edrenkin, Jackson Kontny, Jan, Javier Luraschi,\nJay Young, Jayaram Bobba, Jeff, Jeff Carpenter, Jeremy Sharpe, Jeroen B\u00e9Dorf, Jimmy Jia,\nJinze Bai, Jiongyan Zhang, Joe Castagneri, Johan Ju, Josh Varty, Julian Niedermeier,\nJxKing, Karl Lessard, Kb Sriram, Keven Wang, Koan-Sin Tan, Kyle Mills, lanhin, LevineHuang,\nLoki Der Quaeler, Loo Rong Jie, Luke Iwanski, L\u00e1Szl\u00f3 Csomor, Mahdi Abavisani, Mahmoud Abuzaina,\nManHyuk, Marek \u0160Uppa, MathSquared, Mats Linander, Matt Wytock, Matthew Daley, Maximilian Bachl,\nmdymczyk, melvyniandrag, Michael Case, Mike Traynor, miqlas, Namrata-Ibm, Nathan Luehr,\nNathan Van Doorn, Noa Ezra, Nolan Liu, Oleg Zabluda, opensourcemattress, Ouwen Huang,\nPaul Van Eck, peisong, Peng Yu, PinkySan, pks, powderluv, Qiao Hai-Jun, Qiao Longfei,\nRajendra Arora, Ralph Tang, resec, Robin Richtsfeld, Rohan Varma, Ryohei Kuroki, SaintNazaire,\nSamuel He, Sandeep Dcunha, sandipmgiri, Sang Han, scott, Scott Mudge, Se-Won Kim, Simon Perkins,\nSimone Cirillo, Steffen Schmitz, Suvojit Manna, Sylvus, Taehoon Lee, Ted Chang, Thomas Deegan,\nTill Hoffmann, Tim, Toni Kunic, Toon Verstraelen, Tristan Rice, Urs K\u00f6Ster, Utkarsh Upadhyay,\nVish (Ishaya) Abrams, Winnie Tsang, Yan Chen, Yan Facai (\u989c\u53d1\u624d), Yi Yang, Yong Tang,\nYoussef Hesham, Yuan (Terry) Tang, Zhengsheng Wei, zxcqwe4906, \u5f20\u5fd7\u8c6a, \u7530\u4f20\u6b66\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 1.4.1\n\n## Bug Fixes and Other Changes\n* `LinearClassifier` fix.\n\n# Release 1.4.0\n\n## Major Features And Improvements\n* `tf.keras` is now part of the core TensorFlow API.\n* [`tf.data`](http://tensorflow.org/guide/data) is now part of\n  the core TensorFlow API.\n  * The API is now subject to backwards compatibility guarantees.\n  * For a guide to migrating from the `tf.contrib.data` API, see the\n    [README](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/data/README.md).\n  * Major new features include `Dataset.from_generator()` (for building an input\n    pipeline from a Python generator), and the `Dataset.apply()` method for\n    applying custom transformation functions.\n  * Several custom transformation functions have been added, including\n    `tf.contrib.data.batch_and_drop_remainder()` and\n    `tf.contrib.data.sloppy_interleave()`.\n* Add `train_and_evaluate` for simple distributed `Estimator` training.\n* Add `tf.spectral.dct` for computing the DCT-II.\n* Add Mel-Frequency Cepstral Coefficient support to `tf.contrib.signal`\n  (with GPU and gradient support).\n* Add a self-check on `import tensorflow` for Windows DLL issues.\n* Add NCHW support to `tf.depth_to_space` on GPU.\n* TensorFlow Debugger (tfdbg):\n  * Add `eval` command to allow evaluation of arbitrary Python/numpy expressions\n    in tfdbg command-line interface. See\n    [Debugging TensorFlow Programs](https://www.tensorflow.org/guide/debugger)\n    for more details.\n  * Usability improvement: The frequently used tensor filter `has_inf_or_nan` is\n    now added to `Session` wrappers and hooks by default. So there is no need\n    for clients to call `.add_tensor_filter(tf_debug.has_inf_or_nan)` anymore.\n* SinhArcsinh (scalar) distribution added to `contrib.distributions`.\n* Make `GANEstimator` opensource.\n* `Estimator.export_savedmodel()` now includes all valid serving signatures\n  that can be constructed from the Serving Input Receiver and all available\n  ExportOutputs. For instance, a classifier may provide regression- and\n  prediction-flavored outputs, in addition to the classification-flavored one.\n  Building signatures from these allows TF Serving to honor requests using the\n  different APIs (Classify, Regress, and Predict). Furthermore,\n  `serving_input_receiver_fn()` may now specify alternative subsets of nodes\n  that may act as inputs. This allows, for instance, producing a prediction\n  signature for a classifier that accepts raw `Tensors` instead of a serialized\n  `tf.Example`.\n* Add `tf.contrib.bayesflow.hmc`.\n* Add `tf.contrib.distributions.MixtureSameFamily`.\n* Make `Dataset.shuffle()` always reshuffles after each iteration by default.\n* Add `tf.contrib.bayesflow.metropolis_hastings`.\n* Add `log_rate` parameter to `tf.contrib.distributions.Poisson`.\n* Extend `tf.contrib.distributions.bijector` API to handle some non-injective\n  transforms.\n* Java:\n  * Generics (e.g., `Tensor<Integer>`) for improved type-safety\n    (courtesy @andrewcmyers).\n  * Support for multi-dimensional string tensors.\n  * Support loading of custom operations (e.g. many in `tf.contrib`) on Linux\n    and OS X\n* All our prebuilt binaries have been built with CUDA 8 and cuDNN 6.\n  We anticipate releasing TensorFlow 1.5 with CUDA 9 and cuDNN 7.\n\n## Bug Fixes and Other Changes\n* `tf.nn.rnn_cell.DropoutWrapper` is now more careful about dropping out LSTM\n  states.  Specifically, it no longer ever drops the `c` (memory) state of an\n  `LSTMStateTuple`.  The new behavior leads to proper dropout behavior\n  for LSTMs and stacked LSTMs.  This bug fix follows recommendations from\n  published literature, but is a behavioral change.  State dropout behavior\n  may be customized via the new `dropout_state_filter_visitor` argument.\n* Removed `tf.contrib.training.python_input`.  The same behavior, in a more\n  flexible and reproducible package, is available via the new\n  `tf.contrib.data.Dataset.from_generator` method!\n* Fix `tf.contrib.distributions.Affine` incorrectly computing log-det-jacobian.\n* Fix `tf.random_gamma` incorrectly handling non-batch, scalar draws.\n* Resolved a race condition in TensorForest TreePredictionsV4Op.\n* Google Cloud Storage file system, Amazon S3 file system, and Hadoop file\n  system support are now default build options.\n* Custom op libraries must link against libtensorflow_framework.so\n  (installed at `tf.sysconfig.get_lib()`).\n* Change `RunConfig` default behavior to not set a random seed, making random\n  behavior independently random on distributed workers. We expect this to\n  generally improve training performance. Models that do rely on determinism\n  should set a random seed explicitly.\n\n## Breaking Changes to the API\n* The signature of the `tf.contrib.data.rejection_resample()` function has been\n  changed. It now returns a function that can be used as an argument to\n  `Dataset.apply()`.\n* Remove `tf.contrib.data.Iterator.from_dataset()` method. Use\n  `Dataset.make_initializable_iterator()` instead.\n* Remove seldom used and unnecessary `tf.contrib.data.Iterator.dispose_op()`.\n* Reorder some TF-GAN loss functions in a non-backwards compatible way.\n\n## Known Issues\n* In Python 3, `Dataset.from_generator()` does not support Unicode strings.\n  You must convert any strings to bytes objects before yielding them from\n  the generator.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4d55397500, Abdullah Alrasheed, abenmao, Adam Salvail, Aditya Dhulipala, Ag Ramesh,\nAkimasa Kimura, Alan Du, Alan Yee, Alexander, Amit Kushwaha, Amy, Andrei Costinescu,\nAndrei Nigmatulin, Andrew Erlichson, Andrew Myers, Andrew Stepanov, Androbin, AngryPowman,\nAnish Shah, Anton Daitche, Artsiom Chapialiou, asdf2014, Aseem Raj Baranwal, Ash Hall,\nBart Kiers, Batchu Venkat Vishal, ben, Ben Barsdell, Bill Piel, Carl Thom\u00e9, Catalin Voss,\nChangming Sun, Chengzhi Chen, Chi Zeng, Chris Antaki, Chris Donahue, Chris Oelmueller,\nChris Tava, Clayne Robison, Codrut, Courtial Florian, Dalmo Cirne, Dan J, Darren Garvey,\nDavid Kristoffersson, David Norman, David R\u00f6Thlisberger, DavidNorman, Dhruv, DimanNe,\nDorokhov, Duncan Mac-Vicar P, EdwardDixon, EMCP, error.d, FAIJUL, Fan Xia,\nFrancois Xavier, Fred Reiss, Freedom\" Koan-Sin Tan, Fritz Obermeyer, Gao, Xiang,\nGuenther Schmuelling, Guo Yejun (\u90ed\u53f6\u519b), Hans Gaiser, HectorSVC, Hyungsuk Yoon,\nJames Pruegsanusak, Jay Young, Jean Wanka, Jeff Carpenter, Jeremy Rutman, Jeroen B\u00e9Dorf,\nJett Jones, Jimmy Jia, jinghuangintel, jinze1994, JKurland, Joel Hestness, joetoth,\nJohn B Nelson, John Impallomeni, John Lawson, Jonas, Jonathan Dekhtiar, joshkyh, Jun Luan,\nJun Mei, Kai Sasaki, Karl Lessard, karl@kubx.ca, Kb Sriram, Kenichi Ueno, Kevin Slagle,\nKongsea, Lakshay Garg, lhlmgr, Lin Min, liu.guangcong, Loki Der Quaeler, Louie Helm,\nlucasmoura, Luke Iwanski, Lyndon White, Mahmoud Abuzaina, Marcel Puyat, Mark Aaron Shirley,\nMichele Colombo, MtDersvan, Namrata-Ibm, Nathan Luehr, Naurril, Nayana Thorat, Nicolas Lopez,\nNiranjan Hasabnis, Nolan Liu, Nouce, Oliver Hennigh, osdamv, Patrik Erdes,\nPatryk Chrabaszcz, Pavel Christof, Penghao Cen, postBG, Qingqing Cao, Qingying Chen, qjivy,\nRaphael, Rasmi, raymondxyang, Renze Yu, resec, Roffel, Ruben Vereecken, Ryohei Kuroki,\nsandipmgiri, Santiago Castro, Scott Kirkland, Sean Vig, Sebastian Raschka, Sebastian Weiss,\nSergey Kolesnikov, Sergii Khomenko, Shahid, Shivam Kotwalia, Stuart Berg, Sumit Gouthaman,\nsuperzerg, Sven Mayer, tetris, Ti Zhou, Tiago Freitas Pereira, Tian Jin, Tomoaki Oiki,\nVaibhav Sood, vfdev, Vivek Rane, Vladimir Moskva, wangqr, Weber Xie, Will Frey,\nYan Facai (\u989c\u53d1\u624d), yanivbl6, Yaroslav Bulatov, Yixing Lao, Yong Tang, youkaichao,\nYuan (Terry) Tang, Yue Zhang, Yuxin Wu, Ziming Dong, ZxYuan, \u9ec4\u749e\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 1.3.0\n\nSee also [TensorBoard 0.1.4](https://github.com/tensorflow/tensorboard/releases/tag/0.1.4) release notes.\n\n## Major Features and Improvements\n* Added canned estimators to Tensorflow library. List of added estimators:\n  * `DNNClassifier`\n  * `DNNRegressor`\n  * `LinearClassifier`\n  * `LinearRegressor`\n  * `DNNLinearCombinedClassifier`\n  * `DNNLinearCombinedRegressor`.\n* All our prebuilt binaries have been built with cuDNN 6. We anticipate releasing TensorFlow 1.4 with cuDNN 7.\n* `import tensorflow` now goes much faster.\n* Adds a file cache to the GCS filesystem with configurable max staleness for file contents. This permits caching of file contents across close/open boundaries.\n* Added an axis parameter to `tf.gather`.\n* Added a `constant_values` keyword argument to `tf.pad`.\n* Adds `Dataset.interleave` transformation.\n* Add `ConcatenateDataset` to concatenate two datasets.\n* Added Mobilenet support to TensorFlow for Poets training script.\n* Adds a block cache to the GCS filesystem with configurable block size and count.\n* SinhArcSinh bijector added.\n* Added `Dataset.list_files` API.\n* Introduces new operations and Python bindings for the Cloud TPU.\n* Adding TensorFlow-iOS CocoaPod for symmetry with tensorflow-android.\n* Introduces base implementations of ClusterResolvers.\n* Unify memory representations of TensorShape and PartialTensorShape. As a consequence, tensors now have a maximum of 254 dimensions, not 255.\n* Changed references to LIBXSMM to use version 1.8.1.\n* TensorFlow Debugger (tfdbg):\n  * Display summaries of numeric tensor values with the `-s` flag to command `print_tensor` or `pt`.\n  * Display feed values with the `print_feed` or `pf` command and clickable links in the curses UI.\n  * Runtime profiler at the op level and the Python source line level with the `run -p` command.\n* Initial release of the statistical distribution library `tf.distributions`.\n* GPU kernels and speed improvements for unary `tf.where` and `tf.nn.top_k`.\n* Monotonic Attention wrappers added to `tf.contrib.seq2seq`.\n* Added `tf.contrib.signal`, a library for signal processing primitives.\n* Added `tf.contrib.resampler`, containing CPU and GPU ops for differentiable resampling of images.\n\n## Breaking Changes to the API\n* `tf.RewriterConfig` was removed from the Python API after being available in 1.2 release candidates (it was never in an actual release). Graph rewriting is still available, just not as `tf.RewriterConfig`. Instead add an explicit import.\n* Breaking change to `tf.contrib.data.Dataset` APIs that expect a nested structure. Lists are now converted to `tf.Tensor` implicitly. You may need to change uses of lists to tuples in existing code. In addition, dicts are now supported as a nested structure.\n\n## Changes to contrib APIs\n* Adds tf.contrib.nn.rank_sampled_softmax_loss, a sampled-softmax variant that can improve rank loss.\n* `tf.contrib.metrics`.{streaming_covariance,streaming_pearson_correlation} modified to return nan when they have seen less or equal to 1 unit of weight.\n* Adds time series models to contrib. See contrib/timeseries/README.md for details.\n* Adds FULLY_CONNECTED Op to tensorflow/lite/schema.fbs\n\n## Known Issues\n* Tensorflow_gpu compilation fails with Bazel 0.5.3.\n\n## Bug Fixes and Other Changes\n* Fixes `strides` and `begin` dtype mismatch when slicing using int64 Tensor index in python.\n* Improved convolution padding documentation.\n* Add a tag constant, gpu, to present graph with GPU support.\n* `saved_model.utils` now support SparseTensors transparently.\n* A more efficient implementation of non-max suppression.\n* Add support for the shrinkage-type L2 to FtrlOptimizer in addition to the online L2 it already supports.\n* Fix negative variance in moments calculation.\n* Expand UniqueOp Benchmark Tests to cover more collision cases.\n* Improves stability of GCS filesystem on Mac.\n* Add time estimation to HloCostAnalysis.\n* Fixed the bug in Estimator that params in constructor was not a deepcopy of the user provided one. This bugs inadvertently enabled user to mutate the params after the creation of Estimator, leading to potentially undefined behavior.\n* Added None check for save_path in `saver.restore`.\n* Register devices under their legacy names in device_mgr to ease the transition to clusterspec-propagated configurations.\n* VectorExponential added to distributions.\n* Add a bitwise module with bitwise_and, bitwise_or, bitwise_xor, and invert functions.\n* Add fixed-grid ODE integration routines.\n* Allow passing bounds to ScipyOptimizerInterface.\n* Correctness fixes for fft_length parameter to `tf.spectral.rfft` & `tf.spectral.irfft`.\n* Exported model signatures using the 'predict' method will no longer have their input and output keys silently ignored and rewritten to 'inputs' and 'outputs'. If a model was exported with different names before 1.2, and is now served with tensorflow/serving, it will accept requests using 'inputs' and 'outputs'. Starting at 1.2, such a model will accept the keys specified during export. Therefore, inference requests using 'inputs' and 'outputs' may start to fail. To fix this, either update any inference clients to send requests with the actual input and output keys used by the trainer code, or conversely, update the trainer code to name the input and output Tensors 'inputs' and 'outputs', respectively. Signatures using the 'classify' and 'regress' methods are not affected by this change; they will continue to standardize their input and output keys as before.\n* Add in-memory caching to the Dataset API.\n* Set default end_of_sequence variable in datasets iterators to false.\n* [Performance] Increase performance of `tf.layers.conv2d` when setting use_bias=True by 2x by using nn.bias_add.\n* Update iOS examples to use CocoaPods, and moved to tensorflow/examples/ios.\n* Adds a family= attribute in `tf.summary` ops to allow controlling the tab name used in Tensorboard for organizing summaries.\n* When GPU is configured, do not require --config=cuda, instead, automatically build for GPU if this is requested in the configure script.\n* Fix incorrect sampling of small probabilities in CPU/GPU multinomial.\n* Add a list_devices() API on sessions to list devices within a cluster. Additionally, this change augment the ListDevices master API to support specifying a session.\n* Allow uses of over-parameterized separable convolution.\n* TensorForest multi-regression bug fix.\n* Framework now supports armv7, cocoapods.org now displays correct page.\n* Script to create iOS framework for CocoaPods.\n* Android releases of TensorFlow are now pushed to jcenter for easier integration into apps. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/inference_interface/README.md for more details.\n* TensorFlow Debugger (tfdbg):\n  * Fixed a bug that prevented tfdbg from functioning with multi-GPU setups.\n  * Fixed a bug that prevented tfdbg from working with `tf.Session.make_callable`.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4F2E4A2E, Adriano Carmezim, Adri\u00e0 Arrufat, Alan Yee, Alex Lattas, Alex Rothberg,\nAlexandr Baranezky, Ali Siddiqui, Andreas Solleder, Andrei Costinescu, Andrew Hundt,\nAndrobin, Andy Kernahan, Anish Shah, Anthony Platanios, Arvinds-Ds, b1rd, Baptiste\nArnaud, Ben Mabey, Benedikt Linse, Beomsu Kim, Bo Wang, Boyuan Deng, Brett Koonce,\nBruno Rosa, Carl Thom\u00e9, Changming Sun, Chase Roberts, Chirag Bhatia, Chris Antaki,\nChris Hoyean Song, Chris Tava, Christos Nikolaou, Croath Liu, cxx, Czxck001, Daniel\nYlitalo, Danny Goodman, Darren Garvey, David Brailovsky, David Norman, DavidNorman,\ndavidpham87, ddurham2, Dhruv, DimanNe, Drew Hintz, Dustin Tran, Earthson Lu, ethiraj,\nFabian Winnen, Fei Sun, Freedom\" Koan-Sin Tan, Fritz Obermeyer, Gao, Xiang, Gautam,\nGuenther Schmuelling, Gyu-Ho Lee, Hauke Brammer, horance, Humanity123, J Alammar,\nJayeol Chun, Jeroen B\u00e9Dorf, Jianfei Wang, jiefangxuanyan, Jing Jun Yin, Joan Puigcerver,\nJoel Hestness, Johannes Mayer, John Lawson, Johnson145, Jon Malmaud, Jonathan Alvarez-Gutierrez,\nJuang, Yi-Lin, Julian Viereck, Kaarthik Sivashanmugam, Karl Lessard, karl@kubx.ca, Kevin\nCarbone, Kevin Van Der Burgt, Kongsea, ksellesk, lanhin, Lef Ioannidis, Liangliang He,\nLouis Tiao, Luke Iwanski, L\u00e1Szl\u00f3 Csomor, magixsno, Mahmoud Abuzaina, Marcel Hlopko, Mark\nNeumann, Maxwell Paul Brickner, mdfaijul, Micha\u00ebL Defferrard, Micha\u0142 Jastrz\u0119Bski, Michele\nColombo, Mike Brodie, Mosnoi Ion, mouradmourafiq, myPrecious, Nayana Thorat,\nNeeraj Kashyap, Nelson Liu, Niranjan Hasabnis, Olivier Moindrot, orome, Pankaj Gupta, Paul\nVan Eck, peeyush18, Peng Yu, Pierre, preciousdp11, qjivy, Raingo, raoqiyu, ribx, Richard S.\nImaoka, Rishabh Patel, Robert Walecki, Rockford Wei, Ryan Kung, Sahil Dua, Sandip Giri, Sayed\nHadi Hashemi, sgt101, Shitian Ni, Shuolongbj, Siim P\u00f5Der, Simon Perkins, sj6077, SOLARIS,\nSpotlight0xff, Steffen Eberbach, Stephen Fox, superryanguo, Sven Mayer, Tapan Prakash,\nTiago Morais Morgado, Till Hoffmann, Tj Rana, Vadim Markovtsev, vhasanov, Wei Wu,\nwindead, Yan (Asta) Li, Yan Chen, Yann Henon, Yi Wang, Yong Tang, yorkie, Yuan (Terry)\nTang, Yuxin Wu, zhengjiajin, zhongzyd, \u9ec4\u749e\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 1.2.1\n\n## Bug Fixes and Other Changes\n* Updating markdown version required to >= 2.6.8.\n* Support tensors as dropout rates again, by removing the min(max(..))\n\n# Release 1.2.0\n\n## Major Features and Improvements\n* Python 3.6 support on Windows.\n* Added `tf.layers.conv3d_transpose` layer for spatio temporal deconvolution.\n* Added `tf.Session.make_callable()`, which provides a lower overhead means of running a similar step multiple times.\n* Added libverbs-based RDMA support to contrib (courtesy @junshi15 from Yahoo).\n* Bring `tf.feature_column.*` into the API. Non-deprecated functionality from `tf.contrib.layers.*` is moved to `tf.feature_column.*` with cosmetic changes.\n* `RNNCell` objects now subclass `tf.layers.Layer`.  The strictness described\n  in the TensorFlow 1.1 release is gone:  The first time an RNNCell is used,\n  it caches its scope.  All future uses of the RNNCell will reuse variables from\n  that same scope.  This is a breaking change from the behavior of RNNCells\n  in TensorFlow versions <= 1.0.1.  TensorFlow 1.1 had checks in place to\n  ensure old code works correctly with the new semantics; this version\n  allows more flexible uses of RNNCell but can lead to subtle errors if\n  using code meant for TensorFlow <= 1.0.1.  For example, writing:\n  `MultiRNNCell([lstm] * 5)` will now build a 5-layer LSTM stack where each\n  layer shares the **same** parameters.  To get 5 layers each with their own\n  parameters, write: `MultiRNNCell([LSTMCell(...) for _ in range(5)])`.\n  If at all unsure, first test your code with TF 1.1; ensure it raises no\n  errors, and then upgrade to TF 1.2.\n* RNNCells' variable names have been renamed for consistency with Keras layers.\n  Specifically, the previous variable names \"weights\" and \"biases\" have\n  been changed to \"kernel\" and \"bias\", respectively.\n  This may cause backward incompatibility with regard to your old\n  checkpoints containing such RNN cells, in which case you can use the tool\n  [checkpoint_convert script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/tools/checkpoint_convert.py)\n  to convert the variable names in your old checkpoints.\n* Many of the RNN functions and classes that were in the `tf.nn` namespace\n  before the 1.0 release and which were moved to `tf.contrib.rnn` have now\n  been moved back to the core namespace.  This includes\n  `RNNCell`, `LSTMCell`, `GRUCell`, and a number of other cells.  These\n  now reside in `tf.nn.rnn_cell` (with aliases in `tf.contrib.rnn` for backwards\n  compatibility).  The original `tf.nn.rnn` function is now `tf.nn.static_rnn`,\n  and the bidirectional static and state saving static rnn functions are also\n  now back in the `tf.nn` namespace.\n\n  Notable exceptions are the `EmbeddingWrapper`, `InputProjectionWrapper` and\n  `OutputProjectionWrapper`,  which will slowly be moved to deprecation\n  in `tf.contrib.rnn`.  These are inefficient wrappers that should often\n  be replaced by calling `embedding_lookup` or `layers.dense` as pre- or post-\n  processing of the rnn.  For RNN decoding, this functionality has been replaced\n  with an alternative API in `tf.contrib.seq2seq`.\n* Intel MKL Integration (https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture). Intel developed a number of\n  optimized deep learning primitives: In addition to matrix multiplication and\n  convolution, these building blocks include:\n  Direct batched convolution\n  Pooling: maximum, minimum, average\n  Normalization: LRN, batch normalization\n  Activation: rectified linear unit (ReLU)\n  Data manipulation: multi-dimensional transposition (conversion), split,\n  concat, sum and scale.\n* TensorForest Estimator now supports SavedModel export for serving.\n* Support client-provided ClusterSpec's and propagate them to all workers to enable the creation of dynamic TensorFlow clusters.\n* TensorFlow C library now available for Windows.\n* We released a new open-source version of TensorBoard.\n* [`SavedModel CLI`](https://www.tensorflow.org/versions/master/guide/saved_model_cli) tool available to inspect and execute MetaGraph in SavedModel\n* Android releases of TensorFlow are now pushed to jcenter for easier\n  integration into apps. See\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/inference_interface/README.md\n  for more details.\n\n## Deprecations\n\n* TensorFlow 1.2 may be the last time we build with cuDNN 5.1. Starting with\n  TensorFlow 1.3, we will try to build all our prebuilt binaries with cuDNN 6.0.\n  While we will try to keep our source code compatible with cuDNN 5.1, it will\n  be best effort.\n\n## Breaking Changes to the API\n* `org.tensorflow.contrib.android.TensorFlowInferenceInterface` now throws exceptions where possible and has simplified method signatures.\n\n## Changes to contrib APIs\n* Added `tf.contrib.util.create_example`.\n* Added bilinear interpolation to `tf.contrib.image`.\n* Add `tf.contrib.stateless` for random ops with custom seed control.\n* MultivariateNormalFullCovariance added to contrib/distributions/\n* tensorflow/contrib/rnn undergoes RNN cell variable renaming for\n  consistency with Keras layers. Specifically, the previous variable names\n  \"weights\" and \"biases\" are changed to \"kernel\" and \"bias\", respectively.\n  This may cause backward incompatibility with regard to your old\n  checkpoints containing such RNN cells, in which case you can use the\n  [checkpoint_convert script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/tools/checkpoint_convert.py)\n  to convert the variable names in your old checkpoints.\n* Added `tf.contrib.kernel_methods` module with Ops and estimators for primal\n  (explicit) kernel methods in TensorFlow.\n\n## Bug Fixes and Other Changes\n* In python, `Operation.get_attr` on type attributes returns the Python DType\n  version of the type to match expected get_attr documentation rather than the\n  protobuf enum.\n* tensorflow/contrib/rnn undergoes RNN cell variable renaming for\n  consistency with Keras layers. Specifically, the previous variable names\n  \"weights\" and \"biases\" are changed to \"kernel\" and \"bias\", respectively.\n* Changed MIN_SDK version to 8.0 when building iOS libraries.\n* Fixed LIBXSMM integration.\n* Make decode_jpeg/decode_png/decode_gif handle all formats, since users frequently try to decode an image as the wrong type.\n* Improve implicit broadcasting lowering.\n* Improving stability of GCS/BigQuery clients by a faster retrying of stale transmissions.\n* Remove OpKernelConstruction::op_def() as part of minimizing proto dependencies.\n* VectorLaplaceDiag distribution added.\n* Android demo no longer requires libtensorflow_demo.so to run (libtensorflow_inference.so still required)\n* Added `categorical_column_with_vocabulary_file`.\n* Introduce ops for batching/unbatching tensors across Session::Run() calls.\n* Add tf.log_sigmoid(x) = tf.log(tf.sigmoid(x)) = -tf.nn.softplus(-x).\n* Changed hooks lists to immutable tuples, and now allow any iterable for the associated arguments.\n* Introduce TFDecorator.\n* Added an Mfcc op for speech feature generation.\n* Improved DirectSession::Run() overhead and error checking. Feeding a value of the wrong type will now synchronously raise an INVALID_ARGUMENT error instead of asynchronously raising an INTERNAL error. Code that depends on the (undefined) behavior when feeding a tensor of the wrong type may need to be updated.\n* Added unreduced NONE, and reduced MEAN options for losses. Removed \"WEIGHTED_\" prefix from other Reduction constants.\n* assertAllClose now handles dicts.\n* Added Gmock matcher for HloInstructions.\n* Add var name to errors on variable restore.\n* Added an AudioSpectrogram op for audio feature generation.\n* Added `reduction` arg to losses.\n* `tf.placeholder` can represent scalar shapes and partially known.\n* Remove estimator_spec(mode) argument.\n* Added an AudioSpectrogram op for audio feature generation.\n* TensorBoard disables all runs by default if there are more than 40 runs.\n* Removed old doc generator code.\n* GCS file system integration now supports domain buckets, e.g gs://bucket.domain.com/path.\n* Add `tf.summary.text` for outputting text to TensorBoard.\n* The \"run\" command of tfdbg's command-line interface now supports filtering of tensors by node name, op type and tensor dtype.\n* `tf.string_to_number` now supports int64 and float64 outputs.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4F2E4A2E, Aaron Schumacher, Abhi Agg, admcrae, Adriano Carmezim, Adri\u00e0 Arrufat,\nagramesh1, Akimitsu Seo, Alan Mosca, Alex Egg, Alex Rothberg, Alexander Heinecke,\nAlexander Matyasko, Alexandr Baranezky, Alexandre Caulier, Ali Siddiqui, Anand Venkat,\nAndrew Hundt, Androbin, Anmol Sharma, Arie, Arno Leist, Arron Cao, Aur\u00e9Lien Geron, Bairen Yi,\nBeomsu Kim, Carl Thom\u00e9, cfperez, Changming Sun, Corey Wharton, critiqjo, Dalei Li, Daniel\nRasmussen, Daniel Trebbien, Dar\u00edO Here\u00f1\u00fa, David Eng, David Norman, David Y. Zhang, Davy Song, ddurham2,\nDeepak Subburam, Dmytro Kyrychuk, Dominic Rossi, Dominik Schl\u00f6Sser, Dustin Tran,\nEduardo Pinho, Egil Martinsson, Elliot Saba, Eric Bigelow, Erik Smistad, Evan Klitzke,\nFabrizio Milo, Falcon Dai, Fei Gao, FloopCZ, Fung Lam, Gautam, GBLin5566, Greg Peatfield,\nGu Wang, Guenther Schmuelling, Hans Pabst, Harun Gunaydin, Huaizheng, Ido Shamay, Ikaro\nSilva, Ilya Edrenkin, Immexxx, James Mishra, Jamie Cooke, Jay Young, Jayaram Bobba,\nJianfei Wang, jinghua2, Joey Meyer, John Maidens, Jonghoon Jin, Julian Villella,\nJun Kim, Jun Shi, Junwei Pan, jyegerlehner, Karan Desai, Karel Van De Plassche,\nKb Sriram, KhabarlakKonstantin, Koan-Sin Tan, krivard, Kwotsin, Leandro Gracia Gil,\nLi Chen, Liangliang He, Louie Helm, lspvic, Luiz Henrique Soares, L\u00e1Szl\u00f3 Csomor,\nMark Wong, Mathew Wicks, Matthew Rahtz, Maxwell Paul Brickner, Michael Hofmann, Miguel\nFlores Ruiz De Eguino, MikeTam1021, Mortada Mehyar, Mycosynth, Namnamseo,\nNate Harada, Neven Miculinic, Nghia Tran, Nick Lyu, Niranjan Hasabnis, Nishidha, Oleksii\nKuchaiev, Oyesh Mann Singh, Panmari, Patrick, Paul Van Eck, Piyush Chaudhary, Quim Llimona,\nRaingo, Richard Davies, Ruben Vereecken, Sahit Chintalapudi, Sam Abrahams, Santiago Castro,\nScott Sievert, Sean O'Keefe, Sebastian Schlecht, Shane, Shubhankar Deshpande, Spencer Schaber,\nSunyeop Lee, t13m, td2014, Thomas H. P. Andersen, Toby Petty, Umang Mehta,\nVadim Markovtsev, Valentin Iovene, Vincent Zhao, Vit Stepanovs, Vivek Rane, Vu Pham, wannabesrevenge,\nweipingpku, wuhaixutab, wydwww, Xiang Gao, Xiaolin Lin, xiaoyaozhuzi, Yaroslav Bulatov, Yi Liu,\nYoshihiro Sugi, Yuan (Terry) Tang, Yuming Wang, Yuxin Wu, Zader Zheng, Zhaojun Zhang, zhengjiajin,\nZhipengShen, Ziming Dong, zjj2wry\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 1.1.0\n\n## Major Features and Improvements\n* Added Java API support for Windows.\n* Added `tf.spectral` module. Moved existing FFT ops to `tf.spectral` while\n  keeping an alias in the old location (`tf.*`).\n* Added 1D, 2D and 3D Fourier transform ops for real signals to `tf.spectral`.\n* Added a `tf.bincount` function.\n* Added Keras 2 API to contrib.\n* Added a new lightweight queue-like object - `RecordInput`.\n* Added `tf.contrib.image.compose_transforms` function.\n* Bring `tf.estimator.*` into the API. Non-deprecated functionality from `tf.contrib.learn.Estimator` is moved to `tf.estimator.Estimator` with cosmetic changes.\n* Docker images: TF images on gcr.io and Docker Hub are upgraded to ubuntu:16.04.\n* Added the following features to TensorFlow Debugger (tfdbg):\n  * Ability to inspect Python source file against TF ops and tensors (command `print_source` / `ps`)\n  * New navigation bar in Curses-based UI\n  * NodeStepper (command `invoke_stepper`) now uses intermediate tensor dumps. It also uses `TensorHandles` as direct feeds during successive `cont` calls for improved performance and reduced memory consumption.\n* Initial release of installation guides for Java, C, and Go.\n* Added Text Dashboard to TensorBoard.\n\n## Deprecations\n\n* TensorFlow 1.1.0 will be the last time we release a binary with Mac GPU support. Going forward, we will stop testing on Mac GPU systems. We continue to welcome patches that maintain Mac GPU support, and we will try to keep the Mac GPU build working.\n\n## Changes to contrib APIs\n* The behavior of RNNCells is now stricter due to the transition towards making RNNCells act more like Keras layers.\n  * If an RNNCell is used twice in two different variable scopes, an error is raised describing how to avoid this behavior.\n  * If an RNNCell is used in a variable scope with existing conflicting variables, an error is raised showing that the RNNCell must be constructed with argument `reuse=True`.\n* Deprecated contrib/distributions `pmf`, `pdf`, `log_pmf`, `log_pdf`.\n* Moved `bayesflow.special_math` to distributions.\n* `tf.contrib.tensor_forest.python.tensor_forest.RandomForestDeviceAssigner` removed.\n* Changed some MVN classes and parameters:\n  * `tf.contrib.distributions.MultivariateNormalFull` replaced by `tf.contrib.distributions.MultivariateNormalTriL`.\n  * `tf.contrib.distributions.MultivariateNormalCholesky` replaced by `tf.contrib.distributions.MultivariateNormalTriL`\n  * `tf.contrib.distributions.MultivariateNormalDiagWithSoftplusStDev` replaced\n    by `tf.contrib.distributions.MultivariateNormalDiagWithSoftplusScale`\n  * `tf.contrib.distributions.MultivariateNormalDiag` arguments changed from `mu`, `diag_stddev` to `log`, `scale_diag`.\n  * `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT` removed.\n  * `tf.contrib.distributions.MultivariateNormalDiagPlusLowRank` added.\n\n## Bug Fixes and Other Changes\n* Java: Support for loading models exported using the SavedModel API (courtesy @EronWright).\n* Go: Added support for incremental graph execution.\n* Fix a bug in the WALS solver when single-threaded.\n* Added support for integer sparse feature values in `tf.contrib.layers.sparse_column_with_keys`.\n* Fixed `tf.set_random_seed(0)` to be deterministic for all ops.\n* Stability improvements for the GCS file system support.\n* Improved TensorForest performance.\n* Added support for multiple filename globs in `tf.matching_files`.\n* `LogMessage` now includes a timestamp as beginning of a message.\n* Added MultiBox person detector example standalone binary.\n* Android demo: Makefile build functionality added to build.gradle to fully support building TensorFlow demo in Android on Windows.\n* Android demo: read MultiBox priors from txt file rather than protobuf.\n* Added colocation constraints to `StagingArea`.\n* `sparse_matmul_op` reenabled for Android builds.\n* Restrict weights rank to be the same as the broadcast target, to avoid ambiguity on broadcast rules.\n* Upgraded libxsmm to 1.7.1 and applied other changes for performance and memory usage.\n* Fixed bfloat16 integration of LIBXSMM sparse mat-mul.\n* Improved performance and reduce memory usage by allowing ops to forward input buffers to output buffers and perform computations in-place.\n* Improved the performance of CPU assignment for strings.\n* Speed up matrix * vector multiplication and matrix * matrix with unknown shapes.\n* C API: Graph imports now support input remapping, control dependencies, and returning imported nodes (see `TF_GraphImportGraphDefWithReturnOutputs()`)\n* Multiple C++ API updates.\n* Multiple TensorBoard updates including:\n  * Users can now view image summaries at various sampled steps (instead of just the last step).\n  * Bugs involving switching runs as well as the image dashboard are fixed.\n  * Removed data download links from TensorBoard.\n  * TensorBoard uses a relative data directory, for easier embedding.\n  * TensorBoard automatically ignores outliers for domain calculation, and formats proportional values consistently.\n* Multiple tfdbg bug fixes:\n  * Fixed Windows compatibility issues.\n  * Command history now persists across runs.\n  * Bug fix in graph validation related to `tf.while_loops`.\n* Java Maven fixes for bugs with Windows installation.\n* Backport fixes and improvements from external keras.\n* Keras config file handling fix.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nA. Besir Kurtulmus, Adal Chiriliuc, @akash, Alec-Desouza, Alex Rothberg, Alex\nSergeev, Alexander Heinecke, Allen Guo, Andreas Madsen, Ankesh Anand, Anton\nLoss, @Aravind, @Arie, Ashutosh Das, Aur\u00e9Lien Geron, Bairen Yi, @bakunyo, Ben\nVisser, Brady Zhou, Calpa Liu, Changming Sun, Chih Cheng Liang, Christopher\nBerner, Clark Zinzow, @Conchylicultor, Dan Ellis, Dan J, Dan Jarvis, Daniel\nYlitalo, Darren Garvey, David Norman, David Truong, @DavidNorman, Dimitar\nPavlov, Dmitry Persiyanov, @Eddie, @elirex, Erfan Noury, Eron Wright, Evgeny\nMazovetskiy, Fabrizio (Misto) Milo, @fanlu, Fisher Coder, Florian Courtial,\nFranck Dernoncourt, Gagan Goel, Gao, Xiang, @Gautam, Gefu Tang, @guilherme,\n@guschmue, Hannah Provenza, Hans Pabst, @hartb, Hsiao Yi, Huazuo Gao, Igor\nChor\u0105\u017cEwicz, Ivan Smirnov, Jakub Kolodziejczyk, Jason Gavris, Jason Morton, Jay\nYoung, Jayaram Bobba, Jeremy Sawruk, Jiaming Liu, Jihun Choi, @jiqiu, Joan Thibault,\nJohn C F, Jojy George Varghese, Jon Malmaud, Julian Berman, Julian Niedermeier,\nJunpeng Lao, Kai Sasaki, @Kankroc, Karl Lessard, Kyle Bostelmann, @Lezcano, Li\nYi, Luo Yun, @lurker, Mahmoud-Abuzaina, Mandeep Singh, Marek Kolodziej, Mark\nSzepieniec, Martial Hue, Medhat Omr, Memo Akten, Michael Gharbi, Micha\u00ebL Defferrard,\nMilan Straka, @MircoT, @mlucool, Muammar Ibn Faisal, Nayana Thorat, @nghiattran,\nNicholas Connor, Nikolaas Steenbergen, Niraj Patel, Niranjan Hasabnis, @Panmari,\nPavel Bulanov, Philip Pries Henningsen, Philipp Jund, @polonez, Prayag Verma, Rahul\nKavi, Raphael Gontijo Lopes, @rasbt, Raven Iqqe, Reid Pryzant, Richard Shin, Rizwan\nAsif, Russell Kaplan, Ryo Asakura, R\u00fcDiger Busche, Saisai Shao, Sam Abrahams, @sanosay,\nSean Papay, @seaotterman, @selay01, Shaurya Sharma, Sriram Narayanamoorthy, Stefano\nProbst, @taknevski, @tbonza, @teldridge11, Tim Anglade, Tomas Reimers, Tomer Gafner,\nValentin Iovene, Vamsi Sripathi, Viktor Malyi, Vit Stepanovs, Vivek Rane, Vlad Firoiu,\n@wangg12, @will, Xiaoyu Tao, Yaroslav Bulatov, Yi Liu, Yuan (Terry) Tang, @Yufeng,\nYuming Wang, Yuxin Wu, Zafar Takhirov, Ziming Dong\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n\n# Release 1.0.1\n\n## Bug Fixes and Other Changes\n* Change GraphConstructor to not increase the version when importing, but instead take the min of all versions.\n* Google Cloud Storage fixes.\n* Removed `tf.core` and `tf.python` modules from the API. These were never intended to be exposed. Please use the same objects through top-level `tf` module instead.\n\n# Release 1.0.0\n\n## Major Features and Improvements\n* XLA (experimental): initial release of [XLA](https://www.tensorflow.org/versions/master/experimental/xla/), a domain-specific compiler for TensorFlow graphs, that targets CPUs and GPUs.\n* TensorFlow Debugger (tfdbg): command-line interface and API.\n* New python 3 docker images added.\n* Made pip packages pypi compliant. TensorFlow can now be installed by `pip\n  install tensorflow` command.\n* Several python API calls have been changed to resemble NumPy more closely.\n* Android: person detection + tracking demo implementing Scalable Object\n  Detection using Deep Neural Networks.\n* New (experimental) [Java API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java).\n* Add new Android image stylization demo based on \"A Learned Representation For Artistic Style\", and add YOLO object detector support.\n\n## Breaking Changes to the API\nTo help you upgrade your existing TensorFlow Python code to match the API changes below, we have prepared a [conversion script](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility).\n* TensorFlow/models have been moved to a separate github repository.\n* Division and modulus operators (/, //, %) now match Python (flooring)\n  semantics. This applies to `tf.div` and `tf.mod` as well. To obtain forced\n  integer truncation based behaviors you can use `tf.truncatediv`\n  and `tf.truncatemod`.\n* `tf.divide()` is now the recommended division function. `tf.div()` will\n  remain, but its semantics do not respond to Python 3 or `from future`\n  mechanisms.\n* tf.reverse() now takes indices of axes to be reversed. E.g.\n  `tf.reverse(a, [True, False, True])` must now be written as\n  `tf.reverse(a, [0, 2])`. `tf.reverse_v2()` will remain until 1.0 final.\n* `tf.mul`, `tf.sub` and `tf.neg` are deprecated in favor of `tf.multiply`,\n  `tf.subtract` and `tf.negative`.\n* `tf.pack` and `tf.unpack` are deprecated in favor of `tf.stack` and\n  `tf.unstack`.\n* `TensorArray.pack` and `TensorArray.unpack` are getting deprecated in favor of\n  `TensorArray.stack` and `TensorArray.unstack`.\n* The following Python functions have had their arguments changed to use `axis`\n  when referring to specific dimensions. We have kept the old keyword arguments\n  for compatibility currently, but we will be removing them well before the\n  final 1.0.\n  * `tf.argmax`: `dimension` becomes `axis`\n  * `tf.argmin`: `dimension` becomes `axis`\n  * `tf.count_nonzero`: `reduction_indices` becomes `axis`\n  * `tf.expand_dims`: `dim` becomes `axis`\n  * `tf.reduce_all`: `reduction_indices` becomes `axis`\n  * `tf.reduce_any`: `reduction_indices` becomes `axis`\n  * `tf.reduce_join`: `reduction_indices` becomes `axis`\n  * `tf.reduce_logsumexp`: `reduction_indices` becomes `axis`\n  * `tf.reduce_max`: `reduction_indices` becomes `axis`\n  * `tf.reduce_mean`: `reduction_indices` becomes `axis`\n  * `tf.reduce_min`: `reduction_indices` becomes `axis`\n  * `tf.reduce_prod`: `reduction_indices` becomes `axis`\n  * `tf.reduce_sum`: `reduction_indices` becomes `axis`\n  * `tf.reverse_sequence`: `batch_dim` becomes `batch_axis`, `seq_dim` becomes `seq_axis`\n  * `tf.sparse_concat`: `concat_dim` becomes `axis`\n  * `tf.sparse_reduce_sum`: `reduction_axes` becomes `axis`\n  * `tf.sparse_reduce_sum_sparse`: `reduction_axes` becomes `axis`\n  * `tf.sparse_split`: `split_dim` becomes `axis`\n* `tf.listdiff` has been renamed to `tf.setdiff1d` to match NumPy naming.\n* `tf.inv` has been renamed to be `tf.reciprocal` (component-wise reciprocal)\n  to avoid confusion with `np.inv` which is matrix inversion\n* tf.round now uses banker's rounding (round to even) semantics to match NumPy.\n* `tf.split` now takes arguments in a reversed order and with different\n  keywords. In particular, we now match NumPy order as\n  `tf.split(value, num_or_size_splits, axis)`.\n* `tf.sparse_split` now takes arguments in reversed order and with different\n  keywords. In particular we now match NumPy order as\n  `tf.sparse_split(sp_input, num_split, axis)`. NOTE: we have temporarily\n  made `tf.sparse_split` require keyword arguments.\n* `tf.concat` now takes arguments in reversed order and with different keywords. In particular we now match NumPy order as `tf.concat(values, axis, name)`.\n* `tf.image.decode_jpeg` by default uses the faster DCT method, sacrificing\n  a little fidelity for improved speed. One can revert to the old\n  behavior by specifying the attribute `dct_method='INTEGER_ACCURATE'`.\n* `tf.complex_abs` has been removed from the Python interface. `tf.abs`\n  supports complex tensors and should be used instead.\n* In the C++ API (in tensorflow/cc), Input, Output, etc. have moved\n  from the tensorflow::ops namespace to tensorflow.\n* Template.`var_scope` property renamed to `.variable_scope`\n* SyncReplicasOptimizer is removed and SyncReplicasOptimizerV2 renamed to SyncReplicasOptimizer.\n* `tf.zeros_initializer()` and `tf.ones_initializer()` now return a callable\n  that must be called with initializer arguments, in your code replace\n  `tf.zeros_initializer` with `tf.zeros_initializer()`.\n* `SparseTensor.shape` has been renamed to `SparseTensor.dense_shape`.  Same for\n  `SparseTensorValue.shape`.\n* Replace tf.scalar_summary, tf.histogram_summary, tf.audio_summary, tf.image_summary with tf.summary.scalar, tf.summary.histogram, tf.summary.audio, tf.summary.image, respectively. The new summary ops take name rather than tag as their first argument, meaning summary ops now respect TensorFlow name scopes.\n* Replace tf.train.SummaryWriter and tf.train.SummaryWriterCache with tf.summary.FileWriter and tf.summary.FileWriterCache.\n* Removes RegisterShape from public API. Use C++ shape function registration\n  instead.\n* Deprecated `_ref` dtypes from the python API.\n* In the C++ API (in tensorflow/cc), Input, Output, etc. have moved\n  from the tensorflow::ops namespace to tensorflow.\n* Change arg order for `{softmax,sparse_softmax,sigmoid}_cross_entropy_with_logits` to be (labels, predictions), and force use of named args.\n* tf.nn.rnn_cell.* and most functions in tf.nn.rnn.* (with the exception of dynamic_rnn and raw_rnn) are temporarily in tf.contrib.rnn.  They will be moved back into core for TF 1.2.\n* `tf.nn.sampled_softmax_loss` and `tf.nn.nce_loss` have both changed their API such that you need to switch the `inputs, labels` to `labels, inputs` parameters.\n* The shape keyword argument of the `SparseTensor` constructor changes its name to `dense_shape` between Tensorflow 0.12 and Tensorflow 1.0.\n\n## Bug Fixes and Other Changes\n* Numerous C++ API updates.\n* New op: `parallel_stack`.\n* Introducing common tf io compression options constants for\n  RecordReader/RecordWriter.\n* Add `sparse_column_with_vocabulary_file`, to specify a feature column that\n  transform string features to IDs, where the mapping is defined by a vocabulary\n  file.\n* Added `index_to_string_table` which returns a lookup table that maps indices to\n  strings.\n* Add `string_to_index_table`, which returns a lookup table that matches strings\n  to indices.\n* Add a `ParallelForWithWorkerId` function.\n* Add `string_to_index_table`, which returns a lookup table that matches strings\n  to indices.\n* Support restore session from checkpoint files in v2 in `contrib/session_bundle`.\n* Added a tf.contrib.image.rotate function for arbitrary angles.\n* Added `tf.contrib.framework.filter_variables` as a convenience function to\n  filter lists of variables based on regular expressions.\n* `make_template()` takes an optional `custom_getter_ param`.\n* Added comment about how existing directories are handled by\n  `recursive_create_dir`.\n* Added an op for QR factorizations.\n* Divides and mods in Python API now use flooring (Python) semantics.\n* Android: pre-built libs are now built nightly.\n* Android: cmake/gradle build for TensorFlow Inference library under\n  `contrib/android/cmake`\n* Android: Much more robust Session initialization code.\n* Android: TF stats now exposed directly in demo and log when debug mode is\n  active\n* Android: new/better README.md documentation\n* saved_model is available as `tf.saved_model`.\n* Empty op is now stateful.\n* Improve speed of scatter_update on the cpu for ASSIGN operations.\n* Change `reduce_join` to treat `reduction_indices` in the same way as other `reduce_` ops.\n* Move `TensorForestEstimator` to `contrib/tensor_forest`.\n* Enable compiler optimizations by default and allow configuration in configure.\n* `tf.divide` now honors the name field.\n* Make metrics weight broadcasting more strict.\n* Add new queue-like `StagingArea` and new ops: `stage` and `unstage`.\n* Enable inplace update ops for strings on CPU. Speed up string concat.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAaron Hu, Abhishek Aggarwal, Adam Michael, Adriano Carmezim, @AfirSraftGarrier,\nAlexander Novikov, Alexander Rosenberg Johansen, Andrew Gibiansky, Andrew Hundt,\nAnish Shah, Anton Loss, @b0noI, @BoyuanJiang, Carl Thom\u00e9, Chad Kennedy, Comic\nChang, Connor Braa, Daniel N. Lang, Daniel Trebbien,\n@danielgordon10, Darcy Liu, Darren Garvey, Dmitri Lapin, Eron Wright, Evan\nCofer, Fabrizio Milo, Finbarr Timbers, Franck Dernoncourt, Garrett Smith,\n@guschmue, Hao Wei, Henrik Holst, Huazuo Gao, @Ian, @Issac, Jacob Israel,\nJangsoo Park, Jin Kim, Jingtian Peng, John Pope, Kye Bostelmann, Liangliang He,\nLing Zhang, Luheng He, Luke Iwanski, @lvli, Michael Basilyan, Mihir Patel,\nMikalai Drabovich, Morten Just, @newge, Nick Butlin, Nishant Shukla,\nPengfei Ni, Przemyslaw Tredak, @rasbt, @Ronny, Rudolf Rosa, @RustingSword,\nSam Abrahams, Sam Putnam, @SeongAhJo, Shi Jiaxin, @skavulya, Steffen M\u00fcLler,\n@TheUSER123, @tiriplicamihai, @vhasanov, Victor Costan, Vit Stepanovs,\nWangda Tan, Wenjian Huang, Xingdong Zuo, Yaroslav Bulatov, Yota Toyama,\nYuan (Terry) Tang, Yuxin Wu\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n\n# Release 0.12.0\n\n## Major Features and Improvements\n\n* TensorFlow now builds and runs on Microsoft Windows (tested on Windows 10,\n  Windows 7, and Windows Server 2016). Supported languages include Python (via a\n  pip package) and C++. CUDA 8.0 and cuDNN 5.1 are supported for GPU\n  acceleration. Known limitations include: It is not currently possible to load\n  a custom op library. The GCS and HDFS file systems are not currently\n  supported. The following ops are not currently implemented:\n  Dequantize, QuantizeAndDequantize, QuantizedAvgPool,\n  QuantizedBatchNomWithGlobalNormalization, QuantizedBiasAdd, QuantizedConcat,\n  QuantizedConv2D, QuantizedMatmul, QuantizedMaxPool,\n  QuantizeDownAndShrinkRange, QuantizedRelu, QuantizedRelu6, QuantizedReshape,\n  QuantizeV2, RequantizationRange, and Requantize.\n* Go: Experimental API in Go to create and execute graphs\n  (https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go)\n* New checkpoint format becomes the default in `tf.train.Saver`. Old V1\n  checkpoints continue to be readable; controlled by the `write_version`\n  argument, `tf.train.Saver` now by default writes out in the new V2\n  format. It significantly reduces the peak memory required and latency\n  incurred during restore.\n* Added a new library for library of matrix-free (iterative) solvers for linear\n  equations, linear least-squares, eigenvalues and singular values in\n  tensorflow/contrib/solvers. Initial version has lanczos bidiagonalization,\n  conjugate gradients and CGLS.\n* Added gradients for `matrix_solve_ls` and `self_adjoint_eig`.\n* Large cleanup to add second order gradient for ops with C++ gradients and\n  improve existing gradients such that most ops can now be differentiated\n  multiple times.\n* Added a solver for ordinary differential equations,\n  `tf.contrib.integrate.odeint`.\n* New contrib module for tensors with named axes, `tf.contrib.labeled_tensor`.\n* Visualization of embeddings in TensorBoard.\n\n## Breaking Changes to the API\n\n* `BusAdjacency` enum replaced with a protocol buffer `DeviceLocality`.  PCI bus\n  indexing now starts from 1 instead of 0, and `bus_id==0` is used where\n  previously `BUS_ANY` was used.\n* `Env::FileExists` and `FileSystem::FileExists` now return a tensorflow::Status\n  instead of a bool. Any callers to this function can be converted to a bool\n  by adding .ok() to the call.\n* The C API type `TF_SessionWithGraph` has been renamed to `TF_Session`,\n  indicating its preferred use in language bindings for TensorFlow.\n  What was previously `TF_Session` has been renamed to `TF_DeprecatedSession`.\n* Renamed `TF_Port` to `TF_Output` in the C API.\n* Removes RegisterShape from public API. Use C++ shape function registration instead.\n  indexing now starts from 1 instead of 0, and `bus_id==0` is used where\n  previously `BUS_ANY` was used.\n* Most RNN cells and RNN functions now use different variable scopes to be\n  consistent with layers (`tf.contrib.layers`).  This means old checkpoints\n  written using this code will not load after this change without providing\n  `Saver` a list of variable renames.  Examples of variable scope changes\n  include `RNN` -> `rnn` in `tf.nn.rnn`, `tf.nn.dynamic_rnn` and moving from\n  `Linear/Matrix` -> `weights` and `Linear/Bias` -> `biases` in most RNN cells.\n* Deprecated tf.select op. tf.where should be used instead.\n* `SparseTensor.shape` has been renamed to `SparseTensor.dense_shape`.  Same for\n  `SparseTensorValue.shape`.\n* `Env::FileExists` and `FileSystem::FileExists` now return a\n  `tensorflow::Status` instead of a bool. Any callers to this function can be\n  converted to a bool by adding `.ok()` to the call.\n* C API: Type `TF_SessionWithGraph` has been renamed to `TF_Session`, indicating\n  its preferred use in language bindings for TensorFlow. What was previously\n  `TF_Session` has been renamed to `TF_DeprecatedSession`.\n* C API: Renamed `TF_Port` to `TF_Output`.\n* C API: The caller retains ownership of `TF_Tensor` objects provided to\n  `TF_Run`, `TF_SessionRun`, `TF_SetAttrTensor` etc.\n* Renamed `tf.image.per_image_whitening()` to\n  `tf.image.per_image_standardization()`\n* Move Summary protobuf constructors to `tf.summary` submodule.\n* Deprecate `histogram_summary`, `audio_summary`, `scalar_summary`,\n  `image_summary`, `merge_summary`, and `merge_all_summaries`.\n* Combined `batch_*` and regular version of linear algebra and FFT ops. The\n  regular op now handles batches as well. All `batch_*` Python interfaces were\n  removed.\n* `tf.all_variables`, `tf.VARIABLES` and `tf.initialize_all_variables` renamed\n  to `tf.global_variables`, `tf.GLOBAL_VARIABLES` and\n  `tf.global_variables_initializer` respectively.\n* `tf.zeros_initializer()` and `tf.ones_initializer()` now return a callable\n  that must be called with initializer arguments, in your code replace\n  `tf.zeros_initializer` with `tf.zeros_initializer()`\n\n## Bug Fixes and Other Changes\n\n* Use threadsafe version of `lgamma` function.\n* Fix `tf.sqrt` handling of negative arguments.\n* Fixed bug causing incorrect number of threads to be used for multi-threaded\n  benchmarks.\n* Performance optimizations for `batch_matmul` on multi-core CPUs.\n* Improve trace, `matrix_set_diag`, `matrix_diag_part` and their gradients to\n  work for rectangular matrices.\n* Support for SVD of complex valued matrices.\n\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n@a7744hsc, Abhi Agg, @admcrae, Adriano Carmezim, Aki Sukegawa, Alex Kendall,\nAlexander Rosenberg Johansen, @amcrae, Amlan Kar, Andre Simpelo, Andreas Eberle,\nAndrew Hundt, Arnaud Lenglet, @b0noI, Balachander Ramachandran, Ben Barsdell,\nBen Guidarelli, Benjamin Mularczyk, Burness Duan, @c0g, Changming Sun,\n@chanis, Corey Wharton, Dan J, Daniel Trebbien, Darren Garvey, David Brailovsky,\nDavid Jones, Di Zeng, @DjangoPeng, Dr. Kashif Rasul, @drag0, Fabrizio (Misto)\nMilo, Fabr\u00edCio Ceschin, @fp, @Ghedeon, @guschmue, G\u00f6k\u00e7en Eraslan, Haosdent\nHuang, Haroen Viaene, Harold Cooper, Henrik Holst, @hoangmit, Ivan Ukhov, Javier\nDehesa, Jingtian Peng, Jithin Odattu, Joan Pastor, Johan Mathe, Johannes Mayer,\nJongwook Choi, Justus Schwabedal, Kai Wolf, Kamil Hryniewicz, Kamran Amini,\nKaren Brems, Karl Lattimer, @kborer, Ken Shirriff, Kevin Rose, Larissa Laich,\nLaurent Mazare, Leonard Lee, Liang-Chi Hsieh, Liangliang He, Luke Iwanski,\nMarek Kolodziej, Moustafa Alzantot, @MrQianjinsi, @nagachika, Neil Han, Nick\nMeehan, Niels Ole Salscheider, Nikhil Mishra, @nschuc, Ondrej Skopek, Ond\u0159Ej\nFilip, @OscarDPan, Pablo Moyano, Przemyslaw Tredak, @qitaishui, @Quarazy,\n@raix852, Philipp Helo, Sam Abrahams, @SriramRamesh, Till Hoffmann, Tushar Soni,\n@tvn, @tyfkda, Uwe Schmidt, Victor Villas, Vit Stepanovs, Vladislav Gubarev,\n@wujingyue, Xuesong Yang, Yi Liu, Yilei Yang, @youyou3, Yuan (Terry) Tang,\nYuming Wang, Zafar Takhirov, @zhongyuk, Ziming Dong, @guotong1988\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 0.11.0\n\n## Major Features and Improvements\n\n* CUDA 8 support.\n* cuDNN 5 support.\n* HDFS Support.\n* Adds Fused LSTM support via cuDNN 5 in `tensorflow/contrib/cudnn_rnn`.\n* Improved support for NumPy style basic slicing including non-1 strides,\n  ellipses, newaxis, and negative indices. For example complicated expressions\n  like `foo[1, 2:4, tf.newaxis, ..., :-3:-1, :]` are now supported. In addition\n  we have preliminary (non-broadcasting) support for sliced assignment to\n  variables. In particular one can write `var[1:3].assign([1,11,111])`.\n* Deprecated `tf.op_scope` and `tf.variable_op_scope` in favor of a unified `tf.name_scope` and `tf.variable_scope`. The new argument order of `tf.variable_scope` is incompatible with previous versions.\n* Introducing `core/util/tensor_bundle` module: a module to efficiently\n  serialize/deserialize tensors to disk.  Will be used in TF's new checkpoint\n  format.\n* Added tf.svd for computing the singular value decomposition (SVD) of dense\n  matrices or batches of matrices (CPU only).\n* Added gradients for eigenvalues and eigenvectors computed using\n  `self_adjoint_eig` or `self_adjoint_eigvals`.\n* Eliminated `batch_*` methods for most linear algebra and FFT ops and promoted\n  the non-batch version of the ops to handle batches of matrices.\n* Tracing/timeline support for distributed runtime (no GPU profiler yet).\n* C API gives access to inferred shapes with `TF_GraphGetTensorNumDims` and\n  `TF_GraphGetTensorShape`.\n* Shape functions for core ops have moved to C++ via\n  `REGISTER_OP(...).SetShapeFn(...)`.  Python shape inference RegisterShape calls\n  use the C++ shape functions with `common_shapes.call_cpp_shape_fn`.  A future\n  release will remove `RegisterShape` from python.\n\n\n## Bug Fixes and Other Changes\n\n* Documentation now includes operator overloads on Tensor and Variable.\n* `tensorflow.__git_version__` now allows users to identify the version of the\n  code that TensorFlow was compiled with. We also have\n  `tensorflow.__git_compiler__` which identifies the compiler used to compile\n  TensorFlow's core.\n* Improved multi-threaded performance of `batch_matmul`.\n* LSTMCell, BasicLSTMCell, and MultiRNNCell constructors now default to\n  `state_is_tuple=True`.  For a quick fix while transitioning to the new\n  default, simply pass the argument `state_is_tuple=False`.\n* DeviceFactory's AddDevices and CreateDevices functions now return\n  a Status instead of void.\n* Int32 elements of list(type) arguments are no longer placed in host memory by\n  default. If necessary, a list(type) argument to a kernel can be placed in host\n  memory using a HostMemory annotation.\n* `uniform_unit_scaling_initializer()` no longer takes a `full_shape` arg,\n  instead relying on the partition info passed to the initializer function when\n  it's called.\n* The NodeDef protocol message is now defined in its own file `node_def.proto`\n  `instead of graph.proto`.\n* `ops.NoGradient` was renamed `ops.NotDifferentiable`. `ops.NoGradient` will\n  be removed soon.\n* `dot.h` / DotGraph was removed (it was an early analysis tool prior\n  to TensorBoard, no longer that useful).  It remains in history\n  should someone find the code useful.\n* re2 / regexp.h was removed from being a public interface of TF.\n  Should users need regular expressions, they should depend on the RE2\n  library directly rather than via TensorFlow.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAbid K, @afshinrahimi, @AidanGG, Ajay Rao, Aki Sukegawa, Alex Rothberg,\nAlexander Rosenberg Johansen, Andrew Gibiansky, Andrew Thomas, @Appleholic,\nBastiaan Quast, Ben Dilday, Bofu Chen, Brandon Amos, Bryon Gloden, Cissp\u00ae,\n@chanis, Chenyang Liu, Corey Wharton, Daeyun Shin, Daniel Julius Lasiman, Daniel\nWaterworth, Danijar Hafner, Darren Garvey, Denis Gorbachev, @DjangoPeng,\nEgor-Krivov, Elia Palme, Eric Platon, Fabrizio Milo, Gaetan Semet,\nGeorg Nebehay, Gu Wang, Gustav Larsson, @haosdent, Harold Cooper, Hw-Zz,\n@ichuang, Igor Babuschkin, Igor Macedo Quintanilha, Ilya Edrenkin, @ironhead,\nJakub Kolodziejczyk, Jennifer Guo, Jihun Choi, Jonas Rauber, Josh Bleecher\nSnyder, @jpangburn, Jules Gagnon-Marchand, Karen Brems, @kborer, Kirill Bobyrev,\nLaurent Mazare, Longqi Yang, Malith Yapa, Maniteja Nandana, Martin Englund,\nMatthias Winkelmann, @mecab, Mu-Ik Jeon, Nand Dalal, Niels Ole Salscheider,\nNikhil Mishra, Park Jiin, Pieter De Rijk, @raix852, Ritwik Gupta, Sahil Sharma,\nSangheum Hwang, @SergejsRk, Shinichiro Hamaji, Simon Denel, @Steve, @suiyuan2009,\nTiago Jorge, Tijmen Tieleman, @tvn, @tyfkda, Wang Yang, Wei-Ting Kuo, Wenjian\nHuang, Yan Chen, @YenChenLin, Yuan (Terry) Tang, Yuncheng Li, Yunfeng Wang, Zack\nPolizzi, @zhongzyd, Ziming Dong, @perhapszzy\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 0.10.0\n\n## Major Features and Improvements\n\n* Added support for C++ shape inference\n* Added graph-construction C API\n* Major revision to the graph-construction C++ API\n* Support makefile build for iOS\n* Added Mac GPU support\n* Full version of TF-Slim available as `tf.contrib.slim`\n* Added k-Means clustering and WALS matrix factorization\n\n## Bug Fixes and Other Changes\n\n* Allow gradient computation for scalar values.\n* Performance improvements for gRPC\n* Improved support for fp16\n* New high-level ops in tf.contrib.{layers,metrics}\n* New features for TensorBoard, such as shape display, exponential smoothing\n* Faster and more stable Google Cloud Storage (GCS) filesystem support\n* Support for zlib compression and decompression for TFRecordReader and TFRecordWriter\n* Support for reading (animated) GIFs\n* Improved support for SparseTensor\n* Added support for more probability distributions (Dirichlet, Beta, Bernoulli, etc.)\n* Added Python interfaces to reset resource containers.\n* Many bugfixes and performance improvements\n* Many documentation fixes\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAlex Rothberg, Andrew Royer, Austin Marshall, @BlackCoal, Bob Adolf, Brian Diesel, Charles-Emmanuel Dias, @chemelnucfin, Chris Lesniewski, Daeyun Shin, Daniel Rodriguez, Danijar Hafner, Darcy Liu, Kristinn R. Th\u00f3risson, Daniel Castro, Dmitry Savintsev, Kashif Rasul, Dylan Paiton, Emmanuel T. Odeke, Ernest Grzybowski, Gavin Sherry, Gideon Dresdner, Gregory King, Harold Cooper, @heinzbeinz, Henry Saputra, Huarong Huo, Huazuo Gao, Igor Babuschkin, Igor Macedo Quintanilha, Ivan Ukhov, James Fysh, Jan Wilken D\u00f6rrie, Jihun Choi, Johnny Lim, Jonathan Raiman, Justin Francis, @lilac, Li Yi, Marc Khoury, Marco Marchesi, Max Melnick, Micael Carvalho, @mikowals, Mostafa Gazar, Nico Galoppo, Nishant Agrawal, Petr Janda, Yuncheng Li, @raix852, Robert Rose, @Robin-des-Bois, Rohit Girdhar, Sam Abrahams, satok16, Sergey Kishchenko, Sharkd Tu, @shotat, Siddharth Agrawal, Simon Denel, @sono-bfio, SunYeop Lee, Thijs Vogels, @tobegit3hub, @Undo1, Wang Yang, Wenjian Huang, Yaroslav Bulatov, Yuan Tang, Yunfeng Wang, Ziming Dong\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 0.9.0\n\n## Major Features and Improvements\n\n* Python 3.5 support and binaries\n* Added iOS support\n* Added support for processing on GPUs on MacOS\n* Added makefile for better cross-platform build support (C API only)\n* fp16 support and improved complex128 support for many ops\n* Higher level functionality in contrib.{layers,losses,metrics,learn}\n* More features to Tensorboard\n* Improved support for string embedding and sparse features\n* The RNN api is finally \"official\" (see, e.g., `tf.nn.dynamic_rnn`,\n  `tf.nn.rnn`, and the classes in `tf.nn.rnn_cell`).\n* TensorBoard now has an Audio Dashboard, with associated audio summaries.\n\n## Bug Fixes and Other Changes\n\n* Turned on CuDNN Autotune.\n* Added support for using third-party Python optimization algorithms (contrib.opt).\n* Google Cloud Storage filesystem support.\n* HDF5 support\n* Add support for 3d convolutions and pooling.\n* Update gRPC release to 0.14.\n* Eigen version upgrade.\n* Switch to eigen thread pool\n* `tf.nn.moments()` now accepts a `shift` argument. Shifting by a good estimate\n  of the mean improves numerical stability. Also changes the behavior of the\n  `shift` argument to `tf.nn.sufficient_statistics()`.\n* Performance improvements\n* Many bugfixes\n* Many documentation fixes\n* TensorBoard fixes: graphs with only one data point, Nan values,\n  reload button and auto-reload, tooltips in scalar charts, run\n  filtering, stable colors\n* Tensorboard graph visualizer now supports run metadata. Clicking on nodes\n  while viewing a stats for a particular run will show runtime statistics, such\n  as memory or compute usage. Unused nodes will be faded out.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAaron Schumacher, Aidan Dang, Akihiko ITOH, Aki Sukegawa, Arbit Chen, Aziz Alto, Danijar Hafner, Erik Erwitt, Fabrizio Milo, Felix Maximilian M\u00f6ller, Henry Saputra, Sung Kim, Igor Babuschkin, Jan Zikes, Jeremy Barnes, Jesper Steen M\u00f8ller, Johannes Mayer, Justin Harris, Kashif Rasul, Kevin Robinson, Loo Rong Jie, Lucas Moura, \u0141ukasz Bieniasz-Krzywiec, Mario Cho, Maxim Grechkin, Michael Heilman, Mostafa Rahmani, Mourad Mourafiq, @ninotoshi, Orion Reblitz-Richardson, Yuncheng Li, @raoqiyu, Robert DiPietro, Sam Abrahams, Sebastian Raschka, Siddharth Agrawal, @snakecharmer1024, Stephen Roller, Sung Kim, SunYeop Lee, Thijs Vogels, Till Hoffmann, Victor Melo, Ville Kallioniemi, Waleed Abdulla, Wenjian Huang, Yaroslav Bulatov, Yeison Rodriguez, Yuan Tang, Yuxin Wu, @zhongzyd, Ziming Dong, Zohar Jackson\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 0.8.0\n\n## Major Features and Improvements\n\n* Added a distributed runtime using GRPC\n* Move skflow to `contrib/learn`\n* Better linear optimizer in `contrib/linear_optimizer`\n* Random forest implementation in `contrib/tensor_forest`\n* CTC loss and decoders in `contrib/ctc`\n* Basic support for `half` data type\n* Better support for loading user ops (see examples in `contrib/`)\n* Allow use of (non-blocking) Eigen threadpool with `TENSORFLOW_USE_EIGEN_THREADPOOL` define\n* Add an extension mechanism for adding network file system support\n* TensorBoard displays metadata stats (running time, memory usage and device used) and tensor shapes\n\n## Bug Fixes and Other Changes\n\n* Utility for inspecting checkpoints\n* Basic tracing and timeline support\n* Allow building against cuDNN 5 (not incl. RNN/LSTM support)\n* Added instructions and binaries for ProtoBuf library with fast serialization and without 64MB limit\n* Added special functions\n* `bool`-strictness: Tensors have to be explicitly compared to `None`\n* Shape strictness: all fed values must have a shape that is compatible with the tensor they are replacing\n* Exposed `tf.while_loop` (deprecated `control_flow_ops.While`)\n* run() now takes RunOptions and RunMetadata, which enable timing stats\n* Fixed lots of potential overflow problems in op kernels\n* Various performance improvements, especially for RNNs and convolutions\n* Many bugfixes\n* Nightly builds, tutorial tests, many test improvements\n* New examples: transfer learning and deepdream ipython notebook\n* Added tutorials, many documentation fixes.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAbhinav Upadhyay, Aggelos Avgerinos, Alan Wu, Alexander G. de G. Matthews, Aleksandr Yahnev, @amchercashin, Andy Kitchen, Aurelien Geron, Awni Hannun, @BanditCat, Bas Veeling, Cameron Chen, @cg31, Cheng-Lung Sung, Christopher Bonnett, Dan Becker, Dan Van Boxel, Daniel Golden, Danijar Hafner, Danny Goodman, Dave Decker, David Dao, David Kretch, Dongjoon Hyun, Dustin Dorroh, @e-lin, Eurico Doirado, Erik Erwitt, Fabrizio Milo, @gaohuazuo, Iblis Lin, Igor Babuschkin, Isaac Hodes, Isaac Turner, Iv\u00e1n Vall\u00e9s, J Yegerlehner, Jack Zhang, James Wexler, Jan Zikes, Jay Young, Jeff Hodges, @jmtatsch, Johnny Lim, Jonas Meinertz Hansen, Kanit Wongsuphasawat, Kashif Rasul, Ken Shirriff, Kenneth Mitchner, Kenta Yonekura, Konrad Magnusson, Konstantin Lopuhin, @lahwran, @lekaha, @liyongsea, Lucas Adams, @makseq, Mandeep Singh, @manipopopo, Mark Amery, Memo Akten, Michael Heilman, Michael Peteuil, Nathan Daly, Nicolas Fauchereau, @ninotoshi, Olav Nymoen, @panmari, @papelita1234, Pedro Lopes, Pranav Sailesh Mani, RJ Ryan, Rob Culliton, Robert DiPietro, @ronrest, Sam Abrahams, Sarath Shekkizhar, Scott Graham, Sebastian Raschka, Sung Kim, Surya Bhupatiraju, Syed Ahmed, Till Hoffmann, @timsl, @urimend, @vesnica, Vlad Frolov, Vlad Zagorodniy, Wei-Ting Kuo, Wenjian Huang, William Dmitri Breaden Madden, Wladimir Schmidt, Yuan Tang, Yuwen Yan, Yuxin Wu, Yuya Kusakabe, @zhongzyd, @znah.\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n\n# Release 0.7.1\n\n## Bug Fixes and Other Changes\n\n* Added gfile.Open and gfile.Copy, used by input_data.py.\n* Fixed Saver bug when MakeDirs tried to create empty directory.\n* GPU Pip wheels are built with cuda 7.5 and cudnn-v4, making them\n  required for the binary releases. Lower versions of cuda/cudnn can\n  be supported by installing from sources and setting the options\n  during ./configure\n* Fix dataset encoding example for Python3 (@danijar)\n* Fix PIP installation by not packaging protobuf as part of wheel,\n  require protobuf 3.0.0b2.\n* Fix Mac pip installation of numpy by requiring pip >= 1.10.1.\n* Improvements and fixes to Docker image.\n\n\n# Release 0.7.0\n\n## Major Features and Improvements\n\n* Allow using any installed Cuda >= 7.0 and cuDNN >= R2, and add support\n  for cuDNN R4\n* Added a `contrib/` directory for unsupported or experimental features,\n  including higher level `layers` module\n* Added an easy way to add and dynamically load user-defined ops\n* Built out a good suite of tests, things should break less!\n* Added `MetaGraphDef` which makes it easier to save graphs with metadata\n* Added assignments for \"Deep Learning with TensorFlow\" udacity course\n\n\n## Bug Fixes and Other Changes\n\n* Added a versioning framework for `GraphDef`s to ensure compatibility\n* Enforced Python 3 compatibility\n* Internal changes now show up as sensibly separated commits\n* Open-sourced the doc generator\n* Un-fork Eigen\n* Simplified the `BUILD` files and cleaned up C++ headers\n* TensorFlow can now be used as a submodule in another bazel build\n* New ops (e.g., `*fft`, `*_matrix_solve`)\n* Support for more data types in many ops\n* Performance improvements\n* Various bugfixes\n* Documentation fixes and improvements\n\n\n## Breaking Changes to the API\n\n* `AdjustContrast` kernel deprecated, new kernel `AdjustContrastv2` takes and\n  outputs float only. `adjust_contrast` now takes all data types.\n* `adjust_brightness`'s `delta` argument is now always assumed to be in `[0,1]`\n  (as is the norm for images in floating point formats), independent of the\n  data type of the input image.\n* The image processing ops do not take `min` and `max` inputs any more, casting\n  safety is handled by `saturate_cast`, which makes sure over- and underflows\n  are handled before casting to data types with smaller ranges.\n* For C++ API users: `IsLegacyScalar` and `IsLegacyVector` are now gone from\n  `TensorShapeUtils` since TensorFlow is scalar strict within Google (for\n  example, the shape argument to `tf.reshape` can't be a scalar anymore).  The\n  open source release was already scalar strict, so outside Google `IsScalar`\n  and `IsVector` are exact replacements.\n* The following files are being removed from `tensorflow/core/public/`:\n    * `env.h` -> `../platform/env.h`\n    * `status.h` -> `../lib/core/status.h`\n    * `tensor.h` -> `../framework/tensor.h`\n    * `tensor_shape.h` -> `../framework/tensor_shape.h`\n    * `partial_tensor_shape.h` -> `../framework/partial_tensor_shape.h`\n    * `tensorflow_server.h` deleted\n* For C++ API users: `TensorShape::ShortDebugString` has been renamed to\n  `DebugString`, and the previous `DebugString` behavior is gone (it was\n  needlessly verbose and produced a confusing empty string for scalars).\n* `GraphOptions.skip_common_subexpression_elimination` has been removed. All\n  graph optimizer options are now specified via\n  `GraphOptions.OptimizerOptions`.\n* `ASSERT_OK` / `EXPECT_OK` macros conflicted with external projects, so they\n  were renamed `TF_ASSERT_OK`, `TF_EXPECT_OK`.  The existing macros are\n  currently maintained for short-term compatibility but will be removed.\n* The non-public `nn.rnn` and the various `nn.seq2seq` methods now return\n  just the final state instead of the list of all states.\n* `tf.scatter_update` now no longer guarantees that lexicographically largest\n  index be used for update when duplicate entries exist.\n* `tf.image.random_crop(image, [height, width])` is now\n  `tf.random_crop(image, [height, width, depth])`, and `tf.random_crop` works\n  for any rank (not just 3-D images).  The C++ `RandomCrop` op has been replaced\n  with pure Python.\n* Renamed `tf.test.GetTempDir` and `tf.test.IsBuiltWithCuda` to\n  `tf.test.get_temp_dir` and `tf.test.is_built_with_cuda` for PEP-8\n  compatibility.\n* `parse_example`'s interface has changed, the old interface is accessible in\n  `legacy_parse_example` (same for related functions).\n* New `Variable`s are not added to the same collection several times even if\n  a list with duplicates is passed to the constructor.\n* The Python API will now properly set the `list` member of `AttrValue` in\n  constructed `GraphDef` messages for empty lists.  The serialization of some\n  graphs will change, but the change is both forwards and backwards compatible.\n  It will break tests that compare a generated `GraphDef` to a golden serialized\n  `GraphDef` (which is discouraged).\n\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAkiomi Kamakura, Alex Vig, Alexander Rosenberg Johansen, Andre Cruz, Arun Ahuja,\nBart Coppens, Bernardo Pires, Carl Vondrick, Cesar Salgado, Chen Yu,\nChristian Jauvin, Damien Aymeric, Dan Vanderkam, Denny Britz, Dongjoon Hyun,\nEren G\u00fcven, Erik Erwitt, Fabrizio Milo, G. Hussain Chinoy, Jim Fleming,\nJoao Felipe Santos, Jonas Meinertz Hansen, Joshi Rekha, Julian Viereck,\nKeiji Ariyama, Kenton Lee, Krishna Sankar, Kristina Chodorow, Linchao Zhu,\nLukas Krecan, Mark Borgerding, Mark Daoust, Moussa Taifi,\nNathan Howell, Naveen Sundar Govindarajulu, Nick Sweeting, Niklas Riekenbrauck,\nOlivier Grisel, Patrick Christ, Povilas Liubauskas, Rainer Wasserfuhr,\nRomain Thouvenin, Sagan Bolliger, Sam Abrahams, Taehoon Kim, Timothy J Laurent,\nVlad Zavidovych, Yangqing Jia, Yi-Lin Juang, Yuxin Wu, Zachary Lipton,\nZero Chen, Alan Wu, @brchiu, @emmjaykay, @jalammar, @Mandar-Shinde,\n@nsipplswezey, @ninotoshi, @panmari, @prolearner and @rizzomichaelg.\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n\n# Release 0.6.0\n\n## Major Features and Improvements\n\n* Python 3.3+ support via changes to python codebase and ability\n  to specify python version via ./configure.\n\n* Some improvements to GPU performance and memory usage:\n  [convnet benchmarks](https://github.com/soumith/convnet-benchmarks/issues/66)\n  roughly equivalent with native cudnn v2 performance.  Improvements mostly due\n  to moving to 32-bit indices, faster shuffling kernels.  More improvements to\n  come in later releases.\n\n\n## Bug Fixes\n\n* Lots of fixes to documentation and tutorials, many contributed\n  by the public.\n\n* 271 closed issues on github issues.\n\n## Backwards-Incompatible Changes\n\n* `tf.nn.fixed_unigram_candidate_sampler` changed its default 'distortion'\n  attribute from 0.0 to 1.0. This was a bug in the original release\n  that is now fixed.\n\n# Release 0.5.0\n\nInitial release of TensorFlow.\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: disable=protected-access\n\"\"\"A `Network` is way to compose layers: the topological form of a `Model`.\"\"\"\n\nimport collections\nimport copy\nimport itertools\nimport warnings\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.keras.engine import base_layer_utils\nfrom tensorflow.python.keras.engine import input_layer as input_layer_module\nfrom tensorflow.python.keras.engine import input_spec\nfrom tensorflow.python.keras.engine import node as node_module\nfrom tensorflow.python.keras.engine import training as training_lib\nfrom tensorflow.python.keras.engine import training_utils\nfrom tensorflow.python.keras.saving.saved_model import network_serialization\nfrom tensorflow.python.keras.utils import generic_utils\nfrom tensorflow.python.keras.utils import tf_inspect\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training.tracking import base as trackable\nfrom tensorflow.python.util import nest\nfrom tensorflow.tools.docs import doc_controls\n\n\n# pylint: disable=g-classes-have-attributes\nclass Functional(training_lib.Model):\n  \"\"\"A `Functional` model is a `Model` defined as a directed graph of layers.\n\n  Three types of `Model` exist: subclassed `Model`, `Functional` model,\n  and `Sequential` (a special case of `Functional`).\n  In general, more Keras features are supported with `Functional`\n  than with subclassed `Model`s, specifically:\n\n  - Model cloning (`keras.models.clone`)\n  - Serialization (`model.get_config()/from_config`, `model.to_json()/to_yaml()`\n  - Whole-model saving (`model.save()`)\n\n  A `Functional` model can be instantiated by passing two arguments to\n  `__init__`. The first argument is the `keras.Input` Tensors that represent\n  the inputs to the model. The second argument specifies the output\n  tensors that represent the outputs of this model. Both arguments can be a\n  nested structure of tensors.\n\n  Example:\n\n  ```\n  inputs = {'x1': keras.Input(shape=(10,)), 'x2': keras.Input(shape=(1,))}\n  t = keras.layers.Dense(1, activation='relu')(inputs['x1'])\n  outputs = keras.layers.Add()([t, inputs['x2'])\n  model = keras.Model(inputs, outputs)\n  ```\n\n  A `Functional` model constructed using the Functional API can also include raw\n  TensorFlow functions, with the exception of functions that create Variables\n  or assign ops.\n\n  Example:\n\n  ```\n  inputs = keras.Input(shape=(10,))\n  x = keras.layers.Dense(1)(inputs)\n  outputs = tf.nn.relu(x)\n  model = keras.Model(inputs, outputs)\n  ```\n\n  Args:\n    inputs: List of input tensors (must be created via `tf.keras.Input()`).\n    outputs: List of output tensors.\n    name: String, optional. Name of the model.\n    trainable: Boolean, optional. If the model's variables should be trainable.\n  \"\"\"\n\n  # See tf.Module for the usage of this property.\n  # The key of _layer_call_argspecs is a layer. tf.Module._flatten will fail to\n  # flatten the key since it is trying to convert Trackable/Layer to a string.\n  _TF_MODULE_IGNORED_PROPERTIES = frozenset(itertools.chain(\n      ('_layer_call_argspecs', '_compiled_trainable_state',\n       '_output_mask_cache', '_output_tensor_cache', '_output_shape_cache'),\n      training_lib.Model._TF_MODULE_IGNORED_PROPERTIES\n  ))\n\n  @trackable.no_automatic_dependency_tracking\n  def __init__(self, inputs, outputs, name=None, trainable=True,\n               **kwargs):\n    # This is used by the Model class, since we have some logic to swap the\n    # class in the __new__ method, which will lead to __init__ get invoked\n    # twice. Using the skip_init to skip one of the invocation of __init__ to\n    # avoid any side effects\n    skip_init = kwargs.pop('skip_init', False)\n    if skip_init:\n      return\n    generic_utils.validate_kwargs(kwargs, {})\n    super(Functional, self).__init__(name=name, trainable=trainable)\n    self._init_graph_network(inputs, outputs)\n\n  @trackable.no_automatic_dependency_tracking\n  def _init_graph_network(self, inputs, outputs):\n    # This method is needed for Sequential to reinitialize graph network when\n    # layer is added or removed.\n    self._is_graph_network = True\n\n    # Normalize and set self.inputs, self.outputs.\n    if isinstance(inputs, list) and len(nest.flatten(inputs)) == 1:\n      inputs = inputs[0]\n    if isinstance(outputs, list) and len(nest.flatten(outputs)) == 1:\n      outputs = outputs[0]\n    self._nested_inputs = inputs\n    self._nested_outputs = outputs\n    self.inputs = nest.flatten(inputs)\n    self.outputs = nest.flatten(outputs)\n\n    # Models constructed with a single Tensor or list of Tensors can\n    # be called with a dict, where the keys of the dict are the names\n    # of the `Input` objects. Extra keys are ignored with warning.\n    if not nest.is_nested(self._nested_inputs):\n      self._enable_dict_to_input_mapping = True\n    elif (isinstance(self._nested_inputs, (list, tuple)) and\n          not any(nest.is_nested(t) for t in self._nested_inputs)):\n      self._enable_dict_to_input_mapping = True\n    elif (isinstance(self._nested_inputs, dict) and\n          not any(nest.is_nested(t) for t in self._nested_inputs.values())):\n      self._enable_dict_to_input_mapping = True\n    else:\n      self._enable_dict_to_input_mapping = False\n\n    if not ops.executing_eagerly_outside_functions():\n      if any(not hasattr(tensor, '_keras_history') for tensor in self.outputs):\n        base_layer_utils.create_keras_history(self._nested_outputs)\n\n    self._validate_graph_inputs_and_outputs()\n\n    # A Network does not create weights of its own, thus it is already\n    # built.\n    self.built = True\n    self._build_input_shape = nest.map_structure(lambda x: x.shape, inputs)\n    self._compute_output_and_mask_jointly = True\n    # `_expects_training_arg` is True since the `training` argument is always\n    # present in the signature of the `call` method of a graph network.\n    self._expects_training_arg = True\n    self._expects_mask_arg = True\n    # A graph network does not autocast inputs, as its layers will cast them\n    # instead.\n    self._autocast = False\n\n    self._input_layers = []\n    self._output_layers = []\n    self._input_coordinates = []\n    self._output_coordinates = []\n\n    # This is for performance optimization when calling the Network on new\n    # inputs. Every time the Network is called on a set on input tensors,\n    # we compute the output tensors, output masks and output shapes in one pass,\n    # then cache them here. When any of these outputs is queried later, we\n    # retrieve it from there instead of recomputing it.\n    self._output_mask_cache = {}\n    self._output_tensor_cache = {}\n    self._output_shape_cache = {}\n\n    # Build self._output_layers:\n    for x in self.outputs:\n      layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access\n      self._output_layers.append(layer)\n      self._output_coordinates.append((layer, node_index, tensor_index))\n\n    # Build self._input_layers:\n    for x in self.inputs:\n      layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access\n      # It's supposed to be an input layer, so only one node\n      # and one tensor output.\n      assert node_index == 0\n      assert tensor_index == 0\n      self._input_layers.append(layer)\n      self._input_coordinates.append((layer, node_index, tensor_index))\n\n    # Keep track of the network's nodes and layers.\n    nodes, nodes_by_depth, layers, _ = _map_graph_network(\n        self.inputs, self.outputs)\n    self._network_nodes = nodes\n    self._nodes_by_depth = nodes_by_depth\n    self._self_tracked_trackables = layers\n    self._layer_call_argspecs = {}\n    for layer in self._self_tracked_trackables:\n      self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)\n\n    # Build self.input_names and self.output_names.\n    self._set_output_names()\n    self.input_names = []\n    self._feed_input_names = []\n    self._feed_inputs = []\n    self._feed_input_shapes = []\n    for layer in self._input_layers:\n      self.input_names.append(layer.name)\n      if layer.is_placeholder:\n        self._feed_input_names.append(layer.name)\n        # Use batch_input_shape here because non-eager composite tensors may not\n        # have a shape attribute that's meaningful (sparse, for instance, has\n        # a tensor that's non-constant and needs to be fed). This means that\n        # input layers that create placeholders will need to have the\n        # batch_input_shape attr to allow for input shape validation.\n        self._feed_input_shapes.append(layer._batch_input_shape)\n        self._feed_inputs.append(layer.input)\n\n    self._compute_tensor_usage_count()\n    self._set_save_spec(self._nested_inputs)\n    tf_utils.assert_no_legacy_layers(self.layers)\n\n  @property\n  def input(self):\n    \"\"\"Retrieves the input tensor(s) of a layer.\n\n    Only applicable if the layer has exactly one input,\n    i.e. if it is connected to one incoming layer.\n\n    Returns:\n        Input tensor or list of input tensors.\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n      AttributeError: If no inbound nodes are found.\n    \"\"\"\n    return self._nested_inputs\n\n  @property\n  def input_shape(self):\n    \"\"\"Retrieves the input shape(s) of a layer.\n\n    Only applicable if the layer has exactly one input,\n    i.e. if it is connected to one incoming layer, or if all inputs\n    have the same shape.\n\n    Returns:\n        Input shape, as an integer shape tuple\n        (or list of shape tuples, one tuple per input tensor).\n\n    Raises:\n        AttributeError: if the layer has no defined input_shape.\n        RuntimeError: if called in Eager mode.\n    \"\"\"\n    return nest.map_structure(backend.int_shape, self.input)\n\n  @property\n  def input_spec(self):\n    if hasattr(self, '_manual_input_spec'):\n      return self._manual_input_spec\n    if (isinstance(self._nested_inputs, (dict, list, tuple)) and\n        len(self._nested_inputs) != len(self.inputs)):\n      # Case where we have a nested structure.\n      # In such a case we can't safely run any checks.\n      return None\n    if isinstance(self._nested_inputs, dict):\n      # Case where `_nested_inputs` is a plain dict of Inputs.\n      names = sorted(self._nested_inputs.keys())\n      return [input_spec.InputSpec(\n          shape=shape_with_no_batch_size(self._nested_inputs[name]),\n          allow_last_axis_squeeze=True, name=name) for name in names]\n    else:\n      # Single input, or list / tuple of inputs.\n      # The data may be passed as a dict keyed by input name.\n      return [input_spec.InputSpec(\n          shape=shape_with_no_batch_size(x), allow_last_axis_squeeze=True,\n          name=x._keras_history.layer.name) for x in self.inputs]\n\n  @input_spec.setter\n  def input_spec(self, value):\n    self._manual_input_spec = value\n\n  @property\n  def output(self):\n    \"\"\"Retrieves the output tensor(s) of a layer.\n\n    Only applicable if the layer has exactly one output,\n    i.e. if it is connected to one incoming layer.\n\n    Returns:\n      Output tensor or list of output tensors.\n\n    Raises:\n      AttributeError: if the layer is connected to more than one incoming\n        layers.\n      RuntimeError: if called in Eager mode.\n    \"\"\"\n    return self._nested_outputs\n\n  @property\n  def output_shape(self):\n    \"\"\"Retrieves the output shape(s) of a layer.\n\n    Only applicable if the layer has one output,\n    or if all outputs have the same shape.\n\n    Returns:\n        Output shape, as an integer shape tuple\n        (or list of shape tuples, one tuple per output tensor).\n\n    Raises:\n        AttributeError: if the layer has no defined output shape.\n        RuntimeError: if called in Eager mode.\n    \"\"\"\n    return nest.map_structure(backend.int_shape, self.output)\n\n  def _set_output_names(self):\n    \"\"\"Assigns unique names to the Network's outputs.\n\n    Output layers with multiple output tensors would otherwise lead to duplicate\n    names in self.output_names.\n    \"\"\"\n    uniquified = []\n    output_names = set()\n    prefix_count = {}\n    for layer in self._output_layers:\n      proposal = layer.name\n      while proposal in output_names:\n        existing_count = prefix_count.get(layer.name, 1)\n        proposal = '{}_{}'.format(layer.name, existing_count)\n        prefix_count[layer.name] = existing_count + 1\n      output_names.add(proposal)\n      uniquified.append(proposal)\n    self.output_names = uniquified\n\n  @property\n  def _layer_checkpoint_dependencies(self):\n    \"\"\"Dictionary of layer dependencies to be included in the checkpoint.\"\"\"\n    weight_layer_index = 0\n\n    dependencies = collections.OrderedDict()\n    for layer_index, layer in enumerate(self.layers):\n      try:\n        if layer.weights:\n          # Keep a separate index for layers which have weights. This allows\n          # users to insert Layers without weights anywhere in the network\n          # without breaking checkpoints.\n          dependencies['layer_with_weights-%d' % weight_layer_index] = layer\n          weight_layer_index += 1\n      except ValueError:\n        # The layer might have weights, but may not be built yet. We just treat\n        # it as layer without weight.\n        pass\n\n      # Even if it doesn't have weights, we should still track everything in\n      # case it has/will have Trackable dependencies.\n      dependencies['layer-%d' % layer_index] = layer\n    return dependencies\n\n  @property\n  def _checkpoint_dependencies(self):\n    dependencies = [\n        trackable.TrackableReference(name=name, ref=layer)\n        for name, layer in self._layer_checkpoint_dependencies.items()]\n    dependencies.extend(super(Functional, self)._checkpoint_dependencies)\n    return dependencies\n\n  def _lookup_dependency(self, name):\n    layer_dependencies = self._layer_checkpoint_dependencies\n    if name in layer_dependencies:\n      return layer_dependencies[name]\n    return super(Functional, self)._lookup_dependency(name)\n\n  def _handle_deferred_layer_dependencies(self, layers):\n    \"\"\"Handles layer checkpoint dependencies that are added after init.\"\"\"\n    layer_checkpoint_dependencies = self._layer_checkpoint_dependencies\n    layer_to_name = {v: k for k, v in layer_checkpoint_dependencies.items()}\n    for layer in layers:\n      if layer in layer_to_name:\n        self._handle_deferred_dependencies(name=layer_to_name[layer],\n                                           trackable=layer)\n\n  @property\n  def _should_compute_mask(self):\n    return True\n\n  def compute_mask(self, inputs, mask):\n    # TODO(omalleyt): b/123540974 This function is not really safe to call\n    # by itself because it will duplicate any updates and losses in graph\n    # mode by `call`ing the Layers again.\n    output_tensors = self._run_internal_graph(inputs, mask=mask)\n    return nest.map_structure(lambda t: getattr(t, '_keras_mask', None),\n                              output_tensors)\n\n  @doc_controls.do_not_doc_inheritable\n  def call(self, inputs, training=None, mask=None):\n    \"\"\"Calls the model on new inputs.\n\n    In this case `call` just reapplies\n    all ops in the graph to the new inputs\n    (e.g. build a new computational graph from the provided inputs).\n\n    Args:\n        inputs: A tensor or list of tensors.\n        training: Boolean or boolean scalar tensor, indicating whether to run\n          the `Network` in training mode or inference mode.\n        mask: A mask or list of masks. A mask can be\n            either a tensor or None (no mask).\n\n    Returns:\n        A tensor if there is a single output, or\n        a list of tensors if there are more than one outputs.\n    \"\"\"\n    return self._run_internal_graph(\n        inputs, training=training, mask=mask)\n\n  def compute_output_shape(self, input_shape):\n    # Convert any shapes in tuple format to TensorShapes.\n    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n\n    if len(nest.flatten(input_shape)) != len(nest.flatten(self._input_layers)):\n      raise ValueError('Invalid input_shape argument ' + str(input_shape) +\n                       ': model has ' + str(len(self._input_layers)) +\n                       ' tensor inputs.')\n\n    # Use the tuple of TensorShape as the cache key, since tuple is hashable\n    # and can be used as hash key.\n    try:\n      cache_key = tuple(tf_utils.convert_shapes(input_shape, to_tuples=True))\n      if cache_key in self._output_shape_cache:\n        # Cache hit. Return shapes as TensorShapes.\n        return self._output_shape_cache[cache_key]\n    except ValueError:\n      # In case there are unknown TensorShape, eg for sparse tensor input,\n      # We skip the caching since the shape is unknown.\n      pass\n\n    layers_to_output_shapes = {}\n    for layer, shape in zip(self._input_layers, nest.flatten(input_shape)):\n      # It's an input layer: then `compute_output_shape` is identity,\n      # and there is only one node and one tensor..\n      shape_key = layer.name + '_0_0'\n      layers_to_output_shapes[shape_key] = shape\n\n    depth_keys = list(self._nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n    # Iterate over nodes, by depth level.\n    if len(depth_keys) > 1:\n      for depth in depth_keys:\n        nodes = self._nodes_by_depth[depth]\n        for node in nodes:\n          layer = node.layer\n          if layer in self._input_layers:\n            # We've already covered the input layers\n            # a few lines above.\n            continue\n          # Get the input shapes for the first argument of the node\n          layer_input_shapes = []\n          layer_inputs = node.call_args[0]\n          for layer_input in nest.flatten(layer_inputs):\n            kh = layer_input._keras_history\n            input_layer_key = kh.layer.name + '_%s_%s' % (kh.node_index,\n                                                          kh.tensor_index)\n            layer_input_shapes.append(layers_to_output_shapes[input_layer_key])\n          layer_input_shapes = nest.pack_sequence_as(layer_inputs,\n                                                     layer_input_shapes)\n          # Layers expect shapes to be tuples for `compute_output_shape`.\n          layer_input_shapes = tf_utils.convert_shapes(\n              layer_input_shapes, to_tuples=True)\n          layer_output_shapes = layer.compute_output_shape(layer_input_shapes)\n          # Convert back to TensorShapes.\n          layer_output_shapes = tf_utils.convert_shapes(\n              layer_output_shapes, to_tuples=False)\n\n          node_index = layer._inbound_nodes.index(node)  # pylint: disable=protected-access\n          for j, shape in enumerate(nest.flatten(layer_output_shapes)):\n            shape_key = layer.name + '_%s_%s' % (node_index, j)\n            layers_to_output_shapes[shape_key] = shape\n\n      # Read final output shapes from layers_to_output_shapes.\n      output_shapes = []\n      for i in range(len(self._output_layers)):\n        layer, node_index, tensor_index = self._output_coordinates[i]\n        shape_key = layer.name + '_%s_%s' % (node_index, tensor_index)\n        output_shapes.append(layers_to_output_shapes[shape_key])\n      output_shapes = nest.pack_sequence_as(self._nested_outputs, output_shapes)\n      # Store in cache.\n      self._output_shape_cache[cache_key] = output_shapes\n\n    # Return shapes as TensorShapes.\n    return output_shapes\n\n  def _init_set_name(self, name, zero_based=True):\n    if not name:\n      cls_name = self.__class__.__name__\n      if self.__class__ == Functional:\n        # Hide the functional class name from user, since its not a public\n        # visible class. Use \"Model\" instead,\n        cls_name = 'Model'\n      self._name = backend.unique_object_name(\n          generic_utils.to_snake_case(cls_name),\n          zero_based=zero_based)\n    else:\n      self._name = name\n\n  def _run_internal_graph(self, inputs, training=None, mask=None):\n    \"\"\"Computes output tensors for new inputs.\n\n    # Note:\n        - Can be run on non-Keras tensors.\n\n    Args:\n        inputs: Tensor or nested structure of Tensors.\n        training: Boolean learning phase.\n        mask: (Optional) Tensor or nested structure of Tensors.\n\n    Returns:\n        output_tensors\n    \"\"\"\n    inputs = self._flatten_to_reference_inputs(inputs)\n    if mask is None:\n      masks = [None] * len(inputs)\n    else:\n      masks = self._flatten_to_reference_inputs(mask)\n    for input_t, mask in zip(inputs, masks):\n      input_t._keras_mask = mask\n\n    # Dictionary mapping reference tensors to computed tensors.\n    tensor_dict = {}\n    tensor_usage_count = self._tensor_usage_count\n    for x, y in zip(self.inputs, inputs):\n      y = self._conform_to_reference_input(y, ref_input=x)\n      x_id = str(id(x))\n      tensor_dict[x_id] = [y] * tensor_usage_count[x_id]\n\n    nodes_by_depth = self._nodes_by_depth\n    depth_keys = list(nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n\n    for depth in depth_keys:\n      nodes = nodes_by_depth[depth]\n      for node in nodes:\n        if node.is_input:\n          continue  # Input tensors already exist.\n\n        if any(t_id not in tensor_dict for t_id in node.flat_input_ids):\n          continue  # Node is not computable, try skipping.\n\n        args, kwargs = node.map_arguments(tensor_dict)\n        outputs = node.layer(*args, **kwargs)\n\n        # Update tensor_dict.\n        for x_id, y in zip(node.flat_output_ids, nest.flatten(outputs)):\n          tensor_dict[x_id] = [y] * tensor_usage_count[x_id]\n\n    output_tensors = []\n    for x in self.outputs:\n      x_id = str(id(x))\n      assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n      output_tensors.append(tensor_dict[x_id].pop())\n\n    return nest.pack_sequence_as(self._nested_outputs, output_tensors)\n\n  def _flatten_to_reference_inputs(self, tensors):\n    \"\"\"Maps `tensors` to their respective `keras.Input`.\"\"\"\n    if self._enable_dict_to_input_mapping and isinstance(tensors, dict):\n      ref_inputs = self._nested_inputs\n      if not nest.is_nested(ref_inputs):\n        ref_inputs = [self._nested_inputs]\n      if isinstance(ref_inputs, dict):\n        # In the case that the graph is constructed with dict input tensors,\n        # We will use the original dict key to map with the keys in the input\n        # data. Note that the model.inputs is using nest.flatten to process the\n        # input tensors, which means the dict input tensors are ordered by their\n        # keys.\n        ref_input_names = sorted(ref_inputs.keys())\n      else:\n        ref_input_names = [inp._keras_history.layer.name for inp in ref_inputs]\n\n      # Raise an warning if there are more input data comparing to input tensor\n      if len(tensors) > len(ref_input_names):\n        warnings.warn(\n            'Input dict contained keys {} which did not match any model input. '\n            'They will be ignored by the model.'.format(\n                [n for n in tensors.keys() if n not in ref_input_names])\n            )\n\n      try:\n        # Flatten in the order `Input`s were passed during Model construction.\n        return [tensors[n] for n in ref_input_names]\n      except KeyError:\n        # TODO(b/151582614)\n        return nest.flatten(tensors)\n\n    # Otherwise both self.inputs and tensors will already be in same order.\n    return nest.flatten(tensors)\n\n  def _conform_to_reference_input(self, tensor, ref_input):\n    \"\"\"Set shape and dtype based on `keras.Input`s.\"\"\"\n    if isinstance(tensor, ops.Tensor):\n      # Allow (None,) and (None, 1) Tensors to be passed interchangeably. Use\n      # the shape specified by the `keras.Input`.\n      t_shape = tensor.shape\n      t_rank = t_shape.rank\n      ref_shape = ref_input.shape\n      ref_rank = ref_shape.rank\n      keras_history = getattr(tensor, '_keras_history', None)\n      if t_rank is not None and ref_rank is not None:\n        # Should squeeze last dimension.\n        # True if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).\n        if (t_rank == ref_rank + 1 and t_shape[-1] == 1):\n          tensor = array_ops.squeeze_v2(tensor, axis=-1)\n        # Should expand last_dimension.\n        # True if tensor is (BATCH, ...) and reference is (BATCH, ..., 1).\n        elif (t_rank == ref_rank - 1 and ref_shape[-1] == 1):\n          tensor = array_ops.expand_dims_v2(tensor, axis=-1)\n      if keras_history is not None:  # Restore keras history.\n        tensor._keras_history = keras_history\n\n      # Add shape hints to Tensors that may have None shape dims but have shapes\n      # defined by the `keras.Input` (not applicable in eager mode).\n      if not context.executing_eagerly():\n        try:\n          tensor.set_shape(tensor.shape.merge_with(ref_input.shape))\n        except ValueError:\n          logging.warning(\n              'Model was constructed with shape {} for input {}, but it was '\n              'called on an input with incompatible shape {}.'.format(\n                  ref_input.shape, ref_input, tensor.shape))\n\n      # Dtype casting.\n      tensor = math_ops.cast(tensor, dtype=ref_input.dtype)\n    elif tf_utils.is_extension_type(tensor):\n      # Dtype casting (If the extension type has a non-variant dtype and\n      # supports being cast)\n      ref_input_dtype = getattr(ref_input, 'dtype', None)\n      if ref_input_dtype is not None and ref_input_dtype != dtypes.variant:\n        tensor = math_ops.cast(tensor, dtype=ref_input_dtype)\n\n    return tensor\n\n  def get_config(self):\n    return copy.deepcopy(get_network_config(self))\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None):\n    \"\"\"Instantiates a Model from its config (output of `get_config()`).\n\n    Args:\n        config: Model config dictionary.\n        custom_objects: Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    Returns:\n        A model instance.\n\n    Raises:\n        ValueError: In case of improperly formatted config dict.\n    \"\"\"\n    with generic_utils.SharedObjectLoadingScope():\n      input_tensors, output_tensors, created_layers = reconstruct_from_config(\n          config, custom_objects)\n      model = cls(inputs=input_tensors, outputs=output_tensors,\n                  name=config.get('name'))\n      connect_ancillary_layers(model, created_layers)\n      return model\n\n  def _validate_graph_inputs_and_outputs(self):\n    \"\"\"Validates the inputs and outputs of a Graph Network.\"\"\"\n    # Check for redundancy in inputs.\n    if len({id(i) for i in self.inputs}) != len(self.inputs):\n      raise ValueError('The list of inputs passed to the model '\n                       'is redundant. '\n                       'All inputs should only appear once.'\n                       ' Found: ' + str(self.inputs))\n\n    for x in self.inputs:\n      # Check that x has appropriate `_keras_history` metadata.\n      if not hasattr(x, '_keras_history'):\n        cls_name = self.__class__.__name__\n        raise ValueError('Input tensors to a ' + cls_name + ' ' +\n                         'must come from `tf.keras.Input`. '\n                         'Received: ' + str(x) +\n                         ' (missing previous layer metadata).')\n      # Check that x is an input tensor.\n      # pylint: disable=protected-access\n      layer = x._keras_history.layer\n      if len(layer._inbound_nodes) > 1 or (\n          layer._inbound_nodes and not layer._inbound_nodes[0].is_input):\n        cls_name = self.__class__.__name__\n        logging.warning(cls_name + ' model inputs must come from '\n                        '`tf.keras.Input` (thus holding past layer metadata), '\n                        'they cannot be the output of '\n                        'a previous non-Input layer. '\n                        'Here, a tensor specified as '\n                        'input to \"' + self.name + '\" was not an Input tensor, '\n                        'it was generated by layer ' + layer.name + '.\\n'\n                        'Note that input tensors are '\n                        'instantiated via `tensor = tf.keras.Input(shape)`.\\n'\n                        'The tensor that caused the issue was: ' + str(x.name))\n\n    # Check compatibility of batch sizes of Input Layers.\n    input_batch_sizes = [\n        training_utils.get_static_batch_size(x._keras_history.layer)\n        for x in self.inputs\n    ]\n    consistent_batch_size = None\n    for batch_size in input_batch_sizes:\n      if batch_size is not None:\n        if (consistent_batch_size is not None and\n            batch_size != consistent_batch_size):\n          raise ValueError('The specified batch sizes of the Input Layers'\n                           ' are incompatible. Found batch sizes: {}'.format(\n                               input_batch_sizes))\n        consistent_batch_size = batch_size\n\n    for x in self.outputs:\n      if not hasattr(x, '_keras_history'):\n        cls_name = self.__class__.__name__\n        raise ValueError('Output tensors of a ' + cls_name + ' model must be '\n                         'the output of a TensorFlow `Layer` '\n                         '(thus holding past layer metadata). Found: ' + str(x))\n\n  def _insert_layers(self, layers, relevant_nodes=None):\n    \"\"\"Inserts Layers into the Network after Network creation.\n\n    This is only valid for Keras Graph Networks.  Layers added via this function\n    will be included in the `call` computation and `get_config` of this Network.\n    They will not be added to the Network's outputs.\n\n\n    Args:\n      layers: Arbitrary nested structure of Layers. Layers must be reachable\n        from one or more of the `keras.Input` Tensors that correspond to this\n        Network's inputs.\n      relevant_nodes: Nodes from the Layers that should be considered part of\n        this Network. If `None`, all Nodes will be considered part of this\n        Network.\n\n    Raises:\n      ValueError: If the layers depend on `Input`s not found in this Model.\n    \"\"\"\n    layers = nest.flatten(layers)\n    tf_utils.assert_no_legacy_layers(layers)\n    node_to_depth = {}\n    for depth, nodes in self._nodes_by_depth.items():\n      node_to_depth.update({node: depth for node in nodes})\n    # The nodes of these Layers that are relevant to this Network. If not\n    # provided, assume all Nodes are relevant\n    if not relevant_nodes:\n      relevant_nodes = nest.flatten([layer._inbound_nodes for layer in layers])\n    network_nodes = set(relevant_nodes + list(node_to_depth.keys()))\n\n    def _get_min_depth(node):\n      \"\"\"Gets the minimum depth at which node can be computed.\"\"\"\n      min_depth = 0\n      for layer, node_id, _, _ in node.iterate_inbound():\n        inbound_node = layer._inbound_nodes[node_id]\n        if inbound_node in node_to_depth:\n          min_depth = min(min_depth, node_to_depth[inbound_node])\n        elif inbound_node not in network_nodes:\n          continue\n        else:\n          # Previous relevant nodes haven't been processed yet.\n          return None\n      # New node is one shallower than its shallowest input.\n      return min_depth - 1\n\n    # Insert nodes into `_nodes_by_depth` and other node attrs.\n    unprocessed_nodes = copy.copy(relevant_nodes)\n    i = 0\n    while unprocessed_nodes:\n      i += 1\n      # Do a sanity check. This can occur if `Input`s from outside this Model\n      # are being relied on.\n      if i > 10000:\n        raise ValueError('Layers could not be added due to missing '\n                         'dependencies.')\n\n      node = unprocessed_nodes.pop(0)\n      depth = _get_min_depth(node)\n      if depth is None:  # Defer until inbound nodes are processed.\n        unprocessed_nodes.append(node)\n        continue\n      node_key = _make_node_key(node.layer.name,\n                                node.layer._inbound_nodes.index(node))\n      if node_key not in self._network_nodes:\n        node_to_depth[node] = depth\n        self._network_nodes.add(node_key)\n        self._nodes_by_depth[depth].append(node)\n\n    # Insert layers and update other layer attrs.\n    layer_set = set(self._self_tracked_trackables)\n    deferred_layers = []\n    for layer in layers:\n      if layer not in layer_set:\n        self._self_tracked_trackables.append(layer)\n        deferred_layers.append(layer)\n        self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)\n        layer_set.add(layer)\n    self._handle_deferred_layer_dependencies(deferred_layers)\n\n    self._compute_tensor_usage_count()\n\n  def _compute_tensor_usage_count(self):\n    \"\"\"Compute the #. of tensor usages for all the output tensors of layers.\n\n    The computed tensor usage count is saved as `self._tensor_usage_count`. This\n    is later used for saving memory in eager computation by releasing\n    no-longer-needed tensors as early as possible.\n    \"\"\"\n    tensor_usage_count = collections.Counter()\n    available_tensors = set(str(id(tensor)) for tensor in self.inputs)\n\n    depth_keys = list(self._nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n    depth_keys = depth_keys[1:]\n\n    for depth in depth_keys:\n      for node in self._nodes_by_depth[depth]:\n        input_tensors = {\n            str(id(tensor)) for tensor in nest.flatten(node.keras_inputs)\n        }\n        if input_tensors.issubset(available_tensors):\n          for tensor in nest.flatten(node.keras_inputs):\n            tensor_usage_count[str(id(tensor))] += 1\n\n          for output_tensor in nest.flatten(node.outputs):\n            available_tensors.add(str(id(output_tensor)))\n\n    for tensor in self.outputs:\n      tensor_usage_count[str(id(tensor))] += 1\n\n    self._tensor_usage_count = tensor_usage_count\n\n  def _assert_weights_created(self):\n    # Override the implementation in Model.\n    # The Functional model should always have weight created already.\n    return\n\n  def _graph_network_add_loss(self, symbolic_loss):\n    new_nodes, new_layers = _map_subgraph_network(self.inputs, [symbolic_loss])\n    # Losses must be keyed on inputs no matter what in order to be supported in\n    # DistributionStrategy.\n    add_loss_layer = base_layer.AddLoss(\n        unconditional=False, dtype=symbolic_loss.dtype)\n    add_loss_layer(symbolic_loss)\n    new_nodes.extend(add_loss_layer.inbound_nodes)\n    new_layers.append(add_loss_layer)\n    self._insert_layers(new_layers, new_nodes)\n\n  def _graph_network_add_metric(self, value, aggregation, name):\n    new_nodes, new_layers = _map_subgraph_network(self.inputs, [value])\n    add_metric_layer = base_layer.AddMetric(\n        aggregation, name, dtype=value.dtype)\n    add_metric_layer(value)\n    new_nodes.extend(add_metric_layer.inbound_nodes)\n    new_layers.append(add_metric_layer)\n    self._insert_layers(new_layers, new_nodes)\n\n  @property\n  def _trackable_saved_model_saver(self):\n    return network_serialization.NetworkSavedModelSaver(self)\n\n  def _get_save_spec(self, dynamic_batch=True):\n    if getattr(self, '_has_explicit_input_shape', True):\n      # Functional models and Sequential models that have an explicit input\n      # shape should use the batch size set by the input layer.\n      dynamic_batch = False\n    return super(Functional, self)._get_save_spec(dynamic_batch)\n\n\ndef _make_node_key(layer_name, node_index):\n  return layer_name + '_ib-' + str(node_index)\n\n\ndef _map_graph_network(inputs, outputs):\n  \"\"\"Validates a network's topology and gather its layers and nodes.\n\n  Args:\n    inputs: List of input tensors.\n    outputs: List of outputs tensors.\n\n  Returns:\n    A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.\n    - nodes: list of Node instances.\n    - nodes_by_depth: dict mapping ints (depth) to lists of node instances.\n    - layers: list of Layer instances.\n    - layers_by_depth: dict mapping ints (depth) to lists of layer instances.\n\n  Raises:\n    ValueError: In case the network is not valid (e.g. disconnected graph).\n  \"\"\"\n  # \"depth\" is number of layers between output Node and the Node.\n  # Nodes are ordered from inputs -> outputs.\n  nodes_in_decreasing_depth, layer_indices = _build_map(outputs)\n  network_nodes = {\n      _make_node_key(node.layer.name, node.layer._inbound_nodes.index(node))\n      for node in nodes_in_decreasing_depth\n  }\n\n  nodes_depths = {}  # dict {node: depth value}\n  layers_depths = {}  # dict {layer: depth value}\n\n  for node in reversed(nodes_in_decreasing_depth):\n    # If the depth is not set, the node has no outbound nodes (depth 0).\n    depth = nodes_depths.setdefault(node, 0)\n\n    # Update the depth of the corresponding layer\n    previous_depth = layers_depths.get(node.layer, 0)\n    # If we've seen this layer before at a higher depth,\n    # we should use that depth instead of the node depth.\n    # This is necessary for shared layers that have inputs at different\n    # depth levels in the graph.\n    depth = max(depth, previous_depth)\n    layers_depths[node.layer] = depth\n    nodes_depths[node] = depth\n\n    # Update the depth of inbound nodes.\n    # The \"depth\" of a node is the max of the depths\n    # of all nodes it is connected to + 1.\n    for node_dep in node.parent_nodes:\n      previous_depth = nodes_depths.get(node_dep, 0)\n      nodes_depths[node_dep] = max(depth + 1, previous_depth)\n\n  # Handle inputs that are not connected to outputs.\n  # We do not error out here because the inputs may be used to compute losses\n  # and metrics.\n  for input_t in inputs:\n    input_layer = input_t._keras_history[0]\n    if input_layer not in layers_depths:\n      layers_depths[input_layer] = 0\n      layer_indices[input_layer] = -1\n      nodes_depths[input_layer._inbound_nodes[0]] = 0\n      network_nodes.add(_make_node_key(input_layer.name, 0))\n\n  # Build a dict {depth: list of nodes with this depth}\n  nodes_by_depth = collections.defaultdict(list)\n  for node, depth in nodes_depths.items():\n    nodes_by_depth[depth].append(node)\n\n  # Build a dict {depth: list of layers with this depth}\n  layers_by_depth = collections.defaultdict(list)\n  for layer, depth in layers_depths.items():\n    layers_by_depth[depth].append(layer)\n\n  # Get sorted list of layer depths.\n  depth_keys = list(layers_by_depth.keys())\n  depth_keys.sort(reverse=True)\n\n  # Set self.layers ordered by depth.\n  layers = []\n  for depth in depth_keys:\n    layers_for_depth = layers_by_depth[depth]\n    # Network.layers needs to have a deterministic order:\n    # here we order them by traversal order.\n    layers_for_depth.sort(key=lambda x: layer_indices[x])\n    layers.extend(layers_for_depth)\n\n  # Get sorted list of node depths.\n  depth_keys = list(nodes_by_depth.keys())\n  depth_keys.sort(reverse=True)\n\n  # Check that all tensors required are computable.\n  # computable_tensors: all tensors in the graph\n  # that can be computed from the inputs provided.\n  computable_tensors = set()\n  for x in inputs:\n    computable_tensors.add(id(x))\n\n  layers_with_complete_input = []  # To provide a better error msg.\n  for depth in depth_keys:\n    for node in nodes_by_depth[depth]:\n      layer = node.layer\n      if layer and not node.is_input:\n        for x in nest.flatten(node.keras_inputs):\n          if id(x) not in computable_tensors:\n            raise ValueError('Graph disconnected: '\n                             'cannot obtain value for tensor ' + str(x) +\n                             ' at layer \"' + layer.name + '\". '\n                             'The following previous layers '\n                             'were accessed without issue: ' +\n                             str(layers_with_complete_input))\n        for x in nest.flatten(node.outputs):\n          computable_tensors.add(id(x))\n        layers_with_complete_input.append(layer.name)\n\n  # Ensure name unicity, which will be crucial for serialization\n  # (since serialized nodes refer to layers by their name).\n  all_names = [layer.name for layer in layers]\n  for name in all_names:\n    if all_names.count(name) != 1:\n      raise ValueError('The name \"' + name + '\" is used ' +\n                       str(all_names.count(name)) + ' times in the model. '\n                       'All layer names should be unique.')\n  return network_nodes, nodes_by_depth, layers, layers_by_depth\n\n\ndef _build_map(outputs):\n  \"\"\"This method topologically sorts nodes in order from inputs to outputs.\n\n  It uses a depth-first search to topologically sort nodes that appear in the\n  _keras_history connectivity metadata of `outputs`.\n\n  Args:\n    outputs: the output tensors whose _keras_history metadata should be walked.\n    This may be an arbitrary nested structure.\n\n  Returns:\n    A tuple like (ordered_nodes, layer_to_first_traversal_index)\n    ordered_nodes: list of nodes appearing in the keras history, topologically\n      sorted from original inputs to the `outputs`.\n      (If outputs have different sets of ancestors, the inputs to one output\n      may appear after a different output).\n    layer_to_first_traversal_index:\n      A dict mapping layer to the traversal index in the DFS where it is\n      seen. Note: if a layer is shared by several nodes, the dict will only\n      store the index corresponding to the *first* time the layer seen.\n  \"\"\"\n  finished_nodes = set()\n  nodes_in_progress = set()\n  nodes_in_decreasing_depth = []  # nodes from inputs -> outputs.\n  layer_indices = {}  # layer -> in traversal order.\n  for output in nest.flatten(outputs):\n    _build_map_helper(output, finished_nodes, nodes_in_progress,\n                      nodes_in_decreasing_depth, layer_indices)\n  return nodes_in_decreasing_depth, layer_indices\n\n\ndef _build_map_helper(tensor, finished_nodes, nodes_in_progress,\n                      nodes_in_decreasing_depth, layer_indices):\n  \"\"\"Recursive helper for `_build_map`.\"\"\"\n  layer, node_index, _ = tensor._keras_history  # pylint: disable=protected-access\n  node = layer._inbound_nodes[node_index]  # pylint: disable=protected-access\n\n  # Don't repeat work for shared subgraphs\n  if node in finished_nodes:\n    return\n\n  # Prevent cycles.\n  if node in nodes_in_progress:\n    raise ValueError('The tensor ' + str(tensor) + ' at layer \"' + layer.name +\n                     '\" is part of a cycle.')\n\n  # Store the traversal order for layer sorting.\n  if layer not in layer_indices:\n    layer_indices[layer] = len(layer_indices)\n\n  # Propagate to all previous tensors connected to this node.\n  nodes_in_progress.add(node)\n  if not node.is_input:\n    for tensor in node.keras_inputs:\n      _build_map_helper(tensor, finished_nodes, nodes_in_progress,\n                        nodes_in_decreasing_depth, layer_indices)\n\n  finished_nodes.add(node)\n  nodes_in_progress.remove(node)\n  nodes_in_decreasing_depth.append(node)\n\n\ndef _map_subgraph_network(inputs, outputs):\n  \"\"\"Returns the nodes and layers in the topology from `inputs` to `outputs`.\n\n  Args:\n    inputs: List of input tensors.\n    outputs: List of output tensors.\n\n  Returns:\n    A tuple of List{Node] and List[Layer].\n  \"\"\"\n  if not ops.executing_eagerly_outside_functions():\n    base_layer_utils.create_keras_history(outputs)\n  # Keep only nodes and layers in the topology between inputs and outputs.\n  _, nodes_by_depth, layers, _ = _map_graph_network(inputs, outputs)\n  return nest.flatten([nodes for nodes in nodes_by_depth.values()]), layers\n\n\ndef _should_skip_first_node(layer):\n  \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"\n  # Networks that are constructed with an Input layer/shape start with a\n  # pre-existing node linking their input to output. This node is excluded from\n  # the network config.\n  if layer._self_tracked_trackables:\n    return (isinstance(layer, Functional) and\n            # Filter out Sequential models without an input shape.\n            isinstance(layer._self_tracked_trackables[0],\n                       input_layer_module.InputLayer))\n  else:\n    return isinstance(layer, Functional)\n\n\ndef connect_ancillary_layers(model, created_layers):\n  \"\"\"Adds layers that are not connected to the outputs to the model.\"\"\"\n  # Layers not connected to outputs, such as those added in `add_loss`.\n  ancillary_layers = [\n      layer for layer in created_layers.values() if layer not in model.layers\n  ]\n  if ancillary_layers:\n    relevant_nodes = nest.flatten([\n        layer.inbound_nodes[1:]\n        if _should_skip_first_node(layer) else layer.inbound_nodes\n        for layer in created_layers.values()\n    ])\n    model._insert_layers(ancillary_layers, relevant_nodes)\n  return model\n\n\ndef reconstruct_from_config(config, custom_objects=None, created_layers=None):\n  \"\"\"Reconstructs graph from config object.\n\n  Args:\n    config: Dictionary returned from Network.get_config()\n    custom_objects: Optional dictionary mapping names (strings) to custom\n      classes or functions to be considered during deserialization.\n    created_layers: Optional dictionary mapping names to Layer objects. Any\n      layer not in this dictionary will be created and added to the dict.\n      This function will add new nodes to all layers (excluding InputLayers),\n      instead of re-using pre-existing nodes in the layers.\n\n  Returns:\n    Tuple of (input tensors, output tensors, dictionary of created layers)\n  \"\"\"\n  # Layer instances created during the graph reconstruction process.\n  created_layers = created_layers or collections.OrderedDict()\n\n  # Maps input data (tuple of inbound layer name, node index) from the config\n  # to node indices in the newly generated model. The node indices may be\n  # different if the layers have already been called previously.\n  node_index_map = {}\n  node_count_by_layer = {}\n\n  # Dictionary mapping layer instances to\n  # node data that specifies a layer call.\n  # It acts as a queue that maintains any unprocessed\n  # layer call until it becomes possible to process it\n  # (i.e. until the input tensors to the call all exist).\n  unprocessed_nodes = {}\n\n  def add_unprocessed_node(layer, node_data):\n    if layer not in unprocessed_nodes:\n      unprocessed_nodes[layer] = [node_data]\n    else:\n      unprocessed_nodes[layer].append(node_data)\n\n  def get_node_index(layer, config_node_index):\n    \"\"\"Returns node index in layer (might differ from config_node_index).\"\"\"\n    if isinstance(layer, input_layer_module.InputLayer):\n      return 0\n    return node_index_map.get((layer.name, config_node_index), None)\n\n  def _deserialize_keras_tensors(kwargs, layer_map):\n    \"\"\"Deserializes Keras Tensors passed to `call`..\"\"\"\n\n    def _deserialize_keras_tensor(t):\n      \"\"\"Deserializes a single Keras Tensor passed to `call`.\"\"\"\n      if isinstance(t, tf_utils.ListWrapper):\n        t = t.as_list()\n        layer_name = t[0]\n        node_index = t[1]\n        tensor_index = t[2]\n\n        layer = layer_map[layer_name]\n        new_node_index = get_node_index(layer, node_index)\n        if new_node_index is None:\n          # The inbound node may not have been processed yet,\n          # (This can happen e.g. if it depends on a different set\n          # of inputs than those that have been processed already).\n          # raise an IndexError so that the current node puts itself\n          # back on the unprocessed queue.\n          # Caution: This may lead to infinite loops for malformed\n          # network configurations! (or when there is a bug in\n          # the network config loading code).\n          raise IndexError\n        node = layer._inbound_nodes[new_node_index]\n        return nest.flatten(node.outputs)[tensor_index]\n      return t\n\n    kwargs = tf_utils.convert_inner_node_data(kwargs, wrap=True)\n    return nest.map_structure(_deserialize_keras_tensor, kwargs)\n\n  def process_node(layer, node_data):\n    \"\"\"Deserialize a node.\n\n    Args:\n        layer: layer instance.\n        node_data: Nested structure of `ListWrapper`.\n\n    Raises:\n        ValueError: In case of improperly formatted `node_data`.\n    \"\"\"\n    input_tensors = []\n    for input_data in nest.flatten(node_data):\n      input_data = input_data.as_list()\n      inbound_layer_name = input_data[0]\n      inbound_node_index = input_data[1]\n      inbound_tensor_index = input_data[2]\n      if len(input_data) == 3:\n        kwargs = {}\n      elif len(input_data) == 4:\n        kwargs = input_data[3]\n        try:\n          kwargs = _deserialize_keras_tensors(kwargs, created_layers)\n        except IndexError:\n          # Happens if keras tensors in kwargs are still unprocessed\n          add_unprocessed_node(layer, node_data)\n          return\n      else:\n        raise ValueError('Improperly formatted model config.')\n\n      if inbound_layer_name != node_module._CONSTANT_VALUE:\n        inbound_layer = created_layers[inbound_layer_name]\n        inbound_node_index = get_node_index(inbound_layer, inbound_node_index)\n\n        if inbound_node_index is None:\n          add_unprocessed_node(layer, node_data)\n          return\n        inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n        input_tensors.append(\n            nest.flatten(inbound_node.outputs)[inbound_tensor_index])\n      else:\n        # We received a constant w/ no Keras history attached\n        input_tensors.append(inbound_tensor_index)\n    input_tensors = nest.pack_sequence_as(node_data, input_tensors)\n    # Call layer on its inputs, thus creating the node\n    # and building the layer if needed.\n    if input_tensors is not None:\n      if not layer._preserve_input_structure_in_config:\n        input_tensors = (\n            base_layer_utils.unnest_if_single_tensor(input_tensors))\n      output_tensors = layer(input_tensors, **kwargs)\n\n      # Update node index map.\n      output_index = nest.flatten(output_tensors)[0]._keras_history.node_index\n      node_index_map[(layer.name, node_count_by_layer[layer])] = output_index\n      node_count_by_layer[layer] += 1\n\n  def process_layer(layer_data):\n    \"\"\"Deserializes a layer, then call it on appropriate inputs.\n\n    Args:\n        layer_data: layer config dict.\n\n    Raises:\n        ValueError: In case of improperly formatted `layer_data` dict.\n    \"\"\"\n    layer_name = layer_data['name']\n\n    if layer_name in created_layers:\n      layer = created_layers[layer_name]\n    else:\n      # Instantiate layer.\n      from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\n\n      layer = deserialize_layer(layer_data, custom_objects=custom_objects)\n      created_layers[layer_name] = layer\n\n    node_count_by_layer[layer] = int(_should_skip_first_node(layer))\n\n    # Gather layer inputs and convert to `ListWrapper` objects.\n    inbound_nodes_data = layer_data['inbound_nodes']\n    inbound_nodes_data = tf_utils.convert_inner_node_data(\n        inbound_nodes_data, wrap=True)\n    for node_data in inbound_nodes_data:\n      # We don't process nodes (i.e. make layer calls)\n      # on the fly because the inbound node may not yet exist,\n      # in case of layer shared at different topological depths\n      # (e.g. a model such as A(B(A(B(x)))))\n      add_unprocessed_node(layer, node_data)\n\n  # First, we create all layers and enqueue nodes to be processed\n  for layer_data in config['layers']:\n    process_layer(layer_data)\n  # Then we process nodes in order of layer depth.\n  # Nodes that cannot yet be processed (if the inbound node\n  # does not yet exist) are re-enqueued, and the process\n  # is repeated until all nodes are processed.\n  while unprocessed_nodes:\n    for layer_data in config['layers']:\n      layer = created_layers[layer_data['name']]\n      if layer in unprocessed_nodes:\n        for node_data in unprocessed_nodes.pop(layer):\n          process_node(layer, node_data)\n\n  input_tensors = []\n  output_tensors = []\n\n  input_layers = tf_utils.convert_inner_node_data(\n      config['input_layers'], wrap=True)\n  for layer_data in nest.flatten(input_layers):\n    layer_name, node_index, tensor_index = layer_data.as_list()\n    assert layer_name in created_layers\n    layer = created_layers[layer_name]\n    node_index = get_node_index(layer, node_index)\n    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n    input_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])\n\n  output_layers = tf_utils.convert_inner_node_data(\n      config['output_layers'], wrap=True)\n  for layer_data in nest.flatten(output_layers):\n    layer_name, node_index, tensor_index = layer_data.as_list()\n    assert layer_name in created_layers\n    layer = created_layers[layer_name]\n    node_index = get_node_index(layer, node_index)\n    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n    output_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])\n\n  input_tensors = nest.pack_sequence_as(input_layers, input_tensors)\n  output_tensors = nest.pack_sequence_as(output_layers, output_tensors)\n  return input_tensors, output_tensors, created_layers\n\n\ndef get_network_config(network, serialize_layer_fn=None):\n  \"\"\"Builds the config, which consists of the node graph and serialized layers.\n\n  Args:\n    network: A Network object.\n    serialize_layer_fn: Function used to serialize layers.\n\n  Returns:\n    Config dictionary.\n  \"\"\"\n  serialize_layer_fn = (\n      serialize_layer_fn or generic_utils.serialize_keras_object)\n  config = {\n      'name': network.name,\n  }\n  node_conversion_map = {}\n  for layer in network.layers:\n    kept_nodes = 1 if _should_skip_first_node(layer) else 0\n    for original_node_index, node in enumerate(layer._inbound_nodes):\n      node_key = _make_node_key(layer.name, original_node_index)\n      if node_key in network._network_nodes:\n        node_conversion_map[node_key] = kept_nodes\n        kept_nodes += 1\n  layer_configs = []\n\n  with generic_utils.SharedObjectSavingScope():\n    for layer in network.layers:  # From the earliest layers on.\n      filtered_inbound_nodes = []\n      for original_node_index, node in enumerate(layer._inbound_nodes):\n        node_key = _make_node_key(layer.name, original_node_index)\n        if node_key in network._network_nodes and not node.is_input:\n          # The node is relevant to the model:\n          # add to filtered_inbound_nodes.\n          node_data = node.serialize(_make_node_key, node_conversion_map)\n          filtered_inbound_nodes.append(node_data)\n\n      layer_config = serialize_layer_fn(layer)\n      layer_config['name'] = layer.name\n      layer_config['inbound_nodes'] = filtered_inbound_nodes\n      layer_configs.append(layer_config)\n    config['layers'] = layer_configs\n\n  # Gather info about inputs and outputs.\n  model_inputs = []\n  for i in range(len(network._input_layers)):\n    layer, node_index, tensor_index = network._input_coordinates[i]\n    node_key = _make_node_key(layer.name, node_index)\n    if node_key not in network._network_nodes:\n      continue\n    new_node_index = node_conversion_map[node_key]\n    model_inputs.append(\n        tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))\n  model_inputs = nest.pack_sequence_as(network._nested_inputs, model_inputs)\n  # Preserve external Keras compat for Models with single input.\n  if not nest.is_nested(model_inputs):\n    model_inputs = [model_inputs]\n  model_inputs = tf_utils.convert_inner_node_data(model_inputs)\n  config['input_layers'] = model_inputs\n\n  model_outputs = []\n  for i in range(len(network._output_layers)):\n    layer, node_index, tensor_index = network._output_coordinates[i]\n    node_key = _make_node_key(layer.name, node_index)\n    if node_key not in network._network_nodes:\n      continue\n    new_node_index = node_conversion_map[node_key]\n    model_outputs.append(\n        tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))\n  model_outputs = nest.pack_sequence_as(network._nested_outputs, model_outputs)\n  # Preserve external Keras compat for Models with single output.\n  if not nest.is_nested(model_outputs):\n    model_outputs = [model_outputs]\n  model_outputs = tf_utils.convert_inner_node_data(model_outputs)\n  config['output_layers'] = model_outputs\n  return config\n\n\ndef shape_with_no_batch_size(x):\n  if x.shape.rank is None:\n    return None\n  shape = x.shape.as_list()\n  if shape:\n    shape[0] = None\n  return shape\n\n\nclass ModuleWrapper(base_layer.Layer):\n  \"\"\"Wrapper for `tf.Module`s to support the Functional and Sequential API.\"\"\"\n\n  def __init__(self, module, method_name=None, **kwargs):\n    \"\"\"Initializes the wrapper Layer for this module.\n\n    Args:\n      module: The `tf.Module` instance to be wrapped.\n      method_name: (Optional) str. The name of the method to use as the forward\n        pass of the module. If not set, defaults to '__call__' if defined, or\n        'call'.\n      **kwargs: Additional keywrod arguments. See `tf.keras.layers.Layer`.\n\n    Raises:\n      ValueError: If `method` is not defined on `module`.\n    \"\"\"\n    super(ModuleWrapper, self).__init__(**kwargs)\n    if method_name is None:\n      if hasattr(module, '__call__'):\n        method_name = '__call__'\n      elif hasattr(module, 'call'):\n        method_name = 'call'\n    if method_name is None or not hasattr(module, method_name):\n      raise ValueError('{} is not defined on object {}'.format(\n          method_name, module))\n\n    self._module = module\n    self._method_name = method_name\n\n    # Check if module.__call__ has a `training` arg or accepts `**kwargs`.\n    method = getattr(module, method_name)\n    method_arg_spec = tf_inspect.getfullargspec(method)\n    self._expects_training_arg = ('training' in method_arg_spec.args or\n                                  method_arg_spec.varkw is not None)\n    self._expects_mask_arg = ('mask' in method_arg_spec.args or\n                              method_arg_spec.varkw is not None)\n\n  def call(self, *args, **kwargs):\n    if 'training' in kwargs and not self._expects_training_arg:\n      kwargs.pop('training')\n    if 'mask' in kwargs and not self._expects_mask_arg:\n      kwargs.pop('mask')\n    return getattr(self._module, self._method_name)(*args, **kwargs)\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#,============================================================================\n\"\"\"Tests for layer graphs construction & handling.\"\"\"\n\nimport warnings\n\nimport numpy as np\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras import combinations\nfrom tensorflow.python.keras import initializers\nfrom tensorflow.python.keras import keras_parameterized\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras import testing_utils\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.keras.engine import functional\nfrom tensorflow.python.keras.engine import input_layer as input_layer_lib\nfrom tensorflow.python.keras.engine import sequential\nfrom tensorflow.python.keras.engine import training as training_lib\nfrom tensorflow.python.keras.utils import layer_utils\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training.tracking.util import Checkpoint\n\ntry:\n  import yaml  # pylint:disable=g-import-not-at-top\nexcept ImportError:\n  yaml = None\n\n\nclass NetworkConstructionTest(keras_parameterized.TestCase):\n\n  def test_default_model_name(self):\n    inputs = input_layer_lib.Input(shape=(1,))\n    outputs = layers.Dense(1, activation='relu')(inputs)\n    model = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model.name, 'model')\n\n    model_2 = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model_2.name, 'model_1')\n\n    model_3 = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model_3.name, 'model_2')\n\n  def test_get_updates(self):\n\n    class MyLayer(layers.Layer):\n\n      def build(self, input_shape):\n        self.a = self.add_variable('a',\n                                   (1, 1),\n                                   'float32',\n                                   trainable=False)\n        self.b = self.add_variable('b',\n                                   (1, 1),\n                                   'float32',\n                                   trainable=False)\n        self.add_update(state_ops.assign_add(self.a, [[1.]],\n                                             name='unconditional_update'))\n        self.built = True\n\n      def call(self, inputs):\n        self.add_update(state_ops.assign_add(self.b, inputs,\n                                             name='conditional_update'),\n                        inputs=True)\n        return inputs + 1\n\n    with ops.Graph().as_default():\n      x1 = input_layer_lib.Input(shape=(1,))\n      layer = MyLayer()\n      _ = layer(x1)\n\n      self.assertEqual(len(layer.updates), 2)\n\n      x2 = input_layer_lib.Input(shape=(1,))\n      y2 = layer(x2)\n\n      self.assertEqual(len(layer.updates), 3)\n\n      network = functional.Functional(x2, y2)\n      self.assertEqual(len(network.updates), 3)\n\n      x3 = input_layer_lib.Input(shape=(1,))\n      _ = layer(x3)\n      self.assertEqual(len(network.updates), 4)\n\n      x4 = input_layer_lib.Input(shape=(1,))\n      _ = network(x4)\n      self.assertEqual(len(network.updates), 5)\n\n      network.add_update(state_ops.assign_add(layer.a, [[1]]))\n      self.assertEqual(len(network.updates), 6)\n\n      network.add_update(state_ops.assign_add(layer.b, x4), inputs=True)\n      self.assertEqual(len(network.updates), 7)\n\n  @combinations.generate(combinations.combine(mode=['graph']))\n  def test_get_updates_bn(self):\n    x1 = input_layer_lib.Input(shape=(1,))\n    layer = layers.BatchNormalization()\n    _ = layer(x1)\n\n    self.assertEqual(len(layer.updates), 2)\n\n  def test_get_layer(self):\n    # create a simple network\n    x = input_layer_lib.Input(shape=(32,))\n    dense_a = layers.Dense(4, name='dense_a')\n    dense_b = layers.Dense(2, name='dense_b')\n    y = dense_b(dense_a(x))\n    network = functional.Functional(x, y, name='dense_network')\n\n    # test various get_layer by index\n    self.assertEqual(network.get_layer(index=1), dense_a)\n\n    # test invalid get_layer by index\n    with self.assertRaisesRegex(\n        ValueError, 'Was asked to retrieve layer at index ' + str(3) +\n        ' but model only has ' + str(len(network.layers)) + ' layers.'):\n      network.get_layer(index=3)\n\n    # test that only one between name and index is requested\n    with self.assertRaisesRegex(ValueError,\n                                'Provide only a layer name or a layer index'):\n      network.get_layer(index=1, name='dense_b')\n\n    # test that a name or an index must be provided\n    with self.assertRaisesRegex(ValueError,\n                                'Provide either a layer name or layer index.'):\n      network.get_layer()\n\n    # test various get_layer by name\n    self.assertEqual(network.get_layer(name='dense_a'), dense_a)\n\n    # test invalid get_layer by name\n    with self.assertRaisesRegex(ValueError, 'No such layer: dense_c.'):\n      network.get_layer(name='dense_c')\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributes(self):\n    # test layer attributes / methods related to cross-layer connectivity.\n    a = input_layer_lib.Input(shape=(32,), name='input_a')\n    b = input_layer_lib.Input(shape=(32,), name='input_b')\n\n    # test input, output, input_shape, output_shape\n    test_layer = layers.Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    self.assertIs(test_layer.input, a)\n    self.assertIs(test_layer.output, a_test)\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, (None, 16))\n\n    # test `get_*_at` methods\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    self.assertIs(dense.get_input_at(0), a)\n    self.assertIs(dense.get_input_at(1), b)\n    self.assertIs(dense.get_output_at(0), a_2)\n    self.assertIs(dense.get_output_at(1), b_2)\n    self.assertEqual(dense.get_input_shape_at(0), (None, 32))\n    self.assertEqual(dense.get_input_shape_at(1), (None, 32))\n    self.assertEqual(dense.get_output_shape_at(0), (None, 16))\n    self.assertEqual(dense.get_output_shape_at(1), (None, 16))\n\n    # Test invalid value for attribute retrieval.\n    with self.assertRaises(ValueError):\n      dense.get_input_at(2)\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.input\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.output\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.output_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.input_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      a = input_layer_lib.Input(shape=(3, 32))\n      a = input_layer_lib.Input(shape=(5, 32))\n      a_2 = dense(a)\n      b_2 = dense(b)\n      _ = new_dense.input_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      a = input_layer_lib.Input(shape=(3, 32))\n      a = input_layer_lib.Input(shape=(5, 32))\n      a_2 = dense(a)\n      b_2 = dense(b)\n      _ = new_dense.output_shape\n\n  def _assertAllIs(self, a, b):\n    self.assertTrue(all(x is y for x, y in zip(a, b)))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributesMultiOutputLayer(self):\n\n    class PowersLayer(layers.Layer):\n\n      def call(self, inputs):\n        return [inputs**2, inputs**3]\n\n    x = input_layer_lib.Input(shape=(32,))\n    test_layer = PowersLayer()\n    p1, p2 = test_layer(x)  # pylint: disable=not-callable\n\n    self.assertIs(test_layer.input, x)\n    self._assertAllIs(test_layer.output, [p1, p2])\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, [(None, 32), (None, 32)])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributesMultiInputLayer(self):\n\n    class AddLayer(layers.Layer):\n\n      def call(self, inputs):\n        assert len(inputs) == 2\n        return inputs[0] + inputs[1]\n\n    a = input_layer_lib.Input(shape=(32,))\n    b = input_layer_lib.Input(shape=(32,))\n    test_layer = AddLayer()\n    y = test_layer([a, b])  # pylint: disable=not-callable\n\n    self._assertAllIs(test_layer.input, [a, b])\n    self.assertIs(test_layer.output, y)\n    self.assertEqual(test_layer.input_shape, [(None, 32), (None, 32)])\n    self.assertEqual(test_layer.output_shape, (None, 32))\n\n  def testBasicNetwork(self):\n    with ops.Graph().as_default():\n      # minimum viable network\n      x = input_layer_lib.Input(shape=(32,))\n      dense = layers.Dense(2)\n      y = dense(x)\n      network = functional.Functional(x, y, name='dense_network')\n\n      # test basic attributes\n      self.assertEqual(network.name, 'dense_network')\n      self.assertEqual(len(network.layers), 2)  # InputLayer + Dense\n      self.assertEqual(network.layers[1], dense)\n      self._assertAllIs(network.weights, dense.weights)\n      self._assertAllIs(network.trainable_weights, dense.trainable_weights)\n      self._assertAllIs(network.non_trainable_weights,\n                        dense.non_trainable_weights)\n\n      # test callability on Input\n      x_2 = input_layer_lib.Input(shape=(32,))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 2])\n\n      # test callability on regular tensor\n      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 2])\n\n      # test network `trainable` attribute\n      network.trainable = False\n      self._assertAllIs(network.weights, dense.weights)\n      self.assertEqual(network.trainable_weights, [])\n      self._assertAllIs(network.non_trainable_weights,\n                        dense.trainable_weights + dense.non_trainable_weights)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_trainable_weights(self):\n    a = layers.Input(shape=(2,))\n    b = layers.Dense(1)(a)\n    model = training_lib.Model(a, b)\n\n    weights = model.weights\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    model.trainable = True\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.layers[1].trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    # sequential model\n    model = sequential.Sequential()\n    model.add(layers.Dense(1, input_dim=2))\n    weights = model.weights\n\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    model.trainable = True\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.layers[0].trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n  def test_layer_call_arguments(self):\n    with ops.Graph().as_default():\n      # Test the ability to pass and serialize arguments to `call`.\n      inp = layers.Input(shape=(2,))\n      x = layers.Dense(3)(inp)\n      x = layers.Dropout(0.5)(x, training=True)\n      model = training_lib.Model(inp, x)\n      # Would be `dropout/cond/Merge` by default\n      self.assertIn('dropout', model.output.op.name)\n\n      # Test that argument is kept when applying the model\n      inp2 = layers.Input(shape=(2,))\n      out2 = model(inp2)\n      self.assertIn('dropout', out2.op.name)\n\n      # Test that argument is kept after loading a model\n      config = model.get_config()\n      model = training_lib.Model.from_config(config)\n      self.assertIn('dropout', model.output.op.name)\n\n  def test_node_construction(self):\n    # test basics\n    a = layers.Input(shape=(32,), name='input_a')\n    b = layers.Input(shape=(32,), name='input_b')\n\n    with self.assertRaises(ValueError):\n      _ = layers.Input(shape=(32,), batch_shape=(10, 32))\n    with self.assertRaises(ValueError):\n      _ = layers.Input(shape=(32,), unknown_kwarg=None)\n\n    self.assertListEqual(a.shape.as_list(), [None, 32])\n    a_layer, a_node_index, a_tensor_index = a._keras_history\n    b_layer, _, _ = b._keras_history\n    self.assertEqual(len(a_layer._inbound_nodes), 1)\n    self.assertEqual(a_tensor_index, 0)\n    node = a_layer._inbound_nodes[a_node_index]\n    self.assertEqual(node.outbound_layer, a_layer)\n\n    self.assertListEqual(node.inbound_layers, [])\n    self.assertListEqual(node.input_tensors, [a])\n    self.assertListEqual(node.input_shapes, [(None, 32)])\n    self.assertListEqual(node.output_tensors, [a])\n    self.assertListEqual(node.output_shapes, [(None, 32)])\n\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    self.assertEqual(len(dense._inbound_nodes), 2)\n    self.assertEqual(len(dense._outbound_nodes), 0)\n    self.assertEqual(dense._inbound_nodes[0].inbound_layers, a_layer)\n    self.assertEqual(dense._inbound_nodes[0].outbound_layer, dense)\n    self.assertEqual(dense._inbound_nodes[1].inbound_layers, b_layer)\n    self.assertEqual(dense._inbound_nodes[1].outbound_layer, dense)\n    self.assertIs(dense._inbound_nodes[0].input_tensors, a)\n    self.assertIs(dense._inbound_nodes[1].input_tensors, b)\n\n    # test layer properties\n    test_layer = layers.Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    self.assertListEqual(test_layer.kernel.shape.as_list(), [32, 16])\n    self.assertIs(test_layer.input, a)\n    self.assertIs(test_layer.output, a_test)\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, (None, 16))\n\n    self.assertIs(dense.get_input_at(0), a)\n    self.assertIs(dense.get_input_at(1), b)\n    self.assertIs(dense.get_output_at(0), a_2)\n    self.assertIs(dense.get_output_at(1), b_2)\n    self.assertEqual(dense.get_input_shape_at(0), (None, 32))\n    self.assertEqual(dense.get_input_shape_at(1), (None, 32))\n    self.assertEqual(dense.get_output_shape_at(0), (None, 16))\n    self.assertEqual(dense.get_output_shape_at(1), (None, 16))\n    self.assertEqual(dense.get_input_mask_at(0), None)\n    self.assertEqual(dense.get_input_mask_at(1), None)\n    self.assertEqual(dense.get_output_mask_at(0), None)\n    self.assertEqual(dense.get_output_mask_at(1), None)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_multi_input_layer(self):\n    with self.cached_session():\n      # test multi-input layer\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      self.assertListEqual(merged.shape.as_list(), [None, 16 * 2])\n      merge_layer, merge_node_index, merge_tensor_index = merged._keras_history\n\n      self.assertEqual(merge_node_index, 0)\n      self.assertEqual(merge_tensor_index, 0)\n\n      self.assertEqual(len(merge_layer._inbound_nodes), 1)\n      self.assertEqual(len(merge_layer._outbound_nodes), 0)\n\n      self.assertEqual(len(merge_layer._inbound_nodes[0].input_tensors), 2)\n      self.assertEqual(len(merge_layer._inbound_nodes[0].inbound_layers), 2)\n\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n      self.assertEqual(len(model.layers), 6)\n      output_shapes = model.compute_output_shape([(None, 32), (None, 32)])\n      self.assertListEqual(output_shapes[0].as_list(), [None, 64])\n      self.assertListEqual(output_shapes[1].as_list(), [None, 5])\n      self.assertListEqual(\n          model.compute_mask([a, b], [None, None]), [None, None])\n\n      # we don't check names of first 2 layers (inputs) because\n      # ordering of same-level layers is not fixed\n      self.assertListEqual([l.name for l in model.layers][2:],\n                           ['dense_1', 'merge', 'dense_2', 'dense_3'])\n      self.assertListEqual([l.name for l in model._input_layers],\n                           ['input_a', 'input_b'])\n      self.assertListEqual([l.name for l in model._output_layers],\n                           ['dense_2', 'dense_3'])\n\n      # actually run model\n      fn = backend.function(model.inputs, model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])\n\n      # test get_source_inputs\n      self._assertAllIs(layer_utils.get_source_inputs(c), [a, b])\n\n      # serialization / deserialization\n      json_config = model.to_json()\n      recreated_model = models.model_from_json(json_config)\n      recreated_model.compile('rmsprop', 'mse')\n\n      self.assertListEqual([l.name for l in recreated_model.layers][2:],\n                           ['dense_1', 'merge', 'dense_2', 'dense_3'])\n      self.assertListEqual([l.name for l in recreated_model._input_layers],\n                           ['input_a', 'input_b'])\n      self.assertListEqual([l.name for l in recreated_model._output_layers],\n                           ['dense_2', 'dense_3'])\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])\n\n  def test_multi_output_layer_output_names(self):\n    inp = layers.Input(name='inp', shape=(None,), dtype=dtypes.float32)\n\n    class _MultiOutput(layers.Layer):\n\n      def call(self, x):\n        return x + 1., x + 2.\n\n    out = _MultiOutput(name='out')(inp)\n    model = training_lib.Model(inp, out)\n    self.assertEqual(['out', 'out_1'], model.output_names)\n    self.assertAllClose([2., 3.], model(1.))\n\n  def test_recursion(self):\n    with ops.Graph().as_default(), self.cached_session():\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      e = layers.Input(shape=(32,), name='input_e')\n      f = layers.Input(shape=(32,), name='input_f')\n      self.assertEqual(len(model.inputs), 2)\n      g, h = model([e, f])\n      self.assertEqual(len(model.inputs), 2)\n      self.assertEqual(g.name, 'model/dense_2/BiasAdd:0')\n\n      self.assertListEqual(g.shape.as_list(), c.shape.as_list())\n      self.assertListEqual(h.shape.as_list(), d.shape.as_list())\n\n      # test separate manipulation of different layer outputs\n      i = layers.Dense(7, name='dense_4')(h)\n\n      final_model = training_lib.Model(\n          inputs=[e, f], outputs=[i, g], name='final')\n      self.assertEqual(len(final_model.inputs), 2)\n      self.assertEqual(len(final_model.outputs), 2)\n      self.assertEqual(len(final_model.layers), 4)\n\n      # we don't check names of first 2 layers (inputs) because\n      # ordering of same-level layers is not fixed\n      self.assertListEqual([layer.name for layer in final_model.layers][2:],\n                           ['model', 'dense_4'])\n      self.assertListEqual(\n          model.compute_mask([e, f], [None, None]), [None, None])\n      self.assertListEqual(\n          final_model.compute_output_shape([(10, 32), (10, 32)]), [(10, 7),\n                                                                   (10, 64)])\n\n      # run recursive model\n      fn = backend.function(final_model.inputs, final_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n\n      # test serialization\n      model_config = final_model.get_config()\n      recreated_model = models.Model.from_config(model_config)\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_multi_input_multi_output_recursion(self):\n    with self.cached_session():\n      # test multi-input multi-output\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      j = layers.Input(shape=(32,), name='input_j')\n      k = layers.Input(shape=(32,), name='input_k')\n      _, n = model([j, k])\n\n      o = layers.Input(shape=(32,), name='input_o')\n      p = layers.Input(shape=(32,), name='input_p')\n      q, _ = model([o, p])\n\n      self.assertListEqual(n.shape.as_list(), [None, 5])\n      self.assertListEqual(q.shape.as_list(), [None, 64])\n      s = layers.concatenate([n, q], name='merge_nq')\n      self.assertListEqual(s.shape.as_list(), [None, 64 + 5])\n\n      # test with single output as 1-elem list\n      multi_io_model = training_lib.Model([j, k, o, p], [s])\n\n      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      # test with single output as tensor\n      multi_io_model = training_lib.Model([j, k, o, p], s)\n\n      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      # note that the output of the function will still be a 1-elem list\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      # test serialization\n      model_config = multi_io_model.get_config()\n      recreated_model = models.Model.from_config(model_config)\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      # note that the output of the function will still be a 1-elem list\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      config = model.get_config()\n      models.Model.from_config(config)\n\n      model.summary()\n      json_str = model.to_json()\n      models.model_from_json(json_str)\n\n      if yaml is not None:\n        yaml_str = model.to_yaml()\n        models.model_from_yaml(yaml_str)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_invalid_graphs(self):\n    a = layers.Input(shape=(32,), name='input_a')\n    b = layers.Input(shape=(32,), name='input_b')\n\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n    merged = layers.concatenate([a_2, b_2], name='merge')\n    c = layers.Dense(64, name='dense_2')(merged)\n    d = layers.Dense(5, name='dense_3')(c)\n\n    model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n    # input is not an Input tensor\n    j = layers.Input(shape=(32,), name='input_j')\n    j = layers.Dense(32)(j)\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k], [m, n])\n\n    # disconnected graph\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j], [m, n])\n\n    # redundant outputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    training_lib.Model([j, k], [m, n, n])\n\n    # redundant inputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k, j], [m, n])\n\n    # i have not idea what I'm doing: garbage as inputs/outputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k], [m, n, 0])\n\n  def test_raw_tf_compatibility(self):\n    with ops.Graph().as_default():\n      # test calling layers/models on TF tensors\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      j = layers.Input(shape=(32,), name='input_j')\n      k = layers.Input(shape=(32,), name='input_k')\n      self.assertEqual(len(model.inputs), 2)\n      m, n = model([j, k])\n      self.assertEqual(len(model.inputs), 2)\n      tf_model = training_lib.Model([j, k], [m, n])\n\n      j_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))\n      k_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))\n      m_tf, n_tf = tf_model([j_tf, k_tf])\n      self.assertListEqual(m_tf.shape.as_list(), [None, 64])\n      self.assertListEqual(n_tf.shape.as_list(), [None, 5])\n\n      # test merge\n      layers.concatenate([j_tf, k_tf], axis=1)\n      layers.add([j_tf, k_tf])\n\n      # test tensor input\n      x = array_ops.placeholder(shape=(None, 2), dtype=dtypes.float32)\n      layers.InputLayer(input_tensor=x)\n\n      x = layers.Input(tensor=x)\n      layers.Dense(2)(x)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_basic_masking(self):\n    a = layers.Input(shape=(10, 32), name='input_a')\n    b = layers.Masking()(a)\n    model = training_lib.Model(a, b)\n    self.assertEqual(model.output_mask.shape.as_list(), [None, 10])\n\n  def testMaskingSingleInput(self):\n\n    class MaskedLayer(layers.Layer):\n\n      def call(self, inputs, mask=None):\n        if mask is not None:\n          return inputs * mask\n        return inputs\n\n      def compute_mask(self, inputs, mask=None):\n        return array_ops.ones_like(inputs)\n\n    if context.executing_eagerly():\n      a = constant_op.constant([2] * 32)\n      mask = constant_op.constant([0, 1] * 16)\n      a._keras_mask = mask\n      b = MaskedLayer().apply(a)\n      self.assertTrue(hasattr(b, '_keras_mask'))\n      self.assertAllEqual(\n          self.evaluate(array_ops.ones_like(mask)),\n          self.evaluate(getattr(b, '_keras_mask')))\n      self.assertAllEqual(self.evaluate(a * mask), self.evaluate(b))\n    else:\n      x = input_layer_lib.Input(shape=(32,))\n      y = MaskedLayer()(x)  # pylint: disable=not-callable\n      network = functional.Functional(x, y)\n\n      # test callability on Input\n      x_2 = input_layer_lib.Input(shape=(32,))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 32])\n\n      # test callability on regular tensor\n      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 32])\n\n  def test_activity_regularization_with_model_composition(self):\n\n    def reg(x):\n      return math_ops.reduce_sum(x)\n\n    net_a_input = input_layer_lib.Input((2,))\n    net_a = net_a_input\n    net_a = layers.Dense(\n        2, kernel_initializer='ones', use_bias=False, activity_regularizer=reg)(\n            net_a)\n    model_a = training_lib.Model([net_a_input], [net_a])\n\n    net_b_input = input_layer_lib.Input((2,))\n    net_b = model_a(net_b_input)\n    model_b = training_lib.Model([net_b_input], [net_b])\n\n    model_b.compile(optimizer='sgd', loss=None)\n    x = np.ones((1, 2))\n    loss = model_b.evaluate(x)\n    self.assertEqual(loss, 4.)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_layer_sharing_at_heterogenous_depth(self):\n    x_val = np.random.random((10, 5))\n\n    x = input_layer_lib.Input(shape=(5,))\n    a = layers.Dense(5, name='A')\n    b = layers.Dense(5, name='B')\n    output = a(b(a(b(x))))\n    m = training_lib.Model(x, output)\n    m.run_eagerly = testing_utils.should_run_eagerly()\n\n    output_val = m.predict(x_val)\n\n    config = m.get_config()\n    weights = m.get_weights()\n\n    m2 = models.Model.from_config(config)\n    m2.set_weights(weights)\n\n    output_val_2 = m2.predict(x_val)\n    self.assertAllClose(output_val, output_val_2, atol=1e-6)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_layer_sharing_at_heterogenous_depth_with_concat(self):\n    input_shape = (16, 9, 3)\n    input_layer = input_layer_lib.Input(shape=input_shape)\n\n    a = layers.Dense(3, name='dense_A')\n    b = layers.Dense(3, name='dense_B')\n    c = layers.Dense(3, name='dense_C')\n\n    x1 = b(a(input_layer))\n    x2 = a(c(input_layer))\n    output = layers.concatenate([x1, x2])\n\n    m = training_lib.Model(inputs=input_layer, outputs=output)\n    m.run_eagerly = testing_utils.should_run_eagerly()\n\n    x_val = np.random.random((10, 16, 9, 3))\n    output_val = m.predict(x_val)\n\n    config = m.get_config()\n    weights = m.get_weights()\n\n    m2 = models.Model.from_config(config)\n    m2.set_weights(weights)\n\n    output_val_2 = m2.predict(x_val)\n    self.assertAllClose(output_val, output_val_2, atol=1e-6)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_explicit_training_argument(self):\n    a = layers.Input(shape=(2,))\n    b = layers.Dropout(0.5)(a)\n    base_model = training_lib.Model(a, b)\n\n    a = layers.Input(shape=(2,))\n    b = base_model(a, training=False)\n    model = training_lib.Model(a, b)\n\n    x = np.ones((100, 2))\n    y = np.ones((100, 2))\n    model.compile(\n        optimizer='sgd',\n        loss='mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    loss = model.train_on_batch(x, y)\n    self.assertEqual(loss, 0)  # In inference mode, output is equal to input.\n\n    a = layers.Input(shape=(2,))\n    b = base_model(a, training=True)\n    model = training_lib.Model(a, b)\n    preds = model.predict(x)\n    self.assertEqual(np.min(preds), 0.)  # At least one unit was dropped.\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_mask_derived_from_keras_layer(self):\n    inputs = input_layer_lib.Input((5, 10))\n    mask = input_layer_lib.Input((5,))\n    outputs = layers.RNN(layers.LSTMCell(100))(inputs, mask=mask)\n    model = training_lib.Model([inputs, mask], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # All data is masked, returned values are 0's.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.ones((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # Data is not masked, returned values are random.\n    self.assertGreater(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(model.get_config())\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # All data is masked, returned values are 0's.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.ones((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # Data is not masked, returned values are random.\n    self.assertGreater(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_arg_derived_from_keras_layer(self):\n\n    class MyAdd(layers.Layer):\n\n      def call(self, x1, x2):\n        return x1 + x2\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    outputs = MyAdd()(input1, input2)\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check serialization.\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MyAdd': MyAdd})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations(mode='eager'),)\n  def test_only_some_in_first_arg_derived_from_keras_layer_keras_tensors(self):\n    # This functionality is unsupported in v1 graphs\n\n    class MyAddAll(layers.Layer):\n\n      def call(self, inputs):\n        x = inputs[0]\n        for inp in inputs[1:]:\n          if inp is not None:\n            x = x + inp\n        return x\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    layer = MyAddAll()\n    outputs = layer([0.0, input1, None, input2, None])\n    model = training_lib.Model([input1, input2], outputs)\n    self.assertIn(layer, model.layers)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check serialization.\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MyAddAll': MyAddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(\n      combinations.times(\n          combinations.keras_mode_combinations(),\n          combinations.combine(share_already_used_layer=[True, False])))\n  def test_call_kwarg_derived_from_keras_layer(self, share_already_used_layer):\n\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        if x2 is not None:\n          return x1 + x2\n        return x1\n\n    class IdentityLayer(layers.Layer):\n\n      def call(self, x):\n        return x\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    identity_layer = IdentityLayer()\n\n    if share_already_used_layer:\n      # We have had model serialization/deserialization break in the past:\n      # when a layer was previously used to construct other functional models\n      # and had a non-empty list of inbound nodes before being used to define\n      # the model being serialized/deserialized.\n      # (The serialization/deserialization was not correctly adjusting\n      # the node_index serialization/deserialization).\n      # So, we explicitly test this case.\n      training_lib.Model([input1], identity_layer(input1))\n\n    outputs = MaybeAdd()(input1, x2=identity_layer(input2))\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(),\n        custom_objects={\n            'MaybeAdd': MaybeAdd,\n            'IdentityLayer': IdentityLayer\n        })\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_kwarg_dtype_serialization(self):\n\n    class Double(layers.Layer):\n\n      def call(self, x1, dtype=None):\n        return math_ops.cast(x1 + x1, dtype=dtype)\n\n    input1 = input_layer_lib.Input(10)\n    outputs = Double()(input1, dtype=dtypes.float16)\n    model = training_lib.Model([input1], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check the output dtype\n    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'Double': Double})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check the output dtype\n    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_kwarg_nonserializable(self):\n\n    class Double(layers.Layer):\n\n      def call(self, x1, kwarg=None):\n        return x1 + x1\n\n    class NonSerializable(object):\n\n      def __init__(self, foo=None):\n        self.foo = foo\n\n    input1 = input_layer_lib.Input(10)\n    outputs = Double()(input1, kwarg=NonSerializable())\n    model = training_lib.Model([input1], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    with self.assertRaisesRegex(\n        TypeError, 'Layer double was passed non-JSON-serializable arguments.'):\n      model.get_config()\n\n  @combinations.generate(\n      combinations.times(\n          combinations.keras_mode_combinations(),\n          combinations.combine(share_already_used_layer=[True, False])))\n  def test_call_kwarg_derived_from_keras_layer_and_first_arg_is_constant(\n      self, share_already_used_layer):\n\n    class IdentityLayer(layers.Layer):\n\n      def call(self, x):\n        return x\n\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        if x2 is not None:\n          return x1 + x2\n        return x1\n\n    input2 = input_layer_lib.Input(10)\n    identity_layer = IdentityLayer()\n    if share_already_used_layer:\n      # We have had model serialization/deserialization break in the past:\n      # when a layer was previously used to construct other functional models\n      # and had a non-empty list of inbound nodes before being used to define\n      # the model being serialized/deserialized.\n      # (The serialization/deserialization was not correctly adjusting\n      # the node_index serialization/deserialization).\n      # So, we explicitly test this case.\n      training_lib.Model([input2], identity_layer(input2))\n\n    outputs = MaybeAdd()(3., x2=identity_layer(input2))\n    model = training_lib.Model([input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=7 * np.ones((10, 10)),\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(),\n        custom_objects={\n            'MaybeAdd': MaybeAdd,\n            'IdentityLayer': IdentityLayer\n        })\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=7 * np.ones((10, 10)),\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_composite_call_kwarg_derived_from_keras_layer(self):\n\n    # Create a test layer that accepts composite tensor inputs.\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        # We need to convert this to a tensor for loss calculations -\n        # losses don't play nicely with ragged tensors yet.\n        if x2 is not None:\n          return (x1 + x2).to_tensor(default_value=0)\n        return x1.to_tensor(default_value=0)\n\n    input1 = input_layer_lib.Input((None,), ragged=True)\n    input2 = input_layer_lib.Input((None,), ragged=True)\n    outputs = MaybeAdd()(input1, x2=input2)\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    input_data = [\n        ragged_factory_ops.constant([[3.0, 3.0], [3.0, 3.0], [3.0]]),\n        ragged_factory_ops.constant([[7.0, 7.0], [7.0, 7.0], [7.0]])\n    ]\n    expected_data = np.array([[10.0, 10.0], [10.0, 10.0], [10.0, 0.0]])\n\n    history = model.fit(x=input_data, y=expected_data)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MaybeAdd': MaybeAdd})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(x=input_data, y=expected_data)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations(mode='eager'))\n  def test_call_some_not_all_nested_in_first_arg_derived_from_keras_layer(self):\n    # This functionality is unsupported in v1 graphs\n\n    class AddAll(layers.Layer):\n\n      def call(self, x1_x2, x3):\n        x1, x2 = x1_x2\n        out = x1 + x2\n        if x3 is not None:\n          for t in x3.values():\n            out += t\n        return out\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    input3 = input_layer_lib.Input(10)\n\n    layer = AddAll()\n    outputs = layer(\n        [input1, 4 * array_ops.ones((1, 10))],\n        x3={\n            'a': input2,\n            'b': input3,\n            'c': 5 * array_ops.ones((1, 10))\n        })\n    model = training_lib.Model([input1, input2, input3], outputs)\n    self.assertIn(layer, model.layers)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'AddAll': AddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_nested_arg_derived_from_keras_layer(self):\n\n    class AddAll(layers.Layer):\n\n      def call(self, x1, x2, x3=None):\n        out = x1 + x2\n        if x3 is not None:\n          for t in x3.values():\n            out += t\n        return out\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    input3 = input_layer_lib.Input(10)\n    outputs = AddAll()(\n        input1,\n        4 * array_ops.ones((1, 10)),\n        x3={\n            'a': input2,\n            'b': input3,\n            'c': 5 * array_ops.ones((1, 10))\n        })\n    model = training_lib.Model([input1, input2, input3], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'AddAll': AddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_multi_output_model_with_none_masking(self):\n    def func(x):\n      return [x * 0.2, x * 0.3]\n\n    def output_shape(input_shape):\n      return [input_shape, input_shape]\n\n    i = layers.Input(shape=(3, 2, 1))\n    o = layers.Lambda(function=func, output_shape=output_shape)(i)\n\n    self.assertEqual(backend.int_shape(o[0]), (None, 3, 2, 1))\n    self.assertEqual(backend.int_shape(o[1]), (None, 3, 2, 1))\n\n    o = layers.add(o)\n    model = training_lib.Model(i, o)\n    model.run_eagerly = testing_utils.should_run_eagerly()\n\n    i2 = layers.Input(shape=(3, 2, 1))\n    o2 = model(i2)\n    model2 = training_lib.Model(i2, o2)\n    model2.run_eagerly = testing_utils.should_run_eagerly()\n\n    x = np.random.random((4, 3, 2, 1))\n    out = model2.predict(x)\n    assert out.shape == (4, 3, 2, 1)\n    self.assertAllClose(out, x * 0.2 + x * 0.3, atol=1e-4)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_constant_initializer_with_numpy(self):\n    initializer = initializers.Constant(np.ones((3, 2)))\n    model = sequential.Sequential()\n    model.add(layers.Dense(2, input_shape=(3,), kernel_initializer=initializer))\n    model.add(layers.Dense(3))\n    model.compile(\n        loss='mse',\n        optimizer='sgd',\n        metrics=['acc'],\n        run_eagerly=testing_utils.should_run_eagerly())\n\n    json_str = model.to_json()\n    models.model_from_json(json_str)\n\n    if yaml is not None:\n      yaml_str = model.to_yaml()\n      models.model_from_yaml(yaml_str)\n\n  def test_subclassed_error_if_init_not_called(self):\n\n    class MyNetwork(training_lib.Model):\n\n      def __init__(self):\n        self._foo = [layers.Dense(10), layers.Dense(10)]\n\n    with self.assertRaisesRegex(RuntimeError, 'forgot to call'):\n      MyNetwork()\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_int_input_shape(self):\n    inputs = input_layer_lib.Input(10)\n    self.assertEqual([None, 10], inputs.shape.as_list())\n\n    inputs_with_batch = input_layer_lib.Input(batch_size=20, shape=5)\n    self.assertEqual([20, 5], inputs_with_batch.shape.as_list())\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_model_initialization(self):\n    # Functional model\n    inputs = input_layer_lib.Input(shape=(32,))\n    outputs = layers.Dense(4)(inputs)\n\n    with self.assertRaisesRegex(TypeError,\n                                'Keyword argument not understood'):\n      model = training_lib.Model(\n          inputs, outputs, name='m', trainable=False, dtype='int64')\n    with self.assertRaisesRegex(TypeError,\n                                'Keyword argument not understood'):\n      model = training_lib.Model(\n          inputs, outputs, name='m', trainable=False, dynamic=False)\n\n    model = training_lib.Model(inputs, outputs, name='m', trainable=False)\n    self.assertEqual('m', model.name)\n    self.assertFalse(model.trainable)\n    self.assertFalse(model.dynamic)\n\n    class SubclassModel(training_lib.Model):\n      pass\n    # Subclassed model\n    model = SubclassModel(\n        name='subclassed', trainable=True, dtype='int64', dynamic=True)\n    self.assertEqual('subclassed', model.name)\n    self.assertTrue(model.dynamic)\n    self.assertTrue(model.trainable)\n    w = model.add_weight('w', [], initializer=initializers.Constant(1))\n    self.assertEqual(dtypes.int64, w.dtype)\n\n  def test_disconnected_inputs(self):\n    input_tensor1 = input_layer_lib.Input(shape=[200], name='a')\n    input_tensor2 = input_layer_lib.Input(shape=[10], name='b')\n    output_tensor1 = layers.Dense(units=10)(input_tensor1)\n\n    net = functional.Functional(\n        inputs=[input_tensor1, input_tensor2], outputs=[output_tensor1])\n    net2 = functional.Functional.from_config(net.get_config())\n    self.assertLen(net2.inputs, 2)\n    self.assertEqual('a', net2.layers[0].name)\n    self.assertEqual('b', net2.layers[1].name)\n\n  @combinations.generate(combinations.keras_model_type_combinations())\n  def test_dependency_tracking(self):\n    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n    model.trackable = Checkpoint()\n    self.assertIn('trackable', model._unconditional_dependency_names)\n    self.assertEqual(model.trackable, model._lookup_dependency('trackable'))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_model_construction_in_tf_function(self):\n\n    d = {'model': None}\n\n    @def_function.function\n    def fn(x):\n      if d['model'] is None:\n        # Check that Functional can be built in a `tf.function`.\n        inputs = input_layer_lib.Input(10)\n        outputs = layers.Dense(1)(inputs)\n        model = functional.Functional(inputs, outputs)\n        d['model'] = model\n      else:\n        model = d['model']\n\n      return model(x)\n\n    x = array_ops.ones((10, 10))\n    y = fn(x)\n    self.assertEqual(y.shape.as_list(), [10, 1])\n\n\nclass DeferredModeTest(keras_parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testSimpleNetworkBuilding(self):\n    inputs = input_layer_lib.Input(shape=(32,))\n    if context.executing_eagerly():\n      self.assertEqual(inputs.dtype.name, 'float32')\n      self.assertEqual(inputs.shape.as_list(), [None, 32])\n\n    x = layers.Dense(2)(inputs)\n    if context.executing_eagerly():\n      self.assertEqual(x.dtype.name, 'float32')\n      self.assertEqual(x.shape.as_list(), [None, 2])\n\n    outputs = layers.Dense(4)(x)\n    network = functional.Functional(inputs, outputs)\n    self.assertIsInstance(network, functional.Functional)\n\n    if context.executing_eagerly():\n      # It should be possible to call such a network on EagerTensors.\n      inputs = constant_op.constant(\n          np.random.random((10, 32)).astype('float32'))\n      outputs = network(inputs)\n      self.assertEqual(outputs.shape.as_list(), [10, 4])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testMultiIONetworkBuilding(self):\n    input_a = input_layer_lib.Input(shape=(32,))\n    input_b = input_layer_lib.Input(shape=(16,))\n    a = layers.Dense(16)(input_a)\n\n    class AddLayer(layers.Layer):\n\n      def call(self, inputs):\n        return inputs[0] + inputs[1]\n\n    c = AddLayer()([a, input_b])  # pylint: disable=not-callable\n    c = layers.Dense(2)(c)\n\n    network = functional.Functional([input_a, input_b], [a, c])\n    if context.executing_eagerly():\n      a_val = constant_op.constant(\n          np.random.random((10, 32)).astype('float32'))\n      b_val = constant_op.constant(\n          np.random.random((10, 16)).astype('float32'))\n      outputs = network([a_val, b_val])\n      self.assertEqual(len(outputs), 2)\n      self.assertEqual(outputs[0].shape.as_list(), [10, 16])\n      self.assertEqual(outputs[1].shape.as_list(), [10, 2])\n\n\nclass DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n\n  def _testShapeInference(self, model, input_shape, expected_output_shape):\n    input_value = np.random.random(input_shape)\n    output_value = model.predict(input_value)\n    self.assertEqual(output_value.shape, expected_output_shape)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testSingleInputCase(self):\n\n    class LayerWithOneInput(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs):\n        return backend.dot(inputs, self.w)\n\n    inputs = input_layer_lib.Input(shape=(3,))\n    layer = LayerWithOneInput()\n\n    if context.executing_eagerly():\n      self.assertEqual(\n          layer.compute_output_shape((None, 3)).as_list(), [None, 4])\n      # As a side-effect, compute_output_shape builds the layer.\n      self.assertTrue(layer.built)\n      # We can still query the layer's compute_output_shape with compatible\n      # input shapes.\n      self.assertEqual(\n          layer.compute_output_shape((6, 3)).as_list(), [6, 4])\n\n    outputs = layer(inputs)\n    model = training_lib.Model(inputs, outputs)\n    self._testShapeInference(model, (2, 3), (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testMultiInputOutputCase(self):\n\n    class MultiInputOutputLayer(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs):\n        a = backend.dot(inputs[0], self.w)\n        b = a + inputs[1]\n        return [a, b]\n\n    input_a = input_layer_lib.Input(shape=(3,))\n    input_b = input_layer_lib.Input(shape=(4,))\n    output_a, output_b = MultiInputOutputLayer()([input_a, input_b])\n    model = training_lib.Model([input_a, input_b], [output_a, output_b])\n    output_a_val, output_b_val = model.predict(\n        [np.random.random((2, 3)), np.random.random((2, 4))])\n    self.assertEqual(output_a_val.shape, (2, 4))\n    self.assertEqual(output_b_val.shape, (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTrainingArgument(self):\n\n    class LayerWithTrainingArg(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs, training):\n        return backend.dot(inputs, self.w)\n\n    inputs = input_layer_lib.Input(shape=(3,))\n    outputs = LayerWithTrainingArg()(inputs, training=False)\n    model = training_lib.Model(inputs, outputs)\n    self._testShapeInference(model, (2, 3), (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShape(self):\n\n    class Model(training_lib.Model):\n\n      def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n        self.pool = layers.GlobalAveragePooling2D()\n        self.fc = layers.Dense(3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.fc(x)\n        return x\n\n    model = Model()\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShapeWithCompoundModel(self):\n\n    class BasicBlock(training_lib.Model):\n\n      def __init__(self):\n        super(BasicBlock, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n        self.pool = layers.GlobalAveragePooling2D()\n        self.dense = layers.Dense(3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.dense(x)\n        return x\n\n    class CompoundModel(training_lib.Model):\n\n      def __init__(self):\n        super(CompoundModel, self).__init__()\n        self.block = BasicBlock()\n\n      def call(self, x):\n        x = self.block(x)  # pylint: disable=not-callable\n        return x\n\n    model = CompoundModel()\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)  # pylint: disable=not-callable\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShapeWithFunctionalAPI(self):\n\n    class BasicBlock(training_lib.Model):\n      # Inheriting from layers.Layer since we are calling this layer\n      # inside a model created using functional API.\n\n      def __init__(self):\n        super(BasicBlock, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        return x\n\n    input_layer = layers.Input(shape=(None, None, 1))\n    x = BasicBlock()(input_layer)\n    x = layers.GlobalAveragePooling2D()(x)\n    output_layer = layers.Dense(3)(x)\n\n    model = training_lib.Model(inputs=input_layer, outputs=output_layer)\n\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_sequential_as_downstream_of_masking_layer(self):\n    inputs = layers.Input(shape=(3, 4))\n    x = layers.Masking(mask_value=0., input_shape=(3, 4))(inputs)\n\n    s = sequential.Sequential()\n    s.add(layers.Dense(5, input_shape=(4,)))\n\n    x = layers.wrappers.TimeDistributed(s)(x)\n    model = training_lib.Model(inputs=inputs, outputs=x)\n    model.compile(\n        optimizer='rmsprop',\n        loss='mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n\n    model_input = np.random.randint(\n        low=1, high=5, size=(10, 3, 4)).astype('float32')\n    for i in range(4):\n      model_input[i, i:, :] = 0.\n    model.fit(model_input,\n              np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n\n    if not context.executing_eagerly():\n      # Note: this doesn't work in eager due to DeferredTensor/ops compatibility\n      # issue.\n      mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]\n      mask_outputs += [model.layers[2].compute_mask(\n          model.layers[2].input, mask_outputs[-1])]\n      func = backend.function([model.input], mask_outputs)\n      mask_outputs_val = func([model_input])\n      self.assertAllClose(mask_outputs_val[0], np.any(model_input, axis=-1))\n      self.assertAllClose(mask_outputs_val[1], np.any(model_input, axis=-1))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_external_keras_serialization_compat_input_layers(self):\n    inputs = input_layer_lib.Input(shape=(10,))\n    outputs = layers.Dense(1)(inputs)\n    model = training_lib.Model(inputs, outputs)\n    config = model.get_config()\n    # Checks that single inputs and outputs are still saved as 1-element lists.\n    # Saving as 1-element lists or not is equivalent in TF Keras, but only the\n    # 1-element list format is supported in TF.js and keras-team/Keras.\n    self.assertLen(config['input_layers'], 1)\n    self.assertLen(config['output_layers'], 1)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_external_keras_serialization_compat_inbound_nodes(self):\n    # Check single Tensor input.\n    inputs = input_layer_lib.Input(shape=(10,), name='in')\n    outputs = layers.Dense(1)(inputs)\n    model = training_lib.Model(inputs, outputs)\n    config = model.get_config()\n    self.assertEqual(config['layers'][1]['inbound_nodes'], [[['in', 0, 0, {}]]])\n\n    # Check multiple Tensor input.\n    inputs1 = input_layer_lib.Input(shape=(10,), name='in1')\n    inputs2 = input_layer_lib.Input(shape=(10,), name='in2')\n    outputs = layers.Add()([inputs1, inputs2])\n    model = training_lib.Model([inputs1, inputs2], outputs)\n    config = model.get_config()\n    self.assertEqual(config['layers'][2]['inbound_nodes'],\n                     [[['in1', 0, 0, {}], ['in2', 0, 0, {}]]])\n\n  @combinations.generate(combinations.combine(mode=['eager']))\n  def test_dict_inputs_tensors(self):\n    # Note that this test is running with v2 eager only, since the v1\n    # will behave differently wrt to dict input for training.\n    inputs = {\n        'sentence2': input_layer_lib.Input(\n            shape=(), name='a', dtype=dtypes.string),\n        'sentence1': input_layer_lib.Input(\n            shape=(), name='b', dtype=dtypes.string),\n    }\n    strlen = layers.Lambda(string_ops.string_length_v2)\n    diff = layers.Subtract()(\n        [strlen(inputs['sentence1']), strlen(inputs['sentence2'])])\n    diff = math_ops.cast(diff, dtypes.float32)\n    model = training_lib.Model(inputs, diff)\n\n    extra_keys = {\n        'sentence1': constant_op.constant(['brown fox', 'lazy dog']),\n        'sentence2': constant_op.constant(['owl', 'cheeky cat']),\n        'label': constant_op.constant([0, 1]),\n    }\n\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model(extra_keys)\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    model.compile('sgd', 'mse')\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model.fit(extra_keys, y=constant_op.constant([0, 1]), steps_per_epoch=1)\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model.evaluate(extra_keys, constant_op.constant([0, 1]))\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    # Make sure the model inputs are sorted with the dict keys.\n    self.assertEqual(model.inputs[0]._keras_history.layer.name, 'b')\n    self.assertEqual(model.inputs[1]._keras_history.layer.name, 'a')\n\n\nclass GraphUtilsTest(test.TestCase):\n\n  def testGetReachableFromInputs(self):\n\n    with ops.Graph().as_default(), self.cached_session():\n      pl_1 = array_ops.placeholder(shape=None, dtype='float32')\n      pl_2 = array_ops.placeholder(shape=None, dtype='float32')\n      pl_3 = array_ops.placeholder(shape=None, dtype='float32')\n      x_1 = pl_1 + pl_2\n      x_2 = pl_2 * 2\n      x_3 = pl_3 + 1\n      x_4 = x_1 + x_2\n      x_5 = x_3 * pl_1\n\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_1]),\n          {pl_1, x_1, x_4, x_5, x_1.op, x_4.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_1, pl_2]),\n          {pl_1, pl_2, x_1, x_2, x_4, x_5, x_1.op, x_2.op, x_4.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_3]),\n          {pl_3, x_3, x_5, x_3.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([x_3]), {x_3, x_5, x_5.op})\n\n\nclass NestedNetworkTest(keras_parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_inputs_network(self):\n    inputs = {\n        'x1': input_layer_lib.Input(shape=(1,)),\n        'x2': input_layer_lib.Input(shape=(1,))\n    }\n    outputs = layers.Add()([inputs['x1'], inputs['x2']])\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    result_tensor = network({\n        'x1': array_ops.ones((1, 1), 'float32'),\n        'x2': array_ops.ones((1, 1), 'float32')\n    })\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result, [[2.]])\n\n    # TODO(b/122726584): Investigate why concrete batch is flaky in some builds.\n    output_shape = network.compute_output_shape({\n        'x1': (None, 1),\n        'x2': (None, 1)\n    })\n    self.assertListEqual(output_shape.as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_outputs_network(self):\n    inputs = input_layer_lib.Input(shape=(1,))\n    outputs = {\n        'x+x': layers.Add()([inputs, inputs]),\n        'x*x': layers.Multiply()([inputs, inputs])\n    }\n\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    result_tensor = network(array_ops.ones((1, 1), 'float32'))\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result['x+x'], [[2.]])\n    self.assertAllEqual(result['x*x'], [[1.]])\n\n    output_shape = network.compute_output_shape((None, 1))\n    self.assertListEqual(output_shape['x+x'].as_list(), [None, 1])\n    self.assertListEqual(output_shape['x*x'].as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_network_inside_network(self):\n    inner_inputs = {\n        'x1': input_layer_lib.Input(shape=(1,)),\n        'x2': input_layer_lib.Input(shape=(1,))\n    }\n    inner_outputs = {\n        'x1+x2': layers.Add()([inner_inputs['x1'], inner_inputs['x2']]),\n        'x1*x2': layers.Multiply()([inner_inputs['x1'], inner_inputs['x2']])\n    }\n    inner_network = functional.Functional(\n        inner_inputs, inner_outputs)\n\n    inputs = [\n        input_layer_lib.Input(shape=(1,)),\n        input_layer_lib.Input(shape=(1,))\n    ]\n    middle = inner_network({'x1': inputs[0], 'x2': inputs[1]})\n    outputs = layers.Add()([middle['x1+x2'], middle['x1*x2']])\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    # Computes: `(x1+x2) + (x1*x2)`\n    result_tensor = network(\n        [array_ops.ones((1, 1), 'float32'),\n         array_ops.ones((1, 1), 'float32')])\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result, [[3.]])\n\n    output_shape = network.compute_output_shape([(None, 1), (None, 1)])\n    self.assertListEqual(output_shape.as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph']))\n  def test_updates_with_direct_call(self):\n    inputs = input_layer_lib.Input(shape=(10,))\n    x = layers.BatchNormalization()(inputs)\n    x = layers.Dense(10)(x)\n    model = training_lib.Model(inputs, x)\n\n    ph = backend.placeholder(shape=(10, 10))\n    model(ph)\n\n    self.assertLen(model.updates, 4)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_dict_mapping_input(self):\n\n    class ReturnFirst(layers.Layer):\n\n      def call(self, inputs):\n        b, _ = inputs\n        return b\n\n    # Checks that inputs are put in same order as the\n    # Model was constructed with.\n    b = input_layer_lib.Input(shape=(10,), name='b')\n    a = input_layer_lib.Input(shape=(10,), name='a')\n    outputs = ReturnFirst()([b, a])\n\n    b_val = array_ops.ones((10, 10))\n    a_val = array_ops.zeros((10, 10))\n\n    model = training_lib.Model([b, a], outputs)\n    res = model({'a': a_val, 'b': b_val})\n    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n\n    reversed_model = training_lib.Model([a, b], outputs)\n    res = reversed_model({'a': a_val, 'b': b_val})\n    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_dict_mapping_single_input(self):\n    b = input_layer_lib.Input(shape=(1,), name='b')\n    outputs = b * 2\n    model = training_lib.Model(b, outputs)\n\n    b_val = array_ops.ones((1, 1))\n    extra_val = array_ops.ones((1, 10))\n\n    inputs = {'a': extra_val, 'b': b_val}\n    res = model(inputs)\n\n    # Check that 'b' was used and 'a' was ignored.\n    self.assertEqual(res.shape.as_list(), [1, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_dict_mapping(self):\n    a = input_layer_lib.Input(shape=(1,), dtype='int32', name='a')\n    b = input_layer_lib.Input(shape=(1,), dtype='int32', name='b')\n    c = input_layer_lib.Input(shape=(1,), dtype='int32', name='c')\n    d = input_layer_lib.Input(shape=(1,), dtype='int32', name='d')\n    inputs = {'a': (a, b), 'c': (c, d)}\n    outputs = 1000 * a + 100 * b + 10 * c + d\n    model = training_lib.Model(inputs, outputs)\n\n    a_val = array_ops.ones((1, 1), dtype='int32')\n    b_val = 2 * array_ops.ones((1, 1), dtype='int32')\n    c_val = 3 * array_ops.ones((1, 1), dtype='int32')\n    d_val = 4 * array_ops.ones((1, 1), dtype='int32')\n\n    inputs_val = {'a': (a_val, b_val), 'c': (c_val, d_val)}\n    res = model(inputs_val)\n\n    # Check that inputs were flattened in the correct order.\n    self.assertFalse(model._enable_dict_to_input_mapping)\n    self.assertEqual(self.evaluate(res), [1234])\n\n\n@combinations.generate(combinations.keras_mode_combinations())\nclass AddLossTest(keras_parameterized.TestCase):\n\n  def test_add_loss_outside_call_only_loss(self):\n    inputs = input_layer_lib.Input((10,))\n    mid = layers.Dense(10)(inputs)\n    outputs = layers.Dense(1)(mid)\n    model = training_lib.Model(inputs, outputs)\n    model.add_loss(math_ops.reduce_mean(outputs))\n    self.assertLen(model.losses, 1)\n\n    initial_weights = model.get_weights()\n\n    x = np.ones((10, 10))\n    model.compile(\n        'sgd',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model.fit(x, batch_size=2, epochs=1)\n\n    model2 = model.from_config(model.get_config())\n    model2.compile(\n        'sgd',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model2.set_weights(initial_weights)\n    model2.fit(x, batch_size=2, epochs=1)\n\n    # The TFOpLayer and the AddLoss layer are serialized.\n    self.assertLen(model2.layers, 5)\n    self.assertAllClose(model.get_weights(), model2.get_weights())\n\n  def test_add_loss_outside_call_multiple_losses(self):\n    inputs = input_layer_lib.Input((10,))\n    x1 = layers.Dense(10)(inputs)\n    x2 = layers.Dense(10)(x1)\n    outputs = layers.Dense(1)(x2)\n    model = training_lib.Model(inputs, outputs)\n    model.add_loss(math_ops.reduce_sum(x1 * x2))\n    model.add_loss(math_ops.reduce_mean(outputs))\n    self.assertLen(model.losses, 2)\n\n    initial_weights = model.get_weights()\n\n    x, y = np.ones((10, 10)), np.ones((10, 1))\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model.fit(x, y, batch_size=2, epochs=1)\n\n    model2 = model.from_config(model.get_config())\n    model2.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model2.set_weights(initial_weights)\n    model2.fit(x, y, batch_size=2, epochs=1)\n\n    self.assertAllClose(model.get_weights(), model2.get_weights())\n\n  def test_add_loss_crossentropy_backtracking(self):\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((1,))\n    outputs = layers.Dense(1, activation='sigmoid')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.binary_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.random((2, 1))\n    model.fit([x, y])\n\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((2,))\n    outputs = layers.Dense(2, activation='softmax')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.categorical_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.random((2, 2))\n    model.fit([x, y])\n\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((1,), dtype='int32')\n    outputs = layers.Dense(2, activation='softmax')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.sparse_categorical_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.randint(0, 2, size=(2, 1))\n    model.fit([x, y])\n\n\n@combinations.generate(combinations.keras_mode_combinations())\nclass WeightAccessTest(keras_parameterized.TestCase):\n\n  def test_functional_model(self):\n    inputs = input_layer_lib.Input((10,))\n    x1 = layers.Dense(10)(inputs)\n    x2 = layers.Dense(10)(x1)\n    outputs = layers.Dense(1)(x2)\n    model = training_lib.Model(inputs, outputs)\n\n    self.assertEqual(len(model.weights), 6)\n\n  def test_sequential_model_with_input_shape(self):\n    x1 = layers.Dense(10, input_shape=(10,))\n    x2 = layers.Dense(10)\n    x3 = layers.Dense(1)\n    model = sequential.Sequential([x1, x2, x3])\n\n    self.assertEqual(len(model.weights), 6)\n\n  def test_sequential_model_without_input_shape(self):\n    x1 = layers.Dense(10)\n    x2 = layers.Dense(10)\n    x3 = layers.Dense(1)\n    model = sequential.Sequential([x1, x2, x3])\n\n    with self.assertRaisesRegex(\n        ValueError, 'Weights for model .* have not yet been created'):\n      _ = model.weights\n\n  def test_subclass_model_with_build_method(self):\n\n    class SubclassModel(models.Model):\n\n      def build(self, input_shape):\n        self.w = self.add_weight(shape=input_shape[-1], initializer='ones')\n\n      def call(self, inputs):\n        return inputs * self.w\n\n    model = SubclassModel()\n\n    with self.assertRaisesRegex(\n        ValueError, 'Weights for model .* have not yet been created'):\n      _ = model.weights\n\n    model(input_layer_lib.Input((10,)))\n    self.assertEqual(len(model.weights), 1)\n\n  def test_subclass_model_without_build_method(self):\n\n    class SubclassModel(models.Model):\n\n      def __init__(self):\n        super(SubclassModel, self).__init__()\n        self.w = self.add_weight(shape=(), initializer='ones')\n\n      def call(self, inputs):\n        return inputs * self.w\n\n    model = SubclassModel()\n    self.assertEqual(len(model.weights), 1)\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass DTypeTest(keras_parameterized.TestCase):\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_graph_network_dtype(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    network = functional.Functional(inputs, outputs)\n    self.assertEqual(network.dtype, 'float32')\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_subclassed_network_dtype(self):\n\n    class IdentityNetwork(training_lib.Model):\n\n      def call(self, inputs):\n        return inputs\n\n    network = IdentityNetwork()\n    self.assertEqual(network.dtype, 'float32')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float32')\n\n    network = IdentityNetwork(dtype='float16')\n    self.assertEqual(network.dtype, 'float16')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float16')\n\n    network = IdentityNetwork(autocast=False)\n    self.assertEqual(network.dtype, 'float32')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float64')\n\n\nclass AttrTrackingLayer(base_layer.Layer):\n  \"\"\"Count how many times `dynamic` and `stateful` are called.\n\n  These counts are used to test that the attribute cache behaves as expected.\n  \"\"\"\n  def __init__(self, *args, **kwargs):\n    self.stateful_count = 0\n    self.dynamic_count = 0\n    super(AttrTrackingLayer, self).__init__(*args, **kwargs)\n\n  @base_layer.Layer.stateful.getter\n  def stateful(self):\n    self.stateful_count += 1\n    return super(AttrTrackingLayer, self).stateful\n\n  @property\n  def dynamic(self):\n    self.dynamic_count += 1\n    return super(AttrTrackingLayer, self).dynamic\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass CacheCorrectnessTest(keras_parameterized.TestCase):\n\n  def layer_and_network_test(self):\n    # Top level layer\n    network = functional.Functional()\n\n    layer_0 = AttrTrackingLayer()\n\n    sub_network = functional.Functional()\n    layer_1 = AttrTrackingLayer(dynamic=True)\n    layer_2 = AttrTrackingLayer()\n    sub_network.sub_layers = [layer_1, layer_2]\n\n    network.sub_layer = layer_0\n\n    for _ in range(2):\n      self.assertEqual(network.dynamic, False)\n      self.assertEqual(network.stateful, False)\n\n      # The second pass should be a cache hit.\n      self.assertEqual(layer_0.dynamic_count, 1)\n      self.assertEqual(layer_0.stateful_count, 1)\n\n    # Mutations of the sub-layer should force recalculation of the network's\n    # stateful attribute. (mutations bubble up.)\n    layer_0.stateful = True\n    self.assertEqual(network.stateful, True)\n    self.assertEqual(layer_0.stateful_count, 2)\n\n    layer_0.stateful = False\n    self.assertEqual(network.stateful, False)\n    self.assertEqual(layer_0.stateful_count, 3)\n\n    # But changing stateful should not affect dynamic.\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(layer_0.dynamic_count, 1)\n\n    network.sub_network = sub_network\n\n    # Adding to the topology should invalidate the cache and reflect in the top\n    # level network.\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 2)\n    self.assertEqual(layer_1.dynamic_count, 1)\n\n    # Still dynamic, but we need to recompute.\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 3)\n    self.assertEqual(layer_1.dynamic_count, 2)\n\n    # Now that we've removed the dynamic layer deep in the layer hierarchy, we\n    # need to make sure that that bubbles up through all the levels.\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(layer_0.dynamic_count, 4)\n    self.assertEqual(layer_1.dynamic_count, 2)\n\n    # Now check with a tracked dict.\n    sub_network.sub_layers = {\n        \"layer_1\": layer_1,\n        \"layer_2\": layer_2,\n    }\n\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 5)\n    self.assertEqual(layer_1.dynamic_count, 3)\n\n    # In-place assignment should still invalidate the cache.\n    sub_network.sub_layers[\"layer_1\"] = layer_1\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 6)\n    self.assertEqual(layer_1.dynamic_count, 4)\n\n    sub_network.sub_layers[\"layer_1\"] = None\n    for _ in range(2):\n      self.assertEqual(network.dynamic, False)\n      self.assertEqual(layer_0.dynamic_count, 7)\n      self.assertEqual(layer_1.dynamic_count, 4)\n\n    layer_3 = AttrTrackingLayer()\n    layer_3.stateful = True\n\n    sub_network.sub_layers = None\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(network.stateful, False)\n\n    # Test duplicate layers.\n    sub_network.sub_layers = [layer_1, layer_1, layer_1, layer_3]\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(network.stateful, True)\n\n    for _ in range(3):\n      sub_network.sub_layers.pop()\n      self.assertEqual(network.dynamic, True)\n      self.assertEqual(network.stateful, False)\n\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(network.stateful, False)\n\n  def test_compute_output_shape_cache(self):\n    # See https://github.com/tensorflow/tensorflow/issues/32029.\n    x = input_layer_lib.Input(shape=(None, 32))\n    dense = layers.Dense(2)\n    y = dense(x)\n    network = functional.Functional(x, y, name='dense_network')\n\n    for i in range(999, 1024):\n      self.assertEqual(network.compute_output_shape((1, i, 32)), (1, i, 2))\n\n  def test_2d_inputs_squeezed_to_1d(self):\n    input_1d = input_layer_lib.Input(shape=())\n    outputs = input_1d * 2.\n    net = functional.Functional(input_1d, outputs)\n\n    x = np.ones((10, 1))\n    y = net(x)\n    self.assertEqual(y.shape.rank, 1)\n\n  def test_1d_inputs_expanded_to_2d(self):\n    input_1d = input_layer_lib.Input(shape=(1,))\n    outputs = input_1d * 2.\n    net = functional.Functional(input_1d, outputs)\n\n    x = np.ones((10,))\n    y = net(x)\n    self.assertEqual(y.shape.rank, 2)\n\n  def test_training_passed_during_construction(self):\n\n    def _call(inputs, training):\n      if training is None:\n        return inputs * -1.0\n      elif training:\n        return inputs\n      else:\n        return inputs * 0.0\n\n    class MyLayer(base_layer.Layer):\n\n      def call(self, inputs, training=True):\n        return _call(inputs, training)\n\n    my_layer = MyLayer()\n    x = np.ones((1, 10))\n\n    # Hard-coded `true` value passed during construction is respected.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=True)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, True))\n    self.assertAllEqual(network(x), _call(x, True))\n\n    # Hard-coded `false` value passed during construction is respected.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=False)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, False))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    self.assertAllEqual(network(x), _call(x, False))\n\n    if context.executing_eagerly():\n      # In v2, construction still works when no `training` is specified\n      # When no value passed during construction, it uses the local default.\n      inputs = input_layer_lib.Input(10)\n      outputs = my_layer(inputs)\n      network = functional.Functional(inputs, outputs)\n      self.assertAllEqual(network(x, training=True), _call(x, True))\n      self.assertAllEqual(network(x, training=False), _call(x, False))\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n\n    # `None` value passed positionally during construction is ignored at runtime\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, None)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    if context.executing_eagerly():\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n    else:\n      # in v1 training would have defaulted to using the `None` inside the layer\n      # if training is not passed at runtime\n      self.assertAllEqual(network(x), _call(x, None))\n\n    # `None` value passed as kwarg during construction is ignored at runtime.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=None)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    if context.executing_eagerly():\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n    else:\n      # in v1 training would have defaulted to using the `None` inside the layer\n      # if training is not passed at runtime\n      self.assertAllEqual(network(x), _call(x, None))\n\n\nclass InputsOutputsErrorTest(keras_parameterized.TestCase):\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_input_error(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    with self.assertRaisesRegex(\n        TypeError, \"('Keyword argument not understood:', 'input')\"):\n      models.Model(input=inputs, outputs=outputs)\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_output_error(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    with self.assertRaisesRegex(\n        TypeError, \"('Keyword argument not understood:', 'output')\"):\n      models.Model(inputs=inputs, output=outputs)\n\n  def test_input_spec(self):\n    if not context.executing_eagerly():\n      return\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    model = models.Model(inputs, outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model(np.zeros((3, 11)))\n\n  def test_input_spec_list_of_inputs(self):\n    if not context.executing_eagerly():\n      return\n    input_1 = input_layer_lib.Input((10,), name='1')\n    input_2 = input_layer_lib.Input((5,), name='2')\n    x = layers.Concatenate()([input_1, input_2])\n    outputs = layers.Dense(10)(x)\n    model = models.Model([input_1, input_2], outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'.*expects 2 input.*'):\n      model(np.zeros((3, 10)))\n    with self.assertRaisesRegex(\n        ValueError, r'.*expects 2 input.*'):\n      model([np.zeros((3, 10)), np.zeros((3, 5)), np.zeros((3, 10))])\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model([np.zeros((3, 10)), np.zeros((3, 6))])\n\n    # Test passing data via dict keyed by input name\n    with self.assertRaisesRegex(\n        ValueError, r'Missing data for input.*'):\n      model({'1': np.zeros((3, 10))})\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})\n\n  def test_input_spec_dict(self):\n    if not context.executing_eagerly():\n      return\n    input_1 = input_layer_lib.Input((10,))\n    input_2 = input_layer_lib.Input((5,))\n    x = layers.Concatenate()([input_1, input_2])\n    outputs = layers.Dense(10)(x)\n    model = models.Model({'1': input_1, '2': input_2}, outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'Missing data for input.*'):\n      model({'1': np.zeros((3, 10))})\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})\n\n\nclass FunctionalSubclassModel(training_lib.Model):\n\n  def __init__(self, *args, **kwargs):\n    self.foo = {'foo': 'bar'}  # Make sure users can assign dict attributes\n    my_input = input_layer_lib.Input(shape=(16,))\n    dense = layers.Dense(32, activation='relu')\n    output = dense(my_input)\n    outputs = {'output': output}\n    super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)\n\n\nclass MixinClass(object):\n\n  def __init__(self, foo, **kwargs):\n    self._foo = foo\n    super().__init__(**kwargs)\n\n  def get_foo(self):\n    return self._foo\n\n\nclass SubclassedModel(training_lib.Model):\n\n  def __init__(self, bar, **kwargs):\n    self._bar = bar\n    super().__init__(**kwargs)\n\n  def get_bar(self):\n    return self._bar\n\n\nclass MultipleInheritanceModelTest(keras_parameterized.TestCase):\n\n  def testFunctionalSubclass(self):\n    m = FunctionalSubclassModel()\n    # Some smoke test for the weights and output shape of the model\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n\n  def testFunctionalSubclassPreMixin(self):\n    class MixedFunctionalSubclassModel(MixinClass, FunctionalSubclassModel):\n      pass\n\n    m = MixedFunctionalSubclassModel(foo='123')\n    self.assertTrue(m._is_graph_network)\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n    self.assertEqual(m.get_foo(), '123')\n\n  def testFunctionalSubclassPostMixin(self):\n    # Make sure the the mixin class is also init correct when the order changed.\n\n    class MixedFunctionalSubclassModel(FunctionalSubclassModel, MixinClass):\n      pass\n\n    m = MixedFunctionalSubclassModel(foo='123')\n    self.assertTrue(m._is_graph_network)\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n    self.assertEqual(m.get_foo(), '123')\n\n  def testSubclassModelPreMixin(self):\n    class MixedSubclassModel(MixinClass, SubclassedModel):\n      pass\n\n    m = MixedSubclassModel(foo='123', bar='456')\n    self.assertFalse(m._is_graph_network)\n    self.assertEqual(m.get_foo(), '123')\n    self.assertEqual(m.get_bar(), '456')\n\n\nif __name__ == '__main__':\n  test.main()\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Training-related part of the Keras engine.\"\"\"\n\nimport copy\nimport itertools\nimport json\nimport os\nimport warnings\nimport weakref\n\nfrom tensorflow.python.autograph.lang import directives\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.data.ops import options as options_lib\nfrom tensorflow.python.distribute import collective_all_reduce_strategy\nfrom tensorflow.python.distribute import distribution_strategy_context as ds_context\nfrom tensorflow.python.distribute import values as ds_values\nfrom tensorflow.python.distribute.coordinator import cluster_coordinator\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import composite_tensor\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import func_graph\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras import callbacks as callbacks_module\nfrom tensorflow.python.keras import optimizer_v1\nfrom tensorflow.python.keras import optimizers\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.keras.engine import base_layer_utils\nfrom tensorflow.python.keras.engine import compile_utils\nfrom tensorflow.python.keras.engine import data_adapter\nfrom tensorflow.python.keras.engine import training_utils\nfrom tensorflow.python.keras.mixed_precision import loss_scale_optimizer as lso\nfrom tensorflow.python.keras.mixed_precision import policy\nfrom tensorflow.python.keras.saving import hdf5_format\nfrom tensorflow.python.keras.saving import save\nfrom tensorflow.python.keras.saving import saving_utils\nfrom tensorflow.python.keras.saving.saved_model import json_utils\nfrom tensorflow.python.keras.saving.saved_model import model_serialization\nfrom tensorflow.python.keras.utils import generic_utils\nfrom tensorflow.python.keras.utils import layer_utils\nfrom tensorflow.python.keras.utils import object_identity\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.keras.utils import version_utils\nfrom tensorflow.python.keras.utils.io_utils import ask_to_proceed_with_overwrite\nfrom tensorflow.python.keras.utils.io_utils import path_to_string\nfrom tensorflow.python.keras.utils.mode_keys import ModeKeys\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import summary_ops_v2\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.profiler import trace\nfrom tensorflow.python.saved_model import constants as sm_constants\nfrom tensorflow.python.saved_model import loader_impl as sm_loader\nfrom tensorflow.python.training import checkpoint_management\nfrom tensorflow.python.training import py_checkpoint_reader\nfrom tensorflow.python.training.tracking import base as trackable\nfrom tensorflow.python.training.tracking import graph_view as graph_view_lib\nfrom tensorflow.python.training.tracking import util as trackable_utils\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util import tf_decorator\nfrom tensorflow.python.util.tf_export import keras_export\nfrom tensorflow.tools.docs import doc_controls\n\n\n# pylint: disable=g-import-not-at-top\ntry:\n  import h5py\nexcept ImportError:\n  h5py = None\n\ntry:\n  import yaml\nexcept ImportError:\n  yaml = None\n# pylint: enable=g-import-not-at-top\n\n\ndef disable_multi_worker(method):\n  \"\"\"Decorator that disallows multi-worker use of `method`.\"\"\"\n\n  def _method_wrapper(self, *args, **kwargs):\n    if self._in_multi_worker_mode():  # pylint: disable=protected-access\n      raise ValueError('{} is not supported in multi-worker mode.'.format(\n          method.__name__))\n    return method(self, *args, **kwargs)\n\n  return tf_decorator.make_decorator(\n      target=method, decorator_func=_method_wrapper)\n\n\ndef inject_functional_model_class(cls):\n  \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\n  from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\n  from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top\n  if cls == Model or cls == training_v1.Model:\n    return functional.Functional\n  # In case there is any multiple inheritance, we stop injecting the\n  # class if keras model is not in its class hierarchy.\n  if cls == object:\n    return object\n\n  cls.__bases__ = tuple(inject_functional_model_class(base)\n                        for base in cls.__bases__)\n  # Trigger any `__new__` class swapping that needed to happen on `Functional`\n  # but did not because functional was not in the class hierarchy.\n  cls.__new__(cls)\n\n  return cls\n\n\ndef is_functional_model_init_params(args, kwargs):\n  return (len(args) == 2 or\n          len(args) == 1 and 'outputs' in kwargs or\n          'inputs' in kwargs and 'outputs' in kwargs)\n\n\n@keras_export('keras.Model', 'keras.models.Model')\nclass Model(base_layer.Layer, version_utils.ModelVersionSelector):\n  \"\"\"`Model` groups layers into an object with training and inference features.\n\n  Args:\n      inputs: The input(s) of the model: a `keras.Input` object or list of\n          `keras.Input` objects.\n      outputs: The output(s) of the model. See Functional API example below.\n      name: String, the name of the model.\n\n  There are two ways to instantiate a `Model`:\n\n  1 - With the \"Functional API\", where you start from `Input`,\n  you chain layer calls to specify the model's forward pass,\n  and finally you create your model from inputs and outputs:\n\n  ```python\n  import tensorflow as tf\n\n  inputs = tf.keras.Input(shape=(3,))\n  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  ```\n\n  Note: Only dicts, lists, and tuples of input tensors are supported. Nested\n  inputs are not supported (e.g. lists of list or dicts of dict).\n\n  2 - By subclassing the `Model` class: in that case, you should define your\n  layers in `__init__` and you should implement the model's forward pass\n  in `call`.\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n\n    def call(self, inputs):\n      x = self.dense1(inputs)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  If you subclass `Model`, you can optionally have\n  a `training` argument (boolean) in `call`, which you can use to specify\n  a different behavior in training and inference:\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n      self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n      x = self.dense1(inputs)\n      if training:\n        x = self.dropout(x, training=training)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  Once the model is created, you can config the model with losses and metrics\n  with `model.compile()`, train the model with `model.fit()`, or use the model\n  to do prediction with `model.predict()`.\n  \"\"\"\n  _TF_MODULE_IGNORED_PROPERTIES = frozenset(\n      itertools.chain(('_train_counter', '_test_counter', '_predict_counter',\n                       '_steps_per_execution'),\n                      base_layer.Layer._TF_MODULE_IGNORED_PROPERTIES))  # pylint: disable=protected-access\n\n  def __new__(cls, *args, **kwargs):\n    # Signature detection\n    if is_functional_model_init_params(args, kwargs) and cls == Model:\n      # Functional model\n      from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\n      return functional.Functional(skip_init=True, *args, **kwargs)\n    else:\n      return super(Model, cls).__new__(cls, *args, **kwargs)\n\n  @trackable.no_automatic_dependency_tracking\n  def __init__(self, *args, **kwargs):\n    self._is_model_for_instrumentation = True\n\n    # Special case for Subclassed Functional Model, which we couldn't detect\n    # when __new__ is called. We only realize it is a functional model when it\n    # calls super.__init__ with input and output tensor.\n    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\n    if (is_functional_model_init_params(args, kwargs) and\n        not isinstance(self, functional.Functional)):\n      # Filter the kwargs for multiple inheritance.\n      supported_kwargs = ['inputs', 'outputs', 'name', 'trainable', 'skip_init']\n      model_kwargs = {k: kwargs[k] for k in kwargs if k in supported_kwargs}\n      other_kwargs = {k: kwargs[k] for k in kwargs if k not in supported_kwargs}\n      inject_functional_model_class(self.__class__)\n      functional.Functional.__init__(self, *args, **model_kwargs)\n\n      # In case there is any multiple inheritance here, we need to call the\n      # __init__ for any class that appears after the Functional class.\n      clz_to_init = []\n      found_functional_class = False\n      for clz in self.__class__.__bases__:\n        if issubclass(clz, functional.Functional):\n          found_functional_class = True\n          continue\n        if found_functional_class:\n          clz_to_init.append(clz)\n\n      if clz_to_init:\n        for clz in clz_to_init:\n          clz.__init__(self, *args, **other_kwargs)\n      elif other_kwargs:\n        # In case there are unused kwargs, we should raise an error to user, in\n        # case they have a typo in the param name.\n        raise TypeError(\n            'The following keyword arguments aren\\'t supported: {}'.format(\n                other_kwargs))\n      return\n\n    # The following are implemented as property functions:\n    # self.trainable_weights\n    # self.non_trainable_weights\n    # `inputs` / `outputs` will only appear in kwargs if either are misspelled.\n    generic_utils.validate_kwargs(kwargs, {\n        'trainable', 'dtype', 'dynamic', 'name', 'autocast', 'inputs', 'outputs'\n    })\n    super(Model, self).__init__(**kwargs)\n    # By default, Model is a subclass model, which is not in graph network.\n    self._is_graph_network = False\n\n    self.inputs = None\n    self.outputs = None\n    self.input_names = None\n    self.output_names = None\n    # stop_training is used by callback to stop training when error happens\n    self.stop_training = False\n    self.history = None\n    # These objects are used in the default `Model.compile`. They are not\n    # guaranteed to be set after `Model.compile` is called, as users can\n    # override compile with custom logic.\n    self.compiled_loss = None\n    self.compiled_metrics = None\n\n    # This is True for Sequential networks and Functional networks.\n    self._compute_output_and_mask_jointly = False\n\n    # Don't reset compilation if already done. This may occur if calling\n    # `__init__` (or `_init_graph_network`) on an already-compiled model\n    # such as a Sequential model. Sequential models may need to rebuild\n    # themselves after compilation.\n    self._maybe_create_attribute('_is_compiled', False)\n    self._maybe_create_attribute('optimizer', None)\n\n    # Model must be created under scope of DistStrat it will be trained with.\n    if ds_context.has_strategy():\n      self._distribution_strategy = ds_context.get_strategy()\n    else:\n      self._distribution_strategy = None\n\n    self._cluster_coordinator = None\n\n    # Defaults to value of `tf.config.experimental_functions_run_eagerly`.\n    self._run_eagerly = None\n    # Initialize cache attrs.\n    self._reset_compile_cache()\n\n    # Fault-tolerance handler. Set in `ModelCheckpoint`.\n    self._training_state = None\n    self._saved_model_inputs_spec = None\n    self._trackable_saver = saver_with_op_caching(self)\n\n    self._steps_per_execution = None\n\n    self._init_batch_counters()\n    self._base_model_initialized = True\n\n  @trackable.no_automatic_dependency_tracking\n  def _init_batch_counters(self):\n    # Untracked Variables, used to keep track of mini-batches seen in `fit`,\n    # `evaluate`, and `predict`.\n    agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\n    self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)\n    self._predict_counter = variables.Variable(\n        0, dtype='int64', aggregation=agg)\n\n  def __setattr__(self, name, value):\n    if not getattr(self, '_self_setattr_tracking', True):\n      super(Model, self).__setattr__(name, value)\n      return\n\n    if all(\n        isinstance(v, (base_layer.Layer, variables.Variable)) or\n        base_layer_utils.has_weights(v) for v in nest.flatten(value)):\n      try:\n        self._base_model_initialized\n      except AttributeError:\n        raise RuntimeError(\n            'It looks like you are subclassing `Model` and you '\n            'forgot to call `super().__init__()`.'\n            ' Always start with this line.')\n\n    super(Model, self).__setattr__(name, value)\n\n  @generic_utils.default\n  def build(self, input_shape):\n    \"\"\"Builds the model based on input shapes received.\n\n    This is to be used for subclassed models, which do not know at instantiation\n    time what their inputs look like.\n\n    This method only exists for users who want to call `model.build()` in a\n    standalone way (as a substitute for calling the model on real data to\n    build it). It will never be called by the framework (and thus it will\n    never throw unexpected errors in an unrelated workflow).\n\n    Args:\n     input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n         shapes are tuples, integers, or TensorShapes.\n\n    Raises:\n      ValueError:\n        1. In case of invalid user-provided data (not of type tuple,\n           list, TensorShape, or dict).\n        2. If the model requires call arguments that are agnostic\n           to the input shapes (positional or kwarg in call signature).\n        3. If not all layers were properly built.\n        4. If float type inputs are not supported within the layers.\n\n      In each of these cases, the user should build their model by calling it\n      on real tensor data.\n    \"\"\"\n    if self._is_graph_network:\n      super(Model, self).build(input_shape)\n      return\n\n    if input_shape is None:\n      raise ValueError('Input shape must be defined when calling build on a '\n                       'model subclass network.')\n    valid_types = (tuple, list, tensor_shape.TensorShape, dict)\n    if not isinstance(input_shape, valid_types):\n      raise ValueError('Specified input shape is not one of the valid types. '\n                       'Please specify a batch input shape of type tuple or '\n                       'list of input shapes. User provided '\n                       'input type: {}'.format(type(input_shape)))\n\n    if input_shape and not self.inputs:\n      # We create placeholders for the `None`s in the shape and build the model\n      # in a Graph. Since tf.Variable is compatible with both eager execution\n      # and graph building, the variables created after building the model in\n      # a Graph are still valid when executing eagerly.\n      if context.executing_eagerly():\n        graph = func_graph.FuncGraph('build_graph')\n      else:\n        graph = backend.get_graph()\n      with graph.as_default():\n        if (isinstance(input_shape, list) and\n            all(d is None or isinstance(d, int) for d in input_shape)):\n          input_shape = tuple(input_shape)\n        if isinstance(input_shape, list):\n          x = [base_layer_utils.generate_placeholders_from_shape(shape)\n               for shape in input_shape]\n        elif isinstance(input_shape, dict):\n          x = {\n              k: base_layer_utils.generate_placeholders_from_shape(shape)\n              for k, shape in input_shape.items()\n          }\n        else:\n          x = base_layer_utils.generate_placeholders_from_shape(input_shape)\n\n        kwargs = {}\n        call_signature = self._call_full_argspec\n        call_args = call_signature.args\n        # Exclude `self`, `inputs`, and any argument with a default value.\n        if len(call_args) > 2:\n          if call_signature.defaults:\n            call_args = call_args[2:-len(call_signature.defaults)]\n          else:\n            call_args = call_args[2:]\n          for arg in call_args:\n            if arg == 'training':\n              # Case where `training` is a positional arg with no default.\n              kwargs['training'] = False\n            else:\n              # Has invalid call signature with unknown positional arguments.\n              raise ValueError(\n                  'Currently, you cannot build your model if it has '\n                  'positional or keyword arguments that are not '\n                  'inputs to the model, but are required for its '\n                  '`call` method. Instead, in order to instantiate '\n                  'and build your model, `call` your model on real '\n                  'tensor data with all expected call arguments.')\n        elif len(call_args) < 2:\n          # Signature without `inputs`.\n          raise ValueError('You can only call `build` on a model if its `call` '\n                           'method accepts an `inputs` argument.')\n        try:\n          self.call(x, **kwargs)\n        except (errors.InvalidArgumentError, TypeError):\n          raise ValueError('You cannot build your model by calling `build` '\n                           'if your layers do not support float type inputs. '\n                           'Instead, in order to instantiate and build your '\n                           'model, `call` your model on real tensor data (of '\n                           'the correct dtype).')\n    super(Model, self).build(input_shape)\n\n  @doc_controls.doc_in_current_and_subclasses\n  def call(self, inputs, training=None, mask=None):\n    \"\"\"Calls the model on new inputs.\n\n    In this case `call` just reapplies\n    all ops in the graph to the new inputs\n    (e.g. build a new computational graph from the provided inputs).\n\n    Note: This method should not be called directly. It is only meant to be\n    overridden when subclassing `tf.keras.Model`.\n    To call a model on an input, always use the `__call__` method,\n    i.e. `model(inputs)`, which relies on the underlying `call` method.\n\n    Args:\n        inputs: Input tensor, or dict/list/tuple of input tensors.\n        training: Boolean or boolean scalar tensor, indicating whether to run\n          the `Network` in training mode or inference mode.\n        mask: A mask or list of masks. A mask can be\n            either a tensor or None (no mask).\n\n    Returns:\n        A tensor if there is a single output, or\n        a list of tensors if there are more than one outputs.\n    \"\"\"\n    raise NotImplementedError('When subclassing the `Model` class, you should '\n                              'implement a `call` method.')\n\n  def compile(self,\n              optimizer='rmsprop',\n              loss=None,\n              metrics=None,\n              loss_weights=None,\n              weighted_metrics=None,\n              run_eagerly=None,\n              steps_per_execution=None,\n              **kwargs):\n    \"\"\"Configures the model for training.\n\n    Args:\n        optimizer: String (name of optimizer) or optimizer instance. See\n          `tf.keras.optimizers`.\n        loss: String (name of objective function), objective function or\n          `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n          function is any callable with the signature `loss = fn(y_true,\n          y_pred)`, where y_true = ground truth values with shape =\n          `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n          categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n          y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n          returns a weighted loss float tensor. If a custom `Loss` instance is\n          used and reduction is set to `None`, return value has the shape\n          `[batch_size, d0, .. dN-1]` i.e. per-sample or per-timestep loss\n          values; otherwise, it is a scalar. If the model has multiple outputs,\n          you can use a different loss on each output by passing a dictionary\n          or a list of losses. The loss value that will be minimized by the\n          model will then be the sum of all individual losses, unless\n          `loss_weights` is specified.\n        metrics: List of metrics to be evaluated by the model during training\n          and testing. Each of this can be a string (name of a built-in\n          function), function or a `tf.keras.metrics.Metric` instance. See\n          `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n          function is any callable with the signature `result = fn(y_true,\n          y_pred)`. To specify different metrics for different outputs of a\n          multi-output model, you could also pass a dictionary, such as\n          `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n          You can also pass a list to specify a metric or a list of metrics\n          for each output, such as `metrics=[['accuracy'], ['accuracy', 'mse']]`\n          or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n          strings 'accuracy' or 'acc', we convert this to one of\n          `tf.keras.metrics.BinaryAccuracy`,\n          `tf.keras.metrics.CategoricalAccuracy`,\n          `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n          function used and the model output shape. We do a similar\n          conversion for the strings 'crossentropy' and 'ce' as well.\n        loss_weights: Optional list or dictionary specifying scalar coefficients\n          (Python floats) to weight the loss contributions of different model\n          outputs. The loss value that will be minimized by the model will then\n          be the *weighted sum* of all individual losses, weighted by the\n          `loss_weights` coefficients.\n            If a list, it is expected to have a 1:1 mapping to the model's\n              outputs. If a dict, it is expected to map output names (strings)\n              to scalar coefficients.\n        weighted_metrics: List of metrics to be evaluated and weighted by\n          `sample_weight` or `class_weight` during training and testing.\n        run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n          logic will not be wrapped in a `tf.function`. Recommended to leave\n          this as `None` unless your `Model` cannot be run inside a\n          `tf.function`. `run_eagerly=True` is not supported when using\n          `tf.distribute.experimental.ParameterServerStrategy`.\n        steps_per_execution: Int. Defaults to 1. The number of batches to\n          run during each `tf.function` call. Running multiple batches\n          inside a single `tf.function` call can greatly improve performance\n          on TPUs or small models with a large Python overhead.\n          At most, one full epoch will be run each\n          execution. If a number larger than the size of the epoch is passed,\n          the execution will be truncated to the size of the epoch.\n          Note that if `steps_per_execution` is set to `N`,\n          `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n          will only be called every `N` batches\n          (i.e. before/after each `tf.function` execution).\n        **kwargs: Arguments supported for backwards compatibility only.\n\n    Raises:\n        ValueError: In case of invalid arguments for\n            `optimizer`, `loss` or `metrics`.\n    \"\"\"\n    with self.distribute_strategy.scope():\n      if 'experimental_steps_per_execution' in kwargs:\n        logging.warning('The argument `steps_per_execution` is no longer '\n                        'experimental. Pass `steps_per_execution` instead of '\n                        '`experimental_steps_per_execution`.')\n        if not steps_per_execution:\n          steps_per_execution = kwargs.pop('experimental_steps_per_execution')\n\n      # When compiling from an already-serialized model, we do not want to\n      # reapply some processing steps (e.g. metric renaming for multi-output\n      # models, which have prefixes added for each corresponding output name).\n      from_serialized = kwargs.pop('from_serialized', False)\n\n      self._validate_compile(optimizer, metrics, **kwargs)\n      self._run_eagerly = run_eagerly\n\n      self.optimizer = self._get_optimizer(optimizer)\n      self.compiled_loss = compile_utils.LossesContainer(\n          loss, loss_weights, output_names=self.output_names)\n      self.compiled_metrics = compile_utils.MetricsContainer(\n          metrics, weighted_metrics, output_names=self.output_names,\n          from_serialized=from_serialized)\n\n      self._configure_steps_per_execution(steps_per_execution or 1)\n\n      # Initializes attrs that are reset each time `compile` is called.\n      self._reset_compile_cache()\n      self._is_compiled = True\n\n      self.loss = loss or {}  # Backwards compat.\n\n  def _get_optimizer(self, optimizer):\n    \"\"\"Wraps `optimizer` in `LossScaleOptimizer` if necessary.\"\"\"\n    # The deprecated PolicyV1 has a loss_scale, which we use for backwards\n    # compatibility to match TF 2.3 behavior. The new Policy does not have a\n    # loss_scale, so we use dynamic loss scaling if the mixed_float16 policy is\n    # used.\n    if isinstance(self._dtype_policy, policy.PolicyV1):\n      loss_scale = self._dtype_policy.loss_scale\n    elif self._dtype_policy.name == 'mixed_float16':\n      loss_scale = 'dynamic'\n    else:\n      loss_scale = None\n\n    def _get_single_optimizer(opt):\n      opt = optimizers.get(opt)\n      if (loss_scale is not None and\n          not isinstance(opt, lso.LossScaleOptimizer)):\n        if loss_scale == 'dynamic':\n          opt = lso.LossScaleOptimizer(opt)\n        else:\n          opt = lso.LossScaleOptimizerV1(opt, loss_scale)\n      return opt\n\n    return nest.map_structure(_get_single_optimizer, optimizer)\n\n  @trackable.no_automatic_dependency_tracking\n  def _reset_compile_cache(self):\n    self.train_function = None\n    self.test_function = None\n    self.predict_function = None\n    # Used to cache the `tf.function`'ed `train_function` to be logged in\n    # TensorBoard, since the original `train_function` is not necessarily\n    # a `tf.function` (e.g., with ParameterServerStrategy, the `train_function`\n    # is a scheduling of the actual training function to a remote worker).\n    self.train_tf_function = None\n\n    # Used to cache `trainable` attr of `Layer`s for `fit`.\n    self._compiled_trainable_state = self._get_trainable_state()\n\n  @trackable.no_automatic_dependency_tracking\n  def _configure_steps_per_execution(self, steps_per_execution):\n    self._steps_per_execution = variables.Variable(\n        steps_per_execution,\n        dtype='int64',\n        aggregation=variables.VariableAggregationV2.ONLY_FIRST_REPLICA)\n\n  @property\n  def _should_compute_mask(self):\n    return False\n\n  @property\n  def metrics(self):\n    \"\"\"Returns the model's metrics added using `compile`, `add_metric` APIs.\n\n    Note: Metrics passed to `compile()` are available only after a `keras.Model`\n    has been trained/evaluated on actual data.\n\n    Examples:\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> outputs = tf.keras.layers.Dense(2)(inputs)\n    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n    >>> [m.name for m in model.metrics]\n    []\n\n    >>> x = np.random.random((2, 3))\n    >>> y = np.random.randint(0, 2, (2, 2))\n    >>> model.fit(x, y)\n    >>> [m.name for m in model.metrics]\n    ['loss', 'mae']\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> d = tf.keras.layers.Dense(2, name='out')\n    >>> output_1 = d(inputs)\n    >>> output_2 = d(inputs)\n    >>> model = tf.keras.models.Model(\n    ...    inputs=inputs, outputs=[output_1, output_2])\n    >>> model.add_metric(\n    ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n    >>> model.fit(x, (y, y))\n    >>> [m.name for m in model.metrics]\n    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n    'out_1_acc', 'mean']\n\n    \"\"\"\n    metrics = []\n    if self._is_compiled:\n      # TODO(omalleyt): Track `LossesContainer` and `MetricsContainer` objects\n      # so that attr names are not load-bearing.\n      if self.compiled_loss is not None:\n        metrics += self.compiled_loss.metrics\n      if self.compiled_metrics is not None:\n        metrics += self.compiled_metrics.metrics\n\n    for l in self._flatten_layers():\n      metrics.extend(l._metrics)  # pylint: disable=protected-access\n    return metrics\n\n  @property\n  def metrics_names(self):\n    \"\"\"Returns the model's display labels for all outputs.\n\n    Note: `metrics_names` are available only after a `keras.Model` has been\n    trained/evaluated on actual data.\n\n    Examples:\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> outputs = tf.keras.layers.Dense(2)(inputs)\n    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n    >>> model.metrics_names\n    []\n\n    >>> x = np.random.random((2, 3))\n    >>> y = np.random.randint(0, 2, (2, 2))\n    >>> model.fit(x, y)\n    >>> model.metrics_names\n    ['loss', 'mae']\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> d = tf.keras.layers.Dense(2, name='out')\n    >>> output_1 = d(inputs)\n    >>> output_2 = d(inputs)\n    >>> model = tf.keras.models.Model(\n    ...    inputs=inputs, outputs=[output_1, output_2])\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n    >>> model.fit(x, (y, y))\n    >>> model.metrics_names\n    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n    'out_1_acc']\n\n    \"\"\"\n\n    # This property includes all output names including `loss` and per-output\n    # losses for backward compatibility.\n    return [m.name for m in self.metrics]\n\n  @property\n  def distribute_strategy(self):\n    \"\"\"The `tf.distribute.Strategy` this model was created under.\"\"\"\n    return self._distribution_strategy or ds_context.get_strategy()\n\n  @property\n  def run_eagerly(self):\n    \"\"\"Settable attribute indicating whether the model should run eagerly.\n\n    Running eagerly means that your model will be run step by step,\n    like Python code. Your model might run slower, but it should become easier\n    for you to debug it by stepping into individual layer calls.\n\n    By default, we will attempt to compile your model to a static graph to\n    deliver the best execution performance.\n\n    Returns:\n      Boolean, whether the model should run eagerly.\n    \"\"\"\n    if self.dynamic and self._run_eagerly is False:  # pylint:disable=g-bool-id-comparison\n      # TODO(fchollet): consider using py_func to enable this.\n      raise ValueError('Your model contains layers that can only be '\n                       'successfully run in eager execution (layers '\n                       'constructed with `dynamic=True`). '\n                       'You cannot set `run_eagerly=False`.')\n\n    if self._cluster_coordinator and self._run_eagerly:\n      raise ValueError('When using `Model` with `ParameterServerStrategy`, '\n                       '`run_eagerly` is not supported.')\n\n    # Run eagerly logic, by priority:\n    # (1) Dynamic models must be run eagerly.\n    # (2) Explicitly setting run_eagerly causes a Model to be run eagerly.\n    # (3) Not explicitly setting run_eagerly defaults to TF's global setting.\n    return (self.dynamic or self._run_eagerly or\n            (def_function.functions_run_eagerly() and\n             self._run_eagerly is None))\n\n  @run_eagerly.setter\n  def run_eagerly(self, value):\n    self._run_eagerly = value\n\n  def train_step(self, data):\n    \"\"\"The logic for one training step.\n\n    This method can be overridden to support custom training logic.\n    For concrete examples of how to override this method see\n    [Customizing what happends in fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n    This method is called by `Model.make_train_function`.\n\n    This method should contain the mathematical logic for one step of training.\n    This typically includes the forward pass, loss calculation, backpropagation,\n    and metric updates.\n\n    Configuration details for *how* this logic is run (e.g. `tf.function` and\n    `tf.distribute.Strategy` settings), should be left to\n    `Model.make_train_function`, which can also be overridden.\n\n    Args:\n      data: A nested structure of `Tensor`s.\n\n    Returns:\n      A `dict` containing values that will be passed to\n      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n      values of the `Model`'s metrics are returned. Example:\n      `{'loss': 0.2, 'accuracy': 0.7}`.\n\n    \"\"\"\n    # These are the only transformations `Model.fit` applies to user-input\n    # data when a `tf.data.Dataset` is provided.\n    data = data_adapter.expand_1d(data)\n    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n    # Run forward pass.\n    with backprop.GradientTape() as tape:\n      y_pred = self(x, training=True)\n      loss = self.compiled_loss(\n          y, y_pred, sample_weight, regularization_losses=self.losses)\n    # Run backwards pass.\n    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    # Collect metrics to return\n    return_metrics = {}\n    for metric in self.metrics:\n      result = metric.result()\n      if isinstance(result, dict):\n        return_metrics.update(result)\n      else:\n        return_metrics[metric.name] = result\n    return return_metrics\n\n  def make_train_function(self):\n    \"\"\"Creates a function that executes one step of training.\n\n    This method can be overridden to support custom training logic.\n    This method is called by `Model.fit` and `Model.train_on_batch`.\n\n    Typically, this method directly controls `tf.function` and\n    `tf.distribute.Strategy` settings, and delegates the actual training\n    logic to `Model.train_step`.\n\n    This function is cached the first time `Model.fit` or\n    `Model.train_on_batch` is called. The cache is cleared whenever\n    `Model.compile` is called.\n\n    Returns:\n      Function. The function created by this method should accept a\n      `tf.data.Iterator`, and return a `dict` containing values that will\n      be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n      `{'loss': 0.2, 'accuracy': 0.7}`.\n    \"\"\"\n    if self.train_function is not None:\n      return self.train_function\n\n    def step_function(model, iterator):\n      \"\"\"Runs a single training step.\"\"\"\n\n      def run_step(data):\n        outputs = model.train_step(data)\n        # Ensure counter is updated only if `train_step` succeeds.\n        with ops.control_dependencies(_minimum_control_deps(outputs)):\n          model._train_counter.assign_add(1)  # pylint: disable=protected-access\n        return outputs\n\n      data = next(iterator)\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n      outputs = reduce_per_replica(\n          outputs, self.distribute_strategy, reduction='first')\n      write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access\n      return outputs\n\n    if self._steps_per_execution.numpy().item() == 1:\n\n      def train_function(iterator):\n        \"\"\"Runs a training execution with one step.\"\"\"\n        return step_function(self, iterator)\n\n    else:\n\n      def train_function(iterator):\n        \"\"\"Runs a training execution with multiple steps.\"\"\"\n        for _ in math_ops.range(self._steps_per_execution):\n          outputs = step_function(self, iterator)\n        return outputs\n\n    if not self.run_eagerly:\n      train_function = def_function.function(\n          train_function, experimental_relax_shapes=True)\n      self.train_tf_function = train_function\n\n    self.train_function = train_function\n\n    if self._cluster_coordinator:\n      self.train_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n          train_function, args=(iterator,))\n\n    return self.train_function\n\n  def fit(self,\n          x=None,\n          y=None,\n          batch_size=None,\n          epochs=1,\n          verbose='auto',\n          callbacks=None,\n          validation_split=0.,\n          validation_data=None,\n          shuffle=True,\n          class_weight=None,\n          sample_weight=None,\n          initial_epoch=0,\n          steps_per_epoch=None,\n          validation_steps=None,\n          validation_batch_size=None,\n          validation_freq=1,\n          max_queue_size=10,\n          workers=1,\n          use_multiprocessing=False):\n    \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays\n            (in case the model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors\n            (in case the model has multiple inputs).\n          - A dict mapping input names to the corresponding array/tensors,\n            if the model has named inputs.\n          - A `tf.data` dataset. Should return a tuple\n            of either `(inputs, targets)` or\n            `(inputs, targets, sample_weights)`.\n          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n            or `(inputs, targets, sample_weights)`.\n          - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n            callable that takes a single argument of type\n            `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n            `DatasetCreator` should be used when users prefer to specify the\n            per-replica batching and sharding logic for the `Dataset`.\n            See `tf.keras.utils.experimental.DatasetCreator` doc for more\n            information.\n          A more detailed description of unpacking behavior for iterator types\n          (Dataset, generator, Sequence) is given below. If using\n          `tf.distribute.experimental.ParameterServerStrategy`, only\n          `DatasetCreator` type is supported for `x`.\n        y: Target data. Like the input data `x`,\n          it could be either Numpy array(s) or TensorFlow tensor(s).\n          It should be consistent with `x` (you cannot have Numpy inputs and\n          tensor targets, or inversely). If `x` is a dataset, generator,\n          or `keras.utils.Sequence` instance, `y` should\n          not be specified (since targets will be obtained from `x`).\n        batch_size: Integer or `None`.\n            Number of samples per gradient update.\n            If unspecified, `batch_size` will default to 32.\n            Do not specify the `batch_size` if your data is in the\n            form of datasets, generators, or `keras.utils.Sequence` instances\n            (since they generate batches).\n        epochs: Integer. Number of epochs to train the model.\n            An epoch is an iteration over the entire `x` and `y`\n            data provided.\n            Note that in conjunction with `initial_epoch`,\n            `epochs` is to be understood as \"final epoch\".\n            The model is not trained for a number of iterations\n            given by `epochs`, but merely until the epoch\n            of index `epochs` is reached.\n        verbose: 'auto', 0, 1, or 2. Verbosity mode.\n            0 = silent, 1 = progress bar, 2 = one line per epoch.\n            'auto' defaults to 1 for most cases, but 2 when used with\n            `ParameterServerStrategy`. Note that the progress bar is not\n            particularly useful when logged to a file, so verbose=2 is\n            recommended when not running interactively (eg, in a production\n            environment).\n        callbacks: List of `keras.callbacks.Callback` instances.\n            List of callbacks to apply during training.\n            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n            and `tf.keras.callbacks.History` callbacks are created automatically\n            and need not be passed into `model.fit`.\n            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n            `verbose` argument to `model.fit`.\n            Callbacks with batch-level calls are currently unsupported with\n            `tf.distribute.experimental.ParameterServerStrategy`, and users are\n            advised to implement epoch-level calls instead with an appropriate\n            `steps_per_epoch` value.\n        validation_split: Float between 0 and 1.\n            Fraction of the training data to be used as validation data.\n            The model will set apart this fraction of the training data,\n            will not train on it, and will evaluate\n            the loss and any model metrics\n            on this data at the end of each epoch.\n            The validation data is selected from the last samples\n            in the `x` and `y` data provided, before shuffling. This argument is\n            not supported when `x` is a dataset, generator or\n           `keras.utils.Sequence` instance.\n            `validation_split` is not yet supported with\n            `tf.distribute.experimental.ParameterServerStrategy`.\n        validation_data: Data on which to evaluate\n            the loss and any model metrics at the end of each epoch.\n            The model will not be trained on this data. Thus, note the fact\n            that the validation loss of data provided using `validation_split`\n            or `validation_data` is not affected by regularization layers like\n            noise and dropout.\n            `validation_data` will override `validation_split`.\n            `validation_data` could be:\n              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n              - A `tf.data.Dataset`.\n              - A Python generator or `keras.utils.Sequence` returning\n              `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n            `validation_data` is not yet supported with\n            `tf.distribute.experimental.ParameterServerStrategy`.\n        shuffle: Boolean (whether to shuffle the training data\n            before each epoch) or str (for 'batch'). This argument is ignored\n            when `x` is a generator or an object of tf.data.Dataset.\n            'batch' is a special option for dealing\n            with the limitations of HDF5 data; it shuffles in batch-sized\n            chunks. Has no effect when `steps_per_epoch` is not `None`.\n        class_weight: Optional dictionary mapping class indices (integers)\n            to a weight (float) value, used for weighting the loss function\n            (during training only).\n            This can be useful to tell the model to\n            \"pay more attention\" to samples from\n            an under-represented class.\n        sample_weight: Optional Numpy array of weights for\n            the training samples, used for weighting the loss function\n            (during training only). You can either pass a flat (1D)\n            Numpy array with the same length as the input samples\n            (1:1 mapping between weights and samples),\n            or in the case of temporal data,\n            you can pass a 2D array with shape\n            `(samples, sequence_length)`,\n            to apply a different weight to every timestep of every sample. This\n            argument is not supported when `x` is a dataset, generator, or\n           `keras.utils.Sequence` instance, instead provide the sample_weights\n            as the third element of `x`.\n        initial_epoch: Integer.\n            Epoch at which to start training\n            (useful for resuming a previous training run).\n        steps_per_epoch: Integer or `None`.\n            Total number of steps (batches of samples)\n            before declaring one epoch finished and starting the\n            next epoch. When training with input tensors such as\n            TensorFlow data tensors, the default `None` is equal to\n            the number of samples in your dataset divided by\n            the batch size, or 1 if that cannot be determined. If x is a\n            `tf.data` dataset, and 'steps_per_epoch'\n            is None, the epoch will run until the input dataset is exhausted.\n            When passing an infinitely repeating dataset, you must specify the\n            `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n            will run indefinitely with an infinitely repeating dataset.\n            This argument is not supported with array inputs.\n            When using `tf.distribute.experimental.ParameterServerStrategy`:\n              * `steps_per_epoch=None` is not supported.\n        validation_steps: Only relevant if `validation_data` is provided and\n            is a `tf.data` dataset. Total number of steps (batches of\n            samples) to draw before stopping when performing validation\n            at the end of every epoch. If 'validation_steps' is None, validation\n            will run until the `validation_data` dataset is exhausted. In the\n            case of an infinitely repeated dataset, it will run into an\n            infinite loop. If 'validation_steps' is specified and only part of\n            the dataset will be consumed, the evaluation will start from the\n            beginning of the dataset at each epoch. This ensures that the same\n            validation samples are used every time.\n        validation_batch_size: Integer or `None`.\n            Number of samples per validation batch.\n            If unspecified, will default to `batch_size`.\n            Do not specify the `validation_batch_size` if your data is in the\n            form of datasets, generators, or `keras.utils.Sequence` instances\n            (since they generate batches).\n        validation_freq: Only relevant if validation data is provided. Integer\n            or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n            If an integer, specifies how many training epochs to run before a\n            new validation run is performed, e.g. `validation_freq=2` runs\n            validation every 2 epochs. If a Container, specifies the epochs on\n            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n            validation at the end of the 1st, 2nd, and 10th epochs.\n        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n            input only. Maximum size for the generator queue.\n            If unspecified, `max_queue_size` will default to 10.\n        workers: Integer. Used for generator or `keras.utils.Sequence` input\n            only. Maximum number of processes to spin up\n            when using process-based threading. If unspecified, `workers`\n            will default to 1.\n        use_multiprocessing: Boolean. Used for generator or\n            `keras.utils.Sequence` input only. If `True`, use process-based\n            threading. If unspecified, `use_multiprocessing` will default to\n            `False`. Note that because this implementation relies on\n            multiprocessing, you should not pass non-picklable arguments to\n            the generator as they can't be passed easily to children processes.\n\n    Unpacking behavior for iterator-like inputs:\n        A common pattern is to pass a tf.data.Dataset, generator, or\n      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n      yield not only features (x) but optionally targets (y) and sample weights.\n      Keras requires that the output of such iterator-likes be unambiguous. The\n      iterator should return a tuple of length 1, 2, or 3, where the optional\n      second and third elements will be used for y and sample_weight\n      respectively. Any other type provided will be wrapped in a length one\n      tuple, effectively treating everything as 'x'. When yielding dicts, they\n      should still adhere to the top-level tuple structure.\n      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n      features, targets, and weights from the keys of a single dict.\n        A notable unsupported data type is the namedtuple. The reason is that\n      it behaves like both an ordered datatype (tuple) and a mapping\n      datatype (dict). So given a namedtuple of the form:\n          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n      it is ambiguous whether to reverse the order of the elements when\n      interpreting the value. Even worse is a tuple of the form:\n          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n      where it is unclear if the tuple was intended to be unpacked into x, y,\n      and sample_weight or passed through as a single element to `x`. As a\n      result the data processing code will simply raise a ValueError if it\n      encounters a namedtuple. (Along with instructions to remedy the issue.)\n\n    Returns:\n        A `History` object. Its `History.history` attribute is\n        a record of training loss values and metrics values\n        at successive epochs, as well as validation loss values\n        and validation metrics values (if applicable).\n\n    Raises:\n        RuntimeError: 1. If the model was never compiled or,\n        2. If `model.fit` is  wrapped in `tf.function`.\n\n        ValueError: In case of mismatch between the provided input data\n            and what the model expects or when the input data is empty.\n    \"\"\"\n    # Legacy graph support is contained in `training_v1.Model`.\n    version_utils.disallow_legacy_graph('Model', 'fit')\n    self._assert_compile_was_called()\n    self._check_call_args('fit')\n    _disallow_inside_tf_function('fit')\n\n    if verbose == 'auto':\n      if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n        verbose = 2  # Default to epoch-level logging for PSStrategy.\n      else:\n        verbose = 1  # Default to batch-level logging otherwise.\n\n    if validation_split:\n      # Create the validation data using the training data. Only supported for\n      # `Tensor` and `NumPy` input.\n      (x, y, sample_weight), validation_data = (\n          data_adapter.train_validation_split(\n              (x, y, sample_weight), validation_split=validation_split))\n\n    if validation_data:\n      val_x, val_y, val_sample_weight = (\n          data_adapter.unpack_x_y_sample_weight(validation_data))\n\n    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n      self._cluster_coordinator = cluster_coordinator.ClusterCoordinator(\n          self.distribute_strategy)\n\n    with self.distribute_strategy.scope(), \\\n         training_utils.RespectCompiledTrainableState(self):\n      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n      data_handler = data_adapter.get_data_handler(\n          x=x,\n          y=y,\n          sample_weight=sample_weight,\n          batch_size=batch_size,\n          steps_per_epoch=steps_per_epoch,\n          initial_epoch=initial_epoch,\n          epochs=epochs,\n          shuffle=shuffle,\n          class_weight=class_weight,\n          max_queue_size=max_queue_size,\n          workers=workers,\n          use_multiprocessing=use_multiprocessing,\n          model=self,\n          steps_per_execution=self._steps_per_execution)\n\n      # Container that configures and calls `tf.keras.Callback`s.\n      if not isinstance(callbacks, callbacks_module.CallbackList):\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=True,\n            add_progbar=verbose != 0,\n            model=self,\n            verbose=verbose,\n            epochs=epochs,\n            steps=data_handler.inferred_steps)\n\n      self.stop_training = False\n      self.train_function = self.make_train_function()\n      self._train_counter.assign(0)\n      callbacks.on_train_begin()\n      training_logs = None\n      # Handle fault-tolerance for multi-worker.\n      # TODO(omalleyt): Fix the ordering issues that mean this has to\n      # happen after `callbacks.on_train_begin`.\n      data_handler._initial_epoch = (  # pylint: disable=protected-access\n          self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n      logs = None\n      for epoch, iterator in data_handler.enumerate_epochs():\n        self.reset_metrics()\n        callbacks.on_epoch_begin(epoch)\n        with data_handler.catch_stop_iteration():\n          for step in data_handler.steps():\n            with trace.Trace(\n                'train',\n                epoch_num=epoch,\n                step_num=step,\n                batch_size=batch_size,\n                _r=1):\n              callbacks.on_train_batch_begin(step)\n              tmp_logs = self.train_function(iterator)\n              if data_handler.should_sync:\n                context.async_wait()\n              logs = tmp_logs  # No error, now safe to assign to logs.\n              end_step = step + data_handler.step_increment\n              callbacks.on_train_batch_end(end_step, logs)\n              if self.stop_training:\n                break\n\n        logs = tf_utils.sync_to_numpy_or_python_type(logs)\n        if logs is None:\n          raise ValueError('Expect x to be a non-empty array or dataset.')\n        epoch_logs = copy.copy(logs)\n\n        # Run validation.\n        if validation_data and self._should_eval(epoch, validation_freq):\n          # Create data_handler for evaluation and cache it.\n          if getattr(self, '_eval_data_handler', None) is None:\n            self._eval_data_handler = data_adapter.get_data_handler(\n                x=val_x,\n                y=val_y,\n                sample_weight=val_sample_weight,\n                batch_size=validation_batch_size or batch_size,\n                steps_per_epoch=validation_steps,\n                initial_epoch=0,\n                epochs=1,\n                max_queue_size=max_queue_size,\n                workers=workers,\n                use_multiprocessing=use_multiprocessing,\n                model=self,\n                steps_per_execution=self._steps_per_execution)\n          val_logs = self.evaluate(\n              x=val_x,\n              y=val_y,\n              sample_weight=val_sample_weight,\n              batch_size=validation_batch_size or batch_size,\n              steps=validation_steps,\n              callbacks=callbacks,\n              max_queue_size=max_queue_size,\n              workers=workers,\n              use_multiprocessing=use_multiprocessing,\n              return_dict=True,\n              _use_cached_eval_dataset=True)\n          val_logs = {'val_' + name: val for name, val in val_logs.items()}\n          epoch_logs.update(val_logs)\n\n        callbacks.on_epoch_end(epoch, epoch_logs)\n        training_logs = epoch_logs\n        if self.stop_training:\n          break\n\n      # If eval data_hanlder exists, delete it after all epochs are done.\n      if getattr(self, '_eval_data_handler', None) is not None:\n        del self._eval_data_handler\n      callbacks.on_train_end(logs=training_logs)\n      return self.history\n\n  def test_step(self, data):\n    \"\"\"The logic for one evaluation step.\n\n    This method can be overridden to support custom evaluation logic.\n    This method is called by `Model.make_test_function`.\n\n    This function should contain the mathematical logic for one step of\n    evaluation.\n    This typically includes the forward pass, loss calculation, and metrics\n    updates.\n\n    Configuration details for *how* this logic is run (e.g. `tf.function` and\n    `tf.distribute.Strategy` settings), should be left to\n    `Model.make_test_function`, which can also be overridden.\n\n    Args:\n      data: A nested structure of `Tensor`s.\n\n    Returns:\n      A `dict` containing values that will be passed to\n      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n      values of the `Model`'s metrics are returned.\n    \"\"\"\n    data = data_adapter.expand_1d(data)\n    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n\n    y_pred = self(x, training=False)\n    # Updates stateful loss metrics.\n    self.compiled_loss(\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    # Collect metrics to return\n    return_metrics = {}\n    for metric in self.metrics:\n      result = metric.result()\n      if isinstance(result, dict):\n        return_metrics.update(result)\n      else:\n        return_metrics[metric.name] = result\n    return return_metrics\n\n  def make_test_function(self):\n    \"\"\"Creates a function that executes one step of evaluation.\n\n    This method can be overridden to support custom evaluation logic.\n    This method is called by `Model.evaluate` and `Model.test_on_batch`.\n\n    Typically, this method directly controls `tf.function` and\n    `tf.distribute.Strategy` settings, and delegates the actual evaluation\n    logic to `Model.test_step`.\n\n    This function is cached the first time `Model.evaluate` or\n    `Model.test_on_batch` is called. The cache is cleared whenever\n    `Model.compile` is called.\n\n    Returns:\n      Function. The function created by this method should accept a\n      `tf.data.Iterator`, and return a `dict` containing values that will\n      be passed to `tf.keras.Callbacks.on_test_batch_end`.\n    \"\"\"\n    if self.test_function is not None:\n      return self.test_function\n\n    def step_function(model, iterator):\n      \"\"\"Runs a single evaluation step.\"\"\"\n\n      def run_step(data):\n        outputs = model.test_step(data)\n        # Ensure counter is updated only if `test_step` succeeds.\n        with ops.control_dependencies(_minimum_control_deps(outputs)):\n          model._test_counter.assign_add(1)  # pylint: disable=protected-access\n        return outputs\n\n      data = next(iterator)\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n      outputs = reduce_per_replica(\n          outputs, self.distribute_strategy, reduction='first')\n      return outputs\n\n    if self._steps_per_execution.numpy().item() == 1:\n\n      def test_function(iterator):\n        \"\"\"Runs an evaluation execution with one step.\"\"\"\n        return step_function(self, iterator)\n\n    else:\n\n      def test_function(iterator):\n        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"\n        for _ in math_ops.range(self._steps_per_execution):\n          outputs = step_function(self, iterator)\n        return outputs\n\n    if not self.run_eagerly:\n      test_function = def_function.function(\n          test_function, experimental_relax_shapes=True)\n\n    self.test_function = test_function\n\n    if self._cluster_coordinator:\n      self.test_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n          test_function, args=(iterator,))\n\n    return self.test_function\n\n  def evaluate(self,\n               x=None,\n               y=None,\n               batch_size=None,\n               verbose=1,\n               sample_weight=None,\n               steps=None,\n               callbacks=None,\n               max_queue_size=10,\n               workers=1,\n               use_multiprocessing=False,\n               return_dict=False,\n               **kwargs):\n    \"\"\"Returns the loss value & metrics values for the model in test mode.\n\n    Computation is done in batches (see the `batch_size` arg.)\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays\n            (in case the model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors\n            (in case the model has multiple inputs).\n          - A dict mapping input names to the corresponding array/tensors,\n            if the model has named inputs.\n          - A `tf.data` dataset. Should return a tuple\n            of either `(inputs, targets)` or\n            `(inputs, targets, sample_weights)`.\n          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n            or `(inputs, targets, sample_weights)`.\n          A more detailed description of unpacking behavior for iterator types\n          (Dataset, generator, Sequence) is given in the `Unpacking behavior\n          for iterator-like inputs` section of `Model.fit`.\n        y: Target data. Like the input data `x`, it could be either Numpy\n          array(s) or TensorFlow tensor(s). It should be consistent with `x`\n          (you cannot have Numpy inputs and tensor targets, or inversely). If\n          `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n          should not be specified (since targets will be obtained from the\n          iterator/dataset).\n        batch_size: Integer or `None`. Number of samples per batch of\n          computation. If unspecified, `batch_size` will default to 32. Do not\n          specify the `batch_size` if your data is in the form of a dataset,\n          generators, or `keras.utils.Sequence` instances (since they generate\n          batches).\n        verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n        sample_weight: Optional Numpy array of weights for the test samples,\n          used for weighting the loss function. You can either pass a flat (1D)\n          Numpy array with the same length as the input samples\n            (1:1 mapping between weights and samples), or in the case of\n              temporal data, you can pass a 2D array with shape `(samples,\n              sequence_length)`, to apply a different weight to every timestep\n              of every sample. This argument is not supported when `x` is a\n              dataset, instead pass sample weights as the third element of `x`.\n        steps: Integer or `None`. Total number of steps (batches of samples)\n          before declaring the evaluation round finished. Ignored with the\n          default value of `None`. If x is a `tf.data` dataset and `steps` is\n          None, 'evaluate' will run until the dataset is exhausted. This\n          argument is not supported with array inputs.\n        callbacks: List of `keras.callbacks.Callback` instances. List of\n          callbacks to apply during evaluation. See\n          [callbacks](/api_docs/python/tf/keras/callbacks).\n        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n          input only. Maximum size for the generator queue. If unspecified,\n          `max_queue_size` will default to 10.\n        workers: Integer. Used for generator or `keras.utils.Sequence` input\n          only. Maximum number of processes to spin up when using process-based\n          threading. If unspecified, `workers` will default to 1.\n        use_multiprocessing: Boolean. Used for generator or\n          `keras.utils.Sequence` input only. If `True`, use process-based\n          threading. If unspecified, `use_multiprocessing` will default to\n          `False`. Note that because this implementation relies on\n          multiprocessing, you should not pass non-picklable arguments to the\n          generator as they can't be passed easily to children processes.\n        return_dict: If `True`, loss and metric results are returned as a dict,\n          with each key being the name of the metric. If `False`, they are\n          returned as a list.\n        **kwargs: Unused at this time.\n\n    See the discussion of `Unpacking behavior for iterator-like inputs` for\n    `Model.fit`.\n\n    `Model.evaluate` is not yet supported with\n    `tf.distribute.experimental.ParameterServerStrategy`.\n\n    Returns:\n        Scalar test loss (if the model has a single output and no metrics)\n        or list of scalars (if the model has multiple outputs\n        and/or metrics). The attribute `model.metrics_names` will give you\n        the display labels for the scalar outputs.\n\n    Raises:\n        RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n        ValueError: in case of invalid arguments.\n    \"\"\"\n    version_utils.disallow_legacy_graph('Model', 'evaluate')\n    self._assert_compile_was_called()\n    self._check_call_args('evaluate')\n    _disallow_inside_tf_function('evaluate')\n    use_cached_eval_dataset = kwargs.pop('_use_cached_eval_dataset', False)\n    if kwargs:\n      raise TypeError('Invalid keyword arguments: %s' % (kwargs,))\n\n    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n      self._cluster_coordinator = cluster_coordinator.ClusterCoordinator(\n          self.distribute_strategy)\n\n    with self.distribute_strategy.scope():\n      # Use cached evaluation data only when it's called in `Model.fit`\n      if (use_cached_eval_dataset\n          and getattr(self, '_eval_data_handler', None) is not None):\n        data_handler = self._eval_data_handler\n      else:\n        # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n        data_handler = data_adapter.get_data_handler(\n            x=x,\n            y=y,\n            sample_weight=sample_weight,\n            batch_size=batch_size,\n            steps_per_epoch=steps,\n            initial_epoch=0,\n            epochs=1,\n            max_queue_size=max_queue_size,\n            workers=workers,\n            use_multiprocessing=use_multiprocessing,\n            model=self,\n            steps_per_execution=self._steps_per_execution)\n\n      # Container that configures and calls `tf.keras.Callback`s.\n      if not isinstance(callbacks, callbacks_module.CallbackList):\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=True,\n            add_progbar=verbose != 0,\n            model=self,\n            verbose=verbose,\n            epochs=1,\n            steps=data_handler.inferred_steps)\n\n      logs = {}\n      self.test_function = self.make_test_function()\n      self._test_counter.assign(0)\n      callbacks.on_test_begin()\n      for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n        self.reset_metrics()\n        with data_handler.catch_stop_iteration():\n          for step in data_handler.steps():\n            with trace.Trace('test', step_num=step, _r=1):\n              callbacks.on_test_batch_begin(step)\n              tmp_logs = self.test_function(iterator)\n              if data_handler.should_sync:\n                context.async_wait()\n              logs = tmp_logs  # No error, now safe to assign to logs.\n              end_step = step + data_handler.step_increment\n              callbacks.on_test_batch_end(end_step, logs)\n      logs = tf_utils.sync_to_numpy_or_python_type(logs)\n      callbacks.on_test_end(logs=logs)\n\n      if return_dict:\n        return logs\n      else:\n        return flatten_metrics_in_order(logs, self.metrics_names)\n\n  def predict_step(self, data):\n    \"\"\"The logic for one inference step.\n\n    This method can be overridden to support custom inference logic.\n    This method is called by `Model.make_predict_function`.\n\n    This method should contain the mathematical logic for one step of inference.\n    This typically includes the forward pass.\n\n    Configuration details for *how* this logic is run (e.g. `tf.function` and\n    `tf.distribute.Strategy` settings), should be left to\n    `Model.make_predict_function`, which can also be overridden.\n\n    Args:\n      data: A nested structure of `Tensor`s.\n\n    Returns:\n      The result of one inference step, typically the output of calling the\n      `Model` on data.\n    \"\"\"\n    data = data_adapter.expand_1d(data)\n    x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\n    return self(x, training=False)\n\n  def make_predict_function(self):\n    \"\"\"Creates a function that executes one step of inference.\n\n    This method can be overridden to support custom inference logic.\n    This method is called by `Model.predict` and `Model.predict_on_batch`.\n\n    Typically, this method directly controls `tf.function` and\n    `tf.distribute.Strategy` settings, and delegates the actual evaluation\n    logic to `Model.predict_step`.\n\n    This function is cached the first time `Model.predict` or\n    `Model.predict_on_batch` is called. The cache is cleared whenever\n    `Model.compile` is called.\n\n    Returns:\n      Function. The function created by this method should accept a\n      `tf.data.Iterator`, and return the outputs of the `Model`.\n    \"\"\"\n    if self.predict_function is not None:\n      return self.predict_function\n\n    def step_function(model, iterator):\n      \"\"\"Runs a single evaluation step.\"\"\"\n\n      def run_step(data):\n        outputs = model.predict_step(data)\n        # Ensure counter is updated only if `test_step` succeeds.\n        with ops.control_dependencies(_minimum_control_deps(outputs)):\n          model._predict_counter.assign_add(1)  # pylint: disable=protected-access\n        return outputs\n\n      data = next(iterator)\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n      outputs = reduce_per_replica(\n          outputs, self.distribute_strategy, reduction='concat')\n      return outputs\n\n    if (self._steps_per_execution is None or\n        self._steps_per_execution.numpy().item() == 1):\n\n      def predict_function(iterator):\n        \"\"\"Runs an evaluation execution with one step.\"\"\"\n        return step_function(self, iterator)\n\n    else:\n\n      def predict_function(iterator):\n        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"\n        outputs = step_function(self, iterator)\n        for _ in math_ops.range(self._steps_per_execution - 1):\n          directives.set_loop_options(\n              shape_invariants=[(\n                  t, tf_utils.get_tensor_spec(t, dynamic_batch=True).shape)\n                                for t in nest.flatten(outputs)])\n          step_outputs = step_function(self, iterator)\n          outputs = nest.map_structure(lambda t1, t2: concat([t1, t2]), outputs,\n                                       step_outputs)\n        return outputs\n\n    if not self.run_eagerly:\n      predict_function = def_function.function(\n          predict_function, experimental_relax_shapes=True)\n\n    self.predict_function = predict_function\n    return self.predict_function\n\n  def predict(self,\n              x,\n              batch_size=None,\n              verbose=0,\n              steps=None,\n              callbacks=None,\n              max_queue_size=10,\n              workers=1,\n              use_multiprocessing=False):\n    \"\"\"Generates output predictions for the input samples.\n\n    Computation is done in batches. This method is designed for performance in\n    large scale inputs. For small amount of inputs that fit in one batch,\n    directly using `__call__` is recommended for faster execution, e.g.,\n    `model(x)`, or `model(x, training=False)` if you have layers such as\n    `tf.keras.layers.BatchNormalization` that behaves differently during\n    inference. Also, note the fact that test loss is not affected by\n    regularization layers like noise and dropout.\n\n    Args:\n        x: Input samples. It could be:\n          - A Numpy array (or array-like), or a list of arrays\n            (in case the model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors\n            (in case the model has multiple inputs).\n          - A `tf.data` dataset.\n          - A generator or `keras.utils.Sequence` instance.\n          A more detailed description of unpacking behavior for iterator types\n          (Dataset, generator, Sequence) is given in the `Unpacking behavior\n          for iterator-like inputs` section of `Model.fit`.\n        batch_size: Integer or `None`.\n            Number of samples per batch.\n            If unspecified, `batch_size` will default to 32.\n            Do not specify the `batch_size` if your data is in the\n            form of dataset, generators, or `keras.utils.Sequence` instances\n            (since they generate batches).\n        verbose: Verbosity mode, 0 or 1.\n        steps: Total number of steps (batches of samples)\n            before declaring the prediction round finished.\n            Ignored with the default value of `None`. If x is a `tf.data`\n            dataset and `steps` is None, `predict` will\n            run until the input dataset is exhausted.\n        callbacks: List of `keras.callbacks.Callback` instances.\n            List of callbacks to apply during prediction.\n            See [callbacks](/api_docs/python/tf/keras/callbacks).\n        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n            input only. Maximum size for the generator queue.\n            If unspecified, `max_queue_size` will default to 10.\n        workers: Integer. Used for generator or `keras.utils.Sequence` input\n            only. Maximum number of processes to spin up when using\n            process-based threading. If unspecified, `workers` will default\n            to 1.\n        use_multiprocessing: Boolean. Used for generator or\n            `keras.utils.Sequence` input only. If `True`, use process-based\n            threading. If unspecified, `use_multiprocessing` will default to\n            `False`. Note that because this implementation relies on\n            multiprocessing, you should not pass non-picklable arguments to\n            the generator as they can't be passed easily to children processes.\n\n    See the discussion of `Unpacking behavior for iterator-like inputs` for\n    `Model.fit`. Note that Model.predict uses the same interpretation rules as\n    `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n    three methods.\n\n    Returns:\n        Numpy array(s) of predictions.\n\n    Raises:\n        RuntimeError: If `model.predict` is wrapped in `tf.function`.\n        ValueError: In case of mismatch between the provided\n            input data and the model's expectations,\n            or in case a stateful model receives a number of samples\n            that is not a multiple of the batch size.\n    \"\"\"\n    version_utils.disallow_legacy_graph('Model', 'predict')\n    self._check_call_args('predict')\n    _disallow_inside_tf_function('predict')\n\n    # TODO(yashkatariya): Cache model on the coordinator for faster prediction.\n    # If running under PSS, then swap it with OneDeviceStrategy so that\n    # execution will run on the coordinator.\n    original_pss_strategy = None\n    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n      original_pss_strategy = self.distribute_strategy\n      self._distribution_strategy = None\n\n    # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not\n    # needed in `.predict()` because all the predictions happen on the\n    # coordinator/locally.\n    if self._cluster_coordinator:\n      self._cluster_coordinator = None\n\n    outputs = None\n    with self.distribute_strategy.scope():\n      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n      dataset_types = (dataset_ops.DatasetV1, dataset_ops.DatasetV2)\n      if (self._in_multi_worker_mode() or _is_tpu_multi_host(\n          self.distribute_strategy)) and isinstance(x, dataset_types):\n        try:\n          options = options_lib.Options()\n          data_option = options_lib.AutoShardPolicy.DATA\n          options.experimental_distribute.auto_shard_policy = data_option\n          x = x.with_options(options)\n        except ValueError:\n          warnings.warn('Using Model.predict with '\n                        'MultiWorkerDistributionStrategy or TPUStrategy and '\n                        'AutoShardPolicy.FILE might lead to out-of-order result'\n                        '. Consider setting it to AutoShardPolicy.DATA.')\n\n      data_handler = data_adapter.get_data_handler(\n          x=x,\n          batch_size=batch_size,\n          steps_per_epoch=steps,\n          initial_epoch=0,\n          epochs=1,\n          max_queue_size=max_queue_size,\n          workers=workers,\n          use_multiprocessing=use_multiprocessing,\n          model=self,\n          steps_per_execution=self._steps_per_execution)\n\n      # Container that configures and calls `tf.keras.Callback`s.\n      if not isinstance(callbacks, callbacks_module.CallbackList):\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=True,\n            add_progbar=verbose != 0,\n            model=self,\n            verbose=verbose,\n            epochs=1,\n            steps=data_handler.inferred_steps)\n\n      self.predict_function = self.make_predict_function()\n      self._predict_counter.assign(0)\n      callbacks.on_predict_begin()\n      batch_outputs = None\n      for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n        with data_handler.catch_stop_iteration():\n          for step in data_handler.steps():\n            callbacks.on_predict_batch_begin(step)\n            tmp_batch_outputs = self.predict_function(iterator)\n            if data_handler.should_sync:\n              context.async_wait()\n            batch_outputs = tmp_batch_outputs  # No error, now safe to assign.\n            if outputs is None:\n              outputs = nest.map_structure(lambda batch_output: [batch_output],\n                                           batch_outputs)\n            else:\n              nest.map_structure_up_to(\n                  batch_outputs,\n                  lambda output, batch_output: output.append(batch_output),\n                  outputs, batch_outputs)\n            end_step = step + data_handler.step_increment\n            callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})\n      if batch_outputs is None:\n        raise ValueError('Expect x to be a non-empty array or dataset.')\n      callbacks.on_predict_end()\n    all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)\n\n    # If originally PSS strategy was used, then replace it back since predict\n    # is running under `OneDeviceStrategy` after the swap and once its done\n    # we need to replace it back to PSS again.\n    if original_pss_strategy is not None:\n      self._distribution_strategy = original_pss_strategy\n\n    return tf_utils.sync_to_numpy_or_python_type(all_outputs)\n\n  def reset_metrics(self):\n    \"\"\"Resets the state of all the metrics in the model.\n\n    Examples:\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> outputs = tf.keras.layers.Dense(2)(inputs)\n    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n\n    >>> x = np.random.random((2, 3))\n    >>> y = np.random.randint(0, 2, (2, 2))\n    >>> _ = model.fit(x, y, verbose=0)\n    >>> assert all(float(m.result()) for m in model.metrics)\n\n    >>> model.reset_metrics()\n    >>> assert all(float(m.result()) == 0 for m in model.metrics)\n\n    \"\"\"\n    for m in self.metrics:\n      m.reset_state()\n\n  def train_on_batch(self,\n                     x,\n                     y=None,\n                     sample_weight=None,\n                     class_weight=None,\n                     reset_metrics=True,\n                     return_dict=False):\n    \"\"\"Runs a single gradient update on a single batch of data.\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays\n              (in case the model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors\n              (in case the model has multiple inputs).\n          - A dict mapping input names to the corresponding array/tensors,\n              if the model has named inputs.\n        y: Target data. Like the input data `x`, it could be either Numpy\n          array(s) or TensorFlow tensor(s). It should be consistent with `x`\n          (you cannot have Numpy inputs and tensor targets, or inversely).\n        sample_weight: Optional array of the same length as x, containing\n          weights to apply to the model's loss for each sample. In the case of\n          temporal data, you can pass a 2D array with shape (samples,\n          sequence_length), to apply a different weight to every timestep of\n          every sample.\n        class_weight: Optional dictionary mapping class indices (integers) to a\n          weight (float) to apply to the model's loss for the samples from this\n          class during training. This can be useful to tell the model to \"pay\n          more attention\" to samples from an under-represented class.\n        reset_metrics: If `True`, the metrics returned will be only for this\n          batch. If `False`, the metrics will be statefully accumulated across\n          batches.\n        return_dict: If `True`, loss and metric results are returned as a dict,\n          with each key being the name of the metric. If `False`, they are\n          returned as a list.\n\n    Returns:\n        Scalar training loss\n        (if the model has a single output and no metrics)\n        or list of scalars (if the model has multiple outputs\n        and/or metrics). The attribute `model.metrics_names` will give you\n        the display labels for the scalar outputs.\n\n    Raises:\n      RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n      ValueError: In case of invalid user-provided arguments.\n    \"\"\"\n    self._assert_compile_was_called()\n    self._check_call_args('train_on_batch')\n    _disallow_inside_tf_function('train_on_batch')\n    with self.distribute_strategy.scope(), \\\n         training_utils.RespectCompiledTrainableState(self):\n      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n                                                    y, sample_weight,\n                                                    class_weight)\n      self.train_function = self.make_train_function()\n      logs = self.train_function(iterator)\n\n    if reset_metrics:\n      self.reset_metrics()\n    logs = tf_utils.sync_to_numpy_or_python_type(logs)\n    if return_dict:\n      return logs\n    else:\n      return flatten_metrics_in_order(logs, self.metrics_names)\n\n  def test_on_batch(self,\n                    x,\n                    y=None,\n                    sample_weight=None,\n                    reset_metrics=True,\n                    return_dict=False):\n    \"\"\"Test the model on a single batch of samples.\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays (in case the\n              model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors (in case the model has\n              multiple inputs).\n          - A dict mapping input names to the corresponding array/tensors, if\n              the model has named inputs.\n        y: Target data. Like the input data `x`, it could be either Numpy\n          array(s) or TensorFlow tensor(s). It should be consistent with `x`\n          (you cannot have Numpy inputs and tensor targets, or inversely).\n        sample_weight: Optional array of the same length as x, containing\n          weights to apply to the model's loss for each sample. In the case of\n          temporal data, you can pass a 2D array with shape (samples,\n          sequence_length), to apply a different weight to every timestep of\n          every sample.\n        reset_metrics: If `True`, the metrics returned will be only for this\n          batch. If `False`, the metrics will be statefully accumulated across\n          batches.\n        return_dict: If `True`, loss and metric results are returned as a dict,\n          with each key being the name of the metric. If `False`, they are\n          returned as a list.\n\n    Returns:\n        Scalar test loss (if the model has a single output and no metrics)\n        or list of scalars (if the model has multiple outputs\n        and/or metrics). The attribute `model.metrics_names` will give you\n        the display labels for the scalar outputs.\n\n    Raises:\n        RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n        ValueError: In case of invalid user-provided arguments.\n    \"\"\"\n    self._assert_compile_was_called()\n    self._check_call_args('test_on_batch')\n    _disallow_inside_tf_function('test_on_batch')\n    with self.distribute_strategy.scope():\n      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n                                                    y, sample_weight)\n      self.test_function = self.make_test_function()\n      logs = self.test_function(iterator)\n\n    if reset_metrics:\n      self.reset_metrics()\n    logs = tf_utils.sync_to_numpy_or_python_type(logs)\n    if return_dict:\n      return logs\n    else:\n      return flatten_metrics_in_order(logs, self.metrics_names)\n\n  def predict_on_batch(self, x):\n    \"\"\"Returns predictions for a single batch of samples.\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays (in case the\n              model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors (in case the model has\n              multiple inputs).\n\n    Returns:\n        Numpy array(s) of predictions.\n\n    Raises:\n        RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n        ValueError: In case of mismatch between given number of inputs and\n          expectations of the model.\n    \"\"\"\n    self._check_call_args('predict_on_batch')\n    _disallow_inside_tf_function('predict_on_batch')\n    with self.distribute_strategy.scope():\n      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x)\n      self.predict_function = self.make_predict_function()\n      outputs = self.predict_function(iterator)\n    return tf_utils.sync_to_numpy_or_python_type(outputs)\n\n  def fit_generator(self,\n                    generator,\n                    steps_per_epoch=None,\n                    epochs=1,\n                    verbose=1,\n                    callbacks=None,\n                    validation_data=None,\n                    validation_steps=None,\n                    validation_freq=1,\n                    class_weight=None,\n                    max_queue_size=10,\n                    workers=1,\n                    use_multiprocessing=False,\n                    shuffle=True,\n                    initial_epoch=0):\n    \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\n\n    DEPRECATED:\n      `Model.fit` now supports generators, so there is no longer any need to use\n      this endpoint.\n    \"\"\"\n    warnings.warn('`Model.fit_generator` is deprecated and '\n                  'will be removed in a future version. '\n                  'Please use `Model.fit`, which supports generators.')\n    return self.fit(\n        generator,\n        steps_per_epoch=steps_per_epoch,\n        epochs=epochs,\n        verbose=verbose,\n        callbacks=callbacks,\n        validation_data=validation_data,\n        validation_steps=validation_steps,\n        validation_freq=validation_freq,\n        class_weight=class_weight,\n        max_queue_size=max_queue_size,\n        workers=workers,\n        use_multiprocessing=use_multiprocessing,\n        shuffle=shuffle,\n        initial_epoch=initial_epoch)\n\n  def evaluate_generator(self,\n                         generator,\n                         steps=None,\n                         callbacks=None,\n                         max_queue_size=10,\n                         workers=1,\n                         use_multiprocessing=False,\n                         verbose=0):\n    \"\"\"Evaluates the model on a data generator.\n\n    DEPRECATED:\n      `Model.evaluate` now supports generators, so there is no longer any need\n      to use this endpoint.\n    \"\"\"\n    warnings.warn('`Model.evaluate_generator` is deprecated and '\n                  'will be removed in a future version. '\n                  'Please use `Model.evaluate`, which supports generators.')\n    self._check_call_args('evaluate_generator')\n\n    return self.evaluate(\n        generator,\n        steps=steps,\n        max_queue_size=max_queue_size,\n        workers=workers,\n        use_multiprocessing=use_multiprocessing,\n        verbose=verbose,\n        callbacks=callbacks)\n\n  def predict_generator(self,\n                        generator,\n                        steps=None,\n                        callbacks=None,\n                        max_queue_size=10,\n                        workers=1,\n                        use_multiprocessing=False,\n                        verbose=0):\n    \"\"\"Generates predictions for the input samples from a data generator.\n\n    DEPRECATED:\n      `Model.predict` now supports generators, so there is no longer any need\n      to use this endpoint.\n    \"\"\"\n    warnings.warn('`Model.predict_generator` is deprecated and '\n                  'will be removed in a future version. '\n                  'Please use `Model.predict`, which supports generators.')\n    return self.predict(\n        generator,\n        steps=steps,\n        max_queue_size=max_queue_size,\n        workers=workers,\n        use_multiprocessing=use_multiprocessing,\n        verbose=verbose,\n        callbacks=callbacks)\n\n  ######################################################################\n  # Functions below are not training related. They are for model weights\n  # tracking, save/load, serialization, etc.\n  ######################################################################\n\n  @property\n  def trainable_weights(self):\n    self._assert_weights_created()\n    if not self._trainable:\n      return []\n    trainable_variables = []\n    for trackable_obj in self._self_tracked_trackables:\n      trainable_variables += trackable_obj.trainable_variables\n    trainable_variables += self._trainable_weights\n    return self._dedup_weights(trainable_variables)\n\n  @property\n  def non_trainable_weights(self):\n    self._assert_weights_created()\n    non_trainable_variables = []\n    for trackable_obj in self._self_tracked_trackables:\n      non_trainable_variables += trackable_obj.non_trainable_variables\n\n    if not self._trainable:\n      # Return order is all trainable vars, then all non-trainable vars.\n      trainable_variables = []\n      for trackable_obj in self._self_tracked_trackables:\n        trainable_variables += trackable_obj.trainable_variables\n\n      non_trainable_variables = (\n          trainable_variables + self._trainable_weights +\n          non_trainable_variables + self._non_trainable_weights)\n    else:\n      non_trainable_variables = (\n          non_trainable_variables + self._non_trainable_weights)\n\n    return self._dedup_weights(non_trainable_variables)\n\n  def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    Returns:\n        A flat list of Numpy arrays.\n    \"\"\"\n    with self.distribute_strategy.scope():\n      return super(Model, self).get_weights()\n\n  def save(self,\n           filepath,\n           overwrite=True,\n           include_optimizer=True,\n           save_format=None,\n           signatures=None,\n           options=None,\n           save_traces=True):\n    # pylint: disable=line-too-long\n    \"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\n\n    Please see `tf.keras.models.save_model` or the\n    [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n    for details.\n\n    Args:\n        filepath: String, PathLike, path to SavedModel or H5 file to save the\n            model.\n        overwrite: Whether to silently overwrite any existing file at the\n            target location, or provide the user with a manual prompt.\n        include_optimizer: If True, save optimizer's state together.\n        save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n            model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n            and 'h5' in TF 1.X.\n        signatures: Signatures to save with the SavedModel. Applicable to the\n            'tf' format only. Please see the `signatures` argument in\n            `tf.saved_model.save` for details.\n        options: (only applies to SavedModel format)\n            `tf.saved_model.SaveOptions` object that specifies options for\n            saving to SavedModel.\n        save_traces: (only applies to SavedModel format) When enabled, the\n            SavedModel will store the function traces for each layer. This\n            can be disabled, so that only the configs of each layer are stored.\n            Defaults to `True`. Disabling this will decrease serialization time\n            and reduce file size, but it requires that all custom layers/models\n            implement a `get_config()` method.\n\n    Example:\n\n    ```python\n    from keras.models import load_model\n\n    model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n    del model  # deletes the existing model\n\n    # returns a compiled model\n    # identical to the previous one\n    model = load_model('my_model.h5')\n    ```\n    \"\"\"\n    # pylint: enable=line-too-long\n    save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n                    signatures, options, save_traces)\n\n  def save_weights(self,\n                   filepath,\n                   overwrite=True,\n                   save_format=None,\n                   options=None):\n    \"\"\"Saves all layer weights.\n\n    Either saves in HDF5 or in TensorFlow format based on the `save_format`\n    argument.\n\n    When saving in HDF5 format, the weight file has:\n      - `layer_names` (attribute), a list of strings\n          (ordered names of model layers).\n      - For every layer, a `group` named `layer.name`\n          - For every such layer group, a group attribute `weight_names`,\n              a list of strings\n              (ordered names of weights tensor of the layer).\n          - For every weight in the layer, a dataset\n              storing the weight value, named after the weight tensor.\n\n    When saving in TensorFlow format, all objects referenced by the network are\n    saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n    instances or `Optimizer` instances assigned to object attributes. For\n    networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n    outputs)`, `Layer` instances used by the network are tracked/saved\n    automatically. For user-defined classes which inherit from `tf.keras.Model`,\n    `Layer` instances must be assigned to object attributes, typically in the\n    constructor. See the documentation of `tf.train.Checkpoint` and\n    `tf.keras.Model` for details.\n\n    While the formats are the same, do not mix `save_weights` and\n    `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n    loaded using `Model.load_weights`. Checkpoints saved using\n    `tf.train.Checkpoint.save` should be restored using the corresponding\n    `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n    `save_weights` for training checkpoints.\n\n    The TensorFlow format matches objects and variables by starting at a root\n    object, `self` for `save_weights`, and greedily matching attribute\n    names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n    is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n    means saving a `tf.keras.Model` using `save_weights` and loading into a\n    `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n    the `Model`'s variables. See the [guide to training\n    checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n    on the TensorFlow format.\n\n    Args:\n        filepath: String or PathLike, path to the file to save the weights to.\n            When saving in TensorFlow format, this is the prefix used for\n            checkpoint files (multiple files are generated). Note that the '.h5'\n            suffix causes weights to be saved in HDF5 format.\n        overwrite: Whether to silently overwrite any existing file at the\n            target location, or provide the user with a manual prompt.\n        save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n            '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n            `None` defaults to 'tf'.\n        options: Optional `tf.train.CheckpointOptions` object that specifies\n            options for saving weights.\n\n    Raises:\n        ImportError: If h5py is not available when attempting to save in HDF5\n            format.\n        ValueError: For invalid/unknown format arguments.\n    \"\"\"\n    self._assert_weights_created()\n    filepath = path_to_string(filepath)\n    filepath_is_h5 = saving_utils.is_hdf5_filepath(filepath)\n    if save_format is None:\n      if filepath_is_h5:\n        save_format = 'h5'\n      else:\n        save_format = 'tf'\n    else:\n      user_format = save_format.lower().strip()\n      if user_format in ('tensorflow', 'tf'):\n        save_format = 'tf'\n      elif user_format in ('hdf5', 'h5', 'keras'):\n        save_format = 'h5'\n      else:\n        raise ValueError(\n            'Unknown format \"%s\". Was expecting one of {\"tf\", \"h5\"}.' % (\n                save_format,))\n    if save_format == 'tf' and filepath_is_h5:\n      raise ValueError(\n          ('save_weights got save_format=\"tf\"/\"tensorflow\", but the '\n           'filepath (\"%s\") looks like an HDF5 file. Omit the \".h5\"/\".keras\" '\n           'when saving in TensorFlow format.')\n          % filepath)\n\n    if save_format == 'h5' and h5py is None:\n      raise ImportError(\n          '`save_weights` requires h5py when saving in hdf5.')\n    if save_format == 'tf':\n      check_filepath = filepath + '.index'\n    else:\n      check_filepath = filepath\n    # If file exists and should not be overwritten:\n    if not overwrite and os.path.isfile(check_filepath):\n      proceed = ask_to_proceed_with_overwrite(check_filepath)\n      if not proceed:\n        return\n    if save_format == 'h5':\n      with h5py.File(filepath, 'w') as f:\n        hdf5_format.save_weights_to_hdf5_group(f, self.layers)\n    else:\n      if context.executing_eagerly():\n        session = None\n      else:\n        session = backend.get_session()\n      self._trackable_saver.save(filepath, session=session, options=options)\n      # Record this checkpoint so it's visible from tf.train.latest_checkpoint.\n      checkpoint_management.update_checkpoint_state_internal(\n          save_dir=os.path.dirname(filepath),\n          model_checkpoint_path=filepath,\n          save_relative_paths=True,\n          all_model_checkpoint_paths=[filepath])\n\n  def load_weights(self,\n                   filepath,\n                   by_name=False,\n                   skip_mismatch=False,\n                   options=None):\n    \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n\n    If `by_name` is False weights are loaded based on the network's\n    topology. This means the architecture should be the same as when the weights\n    were saved.  Note that layers that don't have weights are not taken into\n    account in the topological ordering, so adding or removing layers is fine as\n    long as they don't have weights.\n\n    If `by_name` is True, weights are loaded into layers only if they share the\n    same name. This is useful for fine-tuning or transfer-learning models where\n    some of the layers have changed.\n\n    Only topological loading (`by_name=False`) is supported when loading weights\n    from the TensorFlow format. Note that topological loading differs slightly\n    between TensorFlow and HDF5 formats for user-defined classes inheriting from\n    `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n    TensorFlow format loads based on the object-local names of attributes to\n    which layers are assigned in the `Model`'s constructor.\n\n    Args:\n        filepath: String, path to the weights file to load. For weight files in\n            TensorFlow format, this is the file prefix (the same as was passed\n            to `save_weights`). This can also be a path to a SavedModel\n            saved from `model.save`.\n        by_name: Boolean, whether to load weights by name or by topological\n            order. Only topological loading is supported for weight files in\n            TensorFlow format.\n        skip_mismatch: Boolean, whether to skip loading of layers where there is\n            a mismatch in the number of weights, or a mismatch in the shape of\n            the weight (only valid when `by_name=True`).\n        options: Optional `tf.train.CheckpointOptions` object that specifies\n            options for loading weights.\n\n    Returns:\n        When loading a weight file in TensorFlow format, returns the same status\n        object as `tf.train.Checkpoint.restore`. When graph building, restore\n        ops are run automatically as soon as the network is built (on first call\n        for user-defined classes inheriting from `Model`, immediately if it is\n        already built).\n\n        When loading weights in HDF5 format, returns `None`.\n\n    Raises:\n        ImportError: If h5py is not available and the weight file is in HDF5\n            format.\n        ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n          `False`.\n    \"\"\"\n    if backend.is_tpu_strategy(self._distribution_strategy):\n      if (self._distribution_strategy.extended.steps_per_run > 1 and\n          (not saving_utils.is_hdf5_filepath(filepath))):\n        raise ValueError('Load weights is not yet supported with TPUStrategy '\n                         'with steps_per_run greater than 1.')\n    if skip_mismatch and not by_name:\n      raise ValueError(\n          'When calling model.load_weights, skip_mismatch can only be set to '\n          'True when by_name is True.')\n\n    filepath, save_format = _detect_save_format(filepath)\n    if save_format == 'tf':\n      status = self._trackable_saver.restore(filepath, options)\n      if by_name:\n        raise NotImplementedError(\n            'Weights may only be loaded based on topology into Models when '\n            'loading TensorFlow-formatted weights (got by_name=True to '\n            'load_weights).')\n      if not context.executing_eagerly():\n        session = backend.get_session()\n        # Restore existing variables (if any) immediately, and set up a\n        # streaming restore for any variables created in the future.\n        trackable_utils.streaming_restore(status=status, session=session)\n      status.assert_nontrivial_match()\n    else:\n      status = None\n      if h5py is None:\n        raise ImportError(\n            '`load_weights` requires h5py when loading weights from HDF5.')\n      if not self._is_graph_network and not self.built:\n        raise ValueError(\n            'Unable to load weights saved in HDF5 format into a subclassed '\n            'Model which has not created its variables yet. Call the Model '\n            'first, then load the weights.')\n      self._assert_weights_created()\n      with h5py.File(filepath, 'r') as f:\n        if 'layer_names' not in f.attrs and 'model_weights' in f:\n          f = f['model_weights']\n        if by_name:\n          hdf5_format.load_weights_from_hdf5_group_by_name(\n              f, self.layers, skip_mismatch=skip_mismatch)\n        else:\n          hdf5_format.load_weights_from_hdf5_group(f, self.layers)\n\n    # Perform any layer defined finalization of the layer state.\n    for layer in self.layers:\n      layer.finalize_state()\n    return status\n\n  def _updated_config(self):\n    \"\"\"Util shared between different serialization methods.\n\n    Returns:\n        Model config with Keras version information added.\n    \"\"\"\n    from tensorflow.python.keras import __version__ as keras_version  # pylint: disable=g-import-not-at-top\n\n    config = self.get_config()\n    model_config = {\n        'class_name': self.__class__.__name__,\n        'config': config,\n        'keras_version': keras_version,\n        'backend': backend.backend()\n    }\n    return model_config\n\n  def get_config(self):\n    raise NotImplementedError\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None):\n    # `from_config` assumes `cls` is either `Functional` or a child class of\n    # `Functional`. In the case that `cls` is meant to behave like a child class\n    # of `Functional` but only inherits from the `Model` class, we have to call\n    # `cls(...)` instead of `Functional.from_config`.\n    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\n    with generic_utils.SharedObjectLoadingScope():\n      input_tensors, output_tensors, created_layers = (\n          functional.reconstruct_from_config(config, custom_objects))\n      # Initialize a model belonging to `cls`, which can be user-defined or\n      # `Functional`.\n      model = cls(inputs=input_tensors, outputs=output_tensors,\n                  name=config.get('name'))\n      functional.connect_ancillary_layers(model, created_layers)\n      return model\n\n  def to_json(self, **kwargs):\n    \"\"\"Returns a JSON string containing the network configuration.\n\n    To load a network from a JSON save file, use\n    `keras.models.model_from_json(json_string, custom_objects={})`.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `json.dumps()`.\n\n    Returns:\n        A JSON string.\n    \"\"\"\n    model_config = self._updated_config()\n    return json.dumps(\n        model_config, default=json_utils.get_json_type, **kwargs)\n\n  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        ImportError: if yaml module is not found.\n    \"\"\"\n    if yaml is None:\n      raise ImportError(\n          'Requires yaml module installed (`pip install pyyaml`).')\n    return yaml.dump(self._updated_config(), **kwargs)\n\n  def reset_states(self):\n    for layer in self.layers:\n      if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):\n        layer.reset_states()\n\n  @property\n  @doc_controls.do_not_generate_docs\n  def state_updates(self):\n    \"\"\"Deprecated, do NOT use!\n\n    Returns the `updates` from all layers that are stateful.\n\n    This is useful for separating training updates and\n    state updates, e.g. when we need to update a layer's internal state\n    during prediction.\n\n    Returns:\n        A list of update ops.\n    \"\"\"\n    warnings.warn('`Model.state_updates` will be removed in a future version. '\n                  'This property should not be used in TensorFlow 2.0, '\n                  'as `updates` are applied automatically.')\n    state_updates = []\n    for layer in self.layers:\n      if getattr(layer, 'stateful', False):\n        if hasattr(layer, 'updates'):\n          state_updates += layer.updates\n    return state_updates\n\n  @property\n  def weights(self):\n    \"\"\"Returns the list of all layer variables/weights.\n\n    Note: This will not track the weights of nested `tf.Modules` that are not\n    themselves Keras layers.\n\n    Returns:\n      A list of variables.\n    \"\"\"\n    return self._dedup_weights(self._undeduplicated_weights)\n\n  @property\n  def _undeduplicated_weights(self):\n    \"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\n    self._assert_weights_created()\n    weights = []\n    for layer in self._self_tracked_trackables:\n      weights += layer.variables\n    weights += (self._trainable_weights + self._non_trainable_weights)\n    return weights\n\n  def summary(self, line_length=None, positions=None, print_fn=None):\n    \"\"\"Prints a string summary of the network.\n\n    Args:\n        line_length: Total length of printed lines\n            (e.g. set this to adapt the display to different\n            terminal window sizes).\n        positions: Relative or absolute positions of log elements\n            in each line. If not provided,\n            defaults to `[.33, .55, .67, 1.]`.\n        print_fn: Print function to use. Defaults to `print`.\n            It will be called on each line of the summary.\n            You can set it to a custom function\n            in order to capture the string summary.\n\n    Raises:\n        ValueError: if `summary()` is called before the model is built.\n    \"\"\"\n    if not self.built:\n      raise ValueError('This model has not yet been built. '\n                       'Build the model first by calling `build()` or calling '\n                       '`fit()` with some data, or specify '\n                       'an `input_shape` argument in the first layer(s) for '\n                       'automatic build.')\n    layer_utils.print_summary(self,\n                              line_length=line_length,\n                              positions=positions,\n                              print_fn=print_fn)\n\n  @property\n  def layers(self):\n    return list(self._flatten_layers(include_self=False, recursive=False))\n\n  def get_layer(self, name=None, index=None):\n    \"\"\"Retrieves a layer based on either its name (unique) or index.\n\n    If `name` and `index` are both provided, `index` will take precedence.\n    Indices are based on order of horizontal graph traversal (bottom-up).\n\n    Args:\n        name: String, name of layer.\n        index: Integer, index of layer.\n\n    Returns:\n        A layer instance.\n\n    Raises:\n        ValueError: In case of invalid layer name or index.\n    \"\"\"\n    # TODO(fchollet): We could build a dictionary based on layer names\n    # since they are constant, but we have not done that yet.\n    if index is not None and name is not None:\n      raise ValueError('Provide only a layer name or a layer index.')\n\n    if index is not None:\n      if len(self.layers) <= index:\n        raise ValueError('Was asked to retrieve layer at index ' + str(index) +\n                         ' but model only has ' + str(len(self.layers)) +\n                         ' layers.')\n      else:\n        return self.layers[index]\n\n    if name is not None:\n      for layer in self.layers:\n        if layer.name == name:\n          return layer\n      raise ValueError('No such layer: ' + name + '.')\n    raise ValueError('Provide either a layer name or layer index.')\n\n  @trackable.no_automatic_dependency_tracking\n  def _set_save_spec(self, inputs):\n    if self._saved_model_inputs_spec is not None:\n      return  # Already set.\n\n    input_names = self.input_names\n    if not input_names:\n      input_names = compile_utils.create_pseudo_input_names(inputs)\n\n    flat_inputs = nest.flatten(inputs)\n    specs = []\n    for name, tensor in zip(input_names, flat_inputs):\n      specs.append(\n          tf_utils.get_tensor_spec(tensor, dynamic_batch=False, name=name))\n    specs = nest.pack_sequence_as(inputs, specs)\n\n    self._saved_model_inputs_spec = specs\n\n    # Store the input shapes\n    if (self.__class__.__name__ == 'Sequential' and\n        self._build_input_shape is None):\n      self._build_input_shape = nest.map_structure(\n          lambda x: None if x is None else x.shape, specs)\n\n  def _assert_weights_created(self):\n    \"\"\"Asserts that all the weights for the model have been created.\n\n    For a non-dynamic model, the weights must already be created after the\n    layer has been called. For a dynamic model, the exact list of weights can\n    never be known for certain since it may change at any time during execution.\n\n    We run this check right before accessing weights or getting the Numpy value\n    for the current weights. Otherwise, if the layer has never been called,\n    the user would just get an empty list, which is misleading.\n\n    Raises:\n      ValueError: if the weights of the network has not yet been created.\n    \"\"\"\n    if self.dynamic:\n      return\n\n    if ('build' in self.__class__.__dict__ and\n        self.__class__ != Model and\n        not self.built):\n      # For any model that has customized build() method but hasn't\n      # been invoked yet, this will cover both sequential and subclass model.\n      # Also make sure to exclude Model class itself which has build() defined.\n      raise ValueError('Weights for model %s have not yet been created. '\n                       'Weights are created when the Model is first called on '\n                       'inputs or `build()` is called with an `input_shape`.' %\n                       self.name)\n\n  def _check_call_args(self, method_name):\n    \"\"\"Check that `call` has only one positional arg.\"\"\"\n    # Always allow first arg, regardless of arg name.\n    fullargspec = self._call_full_argspec\n    if fullargspec.defaults:\n      positional_args = fullargspec.args[:-len(fullargspec.defaults)]\n    else:\n      positional_args = fullargspec.args\n    if 'training' in positional_args:\n      positional_args.remove('training')\n\n    # self and first arg can be positional.\n    if len(positional_args) > 2:\n      extra_args = positional_args[2:]\n      raise ValueError(\n          'Models passed to `' + method_name + '` can only have `training` '\n          'and the first argument in `call` as positional arguments, '\n          'found: ' + str(extra_args) + '.')\n\n  def _validate_compile(self, optimizer, metrics, **kwargs):\n    \"\"\"Performs validation checks for the default `compile`.\"\"\"\n    if any(\n        isinstance(opt, optimizer_v1.Optimizer)\n        for opt in nest.flatten(optimizer)):\n      raise ValueError(\n          '`tf.compat.v1.keras` Optimizer (', optimizer, ') is '\n          'not supported when eager execution is enabled. Use a '\n          '`tf.keras` Optimizer instead, or disable eager '\n          'execution.')\n\n    kwargs.pop('cloning', None)  # Legacy DistStrat argument, never used.\n    kwargs.pop('experimental_run_tf_function', None)  # Always `True`.\n    if kwargs.pop('distribute', None) is not None:\n      raise ValueError(\n          'Distribute argument in compile is not available in TF 2.0 please '\n          'create the model under the distribution strategy scope.')\n    if kwargs.pop('target_tensors', None) is not None:\n      raise ValueError(\n          'target_tensors argument is not supported when executing eagerly.')\n    invalid_kwargs = set(kwargs) - {'sample_weight_mode'}\n    if invalid_kwargs:\n      raise TypeError('Invalid keyword argument(s) in `compile`: %s' %\n                      (invalid_kwargs,))\n\n    # Model must be created and compiled with the same DistStrat.\n    if self.built and ds_context.has_strategy():\n      strategy = ds_context.get_strategy()\n      for v in self.variables:\n        if not strategy.extended.variable_created_in_scope(v):\n          raise ValueError(\n              'Variable (%s) was not created in the distribution strategy '\n              'scope of (%s). It is most likely due to not all layers or '\n              'the model or optimizer being created outside the distribution '\n              'strategy scope. Try to make sure your code looks similar '\n              'to the following.\\n'\n              'with strategy.scope():\\n'\n              '  model=_create_model()\\n'\n              '  model.compile(...)' % (v, strategy))\n\n    # Model metrics must be created in the same distribution strategy scope\n    # as the model.\n    strategy = self.distribute_strategy\n    for metric in nest.flatten(metrics):\n      for v in getattr(metric, 'variables', []):\n        if not strategy.extended.variable_created_in_scope(v):\n          raise ValueError(\n              'Metric (%s) passed to model.compile was created inside of a '\n              'different distribution strategy scope than the model. All '\n              'metrics must be created in the same distribution strategy '\n              'scope as the model (in this case %s). If you pass in a string '\n              'identifier for a metric to compile the metric will '\n              'automatically be created in the correct distribution '\n              'strategy scope.' % (metric, strategy)\n          )\n\n    # Model metrics must be created in the same distribution strategy scope\n    # as the model.\n    for opt in nest.flatten(optimizer):\n      for v in getattr(opt, '_weights', []):\n        if not strategy.extended.variable_created_in_scope(v):\n          raise ValueError(\n              'Optimizer (%s) passed to model.compile was created inside of a '\n              'different distribution strategy scope than the model. All '\n              'optimizers must be created in the same distribution strategy '\n              'scope as the model (in this case %s). If you pass in a string '\n              'identifier for an optimizer to compile the optimizer will '\n              'automatically be created in the correct distribution '\n              'strategy scope.' % (opt, strategy))\n\n  def _maybe_load_initial_epoch_from_ckpt(self, initial_epoch):\n    \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.\n\n    Refer to tensorflow/python/keras/distribute/worker_training_state.py\n    for more information.\n\n    Args:\n      initial_epoch: The original initial_epoch user passes in in `fit()`.\n\n    Returns:\n      If the training is recovering from previous failure under multi-worker\n      training setting, return the epoch the training is supposed to continue\n      at. Otherwise, return the `initial_epoch` the user passes in.\n    \"\"\"\n    if self._training_state is not None:\n      return self._training_state.maybe_load_initial_epoch_from_ckpt(\n          initial_epoch, mode=ModeKeys.TRAIN)\n    return initial_epoch\n\n  def _assert_compile_was_called(self):\n    # Checks whether `compile` has been called. If it has been called,\n    # then the optimizer is set. This is different from whether the\n    # model is compiled\n    # (i.e. whether the model is built and its inputs/outputs are set).\n    if not self._is_compiled:\n      raise RuntimeError('You must compile your model before '\n                         'training/testing. '\n                         'Use `model.compile(optimizer, loss)`.')\n\n  def _set_inputs(self, inputs, outputs=None, training=None):\n    \"\"\"This method is for compat with Modelv1. Only inputs are needed here.\"\"\"\n    self._set_save_spec(inputs)\n\n  @property\n  def _trackable_saved_model_saver(self):\n    return model_serialization.ModelSavedModelSaver(self)\n\n  def _list_functions_for_serialization(self, serialization_cache):\n    # SavedModel needs to ignore the execution functions.\n    train_function = self.train_function\n    test_function = self.test_function\n    predict_function = self.predict_function\n    train_tf_function = self.train_tf_function\n    self.train_function = None\n    self.test_function = None\n    self.predict_function = None\n    self.train_tf_function = None\n    functions = super(\n        Model, self)._list_functions_for_serialization(serialization_cache)\n    self.train_function = train_function\n    self.test_function = test_function\n    self.predict_function = predict_function\n    self.train_tf_function = train_tf_function\n    return functions\n\n  def _should_eval(self, epoch, validation_freq):\n    epoch = epoch + 1  # one-index the user-facing epoch.\n    if isinstance(validation_freq, int):\n      return epoch % validation_freq == 0\n    elif isinstance(validation_freq, list):\n      return epoch in validation_freq\n    else:\n      raise ValueError('Expected `validation_freq` to be a list or int.')\n\n  ######################################################################\n  # Functions below exist only as v1 / v2 compatibility shims.\n  ######################################################################\n\n  def _get_compile_args(self, user_metrics=True):\n    \"\"\"Used for saving or cloning a Model.\n\n    Args:\n      user_metrics: Whether to return user-supplied metrics or `Metric` objects.\n        Defaults to returning the user-supplied metrics.\n\n    Returns:\n      Dictionary of arguments that were used when compiling the model.\n    \"\"\"\n    self._assert_compile_was_called()\n    # pylint: disable=protected-access\n\n    saved_metrics = self.compiled_metrics._user_metrics\n    saved_weighted_metrics = self.compiled_metrics._user_weighted_metrics\n\n    if not user_metrics:\n      if saved_metrics is not None:\n        saved_metrics = self.compiled_metrics._metrics\n      if saved_weighted_metrics is not None:\n        saved_weighted_metrics = self.compiled_metrics._weighted_metrics\n\n    compile_args = {\n        'optimizer': self.optimizer,\n        'loss': self.compiled_loss._user_losses,\n        'metrics': saved_metrics,\n        'weighted_metrics': saved_weighted_metrics,\n        'loss_weights': self.compiled_loss._user_loss_weights,\n    }\n    # pylint: enable=protected-access\n    return compile_args\n\n  def _get_callback_model(self):\n    return self\n\n  def _in_multi_worker_mode(self):\n    return self.distribute_strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access\n\n  @property\n  def _compile_was_called(self):\n    return self._is_compiled\n\n\ndef reduce_per_replica(values, strategy, reduction='first'):\n  \"\"\"Reduce PerReplica objects.\n\n  Args:\n    values: Structure of `PerReplica` objects or `Tensor`s. `Tensor`s are\n      returned as-is.\n    strategy: `tf.distribute.Strategy` object.\n    reduction: One of 'first', 'concat'.\n\n  Returns:\n    Structure of `Tensor`s.\n  \"\"\"\n\n  def _reduce(v):\n    \"\"\"Reduce a single `PerReplica` object.\"\"\"\n    if reduction == 'concat' and _collective_all_reduce_multi_worker(strategy):\n      return _multi_worker_concat(v, strategy)\n    if not _is_per_replica_instance(v):\n      return v\n    elif reduction == 'first':\n      return strategy.unwrap(v)[0]\n    elif reduction == 'concat':\n      if _is_tpu_multi_host(strategy):\n        return _tpu_multi_host_concat(v, strategy)\n      else:\n        return concat(strategy.unwrap(v))\n    else:\n      raise ValueError('`reduction` must be \"first\" or \"concat\".')\n\n  return nest.map_structure(_reduce, values)\n\n\ndef concat(tensors, axis=0):\n  \"\"\"Concats `tensor`s along `axis`.\"\"\"\n  if isinstance(tensors[0], sparse_tensor.SparseTensor):\n    return sparse_ops.sparse_concat_v2(axis=axis, sp_inputs=tensors)\n  return array_ops.concat(tensors, axis=axis)\n\n\ndef _is_tpu_multi_host(strategy):\n  return (backend.is_tpu_strategy(strategy) and\n          strategy.extended.num_hosts > 1)\n\n\ndef _tpu_multi_host_concat(v, strategy):\n  \"\"\"Correctly order TPU PerReplica objects.\"\"\"\n  replicas = strategy.unwrap(v)\n  # When distributed datasets are created from Tensors / NumPy,\n  # TPUStrategy.experimental_distribute_dataset shards data in\n  # (Replica, Host) order, and TPUStrategy.unwrap returns it in\n  # (Host, Replica) order.\n  # TODO(b/150317897): Figure out long-term plan here.\n  num_replicas_per_host = strategy.extended.num_replicas_per_host\n  ordered_replicas = []\n  for replica_id in range(num_replicas_per_host):\n    ordered_replicas += replicas[replica_id::num_replicas_per_host]\n  return concat(ordered_replicas)\n\n\ndef _collective_all_reduce_multi_worker(strategy):\n  return (isinstance(strategy,\n                     collective_all_reduce_strategy.CollectiveAllReduceStrategy)\n         ) and strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access\n\n\n# TODO(wxinyi): merge this with _tpu_multi_host_concat once we have all_gather\n# for all strategies\ndef _multi_worker_concat(v, strategy):\n  \"\"\"Order PerReplica objects for CollectiveAllReduceStrategy and concat.\"\"\"\n  replicas = strategy.gather(v, axis=0)\n  # v might not have the same shape on different replicas\n  if _is_per_replica_instance(v):\n    shapes = array_ops.concat([\n        array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)\n        for single_value in v.values\n    ],\n                              axis=0)\n    all_shapes = strategy.gather(shapes, axis=0)\n  else:\n    # v is a tensor. This may happen when, say, we have 2x1 multi-worker.\n    all_shapes = strategy.gather(\n        array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0), axis=0)\n\n  replicas = array_ops.split(\n      replicas,\n      num_or_size_splits=all_shapes,\n      num=strategy.num_replicas_in_sync)\n  ordered_replicas = []\n  num_replicas_per_worker = len(strategy.extended.worker_devices)\n  for replica_id in range(num_replicas_per_worker):\n    ordered_replicas += replicas[replica_id::num_replicas_per_worker]\n  return concat(ordered_replicas)\n\n\ndef _is_scalar(x):\n  return isinstance(x, (ops.Tensor, variables.Variable)) and x.shape.rank == 0\n\n\ndef write_scalar_summaries(logs, step):\n  for name, value in logs.items():\n    if _is_scalar(value):\n      summary_ops_v2.scalar('batch_' + name, value, step=step)\n\n\ndef _minimum_control_deps(outputs):\n  \"\"\"Returns the minimum control dependencies to ensure step succeeded.\"\"\"\n  if context.executing_eagerly():\n    return []  # Control dependencies not needed.\n  outputs = nest.flatten(outputs, expand_composites=True)\n  for out in outputs:\n    # Variables can't be control dependencies.\n    if not isinstance(out, variables.Variable):\n      return [out]  # Return first Tensor or Op from outputs.\n  return []  # No viable Tensor or Op to use for control deps.\n\n\ndef _disallow_inside_tf_function(method_name):\n  if ops.inside_function():\n    error_msg = (\n        'Detected a call to `Model.{method_name}` inside a `tf.function`. '\n        '`Model.{method_name} is a high-level endpoint that manages its own '\n        '`tf.function`. Please move the call to `Model.{method_name}` outside '\n        'of all enclosing `tf.function`s. Note that you can call a `Model` '\n        'directly on `Tensor`s inside a `tf.function` like: `model(x)`.'\n    ).format(method_name=method_name)\n    raise RuntimeError(error_msg)\n\n\ndef _detect_save_format(filepath):\n  \"\"\"Returns path to weights file and save format.\"\"\"\n\n  filepath = path_to_string(filepath)\n  if saving_utils.is_hdf5_filepath(filepath):\n    return filepath, 'h5'\n\n  # Filepath could be a TensorFlow checkpoint file prefix or SavedModel\n  # directory. It's possible for filepath to be both a prefix and directory.\n  # Prioritize checkpoint over SavedModel.\n  if _is_readable_tf_checkpoint(filepath):\n    save_format = 'tf'\n  elif sm_loader.contains_saved_model(filepath):\n    ckpt_path = os.path.join(filepath, sm_constants.VARIABLES_DIRECTORY,\n                             sm_constants.VARIABLES_FILENAME)\n    if _is_readable_tf_checkpoint(ckpt_path):\n      filepath = ckpt_path\n      save_format = 'tf'\n    else:\n      raise ValueError('Unable to load weights. filepath {} appears to be a '\n                       'SavedModel directory, but checkpoint either doesn\\'t '\n                       'exist, or is incorrectly formatted.'.format(filepath))\n  else:\n    # Not a TensorFlow checkpoint. This filepath is likely an H5 file that\n    # doesn't have the hdf5/keras extensions.\n    save_format = 'h5'\n  return filepath, save_format\n\n\ndef _is_readable_tf_checkpoint(filepath):\n  try:\n    py_checkpoint_reader.NewCheckpointReader(filepath)\n    return True\n  except errors_impl.DataLossError:\n    # The checkpoint is not readable in TensorFlow format.\n    return False\n\n\ndef flatten_metrics_in_order(logs, metrics_names):\n  \"\"\"Turns the `logs` dict into a list as per key order of `metrics_names`.\"\"\"\n  results = []\n  for name in metrics_names:\n    if name in logs:\n      results.append(logs[name])\n  for key in sorted(logs.keys()):\n    if key not in metrics_names:\n      results.append(logs[key])\n  if len(results) == 1:\n    return results[0]\n  return results\n\n\ndef _is_per_replica_instance(obj):\n  return (isinstance(obj, ds_values.DistributedValues) and\n          isinstance(obj, composite_tensor.CompositeTensor))\n\n\ndef saver_with_op_caching(obj):\n  if context.executing_eagerly():\n    saveables_cache = None\n  else:\n    saveables_cache = object_identity.ObjectIdentityWeakKeyDictionary()\n  return trackable_utils.TrackableSaver(\n      graph_view_lib.ObjectGraphView(\n          weakref.ref(obj), saveables_cache=saveables_cache))\n", "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: disable=protected-access\n\"\"\"Functions that save the model's config into different formats.\"\"\"\n\nfrom tensorflow.python.keras.saving.saved_model import json_utils\nfrom tensorflow.python.util.tf_export import keras_export\n\n# pylint: disable=g-import-not-at-top\ntry:\n  import yaml\nexcept ImportError:\n  yaml = None\n# pylint: enable=g-import-not-at-top\n\n\n@keras_export('keras.models.model_from_config')\ndef model_from_config(config, custom_objects=None):\n  \"\"\"Instantiates a Keras model from its config.\n \n  Usage:\n  ```\n  # for a Functional API model\n  tf.keras.Model().from_config(model.get_config())\n\n  # for a Sequential model\n  tf.keras.Sequential().from_config(model.get_config())\n  ```\n\n  Args:\n      config: Configuration dictionary.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      TypeError: if `config` is not a dictionary.\n  \"\"\"\n  if isinstance(config, list):\n    raise TypeError('`model_from_config` expects a dictionary, not a list. '\n                    'Maybe you meant to use '\n                    '`Sequential.from_config(config)`?')\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n\n\n@keras_export('keras.models.model_from_yaml')\ndef model_from_yaml(yaml_string, custom_objects=None):\n  \"\"\"Parses a yaml model configuration file and returns a model instance.\n\n  Usage:\n\n  >>> model = tf.keras.Sequential([\n  ...     tf.keras.layers.Dense(5, input_shape=(3,)),\n  ...     tf.keras.layers.Softmax()])\n  >>> try:\n  ...   import yaml\n  ...   config = model.to_yaml()\n  ...   loaded_model = tf.keras.models.model_from_yaml(config)\n  ... except ImportError:\n  ...   pass\n\n  Args:\n      yaml_string: YAML string or open file encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      ImportError: if yaml module is not found.\n  \"\"\"\n  if yaml is None:\n    raise ImportError('Requires yaml module installed (`pip install pyyaml`).')\n  # The method unsafe_load only exists in PyYAML 5.x+, so which branch of the\n  # try block is covered by tests depends on the installed version of PyYAML.\n  try:\n    # PyYAML 5.x+\n    config = yaml.unsafe_load(yaml_string)\n  except AttributeError:\n    config = yaml.load(yaml_string)\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n\n\n@keras_export('keras.models.model_from_json')\ndef model_from_json(json_string, custom_objects=None):\n  \"\"\"Parses a JSON model configuration string and returns a model instance.\n\n  Usage:\n\n  >>> model = tf.keras.Sequential([\n  ...     tf.keras.layers.Dense(5, input_shape=(3,)),\n  ...     tf.keras.layers.Softmax()])\n  >>> config = model.to_json()\n  >>> loaded_model = tf.keras.models.model_from_json(config)\n\n  Args:\n      json_string: JSON string encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n  \"\"\"\n  config = json_utils.decode(json_string)\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n"], "fixing_code": ["# Release 2.7.0\n\n<INSERT SMALL BLURB ABOUT RELEASE FOCUS AREA AND POTENTIAL TOOLCHAIN CHANGES>\n\n## Breaking Changes\n\n* `tf.keras`:\n  * The methods `Model.fit()`, `Model.predict()`, and `Model.evaluate()` will\n    no longer uprank input data of shape `(batch_size,)`\n    to become `(batch_size, 1)`. This enables `Model` subclasses to process\n    scalar data in their `train_step()`/`test_step()`/`predict_step()` methods.\n    Note that this change may break certain subclassed models.\n    You can revert back to the previous behavior by adding upranking yourself\n    in the `train_step()`/`test_step()`/`predict_step()` methods, e.g.\n    `if x.shape.rank == 1: x = tf.expand_dims(x, axis=-1)`.\n    Functional models as well as Sequential models built with an explicit\n    input shape are not affected.\n  * The methods `Model.to_yaml()` and `keras.models.model_from_yaml` have been\n    replaced to raise a `RuntimeError` as they can be abused to cause arbitrary\n    code execution. It is recommended to use JSON serialization instead of YAML,\n    or, a better alternative, serialize to H5.\n\n* `tf.lite`:\n  * Rename fields `SignatureDef` table in schema to maximize the parity with\n    TF SavedModel's Signature concept.\n\n* TF Core:\n    *   `tf.Graph.get_name_scope()` now always returns a string, as documented.\n        Previously, when called within `name_scope(\"\")` or `name_scope(None)`\n        contexts, it returned None; now it returns the empty string.\n    *   `tensorflow/core/ir/` contains a new MLIR-based Graph dialect that is\n        isomorphic to GraphDef and will be used to replace GraphDef-based (e.g.,\n        Grappler) optimizations.\n\n## Known Caveats\n\n*<CAVEATS REGARDING THE RELEASE (BUT NOT BREAKING CHANGES).>\n*<ADDING/BUMPING DEPENDENCIES SHOULD GO HERE>\n*<KNOWN LACK OF SUPPORT ON SOME PLATFORM, SHOULD GO HERE>\n\n## Major Features and Improvements\n\n* Improvements to the TensorFlow debugging experience:\n  * Previously, TensorFlow error stack traces involved many internal frames,\n    which could be challenging to read through,\n    while not being actionable for end users.\n    As of TF 2.7, TensorFlow filters internal frames in most errors that it\n    raises, to keep stack traces short, readable, and focused on what's\n    actionable for end users (their own code).\n\n    This behavior can be disabled by calling\n    `tf.debugging.disable_traceback_filtering()`, and can be re-enabled via\n    `tf.debugging.enable_traceback_filtering()`. If you are debugging a\n    TensorFlow-internal issue (e.g. to prepare a TensorFlow PR), make sure\n    to disable traceback filtering. You can check whether this feature is\n    currently enabled by calling\n    `tf.debugging.is_traceback_filtering_enabled()`.\n\n    Note that this feature is only available with Python 3.7 or higher.\n\n  * Improve the informativeness of error messages raised by Keras\n    `Layer.__call__()`, by adding the full list of argument values passed to the\n    layer in every exception.\n\n*   TF1 -> TF2 Migration\n    * Introduced the `tf.compat.v1.keras.utils.track_tf1_style_variables`\n      decorator, which enables using large classes of tf1-style variable_scope,\n      `get_variable`, and `compat.v1.layer`-based components from within TF2\n      models running with TF2 behavior enabled.\n\n*  `tf.data`:\n    *   tf.data service now supports auto-sharding. Users specify the sharding\n        policy with `tf.data.experimental.service.ShardingPolicy` enum. It can\n        be one of OFF (equivalent to today's `\"parallel_epochs\"` mode), DYNAMIC\n        (equivalent to today's `\"distributed_epoch\"` mode), or one of the static\n        sharding policies: FILE, DATA, FILE_OR_DATA, or HINT (corresponding to\n        values of `tf.data.experimental.AutoShardPolicy`).\n\n        Static sharding (auto-sharding) requires the number of tf.data service\n        workers be fixed. Users need to specify the worker addresses in\n        `tensorflow.data.experimental.DispatcherConfig`.\n*  Keras:\n  *  `tf.keras.layers.Conv` now includes a public `convolution_op` method.\n      This method can be used to simplify the implementation of Conv subclasses.\n      There are two primary ways to use this new method.  The first is to use the method directly in your own `call` method:\n      ```\n        class StandardizedConv2D(tf.keras.layers.Conv2D):\n          def call(self, inputs):\n            mean, var = tf.nn.moments(self.kernel, axes=[0, 1, 2], keepdims=True)\n            return self.convolution_op(inputs, (self.kernel - mean) / tf.sqrt(var + 1e-10))\n      ```\n      Alternatively, you can override `convolution_op`:\n      ```\n        class StandardizedConv2D(tf.keras.Layer):\n          def convolution_op(self, inputs, kernel):\n            mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)\n            # Author code uses std + 1e-5\n            return super().convolution_op(inputs, (kernel - mean) / tf.sqrt(var + 1e-10))\n      ```\n\n## Bug Fixes and Other Changes\n\n*<SIMILAR TO ABOVE SECTION, BUT FOR OTHER IMPORTANT CHANGES / BUG FIXES>\n*<IF A CHANGE CLOSES A GITHUB ISSUE, IT SHOULD BE DOCUMENTED HERE>\n*<NOTES SHOULD BE GROUPED PER AREA>\n*   TF Core:\n    * Random number generation (RNG) system\n        *   Added argument `alg` to `tf.random.stateless_*` functions to explicitly select the RNG algorithm.\n        *   Added `tf.nn.experimental.stateless_dropout`, a stateless version of `tf.nn.dropout`.\n        *   `tf.random.Generator` now can be created inside the scope of `tf.distribute.experimental.CentralStorageStrategy`.\n*   `tf.data`:\n    *   Promoting `tf.data.Options.experimental_deterministic` API to\n        `tf.data.Options.deterministic` and deprecating the experimental\n        endpoint.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n<INSERT>, <NAME>, <HERE>, <USING>, <GITHUB>, <HANDLE>\n\n# Release 2.6.0\n\n<INSERT SMALL BLURB ABOUT RELEASE FOCUS AREA AND POTENTIAL TOOLCHAIN CHANGES>\n\n## Breaking Changes\n\n*   `tf.train.experimental.enable_mixed_precision_graph_rewrite` is removed, as\n    the API only works in graph mode and is not customizable. The function is\n    still accessible under\n    `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite`, but it\n    is recommended to use the\n    [Keras mixed precision API](https://www.tensorflow.org/guide/mixed_precision)\n    instead.\n\n*   `tf.lite`:\n\n    *   Remove `experimental.nn.dynamic_rnn`, `experimental.nn.TfLiteRNNCell`\n        and `experimental.nn.TfLiteLSTMCell` since they're no longer supported.\n        It's recommended to just use\n        [keras lstm](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n        instead.\n\n*   Keras been split into a separate PIP package (`keras`), and its code has\n    been moved to the GitHub\n    repository[keras-team/keras](http://github.com/keras-team/keras). The API\n    endpoints for `tf.keras` stay unchanged, but are now backed by the `keras`\n    PIP package. The existing code in tensorflow/python/keras is a staled copy\n    and will be removed in future release (2.7). Please remove any imports to\n    `tensorflow.python.keras` and replace them with public tf.keras API instead.\n\n*   Modular File System Migration\n\n    *   S3 and HDFS file system supports have been migrated to modular file\n        systems and is now available in https://github.com/tensorflow/io. The\n        tensorflow-io python package should be installed for S3 and HDFS support\n        with tensorflow.\n\n*<DOCUMENT BREAKING CHANGES HERE>\n*<THIS SECTION SHOULD CONTAIN API, ABI AND BEHAVIORAL BREAKING CHANGES>\n\n## Known Caveats\n\n*<CAVEATS REGARDING THE RELEASE (BUT NOT BREAKING CHANGES).>\n*<ADDING/BUMPING DEPENDENCIES SHOULD GO HERE>\n*<KNOWN LACK OF SUPPORT ON SOME PLATFORM, SHOULD GO HERE>\n\n* TF Core:\n    * A longstanding bug in `tf.while_loop`, which caused it to execute\n      sequentially, even when `parallel_iterations>1`, has now been fixed.\n      However, the increased parallelism may result in increased memory use.\n      Users who experience unwanted regressions should reset their\n      `while_loop`'s `parallel_iterations` value to 1, which is consistent with\n      prior behavior.\n\n* `tf.keras`:\n  * The `trainable` argument when creating a Keras Layer must now be a boolean\n    (previously there was no validation and `None` values were allowed).\n\n## Major Features and Improvements\n\n*<INSERT MAJOR FEATURE HERE, USING MARKDOWN SYNTAX>\n*<IF RELEASE CONTAINS MULTIPLE FEATURES FROM SAME AREA, GROUP THEM TOGETHER>\n\n* `tf.keras`:\n    *   Keras has been split into a separate PIP package (`keras`),\n        and its code has been moved to the GitHub repository\n        [keras-team/keras](http://github.com/keras-team/keras).\n        The API endpoints for `tf.keras` stay unchanged,\n        but are now backed by the `keras` PIP package. All Keras-related\n        PRs and issues should now be directed to the GitHub repository\n        [keras-team/keras](http://github.com/keras-team/keras).\n    *   `tf.keras.utils.experimental.DatasetCreator` now takes an optional\n        `tf.distribute.InputOptions` for specific options when used with\n        distribution.\n    *   `tf.keras.experimental.SidecarEvaluator` is now available for a program\n        intended to be run on an evaluator task, which is commonly used to\n        supplement a training cluster running with\n        `tf.distribute.experimental.ParameterServerStrategy` (see\n        https://www.tensorflow.org/tutorials/distribute/parameter_server_training).\n        It can also be used with single-worker training or other strategies.\n        See docstring for more info.\n    *   Preprocessing layers moved from experimental to core.\n        *   Import paths moved from\n            `tf.keras.layers.preprocessing.experimental` to `tf.keras.layers`.\n    *   Updates to Preprocessing layers API for consistency and clarity:\n        *   `StringLookup` and `IntegerLookup` default for `mask_token` changed\n            to `None`. This matches the default masking behavior of `Hashing`\n            and `Embedding` layers. To keep existing behavior, pass\n            `mask_token=\"\"` during layer creation.\n        *   Renamed `\"binary\"` output mode to `\"multi_hot\"` for\n            `CategoryEncoding`, `StringLookup`, `IntegerLookup`, and\n            `TextVectorization`. Multi-hot encoding will no longer\n            automatically uprank rank 1 inputs, so these layers can now\n            multi-hot encode unbatched multi-dimensional samples.\n        *   Added a new output mode `\"one_hot\"` for `CategoryEncoding`,\n            `StringLookup`, `IntegerLookup`, which will encode each element in\n            an input batch individually, and automatically append a new output\n            dimension if necessary. Use this mode on rank 1 inputs for the old\n            `\"binary\"` behavior of one-hot encoding a batch of scalars.\n        *   `Normalization` will no longer automatically uprank rank 1 inputs,\n            allowing normalization of unbatched multi-dimensional samples.\n\n* `tf.lite`:\n    *   The recommended Android NDK version for building TensorFlow Lite has\n        been changed from r18b to r19c.\n    *   Supports int64 for mul.\n    *   Supports native variable builtin ops - ReadVariable, AssignVariable.\n    *   Converter:\n        *  Experimental support for variables in TFLite. To enable through\n           conversion, users need to set\n           `experimental_enable_resource_variables` on tf.lite.TFLiteConverter\n           to True.\n           Note: mutable variables is only available using from_saved_model\n           in this release, support for other methods is coming soon.\n        *  Old Converter (TOCO) is getting removed from next release.\n           It's been deprecated for few releases already.\n* `tf.saved_model`:\n    *   SavedModels can now save custom gradients. Use the option\n        `tf.saved_model.SaveOption(experimental_custom_gradients=True)` to\n        enable this feature. The documentation in [Advanced autodiff]\n        (https://www.tensorflow.org/guide/advanced_autodiff#custom_gradients)\n        has been updated.\n    *   Object metadata has now been deprecated and no longer saved to the\n        SavedModel.\n*   TF Core:\n    *   Added `tf.config.experimental.reset_memory_stats` to reset the tracked\n        peak memory returned by `tf.config.experimental.get_memory_info`.\n*  `tf.data`:\n    *   Added checkpointing support for `tf.data.experimental.save()`.\n    *   Added `target_workers` param to `data_service_ops.from_dataset_id` and\n        `data_service_ops.distribute`. Users can specify `\"AUTO\"`, `\"ANY\"`, or\n        `\"LOCAL\"` (case insensitive). If `\"AUTO\"`, tf.data service runtime\n        decides which workers to read from. If `\"ANY\"`, TF workers read from any\n        tf.data service workers. If `\"LOCAL\"`, TF workers will only read from\n        local in-processs tf.data service workers. `\"AUTO\"` works well for most\n        cases, while users can specify other targets. For example, `\"LOCAL\"`\n        would help avoid RPCs and data copy if every TF worker colocates with a\n        tf.data service worker. Currently, `\"AUTO\"` reads from any tf.data\n        service workers to preserve existing behavior. The default value is\n        `\"AUTO\"`.\n\n## Bug Fixes and Other Changes\n\n*<SIMILAR TO ABOVE SECTION, BUT FOR OTHER IMPORTANT CHANGES / BUG FIXES>\n*<IF A CHANGE CLOSES A GITHUB ISSUE, IT SHOULD BE DOCUMENTED HERE>\n*<NOTES SHOULD BE GROUPED PER AREA>\n*   TF Core:\n    *   Added `tf.lookup.experimental.MutableHashTable`, which provides a\n        generic mutable hash table implementation.\n        *   Compared to `tf.lookup.experimental.DenseHashTable` this offers\n        lower overall memory usage, and a cleaner API. It does not require\n        specifying a `delete_key` and `empty_key` that cannot be inserted into\n        the table.\n    *   Added support for specifying number of subdivisions in all reduce host\n        collective. This parallelizes work on CPU and speeds up the collective\n        performance. Default behavior is unchanged.\n    *   Add an option `perturb_singular` to `tf.linalg.tridiagonal_solve` that\n        allows solving linear systems with a numerically singular tridiagonal\n        matrix, e.g. for use in inverse iteration.\n    *   Added `tf.linalg.eigh_tridiagonal` that computes the eigenvalues of a\n        Hermitian tridiagonal matrix.\n    *   `tf.constant` now places its output on the current default device.\n    *   SavedModel\n        *   Added `tf.saved_model.experimental.TrackableResource`, which allows\n            the creation of custom wrapper objects for resource tensors.\n        *   Added a SavedModel load option to allow restoring partial\n            checkpoints into the SavedModel. See [`tf.saved_model.LoadOptions`]\n  (https://www.tensorflow.org/api_docs/python/tf/saved_model/LoadOptions)\n            for details.\n    *   Added a new op `SparseSegmentSumGrad` to match the other sparse segment\n        gradient ops and avoid an extra gather operation that was in the\n        previous gradient implementation.\n    *   Added a new session config setting `internal_fragmentation_fraction`,\n        which controls when the BFC Allocator needs to split an oversized chunk\n        to satisfy an allocation request.\n    *   Added `tf.get_current_name_scope()` which returns the current full name\n        scope string that will be prepended to op names.\n*   `tf.data`:\n    *   Promoting `tf.data.experimental.bucket_by_sequence_length` API to\n        `tf.data.Dataset.bucket_by_sequence_length` and deprecating the\n        experimental endpoint.\n    *   Promoting `tf.data.experimental.get_single_element` API to\n        `tf.data.Dataset.get_single_element` and deprecating the experimental\n        endpoint.\n    *   Promoting `tf.data.experimental.group_by_window` API to\n        `tf.data.Dataset.group_by_window` and deprecating the experimental\n        endpoint.\n    *   Promoting `tf.data.experimental.RandomDataset` API to\n        `tf.data.Dataset.random` and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.scan` API to `tf.data.Dataset.scan`\n        and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.snapshot` API to\n        `tf.data.Dataset.shapshot` and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.take_while` API to\n        `tf.data.Dataset.take_while` and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.ThreadingOptions` API to\n        `tf.data.ThreadingOptions` and deprecating the experimental endpoint.\n    *   Promoting `tf.data.experimental.unique` API to\n        `tf.data.Dataset.unique` and deprecating the experimental endpoint.\n    *   Added `stop_on_empty_dataset` parameter to `sample_from_datasets` and\n        `choose_from_datasets`. Setting `stop_on_empty_dataset=True` will stop\n        sampling if it encounters an empty dataset. This preserves the sampling\n        ratio throughout training. The prior behavior was to continue sampling,\n        skipping over exhausted datasets, until all datasets are exhausted. By\n        default, the original behavior (`stop_on_empty_dataset=False`) is\n        preserved.\n    *   Removed previously deprecated tf.data statistics related APIs:\n        *   `tf.data.Options.experimental_stats`\n        *   `tf.data.experimental.StatsAggregator`\n        *   `tf.data.experimental.StatsOptions.*`\n        *   `tf.data.experimental.bytes_produced_stats`\n        *   `tf.data.experimental.latency_stats`\n    *   Removed the following experimental tf.data optimization APIs:\n        *   `tf.data.experimental.MapVectorizationOptions.*`\n        *   `tf.data.experimental.OptimizationOptions.filter_with_random_uniform_fusion`\n        *   `tf.data.experimental.OptimizationOptions.hoist_random_uniform`\n        *   `tf.data.experimental.OptimizationOptions.map_vectorization`                 *   `tf.data.experimental.OptimizationOptions.reorder_data_discarding_ops`\n*   `tf.keras`:\n    *   Fix usage of `__getitem__` slicing in Keras Functional APIs when the\n        inputs are `RaggedTensor` objects.\n    *   Add `keepdims` argument to all `GlobalPooling` layers.\n    *   Add `include_preprocessing` argument to `MobileNetV3` architectures to\n        control the inclusion of `Rescaling` layer in the model.\n    *   Add optional argument (`force`) to `make_(train|test|predict)_funtion`\n        methods to skip the cached function and generate a new one. This is\n\tuseful to regenerate in a single call the compiled training function\n\twhen any `.trainable` attribute of any model's layer has changed.\n    *   Models now have a `save_spec` property which contains the `TensorSpec`\n        specs for calling the model. This spec is automatically saved when\n        the model is called for the first time.\n*   `tf.linalg`:\n    *   Add `CompositeTensor` as a base class to `LinearOperator`.\n*   `tf.lite`:\n    *   Fix mean op reference quantization rounding issue.\n    *   Added `framework_stable` BUILD target, which links in only the\n        non-experimental TF Lite APIs.\n    *   Remove deprecated Java `Interpreter` methods:\n        *    `modifyGraphWithDelegate` - Use `Interpreter.Options.addDelegate`\n        *    `setNumThreads` - Use `Interpreter.Options.setNumThreads`\n    *   Add Conv3DTranspose as a builtin op.\n*   `tf.summary`:\n    *   Fix `tf.summary.should_record_summaries()` so it correctly reflects when\n        summaries will be written, even when `tf.summary.record_if()` is not\n        in effect, by returning True tensor if default writer is present.\n*   `Grappler`:\n    *   Disable default Grappler optimization timeout to make the optimization\n        pipeline deterministic. This may lead to increased model loading time,\n        because time spent in graph optimizations is now unbounded (was 20\n        minutes).\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n<INSERT>, <NAME>, <HERE>, <USING>, <GITHUB>, <HANDLE>\n\n# Release 2.4.2\n\nThis release introduces several vulnerability fixes:\n\n*   Fixes a heap buffer overflow in `RaggedBinCount`\n    ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n*   Fixes a heap out of bounds write in `RaggedBinCount`\n    ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n*   Fixes a type confusion during tensor casts which leads to dereferencing null\n    pointers\n    ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n*   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n    ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n*   Fixes a null pointer dereference via invalid Ragged Tensors\n    ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n*   Fixes a division by zero in `Conv3D`\n    ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n*   Fixes vulnerabilities where session operations in eager mode lead to null\n    pointer dereferences\n    ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n*   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n    ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n*   Fixes a segfault in `SparseCountSparseOutput`\n    ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n*   Fixes a heap buffer overflow in `Conv3DBackprop*`\n    ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n*   Fixes a division by 0 in `Conv3DBackprop*`\n    ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n*   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n    ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n*   Fixes a division by 0 in `Conv2DBackpropFilter`\n    ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n*   Fixes a division by 0 in `Conv2DBackpropInput`\n    ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n*   Fixes a division by 0 in `Conv2D`\n    ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n*   Fixes a division by 0 in `QuantizedConv2D`\n    ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n*   Fixes a division by 0 in `QuantizedMul`\n    ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n*   Fixes vulnerabilities caused by invalid validation in\n    `SparseMatrixSparseCholesky`\n    ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n*   Fixes a heap buffer overflow caused by rounding\n    ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n    ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n*   Fixes a heap out of bounds read in `RaggedCross`\n    ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n*   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n    ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n*   Fixes a heap buffer overflow in `QuantizedMul`\n    ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n*   Fixes a `CHECK`-fail in `SparseConcat`\n    ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n*   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n    ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n*   Fixes a heap buffer overflow in `QuantizedReshape`\n    ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n*   Fixes a division by zero in `Conv2DBackpropFilter`\n    ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n*   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n    ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n*   Fixes a heap buffer overflow in `StringNGrams`\n    ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n*   Fixes a null pointer dereference in `StringNGrams`\n    ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n*   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n    ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n*   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n    ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n*   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n    ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n*   Fixes a division by 0 in `QuantizedBiasAdd`\n    ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n*   Fixes a heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n*   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n*   Fixes a division by 0 in `QuantizedAdd`\n    ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n*   Fixes a division by 0 in `FractionalAvgPool`\n    ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n*   Fixes an OOB read in `MatrixTriangularSolve`\n    ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n*   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n    ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n*   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n    ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n*   Fixes a division by 0 in `DenseCountSparseOutput`\n    ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n*   Fixes a division by 0 in `FusedBatchNorm`\n    ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n*   Fixes a division by 0 in `SparseMatMul`\n    ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n*   Fixes a division by 0 in `Reverse`\n    ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n*   Fixes a heap buffer overflow in `SparseSplit`\n    ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n*   Fixes a heap OOB access in unicode ops\n    ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n*   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n    ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n*   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n    ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n    ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n    ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n*   Fixes a null pointer dereference in `EditDistance`\n    ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n*   Fixes a null pointer dereference in `SparseFillEmptyRows`\n    ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n*   Fixes a heap OOB access in `Dilation2DBackpropInput`\n    ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n*   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n    ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n*   Fixes a set of vulnerabilities caused by lack of validation in\n    `SparseDenseCwiseMul`\n    ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n*   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n*   Fixes a heap out of bounds read in `RequantizationRange`\n    ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n*   Fixes a memory corruption in `DrawBoundingBoxesV2`\n    ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n*   Fixes a reference binding to nullptr in `SdcaOptimizer`\n    ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n*   Fixes an overflow and a denial of service in `tf.raw_ops.ReverseSequence`\n    ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n*   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n*   Fixes an undefined behavior in `MaxPool3DGradGrad`\n    ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n*   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n    ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n*   Fixes a heap buffer overflow in `AvgPool3DGrad`\n    ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n*   Fixes an undefined behavior and a `CHECK`-fail in `FractionalMaxPoolGrad`\n    ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n*   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n    ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n*   Fixes a heap buffer overflow in `MaxPoolGrad`\n    ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n*   Fixes a segfault in `CTCBeamSearchDecoder`\n    ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n*   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n    ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n*   Fixes a `CHECK`-fail due to integer overflow\n    ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n*   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n    ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n*   Fixes a division by zero in padding computation in TFLite\n    ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n*   Fixes a division by zero in optimized pooling implementations in TFLite\n    ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n    ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n*   Fixes a division by zero in TFLite's implementation of `GatherNd`\n    ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n*   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n    ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n*   Fixes a heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n    ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n*   Fixes a null pointer dereference in TFLite's `Reshape` operator\n    ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n*   Fixes a stack overflow due to looping TFLite subgraph\n    ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n*   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n    ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n*   Fixes a division by zero in TFLite's convolution code\n    ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n*   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n    ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n*   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n    ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n    ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n*   Fixes a division by zero in TFLite's implementation of `SVDF`\n    ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n*   Fixes a division by zero in TFLite's implementation of `Split`\n    ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n*   Fixes a division by zero in TFLite's implementation of `OneHot`\n    ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n*   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n    ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n*   Fixes a division by zero in TFLite's implementation of hashtable lookup\n    ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n*   Fixes a integer overflow in TFLite concatentation\n    ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n*   Fixes a integer overflow in TFLite memory allocation\n    ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n*   Fixes a heap OOB write in TFLite\n    ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n*   Fixes a heap OOB read in TFLite\n    ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n*   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n    ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n    ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `SparseSparseMinimum`\n    ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n    ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n*   Fixes vulnerabilities caused by invalid validation in\n    `QuantizeAndDequantizeV2`\n    ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n*   Fixes a heap buffer overflow in `BandedTriangularSolve`\n    ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `tf.raw_ops.CTCLoss`\n    ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n*   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n    ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n*   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n    ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n*   Fixes a null dereference in Grappler's `TrySimplify`\n    ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n*   Fixes a crash in `tf.transpose` with complex inputs\n    ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n*   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n    ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n*   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n    ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n*   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n    ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n*   Updates `curl` to `7.76.0` to handle\n    [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n    [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n    [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n    [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n    [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n    and\n    [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n# Release 2.3.3\n\nThis release introduces several vulnerability fixes:\n\n*   Fixes a heap buffer overflow in `RaggedBinCount`\n    ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n*   Fixes a heap out of bounds write in `RaggedBinCount`\n    ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n*   Fixes a type confusion during tensor casts which leads to dereferencing null\n    pointers\n    ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n*   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n    ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n*   Fixes a null pointer dereference via invalid Ragged Tensors\n    ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n*   Fixes a division by zero in `Conv3D`\n    ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n*   Fixes vulnerabilities where session operations in eager mode lead to null\n    pointer dereferences\n    ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n*   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n    ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n*   Fixes a segfault in `SparseCountSparseOutput`\n    ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n*   Fixes a heap buffer overflow in `Conv3DBackprop*`\n    ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n*   Fixes a division by 0 in `Conv3DBackprop*`\n    ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n*   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n    ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n*   Fixes a division by 0 in `Conv2DBackpropFilter`\n    ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n*   Fixes a division by 0 in `Conv2DBackpropInput`\n    ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n*   Fixes a division by 0 in `Conv2D`\n    ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n*   Fixes a division by 0 in `QuantizedConv2D`\n    ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n*   Fixes a division by 0 in `QuantizedMul`\n    ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n*   Fixes vulnerabilities caused by invalid validation in\n    `SparseMatrixSparseCholesky`\n    ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n*   Fixes a heap buffer overflow caused by rounding\n    ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n    ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n*   Fixes a heap out of bounds read in `RaggedCross`\n    ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n*   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n    ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n*   Fixes a heap buffer overflow in `QuantizedMul`\n    ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n*   Fixes a `CHECK`-fail in `SparseConcat`\n    ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n*   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n    ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n*   Fixes a heap buffer overflow in `QuantizedReshape`\n    ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n*   Fixes a division by zero in `Conv2DBackpropFilter`\n    ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n*   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n    ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n*   Fixes a heap buffer overflow in `StringNGrams`\n    ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n*   Fixes a null pointer dereference in `StringNGrams`\n    ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n*   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n    ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n*   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n    ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n*   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n    ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n*   Fixes a division by 0 in `QuantizedBiasAdd`\n    ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n*   Fixes a heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n*   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n*   Fixes a division by 0 in `QuantizedAdd`\n    ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n*   Fixes a division by 0 in `FractionalAvgPool`\n    ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n*   Fixes an OOB read in `MatrixTriangularSolve`\n    ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n*   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n    ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n*   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n    ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n*   Fixes a division by 0 in `DenseCountSparseOutput`\n    ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n*   Fixes a division by 0 in `FusedBatchNorm`\n    ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n*   Fixes a division by 0 in `SparseMatMul`\n    ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n*   Fixes a division by 0 in `Reverse`\n    ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n*   Fixes a heap buffer overflow in `SparseSplit`\n    ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n*   Fixes a heap OOB access in unicode ops\n    ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n*   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n    ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n*   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n    ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n    ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n    ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n*   Fixes a null pointer dereference in `EditDistance`\n    ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n*   Fixes a null pointer dereference in `SparseFillEmptyRows`\n    ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n*   Fixes a heap OOB access in `Dilation2DBackpropInput`\n    ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n*   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n    ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n*   Fixes a set of vulnerabilities caused by lack of validation in\n    `SparseDenseCwiseMul`\n    ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n*   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n*   Fixes a heap out of bounds read in `RequantizationRange`\n    ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n*   Fixes a memory corruption in `DrawBoundingBoxesV2`\n    ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n*   Fixes a reference binding to nullptr in `SdcaOptimizer`\n    ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n*   Fixes an overflow and a denial of service in `tf.raw_ops.ReverseSequence`\n    ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n*   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n*   Fixes an undefined behavior in `MaxPool3DGradGrad`\n    ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n*   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n    ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n*   Fixes a heap buffer overflow in `AvgPool3DGrad`\n    ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n*   Fixes an undefined behavior and a `CHECK`-fail in `FractionalMaxPoolGrad`\n    ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n*   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n    ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n*   Fixes a heap buffer overflow in `MaxPoolGrad`\n    ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n*   Fixes a segfault in `CTCBeamSearchDecoder`\n    ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n*   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n    ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n*   Fixes a `CHECK`-fail due to integer overflow\n    ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n*   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n    ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n*   Fixes a division by zero in padding computation in TFLite\n    ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n*   Fixes a division by zero in optimized pooling implementations in TFLite\n    ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n    ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n*   Fixes a division by zero in TFLite's implementation of `GatherNd`\n    ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n*   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n    ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n*   Fixes a heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n    ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n*   Fixes a null pointer dereference in TFLite's `Reshape` operator\n    ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n*   Fixes a stack overflow due to looping TFLite subgraph\n    ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n*   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n    ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n*   Fixes a division by zero in TFLite's convolution code\n    ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n*   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n    ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n*   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n    ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n    ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n*   Fixes a division by zero in TFLite's implementation of `SVDF`\n    ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n*   Fixes a division by zero in TFLite's implementation of `Split`\n    ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n*   Fixes a division by zero in TFLite's implementation of `OneHot`\n    ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n*   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n    ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n*   Fixes a division by zero in TFLite's implementation of hashtable lookup\n    ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n*   Fixes a integer overflow in TFLite concatentation\n    ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n*   Fixes a integer overflow in TFLite memory allocation\n    ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n*   Fixes a heap OOB write in TFLite\n    ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n*   Fixes a heap OOB read in TFLite\n    ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n*   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n    ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n    ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `SparseSparseMinimum`\n    ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n    ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n*   Fixes vulnerabilities caused by invalid validation in\n    `QuantizeAndDequantizeV2`\n    ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n*   Fixes a heap buffer overflow in `BandedTriangularSolve`\n    ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `tf.raw_ops.CTCLoss`\n    ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n*   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n    ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n*   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n    ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n*   Fixes a null dereference in Grappler's `TrySimplify`\n    ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n*   Fixes a crash in `tf.transpose` with complex inputs\n    ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n*   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n    ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n*   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n    ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n*   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n    ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n*   Updates `curl` to `7.76.0` to handle\n    [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n    [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n    [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n    [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n    [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n    and\n    [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n# Release 2.2.3\n\nThis release introduces several vulnerability fixes:\n\n*   Fixes a heap buffer overflow in `RaggedBinCount`\n    ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n*   Fixes a heap out of bounds write in `RaggedBinCount`\n    ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n*   Fixes a type confusion during tensor casts which leads to dereferencing null\n    pointers\n    ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n*   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n    ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n*   Fixes a null pointer dereference via invalid Ragged Tensors\n    ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n*   Fixes a division by zero in `Conv3D`\n    ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n*   Fixes vulnerabilities where session operations in eager mode lead to null\n    pointer dereferences\n    ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n*   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n    ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n*   Fixes a segfault in `SparseCountSparseOutput`\n    ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n*   Fixes a heap buffer overflow in `Conv3DBackprop*`\n    ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n*   Fixes a division by 0 in `Conv3DBackprop*`\n    ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n*   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n    ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n*   Fixes a division by 0 in `Conv2DBackpropFilter`\n    ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n*   Fixes a division by 0 in `Conv2DBackpropInput`\n    ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n*   Fixes a division by 0 in `Conv2D`\n    ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n*   Fixes a division by 0 in `QuantizedConv2D`\n    ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n*   Fixes a division by 0 in `QuantizedMul`\n    ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n*   Fixes vulnerabilities caused by invalid validation in\n    `SparseMatrixSparseCholesky`\n    ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n*   Fixes a heap buffer overflow caused by rounding\n    ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n    ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n*   Fixes a heap out of bounds read in `RaggedCross`\n    ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n*   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n    ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n*   Fixes a heap buffer overflow in `QuantizedMul`\n    ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n*   Fixes a `CHECK`-fail in `SparseConcat`\n    ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n*   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n    ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n*   Fixes a heap buffer overflow in `QuantizedReshape`\n    ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n*   Fixes a division by zero in `Conv2DBackpropFilter`\n    ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n*   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n    ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n*   Fixes a heap buffer overflow in `StringNGrams`\n    ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n*   Fixes a null pointer dereference in `StringNGrams`\n    ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n*   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n    ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n*   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n    ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n*   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n    ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n*   Fixes a division by 0 in `QuantizedBiasAdd`\n    ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n*   Fixes a heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n*   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n*   Fixes a division by 0 in `QuantizedAdd`\n    ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n*   Fixes a division by 0 in `FractionalAvgPool`\n    ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n*   Fixes an OOB read in `MatrixTriangularSolve`\n    ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n*   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n    ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n*   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n    ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n*   Fixes a division by 0 in `DenseCountSparseOutput`\n    ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n*   Fixes a division by 0 in `FusedBatchNorm`\n    ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n*   Fixes a division by 0 in `SparseMatMul`\n    ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n*   Fixes a division by 0 in `Reverse`\n    ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n*   Fixes a heap buffer overflow in `SparseSplit`\n    ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n*   Fixes a heap OOB access in unicode ops\n    ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n*   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n    ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n*   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n    ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n    ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n    ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n*   Fixes a null pointer dereference in `EditDistance`\n    ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n*   Fixes a null pointer dereference in `SparseFillEmptyRows`\n    ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n*   Fixes a heap OOB access in `Dilation2DBackpropInput`\n    ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n*   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n    ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n*   Fixes a set of vulnerabilities caused by lack of validation in\n    `SparseDenseCwiseMul`\n    ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n*   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n*   Fixes a heap out of bounds read in `RequantizationRange`\n    ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n*   Fixes a memory corruption in `DrawBoundingBoxesV2`\n    ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n*   Fixes a reference binding to nullptr in `SdcaOptimizer`\n    ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n*   Fixes an overflow and a denial of service in `tf.raw_ops.ReverseSequence`\n    ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n*   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n*   Fixes an undefined behavior in `MaxPool3DGradGrad`\n    ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n*   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n    ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n*   Fixes a heap buffer overflow in `AvgPool3DGrad`\n    ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n*   Fixes an undefined behavior and a `CHECK`-fail in `FractionalMaxPoolGrad`\n    ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n*   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n    ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n*   Fixes a heap buffer overflow in `MaxPoolGrad`\n    ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n*   Fixes a segfault in `CTCBeamSearchDecoder`\n    ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n*   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n    ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n*   Fixes a `CHECK`-fail due to integer overflow\n    ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n*   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n    ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n*   Fixes a division by zero in padding computation in TFLite\n    ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n*   Fixes a division by zero in optimized pooling implementations in TFLite\n    ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n    ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n*   Fixes a division by zero in TFLite's implementation of `GatherNd`\n    ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n*   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n    ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n*   Fixes a heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n    ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n*   Fixes a null pointer dereference in TFLite's `Reshape` operator\n    ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n*   Fixes a stack overflow due to looping TFLite subgraph\n    ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n*   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n    ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n*   Fixes a division by zero in TFLite's convolution code\n    ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n*   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n    ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n*   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n    ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n    ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n*   Fixes a division by zero in TFLite's implementation of `SVDF`\n    ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n*   Fixes a division by zero in TFLite's implementation of `Split`\n    ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n*   Fixes a division by zero in TFLite's implementation of `OneHot`\n    ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n*   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n    ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n*   Fixes a division by zero in TFLite's implementation of hashtable lookup\n    ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n*   Fixes a integer overflow in TFLite concatentation\n    ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n*   Fixes a integer overflow in TFLite memory allocation\n    ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n*   Fixes a heap OOB write in TFLite\n    ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n*   Fixes a heap OOB read in TFLite\n    ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n*   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n    ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n    ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `SparseSparseMinimum`\n    ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n    ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n*   Fixes vulnerabilities caused by invalid validation in\n    `QuantizeAndDequantizeV2`\n    ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n*   Fixes a heap buffer overflow in `BandedTriangularSolve`\n    ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `tf.raw_ops.CTCLoss`\n    ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n*   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n    ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n*   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n    ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n*   Fixes a null dereference in Grappler's `TrySimplify`\n    ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n*   Fixes a crash in `tf.transpose` with complex inputs\n    ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n*   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n    ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n*   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n    ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n*   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n    ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n*   Updates `curl` to `7.76.0` to handle\n    [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n    [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n    [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n    [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n    [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n    and\n    [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n# Release 2.1.4\n\nThis release introduces several vulnerability fixes:\n\n*   Fixes a heap buffer overflow in `RaggedBinCount`\n    ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n*   Fixes a heap out of bounds write in `RaggedBinCount`\n    ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n*   Fixes a type confusion during tensor casts which leads to dereferencing null\n    pointers\n    ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n*   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n    ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n*   Fixes a null pointer dereference via invalid Ragged Tensors\n    ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n*   Fixes a division by zero in `Conv3D`\n    ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n*   Fixes vulnerabilities where session operations in eager mode lead to null\n    pointer dereferences\n    ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n*   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n    ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n*   Fixes a segfault in `SparseCountSparseOutput`\n    ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n*   Fixes a heap buffer overflow in `Conv3DBackprop*`\n    ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n*   Fixes a division by 0 in `Conv3DBackprop*`\n    ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n*   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n    ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n*   Fixes a division by 0 in `Conv2DBackpropFilter`\n    ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n*   Fixes a division by 0 in `Conv2DBackpropInput`\n    ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n*   Fixes a division by 0 in `Conv2D`\n    ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n*   Fixes a division by 0 in `QuantizedConv2D`\n    ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n*   Fixes a division by 0 in `QuantizedMul`\n    ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n*   Fixes vulnerabilities caused by invalid validation in\n    `SparseMatrixSparseCholesky`\n    ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n*   Fixes a heap buffer overflow caused by rounding\n    ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n    ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n*   Fixes a heap out of bounds read in `RaggedCross`\n    ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n*   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n    ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n*   Fixes a heap buffer overflow in `QuantizedMul`\n    ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n*   Fixes a `CHECK`-fail in `SparseConcat`\n    ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n*   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n    ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n*   Fixes a heap buffer overflow in `QuantizedReshape`\n    ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n*   Fixes a division by zero in `Conv2DBackpropFilter`\n    ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n*   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n    ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n*   Fixes a heap buffer overflow in `StringNGrams`\n    ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n*   Fixes a null pointer dereference in `StringNGrams`\n    ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n*   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n    ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n*   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n    ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n*   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n    ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n*   Fixes a division by 0 in `QuantizedBiasAdd`\n    ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n*   Fixes a heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n*   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n    ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n*   Fixes a division by 0 in `QuantizedAdd`\n    ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n*   Fixes a division by 0 in `FractionalAvgPool`\n    ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n*   Fixes an OOB read in `MatrixTriangularSolve`\n    ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n*   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n    ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n*   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n    ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n*   Fixes a division by 0 in `DenseCountSparseOutput`\n    ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n*   Fixes a division by 0 in `FusedBatchNorm`\n    ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n*   Fixes a division by 0 in `SparseMatMul`\n    ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n*   Fixes a division by 0 in `Reverse`\n    ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n*   Fixes a heap buffer overflow in `SparseSplit`\n    ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n*   Fixes a heap OOB access in unicode ops\n    ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n*   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n    ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n*   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n    ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n    ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n*   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n    ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n*   Fixes a null pointer dereference in `EditDistance`\n    ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n*   Fixes a null pointer dereference in `SparseFillEmptyRows`\n    ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n*   Fixes a heap OOB access in `Dilation2DBackpropInput`\n    ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n*   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n    ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n*   Fixes a set of vulnerabilities caused by lack of validation in\n    `SparseDenseCwiseMul`\n    ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n*   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n*   Fixes a heap out of bounds read in `RequantizationRange`\n    ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n*   Fixes a memory corruption in `DrawBoundingBoxesV2`\n    ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n*   Fixes a reference binding to nullptr in `SdcaOptimizer`\n    ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n*   Fixes an overflow and a denial of service in `tf.raw_ops.ReverseSequence`\n    ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n*   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n    ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n*   Fixes an undefined behavior in `MaxPool3DGradGrad`\n    ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n*   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n    ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n*   Fixes a heap buffer overflow in `AvgPool3DGrad`\n    ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n*   Fixes an undefined behavior and a `CHECK`-fail in `FractionalMaxPoolGrad`\n    ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n*   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n    ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n*   Fixes a heap buffer overflow in `MaxPoolGrad`\n    ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n*   Fixes a segfault in `CTCBeamSearchDecoder`\n    ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n*   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n    ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n*   Fixes a `CHECK`-fail due to integer overflow\n    ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n*   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n    ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n*   Fixes a division by zero in padding computation in TFLite\n    ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n*   Fixes a division by zero in optimized pooling implementations in TFLite\n    ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n    ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n*   Fixes a division by zero in TFLite's implementation of `GatherNd`\n    ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n*   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n    ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n*   Fixes a heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n    ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n*   Fixes a null pointer dereference in TFLite's `Reshape` operator\n    ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n*   Fixes a stack overflow due to looping TFLite subgraph\n    ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n*   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n    ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n*   Fixes a division by zero in TFLite's convolution code\n    ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n*   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n    ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n*   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n    ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n*   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n    ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n*   Fixes a division by zero in TFLite's implementation of `SVDF`\n    ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n*   Fixes a division by zero in TFLite's implementation of `Split`\n    ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n*   Fixes a division by zero in TFLite's implementation of `OneHot`\n    ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n*   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n    ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n*   Fixes a division by zero in TFLite's implementation of hashtable lookup\n    ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n*   Fixes a integer overflow in TFLite concatentation\n    ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n*   Fixes a integer overflow in TFLite memory allocation\n    ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n*   Fixes a heap OOB write in TFLite\n    ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n*   Fixes a heap OOB read in TFLite\n    ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n*   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n    ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n    ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `SparseSparseMinimum`\n    ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n*   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n    ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n*   Fixes vulnerabilities caused by invalid validation in\n    `QuantizeAndDequantizeV2`\n    ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n*   Fixes a heap buffer overflow in `BandedTriangularSolve`\n    ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n*   Fixes vulnerabilities caused by incomplete validation in\n    `tf.raw_ops.CTCLoss`\n    ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n*   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n    ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n*   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n    ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n*   Fixes a null dereference in Grappler's `TrySimplify`\n    ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n*   Fixes a crash in `tf.transpose` with complex inputs\n    ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n*   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n    ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n*   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n    ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n*   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n    ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n*   Updates `curl` to `7.76.0` to handle\n    [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n    [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n    [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n    [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n    [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n    and\n    [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n# Release 2.5.0\n\n## Major Features and Improvements\n\n*   Support for Python3.9 has been added.\n*   `tf.data`:\n    *   `tf.data` service now supports strict round-robin reads, which is useful\n        for synchronous training workloads where example sizes vary. With strict\n        round robin reads, users can guarantee that consumers get similar-sized\n        examples in the same step.\n    *   tf.data service now supports optional compression. Previously data would\n        always be compressed, but now you can disable compression by passing\n        `compression=None` to `tf.data.experimental.service.distribute(...)`.\n    *   `tf.data.Dataset.batch()` now supports `num_parallel_calls` and\n        `deterministic` arguments. `num_parallel_calls` is used to indicate that\n        multiple input batches should be computed in parallel. With\n        `num_parallel_calls` set, `deterministic` is used to indicate that\n        outputs can be obtained in the non-deterministic order.\n    *   Options returned by `tf.data.Dataset.options()` are no longer mutable.\n    *   tf.data input pipelines can now be executed in debug mode, which\n        disables any asynchrony, parallelism, or non-determinism and forces\n        Python execution (as opposed to trace-compiled graph execution) of\n        user-defined functions passed into transformations such as `map`. The\n        debug mode can be enabled through\n        `tf.data.experimental.enable_debug_mode()`.\n*   `tf.lite`\n    *   Enabled the new MLIR-based quantization backend by default\n        *   The new backend is used for 8 bits full integer post-training\n            quantization\n        *   The new backend removes the redundant rescales and fixes some bugs\n            (shared weight/bias, extremely small scales, etc)\n        *   Set `experimental_new_quantizer` in tf.lite.TFLiteConverter to False\n            to disable this change\n*   `tf.keras`\n    *   `tf.keras.metrics.AUC` now support logit predictions.\n    *   Enabled a new supported input type in `Model.fit`,\n        `tf.keras.utils.experimental.DatasetCreator`, which takes a callable,\n        `dataset_fn`. `DatasetCreator` is intended to work across all\n        `tf.distribute` strategies, and is the only input type supported for\n        Parameter Server strategy.\n*   `tf.distribute`\n    *   `tf.distribute.experimental.ParameterServerStrategy` now supports\n        training with Keras `Model.fit` when used with `DatasetCreator`.\n    *   Creating `tf.random.Generator` under `tf.distribute.Strategy` scopes is\n        now allowed (except for\n        `tf.distribute.experimental.CentralStorageStrategy` and\n        `tf.distribute.experimental.ParameterServerStrategy`). Different\n        replicas will get different random-number streams.\n*   TPU embedding support\n    *   Added `profile_data_directory` to `EmbeddingConfigSpec` in\n        `_tpu_estimator_embedding.py`. This allows embedding lookup statistics\n        gathered at runtime to be used in embedding layer partitioning\n        decisions.\n*   PluggableDevice\n    *   Third-party devices can now connect to TensorFlow as plug-ins through\n        [StreamExecutor C API](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md).\n        and\n        [PluggableDevice](https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md)\n        interface.\n        *   Add custom ops and kernels through\n            [kernel and op registration C API](https://github.com/tensorflow/community/blob/master/rfcs/20190814-kernel-and-op-registration.md).\n        *   Register custom graph optimization passes with\n            [graph optimization C API](https://github.com/tensorflow/community/blob/master/rfcs/20201027-modular-tensorflow-graph-c-api.md).\n*   [oneAPI Deep Neural Network Library (oneDNN)](https://github.com/oneapi-src/oneDNN)\n    CPU performance optimizations from\n    [Intel-optimized TensorFlow](https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html)\n    are now available in the official x86-64 Linux and Windows builds.\n    *   They are off by default. Enable them by setting the environment variable\n        `TF_ENABLE_ONEDNN_OPTS=1`.\n    *   We do not recommend using them in GPU systems, as they have not been\n        sufficiently tested with GPUs yet.\n*   TensorFlow pip packages are now built with CUDA11.2 and cuDNN 8.1.0\n\n## Breaking Changes\n\n*   The `TF_CPP_MIN_VLOG_LEVEL` environment variable has been renamed to to\n    `TF_CPP_MAX_VLOG_LEVEL` which correctly describes its effect.\n\n## Bug Fixes and Other Changes\n\n*   `tf.keras`:\n\n    *   Preprocessing layers API consistency changes:\n        *   `StringLookup` added `output_mode`, `sparse`, and\n            `pad_to_max_tokens` arguments with same semantics as\n            `TextVectorization`.\n        *   `IntegerLookup` added `output_mode`, `sparse`, and\n            `pad_to_max_tokens` arguments with same semantics as\n            `TextVectorization`. Renamed `max_values`, `oov_value` and\n            `mask_value` to `max_tokens`, `oov_token` and `mask_token` to align\n            with `StringLookup` and `TextVectorization`.\n        *   `TextVectorization` default for `pad_to_max_tokens` switched to\n            False.\n        *   `CategoryEncoding` no longer supports `adapt`, `IntegerLookup` now\n            supports equivalent functionality. `max_tokens` argument renamed to\n            `num_tokens`.\n        *   `Discretization` added `num_bins` argument for learning bins\n            boundaries through calling `adapt` on a dataset. Renamed `bins`\n            argument to `bin_boundaries` for specifying bins without `adapt`.\n    *   Improvements to model saving/loading:\n        *   `model.load_weights` now accepts paths to saved models.\n    *   Keras inputs can now be created directly from arbitrary `tf.TypeSpecs`.\n    *   Two new learning rate schedules added:\n        `tf.keras.optimizers.schedules.CosineDecay`\n        and`tf.keras.optimizers.schedules.CosineDecayRestarts`.\n\n*   `tf.data`:\n\n    *   Exposing `tf.data.experimental.ExternalStatePolicy`, which can be used\n        to control how external state should be handled during dataset\n        serialization or iterator checkpointing.\n    *   Changing `tf.data.experimental.save` to store the type specification of\n        the dataset elements. This avoids the need for explicitly specifying the\n        `element_spec` argument of `tf.data.experimental.load` when loading the\n        previously saved dataset.\n    *   Add `.element_spec` property to `tf.data.DatasetSpec` to access the\n        inner spec. This can be used to extract the structure of nested\n        datasets.\n    *   Add `tf.data.experimental.AutoShardingPolicy.HINT` which can be used to\n        provide hints to tf.distribute-based auto-sharding as to where in the\n        input pipeline to insert sharding transformations.\n    *   Make tf.data.Options persistent across `tf.function` and `GraphDef`\n        boundaries.\n\n*   XLA compilation:\n\n    *   `tf.function(experimental_compile=True)` has become a stable API,\n        renamed `tf.function(jit_compile=True)`.\n    *   XLA can now compile MirroredStrategy: the step function passed\n        to`strategy.run` can now be annoted with `jit_compile=True`.\n\n*   `tf.distribute`:\n\n    *   Rename `experimental_prefetch_to_device` in `tf.distribute.InputOptions`\n        to `experimental_fetch_to_device` to better reflect the purpose.\n\n*   `tf.lite`:\n\n    *   class `tflite::Subgraph`:\n        *   Removed the `tensors()` method and the non-const overload of the\n            `nodes_and_registration()` method, both of which were previously\n            documented as temporary and to be removed.\n            *   Uses of `tensors()` can be replaced by calling the existing\n                methods `tensors_size()` and `tensor(int)`.\n            *   Uses of the non-const overload of `nodes_and_registration` can\n                be replaced by calling the existing methods `nodes_size()` and\n                `context()`, and then calling the `GetNodeAndRegistration`\n                method in the `TfLiteContext` returned by `context()`.\n    *   NNAPI\n        *   Removed deprecated `Interpreter::UseNNAPI(bool)` C++ API.\n            *   Use `NnApiDelegate()` and related delegate configuration methods\n                directly.\n        *   Replaced the model cache key for models computation algorithm with\n            one guaranteed to be stable across runs.\n    *   16 bits quantization\n        *   Added int16x8 support for ABS, REDUCE_MAX and REDUCE_MIN operators.\n        *   Additional tests and fixes for ADD and SUB operators.\n    *   Added support for saved model's session initializer through\n        `TFLiteConverter.from_saved_model`.\n    *   Added DEPTH_TO_SPACE support in Post training quantization.\n    *   Added dynamic range quantization support for the BatchMatMul op.\n        *   Both symmetric and asymmetric quantized input tensor are supported.\n    *   Add `RFFT2D` as builtin op. (`RFFT2D` also supports `RFFTD`.) Currently\n        only supports float32 input.\n    *   Add 5D support to `SLICE` op.\n    *   TFLite Supports SingatureDef:\n        *   TFLiteConverter exports models with SignatureDef\n        *   Interpreter supports getting a list of signatures and getting\n            callable function for a given signaturedef.\n    *   Add int8 support for `ReshapeV2`.\n    *   Add experimental support for optimization with sparsity.\n    *   Add nominal support for unsigned 32-bit integer tensor types. Note that\n        very few TFLite kernels support this type natively, so its use in mobile\n        ML authoring is generally discouraged.\n    *   Add support for static hash tables through\n        `TFLiteConverter.from_saved_model`.\n    *   The Python TF Lite Interpreter bindings now has an option\n        `experimental_preserve_all_tensors` to aid in debugging conversion.\n    *   Quantized x86 execution defaults to Ruy GEMM library for platforms with\n        AVX support.\n    *   Deprecate\n        `tf.compat.v1.lite.experimental.get_potentially_supported_ops`. Use\n        `tf.lite.TFLiteConverter` directly to check whether a model is\n        convertible.\n    *   Add support to select one of three different built-in op resolvers\n    *   Enabled post training with calibrations for models that require user\n        provided TensorFlow Lite custom op libraries via\n        `converter.target_spec._experimental_custom_op_registerers`. used in\n        Python Interpreter API.\n\n*   TF Core:\n\n    *   Corrected higher-order gradients of control flow constructs (`tf.cond`,\n        `tf.while_loop`, and compositions like `tf.foldl`) computed with\n        `tf.GradientTape` inside a `tf.function`.\n    *   Changed the default step size in `gradient_checker_v2.compute_gradients`\n        to be exactly representable as a binary floating point numbers. This\n        avoids poluting gradient approximations needlessly, which is some cases\n        leads to false negatives in op gradient tests.\n    *   Added `tf.config.experimental.get_memory_info`, returning a dict with\n        the current and peak memory usage. Deprecated\n        `tf.config.experimental.get_memory_usage` in favor of this new function.\n    *   Extended `tf.config.experimental.enable_tensor_float_32_execution` to\n        control Tensor-Float-32 evaluation in RNNs.\n    *   Added a 'experimental_payloads' field to tf.errors.OpError and its\n        subclasses to support more detailed error reporting. This is inspired\n        from Abseil Status payloads:\n        https://github.com/abseil/abseil-cpp/blob/master/absl/status/status.h\n\n*   `tf.summary`:\n\n    *   New `tf.summary.graph` allows manual write of TensorFlow graph\n        (`tf.Graph` or `tf.compat.v1.GraphDef`) as a summary. This is not a\n        replacement for the trace-based API.\n\n*   Set `/d2ReducedOptimizeHugeFunctions` by default for Windows builds. This\n    provides a big compile-time speedup, and effectively raises the minimum\n    supported MSVC version to 16.4 (current: 16.8).\n\n    *   See:\n        https://groups.google.com/a/tensorflow.org/d/topic/build/SsW98Eo7l3o/discussion\n\n*   TensorRT\n\n    *   Removed the deprecated `session_config` parameter for the TF1-TRT\n        converter `TrtGraphConverter`. Previously, we issued a warning when the\n        value of the parameter is not None.\n    *   The TF2-TRT converter `TrtGraphConverterV2` takes an object of class\n        TrtConversionParams as a parameter. Removed three deprecated fields from\n        this class: `rewriter_config_template`, `is_dynamic_op`, and\n        `max_batch_size`. Previously, we issued a warning when the value of\n        `rewriter_config_template` is not None. We issued an error when the\n        value of `is_dynamic_op` is not True. We didn't use the value for\n        `max_batch_size` for building TensorRT engines. Add parameters\n        `use_dynamic_shape` to enable dynamic shape support. The default is to\n        disable dynamic shape support. Add `dynamic_shape_profile_strategy` for\n        selecting a dynamic shape profile strategy. The default is profile\n        strategy is `Range`.\n    *   Issue a warning when function get_tensorrt_rewriter_config is used.\n\n*   TF XLA\n\n    *   Add new enum value `MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED` to\n        `tf.config.experimental.mlir_bridge_rollout` to enable a \\\"safe\\\" mode.\n        This runs the MLIR bridge only when an analysis of the graph only when\n        an analysis of the graph determines that it is safe to run.\n    *   Add new enum value `MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED'\n        to`tf.config.experimental.mlir_bridge_rollout` to enable a fallback for\n        the MLIR bridge in a \\\"safe\\\" mode. This runs the MLIR bridge in a\n        FallbackEnabled mode when an analysis of the graph determines that the\n        graph does not have unsupported features.\n\n*   Deterministic Op Functionality:\n\n    *   Add determinism-unimplemented exception-throwing to the segment-sum ops.\n        When the environment variable `TF_DETERMINISTIC_OPS` is set to `\"true\"`\n        or `\"1\"` (when op-determinism is expected), an attempt to run the\n        following ops on a GPU will throw `tf.errors.UnimplementedError` (with\n        an understandable message) when `data` is a floating-point type,\n        including complex types (if supported): `tf.math.segment_prod`,\n        `tf.math.segment_sum`, `tf.math.unsorted_segment_mean`,\n        `tf.math.unsorted_segment_sqrt_n`, `tf.math.unsorted_segment_prod`,\n        `tf.math.unsorted_segment_sum`, and therefore also\n        `tf.convert_to_tensor` when `value` is of type `tf.IndexedSlices` (such\n        as in the back prop though `tf.gather` into a dense embedding). See\n        issue [39751](https://github.com/tensorflow/tensorflow/issues/39751)\n        which this change addresses, but does not solve. This exception-throwing\n        behavior can be disabled by setting the environment variable\n        `TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS` to `\"true\"` or\n        `\"1\"`. For more information about these changes, see the description in\n        pull request\n        [47772](https://github.com/tensorflow/tensorflow/pull/47772).\n    *   In previous versions of TensorFlow, when a GPU was available,\n        `tf.sparse.sparse_dense_matmul` introduced truly random noise in the\n        forward path for data of type `tf.float32` but not for data of type\n        `tf.float64` (for which there was no GPU implementation). In this\n        current release, GPU support for other floating-point types\n        (`tf.float16`, `tf.float64`, `tf.complex64`, and `tf.complex128`) has\n        been added for this op. If you were relying on the determinism of the\n        `tf.float64` CPU implementation being automatically selected because of\n        the absence of the `tf.float64` GPU implementation, you with either need\n        to force the op to run on the CPU or use a different data type.\n\n*   Security\n\n    *   Fixes a heap buffer overflow in `RaggedBinCount`\n        ([CVE-2021-29512](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29512))\n    *   Fixes a heap out of bounds write in `RaggedBinCount`\n        ([CVE-2021-29514](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29514))\n    *   Fixes a type confusion during tensor casts which leads to dereferencing\n        null pointers\n        ([CVE-2021-29513](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29513))\n    *   Fixes a reference binding to null pointer in `MatrixDiag*` ops\n        ([CVE-2021-29515](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29515))\n    *   Fixes a null pointer dereference via invalid Ragged Tensors\n        ([CVE-2021-29516](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29516))\n    *   Fixes a division by zero in `Conv3D`\n        ([CVE-2021-29517](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29517))\n    *   Fixes vulnerabilities where session operations in eager mode lead to\n        null pointer dereferences\n        ([CVE-2021-29518](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29518))\n    *   Fixes a `CHECK`-fail in `SparseCross` caused by type confusion\n        ([CVE-2021-29519](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29519))\n    *   Fixes a segfault in `SparseCountSparseOutput`\n        ([CVE-2021-29521](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29521))\n    *   Fixes a heap buffer overflow in `Conv3DBackprop*`\n        ([CVE-2021-29520](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29520))\n    *   Fixes a division by 0 in `Conv3DBackprop*`\n        ([CVE-2021-29522](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29522))\n    *   Fixes a `CHECK`-fail in `AddManySparseToTensorsMap`\n        ([CVE-2021-29523](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29523))\n    *   Fixes a division by 0 in `Conv2DBackpropFilter`\n        ([CVE-2021-29524](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29524))\n    *   Fixes a division by 0 in `Conv2DBackpropInput`\n        ([CVE-2021-29525](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29525))\n    *   Fixes a division by 0 in `Conv2D`\n        ([CVE-2021-29526](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29526))\n    *   Fixes a division by 0 in `QuantizedConv2D`\n        ([CVE-2021-29527](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29527))\n    *   Fixes a division by 0 in `QuantizedMul`\n        ([CVE-2021-29528](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29528))\n    *   Fixes vulnerabilities caused by invalid validation in\n        `SparseMatrixSparseCholesky`\n        ([CVE-2021-29530](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29530))\n    *   Fixes a heap buffer overflow caused by rounding\n        ([CVE-2021-29529](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29529))\n    *   Fixes a `CHECK`-fail in `tf.raw_ops.EncodePng`\n        ([CVE-2021-29531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29531))\n    *   Fixes a heap out of bounds read in `RaggedCross`\n        ([CVE-2021-29532](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29532))\n    *   Fixes a `CHECK`-fail in `DrawBoundingBoxes`\n        ([CVE-2021-29533](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29533))\n    *   Fixes a heap buffer overflow in `QuantizedMul`\n        ([CVE-2021-29535](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29535))\n    *   Fixes a `CHECK`-fail in `SparseConcat`\n        ([CVE-2021-29534](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29534))\n    *   Fixes a heap buffer overflow in `QuantizedResizeBilinear`\n        ([CVE-2021-29537](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29537))\n    *   Fixes a heap buffer overflow in `QuantizedReshape`\n        ([CVE-2021-29536](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29536))\n    *   Fixes a division by zero in `Conv2DBackpropFilter`\n        ([CVE-2021-29538](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29538))\n    *   Fixes a heap buffer overflow in `Conv2DBackpropFilter`\n        ([CVE-2021-29540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29540))\n    *   Fixes a heap buffer overflow in `StringNGrams`\n        ([CVE-2021-29542](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29542))\n    *   Fixes a null pointer dereference in `StringNGrams`\n        ([CVE-2021-29541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29541))\n    *   Fixes a `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n        ([CVE-2021-29544](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29544))\n    *   Fixes a `CHECK`-fail in `CTCGreedyDecoder`\n        ([CVE-2021-29543](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29543))\n    *   Fixes a heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n        ([CVE-2021-29545](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29545))\n    *   Fixes a division by 0 in `QuantizedBiasAdd`\n        ([CVE-2021-29546](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29546))\n    *   Fixes a heap out of bounds in\n        `QuantizedBatchNormWithGlobalNormalization`\n        ([CVE-2021-29547](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29547))\n    *   Fixes a division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n        ([CVE-2021-29548](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29548))\n    *   Fixes a division by 0 in `QuantizedAdd`\n        ([CVE-2021-29549](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29549))\n    *   Fixes a division by 0 in `FractionalAvgPool`\n        ([CVE-2021-29550](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29550))\n    *   Fixes an OOB read in `MatrixTriangularSolve`\n        ([CVE-2021-29551](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29551))\n    *   Fixes a heap OOB in `QuantizeAndDequantizeV3`\n        ([CVE-2021-29553](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29553))\n    *   Fixes a `CHECK`-failure in `UnsortedSegmentJoin`\n        ([CVE-2021-29552](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29552))\n    *   Fixes a division by 0 in `DenseCountSparseOutput`\n        ([CVE-2021-29554](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29554))\n    *   Fixes a division by 0 in `FusedBatchNorm`\n        ([CVE-2021-29555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29555))\n    *   Fixes a division by 0 in `SparseMatMul`\n        ([CVE-2021-29557](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29557))\n    *   Fixes a division by 0 in `Reverse`\n        ([CVE-2021-29556](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29556))\n    *   Fixes a heap buffer overflow in `SparseSplit`\n        ([CVE-2021-29558](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558))\n    *   Fixes a heap OOB access in unicode ops\n        ([CVE-2021-29559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29559))\n    *   Fixes a heap buffer overflow in `RaggedTensorToTensor`\n        ([CVE-2021-29560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29560))\n    *   Fixes a `CHECK`-fail in `LoadAndRemapMatrix`\n        ([CVE-2021-29561](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29561))\n    *   Fixes a `CHECK`-fail in `tf.raw_ops.IRFFT`\n        ([CVE-2021-29562](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29562))\n    *   Fixes a `CHECK`-fail in `tf.raw_ops.RFFT`\n        ([CVE-2021-29563](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29563))\n    *   Fixes a null pointer dereference in `EditDistance`\n        ([CVE-2021-29564](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29564))\n    *   Fixes a null pointer dereference in `SparseFillEmptyRows`\n        ([CVE-2021-29565](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29565))\n    *   Fixes a heap OOB access in `Dilation2DBackpropInput`\n        ([CVE-2021-29566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29566))\n    *   Fixes a reference binding to null in `ParameterizedTruncatedNormal`\n        ([CVE-2021-29568](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29568))\n    *   Fixes a set of vulnerabilities caused by lack of validation in\n        `SparseDenseCwiseMul`\n        ([CVE-2021-29567](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29567))\n    *   Fixes a heap out of bounds read in `MaxPoolGradWithArgmax`\n        ([CVE-2021-29570](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29570))\n    *   Fixes a heap out of bounds read in `RequantizationRange`\n        ([CVE-2021-29569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29569))\n    *   Fixes a memory corruption in `DrawBoundingBoxesV2`\n        ([CVE-2021-29571](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29571))\n    *   Fixes a reference binding to nullptr in `SdcaOptimizer`\n        ([CVE-2021-29572](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29572))\n    *   Fixes an overflow and a denial of service in\n        `tf.raw_ops.ReverseSequence`\n        ([CVE-2021-29575](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29575))\n    *   Fixes a division by 0 in `MaxPoolGradWithArgmax`\n        ([CVE-2021-29573](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29573))\n    *   Fixes an undefined behavior in `MaxPool3DGradGrad`\n        ([CVE-2021-29574](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29574))\n    *   Fixes a heap buffer overflow in `MaxPool3DGradGrad`\n        ([CVE-2021-29576](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29576))\n    *   Fixes a heap buffer overflow in `AvgPool3DGrad`\n        ([CVE-2021-29577](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29577))\n    *   Fixes an undefined behavior and a `CHECK`-fail in\n        `FractionalMaxPoolGrad`\n        ([CVE-2021-29580](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29580))\n    *   Fixes a heap buffer overflow in `FractionalAvgPoolGrad`\n        ([CVE-2021-29578](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29578))\n    *   Fixes a heap buffer overflow in `MaxPoolGrad`\n        ([CVE-2021-29579](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29579))\n    *   Fixes a segfault in `CTCBeamSearchDecoder`\n        ([CVE-2021-29581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29581))\n    *   Fixes a heap OOB read in `tf.raw_ops.Dequantize`\n        ([CVE-2021-29582](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29582))\n    *   Fixes a `CHECK`-fail due to integer overflow\n        ([CVE-2021-29584](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29584))\n    *   Fixes a heap buffer overflow and undefined behavior in `FusedBatchNorm`\n        ([CVE-2021-29583](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29583))\n    *   Fixes a division by zero in padding computation in TFLite\n        ([CVE-2021-29585](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29585))\n    *   Fixes a division by zero in optimized pooling implementations in TFLite\n        ([CVE-2021-29586](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29586))\n    *   Fixes a division by zero in TFLite's implementation of `SpaceToDepth`\n        ([CVE-2021-29587](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29587))\n    *   Fixes a division by zero in TFLite's implementation of `GatherNd`\n        ([CVE-2021-29589](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29589))\n    *   Fixes a division by zero in TFLite's implementation of `TransposeConv`\n        ([CVE-2021-29588](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29588))\n    *   Fixes a heap OOB read in TFLite's implementation of `Minimum` or\n        `Maximum`\n        ([CVE-2021-29590](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29590))\n    *   Fixes a null pointer dereference in TFLite's `Reshape` operator\n        ([CVE-2021-29592](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29592))\n    *   Fixes a stack overflow due to looping TFLite subgraph\n        ([CVE-2021-29591](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29591))\n    *   Fixes a division by zero in TFLite's implementation of `DepthToSpace`\n        ([CVE-2021-29595](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595))\n    *   Fixes a division by zero in TFLite's convolution code\n        ([CVE-2021-29594](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29594))\n    *   Fixes a division by zero in TFLite's implementation of `EmbeddingLookup`\n        ([CVE-2021-29596](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29596))\n    *   Fixes a division by zero in TFLite's implementation of `BatchToSpaceNd`\n        ([CVE-2021-29593](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29593))\n    *   Fixes a division by zero in TFLite's implementation of `SpaceToBatchNd`\n        ([CVE-2021-29597](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29597))\n    *   Fixes a division by zero in TFLite's implementation of `SVDF`\n        ([CVE-2021-29598](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29598))\n    *   Fixes a division by zero in TFLite's implementation of `Split`\n        ([CVE-2021-29599](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29599))\n    *   Fixes a division by zero in TFLite's implementation of `OneHot`\n        ([CVE-2021-29600](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29600))\n    *   Fixes a division by zero in TFLite's implementation of `DepthwiseConv`\n        ([CVE-2021-29602](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29602))\n    *   Fixes a division by zero in TFLite's implementation of hashtable lookup\n        ([CVE-2021-29604](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29604))\n    *   Fixes a integer overflow in TFLite concatentation\n        ([CVE-2021-29601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29601))\n    *   Fixes a integer overflow in TFLite memory allocation\n        ([CVE-2021-29605](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29605))\n    *   Fixes a heap OOB write in TFLite\n        ([CVE-2021-29603](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29603))\n    *   Fixes a heap OOB read in TFLite\n        ([CVE-2021-29606](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29606))\n    *   Fixes a heap OOB and null pointer dereference in `RaggedTensorToTensor`\n        ([CVE-2021-29608](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29608))\n    *   Fixes vulnerabilities caused by incomplete validation in `SparseAdd`\n        ([CVE-2021-29609](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29609))\n    *   Fixes vulnerabilities caused by incomplete validation in\n        `SparseSparseMinimum`\n        ([CVE-2021-29607](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29607))\n    *   Fixes vulnerabilities caused by incomplete validation in `SparseReshape`\n        ([CVE-2021-29611](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29611))\n    *   Fixes vulnerabilities caused by invalid validation in\n        `QuantizeAndDequantizeV2`\n        ([CVE-2021-29610](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29610))\n    *   Fixes a heap buffer overflow in `BandedTriangularSolve`\n        ([CVE-2021-29612](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29612))\n    *   Fixes vulnerabilities caused by incomplete validation in\n        `tf.raw_ops.CTCLoss`\n        ([CVE-2021-29613](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29613))\n    *   Fixes an interpreter crash from vulnerabilities in `tf.io.decode_raw`\n        ([CVE-2021-29614](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29614))\n    *   Fixes a stack overflow in `ParseAttrValue` with nested tensors\n        ([CVE-2021-29615](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29615))\n    *   Fixes a null dereference in Grappler's `TrySimplify`\n        ([CVE-2021-29616](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29616))\n    *   Fixes a crash in `tf.transpose` with complex inputs\n        ([CVE-2021-29618](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29618))\n    *   Fixes a crash in `tf.strings.substr` due to `CHECK`-fail\n        ([CVE-2021-29617](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29617))\n    *   Fixes a segfault in `tf.raw_ops.SparseCountSparseOutput`\n        ([CVE-2021-29619](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29619))\n    *   Fixes a segfault in `tf.raw_ops.ImmutableConst`\n        ([CVE-2021-29539](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29539))\n    *   Updates `curl` to `7.76.0` to handle\n        [CVE-2020-8169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8169),\n        [CVE-2020-8177](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8177),\n        [CVE-2020-8231](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8231),\n        [CVE-2020-8284](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8284),\n        [CVE-2020-8285](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8285)\n        and\n        [CVE-2020-8286](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8286).\n\n*   Other\n\n    *   Added `show_debug_info` to `mlir.convert_graph_def` and\n        `mlir.convert_function`.\n    *   Added\n        [Arm Compute Library (ACL)](https://github.com/ARM-software/ComputeLibrary)\n        support to `--config=mkl_aarch64` build.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n8bitmp3, Aaron S. Mondal, Abhilash Mahendrakar, Abhinav Upadhyay, Abhishek\nKulkarni, Abolfazl Shahbazi, Adam Hillier, Aditya Kane, Ag Ramesh, ahmedsabie,\nAlbert Villanova Del Moral, Aleksey Vitebskiy, Alex Hoffman, Alexander Bayandin,\nAlfie Edwards, Aman Kishore, Amogh Joshi, andreABbauer, Andrew Goodbody, Andrzej\nPomirski, Artemiy Ryabinkov, Ashish Jha, ather, Ayan Moitra, Bairen Yi, Bart\nRibbers, Bas Aarts, Behzad Abghari, Ben Arnao, Ben Barsdell, Benjamin Klimczak,\nbhack, Brendan Collins, Can Wang, Cheng Ren, Chris Leary, Chris Olivier, Clemens\nGiuliani, Cloud Han, Corey Cole, Cui, Yifeng, Cuong V. Nguyen, Daniel Moore,\nDawid Wojciechowski, Ddavis-2015, Dean Wyatte, Denisa Roberts, dependabot[bot],\nDmitry Volodin, Dominic Jack, Duncan Riach, dushuai, Elena Zhelezina, Eli\nOsherovich, Erik Smistad, ewsn1593, Felix Fent, fo40225, Fran\u00e7ois Chollet,\nFrederic Bastien, Freedom\" Koan-Sin Tan, fsx950223, ganand1, gbaned, Georgiy\nManuilov, gerbauz, Guillaume Klein, Guozhong Zhuang, Harry Slatyer, Harsh188,\nhenri, Henri Woodcock, Hiran Sarkar, Hollow Man, H\u00e5kon Sandsmark, I Wayan\nDharmana, icysapphire, Ikko Ashimine, Jab Hofmeier, Jack Hessel, Jacob Valdez,\nJakub Jatczak, James Bernardi, Jared Smolens, Jason Zaman, jedlimlx, Jenny\nPlunkett, Jens Elofsson, Jerry Shih, jgehw, Jia Fu Low, Jim Fisher, jpodivin,\nJulien Stephan, Jungsub Lim, Junha Park, Junhyuk So, justkw, Kaixi Hou,\nkashyapraval, Kasra Bigdeli, Kazuaki Ishizaki, Keith Mok, Kevin Cheng, kopytjuk,\nKristian Hartikainen, ksood12345, Kulin Seth, kushanam, latyas, Lequn Chen,\nLeslie-Fang, Long M. L\u01b0u, Lukas Geiger, machineko, Mahmoud Abuzaina, Manish, Mao\nYunfei, Maozhou, Ge, Marcin Juszkiewicz, Marcin Owsiany, Marconi Jiang, Marcos\nPereira, Maria Romanenko Vexlard, Maria Vexlard, Marius Brehler, marload, Martin\nKubov\u010d\u00edk, Matej, Mateusz Holenko, Maxiwell S. Garcia, Mazhar, mazharul,\nmbhuiyan, mdfaijul, Michael Gielda, Michael Kuchnik, Michal Szutenberg, Mikhail\nStepanov, Milan Straka, Mitchel Humpherys, Mohamed Moselhy, Mohamed Nour\nAbouelseoud, M\u00e5ns Bermell, M\u00e5ns Nilsson, Nathan Luehr, Nico Jahn, Niroop\nAmmbashankar, Oceania2018, Omri Steiner, Orivej Desh, Oskar Flordal, oujiafan,\nPatrik Laurell, Paul B. Isaac'S, Paul Klinger, Pawel Piskorski, Pedro Marques,\nPhat Tran, Piotr Zierhoffer, piyushdatta, Pnikam-Cad, Prashant Kumar, Prateek\nGupta, PratsBhatt, Pravin Karandikar, qqq.jq, QQ\u55b5, Quintin, Rama Ketineni,\nravikyram, Rehan Guha, rhdong, rmothukuru, Roger Cheng, Rohit Santhanam, rposts,\nRsanthanam-Amd, rsun, Rsun-Bdti, Ryan Kuester, ryanking13, Saduf2019, Sami Kama,\nSamuel Marks, Scott Tseng, Sean Moriarity, Sergey Popov, Sergii Khomenko, Sheng,\nYang, shwetaoj, Sidong-Wei, Simon Maurer, Simrit Kaur, Srini511, Srinivasan\nNarayanamoorthy, Stephan, Stephen Matthews, Sungmann Cho, Sunoru, Suraj Sudhir,\nSuraj Upadhyay, Taebum Kim, Takayoshi Koizumi, Tamas Bela Feher, Teng Lu,\nThibaut Goetghebuer-Planchon, Tomwildenhain-Microsoft, Tony, Traun Leyden, Trent\nLo, TVLIgnacy, Tzu-Wei Sung, vaibhav, Vignesh Kothapalli, Vikram Dattu,\nviktprog, Vinayaka Bandishti, Vincent Abriou, Vishakha Agrawal, Vivek Panyam,\nVladimir Silyaev, V\u00f5 V\u0103n Ngh\u0129a, wamuir, Wang, Yanzhang, wangsiyu, Waqar Hameed,\nwxinix, Xiao Yang, xiaohong1031, Xiaoming (Jason) Cui, Xinan Jiang, Yair\nEhrenwald, Yajush Vyas, Yasir Modak, Yimei Sun, Yong Tang, Yosshi999,\nyoushenmebutuo, yqtianust, Yuan Tang, yuanbopeng, Yuriy Chernyshov, Yuta\nFukasawa, Zachary Deane-Mayer, Zeno Gantner, Zhoulong Jiang, zhuyie, zilinzhu,\n\u5f6d\u9707\u4e1c\n\n# Release 2.4.1\n\n* This release removes the AVX2 requirement from TF 2.4.0.\n\n# Release 2.3.2\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Solves an OOM issue on TPUs when XLA contexts use fused average updates\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n\n# Release 2.2.2\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Prevents memory leaks in loading `SavedModel`s that import functions\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n\n# Release 2.1.3\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n* Newer ROCm versions are supported on the 2.1 branch.\n\n# Release 2.0.4\n\nNote that this is the last patch release for the TensorFlow 2.0.x series.\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n\n# Release 1.15.5\n\nNote that this is the last patch release for the TensorFlow 1.x series.\n\n## Bug Fixes and Other Changes\n* Fixes an access to unitialized memory in Eigen code\n  ([CVE-2020-26266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26266))\n* Fixes a security vulnerability caused by lack of validation in\n  `tf.raw_ops.DataFormatVecPermute` and `tf.raw_ops.DataFormatDimMap`\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n* Fixes a vulnerability caused by attempting to write to immutable memory region in\n  `tf.raw_ops.ImmutableConst`\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268)\n* Fixes a `CHECK`-fail in LSTM with zero-length input\n  ([CVE-2020-26270](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26270))\n* Fixes a security vulnerability caused by accessing heap data outside of bounds\n  when loading a specially crafted `SavedModel`\n  ([CVE-2020-26271](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26271))\n* Updates `libjpeg-turbo` to `2.0.5` to handle\n  [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790).\n* Updates `junit` to `4.13.1` to handle\n  [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250).\n* Updates `PCRE` to `8.44` to handle\n  [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838)\n  and\n  [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155).\n* Updates `sqlite3` to `3.44.0` to keep in sync with master branch.\n\n# Release 2.4.0\n\n ## Major Features and Improvements\n\n* `tf.distribute` introduces experimental support for asynchronous training of\n  models via the [`tf.distribute.experimental.ParameterServerStrategy`]\n  (https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy)\n  API. Please see the [tutorial](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n  to learn more.\n\n* [`MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy)\n  is now a stable API and is no longer considered experimental. Some of the\n  major improvements involve handling peer failure and many bug fixes. Please\n  check out the detailed tutorial on [Multi-worker training with Keras]\n  (https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras).\n\n* Introduces experimental support for a new module named [`tf.experimental.numpy`]\n  (https://www.tensorflow.org/api_docs/python/tf/experimental/numpy) which is a\n  NumPy-compatible API for writing TF programs. See the [detailed guide]\n  (https://www.tensorflow.org/guide/tf_numpy) to learn more. Additional details below.\n\n* Adds Support for\n  [TensorFloat-32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)\n  on Ampere based GPUs. TensorFloat-32, or TF32 for short, is a math mode for\n  NVIDIA Ampere based GPUs and is enabled by default.\n\n* A major refactoring of the internals of the Keras Functional API has been\n  completed, that should improve the reliability, stability, and performance of\n  constructing Functional models.\n\n* Keras mixed precision API [`tf.keras.mixed_precision`]\n  (https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision?version=nightly)\n  is no longer experimental and allows the use of 16-bit floating point formats\n  during training, improving performance by up to 3x on GPUs and 60% on TPUs.\n  Please see below for additional details.\n\n* TensorFlow Profiler now supports profiling `MultiWorkerMirroredStrategy` and\n  tracing multiple workers using the [sampling mode API]\n  (https://www.tensorflow.org/guide/profiler#profiling_apis).\n\n* TFLite Profiler for Android is available. See the detailed [guide]\n  (https://www.tensorflow.org/lite/performance/measurement#trace_tensorflow_lite_internals_in_android)\n  to learn more.\n\n* TensorFlow pip packages are now built with CUDA11 and cuDNN 8.0.2.\n\n## Breaking Changes\n\n* TF Core:\n  * Certain float32 ops run in lower precision on Ampere based GPUs, including\n  matmuls and convolutions, due to the use of [TensorFloat-32]\n  (https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/).\n  Specifically, inputs to such ops are rounded from 23 bits of precision to 10\n  bits of precision. This is unlikely to cause issues in practice for deep learning\n  models. In some cases, TensorFloat-32 is also used for complex64 ops.\n  TensorFloat-32 can be disabled by running `tf.config.experimental.enable_tensor_float_32_execution(False)`.\n  * The byte layout for string tensors across the C-API has been updated to match\n  TF Core/C++; i.e., a contiguous array of `tensorflow::tstring`/`TF_TString`s.\n  * C-API functions `TF_StringDecode`, `TF_StringEncode`, and `TF_StringEncodedSize`\n  are no longer relevant and have been removed; see `core/platform/ctstring.h` for\n  string access/modification in C.\n  * `tensorflow.python`, `tensorflow.core` and `tensorflow.compiler` modules are\n  now hidden. These modules are not part of TensorFlow public API.\n  * `tf.raw_ops.Max` and `tf.raw_ops.Min` no longer accept inputs of type\n  `tf.complex64` or `tf.complex128`, because the behavior of these ops is not\n  well defined for complex types.\n  * XLA:CPU and XLA:GPU devices are no longer registered by default. Use\n  `TF_XLA_FLAGS=--tf_xla_enable_xla_devices` if you really need them, but this\n  flag will eventually be removed in subsequent releases.\n\n* `tf.keras`:\n  * The `steps_per_execution` argument in `model.compile()` is no longer experimental;\n  if you were passing `experimental_steps_per_execution`, rename it to\n  `steps_per_execution` in your code. This argument controls the number of batches\n  to run during each `tf.function` call when calling `model.fit()`. Running multiple\n  batches inside a single `tf.function` call can greatly improve performance on\n  TPUs or small models with a large Python overhead.\n  * A **major refactoring** of the internals of the Keras Functional API may affect code that\n  is relying on certain internal details:\n    * Code that uses `isinstance(x, tf.Tensor)` instead of `tf.is_tensor` when\n  checking Keras symbolic inputs/outputs should switch to using `tf.is_tensor`.\n    * Code that is overly dependent on the exact names attached to symbolic tensors\n  (e.g. assumes there will be \":0\" at the end of the inputs, treats names as\n  unique identifiers instead of using `tensor.ref()`, etc.) may break.\n    * Code that uses full path for `get_concrete_function` to trace Keras symbolic\n  inputs directly should switch to building matching `tf.TensorSpec`s directly and\n  tracing the `TensorSpec` objects.\n    * Code that relies on the exact number and names of the op layers that TensorFlow\n  operations  were converted into may have changed.\n    * Code that uses `tf.map_fn`/`tf.cond`/`tf.while_loop`/control flow as op layers\n  and  happens to work before TF 2.4. These will explicitly be unsupported now.\n  Converting these ops to Functional API op layers was unreliable before TF 2.4,\n  and prone to erroring incomprehensibly  or being silently buggy.\n    * Code that directly asserts on a Keras symbolic value in cases where ops\n  like `tf.rank` used to  return a static or symbolic value depending on if the\n  input had a fully static shape or not. Now these ops always return symbolic values.\n    * Code already susceptible to leaking tensors outside of graphs becomes slightly\n  more likely to do so now.\n    * Code that tries directly getting gradients with respect to symbolic Keras\n  inputs/outputs. Use `GradientTape` on the actual Tensors passed to the already-constructed\n  model instead.\n    * Code that requires very tricky shape manipulation via converted op layers\n  in order to work, where the Keras symbolic shape inference proves insufficient.\n    * Code that tries manually walking a `tf.keras.Model` layer by layer and assumes\n  layers only ever have one positional argument. This assumption doesn't hold\n  true before TF 2.4 either, but is more likely to cause issues now.\n    * Code that manually enters `keras.backend.get_graph()` before building a\n  functional model is no longer needed.\n    * Start enforcing input shape assumptions when calling Functional API Keras\n  models. This may potentially break some users, in case there is a mismatch\n  between the shape used when creating `Input` objects in a Functional model,\n  and the shape of the data passed to that model. You can fix this mismatch by\n  either calling the model with correctly-shaped data, or by relaxing `Input` shape\n  assumptions (note that you can pass shapes with `None` entries for axes that\n  are meant to be dynamic). You can also disable the input checking entirely by\n  setting `model.input_spec = None`.\n  * Several changes have been made to `tf.keras.mixed_precision.experimental`.\n  Note that it is now recommended to use the non-experimental\n  `tf.keras.mixed_precision` API.\n   * `AutoCastVariable.dtype` now refers to the actual variable dtype, not the\n  dtype it will be casted to.\n   * When mixed precision is enabled, `tf.keras.layers.Embedding` now outputs a\n  float16 or bfloat16 tensor instead of a float32 tensor.\n   * The property `tf.keras.mixed_precision.experimental.LossScaleOptimizer.loss_scale`\n  is now a tensor, not a `LossScale` object. This means to get a loss scale\n  of a `LossScaleOptimizer` as a tensor, you must now call `opt.loss_scale`instead of `opt.loss_scale()`.\n   * The property `should_cast_variables` has been removed from `tf.keras.mixed_precision.experimental.Policy`\n   * When passing a `tf.mixed_precision.experimental.DynamicLossScale` to `tf.keras.mixed_precision.experimental.LossScaleOptimizer`,\n  the `DynamicLossScale`'s multiplier must be 2.\n   * When passing a `tf.mixed_precision.experimental.DynamicLossScale` to\n  `tf.keras.mixed_precision.experimental.LossScaleOptimizer`, the weights of\n  the `DynanmicLossScale` are copied into the `LossScaleOptimizer` instead of being reused.\n  This means modifying the weights of the `DynamicLossScale` will no longer affect the weights of the LossScaleOptimizer, and vice versa.\n   * The global policy can no longer be set to a non-floating point policy in `tf.keras.mixed_precision.experimental.set_policy`\n   * In `Layer.call`, `AutoCastVariable`s will no longer be casted within\n  `MirroredStrategy.run` or `ReplicaContext.merge_call`. This is because a thread local\n  variable is used to determine whether `AutoCastVariable`s are casted, and those\n  two functions run with a different thread. Note this only applies if one of\n  these two functions is called within `Layer.call`; if one of those two functions calls `Layer.call`, `AutoCastVariable`s will still be casted.\n\n* `tf.data`:\n  * `tf.data.experimental.service.DispatchServer` now takes a config tuple\n  instead of individual arguments. Usages should be updated to\n  `tf.data.experimental.service.DispatchServer(dispatcher_config)`.\n  * `tf.data.experimental.service.WorkerServer` now takes a config tuple instead\n  of individual arguments. Usages should be updated to  `tf.data.experimental.service.WorkerServer(worker_config)`.\n\n* `tf.distribute`:\n  * Removes `tf.distribute.Strategy.experimental_make_numpy_dataset`. Please use\n  `tf.data.Dataset.from_tensor_slices` instead.\n  * Renames `experimental_hints` in `tf.distribute.StrategyExtended.reduce_to`,\n  `tf.distribute.StrategyExtended.batch_reduce_to`, `tf.distribute.ReplicaContext.all_reduce`\n  to `options`.\n  * Renames `tf.distribute.experimental.CollectiveHints` to `tf.distribute.experimental.CommunicationOptions`.\n  * Renames `tf.distribute.experimental.CollectiveCommunication` to `tf.distribute.experimental.CommunicationImplementation`.\n  * Renames `tf.distribute.Strategy.experimental_distribute_datasets_from_function` to `distribute_datasets_from_function` as it is no longer experimental.\n  * Removes `tf.distribute.Strategy.experimental_run_v2` method, which was deprecated in TF 2.2.\n\n* `tf.lite`:\n  * `tf.quantization.quantize_and_dequantize_v2` has been introduced, which updates the gradient definition for quantization which is outside the range\n     to be 0. To simulate the V1 the behavior of `tf.quantization.quantize_and_dequantize(...)` use\n  `tf.grad_pass_through(tf.quantization.quantize_and_dequantize_v2)(...)`.\n\n* Building TensorFlow:\n  * Windows platform builds: TensorFlow on Windows under MSVC is now built with\n  `--copt=/experimental:preprocessor --host_copt=/experimental:preprocessor`\n  (see `.bazelrc` for more details). Builds including TensorFlow may fail with\n  unexpected syntax errors if these flags are absent. See also\n  [this thread on SIG Build](https://groups.google.com/a/tensorflow.org/g/build/c/LbAw8RILvTg/m/ttnuhYU2BgAJ).\n\n## Known Caveats\n  * `tf.keras.mixed_precision`\n    * When using mixed precision, calling `RMSprop.apply_gradients` or\n  `Nadam.apply_gradients` outside a `tf.function` does not work and will raise\n  the AttributeError \"Tensor.op is meaningless when eager execution is enabled\".\n  See this [issue](https://github.com/tensorflow/tensorflow/issues/45536) for details and a workaround.\n\n## Bug Fixes and Other Changes\n\n### TF Core:\n  * Introduces experimental support for a new module named [`tf.experimental.numpy`]\n  (https://www.tensorflow.org/api_docs/python/tf/experimental/numpy), which is a\n  NumPy-compatible API for writing TF programs. This module provides class\n  `ndarray`, which mimics the `ndarray` class in NumPy, and wraps an immutable\n  `tf.Tensor` under the hood. A subset of NumPy functions (e.g. `numpy.add`) are\n  provided. Their inter-operation with TF facilities is seamless in most cases.\n    See [tensorflow/python/ops/numpy_ops/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/README.md)\n    for details of what operations are supported and what are the differences\n  from NumPy.\n  * `tf.types.experimental.TensorLike` is a new `Union` type that can be used as\n  type annotation for variables representing a Tensor or a value\n    that can be converted to Tensor by `tf.convert_to_tensor`.\n  * Calling ops with a python constants or numpy values is now consistent with\n  tf.convert_to_tensor behavior. This avoids operations like\n    tf.reshape truncating inputs such as from int64 to int32.\n  * Adds `tf.sparse.map_values` to apply a function to the `.value`s of\n  `SparseTensor` arguments.\n  * The Python bitwise operators for `Tensor` (`__and__`, `__or__`, `__xor__` and `__invert__` now support non-`bool`\n  arguments and apply the corresponding bitwise ops. `bool` arguments continue\n  to be supported and dispatch to logical ops. This brings them more in line with\n  Python and NumPy behavior.\n  * Adds `tf.SparseTensor.with_values`. This returns a new SparseTensor with the same sparsity pattern, but with new provided values. It is\n    similar to the `with_values` function of `RaggedTensor`.\n  * Adds `StatelessCase` op, and uses it if none of case branches has stateful ops.\n  * Adds `tf.config.experimental.get_memory_usage` to return total memory usage of the device.\n  * Adds gradients for `RaggedTensorToVariant` and `RaggedTensorFromVariant`.\n  * Improve shape inference of nested function calls by supporting constant\n  folding across Arg nodes which makes more static values available to shape\n  inference functions.\n* `tf.debugging`:\n  * `tf.debugging.assert_shapes()` now works on `SparseTensor`s (Fixes [#36268](https://github.com/tensorflow/tensorflow/issues/36268)).\n* GPU\n  * Adds Support for [TensorFloat-32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)\n  on Ampere based GPUs.TensorFloat-32, or TF32 for short, is a math mode for\n  NVIDIA Ampere based GPUs which causes certain float32 ops, such as matrix\n  multiplications and convolutions, to run much faster on Ampere GPUs but with\n  reduced precision. This reduced precision has not been found to effect\n  convergence quality of deep learning models in practice. TensorFloat-32 is\n  enabled by default, but can be disabled with `tf.config.experimental.enable_tensor_float_32_execution`.\n* `tf.math`:\n  * Adds `tf.math.erfcinv`, the inverse to `tf.math.erfc`.\n* `tf.nn`:\n  *   `tf.nn.max_pool2d` now supports explicit padding.\n* `tf.image`:\n  * Adds deterministic `tf.image.stateless_random_*` functions for each\n  `tf.image.random_*` function. Added a new op `stateless_sample_distorted_bounding_box`\n  which is a deterministic version of `sample_distorted_bounding_box` op.\n  Given the same seed, these stateless functions/ops produce the same results\n  independent of how many times the function is called, and independent of global seed settings.\n  * Adds deterministic `tf.image.resize` backprop CUDA kernels for\n  `method=ResizeMethod.BILINEAR` (the default method). Enable by setting the environment\n  variable `TF_DETERMINISTIC_OPS` to `\"true\"` or `\"1\"`.\n* `tf.print`:\n  * Bug fix in `tf.print()` with `OrderedDict` where if an `OrderedDict`\n  didn't have the keys sorted, the keys and values were not being printed\n    in accordance with their correct mapping.\n* `tf.train.Checkpoint`:\n  * Now accepts a `root` argument in the initialization, which generates a\n  checkpoint with a root object. This allows users to create a `Checkpoint`\n  object that     is compatible with Keras `model.save_weights()` and\n  `model.load_weights`. The checkpoint is also compatible with the checkpoint\n  saved in the `variables/` folder in the SavedModel.\n  * When restoring, `save_path` can be a path to a SavedModel. The function will\n  automatically find the checkpoint in the SavedModel.\n\n### `tf.data`:\n  * Adds new `tf.data.experimental.service.register_dataset` and\n  `tf.data.experimental.service.from_dataset_id` APIs to enable one process to\n  register a dataset with the tf.data service, and another process to consume\n  data from the dataset.\n  * Adds support for dispatcher fault tolerance. To enable fault tolerance,\n  configure a `work_dir` when running your dispatcher server and set\n  `dispatcher_fault_tolerance=True`. The dispatcher will store its state to\n  `work_dir`, so that on restart it can continue from its previous state after restart.\n  * Adds support for sharing dataset graphs via shared filesystem instead of\n  over RPC. This reduces load on the dispatcher, improving performance\n    of distributing datasets. For this to work, the dispatcher's `work_dir`\n  must be accessible from workers. If the worker fails to read from the `work_dir`,\n  it falls back to using RPC for dataset graph transfer.\n  * Adds support for a new \"distributed_epoch\" processing mode.\n  This processing mode distributes a dataset across all tf.data workers,\n    instead of having each worker process the full dataset. See\n  [the tf.data service docs](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service#understand_processing_mode)\n  to learn more.\n  * Adds optional `exclude_cols` parameter to CsvDataset. This parameter is the\n  complement of `select_cols`; at most one of these should be specified.\n  * We have implemented an optimization which reorders data-discarding\n  transformations such as `take` and `shard` to happen earlier in the dataset\n  when it is safe to do so. The optimization can be disabled via the\n  `experimental_optimization.reorder_data_discarding_ops` dataset option.\n  * `tf.data.Options` were previously immutable and can now be overridden.\n  * `tf.data.Dataset.from_generator` now supports Ragged and Sparse tensors with\n  a new `output_signature` argument, which allows `from_generator` to produce any\n  type describable by a `tf.TypeSpec`.\n  * `tf.data.experimental.AUTOTUNE` is now available in the core API as `tf.data.AUTOTUNE`.\n\n### `tf.distribute`:\n  * Introduces experimental support for asynchronous training of models via\n  `tf.distribute.experimental.ParameterServerStrategy`:\n    * Replaces the existing `tf.distribute.experimental.ParameterServerStrategy`\n  symbol with a new class that is for parameter server training in TF2. Usage of\n  the old symbol, usually with Estimator API, should be **replaced** with\n  [`tf.compat.v1.distribute.experimental.ParameterServerStrategy`].\n    * Added `tf.distribute.experimental.coordinator.*` namespace, including the\n  main API `ClusterCoordinator` for coordinating the training cluster, the\n  related data structure `RemoteValue` and `PerWorkerValue`.\n  * `MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy)\n  is now a stable API and is no longer considered experimental. Some of the major\n  improvements involve handling peer failure and many bug fixes. Please check out\n  the detailed tutorial on [Multi-worer training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras).\n  * Adds `tf.distribute.Strategy.gather` and `tf.distribute.ReplicaContext.all_gather`\n  APIs to support gathering dense distributed values.\n  * Fixes various issues with saving a distributed model.\n\n### `tf.keras`:\n  * Improvements from the Functional API refactoring:\n    * Functional model construction does not need to maintain a global workspace\n  graph, removing memory leaks especially when building many models or very large models.\n    * Functional model construction should be ~8-10% faster on average.\n    * Functional models can now contain non-symbolic values in their call inputs\n  inside of the first positional argument.\n    * Several classes of TF ops that were not reliably converted to Keras layers\n  during functional API construction should now work, e.g.`tf.image.ssim_multiscale`\n    * Error messages when Functional API construction goes wrong (and when ops cannot be converted to Keras layers automatically) should be\n      clearer and easier to understand.\n  * `Optimizer.minimize` can now accept a loss `Tensor` and a `GradientTape`\n  as an alternative to accepting a `callable` loss.\n  * Adds `beta` hyperparameter to [FTRL](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl)\n  optimizer classes (Keras and others) to match [FTRL paper](https://research.google.com/pubs/archive/41159.pdf).\n  * `Optimizer.__init__` now accepts a `gradient_aggregator` to allow for customization\n  of how gradients are aggregated across devices, as well as `gradients_transformers`\n  to allow for custom gradient transformations (such as gradient clipping).\n  * Improvements to Keras preprocessing layers:\n    * TextVectorization can now accept a vocabulary list or file as an init arg.\n    * Normalization can now accept mean and variance values as init args.\n  * In `Attention` and `AdditiveAttention` layers, the `call()` method now accepts a `return_attention_scores` argument. When set to\n    True, the layer returns the attention scores as an additional output argument.\n  * Adds `tf.metrics.log_cosh` and `tf.metrics.logcosh` API entrypoints with the\n  same implementation as their `tf.losses` equivalent.\n  * For Keras model, the individual call of `Model.evaluate` uses no cached data\n  for evaluation, while `Model.fit` uses cached data when `validation_data` arg\n  is provided for better performance.\n  * Adds a `save_traces` argument to `model.save`/ `tf.keras.models.save_model`\n  which determines whether the SavedModel format stores the Keras model/layer call\n  functions. The traced functions allow Keras to revive custom models and layers\n  without the original class definition, but if this isn't required the tracing\n  can be disabled with the added option.\n  * The `tf.keras.mixed_precision` API is now non-experimental.\n  The non-experimental API differs from the experimental API in several ways.\n    * `tf.keras.mixed_precision.Policy` no longer takes in a `tf.mixed_precision.\n  experimental.LossScale` in the constructor, and no longer has a `LossScale`\n  associated with it. Instead, `Model.compile` will automatically wrap the optimizer\n  with a `LossScaleOptimizer` using dynamic loss scaling if `Policy.name`\n  is \"mixed_float16\".\n    * `tf.keras.mixed_precision.LossScaleOptimizer`'s constructor takes in different\n  arguments. In particular, it no longer takes in a `LossScale`, and there is\n  no longer a `LossScale` associated with the `LossScaleOptimizer`. Instead,\n  `LossScaleOptimizer` directly implements fixed or dynamic loss scaling. See the\n  documentation of [`tf.keras.mixed_precision.experimental.LossScaleOptimizer`]\n  (https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer?version=nightly)\n  for details on the differences between the experimental `LossScaleOptimizer`\n  and the new non-experimental `LossScaleOptimizer`.\n    * `tf.mixed_precision.experimental.LossScale` and its subclasses are\n  deprecated, as all of its functionality now exists within `tf.keras.mixed_precision.LossScaleOptimizer`\n\n### `tf.lite`:\n  * `TFLiteConverter`:\n    * Support optional flags `inference_input_type` and `inference_output_type`\n  for full integer quantized models. This allows users to modify the model input\n  and output type to integer types (`tf.int8`, `tf.uint8`) instead of defaulting\n  to float type (`tf.float32`).\n  * NNAPI\n    * Adds NNAPI Delegation support for requantization use cases by converting\n  the operation into a dequantize-quantize pair.\n    * Removes deprecated `Interpreter.setUseNNAPI(boolean)` Java API. Use\n  `Interpreter.Options.setUseNNAPI` instead.\n    * Deprecates `Interpreter::UseNNAPI(bool)` C++ API. Use `NnApiDelegate()`\n  and related delegate configuration methods directly.\n    * Deprecates `Interpreter::SetAllowFp16PrecisionForFp32(bool)` C++ API.\n  Prefer controlling this via delegate options, e.g. `tflite::StatefulNnApiDelegate::Options::allow_fp16'\n  or `TfLiteGpuDelegateOptionsV2::is_precision_loss_allowed`.\n  * GPU\n    * GPU acceleration now supports quantized models by default\n  * `DynamicBuffer::AddJoinedString()` will now add a separator if the first string to be joined is empty.\n  *  Adds support for cumulative sum (cumsum), both as builtin op and MLIR conversion.\n\n### `TensorRT`\n  * Issues a warning when the `session_config` parameter for the TF1 converter\n  is used or the `rewrite_config_template` field in the TF2 converter parameter\n  object is used.\n\n### TPU Enhancements:\n  * Adds support for the `beta` parameter of the FTRL optimizer for TPU\n  embeddings. Users of other TensorFlow platforms can implement equivalent\n  behavior by adjusting the `l2` parameter.\n\n### XLA Support:\n  * xla.experimental.compile is deprecated, use `tf.function(experimental_compile=True)` instead.\n  * Adds `tf.function.experimental_get_compiler_ir` which returns compiler IR\n  (currently 'hlo' and 'optimized_hlo') for given input for given function.\n\n### Security:\n  * Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`,\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n  * Fixes three vulnerabilities in conversion to DLPack format\n    * [CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n    * [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n    * [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193)\n  * Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n    * [CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n    * [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195)\n  * Fixes several vulnerabilities in `RaggedCountSparseOutput` and `SparseCountSparseOutput` operations\n    * [CVE-2020-15196](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15196),\n    * [CVE-2020-15197](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15197),\n    * [CVE-2020-15198](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15198),\n    * [CVE-2020-15199](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15199),\n    * [CVE-2020-15200](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15200),\n    * [CVE-2020-15201](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15201)\n  * Fixes an integer truncation vulnerability in code using the work sharder API,\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n  * Fixes a format string vulnerability in `tf.strings.as_string`,\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n  * Fixes segfault raised by calling session-only ops in eager mode,\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n  * Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`,\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n  * Fixes segfaults caused by incomplete `SavedModel` validation,\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n  * Fixes a data corruption due to a bug in negative indexing support in TFLite,\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n  * Fixes a data corruption due to dimension mismatch in TFLite,\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n  * Fixes several vulnerabilities in TFLite saved model format\n    * [CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n    * [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n    * [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211)\n  * Fixes several vulnerabilities in TFLite implementation of segment sum\n    * [CVE-2020-15212](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212),\n    * [CVE-2020-15213](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213),\n    * [CVE-2020-15214](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214)\n  * Fixes a segfault in `tf.quantization.quantize_and_dequantize`,\n  ([CVE-2020-15265](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15265))\n  * Fixes an undefined behavior float cast causing a crash,\n  ([CVE-2020-15266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15266))\n  * Fixes a lack of validation in `tf.raw_ops.DataFormatVecPermute` and\n  `tf.raw_ops.DataFormatDimMap` which can cause uninitialized memory access,\n  read outside bounds of arrays, data corruption and segmentation faults\n  ([CVE-2020-26267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26267))\n  * Fixes a crash caused by writing to read only memory region\n  ([CVE-2020-26268](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26268))\n  * Fixes a heap out of bounds access in filesystem globbing implementation\n  ([CVE-2020-26269](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26269))\n\n### Other:\n  * We have replaced uses of \"whitelist\" and \"blacklist\" with \"allowlist\" and\n  \"denylist\" where possible. Please see [this list](https://developers.google.com/style/word-list#blacklist) for more context.\n  * Adds `tf.config.experimental.mlir_bridge_rollout` which will help us rollout the new MLIR TPU bridge.\n  * Adds `tf.experimental.register_filesystem_plugin` to load modular filesystem plugins from Python\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google as well as the following external contributors:\n\n8bitmp3, aaa.jq, Abhineet Choudhary, Abolfazl Shahbazi, acxz, Adam Hillier, Adrian Garcia Badaracco, Ag Ramesh, ahmedsabie, Alan Anderson, Alexander Grund, Alexandre Lissy, Alexey Ivanov, Amedeo Cavallo, anencore94, Aniket Kumar Singh, Anthony Platanios, Ashwin Phadke, Balint Cristian, Basit Ayantunde, bbbboom, Ben Barsdell, Benjamin Chetioui, Benjamin Peterson, bhack, Bhanu Prakash Bandaru Venkata, Biagio Montaruli, Brent M. Spell, bubblebooy, bzhao, cfRod, Cheng Chen, Cheng(Kit) Chen, Chris Tessum, Christian, chuanqiw, codeadmin_peritiae, COTASPAR, CuiYifeng, danielknobe, danielyou0230, dannyfriar, daria, DarrenZhang01, Denisa Roberts, dependabot[bot], Deven Desai, Dmitry Volodin, Dmitry Zakharov, drebain, Duncan Riach, Eduard Feicho, Ehsan Toosi, Elena Zhelezina, emlaprise2358, Eugene Kuznetsov, Evaderan-Lab, Evgeniy Polyakov, Fausto Morales, Felix Johnny, fo40225, Frederic Bastien, Fredrik Knutsson, fsx950223, Gaurav Singh, Gauri1 Deshpande, George Grzegorz Pawelczak, gerbauz, Gianluca Baratti, Giorgio Arena, Gmc2, Guozhong Zhuang, Hannes Achleitner, Harirai, HarisWang, Harsh188, hedgehog91, Hemal Mamtora, Hideto Ueno, Hugh Ku, Ian Beauregard, Ilya Persky, jacco, Jakub Ber\u00e1nek, Jan Jongboom, Javier Montalt Tordera, Jens Elofsson, Jerry Shih, jerryyin, jgehw, Jinjing Zhou, jma, jmsmdy, Johan Nordstr\u00f6m, John Poole, Jonah Kohn, Jonathan Dekhtiar, jpodivin, Jung Daun, Kai Katsumata, Kaixi Hou, Kamil Rakoczy, Kaustubh Maske Patil, Kazuaki Ishizaki, Kedar Sovani, Koan-Sin Tan, Koki Ibukuro, Krzysztof Laskowski, Kushagra Sharma, Kushan Ahmadian, Lakshay Tokas, Leicong Li, levinxo, Lukas Geiger, Maderator, Mahmoud Abuzaina, Mao Yunfei, Marius Brehler, markf, Martin Hwasser, Martin Kubov\u010d\u00edk, Matt Conley, Matthias, mazharul, mdfaijul, Michael137, MichelBr, Mikhail Startsev, Milan Straka, Ml-0, Myung-Hyun Kim, M\u00e5ns Nilsson, Nathan Luehr, ngc92, nikochiko, Niranjan Hasabnis, nyagato_00, Oceania2018, Oleg Guba, Ongun Kanat, OscarVanL, Patrik Laurell, Paul Tanger, Peter Sobot, Phil Pearl, PlusPlusUltra, Poedator, Prasad Nikam, Rahul-Kamat, Rajeshwar Reddy T, redwrasse, Rickard, Robert Szczepanski, Rohan Lekhwani, Sam Holt, Sami Kama, Samuel Holt, Sandeep Giri, sboshin, Sean Settle, settle, Sharada Shiddibhavi, Shawn Presser, ShengYang1, Shi,Guangyong, Shuxiang Gao, Sicong Li, Sidong-Wei, Srihari Humbarwadi, Srinivasan Narayanamoorthy, Steenu Johnson, Steven Clarkson, stjohnso98, Tamas Bela Feher, Tamas Nyiri, Tarandeep Singh, Teng Lu, Thibaut Goetghebuer-Planchon, Tim Bradley, Tomasz Strejczek, Tongzhou Wang, Torsten Rudolf, Trent Lo, Ty Mick, Tzu-Wei Sung, Varghese, Jojimon, Vignesh Kothapalli, Vishakha Agrawal, Vividha, Vladimir Menshakov, Vladimir Silyaev, VoVAllen, V\u00f5 V\u0103n Ngh\u0129a, wondertx, xiaohong1031, Xiaoming (Jason) Cui, Xinan Jiang, Yair Ehrenwald, Yasir Modak, Yasuhiro Matsumoto, Yimei Sun, Yiwen Li, Yixing, Yoav Ramon, Yong Tang, Yong Wu, yuanbopeng, Yunmo Koo, Zhangqiang, Zhou Peng, ZhuBaohe, zilinzhu, zmx\n\n\n# Release 2.3.1\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes several vulnerabilities in `RaggedCountSparseOutput` and\n  `SparseCountSparseOutput` operations\n  ([CVE-2020-15196](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15196),\n  [CVE-2020-15197](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15197),\n  [CVE-2020-15198](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15198),\n  [CVE-2020-15199](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15199),\n  [CVE-2020-15200](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15200),\n  [CVE-2020-15201](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15201))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Fixes several vulnerabilities in TFLite implementation of segment sum\n  ([CVE-2020-15212](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212),\n  [CVE-2020-15213](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213),\n  [CVE-2020-15214](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Fixes deprecated usage of `collections` API\n* Removes `scipy` dependency from `setup.py` since TensorFlow does not need it\n  to install the pip package\n\n\n# Release 2.2.1\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Fixes several vulnerabilities in TFLite implementation of segment sum\n  ([CVE-2020-15212](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212),\n  [CVE-2020-15213](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213),\n  [CVE-2020-15214](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327),\n  [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655),\n  [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656),\n  [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434),\n  [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435),\n  [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630),\n  [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631),\n  [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871),\n  and\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Fixes deprecated usage of `collections` API\n* Removes `scipy` dependency from `setup.py` since TensorFlow does not need it\n  to install the pip package\n\n\n# Release 2.1.2\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327),\n  [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655),\n  [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656),\n  [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434),\n  [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435),\n  [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630),\n  [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631),\n  [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871),\n  and\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Removes `scipy` dependency from `setup.py` since TensorFlow does not need it\n  to install the pip package\n* Switches ROCM builds to use ROCM 3.7\n\n\n# Release 2.0.3\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327),\n  [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655),\n  [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656),\n  [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434),\n  [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435),\n  [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630),\n  [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631),\n  [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871),\n  and\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Pins `numpy` to 1.18.5 to prevent ABI breakage when compiling code that uses\n  both NumPy and TensorFlow headers.\n\n\n# Release 1.15.4\n\n## Bug Fixes and Other Changes\n* Fixes an undefined behavior causing a segfault in `tf.raw_ops.Switch`\n  ([CVE-2020-15190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190))\n* Fixes three vulnerabilities in conversion to DLPack format\n  ([CVE-2020-15191](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191),\n  [CVE-2020-15192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192),\n  [CVE-2020-15193](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193))\n* Fixes two vulnerabilities in `SparseFillEmptyRowsGrad`\n  ([CVE-2020-15194](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194),\n  [CVE-2020-15195](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195))\n* Fixes an integer truncation vulnerability in code using the work sharder API\n  ([CVE-2020-15202](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202))\n* Fixes a format string vulnerability in `tf.strings.as_string`\n  ([CVE-2020-15203](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203))\n* Fixes segfault raised by calling session-only ops in eager mode\n  ([CVE-2020-15204](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204))\n* Fixes data leak and potential ASLR violation from `tf.raw_ops.StringNGrams`\n  ([CVE-2020-15205](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205))\n* Fixes segfaults caused by incomplete `SavedModel` validation\n  ([CVE-2020-15206](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206))\n* Fixes a data corruption due to a bug in negative indexing support in TFLite\n  ([CVE-2020-15207](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207))\n* Fixes a data corruption due to dimension mismatch in TFLite\n  ([CVE-2020-15208](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208))\n* Fixes several vulnerabilities in TFLite saved model format\n  ([CVE-2020-15209](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209),\n  [CVE-2020-15210](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210),\n  [CVE-2020-15211](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211))\n* Updates `sqlite3` to `3.33.00` to handle\n  [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327),\n  [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655),\n  [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656),\n  [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434),\n  [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435),\n  [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630),\n  [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631),\n  [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871),\n  and\n  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).\n* Fixes #41630 by including `max_seq_length` in CuDNN descriptor cache key\n* Pins `numpy` to 1.18.5 to prevent ABI breakage when compiling code that uses\n  both NumPy and TensorFlow headers.\n\n\n# Release 2.3.0\n\n## Major Features and Improvements\n\n*   `tf.data` adds two new mechanisms to solve input pipeline bottlenecks and\n    save resources:\n\n    *   [snapshot](https://www.tensorflow.org/api_docs/python/tf/data/experimental/snapshot)\n    *   [tf.data service](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service).\n\n    In addition checkout the detailed\n    [guide](https://www.tensorflow.org/guide/data_performance_analysis) for\n    analyzing input pipeline performance with TF Profiler.\n\n*   [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)\n    is now a stable API and no longer considered experimental for TensorFlow.\n    (earlier `tf.distribute.experimental.TPUStrategy`).\n\n*   [TF Profiler](https://www.tensorflow.org/guide/profiler) introduces two new\n    tools: a memory profiler to visualize your model\u2019s memory usage over time\n    and a [python tracer](https://www.tensorflow.org/guide/profiler#events)\n    which allows you to trace python function calls in your model. Usability\n    improvements include better diagnostic messages and\n    [profile options](https://tensorflow.org/guide/profiler#collect_performance_data)\n    to customize the host and device trace verbosity level.\n\n*   Introduces experimental support for Keras Preprocessing Layers API\n    ([`tf.keras.layers.experimental.preprocessing.*`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing?version=nightly))\n    to handle data preprocessing operations, with support for composite tensor\n    inputs. Please see below for additional details on these layers.\n\n*   TFLite now properly supports dynamic shapes during conversion and inference.\n    We\u2019ve also added opt-in support on Android and iOS for\n    [XNNPACK](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack),\n    a highly optimized set of CPU kernels, as well as opt-in support for\n    [executing quantized models on the GPU](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/gpu_advanced.md#running-quantized-models-experimental).\n\n*   Libtensorflow packages are available in GCS starting this release. We have\n    also started to\n    [release a nightly version of these packages](https://github.com/tensorflow/tensorflow#official-builds).\n\n*   The experimental Python API\n    [`tf.debugging.experimental.enable_dump_debug_info()`](https://www.tensorflow.org/api_docs/python/tf/debugging/experimental/enable_dump_debug_info)\n    now allows you to instrument a TensorFlow program and dump debugging\n    information to a directory on the file system. The directory can be read and\n    visualized by a new interactive dashboard in TensorBoard 2.3 called\n    [Debugger V2](https://www.tensorflow.org/tensorboard/debugger_v2), which\n    reveals the details of the TensorFlow program including graph structures,\n    history of op executions at the Python (eager) and intra-graph levels, the\n    runtime dtype, shape, and numerical composition of tensors, as well as their\n    code locations.\n\n## Breaking Changes\n\n*   Increases the **minimum bazel version** required to build TF to **3.1.0**.\n*   `tf.data`\n    *   Makes the following (breaking) changes to the `tf.data`.\n    *   C++ API: - `IteratorBase::RestoreInternal`,\n        `IteratorBase::SaveInternal`, and `DatasetBase::CheckExternalState`\n        become pure-virtual and subclasses are now expected to provide an\n        implementation.\n    *   The deprecated `DatasetBase::IsStateful` method is removed in favor of\n        `DatasetBase::CheckExternalState`.\n    *   Deprecated overrides of `DatasetBase::MakeIterator` and\n        `MakeIteratorFromInputElement` are removed.\n    *   The signature of `tensorflow::data::IteratorBase::SaveInternal` and\n        `tensorflow::data::IteratorBase::SaveInput` has been extended with\n        `SerializationContext` argument to enable overriding the default policy\n        for the handling external state during iterator checkpointing. This is\n        not a backwards compatible change and all subclasses of `IteratorBase`\n        *need to be updated* accordingly.\n*   `tf.keras`\n    *   Add a new `BackupAndRestore` callback for handling distributed training\n        failures & restarts. Please take a look at this\n        [tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)\n        for details on how to use the callback.\n*   `tf.image.extract_glimpse` has been updated to correctly process the case\n    where `centered=False` and `normalized=False`. This is a breaking change as\n    the output is different from (incorrect) previous versions. Note this\n    breaking change only impacts `tf.image.extract_glimpse` and\n    `tf.compat.v2.image.extract_glimpse` API endpoints. The behavior of\n    `tf.compat.v1.image.extract_glimpse` does not change. The behavior of\n    existing C++ kernel `ExtractGlimpse` does not change either, so saved models\n    using `tf.raw_ops.ExtractGlimpse` will not be impacted.\n\n## Known Caveats\n  * `tf.lite`\n    * Keras-based LSTM models must be converted with an explicit batch size in the input layer.\n\n## Bug Fixes and Other Changes\n\n### TF Core:\n  * Set `tf2_behavior` to 1 to enable V2 for early loading cases.\n  * Add `execute_fn_for_device function` to dynamically choose the implementation based on underlying device placement.\n  * Eager:\n    * Add `reduce_logsumexp` benchmark with experiment compile.\n    * Give `EagerTensor`s a meaningful `__array__` implementation.\n    * Add another version of defun matmul for performance analysis.\n  * `tf.function`/AutoGraph:\n    * `AutoGraph` now includes into TensorFlow loops any variables that are closed over by local functions. Previously, such variables were sometimes incorrectly ignored.\n    * functions returned by the `get_concrete_function` method of `tf.function` objects can now be called with arguments consistent with the original arguments or type specs passed to `get_concrete_function`.  This calling convention is now the preferred way to use concrete functions with nested values and composite tensors. Please check the [guide](https://www.tensorflow.org/guide/concrete_function) for more details on `concrete_ function`.\n    * Update `tf.function`'s `experimental_relax_shapes` to handle composite tensors appropriately.\n    * Optimize `tf.function` invocation, by removing redundant list converter.\n    * `tf.function` will retrace when called with a different variable instead of simply using the `dtype` & `shape`.\n    * [Improve support](https://github.com/tensorflow/tensorflow/issues/33862) for dynamically-sized TensorArray inside `tf.function`.\n  * `tf.math`:\n    * Narrow down `argmin`/`argmax` contract to always return the smallest index for ties.\n    * `tf.math.reduce_variance` and `tf.math.reduce_std` return correct computation for complex types and no longer support integer types.\n    * Add Bessel functions of order 0,1 to `tf.math.special`.\n    * `tf.divide` now always returns a tensor to be consistent with documentation and other APIs.\n  * `tf.image`:\n    * Replaced [`tf.image.non_max_suppression_padded`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/image/non_max_suppression_padded?hl=en) with a new implementation that supports batched inputs, which is considerably faster on TPUs and GPUs. Boxes with area=0 will be ignored. Existing usage with single inputs should still work as before.\n  * `tf.linalg`\n    * Add `tf.linalg.banded_triangular_solve`.\n  * `tf.random`:\n    * Add `tf.random.stateless_parameterized_truncated_normal`.\n  * `tf.ragged`:\n    * Add `tf.ragged.cross` and `tf.ragged.cross_hashed` operations.\n  * `tf.RaggedTensor`:\n    * `RaggedTensor.to_tensor()` now preserves static shape.\n    * Add `tf.strings.format()` and `tf.print()` to support RaggedTensors.\n  * `tf.saved_model`:\n    * `@tf.function` from SavedModel no longer ignores args after a `RaggedTensor` when selecting the concrete function to run.\n    * Fix save model issue for ops with a list of functions.\n    * Add `tf.saved_model.LoadOptions` with [`experimental_io_device`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/saved_model/LoadOptions?hl=en) as arg with default value `None` to choose the I/O device for loading models and weights.\n    * Update `tf.saved_model.SaveOptions` with [`experimental_io_device`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/saved_model/SaveOptions?hl=en) as arg with default value `None` to choose the I/O device for saving models and weights.\n    * Mutable tables now restore checkpointed values when loaded from SavedModel.\n    * The user object metadata field in the SavedModel proto has been deprecated as part of the updates to Keras SavedModel. Keras was the only consumer of this field prior to the update.\n  * GPU\n    * TF 2.3 includes PTX kernels only for [compute capability](https://developer.nvidia.com/cuda-gpus) 7.0 to reduce the TF pip binary size.  Earlier releases included PTX for a variety of older compute capabilities.\n    * Remove environmental variable `TF_USE_CUDNN`.\n  * Others\n    * Retain parent namescope for ops added inside `tf.while_loop`/`tf.cond`/`tf.switch_case`.\n    * Update `tf.vectorized_map` to support vectorizing `tf.while_loop` and TensorList operations.\n    * `tf.custom_gradient` can now be applied to functions that accept nested structures of `tensors` as inputs (instead of just a list of tensors). Note that Python structures such as tuples and lists now won't be treated as tensors, so if you still want them to be treated that way, you need to wrap them with `tf.convert_to_tensor`.\n    * No lowering on gradient case op when input is `DeviceIndex` op.\n    * Extend the ragged version of `tf.gather` to support `batch_dims` and `axis` args.\n    * Update `tf.map_fn` to support RaggedTensors and SparseTensors.\n    * Deprecate `tf.group`. It is not useful in eager mode.\n    * Add CPU and GPU implementation of modified variation of [`FTRL`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/raw_ops/ApplyFtrl)/[`FTRLV2`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/raw_ops/ApplyFtrlV2) that can triggerred by `multiply_linear_by_lr` allowing a learning rate of zero.\n\n### `tf.data`:\n  * `tf.data.experimental.dense_to_ragged_batch` works correctly with tuples.\n  * `tf.data.experimental.dense_to_ragged_batch` to output variable ragged rank.\n  * `tf.data.experimental.cardinality` is now a method on `tf.data.Dataset`.\n  * `tf.data.Dataset` now supports `len(Dataset)` when the cardinality is finite.\n\n### `tf.distribute`:\n  * Expose experimental [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/distribute/DistributedDataset?hl=en) and [`tf.distribute.DistributedIterator`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/distribute/DistributedIterator) to distribute input data when using `tf.distribute` to scale training on multiple devices.\n    * Added a [`get_next_as_optional`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/distribute/DistributedIterator?hl=en#get_next_as_optional) method for [`tf.distribute.DistributedIterator`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/distribute/DistributedIterator?hl=en) class to return a `tf.experimental.Optional` instance that contains the next value for all replicas or none instead of raising an out of range error. Also see *new* [guide on input distribution](https://www.tensorflow.org/tutorials/distribute/input).\n  * Allow var.assign on MirroredVariables with aggregation=NONE in replica context. Previously this would raise an error. We now allow this because many users and library writers find using `.assign` in replica context to be more convenient, instead of having to use `Strategy.extended.update` which was the previous way of updating variables in this situation.\n  * `tf.distribute.experimental.MultiWorkerMirroredStrategy` adds support for partial batches. Workers running out of data now continue to participate in the training with empty inputs, instead of raising an error. Learn more about [partial batches here](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\n  * Improve the performance of reading metrics eagerly under `tf.distribute.experimental.MultiWorkerMirroredStrategy`.\n  * Fix the issue that `strategy.reduce()` inside `tf.function` may raise exceptions when the values to reduce are from loops or if-clauses.\n  * Fix the issue that `tf.distribute.MirroredStrategy` cannot be used together with `tf.distribute.experimental.MultiWorkerMirroredStrategy`.\n  * Add a `tf.distribute.cluster_resolver.TPUClusterResolver.connect` API to simplify TPU initialization.\n  * Add `tf.distribute.Strategy.gather` and `tf.distribute.ReplicaContext.all_gather` methods to gather and concatenate `tf.distribute.DistributedValues` across workers and devices.\n\n### `tf.keras`:\n  * Introduces experimental preprocessing layers API (`tf.keras.layers.experimental.preprocessing`)  to handle data preprocessing operations such as categorical feature encoding, text vectorization, data normalization, and data discretization (binning). The newly added layers provide a replacement for the  legacy feature column API, and support composite tensor inputs.\n  * Added **categorical data** processing layers:\n    * `IntegerLookup` & `StringLookup`: build an index of categorical feature values\n    * `CategoryEncoding`: turn integer-encoded categories into one-hot, multi-hot, or tf-idf encoded representations\n    * `CategoryCrossing`: create new categorical features representing co-occurrences of previous categorical feature values\n    * `Hashing`: the hashing trick, for large-vocabulary categorical features\n    * `Discretization`: turn continuous numerical features into categorical features by binning their values\n  * Improved **image preprocessing** layers: `CenterCrop`, `Rescaling`\n  * Improved **image augmentation** layers: `RandomCrop`, `RandomFlip`, `RandomTranslation`, `RandomRotation`, `RandomHeight`, `RandomWidth`, `RandomZoom`, `RandomContrast`\n  * Improved **`TextVectorization`** layer, which handles string tokenization, n-gram generation, and token encoding\n    * The `TextVectorization` layer now accounts for the mask_token as part of the vocabulary size when output_mode='int'. This means that, if you have a max_tokens value of 5000, your output will have 5000 unique values (not 5001 as before).\n    * Change the return value of `TextVectorization.get_vocabulary()` from `byte` to `string`. Users who previously were calling 'decode' on the output of this method should no longer need to do so.\n  * Introduce new Keras dataset generation utilities :\n    * **[`image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory)** is a utility based on `tf.data.Dataset`, meant to replace the legacy `ImageDataGenerator`. It takes you from a structured directory of images to a labeled dataset, in one function call. Note that it doesn't perform image data augmentation (which is meant to be done using preprocessing layers).\n    * **[`text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory)** takes you from a structured directory of text files to a labeled dataset, in one function call.\n    * **[`timeseries_dataset_from_array`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array)** is a `tf.data.Dataset`-based replacement of the legacy `TimeseriesGenerator`. It takes you from an array of timeseries data to a dataset of shifting windows with their targets.\n  * Added [`experimental_steps_per_execution`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/Model?hl=en#compile)\n arg to `model.compile` to indicate the number of batches to run per `tf.function` call. This can speed up Keras Models on TPUs up to 3x.\n  * Extends `tf.keras.layers.Lambda` layers to support multi-argument lambdas, and keyword arguments when calling the layer.\n  * Functional models now get constructed if *any* tensor in a layer call's arguments/keyword arguments comes from a keras input. Previously the functional api would only work if all of the elements in the first argument to the layer came from a keras input.\n  * Clean up `BatchNormalization` layer's `trainable` property to act like standard python state when it's used inside `tf.functions` (frozen at tracing time), instead of acting like a pseudo-variable whose updates *kind of sometimes* get reflected in already-traced `tf.function` traces.\n  * Add the `Conv1DTranspose` layer.\n  * Refine the semantics of `SensitivitySpecificityBase` derived metrics. See the updated API docstrings for [`tf.keras.metrics.SensitivityAtSpecificity`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/metrics/SensitivityAtSpecificity) and [`tf.keras.metrics.SpecificityAtSensitivty`](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/metrics/SpecificityAtSensitivity).\n\n### `tf.lite`:\n  * Converter\n      * Restored `inference_input_type` and `inference_output_type` flags in TF 2.x TFLiteConverter (backward compatible with TF 1.x) to support integer (tf.int8, tf.uint8) input and output types in post training full integer quantized models.\n      * Added support for converting and resizing models with dynamic (placeholder) dimensions. Previously, there was only limited support for dynamic batch size, and even that did not guarantee that the model could be properly resized at runtime.\n       * Enabled experimental support for a new quantization mode with 16-bit activations and 8-bit weights. See `lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8`.\n  * CPU\n      * Fix an issue w/ dynamic weights and `Conv2D` on x86.\n      * Add a runtime Android flag for enabling `XNNPACK` for optimized CPU performance.\n      * Add a runtime iOS flag for enabling `XNNPACK` for optimized CPU performance.\n      * Add a compiler flag to enable building a TFLite library that applies `XNNPACK` delegate automatically when the model has a `fp32` operation.\n  * GPU\n      * Allow GPU acceleration starting with internal graph nodes\n      * Experimental support for quantized models with the Android GPU delegate\n      * Add GPU delegate whitelist.\n      * Rename GPU whitelist -> compatibility (list).\n      * Improve GPU compatibility list entries from crash reports.\n  * NNAPI\n      * Set default value for `StatefulNnApiDelegate::Options::max_number_delegated_partitions` to 3.\n      * Add capability to disable `NNAPI` CPU and check `NNAPI` Errno.\n      * Fix crashes when using `NNAPI` with target accelerator specified with model containing Conv2d or FullyConnected or LSTM nodes with quantized weights.\n      * Fix `ANEURALNETWORKS_BAD_DATA` execution failures with `sum`/`max`/`min`/`reduce` operations with `scalar` inputs.\n  * Hexagon\n      * TFLite Hexagon Delegate out of experimental.\n      * Experimental `int8` support for most hexagon ops.\n      * Experimental per-channel quant support for `conv` in Hexagon delegate.\n      * Support dynamic batch size in C++ API.\n  * CoreML\n     * Opensource CoreML delegate\n  * Misc\n      * Enable building Android TFLite targets on Windows\n      * Add support for `BatchMatMul`.\n      * Add support for `half_pixel_centers` with `ResizeNearestNeighbor`.\n      * Add 3D support for `BatchToSpaceND`.\n      * Add 5D support for `BroadcastSub`, `Maximum`, `Minimum`, `Transpose` and `BroadcastDiv`.\n      * Rename `kTfLiteActRelu1` to `kTfLiteActReluN1To1`.\n      * Enable flex delegate on tensorflow.lite.Interpreter Python package.\n      * Add `Buckettize`, `SparseCross` and `BoostedTreesBucketize` to the flex whitelist.\n      * Add support for selective registration of flex ops.\n      * Add missing kernels for flex delegate whitelisted ops.\n      * Fix issue when using direct `ByteBuffer` inputs with graphs that have dynamic shapes.\n      * Fix error checking supported operations in a model containing `HardSwish`.\n\n### Packaging Support\n  * Added `tf.sysconfig.get_build_info()`. Returns a dict that describes the build environment of the currently installed TensorFlow package, e.g. the NVIDIA CUDA and NVIDIA CuDNN versions used when TensorFlow was built.\n\n### Profiler\n  * Fix a subtle use-after-free issue in `XStatVisitor::RefValue()`.\n\n### TPU Enhancements\n  * Adds 3D mesh support in TPU configurations ops.\n  * Added TPU code for `FTRL` with `multiply_linear_by_lr`.\n  * Silently adds a new file system registry at `gstpu`.\n  * Support `restartType` in cloud tpu client.\n  * Depend on a specific version of google-api-python-client.\n  * Fixes apiclient import.\n\n### Tracing and Debugging\n  * Add a `TFE_Py_Execute` traceme.\n\n### XLA Support\n  * Implement stable `argmin` and `argmax`\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n902449@58880@bigcat_chen@ASIC, Abdul Baseer Khan, Abhineet Choudhary, Abolfazl Shahbazi, Adam Hillier, ag.ramesh, Agoniii, Ajay P, Alex Hoffman, Alexander Bayandin, Alexander Grund, Alexandre Abadie, Alexey Rogachevskiy, amoitra, Andrew Stevens, Angus-Luo, Anshuman Tripathy, Anush Elangovan, Artem Mavrin, Ashutosh Hathidara, autoih, Ayushman Kumar, ayushmankumar7, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, bhack, Bharat Raghunathan, Biagio Montaruli, Bigcat-Himax, blueyi, Bryan Cutler, Byambaa, Carlos Hernandez-Vaquero, Chen Lei, Chris Knorowski, Christian Clauss, chuanqiw, CuiYifeng, Daniel Situnayake, Daria Zhuravleva, Dayananda-V, Deven Desai, Devi Sandeep Endluri, Dmitry Zakharov, Dominic Jack, Duncan Riach, Edgar Liberis, Ehsan Toosi, ekuznetsov139, Elena Zhelezina, Eugene Kuznetsov, Eugene Mikhantiev, Evgenii Zheltonozhskii, Fabio Di Domenico, Fausto Morales, Fei Sun, feihugis, Felix E. Klee, flyingcat, Frederic Bastien, Fredrik Knutsson, frreiss, fsx950223, ganler, Gaurav Singh, Georgios Pinitas, Gian Marco Iodice, Giorgio Arena, Giuseppe Rossini, Gregory Keith, Guozhong Zhuang, gurushantj, Hahn Anselm, Harald Husum, Harjyot Bagga, Hristo Vrigazov, Ilya Persky, Ir1d, Itamar Turner-Trauring, jacco, Jake Tae, Janosh Riebesell, Jason Zaman, jayanth, Jeff Daily, Jens Elofsson, Jinzhe Zeng, JLZ, Jonas Skog, Jonathan Dekhtiar, Josh Meyer, Joshua Chia, Judd, justkw, Kaixi Hou, Kam D Kasravi, Kamil Rakoczy, Karol Gugala, Kayou, Kazuaki Ishizaki, Keith Smiley, Khaled Besrour, Kilaru Yasaswi Sri Chandra Gandhi, Kim, Young Soo, Kristian Hartikainen, Kwabena W. Agyeman, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Geiger, Lutz Roeder, M\\U00E5Ns Nilsson, Mahmoud Abuzaina, Manish, Marcel Koester, Marcin Sielski, marload, Martin Jul, Matt Conley, mdfaijul, Meng, Peng, Meteorix, Michael K\u00e4ufl, Michael137, Milan Straka, Mitchell Vitez, Ml-0, Mokke Meguru, Mshr-H, nammbash, Nathan Luehr, naumkin, Neeraj Bhadani, ngc92, Nick Morgan, nihui, Niranjan Hasabnis, Niranjan Yadla, Nishidha Panpaliya, Oceania2018, oclyke, Ouyang Jin, OverLordGoldDragon, Owen Lyke, Patrick Hemmer, Paul Andrey, Peng Sun, periannath, Phil Pearl, Prashant Dandriyal, Prashant Kumar, Rahul Huilgol, Rajan Singh, Rajeshwar Reddy T, rangjiaheng, Rishit Dagli, Rohan Reddy, rpalakkal, rposts, Ruan Kunliang, Rushabh Vasani, Ryohei Ikegami, Semun Lee, Seo-Inyoung, Sergey Mironov, Sharada Shiddibhavi, ShengYang1, Shraiysh Vaishay, Shunya Ueta, shwetaoj, Siyavash Najafzade, Srinivasan Narayanamoorthy, Stephan Uphoff, storypku, sunchenggen, sunway513, Sven-Hendrik Haase, Swapnil Parekh, Tamas Bela Feher, Teng Lu, tigertang, tomas, Tomohiro Ubukata, tongxuan.ltx, Tony Tonev, Tzu-Wei Huang, T\u00e9o Bouvard, Uday Bondhugula, Vaibhav Jade, Vijay Tadikamalla, Vikram Dattu, Vincent Abriou, Vishnuvardhan Janapati, Vo Van Nghia, VoVAllen, Will Battel, William D. Irons, wyzhao, Xiaoming (Jason) Cui, Xiaoquan Kong, Xinan Jiang, xutianming, Yair Ehrenwald, Yasir Modak, Yasuhiro Matsumoto, Yixing Fu, Yong Tang, Yuan Tang, zhaozheng09, Zilin Zhu, zilinzhu, \u5f20\u5fd7\u8c6a\n\n# Release 2.1.1\n\n## Bug Fixes and Other Changes\n* Updates `sqlite3` to `3.31.01` to handle [CVE-2019-19880](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880), [CVE-2019-19244](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244) and [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645)\n* Updates `curl` to `7.69.1` to handle [CVE-2019-15601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601)\n* Updates `libjpeg-turbo` to `2.0.4` to handle [CVE-2018-19664](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664), [CVE-2018-20330](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330) and [CVE-2019-13960](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960)\n* Updates Apache Spark to `2.4.5` to handle [CVE-2019-10099](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099), [CVE-2018-17190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190) and [CVE-2018-11770](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770)\n* Fixes a versioning bug which causes Keras layers from TF 1.x to be used instead of those from TF 2.x\n\n# Release 2.0.2\n\n## Bug Fixes and Other Changes\n* Updates `sqlite3` to `3.31.01` to handle [CVE-2019-19880](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880), [CVE-2019-19244](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244) and [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645)\n* Updates `curl` to `7.69.1` to handle [CVE-2019-15601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601)\n* Updates `libjpeg-turbo` to `2.0.4` to handle [CVE-2018-19664](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664), [CVE-2018-20330](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330) and [CVE-2019-13960](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960)\n* Updates Apache Spark to `2.4.5` to handle [CVE-2019-10099](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099), [CVE-2018-17190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190) and [CVE-2018-11770](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770)\n\n# Release 1.15.3\n\n## Bug Fixes and Other Changes\n* Updates `sqlite3` to `3.31.01` to handle [CVE-2019-19880](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880), [CVE-2019-19244](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244) and [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645)\n* Updates `curl` to `7.69.1` to handle [CVE-2019-15601](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601)\n* Updates `libjpeg-turbo` to `2.0.4` to handle [CVE-2018-19664](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664), [CVE-2018-20330](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330) and [CVE-2019-13960](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960)\n* Updates Apache Spark to `2.4.5` to handle [CVE-2019-10099](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099), [CVE-2018-17190](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190) and [CVE-2018-11770](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770)\n\n# Release 2.2.0\n\nTensorFlow 2.2 discontinues support for Python 2, [previously announced](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ) as following [Python 2's EOL on January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update).\n\nCoinciding with this change, new releases of [TensorFlow's Docker images](https://hub.docker.com/r/tensorflow/tensorflow/) provide Python 3 exclusively. Because all images now use Python 3, Docker tags containing `-py3` will no longer be provided and existing `-py3` tags like `latest-py3` will not be updated.\n\n## Major Features and Improvements\n\n* Replaced the scalar type for string tensors from `std::string` to `tensorflow::tstring` which is now ABI stable.\n* A new Profiler for TF 2 for CPU/GPU/TPU. It offers both device and host performance analysis, including input pipeline and TF Ops. Optimization advisory is provided whenever possible. Please see [this tutorial](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) and [guide](https://www.tensorflow.org/guide/profiler) for usage guidelines.\n* Export C++ functions to Python using `pybind11` as opposed to `SWIG` as a part of our [deprecation of swig efforts](https://github.com/tensorflow/community/blob/master/rfcs/20190208-pybind11.md).\n* `tf.distribute`:\n  * Support added for global sync `BatchNormalization` by using the newly added `tf.keras.layers.experimental.SyncBatchNormalization` layer. This layer will sync `BatchNormalization` statistics every step across all replicas taking part in sync training.\n  * Performance improvements for GPU multi-worker distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy`\n    * Update NVIDIA `NCCL` to `2.5.7-1` for better performance and performance tuning. Please see [nccl developer guide](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html) for more information on this.\n    * Support gradient `allreduce` in `float16`. See this [example](https://github.com/tensorflow/models/blob/master/official/staging/training/grad_utils.py) usage.\n    * Experimental support of [all reduce gradient packing](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CollectiveHints) to allow overlapping gradient aggregation with backward path computation.\n    * Deprecated `experimental_run_v2` method for distribution strategies and renamed the method `run` as it is no longer experimental.\n    * Add CompositeTensor support for DistributedIterators. This should help prevent unnecessary function retracing and memory leaks.\n* `tf.keras`:\n  * `Model.fit` major improvements:\n     * You can now use custom training logic with `Model.fit` by overriding `Model.train_step`.\n     * Easily write state-of-the-art training loops without worrying about all of the features `Model.fit` handles for you (distribution strategies, callbacks, data formats, looping logic, etc)\n     * See the default [`Model.train_step`](https://github.com/tensorflow/tensorflow/blob/1381fc8e15e22402417b98e3881dfd409998daea/tensorflow/python/keras/engine/training.py#L540) for an example of what this function should look like. Same applies for validation and inference via `Model.test_step` and `Model.predict_step`.\n     * SavedModel uses its own `Model._saved_model_inputs_spec` attr now instead of\n       relying on `Model.inputs` and `Model.input_names`, which are no longer set for subclass Models.\n       This attr is set in eager, `tf.function`, and graph modes. This gets rid of the need for users to\n       manually call `Model._set_inputs` when using Custom Training Loops(CTLs).\n     * Dynamic shapes are supported for generators by calling the Model on the first batch we \"peek\" from the generator.\n       This used to happen implicitly in `Model._standardize_user_data`. Long-term, a solution where the\n       `DataAdapter` doesn't need to call the Model is probably preferable.\n  * The SavedModel format now supports all Keras built-in layers (including metrics, preprocessing layers, and stateful RNN layers)\n  * Update Keras batch normalization layer to use the running mean and average computation in the `fused_batch_norm`. You should see significant performance improvements when using `fused_batch_norm` in Eager mode.\n\n* `tf.lite`:\n  * Enable TFLite experimental new converter by default.\n* XLA\n  * XLA now builds and works on windows. All prebuilt packages come with XLA available.\n  * XLA can be [enabled for a `tf.function`](https://www.tensorflow.org/xla#explicit_compilation_with_tffunction\n) with \u201ccompile or throw exception\u201d semantics on CPU and GPU.\n\n## Breaking Changes\n* `tf.keras`:\n  * In `tf.keras.applications` the name of the \"top\" layer has been standardized to \"predictions\". This is only a problem if your code relies on the exact name of the layer.\n  * Huber loss function has been updated to be consistent with other Keras losses. It now computes mean over the last axis of per-sample losses before applying the reduction function.\n* AutoGraph no longer converts functions passed to `tf.py_function`, `tf.py_func` and `tf.numpy_function`.\n* Deprecating `XLA_CPU` and `XLA_GPU` devices with this release.\n* Increasing the minimum bazel version to build TF to 2.0.0 to use Bazel's `cc_experimental_shared_library`.\n* Keras compile/fit behavior for functional and subclassed models have been unified. Model properties such as `metrics`, `metrics_names` will now be available only after **training/evaluating the model on actual data** for functional models. `metrics` will **now include** model `loss` and output losses.`loss_functions` property has been removed from the model. This was an undocumented property that was accidentally public and has now been removed.\n\n## Known Caveats\n* The current TensorFlow release now **requires** [gast](https://pypi.org/project/gast/) version 0.3.3.\n\n## Bug Fixes and Other Changes\n\n*   `tf.data`:\n    *   Removed `autotune_algorithm` from experimental optimization options.\n*   TF Core:\n    *   `tf.constant` always creates CPU tensors irrespective of the current\n        device context.\n    *   Eager `TensorHandles` maintain a list of mirrors for any copies to local\n        or remote devices. This avoids any redundant copies due to op execution.\n    *   For `tf.Tensor` & `tf.Variable`, `.experimental_ref()` is no longer\n        experimental and is available as simply `.ref()`.\n    *   `pfor/vectorized_map`: Added support for vectorizing 56 more ops.\n        Vectorizing `tf.cond` is also supported now.\n    *   Set as much partial shape as we can infer statically within the gradient\n        impl of the gather op.\n    *   Gradient of `tf.while_loop` emits `StatelessWhile` op if `cond` and body\n        functions are stateless. This allows multiple gradients while ops to run\n        in parallel under distribution strategy.\n    *   Speed up `GradientTape` in eager mode by auto-generating list of op\n        inputs/outputs which are unused and hence not cached for gradient\n        functions.\n    *   Support `back_prop=False` in `while_v2` but mark it as deprecated.\n    *   Improve error message when attempting to use `None` in data-dependent\n        control flow.\n    *   Add `RaggedTensor.numpy()`.\n    *   Update `RaggedTensor.__getitem__` to preserve uniform dimensions & allow\n        indexing into uniform dimensions.\n    *   Update `tf.expand_dims` to always insert the new dimension as a\n        non-ragged dimension.\n    *   Update `tf.embedding_lookup` to use `partition_strategy` and `max_norm`\n        when `ids` is ragged.\n    *   Allow `batch_dims==rank(indices)` in `tf.gather`.\n    *   Add support for bfloat16 in `tf.print`.\n*   `tf.distribute`:\n    *   Support `embedding_column` with variable-length input features for\n        `MultiWorkerMirroredStrategy`.\n*   `tf.keras`:\n    *   Added `experimental_aggregate_gradients` argument to\n        `tf.keras.optimizer.Optimizer.apply_gradients`. This allows custom\n        gradient aggregation and processing aggregated gradients in custom\n        training loop.\n    *   Allow `pathlib.Path` paths for loading models via Keras API.\n*   `tf.function`/AutoGraph:\n    *   AutoGraph is now available in `ReplicaContext.merge_call`,\n        `Strategy.extended.update` and `Strategy.extended.update_non_slot`.\n    *   Experimental support for shape invariants has been enabled in\n        `tf.function`. See the API docs for\n        `tf.autograph.experimental.set_loop_options` for additional info.\n    *   AutoGraph error messages now exclude frames corresponding to APIs\n        internal to AutoGraph.\n    *   Improve shape inference for `tf.function` input arguments to unlock more\n        Grappler optimizations in TensorFlow 2.x.\n    *   Improve automatic control dependency management of resources by allowing\n        resource reads to occur in parallel and synchronizing only on writes.\n    *   Fix execution order of multiple stateful calls to `experimental_run_v2`\n        in `tf.function`.\n    *   You can now iterate over `RaggedTensors` using a for loop inside\n        `tf.function`.\n*   `tf.lite`:\n    *   Migrated the `tf.lite` C inference API out of experimental into lite/c.\n    *   Add an option to disallow `NNAPI` CPU / partial acceleration on Android\n        10\n    *   TFLite Android AARs now include the C headers and APIs are required to\n        use TFLite from native code.\n    *   Refactors the delegate and delegate kernel sources to allow usage in the\n        linter.\n    *   Limit delegated ops to actually supported ones if a device name is\n        specified or `NNAPI` CPU Fallback is disabled.\n    *   TFLite now supports `tf.math.reciprocal1` op by lowering to `tf.div op`.\n    *   TFLite's unpack op now supports boolean tensor inputs.\n    *   Microcontroller and embedded code moved from experimental to main\n        TensorFlow Lite folder\n    *   Check for large TFLite tensors.\n    *   Fix GPU delegate crash with C++17.\n    *   Add 5D support to TFLite `strided_slice`.\n    *   Fix error in delegation of `DEPTH_TO_SPACE` to `NNAPI` causing op not to\n        be accelerated.\n    *   Fix segmentation fault when running a model with LSTM nodes using\n        `NNAPI` Delegate\n    *   Fix `NNAPI` delegate failure when an operand for Maximum/Minimum\n        operation is a scalar.\n    *   Fix `NNAPI` delegate failure when Axis input for reduce operation is a\n        scalar.\n    *   Expose option to limit the number of partitions that will be delegated\n        to `NNAPI`.\n    *   If a target accelerator is specified, use its feature level to determine\n        operations to delegate instead of SDK version.\n*   `tf.random`:\n    *   Various random number generation improvements:\n    *   Add a fast path for default `random_uniform`\n    *   `random_seed` documentation improvement.\n    *   `RandomBinomial` broadcasts and appends the sample shape to the left\n        rather than the right.\n    *   Added `tf.random.stateless_binomial`, `tf.random.stateless_gamma`,\n        `tf.random.stateless_poisson`\n    *   `tf.random.stateless_uniform` now supports unbounded sampling of `int`\n        types.\n*   Math and Linear Algebra:\n    *   Add `tf.linalg.LinearOperatorTridiag`.\n    *   Add `LinearOperatorBlockLowerTriangular`\n    *   Add broadcasting support to\n        tf.linalg.triangular_solve[#26204](https://github.com/tensorflow/tensorflow/issues/26204),\n        tf.math.invert_permutation.\n    *   Add `tf.math.sobol_sample` op.\n    *   Add `tf.math.xlog1py`.\n    *   Add `tf.math.special.{dawsn,expi,fresnel_cos,fresnel_sin,spence}`.\n    *   Add a Modified Discrete Cosine Transform (MDCT) and its inverse to\n        `tf.signal`.\n*   TPU Enhancements:\n    *   Refactor `TpuClusterResolver` to move shared logic to a separate pip\n        package.\n    *   Support configuring TPU software version from cloud tpu client.\n    *   Allowed TPU embedding weight decay factor to be multiplied by learning\n        rate.\n*   XLA Support:\n    *   Add standalone XLA AOT runtime target + relevant .cc sources to pip\n        package.\n    *   Add check for memory alignment to MemoryAllocation::MemoryAllocation()\n        on 32-bit ARM. This ensures a deterministic early exit instead of a hard\n        to debug bus error later.\n    *   `saved_model_cli aot_compile_cpu` allows you to compile saved models to\n        XLA header+object files and include them in your C++ programs.\n    *   Enable `Igamma`, `Igammac` for XLA.\n*   Deterministic Op Functionality:\n    *   XLA reduction emitter is deterministic when the environment variable\n        `TF_DETERMINISTIC_OPS` is set to \"true\" or \"1\". This extends\n        deterministic `tf.nn.bias_add` back-prop functionality (and therefore\n        also deterministic back-prop of bias-addition in Keras layers) to\n        include when XLA JIT compilation is enabled.\n    *   Fix problem, when running on a CUDA GPU and when either environment\n        variable `TF_DETERMINISTIC_OPS` or environment variable\n        `TF_CUDNN_DETERMINISTIC` is set to \"true\" or \"1\", in which some layer\n        configurations led to an exception with the message \"No algorithm\n        worked!\"\n*   Tracing and Debugging:\n    *   Add source, destination name to `_send` traceme to allow easier\n        debugging.\n    *   Add traceme event to `fastpathexecute`.\n*   Other:\n    *   Fix an issue with AUC.reset_states for multi-label AUC\n        [#35852](https://github.com/tensorflow/tensorflow/issues/35852)\n    *   Fix the TF upgrade script to not delete files when there is a parsing\n        error and the output mode is `in-place`.\n    *   Move `tensorflow/core:framework/*_pyclif` rules to\n        `tensorflow/core/framework:*_pyclif`.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n372046933, 8bitmp3, aaronhma, Abin Shahab, Aditya Patwardhan, Agoniii, Ahti Kitsik, Alan Yee, Albin Joy, Alex Hoffman, Alexander Grund, Alexandre E. Eichenberger, Amit Kumar Jaiswal, amoitra, Andrew Anderson, Angus-Luo, Anthony Barbier, Anton Kachatkou, Anuj Rawat, archis, Arpan-Dhatt, Arvind Sundararajan, Ashutosh Hathidara, autoih, Bairen Yi, Balint Cristian, Bas Aarts, BashirSbaiti, Basit Ayantunde, Ben Barsdell, Benjamin Gaillard, boron, Brett Koonce, Bryan Cutler, Christian Goll, Christian Sachs, Clayne Robison, comet, Daniel Falbel, Daria Zhuravleva, darsh8200, David Truby, Dayananda-V, deepakm, Denis Khalikov, Devansh Singh, Dheeraj R Reddy, Diederik Van Liere, Diego Caballero, Dominic Jack, dothinking, Douman, Drake Gens, Duncan Riach, Ehsan Toosi, ekuznetsov139, Elena Zhelezina, elzino, Ending2015a, Eric Schweitz, Erik Zettel, Ethan Saadia, Eugene Kuznetsov, Evgeniy Zheltonozhskiy, Ewout Ter Hoeven, exfalso, FAIJUL, Fangjun Kuang, Fei Hu, Frank Laub, Frederic Bastien, Fredrik Knutsson, frreiss, Fr\u00e9d\u00e9ric Rechtenstein, fsx950223, Gaurav Singh, gbaned, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, Hans Gaiser, Hans Pabst, Haoyu Wu, Harry Slatyer, hsahovic, Hugo, Hugo Sj\u00f6berg, IrinaM21, jacco, Jake Tae, Jean-Denis Lesage, Jean-Michel Gorius, Jeff Daily, Jens Elofsson, Jerry Shih, jerryyin, Jin Mingjian, Jinjing Zhou, JKIsaacLee, jojimonv, Jonathan Dekhtiar, Jose Ignacio Gomez, Joseph-Rance, Judd, Julian Gross, Kaixi Hou, Kaustubh Maske Patil, Keunwoo Choi, Kevin Hanselman, Khor Chean Wei, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan, Koki Ibukuro, Kristian Holsheimer, kurileo, Lakshay Tokas, Lee Netherton, leike666666, Leslie-Fang-Intel, Li, Guizi, LIUJIAN435, Lukas Geiger, Lyo Nguyen, madisetti, Maher Jendoubi, Mahmoud Abuzaina, Manuel Freiberger, Marcel Koester, Marco Jacopo Ferrarotti, Markus Franke, marload, Mbah-Javis, mbhuiyan, Meng Zhang, Michael Liao, MichaelKonobeev, Michal Tarnowski, Milan Straka, minoring, Mohamed Nour Abouelseoud, MoussaMM, Mrinal Jain, mrTsjolder, M\u00e5ns Nilsson, Namrata Bhave, Nicholas Gao, Niels Ole Salscheider, nikochiko, Niranjan Hasabnis, Nishidha Panpaliya, nmostafa, Noah Trenaman, nuka137, Officium, Owen L - Sfe, Pallavi G, Paul Andrey, Peng Sun, Peng Wu, Phil Pearl, PhilipMay, pingsutw, Pooya Davoodi, PragmaTwice, pshiko, Qwerty71, R Gomathi, Rahul Huilgol, Richard Xiao, Rick Wierenga, Roberto Rosmaninho, ruchit2801, Rushabh Vasani, Sami, Sana Damani, Sarvesh Dubey, Sasan Jafarnejad, Sergii Khomenko, Shane Smiskol, Shaochen Shi, sharkdtu, Shawn Presser, ShengYang1, Shreyash Patodia, Shyam Sundar Dhanabalan, Siju Samuel, Somyajit Chakraborty Sam, Srihari Humbarwadi, srinivasan.narayanamoorthy, Srishti Yadav, Steph-En-M, Stephan Uphoff, Stephen Mugisha, SumanSudhir, Taehun Kim, Tamas Bela Feher, TengLu, Tetragramm, Thierry Herrmann, Tian Jin, tigertang, Tom Carchrae, Tom Forbes, Trent Lo, Victor Peng, vijayphoenix, Vincent Abriou, Vishal Bhola, Vishnuvardhan Janapati, vladbataev, VoVAllen, Wallyss Lima, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, William Zhang, Xiaoming (Jason) Cui, Xiaoquan Kong, Xinan Jiang, Yasir Modak, Yasuhiro Matsumoto, Yaxun (Sam) Liu, Yong Tang, Ytyt-Yt, yuan, Yuan Mingshuai, Yuan Tang, Yuki Ueda, Yusup, zhangshijin, zhuwenxi\n\n# Release 2.0.1\n\n## Bug Fixes and Other Changes\n* Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))\n* Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)\n* Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)\n\n\n# Release 1.15.2\n\n## Bug Fixes and Other Changes\n* Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))\n* Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)\n* Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)\n\n\n# Release 2.1.0\n\nTensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.\n\n## Major Features and Improvements\n\n*   The `tensorflow` pip package now includes GPU support by default (same as\n    `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and\n    without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only\n    packages can be downloaded at `tensorflow-cpu` for users who are concerned\n    about package size.\n*   **Windows users:** Officially-released `tensorflow` Pip packages are now\n    built with Visual Studio 2019 version 16.4 in order to take advantage of the\n    new `/d2ReducedOptimizeHugeFunctions` compiler flag. To use these new\n    packages, you must install \"Microsoft Visual C++ Redistributable for Visual\n    Studio 2015, 2017 and 2019\", available from Microsoft's website\n    [here](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads).\n    *   This does not change the minimum required version for building\n        TensorFlow from source on Windows, but builds enabling\n        `EIGEN_STRONG_INLINE` can take over 48 hours to compile without this\n        flag. Refer to `configure.py` for more information about\n        `EIGEN_STRONG_INLINE` and `/d2ReducedOptimizeHugeFunctions`.\n    *   If either of the required DLLs, `msvcp140.dll` (old) or `msvcp140_1.dll`\n        (new), are missing on your machine, `import tensorflow` will print a\n        warning message.\n*   The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.\n*   `tf.keras`\n    *   Experimental support for mixed precision is available on GPUs and Cloud\n        TPUs. See\n        [usage guide](https://www.tensorflow.org/guide/keras/mixed_precision).\n    *   Introduced the `TextVectorization` layer, which takes as input raw\n        strings and takes care of text standardization, tokenization, n-gram\n        generation, and vocabulary indexing. See this\n        [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3).\n    *   Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be\n        outside of the DistributionStrategy scope, as long as the model was\n        constructed inside of a scope.\n    *   Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and\n        `.predict` is available for Cloud TPUs, Cloud TPU, for all types of\n        Keras models (sequential, functional and subclassing models).\n    *   Automatic outside compilation is now enabled for Cloud TPUs. This allows\n        `tf.summary` to be used more conveniently with Cloud TPUs.\n    *   Dynamic batch sizes with DistributionStrategy and Keras are supported on\n        Cloud TPUs.\n    *   Support for `.fit`, `.evaluate`, `.predict` on TPU using numpy data, in\n        addition to `tf.data.Dataset`.\n    *   Keras reference implementations for many popular models are available in\n        the TensorFlow\n        [Model Garden](https://github.com/tensorflow/models/tree/master/official).\n*   `tf.data`\n    *   Changes rebatching for `tf.data datasets` + DistributionStrategy for\n        better performance. Note that the dataset also behaves slightly\n        differently, in that the rebatched dataset cardinality will always be a\n        multiple of the number of replicas.\n    *   `tf.data.Dataset` now supports automatic data distribution and sharding\n        in distributed environments, including on TPU pods.\n    *   Distribution policies for `tf.data.Dataset` can now be tuned with 1.\n        `tf.data.experimental.AutoShardPolicy(OFF, AUTO, FILE, DATA)` 2.\n        `tf.data.experimental.ExternalStatePolicy(WARN, IGNORE, FAIL)`\n*   `tf.debugging`\n    *   Add `tf.debugging.enable_check_numerics()` and\n        `tf.debugging.disable_check_numerics()` to help debugging the root\n        causes of issues involving infinities and `NaN`s.\n*   `tf.distribute`\n    *   Custom training loop support on TPUs and TPU pods is available through\n        `strategy.experimental_distribute_dataset`,\n        `strategy.experimental_distribute_datasets_from_function`,\n        `strategy.experimental_run_v2`, `strategy.reduce`.\n    *   Support for a global distribution strategy through\n        `tf.distribute.experimental_set_strategy(),` in addition to\n        `strategy.scope()`.\n*   `TensorRT`\n    *   [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new)\n        is now supported and enabled by default. This adds support for more\n        TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D,\n        MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the\n        TensorFlow-TensorRT python conversion API is exported as\n        `tf.experimental.tensorrt.Converter`.\n*   Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to\n    \"true\" or \"1\", this environment variable makes `tf.nn.bias_add` operate\n    deterministically (i.e. reproducibly), but currently only when XLA JIT\n    compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to \"true\" or\n    \"1\" also makes cuDNN convolution and max-pooling operate deterministically.\n    This makes Keras Conv\\*D and MaxPool\\*D layers operate deterministically in\n    both the forward and backward directions when running on a CUDA-enabled GPU.\n\n## Breaking Changes\n* Deletes `Operation.traceback_with_start_lines` for which we know of no usages.\n* Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.\n* Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\n* The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.\n* `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.\n* `tf.config.experimental_list_devices` has been removed, please use\n`tf.config.list_logical_devices`.\n\n## Bug Fixes and Other Changes\n* `tf.data`\n  * Fixes concurrency issue with `tf.data.experimental.parallel_interleave` with `sloppy=True`.\n  * Add `tf.data.experimental.dense_to_ragged_batch()`.\n  * Extend `tf.data` parsing ops to support `RaggedTensors`.\n* `tf.distribute`\n  * Fix issue where GRU would crash or give incorrect output when a `tf.distribute.Strategy` was used.\n* `tf.estimator`\n  * Added option in `tf.estimator.CheckpointSaverHook` to not save the `GraphDef`.\n  * Moving the checkpoint reader from swig to pybind11.\n* `tf.keras`\n  * Export `depthwise_conv2d` in `tf.keras.backend`.\n  * In Keras Layers and Models, Variables in `trainable_weights`, `non_trainable_weights`, and `weights` are explicitly deduplicated.\n  * Keras `model.load_weights` now accepts `skip_mismatch` as an argument. This was available in external Keras, and has now been copied over to `tf.keras`.\n  * Fix the input shape caching behavior of Keras convolutional layers.\n  * `Model.fit_generator`, `Model.evaluate_generator`, `Model.predict_generator`, `Model.train_on_batch`, `Model.test_on_batch`, and `Model.predict_on_batch` methods now respect the `run_eagerly` property, and will correctly run using `tf.function` by default. Note that `Model.fit_generator`, `Model.evaluate_generator`, and `Model.predict_generator` are deprecated endpoints. They are subsumed by `Model.fit`, `Model.evaluate`, and `Model.predict` which now support generators and Sequences.\n* `tf.lite`\n  * Legalization for `NMS` ops in TFLite.\n  * add `narrow_range` and `axis` to `quantize_v2` and `dequantize` ops.\n  * Added support for `FusedBatchNormV3` in converter.\n  * Add an `errno`-like field to `NNAPI` delegate for detecting `NNAPI` errors for fallback behaviour.\n  * Refactors `NNAPI` Delegate to support detailed reason why an operation is not accelerated.\n  * Converts hardswish subgraphs into atomic ops.\n* Other\n  * Critical stability updates for TPUs, especially in cases where the XLA compiler produces compilation errors.\n  * TPUs can now be re-initialized multiple times, using `tf.tpu.experimental.initialize_tpu_system`.\n  * Add `RaggedTensor.merge_dims()`.\n  * Added new `uniform_row_length` row-partitioning tensor to `RaggedTensor`.\n  * Add `shape` arg to `RaggedTensor.to_tensor`; Improve speed of `RaggedTensor.to_tensor`.\n  * `tf.io.parse_sequence_example` and `tf.io.parse_single_sequence_example` now support ragged features.\n  * Fix `while_v2` with variables in custom gradient.\n  * Support taking gradients of V2 `tf.cond` and `tf.while_loop` using `LookupTable`.\n  * Fix bug where `vectorized_map` failed on inputs with unknown static shape.\n  * Add preliminary support for sparse CSR matrices.\n  * Tensor equality with `None` now behaves as expected.\n  * Make calls to `tf.function(f)()`, `tf.function(f).get_concrete_function` and `tf.function(f).get_initialization_function` thread-safe.\n  * Extend `tf.identity` to work with CompositeTensors (such as SparseTensor)\n  * Added more `dtypes` and zero-sized inputs to `Einsum` Op and improved its performance\n  * Enable multi-worker `NCCL` `all-reduce` inside functions executing eagerly.\n  * Added complex128 support to `RFFT`, `RFFT2D`, `RFFT3D`, `IRFFT`, `IRFFT2D`, and `IRFFT3D`.\n  * Add `pfor` converter for `SelfAdjointEigV2`.\n  * Add `tf.math.ndtri` and `tf.math.erfinv`.\n  * Add `tf.config.experimental.enable_mlir_bridge` to allow using MLIR compiler bridge in eager model.\n  * Added support for MatrixSolve on Cloud TPU / XLA.\n  * Added `tf.autodiff.ForwardAccumulator` for forward-mode autodiff\n  * Add `LinearOperatorPermutation`.\n  * A few performance optimizations on `tf.reduce_logsumexp`.\n  * Added multilabel handling to `AUC` metric\n  * Optimization on `zeros_like`.\n  * Dimension constructor now requires `None` or types with an `__index__` method.\n  * Add `tf.random.uniform` microbenchmark.\n  * Use `_protogen` suffix for proto library targets instead of `_cc_protogen` suffix.\n  * Moving the checkpoint reader from `swig` to `pybind11`.\n  * `tf.device` & `MirroredStrategy` now supports passing in a `tf.config.LogicalDevice`\n  * If you're building Tensorflow from source, consider using [bazelisk](https://github.com/bazelbuild/bazelisk) to automatically download and use the correct Bazel version. Bazelisk reads the `.bazelversion` file at the root of the project directory.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n8bitmp3, Aaron Ma, Abd\u00fcLhamit Yilmaz, Abhai Kollara, aflc, Ag Ramesh, Albert Z. Guo, Alex Torres, amoitra, Andrii Prymostka, angeliand, Anshuman Tripathy, Anthony Barbier, Anton Kachatkou, Anubh-V, Anuja Jakhade, Artem Ryabov, autoih, Bairen Yi, Bas Aarts, Basit Ayantunde, Ben Barsdell, Bhavani Subramanian, Brett Koonce, candy.dc, Captain-Pool, caster, cathy, Chong Yan, Choong Yin Thong, Clayne Robison, Colle, Dan Ganea, David Norman, David Refaeli, dengziming, Diego Caballero, Divyanshu, djshen, Douman, Duncan Riach, EFanZh, Elena Zhelezina, Eric Schweitz, Evgenii Zheltonozhskii, Fei Hu, fo40225, Fred Reiss, Frederic Bastien, Fredrik Knutsson, fsx950223, fwcore, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, giuros01, Gomathi Ramamurthy, Guozhong Zhuang, Haifeng Jin, Haoyu Wu, HarikrishnanBalagopal, HJYOO, Huang Chen-Yi, Ilham Firdausi Putra, Imran Salam, Jared Nielsen, Jason Zaman, Jasper Vicenti, Jeff Daily, Jeff Poznanovic, Jens Elofsson, Jerry Shih, jerryyin, Jesper Dramsch, jim.meyer, Jongwon Lee, Jun Wan, Junyuan Xie, Kaixi Hou, kamalkraj, Kan Chen, Karthik Muthuraman, Keiji Ariyama, Kevin Rose, Kevin Wang, Koan-Sin Tan, kstuedem, Kwabena W. Agyeman, Lakshay Tokas, latyas, Leslie-Fang-Intel, Li, Guizi, Luciano Resende, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manuel Freiberger, Mark Ryan, Martin Mlostek, Masaki Kozuki, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Muhwan Kim, Nagy Mostafa, nammbash, Nathan Luehr, Nathan Wells, Niranjan Hasabnis, Oleksii Volkovskyi, Olivier Moindrot, olramde, Ouyang Jin, OverLordGoldDragon, Pallavi G, Paul Andrey, Paul Wais, pkanwar23, Pooya Davoodi, Prabindh Sundareson, Rajeshwar Reddy T, Ralovich, Kristof, Refraction-Ray, Richard Barnes, richardbrks, Robert Herbig, Romeo Kienzler, Ryan Mccormick, saishruthi, Saket Khandelwal, Sami Kama, Sana Damani, Satoshi Tanaka, Sergey Mironov, Sergii Khomenko, Shahid, Shawn Presser, ShengYang1, Siddhartha Bagaria, Simon Plovyt, skeydan, srinivasan.narayanamoorthy, Stephen Mugisha, sunway513, Takeshi Watanabe, Taylor Jakobson, TengLu, TheMindVirus, ThisIsIsaac, Tim Gates, Timothy Liu, Tomer Gafner, Trent Lo, Trevor Hickey, Trevor Morris, vcarpani, Wei Wang, Wen-Heng (Jack) Chung, wenshuai, Wenshuai-Xiaomi, wenxizhu, william, William D. Irons, Xinan Jiang, Yannic, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Youwei Song, Zaccharie Ramzi, Zhang, Zhenyu Guo, \u738b\u632f\u534e (Zhenhua Wang), \u97e9\u8463, \uc774\uc911\uac74 Isaac Lee\n\n# Release 1.15.0\nThis is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.\n\n## Major Features and Improvements\n* As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\n* TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.\nThis enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.\n* EagerTensor now supports numpy buffer interface for tensors.\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.\n* Adds `enable_tensor_equality()`, which switches the behavior such that:\n  * Tensors are no longer hashable.\n  * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.\n\n## Breaking Changes\n* Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\n* TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\n* `tf.keras`:\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\n  * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer \"layer-name\" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\n  * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\n\n## Bug Fixes and Other Changes\n* `tf.estimator`:\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.\n  * Fix tests in canned estimators.\n  * Expose Head as public API.\n  * Fixes critical bugs that help with `DenseFeatures` usability in TF2\n* `tf.data`:\n  * Promoting `unbatch` from experimental to core API.\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\n* `tf.keras`:\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.\n  * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\n  * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\n* `tf.lite`\n  * Add `GATHER` support to NN API delegate.\n  * tflite object detection script has a debug mode.\n  * Add delegate support for `QUANTIZE`.\n  * Added evaluation script for COCO minival.\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\n  * Converts hardswish subgraphs into atomic ops.\n* Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\n* `parallel_for`: Add converter for `MatrixDiag`.\n* Add `narrow_range` attribute to `QuantizeAndDequantizeV2` and V3.\n* Added new op: `tf.strings.unsorted_segment_join`.\n* Add HW acceleration support for `topK_v2`.\n* Add new `TypeSpec` classes.\n* CloudBigtable version updated to v0.10.0.\n* Expose `Head` as public API.\n* Update docstring for gather to properly describe the non-empty `batch_dims` case.\n* Added `tf.sparse.from_dense` utility function.\n* Improved ragged tensor support in `TensorFlowTestCase`.\n* Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\n* `ResizeInputTensor` now works for all delegates.\n* Add `EXPAND_DIMS` support to NN API delegate TEST:  expand_dims_test\n* `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\n* `tf.cond`, `tf.while` and `if` and `while` in AutoGraph now accept a nonscalar predicate if has a single element. This does not affect non-V2 control flow.\n* `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\n* Refactors code in Quant8 LSTM support to reduce TFLite binary size.\n* Add support of local soft device placement for eager op.\n* Add HW acceleration support for `LogSoftMax`.\n* Added a function `nested_value_rowids` for ragged tensors.\n* Add guard to avoid acceleration of L2 Normalization with input rank != 4\n* Add `tf.math.cumulative_logsumexp operation`.\n* Add `tf.ragged.stack`.\n* Fix memory allocation problem when calling `AddNewInputConstantTensor`.\n* Delegate application failure leaves interpreter in valid state.\n* Add check for correct memory alignment to `MemoryAllocation::MemoryAllocation()`.\n* Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\n* Added support for `FusedBatchNormV3` in converter.\n* A ragged to dense op for directly calculating tensors.\n* Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\na6802739, Aaron Ma, Abdullah Selek, Abolfazl Shahbazi, Ag Ramesh, Albert Z. Guo, Albin Joy, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Amit Srivastava, amoitra, Andrew Lihonosov, Andrii Prymostka, Anuj Rawat, Astropeak, Ayush Agrawal, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bryan Cutler, candy.dc, Cao Zongyan, Captain-Pool, Casper Da Costa-Luis, Chen Guoyin, Cheng Chang, chengchingwen, Chong Yan, Choong Yin Thong, Christopher Yeh, Clayne Robison, Coady, Patrick, Dan Ganea, David Norman, Denis Khalikov, Deven Desai, Diego Caballero, Duncan Dean, Duncan Riach, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Fangjun Kuang, Fei Hu, fo40225, formath, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, George Grzegorz Pawelczak, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, haison, Haraldur T\u00f3Mas Hallgr\u00edMsson, HarikrishnanBalagopal, H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, Jeroen B\u00e9Dorf, Jerry Shih, jerryyin, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Joon, Josh Beal, Julian Niedermeier, Jun Wan, Junqin Zhang, Junyuan Xie, Justin Tunis, Kaixi Hou, Karl Lessard, Karthik Muthuraman, Kbhute-Ibm, khanhlvg, Koock Yoon, kstuedem, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, minds, mpppk, musikisomorphie, Nagy Mostafa, Nayana Thorat, Neil, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, ocjosen, olramde, Pariksheet Pinjari, Patrick J. Lopresti, Patrik Gustavsson, per1234, PeterLee, Phan Van Nguyen Duc, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, richardbrks, robert, RonLek, Ryan Jiang, saishruthi, Saket Khandelwal, Saleem Abdulrasool, Sami Kama, Sana-Damani, Sergii Khomenko, Severen Redwood, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, Srini511, srinivasan.narayanamoorthy, Sumesh Udayakumaran, Sungmann Cho, Tae-Hwan Jung, Taehoon Lee, Takeshi Watanabe, TengLu, terryky, TheMindVirus, ThisIsIsaac, Till Hoffmann, Timothy Liu, Tomer Gafner, Tongxuan Liu, Trent Lo, Trevor Morris, Uday Bondhugula, Vasileios Lioutas, vbvg2008, Vishnuvardhan Janapati, Vivek Suryamurthy, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xinan Jiang, Xinping Wang, Yann-Yy, Yasir Modak, Yong Tang, Yongfeng Gu, Yuchen Ying, Yuxin Wu, zyeric, \u738b\u632f\u534e (Zhenhua Wang)\n\n# Release 2.0.0\n\n## Major Features and Improvements\n\nTensorFlow 2.0 focuses on **simplicity** and **ease of use**, featuring updates like:\n\n* Easy model building with Keras and eager execution.\n* Robust model deployment in production on any platform.\n* Powerful experimentation for research.\n* API simplification by reducing duplication and removing deprecated endpoints.\n\nFor details on best practices with 2.0, see [the Effective 2.0 guide](https://www.tensorflow.org/beta/guide/effective_tf2)\n\n\nFor information on upgrading your existing TensorFlow 1.x models, please refer to our [Upgrade](https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5) and [Migration](https://www.tensorflow.org/beta/guide/migration_guide) guides. We have also released a collection of [tutorials and getting started guides](https://www.tensorflow.org/beta).\n\n## Highlights\n\n*   TF 2.0 delivers Keras as the central high level API used to build and train\n    models. Keras provides several model-building APIs such as Sequential,\n    Functional, and Subclassing along with eager execution, for immediate\n    iteration and intuitive debugging, and `tf.data`, for building scalable\n    input pipelines. Checkout\n    [guide](https://www.tensorflow.org/beta/guide/keras/overview) for additional\n    details.\n*   Distribution Strategy: TF 2.0 users will be able to use the\n    [`tf.distribute.Strategy`](https://www.tensorflow.org/beta/guide/distribute_strategy)\n    API to distribute training with minimal code changes, yielding great\n    out-of-the-box performance. It supports distributed training with Keras\n    model.fit, as well as with custom training loops. Multi-GPU support is\n    available, along with experimental support for multi worker and Cloud TPUs.\n    Check out the\n    [guide](https://www.tensorflow.org/beta/guide/distribute_strategy) for more\n    details.\n*   Functions, not Sessions. The traditional declarative programming model of\n    building a graph and executing it via a `tf.Session` is discouraged, and\n    replaced with by writing regular Python functions. Using the `tf.function`\n    decorator, such functions can be turned into graphs which can be executed\n    remotely, serialized, and optimized for performance.\n*   Unification of `tf.train.Optimizers` and `tf.keras.Optimizers`. Use\n    `tf.keras.Optimizers` for TF2.0. `compute_gradients` is removed as public\n    API, use `GradientTape` to compute gradients.\n*   AutoGraph translates Python control flow into TensorFlow expressions,\n    allowing users to write regular Python inside `tf.function`-decorated\n    functions. AutoGraph is also applied in functions used with tf.data,\n    tf.distribute and tf.keras APIs.\n*   Unification of exchange formats to SavedModel. All TensorFlow ecosystem\n    projects (TensorFlow Lite, TensorFlow JS, TensorFlow Serving, TensorFlow\n    Hub) accept SavedModels. Model state should be saved to and restored from\n    SavedModels.\n*   API Changes: Many API symbols have been renamed or removed, and argument\n    names have changed. Many of these changes are motivated by consistency and\n    clarity. The 1.x API remains available in the compat.v1 module. A list of\n    all symbol changes can be found\n    [here](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0).\n    *   API clean-up, included removing `tf.app`, `tf.flags`, and `tf.logging`\n        in favor of [absl-py](https://github.com/abseil/abseil-py).\n*   No more global variables with helper methods like\n    `tf.global_variables_initializer` and `tf.get_global_step`.\n*   Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()`\n    for enabling/disabling v2 control flow.\n*   Enable v2 control flow as part of `tf.enable_v2_behavior()` and\n    `TF2_BEHAVIOR=1`.\n*   Fixes autocomplete for most TensorFlow API references by switching to use\n    relative imports in API `__init__.py` files.\n*   Auto Mixed-Precision graph optimizer simplifies converting models to\n    `float16` for acceleration on Volta and Turing Tensor Cores. This feature\n    can be enabled by wrapping an optimizer class with\n    `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.\n*   Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to \"true\" or \"1\"\n    forces the selection of deterministic cuDNN convolution and max-pooling\n    algorithms. When this is enabled, the algorithm selection procedure itself\n    is also deterministic.\n\n## Breaking Changes\n* Many backwards incompatible API changes have been made to clean up the APIs and make them more consistent.\n* Toolchains:\n  * TensorFlow 2.0.0 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\n  * Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\n  Removed the `freeze_graph` command line tool; `SavedModel` should be used in place of frozen graphs.\n\n* `tf.contrib`:\n  * `tf.contrib` has been deprecated, and functionality has been either migrated to the core TensorFlow API, to an ecosystem project such as [tensorflow/addons](https://www.github.com/tensorflow/addons) or [tensorflow/io](https://www.github.com/tensorflow/io), or removed entirely.\n  * Remove `tf.contrib.timeseries` dependency on TF distributions.\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in `early_stopping.py`.\n\n* `tf.estimator`:\n  * Premade estimators in the tf.estimator.DNN/Linear/DNNLinearCombined family have been updated to use `tf.keras.optimizers` instead of the `tf.compat.v1.train.Optimizer`s. If you do not pass in an `optimizer=` arg or if you use a string, the premade estimator will use the Keras optimizer. This is checkpoint breaking, as the optimizers have separate variables. A checkpoint converter tool for converting optimizers is included with the release,  but if you want to avoid any change, switch to the v1 version of the estimator:  `tf.compat.v1.estimator.DNN/Linear/DNNLinearCombined*`.\n  * Default aggregation for canned Estimators is now `SUM_OVER_BATCH_SIZE`. To maintain previous default behavior, please pass `SUM` as the loss aggregation method.\n  * Canned Estimators don\u2019t support `input_layer_partitioner` arg in the API. If you have this arg, you will have to switch to `tf.compat.v1 canned Estimators`.\n  * `Estimator.export_savedmodel` has been renamed to `export_saved_model`.\n  * When saving to SavedModel, Estimators will strip default op attributes. This is almost always the correct behavior, as it is more forwards compatible, but if you require that default attributes to be saved with the model, please use `tf.compat.v1.Estimator`.\n  * Feature Columns have been upgraded to be more Eager-friendly and to work with Keras. As a result, `tf.feature_column.input_layer` has been deprecated in favor of `tf.keras.layers.DenseFeatures`. v1 feature columns have direct analogues in v2 except for `shared_embedding_columns`, which are not cross-compatible with v1 and v2. Use `tf.feature_column.shared_embeddings` instead.\n\n* `tf.keras`:\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config.  To configure the number of threads, use `tf.config.threading` APIs.\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel. HDF5 files are still supported.\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\n  * Layers now default to float32, and automatically cast their inputs to the layer's dtype. If you had a model that used float64, it will probably silently use float32 in TensorFlow 2, and a warning will be issued that starts with `Layer <layer-name>` is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\n\n* `tf.lite`:\n  * Removed `lite.OpHint`, `lite.experimental`, and `lite.constant` from 2.0 API.\n* Tensors are no longer hashable, but instead compare element-wise with `==` and `!=`. Use `tf.compat.v1.disable_tensor_equality()` to return to the previous behavior.\n* Performing equality operations on Tensors or Variables with incompatible shapes an exception is no longer thrown. Instead `__eq__` returns False and `__ne__` returns True.\n* Removed `tf.string_split` from v2 API.\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\n* Add `UnifiedGRU` as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from `hard_sigmoid` to `sigmoid`, and `reset_after` to True in 2.0. Historically recurrent activation is `hard_sigmoid` since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\n* `CUDNN_INSTALL_PATH`, `TENSORRT_INSTALL_PATH`, `NCCL_INSTALL_PATH`, `NCCL_HDR_PATH` are deprecated. Use `TF_CUDA_PATHS` instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\n\nRefer to our [public project status tracker](https://github.com/orgs/tensorflow/projects/4) and [issues tagged with `2.0`](https://github.com/tensorflow/tensorflow/issues?q=is%3Aopen+is%3Aissue+label%3A2.0) on GitHub for insight into recent issues and development progress.\n\nIf you experience any snags when using TF 2.0, please let us know at the [TF 2.0 Testing User Group](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/testing). We have a support mailing list as well as weekly testing meetings, and would love to hear your migration feedback and questions.\n\n\n## Bug Fixes and Other Changes\n\n*   `tf.contrib`:\n\n    *   Expose `tf.contrib.proto.*` ops in `tf.io` (they will exist in TF2)\n\n*   `tf.data`:\n\n    *   Add support for TensorArrays to `tf.data Dataset`.\n    *   Integrate Ragged Tensors with `tf.data`.\n    *   All core and experimental tf.data transformations that input\n        user-defined functions can span multiple devices now.\n    *   Extending the TF 2.0 support for `shuffle(...,\n        reshuffle_each_iteration=True)` and `cache()` to work across different\n        Python iterators for the same dataset.\n    *   Removing the `experimental_numa_aware` option from `tf.data.Options`.\n    *   Add `num_parallel_reads` and passing in a Dataset containing filenames\n        into `TextLineDataset` and `FixedLengthRecordDataset`.\n    *   Add support for defaulting the value of `cycle_length` argument of\n        `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\n    *   Promoting `tf.data.experimental.enumerate_dataset` to core as\n        `tf.data.Dataset.enumerate`.\n    *   Promoting `tf.data.experimental.unbatch` to core as\n        `tf.data.Dataset.unbatch`.\n    *   Adds option for introducing slack in the pipeline to reduce CPU\n        contention, via `tf.data.Options().experimental_slack = True`\n    *   Added experimental support for parallel batching to `batch()` and\n        `padded_batch()`. This functionality can be enabled through\n        `tf.data.Options()`.\n    *   Support cancellation of long-running `reduce`.\n    *   Now we use `dataset` node name as prefix instead of the op name, to\n        identify the component correctly in metrics, for pipelines with repeated\n        components.\n    *   Improve the performance of datasets using `from_tensors()`.\n    *   Promoting `unbatch` from experimental to core API.\n    *   Adding support for datasets as inputs to `from_tensors` and\n        `from_tensor_slices` and batching and unbatching of nested datasets.\n\n*   `tf.distribute`:\n\n    *   Enable `tf.distribute.experimental.MultiWorkerMirroredStrategy` working\n        in eager mode.\n    *   Callbacks are supported in `MultiWorkerMirroredStrategy`.\n    *   Disable `run_eagerly` and distribution strategy if there are symbolic\n        tensors added to the model using `add_metric` or `add_loss`.\n    *   Loss and gradients should now more reliably be correctly scaled w.r.t.\n        the global batch size when using a `tf.distribute.Strategy`.\n    *   Set default loss reduction as `AUTO` for improving reliability of loss\n        scaling with distribution strategy and custom training loops. `AUTO`\n        indicates that the reduction option will be determined by the usage\n        context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`.\n        When used in distribution strategy scope, outside of built-in training\n        loops such as `tf.keras` `compile` and `fit`, we expect reduction value\n        to be 'None' or 'SUM'. Using other values will raise an error.\n    *   Support for multi-host `ncclAllReduce` in Distribution Strategy.\n\n*   `tf.estimator`:\n\n    *   Replace `tf.contrib.estimator.add_metrics` with\n        `tf.estimator.add_metrics`\n    *   Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs`\n    *   Replace contrib references with `tf.estimator.experimental.*` for apis\n        in early_s in Estimator\n    *   Canned Estimators will now use keras optimizers by default. An error\n        will be raised if tf.train.Optimizers are used, and you will have to\n        switch to tf.keras.optimizers or tf.compat.v1 canned Estimators.\n    *   A checkpoint converter for canned Estimators has been provided to\n        transition canned Estimators that are warm started from\n        `tf.train.Optimizers` to `tf.keras.optimizers`.\n    *   Losses are scaled in canned estimator v2 and not in the optimizers\n        anymore. If you are using Estimator + distribution strategy + optimikzer\n        v1 then the behavior does not change. This implies that if you are using\n        custom estimator with optimizer v2, you have to scale losses. We have\n        new utilities to help scale losses `tf.nn.compute_average_loss`,\n        `tf.nn.scale_regularization_loss`.\n\n*   `tf.keras`:\n\n    *   Premade models (including Linear and WideDeep) have been introduced for\n        the purpose of replacing Premade estimators.\n    *   Model saving changes\n    *   `model.save` and `tf.saved_model.save` may now save to the TensorFlow\n        SavedModel format. The model can be restored using\n        `tf.keras.models.load_model`. HDF5 files are still supported, and may be\n        used by specifying `save_format=\"h5\"` when saving.\n    *   Raw TensorFlow functions can now be used in conjunction with the Keras\n        Functional API during model creation. This obviates the need for users\n        to create Lambda layers in most cases when using the Functional API.\n        Like Lambda layers, TensorFlow functions that result in Variable\n        creation or assign ops are not supported.\n    *   Add support for passing list of lists to the `metrics` argument in Keras\n        `compile`.\n    *   Add `tf.keras.layers.AbstractRNNCell` as the preferred implementation\n        for RNN cells in TF v2. User can use it to implement RNN cells with\n        custom behavior.\n    *   Keras training and validation curves are shown on the same plot when\n        using the TensorBoard callback.\n    *   Switched Keras `fit/evaluate/predict` execution to use only a single\n        unified path by default unless eager execution has been explicitly\n        disabled, regardless of input type. This unified path places an\n        eager-friendly training step inside of a `tf.function`. With this\n    *   All input types are converted to `Dataset`.\n    *   The path assumes there is always a distribution strategy. when\n        distribution strategy is not specified the path uses a no-op\n        distribution strategy.\n    *   The training step is wrapped in `tf.function` unless `run_eagerly=True`\n        is set in compile. The single path execution code does not yet support\n        all use cases. We fallback to the existing v1 execution paths if your\n        model contains the following:\n        1.  `sample_weight_mode` in compile\n        2.  `weighted_metrics` in compile\n        3.  v1 optimizer\n        4.  target tensors in compile If you are experiencing any issues because\n            of this change, please inform us (file an issue) about your use case\n            and you can unblock yourself by setting\n            `experimental_run_tf_function=False` in compile meanwhile. We have\n            seen couple of use cases where the model usage pattern is not as\n            expected and would not work with this change.\n    *   output tensors of one layer is used in the constructor of another.\n    *   symbolic tensors outside the scope of the model are used in custom loss\n        functions. The flag can be disabled for these cases and ideally the\n        usage pattern will need to be fixed.\n    *   Mark Keras `set_session` as `compat.v1` only.\n    *   `tf.keras.estimator.model_to_estimator` now supports exporting to\n        `tf.train.Checkpoint format`, which allows the saved checkpoints to be\n        compatible with `model.load_weights`.\n    *   `keras.backend.resize_images` (and consequently,\n        `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing\n        implementation was fixed.\n    *   Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D`\n        and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor`\n        to store weights, allowing a dramatic speedup for large sparse models.\n    *   Raise error if `batch_size` argument is used when input is\n        dataset/generator/keras sequence.\n    *   Update TF 2.0 `keras.backend.name_scope` to use TF 2.0 `name_scope`.\n    *   Add v2 module aliases for losses, metrics, initializers and optimizers:\n        `tf.losses = tf.keras.losses` & `tf.metrics = tf.keras.metrics` &\n        `tf.initializers = tf.keras.initializers` & `tf.optimizers =\n        tf.keras.optimizers`.\n    *   Updates binary cross entropy logic in Keras when input is probabilities.\n        Instead of converting probabilities to logits, we are using the cross\n        entropy formula for probabilities.\n    *   Added public APIs for `cumsum` and `cumprod` keras backend functions.\n    *   Add support for temporal sample weight mode in subclassed models.\n    *   Raise `ValueError` if an integer is passed to the training APIs.\n    *   Added fault-tolerance support for training Keras model via `model.fit()`\n        with `MultiWorkerMirroredStrategy`, tutorial available.\n    *   Custom Callback tutorial is now available.\n    *   To train with `tf.distribute`, Keras API is recommended over estimator.\n    *   `steps_per_epoch` and `steps` arguments are supported with numpy arrays.\n    *   New error message when unexpected keys are used in\n        sample_weight/class_weight dictionaries\n    *   Losses are scaled in Keras compile/fit and not in the optimizers\n        anymore. If you are using custom training loop, we have new utilities to\n        help scale losses `tf.nn.compute_average_loss`,\n        `tf.nn.scale_regularization_loss`.\n    *   `Layer` apply and add_variable APIs are deprecated.\n    *   Added support for channels first data format in cross entropy losses\n        with logits and support for tensors with unknown ranks.\n    *   Error messages will be raised if `add_update`, `add_metric`, `add_loss`,\n        activity regularizers are used inside of a control flow branch.\n    *   New loss reduction types:\n    *   `AUTO`: Indicates that the reduction option will be determined by the\n        usage context. For almost all cases this defaults to\n        `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside\n        of built-in training loops such as `tf.keras` `compile` and `fit`, we\n        expect reduction value to be `SUM` or `NONE`. Using `AUTO` in that case\n        will raise an error.\n    *   `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis\n        specified by loss function). When this reduction type used with built-in\n        Keras training loops like `fit`/`evaluate`, the unreduced vector loss is\n        passed to the optimizer but the reported loss will be a scalar value.\n    *   `SUM`: Scalar sum of weighted losses. 4. `SUM_OVER_BATCH_SIZE`: Scalar\n        `SUM` divided by number of elements in losses. This reduction type is\n        not supported when used with `tf.distribute.Strategy` outside of\n        built-in training loops like `tf.keras` `compile`/`fit`.\n    *   Wraps losses passed to the `compile` API (strings and v1 losses) which\n        are not instances of v2 `Loss` class in `LossWrapper` class. => All\n        losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\n    *   `model.add_loss(symbolic_tensor)` should work in ambient eager.\n    *   Update metric name to always reflect what the user has given in compile.\n        Affects following cases\n    *   When name is given as 'accuracy'/'crossentropy'\n    *   When an aliased function name is used eg. 'mse'\n    *   Removing the `weighted` prefix from weighted metric names.\n    *   Allow non-Tensors through v2 losses.\n    *   Add v2 sparse categorical crossentropy metric.\n    *   Add v2 APIs for `AUCCurve` and `AUCSummationMethod` enums.\n    *   `add_update` can now be passed a zero-arg callable in order to support\n        turning off the update when setting `trainable=False` on a Layer of a\n        Model compiled with `run_eagerly=True`.\n    *   Standardize the LayerNormalization API by replacing the args `norm_axis`\n        and `params_axis` with `axis`.\n    *   Fixed critical bugs that help with DenseFeatures usability in TF2\n\n*   `tf.lite`:\n\n    *   Added evaluation script for `COCO` minival\n    *   Add delegate support for `QUANTIZE`.\n    *   Add `GATHER` support to NN API delegate.\n    *   Added support for TFLiteConverter Python API in 2.0. Contains functions\n        from_saved_model, from_keras_file, and from_concrete_functions.\n    *   Add `EXPAND_DIMS` support to NN API delegate TEST.\n    *   Add `narrow_range` attribute to QuantizeAndDequantizeV2 and V3.\n    *   Added support for `tflite_convert` command line tool in 2.0.\n    *   Post-training quantization tool supports quantizing weights shared by\n        multiple operations. The models made with versions of this tool will use\n        INT8 types for weights and will only be executable interpreters from\n        this version onwards.\n    *   Post-training quantization tool supports fp16 weights and GPU delegate\n        acceleration for fp16.\n    *   Add delegate support for `QUANTIZED_16BIT_LSTM`.\n    *   Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\n\n*   TensorRT\n\n    *   Add TensorFlow 2.0-compatible `TrtGraphConverterV2` API for TensorRT\n        conversion. TensorRT initialization arguments are now passed wrapped in\n        a named-tuple, `TrtConversionParams`, rather than as separate arguments\n        as in `TrtGraphConverter`.\n    *   Changed API to optimize TensorRT engines during graph optimization. This\n        is now done by calling `converter.build()` where previously\n        `is_dynamic_op=False` would be set.\n    *   `converter.convert()` no longer returns a `tf.function`. Now the\n        function must be accessed from the saved model.\n    *   The `converter.calibrate()` method has been removed. To trigger\n        calibration, a `calibration_input_fn` should be provided to\n        `converter.convert()`.\n\n*   Other:\n\n    *   Fix accidental quadratic graph construction cost in graph-mode\n        `tf.gradients()`.\n    *   ResourceVariable's gather op supports batch dimensions.\n    *   ResourceVariable support for `gather_nd`.\n    *   `ResourceVariable` and `Variable` no longer accepts `constraint` in the\n        constructor, nor expose it as a @property.\n    *   Added gradient for `SparseToDense` op.\n    *   Expose a flag that allows the number of threads to vary across Python\n        benchmarks.\n    *   `image.resize` in 2.0 now supports gradients for the new resize kernels.\n    *   `image.resize` now considers proper pixel centers and has new kernels\n        (incl. anti-aliasing).\n    *   Renamed `tf.image` functions to remove duplicate \"image\" where it is\n        redundant.\n    *   Variadic reduce is supported on CPU Variadic reduce is supported on CPU\n    *   Remove unused `StringViewVariantWrapper`.\n    *   Delete unused `Fingerprint64Map` op registration\n    *   Add broadcasting support to `tf.matmul`.\n    *   Add C++ Gradient for `BatchMatMulV2`.\n    *   Add `tf.math.cumulative_logsumexp` operation.\n    *   Add ellipsis (...) support for `tf.einsum()`.\n    *   Add expand_composites argument to all `nest.*` methods.\n    *   Added `strings.byte_split`.\n    *   Add a new \"result_type\" parameter to `tf.strings.split`.\n    *   Add name argument to `tf.string_split` and `tf.strings_split`.\n    *   Extend `tf.strings.split` to support inputs with any rank.\n    *   Added `tf.random.binomial`.\n    *   Added `key` and `skip` methods to `random.experimental.Generator`.\n    *   Extend `tf.function` with basic support for CompositeTensors arguments\n        (such as `SparseTensor` and `RaggedTensor`).\n    *   `parallel_for.pfor`: add converters for Softmax, LogSoftmax, IsNaN, All,\n        Any, and MatrixSetDiag.\n    *   `parallel_for`: add converters for LowerTriangularSolve and Cholesky.\n    *   `parallel_for`: add converters for `LogMatrixDeterminant` and\n        `MatrixBandPart`.\n    *   `parallel_for`: Add converter for `MatrixDiag`.\n    *   `parallel_for`: Add converters for `OneHot`, `LowerBound`, `UpperBound`.\n    *   `parallel_for`: add converter for `BroadcastTo`.\n    *   Add `pfor` converter for `Squeeze`.\n    *   Add `RaggedTensor.placeholder()`.\n    *   Add ragged tensor support to `tf.squeeze`.\n    *   Update RaggedTensors to support int32 row_splits.\n    *   Allow `LinearOperator.solve` to take a `LinearOperator`.\n    *   Allow all dtypes for `LinearOperatorCirculant`.\n    *   Introduce MaxParallelism method\n    *   Add `LinearOperatorHouseholder`.\n    *   Adds Philox support to new stateful RNG's XLA path.\n    *   Added `TensorSpec` support for CompositeTensors.\n    *   Added `tf.linalg.tridiagonal_solve` op.\n    *   Added partial_pivoting input parameter to `tf.linalg.tridiagonal_solve`.\n    *   Added gradient to `tf.linalg.tridiagonal_solve`.\n    *   Added `tf.linalg.tridiagonal_mul op`.\n    *   Added GPU implementation of `tf.linalg.tridiagonal_matmul`.\n    *   Added `LinearOperatorToeplitz`.\n    *   Upgraded LIBXSMM to version 1.11.\n    *   Uniform processing of quantized embeddings by Gather and EmbeddingLookup\n        Ops.\n    *   Correct a misstatement in the documentation of the sparse softmax cross\n        entropy logit parameter.\n    *   Add `tf.ragged.boolean_mask`.\n    *   `tf.switch_case` added, which selects a branch_fn based on a\n        branch_index.\n    *   The C++ kernel of gather op supports batch dimensions.\n    *   Fixed default value and documentation for `trainable` arg of\n        tf.Variable.\n    *   `EagerTensor` now supports numpy buffer interface for tensors.\n    *   This change bumps the version number of the `FullyConnected` Op to 5.\n    *   Added new op: `tf.strings.unsorted_segment_join`.\n    *   Added HW acceleration support for `topK_v2`.\n    *   CloudBigtable version updated to v0.10.0 BEGIN_PUBLIC CloudBigtable\n        version updated to v0.10.0.\n    *   Expose `Head` as public API.\n    *   Added `tf.sparse.from_dense` utility function.\n    *   Improved ragged tensor support in `TensorFlowTestCase`.\n    *   Added a function `nested_value_rowids` for ragged tensors.\n    *   Added `tf.ragged.stack`.\n    *   Makes the a-normal form transformation in Pyct configurable as to which\n        nodes are converted to variables and which are not.\n    *   `ResizeInputTensor` now works for all delegates.\n    *   `tf.cond` emits a StatelessIf op if the branch functions are stateless\n        and do not touch any resources.\n    *   Add support of local soft device placement for eager op.\n    *   Pass partial_pivoting to the `_TridiagonalSolveGrad`.\n    *   Add HW acceleration support for `LogSoftMax`.\n    *   Add guard to avoid acceleration of L2 Normalization with input rank != 4\n    *   Fix memory allocation problem when calling `AddNewInputConstantTensor`.\n    *   Delegate application failure leaves interpreter in valid state\n    *   `tf.while_loop` emits a StatelessWhile op if the cond and body functions\n        are stateless and do not touch any resources.\n    *   `tf.cond`, `tf.while` and if and while in AutoGraph now accept a\n        nonscalar predicate if has a single element. This does not affect non-V2\n        control flow.\n    *   Fix potential security vulnerability where decoding variant tensors from\n        proto could result in heap out of bounds memory access.\n    *   Only create a GCS directory object if the object does not already exist.\n    *   Introduce `dynamic` constructor argument in Layer and Model, which\n        should be set to `True` when using imperative control flow in the `call`\n        method.\n    *   Begin adding Go wrapper for C Eager API.\n    *   XLA HLO graphs can be inspected with interactive_graphviz tool now.\n    *   Add dataset ops to the graph (or create kernels in Eager execution)\n        during the python Dataset object creation instead doing it during\n        Iterator creation time.\n    *   Add `batch_dims` argument to `tf.gather`.\n    *   The behavior of `tf.gather` is now correct when `axis=None` and\n        `batch_dims<0`.\n    *   Update docstring for gather to properly describe the non-empty\n        `batch_dims` case.\n    *   Removing of dtype in the constructor of initializers and partition_info\n        in call.\n    *   Add `tf.math.nextafter` op.\n    *   Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically\n        dispatches the best kernel implementation based on CPU vector\n        architecture. To disable them, build with\n        `--define=tensorflow_mkldnn_contraction_kernel=0`.\n    *   `tf.linspace(start, stop, num)` now always uses \"stop\" as last value\n        (for num > 1)\n    *   Added top-k to precision and recall to keras metrics.\n    *   Add a ragged size op and register it to the op dispatcher\n    *   Transitive dependencies on :`pooling_ops` were removed. Some users may\n        need to add explicit dependencies on :`pooling_ops` if they reference\n        the operators from that library.\n    *   Add `CompositeTensor` base class.\n    *   Malformed gif images could result in an access out of bounds in the\n        color palette of the frame. This has been fixed now\n    *   Add templates and interfaces for creating lookup tables\n    *   `Tensor::UnsafeCopyFromInternal` deprecated in favor\n        `Tensor::BitcastFrom`.\n    *   In `map_vectorization` optimization, reduce the degree of parallelism in\n        the vectorized map node.\n    *   Add variant wrapper for `absl::string_view`.\n    *   Add OpKernels for some stateless maps.\n    *   DType is no longer convertible to an int. Use `dtype.as_datatype_enum`\n        instead of `int(dtype)` to get the same result.\n    *   Support both binary and -1/1 label input in v2 hinge and squared hinge\n        losses.\n    *   Added `LinearOperator.adjoint` and `LinearOperator.H` (alias).\n    *   Expose CriticalSection in core as `tf.CriticalSection`.\n    *   Enhanced graphviz output.\n    *   Add opkernel templates for common table operations.\n    *   Fix callbacks do not log values in eager mode when a deferred build\n        model is used.\n    *   `SignatureDef` util functions have been deprecated.\n    *   Update `Fingerprint64Map` to use aliases\n    *   Add legacy string flat hash map op kernels.\n    *   Add support for `add_metric` in the graph function mode.\n    *   Updating cosine similarity loss - removed the negate sign from cosine\n        similarity.\n    *   Changed default for gradient accumulation for TPU embeddings to true.\n    *   Adds summary trace API for collecting graph and profile information.\n    *   The `precision_mode` argument to `TrtGraphConverter` is now case\n        insensitive.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n1e100, a6802739, 4d55397500, a6802739, Abdullah Selek, abenmao, Abolfazl\nShahbazi, Adam Richter, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Alex, Alex\nItkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Aman Patel,\nAmit, Amit Kumar Jaiswal, Amit Srivastava, amoitra, Andreas Eberle, Andrew\nLihonosov, Andy Craze, Anshuman Tripathy, Anthony Hsu, Anthony Platanios, Anuj\nRawat, arp95, Arpit Shah, Armen Poghosov, armenpoghosov, Astropeak, Ashwin\nRamaswami, Arpit Shah, Augustina Ragwitz, Aurelien Geron, Aur\u00e9Lien Geron,\navasid, aweers, awesomealex1, Ayush Agrawal, Bas Aarts, Bastian Eichenberger,\nBairen Yi, Bayberry Z, Ben Barsdell, Benjamin Peterson, bhack, Bharat\nRaghunathan, Bhavani Subramanian, Bin Fan, blairhan, Bl\u00e9Nesi Attila, Bodin-E,\nBrandon Carter, Bryan Cutler, candy.dc, Cao Zongyan, Casper Da Costa-Luis, Chao\nLiu, Chen Guoyin, chenchc, chengchingwen, chie8842, Christian Hansen, Christoph\nBoeddeker, Christopher Yeh, Clayne Robison, Coady, Patrick, crafet, csukuangfj,\nctiijima, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Daniel Rasmussen, Daniel\nSalvadori, Dave Airlie, David Norman, Dayananda V, delock, Denis Khalikov, Deven\nDesai, Dheeraj Rajaram Reddy, Diego Caballero, dmitrievanthony, Donovan Ong,\nDrew Szurko, Duncan Dean, Duncan Riach, Dustin Neighly, Dwight J Lyle, Eamon\nIto-Fisher, eashtian3, Edward Forgacs, EFanZh, ejot, Elroy Ashtian Jr, Eric\nSchweitz, Evgeniy Polyakov, Fangjun Kuang, Federico Martinez, Fei Hu, Felix\nLemke, Filip Matzner, FlashTek, fo40225, formath, Franc\u0327Ois Chollet, frreiss,\nFred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel,\nGautam, gehring, Geoffrey Irving, George Grzegorz Pawelczak, Grzegorz Pawelczak,\nGeorge Sterpu, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein,\nGurpreet Singh, Gustavo Lima Chaves, Gyoung-Yoon Ryoo, haison, Hanton Yang,\nHanGuo97, Haraldur T\u00f3Mas Hallgr\u00edMsson, Hari Shankar, hehongliang, Heungsub Lee,\nHoeseong Kim, Huan Li (\u674e\u5353\u6853), H\u00e5Kon Sandsmark, I-Hong, I-Hong Jhuo, Ilham\nFirdausi Putra, Ilango R, Imran Salam, Innovimax, Jacky Ko, Irene Dea, Ivan\nHabernal, Jakub Lipinski, Jacky, Jason Zaman, Jason Zavaglia, jayhpark530,\njcf94, jefby, Jeff Daily, Jeff Poznanovic, Jeffrey Poznanovic, Jekyll Lai, jer,\nJeroen B\u00e9Dorf, jerryyin, jhalakp, jiakai, Jia Qingtong, Jiankang, JiangXIAO, Joe\nBowser, Joe Q, Joe Quadrino, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese,\nJonas Rauber, Jonathan Kyl, Jonathan, Joon, Joppe Geluykens, Joseph Friedman,\nJosh Beal, jtressle, Julian Niedermeier, Junqin Zhang, Justin Dujardin, Justin\nTunis, jwu, K. Hodges, kaixih, Kaixi Hou, kjopek, Karl Lessard, Karl\nWeinmeister, Karthik Muthuraman, Kashif Rasul, Kay Zhu, Kbhute-Ibm, KDR, Keno\nFischer, Kevin Mader, khanhlvg, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan,\nKoock Yoon, kouml, ktaebum, Kyuwon Kim, Lakshay Tokas, Laurent Le Brun,\nleike666666, leonard951, Leslie-Fang, Letian Kang, Li, Guizi, Loo Rong Jie,\nLucas Hendren, Lukas Folle, Lukas Geiger, Luke Han, luxupu, lvli, Ma, Guokai,\nMahmoud Abuzaina, Maksym Kysylov, Mandar Deshpande, manhyuk, Manraj Singh\nGrover, Marco Gaido, Marek Drozdowski, Margaret Maynard-Reid, Mark Ryan, mars20,\nMateusz Chudyk, Matt Conley, mbhuiyan, mdfaijul, Mei Jie, Melissa Grueter,\nmerturl, MichaelKonobeev, Michael K\u00e4Ufl, Michal W. Tarnowski, Micka\u00ebL\nSchoentgen, Miguel Morin, Mihail Salnikov, Mikalai Drabovich, Mike Arpaia, Mike\nHolcomb, minds, monklof, Moses Marin, mpppk, Mr. Metal, Mshr-H, musikisomorphie,\nnammbash, Natalia Gimelshein, Nathan Luehr, Nayana-Ibm, Nayana Thorat, neargye,\nNeeraj Pradhan, Nehal J Wani, Neil, Nick, Nick Lewycky, Niels Ole Salscheider,\nNiklas Silfverstr\u00f6M, Niranjan Hasabnis, Nuka-137, Nutti, ocjosen, olicht,\nomeir1, P Sudeepam, Paige Bailey, Palmer Lao, Pan Daoxin, Pariksheet Pinjari,\nPasquale Minervini, Patrick J. Lopresti, Patrik Gustavsson, Pavel Akhtyamov,\nPavel Samolysov, PENGWA, per1234, PeterLee, Phan Van Nguyen Duc, Philipp Jund,\nPhillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, R\nS Nikhil Krishna, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben\nMorais, robert, Rohit Gupta, Roland Zimmermann, Roman Soldatow, RonLek, Ruizhe,\nRyan Jiang, saishruthi, Saleem Abdulrasool, Samantha Andow, Sami Kama,\nSana-Damani, Saurabh Deoras, sdamani, Sean Morgan, seanshpark, Sebastien Iooss,\nServ-Inc, Severen Redwood, Shahzad Lone, Shashank Gupta, shashvat, Shashvat\nChand Shahi, Shubham Goyal, Shashi, Sigrid Keydana, Siju, Siju Samuel,\nsleighsoft, smilu97, Snease-Abq, Son Tran, Spencer Schaber, sremedios, Srini511,\nsrinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Subin, Sumesh Udayakumaran,\nSungmann Cho, sunway513, Supriya Rao, sxwang, Tae-Hwan Jung, Taehoon Lee, Takeo\nSawada, Taylor Jakobson, Taylor Thornton, Ted Chang, TengLu, terryky,\nThisIsIsaac, ThisIsPIRI, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Till\nHoffmann, Tim Zaman, tomguluson92, Tongxuan Liu, Trent Lo, Trevor Morris,\nTungJerry, Tyorden, Uday Bondhugula, v1incent, Vagif, Vasileios Lioutas,\nvbvg2008, vcarpani, Vijay Ravichandran, Vikram Tiwari,Viktor Gal, Vishwak\nSrinivasan, Vincent, Vishnuvardhan Janapati, Vitor-Alves, Vivek Suryamurthy,\nwangsiyu, wateryzephyr, WeberXie, Wei Wang, WeijieSun, Wen-Heng (Jack) Chung,\nwenxizhu, Will Battel, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui,\nXiaoquan Kong, Xin, Xinping Wang, Yan Facai (\u989c\u53d1\u624d), Yann-Yy, Yasir Modak,\nYasuhiro Matsumoto, ymodak, Yong Tang, Yongfeng Gu, Younes Khoudli, Yuan Lin,\nYuan (Terry) Tang, Yuchen Ying, Yves-Noel Weweler, zhangyujing, zjjott, zyeric,\n\u738b\u632f\u534e (Zhenhua Wang), \u9ec4\u946b\n\n# Release 1.14.0\n\n## Major Features and Improvements\n\n*   This is the first 1.x release containing the compat.v2 module. This module\n    is required to allow libraries to publish code which works in both 1.x and\n    2.x. After this release, no backwards incompatible changes are allowed in\n    the 2.0 Python API.\n*   Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically\n    dispatches the best kernel implementation based on CPU vector architecture.\n    To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\n\n## Behavioral changes\n\n*   Set default loss reduction as `AUTO` for improving reliability of loss\n    scaling with distribution strategy and custom training loops. `AUTO`\n    indicates that the reduction option will be determined by the usage context.\n    For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in\n    distribution strategy scope, outside of built-in training loops such as\n    `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or\n    'SUM'. Using other values will raise an error.\n*   Wraps losses passed to the `compile` API (strings and v1 losses) which are\n    not instances of v2 `Loss` class in `LossWrapper` class. => All losses will\n    now use `SUM_OVER_BATCH_SIZE` reduction as default.\n*   Disable `run_eagerly` and distribution strategy if there are symbolic\n    tensors added to the model using `add_metric` or `add_loss`.\n*   tf.linspace(start, stop, num) now always uses \"stop\" as last value (for\n    num > 1)\n*   `ResourceVariable` and `Variable` no longer accepts `constraint` in the\n    constructor, nor expose it as a @property.\n*   The behavior of tf.gather is now correct when axis=None and batch_dims<0.\n*   Only create a GCS directory object if the object does not already exist.\n*   In `map_vectorization` optimization, reduce the degree of parallelism in the\n    vectorized map node.\n*   Bug fix: loss and gradients should now more reliably be correctly scaled\n    w.r.t. the global batch size when using a tf.distribute.Strategy.\n*   Updating cosine similarity loss - removed the negate sign from cosine\n    similarity.\n*   DType is no longer convertible to an int. Use dtype.as_datatype_enum instead\n    of int(dtype) to get the same result.\n*   Changed default for gradient accumulation for TPU embeddings to true.\n*   Callbacks now log values in eager mode when a deferred build model is used.\n*   Transitive dependencies on :pooling_ops were removed. Some users may need to\n    add explicit dependencies on :pooling_ops if they reference the operators\n    from that library.\n*   tf.keras.optimizers default learning rate changes:\n    *   Adadelta: 1.000 to 0.001\n    *   Adagrad: 0.01 to 0.001\n    *   Adamax: 0.002 to 0.001\n    *   NAdam: 0.002 to 0.001\n\n## Bug Fixes and Other Changes\n\n*   Documentation\n*   Deprecations and Symbol renames.\n    *   Remove unused StringViewVariantWrapper\n    *   Delete unused Fingerprint64Map op registration\n    *   SignatureDef util functions have been deprecated.\n    *   Renamed tf.image functions to remove duplicate \"image\" where it is\n        redundant.\n    *   tf.keras.experimental.export renamed to\n        tf.keras.experimental.export_saved_model\n    *   Standardize the LayerNormalization API by replacing the args `norm_axis`\n        and `params_axis` with `axis`.\n    *   Tensor::UnsafeCopyFromInternal deprecated in favor Tensor::BitcastFrom\n*   Keras & Python API\n    *   Add v2 module aliases for:\n    *   tf.initializers => tf.keras.initializers\n    *   tf.losses => tf.keras.losses & tf.metrics => tf.keras.metrics\n    *   tf.optimizers => tf.keras.optimizers\n    *   Add tf.keras.layers.AbstractRNNCell as the preferred implementation of\n        RNN cell for TF v2. User can use it to implement RNN cell with custom\n        behavior.\n    *   Adding `clear_losses` API to be able to clear losses at the end of\n        forward pass in a custom training loop in eager.\n    *   Add support for passing list of lists to the `metrics` param in Keras\n        `compile`.\n    *   Added top-k to precision and recall to keras metrics.\n    *   Adding public APIs for `cumsum` and `cumprod` keras backend functions.\n    *   Fix: model.add_loss(symbolic_tensor) should work in ambient eager.\n    *   Add name argument to tf.string_split and tf.strings_split\n    *   Minor change to SavedModels exported from Keras using\n        tf.keras.experimental.export. (SignatureDef key for evaluation mode is\n        now \"eval\" instead of \"test\"). This will be reverted back to \"test\" in\n        the near future.\n    *   Updates binary cross entropy logic in Keras when input is probabilities.\n        Instead of converting probabilities to logits, we are using the cross\n        entropy formula for probabilities.\n    *   Raw TensorFlow functions can now be used in conjunction with the Keras\n        Functional API during model creation. This obviates the need for users\n        to create Lambda layers in most cases when using the Functional API.\n        Like Lambda layers, TensorFlow functions that result in Variable\n        creation or assign ops are not supported.\n    *   Keras training and validation curves are shown on the same plot.\n    *   Introduce `dynamic` constructor argument in Layer and Model, which\n        should be set to True when using imperative control flow in the `call`\n        method.\n    *   Removing of dtype in the constructor of initializers and partition_info\n        in call.\n*   New ops and improved op functionality\n    *   Add OpKernels for some stateless maps\n    *   Add v2 APIs for AUCCurve and AUCSummationMethod\n        enums. #tf-metrics-convergence\n    *   Add tf.math.nextafter op.\n    *   Add CompositeTensor base class.\n    *   Add tf.linalg.tridiagonal_solve op.\n    *   Add opkernel templates for common table operations.\n    *   Added support for TFLite in TensorFlow 2.0.\n    *   Adds summary trace API for collecting graph and profile information.\n    *   Add batch_dims argument to tf.gather.\n    *   Add support for `add_metric` in the graph function mode.\n    *   Add C++ Gradient for BatchMatMulV2.\n    *   Added tf.random.binomial\n    *   Added gradient for SparseToDense op.\n    *   Add legacy string flat hash map op kernels\n    *   Add a ragged size op and register it to the op dispatcher\n    *   Add broadcasting support to tf.matmul.\n    *   Add ellipsis (...) support for tf.einsum()\n    *   Added LinearOperator.adjoint and LinearOperator.H (alias).\n    *   Added GPU implementation of tf.linalg.tridiagonal_solve.\n    *   Added strings.byte_split\n    *   Add RaggedTensor.placeholder()\n    *   Add a new \"result_type\" parameter to tf.strings.split\n    *   `add_update` can now be passed a zero-arg callable in order to support\n        turning off the update when setting `trainable=False` on a Layer of a\n        Model compiled with `run_eagerly=True`.\n    *   Add variant wrapper for absl::string_view\n    *   Add expand_composites argument to all nest.* methods.\n    *   Add pfor converter for Squeeze.\n    *   Bug fix for tf.tile gradient\n    *   Expose CriticalSection in core as tf.CriticalSection.\n    *   Update Fingerprint64Map to use aliases\n    *   ResourceVariable support for gather_nd.\n    *   ResourceVariable's gather op supports batch dimensions.\n    *   Variadic reduce is supported on CPU\n    *   Extend tf.function with basic support for CompositeTensors arguments\n        (such as SparseTensor and RaggedTensor).\n    *   Add templates and interfaces for creating lookup tables\n    *   Post-training quantization tool supports quantizing weights shared by\n        multiple operations. The models made with versions of this tool will use\n        INT8 types for weights and will only be executable interpreters from\n        this version onwards.\n    *   Malformed gif images could result in an access out of bounds in the\n        color palette of the frame. This has been fixed now\n    *   image.resize now considers proper pixel centers and has new kernels\n        (incl. anti-aliasing).\n    *   Added an isotonic regression solver (tf.nn.isotonic_regression).\n*   Performance\n    *   Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically\n        dispatches the best kernel implementation based on CPU vector\n        architecture. To disable them, build with\n        --define=tensorflow_mkldnn_contraction_kernel=0.\n    *   Support for multi-host ncclAllReduce in Distribution Strategy.\n    *   Expose a flag that allows the number of threads to vary across Python\n        benchmarks.\n*   TensorFlow 2.0 Development\n    *   Add v2 sparse categorical crossentropy metric.\n    *   Allow non-Tensors through v2 losses.\n    *   Add UnifiedGRU as the new GRU implementation for tf2.0. Change the\n        default recurrent activation function for GRU from 'hard_sigmoid' to\n        'sigmoid', and 'reset_after' to True in 2.0. Historically recurrent\n        activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new\n        unified backend between CPU and GPU mode, since the CuDNN kernel is\n        using sigmoid, we change the default for CPU mode to sigmoid as well.\n        With that, the default GRU will be compatible with both CPU and GPU\n        kernel. This will enable user with GPU to use CuDNN kernel by default\n        and get a 10x performance boost in training. Note that this is\n        checkpoint breaking change. If user want to use their 1.x pre-trained\n        checkpoint, please construct the layer with\n        GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback\n        to 1.x behavior.\n    *   TF 2.0 - Update metric name to always reflect what the user has given in\n        compile. Affects following cases 1. When name is given as\n        'accuracy'/'crossentropy' 2. When an aliased function name is used eg.\n        'mse' 3. Removing the `weighted` prefix from weighted metric names.\n    *   Begin adding Go wrapper for C Eager API\n    *   image.resize in 2.0 now supports gradients for the new resize kernels.\n    *   removed tf.string_split from v2 API\n    *   Expose tf.contrib.proto.* ops in tf.io (they will exist in TF2)\n    *   \"Updates the TFLiteConverter API in 2.0. Changes from_concrete_function\n        to from_concrete_functions.\"\n    *   Enable tf.distribute.experimental.MultiWorkerMirroredStrategy working in\n        eager mode.\n    *   Support both binary and -1/1 label input in v2 hinge and squared hinge\n        losses.\n*   TensorFlow Lite\n    *   \"Adds support for tflite_convert in 2.0.\"\n    *   \"Remove lite.OpHint, lite.experimental, and lite.constant from 2.0 API.\"\n*   tf.contrib\n    *   Added Neural Turing Implementation as described in\n        https://arxiv.org/abs/1807.08518.\n    *   Remove tf.contrib.timeseries dependency on TF distributions.\n*   tf.data\n    *   Add num_parallel_reads and passing in a Dataset containing filenames\n        into TextLineDataset and FixedLengthRecordDataset\n    *   Going forward we operate in TF 2.0, this change is part of the effort to\n        slowly converting XYZDataset to DatasetV2 type which is the official\n        version going to be used in TF 2.0 and motivated by some compatibility\n        issue found, _BigtableXYZDataset (of type DatasetV2) does not implement\n        the _as_variant_tensor() of DatasetV1, when moving contrib.bigtable to\n        tensorflow_io. Converting into DatasetV2 removes the overheads to\n        maintain V1 while we are moving into TF 2.0.\n    *   Add dataset ops to the graph (or create kernels in Eager execution)\n        during the python Dataset object creation instead doing it during\n        Iterator creation time.\n    *   Add support for TensorArrays to tf.data Dataset.\n    *   Switching tf.data functions to use `defun`, providing an escape hatch to\n        continue using the legacy `Defun`.\n*   Toolchains\n    *   CUDNN_INSTALL_PATH, TENSORRT_INSTALL_PATH, NCCL_INSTALL_PATH,\n        NCCL_HDR_PATH are deprecated. Use TF_CUDA_PATHS instead which supports a\n        comma-separated list of base paths that are searched to find CUDA\n        libraries and headers.\n    *   TF code now resides in `tensorflow_core` and `tensorflow` is just a\n        virtual pip package. No code changes are needed for projects using\n        TensorFlow, the change is transparent\n*   XLA\n    *   XLA HLO graphs can be inspected with interactive_graphviz tool now.\n*   Estimator\n    *   Use tf.compat.v1.estimator.inputs instead of tf.estimator.inputs\n    *   Replace contrib references with tf.estimator.experimental.* for apis in\n        early_stopping.py\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n1e100, 4d55397500, a6802739, abenmao, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy,\nAlex, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, Andreas Eberle,\nAndy Craze, Anthony Platanios, Armen Poghosov, armenpoghosov, arp95, Arpit Shah,\nAshwin Ramaswami, Aurelien Geron, Aur\u00e9Lien Geron, aweers, awesomealex1, Ayush\nAgrawal, Ben Barsdell, Bharat Raghunathan, Bhavani Subramanian, blairhan,\nBl\u00e9Nesi Attila, Brandon Carter, candy.dc, Chao Liu, chenchc, chie8842, Christian\nHansen, Christian Sigg, Clayne Robison, crafet, csukuangfj, ctiijima, Dan\nJarvis, Dan Lazewatsky, Daniel Ingram, Daniel Salvadori, Dave Airlie, David\nNorman, Dayananda V, Dayananda-V, delock, Denis Khalikov, Deven Desai, Dheeraj\nRajaram Reddy, dmitrievanthony, Donovan Ong, Drew Szurko, Duncan Riach, Dustin\nNeighly, Edward Forgacs, EFanZh, Fei Hu, Felix Lemke, Filip Matzner, fo40225,\nfrreiss, Gautam, gehring, Geoffrey Irving, Grzegorz George Pawelczak, Grzegorz\nPawelczak, Gyoung-Yoon Ryoo, HanGuo97, Hanton Yang, Hari Shankar, hehongliang,\nHeungsub Lee, Hoeseong Kim, I-Hong Jhuo, Ilango R, Innovimax, Irene Dea, Jacky\nKo, Jakub Lipinski, Jason Zaman, jcf94, Jeffrey Poznanovic, Jens Elofsson,\nJeroen B\u00e9Dorf, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Joeran Beel, Jonas\nRauber, Jonathan, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu,\nK Yasaswi Sri Chandra Gandhi, K. Hodges, Kaixi Hou, Karl Lessard, Karl\nWeinmeister, Karthik Muthuraman, Kashif Rasul, KDR, Keno Fischer, Kevin Mader,\nkjopek, Koan-Sin Tan, kouml, ktaebum, Lakshay Tokas, Laurent Le Brun, Letian\nKang, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, luxupu,\nMa, Guokai, Mahmoud Abuzaina, Mandar Deshpande, manhyuk, Marco Gaido, Marek\nDrozdowski, Mark Collier, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley,\nMattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL\nSchoentgen, Miguel Morin, Mihail Salnikov, Mike Arpaia, Mike Holcomb, monklof,\nMoses Marin, Mshr-H, nammbash, Natalia Gimelshein, Nayana-Ibm, neargye, Neeraj\nPradhan, Nehal J Wani, Nick, Niels Ole Salscheider, Niranjan Hasabnis, nlewycky,\nNuka-137, Nutti, olicht, P Sudeepam, Palmer Lao, Pan Daoxin, Pariksheet Pinjari,\nPavel Samolysov, PENGWA, Pooya Davoodi, R S Nikhil Krishna, Rohit Gupta, Roman\nSoldatow, rthadur, Ruizhe, Ryan Jiang, Samantha Andow, Sami Kama, Sana-Damani,\nSaurabh Deoras, sdamani, seanshpark, Sebastien Iooss, Serv-Inc, Shahzad Lone,\nShashank Gupta, Shashi, shashvat, shashvatshahi1998, Siju, Siju Samuel,\nSnease-Abq, Spencer Schaber, sremedios, srinivasan.narayanamoorthy, Steve Lang,\nSteve Nesae, Sumesh Udayakumaran, Supriya Rao, Taylor Jakobson, Taylor Thornton,\nTed Chang, ThisIsPIRI, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Tim Zaman,\ntomguluson92, Tongxuan Liu, TungJerry, v1incent, Vagif, vcarpani, Vikram Tiwari,\nVishwak Srinivasan, Vitor-Alves, wangsiyu, wateryzephyr, WeberXie, WeijieSun,\nWen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, wyzhao, Xin,\nYasuhiro Matsumoto, ymodak, Yong Tang, Younes Khoudli, Yuan Lin, Yves-Noel\nWeweler, Zantares, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Wang Zhenhua), \u9ec4\u946b\n\n# Release 1.12.3\n\n## Bug Fixes and Other Changes\n\n*   Updates `png_archive` dependency to 1.6.37 to not be affected by\n    CVE-2019-7317, CVE-2018-13785, and CVE-2018-14048.\n*   Updates `sqlite` dependency to 3.28.0 to not be affected by CVE-2018-20506,\n    CVE-2018-20346, and CVE-2018-20505.\n\n# Release 1.12.2\n\n## Bug Fixes and Other Changes\n\n*   Fixes a potential security vulnerability where carefully crafted GIF images\n    can produce a null pointer dereference during decoding.\n\n# Release 1.13.0\n\n## Major Features and Improvements\n\n* TensorFlow Lite has moved from contrib to core. This means that Python modules are under `tf.lite` and source code is now under `tensorflow/lite` rather than `tensorflow/contrib/lite`.\n* TensorFlow GPU binaries are now built against CUDA 10 and TensorRT 5.0.\n* Support for Python3.7 on all operating systems.\n* Moved NCCL to core.\n\n## Behavioral changes\n\n* Disallow conversion of python floating types to uint32/64 (matching behavior of other integer types) in `tf.constant`.\n* Make the `gain` argument of convolutional orthogonal initializers (`convolutional_delta_orthogonal`, `convolutional_orthogonal_1D`, `convolutional_orthogonal_2D`, `convolutional_orthogonal_3D`) have consistent behavior with the `tf.initializers.orthogonal` initializer, i.e. scale the output l2-norm by `gain` and NOT by `sqrt(gain)`. (Note that these functions are currently in `tf.contrib` which is not guaranteed backward compatible).\n\n## Bug Fixes and Other Changes\n\n*   Documentation\n    *   Update the doc with the details about the rounding mode used in\n        quantize_and_dequantize_v2.\n    *   Clarify that tensorflow::port::InitMain() _should_ be called before\n        using the TensorFlow library. Programs failing to do this are not\n        portable to all platforms.\n*   Deprecations and Symbol renames.\n    *   Removing deprecations for the following endpoints: `tf.acos`,\n        `tf.acosh`, `tf.add`, `tf.as_string`, `tf.asin`, `tf.asinh`, `tf.atan`,\n        `tf.atan2`, `tf.atanh`, `tf.cos`, `tf.cosh`, `tf.equal`, `tf.exp`,\n        `tf.floor`, `tf.greater`, `tf.greater_equal`, `tf.less`,\n        `tf.less_equal`, `tf.log`, `tf.logp1`, `tf.logical_and`,\n        `tf.logical_not`, `tf.logical_or`, `tf.maximum`, `tf.minimum`,\n        `tf.not_equal`, `tf.sin`, `tf.sinh`, `tf.tan`\n    *   Deprecate `tf.data.Dataset.shard`.\n    *   Deprecate `saved_model.loader.load` which is replaced by\n        `saved_model.load` and `saved_model.main_op`, which will be replaced by\n        `saved_model.main_op` in V2.\n    *   Deprecate tf.QUANTIZED_DTYPES. The official new symbol is\n        tf.dtypes.QUANTIZED_DTYPES.\n    *   Update sklearn imports for deprecated packages.\n    *   Deprecate `Variable.count_up_to` and `tf.count_up_to` in favor of\n        `Dataset.range`.\n    *   Export `confusion_matrix` op as `tf.math.confusion_matrix` instead of\n        `tf.train.confusion_matrix`.\n    *   Add `tf.dtypes.` endpoint for every constant in dtypes.py. Moving\n        endpoints in versions.py to corresponding endpoints in `tf.sysconfig.`\n        and `tf.version.`. Moving all constants under `tf.saved_model`\n        submodules to `tf.saved_model` module. New endpoints are added in V1 and\n        V2 but existing endpoint removals are only applied in V2.\n    *   Deprecates behavior where device assignment overrides collocation\n        constraints inside a collocation context manager.\n*   Keras & Python API\n    *   Add to Keras functionality analogous to\n        `tf.register_tensor_conversion_function`.\n    *   Subclassed Keras models can now be saved through\n        `tf.contrib.saved_model.save_keras_model`.\n    *   `LinearOperator.matmul` now returns a new `LinearOperator`.\n*   New ops and improved op functionality\n    *   Add a Nearest Neighbor Resize op.\n    *   Add an `ignore_unknown` argument to `parse_values` which suppresses\n        ValueError for unknown hyperparameter types. Such * Add\n        `tf.linalg.matvec` convenience function.\n    *   `tf.einsum()`raises `ValueError` for unsupported equations like\n        `\"ii->\"`.\n    *   Add DCT-I and IDCT-I in `tf.signal.dct` and `tf.signal.idct`.\n    *   Add LU decomposition op.\n    *   Add quantile loss to gradient boosted trees in estimator.\n    *   Add `round_mode` to `QuantizeAndDequantizeV2` op to select rounding\n        algorithm.\n    *   Add `unicode_encode`, `unicode_decode`, `unicode_decode_with_offsets`,\n        `unicode_split`, `unicode_split_with_offset`, and `unicode_transcode`\n        ops. Amongst other things, this Op adds the ability to encode, decode,\n        and transcode a variety of input text encoding formats into the main\n        Unicode encodings (UTF-8, UTF-16-BE, UTF-32-BE)\n    *   Add \"unit\" attribute to the substr op, which allows obtaining the\n        substring of a string containing unicode characters.\n    *   Broadcasting support for Ragged Tensors.\n    *   `SpaceToDepth` supports uint8 data type.\n    *   Support multi-label quantile regression in estimator.\n    *   We now use \"div\" as the default partition_strategy in\n        `tf.nn.safe_embedding_lookup_sparse`, `tf.nn.sampled_softmax` and\n        `tf.nn.nce_loss`. hyperparameter are ignored.\n*   Performance\n    *   Improve performance of GPU cumsum/cumprod by up to 300x.\n    *   Added support for weight decay in most TPU embedding optimizers,\n        including AdamW and MomentumW.\n*   TensorFlow 2.0 Development\n    *   Add a command line tool to convert to TF2.0, tf_upgrade_v2\n    *   Merge `tf.spectral` into `tf.signal` for TensorFlow 2.0.\n    *   Change the default recurrent activation function for LSTM from\n        'hard_sigmoid' to 'sigmoid' in 2.0. Historically recurrent activation is\n        'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend\n        between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we\n        change the default for CPU mode to sigmoid as well. With that, the\n        default LSTM will be compatible with both CPU and GPU kernel. This will\n        enable user with GPU to use CuDNN kernel by default and get a 10x\n        performance boost in training. Note that this is checkpoint breaking\n        change. If user want to use their 1.x pre-trained checkpoint, please\n        construct the layer with LSTM(recurrent_activation='hard_sigmoid') to\n        fallback to 1.x behavior.\n*   TensorFlow Lite\n    *   Move from `tensorflow/contrib/lite` to `tensorflow/lite`.\n    *   Add experimental Java API for injecting TensorFlow Lite delegates\n    *   Add support for strings in TensorFlow Lite Java API.\n*   `tf.contrib`:\n    *   Add Apache Ignite Filesystem plugin to support accessing Apache IGFS.\n    *   Dropout now takes `rate` argument, `keep_prob` is deprecated.\n    *   Estimator occurrences references `tf.contrib.estimator` were changed to\n        `tf.estimator`:\n    *   `tf.contrib.estimator.BaselineEstimator` with\n        `tf.estimator.BaselineEstimator`\n    *   `tf.contrib.estimator.DNNLinearCombinedEstimator` with\n        `tf.estimator.DNNLinearCombinedEstimator`\n    *   `tf.contrib.estimator.DNNEstimator` with `tf.estimator.DNNEstimator`\n    *   `tf.contrib.estimator.LinearEstimator` with\n        `tf.estimator.LinearEstimator`\n    *   `tf.contrib.estimator.InMemoryEvaluatorHook` and\n        tf.estimator.experimental.InMemoryEvaluatorHook`.\n    *   `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with\n        `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`.\n    *   Expose `tf.distribute.Strategy as the new name for\n        tf.contrib.distribute.DistributionStrategy.\n    *   Migrate linear optimizer from contrib to core.\n    *   Move `tf.contrib.signal` to `tf.signal` (preserving aliases in\n        tf.contrib.signal).\n    *   Users of `tf.contrib.estimator.export_all_saved_models` and related\n        should switch to\n        `tf.estimator.Estimator.experimental_export_all_saved_models`.\n*   tf.data:\n    *   Add `tf.data.experimental.StatsOptions()`, to configure options to\n        collect statistics from `tf.data.Dataset` pipeline using\n        `StatsAggregator`. Add nested option, `experimental_stats` (which takes\n        a `tf.data.experimen tal.StatsOptions` object), to `tf.data.Options`.\n        Deprecates `tf.data.experimental.set_stats_agregator`.\n    *   Performance optimizations:\n    *   Add `tf.data.experimental.OptimizationOptions()`, to configure options\n        to enable `tf.data` performance optimizations. Add nested option,\n        `experimental_optimization` (which takes a\n        `tf.data.experimental.OptimizationOptions` object), to\n        `tf.data.Options`. Remove performance optimization options from\n        `tf.data.Options`, and add them under\n        `tf.data.experimental.OptimizationOptions` instead.\n    *   Enable `map_and_batch_fusion` and `noop_elimination` optimizations by\n        default. They can be disabled by configuring\n        `tf.data.experimental.OptimizationOptions` to set `map_and_batch =\n        False` or `noop_elimination = False` respectively. To disable all\n        default optimizations, set `apply_default_optimizations = False`.\n    *   Support parallel map in `map_and_filter_fusion`.\n    *   Disable static optimizations for input pipelines that use non-resource\n        `tf.Variable`s.\n    *   Add NUMA-aware MapAndBatch dataset.\n    *   Deprecate `tf.data.Dataset.make_one_shot_iterator()` in V1, removed it\n        from V2, and added tf.compat.v1.data.make_one_shot_iterator()`.\n    *   Deprecate `tf.data.Dataset.make_initializable_iterator()` in V1, removed\n        it from V2, and added `tf.compat.v1.data.make_initializable_iterator()`.\n    *   Enable nested dataset support in core `tf.data` transformations.\n    *   For `tf.data.Dataset` implementers: Added\n        `tf.data.Dataset._element_structured property` to replace\n        `Dataset.output_{types,shapes,classes}`.\n    *   Make `num_parallel_calls` of `tf.data.Dataset.interleave` and\n        `tf.data.Dataset.map` work in Eager mode.\n*   Toolchains\n    *   Fixed OpenSSL compatibility by avoiding `EVP_MD_CTX_destroy`.\n    *   Added bounds checking to printing deprecation warnings.\n    *   Upgraded CUDA dependency to 10.0\n    *   To build with Android NDK r14b, add \"#include <linux/compiler.h>\" to\n        android-ndk-r14b/platforms/android-14/arch-*/usr/include/linux/futex.h\n    *   Removed `:android_tensorflow_lib_selective_registration*` targets, use\n        `:android_tensorflow_lib_lite*` targets instead.\n*   XLA\n    *   Move `RoundToEven` function to xla/client/lib/math.h.\n    *   A new environment variable `TF_XLA_DEBUG_OPTIONS_PASSTHROUGH` set to \"1\"\n        or \"true\" allows the debug options passed within an XRTCompile op to be\n        passed directly to the XLA compilation backend. If such variable is not\n        set (service side), only a restricted set will be passed through.\n    *   Allow the XRTCompile op to return the ProgramShape resulted form the XLA\n        compilation as a second return argument.\n    *   XLA HLO graphs can now be rendered as SVG/HTML.\n*   Estimator\n    *   Replace all occurrences of `tf.contrib.estimator.BaselineEstimator` with\n        `tf.estimator.BaselineEstimator`\n    *   Replace all occurrences of\n        `tf.contrib.estimator.DNNLinearCombinedEstimator` with\n        `tf.estimator.DNNLinearCombinedEstimator`\n    *   Replace all occurrences of `tf.contrib.estimator.DNNEstimator` with\n        `tf.estimator.DNNEstimator`\n    *   Replace all occurrences of `tf.contrib.estimator.LinearEstimator` with\n        `tf.estimator.LinearEstimator`\n    *   Users of `tf.contrib.estimator.export_all_saved_models` and related\n        should switch to\n        `tf.estimator.Estimator.experimental_export_all_saved_models`.\n    *   Update `regression_head` to the new Head API for Canned Estimator V2.\n    *   Switch `multi_class_head` to Head API for Canned Estimator V2.\n    *   Replace all occurrences of `tf.contrib.estimator.InMemoryEvaluatorHook`\n        and `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with\n        `tf.estimator.experimental.InMemoryEvaluatorHook` and\n        `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`\n    *   Migrate linear optimizer from contrib to core.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAbhinav Upadhyay, Ag Ramesh, akikaaa, Alexis Louis, Anders Huss, Andreas Madsen, Andrew Banchich, Andy Craze, Anton Dmitriev, Artem Malykh, Avijit-Nervana, Balint Cristian, Benjamin Tan Wei Hao, Bhavani Subramanian, Brendan Finan, Brian Nemsick, Bryan Cutler, By Shen, Cao Zongyan, Castiel, Chris Antaki, Christian Goll, Cibifang, Clayne Robison, Codrut Grosu, Cong Xu, Dalmo Cirne, Daniel Hunter, Dougal J. Sutherland, Edvard Fagerholm, EFanZh, Erik Smistad, Evgeniy Polyakov, Feiyang Chen, franklin5, Fred Reiss, Gautam, gehring, Geoffrey Irving, George Sterpu, Gitea, Grzegorz George Pawelczak, Guozhong Zhuang, himkt, Hoeseong Kim, Huan Li (\u674e\u5353\u6853), HuiyangFei, hyunyoung, Isaac Burbank, jackonan, Jacky Ko, Jason Furmanek, Jason Zaman, Javier Luraschi, Jiang,Zhoulong, joaak, John Lin, Jonathan Wyatt Hoech, josephyearsley, Josh Gordon, Julian Niedermeier, Karl Lessard, Keno Fischer, lanhin, Leon Graser, leondgarse, Li, Guizi, Li, Yiqiang, lxl910915, Mahmoud Abuzaina, manhyuk, Marcela Morales Quispe, margaretmz, Matt Conley, Max Pumperla, mbhuiyan, mdfaijul, Meng, Peng, Michael, Michael Gielda, mrTsjolder, Muhammad Wildan, neargye, Nehal J Wani, NEWPLAN, Niranjan Hasabnis, Nutti, olicht, Pan Daoxin, Pedro Monreal, Peng Yu, pillarpond, Pooya Davoodi, qiezi, Rholais Lii, Richard Yu, Rin Arakaki, Roger Iyengar, sahilbadyal, Sami Kama, Sandip Giri, Scott Leishman, Serge Panev, Seunghoon Park, Shafi Dayatar, shengfuintel, Shimin Guo, Siju, silent567, Stefan Dyulgerov, steven, Tao Wei, Thor Johnsen, Tingbo Lu, tomguluson92, Tongxuan Liu, Trevor Morris, Ubuntu, Vadim Borisov, vanderliang, wangsiyu, Wen Yun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Xiaoming (Jason) Cui, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yaniv Blumenfeld, Yash Gaurkar, Yicheng Fan, Yong Tang, Yongjoon Lee, Yuan (Terry) Tang, Yuxin Wu, zldrobit\n\n# Release 1.12.0\n\n## Major Features and Improvements\n\n*   Keras models can now be directly exported to the SavedModel\n    format(`tf.contrib.saved_model.save_keras_model()`) and used with Tensorflow\n    Serving.\n*   Keras models now support evaluating with a `tf.data.Dataset`.\n*   TensorFlow binaries are built with XLA support linked in by default.\n*   Ignite Dataset added to contrib/ignite that allows to work with Apache\n    Ignite.\n\n## Bug Fixes and Other Changes\n\n*   tf.data:\n    *   tf.data users can now represent, get, and set options of TensorFlow\n        input pipelines using `tf.data.Options()`, `tf.data.Dataset.options()`,\n        and `tf.data.Dataset.with_options()` respectively.\n    *   New `tf.data.Dataset.reduce()` API allows users to reduce a finite\n        dataset to a single element using a user-provided reduce function.\n    *   New `tf.data.Dataset.window()` API allows users to create finite windows\n        of input dataset; when combined with the `tf.data.Dataset.reduce()` API,\n        this allows users to implement customized batching.\n    *   All C++ code moves to the `tensorflow::data` namespace.\n    *   Add support for `num_parallel_calls` to `tf.data.Dataset.interleave`.\n*   `tf.contrib`:\n    *   Remove `tf.contrib.linalg`. `tf.linalg` should be used instead.\n    *   Replace any calls to `tf.contrib.get_signature_def_by_key(metagraph_def,\n        signature_def_key)` with\n        `meta_graph_def.signature_def[signature_def_key]`. Catching a ValueError\n        exception thrown by `tf.contrib.get_signature_def_by_key` should be\n        replaced by catching a KeyError exception.\n*   `tf.contrib.data`\n    *   Deprecate, and replace by tf.data.experimental.\n*   Other:\n    *   Instead of jemalloc, revert back to using system malloc since it\n        simplifies build and has comparable performance.\n    *   Remove integer types from `tf.nn.softplus` and `tf.nn.softsign` OpDefs.\n        This is a bugfix; these ops were never meant to support integers.\n    *   Allow subslicing Tensors with a single dimension.\n    *   Add option to calculate string length in Unicode characters.\n    *   Add functionality to SubSlice a tensor.\n    *   Add searchsorted (ie lower/upper_bound) op.\n    *   Add model explainability to Boosted Trees.\n    *   Support negative positions for tf.substr.\n    *   There was previously a bug in the bijector_impl where the\n        _reduce_jacobian_det_over_event does not handle scalar ILDJ\n        implementations properly.\n    *   In tf eager execution, allow re-entering a GradientTape context.\n    *   Add tf_api_version flag. If --define=tf_api_version=2 flag is passed in,\n        then bazel will build TensorFlow API version 2.0. Note that TensorFlow\n        2.0 is under active development and has no guarantees at this point.\n    *   Add additional compression options to TfRecordWriter.\n    *   Performance improvements for regex full match operations.\n    *   Replace tf.GraphKeys.VARIABLES with `tf.GraphKeys.GLOBAL_VARIABLES`.\n    *   Remove unused dynamic learning rate support.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n(David) Siu-Kei Muk, Ag Ramesh, Anton Dmitriev, Artem Sobolev, Avijit-Nervana,\nBairen Yi, Bruno Goncalves, By Shen, candy.dc, Cheng Chen, Clayne Robison,\ncoder3101, Dao Zhang, Elms, Fei Hu, feiquan, Geoffrey Irving, Guozhong Zhuang,\nhellcom, Hoeseong Kim, imsheridan, Jason Furmanek, Jason Zaman, Jenny Sahng,\njiefangxuanyan, Johannes Bannhofer, Jonathan Homer, Koan-Sin Tan, kouml, Loo\nRong Jie, Lukas Geiger, manipopopo, Ming Li, Moritz Kr\u00f6Ger, Naurril, Niranjan\nHasabnis, Pan Daoxin, Peng Yu, pengwa, rasmi, Roger Xin, Roland Fernandez, Sami\nKama, Samuel Matzek, Sangjung Woo, Sergei Lebedev, Sergii Khomenko, shaohua,\nShaohua Zhang, Shujian2015, Sunitha Kambhampati, tomguluson92, Vin\u00edCius Camargo,\nwangsiyu, weidankong, Wen-Heng (Jack) Chung, William D. Irons, Xin Jin, Yan\nFacai (\u989c\u53d1\u624d), Yanbo Liang, Yash Katariya, Yong Tang, \u5728\u539f\u4f50\u4e3a\n\n# Release 1.11.0\n\n## Major Features and Improvements\n\n*   Nvidia GPU:\n    *   Prebuilt binaries are now (as of TensorFlow 1.11) built against cuDNN\n        7.2 and TensorRT 4. See updated install guides:\n        [Installing TensorFlow on Ubuntu](https://www.tensorflow.org/install/install_linux#tensorflow_gpu_support)\n*   Google Cloud TPU:\n    *   Experimental tf.data integration for Keras on Google Cloud TPUs.\n    *   Experimental / preview support for eager execution on Google Cloud TPUs.\n*   DistributionStrategy:\n    *   Add multi-GPU DistributionStrategy support in tf.keras. Users can now\n        use `fit`, `evaluate` and `predict` to distribute their model on\n        multiple GPUs.\n    *   Add multi-worker DistributionStrategy and standalone client support in\n        Estimator. See\n        [README](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute)\n        for more details.\n*   Add C, C++, and Python functions for querying kernels.\n\n## Breaking Changes\n\n* Keras:\n  * The default values for tf.keras `RandomUniform`, `RandomNormal`, and `TruncatedNormal` initializers have been changed to match those in external Keras.\n  * Breaking change: `model.get_config()` on a Sequential model now returns a config dictionary (consistent with other Model instances) instead of a list of configs for the underlying layers.\n\n## Bug Fixes and Other Changes\n\n*   C++:\n    *   Changed the signature of SessionFactory::NewSession so that it can\n        return a meaningful error message on failure.\n*   tf.data:\n    *   Remove `num_parallel_parser_calls` argument from\n        `tf.contrib.data.make_csv_dataset()`. [tf.data] Remove\n        `num_parallel_parser_calls` argument from\n        `tf.contrib.data.make_csv_dataset()`.\n    *   `tf.data.Dataset.list_files()` raises an exception at initialization\n        time if the argument matches no files.\n    *   Renamed BigTable class to BigtableTable for clarity\n    *   Document use of the Cloud Bigtable API\n    *   Add `tf.contrib.data.reduce_dataset` which can be used to reduce a\n        dataset to a single element.\n    *   Generalization of `tf.contrib.data.sliding_window_batch`.\n*   INC:\n    *   Runtime improvements to triangular solve.\n*   `tf.contrib`:\n    *   Add an `implementation` argument to `tf.keras.layers.LocallyConnected2D`\n        and `tf.keras.layers.LocallyConnected1D`. The new mode\n        (`implementation=2`) performs forward pass as a single dense matrix\n        multiplication, allowing dramatic speedups in certain scenarios (but\n        worse performance in others - see docstring). The option also allows to\n        use `padding=same`.\n    *   Add documentation clarifying the differences between tf.fill and\n        tf.constant.\n    *   Add experimental IndexedDatasets.\n    *   Add selective registration target using the lite proto runtime.\n    *   Add simple Tensor and DataType classes to TensorFlow Lite Java\n    *   Add support for bitcasting to/from uint32 and uint64.\n    *   Added a subclass of Estimator that can be created from a SavedModel\n        (SavedModelEstimator).\n    *   Adds leaf index modes as an argument.\n    *   Allow a different output shape from the input in\n        tf.contrib.image.transform.\n    *   Change the state_size order of the StackedRNNCell to be natural order.\n        To keep the existing behavior, user can add reverse_state_order=True\n        when constructing the StackedRNNCells.\n    *   Deprecate self.test_session() in favor of self.session() or\n        self.cached_session().\n    *   Directly import tensor.proto.h (the transitive import will be removed\n        from tensor.h soon).\n    *   Estimator.train() now supports tf.contrib.summary.\\* summaries out of\n        the box; each call to .train() will now create a separate tfevents file\n        rather than re-using a shared one.\n    *   Fix FTRL L2-shrinkage behavior: the gradient from the L2 shrinkage term\n        should not end up in the accumulator.\n    *   Fix toco compilation/execution on Windows.\n    *   GoogleZoneProvider class added to detect which Google Cloud Engine zone\n        tensorflow is running in.\n    *   It is now safe to call any of the C API's TF_Delete\\* functions on\n        nullptr.\n    *   Log some errors on Android to logcat.\n    *   Match FakeQuant numerics in TFLite to improve accuracy of TFLite\n        quantized inference models.\n    *   Optional bucket location check for the GCS Filesystem.\n    *   Performance enhancements for StringSplitOp & StringSplitV2Op.\n    *   Performance improvements for regex replace operations.\n    *   TFRecordWriter now raises an error if .write() fails.\n    *   TPU: More helpful error messages in TPUClusterResolvers.\n    *   The legacy_init_op argument to SavedModelBuilder methods for adding\n        MetaGraphs has been deprecated. Please use the equivalent main_op\n        argument instead. As part of this, we now explicitly check for a single\n        main_op or legacy_init_op at the time of SavedModel building, whereas\n        the check on main_op was previously only done at load time.\n    *   The protocol used for Estimator training is now configurable in\n        RunConfig.\n    *   Triangular solve performance improvements.\n    *   Unify RNN cell interface between TF and Keras. Add new\n        get_initial_state() to Keras and TF RNN cell, which will use to replace\n        the existing zero_state() method.\n    *   Update initialization of variables in Keras.\n    *   Updates to \"constrained_optimization\" in tensorflow/contrib.\n    *   boosted trees: adding pruning mode.\n    *   tf.train.Checkpoint does not delete old checkpoints by default.\n    *   tfdbg: Limit the total disk space occupied by dumped tensor data to 100\n        GBytes. Add environment variable `TFDBG_DISK_BYTES_LIMIT` to allow\n        adjustment of this upper limit.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAapeli, adoda, Ag Ramesh, Amogh Mannekote, Andrew Gibiansky, Andy Craze, Anirudh Koul, Aurelien Geron, Avijit, Avijit-Nervana, Ben, Benjamin H. Myara, bhack, Brett Koonce, Cao Zongyan, cbockman, cheerss, Chikanaga Tomoyuki, Clayne Robison, cosine0, Cui Wei, Dan J, David, David Norman, Dmitry Klimenkov, Eliel Hojman, Florian Courtial, fo40225, formath, Geoffrey Irving, gracehoney, Grzegorz Pawelczak, Guoliang Hua, Guozhong Zhuang, Herman Zvonimir Do\u0161Ilovi\u0107, HuiyangFei, Jacker, Jan H\u00fcNnemeyer, Jason Taylor, Jason Zaman, Jesse, Jiang,Zhoulong, Jiawei Zhang, Jie, Joe Yearsley, Johannes Schmitz, Jon Perl, Jon Triebenbach, Jonathan, Jonathan Hseu, Jongmin Park, Justin Shenk, karl@kubx.ca, Kate Hodesdon, Kb Sriram, Keishi Hattori, Kenneth Blomqvist, Koan-Sin Tan, Li Liangbin, Li, Yiqiang, Loo Rong Jie, Madiyar, Mahmoud Abuzaina, Mark Ryan, Matt Dodge, mbhuiyan, melvinljy96, Miguel Mota, Nafis Sadat, Nathan Luehr, naurril, Nehal J Wani, Niall Moran, Niranjan Hasabnis, Nishidha Panpaliya, npow, olicht, Pei Zhang, Peng Wang (Simpeng), Peng Yu, Philipp Jund, Pradeep Banavara, Pratik Kalshetti, qwertWZ, Rakesh Chada, Randy West, Ray Kim, Rholais Lii, Robin Richtsfeld, Rodrigo Silveira, Ruizhi, Santosh Kumar, Seb Bro, Sergei Lebedev, sfujiwara, Shaba Abhiram, Shashi, SneakyFish5, Soila Kavulya, Stefan Dyulgerov, Steven Winston, Sunitha Kambhampati, Surry Shome, Taehoon Lee, Thor Johnsen, Tristan Rice, TShapinsky, tucan, tucan9389, Vicente Reyes, Vilmar-Hillow, Vitaly Lavrukhin, wangershi, weidan.kong, weidankong, Wen-Heng (Jack) Chung, William D. Irons, Wim Glenn, XFeiF, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yong Tang, Yoshihiro Yamazaki, Yuan (Terry) Tang, Yuan, Man, zhaoyongke, \u00c1Ron\nRicardo Perez-Lopez, \u5f20\u5929\u542f, \u5f20\u6653\u98de\n\n\n# Release 1.10.1\n## Bug Fixes and Other Changes\n\n* `tf.keras`:\n  * Fixing keras on Cloud TPUs. No new binaries will be built for Windows.\n\n\n# Release 1.10.0\n\n## Major Features And Improvements\n\n* The `tf.lite` runtime now supports `complex64`.\n* Initial [Google Cloud Bigtable integration](https://github.com/tensorflow/tensorflow/tree/r1.10/tensorflow/contrib/bigtable) for `tf.data`.\n* Improved local run behavior in `tf.estimator.train_and_evaluate` which does not reload checkpoints for evaluation.\n* `RunConfig` now sets device_filters to restrict how workers and PS can communicate. This can speed up training and ensure clean shutdowns in some situations. But if you have jobs that require communication between workers, you will have to set custom session_options in your `RunConfig`.\n* Moved Distributions and Bijectors from `tf.contrib.distributions` to [Tensorflow Probability (TFP)](https://github.com/tensorflow/probability). `tf.contrib.distributions` is now deprecated and will be removed by the end of 2018.\n* Adding new endpoints for existing tensorflow symbols. These endpoints are going to be the preferred endpoints going forward and may replace some of the existing endpoints in the future. See below for the complete list. New symbols have been added to the following modules: [`tf.debugging`](https://www.tensorflow.org/versions/master/api_docs/python/tf/debugging), [`tf.dtypes`](https://www.tensorflow.org/versions/master/api_docs/python/tf/dtypes), [`tf.image`](https://www.tensorflow.org/versions/master/api_docs/python/tf/image), [`tf.io`](https://www.tensorflow.org/versions/master/api_docs/python/tf/io), [`tf.linalg`](https://www.tensorflow.org/versions/master/api_docs/python/tf/linalg), [`tf.manip`](https://www.tensorflow.org/versions/master/api_docs/python/tf/manip), [`tf.math`](https://www.tensorflow.org/versions/master/api_docs/python/tf/math), [`tf.quantization`](https://www.tensorflow.org/versions/master/api_docs/python/tf/quantization), [`tf.strings`](https://www.tensorflow.org/versions/master/api_docs/python/tf/strings)\n\n## Breaking Changes\n\n* Prebuilt binaries are now (as of TensorFlow 1.10) built against NCCL 2.2 and no longer include NCCL in the binary install. TensorFlow usage with multiple GPUs and NCCL requires upgrade to [NCCL 2.2](https://developer.nvidia.com/nccl). See updated install guides: [TensorFlow GPU support](https://www.tensorflow.org/install/gpu) and [Build TensorFlow from source](https://www.tensorflow.org/install/source).\n* Starting from TensorFlow 1.11, Windows builds will use Bazel. Therefore, we will drop official support for cmake.\n\n## Bug Fixes and Other Changes\n\n* `tf.data`:\n  * `tf.contrib.data.group_by_reducer()` is now available via the public API.\n  * `tf.contrib.data.choose_from_datasets()` is now available via the public API.\n  * Adding `drop_remainder` argument to `tf.data.Dataset.batch()` and `tf.data.Dataset.padded_batch()`, deprecating `tf.contrib.data.batch_and_drop_remainder()` and `tf.contrib.data.padded_batch_and_drop_remainder()`.\n* `tf.estimator`:\n  * `Estimator`s now use custom savers included in `EstimatorSpec` scaffolds for saving SavedModels during export.\n  * `EstimatorSpec` will now add a default prediction output for export if no `export_output` is provided, eliminating the need to explicitly include a `PredictOutput` object in the `model_fn` for simple use-cases.\n  * Support sparse_combiner in canned Linear Estimators.\n  * Added batch normalization to `DNNClassifier`, `DNNRegressor`, and `DNNEstimator`.\n  * Adding ranking support for boosted trees.\n  * Adding center bias option for boosted trees.\n* Add `synchronization` and `aggregation` args to get_variable(). These args will be used for distributed variables.\n* Add `synchronization` and `aggregation` args to the layer `add_weight()` API. These args will be used for distributed variables.\n* `tf.losses.*` do not add to the global collection when executing eagerly (to avoid leaking memory).\n* Support different summary and checkpoint directories in `tf.train.MonitoredTrainingSession()`.\n* Added IndRNN, IndyGRU, and IndyLSTM cells to `tf.contrib.rnn`.\n* Add safe static factory functions for SparseTensor and convert all CHECKs to DCHECKs. Using the constructor directly is unsafe and deprecated.\n* Make the Bigtable client connection pool configurable & increase the default # of connections for performance.\n* Added derivative of `tf.random_gamma` with respect to the alpha parameter.\n* Added derivative of `tf.igamma(a, x)` and `tf.igammac(a, x)` with respect to a.\n* Modified Bessel functions of order zero and one.\n* Add FillTriangular Bijector to create triangular matrices.\n* Added support for Type III DCT, and `tf.spectral.idct(type=2|3)`.\n* Correctly handle CuDNN RNN weight loaded when nest in `TimeDistributed`.\n* Adding per-element weight support for `WALSComputePartialLhsAndRhsOp`.\n* ZerosLike and OnesLike ops treated as constants by Graph Transform Tool.\n* Gamma distribution and the derived distributions (Beta, Dirichlet, Student's t, inverse Gamma) now fully reparameterized.\n* Java: Experimental wrapper classes to make graph generation easier. Thanks @karllessard and @kbsriram\n* Build & link in secure gRPC components (switch from the insecure grpc dependency to secure grpc dependency).\n* Adding new endpoints for existing tensorflow symbols. These endpoints are going to be the preferred endpoints going forward and may replace some of the existing endpoints in the future. List of new endpoints:\n  * New endpoints in `tf.image` namespace: `tf.image.extract_image_patches`\n  * New endpoints in `tf.debugging` namespace: `tf.debugging.check_numerics`, `tf.debugging.is_finite`, `tf.debugging.is_inf`, `tf.debugging.is_nan`.\n  * New endpoints in `tf.dtypes` namespace: `tf.dtypes.as_string`.\n  * New endpoints in `tf.io` namespace: `tf.io.decode_base64`, `tf.io.decode_compressed`, `tf.io.decode_json_example`, `tf.io.decode_raw`, `tf.io.encode_base64`, `tf.io.matching_files`, `tf.io.parse_tensor`, `tf.io.read_file, `tf.io.write_file`.\n  * New endpoints in tf.linalg namespace: `tf.linalg.cross`, `tf.linalg.tensor_diag` (corresponds to `tf.diag`), `tf.linalg.tensor_diag_part` (corresponds to `tf.diag_part`).\n  * New endpoints in tf.manip namespace: `tf.manip.batch_to_space_nd`, `tf.manip.gather_nd`, `tf.manip.reshape`, `tf.manip.reverse`, `tf.manip.scatter_nd`, `tf.manip.space_to_batch_nd`, `tf.manip.tile`\n  * New endpoints in tf.math namespace: `tf.math.acos`, `tf.math.acosh`, `tf.math.add`, `tf.math.asin`, `tf.math.asinh`, `tf.math.atan`, `tf.math.atan2`, `tf.math.atanh`, `tf.math.betainc`, `tf.math.ceil`, `tf.math.cos`, `tf.math.cosh`, `tf.math.digamma`, `tf.math.equal`, `tf.math.erfc`, `tf.math.exp`, `tf.math.expm1`, `tf.math.floor`, `tf.math.greater`, `tf.math.greater_equal`, `tf.math.igamma`, `tf.math.igammac`, `tf.math.invert_permutation`, `tf.math.less`, `tf.math.less_equal`, `tf.math.lgamma`, `tf.math.log`, `tf.math.log1p`, `tf.math.logical_and`, `tf.math.logical_not`, `tf.math.logical_or`, `tf.math.maximum`, `tf.math.minimum`, `tf.math.not_equal`, `tf.math.polygamma`, `tf.math.reciprocal`, `tf.math.rint`, `tf.math.rsqrt`, `tf.math.segment_max`, `tf.math.segment_mean`, `tf.math.segment_min`, `tf.math.segment_prod`, `tf.math.segment_sum`, `tf.math.sin`, `tf.math.sinh`, `tf.math.softplus`, `tf.math.softsign`, `tf.math.squared_difference`, `tf.math.tan`, `tf.math.unsorted_segment_max`, `tf.math.unsorted_segment_min`, `tf.math.unsorted_segment_prod`, `tf.math.unsorted_segment_sum`, `tf.math.zeta`.\n  * New endpoints in `tf.quantization` namespace: `tf.quantization.dequantize`, `tf.quantization.fake_quant_with_min_max_args`, `tf.quantization.fake_quant_with_min_max_args_gradient`, `tf.quantization.fake_quant_with_min_max_vars`,  `tf.quantization.fake_quant_with_min_max_vars_gradient`, `tf.quantization.fake_quant_with_min_max_vars_per_channel`,  `tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient`.\n  * New endpoints in tf.strings namespace: `tf.strings.join` (corresponds to `tf.string_join`), `tf.strings.regex_replace`, `tf.strings.to_number` (corresponds to `tf.string_to_number`), `tf.strings.strip` (corresponds to `tf.string_strip`), `tf.strings.substr`, `tf.strings.to_hash_bucket` (corresponds to `tf.string_to_hash_bucket`), `tf.strings.to_hash_bucket_fast` (corresponds to `tf.string_to_hash_bucket_fast`), `tf.strings.to_hash_bucket_strong` (corresponds to `tf.string_to_hash_bucket_strong`).\n\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAg Ramesh, Alex Wiltschko, Alexander Pantyukhin, Amogh Mannekote, An Jiaoyang, Andrei Nigmatulin, Andrew Ginns, Bj\u00f8Rn Moholt, Brett Koonce, Chengzhi Chen, Chinmay Das, Christian Ertler, Christoph Boeddeker, Clayne Robison, Courtial Florian, ctiijima, Dan Douthit, Dan J, Dan Ringwalt, EFanZh, Emanuele Ballarin, eqy, Evgeniy Zheltonozhskiy, Freedom\" Koan-Sin Tan, Fr\u00e9D\u00e9Ric Branchaud-Charron, G K, gracehoney, Guillaume Klein, Guozhong Zhuang, Hsien-Yang Li, hsm207, ImSheridan, Jayaram Bobba, Jiandong Ruan, Jie, Joel Shor, Jonas Rauber, Jongmin Baek, jsawruk, Karan Kaw, Karl Lessard, karl@kubx.ca, Kb Sriram, KinmanLam, leiiwang, Li, Yiqiang, Loo Rong Jie, Mahmoud Abuzaina, Mahmoud Aslan, ManHyuk, Martin Patz, Martin Zeitler, mktozk, Mohammad Ashraf Bhuiyan, mrTsjolder, Naman Bhalla, Nick Felt, Nicolas Lopez, Niranjan Hasabnis, Nishidha Panpaliya, Nitish, nrstott, Nutti, Parag Jain, PeterLee, Philipp Jund, Rach L, Rafal Wojdyla, Roland Zimmermann, Sergei Lebedev, SneakyFish5, Soila Kavulya, Sriram Veturi, Steven Schmatz, Taehoon Lee, Tang, Wenyi, Taras Sereda, Ted Chang, Tim Zaman, Tristan Rice, tucan, vchigrin, Vikram Tiwari, Vincent, WeberXie, William D. Irons, Yan Facai (\u989c\u53d1\u624d), Yong Tang, Yu Yi, Yuxin Wu, Z\u00e9 Vin\u00edCius\n\n# Release 1.9.0\n\n## Major Features And Improvements\n* Updated docs for `tf.keras`: New Keras-based [get started](http://tensorflow.org/versions/r1.9/get_started),\n  and [programmers guide page](http://tensorflow.org/versions/r1.9/programmers_guide/keras).\n* Update `tf.keras` to the Keras 2.1.6 API.\n* Added [`tf.keras.layers.CuDNNGRU`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/keras/layers/CuDNNGRU) and [`tf.keras.layers.CuDNNLSTM`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/keras/layers/CuDNNLSTM) layers. [Try it](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb?linkId=53292082).\n* Adding support of core [feature columns](https://www.tensorflow.org/get_started/feature_columns) and [losses](https://www.tensorflow.org/api_docs/python/tf/losses) to [gradient boosted trees estimators](https://github.com/tensorflow/models/tree/master/official/r1/boosted_trees).\n* The [python interface](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/lite)\n  for the [TFLite Optimizing Converter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/README.md)\n  has been expanded, and the command line interface (AKA: `toco`, `tflite_convert`) is once again\n  included in the standard `pip` installation.\n* Improved data-loading and text processing with:\n    * [`tf.decode_compressed`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/decode_compressed)\n    * [`tf.string_strip`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/string_strip)\n    * [`tf.strings.regex_full_match`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/strings/regex_full_match)\n* Added experimental support for new pre-made Estimators:\n  * [`tf.contrib.estimator.BaselineEstimator`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/estimator/BaselineEstimator)\n  * [`tf.contrib.estimator.RNNClassifier`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/estimator/RNNEstimator)\n  * [`tf.contrib.estimator.RNNEstimator`](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/estimator/RNNClassifier)\n* The [distributions.Bijector](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/distributions/bijectors/Bijector)\n  API supports broadcasting for Bijectors with new API changes.\n\n## Breaking Changes\n  * If you're opening empty variable scopes; replace `variable_scope('', ...)` by\n    `variable_scope(tf.get_variable_scope(), ...)`.\n  * Headers used for building custom ops have been moved from site-packages/external into site-packages/tensorflow/include/external.\n\n## Bug Fixes and Other Changes\n\n*   `tfe.Network` is deprecated. Please inherit from `tf.keras.Model`.\n*   Layered variable names have changed in the following conditions:\n    *   Using `tf.keras.layers` with custom variable scopes.\n    *   Using `tf.layers` in a subclassed `tf.keras.Model` class. See\n        [here](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/layers)\n        for more details\n*   `tf.data`:\n    *   `Dataset.from_generator()` now accepts an `args` list, in order to\n        create nested generators.\n    *   `Dataset.list_files()` now produces deterministic results when\n        `shuffle=False` or a `seed` is passed.\n    *   `tf.contrib.data.sample_from_datasets()` and\n        `tf.contrib.data.choose_from_datasets()` make it easier to sample or\n        deterministically choose elements from multiple datasets.\n    *   `tf.contrib.data.make_csv_dataset()` now supports line breaks in quoted\n        strings, and two infrequently used arguments removed.\n    *   (C++) `DatasetBase::DebugString()` is now `const`.\n    *   (C++) `DatasetBase::MakeIterator()` has been renamed to\n        `DatasetBase::MakeIteratorInternal()`.\n    *   (C++) `IteratorBase::Initialize()` method was added to support raising\n        errors during iterator construction.\n*   Eager Execution:\n    *   Added the ability to pause recording operations for gradient computation\n        via `tf.GradientTape.stop_recording`.\n    *   Updated documentation, introductory notebooks.\n*   `tf.keras`:\n    *   Move Keras code out of _impl folder and remove API files.\n    *   `tf.keras.Model.save_weights` now saves in TensorFlow format by default.\n    *   Enable dataset iterators to be passed to `tf.keras.Model` training/eval\n        methods.\n*   TensorFlow Debugger (tfdbg) CLI: fix an issue in which the TensorBoard\n    Debugger Plugin could not handle total source file size exceeding gRPC\n    message size limit (4 MB).\n*   `tf.contrib`:\n    *   `tf.contrib.framework.zero_initializer` supports ResourceVariable.\n    *   Adding \"constrained_optimization\" to tensorflow/contrib.\n*   Other:\n    *   Add GCS Configuration Ops.\n    *   Changing signature of `MakeIterator` to enable propagating error status.\n    *   KL divergence for two Dirichlet distributions.\n    *   More consistent GcsFileSystem behavior for certain reads past EOF.\n    *   Update benchmark for tf.scan to match ranges across eager and graph\n        modes.\n    *   Fixed bug in `tf.reduce_prod gradient` for complex dtypes.\n    *   Allow the use of '.' in variables (e.g. \"hparams.parse('a.b=1.0')\"),\n        which would previously raise an error. This will correspond to an\n        attribute name with an embedded '.' symbol (e.g. 'a.b'), which can only\n        be accessed indirectly (e.g. through getattr and setattr). To set this\n        up the user will first need to explicitly add the variable to the hparam\n        object (e.g. \"hparams.add_hparam(name='a.b', value=0.0)\").\n    *   Benchmark for tf.scan in graph and eager modes.\n    *   Added complex128 support to FFT, FFT2D, FFT3D, IFFT, IFFT2D, and IFFT3D.\n    *   Making ids unique in `nn.embedding_lookup_sparse`. This helps to reduce\n        RPC calls for looking up the embeddings when there are repeated ids in\n        the batch.\n    *   Support indicator column in boosted trees.\n    *   Prevent `tf.gradients()` from backpropagating through integer tensors.\n    *   LinearOperator[1D,2D,3D]Circulant added to `tensorflow.linalg`.\n    *   Conv3D, Conv3DBackpropInput, Conv3DBackpropFilter now supports\n        arbitrary.\n    *   Added `tf.train.Checkpoint` for reading/writing object-based\n        checkpoints.\n    *   Added LinearOperatorKronecker, a dense-free implementation of the\n        Kronecker Product.\n    *   Allow LinearOperator to broadcast.\n    *   SavedModelBuilder will now deduplicate asset names that point to files\n        with the same basename and the same contents. Note that this may result\n        in new asset files included in SavedModels in cases where assets with\n        the same name but different contents were previously overwriting each\n        other.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAbdullah Alrasheed, Achal Shah, Ad-530, ADiegoCAlonso, Aditya Yogi, Ag Ramesh, akindyakov, Andy Kernahan, Anya Petrova, Aurelien Geron, Ben, Ben Barsdell, Bhavani-Subramanian, braincodercn, Brett Koonce, Brian Nemsick, Brian Zier, Bryan Heden, candy.dc, cclauss, Clayne Robison, ctiijima, Dalmo Cirne, David Norman, David T.H. Kao, DosLin, ekelsen, Elson Rodriguez, Erik Smistad, Felix Abecassis, Fergal Cotter, fo40225, foo0x29a, Freedom\" Koan-Sin Tan, Fr\u00e9D\u00e9Ric Branchaud-Charron, gdh1995, Geoffrey Irving, Giuseppe, gracehoney, Guido Zuidhof, Guillaume Klein, Guozhong Zhuang, Haggai, Harald Husum, imsheridan, Ivan Zhang, Jan Zikes, Jayaram Bobba, Jesse Benson, Jesse Gumz, Jiajia Li, Jie, jinghuangintel, Jingwen, jjsjann123, Joe Yearsley, Joel Hestness, Joel Shor, josephyearsley, Junpeng Lao, Karol M. Langner, Kb Sriram, krantideep95, Krish Ravindranath, Letian Feng, Loo Rong Jie, Lukas Geiger, Maciej, Mahmoud Abuzaina, ManHyuk, Mark Ryan, mbhuiyan, Michal Turek, Mostafa Alaa, Myungsung Kwak, Nand Dalal, Nehal J Wani, Neil Tenenholtz, ngc92, Nicholas Nadeau, P.Eng., Avs, Niranjan Hasabnis, P-Hidringer, Paul Van Eck, Peng Yu, Qing Zhao, Qingying Chen, Quanlong, Rajendra Arora, Rholais Lii, rmanyari, Robin Richtsfeld, Russell Klopfer, Sagi, Sam Sendelbach, Sandeep N Gupta, Sandip Giri, Sarah Edkins, Scott Tseng, Sdalbsoo, Sergii Khomenko, Seungwoo Choi (Biggie), Seyed Majid Azimi, Shaoning Zeng, shengfuintel, Siu Kei, Muk, Smit Shilu, soonson, Stefan Schweter, Sukhwan Kim, Sunitha Kambhampati, Taehoon Lee, tamimaddari82, Tang, Wenyi, Ted Chang, u2takey, Utkarsh Upadhyay, Vadim Markovtsev, voegtlel, Wai Hon Law, wangsiyu, Wenhao Hu, wenhao.hu, William D. Irons, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yihong Wang, Yilei (Dolee) Yang, Yong Tang, Yuan (Terry) Tang\n\n# Release 1.8.0\n\n## Major Features And Improvements\n* Can now pass `tf.contrib.distribute.MirroredStrategy()` to `tf.estimator.RunConfig()` to run an Estimator model on multiple GPUs on one machine.\n* Add `tf.contrib.data.prefetch_to_device()`, which supports prefetching to GPU memory.\n* Added Gradient Boosted Trees as pre-made Estimators: BoostedTreesClassifier, BoostedTreesRegressor.\n* Add 3rd generation pipeline config for Cloud TPUs which improves performance and usability.\n* `tf.contrib.bayesflow` is moving out to it's own repo.\n* Added `tf.contrib.{proto,rpc}` to allow generic proto parsing and RPC communication<sup>[1](#rpc-issue)</sup>.\n\n## Bug Fixes and Other Changes\n* `tf.data`:\n  * Add `tf.contrib.data.prefetch_to_device`, which enables prefetching dataset elements to GPU memory.\n  * Add `tf.contrib.data.AUTOTUNE`, which allows the tf.data runtime to automatically tune the prefetch buffer sizes based on your system and environment.\n  * Add `tf.contrib.data.make_csv_dataset` for building datasets of CSV files.\n* Eager Execution:\n  * With eager execution Datasets can now be used as standard python iterators (`for batch in dataset:`). Both `Dataset.__iter__()` and `Dataset.make_one_shot_iterator()` can now be used to create iterators when eager execution is enabled.\n  * Automatic device placement has been enabled (i.e., use a GPU if available automatically, without requiring an explicit `with tf.device(\u201c/gpu:0\u201d)`) (Fixes #14133)\n  * `tf.GradientTape` has moved out of contrib.\n* `tf.keras`:\n  * Added the fashion mnist dataset.\n  * New data preprocessing functions: `image/random_brightness`, `sequence/TimeseriesGenerator`, and `text/hashing_trick`.\n* Accelerated Linear Algebra (XLA):\n  * Select and scatter in reference util and evaluator now use lexicographical order to break ties.\n* TensorFlow Debugger (tfdbg) CLI:\n  * During tensor-filter operations, allow exclusion of nodes by regular expressions.\n  * Fix spurious background colors in some text terminals.\n* `tf.contrib`:\n  * Add meta-distribution BatchReshape which reshapes batch dimensions.\n  * `tf.contrib.layers.recompute_grad` works for explicit gradient checkpointing on TPU.\n  * Add `tf.contrib.framework.argsort`.\n  * Allow `DNNBoostedTreeCombinedEstimator` to work with core versions of feature columns and losses.\n  * Add non-linear image warping ops: `tf.contrib.image.sparse_image_warp`, `tf.contrib.image.dense_image_warp`, and `tf.contrib.image.interpolate_spline`.\n  * Fix bug in `tf.contrib.opt.MultitaskOptimizerWrapper` where types of tensors were mismatched.\n* Other:\n  * Low-level graph construction now calls the TensorFlow C API. This change should be invisible to most users, but can be disabled by setting the environment variable `TF_C_API_GRAPH_CONSTRUCTION=0` in this release. Future releases will remove the ability to disable this change. Please [file a bug](https://github.com/tensorflow/tensorflow/issues/new) if you find yourself using this escape hatch.\n  * Add description of shapes and a pointer to tutorial notebook in `tf.distributions.Distribution`.\n  * Update scatter operations:\n    * Add `tf.scatter_min` and `tf.scatter_max`\n    * Extend scatter operations to work with a scalar update parameter.\n  * Move cuDNN RNN ops to core for use in TensorFlow codebase only.\n  * Add `float64` support for `Conv2d`, `Conv2dBackpropInput`, and `Conv2dBackpropFilter`.\n  * Add `float64` support for `AvgPool`/`AvgPoolGrad`.\n  * Make graph name scope thread local so that they work correctly in multi-threaded environments.\n  * Update nsync synchronization library to avoid slow primitives on Linux.\n  * Removed need to put nsync/public on C include path when building custom ops.\n  * Add `tf.image.psnr`, `tf.image.ssim`, `tf.image.ssim_multiscale`, `tf.image.image_gradients`, `tf.image.sobel_edges`.\n  * Add links to https://js.tensorflow.org.\n  * Fix non-uniformity of orthogonal matrices.\n  * Fix bug where multi-image Estimator eval summaries were not displayed correctly.\n\n<a name=\"rpc-issue\"><sup>1</sup></a> The cancellation logic of the RPC op contains a concurrency error. A fix has been submitted to master and will be part of the next release.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4d55397500, Aghasy, Alan Du, Alan Lee, Alan Yee, Alex Wiltschko, Animesh Karnewar, Ankit Gupta, Anton Matosov, Aris L, Ben Barsdell, Brent Yi, Brett Koonce, Carl Thom\u00e9, cbockman, Chikanaga Tomoyuki, Chris Tava, C\u00e9Dric Deltheil, Dahan Gong, Dalmo Cirne, Daniel Erenrich, David Norman, DavidNorman, Edd Wilder-James, Fanjin Zeng, Felix Abecassis, fo40225, George Sterpu, Giovanni Terlingen, Gor Baghdasaryan, Guillaume Klein, Hanchen Li, Ilya Polenov, Jakub Kolodziejczyk, Jason Sadler, Jayaram Bobba, Jerry Liu, jinghuangintel, Jiongyan Zhang (\u5f20\u70af\u884d), Joel Shor, Jong Wook Kim, Julian Eisenschlos, Karl Lessard, Krish Ravindranath, Loo Rong Jie, Lukas Geiger, Luke Iwanski, Mahmoud Abuzaina, ManHyuk, Marvin Richter, Maximilian Mitchell, Mohammad Ashraf Bhuiyan, msofka, Mustafa Kasap, Nathan Burnham, Nathan Luehr, Naveen Marri, ngc92, nio1814, Oleg Zabluda, Ou Changkun, Panos Ipeirotis, Paul Van Eck, Peter Lee, Piotr Czapla, qjivy, Rholais Lii, Rodrigo Formigone, Russell Klopfer, ryantimjohn, Sang Han, Sebasti\u00e1N Ram\u00edRez, shengfuintel, Siby Jose Plathottam, Silver Chan, Stanislaw Antol, Taehoon Lee, Tarang Chugh, Ted Chang, Thomas Bastiani, Xian Xu, Xiaoming (Jason) Cui, Yan Facai (\u989c\u53d1\u624d), yaox12, Yashal Shakti Kanungo, Yong Tang, Yuan (Terry) Tang, Yuxin Wu, Ziyue(Louis) Lu\n\n# Release 1.7.0\n\n## Major Features And Improvements\n* Eager mode is moving out of contrib, try `tf.enable_eager_execution()`.\n* Graph rewrites emulating fixed-point quantization compatible with TensorFlow Lite, supported by new `tf.contrib.quantize` package.\n* Easily customize gradient computation with `tf.custom_gradient`.\n* [TensorBoard Debugger Plugin](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/debugger/README.md), the graphical user interface (GUI) of TensorFlow Debugger (tfdbg), is now in alpha.\n* Experimental support for reading a sqlite database as a `Dataset` with new `tf.contrib.data.SqlDataset`.\n* Distributed Mutex / CriticalSection added to `tf.contrib.framework.CriticalSection`.\n* Better text processing with `tf.regex_replace`.\n* Easy, efficient sequence input with `tf.contrib.data.bucket_by_sequence_length`\n* Initial support for `tf.contrib.tensorrt` that enables native TensorRT in\n  TensorFlow.\n\n## Bug Fixes and Other Changes\n* Accelerated Linear Algebra (XLA):\n  * Add `MaxPoolGradGrad` support for XLA\n  * CSE pass from Tensorflow is now disabled in XLA.\n* `tf.data`:\n  * `tf.data.Dataset`\n    * Add support for building C++ Dataset op kernels as external libraries, using the `tf.load_op_library()` mechanism.\n    * `Dataset.list_files()` now shuffles its output by default.\n    * `Dataset.shuffle(..., seed=tf.constant(0, dtype=tf.int64))` now yields the same sequence of elements as `Dataset.shuffle(..., seed=0)`.\n  * Add `num_parallel_reads` argument to `tf.data.TFRecordDataset`.\n* `tf.contrib`:\n  * `tf.contrib.bayesflow.halton_sequence` now supports randomization.\n  * Add support for scalars in `tf.contrib.all_reduce`.\n  * Add `effective_sample_size` to `tf.contrib.bayesflow.mcmc_diagnostics`.\n  * Add `potential_scale_reduction` to `tf.contrib.bayesflow.mcmc_diagnostics`.\n  * Add `BatchNormalization`, `Kumaraswamy` bijectors.\n  * Deprecate `tf.contrib.learn`. Please check contrib/learn/README.md for instructions on how to convert existing code.\n  * `tf.contrib.data`\n    * Remove deprecated `tf.contrib.data.Dataset`, `tf.contrib.data.Iterator`, `tf.contrib.data.FixedLengthRecordDataset`, `tf.contrib.data.TextLineDataset`, and `tf.contrib.data.TFRecordDataset` classes.\n    * Added `bucket_by_sequence_length`, `sliding_window_batch`, and `make_batched_features_dataset`\n  * Remove unmaintained `tf.contrib.ndlstm`. You can find it externally at https://github.com/tmbarchive/tfndlstm.\n  * Moved most of `tf.contrib.bayesflow` to its own repo: `tfp`\n* Other:\n  * tf.py_func now reports the full stack trace if an exception occurs.\n  * Integrate `TPUClusterResolver` with GKE's integration for Cloud TPUs.\n  * Add a library for statistical testing of samplers.\n  * Add Helpers to stream data from the GCE VM to a Cloud TPU.\n  * Integrate ClusterResolvers with TPUEstimator.\n  * Unify metropolis_hastings interface with HMC kernel.\n  * Move LIBXSMM convolutions to a separate --define flag so that they are disabled by default.\n  * Fix `MomentumOptimizer` lambda.\n  * Reduce `tfp.layers` boilerplate via programmable docstrings.\n  * Add `auc_with_confidence_intervals`, a method for computing the AUC and confidence interval with linearithmic time complexity.\n  * `regression_head` now accepts customized link function, to satisfy the usage that user can define their own link function if the `array_ops.identity` does not meet the requirement.\n  * Fix `initialized_value` and `initial_value` behaviors for `ResourceVariables` created from `VariableDef` protos.\n  * Add TensorSpec to represent the specification of Tensors.\n  * Constant folding pass is now deterministic.\n  * Support `float16` `dtype` in `tf.linalg.*`.\n  * Add `tf.estimator.export.TensorServingInputReceiver` that allows `tf.estimator.Estimator.export_savedmodel` to pass raw tensors to model functions.\n\n## Deprecations\n\n* TensorFlow 1.7 may be the last time we support Cuda versions below 8.0.\n  Starting with TensorFlow 1.8 release, 8.0 will be the minimum supported\n  version.\n* TensorFlow 1.7 may be the last time we support cuDNN versions below 6.0.\n  Starting with TensorFlow 1.8 release, 6.0 will be the minimum supported\n  version.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4d55397500, Abe, Alistair Low, Andy Kernahan, Appledore, Ben, Ben Barsdell, Boris Pfahringer, Brad Wannow, Brett Koonce, Carl Thom\u00e9, cclauss, Chengzhi Chen, Chris Drake, Christopher Yeh, Clayne Robison, Codrut Grosu, Daniel Trebbien, Danny Goodman, David Goodwin, David Norman, Deron Eriksson, Donggeon Lim, Donny Viszneki, DosLin, DylanDmitri, Francisco Guerrero, Fred Reiss, gdh1995, Giuseppe, Glenn Weidner, gracehoney, Guozhong Zhuang, Haichen \"Hc\" Li, Harald Husum, harumitsu.nobuta, Henry Spivey, hsm207, Jekyll Song, Jerome, Jiongyan Zhang, jjsjann123, John Sungjin Park, Johnson145, JoshVarty, Julian Wolff, Jun Wang, June-One, Kamil Sindi, Kb Sriram, Kdavis-Mozilla, Kenji, lazypanda1, Liang-Chi Hsieh, Loo Rong Jie, Mahesh Bhosale, MandarJKulkarni, ManHyuk, Marcus Ong, Marshal Hayes, Martin Pool, matthieudelaro, mdfaijul, mholzel, Michael Zhou, Ming Li, Minmin Sun, Myungjoo Ham, MyungsungKwak, Naman Kamra, Peng Yu, Penghao Cen, Phil, Raghuraman-K, resec, Rohin Mohanadas, Sandeep N Gupta, Scott Tseng, seaotterman, Seo Sanghyeon, Sergei Lebedev, Ted Chang, terrytangyuan, Tim H, tkunic, Tod, vihanjain, Yan Facai (\u989c\u53d1\u624d), Yin Li, Yong Tang, Yukun Chen, Yusuke Yamada\n\n\n\n# Release 1.6.0\n\n## Breaking Changes\n* Prebuilt binaries are now built against CUDA 9.0 and cuDNN 7.\n* Prebuilt binaries will use AVX instructions. This may break TF on older CPUs.\n\n## Major Features And Improvements\n* New Optimizer internal API for non-slot variables. Descendants of AdamOptimizer that access _beta[12]_power will need to be updated.\n* `tf.estimator.{FinalExporter,LatestExporter}` now export stripped SavedModels. This improves forward compatibility of the SavedModel.\n* FFT support added to XLA CPU/GPU.\n\n## Bug Fixes and Other Changes\n* Documentation updates:\n  * Added a second version of Getting Started, which is aimed at ML\nnewcomers.\n  * Clarified documentation on `resize_images.align_corners` parameter.\n  * Additional documentation for TPUs.\n* Google Cloud Storage (GCS):\n  * Add client-side throttle.\n  * Add a `FlushCaches()` method to the FileSystem interface, with an implementation for GcsFileSystem.\n* Other:\n  * Add `tf.contrib.distributions.Kumaraswamy`.\n  * `RetryingFileSystem::FlushCaches()` calls the base FileSystem's `FlushCaches()`.\n  * Add `auto_correlation` to distributions.\n  * Add `tf.contrib.distributions.Autoregressive`.\n  * Add SeparableConv1D layer.\n  * Add convolutional Flipout layers.\n  * When both inputs of `tf.matmul` are bfloat16, it returns bfloat16, instead of float32.\n  * Added `tf.contrib.image.connected_components`.\n  * Add `tf.contrib.framework.CriticalSection` that allows atomic variable access.\n  * Output variance over trees predictions for classifications tasks.\n  * For `pt` and `eval` commands, allow writing tensor values to filesystem as numpy files.\n  * gRPC: Propagate truncated errors (instead of returning gRPC internal error).\n  * Augment `parallel_interleave` to support 2 kinds of prefetching.\n  * Improved XLA support for C64-related ops log, pow, atan2, tanh.\n  * Add probabilistic convolutional layers.\n\n## API Changes\n* Introducing `prepare_variance` boolean with default setting to False for backward compatibility.\n* Move `layers_dense_variational_impl.py` to `layers_dense_variational.py`.\n\n## Known Bugs\n* Using XLA:GPU with CUDA 9 and CUDA 9.1 results in garbage results and/or\n  `CUDA_ILLEGAL_ADDRESS` failures.\n\n  Google discovered in mid-December 2017 that the PTX-to-SASS compiler in CUDA 9\n  and CUDA 9.1 sometimes does not properly compute the carry bit when\n  decomposing 64-bit address calculations with large offsets (e.g. `load [x +\n  large_constant]`) into 32-bit arithmetic in SASS.\n\n  As a result, these versions of `ptxas` miscompile most XLA programs which use\n  more than 4GB of temp memory.  This results in garbage results and/or\n  `CUDA_ERROR_ILLEGAL_ADDRESS` failures.\n\n  A fix in CUDA 9.1.121 is expected in late February 2018.  We do not expect a\n  fix for CUDA 9.0.x.  Until the fix is available, the only workaround is to\n  [downgrade](https://developer.nvidia.com/cuda-toolkit-archive) to CUDA 8.0.x\n  or disable XLA:GPU.\n\n  TensorFlow will print a warning if you use XLA:GPU with a known-bad version of\n  CUDA; see e00ba24c4038e7644da417ddc639169b6ea59122.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4d55397500, Ag Ramesh, Aiden Scandella, Akimasa Kimura, Alex Rothberg, Allen Goodman,\namilioto, Andrei Costinescu, Andrei Nigmatulin, Anjum Sayed, Anthony Platanios,\nAnush Elangovan, Armando Fandango, Ashish Kumar Ram, Ashwini Shukla, Ben, Bhavani Subramanian,\nBrett Koonce, Carl Thom\u00e9, cclauss, Cesc, Changming Sun, Christoph Boeddeker, Clayne Robison,\nClemens Schulz, Clint (Woonhyuk Baek), codrut3, Cole Gerdemann, Colin Raffel, Daniel Trebbien,\nDaniel Ylitalo, Daniel Zhang, Daniyar, Darjan Salaj, Dave Maclachlan, David Norman, Dong--Jian,\ndongsamb, dssgsra, Edward H, eladweiss, elilienstein, Eric Lilienstein, error.d, Eunji Jeong, fanlu,\nFlorian Courtial, fo40225, Fred, Gregg Helt, Guozhong Zhuang, Hanchen Li, hsm207, hyunyoung2,\nImSheridan, Ishant Mrinal Haloi, Jacky Ko, Jay Young, Jean Flaherty, Jerome, JerrikEph, Jesse\nKinkead, jfaath, Jian Lin, jinghuangintel, Jiongyan Zhang, Joel Hestness, Joel Shor, Johnny Chan,\nJulian Niedermeier, Julian Wolff, JxKing, K-W-W, Karl Lessard, Kasper Marstal, Keiji Ariyama,\nKoan-Sin Tan, Loki Der Quaeler, Loo Rong Jie, Luke Schaefer, Lynn Jackson, ManHyuk, Matt Basta,\nMatt Smith, Matthew Schulkind, Michael, michaelkhan3, Miguel Piedrafita, Mikalai Drabovich,\nMike Knapp, mjwen, mktozk, Mohamed Aly, Mohammad Ashraf Bhuiyan, Myungjoo Ham, Naman Bhalla,\nNamrata-Ibm, Nathan Luehr, nathansilberman, Netzeband, Niranjan Hasabnis, Omar Aflak, Ozge\nYalcinkaya, Parth P Panchal, patrickzzy, Patryk Chrabaszcz, Paul Van Eck, Pawe\u0142 Kapica, Peng Yu,\nPhilip Yang, Pierre Blondeau, Po-Hsien Chu, powderluv, Puyu Wang, Rajendra Arora, Rasmus, Renat\nIdrisov, resec, Robin Richtsfeld, Ronald Eddy Jr, Sahil Singh, Sam Matzek, Sami Kama, sandipmgiri,\nSantiago Castro, Sayed Hadi Hashemi, Scott Tseng, Sergii Khomenko, Shahid, Shengpeng Liu, Shreyash\nSharma, Shrinidhi Kl, Simone Cirillo, simsicon, Stanislav Levental, starsblinking, Stephen Lumenta,\nSteven Hickson, Su Tang, Taehoon Lee, Takuya Wakisaka, Ted Chang, Ted Ying, Tijmen Verhulsdonck,\nTimofey Kondrashov, vade, vaibhav, Valentin Khrulkov, vchigrin, Victor Costan, Viraj Navkal,\nVivek Rane, wagonhelm, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yaroslav Bulatov, yegord, Yong Tang,\nYoni Tsafir, yordun, Yuan (Terry) Tang, Yuxin Wu, zhengdi, Zhengsheng Wei, \u7530\u4f20\u6b66\n\n# Release 1.5.0\n\n## Breaking Changes\n* Prebuilt binaries are now built against CUDA 9.0 and cuDNN 7.\n* Starting from 1.6 release, our prebuilt binaries will use AVX instructions.\n  This may break TF on older CPUs.\n\n## Major Features And Improvements\n* [Eager execution](https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/eager)\n  preview version is now available.\n* [TensorFlow Lite](https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/lite)\n  dev preview is now available.\n* CUDA 9.0 and cuDNN 7 support.\n* Accelerated Linear Algebra (XLA):\n  * Add `complex64` support to XLA compiler.\n  * `bfloat` support is now added to XLA infrastructure.\n  * Make `ClusterSpec` propagation work with XLA devices.\n  * Use a deterministic executor to generate XLA graph.\n* `tf.contrib`:\n  * `tf.contrib.distributions`:\n    * Add `tf.contrib.distributions.Autoregressive`.\n    * Make `tf.contrib.distributions` QuadratureCompound classes support batch\n    * Infer `tf.contrib.distributions.RelaxedOneHotCategorical` `dtype` from arguments.\n    * Make `tf.contrib.distributions` quadrature family parameterized by\n      `quadrature_grid_and_prob` vs `quadrature_degree`.\n    * `auto_correlation` added to `tf.contrib.distributions`\n  * Add `tf.contrib.bayesflow.layers`, a collection of probabilistic (neural) layers.\n  * Add `tf.contrib.bayesflow.halton_sequence`.\n  * Add `tf.contrib.data.make_saveable_from_iterator.`\n  * Add `tf.contrib.data.shuffle_and_repeat`.\n  * Add new custom transformation: `tf.contrib.data.scan()`.\n  * `tf.contrib.distributions.bijectors`:\n    * Add `tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow`.\n    * Add `tf.contrib.distributions.bijectors.Permute`.\n    * Add `tf.contrib.distributions.bijectors.Gumbel`.\n    * Add `tf.contrib.distributions.bijectors.Reshape`.\n    * Support shape inference (i.e., shapes containing -1) in the Reshape bijector.\n* Add `streaming_precision_recall_at_equal_thresholds,` a method for computing\n  streaming precision and recall with `O(num_thresholds + size of predictions)`\n  time and space complexity.\n* Change `RunConfig` default behavior to not set a random seed, making random\n  behavior independently random on distributed workers. We expect this to\n  generally improve training performance. Models that do rely on determinism\n  should set a random seed explicitly.\n* Replaced the implementation of `tf.flags` with `absl.flags`.\n* Add support for `CUBLAS_TENSOR_OP_MATH` in fp16 GEMM\n* Add support for CUDA on NVIDIA Tegra devices\n\n## Bug Fixes and Other Changes\n* Documentation updates:\n  * Clarified that you can only install TensorFlow on 64-bit machines.\n  * Added a short doc explaining how `Estimator`s save checkpoints.\n  * Add documentation for ops supported by the `tf2xla` bridge.\n  * Fix minor typos in the doc of `SpaceToDepth` and `DepthToSpace`.\n  * Updated documentation comments in `mfcc_mel_filterbank.h` and `mfcc.h` to\n    clarify that the input domain is squared magnitude spectra and the weighting\n    is done on linear magnitude spectra (sqrt of inputs).\n  * Change `tf.contrib.distributions` docstring examples to use `tfd` alias\n    rather than `ds`, `bs`.\n  * Fix docstring typos in `tf.distributions.bijectors.Bijector`.\n  * `tf.assert_equal` no longer raises `ValueError.` It now raises\n    `InvalidArgumentError,` as documented.\n  * Update Getting Started docs and API intro.\n* Google Cloud Storage (GCS):\n  * Add userspace DNS caching for the GCS client.\n  * Customize request timeouts for the GCS filesystem.\n  * Improve GCS filesystem caching.\n* Bug Fixes:\n  * Fix bug where partitioned integer variables got their wrong shapes. Before\n  * Fix correctness bug in CPU and GPU implementations of Adadelta.\n  * Fix a bug in `import_meta_graph`'s handling of partitioned variables when\n    importing into a scope. WARNING: This may break loading checkpoints of\n    graphs with partitioned variables saved after using `import_meta_graph` with\n    a non-empty `import_scope` argument.\n  * Fix bug in offline debugger which prevented viewing events.\n  * Added the `WorkerService.DeleteWorkerSession` method to the gRPC interface,\n    to fix a memory leak. Ensure that your master and worker servers are running\n    the same version of TensorFlow to avoid compatibility issues.\n  * Fix bug in peephole implementation of BlockLSTM cell.\n  * Fix bug by casting dtype of `log_det_jacobian` to match `log_prob` in\n    `TransformedDistribution`.\n  * Fix a bug in `import_meta_graph`'s handling of partitioned variables when\n  * Ensure `tf.distributions.Multinomial` doesn't underflow in `log_prob`.\n    Before this change, all partitions of an integer variable were initialized\n    with the shape of the unpartitioned variable; after this change they are\n    initialized correctly.\n* Other:\n  * Add necessary shape util support for bfloat16.\n  * Add a way to run ops using a step function to MonitoredSession.\n  * Add `DenseFlipout` probabilistic layer.\n  * A new flag `ignore_live_threads` is available on train. If set to `True`, it\n    will ignore threads that remain running when tearing down infrastructure\n    after successfully completing training, instead of throwing a RuntimeError.\n  * Restandardize `DenseVariational` as simpler template for other probabilistic\n    layers.\n  * `tf.data` now supports `tf.SparseTensor` components in dataset elements.\n  * It is now possible to iterate over `Tensor`s.\n  * Allow `SparseSegmentReduction` ops to have missing segment IDs.\n  * Modify custom export strategy to account for multidimensional sparse float\n    splits.\n  * `Conv2D`, `Conv2DBackpropInput`, `Conv2DBackpropFilter` now supports arbitrary\n    dilations with GPU and cuDNNv6 support.\n  * `Estimator` now supports `Dataset`: `input_fn` can return a `Dataset`\n    instead of `Tensor`s.\n  * Add `RevBlock`, a memory-efficient implementation of reversible residual layers.\n  * Reduce BFCAllocator internal fragmentation.\n  * Add `cross_entropy` and `kl_divergence` to `tf.distributions.Distribution`.\n  * Add `tf.nn.softmax_cross_entropy_with_logits_v2` which enables backprop\n    w.r.t. the labels.\n  * GPU back-end now uses `ptxas` to compile generated PTX.\n  * `BufferAssignment`'s protocol buffer dump is now deterministic.\n  * Change embedding op to use parallel version of `DynamicStitch`.\n  * Add support for sparse multidimensional feature columns.\n  * Speed up the case for sparse float columns that have only 1 value.\n  * Allow sparse float splits to support multivalent feature columns.\n  * Add `quantile` to `tf.distributions.TransformedDistribution`.\n  * Add `NCHW_VECT_C` support for `tf.depth_to_space` on GPU.\n  * Add `NCHW_VECT_C` support for `tf.space_to_depth` on GPU.\n\n## API Changes\n* Rename `SqueezeDims` attribute to `Axis` in C++ API for Squeeze op.\n* `Stream::BlockHostUntilDone` now returns Status rather than bool.\n* Minor refactor: move stats files from `stochastic` to `common` and remove\n  `stochastic`.\n\n## Known Bugs\n* Using XLA:GPU with CUDA 9 and CUDA 9.1 results in garbage results and/or\n  `CUDA_ILLEGAL_ADDRESS` failures.\n\n  Google discovered in mid-December 2017 that the PTX-to-SASS compiler in CUDA 9\n  and CUDA 9.1 sometimes does not properly compute the carry bit when\n  decomposing 64-bit address calculations with large offsets (e.g. `load [x +\n  large_constant]`) into 32-bit arithmetic in SASS.\n\n  As a result, these versions of `ptxas` miscompile most XLA programs which use\n  more than 4GB of temp memory.  This results in garbage results and/or\n  `CUDA_ERROR_ILLEGAL_ADDRESS` failures.\n\n  A fix in CUDA 9.1.121 is expected in late February 2018.  We do not expect a\n  fix for CUDA 9.0.x.  Until the fix is available, the only workaround is to\n  [downgrade](https://developer.nvidia.com/cuda-toolkit-archive) to CUDA 8.0.x\n  or disable XLA:GPU.\n\n  TensorFlow will print a warning if you use XLA:GPU with a known-bad version of\n  CUDA; see e00ba24c4038e7644da417ddc639169b6ea59122.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAdam Zahran, Ag Ramesh, Alan Lee, Alan Yee, Alex Sergeev, Alexander, Amir H. Jadidinejad,\nAmy, Anastasios Doumoulakis, Andrei Costinescu, Andrei Nigmatulin, Anthony Platanios,\nAnush Elangovan, arixlin, Armen Donigian, Art\u00ebM Sobolev, Atlas7, Ben Barsdell, Bill Prin,\nBo Wang, Brett Koonce, Cameron Thomas, Carl Thom\u00e9, Cem Eteke, cglewis, Changming Sun,\nCharles Shenton, Chi-Hung, Chris Donahue, Chris Filo Gorgolewski, Chris Hoyean Song,\nChris Tava, Christian Grail, Christoph Boeddeker, cinqS, Clayne Robison, codrut3, concerttttt,\nCQY, Dan Becker, Dan Jarvis, Daniel Zhang, David Norman, dmaclach, Dmitry Trifonov,\nDonggeon Lim, dongpilYu, Dr. Kashif Rasul, Edd Wilder-James, Eric Lv, fcharras, Felix Abecassis,\nFirefoxMetzger, formath, FredZhang, Gaojin Cao, Gary Deer, Guenther Schmuelling, Hanchen Li,\nHanmin Qin, hannesa2, hyunyoung2, Ilya Edrenkin, Jackson Kontny, Jan, Javier Luraschi,\nJay Young, Jayaram Bobba, Jeff, Jeff Carpenter, Jeremy Sharpe, Jeroen B\u00e9Dorf, Jimmy Jia,\nJinze Bai, Jiongyan Zhang, Joe Castagneri, Johan Ju, Josh Varty, Julian Niedermeier,\nJxKing, Karl Lessard, Kb Sriram, Keven Wang, Koan-Sin Tan, Kyle Mills, lanhin, LevineHuang,\nLoki Der Quaeler, Loo Rong Jie, Luke Iwanski, L\u00e1Szl\u00f3 Csomor, Mahdi Abavisani, Mahmoud Abuzaina,\nManHyuk, Marek \u0160Uppa, MathSquared, Mats Linander, Matt Wytock, Matthew Daley, Maximilian Bachl,\nmdymczyk, melvyniandrag, Michael Case, Mike Traynor, miqlas, Namrata-Ibm, Nathan Luehr,\nNathan Van Doorn, Noa Ezra, Nolan Liu, Oleg Zabluda, opensourcemattress, Ouwen Huang,\nPaul Van Eck, peisong, Peng Yu, PinkySan, pks, powderluv, Qiao Hai-Jun, Qiao Longfei,\nRajendra Arora, Ralph Tang, resec, Robin Richtsfeld, Rohan Varma, Ryohei Kuroki, SaintNazaire,\nSamuel He, Sandeep Dcunha, sandipmgiri, Sang Han, scott, Scott Mudge, Se-Won Kim, Simon Perkins,\nSimone Cirillo, Steffen Schmitz, Suvojit Manna, Sylvus, Taehoon Lee, Ted Chang, Thomas Deegan,\nTill Hoffmann, Tim, Toni Kunic, Toon Verstraelen, Tristan Rice, Urs K\u00f6Ster, Utkarsh Upadhyay,\nVish (Ishaya) Abrams, Winnie Tsang, Yan Chen, Yan Facai (\u989c\u53d1\u624d), Yi Yang, Yong Tang,\nYoussef Hesham, Yuan (Terry) Tang, Zhengsheng Wei, zxcqwe4906, \u5f20\u5fd7\u8c6a, \u7530\u4f20\u6b66\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 1.4.1\n\n## Bug Fixes and Other Changes\n* `LinearClassifier` fix.\n\n# Release 1.4.0\n\n## Major Features And Improvements\n* `tf.keras` is now part of the core TensorFlow API.\n* [`tf.data`](http://tensorflow.org/guide/data) is now part of\n  the core TensorFlow API.\n  * The API is now subject to backwards compatibility guarantees.\n  * For a guide to migrating from the `tf.contrib.data` API, see the\n    [README](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/data/README.md).\n  * Major new features include `Dataset.from_generator()` (for building an input\n    pipeline from a Python generator), and the `Dataset.apply()` method for\n    applying custom transformation functions.\n  * Several custom transformation functions have been added, including\n    `tf.contrib.data.batch_and_drop_remainder()` and\n    `tf.contrib.data.sloppy_interleave()`.\n* Add `train_and_evaluate` for simple distributed `Estimator` training.\n* Add `tf.spectral.dct` for computing the DCT-II.\n* Add Mel-Frequency Cepstral Coefficient support to `tf.contrib.signal`\n  (with GPU and gradient support).\n* Add a self-check on `import tensorflow` for Windows DLL issues.\n* Add NCHW support to `tf.depth_to_space` on GPU.\n* TensorFlow Debugger (tfdbg):\n  * Add `eval` command to allow evaluation of arbitrary Python/numpy expressions\n    in tfdbg command-line interface. See\n    [Debugging TensorFlow Programs](https://www.tensorflow.org/guide/debugger)\n    for more details.\n  * Usability improvement: The frequently used tensor filter `has_inf_or_nan` is\n    now added to `Session` wrappers and hooks by default. So there is no need\n    for clients to call `.add_tensor_filter(tf_debug.has_inf_or_nan)` anymore.\n* SinhArcsinh (scalar) distribution added to `contrib.distributions`.\n* Make `GANEstimator` opensource.\n* `Estimator.export_savedmodel()` now includes all valid serving signatures\n  that can be constructed from the Serving Input Receiver and all available\n  ExportOutputs. For instance, a classifier may provide regression- and\n  prediction-flavored outputs, in addition to the classification-flavored one.\n  Building signatures from these allows TF Serving to honor requests using the\n  different APIs (Classify, Regress, and Predict). Furthermore,\n  `serving_input_receiver_fn()` may now specify alternative subsets of nodes\n  that may act as inputs. This allows, for instance, producing a prediction\n  signature for a classifier that accepts raw `Tensors` instead of a serialized\n  `tf.Example`.\n* Add `tf.contrib.bayesflow.hmc`.\n* Add `tf.contrib.distributions.MixtureSameFamily`.\n* Make `Dataset.shuffle()` always reshuffles after each iteration by default.\n* Add `tf.contrib.bayesflow.metropolis_hastings`.\n* Add `log_rate` parameter to `tf.contrib.distributions.Poisson`.\n* Extend `tf.contrib.distributions.bijector` API to handle some non-injective\n  transforms.\n* Java:\n  * Generics (e.g., `Tensor<Integer>`) for improved type-safety\n    (courtesy @andrewcmyers).\n  * Support for multi-dimensional string tensors.\n  * Support loading of custom operations (e.g. many in `tf.contrib`) on Linux\n    and OS X\n* All our prebuilt binaries have been built with CUDA 8 and cuDNN 6.\n  We anticipate releasing TensorFlow 1.5 with CUDA 9 and cuDNN 7.\n\n## Bug Fixes and Other Changes\n* `tf.nn.rnn_cell.DropoutWrapper` is now more careful about dropping out LSTM\n  states.  Specifically, it no longer ever drops the `c` (memory) state of an\n  `LSTMStateTuple`.  The new behavior leads to proper dropout behavior\n  for LSTMs and stacked LSTMs.  This bug fix follows recommendations from\n  published literature, but is a behavioral change.  State dropout behavior\n  may be customized via the new `dropout_state_filter_visitor` argument.\n* Removed `tf.contrib.training.python_input`.  The same behavior, in a more\n  flexible and reproducible package, is available via the new\n  `tf.contrib.data.Dataset.from_generator` method!\n* Fix `tf.contrib.distributions.Affine` incorrectly computing log-det-jacobian.\n* Fix `tf.random_gamma` incorrectly handling non-batch, scalar draws.\n* Resolved a race condition in TensorForest TreePredictionsV4Op.\n* Google Cloud Storage file system, Amazon S3 file system, and Hadoop file\n  system support are now default build options.\n* Custom op libraries must link against libtensorflow_framework.so\n  (installed at `tf.sysconfig.get_lib()`).\n* Change `RunConfig` default behavior to not set a random seed, making random\n  behavior independently random on distributed workers. We expect this to\n  generally improve training performance. Models that do rely on determinism\n  should set a random seed explicitly.\n\n## Breaking Changes to the API\n* The signature of the `tf.contrib.data.rejection_resample()` function has been\n  changed. It now returns a function that can be used as an argument to\n  `Dataset.apply()`.\n* Remove `tf.contrib.data.Iterator.from_dataset()` method. Use\n  `Dataset.make_initializable_iterator()` instead.\n* Remove seldom used and unnecessary `tf.contrib.data.Iterator.dispose_op()`.\n* Reorder some TF-GAN loss functions in a non-backwards compatible way.\n\n## Known Issues\n* In Python 3, `Dataset.from_generator()` does not support Unicode strings.\n  You must convert any strings to bytes objects before yielding them from\n  the generator.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4d55397500, Abdullah Alrasheed, abenmao, Adam Salvail, Aditya Dhulipala, Ag Ramesh,\nAkimasa Kimura, Alan Du, Alan Yee, Alexander, Amit Kushwaha, Amy, Andrei Costinescu,\nAndrei Nigmatulin, Andrew Erlichson, Andrew Myers, Andrew Stepanov, Androbin, AngryPowman,\nAnish Shah, Anton Daitche, Artsiom Chapialiou, asdf2014, Aseem Raj Baranwal, Ash Hall,\nBart Kiers, Batchu Venkat Vishal, ben, Ben Barsdell, Bill Piel, Carl Thom\u00e9, Catalin Voss,\nChangming Sun, Chengzhi Chen, Chi Zeng, Chris Antaki, Chris Donahue, Chris Oelmueller,\nChris Tava, Clayne Robison, Codrut, Courtial Florian, Dalmo Cirne, Dan J, Darren Garvey,\nDavid Kristoffersson, David Norman, David R\u00f6Thlisberger, DavidNorman, Dhruv, DimanNe,\nDorokhov, Duncan Mac-Vicar P, EdwardDixon, EMCP, error.d, FAIJUL, Fan Xia,\nFrancois Xavier, Fred Reiss, Freedom\" Koan-Sin Tan, Fritz Obermeyer, Gao, Xiang,\nGuenther Schmuelling, Guo Yejun (\u90ed\u53f6\u519b), Hans Gaiser, HectorSVC, Hyungsuk Yoon,\nJames Pruegsanusak, Jay Young, Jean Wanka, Jeff Carpenter, Jeremy Rutman, Jeroen B\u00e9Dorf,\nJett Jones, Jimmy Jia, jinghuangintel, jinze1994, JKurland, Joel Hestness, joetoth,\nJohn B Nelson, John Impallomeni, John Lawson, Jonas, Jonathan Dekhtiar, joshkyh, Jun Luan,\nJun Mei, Kai Sasaki, Karl Lessard, karl@kubx.ca, Kb Sriram, Kenichi Ueno, Kevin Slagle,\nKongsea, Lakshay Garg, lhlmgr, Lin Min, liu.guangcong, Loki Der Quaeler, Louie Helm,\nlucasmoura, Luke Iwanski, Lyndon White, Mahmoud Abuzaina, Marcel Puyat, Mark Aaron Shirley,\nMichele Colombo, MtDersvan, Namrata-Ibm, Nathan Luehr, Naurril, Nayana Thorat, Nicolas Lopez,\nNiranjan Hasabnis, Nolan Liu, Nouce, Oliver Hennigh, osdamv, Patrik Erdes,\nPatryk Chrabaszcz, Pavel Christof, Penghao Cen, postBG, Qingqing Cao, Qingying Chen, qjivy,\nRaphael, Rasmi, raymondxyang, Renze Yu, resec, Roffel, Ruben Vereecken, Ryohei Kuroki,\nsandipmgiri, Santiago Castro, Scott Kirkland, Sean Vig, Sebastian Raschka, Sebastian Weiss,\nSergey Kolesnikov, Sergii Khomenko, Shahid, Shivam Kotwalia, Stuart Berg, Sumit Gouthaman,\nsuperzerg, Sven Mayer, tetris, Ti Zhou, Tiago Freitas Pereira, Tian Jin, Tomoaki Oiki,\nVaibhav Sood, vfdev, Vivek Rane, Vladimir Moskva, wangqr, Weber Xie, Will Frey,\nYan Facai (\u989c\u53d1\u624d), yanivbl6, Yaroslav Bulatov, Yixing Lao, Yong Tang, youkaichao,\nYuan (Terry) Tang, Yue Zhang, Yuxin Wu, Ziming Dong, ZxYuan, \u9ec4\u749e\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 1.3.0\n\nSee also [TensorBoard 0.1.4](https://github.com/tensorflow/tensorboard/releases/tag/0.1.4) release notes.\n\n## Major Features and Improvements\n* Added canned estimators to Tensorflow library. List of added estimators:\n  * `DNNClassifier`\n  * `DNNRegressor`\n  * `LinearClassifier`\n  * `LinearRegressor`\n  * `DNNLinearCombinedClassifier`\n  * `DNNLinearCombinedRegressor`.\n* All our prebuilt binaries have been built with cuDNN 6. We anticipate releasing TensorFlow 1.4 with cuDNN 7.\n* `import tensorflow` now goes much faster.\n* Adds a file cache to the GCS filesystem with configurable max staleness for file contents. This permits caching of file contents across close/open boundaries.\n* Added an axis parameter to `tf.gather`.\n* Added a `constant_values` keyword argument to `tf.pad`.\n* Adds `Dataset.interleave` transformation.\n* Add `ConcatenateDataset` to concatenate two datasets.\n* Added Mobilenet support to TensorFlow for Poets training script.\n* Adds a block cache to the GCS filesystem with configurable block size and count.\n* SinhArcSinh bijector added.\n* Added `Dataset.list_files` API.\n* Introduces new operations and Python bindings for the Cloud TPU.\n* Adding TensorFlow-iOS CocoaPod for symmetry with tensorflow-android.\n* Introduces base implementations of ClusterResolvers.\n* Unify memory representations of TensorShape and PartialTensorShape. As a consequence, tensors now have a maximum of 254 dimensions, not 255.\n* Changed references to LIBXSMM to use version 1.8.1.\n* TensorFlow Debugger (tfdbg):\n  * Display summaries of numeric tensor values with the `-s` flag to command `print_tensor` or `pt`.\n  * Display feed values with the `print_feed` or `pf` command and clickable links in the curses UI.\n  * Runtime profiler at the op level and the Python source line level with the `run -p` command.\n* Initial release of the statistical distribution library `tf.distributions`.\n* GPU kernels and speed improvements for unary `tf.where` and `tf.nn.top_k`.\n* Monotonic Attention wrappers added to `tf.contrib.seq2seq`.\n* Added `tf.contrib.signal`, a library for signal processing primitives.\n* Added `tf.contrib.resampler`, containing CPU and GPU ops for differentiable resampling of images.\n\n## Breaking Changes to the API\n* `tf.RewriterConfig` was removed from the Python API after being available in 1.2 release candidates (it was never in an actual release). Graph rewriting is still available, just not as `tf.RewriterConfig`. Instead add an explicit import.\n* Breaking change to `tf.contrib.data.Dataset` APIs that expect a nested structure. Lists are now converted to `tf.Tensor` implicitly. You may need to change uses of lists to tuples in existing code. In addition, dicts are now supported as a nested structure.\n\n## Changes to contrib APIs\n* Adds tf.contrib.nn.rank_sampled_softmax_loss, a sampled-softmax variant that can improve rank loss.\n* `tf.contrib.metrics`.{streaming_covariance,streaming_pearson_correlation} modified to return nan when they have seen less or equal to 1 unit of weight.\n* Adds time series models to contrib. See contrib/timeseries/README.md for details.\n* Adds FULLY_CONNECTED Op to tensorflow/lite/schema.fbs\n\n## Known Issues\n* Tensorflow_gpu compilation fails with Bazel 0.5.3.\n\n## Bug Fixes and Other Changes\n* Fixes `strides` and `begin` dtype mismatch when slicing using int64 Tensor index in python.\n* Improved convolution padding documentation.\n* Add a tag constant, gpu, to present graph with GPU support.\n* `saved_model.utils` now support SparseTensors transparently.\n* A more efficient implementation of non-max suppression.\n* Add support for the shrinkage-type L2 to FtrlOptimizer in addition to the online L2 it already supports.\n* Fix negative variance in moments calculation.\n* Expand UniqueOp Benchmark Tests to cover more collision cases.\n* Improves stability of GCS filesystem on Mac.\n* Add time estimation to HloCostAnalysis.\n* Fixed the bug in Estimator that params in constructor was not a deepcopy of the user provided one. This bugs inadvertently enabled user to mutate the params after the creation of Estimator, leading to potentially undefined behavior.\n* Added None check for save_path in `saver.restore`.\n* Register devices under their legacy names in device_mgr to ease the transition to clusterspec-propagated configurations.\n* VectorExponential added to distributions.\n* Add a bitwise module with bitwise_and, bitwise_or, bitwise_xor, and invert functions.\n* Add fixed-grid ODE integration routines.\n* Allow passing bounds to ScipyOptimizerInterface.\n* Correctness fixes for fft_length parameter to `tf.spectral.rfft` & `tf.spectral.irfft`.\n* Exported model signatures using the 'predict' method will no longer have their input and output keys silently ignored and rewritten to 'inputs' and 'outputs'. If a model was exported with different names before 1.2, and is now served with tensorflow/serving, it will accept requests using 'inputs' and 'outputs'. Starting at 1.2, such a model will accept the keys specified during export. Therefore, inference requests using 'inputs' and 'outputs' may start to fail. To fix this, either update any inference clients to send requests with the actual input and output keys used by the trainer code, or conversely, update the trainer code to name the input and output Tensors 'inputs' and 'outputs', respectively. Signatures using the 'classify' and 'regress' methods are not affected by this change; they will continue to standardize their input and output keys as before.\n* Add in-memory caching to the Dataset API.\n* Set default end_of_sequence variable in datasets iterators to false.\n* [Performance] Increase performance of `tf.layers.conv2d` when setting use_bias=True by 2x by using nn.bias_add.\n* Update iOS examples to use CocoaPods, and moved to tensorflow/examples/ios.\n* Adds a family= attribute in `tf.summary` ops to allow controlling the tab name used in Tensorboard for organizing summaries.\n* When GPU is configured, do not require --config=cuda, instead, automatically build for GPU if this is requested in the configure script.\n* Fix incorrect sampling of small probabilities in CPU/GPU multinomial.\n* Add a list_devices() API on sessions to list devices within a cluster. Additionally, this change augment the ListDevices master API to support specifying a session.\n* Allow uses of over-parameterized separable convolution.\n* TensorForest multi-regression bug fix.\n* Framework now supports armv7, cocoapods.org now displays correct page.\n* Script to create iOS framework for CocoaPods.\n* Android releases of TensorFlow are now pushed to jcenter for easier integration into apps. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/inference_interface/README.md for more details.\n* TensorFlow Debugger (tfdbg):\n  * Fixed a bug that prevented tfdbg from functioning with multi-GPU setups.\n  * Fixed a bug that prevented tfdbg from working with `tf.Session.make_callable`.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4F2E4A2E, Adriano Carmezim, Adri\u00e0 Arrufat, Alan Yee, Alex Lattas, Alex Rothberg,\nAlexandr Baranezky, Ali Siddiqui, Andreas Solleder, Andrei Costinescu, Andrew Hundt,\nAndrobin, Andy Kernahan, Anish Shah, Anthony Platanios, Arvinds-Ds, b1rd, Baptiste\nArnaud, Ben Mabey, Benedikt Linse, Beomsu Kim, Bo Wang, Boyuan Deng, Brett Koonce,\nBruno Rosa, Carl Thom\u00e9, Changming Sun, Chase Roberts, Chirag Bhatia, Chris Antaki,\nChris Hoyean Song, Chris Tava, Christos Nikolaou, Croath Liu, cxx, Czxck001, Daniel\nYlitalo, Danny Goodman, Darren Garvey, David Brailovsky, David Norman, DavidNorman,\ndavidpham87, ddurham2, Dhruv, DimanNe, Drew Hintz, Dustin Tran, Earthson Lu, ethiraj,\nFabian Winnen, Fei Sun, Freedom\" Koan-Sin Tan, Fritz Obermeyer, Gao, Xiang, Gautam,\nGuenther Schmuelling, Gyu-Ho Lee, Hauke Brammer, horance, Humanity123, J Alammar,\nJayeol Chun, Jeroen B\u00e9Dorf, Jianfei Wang, jiefangxuanyan, Jing Jun Yin, Joan Puigcerver,\nJoel Hestness, Johannes Mayer, John Lawson, Johnson145, Jon Malmaud, Jonathan Alvarez-Gutierrez,\nJuang, Yi-Lin, Julian Viereck, Kaarthik Sivashanmugam, Karl Lessard, karl@kubx.ca, Kevin\nCarbone, Kevin Van Der Burgt, Kongsea, ksellesk, lanhin, Lef Ioannidis, Liangliang He,\nLouis Tiao, Luke Iwanski, L\u00e1Szl\u00f3 Csomor, magixsno, Mahmoud Abuzaina, Marcel Hlopko, Mark\nNeumann, Maxwell Paul Brickner, mdfaijul, Micha\u00ebL Defferrard, Micha\u0142 Jastrz\u0119Bski, Michele\nColombo, Mike Brodie, Mosnoi Ion, mouradmourafiq, myPrecious, Nayana Thorat,\nNeeraj Kashyap, Nelson Liu, Niranjan Hasabnis, Olivier Moindrot, orome, Pankaj Gupta, Paul\nVan Eck, peeyush18, Peng Yu, Pierre, preciousdp11, qjivy, Raingo, raoqiyu, ribx, Richard S.\nImaoka, Rishabh Patel, Robert Walecki, Rockford Wei, Ryan Kung, Sahil Dua, Sandip Giri, Sayed\nHadi Hashemi, sgt101, Shitian Ni, Shuolongbj, Siim P\u00f5Der, Simon Perkins, sj6077, SOLARIS,\nSpotlight0xff, Steffen Eberbach, Stephen Fox, superryanguo, Sven Mayer, Tapan Prakash,\nTiago Morais Morgado, Till Hoffmann, Tj Rana, Vadim Markovtsev, vhasanov, Wei Wu,\nwindead, Yan (Asta) Li, Yan Chen, Yann Henon, Yi Wang, Yong Tang, yorkie, Yuan (Terry)\nTang, Yuxin Wu, zhengjiajin, zhongzyd, \u9ec4\u749e\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 1.2.1\n\n## Bug Fixes and Other Changes\n* Updating markdown version required to >= 2.6.8.\n* Support tensors as dropout rates again, by removing the min(max(..))\n\n# Release 1.2.0\n\n## Major Features and Improvements\n* Python 3.6 support on Windows.\n* Added `tf.layers.conv3d_transpose` layer for spatio temporal deconvolution.\n* Added `tf.Session.make_callable()`, which provides a lower overhead means of running a similar step multiple times.\n* Added libverbs-based RDMA support to contrib (courtesy @junshi15 from Yahoo).\n* Bring `tf.feature_column.*` into the API. Non-deprecated functionality from `tf.contrib.layers.*` is moved to `tf.feature_column.*` with cosmetic changes.\n* `RNNCell` objects now subclass `tf.layers.Layer`.  The strictness described\n  in the TensorFlow 1.1 release is gone:  The first time an RNNCell is used,\n  it caches its scope.  All future uses of the RNNCell will reuse variables from\n  that same scope.  This is a breaking change from the behavior of RNNCells\n  in TensorFlow versions <= 1.0.1.  TensorFlow 1.1 had checks in place to\n  ensure old code works correctly with the new semantics; this version\n  allows more flexible uses of RNNCell but can lead to subtle errors if\n  using code meant for TensorFlow <= 1.0.1.  For example, writing:\n  `MultiRNNCell([lstm] * 5)` will now build a 5-layer LSTM stack where each\n  layer shares the **same** parameters.  To get 5 layers each with their own\n  parameters, write: `MultiRNNCell([LSTMCell(...) for _ in range(5)])`.\n  If at all unsure, first test your code with TF 1.1; ensure it raises no\n  errors, and then upgrade to TF 1.2.\n* RNNCells' variable names have been renamed for consistency with Keras layers.\n  Specifically, the previous variable names \"weights\" and \"biases\" have\n  been changed to \"kernel\" and \"bias\", respectively.\n  This may cause backward incompatibility with regard to your old\n  checkpoints containing such RNN cells, in which case you can use the tool\n  [checkpoint_convert script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/tools/checkpoint_convert.py)\n  to convert the variable names in your old checkpoints.\n* Many of the RNN functions and classes that were in the `tf.nn` namespace\n  before the 1.0 release and which were moved to `tf.contrib.rnn` have now\n  been moved back to the core namespace.  This includes\n  `RNNCell`, `LSTMCell`, `GRUCell`, and a number of other cells.  These\n  now reside in `tf.nn.rnn_cell` (with aliases in `tf.contrib.rnn` for backwards\n  compatibility).  The original `tf.nn.rnn` function is now `tf.nn.static_rnn`,\n  and the bidirectional static and state saving static rnn functions are also\n  now back in the `tf.nn` namespace.\n\n  Notable exceptions are the `EmbeddingWrapper`, `InputProjectionWrapper` and\n  `OutputProjectionWrapper`,  which will slowly be moved to deprecation\n  in `tf.contrib.rnn`.  These are inefficient wrappers that should often\n  be replaced by calling `embedding_lookup` or `layers.dense` as pre- or post-\n  processing of the rnn.  For RNN decoding, this functionality has been replaced\n  with an alternative API in `tf.contrib.seq2seq`.\n* Intel MKL Integration (https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture). Intel developed a number of\n  optimized deep learning primitives: In addition to matrix multiplication and\n  convolution, these building blocks include:\n  Direct batched convolution\n  Pooling: maximum, minimum, average\n  Normalization: LRN, batch normalization\n  Activation: rectified linear unit (ReLU)\n  Data manipulation: multi-dimensional transposition (conversion), split,\n  concat, sum and scale.\n* TensorForest Estimator now supports SavedModel export for serving.\n* Support client-provided ClusterSpec's and propagate them to all workers to enable the creation of dynamic TensorFlow clusters.\n* TensorFlow C library now available for Windows.\n* We released a new open-source version of TensorBoard.\n* [`SavedModel CLI`](https://www.tensorflow.org/versions/master/guide/saved_model_cli) tool available to inspect and execute MetaGraph in SavedModel\n* Android releases of TensorFlow are now pushed to jcenter for easier\n  integration into apps. See\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/inference_interface/README.md\n  for more details.\n\n## Deprecations\n\n* TensorFlow 1.2 may be the last time we build with cuDNN 5.1. Starting with\n  TensorFlow 1.3, we will try to build all our prebuilt binaries with cuDNN 6.0.\n  While we will try to keep our source code compatible with cuDNN 5.1, it will\n  be best effort.\n\n## Breaking Changes to the API\n* `org.tensorflow.contrib.android.TensorFlowInferenceInterface` now throws exceptions where possible and has simplified method signatures.\n\n## Changes to contrib APIs\n* Added `tf.contrib.util.create_example`.\n* Added bilinear interpolation to `tf.contrib.image`.\n* Add `tf.contrib.stateless` for random ops with custom seed control.\n* MultivariateNormalFullCovariance added to contrib/distributions/\n* tensorflow/contrib/rnn undergoes RNN cell variable renaming for\n  consistency with Keras layers. Specifically, the previous variable names\n  \"weights\" and \"biases\" are changed to \"kernel\" and \"bias\", respectively.\n  This may cause backward incompatibility with regard to your old\n  checkpoints containing such RNN cells, in which case you can use the\n  [checkpoint_convert script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/tools/checkpoint_convert.py)\n  to convert the variable names in your old checkpoints.\n* Added `tf.contrib.kernel_methods` module with Ops and estimators for primal\n  (explicit) kernel methods in TensorFlow.\n\n## Bug Fixes and Other Changes\n* In python, `Operation.get_attr` on type attributes returns the Python DType\n  version of the type to match expected get_attr documentation rather than the\n  protobuf enum.\n* tensorflow/contrib/rnn undergoes RNN cell variable renaming for\n  consistency with Keras layers. Specifically, the previous variable names\n  \"weights\" and \"biases\" are changed to \"kernel\" and \"bias\", respectively.\n* Changed MIN_SDK version to 8.0 when building iOS libraries.\n* Fixed LIBXSMM integration.\n* Make decode_jpeg/decode_png/decode_gif handle all formats, since users frequently try to decode an image as the wrong type.\n* Improve implicit broadcasting lowering.\n* Improving stability of GCS/BigQuery clients by a faster retrying of stale transmissions.\n* Remove OpKernelConstruction::op_def() as part of minimizing proto dependencies.\n* VectorLaplaceDiag distribution added.\n* Android demo no longer requires libtensorflow_demo.so to run (libtensorflow_inference.so still required)\n* Added `categorical_column_with_vocabulary_file`.\n* Introduce ops for batching/unbatching tensors across Session::Run() calls.\n* Add tf.log_sigmoid(x) = tf.log(tf.sigmoid(x)) = -tf.nn.softplus(-x).\n* Changed hooks lists to immutable tuples, and now allow any iterable for the associated arguments.\n* Introduce TFDecorator.\n* Added an Mfcc op for speech feature generation.\n* Improved DirectSession::Run() overhead and error checking. Feeding a value of the wrong type will now synchronously raise an INVALID_ARGUMENT error instead of asynchronously raising an INTERNAL error. Code that depends on the (undefined) behavior when feeding a tensor of the wrong type may need to be updated.\n* Added unreduced NONE, and reduced MEAN options for losses. Removed \"WEIGHTED_\" prefix from other Reduction constants.\n* assertAllClose now handles dicts.\n* Added Gmock matcher for HloInstructions.\n* Add var name to errors on variable restore.\n* Added an AudioSpectrogram op for audio feature generation.\n* Added `reduction` arg to losses.\n* `tf.placeholder` can represent scalar shapes and partially known.\n* Remove estimator_spec(mode) argument.\n* Added an AudioSpectrogram op for audio feature generation.\n* TensorBoard disables all runs by default if there are more than 40 runs.\n* Removed old doc generator code.\n* GCS file system integration now supports domain buckets, e.g gs://bucket.domain.com/path.\n* Add `tf.summary.text` for outputting text to TensorBoard.\n* The \"run\" command of tfdbg's command-line interface now supports filtering of tensors by node name, op type and tensor dtype.\n* `tf.string_to_number` now supports int64 and float64 outputs.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n4F2E4A2E, Aaron Schumacher, Abhi Agg, admcrae, Adriano Carmezim, Adri\u00e0 Arrufat,\nagramesh1, Akimitsu Seo, Alan Mosca, Alex Egg, Alex Rothberg, Alexander Heinecke,\nAlexander Matyasko, Alexandr Baranezky, Alexandre Caulier, Ali Siddiqui, Anand Venkat,\nAndrew Hundt, Androbin, Anmol Sharma, Arie, Arno Leist, Arron Cao, Aur\u00e9Lien Geron, Bairen Yi,\nBeomsu Kim, Carl Thom\u00e9, cfperez, Changming Sun, Corey Wharton, critiqjo, Dalei Li, Daniel\nRasmussen, Daniel Trebbien, Dar\u00edO Here\u00f1\u00fa, David Eng, David Norman, David Y. Zhang, Davy Song, ddurham2,\nDeepak Subburam, Dmytro Kyrychuk, Dominic Rossi, Dominik Schl\u00f6Sser, Dustin Tran,\nEduardo Pinho, Egil Martinsson, Elliot Saba, Eric Bigelow, Erik Smistad, Evan Klitzke,\nFabrizio Milo, Falcon Dai, Fei Gao, FloopCZ, Fung Lam, Gautam, GBLin5566, Greg Peatfield,\nGu Wang, Guenther Schmuelling, Hans Pabst, Harun Gunaydin, Huaizheng, Ido Shamay, Ikaro\nSilva, Ilya Edrenkin, Immexxx, James Mishra, Jamie Cooke, Jay Young, Jayaram Bobba,\nJianfei Wang, jinghua2, Joey Meyer, John Maidens, Jonghoon Jin, Julian Villella,\nJun Kim, Jun Shi, Junwei Pan, jyegerlehner, Karan Desai, Karel Van De Plassche,\nKb Sriram, KhabarlakKonstantin, Koan-Sin Tan, krivard, Kwotsin, Leandro Gracia Gil,\nLi Chen, Liangliang He, Louie Helm, lspvic, Luiz Henrique Soares, L\u00e1Szl\u00f3 Csomor,\nMark Wong, Mathew Wicks, Matthew Rahtz, Maxwell Paul Brickner, Michael Hofmann, Miguel\nFlores Ruiz De Eguino, MikeTam1021, Mortada Mehyar, Mycosynth, Namnamseo,\nNate Harada, Neven Miculinic, Nghia Tran, Nick Lyu, Niranjan Hasabnis, Nishidha, Oleksii\nKuchaiev, Oyesh Mann Singh, Panmari, Patrick, Paul Van Eck, Piyush Chaudhary, Quim Llimona,\nRaingo, Richard Davies, Ruben Vereecken, Sahit Chintalapudi, Sam Abrahams, Santiago Castro,\nScott Sievert, Sean O'Keefe, Sebastian Schlecht, Shane, Shubhankar Deshpande, Spencer Schaber,\nSunyeop Lee, t13m, td2014, Thomas H. P. Andersen, Toby Petty, Umang Mehta,\nVadim Markovtsev, Valentin Iovene, Vincent Zhao, Vit Stepanovs, Vivek Rane, Vu Pham, wannabesrevenge,\nweipingpku, wuhaixutab, wydwww, Xiang Gao, Xiaolin Lin, xiaoyaozhuzi, Yaroslav Bulatov, Yi Liu,\nYoshihiro Sugi, Yuan (Terry) Tang, Yuming Wang, Yuxin Wu, Zader Zheng, Zhaojun Zhang, zhengjiajin,\nZhipengShen, Ziming Dong, zjj2wry\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 1.1.0\n\n## Major Features and Improvements\n* Added Java API support for Windows.\n* Added `tf.spectral` module. Moved existing FFT ops to `tf.spectral` while\n  keeping an alias in the old location (`tf.*`).\n* Added 1D, 2D and 3D Fourier transform ops for real signals to `tf.spectral`.\n* Added a `tf.bincount` function.\n* Added Keras 2 API to contrib.\n* Added a new lightweight queue-like object - `RecordInput`.\n* Added `tf.contrib.image.compose_transforms` function.\n* Bring `tf.estimator.*` into the API. Non-deprecated functionality from `tf.contrib.learn.Estimator` is moved to `tf.estimator.Estimator` with cosmetic changes.\n* Docker images: TF images on gcr.io and Docker Hub are upgraded to ubuntu:16.04.\n* Added the following features to TensorFlow Debugger (tfdbg):\n  * Ability to inspect Python source file against TF ops and tensors (command `print_source` / `ps`)\n  * New navigation bar in Curses-based UI\n  * NodeStepper (command `invoke_stepper`) now uses intermediate tensor dumps. It also uses `TensorHandles` as direct feeds during successive `cont` calls for improved performance and reduced memory consumption.\n* Initial release of installation guides for Java, C, and Go.\n* Added Text Dashboard to TensorBoard.\n\n## Deprecations\n\n* TensorFlow 1.1.0 will be the last time we release a binary with Mac GPU support. Going forward, we will stop testing on Mac GPU systems. We continue to welcome patches that maintain Mac GPU support, and we will try to keep the Mac GPU build working.\n\n## Changes to contrib APIs\n* The behavior of RNNCells is now stricter due to the transition towards making RNNCells act more like Keras layers.\n  * If an RNNCell is used twice in two different variable scopes, an error is raised describing how to avoid this behavior.\n  * If an RNNCell is used in a variable scope with existing conflicting variables, an error is raised showing that the RNNCell must be constructed with argument `reuse=True`.\n* Deprecated contrib/distributions `pmf`, `pdf`, `log_pmf`, `log_pdf`.\n* Moved `bayesflow.special_math` to distributions.\n* `tf.contrib.tensor_forest.python.tensor_forest.RandomForestDeviceAssigner` removed.\n* Changed some MVN classes and parameters:\n  * `tf.contrib.distributions.MultivariateNormalFull` replaced by `tf.contrib.distributions.MultivariateNormalTriL`.\n  * `tf.contrib.distributions.MultivariateNormalCholesky` replaced by `tf.contrib.distributions.MultivariateNormalTriL`\n  * `tf.contrib.distributions.MultivariateNormalDiagWithSoftplusStDev` replaced\n    by `tf.contrib.distributions.MultivariateNormalDiagWithSoftplusScale`\n  * `tf.contrib.distributions.MultivariateNormalDiag` arguments changed from `mu`, `diag_stddev` to `log`, `scale_diag`.\n  * `tf.contrib.distributions.MultivariateNormalDiagPlusVDVT` removed.\n  * `tf.contrib.distributions.MultivariateNormalDiagPlusLowRank` added.\n\n## Bug Fixes and Other Changes\n* Java: Support for loading models exported using the SavedModel API (courtesy @EronWright).\n* Go: Added support for incremental graph execution.\n* Fix a bug in the WALS solver when single-threaded.\n* Added support for integer sparse feature values in `tf.contrib.layers.sparse_column_with_keys`.\n* Fixed `tf.set_random_seed(0)` to be deterministic for all ops.\n* Stability improvements for the GCS file system support.\n* Improved TensorForest performance.\n* Added support for multiple filename globs in `tf.matching_files`.\n* `LogMessage` now includes a timestamp as beginning of a message.\n* Added MultiBox person detector example standalone binary.\n* Android demo: Makefile build functionality added to build.gradle to fully support building TensorFlow demo in Android on Windows.\n* Android demo: read MultiBox priors from txt file rather than protobuf.\n* Added colocation constraints to `StagingArea`.\n* `sparse_matmul_op` reenabled for Android builds.\n* Restrict weights rank to be the same as the broadcast target, to avoid ambiguity on broadcast rules.\n* Upgraded libxsmm to 1.7.1 and applied other changes for performance and memory usage.\n* Fixed bfloat16 integration of LIBXSMM sparse mat-mul.\n* Improved performance and reduce memory usage by allowing ops to forward input buffers to output buffers and perform computations in-place.\n* Improved the performance of CPU assignment for strings.\n* Speed up matrix * vector multiplication and matrix * matrix with unknown shapes.\n* C API: Graph imports now support input remapping, control dependencies, and returning imported nodes (see `TF_GraphImportGraphDefWithReturnOutputs()`)\n* Multiple C++ API updates.\n* Multiple TensorBoard updates including:\n  * Users can now view image summaries at various sampled steps (instead of just the last step).\n  * Bugs involving switching runs as well as the image dashboard are fixed.\n  * Removed data download links from TensorBoard.\n  * TensorBoard uses a relative data directory, for easier embedding.\n  * TensorBoard automatically ignores outliers for domain calculation, and formats proportional values consistently.\n* Multiple tfdbg bug fixes:\n  * Fixed Windows compatibility issues.\n  * Command history now persists across runs.\n  * Bug fix in graph validation related to `tf.while_loops`.\n* Java Maven fixes for bugs with Windows installation.\n* Backport fixes and improvements from external keras.\n* Keras config file handling fix.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nA. Besir Kurtulmus, Adal Chiriliuc, @akash, Alec-Desouza, Alex Rothberg, Alex\nSergeev, Alexander Heinecke, Allen Guo, Andreas Madsen, Ankesh Anand, Anton\nLoss, @Aravind, @Arie, Ashutosh Das, Aur\u00e9Lien Geron, Bairen Yi, @bakunyo, Ben\nVisser, Brady Zhou, Calpa Liu, Changming Sun, Chih Cheng Liang, Christopher\nBerner, Clark Zinzow, @Conchylicultor, Dan Ellis, Dan J, Dan Jarvis, Daniel\nYlitalo, Darren Garvey, David Norman, David Truong, @DavidNorman, Dimitar\nPavlov, Dmitry Persiyanov, @Eddie, @elirex, Erfan Noury, Eron Wright, Evgeny\nMazovetskiy, Fabrizio (Misto) Milo, @fanlu, Fisher Coder, Florian Courtial,\nFranck Dernoncourt, Gagan Goel, Gao, Xiang, @Gautam, Gefu Tang, @guilherme,\n@guschmue, Hannah Provenza, Hans Pabst, @hartb, Hsiao Yi, Huazuo Gao, Igor\nChor\u0105\u017cEwicz, Ivan Smirnov, Jakub Kolodziejczyk, Jason Gavris, Jason Morton, Jay\nYoung, Jayaram Bobba, Jeremy Sawruk, Jiaming Liu, Jihun Choi, @jiqiu, Joan Thibault,\nJohn C F, Jojy George Varghese, Jon Malmaud, Julian Berman, Julian Niedermeier,\nJunpeng Lao, Kai Sasaki, @Kankroc, Karl Lessard, Kyle Bostelmann, @Lezcano, Li\nYi, Luo Yun, @lurker, Mahmoud-Abuzaina, Mandeep Singh, Marek Kolodziej, Mark\nSzepieniec, Martial Hue, Medhat Omr, Memo Akten, Michael Gharbi, Micha\u00ebL Defferrard,\nMilan Straka, @MircoT, @mlucool, Muammar Ibn Faisal, Nayana Thorat, @nghiattran,\nNicholas Connor, Nikolaas Steenbergen, Niraj Patel, Niranjan Hasabnis, @Panmari,\nPavel Bulanov, Philip Pries Henningsen, Philipp Jund, @polonez, Prayag Verma, Rahul\nKavi, Raphael Gontijo Lopes, @rasbt, Raven Iqqe, Reid Pryzant, Richard Shin, Rizwan\nAsif, Russell Kaplan, Ryo Asakura, R\u00fcDiger Busche, Saisai Shao, Sam Abrahams, @sanosay,\nSean Papay, @seaotterman, @selay01, Shaurya Sharma, Sriram Narayanamoorthy, Stefano\nProbst, @taknevski, @tbonza, @teldridge11, Tim Anglade, Tomas Reimers, Tomer Gafner,\nValentin Iovene, Vamsi Sripathi, Viktor Malyi, Vit Stepanovs, Vivek Rane, Vlad Firoiu,\n@wangg12, @will, Xiaoyu Tao, Yaroslav Bulatov, Yi Liu, Yuan (Terry) Tang, @Yufeng,\nYuming Wang, Yuxin Wu, Zafar Takhirov, Ziming Dong\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n\n# Release 1.0.1\n\n## Bug Fixes and Other Changes\n* Change GraphConstructor to not increase the version when importing, but instead take the min of all versions.\n* Google Cloud Storage fixes.\n* Removed `tf.core` and `tf.python` modules from the API. These were never intended to be exposed. Please use the same objects through top-level `tf` module instead.\n\n# Release 1.0.0\n\n## Major Features and Improvements\n* XLA (experimental): initial release of [XLA](https://www.tensorflow.org/versions/master/experimental/xla/), a domain-specific compiler for TensorFlow graphs, that targets CPUs and GPUs.\n* TensorFlow Debugger (tfdbg): command-line interface and API.\n* New python 3 docker images added.\n* Made pip packages pypi compliant. TensorFlow can now be installed by `pip\n  install tensorflow` command.\n* Several python API calls have been changed to resemble NumPy more closely.\n* Android: person detection + tracking demo implementing Scalable Object\n  Detection using Deep Neural Networks.\n* New (experimental) [Java API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java).\n* Add new Android image stylization demo based on \"A Learned Representation For Artistic Style\", and add YOLO object detector support.\n\n## Breaking Changes to the API\nTo help you upgrade your existing TensorFlow Python code to match the API changes below, we have prepared a [conversion script](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility).\n* TensorFlow/models have been moved to a separate github repository.\n* Division and modulus operators (/, //, %) now match Python (flooring)\n  semantics. This applies to `tf.div` and `tf.mod` as well. To obtain forced\n  integer truncation based behaviors you can use `tf.truncatediv`\n  and `tf.truncatemod`.\n* `tf.divide()` is now the recommended division function. `tf.div()` will\n  remain, but its semantics do not respond to Python 3 or `from future`\n  mechanisms.\n* tf.reverse() now takes indices of axes to be reversed. E.g.\n  `tf.reverse(a, [True, False, True])` must now be written as\n  `tf.reverse(a, [0, 2])`. `tf.reverse_v2()` will remain until 1.0 final.\n* `tf.mul`, `tf.sub` and `tf.neg` are deprecated in favor of `tf.multiply`,\n  `tf.subtract` and `tf.negative`.\n* `tf.pack` and `tf.unpack` are deprecated in favor of `tf.stack` and\n  `tf.unstack`.\n* `TensorArray.pack` and `TensorArray.unpack` are getting deprecated in favor of\n  `TensorArray.stack` and `TensorArray.unstack`.\n* The following Python functions have had their arguments changed to use `axis`\n  when referring to specific dimensions. We have kept the old keyword arguments\n  for compatibility currently, but we will be removing them well before the\n  final 1.0.\n  * `tf.argmax`: `dimension` becomes `axis`\n  * `tf.argmin`: `dimension` becomes `axis`\n  * `tf.count_nonzero`: `reduction_indices` becomes `axis`\n  * `tf.expand_dims`: `dim` becomes `axis`\n  * `tf.reduce_all`: `reduction_indices` becomes `axis`\n  * `tf.reduce_any`: `reduction_indices` becomes `axis`\n  * `tf.reduce_join`: `reduction_indices` becomes `axis`\n  * `tf.reduce_logsumexp`: `reduction_indices` becomes `axis`\n  * `tf.reduce_max`: `reduction_indices` becomes `axis`\n  * `tf.reduce_mean`: `reduction_indices` becomes `axis`\n  * `tf.reduce_min`: `reduction_indices` becomes `axis`\n  * `tf.reduce_prod`: `reduction_indices` becomes `axis`\n  * `tf.reduce_sum`: `reduction_indices` becomes `axis`\n  * `tf.reverse_sequence`: `batch_dim` becomes `batch_axis`, `seq_dim` becomes `seq_axis`\n  * `tf.sparse_concat`: `concat_dim` becomes `axis`\n  * `tf.sparse_reduce_sum`: `reduction_axes` becomes `axis`\n  * `tf.sparse_reduce_sum_sparse`: `reduction_axes` becomes `axis`\n  * `tf.sparse_split`: `split_dim` becomes `axis`\n* `tf.listdiff` has been renamed to `tf.setdiff1d` to match NumPy naming.\n* `tf.inv` has been renamed to be `tf.reciprocal` (component-wise reciprocal)\n  to avoid confusion with `np.inv` which is matrix inversion\n* tf.round now uses banker's rounding (round to even) semantics to match NumPy.\n* `tf.split` now takes arguments in a reversed order and with different\n  keywords. In particular, we now match NumPy order as\n  `tf.split(value, num_or_size_splits, axis)`.\n* `tf.sparse_split` now takes arguments in reversed order and with different\n  keywords. In particular we now match NumPy order as\n  `tf.sparse_split(sp_input, num_split, axis)`. NOTE: we have temporarily\n  made `tf.sparse_split` require keyword arguments.\n* `tf.concat` now takes arguments in reversed order and with different keywords. In particular we now match NumPy order as `tf.concat(values, axis, name)`.\n* `tf.image.decode_jpeg` by default uses the faster DCT method, sacrificing\n  a little fidelity for improved speed. One can revert to the old\n  behavior by specifying the attribute `dct_method='INTEGER_ACCURATE'`.\n* `tf.complex_abs` has been removed from the Python interface. `tf.abs`\n  supports complex tensors and should be used instead.\n* In the C++ API (in tensorflow/cc), Input, Output, etc. have moved\n  from the tensorflow::ops namespace to tensorflow.\n* Template.`var_scope` property renamed to `.variable_scope`\n* SyncReplicasOptimizer is removed and SyncReplicasOptimizerV2 renamed to SyncReplicasOptimizer.\n* `tf.zeros_initializer()` and `tf.ones_initializer()` now return a callable\n  that must be called with initializer arguments, in your code replace\n  `tf.zeros_initializer` with `tf.zeros_initializer()`.\n* `SparseTensor.shape` has been renamed to `SparseTensor.dense_shape`.  Same for\n  `SparseTensorValue.shape`.\n* Replace tf.scalar_summary, tf.histogram_summary, tf.audio_summary, tf.image_summary with tf.summary.scalar, tf.summary.histogram, tf.summary.audio, tf.summary.image, respectively. The new summary ops take name rather than tag as their first argument, meaning summary ops now respect TensorFlow name scopes.\n* Replace tf.train.SummaryWriter and tf.train.SummaryWriterCache with tf.summary.FileWriter and tf.summary.FileWriterCache.\n* Removes RegisterShape from public API. Use C++ shape function registration\n  instead.\n* Deprecated `_ref` dtypes from the python API.\n* In the C++ API (in tensorflow/cc), Input, Output, etc. have moved\n  from the tensorflow::ops namespace to tensorflow.\n* Change arg order for `{softmax,sparse_softmax,sigmoid}_cross_entropy_with_logits` to be (labels, predictions), and force use of named args.\n* tf.nn.rnn_cell.* and most functions in tf.nn.rnn.* (with the exception of dynamic_rnn and raw_rnn) are temporarily in tf.contrib.rnn.  They will be moved back into core for TF 1.2.\n* `tf.nn.sampled_softmax_loss` and `tf.nn.nce_loss` have both changed their API such that you need to switch the `inputs, labels` to `labels, inputs` parameters.\n* The shape keyword argument of the `SparseTensor` constructor changes its name to `dense_shape` between Tensorflow 0.12 and Tensorflow 1.0.\n\n## Bug Fixes and Other Changes\n* Numerous C++ API updates.\n* New op: `parallel_stack`.\n* Introducing common tf io compression options constants for\n  RecordReader/RecordWriter.\n* Add `sparse_column_with_vocabulary_file`, to specify a feature column that\n  transform string features to IDs, where the mapping is defined by a vocabulary\n  file.\n* Added `index_to_string_table` which returns a lookup table that maps indices to\n  strings.\n* Add `string_to_index_table`, which returns a lookup table that matches strings\n  to indices.\n* Add a `ParallelForWithWorkerId` function.\n* Add `string_to_index_table`, which returns a lookup table that matches strings\n  to indices.\n* Support restore session from checkpoint files in v2 in `contrib/session_bundle`.\n* Added a tf.contrib.image.rotate function for arbitrary angles.\n* Added `tf.contrib.framework.filter_variables` as a convenience function to\n  filter lists of variables based on regular expressions.\n* `make_template()` takes an optional `custom_getter_ param`.\n* Added comment about how existing directories are handled by\n  `recursive_create_dir`.\n* Added an op for QR factorizations.\n* Divides and mods in Python API now use flooring (Python) semantics.\n* Android: pre-built libs are now built nightly.\n* Android: cmake/gradle build for TensorFlow Inference library under\n  `contrib/android/cmake`\n* Android: Much more robust Session initialization code.\n* Android: TF stats now exposed directly in demo and log when debug mode is\n  active\n* Android: new/better README.md documentation\n* saved_model is available as `tf.saved_model`.\n* Empty op is now stateful.\n* Improve speed of scatter_update on the cpu for ASSIGN operations.\n* Change `reduce_join` to treat `reduction_indices` in the same way as other `reduce_` ops.\n* Move `TensorForestEstimator` to `contrib/tensor_forest`.\n* Enable compiler optimizations by default and allow configuration in configure.\n* `tf.divide` now honors the name field.\n* Make metrics weight broadcasting more strict.\n* Add new queue-like `StagingArea` and new ops: `stage` and `unstage`.\n* Enable inplace update ops for strings on CPU. Speed up string concat.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAaron Hu, Abhishek Aggarwal, Adam Michael, Adriano Carmezim, @AfirSraftGarrier,\nAlexander Novikov, Alexander Rosenberg Johansen, Andrew Gibiansky, Andrew Hundt,\nAnish Shah, Anton Loss, @b0noI, @BoyuanJiang, Carl Thom\u00e9, Chad Kennedy, Comic\nChang, Connor Braa, Daniel N. Lang, Daniel Trebbien,\n@danielgordon10, Darcy Liu, Darren Garvey, Dmitri Lapin, Eron Wright, Evan\nCofer, Fabrizio Milo, Finbarr Timbers, Franck Dernoncourt, Garrett Smith,\n@guschmue, Hao Wei, Henrik Holst, Huazuo Gao, @Ian, @Issac, Jacob Israel,\nJangsoo Park, Jin Kim, Jingtian Peng, John Pope, Kye Bostelmann, Liangliang He,\nLing Zhang, Luheng He, Luke Iwanski, @lvli, Michael Basilyan, Mihir Patel,\nMikalai Drabovich, Morten Just, @newge, Nick Butlin, Nishant Shukla,\nPengfei Ni, Przemyslaw Tredak, @rasbt, @Ronny, Rudolf Rosa, @RustingSword,\nSam Abrahams, Sam Putnam, @SeongAhJo, Shi Jiaxin, @skavulya, Steffen M\u00fcLler,\n@TheUSER123, @tiriplicamihai, @vhasanov, Victor Costan, Vit Stepanovs,\nWangda Tan, Wenjian Huang, Xingdong Zuo, Yaroslav Bulatov, Yota Toyama,\nYuan (Terry) Tang, Yuxin Wu\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n\n# Release 0.12.0\n\n## Major Features and Improvements\n\n* TensorFlow now builds and runs on Microsoft Windows (tested on Windows 10,\n  Windows 7, and Windows Server 2016). Supported languages include Python (via a\n  pip package) and C++. CUDA 8.0 and cuDNN 5.1 are supported for GPU\n  acceleration. Known limitations include: It is not currently possible to load\n  a custom op library. The GCS and HDFS file systems are not currently\n  supported. The following ops are not currently implemented:\n  Dequantize, QuantizeAndDequantize, QuantizedAvgPool,\n  QuantizedBatchNomWithGlobalNormalization, QuantizedBiasAdd, QuantizedConcat,\n  QuantizedConv2D, QuantizedMatmul, QuantizedMaxPool,\n  QuantizeDownAndShrinkRange, QuantizedRelu, QuantizedRelu6, QuantizedReshape,\n  QuantizeV2, RequantizationRange, and Requantize.\n* Go: Experimental API in Go to create and execute graphs\n  (https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go)\n* New checkpoint format becomes the default in `tf.train.Saver`. Old V1\n  checkpoints continue to be readable; controlled by the `write_version`\n  argument, `tf.train.Saver` now by default writes out in the new V2\n  format. It significantly reduces the peak memory required and latency\n  incurred during restore.\n* Added a new library for library of matrix-free (iterative) solvers for linear\n  equations, linear least-squares, eigenvalues and singular values in\n  tensorflow/contrib/solvers. Initial version has lanczos bidiagonalization,\n  conjugate gradients and CGLS.\n* Added gradients for `matrix_solve_ls` and `self_adjoint_eig`.\n* Large cleanup to add second order gradient for ops with C++ gradients and\n  improve existing gradients such that most ops can now be differentiated\n  multiple times.\n* Added a solver for ordinary differential equations,\n  `tf.contrib.integrate.odeint`.\n* New contrib module for tensors with named axes, `tf.contrib.labeled_tensor`.\n* Visualization of embeddings in TensorBoard.\n\n## Breaking Changes to the API\n\n* `BusAdjacency` enum replaced with a protocol buffer `DeviceLocality`.  PCI bus\n  indexing now starts from 1 instead of 0, and `bus_id==0` is used where\n  previously `BUS_ANY` was used.\n* `Env::FileExists` and `FileSystem::FileExists` now return a tensorflow::Status\n  instead of a bool. Any callers to this function can be converted to a bool\n  by adding .ok() to the call.\n* The C API type `TF_SessionWithGraph` has been renamed to `TF_Session`,\n  indicating its preferred use in language bindings for TensorFlow.\n  What was previously `TF_Session` has been renamed to `TF_DeprecatedSession`.\n* Renamed `TF_Port` to `TF_Output` in the C API.\n* Removes RegisterShape from public API. Use C++ shape function registration instead.\n  indexing now starts from 1 instead of 0, and `bus_id==0` is used where\n  previously `BUS_ANY` was used.\n* Most RNN cells and RNN functions now use different variable scopes to be\n  consistent with layers (`tf.contrib.layers`).  This means old checkpoints\n  written using this code will not load after this change without providing\n  `Saver` a list of variable renames.  Examples of variable scope changes\n  include `RNN` -> `rnn` in `tf.nn.rnn`, `tf.nn.dynamic_rnn` and moving from\n  `Linear/Matrix` -> `weights` and `Linear/Bias` -> `biases` in most RNN cells.\n* Deprecated tf.select op. tf.where should be used instead.\n* `SparseTensor.shape` has been renamed to `SparseTensor.dense_shape`.  Same for\n  `SparseTensorValue.shape`.\n* `Env::FileExists` and `FileSystem::FileExists` now return a\n  `tensorflow::Status` instead of a bool. Any callers to this function can be\n  converted to a bool by adding `.ok()` to the call.\n* C API: Type `TF_SessionWithGraph` has been renamed to `TF_Session`, indicating\n  its preferred use in language bindings for TensorFlow. What was previously\n  `TF_Session` has been renamed to `TF_DeprecatedSession`.\n* C API: Renamed `TF_Port` to `TF_Output`.\n* C API: The caller retains ownership of `TF_Tensor` objects provided to\n  `TF_Run`, `TF_SessionRun`, `TF_SetAttrTensor` etc.\n* Renamed `tf.image.per_image_whitening()` to\n  `tf.image.per_image_standardization()`\n* Move Summary protobuf constructors to `tf.summary` submodule.\n* Deprecate `histogram_summary`, `audio_summary`, `scalar_summary`,\n  `image_summary`, `merge_summary`, and `merge_all_summaries`.\n* Combined `batch_*` and regular version of linear algebra and FFT ops. The\n  regular op now handles batches as well. All `batch_*` Python interfaces were\n  removed.\n* `tf.all_variables`, `tf.VARIABLES` and `tf.initialize_all_variables` renamed\n  to `tf.global_variables`, `tf.GLOBAL_VARIABLES` and\n  `tf.global_variables_initializer` respectively.\n* `tf.zeros_initializer()` and `tf.ones_initializer()` now return a callable\n  that must be called with initializer arguments, in your code replace\n  `tf.zeros_initializer` with `tf.zeros_initializer()`\n\n## Bug Fixes and Other Changes\n\n* Use threadsafe version of `lgamma` function.\n* Fix `tf.sqrt` handling of negative arguments.\n* Fixed bug causing incorrect number of threads to be used for multi-threaded\n  benchmarks.\n* Performance optimizations for `batch_matmul` on multi-core CPUs.\n* Improve trace, `matrix_set_diag`, `matrix_diag_part` and their gradients to\n  work for rectangular matrices.\n* Support for SVD of complex valued matrices.\n\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\n@a7744hsc, Abhi Agg, @admcrae, Adriano Carmezim, Aki Sukegawa, Alex Kendall,\nAlexander Rosenberg Johansen, @amcrae, Amlan Kar, Andre Simpelo, Andreas Eberle,\nAndrew Hundt, Arnaud Lenglet, @b0noI, Balachander Ramachandran, Ben Barsdell,\nBen Guidarelli, Benjamin Mularczyk, Burness Duan, @c0g, Changming Sun,\n@chanis, Corey Wharton, Dan J, Daniel Trebbien, Darren Garvey, David Brailovsky,\nDavid Jones, Di Zeng, @DjangoPeng, Dr. Kashif Rasul, @drag0, Fabrizio (Misto)\nMilo, Fabr\u00edCio Ceschin, @fp, @Ghedeon, @guschmue, G\u00f6k\u00e7en Eraslan, Haosdent\nHuang, Haroen Viaene, Harold Cooper, Henrik Holst, @hoangmit, Ivan Ukhov, Javier\nDehesa, Jingtian Peng, Jithin Odattu, Joan Pastor, Johan Mathe, Johannes Mayer,\nJongwook Choi, Justus Schwabedal, Kai Wolf, Kamil Hryniewicz, Kamran Amini,\nKaren Brems, Karl Lattimer, @kborer, Ken Shirriff, Kevin Rose, Larissa Laich,\nLaurent Mazare, Leonard Lee, Liang-Chi Hsieh, Liangliang He, Luke Iwanski,\nMarek Kolodziej, Moustafa Alzantot, @MrQianjinsi, @nagachika, Neil Han, Nick\nMeehan, Niels Ole Salscheider, Nikhil Mishra, @nschuc, Ondrej Skopek, Ond\u0159Ej\nFilip, @OscarDPan, Pablo Moyano, Przemyslaw Tredak, @qitaishui, @Quarazy,\n@raix852, Philipp Helo, Sam Abrahams, @SriramRamesh, Till Hoffmann, Tushar Soni,\n@tvn, @tyfkda, Uwe Schmidt, Victor Villas, Vit Stepanovs, Vladislav Gubarev,\n@wujingyue, Xuesong Yang, Yi Liu, Yilei Yang, @youyou3, Yuan (Terry) Tang,\nYuming Wang, Zafar Takhirov, @zhongyuk, Ziming Dong, @guotong1988\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 0.11.0\n\n## Major Features and Improvements\n\n* CUDA 8 support.\n* cuDNN 5 support.\n* HDFS Support.\n* Adds Fused LSTM support via cuDNN 5 in `tensorflow/contrib/cudnn_rnn`.\n* Improved support for NumPy style basic slicing including non-1 strides,\n  ellipses, newaxis, and negative indices. For example complicated expressions\n  like `foo[1, 2:4, tf.newaxis, ..., :-3:-1, :]` are now supported. In addition\n  we have preliminary (non-broadcasting) support for sliced assignment to\n  variables. In particular one can write `var[1:3].assign([1,11,111])`.\n* Deprecated `tf.op_scope` and `tf.variable_op_scope` in favor of a unified `tf.name_scope` and `tf.variable_scope`. The new argument order of `tf.variable_scope` is incompatible with previous versions.\n* Introducing `core/util/tensor_bundle` module: a module to efficiently\n  serialize/deserialize tensors to disk.  Will be used in TF's new checkpoint\n  format.\n* Added tf.svd for computing the singular value decomposition (SVD) of dense\n  matrices or batches of matrices (CPU only).\n* Added gradients for eigenvalues and eigenvectors computed using\n  `self_adjoint_eig` or `self_adjoint_eigvals`.\n* Eliminated `batch_*` methods for most linear algebra and FFT ops and promoted\n  the non-batch version of the ops to handle batches of matrices.\n* Tracing/timeline support for distributed runtime (no GPU profiler yet).\n* C API gives access to inferred shapes with `TF_GraphGetTensorNumDims` and\n  `TF_GraphGetTensorShape`.\n* Shape functions for core ops have moved to C++ via\n  `REGISTER_OP(...).SetShapeFn(...)`.  Python shape inference RegisterShape calls\n  use the C++ shape functions with `common_shapes.call_cpp_shape_fn`.  A future\n  release will remove `RegisterShape` from python.\n\n\n## Bug Fixes and Other Changes\n\n* Documentation now includes operator overloads on Tensor and Variable.\n* `tensorflow.__git_version__` now allows users to identify the version of the\n  code that TensorFlow was compiled with. We also have\n  `tensorflow.__git_compiler__` which identifies the compiler used to compile\n  TensorFlow's core.\n* Improved multi-threaded performance of `batch_matmul`.\n* LSTMCell, BasicLSTMCell, and MultiRNNCell constructors now default to\n  `state_is_tuple=True`.  For a quick fix while transitioning to the new\n  default, simply pass the argument `state_is_tuple=False`.\n* DeviceFactory's AddDevices and CreateDevices functions now return\n  a Status instead of void.\n* Int32 elements of list(type) arguments are no longer placed in host memory by\n  default. If necessary, a list(type) argument to a kernel can be placed in host\n  memory using a HostMemory annotation.\n* `uniform_unit_scaling_initializer()` no longer takes a `full_shape` arg,\n  instead relying on the partition info passed to the initializer function when\n  it's called.\n* The NodeDef protocol message is now defined in its own file `node_def.proto`\n  `instead of graph.proto`.\n* `ops.NoGradient` was renamed `ops.NotDifferentiable`. `ops.NoGradient` will\n  be removed soon.\n* `dot.h` / DotGraph was removed (it was an early analysis tool prior\n  to TensorBoard, no longer that useful).  It remains in history\n  should someone find the code useful.\n* re2 / regexp.h was removed from being a public interface of TF.\n  Should users need regular expressions, they should depend on the RE2\n  library directly rather than via TensorFlow.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAbid K, @afshinrahimi, @AidanGG, Ajay Rao, Aki Sukegawa, Alex Rothberg,\nAlexander Rosenberg Johansen, Andrew Gibiansky, Andrew Thomas, @Appleholic,\nBastiaan Quast, Ben Dilday, Bofu Chen, Brandon Amos, Bryon Gloden, Cissp\u00ae,\n@chanis, Chenyang Liu, Corey Wharton, Daeyun Shin, Daniel Julius Lasiman, Daniel\nWaterworth, Danijar Hafner, Darren Garvey, Denis Gorbachev, @DjangoPeng,\nEgor-Krivov, Elia Palme, Eric Platon, Fabrizio Milo, Gaetan Semet,\nGeorg Nebehay, Gu Wang, Gustav Larsson, @haosdent, Harold Cooper, Hw-Zz,\n@ichuang, Igor Babuschkin, Igor Macedo Quintanilha, Ilya Edrenkin, @ironhead,\nJakub Kolodziejczyk, Jennifer Guo, Jihun Choi, Jonas Rauber, Josh Bleecher\nSnyder, @jpangburn, Jules Gagnon-Marchand, Karen Brems, @kborer, Kirill Bobyrev,\nLaurent Mazare, Longqi Yang, Malith Yapa, Maniteja Nandana, Martin Englund,\nMatthias Winkelmann, @mecab, Mu-Ik Jeon, Nand Dalal, Niels Ole Salscheider,\nNikhil Mishra, Park Jiin, Pieter De Rijk, @raix852, Ritwik Gupta, Sahil Sharma,\nSangheum Hwang, @SergejsRk, Shinichiro Hamaji, Simon Denel, @Steve, @suiyuan2009,\nTiago Jorge, Tijmen Tieleman, @tvn, @tyfkda, Wang Yang, Wei-Ting Kuo, Wenjian\nHuang, Yan Chen, @YenChenLin, Yuan (Terry) Tang, Yuncheng Li, Yunfeng Wang, Zack\nPolizzi, @zhongzyd, Ziming Dong, @perhapszzy\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 0.10.0\n\n## Major Features and Improvements\n\n* Added support for C++ shape inference\n* Added graph-construction C API\n* Major revision to the graph-construction C++ API\n* Support makefile build for iOS\n* Added Mac GPU support\n* Full version of TF-Slim available as `tf.contrib.slim`\n* Added k-Means clustering and WALS matrix factorization\n\n## Bug Fixes and Other Changes\n\n* Allow gradient computation for scalar values.\n* Performance improvements for gRPC\n* Improved support for fp16\n* New high-level ops in tf.contrib.{layers,metrics}\n* New features for TensorBoard, such as shape display, exponential smoothing\n* Faster and more stable Google Cloud Storage (GCS) filesystem support\n* Support for zlib compression and decompression for TFRecordReader and TFRecordWriter\n* Support for reading (animated) GIFs\n* Improved support for SparseTensor\n* Added support for more probability distributions (Dirichlet, Beta, Bernoulli, etc.)\n* Added Python interfaces to reset resource containers.\n* Many bugfixes and performance improvements\n* Many documentation fixes\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAlex Rothberg, Andrew Royer, Austin Marshall, @BlackCoal, Bob Adolf, Brian Diesel, Charles-Emmanuel Dias, @chemelnucfin, Chris Lesniewski, Daeyun Shin, Daniel Rodriguez, Danijar Hafner, Darcy Liu, Kristinn R. Th\u00f3risson, Daniel Castro, Dmitry Savintsev, Kashif Rasul, Dylan Paiton, Emmanuel T. Odeke, Ernest Grzybowski, Gavin Sherry, Gideon Dresdner, Gregory King, Harold Cooper, @heinzbeinz, Henry Saputra, Huarong Huo, Huazuo Gao, Igor Babuschkin, Igor Macedo Quintanilha, Ivan Ukhov, James Fysh, Jan Wilken D\u00f6rrie, Jihun Choi, Johnny Lim, Jonathan Raiman, Justin Francis, @lilac, Li Yi, Marc Khoury, Marco Marchesi, Max Melnick, Micael Carvalho, @mikowals, Mostafa Gazar, Nico Galoppo, Nishant Agrawal, Petr Janda, Yuncheng Li, @raix852, Robert Rose, @Robin-des-Bois, Rohit Girdhar, Sam Abrahams, satok16, Sergey Kishchenko, Sharkd Tu, @shotat, Siddharth Agrawal, Simon Denel, @sono-bfio, SunYeop Lee, Thijs Vogels, @tobegit3hub, @Undo1, Wang Yang, Wenjian Huang, Yaroslav Bulatov, Yuan Tang, Yunfeng Wang, Ziming Dong\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 0.9.0\n\n## Major Features and Improvements\n\n* Python 3.5 support and binaries\n* Added iOS support\n* Added support for processing on GPUs on MacOS\n* Added makefile for better cross-platform build support (C API only)\n* fp16 support and improved complex128 support for many ops\n* Higher level functionality in contrib.{layers,losses,metrics,learn}\n* More features to Tensorboard\n* Improved support for string embedding and sparse features\n* The RNN api is finally \"official\" (see, e.g., `tf.nn.dynamic_rnn`,\n  `tf.nn.rnn`, and the classes in `tf.nn.rnn_cell`).\n* TensorBoard now has an Audio Dashboard, with associated audio summaries.\n\n## Bug Fixes and Other Changes\n\n* Turned on CuDNN Autotune.\n* Added support for using third-party Python optimization algorithms (contrib.opt).\n* Google Cloud Storage filesystem support.\n* HDF5 support\n* Add support for 3d convolutions and pooling.\n* Update gRPC release to 0.14.\n* Eigen version upgrade.\n* Switch to eigen thread pool\n* `tf.nn.moments()` now accepts a `shift` argument. Shifting by a good estimate\n  of the mean improves numerical stability. Also changes the behavior of the\n  `shift` argument to `tf.nn.sufficient_statistics()`.\n* Performance improvements\n* Many bugfixes\n* Many documentation fixes\n* TensorBoard fixes: graphs with only one data point, Nan values,\n  reload button and auto-reload, tooltips in scalar charts, run\n  filtering, stable colors\n* Tensorboard graph visualizer now supports run metadata. Clicking on nodes\n  while viewing a stats for a particular run will show runtime statistics, such\n  as memory or compute usage. Unused nodes will be faded out.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAaron Schumacher, Aidan Dang, Akihiko ITOH, Aki Sukegawa, Arbit Chen, Aziz Alto, Danijar Hafner, Erik Erwitt, Fabrizio Milo, Felix Maximilian M\u00f6ller, Henry Saputra, Sung Kim, Igor Babuschkin, Jan Zikes, Jeremy Barnes, Jesper Steen M\u00f8ller, Johannes Mayer, Justin Harris, Kashif Rasul, Kevin Robinson, Loo Rong Jie, Lucas Moura, \u0141ukasz Bieniasz-Krzywiec, Mario Cho, Maxim Grechkin, Michael Heilman, Mostafa Rahmani, Mourad Mourafiq, @ninotoshi, Orion Reblitz-Richardson, Yuncheng Li, @raoqiyu, Robert DiPietro, Sam Abrahams, Sebastian Raschka, Siddharth Agrawal, @snakecharmer1024, Stephen Roller, Sung Kim, SunYeop Lee, Thijs Vogels, Till Hoffmann, Victor Melo, Ville Kallioniemi, Waleed Abdulla, Wenjian Huang, Yaroslav Bulatov, Yeison Rodriguez, Yuan Tang, Yuxin Wu, @zhongzyd, Ziming Dong, Zohar Jackson\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n# Release 0.8.0\n\n## Major Features and Improvements\n\n* Added a distributed runtime using GRPC\n* Move skflow to `contrib/learn`\n* Better linear optimizer in `contrib/linear_optimizer`\n* Random forest implementation in `contrib/tensor_forest`\n* CTC loss and decoders in `contrib/ctc`\n* Basic support for `half` data type\n* Better support for loading user ops (see examples in `contrib/`)\n* Allow use of (non-blocking) Eigen threadpool with `TENSORFLOW_USE_EIGEN_THREADPOOL` define\n* Add an extension mechanism for adding network file system support\n* TensorBoard displays metadata stats (running time, memory usage and device used) and tensor shapes\n\n## Bug Fixes and Other Changes\n\n* Utility for inspecting checkpoints\n* Basic tracing and timeline support\n* Allow building against cuDNN 5 (not incl. RNN/LSTM support)\n* Added instructions and binaries for ProtoBuf library with fast serialization and without 64MB limit\n* Added special functions\n* `bool`-strictness: Tensors have to be explicitly compared to `None`\n* Shape strictness: all fed values must have a shape that is compatible with the tensor they are replacing\n* Exposed `tf.while_loop` (deprecated `control_flow_ops.While`)\n* run() now takes RunOptions and RunMetadata, which enable timing stats\n* Fixed lots of potential overflow problems in op kernels\n* Various performance improvements, especially for RNNs and convolutions\n* Many bugfixes\n* Nightly builds, tutorial tests, many test improvements\n* New examples: transfer learning and deepdream ipython notebook\n* Added tutorials, many documentation fixes.\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAbhinav Upadhyay, Aggelos Avgerinos, Alan Wu, Alexander G. de G. Matthews, Aleksandr Yahnev, @amchercashin, Andy Kitchen, Aurelien Geron, Awni Hannun, @BanditCat, Bas Veeling, Cameron Chen, @cg31, Cheng-Lung Sung, Christopher Bonnett, Dan Becker, Dan Van Boxel, Daniel Golden, Danijar Hafner, Danny Goodman, Dave Decker, David Dao, David Kretch, Dongjoon Hyun, Dustin Dorroh, @e-lin, Eurico Doirado, Erik Erwitt, Fabrizio Milo, @gaohuazuo, Iblis Lin, Igor Babuschkin, Isaac Hodes, Isaac Turner, Iv\u00e1n Vall\u00e9s, J Yegerlehner, Jack Zhang, James Wexler, Jan Zikes, Jay Young, Jeff Hodges, @jmtatsch, Johnny Lim, Jonas Meinertz Hansen, Kanit Wongsuphasawat, Kashif Rasul, Ken Shirriff, Kenneth Mitchner, Kenta Yonekura, Konrad Magnusson, Konstantin Lopuhin, @lahwran, @lekaha, @liyongsea, Lucas Adams, @makseq, Mandeep Singh, @manipopopo, Mark Amery, Memo Akten, Michael Heilman, Michael Peteuil, Nathan Daly, Nicolas Fauchereau, @ninotoshi, Olav Nymoen, @panmari, @papelita1234, Pedro Lopes, Pranav Sailesh Mani, RJ Ryan, Rob Culliton, Robert DiPietro, @ronrest, Sam Abrahams, Sarath Shekkizhar, Scott Graham, Sebastian Raschka, Sung Kim, Surya Bhupatiraju, Syed Ahmed, Till Hoffmann, @timsl, @urimend, @vesnica, Vlad Frolov, Vlad Zagorodniy, Wei-Ting Kuo, Wenjian Huang, William Dmitri Breaden Madden, Wladimir Schmidt, Yuan Tang, Yuwen Yan, Yuxin Wu, Yuya Kusakabe, @zhongzyd, @znah.\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n\n# Release 0.7.1\n\n## Bug Fixes and Other Changes\n\n* Added gfile.Open and gfile.Copy, used by input_data.py.\n* Fixed Saver bug when MakeDirs tried to create empty directory.\n* GPU Pip wheels are built with cuda 7.5 and cudnn-v4, making them\n  required for the binary releases. Lower versions of cuda/cudnn can\n  be supported by installing from sources and setting the options\n  during ./configure\n* Fix dataset encoding example for Python3 (@danijar)\n* Fix PIP installation by not packaging protobuf as part of wheel,\n  require protobuf 3.0.0b2.\n* Fix Mac pip installation of numpy by requiring pip >= 1.10.1.\n* Improvements and fixes to Docker image.\n\n\n# Release 0.7.0\n\n## Major Features and Improvements\n\n* Allow using any installed Cuda >= 7.0 and cuDNN >= R2, and add support\n  for cuDNN R4\n* Added a `contrib/` directory for unsupported or experimental features,\n  including higher level `layers` module\n* Added an easy way to add and dynamically load user-defined ops\n* Built out a good suite of tests, things should break less!\n* Added `MetaGraphDef` which makes it easier to save graphs with metadata\n* Added assignments for \"Deep Learning with TensorFlow\" udacity course\n\n\n## Bug Fixes and Other Changes\n\n* Added a versioning framework for `GraphDef`s to ensure compatibility\n* Enforced Python 3 compatibility\n* Internal changes now show up as sensibly separated commits\n* Open-sourced the doc generator\n* Un-fork Eigen\n* Simplified the `BUILD` files and cleaned up C++ headers\n* TensorFlow can now be used as a submodule in another bazel build\n* New ops (e.g., `*fft`, `*_matrix_solve`)\n* Support for more data types in many ops\n* Performance improvements\n* Various bugfixes\n* Documentation fixes and improvements\n\n\n## Breaking Changes to the API\n\n* `AdjustContrast` kernel deprecated, new kernel `AdjustContrastv2` takes and\n  outputs float only. `adjust_contrast` now takes all data types.\n* `adjust_brightness`'s `delta` argument is now always assumed to be in `[0,1]`\n  (as is the norm for images in floating point formats), independent of the\n  data type of the input image.\n* The image processing ops do not take `min` and `max` inputs any more, casting\n  safety is handled by `saturate_cast`, which makes sure over- and underflows\n  are handled before casting to data types with smaller ranges.\n* For C++ API users: `IsLegacyScalar` and `IsLegacyVector` are now gone from\n  `TensorShapeUtils` since TensorFlow is scalar strict within Google (for\n  example, the shape argument to `tf.reshape` can't be a scalar anymore).  The\n  open source release was already scalar strict, so outside Google `IsScalar`\n  and `IsVector` are exact replacements.\n* The following files are being removed from `tensorflow/core/public/`:\n    * `env.h` -> `../platform/env.h`\n    * `status.h` -> `../lib/core/status.h`\n    * `tensor.h` -> `../framework/tensor.h`\n    * `tensor_shape.h` -> `../framework/tensor_shape.h`\n    * `partial_tensor_shape.h` -> `../framework/partial_tensor_shape.h`\n    * `tensorflow_server.h` deleted\n* For C++ API users: `TensorShape::ShortDebugString` has been renamed to\n  `DebugString`, and the previous `DebugString` behavior is gone (it was\n  needlessly verbose and produced a confusing empty string for scalars).\n* `GraphOptions.skip_common_subexpression_elimination` has been removed. All\n  graph optimizer options are now specified via\n  `GraphOptions.OptimizerOptions`.\n* `ASSERT_OK` / `EXPECT_OK` macros conflicted with external projects, so they\n  were renamed `TF_ASSERT_OK`, `TF_EXPECT_OK`.  The existing macros are\n  currently maintained for short-term compatibility but will be removed.\n* The non-public `nn.rnn` and the various `nn.seq2seq` methods now return\n  just the final state instead of the list of all states.\n* `tf.scatter_update` now no longer guarantees that lexicographically largest\n  index be used for update when duplicate entries exist.\n* `tf.image.random_crop(image, [height, width])` is now\n  `tf.random_crop(image, [height, width, depth])`, and `tf.random_crop` works\n  for any rank (not just 3-D images).  The C++ `RandomCrop` op has been replaced\n  with pure Python.\n* Renamed `tf.test.GetTempDir` and `tf.test.IsBuiltWithCuda` to\n  `tf.test.get_temp_dir` and `tf.test.is_built_with_cuda` for PEP-8\n  compatibility.\n* `parse_example`'s interface has changed, the old interface is accessible in\n  `legacy_parse_example` (same for related functions).\n* New `Variable`s are not added to the same collection several times even if\n  a list with duplicates is passed to the constructor.\n* The Python API will now properly set the `list` member of `AttrValue` in\n  constructed `GraphDef` messages for empty lists.  The serialization of some\n  graphs will change, but the change is both forwards and backwards compatible.\n  It will break tests that compare a generated `GraphDef` to a golden serialized\n  `GraphDef` (which is discouraged).\n\n\n## Thanks to our Contributors\n\nThis release contains contributions from many people at Google, as well as:\n\nAkiomi Kamakura, Alex Vig, Alexander Rosenberg Johansen, Andre Cruz, Arun Ahuja,\nBart Coppens, Bernardo Pires, Carl Vondrick, Cesar Salgado, Chen Yu,\nChristian Jauvin, Damien Aymeric, Dan Vanderkam, Denny Britz, Dongjoon Hyun,\nEren G\u00fcven, Erik Erwitt, Fabrizio Milo, G. Hussain Chinoy, Jim Fleming,\nJoao Felipe Santos, Jonas Meinertz Hansen, Joshi Rekha, Julian Viereck,\nKeiji Ariyama, Kenton Lee, Krishna Sankar, Kristina Chodorow, Linchao Zhu,\nLukas Krecan, Mark Borgerding, Mark Daoust, Moussa Taifi,\nNathan Howell, Naveen Sundar Govindarajulu, Nick Sweeting, Niklas Riekenbrauck,\nOlivier Grisel, Patrick Christ, Povilas Liubauskas, Rainer Wasserfuhr,\nRomain Thouvenin, Sagan Bolliger, Sam Abrahams, Taehoon Kim, Timothy J Laurent,\nVlad Zavidovych, Yangqing Jia, Yi-Lin Juang, Yuxin Wu, Zachary Lipton,\nZero Chen, Alan Wu, @brchiu, @emmjaykay, @jalammar, @Mandar-Shinde,\n@nsipplswezey, @ninotoshi, @panmari, @prolearner and @rizzomichaelg.\n\nWe are also grateful to all who filed issues or helped resolve them, asked and\nanswered questions, and were part of inspiring discussions.\n\n\n# Release 0.6.0\n\n## Major Features and Improvements\n\n* Python 3.3+ support via changes to python codebase and ability\n  to specify python version via ./configure.\n\n* Some improvements to GPU performance and memory usage:\n  [convnet benchmarks](https://github.com/soumith/convnet-benchmarks/issues/66)\n  roughly equivalent with native cudnn v2 performance.  Improvements mostly due\n  to moving to 32-bit indices, faster shuffling kernels.  More improvements to\n  come in later releases.\n\n\n## Bug Fixes\n\n* Lots of fixes to documentation and tutorials, many contributed\n  by the public.\n\n* 271 closed issues on github issues.\n\n## Backwards-Incompatible Changes\n\n* `tf.nn.fixed_unigram_candidate_sampler` changed its default 'distortion'\n  attribute from 0.0 to 1.0. This was a bug in the original release\n  that is now fixed.\n\n# Release 0.5.0\n\nInitial release of TensorFlow.\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: disable=protected-access\n\"\"\"A `Network` is way to compose layers: the topological form of a `Model`.\"\"\"\n\nimport collections\nimport copy\nimport itertools\nimport warnings\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.keras.engine import base_layer_utils\nfrom tensorflow.python.keras.engine import input_layer as input_layer_module\nfrom tensorflow.python.keras.engine import input_spec\nfrom tensorflow.python.keras.engine import node as node_module\nfrom tensorflow.python.keras.engine import training as training_lib\nfrom tensorflow.python.keras.engine import training_utils\nfrom tensorflow.python.keras.saving.saved_model import network_serialization\nfrom tensorflow.python.keras.utils import generic_utils\nfrom tensorflow.python.keras.utils import tf_inspect\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training.tracking import base as trackable\nfrom tensorflow.python.util import nest\nfrom tensorflow.tools.docs import doc_controls\n\n\n# pylint: disable=g-classes-have-attributes\nclass Functional(training_lib.Model):\n  \"\"\"A `Functional` model is a `Model` defined as a directed graph of layers.\n\n  Three types of `Model` exist: subclassed `Model`, `Functional` model,\n  and `Sequential` (a special case of `Functional`).\n  In general, more Keras features are supported with `Functional`\n  than with subclassed `Model`s, specifically:\n\n  - Model cloning (`keras.models.clone`)\n  - Serialization (`model.get_config()/from_config`, `model.to_json()`\n  - Whole-model saving (`model.save()`)\n\n  A `Functional` model can be instantiated by passing two arguments to\n  `__init__`. The first argument is the `keras.Input` Tensors that represent\n  the inputs to the model. The second argument specifies the output\n  tensors that represent the outputs of this model. Both arguments can be a\n  nested structure of tensors.\n\n  Example:\n\n  ```\n  inputs = {'x1': keras.Input(shape=(10,)), 'x2': keras.Input(shape=(1,))}\n  t = keras.layers.Dense(1, activation='relu')(inputs['x1'])\n  outputs = keras.layers.Add()([t, inputs['x2'])\n  model = keras.Model(inputs, outputs)\n  ```\n\n  A `Functional` model constructed using the Functional API can also include raw\n  TensorFlow functions, with the exception of functions that create Variables\n  or assign ops.\n\n  Example:\n\n  ```\n  inputs = keras.Input(shape=(10,))\n  x = keras.layers.Dense(1)(inputs)\n  outputs = tf.nn.relu(x)\n  model = keras.Model(inputs, outputs)\n  ```\n\n  Args:\n    inputs: List of input tensors (must be created via `tf.keras.Input()`).\n    outputs: List of output tensors.\n    name: String, optional. Name of the model.\n    trainable: Boolean, optional. If the model's variables should be trainable.\n  \"\"\"\n\n  # See tf.Module for the usage of this property.\n  # The key of _layer_call_argspecs is a layer. tf.Module._flatten will fail to\n  # flatten the key since it is trying to convert Trackable/Layer to a string.\n  _TF_MODULE_IGNORED_PROPERTIES = frozenset(itertools.chain(\n      ('_layer_call_argspecs', '_compiled_trainable_state',\n       '_output_mask_cache', '_output_tensor_cache', '_output_shape_cache'),\n      training_lib.Model._TF_MODULE_IGNORED_PROPERTIES\n  ))\n\n  @trackable.no_automatic_dependency_tracking\n  def __init__(self, inputs, outputs, name=None, trainable=True,\n               **kwargs):\n    # This is used by the Model class, since we have some logic to swap the\n    # class in the __new__ method, which will lead to __init__ get invoked\n    # twice. Using the skip_init to skip one of the invocation of __init__ to\n    # avoid any side effects\n    skip_init = kwargs.pop('skip_init', False)\n    if skip_init:\n      return\n    generic_utils.validate_kwargs(kwargs, {})\n    super(Functional, self).__init__(name=name, trainable=trainable)\n    self._init_graph_network(inputs, outputs)\n\n  @trackable.no_automatic_dependency_tracking\n  def _init_graph_network(self, inputs, outputs):\n    # This method is needed for Sequential to reinitialize graph network when\n    # layer is added or removed.\n    self._is_graph_network = True\n\n    # Normalize and set self.inputs, self.outputs.\n    if isinstance(inputs, list) and len(nest.flatten(inputs)) == 1:\n      inputs = inputs[0]\n    if isinstance(outputs, list) and len(nest.flatten(outputs)) == 1:\n      outputs = outputs[0]\n    self._nested_inputs = inputs\n    self._nested_outputs = outputs\n    self.inputs = nest.flatten(inputs)\n    self.outputs = nest.flatten(outputs)\n\n    # Models constructed with a single Tensor or list of Tensors can\n    # be called with a dict, where the keys of the dict are the names\n    # of the `Input` objects. Extra keys are ignored with warning.\n    if not nest.is_nested(self._nested_inputs):\n      self._enable_dict_to_input_mapping = True\n    elif (isinstance(self._nested_inputs, (list, tuple)) and\n          not any(nest.is_nested(t) for t in self._nested_inputs)):\n      self._enable_dict_to_input_mapping = True\n    elif (isinstance(self._nested_inputs, dict) and\n          not any(nest.is_nested(t) for t in self._nested_inputs.values())):\n      self._enable_dict_to_input_mapping = True\n    else:\n      self._enable_dict_to_input_mapping = False\n\n    if not ops.executing_eagerly_outside_functions():\n      if any(not hasattr(tensor, '_keras_history') for tensor in self.outputs):\n        base_layer_utils.create_keras_history(self._nested_outputs)\n\n    self._validate_graph_inputs_and_outputs()\n\n    # A Network does not create weights of its own, thus it is already\n    # built.\n    self.built = True\n    self._build_input_shape = nest.map_structure(lambda x: x.shape, inputs)\n    self._compute_output_and_mask_jointly = True\n    # `_expects_training_arg` is True since the `training` argument is always\n    # present in the signature of the `call` method of a graph network.\n    self._expects_training_arg = True\n    self._expects_mask_arg = True\n    # A graph network does not autocast inputs, as its layers will cast them\n    # instead.\n    self._autocast = False\n\n    self._input_layers = []\n    self._output_layers = []\n    self._input_coordinates = []\n    self._output_coordinates = []\n\n    # This is for performance optimization when calling the Network on new\n    # inputs. Every time the Network is called on a set on input tensors,\n    # we compute the output tensors, output masks and output shapes in one pass,\n    # then cache them here. When any of these outputs is queried later, we\n    # retrieve it from there instead of recomputing it.\n    self._output_mask_cache = {}\n    self._output_tensor_cache = {}\n    self._output_shape_cache = {}\n\n    # Build self._output_layers:\n    for x in self.outputs:\n      layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access\n      self._output_layers.append(layer)\n      self._output_coordinates.append((layer, node_index, tensor_index))\n\n    # Build self._input_layers:\n    for x in self.inputs:\n      layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access\n      # It's supposed to be an input layer, so only one node\n      # and one tensor output.\n      assert node_index == 0\n      assert tensor_index == 0\n      self._input_layers.append(layer)\n      self._input_coordinates.append((layer, node_index, tensor_index))\n\n    # Keep track of the network's nodes and layers.\n    nodes, nodes_by_depth, layers, _ = _map_graph_network(\n        self.inputs, self.outputs)\n    self._network_nodes = nodes\n    self._nodes_by_depth = nodes_by_depth\n    self._self_tracked_trackables = layers\n    self._layer_call_argspecs = {}\n    for layer in self._self_tracked_trackables:\n      self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)\n\n    # Build self.input_names and self.output_names.\n    self._set_output_names()\n    self.input_names = []\n    self._feed_input_names = []\n    self._feed_inputs = []\n    self._feed_input_shapes = []\n    for layer in self._input_layers:\n      self.input_names.append(layer.name)\n      if layer.is_placeholder:\n        self._feed_input_names.append(layer.name)\n        # Use batch_input_shape here because non-eager composite tensors may not\n        # have a shape attribute that's meaningful (sparse, for instance, has\n        # a tensor that's non-constant and needs to be fed). This means that\n        # input layers that create placeholders will need to have the\n        # batch_input_shape attr to allow for input shape validation.\n        self._feed_input_shapes.append(layer._batch_input_shape)\n        self._feed_inputs.append(layer.input)\n\n    self._compute_tensor_usage_count()\n    self._set_save_spec(self._nested_inputs)\n    tf_utils.assert_no_legacy_layers(self.layers)\n\n  @property\n  def input(self):\n    \"\"\"Retrieves the input tensor(s) of a layer.\n\n    Only applicable if the layer has exactly one input,\n    i.e. if it is connected to one incoming layer.\n\n    Returns:\n        Input tensor or list of input tensors.\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n      AttributeError: If no inbound nodes are found.\n    \"\"\"\n    return self._nested_inputs\n\n  @property\n  def input_shape(self):\n    \"\"\"Retrieves the input shape(s) of a layer.\n\n    Only applicable if the layer has exactly one input,\n    i.e. if it is connected to one incoming layer, or if all inputs\n    have the same shape.\n\n    Returns:\n        Input shape, as an integer shape tuple\n        (or list of shape tuples, one tuple per input tensor).\n\n    Raises:\n        AttributeError: if the layer has no defined input_shape.\n        RuntimeError: if called in Eager mode.\n    \"\"\"\n    return nest.map_structure(backend.int_shape, self.input)\n\n  @property\n  def input_spec(self):\n    if hasattr(self, '_manual_input_spec'):\n      return self._manual_input_spec\n    if (isinstance(self._nested_inputs, (dict, list, tuple)) and\n        len(self._nested_inputs) != len(self.inputs)):\n      # Case where we have a nested structure.\n      # In such a case we can't safely run any checks.\n      return None\n    if isinstance(self._nested_inputs, dict):\n      # Case where `_nested_inputs` is a plain dict of Inputs.\n      names = sorted(self._nested_inputs.keys())\n      return [input_spec.InputSpec(\n          shape=shape_with_no_batch_size(self._nested_inputs[name]),\n          allow_last_axis_squeeze=True, name=name) for name in names]\n    else:\n      # Single input, or list / tuple of inputs.\n      # The data may be passed as a dict keyed by input name.\n      return [input_spec.InputSpec(\n          shape=shape_with_no_batch_size(x), allow_last_axis_squeeze=True,\n          name=x._keras_history.layer.name) for x in self.inputs]\n\n  @input_spec.setter\n  def input_spec(self, value):\n    self._manual_input_spec = value\n\n  @property\n  def output(self):\n    \"\"\"Retrieves the output tensor(s) of a layer.\n\n    Only applicable if the layer has exactly one output,\n    i.e. if it is connected to one incoming layer.\n\n    Returns:\n      Output tensor or list of output tensors.\n\n    Raises:\n      AttributeError: if the layer is connected to more than one incoming\n        layers.\n      RuntimeError: if called in Eager mode.\n    \"\"\"\n    return self._nested_outputs\n\n  @property\n  def output_shape(self):\n    \"\"\"Retrieves the output shape(s) of a layer.\n\n    Only applicable if the layer has one output,\n    or if all outputs have the same shape.\n\n    Returns:\n        Output shape, as an integer shape tuple\n        (or list of shape tuples, one tuple per output tensor).\n\n    Raises:\n        AttributeError: if the layer has no defined output shape.\n        RuntimeError: if called in Eager mode.\n    \"\"\"\n    return nest.map_structure(backend.int_shape, self.output)\n\n  def _set_output_names(self):\n    \"\"\"Assigns unique names to the Network's outputs.\n\n    Output layers with multiple output tensors would otherwise lead to duplicate\n    names in self.output_names.\n    \"\"\"\n    uniquified = []\n    output_names = set()\n    prefix_count = {}\n    for layer in self._output_layers:\n      proposal = layer.name\n      while proposal in output_names:\n        existing_count = prefix_count.get(layer.name, 1)\n        proposal = '{}_{}'.format(layer.name, existing_count)\n        prefix_count[layer.name] = existing_count + 1\n      output_names.add(proposal)\n      uniquified.append(proposal)\n    self.output_names = uniquified\n\n  @property\n  def _layer_checkpoint_dependencies(self):\n    \"\"\"Dictionary of layer dependencies to be included in the checkpoint.\"\"\"\n    weight_layer_index = 0\n\n    dependencies = collections.OrderedDict()\n    for layer_index, layer in enumerate(self.layers):\n      try:\n        if layer.weights:\n          # Keep a separate index for layers which have weights. This allows\n          # users to insert Layers without weights anywhere in the network\n          # without breaking checkpoints.\n          dependencies['layer_with_weights-%d' % weight_layer_index] = layer\n          weight_layer_index += 1\n      except ValueError:\n        # The layer might have weights, but may not be built yet. We just treat\n        # it as layer without weight.\n        pass\n\n      # Even if it doesn't have weights, we should still track everything in\n      # case it has/will have Trackable dependencies.\n      dependencies['layer-%d' % layer_index] = layer\n    return dependencies\n\n  @property\n  def _checkpoint_dependencies(self):\n    dependencies = [\n        trackable.TrackableReference(name=name, ref=layer)\n        for name, layer in self._layer_checkpoint_dependencies.items()]\n    dependencies.extend(super(Functional, self)._checkpoint_dependencies)\n    return dependencies\n\n  def _lookup_dependency(self, name):\n    layer_dependencies = self._layer_checkpoint_dependencies\n    if name in layer_dependencies:\n      return layer_dependencies[name]\n    return super(Functional, self)._lookup_dependency(name)\n\n  def _handle_deferred_layer_dependencies(self, layers):\n    \"\"\"Handles layer checkpoint dependencies that are added after init.\"\"\"\n    layer_checkpoint_dependencies = self._layer_checkpoint_dependencies\n    layer_to_name = {v: k for k, v in layer_checkpoint_dependencies.items()}\n    for layer in layers:\n      if layer in layer_to_name:\n        self._handle_deferred_dependencies(name=layer_to_name[layer],\n                                           trackable=layer)\n\n  @property\n  def _should_compute_mask(self):\n    return True\n\n  def compute_mask(self, inputs, mask):\n    # TODO(omalleyt): b/123540974 This function is not really safe to call\n    # by itself because it will duplicate any updates and losses in graph\n    # mode by `call`ing the Layers again.\n    output_tensors = self._run_internal_graph(inputs, mask=mask)\n    return nest.map_structure(lambda t: getattr(t, '_keras_mask', None),\n                              output_tensors)\n\n  @doc_controls.do_not_doc_inheritable\n  def call(self, inputs, training=None, mask=None):\n    \"\"\"Calls the model on new inputs.\n\n    In this case `call` just reapplies\n    all ops in the graph to the new inputs\n    (e.g. build a new computational graph from the provided inputs).\n\n    Args:\n        inputs: A tensor or list of tensors.\n        training: Boolean or boolean scalar tensor, indicating whether to run\n          the `Network` in training mode or inference mode.\n        mask: A mask or list of masks. A mask can be\n            either a tensor or None (no mask).\n\n    Returns:\n        A tensor if there is a single output, or\n        a list of tensors if there are more than one outputs.\n    \"\"\"\n    return self._run_internal_graph(\n        inputs, training=training, mask=mask)\n\n  def compute_output_shape(self, input_shape):\n    # Convert any shapes in tuple format to TensorShapes.\n    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n\n    if len(nest.flatten(input_shape)) != len(nest.flatten(self._input_layers)):\n      raise ValueError('Invalid input_shape argument ' + str(input_shape) +\n                       ': model has ' + str(len(self._input_layers)) +\n                       ' tensor inputs.')\n\n    # Use the tuple of TensorShape as the cache key, since tuple is hashable\n    # and can be used as hash key.\n    try:\n      cache_key = tuple(tf_utils.convert_shapes(input_shape, to_tuples=True))\n      if cache_key in self._output_shape_cache:\n        # Cache hit. Return shapes as TensorShapes.\n        return self._output_shape_cache[cache_key]\n    except ValueError:\n      # In case there are unknown TensorShape, eg for sparse tensor input,\n      # We skip the caching since the shape is unknown.\n      pass\n\n    layers_to_output_shapes = {}\n    for layer, shape in zip(self._input_layers, nest.flatten(input_shape)):\n      # It's an input layer: then `compute_output_shape` is identity,\n      # and there is only one node and one tensor..\n      shape_key = layer.name + '_0_0'\n      layers_to_output_shapes[shape_key] = shape\n\n    depth_keys = list(self._nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n    # Iterate over nodes, by depth level.\n    if len(depth_keys) > 1:\n      for depth in depth_keys:\n        nodes = self._nodes_by_depth[depth]\n        for node in nodes:\n          layer = node.layer\n          if layer in self._input_layers:\n            # We've already covered the input layers\n            # a few lines above.\n            continue\n          # Get the input shapes for the first argument of the node\n          layer_input_shapes = []\n          layer_inputs = node.call_args[0]\n          for layer_input in nest.flatten(layer_inputs):\n            kh = layer_input._keras_history\n            input_layer_key = kh.layer.name + '_%s_%s' % (kh.node_index,\n                                                          kh.tensor_index)\n            layer_input_shapes.append(layers_to_output_shapes[input_layer_key])\n          layer_input_shapes = nest.pack_sequence_as(layer_inputs,\n                                                     layer_input_shapes)\n          # Layers expect shapes to be tuples for `compute_output_shape`.\n          layer_input_shapes = tf_utils.convert_shapes(\n              layer_input_shapes, to_tuples=True)\n          layer_output_shapes = layer.compute_output_shape(layer_input_shapes)\n          # Convert back to TensorShapes.\n          layer_output_shapes = tf_utils.convert_shapes(\n              layer_output_shapes, to_tuples=False)\n\n          node_index = layer._inbound_nodes.index(node)  # pylint: disable=protected-access\n          for j, shape in enumerate(nest.flatten(layer_output_shapes)):\n            shape_key = layer.name + '_%s_%s' % (node_index, j)\n            layers_to_output_shapes[shape_key] = shape\n\n      # Read final output shapes from layers_to_output_shapes.\n      output_shapes = []\n      for i in range(len(self._output_layers)):\n        layer, node_index, tensor_index = self._output_coordinates[i]\n        shape_key = layer.name + '_%s_%s' % (node_index, tensor_index)\n        output_shapes.append(layers_to_output_shapes[shape_key])\n      output_shapes = nest.pack_sequence_as(self._nested_outputs, output_shapes)\n      # Store in cache.\n      self._output_shape_cache[cache_key] = output_shapes\n\n    # Return shapes as TensorShapes.\n    return output_shapes\n\n  def _init_set_name(self, name, zero_based=True):\n    if not name:\n      cls_name = self.__class__.__name__\n      if self.__class__ == Functional:\n        # Hide the functional class name from user, since its not a public\n        # visible class. Use \"Model\" instead,\n        cls_name = 'Model'\n      self._name = backend.unique_object_name(\n          generic_utils.to_snake_case(cls_name),\n          zero_based=zero_based)\n    else:\n      self._name = name\n\n  def _run_internal_graph(self, inputs, training=None, mask=None):\n    \"\"\"Computes output tensors for new inputs.\n\n    # Note:\n        - Can be run on non-Keras tensors.\n\n    Args:\n        inputs: Tensor or nested structure of Tensors.\n        training: Boolean learning phase.\n        mask: (Optional) Tensor or nested structure of Tensors.\n\n    Returns:\n        output_tensors\n    \"\"\"\n    inputs = self._flatten_to_reference_inputs(inputs)\n    if mask is None:\n      masks = [None] * len(inputs)\n    else:\n      masks = self._flatten_to_reference_inputs(mask)\n    for input_t, mask in zip(inputs, masks):\n      input_t._keras_mask = mask\n\n    # Dictionary mapping reference tensors to computed tensors.\n    tensor_dict = {}\n    tensor_usage_count = self._tensor_usage_count\n    for x, y in zip(self.inputs, inputs):\n      y = self._conform_to_reference_input(y, ref_input=x)\n      x_id = str(id(x))\n      tensor_dict[x_id] = [y] * tensor_usage_count[x_id]\n\n    nodes_by_depth = self._nodes_by_depth\n    depth_keys = list(nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n\n    for depth in depth_keys:\n      nodes = nodes_by_depth[depth]\n      for node in nodes:\n        if node.is_input:\n          continue  # Input tensors already exist.\n\n        if any(t_id not in tensor_dict for t_id in node.flat_input_ids):\n          continue  # Node is not computable, try skipping.\n\n        args, kwargs = node.map_arguments(tensor_dict)\n        outputs = node.layer(*args, **kwargs)\n\n        # Update tensor_dict.\n        for x_id, y in zip(node.flat_output_ids, nest.flatten(outputs)):\n          tensor_dict[x_id] = [y] * tensor_usage_count[x_id]\n\n    output_tensors = []\n    for x in self.outputs:\n      x_id = str(id(x))\n      assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n      output_tensors.append(tensor_dict[x_id].pop())\n\n    return nest.pack_sequence_as(self._nested_outputs, output_tensors)\n\n  def _flatten_to_reference_inputs(self, tensors):\n    \"\"\"Maps `tensors` to their respective `keras.Input`.\"\"\"\n    if self._enable_dict_to_input_mapping and isinstance(tensors, dict):\n      ref_inputs = self._nested_inputs\n      if not nest.is_nested(ref_inputs):\n        ref_inputs = [self._nested_inputs]\n      if isinstance(ref_inputs, dict):\n        # In the case that the graph is constructed with dict input tensors,\n        # We will use the original dict key to map with the keys in the input\n        # data. Note that the model.inputs is using nest.flatten to process the\n        # input tensors, which means the dict input tensors are ordered by their\n        # keys.\n        ref_input_names = sorted(ref_inputs.keys())\n      else:\n        ref_input_names = [inp._keras_history.layer.name for inp in ref_inputs]\n\n      # Raise an warning if there are more input data comparing to input tensor\n      if len(tensors) > len(ref_input_names):\n        warnings.warn(\n            'Input dict contained keys {} which did not match any model input. '\n            'They will be ignored by the model.'.format(\n                [n for n in tensors.keys() if n not in ref_input_names])\n            )\n\n      try:\n        # Flatten in the order `Input`s were passed during Model construction.\n        return [tensors[n] for n in ref_input_names]\n      except KeyError:\n        # TODO(b/151582614)\n        return nest.flatten(tensors)\n\n    # Otherwise both self.inputs and tensors will already be in same order.\n    return nest.flatten(tensors)\n\n  def _conform_to_reference_input(self, tensor, ref_input):\n    \"\"\"Set shape and dtype based on `keras.Input`s.\"\"\"\n    if isinstance(tensor, ops.Tensor):\n      # Allow (None,) and (None, 1) Tensors to be passed interchangeably. Use\n      # the shape specified by the `keras.Input`.\n      t_shape = tensor.shape\n      t_rank = t_shape.rank\n      ref_shape = ref_input.shape\n      ref_rank = ref_shape.rank\n      keras_history = getattr(tensor, '_keras_history', None)\n      if t_rank is not None and ref_rank is not None:\n        # Should squeeze last dimension.\n        # True if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).\n        if (t_rank == ref_rank + 1 and t_shape[-1] == 1):\n          tensor = array_ops.squeeze_v2(tensor, axis=-1)\n        # Should expand last_dimension.\n        # True if tensor is (BATCH, ...) and reference is (BATCH, ..., 1).\n        elif (t_rank == ref_rank - 1 and ref_shape[-1] == 1):\n          tensor = array_ops.expand_dims_v2(tensor, axis=-1)\n      if keras_history is not None:  # Restore keras history.\n        tensor._keras_history = keras_history\n\n      # Add shape hints to Tensors that may have None shape dims but have shapes\n      # defined by the `keras.Input` (not applicable in eager mode).\n      if not context.executing_eagerly():\n        try:\n          tensor.set_shape(tensor.shape.merge_with(ref_input.shape))\n        except ValueError:\n          logging.warning(\n              'Model was constructed with shape {} for input {}, but it was '\n              'called on an input with incompatible shape {}.'.format(\n                  ref_input.shape, ref_input, tensor.shape))\n\n      # Dtype casting.\n      tensor = math_ops.cast(tensor, dtype=ref_input.dtype)\n    elif tf_utils.is_extension_type(tensor):\n      # Dtype casting (If the extension type has a non-variant dtype and\n      # supports being cast)\n      ref_input_dtype = getattr(ref_input, 'dtype', None)\n      if ref_input_dtype is not None and ref_input_dtype != dtypes.variant:\n        tensor = math_ops.cast(tensor, dtype=ref_input_dtype)\n\n    return tensor\n\n  def get_config(self):\n    return copy.deepcopy(get_network_config(self))\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None):\n    \"\"\"Instantiates a Model from its config (output of `get_config()`).\n\n    Args:\n        config: Model config dictionary.\n        custom_objects: Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    Returns:\n        A model instance.\n\n    Raises:\n        ValueError: In case of improperly formatted config dict.\n    \"\"\"\n    with generic_utils.SharedObjectLoadingScope():\n      input_tensors, output_tensors, created_layers = reconstruct_from_config(\n          config, custom_objects)\n      model = cls(inputs=input_tensors, outputs=output_tensors,\n                  name=config.get('name'))\n      connect_ancillary_layers(model, created_layers)\n      return model\n\n  def _validate_graph_inputs_and_outputs(self):\n    \"\"\"Validates the inputs and outputs of a Graph Network.\"\"\"\n    # Check for redundancy in inputs.\n    if len({id(i) for i in self.inputs}) != len(self.inputs):\n      raise ValueError('The list of inputs passed to the model '\n                       'is redundant. '\n                       'All inputs should only appear once.'\n                       ' Found: ' + str(self.inputs))\n\n    for x in self.inputs:\n      # Check that x has appropriate `_keras_history` metadata.\n      if not hasattr(x, '_keras_history'):\n        cls_name = self.__class__.__name__\n        raise ValueError('Input tensors to a ' + cls_name + ' ' +\n                         'must come from `tf.keras.Input`. '\n                         'Received: ' + str(x) +\n                         ' (missing previous layer metadata).')\n      # Check that x is an input tensor.\n      # pylint: disable=protected-access\n      layer = x._keras_history.layer\n      if len(layer._inbound_nodes) > 1 or (\n          layer._inbound_nodes and not layer._inbound_nodes[0].is_input):\n        cls_name = self.__class__.__name__\n        logging.warning(cls_name + ' model inputs must come from '\n                        '`tf.keras.Input` (thus holding past layer metadata), '\n                        'they cannot be the output of '\n                        'a previous non-Input layer. '\n                        'Here, a tensor specified as '\n                        'input to \"' + self.name + '\" was not an Input tensor, '\n                        'it was generated by layer ' + layer.name + '.\\n'\n                        'Note that input tensors are '\n                        'instantiated via `tensor = tf.keras.Input(shape)`.\\n'\n                        'The tensor that caused the issue was: ' + str(x.name))\n\n    # Check compatibility of batch sizes of Input Layers.\n    input_batch_sizes = [\n        training_utils.get_static_batch_size(x._keras_history.layer)\n        for x in self.inputs\n    ]\n    consistent_batch_size = None\n    for batch_size in input_batch_sizes:\n      if batch_size is not None:\n        if (consistent_batch_size is not None and\n            batch_size != consistent_batch_size):\n          raise ValueError('The specified batch sizes of the Input Layers'\n                           ' are incompatible. Found batch sizes: {}'.format(\n                               input_batch_sizes))\n        consistent_batch_size = batch_size\n\n    for x in self.outputs:\n      if not hasattr(x, '_keras_history'):\n        cls_name = self.__class__.__name__\n        raise ValueError('Output tensors of a ' + cls_name + ' model must be '\n                         'the output of a TensorFlow `Layer` '\n                         '(thus holding past layer metadata). Found: ' + str(x))\n\n  def _insert_layers(self, layers, relevant_nodes=None):\n    \"\"\"Inserts Layers into the Network after Network creation.\n\n    This is only valid for Keras Graph Networks.  Layers added via this function\n    will be included in the `call` computation and `get_config` of this Network.\n    They will not be added to the Network's outputs.\n\n\n    Args:\n      layers: Arbitrary nested structure of Layers. Layers must be reachable\n        from one or more of the `keras.Input` Tensors that correspond to this\n        Network's inputs.\n      relevant_nodes: Nodes from the Layers that should be considered part of\n        this Network. If `None`, all Nodes will be considered part of this\n        Network.\n\n    Raises:\n      ValueError: If the layers depend on `Input`s not found in this Model.\n    \"\"\"\n    layers = nest.flatten(layers)\n    tf_utils.assert_no_legacy_layers(layers)\n    node_to_depth = {}\n    for depth, nodes in self._nodes_by_depth.items():\n      node_to_depth.update({node: depth for node in nodes})\n    # The nodes of these Layers that are relevant to this Network. If not\n    # provided, assume all Nodes are relevant\n    if not relevant_nodes:\n      relevant_nodes = nest.flatten([layer._inbound_nodes for layer in layers])\n    network_nodes = set(relevant_nodes + list(node_to_depth.keys()))\n\n    def _get_min_depth(node):\n      \"\"\"Gets the minimum depth at which node can be computed.\"\"\"\n      min_depth = 0\n      for layer, node_id, _, _ in node.iterate_inbound():\n        inbound_node = layer._inbound_nodes[node_id]\n        if inbound_node in node_to_depth:\n          min_depth = min(min_depth, node_to_depth[inbound_node])\n        elif inbound_node not in network_nodes:\n          continue\n        else:\n          # Previous relevant nodes haven't been processed yet.\n          return None\n      # New node is one shallower than its shallowest input.\n      return min_depth - 1\n\n    # Insert nodes into `_nodes_by_depth` and other node attrs.\n    unprocessed_nodes = copy.copy(relevant_nodes)\n    i = 0\n    while unprocessed_nodes:\n      i += 1\n      # Do a sanity check. This can occur if `Input`s from outside this Model\n      # are being relied on.\n      if i > 10000:\n        raise ValueError('Layers could not be added due to missing '\n                         'dependencies.')\n\n      node = unprocessed_nodes.pop(0)\n      depth = _get_min_depth(node)\n      if depth is None:  # Defer until inbound nodes are processed.\n        unprocessed_nodes.append(node)\n        continue\n      node_key = _make_node_key(node.layer.name,\n                                node.layer._inbound_nodes.index(node))\n      if node_key not in self._network_nodes:\n        node_to_depth[node] = depth\n        self._network_nodes.add(node_key)\n        self._nodes_by_depth[depth].append(node)\n\n    # Insert layers and update other layer attrs.\n    layer_set = set(self._self_tracked_trackables)\n    deferred_layers = []\n    for layer in layers:\n      if layer not in layer_set:\n        self._self_tracked_trackables.append(layer)\n        deferred_layers.append(layer)\n        self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)\n        layer_set.add(layer)\n    self._handle_deferred_layer_dependencies(deferred_layers)\n\n    self._compute_tensor_usage_count()\n\n  def _compute_tensor_usage_count(self):\n    \"\"\"Compute the #. of tensor usages for all the output tensors of layers.\n\n    The computed tensor usage count is saved as `self._tensor_usage_count`. This\n    is later used for saving memory in eager computation by releasing\n    no-longer-needed tensors as early as possible.\n    \"\"\"\n    tensor_usage_count = collections.Counter()\n    available_tensors = set(str(id(tensor)) for tensor in self.inputs)\n\n    depth_keys = list(self._nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n    depth_keys = depth_keys[1:]\n\n    for depth in depth_keys:\n      for node in self._nodes_by_depth[depth]:\n        input_tensors = {\n            str(id(tensor)) for tensor in nest.flatten(node.keras_inputs)\n        }\n        if input_tensors.issubset(available_tensors):\n          for tensor in nest.flatten(node.keras_inputs):\n            tensor_usage_count[str(id(tensor))] += 1\n\n          for output_tensor in nest.flatten(node.outputs):\n            available_tensors.add(str(id(output_tensor)))\n\n    for tensor in self.outputs:\n      tensor_usage_count[str(id(tensor))] += 1\n\n    self._tensor_usage_count = tensor_usage_count\n\n  def _assert_weights_created(self):\n    # Override the implementation in Model.\n    # The Functional model should always have weight created already.\n    return\n\n  def _graph_network_add_loss(self, symbolic_loss):\n    new_nodes, new_layers = _map_subgraph_network(self.inputs, [symbolic_loss])\n    # Losses must be keyed on inputs no matter what in order to be supported in\n    # DistributionStrategy.\n    add_loss_layer = base_layer.AddLoss(\n        unconditional=False, dtype=symbolic_loss.dtype)\n    add_loss_layer(symbolic_loss)\n    new_nodes.extend(add_loss_layer.inbound_nodes)\n    new_layers.append(add_loss_layer)\n    self._insert_layers(new_layers, new_nodes)\n\n  def _graph_network_add_metric(self, value, aggregation, name):\n    new_nodes, new_layers = _map_subgraph_network(self.inputs, [value])\n    add_metric_layer = base_layer.AddMetric(\n        aggregation, name, dtype=value.dtype)\n    add_metric_layer(value)\n    new_nodes.extend(add_metric_layer.inbound_nodes)\n    new_layers.append(add_metric_layer)\n    self._insert_layers(new_layers, new_nodes)\n\n  @property\n  def _trackable_saved_model_saver(self):\n    return network_serialization.NetworkSavedModelSaver(self)\n\n  def _get_save_spec(self, dynamic_batch=True):\n    if getattr(self, '_has_explicit_input_shape', True):\n      # Functional models and Sequential models that have an explicit input\n      # shape should use the batch size set by the input layer.\n      dynamic_batch = False\n    return super(Functional, self)._get_save_spec(dynamic_batch)\n\n\ndef _make_node_key(layer_name, node_index):\n  return layer_name + '_ib-' + str(node_index)\n\n\ndef _map_graph_network(inputs, outputs):\n  \"\"\"Validates a network's topology and gather its layers and nodes.\n\n  Args:\n    inputs: List of input tensors.\n    outputs: List of outputs tensors.\n\n  Returns:\n    A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.\n    - nodes: list of Node instances.\n    - nodes_by_depth: dict mapping ints (depth) to lists of node instances.\n    - layers: list of Layer instances.\n    - layers_by_depth: dict mapping ints (depth) to lists of layer instances.\n\n  Raises:\n    ValueError: In case the network is not valid (e.g. disconnected graph).\n  \"\"\"\n  # \"depth\" is number of layers between output Node and the Node.\n  # Nodes are ordered from inputs -> outputs.\n  nodes_in_decreasing_depth, layer_indices = _build_map(outputs)\n  network_nodes = {\n      _make_node_key(node.layer.name, node.layer._inbound_nodes.index(node))\n      for node in nodes_in_decreasing_depth\n  }\n\n  nodes_depths = {}  # dict {node: depth value}\n  layers_depths = {}  # dict {layer: depth value}\n\n  for node in reversed(nodes_in_decreasing_depth):\n    # If the depth is not set, the node has no outbound nodes (depth 0).\n    depth = nodes_depths.setdefault(node, 0)\n\n    # Update the depth of the corresponding layer\n    previous_depth = layers_depths.get(node.layer, 0)\n    # If we've seen this layer before at a higher depth,\n    # we should use that depth instead of the node depth.\n    # This is necessary for shared layers that have inputs at different\n    # depth levels in the graph.\n    depth = max(depth, previous_depth)\n    layers_depths[node.layer] = depth\n    nodes_depths[node] = depth\n\n    # Update the depth of inbound nodes.\n    # The \"depth\" of a node is the max of the depths\n    # of all nodes it is connected to + 1.\n    for node_dep in node.parent_nodes:\n      previous_depth = nodes_depths.get(node_dep, 0)\n      nodes_depths[node_dep] = max(depth + 1, previous_depth)\n\n  # Handle inputs that are not connected to outputs.\n  # We do not error out here because the inputs may be used to compute losses\n  # and metrics.\n  for input_t in inputs:\n    input_layer = input_t._keras_history[0]\n    if input_layer not in layers_depths:\n      layers_depths[input_layer] = 0\n      layer_indices[input_layer] = -1\n      nodes_depths[input_layer._inbound_nodes[0]] = 0\n      network_nodes.add(_make_node_key(input_layer.name, 0))\n\n  # Build a dict {depth: list of nodes with this depth}\n  nodes_by_depth = collections.defaultdict(list)\n  for node, depth in nodes_depths.items():\n    nodes_by_depth[depth].append(node)\n\n  # Build a dict {depth: list of layers with this depth}\n  layers_by_depth = collections.defaultdict(list)\n  for layer, depth in layers_depths.items():\n    layers_by_depth[depth].append(layer)\n\n  # Get sorted list of layer depths.\n  depth_keys = list(layers_by_depth.keys())\n  depth_keys.sort(reverse=True)\n\n  # Set self.layers ordered by depth.\n  layers = []\n  for depth in depth_keys:\n    layers_for_depth = layers_by_depth[depth]\n    # Network.layers needs to have a deterministic order:\n    # here we order them by traversal order.\n    layers_for_depth.sort(key=lambda x: layer_indices[x])\n    layers.extend(layers_for_depth)\n\n  # Get sorted list of node depths.\n  depth_keys = list(nodes_by_depth.keys())\n  depth_keys.sort(reverse=True)\n\n  # Check that all tensors required are computable.\n  # computable_tensors: all tensors in the graph\n  # that can be computed from the inputs provided.\n  computable_tensors = set()\n  for x in inputs:\n    computable_tensors.add(id(x))\n\n  layers_with_complete_input = []  # To provide a better error msg.\n  for depth in depth_keys:\n    for node in nodes_by_depth[depth]:\n      layer = node.layer\n      if layer and not node.is_input:\n        for x in nest.flatten(node.keras_inputs):\n          if id(x) not in computable_tensors:\n            raise ValueError('Graph disconnected: '\n                             'cannot obtain value for tensor ' + str(x) +\n                             ' at layer \"' + layer.name + '\". '\n                             'The following previous layers '\n                             'were accessed without issue: ' +\n                             str(layers_with_complete_input))\n        for x in nest.flatten(node.outputs):\n          computable_tensors.add(id(x))\n        layers_with_complete_input.append(layer.name)\n\n  # Ensure name unicity, which will be crucial for serialization\n  # (since serialized nodes refer to layers by their name).\n  all_names = [layer.name for layer in layers]\n  for name in all_names:\n    if all_names.count(name) != 1:\n      raise ValueError('The name \"' + name + '\" is used ' +\n                       str(all_names.count(name)) + ' times in the model. '\n                       'All layer names should be unique.')\n  return network_nodes, nodes_by_depth, layers, layers_by_depth\n\n\ndef _build_map(outputs):\n  \"\"\"This method topologically sorts nodes in order from inputs to outputs.\n\n  It uses a depth-first search to topologically sort nodes that appear in the\n  _keras_history connectivity metadata of `outputs`.\n\n  Args:\n    outputs: the output tensors whose _keras_history metadata should be walked.\n    This may be an arbitrary nested structure.\n\n  Returns:\n    A tuple like (ordered_nodes, layer_to_first_traversal_index)\n    ordered_nodes: list of nodes appearing in the keras history, topologically\n      sorted from original inputs to the `outputs`.\n      (If outputs have different sets of ancestors, the inputs to one output\n      may appear after a different output).\n    layer_to_first_traversal_index:\n      A dict mapping layer to the traversal index in the DFS where it is\n      seen. Note: if a layer is shared by several nodes, the dict will only\n      store the index corresponding to the *first* time the layer seen.\n  \"\"\"\n  finished_nodes = set()\n  nodes_in_progress = set()\n  nodes_in_decreasing_depth = []  # nodes from inputs -> outputs.\n  layer_indices = {}  # layer -> in traversal order.\n  for output in nest.flatten(outputs):\n    _build_map_helper(output, finished_nodes, nodes_in_progress,\n                      nodes_in_decreasing_depth, layer_indices)\n  return nodes_in_decreasing_depth, layer_indices\n\n\ndef _build_map_helper(tensor, finished_nodes, nodes_in_progress,\n                      nodes_in_decreasing_depth, layer_indices):\n  \"\"\"Recursive helper for `_build_map`.\"\"\"\n  layer, node_index, _ = tensor._keras_history  # pylint: disable=protected-access\n  node = layer._inbound_nodes[node_index]  # pylint: disable=protected-access\n\n  # Don't repeat work for shared subgraphs\n  if node in finished_nodes:\n    return\n\n  # Prevent cycles.\n  if node in nodes_in_progress:\n    raise ValueError('The tensor ' + str(tensor) + ' at layer \"' + layer.name +\n                     '\" is part of a cycle.')\n\n  # Store the traversal order for layer sorting.\n  if layer not in layer_indices:\n    layer_indices[layer] = len(layer_indices)\n\n  # Propagate to all previous tensors connected to this node.\n  nodes_in_progress.add(node)\n  if not node.is_input:\n    for tensor in node.keras_inputs:\n      _build_map_helper(tensor, finished_nodes, nodes_in_progress,\n                        nodes_in_decreasing_depth, layer_indices)\n\n  finished_nodes.add(node)\n  nodes_in_progress.remove(node)\n  nodes_in_decreasing_depth.append(node)\n\n\ndef _map_subgraph_network(inputs, outputs):\n  \"\"\"Returns the nodes and layers in the topology from `inputs` to `outputs`.\n\n  Args:\n    inputs: List of input tensors.\n    outputs: List of output tensors.\n\n  Returns:\n    A tuple of List{Node] and List[Layer].\n  \"\"\"\n  if not ops.executing_eagerly_outside_functions():\n    base_layer_utils.create_keras_history(outputs)\n  # Keep only nodes and layers in the topology between inputs and outputs.\n  _, nodes_by_depth, layers, _ = _map_graph_network(inputs, outputs)\n  return nest.flatten([nodes for nodes in nodes_by_depth.values()]), layers\n\n\ndef _should_skip_first_node(layer):\n  \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"\n  # Networks that are constructed with an Input layer/shape start with a\n  # pre-existing node linking their input to output. This node is excluded from\n  # the network config.\n  if layer._self_tracked_trackables:\n    return (isinstance(layer, Functional) and\n            # Filter out Sequential models without an input shape.\n            isinstance(layer._self_tracked_trackables[0],\n                       input_layer_module.InputLayer))\n  else:\n    return isinstance(layer, Functional)\n\n\ndef connect_ancillary_layers(model, created_layers):\n  \"\"\"Adds layers that are not connected to the outputs to the model.\"\"\"\n  # Layers not connected to outputs, such as those added in `add_loss`.\n  ancillary_layers = [\n      layer for layer in created_layers.values() if layer not in model.layers\n  ]\n  if ancillary_layers:\n    relevant_nodes = nest.flatten([\n        layer.inbound_nodes[1:]\n        if _should_skip_first_node(layer) else layer.inbound_nodes\n        for layer in created_layers.values()\n    ])\n    model._insert_layers(ancillary_layers, relevant_nodes)\n  return model\n\n\ndef reconstruct_from_config(config, custom_objects=None, created_layers=None):\n  \"\"\"Reconstructs graph from config object.\n\n  Args:\n    config: Dictionary returned from Network.get_config()\n    custom_objects: Optional dictionary mapping names (strings) to custom\n      classes or functions to be considered during deserialization.\n    created_layers: Optional dictionary mapping names to Layer objects. Any\n      layer not in this dictionary will be created and added to the dict.\n      This function will add new nodes to all layers (excluding InputLayers),\n      instead of re-using pre-existing nodes in the layers.\n\n  Returns:\n    Tuple of (input tensors, output tensors, dictionary of created layers)\n  \"\"\"\n  # Layer instances created during the graph reconstruction process.\n  created_layers = created_layers or collections.OrderedDict()\n\n  # Maps input data (tuple of inbound layer name, node index) from the config\n  # to node indices in the newly generated model. The node indices may be\n  # different if the layers have already been called previously.\n  node_index_map = {}\n  node_count_by_layer = {}\n\n  # Dictionary mapping layer instances to\n  # node data that specifies a layer call.\n  # It acts as a queue that maintains any unprocessed\n  # layer call until it becomes possible to process it\n  # (i.e. until the input tensors to the call all exist).\n  unprocessed_nodes = {}\n\n  def add_unprocessed_node(layer, node_data):\n    if layer not in unprocessed_nodes:\n      unprocessed_nodes[layer] = [node_data]\n    else:\n      unprocessed_nodes[layer].append(node_data)\n\n  def get_node_index(layer, config_node_index):\n    \"\"\"Returns node index in layer (might differ from config_node_index).\"\"\"\n    if isinstance(layer, input_layer_module.InputLayer):\n      return 0\n    return node_index_map.get((layer.name, config_node_index), None)\n\n  def _deserialize_keras_tensors(kwargs, layer_map):\n    \"\"\"Deserializes Keras Tensors passed to `call`..\"\"\"\n\n    def _deserialize_keras_tensor(t):\n      \"\"\"Deserializes a single Keras Tensor passed to `call`.\"\"\"\n      if isinstance(t, tf_utils.ListWrapper):\n        t = t.as_list()\n        layer_name = t[0]\n        node_index = t[1]\n        tensor_index = t[2]\n\n        layer = layer_map[layer_name]\n        new_node_index = get_node_index(layer, node_index)\n        if new_node_index is None:\n          # The inbound node may not have been processed yet,\n          # (This can happen e.g. if it depends on a different set\n          # of inputs than those that have been processed already).\n          # raise an IndexError so that the current node puts itself\n          # back on the unprocessed queue.\n          # Caution: This may lead to infinite loops for malformed\n          # network configurations! (or when there is a bug in\n          # the network config loading code).\n          raise IndexError\n        node = layer._inbound_nodes[new_node_index]\n        return nest.flatten(node.outputs)[tensor_index]\n      return t\n\n    kwargs = tf_utils.convert_inner_node_data(kwargs, wrap=True)\n    return nest.map_structure(_deserialize_keras_tensor, kwargs)\n\n  def process_node(layer, node_data):\n    \"\"\"Deserialize a node.\n\n    Args:\n        layer: layer instance.\n        node_data: Nested structure of `ListWrapper`.\n\n    Raises:\n        ValueError: In case of improperly formatted `node_data`.\n    \"\"\"\n    input_tensors = []\n    for input_data in nest.flatten(node_data):\n      input_data = input_data.as_list()\n      inbound_layer_name = input_data[0]\n      inbound_node_index = input_data[1]\n      inbound_tensor_index = input_data[2]\n      if len(input_data) == 3:\n        kwargs = {}\n      elif len(input_data) == 4:\n        kwargs = input_data[3]\n        try:\n          kwargs = _deserialize_keras_tensors(kwargs, created_layers)\n        except IndexError:\n          # Happens if keras tensors in kwargs are still unprocessed\n          add_unprocessed_node(layer, node_data)\n          return\n      else:\n        raise ValueError('Improperly formatted model config.')\n\n      if inbound_layer_name != node_module._CONSTANT_VALUE:\n        inbound_layer = created_layers[inbound_layer_name]\n        inbound_node_index = get_node_index(inbound_layer, inbound_node_index)\n\n        if inbound_node_index is None:\n          add_unprocessed_node(layer, node_data)\n          return\n        inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n        input_tensors.append(\n            nest.flatten(inbound_node.outputs)[inbound_tensor_index])\n      else:\n        # We received a constant w/ no Keras history attached\n        input_tensors.append(inbound_tensor_index)\n    input_tensors = nest.pack_sequence_as(node_data, input_tensors)\n    # Call layer on its inputs, thus creating the node\n    # and building the layer if needed.\n    if input_tensors is not None:\n      if not layer._preserve_input_structure_in_config:\n        input_tensors = (\n            base_layer_utils.unnest_if_single_tensor(input_tensors))\n      output_tensors = layer(input_tensors, **kwargs)\n\n      # Update node index map.\n      output_index = nest.flatten(output_tensors)[0]._keras_history.node_index\n      node_index_map[(layer.name, node_count_by_layer[layer])] = output_index\n      node_count_by_layer[layer] += 1\n\n  def process_layer(layer_data):\n    \"\"\"Deserializes a layer, then call it on appropriate inputs.\n\n    Args:\n        layer_data: layer config dict.\n\n    Raises:\n        ValueError: In case of improperly formatted `layer_data` dict.\n    \"\"\"\n    layer_name = layer_data['name']\n\n    if layer_name in created_layers:\n      layer = created_layers[layer_name]\n    else:\n      # Instantiate layer.\n      from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\n\n      layer = deserialize_layer(layer_data, custom_objects=custom_objects)\n      created_layers[layer_name] = layer\n\n    node_count_by_layer[layer] = int(_should_skip_first_node(layer))\n\n    # Gather layer inputs and convert to `ListWrapper` objects.\n    inbound_nodes_data = layer_data['inbound_nodes']\n    inbound_nodes_data = tf_utils.convert_inner_node_data(\n        inbound_nodes_data, wrap=True)\n    for node_data in inbound_nodes_data:\n      # We don't process nodes (i.e. make layer calls)\n      # on the fly because the inbound node may not yet exist,\n      # in case of layer shared at different topological depths\n      # (e.g. a model such as A(B(A(B(x)))))\n      add_unprocessed_node(layer, node_data)\n\n  # First, we create all layers and enqueue nodes to be processed\n  for layer_data in config['layers']:\n    process_layer(layer_data)\n  # Then we process nodes in order of layer depth.\n  # Nodes that cannot yet be processed (if the inbound node\n  # does not yet exist) are re-enqueued, and the process\n  # is repeated until all nodes are processed.\n  while unprocessed_nodes:\n    for layer_data in config['layers']:\n      layer = created_layers[layer_data['name']]\n      if layer in unprocessed_nodes:\n        for node_data in unprocessed_nodes.pop(layer):\n          process_node(layer, node_data)\n\n  input_tensors = []\n  output_tensors = []\n\n  input_layers = tf_utils.convert_inner_node_data(\n      config['input_layers'], wrap=True)\n  for layer_data in nest.flatten(input_layers):\n    layer_name, node_index, tensor_index = layer_data.as_list()\n    assert layer_name in created_layers\n    layer = created_layers[layer_name]\n    node_index = get_node_index(layer, node_index)\n    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n    input_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])\n\n  output_layers = tf_utils.convert_inner_node_data(\n      config['output_layers'], wrap=True)\n  for layer_data in nest.flatten(output_layers):\n    layer_name, node_index, tensor_index = layer_data.as_list()\n    assert layer_name in created_layers\n    layer = created_layers[layer_name]\n    node_index = get_node_index(layer, node_index)\n    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n    output_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])\n\n  input_tensors = nest.pack_sequence_as(input_layers, input_tensors)\n  output_tensors = nest.pack_sequence_as(output_layers, output_tensors)\n  return input_tensors, output_tensors, created_layers\n\n\ndef get_network_config(network, serialize_layer_fn=None):\n  \"\"\"Builds the config, which consists of the node graph and serialized layers.\n\n  Args:\n    network: A Network object.\n    serialize_layer_fn: Function used to serialize layers.\n\n  Returns:\n    Config dictionary.\n  \"\"\"\n  serialize_layer_fn = (\n      serialize_layer_fn or generic_utils.serialize_keras_object)\n  config = {\n      'name': network.name,\n  }\n  node_conversion_map = {}\n  for layer in network.layers:\n    kept_nodes = 1 if _should_skip_first_node(layer) else 0\n    for original_node_index, node in enumerate(layer._inbound_nodes):\n      node_key = _make_node_key(layer.name, original_node_index)\n      if node_key in network._network_nodes:\n        node_conversion_map[node_key] = kept_nodes\n        kept_nodes += 1\n  layer_configs = []\n\n  with generic_utils.SharedObjectSavingScope():\n    for layer in network.layers:  # From the earliest layers on.\n      filtered_inbound_nodes = []\n      for original_node_index, node in enumerate(layer._inbound_nodes):\n        node_key = _make_node_key(layer.name, original_node_index)\n        if node_key in network._network_nodes and not node.is_input:\n          # The node is relevant to the model:\n          # add to filtered_inbound_nodes.\n          node_data = node.serialize(_make_node_key, node_conversion_map)\n          filtered_inbound_nodes.append(node_data)\n\n      layer_config = serialize_layer_fn(layer)\n      layer_config['name'] = layer.name\n      layer_config['inbound_nodes'] = filtered_inbound_nodes\n      layer_configs.append(layer_config)\n    config['layers'] = layer_configs\n\n  # Gather info about inputs and outputs.\n  model_inputs = []\n  for i in range(len(network._input_layers)):\n    layer, node_index, tensor_index = network._input_coordinates[i]\n    node_key = _make_node_key(layer.name, node_index)\n    if node_key not in network._network_nodes:\n      continue\n    new_node_index = node_conversion_map[node_key]\n    model_inputs.append(\n        tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))\n  model_inputs = nest.pack_sequence_as(network._nested_inputs, model_inputs)\n  # Preserve external Keras compat for Models with single input.\n  if not nest.is_nested(model_inputs):\n    model_inputs = [model_inputs]\n  model_inputs = tf_utils.convert_inner_node_data(model_inputs)\n  config['input_layers'] = model_inputs\n\n  model_outputs = []\n  for i in range(len(network._output_layers)):\n    layer, node_index, tensor_index = network._output_coordinates[i]\n    node_key = _make_node_key(layer.name, node_index)\n    if node_key not in network._network_nodes:\n      continue\n    new_node_index = node_conversion_map[node_key]\n    model_outputs.append(\n        tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))\n  model_outputs = nest.pack_sequence_as(network._nested_outputs, model_outputs)\n  # Preserve external Keras compat for Models with single output.\n  if not nest.is_nested(model_outputs):\n    model_outputs = [model_outputs]\n  model_outputs = tf_utils.convert_inner_node_data(model_outputs)\n  config['output_layers'] = model_outputs\n  return config\n\n\ndef shape_with_no_batch_size(x):\n  if x.shape.rank is None:\n    return None\n  shape = x.shape.as_list()\n  if shape:\n    shape[0] = None\n  return shape\n\n\nclass ModuleWrapper(base_layer.Layer):\n  \"\"\"Wrapper for `tf.Module`s to support the Functional and Sequential API.\"\"\"\n\n  def __init__(self, module, method_name=None, **kwargs):\n    \"\"\"Initializes the wrapper Layer for this module.\n\n    Args:\n      module: The `tf.Module` instance to be wrapped.\n      method_name: (Optional) str. The name of the method to use as the forward\n        pass of the module. If not set, defaults to '__call__' if defined, or\n        'call'.\n      **kwargs: Additional keywrod arguments. See `tf.keras.layers.Layer`.\n\n    Raises:\n      ValueError: If `method` is not defined on `module`.\n    \"\"\"\n    super(ModuleWrapper, self).__init__(**kwargs)\n    if method_name is None:\n      if hasattr(module, '__call__'):\n        method_name = '__call__'\n      elif hasattr(module, 'call'):\n        method_name = 'call'\n    if method_name is None or not hasattr(module, method_name):\n      raise ValueError('{} is not defined on object {}'.format(\n          method_name, module))\n\n    self._module = module\n    self._method_name = method_name\n\n    # Check if module.__call__ has a `training` arg or accepts `**kwargs`.\n    method = getattr(module, method_name)\n    method_arg_spec = tf_inspect.getfullargspec(method)\n    self._expects_training_arg = ('training' in method_arg_spec.args or\n                                  method_arg_spec.varkw is not None)\n    self._expects_mask_arg = ('mask' in method_arg_spec.args or\n                              method_arg_spec.varkw is not None)\n\n  def call(self, *args, **kwargs):\n    if 'training' in kwargs and not self._expects_training_arg:\n      kwargs.pop('training')\n    if 'mask' in kwargs and not self._expects_mask_arg:\n      kwargs.pop('mask')\n    return getattr(self._module, self._method_name)(*args, **kwargs)\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#,============================================================================\n\"\"\"Tests for layer graphs construction & handling.\"\"\"\n\nimport warnings\n\nimport numpy as np\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras import combinations\nfrom tensorflow.python.keras import initializers\nfrom tensorflow.python.keras import keras_parameterized\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras import testing_utils\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.keras.engine import functional\nfrom tensorflow.python.keras.engine import input_layer as input_layer_lib\nfrom tensorflow.python.keras.engine import sequential\nfrom tensorflow.python.keras.engine import training as training_lib\nfrom tensorflow.python.keras.utils import layer_utils\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training.tracking.util import Checkpoint\n\n\nclass NetworkConstructionTest(keras_parameterized.TestCase):\n\n  def test_default_model_name(self):\n    inputs = input_layer_lib.Input(shape=(1,))\n    outputs = layers.Dense(1, activation='relu')(inputs)\n    model = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model.name, 'model')\n\n    model_2 = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model_2.name, 'model_1')\n\n    model_3 = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model_3.name, 'model_2')\n\n  def test_get_updates(self):\n\n    class MyLayer(layers.Layer):\n\n      def build(self, input_shape):\n        self.a = self.add_variable('a',\n                                   (1, 1),\n                                   'float32',\n                                   trainable=False)\n        self.b = self.add_variable('b',\n                                   (1, 1),\n                                   'float32',\n                                   trainable=False)\n        self.add_update(state_ops.assign_add(self.a, [[1.]],\n                                             name='unconditional_update'))\n        self.built = True\n\n      def call(self, inputs):\n        self.add_update(state_ops.assign_add(self.b, inputs,\n                                             name='conditional_update'),\n                        inputs=True)\n        return inputs + 1\n\n    with ops.Graph().as_default():\n      x1 = input_layer_lib.Input(shape=(1,))\n      layer = MyLayer()\n      _ = layer(x1)\n\n      self.assertEqual(len(layer.updates), 2)\n\n      x2 = input_layer_lib.Input(shape=(1,))\n      y2 = layer(x2)\n\n      self.assertEqual(len(layer.updates), 3)\n\n      network = functional.Functional(x2, y2)\n      self.assertEqual(len(network.updates), 3)\n\n      x3 = input_layer_lib.Input(shape=(1,))\n      _ = layer(x3)\n      self.assertEqual(len(network.updates), 4)\n\n      x4 = input_layer_lib.Input(shape=(1,))\n      _ = network(x4)\n      self.assertEqual(len(network.updates), 5)\n\n      network.add_update(state_ops.assign_add(layer.a, [[1]]))\n      self.assertEqual(len(network.updates), 6)\n\n      network.add_update(state_ops.assign_add(layer.b, x4), inputs=True)\n      self.assertEqual(len(network.updates), 7)\n\n  @combinations.generate(combinations.combine(mode=['graph']))\n  def test_get_updates_bn(self):\n    x1 = input_layer_lib.Input(shape=(1,))\n    layer = layers.BatchNormalization()\n    _ = layer(x1)\n\n    self.assertEqual(len(layer.updates), 2)\n\n  def test_get_layer(self):\n    # create a simple network\n    x = input_layer_lib.Input(shape=(32,))\n    dense_a = layers.Dense(4, name='dense_a')\n    dense_b = layers.Dense(2, name='dense_b')\n    y = dense_b(dense_a(x))\n    network = functional.Functional(x, y, name='dense_network')\n\n    # test various get_layer by index\n    self.assertEqual(network.get_layer(index=1), dense_a)\n\n    # test invalid get_layer by index\n    with self.assertRaisesRegex(\n        ValueError, 'Was asked to retrieve layer at index ' + str(3) +\n        ' but model only has ' + str(len(network.layers)) + ' layers.'):\n      network.get_layer(index=3)\n\n    # test that only one between name and index is requested\n    with self.assertRaisesRegex(ValueError,\n                                'Provide only a layer name or a layer index'):\n      network.get_layer(index=1, name='dense_b')\n\n    # test that a name or an index must be provided\n    with self.assertRaisesRegex(ValueError,\n                                'Provide either a layer name or layer index.'):\n      network.get_layer()\n\n    # test various get_layer by name\n    self.assertEqual(network.get_layer(name='dense_a'), dense_a)\n\n    # test invalid get_layer by name\n    with self.assertRaisesRegex(ValueError, 'No such layer: dense_c.'):\n      network.get_layer(name='dense_c')\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributes(self):\n    # test layer attributes / methods related to cross-layer connectivity.\n    a = input_layer_lib.Input(shape=(32,), name='input_a')\n    b = input_layer_lib.Input(shape=(32,), name='input_b')\n\n    # test input, output, input_shape, output_shape\n    test_layer = layers.Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    self.assertIs(test_layer.input, a)\n    self.assertIs(test_layer.output, a_test)\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, (None, 16))\n\n    # test `get_*_at` methods\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    self.assertIs(dense.get_input_at(0), a)\n    self.assertIs(dense.get_input_at(1), b)\n    self.assertIs(dense.get_output_at(0), a_2)\n    self.assertIs(dense.get_output_at(1), b_2)\n    self.assertEqual(dense.get_input_shape_at(0), (None, 32))\n    self.assertEqual(dense.get_input_shape_at(1), (None, 32))\n    self.assertEqual(dense.get_output_shape_at(0), (None, 16))\n    self.assertEqual(dense.get_output_shape_at(1), (None, 16))\n\n    # Test invalid value for attribute retrieval.\n    with self.assertRaises(ValueError):\n      dense.get_input_at(2)\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.input\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.output\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.output_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.input_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      a = input_layer_lib.Input(shape=(3, 32))\n      a = input_layer_lib.Input(shape=(5, 32))\n      a_2 = dense(a)\n      b_2 = dense(b)\n      _ = new_dense.input_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      a = input_layer_lib.Input(shape=(3, 32))\n      a = input_layer_lib.Input(shape=(5, 32))\n      a_2 = dense(a)\n      b_2 = dense(b)\n      _ = new_dense.output_shape\n\n  def _assertAllIs(self, a, b):\n    self.assertTrue(all(x is y for x, y in zip(a, b)))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributesMultiOutputLayer(self):\n\n    class PowersLayer(layers.Layer):\n\n      def call(self, inputs):\n        return [inputs**2, inputs**3]\n\n    x = input_layer_lib.Input(shape=(32,))\n    test_layer = PowersLayer()\n    p1, p2 = test_layer(x)  # pylint: disable=not-callable\n\n    self.assertIs(test_layer.input, x)\n    self._assertAllIs(test_layer.output, [p1, p2])\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, [(None, 32), (None, 32)])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributesMultiInputLayer(self):\n\n    class AddLayer(layers.Layer):\n\n      def call(self, inputs):\n        assert len(inputs) == 2\n        return inputs[0] + inputs[1]\n\n    a = input_layer_lib.Input(shape=(32,))\n    b = input_layer_lib.Input(shape=(32,))\n    test_layer = AddLayer()\n    y = test_layer([a, b])  # pylint: disable=not-callable\n\n    self._assertAllIs(test_layer.input, [a, b])\n    self.assertIs(test_layer.output, y)\n    self.assertEqual(test_layer.input_shape, [(None, 32), (None, 32)])\n    self.assertEqual(test_layer.output_shape, (None, 32))\n\n  def testBasicNetwork(self):\n    with ops.Graph().as_default():\n      # minimum viable network\n      x = input_layer_lib.Input(shape=(32,))\n      dense = layers.Dense(2)\n      y = dense(x)\n      network = functional.Functional(x, y, name='dense_network')\n\n      # test basic attributes\n      self.assertEqual(network.name, 'dense_network')\n      self.assertEqual(len(network.layers), 2)  # InputLayer + Dense\n      self.assertEqual(network.layers[1], dense)\n      self._assertAllIs(network.weights, dense.weights)\n      self._assertAllIs(network.trainable_weights, dense.trainable_weights)\n      self._assertAllIs(network.non_trainable_weights,\n                        dense.non_trainable_weights)\n\n      # test callability on Input\n      x_2 = input_layer_lib.Input(shape=(32,))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 2])\n\n      # test callability on regular tensor\n      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 2])\n\n      # test network `trainable` attribute\n      network.trainable = False\n      self._assertAllIs(network.weights, dense.weights)\n      self.assertEqual(network.trainable_weights, [])\n      self._assertAllIs(network.non_trainable_weights,\n                        dense.trainable_weights + dense.non_trainable_weights)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_trainable_weights(self):\n    a = layers.Input(shape=(2,))\n    b = layers.Dense(1)(a)\n    model = training_lib.Model(a, b)\n\n    weights = model.weights\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    model.trainable = True\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.layers[1].trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    # sequential model\n    model = sequential.Sequential()\n    model.add(layers.Dense(1, input_dim=2))\n    weights = model.weights\n\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    model.trainable = True\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.layers[0].trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n  def test_layer_call_arguments(self):\n    with ops.Graph().as_default():\n      # Test the ability to pass and serialize arguments to `call`.\n      inp = layers.Input(shape=(2,))\n      x = layers.Dense(3)(inp)\n      x = layers.Dropout(0.5)(x, training=True)\n      model = training_lib.Model(inp, x)\n      # Would be `dropout/cond/Merge` by default\n      self.assertIn('dropout', model.output.op.name)\n\n      # Test that argument is kept when applying the model\n      inp2 = layers.Input(shape=(2,))\n      out2 = model(inp2)\n      self.assertIn('dropout', out2.op.name)\n\n      # Test that argument is kept after loading a model\n      config = model.get_config()\n      model = training_lib.Model.from_config(config)\n      self.assertIn('dropout', model.output.op.name)\n\n  def test_node_construction(self):\n    # test basics\n    a = layers.Input(shape=(32,), name='input_a')\n    b = layers.Input(shape=(32,), name='input_b')\n\n    with self.assertRaises(ValueError):\n      _ = layers.Input(shape=(32,), batch_shape=(10, 32))\n    with self.assertRaises(ValueError):\n      _ = layers.Input(shape=(32,), unknown_kwarg=None)\n\n    self.assertListEqual(a.shape.as_list(), [None, 32])\n    a_layer, a_node_index, a_tensor_index = a._keras_history\n    b_layer, _, _ = b._keras_history\n    self.assertEqual(len(a_layer._inbound_nodes), 1)\n    self.assertEqual(a_tensor_index, 0)\n    node = a_layer._inbound_nodes[a_node_index]\n    self.assertEqual(node.outbound_layer, a_layer)\n\n    self.assertListEqual(node.inbound_layers, [])\n    self.assertListEqual(node.input_tensors, [a])\n    self.assertListEqual(node.input_shapes, [(None, 32)])\n    self.assertListEqual(node.output_tensors, [a])\n    self.assertListEqual(node.output_shapes, [(None, 32)])\n\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    self.assertEqual(len(dense._inbound_nodes), 2)\n    self.assertEqual(len(dense._outbound_nodes), 0)\n    self.assertEqual(dense._inbound_nodes[0].inbound_layers, a_layer)\n    self.assertEqual(dense._inbound_nodes[0].outbound_layer, dense)\n    self.assertEqual(dense._inbound_nodes[1].inbound_layers, b_layer)\n    self.assertEqual(dense._inbound_nodes[1].outbound_layer, dense)\n    self.assertIs(dense._inbound_nodes[0].input_tensors, a)\n    self.assertIs(dense._inbound_nodes[1].input_tensors, b)\n\n    # test layer properties\n    test_layer = layers.Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    self.assertListEqual(test_layer.kernel.shape.as_list(), [32, 16])\n    self.assertIs(test_layer.input, a)\n    self.assertIs(test_layer.output, a_test)\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, (None, 16))\n\n    self.assertIs(dense.get_input_at(0), a)\n    self.assertIs(dense.get_input_at(1), b)\n    self.assertIs(dense.get_output_at(0), a_2)\n    self.assertIs(dense.get_output_at(1), b_2)\n    self.assertEqual(dense.get_input_shape_at(0), (None, 32))\n    self.assertEqual(dense.get_input_shape_at(1), (None, 32))\n    self.assertEqual(dense.get_output_shape_at(0), (None, 16))\n    self.assertEqual(dense.get_output_shape_at(1), (None, 16))\n    self.assertEqual(dense.get_input_mask_at(0), None)\n    self.assertEqual(dense.get_input_mask_at(1), None)\n    self.assertEqual(dense.get_output_mask_at(0), None)\n    self.assertEqual(dense.get_output_mask_at(1), None)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_multi_input_layer(self):\n    with self.cached_session():\n      # test multi-input layer\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      self.assertListEqual(merged.shape.as_list(), [None, 16 * 2])\n      merge_layer, merge_node_index, merge_tensor_index = merged._keras_history\n\n      self.assertEqual(merge_node_index, 0)\n      self.assertEqual(merge_tensor_index, 0)\n\n      self.assertEqual(len(merge_layer._inbound_nodes), 1)\n      self.assertEqual(len(merge_layer._outbound_nodes), 0)\n\n      self.assertEqual(len(merge_layer._inbound_nodes[0].input_tensors), 2)\n      self.assertEqual(len(merge_layer._inbound_nodes[0].inbound_layers), 2)\n\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n      self.assertEqual(len(model.layers), 6)\n      output_shapes = model.compute_output_shape([(None, 32), (None, 32)])\n      self.assertListEqual(output_shapes[0].as_list(), [None, 64])\n      self.assertListEqual(output_shapes[1].as_list(), [None, 5])\n      self.assertListEqual(\n          model.compute_mask([a, b], [None, None]), [None, None])\n\n      # we don't check names of first 2 layers (inputs) because\n      # ordering of same-level layers is not fixed\n      self.assertListEqual([l.name for l in model.layers][2:],\n                           ['dense_1', 'merge', 'dense_2', 'dense_3'])\n      self.assertListEqual([l.name for l in model._input_layers],\n                           ['input_a', 'input_b'])\n      self.assertListEqual([l.name for l in model._output_layers],\n                           ['dense_2', 'dense_3'])\n\n      # actually run model\n      fn = backend.function(model.inputs, model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])\n\n      # test get_source_inputs\n      self._assertAllIs(layer_utils.get_source_inputs(c), [a, b])\n\n      # serialization / deserialization\n      json_config = model.to_json()\n      recreated_model = models.model_from_json(json_config)\n      recreated_model.compile('rmsprop', 'mse')\n\n      self.assertListEqual([l.name for l in recreated_model.layers][2:],\n                           ['dense_1', 'merge', 'dense_2', 'dense_3'])\n      self.assertListEqual([l.name for l in recreated_model._input_layers],\n                           ['input_a', 'input_b'])\n      self.assertListEqual([l.name for l in recreated_model._output_layers],\n                           ['dense_2', 'dense_3'])\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])\n\n  def test_multi_output_layer_output_names(self):\n    inp = layers.Input(name='inp', shape=(None,), dtype=dtypes.float32)\n\n    class _MultiOutput(layers.Layer):\n\n      def call(self, x):\n        return x + 1., x + 2.\n\n    out = _MultiOutput(name='out')(inp)\n    model = training_lib.Model(inp, out)\n    self.assertEqual(['out', 'out_1'], model.output_names)\n    self.assertAllClose([2., 3.], model(1.))\n\n  def test_recursion(self):\n    with ops.Graph().as_default(), self.cached_session():\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      e = layers.Input(shape=(32,), name='input_e')\n      f = layers.Input(shape=(32,), name='input_f')\n      self.assertEqual(len(model.inputs), 2)\n      g, h = model([e, f])\n      self.assertEqual(len(model.inputs), 2)\n      self.assertEqual(g.name, 'model/dense_2/BiasAdd:0')\n\n      self.assertListEqual(g.shape.as_list(), c.shape.as_list())\n      self.assertListEqual(h.shape.as_list(), d.shape.as_list())\n\n      # test separate manipulation of different layer outputs\n      i = layers.Dense(7, name='dense_4')(h)\n\n      final_model = training_lib.Model(\n          inputs=[e, f], outputs=[i, g], name='final')\n      self.assertEqual(len(final_model.inputs), 2)\n      self.assertEqual(len(final_model.outputs), 2)\n      self.assertEqual(len(final_model.layers), 4)\n\n      # we don't check names of first 2 layers (inputs) because\n      # ordering of same-level layers is not fixed\n      self.assertListEqual([layer.name for layer in final_model.layers][2:],\n                           ['model', 'dense_4'])\n      self.assertListEqual(\n          model.compute_mask([e, f], [None, None]), [None, None])\n      self.assertListEqual(\n          final_model.compute_output_shape([(10, 32), (10, 32)]), [(10, 7),\n                                                                   (10, 64)])\n\n      # run recursive model\n      fn = backend.function(final_model.inputs, final_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n\n      # test serialization\n      model_config = final_model.get_config()\n      recreated_model = models.Model.from_config(model_config)\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_multi_input_multi_output_recursion(self):\n    with self.cached_session():\n      # test multi-input multi-output\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      j = layers.Input(shape=(32,), name='input_j')\n      k = layers.Input(shape=(32,), name='input_k')\n      _, n = model([j, k])\n\n      o = layers.Input(shape=(32,), name='input_o')\n      p = layers.Input(shape=(32,), name='input_p')\n      q, _ = model([o, p])\n\n      self.assertListEqual(n.shape.as_list(), [None, 5])\n      self.assertListEqual(q.shape.as_list(), [None, 64])\n      s = layers.concatenate([n, q], name='merge_nq')\n      self.assertListEqual(s.shape.as_list(), [None, 64 + 5])\n\n      # test with single output as 1-elem list\n      multi_io_model = training_lib.Model([j, k, o, p], [s])\n\n      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      # test with single output as tensor\n      multi_io_model = training_lib.Model([j, k, o, p], s)\n\n      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      # note that the output of the function will still be a 1-elem list\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      # test serialization\n      model_config = multi_io_model.get_config()\n      recreated_model = models.Model.from_config(model_config)\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      # note that the output of the function will still be a 1-elem list\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      config = model.get_config()\n      models.Model.from_config(config)\n\n      model.summary()\n      json_str = model.to_json()\n      models.model_from_json(json_str)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_invalid_graphs(self):\n    a = layers.Input(shape=(32,), name='input_a')\n    b = layers.Input(shape=(32,), name='input_b')\n\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n    merged = layers.concatenate([a_2, b_2], name='merge')\n    c = layers.Dense(64, name='dense_2')(merged)\n    d = layers.Dense(5, name='dense_3')(c)\n\n    model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n    # input is not an Input tensor\n    j = layers.Input(shape=(32,), name='input_j')\n    j = layers.Dense(32)(j)\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k], [m, n])\n\n    # disconnected graph\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j], [m, n])\n\n    # redundant outputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    training_lib.Model([j, k], [m, n, n])\n\n    # redundant inputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k, j], [m, n])\n\n    # i have not idea what I'm doing: garbage as inputs/outputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k], [m, n, 0])\n\n  def test_raw_tf_compatibility(self):\n    with ops.Graph().as_default():\n      # test calling layers/models on TF tensors\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      j = layers.Input(shape=(32,), name='input_j')\n      k = layers.Input(shape=(32,), name='input_k')\n      self.assertEqual(len(model.inputs), 2)\n      m, n = model([j, k])\n      self.assertEqual(len(model.inputs), 2)\n      tf_model = training_lib.Model([j, k], [m, n])\n\n      j_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))\n      k_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))\n      m_tf, n_tf = tf_model([j_tf, k_tf])\n      self.assertListEqual(m_tf.shape.as_list(), [None, 64])\n      self.assertListEqual(n_tf.shape.as_list(), [None, 5])\n\n      # test merge\n      layers.concatenate([j_tf, k_tf], axis=1)\n      layers.add([j_tf, k_tf])\n\n      # test tensor input\n      x = array_ops.placeholder(shape=(None, 2), dtype=dtypes.float32)\n      layers.InputLayer(input_tensor=x)\n\n      x = layers.Input(tensor=x)\n      layers.Dense(2)(x)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_basic_masking(self):\n    a = layers.Input(shape=(10, 32), name='input_a')\n    b = layers.Masking()(a)\n    model = training_lib.Model(a, b)\n    self.assertEqual(model.output_mask.shape.as_list(), [None, 10])\n\n  def testMaskingSingleInput(self):\n\n    class MaskedLayer(layers.Layer):\n\n      def call(self, inputs, mask=None):\n        if mask is not None:\n          return inputs * mask\n        return inputs\n\n      def compute_mask(self, inputs, mask=None):\n        return array_ops.ones_like(inputs)\n\n    if context.executing_eagerly():\n      a = constant_op.constant([2] * 32)\n      mask = constant_op.constant([0, 1] * 16)\n      a._keras_mask = mask\n      b = MaskedLayer().apply(a)\n      self.assertTrue(hasattr(b, '_keras_mask'))\n      self.assertAllEqual(\n          self.evaluate(array_ops.ones_like(mask)),\n          self.evaluate(getattr(b, '_keras_mask')))\n      self.assertAllEqual(self.evaluate(a * mask), self.evaluate(b))\n    else:\n      x = input_layer_lib.Input(shape=(32,))\n      y = MaskedLayer()(x)  # pylint: disable=not-callable\n      network = functional.Functional(x, y)\n\n      # test callability on Input\n      x_2 = input_layer_lib.Input(shape=(32,))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 32])\n\n      # test callability on regular tensor\n      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 32])\n\n  def test_activity_regularization_with_model_composition(self):\n\n    def reg(x):\n      return math_ops.reduce_sum(x)\n\n    net_a_input = input_layer_lib.Input((2,))\n    net_a = net_a_input\n    net_a = layers.Dense(\n        2, kernel_initializer='ones', use_bias=False, activity_regularizer=reg)(\n            net_a)\n    model_a = training_lib.Model([net_a_input], [net_a])\n\n    net_b_input = input_layer_lib.Input((2,))\n    net_b = model_a(net_b_input)\n    model_b = training_lib.Model([net_b_input], [net_b])\n\n    model_b.compile(optimizer='sgd', loss=None)\n    x = np.ones((1, 2))\n    loss = model_b.evaluate(x)\n    self.assertEqual(loss, 4.)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_layer_sharing_at_heterogenous_depth(self):\n    x_val = np.random.random((10, 5))\n\n    x = input_layer_lib.Input(shape=(5,))\n    a = layers.Dense(5, name='A')\n    b = layers.Dense(5, name='B')\n    output = a(b(a(b(x))))\n    m = training_lib.Model(x, output)\n    m.run_eagerly = testing_utils.should_run_eagerly()\n\n    output_val = m.predict(x_val)\n\n    config = m.get_config()\n    weights = m.get_weights()\n\n    m2 = models.Model.from_config(config)\n    m2.set_weights(weights)\n\n    output_val_2 = m2.predict(x_val)\n    self.assertAllClose(output_val, output_val_2, atol=1e-6)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_layer_sharing_at_heterogenous_depth_with_concat(self):\n    input_shape = (16, 9, 3)\n    input_layer = input_layer_lib.Input(shape=input_shape)\n\n    a = layers.Dense(3, name='dense_A')\n    b = layers.Dense(3, name='dense_B')\n    c = layers.Dense(3, name='dense_C')\n\n    x1 = b(a(input_layer))\n    x2 = a(c(input_layer))\n    output = layers.concatenate([x1, x2])\n\n    m = training_lib.Model(inputs=input_layer, outputs=output)\n    m.run_eagerly = testing_utils.should_run_eagerly()\n\n    x_val = np.random.random((10, 16, 9, 3))\n    output_val = m.predict(x_val)\n\n    config = m.get_config()\n    weights = m.get_weights()\n\n    m2 = models.Model.from_config(config)\n    m2.set_weights(weights)\n\n    output_val_2 = m2.predict(x_val)\n    self.assertAllClose(output_val, output_val_2, atol=1e-6)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_explicit_training_argument(self):\n    a = layers.Input(shape=(2,))\n    b = layers.Dropout(0.5)(a)\n    base_model = training_lib.Model(a, b)\n\n    a = layers.Input(shape=(2,))\n    b = base_model(a, training=False)\n    model = training_lib.Model(a, b)\n\n    x = np.ones((100, 2))\n    y = np.ones((100, 2))\n    model.compile(\n        optimizer='sgd',\n        loss='mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    loss = model.train_on_batch(x, y)\n    self.assertEqual(loss, 0)  # In inference mode, output is equal to input.\n\n    a = layers.Input(shape=(2,))\n    b = base_model(a, training=True)\n    model = training_lib.Model(a, b)\n    preds = model.predict(x)\n    self.assertEqual(np.min(preds), 0.)  # At least one unit was dropped.\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_mask_derived_from_keras_layer(self):\n    inputs = input_layer_lib.Input((5, 10))\n    mask = input_layer_lib.Input((5,))\n    outputs = layers.RNN(layers.LSTMCell(100))(inputs, mask=mask)\n    model = training_lib.Model([inputs, mask], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # All data is masked, returned values are 0's.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.ones((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # Data is not masked, returned values are random.\n    self.assertGreater(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(model.get_config())\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # All data is masked, returned values are 0's.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.ones((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # Data is not masked, returned values are random.\n    self.assertGreater(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_arg_derived_from_keras_layer(self):\n\n    class MyAdd(layers.Layer):\n\n      def call(self, x1, x2):\n        return x1 + x2\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    outputs = MyAdd()(input1, input2)\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check serialization.\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MyAdd': MyAdd})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations(mode='eager'),)\n  def test_only_some_in_first_arg_derived_from_keras_layer_keras_tensors(self):\n    # This functionality is unsupported in v1 graphs\n\n    class MyAddAll(layers.Layer):\n\n      def call(self, inputs):\n        x = inputs[0]\n        for inp in inputs[1:]:\n          if inp is not None:\n            x = x + inp\n        return x\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    layer = MyAddAll()\n    outputs = layer([0.0, input1, None, input2, None])\n    model = training_lib.Model([input1, input2], outputs)\n    self.assertIn(layer, model.layers)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check serialization.\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MyAddAll': MyAddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(\n      combinations.times(\n          combinations.keras_mode_combinations(),\n          combinations.combine(share_already_used_layer=[True, False])))\n  def test_call_kwarg_derived_from_keras_layer(self, share_already_used_layer):\n\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        if x2 is not None:\n          return x1 + x2\n        return x1\n\n    class IdentityLayer(layers.Layer):\n\n      def call(self, x):\n        return x\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    identity_layer = IdentityLayer()\n\n    if share_already_used_layer:\n      # We have had model serialization/deserialization break in the past:\n      # when a layer was previously used to construct other functional models\n      # and had a non-empty list of inbound nodes before being used to define\n      # the model being serialized/deserialized.\n      # (The serialization/deserialization was not correctly adjusting\n      # the node_index serialization/deserialization).\n      # So, we explicitly test this case.\n      training_lib.Model([input1], identity_layer(input1))\n\n    outputs = MaybeAdd()(input1, x2=identity_layer(input2))\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(),\n        custom_objects={\n            'MaybeAdd': MaybeAdd,\n            'IdentityLayer': IdentityLayer\n        })\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_kwarg_dtype_serialization(self):\n\n    class Double(layers.Layer):\n\n      def call(self, x1, dtype=None):\n        return math_ops.cast(x1 + x1, dtype=dtype)\n\n    input1 = input_layer_lib.Input(10)\n    outputs = Double()(input1, dtype=dtypes.float16)\n    model = training_lib.Model([input1], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check the output dtype\n    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'Double': Double})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check the output dtype\n    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_kwarg_nonserializable(self):\n\n    class Double(layers.Layer):\n\n      def call(self, x1, kwarg=None):\n        return x1 + x1\n\n    class NonSerializable(object):\n\n      def __init__(self, foo=None):\n        self.foo = foo\n\n    input1 = input_layer_lib.Input(10)\n    outputs = Double()(input1, kwarg=NonSerializable())\n    model = training_lib.Model([input1], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    with self.assertRaisesRegex(\n        TypeError, 'Layer double was passed non-JSON-serializable arguments.'):\n      model.get_config()\n\n  @combinations.generate(\n      combinations.times(\n          combinations.keras_mode_combinations(),\n          combinations.combine(share_already_used_layer=[True, False])))\n  def test_call_kwarg_derived_from_keras_layer_and_first_arg_is_constant(\n      self, share_already_used_layer):\n\n    class IdentityLayer(layers.Layer):\n\n      def call(self, x):\n        return x\n\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        if x2 is not None:\n          return x1 + x2\n        return x1\n\n    input2 = input_layer_lib.Input(10)\n    identity_layer = IdentityLayer()\n    if share_already_used_layer:\n      # We have had model serialization/deserialization break in the past:\n      # when a layer was previously used to construct other functional models\n      # and had a non-empty list of inbound nodes before being used to define\n      # the model being serialized/deserialized.\n      # (The serialization/deserialization was not correctly adjusting\n      # the node_index serialization/deserialization).\n      # So, we explicitly test this case.\n      training_lib.Model([input2], identity_layer(input2))\n\n    outputs = MaybeAdd()(3., x2=identity_layer(input2))\n    model = training_lib.Model([input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=7 * np.ones((10, 10)),\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(),\n        custom_objects={\n            'MaybeAdd': MaybeAdd,\n            'IdentityLayer': IdentityLayer\n        })\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=7 * np.ones((10, 10)),\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_composite_call_kwarg_derived_from_keras_layer(self):\n\n    # Create a test layer that accepts composite tensor inputs.\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        # We need to convert this to a tensor for loss calculations -\n        # losses don't play nicely with ragged tensors yet.\n        if x2 is not None:\n          return (x1 + x2).to_tensor(default_value=0)\n        return x1.to_tensor(default_value=0)\n\n    input1 = input_layer_lib.Input((None,), ragged=True)\n    input2 = input_layer_lib.Input((None,), ragged=True)\n    outputs = MaybeAdd()(input1, x2=input2)\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    input_data = [\n        ragged_factory_ops.constant([[3.0, 3.0], [3.0, 3.0], [3.0]]),\n        ragged_factory_ops.constant([[7.0, 7.0], [7.0, 7.0], [7.0]])\n    ]\n    expected_data = np.array([[10.0, 10.0], [10.0, 10.0], [10.0, 0.0]])\n\n    history = model.fit(x=input_data, y=expected_data)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MaybeAdd': MaybeAdd})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(x=input_data, y=expected_data)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations(mode='eager'))\n  def test_call_some_not_all_nested_in_first_arg_derived_from_keras_layer(self):\n    # This functionality is unsupported in v1 graphs\n\n    class AddAll(layers.Layer):\n\n      def call(self, x1_x2, x3):\n        x1, x2 = x1_x2\n        out = x1 + x2\n        if x3 is not None:\n          for t in x3.values():\n            out += t\n        return out\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    input3 = input_layer_lib.Input(10)\n\n    layer = AddAll()\n    outputs = layer(\n        [input1, 4 * array_ops.ones((1, 10))],\n        x3={\n            'a': input2,\n            'b': input3,\n            'c': 5 * array_ops.ones((1, 10))\n        })\n    model = training_lib.Model([input1, input2, input3], outputs)\n    self.assertIn(layer, model.layers)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'AddAll': AddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_nested_arg_derived_from_keras_layer(self):\n\n    class AddAll(layers.Layer):\n\n      def call(self, x1, x2, x3=None):\n        out = x1 + x2\n        if x3 is not None:\n          for t in x3.values():\n            out += t\n        return out\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    input3 = input_layer_lib.Input(10)\n    outputs = AddAll()(\n        input1,\n        4 * array_ops.ones((1, 10)),\n        x3={\n            'a': input2,\n            'b': input3,\n            'c': 5 * array_ops.ones((1, 10))\n        })\n    model = training_lib.Model([input1, input2, input3], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'AddAll': AddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_multi_output_model_with_none_masking(self):\n    def func(x):\n      return [x * 0.2, x * 0.3]\n\n    def output_shape(input_shape):\n      return [input_shape, input_shape]\n\n    i = layers.Input(shape=(3, 2, 1))\n    o = layers.Lambda(function=func, output_shape=output_shape)(i)\n\n    self.assertEqual(backend.int_shape(o[0]), (None, 3, 2, 1))\n    self.assertEqual(backend.int_shape(o[1]), (None, 3, 2, 1))\n\n    o = layers.add(o)\n    model = training_lib.Model(i, o)\n    model.run_eagerly = testing_utils.should_run_eagerly()\n\n    i2 = layers.Input(shape=(3, 2, 1))\n    o2 = model(i2)\n    model2 = training_lib.Model(i2, o2)\n    model2.run_eagerly = testing_utils.should_run_eagerly()\n\n    x = np.random.random((4, 3, 2, 1))\n    out = model2.predict(x)\n    assert out.shape == (4, 3, 2, 1)\n    self.assertAllClose(out, x * 0.2 + x * 0.3, atol=1e-4)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_constant_initializer_with_numpy(self):\n    initializer = initializers.Constant(np.ones((3, 2)))\n    model = sequential.Sequential()\n    model.add(layers.Dense(2, input_shape=(3,), kernel_initializer=initializer))\n    model.add(layers.Dense(3))\n    model.compile(\n        loss='mse',\n        optimizer='sgd',\n        metrics=['acc'],\n        run_eagerly=testing_utils.should_run_eagerly())\n\n    json_str = model.to_json()\n    models.model_from_json(json_str)\n\n  def test_subclassed_error_if_init_not_called(self):\n\n    class MyNetwork(training_lib.Model):\n\n      def __init__(self):\n        self._foo = [layers.Dense(10), layers.Dense(10)]\n\n    with self.assertRaisesRegex(RuntimeError, 'forgot to call'):\n      MyNetwork()\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_int_input_shape(self):\n    inputs = input_layer_lib.Input(10)\n    self.assertEqual([None, 10], inputs.shape.as_list())\n\n    inputs_with_batch = input_layer_lib.Input(batch_size=20, shape=5)\n    self.assertEqual([20, 5], inputs_with_batch.shape.as_list())\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_model_initialization(self):\n    # Functional model\n    inputs = input_layer_lib.Input(shape=(32,))\n    outputs = layers.Dense(4)(inputs)\n\n    with self.assertRaisesRegex(TypeError,\n                                'Keyword argument not understood'):\n      model = training_lib.Model(\n          inputs, outputs, name='m', trainable=False, dtype='int64')\n    with self.assertRaisesRegex(TypeError,\n                                'Keyword argument not understood'):\n      model = training_lib.Model(\n          inputs, outputs, name='m', trainable=False, dynamic=False)\n\n    model = training_lib.Model(inputs, outputs, name='m', trainable=False)\n    self.assertEqual('m', model.name)\n    self.assertFalse(model.trainable)\n    self.assertFalse(model.dynamic)\n\n    class SubclassModel(training_lib.Model):\n      pass\n    # Subclassed model\n    model = SubclassModel(\n        name='subclassed', trainable=True, dtype='int64', dynamic=True)\n    self.assertEqual('subclassed', model.name)\n    self.assertTrue(model.dynamic)\n    self.assertTrue(model.trainable)\n    w = model.add_weight('w', [], initializer=initializers.Constant(1))\n    self.assertEqual(dtypes.int64, w.dtype)\n\n  def test_disconnected_inputs(self):\n    input_tensor1 = input_layer_lib.Input(shape=[200], name='a')\n    input_tensor2 = input_layer_lib.Input(shape=[10], name='b')\n    output_tensor1 = layers.Dense(units=10)(input_tensor1)\n\n    net = functional.Functional(\n        inputs=[input_tensor1, input_tensor2], outputs=[output_tensor1])\n    net2 = functional.Functional.from_config(net.get_config())\n    self.assertLen(net2.inputs, 2)\n    self.assertEqual('a', net2.layers[0].name)\n    self.assertEqual('b', net2.layers[1].name)\n\n  @combinations.generate(combinations.keras_model_type_combinations())\n  def test_dependency_tracking(self):\n    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n    model.trackable = Checkpoint()\n    self.assertIn('trackable', model._unconditional_dependency_names)\n    self.assertEqual(model.trackable, model._lookup_dependency('trackable'))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_model_construction_in_tf_function(self):\n\n    d = {'model': None}\n\n    @def_function.function\n    def fn(x):\n      if d['model'] is None:\n        # Check that Functional can be built in a `tf.function`.\n        inputs = input_layer_lib.Input(10)\n        outputs = layers.Dense(1)(inputs)\n        model = functional.Functional(inputs, outputs)\n        d['model'] = model\n      else:\n        model = d['model']\n\n      return model(x)\n\n    x = array_ops.ones((10, 10))\n    y = fn(x)\n    self.assertEqual(y.shape.as_list(), [10, 1])\n\n\nclass DeferredModeTest(keras_parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testSimpleNetworkBuilding(self):\n    inputs = input_layer_lib.Input(shape=(32,))\n    if context.executing_eagerly():\n      self.assertEqual(inputs.dtype.name, 'float32')\n      self.assertEqual(inputs.shape.as_list(), [None, 32])\n\n    x = layers.Dense(2)(inputs)\n    if context.executing_eagerly():\n      self.assertEqual(x.dtype.name, 'float32')\n      self.assertEqual(x.shape.as_list(), [None, 2])\n\n    outputs = layers.Dense(4)(x)\n    network = functional.Functional(inputs, outputs)\n    self.assertIsInstance(network, functional.Functional)\n\n    if context.executing_eagerly():\n      # It should be possible to call such a network on EagerTensors.\n      inputs = constant_op.constant(\n          np.random.random((10, 32)).astype('float32'))\n      outputs = network(inputs)\n      self.assertEqual(outputs.shape.as_list(), [10, 4])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testMultiIONetworkBuilding(self):\n    input_a = input_layer_lib.Input(shape=(32,))\n    input_b = input_layer_lib.Input(shape=(16,))\n    a = layers.Dense(16)(input_a)\n\n    class AddLayer(layers.Layer):\n\n      def call(self, inputs):\n        return inputs[0] + inputs[1]\n\n    c = AddLayer()([a, input_b])  # pylint: disable=not-callable\n    c = layers.Dense(2)(c)\n\n    network = functional.Functional([input_a, input_b], [a, c])\n    if context.executing_eagerly():\n      a_val = constant_op.constant(\n          np.random.random((10, 32)).astype('float32'))\n      b_val = constant_op.constant(\n          np.random.random((10, 16)).astype('float32'))\n      outputs = network([a_val, b_val])\n      self.assertEqual(len(outputs), 2)\n      self.assertEqual(outputs[0].shape.as_list(), [10, 16])\n      self.assertEqual(outputs[1].shape.as_list(), [10, 2])\n\n\nclass DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n\n  def _testShapeInference(self, model, input_shape, expected_output_shape):\n    input_value = np.random.random(input_shape)\n    output_value = model.predict(input_value)\n    self.assertEqual(output_value.shape, expected_output_shape)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testSingleInputCase(self):\n\n    class LayerWithOneInput(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs):\n        return backend.dot(inputs, self.w)\n\n    inputs = input_layer_lib.Input(shape=(3,))\n    layer = LayerWithOneInput()\n\n    if context.executing_eagerly():\n      self.assertEqual(\n          layer.compute_output_shape((None, 3)).as_list(), [None, 4])\n      # As a side-effect, compute_output_shape builds the layer.\n      self.assertTrue(layer.built)\n      # We can still query the layer's compute_output_shape with compatible\n      # input shapes.\n      self.assertEqual(\n          layer.compute_output_shape((6, 3)).as_list(), [6, 4])\n\n    outputs = layer(inputs)\n    model = training_lib.Model(inputs, outputs)\n    self._testShapeInference(model, (2, 3), (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testMultiInputOutputCase(self):\n\n    class MultiInputOutputLayer(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs):\n        a = backend.dot(inputs[0], self.w)\n        b = a + inputs[1]\n        return [a, b]\n\n    input_a = input_layer_lib.Input(shape=(3,))\n    input_b = input_layer_lib.Input(shape=(4,))\n    output_a, output_b = MultiInputOutputLayer()([input_a, input_b])\n    model = training_lib.Model([input_a, input_b], [output_a, output_b])\n    output_a_val, output_b_val = model.predict(\n        [np.random.random((2, 3)), np.random.random((2, 4))])\n    self.assertEqual(output_a_val.shape, (2, 4))\n    self.assertEqual(output_b_val.shape, (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTrainingArgument(self):\n\n    class LayerWithTrainingArg(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs, training):\n        return backend.dot(inputs, self.w)\n\n    inputs = input_layer_lib.Input(shape=(3,))\n    outputs = LayerWithTrainingArg()(inputs, training=False)\n    model = training_lib.Model(inputs, outputs)\n    self._testShapeInference(model, (2, 3), (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShape(self):\n\n    class Model(training_lib.Model):\n\n      def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n        self.pool = layers.GlobalAveragePooling2D()\n        self.fc = layers.Dense(3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.fc(x)\n        return x\n\n    model = Model()\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShapeWithCompoundModel(self):\n\n    class BasicBlock(training_lib.Model):\n\n      def __init__(self):\n        super(BasicBlock, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n        self.pool = layers.GlobalAveragePooling2D()\n        self.dense = layers.Dense(3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.dense(x)\n        return x\n\n    class CompoundModel(training_lib.Model):\n\n      def __init__(self):\n        super(CompoundModel, self).__init__()\n        self.block = BasicBlock()\n\n      def call(self, x):\n        x = self.block(x)  # pylint: disable=not-callable\n        return x\n\n    model = CompoundModel()\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)  # pylint: disable=not-callable\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShapeWithFunctionalAPI(self):\n\n    class BasicBlock(training_lib.Model):\n      # Inheriting from layers.Layer since we are calling this layer\n      # inside a model created using functional API.\n\n      def __init__(self):\n        super(BasicBlock, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        return x\n\n    input_layer = layers.Input(shape=(None, None, 1))\n    x = BasicBlock()(input_layer)\n    x = layers.GlobalAveragePooling2D()(x)\n    output_layer = layers.Dense(3)(x)\n\n    model = training_lib.Model(inputs=input_layer, outputs=output_layer)\n\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_sequential_as_downstream_of_masking_layer(self):\n    inputs = layers.Input(shape=(3, 4))\n    x = layers.Masking(mask_value=0., input_shape=(3, 4))(inputs)\n\n    s = sequential.Sequential()\n    s.add(layers.Dense(5, input_shape=(4,)))\n\n    x = layers.wrappers.TimeDistributed(s)(x)\n    model = training_lib.Model(inputs=inputs, outputs=x)\n    model.compile(\n        optimizer='rmsprop',\n        loss='mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n\n    model_input = np.random.randint(\n        low=1, high=5, size=(10, 3, 4)).astype('float32')\n    for i in range(4):\n      model_input[i, i:, :] = 0.\n    model.fit(model_input,\n              np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n\n    if not context.executing_eagerly():\n      # Note: this doesn't work in eager due to DeferredTensor/ops compatibility\n      # issue.\n      mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]\n      mask_outputs += [model.layers[2].compute_mask(\n          model.layers[2].input, mask_outputs[-1])]\n      func = backend.function([model.input], mask_outputs)\n      mask_outputs_val = func([model_input])\n      self.assertAllClose(mask_outputs_val[0], np.any(model_input, axis=-1))\n      self.assertAllClose(mask_outputs_val[1], np.any(model_input, axis=-1))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_external_keras_serialization_compat_input_layers(self):\n    inputs = input_layer_lib.Input(shape=(10,))\n    outputs = layers.Dense(1)(inputs)\n    model = training_lib.Model(inputs, outputs)\n    config = model.get_config()\n    # Checks that single inputs and outputs are still saved as 1-element lists.\n    # Saving as 1-element lists or not is equivalent in TF Keras, but only the\n    # 1-element list format is supported in TF.js and keras-team/Keras.\n    self.assertLen(config['input_layers'], 1)\n    self.assertLen(config['output_layers'], 1)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_external_keras_serialization_compat_inbound_nodes(self):\n    # Check single Tensor input.\n    inputs = input_layer_lib.Input(shape=(10,), name='in')\n    outputs = layers.Dense(1)(inputs)\n    model = training_lib.Model(inputs, outputs)\n    config = model.get_config()\n    self.assertEqual(config['layers'][1]['inbound_nodes'], [[['in', 0, 0, {}]]])\n\n    # Check multiple Tensor input.\n    inputs1 = input_layer_lib.Input(shape=(10,), name='in1')\n    inputs2 = input_layer_lib.Input(shape=(10,), name='in2')\n    outputs = layers.Add()([inputs1, inputs2])\n    model = training_lib.Model([inputs1, inputs2], outputs)\n    config = model.get_config()\n    self.assertEqual(config['layers'][2]['inbound_nodes'],\n                     [[['in1', 0, 0, {}], ['in2', 0, 0, {}]]])\n\n  @combinations.generate(combinations.combine(mode=['eager']))\n  def test_dict_inputs_tensors(self):\n    # Note that this test is running with v2 eager only, since the v1\n    # will behave differently wrt to dict input for training.\n    inputs = {\n        'sentence2': input_layer_lib.Input(\n            shape=(), name='a', dtype=dtypes.string),\n        'sentence1': input_layer_lib.Input(\n            shape=(), name='b', dtype=dtypes.string),\n    }\n    strlen = layers.Lambda(string_ops.string_length_v2)\n    diff = layers.Subtract()(\n        [strlen(inputs['sentence1']), strlen(inputs['sentence2'])])\n    diff = math_ops.cast(diff, dtypes.float32)\n    model = training_lib.Model(inputs, diff)\n\n    extra_keys = {\n        'sentence1': constant_op.constant(['brown fox', 'lazy dog']),\n        'sentence2': constant_op.constant(['owl', 'cheeky cat']),\n        'label': constant_op.constant([0, 1]),\n    }\n\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model(extra_keys)\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    model.compile('sgd', 'mse')\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model.fit(extra_keys, y=constant_op.constant([0, 1]), steps_per_epoch=1)\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model.evaluate(extra_keys, constant_op.constant([0, 1]))\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    # Make sure the model inputs are sorted with the dict keys.\n    self.assertEqual(model.inputs[0]._keras_history.layer.name, 'b')\n    self.assertEqual(model.inputs[1]._keras_history.layer.name, 'a')\n\n\nclass GraphUtilsTest(test.TestCase):\n\n  def testGetReachableFromInputs(self):\n\n    with ops.Graph().as_default(), self.cached_session():\n      pl_1 = array_ops.placeholder(shape=None, dtype='float32')\n      pl_2 = array_ops.placeholder(shape=None, dtype='float32')\n      pl_3 = array_ops.placeholder(shape=None, dtype='float32')\n      x_1 = pl_1 + pl_2\n      x_2 = pl_2 * 2\n      x_3 = pl_3 + 1\n      x_4 = x_1 + x_2\n      x_5 = x_3 * pl_1\n\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_1]),\n          {pl_1, x_1, x_4, x_5, x_1.op, x_4.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_1, pl_2]),\n          {pl_1, pl_2, x_1, x_2, x_4, x_5, x_1.op, x_2.op, x_4.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_3]),\n          {pl_3, x_3, x_5, x_3.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([x_3]), {x_3, x_5, x_5.op})\n\n\nclass NestedNetworkTest(keras_parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_inputs_network(self):\n    inputs = {\n        'x1': input_layer_lib.Input(shape=(1,)),\n        'x2': input_layer_lib.Input(shape=(1,))\n    }\n    outputs = layers.Add()([inputs['x1'], inputs['x2']])\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    result_tensor = network({\n        'x1': array_ops.ones((1, 1), 'float32'),\n        'x2': array_ops.ones((1, 1), 'float32')\n    })\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result, [[2.]])\n\n    # TODO(b/122726584): Investigate why concrete batch is flaky in some builds.\n    output_shape = network.compute_output_shape({\n        'x1': (None, 1),\n        'x2': (None, 1)\n    })\n    self.assertListEqual(output_shape.as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_outputs_network(self):\n    inputs = input_layer_lib.Input(shape=(1,))\n    outputs = {\n        'x+x': layers.Add()([inputs, inputs]),\n        'x*x': layers.Multiply()([inputs, inputs])\n    }\n\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    result_tensor = network(array_ops.ones((1, 1), 'float32'))\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result['x+x'], [[2.]])\n    self.assertAllEqual(result['x*x'], [[1.]])\n\n    output_shape = network.compute_output_shape((None, 1))\n    self.assertListEqual(output_shape['x+x'].as_list(), [None, 1])\n    self.assertListEqual(output_shape['x*x'].as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_network_inside_network(self):\n    inner_inputs = {\n        'x1': input_layer_lib.Input(shape=(1,)),\n        'x2': input_layer_lib.Input(shape=(1,))\n    }\n    inner_outputs = {\n        'x1+x2': layers.Add()([inner_inputs['x1'], inner_inputs['x2']]),\n        'x1*x2': layers.Multiply()([inner_inputs['x1'], inner_inputs['x2']])\n    }\n    inner_network = functional.Functional(\n        inner_inputs, inner_outputs)\n\n    inputs = [\n        input_layer_lib.Input(shape=(1,)),\n        input_layer_lib.Input(shape=(1,))\n    ]\n    middle = inner_network({'x1': inputs[0], 'x2': inputs[1]})\n    outputs = layers.Add()([middle['x1+x2'], middle['x1*x2']])\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    # Computes: `(x1+x2) + (x1*x2)`\n    result_tensor = network(\n        [array_ops.ones((1, 1), 'float32'),\n         array_ops.ones((1, 1), 'float32')])\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result, [[3.]])\n\n    output_shape = network.compute_output_shape([(None, 1), (None, 1)])\n    self.assertListEqual(output_shape.as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph']))\n  def test_updates_with_direct_call(self):\n    inputs = input_layer_lib.Input(shape=(10,))\n    x = layers.BatchNormalization()(inputs)\n    x = layers.Dense(10)(x)\n    model = training_lib.Model(inputs, x)\n\n    ph = backend.placeholder(shape=(10, 10))\n    model(ph)\n\n    self.assertLen(model.updates, 4)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_dict_mapping_input(self):\n\n    class ReturnFirst(layers.Layer):\n\n      def call(self, inputs):\n        b, _ = inputs\n        return b\n\n    # Checks that inputs are put in same order as the\n    # Model was constructed with.\n    b = input_layer_lib.Input(shape=(10,), name='b')\n    a = input_layer_lib.Input(shape=(10,), name='a')\n    outputs = ReturnFirst()([b, a])\n\n    b_val = array_ops.ones((10, 10))\n    a_val = array_ops.zeros((10, 10))\n\n    model = training_lib.Model([b, a], outputs)\n    res = model({'a': a_val, 'b': b_val})\n    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n\n    reversed_model = training_lib.Model([a, b], outputs)\n    res = reversed_model({'a': a_val, 'b': b_val})\n    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_dict_mapping_single_input(self):\n    b = input_layer_lib.Input(shape=(1,), name='b')\n    outputs = b * 2\n    model = training_lib.Model(b, outputs)\n\n    b_val = array_ops.ones((1, 1))\n    extra_val = array_ops.ones((1, 10))\n\n    inputs = {'a': extra_val, 'b': b_val}\n    res = model(inputs)\n\n    # Check that 'b' was used and 'a' was ignored.\n    self.assertEqual(res.shape.as_list(), [1, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_dict_mapping(self):\n    a = input_layer_lib.Input(shape=(1,), dtype='int32', name='a')\n    b = input_layer_lib.Input(shape=(1,), dtype='int32', name='b')\n    c = input_layer_lib.Input(shape=(1,), dtype='int32', name='c')\n    d = input_layer_lib.Input(shape=(1,), dtype='int32', name='d')\n    inputs = {'a': (a, b), 'c': (c, d)}\n    outputs = 1000 * a + 100 * b + 10 * c + d\n    model = training_lib.Model(inputs, outputs)\n\n    a_val = array_ops.ones((1, 1), dtype='int32')\n    b_val = 2 * array_ops.ones((1, 1), dtype='int32')\n    c_val = 3 * array_ops.ones((1, 1), dtype='int32')\n    d_val = 4 * array_ops.ones((1, 1), dtype='int32')\n\n    inputs_val = {'a': (a_val, b_val), 'c': (c_val, d_val)}\n    res = model(inputs_val)\n\n    # Check that inputs were flattened in the correct order.\n    self.assertFalse(model._enable_dict_to_input_mapping)\n    self.assertEqual(self.evaluate(res), [1234])\n\n\n@combinations.generate(combinations.keras_mode_combinations())\nclass AddLossTest(keras_parameterized.TestCase):\n\n  def test_add_loss_outside_call_only_loss(self):\n    inputs = input_layer_lib.Input((10,))\n    mid = layers.Dense(10)(inputs)\n    outputs = layers.Dense(1)(mid)\n    model = training_lib.Model(inputs, outputs)\n    model.add_loss(math_ops.reduce_mean(outputs))\n    self.assertLen(model.losses, 1)\n\n    initial_weights = model.get_weights()\n\n    x = np.ones((10, 10))\n    model.compile(\n        'sgd',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model.fit(x, batch_size=2, epochs=1)\n\n    model2 = model.from_config(model.get_config())\n    model2.compile(\n        'sgd',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model2.set_weights(initial_weights)\n    model2.fit(x, batch_size=2, epochs=1)\n\n    # The TFOpLayer and the AddLoss layer are serialized.\n    self.assertLen(model2.layers, 5)\n    self.assertAllClose(model.get_weights(), model2.get_weights())\n\n  def test_add_loss_outside_call_multiple_losses(self):\n    inputs = input_layer_lib.Input((10,))\n    x1 = layers.Dense(10)(inputs)\n    x2 = layers.Dense(10)(x1)\n    outputs = layers.Dense(1)(x2)\n    model = training_lib.Model(inputs, outputs)\n    model.add_loss(math_ops.reduce_sum(x1 * x2))\n    model.add_loss(math_ops.reduce_mean(outputs))\n    self.assertLen(model.losses, 2)\n\n    initial_weights = model.get_weights()\n\n    x, y = np.ones((10, 10)), np.ones((10, 1))\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model.fit(x, y, batch_size=2, epochs=1)\n\n    model2 = model.from_config(model.get_config())\n    model2.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model2.set_weights(initial_weights)\n    model2.fit(x, y, batch_size=2, epochs=1)\n\n    self.assertAllClose(model.get_weights(), model2.get_weights())\n\n  def test_add_loss_crossentropy_backtracking(self):\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((1,))\n    outputs = layers.Dense(1, activation='sigmoid')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.binary_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.random((2, 1))\n    model.fit([x, y])\n\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((2,))\n    outputs = layers.Dense(2, activation='softmax')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.categorical_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.random((2, 2))\n    model.fit([x, y])\n\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((1,), dtype='int32')\n    outputs = layers.Dense(2, activation='softmax')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.sparse_categorical_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.randint(0, 2, size=(2, 1))\n    model.fit([x, y])\n\n\n@combinations.generate(combinations.keras_mode_combinations())\nclass WeightAccessTest(keras_parameterized.TestCase):\n\n  def test_functional_model(self):\n    inputs = input_layer_lib.Input((10,))\n    x1 = layers.Dense(10)(inputs)\n    x2 = layers.Dense(10)(x1)\n    outputs = layers.Dense(1)(x2)\n    model = training_lib.Model(inputs, outputs)\n\n    self.assertEqual(len(model.weights), 6)\n\n  def test_sequential_model_with_input_shape(self):\n    x1 = layers.Dense(10, input_shape=(10,))\n    x2 = layers.Dense(10)\n    x3 = layers.Dense(1)\n    model = sequential.Sequential([x1, x2, x3])\n\n    self.assertEqual(len(model.weights), 6)\n\n  def test_sequential_model_without_input_shape(self):\n    x1 = layers.Dense(10)\n    x2 = layers.Dense(10)\n    x3 = layers.Dense(1)\n    model = sequential.Sequential([x1, x2, x3])\n\n    with self.assertRaisesRegex(\n        ValueError, 'Weights for model .* have not yet been created'):\n      _ = model.weights\n\n  def test_subclass_model_with_build_method(self):\n\n    class SubclassModel(models.Model):\n\n      def build(self, input_shape):\n        self.w = self.add_weight(shape=input_shape[-1], initializer='ones')\n\n      def call(self, inputs):\n        return inputs * self.w\n\n    model = SubclassModel()\n\n    with self.assertRaisesRegex(\n        ValueError, 'Weights for model .* have not yet been created'):\n      _ = model.weights\n\n    model(input_layer_lib.Input((10,)))\n    self.assertEqual(len(model.weights), 1)\n\n  def test_subclass_model_without_build_method(self):\n\n    class SubclassModel(models.Model):\n\n      def __init__(self):\n        super(SubclassModel, self).__init__()\n        self.w = self.add_weight(shape=(), initializer='ones')\n\n      def call(self, inputs):\n        return inputs * self.w\n\n    model = SubclassModel()\n    self.assertEqual(len(model.weights), 1)\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass DTypeTest(keras_parameterized.TestCase):\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_graph_network_dtype(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    network = functional.Functional(inputs, outputs)\n    self.assertEqual(network.dtype, 'float32')\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_subclassed_network_dtype(self):\n\n    class IdentityNetwork(training_lib.Model):\n\n      def call(self, inputs):\n        return inputs\n\n    network = IdentityNetwork()\n    self.assertEqual(network.dtype, 'float32')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float32')\n\n    network = IdentityNetwork(dtype='float16')\n    self.assertEqual(network.dtype, 'float16')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float16')\n\n    network = IdentityNetwork(autocast=False)\n    self.assertEqual(network.dtype, 'float32')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float64')\n\n\nclass AttrTrackingLayer(base_layer.Layer):\n  \"\"\"Count how many times `dynamic` and `stateful` are called.\n\n  These counts are used to test that the attribute cache behaves as expected.\n  \"\"\"\n  def __init__(self, *args, **kwargs):\n    self.stateful_count = 0\n    self.dynamic_count = 0\n    super(AttrTrackingLayer, self).__init__(*args, **kwargs)\n\n  @base_layer.Layer.stateful.getter\n  def stateful(self):\n    self.stateful_count += 1\n    return super(AttrTrackingLayer, self).stateful\n\n  @property\n  def dynamic(self):\n    self.dynamic_count += 1\n    return super(AttrTrackingLayer, self).dynamic\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass CacheCorrectnessTest(keras_parameterized.TestCase):\n\n  def layer_and_network_test(self):\n    # Top level layer\n    network = functional.Functional()\n\n    layer_0 = AttrTrackingLayer()\n\n    sub_network = functional.Functional()\n    layer_1 = AttrTrackingLayer(dynamic=True)\n    layer_2 = AttrTrackingLayer()\n    sub_network.sub_layers = [layer_1, layer_2]\n\n    network.sub_layer = layer_0\n\n    for _ in range(2):\n      self.assertEqual(network.dynamic, False)\n      self.assertEqual(network.stateful, False)\n\n      # The second pass should be a cache hit.\n      self.assertEqual(layer_0.dynamic_count, 1)\n      self.assertEqual(layer_0.stateful_count, 1)\n\n    # Mutations of the sub-layer should force recalculation of the network's\n    # stateful attribute. (mutations bubble up.)\n    layer_0.stateful = True\n    self.assertEqual(network.stateful, True)\n    self.assertEqual(layer_0.stateful_count, 2)\n\n    layer_0.stateful = False\n    self.assertEqual(network.stateful, False)\n    self.assertEqual(layer_0.stateful_count, 3)\n\n    # But changing stateful should not affect dynamic.\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(layer_0.dynamic_count, 1)\n\n    network.sub_network = sub_network\n\n    # Adding to the topology should invalidate the cache and reflect in the top\n    # level network.\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 2)\n    self.assertEqual(layer_1.dynamic_count, 1)\n\n    # Still dynamic, but we need to recompute.\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 3)\n    self.assertEqual(layer_1.dynamic_count, 2)\n\n    # Now that we've removed the dynamic layer deep in the layer hierarchy, we\n    # need to make sure that that bubbles up through all the levels.\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(layer_0.dynamic_count, 4)\n    self.assertEqual(layer_1.dynamic_count, 2)\n\n    # Now check with a tracked dict.\n    sub_network.sub_layers = {\n        \"layer_1\": layer_1,\n        \"layer_2\": layer_2,\n    }\n\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 5)\n    self.assertEqual(layer_1.dynamic_count, 3)\n\n    # In-place assignment should still invalidate the cache.\n    sub_network.sub_layers[\"layer_1\"] = layer_1\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 6)\n    self.assertEqual(layer_1.dynamic_count, 4)\n\n    sub_network.sub_layers[\"layer_1\"] = None\n    for _ in range(2):\n      self.assertEqual(network.dynamic, False)\n      self.assertEqual(layer_0.dynamic_count, 7)\n      self.assertEqual(layer_1.dynamic_count, 4)\n\n    layer_3 = AttrTrackingLayer()\n    layer_3.stateful = True\n\n    sub_network.sub_layers = None\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(network.stateful, False)\n\n    # Test duplicate layers.\n    sub_network.sub_layers = [layer_1, layer_1, layer_1, layer_3]\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(network.stateful, True)\n\n    for _ in range(3):\n      sub_network.sub_layers.pop()\n      self.assertEqual(network.dynamic, True)\n      self.assertEqual(network.stateful, False)\n\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(network.stateful, False)\n\n  def test_compute_output_shape_cache(self):\n    # See https://github.com/tensorflow/tensorflow/issues/32029.\n    x = input_layer_lib.Input(shape=(None, 32))\n    dense = layers.Dense(2)\n    y = dense(x)\n    network = functional.Functional(x, y, name='dense_network')\n\n    for i in range(999, 1024):\n      self.assertEqual(network.compute_output_shape((1, i, 32)), (1, i, 2))\n\n  def test_2d_inputs_squeezed_to_1d(self):\n    input_1d = input_layer_lib.Input(shape=())\n    outputs = input_1d * 2.\n    net = functional.Functional(input_1d, outputs)\n\n    x = np.ones((10, 1))\n    y = net(x)\n    self.assertEqual(y.shape.rank, 1)\n\n  def test_1d_inputs_expanded_to_2d(self):\n    input_1d = input_layer_lib.Input(shape=(1,))\n    outputs = input_1d * 2.\n    net = functional.Functional(input_1d, outputs)\n\n    x = np.ones((10,))\n    y = net(x)\n    self.assertEqual(y.shape.rank, 2)\n\n  def test_training_passed_during_construction(self):\n\n    def _call(inputs, training):\n      if training is None:\n        return inputs * -1.0\n      elif training:\n        return inputs\n      else:\n        return inputs * 0.0\n\n    class MyLayer(base_layer.Layer):\n\n      def call(self, inputs, training=True):\n        return _call(inputs, training)\n\n    my_layer = MyLayer()\n    x = np.ones((1, 10))\n\n    # Hard-coded `true` value passed during construction is respected.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=True)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, True))\n    self.assertAllEqual(network(x), _call(x, True))\n\n    # Hard-coded `false` value passed during construction is respected.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=False)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, False))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    self.assertAllEqual(network(x), _call(x, False))\n\n    if context.executing_eagerly():\n      # In v2, construction still works when no `training` is specified\n      # When no value passed during construction, it uses the local default.\n      inputs = input_layer_lib.Input(10)\n      outputs = my_layer(inputs)\n      network = functional.Functional(inputs, outputs)\n      self.assertAllEqual(network(x, training=True), _call(x, True))\n      self.assertAllEqual(network(x, training=False), _call(x, False))\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n\n    # `None` value passed positionally during construction is ignored at runtime\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, None)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    if context.executing_eagerly():\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n    else:\n      # in v1 training would have defaulted to using the `None` inside the layer\n      # if training is not passed at runtime\n      self.assertAllEqual(network(x), _call(x, None))\n\n    # `None` value passed as kwarg during construction is ignored at runtime.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=None)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    if context.executing_eagerly():\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n    else:\n      # in v1 training would have defaulted to using the `None` inside the layer\n      # if training is not passed at runtime\n      self.assertAllEqual(network(x), _call(x, None))\n\n\nclass InputsOutputsErrorTest(keras_parameterized.TestCase):\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_input_error(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    with self.assertRaisesRegex(\n        TypeError, \"('Keyword argument not understood:', 'input')\"):\n      models.Model(input=inputs, outputs=outputs)\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_output_error(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    with self.assertRaisesRegex(\n        TypeError, \"('Keyword argument not understood:', 'output')\"):\n      models.Model(inputs=inputs, output=outputs)\n\n  def test_input_spec(self):\n    if not context.executing_eagerly():\n      return\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    model = models.Model(inputs, outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model(np.zeros((3, 11)))\n\n  def test_input_spec_list_of_inputs(self):\n    if not context.executing_eagerly():\n      return\n    input_1 = input_layer_lib.Input((10,), name='1')\n    input_2 = input_layer_lib.Input((5,), name='2')\n    x = layers.Concatenate()([input_1, input_2])\n    outputs = layers.Dense(10)(x)\n    model = models.Model([input_1, input_2], outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'.*expects 2 input.*'):\n      model(np.zeros((3, 10)))\n    with self.assertRaisesRegex(\n        ValueError, r'.*expects 2 input.*'):\n      model([np.zeros((3, 10)), np.zeros((3, 5)), np.zeros((3, 10))])\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model([np.zeros((3, 10)), np.zeros((3, 6))])\n\n    # Test passing data via dict keyed by input name\n    with self.assertRaisesRegex(\n        ValueError, r'Missing data for input.*'):\n      model({'1': np.zeros((3, 10))})\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})\n\n  def test_input_spec_dict(self):\n    if not context.executing_eagerly():\n      return\n    input_1 = input_layer_lib.Input((10,))\n    input_2 = input_layer_lib.Input((5,))\n    x = layers.Concatenate()([input_1, input_2])\n    outputs = layers.Dense(10)(x)\n    model = models.Model({'1': input_1, '2': input_2}, outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'Missing data for input.*'):\n      model({'1': np.zeros((3, 10))})\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})\n\n\nclass FunctionalSubclassModel(training_lib.Model):\n\n  def __init__(self, *args, **kwargs):\n    self.foo = {'foo': 'bar'}  # Make sure users can assign dict attributes\n    my_input = input_layer_lib.Input(shape=(16,))\n    dense = layers.Dense(32, activation='relu')\n    output = dense(my_input)\n    outputs = {'output': output}\n    super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)\n\n\nclass MixinClass(object):\n\n  def __init__(self, foo, **kwargs):\n    self._foo = foo\n    super().__init__(**kwargs)\n\n  def get_foo(self):\n    return self._foo\n\n\nclass SubclassedModel(training_lib.Model):\n\n  def __init__(self, bar, **kwargs):\n    self._bar = bar\n    super().__init__(**kwargs)\n\n  def get_bar(self):\n    return self._bar\n\n\nclass MultipleInheritanceModelTest(keras_parameterized.TestCase):\n\n  def testFunctionalSubclass(self):\n    m = FunctionalSubclassModel()\n    # Some smoke test for the weights and output shape of the model\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n\n  def testFunctionalSubclassPreMixin(self):\n    class MixedFunctionalSubclassModel(MixinClass, FunctionalSubclassModel):\n      pass\n\n    m = MixedFunctionalSubclassModel(foo='123')\n    self.assertTrue(m._is_graph_network)\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n    self.assertEqual(m.get_foo(), '123')\n\n  def testFunctionalSubclassPostMixin(self):\n    # Make sure the the mixin class is also init correct when the order changed.\n\n    class MixedFunctionalSubclassModel(FunctionalSubclassModel, MixinClass):\n      pass\n\n    m = MixedFunctionalSubclassModel(foo='123')\n    self.assertTrue(m._is_graph_network)\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n    self.assertEqual(m.get_foo(), '123')\n\n  def testSubclassModelPreMixin(self):\n    class MixedSubclassModel(MixinClass, SubclassedModel):\n      pass\n\n    m = MixedSubclassModel(foo='123', bar='456')\n    self.assertFalse(m._is_graph_network)\n    self.assertEqual(m.get_foo(), '123')\n    self.assertEqual(m.get_bar(), '456')\n\n\nif __name__ == '__main__':\n  test.main()\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Training-related part of the Keras engine.\"\"\"\n\nimport copy\nimport itertools\nimport json\nimport os\nimport warnings\nimport weakref\n\nfrom tensorflow.python.autograph.lang import directives\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.data.ops import options as options_lib\nfrom tensorflow.python.distribute import collective_all_reduce_strategy\nfrom tensorflow.python.distribute import distribution_strategy_context as ds_context\nfrom tensorflow.python.distribute import values as ds_values\nfrom tensorflow.python.distribute.coordinator import cluster_coordinator\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import composite_tensor\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import func_graph\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras import callbacks as callbacks_module\nfrom tensorflow.python.keras import optimizer_v1\nfrom tensorflow.python.keras import optimizers\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.keras.engine import base_layer_utils\nfrom tensorflow.python.keras.engine import compile_utils\nfrom tensorflow.python.keras.engine import data_adapter\nfrom tensorflow.python.keras.engine import training_utils\nfrom tensorflow.python.keras.mixed_precision import loss_scale_optimizer as lso\nfrom tensorflow.python.keras.mixed_precision import policy\nfrom tensorflow.python.keras.saving import hdf5_format\nfrom tensorflow.python.keras.saving import save\nfrom tensorflow.python.keras.saving import saving_utils\nfrom tensorflow.python.keras.saving.saved_model import json_utils\nfrom tensorflow.python.keras.saving.saved_model import model_serialization\nfrom tensorflow.python.keras.utils import generic_utils\nfrom tensorflow.python.keras.utils import layer_utils\nfrom tensorflow.python.keras.utils import object_identity\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.keras.utils import version_utils\nfrom tensorflow.python.keras.utils.io_utils import ask_to_proceed_with_overwrite\nfrom tensorflow.python.keras.utils.io_utils import path_to_string\nfrom tensorflow.python.keras.utils.mode_keys import ModeKeys\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import summary_ops_v2\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.profiler import trace\nfrom tensorflow.python.saved_model import constants as sm_constants\nfrom tensorflow.python.saved_model import loader_impl as sm_loader\nfrom tensorflow.python.training import checkpoint_management\nfrom tensorflow.python.training import py_checkpoint_reader\nfrom tensorflow.python.training.tracking import base as trackable\nfrom tensorflow.python.training.tracking import graph_view as graph_view_lib\nfrom tensorflow.python.training.tracking import util as trackable_utils\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util import tf_decorator\nfrom tensorflow.python.util.tf_export import keras_export\nfrom tensorflow.tools.docs import doc_controls\n\n\n# pylint: disable=g-import-not-at-top\ntry:\n  import h5py\nexcept ImportError:\n  h5py = None\n# pylint: enable=g-import-not-at-top\n\n\ndef disable_multi_worker(method):\n  \"\"\"Decorator that disallows multi-worker use of `method`.\"\"\"\n\n  def _method_wrapper(self, *args, **kwargs):\n    if self._in_multi_worker_mode():  # pylint: disable=protected-access\n      raise ValueError('{} is not supported in multi-worker mode.'.format(\n          method.__name__))\n    return method(self, *args, **kwargs)\n\n  return tf_decorator.make_decorator(\n      target=method, decorator_func=_method_wrapper)\n\n\ndef inject_functional_model_class(cls):\n  \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\n  from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\n  from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top\n  if cls == Model or cls == training_v1.Model:\n    return functional.Functional\n  # In case there is any multiple inheritance, we stop injecting the\n  # class if keras model is not in its class hierarchy.\n  if cls == object:\n    return object\n\n  cls.__bases__ = tuple(inject_functional_model_class(base)\n                        for base in cls.__bases__)\n  # Trigger any `__new__` class swapping that needed to happen on `Functional`\n  # but did not because functional was not in the class hierarchy.\n  cls.__new__(cls)\n\n  return cls\n\n\ndef is_functional_model_init_params(args, kwargs):\n  return (len(args) == 2 or\n          len(args) == 1 and 'outputs' in kwargs or\n          'inputs' in kwargs and 'outputs' in kwargs)\n\n\n@keras_export('keras.Model', 'keras.models.Model')\nclass Model(base_layer.Layer, version_utils.ModelVersionSelector):\n  \"\"\"`Model` groups layers into an object with training and inference features.\n\n  Args:\n      inputs: The input(s) of the model: a `keras.Input` object or list of\n          `keras.Input` objects.\n      outputs: The output(s) of the model. See Functional API example below.\n      name: String, the name of the model.\n\n  There are two ways to instantiate a `Model`:\n\n  1 - With the \"Functional API\", where you start from `Input`,\n  you chain layer calls to specify the model's forward pass,\n  and finally you create your model from inputs and outputs:\n\n  ```python\n  import tensorflow as tf\n\n  inputs = tf.keras.Input(shape=(3,))\n  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  ```\n\n  Note: Only dicts, lists, and tuples of input tensors are supported. Nested\n  inputs are not supported (e.g. lists of list or dicts of dict).\n\n  2 - By subclassing the `Model` class: in that case, you should define your\n  layers in `__init__` and you should implement the model's forward pass\n  in `call`.\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n\n    def call(self, inputs):\n      x = self.dense1(inputs)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  If you subclass `Model`, you can optionally have\n  a `training` argument (boolean) in `call`, which you can use to specify\n  a different behavior in training and inference:\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n      self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n      x = self.dense1(inputs)\n      if training:\n        x = self.dropout(x, training=training)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  Once the model is created, you can config the model with losses and metrics\n  with `model.compile()`, train the model with `model.fit()`, or use the model\n  to do prediction with `model.predict()`.\n  \"\"\"\n  _TF_MODULE_IGNORED_PROPERTIES = frozenset(\n      itertools.chain(('_train_counter', '_test_counter', '_predict_counter',\n                       '_steps_per_execution'),\n                      base_layer.Layer._TF_MODULE_IGNORED_PROPERTIES))  # pylint: disable=protected-access\n\n  def __new__(cls, *args, **kwargs):\n    # Signature detection\n    if is_functional_model_init_params(args, kwargs) and cls == Model:\n      # Functional model\n      from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\n      return functional.Functional(skip_init=True, *args, **kwargs)\n    else:\n      return super(Model, cls).__new__(cls, *args, **kwargs)\n\n  @trackable.no_automatic_dependency_tracking\n  def __init__(self, *args, **kwargs):\n    self._is_model_for_instrumentation = True\n\n    # Special case for Subclassed Functional Model, which we couldn't detect\n    # when __new__ is called. We only realize it is a functional model when it\n    # calls super.__init__ with input and output tensor.\n    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\n    if (is_functional_model_init_params(args, kwargs) and\n        not isinstance(self, functional.Functional)):\n      # Filter the kwargs for multiple inheritance.\n      supported_kwargs = ['inputs', 'outputs', 'name', 'trainable', 'skip_init']\n      model_kwargs = {k: kwargs[k] for k in kwargs if k in supported_kwargs}\n      other_kwargs = {k: kwargs[k] for k in kwargs if k not in supported_kwargs}\n      inject_functional_model_class(self.__class__)\n      functional.Functional.__init__(self, *args, **model_kwargs)\n\n      # In case there is any multiple inheritance here, we need to call the\n      # __init__ for any class that appears after the Functional class.\n      clz_to_init = []\n      found_functional_class = False\n      for clz in self.__class__.__bases__:\n        if issubclass(clz, functional.Functional):\n          found_functional_class = True\n          continue\n        if found_functional_class:\n          clz_to_init.append(clz)\n\n      if clz_to_init:\n        for clz in clz_to_init:\n          clz.__init__(self, *args, **other_kwargs)\n      elif other_kwargs:\n        # In case there are unused kwargs, we should raise an error to user, in\n        # case they have a typo in the param name.\n        raise TypeError(\n            'The following keyword arguments aren\\'t supported: {}'.format(\n                other_kwargs))\n      return\n\n    # The following are implemented as property functions:\n    # self.trainable_weights\n    # self.non_trainable_weights\n    # `inputs` / `outputs` will only appear in kwargs if either are misspelled.\n    generic_utils.validate_kwargs(kwargs, {\n        'trainable', 'dtype', 'dynamic', 'name', 'autocast', 'inputs', 'outputs'\n    })\n    super(Model, self).__init__(**kwargs)\n    # By default, Model is a subclass model, which is not in graph network.\n    self._is_graph_network = False\n\n    self.inputs = None\n    self.outputs = None\n    self.input_names = None\n    self.output_names = None\n    # stop_training is used by callback to stop training when error happens\n    self.stop_training = False\n    self.history = None\n    # These objects are used in the default `Model.compile`. They are not\n    # guaranteed to be set after `Model.compile` is called, as users can\n    # override compile with custom logic.\n    self.compiled_loss = None\n    self.compiled_metrics = None\n\n    # This is True for Sequential networks and Functional networks.\n    self._compute_output_and_mask_jointly = False\n\n    # Don't reset compilation if already done. This may occur if calling\n    # `__init__` (or `_init_graph_network`) on an already-compiled model\n    # such as a Sequential model. Sequential models may need to rebuild\n    # themselves after compilation.\n    self._maybe_create_attribute('_is_compiled', False)\n    self._maybe_create_attribute('optimizer', None)\n\n    # Model must be created under scope of DistStrat it will be trained with.\n    if ds_context.has_strategy():\n      self._distribution_strategy = ds_context.get_strategy()\n    else:\n      self._distribution_strategy = None\n\n    self._cluster_coordinator = None\n\n    # Defaults to value of `tf.config.experimental_functions_run_eagerly`.\n    self._run_eagerly = None\n    # Initialize cache attrs.\n    self._reset_compile_cache()\n\n    # Fault-tolerance handler. Set in `ModelCheckpoint`.\n    self._training_state = None\n    self._saved_model_inputs_spec = None\n    self._trackable_saver = saver_with_op_caching(self)\n\n    self._steps_per_execution = None\n\n    self._init_batch_counters()\n    self._base_model_initialized = True\n\n  @trackable.no_automatic_dependency_tracking\n  def _init_batch_counters(self):\n    # Untracked Variables, used to keep track of mini-batches seen in `fit`,\n    # `evaluate`, and `predict`.\n    agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\n    self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)\n    self._predict_counter = variables.Variable(\n        0, dtype='int64', aggregation=agg)\n\n  def __setattr__(self, name, value):\n    if not getattr(self, '_self_setattr_tracking', True):\n      super(Model, self).__setattr__(name, value)\n      return\n\n    if all(\n        isinstance(v, (base_layer.Layer, variables.Variable)) or\n        base_layer_utils.has_weights(v) for v in nest.flatten(value)):\n      try:\n        self._base_model_initialized\n      except AttributeError:\n        raise RuntimeError(\n            'It looks like you are subclassing `Model` and you '\n            'forgot to call `super().__init__()`.'\n            ' Always start with this line.')\n\n    super(Model, self).__setattr__(name, value)\n\n  @generic_utils.default\n  def build(self, input_shape):\n    \"\"\"Builds the model based on input shapes received.\n\n    This is to be used for subclassed models, which do not know at instantiation\n    time what their inputs look like.\n\n    This method only exists for users who want to call `model.build()` in a\n    standalone way (as a substitute for calling the model on real data to\n    build it). It will never be called by the framework (and thus it will\n    never throw unexpected errors in an unrelated workflow).\n\n    Args:\n     input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n         shapes are tuples, integers, or TensorShapes.\n\n    Raises:\n      ValueError:\n        1. In case of invalid user-provided data (not of type tuple,\n           list, TensorShape, or dict).\n        2. If the model requires call arguments that are agnostic\n           to the input shapes (positional or kwarg in call signature).\n        3. If not all layers were properly built.\n        4. If float type inputs are not supported within the layers.\n\n      In each of these cases, the user should build their model by calling it\n      on real tensor data.\n    \"\"\"\n    if self._is_graph_network:\n      super(Model, self).build(input_shape)\n      return\n\n    if input_shape is None:\n      raise ValueError('Input shape must be defined when calling build on a '\n                       'model subclass network.')\n    valid_types = (tuple, list, tensor_shape.TensorShape, dict)\n    if not isinstance(input_shape, valid_types):\n      raise ValueError('Specified input shape is not one of the valid types. '\n                       'Please specify a batch input shape of type tuple or '\n                       'list of input shapes. User provided '\n                       'input type: {}'.format(type(input_shape)))\n\n    if input_shape and not self.inputs:\n      # We create placeholders for the `None`s in the shape and build the model\n      # in a Graph. Since tf.Variable is compatible with both eager execution\n      # and graph building, the variables created after building the model in\n      # a Graph are still valid when executing eagerly.\n      if context.executing_eagerly():\n        graph = func_graph.FuncGraph('build_graph')\n      else:\n        graph = backend.get_graph()\n      with graph.as_default():\n        if (isinstance(input_shape, list) and\n            all(d is None or isinstance(d, int) for d in input_shape)):\n          input_shape = tuple(input_shape)\n        if isinstance(input_shape, list):\n          x = [base_layer_utils.generate_placeholders_from_shape(shape)\n               for shape in input_shape]\n        elif isinstance(input_shape, dict):\n          x = {\n              k: base_layer_utils.generate_placeholders_from_shape(shape)\n              for k, shape in input_shape.items()\n          }\n        else:\n          x = base_layer_utils.generate_placeholders_from_shape(input_shape)\n\n        kwargs = {}\n        call_signature = self._call_full_argspec\n        call_args = call_signature.args\n        # Exclude `self`, `inputs`, and any argument with a default value.\n        if len(call_args) > 2:\n          if call_signature.defaults:\n            call_args = call_args[2:-len(call_signature.defaults)]\n          else:\n            call_args = call_args[2:]\n          for arg in call_args:\n            if arg == 'training':\n              # Case where `training` is a positional arg with no default.\n              kwargs['training'] = False\n            else:\n              # Has invalid call signature with unknown positional arguments.\n              raise ValueError(\n                  'Currently, you cannot build your model if it has '\n                  'positional or keyword arguments that are not '\n                  'inputs to the model, but are required for its '\n                  '`call` method. Instead, in order to instantiate '\n                  'and build your model, `call` your model on real '\n                  'tensor data with all expected call arguments.')\n        elif len(call_args) < 2:\n          # Signature without `inputs`.\n          raise ValueError('You can only call `build` on a model if its `call` '\n                           'method accepts an `inputs` argument.')\n        try:\n          self.call(x, **kwargs)\n        except (errors.InvalidArgumentError, TypeError):\n          raise ValueError('You cannot build your model by calling `build` '\n                           'if your layers do not support float type inputs. '\n                           'Instead, in order to instantiate and build your '\n                           'model, `call` your model on real tensor data (of '\n                           'the correct dtype).')\n    super(Model, self).build(input_shape)\n\n  @doc_controls.doc_in_current_and_subclasses\n  def call(self, inputs, training=None, mask=None):\n    \"\"\"Calls the model on new inputs.\n\n    In this case `call` just reapplies\n    all ops in the graph to the new inputs\n    (e.g. build a new computational graph from the provided inputs).\n\n    Note: This method should not be called directly. It is only meant to be\n    overridden when subclassing `tf.keras.Model`.\n    To call a model on an input, always use the `__call__` method,\n    i.e. `model(inputs)`, which relies on the underlying `call` method.\n\n    Args:\n        inputs: Input tensor, or dict/list/tuple of input tensors.\n        training: Boolean or boolean scalar tensor, indicating whether to run\n          the `Network` in training mode or inference mode.\n        mask: A mask or list of masks. A mask can be\n            either a tensor or None (no mask).\n\n    Returns:\n        A tensor if there is a single output, or\n        a list of tensors if there are more than one outputs.\n    \"\"\"\n    raise NotImplementedError('When subclassing the `Model` class, you should '\n                              'implement a `call` method.')\n\n  def compile(self,\n              optimizer='rmsprop',\n              loss=None,\n              metrics=None,\n              loss_weights=None,\n              weighted_metrics=None,\n              run_eagerly=None,\n              steps_per_execution=None,\n              **kwargs):\n    \"\"\"Configures the model for training.\n\n    Args:\n        optimizer: String (name of optimizer) or optimizer instance. See\n          `tf.keras.optimizers`.\n        loss: String (name of objective function), objective function or\n          `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n          function is any callable with the signature `loss = fn(y_true,\n          y_pred)`, where y_true = ground truth values with shape =\n          `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n          categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n          y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n          returns a weighted loss float tensor. If a custom `Loss` instance is\n          used and reduction is set to `None`, return value has the shape\n          `[batch_size, d0, .. dN-1]` i.e. per-sample or per-timestep loss\n          values; otherwise, it is a scalar. If the model has multiple outputs,\n          you can use a different loss on each output by passing a dictionary\n          or a list of losses. The loss value that will be minimized by the\n          model will then be the sum of all individual losses, unless\n          `loss_weights` is specified.\n        metrics: List of metrics to be evaluated by the model during training\n          and testing. Each of this can be a string (name of a built-in\n          function), function or a `tf.keras.metrics.Metric` instance. See\n          `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n          function is any callable with the signature `result = fn(y_true,\n          y_pred)`. To specify different metrics for different outputs of a\n          multi-output model, you could also pass a dictionary, such as\n          `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n          You can also pass a list to specify a metric or a list of metrics\n          for each output, such as `metrics=[['accuracy'], ['accuracy', 'mse']]`\n          or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n          strings 'accuracy' or 'acc', we convert this to one of\n          `tf.keras.metrics.BinaryAccuracy`,\n          `tf.keras.metrics.CategoricalAccuracy`,\n          `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n          function used and the model output shape. We do a similar\n          conversion for the strings 'crossentropy' and 'ce' as well.\n        loss_weights: Optional list or dictionary specifying scalar coefficients\n          (Python floats) to weight the loss contributions of different model\n          outputs. The loss value that will be minimized by the model will then\n          be the *weighted sum* of all individual losses, weighted by the\n          `loss_weights` coefficients.\n            If a list, it is expected to have a 1:1 mapping to the model's\n              outputs. If a dict, it is expected to map output names (strings)\n              to scalar coefficients.\n        weighted_metrics: List of metrics to be evaluated and weighted by\n          `sample_weight` or `class_weight` during training and testing.\n        run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n          logic will not be wrapped in a `tf.function`. Recommended to leave\n          this as `None` unless your `Model` cannot be run inside a\n          `tf.function`. `run_eagerly=True` is not supported when using\n          `tf.distribute.experimental.ParameterServerStrategy`.\n        steps_per_execution: Int. Defaults to 1. The number of batches to\n          run during each `tf.function` call. Running multiple batches\n          inside a single `tf.function` call can greatly improve performance\n          on TPUs or small models with a large Python overhead.\n          At most, one full epoch will be run each\n          execution. If a number larger than the size of the epoch is passed,\n          the execution will be truncated to the size of the epoch.\n          Note that if `steps_per_execution` is set to `N`,\n          `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n          will only be called every `N` batches\n          (i.e. before/after each `tf.function` execution).\n        **kwargs: Arguments supported for backwards compatibility only.\n\n    Raises:\n        ValueError: In case of invalid arguments for\n            `optimizer`, `loss` or `metrics`.\n    \"\"\"\n    with self.distribute_strategy.scope():\n      if 'experimental_steps_per_execution' in kwargs:\n        logging.warning('The argument `steps_per_execution` is no longer '\n                        'experimental. Pass `steps_per_execution` instead of '\n                        '`experimental_steps_per_execution`.')\n        if not steps_per_execution:\n          steps_per_execution = kwargs.pop('experimental_steps_per_execution')\n\n      # When compiling from an already-serialized model, we do not want to\n      # reapply some processing steps (e.g. metric renaming for multi-output\n      # models, which have prefixes added for each corresponding output name).\n      from_serialized = kwargs.pop('from_serialized', False)\n\n      self._validate_compile(optimizer, metrics, **kwargs)\n      self._run_eagerly = run_eagerly\n\n      self.optimizer = self._get_optimizer(optimizer)\n      self.compiled_loss = compile_utils.LossesContainer(\n          loss, loss_weights, output_names=self.output_names)\n      self.compiled_metrics = compile_utils.MetricsContainer(\n          metrics, weighted_metrics, output_names=self.output_names,\n          from_serialized=from_serialized)\n\n      self._configure_steps_per_execution(steps_per_execution or 1)\n\n      # Initializes attrs that are reset each time `compile` is called.\n      self._reset_compile_cache()\n      self._is_compiled = True\n\n      self.loss = loss or {}  # Backwards compat.\n\n  def _get_optimizer(self, optimizer):\n    \"\"\"Wraps `optimizer` in `LossScaleOptimizer` if necessary.\"\"\"\n    # The deprecated PolicyV1 has a loss_scale, which we use for backwards\n    # compatibility to match TF 2.3 behavior. The new Policy does not have a\n    # loss_scale, so we use dynamic loss scaling if the mixed_float16 policy is\n    # used.\n    if isinstance(self._dtype_policy, policy.PolicyV1):\n      loss_scale = self._dtype_policy.loss_scale\n    elif self._dtype_policy.name == 'mixed_float16':\n      loss_scale = 'dynamic'\n    else:\n      loss_scale = None\n\n    def _get_single_optimizer(opt):\n      opt = optimizers.get(opt)\n      if (loss_scale is not None and\n          not isinstance(opt, lso.LossScaleOptimizer)):\n        if loss_scale == 'dynamic':\n          opt = lso.LossScaleOptimizer(opt)\n        else:\n          opt = lso.LossScaleOptimizerV1(opt, loss_scale)\n      return opt\n\n    return nest.map_structure(_get_single_optimizer, optimizer)\n\n  @trackable.no_automatic_dependency_tracking\n  def _reset_compile_cache(self):\n    self.train_function = None\n    self.test_function = None\n    self.predict_function = None\n    # Used to cache the `tf.function`'ed `train_function` to be logged in\n    # TensorBoard, since the original `train_function` is not necessarily\n    # a `tf.function` (e.g., with ParameterServerStrategy, the `train_function`\n    # is a scheduling of the actual training function to a remote worker).\n    self.train_tf_function = None\n\n    # Used to cache `trainable` attr of `Layer`s for `fit`.\n    self._compiled_trainable_state = self._get_trainable_state()\n\n  @trackable.no_automatic_dependency_tracking\n  def _configure_steps_per_execution(self, steps_per_execution):\n    self._steps_per_execution = variables.Variable(\n        steps_per_execution,\n        dtype='int64',\n        aggregation=variables.VariableAggregationV2.ONLY_FIRST_REPLICA)\n\n  @property\n  def _should_compute_mask(self):\n    return False\n\n  @property\n  def metrics(self):\n    \"\"\"Returns the model's metrics added using `compile`, `add_metric` APIs.\n\n    Note: Metrics passed to `compile()` are available only after a `keras.Model`\n    has been trained/evaluated on actual data.\n\n    Examples:\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> outputs = tf.keras.layers.Dense(2)(inputs)\n    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n    >>> [m.name for m in model.metrics]\n    []\n\n    >>> x = np.random.random((2, 3))\n    >>> y = np.random.randint(0, 2, (2, 2))\n    >>> model.fit(x, y)\n    >>> [m.name for m in model.metrics]\n    ['loss', 'mae']\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> d = tf.keras.layers.Dense(2, name='out')\n    >>> output_1 = d(inputs)\n    >>> output_2 = d(inputs)\n    >>> model = tf.keras.models.Model(\n    ...    inputs=inputs, outputs=[output_1, output_2])\n    >>> model.add_metric(\n    ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n    >>> model.fit(x, (y, y))\n    >>> [m.name for m in model.metrics]\n    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n    'out_1_acc', 'mean']\n\n    \"\"\"\n    metrics = []\n    if self._is_compiled:\n      # TODO(omalleyt): Track `LossesContainer` and `MetricsContainer` objects\n      # so that attr names are not load-bearing.\n      if self.compiled_loss is not None:\n        metrics += self.compiled_loss.metrics\n      if self.compiled_metrics is not None:\n        metrics += self.compiled_metrics.metrics\n\n    for l in self._flatten_layers():\n      metrics.extend(l._metrics)  # pylint: disable=protected-access\n    return metrics\n\n  @property\n  def metrics_names(self):\n    \"\"\"Returns the model's display labels for all outputs.\n\n    Note: `metrics_names` are available only after a `keras.Model` has been\n    trained/evaluated on actual data.\n\n    Examples:\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> outputs = tf.keras.layers.Dense(2)(inputs)\n    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n    >>> model.metrics_names\n    []\n\n    >>> x = np.random.random((2, 3))\n    >>> y = np.random.randint(0, 2, (2, 2))\n    >>> model.fit(x, y)\n    >>> model.metrics_names\n    ['loss', 'mae']\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> d = tf.keras.layers.Dense(2, name='out')\n    >>> output_1 = d(inputs)\n    >>> output_2 = d(inputs)\n    >>> model = tf.keras.models.Model(\n    ...    inputs=inputs, outputs=[output_1, output_2])\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n    >>> model.fit(x, (y, y))\n    >>> model.metrics_names\n    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n    'out_1_acc']\n\n    \"\"\"\n\n    # This property includes all output names including `loss` and per-output\n    # losses for backward compatibility.\n    return [m.name for m in self.metrics]\n\n  @property\n  def distribute_strategy(self):\n    \"\"\"The `tf.distribute.Strategy` this model was created under.\"\"\"\n    return self._distribution_strategy or ds_context.get_strategy()\n\n  @property\n  def run_eagerly(self):\n    \"\"\"Settable attribute indicating whether the model should run eagerly.\n\n    Running eagerly means that your model will be run step by step,\n    like Python code. Your model might run slower, but it should become easier\n    for you to debug it by stepping into individual layer calls.\n\n    By default, we will attempt to compile your model to a static graph to\n    deliver the best execution performance.\n\n    Returns:\n      Boolean, whether the model should run eagerly.\n    \"\"\"\n    if self.dynamic and self._run_eagerly is False:  # pylint:disable=g-bool-id-comparison\n      # TODO(fchollet): consider using py_func to enable this.\n      raise ValueError('Your model contains layers that can only be '\n                       'successfully run in eager execution (layers '\n                       'constructed with `dynamic=True`). '\n                       'You cannot set `run_eagerly=False`.')\n\n    if self._cluster_coordinator and self._run_eagerly:\n      raise ValueError('When using `Model` with `ParameterServerStrategy`, '\n                       '`run_eagerly` is not supported.')\n\n    # Run eagerly logic, by priority:\n    # (1) Dynamic models must be run eagerly.\n    # (2) Explicitly setting run_eagerly causes a Model to be run eagerly.\n    # (3) Not explicitly setting run_eagerly defaults to TF's global setting.\n    return (self.dynamic or self._run_eagerly or\n            (def_function.functions_run_eagerly() and\n             self._run_eagerly is None))\n\n  @run_eagerly.setter\n  def run_eagerly(self, value):\n    self._run_eagerly = value\n\n  def train_step(self, data):\n    \"\"\"The logic for one training step.\n\n    This method can be overridden to support custom training logic.\n    For concrete examples of how to override this method see\n    [Customizing what happends in fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n    This method is called by `Model.make_train_function`.\n\n    This method should contain the mathematical logic for one step of training.\n    This typically includes the forward pass, loss calculation, backpropagation,\n    and metric updates.\n\n    Configuration details for *how* this logic is run (e.g. `tf.function` and\n    `tf.distribute.Strategy` settings), should be left to\n    `Model.make_train_function`, which can also be overridden.\n\n    Args:\n      data: A nested structure of `Tensor`s.\n\n    Returns:\n      A `dict` containing values that will be passed to\n      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n      values of the `Model`'s metrics are returned. Example:\n      `{'loss': 0.2, 'accuracy': 0.7}`.\n\n    \"\"\"\n    # These are the only transformations `Model.fit` applies to user-input\n    # data when a `tf.data.Dataset` is provided.\n    data = data_adapter.expand_1d(data)\n    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n    # Run forward pass.\n    with backprop.GradientTape() as tape:\n      y_pred = self(x, training=True)\n      loss = self.compiled_loss(\n          y, y_pred, sample_weight, regularization_losses=self.losses)\n    # Run backwards pass.\n    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    # Collect metrics to return\n    return_metrics = {}\n    for metric in self.metrics:\n      result = metric.result()\n      if isinstance(result, dict):\n        return_metrics.update(result)\n      else:\n        return_metrics[metric.name] = result\n    return return_metrics\n\n  def make_train_function(self):\n    \"\"\"Creates a function that executes one step of training.\n\n    This method can be overridden to support custom training logic.\n    This method is called by `Model.fit` and `Model.train_on_batch`.\n\n    Typically, this method directly controls `tf.function` and\n    `tf.distribute.Strategy` settings, and delegates the actual training\n    logic to `Model.train_step`.\n\n    This function is cached the first time `Model.fit` or\n    `Model.train_on_batch` is called. The cache is cleared whenever\n    `Model.compile` is called.\n\n    Returns:\n      Function. The function created by this method should accept a\n      `tf.data.Iterator`, and return a `dict` containing values that will\n      be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n      `{'loss': 0.2, 'accuracy': 0.7}`.\n    \"\"\"\n    if self.train_function is not None:\n      return self.train_function\n\n    def step_function(model, iterator):\n      \"\"\"Runs a single training step.\"\"\"\n\n      def run_step(data):\n        outputs = model.train_step(data)\n        # Ensure counter is updated only if `train_step` succeeds.\n        with ops.control_dependencies(_minimum_control_deps(outputs)):\n          model._train_counter.assign_add(1)  # pylint: disable=protected-access\n        return outputs\n\n      data = next(iterator)\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n      outputs = reduce_per_replica(\n          outputs, self.distribute_strategy, reduction='first')\n      write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access\n      return outputs\n\n    if self._steps_per_execution.numpy().item() == 1:\n\n      def train_function(iterator):\n        \"\"\"Runs a training execution with one step.\"\"\"\n        return step_function(self, iterator)\n\n    else:\n\n      def train_function(iterator):\n        \"\"\"Runs a training execution with multiple steps.\"\"\"\n        for _ in math_ops.range(self._steps_per_execution):\n          outputs = step_function(self, iterator)\n        return outputs\n\n    if not self.run_eagerly:\n      train_function = def_function.function(\n          train_function, experimental_relax_shapes=True)\n      self.train_tf_function = train_function\n\n    self.train_function = train_function\n\n    if self._cluster_coordinator:\n      self.train_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n          train_function, args=(iterator,))\n\n    return self.train_function\n\n  def fit(self,\n          x=None,\n          y=None,\n          batch_size=None,\n          epochs=1,\n          verbose='auto',\n          callbacks=None,\n          validation_split=0.,\n          validation_data=None,\n          shuffle=True,\n          class_weight=None,\n          sample_weight=None,\n          initial_epoch=0,\n          steps_per_epoch=None,\n          validation_steps=None,\n          validation_batch_size=None,\n          validation_freq=1,\n          max_queue_size=10,\n          workers=1,\n          use_multiprocessing=False):\n    \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays\n            (in case the model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors\n            (in case the model has multiple inputs).\n          - A dict mapping input names to the corresponding array/tensors,\n            if the model has named inputs.\n          - A `tf.data` dataset. Should return a tuple\n            of either `(inputs, targets)` or\n            `(inputs, targets, sample_weights)`.\n          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n            or `(inputs, targets, sample_weights)`.\n          - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n            callable that takes a single argument of type\n            `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n            `DatasetCreator` should be used when users prefer to specify the\n            per-replica batching and sharding logic for the `Dataset`.\n            See `tf.keras.utils.experimental.DatasetCreator` doc for more\n            information.\n          A more detailed description of unpacking behavior for iterator types\n          (Dataset, generator, Sequence) is given below. If using\n          `tf.distribute.experimental.ParameterServerStrategy`, only\n          `DatasetCreator` type is supported for `x`.\n        y: Target data. Like the input data `x`,\n          it could be either Numpy array(s) or TensorFlow tensor(s).\n          It should be consistent with `x` (you cannot have Numpy inputs and\n          tensor targets, or inversely). If `x` is a dataset, generator,\n          or `keras.utils.Sequence` instance, `y` should\n          not be specified (since targets will be obtained from `x`).\n        batch_size: Integer or `None`.\n            Number of samples per gradient update.\n            If unspecified, `batch_size` will default to 32.\n            Do not specify the `batch_size` if your data is in the\n            form of datasets, generators, or `keras.utils.Sequence` instances\n            (since they generate batches).\n        epochs: Integer. Number of epochs to train the model.\n            An epoch is an iteration over the entire `x` and `y`\n            data provided.\n            Note that in conjunction with `initial_epoch`,\n            `epochs` is to be understood as \"final epoch\".\n            The model is not trained for a number of iterations\n            given by `epochs`, but merely until the epoch\n            of index `epochs` is reached.\n        verbose: 'auto', 0, 1, or 2. Verbosity mode.\n            0 = silent, 1 = progress bar, 2 = one line per epoch.\n            'auto' defaults to 1 for most cases, but 2 when used with\n            `ParameterServerStrategy`. Note that the progress bar is not\n            particularly useful when logged to a file, so verbose=2 is\n            recommended when not running interactively (eg, in a production\n            environment).\n        callbacks: List of `keras.callbacks.Callback` instances.\n            List of callbacks to apply during training.\n            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n            and `tf.keras.callbacks.History` callbacks are created automatically\n            and need not be passed into `model.fit`.\n            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n            `verbose` argument to `model.fit`.\n            Callbacks with batch-level calls are currently unsupported with\n            `tf.distribute.experimental.ParameterServerStrategy`, and users are\n            advised to implement epoch-level calls instead with an appropriate\n            `steps_per_epoch` value.\n        validation_split: Float between 0 and 1.\n            Fraction of the training data to be used as validation data.\n            The model will set apart this fraction of the training data,\n            will not train on it, and will evaluate\n            the loss and any model metrics\n            on this data at the end of each epoch.\n            The validation data is selected from the last samples\n            in the `x` and `y` data provided, before shuffling. This argument is\n            not supported when `x` is a dataset, generator or\n           `keras.utils.Sequence` instance.\n            `validation_split` is not yet supported with\n            `tf.distribute.experimental.ParameterServerStrategy`.\n        validation_data: Data on which to evaluate\n            the loss and any model metrics at the end of each epoch.\n            The model will not be trained on this data. Thus, note the fact\n            that the validation loss of data provided using `validation_split`\n            or `validation_data` is not affected by regularization layers like\n            noise and dropout.\n            `validation_data` will override `validation_split`.\n            `validation_data` could be:\n              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n              - A `tf.data.Dataset`.\n              - A Python generator or `keras.utils.Sequence` returning\n              `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n            `validation_data` is not yet supported with\n            `tf.distribute.experimental.ParameterServerStrategy`.\n        shuffle: Boolean (whether to shuffle the training data\n            before each epoch) or str (for 'batch'). This argument is ignored\n            when `x` is a generator or an object of tf.data.Dataset.\n            'batch' is a special option for dealing\n            with the limitations of HDF5 data; it shuffles in batch-sized\n            chunks. Has no effect when `steps_per_epoch` is not `None`.\n        class_weight: Optional dictionary mapping class indices (integers)\n            to a weight (float) value, used for weighting the loss function\n            (during training only).\n            This can be useful to tell the model to\n            \"pay more attention\" to samples from\n            an under-represented class.\n        sample_weight: Optional Numpy array of weights for\n            the training samples, used for weighting the loss function\n            (during training only). You can either pass a flat (1D)\n            Numpy array with the same length as the input samples\n            (1:1 mapping between weights and samples),\n            or in the case of temporal data,\n            you can pass a 2D array with shape\n            `(samples, sequence_length)`,\n            to apply a different weight to every timestep of every sample. This\n            argument is not supported when `x` is a dataset, generator, or\n           `keras.utils.Sequence` instance, instead provide the sample_weights\n            as the third element of `x`.\n        initial_epoch: Integer.\n            Epoch at which to start training\n            (useful for resuming a previous training run).\n        steps_per_epoch: Integer or `None`.\n            Total number of steps (batches of samples)\n            before declaring one epoch finished and starting the\n            next epoch. When training with input tensors such as\n            TensorFlow data tensors, the default `None` is equal to\n            the number of samples in your dataset divided by\n            the batch size, or 1 if that cannot be determined. If x is a\n            `tf.data` dataset, and 'steps_per_epoch'\n            is None, the epoch will run until the input dataset is exhausted.\n            When passing an infinitely repeating dataset, you must specify the\n            `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n            will run indefinitely with an infinitely repeating dataset.\n            This argument is not supported with array inputs.\n            When using `tf.distribute.experimental.ParameterServerStrategy`:\n              * `steps_per_epoch=None` is not supported.\n        validation_steps: Only relevant if `validation_data` is provided and\n            is a `tf.data` dataset. Total number of steps (batches of\n            samples) to draw before stopping when performing validation\n            at the end of every epoch. If 'validation_steps' is None, validation\n            will run until the `validation_data` dataset is exhausted. In the\n            case of an infinitely repeated dataset, it will run into an\n            infinite loop. If 'validation_steps' is specified and only part of\n            the dataset will be consumed, the evaluation will start from the\n            beginning of the dataset at each epoch. This ensures that the same\n            validation samples are used every time.\n        validation_batch_size: Integer or `None`.\n            Number of samples per validation batch.\n            If unspecified, will default to `batch_size`.\n            Do not specify the `validation_batch_size` if your data is in the\n            form of datasets, generators, or `keras.utils.Sequence` instances\n            (since they generate batches).\n        validation_freq: Only relevant if validation data is provided. Integer\n            or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n            If an integer, specifies how many training epochs to run before a\n            new validation run is performed, e.g. `validation_freq=2` runs\n            validation every 2 epochs. If a Container, specifies the epochs on\n            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n            validation at the end of the 1st, 2nd, and 10th epochs.\n        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n            input only. Maximum size for the generator queue.\n            If unspecified, `max_queue_size` will default to 10.\n        workers: Integer. Used for generator or `keras.utils.Sequence` input\n            only. Maximum number of processes to spin up\n            when using process-based threading. If unspecified, `workers`\n            will default to 1.\n        use_multiprocessing: Boolean. Used for generator or\n            `keras.utils.Sequence` input only. If `True`, use process-based\n            threading. If unspecified, `use_multiprocessing` will default to\n            `False`. Note that because this implementation relies on\n            multiprocessing, you should not pass non-picklable arguments to\n            the generator as they can't be passed easily to children processes.\n\n    Unpacking behavior for iterator-like inputs:\n        A common pattern is to pass a tf.data.Dataset, generator, or\n      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n      yield not only features (x) but optionally targets (y) and sample weights.\n      Keras requires that the output of such iterator-likes be unambiguous. The\n      iterator should return a tuple of length 1, 2, or 3, where the optional\n      second and third elements will be used for y and sample_weight\n      respectively. Any other type provided will be wrapped in a length one\n      tuple, effectively treating everything as 'x'. When yielding dicts, they\n      should still adhere to the top-level tuple structure.\n      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n      features, targets, and weights from the keys of a single dict.\n        A notable unsupported data type is the namedtuple. The reason is that\n      it behaves like both an ordered datatype (tuple) and a mapping\n      datatype (dict). So given a namedtuple of the form:\n          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n      it is ambiguous whether to reverse the order of the elements when\n      interpreting the value. Even worse is a tuple of the form:\n          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n      where it is unclear if the tuple was intended to be unpacked into x, y,\n      and sample_weight or passed through as a single element to `x`. As a\n      result the data processing code will simply raise a ValueError if it\n      encounters a namedtuple. (Along with instructions to remedy the issue.)\n\n    Returns:\n        A `History` object. Its `History.history` attribute is\n        a record of training loss values and metrics values\n        at successive epochs, as well as validation loss values\n        and validation metrics values (if applicable).\n\n    Raises:\n        RuntimeError: 1. If the model was never compiled or,\n        2. If `model.fit` is  wrapped in `tf.function`.\n\n        ValueError: In case of mismatch between the provided input data\n            and what the model expects or when the input data is empty.\n    \"\"\"\n    # Legacy graph support is contained in `training_v1.Model`.\n    version_utils.disallow_legacy_graph('Model', 'fit')\n    self._assert_compile_was_called()\n    self._check_call_args('fit')\n    _disallow_inside_tf_function('fit')\n\n    if verbose == 'auto':\n      if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n        verbose = 2  # Default to epoch-level logging for PSStrategy.\n      else:\n        verbose = 1  # Default to batch-level logging otherwise.\n\n    if validation_split:\n      # Create the validation data using the training data. Only supported for\n      # `Tensor` and `NumPy` input.\n      (x, y, sample_weight), validation_data = (\n          data_adapter.train_validation_split(\n              (x, y, sample_weight), validation_split=validation_split))\n\n    if validation_data:\n      val_x, val_y, val_sample_weight = (\n          data_adapter.unpack_x_y_sample_weight(validation_data))\n\n    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n      self._cluster_coordinator = cluster_coordinator.ClusterCoordinator(\n          self.distribute_strategy)\n\n    with self.distribute_strategy.scope(), \\\n         training_utils.RespectCompiledTrainableState(self):\n      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n      data_handler = data_adapter.get_data_handler(\n          x=x,\n          y=y,\n          sample_weight=sample_weight,\n          batch_size=batch_size,\n          steps_per_epoch=steps_per_epoch,\n          initial_epoch=initial_epoch,\n          epochs=epochs,\n          shuffle=shuffle,\n          class_weight=class_weight,\n          max_queue_size=max_queue_size,\n          workers=workers,\n          use_multiprocessing=use_multiprocessing,\n          model=self,\n          steps_per_execution=self._steps_per_execution)\n\n      # Container that configures and calls `tf.keras.Callback`s.\n      if not isinstance(callbacks, callbacks_module.CallbackList):\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=True,\n            add_progbar=verbose != 0,\n            model=self,\n            verbose=verbose,\n            epochs=epochs,\n            steps=data_handler.inferred_steps)\n\n      self.stop_training = False\n      self.train_function = self.make_train_function()\n      self._train_counter.assign(0)\n      callbacks.on_train_begin()\n      training_logs = None\n      # Handle fault-tolerance for multi-worker.\n      # TODO(omalleyt): Fix the ordering issues that mean this has to\n      # happen after `callbacks.on_train_begin`.\n      data_handler._initial_epoch = (  # pylint: disable=protected-access\n          self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n      logs = None\n      for epoch, iterator in data_handler.enumerate_epochs():\n        self.reset_metrics()\n        callbacks.on_epoch_begin(epoch)\n        with data_handler.catch_stop_iteration():\n          for step in data_handler.steps():\n            with trace.Trace(\n                'train',\n                epoch_num=epoch,\n                step_num=step,\n                batch_size=batch_size,\n                _r=1):\n              callbacks.on_train_batch_begin(step)\n              tmp_logs = self.train_function(iterator)\n              if data_handler.should_sync:\n                context.async_wait()\n              logs = tmp_logs  # No error, now safe to assign to logs.\n              end_step = step + data_handler.step_increment\n              callbacks.on_train_batch_end(end_step, logs)\n              if self.stop_training:\n                break\n\n        logs = tf_utils.sync_to_numpy_or_python_type(logs)\n        if logs is None:\n          raise ValueError('Expect x to be a non-empty array or dataset.')\n        epoch_logs = copy.copy(logs)\n\n        # Run validation.\n        if validation_data and self._should_eval(epoch, validation_freq):\n          # Create data_handler for evaluation and cache it.\n          if getattr(self, '_eval_data_handler', None) is None:\n            self._eval_data_handler = data_adapter.get_data_handler(\n                x=val_x,\n                y=val_y,\n                sample_weight=val_sample_weight,\n                batch_size=validation_batch_size or batch_size,\n                steps_per_epoch=validation_steps,\n                initial_epoch=0,\n                epochs=1,\n                max_queue_size=max_queue_size,\n                workers=workers,\n                use_multiprocessing=use_multiprocessing,\n                model=self,\n                steps_per_execution=self._steps_per_execution)\n          val_logs = self.evaluate(\n              x=val_x,\n              y=val_y,\n              sample_weight=val_sample_weight,\n              batch_size=validation_batch_size or batch_size,\n              steps=validation_steps,\n              callbacks=callbacks,\n              max_queue_size=max_queue_size,\n              workers=workers,\n              use_multiprocessing=use_multiprocessing,\n              return_dict=True,\n              _use_cached_eval_dataset=True)\n          val_logs = {'val_' + name: val for name, val in val_logs.items()}\n          epoch_logs.update(val_logs)\n\n        callbacks.on_epoch_end(epoch, epoch_logs)\n        training_logs = epoch_logs\n        if self.stop_training:\n          break\n\n      # If eval data_hanlder exists, delete it after all epochs are done.\n      if getattr(self, '_eval_data_handler', None) is not None:\n        del self._eval_data_handler\n      callbacks.on_train_end(logs=training_logs)\n      return self.history\n\n  def test_step(self, data):\n    \"\"\"The logic for one evaluation step.\n\n    This method can be overridden to support custom evaluation logic.\n    This method is called by `Model.make_test_function`.\n\n    This function should contain the mathematical logic for one step of\n    evaluation.\n    This typically includes the forward pass, loss calculation, and metrics\n    updates.\n\n    Configuration details for *how* this logic is run (e.g. `tf.function` and\n    `tf.distribute.Strategy` settings), should be left to\n    `Model.make_test_function`, which can also be overridden.\n\n    Args:\n      data: A nested structure of `Tensor`s.\n\n    Returns:\n      A `dict` containing values that will be passed to\n      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n      values of the `Model`'s metrics are returned.\n    \"\"\"\n    data = data_adapter.expand_1d(data)\n    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n\n    y_pred = self(x, training=False)\n    # Updates stateful loss metrics.\n    self.compiled_loss(\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    # Collect metrics to return\n    return_metrics = {}\n    for metric in self.metrics:\n      result = metric.result()\n      if isinstance(result, dict):\n        return_metrics.update(result)\n      else:\n        return_metrics[metric.name] = result\n    return return_metrics\n\n  def make_test_function(self):\n    \"\"\"Creates a function that executes one step of evaluation.\n\n    This method can be overridden to support custom evaluation logic.\n    This method is called by `Model.evaluate` and `Model.test_on_batch`.\n\n    Typically, this method directly controls `tf.function` and\n    `tf.distribute.Strategy` settings, and delegates the actual evaluation\n    logic to `Model.test_step`.\n\n    This function is cached the first time `Model.evaluate` or\n    `Model.test_on_batch` is called. The cache is cleared whenever\n    `Model.compile` is called.\n\n    Returns:\n      Function. The function created by this method should accept a\n      `tf.data.Iterator`, and return a `dict` containing values that will\n      be passed to `tf.keras.Callbacks.on_test_batch_end`.\n    \"\"\"\n    if self.test_function is not None:\n      return self.test_function\n\n    def step_function(model, iterator):\n      \"\"\"Runs a single evaluation step.\"\"\"\n\n      def run_step(data):\n        outputs = model.test_step(data)\n        # Ensure counter is updated only if `test_step` succeeds.\n        with ops.control_dependencies(_minimum_control_deps(outputs)):\n          model._test_counter.assign_add(1)  # pylint: disable=protected-access\n        return outputs\n\n      data = next(iterator)\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n      outputs = reduce_per_replica(\n          outputs, self.distribute_strategy, reduction='first')\n      return outputs\n\n    if self._steps_per_execution.numpy().item() == 1:\n\n      def test_function(iterator):\n        \"\"\"Runs an evaluation execution with one step.\"\"\"\n        return step_function(self, iterator)\n\n    else:\n\n      def test_function(iterator):\n        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"\n        for _ in math_ops.range(self._steps_per_execution):\n          outputs = step_function(self, iterator)\n        return outputs\n\n    if not self.run_eagerly:\n      test_function = def_function.function(\n          test_function, experimental_relax_shapes=True)\n\n    self.test_function = test_function\n\n    if self._cluster_coordinator:\n      self.test_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n          test_function, args=(iterator,))\n\n    return self.test_function\n\n  def evaluate(self,\n               x=None,\n               y=None,\n               batch_size=None,\n               verbose=1,\n               sample_weight=None,\n               steps=None,\n               callbacks=None,\n               max_queue_size=10,\n               workers=1,\n               use_multiprocessing=False,\n               return_dict=False,\n               **kwargs):\n    \"\"\"Returns the loss value & metrics values for the model in test mode.\n\n    Computation is done in batches (see the `batch_size` arg.)\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays\n            (in case the model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors\n            (in case the model has multiple inputs).\n          - A dict mapping input names to the corresponding array/tensors,\n            if the model has named inputs.\n          - A `tf.data` dataset. Should return a tuple\n            of either `(inputs, targets)` or\n            `(inputs, targets, sample_weights)`.\n          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n            or `(inputs, targets, sample_weights)`.\n          A more detailed description of unpacking behavior for iterator types\n          (Dataset, generator, Sequence) is given in the `Unpacking behavior\n          for iterator-like inputs` section of `Model.fit`.\n        y: Target data. Like the input data `x`, it could be either Numpy\n          array(s) or TensorFlow tensor(s). It should be consistent with `x`\n          (you cannot have Numpy inputs and tensor targets, or inversely). If\n          `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n          should not be specified (since targets will be obtained from the\n          iterator/dataset).\n        batch_size: Integer or `None`. Number of samples per batch of\n          computation. If unspecified, `batch_size` will default to 32. Do not\n          specify the `batch_size` if your data is in the form of a dataset,\n          generators, or `keras.utils.Sequence` instances (since they generate\n          batches).\n        verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n        sample_weight: Optional Numpy array of weights for the test samples,\n          used for weighting the loss function. You can either pass a flat (1D)\n          Numpy array with the same length as the input samples\n            (1:1 mapping between weights and samples), or in the case of\n              temporal data, you can pass a 2D array with shape `(samples,\n              sequence_length)`, to apply a different weight to every timestep\n              of every sample. This argument is not supported when `x` is a\n              dataset, instead pass sample weights as the third element of `x`.\n        steps: Integer or `None`. Total number of steps (batches of samples)\n          before declaring the evaluation round finished. Ignored with the\n          default value of `None`. If x is a `tf.data` dataset and `steps` is\n          None, 'evaluate' will run until the dataset is exhausted. This\n          argument is not supported with array inputs.\n        callbacks: List of `keras.callbacks.Callback` instances. List of\n          callbacks to apply during evaluation. See\n          [callbacks](/api_docs/python/tf/keras/callbacks).\n        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n          input only. Maximum size for the generator queue. If unspecified,\n          `max_queue_size` will default to 10.\n        workers: Integer. Used for generator or `keras.utils.Sequence` input\n          only. Maximum number of processes to spin up when using process-based\n          threading. If unspecified, `workers` will default to 1.\n        use_multiprocessing: Boolean. Used for generator or\n          `keras.utils.Sequence` input only. If `True`, use process-based\n          threading. If unspecified, `use_multiprocessing` will default to\n          `False`. Note that because this implementation relies on\n          multiprocessing, you should not pass non-picklable arguments to the\n          generator as they can't be passed easily to children processes.\n        return_dict: If `True`, loss and metric results are returned as a dict,\n          with each key being the name of the metric. If `False`, they are\n          returned as a list.\n        **kwargs: Unused at this time.\n\n    See the discussion of `Unpacking behavior for iterator-like inputs` for\n    `Model.fit`.\n\n    `Model.evaluate` is not yet supported with\n    `tf.distribute.experimental.ParameterServerStrategy`.\n\n    Returns:\n        Scalar test loss (if the model has a single output and no metrics)\n        or list of scalars (if the model has multiple outputs\n        and/or metrics). The attribute `model.metrics_names` will give you\n        the display labels for the scalar outputs.\n\n    Raises:\n        RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n        ValueError: in case of invalid arguments.\n    \"\"\"\n    version_utils.disallow_legacy_graph('Model', 'evaluate')\n    self._assert_compile_was_called()\n    self._check_call_args('evaluate')\n    _disallow_inside_tf_function('evaluate')\n    use_cached_eval_dataset = kwargs.pop('_use_cached_eval_dataset', False)\n    if kwargs:\n      raise TypeError('Invalid keyword arguments: %s' % (kwargs,))\n\n    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n      self._cluster_coordinator = cluster_coordinator.ClusterCoordinator(\n          self.distribute_strategy)\n\n    with self.distribute_strategy.scope():\n      # Use cached evaluation data only when it's called in `Model.fit`\n      if (use_cached_eval_dataset\n          and getattr(self, '_eval_data_handler', None) is not None):\n        data_handler = self._eval_data_handler\n      else:\n        # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n        data_handler = data_adapter.get_data_handler(\n            x=x,\n            y=y,\n            sample_weight=sample_weight,\n            batch_size=batch_size,\n            steps_per_epoch=steps,\n            initial_epoch=0,\n            epochs=1,\n            max_queue_size=max_queue_size,\n            workers=workers,\n            use_multiprocessing=use_multiprocessing,\n            model=self,\n            steps_per_execution=self._steps_per_execution)\n\n      # Container that configures and calls `tf.keras.Callback`s.\n      if not isinstance(callbacks, callbacks_module.CallbackList):\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=True,\n            add_progbar=verbose != 0,\n            model=self,\n            verbose=verbose,\n            epochs=1,\n            steps=data_handler.inferred_steps)\n\n      logs = {}\n      self.test_function = self.make_test_function()\n      self._test_counter.assign(0)\n      callbacks.on_test_begin()\n      for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n        self.reset_metrics()\n        with data_handler.catch_stop_iteration():\n          for step in data_handler.steps():\n            with trace.Trace('test', step_num=step, _r=1):\n              callbacks.on_test_batch_begin(step)\n              tmp_logs = self.test_function(iterator)\n              if data_handler.should_sync:\n                context.async_wait()\n              logs = tmp_logs  # No error, now safe to assign to logs.\n              end_step = step + data_handler.step_increment\n              callbacks.on_test_batch_end(end_step, logs)\n      logs = tf_utils.sync_to_numpy_or_python_type(logs)\n      callbacks.on_test_end(logs=logs)\n\n      if return_dict:\n        return logs\n      else:\n        return flatten_metrics_in_order(logs, self.metrics_names)\n\n  def predict_step(self, data):\n    \"\"\"The logic for one inference step.\n\n    This method can be overridden to support custom inference logic.\n    This method is called by `Model.make_predict_function`.\n\n    This method should contain the mathematical logic for one step of inference.\n    This typically includes the forward pass.\n\n    Configuration details for *how* this logic is run (e.g. `tf.function` and\n    `tf.distribute.Strategy` settings), should be left to\n    `Model.make_predict_function`, which can also be overridden.\n\n    Args:\n      data: A nested structure of `Tensor`s.\n\n    Returns:\n      The result of one inference step, typically the output of calling the\n      `Model` on data.\n    \"\"\"\n    data = data_adapter.expand_1d(data)\n    x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\n    return self(x, training=False)\n\n  def make_predict_function(self):\n    \"\"\"Creates a function that executes one step of inference.\n\n    This method can be overridden to support custom inference logic.\n    This method is called by `Model.predict` and `Model.predict_on_batch`.\n\n    Typically, this method directly controls `tf.function` and\n    `tf.distribute.Strategy` settings, and delegates the actual evaluation\n    logic to `Model.predict_step`.\n\n    This function is cached the first time `Model.predict` or\n    `Model.predict_on_batch` is called. The cache is cleared whenever\n    `Model.compile` is called.\n\n    Returns:\n      Function. The function created by this method should accept a\n      `tf.data.Iterator`, and return the outputs of the `Model`.\n    \"\"\"\n    if self.predict_function is not None:\n      return self.predict_function\n\n    def step_function(model, iterator):\n      \"\"\"Runs a single evaluation step.\"\"\"\n\n      def run_step(data):\n        outputs = model.predict_step(data)\n        # Ensure counter is updated only if `test_step` succeeds.\n        with ops.control_dependencies(_minimum_control_deps(outputs)):\n          model._predict_counter.assign_add(1)  # pylint: disable=protected-access\n        return outputs\n\n      data = next(iterator)\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n      outputs = reduce_per_replica(\n          outputs, self.distribute_strategy, reduction='concat')\n      return outputs\n\n    if (self._steps_per_execution is None or\n        self._steps_per_execution.numpy().item() == 1):\n\n      def predict_function(iterator):\n        \"\"\"Runs an evaluation execution with one step.\"\"\"\n        return step_function(self, iterator)\n\n    else:\n\n      def predict_function(iterator):\n        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"\n        outputs = step_function(self, iterator)\n        for _ in math_ops.range(self._steps_per_execution - 1):\n          directives.set_loop_options(\n              shape_invariants=[(\n                  t, tf_utils.get_tensor_spec(t, dynamic_batch=True).shape)\n                                for t in nest.flatten(outputs)])\n          step_outputs = step_function(self, iterator)\n          outputs = nest.map_structure(lambda t1, t2: concat([t1, t2]), outputs,\n                                       step_outputs)\n        return outputs\n\n    if not self.run_eagerly:\n      predict_function = def_function.function(\n          predict_function, experimental_relax_shapes=True)\n\n    self.predict_function = predict_function\n    return self.predict_function\n\n  def predict(self,\n              x,\n              batch_size=None,\n              verbose=0,\n              steps=None,\n              callbacks=None,\n              max_queue_size=10,\n              workers=1,\n              use_multiprocessing=False):\n    \"\"\"Generates output predictions for the input samples.\n\n    Computation is done in batches. This method is designed for performance in\n    large scale inputs. For small amount of inputs that fit in one batch,\n    directly using `__call__` is recommended for faster execution, e.g.,\n    `model(x)`, or `model(x, training=False)` if you have layers such as\n    `tf.keras.layers.BatchNormalization` that behaves differently during\n    inference. Also, note the fact that test loss is not affected by\n    regularization layers like noise and dropout.\n\n    Args:\n        x: Input samples. It could be:\n          - A Numpy array (or array-like), or a list of arrays\n            (in case the model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors\n            (in case the model has multiple inputs).\n          - A `tf.data` dataset.\n          - A generator or `keras.utils.Sequence` instance.\n          A more detailed description of unpacking behavior for iterator types\n          (Dataset, generator, Sequence) is given in the `Unpacking behavior\n          for iterator-like inputs` section of `Model.fit`.\n        batch_size: Integer or `None`.\n            Number of samples per batch.\n            If unspecified, `batch_size` will default to 32.\n            Do not specify the `batch_size` if your data is in the\n            form of dataset, generators, or `keras.utils.Sequence` instances\n            (since they generate batches).\n        verbose: Verbosity mode, 0 or 1.\n        steps: Total number of steps (batches of samples)\n            before declaring the prediction round finished.\n            Ignored with the default value of `None`. If x is a `tf.data`\n            dataset and `steps` is None, `predict` will\n            run until the input dataset is exhausted.\n        callbacks: List of `keras.callbacks.Callback` instances.\n            List of callbacks to apply during prediction.\n            See [callbacks](/api_docs/python/tf/keras/callbacks).\n        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n            input only. Maximum size for the generator queue.\n            If unspecified, `max_queue_size` will default to 10.\n        workers: Integer. Used for generator or `keras.utils.Sequence` input\n            only. Maximum number of processes to spin up when using\n            process-based threading. If unspecified, `workers` will default\n            to 1.\n        use_multiprocessing: Boolean. Used for generator or\n            `keras.utils.Sequence` input only. If `True`, use process-based\n            threading. If unspecified, `use_multiprocessing` will default to\n            `False`. Note that because this implementation relies on\n            multiprocessing, you should not pass non-picklable arguments to\n            the generator as they can't be passed easily to children processes.\n\n    See the discussion of `Unpacking behavior for iterator-like inputs` for\n    `Model.fit`. Note that Model.predict uses the same interpretation rules as\n    `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n    three methods.\n\n    Returns:\n        Numpy array(s) of predictions.\n\n    Raises:\n        RuntimeError: If `model.predict` is wrapped in `tf.function`.\n        ValueError: In case of mismatch between the provided\n            input data and the model's expectations,\n            or in case a stateful model receives a number of samples\n            that is not a multiple of the batch size.\n    \"\"\"\n    version_utils.disallow_legacy_graph('Model', 'predict')\n    self._check_call_args('predict')\n    _disallow_inside_tf_function('predict')\n\n    # TODO(yashkatariya): Cache model on the coordinator for faster prediction.\n    # If running under PSS, then swap it with OneDeviceStrategy so that\n    # execution will run on the coordinator.\n    original_pss_strategy = None\n    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n      original_pss_strategy = self.distribute_strategy\n      self._distribution_strategy = None\n\n    # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not\n    # needed in `.predict()` because all the predictions happen on the\n    # coordinator/locally.\n    if self._cluster_coordinator:\n      self._cluster_coordinator = None\n\n    outputs = None\n    with self.distribute_strategy.scope():\n      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n      dataset_types = (dataset_ops.DatasetV1, dataset_ops.DatasetV2)\n      if (self._in_multi_worker_mode() or _is_tpu_multi_host(\n          self.distribute_strategy)) and isinstance(x, dataset_types):\n        try:\n          options = options_lib.Options()\n          data_option = options_lib.AutoShardPolicy.DATA\n          options.experimental_distribute.auto_shard_policy = data_option\n          x = x.with_options(options)\n        except ValueError:\n          warnings.warn('Using Model.predict with '\n                        'MultiWorkerDistributionStrategy or TPUStrategy and '\n                        'AutoShardPolicy.FILE might lead to out-of-order result'\n                        '. Consider setting it to AutoShardPolicy.DATA.')\n\n      data_handler = data_adapter.get_data_handler(\n          x=x,\n          batch_size=batch_size,\n          steps_per_epoch=steps,\n          initial_epoch=0,\n          epochs=1,\n          max_queue_size=max_queue_size,\n          workers=workers,\n          use_multiprocessing=use_multiprocessing,\n          model=self,\n          steps_per_execution=self._steps_per_execution)\n\n      # Container that configures and calls `tf.keras.Callback`s.\n      if not isinstance(callbacks, callbacks_module.CallbackList):\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=True,\n            add_progbar=verbose != 0,\n            model=self,\n            verbose=verbose,\n            epochs=1,\n            steps=data_handler.inferred_steps)\n\n      self.predict_function = self.make_predict_function()\n      self._predict_counter.assign(0)\n      callbacks.on_predict_begin()\n      batch_outputs = None\n      for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n        with data_handler.catch_stop_iteration():\n          for step in data_handler.steps():\n            callbacks.on_predict_batch_begin(step)\n            tmp_batch_outputs = self.predict_function(iterator)\n            if data_handler.should_sync:\n              context.async_wait()\n            batch_outputs = tmp_batch_outputs  # No error, now safe to assign.\n            if outputs is None:\n              outputs = nest.map_structure(lambda batch_output: [batch_output],\n                                           batch_outputs)\n            else:\n              nest.map_structure_up_to(\n                  batch_outputs,\n                  lambda output, batch_output: output.append(batch_output),\n                  outputs, batch_outputs)\n            end_step = step + data_handler.step_increment\n            callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})\n      if batch_outputs is None:\n        raise ValueError('Expect x to be a non-empty array or dataset.')\n      callbacks.on_predict_end()\n    all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)\n\n    # If originally PSS strategy was used, then replace it back since predict\n    # is running under `OneDeviceStrategy` after the swap and once its done\n    # we need to replace it back to PSS again.\n    if original_pss_strategy is not None:\n      self._distribution_strategy = original_pss_strategy\n\n    return tf_utils.sync_to_numpy_or_python_type(all_outputs)\n\n  def reset_metrics(self):\n    \"\"\"Resets the state of all the metrics in the model.\n\n    Examples:\n\n    >>> inputs = tf.keras.layers.Input(shape=(3,))\n    >>> outputs = tf.keras.layers.Dense(2)(inputs)\n    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n\n    >>> x = np.random.random((2, 3))\n    >>> y = np.random.randint(0, 2, (2, 2))\n    >>> _ = model.fit(x, y, verbose=0)\n    >>> assert all(float(m.result()) for m in model.metrics)\n\n    >>> model.reset_metrics()\n    >>> assert all(float(m.result()) == 0 for m in model.metrics)\n\n    \"\"\"\n    for m in self.metrics:\n      m.reset_state()\n\n  def train_on_batch(self,\n                     x,\n                     y=None,\n                     sample_weight=None,\n                     class_weight=None,\n                     reset_metrics=True,\n                     return_dict=False):\n    \"\"\"Runs a single gradient update on a single batch of data.\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays\n              (in case the model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors\n              (in case the model has multiple inputs).\n          - A dict mapping input names to the corresponding array/tensors,\n              if the model has named inputs.\n        y: Target data. Like the input data `x`, it could be either Numpy\n          array(s) or TensorFlow tensor(s). It should be consistent with `x`\n          (you cannot have Numpy inputs and tensor targets, or inversely).\n        sample_weight: Optional array of the same length as x, containing\n          weights to apply to the model's loss for each sample. In the case of\n          temporal data, you can pass a 2D array with shape (samples,\n          sequence_length), to apply a different weight to every timestep of\n          every sample.\n        class_weight: Optional dictionary mapping class indices (integers) to a\n          weight (float) to apply to the model's loss for the samples from this\n          class during training. This can be useful to tell the model to \"pay\n          more attention\" to samples from an under-represented class.\n        reset_metrics: If `True`, the metrics returned will be only for this\n          batch. If `False`, the metrics will be statefully accumulated across\n          batches.\n        return_dict: If `True`, loss and metric results are returned as a dict,\n          with each key being the name of the metric. If `False`, they are\n          returned as a list.\n\n    Returns:\n        Scalar training loss\n        (if the model has a single output and no metrics)\n        or list of scalars (if the model has multiple outputs\n        and/or metrics). The attribute `model.metrics_names` will give you\n        the display labels for the scalar outputs.\n\n    Raises:\n      RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n      ValueError: In case of invalid user-provided arguments.\n    \"\"\"\n    self._assert_compile_was_called()\n    self._check_call_args('train_on_batch')\n    _disallow_inside_tf_function('train_on_batch')\n    with self.distribute_strategy.scope(), \\\n         training_utils.RespectCompiledTrainableState(self):\n      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n                                                    y, sample_weight,\n                                                    class_weight)\n      self.train_function = self.make_train_function()\n      logs = self.train_function(iterator)\n\n    if reset_metrics:\n      self.reset_metrics()\n    logs = tf_utils.sync_to_numpy_or_python_type(logs)\n    if return_dict:\n      return logs\n    else:\n      return flatten_metrics_in_order(logs, self.metrics_names)\n\n  def test_on_batch(self,\n                    x,\n                    y=None,\n                    sample_weight=None,\n                    reset_metrics=True,\n                    return_dict=False):\n    \"\"\"Test the model on a single batch of samples.\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays (in case the\n              model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors (in case the model has\n              multiple inputs).\n          - A dict mapping input names to the corresponding array/tensors, if\n              the model has named inputs.\n        y: Target data. Like the input data `x`, it could be either Numpy\n          array(s) or TensorFlow tensor(s). It should be consistent with `x`\n          (you cannot have Numpy inputs and tensor targets, or inversely).\n        sample_weight: Optional array of the same length as x, containing\n          weights to apply to the model's loss for each sample. In the case of\n          temporal data, you can pass a 2D array with shape (samples,\n          sequence_length), to apply a different weight to every timestep of\n          every sample.\n        reset_metrics: If `True`, the metrics returned will be only for this\n          batch. If `False`, the metrics will be statefully accumulated across\n          batches.\n        return_dict: If `True`, loss and metric results are returned as a dict,\n          with each key being the name of the metric. If `False`, they are\n          returned as a list.\n\n    Returns:\n        Scalar test loss (if the model has a single output and no metrics)\n        or list of scalars (if the model has multiple outputs\n        and/or metrics). The attribute `model.metrics_names` will give you\n        the display labels for the scalar outputs.\n\n    Raises:\n        RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n        ValueError: In case of invalid user-provided arguments.\n    \"\"\"\n    self._assert_compile_was_called()\n    self._check_call_args('test_on_batch')\n    _disallow_inside_tf_function('test_on_batch')\n    with self.distribute_strategy.scope():\n      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n                                                    y, sample_weight)\n      self.test_function = self.make_test_function()\n      logs = self.test_function(iterator)\n\n    if reset_metrics:\n      self.reset_metrics()\n    logs = tf_utils.sync_to_numpy_or_python_type(logs)\n    if return_dict:\n      return logs\n    else:\n      return flatten_metrics_in_order(logs, self.metrics_names)\n\n  def predict_on_batch(self, x):\n    \"\"\"Returns predictions for a single batch of samples.\n\n    Args:\n        x: Input data. It could be:\n          - A Numpy array (or array-like), or a list of arrays (in case the\n              model has multiple inputs).\n          - A TensorFlow tensor, or a list of tensors (in case the model has\n              multiple inputs).\n\n    Returns:\n        Numpy array(s) of predictions.\n\n    Raises:\n        RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n        ValueError: In case of mismatch between given number of inputs and\n          expectations of the model.\n    \"\"\"\n    self._check_call_args('predict_on_batch')\n    _disallow_inside_tf_function('predict_on_batch')\n    with self.distribute_strategy.scope():\n      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x)\n      self.predict_function = self.make_predict_function()\n      outputs = self.predict_function(iterator)\n    return tf_utils.sync_to_numpy_or_python_type(outputs)\n\n  def fit_generator(self,\n                    generator,\n                    steps_per_epoch=None,\n                    epochs=1,\n                    verbose=1,\n                    callbacks=None,\n                    validation_data=None,\n                    validation_steps=None,\n                    validation_freq=1,\n                    class_weight=None,\n                    max_queue_size=10,\n                    workers=1,\n                    use_multiprocessing=False,\n                    shuffle=True,\n                    initial_epoch=0):\n    \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\n\n    DEPRECATED:\n      `Model.fit` now supports generators, so there is no longer any need to use\n      this endpoint.\n    \"\"\"\n    warnings.warn('`Model.fit_generator` is deprecated and '\n                  'will be removed in a future version. '\n                  'Please use `Model.fit`, which supports generators.')\n    return self.fit(\n        generator,\n        steps_per_epoch=steps_per_epoch,\n        epochs=epochs,\n        verbose=verbose,\n        callbacks=callbacks,\n        validation_data=validation_data,\n        validation_steps=validation_steps,\n        validation_freq=validation_freq,\n        class_weight=class_weight,\n        max_queue_size=max_queue_size,\n        workers=workers,\n        use_multiprocessing=use_multiprocessing,\n        shuffle=shuffle,\n        initial_epoch=initial_epoch)\n\n  def evaluate_generator(self,\n                         generator,\n                         steps=None,\n                         callbacks=None,\n                         max_queue_size=10,\n                         workers=1,\n                         use_multiprocessing=False,\n                         verbose=0):\n    \"\"\"Evaluates the model on a data generator.\n\n    DEPRECATED:\n      `Model.evaluate` now supports generators, so there is no longer any need\n      to use this endpoint.\n    \"\"\"\n    warnings.warn('`Model.evaluate_generator` is deprecated and '\n                  'will be removed in a future version. '\n                  'Please use `Model.evaluate`, which supports generators.')\n    self._check_call_args('evaluate_generator')\n\n    return self.evaluate(\n        generator,\n        steps=steps,\n        max_queue_size=max_queue_size,\n        workers=workers,\n        use_multiprocessing=use_multiprocessing,\n        verbose=verbose,\n        callbacks=callbacks)\n\n  def predict_generator(self,\n                        generator,\n                        steps=None,\n                        callbacks=None,\n                        max_queue_size=10,\n                        workers=1,\n                        use_multiprocessing=False,\n                        verbose=0):\n    \"\"\"Generates predictions for the input samples from a data generator.\n\n    DEPRECATED:\n      `Model.predict` now supports generators, so there is no longer any need\n      to use this endpoint.\n    \"\"\"\n    warnings.warn('`Model.predict_generator` is deprecated and '\n                  'will be removed in a future version. '\n                  'Please use `Model.predict`, which supports generators.')\n    return self.predict(\n        generator,\n        steps=steps,\n        max_queue_size=max_queue_size,\n        workers=workers,\n        use_multiprocessing=use_multiprocessing,\n        verbose=verbose,\n        callbacks=callbacks)\n\n  ######################################################################\n  # Functions below are not training related. They are for model weights\n  # tracking, save/load, serialization, etc.\n  ######################################################################\n\n  @property\n  def trainable_weights(self):\n    self._assert_weights_created()\n    if not self._trainable:\n      return []\n    trainable_variables = []\n    for trackable_obj in self._self_tracked_trackables:\n      trainable_variables += trackable_obj.trainable_variables\n    trainable_variables += self._trainable_weights\n    return self._dedup_weights(trainable_variables)\n\n  @property\n  def non_trainable_weights(self):\n    self._assert_weights_created()\n    non_trainable_variables = []\n    for trackable_obj in self._self_tracked_trackables:\n      non_trainable_variables += trackable_obj.non_trainable_variables\n\n    if not self._trainable:\n      # Return order is all trainable vars, then all non-trainable vars.\n      trainable_variables = []\n      for trackable_obj in self._self_tracked_trackables:\n        trainable_variables += trackable_obj.trainable_variables\n\n      non_trainable_variables = (\n          trainable_variables + self._trainable_weights +\n          non_trainable_variables + self._non_trainable_weights)\n    else:\n      non_trainable_variables = (\n          non_trainable_variables + self._non_trainable_weights)\n\n    return self._dedup_weights(non_trainable_variables)\n\n  def get_weights(self):\n    \"\"\"Retrieves the weights of the model.\n\n    Returns:\n        A flat list of Numpy arrays.\n    \"\"\"\n    with self.distribute_strategy.scope():\n      return super(Model, self).get_weights()\n\n  def save(self,\n           filepath,\n           overwrite=True,\n           include_optimizer=True,\n           save_format=None,\n           signatures=None,\n           options=None,\n           save_traces=True):\n    # pylint: disable=line-too-long\n    \"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\n\n    Please see `tf.keras.models.save_model` or the\n    [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n    for details.\n\n    Args:\n        filepath: String, PathLike, path to SavedModel or H5 file to save the\n            model.\n        overwrite: Whether to silently overwrite any existing file at the\n            target location, or provide the user with a manual prompt.\n        include_optimizer: If True, save optimizer's state together.\n        save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n            model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n            and 'h5' in TF 1.X.\n        signatures: Signatures to save with the SavedModel. Applicable to the\n            'tf' format only. Please see the `signatures` argument in\n            `tf.saved_model.save` for details.\n        options: (only applies to SavedModel format)\n            `tf.saved_model.SaveOptions` object that specifies options for\n            saving to SavedModel.\n        save_traces: (only applies to SavedModel format) When enabled, the\n            SavedModel will store the function traces for each layer. This\n            can be disabled, so that only the configs of each layer are stored.\n            Defaults to `True`. Disabling this will decrease serialization time\n            and reduce file size, but it requires that all custom layers/models\n            implement a `get_config()` method.\n\n    Example:\n\n    ```python\n    from keras.models import load_model\n\n    model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n    del model  # deletes the existing model\n\n    # returns a compiled model\n    # identical to the previous one\n    model = load_model('my_model.h5')\n    ```\n    \"\"\"\n    # pylint: enable=line-too-long\n    save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n                    signatures, options, save_traces)\n\n  def save_weights(self,\n                   filepath,\n                   overwrite=True,\n                   save_format=None,\n                   options=None):\n    \"\"\"Saves all layer weights.\n\n    Either saves in HDF5 or in TensorFlow format based on the `save_format`\n    argument.\n\n    When saving in HDF5 format, the weight file has:\n      - `layer_names` (attribute), a list of strings\n          (ordered names of model layers).\n      - For every layer, a `group` named `layer.name`\n          - For every such layer group, a group attribute `weight_names`,\n              a list of strings\n              (ordered names of weights tensor of the layer).\n          - For every weight in the layer, a dataset\n              storing the weight value, named after the weight tensor.\n\n    When saving in TensorFlow format, all objects referenced by the network are\n    saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n    instances or `Optimizer` instances assigned to object attributes. For\n    networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n    outputs)`, `Layer` instances used by the network are tracked/saved\n    automatically. For user-defined classes which inherit from `tf.keras.Model`,\n    `Layer` instances must be assigned to object attributes, typically in the\n    constructor. See the documentation of `tf.train.Checkpoint` and\n    `tf.keras.Model` for details.\n\n    While the formats are the same, do not mix `save_weights` and\n    `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n    loaded using `Model.load_weights`. Checkpoints saved using\n    `tf.train.Checkpoint.save` should be restored using the corresponding\n    `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n    `save_weights` for training checkpoints.\n\n    The TensorFlow format matches objects and variables by starting at a root\n    object, `self` for `save_weights`, and greedily matching attribute\n    names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n    is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n    means saving a `tf.keras.Model` using `save_weights` and loading into a\n    `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n    the `Model`'s variables. See the [guide to training\n    checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n    on the TensorFlow format.\n\n    Args:\n        filepath: String or PathLike, path to the file to save the weights to.\n            When saving in TensorFlow format, this is the prefix used for\n            checkpoint files (multiple files are generated). Note that the '.h5'\n            suffix causes weights to be saved in HDF5 format.\n        overwrite: Whether to silently overwrite any existing file at the\n            target location, or provide the user with a manual prompt.\n        save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n            '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n            `None` defaults to 'tf'.\n        options: Optional `tf.train.CheckpointOptions` object that specifies\n            options for saving weights.\n\n    Raises:\n        ImportError: If h5py is not available when attempting to save in HDF5\n            format.\n        ValueError: For invalid/unknown format arguments.\n    \"\"\"\n    self._assert_weights_created()\n    filepath = path_to_string(filepath)\n    filepath_is_h5 = saving_utils.is_hdf5_filepath(filepath)\n    if save_format is None:\n      if filepath_is_h5:\n        save_format = 'h5'\n      else:\n        save_format = 'tf'\n    else:\n      user_format = save_format.lower().strip()\n      if user_format in ('tensorflow', 'tf'):\n        save_format = 'tf'\n      elif user_format in ('hdf5', 'h5', 'keras'):\n        save_format = 'h5'\n      else:\n        raise ValueError(\n            'Unknown format \"%s\". Was expecting one of {\"tf\", \"h5\"}.' % (\n                save_format,))\n    if save_format == 'tf' and filepath_is_h5:\n      raise ValueError(\n          ('save_weights got save_format=\"tf\"/\"tensorflow\", but the '\n           'filepath (\"%s\") looks like an HDF5 file. Omit the \".h5\"/\".keras\" '\n           'when saving in TensorFlow format.')\n          % filepath)\n\n    if save_format == 'h5' and h5py is None:\n      raise ImportError(\n          '`save_weights` requires h5py when saving in hdf5.')\n    if save_format == 'tf':\n      check_filepath = filepath + '.index'\n    else:\n      check_filepath = filepath\n    # If file exists and should not be overwritten:\n    if not overwrite and os.path.isfile(check_filepath):\n      proceed = ask_to_proceed_with_overwrite(check_filepath)\n      if not proceed:\n        return\n    if save_format == 'h5':\n      with h5py.File(filepath, 'w') as f:\n        hdf5_format.save_weights_to_hdf5_group(f, self.layers)\n    else:\n      if context.executing_eagerly():\n        session = None\n      else:\n        session = backend.get_session()\n      self._trackable_saver.save(filepath, session=session, options=options)\n      # Record this checkpoint so it's visible from tf.train.latest_checkpoint.\n      checkpoint_management.update_checkpoint_state_internal(\n          save_dir=os.path.dirname(filepath),\n          model_checkpoint_path=filepath,\n          save_relative_paths=True,\n          all_model_checkpoint_paths=[filepath])\n\n  def load_weights(self,\n                   filepath,\n                   by_name=False,\n                   skip_mismatch=False,\n                   options=None):\n    \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n\n    If `by_name` is False weights are loaded based on the network's\n    topology. This means the architecture should be the same as when the weights\n    were saved.  Note that layers that don't have weights are not taken into\n    account in the topological ordering, so adding or removing layers is fine as\n    long as they don't have weights.\n\n    If `by_name` is True, weights are loaded into layers only if they share the\n    same name. This is useful for fine-tuning or transfer-learning models where\n    some of the layers have changed.\n\n    Only topological loading (`by_name=False`) is supported when loading weights\n    from the TensorFlow format. Note that topological loading differs slightly\n    between TensorFlow and HDF5 formats for user-defined classes inheriting from\n    `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n    TensorFlow format loads based on the object-local names of attributes to\n    which layers are assigned in the `Model`'s constructor.\n\n    Args:\n        filepath: String, path to the weights file to load. For weight files in\n            TensorFlow format, this is the file prefix (the same as was passed\n            to `save_weights`). This can also be a path to a SavedModel\n            saved from `model.save`.\n        by_name: Boolean, whether to load weights by name or by topological\n            order. Only topological loading is supported for weight files in\n            TensorFlow format.\n        skip_mismatch: Boolean, whether to skip loading of layers where there is\n            a mismatch in the number of weights, or a mismatch in the shape of\n            the weight (only valid when `by_name=True`).\n        options: Optional `tf.train.CheckpointOptions` object that specifies\n            options for loading weights.\n\n    Returns:\n        When loading a weight file in TensorFlow format, returns the same status\n        object as `tf.train.Checkpoint.restore`. When graph building, restore\n        ops are run automatically as soon as the network is built (on first call\n        for user-defined classes inheriting from `Model`, immediately if it is\n        already built).\n\n        When loading weights in HDF5 format, returns `None`.\n\n    Raises:\n        ImportError: If h5py is not available and the weight file is in HDF5\n            format.\n        ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n          `False`.\n    \"\"\"\n    if backend.is_tpu_strategy(self._distribution_strategy):\n      if (self._distribution_strategy.extended.steps_per_run > 1 and\n          (not saving_utils.is_hdf5_filepath(filepath))):\n        raise ValueError('Load weights is not yet supported with TPUStrategy '\n                         'with steps_per_run greater than 1.')\n    if skip_mismatch and not by_name:\n      raise ValueError(\n          'When calling model.load_weights, skip_mismatch can only be set to '\n          'True when by_name is True.')\n\n    filepath, save_format = _detect_save_format(filepath)\n    if save_format == 'tf':\n      status = self._trackable_saver.restore(filepath, options)\n      if by_name:\n        raise NotImplementedError(\n            'Weights may only be loaded based on topology into Models when '\n            'loading TensorFlow-formatted weights (got by_name=True to '\n            'load_weights).')\n      if not context.executing_eagerly():\n        session = backend.get_session()\n        # Restore existing variables (if any) immediately, and set up a\n        # streaming restore for any variables created in the future.\n        trackable_utils.streaming_restore(status=status, session=session)\n      status.assert_nontrivial_match()\n    else:\n      status = None\n      if h5py is None:\n        raise ImportError(\n            '`load_weights` requires h5py when loading weights from HDF5.')\n      if not self._is_graph_network and not self.built:\n        raise ValueError(\n            'Unable to load weights saved in HDF5 format into a subclassed '\n            'Model which has not created its variables yet. Call the Model '\n            'first, then load the weights.')\n      self._assert_weights_created()\n      with h5py.File(filepath, 'r') as f:\n        if 'layer_names' not in f.attrs and 'model_weights' in f:\n          f = f['model_weights']\n        if by_name:\n          hdf5_format.load_weights_from_hdf5_group_by_name(\n              f, self.layers, skip_mismatch=skip_mismatch)\n        else:\n          hdf5_format.load_weights_from_hdf5_group(f, self.layers)\n\n    # Perform any layer defined finalization of the layer state.\n    for layer in self.layers:\n      layer.finalize_state()\n    return status\n\n  def _updated_config(self):\n    \"\"\"Util shared between different serialization methods.\n\n    Returns:\n        Model config with Keras version information added.\n    \"\"\"\n    from tensorflow.python.keras import __version__ as keras_version  # pylint: disable=g-import-not-at-top\n\n    config = self.get_config()\n    model_config = {\n        'class_name': self.__class__.__name__,\n        'config': config,\n        'keras_version': keras_version,\n        'backend': backend.backend()\n    }\n    return model_config\n\n  def get_config(self):\n    raise NotImplementedError\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None):\n    # `from_config` assumes `cls` is either `Functional` or a child class of\n    # `Functional`. In the case that `cls` is meant to behave like a child class\n    # of `Functional` but only inherits from the `Model` class, we have to call\n    # `cls(...)` instead of `Functional.from_config`.\n    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\n    with generic_utils.SharedObjectLoadingScope():\n      input_tensors, output_tensors, created_layers = (\n          functional.reconstruct_from_config(config, custom_objects))\n      # Initialize a model belonging to `cls`, which can be user-defined or\n      # `Functional`.\n      model = cls(inputs=input_tensors, outputs=output_tensors,\n                  name=config.get('name'))\n      functional.connect_ancillary_layers(model, created_layers)\n      return model\n\n  def to_json(self, **kwargs):\n    \"\"\"Returns a JSON string containing the network configuration.\n\n    To load a network from a JSON save file, use\n    `keras.models.model_from_json(json_string, custom_objects={})`.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `json.dumps()`.\n\n    Returns:\n        A JSON string.\n    \"\"\"\n    model_config = self._updated_config()\n    return json.dumps(\n        model_config, default=json_utils.get_json_type, **kwargs)\n\n  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    Note: Since TF 2.6, this method is no longer supported and will raise a\n    RuntimeError.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        RuntimeError: announces that the method poses a security risk\n    \"\"\"\n    raise RuntimeError(\n        'Method `model.to_yaml()` has been removed due to security risk of '\n        'arbitrary code execution. Please use `model.to_json()` instead.'\n    )\n\n  def reset_states(self):\n    for layer in self.layers:\n      if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):\n        layer.reset_states()\n\n  @property\n  @doc_controls.do_not_generate_docs\n  def state_updates(self):\n    \"\"\"Deprecated, do NOT use!\n\n    Returns the `updates` from all layers that are stateful.\n\n    This is useful for separating training updates and\n    state updates, e.g. when we need to update a layer's internal state\n    during prediction.\n\n    Returns:\n        A list of update ops.\n    \"\"\"\n    warnings.warn('`Model.state_updates` will be removed in a future version. '\n                  'This property should not be used in TensorFlow 2.0, '\n                  'as `updates` are applied automatically.')\n    state_updates = []\n    for layer in self.layers:\n      if getattr(layer, 'stateful', False):\n        if hasattr(layer, 'updates'):\n          state_updates += layer.updates\n    return state_updates\n\n  @property\n  def weights(self):\n    \"\"\"Returns the list of all layer variables/weights.\n\n    Note: This will not track the weights of nested `tf.Modules` that are not\n    themselves Keras layers.\n\n    Returns:\n      A list of variables.\n    \"\"\"\n    return self._dedup_weights(self._undeduplicated_weights)\n\n  @property\n  def _undeduplicated_weights(self):\n    \"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\n    self._assert_weights_created()\n    weights = []\n    for layer in self._self_tracked_trackables:\n      weights += layer.variables\n    weights += (self._trainable_weights + self._non_trainable_weights)\n    return weights\n\n  def summary(self, line_length=None, positions=None, print_fn=None):\n    \"\"\"Prints a string summary of the network.\n\n    Args:\n        line_length: Total length of printed lines\n            (e.g. set this to adapt the display to different\n            terminal window sizes).\n        positions: Relative or absolute positions of log elements\n            in each line. If not provided,\n            defaults to `[.33, .55, .67, 1.]`.\n        print_fn: Print function to use. Defaults to `print`.\n            It will be called on each line of the summary.\n            You can set it to a custom function\n            in order to capture the string summary.\n\n    Raises:\n        ValueError: if `summary()` is called before the model is built.\n    \"\"\"\n    if not self.built:\n      raise ValueError('This model has not yet been built. '\n                       'Build the model first by calling `build()` or calling '\n                       '`fit()` with some data, or specify '\n                       'an `input_shape` argument in the first layer(s) for '\n                       'automatic build.')\n    layer_utils.print_summary(self,\n                              line_length=line_length,\n                              positions=positions,\n                              print_fn=print_fn)\n\n  @property\n  def layers(self):\n    return list(self._flatten_layers(include_self=False, recursive=False))\n\n  def get_layer(self, name=None, index=None):\n    \"\"\"Retrieves a layer based on either its name (unique) or index.\n\n    If `name` and `index` are both provided, `index` will take precedence.\n    Indices are based on order of horizontal graph traversal (bottom-up).\n\n    Args:\n        name: String, name of layer.\n        index: Integer, index of layer.\n\n    Returns:\n        A layer instance.\n\n    Raises:\n        ValueError: In case of invalid layer name or index.\n    \"\"\"\n    # TODO(fchollet): We could build a dictionary based on layer names\n    # since they are constant, but we have not done that yet.\n    if index is not None and name is not None:\n      raise ValueError('Provide only a layer name or a layer index.')\n\n    if index is not None:\n      if len(self.layers) <= index:\n        raise ValueError('Was asked to retrieve layer at index ' + str(index) +\n                         ' but model only has ' + str(len(self.layers)) +\n                         ' layers.')\n      else:\n        return self.layers[index]\n\n    if name is not None:\n      for layer in self.layers:\n        if layer.name == name:\n          return layer\n      raise ValueError('No such layer: ' + name + '.')\n    raise ValueError('Provide either a layer name or layer index.')\n\n  @trackable.no_automatic_dependency_tracking\n  def _set_save_spec(self, inputs):\n    if self._saved_model_inputs_spec is not None:\n      return  # Already set.\n\n    input_names = self.input_names\n    if not input_names:\n      input_names = compile_utils.create_pseudo_input_names(inputs)\n\n    flat_inputs = nest.flatten(inputs)\n    specs = []\n    for name, tensor in zip(input_names, flat_inputs):\n      specs.append(\n          tf_utils.get_tensor_spec(tensor, dynamic_batch=False, name=name))\n    specs = nest.pack_sequence_as(inputs, specs)\n\n    self._saved_model_inputs_spec = specs\n\n    # Store the input shapes\n    if (self.__class__.__name__ == 'Sequential' and\n        self._build_input_shape is None):\n      self._build_input_shape = nest.map_structure(\n          lambda x: None if x is None else x.shape, specs)\n\n  def _assert_weights_created(self):\n    \"\"\"Asserts that all the weights for the model have been created.\n\n    For a non-dynamic model, the weights must already be created after the\n    layer has been called. For a dynamic model, the exact list of weights can\n    never be known for certain since it may change at any time during execution.\n\n    We run this check right before accessing weights or getting the Numpy value\n    for the current weights. Otherwise, if the layer has never been called,\n    the user would just get an empty list, which is misleading.\n\n    Raises:\n      ValueError: if the weights of the network has not yet been created.\n    \"\"\"\n    if self.dynamic:\n      return\n\n    if ('build' in self.__class__.__dict__ and\n        self.__class__ != Model and\n        not self.built):\n      # For any model that has customized build() method but hasn't\n      # been invoked yet, this will cover both sequential and subclass model.\n      # Also make sure to exclude Model class itself which has build() defined.\n      raise ValueError('Weights for model %s have not yet been created. '\n                       'Weights are created when the Model is first called on '\n                       'inputs or `build()` is called with an `input_shape`.' %\n                       self.name)\n\n  def _check_call_args(self, method_name):\n    \"\"\"Check that `call` has only one positional arg.\"\"\"\n    # Always allow first arg, regardless of arg name.\n    fullargspec = self._call_full_argspec\n    if fullargspec.defaults:\n      positional_args = fullargspec.args[:-len(fullargspec.defaults)]\n    else:\n      positional_args = fullargspec.args\n    if 'training' in positional_args:\n      positional_args.remove('training')\n\n    # self and first arg can be positional.\n    if len(positional_args) > 2:\n      extra_args = positional_args[2:]\n      raise ValueError(\n          'Models passed to `' + method_name + '` can only have `training` '\n          'and the first argument in `call` as positional arguments, '\n          'found: ' + str(extra_args) + '.')\n\n  def _validate_compile(self, optimizer, metrics, **kwargs):\n    \"\"\"Performs validation checks for the default `compile`.\"\"\"\n    if any(\n        isinstance(opt, optimizer_v1.Optimizer)\n        for opt in nest.flatten(optimizer)):\n      raise ValueError(\n          '`tf.compat.v1.keras` Optimizer (', optimizer, ') is '\n          'not supported when eager execution is enabled. Use a '\n          '`tf.keras` Optimizer instead, or disable eager '\n          'execution.')\n\n    kwargs.pop('cloning', None)  # Legacy DistStrat argument, never used.\n    kwargs.pop('experimental_run_tf_function', None)  # Always `True`.\n    if kwargs.pop('distribute', None) is not None:\n      raise ValueError(\n          'Distribute argument in compile is not available in TF 2.0 please '\n          'create the model under the distribution strategy scope.')\n    if kwargs.pop('target_tensors', None) is not None:\n      raise ValueError(\n          'target_tensors argument is not supported when executing eagerly.')\n    invalid_kwargs = set(kwargs) - {'sample_weight_mode'}\n    if invalid_kwargs:\n      raise TypeError('Invalid keyword argument(s) in `compile`: %s' %\n                      (invalid_kwargs,))\n\n    # Model must be created and compiled with the same DistStrat.\n    if self.built and ds_context.has_strategy():\n      strategy = ds_context.get_strategy()\n      for v in self.variables:\n        if not strategy.extended.variable_created_in_scope(v):\n          raise ValueError(\n              'Variable (%s) was not created in the distribution strategy '\n              'scope of (%s). It is most likely due to not all layers or '\n              'the model or optimizer being created outside the distribution '\n              'strategy scope. Try to make sure your code looks similar '\n              'to the following.\\n'\n              'with strategy.scope():\\n'\n              '  model=_create_model()\\n'\n              '  model.compile(...)' % (v, strategy))\n\n    # Model metrics must be created in the same distribution strategy scope\n    # as the model.\n    strategy = self.distribute_strategy\n    for metric in nest.flatten(metrics):\n      for v in getattr(metric, 'variables', []):\n        if not strategy.extended.variable_created_in_scope(v):\n          raise ValueError(\n              'Metric (%s) passed to model.compile was created inside of a '\n              'different distribution strategy scope than the model. All '\n              'metrics must be created in the same distribution strategy '\n              'scope as the model (in this case %s). If you pass in a string '\n              'identifier for a metric to compile the metric will '\n              'automatically be created in the correct distribution '\n              'strategy scope.' % (metric, strategy)\n          )\n\n    # Model metrics must be created in the same distribution strategy scope\n    # as the model.\n    for opt in nest.flatten(optimizer):\n      for v in getattr(opt, '_weights', []):\n        if not strategy.extended.variable_created_in_scope(v):\n          raise ValueError(\n              'Optimizer (%s) passed to model.compile was created inside of a '\n              'different distribution strategy scope than the model. All '\n              'optimizers must be created in the same distribution strategy '\n              'scope as the model (in this case %s). If you pass in a string '\n              'identifier for an optimizer to compile the optimizer will '\n              'automatically be created in the correct distribution '\n              'strategy scope.' % (opt, strategy))\n\n  def _maybe_load_initial_epoch_from_ckpt(self, initial_epoch):\n    \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.\n\n    Refer to tensorflow/python/keras/distribute/worker_training_state.py\n    for more information.\n\n    Args:\n      initial_epoch: The original initial_epoch user passes in in `fit()`.\n\n    Returns:\n      If the training is recovering from previous failure under multi-worker\n      training setting, return the epoch the training is supposed to continue\n      at. Otherwise, return the `initial_epoch` the user passes in.\n    \"\"\"\n    if self._training_state is not None:\n      return self._training_state.maybe_load_initial_epoch_from_ckpt(\n          initial_epoch, mode=ModeKeys.TRAIN)\n    return initial_epoch\n\n  def _assert_compile_was_called(self):\n    # Checks whether `compile` has been called. If it has been called,\n    # then the optimizer is set. This is different from whether the\n    # model is compiled\n    # (i.e. whether the model is built and its inputs/outputs are set).\n    if not self._is_compiled:\n      raise RuntimeError('You must compile your model before '\n                         'training/testing. '\n                         'Use `model.compile(optimizer, loss)`.')\n\n  def _set_inputs(self, inputs, outputs=None, training=None):\n    \"\"\"This method is for compat with Modelv1. Only inputs are needed here.\"\"\"\n    self._set_save_spec(inputs)\n\n  @property\n  def _trackable_saved_model_saver(self):\n    return model_serialization.ModelSavedModelSaver(self)\n\n  def _list_functions_for_serialization(self, serialization_cache):\n    # SavedModel needs to ignore the execution functions.\n    train_function = self.train_function\n    test_function = self.test_function\n    predict_function = self.predict_function\n    train_tf_function = self.train_tf_function\n    self.train_function = None\n    self.test_function = None\n    self.predict_function = None\n    self.train_tf_function = None\n    functions = super(\n        Model, self)._list_functions_for_serialization(serialization_cache)\n    self.train_function = train_function\n    self.test_function = test_function\n    self.predict_function = predict_function\n    self.train_tf_function = train_tf_function\n    return functions\n\n  def _should_eval(self, epoch, validation_freq):\n    epoch = epoch + 1  # one-index the user-facing epoch.\n    if isinstance(validation_freq, int):\n      return epoch % validation_freq == 0\n    elif isinstance(validation_freq, list):\n      return epoch in validation_freq\n    else:\n      raise ValueError('Expected `validation_freq` to be a list or int.')\n\n  ######################################################################\n  # Functions below exist only as v1 / v2 compatibility shims.\n  ######################################################################\n\n  def _get_compile_args(self, user_metrics=True):\n    \"\"\"Used for saving or cloning a Model.\n\n    Args:\n      user_metrics: Whether to return user-supplied metrics or `Metric` objects.\n        Defaults to returning the user-supplied metrics.\n\n    Returns:\n      Dictionary of arguments that were used when compiling the model.\n    \"\"\"\n    self._assert_compile_was_called()\n    # pylint: disable=protected-access\n\n    saved_metrics = self.compiled_metrics._user_metrics\n    saved_weighted_metrics = self.compiled_metrics._user_weighted_metrics\n\n    if not user_metrics:\n      if saved_metrics is not None:\n        saved_metrics = self.compiled_metrics._metrics\n      if saved_weighted_metrics is not None:\n        saved_weighted_metrics = self.compiled_metrics._weighted_metrics\n\n    compile_args = {\n        'optimizer': self.optimizer,\n        'loss': self.compiled_loss._user_losses,\n        'metrics': saved_metrics,\n        'weighted_metrics': saved_weighted_metrics,\n        'loss_weights': self.compiled_loss._user_loss_weights,\n    }\n    # pylint: enable=protected-access\n    return compile_args\n\n  def _get_callback_model(self):\n    return self\n\n  def _in_multi_worker_mode(self):\n    return self.distribute_strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access\n\n  @property\n  def _compile_was_called(self):\n    return self._is_compiled\n\n\ndef reduce_per_replica(values, strategy, reduction='first'):\n  \"\"\"Reduce PerReplica objects.\n\n  Args:\n    values: Structure of `PerReplica` objects or `Tensor`s. `Tensor`s are\n      returned as-is.\n    strategy: `tf.distribute.Strategy` object.\n    reduction: One of 'first', 'concat'.\n\n  Returns:\n    Structure of `Tensor`s.\n  \"\"\"\n\n  def _reduce(v):\n    \"\"\"Reduce a single `PerReplica` object.\"\"\"\n    if reduction == 'concat' and _collective_all_reduce_multi_worker(strategy):\n      return _multi_worker_concat(v, strategy)\n    if not _is_per_replica_instance(v):\n      return v\n    elif reduction == 'first':\n      return strategy.unwrap(v)[0]\n    elif reduction == 'concat':\n      if _is_tpu_multi_host(strategy):\n        return _tpu_multi_host_concat(v, strategy)\n      else:\n        return concat(strategy.unwrap(v))\n    else:\n      raise ValueError('`reduction` must be \"first\" or \"concat\".')\n\n  return nest.map_structure(_reduce, values)\n\n\ndef concat(tensors, axis=0):\n  \"\"\"Concats `tensor`s along `axis`.\"\"\"\n  if isinstance(tensors[0], sparse_tensor.SparseTensor):\n    return sparse_ops.sparse_concat_v2(axis=axis, sp_inputs=tensors)\n  return array_ops.concat(tensors, axis=axis)\n\n\ndef _is_tpu_multi_host(strategy):\n  return (backend.is_tpu_strategy(strategy) and\n          strategy.extended.num_hosts > 1)\n\n\ndef _tpu_multi_host_concat(v, strategy):\n  \"\"\"Correctly order TPU PerReplica objects.\"\"\"\n  replicas = strategy.unwrap(v)\n  # When distributed datasets are created from Tensors / NumPy,\n  # TPUStrategy.experimental_distribute_dataset shards data in\n  # (Replica, Host) order, and TPUStrategy.unwrap returns it in\n  # (Host, Replica) order.\n  # TODO(b/150317897): Figure out long-term plan here.\n  num_replicas_per_host = strategy.extended.num_replicas_per_host\n  ordered_replicas = []\n  for replica_id in range(num_replicas_per_host):\n    ordered_replicas += replicas[replica_id::num_replicas_per_host]\n  return concat(ordered_replicas)\n\n\ndef _collective_all_reduce_multi_worker(strategy):\n  return (isinstance(strategy,\n                     collective_all_reduce_strategy.CollectiveAllReduceStrategy)\n         ) and strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access\n\n\n# TODO(wxinyi): merge this with _tpu_multi_host_concat once we have all_gather\n# for all strategies\ndef _multi_worker_concat(v, strategy):\n  \"\"\"Order PerReplica objects for CollectiveAllReduceStrategy and concat.\"\"\"\n  replicas = strategy.gather(v, axis=0)\n  # v might not have the same shape on different replicas\n  if _is_per_replica_instance(v):\n    shapes = array_ops.concat([\n        array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)\n        for single_value in v.values\n    ],\n                              axis=0)\n    all_shapes = strategy.gather(shapes, axis=0)\n  else:\n    # v is a tensor. This may happen when, say, we have 2x1 multi-worker.\n    all_shapes = strategy.gather(\n        array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0), axis=0)\n\n  replicas = array_ops.split(\n      replicas,\n      num_or_size_splits=all_shapes,\n      num=strategy.num_replicas_in_sync)\n  ordered_replicas = []\n  num_replicas_per_worker = len(strategy.extended.worker_devices)\n  for replica_id in range(num_replicas_per_worker):\n    ordered_replicas += replicas[replica_id::num_replicas_per_worker]\n  return concat(ordered_replicas)\n\n\ndef _is_scalar(x):\n  return isinstance(x, (ops.Tensor, variables.Variable)) and x.shape.rank == 0\n\n\ndef write_scalar_summaries(logs, step):\n  for name, value in logs.items():\n    if _is_scalar(value):\n      summary_ops_v2.scalar('batch_' + name, value, step=step)\n\n\ndef _minimum_control_deps(outputs):\n  \"\"\"Returns the minimum control dependencies to ensure step succeeded.\"\"\"\n  if context.executing_eagerly():\n    return []  # Control dependencies not needed.\n  outputs = nest.flatten(outputs, expand_composites=True)\n  for out in outputs:\n    # Variables can't be control dependencies.\n    if not isinstance(out, variables.Variable):\n      return [out]  # Return first Tensor or Op from outputs.\n  return []  # No viable Tensor or Op to use for control deps.\n\n\ndef _disallow_inside_tf_function(method_name):\n  if ops.inside_function():\n    error_msg = (\n        'Detected a call to `Model.{method_name}` inside a `tf.function`. '\n        '`Model.{method_name} is a high-level endpoint that manages its own '\n        '`tf.function`. Please move the call to `Model.{method_name}` outside '\n        'of all enclosing `tf.function`s. Note that you can call a `Model` '\n        'directly on `Tensor`s inside a `tf.function` like: `model(x)`.'\n    ).format(method_name=method_name)\n    raise RuntimeError(error_msg)\n\n\ndef _detect_save_format(filepath):\n  \"\"\"Returns path to weights file and save format.\"\"\"\n\n  filepath = path_to_string(filepath)\n  if saving_utils.is_hdf5_filepath(filepath):\n    return filepath, 'h5'\n\n  # Filepath could be a TensorFlow checkpoint file prefix or SavedModel\n  # directory. It's possible for filepath to be both a prefix and directory.\n  # Prioritize checkpoint over SavedModel.\n  if _is_readable_tf_checkpoint(filepath):\n    save_format = 'tf'\n  elif sm_loader.contains_saved_model(filepath):\n    ckpt_path = os.path.join(filepath, sm_constants.VARIABLES_DIRECTORY,\n                             sm_constants.VARIABLES_FILENAME)\n    if _is_readable_tf_checkpoint(ckpt_path):\n      filepath = ckpt_path\n      save_format = 'tf'\n    else:\n      raise ValueError('Unable to load weights. filepath {} appears to be a '\n                       'SavedModel directory, but checkpoint either doesn\\'t '\n                       'exist, or is incorrectly formatted.'.format(filepath))\n  else:\n    # Not a TensorFlow checkpoint. This filepath is likely an H5 file that\n    # doesn't have the hdf5/keras extensions.\n    save_format = 'h5'\n  return filepath, save_format\n\n\ndef _is_readable_tf_checkpoint(filepath):\n  try:\n    py_checkpoint_reader.NewCheckpointReader(filepath)\n    return True\n  except errors_impl.DataLossError:\n    # The checkpoint is not readable in TensorFlow format.\n    return False\n\n\ndef flatten_metrics_in_order(logs, metrics_names):\n  \"\"\"Turns the `logs` dict into a list as per key order of `metrics_names`.\"\"\"\n  results = []\n  for name in metrics_names:\n    if name in logs:\n      results.append(logs[name])\n  for key in sorted(logs.keys()):\n    if key not in metrics_names:\n      results.append(logs[key])\n  if len(results) == 1:\n    return results[0]\n  return results\n\n\ndef _is_per_replica_instance(obj):\n  return (isinstance(obj, ds_values.DistributedValues) and\n          isinstance(obj, composite_tensor.CompositeTensor))\n\n\ndef saver_with_op_caching(obj):\n  if context.executing_eagerly():\n    saveables_cache = None\n  else:\n    saveables_cache = object_identity.ObjectIdentityWeakKeyDictionary()\n  return trackable_utils.TrackableSaver(\n      graph_view_lib.ObjectGraphView(\n          weakref.ref(obj), saveables_cache=saveables_cache))\n", "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: disable=protected-access\n\"\"\"Functions that save the model's config into different formats.\"\"\"\n\nfrom tensorflow.python.keras.saving.saved_model import json_utils\nfrom tensorflow.python.util.tf_export import keras_export\n\n\n@keras_export('keras.models.model_from_config')\ndef model_from_config(config, custom_objects=None):\n  \"\"\"Instantiates a Keras model from its config.\n\n  Usage:\n  ```\n  # for a Functional API model\n  tf.keras.Model().from_config(model.get_config())\n\n  # for a Sequential model\n  tf.keras.Sequential().from_config(model.get_config())\n  ```\n\n  Args:\n      config: Configuration dictionary.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      TypeError: if `config` is not a dictionary.\n  \"\"\"\n  if isinstance(config, list):\n    raise TypeError('`model_from_config` expects a dictionary, not a list. '\n                    'Maybe you meant to use '\n                    '`Sequential.from_config(config)`?')\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n\n\n@keras_export('keras.models.model_from_yaml')\ndef model_from_yaml(yaml_string, custom_objects=None):\n  \"\"\"Parses a yaml model configuration file and returns a model instance.\n\n  Note: Since TF 2.6, this method is no longer supported and will raise a\n  RuntimeError.\n\n  Args:\n      yaml_string: YAML string or open file encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      RuntimeError: announces that the method poses a security risk\n  \"\"\"\n  raise RuntimeError(\n      'Method `model_from_yaml()` has been removed due to security risk of '\n      'arbitrary code execution. Please use `Model.to_json()` and '\n      '`model_from_json()` instead.'\n  )\n\n\n@keras_export('keras.models.model_from_json')\ndef model_from_json(json_string, custom_objects=None):\n  \"\"\"Parses a JSON model configuration string and returns a model instance.\n\n  Usage:\n\n  >>> model = tf.keras.Sequential([\n  ...     tf.keras.layers.Dense(5, input_shape=(3,)),\n  ...     tf.keras.layers.Softmax()])\n  >>> config = model.to_json()\n  >>> loaded_model = tf.keras.models.model_from_json(config)\n\n  Args:\n      json_string: JSON string encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n  \"\"\"\n  config = json_utils.decode(json_string)\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n"], "filenames": ["RELEASE.md", "tensorflow/python/keras/engine/functional.py", "tensorflow/python/keras/engine/functional_test.py", "tensorflow/python/keras/engine/training.py", "tensorflow/python/keras/saving/model_config.py"], "buggy_code_start_loc": [17, 56, 49, 90, 21], "buggy_code_end_loc": [17, 57, 1368, 2440, 101], "fixing_code_start_loc": [18, 56, 48, 89, 20], "fixing_code_end_loc": [22, 57, 1354, 2438, 79], "type": "CWE-502", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions TensorFlow and Keras can be tricked to perform arbitrary code execution when deserializing a Keras model from YAML format. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/python/keras/saving/model_config.py#L66-L104) uses `yaml.unsafe_load` which can perform arbitrary code execution on the input. Given that YAML format support requires a significant amount of work, we have removed it for now. We have patched the issue in GitHub commit 23d6383eb6c14084a8fc3bdf164043b974818012. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-37678", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T23:15:08.190", "lastModified": "2021-08-19T13:44:46.230", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions TensorFlow and Keras can be tricked to perform arbitrary code execution when deserializing a Keras model from YAML format. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/python/keras/saving/model_config.py#L66-L104) uses `yaml.unsafe_load` which can perform arbitrary code execution on the input. Given that YAML format support requires a significant amount of work, we have removed it for now. We have patched the issue in GitHub commit 23d6383eb6c14084a8fc3bdf164043b974818012. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;En las versiones afectadas, se puede enga\u00f1ar a TensorFlow y Keras para llevar a cabo una ejecuci\u00f3n de c\u00f3digo arbitrario al deserializar un modelo de Keras desde el formato YAML.&#xa0;La [implementaci\u00f3n] (https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/python/keras/saving/model_config.py#L66-L104) usa \"yaml.unsafe_load\" en la entrada.&#xa0;Dado que la compatibilidad con el formato YAML requiere una gran cantidad de trabajo, lo hemos eliminado por ahora.&#xa0;Hemos solucionado el problema en el commit de GitHub 23d6383eb6c14084a8fc3bdf164043b974818012.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.6.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.5.1, TensorFlow versi\u00f3n 2.4.3 y TensorFlow versi\u00f3n 2.3.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan se encuentran en el rango admitido."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.0, "impactScore": 6.0}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.3, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 2.5, "impactScore": 6.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-502"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/23d6383eb6c14084a8fc3bdf164043b974818012", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-r6jx-9g48-2r5r", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/23d6383eb6c14084a8fc3bdf164043b974818012"}}