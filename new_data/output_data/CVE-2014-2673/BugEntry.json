{"buggy_code": ["/*\n *  Derived from \"arch/i386/kernel/process.c\"\n *    Copyright (C) 1995  Linus Torvalds\n *\n *  Updated and modified by Cort Dougan (cort@cs.nmt.edu) and\n *  Paul Mackerras (paulus@cs.anu.edu.au)\n *\n *  PowerPC version\n *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)\n *\n *  This program is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU General Public License\n *  as published by the Free Software Foundation; either version\n *  2 of the License, or (at your option) any later version.\n */\n\n#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/stddef.h>\n#include <linux/unistd.h>\n#include <linux/ptrace.h>\n#include <linux/slab.h>\n#include <linux/user.h>\n#include <linux/elf.h>\n#include <linux/prctl.h>\n#include <linux/init_task.h>\n#include <linux/export.h>\n#include <linux/kallsyms.h>\n#include <linux/mqueue.h>\n#include <linux/hardirq.h>\n#include <linux/utsname.h>\n#include <linux/ftrace.h>\n#include <linux/kernel_stat.h>\n#include <linux/personality.h>\n#include <linux/random.h>\n#include <linux/hw_breakpoint.h>\n\n#include <asm/pgtable.h>\n#include <asm/uaccess.h>\n#include <asm/io.h>\n#include <asm/processor.h>\n#include <asm/mmu.h>\n#include <asm/prom.h>\n#include <asm/machdep.h>\n#include <asm/time.h>\n#include <asm/runlatch.h>\n#include <asm/syscalls.h>\n#include <asm/switch_to.h>\n#include <asm/tm.h>\n#include <asm/debug.h>\n#ifdef CONFIG_PPC64\n#include <asm/firmware.h>\n#endif\n#include <linux/kprobes.h>\n#include <linux/kdebug.h>\n\n/* Transactional Memory debug */\n#ifdef TM_DEBUG_SW\n#define TM_DEBUG(x...) printk(KERN_INFO x)\n#else\n#define TM_DEBUG(x...) do { } while(0)\n#endif\n\nextern unsigned long _get_SP(void);\n\n#ifndef CONFIG_SMP\nstruct task_struct *last_task_used_math = NULL;\nstruct task_struct *last_task_used_altivec = NULL;\nstruct task_struct *last_task_used_vsx = NULL;\nstruct task_struct *last_task_used_spe = NULL;\n#endif\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nvoid giveup_fpu_maybe_transactional(struct task_struct *tsk)\n{\n\t/*\n\t * If we are saving the current thread's registers, and the\n\t * thread is in a transactional state, set the TIF_RESTORE_TM\n\t * bit so that we know to restore the registers before\n\t * returning to userspace.\n\t */\n\tif (tsk == current && tsk->thread.regs &&\n\t    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&\n\t    !test_thread_flag(TIF_RESTORE_TM)) {\n\t\ttsk->thread.tm_orig_msr = tsk->thread.regs->msr;\n\t\tset_thread_flag(TIF_RESTORE_TM);\n\t}\n\n\tgiveup_fpu(tsk);\n}\n\nvoid giveup_altivec_maybe_transactional(struct task_struct *tsk)\n{\n\t/*\n\t * If we are saving the current thread's registers, and the\n\t * thread is in a transactional state, set the TIF_RESTORE_TM\n\t * bit so that we know to restore the registers before\n\t * returning to userspace.\n\t */\n\tif (tsk == current && tsk->thread.regs &&\n\t    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&\n\t    !test_thread_flag(TIF_RESTORE_TM)) {\n\t\ttsk->thread.tm_orig_msr = tsk->thread.regs->msr;\n\t\tset_thread_flag(TIF_RESTORE_TM);\n\t}\n\n\tgiveup_altivec(tsk);\n}\n\n#else\n#define giveup_fpu_maybe_transactional(tsk)\tgiveup_fpu(tsk)\n#define giveup_altivec_maybe_transactional(tsk)\tgiveup_altivec(tsk)\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n\n#ifdef CONFIG_PPC_FPU\n/*\n * Make sure the floating-point register state in the\n * the thread_struct is up to date for task tsk.\n */\nvoid flush_fp_to_thread(struct task_struct *tsk)\n{\n\tif (tsk->thread.regs) {\n\t\t/*\n\t\t * We need to disable preemption here because if we didn't,\n\t\t * another process could get scheduled after the regs->msr\n\t\t * test but before we have finished saving the FP registers\n\t\t * to the thread_struct.  That process could take over the\n\t\t * FPU, and then when we get scheduled again we would store\n\t\t * bogus values for the remaining FP registers.\n\t\t */\n\t\tpreempt_disable();\n\t\tif (tsk->thread.regs->msr & MSR_FP) {\n#ifdef CONFIG_SMP\n\t\t\t/*\n\t\t\t * This should only ever be called for current or\n\t\t\t * for a stopped child process.  Since we save away\n\t\t\t * the FP register state on context switch on SMP,\n\t\t\t * there is something wrong if a stopped child appears\n\t\t\t * to still have its FP state in the CPU registers.\n\t\t\t */\n\t\t\tBUG_ON(tsk != current);\n#endif\n\t\t\tgiveup_fpu_maybe_transactional(tsk);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL_GPL(flush_fp_to_thread);\n#endif /* CONFIG_PPC_FPU */\n\nvoid enable_kernel_fp(void)\n{\n\tWARN_ON(preemptible());\n\n#ifdef CONFIG_SMP\n\tif (current->thread.regs && (current->thread.regs->msr & MSR_FP))\n\t\tgiveup_fpu_maybe_transactional(current);\n\telse\n\t\tgiveup_fpu(NULL);\t/* just enables FP for kernel */\n#else\n\tgiveup_fpu_maybe_transactional(last_task_used_math);\n#endif /* CONFIG_SMP */\n}\nEXPORT_SYMBOL(enable_kernel_fp);\n\n#ifdef CONFIG_ALTIVEC\nvoid enable_kernel_altivec(void)\n{\n\tWARN_ON(preemptible());\n\n#ifdef CONFIG_SMP\n\tif (current->thread.regs && (current->thread.regs->msr & MSR_VEC))\n\t\tgiveup_altivec_maybe_transactional(current);\n\telse\n\t\tgiveup_altivec_notask();\n#else\n\tgiveup_altivec_maybe_transactional(last_task_used_altivec);\n#endif /* CONFIG_SMP */\n}\nEXPORT_SYMBOL(enable_kernel_altivec);\n\n/*\n * Make sure the VMX/Altivec register state in the\n * the thread_struct is up to date for task tsk.\n */\nvoid flush_altivec_to_thread(struct task_struct *tsk)\n{\n\tif (tsk->thread.regs) {\n\t\tpreempt_disable();\n\t\tif (tsk->thread.regs->msr & MSR_VEC) {\n#ifdef CONFIG_SMP\n\t\t\tBUG_ON(tsk != current);\n#endif\n\t\t\tgiveup_altivec_maybe_transactional(tsk);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL_GPL(flush_altivec_to_thread);\n#endif /* CONFIG_ALTIVEC */\n\n#ifdef CONFIG_VSX\n#if 0\n/* not currently used, but some crazy RAID module might want to later */\nvoid enable_kernel_vsx(void)\n{\n\tWARN_ON(preemptible());\n\n#ifdef CONFIG_SMP\n\tif (current->thread.regs && (current->thread.regs->msr & MSR_VSX))\n\t\tgiveup_vsx(current);\n\telse\n\t\tgiveup_vsx(NULL);\t/* just enable vsx for kernel - force */\n#else\n\tgiveup_vsx(last_task_used_vsx);\n#endif /* CONFIG_SMP */\n}\nEXPORT_SYMBOL(enable_kernel_vsx);\n#endif\n\nvoid giveup_vsx(struct task_struct *tsk)\n{\n\tgiveup_fpu_maybe_transactional(tsk);\n\tgiveup_altivec_maybe_transactional(tsk);\n\t__giveup_vsx(tsk);\n}\n\nvoid flush_vsx_to_thread(struct task_struct *tsk)\n{\n\tif (tsk->thread.regs) {\n\t\tpreempt_disable();\n\t\tif (tsk->thread.regs->msr & MSR_VSX) {\n#ifdef CONFIG_SMP\n\t\t\tBUG_ON(tsk != current);\n#endif\n\t\t\tgiveup_vsx(tsk);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL_GPL(flush_vsx_to_thread);\n#endif /* CONFIG_VSX */\n\n#ifdef CONFIG_SPE\n\nvoid enable_kernel_spe(void)\n{\n\tWARN_ON(preemptible());\n\n#ifdef CONFIG_SMP\n\tif (current->thread.regs && (current->thread.regs->msr & MSR_SPE))\n\t\tgiveup_spe(current);\n\telse\n\t\tgiveup_spe(NULL);\t/* just enable SPE for kernel - force */\n#else\n\tgiveup_spe(last_task_used_spe);\n#endif /* __SMP __ */\n}\nEXPORT_SYMBOL(enable_kernel_spe);\n\nvoid flush_spe_to_thread(struct task_struct *tsk)\n{\n\tif (tsk->thread.regs) {\n\t\tpreempt_disable();\n\t\tif (tsk->thread.regs->msr & MSR_SPE) {\n#ifdef CONFIG_SMP\n\t\t\tBUG_ON(tsk != current);\n#endif\n\t\t\ttsk->thread.spefscr = mfspr(SPRN_SPEFSCR);\n\t\t\tgiveup_spe(tsk);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\n#endif /* CONFIG_SPE */\n\n#ifndef CONFIG_SMP\n/*\n * If we are doing lazy switching of CPU state (FP, altivec or SPE),\n * and the current task has some state, discard it.\n */\nvoid discard_lazy_cpu_state(void)\n{\n\tpreempt_disable();\n\tif (last_task_used_math == current)\n\t\tlast_task_used_math = NULL;\n#ifdef CONFIG_ALTIVEC\n\tif (last_task_used_altivec == current)\n\t\tlast_task_used_altivec = NULL;\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_VSX\n\tif (last_task_used_vsx == current)\n\t\tlast_task_used_vsx = NULL;\n#endif /* CONFIG_VSX */\n#ifdef CONFIG_SPE\n\tif (last_task_used_spe == current)\n\t\tlast_task_used_spe = NULL;\n#endif\n\tpreempt_enable();\n}\n#endif /* CONFIG_SMP */\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\nvoid do_send_trap(struct pt_regs *regs, unsigned long address,\n\t\t  unsigned long error_code, int signal_code, int breakpt)\n{\n\tsiginfo_t info;\n\n\tcurrent->thread.trap_nr = signal_code;\n\tif (notify_die(DIE_DABR_MATCH, \"dabr_match\", regs, error_code,\n\t\t\t11, SIGSEGV) == NOTIFY_STOP)\n\t\treturn;\n\n\t/* Deliver the signal to userspace */\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = breakpt;\t/* breakpoint or watchpoint id */\n\tinfo.si_code = signal_code;\n\tinfo.si_addr = (void __user *)address;\n\tforce_sig_info(SIGTRAP, &info, current);\n}\n#else\t/* !CONFIG_PPC_ADV_DEBUG_REGS */\nvoid do_break (struct pt_regs *regs, unsigned long address,\n\t\t    unsigned long error_code)\n{\n\tsiginfo_t info;\n\n\tcurrent->thread.trap_nr = TRAP_HWBKPT;\n\tif (notify_die(DIE_DABR_MATCH, \"dabr_match\", regs, error_code,\n\t\t\t11, SIGSEGV) == NOTIFY_STOP)\n\t\treturn;\n\n\tif (debugger_break_match(regs))\n\t\treturn;\n\n\t/* Clear the breakpoint */\n\thw_breakpoint_disable();\n\n\t/* Deliver the signal to userspace */\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = 0;\n\tinfo.si_code = TRAP_HWBKPT;\n\tinfo.si_addr = (void __user *)address;\n\tforce_sig_info(SIGTRAP, &info, current);\n}\n#endif\t/* CONFIG_PPC_ADV_DEBUG_REGS */\n\nstatic DEFINE_PER_CPU(struct arch_hw_breakpoint, current_brk);\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n/*\n * Set the debug registers back to their default \"safe\" values.\n */\nstatic void set_debug_reg_defaults(struct thread_struct *thread)\n{\n\tthread->debug.iac1 = thread->debug.iac2 = 0;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\tthread->debug.iac3 = thread->debug.iac4 = 0;\n#endif\n\tthread->debug.dac1 = thread->debug.dac2 = 0;\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\tthread->debug.dvc1 = thread->debug.dvc2 = 0;\n#endif\n\tthread->debug.dbcr0 = 0;\n#ifdef CONFIG_BOOKE\n\t/*\n\t * Force User/Supervisor bits to b11 (user-only MSR[PR]=1)\n\t */\n\tthread->debug.dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |\n\t\t\tDBCR1_IAC3US | DBCR1_IAC4US;\n\t/*\n\t * Force Data Address Compare User/Supervisor bits to be User-only\n\t * (0b11 MSR[PR]=1) and set all other bits in DBCR2 register to be 0.\n\t */\n\tthread->debug.dbcr2 = DBCR2_DAC1US | DBCR2_DAC2US;\n#else\n\tthread->debug.dbcr1 = 0;\n#endif\n}\n\nstatic void prime_debug_regs(struct debug_reg *debug)\n{\n\t/*\n\t * We could have inherited MSR_DE from userspace, since\n\t * it doesn't get cleared on exception entry.  Make sure\n\t * MSR_DE is clear before we enable any debug events.\n\t */\n\tmtmsr(mfmsr() & ~MSR_DE);\n\n\tmtspr(SPRN_IAC1, debug->iac1);\n\tmtspr(SPRN_IAC2, debug->iac2);\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\tmtspr(SPRN_IAC3, debug->iac3);\n\tmtspr(SPRN_IAC4, debug->iac4);\n#endif\n\tmtspr(SPRN_DAC1, debug->dac1);\n\tmtspr(SPRN_DAC2, debug->dac2);\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\tmtspr(SPRN_DVC1, debug->dvc1);\n\tmtspr(SPRN_DVC2, debug->dvc2);\n#endif\n\tmtspr(SPRN_DBCR0, debug->dbcr0);\n\tmtspr(SPRN_DBCR1, debug->dbcr1);\n#ifdef CONFIG_BOOKE\n\tmtspr(SPRN_DBCR2, debug->dbcr2);\n#endif\n}\n/*\n * Unless neither the old or new thread are making use of the\n * debug registers, set the debug registers from the values\n * stored in the new thread.\n */\nvoid switch_booke_debug_regs(struct debug_reg *new_debug)\n{\n\tif ((current->thread.debug.dbcr0 & DBCR0_IDM)\n\t\t|| (new_debug->dbcr0 & DBCR0_IDM))\n\t\t\tprime_debug_regs(new_debug);\n}\nEXPORT_SYMBOL_GPL(switch_booke_debug_regs);\n#else\t/* !CONFIG_PPC_ADV_DEBUG_REGS */\n#ifndef CONFIG_HAVE_HW_BREAKPOINT\nstatic void set_debug_reg_defaults(struct thread_struct *thread)\n{\n\tthread->hw_brk.address = 0;\n\tthread->hw_brk.type = 0;\n\tset_breakpoint(&thread->hw_brk);\n}\n#endif /* !CONFIG_HAVE_HW_BREAKPOINT */\n#endif\t/* CONFIG_PPC_ADV_DEBUG_REGS */\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\nstatic inline int __set_dabr(unsigned long dabr, unsigned long dabrx)\n{\n\tmtspr(SPRN_DAC1, dabr);\n#ifdef CONFIG_PPC_47x\n\tisync();\n#endif\n\treturn 0;\n}\n#elif defined(CONFIG_PPC_BOOK3S)\nstatic inline int __set_dabr(unsigned long dabr, unsigned long dabrx)\n{\n\tmtspr(SPRN_DABR, dabr);\n\tif (cpu_has_feature(CPU_FTR_DABRX))\n\t\tmtspr(SPRN_DABRX, dabrx);\n\treturn 0;\n}\n#else\nstatic inline int __set_dabr(unsigned long dabr, unsigned long dabrx)\n{\n\treturn -EINVAL;\n}\n#endif\n\nstatic inline int set_dabr(struct arch_hw_breakpoint *brk)\n{\n\tunsigned long dabr, dabrx;\n\n\tdabr = brk->address | (brk->type & HW_BRK_TYPE_DABR);\n\tdabrx = ((brk->type >> 3) & 0x7);\n\n\tif (ppc_md.set_dabr)\n\t\treturn ppc_md.set_dabr(dabr, dabrx);\n\n\treturn __set_dabr(dabr, dabrx);\n}\n\nstatic inline int set_dawr(struct arch_hw_breakpoint *brk)\n{\n\tunsigned long dawr, dawrx, mrd;\n\n\tdawr = brk->address;\n\n\tdawrx  = (brk->type & (HW_BRK_TYPE_READ | HW_BRK_TYPE_WRITE)) \\\n\t\t                   << (63 - 58); //* read/write bits */\n\tdawrx |= ((brk->type & (HW_BRK_TYPE_TRANSLATE)) >> 2) \\\n\t\t                   << (63 - 59); //* translate */\n\tdawrx |= (brk->type & (HW_BRK_TYPE_PRIV_ALL)) \\\n\t\t                   >> 3; //* PRIM bits */\n\t/* dawr length is stored in field MDR bits 48:53.  Matches range in\n\t   doublewords (64 bits) baised by -1 eg. 0b000000=1DW and\n\t   0b111111=64DW.\n\t   brk->len is in bytes.\n\t   This aligns up to double word size, shifts and does the bias.\n\t*/\n\tmrd = ((brk->len + 7) >> 3) - 1;\n\tdawrx |= (mrd & 0x3f) << (63 - 53);\n\n\tif (ppc_md.set_dawr)\n\t\treturn ppc_md.set_dawr(dawr, dawrx);\n\tmtspr(SPRN_DAWR, dawr);\n\tmtspr(SPRN_DAWRX, dawrx);\n\treturn 0;\n}\n\nint set_breakpoint(struct arch_hw_breakpoint *brk)\n{\n\t__get_cpu_var(current_brk) = *brk;\n\n\tif (cpu_has_feature(CPU_FTR_DAWR))\n\t\treturn set_dawr(brk);\n\n\treturn set_dabr(brk);\n}\n\n#ifdef CONFIG_PPC64\nDEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);\n#endif\n\nstatic inline bool hw_brk_match(struct arch_hw_breakpoint *a,\n\t\t\t      struct arch_hw_breakpoint *b)\n{\n\tif (a->address != b->address)\n\t\treturn false;\n\tif (a->type != b->type)\n\t\treturn false;\n\tif (a->len != b->len)\n\t\treturn false;\n\treturn true;\n}\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nstatic void tm_reclaim_thread(struct thread_struct *thr,\n\t\t\t      struct thread_info *ti, uint8_t cause)\n{\n\tunsigned long msr_diff = 0;\n\n\t/*\n\t * If FP/VSX registers have been already saved to the\n\t * thread_struct, move them to the transact_fp array.\n\t * We clear the TIF_RESTORE_TM bit since after the reclaim\n\t * the thread will no longer be transactional.\n\t */\n\tif (test_ti_thread_flag(ti, TIF_RESTORE_TM)) {\n\t\tmsr_diff = thr->tm_orig_msr & ~thr->regs->msr;\n\t\tif (msr_diff & MSR_FP)\n\t\t\tmemcpy(&thr->transact_fp, &thr->fp_state,\n\t\t\t       sizeof(struct thread_fp_state));\n\t\tif (msr_diff & MSR_VEC)\n\t\t\tmemcpy(&thr->transact_vr, &thr->vr_state,\n\t\t\t       sizeof(struct thread_vr_state));\n\t\tclear_ti_thread_flag(ti, TIF_RESTORE_TM);\n\t\tmsr_diff &= MSR_FP | MSR_VEC | MSR_VSX | MSR_FE0 | MSR_FE1;\n\t}\n\n\ttm_reclaim(thr, thr->regs->msr, cause);\n\n\t/* Having done the reclaim, we now have the checkpointed\n\t * FP/VSX values in the registers.  These might be valid\n\t * even if we have previously called enable_kernel_fp() or\n\t * flush_fp_to_thread(), so update thr->regs->msr to\n\t * indicate their current validity.\n\t */\n\tthr->regs->msr |= msr_diff;\n}\n\nvoid tm_reclaim_current(uint8_t cause)\n{\n\ttm_enable();\n\ttm_reclaim_thread(&current->thread, current_thread_info(), cause);\n}\n\nstatic inline void tm_reclaim_task(struct task_struct *tsk)\n{\n\t/* We have to work out if we're switching from/to a task that's in the\n\t * middle of a transaction.\n\t *\n\t * In switching we need to maintain a 2nd register state as\n\t * oldtask->thread.ckpt_regs.  We tm_reclaim(oldproc); this saves the\n\t * checkpointed (tbegin) state in ckpt_regs and saves the transactional\n\t * (current) FPRs into oldtask->thread.transact_fpr[].\n\t *\n\t * We also context switch (save) TFHAR/TEXASR/TFIAR in here.\n\t */\n\tstruct thread_struct *thr = &tsk->thread;\n\n\tif (!thr->regs)\n\t\treturn;\n\n\tif (!MSR_TM_ACTIVE(thr->regs->msr))\n\t\tgoto out_and_saveregs;\n\n\t/* Stash the original thread MSR, as giveup_fpu et al will\n\t * modify it.  We hold onto it to see whether the task used\n\t * FP & vector regs.  If the TIF_RESTORE_TM flag is set,\n\t * tm_orig_msr is already set.\n\t */\n\tif (!test_ti_thread_flag(task_thread_info(tsk), TIF_RESTORE_TM))\n\t\tthr->tm_orig_msr = thr->regs->msr;\n\n\tTM_DEBUG(\"--- tm_reclaim on pid %d (NIP=%lx, \"\n\t\t \"ccr=%lx, msr=%lx, trap=%lx)\\n\",\n\t\t tsk->pid, thr->regs->nip,\n\t\t thr->regs->ccr, thr->regs->msr,\n\t\t thr->regs->trap);\n\n\ttm_reclaim_thread(thr, task_thread_info(tsk), TM_CAUSE_RESCHED);\n\n\tTM_DEBUG(\"--- tm_reclaim on pid %d complete\\n\",\n\t\t tsk->pid);\n\nout_and_saveregs:\n\t/* Always save the regs here, even if a transaction's not active.\n\t * This context-switches a thread's TM info SPRs.  We do it here to\n\t * be consistent with the restore path (in recheckpoint) which\n\t * cannot happen later in _switch().\n\t */\n\ttm_save_sprs(thr);\n}\n\nstatic inline void tm_recheckpoint_new_task(struct task_struct *new)\n{\n\tunsigned long msr;\n\n\tif (!cpu_has_feature(CPU_FTR_TM))\n\t\treturn;\n\n\t/* Recheckpoint the registers of the thread we're about to switch to.\n\t *\n\t * If the task was using FP, we non-lazily reload both the original and\n\t * the speculative FP register states.  This is because the kernel\n\t * doesn't see if/when a TM rollback occurs, so if we take an FP\n\t * unavoidable later, we are unable to determine which set of FP regs\n\t * need to be restored.\n\t */\n\tif (!new->thread.regs)\n\t\treturn;\n\n\t/* The TM SPRs are restored here, so that TEXASR.FS can be set\n\t * before the trecheckpoint and no explosion occurs.\n\t */\n\ttm_restore_sprs(&new->thread);\n\n\tif (!MSR_TM_ACTIVE(new->thread.regs->msr))\n\t\treturn;\n\tmsr = new->thread.tm_orig_msr;\n\t/* Recheckpoint to restore original checkpointed register state. */\n\tTM_DEBUG(\"*** tm_recheckpoint of pid %d \"\n\t\t \"(new->msr 0x%lx, new->origmsr 0x%lx)\\n\",\n\t\t new->pid, new->thread.regs->msr, msr);\n\n\t/* This loads the checkpointed FP/VEC state, if used */\n\ttm_recheckpoint(&new->thread, msr);\n\n\t/* This loads the speculative FP/VEC state, if used */\n\tif (msr & MSR_FP) {\n\t\tdo_load_up_transact_fpu(&new->thread);\n\t\tnew->thread.regs->msr |=\n\t\t\t(MSR_FP | new->thread.fpexc_mode);\n\t}\n#ifdef CONFIG_ALTIVEC\n\tif (msr & MSR_VEC) {\n\t\tdo_load_up_transact_altivec(&new->thread);\n\t\tnew->thread.regs->msr |= MSR_VEC;\n\t}\n#endif\n\t/* We may as well turn on VSX too since all the state is restored now */\n\tif (msr & MSR_VSX)\n\t\tnew->thread.regs->msr |= MSR_VSX;\n\n\tTM_DEBUG(\"*** tm_recheckpoint of pid %d complete \"\n\t\t \"(kernel msr 0x%lx)\\n\",\n\t\t new->pid, mfmsr());\n}\n\nstatic inline void __switch_to_tm(struct task_struct *prev)\n{\n\tif (cpu_has_feature(CPU_FTR_TM)) {\n\t\ttm_enable();\n\t\ttm_reclaim_task(prev);\n\t}\n}\n\n/*\n * This is called if we are on the way out to userspace and the\n * TIF_RESTORE_TM flag is set.  It checks if we need to reload\n * FP and/or vector state and does so if necessary.\n * If userspace is inside a transaction (whether active or\n * suspended) and FP/VMX/VSX instructions have ever been enabled\n * inside that transaction, then we have to keep them enabled\n * and keep the FP/VMX/VSX state loaded while ever the transaction\n * continues.  The reason is that if we didn't, and subsequently\n * got a FP/VMX/VSX unavailable interrupt inside a transaction,\n * we don't know whether it's the same transaction, and thus we\n * don't know which of the checkpointed state and the transactional\n * state to use.\n */\nvoid restore_tm_state(struct pt_regs *regs)\n{\n\tunsigned long msr_diff;\n\n\tclear_thread_flag(TIF_RESTORE_TM);\n\tif (!MSR_TM_ACTIVE(regs->msr))\n\t\treturn;\n\n\tmsr_diff = current->thread.tm_orig_msr & ~regs->msr;\n\tmsr_diff &= MSR_FP | MSR_VEC | MSR_VSX;\n\tif (msr_diff & MSR_FP) {\n\t\tfp_enable();\n\t\tload_fp_state(&current->thread.fp_state);\n\t\tregs->msr |= current->thread.fpexc_mode;\n\t}\n\tif (msr_diff & MSR_VEC) {\n\t\tvec_enable();\n\t\tload_vr_state(&current->thread.vr_state);\n\t}\n\tregs->msr |= msr_diff;\n}\n\n#else\n#define tm_recheckpoint_new_task(new)\n#define __switch_to_tm(prev)\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n\nstruct task_struct *__switch_to(struct task_struct *prev,\n\tstruct task_struct *new)\n{\n\tstruct thread_struct *new_thread, *old_thread;\n\tstruct task_struct *last;\n#ifdef CONFIG_PPC_BOOK3S_64\n\tstruct ppc64_tlb_batch *batch;\n#endif\n\n\tWARN_ON(!irqs_disabled());\n\n\t/* Back up the TAR across context switches.\n\t * Note that the TAR is not available for use in the kernel.  (To\n\t * provide this, the TAR should be backed up/restored on exception\n\t * entry/exit instead, and be in pt_regs.  FIXME, this should be in\n\t * pt_regs anyway (for debug).)\n\t * Save the TAR here before we do treclaim/trecheckpoint as these\n\t * will change the TAR.\n\t */\n\tsave_tar(&prev->thread);\n\n\t__switch_to_tm(prev);\n\n#ifdef CONFIG_SMP\n\t/* avoid complexity of lazy save/restore of fpu\n\t * by just saving it every time we switch out if\n\t * this task used the fpu during the last quantum.\n\t *\n\t * If it tries to use the fpu again, it'll trap and\n\t * reload its fp regs.  So we don't have to do a restore\n\t * every switch, just a save.\n\t *  -- Cort\n\t */\n\tif (prev->thread.regs && (prev->thread.regs->msr & MSR_FP))\n\t\tgiveup_fpu(prev);\n#ifdef CONFIG_ALTIVEC\n\t/*\n\t * If the previous thread used altivec in the last quantum\n\t * (thus changing altivec regs) then save them.\n\t * We used to check the VRSAVE register but not all apps\n\t * set it, so we don't rely on it now (and in fact we need\n\t * to save & restore VSCR even if VRSAVE == 0).  -- paulus\n\t *\n\t * On SMP we always save/restore altivec regs just to avoid the\n\t * complexity of changing processors.\n\t *  -- Cort\n\t */\n\tif (prev->thread.regs && (prev->thread.regs->msr & MSR_VEC))\n\t\tgiveup_altivec(prev);\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_VSX\n\tif (prev->thread.regs && (prev->thread.regs->msr & MSR_VSX))\n\t\t/* VMX and FPU registers are already save here */\n\t\t__giveup_vsx(prev);\n#endif /* CONFIG_VSX */\n#ifdef CONFIG_SPE\n\t/*\n\t * If the previous thread used spe in the last quantum\n\t * (thus changing spe regs) then save them.\n\t *\n\t * On SMP we always save/restore spe regs just to avoid the\n\t * complexity of changing processors.\n\t */\n\tif ((prev->thread.regs && (prev->thread.regs->msr & MSR_SPE)))\n\t\tgiveup_spe(prev);\n#endif /* CONFIG_SPE */\n\n#else  /* CONFIG_SMP */\n#ifdef CONFIG_ALTIVEC\n\t/* Avoid the trap.  On smp this this never happens since\n\t * we don't set last_task_used_altivec -- Cort\n\t */\n\tif (new->thread.regs && last_task_used_altivec == new)\n\t\tnew->thread.regs->msr |= MSR_VEC;\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_VSX\n\tif (new->thread.regs && last_task_used_vsx == new)\n\t\tnew->thread.regs->msr |= MSR_VSX;\n#endif /* CONFIG_VSX */\n#ifdef CONFIG_SPE\n\t/* Avoid the trap.  On smp this this never happens since\n\t * we don't set last_task_used_spe\n\t */\n\tif (new->thread.regs && last_task_used_spe == new)\n\t\tnew->thread.regs->msr |= MSR_SPE;\n#endif /* CONFIG_SPE */\n\n#endif /* CONFIG_SMP */\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\tswitch_booke_debug_regs(&new->thread.debug);\n#else\n/*\n * For PPC_BOOK3S_64, we use the hw-breakpoint interfaces that would\n * schedule DABR\n */\n#ifndef CONFIG_HAVE_HW_BREAKPOINT\n\tif (unlikely(!hw_brk_match(&__get_cpu_var(current_brk), &new->thread.hw_brk)))\n\t\tset_breakpoint(&new->thread.hw_brk);\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n#endif\n\n\n\tnew_thread = &new->thread;\n\told_thread = &current->thread;\n\n#ifdef CONFIG_PPC64\n\t/*\n\t * Collect processor utilization data per process\n\t */\n\tif (firmware_has_feature(FW_FEATURE_SPLPAR)) {\n\t\tstruct cpu_usage *cu = &__get_cpu_var(cpu_usage_array);\n\t\tlong unsigned start_tb, current_tb;\n\t\tstart_tb = old_thread->start_tb;\n\t\tcu->current_tb = current_tb = mfspr(SPRN_PURR);\n\t\told_thread->accum_tb += (current_tb - start_tb);\n\t\tnew_thread->start_tb = current_tb;\n\t}\n#endif /* CONFIG_PPC64 */\n\n#ifdef CONFIG_PPC_BOOK3S_64\n\tbatch = &__get_cpu_var(ppc64_tlb_batch);\n\tif (batch->active) {\n\t\tcurrent_thread_info()->local_flags |= _TLF_LAZY_MMU;\n\t\tif (batch->index)\n\t\t\t__flush_tlb_pending(batch);\n\t\tbatch->active = 0;\n\t}\n#endif /* CONFIG_PPC_BOOK3S_64 */\n\n\t/*\n\t * We can't take a PMU exception inside _switch() since there is a\n\t * window where the kernel stack SLB and the kernel stack are out\n\t * of sync. Hard disable here.\n\t */\n\thard_irq_disable();\n\n\ttm_recheckpoint_new_task(new);\n\n\tlast = _switch(old_thread, new_thread);\n\n#ifdef CONFIG_PPC_BOOK3S_64\n\tif (current_thread_info()->local_flags & _TLF_LAZY_MMU) {\n\t\tcurrent_thread_info()->local_flags &= ~_TLF_LAZY_MMU;\n\t\tbatch = &__get_cpu_var(ppc64_tlb_batch);\n\t\tbatch->active = 1;\n\t}\n#endif /* CONFIG_PPC_BOOK3S_64 */\n\n\treturn last;\n}\n\nstatic int instructions_to_print = 16;\n\nstatic void show_instructions(struct pt_regs *regs)\n{\n\tint i;\n\tunsigned long pc = regs->nip - (instructions_to_print * 3 / 4 *\n\t\t\tsizeof(int));\n\n\tprintk(\"Instruction dump:\");\n\n\tfor (i = 0; i < instructions_to_print; i++) {\n\t\tint instr;\n\n\t\tif (!(i % 8))\n\t\t\tprintk(\"\\n\");\n\n#if !defined(CONFIG_BOOKE)\n\t\t/* If executing with the IMMU off, adjust pc rather\n\t\t * than print XXXXXXXX.\n\t\t */\n\t\tif (!(regs->msr & MSR_IR))\n\t\t\tpc = (unsigned long)phys_to_virt(pc);\n#endif\n\n\t\t/* We use __get_user here *only* to avoid an OOPS on a\n\t\t * bad address because the pc *should* only be a\n\t\t * kernel address.\n\t\t */\n\t\tif (!__kernel_text_address(pc) ||\n\t\t     __get_user(instr, (unsigned int __user *)pc)) {\n\t\t\tprintk(KERN_CONT \"XXXXXXXX \");\n\t\t} else {\n\t\t\tif (regs->nip == pc)\n\t\t\t\tprintk(KERN_CONT \"<%08x> \", instr);\n\t\t\telse\n\t\t\t\tprintk(KERN_CONT \"%08x \", instr);\n\t\t}\n\n\t\tpc += sizeof(int);\n\t}\n\n\tprintk(\"\\n\");\n}\n\nstatic struct regbit {\n\tunsigned long bit;\n\tconst char *name;\n} msr_bits[] = {\n#if defined(CONFIG_PPC64) && !defined(CONFIG_BOOKE)\n\t{MSR_SF,\t\"SF\"},\n\t{MSR_HV,\t\"HV\"},\n#endif\n\t{MSR_VEC,\t\"VEC\"},\n\t{MSR_VSX,\t\"VSX\"},\n#ifdef CONFIG_BOOKE\n\t{MSR_CE,\t\"CE\"},\n#endif\n\t{MSR_EE,\t\"EE\"},\n\t{MSR_PR,\t\"PR\"},\n\t{MSR_FP,\t\"FP\"},\n\t{MSR_ME,\t\"ME\"},\n#ifdef CONFIG_BOOKE\n\t{MSR_DE,\t\"DE\"},\n#else\n\t{MSR_SE,\t\"SE\"},\n\t{MSR_BE,\t\"BE\"},\n#endif\n\t{MSR_IR,\t\"IR\"},\n\t{MSR_DR,\t\"DR\"},\n\t{MSR_PMM,\t\"PMM\"},\n#ifndef CONFIG_BOOKE\n\t{MSR_RI,\t\"RI\"},\n\t{MSR_LE,\t\"LE\"},\n#endif\n\t{0,\t\tNULL}\n};\n\nstatic void printbits(unsigned long val, struct regbit *bits)\n{\n\tconst char *sep = \"\";\n\n\tprintk(\"<\");\n\tfor (; bits->bit; ++bits)\n\t\tif (val & bits->bit) {\n\t\t\tprintk(\"%s%s\", sep, bits->name);\n\t\t\tsep = \",\";\n\t\t}\n\tprintk(\">\");\n}\n\n#ifdef CONFIG_PPC64\n#define REG\t\t\"%016lx\"\n#define REGS_PER_LINE\t4\n#define LAST_VOLATILE\t13\n#else\n#define REG\t\t\"%08lx\"\n#define REGS_PER_LINE\t8\n#define LAST_VOLATILE\t12\n#endif\n\nvoid show_regs(struct pt_regs * regs)\n{\n\tint i, trap;\n\n\tshow_regs_print_info(KERN_DEFAULT);\n\n\tprintk(\"NIP: \"REG\" LR: \"REG\" CTR: \"REG\"\\n\",\n\t       regs->nip, regs->link, regs->ctr);\n\tprintk(\"REGS: %p TRAP: %04lx   %s  (%s)\\n\",\n\t       regs, regs->trap, print_tainted(), init_utsname()->release);\n\tprintk(\"MSR: \"REG\" \", regs->msr);\n\tprintbits(regs->msr, msr_bits);\n\tprintk(\"  CR: %08lx  XER: %08lx\\n\", regs->ccr, regs->xer);\n\ttrap = TRAP(regs);\n\tif ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))\n\t\tprintk(\"CFAR: \"REG\" \", regs->orig_gpr3);\n\tif (trap == 0x200 || trap == 0x300 || trap == 0x600)\n#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)\n\t\tprintk(\"DEAR: \"REG\" ESR: \"REG\" \", regs->dar, regs->dsisr);\n#else\n\t\tprintk(\"DAR: \"REG\" DSISR: %08lx \", regs->dar, regs->dsisr);\n#endif\n#ifdef CONFIG_PPC64\n\tprintk(\"SOFTE: %ld \", regs->softe);\n#endif\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n\tif (MSR_TM_ACTIVE(regs->msr))\n\t\tprintk(\"\\nPACATMSCRATCH: %016llx \", get_paca()->tm_scratch);\n#endif\n\n\tfor (i = 0;  i < 32;  i++) {\n\t\tif ((i % REGS_PER_LINE) == 0)\n\t\t\tprintk(\"\\nGPR%02d: \", i);\n\t\tprintk(REG \" \", regs->gpr[i]);\n\t\tif (i == LAST_VOLATILE && !FULL_REGS(regs))\n\t\t\tbreak;\n\t}\n\tprintk(\"\\n\");\n#ifdef CONFIG_KALLSYMS\n\t/*\n\t * Lookup NIP late so we have the best change of getting the\n\t * above info out without failing\n\t */\n\tprintk(\"NIP [\"REG\"] %pS\\n\", regs->nip, (void *)regs->nip);\n\tprintk(\"LR [\"REG\"] %pS\\n\", regs->link, (void *)regs->link);\n#endif\n\tshow_stack(current, (unsigned long *) regs->gpr[1]);\n\tif (!user_mode(regs))\n\t\tshow_instructions(regs);\n}\n\nvoid exit_thread(void)\n{\n\tdiscard_lazy_cpu_state();\n}\n\nvoid flush_thread(void)\n{\n\tdiscard_lazy_cpu_state();\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\tflush_ptrace_hw_breakpoint(current);\n#else /* CONFIG_HAVE_HW_BREAKPOINT */\n\tset_debug_reg_defaults(&current->thread);\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n}\n\nvoid\nrelease_thread(struct task_struct *t)\n{\n}\n\n/*\n * this gets called so that we can store coprocessor state into memory and\n * copy the current task into the new thread.\n */\nint arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)\n{\n\tflush_fp_to_thread(src);\n\tflush_altivec_to_thread(src);\n\tflush_vsx_to_thread(src);\n\tflush_spe_to_thread(src);\n\n\t*dst = *src;\n\n\tclear_task_ebb(dst);\n\n\treturn 0;\n}\n\n/*\n * Copy a thread..\n */\nextern unsigned long dscr_default; /* defined in arch/powerpc/kernel/sysfs.c */\n\nint copy_thread(unsigned long clone_flags, unsigned long usp,\n\t\tunsigned long arg, struct task_struct *p)\n{\n\tstruct pt_regs *childregs, *kregs;\n\textern void ret_from_fork(void);\n\textern void ret_from_kernel_thread(void);\n\tvoid (*f)(void);\n\tunsigned long sp = (unsigned long)task_stack_page(p) + THREAD_SIZE;\n\n\t/* Copy registers */\n\tsp -= sizeof(struct pt_regs);\n\tchildregs = (struct pt_regs *) sp;\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tstruct thread_info *ti = (void *)task_stack_page(p);\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tchildregs->gpr[1] = sp + sizeof(struct pt_regs);\n\t\tchildregs->gpr[14] = usp;\t/* function */\n#ifdef CONFIG_PPC64\n\t\tclear_tsk_thread_flag(p, TIF_32BIT);\n\t\tchildregs->softe = 1;\n#endif\n\t\tchildregs->gpr[15] = arg;\n\t\tp->thread.regs = NULL;\t/* no user register state */\n\t\tti->flags |= _TIF_RESTOREALL;\n\t\tf = ret_from_kernel_thread;\n\t} else {\n\t\tstruct pt_regs *regs = current_pt_regs();\n\t\tCHECK_FULL_REGS(regs);\n\t\t*childregs = *regs;\n\t\tif (usp)\n\t\t\tchildregs->gpr[1] = usp;\n\t\tp->thread.regs = childregs;\n\t\tchildregs->gpr[3] = 0;  /* Result from fork() */\n\t\tif (clone_flags & CLONE_SETTLS) {\n#ifdef CONFIG_PPC64\n\t\t\tif (!is_32bit_task())\n\t\t\t\tchildregs->gpr[13] = childregs->gpr[6];\n\t\t\telse\n#endif\n\t\t\t\tchildregs->gpr[2] = childregs->gpr[6];\n\t\t}\n\n\t\tf = ret_from_fork;\n\t}\n\tsp -= STACK_FRAME_OVERHEAD;\n\n\t/*\n\t * The way this works is that at some point in the future\n\t * some task will call _switch to switch to the new task.\n\t * That will pop off the stack frame created below and start\n\t * the new task running at ret_from_fork.  The new task will\n\t * do some house keeping and then return from the fork or clone\n\t * system call, using the stack frame created above.\n\t */\n\t((unsigned long *)sp)[0] = 0;\n\tsp -= sizeof(struct pt_regs);\n\tkregs = (struct pt_regs *) sp;\n\tsp -= STACK_FRAME_OVERHEAD;\n\tp->thread.ksp = sp;\n#ifdef CONFIG_PPC32\n\tp->thread.ksp_limit = (unsigned long)task_stack_page(p) +\n\t\t\t\t_ALIGN_UP(sizeof(struct thread_info), 16);\n#endif\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\tp->thread.ptrace_bps[0] = NULL;\n#endif\n\n\tp->thread.fp_save_area = NULL;\n#ifdef CONFIG_ALTIVEC\n\tp->thread.vr_save_area = NULL;\n#endif\n\n#ifdef CONFIG_PPC_STD_MMU_64\n\tif (mmu_has_feature(MMU_FTR_SLB)) {\n\t\tunsigned long sp_vsid;\n\t\tunsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;\n\n\t\tif (mmu_has_feature(MMU_FTR_1T_SEGMENT))\n\t\t\tsp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_1T)\n\t\t\t\t<< SLB_VSID_SHIFT_1T;\n\t\telse\n\t\t\tsp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_256M)\n\t\t\t\t<< SLB_VSID_SHIFT;\n\t\tsp_vsid |= SLB_VSID_KERNEL | llp;\n\t\tp->thread.ksp_vsid = sp_vsid;\n\t}\n#endif /* CONFIG_PPC_STD_MMU_64 */\n#ifdef CONFIG_PPC64 \n\tif (cpu_has_feature(CPU_FTR_DSCR)) {\n\t\tp->thread.dscr_inherit = current->thread.dscr_inherit;\n\t\tp->thread.dscr = current->thread.dscr;\n\t}\n\tif (cpu_has_feature(CPU_FTR_HAS_PPR))\n\t\tp->thread.ppr = INIT_PPR;\n#endif\n\t/*\n\t * The PPC64 ABI makes use of a TOC to contain function \n\t * pointers.  The function (ret_from_except) is actually a pointer\n\t * to the TOC entry.  The first entry is a pointer to the actual\n\t * function.\n\t */\n#ifdef CONFIG_PPC64\n\tkregs->nip = *((unsigned long *)f);\n#else\n\tkregs->nip = (unsigned long)f;\n#endif\n\treturn 0;\n}\n\n/*\n * Set up a thread for executing a new program\n */\nvoid start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)\n{\n#ifdef CONFIG_PPC64\n\tunsigned long load_addr = regs->gpr[2];\t/* saved by ELF_PLAT_INIT */\n#endif\n\n\t/*\n\t * If we exec out of a kernel thread then thread.regs will not be\n\t * set.  Do it now.\n\t */\n\tif (!current->thread.regs) {\n\t\tstruct pt_regs *regs = task_stack_page(current) + THREAD_SIZE;\n\t\tcurrent->thread.regs = regs - 1;\n\t}\n\n\tmemset(regs->gpr, 0, sizeof(regs->gpr));\n\tregs->ctr = 0;\n\tregs->link = 0;\n\tregs->xer = 0;\n\tregs->ccr = 0;\n\tregs->gpr[1] = sp;\n\n\t/*\n\t * We have just cleared all the nonvolatile GPRs, so make\n\t * FULL_REGS(regs) return true.  This is necessary to allow\n\t * ptrace to examine the thread immediately after exec.\n\t */\n\tregs->trap &= ~1UL;\n\n#ifdef CONFIG_PPC32\n\tregs->mq = 0;\n\tregs->nip = start;\n\tregs->msr = MSR_USER;\n#else\n\tif (!is_32bit_task()) {\n\t\tunsigned long entry;\n\n\t\tif (is_elf2_task()) {\n\t\t\t/* Look ma, no function descriptors! */\n\t\t\tentry = start;\n\n\t\t\t/*\n\t\t\t * Ulrich says:\n\t\t\t *   The latest iteration of the ABI requires that when\n\t\t\t *   calling a function (at its global entry point),\n\t\t\t *   the caller must ensure r12 holds the entry point\n\t\t\t *   address (so that the function can quickly\n\t\t\t *   establish addressability).\n\t\t\t */\n\t\t\tregs->gpr[12] = start;\n\t\t\t/* Make sure that's restored on entry to userspace. */\n\t\t\tset_thread_flag(TIF_RESTOREALL);\n\t\t} else {\n\t\t\tunsigned long toc;\n\n\t\t\t/* start is a relocated pointer to the function\n\t\t\t * descriptor for the elf _start routine.  The first\n\t\t\t * entry in the function descriptor is the entry\n\t\t\t * address of _start and the second entry is the TOC\n\t\t\t * value we need to use.\n\t\t\t */\n\t\t\t__get_user(entry, (unsigned long __user *)start);\n\t\t\t__get_user(toc, (unsigned long __user *)start+1);\n\n\t\t\t/* Check whether the e_entry function descriptor entries\n\t\t\t * need to be relocated before we can use them.\n\t\t\t */\n\t\t\tif (load_addr != 0) {\n\t\t\t\tentry += load_addr;\n\t\t\t\ttoc   += load_addr;\n\t\t\t}\n\t\t\tregs->gpr[2] = toc;\n\t\t}\n\t\tregs->nip = entry;\n\t\tregs->msr = MSR_USER64;\n\t} else {\n\t\tregs->nip = start;\n\t\tregs->gpr[2] = 0;\n\t\tregs->msr = MSR_USER32;\n\t}\n#endif\n\tdiscard_lazy_cpu_state();\n#ifdef CONFIG_VSX\n\tcurrent->thread.used_vsr = 0;\n#endif\n\tmemset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));\n\tcurrent->thread.fp_save_area = NULL;\n#ifdef CONFIG_ALTIVEC\n\tmemset(&current->thread.vr_state, 0, sizeof(current->thread.vr_state));\n\tcurrent->thread.vr_state.vscr.u[3] = 0x00010000; /* Java mode disabled */\n\tcurrent->thread.vr_save_area = NULL;\n\tcurrent->thread.vrsave = 0;\n\tcurrent->thread.used_vr = 0;\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_SPE\n\tmemset(current->thread.evr, 0, sizeof(current->thread.evr));\n\tcurrent->thread.acc = 0;\n\tcurrent->thread.spefscr = 0;\n\tcurrent->thread.used_spe = 0;\n#endif /* CONFIG_SPE */\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n\tif (cpu_has_feature(CPU_FTR_TM))\n\t\tregs->msr |= MSR_TM;\n\tcurrent->thread.tm_tfhar = 0;\n\tcurrent->thread.tm_texasr = 0;\n\tcurrent->thread.tm_tfiar = 0;\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n}\n\n#define PR_FP_ALL_EXCEPT (PR_FP_EXC_DIV | PR_FP_EXC_OVF | PR_FP_EXC_UND \\\n\t\t| PR_FP_EXC_RES | PR_FP_EXC_INV)\n\nint set_fpexc_mode(struct task_struct *tsk, unsigned int val)\n{\n\tstruct pt_regs *regs = tsk->thread.regs;\n\n\t/* This is a bit hairy.  If we are an SPE enabled  processor\n\t * (have embedded fp) we store the IEEE exception enable flags in\n\t * fpexc_mode.  fpexc_mode is also used for setting FP exception\n\t * mode (asyn, precise, disabled) for 'Classic' FP. */\n\tif (val & PR_FP_EXC_SW_ENABLE) {\n#ifdef CONFIG_SPE\n\t\tif (cpu_has_feature(CPU_FTR_SPE)) {\n\t\t\t/*\n\t\t\t * When the sticky exception bits are set\n\t\t\t * directly by userspace, it must call prctl\n\t\t\t * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE\n\t\t\t * in the existing prctl settings) or\n\t\t\t * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in\n\t\t\t * the bits being set).  <fenv.h> functions\n\t\t\t * saving and restoring the whole\n\t\t\t * floating-point environment need to do so\n\t\t\t * anyway to restore the prctl settings from\n\t\t\t * the saved environment.\n\t\t\t */\n\t\t\ttsk->thread.spefscr_last = mfspr(SPRN_SPEFSCR);\n\t\t\ttsk->thread.fpexc_mode = val &\n\t\t\t\t(PR_FP_EXC_SW_ENABLE | PR_FP_ALL_EXCEPT);\n\t\t\treturn 0;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n#else\n\t\treturn -EINVAL;\n#endif\n\t}\n\n\t/* on a CONFIG_SPE this does not hurt us.  The bits that\n\t * __pack_fe01 use do not overlap with bits used for\n\t * PR_FP_EXC_SW_ENABLE.  Additionally, the MSR[FE0,FE1] bits\n\t * on CONFIG_SPE implementations are reserved so writing to\n\t * them does not change anything */\n\tif (val > PR_FP_EXC_PRECISE)\n\t\treturn -EINVAL;\n\ttsk->thread.fpexc_mode = __pack_fe01(val);\n\tif (regs != NULL && (regs->msr & MSR_FP) != 0)\n\t\tregs->msr = (regs->msr & ~(MSR_FE0|MSR_FE1))\n\t\t\t| tsk->thread.fpexc_mode;\n\treturn 0;\n}\n\nint get_fpexc_mode(struct task_struct *tsk, unsigned long adr)\n{\n\tunsigned int val;\n\n\tif (tsk->thread.fpexc_mode & PR_FP_EXC_SW_ENABLE)\n#ifdef CONFIG_SPE\n\t\tif (cpu_has_feature(CPU_FTR_SPE)) {\n\t\t\t/*\n\t\t\t * When the sticky exception bits are set\n\t\t\t * directly by userspace, it must call prctl\n\t\t\t * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE\n\t\t\t * in the existing prctl settings) or\n\t\t\t * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in\n\t\t\t * the bits being set).  <fenv.h> functions\n\t\t\t * saving and restoring the whole\n\t\t\t * floating-point environment need to do so\n\t\t\t * anyway to restore the prctl settings from\n\t\t\t * the saved environment.\n\t\t\t */\n\t\t\ttsk->thread.spefscr_last = mfspr(SPRN_SPEFSCR);\n\t\t\tval = tsk->thread.fpexc_mode;\n\t\t} else\n\t\t\treturn -EINVAL;\n#else\n\t\treturn -EINVAL;\n#endif\n\telse\n\t\tval = __unpack_fe01(tsk->thread.fpexc_mode);\n\treturn put_user(val, (unsigned int __user *) adr);\n}\n\nint set_endian(struct task_struct *tsk, unsigned int val)\n{\n\tstruct pt_regs *regs = tsk->thread.regs;\n\n\tif ((val == PR_ENDIAN_LITTLE && !cpu_has_feature(CPU_FTR_REAL_LE)) ||\n\t    (val == PR_ENDIAN_PPC_LITTLE && !cpu_has_feature(CPU_FTR_PPC_LE)))\n\t\treturn -EINVAL;\n\n\tif (regs == NULL)\n\t\treturn -EINVAL;\n\n\tif (val == PR_ENDIAN_BIG)\n\t\tregs->msr &= ~MSR_LE;\n\telse if (val == PR_ENDIAN_LITTLE || val == PR_ENDIAN_PPC_LITTLE)\n\t\tregs->msr |= MSR_LE;\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint get_endian(struct task_struct *tsk, unsigned long adr)\n{\n\tstruct pt_regs *regs = tsk->thread.regs;\n\tunsigned int val;\n\n\tif (!cpu_has_feature(CPU_FTR_PPC_LE) &&\n\t    !cpu_has_feature(CPU_FTR_REAL_LE))\n\t\treturn -EINVAL;\n\n\tif (regs == NULL)\n\t\treturn -EINVAL;\n\n\tif (regs->msr & MSR_LE) {\n\t\tif (cpu_has_feature(CPU_FTR_REAL_LE))\n\t\t\tval = PR_ENDIAN_LITTLE;\n\t\telse\n\t\t\tval = PR_ENDIAN_PPC_LITTLE;\n\t} else\n\t\tval = PR_ENDIAN_BIG;\n\n\treturn put_user(val, (unsigned int __user *)adr);\n}\n\nint set_unalign_ctl(struct task_struct *tsk, unsigned int val)\n{\n\ttsk->thread.align_ctl = val;\n\treturn 0;\n}\n\nint get_unalign_ctl(struct task_struct *tsk, unsigned long adr)\n{\n\treturn put_user(tsk->thread.align_ctl, (unsigned int __user *)adr);\n}\n\nstatic inline int valid_irq_stack(unsigned long sp, struct task_struct *p,\n\t\t\t\t  unsigned long nbytes)\n{\n\tunsigned long stack_page;\n\tunsigned long cpu = task_cpu(p);\n\n\t/*\n\t * Avoid crashing if the stack has overflowed and corrupted\n\t * task_cpu(p), which is in the thread_info struct.\n\t */\n\tif (cpu < NR_CPUS && cpu_possible(cpu)) {\n\t\tstack_page = (unsigned long) hardirq_ctx[cpu];\n\t\tif (sp >= stack_page + sizeof(struct thread_struct)\n\t\t    && sp <= stack_page + THREAD_SIZE - nbytes)\n\t\t\treturn 1;\n\n\t\tstack_page = (unsigned long) softirq_ctx[cpu];\n\t\tif (sp >= stack_page + sizeof(struct thread_struct)\n\t\t    && sp <= stack_page + THREAD_SIZE - nbytes)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nint validate_sp(unsigned long sp, struct task_struct *p,\n\t\t       unsigned long nbytes)\n{\n\tunsigned long stack_page = (unsigned long)task_stack_page(p);\n\n\tif (sp >= stack_page + sizeof(struct thread_struct)\n\t    && sp <= stack_page + THREAD_SIZE - nbytes)\n\t\treturn 1;\n\n\treturn valid_irq_stack(sp, p, nbytes);\n}\n\nEXPORT_SYMBOL(validate_sp);\n\nunsigned long get_wchan(struct task_struct *p)\n{\n\tunsigned long ip, sp;\n\tint count = 0;\n\n\tif (!p || p == current || p->state == TASK_RUNNING)\n\t\treturn 0;\n\n\tsp = p->thread.ksp;\n\tif (!validate_sp(sp, p, STACK_FRAME_OVERHEAD))\n\t\treturn 0;\n\n\tdo {\n\t\tsp = *(unsigned long *)sp;\n\t\tif (!validate_sp(sp, p, STACK_FRAME_OVERHEAD))\n\t\t\treturn 0;\n\t\tif (count > 0) {\n\t\t\tip = ((unsigned long *)sp)[STACK_FRAME_LR_SAVE];\n\t\t\tif (!in_sched_functions(ip))\n\t\t\t\treturn ip;\n\t\t}\n\t} while (count++ < 16);\n\treturn 0;\n}\n\nstatic int kstack_depth_to_print = CONFIG_PRINT_STACK_DEPTH;\n\nvoid show_stack(struct task_struct *tsk, unsigned long *stack)\n{\n\tunsigned long sp, ip, lr, newsp;\n\tint count = 0;\n\tint firstframe = 1;\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tint curr_frame = current->curr_ret_stack;\n\textern void return_to_handler(void);\n\tunsigned long rth = (unsigned long)return_to_handler;\n\tunsigned long mrth = -1;\n#ifdef CONFIG_PPC64\n\textern void mod_return_to_handler(void);\n\trth = *(unsigned long *)rth;\n\tmrth = (unsigned long)mod_return_to_handler;\n\tmrth = *(unsigned long *)mrth;\n#endif\n#endif\n\n\tsp = (unsigned long) stack;\n\tif (tsk == NULL)\n\t\ttsk = current;\n\tif (sp == 0) {\n\t\tif (tsk == current)\n\t\t\tasm(\"mr %0,1\" : \"=r\" (sp));\n\t\telse\n\t\t\tsp = tsk->thread.ksp;\n\t}\n\n\tlr = 0;\n\tprintk(\"Call Trace:\\n\");\n\tdo {\n\t\tif (!validate_sp(sp, tsk, STACK_FRAME_OVERHEAD))\n\t\t\treturn;\n\n\t\tstack = (unsigned long *) sp;\n\t\tnewsp = stack[0];\n\t\tip = stack[STACK_FRAME_LR_SAVE];\n\t\tif (!firstframe || ip != lr) {\n\t\t\tprintk(\"[\"REG\"] [\"REG\"] %pS\", sp, ip, (void *)ip);\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\t\tif ((ip == rth || ip == mrth) && curr_frame >= 0) {\n\t\t\t\tprintk(\" (%pS)\",\n\t\t\t\t       (void *)current->ret_stack[curr_frame].ret);\n\t\t\t\tcurr_frame--;\n\t\t\t}\n#endif\n\t\t\tif (firstframe)\n\t\t\t\tprintk(\" (unreliable)\");\n\t\t\tprintk(\"\\n\");\n\t\t}\n\t\tfirstframe = 0;\n\n\t\t/*\n\t\t * See if this is an exception frame.\n\t\t * We look for the \"regshere\" marker in the current frame.\n\t\t */\n\t\tif (validate_sp(sp, tsk, STACK_INT_FRAME_SIZE)\n\t\t    && stack[STACK_FRAME_MARKER] == STACK_FRAME_REGS_MARKER) {\n\t\t\tstruct pt_regs *regs = (struct pt_regs *)\n\t\t\t\t(sp + STACK_FRAME_OVERHEAD);\n\t\t\tlr = regs->link;\n\t\t\tprintk(\"--- Exception: %lx at %pS\\n    LR = %pS\\n\",\n\t\t\t       regs->trap, (void *)regs->nip, (void *)lr);\n\t\t\tfirstframe = 1;\n\t\t}\n\n\t\tsp = newsp;\n\t} while (count++ < kstack_depth_to_print);\n}\n\n#ifdef CONFIG_PPC64\n/* Called with hard IRQs off */\nvoid notrace __ppc64_runlatch_on(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\tunsigned long ctrl;\n\n\tctrl = mfspr(SPRN_CTRLF);\n\tctrl |= CTRL_RUNLATCH;\n\tmtspr(SPRN_CTRLT, ctrl);\n\n\tti->local_flags |= _TLF_RUNLATCH;\n}\n\n/* Called with hard IRQs off */\nvoid notrace __ppc64_runlatch_off(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\tunsigned long ctrl;\n\n\tti->local_flags &= ~_TLF_RUNLATCH;\n\n\tctrl = mfspr(SPRN_CTRLF);\n\tctrl &= ~CTRL_RUNLATCH;\n\tmtspr(SPRN_CTRLT, ctrl);\n}\n#endif /* CONFIG_PPC64 */\n\nunsigned long arch_align_stack(unsigned long sp)\n{\n\tif (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)\n\t\tsp -= get_random_int() & ~PAGE_MASK;\n\treturn sp & ~0xf;\n}\n\nstatic inline unsigned long brk_rnd(void)\n{\n        unsigned long rnd = 0;\n\n\t/* 8MB for 32bit, 1GB for 64bit */\n\tif (is_32bit_task())\n\t\trnd = (long)(get_random_int() % (1<<(23-PAGE_SHIFT)));\n\telse\n\t\trnd = (long)(get_random_int() % (1<<(30-PAGE_SHIFT)));\n\n\treturn rnd << PAGE_SHIFT;\n}\n\nunsigned long arch_randomize_brk(struct mm_struct *mm)\n{\n\tunsigned long base = mm->brk;\n\tunsigned long ret;\n\n#ifdef CONFIG_PPC_STD_MMU_64\n\t/*\n\t * If we are using 1TB segments and we are allowed to randomise\n\t * the heap, we can put it above 1TB so it is backed by a 1TB\n\t * segment. Otherwise the heap will be in the bottom 1TB\n\t * which always uses 256MB segments and this may result in a\n\t * performance penalty.\n\t */\n\tif (!is_32bit_task() && (mmu_highuser_ssize == MMU_SEGSIZE_1T))\n\t\tbase = max_t(unsigned long, mm->brk, 1UL << SID_SHIFT_1T);\n#endif\n\n\tret = PAGE_ALIGN(base + brk_rnd());\n\n\tif (ret < mm->brk)\n\t\treturn mm->brk;\n\n\treturn ret;\n}\n\nunsigned long randomize_et_dyn(unsigned long base)\n{\n\tunsigned long ret = PAGE_ALIGN(base + brk_rnd());\n\n\tif (ret < base)\n\t\treturn base;\n\n\treturn ret;\n}\n"], "fixing_code": ["/*\n *  Derived from \"arch/i386/kernel/process.c\"\n *    Copyright (C) 1995  Linus Torvalds\n *\n *  Updated and modified by Cort Dougan (cort@cs.nmt.edu) and\n *  Paul Mackerras (paulus@cs.anu.edu.au)\n *\n *  PowerPC version\n *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)\n *\n *  This program is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU General Public License\n *  as published by the Free Software Foundation; either version\n *  2 of the License, or (at your option) any later version.\n */\n\n#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/stddef.h>\n#include <linux/unistd.h>\n#include <linux/ptrace.h>\n#include <linux/slab.h>\n#include <linux/user.h>\n#include <linux/elf.h>\n#include <linux/prctl.h>\n#include <linux/init_task.h>\n#include <linux/export.h>\n#include <linux/kallsyms.h>\n#include <linux/mqueue.h>\n#include <linux/hardirq.h>\n#include <linux/utsname.h>\n#include <linux/ftrace.h>\n#include <linux/kernel_stat.h>\n#include <linux/personality.h>\n#include <linux/random.h>\n#include <linux/hw_breakpoint.h>\n\n#include <asm/pgtable.h>\n#include <asm/uaccess.h>\n#include <asm/io.h>\n#include <asm/processor.h>\n#include <asm/mmu.h>\n#include <asm/prom.h>\n#include <asm/machdep.h>\n#include <asm/time.h>\n#include <asm/runlatch.h>\n#include <asm/syscalls.h>\n#include <asm/switch_to.h>\n#include <asm/tm.h>\n#include <asm/debug.h>\n#ifdef CONFIG_PPC64\n#include <asm/firmware.h>\n#endif\n#include <linux/kprobes.h>\n#include <linux/kdebug.h>\n\n/* Transactional Memory debug */\n#ifdef TM_DEBUG_SW\n#define TM_DEBUG(x...) printk(KERN_INFO x)\n#else\n#define TM_DEBUG(x...) do { } while(0)\n#endif\n\nextern unsigned long _get_SP(void);\n\n#ifndef CONFIG_SMP\nstruct task_struct *last_task_used_math = NULL;\nstruct task_struct *last_task_used_altivec = NULL;\nstruct task_struct *last_task_used_vsx = NULL;\nstruct task_struct *last_task_used_spe = NULL;\n#endif\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nvoid giveup_fpu_maybe_transactional(struct task_struct *tsk)\n{\n\t/*\n\t * If we are saving the current thread's registers, and the\n\t * thread is in a transactional state, set the TIF_RESTORE_TM\n\t * bit so that we know to restore the registers before\n\t * returning to userspace.\n\t */\n\tif (tsk == current && tsk->thread.regs &&\n\t    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&\n\t    !test_thread_flag(TIF_RESTORE_TM)) {\n\t\ttsk->thread.tm_orig_msr = tsk->thread.regs->msr;\n\t\tset_thread_flag(TIF_RESTORE_TM);\n\t}\n\n\tgiveup_fpu(tsk);\n}\n\nvoid giveup_altivec_maybe_transactional(struct task_struct *tsk)\n{\n\t/*\n\t * If we are saving the current thread's registers, and the\n\t * thread is in a transactional state, set the TIF_RESTORE_TM\n\t * bit so that we know to restore the registers before\n\t * returning to userspace.\n\t */\n\tif (tsk == current && tsk->thread.regs &&\n\t    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&\n\t    !test_thread_flag(TIF_RESTORE_TM)) {\n\t\ttsk->thread.tm_orig_msr = tsk->thread.regs->msr;\n\t\tset_thread_flag(TIF_RESTORE_TM);\n\t}\n\n\tgiveup_altivec(tsk);\n}\n\n#else\n#define giveup_fpu_maybe_transactional(tsk)\tgiveup_fpu(tsk)\n#define giveup_altivec_maybe_transactional(tsk)\tgiveup_altivec(tsk)\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n\n#ifdef CONFIG_PPC_FPU\n/*\n * Make sure the floating-point register state in the\n * the thread_struct is up to date for task tsk.\n */\nvoid flush_fp_to_thread(struct task_struct *tsk)\n{\n\tif (tsk->thread.regs) {\n\t\t/*\n\t\t * We need to disable preemption here because if we didn't,\n\t\t * another process could get scheduled after the regs->msr\n\t\t * test but before we have finished saving the FP registers\n\t\t * to the thread_struct.  That process could take over the\n\t\t * FPU, and then when we get scheduled again we would store\n\t\t * bogus values for the remaining FP registers.\n\t\t */\n\t\tpreempt_disable();\n\t\tif (tsk->thread.regs->msr & MSR_FP) {\n#ifdef CONFIG_SMP\n\t\t\t/*\n\t\t\t * This should only ever be called for current or\n\t\t\t * for a stopped child process.  Since we save away\n\t\t\t * the FP register state on context switch on SMP,\n\t\t\t * there is something wrong if a stopped child appears\n\t\t\t * to still have its FP state in the CPU registers.\n\t\t\t */\n\t\t\tBUG_ON(tsk != current);\n#endif\n\t\t\tgiveup_fpu_maybe_transactional(tsk);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL_GPL(flush_fp_to_thread);\n#endif /* CONFIG_PPC_FPU */\n\nvoid enable_kernel_fp(void)\n{\n\tWARN_ON(preemptible());\n\n#ifdef CONFIG_SMP\n\tif (current->thread.regs && (current->thread.regs->msr & MSR_FP))\n\t\tgiveup_fpu_maybe_transactional(current);\n\telse\n\t\tgiveup_fpu(NULL);\t/* just enables FP for kernel */\n#else\n\tgiveup_fpu_maybe_transactional(last_task_used_math);\n#endif /* CONFIG_SMP */\n}\nEXPORT_SYMBOL(enable_kernel_fp);\n\n#ifdef CONFIG_ALTIVEC\nvoid enable_kernel_altivec(void)\n{\n\tWARN_ON(preemptible());\n\n#ifdef CONFIG_SMP\n\tif (current->thread.regs && (current->thread.regs->msr & MSR_VEC))\n\t\tgiveup_altivec_maybe_transactional(current);\n\telse\n\t\tgiveup_altivec_notask();\n#else\n\tgiveup_altivec_maybe_transactional(last_task_used_altivec);\n#endif /* CONFIG_SMP */\n}\nEXPORT_SYMBOL(enable_kernel_altivec);\n\n/*\n * Make sure the VMX/Altivec register state in the\n * the thread_struct is up to date for task tsk.\n */\nvoid flush_altivec_to_thread(struct task_struct *tsk)\n{\n\tif (tsk->thread.regs) {\n\t\tpreempt_disable();\n\t\tif (tsk->thread.regs->msr & MSR_VEC) {\n#ifdef CONFIG_SMP\n\t\t\tBUG_ON(tsk != current);\n#endif\n\t\t\tgiveup_altivec_maybe_transactional(tsk);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL_GPL(flush_altivec_to_thread);\n#endif /* CONFIG_ALTIVEC */\n\n#ifdef CONFIG_VSX\n#if 0\n/* not currently used, but some crazy RAID module might want to later */\nvoid enable_kernel_vsx(void)\n{\n\tWARN_ON(preemptible());\n\n#ifdef CONFIG_SMP\n\tif (current->thread.regs && (current->thread.regs->msr & MSR_VSX))\n\t\tgiveup_vsx(current);\n\telse\n\t\tgiveup_vsx(NULL);\t/* just enable vsx for kernel - force */\n#else\n\tgiveup_vsx(last_task_used_vsx);\n#endif /* CONFIG_SMP */\n}\nEXPORT_SYMBOL(enable_kernel_vsx);\n#endif\n\nvoid giveup_vsx(struct task_struct *tsk)\n{\n\tgiveup_fpu_maybe_transactional(tsk);\n\tgiveup_altivec_maybe_transactional(tsk);\n\t__giveup_vsx(tsk);\n}\n\nvoid flush_vsx_to_thread(struct task_struct *tsk)\n{\n\tif (tsk->thread.regs) {\n\t\tpreempt_disable();\n\t\tif (tsk->thread.regs->msr & MSR_VSX) {\n#ifdef CONFIG_SMP\n\t\t\tBUG_ON(tsk != current);\n#endif\n\t\t\tgiveup_vsx(tsk);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL_GPL(flush_vsx_to_thread);\n#endif /* CONFIG_VSX */\n\n#ifdef CONFIG_SPE\n\nvoid enable_kernel_spe(void)\n{\n\tWARN_ON(preemptible());\n\n#ifdef CONFIG_SMP\n\tif (current->thread.regs && (current->thread.regs->msr & MSR_SPE))\n\t\tgiveup_spe(current);\n\telse\n\t\tgiveup_spe(NULL);\t/* just enable SPE for kernel - force */\n#else\n\tgiveup_spe(last_task_used_spe);\n#endif /* __SMP __ */\n}\nEXPORT_SYMBOL(enable_kernel_spe);\n\nvoid flush_spe_to_thread(struct task_struct *tsk)\n{\n\tif (tsk->thread.regs) {\n\t\tpreempt_disable();\n\t\tif (tsk->thread.regs->msr & MSR_SPE) {\n#ifdef CONFIG_SMP\n\t\t\tBUG_ON(tsk != current);\n#endif\n\t\t\ttsk->thread.spefscr = mfspr(SPRN_SPEFSCR);\n\t\t\tgiveup_spe(tsk);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\n#endif /* CONFIG_SPE */\n\n#ifndef CONFIG_SMP\n/*\n * If we are doing lazy switching of CPU state (FP, altivec or SPE),\n * and the current task has some state, discard it.\n */\nvoid discard_lazy_cpu_state(void)\n{\n\tpreempt_disable();\n\tif (last_task_used_math == current)\n\t\tlast_task_used_math = NULL;\n#ifdef CONFIG_ALTIVEC\n\tif (last_task_used_altivec == current)\n\t\tlast_task_used_altivec = NULL;\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_VSX\n\tif (last_task_used_vsx == current)\n\t\tlast_task_used_vsx = NULL;\n#endif /* CONFIG_VSX */\n#ifdef CONFIG_SPE\n\tif (last_task_used_spe == current)\n\t\tlast_task_used_spe = NULL;\n#endif\n\tpreempt_enable();\n}\n#endif /* CONFIG_SMP */\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\nvoid do_send_trap(struct pt_regs *regs, unsigned long address,\n\t\t  unsigned long error_code, int signal_code, int breakpt)\n{\n\tsiginfo_t info;\n\n\tcurrent->thread.trap_nr = signal_code;\n\tif (notify_die(DIE_DABR_MATCH, \"dabr_match\", regs, error_code,\n\t\t\t11, SIGSEGV) == NOTIFY_STOP)\n\t\treturn;\n\n\t/* Deliver the signal to userspace */\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = breakpt;\t/* breakpoint or watchpoint id */\n\tinfo.si_code = signal_code;\n\tinfo.si_addr = (void __user *)address;\n\tforce_sig_info(SIGTRAP, &info, current);\n}\n#else\t/* !CONFIG_PPC_ADV_DEBUG_REGS */\nvoid do_break (struct pt_regs *regs, unsigned long address,\n\t\t    unsigned long error_code)\n{\n\tsiginfo_t info;\n\n\tcurrent->thread.trap_nr = TRAP_HWBKPT;\n\tif (notify_die(DIE_DABR_MATCH, \"dabr_match\", regs, error_code,\n\t\t\t11, SIGSEGV) == NOTIFY_STOP)\n\t\treturn;\n\n\tif (debugger_break_match(regs))\n\t\treturn;\n\n\t/* Clear the breakpoint */\n\thw_breakpoint_disable();\n\n\t/* Deliver the signal to userspace */\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = 0;\n\tinfo.si_code = TRAP_HWBKPT;\n\tinfo.si_addr = (void __user *)address;\n\tforce_sig_info(SIGTRAP, &info, current);\n}\n#endif\t/* CONFIG_PPC_ADV_DEBUG_REGS */\n\nstatic DEFINE_PER_CPU(struct arch_hw_breakpoint, current_brk);\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n/*\n * Set the debug registers back to their default \"safe\" values.\n */\nstatic void set_debug_reg_defaults(struct thread_struct *thread)\n{\n\tthread->debug.iac1 = thread->debug.iac2 = 0;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\tthread->debug.iac3 = thread->debug.iac4 = 0;\n#endif\n\tthread->debug.dac1 = thread->debug.dac2 = 0;\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\tthread->debug.dvc1 = thread->debug.dvc2 = 0;\n#endif\n\tthread->debug.dbcr0 = 0;\n#ifdef CONFIG_BOOKE\n\t/*\n\t * Force User/Supervisor bits to b11 (user-only MSR[PR]=1)\n\t */\n\tthread->debug.dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |\n\t\t\tDBCR1_IAC3US | DBCR1_IAC4US;\n\t/*\n\t * Force Data Address Compare User/Supervisor bits to be User-only\n\t * (0b11 MSR[PR]=1) and set all other bits in DBCR2 register to be 0.\n\t */\n\tthread->debug.dbcr2 = DBCR2_DAC1US | DBCR2_DAC2US;\n#else\n\tthread->debug.dbcr1 = 0;\n#endif\n}\n\nstatic void prime_debug_regs(struct debug_reg *debug)\n{\n\t/*\n\t * We could have inherited MSR_DE from userspace, since\n\t * it doesn't get cleared on exception entry.  Make sure\n\t * MSR_DE is clear before we enable any debug events.\n\t */\n\tmtmsr(mfmsr() & ~MSR_DE);\n\n\tmtspr(SPRN_IAC1, debug->iac1);\n\tmtspr(SPRN_IAC2, debug->iac2);\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\tmtspr(SPRN_IAC3, debug->iac3);\n\tmtspr(SPRN_IAC4, debug->iac4);\n#endif\n\tmtspr(SPRN_DAC1, debug->dac1);\n\tmtspr(SPRN_DAC2, debug->dac2);\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\tmtspr(SPRN_DVC1, debug->dvc1);\n\tmtspr(SPRN_DVC2, debug->dvc2);\n#endif\n\tmtspr(SPRN_DBCR0, debug->dbcr0);\n\tmtspr(SPRN_DBCR1, debug->dbcr1);\n#ifdef CONFIG_BOOKE\n\tmtspr(SPRN_DBCR2, debug->dbcr2);\n#endif\n}\n/*\n * Unless neither the old or new thread are making use of the\n * debug registers, set the debug registers from the values\n * stored in the new thread.\n */\nvoid switch_booke_debug_regs(struct debug_reg *new_debug)\n{\n\tif ((current->thread.debug.dbcr0 & DBCR0_IDM)\n\t\t|| (new_debug->dbcr0 & DBCR0_IDM))\n\t\t\tprime_debug_regs(new_debug);\n}\nEXPORT_SYMBOL_GPL(switch_booke_debug_regs);\n#else\t/* !CONFIG_PPC_ADV_DEBUG_REGS */\n#ifndef CONFIG_HAVE_HW_BREAKPOINT\nstatic void set_debug_reg_defaults(struct thread_struct *thread)\n{\n\tthread->hw_brk.address = 0;\n\tthread->hw_brk.type = 0;\n\tset_breakpoint(&thread->hw_brk);\n}\n#endif /* !CONFIG_HAVE_HW_BREAKPOINT */\n#endif\t/* CONFIG_PPC_ADV_DEBUG_REGS */\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\nstatic inline int __set_dabr(unsigned long dabr, unsigned long dabrx)\n{\n\tmtspr(SPRN_DAC1, dabr);\n#ifdef CONFIG_PPC_47x\n\tisync();\n#endif\n\treturn 0;\n}\n#elif defined(CONFIG_PPC_BOOK3S)\nstatic inline int __set_dabr(unsigned long dabr, unsigned long dabrx)\n{\n\tmtspr(SPRN_DABR, dabr);\n\tif (cpu_has_feature(CPU_FTR_DABRX))\n\t\tmtspr(SPRN_DABRX, dabrx);\n\treturn 0;\n}\n#else\nstatic inline int __set_dabr(unsigned long dabr, unsigned long dabrx)\n{\n\treturn -EINVAL;\n}\n#endif\n\nstatic inline int set_dabr(struct arch_hw_breakpoint *brk)\n{\n\tunsigned long dabr, dabrx;\n\n\tdabr = brk->address | (brk->type & HW_BRK_TYPE_DABR);\n\tdabrx = ((brk->type >> 3) & 0x7);\n\n\tif (ppc_md.set_dabr)\n\t\treturn ppc_md.set_dabr(dabr, dabrx);\n\n\treturn __set_dabr(dabr, dabrx);\n}\n\nstatic inline int set_dawr(struct arch_hw_breakpoint *brk)\n{\n\tunsigned long dawr, dawrx, mrd;\n\n\tdawr = brk->address;\n\n\tdawrx  = (brk->type & (HW_BRK_TYPE_READ | HW_BRK_TYPE_WRITE)) \\\n\t\t                   << (63 - 58); //* read/write bits */\n\tdawrx |= ((brk->type & (HW_BRK_TYPE_TRANSLATE)) >> 2) \\\n\t\t                   << (63 - 59); //* translate */\n\tdawrx |= (brk->type & (HW_BRK_TYPE_PRIV_ALL)) \\\n\t\t                   >> 3; //* PRIM bits */\n\t/* dawr length is stored in field MDR bits 48:53.  Matches range in\n\t   doublewords (64 bits) baised by -1 eg. 0b000000=1DW and\n\t   0b111111=64DW.\n\t   brk->len is in bytes.\n\t   This aligns up to double word size, shifts and does the bias.\n\t*/\n\tmrd = ((brk->len + 7) >> 3) - 1;\n\tdawrx |= (mrd & 0x3f) << (63 - 53);\n\n\tif (ppc_md.set_dawr)\n\t\treturn ppc_md.set_dawr(dawr, dawrx);\n\tmtspr(SPRN_DAWR, dawr);\n\tmtspr(SPRN_DAWRX, dawrx);\n\treturn 0;\n}\n\nint set_breakpoint(struct arch_hw_breakpoint *brk)\n{\n\t__get_cpu_var(current_brk) = *brk;\n\n\tif (cpu_has_feature(CPU_FTR_DAWR))\n\t\treturn set_dawr(brk);\n\n\treturn set_dabr(brk);\n}\n\n#ifdef CONFIG_PPC64\nDEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);\n#endif\n\nstatic inline bool hw_brk_match(struct arch_hw_breakpoint *a,\n\t\t\t      struct arch_hw_breakpoint *b)\n{\n\tif (a->address != b->address)\n\t\treturn false;\n\tif (a->type != b->type)\n\t\treturn false;\n\tif (a->len != b->len)\n\t\treturn false;\n\treturn true;\n}\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nstatic void tm_reclaim_thread(struct thread_struct *thr,\n\t\t\t      struct thread_info *ti, uint8_t cause)\n{\n\tunsigned long msr_diff = 0;\n\n\t/*\n\t * If FP/VSX registers have been already saved to the\n\t * thread_struct, move them to the transact_fp array.\n\t * We clear the TIF_RESTORE_TM bit since after the reclaim\n\t * the thread will no longer be transactional.\n\t */\n\tif (test_ti_thread_flag(ti, TIF_RESTORE_TM)) {\n\t\tmsr_diff = thr->tm_orig_msr & ~thr->regs->msr;\n\t\tif (msr_diff & MSR_FP)\n\t\t\tmemcpy(&thr->transact_fp, &thr->fp_state,\n\t\t\t       sizeof(struct thread_fp_state));\n\t\tif (msr_diff & MSR_VEC)\n\t\t\tmemcpy(&thr->transact_vr, &thr->vr_state,\n\t\t\t       sizeof(struct thread_vr_state));\n\t\tclear_ti_thread_flag(ti, TIF_RESTORE_TM);\n\t\tmsr_diff &= MSR_FP | MSR_VEC | MSR_VSX | MSR_FE0 | MSR_FE1;\n\t}\n\n\ttm_reclaim(thr, thr->regs->msr, cause);\n\n\t/* Having done the reclaim, we now have the checkpointed\n\t * FP/VSX values in the registers.  These might be valid\n\t * even if we have previously called enable_kernel_fp() or\n\t * flush_fp_to_thread(), so update thr->regs->msr to\n\t * indicate their current validity.\n\t */\n\tthr->regs->msr |= msr_diff;\n}\n\nvoid tm_reclaim_current(uint8_t cause)\n{\n\ttm_enable();\n\ttm_reclaim_thread(&current->thread, current_thread_info(), cause);\n}\n\nstatic inline void tm_reclaim_task(struct task_struct *tsk)\n{\n\t/* We have to work out if we're switching from/to a task that's in the\n\t * middle of a transaction.\n\t *\n\t * In switching we need to maintain a 2nd register state as\n\t * oldtask->thread.ckpt_regs.  We tm_reclaim(oldproc); this saves the\n\t * checkpointed (tbegin) state in ckpt_regs and saves the transactional\n\t * (current) FPRs into oldtask->thread.transact_fpr[].\n\t *\n\t * We also context switch (save) TFHAR/TEXASR/TFIAR in here.\n\t */\n\tstruct thread_struct *thr = &tsk->thread;\n\n\tif (!thr->regs)\n\t\treturn;\n\n\tif (!MSR_TM_ACTIVE(thr->regs->msr))\n\t\tgoto out_and_saveregs;\n\n\t/* Stash the original thread MSR, as giveup_fpu et al will\n\t * modify it.  We hold onto it to see whether the task used\n\t * FP & vector regs.  If the TIF_RESTORE_TM flag is set,\n\t * tm_orig_msr is already set.\n\t */\n\tif (!test_ti_thread_flag(task_thread_info(tsk), TIF_RESTORE_TM))\n\t\tthr->tm_orig_msr = thr->regs->msr;\n\n\tTM_DEBUG(\"--- tm_reclaim on pid %d (NIP=%lx, \"\n\t\t \"ccr=%lx, msr=%lx, trap=%lx)\\n\",\n\t\t tsk->pid, thr->regs->nip,\n\t\t thr->regs->ccr, thr->regs->msr,\n\t\t thr->regs->trap);\n\n\ttm_reclaim_thread(thr, task_thread_info(tsk), TM_CAUSE_RESCHED);\n\n\tTM_DEBUG(\"--- tm_reclaim on pid %d complete\\n\",\n\t\t tsk->pid);\n\nout_and_saveregs:\n\t/* Always save the regs here, even if a transaction's not active.\n\t * This context-switches a thread's TM info SPRs.  We do it here to\n\t * be consistent with the restore path (in recheckpoint) which\n\t * cannot happen later in _switch().\n\t */\n\ttm_save_sprs(thr);\n}\n\nstatic inline void tm_recheckpoint_new_task(struct task_struct *new)\n{\n\tunsigned long msr;\n\n\tif (!cpu_has_feature(CPU_FTR_TM))\n\t\treturn;\n\n\t/* Recheckpoint the registers of the thread we're about to switch to.\n\t *\n\t * If the task was using FP, we non-lazily reload both the original and\n\t * the speculative FP register states.  This is because the kernel\n\t * doesn't see if/when a TM rollback occurs, so if we take an FP\n\t * unavoidable later, we are unable to determine which set of FP regs\n\t * need to be restored.\n\t */\n\tif (!new->thread.regs)\n\t\treturn;\n\n\t/* The TM SPRs are restored here, so that TEXASR.FS can be set\n\t * before the trecheckpoint and no explosion occurs.\n\t */\n\ttm_restore_sprs(&new->thread);\n\n\tif (!MSR_TM_ACTIVE(new->thread.regs->msr))\n\t\treturn;\n\tmsr = new->thread.tm_orig_msr;\n\t/* Recheckpoint to restore original checkpointed register state. */\n\tTM_DEBUG(\"*** tm_recheckpoint of pid %d \"\n\t\t \"(new->msr 0x%lx, new->origmsr 0x%lx)\\n\",\n\t\t new->pid, new->thread.regs->msr, msr);\n\n\t/* This loads the checkpointed FP/VEC state, if used */\n\ttm_recheckpoint(&new->thread, msr);\n\n\t/* This loads the speculative FP/VEC state, if used */\n\tif (msr & MSR_FP) {\n\t\tdo_load_up_transact_fpu(&new->thread);\n\t\tnew->thread.regs->msr |=\n\t\t\t(MSR_FP | new->thread.fpexc_mode);\n\t}\n#ifdef CONFIG_ALTIVEC\n\tif (msr & MSR_VEC) {\n\t\tdo_load_up_transact_altivec(&new->thread);\n\t\tnew->thread.regs->msr |= MSR_VEC;\n\t}\n#endif\n\t/* We may as well turn on VSX too since all the state is restored now */\n\tif (msr & MSR_VSX)\n\t\tnew->thread.regs->msr |= MSR_VSX;\n\n\tTM_DEBUG(\"*** tm_recheckpoint of pid %d complete \"\n\t\t \"(kernel msr 0x%lx)\\n\",\n\t\t new->pid, mfmsr());\n}\n\nstatic inline void __switch_to_tm(struct task_struct *prev)\n{\n\tif (cpu_has_feature(CPU_FTR_TM)) {\n\t\ttm_enable();\n\t\ttm_reclaim_task(prev);\n\t}\n}\n\n/*\n * This is called if we are on the way out to userspace and the\n * TIF_RESTORE_TM flag is set.  It checks if we need to reload\n * FP and/or vector state and does so if necessary.\n * If userspace is inside a transaction (whether active or\n * suspended) and FP/VMX/VSX instructions have ever been enabled\n * inside that transaction, then we have to keep them enabled\n * and keep the FP/VMX/VSX state loaded while ever the transaction\n * continues.  The reason is that if we didn't, and subsequently\n * got a FP/VMX/VSX unavailable interrupt inside a transaction,\n * we don't know whether it's the same transaction, and thus we\n * don't know which of the checkpointed state and the transactional\n * state to use.\n */\nvoid restore_tm_state(struct pt_regs *regs)\n{\n\tunsigned long msr_diff;\n\n\tclear_thread_flag(TIF_RESTORE_TM);\n\tif (!MSR_TM_ACTIVE(regs->msr))\n\t\treturn;\n\n\tmsr_diff = current->thread.tm_orig_msr & ~regs->msr;\n\tmsr_diff &= MSR_FP | MSR_VEC | MSR_VSX;\n\tif (msr_diff & MSR_FP) {\n\t\tfp_enable();\n\t\tload_fp_state(&current->thread.fp_state);\n\t\tregs->msr |= current->thread.fpexc_mode;\n\t}\n\tif (msr_diff & MSR_VEC) {\n\t\tvec_enable();\n\t\tload_vr_state(&current->thread.vr_state);\n\t}\n\tregs->msr |= msr_diff;\n}\n\n#else\n#define tm_recheckpoint_new_task(new)\n#define __switch_to_tm(prev)\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n\nstruct task_struct *__switch_to(struct task_struct *prev,\n\tstruct task_struct *new)\n{\n\tstruct thread_struct *new_thread, *old_thread;\n\tstruct task_struct *last;\n#ifdef CONFIG_PPC_BOOK3S_64\n\tstruct ppc64_tlb_batch *batch;\n#endif\n\n\tWARN_ON(!irqs_disabled());\n\n\t/* Back up the TAR across context switches.\n\t * Note that the TAR is not available for use in the kernel.  (To\n\t * provide this, the TAR should be backed up/restored on exception\n\t * entry/exit instead, and be in pt_regs.  FIXME, this should be in\n\t * pt_regs anyway (for debug).)\n\t * Save the TAR here before we do treclaim/trecheckpoint as these\n\t * will change the TAR.\n\t */\n\tsave_tar(&prev->thread);\n\n\t__switch_to_tm(prev);\n\n#ifdef CONFIG_SMP\n\t/* avoid complexity of lazy save/restore of fpu\n\t * by just saving it every time we switch out if\n\t * this task used the fpu during the last quantum.\n\t *\n\t * If it tries to use the fpu again, it'll trap and\n\t * reload its fp regs.  So we don't have to do a restore\n\t * every switch, just a save.\n\t *  -- Cort\n\t */\n\tif (prev->thread.regs && (prev->thread.regs->msr & MSR_FP))\n\t\tgiveup_fpu(prev);\n#ifdef CONFIG_ALTIVEC\n\t/*\n\t * If the previous thread used altivec in the last quantum\n\t * (thus changing altivec regs) then save them.\n\t * We used to check the VRSAVE register but not all apps\n\t * set it, so we don't rely on it now (and in fact we need\n\t * to save & restore VSCR even if VRSAVE == 0).  -- paulus\n\t *\n\t * On SMP we always save/restore altivec regs just to avoid the\n\t * complexity of changing processors.\n\t *  -- Cort\n\t */\n\tif (prev->thread.regs && (prev->thread.regs->msr & MSR_VEC))\n\t\tgiveup_altivec(prev);\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_VSX\n\tif (prev->thread.regs && (prev->thread.regs->msr & MSR_VSX))\n\t\t/* VMX and FPU registers are already save here */\n\t\t__giveup_vsx(prev);\n#endif /* CONFIG_VSX */\n#ifdef CONFIG_SPE\n\t/*\n\t * If the previous thread used spe in the last quantum\n\t * (thus changing spe regs) then save them.\n\t *\n\t * On SMP we always save/restore spe regs just to avoid the\n\t * complexity of changing processors.\n\t */\n\tif ((prev->thread.regs && (prev->thread.regs->msr & MSR_SPE)))\n\t\tgiveup_spe(prev);\n#endif /* CONFIG_SPE */\n\n#else  /* CONFIG_SMP */\n#ifdef CONFIG_ALTIVEC\n\t/* Avoid the trap.  On smp this this never happens since\n\t * we don't set last_task_used_altivec -- Cort\n\t */\n\tif (new->thread.regs && last_task_used_altivec == new)\n\t\tnew->thread.regs->msr |= MSR_VEC;\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_VSX\n\tif (new->thread.regs && last_task_used_vsx == new)\n\t\tnew->thread.regs->msr |= MSR_VSX;\n#endif /* CONFIG_VSX */\n#ifdef CONFIG_SPE\n\t/* Avoid the trap.  On smp this this never happens since\n\t * we don't set last_task_used_spe\n\t */\n\tif (new->thread.regs && last_task_used_spe == new)\n\t\tnew->thread.regs->msr |= MSR_SPE;\n#endif /* CONFIG_SPE */\n\n#endif /* CONFIG_SMP */\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\tswitch_booke_debug_regs(&new->thread.debug);\n#else\n/*\n * For PPC_BOOK3S_64, we use the hw-breakpoint interfaces that would\n * schedule DABR\n */\n#ifndef CONFIG_HAVE_HW_BREAKPOINT\n\tif (unlikely(!hw_brk_match(&__get_cpu_var(current_brk), &new->thread.hw_brk)))\n\t\tset_breakpoint(&new->thread.hw_brk);\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n#endif\n\n\n\tnew_thread = &new->thread;\n\told_thread = &current->thread;\n\n#ifdef CONFIG_PPC64\n\t/*\n\t * Collect processor utilization data per process\n\t */\n\tif (firmware_has_feature(FW_FEATURE_SPLPAR)) {\n\t\tstruct cpu_usage *cu = &__get_cpu_var(cpu_usage_array);\n\t\tlong unsigned start_tb, current_tb;\n\t\tstart_tb = old_thread->start_tb;\n\t\tcu->current_tb = current_tb = mfspr(SPRN_PURR);\n\t\told_thread->accum_tb += (current_tb - start_tb);\n\t\tnew_thread->start_tb = current_tb;\n\t}\n#endif /* CONFIG_PPC64 */\n\n#ifdef CONFIG_PPC_BOOK3S_64\n\tbatch = &__get_cpu_var(ppc64_tlb_batch);\n\tif (batch->active) {\n\t\tcurrent_thread_info()->local_flags |= _TLF_LAZY_MMU;\n\t\tif (batch->index)\n\t\t\t__flush_tlb_pending(batch);\n\t\tbatch->active = 0;\n\t}\n#endif /* CONFIG_PPC_BOOK3S_64 */\n\n\t/*\n\t * We can't take a PMU exception inside _switch() since there is a\n\t * window where the kernel stack SLB and the kernel stack are out\n\t * of sync. Hard disable here.\n\t */\n\thard_irq_disable();\n\n\ttm_recheckpoint_new_task(new);\n\n\tlast = _switch(old_thread, new_thread);\n\n#ifdef CONFIG_PPC_BOOK3S_64\n\tif (current_thread_info()->local_flags & _TLF_LAZY_MMU) {\n\t\tcurrent_thread_info()->local_flags &= ~_TLF_LAZY_MMU;\n\t\tbatch = &__get_cpu_var(ppc64_tlb_batch);\n\t\tbatch->active = 1;\n\t}\n#endif /* CONFIG_PPC_BOOK3S_64 */\n\n\treturn last;\n}\n\nstatic int instructions_to_print = 16;\n\nstatic void show_instructions(struct pt_regs *regs)\n{\n\tint i;\n\tunsigned long pc = regs->nip - (instructions_to_print * 3 / 4 *\n\t\t\tsizeof(int));\n\n\tprintk(\"Instruction dump:\");\n\n\tfor (i = 0; i < instructions_to_print; i++) {\n\t\tint instr;\n\n\t\tif (!(i % 8))\n\t\t\tprintk(\"\\n\");\n\n#if !defined(CONFIG_BOOKE)\n\t\t/* If executing with the IMMU off, adjust pc rather\n\t\t * than print XXXXXXXX.\n\t\t */\n\t\tif (!(regs->msr & MSR_IR))\n\t\t\tpc = (unsigned long)phys_to_virt(pc);\n#endif\n\n\t\t/* We use __get_user here *only* to avoid an OOPS on a\n\t\t * bad address because the pc *should* only be a\n\t\t * kernel address.\n\t\t */\n\t\tif (!__kernel_text_address(pc) ||\n\t\t     __get_user(instr, (unsigned int __user *)pc)) {\n\t\t\tprintk(KERN_CONT \"XXXXXXXX \");\n\t\t} else {\n\t\t\tif (regs->nip == pc)\n\t\t\t\tprintk(KERN_CONT \"<%08x> \", instr);\n\t\t\telse\n\t\t\t\tprintk(KERN_CONT \"%08x \", instr);\n\t\t}\n\n\t\tpc += sizeof(int);\n\t}\n\n\tprintk(\"\\n\");\n}\n\nstatic struct regbit {\n\tunsigned long bit;\n\tconst char *name;\n} msr_bits[] = {\n#if defined(CONFIG_PPC64) && !defined(CONFIG_BOOKE)\n\t{MSR_SF,\t\"SF\"},\n\t{MSR_HV,\t\"HV\"},\n#endif\n\t{MSR_VEC,\t\"VEC\"},\n\t{MSR_VSX,\t\"VSX\"},\n#ifdef CONFIG_BOOKE\n\t{MSR_CE,\t\"CE\"},\n#endif\n\t{MSR_EE,\t\"EE\"},\n\t{MSR_PR,\t\"PR\"},\n\t{MSR_FP,\t\"FP\"},\n\t{MSR_ME,\t\"ME\"},\n#ifdef CONFIG_BOOKE\n\t{MSR_DE,\t\"DE\"},\n#else\n\t{MSR_SE,\t\"SE\"},\n\t{MSR_BE,\t\"BE\"},\n#endif\n\t{MSR_IR,\t\"IR\"},\n\t{MSR_DR,\t\"DR\"},\n\t{MSR_PMM,\t\"PMM\"},\n#ifndef CONFIG_BOOKE\n\t{MSR_RI,\t\"RI\"},\n\t{MSR_LE,\t\"LE\"},\n#endif\n\t{0,\t\tNULL}\n};\n\nstatic void printbits(unsigned long val, struct regbit *bits)\n{\n\tconst char *sep = \"\";\n\n\tprintk(\"<\");\n\tfor (; bits->bit; ++bits)\n\t\tif (val & bits->bit) {\n\t\t\tprintk(\"%s%s\", sep, bits->name);\n\t\t\tsep = \",\";\n\t\t}\n\tprintk(\">\");\n}\n\n#ifdef CONFIG_PPC64\n#define REG\t\t\"%016lx\"\n#define REGS_PER_LINE\t4\n#define LAST_VOLATILE\t13\n#else\n#define REG\t\t\"%08lx\"\n#define REGS_PER_LINE\t8\n#define LAST_VOLATILE\t12\n#endif\n\nvoid show_regs(struct pt_regs * regs)\n{\n\tint i, trap;\n\n\tshow_regs_print_info(KERN_DEFAULT);\n\n\tprintk(\"NIP: \"REG\" LR: \"REG\" CTR: \"REG\"\\n\",\n\t       regs->nip, regs->link, regs->ctr);\n\tprintk(\"REGS: %p TRAP: %04lx   %s  (%s)\\n\",\n\t       regs, regs->trap, print_tainted(), init_utsname()->release);\n\tprintk(\"MSR: \"REG\" \", regs->msr);\n\tprintbits(regs->msr, msr_bits);\n\tprintk(\"  CR: %08lx  XER: %08lx\\n\", regs->ccr, regs->xer);\n\ttrap = TRAP(regs);\n\tif ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))\n\t\tprintk(\"CFAR: \"REG\" \", regs->orig_gpr3);\n\tif (trap == 0x200 || trap == 0x300 || trap == 0x600)\n#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)\n\t\tprintk(\"DEAR: \"REG\" ESR: \"REG\" \", regs->dar, regs->dsisr);\n#else\n\t\tprintk(\"DAR: \"REG\" DSISR: %08lx \", regs->dar, regs->dsisr);\n#endif\n#ifdef CONFIG_PPC64\n\tprintk(\"SOFTE: %ld \", regs->softe);\n#endif\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n\tif (MSR_TM_ACTIVE(regs->msr))\n\t\tprintk(\"\\nPACATMSCRATCH: %016llx \", get_paca()->tm_scratch);\n#endif\n\n\tfor (i = 0;  i < 32;  i++) {\n\t\tif ((i % REGS_PER_LINE) == 0)\n\t\t\tprintk(\"\\nGPR%02d: \", i);\n\t\tprintk(REG \" \", regs->gpr[i]);\n\t\tif (i == LAST_VOLATILE && !FULL_REGS(regs))\n\t\t\tbreak;\n\t}\n\tprintk(\"\\n\");\n#ifdef CONFIG_KALLSYMS\n\t/*\n\t * Lookup NIP late so we have the best change of getting the\n\t * above info out without failing\n\t */\n\tprintk(\"NIP [\"REG\"] %pS\\n\", regs->nip, (void *)regs->nip);\n\tprintk(\"LR [\"REG\"] %pS\\n\", regs->link, (void *)regs->link);\n#endif\n\tshow_stack(current, (unsigned long *) regs->gpr[1]);\n\tif (!user_mode(regs))\n\t\tshow_instructions(regs);\n}\n\nvoid exit_thread(void)\n{\n\tdiscard_lazy_cpu_state();\n}\n\nvoid flush_thread(void)\n{\n\tdiscard_lazy_cpu_state();\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\tflush_ptrace_hw_breakpoint(current);\n#else /* CONFIG_HAVE_HW_BREAKPOINT */\n\tset_debug_reg_defaults(&current->thread);\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n}\n\nvoid\nrelease_thread(struct task_struct *t)\n{\n}\n\n/*\n * this gets called so that we can store coprocessor state into memory and\n * copy the current task into the new thread.\n */\nint arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)\n{\n\tflush_fp_to_thread(src);\n\tflush_altivec_to_thread(src);\n\tflush_vsx_to_thread(src);\n\tflush_spe_to_thread(src);\n\t/*\n\t * Flush TM state out so we can copy it.  __switch_to_tm() does this\n\t * flush but it removes the checkpointed state from the current CPU and\n\t * transitions the CPU out of TM mode.  Hence we need to call\n\t * tm_recheckpoint_new_task() (on the same task) to restore the\n\t * checkpointed state back and the TM mode.\n\t */\n\t__switch_to_tm(src);\n\ttm_recheckpoint_new_task(src);\n\n\t*dst = *src;\n\n\tclear_task_ebb(dst);\n\n\treturn 0;\n}\n\n/*\n * Copy a thread..\n */\nextern unsigned long dscr_default; /* defined in arch/powerpc/kernel/sysfs.c */\n\nint copy_thread(unsigned long clone_flags, unsigned long usp,\n\t\tunsigned long arg, struct task_struct *p)\n{\n\tstruct pt_regs *childregs, *kregs;\n\textern void ret_from_fork(void);\n\textern void ret_from_kernel_thread(void);\n\tvoid (*f)(void);\n\tunsigned long sp = (unsigned long)task_stack_page(p) + THREAD_SIZE;\n\n\t/* Copy registers */\n\tsp -= sizeof(struct pt_regs);\n\tchildregs = (struct pt_regs *) sp;\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tstruct thread_info *ti = (void *)task_stack_page(p);\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tchildregs->gpr[1] = sp + sizeof(struct pt_regs);\n\t\tchildregs->gpr[14] = usp;\t/* function */\n#ifdef CONFIG_PPC64\n\t\tclear_tsk_thread_flag(p, TIF_32BIT);\n\t\tchildregs->softe = 1;\n#endif\n\t\tchildregs->gpr[15] = arg;\n\t\tp->thread.regs = NULL;\t/* no user register state */\n\t\tti->flags |= _TIF_RESTOREALL;\n\t\tf = ret_from_kernel_thread;\n\t} else {\n\t\tstruct pt_regs *regs = current_pt_regs();\n\t\tCHECK_FULL_REGS(regs);\n\t\t*childregs = *regs;\n\t\tif (usp)\n\t\t\tchildregs->gpr[1] = usp;\n\t\tp->thread.regs = childregs;\n\t\tchildregs->gpr[3] = 0;  /* Result from fork() */\n\t\tif (clone_flags & CLONE_SETTLS) {\n#ifdef CONFIG_PPC64\n\t\t\tif (!is_32bit_task())\n\t\t\t\tchildregs->gpr[13] = childregs->gpr[6];\n\t\t\telse\n#endif\n\t\t\t\tchildregs->gpr[2] = childregs->gpr[6];\n\t\t}\n\n\t\tf = ret_from_fork;\n\t}\n\tsp -= STACK_FRAME_OVERHEAD;\n\n\t/*\n\t * The way this works is that at some point in the future\n\t * some task will call _switch to switch to the new task.\n\t * That will pop off the stack frame created below and start\n\t * the new task running at ret_from_fork.  The new task will\n\t * do some house keeping and then return from the fork or clone\n\t * system call, using the stack frame created above.\n\t */\n\t((unsigned long *)sp)[0] = 0;\n\tsp -= sizeof(struct pt_regs);\n\tkregs = (struct pt_regs *) sp;\n\tsp -= STACK_FRAME_OVERHEAD;\n\tp->thread.ksp = sp;\n#ifdef CONFIG_PPC32\n\tp->thread.ksp_limit = (unsigned long)task_stack_page(p) +\n\t\t\t\t_ALIGN_UP(sizeof(struct thread_info), 16);\n#endif\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\tp->thread.ptrace_bps[0] = NULL;\n#endif\n\n\tp->thread.fp_save_area = NULL;\n#ifdef CONFIG_ALTIVEC\n\tp->thread.vr_save_area = NULL;\n#endif\n\n#ifdef CONFIG_PPC_STD_MMU_64\n\tif (mmu_has_feature(MMU_FTR_SLB)) {\n\t\tunsigned long sp_vsid;\n\t\tunsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;\n\n\t\tif (mmu_has_feature(MMU_FTR_1T_SEGMENT))\n\t\t\tsp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_1T)\n\t\t\t\t<< SLB_VSID_SHIFT_1T;\n\t\telse\n\t\t\tsp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_256M)\n\t\t\t\t<< SLB_VSID_SHIFT;\n\t\tsp_vsid |= SLB_VSID_KERNEL | llp;\n\t\tp->thread.ksp_vsid = sp_vsid;\n\t}\n#endif /* CONFIG_PPC_STD_MMU_64 */\n#ifdef CONFIG_PPC64 \n\tif (cpu_has_feature(CPU_FTR_DSCR)) {\n\t\tp->thread.dscr_inherit = current->thread.dscr_inherit;\n\t\tp->thread.dscr = current->thread.dscr;\n\t}\n\tif (cpu_has_feature(CPU_FTR_HAS_PPR))\n\t\tp->thread.ppr = INIT_PPR;\n#endif\n\t/*\n\t * The PPC64 ABI makes use of a TOC to contain function \n\t * pointers.  The function (ret_from_except) is actually a pointer\n\t * to the TOC entry.  The first entry is a pointer to the actual\n\t * function.\n\t */\n#ifdef CONFIG_PPC64\n\tkregs->nip = *((unsigned long *)f);\n#else\n\tkregs->nip = (unsigned long)f;\n#endif\n\treturn 0;\n}\n\n/*\n * Set up a thread for executing a new program\n */\nvoid start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)\n{\n#ifdef CONFIG_PPC64\n\tunsigned long load_addr = regs->gpr[2];\t/* saved by ELF_PLAT_INIT */\n#endif\n\n\t/*\n\t * If we exec out of a kernel thread then thread.regs will not be\n\t * set.  Do it now.\n\t */\n\tif (!current->thread.regs) {\n\t\tstruct pt_regs *regs = task_stack_page(current) + THREAD_SIZE;\n\t\tcurrent->thread.regs = regs - 1;\n\t}\n\n\tmemset(regs->gpr, 0, sizeof(regs->gpr));\n\tregs->ctr = 0;\n\tregs->link = 0;\n\tregs->xer = 0;\n\tregs->ccr = 0;\n\tregs->gpr[1] = sp;\n\n\t/*\n\t * We have just cleared all the nonvolatile GPRs, so make\n\t * FULL_REGS(regs) return true.  This is necessary to allow\n\t * ptrace to examine the thread immediately after exec.\n\t */\n\tregs->trap &= ~1UL;\n\n#ifdef CONFIG_PPC32\n\tregs->mq = 0;\n\tregs->nip = start;\n\tregs->msr = MSR_USER;\n#else\n\tif (!is_32bit_task()) {\n\t\tunsigned long entry;\n\n\t\tif (is_elf2_task()) {\n\t\t\t/* Look ma, no function descriptors! */\n\t\t\tentry = start;\n\n\t\t\t/*\n\t\t\t * Ulrich says:\n\t\t\t *   The latest iteration of the ABI requires that when\n\t\t\t *   calling a function (at its global entry point),\n\t\t\t *   the caller must ensure r12 holds the entry point\n\t\t\t *   address (so that the function can quickly\n\t\t\t *   establish addressability).\n\t\t\t */\n\t\t\tregs->gpr[12] = start;\n\t\t\t/* Make sure that's restored on entry to userspace. */\n\t\t\tset_thread_flag(TIF_RESTOREALL);\n\t\t} else {\n\t\t\tunsigned long toc;\n\n\t\t\t/* start is a relocated pointer to the function\n\t\t\t * descriptor for the elf _start routine.  The first\n\t\t\t * entry in the function descriptor is the entry\n\t\t\t * address of _start and the second entry is the TOC\n\t\t\t * value we need to use.\n\t\t\t */\n\t\t\t__get_user(entry, (unsigned long __user *)start);\n\t\t\t__get_user(toc, (unsigned long __user *)start+1);\n\n\t\t\t/* Check whether the e_entry function descriptor entries\n\t\t\t * need to be relocated before we can use them.\n\t\t\t */\n\t\t\tif (load_addr != 0) {\n\t\t\t\tentry += load_addr;\n\t\t\t\ttoc   += load_addr;\n\t\t\t}\n\t\t\tregs->gpr[2] = toc;\n\t\t}\n\t\tregs->nip = entry;\n\t\tregs->msr = MSR_USER64;\n\t} else {\n\t\tregs->nip = start;\n\t\tregs->gpr[2] = 0;\n\t\tregs->msr = MSR_USER32;\n\t}\n#endif\n\tdiscard_lazy_cpu_state();\n#ifdef CONFIG_VSX\n\tcurrent->thread.used_vsr = 0;\n#endif\n\tmemset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));\n\tcurrent->thread.fp_save_area = NULL;\n#ifdef CONFIG_ALTIVEC\n\tmemset(&current->thread.vr_state, 0, sizeof(current->thread.vr_state));\n\tcurrent->thread.vr_state.vscr.u[3] = 0x00010000; /* Java mode disabled */\n\tcurrent->thread.vr_save_area = NULL;\n\tcurrent->thread.vrsave = 0;\n\tcurrent->thread.used_vr = 0;\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_SPE\n\tmemset(current->thread.evr, 0, sizeof(current->thread.evr));\n\tcurrent->thread.acc = 0;\n\tcurrent->thread.spefscr = 0;\n\tcurrent->thread.used_spe = 0;\n#endif /* CONFIG_SPE */\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n\tif (cpu_has_feature(CPU_FTR_TM))\n\t\tregs->msr |= MSR_TM;\n\tcurrent->thread.tm_tfhar = 0;\n\tcurrent->thread.tm_texasr = 0;\n\tcurrent->thread.tm_tfiar = 0;\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n}\n\n#define PR_FP_ALL_EXCEPT (PR_FP_EXC_DIV | PR_FP_EXC_OVF | PR_FP_EXC_UND \\\n\t\t| PR_FP_EXC_RES | PR_FP_EXC_INV)\n\nint set_fpexc_mode(struct task_struct *tsk, unsigned int val)\n{\n\tstruct pt_regs *regs = tsk->thread.regs;\n\n\t/* This is a bit hairy.  If we are an SPE enabled  processor\n\t * (have embedded fp) we store the IEEE exception enable flags in\n\t * fpexc_mode.  fpexc_mode is also used for setting FP exception\n\t * mode (asyn, precise, disabled) for 'Classic' FP. */\n\tif (val & PR_FP_EXC_SW_ENABLE) {\n#ifdef CONFIG_SPE\n\t\tif (cpu_has_feature(CPU_FTR_SPE)) {\n\t\t\t/*\n\t\t\t * When the sticky exception bits are set\n\t\t\t * directly by userspace, it must call prctl\n\t\t\t * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE\n\t\t\t * in the existing prctl settings) or\n\t\t\t * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in\n\t\t\t * the bits being set).  <fenv.h> functions\n\t\t\t * saving and restoring the whole\n\t\t\t * floating-point environment need to do so\n\t\t\t * anyway to restore the prctl settings from\n\t\t\t * the saved environment.\n\t\t\t */\n\t\t\ttsk->thread.spefscr_last = mfspr(SPRN_SPEFSCR);\n\t\t\ttsk->thread.fpexc_mode = val &\n\t\t\t\t(PR_FP_EXC_SW_ENABLE | PR_FP_ALL_EXCEPT);\n\t\t\treturn 0;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n#else\n\t\treturn -EINVAL;\n#endif\n\t}\n\n\t/* on a CONFIG_SPE this does not hurt us.  The bits that\n\t * __pack_fe01 use do not overlap with bits used for\n\t * PR_FP_EXC_SW_ENABLE.  Additionally, the MSR[FE0,FE1] bits\n\t * on CONFIG_SPE implementations are reserved so writing to\n\t * them does not change anything */\n\tif (val > PR_FP_EXC_PRECISE)\n\t\treturn -EINVAL;\n\ttsk->thread.fpexc_mode = __pack_fe01(val);\n\tif (regs != NULL && (regs->msr & MSR_FP) != 0)\n\t\tregs->msr = (regs->msr & ~(MSR_FE0|MSR_FE1))\n\t\t\t| tsk->thread.fpexc_mode;\n\treturn 0;\n}\n\nint get_fpexc_mode(struct task_struct *tsk, unsigned long adr)\n{\n\tunsigned int val;\n\n\tif (tsk->thread.fpexc_mode & PR_FP_EXC_SW_ENABLE)\n#ifdef CONFIG_SPE\n\t\tif (cpu_has_feature(CPU_FTR_SPE)) {\n\t\t\t/*\n\t\t\t * When the sticky exception bits are set\n\t\t\t * directly by userspace, it must call prctl\n\t\t\t * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE\n\t\t\t * in the existing prctl settings) or\n\t\t\t * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in\n\t\t\t * the bits being set).  <fenv.h> functions\n\t\t\t * saving and restoring the whole\n\t\t\t * floating-point environment need to do so\n\t\t\t * anyway to restore the prctl settings from\n\t\t\t * the saved environment.\n\t\t\t */\n\t\t\ttsk->thread.spefscr_last = mfspr(SPRN_SPEFSCR);\n\t\t\tval = tsk->thread.fpexc_mode;\n\t\t} else\n\t\t\treturn -EINVAL;\n#else\n\t\treturn -EINVAL;\n#endif\n\telse\n\t\tval = __unpack_fe01(tsk->thread.fpexc_mode);\n\treturn put_user(val, (unsigned int __user *) adr);\n}\n\nint set_endian(struct task_struct *tsk, unsigned int val)\n{\n\tstruct pt_regs *regs = tsk->thread.regs;\n\n\tif ((val == PR_ENDIAN_LITTLE && !cpu_has_feature(CPU_FTR_REAL_LE)) ||\n\t    (val == PR_ENDIAN_PPC_LITTLE && !cpu_has_feature(CPU_FTR_PPC_LE)))\n\t\treturn -EINVAL;\n\n\tif (regs == NULL)\n\t\treturn -EINVAL;\n\n\tif (val == PR_ENDIAN_BIG)\n\t\tregs->msr &= ~MSR_LE;\n\telse if (val == PR_ENDIAN_LITTLE || val == PR_ENDIAN_PPC_LITTLE)\n\t\tregs->msr |= MSR_LE;\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint get_endian(struct task_struct *tsk, unsigned long adr)\n{\n\tstruct pt_regs *regs = tsk->thread.regs;\n\tunsigned int val;\n\n\tif (!cpu_has_feature(CPU_FTR_PPC_LE) &&\n\t    !cpu_has_feature(CPU_FTR_REAL_LE))\n\t\treturn -EINVAL;\n\n\tif (regs == NULL)\n\t\treturn -EINVAL;\n\n\tif (regs->msr & MSR_LE) {\n\t\tif (cpu_has_feature(CPU_FTR_REAL_LE))\n\t\t\tval = PR_ENDIAN_LITTLE;\n\t\telse\n\t\t\tval = PR_ENDIAN_PPC_LITTLE;\n\t} else\n\t\tval = PR_ENDIAN_BIG;\n\n\treturn put_user(val, (unsigned int __user *)adr);\n}\n\nint set_unalign_ctl(struct task_struct *tsk, unsigned int val)\n{\n\ttsk->thread.align_ctl = val;\n\treturn 0;\n}\n\nint get_unalign_ctl(struct task_struct *tsk, unsigned long adr)\n{\n\treturn put_user(tsk->thread.align_ctl, (unsigned int __user *)adr);\n}\n\nstatic inline int valid_irq_stack(unsigned long sp, struct task_struct *p,\n\t\t\t\t  unsigned long nbytes)\n{\n\tunsigned long stack_page;\n\tunsigned long cpu = task_cpu(p);\n\n\t/*\n\t * Avoid crashing if the stack has overflowed and corrupted\n\t * task_cpu(p), which is in the thread_info struct.\n\t */\n\tif (cpu < NR_CPUS && cpu_possible(cpu)) {\n\t\tstack_page = (unsigned long) hardirq_ctx[cpu];\n\t\tif (sp >= stack_page + sizeof(struct thread_struct)\n\t\t    && sp <= stack_page + THREAD_SIZE - nbytes)\n\t\t\treturn 1;\n\n\t\tstack_page = (unsigned long) softirq_ctx[cpu];\n\t\tif (sp >= stack_page + sizeof(struct thread_struct)\n\t\t    && sp <= stack_page + THREAD_SIZE - nbytes)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nint validate_sp(unsigned long sp, struct task_struct *p,\n\t\t       unsigned long nbytes)\n{\n\tunsigned long stack_page = (unsigned long)task_stack_page(p);\n\n\tif (sp >= stack_page + sizeof(struct thread_struct)\n\t    && sp <= stack_page + THREAD_SIZE - nbytes)\n\t\treturn 1;\n\n\treturn valid_irq_stack(sp, p, nbytes);\n}\n\nEXPORT_SYMBOL(validate_sp);\n\nunsigned long get_wchan(struct task_struct *p)\n{\n\tunsigned long ip, sp;\n\tint count = 0;\n\n\tif (!p || p == current || p->state == TASK_RUNNING)\n\t\treturn 0;\n\n\tsp = p->thread.ksp;\n\tif (!validate_sp(sp, p, STACK_FRAME_OVERHEAD))\n\t\treturn 0;\n\n\tdo {\n\t\tsp = *(unsigned long *)sp;\n\t\tif (!validate_sp(sp, p, STACK_FRAME_OVERHEAD))\n\t\t\treturn 0;\n\t\tif (count > 0) {\n\t\t\tip = ((unsigned long *)sp)[STACK_FRAME_LR_SAVE];\n\t\t\tif (!in_sched_functions(ip))\n\t\t\t\treturn ip;\n\t\t}\n\t} while (count++ < 16);\n\treturn 0;\n}\n\nstatic int kstack_depth_to_print = CONFIG_PRINT_STACK_DEPTH;\n\nvoid show_stack(struct task_struct *tsk, unsigned long *stack)\n{\n\tunsigned long sp, ip, lr, newsp;\n\tint count = 0;\n\tint firstframe = 1;\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tint curr_frame = current->curr_ret_stack;\n\textern void return_to_handler(void);\n\tunsigned long rth = (unsigned long)return_to_handler;\n\tunsigned long mrth = -1;\n#ifdef CONFIG_PPC64\n\textern void mod_return_to_handler(void);\n\trth = *(unsigned long *)rth;\n\tmrth = (unsigned long)mod_return_to_handler;\n\tmrth = *(unsigned long *)mrth;\n#endif\n#endif\n\n\tsp = (unsigned long) stack;\n\tif (tsk == NULL)\n\t\ttsk = current;\n\tif (sp == 0) {\n\t\tif (tsk == current)\n\t\t\tasm(\"mr %0,1\" : \"=r\" (sp));\n\t\telse\n\t\t\tsp = tsk->thread.ksp;\n\t}\n\n\tlr = 0;\n\tprintk(\"Call Trace:\\n\");\n\tdo {\n\t\tif (!validate_sp(sp, tsk, STACK_FRAME_OVERHEAD))\n\t\t\treturn;\n\n\t\tstack = (unsigned long *) sp;\n\t\tnewsp = stack[0];\n\t\tip = stack[STACK_FRAME_LR_SAVE];\n\t\tif (!firstframe || ip != lr) {\n\t\t\tprintk(\"[\"REG\"] [\"REG\"] %pS\", sp, ip, (void *)ip);\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\t\tif ((ip == rth || ip == mrth) && curr_frame >= 0) {\n\t\t\t\tprintk(\" (%pS)\",\n\t\t\t\t       (void *)current->ret_stack[curr_frame].ret);\n\t\t\t\tcurr_frame--;\n\t\t\t}\n#endif\n\t\t\tif (firstframe)\n\t\t\t\tprintk(\" (unreliable)\");\n\t\t\tprintk(\"\\n\");\n\t\t}\n\t\tfirstframe = 0;\n\n\t\t/*\n\t\t * See if this is an exception frame.\n\t\t * We look for the \"regshere\" marker in the current frame.\n\t\t */\n\t\tif (validate_sp(sp, tsk, STACK_INT_FRAME_SIZE)\n\t\t    && stack[STACK_FRAME_MARKER] == STACK_FRAME_REGS_MARKER) {\n\t\t\tstruct pt_regs *regs = (struct pt_regs *)\n\t\t\t\t(sp + STACK_FRAME_OVERHEAD);\n\t\t\tlr = regs->link;\n\t\t\tprintk(\"--- Exception: %lx at %pS\\n    LR = %pS\\n\",\n\t\t\t       regs->trap, (void *)regs->nip, (void *)lr);\n\t\t\tfirstframe = 1;\n\t\t}\n\n\t\tsp = newsp;\n\t} while (count++ < kstack_depth_to_print);\n}\n\n#ifdef CONFIG_PPC64\n/* Called with hard IRQs off */\nvoid notrace __ppc64_runlatch_on(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\tunsigned long ctrl;\n\n\tctrl = mfspr(SPRN_CTRLF);\n\tctrl |= CTRL_RUNLATCH;\n\tmtspr(SPRN_CTRLT, ctrl);\n\n\tti->local_flags |= _TLF_RUNLATCH;\n}\n\n/* Called with hard IRQs off */\nvoid notrace __ppc64_runlatch_off(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\tunsigned long ctrl;\n\n\tti->local_flags &= ~_TLF_RUNLATCH;\n\n\tctrl = mfspr(SPRN_CTRLF);\n\tctrl &= ~CTRL_RUNLATCH;\n\tmtspr(SPRN_CTRLT, ctrl);\n}\n#endif /* CONFIG_PPC64 */\n\nunsigned long arch_align_stack(unsigned long sp)\n{\n\tif (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)\n\t\tsp -= get_random_int() & ~PAGE_MASK;\n\treturn sp & ~0xf;\n}\n\nstatic inline unsigned long brk_rnd(void)\n{\n        unsigned long rnd = 0;\n\n\t/* 8MB for 32bit, 1GB for 64bit */\n\tif (is_32bit_task())\n\t\trnd = (long)(get_random_int() % (1<<(23-PAGE_SHIFT)));\n\telse\n\t\trnd = (long)(get_random_int() % (1<<(30-PAGE_SHIFT)));\n\n\treturn rnd << PAGE_SHIFT;\n}\n\nunsigned long arch_randomize_brk(struct mm_struct *mm)\n{\n\tunsigned long base = mm->brk;\n\tunsigned long ret;\n\n#ifdef CONFIG_PPC_STD_MMU_64\n\t/*\n\t * If we are using 1TB segments and we are allowed to randomise\n\t * the heap, we can put it above 1TB so it is backed by a 1TB\n\t * segment. Otherwise the heap will be in the bottom 1TB\n\t * which always uses 256MB segments and this may result in a\n\t * performance penalty.\n\t */\n\tif (!is_32bit_task() && (mmu_highuser_ssize == MMU_SEGSIZE_1T))\n\t\tbase = max_t(unsigned long, mm->brk, 1UL << SID_SHIFT_1T);\n#endif\n\n\tret = PAGE_ALIGN(base + brk_rnd());\n\n\tif (ret < mm->brk)\n\t\treturn mm->brk;\n\n\treturn ret;\n}\n\nunsigned long randomize_et_dyn(unsigned long base)\n{\n\tunsigned long ret = PAGE_ALIGN(base + brk_rnd());\n\n\tif (ret < base)\n\t\treturn base;\n\n\treturn ret;\n}\n"], "filenames": ["arch/powerpc/kernel/process.c"], "buggy_code_start_loc": [1050], "buggy_code_end_loc": [1050], "fixing_code_start_loc": [1051], "fixing_code_end_loc": [1060], "type": "CWE-20", "message": "The arch_dup_task_struct function in the Transactional Memory (TM) implementation in arch/powerpc/kernel/process.c in the Linux kernel before 3.13.7 on the powerpc platform does not properly interact with the clone and fork system calls, which allows local users to cause a denial of service (Program Check and system crash) via certain instructions that are executed with the processor in the Transactional state.", "other": {"cve": {"id": "CVE-2014-2673", "sourceIdentifier": "cve@mitre.org", "published": "2014-04-01T06:35:53.780", "lastModified": "2020-08-27T17:08:07.360", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The arch_dup_task_struct function in the Transactional Memory (TM) implementation in arch/powerpc/kernel/process.c in the Linux kernel before 3.13.7 on the powerpc platform does not properly interact with the clone and fork system calls, which allows local users to cause a denial of service (Program Check and system crash) via certain instructions that are executed with the processor in the Transactional state."}, {"lang": "es", "value": "La funci\u00f3n arch_dup_task_struct en la implementaci\u00f3n Transactional Memory (TM) en arch/powerpc/kernel/process.c en el kernel de Linux anterior a 3.13.7 en la plataforma powerpc no interact\u00faa debidamente con las llamadas de sistema clon y fork, lo que permite a usuarios locales causar una denegaci\u00f3n de servicio (comprobaci\u00f3n de programa y ca\u00edda de sistema) a trav\u00e9s de ciertas instrucciones que son ejecutadas con el procesador en el estado transaccional."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.7}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.13.7", "matchCriteriaId": "D5D979C5-6EB3-436E-9207-26EC4335978D"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=621b5060e823301d0cba4cb52a7ee3491922d291", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.13.7", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/03/30/5", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/66477", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://exchange.xforce.ibmcloud.com/vulnerabilities/92113", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/621b5060e823301d0cba4cb52a7ee3491922d291", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.12.15", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/621b5060e823301d0cba4cb52a7ee3491922d291"}}