{"buggy_code": ["/*\n * Copyright (c) 2006 Oracle.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/in.h>\n#include <linux/device.h>\n#include <linux/dmapool.h>\n\n#include \"rds.h\"\n#include \"ib.h\"\n\nstatic char *rds_ib_wc_status_strings[] = {\n#define RDS_IB_WC_STATUS_STR(foo) \\\n\t\t[IB_WC_##foo] = __stringify(IB_WC_##foo)\n\tRDS_IB_WC_STATUS_STR(SUCCESS),\n\tRDS_IB_WC_STATUS_STR(LOC_LEN_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_QP_OP_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_EEC_OP_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_PROT_ERR),\n\tRDS_IB_WC_STATUS_STR(WR_FLUSH_ERR),\n\tRDS_IB_WC_STATUS_STR(MW_BIND_ERR),\n\tRDS_IB_WC_STATUS_STR(BAD_RESP_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_ACCESS_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_INV_REQ_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_ACCESS_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_OP_ERR),\n\tRDS_IB_WC_STATUS_STR(RETRY_EXC_ERR),\n\tRDS_IB_WC_STATUS_STR(RNR_RETRY_EXC_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_RDD_VIOL_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_INV_RD_REQ_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_ABORT_ERR),\n\tRDS_IB_WC_STATUS_STR(INV_EECN_ERR),\n\tRDS_IB_WC_STATUS_STR(INV_EEC_STATE_ERR),\n\tRDS_IB_WC_STATUS_STR(FATAL_ERR),\n\tRDS_IB_WC_STATUS_STR(RESP_TIMEOUT_ERR),\n\tRDS_IB_WC_STATUS_STR(GENERAL_ERR),\n#undef RDS_IB_WC_STATUS_STR\n};\n\nchar *rds_ib_wc_status_str(enum ib_wc_status status)\n{\n\treturn rds_str_array(rds_ib_wc_status_strings,\n\t\t\t     ARRAY_SIZE(rds_ib_wc_status_strings), status);\n}\n\n/*\n * Convert IB-specific error message to RDS error message and call core\n * completion handler.\n */\nstatic void rds_ib_send_complete(struct rds_message *rm,\n\t\t\t\t int wc_status,\n\t\t\t\t void (*complete)(struct rds_message *rm, int status))\n{\n\tint notify_status;\n\n\tswitch (wc_status) {\n\tcase IB_WC_WR_FLUSH_ERR:\n\t\treturn;\n\n\tcase IB_WC_SUCCESS:\n\t\tnotify_status = RDS_RDMA_SUCCESS;\n\t\tbreak;\n\n\tcase IB_WC_REM_ACCESS_ERR:\n\t\tnotify_status = RDS_RDMA_REMOTE_ERROR;\n\t\tbreak;\n\n\tdefault:\n\t\tnotify_status = RDS_RDMA_OTHER_ERROR;\n\t\tbreak;\n\t}\n\tcomplete(rm, notify_status);\n}\n\nstatic void rds_ib_send_unmap_data(struct rds_ib_connection *ic,\n\t\t\t\t   struct rm_data_op *op,\n\t\t\t\t   int wc_status)\n{\n\tif (op->op_nents)\n\t\tib_dma_unmap_sg(ic->i_cm_id->device,\n\t\t\t\top->op_sg, op->op_nents,\n\t\t\t\tDMA_TO_DEVICE);\n}\n\nstatic void rds_ib_send_unmap_rdma(struct rds_ib_connection *ic,\n\t\t\t\t   struct rm_rdma_op *op,\n\t\t\t\t   int wc_status)\n{\n\tif (op->op_mapped) {\n\t\tib_dma_unmap_sg(ic->i_cm_id->device,\n\t\t\t\top->op_sg, op->op_nents,\n\t\t\t\top->op_write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);\n\t\top->op_mapped = 0;\n\t}\n\n\t/* If the user asked for a completion notification on this\n\t * message, we can implement three different semantics:\n\t *  1.\tNotify when we received the ACK on the RDS message\n\t *\tthat was queued with the RDMA. This provides reliable\n\t *\tnotification of RDMA status at the expense of a one-way\n\t *\tpacket delay.\n\t *  2.\tNotify when the IB stack gives us the completion event for\n\t *\tthe RDMA operation.\n\t *  3.\tNotify when the IB stack gives us the completion event for\n\t *\tthe accompanying RDS messages.\n\t * Here, we implement approach #3. To implement approach #2,\n\t * we would need to take an event for the rdma WR. To implement #1,\n\t * don't call rds_rdma_send_complete at all, and fall back to the notify\n\t * handling in the ACK processing code.\n\t *\n\t * Note: There's no need to explicitly sync any RDMA buffers using\n\t * ib_dma_sync_sg_for_cpu - the completion for the RDMA\n\t * operation itself unmapped the RDMA buffers, which takes care\n\t * of synching.\n\t */\n\trds_ib_send_complete(container_of(op, struct rds_message, rdma),\n\t\t\t     wc_status, rds_rdma_send_complete);\n\n\tif (op->op_write)\n\t\trds_stats_add(s_send_rdma_bytes, op->op_bytes);\n\telse\n\t\trds_stats_add(s_recv_rdma_bytes, op->op_bytes);\n}\n\nstatic void rds_ib_send_unmap_atomic(struct rds_ib_connection *ic,\n\t\t\t\t     struct rm_atomic_op *op,\n\t\t\t\t     int wc_status)\n{\n\t/* unmap atomic recvbuf */\n\tif (op->op_mapped) {\n\t\tib_dma_unmap_sg(ic->i_cm_id->device, op->op_sg, 1,\n\t\t\t\tDMA_FROM_DEVICE);\n\t\top->op_mapped = 0;\n\t}\n\n\trds_ib_send_complete(container_of(op, struct rds_message, atomic),\n\t\t\t     wc_status, rds_atomic_send_complete);\n\n\tif (op->op_type == RDS_ATOMIC_TYPE_CSWP)\n\t\trds_ib_stats_inc(s_ib_atomic_cswp);\n\telse\n\t\trds_ib_stats_inc(s_ib_atomic_fadd);\n}\n\n/*\n * Unmap the resources associated with a struct send_work.\n *\n * Returns the rm for no good reason other than it is unobtainable\n * other than by switching on wr.opcode, currently, and the caller,\n * the event handler, needs it.\n */\nstatic struct rds_message *rds_ib_send_unmap_op(struct rds_ib_connection *ic,\n\t\t\t\t\t\tstruct rds_ib_send_work *send,\n\t\t\t\t\t\tint wc_status)\n{\n\tstruct rds_message *rm = NULL;\n\n\t/* In the error case, wc.opcode sometimes contains garbage */\n\tswitch (send->s_wr.opcode) {\n\tcase IB_WR_SEND:\n\t\tif (send->s_op) {\n\t\t\trm = container_of(send->s_op, struct rds_message, data);\n\t\t\trds_ib_send_unmap_data(ic, send->s_op, wc_status);\n\t\t}\n\t\tbreak;\n\tcase IB_WR_RDMA_WRITE:\n\tcase IB_WR_RDMA_READ:\n\t\tif (send->s_op) {\n\t\t\trm = container_of(send->s_op, struct rds_message, rdma);\n\t\t\trds_ib_send_unmap_rdma(ic, send->s_op, wc_status);\n\t\t}\n\t\tbreak;\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\tif (send->s_op) {\n\t\t\trm = container_of(send->s_op, struct rds_message, atomic);\n\t\t\trds_ib_send_unmap_atomic(ic, send->s_op, wc_status);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tif (printk_ratelimit())\n\t\t\tprintk(KERN_NOTICE\n\t\t\t       \"RDS/IB: %s: unexpected opcode 0x%x in WR!\\n\",\n\t\t\t       __func__, send->s_wr.opcode);\n\t\tbreak;\n\t}\n\n\tsend->s_wr.opcode = 0xdead;\n\n\treturn rm;\n}\n\nvoid rds_ib_send_init_ring(struct rds_ib_connection *ic)\n{\n\tstruct rds_ib_send_work *send;\n\tu32 i;\n\n\tfor (i = 0, send = ic->i_sends; i < ic->i_send_ring.w_nr; i++, send++) {\n\t\tstruct ib_sge *sge;\n\n\t\tsend->s_op = NULL;\n\n\t\tsend->s_wr.wr_id = i;\n\t\tsend->s_wr.sg_list = send->s_sge;\n\t\tsend->s_wr.ex.imm_data = 0;\n\n\t\tsge = &send->s_sge[0];\n\t\tsge->addr = ic->i_send_hdrs_dma + (i * sizeof(struct rds_header));\n\t\tsge->length = sizeof(struct rds_header);\n\t\tsge->lkey = ic->i_mr->lkey;\n\n\t\tsend->s_sge[1].lkey = ic->i_mr->lkey;\n\t}\n}\n\nvoid rds_ib_send_clear_ring(struct rds_ib_connection *ic)\n{\n\tstruct rds_ib_send_work *send;\n\tu32 i;\n\n\tfor (i = 0, send = ic->i_sends; i < ic->i_send_ring.w_nr; i++, send++) {\n\t\tif (send->s_op && send->s_wr.opcode != 0xdead)\n\t\t\trds_ib_send_unmap_op(ic, send, IB_WC_WR_FLUSH_ERR);\n\t}\n}\n\n/*\n * The only fast path caller always has a non-zero nr, so we don't\n * bother testing nr before performing the atomic sub.\n */\nstatic void rds_ib_sub_signaled(struct rds_ib_connection *ic, int nr)\n{\n\tif ((atomic_sub_return(nr, &ic->i_signaled_sends) == 0) &&\n\t    waitqueue_active(&rds_ib_ring_empty_wait))\n\t\twake_up(&rds_ib_ring_empty_wait);\n\tBUG_ON(atomic_read(&ic->i_signaled_sends) < 0);\n}\n\n/*\n * The _oldest/_free ring operations here race cleanly with the alloc/unalloc\n * operations performed in the send path.  As the sender allocs and potentially\n * unallocs the next free entry in the ring it doesn't alter which is\n * the next to be freed, which is what this is concerned with.\n */\nvoid rds_ib_send_cq_comp_handler(struct ib_cq *cq, void *context)\n{\n\tstruct rds_connection *conn = context;\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct rds_message *rm = NULL;\n\tstruct ib_wc wc;\n\tstruct rds_ib_send_work *send;\n\tu32 completed;\n\tu32 oldest;\n\tu32 i = 0;\n\tint ret;\n\tint nr_sig = 0;\n\n\trdsdebug(\"cq %p conn %p\\n\", cq, conn);\n\trds_ib_stats_inc(s_ib_tx_cq_call);\n\tret = ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);\n\tif (ret)\n\t\trdsdebug(\"ib_req_notify_cq send failed: %d\\n\", ret);\n\n\twhile (ib_poll_cq(cq, 1, &wc) > 0) {\n\t\trdsdebug(\"wc wr_id 0x%llx status %u (%s) byte_len %u imm_data %u\\n\",\n\t\t\t (unsigned long long)wc.wr_id, wc.status,\n\t\t\t rds_ib_wc_status_str(wc.status), wc.byte_len,\n\t\t\t be32_to_cpu(wc.ex.imm_data));\n\t\trds_ib_stats_inc(s_ib_tx_cq_event);\n\n\t\tif (wc.wr_id == RDS_IB_ACK_WR_ID) {\n\t\t\tif (ic->i_ack_queued + HZ/2 < jiffies)\n\t\t\t\trds_ib_stats_inc(s_ib_tx_stalled);\n\t\t\trds_ib_ack_send_complete(ic);\n\t\t\tcontinue;\n\t\t}\n\n\t\toldest = rds_ib_ring_oldest(&ic->i_send_ring);\n\n\t\tcompleted = rds_ib_ring_completed(&ic->i_send_ring, wc.wr_id, oldest);\n\n\t\tfor (i = 0; i < completed; i++) {\n\t\t\tsend = &ic->i_sends[oldest];\n\t\t\tif (send->s_wr.send_flags & IB_SEND_SIGNALED)\n\t\t\t\tnr_sig++;\n\n\t\t\trm = rds_ib_send_unmap_op(ic, send, wc.status);\n\n\t\t\tif (send->s_queued + HZ/2 < jiffies)\n\t\t\t\trds_ib_stats_inc(s_ib_tx_stalled);\n\n\t\t\tif (send->s_op) {\n\t\t\t\tif (send->s_op == rm->m_final_op) {\n\t\t\t\t\t/* If anyone waited for this message to get flushed out, wake\n\t\t\t\t\t * them up now */\n\t\t\t\t\trds_message_unmapped(rm);\n\t\t\t\t}\n\t\t\t\trds_message_put(rm);\n\t\t\t\tsend->s_op = NULL;\n\t\t\t}\n\n\t\t\toldest = (oldest + 1) % ic->i_send_ring.w_nr;\n\t\t}\n\n\t\trds_ib_ring_free(&ic->i_send_ring, completed);\n\t\trds_ib_sub_signaled(ic, nr_sig);\n\t\tnr_sig = 0;\n\n\t\tif (test_and_clear_bit(RDS_LL_SEND_FULL, &conn->c_flags) ||\n\t\t    test_bit(0, &conn->c_map_queued))\n\t\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 0);\n\n\t\t/* We expect errors as the qp is drained during shutdown */\n\t\tif (wc.status != IB_WC_SUCCESS && rds_conn_up(conn)) {\n\t\t\trds_ib_conn_error(conn, \"send completion on %pI4 had status \"\n\t\t\t\t\t  \"%u (%s), disconnecting and reconnecting\\n\",\n\t\t\t\t\t  &conn->c_faddr, wc.status,\n\t\t\t\t\t  rds_ib_wc_status_str(wc.status));\n\t\t}\n\t}\n}\n\n/*\n * This is the main function for allocating credits when sending\n * messages.\n *\n * Conceptually, we have two counters:\n *  -\tsend credits: this tells us how many WRs we're allowed\n *\tto submit without overruning the reciever's queue. For\n *\teach SEND WR we post, we decrement this by one.\n *\n *  -\tposted credits: this tells us how many WRs we recently\n *\tposted to the receive queue. This value is transferred\n *\tto the peer as a \"credit update\" in a RDS header field.\n *\tEvery time we transmit credits to the peer, we subtract\n *\tthe amount of transferred credits from this counter.\n *\n * It is essential that we avoid situations where both sides have\n * exhausted their send credits, and are unable to send new credits\n * to the peer. We achieve this by requiring that we send at least\n * one credit update to the peer before exhausting our credits.\n * When new credits arrive, we subtract one credit that is withheld\n * until we've posted new buffers and are ready to transmit these\n * credits (see rds_ib_send_add_credits below).\n *\n * The RDS send code is essentially single-threaded; rds_send_xmit\n * sets RDS_IN_XMIT to ensure exclusive access to the send ring.\n * However, the ACK sending code is independent and can race with\n * message SENDs.\n *\n * In the send path, we need to update the counters for send credits\n * and the counter of posted buffers atomically - when we use the\n * last available credit, we cannot allow another thread to race us\n * and grab the posted credits counter.  Hence, we have to use a\n * spinlock to protect the credit counter, or use atomics.\n *\n * Spinlocks shared between the send and the receive path are bad,\n * because they create unnecessary delays. An early implementation\n * using a spinlock showed a 5% degradation in throughput at some\n * loads.\n *\n * This implementation avoids spinlocks completely, putting both\n * counters into a single atomic, and updating that atomic using\n * atomic_add (in the receive path, when receiving fresh credits),\n * and using atomic_cmpxchg when updating the two counters.\n */\nint rds_ib_send_grab_credits(struct rds_ib_connection *ic,\n\t\t\t     u32 wanted, u32 *adv_credits, int need_posted, int max_posted)\n{\n\tunsigned int avail, posted, got = 0, advertise;\n\tlong oldval, newval;\n\n\t*adv_credits = 0;\n\tif (!ic->i_flowctl)\n\t\treturn wanted;\n\ntry_again:\n\tadvertise = 0;\n\toldval = newval = atomic_read(&ic->i_credits);\n\tposted = IB_GET_POST_CREDITS(oldval);\n\tavail = IB_GET_SEND_CREDITS(oldval);\n\n\trdsdebug(\"rds_ib_send_grab_credits(%u): credits=%u posted=%u\\n\",\n\t\t\twanted, avail, posted);\n\n\t/* The last credit must be used to send a credit update. */\n\tif (avail && !posted)\n\t\tavail--;\n\n\tif (avail < wanted) {\n\t\tstruct rds_connection *conn = ic->i_cm_id->context;\n\n\t\t/* Oops, there aren't that many credits left! */\n\t\tset_bit(RDS_LL_SEND_FULL, &conn->c_flags);\n\t\tgot = avail;\n\t} else {\n\t\t/* Sometimes you get what you want, lalala. */\n\t\tgot = wanted;\n\t}\n\tnewval -= IB_SET_SEND_CREDITS(got);\n\n\t/*\n\t * If need_posted is non-zero, then the caller wants\n\t * the posted regardless of whether any send credits are\n\t * available.\n\t */\n\tif (posted && (got || need_posted)) {\n\t\tadvertise = min_t(unsigned int, posted, max_posted);\n\t\tnewval -= IB_SET_POST_CREDITS(advertise);\n\t}\n\n\t/* Finally bill everything */\n\tif (atomic_cmpxchg(&ic->i_credits, oldval, newval) != oldval)\n\t\tgoto try_again;\n\n\t*adv_credits = advertise;\n\treturn got;\n}\n\nvoid rds_ib_send_add_credits(struct rds_connection *conn, unsigned int credits)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\n\tif (credits == 0)\n\t\treturn;\n\n\trdsdebug(\"rds_ib_send_add_credits(%u): current=%u%s\\n\",\n\t\t\tcredits,\n\t\t\tIB_GET_SEND_CREDITS(atomic_read(&ic->i_credits)),\n\t\t\ttest_bit(RDS_LL_SEND_FULL, &conn->c_flags) ? \", ll_send_full\" : \"\");\n\n\tatomic_add(IB_SET_SEND_CREDITS(credits), &ic->i_credits);\n\tif (test_and_clear_bit(RDS_LL_SEND_FULL, &conn->c_flags))\n\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 0);\n\n\tWARN_ON(IB_GET_SEND_CREDITS(credits) >= 16384);\n\n\trds_ib_stats_inc(s_ib_rx_credit_updates);\n}\n\nvoid rds_ib_advertise_credits(struct rds_connection *conn, unsigned int posted)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\n\tif (posted == 0)\n\t\treturn;\n\n\tatomic_add(IB_SET_POST_CREDITS(posted), &ic->i_credits);\n\n\t/* Decide whether to send an update to the peer now.\n\t * If we would send a credit update for every single buffer we\n\t * post, we would end up with an ACK storm (ACK arrives,\n\t * consumes buffer, we refill the ring, send ACK to remote\n\t * advertising the newly posted buffer... ad inf)\n\t *\n\t * Performance pretty much depends on how often we send\n\t * credit updates - too frequent updates mean lots of ACKs.\n\t * Too infrequent updates, and the peer will run out of\n\t * credits and has to throttle.\n\t * For the time being, 16 seems to be a good compromise.\n\t */\n\tif (IB_GET_POST_CREDITS(atomic_read(&ic->i_credits)) >= 16)\n\t\tset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\n}\n\nstatic inline int rds_ib_set_wr_signal_state(struct rds_ib_connection *ic,\n\t\t\t\t\t     struct rds_ib_send_work *send,\n\t\t\t\t\t     bool notify)\n{\n\t/*\n\t * We want to delay signaling completions just enough to get\n\t * the batching benefits but not so much that we create dead time\n\t * on the wire.\n\t */\n\tif (ic->i_unsignaled_wrs-- == 0 || notify) {\n\t\tic->i_unsignaled_wrs = rds_ib_sysctl_max_unsig_wrs;\n\t\tsend->s_wr.send_flags |= IB_SEND_SIGNALED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * This can be called multiple times for a given message.  The first time\n * we see a message we map its scatterlist into the IB device so that\n * we can provide that mapped address to the IB scatter gather entries\n * in the IB work requests.  We translate the scatterlist into a series\n * of work requests that fragment the message.  These work requests complete\n * in order so we pass ownership of the message to the completion handler\n * once we send the final fragment.\n *\n * The RDS core uses the c_send_lock to only enter this function once\n * per connection.  This makes sure that the tx ring alloc/unalloc pairs\n * don't get out of sync and confuse the ring.\n */\nint rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,\n\t\tunsigned int hdr_off, unsigned int sg, unsigned int off)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct ib_device *dev = ic->i_cm_id->device;\n\tstruct rds_ib_send_work *send = NULL;\n\tstruct rds_ib_send_work *first;\n\tstruct rds_ib_send_work *prev;\n\tstruct ib_send_wr *failed_wr;\n\tstruct scatterlist *scat;\n\tu32 pos;\n\tu32 i;\n\tu32 work_alloc;\n\tu32 credit_alloc = 0;\n\tu32 posted;\n\tu32 adv_credits = 0;\n\tint send_flags = 0;\n\tint bytes_sent = 0;\n\tint ret;\n\tint flow_controlled = 0;\n\tint nr_sig = 0;\n\n\tBUG_ON(off % RDS_FRAG_SIZE);\n\tBUG_ON(hdr_off != 0 && hdr_off != sizeof(struct rds_header));\n\n\t/* Do not send cong updates to IB loopback */\n\tif (conn->c_loopback\n\t    && rm->m_inc.i_hdr.h_flags & RDS_FLAG_CONG_BITMAP) {\n\t\trds_cong_map_updated(conn->c_fcong, ~(u64) 0);\n\t\treturn sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;\n\t}\n\n\t/* FIXME we may overallocate here */\n\tif (be32_to_cpu(rm->m_inc.i_hdr.h_len) == 0)\n\t\ti = 1;\n\telse\n\t\ti = ceil(be32_to_cpu(rm->m_inc.i_hdr.h_len), RDS_FRAG_SIZE);\n\n\twork_alloc = rds_ib_ring_alloc(&ic->i_send_ring, i, &pos);\n\tif (work_alloc == 0) {\n\t\tset_bit(RDS_LL_SEND_FULL, &conn->c_flags);\n\t\trds_ib_stats_inc(s_ib_tx_ring_full);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (ic->i_flowctl) {\n\t\tcredit_alloc = rds_ib_send_grab_credits(ic, work_alloc, &posted, 0, RDS_MAX_ADV_CREDIT);\n\t\tadv_credits += posted;\n\t\tif (credit_alloc < work_alloc) {\n\t\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc - credit_alloc);\n\t\t\twork_alloc = credit_alloc;\n\t\t\tflow_controlled = 1;\n\t\t}\n\t\tif (work_alloc == 0) {\n\t\t\tset_bit(RDS_LL_SEND_FULL, &conn->c_flags);\n\t\t\trds_ib_stats_inc(s_ib_tx_throttle);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* map the message the first time we see it */\n\tif (!ic->i_data_op) {\n\t\tif (rm->data.op_nents) {\n\t\t\trm->data.op_count = ib_dma_map_sg(dev,\n\t\t\t\t\t\t\t  rm->data.op_sg,\n\t\t\t\t\t\t\t  rm->data.op_nents,\n\t\t\t\t\t\t\t  DMA_TO_DEVICE);\n\t\t\trdsdebug(\"ic %p mapping rm %p: %d\\n\", ic, rm, rm->data.op_count);\n\t\t\tif (rm->data.op_count == 0) {\n\t\t\t\trds_ib_stats_inc(s_ib_tx_sg_mapping_failure);\n\t\t\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\t\t\tret = -ENOMEM; /* XXX ? */\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\trm->data.op_count = 0;\n\t\t}\n\n\t\trds_message_addref(rm);\n\t\tic->i_data_op = &rm->data;\n\n\t\t/* Finalize the header */\n\t\tif (test_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags))\n\t\t\trm->m_inc.i_hdr.h_flags |= RDS_FLAG_ACK_REQUIRED;\n\t\tif (test_bit(RDS_MSG_RETRANSMITTED, &rm->m_flags))\n\t\t\trm->m_inc.i_hdr.h_flags |= RDS_FLAG_RETRANSMITTED;\n\n\t\t/* If it has a RDMA op, tell the peer we did it. This is\n\t\t * used by the peer to release use-once RDMA MRs. */\n\t\tif (rm->rdma.op_active) {\n\t\t\tstruct rds_ext_header_rdma ext_hdr;\n\n\t\t\text_hdr.h_rdma_rkey = cpu_to_be32(rm->rdma.op_rkey);\n\t\t\trds_message_add_extension(&rm->m_inc.i_hdr,\n\t\t\t\t\tRDS_EXTHDR_RDMA, &ext_hdr, sizeof(ext_hdr));\n\t\t}\n\t\tif (rm->m_rdma_cookie) {\n\t\t\trds_message_add_rdma_dest_extension(&rm->m_inc.i_hdr,\n\t\t\t\t\trds_rdma_cookie_key(rm->m_rdma_cookie),\n\t\t\t\t\trds_rdma_cookie_offset(rm->m_rdma_cookie));\n\t\t}\n\n\t\t/* Note - rds_ib_piggyb_ack clears the ACK_REQUIRED bit, so\n\t\t * we should not do this unless we have a chance of at least\n\t\t * sticking the header into the send ring. Which is why we\n\t\t * should call rds_ib_ring_alloc first. */\n\t\trm->m_inc.i_hdr.h_ack = cpu_to_be64(rds_ib_piggyb_ack(ic));\n\t\trds_message_make_checksum(&rm->m_inc.i_hdr);\n\n\t\t/*\n\t\t * Update adv_credits since we reset the ACK_REQUIRED bit.\n\t\t */\n\t\tif (ic->i_flowctl) {\n\t\t\trds_ib_send_grab_credits(ic, 0, &posted, 1, RDS_MAX_ADV_CREDIT - adv_credits);\n\t\t\tadv_credits += posted;\n\t\t\tBUG_ON(adv_credits > 255);\n\t\t}\n\t}\n\n\t/* Sometimes you want to put a fence between an RDMA\n\t * READ and the following SEND.\n\t * We could either do this all the time\n\t * or when requested by the user. Right now, we let\n\t * the application choose.\n\t */\n\tif (rm->rdma.op_active && rm->rdma.op_fence)\n\t\tsend_flags = IB_SEND_FENCE;\n\n\t/* Each frag gets a header. Msgs may be 0 bytes */\n\tsend = &ic->i_sends[pos];\n\tfirst = send;\n\tprev = NULL;\n\tscat = &ic->i_data_op->op_sg[sg];\n\ti = 0;\n\tdo {\n\t\tunsigned int len = 0;\n\n\t\t/* Set up the header */\n\t\tsend->s_wr.send_flags = send_flags;\n\t\tsend->s_wr.opcode = IB_WR_SEND;\n\t\tsend->s_wr.num_sge = 1;\n\t\tsend->s_wr.next = NULL;\n\t\tsend->s_queued = jiffies;\n\t\tsend->s_op = NULL;\n\n\t\tsend->s_sge[0].addr = ic->i_send_hdrs_dma\n\t\t\t+ (pos * sizeof(struct rds_header));\n\t\tsend->s_sge[0].length = sizeof(struct rds_header);\n\n\t\tmemcpy(&ic->i_send_hdrs[pos], &rm->m_inc.i_hdr, sizeof(struct rds_header));\n\n\t\t/* Set up the data, if present */\n\t\tif (i < work_alloc\n\t\t    && scat != &rm->data.op_sg[rm->data.op_count]) {\n\t\t\tlen = min(RDS_FRAG_SIZE, ib_sg_dma_len(dev, scat) - off);\n\t\t\tsend->s_wr.num_sge = 2;\n\n\t\t\tsend->s_sge[1].addr = ib_sg_dma_address(dev, scat) + off;\n\t\t\tsend->s_sge[1].length = len;\n\n\t\t\tbytes_sent += len;\n\t\t\toff += len;\n\t\t\tif (off == ib_sg_dma_len(dev, scat)) {\n\t\t\t\tscat++;\n\t\t\t\toff = 0;\n\t\t\t}\n\t\t}\n\n\t\trds_ib_set_wr_signal_state(ic, send, 0);\n\n\t\t/*\n\t\t * Always signal the last one if we're stopping due to flow control.\n\t\t */\n\t\tif (ic->i_flowctl && flow_controlled && i == (work_alloc-1))\n\t\t\tsend->s_wr.send_flags |= IB_SEND_SIGNALED | IB_SEND_SOLICITED;\n\n\t\tif (send->s_wr.send_flags & IB_SEND_SIGNALED)\n\t\t\tnr_sig++;\n\n\t\trdsdebug(\"send %p wr %p num_sge %u next %p\\n\", send,\n\t\t\t &send->s_wr, send->s_wr.num_sge, send->s_wr.next);\n\n\t\tif (ic->i_flowctl && adv_credits) {\n\t\t\tstruct rds_header *hdr = &ic->i_send_hdrs[pos];\n\n\t\t\t/* add credit and redo the header checksum */\n\t\t\thdr->h_credit = adv_credits;\n\t\t\trds_message_make_checksum(hdr);\n\t\t\tadv_credits = 0;\n\t\t\trds_ib_stats_inc(s_ib_tx_credit_updates);\n\t\t}\n\n\t\tif (prev)\n\t\t\tprev->s_wr.next = &send->s_wr;\n\t\tprev = send;\n\n\t\tpos = (pos + 1) % ic->i_send_ring.w_nr;\n\t\tsend = &ic->i_sends[pos];\n\t\ti++;\n\n\t} while (i < work_alloc\n\t\t && scat != &rm->data.op_sg[rm->data.op_count]);\n\n\t/* Account the RDS header in the number of bytes we sent, but just once.\n\t * The caller has no concept of fragmentation. */\n\tif (hdr_off == 0)\n\t\tbytes_sent += sizeof(struct rds_header);\n\n\t/* if we finished the message then send completion owns it */\n\tif (scat == &rm->data.op_sg[rm->data.op_count]) {\n\t\tprev->s_op = ic->i_data_op;\n\t\tprev->s_wr.send_flags |= IB_SEND_SOLICITED;\n\t\tic->i_data_op = NULL;\n\t}\n\n\t/* Put back wrs & credits we didn't use */\n\tif (i < work_alloc) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc - i);\n\t\twork_alloc = i;\n\t}\n\tif (ic->i_flowctl && i < credit_alloc)\n\t\trds_ib_send_add_credits(conn, credit_alloc - i);\n\n\tif (nr_sig)\n\t\tatomic_add(nr_sig, &ic->i_signaled_sends);\n\n\t/* XXX need to worry about failed_wr and partial sends. */\n\tfailed_wr = &first->s_wr;\n\tret = ib_post_send(ic->i_cm_id->qp, &first->s_wr, &failed_wr);\n\trdsdebug(\"ic %p first %p (wr %p) ret %d wr %p\\n\", ic,\n\t\t first, &first->s_wr, ret, failed_wr);\n\tBUG_ON(failed_wr != &first->s_wr);\n\tif (ret) {\n\t\tprintk(KERN_WARNING \"RDS/IB: ib_post_send to %pI4 \"\n\t\t       \"returned %d\\n\", &conn->c_faddr, ret);\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_sub_signaled(ic, nr_sig);\n\t\tif (prev->s_op) {\n\t\t\tic->i_data_op = prev->s_op;\n\t\t\tprev->s_op = NULL;\n\t\t}\n\n\t\trds_ib_conn_error(ic->conn, \"ib_post_send failed\\n\");\n\t\tgoto out;\n\t}\n\n\tret = bytes_sent;\nout:\n\tBUG_ON(adv_credits);\n\treturn ret;\n}\n\n/*\n * Issue atomic operation.\n * A simplified version of the rdma case, we always map 1 SG, and\n * only 8 bytes, for the return value from the atomic operation.\n */\nint rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct rds_ib_send_work *send = NULL;\n\tstruct ib_send_wr *failed_wr;\n\tstruct rds_ib_device *rds_ibdev;\n\tu32 pos;\n\tu32 work_alloc;\n\tint ret;\n\tint nr_sig = 0;\n\n\trds_ibdev = ib_get_client_data(ic->i_cm_id->device, &rds_ib_client);\n\n\twork_alloc = rds_ib_ring_alloc(&ic->i_send_ring, 1, &pos);\n\tif (work_alloc != 1) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_stats_inc(s_ib_tx_ring_full);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* address of send request in ring */\n\tsend = &ic->i_sends[pos];\n\tsend->s_queued = jiffies;\n\n\tif (op->op_type == RDS_ATOMIC_TYPE_CSWP) {\n\t\tsend->s_wr.opcode = IB_WR_MASKED_ATOMIC_CMP_AND_SWP;\n\t\tsend->s_wr.wr.atomic.compare_add = op->op_m_cswp.compare;\n\t\tsend->s_wr.wr.atomic.swap = op->op_m_cswp.swap;\n\t\tsend->s_wr.wr.atomic.compare_add_mask = op->op_m_cswp.compare_mask;\n\t\tsend->s_wr.wr.atomic.swap_mask = op->op_m_cswp.swap_mask;\n\t} else { /* FADD */\n\t\tsend->s_wr.opcode = IB_WR_MASKED_ATOMIC_FETCH_AND_ADD;\n\t\tsend->s_wr.wr.atomic.compare_add = op->op_m_fadd.add;\n\t\tsend->s_wr.wr.atomic.swap = 0;\n\t\tsend->s_wr.wr.atomic.compare_add_mask = op->op_m_fadd.nocarry_mask;\n\t\tsend->s_wr.wr.atomic.swap_mask = 0;\n\t}\n\tnr_sig = rds_ib_set_wr_signal_state(ic, send, op->op_notify);\n\tsend->s_wr.num_sge = 1;\n\tsend->s_wr.next = NULL;\n\tsend->s_wr.wr.atomic.remote_addr = op->op_remote_addr;\n\tsend->s_wr.wr.atomic.rkey = op->op_rkey;\n\tsend->s_op = op;\n\trds_message_addref(container_of(send->s_op, struct rds_message, atomic));\n\n\t/* map 8 byte retval buffer to the device */\n\tret = ib_dma_map_sg(ic->i_cm_id->device, op->op_sg, 1, DMA_FROM_DEVICE);\n\trdsdebug(\"ic %p mapping atomic op %p. mapped %d pg\\n\", ic, op, ret);\n\tif (ret != 1) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_stats_inc(s_ib_tx_sg_mapping_failure);\n\t\tret = -ENOMEM; /* XXX ? */\n\t\tgoto out;\n\t}\n\n\t/* Convert our struct scatterlist to struct ib_sge */\n\tsend->s_sge[0].addr = ib_sg_dma_address(ic->i_cm_id->device, op->op_sg);\n\tsend->s_sge[0].length = ib_sg_dma_len(ic->i_cm_id->device, op->op_sg);\n\tsend->s_sge[0].lkey = ic->i_mr->lkey;\n\n\trdsdebug(\"rva %Lx rpa %Lx len %u\\n\", op->op_remote_addr,\n\t\t send->s_sge[0].addr, send->s_sge[0].length);\n\n\tif (nr_sig)\n\t\tatomic_add(nr_sig, &ic->i_signaled_sends);\n\n\tfailed_wr = &send->s_wr;\n\tret = ib_post_send(ic->i_cm_id->qp, &send->s_wr, &failed_wr);\n\trdsdebug(\"ic %p send %p (wr %p) ret %d wr %p\\n\", ic,\n\t\t send, &send->s_wr, ret, failed_wr);\n\tBUG_ON(failed_wr != &send->s_wr);\n\tif (ret) {\n\t\tprintk(KERN_WARNING \"RDS/IB: atomic ib_post_send to %pI4 \"\n\t\t       \"returned %d\\n\", &conn->c_faddr, ret);\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_sub_signaled(ic, nr_sig);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(failed_wr != &send->s_wr)) {\n\t\tprintk(KERN_WARNING \"RDS/IB: atomic ib_post_send() rc=%d, but failed_wqe updated!\\n\", ret);\n\t\tBUG_ON(failed_wr != &send->s_wr);\n\t}\n\nout:\n\treturn ret;\n}\n\nint rds_ib_xmit_rdma(struct rds_connection *conn, struct rm_rdma_op *op)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct rds_ib_send_work *send = NULL;\n\tstruct rds_ib_send_work *first;\n\tstruct rds_ib_send_work *prev;\n\tstruct ib_send_wr *failed_wr;\n\tstruct scatterlist *scat;\n\tunsigned long len;\n\tu64 remote_addr = op->op_remote_addr;\n\tu32 max_sge = ic->rds_ibdev->max_sge;\n\tu32 pos;\n\tu32 work_alloc;\n\tu32 i;\n\tu32 j;\n\tint sent;\n\tint ret;\n\tint num_sge;\n\tint nr_sig = 0;\n\n\t/* map the op the first time we see it */\n\tif (!op->op_mapped) {\n\t\top->op_count = ib_dma_map_sg(ic->i_cm_id->device,\n\t\t\t\t\t     op->op_sg, op->op_nents, (op->op_write) ?\n\t\t\t\t\t     DMA_TO_DEVICE : DMA_FROM_DEVICE);\n\t\trdsdebug(\"ic %p mapping op %p: %d\\n\", ic, op, op->op_count);\n\t\tif (op->op_count == 0) {\n\t\t\trds_ib_stats_inc(s_ib_tx_sg_mapping_failure);\n\t\t\tret = -ENOMEM; /* XXX ? */\n\t\t\tgoto out;\n\t\t}\n\n\t\top->op_mapped = 1;\n\t}\n\n\t/*\n\t * Instead of knowing how to return a partial rdma read/write we insist that there\n\t * be enough work requests to send the entire message.\n\t */\n\ti = ceil(op->op_count, max_sge);\n\n\twork_alloc = rds_ib_ring_alloc(&ic->i_send_ring, i, &pos);\n\tif (work_alloc != i) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_stats_inc(s_ib_tx_ring_full);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tsend = &ic->i_sends[pos];\n\tfirst = send;\n\tprev = NULL;\n\tscat = &op->op_sg[0];\n\tsent = 0;\n\tnum_sge = op->op_count;\n\n\tfor (i = 0; i < work_alloc && scat != &op->op_sg[op->op_count]; i++) {\n\t\tsend->s_wr.send_flags = 0;\n\t\tsend->s_queued = jiffies;\n\t\tsend->s_op = NULL;\n\n\t\tnr_sig += rds_ib_set_wr_signal_state(ic, send, op->op_notify);\n\n\t\tsend->s_wr.opcode = op->op_write ? IB_WR_RDMA_WRITE : IB_WR_RDMA_READ;\n\t\tsend->s_wr.wr.rdma.remote_addr = remote_addr;\n\t\tsend->s_wr.wr.rdma.rkey = op->op_rkey;\n\n\t\tif (num_sge > max_sge) {\n\t\t\tsend->s_wr.num_sge = max_sge;\n\t\t\tnum_sge -= max_sge;\n\t\t} else {\n\t\t\tsend->s_wr.num_sge = num_sge;\n\t\t}\n\n\t\tsend->s_wr.next = NULL;\n\n\t\tif (prev)\n\t\t\tprev->s_wr.next = &send->s_wr;\n\n\t\tfor (j = 0; j < send->s_wr.num_sge && scat != &op->op_sg[op->op_count]; j++) {\n\t\t\tlen = ib_sg_dma_len(ic->i_cm_id->device, scat);\n\t\t\tsend->s_sge[j].addr =\n\t\t\t\t ib_sg_dma_address(ic->i_cm_id->device, scat);\n\t\t\tsend->s_sge[j].length = len;\n\t\t\tsend->s_sge[j].lkey = ic->i_mr->lkey;\n\n\t\t\tsent += len;\n\t\t\trdsdebug(\"ic %p sent %d remote_addr %llu\\n\", ic, sent, remote_addr);\n\n\t\t\tremote_addr += len;\n\t\t\tscat++;\n\t\t}\n\n\t\trdsdebug(\"send %p wr %p num_sge %u next %p\\n\", send,\n\t\t\t&send->s_wr, send->s_wr.num_sge, send->s_wr.next);\n\n\t\tprev = send;\n\t\tif (++send == &ic->i_sends[ic->i_send_ring.w_nr])\n\t\t\tsend = ic->i_sends;\n\t}\n\n\t/* give a reference to the last op */\n\tif (scat == &op->op_sg[op->op_count]) {\n\t\tprev->s_op = op;\n\t\trds_message_addref(container_of(op, struct rds_message, rdma));\n\t}\n\n\tif (i < work_alloc) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc - i);\n\t\twork_alloc = i;\n\t}\n\n\tif (nr_sig)\n\t\tatomic_add(nr_sig, &ic->i_signaled_sends);\n\n\tfailed_wr = &first->s_wr;\n\tret = ib_post_send(ic->i_cm_id->qp, &first->s_wr, &failed_wr);\n\trdsdebug(\"ic %p first %p (wr %p) ret %d wr %p\\n\", ic,\n\t\t first, &first->s_wr, ret, failed_wr);\n\tBUG_ON(failed_wr != &first->s_wr);\n\tif (ret) {\n\t\tprintk(KERN_WARNING \"RDS/IB: rdma ib_post_send to %pI4 \"\n\t\t       \"returned %d\\n\", &conn->c_faddr, ret);\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_sub_signaled(ic, nr_sig);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(failed_wr != &first->s_wr)) {\n\t\tprintk(KERN_WARNING \"RDS/IB: ib_post_send() rc=%d, but failed_wqe updated!\\n\", ret);\n\t\tBUG_ON(failed_wr != &first->s_wr);\n\t}\n\n\nout:\n\treturn ret;\n}\n\nvoid rds_ib_xmit_complete(struct rds_connection *conn)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\n\t/* We may have a pending ACK or window update we were unable\n\t * to send previously (due to flow control). Try again. */\n\trds_ib_attempt_ack(ic);\n}\n", "/*\n * Copyright (c) 2006 Oracle.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/in.h>\n\n#include \"rds.h\"\n#include \"loop.h\"\n\nstatic DEFINE_SPINLOCK(loop_conns_lock);\nstatic LIST_HEAD(loop_conns);\n\n/*\n * This 'loopback' transport is a special case for flows that originate\n * and terminate on the same machine.\n *\n * Connection build-up notices if the destination address is thought of\n * as a local address by a transport.  At that time it decides to use the\n * loopback transport instead of the bound transport of the sending socket.\n *\n * The loopback transport's sending path just hands the sent rds_message\n * straight to the receiving path via an embedded rds_incoming.\n */\n\n/*\n * Usually a message transits both the sender and receiver's conns as it\n * flows to the receiver.  In the loopback case, though, the receive path\n * is handed the sending conn so the sense of the addresses is reversed.\n */\nstatic int rds_loop_xmit(struct rds_connection *conn, struct rds_message *rm,\n\t\t\t unsigned int hdr_off, unsigned int sg,\n\t\t\t unsigned int off)\n{\n\t/* Do not send cong updates to loopback */\n\tif (rm->m_inc.i_hdr.h_flags & RDS_FLAG_CONG_BITMAP) {\n\t\trds_cong_map_updated(conn->c_fcong, ~(u64) 0);\n\t\treturn sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;\n\t}\n\n\tBUG_ON(hdr_off || sg || off);\n\n\trds_inc_init(&rm->m_inc, conn, conn->c_laddr);\n\t/* For the embedded inc. Matching put is in loop_inc_free() */\n\trds_message_addref(rm);\n\n\trds_recv_incoming(conn, conn->c_laddr, conn->c_faddr, &rm->m_inc,\n\t\t\t  GFP_KERNEL, KM_USER0);\n\n\trds_send_drop_acked(conn, be64_to_cpu(rm->m_inc.i_hdr.h_sequence),\n\t\t\t    NULL);\n\n\trds_inc_put(&rm->m_inc);\n\n\treturn sizeof(struct rds_header) + be32_to_cpu(rm->m_inc.i_hdr.h_len);\n}\n\n/*\n * See rds_loop_xmit(). Since our inc is embedded in the rm, we\n * make sure the rm lives at least until the inc is done.\n */\nstatic void rds_loop_inc_free(struct rds_incoming *inc)\n{\n        struct rds_message *rm = container_of(inc, struct rds_message, m_inc);\n        rds_message_put(rm);\n}\n\n/* we need to at least give the thread something to succeed */\nstatic int rds_loop_recv(struct rds_connection *conn)\n{\n\treturn 0;\n}\n\nstruct rds_loop_connection {\n\tstruct list_head loop_node;\n\tstruct rds_connection *conn;\n};\n\n/*\n * Even the loopback transport needs to keep track of its connections,\n * so it can call rds_conn_destroy() on them on exit. N.B. there are\n * 1+ loopback addresses (127.*.*.*) so it's not a bug to have\n * multiple loopback conns allocated, although rather useless.\n */\nstatic int rds_loop_conn_alloc(struct rds_connection *conn, gfp_t gfp)\n{\n\tstruct rds_loop_connection *lc;\n\tunsigned long flags;\n\n\tlc = kzalloc(sizeof(struct rds_loop_connection), GFP_KERNEL);\n\tif (!lc)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&lc->loop_node);\n\tlc->conn = conn;\n\tconn->c_transport_data = lc;\n\n\tspin_lock_irqsave(&loop_conns_lock, flags);\n\tlist_add_tail(&lc->loop_node, &loop_conns);\n\tspin_unlock_irqrestore(&loop_conns_lock, flags);\n\n\treturn 0;\n}\n\nstatic void rds_loop_conn_free(void *arg)\n{\n\tstruct rds_loop_connection *lc = arg;\n\tunsigned long flags;\n\n\trdsdebug(\"lc %p\\n\", lc);\n\tspin_lock_irqsave(&loop_conns_lock, flags);\n\tlist_del(&lc->loop_node);\n\tspin_unlock_irqrestore(&loop_conns_lock, flags);\n\tkfree(lc);\n}\n\nstatic int rds_loop_conn_connect(struct rds_connection *conn)\n{\n\trds_connect_complete(conn);\n\treturn 0;\n}\n\nstatic void rds_loop_conn_shutdown(struct rds_connection *conn)\n{\n}\n\nvoid rds_loop_exit(void)\n{\n\tstruct rds_loop_connection *lc, *_lc;\n\tLIST_HEAD(tmp_list);\n\n\t/* avoid calling conn_destroy with irqs off */\n\tspin_lock_irq(&loop_conns_lock);\n\tlist_splice(&loop_conns, &tmp_list);\n\tINIT_LIST_HEAD(&loop_conns);\n\tspin_unlock_irq(&loop_conns_lock);\n\n\tlist_for_each_entry_safe(lc, _lc, &tmp_list, loop_node) {\n\t\tWARN_ON(lc->conn->c_passive);\n\t\trds_conn_destroy(lc->conn);\n\t}\n}\n\n/*\n * This is missing .xmit_* because loop doesn't go through generic\n * rds_send_xmit() and doesn't call rds_recv_incoming().  .listen_stop and\n * .laddr_check are missing because transport.c doesn't iterate over\n * rds_loop_transport.\n */\nstruct rds_transport rds_loop_transport = {\n\t.xmit\t\t\t= rds_loop_xmit,\n\t.recv\t\t\t= rds_loop_recv,\n\t.conn_alloc\t\t= rds_loop_conn_alloc,\n\t.conn_free\t\t= rds_loop_conn_free,\n\t.conn_connect\t\t= rds_loop_conn_connect,\n\t.conn_shutdown\t\t= rds_loop_conn_shutdown,\n\t.inc_copy_to_user\t= rds_message_inc_copy_to_user,\n\t.inc_free\t\t= rds_loop_inc_free,\n\t.t_name\t\t\t= \"loopback\",\n};\n"], "fixing_code": ["/*\n * Copyright (c) 2006 Oracle.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/in.h>\n#include <linux/device.h>\n#include <linux/dmapool.h>\n\n#include \"rds.h\"\n#include \"ib.h\"\n\nstatic char *rds_ib_wc_status_strings[] = {\n#define RDS_IB_WC_STATUS_STR(foo) \\\n\t\t[IB_WC_##foo] = __stringify(IB_WC_##foo)\n\tRDS_IB_WC_STATUS_STR(SUCCESS),\n\tRDS_IB_WC_STATUS_STR(LOC_LEN_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_QP_OP_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_EEC_OP_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_PROT_ERR),\n\tRDS_IB_WC_STATUS_STR(WR_FLUSH_ERR),\n\tRDS_IB_WC_STATUS_STR(MW_BIND_ERR),\n\tRDS_IB_WC_STATUS_STR(BAD_RESP_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_ACCESS_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_INV_REQ_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_ACCESS_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_OP_ERR),\n\tRDS_IB_WC_STATUS_STR(RETRY_EXC_ERR),\n\tRDS_IB_WC_STATUS_STR(RNR_RETRY_EXC_ERR),\n\tRDS_IB_WC_STATUS_STR(LOC_RDD_VIOL_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_INV_RD_REQ_ERR),\n\tRDS_IB_WC_STATUS_STR(REM_ABORT_ERR),\n\tRDS_IB_WC_STATUS_STR(INV_EECN_ERR),\n\tRDS_IB_WC_STATUS_STR(INV_EEC_STATE_ERR),\n\tRDS_IB_WC_STATUS_STR(FATAL_ERR),\n\tRDS_IB_WC_STATUS_STR(RESP_TIMEOUT_ERR),\n\tRDS_IB_WC_STATUS_STR(GENERAL_ERR),\n#undef RDS_IB_WC_STATUS_STR\n};\n\nchar *rds_ib_wc_status_str(enum ib_wc_status status)\n{\n\treturn rds_str_array(rds_ib_wc_status_strings,\n\t\t\t     ARRAY_SIZE(rds_ib_wc_status_strings), status);\n}\n\n/*\n * Convert IB-specific error message to RDS error message and call core\n * completion handler.\n */\nstatic void rds_ib_send_complete(struct rds_message *rm,\n\t\t\t\t int wc_status,\n\t\t\t\t void (*complete)(struct rds_message *rm, int status))\n{\n\tint notify_status;\n\n\tswitch (wc_status) {\n\tcase IB_WC_WR_FLUSH_ERR:\n\t\treturn;\n\n\tcase IB_WC_SUCCESS:\n\t\tnotify_status = RDS_RDMA_SUCCESS;\n\t\tbreak;\n\n\tcase IB_WC_REM_ACCESS_ERR:\n\t\tnotify_status = RDS_RDMA_REMOTE_ERROR;\n\t\tbreak;\n\n\tdefault:\n\t\tnotify_status = RDS_RDMA_OTHER_ERROR;\n\t\tbreak;\n\t}\n\tcomplete(rm, notify_status);\n}\n\nstatic void rds_ib_send_unmap_data(struct rds_ib_connection *ic,\n\t\t\t\t   struct rm_data_op *op,\n\t\t\t\t   int wc_status)\n{\n\tif (op->op_nents)\n\t\tib_dma_unmap_sg(ic->i_cm_id->device,\n\t\t\t\top->op_sg, op->op_nents,\n\t\t\t\tDMA_TO_DEVICE);\n}\n\nstatic void rds_ib_send_unmap_rdma(struct rds_ib_connection *ic,\n\t\t\t\t   struct rm_rdma_op *op,\n\t\t\t\t   int wc_status)\n{\n\tif (op->op_mapped) {\n\t\tib_dma_unmap_sg(ic->i_cm_id->device,\n\t\t\t\top->op_sg, op->op_nents,\n\t\t\t\top->op_write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);\n\t\top->op_mapped = 0;\n\t}\n\n\t/* If the user asked for a completion notification on this\n\t * message, we can implement three different semantics:\n\t *  1.\tNotify when we received the ACK on the RDS message\n\t *\tthat was queued with the RDMA. This provides reliable\n\t *\tnotification of RDMA status at the expense of a one-way\n\t *\tpacket delay.\n\t *  2.\tNotify when the IB stack gives us the completion event for\n\t *\tthe RDMA operation.\n\t *  3.\tNotify when the IB stack gives us the completion event for\n\t *\tthe accompanying RDS messages.\n\t * Here, we implement approach #3. To implement approach #2,\n\t * we would need to take an event for the rdma WR. To implement #1,\n\t * don't call rds_rdma_send_complete at all, and fall back to the notify\n\t * handling in the ACK processing code.\n\t *\n\t * Note: There's no need to explicitly sync any RDMA buffers using\n\t * ib_dma_sync_sg_for_cpu - the completion for the RDMA\n\t * operation itself unmapped the RDMA buffers, which takes care\n\t * of synching.\n\t */\n\trds_ib_send_complete(container_of(op, struct rds_message, rdma),\n\t\t\t     wc_status, rds_rdma_send_complete);\n\n\tif (op->op_write)\n\t\trds_stats_add(s_send_rdma_bytes, op->op_bytes);\n\telse\n\t\trds_stats_add(s_recv_rdma_bytes, op->op_bytes);\n}\n\nstatic void rds_ib_send_unmap_atomic(struct rds_ib_connection *ic,\n\t\t\t\t     struct rm_atomic_op *op,\n\t\t\t\t     int wc_status)\n{\n\t/* unmap atomic recvbuf */\n\tif (op->op_mapped) {\n\t\tib_dma_unmap_sg(ic->i_cm_id->device, op->op_sg, 1,\n\t\t\t\tDMA_FROM_DEVICE);\n\t\top->op_mapped = 0;\n\t}\n\n\trds_ib_send_complete(container_of(op, struct rds_message, atomic),\n\t\t\t     wc_status, rds_atomic_send_complete);\n\n\tif (op->op_type == RDS_ATOMIC_TYPE_CSWP)\n\t\trds_ib_stats_inc(s_ib_atomic_cswp);\n\telse\n\t\trds_ib_stats_inc(s_ib_atomic_fadd);\n}\n\n/*\n * Unmap the resources associated with a struct send_work.\n *\n * Returns the rm for no good reason other than it is unobtainable\n * other than by switching on wr.opcode, currently, and the caller,\n * the event handler, needs it.\n */\nstatic struct rds_message *rds_ib_send_unmap_op(struct rds_ib_connection *ic,\n\t\t\t\t\t\tstruct rds_ib_send_work *send,\n\t\t\t\t\t\tint wc_status)\n{\n\tstruct rds_message *rm = NULL;\n\n\t/* In the error case, wc.opcode sometimes contains garbage */\n\tswitch (send->s_wr.opcode) {\n\tcase IB_WR_SEND:\n\t\tif (send->s_op) {\n\t\t\trm = container_of(send->s_op, struct rds_message, data);\n\t\t\trds_ib_send_unmap_data(ic, send->s_op, wc_status);\n\t\t}\n\t\tbreak;\n\tcase IB_WR_RDMA_WRITE:\n\tcase IB_WR_RDMA_READ:\n\t\tif (send->s_op) {\n\t\t\trm = container_of(send->s_op, struct rds_message, rdma);\n\t\t\trds_ib_send_unmap_rdma(ic, send->s_op, wc_status);\n\t\t}\n\t\tbreak;\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\tif (send->s_op) {\n\t\t\trm = container_of(send->s_op, struct rds_message, atomic);\n\t\t\trds_ib_send_unmap_atomic(ic, send->s_op, wc_status);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tif (printk_ratelimit())\n\t\t\tprintk(KERN_NOTICE\n\t\t\t       \"RDS/IB: %s: unexpected opcode 0x%x in WR!\\n\",\n\t\t\t       __func__, send->s_wr.opcode);\n\t\tbreak;\n\t}\n\n\tsend->s_wr.opcode = 0xdead;\n\n\treturn rm;\n}\n\nvoid rds_ib_send_init_ring(struct rds_ib_connection *ic)\n{\n\tstruct rds_ib_send_work *send;\n\tu32 i;\n\n\tfor (i = 0, send = ic->i_sends; i < ic->i_send_ring.w_nr; i++, send++) {\n\t\tstruct ib_sge *sge;\n\n\t\tsend->s_op = NULL;\n\n\t\tsend->s_wr.wr_id = i;\n\t\tsend->s_wr.sg_list = send->s_sge;\n\t\tsend->s_wr.ex.imm_data = 0;\n\n\t\tsge = &send->s_sge[0];\n\t\tsge->addr = ic->i_send_hdrs_dma + (i * sizeof(struct rds_header));\n\t\tsge->length = sizeof(struct rds_header);\n\t\tsge->lkey = ic->i_mr->lkey;\n\n\t\tsend->s_sge[1].lkey = ic->i_mr->lkey;\n\t}\n}\n\nvoid rds_ib_send_clear_ring(struct rds_ib_connection *ic)\n{\n\tstruct rds_ib_send_work *send;\n\tu32 i;\n\n\tfor (i = 0, send = ic->i_sends; i < ic->i_send_ring.w_nr; i++, send++) {\n\t\tif (send->s_op && send->s_wr.opcode != 0xdead)\n\t\t\trds_ib_send_unmap_op(ic, send, IB_WC_WR_FLUSH_ERR);\n\t}\n}\n\n/*\n * The only fast path caller always has a non-zero nr, so we don't\n * bother testing nr before performing the atomic sub.\n */\nstatic void rds_ib_sub_signaled(struct rds_ib_connection *ic, int nr)\n{\n\tif ((atomic_sub_return(nr, &ic->i_signaled_sends) == 0) &&\n\t    waitqueue_active(&rds_ib_ring_empty_wait))\n\t\twake_up(&rds_ib_ring_empty_wait);\n\tBUG_ON(atomic_read(&ic->i_signaled_sends) < 0);\n}\n\n/*\n * The _oldest/_free ring operations here race cleanly with the alloc/unalloc\n * operations performed in the send path.  As the sender allocs and potentially\n * unallocs the next free entry in the ring it doesn't alter which is\n * the next to be freed, which is what this is concerned with.\n */\nvoid rds_ib_send_cq_comp_handler(struct ib_cq *cq, void *context)\n{\n\tstruct rds_connection *conn = context;\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct rds_message *rm = NULL;\n\tstruct ib_wc wc;\n\tstruct rds_ib_send_work *send;\n\tu32 completed;\n\tu32 oldest;\n\tu32 i = 0;\n\tint ret;\n\tint nr_sig = 0;\n\n\trdsdebug(\"cq %p conn %p\\n\", cq, conn);\n\trds_ib_stats_inc(s_ib_tx_cq_call);\n\tret = ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);\n\tif (ret)\n\t\trdsdebug(\"ib_req_notify_cq send failed: %d\\n\", ret);\n\n\twhile (ib_poll_cq(cq, 1, &wc) > 0) {\n\t\trdsdebug(\"wc wr_id 0x%llx status %u (%s) byte_len %u imm_data %u\\n\",\n\t\t\t (unsigned long long)wc.wr_id, wc.status,\n\t\t\t rds_ib_wc_status_str(wc.status), wc.byte_len,\n\t\t\t be32_to_cpu(wc.ex.imm_data));\n\t\trds_ib_stats_inc(s_ib_tx_cq_event);\n\n\t\tif (wc.wr_id == RDS_IB_ACK_WR_ID) {\n\t\t\tif (ic->i_ack_queued + HZ/2 < jiffies)\n\t\t\t\trds_ib_stats_inc(s_ib_tx_stalled);\n\t\t\trds_ib_ack_send_complete(ic);\n\t\t\tcontinue;\n\t\t}\n\n\t\toldest = rds_ib_ring_oldest(&ic->i_send_ring);\n\n\t\tcompleted = rds_ib_ring_completed(&ic->i_send_ring, wc.wr_id, oldest);\n\n\t\tfor (i = 0; i < completed; i++) {\n\t\t\tsend = &ic->i_sends[oldest];\n\t\t\tif (send->s_wr.send_flags & IB_SEND_SIGNALED)\n\t\t\t\tnr_sig++;\n\n\t\t\trm = rds_ib_send_unmap_op(ic, send, wc.status);\n\n\t\t\tif (send->s_queued + HZ/2 < jiffies)\n\t\t\t\trds_ib_stats_inc(s_ib_tx_stalled);\n\n\t\t\tif (send->s_op) {\n\t\t\t\tif (send->s_op == rm->m_final_op) {\n\t\t\t\t\t/* If anyone waited for this message to get flushed out, wake\n\t\t\t\t\t * them up now */\n\t\t\t\t\trds_message_unmapped(rm);\n\t\t\t\t}\n\t\t\t\trds_message_put(rm);\n\t\t\t\tsend->s_op = NULL;\n\t\t\t}\n\n\t\t\toldest = (oldest + 1) % ic->i_send_ring.w_nr;\n\t\t}\n\n\t\trds_ib_ring_free(&ic->i_send_ring, completed);\n\t\trds_ib_sub_signaled(ic, nr_sig);\n\t\tnr_sig = 0;\n\n\t\tif (test_and_clear_bit(RDS_LL_SEND_FULL, &conn->c_flags) ||\n\t\t    test_bit(0, &conn->c_map_queued))\n\t\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 0);\n\n\t\t/* We expect errors as the qp is drained during shutdown */\n\t\tif (wc.status != IB_WC_SUCCESS && rds_conn_up(conn)) {\n\t\t\trds_ib_conn_error(conn, \"send completion on %pI4 had status \"\n\t\t\t\t\t  \"%u (%s), disconnecting and reconnecting\\n\",\n\t\t\t\t\t  &conn->c_faddr, wc.status,\n\t\t\t\t\t  rds_ib_wc_status_str(wc.status));\n\t\t}\n\t}\n}\n\n/*\n * This is the main function for allocating credits when sending\n * messages.\n *\n * Conceptually, we have two counters:\n *  -\tsend credits: this tells us how many WRs we're allowed\n *\tto submit without overruning the reciever's queue. For\n *\teach SEND WR we post, we decrement this by one.\n *\n *  -\tposted credits: this tells us how many WRs we recently\n *\tposted to the receive queue. This value is transferred\n *\tto the peer as a \"credit update\" in a RDS header field.\n *\tEvery time we transmit credits to the peer, we subtract\n *\tthe amount of transferred credits from this counter.\n *\n * It is essential that we avoid situations where both sides have\n * exhausted their send credits, and are unable to send new credits\n * to the peer. We achieve this by requiring that we send at least\n * one credit update to the peer before exhausting our credits.\n * When new credits arrive, we subtract one credit that is withheld\n * until we've posted new buffers and are ready to transmit these\n * credits (see rds_ib_send_add_credits below).\n *\n * The RDS send code is essentially single-threaded; rds_send_xmit\n * sets RDS_IN_XMIT to ensure exclusive access to the send ring.\n * However, the ACK sending code is independent and can race with\n * message SENDs.\n *\n * In the send path, we need to update the counters for send credits\n * and the counter of posted buffers atomically - when we use the\n * last available credit, we cannot allow another thread to race us\n * and grab the posted credits counter.  Hence, we have to use a\n * spinlock to protect the credit counter, or use atomics.\n *\n * Spinlocks shared between the send and the receive path are bad,\n * because they create unnecessary delays. An early implementation\n * using a spinlock showed a 5% degradation in throughput at some\n * loads.\n *\n * This implementation avoids spinlocks completely, putting both\n * counters into a single atomic, and updating that atomic using\n * atomic_add (in the receive path, when receiving fresh credits),\n * and using atomic_cmpxchg when updating the two counters.\n */\nint rds_ib_send_grab_credits(struct rds_ib_connection *ic,\n\t\t\t     u32 wanted, u32 *adv_credits, int need_posted, int max_posted)\n{\n\tunsigned int avail, posted, got = 0, advertise;\n\tlong oldval, newval;\n\n\t*adv_credits = 0;\n\tif (!ic->i_flowctl)\n\t\treturn wanted;\n\ntry_again:\n\tadvertise = 0;\n\toldval = newval = atomic_read(&ic->i_credits);\n\tposted = IB_GET_POST_CREDITS(oldval);\n\tavail = IB_GET_SEND_CREDITS(oldval);\n\n\trdsdebug(\"rds_ib_send_grab_credits(%u): credits=%u posted=%u\\n\",\n\t\t\twanted, avail, posted);\n\n\t/* The last credit must be used to send a credit update. */\n\tif (avail && !posted)\n\t\tavail--;\n\n\tif (avail < wanted) {\n\t\tstruct rds_connection *conn = ic->i_cm_id->context;\n\n\t\t/* Oops, there aren't that many credits left! */\n\t\tset_bit(RDS_LL_SEND_FULL, &conn->c_flags);\n\t\tgot = avail;\n\t} else {\n\t\t/* Sometimes you get what you want, lalala. */\n\t\tgot = wanted;\n\t}\n\tnewval -= IB_SET_SEND_CREDITS(got);\n\n\t/*\n\t * If need_posted is non-zero, then the caller wants\n\t * the posted regardless of whether any send credits are\n\t * available.\n\t */\n\tif (posted && (got || need_posted)) {\n\t\tadvertise = min_t(unsigned int, posted, max_posted);\n\t\tnewval -= IB_SET_POST_CREDITS(advertise);\n\t}\n\n\t/* Finally bill everything */\n\tif (atomic_cmpxchg(&ic->i_credits, oldval, newval) != oldval)\n\t\tgoto try_again;\n\n\t*adv_credits = advertise;\n\treturn got;\n}\n\nvoid rds_ib_send_add_credits(struct rds_connection *conn, unsigned int credits)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\n\tif (credits == 0)\n\t\treturn;\n\n\trdsdebug(\"rds_ib_send_add_credits(%u): current=%u%s\\n\",\n\t\t\tcredits,\n\t\t\tIB_GET_SEND_CREDITS(atomic_read(&ic->i_credits)),\n\t\t\ttest_bit(RDS_LL_SEND_FULL, &conn->c_flags) ? \", ll_send_full\" : \"\");\n\n\tatomic_add(IB_SET_SEND_CREDITS(credits), &ic->i_credits);\n\tif (test_and_clear_bit(RDS_LL_SEND_FULL, &conn->c_flags))\n\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 0);\n\n\tWARN_ON(IB_GET_SEND_CREDITS(credits) >= 16384);\n\n\trds_ib_stats_inc(s_ib_rx_credit_updates);\n}\n\nvoid rds_ib_advertise_credits(struct rds_connection *conn, unsigned int posted)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\n\tif (posted == 0)\n\t\treturn;\n\n\tatomic_add(IB_SET_POST_CREDITS(posted), &ic->i_credits);\n\n\t/* Decide whether to send an update to the peer now.\n\t * If we would send a credit update for every single buffer we\n\t * post, we would end up with an ACK storm (ACK arrives,\n\t * consumes buffer, we refill the ring, send ACK to remote\n\t * advertising the newly posted buffer... ad inf)\n\t *\n\t * Performance pretty much depends on how often we send\n\t * credit updates - too frequent updates mean lots of ACKs.\n\t * Too infrequent updates, and the peer will run out of\n\t * credits and has to throttle.\n\t * For the time being, 16 seems to be a good compromise.\n\t */\n\tif (IB_GET_POST_CREDITS(atomic_read(&ic->i_credits)) >= 16)\n\t\tset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\n}\n\nstatic inline int rds_ib_set_wr_signal_state(struct rds_ib_connection *ic,\n\t\t\t\t\t     struct rds_ib_send_work *send,\n\t\t\t\t\t     bool notify)\n{\n\t/*\n\t * We want to delay signaling completions just enough to get\n\t * the batching benefits but not so much that we create dead time\n\t * on the wire.\n\t */\n\tif (ic->i_unsignaled_wrs-- == 0 || notify) {\n\t\tic->i_unsignaled_wrs = rds_ib_sysctl_max_unsig_wrs;\n\t\tsend->s_wr.send_flags |= IB_SEND_SIGNALED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * This can be called multiple times for a given message.  The first time\n * we see a message we map its scatterlist into the IB device so that\n * we can provide that mapped address to the IB scatter gather entries\n * in the IB work requests.  We translate the scatterlist into a series\n * of work requests that fragment the message.  These work requests complete\n * in order so we pass ownership of the message to the completion handler\n * once we send the final fragment.\n *\n * The RDS core uses the c_send_lock to only enter this function once\n * per connection.  This makes sure that the tx ring alloc/unalloc pairs\n * don't get out of sync and confuse the ring.\n */\nint rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,\n\t\tunsigned int hdr_off, unsigned int sg, unsigned int off)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct ib_device *dev = ic->i_cm_id->device;\n\tstruct rds_ib_send_work *send = NULL;\n\tstruct rds_ib_send_work *first;\n\tstruct rds_ib_send_work *prev;\n\tstruct ib_send_wr *failed_wr;\n\tstruct scatterlist *scat;\n\tu32 pos;\n\tu32 i;\n\tu32 work_alloc;\n\tu32 credit_alloc = 0;\n\tu32 posted;\n\tu32 adv_credits = 0;\n\tint send_flags = 0;\n\tint bytes_sent = 0;\n\tint ret;\n\tint flow_controlled = 0;\n\tint nr_sig = 0;\n\n\tBUG_ON(off % RDS_FRAG_SIZE);\n\tBUG_ON(hdr_off != 0 && hdr_off != sizeof(struct rds_header));\n\n\t/* Do not send cong updates to IB loopback */\n\tif (conn->c_loopback\n\t    && rm->m_inc.i_hdr.h_flags & RDS_FLAG_CONG_BITMAP) {\n\t\trds_cong_map_updated(conn->c_fcong, ~(u64) 0);\n\t\tscat = &rm->data.op_sg[sg];\n\t\tret = sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;\n\t\tret = min_t(int, ret, scat->length - conn->c_xmit_data_off);\n\t\treturn ret;\n\t}\n\n\t/* FIXME we may overallocate here */\n\tif (be32_to_cpu(rm->m_inc.i_hdr.h_len) == 0)\n\t\ti = 1;\n\telse\n\t\ti = ceil(be32_to_cpu(rm->m_inc.i_hdr.h_len), RDS_FRAG_SIZE);\n\n\twork_alloc = rds_ib_ring_alloc(&ic->i_send_ring, i, &pos);\n\tif (work_alloc == 0) {\n\t\tset_bit(RDS_LL_SEND_FULL, &conn->c_flags);\n\t\trds_ib_stats_inc(s_ib_tx_ring_full);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (ic->i_flowctl) {\n\t\tcredit_alloc = rds_ib_send_grab_credits(ic, work_alloc, &posted, 0, RDS_MAX_ADV_CREDIT);\n\t\tadv_credits += posted;\n\t\tif (credit_alloc < work_alloc) {\n\t\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc - credit_alloc);\n\t\t\twork_alloc = credit_alloc;\n\t\t\tflow_controlled = 1;\n\t\t}\n\t\tif (work_alloc == 0) {\n\t\t\tset_bit(RDS_LL_SEND_FULL, &conn->c_flags);\n\t\t\trds_ib_stats_inc(s_ib_tx_throttle);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* map the message the first time we see it */\n\tif (!ic->i_data_op) {\n\t\tif (rm->data.op_nents) {\n\t\t\trm->data.op_count = ib_dma_map_sg(dev,\n\t\t\t\t\t\t\t  rm->data.op_sg,\n\t\t\t\t\t\t\t  rm->data.op_nents,\n\t\t\t\t\t\t\t  DMA_TO_DEVICE);\n\t\t\trdsdebug(\"ic %p mapping rm %p: %d\\n\", ic, rm, rm->data.op_count);\n\t\t\tif (rm->data.op_count == 0) {\n\t\t\t\trds_ib_stats_inc(s_ib_tx_sg_mapping_failure);\n\t\t\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\t\t\tret = -ENOMEM; /* XXX ? */\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\trm->data.op_count = 0;\n\t\t}\n\n\t\trds_message_addref(rm);\n\t\tic->i_data_op = &rm->data;\n\n\t\t/* Finalize the header */\n\t\tif (test_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags))\n\t\t\trm->m_inc.i_hdr.h_flags |= RDS_FLAG_ACK_REQUIRED;\n\t\tif (test_bit(RDS_MSG_RETRANSMITTED, &rm->m_flags))\n\t\t\trm->m_inc.i_hdr.h_flags |= RDS_FLAG_RETRANSMITTED;\n\n\t\t/* If it has a RDMA op, tell the peer we did it. This is\n\t\t * used by the peer to release use-once RDMA MRs. */\n\t\tif (rm->rdma.op_active) {\n\t\t\tstruct rds_ext_header_rdma ext_hdr;\n\n\t\t\text_hdr.h_rdma_rkey = cpu_to_be32(rm->rdma.op_rkey);\n\t\t\trds_message_add_extension(&rm->m_inc.i_hdr,\n\t\t\t\t\tRDS_EXTHDR_RDMA, &ext_hdr, sizeof(ext_hdr));\n\t\t}\n\t\tif (rm->m_rdma_cookie) {\n\t\t\trds_message_add_rdma_dest_extension(&rm->m_inc.i_hdr,\n\t\t\t\t\trds_rdma_cookie_key(rm->m_rdma_cookie),\n\t\t\t\t\trds_rdma_cookie_offset(rm->m_rdma_cookie));\n\t\t}\n\n\t\t/* Note - rds_ib_piggyb_ack clears the ACK_REQUIRED bit, so\n\t\t * we should not do this unless we have a chance of at least\n\t\t * sticking the header into the send ring. Which is why we\n\t\t * should call rds_ib_ring_alloc first. */\n\t\trm->m_inc.i_hdr.h_ack = cpu_to_be64(rds_ib_piggyb_ack(ic));\n\t\trds_message_make_checksum(&rm->m_inc.i_hdr);\n\n\t\t/*\n\t\t * Update adv_credits since we reset the ACK_REQUIRED bit.\n\t\t */\n\t\tif (ic->i_flowctl) {\n\t\t\trds_ib_send_grab_credits(ic, 0, &posted, 1, RDS_MAX_ADV_CREDIT - adv_credits);\n\t\t\tadv_credits += posted;\n\t\t\tBUG_ON(adv_credits > 255);\n\t\t}\n\t}\n\n\t/* Sometimes you want to put a fence between an RDMA\n\t * READ and the following SEND.\n\t * We could either do this all the time\n\t * or when requested by the user. Right now, we let\n\t * the application choose.\n\t */\n\tif (rm->rdma.op_active && rm->rdma.op_fence)\n\t\tsend_flags = IB_SEND_FENCE;\n\n\t/* Each frag gets a header. Msgs may be 0 bytes */\n\tsend = &ic->i_sends[pos];\n\tfirst = send;\n\tprev = NULL;\n\tscat = &ic->i_data_op->op_sg[sg];\n\ti = 0;\n\tdo {\n\t\tunsigned int len = 0;\n\n\t\t/* Set up the header */\n\t\tsend->s_wr.send_flags = send_flags;\n\t\tsend->s_wr.opcode = IB_WR_SEND;\n\t\tsend->s_wr.num_sge = 1;\n\t\tsend->s_wr.next = NULL;\n\t\tsend->s_queued = jiffies;\n\t\tsend->s_op = NULL;\n\n\t\tsend->s_sge[0].addr = ic->i_send_hdrs_dma\n\t\t\t+ (pos * sizeof(struct rds_header));\n\t\tsend->s_sge[0].length = sizeof(struct rds_header);\n\n\t\tmemcpy(&ic->i_send_hdrs[pos], &rm->m_inc.i_hdr, sizeof(struct rds_header));\n\n\t\t/* Set up the data, if present */\n\t\tif (i < work_alloc\n\t\t    && scat != &rm->data.op_sg[rm->data.op_count]) {\n\t\t\tlen = min(RDS_FRAG_SIZE, ib_sg_dma_len(dev, scat) - off);\n\t\t\tsend->s_wr.num_sge = 2;\n\n\t\t\tsend->s_sge[1].addr = ib_sg_dma_address(dev, scat) + off;\n\t\t\tsend->s_sge[1].length = len;\n\n\t\t\tbytes_sent += len;\n\t\t\toff += len;\n\t\t\tif (off == ib_sg_dma_len(dev, scat)) {\n\t\t\t\tscat++;\n\t\t\t\toff = 0;\n\t\t\t}\n\t\t}\n\n\t\trds_ib_set_wr_signal_state(ic, send, 0);\n\n\t\t/*\n\t\t * Always signal the last one if we're stopping due to flow control.\n\t\t */\n\t\tif (ic->i_flowctl && flow_controlled && i == (work_alloc-1))\n\t\t\tsend->s_wr.send_flags |= IB_SEND_SIGNALED | IB_SEND_SOLICITED;\n\n\t\tif (send->s_wr.send_flags & IB_SEND_SIGNALED)\n\t\t\tnr_sig++;\n\n\t\trdsdebug(\"send %p wr %p num_sge %u next %p\\n\", send,\n\t\t\t &send->s_wr, send->s_wr.num_sge, send->s_wr.next);\n\n\t\tif (ic->i_flowctl && adv_credits) {\n\t\t\tstruct rds_header *hdr = &ic->i_send_hdrs[pos];\n\n\t\t\t/* add credit and redo the header checksum */\n\t\t\thdr->h_credit = adv_credits;\n\t\t\trds_message_make_checksum(hdr);\n\t\t\tadv_credits = 0;\n\t\t\trds_ib_stats_inc(s_ib_tx_credit_updates);\n\t\t}\n\n\t\tif (prev)\n\t\t\tprev->s_wr.next = &send->s_wr;\n\t\tprev = send;\n\n\t\tpos = (pos + 1) % ic->i_send_ring.w_nr;\n\t\tsend = &ic->i_sends[pos];\n\t\ti++;\n\n\t} while (i < work_alloc\n\t\t && scat != &rm->data.op_sg[rm->data.op_count]);\n\n\t/* Account the RDS header in the number of bytes we sent, but just once.\n\t * The caller has no concept of fragmentation. */\n\tif (hdr_off == 0)\n\t\tbytes_sent += sizeof(struct rds_header);\n\n\t/* if we finished the message then send completion owns it */\n\tif (scat == &rm->data.op_sg[rm->data.op_count]) {\n\t\tprev->s_op = ic->i_data_op;\n\t\tprev->s_wr.send_flags |= IB_SEND_SOLICITED;\n\t\tic->i_data_op = NULL;\n\t}\n\n\t/* Put back wrs & credits we didn't use */\n\tif (i < work_alloc) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc - i);\n\t\twork_alloc = i;\n\t}\n\tif (ic->i_flowctl && i < credit_alloc)\n\t\trds_ib_send_add_credits(conn, credit_alloc - i);\n\n\tif (nr_sig)\n\t\tatomic_add(nr_sig, &ic->i_signaled_sends);\n\n\t/* XXX need to worry about failed_wr and partial sends. */\n\tfailed_wr = &first->s_wr;\n\tret = ib_post_send(ic->i_cm_id->qp, &first->s_wr, &failed_wr);\n\trdsdebug(\"ic %p first %p (wr %p) ret %d wr %p\\n\", ic,\n\t\t first, &first->s_wr, ret, failed_wr);\n\tBUG_ON(failed_wr != &first->s_wr);\n\tif (ret) {\n\t\tprintk(KERN_WARNING \"RDS/IB: ib_post_send to %pI4 \"\n\t\t       \"returned %d\\n\", &conn->c_faddr, ret);\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_sub_signaled(ic, nr_sig);\n\t\tif (prev->s_op) {\n\t\t\tic->i_data_op = prev->s_op;\n\t\t\tprev->s_op = NULL;\n\t\t}\n\n\t\trds_ib_conn_error(ic->conn, \"ib_post_send failed\\n\");\n\t\tgoto out;\n\t}\n\n\tret = bytes_sent;\nout:\n\tBUG_ON(adv_credits);\n\treturn ret;\n}\n\n/*\n * Issue atomic operation.\n * A simplified version of the rdma case, we always map 1 SG, and\n * only 8 bytes, for the return value from the atomic operation.\n */\nint rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct rds_ib_send_work *send = NULL;\n\tstruct ib_send_wr *failed_wr;\n\tstruct rds_ib_device *rds_ibdev;\n\tu32 pos;\n\tu32 work_alloc;\n\tint ret;\n\tint nr_sig = 0;\n\n\trds_ibdev = ib_get_client_data(ic->i_cm_id->device, &rds_ib_client);\n\n\twork_alloc = rds_ib_ring_alloc(&ic->i_send_ring, 1, &pos);\n\tif (work_alloc != 1) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_stats_inc(s_ib_tx_ring_full);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* address of send request in ring */\n\tsend = &ic->i_sends[pos];\n\tsend->s_queued = jiffies;\n\n\tif (op->op_type == RDS_ATOMIC_TYPE_CSWP) {\n\t\tsend->s_wr.opcode = IB_WR_MASKED_ATOMIC_CMP_AND_SWP;\n\t\tsend->s_wr.wr.atomic.compare_add = op->op_m_cswp.compare;\n\t\tsend->s_wr.wr.atomic.swap = op->op_m_cswp.swap;\n\t\tsend->s_wr.wr.atomic.compare_add_mask = op->op_m_cswp.compare_mask;\n\t\tsend->s_wr.wr.atomic.swap_mask = op->op_m_cswp.swap_mask;\n\t} else { /* FADD */\n\t\tsend->s_wr.opcode = IB_WR_MASKED_ATOMIC_FETCH_AND_ADD;\n\t\tsend->s_wr.wr.atomic.compare_add = op->op_m_fadd.add;\n\t\tsend->s_wr.wr.atomic.swap = 0;\n\t\tsend->s_wr.wr.atomic.compare_add_mask = op->op_m_fadd.nocarry_mask;\n\t\tsend->s_wr.wr.atomic.swap_mask = 0;\n\t}\n\tnr_sig = rds_ib_set_wr_signal_state(ic, send, op->op_notify);\n\tsend->s_wr.num_sge = 1;\n\tsend->s_wr.next = NULL;\n\tsend->s_wr.wr.atomic.remote_addr = op->op_remote_addr;\n\tsend->s_wr.wr.atomic.rkey = op->op_rkey;\n\tsend->s_op = op;\n\trds_message_addref(container_of(send->s_op, struct rds_message, atomic));\n\n\t/* map 8 byte retval buffer to the device */\n\tret = ib_dma_map_sg(ic->i_cm_id->device, op->op_sg, 1, DMA_FROM_DEVICE);\n\trdsdebug(\"ic %p mapping atomic op %p. mapped %d pg\\n\", ic, op, ret);\n\tif (ret != 1) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_stats_inc(s_ib_tx_sg_mapping_failure);\n\t\tret = -ENOMEM; /* XXX ? */\n\t\tgoto out;\n\t}\n\n\t/* Convert our struct scatterlist to struct ib_sge */\n\tsend->s_sge[0].addr = ib_sg_dma_address(ic->i_cm_id->device, op->op_sg);\n\tsend->s_sge[0].length = ib_sg_dma_len(ic->i_cm_id->device, op->op_sg);\n\tsend->s_sge[0].lkey = ic->i_mr->lkey;\n\n\trdsdebug(\"rva %Lx rpa %Lx len %u\\n\", op->op_remote_addr,\n\t\t send->s_sge[0].addr, send->s_sge[0].length);\n\n\tif (nr_sig)\n\t\tatomic_add(nr_sig, &ic->i_signaled_sends);\n\n\tfailed_wr = &send->s_wr;\n\tret = ib_post_send(ic->i_cm_id->qp, &send->s_wr, &failed_wr);\n\trdsdebug(\"ic %p send %p (wr %p) ret %d wr %p\\n\", ic,\n\t\t send, &send->s_wr, ret, failed_wr);\n\tBUG_ON(failed_wr != &send->s_wr);\n\tif (ret) {\n\t\tprintk(KERN_WARNING \"RDS/IB: atomic ib_post_send to %pI4 \"\n\t\t       \"returned %d\\n\", &conn->c_faddr, ret);\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_sub_signaled(ic, nr_sig);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(failed_wr != &send->s_wr)) {\n\t\tprintk(KERN_WARNING \"RDS/IB: atomic ib_post_send() rc=%d, but failed_wqe updated!\\n\", ret);\n\t\tBUG_ON(failed_wr != &send->s_wr);\n\t}\n\nout:\n\treturn ret;\n}\n\nint rds_ib_xmit_rdma(struct rds_connection *conn, struct rm_rdma_op *op)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct rds_ib_send_work *send = NULL;\n\tstruct rds_ib_send_work *first;\n\tstruct rds_ib_send_work *prev;\n\tstruct ib_send_wr *failed_wr;\n\tstruct scatterlist *scat;\n\tunsigned long len;\n\tu64 remote_addr = op->op_remote_addr;\n\tu32 max_sge = ic->rds_ibdev->max_sge;\n\tu32 pos;\n\tu32 work_alloc;\n\tu32 i;\n\tu32 j;\n\tint sent;\n\tint ret;\n\tint num_sge;\n\tint nr_sig = 0;\n\n\t/* map the op the first time we see it */\n\tif (!op->op_mapped) {\n\t\top->op_count = ib_dma_map_sg(ic->i_cm_id->device,\n\t\t\t\t\t     op->op_sg, op->op_nents, (op->op_write) ?\n\t\t\t\t\t     DMA_TO_DEVICE : DMA_FROM_DEVICE);\n\t\trdsdebug(\"ic %p mapping op %p: %d\\n\", ic, op, op->op_count);\n\t\tif (op->op_count == 0) {\n\t\t\trds_ib_stats_inc(s_ib_tx_sg_mapping_failure);\n\t\t\tret = -ENOMEM; /* XXX ? */\n\t\t\tgoto out;\n\t\t}\n\n\t\top->op_mapped = 1;\n\t}\n\n\t/*\n\t * Instead of knowing how to return a partial rdma read/write we insist that there\n\t * be enough work requests to send the entire message.\n\t */\n\ti = ceil(op->op_count, max_sge);\n\n\twork_alloc = rds_ib_ring_alloc(&ic->i_send_ring, i, &pos);\n\tif (work_alloc != i) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_stats_inc(s_ib_tx_ring_full);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tsend = &ic->i_sends[pos];\n\tfirst = send;\n\tprev = NULL;\n\tscat = &op->op_sg[0];\n\tsent = 0;\n\tnum_sge = op->op_count;\n\n\tfor (i = 0; i < work_alloc && scat != &op->op_sg[op->op_count]; i++) {\n\t\tsend->s_wr.send_flags = 0;\n\t\tsend->s_queued = jiffies;\n\t\tsend->s_op = NULL;\n\n\t\tnr_sig += rds_ib_set_wr_signal_state(ic, send, op->op_notify);\n\n\t\tsend->s_wr.opcode = op->op_write ? IB_WR_RDMA_WRITE : IB_WR_RDMA_READ;\n\t\tsend->s_wr.wr.rdma.remote_addr = remote_addr;\n\t\tsend->s_wr.wr.rdma.rkey = op->op_rkey;\n\n\t\tif (num_sge > max_sge) {\n\t\t\tsend->s_wr.num_sge = max_sge;\n\t\t\tnum_sge -= max_sge;\n\t\t} else {\n\t\t\tsend->s_wr.num_sge = num_sge;\n\t\t}\n\n\t\tsend->s_wr.next = NULL;\n\n\t\tif (prev)\n\t\t\tprev->s_wr.next = &send->s_wr;\n\n\t\tfor (j = 0; j < send->s_wr.num_sge && scat != &op->op_sg[op->op_count]; j++) {\n\t\t\tlen = ib_sg_dma_len(ic->i_cm_id->device, scat);\n\t\t\tsend->s_sge[j].addr =\n\t\t\t\t ib_sg_dma_address(ic->i_cm_id->device, scat);\n\t\t\tsend->s_sge[j].length = len;\n\t\t\tsend->s_sge[j].lkey = ic->i_mr->lkey;\n\n\t\t\tsent += len;\n\t\t\trdsdebug(\"ic %p sent %d remote_addr %llu\\n\", ic, sent, remote_addr);\n\n\t\t\tremote_addr += len;\n\t\t\tscat++;\n\t\t}\n\n\t\trdsdebug(\"send %p wr %p num_sge %u next %p\\n\", send,\n\t\t\t&send->s_wr, send->s_wr.num_sge, send->s_wr.next);\n\n\t\tprev = send;\n\t\tif (++send == &ic->i_sends[ic->i_send_ring.w_nr])\n\t\t\tsend = ic->i_sends;\n\t}\n\n\t/* give a reference to the last op */\n\tif (scat == &op->op_sg[op->op_count]) {\n\t\tprev->s_op = op;\n\t\trds_message_addref(container_of(op, struct rds_message, rdma));\n\t}\n\n\tif (i < work_alloc) {\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc - i);\n\t\twork_alloc = i;\n\t}\n\n\tif (nr_sig)\n\t\tatomic_add(nr_sig, &ic->i_signaled_sends);\n\n\tfailed_wr = &first->s_wr;\n\tret = ib_post_send(ic->i_cm_id->qp, &first->s_wr, &failed_wr);\n\trdsdebug(\"ic %p first %p (wr %p) ret %d wr %p\\n\", ic,\n\t\t first, &first->s_wr, ret, failed_wr);\n\tBUG_ON(failed_wr != &first->s_wr);\n\tif (ret) {\n\t\tprintk(KERN_WARNING \"RDS/IB: rdma ib_post_send to %pI4 \"\n\t\t       \"returned %d\\n\", &conn->c_faddr, ret);\n\t\trds_ib_ring_unalloc(&ic->i_send_ring, work_alloc);\n\t\trds_ib_sub_signaled(ic, nr_sig);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(failed_wr != &first->s_wr)) {\n\t\tprintk(KERN_WARNING \"RDS/IB: ib_post_send() rc=%d, but failed_wqe updated!\\n\", ret);\n\t\tBUG_ON(failed_wr != &first->s_wr);\n\t}\n\n\nout:\n\treturn ret;\n}\n\nvoid rds_ib_xmit_complete(struct rds_connection *conn)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\n\t/* We may have a pending ACK or window update we were unable\n\t * to send previously (due to flow control). Try again. */\n\trds_ib_attempt_ack(ic);\n}\n", "/*\n * Copyright (c) 2006 Oracle.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/in.h>\n\n#include \"rds.h\"\n#include \"loop.h\"\n\nstatic DEFINE_SPINLOCK(loop_conns_lock);\nstatic LIST_HEAD(loop_conns);\n\n/*\n * This 'loopback' transport is a special case for flows that originate\n * and terminate on the same machine.\n *\n * Connection build-up notices if the destination address is thought of\n * as a local address by a transport.  At that time it decides to use the\n * loopback transport instead of the bound transport of the sending socket.\n *\n * The loopback transport's sending path just hands the sent rds_message\n * straight to the receiving path via an embedded rds_incoming.\n */\n\n/*\n * Usually a message transits both the sender and receiver's conns as it\n * flows to the receiver.  In the loopback case, though, the receive path\n * is handed the sending conn so the sense of the addresses is reversed.\n */\nstatic int rds_loop_xmit(struct rds_connection *conn, struct rds_message *rm,\n\t\t\t unsigned int hdr_off, unsigned int sg,\n\t\t\t unsigned int off)\n{\n\tstruct scatterlist *sgp = &rm->data.op_sg[sg];\n\tint ret = sizeof(struct rds_header) +\n\t\t\tbe32_to_cpu(rm->m_inc.i_hdr.h_len);\n\n\t/* Do not send cong updates to loopback */\n\tif (rm->m_inc.i_hdr.h_flags & RDS_FLAG_CONG_BITMAP) {\n\t\trds_cong_map_updated(conn->c_fcong, ~(u64) 0);\n\t\tret = min_t(int, ret, sgp->length - conn->c_xmit_data_off);\n\t\tgoto out;\n\t}\n\n\tBUG_ON(hdr_off || sg || off);\n\n\trds_inc_init(&rm->m_inc, conn, conn->c_laddr);\n\t/* For the embedded inc. Matching put is in loop_inc_free() */\n\trds_message_addref(rm);\n\n\trds_recv_incoming(conn, conn->c_laddr, conn->c_faddr, &rm->m_inc,\n\t\t\t  GFP_KERNEL, KM_USER0);\n\n\trds_send_drop_acked(conn, be64_to_cpu(rm->m_inc.i_hdr.h_sequence),\n\t\t\t    NULL);\n\n\trds_inc_put(&rm->m_inc);\nout:\n\treturn ret;\n}\n\n/*\n * See rds_loop_xmit(). Since our inc is embedded in the rm, we\n * make sure the rm lives at least until the inc is done.\n */\nstatic void rds_loop_inc_free(struct rds_incoming *inc)\n{\n        struct rds_message *rm = container_of(inc, struct rds_message, m_inc);\n        rds_message_put(rm);\n}\n\n/* we need to at least give the thread something to succeed */\nstatic int rds_loop_recv(struct rds_connection *conn)\n{\n\treturn 0;\n}\n\nstruct rds_loop_connection {\n\tstruct list_head loop_node;\n\tstruct rds_connection *conn;\n};\n\n/*\n * Even the loopback transport needs to keep track of its connections,\n * so it can call rds_conn_destroy() on them on exit. N.B. there are\n * 1+ loopback addresses (127.*.*.*) so it's not a bug to have\n * multiple loopback conns allocated, although rather useless.\n */\nstatic int rds_loop_conn_alloc(struct rds_connection *conn, gfp_t gfp)\n{\n\tstruct rds_loop_connection *lc;\n\tunsigned long flags;\n\n\tlc = kzalloc(sizeof(struct rds_loop_connection), GFP_KERNEL);\n\tif (!lc)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&lc->loop_node);\n\tlc->conn = conn;\n\tconn->c_transport_data = lc;\n\n\tspin_lock_irqsave(&loop_conns_lock, flags);\n\tlist_add_tail(&lc->loop_node, &loop_conns);\n\tspin_unlock_irqrestore(&loop_conns_lock, flags);\n\n\treturn 0;\n}\n\nstatic void rds_loop_conn_free(void *arg)\n{\n\tstruct rds_loop_connection *lc = arg;\n\tunsigned long flags;\n\n\trdsdebug(\"lc %p\\n\", lc);\n\tspin_lock_irqsave(&loop_conns_lock, flags);\n\tlist_del(&lc->loop_node);\n\tspin_unlock_irqrestore(&loop_conns_lock, flags);\n\tkfree(lc);\n}\n\nstatic int rds_loop_conn_connect(struct rds_connection *conn)\n{\n\trds_connect_complete(conn);\n\treturn 0;\n}\n\nstatic void rds_loop_conn_shutdown(struct rds_connection *conn)\n{\n}\n\nvoid rds_loop_exit(void)\n{\n\tstruct rds_loop_connection *lc, *_lc;\n\tLIST_HEAD(tmp_list);\n\n\t/* avoid calling conn_destroy with irqs off */\n\tspin_lock_irq(&loop_conns_lock);\n\tlist_splice(&loop_conns, &tmp_list);\n\tINIT_LIST_HEAD(&loop_conns);\n\tspin_unlock_irq(&loop_conns_lock);\n\n\tlist_for_each_entry_safe(lc, _lc, &tmp_list, loop_node) {\n\t\tWARN_ON(lc->conn->c_passive);\n\t\trds_conn_destroy(lc->conn);\n\t}\n}\n\n/*\n * This is missing .xmit_* because loop doesn't go through generic\n * rds_send_xmit() and doesn't call rds_recv_incoming().  .listen_stop and\n * .laddr_check are missing because transport.c doesn't iterate over\n * rds_loop_transport.\n */\nstruct rds_transport rds_loop_transport = {\n\t.xmit\t\t\t= rds_loop_xmit,\n\t.recv\t\t\t= rds_loop_recv,\n\t.conn_alloc\t\t= rds_loop_conn_alloc,\n\t.conn_free\t\t= rds_loop_conn_free,\n\t.conn_connect\t\t= rds_loop_conn_connect,\n\t.conn_shutdown\t\t= rds_loop_conn_shutdown,\n\t.inc_copy_to_user\t= rds_message_inc_copy_to_user,\n\t.inc_free\t\t= rds_loop_inc_free,\n\t.t_name\t\t\t= \"loopback\",\n};\n"], "filenames": ["net/rds/ib_send.c", "net/rds/loop.c"], "buggy_code_start_loc": [554, 63], "buggy_code_end_loc": [555, 85], "fixing_code_start_loc": [554, 64], "fixing_code_end_loc": [558, 90], "type": "NVD-CWE-Other", "message": "The Reliable Datagram Sockets (RDS) subsystem in the Linux kernel before 2.6.38 does not properly handle congestion map updates, which allows local users to cause a denial of service (BUG_ON and system crash) via vectors involving (1) a loopback (aka loop) transmit operation or (2) an InfiniBand (aka ib) transmit operation.", "other": {"cve": {"id": "CVE-2011-1023", "sourceIdentifier": "secalert@redhat.com", "published": "2012-06-21T23:55:01.943", "lastModified": "2023-02-13T01:18:48.270", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Reliable Datagram Sockets (RDS) subsystem in the Linux kernel before 2.6.38 does not properly handle congestion map updates, which allows local users to cause a denial of service (BUG_ON and system crash) via vectors involving (1) a loopback (aka loop) transmit operation or (2) an InfiniBand (aka ib) transmit operation."}, {"lang": "es", "value": "El subsistema 'Reliable Datagram Sockets' (SDR) del kernel de Linux en versiones anteriores a v2.6.38 no gestiona correctamente las actualizaciones del mapa de congestiones, lo que permite a usuarios locales causar una denegaci\u00f3n de servicio (ca\u00edda del sistema) a trav\u00e9s de vectores relacionados con (1) una opeeraci\u00f3n de transmisi\u00f3n 'loopback' (loop) o (2) una operaci\u00f3n de transmisi\u00f3n 'InfiniBand' (ib)."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.6.37.6", "matchCriteriaId": "084AC06C-2438-4BD5-A28D-8D24DE515135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37:*:*:*:*:*:*:*", "matchCriteriaId": "18CBFC41-E9A9-456C-8A61-8DB2E6CE2E98"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37:rc1:*:*:*:*:*:*", "matchCriteriaId": "2EA6C6E6-CAD5-4D43-AD96-66D5ACBB91CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37:rc2:*:*:*:*:*:*", "matchCriteriaId": "71905CF7-7C7B-43AC-970D-D3187A807903"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37:rc3:*:*:*:*:*:*", "matchCriteriaId": "201421C9-E054-4FEB-A37A-8C314F242FBC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37:rc4:*:*:*:*:*:*", "matchCriteriaId": "F157225D-C62C-465D-A758-DE6A6C48C397"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37:rc5:*:*:*:*:*:*", "matchCriteriaId": "77BB49A9-39D0-49C4-A241-D1537590F508"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37.1:*:*:*:*:*:*:*", "matchCriteriaId": "CF9A28CB-B307-4D0B-AC45-73964F766B09"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37.2:*:*:*:*:*:*:*", "matchCriteriaId": "51838021-099C-4135-94E6-EC0276BAB750"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37.3:*:*:*:*:*:*:*", "matchCriteriaId": "E0F4EBC4-1C6D-428C-9F23-8508EBBD3588"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37.4:*:*:*:*:*:*:*", "matchCriteriaId": "B86233BC-2B7D-44EB-8253-458E89046B17"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.37.5:*:*:*:*:*:*:*", "matchCriteriaId": "110BB215-C869-4BEB-962D-81A1F9F896B6"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v2.6/ChangeLog-2.6.38", "source": "secalert@redhat.com"}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=6094628bfd94323fc1cea05ec2c6affd98c18f7f", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/03/03/2", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=680345", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/6094628bfd94323fc1cea05ec2c6affd98c18f7f", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/6094628bfd94323fc1cea05ec2c6affd98c18f7f"}}