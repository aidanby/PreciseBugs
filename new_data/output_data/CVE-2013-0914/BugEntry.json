{"buggy_code": ["/*\n *  linux/kernel/signal.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson\n *\n *  2003-06-02  Jim Houston - Concurrent Computer Corp.\n *\t\tChanges to use preallocated sigqueue structures\n *\t\tto allow signals to be sent reliably.\n */\n\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/tty.h>\n#include <linux/binfmts.h>\n#include <linux/coredump.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/ptrace.h>\n#include <linux/signal.h>\n#include <linux/signalfd.h>\n#include <linux/ratelimit.h>\n#include <linux/tracehook.h>\n#include <linux/capability.h>\n#include <linux/freezer.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/user_namespace.h>\n#include <linux/uprobes.h>\n#include <linux/compat.h>\n#define CREATE_TRACE_POINTS\n#include <trace/events/signal.h>\n\n#include <asm/param.h>\n#include <asm/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/siginfo.h>\n#include <asm/cacheflush.h>\n#include \"audit.h\"\t/* audit_signal_info() */\n\n/*\n * SLAB caches for signal bits.\n */\n\nstatic struct kmem_cache *sigqueue_cachep;\n\nint print_fatal_signals __read_mostly;\n\nstatic void __user *sig_handler(struct task_struct *t, int sig)\n{\n\treturn t->sighand->action[sig - 1].sa.sa_handler;\n}\n\nstatic int sig_handler_ignored(void __user *handler, int sig)\n{\n\t/* Is it explicitly or implicitly ignored? */\n\treturn handler == SIG_IGN ||\n\t\t(handler == SIG_DFL && sig_kernel_ignore(sig));\n}\n\nstatic int sig_task_ignored(struct task_struct *t, int sig, bool force)\n{\n\tvoid __user *handler;\n\n\thandler = sig_handler(t, sig);\n\n\tif (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\thandler == SIG_DFL && !force)\n\t\treturn 1;\n\n\treturn sig_handler_ignored(handler, sig);\n}\n\nstatic int sig_ignored(struct task_struct *t, int sig, bool force)\n{\n\t/*\n\t * Blocked signals are never ignored, since the\n\t * signal handler may change by the time it is\n\t * unblocked.\n\t */\n\tif (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))\n\t\treturn 0;\n\n\tif (!sig_task_ignored(t, sig, force))\n\t\treturn 0;\n\n\t/*\n\t * Tracers may want to know about even ignored signals.\n\t */\n\treturn !t->ptrace;\n}\n\n/*\n * Re-calculate pending state from the set of locally pending\n * signals, globally pending signals, and blocked signals.\n */\nstatic inline int has_pending_signals(sigset_t *signal, sigset_t *blocked)\n{\n\tunsigned long ready;\n\tlong i;\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = _NSIG_WORDS, ready = 0; --i >= 0 ;)\n\t\t\tready |= signal->sig[i] &~ blocked->sig[i];\n\t\tbreak;\n\n\tcase 4: ready  = signal->sig[3] &~ blocked->sig[3];\n\t\tready |= signal->sig[2] &~ blocked->sig[2];\n\t\tready |= signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 2: ready  = signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 1: ready  = signal->sig[0] &~ blocked->sig[0];\n\t}\n\treturn ready !=\t0;\n}\n\n#define PENDING(p,b) has_pending_signals(&(p)->signal, (b))\n\nstatic int recalc_sigpending_tsk(struct task_struct *t)\n{\n\tif ((t->jobctl & JOBCTL_PENDING_MASK) ||\n\t    PENDING(&t->pending, &t->blocked) ||\n\t    PENDING(&t->signal->shared_pending, &t->blocked)) {\n\t\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t\treturn 1;\n\t}\n\t/*\n\t * We must never clear the flag in another thread, or in current\n\t * when it's possible the current syscall is returning -ERESTART*.\n\t * So we don't clear it here, and only callers who know they should do.\n\t */\n\treturn 0;\n}\n\n/*\n * After recalculating TIF_SIGPENDING, we need to make sure the task wakes up.\n * This is superfluous when called on current, the wakeup is a harmless no-op.\n */\nvoid recalc_sigpending_and_wake(struct task_struct *t)\n{\n\tif (recalc_sigpending_tsk(t))\n\t\tsignal_wake_up(t, 0);\n}\n\nvoid recalc_sigpending(void)\n{\n\tif (!recalc_sigpending_tsk(current) && !freezing(current))\n\t\tclear_thread_flag(TIF_SIGPENDING);\n\n}\n\n/* Given the mask, find the first available signal that should be serviced. */\n\n#define SYNCHRONOUS_MASK \\\n\t(sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \\\n\t sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))\n\nint next_signal(struct sigpending *pending, sigset_t *mask)\n{\n\tunsigned long i, *s, *m, x;\n\tint sig = 0;\n\n\ts = pending->signal.sig;\n\tm = mask->sig;\n\n\t/*\n\t * Handle the first word specially: it contains the\n\t * synchronous signals that need to be dequeued first.\n\t */\n\tx = *s &~ *m;\n\tif (x) {\n\t\tif (x & SYNCHRONOUS_MASK)\n\t\t\tx &= SYNCHRONOUS_MASK;\n\t\tsig = ffz(~x) + 1;\n\t\treturn sig;\n\t}\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = 1; i < _NSIG_WORDS; ++i) {\n\t\t\tx = *++s &~ *++m;\n\t\t\tif (!x)\n\t\t\t\tcontinue;\n\t\t\tsig = ffz(~x) + i*_NSIG_BPW + 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 2:\n\t\tx = s[1] &~ m[1];\n\t\tif (!x)\n\t\t\tbreak;\n\t\tsig = ffz(~x) + _NSIG_BPW + 1;\n\t\tbreak;\n\n\tcase 1:\n\t\t/* Nothing to do */\n\t\tbreak;\n\t}\n\n\treturn sig;\n}\n\nstatic inline void print_dropped_signal(int sig)\n{\n\tstatic DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);\n\n\tif (!print_fatal_signals)\n\t\treturn;\n\n\tif (!__ratelimit(&ratelimit_state))\n\t\treturn;\n\n\tprintk(KERN_INFO \"%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\\n\",\n\t\t\t\tcurrent->comm, current->pid, sig);\n}\n\n/**\n * task_set_jobctl_pending - set jobctl pending bits\n * @task: target task\n * @mask: pending bits to set\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK | %JOBCTL_STOP_CONSUME | %JOBCTL_STOP_SIGMASK |\n * %JOBCTL_TRAPPING.  If stop signo is being set, the existing signo is\n * cleared.  If @task is already being killed or exiting, this function\n * becomes noop.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if @mask is set, %false if made noop because @task was dying.\n */\nbool task_set_jobctl_pending(struct task_struct *task, unsigned int mask)\n{\n\tBUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |\n\t\t\tJOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));\n\tBUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));\n\n\tif (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))\n\t\treturn false;\n\n\tif (mask & JOBCTL_STOP_SIGMASK)\n\t\ttask->jobctl &= ~JOBCTL_STOP_SIGMASK;\n\n\ttask->jobctl |= mask;\n\treturn true;\n}\n\n/**\n * task_clear_jobctl_trapping - clear jobctl trapping bit\n * @task: target task\n *\n * If JOBCTL_TRAPPING is set, a ptracer is waiting for us to enter TRACED.\n * Clear it and wake up the ptracer.  Note that we don't need any further\n * locking.  @task->siglock guarantees that @task->parent points to the\n * ptracer.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_trapping(struct task_struct *task)\n{\n\tif (unlikely(task->jobctl & JOBCTL_TRAPPING)) {\n\t\ttask->jobctl &= ~JOBCTL_TRAPPING;\n\t\twake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);\n\t}\n}\n\n/**\n * task_clear_jobctl_pending - clear jobctl pending bits\n * @task: target task\n * @mask: pending bits to clear\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK.  If %JOBCTL_STOP_PENDING is being cleared, other\n * STOP bits are cleared together.\n *\n * If clearing of @mask leaves no stop or trap pending, this function calls\n * task_clear_jobctl_trapping().\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_pending(struct task_struct *task, unsigned int mask)\n{\n\tBUG_ON(mask & ~JOBCTL_PENDING_MASK);\n\n\tif (mask & JOBCTL_STOP_PENDING)\n\t\tmask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;\n\n\ttask->jobctl &= ~mask;\n\n\tif (!(task->jobctl & JOBCTL_PENDING_MASK))\n\t\ttask_clear_jobctl_trapping(task);\n}\n\n/**\n * task_participate_group_stop - participate in a group stop\n * @task: task participating in a group stop\n *\n * @task has %JOBCTL_STOP_PENDING set and is participating in a group stop.\n * Group stop states are cleared and the group stop count is consumed if\n * %JOBCTL_STOP_CONSUME was set.  If the consumption completes the group\n * stop, the appropriate %SIGNAL_* flags are set.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if group stop completion should be notified to the parent, %false\n * otherwise.\n */\nstatic bool task_participate_group_stop(struct task_struct *task)\n{\n\tstruct signal_struct *sig = task->signal;\n\tbool consume = task->jobctl & JOBCTL_STOP_CONSUME;\n\n\tWARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));\n\n\ttask_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);\n\n\tif (!consume)\n\t\treturn false;\n\n\tif (!WARN_ON_ONCE(sig->group_stop_count == 0))\n\t\tsig->group_stop_count--;\n\n\t/*\n\t * Tell the caller to notify completion iff we are entering into a\n\t * fresh group stop.  Read comment in do_signal_stop() for details.\n\t */\n\tif (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {\n\t\tsig->flags = SIGNAL_STOP_STOPPED;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * allocate a new signal queue record\n * - this may be called without locks if and only if t == current, otherwise an\n *   appropriate lock must be held to stop the target task from exiting\n */\nstatic struct sigqueue *\n__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)\n{\n\tstruct sigqueue *q = NULL;\n\tstruct user_struct *user;\n\n\t/*\n\t * Protect access to @t credentials. This can go away when all\n\t * callers hold rcu read lock.\n\t */\n\trcu_read_lock();\n\tuser = get_uid(__task_cred(t)->user);\n\tatomic_inc(&user->sigpending);\n\trcu_read_unlock();\n\n\tif (override_rlimit ||\n\t    atomic_read(&user->sigpending) <=\n\t\t\ttask_rlimit(t, RLIMIT_SIGPENDING)) {\n\t\tq = kmem_cache_alloc(sigqueue_cachep, flags);\n\t} else {\n\t\tprint_dropped_signal(sig);\n\t}\n\n\tif (unlikely(q == NULL)) {\n\t\tatomic_dec(&user->sigpending);\n\t\tfree_uid(user);\n\t} else {\n\t\tINIT_LIST_HEAD(&q->list);\n\t\tq->flags = 0;\n\t\tq->user = user;\n\t}\n\n\treturn q;\n}\n\nstatic void __sigqueue_free(struct sigqueue *q)\n{\n\tif (q->flags & SIGQUEUE_PREALLOC)\n\t\treturn;\n\tatomic_dec(&q->user->sigpending);\n\tfree_uid(q->user);\n\tkmem_cache_free(sigqueue_cachep, q);\n}\n\nvoid flush_sigqueue(struct sigpending *queue)\n{\n\tstruct sigqueue *q;\n\n\tsigemptyset(&queue->signal);\n\twhile (!list_empty(&queue->list)) {\n\t\tq = list_entry(queue->list.next, struct sigqueue , list);\n\t\tlist_del_init(&q->list);\n\t\t__sigqueue_free(q);\n\t}\n}\n\n/*\n * Flush all pending signals for a task.\n */\nvoid __flush_signals(struct task_struct *t)\n{\n\tclear_tsk_thread_flag(t, TIF_SIGPENDING);\n\tflush_sigqueue(&t->pending);\n\tflush_sigqueue(&t->signal->shared_pending);\n}\n\nvoid flush_signals(struct task_struct *t)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\t__flush_signals(t);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n}\n\nstatic void __flush_itimer_signals(struct sigpending *pending)\n{\n\tsigset_t signal, retain;\n\tstruct sigqueue *q, *n;\n\n\tsignal = pending->signal;\n\tsigemptyset(&retain);\n\n\tlist_for_each_entry_safe(q, n, &pending->list, list) {\n\t\tint sig = q->info.si_signo;\n\n\t\tif (likely(q->info.si_code != SI_TIMER)) {\n\t\t\tsigaddset(&retain, sig);\n\t\t} else {\n\t\t\tsigdelset(&signal, sig);\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\n\tsigorsets(&pending->signal, &signal, &retain);\n}\n\nvoid flush_itimer_signals(void)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\n\t__flush_itimer_signals(&tsk->pending);\n\t__flush_itimer_signals(&tsk->signal->shared_pending);\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n}\n\nvoid ignore_signals(struct task_struct *t)\n{\n\tint i;\n\n\tfor (i = 0; i < _NSIG; ++i)\n\t\tt->sighand->action[i].sa.sa_handler = SIG_IGN;\n\n\tflush_signals(t);\n}\n\n/*\n * Flush all handlers for a task.\n */\n\nvoid\nflush_signal_handlers(struct task_struct *t, int force_default)\n{\n\tint i;\n\tstruct k_sigaction *ka = &t->sighand->action[0];\n\tfor (i = _NSIG ; i != 0 ; i--) {\n\t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n\t\t\tka->sa.sa_handler = SIG_DFL;\n\t\tka->sa.sa_flags = 0;\n\t\tsigemptyset(&ka->sa.sa_mask);\n\t\tka++;\n\t}\n}\n\nint unhandled_signal(struct task_struct *tsk, int sig)\n{\n\tvoid __user *handler = tsk->sighand->action[sig-1].sa.sa_handler;\n\tif (is_global_init(tsk))\n\t\treturn 1;\n\tif (handler != SIG_IGN && handler != SIG_DFL)\n\t\treturn 0;\n\t/* if ptraced, let the tracer determine */\n\treturn !tsk->ptrace;\n}\n\n/*\n * Notify the system that a driver wants to block all signals for this\n * process, and wants to be notified if any signals at all were to be\n * sent/acted upon.  If the notifier routine returns non-zero, then the\n * signal will be acted upon after all.  If the notifier routine returns 0,\n * then then signal will be blocked.  Only one block per process is\n * allowed.  priv is a pointer to private data that the notifier routine\n * can use to determine if the signal should be blocked or not.\n */\nvoid\nblock_all_signals(int (*notifier)(void *priv), void *priv, sigset_t *mask)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\tcurrent->notifier_mask = mask;\n\tcurrent->notifier_data = priv;\n\tcurrent->notifier = notifier;\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\n/* Notify the system that blocking has ended. */\n\nvoid\nunblock_all_signals(void)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\tcurrent->notifier = NULL;\n\tcurrent->notifier_data = NULL;\n\trecalc_sigpending();\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\nstatic void collect_signal(int sig, struct sigpending *list, siginfo_t *info)\n{\n\tstruct sigqueue *q, *first = NULL;\n\n\t/*\n\t * Collect the siginfo appropriate to this signal.  Check if\n\t * there is another siginfo for the same signal.\n\t*/\n\tlist_for_each_entry(q, &list->list, list) {\n\t\tif (q->info.si_signo == sig) {\n\t\t\tif (first)\n\t\t\t\tgoto still_pending;\n\t\t\tfirst = q;\n\t\t}\n\t}\n\n\tsigdelset(&list->signal, sig);\n\n\tif (first) {\nstill_pending:\n\t\tlist_del_init(&first->list);\n\t\tcopy_siginfo(info, &first->info);\n\t\t__sigqueue_free(first);\n\t} else {\n\t\t/*\n\t\t * Ok, it wasn't in the queue.  This must be\n\t\t * a fast-pathed signal or we must have been\n\t\t * out of queue space.  So zero out the info.\n\t\t */\n\t\tinfo->si_signo = sig;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\tinfo->si_pid = 0;\n\t\tinfo->si_uid = 0;\n\t}\n}\n\nstatic int __dequeue_signal(struct sigpending *pending, sigset_t *mask,\n\t\t\tsiginfo_t *info)\n{\n\tint sig = next_signal(pending, mask);\n\n\tif (sig) {\n\t\tif (current->notifier) {\n\t\t\tif (sigismember(current->notifier_mask, sig)) {\n\t\t\t\tif (!(current->notifier)(current->notifier_data)) {\n\t\t\t\t\tclear_thread_flag(TIF_SIGPENDING);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcollect_signal(sig, pending, info);\n\t}\n\n\treturn sig;\n}\n\n/*\n * Dequeue a signal and return the element to the caller, which is\n * expected to free it.\n *\n * All callers have to hold the siglock.\n */\nint dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)\n{\n\tint signr;\n\n\t/* We only dequeue private signals from ourselves, we don't let\n\t * signalfd steal them\n\t */\n\tsignr = __dequeue_signal(&tsk->pending, mask, info);\n\tif (!signr) {\n\t\tsignr = __dequeue_signal(&tsk->signal->shared_pending,\n\t\t\t\t\t mask, info);\n\t\t/*\n\t\t * itimer signal ?\n\t\t *\n\t\t * itimers are process shared and we restart periodic\n\t\t * itimers in the signal delivery path to prevent DoS\n\t\t * attacks in the high resolution timer case. This is\n\t\t * compliant with the old way of self-restarting\n\t\t * itimers, as the SIGALRM is a legacy signal and only\n\t\t * queued once. Changing the restart behaviour to\n\t\t * restart the timer in the signal dequeue path is\n\t\t * reducing the timer noise on heavy loaded !highres\n\t\t * systems too.\n\t\t */\n\t\tif (unlikely(signr == SIGALRM)) {\n\t\t\tstruct hrtimer *tmr = &tsk->signal->real_timer;\n\n\t\t\tif (!hrtimer_is_queued(tmr) &&\n\t\t\t    tsk->signal->it_real_incr.tv64 != 0) {\n\t\t\t\thrtimer_forward(tmr, tmr->base->get_time(),\n\t\t\t\t\t\ttsk->signal->it_real_incr);\n\t\t\t\thrtimer_restart(tmr);\n\t\t\t}\n\t\t}\n\t}\n\n\trecalc_sigpending();\n\tif (!signr)\n\t\treturn 0;\n\n\tif (unlikely(sig_kernel_stop(signr))) {\n\t\t/*\n\t\t * Set a marker that we have dequeued a stop signal.  Our\n\t\t * caller might release the siglock and then the pending\n\t\t * stop signal it is about to process is no longer in the\n\t\t * pending bitmasks, but must still be cleared by a SIGCONT\n\t\t * (and overruled by a SIGKILL).  So those cases clear this\n\t\t * shared flag after we've set it.  Note that this flag may\n\t\t * remain set after the signal we return is ignored or\n\t\t * handled.  That doesn't matter because its only purpose\n\t\t * is to alert stop-signal processing code when another\n\t\t * processor has come along and cleared the flag.\n\t\t */\n\t\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\t}\n\tif ((info->si_code & __SI_MASK) == __SI_TIMER && info->si_sys_private) {\n\t\t/*\n\t\t * Release the siglock to ensure proper locking order\n\t\t * of timer locks outside of siglocks.  Note, we leave\n\t\t * irqs disabled here, since the posix-timers code is\n\t\t * about to disable them again anyway.\n\t\t */\n\t\tspin_unlock(&tsk->sighand->siglock);\n\t\tdo_schedule_next_timer(info);\n\t\tspin_lock(&tsk->sighand->siglock);\n\t}\n\treturn signr;\n}\n\n/*\n * Tell a process that it has a new active signal..\n *\n * NOTE! we rely on the previous spin_lock to\n * lock interrupts for us! We can only be called with\n * \"siglock\" held, and the local interrupt must\n * have been disabled when that got acquired!\n *\n * No need to set need_resched since signal event passing\n * goes through ->blocked\n */\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}\n\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n *\n * This version takes a sigset mask and looks at all signals,\n * not just those in the first mask word.\n */\nstatic int rm_from_queue_full(sigset_t *mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\tsigset_t m;\n\n\tsigandsets(&m, mask, &s->signal);\n\tif (sigisemptyset(&m))\n\t\treturn 0;\n\n\tsigandnsets(&s->signal, &s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (sigismember(mask, q->info.si_signo)) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\treturn 1;\n}\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n */\nstatic int rm_from_queue(unsigned long mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\n\tif (!sigtestsetmask(&s->signal, mask))\n\t\treturn 0;\n\n\tsigdelsetmask(&s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (q->info.si_signo < SIGRTMIN &&\n\t\t    (mask & sigmask(q->info.si_signo))) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic inline int is_si_special(const struct siginfo *info)\n{\n\treturn info <= SEND_SIG_FORCED;\n}\n\nstatic inline bool si_fromuser(const struct siginfo *info)\n{\n\treturn info == SEND_SIG_NOINFO ||\n\t\t(!is_si_special(info) && SI_FROMUSER(info));\n}\n\n/*\n * called with RCU read lock from check_kill_permission()\n */\nstatic int kill_ok_by_cred(struct task_struct *t)\n{\n\tconst struct cred *cred = current_cred();\n\tconst struct cred *tcred = __task_cred(t);\n\n\tif (uid_eq(cred->euid, tcred->suid) ||\n\t    uid_eq(cred->euid, tcred->uid)  ||\n\t    uid_eq(cred->uid,  tcred->suid) ||\n\t    uid_eq(cred->uid,  tcred->uid))\n\t\treturn 1;\n\n\tif (ns_capable(tcred->user_ns, CAP_KILL))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n/*\n * Bad permissions for sending the signal\n * - the caller must hold the RCU read lock\n */\nstatic int check_kill_permission(int sig, struct siginfo *info,\n\t\t\t\t struct task_struct *t)\n{\n\tstruct pid *sid;\n\tint error;\n\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\tif (!si_fromuser(info))\n\t\treturn 0;\n\n\terror = audit_signal_info(sig, t); /* Let audit system see the signal */\n\tif (error)\n\t\treturn error;\n\n\tif (!same_thread_group(current, t) &&\n\t    !kill_ok_by_cred(t)) {\n\t\tswitch (sig) {\n\t\tcase SIGCONT:\n\t\t\tsid = task_session(t);\n\t\t\t/*\n\t\t\t * We don't return the error if sid == NULL. The\n\t\t\t * task was unhashed, the caller must notice this.\n\t\t\t */\n\t\t\tif (!sid || sid == task_session(current))\n\t\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n\n\treturn security_task_kill(t, info, sig, 0);\n}\n\n/**\n * ptrace_trap_notify - schedule trap to notify ptracer\n * @t: tracee wanting to notify tracer\n *\n * This function schedules sticky ptrace trap which is cleared on the next\n * TRAP_STOP to notify ptracer of an event.  @t must have been seized by\n * ptracer.\n *\n * If @t is running, STOP trap will be taken.  If trapped for STOP and\n * ptracer is listening for events, tracee is woken up so that it can\n * re-trap for the new event.  If trapped otherwise, STOP trap will be\n * eventually taken without returning to userland after the existing traps\n * are finished by PTRACE_CONT.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nstatic void ptrace_trap_notify(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n\tassert_spin_locked(&t->sighand->siglock);\n\n\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}\n\n/*\n * Handle magic process-wide effects of stop/continue signals. Unlike\n * the signal actions, these happen immediately at signal-generation\n * time regardless of blocking, ignoring, or handling.  This does the\n * actual continuing for SIGCONT, but not the actual stopping for stop\n * signals. The process stop is done as a signal action for SIG_DFL.\n *\n * Returns true if the signal should be actually delivered, otherwise\n * it should be dropped.\n */\nstatic int prepare_signal(int sig, struct task_struct *p, bool force)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\tif (unlikely(signal->flags & SIGNAL_GROUP_EXIT)) {\n\t\t/*\n\t\t * The process is in the middle of dying, nothing to do.\n\t\t */\n\t} else if (sig_kernel_stop(sig)) {\n\t\t/*\n\t\t * This is a stop signal.  Remove SIGCONT from all queues.\n\t\t */\n\t\trm_from_queue(sigmask(SIGCONT), &signal->shared_pending);\n\t\tt = p;\n\t\tdo {\n\t\t\trm_from_queue(sigmask(SIGCONT), &t->pending);\n\t\t} while_each_thread(p, t);\n\t} else if (sig == SIGCONT) {\n\t\tunsigned int why;\n\t\t/*\n\t\t * Remove all stop signals from all queues, wake all threads.\n\t\t */\n\t\trm_from_queue(SIG_KERNEL_STOP_MASK, &signal->shared_pending);\n\t\tt = p;\n\t\tdo {\n\t\t\ttask_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);\n\t\t\trm_from_queue(SIG_KERNEL_STOP_MASK, &t->pending);\n\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\twake_up_state(t, __TASK_STOPPED);\n\t\t\telse\n\t\t\t\tptrace_trap_notify(t);\n\t\t} while_each_thread(p, t);\n\n\t\t/*\n\t\t * Notify the parent with CLD_CONTINUED if we were stopped.\n\t\t *\n\t\t * If we were in the middle of a group stop, we pretend it\n\t\t * was already finished, and then continued. Since SIGCHLD\n\t\t * doesn't queue we report only CLD_STOPPED, as if the next\n\t\t * CLD_CONTINUED was dropped.\n\t\t */\n\t\twhy = 0;\n\t\tif (signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\twhy |= SIGNAL_CLD_CONTINUED;\n\t\telse if (signal->group_stop_count)\n\t\t\twhy |= SIGNAL_CLD_STOPPED;\n\n\t\tif (why) {\n\t\t\t/*\n\t\t\t * The first thread which returns from do_signal_stop()\n\t\t\t * will take ->siglock, notice SIGNAL_CLD_MASK, and\n\t\t\t * notify its parent. See get_signal_to_deliver().\n\t\t\t */\n\t\t\tsignal->flags = why | SIGNAL_STOP_CONTINUED;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tsignal->group_exit_code = 0;\n\t\t}\n\t}\n\n\treturn !sig_ignored(p, sig, force);\n}\n\n/*\n * Test if P wants to take SIG.  After we've checked all threads with this,\n * it's equivalent to finding no threads not blocking SIG.  Any threads not\n * blocking SIG were ruled out because they are not running and already\n * have pending signals.  Such threads will dequeue from the shared queue\n * as soon as they're available, so putting the signal on the shared queue\n * will be equivalent to sending it to one such thread.\n */\nstatic inline int wants_signal(int sig, struct task_struct *p)\n{\n\tif (sigismember(&p->blocked, sig))\n\t\treturn 0;\n\tif (p->flags & PF_EXITING)\n\t\treturn 0;\n\tif (sig == SIGKILL)\n\t\treturn 1;\n\tif (task_is_stopped_or_traced(p))\n\t\treturn 0;\n\treturn task_curr(p) || !signal_pending(p);\n}\n\nstatic void complete_signal(int sig, struct task_struct *p, int group)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\t/*\n\t * Now find a thread we can wake up to take the signal off the queue.\n\t *\n\t * If the main thread wants the signal, it gets first crack.\n\t * Probably the least surprising to the average bear.\n\t */\n\tif (wants_signal(sig, p))\n\t\tt = p;\n\telse if (!group || thread_group_empty(p))\n\t\t/*\n\t\t * There is just one thread and it does not need to be woken.\n\t\t * It will dequeue unblocked signals before it runs again.\n\t\t */\n\t\treturn;\n\telse {\n\t\t/*\n\t\t * Otherwise try to find a suitable thread.\n\t\t */\n\t\tt = signal->curr_target;\n\t\twhile (!wants_signal(sig, t)) {\n\t\t\tt = next_thread(t);\n\t\t\tif (t == signal->curr_target)\n\t\t\t\t/*\n\t\t\t\t * No thread needs to be woken.\n\t\t\t\t * Any eligible threads will see\n\t\t\t\t * the signal in the queue soon.\n\t\t\t\t */\n\t\t\t\treturn;\n\t\t}\n\t\tsignal->curr_target = t;\n\t}\n\n\t/*\n\t * Found a killable thread.  If the signal will be fatal,\n\t * then start taking the whole group down immediately.\n\t */\n\tif (sig_fatal(p, sig) &&\n\t    !(signal->flags & (SIGNAL_UNKILLABLE | SIGNAL_GROUP_EXIT)) &&\n\t    !sigismember(&t->real_blocked, sig) &&\n\t    (sig == SIGKILL || !t->ptrace)) {\n\t\t/*\n\t\t * This signal will be fatal to the whole group.\n\t\t */\n\t\tif (!sig_kernel_coredump(sig)) {\n\t\t\t/*\n\t\t\t * Start a group exit and wake everybody up.\n\t\t\t * This way we don't have other threads\n\t\t\t * running and doing things after a slower\n\t\t\t * thread has the fatal signal pending.\n\t\t\t */\n\t\t\tsignal->flags = SIGNAL_GROUP_EXIT;\n\t\t\tsignal->group_exit_code = sig;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tt = p;\n\t\t\tdo {\n\t\t\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\t\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\t\t\tsignal_wake_up(t, 1);\n\t\t\t} while_each_thread(p, t);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * The signal is already in the shared-pending queue.\n\t * Tell the chosen thread to wake up and dequeue it.\n\t */\n\tsignal_wake_up(t, sig == SIGKILL);\n\treturn;\n}\n\nstatic inline int legacy_queue(struct sigpending *signals, int sig)\n{\n\treturn (sig < SIGRTMIN) && sigismember(&signals->signal, sig);\n}\n\n#ifdef CONFIG_USER_NS\nstatic inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)\n{\n\tif (current_user_ns() == task_cred_xxx(t, user_ns))\n\t\treturn;\n\n\tif (SI_FROMKERNEL(info))\n\t\treturn;\n\n\trcu_read_lock();\n\tinfo->si_uid = from_kuid_munged(task_cred_xxx(t, user_ns),\n\t\t\t\t\tmake_kuid(current_user_ns(), info->si_uid));\n\trcu_read_unlock();\n}\n#else\nstatic inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)\n{\n\treturn;\n}\n#endif\n\nstatic int __send_signal(int sig, struct siginfo *info, struct task_struct *t,\n\t\t\tint group, int from_ancestor_ns)\n{\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint override_rlimit;\n\tint ret = 0, result;\n\n\tassert_spin_locked(&t->sighand->siglock);\n\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t,\n\t\t\tfrom_ancestor_ns || (info == SEND_SIG_FORCED)))\n\t\tgoto ret;\n\n\tpending = group ? &t->signal->shared_pending : &t->pending;\n\t/*\n\t * Short-circuit ignored signals and support queuing\n\t * exactly one non-rt signal, so that we can get more\n\t * detailed information about the cause of the signal.\n\t */\n\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\tif (legacy_queue(pending, sig))\n\t\tgoto ret;\n\n\tresult = TRACE_SIGNAL_DELIVERED;\n\t/*\n\t * fast-pathed signals for kernel-internal things like SIGSTOP\n\t * or SIGKILL.\n\t */\n\tif (info == SEND_SIG_FORCED)\n\t\tgoto out_set;\n\n\t/*\n\t * Real-time signals must be queued if sent by sigqueue, or\n\t * some other real-time mechanism.  It is implementation\n\t * defined whether kill() does so.  We attempt to do so, on\n\t * the principle of least surprise, but since kill is not\n\t * allowed to fail with EAGAIN when low on memory we just\n\t * make sure at least one signal gets delivered and don't\n\t * pass on the info struct.\n\t */\n\tif (sig < SIGRTMIN)\n\t\toverride_rlimit = (is_si_special(info) || info->si_code >= 0);\n\telse\n\t\toverride_rlimit = 0;\n\n\tq = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE,\n\t\toverride_rlimit);\n\tif (q) {\n\t\tlist_add_tail(&q->list, &pending->list);\n\t\tswitch ((unsigned long) info) {\n\t\tcase (unsigned long) SEND_SIG_NOINFO:\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_USER;\n\t\t\tq->info.si_pid = task_tgid_nr_ns(current,\n\t\t\t\t\t\t\ttask_active_pid_ns(t));\n\t\t\tq->info.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\t\t\tbreak;\n\t\tcase (unsigned long) SEND_SIG_PRIV:\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_KERNEL;\n\t\t\tq->info.si_pid = 0;\n\t\t\tq->info.si_uid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcopy_siginfo(&q->info, info);\n\t\t\tif (from_ancestor_ns)\n\t\t\t\tq->info.si_pid = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tuserns_fixup_signal_uid(&q->info, t);\n\n\t} else if (!is_si_special(info)) {\n\t\tif (sig >= SIGRTMIN && info->si_code != SI_USER) {\n\t\t\t/*\n\t\t\t * Queue overflow, abort.  We may abort if the\n\t\t\t * signal was rt and sent by user using something\n\t\t\t * other than kill().\n\t\t\t */\n\t\t\tresult = TRACE_SIGNAL_OVERFLOW_FAIL;\n\t\t\tret = -EAGAIN;\n\t\t\tgoto ret;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is a silent loss of information.  We still\n\t\t\t * send the signal, but the *info bits are lost.\n\t\t\t */\n\t\t\tresult = TRACE_SIGNAL_LOSE_INFO;\n\t\t}\n\t}\n\nout_set:\n\tsignalfd_notify(t, sig);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, group);\nret:\n\ttrace_signal_generate(sig, info, t, group, result);\n\treturn ret;\n}\n\nstatic int send_signal(int sig, struct siginfo *info, struct task_struct *t,\n\t\t\tint group)\n{\n\tint from_ancestor_ns = 0;\n\n#ifdef CONFIG_PID_NS\n\tfrom_ancestor_ns = si_fromuser(info) &&\n\t\t\t   !task_pid_nr_ns(current, task_active_pid_ns(t));\n#endif\n\n\treturn __send_signal(sig, info, t, group, from_ancestor_ns);\n}\n\nstatic void print_fatal_signal(int signr)\n{\n\tstruct pt_regs *regs = signal_pt_regs();\n\tprintk(KERN_INFO \"%s/%d: potentially unexpected fatal signal %d.\\n\",\n\t\tcurrent->comm, task_pid_nr(current), signr);\n\n#if defined(__i386__) && !defined(__arch_um__)\n\tprintk(KERN_INFO \"code at %08lx: \", regs->ip);\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < 16; i++) {\n\t\t\tunsigned char insn;\n\n\t\t\tif (get_user(insn, (unsigned char *)(regs->ip + i)))\n\t\t\t\tbreak;\n\t\t\tprintk(KERN_CONT \"%02x \", insn);\n\t\t}\n\t}\n\tprintk(KERN_CONT \"\\n\");\n#endif\n\tpreempt_disable();\n\tshow_regs(regs);\n\tpreempt_enable();\n}\n\nstatic int __init setup_print_fatal_signals(char *str)\n{\n\tget_option (&str, &print_fatal_signals);\n\n\treturn 1;\n}\n\n__setup(\"print-fatal-signals=\", setup_print_fatal_signals);\n\nint\n__group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\treturn send_signal(sig, info, p, 1);\n}\n\nstatic int\nspecific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)\n{\n\treturn send_signal(sig, info, t, 0);\n}\n\nint do_send_sig_info(int sig, struct siginfo *info, struct task_struct *p,\n\t\t\tbool group)\n{\n\tunsigned long flags;\n\tint ret = -ESRCH;\n\n\tif (lock_task_sighand(p, &flags)) {\n\t\tret = send_signal(sig, info, p, group);\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Force a signal that the process can't ignore: if necessary\n * we unblock the signal and change any SIG_IGN to SIG_DFL.\n *\n * Note: If we unblock the signal, we always reset it to SIG_DFL,\n * since we do not want to have a signal handler that was blocked\n * be invoked when user space had explicitly blocked it.\n *\n * We don't want to have recursive SIGSEGV's etc, for example,\n * that is why we also clear SIGNAL_UNKILLABLE.\n */\nint\nforce_sig_info(int sig, struct siginfo *info, struct task_struct *t)\n{\n\tunsigned long int flags;\n\tint ret, blocked, ignored;\n\tstruct k_sigaction *action;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\taction = &t->sighand->action[sig-1];\n\tignored = action->sa.sa_handler == SIG_IGN;\n\tblocked = sigismember(&t->blocked, sig);\n\tif (blocked || ignored) {\n\t\taction->sa.sa_handler = SIG_DFL;\n\t\tif (blocked) {\n\t\t\tsigdelset(&t->blocked, sig);\n\t\t\trecalc_sigpending_and_wake(t);\n\t\t}\n\t}\n\tif (action->sa.sa_handler == SIG_DFL)\n\t\tt->signal->flags &= ~SIGNAL_UNKILLABLE;\n\tret = specific_send_sig_info(sig, info, t);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n\n\treturn ret;\n}\n\n/*\n * Nuke all other threads in the group.\n */\nint zap_other_threads(struct task_struct *p)\n{\n\tstruct task_struct *t = p;\n\tint count = 0;\n\n\tp->signal->group_stop_count = 0;\n\n\twhile_each_thread(p, t) {\n\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\tcount++;\n\n\t\t/* Don't bother with already dead threads */\n\t\tif (t->exit_state)\n\t\t\tcontinue;\n\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\tsignal_wake_up(t, 1);\n\t}\n\n\treturn count;\n}\n\nstruct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t   unsigned long *flags)\n{\n\tstruct sighand_struct *sighand;\n\n\tfor (;;) {\n\t\tlocal_irq_save(*flags);\n\t\trcu_read_lock();\n\t\tsighand = rcu_dereference(tsk->sighand);\n\t\tif (unlikely(sighand == NULL)) {\n\t\t\trcu_read_unlock();\n\t\t\tlocal_irq_restore(*flags);\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_lock(&sighand->siglock);\n\t\tif (likely(sighand == tsk->sighand)) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&sighand->siglock);\n\t\trcu_read_unlock();\n\t\tlocal_irq_restore(*flags);\n\t}\n\n\treturn sighand;\n}\n\n/*\n * send signal info to all the members of a group\n */\nint group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = check_kill_permission(sig, info, p);\n\trcu_read_unlock();\n\n\tif (!ret && sig)\n\t\tret = do_send_sig_info(sig, info, p, true);\n\n\treturn ret;\n}\n\n/*\n * __kill_pgrp_info() sends a signal to a process group: this is what the tty\n * control characters do (^C, ^Z etc)\n * - the caller must hold at least a readlock on tasklist_lock\n */\nint __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp)\n{\n\tstruct task_struct *p = NULL;\n\tint retval, success;\n\n\tsuccess = 0;\n\tretval = -ESRCH;\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tint err = group_send_sig_info(sig, info, p);\n\t\tsuccess |= !err;\n\t\tretval = err;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\treturn success ? 0 : retval;\n}\n\nint kill_pid_info(int sig, struct siginfo *info, struct pid *pid)\n{\n\tint error = -ESRCH;\n\tstruct task_struct *p;\n\n\trcu_read_lock();\nretry:\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (p) {\n\t\terror = group_send_sig_info(sig, info, p);\n\t\tif (unlikely(error == -ESRCH))\n\t\t\t/*\n\t\t\t * The task was unhashed in between, try again.\n\t\t\t * If it is dead, pid_task() will return NULL,\n\t\t\t * if we race with de_thread() it will find the\n\t\t\t * new leader.\n\t\t\t */\n\t\t\tgoto retry;\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nint kill_proc_info(int sig, struct siginfo *info, pid_t pid)\n{\n\tint error;\n\trcu_read_lock();\n\terror = kill_pid_info(sig, info, find_vpid(pid));\n\trcu_read_unlock();\n\treturn error;\n}\n\nstatic int kill_as_cred_perm(const struct cred *cred,\n\t\t\t     struct task_struct *target)\n{\n\tconst struct cred *pcred = __task_cred(target);\n\tif (!uid_eq(cred->euid, pcred->suid) && !uid_eq(cred->euid, pcred->uid) &&\n\t    !uid_eq(cred->uid,  pcred->suid) && !uid_eq(cred->uid,  pcred->uid))\n\t\treturn 0;\n\treturn 1;\n}\n\n/* like kill_pid_info(), but doesn't use uid/euid of \"current\" */\nint kill_pid_info_as_cred(int sig, struct siginfo *info, struct pid *pid,\n\t\t\t const struct cred *cred, u32 secid)\n{\n\tint ret = -EINVAL;\n\tstruct task_struct *p;\n\tunsigned long flags;\n\n\tif (!valid_signal(sig))\n\t\treturn ret;\n\n\trcu_read_lock();\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (!p) {\n\t\tret = -ESRCH;\n\t\tgoto out_unlock;\n\t}\n\tif (si_fromuser(info) && !kill_as_cred_perm(cred, p)) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\tret = security_task_kill(p, info, sig, secid);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tif (sig) {\n\t\tif (lock_task_sighand(p, &flags)) {\n\t\t\tret = __send_signal(sig, info, p, 1, 0);\n\t\t\tunlock_task_sighand(p, &flags);\n\t\t} else\n\t\t\tret = -ESRCH;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kill_pid_info_as_cred);\n\n/*\n * kill_something_info() interprets pid in interesting ways just like kill(2).\n *\n * POSIX specifies that kill(-1,sig) is unspecified, but what we have\n * is probably wrong.  Should make it like BSD or SYSV.\n */\n\nstatic int kill_something_info(int sig, struct siginfo *info, pid_t pid)\n{\n\tint ret;\n\n\tif (pid > 0) {\n\t\trcu_read_lock();\n\t\tret = kill_pid_info(sig, info, find_vpid(pid));\n\t\trcu_read_unlock();\n\t\treturn ret;\n\t}\n\n\tread_lock(&tasklist_lock);\n\tif (pid != -1) {\n\t\tret = __kill_pgrp_info(sig, info,\n\t\t\t\tpid ? find_vpid(-pid) : task_pgrp(current));\n\t} else {\n\t\tint retval = 0, count = 0;\n\t\tstruct task_struct * p;\n\n\t\tfor_each_process(p) {\n\t\t\tif (task_pid_vnr(p) > 1 &&\n\t\t\t\t\t!same_thread_group(p, current)) {\n\t\t\t\tint err = group_send_sig_info(sig, info, p);\n\t\t\t\t++count;\n\t\t\t\tif (err != -EPERM)\n\t\t\t\t\tretval = err;\n\t\t\t}\n\t\t}\n\t\tret = count ? retval : -ESRCH;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * These are for backward compatibility with the rest of the kernel source.\n */\n\nint send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\t/*\n\t * Make sure legacy kernel users don't send in bad values\n\t * (normal paths check this in check_kill_permission).\n\t */\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\treturn do_send_sig_info(sig, info, p, false);\n}\n\n#define __si_special(priv) \\\n\t((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)\n\nint\nsend_sig(int sig, struct task_struct *p, int priv)\n{\n\treturn send_sig_info(sig, __si_special(priv), p);\n}\n\nvoid\nforce_sig(int sig, struct task_struct *p)\n{\n\tforce_sig_info(sig, SEND_SIG_PRIV, p);\n}\n\n/*\n * When things go south during signal handling, we\n * will force a SIGSEGV. And if the signal that caused\n * the problem was already a SIGSEGV, we'll want to\n * make sure we don't even try to deliver the signal..\n */\nint\nforce_sigsegv(int sig, struct task_struct *p)\n{\n\tif (sig == SIGSEGV) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&p->sighand->siglock, flags);\n\t\tp->sighand->action[sig - 1].sa.sa_handler = SIG_DFL;\n\t\tspin_unlock_irqrestore(&p->sighand->siglock, flags);\n\t}\n\tforce_sig(SIGSEGV, p);\n\treturn 0;\n}\n\nint kill_pgrp(struct pid *pid, int sig, int priv)\n{\n\tint ret;\n\n\tread_lock(&tasklist_lock);\n\tret = __kill_pgrp_info(sig, __si_special(priv), pid);\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kill_pgrp);\n\nint kill_pid(struct pid *pid, int sig, int priv)\n{\n\treturn kill_pid_info(sig, __si_special(priv), pid);\n}\nEXPORT_SYMBOL(kill_pid);\n\n/*\n * These functions support sending signals using preallocated sigqueue\n * structures.  This is needed \"because realtime applications cannot\n * afford to lose notifications of asynchronous events, like timer\n * expirations or I/O completions\".  In the case of POSIX Timers\n * we allocate the sigqueue structure from the timer_create.  If this\n * allocation fails we are able to report the failure to the application\n * with an EAGAIN error.\n */\nstruct sigqueue *sigqueue_alloc(void)\n{\n\tstruct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);\n\n\tif (q)\n\t\tq->flags |= SIGQUEUE_PREALLOC;\n\n\treturn q;\n}\n\nvoid sigqueue_free(struct sigqueue *q)\n{\n\tunsigned long flags;\n\tspinlock_t *lock = &current->sighand->siglock;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\t/*\n\t * We must hold ->siglock while testing q->list\n\t * to serialize with collect_signal() or with\n\t * __exit_signal()->flush_sigqueue().\n\t */\n\tspin_lock_irqsave(lock, flags);\n\tq->flags &= ~SIGQUEUE_PREALLOC;\n\t/*\n\t * If it is queued it will be freed when dequeued,\n\t * like the \"regular\" sigqueue.\n\t */\n\tif (!list_empty(&q->list))\n\t\tq = NULL;\n\tspin_unlock_irqrestore(lock, flags);\n\n\tif (q)\n\t\t__sigqueue_free(q);\n}\n\nint send_sigqueue(struct sigqueue *q, struct task_struct *t, int group)\n{\n\tint sig = q->info.si_signo;\n\tstruct sigpending *pending;\n\tunsigned long flags;\n\tint ret, result;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\n\tret = -1;\n\tif (!likely(lock_task_sighand(t, &flags)))\n\t\tgoto ret;\n\n\tret = 1; /* the signal is ignored */\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t, false))\n\t\tgoto out;\n\n\tret = 0;\n\tif (unlikely(!list_empty(&q->list))) {\n\t\t/*\n\t\t * If an SI_TIMER entry is already queue just increment\n\t\t * the overrun count.\n\t\t */\n\t\tBUG_ON(q->info.si_code != SI_TIMER);\n\t\tq->info.si_overrun++;\n\t\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\t\tgoto out;\n\t}\n\tq->info.si_overrun = 0;\n\n\tsignalfd_notify(t, sig);\n\tpending = group ? &t->signal->shared_pending : &t->pending;\n\tlist_add_tail(&q->list, &pending->list);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, group);\n\tresult = TRACE_SIGNAL_DELIVERED;\nout:\n\ttrace_signal_generate(sig, &q->info, t, group, result);\n\tunlock_task_sighand(t, &flags);\nret:\n\treturn ret;\n}\n\n/*\n * Let a parent know about the death of a child.\n * For a stopped/continued status change, use do_notify_parent_cldstop instead.\n *\n * Returns true if our parent ignored us and so we've switched to\n * self-reaping.\n */\nbool do_notify_parent(struct task_struct *tsk, int sig)\n{\n\tstruct siginfo info;\n\tunsigned long flags;\n\tstruct sighand_struct *psig;\n\tbool autoreap = false;\n\tcputime_t utime, stime;\n\n\tBUG_ON(sig == -1);\n\n \t/* do_notify_parent_cldstop should have been called instead.  */\n \tBUG_ON(task_is_stopped_or_traced(tsk));\n\n\tBUG_ON(!tsk->ptrace &&\n\t       (tsk->group_leader != tsk || !thread_group_empty(tsk)));\n\n\tif (sig != SIGCHLD) {\n\t\t/*\n\t\t * This is only possible if parent == real_parent.\n\t\t * Check if it has changed security domain.\n\t\t */\n\t\tif (tsk->parent_exec_id != tsk->parent->self_exec_id)\n\t\t\tsig = SIGCHLD;\n\t}\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\t/*\n\t * We are under tasklist_lock here so our parent is tied to\n\t * us and cannot change.\n\t *\n\t * task_active_pid_ns will always return the same pid namespace\n\t * until a task passes through release_task.\n\t *\n\t * write_lock() currently calls preempt_disable() which is the\n\t * same as rcu_read_lock(), but according to Oleg, this is not\n\t * correct to rely on this\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),\n\t\t\t\t       task_uid(tsk));\n\trcu_read_unlock();\n\n\ttask_cputime(tsk, &utime, &stime);\n\tinfo.si_utime = cputime_to_clock_t(utime + tsk->signal->utime);\n\tinfo.si_stime = cputime_to_clock_t(stime + tsk->signal->stime);\n\n\tinfo.si_status = tsk->exit_code & 0x7f;\n\tif (tsk->exit_code & 0x80)\n\t\tinfo.si_code = CLD_DUMPED;\n\telse if (tsk->exit_code & 0x7f)\n\t\tinfo.si_code = CLD_KILLED;\n\telse {\n\t\tinfo.si_code = CLD_EXITED;\n\t\tinfo.si_status = tsk->exit_code >> 8;\n\t}\n\n\tpsig = tsk->parent->sighand;\n\tspin_lock_irqsave(&psig->siglock, flags);\n\tif (!tsk->ptrace && sig == SIGCHLD &&\n\t    (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||\n\t     (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {\n\t\t/*\n\t\t * We are exiting and our parent doesn't care.  POSIX.1\n\t\t * defines special semantics for setting SIGCHLD to SIG_IGN\n\t\t * or setting the SA_NOCLDWAIT flag: we should be reaped\n\t\t * automatically and not left for our parent's wait4 call.\n\t\t * Rather than having the parent do it as a magic kind of\n\t\t * signal handler, we just set this to tell do_exit that we\n\t\t * can be cleaned up without becoming a zombie.  Note that\n\t\t * we still call __wake_up_parent in this case, because a\n\t\t * blocked sys_wait4 might now return -ECHILD.\n\t\t *\n\t\t * Whether we send SIGCHLD or not for SA_NOCLDWAIT\n\t\t * is implementation-defined: we do (if you don't want\n\t\t * it, just use SIG_IGN instead).\n\t\t */\n\t\tautoreap = true;\n\t\tif (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)\n\t\t\tsig = 0;\n\t}\n\tif (valid_signal(sig) && sig)\n\t\t__group_send_sig_info(sig, &info, tsk->parent);\n\t__wake_up_parent(tsk, tsk->parent);\n\tspin_unlock_irqrestore(&psig->siglock, flags);\n\n\treturn autoreap;\n}\n\n/**\n * do_notify_parent_cldstop - notify parent of stopped/continued state change\n * @tsk: task reporting the state change\n * @for_ptracer: the notification is for ptracer\n * @why: CLD_{CONTINUED|STOPPED|TRAPPED} to report\n *\n * Notify @tsk's parent that the stopped/continued state has changed.  If\n * @for_ptracer is %false, @tsk's group leader notifies to its real parent.\n * If %true, @tsk reports to @tsk->parent which should be the ptracer.\n *\n * CONTEXT:\n * Must be called with tasklist_lock at least read locked.\n */\nstatic void do_notify_parent_cldstop(struct task_struct *tsk,\n\t\t\t\t     bool for_ptracer, int why)\n{\n\tstruct siginfo info;\n\tunsigned long flags;\n\tstruct task_struct *parent;\n\tstruct sighand_struct *sighand;\n\tcputime_t utime, stime;\n\n\tif (for_ptracer) {\n\t\tparent = tsk->parent;\n\t} else {\n\t\ttsk = tsk->group_leader;\n\t\tparent = tsk->real_parent;\n\t}\n\n\tinfo.si_signo = SIGCHLD;\n\tinfo.si_errno = 0;\n\t/*\n\t * see comment in do_notify_parent() about the following 4 lines\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));\n\trcu_read_unlock();\n\n\ttask_cputime(tsk, &utime, &stime);\n\tinfo.si_utime = cputime_to_clock_t(utime);\n\tinfo.si_stime = cputime_to_clock_t(stime);\n\n \tinfo.si_code = why;\n \tswitch (why) {\n \tcase CLD_CONTINUED:\n \t\tinfo.si_status = SIGCONT;\n \t\tbreak;\n \tcase CLD_STOPPED:\n \t\tinfo.si_status = tsk->signal->group_exit_code & 0x7f;\n \t\tbreak;\n \tcase CLD_TRAPPED:\n \t\tinfo.si_status = tsk->exit_code & 0x7f;\n \t\tbreak;\n \tdefault:\n \t\tBUG();\n \t}\n\n\tsighand = parent->sighand;\n\tspin_lock_irqsave(&sighand->siglock, flags);\n\tif (sighand->action[SIGCHLD-1].sa.sa_handler != SIG_IGN &&\n\t    !(sighand->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))\n\t\t__group_send_sig_info(SIGCHLD, &info, parent);\n\t/*\n\t * Even if SIGCHLD is not generated, we must wake up wait4 calls.\n\t */\n\t__wake_up_parent(tsk, parent);\n\tspin_unlock_irqrestore(&sighand->siglock, flags);\n}\n\nstatic inline int may_ptrace_stop(void)\n{\n\tif (!likely(current->ptrace))\n\t\treturn 0;\n\t/*\n\t * Are we in the middle of do_coredump?\n\t * If so and our tracer is also part of the coredump stopping\n\t * is a deadlock situation, and pointless because our tracer\n\t * is dead so don't allow us to stop.\n\t * If SIGKILL was already sent before the caller unlocked\n\t * ->siglock we must see ->core_state != NULL. Otherwise it\n\t * is safe to enter schedule().\n\t *\n\t * This is almost outdated, a task with the pending SIGKILL can't\n\t * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported\n\t * after SIGKILL was already dequeued.\n\t */\n\tif (unlikely(current->mm->core_state) &&\n\t    unlikely(current->mm == current->parent->mm))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * Return non-zero if there is a SIGKILL that should be waking us up.\n * Called with the siglock held.\n */\nstatic int sigkill_pending(struct task_struct *tsk)\n{\n\treturn\tsigismember(&tsk->pending.signal, SIGKILL) ||\n\t\tsigismember(&tsk->signal->shared_pending.signal, SIGKILL);\n}\n\n/*\n * This must be called with current->sighand->siglock held.\n *\n * This should be the path for all ptrace stops.\n * We always set current->last_siginfo while stopped here.\n * That makes it a way to test a stopped process for\n * being ptrace-stopped vs being job-control-stopped.\n *\n * If we actually decide not to stop at all because the tracer\n * is gone, we keep current->exit_code unless clear_code.\n */\nstatic void ptrace_stop(int exit_code, int why, int clear_code, siginfo_t *info)\n\t__releases(&current->sighand->siglock)\n\t__acquires(&current->sighand->siglock)\n{\n\tbool gstop_done = false;\n\n\tif (arch_ptrace_stop_needed(exit_code, info)) {\n\t\t/*\n\t\t * The arch code has something special to do before a\n\t\t * ptrace stop.  This is allowed to block, e.g. for faults\n\t\t * on user stack pages.  We can't keep the siglock while\n\t\t * calling arch_ptrace_stop, so we must release it now.\n\t\t * To preserve proper semantics, we must do this before\n\t\t * any signal bookkeeping like checking group_stop_count.\n\t\t * Meanwhile, a SIGKILL could come in before we retake the\n\t\t * siglock.  That must prevent us from sleeping in TASK_TRACED.\n\t\t * So after regaining the lock, we must check for SIGKILL.\n\t\t */\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tarch_ptrace_stop(exit_code, info);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\tif (sigkill_pending(current))\n\t\t\treturn;\n\t}\n\n\t/*\n\t * We're committing to trapping.  TRACED should be visible before\n\t * TRAPPING is cleared; otherwise, the tracer might fail do_wait().\n\t * Also, transition to TRACED and updates to ->jobctl should be\n\t * atomic with respect to siglock and should be done after the arch\n\t * hook as siglock is released and regrabbed across it.\n\t */\n\tset_current_state(TASK_TRACED);\n\n\tcurrent->last_siginfo = info;\n\tcurrent->exit_code = exit_code;\n\n\t/*\n\t * If @why is CLD_STOPPED, we're trapping to participate in a group\n\t * stop.  Do the bookkeeping.  Note that if SIGCONT was delievered\n\t * across siglock relocks since INTERRUPT was scheduled, PENDING\n\t * could be clear now.  We act as if SIGCONT is received after\n\t * TASK_TRACED is entered - ignore it.\n\t */\n\tif (why == CLD_STOPPED && (current->jobctl & JOBCTL_STOP_PENDING))\n\t\tgstop_done = task_participate_group_stop(current);\n\n\t/* any trap clears pending STOP trap, STOP trap clears NOTIFY */\n\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\tif (info && info->si_code >> 8 == PTRACE_EVENT_STOP)\n\t\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY);\n\n\t/* entering a trap, clear TRAPPING */\n\ttask_clear_jobctl_trapping(current);\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\tread_lock(&tasklist_lock);\n\tif (may_ptrace_stop()) {\n\t\t/*\n\t\t * Notify parents of the stop.\n\t\t *\n\t\t * While ptraced, there are two parents - the ptracer and\n\t\t * the real_parent of the group_leader.  The ptracer should\n\t\t * know about every stop while the real parent is only\n\t\t * interested in the completion of group stop.  The states\n\t\t * for the two don't interact with each other.  Notify\n\t\t * separately unless they're gonna be duplicates.\n\t\t */\n\t\tdo_notify_parent_cldstop(current, true, why);\n\t\tif (gstop_done && ptrace_reparented(current))\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/*\n\t\t * Don't want to allow preemption here, because\n\t\t * sys_ptrace() needs this task to be inactive.\n\t\t *\n\t\t * XXX: implement read_unlock_no_resched().\n\t\t */\n\t\tpreempt_disable();\n\t\tread_unlock(&tasklist_lock);\n\t\tpreempt_enable_no_resched();\n\t\tfreezable_schedule();\n\t} else {\n\t\t/*\n\t\t * By the time we got the lock, our tracer went away.\n\t\t * Don't drop the lock yet, another tracer may come.\n\t\t *\n\t\t * If @gstop_done, the ptracer went away between group stop\n\t\t * completion and here.  During detach, it would have set\n\t\t * JOBCTL_STOP_PENDING on us and we'll re-enter\n\t\t * TASK_STOPPED in do_signal_stop() on return, so notifying\n\t\t * the real parent of the group stop completion is enough.\n\t\t */\n\t\tif (gstop_done)\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/* tasklist protects us from ptrace_freeze_traced() */\n\t\t__set_current_state(TASK_RUNNING);\n\t\tif (clear_code)\n\t\t\tcurrent->exit_code = 0;\n\t\tread_unlock(&tasklist_lock);\n\t}\n\n\t/*\n\t * We are back.  Now reacquire the siglock before touching\n\t * last_siginfo, so that we are sure to have synchronized with\n\t * any signal-sending on another CPU that wants to examine it.\n\t */\n\tspin_lock_irq(&current->sighand->siglock);\n\tcurrent->last_siginfo = NULL;\n\n\t/* LISTENING can be set only during STOP traps, clear it */\n\tcurrent->jobctl &= ~JOBCTL_LISTENING;\n\n\t/*\n\t * Queued signals ignored us while we were stopped for tracing.\n\t * So check for any that we should take before resuming user mode.\n\t * This sets TIF_SIGPENDING, but never clears it.\n\t */\n\trecalc_sigpending_tsk(current);\n}\n\nstatic void ptrace_do_notify(int signr, int exit_code, int why)\n{\n\tsiginfo_t info;\n\n\tmemset(&info, 0, sizeof info);\n\tinfo.si_signo = signr;\n\tinfo.si_code = exit_code;\n\tinfo.si_pid = task_pid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\t/* Let the debugger run.  */\n\tptrace_stop(exit_code, why, 1, &info);\n}\n\nvoid ptrace_notify(int exit_code)\n{\n\tBUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED);\n\tspin_unlock_irq(&current->sighand->siglock);\n}\n\n/**\n * do_signal_stop - handle group stop for SIGSTOP and other stop signals\n * @signr: signr causing group stop if initiating\n *\n * If %JOBCTL_STOP_PENDING is not set yet, initiate group stop with @signr\n * and participate in it.  If already set, participate in the existing\n * group stop.  If participated in a group stop (and thus slept), %true is\n * returned with siglock released.\n *\n * If ptraced, this function doesn't handle stop itself.  Instead,\n * %JOBCTL_TRAP_STOP is scheduled and %false is returned with siglock\n * untouched.  The caller must ensure that INTERRUPT trap handling takes\n * places afterwards.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which is released\n * on %true return.\n *\n * RETURNS:\n * %false if group stop is already cancelled or ptrace trap is scheduled.\n * %true if participated in group stop.\n */\nstatic bool do_signal_stop(int signr)\n\t__releases(&current->sighand->siglock)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tif (!(current->jobctl & JOBCTL_STOP_PENDING)) {\n\t\tunsigned int gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\n\t\tstruct task_struct *t;\n\n\t\t/* signr will be recorded in task->jobctl for retries */\n\t\tWARN_ON_ONCE(signr & ~JOBCTL_STOP_SIGMASK);\n\n\t\tif (!likely(current->jobctl & JOBCTL_STOP_DEQUEUED) ||\n\t\t    unlikely(signal_group_exit(sig)))\n\t\t\treturn false;\n\t\t/*\n\t\t * There is no group stop already in progress.  We must\n\t\t * initiate one now.\n\t\t *\n\t\t * While ptraced, a task may be resumed while group stop is\n\t\t * still in effect and then receive a stop signal and\n\t\t * initiate another group stop.  This deviates from the\n\t\t * usual behavior as two consecutive stop signals can't\n\t\t * cause two group stops when !ptraced.  That is why we\n\t\t * also check !task_is_stopped(t) below.\n\t\t *\n\t\t * The condition can be distinguished by testing whether\n\t\t * SIGNAL_STOP_STOPPED is already set.  Don't generate\n\t\t * group_exit_code in such case.\n\t\t *\n\t\t * This is not necessary for SIGNAL_STOP_CONTINUED because\n\t\t * an intervening stop signal is required to cause two\n\t\t * continued events regardless of ptrace.\n\t\t */\n\t\tif (!(sig->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsig->group_exit_code = signr;\n\n\t\tsig->group_stop_count = 0;\n\n\t\tif (task_set_jobctl_pending(current, signr | gstop))\n\t\t\tsig->group_stop_count++;\n\n\t\tfor (t = next_thread(current); t != current;\n\t\t     t = next_thread(t)) {\n\t\t\t/*\n\t\t\t * Setting state to TASK_STOPPED for a group\n\t\t\t * stop is always done with the siglock held,\n\t\t\t * so this check has no races.\n\t\t\t */\n\t\t\tif (!task_is_stopped(t) &&\n\t\t\t    task_set_jobctl_pending(t, signr | gstop)) {\n\t\t\t\tsig->group_stop_count++;\n\t\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\t\tsignal_wake_up(t, 0);\n\t\t\t\telse\n\t\t\t\t\tptrace_trap_notify(t);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (likely(!current->ptrace)) {\n\t\tint notify = 0;\n\n\t\t/*\n\t\t * If there are no other threads in the group, or if there\n\t\t * is a group stop in progress and we are the last to stop,\n\t\t * report to the parent.\n\t\t */\n\t\tif (task_participate_group_stop(current))\n\t\t\tnotify = CLD_STOPPED;\n\n\t\t__set_current_state(TASK_STOPPED);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent of the group stop completion.  Because\n\t\t * we're not holding either the siglock or tasklist_lock\n\t\t * here, ptracer may attach inbetween; however, this is for\n\t\t * group stop and should always be delivered to the real\n\t\t * parent of the group leader.  The new ptracer will get\n\t\t * its notification when this task transitions into\n\t\t * TASK_TRACED.\n\t\t */\n\t\tif (notify) {\n\t\t\tread_lock(&tasklist_lock);\n\t\t\tdo_notify_parent_cldstop(current, false, notify);\n\t\t\tread_unlock(&tasklist_lock);\n\t\t}\n\n\t\t/* Now we don't run again until woken by SIGCONT or SIGKILL */\n\t\tfreezable_schedule();\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * While ptraced, group stop is handled by STOP trap.\n\t\t * Schedule it and let the caller deal with it.\n\t\t */\n\t\ttask_set_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\t\treturn false;\n\t}\n}\n\n/**\n * do_jobctl_trap - take care of ptrace jobctl traps\n *\n * When PT_SEIZED, it's used for both group stop and explicit\n * SEIZE/INTERRUPT traps.  Both generate PTRACE_EVENT_STOP trap with\n * accompanying siginfo.  If stopped, lower eight bits of exit_code contain\n * the stop signal; otherwise, %SIGTRAP.\n *\n * When !PT_SEIZED, it's used only for group stop trap with stop signal\n * number as exit_code and no siginfo.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which may be\n * released and re-acquired before returning with intervening sleep.\n */\nstatic void do_jobctl_trap(void)\n{\n\tstruct signal_struct *signal = current->signal;\n\tint signr = current->jobctl & JOBCTL_STOP_SIGMASK;\n\n\tif (current->ptrace & PT_SEIZED) {\n\t\tif (!signal->group_stop_count &&\n\t\t    !(signal->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsignr = SIGTRAP;\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),\n\t\t\t\t CLD_STOPPED);\n\t} else {\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_stop(signr, CLD_STOPPED, 0, NULL);\n\t\tcurrent->exit_code = 0;\n\t}\n}\n\nstatic int ptrace_signal(int signr, siginfo_t *info)\n{\n\tptrace_signal_deliver();\n\t/*\n\t * We do not check sig_kernel_stop(signr) but set this marker\n\t * unconditionally because we do not know whether debugger will\n\t * change signr. This flag has no meaning unless we are going\n\t * to stop after return from ptrace_stop(). In this case it will\n\t * be checked in do_signal_stop(), we should only stop if it was\n\t * not cleared by SIGCONT while we were sleeping. See also the\n\t * comment in dequeue_signal().\n\t */\n\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\tptrace_stop(signr, CLD_TRAPPED, 0, info);\n\n\t/* We're back.  Did the debugger cancel the sig?  */\n\tsignr = current->exit_code;\n\tif (signr == 0)\n\t\treturn signr;\n\n\tcurrent->exit_code = 0;\n\n\t/*\n\t * Update the siginfo structure if the signal has\n\t * changed.  If the debugger wanted something\n\t * specific in the siginfo structure then it should\n\t * have updated *info via PTRACE_SETSIGINFO.\n\t */\n\tif (signr != info->si_signo) {\n\t\tinfo->si_signo = signr;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\trcu_read_lock();\n\t\tinfo->si_pid = task_pid_vnr(current->parent);\n\t\tinfo->si_uid = from_kuid_munged(current_user_ns(),\n\t\t\t\t\t\ttask_uid(current->parent));\n\t\trcu_read_unlock();\n\t}\n\n\t/* If the (new) signal is now blocked, requeue it.  */\n\tif (sigismember(&current->blocked, signr)) {\n\t\tspecific_send_sig_info(signr, info, current);\n\t\tsignr = 0;\n\t}\n\n\treturn signr;\n}\n\nint get_signal_to_deliver(siginfo_t *info, struct k_sigaction *return_ka,\n\t\t\t  struct pt_regs *regs, void *cookie)\n{\n\tstruct sighand_struct *sighand = current->sighand;\n\tstruct signal_struct *signal = current->signal;\n\tint signr;\n\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tif (unlikely(uprobe_deny_signal()))\n\t\treturn 0;\n\n\t/*\n\t * Do this once, we can't return to user-mode if freezing() == T.\n\t * do_signal_stop() and ptrace_stop() do freezable_schedule() and\n\t * thus do not need another check after return.\n\t */\n\ttry_to_freeze();\n\nrelock:\n\tspin_lock_irq(&sighand->siglock);\n\t/*\n\t * Every stopped thread goes here after wakeup. Check to see if\n\t * we should notify the parent, prepare_signal(SIGCONT) encodes\n\t * the CLD_ si_code into SIGNAL_CLD_MASK bits.\n\t */\n\tif (unlikely(signal->flags & SIGNAL_CLD_MASK)) {\n\t\tint why;\n\n\t\tif (signal->flags & SIGNAL_CLD_CONTINUED)\n\t\t\twhy = CLD_CONTINUED;\n\t\telse\n\t\t\twhy = CLD_STOPPED;\n\n\t\tsignal->flags &= ~SIGNAL_CLD_MASK;\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent that we're continuing.  This event is\n\t\t * always per-process and doesn't make whole lot of sense\n\t\t * for ptracers, who shouldn't consume the state via\n\t\t * wait(2) either, but, for backward compatibility, notify\n\t\t * the ptracer of the group leader too unless it's gonna be\n\t\t * a duplicate.\n\t\t */\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\tif (ptrace_reparented(current->group_leader))\n\t\t\tdo_notify_parent_cldstop(current->group_leader,\n\t\t\t\t\t\ttrue, why);\n\t\tread_unlock(&tasklist_lock);\n\n\t\tgoto relock;\n\t}\n\n\tfor (;;) {\n\t\tstruct k_sigaction *ka;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&\n\t\t    do_signal_stop(0))\n\t\t\tgoto relock;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_TRAP_MASK)) {\n\t\t\tdo_jobctl_trap();\n\t\t\tspin_unlock_irq(&sighand->siglock);\n\t\t\tgoto relock;\n\t\t}\n\n\t\tsignr = dequeue_signal(current, &current->blocked, info);\n\n\t\tif (!signr)\n\t\t\tbreak; /* will return 0 */\n\n\t\tif (unlikely(current->ptrace) && signr != SIGKILL) {\n\t\t\tsignr = ptrace_signal(signr, info);\n\t\t\tif (!signr)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tka = &sighand->action[signr-1];\n\n\t\t/* Trace actually delivered signals. */\n\t\ttrace_signal_deliver(signr, info, ka);\n\n\t\tif (ka->sa.sa_handler == SIG_IGN) /* Do nothing.  */\n\t\t\tcontinue;\n\t\tif (ka->sa.sa_handler != SIG_DFL) {\n\t\t\t/* Run the handler.  */\n\t\t\t*return_ka = *ka;\n\n\t\t\tif (ka->sa.sa_flags & SA_ONESHOT)\n\t\t\t\tka->sa.sa_handler = SIG_DFL;\n\n\t\t\tbreak; /* will return non-zero \"signr\" value */\n\t\t}\n\n\t\t/*\n\t\t * Now we are doing the default action for this signal.\n\t\t */\n\t\tif (sig_kernel_ignore(signr)) /* Default is nothing. */\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Global init gets no signals it doesn't want.\n\t\t * Container-init gets no signals it doesn't want from same\n\t\t * container.\n\t\t *\n\t\t * Note that if global/container-init sees a sig_kernel_only()\n\t\t * signal here, the signal must have been generated internally\n\t\t * or must have come from an ancestor namespace. In either\n\t\t * case, the signal cannot be dropped.\n\t\t */\n\t\tif (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\t\t!sig_kernel_only(signr))\n\t\t\tcontinue;\n\n\t\tif (sig_kernel_stop(signr)) {\n\t\t\t/*\n\t\t\t * The default action is to stop all threads in\n\t\t\t * the thread group.  The job control signals\n\t\t\t * do nothing in an orphaned pgrp, but SIGSTOP\n\t\t\t * always works.  Note that siglock needs to be\n\t\t\t * dropped during the call to is_orphaned_pgrp()\n\t\t\t * because of lock ordering with tasklist_lock.\n\t\t\t * This allows an intervening SIGCONT to be posted.\n\t\t\t * We need to check for that and bail out if necessary.\n\t\t\t */\n\t\t\tif (signr != SIGSTOP) {\n\t\t\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t\t\t/* signals can be posted during this window */\n\n\t\t\t\tif (is_current_pgrp_orphaned())\n\t\t\t\t\tgoto relock;\n\n\t\t\t\tspin_lock_irq(&sighand->siglock);\n\t\t\t}\n\n\t\t\tif (likely(do_signal_stop(info->si_signo))) {\n\t\t\t\t/* It released the siglock.  */\n\t\t\t\tgoto relock;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We didn't actually stop, due to a race\n\t\t\t * with SIGCONT or something like that.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Anything else is fatal, maybe with a core dump.\n\t\t */\n\t\tcurrent->flags |= PF_SIGNALED;\n\n\t\tif (sig_kernel_coredump(signr)) {\n\t\t\tif (print_fatal_signals)\n\t\t\t\tprint_fatal_signal(info->si_signo);\n\t\t\t/*\n\t\t\t * If it was able to dump core, this kills all\n\t\t\t * other threads in the group and synchronizes with\n\t\t\t * their demise.  If we lost the race with another\n\t\t\t * thread getting here, it set group_exit_code\n\t\t\t * first and our do_group_exit call below will use\n\t\t\t * that value and ignore the one we pass it.\n\t\t\t */\n\t\t\tdo_coredump(info);\n\t\t}\n\n\t\t/*\n\t\t * Death signals, no core dump.\n\t\t */\n\t\tdo_group_exit(info->si_signo);\n\t\t/* NOTREACHED */\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n\treturn signr;\n}\n\n/**\n * signal_delivered - \n * @sig:\t\tnumber of signal being delivered\n * @info:\t\tsiginfo_t of signal being delivered\n * @ka:\t\t\tsigaction setting that chose the handler\n * @regs:\t\tuser register state\n * @stepping:\t\tnonzero if debugger single-step or block-step in use\n *\n * This function should be called when a signal has succesfully been\n * delivered. It updates the blocked signals accordingly (@ka->sa.sa_mask\n * is always blocked, and the signal itself is blocked unless %SA_NODEFER\n * is set in @ka->sa.sa_flags.  Tracing is notified.\n */\nvoid signal_delivered(int sig, siginfo_t *info, struct k_sigaction *ka,\n\t\t\tstruct pt_regs *regs, int stepping)\n{\n\tsigset_t blocked;\n\n\t/* A signal was successfully delivered, and the\n\t   saved sigmask was stored on the signal frame,\n\t   and will be restored by sigreturn.  So we can\n\t   simply clear the restore sigmask flag.  */\n\tclear_restore_sigmask();\n\n\tsigorsets(&blocked, &current->blocked, &ka->sa.sa_mask);\n\tif (!(ka->sa.sa_flags & SA_NODEFER))\n\t\tsigaddset(&blocked, sig);\n\tset_current_blocked(&blocked);\n\ttracehook_signal_handler(sig, info, ka, regs, stepping);\n}\n\nvoid signal_setup_done(int failed, struct ksignal *ksig, int stepping)\n{\n\tif (failed)\n\t\tforce_sigsegv(ksig->sig, current);\n\telse\n\t\tsignal_delivered(ksig->sig, &ksig->info, &ksig->ka,\n\t\t\tsignal_pt_regs(), stepping);\n}\n\n/*\n * It could be that complete_signal() picked us to notify about the\n * group-wide signal. Other threads should be notified now to take\n * the shared signals in @which since we will not.\n */\nstatic void retarget_shared_pending(struct task_struct *tsk, sigset_t *which)\n{\n\tsigset_t retarget;\n\tstruct task_struct *t;\n\n\tsigandsets(&retarget, &tsk->signal->shared_pending.signal, which);\n\tif (sigisemptyset(&retarget))\n\t\treturn;\n\n\tt = tsk;\n\twhile_each_thread(tsk, t) {\n\t\tif (t->flags & PF_EXITING)\n\t\t\tcontinue;\n\n\t\tif (!has_pending_signals(&retarget, &t->blocked))\n\t\t\tcontinue;\n\t\t/* Remove the signals this thread can handle. */\n\t\tsigandsets(&retarget, &retarget, &t->blocked);\n\n\t\tif (!signal_pending(t))\n\t\t\tsignal_wake_up(t, 0);\n\n\t\tif (sigisemptyset(&retarget))\n\t\t\tbreak;\n\t}\n}\n\nvoid exit_signals(struct task_struct *tsk)\n{\n\tint group_stop = 0;\n\tsigset_t unblocked;\n\n\t/*\n\t * @tsk is about to have PF_EXITING set - lock out users which\n\t * expect stable threadgroup.\n\t */\n\tthreadgroup_change_begin(tsk);\n\n\tif (thread_group_empty(tsk) || signal_group_exit(tsk->signal)) {\n\t\ttsk->flags |= PF_EXITING;\n\t\tthreadgroup_change_end(tsk);\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t/*\n\t * From now this task is not visible for group-wide signals,\n\t * see wants_signal(), do_signal_stop().\n\t */\n\ttsk->flags |= PF_EXITING;\n\n\tthreadgroup_change_end(tsk);\n\n\tif (!signal_pending(tsk))\n\t\tgoto out;\n\n\tunblocked = tsk->blocked;\n\tsignotset(&unblocked);\n\tretarget_shared_pending(tsk, &unblocked);\n\n\tif (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&\n\t    task_participate_group_stop(tsk))\n\t\tgroup_stop = CLD_STOPPED;\nout:\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t/*\n\t * If group stop has completed, deliver the notification.  This\n\t * should always go to the real parent of the group leader.\n\t */\n\tif (unlikely(group_stop)) {\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(tsk, false, group_stop);\n\t\tread_unlock(&tasklist_lock);\n\t}\n}\n\nEXPORT_SYMBOL(recalc_sigpending);\nEXPORT_SYMBOL_GPL(dequeue_signal);\nEXPORT_SYMBOL(flush_signals);\nEXPORT_SYMBOL(force_sig);\nEXPORT_SYMBOL(send_sig);\nEXPORT_SYMBOL(send_sig_info);\nEXPORT_SYMBOL(sigprocmask);\nEXPORT_SYMBOL(block_all_signals);\nEXPORT_SYMBOL(unblock_all_signals);\n\n\n/*\n * System call entry points.\n */\n\n/**\n *  sys_restart_syscall - restart a system call\n */\nSYSCALL_DEFINE0(restart_syscall)\n{\n\tstruct restart_block *restart = &current_thread_info()->restart_block;\n\treturn restart->fn(restart);\n}\n\nlong do_no_restart_syscall(struct restart_block *param)\n{\n\treturn -EINTR;\n}\n\nstatic void __set_task_blocked(struct task_struct *tsk, const sigset_t *newset)\n{\n\tif (signal_pending(tsk) && !thread_group_empty(tsk)) {\n\t\tsigset_t newblocked;\n\t\t/* A set of now blocked but previously unblocked signals. */\n\t\tsigandnsets(&newblocked, newset, &current->blocked);\n\t\tretarget_shared_pending(tsk, &newblocked);\n\t}\n\ttsk->blocked = *newset;\n\trecalc_sigpending();\n}\n\n/**\n * set_current_blocked - change current->blocked mask\n * @newset: new mask\n *\n * It is wrong to change ->blocked directly, this helper should be used\n * to ensure the process can't miss a shared signal we are going to block.\n */\nvoid set_current_blocked(sigset_t *newset)\n{\n\tsigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t__set_current_blocked(newset);\n}\n\nvoid __set_current_blocked(const sigset_t *newset)\n{\n\tstruct task_struct *tsk = current;\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t__set_task_blocked(tsk, newset);\n\tspin_unlock_irq(&tsk->sighand->siglock);\n}\n\n/*\n * This is also useful for kernel threads that want to temporarily\n * (or permanently) block certain signals.\n *\n * NOTE! Unlike the user-mode sys_sigprocmask(), the kernel\n * interface happily blocks \"unblockable\" signals like SIGKILL\n * and friends.\n */\nint sigprocmask(int how, sigset_t *set, sigset_t *oldset)\n{\n\tstruct task_struct *tsk = current;\n\tsigset_t newset;\n\n\t/* Lockless, only current can change ->blocked, never from irq */\n\tif (oldset)\n\t\t*oldset = tsk->blocked;\n\n\tswitch (how) {\n\tcase SIG_BLOCK:\n\t\tsigorsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_UNBLOCK:\n\t\tsigandnsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_SETMASK:\n\t\tnewset = *set;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t__set_current_blocked(&newset);\n\treturn 0;\n}\n\n/**\n *  sys_rt_sigprocmask - change the list of currently blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: stores pending signals\n *  @oset: previous value of signal mask if non-null\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigprocmask, int, how, sigset_t __user *, nset,\n\t\tsigset_t __user *, oset, size_t, sigsetsize)\n{\n\tsigset_t old_set, new_set;\n\tint error;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\told_set = current->blocked;\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigprocmask, int, how, compat_sigset_t __user *, nset,\n\t\tcompat_sigset_t __user *, oset, compat_size_t, sigsetsize)\n{\n#ifdef __BIG_ENDIAN\n\tsigset_t old_set = current->blocked;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (nset) {\n\t\tcompat_sigset_t new32;\n\t\tsigset_t new_set;\n\t\tint error;\n\t\tif (copy_from_user(&new32, nset, sizeof(compat_sigset_t)))\n\t\t\treturn -EFAULT;\n\n\t\tsigset_from_compat(&new_set, &new32);\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif (oset) {\n\t\tcompat_sigset_t old32;\n\t\tsigset_to_compat(&old32, &old_set);\n\t\tif (copy_to_user(oset, &old32, sizeof(compat_sigset_t)))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n#else\n\treturn sys_rt_sigprocmask(how, (sigset_t __user *)nset,\n\t\t\t\t  (sigset_t __user *)oset, sigsetsize);\n#endif\n}\n#endif\n\nstatic int do_sigpending(void *set, unsigned long sigsetsize)\n{\n\tif (sigsetsize > sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tsigorsets(set, &current->pending.signal,\n\t\t  &current->signal->shared_pending.signal);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\t/* Outside the lock because only this thread touches it.  */\n\tsigandsets(set, &current->blocked, set);\n\treturn 0;\n}\n\n/**\n *  sys_rt_sigpending - examine a pending signal that has been raised\n *\t\t\twhile blocked\n *  @uset: stores pending signals\n *  @sigsetsize: size of sigset_t type or larger\n */\nSYSCALL_DEFINE2(rt_sigpending, sigset_t __user *, uset, size_t, sigsetsize)\n{\n\tsigset_t set;\n\tint err = do_sigpending(&set, sigsetsize);\n\tif (!err && copy_to_user(uset, &set, sigsetsize))\n\t\terr = -EFAULT;\n\treturn err;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigpending, compat_sigset_t __user *, uset,\n\t\tcompat_size_t, sigsetsize)\n{\n#ifdef __BIG_ENDIAN\n\tsigset_t set;\n\tint err = do_sigpending(&set, sigsetsize);\n\tif (!err) {\n\t\tcompat_sigset_t set32;\n\t\tsigset_to_compat(&set32, &set);\n\t\t/* we can get here only if sigsetsize <= sizeof(set) */\n\t\tif (copy_to_user(uset, &set32, sigsetsize))\n\t\t\terr = -EFAULT;\n\t}\n\treturn err;\n#else\n\treturn sys_rt_sigpending((sigset_t __user *)uset, sigsetsize);\n#endif\n}\n#endif\n\n#ifndef HAVE_ARCH_COPY_SIGINFO_TO_USER\n\nint copy_siginfo_to_user(siginfo_t __user *to, siginfo_t *from)\n{\n\tint err;\n\n\tif (!access_ok (VERIFY_WRITE, to, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\tif (from->si_code < 0)\n\t\treturn __copy_to_user(to, from, sizeof(siginfo_t))\n\t\t\t? -EFAULT : 0;\n\t/*\n\t * If you change siginfo_t structure, please be sure\n\t * this code is fixed accordingly.\n\t * Please remember to update the signalfd_copyinfo() function\n\t * inside fs/signalfd.c too, in case siginfo_t changes.\n\t * It should never copy any pad contained in the structure\n\t * to avoid security leaks, but must copy the generic\n\t * 3 ints plus the relevant union member.\n\t */\n\terr = __put_user(from->si_signo, &to->si_signo);\n\terr |= __put_user(from->si_errno, &to->si_errno);\n\terr |= __put_user((short)from->si_code, &to->si_code);\n\tswitch (from->si_code & __SI_MASK) {\n\tcase __SI_KILL:\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\tbreak;\n\tcase __SI_TIMER:\n\t\t err |= __put_user(from->si_tid, &to->si_tid);\n\t\t err |= __put_user(from->si_overrun, &to->si_overrun);\n\t\t err |= __put_user(from->si_ptr, &to->si_ptr);\n\t\tbreak;\n\tcase __SI_POLL:\n\t\terr |= __put_user(from->si_band, &to->si_band);\n\t\terr |= __put_user(from->si_fd, &to->si_fd);\n\t\tbreak;\n\tcase __SI_FAULT:\n\t\terr |= __put_user(from->si_addr, &to->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\terr |= __put_user(from->si_trapno, &to->si_trapno);\n#endif\n#ifdef BUS_MCEERR_AO\n\t\t/*\n\t\t * Other callers might not initialize the si_lsb field,\n\t\t * so check explicitly for the right codes here.\n\t\t */\n\t\tif (from->si_code == BUS_MCEERR_AR || from->si_code == BUS_MCEERR_AO)\n\t\t\terr |= __put_user(from->si_addr_lsb, &to->si_addr_lsb);\n#endif\n\t\tbreak;\n\tcase __SI_CHLD:\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\terr |= __put_user(from->si_status, &to->si_status);\n\t\terr |= __put_user(from->si_utime, &to->si_utime);\n\t\terr |= __put_user(from->si_stime, &to->si_stime);\n\t\tbreak;\n\tcase __SI_RT: /* This is not generated by the kernel as of now. */\n\tcase __SI_MESGQ: /* But this is */\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\terr |= __put_user(from->si_ptr, &to->si_ptr);\n\t\tbreak;\n#ifdef __ARCH_SIGSYS\n\tcase __SI_SYS:\n\t\terr |= __put_user(from->si_call_addr, &to->si_call_addr);\n\t\terr |= __put_user(from->si_syscall, &to->si_syscall);\n\t\terr |= __put_user(from->si_arch, &to->si_arch);\n\t\tbreak;\n#endif\n\tdefault: /* this is just in case for now ... */\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\tbreak;\n\t}\n\treturn err;\n}\n\n#endif\n\n/**\n *  do_sigtimedwait - wait for queued signals specified in @which\n *  @which: queued signals to wait for\n *  @info: if non-null, the signal's siginfo is returned here\n *  @ts: upper bound on process time suspension\n */\nint do_sigtimedwait(const sigset_t *which, siginfo_t *info,\n\t\t\tconst struct timespec *ts)\n{\n\tstruct task_struct *tsk = current;\n\tlong timeout = MAX_SCHEDULE_TIMEOUT;\n\tsigset_t mask = *which;\n\tint sig;\n\n\tif (ts) {\n\t\tif (!timespec_valid(ts))\n\t\t\treturn -EINVAL;\n\t\ttimeout = timespec_to_jiffies(ts);\n\t\t/*\n\t\t * We can be close to the next tick, add another one\n\t\t * to ensure we will wait at least the time asked for.\n\t\t */\n\t\tif (ts->tv_sec || ts->tv_nsec)\n\t\t\ttimeout++;\n\t}\n\n\t/*\n\t * Invert the set of allowed signals to get those we want to block.\n\t */\n\tsigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\tsignotset(&mask);\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\tsig = dequeue_signal(tsk, &mask, info);\n\tif (!sig && timeout) {\n\t\t/*\n\t\t * None ready, temporarily unblock those we're interested\n\t\t * while we are sleeping in so that we'll be awakened when\n\t\t * they arrive. Unblocking is always fine, we can avoid\n\t\t * set_current_blocked().\n\t\t */\n\t\ttsk->real_blocked = tsk->blocked;\n\t\tsigandsets(&tsk->blocked, &tsk->blocked, &mask);\n\t\trecalc_sigpending();\n\t\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\n\t\tspin_lock_irq(&tsk->sighand->siglock);\n\t\t__set_task_blocked(tsk, &tsk->real_blocked);\n\t\tsiginitset(&tsk->real_blocked, 0);\n\t\tsig = dequeue_signal(tsk, &mask, info);\n\t}\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\tif (sig)\n\t\treturn sig;\n\treturn timeout ? -EINTR : -EAGAIN;\n}\n\n/**\n *  sys_rt_sigtimedwait - synchronously wait for queued signals specified\n *\t\t\tin @uthese\n *  @uthese: queued signals to wait for\n *  @uinfo: if non-null, the signal's siginfo is returned here\n *  @uts: upper bound on process time suspension\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigtimedwait, const sigset_t __user *, uthese,\n\t\tsiginfo_t __user *, uinfo, const struct timespec __user *, uts,\n\t\tsize_t, sigsetsize)\n{\n\tsigset_t these;\n\tstruct timespec ts;\n\tsiginfo_t info;\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&these, uthese, sizeof(these)))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (copy_from_user(&ts, uts, sizeof(ts)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\n/**\n *  sys_kill - send a signal to a process\n *  @pid: the PID of the process\n *  @sig: signal to be sent\n */\nSYSCALL_DEFINE2(kill, pid_t, pid, int, sig)\n{\n\tstruct siginfo info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_USER;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn kill_something_info(sig, &info, pid);\n}\n\nstatic int\ndo_send_specific(pid_t tgid, pid_t pid, int sig, struct siginfo *info)\n{\n\tstruct task_struct *p;\n\tint error = -ESRCH;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p && (tgid <= 0 || task_tgid_vnr(p) == tgid)) {\n\t\terror = check_kill_permission(sig, info, p);\n\t\t/*\n\t\t * The null signal is a permissions and process existence\n\t\t * probe.  No signal is actually delivered.\n\t\t */\n\t\tif (!error && sig) {\n\t\t\terror = do_send_sig_info(sig, info, p, false);\n\t\t\t/*\n\t\t\t * If lock_task_sighand() failed we pretend the task\n\t\t\t * dies after receiving the signal. The window is tiny,\n\t\t\t * and the signal is private anyway.\n\t\t\t */\n\t\t\tif (unlikely(error == -ESRCH))\n\t\t\t\terror = 0;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nstatic int do_tkill(pid_t tgid, pid_t pid, int sig)\n{\n\tstruct siginfo info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_TKILL;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn do_send_specific(tgid, pid, sig, &info);\n}\n\n/**\n *  sys_tgkill - send signal to one specific thread\n *  @tgid: the thread group ID of the thread\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *\n *  This syscall also checks the @tgid and returns -ESRCH even if the PID\n *  exists but it's not belonging to the target process anymore. This\n *  method solves the problem of threads exiting and PIDs getting reused.\n */\nSYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(tgid, pid, sig);\n}\n\n/**\n *  sys_tkill - send signal to one specific task\n *  @pid: the PID of the task\n *  @sig: signal to be sent\n *\n *  Send a signal to only one task, even if it's a CLONE_THREAD task.\n */\nSYSCALL_DEFINE2(tkill, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(0, pid, sig);\n}\n\nstatic int do_rt_sigqueueinfo(pid_t pid, int sig, siginfo_t *info)\n{\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\n\t    (task_pid_vnr(current) != pid)) {\n\t\t/* We used to allow any < 0 si_code */\n\t\tWARN_ON_ONCE(info->si_code < 0);\n\t\treturn -EPERM;\n\t}\n\tinfo->si_signo = sig;\n\n\t/* POSIX.1b doesn't mention process groups.  */\n\treturn kill_proc_info(sig, info, pid);\n}\n\n/**\n *  sys_rt_sigqueueinfo - send signal information to a signal\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *  @uinfo: signal info to be sent\n */\nSYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tsiginfo_t info;\n\tif (copy_from_user(&info, uinfo, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\treturn do_rt_sigqueueinfo(pid, sig, &info);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(rt_sigqueueinfo,\n\t\t\tcompat_pid_t, pid,\n\t\t\tint, sig,\n\t\t\tstruct compat_siginfo __user *, uinfo)\n{\n\tsiginfo_t info;\n\tint ret = copy_siginfo_from_user32(&info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_sigqueueinfo(pid, sig, &info);\n}\n#endif\n\nstatic int do_rt_tgsigqueueinfo(pid_t tgid, pid_t pid, int sig, siginfo_t *info)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif (((info->si_code >= 0 || info->si_code == SI_TKILL)) &&\n\t    (task_pid_vnr(current) != pid)) {\n\t\t/* We used to allow any < 0 si_code */\n\t\tWARN_ON_ONCE(info->si_code < 0);\n\t\treturn -EPERM;\n\t}\n\tinfo->si_signo = sig;\n\n\treturn do_send_specific(tgid, pid, sig, info);\n}\n\nSYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tsiginfo_t info;\n\n\tif (copy_from_user(&info, uinfo, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_tgsigqueueinfo,\n\t\t\tcompat_pid_t, tgid,\n\t\t\tcompat_pid_t, pid,\n\t\t\tint, sig,\n\t\t\tstruct compat_siginfo __user *, uinfo)\n{\n\tsiginfo_t info;\n\n\tif (copy_siginfo_from_user32(&info, uinfo))\n\t\treturn -EFAULT;\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n#endif\n\nint do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)\n{\n\tstruct task_struct *t = current;\n\tstruct k_sigaction *k;\n\tsigset_t mask;\n\n\tif (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))\n\t\treturn -EINVAL;\n\n\tk = &t->sighand->action[sig-1];\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (oact)\n\t\t*oact = *k;\n\n\tif (act) {\n\t\tsigdelsetmask(&act->sa.sa_mask,\n\t\t\t      sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t\t*k = *act;\n\t\t/*\n\t\t * POSIX 3.3.1.3:\n\t\t *  \"Setting a signal action to SIG_IGN for a signal that is\n\t\t *   pending shall cause the pending signal to be discarded,\n\t\t *   whether or not it is blocked.\"\n\t\t *\n\t\t *  \"Setting a signal action to SIG_DFL for a signal that is\n\t\t *   pending and whose default action is to ignore the signal\n\t\t *   (for example, SIGCHLD), shall cause the pending signal to\n\t\t *   be discarded, whether or not it is blocked\"\n\t\t */\n\t\tif (sig_handler_ignored(sig_handler(t, sig), sig)) {\n\t\t\tsigemptyset(&mask);\n\t\t\tsigaddset(&mask, sig);\n\t\t\trm_from_queue_full(&mask, &t->signal->shared_pending);\n\t\t\tdo {\n\t\t\t\trm_from_queue_full(&mask, &t->pending);\n\t\t\t\tt = next_thread(t);\n\t\t\t} while (t != current);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn 0;\n}\n\nstatic int \ndo_sigaltstack (const stack_t __user *uss, stack_t __user *uoss, unsigned long sp)\n{\n\tstack_t oss;\n\tint error;\n\n\toss.ss_sp = (void __user *) current->sas_ss_sp;\n\toss.ss_size = current->sas_ss_size;\n\toss.ss_flags = sas_ss_flags(sp);\n\n\tif (uss) {\n\t\tvoid __user *ss_sp;\n\t\tsize_t ss_size;\n\t\tint ss_flags;\n\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_READ, uss, sizeof(*uss)))\n\t\t\tgoto out;\n\t\terror = __get_user(ss_sp, &uss->ss_sp) |\n\t\t\t__get_user(ss_flags, &uss->ss_flags) |\n\t\t\t__get_user(ss_size, &uss->ss_size);\n\t\tif (error)\n\t\t\tgoto out;\n\n\t\terror = -EPERM;\n\t\tif (on_sig_stack(sp))\n\t\t\tgoto out;\n\n\t\terror = -EINVAL;\n\t\t/*\n\t\t * Note - this code used to test ss_flags incorrectly:\n\t\t *  \t  old code may have been written using ss_flags==0\n\t\t *\t  to mean ss_flags==SS_ONSTACK (as this was the only\n\t\t *\t  way that worked) - this fix preserves that older\n\t\t *\t  mechanism.\n\t\t */\n\t\tif (ss_flags != SS_DISABLE && ss_flags != SS_ONSTACK && ss_flags != 0)\n\t\t\tgoto out;\n\n\t\tif (ss_flags == SS_DISABLE) {\n\t\t\tss_size = 0;\n\t\t\tss_sp = NULL;\n\t\t} else {\n\t\t\terror = -ENOMEM;\n\t\t\tif (ss_size < MINSIGSTKSZ)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tcurrent->sas_ss_sp = (unsigned long) ss_sp;\n\t\tcurrent->sas_ss_size = ss_size;\n\t}\n\n\terror = 0;\n\tif (uoss) {\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_WRITE, uoss, sizeof(*uoss)))\n\t\t\tgoto out;\n\t\terror = __put_user(oss.ss_sp, &uoss->ss_sp) |\n\t\t\t__put_user(oss.ss_size, &uoss->ss_size) |\n\t\t\t__put_user(oss.ss_flags, &uoss->ss_flags);\n\t}\n\nout:\n\treturn error;\n}\nSYSCALL_DEFINE2(sigaltstack,const stack_t __user *,uss, stack_t __user *,uoss)\n{\n\treturn do_sigaltstack(uss, uoss, current_user_stack_pointer());\n}\n\nint restore_altstack(const stack_t __user *uss)\n{\n\tint err = do_sigaltstack(uss, NULL, current_user_stack_pointer());\n\t/* squash all but EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __save_altstack(stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\treturn  __put_user((void __user *)t->sas_ss_sp, &uss->ss_sp) |\n\t\t__put_user(sas_ss_flags(sp), &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(sigaltstack,\n\t\t\tconst compat_stack_t __user *, uss_ptr,\n\t\t\tcompat_stack_t __user *, uoss_ptr)\n{\n\tstack_t uss, uoss;\n\tint ret;\n\tmm_segment_t seg;\n\n\tif (uss_ptr) {\n\t\tcompat_stack_t uss32;\n\n\t\tmemset(&uss, 0, sizeof(stack_t));\n\t\tif (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))\n\t\t\treturn -EFAULT;\n\t\tuss.ss_sp = compat_ptr(uss32.ss_sp);\n\t\tuss.ss_flags = uss32.ss_flags;\n\t\tuss.ss_size = uss32.ss_size;\n\t}\n\tseg = get_fs();\n\tset_fs(KERNEL_DS);\n\tret = do_sigaltstack((stack_t __force __user *) (uss_ptr ? &uss : NULL),\n\t\t\t     (stack_t __force __user *) &uoss,\n\t\t\t     compat_user_stack_pointer());\n\tset_fs(seg);\n\tif (ret >= 0 && uoss_ptr)  {\n\t\tif (!access_ok(VERIFY_WRITE, uoss_ptr, sizeof(compat_stack_t)) ||\n\t\t    __put_user(ptr_to_compat(uoss.ss_sp), &uoss_ptr->ss_sp) ||\n\t\t    __put_user(uoss.ss_flags, &uoss_ptr->ss_flags) ||\n\t\t    __put_user(uoss.ss_size, &uoss_ptr->ss_size))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\nint compat_restore_altstack(const compat_stack_t __user *uss)\n{\n\tint err = compat_sys_sigaltstack(uss, NULL);\n\t/* squash all but -EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\treturn  __put_user(ptr_to_compat((void __user *)t->sas_ss_sp), &uss->ss_sp) |\n\t\t__put_user(sas_ss_flags(sp), &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPENDING\n\n/**\n *  sys_sigpending - examine pending signals\n *  @set: where mask of pending signal is returned\n */\nSYSCALL_DEFINE1(sigpending, old_sigset_t __user *, set)\n{\n\treturn sys_rt_sigpending((sigset_t __user *)set, sizeof(old_sigset_t)); \n}\n\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\n/**\n *  sys_sigprocmask - examine and change blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: signals to add or remove (if non-null)\n *  @oset: previous value of signal mask if non-null\n *\n * Some platforms have their own version with special arguments;\n * others support only sys_rt_sigprocmask.\n */\n\nSYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,\n\t\told_sigset_t __user *, oset)\n{\n\told_sigset_t old_set, new_set;\n\tsigset_t new_blocked;\n\n\told_set = current->blocked.sig[0];\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(*nset)))\n\t\t\treturn -EFAULT;\n\n\t\tnew_blocked = current->blocked;\n\n\t\tswitch (how) {\n\t\tcase SIG_BLOCK:\n\t\t\tsigaddsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_UNBLOCK:\n\t\t\tsigdelsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_SETMASK:\n\t\t\tnew_blocked.sig[0] = new_set;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tset_current_blocked(&new_blocked);\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(*oset)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n#endif /* __ARCH_WANT_SYS_SIGPROCMASK */\n\n#ifndef CONFIG_ODD_RT_SIGACTION\n/**\n *  sys_rt_sigaction - alter an action taken by a process\n *  @sig: signal to be sent\n *  @act: new sigaction\n *  @oact: used to save the previous sigaction\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct sigaction __user *, act,\n\t\tstruct sigaction __user *, oact,\n\t\tsize_t, sigsetsize)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret = -EINVAL;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\tgoto out;\n\n\tif (act) {\n\t\tif (copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);\n\n\tif (!ret && oact) {\n\t\tif (copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))\n\t\t\treturn -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct compat_sigaction __user *, act,\n\t\tstruct compat_sigaction __user *, oact,\n\t\tcompat_size_t, sigsetsize)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tcompat_sigset_t mask;\n#ifdef __ARCH_HAS_SA_RESTORER\n\tcompat_uptr_t restorer;\n#endif\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(compat_sigset_t))\n\t\treturn -EINVAL;\n\n\tif (act) {\n\t\tcompat_uptr_t handler;\n\t\tret = get_user(handler, &act->sa_handler);\n\t\tnew_ka.sa.sa_handler = compat_ptr(handler);\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tret |= get_user(restorer, &act->sa_restorer);\n\t\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\n#endif\n\t\tret |= copy_from_user(&mask, &act->sa_mask, sizeof(mask));\n\t\tret |= __get_user(new_ka.sa.sa_flags, &act->sa_flags);\n\t\tif (ret)\n\t\t\treturn -EFAULT;\n\t\tsigset_from_compat(&new_ka.sa.sa_mask, &mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\tif (!ret && oact) {\n\t\tsigset_to_compat(&mask, &old_ka.sa.sa_mask);\n\t\tret = put_user(ptr_to_compat(old_ka.sa.sa_handler), \n\t\t\t       &oact->sa_handler);\n\t\tret |= copy_to_user(&oact->sa_mask, &mask, sizeof(mask));\n\t\tret |= __put_user(old_ka.sa.sa_flags, &oact->sa_flags);\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tret |= put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n\t\t\t\t&oact->sa_restorer);\n#endif\n\t}\n\treturn ret;\n}\n#endif\n#endif /* !CONFIG_ODD_RT_SIGACTION */\n\n#ifdef CONFIG_OLD_SIGACTION\nSYSCALL_DEFINE3(sigaction, int, sig,\n\t\tconst struct old_sigaction __user *, act,\n\t        struct old_sigaction __user *, oact)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tint ret;\n\n\tif (act) {\n\t\told_sigset_t mask;\n\t\tif (!access_ok(VERIFY_READ, act, sizeof(*act)) ||\n\t\t    __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||\n\t\t    __get_user(new_ka.sa.sa_restorer, &act->sa_restorer) ||\n\t\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n\t\t    __get_user(mask, &act->sa_mask))\n\t\t\treturn -EFAULT;\n#ifdef __ARCH_HAS_KA_RESTORER\n\t\tnew_ka.ka_restorer = NULL;\n#endif\n\t\tsiginitset(&new_ka.sa.sa_mask, mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n\tif (!ret && oact) {\n\t\tif (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||\n\t\t    __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||\n\t\t    __put_user(old_ka.sa.sa_restorer, &oact->sa_restorer) ||\n\t\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n\t\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn ret;\n}\n#endif\n#ifdef CONFIG_COMPAT_OLD_SIGACTION\nCOMPAT_SYSCALL_DEFINE3(sigaction, int, sig,\n\t\tconst struct compat_old_sigaction __user *, act,\n\t        struct compat_old_sigaction __user *, oact)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tint ret;\n\tcompat_old_sigset_t mask;\n\tcompat_uptr_t handler, restorer;\n\n\tif (act) {\n\t\tif (!access_ok(VERIFY_READ, act, sizeof(*act)) ||\n\t\t    __get_user(handler, &act->sa_handler) ||\n\t\t    __get_user(restorer, &act->sa_restorer) ||\n\t\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n\t\t    __get_user(mask, &act->sa_mask))\n\t\t\treturn -EFAULT;\n\n#ifdef __ARCH_HAS_KA_RESTORER\n\t\tnew_ka.ka_restorer = NULL;\n#endif\n\t\tnew_ka.sa.sa_handler = compat_ptr(handler);\n\t\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\n\t\tsiginitset(&new_ka.sa.sa_mask, mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n\tif (!ret && oact) {\n\t\tif (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||\n\t\t    __put_user(ptr_to_compat(old_ka.sa.sa_handler),\n\t\t\t       &oact->sa_handler) ||\n\t\t    __put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n\t\t\t       &oact->sa_restorer) ||\n\t\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n\t\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n\t\t\treturn -EFAULT;\n\t}\n\treturn ret;\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_SGETMASK\n\n/*\n * For backwards compatibility.  Functionality superseded by sigprocmask.\n */\nSYSCALL_DEFINE0(sgetmask)\n{\n\t/* SMP safe */\n\treturn current->blocked.sig[0];\n}\n\nSYSCALL_DEFINE1(ssetmask, int, newmask)\n{\n\tint old = current->blocked.sig[0];\n\tsigset_t newset;\n\n\tsiginitset(&newset, newmask);\n\tset_current_blocked(&newset);\n\n\treturn old;\n}\n#endif /* __ARCH_WANT_SGETMASK */\n\n#ifdef __ARCH_WANT_SYS_SIGNAL\n/*\n * For backwards compatibility.  Functionality superseded by sigaction.\n */\nSYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret;\n\n\tnew_sa.sa.sa_handler = handler;\n\tnew_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;\n\tsigemptyset(&new_sa.sa.sa_mask);\n\n\tret = do_sigaction(sig, &new_sa, &old_sa);\n\n\treturn ret ? ret : (unsigned long)old_sa.sa.sa_handler;\n}\n#endif /* __ARCH_WANT_SYS_SIGNAL */\n\n#ifdef __ARCH_WANT_SYS_PAUSE\n\nSYSCALL_DEFINE0(pause)\n{\n\twhile (!signal_pending(current)) {\n\t\tcurrent->state = TASK_INTERRUPTIBLE;\n\t\tschedule();\n\t}\n\treturn -ERESTARTNOHAND;\n}\n\n#endif\n\nint sigsuspend(sigset_t *set)\n{\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(set);\n\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tschedule();\n\tset_restore_sigmask();\n\treturn -ERESTARTNOHAND;\n}\n\n/**\n *  sys_rt_sigsuspend - replace the signal mask for a value with the\n *\t@unewset value until a signal is received\n *  @unewset: new signal mask value\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)\n{\n\tsigset_t newset;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&newset, unewset, sizeof(newset)))\n\t\treturn -EFAULT;\n\treturn sigsuspend(&newset);\n}\n \n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigsuspend, compat_sigset_t __user *, unewset, compat_size_t, sigsetsize)\n{\n#ifdef __BIG_ENDIAN\n\tsigset_t newset;\n\tcompat_sigset_t newset32;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&newset32, unewset, sizeof(compat_sigset_t)))\n\t\treturn -EFAULT;\n\tsigset_from_compat(&newset, &newset32);\n\treturn sigsuspend(&newset);\n#else\n\t/* on little-endian bitmaps don't care about granularity */\n\treturn sys_rt_sigsuspend((sigset_t __user *)unewset, sigsetsize);\n#endif\n}\n#endif\n\n#ifdef CONFIG_OLD_SIGSUSPEND\nSYSCALL_DEFINE1(sigsuspend, old_sigset_t, mask)\n{\n\tsigset_t blocked;\n\tsiginitset(&blocked, mask);\n\treturn sigsuspend(&blocked);\n}\n#endif\n#ifdef CONFIG_OLD_SIGSUSPEND3\nSYSCALL_DEFINE3(sigsuspend, int, unused1, int, unused2, old_sigset_t, mask)\n{\n\tsigset_t blocked;\n\tsiginitset(&blocked, mask);\n\treturn sigsuspend(&blocked);\n}\n#endif\n\n__attribute__((weak)) const char *arch_vma_name(struct vm_area_struct *vma)\n{\n\treturn NULL;\n}\n\nvoid __init signals_init(void)\n{\n\tsigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC);\n}\n\n#ifdef CONFIG_KGDB_KDB\n#include <linux/kdb.h>\n/*\n * kdb_send_sig_info - Allows kdb to send signals without exposing\n * signal internals.  This function checks if the required locks are\n * available before calling the main signal code, to avoid kdb\n * deadlocks.\n */\nvoid\nkdb_send_sig_info(struct task_struct *t, struct siginfo *info)\n{\n\tstatic struct task_struct *kdb_prev_t;\n\tint sig, new_t;\n\tif (!spin_trylock(&t->sighand->siglock)) {\n\t\tkdb_printf(\"Can't do kill command now.\\n\"\n\t\t\t   \"The sigmask lock is held somewhere else in \"\n\t\t\t   \"kernel, try again later\\n\");\n\t\treturn;\n\t}\n\tspin_unlock(&t->sighand->siglock);\n\tnew_t = kdb_prev_t != t;\n\tkdb_prev_t = t;\n\tif (t->state != TASK_RUNNING && new_t) {\n\t\tkdb_printf(\"Process is not RUNNING, sending a signal from \"\n\t\t\t   \"kdb risks deadlock\\n\"\n\t\t\t   \"on the run queue locks. \"\n\t\t\t   \"The signal has _not_ been sent.\\n\"\n\t\t\t   \"Reissue the kill command if you want to risk \"\n\t\t\t   \"the deadlock.\\n\");\n\t\treturn;\n\t}\n\tsig = info->si_signo;\n\tif (send_sig_info(sig, info, t))\n\t\tkdb_printf(\"Fail to deliver Signal %d to process %d.\\n\",\n\t\t\t   sig, t->pid);\n\telse\n\t\tkdb_printf(\"Signal %d is sent to process %d.\\n\", sig, t->pid);\n}\n#endif\t/* CONFIG_KGDB_KDB */\n"], "fixing_code": ["/*\n *  linux/kernel/signal.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson\n *\n *  2003-06-02  Jim Houston - Concurrent Computer Corp.\n *\t\tChanges to use preallocated sigqueue structures\n *\t\tto allow signals to be sent reliably.\n */\n\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/tty.h>\n#include <linux/binfmts.h>\n#include <linux/coredump.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/ptrace.h>\n#include <linux/signal.h>\n#include <linux/signalfd.h>\n#include <linux/ratelimit.h>\n#include <linux/tracehook.h>\n#include <linux/capability.h>\n#include <linux/freezer.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/user_namespace.h>\n#include <linux/uprobes.h>\n#include <linux/compat.h>\n#define CREATE_TRACE_POINTS\n#include <trace/events/signal.h>\n\n#include <asm/param.h>\n#include <asm/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/siginfo.h>\n#include <asm/cacheflush.h>\n#include \"audit.h\"\t/* audit_signal_info() */\n\n/*\n * SLAB caches for signal bits.\n */\n\nstatic struct kmem_cache *sigqueue_cachep;\n\nint print_fatal_signals __read_mostly;\n\nstatic void __user *sig_handler(struct task_struct *t, int sig)\n{\n\treturn t->sighand->action[sig - 1].sa.sa_handler;\n}\n\nstatic int sig_handler_ignored(void __user *handler, int sig)\n{\n\t/* Is it explicitly or implicitly ignored? */\n\treturn handler == SIG_IGN ||\n\t\t(handler == SIG_DFL && sig_kernel_ignore(sig));\n}\n\nstatic int sig_task_ignored(struct task_struct *t, int sig, bool force)\n{\n\tvoid __user *handler;\n\n\thandler = sig_handler(t, sig);\n\n\tif (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\thandler == SIG_DFL && !force)\n\t\treturn 1;\n\n\treturn sig_handler_ignored(handler, sig);\n}\n\nstatic int sig_ignored(struct task_struct *t, int sig, bool force)\n{\n\t/*\n\t * Blocked signals are never ignored, since the\n\t * signal handler may change by the time it is\n\t * unblocked.\n\t */\n\tif (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))\n\t\treturn 0;\n\n\tif (!sig_task_ignored(t, sig, force))\n\t\treturn 0;\n\n\t/*\n\t * Tracers may want to know about even ignored signals.\n\t */\n\treturn !t->ptrace;\n}\n\n/*\n * Re-calculate pending state from the set of locally pending\n * signals, globally pending signals, and blocked signals.\n */\nstatic inline int has_pending_signals(sigset_t *signal, sigset_t *blocked)\n{\n\tunsigned long ready;\n\tlong i;\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = _NSIG_WORDS, ready = 0; --i >= 0 ;)\n\t\t\tready |= signal->sig[i] &~ blocked->sig[i];\n\t\tbreak;\n\n\tcase 4: ready  = signal->sig[3] &~ blocked->sig[3];\n\t\tready |= signal->sig[2] &~ blocked->sig[2];\n\t\tready |= signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 2: ready  = signal->sig[1] &~ blocked->sig[1];\n\t\tready |= signal->sig[0] &~ blocked->sig[0];\n\t\tbreak;\n\n\tcase 1: ready  = signal->sig[0] &~ blocked->sig[0];\n\t}\n\treturn ready !=\t0;\n}\n\n#define PENDING(p,b) has_pending_signals(&(p)->signal, (b))\n\nstatic int recalc_sigpending_tsk(struct task_struct *t)\n{\n\tif ((t->jobctl & JOBCTL_PENDING_MASK) ||\n\t    PENDING(&t->pending, &t->blocked) ||\n\t    PENDING(&t->signal->shared_pending, &t->blocked)) {\n\t\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t\treturn 1;\n\t}\n\t/*\n\t * We must never clear the flag in another thread, or in current\n\t * when it's possible the current syscall is returning -ERESTART*.\n\t * So we don't clear it here, and only callers who know they should do.\n\t */\n\treturn 0;\n}\n\n/*\n * After recalculating TIF_SIGPENDING, we need to make sure the task wakes up.\n * This is superfluous when called on current, the wakeup is a harmless no-op.\n */\nvoid recalc_sigpending_and_wake(struct task_struct *t)\n{\n\tif (recalc_sigpending_tsk(t))\n\t\tsignal_wake_up(t, 0);\n}\n\nvoid recalc_sigpending(void)\n{\n\tif (!recalc_sigpending_tsk(current) && !freezing(current))\n\t\tclear_thread_flag(TIF_SIGPENDING);\n\n}\n\n/* Given the mask, find the first available signal that should be serviced. */\n\n#define SYNCHRONOUS_MASK \\\n\t(sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \\\n\t sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))\n\nint next_signal(struct sigpending *pending, sigset_t *mask)\n{\n\tunsigned long i, *s, *m, x;\n\tint sig = 0;\n\n\ts = pending->signal.sig;\n\tm = mask->sig;\n\n\t/*\n\t * Handle the first word specially: it contains the\n\t * synchronous signals that need to be dequeued first.\n\t */\n\tx = *s &~ *m;\n\tif (x) {\n\t\tif (x & SYNCHRONOUS_MASK)\n\t\t\tx &= SYNCHRONOUS_MASK;\n\t\tsig = ffz(~x) + 1;\n\t\treturn sig;\n\t}\n\n\tswitch (_NSIG_WORDS) {\n\tdefault:\n\t\tfor (i = 1; i < _NSIG_WORDS; ++i) {\n\t\t\tx = *++s &~ *++m;\n\t\t\tif (!x)\n\t\t\t\tcontinue;\n\t\t\tsig = ffz(~x) + i*_NSIG_BPW + 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 2:\n\t\tx = s[1] &~ m[1];\n\t\tif (!x)\n\t\t\tbreak;\n\t\tsig = ffz(~x) + _NSIG_BPW + 1;\n\t\tbreak;\n\n\tcase 1:\n\t\t/* Nothing to do */\n\t\tbreak;\n\t}\n\n\treturn sig;\n}\n\nstatic inline void print_dropped_signal(int sig)\n{\n\tstatic DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);\n\n\tif (!print_fatal_signals)\n\t\treturn;\n\n\tif (!__ratelimit(&ratelimit_state))\n\t\treturn;\n\n\tprintk(KERN_INFO \"%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\\n\",\n\t\t\t\tcurrent->comm, current->pid, sig);\n}\n\n/**\n * task_set_jobctl_pending - set jobctl pending bits\n * @task: target task\n * @mask: pending bits to set\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK | %JOBCTL_STOP_CONSUME | %JOBCTL_STOP_SIGMASK |\n * %JOBCTL_TRAPPING.  If stop signo is being set, the existing signo is\n * cleared.  If @task is already being killed or exiting, this function\n * becomes noop.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if @mask is set, %false if made noop because @task was dying.\n */\nbool task_set_jobctl_pending(struct task_struct *task, unsigned int mask)\n{\n\tBUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |\n\t\t\tJOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));\n\tBUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));\n\n\tif (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))\n\t\treturn false;\n\n\tif (mask & JOBCTL_STOP_SIGMASK)\n\t\ttask->jobctl &= ~JOBCTL_STOP_SIGMASK;\n\n\ttask->jobctl |= mask;\n\treturn true;\n}\n\n/**\n * task_clear_jobctl_trapping - clear jobctl trapping bit\n * @task: target task\n *\n * If JOBCTL_TRAPPING is set, a ptracer is waiting for us to enter TRACED.\n * Clear it and wake up the ptracer.  Note that we don't need any further\n * locking.  @task->siglock guarantees that @task->parent points to the\n * ptracer.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_trapping(struct task_struct *task)\n{\n\tif (unlikely(task->jobctl & JOBCTL_TRAPPING)) {\n\t\ttask->jobctl &= ~JOBCTL_TRAPPING;\n\t\twake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);\n\t}\n}\n\n/**\n * task_clear_jobctl_pending - clear jobctl pending bits\n * @task: target task\n * @mask: pending bits to clear\n *\n * Clear @mask from @task->jobctl.  @mask must be subset of\n * %JOBCTL_PENDING_MASK.  If %JOBCTL_STOP_PENDING is being cleared, other\n * STOP bits are cleared together.\n *\n * If clearing of @mask leaves no stop or trap pending, this function calls\n * task_clear_jobctl_trapping().\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nvoid task_clear_jobctl_pending(struct task_struct *task, unsigned int mask)\n{\n\tBUG_ON(mask & ~JOBCTL_PENDING_MASK);\n\n\tif (mask & JOBCTL_STOP_PENDING)\n\t\tmask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;\n\n\ttask->jobctl &= ~mask;\n\n\tif (!(task->jobctl & JOBCTL_PENDING_MASK))\n\t\ttask_clear_jobctl_trapping(task);\n}\n\n/**\n * task_participate_group_stop - participate in a group stop\n * @task: task participating in a group stop\n *\n * @task has %JOBCTL_STOP_PENDING set and is participating in a group stop.\n * Group stop states are cleared and the group stop count is consumed if\n * %JOBCTL_STOP_CONSUME was set.  If the consumption completes the group\n * stop, the appropriate %SIGNAL_* flags are set.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n *\n * RETURNS:\n * %true if group stop completion should be notified to the parent, %false\n * otherwise.\n */\nstatic bool task_participate_group_stop(struct task_struct *task)\n{\n\tstruct signal_struct *sig = task->signal;\n\tbool consume = task->jobctl & JOBCTL_STOP_CONSUME;\n\n\tWARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));\n\n\ttask_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);\n\n\tif (!consume)\n\t\treturn false;\n\n\tif (!WARN_ON_ONCE(sig->group_stop_count == 0))\n\t\tsig->group_stop_count--;\n\n\t/*\n\t * Tell the caller to notify completion iff we are entering into a\n\t * fresh group stop.  Read comment in do_signal_stop() for details.\n\t */\n\tif (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {\n\t\tsig->flags = SIGNAL_STOP_STOPPED;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * allocate a new signal queue record\n * - this may be called without locks if and only if t == current, otherwise an\n *   appropriate lock must be held to stop the target task from exiting\n */\nstatic struct sigqueue *\n__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)\n{\n\tstruct sigqueue *q = NULL;\n\tstruct user_struct *user;\n\n\t/*\n\t * Protect access to @t credentials. This can go away when all\n\t * callers hold rcu read lock.\n\t */\n\trcu_read_lock();\n\tuser = get_uid(__task_cred(t)->user);\n\tatomic_inc(&user->sigpending);\n\trcu_read_unlock();\n\n\tif (override_rlimit ||\n\t    atomic_read(&user->sigpending) <=\n\t\t\ttask_rlimit(t, RLIMIT_SIGPENDING)) {\n\t\tq = kmem_cache_alloc(sigqueue_cachep, flags);\n\t} else {\n\t\tprint_dropped_signal(sig);\n\t}\n\n\tif (unlikely(q == NULL)) {\n\t\tatomic_dec(&user->sigpending);\n\t\tfree_uid(user);\n\t} else {\n\t\tINIT_LIST_HEAD(&q->list);\n\t\tq->flags = 0;\n\t\tq->user = user;\n\t}\n\n\treturn q;\n}\n\nstatic void __sigqueue_free(struct sigqueue *q)\n{\n\tif (q->flags & SIGQUEUE_PREALLOC)\n\t\treturn;\n\tatomic_dec(&q->user->sigpending);\n\tfree_uid(q->user);\n\tkmem_cache_free(sigqueue_cachep, q);\n}\n\nvoid flush_sigqueue(struct sigpending *queue)\n{\n\tstruct sigqueue *q;\n\n\tsigemptyset(&queue->signal);\n\twhile (!list_empty(&queue->list)) {\n\t\tq = list_entry(queue->list.next, struct sigqueue , list);\n\t\tlist_del_init(&q->list);\n\t\t__sigqueue_free(q);\n\t}\n}\n\n/*\n * Flush all pending signals for a task.\n */\nvoid __flush_signals(struct task_struct *t)\n{\n\tclear_tsk_thread_flag(t, TIF_SIGPENDING);\n\tflush_sigqueue(&t->pending);\n\tflush_sigqueue(&t->signal->shared_pending);\n}\n\nvoid flush_signals(struct task_struct *t)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\t__flush_signals(t);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n}\n\nstatic void __flush_itimer_signals(struct sigpending *pending)\n{\n\tsigset_t signal, retain;\n\tstruct sigqueue *q, *n;\n\n\tsignal = pending->signal;\n\tsigemptyset(&retain);\n\n\tlist_for_each_entry_safe(q, n, &pending->list, list) {\n\t\tint sig = q->info.si_signo;\n\n\t\tif (likely(q->info.si_code != SI_TIMER)) {\n\t\t\tsigaddset(&retain, sig);\n\t\t} else {\n\t\t\tsigdelset(&signal, sig);\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\n\tsigorsets(&pending->signal, &signal, &retain);\n}\n\nvoid flush_itimer_signals(void)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tsk->sighand->siglock, flags);\n\t__flush_itimer_signals(&tsk->pending);\n\t__flush_itimer_signals(&tsk->signal->shared_pending);\n\tspin_unlock_irqrestore(&tsk->sighand->siglock, flags);\n}\n\nvoid ignore_signals(struct task_struct *t)\n{\n\tint i;\n\n\tfor (i = 0; i < _NSIG; ++i)\n\t\tt->sighand->action[i].sa.sa_handler = SIG_IGN;\n\n\tflush_signals(t);\n}\n\n/*\n * Flush all handlers for a task.\n */\n\nvoid\nflush_signal_handlers(struct task_struct *t, int force_default)\n{\n\tint i;\n\tstruct k_sigaction *ka = &t->sighand->action[0];\n\tfor (i = _NSIG ; i != 0 ; i--) {\n\t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n\t\t\tka->sa.sa_handler = SIG_DFL;\n\t\tka->sa.sa_flags = 0;\n#ifdef SA_RESTORER\n\t\tka->sa.sa_restorer = NULL;\n#endif\n\t\tsigemptyset(&ka->sa.sa_mask);\n\t\tka++;\n\t}\n}\n\nint unhandled_signal(struct task_struct *tsk, int sig)\n{\n\tvoid __user *handler = tsk->sighand->action[sig-1].sa.sa_handler;\n\tif (is_global_init(tsk))\n\t\treturn 1;\n\tif (handler != SIG_IGN && handler != SIG_DFL)\n\t\treturn 0;\n\t/* if ptraced, let the tracer determine */\n\treturn !tsk->ptrace;\n}\n\n/*\n * Notify the system that a driver wants to block all signals for this\n * process, and wants to be notified if any signals at all were to be\n * sent/acted upon.  If the notifier routine returns non-zero, then the\n * signal will be acted upon after all.  If the notifier routine returns 0,\n * then then signal will be blocked.  Only one block per process is\n * allowed.  priv is a pointer to private data that the notifier routine\n * can use to determine if the signal should be blocked or not.\n */\nvoid\nblock_all_signals(int (*notifier)(void *priv), void *priv, sigset_t *mask)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\tcurrent->notifier_mask = mask;\n\tcurrent->notifier_data = priv;\n\tcurrent->notifier = notifier;\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\n/* Notify the system that blocking has ended. */\n\nvoid\nunblock_all_signals(void)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\tcurrent->notifier = NULL;\n\tcurrent->notifier_data = NULL;\n\trecalc_sigpending();\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\nstatic void collect_signal(int sig, struct sigpending *list, siginfo_t *info)\n{\n\tstruct sigqueue *q, *first = NULL;\n\n\t/*\n\t * Collect the siginfo appropriate to this signal.  Check if\n\t * there is another siginfo for the same signal.\n\t*/\n\tlist_for_each_entry(q, &list->list, list) {\n\t\tif (q->info.si_signo == sig) {\n\t\t\tif (first)\n\t\t\t\tgoto still_pending;\n\t\t\tfirst = q;\n\t\t}\n\t}\n\n\tsigdelset(&list->signal, sig);\n\n\tif (first) {\nstill_pending:\n\t\tlist_del_init(&first->list);\n\t\tcopy_siginfo(info, &first->info);\n\t\t__sigqueue_free(first);\n\t} else {\n\t\t/*\n\t\t * Ok, it wasn't in the queue.  This must be\n\t\t * a fast-pathed signal or we must have been\n\t\t * out of queue space.  So zero out the info.\n\t\t */\n\t\tinfo->si_signo = sig;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\tinfo->si_pid = 0;\n\t\tinfo->si_uid = 0;\n\t}\n}\n\nstatic int __dequeue_signal(struct sigpending *pending, sigset_t *mask,\n\t\t\tsiginfo_t *info)\n{\n\tint sig = next_signal(pending, mask);\n\n\tif (sig) {\n\t\tif (current->notifier) {\n\t\t\tif (sigismember(current->notifier_mask, sig)) {\n\t\t\t\tif (!(current->notifier)(current->notifier_data)) {\n\t\t\t\t\tclear_thread_flag(TIF_SIGPENDING);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcollect_signal(sig, pending, info);\n\t}\n\n\treturn sig;\n}\n\n/*\n * Dequeue a signal and return the element to the caller, which is\n * expected to free it.\n *\n * All callers have to hold the siglock.\n */\nint dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)\n{\n\tint signr;\n\n\t/* We only dequeue private signals from ourselves, we don't let\n\t * signalfd steal them\n\t */\n\tsignr = __dequeue_signal(&tsk->pending, mask, info);\n\tif (!signr) {\n\t\tsignr = __dequeue_signal(&tsk->signal->shared_pending,\n\t\t\t\t\t mask, info);\n\t\t/*\n\t\t * itimer signal ?\n\t\t *\n\t\t * itimers are process shared and we restart periodic\n\t\t * itimers in the signal delivery path to prevent DoS\n\t\t * attacks in the high resolution timer case. This is\n\t\t * compliant with the old way of self-restarting\n\t\t * itimers, as the SIGALRM is a legacy signal and only\n\t\t * queued once. Changing the restart behaviour to\n\t\t * restart the timer in the signal dequeue path is\n\t\t * reducing the timer noise on heavy loaded !highres\n\t\t * systems too.\n\t\t */\n\t\tif (unlikely(signr == SIGALRM)) {\n\t\t\tstruct hrtimer *tmr = &tsk->signal->real_timer;\n\n\t\t\tif (!hrtimer_is_queued(tmr) &&\n\t\t\t    tsk->signal->it_real_incr.tv64 != 0) {\n\t\t\t\thrtimer_forward(tmr, tmr->base->get_time(),\n\t\t\t\t\t\ttsk->signal->it_real_incr);\n\t\t\t\thrtimer_restart(tmr);\n\t\t\t}\n\t\t}\n\t}\n\n\trecalc_sigpending();\n\tif (!signr)\n\t\treturn 0;\n\n\tif (unlikely(sig_kernel_stop(signr))) {\n\t\t/*\n\t\t * Set a marker that we have dequeued a stop signal.  Our\n\t\t * caller might release the siglock and then the pending\n\t\t * stop signal it is about to process is no longer in the\n\t\t * pending bitmasks, but must still be cleared by a SIGCONT\n\t\t * (and overruled by a SIGKILL).  So those cases clear this\n\t\t * shared flag after we've set it.  Note that this flag may\n\t\t * remain set after the signal we return is ignored or\n\t\t * handled.  That doesn't matter because its only purpose\n\t\t * is to alert stop-signal processing code when another\n\t\t * processor has come along and cleared the flag.\n\t\t */\n\t\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\t}\n\tif ((info->si_code & __SI_MASK) == __SI_TIMER && info->si_sys_private) {\n\t\t/*\n\t\t * Release the siglock to ensure proper locking order\n\t\t * of timer locks outside of siglocks.  Note, we leave\n\t\t * irqs disabled here, since the posix-timers code is\n\t\t * about to disable them again anyway.\n\t\t */\n\t\tspin_unlock(&tsk->sighand->siglock);\n\t\tdo_schedule_next_timer(info);\n\t\tspin_lock(&tsk->sighand->siglock);\n\t}\n\treturn signr;\n}\n\n/*\n * Tell a process that it has a new active signal..\n *\n * NOTE! we rely on the previous spin_lock to\n * lock interrupts for us! We can only be called with\n * \"siglock\" held, and the local interrupt must\n * have been disabled when that got acquired!\n *\n * No need to set need_resched since signal event passing\n * goes through ->blocked\n */\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}\n\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n *\n * This version takes a sigset mask and looks at all signals,\n * not just those in the first mask word.\n */\nstatic int rm_from_queue_full(sigset_t *mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\tsigset_t m;\n\n\tsigandsets(&m, mask, &s->signal);\n\tif (sigisemptyset(&m))\n\t\treturn 0;\n\n\tsigandnsets(&s->signal, &s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (sigismember(mask, q->info.si_signo)) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\treturn 1;\n}\n/*\n * Remove signals in mask from the pending set and queue.\n * Returns 1 if any signals were found.\n *\n * All callers must be holding the siglock.\n */\nstatic int rm_from_queue(unsigned long mask, struct sigpending *s)\n{\n\tstruct sigqueue *q, *n;\n\n\tif (!sigtestsetmask(&s->signal, mask))\n\t\treturn 0;\n\n\tsigdelsetmask(&s->signal, mask);\n\tlist_for_each_entry_safe(q, n, &s->list, list) {\n\t\tif (q->info.si_signo < SIGRTMIN &&\n\t\t    (mask & sigmask(q->info.si_signo))) {\n\t\t\tlist_del_init(&q->list);\n\t\t\t__sigqueue_free(q);\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic inline int is_si_special(const struct siginfo *info)\n{\n\treturn info <= SEND_SIG_FORCED;\n}\n\nstatic inline bool si_fromuser(const struct siginfo *info)\n{\n\treturn info == SEND_SIG_NOINFO ||\n\t\t(!is_si_special(info) && SI_FROMUSER(info));\n}\n\n/*\n * called with RCU read lock from check_kill_permission()\n */\nstatic int kill_ok_by_cred(struct task_struct *t)\n{\n\tconst struct cred *cred = current_cred();\n\tconst struct cred *tcred = __task_cred(t);\n\n\tif (uid_eq(cred->euid, tcred->suid) ||\n\t    uid_eq(cred->euid, tcred->uid)  ||\n\t    uid_eq(cred->uid,  tcred->suid) ||\n\t    uid_eq(cred->uid,  tcred->uid))\n\t\treturn 1;\n\n\tif (ns_capable(tcred->user_ns, CAP_KILL))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n/*\n * Bad permissions for sending the signal\n * - the caller must hold the RCU read lock\n */\nstatic int check_kill_permission(int sig, struct siginfo *info,\n\t\t\t\t struct task_struct *t)\n{\n\tstruct pid *sid;\n\tint error;\n\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\tif (!si_fromuser(info))\n\t\treturn 0;\n\n\terror = audit_signal_info(sig, t); /* Let audit system see the signal */\n\tif (error)\n\t\treturn error;\n\n\tif (!same_thread_group(current, t) &&\n\t    !kill_ok_by_cred(t)) {\n\t\tswitch (sig) {\n\t\tcase SIGCONT:\n\t\t\tsid = task_session(t);\n\t\t\t/*\n\t\t\t * We don't return the error if sid == NULL. The\n\t\t\t * task was unhashed, the caller must notice this.\n\t\t\t */\n\t\t\tif (!sid || sid == task_session(current))\n\t\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n\n\treturn security_task_kill(t, info, sig, 0);\n}\n\n/**\n * ptrace_trap_notify - schedule trap to notify ptracer\n * @t: tracee wanting to notify tracer\n *\n * This function schedules sticky ptrace trap which is cleared on the next\n * TRAP_STOP to notify ptracer of an event.  @t must have been seized by\n * ptracer.\n *\n * If @t is running, STOP trap will be taken.  If trapped for STOP and\n * ptracer is listening for events, tracee is woken up so that it can\n * re-trap for the new event.  If trapped otherwise, STOP trap will be\n * eventually taken without returning to userland after the existing traps\n * are finished by PTRACE_CONT.\n *\n * CONTEXT:\n * Must be called with @task->sighand->siglock held.\n */\nstatic void ptrace_trap_notify(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n\tassert_spin_locked(&t->sighand->siglock);\n\n\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}\n\n/*\n * Handle magic process-wide effects of stop/continue signals. Unlike\n * the signal actions, these happen immediately at signal-generation\n * time regardless of blocking, ignoring, or handling.  This does the\n * actual continuing for SIGCONT, but not the actual stopping for stop\n * signals. The process stop is done as a signal action for SIG_DFL.\n *\n * Returns true if the signal should be actually delivered, otherwise\n * it should be dropped.\n */\nstatic int prepare_signal(int sig, struct task_struct *p, bool force)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\tif (unlikely(signal->flags & SIGNAL_GROUP_EXIT)) {\n\t\t/*\n\t\t * The process is in the middle of dying, nothing to do.\n\t\t */\n\t} else if (sig_kernel_stop(sig)) {\n\t\t/*\n\t\t * This is a stop signal.  Remove SIGCONT from all queues.\n\t\t */\n\t\trm_from_queue(sigmask(SIGCONT), &signal->shared_pending);\n\t\tt = p;\n\t\tdo {\n\t\t\trm_from_queue(sigmask(SIGCONT), &t->pending);\n\t\t} while_each_thread(p, t);\n\t} else if (sig == SIGCONT) {\n\t\tunsigned int why;\n\t\t/*\n\t\t * Remove all stop signals from all queues, wake all threads.\n\t\t */\n\t\trm_from_queue(SIG_KERNEL_STOP_MASK, &signal->shared_pending);\n\t\tt = p;\n\t\tdo {\n\t\t\ttask_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);\n\t\t\trm_from_queue(SIG_KERNEL_STOP_MASK, &t->pending);\n\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\twake_up_state(t, __TASK_STOPPED);\n\t\t\telse\n\t\t\t\tptrace_trap_notify(t);\n\t\t} while_each_thread(p, t);\n\n\t\t/*\n\t\t * Notify the parent with CLD_CONTINUED if we were stopped.\n\t\t *\n\t\t * If we were in the middle of a group stop, we pretend it\n\t\t * was already finished, and then continued. Since SIGCHLD\n\t\t * doesn't queue we report only CLD_STOPPED, as if the next\n\t\t * CLD_CONTINUED was dropped.\n\t\t */\n\t\twhy = 0;\n\t\tif (signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\twhy |= SIGNAL_CLD_CONTINUED;\n\t\telse if (signal->group_stop_count)\n\t\t\twhy |= SIGNAL_CLD_STOPPED;\n\n\t\tif (why) {\n\t\t\t/*\n\t\t\t * The first thread which returns from do_signal_stop()\n\t\t\t * will take ->siglock, notice SIGNAL_CLD_MASK, and\n\t\t\t * notify its parent. See get_signal_to_deliver().\n\t\t\t */\n\t\t\tsignal->flags = why | SIGNAL_STOP_CONTINUED;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tsignal->group_exit_code = 0;\n\t\t}\n\t}\n\n\treturn !sig_ignored(p, sig, force);\n}\n\n/*\n * Test if P wants to take SIG.  After we've checked all threads with this,\n * it's equivalent to finding no threads not blocking SIG.  Any threads not\n * blocking SIG were ruled out because they are not running and already\n * have pending signals.  Such threads will dequeue from the shared queue\n * as soon as they're available, so putting the signal on the shared queue\n * will be equivalent to sending it to one such thread.\n */\nstatic inline int wants_signal(int sig, struct task_struct *p)\n{\n\tif (sigismember(&p->blocked, sig))\n\t\treturn 0;\n\tif (p->flags & PF_EXITING)\n\t\treturn 0;\n\tif (sig == SIGKILL)\n\t\treturn 1;\n\tif (task_is_stopped_or_traced(p))\n\t\treturn 0;\n\treturn task_curr(p) || !signal_pending(p);\n}\n\nstatic void complete_signal(int sig, struct task_struct *p, int group)\n{\n\tstruct signal_struct *signal = p->signal;\n\tstruct task_struct *t;\n\n\t/*\n\t * Now find a thread we can wake up to take the signal off the queue.\n\t *\n\t * If the main thread wants the signal, it gets first crack.\n\t * Probably the least surprising to the average bear.\n\t */\n\tif (wants_signal(sig, p))\n\t\tt = p;\n\telse if (!group || thread_group_empty(p))\n\t\t/*\n\t\t * There is just one thread and it does not need to be woken.\n\t\t * It will dequeue unblocked signals before it runs again.\n\t\t */\n\t\treturn;\n\telse {\n\t\t/*\n\t\t * Otherwise try to find a suitable thread.\n\t\t */\n\t\tt = signal->curr_target;\n\t\twhile (!wants_signal(sig, t)) {\n\t\t\tt = next_thread(t);\n\t\t\tif (t == signal->curr_target)\n\t\t\t\t/*\n\t\t\t\t * No thread needs to be woken.\n\t\t\t\t * Any eligible threads will see\n\t\t\t\t * the signal in the queue soon.\n\t\t\t\t */\n\t\t\t\treturn;\n\t\t}\n\t\tsignal->curr_target = t;\n\t}\n\n\t/*\n\t * Found a killable thread.  If the signal will be fatal,\n\t * then start taking the whole group down immediately.\n\t */\n\tif (sig_fatal(p, sig) &&\n\t    !(signal->flags & (SIGNAL_UNKILLABLE | SIGNAL_GROUP_EXIT)) &&\n\t    !sigismember(&t->real_blocked, sig) &&\n\t    (sig == SIGKILL || !t->ptrace)) {\n\t\t/*\n\t\t * This signal will be fatal to the whole group.\n\t\t */\n\t\tif (!sig_kernel_coredump(sig)) {\n\t\t\t/*\n\t\t\t * Start a group exit and wake everybody up.\n\t\t\t * This way we don't have other threads\n\t\t\t * running and doing things after a slower\n\t\t\t * thread has the fatal signal pending.\n\t\t\t */\n\t\t\tsignal->flags = SIGNAL_GROUP_EXIT;\n\t\t\tsignal->group_exit_code = sig;\n\t\t\tsignal->group_stop_count = 0;\n\t\t\tt = p;\n\t\t\tdo {\n\t\t\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\t\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\t\t\tsignal_wake_up(t, 1);\n\t\t\t} while_each_thread(p, t);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * The signal is already in the shared-pending queue.\n\t * Tell the chosen thread to wake up and dequeue it.\n\t */\n\tsignal_wake_up(t, sig == SIGKILL);\n\treturn;\n}\n\nstatic inline int legacy_queue(struct sigpending *signals, int sig)\n{\n\treturn (sig < SIGRTMIN) && sigismember(&signals->signal, sig);\n}\n\n#ifdef CONFIG_USER_NS\nstatic inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)\n{\n\tif (current_user_ns() == task_cred_xxx(t, user_ns))\n\t\treturn;\n\n\tif (SI_FROMKERNEL(info))\n\t\treturn;\n\n\trcu_read_lock();\n\tinfo->si_uid = from_kuid_munged(task_cred_xxx(t, user_ns),\n\t\t\t\t\tmake_kuid(current_user_ns(), info->si_uid));\n\trcu_read_unlock();\n}\n#else\nstatic inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)\n{\n\treturn;\n}\n#endif\n\nstatic int __send_signal(int sig, struct siginfo *info, struct task_struct *t,\n\t\t\tint group, int from_ancestor_ns)\n{\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint override_rlimit;\n\tint ret = 0, result;\n\n\tassert_spin_locked(&t->sighand->siglock);\n\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t,\n\t\t\tfrom_ancestor_ns || (info == SEND_SIG_FORCED)))\n\t\tgoto ret;\n\n\tpending = group ? &t->signal->shared_pending : &t->pending;\n\t/*\n\t * Short-circuit ignored signals and support queuing\n\t * exactly one non-rt signal, so that we can get more\n\t * detailed information about the cause of the signal.\n\t */\n\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\tif (legacy_queue(pending, sig))\n\t\tgoto ret;\n\n\tresult = TRACE_SIGNAL_DELIVERED;\n\t/*\n\t * fast-pathed signals for kernel-internal things like SIGSTOP\n\t * or SIGKILL.\n\t */\n\tif (info == SEND_SIG_FORCED)\n\t\tgoto out_set;\n\n\t/*\n\t * Real-time signals must be queued if sent by sigqueue, or\n\t * some other real-time mechanism.  It is implementation\n\t * defined whether kill() does so.  We attempt to do so, on\n\t * the principle of least surprise, but since kill is not\n\t * allowed to fail with EAGAIN when low on memory we just\n\t * make sure at least one signal gets delivered and don't\n\t * pass on the info struct.\n\t */\n\tif (sig < SIGRTMIN)\n\t\toverride_rlimit = (is_si_special(info) || info->si_code >= 0);\n\telse\n\t\toverride_rlimit = 0;\n\n\tq = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE,\n\t\toverride_rlimit);\n\tif (q) {\n\t\tlist_add_tail(&q->list, &pending->list);\n\t\tswitch ((unsigned long) info) {\n\t\tcase (unsigned long) SEND_SIG_NOINFO:\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_USER;\n\t\t\tq->info.si_pid = task_tgid_nr_ns(current,\n\t\t\t\t\t\t\ttask_active_pid_ns(t));\n\t\t\tq->info.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\t\t\tbreak;\n\t\tcase (unsigned long) SEND_SIG_PRIV:\n\t\t\tq->info.si_signo = sig;\n\t\t\tq->info.si_errno = 0;\n\t\t\tq->info.si_code = SI_KERNEL;\n\t\t\tq->info.si_pid = 0;\n\t\t\tq->info.si_uid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcopy_siginfo(&q->info, info);\n\t\t\tif (from_ancestor_ns)\n\t\t\t\tq->info.si_pid = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tuserns_fixup_signal_uid(&q->info, t);\n\n\t} else if (!is_si_special(info)) {\n\t\tif (sig >= SIGRTMIN && info->si_code != SI_USER) {\n\t\t\t/*\n\t\t\t * Queue overflow, abort.  We may abort if the\n\t\t\t * signal was rt and sent by user using something\n\t\t\t * other than kill().\n\t\t\t */\n\t\t\tresult = TRACE_SIGNAL_OVERFLOW_FAIL;\n\t\t\tret = -EAGAIN;\n\t\t\tgoto ret;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is a silent loss of information.  We still\n\t\t\t * send the signal, but the *info bits are lost.\n\t\t\t */\n\t\t\tresult = TRACE_SIGNAL_LOSE_INFO;\n\t\t}\n\t}\n\nout_set:\n\tsignalfd_notify(t, sig);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, group);\nret:\n\ttrace_signal_generate(sig, info, t, group, result);\n\treturn ret;\n}\n\nstatic int send_signal(int sig, struct siginfo *info, struct task_struct *t,\n\t\t\tint group)\n{\n\tint from_ancestor_ns = 0;\n\n#ifdef CONFIG_PID_NS\n\tfrom_ancestor_ns = si_fromuser(info) &&\n\t\t\t   !task_pid_nr_ns(current, task_active_pid_ns(t));\n#endif\n\n\treturn __send_signal(sig, info, t, group, from_ancestor_ns);\n}\n\nstatic void print_fatal_signal(int signr)\n{\n\tstruct pt_regs *regs = signal_pt_regs();\n\tprintk(KERN_INFO \"%s/%d: potentially unexpected fatal signal %d.\\n\",\n\t\tcurrent->comm, task_pid_nr(current), signr);\n\n#if defined(__i386__) && !defined(__arch_um__)\n\tprintk(KERN_INFO \"code at %08lx: \", regs->ip);\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < 16; i++) {\n\t\t\tunsigned char insn;\n\n\t\t\tif (get_user(insn, (unsigned char *)(regs->ip + i)))\n\t\t\t\tbreak;\n\t\t\tprintk(KERN_CONT \"%02x \", insn);\n\t\t}\n\t}\n\tprintk(KERN_CONT \"\\n\");\n#endif\n\tpreempt_disable();\n\tshow_regs(regs);\n\tpreempt_enable();\n}\n\nstatic int __init setup_print_fatal_signals(char *str)\n{\n\tget_option (&str, &print_fatal_signals);\n\n\treturn 1;\n}\n\n__setup(\"print-fatal-signals=\", setup_print_fatal_signals);\n\nint\n__group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\treturn send_signal(sig, info, p, 1);\n}\n\nstatic int\nspecific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)\n{\n\treturn send_signal(sig, info, t, 0);\n}\n\nint do_send_sig_info(int sig, struct siginfo *info, struct task_struct *p,\n\t\t\tbool group)\n{\n\tunsigned long flags;\n\tint ret = -ESRCH;\n\n\tif (lock_task_sighand(p, &flags)) {\n\t\tret = send_signal(sig, info, p, group);\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Force a signal that the process can't ignore: if necessary\n * we unblock the signal and change any SIG_IGN to SIG_DFL.\n *\n * Note: If we unblock the signal, we always reset it to SIG_DFL,\n * since we do not want to have a signal handler that was blocked\n * be invoked when user space had explicitly blocked it.\n *\n * We don't want to have recursive SIGSEGV's etc, for example,\n * that is why we also clear SIGNAL_UNKILLABLE.\n */\nint\nforce_sig_info(int sig, struct siginfo *info, struct task_struct *t)\n{\n\tunsigned long int flags;\n\tint ret, blocked, ignored;\n\tstruct k_sigaction *action;\n\n\tspin_lock_irqsave(&t->sighand->siglock, flags);\n\taction = &t->sighand->action[sig-1];\n\tignored = action->sa.sa_handler == SIG_IGN;\n\tblocked = sigismember(&t->blocked, sig);\n\tif (blocked || ignored) {\n\t\taction->sa.sa_handler = SIG_DFL;\n\t\tif (blocked) {\n\t\t\tsigdelset(&t->blocked, sig);\n\t\t\trecalc_sigpending_and_wake(t);\n\t\t}\n\t}\n\tif (action->sa.sa_handler == SIG_DFL)\n\t\tt->signal->flags &= ~SIGNAL_UNKILLABLE;\n\tret = specific_send_sig_info(sig, info, t);\n\tspin_unlock_irqrestore(&t->sighand->siglock, flags);\n\n\treturn ret;\n}\n\n/*\n * Nuke all other threads in the group.\n */\nint zap_other_threads(struct task_struct *p)\n{\n\tstruct task_struct *t = p;\n\tint count = 0;\n\n\tp->signal->group_stop_count = 0;\n\n\twhile_each_thread(p, t) {\n\t\ttask_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);\n\t\tcount++;\n\n\t\t/* Don't bother with already dead threads */\n\t\tif (t->exit_state)\n\t\t\tcontinue;\n\t\tsigaddset(&t->pending.signal, SIGKILL);\n\t\tsignal_wake_up(t, 1);\n\t}\n\n\treturn count;\n}\n\nstruct sighand_struct *__lock_task_sighand(struct task_struct *tsk,\n\t\t\t\t\t   unsigned long *flags)\n{\n\tstruct sighand_struct *sighand;\n\n\tfor (;;) {\n\t\tlocal_irq_save(*flags);\n\t\trcu_read_lock();\n\t\tsighand = rcu_dereference(tsk->sighand);\n\t\tif (unlikely(sighand == NULL)) {\n\t\t\trcu_read_unlock();\n\t\t\tlocal_irq_restore(*flags);\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_lock(&sighand->siglock);\n\t\tif (likely(sighand == tsk->sighand)) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&sighand->siglock);\n\t\trcu_read_unlock();\n\t\tlocal_irq_restore(*flags);\n\t}\n\n\treturn sighand;\n}\n\n/*\n * send signal info to all the members of a group\n */\nint group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = check_kill_permission(sig, info, p);\n\trcu_read_unlock();\n\n\tif (!ret && sig)\n\t\tret = do_send_sig_info(sig, info, p, true);\n\n\treturn ret;\n}\n\n/*\n * __kill_pgrp_info() sends a signal to a process group: this is what the tty\n * control characters do (^C, ^Z etc)\n * - the caller must hold at least a readlock on tasklist_lock\n */\nint __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp)\n{\n\tstruct task_struct *p = NULL;\n\tint retval, success;\n\n\tsuccess = 0;\n\tretval = -ESRCH;\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tint err = group_send_sig_info(sig, info, p);\n\t\tsuccess |= !err;\n\t\tretval = err;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\treturn success ? 0 : retval;\n}\n\nint kill_pid_info(int sig, struct siginfo *info, struct pid *pid)\n{\n\tint error = -ESRCH;\n\tstruct task_struct *p;\n\n\trcu_read_lock();\nretry:\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (p) {\n\t\terror = group_send_sig_info(sig, info, p);\n\t\tif (unlikely(error == -ESRCH))\n\t\t\t/*\n\t\t\t * The task was unhashed in between, try again.\n\t\t\t * If it is dead, pid_task() will return NULL,\n\t\t\t * if we race with de_thread() it will find the\n\t\t\t * new leader.\n\t\t\t */\n\t\t\tgoto retry;\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nint kill_proc_info(int sig, struct siginfo *info, pid_t pid)\n{\n\tint error;\n\trcu_read_lock();\n\terror = kill_pid_info(sig, info, find_vpid(pid));\n\trcu_read_unlock();\n\treturn error;\n}\n\nstatic int kill_as_cred_perm(const struct cred *cred,\n\t\t\t     struct task_struct *target)\n{\n\tconst struct cred *pcred = __task_cred(target);\n\tif (!uid_eq(cred->euid, pcred->suid) && !uid_eq(cred->euid, pcred->uid) &&\n\t    !uid_eq(cred->uid,  pcred->suid) && !uid_eq(cred->uid,  pcred->uid))\n\t\treturn 0;\n\treturn 1;\n}\n\n/* like kill_pid_info(), but doesn't use uid/euid of \"current\" */\nint kill_pid_info_as_cred(int sig, struct siginfo *info, struct pid *pid,\n\t\t\t const struct cred *cred, u32 secid)\n{\n\tint ret = -EINVAL;\n\tstruct task_struct *p;\n\tunsigned long flags;\n\n\tif (!valid_signal(sig))\n\t\treturn ret;\n\n\trcu_read_lock();\n\tp = pid_task(pid, PIDTYPE_PID);\n\tif (!p) {\n\t\tret = -ESRCH;\n\t\tgoto out_unlock;\n\t}\n\tif (si_fromuser(info) && !kill_as_cred_perm(cred, p)) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\tret = security_task_kill(p, info, sig, secid);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tif (sig) {\n\t\tif (lock_task_sighand(p, &flags)) {\n\t\t\tret = __send_signal(sig, info, p, 1, 0);\n\t\t\tunlock_task_sighand(p, &flags);\n\t\t} else\n\t\t\tret = -ESRCH;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kill_pid_info_as_cred);\n\n/*\n * kill_something_info() interprets pid in interesting ways just like kill(2).\n *\n * POSIX specifies that kill(-1,sig) is unspecified, but what we have\n * is probably wrong.  Should make it like BSD or SYSV.\n */\n\nstatic int kill_something_info(int sig, struct siginfo *info, pid_t pid)\n{\n\tint ret;\n\n\tif (pid > 0) {\n\t\trcu_read_lock();\n\t\tret = kill_pid_info(sig, info, find_vpid(pid));\n\t\trcu_read_unlock();\n\t\treturn ret;\n\t}\n\n\tread_lock(&tasklist_lock);\n\tif (pid != -1) {\n\t\tret = __kill_pgrp_info(sig, info,\n\t\t\t\tpid ? find_vpid(-pid) : task_pgrp(current));\n\t} else {\n\t\tint retval = 0, count = 0;\n\t\tstruct task_struct * p;\n\n\t\tfor_each_process(p) {\n\t\t\tif (task_pid_vnr(p) > 1 &&\n\t\t\t\t\t!same_thread_group(p, current)) {\n\t\t\t\tint err = group_send_sig_info(sig, info, p);\n\t\t\t\t++count;\n\t\t\t\tif (err != -EPERM)\n\t\t\t\t\tretval = err;\n\t\t\t}\n\t\t}\n\t\tret = count ? retval : -ESRCH;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * These are for backward compatibility with the rest of the kernel source.\n */\n\nint send_sig_info(int sig, struct siginfo *info, struct task_struct *p)\n{\n\t/*\n\t * Make sure legacy kernel users don't send in bad values\n\t * (normal paths check this in check_kill_permission).\n\t */\n\tif (!valid_signal(sig))\n\t\treturn -EINVAL;\n\n\treturn do_send_sig_info(sig, info, p, false);\n}\n\n#define __si_special(priv) \\\n\t((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)\n\nint\nsend_sig(int sig, struct task_struct *p, int priv)\n{\n\treturn send_sig_info(sig, __si_special(priv), p);\n}\n\nvoid\nforce_sig(int sig, struct task_struct *p)\n{\n\tforce_sig_info(sig, SEND_SIG_PRIV, p);\n}\n\n/*\n * When things go south during signal handling, we\n * will force a SIGSEGV. And if the signal that caused\n * the problem was already a SIGSEGV, we'll want to\n * make sure we don't even try to deliver the signal..\n */\nint\nforce_sigsegv(int sig, struct task_struct *p)\n{\n\tif (sig == SIGSEGV) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&p->sighand->siglock, flags);\n\t\tp->sighand->action[sig - 1].sa.sa_handler = SIG_DFL;\n\t\tspin_unlock_irqrestore(&p->sighand->siglock, flags);\n\t}\n\tforce_sig(SIGSEGV, p);\n\treturn 0;\n}\n\nint kill_pgrp(struct pid *pid, int sig, int priv)\n{\n\tint ret;\n\n\tread_lock(&tasklist_lock);\n\tret = __kill_pgrp_info(sig, __si_special(priv), pid);\n\tread_unlock(&tasklist_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kill_pgrp);\n\nint kill_pid(struct pid *pid, int sig, int priv)\n{\n\treturn kill_pid_info(sig, __si_special(priv), pid);\n}\nEXPORT_SYMBOL(kill_pid);\n\n/*\n * These functions support sending signals using preallocated sigqueue\n * structures.  This is needed \"because realtime applications cannot\n * afford to lose notifications of asynchronous events, like timer\n * expirations or I/O completions\".  In the case of POSIX Timers\n * we allocate the sigqueue structure from the timer_create.  If this\n * allocation fails we are able to report the failure to the application\n * with an EAGAIN error.\n */\nstruct sigqueue *sigqueue_alloc(void)\n{\n\tstruct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);\n\n\tif (q)\n\t\tq->flags |= SIGQUEUE_PREALLOC;\n\n\treturn q;\n}\n\nvoid sigqueue_free(struct sigqueue *q)\n{\n\tunsigned long flags;\n\tspinlock_t *lock = &current->sighand->siglock;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\t/*\n\t * We must hold ->siglock while testing q->list\n\t * to serialize with collect_signal() or with\n\t * __exit_signal()->flush_sigqueue().\n\t */\n\tspin_lock_irqsave(lock, flags);\n\tq->flags &= ~SIGQUEUE_PREALLOC;\n\t/*\n\t * If it is queued it will be freed when dequeued,\n\t * like the \"regular\" sigqueue.\n\t */\n\tif (!list_empty(&q->list))\n\t\tq = NULL;\n\tspin_unlock_irqrestore(lock, flags);\n\n\tif (q)\n\t\t__sigqueue_free(q);\n}\n\nint send_sigqueue(struct sigqueue *q, struct task_struct *t, int group)\n{\n\tint sig = q->info.si_signo;\n\tstruct sigpending *pending;\n\tunsigned long flags;\n\tint ret, result;\n\n\tBUG_ON(!(q->flags & SIGQUEUE_PREALLOC));\n\n\tret = -1;\n\tif (!likely(lock_task_sighand(t, &flags)))\n\t\tgoto ret;\n\n\tret = 1; /* the signal is ignored */\n\tresult = TRACE_SIGNAL_IGNORED;\n\tif (!prepare_signal(sig, t, false))\n\t\tgoto out;\n\n\tret = 0;\n\tif (unlikely(!list_empty(&q->list))) {\n\t\t/*\n\t\t * If an SI_TIMER entry is already queue just increment\n\t\t * the overrun count.\n\t\t */\n\t\tBUG_ON(q->info.si_code != SI_TIMER);\n\t\tq->info.si_overrun++;\n\t\tresult = TRACE_SIGNAL_ALREADY_PENDING;\n\t\tgoto out;\n\t}\n\tq->info.si_overrun = 0;\n\n\tsignalfd_notify(t, sig);\n\tpending = group ? &t->signal->shared_pending : &t->pending;\n\tlist_add_tail(&q->list, &pending->list);\n\tsigaddset(&pending->signal, sig);\n\tcomplete_signal(sig, t, group);\n\tresult = TRACE_SIGNAL_DELIVERED;\nout:\n\ttrace_signal_generate(sig, &q->info, t, group, result);\n\tunlock_task_sighand(t, &flags);\nret:\n\treturn ret;\n}\n\n/*\n * Let a parent know about the death of a child.\n * For a stopped/continued status change, use do_notify_parent_cldstop instead.\n *\n * Returns true if our parent ignored us and so we've switched to\n * self-reaping.\n */\nbool do_notify_parent(struct task_struct *tsk, int sig)\n{\n\tstruct siginfo info;\n\tunsigned long flags;\n\tstruct sighand_struct *psig;\n\tbool autoreap = false;\n\tcputime_t utime, stime;\n\n\tBUG_ON(sig == -1);\n\n \t/* do_notify_parent_cldstop should have been called instead.  */\n \tBUG_ON(task_is_stopped_or_traced(tsk));\n\n\tBUG_ON(!tsk->ptrace &&\n\t       (tsk->group_leader != tsk || !thread_group_empty(tsk)));\n\n\tif (sig != SIGCHLD) {\n\t\t/*\n\t\t * This is only possible if parent == real_parent.\n\t\t * Check if it has changed security domain.\n\t\t */\n\t\tif (tsk->parent_exec_id != tsk->parent->self_exec_id)\n\t\t\tsig = SIGCHLD;\n\t}\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\t/*\n\t * We are under tasklist_lock here so our parent is tied to\n\t * us and cannot change.\n\t *\n\t * task_active_pid_ns will always return the same pid namespace\n\t * until a task passes through release_task.\n\t *\n\t * write_lock() currently calls preempt_disable() which is the\n\t * same as rcu_read_lock(), but according to Oleg, this is not\n\t * correct to rely on this\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),\n\t\t\t\t       task_uid(tsk));\n\trcu_read_unlock();\n\n\ttask_cputime(tsk, &utime, &stime);\n\tinfo.si_utime = cputime_to_clock_t(utime + tsk->signal->utime);\n\tinfo.si_stime = cputime_to_clock_t(stime + tsk->signal->stime);\n\n\tinfo.si_status = tsk->exit_code & 0x7f;\n\tif (tsk->exit_code & 0x80)\n\t\tinfo.si_code = CLD_DUMPED;\n\telse if (tsk->exit_code & 0x7f)\n\t\tinfo.si_code = CLD_KILLED;\n\telse {\n\t\tinfo.si_code = CLD_EXITED;\n\t\tinfo.si_status = tsk->exit_code >> 8;\n\t}\n\n\tpsig = tsk->parent->sighand;\n\tspin_lock_irqsave(&psig->siglock, flags);\n\tif (!tsk->ptrace && sig == SIGCHLD &&\n\t    (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||\n\t     (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {\n\t\t/*\n\t\t * We are exiting and our parent doesn't care.  POSIX.1\n\t\t * defines special semantics for setting SIGCHLD to SIG_IGN\n\t\t * or setting the SA_NOCLDWAIT flag: we should be reaped\n\t\t * automatically and not left for our parent's wait4 call.\n\t\t * Rather than having the parent do it as a magic kind of\n\t\t * signal handler, we just set this to tell do_exit that we\n\t\t * can be cleaned up without becoming a zombie.  Note that\n\t\t * we still call __wake_up_parent in this case, because a\n\t\t * blocked sys_wait4 might now return -ECHILD.\n\t\t *\n\t\t * Whether we send SIGCHLD or not for SA_NOCLDWAIT\n\t\t * is implementation-defined: we do (if you don't want\n\t\t * it, just use SIG_IGN instead).\n\t\t */\n\t\tautoreap = true;\n\t\tif (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)\n\t\t\tsig = 0;\n\t}\n\tif (valid_signal(sig) && sig)\n\t\t__group_send_sig_info(sig, &info, tsk->parent);\n\t__wake_up_parent(tsk, tsk->parent);\n\tspin_unlock_irqrestore(&psig->siglock, flags);\n\n\treturn autoreap;\n}\n\n/**\n * do_notify_parent_cldstop - notify parent of stopped/continued state change\n * @tsk: task reporting the state change\n * @for_ptracer: the notification is for ptracer\n * @why: CLD_{CONTINUED|STOPPED|TRAPPED} to report\n *\n * Notify @tsk's parent that the stopped/continued state has changed.  If\n * @for_ptracer is %false, @tsk's group leader notifies to its real parent.\n * If %true, @tsk reports to @tsk->parent which should be the ptracer.\n *\n * CONTEXT:\n * Must be called with tasklist_lock at least read locked.\n */\nstatic void do_notify_parent_cldstop(struct task_struct *tsk,\n\t\t\t\t     bool for_ptracer, int why)\n{\n\tstruct siginfo info;\n\tunsigned long flags;\n\tstruct task_struct *parent;\n\tstruct sighand_struct *sighand;\n\tcputime_t utime, stime;\n\n\tif (for_ptracer) {\n\t\tparent = tsk->parent;\n\t} else {\n\t\ttsk = tsk->group_leader;\n\t\tparent = tsk->real_parent;\n\t}\n\n\tinfo.si_signo = SIGCHLD;\n\tinfo.si_errno = 0;\n\t/*\n\t * see comment in do_notify_parent() about the following 4 lines\n\t */\n\trcu_read_lock();\n\tinfo.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));\n\tinfo.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));\n\trcu_read_unlock();\n\n\ttask_cputime(tsk, &utime, &stime);\n\tinfo.si_utime = cputime_to_clock_t(utime);\n\tinfo.si_stime = cputime_to_clock_t(stime);\n\n \tinfo.si_code = why;\n \tswitch (why) {\n \tcase CLD_CONTINUED:\n \t\tinfo.si_status = SIGCONT;\n \t\tbreak;\n \tcase CLD_STOPPED:\n \t\tinfo.si_status = tsk->signal->group_exit_code & 0x7f;\n \t\tbreak;\n \tcase CLD_TRAPPED:\n \t\tinfo.si_status = tsk->exit_code & 0x7f;\n \t\tbreak;\n \tdefault:\n \t\tBUG();\n \t}\n\n\tsighand = parent->sighand;\n\tspin_lock_irqsave(&sighand->siglock, flags);\n\tif (sighand->action[SIGCHLD-1].sa.sa_handler != SIG_IGN &&\n\t    !(sighand->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))\n\t\t__group_send_sig_info(SIGCHLD, &info, parent);\n\t/*\n\t * Even if SIGCHLD is not generated, we must wake up wait4 calls.\n\t */\n\t__wake_up_parent(tsk, parent);\n\tspin_unlock_irqrestore(&sighand->siglock, flags);\n}\n\nstatic inline int may_ptrace_stop(void)\n{\n\tif (!likely(current->ptrace))\n\t\treturn 0;\n\t/*\n\t * Are we in the middle of do_coredump?\n\t * If so and our tracer is also part of the coredump stopping\n\t * is a deadlock situation, and pointless because our tracer\n\t * is dead so don't allow us to stop.\n\t * If SIGKILL was already sent before the caller unlocked\n\t * ->siglock we must see ->core_state != NULL. Otherwise it\n\t * is safe to enter schedule().\n\t *\n\t * This is almost outdated, a task with the pending SIGKILL can't\n\t * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported\n\t * after SIGKILL was already dequeued.\n\t */\n\tif (unlikely(current->mm->core_state) &&\n\t    unlikely(current->mm == current->parent->mm))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * Return non-zero if there is a SIGKILL that should be waking us up.\n * Called with the siglock held.\n */\nstatic int sigkill_pending(struct task_struct *tsk)\n{\n\treturn\tsigismember(&tsk->pending.signal, SIGKILL) ||\n\t\tsigismember(&tsk->signal->shared_pending.signal, SIGKILL);\n}\n\n/*\n * This must be called with current->sighand->siglock held.\n *\n * This should be the path for all ptrace stops.\n * We always set current->last_siginfo while stopped here.\n * That makes it a way to test a stopped process for\n * being ptrace-stopped vs being job-control-stopped.\n *\n * If we actually decide not to stop at all because the tracer\n * is gone, we keep current->exit_code unless clear_code.\n */\nstatic void ptrace_stop(int exit_code, int why, int clear_code, siginfo_t *info)\n\t__releases(&current->sighand->siglock)\n\t__acquires(&current->sighand->siglock)\n{\n\tbool gstop_done = false;\n\n\tif (arch_ptrace_stop_needed(exit_code, info)) {\n\t\t/*\n\t\t * The arch code has something special to do before a\n\t\t * ptrace stop.  This is allowed to block, e.g. for faults\n\t\t * on user stack pages.  We can't keep the siglock while\n\t\t * calling arch_ptrace_stop, so we must release it now.\n\t\t * To preserve proper semantics, we must do this before\n\t\t * any signal bookkeeping like checking group_stop_count.\n\t\t * Meanwhile, a SIGKILL could come in before we retake the\n\t\t * siglock.  That must prevent us from sleeping in TASK_TRACED.\n\t\t * So after regaining the lock, we must check for SIGKILL.\n\t\t */\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tarch_ptrace_stop(exit_code, info);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\tif (sigkill_pending(current))\n\t\t\treturn;\n\t}\n\n\t/*\n\t * We're committing to trapping.  TRACED should be visible before\n\t * TRAPPING is cleared; otherwise, the tracer might fail do_wait().\n\t * Also, transition to TRACED and updates to ->jobctl should be\n\t * atomic with respect to siglock and should be done after the arch\n\t * hook as siglock is released and regrabbed across it.\n\t */\n\tset_current_state(TASK_TRACED);\n\n\tcurrent->last_siginfo = info;\n\tcurrent->exit_code = exit_code;\n\n\t/*\n\t * If @why is CLD_STOPPED, we're trapping to participate in a group\n\t * stop.  Do the bookkeeping.  Note that if SIGCONT was delievered\n\t * across siglock relocks since INTERRUPT was scheduled, PENDING\n\t * could be clear now.  We act as if SIGCONT is received after\n\t * TASK_TRACED is entered - ignore it.\n\t */\n\tif (why == CLD_STOPPED && (current->jobctl & JOBCTL_STOP_PENDING))\n\t\tgstop_done = task_participate_group_stop(current);\n\n\t/* any trap clears pending STOP trap, STOP trap clears NOTIFY */\n\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\tif (info && info->si_code >> 8 == PTRACE_EVENT_STOP)\n\t\ttask_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY);\n\n\t/* entering a trap, clear TRAPPING */\n\ttask_clear_jobctl_trapping(current);\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\tread_lock(&tasklist_lock);\n\tif (may_ptrace_stop()) {\n\t\t/*\n\t\t * Notify parents of the stop.\n\t\t *\n\t\t * While ptraced, there are two parents - the ptracer and\n\t\t * the real_parent of the group_leader.  The ptracer should\n\t\t * know about every stop while the real parent is only\n\t\t * interested in the completion of group stop.  The states\n\t\t * for the two don't interact with each other.  Notify\n\t\t * separately unless they're gonna be duplicates.\n\t\t */\n\t\tdo_notify_parent_cldstop(current, true, why);\n\t\tif (gstop_done && ptrace_reparented(current))\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/*\n\t\t * Don't want to allow preemption here, because\n\t\t * sys_ptrace() needs this task to be inactive.\n\t\t *\n\t\t * XXX: implement read_unlock_no_resched().\n\t\t */\n\t\tpreempt_disable();\n\t\tread_unlock(&tasklist_lock);\n\t\tpreempt_enable_no_resched();\n\t\tfreezable_schedule();\n\t} else {\n\t\t/*\n\t\t * By the time we got the lock, our tracer went away.\n\t\t * Don't drop the lock yet, another tracer may come.\n\t\t *\n\t\t * If @gstop_done, the ptracer went away between group stop\n\t\t * completion and here.  During detach, it would have set\n\t\t * JOBCTL_STOP_PENDING on us and we'll re-enter\n\t\t * TASK_STOPPED in do_signal_stop() on return, so notifying\n\t\t * the real parent of the group stop completion is enough.\n\t\t */\n\t\tif (gstop_done)\n\t\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\t/* tasklist protects us from ptrace_freeze_traced() */\n\t\t__set_current_state(TASK_RUNNING);\n\t\tif (clear_code)\n\t\t\tcurrent->exit_code = 0;\n\t\tread_unlock(&tasklist_lock);\n\t}\n\n\t/*\n\t * We are back.  Now reacquire the siglock before touching\n\t * last_siginfo, so that we are sure to have synchronized with\n\t * any signal-sending on another CPU that wants to examine it.\n\t */\n\tspin_lock_irq(&current->sighand->siglock);\n\tcurrent->last_siginfo = NULL;\n\n\t/* LISTENING can be set only during STOP traps, clear it */\n\tcurrent->jobctl &= ~JOBCTL_LISTENING;\n\n\t/*\n\t * Queued signals ignored us while we were stopped for tracing.\n\t * So check for any that we should take before resuming user mode.\n\t * This sets TIF_SIGPENDING, but never clears it.\n\t */\n\trecalc_sigpending_tsk(current);\n}\n\nstatic void ptrace_do_notify(int signr, int exit_code, int why)\n{\n\tsiginfo_t info;\n\n\tmemset(&info, 0, sizeof info);\n\tinfo.si_signo = signr;\n\tinfo.si_code = exit_code;\n\tinfo.si_pid = task_pid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\t/* Let the debugger run.  */\n\tptrace_stop(exit_code, why, 1, &info);\n}\n\nvoid ptrace_notify(int exit_code)\n{\n\tBUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED);\n\tspin_unlock_irq(&current->sighand->siglock);\n}\n\n/**\n * do_signal_stop - handle group stop for SIGSTOP and other stop signals\n * @signr: signr causing group stop if initiating\n *\n * If %JOBCTL_STOP_PENDING is not set yet, initiate group stop with @signr\n * and participate in it.  If already set, participate in the existing\n * group stop.  If participated in a group stop (and thus slept), %true is\n * returned with siglock released.\n *\n * If ptraced, this function doesn't handle stop itself.  Instead,\n * %JOBCTL_TRAP_STOP is scheduled and %false is returned with siglock\n * untouched.  The caller must ensure that INTERRUPT trap handling takes\n * places afterwards.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which is released\n * on %true return.\n *\n * RETURNS:\n * %false if group stop is already cancelled or ptrace trap is scheduled.\n * %true if participated in group stop.\n */\nstatic bool do_signal_stop(int signr)\n\t__releases(&current->sighand->siglock)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tif (!(current->jobctl & JOBCTL_STOP_PENDING)) {\n\t\tunsigned int gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;\n\t\tstruct task_struct *t;\n\n\t\t/* signr will be recorded in task->jobctl for retries */\n\t\tWARN_ON_ONCE(signr & ~JOBCTL_STOP_SIGMASK);\n\n\t\tif (!likely(current->jobctl & JOBCTL_STOP_DEQUEUED) ||\n\t\t    unlikely(signal_group_exit(sig)))\n\t\t\treturn false;\n\t\t/*\n\t\t * There is no group stop already in progress.  We must\n\t\t * initiate one now.\n\t\t *\n\t\t * While ptraced, a task may be resumed while group stop is\n\t\t * still in effect and then receive a stop signal and\n\t\t * initiate another group stop.  This deviates from the\n\t\t * usual behavior as two consecutive stop signals can't\n\t\t * cause two group stops when !ptraced.  That is why we\n\t\t * also check !task_is_stopped(t) below.\n\t\t *\n\t\t * The condition can be distinguished by testing whether\n\t\t * SIGNAL_STOP_STOPPED is already set.  Don't generate\n\t\t * group_exit_code in such case.\n\t\t *\n\t\t * This is not necessary for SIGNAL_STOP_CONTINUED because\n\t\t * an intervening stop signal is required to cause two\n\t\t * continued events regardless of ptrace.\n\t\t */\n\t\tif (!(sig->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsig->group_exit_code = signr;\n\n\t\tsig->group_stop_count = 0;\n\n\t\tif (task_set_jobctl_pending(current, signr | gstop))\n\t\t\tsig->group_stop_count++;\n\n\t\tfor (t = next_thread(current); t != current;\n\t\t     t = next_thread(t)) {\n\t\t\t/*\n\t\t\t * Setting state to TASK_STOPPED for a group\n\t\t\t * stop is always done with the siglock held,\n\t\t\t * so this check has no races.\n\t\t\t */\n\t\t\tif (!task_is_stopped(t) &&\n\t\t\t    task_set_jobctl_pending(t, signr | gstop)) {\n\t\t\t\tsig->group_stop_count++;\n\t\t\t\tif (likely(!(t->ptrace & PT_SEIZED)))\n\t\t\t\t\tsignal_wake_up(t, 0);\n\t\t\t\telse\n\t\t\t\t\tptrace_trap_notify(t);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (likely(!current->ptrace)) {\n\t\tint notify = 0;\n\n\t\t/*\n\t\t * If there are no other threads in the group, or if there\n\t\t * is a group stop in progress and we are the last to stop,\n\t\t * report to the parent.\n\t\t */\n\t\tif (task_participate_group_stop(current))\n\t\t\tnotify = CLD_STOPPED;\n\n\t\t__set_current_state(TASK_STOPPED);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent of the group stop completion.  Because\n\t\t * we're not holding either the siglock or tasklist_lock\n\t\t * here, ptracer may attach inbetween; however, this is for\n\t\t * group stop and should always be delivered to the real\n\t\t * parent of the group leader.  The new ptracer will get\n\t\t * its notification when this task transitions into\n\t\t * TASK_TRACED.\n\t\t */\n\t\tif (notify) {\n\t\t\tread_lock(&tasklist_lock);\n\t\t\tdo_notify_parent_cldstop(current, false, notify);\n\t\t\tread_unlock(&tasklist_lock);\n\t\t}\n\n\t\t/* Now we don't run again until woken by SIGCONT or SIGKILL */\n\t\tfreezable_schedule();\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * While ptraced, group stop is handled by STOP trap.\n\t\t * Schedule it and let the caller deal with it.\n\t\t */\n\t\ttask_set_jobctl_pending(current, JOBCTL_TRAP_STOP);\n\t\treturn false;\n\t}\n}\n\n/**\n * do_jobctl_trap - take care of ptrace jobctl traps\n *\n * When PT_SEIZED, it's used for both group stop and explicit\n * SEIZE/INTERRUPT traps.  Both generate PTRACE_EVENT_STOP trap with\n * accompanying siginfo.  If stopped, lower eight bits of exit_code contain\n * the stop signal; otherwise, %SIGTRAP.\n *\n * When !PT_SEIZED, it's used only for group stop trap with stop signal\n * number as exit_code and no siginfo.\n *\n * CONTEXT:\n * Must be called with @current->sighand->siglock held, which may be\n * released and re-acquired before returning with intervening sleep.\n */\nstatic void do_jobctl_trap(void)\n{\n\tstruct signal_struct *signal = current->signal;\n\tint signr = current->jobctl & JOBCTL_STOP_SIGMASK;\n\n\tif (current->ptrace & PT_SEIZED) {\n\t\tif (!signal->group_stop_count &&\n\t\t    !(signal->flags & SIGNAL_STOP_STOPPED))\n\t\t\tsignr = SIGTRAP;\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),\n\t\t\t\t CLD_STOPPED);\n\t} else {\n\t\tWARN_ON_ONCE(!signr);\n\t\tptrace_stop(signr, CLD_STOPPED, 0, NULL);\n\t\tcurrent->exit_code = 0;\n\t}\n}\n\nstatic int ptrace_signal(int signr, siginfo_t *info)\n{\n\tptrace_signal_deliver();\n\t/*\n\t * We do not check sig_kernel_stop(signr) but set this marker\n\t * unconditionally because we do not know whether debugger will\n\t * change signr. This flag has no meaning unless we are going\n\t * to stop after return from ptrace_stop(). In this case it will\n\t * be checked in do_signal_stop(), we should only stop if it was\n\t * not cleared by SIGCONT while we were sleeping. See also the\n\t * comment in dequeue_signal().\n\t */\n\tcurrent->jobctl |= JOBCTL_STOP_DEQUEUED;\n\tptrace_stop(signr, CLD_TRAPPED, 0, info);\n\n\t/* We're back.  Did the debugger cancel the sig?  */\n\tsignr = current->exit_code;\n\tif (signr == 0)\n\t\treturn signr;\n\n\tcurrent->exit_code = 0;\n\n\t/*\n\t * Update the siginfo structure if the signal has\n\t * changed.  If the debugger wanted something\n\t * specific in the siginfo structure then it should\n\t * have updated *info via PTRACE_SETSIGINFO.\n\t */\n\tif (signr != info->si_signo) {\n\t\tinfo->si_signo = signr;\n\t\tinfo->si_errno = 0;\n\t\tinfo->si_code = SI_USER;\n\t\trcu_read_lock();\n\t\tinfo->si_pid = task_pid_vnr(current->parent);\n\t\tinfo->si_uid = from_kuid_munged(current_user_ns(),\n\t\t\t\t\t\ttask_uid(current->parent));\n\t\trcu_read_unlock();\n\t}\n\n\t/* If the (new) signal is now blocked, requeue it.  */\n\tif (sigismember(&current->blocked, signr)) {\n\t\tspecific_send_sig_info(signr, info, current);\n\t\tsignr = 0;\n\t}\n\n\treturn signr;\n}\n\nint get_signal_to_deliver(siginfo_t *info, struct k_sigaction *return_ka,\n\t\t\t  struct pt_regs *regs, void *cookie)\n{\n\tstruct sighand_struct *sighand = current->sighand;\n\tstruct signal_struct *signal = current->signal;\n\tint signr;\n\n\tif (unlikely(current->task_works))\n\t\ttask_work_run();\n\n\tif (unlikely(uprobe_deny_signal()))\n\t\treturn 0;\n\n\t/*\n\t * Do this once, we can't return to user-mode if freezing() == T.\n\t * do_signal_stop() and ptrace_stop() do freezable_schedule() and\n\t * thus do not need another check after return.\n\t */\n\ttry_to_freeze();\n\nrelock:\n\tspin_lock_irq(&sighand->siglock);\n\t/*\n\t * Every stopped thread goes here after wakeup. Check to see if\n\t * we should notify the parent, prepare_signal(SIGCONT) encodes\n\t * the CLD_ si_code into SIGNAL_CLD_MASK bits.\n\t */\n\tif (unlikely(signal->flags & SIGNAL_CLD_MASK)) {\n\t\tint why;\n\n\t\tif (signal->flags & SIGNAL_CLD_CONTINUED)\n\t\t\twhy = CLD_CONTINUED;\n\t\telse\n\t\t\twhy = CLD_STOPPED;\n\n\t\tsignal->flags &= ~SIGNAL_CLD_MASK;\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Notify the parent that we're continuing.  This event is\n\t\t * always per-process and doesn't make whole lot of sense\n\t\t * for ptracers, who shouldn't consume the state via\n\t\t * wait(2) either, but, for backward compatibility, notify\n\t\t * the ptracer of the group leader too unless it's gonna be\n\t\t * a duplicate.\n\t\t */\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(current, false, why);\n\n\t\tif (ptrace_reparented(current->group_leader))\n\t\t\tdo_notify_parent_cldstop(current->group_leader,\n\t\t\t\t\t\ttrue, why);\n\t\tread_unlock(&tasklist_lock);\n\n\t\tgoto relock;\n\t}\n\n\tfor (;;) {\n\t\tstruct k_sigaction *ka;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&\n\t\t    do_signal_stop(0))\n\t\t\tgoto relock;\n\n\t\tif (unlikely(current->jobctl & JOBCTL_TRAP_MASK)) {\n\t\t\tdo_jobctl_trap();\n\t\t\tspin_unlock_irq(&sighand->siglock);\n\t\t\tgoto relock;\n\t\t}\n\n\t\tsignr = dequeue_signal(current, &current->blocked, info);\n\n\t\tif (!signr)\n\t\t\tbreak; /* will return 0 */\n\n\t\tif (unlikely(current->ptrace) && signr != SIGKILL) {\n\t\t\tsignr = ptrace_signal(signr, info);\n\t\t\tif (!signr)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tka = &sighand->action[signr-1];\n\n\t\t/* Trace actually delivered signals. */\n\t\ttrace_signal_deliver(signr, info, ka);\n\n\t\tif (ka->sa.sa_handler == SIG_IGN) /* Do nothing.  */\n\t\t\tcontinue;\n\t\tif (ka->sa.sa_handler != SIG_DFL) {\n\t\t\t/* Run the handler.  */\n\t\t\t*return_ka = *ka;\n\n\t\t\tif (ka->sa.sa_flags & SA_ONESHOT)\n\t\t\t\tka->sa.sa_handler = SIG_DFL;\n\n\t\t\tbreak; /* will return non-zero \"signr\" value */\n\t\t}\n\n\t\t/*\n\t\t * Now we are doing the default action for this signal.\n\t\t */\n\t\tif (sig_kernel_ignore(signr)) /* Default is nothing. */\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Global init gets no signals it doesn't want.\n\t\t * Container-init gets no signals it doesn't want from same\n\t\t * container.\n\t\t *\n\t\t * Note that if global/container-init sees a sig_kernel_only()\n\t\t * signal here, the signal must have been generated internally\n\t\t * or must have come from an ancestor namespace. In either\n\t\t * case, the signal cannot be dropped.\n\t\t */\n\t\tif (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&\n\t\t\t\t!sig_kernel_only(signr))\n\t\t\tcontinue;\n\n\t\tif (sig_kernel_stop(signr)) {\n\t\t\t/*\n\t\t\t * The default action is to stop all threads in\n\t\t\t * the thread group.  The job control signals\n\t\t\t * do nothing in an orphaned pgrp, but SIGSTOP\n\t\t\t * always works.  Note that siglock needs to be\n\t\t\t * dropped during the call to is_orphaned_pgrp()\n\t\t\t * because of lock ordering with tasklist_lock.\n\t\t\t * This allows an intervening SIGCONT to be posted.\n\t\t\t * We need to check for that and bail out if necessary.\n\t\t\t */\n\t\t\tif (signr != SIGSTOP) {\n\t\t\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t\t\t/* signals can be posted during this window */\n\n\t\t\t\tif (is_current_pgrp_orphaned())\n\t\t\t\t\tgoto relock;\n\n\t\t\t\tspin_lock_irq(&sighand->siglock);\n\t\t\t}\n\n\t\t\tif (likely(do_signal_stop(info->si_signo))) {\n\t\t\t\t/* It released the siglock.  */\n\t\t\t\tgoto relock;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We didn't actually stop, due to a race\n\t\t\t * with SIGCONT or something like that.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tspin_unlock_irq(&sighand->siglock);\n\n\t\t/*\n\t\t * Anything else is fatal, maybe with a core dump.\n\t\t */\n\t\tcurrent->flags |= PF_SIGNALED;\n\n\t\tif (sig_kernel_coredump(signr)) {\n\t\t\tif (print_fatal_signals)\n\t\t\t\tprint_fatal_signal(info->si_signo);\n\t\t\t/*\n\t\t\t * If it was able to dump core, this kills all\n\t\t\t * other threads in the group and synchronizes with\n\t\t\t * their demise.  If we lost the race with another\n\t\t\t * thread getting here, it set group_exit_code\n\t\t\t * first and our do_group_exit call below will use\n\t\t\t * that value and ignore the one we pass it.\n\t\t\t */\n\t\t\tdo_coredump(info);\n\t\t}\n\n\t\t/*\n\t\t * Death signals, no core dump.\n\t\t */\n\t\tdo_group_exit(info->si_signo);\n\t\t/* NOTREACHED */\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n\treturn signr;\n}\n\n/**\n * signal_delivered - \n * @sig:\t\tnumber of signal being delivered\n * @info:\t\tsiginfo_t of signal being delivered\n * @ka:\t\t\tsigaction setting that chose the handler\n * @regs:\t\tuser register state\n * @stepping:\t\tnonzero if debugger single-step or block-step in use\n *\n * This function should be called when a signal has succesfully been\n * delivered. It updates the blocked signals accordingly (@ka->sa.sa_mask\n * is always blocked, and the signal itself is blocked unless %SA_NODEFER\n * is set in @ka->sa.sa_flags.  Tracing is notified.\n */\nvoid signal_delivered(int sig, siginfo_t *info, struct k_sigaction *ka,\n\t\t\tstruct pt_regs *regs, int stepping)\n{\n\tsigset_t blocked;\n\n\t/* A signal was successfully delivered, and the\n\t   saved sigmask was stored on the signal frame,\n\t   and will be restored by sigreturn.  So we can\n\t   simply clear the restore sigmask flag.  */\n\tclear_restore_sigmask();\n\n\tsigorsets(&blocked, &current->blocked, &ka->sa.sa_mask);\n\tif (!(ka->sa.sa_flags & SA_NODEFER))\n\t\tsigaddset(&blocked, sig);\n\tset_current_blocked(&blocked);\n\ttracehook_signal_handler(sig, info, ka, regs, stepping);\n}\n\nvoid signal_setup_done(int failed, struct ksignal *ksig, int stepping)\n{\n\tif (failed)\n\t\tforce_sigsegv(ksig->sig, current);\n\telse\n\t\tsignal_delivered(ksig->sig, &ksig->info, &ksig->ka,\n\t\t\tsignal_pt_regs(), stepping);\n}\n\n/*\n * It could be that complete_signal() picked us to notify about the\n * group-wide signal. Other threads should be notified now to take\n * the shared signals in @which since we will not.\n */\nstatic void retarget_shared_pending(struct task_struct *tsk, sigset_t *which)\n{\n\tsigset_t retarget;\n\tstruct task_struct *t;\n\n\tsigandsets(&retarget, &tsk->signal->shared_pending.signal, which);\n\tif (sigisemptyset(&retarget))\n\t\treturn;\n\n\tt = tsk;\n\twhile_each_thread(tsk, t) {\n\t\tif (t->flags & PF_EXITING)\n\t\t\tcontinue;\n\n\t\tif (!has_pending_signals(&retarget, &t->blocked))\n\t\t\tcontinue;\n\t\t/* Remove the signals this thread can handle. */\n\t\tsigandsets(&retarget, &retarget, &t->blocked);\n\n\t\tif (!signal_pending(t))\n\t\t\tsignal_wake_up(t, 0);\n\n\t\tif (sigisemptyset(&retarget))\n\t\t\tbreak;\n\t}\n}\n\nvoid exit_signals(struct task_struct *tsk)\n{\n\tint group_stop = 0;\n\tsigset_t unblocked;\n\n\t/*\n\t * @tsk is about to have PF_EXITING set - lock out users which\n\t * expect stable threadgroup.\n\t */\n\tthreadgroup_change_begin(tsk);\n\n\tif (thread_group_empty(tsk) || signal_group_exit(tsk->signal)) {\n\t\ttsk->flags |= PF_EXITING;\n\t\tthreadgroup_change_end(tsk);\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t/*\n\t * From now this task is not visible for group-wide signals,\n\t * see wants_signal(), do_signal_stop().\n\t */\n\ttsk->flags |= PF_EXITING;\n\n\tthreadgroup_change_end(tsk);\n\n\tif (!signal_pending(tsk))\n\t\tgoto out;\n\n\tunblocked = tsk->blocked;\n\tsignotset(&unblocked);\n\tretarget_shared_pending(tsk, &unblocked);\n\n\tif (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&\n\t    task_participate_group_stop(tsk))\n\t\tgroup_stop = CLD_STOPPED;\nout:\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t/*\n\t * If group stop has completed, deliver the notification.  This\n\t * should always go to the real parent of the group leader.\n\t */\n\tif (unlikely(group_stop)) {\n\t\tread_lock(&tasklist_lock);\n\t\tdo_notify_parent_cldstop(tsk, false, group_stop);\n\t\tread_unlock(&tasklist_lock);\n\t}\n}\n\nEXPORT_SYMBOL(recalc_sigpending);\nEXPORT_SYMBOL_GPL(dequeue_signal);\nEXPORT_SYMBOL(flush_signals);\nEXPORT_SYMBOL(force_sig);\nEXPORT_SYMBOL(send_sig);\nEXPORT_SYMBOL(send_sig_info);\nEXPORT_SYMBOL(sigprocmask);\nEXPORT_SYMBOL(block_all_signals);\nEXPORT_SYMBOL(unblock_all_signals);\n\n\n/*\n * System call entry points.\n */\n\n/**\n *  sys_restart_syscall - restart a system call\n */\nSYSCALL_DEFINE0(restart_syscall)\n{\n\tstruct restart_block *restart = &current_thread_info()->restart_block;\n\treturn restart->fn(restart);\n}\n\nlong do_no_restart_syscall(struct restart_block *param)\n{\n\treturn -EINTR;\n}\n\nstatic void __set_task_blocked(struct task_struct *tsk, const sigset_t *newset)\n{\n\tif (signal_pending(tsk) && !thread_group_empty(tsk)) {\n\t\tsigset_t newblocked;\n\t\t/* A set of now blocked but previously unblocked signals. */\n\t\tsigandnsets(&newblocked, newset, &current->blocked);\n\t\tretarget_shared_pending(tsk, &newblocked);\n\t}\n\ttsk->blocked = *newset;\n\trecalc_sigpending();\n}\n\n/**\n * set_current_blocked - change current->blocked mask\n * @newset: new mask\n *\n * It is wrong to change ->blocked directly, this helper should be used\n * to ensure the process can't miss a shared signal we are going to block.\n */\nvoid set_current_blocked(sigset_t *newset)\n{\n\tsigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t__set_current_blocked(newset);\n}\n\nvoid __set_current_blocked(const sigset_t *newset)\n{\n\tstruct task_struct *tsk = current;\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\t__set_task_blocked(tsk, newset);\n\tspin_unlock_irq(&tsk->sighand->siglock);\n}\n\n/*\n * This is also useful for kernel threads that want to temporarily\n * (or permanently) block certain signals.\n *\n * NOTE! Unlike the user-mode sys_sigprocmask(), the kernel\n * interface happily blocks \"unblockable\" signals like SIGKILL\n * and friends.\n */\nint sigprocmask(int how, sigset_t *set, sigset_t *oldset)\n{\n\tstruct task_struct *tsk = current;\n\tsigset_t newset;\n\n\t/* Lockless, only current can change ->blocked, never from irq */\n\tif (oldset)\n\t\t*oldset = tsk->blocked;\n\n\tswitch (how) {\n\tcase SIG_BLOCK:\n\t\tsigorsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_UNBLOCK:\n\t\tsigandnsets(&newset, &tsk->blocked, set);\n\t\tbreak;\n\tcase SIG_SETMASK:\n\t\tnewset = *set;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t__set_current_blocked(&newset);\n\treturn 0;\n}\n\n/**\n *  sys_rt_sigprocmask - change the list of currently blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: stores pending signals\n *  @oset: previous value of signal mask if non-null\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigprocmask, int, how, sigset_t __user *, nset,\n\t\tsigset_t __user *, oset, size_t, sigsetsize)\n{\n\tsigset_t old_set, new_set;\n\tint error;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\told_set = current->blocked;\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(sigset_t)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigprocmask, int, how, compat_sigset_t __user *, nset,\n\t\tcompat_sigset_t __user *, oset, compat_size_t, sigsetsize)\n{\n#ifdef __BIG_ENDIAN\n\tsigset_t old_set = current->blocked;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (nset) {\n\t\tcompat_sigset_t new32;\n\t\tsigset_t new_set;\n\t\tint error;\n\t\tif (copy_from_user(&new32, nset, sizeof(compat_sigset_t)))\n\t\t\treturn -EFAULT;\n\n\t\tsigset_from_compat(&new_set, &new32);\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\terror = sigprocmask(how, &new_set, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif (oset) {\n\t\tcompat_sigset_t old32;\n\t\tsigset_to_compat(&old32, &old_set);\n\t\tif (copy_to_user(oset, &old32, sizeof(compat_sigset_t)))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n#else\n\treturn sys_rt_sigprocmask(how, (sigset_t __user *)nset,\n\t\t\t\t  (sigset_t __user *)oset, sigsetsize);\n#endif\n}\n#endif\n\nstatic int do_sigpending(void *set, unsigned long sigsetsize)\n{\n\tif (sigsetsize > sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tsigorsets(set, &current->pending.signal,\n\t\t  &current->signal->shared_pending.signal);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\t/* Outside the lock because only this thread touches it.  */\n\tsigandsets(set, &current->blocked, set);\n\treturn 0;\n}\n\n/**\n *  sys_rt_sigpending - examine a pending signal that has been raised\n *\t\t\twhile blocked\n *  @uset: stores pending signals\n *  @sigsetsize: size of sigset_t type or larger\n */\nSYSCALL_DEFINE2(rt_sigpending, sigset_t __user *, uset, size_t, sigsetsize)\n{\n\tsigset_t set;\n\tint err = do_sigpending(&set, sigsetsize);\n\tif (!err && copy_to_user(uset, &set, sigsetsize))\n\t\terr = -EFAULT;\n\treturn err;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigpending, compat_sigset_t __user *, uset,\n\t\tcompat_size_t, sigsetsize)\n{\n#ifdef __BIG_ENDIAN\n\tsigset_t set;\n\tint err = do_sigpending(&set, sigsetsize);\n\tif (!err) {\n\t\tcompat_sigset_t set32;\n\t\tsigset_to_compat(&set32, &set);\n\t\t/* we can get here only if sigsetsize <= sizeof(set) */\n\t\tif (copy_to_user(uset, &set32, sigsetsize))\n\t\t\terr = -EFAULT;\n\t}\n\treturn err;\n#else\n\treturn sys_rt_sigpending((sigset_t __user *)uset, sigsetsize);\n#endif\n}\n#endif\n\n#ifndef HAVE_ARCH_COPY_SIGINFO_TO_USER\n\nint copy_siginfo_to_user(siginfo_t __user *to, siginfo_t *from)\n{\n\tint err;\n\n\tif (!access_ok (VERIFY_WRITE, to, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\tif (from->si_code < 0)\n\t\treturn __copy_to_user(to, from, sizeof(siginfo_t))\n\t\t\t? -EFAULT : 0;\n\t/*\n\t * If you change siginfo_t structure, please be sure\n\t * this code is fixed accordingly.\n\t * Please remember to update the signalfd_copyinfo() function\n\t * inside fs/signalfd.c too, in case siginfo_t changes.\n\t * It should never copy any pad contained in the structure\n\t * to avoid security leaks, but must copy the generic\n\t * 3 ints plus the relevant union member.\n\t */\n\terr = __put_user(from->si_signo, &to->si_signo);\n\terr |= __put_user(from->si_errno, &to->si_errno);\n\terr |= __put_user((short)from->si_code, &to->si_code);\n\tswitch (from->si_code & __SI_MASK) {\n\tcase __SI_KILL:\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\tbreak;\n\tcase __SI_TIMER:\n\t\t err |= __put_user(from->si_tid, &to->si_tid);\n\t\t err |= __put_user(from->si_overrun, &to->si_overrun);\n\t\t err |= __put_user(from->si_ptr, &to->si_ptr);\n\t\tbreak;\n\tcase __SI_POLL:\n\t\terr |= __put_user(from->si_band, &to->si_band);\n\t\terr |= __put_user(from->si_fd, &to->si_fd);\n\t\tbreak;\n\tcase __SI_FAULT:\n\t\terr |= __put_user(from->si_addr, &to->si_addr);\n#ifdef __ARCH_SI_TRAPNO\n\t\terr |= __put_user(from->si_trapno, &to->si_trapno);\n#endif\n#ifdef BUS_MCEERR_AO\n\t\t/*\n\t\t * Other callers might not initialize the si_lsb field,\n\t\t * so check explicitly for the right codes here.\n\t\t */\n\t\tif (from->si_code == BUS_MCEERR_AR || from->si_code == BUS_MCEERR_AO)\n\t\t\terr |= __put_user(from->si_addr_lsb, &to->si_addr_lsb);\n#endif\n\t\tbreak;\n\tcase __SI_CHLD:\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\terr |= __put_user(from->si_status, &to->si_status);\n\t\terr |= __put_user(from->si_utime, &to->si_utime);\n\t\terr |= __put_user(from->si_stime, &to->si_stime);\n\t\tbreak;\n\tcase __SI_RT: /* This is not generated by the kernel as of now. */\n\tcase __SI_MESGQ: /* But this is */\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\terr |= __put_user(from->si_ptr, &to->si_ptr);\n\t\tbreak;\n#ifdef __ARCH_SIGSYS\n\tcase __SI_SYS:\n\t\terr |= __put_user(from->si_call_addr, &to->si_call_addr);\n\t\terr |= __put_user(from->si_syscall, &to->si_syscall);\n\t\terr |= __put_user(from->si_arch, &to->si_arch);\n\t\tbreak;\n#endif\n\tdefault: /* this is just in case for now ... */\n\t\terr |= __put_user(from->si_pid, &to->si_pid);\n\t\terr |= __put_user(from->si_uid, &to->si_uid);\n\t\tbreak;\n\t}\n\treturn err;\n}\n\n#endif\n\n/**\n *  do_sigtimedwait - wait for queued signals specified in @which\n *  @which: queued signals to wait for\n *  @info: if non-null, the signal's siginfo is returned here\n *  @ts: upper bound on process time suspension\n */\nint do_sigtimedwait(const sigset_t *which, siginfo_t *info,\n\t\t\tconst struct timespec *ts)\n{\n\tstruct task_struct *tsk = current;\n\tlong timeout = MAX_SCHEDULE_TIMEOUT;\n\tsigset_t mask = *which;\n\tint sig;\n\n\tif (ts) {\n\t\tif (!timespec_valid(ts))\n\t\t\treturn -EINVAL;\n\t\ttimeout = timespec_to_jiffies(ts);\n\t\t/*\n\t\t * We can be close to the next tick, add another one\n\t\t * to ensure we will wait at least the time asked for.\n\t\t */\n\t\tif (ts->tv_sec || ts->tv_nsec)\n\t\t\ttimeout++;\n\t}\n\n\t/*\n\t * Invert the set of allowed signals to get those we want to block.\n\t */\n\tsigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));\n\tsignotset(&mask);\n\n\tspin_lock_irq(&tsk->sighand->siglock);\n\tsig = dequeue_signal(tsk, &mask, info);\n\tif (!sig && timeout) {\n\t\t/*\n\t\t * None ready, temporarily unblock those we're interested\n\t\t * while we are sleeping in so that we'll be awakened when\n\t\t * they arrive. Unblocking is always fine, we can avoid\n\t\t * set_current_blocked().\n\t\t */\n\t\ttsk->real_blocked = tsk->blocked;\n\t\tsigandsets(&tsk->blocked, &tsk->blocked, &mask);\n\t\trecalc_sigpending();\n\t\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\n\t\tspin_lock_irq(&tsk->sighand->siglock);\n\t\t__set_task_blocked(tsk, &tsk->real_blocked);\n\t\tsiginitset(&tsk->real_blocked, 0);\n\t\tsig = dequeue_signal(tsk, &mask, info);\n\t}\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\tif (sig)\n\t\treturn sig;\n\treturn timeout ? -EINTR : -EAGAIN;\n}\n\n/**\n *  sys_rt_sigtimedwait - synchronously wait for queued signals specified\n *\t\t\tin @uthese\n *  @uthese: queued signals to wait for\n *  @uinfo: if non-null, the signal's siginfo is returned here\n *  @uts: upper bound on process time suspension\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigtimedwait, const sigset_t __user *, uthese,\n\t\tsiginfo_t __user *, uinfo, const struct timespec __user *, uts,\n\t\tsize_t, sigsetsize)\n{\n\tsigset_t these;\n\tstruct timespec ts;\n\tsiginfo_t info;\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&these, uthese, sizeof(these)))\n\t\treturn -EFAULT;\n\n\tif (uts) {\n\t\tif (copy_from_user(&ts, uts, sizeof(ts)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);\n\n\tif (ret > 0 && uinfo) {\n\t\tif (copy_siginfo_to_user(uinfo, &info))\n\t\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\n/**\n *  sys_kill - send a signal to a process\n *  @pid: the PID of the process\n *  @sig: signal to be sent\n */\nSYSCALL_DEFINE2(kill, pid_t, pid, int, sig)\n{\n\tstruct siginfo info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_USER;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn kill_something_info(sig, &info, pid);\n}\n\nstatic int\ndo_send_specific(pid_t tgid, pid_t pid, int sig, struct siginfo *info)\n{\n\tstruct task_struct *p;\n\tint error = -ESRCH;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p && (tgid <= 0 || task_tgid_vnr(p) == tgid)) {\n\t\terror = check_kill_permission(sig, info, p);\n\t\t/*\n\t\t * The null signal is a permissions and process existence\n\t\t * probe.  No signal is actually delivered.\n\t\t */\n\t\tif (!error && sig) {\n\t\t\terror = do_send_sig_info(sig, info, p, false);\n\t\t\t/*\n\t\t\t * If lock_task_sighand() failed we pretend the task\n\t\t\t * dies after receiving the signal. The window is tiny,\n\t\t\t * and the signal is private anyway.\n\t\t\t */\n\t\t\tif (unlikely(error == -ESRCH))\n\t\t\t\terror = 0;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\nstatic int do_tkill(pid_t tgid, pid_t pid, int sig)\n{\n\tstruct siginfo info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code = SI_TKILL;\n\tinfo.si_pid = task_tgid_vnr(current);\n\tinfo.si_uid = from_kuid_munged(current_user_ns(), current_uid());\n\n\treturn do_send_specific(tgid, pid, sig, &info);\n}\n\n/**\n *  sys_tgkill - send signal to one specific thread\n *  @tgid: the thread group ID of the thread\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *\n *  This syscall also checks the @tgid and returns -ESRCH even if the PID\n *  exists but it's not belonging to the target process anymore. This\n *  method solves the problem of threads exiting and PIDs getting reused.\n */\nSYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(tgid, pid, sig);\n}\n\n/**\n *  sys_tkill - send signal to one specific task\n *  @pid: the PID of the task\n *  @sig: signal to be sent\n *\n *  Send a signal to only one task, even if it's a CLONE_THREAD task.\n */\nSYSCALL_DEFINE2(tkill, pid_t, pid, int, sig)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_tkill(0, pid, sig);\n}\n\nstatic int do_rt_sigqueueinfo(pid_t pid, int sig, siginfo_t *info)\n{\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif ((info->si_code >= 0 || info->si_code == SI_TKILL) &&\n\t    (task_pid_vnr(current) != pid)) {\n\t\t/* We used to allow any < 0 si_code */\n\t\tWARN_ON_ONCE(info->si_code < 0);\n\t\treturn -EPERM;\n\t}\n\tinfo->si_signo = sig;\n\n\t/* POSIX.1b doesn't mention process groups.  */\n\treturn kill_proc_info(sig, info, pid);\n}\n\n/**\n *  sys_rt_sigqueueinfo - send signal information to a signal\n *  @pid: the PID of the thread\n *  @sig: signal to be sent\n *  @uinfo: signal info to be sent\n */\nSYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tsiginfo_t info;\n\tif (copy_from_user(&info, uinfo, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\treturn do_rt_sigqueueinfo(pid, sig, &info);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(rt_sigqueueinfo,\n\t\t\tcompat_pid_t, pid,\n\t\t\tint, sig,\n\t\t\tstruct compat_siginfo __user *, uinfo)\n{\n\tsiginfo_t info;\n\tint ret = copy_siginfo_from_user32(&info, uinfo);\n\tif (unlikely(ret))\n\t\treturn ret;\n\treturn do_rt_sigqueueinfo(pid, sig, &info);\n}\n#endif\n\nstatic int do_rt_tgsigqueueinfo(pid_t tgid, pid_t pid, int sig, siginfo_t *info)\n{\n\t/* This is only valid for single tasks */\n\tif (pid <= 0 || tgid <= 0)\n\t\treturn -EINVAL;\n\n\t/* Not even root can pretend to send signals from the kernel.\n\t * Nor can they impersonate a kill()/tgkill(), which adds source info.\n\t */\n\tif (((info->si_code >= 0 || info->si_code == SI_TKILL)) &&\n\t    (task_pid_vnr(current) != pid)) {\n\t\t/* We used to allow any < 0 si_code */\n\t\tWARN_ON_ONCE(info->si_code < 0);\n\t\treturn -EPERM;\n\t}\n\tinfo->si_signo = sig;\n\n\treturn do_send_specific(tgid, pid, sig, info);\n}\n\nSYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid, int, sig,\n\t\tsiginfo_t __user *, uinfo)\n{\n\tsiginfo_t info;\n\n\tif (copy_from_user(&info, uinfo, sizeof(siginfo_t)))\n\t\treturn -EFAULT;\n\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_tgsigqueueinfo,\n\t\t\tcompat_pid_t, tgid,\n\t\t\tcompat_pid_t, pid,\n\t\t\tint, sig,\n\t\t\tstruct compat_siginfo __user *, uinfo)\n{\n\tsiginfo_t info;\n\n\tif (copy_siginfo_from_user32(&info, uinfo))\n\t\treturn -EFAULT;\n\treturn do_rt_tgsigqueueinfo(tgid, pid, sig, &info);\n}\n#endif\n\nint do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)\n{\n\tstruct task_struct *t = current;\n\tstruct k_sigaction *k;\n\tsigset_t mask;\n\n\tif (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))\n\t\treturn -EINVAL;\n\n\tk = &t->sighand->action[sig-1];\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (oact)\n\t\t*oact = *k;\n\n\tif (act) {\n\t\tsigdelsetmask(&act->sa.sa_mask,\n\t\t\t      sigmask(SIGKILL) | sigmask(SIGSTOP));\n\t\t*k = *act;\n\t\t/*\n\t\t * POSIX 3.3.1.3:\n\t\t *  \"Setting a signal action to SIG_IGN for a signal that is\n\t\t *   pending shall cause the pending signal to be discarded,\n\t\t *   whether or not it is blocked.\"\n\t\t *\n\t\t *  \"Setting a signal action to SIG_DFL for a signal that is\n\t\t *   pending and whose default action is to ignore the signal\n\t\t *   (for example, SIGCHLD), shall cause the pending signal to\n\t\t *   be discarded, whether or not it is blocked\"\n\t\t */\n\t\tif (sig_handler_ignored(sig_handler(t, sig), sig)) {\n\t\t\tsigemptyset(&mask);\n\t\t\tsigaddset(&mask, sig);\n\t\t\trm_from_queue_full(&mask, &t->signal->shared_pending);\n\t\t\tdo {\n\t\t\t\trm_from_queue_full(&mask, &t->pending);\n\t\t\t\tt = next_thread(t);\n\t\t\t} while (t != current);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn 0;\n}\n\nstatic int \ndo_sigaltstack (const stack_t __user *uss, stack_t __user *uoss, unsigned long sp)\n{\n\tstack_t oss;\n\tint error;\n\n\toss.ss_sp = (void __user *) current->sas_ss_sp;\n\toss.ss_size = current->sas_ss_size;\n\toss.ss_flags = sas_ss_flags(sp);\n\n\tif (uss) {\n\t\tvoid __user *ss_sp;\n\t\tsize_t ss_size;\n\t\tint ss_flags;\n\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_READ, uss, sizeof(*uss)))\n\t\t\tgoto out;\n\t\terror = __get_user(ss_sp, &uss->ss_sp) |\n\t\t\t__get_user(ss_flags, &uss->ss_flags) |\n\t\t\t__get_user(ss_size, &uss->ss_size);\n\t\tif (error)\n\t\t\tgoto out;\n\n\t\terror = -EPERM;\n\t\tif (on_sig_stack(sp))\n\t\t\tgoto out;\n\n\t\terror = -EINVAL;\n\t\t/*\n\t\t * Note - this code used to test ss_flags incorrectly:\n\t\t *  \t  old code may have been written using ss_flags==0\n\t\t *\t  to mean ss_flags==SS_ONSTACK (as this was the only\n\t\t *\t  way that worked) - this fix preserves that older\n\t\t *\t  mechanism.\n\t\t */\n\t\tif (ss_flags != SS_DISABLE && ss_flags != SS_ONSTACK && ss_flags != 0)\n\t\t\tgoto out;\n\n\t\tif (ss_flags == SS_DISABLE) {\n\t\t\tss_size = 0;\n\t\t\tss_sp = NULL;\n\t\t} else {\n\t\t\terror = -ENOMEM;\n\t\t\tif (ss_size < MINSIGSTKSZ)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tcurrent->sas_ss_sp = (unsigned long) ss_sp;\n\t\tcurrent->sas_ss_size = ss_size;\n\t}\n\n\terror = 0;\n\tif (uoss) {\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_WRITE, uoss, sizeof(*uoss)))\n\t\t\tgoto out;\n\t\terror = __put_user(oss.ss_sp, &uoss->ss_sp) |\n\t\t\t__put_user(oss.ss_size, &uoss->ss_size) |\n\t\t\t__put_user(oss.ss_flags, &uoss->ss_flags);\n\t}\n\nout:\n\treturn error;\n}\nSYSCALL_DEFINE2(sigaltstack,const stack_t __user *,uss, stack_t __user *,uoss)\n{\n\treturn do_sigaltstack(uss, uoss, current_user_stack_pointer());\n}\n\nint restore_altstack(const stack_t __user *uss)\n{\n\tint err = do_sigaltstack(uss, NULL, current_user_stack_pointer());\n\t/* squash all but EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __save_altstack(stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\treturn  __put_user((void __user *)t->sas_ss_sp, &uss->ss_sp) |\n\t\t__put_user(sas_ss_flags(sp), &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(sigaltstack,\n\t\t\tconst compat_stack_t __user *, uss_ptr,\n\t\t\tcompat_stack_t __user *, uoss_ptr)\n{\n\tstack_t uss, uoss;\n\tint ret;\n\tmm_segment_t seg;\n\n\tif (uss_ptr) {\n\t\tcompat_stack_t uss32;\n\n\t\tmemset(&uss, 0, sizeof(stack_t));\n\t\tif (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))\n\t\t\treturn -EFAULT;\n\t\tuss.ss_sp = compat_ptr(uss32.ss_sp);\n\t\tuss.ss_flags = uss32.ss_flags;\n\t\tuss.ss_size = uss32.ss_size;\n\t}\n\tseg = get_fs();\n\tset_fs(KERNEL_DS);\n\tret = do_sigaltstack((stack_t __force __user *) (uss_ptr ? &uss : NULL),\n\t\t\t     (stack_t __force __user *) &uoss,\n\t\t\t     compat_user_stack_pointer());\n\tset_fs(seg);\n\tif (ret >= 0 && uoss_ptr)  {\n\t\tif (!access_ok(VERIFY_WRITE, uoss_ptr, sizeof(compat_stack_t)) ||\n\t\t    __put_user(ptr_to_compat(uoss.ss_sp), &uoss_ptr->ss_sp) ||\n\t\t    __put_user(uoss.ss_flags, &uoss_ptr->ss_flags) ||\n\t\t    __put_user(uoss.ss_size, &uoss_ptr->ss_size))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\nint compat_restore_altstack(const compat_stack_t __user *uss)\n{\n\tint err = compat_sys_sigaltstack(uss, NULL);\n\t/* squash all but -EFAULT for now */\n\treturn err == -EFAULT ? err : 0;\n}\n\nint __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)\n{\n\tstruct task_struct *t = current;\n\treturn  __put_user(ptr_to_compat((void __user *)t->sas_ss_sp), &uss->ss_sp) |\n\t\t__put_user(sas_ss_flags(sp), &uss->ss_flags) |\n\t\t__put_user(t->sas_ss_size, &uss->ss_size);\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPENDING\n\n/**\n *  sys_sigpending - examine pending signals\n *  @set: where mask of pending signal is returned\n */\nSYSCALL_DEFINE1(sigpending, old_sigset_t __user *, set)\n{\n\treturn sys_rt_sigpending((sigset_t __user *)set, sizeof(old_sigset_t)); \n}\n\n#endif\n\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\n/**\n *  sys_sigprocmask - examine and change blocked signals\n *  @how: whether to add, remove, or set signals\n *  @nset: signals to add or remove (if non-null)\n *  @oset: previous value of signal mask if non-null\n *\n * Some platforms have their own version with special arguments;\n * others support only sys_rt_sigprocmask.\n */\n\nSYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,\n\t\told_sigset_t __user *, oset)\n{\n\told_sigset_t old_set, new_set;\n\tsigset_t new_blocked;\n\n\told_set = current->blocked.sig[0];\n\n\tif (nset) {\n\t\tif (copy_from_user(&new_set, nset, sizeof(*nset)))\n\t\t\treturn -EFAULT;\n\n\t\tnew_blocked = current->blocked;\n\n\t\tswitch (how) {\n\t\tcase SIG_BLOCK:\n\t\t\tsigaddsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_UNBLOCK:\n\t\t\tsigdelsetmask(&new_blocked, new_set);\n\t\t\tbreak;\n\t\tcase SIG_SETMASK:\n\t\t\tnew_blocked.sig[0] = new_set;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tset_current_blocked(&new_blocked);\n\t}\n\n\tif (oset) {\n\t\tif (copy_to_user(oset, &old_set, sizeof(*oset)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n#endif /* __ARCH_WANT_SYS_SIGPROCMASK */\n\n#ifndef CONFIG_ODD_RT_SIGACTION\n/**\n *  sys_rt_sigaction - alter an action taken by a process\n *  @sig: signal to be sent\n *  @act: new sigaction\n *  @oact: used to save the previous sigaction\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct sigaction __user *, act,\n\t\tstruct sigaction __user *, oact,\n\t\tsize_t, sigsetsize)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret = -EINVAL;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\tgoto out;\n\n\tif (act) {\n\t\tif (copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);\n\n\tif (!ret && oact) {\n\t\tif (copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))\n\t\t\treturn -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(rt_sigaction, int, sig,\n\t\tconst struct compat_sigaction __user *, act,\n\t\tstruct compat_sigaction __user *, oact,\n\t\tcompat_size_t, sigsetsize)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tcompat_sigset_t mask;\n#ifdef __ARCH_HAS_SA_RESTORER\n\tcompat_uptr_t restorer;\n#endif\n\tint ret;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(compat_sigset_t))\n\t\treturn -EINVAL;\n\n\tif (act) {\n\t\tcompat_uptr_t handler;\n\t\tret = get_user(handler, &act->sa_handler);\n\t\tnew_ka.sa.sa_handler = compat_ptr(handler);\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tret |= get_user(restorer, &act->sa_restorer);\n\t\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\n#endif\n\t\tret |= copy_from_user(&mask, &act->sa_mask, sizeof(mask));\n\t\tret |= __get_user(new_ka.sa.sa_flags, &act->sa_flags);\n\t\tif (ret)\n\t\t\treturn -EFAULT;\n\t\tsigset_from_compat(&new_ka.sa.sa_mask, &mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\tif (!ret && oact) {\n\t\tsigset_to_compat(&mask, &old_ka.sa.sa_mask);\n\t\tret = put_user(ptr_to_compat(old_ka.sa.sa_handler), \n\t\t\t       &oact->sa_handler);\n\t\tret |= copy_to_user(&oact->sa_mask, &mask, sizeof(mask));\n\t\tret |= __put_user(old_ka.sa.sa_flags, &oact->sa_flags);\n#ifdef __ARCH_HAS_SA_RESTORER\n\t\tret |= put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n\t\t\t\t&oact->sa_restorer);\n#endif\n\t}\n\treturn ret;\n}\n#endif\n#endif /* !CONFIG_ODD_RT_SIGACTION */\n\n#ifdef CONFIG_OLD_SIGACTION\nSYSCALL_DEFINE3(sigaction, int, sig,\n\t\tconst struct old_sigaction __user *, act,\n\t        struct old_sigaction __user *, oact)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tint ret;\n\n\tif (act) {\n\t\told_sigset_t mask;\n\t\tif (!access_ok(VERIFY_READ, act, sizeof(*act)) ||\n\t\t    __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||\n\t\t    __get_user(new_ka.sa.sa_restorer, &act->sa_restorer) ||\n\t\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n\t\t    __get_user(mask, &act->sa_mask))\n\t\t\treturn -EFAULT;\n#ifdef __ARCH_HAS_KA_RESTORER\n\t\tnew_ka.ka_restorer = NULL;\n#endif\n\t\tsiginitset(&new_ka.sa.sa_mask, mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n\tif (!ret && oact) {\n\t\tif (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||\n\t\t    __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||\n\t\t    __put_user(old_ka.sa.sa_restorer, &oact->sa_restorer) ||\n\t\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n\t\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn ret;\n}\n#endif\n#ifdef CONFIG_COMPAT_OLD_SIGACTION\nCOMPAT_SYSCALL_DEFINE3(sigaction, int, sig,\n\t\tconst struct compat_old_sigaction __user *, act,\n\t        struct compat_old_sigaction __user *, oact)\n{\n\tstruct k_sigaction new_ka, old_ka;\n\tint ret;\n\tcompat_old_sigset_t mask;\n\tcompat_uptr_t handler, restorer;\n\n\tif (act) {\n\t\tif (!access_ok(VERIFY_READ, act, sizeof(*act)) ||\n\t\t    __get_user(handler, &act->sa_handler) ||\n\t\t    __get_user(restorer, &act->sa_restorer) ||\n\t\t    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||\n\t\t    __get_user(mask, &act->sa_mask))\n\t\t\treturn -EFAULT;\n\n#ifdef __ARCH_HAS_KA_RESTORER\n\t\tnew_ka.ka_restorer = NULL;\n#endif\n\t\tnew_ka.sa.sa_handler = compat_ptr(handler);\n\t\tnew_ka.sa.sa_restorer = compat_ptr(restorer);\n\t\tsiginitset(&new_ka.sa.sa_mask, mask);\n\t}\n\n\tret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);\n\n\tif (!ret && oact) {\n\t\tif (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||\n\t\t    __put_user(ptr_to_compat(old_ka.sa.sa_handler),\n\t\t\t       &oact->sa_handler) ||\n\t\t    __put_user(ptr_to_compat(old_ka.sa.sa_restorer),\n\t\t\t       &oact->sa_restorer) ||\n\t\t    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||\n\t\t    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))\n\t\t\treturn -EFAULT;\n\t}\n\treturn ret;\n}\n#endif\n\n#ifdef __ARCH_WANT_SYS_SGETMASK\n\n/*\n * For backwards compatibility.  Functionality superseded by sigprocmask.\n */\nSYSCALL_DEFINE0(sgetmask)\n{\n\t/* SMP safe */\n\treturn current->blocked.sig[0];\n}\n\nSYSCALL_DEFINE1(ssetmask, int, newmask)\n{\n\tint old = current->blocked.sig[0];\n\tsigset_t newset;\n\n\tsiginitset(&newset, newmask);\n\tset_current_blocked(&newset);\n\n\treturn old;\n}\n#endif /* __ARCH_WANT_SGETMASK */\n\n#ifdef __ARCH_WANT_SYS_SIGNAL\n/*\n * For backwards compatibility.  Functionality superseded by sigaction.\n */\nSYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)\n{\n\tstruct k_sigaction new_sa, old_sa;\n\tint ret;\n\n\tnew_sa.sa.sa_handler = handler;\n\tnew_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;\n\tsigemptyset(&new_sa.sa.sa_mask);\n\n\tret = do_sigaction(sig, &new_sa, &old_sa);\n\n\treturn ret ? ret : (unsigned long)old_sa.sa.sa_handler;\n}\n#endif /* __ARCH_WANT_SYS_SIGNAL */\n\n#ifdef __ARCH_WANT_SYS_PAUSE\n\nSYSCALL_DEFINE0(pause)\n{\n\twhile (!signal_pending(current)) {\n\t\tcurrent->state = TASK_INTERRUPTIBLE;\n\t\tschedule();\n\t}\n\treturn -ERESTARTNOHAND;\n}\n\n#endif\n\nint sigsuspend(sigset_t *set)\n{\n\tcurrent->saved_sigmask = current->blocked;\n\tset_current_blocked(set);\n\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\tschedule();\n\tset_restore_sigmask();\n\treturn -ERESTARTNOHAND;\n}\n\n/**\n *  sys_rt_sigsuspend - replace the signal mask for a value with the\n *\t@unewset value until a signal is received\n *  @unewset: new signal mask value\n *  @sigsetsize: size of sigset_t type\n */\nSYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)\n{\n\tsigset_t newset;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&newset, unewset, sizeof(newset)))\n\t\treturn -EFAULT;\n\treturn sigsuspend(&newset);\n}\n \n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(rt_sigsuspend, compat_sigset_t __user *, unewset, compat_size_t, sigsetsize)\n{\n#ifdef __BIG_ENDIAN\n\tsigset_t newset;\n\tcompat_sigset_t newset32;\n\n\t/* XXX: Don't preclude handling different sized sigset_t's.  */\n\tif (sigsetsize != sizeof(sigset_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&newset32, unewset, sizeof(compat_sigset_t)))\n\t\treturn -EFAULT;\n\tsigset_from_compat(&newset, &newset32);\n\treturn sigsuspend(&newset);\n#else\n\t/* on little-endian bitmaps don't care about granularity */\n\treturn sys_rt_sigsuspend((sigset_t __user *)unewset, sigsetsize);\n#endif\n}\n#endif\n\n#ifdef CONFIG_OLD_SIGSUSPEND\nSYSCALL_DEFINE1(sigsuspend, old_sigset_t, mask)\n{\n\tsigset_t blocked;\n\tsiginitset(&blocked, mask);\n\treturn sigsuspend(&blocked);\n}\n#endif\n#ifdef CONFIG_OLD_SIGSUSPEND3\nSYSCALL_DEFINE3(sigsuspend, int, unused1, int, unused2, old_sigset_t, mask)\n{\n\tsigset_t blocked;\n\tsiginitset(&blocked, mask);\n\treturn sigsuspend(&blocked);\n}\n#endif\n\n__attribute__((weak)) const char *arch_vma_name(struct vm_area_struct *vma)\n{\n\treturn NULL;\n}\n\nvoid __init signals_init(void)\n{\n\tsigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC);\n}\n\n#ifdef CONFIG_KGDB_KDB\n#include <linux/kdb.h>\n/*\n * kdb_send_sig_info - Allows kdb to send signals without exposing\n * signal internals.  This function checks if the required locks are\n * available before calling the main signal code, to avoid kdb\n * deadlocks.\n */\nvoid\nkdb_send_sig_info(struct task_struct *t, struct siginfo *info)\n{\n\tstatic struct task_struct *kdb_prev_t;\n\tint sig, new_t;\n\tif (!spin_trylock(&t->sighand->siglock)) {\n\t\tkdb_printf(\"Can't do kill command now.\\n\"\n\t\t\t   \"The sigmask lock is held somewhere else in \"\n\t\t\t   \"kernel, try again later\\n\");\n\t\treturn;\n\t}\n\tspin_unlock(&t->sighand->siglock);\n\tnew_t = kdb_prev_t != t;\n\tkdb_prev_t = t;\n\tif (t->state != TASK_RUNNING && new_t) {\n\t\tkdb_printf(\"Process is not RUNNING, sending a signal from \"\n\t\t\t   \"kdb risks deadlock\\n\"\n\t\t\t   \"on the run queue locks. \"\n\t\t\t   \"The signal has _not_ been sent.\\n\"\n\t\t\t   \"Reissue the kill command if you want to risk \"\n\t\t\t   \"the deadlock.\\n\");\n\t\treturn;\n\t}\n\tsig = info->si_signo;\n\tif (send_sig_info(sig, info, t))\n\t\tkdb_printf(\"Fail to deliver Signal %d to process %d.\\n\",\n\t\t\t   sig, t->pid);\n\telse\n\t\tkdb_printf(\"Signal %d is sent to process %d.\\n\", sig, t->pid);\n}\n#endif\t/* CONFIG_KGDB_KDB */\n"], "filenames": ["kernel/signal.c"], "buggy_code_start_loc": [487], "buggy_code_end_loc": [487], "fixing_code_start_loc": [488], "fixing_code_end_loc": [491], "type": "CWE-264", "message": "The flush_signal_handlers function in kernel/signal.c in the Linux kernel before 3.8.4 preserves the value of the sa_restorer field across an exec operation, which makes it easier for local users to bypass the ASLR protection mechanism via a crafted application containing a sigaction system call.", "other": {"cve": {"id": "CVE-2013-0914", "sourceIdentifier": "cve-coordination@google.com", "published": "2013-03-22T11:59:11.507", "lastModified": "2014-02-07T04:45:24.997", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The flush_signal_handlers function in kernel/signal.c in the Linux kernel before 3.8.4 preserves the value of the sa_restorer field across an exec operation, which makes it easier for local users to bypass the ASLR protection mechanism via a crafted application containing a sigaction system call."}, {"lang": "es", "value": "La funci\u00f3n flush_signal_handlers en kernel/signal.c en el n\u00facleo de Linux anterior a v3.8.4 conserva el valor del campo sa_restorer a trav\u00e9s de una operaci\u00f3n exec, que hace m\u00e1s f\u00e1cil a los usuarios de la zona para evitar el mecanismo de protecci\u00f3n ASLR a trav\u00e9s de una aplicaci\u00f3n dise\u00f1ado que contenga un sigaction system call llamar."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 3.6}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.8.3", "matchCriteriaId": "7A40E2A8-BBC3-4763-B150-9193F30E9DF0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.0:*:*:*:*:*:*:*", "matchCriteriaId": "1A6E41FB-38CE-49F2-B796-9A5AA648E73F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.1:*:*:*:*:*:*:*", "matchCriteriaId": "93523FE1-5993-46CB-9299-7C8C1A04E873"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.8.2:*:*:*:*:*:*:*", "matchCriteriaId": "27ADC356-6BE9-43A3-9E0B-393DC4B1559A"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=2ca39528c01a933f6689cd6505ce65bd6d68a530", "source": "cve-coordination@google.com", "tags": ["Patch"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2013-07/msg00018.html", "source": "cve-coordination@google.com"}, {"url": "http://lists.opensuse.org/opensuse-updates/2013-12/msg00129.html", "source": "cve-coordination@google.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-1051.html", "source": "cve-coordination@google.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.8.4", "source": "cve-coordination@google.com"}, {"url": "http://www.mandriva.com/security/advisories?name=MDVSA-2013:176", "source": "cve-coordination@google.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/03/11/8", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1787-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1788-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1792-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1793-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1794-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1795-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1796-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1797-1", "source": "cve-coordination@google.com"}, {"url": "http://www.ubuntu.com/usn/USN-1798-1", "source": "cve-coordination@google.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=920499", "source": "cve-coordination@google.com"}, {"url": "https://github.com/torvalds/linux/commit/2ca39528c01a933f6689cd6505ce65bd6d68a530", "source": "cve-coordination@google.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/2ca39528c01a933f6689cd6505ce65bd6d68a530"}}