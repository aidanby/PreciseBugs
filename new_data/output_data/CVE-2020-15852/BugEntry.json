{"buggy_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_IOBITMAP_H\n#define _ASM_X86_IOBITMAP_H\n\n#include <linux/refcount.h>\n#include <asm/processor.h>\n\nstruct io_bitmap {\n\tu64\t\tsequence;\n\trefcount_t\trefcnt;\n\t/* The maximum number of bytes to copy so all zero bits are covered */\n\tunsigned int\tmax;\n\tunsigned long\tbitmap[IO_BITMAP_LONGS];\n};\n\nstruct task_struct;\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nvoid io_bitmap_share(struct task_struct *tsk);\nvoid io_bitmap_exit(struct task_struct *tsk);\n\nvoid native_tss_update_io_bitmap(void);\n\n#ifdef CONFIG_PARAVIRT_XXL\n#include <asm/paravirt.h>\n#else\n#define tss_update_io_bitmap native_tss_update_io_bitmap\n#endif\n\n#else\nstatic inline void io_bitmap_share(struct task_struct *tsk) { }\nstatic inline void io_bitmap_exit(struct task_struct *tsk) { }\nstatic inline void tss_update_io_bitmap(void) { }\n#endif\n\n#endif\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_PARAVIRT_H\n#define _ASM_X86_PARAVIRT_H\n/* Various instructions on x86 need to be replaced for\n * para-virtualization: those hooks are defined here. */\n\n#ifdef CONFIG_PARAVIRT\n#include <asm/pgtable_types.h>\n#include <asm/asm.h>\n#include <asm/nospec-branch.h>\n\n#include <asm/paravirt_types.h>\n\n#ifndef __ASSEMBLY__\n#include <linux/bug.h>\n#include <linux/types.h>\n#include <linux/cpumask.h>\n#include <asm/frame.h>\n\nstatic inline unsigned long long paravirt_sched_clock(void)\n{\n\treturn PVOP_CALL0(unsigned long long, time.sched_clock);\n}\n\nstruct static_key;\nextern struct static_key paravirt_steal_enabled;\nextern struct static_key paravirt_steal_rq_enabled;\n\n__visible void __native_queued_spin_unlock(struct qspinlock *lock);\nbool pv_is_native_spin_unlock(void);\n__visible bool __native_vcpu_is_preempted(long cpu);\nbool pv_is_native_vcpu_is_preempted(void);\n\nstatic inline u64 paravirt_steal_clock(int cpu)\n{\n\treturn PVOP_CALL1(u64, time.steal_clock, cpu);\n}\n\n/* The paravirtualized I/O functions */\nstatic inline void slow_down_io(void)\n{\n\tpv_ops.cpu.io_delay();\n#ifdef REALLY_SLOW_IO\n\tpv_ops.cpu.io_delay();\n\tpv_ops.cpu.io_delay();\n\tpv_ops.cpu.io_delay();\n#endif\n}\n\nvoid native_flush_tlb_local(void);\nvoid native_flush_tlb_global(void);\nvoid native_flush_tlb_one_user(unsigned long addr);\nvoid native_flush_tlb_others(const struct cpumask *cpumask,\n\t\t\t     const struct flush_tlb_info *info);\n\nstatic inline void __flush_tlb_local(void)\n{\n\tPVOP_VCALL0(mmu.flush_tlb_user);\n}\n\nstatic inline void __flush_tlb_global(void)\n{\n\tPVOP_VCALL0(mmu.flush_tlb_kernel);\n}\n\nstatic inline void __flush_tlb_one_user(unsigned long addr)\n{\n\tPVOP_VCALL1(mmu.flush_tlb_one_user, addr);\n}\n\nstatic inline void __flush_tlb_others(const struct cpumask *cpumask,\n\t\t\t\t      const struct flush_tlb_info *info)\n{\n\tPVOP_VCALL2(mmu.flush_tlb_others, cpumask, info);\n}\n\nstatic inline void paravirt_tlb_remove_table(struct mmu_gather *tlb, void *table)\n{\n\tPVOP_VCALL2(mmu.tlb_remove_table, tlb, table);\n}\n\nstatic inline void paravirt_arch_exit_mmap(struct mm_struct *mm)\n{\n\tPVOP_VCALL1(mmu.exit_mmap, mm);\n}\n\n#ifdef CONFIG_PARAVIRT_XXL\nstatic inline void load_sp0(unsigned long sp0)\n{\n\tPVOP_VCALL1(cpu.load_sp0, sp0);\n}\n\n/* The paravirtualized CPUID instruction. */\nstatic inline void __cpuid(unsigned int *eax, unsigned int *ebx,\n\t\t\t   unsigned int *ecx, unsigned int *edx)\n{\n\tPVOP_VCALL4(cpu.cpuid, eax, ebx, ecx, edx);\n}\n\n/*\n * These special macros can be used to get or set a debugging register\n */\nstatic inline unsigned long paravirt_get_debugreg(int reg)\n{\n\treturn PVOP_CALL1(unsigned long, cpu.get_debugreg, reg);\n}\n#define get_debugreg(var, reg) var = paravirt_get_debugreg(reg)\nstatic inline void set_debugreg(unsigned long val, int reg)\n{\n\tPVOP_VCALL2(cpu.set_debugreg, reg, val);\n}\n\nstatic inline unsigned long read_cr0(void)\n{\n\treturn PVOP_CALL0(unsigned long, cpu.read_cr0);\n}\n\nstatic inline void write_cr0(unsigned long x)\n{\n\tPVOP_VCALL1(cpu.write_cr0, x);\n}\n\nstatic inline unsigned long read_cr2(void)\n{\n\treturn PVOP_CALLEE0(unsigned long, mmu.read_cr2);\n}\n\nstatic inline void write_cr2(unsigned long x)\n{\n\tPVOP_VCALL1(mmu.write_cr2, x);\n}\n\nstatic inline unsigned long __read_cr3(void)\n{\n\treturn PVOP_CALL0(unsigned long, mmu.read_cr3);\n}\n\nstatic inline void write_cr3(unsigned long x)\n{\n\tPVOP_VCALL1(mmu.write_cr3, x);\n}\n\nstatic inline void __write_cr4(unsigned long x)\n{\n\tPVOP_VCALL1(cpu.write_cr4, x);\n}\n\nstatic inline void arch_safe_halt(void)\n{\n\tPVOP_VCALL0(irq.safe_halt);\n}\n\nstatic inline void halt(void)\n{\n\tPVOP_VCALL0(irq.halt);\n}\n\nstatic inline void wbinvd(void)\n{\n\tPVOP_VCALL0(cpu.wbinvd);\n}\n\n#define get_kernel_rpl()  (pv_info.kernel_rpl)\n\nstatic inline u64 paravirt_read_msr(unsigned msr)\n{\n\treturn PVOP_CALL1(u64, cpu.read_msr, msr);\n}\n\nstatic inline void paravirt_write_msr(unsigned msr,\n\t\t\t\t      unsigned low, unsigned high)\n{\n\tPVOP_VCALL3(cpu.write_msr, msr, low, high);\n}\n\nstatic inline u64 paravirt_read_msr_safe(unsigned msr, int *err)\n{\n\treturn PVOP_CALL2(u64, cpu.read_msr_safe, msr, err);\n}\n\nstatic inline int paravirt_write_msr_safe(unsigned msr,\n\t\t\t\t\t  unsigned low, unsigned high)\n{\n\treturn PVOP_CALL3(int, cpu.write_msr_safe, msr, low, high);\n}\n\n#define rdmsr(msr, val1, val2)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tu64 _l = paravirt_read_msr(msr);\t\\\n\tval1 = (u32)_l;\t\t\t\t\\\n\tval2 = _l >> 32;\t\t\t\\\n} while (0)\n\n#define wrmsr(msr, val1, val2)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tparavirt_write_msr(msr, val1, val2);\t\\\n} while (0)\n\n#define rdmsrl(msr, val)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tval = paravirt_read_msr(msr);\t\t\\\n} while (0)\n\nstatic inline void wrmsrl(unsigned msr, u64 val)\n{\n\twrmsr(msr, (u32)val, (u32)(val>>32));\n}\n\n#define wrmsr_safe(msr, a, b)\tparavirt_write_msr_safe(msr, a, b)\n\n/* rdmsr with exception handling */\n#define rdmsr_safe(msr, a, b)\t\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tint _err;\t\t\t\t\t\\\n\tu64 _l = paravirt_read_msr_safe(msr, &_err);\t\\\n\t(*a) = (u32)_l;\t\t\t\t\t\\\n\t(*b) = _l >> 32;\t\t\t\t\\\n\t_err;\t\t\t\t\t\t\\\n})\n\nstatic inline int rdmsrl_safe(unsigned msr, unsigned long long *p)\n{\n\tint err;\n\n\t*p = paravirt_read_msr_safe(msr, &err);\n\treturn err;\n}\n\nstatic inline unsigned long long paravirt_read_pmc(int counter)\n{\n\treturn PVOP_CALL1(u64, cpu.read_pmc, counter);\n}\n\n#define rdpmc(counter, low, high)\t\t\\\ndo {\t\t\t\t\t\t\\\n\tu64 _l = paravirt_read_pmc(counter);\t\\\n\tlow = (u32)_l;\t\t\t\t\\\n\thigh = _l >> 32;\t\t\t\\\n} while (0)\n\n#define rdpmcl(counter, val) ((val) = paravirt_read_pmc(counter))\n\nstatic inline void paravirt_alloc_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tPVOP_VCALL2(cpu.alloc_ldt, ldt, entries);\n}\n\nstatic inline void paravirt_free_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tPVOP_VCALL2(cpu.free_ldt, ldt, entries);\n}\n\nstatic inline void load_TR_desc(void)\n{\n\tPVOP_VCALL0(cpu.load_tr_desc);\n}\nstatic inline void load_gdt(const struct desc_ptr *dtr)\n{\n\tPVOP_VCALL1(cpu.load_gdt, dtr);\n}\nstatic inline void load_idt(const struct desc_ptr *dtr)\n{\n\tPVOP_VCALL1(cpu.load_idt, dtr);\n}\nstatic inline void set_ldt(const void *addr, unsigned entries)\n{\n\tPVOP_VCALL2(cpu.set_ldt, addr, entries);\n}\nstatic inline unsigned long paravirt_store_tr(void)\n{\n\treturn PVOP_CALL0(unsigned long, cpu.store_tr);\n}\n\n#define store_tr(tr)\t((tr) = paravirt_store_tr())\nstatic inline void load_TLS(struct thread_struct *t, unsigned cpu)\n{\n\tPVOP_VCALL2(cpu.load_tls, t, cpu);\n}\n\n#ifdef CONFIG_X86_64\nstatic inline void load_gs_index(unsigned int gs)\n{\n\tPVOP_VCALL1(cpu.load_gs_index, gs);\n}\n#endif\n\nstatic inline void write_ldt_entry(struct desc_struct *dt, int entry,\n\t\t\t\t   const void *desc)\n{\n\tPVOP_VCALL3(cpu.write_ldt_entry, dt, entry, desc);\n}\n\nstatic inline void write_gdt_entry(struct desc_struct *dt, int entry,\n\t\t\t\t   void *desc, int type)\n{\n\tPVOP_VCALL4(cpu.write_gdt_entry, dt, entry, desc, type);\n}\n\nstatic inline void write_idt_entry(gate_desc *dt, int entry, const gate_desc *g)\n{\n\tPVOP_VCALL3(cpu.write_idt_entry, dt, entry, g);\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic inline void tss_update_io_bitmap(void)\n{\n\tPVOP_VCALL0(cpu.update_io_bitmap);\n}\n#endif\n\nstatic inline void paravirt_activate_mm(struct mm_struct *prev,\n\t\t\t\t\tstruct mm_struct *next)\n{\n\tPVOP_VCALL2(mmu.activate_mm, prev, next);\n}\n\nstatic inline void paravirt_arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t\t  struct mm_struct *mm)\n{\n\tPVOP_VCALL2(mmu.dup_mmap, oldmm, mm);\n}\n\nstatic inline int paravirt_pgd_alloc(struct mm_struct *mm)\n{\n\treturn PVOP_CALL1(int, mmu.pgd_alloc, mm);\n}\n\nstatic inline void paravirt_pgd_free(struct mm_struct *mm, pgd_t *pgd)\n{\n\tPVOP_VCALL2(mmu.pgd_free, mm, pgd);\n}\n\nstatic inline void paravirt_alloc_pte(struct mm_struct *mm, unsigned long pfn)\n{\n\tPVOP_VCALL2(mmu.alloc_pte, mm, pfn);\n}\nstatic inline void paravirt_release_pte(unsigned long pfn)\n{\n\tPVOP_VCALL1(mmu.release_pte, pfn);\n}\n\nstatic inline void paravirt_alloc_pmd(struct mm_struct *mm, unsigned long pfn)\n{\n\tPVOP_VCALL2(mmu.alloc_pmd, mm, pfn);\n}\n\nstatic inline void paravirt_release_pmd(unsigned long pfn)\n{\n\tPVOP_VCALL1(mmu.release_pmd, pfn);\n}\n\nstatic inline void paravirt_alloc_pud(struct mm_struct *mm, unsigned long pfn)\n{\n\tPVOP_VCALL2(mmu.alloc_pud, mm, pfn);\n}\nstatic inline void paravirt_release_pud(unsigned long pfn)\n{\n\tPVOP_VCALL1(mmu.release_pud, pfn);\n}\n\nstatic inline void paravirt_alloc_p4d(struct mm_struct *mm, unsigned long pfn)\n{\n\tPVOP_VCALL2(mmu.alloc_p4d, mm, pfn);\n}\n\nstatic inline void paravirt_release_p4d(unsigned long pfn)\n{\n\tPVOP_VCALL1(mmu.release_p4d, pfn);\n}\n\nstatic inline pte_t __pte(pteval_t val)\n{\n\tpteval_t ret;\n\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\tret = PVOP_CALLEE2(pteval_t, mmu.make_pte, val, (u64)val >> 32);\n\telse\n\t\tret = PVOP_CALLEE1(pteval_t, mmu.make_pte, val);\n\n\treturn (pte_t) { .pte = ret };\n}\n\nstatic inline pteval_t pte_val(pte_t pte)\n{\n\tpteval_t ret;\n\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\tret = PVOP_CALLEE2(pteval_t, mmu.pte_val,\n\t\t\t\t   pte.pte, (u64)pte.pte >> 32);\n\telse\n\t\tret = PVOP_CALLEE1(pteval_t, mmu.pte_val, pte.pte);\n\n\treturn ret;\n}\n\nstatic inline pgd_t __pgd(pgdval_t val)\n{\n\tpgdval_t ret;\n\n\tif (sizeof(pgdval_t) > sizeof(long))\n\t\tret = PVOP_CALLEE2(pgdval_t, mmu.make_pgd, val, (u64)val >> 32);\n\telse\n\t\tret = PVOP_CALLEE1(pgdval_t, mmu.make_pgd, val);\n\n\treturn (pgd_t) { ret };\n}\n\nstatic inline pgdval_t pgd_val(pgd_t pgd)\n{\n\tpgdval_t ret;\n\n\tif (sizeof(pgdval_t) > sizeof(long))\n\t\tret =  PVOP_CALLEE2(pgdval_t, mmu.pgd_val,\n\t\t\t\t    pgd.pgd, (u64)pgd.pgd >> 32);\n\telse\n\t\tret =  PVOP_CALLEE1(pgdval_t, mmu.pgd_val, pgd.pgd);\n\n\treturn ret;\n}\n\n#define  __HAVE_ARCH_PTEP_MODIFY_PROT_TRANSACTION\nstatic inline pte_t ptep_modify_prot_start(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\t   pte_t *ptep)\n{\n\tpteval_t ret;\n\n\tret = PVOP_CALL3(pteval_t, mmu.ptep_modify_prot_start, vma, addr, ptep);\n\n\treturn (pte_t) { .pte = ret };\n}\n\nstatic inline void ptep_modify_prot_commit(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\t   pte_t *ptep, pte_t old_pte, pte_t pte)\n{\n\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\t/* 5 arg words */\n\t\tpv_ops.mmu.ptep_modify_prot_commit(vma, addr, ptep, pte);\n\telse\n\t\tPVOP_VCALL4(mmu.ptep_modify_prot_commit,\n\t\t\t    vma, addr, ptep, pte.pte);\n}\n\nstatic inline void set_pte(pte_t *ptep, pte_t pte)\n{\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\tPVOP_VCALL3(mmu.set_pte, ptep, pte.pte, (u64)pte.pte >> 32);\n\telse\n\t\tPVOP_VCALL2(mmu.set_pte, ptep, pte.pte);\n}\n\nstatic inline void set_pte_at(struct mm_struct *mm, unsigned long addr,\n\t\t\t      pte_t *ptep, pte_t pte)\n{\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\t/* 5 arg words */\n\t\tpv_ops.mmu.set_pte_at(mm, addr, ptep, pte);\n\telse\n\t\tPVOP_VCALL4(mmu.set_pte_at, mm, addr, ptep, pte.pte);\n}\n\nstatic inline void set_pmd(pmd_t *pmdp, pmd_t pmd)\n{\n\tpmdval_t val = native_pmd_val(pmd);\n\n\tif (sizeof(pmdval_t) > sizeof(long))\n\t\tPVOP_VCALL3(mmu.set_pmd, pmdp, val, (u64)val >> 32);\n\telse\n\t\tPVOP_VCALL2(mmu.set_pmd, pmdp, val);\n}\n\n#if CONFIG_PGTABLE_LEVELS >= 3\nstatic inline pmd_t __pmd(pmdval_t val)\n{\n\tpmdval_t ret;\n\n\tif (sizeof(pmdval_t) > sizeof(long))\n\t\tret = PVOP_CALLEE2(pmdval_t, mmu.make_pmd, val, (u64)val >> 32);\n\telse\n\t\tret = PVOP_CALLEE1(pmdval_t, mmu.make_pmd, val);\n\n\treturn (pmd_t) { ret };\n}\n\nstatic inline pmdval_t pmd_val(pmd_t pmd)\n{\n\tpmdval_t ret;\n\n\tif (sizeof(pmdval_t) > sizeof(long))\n\t\tret =  PVOP_CALLEE2(pmdval_t, mmu.pmd_val,\n\t\t\t\t    pmd.pmd, (u64)pmd.pmd >> 32);\n\telse\n\t\tret =  PVOP_CALLEE1(pmdval_t, mmu.pmd_val, pmd.pmd);\n\n\treturn ret;\n}\n\nstatic inline void set_pud(pud_t *pudp, pud_t pud)\n{\n\tpudval_t val = native_pud_val(pud);\n\n\tif (sizeof(pudval_t) > sizeof(long))\n\t\tPVOP_VCALL3(mmu.set_pud, pudp, val, (u64)val >> 32);\n\telse\n\t\tPVOP_VCALL2(mmu.set_pud, pudp, val);\n}\n#if CONFIG_PGTABLE_LEVELS >= 4\nstatic inline pud_t __pud(pudval_t val)\n{\n\tpudval_t ret;\n\n\tret = PVOP_CALLEE1(pudval_t, mmu.make_pud, val);\n\n\treturn (pud_t) { ret };\n}\n\nstatic inline pudval_t pud_val(pud_t pud)\n{\n\treturn PVOP_CALLEE1(pudval_t, mmu.pud_val, pud.pud);\n}\n\nstatic inline void pud_clear(pud_t *pudp)\n{\n\tset_pud(pudp, __pud(0));\n}\n\nstatic inline void set_p4d(p4d_t *p4dp, p4d_t p4d)\n{\n\tp4dval_t val = native_p4d_val(p4d);\n\n\tPVOP_VCALL2(mmu.set_p4d, p4dp, val);\n}\n\n#if CONFIG_PGTABLE_LEVELS >= 5\n\nstatic inline p4d_t __p4d(p4dval_t val)\n{\n\tp4dval_t ret = PVOP_CALLEE1(p4dval_t, mmu.make_p4d, val);\n\n\treturn (p4d_t) { ret };\n}\n\nstatic inline p4dval_t p4d_val(p4d_t p4d)\n{\n\treturn PVOP_CALLEE1(p4dval_t, mmu.p4d_val, p4d.p4d);\n}\n\nstatic inline void __set_pgd(pgd_t *pgdp, pgd_t pgd)\n{\n\tPVOP_VCALL2(mmu.set_pgd, pgdp, native_pgd_val(pgd));\n}\n\n#define set_pgd(pgdp, pgdval) do {\t\t\t\t\t\\\n\tif (pgtable_l5_enabled())\t\t\t\t\t\t\\\n\t\t__set_pgd(pgdp, pgdval);\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tset_p4d((p4d_t *)(pgdp), (p4d_t) { (pgdval).pgd });\t\\\n} while (0)\n\n#define pgd_clear(pgdp) do {\t\t\t\t\t\t\\\n\tif (pgtable_l5_enabled())\t\t\t\t\t\t\\\n\t\tset_pgd(pgdp, __pgd(0));\t\t\t\t\\\n} while (0)\n\n#endif  /* CONFIG_PGTABLE_LEVELS == 5 */\n\nstatic inline void p4d_clear(p4d_t *p4dp)\n{\n\tset_p4d(p4dp, __p4d(0));\n}\n\n#endif\t/* CONFIG_PGTABLE_LEVELS == 4 */\n\n#endif\t/* CONFIG_PGTABLE_LEVELS >= 3 */\n\n#ifdef CONFIG_X86_PAE\n/* Special-case pte-setting operations for PAE, which can't update a\n   64-bit pte atomically */\nstatic inline void set_pte_atomic(pte_t *ptep, pte_t pte)\n{\n\tPVOP_VCALL3(mmu.set_pte_atomic, ptep, pte.pte, pte.pte >> 32);\n}\n\nstatic inline void pte_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t     pte_t *ptep)\n{\n\tPVOP_VCALL3(mmu.pte_clear, mm, addr, ptep);\n}\n\nstatic inline void pmd_clear(pmd_t *pmdp)\n{\n\tPVOP_VCALL1(mmu.pmd_clear, pmdp);\n}\n#else  /* !CONFIG_X86_PAE */\nstatic inline void set_pte_atomic(pte_t *ptep, pte_t pte)\n{\n\tset_pte(ptep, pte);\n}\n\nstatic inline void pte_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t     pte_t *ptep)\n{\n\tset_pte_at(mm, addr, ptep, __pte(0));\n}\n\nstatic inline void pmd_clear(pmd_t *pmdp)\n{\n\tset_pmd(pmdp, __pmd(0));\n}\n#endif\t/* CONFIG_X86_PAE */\n\n#define  __HAVE_ARCH_START_CONTEXT_SWITCH\nstatic inline void arch_start_context_switch(struct task_struct *prev)\n{\n\tPVOP_VCALL1(cpu.start_context_switch, prev);\n}\n\nstatic inline void arch_end_context_switch(struct task_struct *next)\n{\n\tPVOP_VCALL1(cpu.end_context_switch, next);\n}\n\n#define  __HAVE_ARCH_ENTER_LAZY_MMU_MODE\nstatic inline void arch_enter_lazy_mmu_mode(void)\n{\n\tPVOP_VCALL0(mmu.lazy_mode.enter);\n}\n\nstatic inline void arch_leave_lazy_mmu_mode(void)\n{\n\tPVOP_VCALL0(mmu.lazy_mode.leave);\n}\n\nstatic inline void arch_flush_lazy_mmu_mode(void)\n{\n\tPVOP_VCALL0(mmu.lazy_mode.flush);\n}\n\nstatic inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,\n\t\t\t\tphys_addr_t phys, pgprot_t flags)\n{\n\tpv_ops.mmu.set_fixmap(idx, phys, flags);\n}\n#endif\n\n#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)\n\nstatic __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,\n\t\t\t\t\t\t\tu32 val)\n{\n\tPVOP_VCALL2(lock.queued_spin_lock_slowpath, lock, val);\n}\n\nstatic __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tPVOP_VCALLEE1(lock.queued_spin_unlock, lock);\n}\n\nstatic __always_inline void pv_wait(u8 *ptr, u8 val)\n{\n\tPVOP_VCALL2(lock.wait, ptr, val);\n}\n\nstatic __always_inline void pv_kick(int cpu)\n{\n\tPVOP_VCALL1(lock.kick, cpu);\n}\n\nstatic __always_inline bool pv_vcpu_is_preempted(long cpu)\n{\n\treturn PVOP_CALLEE1(bool, lock.vcpu_is_preempted, cpu);\n}\n\nvoid __raw_callee_save___native_queued_spin_unlock(struct qspinlock *lock);\nbool __raw_callee_save___native_vcpu_is_preempted(long cpu);\n\n#endif /* SMP && PARAVIRT_SPINLOCKS */\n\n#ifdef CONFIG_X86_32\n#define PV_SAVE_REGS \"pushl %ecx; pushl %edx;\"\n#define PV_RESTORE_REGS \"popl %edx; popl %ecx;\"\n\n/* save and restore all caller-save registers, except return value */\n#define PV_SAVE_ALL_CALLER_REGS\t\t\"pushl %ecx;\"\n#define PV_RESTORE_ALL_CALLER_REGS\t\"popl  %ecx;\"\n\n#define PV_FLAGS_ARG \"0\"\n#define PV_EXTRA_CLOBBERS\n#define PV_VEXTRA_CLOBBERS\n#else\n/* save and restore all caller-save registers, except return value */\n#define PV_SAVE_ALL_CALLER_REGS\t\t\t\t\t\t\\\n\t\"push %rcx;\"\t\t\t\t\t\t\t\\\n\t\"push %rdx;\"\t\t\t\t\t\t\t\\\n\t\"push %rsi;\"\t\t\t\t\t\t\t\\\n\t\"push %rdi;\"\t\t\t\t\t\t\t\\\n\t\"push %r8;\"\t\t\t\t\t\t\t\\\n\t\"push %r9;\"\t\t\t\t\t\t\t\\\n\t\"push %r10;\"\t\t\t\t\t\t\t\\\n\t\"push %r11;\"\n#define PV_RESTORE_ALL_CALLER_REGS\t\t\t\t\t\\\n\t\"pop %r11;\"\t\t\t\t\t\t\t\\\n\t\"pop %r10;\"\t\t\t\t\t\t\t\\\n\t\"pop %r9;\"\t\t\t\t\t\t\t\\\n\t\"pop %r8;\"\t\t\t\t\t\t\t\\\n\t\"pop %rdi;\"\t\t\t\t\t\t\t\\\n\t\"pop %rsi;\"\t\t\t\t\t\t\t\\\n\t\"pop %rdx;\"\t\t\t\t\t\t\t\\\n\t\"pop %rcx;\"\n\n/* We save some registers, but all of them, that's too much. We clobber all\n * caller saved registers but the argument parameter */\n#define PV_SAVE_REGS \"pushq %%rdi;\"\n#define PV_RESTORE_REGS \"popq %%rdi;\"\n#define PV_EXTRA_CLOBBERS EXTRA_CLOBBERS, \"rcx\" , \"rdx\", \"rsi\"\n#define PV_VEXTRA_CLOBBERS EXTRA_CLOBBERS, \"rdi\", \"rcx\" , \"rdx\", \"rsi\"\n#define PV_FLAGS_ARG \"D\"\n#endif\n\n/*\n * Generate a thunk around a function which saves all caller-save\n * registers except for the return value.  This allows C functions to\n * be called from assembler code where fewer than normal registers are\n * available.  It may also help code generation around calls from C\n * code if the common case doesn't use many registers.\n *\n * When a callee is wrapped in a thunk, the caller can assume that all\n * arg regs and all scratch registers are preserved across the\n * call. The return value in rax/eax will not be saved, even for void\n * functions.\n */\n#define PV_THUNK_NAME(func) \"__raw_callee_save_\" #func\n#define PV_CALLEE_SAVE_REGS_THUNK(func)\t\t\t\t\t\\\n\textern typeof(func) __raw_callee_save_##func;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tasm(\".pushsection .text;\"\t\t\t\t\t\\\n\t    \".globl \" PV_THUNK_NAME(func) \";\"\t\t\t\t\\\n\t    \".type \" PV_THUNK_NAME(func) \", @function;\"\t\t\t\\\n\t    PV_THUNK_NAME(func) \":\"\t\t\t\t\t\\\n\t    FRAME_BEGIN\t\t\t\t\t\t\t\\\n\t    PV_SAVE_ALL_CALLER_REGS\t\t\t\t\t\\\n\t    \"call \" #func \";\"\t\t\t\t\t\t\\\n\t    PV_RESTORE_ALL_CALLER_REGS\t\t\t\t\t\\\n\t    FRAME_END\t\t\t\t\t\t\t\\\n\t    \"ret;\"\t\t\t\t\t\t\t\\\n\t    \".size \" PV_THUNK_NAME(func) \", .-\" PV_THUNK_NAME(func) \";\"\t\\\n\t    \".popsection\")\n\n/* Get a reference to a callee-save function */\n#define PV_CALLEE_SAVE(func)\t\t\t\t\t\t\\\n\t((struct paravirt_callee_save) { __raw_callee_save_##func })\n\n/* Promise that \"func\" already uses the right calling convention */\n#define __PV_IS_CALLEE_SAVE(func)\t\t\t\\\n\t((struct paravirt_callee_save) { func })\n\n#ifdef CONFIG_PARAVIRT_XXL\nstatic inline notrace unsigned long arch_local_save_flags(void)\n{\n\treturn PVOP_CALLEE0(unsigned long, irq.save_fl);\n}\n\nstatic inline notrace void arch_local_irq_restore(unsigned long f)\n{\n\tPVOP_VCALLEE1(irq.restore_fl, f);\n}\n\nstatic inline notrace void arch_local_irq_disable(void)\n{\n\tPVOP_VCALLEE0(irq.irq_disable);\n}\n\nstatic inline notrace void arch_local_irq_enable(void)\n{\n\tPVOP_VCALLEE0(irq.irq_enable);\n}\n\nstatic inline notrace unsigned long arch_local_irq_save(void)\n{\n\tunsigned long f;\n\n\tf = arch_local_save_flags();\n\tarch_local_irq_disable();\n\treturn f;\n}\n#endif\n\n\n/* Make sure as little as possible of this mess escapes. */\n#undef PARAVIRT_CALL\n#undef __PVOP_CALL\n#undef __PVOP_VCALL\n#undef PVOP_VCALL0\n#undef PVOP_CALL0\n#undef PVOP_VCALL1\n#undef PVOP_CALL1\n#undef PVOP_VCALL2\n#undef PVOP_CALL2\n#undef PVOP_VCALL3\n#undef PVOP_CALL3\n#undef PVOP_VCALL4\n#undef PVOP_CALL4\n\nextern void default_banner(void);\n\n#else  /* __ASSEMBLY__ */\n\n#define _PVSITE(ptype, ops, word, algn)\t\t\\\n771:;\t\t\t\t\t\t\\\n\tops;\t\t\t\t\t\\\n772:;\t\t\t\t\t\t\\\n\t.pushsection .parainstructions,\"a\";\t\\\n\t .align\talgn;\t\t\t\t\\\n\t word 771b;\t\t\t\t\\\n\t .byte ptype;\t\t\t\t\\\n\t .byte 772b-771b;\t\t\t\\\n\t.popsection\n\n\n#define COND_PUSH(set, mask, reg)\t\t\t\\\n\t.if ((~(set)) & mask); push %reg; .endif\n#define COND_POP(set, mask, reg)\t\t\t\\\n\t.if ((~(set)) & mask); pop %reg; .endif\n\n#ifdef CONFIG_X86_64\n\n#define PV_SAVE_REGS(set)\t\t\t\\\n\tCOND_PUSH(set, CLBR_RAX, rax);\t\t\\\n\tCOND_PUSH(set, CLBR_RCX, rcx);\t\t\\\n\tCOND_PUSH(set, CLBR_RDX, rdx);\t\t\\\n\tCOND_PUSH(set, CLBR_RSI, rsi);\t\t\\\n\tCOND_PUSH(set, CLBR_RDI, rdi);\t\t\\\n\tCOND_PUSH(set, CLBR_R8, r8);\t\t\\\n\tCOND_PUSH(set, CLBR_R9, r9);\t\t\\\n\tCOND_PUSH(set, CLBR_R10, r10);\t\t\\\n\tCOND_PUSH(set, CLBR_R11, r11)\n#define PV_RESTORE_REGS(set)\t\t\t\\\n\tCOND_POP(set, CLBR_R11, r11);\t\t\\\n\tCOND_POP(set, CLBR_R10, r10);\t\t\\\n\tCOND_POP(set, CLBR_R9, r9);\t\t\\\n\tCOND_POP(set, CLBR_R8, r8);\t\t\\\n\tCOND_POP(set, CLBR_RDI, rdi);\t\t\\\n\tCOND_POP(set, CLBR_RSI, rsi);\t\t\\\n\tCOND_POP(set, CLBR_RDX, rdx);\t\t\\\n\tCOND_POP(set, CLBR_RCX, rcx);\t\t\\\n\tCOND_POP(set, CLBR_RAX, rax)\n\n#define PARA_PATCH(off)\t\t((off) / 8)\n#define PARA_SITE(ptype, ops)\t_PVSITE(ptype, ops, .quad, 8)\n#define PARA_INDIRECT(addr)\t*addr(%rip)\n#else\n#define PV_SAVE_REGS(set)\t\t\t\\\n\tCOND_PUSH(set, CLBR_EAX, eax);\t\t\\\n\tCOND_PUSH(set, CLBR_EDI, edi);\t\t\\\n\tCOND_PUSH(set, CLBR_ECX, ecx);\t\t\\\n\tCOND_PUSH(set, CLBR_EDX, edx)\n#define PV_RESTORE_REGS(set)\t\t\t\\\n\tCOND_POP(set, CLBR_EDX, edx);\t\t\\\n\tCOND_POP(set, CLBR_ECX, ecx);\t\t\\\n\tCOND_POP(set, CLBR_EDI, edi);\t\t\\\n\tCOND_POP(set, CLBR_EAX, eax)\n\n#define PARA_PATCH(off)\t\t((off) / 4)\n#define PARA_SITE(ptype, ops)\t_PVSITE(ptype, ops, .long, 4)\n#define PARA_INDIRECT(addr)\t*%cs:addr\n#endif\n\n#ifdef CONFIG_PARAVIRT_XXL\n#define INTERRUPT_RETURN\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_CPU_iret),\t\t\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  jmp PARA_INDIRECT(pv_ops+PV_CPU_iret);)\n\n#define DISABLE_INTERRUPTS(clobbers)\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_IRQ_irq_disable),\t\t\t\\\n\t\t  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  call PARA_INDIRECT(pv_ops+PV_IRQ_irq_disable);\t\\\n\t\t  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)\n\n#define ENABLE_INTERRUPTS(clobbers)\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_IRQ_irq_enable),\t\t\t\\\n\t\t  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  call PARA_INDIRECT(pv_ops+PV_IRQ_irq_enable);\t\t\\\n\t\t  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)\n#endif\n\n#ifdef CONFIG_X86_64\n#ifdef CONFIG_PARAVIRT_XXL\n/*\n * If swapgs is used while the userspace stack is still current,\n * there's no way to call a pvop.  The PV replacement *must* be\n * inlined, or the swapgs instruction must be trapped and emulated.\n */\n#define SWAPGS_UNSAFE_STACK\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_CPU_swapgs), swapgs)\n\n/*\n * Note: swapgs is very special, and in practise is either going to be\n * implemented with a single \"swapgs\" instruction or something very\n * special.  Either way, we don't need to save any registers for\n * it.\n */\n#define SWAPGS\t\t\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_CPU_swapgs),\t\t\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  call PARA_INDIRECT(pv_ops+PV_CPU_swapgs);\t\t\\\n\t\t )\n\n#define USERGS_SYSRET64\t\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_CPU_usergs_sysret64),\t\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  jmp PARA_INDIRECT(pv_ops+PV_CPU_usergs_sysret64);)\n\n#ifdef CONFIG_DEBUG_ENTRY\n#define SAVE_FLAGS(clobbers)                                        \\\n\tPARA_SITE(PARA_PATCH(PV_IRQ_save_fl),\t\t\t    \\\n\t\t  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);        \\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t    \\\n\t\t  call PARA_INDIRECT(pv_ops+PV_IRQ_save_fl);\t    \\\n\t\t  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)\n#endif\n#endif /* CONFIG_PARAVIRT_XXL */\n#endif\t/* CONFIG_X86_64 */\n\n#ifdef CONFIG_PARAVIRT_XXL\n\n#define GET_CR2_INTO_AX\t\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_MMU_read_cr2),\t\t\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  call PARA_INDIRECT(pv_ops+PV_MMU_read_cr2);\t\t\\\n\t\t )\n\n#endif /* CONFIG_PARAVIRT_XXL */\n\n\n#endif /* __ASSEMBLY__ */\n#else  /* CONFIG_PARAVIRT */\n# define default_banner x86_init_noop\n#endif /* !CONFIG_PARAVIRT */\n\n#ifndef __ASSEMBLY__\n#ifndef CONFIG_PARAVIRT_XXL\nstatic inline void paravirt_arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t\t  struct mm_struct *mm)\n{\n}\n#endif\n\n#ifndef CONFIG_PARAVIRT\nstatic inline void paravirt_arch_exit_mmap(struct mm_struct *mm)\n{\n}\n#endif\n#endif /* __ASSEMBLY__ */\n#endif /* _ASM_X86_PARAVIRT_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_PARAVIRT_TYPES_H\n#define _ASM_X86_PARAVIRT_TYPES_H\n\n/* Bitmask of what can be clobbered: usually at least eax. */\n#define CLBR_NONE 0\n#define CLBR_EAX  (1 << 0)\n#define CLBR_ECX  (1 << 1)\n#define CLBR_EDX  (1 << 2)\n#define CLBR_EDI  (1 << 3)\n\n#ifdef CONFIG_X86_32\n/* CLBR_ANY should match all regs platform has. For i386, that's just it */\n#define CLBR_ANY  ((1 << 4) - 1)\n\n#define CLBR_ARG_REGS\t(CLBR_EAX | CLBR_EDX | CLBR_ECX)\n#define CLBR_RET_REG\t(CLBR_EAX | CLBR_EDX)\n#define CLBR_SCRATCH\t(0)\n#else\n#define CLBR_RAX  CLBR_EAX\n#define CLBR_RCX  CLBR_ECX\n#define CLBR_RDX  CLBR_EDX\n#define CLBR_RDI  CLBR_EDI\n#define CLBR_RSI  (1 << 4)\n#define CLBR_R8   (1 << 5)\n#define CLBR_R9   (1 << 6)\n#define CLBR_R10  (1 << 7)\n#define CLBR_R11  (1 << 8)\n\n#define CLBR_ANY  ((1 << 9) - 1)\n\n#define CLBR_ARG_REGS\t(CLBR_RDI | CLBR_RSI | CLBR_RDX | \\\n\t\t\t CLBR_RCX | CLBR_R8 | CLBR_R9)\n#define CLBR_RET_REG\t(CLBR_RAX)\n#define CLBR_SCRATCH\t(CLBR_R10 | CLBR_R11)\n\n#endif /* X86_64 */\n\n#define CLBR_CALLEE_SAVE ((CLBR_ARG_REGS | CLBR_SCRATCH) & ~CLBR_RET_REG)\n\n#ifndef __ASSEMBLY__\n\n#include <asm/desc_defs.h>\n#include <asm/kmap_types.h>\n#include <asm/pgtable_types.h>\n#include <asm/nospec-branch.h>\n\nstruct page;\nstruct thread_struct;\nstruct desc_ptr;\nstruct tss_struct;\nstruct mm_struct;\nstruct desc_struct;\nstruct task_struct;\nstruct cpumask;\nstruct flush_tlb_info;\nstruct mmu_gather;\nstruct vm_area_struct;\n\n/*\n * Wrapper type for pointers to code which uses the non-standard\n * calling convention.  See PV_CALL_SAVE_REGS_THUNK below.\n */\nstruct paravirt_callee_save {\n\tvoid *func;\n};\n\n/* general info */\nstruct pv_info {\n#ifdef CONFIG_PARAVIRT_XXL\n\tunsigned int kernel_rpl;\n\tint shared_kernel_pmd;\n\n#ifdef CONFIG_X86_64\n\tu16 extra_user_64bit_cs;  /* __USER_CS if none */\n#endif\n#endif\n\n\tconst char *name;\n};\n\nstruct pv_init_ops {\n\t/*\n\t * Patch may replace one of the defined code sequences with\n\t * arbitrary code, subject to the same register constraints.\n\t * This generally means the code is not free to clobber any\n\t * registers other than EAX.  The patch function should return\n\t * the number of bytes of code generated, as we nop pad the\n\t * rest in generic code.\n\t */\n\tunsigned (*patch)(u8 type, void *insn_buff,\n\t\t\t  unsigned long addr, unsigned len);\n} __no_randomize_layout;\n\n#ifdef CONFIG_PARAVIRT_XXL\nstruct pv_lazy_ops {\n\t/* Set deferred update mode, used for batching operations. */\n\tvoid (*enter)(void);\n\tvoid (*leave)(void);\n\tvoid (*flush)(void);\n} __no_randomize_layout;\n#endif\n\nstruct pv_time_ops {\n\tunsigned long long (*sched_clock)(void);\n\tunsigned long long (*steal_clock)(int cpu);\n} __no_randomize_layout;\n\nstruct pv_cpu_ops {\n\t/* hooks for various privileged instructions */\n\tvoid (*io_delay)(void);\n\n#ifdef CONFIG_PARAVIRT_XXL\n\tunsigned long (*get_debugreg)(int regno);\n\tvoid (*set_debugreg)(int regno, unsigned long value);\n\n\tunsigned long (*read_cr0)(void);\n\tvoid (*write_cr0)(unsigned long);\n\n\tvoid (*write_cr4)(unsigned long);\n\n\t/* Segment descriptor handling */\n\tvoid (*load_tr_desc)(void);\n\tvoid (*load_gdt)(const struct desc_ptr *);\n\tvoid (*load_idt)(const struct desc_ptr *);\n\tvoid (*set_ldt)(const void *desc, unsigned entries);\n\tunsigned long (*store_tr)(void);\n\tvoid (*load_tls)(struct thread_struct *t, unsigned int cpu);\n#ifdef CONFIG_X86_64\n\tvoid (*load_gs_index)(unsigned int idx);\n#endif\n\tvoid (*write_ldt_entry)(struct desc_struct *ldt, int entrynum,\n\t\t\t\tconst void *desc);\n\tvoid (*write_gdt_entry)(struct desc_struct *,\n\t\t\t\tint entrynum, const void *desc, int size);\n\tvoid (*write_idt_entry)(gate_desc *,\n\t\t\t\tint entrynum, const gate_desc *gate);\n\tvoid (*alloc_ldt)(struct desc_struct *ldt, unsigned entries);\n\tvoid (*free_ldt)(struct desc_struct *ldt, unsigned entries);\n\n\tvoid (*load_sp0)(unsigned long sp0);\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n\tvoid (*update_io_bitmap)(void);\n#endif\n\n\tvoid (*wbinvd)(void);\n\n\t/* cpuid emulation, mostly so that caps bits can be disabled */\n\tvoid (*cpuid)(unsigned int *eax, unsigned int *ebx,\n\t\t      unsigned int *ecx, unsigned int *edx);\n\n\t/* Unsafe MSR operations.  These will warn or panic on failure. */\n\tu64 (*read_msr)(unsigned int msr);\n\tvoid (*write_msr)(unsigned int msr, unsigned low, unsigned high);\n\n\t/*\n\t * Safe MSR operations.\n\t * read sets err to 0 or -EIO.  write returns 0 or -EIO.\n\t */\n\tu64 (*read_msr_safe)(unsigned int msr, int *err);\n\tint (*write_msr_safe)(unsigned int msr, unsigned low, unsigned high);\n\n\tu64 (*read_pmc)(int counter);\n\n\t/*\n\t * Switch to usermode gs and return to 64-bit usermode using\n\t * sysret.  Only used in 64-bit kernels to return to 64-bit\n\t * processes.  Usermode register state, including %rsp, must\n\t * already be restored.\n\t */\n\tvoid (*usergs_sysret64)(void);\n\n\t/* Normal iret.  Jump to this with the standard iret stack\n\t   frame set up. */\n\tvoid (*iret)(void);\n\n\tvoid (*swapgs)(void);\n\n\tvoid (*start_context_switch)(struct task_struct *prev);\n\tvoid (*end_context_switch)(struct task_struct *next);\n#endif\n} __no_randomize_layout;\n\nstruct pv_irq_ops {\n#ifdef CONFIG_PARAVIRT_XXL\n\t/*\n\t * Get/set interrupt state.  save_fl and restore_fl are only\n\t * expected to use X86_EFLAGS_IF; all other bits\n\t * returned from save_fl are undefined, and may be ignored by\n\t * restore_fl.\n\t *\n\t * NOTE: These functions callers expect the callee to preserve\n\t * more registers than the standard C calling convention.\n\t */\n\tstruct paravirt_callee_save save_fl;\n\tstruct paravirt_callee_save restore_fl;\n\tstruct paravirt_callee_save irq_disable;\n\tstruct paravirt_callee_save irq_enable;\n\n\tvoid (*safe_halt)(void);\n\tvoid (*halt)(void);\n#endif\n} __no_randomize_layout;\n\nstruct pv_mmu_ops {\n\t/* TLB operations */\n\tvoid (*flush_tlb_user)(void);\n\tvoid (*flush_tlb_kernel)(void);\n\tvoid (*flush_tlb_one_user)(unsigned long addr);\n\tvoid (*flush_tlb_others)(const struct cpumask *cpus,\n\t\t\t\t const struct flush_tlb_info *info);\n\n\tvoid (*tlb_remove_table)(struct mmu_gather *tlb, void *table);\n\n\t/* Hook for intercepting the destruction of an mm_struct. */\n\tvoid (*exit_mmap)(struct mm_struct *mm);\n\n#ifdef CONFIG_PARAVIRT_XXL\n\tstruct paravirt_callee_save read_cr2;\n\tvoid (*write_cr2)(unsigned long);\n\n\tunsigned long (*read_cr3)(void);\n\tvoid (*write_cr3)(unsigned long);\n\n\t/* Hooks for intercepting the creation/use of an mm_struct. */\n\tvoid (*activate_mm)(struct mm_struct *prev,\n\t\t\t    struct mm_struct *next);\n\tvoid (*dup_mmap)(struct mm_struct *oldmm,\n\t\t\t struct mm_struct *mm);\n\n\t/* Hooks for allocating and freeing a pagetable top-level */\n\tint  (*pgd_alloc)(struct mm_struct *mm);\n\tvoid (*pgd_free)(struct mm_struct *mm, pgd_t *pgd);\n\n\t/*\n\t * Hooks for allocating/releasing pagetable pages when they're\n\t * attached to a pagetable\n\t */\n\tvoid (*alloc_pte)(struct mm_struct *mm, unsigned long pfn);\n\tvoid (*alloc_pmd)(struct mm_struct *mm, unsigned long pfn);\n\tvoid (*alloc_pud)(struct mm_struct *mm, unsigned long pfn);\n\tvoid (*alloc_p4d)(struct mm_struct *mm, unsigned long pfn);\n\tvoid (*release_pte)(unsigned long pfn);\n\tvoid (*release_pmd)(unsigned long pfn);\n\tvoid (*release_pud)(unsigned long pfn);\n\tvoid (*release_p4d)(unsigned long pfn);\n\n\t/* Pagetable manipulation functions */\n\tvoid (*set_pte)(pte_t *ptep, pte_t pteval);\n\tvoid (*set_pte_at)(struct mm_struct *mm, unsigned long addr,\n\t\t\t   pte_t *ptep, pte_t pteval);\n\tvoid (*set_pmd)(pmd_t *pmdp, pmd_t pmdval);\n\n\tpte_t (*ptep_modify_prot_start)(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\tpte_t *ptep);\n\tvoid (*ptep_modify_prot_commit)(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\tpte_t *ptep, pte_t pte);\n\n\tstruct paravirt_callee_save pte_val;\n\tstruct paravirt_callee_save make_pte;\n\n\tstruct paravirt_callee_save pgd_val;\n\tstruct paravirt_callee_save make_pgd;\n\n#if CONFIG_PGTABLE_LEVELS >= 3\n#ifdef CONFIG_X86_PAE\n\tvoid (*set_pte_atomic)(pte_t *ptep, pte_t pteval);\n\tvoid (*pte_clear)(struct mm_struct *mm, unsigned long addr,\n\t\t\t  pte_t *ptep);\n\tvoid (*pmd_clear)(pmd_t *pmdp);\n\n#endif\t/* CONFIG_X86_PAE */\n\n\tvoid (*set_pud)(pud_t *pudp, pud_t pudval);\n\n\tstruct paravirt_callee_save pmd_val;\n\tstruct paravirt_callee_save make_pmd;\n\n#if CONFIG_PGTABLE_LEVELS >= 4\n\tstruct paravirt_callee_save pud_val;\n\tstruct paravirt_callee_save make_pud;\n\n\tvoid (*set_p4d)(p4d_t *p4dp, p4d_t p4dval);\n\n#if CONFIG_PGTABLE_LEVELS >= 5\n\tstruct paravirt_callee_save p4d_val;\n\tstruct paravirt_callee_save make_p4d;\n\n\tvoid (*set_pgd)(pgd_t *pgdp, pgd_t pgdval);\n#endif\t/* CONFIG_PGTABLE_LEVELS >= 5 */\n\n#endif\t/* CONFIG_PGTABLE_LEVELS >= 4 */\n\n#endif\t/* CONFIG_PGTABLE_LEVELS >= 3 */\n\n\tstruct pv_lazy_ops lazy_mode;\n\n\t/* dom0 ops */\n\n\t/* Sometimes the physical address is a pfn, and sometimes its\n\t   an mfn.  We can tell which is which from the index. */\n\tvoid (*set_fixmap)(unsigned /* enum fixed_addresses */ idx,\n\t\t\t   phys_addr_t phys, pgprot_t flags);\n#endif\n} __no_randomize_layout;\n\nstruct arch_spinlock;\n#ifdef CONFIG_SMP\n#include <asm/spinlock_types.h>\n#endif\n\nstruct qspinlock;\n\nstruct pv_lock_ops {\n\tvoid (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);\n\tstruct paravirt_callee_save queued_spin_unlock;\n\n\tvoid (*wait)(u8 *ptr, u8 val);\n\tvoid (*kick)(int cpu);\n\n\tstruct paravirt_callee_save vcpu_is_preempted;\n} __no_randomize_layout;\n\n/* This contains all the paravirt structures: we get a convenient\n * number for each function using the offset which we use to indicate\n * what to patch. */\nstruct paravirt_patch_template {\n\tstruct pv_init_ops\tinit;\n\tstruct pv_time_ops\ttime;\n\tstruct pv_cpu_ops\tcpu;\n\tstruct pv_irq_ops\tirq;\n\tstruct pv_mmu_ops\tmmu;\n\tstruct pv_lock_ops\tlock;\n} __no_randomize_layout;\n\nextern struct pv_info pv_info;\nextern struct paravirt_patch_template pv_ops;\n\n#define PARAVIRT_PATCH(x)\t\t\t\t\t\\\n\t(offsetof(struct paravirt_patch_template, x) / sizeof(void *))\n\n#define paravirt_type(op)\t\t\t\t\\\n\t[paravirt_typenum] \"i\" (PARAVIRT_PATCH(op)),\t\\\n\t[paravirt_opptr] \"i\" (&(pv_ops.op))\n#define paravirt_clobber(clobber)\t\t\\\n\t[paravirt_clobber] \"i\" (clobber)\n\n/*\n * Generate some code, and mark it as patchable by the\n * apply_paravirt() alternate instruction patcher.\n */\n#define _paravirt_alt(insn_string, type, clobber)\t\\\n\t\"771:\\n\\t\" insn_string \"\\n\" \"772:\\n\"\t\t\\\n\t\".pushsection .parainstructions,\\\"a\\\"\\n\"\t\\\n\t_ASM_ALIGN \"\\n\"\t\t\t\t\t\\\n\t_ASM_PTR \" 771b\\n\"\t\t\t\t\\\n\t\"  .byte \" type \"\\n\"\t\t\t\t\\\n\t\"  .byte 772b-771b\\n\"\t\t\t\t\\\n\t\"  .short \" clobber \"\\n\"\t\t\t\\\n\t\".popsection\\n\"\n\n/* Generate patchable code, with the default asm parameters. */\n#define paravirt_alt(insn_string)\t\t\t\t\t\\\n\t_paravirt_alt(insn_string, \"%c[paravirt_typenum]\", \"%c[paravirt_clobber]\")\n\n/* Simple instruction patching code. */\n#define NATIVE_LABEL(a,x,b) \"\\n\\t.globl \" a #x \"_\" #b \"\\n\" a #x \"_\" #b \":\\n\\t\"\n\nunsigned paravirt_patch_ident_64(void *insn_buff, unsigned len);\nunsigned paravirt_patch_default(u8 type, void *insn_buff, unsigned long addr, unsigned len);\nunsigned paravirt_patch_insns(void *insn_buff, unsigned len, const char *start, const char *end);\n\nunsigned native_patch(u8 type, void *insn_buff, unsigned long addr, unsigned len);\n\nint paravirt_disable_iospace(void);\n\n/*\n * This generates an indirect call based on the operation type number.\n * The type number, computed in PARAVIRT_PATCH, is derived from the\n * offset into the paravirt_patch_template structure, and can therefore be\n * freely converted back into a structure offset.\n */\n#define PARAVIRT_CALL\t\t\t\t\t\\\n\tANNOTATE_RETPOLINE_SAFE\t\t\t\t\\\n\t\"call *%c[paravirt_opptr];\"\n\n/*\n * These macros are intended to wrap calls through one of the paravirt\n * ops structs, so that they can be later identified and patched at\n * runtime.\n *\n * Normally, a call to a pv_op function is a simple indirect call:\n * (pv_op_struct.operations)(args...).\n *\n * Unfortunately, this is a relatively slow operation for modern CPUs,\n * because it cannot necessarily determine what the destination\n * address is.  In this case, the address is a runtime constant, so at\n * the very least we can patch the call to e a simple direct call, or\n * ideally, patch an inline implementation into the callsite.  (Direct\n * calls are essentially free, because the call and return addresses\n * are completely predictable.)\n *\n * For i386, these macros rely on the standard gcc \"regparm(3)\" calling\n * convention, in which the first three arguments are placed in %eax,\n * %edx, %ecx (in that order), and the remaining arguments are placed\n * on the stack.  All caller-save registers (eax,edx,ecx) are expected\n * to be modified (either clobbered or used for return values).\n * X86_64, on the other hand, already specifies a register-based calling\n * conventions, returning at %rax, with parameteres going on %rdi, %rsi,\n * %rdx, and %rcx. Note that for this reason, x86_64 does not need any\n * special handling for dealing with 4 arguments, unlike i386.\n * However, x86_64 also have to clobber all caller saved registers, which\n * unfortunately, are quite a bit (r8 - r11)\n *\n * The call instruction itself is marked by placing its start address\n * and size into the .parainstructions section, so that\n * apply_paravirt() in arch/i386/kernel/alternative.c can do the\n * appropriate patching under the control of the backend pv_init_ops\n * implementation.\n *\n * Unfortunately there's no way to get gcc to generate the args setup\n * for the call, and then allow the call itself to be generated by an\n * inline asm.  Because of this, we must do the complete arg setup and\n * return value handling from within these macros.  This is fairly\n * cumbersome.\n *\n * There are 5 sets of PVOP_* macros for dealing with 0-4 arguments.\n * It could be extended to more arguments, but there would be little\n * to be gained from that.  For each number of arguments, there are\n * the two VCALL and CALL variants for void and non-void functions.\n *\n * When there is a return value, the invoker of the macro must specify\n * the return type.  The macro then uses sizeof() on that type to\n * determine whether its a 32 or 64 bit value, and places the return\n * in the right register(s) (just %eax for 32-bit, and %edx:%eax for\n * 64-bit). For x86_64 machines, it just returns at %rax regardless of\n * the return value size.\n *\n * 64-bit arguments are passed as a pair of adjacent 32-bit arguments\n * i386 also passes 64-bit arguments as a pair of adjacent 32-bit arguments\n * in low,high order\n *\n * Small structures are passed and returned in registers.  The macro\n * calling convention can't directly deal with this, so the wrapper\n * functions must do this.\n *\n * These PVOP_* macros are only defined within this header.  This\n * means that all uses must be wrapped in inline functions.  This also\n * makes sure the incoming and outgoing types are always correct.\n */\n#ifdef CONFIG_X86_32\n#define PVOP_VCALL_ARGS\t\t\t\t\t\t\t\\\n\tunsigned long __eax = __eax, __edx = __edx, __ecx = __ecx;\n\n#define PVOP_CALL_ARGS\t\t\tPVOP_VCALL_ARGS\n\n#define PVOP_CALL_ARG1(x)\t\t\"a\" ((unsigned long)(x))\n#define PVOP_CALL_ARG2(x)\t\t\"d\" ((unsigned long)(x))\n#define PVOP_CALL_ARG3(x)\t\t\"c\" ((unsigned long)(x))\n\n#define PVOP_VCALL_CLOBBERS\t\t\"=a\" (__eax), \"=d\" (__edx),\t\\\n\t\t\t\t\t\"=c\" (__ecx)\n#define PVOP_CALL_CLOBBERS\t\tPVOP_VCALL_CLOBBERS\n\n#define PVOP_VCALLEE_CLOBBERS\t\t\"=a\" (__eax), \"=d\" (__edx)\n#define PVOP_CALLEE_CLOBBERS\t\tPVOP_VCALLEE_CLOBBERS\n\n#define EXTRA_CLOBBERS\n#define VEXTRA_CLOBBERS\n#else  /* CONFIG_X86_64 */\n/* [re]ax isn't an arg, but the return val */\n#define PVOP_VCALL_ARGS\t\t\t\t\t\t\\\n\tunsigned long __edi = __edi, __esi = __esi,\t\t\\\n\t\t__edx = __edx, __ecx = __ecx, __eax = __eax;\n\n#define PVOP_CALL_ARGS\t\tPVOP_VCALL_ARGS\n\n#define PVOP_CALL_ARG1(x)\t\t\"D\" ((unsigned long)(x))\n#define PVOP_CALL_ARG2(x)\t\t\"S\" ((unsigned long)(x))\n#define PVOP_CALL_ARG3(x)\t\t\"d\" ((unsigned long)(x))\n#define PVOP_CALL_ARG4(x)\t\t\"c\" ((unsigned long)(x))\n\n#define PVOP_VCALL_CLOBBERS\t\"=D\" (__edi),\t\t\t\t\\\n\t\t\t\t\"=S\" (__esi), \"=d\" (__edx),\t\t\\\n\t\t\t\t\"=c\" (__ecx)\n#define PVOP_CALL_CLOBBERS\tPVOP_VCALL_CLOBBERS, \"=a\" (__eax)\n\n/* void functions are still allowed [re]ax for scratch */\n#define PVOP_VCALLEE_CLOBBERS\t\"=a\" (__eax)\n#define PVOP_CALLEE_CLOBBERS\tPVOP_VCALLEE_CLOBBERS\n\n#define EXTRA_CLOBBERS\t , \"r8\", \"r9\", \"r10\", \"r11\"\n#define VEXTRA_CLOBBERS\t , \"rax\", \"r8\", \"r9\", \"r10\", \"r11\"\n#endif\t/* CONFIG_X86_32 */\n\n#ifdef CONFIG_PARAVIRT_DEBUG\n#define PVOP_TEST_NULL(op)\tBUG_ON(pv_ops.op == NULL)\n#else\n#define PVOP_TEST_NULL(op)\t((void)pv_ops.op)\n#endif\n\n#define PVOP_RETMASK(rettype)\t\t\t\t\t\t\\\n\t({\tunsigned long __mask = ~0UL;\t\t\t\t\\\n\t\tswitch (sizeof(rettype)) {\t\t\t\t\\\n\t\tcase 1: __mask =       0xffUL; break;\t\t\t\\\n\t\tcase 2: __mask =     0xffffUL; break;\t\t\t\\\n\t\tcase 4: __mask = 0xffffffffUL; break;\t\t\t\\\n\t\tdefault: break;\t\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t__mask;\t\t\t\t\t\t\t\\\n\t})\n\n\n#define ____PVOP_CALL(rettype, op, clbr, call_clbr, extra_clbr,\t\t\\\n\t\t      pre, post, ...)\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\trettype __ret;\t\t\t\t\t\t\\\n\t\tPVOP_CALL_ARGS;\t\t\t\t\t\t\\\n\t\tPVOP_TEST_NULL(op);\t\t\t\t\t\\\n\t\t/* This is 32-bit specific, but is okay in 64-bit */\t\\\n\t\t/* since this condition will never hold */\t\t\\\n\t\tif (sizeof(rettype) > sizeof(unsigned long)) {\t\t\\\n\t\t\tasm volatile(pre\t\t\t\t\\\n\t\t\t\t     paravirt_alt(PARAVIRT_CALL)\t\\\n\t\t\t\t     post\t\t\t\t\\\n\t\t\t\t     : call_clbr, ASM_CALL_CONSTRAINT\t\\\n\t\t\t\t     : paravirt_type(op),\t\t\\\n\t\t\t\t       paravirt_clobber(clbr),\t\t\\\n\t\t\t\t       ##__VA_ARGS__\t\t\t\\\n\t\t\t\t     : \"memory\", \"cc\" extra_clbr);\t\\\n\t\t\t__ret = (rettype)((((u64)__edx) << 32) | __eax); \\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\tasm volatile(pre\t\t\t\t\\\n\t\t\t\t     paravirt_alt(PARAVIRT_CALL)\t\\\n\t\t\t\t     post\t\t\t\t\\\n\t\t\t\t     : call_clbr, ASM_CALL_CONSTRAINT\t\\\n\t\t\t\t     : paravirt_type(op),\t\t\\\n\t\t\t\t       paravirt_clobber(clbr),\t\t\\\n\t\t\t\t       ##__VA_ARGS__\t\t\t\\\n\t\t\t\t     : \"memory\", \"cc\" extra_clbr);\t\\\n\t\t\t__ret = (rettype)(__eax & PVOP_RETMASK(rettype));\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t__ret;\t\t\t\t\t\t\t\\\n\t})\n\n#define __PVOP_CALL(rettype, op, pre, post, ...)\t\t\t\\\n\t____PVOP_CALL(rettype, op, CLBR_ANY, PVOP_CALL_CLOBBERS,\t\\\n\t\t      EXTRA_CLOBBERS, pre, post, ##__VA_ARGS__)\n\n#define __PVOP_CALLEESAVE(rettype, op, pre, post, ...)\t\t\t\\\n\t____PVOP_CALL(rettype, op.func, CLBR_RET_REG,\t\t\t\\\n\t\t      PVOP_CALLEE_CLOBBERS, ,\t\t\t\t\\\n\t\t      pre, post, ##__VA_ARGS__)\n\n\n#define ____PVOP_VCALL(op, clbr, call_clbr, extra_clbr, pre, post, ...)\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tPVOP_VCALL_ARGS;\t\t\t\t\t\\\n\t\tPVOP_TEST_NULL(op);\t\t\t\t\t\\\n\t\tasm volatile(pre\t\t\t\t\t\\\n\t\t\t     paravirt_alt(PARAVIRT_CALL)\t\t\\\n\t\t\t     post\t\t\t\t\t\\\n\t\t\t     : call_clbr, ASM_CALL_CONSTRAINT\t\t\\\n\t\t\t     : paravirt_type(op),\t\t\t\\\n\t\t\t       paravirt_clobber(clbr),\t\t\t\\\n\t\t\t       ##__VA_ARGS__\t\t\t\t\\\n\t\t\t     : \"memory\", \"cc\" extra_clbr);\t\t\\\n\t})\n\n#define __PVOP_VCALL(op, pre, post, ...)\t\t\t\t\\\n\t____PVOP_VCALL(op, CLBR_ANY, PVOP_VCALL_CLOBBERS,\t\t\\\n\t\t       VEXTRA_CLOBBERS,\t\t\t\t\t\\\n\t\t       pre, post, ##__VA_ARGS__)\n\n#define __PVOP_VCALLEESAVE(op, pre, post, ...)\t\t\t\t\\\n\t____PVOP_VCALL(op.func, CLBR_RET_REG,\t\t\t\t\\\n\t\t      PVOP_VCALLEE_CLOBBERS, ,\t\t\t\t\\\n\t\t      pre, post, ##__VA_ARGS__)\n\n\n\n#define PVOP_CALL0(rettype, op)\t\t\t\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\")\n#define PVOP_VCALL0(op)\t\t\t\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\")\n\n#define PVOP_CALLEE0(rettype, op)\t\t\t\t\t\\\n\t__PVOP_CALLEESAVE(rettype, op, \"\", \"\")\n#define PVOP_VCALLEE0(op)\t\t\t\t\t\t\\\n\t__PVOP_VCALLEESAVE(op, \"\", \"\")\n\n\n#define PVOP_CALL1(rettype, op, arg1)\t\t\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1))\n#define PVOP_VCALL1(op, arg1)\t\t\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\", PVOP_CALL_ARG1(arg1))\n\n#define PVOP_CALLEE1(rettype, op, arg1)\t\t\t\t\t\\\n\t__PVOP_CALLEESAVE(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1))\n#define PVOP_VCALLEE1(op, arg1)\t\t\t\t\t\t\\\n\t__PVOP_VCALLEESAVE(op, \"\", \"\", PVOP_CALL_ARG1(arg1))\n\n\n#define PVOP_CALL2(rettype, op, arg1, arg2)\t\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\\\n\t\t    PVOP_CALL_ARG2(arg2))\n#define PVOP_VCALL2(op, arg1, arg2)\t\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\t\\\n\t\t     PVOP_CALL_ARG2(arg2))\n\n#define PVOP_CALLEE2(rettype, op, arg1, arg2)\t\t\t\t\\\n\t__PVOP_CALLEESAVE(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\\\n\t\t\t  PVOP_CALL_ARG2(arg2))\n#define PVOP_VCALLEE2(op, arg1, arg2)\t\t\t\t\t\\\n\t__PVOP_VCALLEESAVE(op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\\\n\t\t\t   PVOP_CALL_ARG2(arg2))\n\n\n#define PVOP_CALL3(rettype, op, arg1, arg2, arg3)\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\\\n\t\t    PVOP_CALL_ARG2(arg2), PVOP_CALL_ARG3(arg3))\n#define PVOP_VCALL3(op, arg1, arg2, arg3)\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\t\\\n\t\t     PVOP_CALL_ARG2(arg2), PVOP_CALL_ARG3(arg3))\n\n/* This is the only difference in x86_64. We can make it much simpler */\n#ifdef CONFIG_X86_32\n#define PVOP_CALL4(rettype, op, arg1, arg2, arg3, arg4)\t\t\t\\\n\t__PVOP_CALL(rettype, op,\t\t\t\t\t\\\n\t\t    \"push %[_arg4];\", \"lea 4(%%esp),%%esp;\",\t\t\\\n\t\t    PVOP_CALL_ARG1(arg1), PVOP_CALL_ARG2(arg2),\t\t\\\n\t\t    PVOP_CALL_ARG3(arg3), [_arg4] \"mr\" ((u32)(arg4)))\n#define PVOP_VCALL4(op, arg1, arg2, arg3, arg4)\t\t\t\t\\\n\t__PVOP_VCALL(op,\t\t\t\t\t\t\\\n\t\t    \"push %[_arg4];\", \"lea 4(%%esp),%%esp;\",\t\t\\\n\t\t    \"0\" ((u32)(arg1)), \"1\" ((u32)(arg2)),\t\t\\\n\t\t    \"2\" ((u32)(arg3)), [_arg4] \"mr\" ((u32)(arg4)))\n#else\n#define PVOP_CALL4(rettype, op, arg1, arg2, arg3, arg4)\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\",\t\t\t\t\\\n\t\t    PVOP_CALL_ARG1(arg1), PVOP_CALL_ARG2(arg2),\t\t\\\n\t\t    PVOP_CALL_ARG3(arg3), PVOP_CALL_ARG4(arg4))\n#define PVOP_VCALL4(op, arg1, arg2, arg3, arg4)\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\",\t\t\t\t\t\\\n\t\t     PVOP_CALL_ARG1(arg1), PVOP_CALL_ARG2(arg2),\t\\\n\t\t     PVOP_CALL_ARG3(arg3), PVOP_CALL_ARG4(arg4))\n#endif\n\n/* Lazy mode for batching updates / context switch */\nenum paravirt_lazy_mode {\n\tPARAVIRT_LAZY_NONE,\n\tPARAVIRT_LAZY_MMU,\n\tPARAVIRT_LAZY_CPU,\n};\n\nenum paravirt_lazy_mode paravirt_get_lazy_mode(void);\nvoid paravirt_start_context_switch(struct task_struct *prev);\nvoid paravirt_end_context_switch(struct task_struct *next);\n\nvoid paravirt_enter_lazy_mmu(void);\nvoid paravirt_leave_lazy_mmu(void);\nvoid paravirt_flush_lazy_mmu(void);\n\nvoid _paravirt_nop(void);\nu64 _paravirt_ident_64(u64);\n\n#define paravirt_nop\t((void *)_paravirt_nop)\n\n/* These all sit in the .parainstructions section to tell us what to patch. */\nstruct paravirt_patch_site {\n\tu8 *instr;\t\t/* original instructions */\n\tu8 type;\t\t/* type of this instruction */\n\tu8 len;\t\t\t/* length of original instruction */\n};\n\nextern struct paravirt_patch_site __parainstructions[],\n\t__parainstructions_end[];\n\n#endif\t/* __ASSEMBLY__ */\n\n#endif\t/* _ASM_X86_PARAVIRT_TYPES_H */\n", "// SPDX-License-Identifier: GPL-2.0-or-later\n/*  Paravirtualization interfaces\n    Copyright (C) 2006 Rusty Russell IBM Corporation\n\n\n    2007 - x86_64 support added by Glauber de Oliveira Costa, Red Hat Inc\n*/\n\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/efi.h>\n#include <linux/bcd.h>\n#include <linux/highmem.h>\n#include <linux/kprobes.h>\n#include <linux/pgtable.h>\n\n#include <asm/bug.h>\n#include <asm/paravirt.h>\n#include <asm/debugreg.h>\n#include <asm/desc.h>\n#include <asm/setup.h>\n#include <asm/time.h>\n#include <asm/pgalloc.h>\n#include <asm/irq.h>\n#include <asm/delay.h>\n#include <asm/fixmap.h>\n#include <asm/apic.h>\n#include <asm/tlbflush.h>\n#include <asm/timer.h>\n#include <asm/special_insns.h>\n#include <asm/tlb.h>\n#include <asm/io_bitmap.h>\n\n/*\n * nop stub, which must not clobber anything *including the stack* to\n * avoid confusing the entry prologues.\n */\nextern void _paravirt_nop(void);\nasm (\".pushsection .entry.text, \\\"ax\\\"\\n\"\n     \".global _paravirt_nop\\n\"\n     \"_paravirt_nop:\\n\\t\"\n     \"ret\\n\\t\"\n     \".size _paravirt_nop, . - _paravirt_nop\\n\\t\"\n     \".type _paravirt_nop, @function\\n\\t\"\n     \".popsection\");\n\nvoid __init default_banner(void)\n{\n\tprintk(KERN_INFO \"Booting paravirtualized kernel on %s\\n\",\n\t       pv_info.name);\n}\n\n/* Undefined instruction for dealing with missing ops pointers. */\nstatic const unsigned char ud2a[] = { 0x0f, 0x0b };\n\nstruct branch {\n\tunsigned char opcode;\n\tu32 delta;\n} __attribute__((packed));\n\nstatic unsigned paravirt_patch_call(void *insn_buff, const void *target,\n\t\t\t\t    unsigned long addr, unsigned len)\n{\n\tconst int call_len = 5;\n\tstruct branch *b = insn_buff;\n\tunsigned long delta = (unsigned long)target - (addr+call_len);\n\n\tif (len < call_len) {\n\t\tpr_warn(\"paravirt: Failed to patch indirect CALL at %ps\\n\", (void *)addr);\n\t\t/* Kernel might not be viable if patching fails, bail out: */\n\t\tBUG_ON(1);\n\t}\n\n\tb->opcode = 0xe8; /* call */\n\tb->delta = delta;\n\tBUILD_BUG_ON(sizeof(*b) != call_len);\n\n\treturn call_len;\n}\n\n#ifdef CONFIG_PARAVIRT_XXL\n/* identity function, which can be inlined */\nu64 notrace _paravirt_ident_64(u64 x)\n{\n\treturn x;\n}\n\nstatic unsigned paravirt_patch_jmp(void *insn_buff, const void *target,\n\t\t\t\t   unsigned long addr, unsigned len)\n{\n\tstruct branch *b = insn_buff;\n\tunsigned long delta = (unsigned long)target - (addr+5);\n\n\tif (len < 5) {\n#ifdef CONFIG_RETPOLINE\n\t\tWARN_ONCE(1, \"Failing to patch indirect JMP in %ps\\n\", (void *)addr);\n#endif\n\t\treturn len;\t/* call too long for patch site */\n\t}\n\n\tb->opcode = 0xe9;\t/* jmp */\n\tb->delta = delta;\n\n\treturn 5;\n}\n#endif\n\nDEFINE_STATIC_KEY_TRUE(virt_spin_lock_key);\n\nvoid __init native_pv_lock_init(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\tstatic_branch_disable(&virt_spin_lock_key);\n}\n\nunsigned paravirt_patch_default(u8 type, void *insn_buff,\n\t\t\t\tunsigned long addr, unsigned len)\n{\n\t/*\n\t * Neat trick to map patch type back to the call within the\n\t * corresponding structure.\n\t */\n\tvoid *opfunc = *((void **)&pv_ops + type);\n\tunsigned ret;\n\n\tif (opfunc == NULL)\n\t\t/* If there's no function, patch it with a ud2a (BUG) */\n\t\tret = paravirt_patch_insns(insn_buff, len, ud2a, ud2a+sizeof(ud2a));\n\telse if (opfunc == _paravirt_nop)\n\t\tret = 0;\n\n#ifdef CONFIG_PARAVIRT_XXL\n\t/* identity functions just return their single argument */\n\telse if (opfunc == _paravirt_ident_64)\n\t\tret = paravirt_patch_ident_64(insn_buff, len);\n\n\telse if (type == PARAVIRT_PATCH(cpu.iret) ||\n\t\t type == PARAVIRT_PATCH(cpu.usergs_sysret64))\n\t\t/* If operation requires a jmp, then jmp */\n\t\tret = paravirt_patch_jmp(insn_buff, opfunc, addr, len);\n#endif\n\telse\n\t\t/* Otherwise call the function. */\n\t\tret = paravirt_patch_call(insn_buff, opfunc, addr, len);\n\n\treturn ret;\n}\n\nunsigned paravirt_patch_insns(void *insn_buff, unsigned len,\n\t\t\t      const char *start, const char *end)\n{\n\tunsigned insn_len = end - start;\n\n\t/* Alternative instruction is too large for the patch site and we cannot continue: */\n\tBUG_ON(insn_len > len || start == NULL);\n\n\tmemcpy(insn_buff, start, insn_len);\n\n\treturn insn_len;\n}\n\nstruct static_key paravirt_steal_enabled;\nstruct static_key paravirt_steal_rq_enabled;\n\nstatic u64 native_steal_clock(int cpu)\n{\n\treturn 0;\n}\n\n/* These are in entry.S */\nextern void native_iret(void);\nextern void native_usergs_sysret64(void);\n\nstatic struct resource reserve_ioports = {\n\t.start = 0,\n\t.end = IO_SPACE_LIMIT,\n\t.name = \"paravirt-ioport\",\n\t.flags = IORESOURCE_IO | IORESOURCE_BUSY,\n};\n\n/*\n * Reserve the whole legacy IO space to prevent any legacy drivers\n * from wasting time probing for their hardware.  This is a fairly\n * brute-force approach to disabling all non-virtual drivers.\n *\n * Note that this must be called very early to have any effect.\n */\nint paravirt_disable_iospace(void)\n{\n\treturn request_resource(&ioport_resource, &reserve_ioports);\n}\n\nstatic DEFINE_PER_CPU(enum paravirt_lazy_mode, paravirt_lazy_mode) = PARAVIRT_LAZY_NONE;\n\nstatic inline void enter_lazy(enum paravirt_lazy_mode mode)\n{\n\tBUG_ON(this_cpu_read(paravirt_lazy_mode) != PARAVIRT_LAZY_NONE);\n\n\tthis_cpu_write(paravirt_lazy_mode, mode);\n}\n\nstatic void leave_lazy(enum paravirt_lazy_mode mode)\n{\n\tBUG_ON(this_cpu_read(paravirt_lazy_mode) != mode);\n\n\tthis_cpu_write(paravirt_lazy_mode, PARAVIRT_LAZY_NONE);\n}\n\nvoid paravirt_enter_lazy_mmu(void)\n{\n\tenter_lazy(PARAVIRT_LAZY_MMU);\n}\n\nvoid paravirt_leave_lazy_mmu(void)\n{\n\tleave_lazy(PARAVIRT_LAZY_MMU);\n}\n\nvoid paravirt_flush_lazy_mmu(void)\n{\n\tpreempt_disable();\n\n\tif (paravirt_get_lazy_mode() == PARAVIRT_LAZY_MMU) {\n\t\tarch_leave_lazy_mmu_mode();\n\t\tarch_enter_lazy_mmu_mode();\n\t}\n\n\tpreempt_enable();\n}\n\n#ifdef CONFIG_PARAVIRT_XXL\nvoid paravirt_start_context_switch(struct task_struct *prev)\n{\n\tBUG_ON(preemptible());\n\n\tif (this_cpu_read(paravirt_lazy_mode) == PARAVIRT_LAZY_MMU) {\n\t\tarch_leave_lazy_mmu_mode();\n\t\tset_ti_thread_flag(task_thread_info(prev), TIF_LAZY_MMU_UPDATES);\n\t}\n\tenter_lazy(PARAVIRT_LAZY_CPU);\n}\n\nvoid paravirt_end_context_switch(struct task_struct *next)\n{\n\tBUG_ON(preemptible());\n\n\tleave_lazy(PARAVIRT_LAZY_CPU);\n\n\tif (test_and_clear_ti_thread_flag(task_thread_info(next), TIF_LAZY_MMU_UPDATES))\n\t\tarch_enter_lazy_mmu_mode();\n}\n#endif\n\nenum paravirt_lazy_mode paravirt_get_lazy_mode(void)\n{\n\tif (in_interrupt())\n\t\treturn PARAVIRT_LAZY_NONE;\n\n\treturn this_cpu_read(paravirt_lazy_mode);\n}\n\nstruct pv_info pv_info = {\n\t.name = \"bare hardware\",\n#ifdef CONFIG_PARAVIRT_XXL\n\t.kernel_rpl = 0,\n\t.shared_kernel_pmd = 1,\t/* Only used when CONFIG_X86_PAE is set */\n\n#ifdef CONFIG_X86_64\n\t.extra_user_64bit_cs = __USER_CS,\n#endif\n#endif\n};\n\n/* 64-bit pagetable entries */\n#define PTE_IDENT\t__PV_IS_CALLEE_SAVE(_paravirt_ident_64)\n\nstruct paravirt_patch_template pv_ops = {\n\t/* Init ops. */\n\t.init.patch\t\t= native_patch,\n\n\t/* Time ops. */\n\t.time.sched_clock\t= native_sched_clock,\n\t.time.steal_clock\t= native_steal_clock,\n\n\t/* Cpu ops. */\n\t.cpu.io_delay\t\t= native_io_delay,\n\n#ifdef CONFIG_PARAVIRT_XXL\n\t.cpu.cpuid\t\t= native_cpuid,\n\t.cpu.get_debugreg\t= native_get_debugreg,\n\t.cpu.set_debugreg\t= native_set_debugreg,\n\t.cpu.read_cr0\t\t= native_read_cr0,\n\t.cpu.write_cr0\t\t= native_write_cr0,\n\t.cpu.write_cr4\t\t= native_write_cr4,\n\t.cpu.wbinvd\t\t= native_wbinvd,\n\t.cpu.read_msr\t\t= native_read_msr,\n\t.cpu.write_msr\t\t= native_write_msr,\n\t.cpu.read_msr_safe\t= native_read_msr_safe,\n\t.cpu.write_msr_safe\t= native_write_msr_safe,\n\t.cpu.read_pmc\t\t= native_read_pmc,\n\t.cpu.load_tr_desc\t= native_load_tr_desc,\n\t.cpu.set_ldt\t\t= native_set_ldt,\n\t.cpu.load_gdt\t\t= native_load_gdt,\n\t.cpu.load_idt\t\t= native_load_idt,\n\t.cpu.store_tr\t\t= native_store_tr,\n\t.cpu.load_tls\t\t= native_load_tls,\n#ifdef CONFIG_X86_64\n\t.cpu.load_gs_index\t= native_load_gs_index,\n#endif\n\t.cpu.write_ldt_entry\t= native_write_ldt_entry,\n\t.cpu.write_gdt_entry\t= native_write_gdt_entry,\n\t.cpu.write_idt_entry\t= native_write_idt_entry,\n\n\t.cpu.alloc_ldt\t\t= paravirt_nop,\n\t.cpu.free_ldt\t\t= paravirt_nop,\n\n\t.cpu.load_sp0\t\t= native_load_sp0,\n\n#ifdef CONFIG_X86_64\n\t.cpu.usergs_sysret64\t= native_usergs_sysret64,\n#endif\n\t.cpu.iret\t\t= native_iret,\n\t.cpu.swapgs\t\t= native_swapgs,\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n\t.cpu.update_io_bitmap\t= native_tss_update_io_bitmap,\n#endif\n\n\t.cpu.start_context_switch\t= paravirt_nop,\n\t.cpu.end_context_switch\t\t= paravirt_nop,\n\n\t/* Irq ops. */\n\t.irq.save_fl\t\t= __PV_IS_CALLEE_SAVE(native_save_fl),\n\t.irq.restore_fl\t\t= __PV_IS_CALLEE_SAVE(native_restore_fl),\n\t.irq.irq_disable\t= __PV_IS_CALLEE_SAVE(native_irq_disable),\n\t.irq.irq_enable\t\t= __PV_IS_CALLEE_SAVE(native_irq_enable),\n\t.irq.safe_halt\t\t= native_safe_halt,\n\t.irq.halt\t\t= native_halt,\n#endif /* CONFIG_PARAVIRT_XXL */\n\n\t/* Mmu ops. */\n\t.mmu.flush_tlb_user\t= native_flush_tlb_local,\n\t.mmu.flush_tlb_kernel\t= native_flush_tlb_global,\n\t.mmu.flush_tlb_one_user\t= native_flush_tlb_one_user,\n\t.mmu.flush_tlb_others\t= native_flush_tlb_others,\n\t.mmu.tlb_remove_table\t=\n\t\t\t(void (*)(struct mmu_gather *, void *))tlb_remove_page,\n\n\t.mmu.exit_mmap\t\t= paravirt_nop,\n\n#ifdef CONFIG_PARAVIRT_XXL\n\t.mmu.read_cr2\t\t= __PV_IS_CALLEE_SAVE(native_read_cr2),\n\t.mmu.write_cr2\t\t= native_write_cr2,\n\t.mmu.read_cr3\t\t= __native_read_cr3,\n\t.mmu.write_cr3\t\t= native_write_cr3,\n\n\t.mmu.pgd_alloc\t\t= __paravirt_pgd_alloc,\n\t.mmu.pgd_free\t\t= paravirt_nop,\n\n\t.mmu.alloc_pte\t\t= paravirt_nop,\n\t.mmu.alloc_pmd\t\t= paravirt_nop,\n\t.mmu.alloc_pud\t\t= paravirt_nop,\n\t.mmu.alloc_p4d\t\t= paravirt_nop,\n\t.mmu.release_pte\t= paravirt_nop,\n\t.mmu.release_pmd\t= paravirt_nop,\n\t.mmu.release_pud\t= paravirt_nop,\n\t.mmu.release_p4d\t= paravirt_nop,\n\n\t.mmu.set_pte\t\t= native_set_pte,\n\t.mmu.set_pte_at\t\t= native_set_pte_at,\n\t.mmu.set_pmd\t\t= native_set_pmd,\n\n\t.mmu.ptep_modify_prot_start\t= __ptep_modify_prot_start,\n\t.mmu.ptep_modify_prot_commit\t= __ptep_modify_prot_commit,\n\n#if CONFIG_PGTABLE_LEVELS >= 3\n#ifdef CONFIG_X86_PAE\n\t.mmu.set_pte_atomic\t= native_set_pte_atomic,\n\t.mmu.pte_clear\t\t= native_pte_clear,\n\t.mmu.pmd_clear\t\t= native_pmd_clear,\n#endif\n\t.mmu.set_pud\t\t= native_set_pud,\n\n\t.mmu.pmd_val\t\t= PTE_IDENT,\n\t.mmu.make_pmd\t\t= PTE_IDENT,\n\n#if CONFIG_PGTABLE_LEVELS >= 4\n\t.mmu.pud_val\t\t= PTE_IDENT,\n\t.mmu.make_pud\t\t= PTE_IDENT,\n\n\t.mmu.set_p4d\t\t= native_set_p4d,\n\n#if CONFIG_PGTABLE_LEVELS >= 5\n\t.mmu.p4d_val\t\t= PTE_IDENT,\n\t.mmu.make_p4d\t\t= PTE_IDENT,\n\n\t.mmu.set_pgd\t\t= native_set_pgd,\n#endif /* CONFIG_PGTABLE_LEVELS >= 5 */\n#endif /* CONFIG_PGTABLE_LEVELS >= 4 */\n#endif /* CONFIG_PGTABLE_LEVELS >= 3 */\n\n\t.mmu.pte_val\t\t= PTE_IDENT,\n\t.mmu.pgd_val\t\t= PTE_IDENT,\n\n\t.mmu.make_pte\t\t= PTE_IDENT,\n\t.mmu.make_pgd\t\t= PTE_IDENT,\n\n\t.mmu.dup_mmap\t\t= paravirt_nop,\n\t.mmu.activate_mm\t= paravirt_nop,\n\n\t.mmu.lazy_mode = {\n\t\t.enter\t\t= paravirt_nop,\n\t\t.leave\t\t= paravirt_nop,\n\t\t.flush\t\t= paravirt_nop,\n\t},\n\n\t.mmu.set_fixmap\t\t= native_set_fixmap,\n#endif /* CONFIG_PARAVIRT_XXL */\n\n#if defined(CONFIG_PARAVIRT_SPINLOCKS)\n\t/* Lock ops. */\n#ifdef CONFIG_SMP\n\t.lock.queued_spin_lock_slowpath\t= native_queued_spin_lock_slowpath,\n\t.lock.queued_spin_unlock\t=\n\t\t\t\tPV_CALLEE_SAVE(__native_queued_spin_unlock),\n\t.lock.wait\t\t\t= paravirt_nop,\n\t.lock.kick\t\t\t= paravirt_nop,\n\t.lock.vcpu_is_preempted\t\t=\n\t\t\t\tPV_CALLEE_SAVE(__native_vcpu_is_preempted),\n#endif /* SMP */\n#endif\n};\n\n#ifdef CONFIG_PARAVIRT_XXL\n/* At this point, native_get/set_debugreg has real function entries */\nNOKPROBE_SYMBOL(native_get_debugreg);\nNOKPROBE_SYMBOL(native_set_debugreg);\nNOKPROBE_SYMBOL(native_load_idt);\n#endif\n\nEXPORT_SYMBOL(pv_ops);\nEXPORT_SYMBOL_GPL(pv_info);\n", "// SPDX-License-Identifier: GPL-2.0\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/prctl.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/pm.h>\n#include <linux/tick.h>\n#include <linux/random.h>\n#include <linux/user-return-notifier.h>\n#include <linux/dmi.h>\n#include <linux/utsname.h>\n#include <linux/stackprotector.h>\n#include <linux/cpuidle.h>\n#include <linux/acpi.h>\n#include <linux/elf-randomize.h>\n#include <trace/events/power.h>\n#include <linux/hw_breakpoint.h>\n#include <asm/cpu.h>\n#include <asm/apic.h>\n#include <linux/uaccess.h>\n#include <asm/mwait.h>\n#include <asm/fpu/internal.h>\n#include <asm/debugreg.h>\n#include <asm/nmi.h>\n#include <asm/tlbflush.h>\n#include <asm/mce.h>\n#include <asm/vm86.h>\n#include <asm/switch_to.h>\n#include <asm/desc.h>\n#include <asm/prctl.h>\n#include <asm/spec-ctrl.h>\n#include <asm/io_bitmap.h>\n#include <asm/proto.h>\n\n#include \"process.h\"\n\n/*\n * per-CPU TSS segments. Threads are completely 'soft' on Linux,\n * no more per-task TSS's. The TSS size is kept cacheline-aligned\n * so they are allowed to end up in the .data..cacheline_aligned\n * section. Since TSS's are completely CPU-local, we want them\n * on exact cacheline boundaries, to eliminate cacheline ping-pong.\n */\n__visible DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw) = {\n\t.x86_tss = {\n\t\t/*\n\t\t * .sp0 is only used when entering ring 0 from a lower\n\t\t * privilege level.  Since the init task never runs anything\n\t\t * but ring 0 code, there is no need for a valid value here.\n\t\t * Poison it.\n\t\t */\n\t\t.sp0 = (1UL << (BITS_PER_LONG-1)) + 1,\n\n\t\t/*\n\t\t * .sp1 is cpu_current_top_of_stack.  The init task never\n\t\t * runs user code, but cpu_current_top_of_stack should still\n\t\t * be well defined before the first context switch.\n\t\t */\n\t\t.sp1 = TOP_OF_INIT_STACK,\n\n#ifdef CONFIG_X86_32\n\t\t.ss0 = __KERNEL_DS,\n\t\t.ss1 = __KERNEL_CS,\n#endif\n\t\t.io_bitmap_base\t= IO_BITMAP_OFFSET_INVALID,\n\t },\n};\nEXPORT_PER_CPU_SYMBOL(cpu_tss_rw);\n\nDEFINE_PER_CPU(bool, __tss_limit_invalid);\nEXPORT_PER_CPU_SYMBOL_GPL(__tss_limit_invalid);\n\n/*\n * this gets called so that we can store lazy state into memory and copy the\n * current task into the new thread.\n */\nint arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)\n{\n\tmemcpy(dst, src, arch_task_struct_size);\n#ifdef CONFIG_VM86\n\tdst->thread.vm86 = NULL;\n#endif\n\n\treturn fpu__copy(dst, src);\n}\n\n/*\n * Free thread data structures etc..\n */\nvoid exit_thread(struct task_struct *tsk)\n{\n\tstruct thread_struct *t = &tsk->thread;\n\tstruct fpu *fpu = &t->fpu;\n\n\tif (test_thread_flag(TIF_IO_BITMAP))\n\t\tio_bitmap_exit(tsk);\n\n\tfree_vm86(t);\n\n\tfpu__drop(fpu);\n}\n\nstatic int set_new_tls(struct task_struct *p, unsigned long tls)\n{\n\tstruct user_desc __user *utls = (struct user_desc __user *)tls;\n\n\tif (in_ia32_syscall())\n\t\treturn do_set_thread_area(p, -1, utls, 0);\n\telse\n\t\treturn do_set_thread_area_64(p, ARCH_SET_FS, tls);\n}\n\nint copy_thread_tls(unsigned long clone_flags, unsigned long sp,\n\t\t    unsigned long arg, struct task_struct *p, unsigned long tls)\n{\n\tstruct inactive_task_frame *frame;\n\tstruct fork_frame *fork_frame;\n\tstruct pt_regs *childregs;\n\tint ret = 0;\n\n\tchildregs = task_pt_regs(p);\n\tfork_frame = container_of(childregs, struct fork_frame, regs);\n\tframe = &fork_frame->frame;\n\n\tframe->bp = 0;\n\tframe->ret_addr = (unsigned long) ret_from_fork;\n\tp->thread.sp = (unsigned long) fork_frame;\n\tp->thread.io_bitmap = NULL;\n\tmemset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));\n\n#ifdef CONFIG_X86_64\n\tsavesegment(gs, p->thread.gsindex);\n\tp->thread.gsbase = p->thread.gsindex ? 0 : current->thread.gsbase;\n\tsavesegment(fs, p->thread.fsindex);\n\tp->thread.fsbase = p->thread.fsindex ? 0 : current->thread.fsbase;\n\tsavesegment(es, p->thread.es);\n\tsavesegment(ds, p->thread.ds);\n#else\n\tp->thread.sp0 = (unsigned long) (childregs + 1);\n\t/*\n\t * Clear all status flags including IF and set fixed bit. 64bit\n\t * does not have this initialization as the frame does not contain\n\t * flags. The flags consistency (especially vs. AC) is there\n\t * ensured via objtool, which lacks 32bit support.\n\t */\n\tframe->flags = X86_EFLAGS_FIXED;\n#endif\n\n\t/* Kernel thread ? */\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tkthread_frame_init(frame, sp, arg);\n\t\treturn 0;\n\t}\n\n\tframe->bx = 0;\n\t*childregs = *current_pt_regs();\n\tchildregs->ax = 0;\n\tif (sp)\n\t\tchildregs->sp = sp;\n\n#ifdef CONFIG_X86_32\n\ttask_user_gs(p) = get_user_gs(current_pt_regs());\n#endif\n\n\t/* Set a new TLS for the child thread? */\n\tif (clone_flags & CLONE_SETTLS)\n\t\tret = set_new_tls(p, tls);\n\n\tif (!ret && unlikely(test_tsk_thread_flag(current, TIF_IO_BITMAP)))\n\t\tio_bitmap_share(p);\n\n\treturn ret;\n}\n\nvoid flush_thread(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tflush_ptrace_hw_breakpoint(tsk);\n\tmemset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));\n\n\tfpu__clear_all(&tsk->thread.fpu);\n}\n\nvoid disable_TSC(void)\n{\n\tpreempt_disable();\n\tif (!test_and_set_thread_flag(TIF_NOTSC))\n\t\t/*\n\t\t * Must flip the CPU state synchronously with\n\t\t * TIF_NOTSC in the current running context.\n\t\t */\n\t\tcr4_set_bits(X86_CR4_TSD);\n\tpreempt_enable();\n}\n\nstatic void enable_TSC(void)\n{\n\tpreempt_disable();\n\tif (test_and_clear_thread_flag(TIF_NOTSC))\n\t\t/*\n\t\t * Must flip the CPU state synchronously with\n\t\t * TIF_NOTSC in the current running context.\n\t\t */\n\t\tcr4_clear_bits(X86_CR4_TSD);\n\tpreempt_enable();\n}\n\nint get_tsc_mode(unsigned long adr)\n{\n\tunsigned int val;\n\n\tif (test_thread_flag(TIF_NOTSC))\n\t\tval = PR_TSC_SIGSEGV;\n\telse\n\t\tval = PR_TSC_ENABLE;\n\n\treturn put_user(val, (unsigned int __user *)adr);\n}\n\nint set_tsc_mode(unsigned int val)\n{\n\tif (val == PR_TSC_SIGSEGV)\n\t\tdisable_TSC();\n\telse if (val == PR_TSC_ENABLE)\n\t\tenable_TSC();\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nDEFINE_PER_CPU(u64, msr_misc_features_shadow);\n\nstatic void set_cpuid_faulting(bool on)\n{\n\tu64 msrval;\n\n\tmsrval = this_cpu_read(msr_misc_features_shadow);\n\tmsrval &= ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT;\n\tmsrval |= (on << MSR_MISC_FEATURES_ENABLES_CPUID_FAULT_BIT);\n\tthis_cpu_write(msr_misc_features_shadow, msrval);\n\twrmsrl(MSR_MISC_FEATURES_ENABLES, msrval);\n}\n\nstatic void disable_cpuid(void)\n{\n\tpreempt_disable();\n\tif (!test_and_set_thread_flag(TIF_NOCPUID)) {\n\t\t/*\n\t\t * Must flip the CPU state synchronously with\n\t\t * TIF_NOCPUID in the current running context.\n\t\t */\n\t\tset_cpuid_faulting(true);\n\t}\n\tpreempt_enable();\n}\n\nstatic void enable_cpuid(void)\n{\n\tpreempt_disable();\n\tif (test_and_clear_thread_flag(TIF_NOCPUID)) {\n\t\t/*\n\t\t * Must flip the CPU state synchronously with\n\t\t * TIF_NOCPUID in the current running context.\n\t\t */\n\t\tset_cpuid_faulting(false);\n\t}\n\tpreempt_enable();\n}\n\nstatic int get_cpuid_mode(void)\n{\n\treturn !test_thread_flag(TIF_NOCPUID);\n}\n\nstatic int set_cpuid_mode(struct task_struct *task, unsigned long cpuid_enabled)\n{\n\tif (!boot_cpu_has(X86_FEATURE_CPUID_FAULT))\n\t\treturn -ENODEV;\n\n\tif (cpuid_enabled)\n\t\tenable_cpuid();\n\telse\n\t\tdisable_cpuid();\n\n\treturn 0;\n}\n\n/*\n * Called immediately after a successful exec.\n */\nvoid arch_setup_new_exec(void)\n{\n\t/* If cpuid was previously disabled for this task, re-enable it. */\n\tif (test_thread_flag(TIF_NOCPUID))\n\t\tenable_cpuid();\n\n\t/*\n\t * Don't inherit TIF_SSBD across exec boundary when\n\t * PR_SPEC_DISABLE_NOEXEC is used.\n\t */\n\tif (test_thread_flag(TIF_SSBD) &&\n\t    task_spec_ssb_noexec(current)) {\n\t\tclear_thread_flag(TIF_SSBD);\n\t\ttask_clear_spec_ssb_disable(current);\n\t\ttask_clear_spec_ssb_noexec(current);\n\t\tspeculation_ctrl_update(task_thread_info(current)->flags);\n\t}\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic inline void tss_invalidate_io_bitmap(struct tss_struct *tss)\n{\n\t/*\n\t * Invalidate the I/O bitmap by moving io_bitmap_base outside the\n\t * TSS limit so any subsequent I/O access from user space will\n\t * trigger a #GP.\n\t *\n\t * This is correct even when VMEXIT rewrites the TSS limit\n\t * to 0x67 as the only requirement is that the base points\n\t * outside the limit.\n\t */\n\ttss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;\n}\n\nstatic inline void switch_to_bitmap(unsigned long tifp)\n{\n\t/*\n\t * Invalidate I/O bitmap if the previous task used it. This prevents\n\t * any possible leakage of an active I/O bitmap.\n\t *\n\t * If the next task has an I/O bitmap it will handle it on exit to\n\t * user mode.\n\t */\n\tif (tifp & _TIF_IO_BITMAP)\n\t\ttss_invalidate_io_bitmap(this_cpu_ptr(&cpu_tss_rw));\n}\n\nstatic void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)\n{\n\t/*\n\t * Copy at least the byte range of the incoming tasks bitmap which\n\t * covers the permitted I/O ports.\n\t *\n\t * If the previous task which used an I/O bitmap had more bits\n\t * permitted, then the copy needs to cover those as well so they\n\t * get turned off.\n\t */\n\tmemcpy(tss->io_bitmap.bitmap, iobm->bitmap,\n\t       max(tss->io_bitmap.prev_max, iobm->max));\n\n\t/*\n\t * Store the new max and the sequence number of this bitmap\n\t * and a pointer to the bitmap itself.\n\t */\n\ttss->io_bitmap.prev_max = iobm->max;\n\ttss->io_bitmap.prev_sequence = iobm->sequence;\n}\n\n/**\n * tss_update_io_bitmap - Update I/O bitmap before exiting to usermode\n */\nvoid native_tss_update_io_bitmap(void)\n{\n\tstruct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);\n\tstruct thread_struct *t = &current->thread;\n\tu16 *base = &tss->x86_tss.io_bitmap_base;\n\n\tif (!test_thread_flag(TIF_IO_BITMAP)) {\n\t\ttss_invalidate_io_bitmap(tss);\n\t\treturn;\n\t}\n\n\tif (IS_ENABLED(CONFIG_X86_IOPL_IOPERM) && t->iopl_emul == 3) {\n\t\t*base = IO_BITMAP_OFFSET_VALID_ALL;\n\t} else {\n\t\tstruct io_bitmap *iobm = t->io_bitmap;\n\n\t\t/*\n\t\t * Only copy bitmap data when the sequence number differs. The\n\t\t * update time is accounted to the incoming task.\n\t\t */\n\t\tif (tss->io_bitmap.prev_sequence != iobm->sequence)\n\t\t\ttss_copy_io_bitmap(tss, iobm);\n\n\t\t/* Enable the bitmap */\n\t\t*base = IO_BITMAP_OFFSET_VALID_MAP;\n\t}\n\n\t/*\n\t * Make sure that the TSS limit is covering the IO bitmap. It might have\n\t * been cut down by a VMEXIT to 0x67 which would cause a subsequent I/O\n\t * access from user space to trigger a #GP because tbe bitmap is outside\n\t * the TSS limit.\n\t */\n\trefresh_tss_limit();\n}\n#else /* CONFIG_X86_IOPL_IOPERM */\nstatic inline void switch_to_bitmap(unsigned long tifp) { }\n#endif\n\n#ifdef CONFIG_SMP\n\nstruct ssb_state {\n\tstruct ssb_state\t*shared_state;\n\traw_spinlock_t\t\tlock;\n\tunsigned int\t\tdisable_state;\n\tunsigned long\t\tlocal_state;\n};\n\n#define LSTATE_SSB\t0\n\nstatic DEFINE_PER_CPU(struct ssb_state, ssb_state);\n\nvoid speculative_store_bypass_ht_init(void)\n{\n\tstruct ssb_state *st = this_cpu_ptr(&ssb_state);\n\tunsigned int this_cpu = smp_processor_id();\n\tunsigned int cpu;\n\n\tst->local_state = 0;\n\n\t/*\n\t * Shared state setup happens once on the first bringup\n\t * of the CPU. It's not destroyed on CPU hotunplug.\n\t */\n\tif (st->shared_state)\n\t\treturn;\n\n\traw_spin_lock_init(&st->lock);\n\n\t/*\n\t * Go over HT siblings and check whether one of them has set up the\n\t * shared state pointer already.\n\t */\n\tfor_each_cpu(cpu, topology_sibling_cpumask(this_cpu)) {\n\t\tif (cpu == this_cpu)\n\t\t\tcontinue;\n\n\t\tif (!per_cpu(ssb_state, cpu).shared_state)\n\t\t\tcontinue;\n\n\t\t/* Link it to the state of the sibling: */\n\t\tst->shared_state = per_cpu(ssb_state, cpu).shared_state;\n\t\treturn;\n\t}\n\n\t/*\n\t * First HT sibling to come up on the core.  Link shared state of\n\t * the first HT sibling to itself. The siblings on the same core\n\t * which come up later will see the shared state pointer and link\n\t * themself to the state of this CPU.\n\t */\n\tst->shared_state = st;\n}\n\n/*\n * Logic is: First HT sibling enables SSBD for both siblings in the core\n * and last sibling to disable it, disables it for the whole core. This how\n * MSR_SPEC_CTRL works in \"hardware\":\n *\n *  CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL\n */\nstatic __always_inline void amd_set_core_ssb_state(unsigned long tifn)\n{\n\tstruct ssb_state *st = this_cpu_ptr(&ssb_state);\n\tu64 msr = x86_amd_ls_cfg_base;\n\n\tif (!static_cpu_has(X86_FEATURE_ZEN)) {\n\t\tmsr |= ssbd_tif_to_amd_ls_cfg(tifn);\n\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\treturn;\n\t}\n\n\tif (tifn & _TIF_SSBD) {\n\t\t/*\n\t\t * Since this can race with prctl(), block reentry on the\n\t\t * same CPU.\n\t\t */\n\t\tif (__test_and_set_bit(LSTATE_SSB, &st->local_state))\n\t\t\treturn;\n\n\t\tmsr |= x86_amd_ls_cfg_ssbd_mask;\n\n\t\traw_spin_lock(&st->shared_state->lock);\n\t\t/* First sibling enables SSBD: */\n\t\tif (!st->shared_state->disable_state)\n\t\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\tst->shared_state->disable_state++;\n\t\traw_spin_unlock(&st->shared_state->lock);\n\t} else {\n\t\tif (!__test_and_clear_bit(LSTATE_SSB, &st->local_state))\n\t\t\treturn;\n\n\t\traw_spin_lock(&st->shared_state->lock);\n\t\tst->shared_state->disable_state--;\n\t\tif (!st->shared_state->disable_state)\n\t\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\traw_spin_unlock(&st->shared_state->lock);\n\t}\n}\n#else\nstatic __always_inline void amd_set_core_ssb_state(unsigned long tifn)\n{\n\tu64 msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);\n\n\twrmsrl(MSR_AMD64_LS_CFG, msr);\n}\n#endif\n\nstatic __always_inline void amd_set_ssb_virt_state(unsigned long tifn)\n{\n\t/*\n\t * SSBD has the same definition in SPEC_CTRL and VIRT_SPEC_CTRL,\n\t * so ssbd_tif_to_spec_ctrl() just works.\n\t */\n\twrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));\n}\n\n/*\n * Update the MSRs managing speculation control, during context switch.\n *\n * tifp: Previous task's thread flags\n * tifn: Next task's thread flags\n */\nstatic __always_inline void __speculation_ctrl_update(unsigned long tifp,\n\t\t\t\t\t\t      unsigned long tifn)\n{\n\tunsigned long tif_diff = tifp ^ tifn;\n\tu64 msr = x86_spec_ctrl_base;\n\tbool updmsr = false;\n\n\tlockdep_assert_irqs_disabled();\n\n\t/* Handle change of TIF_SSBD depending on the mitigation method. */\n\tif (static_cpu_has(X86_FEATURE_VIRT_SSBD)) {\n\t\tif (tif_diff & _TIF_SSBD)\n\t\t\tamd_set_ssb_virt_state(tifn);\n\t} else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {\n\t\tif (tif_diff & _TIF_SSBD)\n\t\t\tamd_set_core_ssb_state(tifn);\n\t} else if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||\n\t\t   static_cpu_has(X86_FEATURE_AMD_SSBD)) {\n\t\tupdmsr |= !!(tif_diff & _TIF_SSBD);\n\t\tmsr |= ssbd_tif_to_spec_ctrl(tifn);\n\t}\n\n\t/* Only evaluate TIF_SPEC_IB if conditional STIBP is enabled. */\n\tif (IS_ENABLED(CONFIG_SMP) &&\n\t    static_branch_unlikely(&switch_to_cond_stibp)) {\n\t\tupdmsr |= !!(tif_diff & _TIF_SPEC_IB);\n\t\tmsr |= stibp_tif_to_spec_ctrl(tifn);\n\t}\n\n\tif (updmsr)\n\t\twrmsrl(MSR_IA32_SPEC_CTRL, msr);\n}\n\nstatic unsigned long speculation_ctrl_update_tif(struct task_struct *tsk)\n{\n\tif (test_and_clear_tsk_thread_flag(tsk, TIF_SPEC_FORCE_UPDATE)) {\n\t\tif (task_spec_ssb_disable(tsk))\n\t\t\tset_tsk_thread_flag(tsk, TIF_SSBD);\n\t\telse\n\t\t\tclear_tsk_thread_flag(tsk, TIF_SSBD);\n\n\t\tif (task_spec_ib_disable(tsk))\n\t\t\tset_tsk_thread_flag(tsk, TIF_SPEC_IB);\n\t\telse\n\t\t\tclear_tsk_thread_flag(tsk, TIF_SPEC_IB);\n\t}\n\t/* Return the updated threadinfo flags*/\n\treturn task_thread_info(tsk)->flags;\n}\n\nvoid speculation_ctrl_update(unsigned long tif)\n{\n\tunsigned long flags;\n\n\t/* Forced update. Make sure all relevant TIF flags are different */\n\tlocal_irq_save(flags);\n\t__speculation_ctrl_update(~tif, tif);\n\tlocal_irq_restore(flags);\n}\n\n/* Called from seccomp/prctl update */\nvoid speculation_ctrl_update_current(void)\n{\n\tpreempt_disable();\n\tspeculation_ctrl_update(speculation_ctrl_update_tif(current));\n\tpreempt_enable();\n}\n\nstatic inline void cr4_toggle_bits_irqsoff(unsigned long mask)\n{\n\tunsigned long newval, cr4 = this_cpu_read(cpu_tlbstate.cr4);\n\n\tnewval = cr4 ^ mask;\n\tif (newval != cr4) {\n\t\tthis_cpu_write(cpu_tlbstate.cr4, newval);\n\t\t__write_cr4(newval);\n\t}\n}\n\nvoid __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)\n{\n\tunsigned long tifp, tifn;\n\n\ttifn = READ_ONCE(task_thread_info(next_p)->flags);\n\ttifp = READ_ONCE(task_thread_info(prev_p)->flags);\n\n\tswitch_to_bitmap(tifp);\n\n\tpropagate_user_return_notify(prev_p, next_p);\n\n\tif ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&\n\t    arch_has_block_step()) {\n\t\tunsigned long debugctl, msk;\n\n\t\trdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\t\tdebugctl &= ~DEBUGCTLMSR_BTF;\n\t\tmsk = tifn & _TIF_BLOCKSTEP;\n\t\tdebugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;\n\t\twrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\t}\n\n\tif ((tifp ^ tifn) & _TIF_NOTSC)\n\t\tcr4_toggle_bits_irqsoff(X86_CR4_TSD);\n\n\tif ((tifp ^ tifn) & _TIF_NOCPUID)\n\t\tset_cpuid_faulting(!!(tifn & _TIF_NOCPUID));\n\n\tif (likely(!((tifp | tifn) & _TIF_SPEC_FORCE_UPDATE))) {\n\t\t__speculation_ctrl_update(tifp, tifn);\n\t} else {\n\t\tspeculation_ctrl_update_tif(prev_p);\n\t\ttifn = speculation_ctrl_update_tif(next_p);\n\n\t\t/* Enforce MSR update to ensure consistent state */\n\t\t__speculation_ctrl_update(~tifn, tifn);\n\t}\n\n\tif ((tifp ^ tifn) & _TIF_SLD)\n\t\tswitch_to_sld(tifn);\n}\n\n/*\n * Idle related variables and functions\n */\nunsigned long boot_option_idle_override = IDLE_NO_OVERRIDE;\nEXPORT_SYMBOL(boot_option_idle_override);\n\nstatic void (*x86_idle)(void);\n\n#ifndef CONFIG_SMP\nstatic inline void play_dead(void)\n{\n\tBUG();\n}\n#endif\n\nvoid arch_cpu_idle_enter(void)\n{\n\ttsc_verify_tsc_adjust(false);\n\tlocal_touch_nmi();\n}\n\nvoid arch_cpu_idle_dead(void)\n{\n\tplay_dead();\n}\n\n/*\n * Called from the generic idle code.\n */\nvoid arch_cpu_idle(void)\n{\n\tx86_idle();\n}\n\n/*\n * We use this if we don't have any better idle routine..\n */\nvoid __cpuidle default_idle(void)\n{\n\ttrace_cpu_idle_rcuidle(1, smp_processor_id());\n\tsafe_halt();\n\ttrace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());\n}\n#if defined(CONFIG_APM_MODULE) || defined(CONFIG_HALTPOLL_CPUIDLE_MODULE)\nEXPORT_SYMBOL(default_idle);\n#endif\n\n#ifdef CONFIG_XEN\nbool xen_set_default_idle(void)\n{\n\tbool ret = !!x86_idle;\n\n\tx86_idle = default_idle;\n\n\treturn ret;\n}\n#endif\n\nvoid stop_this_cpu(void *dummy)\n{\n\tlocal_irq_disable();\n\t/*\n\t * Remove this CPU:\n\t */\n\tset_cpu_online(smp_processor_id(), false);\n\tdisable_local_APIC();\n\tmcheck_cpu_clear(this_cpu_ptr(&cpu_info));\n\n\t/*\n\t * Use wbinvd on processors that support SME. This provides support\n\t * for performing a successful kexec when going from SME inactive\n\t * to SME active (or vice-versa). The cache must be cleared so that\n\t * if there are entries with the same physical address, both with and\n\t * without the encryption bit, they don't race each other when flushed\n\t * and potentially end up with the wrong entry being committed to\n\t * memory.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_SME))\n\t\tnative_wbinvd();\n\tfor (;;) {\n\t\t/*\n\t\t * Use native_halt() so that memory contents don't change\n\t\t * (stack usage and variables) after possibly issuing the\n\t\t * native_wbinvd() above.\n\t\t */\n\t\tnative_halt();\n\t}\n}\n\n/*\n * AMD Erratum 400 aware idle routine. We handle it the same way as C3 power\n * states (local apic timer and TSC stop).\n */\nstatic void amd_e400_idle(void)\n{\n\t/*\n\t * We cannot use static_cpu_has_bug() here because X86_BUG_AMD_APIC_C1E\n\t * gets set after static_cpu_has() places have been converted via\n\t * alternatives.\n\t */\n\tif (!boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {\n\t\tdefault_idle();\n\t\treturn;\n\t}\n\n\ttick_broadcast_enter();\n\n\tdefault_idle();\n\n\t/*\n\t * The switch back from broadcast mode needs to be called with\n\t * interrupts disabled.\n\t */\n\tlocal_irq_disable();\n\ttick_broadcast_exit();\n\tlocal_irq_enable();\n}\n\n/*\n * Intel Core2 and older machines prefer MWAIT over HALT for C1.\n * We can't rely on cpuidle installing MWAIT, because it will not load\n * on systems that support only C1 -- so the boot default must be MWAIT.\n *\n * Some AMD machines are the opposite, they depend on using HALT.\n *\n * So for default C1, which is used during boot until cpuidle loads,\n * use MWAIT-C1 on Intel HW that has it, else use HALT.\n */\nstatic int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)\n{\n\tif (c->x86_vendor != X86_VENDOR_INTEL)\n\t\treturn 0;\n\n\tif (!cpu_has(c, X86_FEATURE_MWAIT) || boot_cpu_has_bug(X86_BUG_MONITOR))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * MONITOR/MWAIT with no hints, used for default C1 state. This invokes MWAIT\n * with interrupts enabled and no flags, which is backwards compatible with the\n * original MWAIT implementation.\n */\nstatic __cpuidle void mwait_idle(void)\n{\n\tif (!current_set_polling_and_test()) {\n\t\ttrace_cpu_idle_rcuidle(1, smp_processor_id());\n\t\tif (this_cpu_has(X86_BUG_CLFLUSH_MONITOR)) {\n\t\t\tmb(); /* quirk */\n\t\t\tclflush((void *)&current_thread_info()->flags);\n\t\t\tmb(); /* quirk */\n\t\t}\n\n\t\t__monitor((void *)&current_thread_info()->flags, 0, 0);\n\t\tif (!need_resched())\n\t\t\t__sti_mwait(0, 0);\n\t\telse\n\t\t\tlocal_irq_enable();\n\t\ttrace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());\n\t} else {\n\t\tlocal_irq_enable();\n\t}\n\t__current_clr_polling();\n}\n\nvoid select_idle_routine(const struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_SMP\n\tif (boot_option_idle_override == IDLE_POLL && smp_num_siblings > 1)\n\t\tpr_warn_once(\"WARNING: polling idle and HT enabled, performance may degrade\\n\");\n#endif\n\tif (x86_idle || boot_option_idle_override == IDLE_POLL)\n\t\treturn;\n\n\tif (boot_cpu_has_bug(X86_BUG_AMD_E400)) {\n\t\tpr_info(\"using AMD E400 aware idle routine\\n\");\n\t\tx86_idle = amd_e400_idle;\n\t} else if (prefer_mwait_c1_over_halt(c)) {\n\t\tpr_info(\"using mwait in idle threads\\n\");\n\t\tx86_idle = mwait_idle;\n\t} else\n\t\tx86_idle = default_idle;\n}\n\nvoid amd_e400_c1e_apic_setup(void)\n{\n\tif (boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {\n\t\tpr_info(\"Switch to broadcast mode on CPU%d\\n\", smp_processor_id());\n\t\tlocal_irq_disable();\n\t\ttick_broadcast_force();\n\t\tlocal_irq_enable();\n\t}\n}\n\nvoid __init arch_post_acpi_subsys_init(void)\n{\n\tu32 lo, hi;\n\n\tif (!boot_cpu_has_bug(X86_BUG_AMD_E400))\n\t\treturn;\n\n\t/*\n\t * AMD E400 detection needs to happen after ACPI has been enabled. If\n\t * the machine is affected K8_INTP_C1E_ACTIVE_MASK bits are set in\n\t * MSR_K8_INT_PENDING_MSG.\n\t */\n\trdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);\n\tif (!(lo & K8_INTP_C1E_ACTIVE_MASK))\n\t\treturn;\n\n\tboot_cpu_set_bug(X86_BUG_AMD_APIC_C1E);\n\n\tif (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))\n\t\tmark_tsc_unstable(\"TSC halt in AMD C1E\");\n\tpr_info(\"System has AMD C1E enabled\\n\");\n}\n\nstatic int __init idle_setup(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"poll\")) {\n\t\tpr_info(\"using polling idle threads\\n\");\n\t\tboot_option_idle_override = IDLE_POLL;\n\t\tcpu_idle_poll_ctrl(true);\n\t} else if (!strcmp(str, \"halt\")) {\n\t\t/*\n\t\t * When the boot option of idle=halt is added, halt is\n\t\t * forced to be used for CPU idle. In such case CPU C2/C3\n\t\t * won't be used again.\n\t\t * To continue to load the CPU idle driver, don't touch\n\t\t * the boot_option_idle_override.\n\t\t */\n\t\tx86_idle = default_idle;\n\t\tboot_option_idle_override = IDLE_HALT;\n\t} else if (!strcmp(str, \"nomwait\")) {\n\t\t/*\n\t\t * If the boot option of \"idle=nomwait\" is added,\n\t\t * it means that mwait will be disabled for CPU C2/C3\n\t\t * states. In such case it won't touch the variable\n\t\t * of boot_option_idle_override.\n\t\t */\n\t\tboot_option_idle_override = IDLE_NOMWAIT;\n\t} else\n\t\treturn -1;\n\n\treturn 0;\n}\nearly_param(\"idle\", idle_setup);\n\nunsigned long arch_align_stack(unsigned long sp)\n{\n\tif (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)\n\t\tsp -= get_random_int() % 8192;\n\treturn sp & ~0xf;\n}\n\nunsigned long arch_randomize_brk(struct mm_struct *mm)\n{\n\treturn randomize_page(mm->brk, 0x02000000);\n}\n\n/*\n * Called from fs/proc with a reference on @p to find the function\n * which called into schedule(). This needs to be done carefully\n * because the task might wake up and we might look at a stack\n * changing under us.\n */\nunsigned long get_wchan(struct task_struct *p)\n{\n\tunsigned long start, bottom, top, sp, fp, ip, ret = 0;\n\tint count = 0;\n\n\tif (p == current || p->state == TASK_RUNNING)\n\t\treturn 0;\n\n\tif (!try_get_task_stack(p))\n\t\treturn 0;\n\n\tstart = (unsigned long)task_stack_page(p);\n\tif (!start)\n\t\tgoto out;\n\n\t/*\n\t * Layout of the stack page:\n\t *\n\t * ----------- topmax = start + THREAD_SIZE - sizeof(unsigned long)\n\t * PADDING\n\t * ----------- top = topmax - TOP_OF_KERNEL_STACK_PADDING\n\t * stack\n\t * ----------- bottom = start\n\t *\n\t * The tasks stack pointer points at the location where the\n\t * framepointer is stored. The data on the stack is:\n\t * ... IP FP ... IP FP\n\t *\n\t * We need to read FP and IP, so we need to adjust the upper\n\t * bound by another unsigned long.\n\t */\n\ttop = start + THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;\n\ttop -= 2 * sizeof(unsigned long);\n\tbottom = start;\n\n\tsp = READ_ONCE(p->thread.sp);\n\tif (sp < bottom || sp > top)\n\t\tgoto out;\n\n\tfp = READ_ONCE_NOCHECK(((struct inactive_task_frame *)sp)->bp);\n\tdo {\n\t\tif (fp < bottom || fp > top)\n\t\t\tgoto out;\n\t\tip = READ_ONCE_NOCHECK(*(unsigned long *)(fp + sizeof(unsigned long)));\n\t\tif (!in_sched_functions(ip)) {\n\t\t\tret = ip;\n\t\t\tgoto out;\n\t\t}\n\t\tfp = READ_ONCE_NOCHECK(*(unsigned long *)fp);\n\t} while (count++ < 16 && p->state != TASK_RUNNING);\n\nout:\n\tput_task_stack(p);\n\treturn ret;\n}\n\nlong do_arch_prctl_common(struct task_struct *task, int option,\n\t\t\t  unsigned long cpuid_enabled)\n{\n\tswitch (option) {\n\tcase ARCH_GET_CPUID:\n\t\treturn get_cpuid_mode();\n\tcase ARCH_SET_CPUID:\n\t\treturn set_cpuid_mode(task, cpuid_enabled);\n\t}\n\n\treturn -EINVAL;\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Core of Xen paravirt_ops implementation.\n *\n * This file contains the xen_paravirt_ops structure itself, and the\n * implementations for:\n * - privileged instructions\n * - interrupt flags\n * - segment operations\n * - booting and setup\n *\n * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007\n */\n\n#include <linux/cpu.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/preempt.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/delay.h>\n#include <linux/start_kernel.h>\n#include <linux/sched.h>\n#include <linux/kprobes.h>\n#include <linux/memblock.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/page-flags.h>\n#include <linux/highmem.h>\n#include <linux/console.h>\n#include <linux/pci.h>\n#include <linux/gfp.h>\n#include <linux/edd.h>\n#include <linux/frame.h>\n\n#include <xen/xen.h>\n#include <xen/events.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/version.h>\n#include <xen/interface/physdev.h>\n#include <xen/interface/vcpu.h>\n#include <xen/interface/memory.h>\n#include <xen/interface/nmi.h>\n#include <xen/interface/xen-mca.h>\n#include <xen/features.h>\n#include <xen/page.h>\n#include <xen/hvc-console.h>\n#include <xen/acpi.h>\n\n#include <asm/paravirt.h>\n#include <asm/apic.h>\n#include <asm/page.h>\n#include <asm/xen/pci.h>\n#include <asm/xen/hypercall.h>\n#include <asm/xen/hypervisor.h>\n#include <asm/xen/cpuid.h>\n#include <asm/fixmap.h>\n#include <asm/processor.h>\n#include <asm/proto.h>\n#include <asm/msr-index.h>\n#include <asm/traps.h>\n#include <asm/setup.h>\n#include <asm/desc.h>\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include <asm/reboot.h>\n#include <asm/stackprotector.h>\n#include <asm/hypervisor.h>\n#include <asm/mach_traps.h>\n#include <asm/mwait.h>\n#include <asm/pci_x86.h>\n#include <asm/cpu.h>\n#ifdef CONFIG_X86_IOPL_IOPERM\n#include <asm/io_bitmap.h>\n#endif\n\n#ifdef CONFIG_ACPI\n#include <linux/acpi.h>\n#include <asm/acpi.h>\n#include <acpi/pdc_intel.h>\n#include <acpi/processor.h>\n#include <xen/interface/platform.h>\n#endif\n\n#include \"xen-ops.h\"\n#include \"mmu.h\"\n#include \"smp.h\"\n#include \"multicalls.h\"\n#include \"pmu.h\"\n\n#include \"../kernel/cpu/cpu.h\" /* get_cpu_cap() */\n\nvoid *xen_initial_gdt;\n\nstatic int xen_cpu_up_prepare_pv(unsigned int cpu);\nstatic int xen_cpu_dead_pv(unsigned int cpu);\n\nstruct tls_descs {\n\tstruct desc_struct desc[3];\n};\n\n/*\n * Updating the 3 TLS descriptors in the GDT on every task switch is\n * surprisingly expensive so we avoid updating them if they haven't\n * changed.  Since Xen writes different descriptors than the one\n * passed in the update_descriptor hypercall we keep shadow copies to\n * compare against.\n */\nstatic DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);\n\nstatic void __init xen_banner(void)\n{\n\tunsigned version = HYPERVISOR_xen_version(XENVER_version, NULL);\n\tstruct xen_extraversion extra;\n\tHYPERVISOR_xen_version(XENVER_extraversion, &extra);\n\n\tpr_info(\"Booting paravirtualized kernel on %s\\n\", pv_info.name);\n\tprintk(KERN_INFO \"Xen version: %d.%d%s%s\\n\",\n\t       version >> 16, version & 0xffff, extra.extraversion,\n\t       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? \" (preserve-AD)\" : \"\");\n\n#ifdef CONFIG_X86_32\n\tpr_warn(\"WARNING! WARNING! WARNING! WARNING! WARNING! WARNING! WARNING!\\n\"\n\t\t\"Support for running as 32-bit PV-guest under Xen will soon be removed\\n\"\n\t\t\"from the Linux kernel!\\n\"\n\t\t\"Please use either a 64-bit kernel or switch to HVM or PVH mode!\\n\"\n\t\t\"WARNING! WARNING! WARNING! WARNING! WARNING! WARNING! WARNING!\\n\");\n#endif\n}\n\nstatic void __init xen_pv_init_platform(void)\n{\n\tpopulate_extra_pte(fix_to_virt(FIX_PARAVIRT_BOOTMAP));\n\n\tset_fixmap(FIX_PARAVIRT_BOOTMAP, xen_start_info->shared_info);\n\tHYPERVISOR_shared_info = (void *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);\n\n\t/* xen clock uses per-cpu vcpu_info, need to init it for boot cpu */\n\txen_vcpu_info_reset(0);\n\n\t/* pvclock is in shared info area */\n\txen_init_time_ops();\n}\n\nstatic void __init xen_pv_guest_late_init(void)\n{\n#ifndef CONFIG_SMP\n\t/* Setup shared vcpu info for non-smp configurations */\n\txen_setup_vcpu_info_placement();\n#endif\n}\n\n/* Check if running on Xen version (major, minor) or later */\nbool\nxen_running_on_version_or_later(unsigned int major, unsigned int minor)\n{\n\tunsigned int version;\n\n\tif (!xen_domain())\n\t\treturn false;\n\n\tversion = HYPERVISOR_xen_version(XENVER_version, NULL);\n\tif ((((version >> 16) == major) && ((version & 0xffff) >= minor)) ||\n\t\t((version >> 16) > major))\n\t\treturn true;\n\treturn false;\n}\n\nstatic __read_mostly unsigned int cpuid_leaf5_ecx_val;\nstatic __read_mostly unsigned int cpuid_leaf5_edx_val;\n\nstatic void xen_cpuid(unsigned int *ax, unsigned int *bx,\n\t\t      unsigned int *cx, unsigned int *dx)\n{\n\tunsigned maskebx = ~0;\n\n\t/*\n\t * Mask out inconvenient features, to try and disable as many\n\t * unsupported kernel subsystems as possible.\n\t */\n\tswitch (*ax) {\n\tcase CPUID_MWAIT_LEAF:\n\t\t/* Synthesize the values.. */\n\t\t*ax = 0;\n\t\t*bx = 0;\n\t\t*cx = cpuid_leaf5_ecx_val;\n\t\t*dx = cpuid_leaf5_edx_val;\n\t\treturn;\n\n\tcase 0xb:\n\t\t/* Suppress extended topology stuff */\n\t\tmaskebx = 0;\n\t\tbreak;\n\t}\n\n\tasm(XEN_EMULATE_PREFIX \"cpuid\"\n\t\t: \"=a\" (*ax),\n\t\t  \"=b\" (*bx),\n\t\t  \"=c\" (*cx),\n\t\t  \"=d\" (*dx)\n\t\t: \"0\" (*ax), \"2\" (*cx));\n\n\t*bx &= maskebx;\n}\nSTACK_FRAME_NON_STANDARD(xen_cpuid); /* XEN_EMULATE_PREFIX */\n\nstatic bool __init xen_check_mwait(void)\n{\n#ifdef CONFIG_ACPI\n\tstruct xen_platform_op op = {\n\t\t.cmd\t\t\t= XENPF_set_processor_pminfo,\n\t\t.u.set_pminfo.id\t= -1,\n\t\t.u.set_pminfo.type\t= XEN_PM_PDC,\n\t};\n\tuint32_t buf[3];\n\tunsigned int ax, bx, cx, dx;\n\tunsigned int mwait_mask;\n\n\t/* We need to determine whether it is OK to expose the MWAIT\n\t * capability to the kernel to harvest deeper than C3 states from ACPI\n\t * _CST using the processor_harvest_xen.c module. For this to work, we\n\t * need to gather the MWAIT_LEAF values (which the cstate.c code\n\t * checks against). The hypervisor won't expose the MWAIT flag because\n\t * it would break backwards compatibility; so we will find out directly\n\t * from the hardware and hypercall.\n\t */\n\tif (!xen_initial_domain())\n\t\treturn false;\n\n\t/*\n\t * When running under platform earlier than Xen4.2, do not expose\n\t * mwait, to avoid the risk of loading native acpi pad driver\n\t */\n\tif (!xen_running_on_version_or_later(4, 2))\n\t\treturn false;\n\n\tax = 1;\n\tcx = 0;\n\n\tnative_cpuid(&ax, &bx, &cx, &dx);\n\n\tmwait_mask = (1 << (X86_FEATURE_EST % 32)) |\n\t\t     (1 << (X86_FEATURE_MWAIT % 32));\n\n\tif ((cx & mwait_mask) != mwait_mask)\n\t\treturn false;\n\n\t/* We need to emulate the MWAIT_LEAF and for that we need both\n\t * ecx and edx. The hypercall provides only partial information.\n\t */\n\n\tax = CPUID_MWAIT_LEAF;\n\tbx = 0;\n\tcx = 0;\n\tdx = 0;\n\n\tnative_cpuid(&ax, &bx, &cx, &dx);\n\n\t/* Ask the Hypervisor whether to clear ACPI_PDC_C_C2C3_FFH. If so,\n\t * don't expose MWAIT_LEAF and let ACPI pick the IOPORT version of C3.\n\t */\n\tbuf[0] = ACPI_PDC_REVISION_ID;\n\tbuf[1] = 1;\n\tbuf[2] = (ACPI_PDC_C_CAPABILITY_SMP | ACPI_PDC_EST_CAPABILITY_SWSMP);\n\n\tset_xen_guest_handle(op.u.set_pminfo.pdc, buf);\n\n\tif ((HYPERVISOR_platform_op(&op) == 0) &&\n\t    (buf[2] & (ACPI_PDC_C_C1_FFH | ACPI_PDC_C_C2C3_FFH))) {\n\t\tcpuid_leaf5_ecx_val = cx;\n\t\tcpuid_leaf5_edx_val = dx;\n\t}\n\treturn true;\n#else\n\treturn false;\n#endif\n}\n\nstatic bool __init xen_check_xsave(void)\n{\n\tunsigned int cx, xsave_mask;\n\n\tcx = cpuid_ecx(1);\n\n\txsave_mask = (1 << (X86_FEATURE_XSAVE % 32)) |\n\t\t     (1 << (X86_FEATURE_OSXSAVE % 32));\n\n\t/* Xen will set CR4.OSXSAVE if supported and not disabled by force */\n\treturn (cx & xsave_mask) == xsave_mask;\n}\n\nstatic void __init xen_init_capabilities(void)\n{\n\tsetup_force_cpu_cap(X86_FEATURE_XENPV);\n\tsetup_clear_cpu_cap(X86_FEATURE_DCA);\n\tsetup_clear_cpu_cap(X86_FEATURE_APERFMPERF);\n\tsetup_clear_cpu_cap(X86_FEATURE_MTRR);\n\tsetup_clear_cpu_cap(X86_FEATURE_ACC);\n\tsetup_clear_cpu_cap(X86_FEATURE_X2APIC);\n\tsetup_clear_cpu_cap(X86_FEATURE_SME);\n\n\t/*\n\t * Xen PV would need some work to support PCID: CR3 handling as well\n\t * as xen_flush_tlb_others() would need updating.\n\t */\n\tsetup_clear_cpu_cap(X86_FEATURE_PCID);\n\n\tif (!xen_initial_domain())\n\t\tsetup_clear_cpu_cap(X86_FEATURE_ACPI);\n\n\tif (xen_check_mwait())\n\t\tsetup_force_cpu_cap(X86_FEATURE_MWAIT);\n\telse\n\t\tsetup_clear_cpu_cap(X86_FEATURE_MWAIT);\n\n\tif (!xen_check_xsave()) {\n\t\tsetup_clear_cpu_cap(X86_FEATURE_XSAVE);\n\t\tsetup_clear_cpu_cap(X86_FEATURE_OSXSAVE);\n\t}\n}\n\nstatic void xen_set_debugreg(int reg, unsigned long val)\n{\n\tHYPERVISOR_set_debugreg(reg, val);\n}\n\nstatic unsigned long xen_get_debugreg(int reg)\n{\n\treturn HYPERVISOR_get_debugreg(reg);\n}\n\nstatic void xen_end_context_switch(struct task_struct *next)\n{\n\txen_mc_flush();\n\tparavirt_end_context_switch(next);\n}\n\nstatic unsigned long xen_store_tr(void)\n{\n\treturn 0;\n}\n\n/*\n * Set the page permissions for a particular virtual address.  If the\n * address is a vmalloc mapping (or other non-linear mapping), then\n * find the linear mapping of the page and also set its protections to\n * match.\n */\nstatic void set_aliased_prot(void *v, pgprot_t prot)\n{\n\tint level;\n\tpte_t *ptep;\n\tpte_t pte;\n\tunsigned long pfn;\n\tstruct page *page;\n\tunsigned char dummy;\n\n\tptep = lookup_address((unsigned long)v, &level);\n\tBUG_ON(ptep == NULL);\n\n\tpfn = pte_pfn(*ptep);\n\tpage = pfn_to_page(pfn);\n\n\tpte = pfn_pte(pfn, prot);\n\n\t/*\n\t * Careful: update_va_mapping() will fail if the virtual address\n\t * we're poking isn't populated in the page tables.  We don't\n\t * need to worry about the direct map (that's always in the page\n\t * tables), but we need to be careful about vmap space.  In\n\t * particular, the top level page table can lazily propagate\n\t * entries between processes, so if we've switched mms since we\n\t * vmapped the target in the first place, we might not have the\n\t * top-level page table entry populated.\n\t *\n\t * We disable preemption because we want the same mm active when\n\t * we probe the target and when we issue the hypercall.  We'll\n\t * have the same nominal mm, but if we're a kernel thread, lazy\n\t * mm dropping could change our pgd.\n\t *\n\t * Out of an abundance of caution, this uses __get_user() to fault\n\t * in the target address just in case there's some obscure case\n\t * in which the target address isn't readable.\n\t */\n\n\tpreempt_disable();\n\n\tcopy_from_kernel_nofault(&dummy, v, 1);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))\n\t\tBUG();\n\n\tif (!PageHighMem(page)) {\n\t\tvoid *av = __va(PFN_PHYS(pfn));\n\n\t\tif (av != v)\n\t\t\tif (HYPERVISOR_update_va_mapping((unsigned long)av, pte, 0))\n\t\t\t\tBUG();\n\t} else\n\t\tkmap_flush_unused();\n\n\tpreempt_enable();\n}\n\nstatic void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tconst unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;\n\tint i;\n\n\t/*\n\t * We need to mark the all aliases of the LDT pages RO.  We\n\t * don't need to call vm_flush_aliases(), though, since that's\n\t * only responsible for flushing aliases out the TLBs, not the\n\t * page tables, and Xen will flush the TLB for us if needed.\n\t *\n\t * To avoid confusing future readers: none of this is necessary\n\t * to load the LDT.  The hypervisor only checks this when the\n\t * LDT is faulted in due to subsequent descriptor access.\n\t */\n\n\tfor (i = 0; i < entries; i += entries_per_page)\n\t\tset_aliased_prot(ldt + i, PAGE_KERNEL_RO);\n}\n\nstatic void xen_free_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tconst unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;\n\tint i;\n\n\tfor (i = 0; i < entries; i += entries_per_page)\n\t\tset_aliased_prot(ldt + i, PAGE_KERNEL);\n}\n\nstatic void xen_set_ldt(const void *addr, unsigned entries)\n{\n\tstruct mmuext_op *op;\n\tstruct multicall_space mcs = xen_mc_entry(sizeof(*op));\n\n\ttrace_xen_cpu_set_ldt(addr, entries);\n\n\top = mcs.args;\n\top->cmd = MMUEXT_SET_LDT;\n\top->arg1.linear_addr = (unsigned long)addr;\n\top->arg2.nr_ents = entries;\n\n\tMULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_load_gdt(const struct desc_ptr *dtr)\n{\n\tunsigned long va = dtr->address;\n\tunsigned int size = dtr->size + 1;\n\tunsigned long pfn, mfn;\n\tint level;\n\tpte_t *ptep;\n\tvoid *virt;\n\n\t/* @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */\n\tBUG_ON(size > PAGE_SIZE);\n\tBUG_ON(va & ~PAGE_MASK);\n\n\t/*\n\t * The GDT is per-cpu and is in the percpu data area.\n\t * That can be virtually mapped, so we need to do a\n\t * page-walk to get the underlying MFN for the\n\t * hypercall.  The page can also be in the kernel's\n\t * linear range, so we need to RO that mapping too.\n\t */\n\tptep = lookup_address(va, &level);\n\tBUG_ON(ptep == NULL);\n\n\tpfn = pte_pfn(*ptep);\n\tmfn = pfn_to_mfn(pfn);\n\tvirt = __va(PFN_PHYS(pfn));\n\n\tmake_lowmem_page_readonly((void *)va);\n\tmake_lowmem_page_readonly(virt);\n\n\tif (HYPERVISOR_set_gdt(&mfn, size / sizeof(struct desc_struct)))\n\t\tBUG();\n}\n\n/*\n * load_gdt for early boot, when the gdt is only mapped once\n */\nstatic void __init xen_load_gdt_boot(const struct desc_ptr *dtr)\n{\n\tunsigned long va = dtr->address;\n\tunsigned int size = dtr->size + 1;\n\tunsigned long pfn, mfn;\n\tpte_t pte;\n\n\t/* @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */\n\tBUG_ON(size > PAGE_SIZE);\n\tBUG_ON(va & ~PAGE_MASK);\n\n\tpfn = virt_to_pfn(va);\n\tmfn = pfn_to_mfn(pfn);\n\n\tpte = pfn_pte(pfn, PAGE_KERNEL_RO);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)va, pte, 0))\n\t\tBUG();\n\n\tif (HYPERVISOR_set_gdt(&mfn, size / sizeof(struct desc_struct)))\n\t\tBUG();\n}\n\nstatic inline bool desc_equal(const struct desc_struct *d1,\n\t\t\t      const struct desc_struct *d2)\n{\n\treturn !memcmp(d1, d2, sizeof(*d1));\n}\n\nstatic void load_TLS_descriptor(struct thread_struct *t,\n\t\t\t\tunsigned int cpu, unsigned int i)\n{\n\tstruct desc_struct *shadow = &per_cpu(shadow_tls_desc, cpu).desc[i];\n\tstruct desc_struct *gdt;\n\txmaddr_t maddr;\n\tstruct multicall_space mc;\n\n\tif (desc_equal(shadow, &t->tls_array[i]))\n\t\treturn;\n\n\t*shadow = t->tls_array[i];\n\n\tgdt = get_cpu_gdt_rw(cpu);\n\tmaddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);\n\tmc = __xen_mc_entry(0);\n\n\tMULTI_update_descriptor(mc.mc, maddr.maddr, t->tls_array[i]);\n}\n\nstatic void xen_load_tls(struct thread_struct *t, unsigned int cpu)\n{\n\t/*\n\t * XXX sleazy hack: If we're being called in a lazy-cpu zone\n\t * and lazy gs handling is enabled, it means we're in a\n\t * context switch, and %gs has just been saved.  This means we\n\t * can zero it out to prevent faults on exit from the\n\t * hypervisor if the next process has no %gs.  Either way, it\n\t * has been saved, and the new value will get loaded properly.\n\t * This will go away as soon as Xen has been modified to not\n\t * save/restore %gs for normal hypercalls.\n\t *\n\t * On x86_64, this hack is not used for %gs, because gs points\n\t * to KERNEL_GS_BASE (and uses it for PDA references), so we\n\t * must not zero %gs on x86_64\n\t *\n\t * For x86_64, we need to zero %fs, otherwise we may get an\n\t * exception between the new %fs descriptor being loaded and\n\t * %fs being effectively cleared at __switch_to().\n\t */\n\tif (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU) {\n#ifdef CONFIG_X86_32\n\t\tlazy_load_gs(0);\n#else\n\t\tloadsegment(fs, 0);\n#endif\n\t}\n\n\txen_mc_batch();\n\n\tload_TLS_descriptor(t, cpu, 0);\n\tload_TLS_descriptor(t, cpu, 1);\n\tload_TLS_descriptor(t, cpu, 2);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\n#ifdef CONFIG_X86_64\nstatic void xen_load_gs_index(unsigned int idx)\n{\n\tif (HYPERVISOR_set_segment_base(SEGBASE_GS_USER_SEL, idx))\n\t\tBUG();\n}\n#endif\n\nstatic void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,\n\t\t\t\tconst void *ptr)\n{\n\txmaddr_t mach_lp = arbitrary_virt_to_machine(&dt[entrynum]);\n\tu64 entry = *(u64 *)ptr;\n\n\ttrace_xen_cpu_write_ldt_entry(dt, entrynum, entry);\n\n\tpreempt_disable();\n\n\txen_mc_flush();\n\tif (HYPERVISOR_update_descriptor(mach_lp.maddr, entry))\n\t\tBUG();\n\n\tpreempt_enable();\n}\n\n#ifdef CONFIG_X86_64\nvoid noist_exc_debug(struct pt_regs *regs);\n\nDEFINE_IDTENTRY_RAW(xenpv_exc_nmi)\n{\n\t/* On Xen PV, NMI doesn't use IST.  The C part is the sane as native. */\n\texc_nmi(regs);\n}\n\nDEFINE_IDTENTRY_RAW(xenpv_exc_debug)\n{\n\t/*\n\t * There's no IST on Xen PV, but we still need to dispatch\n\t * to the correct handler.\n\t */\n\tif (user_mode(regs))\n\t\tnoist_exc_debug(regs);\n\telse\n\t\texc_debug(regs);\n}\n\nstruct trap_array_entry {\n\tvoid (*orig)(void);\n\tvoid (*xen)(void);\n\tbool ist_okay;\n};\n\n#define TRAP_ENTRY(func, ist_ok) {\t\t\t\\\n\t.orig\t\t= asm_##func,\t\t\t\\\n\t.xen\t\t= xen_asm_##func,\t\t\\\n\t.ist_okay\t= ist_ok }\n\n#define TRAP_ENTRY_REDIR(func, ist_ok) {\t\t\\\n\t.orig\t\t= asm_##func,\t\t\t\\\n\t.xen\t\t= xen_asm_xenpv_##func,\t\t\\\n\t.ist_okay\t= ist_ok }\n\nstatic struct trap_array_entry trap_array[] = {\n\tTRAP_ENTRY_REDIR(exc_debug,\t\t\ttrue  ),\n\tTRAP_ENTRY(exc_double_fault,\t\t\ttrue  ),\n#ifdef CONFIG_X86_MCE\n\tTRAP_ENTRY(exc_machine_check,\t\t\ttrue  ),\n#endif\n\tTRAP_ENTRY_REDIR(exc_nmi,\t\t\ttrue  ),\n\tTRAP_ENTRY(exc_int3,\t\t\t\tfalse ),\n\tTRAP_ENTRY(exc_overflow,\t\t\tfalse ),\n#ifdef CONFIG_IA32_EMULATION\n\t{ entry_INT80_compat,          xen_entry_INT80_compat,          false },\n#endif\n\tTRAP_ENTRY(exc_page_fault,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_divide_error,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_bounds,\t\t\t\tfalse ),\n\tTRAP_ENTRY(exc_invalid_op,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_device_not_available,\t\tfalse ),\n\tTRAP_ENTRY(exc_coproc_segment_overrun,\t\tfalse ),\n\tTRAP_ENTRY(exc_invalid_tss,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_segment_not_present,\t\tfalse ),\n\tTRAP_ENTRY(exc_stack_segment,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_general_protection,\t\tfalse ),\n\tTRAP_ENTRY(exc_spurious_interrupt_bug,\t\tfalse ),\n\tTRAP_ENTRY(exc_coprocessor_error,\t\tfalse ),\n\tTRAP_ENTRY(exc_alignment_check,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_simd_coprocessor_error,\t\tfalse ),\n};\n\nstatic bool __ref get_trap_addr(void **addr, unsigned int ist)\n{\n\tunsigned int nr;\n\tbool ist_okay = false;\n\n\t/*\n\t * Replace trap handler addresses by Xen specific ones.\n\t * Check for known traps using IST and whitelist them.\n\t * The debugger ones are the only ones we care about.\n\t * Xen will handle faults like double_fault, so we should never see\n\t * them.  Warn if there's an unexpected IST-using fault handler.\n\t */\n\tfor (nr = 0; nr < ARRAY_SIZE(trap_array); nr++) {\n\t\tstruct trap_array_entry *entry = trap_array + nr;\n\n\t\tif (*addr == entry->orig) {\n\t\t\t*addr = entry->xen;\n\t\t\tist_okay = entry->ist_okay;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (nr == ARRAY_SIZE(trap_array) &&\n\t    *addr >= (void *)early_idt_handler_array[0] &&\n\t    *addr < (void *)early_idt_handler_array[NUM_EXCEPTION_VECTORS]) {\n\t\tnr = (*addr - (void *)early_idt_handler_array[0]) /\n\t\t     EARLY_IDT_HANDLER_SIZE;\n\t\t*addr = (void *)xen_early_idt_handler_array[nr];\n\t}\n\n\tif (WARN_ON(ist != 0 && !ist_okay))\n\t\treturn false;\n\n\treturn true;\n}\n#endif\n\nstatic int cvt_gate_to_trap(int vector, const gate_desc *val,\n\t\t\t    struct trap_info *info)\n{\n\tunsigned long addr;\n\n\tif (val->bits.type != GATE_TRAP && val->bits.type != GATE_INTERRUPT)\n\t\treturn 0;\n\n\tinfo->vector = vector;\n\n\taddr = gate_offset(val);\n#ifdef CONFIG_X86_64\n\tif (!get_trap_addr((void **)&addr, val->bits.ist))\n\t\treturn 0;\n#endif\t/* CONFIG_X86_64 */\n\tinfo->address = addr;\n\n\tinfo->cs = gate_segment(val);\n\tinfo->flags = val->bits.dpl;\n\t/* interrupt gates clear IF */\n\tif (val->bits.type == GATE_INTERRUPT)\n\t\tinfo->flags |= 1 << 2;\n\n\treturn 1;\n}\n\n/* Locations of each CPU's IDT */\nstatic DEFINE_PER_CPU(struct desc_ptr, idt_desc);\n\n/* Set an IDT entry.  If the entry is part of the current IDT, then\n   also update Xen. */\nstatic void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)\n{\n\tunsigned long p = (unsigned long)&dt[entrynum];\n\tunsigned long start, end;\n\n\ttrace_xen_cpu_write_idt_entry(dt, entrynum, g);\n\n\tpreempt_disable();\n\n\tstart = __this_cpu_read(idt_desc.address);\n\tend = start + __this_cpu_read(idt_desc.size) + 1;\n\n\txen_mc_flush();\n\n\tnative_write_idt_entry(dt, entrynum, g);\n\n\tif (p >= start && (p + 8) <= end) {\n\t\tstruct trap_info info[2];\n\n\t\tinfo[1].address = 0;\n\n\t\tif (cvt_gate_to_trap(entrynum, g, &info[0]))\n\t\t\tif (HYPERVISOR_set_trap_table(info))\n\t\t\t\tBUG();\n\t}\n\n\tpreempt_enable();\n}\n\nstatic void xen_convert_trap_info(const struct desc_ptr *desc,\n\t\t\t\t  struct trap_info *traps)\n{\n\tunsigned in, out, count;\n\n\tcount = (desc->size+1) / sizeof(gate_desc);\n\tBUG_ON(count > 256);\n\n\tfor (in = out = 0; in < count; in++) {\n\t\tgate_desc *entry = (gate_desc *)(desc->address) + in;\n\n\t\tif (cvt_gate_to_trap(in, entry, &traps[out]))\n\t\t\tout++;\n\t}\n\ttraps[out].address = 0;\n}\n\nvoid xen_copy_trap_info(struct trap_info *traps)\n{\n\tconst struct desc_ptr *desc = this_cpu_ptr(&idt_desc);\n\n\txen_convert_trap_info(desc, traps);\n}\n\n/* Load a new IDT into Xen.  In principle this can be per-CPU, so we\n   hold a spinlock to protect the static traps[] array (static because\n   it avoids allocation, and saves stack space). */\nstatic void xen_load_idt(const struct desc_ptr *desc)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\tstatic struct trap_info traps[257];\n\n\ttrace_xen_cpu_load_idt(desc);\n\n\tspin_lock(&lock);\n\n\tmemcpy(this_cpu_ptr(&idt_desc), desc, sizeof(idt_desc));\n\n\txen_convert_trap_info(desc, traps);\n\n\txen_mc_flush();\n\tif (HYPERVISOR_set_trap_table(traps))\n\t\tBUG();\n\n\tspin_unlock(&lock);\n}\n\n/* Write a GDT descriptor entry.  Ignore LDT descriptors, since\n   they're handled differently. */\nstatic void xen_write_gdt_entry(struct desc_struct *dt, int entry,\n\t\t\t\tconst void *desc, int type)\n{\n\ttrace_xen_cpu_write_gdt_entry(dt, entry, desc, type);\n\n\tpreempt_disable();\n\n\tswitch (type) {\n\tcase DESC_LDT:\n\tcase DESC_TSS:\n\t\t/* ignore */\n\t\tbreak;\n\n\tdefault: {\n\t\txmaddr_t maddr = arbitrary_virt_to_machine(&dt[entry]);\n\n\t\txen_mc_flush();\n\t\tif (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))\n\t\t\tBUG();\n\t}\n\n\t}\n\n\tpreempt_enable();\n}\n\n/*\n * Version of write_gdt_entry for use at early boot-time needed to\n * update an entry as simply as possible.\n */\nstatic void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,\n\t\t\t\t\t    const void *desc, int type)\n{\n\ttrace_xen_cpu_write_gdt_entry(dt, entry, desc, type);\n\n\tswitch (type) {\n\tcase DESC_LDT:\n\tcase DESC_TSS:\n\t\t/* ignore */\n\t\tbreak;\n\n\tdefault: {\n\t\txmaddr_t maddr = virt_to_machine(&dt[entry]);\n\n\t\tif (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))\n\t\t\tdt[entry] = *(struct desc_struct *)desc;\n\t}\n\n\t}\n}\n\nstatic void xen_load_sp0(unsigned long sp0)\n{\n\tstruct multicall_space mcs;\n\n\tmcs = xen_mc_entry(0);\n\tMULTI_stack_switch(mcs.mc, __KERNEL_DS, sp0);\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n\tthis_cpu_write(cpu_tss_rw.x86_tss.sp0, sp0);\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic void xen_update_io_bitmap(void)\n{\n\tstruct physdev_set_iobitmap iobitmap;\n\tstruct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);\n\n\tnative_tss_update_io_bitmap();\n\n\tiobitmap.bitmap = (uint8_t *)(&tss->x86_tss) +\n\t\t\t  tss->x86_tss.io_bitmap_base;\n\tif (tss->x86_tss.io_bitmap_base == IO_BITMAP_OFFSET_INVALID)\n\t\tiobitmap.nr_ports = 0;\n\telse\n\t\tiobitmap.nr_ports = IO_BITMAP_BITS;\n\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &iobitmap);\n}\n#endif\n\nstatic void xen_io_delay(void)\n{\n}\n\nstatic DEFINE_PER_CPU(unsigned long, xen_cr0_value);\n\nstatic unsigned long xen_read_cr0(void)\n{\n\tunsigned long cr0 = this_cpu_read(xen_cr0_value);\n\n\tif (unlikely(cr0 == 0)) {\n\t\tcr0 = native_read_cr0();\n\t\tthis_cpu_write(xen_cr0_value, cr0);\n\t}\n\n\treturn cr0;\n}\n\nstatic void xen_write_cr0(unsigned long cr0)\n{\n\tstruct multicall_space mcs;\n\n\tthis_cpu_write(xen_cr0_value, cr0);\n\n\t/* Only pay attention to cr0.TS; everything else is\n\t   ignored. */\n\tmcs = xen_mc_entry(0);\n\n\tMULTI_fpu_taskswitch(mcs.mc, (cr0 & X86_CR0_TS) != 0);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_write_cr4(unsigned long cr4)\n{\n\tcr4 &= ~(X86_CR4_PGE | X86_CR4_PSE | X86_CR4_PCE);\n\n\tnative_write_cr4(cr4);\n}\n\nstatic u64 xen_read_msr_safe(unsigned int msr, int *err)\n{\n\tu64 val;\n\n\tif (pmu_msr_read(msr, &val, err))\n\t\treturn val;\n\n\tval = native_read_msr_safe(msr, err);\n\tswitch (msr) {\n\tcase MSR_IA32_APICBASE:\n\t\tval &= ~X2APIC_ENABLE;\n\t\tbreak;\n\t}\n\treturn val;\n}\n\nstatic int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)\n{\n\tint ret;\n#ifdef CONFIG_X86_64\n\tunsigned int which;\n\tu64 base;\n#endif\n\n\tret = 0;\n\n\tswitch (msr) {\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\t\twhich = SEGBASE_FS; goto set;\n\tcase MSR_KERNEL_GS_BASE:\twhich = SEGBASE_GS_USER; goto set;\n\tcase MSR_GS_BASE:\t\twhich = SEGBASE_GS_KERNEL; goto set;\n\n\tset:\n\t\tbase = ((u64)high << 32) | low;\n\t\tif (HYPERVISOR_set_segment_base(which, base) != 0)\n\t\t\tret = -EIO;\n\t\tbreak;\n#endif\n\n\tcase MSR_STAR:\n\tcase MSR_CSTAR:\n\tcase MSR_LSTAR:\n\tcase MSR_SYSCALL_MASK:\n\tcase MSR_IA32_SYSENTER_CS:\n\tcase MSR_IA32_SYSENTER_ESP:\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\t/* Fast syscall setup is all done in hypercalls, so\n\t\t   these are all ignored.  Stub them out here to stop\n\t\t   Xen console noise. */\n\t\tbreak;\n\n\tdefault:\n\t\tif (!pmu_msr_write(msr, low, high, &ret))\n\t\t\tret = native_write_msr_safe(msr, low, high);\n\t}\n\n\treturn ret;\n}\n\nstatic u64 xen_read_msr(unsigned int msr)\n{\n\t/*\n\t * This will silently swallow a #GP from RDMSR.  It may be worth\n\t * changing that.\n\t */\n\tint err;\n\n\treturn xen_read_msr_safe(msr, &err);\n}\n\nstatic void xen_write_msr(unsigned int msr, unsigned low, unsigned high)\n{\n\t/*\n\t * This will silently swallow a #GP from WRMSR.  It may be worth\n\t * changing that.\n\t */\n\txen_write_msr_safe(msr, low, high);\n}\n\n/* This is called once we have the cpu_possible_mask */\nvoid __init xen_setup_vcpu_info_placement(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\t/* Set up direct vCPU id mapping for PV guests. */\n\t\tper_cpu(xen_vcpu_id, cpu) = cpu;\n\n\t\t/*\n\t\t * xen_vcpu_setup(cpu) can fail  -- in which case it\n\t\t * falls back to the shared_info version for cpus\n\t\t * where xen_vcpu_nr(cpu) < MAX_VIRT_CPUS.\n\t\t *\n\t\t * xen_cpu_up_prepare_pv() handles the rest by failing\n\t\t * them in hotplug.\n\t\t */\n\t\t(void) xen_vcpu_setup(cpu);\n\t}\n\n\t/*\n\t * xen_vcpu_setup managed to place the vcpu_info within the\n\t * percpu area for all cpus, so make use of it.\n\t */\n\tif (xen_have_vcpu_info_placement) {\n\t\tpv_ops.irq.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);\n\t\tpv_ops.irq.restore_fl =\n\t\t\t__PV_IS_CALLEE_SAVE(xen_restore_fl_direct);\n\t\tpv_ops.irq.irq_disable =\n\t\t\t__PV_IS_CALLEE_SAVE(xen_irq_disable_direct);\n\t\tpv_ops.irq.irq_enable =\n\t\t\t__PV_IS_CALLEE_SAVE(xen_irq_enable_direct);\n\t\tpv_ops.mmu.read_cr2 =\n\t\t\t__PV_IS_CALLEE_SAVE(xen_read_cr2_direct);\n\t}\n}\n\nstatic const struct pv_info xen_info __initconst = {\n\t.shared_kernel_pmd = 0,\n\n#ifdef CONFIG_X86_64\n\t.extra_user_64bit_cs = FLAT_USER_CS64,\n#endif\n\t.name = \"Xen\",\n};\n\nstatic const struct pv_cpu_ops xen_cpu_ops __initconst = {\n\t.cpuid = xen_cpuid,\n\n\t.set_debugreg = xen_set_debugreg,\n\t.get_debugreg = xen_get_debugreg,\n\n\t.read_cr0 = xen_read_cr0,\n\t.write_cr0 = xen_write_cr0,\n\n\t.write_cr4 = xen_write_cr4,\n\n\t.wbinvd = native_wbinvd,\n\n\t.read_msr = xen_read_msr,\n\t.write_msr = xen_write_msr,\n\n\t.read_msr_safe = xen_read_msr_safe,\n\t.write_msr_safe = xen_write_msr_safe,\n\n\t.read_pmc = xen_read_pmc,\n\n\t.iret = xen_iret,\n#ifdef CONFIG_X86_64\n\t.usergs_sysret64 = xen_sysret64,\n#endif\n\n\t.load_tr_desc = paravirt_nop,\n\t.set_ldt = xen_set_ldt,\n\t.load_gdt = xen_load_gdt,\n\t.load_idt = xen_load_idt,\n\t.load_tls = xen_load_tls,\n#ifdef CONFIG_X86_64\n\t.load_gs_index = xen_load_gs_index,\n#endif\n\n\t.alloc_ldt = xen_alloc_ldt,\n\t.free_ldt = xen_free_ldt,\n\n\t.store_tr = xen_store_tr,\n\n\t.write_ldt_entry = xen_write_ldt_entry,\n\t.write_gdt_entry = xen_write_gdt_entry,\n\t.write_idt_entry = xen_write_idt_entry,\n\t.load_sp0 = xen_load_sp0,\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n\t.update_io_bitmap = xen_update_io_bitmap,\n#endif\n\t.io_delay = xen_io_delay,\n\n\t/* Xen takes care of %gs when switching to usermode for us */\n\t.swapgs = paravirt_nop,\n\n\t.start_context_switch = paravirt_start_context_switch,\n\t.end_context_switch = xen_end_context_switch,\n};\n\nstatic void xen_restart(char *msg)\n{\n\txen_reboot(SHUTDOWN_reboot);\n}\n\nstatic void xen_machine_halt(void)\n{\n\txen_reboot(SHUTDOWN_poweroff);\n}\n\nstatic void xen_machine_power_off(void)\n{\n\tif (pm_power_off)\n\t\tpm_power_off();\n\txen_reboot(SHUTDOWN_poweroff);\n}\n\nstatic void xen_crash_shutdown(struct pt_regs *regs)\n{\n\txen_reboot(SHUTDOWN_crash);\n}\n\nstatic const struct machine_ops xen_machine_ops __initconst = {\n\t.restart = xen_restart,\n\t.halt = xen_machine_halt,\n\t.power_off = xen_machine_power_off,\n\t.shutdown = xen_machine_halt,\n\t.crash_shutdown = xen_crash_shutdown,\n\t.emergency_restart = xen_emergency_restart,\n};\n\nstatic unsigned char xen_get_nmi_reason(void)\n{\n\tunsigned char reason = 0;\n\n\t/* Construct a value which looks like it came from port 0x61. */\n\tif (test_bit(_XEN_NMIREASON_io_error,\n\t\t     &HYPERVISOR_shared_info->arch.nmi_reason))\n\t\treason |= NMI_REASON_IOCHK;\n\tif (test_bit(_XEN_NMIREASON_pci_serr,\n\t\t     &HYPERVISOR_shared_info->arch.nmi_reason))\n\t\treason |= NMI_REASON_SERR;\n\n\treturn reason;\n}\n\nstatic void __init xen_boot_params_init_edd(void)\n{\n#if IS_ENABLED(CONFIG_EDD)\n\tstruct xen_platform_op op;\n\tstruct edd_info *edd_info;\n\tu32 *mbr_signature;\n\tunsigned nr;\n\tint ret;\n\n\tedd_info = boot_params.eddbuf;\n\tmbr_signature = boot_params.edd_mbr_sig_buffer;\n\n\top.cmd = XENPF_firmware_info;\n\n\top.u.firmware_info.type = XEN_FW_DISK_INFO;\n\tfor (nr = 0; nr < EDDMAXNR; nr++) {\n\t\tstruct edd_info *info = edd_info + nr;\n\n\t\top.u.firmware_info.index = nr;\n\t\tinfo->params.length = sizeof(info->params);\n\t\tset_xen_guest_handle(op.u.firmware_info.u.disk_info.edd_params,\n\t\t\t\t     &info->params);\n\t\tret = HYPERVISOR_platform_op(&op);\n\t\tif (ret)\n\t\t\tbreak;\n\n#define C(x) info->x = op.u.firmware_info.u.disk_info.x\n\t\tC(device);\n\t\tC(version);\n\t\tC(interface_support);\n\t\tC(legacy_max_cylinder);\n\t\tC(legacy_max_head);\n\t\tC(legacy_sectors_per_track);\n#undef C\n\t}\n\tboot_params.eddbuf_entries = nr;\n\n\top.u.firmware_info.type = XEN_FW_DISK_MBR_SIGNATURE;\n\tfor (nr = 0; nr < EDD_MBR_SIG_MAX; nr++) {\n\t\top.u.firmware_info.index = nr;\n\t\tret = HYPERVISOR_platform_op(&op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tmbr_signature[nr] = op.u.firmware_info.u.disk_mbr_signature.mbr_signature;\n\t}\n\tboot_params.edd_mbr_sig_buf_entries = nr;\n#endif\n}\n\n/*\n * Set up the GDT and segment registers for -fstack-protector.  Until\n * we do this, we have to be careful not to call any stack-protected\n * function, which is most of the kernel.\n */\nstatic void __init xen_setup_gdt(int cpu)\n{\n\tpv_ops.cpu.write_gdt_entry = xen_write_gdt_entry_boot;\n\tpv_ops.cpu.load_gdt = xen_load_gdt_boot;\n\n\tsetup_stack_canary_segment(cpu);\n\tswitch_to_new_gdt(cpu);\n\n\tpv_ops.cpu.write_gdt_entry = xen_write_gdt_entry;\n\tpv_ops.cpu.load_gdt = xen_load_gdt;\n}\n\nstatic void __init xen_dom0_set_legacy_features(void)\n{\n\tx86_platform.legacy.rtc = 1;\n}\n\n/* First C function to be called on Xen boot */\nasmlinkage __visible void __init xen_start_kernel(void)\n{\n\tstruct physdev_set_iopl set_iopl;\n\tunsigned long initrd_start = 0;\n\tint rc;\n\n\tif (!xen_start_info)\n\t\treturn;\n\n\txen_domain_type = XEN_PV_DOMAIN;\n\txen_start_flags = xen_start_info->flags;\n\n\txen_setup_features();\n\n\t/* Install Xen paravirt ops */\n\tpv_info = xen_info;\n\tpv_ops.init.patch = paravirt_patch_default;\n\tpv_ops.cpu = xen_cpu_ops;\n\txen_init_irq_ops();\n\n\t/*\n\t * Setup xen_vcpu early because it is needed for\n\t * local_irq_disable(), irqs_disabled(), e.g. in printk().\n\t *\n\t * Don't do the full vcpu_info placement stuff until we have\n\t * the cpu_possible_mask and a non-dummy shared_info.\n\t */\n\txen_vcpu_info_reset(0);\n\n\tx86_platform.get_nmi_reason = xen_get_nmi_reason;\n\n\tx86_init.resources.memory_setup = xen_memory_setup;\n\tx86_init.irqs.intr_mode_select\t= x86_init_noop;\n\tx86_init.irqs.intr_mode_init\t= x86_init_noop;\n\tx86_init.oem.arch_setup = xen_arch_setup;\n\tx86_init.oem.banner = xen_banner;\n\tx86_init.hyper.init_platform = xen_pv_init_platform;\n\tx86_init.hyper.guest_late_init = xen_pv_guest_late_init;\n\n\t/*\n\t * Set up some pagetable state before starting to set any ptes.\n\t */\n\n\txen_setup_machphys_mapping();\n\txen_init_mmu_ops();\n\n\t/* Prevent unwanted bits from being set in PTEs. */\n\t__supported_pte_mask &= ~_PAGE_GLOBAL;\n\t__default_kernel_pte_mask &= ~_PAGE_GLOBAL;\n\n\t/*\n\t * Prevent page tables from being allocated in highmem, even\n\t * if CONFIG_HIGHPTE is enabled.\n\t */\n\t__userpte_alloc_gfp &= ~__GFP_HIGHMEM;\n\n\t/* Get mfn list */\n\txen_build_dynamic_phys_to_machine();\n\n\t/*\n\t * Set up kernel GDT and segment registers, mainly so that\n\t * -fstack-protector code can be executed.\n\t */\n\txen_setup_gdt(0);\n\n\t/* Work out if we support NX */\n\tget_cpu_cap(&boot_cpu_data);\n\tx86_configure_nx();\n\n\t/* Determine virtual and physical address sizes */\n\tget_cpu_address_sizes(&boot_cpu_data);\n\n\t/* Let's presume PV guests always boot on vCPU with id 0. */\n\tper_cpu(xen_vcpu_id, 0) = 0;\n\n\tidt_setup_early_handler();\n\n\txen_init_capabilities();\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\t/*\n\t * set up the basic apic ops.\n\t */\n\txen_init_apic();\n#endif\n\n\tif (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {\n\t\tpv_ops.mmu.ptep_modify_prot_start =\n\t\t\txen_ptep_modify_prot_start;\n\t\tpv_ops.mmu.ptep_modify_prot_commit =\n\t\t\txen_ptep_modify_prot_commit;\n\t}\n\n\tmachine_ops = xen_machine_ops;\n\n\t/*\n\t * The only reliable way to retain the initial address of the\n\t * percpu gdt_page is to remember it here, so we can go and\n\t * mark it RW later, when the initial percpu area is freed.\n\t */\n\txen_initial_gdt = &per_cpu(gdt_page, 0);\n\n\txen_smp_init();\n\n#ifdef CONFIG_ACPI_NUMA\n\t/*\n\t * The pages we from Xen are not related to machine pages, so\n\t * any NUMA information the kernel tries to get from ACPI will\n\t * be meaningless.  Prevent it from trying.\n\t */\n\tacpi_numa = -1;\n#endif\n\tWARN_ON(xen_cpuhp_setup(xen_cpu_up_prepare_pv, xen_cpu_dead_pv));\n\n\tlocal_irq_disable();\n\tearly_boot_irqs_disabled = true;\n\n\txen_raw_console_write(\"mapping kernel into physical memory\\n\");\n\txen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base,\n\t\t\t\t   xen_start_info->nr_pages);\n\txen_reserve_special_pages();\n\n\t/* keep using Xen gdt for now; no urgent need to change it */\n\n#ifdef CONFIG_X86_32\n\tpv_info.kernel_rpl = 1;\n\tif (xen_feature(XENFEAT_supervisor_mode_kernel))\n\t\tpv_info.kernel_rpl = 0;\n#else\n\tpv_info.kernel_rpl = 0;\n#endif\n\t/* set the limit of our address space */\n\txen_reserve_top();\n\n\t/*\n\t * We used to do this in xen_arch_setup, but that is too late\n\t * on AMD were early_cpu_init (run before ->arch_setup()) calls\n\t * early_amd_init which pokes 0xcf8 port.\n\t */\n\tset_iopl.iopl = 1;\n\trc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);\n\tif (rc != 0)\n\t\txen_raw_printk(\"physdev_op failed %d\\n\", rc);\n\n#ifdef CONFIG_X86_32\n\t/* set up basic CPUID stuff */\n\tcpu_detect(&new_cpu_data);\n\tset_cpu_cap(&new_cpu_data, X86_FEATURE_FPU);\n\tnew_cpu_data.x86_capability[CPUID_1_EDX] = cpuid_edx(1);\n#endif\n\n\tif (xen_start_info->mod_start) {\n\t    if (xen_start_info->flags & SIF_MOD_START_PFN)\n\t\tinitrd_start = PFN_PHYS(xen_start_info->mod_start);\n\t    else\n\t\tinitrd_start = __pa(xen_start_info->mod_start);\n\t}\n\n\t/* Poke various useful things into boot_params */\n\tboot_params.hdr.type_of_loader = (9 << 4) | 0;\n\tboot_params.hdr.ramdisk_image = initrd_start;\n\tboot_params.hdr.ramdisk_size = xen_start_info->mod_len;\n\tboot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);\n\tboot_params.hdr.hardware_subarch = X86_SUBARCH_XEN;\n\n\tif (!xen_initial_domain()) {\n\t\tadd_preferred_console(\"xenboot\", 0, NULL);\n\t\tif (pci_xen)\n\t\t\tx86_init.pci.arch_init = pci_xen_init;\n\t} else {\n\t\tconst struct dom0_vga_console_info *info =\n\t\t\t(void *)((char *)xen_start_info +\n\t\t\t\t xen_start_info->console.dom0.info_off);\n\t\tstruct xen_platform_op op = {\n\t\t\t.cmd = XENPF_firmware_info,\n\t\t\t.interface_version = XENPF_INTERFACE_VERSION,\n\t\t\t.u.firmware_info.type = XEN_FW_KBD_SHIFT_FLAGS,\n\t\t};\n\n\t\tx86_platform.set_legacy_features =\n\t\t\t\txen_dom0_set_legacy_features;\n\t\txen_init_vga(info, xen_start_info->console.dom0.info_size);\n\t\txen_start_info->console.domU.mfn = 0;\n\t\txen_start_info->console.domU.evtchn = 0;\n\n\t\tif (HYPERVISOR_platform_op(&op) == 0)\n\t\t\tboot_params.kbd_status = op.u.firmware_info.u.kbd_shift_flags;\n\n\t\t/* Make sure ACS will be enabled */\n\t\tpci_request_acs();\n\n\t\txen_acpi_sleep_register();\n\n\t\t/* Avoid searching for BIOS MP tables */\n\t\tx86_init.mpparse.find_smp_config = x86_init_noop;\n\t\tx86_init.mpparse.get_smp_config = x86_init_uint_noop;\n\n\t\txen_boot_params_init_edd();\n\t}\n\n\tif (!boot_params.screen_info.orig_video_isVGA)\n\t\tadd_preferred_console(\"tty\", 0, NULL);\n\tadd_preferred_console(\"hvc\", 0, NULL);\n\tif (boot_params.screen_info.orig_video_isVGA)\n\t\tadd_preferred_console(\"tty\", 0, NULL);\n\n#ifdef CONFIG_PCI\n\t/* PCI BIOS service won't work from a PV guest. */\n\tpci_probe &= ~PCI_PROBE_BIOS;\n#endif\n\txen_raw_console_write(\"about to get started...\\n\");\n\n\t/* We need this for printk timestamps */\n\txen_setup_runstate_info(0);\n\n\txen_efi_init(&boot_params);\n\n\t/* Start the world */\n#ifdef CONFIG_X86_32\n\ti386_start_kernel();\n#else\n\tcr4_init_shadow(); /* 32b kernel does this in i386_start_kernel() */\n\tx86_64_start_reservations((char *)__pa_symbol(&boot_params));\n#endif\n}\n\nstatic int xen_cpu_up_prepare_pv(unsigned int cpu)\n{\n\tint rc;\n\n\tif (per_cpu(xen_vcpu, cpu) == NULL)\n\t\treturn -ENODEV;\n\n\txen_setup_timer(cpu);\n\n\trc = xen_smp_intr_init(cpu);\n\tif (rc) {\n\t\tWARN(1, \"xen_smp_intr_init() for CPU %d failed: %d\\n\",\n\t\t     cpu, rc);\n\t\treturn rc;\n\t}\n\n\trc = xen_smp_intr_init_pv(cpu);\n\tif (rc) {\n\t\tWARN(1, \"xen_smp_intr_init_pv() for CPU %d failed: %d\\n\",\n\t\t     cpu, rc);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int xen_cpu_dead_pv(unsigned int cpu)\n{\n\txen_smp_intr_free(cpu);\n\txen_smp_intr_free_pv(cpu);\n\n\txen_teardown_timer(cpu);\n\n\treturn 0;\n}\n\nstatic uint32_t __init xen_platform_pv(void)\n{\n\tif (xen_pv_domain())\n\t\treturn xen_cpuid_base();\n\n\treturn 0;\n}\n\nconst __initconst struct hypervisor_x86 x86_hyper_xen_pv = {\n\t.name                   = \"Xen PV\",\n\t.detect                 = xen_platform_pv,\n\t.type\t\t\t= X86_HYPER_XEN_PV,\n\t.runtime.pin_vcpu       = xen_pin_vcpu,\n\t.ignore_nopv\t\t= true,\n};\n"], "fixing_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_IOBITMAP_H\n#define _ASM_X86_IOBITMAP_H\n\n#include <linux/refcount.h>\n#include <asm/processor.h>\n\nstruct io_bitmap {\n\tu64\t\tsequence;\n\trefcount_t\trefcnt;\n\t/* The maximum number of bytes to copy so all zero bits are covered */\n\tunsigned int\tmax;\n\tunsigned long\tbitmap[IO_BITMAP_LONGS];\n};\n\nstruct task_struct;\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nvoid io_bitmap_share(struct task_struct *tsk);\nvoid io_bitmap_exit(struct task_struct *tsk);\n\nstatic inline void native_tss_invalidate_io_bitmap(void)\n{\n\t/*\n\t * Invalidate the I/O bitmap by moving io_bitmap_base outside the\n\t * TSS limit so any subsequent I/O access from user space will\n\t * trigger a #GP.\n\t *\n\t * This is correct even when VMEXIT rewrites the TSS limit\n\t * to 0x67 as the only requirement is that the base points\n\t * outside the limit.\n\t */\n\tthis_cpu_write(cpu_tss_rw.x86_tss.io_bitmap_base,\n\t\t       IO_BITMAP_OFFSET_INVALID);\n}\n\nvoid native_tss_update_io_bitmap(void);\n\n#ifdef CONFIG_PARAVIRT_XXL\n#include <asm/paravirt.h>\n#else\n#define tss_update_io_bitmap native_tss_update_io_bitmap\n#define tss_invalidate_io_bitmap native_tss_invalidate_io_bitmap\n#endif\n\n#else\nstatic inline void io_bitmap_share(struct task_struct *tsk) { }\nstatic inline void io_bitmap_exit(struct task_struct *tsk) { }\nstatic inline void tss_update_io_bitmap(void) { }\n#endif\n\n#endif\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_PARAVIRT_H\n#define _ASM_X86_PARAVIRT_H\n/* Various instructions on x86 need to be replaced for\n * para-virtualization: those hooks are defined here. */\n\n#ifdef CONFIG_PARAVIRT\n#include <asm/pgtable_types.h>\n#include <asm/asm.h>\n#include <asm/nospec-branch.h>\n\n#include <asm/paravirt_types.h>\n\n#ifndef __ASSEMBLY__\n#include <linux/bug.h>\n#include <linux/types.h>\n#include <linux/cpumask.h>\n#include <asm/frame.h>\n\nstatic inline unsigned long long paravirt_sched_clock(void)\n{\n\treturn PVOP_CALL0(unsigned long long, time.sched_clock);\n}\n\nstruct static_key;\nextern struct static_key paravirt_steal_enabled;\nextern struct static_key paravirt_steal_rq_enabled;\n\n__visible void __native_queued_spin_unlock(struct qspinlock *lock);\nbool pv_is_native_spin_unlock(void);\n__visible bool __native_vcpu_is_preempted(long cpu);\nbool pv_is_native_vcpu_is_preempted(void);\n\nstatic inline u64 paravirt_steal_clock(int cpu)\n{\n\treturn PVOP_CALL1(u64, time.steal_clock, cpu);\n}\n\n/* The paravirtualized I/O functions */\nstatic inline void slow_down_io(void)\n{\n\tpv_ops.cpu.io_delay();\n#ifdef REALLY_SLOW_IO\n\tpv_ops.cpu.io_delay();\n\tpv_ops.cpu.io_delay();\n\tpv_ops.cpu.io_delay();\n#endif\n}\n\nvoid native_flush_tlb_local(void);\nvoid native_flush_tlb_global(void);\nvoid native_flush_tlb_one_user(unsigned long addr);\nvoid native_flush_tlb_others(const struct cpumask *cpumask,\n\t\t\t     const struct flush_tlb_info *info);\n\nstatic inline void __flush_tlb_local(void)\n{\n\tPVOP_VCALL0(mmu.flush_tlb_user);\n}\n\nstatic inline void __flush_tlb_global(void)\n{\n\tPVOP_VCALL0(mmu.flush_tlb_kernel);\n}\n\nstatic inline void __flush_tlb_one_user(unsigned long addr)\n{\n\tPVOP_VCALL1(mmu.flush_tlb_one_user, addr);\n}\n\nstatic inline void __flush_tlb_others(const struct cpumask *cpumask,\n\t\t\t\t      const struct flush_tlb_info *info)\n{\n\tPVOP_VCALL2(mmu.flush_tlb_others, cpumask, info);\n}\n\nstatic inline void paravirt_tlb_remove_table(struct mmu_gather *tlb, void *table)\n{\n\tPVOP_VCALL2(mmu.tlb_remove_table, tlb, table);\n}\n\nstatic inline void paravirt_arch_exit_mmap(struct mm_struct *mm)\n{\n\tPVOP_VCALL1(mmu.exit_mmap, mm);\n}\n\n#ifdef CONFIG_PARAVIRT_XXL\nstatic inline void load_sp0(unsigned long sp0)\n{\n\tPVOP_VCALL1(cpu.load_sp0, sp0);\n}\n\n/* The paravirtualized CPUID instruction. */\nstatic inline void __cpuid(unsigned int *eax, unsigned int *ebx,\n\t\t\t   unsigned int *ecx, unsigned int *edx)\n{\n\tPVOP_VCALL4(cpu.cpuid, eax, ebx, ecx, edx);\n}\n\n/*\n * These special macros can be used to get or set a debugging register\n */\nstatic inline unsigned long paravirt_get_debugreg(int reg)\n{\n\treturn PVOP_CALL1(unsigned long, cpu.get_debugreg, reg);\n}\n#define get_debugreg(var, reg) var = paravirt_get_debugreg(reg)\nstatic inline void set_debugreg(unsigned long val, int reg)\n{\n\tPVOP_VCALL2(cpu.set_debugreg, reg, val);\n}\n\nstatic inline unsigned long read_cr0(void)\n{\n\treturn PVOP_CALL0(unsigned long, cpu.read_cr0);\n}\n\nstatic inline void write_cr0(unsigned long x)\n{\n\tPVOP_VCALL1(cpu.write_cr0, x);\n}\n\nstatic inline unsigned long read_cr2(void)\n{\n\treturn PVOP_CALLEE0(unsigned long, mmu.read_cr2);\n}\n\nstatic inline void write_cr2(unsigned long x)\n{\n\tPVOP_VCALL1(mmu.write_cr2, x);\n}\n\nstatic inline unsigned long __read_cr3(void)\n{\n\treturn PVOP_CALL0(unsigned long, mmu.read_cr3);\n}\n\nstatic inline void write_cr3(unsigned long x)\n{\n\tPVOP_VCALL1(mmu.write_cr3, x);\n}\n\nstatic inline void __write_cr4(unsigned long x)\n{\n\tPVOP_VCALL1(cpu.write_cr4, x);\n}\n\nstatic inline void arch_safe_halt(void)\n{\n\tPVOP_VCALL0(irq.safe_halt);\n}\n\nstatic inline void halt(void)\n{\n\tPVOP_VCALL0(irq.halt);\n}\n\nstatic inline void wbinvd(void)\n{\n\tPVOP_VCALL0(cpu.wbinvd);\n}\n\n#define get_kernel_rpl()  (pv_info.kernel_rpl)\n\nstatic inline u64 paravirt_read_msr(unsigned msr)\n{\n\treturn PVOP_CALL1(u64, cpu.read_msr, msr);\n}\n\nstatic inline void paravirt_write_msr(unsigned msr,\n\t\t\t\t      unsigned low, unsigned high)\n{\n\tPVOP_VCALL3(cpu.write_msr, msr, low, high);\n}\n\nstatic inline u64 paravirt_read_msr_safe(unsigned msr, int *err)\n{\n\treturn PVOP_CALL2(u64, cpu.read_msr_safe, msr, err);\n}\n\nstatic inline int paravirt_write_msr_safe(unsigned msr,\n\t\t\t\t\t  unsigned low, unsigned high)\n{\n\treturn PVOP_CALL3(int, cpu.write_msr_safe, msr, low, high);\n}\n\n#define rdmsr(msr, val1, val2)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tu64 _l = paravirt_read_msr(msr);\t\\\n\tval1 = (u32)_l;\t\t\t\t\\\n\tval2 = _l >> 32;\t\t\t\\\n} while (0)\n\n#define wrmsr(msr, val1, val2)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tparavirt_write_msr(msr, val1, val2);\t\\\n} while (0)\n\n#define rdmsrl(msr, val)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tval = paravirt_read_msr(msr);\t\t\\\n} while (0)\n\nstatic inline void wrmsrl(unsigned msr, u64 val)\n{\n\twrmsr(msr, (u32)val, (u32)(val>>32));\n}\n\n#define wrmsr_safe(msr, a, b)\tparavirt_write_msr_safe(msr, a, b)\n\n/* rdmsr with exception handling */\n#define rdmsr_safe(msr, a, b)\t\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tint _err;\t\t\t\t\t\\\n\tu64 _l = paravirt_read_msr_safe(msr, &_err);\t\\\n\t(*a) = (u32)_l;\t\t\t\t\t\\\n\t(*b) = _l >> 32;\t\t\t\t\\\n\t_err;\t\t\t\t\t\t\\\n})\n\nstatic inline int rdmsrl_safe(unsigned msr, unsigned long long *p)\n{\n\tint err;\n\n\t*p = paravirt_read_msr_safe(msr, &err);\n\treturn err;\n}\n\nstatic inline unsigned long long paravirt_read_pmc(int counter)\n{\n\treturn PVOP_CALL1(u64, cpu.read_pmc, counter);\n}\n\n#define rdpmc(counter, low, high)\t\t\\\ndo {\t\t\t\t\t\t\\\n\tu64 _l = paravirt_read_pmc(counter);\t\\\n\tlow = (u32)_l;\t\t\t\t\\\n\thigh = _l >> 32;\t\t\t\\\n} while (0)\n\n#define rdpmcl(counter, val) ((val) = paravirt_read_pmc(counter))\n\nstatic inline void paravirt_alloc_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tPVOP_VCALL2(cpu.alloc_ldt, ldt, entries);\n}\n\nstatic inline void paravirt_free_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tPVOP_VCALL2(cpu.free_ldt, ldt, entries);\n}\n\nstatic inline void load_TR_desc(void)\n{\n\tPVOP_VCALL0(cpu.load_tr_desc);\n}\nstatic inline void load_gdt(const struct desc_ptr *dtr)\n{\n\tPVOP_VCALL1(cpu.load_gdt, dtr);\n}\nstatic inline void load_idt(const struct desc_ptr *dtr)\n{\n\tPVOP_VCALL1(cpu.load_idt, dtr);\n}\nstatic inline void set_ldt(const void *addr, unsigned entries)\n{\n\tPVOP_VCALL2(cpu.set_ldt, addr, entries);\n}\nstatic inline unsigned long paravirt_store_tr(void)\n{\n\treturn PVOP_CALL0(unsigned long, cpu.store_tr);\n}\n\n#define store_tr(tr)\t((tr) = paravirt_store_tr())\nstatic inline void load_TLS(struct thread_struct *t, unsigned cpu)\n{\n\tPVOP_VCALL2(cpu.load_tls, t, cpu);\n}\n\n#ifdef CONFIG_X86_64\nstatic inline void load_gs_index(unsigned int gs)\n{\n\tPVOP_VCALL1(cpu.load_gs_index, gs);\n}\n#endif\n\nstatic inline void write_ldt_entry(struct desc_struct *dt, int entry,\n\t\t\t\t   const void *desc)\n{\n\tPVOP_VCALL3(cpu.write_ldt_entry, dt, entry, desc);\n}\n\nstatic inline void write_gdt_entry(struct desc_struct *dt, int entry,\n\t\t\t\t   void *desc, int type)\n{\n\tPVOP_VCALL4(cpu.write_gdt_entry, dt, entry, desc, type);\n}\n\nstatic inline void write_idt_entry(gate_desc *dt, int entry, const gate_desc *g)\n{\n\tPVOP_VCALL3(cpu.write_idt_entry, dt, entry, g);\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic inline void tss_invalidate_io_bitmap(void)\n{\n\tPVOP_VCALL0(cpu.invalidate_io_bitmap);\n}\n\nstatic inline void tss_update_io_bitmap(void)\n{\n\tPVOP_VCALL0(cpu.update_io_bitmap);\n}\n#endif\n\nstatic inline void paravirt_activate_mm(struct mm_struct *prev,\n\t\t\t\t\tstruct mm_struct *next)\n{\n\tPVOP_VCALL2(mmu.activate_mm, prev, next);\n}\n\nstatic inline void paravirt_arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t\t  struct mm_struct *mm)\n{\n\tPVOP_VCALL2(mmu.dup_mmap, oldmm, mm);\n}\n\nstatic inline int paravirt_pgd_alloc(struct mm_struct *mm)\n{\n\treturn PVOP_CALL1(int, mmu.pgd_alloc, mm);\n}\n\nstatic inline void paravirt_pgd_free(struct mm_struct *mm, pgd_t *pgd)\n{\n\tPVOP_VCALL2(mmu.pgd_free, mm, pgd);\n}\n\nstatic inline void paravirt_alloc_pte(struct mm_struct *mm, unsigned long pfn)\n{\n\tPVOP_VCALL2(mmu.alloc_pte, mm, pfn);\n}\nstatic inline void paravirt_release_pte(unsigned long pfn)\n{\n\tPVOP_VCALL1(mmu.release_pte, pfn);\n}\n\nstatic inline void paravirt_alloc_pmd(struct mm_struct *mm, unsigned long pfn)\n{\n\tPVOP_VCALL2(mmu.alloc_pmd, mm, pfn);\n}\n\nstatic inline void paravirt_release_pmd(unsigned long pfn)\n{\n\tPVOP_VCALL1(mmu.release_pmd, pfn);\n}\n\nstatic inline void paravirt_alloc_pud(struct mm_struct *mm, unsigned long pfn)\n{\n\tPVOP_VCALL2(mmu.alloc_pud, mm, pfn);\n}\nstatic inline void paravirt_release_pud(unsigned long pfn)\n{\n\tPVOP_VCALL1(mmu.release_pud, pfn);\n}\n\nstatic inline void paravirt_alloc_p4d(struct mm_struct *mm, unsigned long pfn)\n{\n\tPVOP_VCALL2(mmu.alloc_p4d, mm, pfn);\n}\n\nstatic inline void paravirt_release_p4d(unsigned long pfn)\n{\n\tPVOP_VCALL1(mmu.release_p4d, pfn);\n}\n\nstatic inline pte_t __pte(pteval_t val)\n{\n\tpteval_t ret;\n\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\tret = PVOP_CALLEE2(pteval_t, mmu.make_pte, val, (u64)val >> 32);\n\telse\n\t\tret = PVOP_CALLEE1(pteval_t, mmu.make_pte, val);\n\n\treturn (pte_t) { .pte = ret };\n}\n\nstatic inline pteval_t pte_val(pte_t pte)\n{\n\tpteval_t ret;\n\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\tret = PVOP_CALLEE2(pteval_t, mmu.pte_val,\n\t\t\t\t   pte.pte, (u64)pte.pte >> 32);\n\telse\n\t\tret = PVOP_CALLEE1(pteval_t, mmu.pte_val, pte.pte);\n\n\treturn ret;\n}\n\nstatic inline pgd_t __pgd(pgdval_t val)\n{\n\tpgdval_t ret;\n\n\tif (sizeof(pgdval_t) > sizeof(long))\n\t\tret = PVOP_CALLEE2(pgdval_t, mmu.make_pgd, val, (u64)val >> 32);\n\telse\n\t\tret = PVOP_CALLEE1(pgdval_t, mmu.make_pgd, val);\n\n\treturn (pgd_t) { ret };\n}\n\nstatic inline pgdval_t pgd_val(pgd_t pgd)\n{\n\tpgdval_t ret;\n\n\tif (sizeof(pgdval_t) > sizeof(long))\n\t\tret =  PVOP_CALLEE2(pgdval_t, mmu.pgd_val,\n\t\t\t\t    pgd.pgd, (u64)pgd.pgd >> 32);\n\telse\n\t\tret =  PVOP_CALLEE1(pgdval_t, mmu.pgd_val, pgd.pgd);\n\n\treturn ret;\n}\n\n#define  __HAVE_ARCH_PTEP_MODIFY_PROT_TRANSACTION\nstatic inline pte_t ptep_modify_prot_start(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\t   pte_t *ptep)\n{\n\tpteval_t ret;\n\n\tret = PVOP_CALL3(pteval_t, mmu.ptep_modify_prot_start, vma, addr, ptep);\n\n\treturn (pte_t) { .pte = ret };\n}\n\nstatic inline void ptep_modify_prot_commit(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\t   pte_t *ptep, pte_t old_pte, pte_t pte)\n{\n\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\t/* 5 arg words */\n\t\tpv_ops.mmu.ptep_modify_prot_commit(vma, addr, ptep, pte);\n\telse\n\t\tPVOP_VCALL4(mmu.ptep_modify_prot_commit,\n\t\t\t    vma, addr, ptep, pte.pte);\n}\n\nstatic inline void set_pte(pte_t *ptep, pte_t pte)\n{\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\tPVOP_VCALL3(mmu.set_pte, ptep, pte.pte, (u64)pte.pte >> 32);\n\telse\n\t\tPVOP_VCALL2(mmu.set_pte, ptep, pte.pte);\n}\n\nstatic inline void set_pte_at(struct mm_struct *mm, unsigned long addr,\n\t\t\t      pte_t *ptep, pte_t pte)\n{\n\tif (sizeof(pteval_t) > sizeof(long))\n\t\t/* 5 arg words */\n\t\tpv_ops.mmu.set_pte_at(mm, addr, ptep, pte);\n\telse\n\t\tPVOP_VCALL4(mmu.set_pte_at, mm, addr, ptep, pte.pte);\n}\n\nstatic inline void set_pmd(pmd_t *pmdp, pmd_t pmd)\n{\n\tpmdval_t val = native_pmd_val(pmd);\n\n\tif (sizeof(pmdval_t) > sizeof(long))\n\t\tPVOP_VCALL3(mmu.set_pmd, pmdp, val, (u64)val >> 32);\n\telse\n\t\tPVOP_VCALL2(mmu.set_pmd, pmdp, val);\n}\n\n#if CONFIG_PGTABLE_LEVELS >= 3\nstatic inline pmd_t __pmd(pmdval_t val)\n{\n\tpmdval_t ret;\n\n\tif (sizeof(pmdval_t) > sizeof(long))\n\t\tret = PVOP_CALLEE2(pmdval_t, mmu.make_pmd, val, (u64)val >> 32);\n\telse\n\t\tret = PVOP_CALLEE1(pmdval_t, mmu.make_pmd, val);\n\n\treturn (pmd_t) { ret };\n}\n\nstatic inline pmdval_t pmd_val(pmd_t pmd)\n{\n\tpmdval_t ret;\n\n\tif (sizeof(pmdval_t) > sizeof(long))\n\t\tret =  PVOP_CALLEE2(pmdval_t, mmu.pmd_val,\n\t\t\t\t    pmd.pmd, (u64)pmd.pmd >> 32);\n\telse\n\t\tret =  PVOP_CALLEE1(pmdval_t, mmu.pmd_val, pmd.pmd);\n\n\treturn ret;\n}\n\nstatic inline void set_pud(pud_t *pudp, pud_t pud)\n{\n\tpudval_t val = native_pud_val(pud);\n\n\tif (sizeof(pudval_t) > sizeof(long))\n\t\tPVOP_VCALL3(mmu.set_pud, pudp, val, (u64)val >> 32);\n\telse\n\t\tPVOP_VCALL2(mmu.set_pud, pudp, val);\n}\n#if CONFIG_PGTABLE_LEVELS >= 4\nstatic inline pud_t __pud(pudval_t val)\n{\n\tpudval_t ret;\n\n\tret = PVOP_CALLEE1(pudval_t, mmu.make_pud, val);\n\n\treturn (pud_t) { ret };\n}\n\nstatic inline pudval_t pud_val(pud_t pud)\n{\n\treturn PVOP_CALLEE1(pudval_t, mmu.pud_val, pud.pud);\n}\n\nstatic inline void pud_clear(pud_t *pudp)\n{\n\tset_pud(pudp, __pud(0));\n}\n\nstatic inline void set_p4d(p4d_t *p4dp, p4d_t p4d)\n{\n\tp4dval_t val = native_p4d_val(p4d);\n\n\tPVOP_VCALL2(mmu.set_p4d, p4dp, val);\n}\n\n#if CONFIG_PGTABLE_LEVELS >= 5\n\nstatic inline p4d_t __p4d(p4dval_t val)\n{\n\tp4dval_t ret = PVOP_CALLEE1(p4dval_t, mmu.make_p4d, val);\n\n\treturn (p4d_t) { ret };\n}\n\nstatic inline p4dval_t p4d_val(p4d_t p4d)\n{\n\treturn PVOP_CALLEE1(p4dval_t, mmu.p4d_val, p4d.p4d);\n}\n\nstatic inline void __set_pgd(pgd_t *pgdp, pgd_t pgd)\n{\n\tPVOP_VCALL2(mmu.set_pgd, pgdp, native_pgd_val(pgd));\n}\n\n#define set_pgd(pgdp, pgdval) do {\t\t\t\t\t\\\n\tif (pgtable_l5_enabled())\t\t\t\t\t\t\\\n\t\t__set_pgd(pgdp, pgdval);\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tset_p4d((p4d_t *)(pgdp), (p4d_t) { (pgdval).pgd });\t\\\n} while (0)\n\n#define pgd_clear(pgdp) do {\t\t\t\t\t\t\\\n\tif (pgtable_l5_enabled())\t\t\t\t\t\t\\\n\t\tset_pgd(pgdp, __pgd(0));\t\t\t\t\\\n} while (0)\n\n#endif  /* CONFIG_PGTABLE_LEVELS == 5 */\n\nstatic inline void p4d_clear(p4d_t *p4dp)\n{\n\tset_p4d(p4dp, __p4d(0));\n}\n\n#endif\t/* CONFIG_PGTABLE_LEVELS == 4 */\n\n#endif\t/* CONFIG_PGTABLE_LEVELS >= 3 */\n\n#ifdef CONFIG_X86_PAE\n/* Special-case pte-setting operations for PAE, which can't update a\n   64-bit pte atomically */\nstatic inline void set_pte_atomic(pte_t *ptep, pte_t pte)\n{\n\tPVOP_VCALL3(mmu.set_pte_atomic, ptep, pte.pte, pte.pte >> 32);\n}\n\nstatic inline void pte_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t     pte_t *ptep)\n{\n\tPVOP_VCALL3(mmu.pte_clear, mm, addr, ptep);\n}\n\nstatic inline void pmd_clear(pmd_t *pmdp)\n{\n\tPVOP_VCALL1(mmu.pmd_clear, pmdp);\n}\n#else  /* !CONFIG_X86_PAE */\nstatic inline void set_pte_atomic(pte_t *ptep, pte_t pte)\n{\n\tset_pte(ptep, pte);\n}\n\nstatic inline void pte_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t     pte_t *ptep)\n{\n\tset_pte_at(mm, addr, ptep, __pte(0));\n}\n\nstatic inline void pmd_clear(pmd_t *pmdp)\n{\n\tset_pmd(pmdp, __pmd(0));\n}\n#endif\t/* CONFIG_X86_PAE */\n\n#define  __HAVE_ARCH_START_CONTEXT_SWITCH\nstatic inline void arch_start_context_switch(struct task_struct *prev)\n{\n\tPVOP_VCALL1(cpu.start_context_switch, prev);\n}\n\nstatic inline void arch_end_context_switch(struct task_struct *next)\n{\n\tPVOP_VCALL1(cpu.end_context_switch, next);\n}\n\n#define  __HAVE_ARCH_ENTER_LAZY_MMU_MODE\nstatic inline void arch_enter_lazy_mmu_mode(void)\n{\n\tPVOP_VCALL0(mmu.lazy_mode.enter);\n}\n\nstatic inline void arch_leave_lazy_mmu_mode(void)\n{\n\tPVOP_VCALL0(mmu.lazy_mode.leave);\n}\n\nstatic inline void arch_flush_lazy_mmu_mode(void)\n{\n\tPVOP_VCALL0(mmu.lazy_mode.flush);\n}\n\nstatic inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,\n\t\t\t\tphys_addr_t phys, pgprot_t flags)\n{\n\tpv_ops.mmu.set_fixmap(idx, phys, flags);\n}\n#endif\n\n#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)\n\nstatic __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,\n\t\t\t\t\t\t\tu32 val)\n{\n\tPVOP_VCALL2(lock.queued_spin_lock_slowpath, lock, val);\n}\n\nstatic __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tPVOP_VCALLEE1(lock.queued_spin_unlock, lock);\n}\n\nstatic __always_inline void pv_wait(u8 *ptr, u8 val)\n{\n\tPVOP_VCALL2(lock.wait, ptr, val);\n}\n\nstatic __always_inline void pv_kick(int cpu)\n{\n\tPVOP_VCALL1(lock.kick, cpu);\n}\n\nstatic __always_inline bool pv_vcpu_is_preempted(long cpu)\n{\n\treturn PVOP_CALLEE1(bool, lock.vcpu_is_preempted, cpu);\n}\n\nvoid __raw_callee_save___native_queued_spin_unlock(struct qspinlock *lock);\nbool __raw_callee_save___native_vcpu_is_preempted(long cpu);\n\n#endif /* SMP && PARAVIRT_SPINLOCKS */\n\n#ifdef CONFIG_X86_32\n#define PV_SAVE_REGS \"pushl %ecx; pushl %edx;\"\n#define PV_RESTORE_REGS \"popl %edx; popl %ecx;\"\n\n/* save and restore all caller-save registers, except return value */\n#define PV_SAVE_ALL_CALLER_REGS\t\t\"pushl %ecx;\"\n#define PV_RESTORE_ALL_CALLER_REGS\t\"popl  %ecx;\"\n\n#define PV_FLAGS_ARG \"0\"\n#define PV_EXTRA_CLOBBERS\n#define PV_VEXTRA_CLOBBERS\n#else\n/* save and restore all caller-save registers, except return value */\n#define PV_SAVE_ALL_CALLER_REGS\t\t\t\t\t\t\\\n\t\"push %rcx;\"\t\t\t\t\t\t\t\\\n\t\"push %rdx;\"\t\t\t\t\t\t\t\\\n\t\"push %rsi;\"\t\t\t\t\t\t\t\\\n\t\"push %rdi;\"\t\t\t\t\t\t\t\\\n\t\"push %r8;\"\t\t\t\t\t\t\t\\\n\t\"push %r9;\"\t\t\t\t\t\t\t\\\n\t\"push %r10;\"\t\t\t\t\t\t\t\\\n\t\"push %r11;\"\n#define PV_RESTORE_ALL_CALLER_REGS\t\t\t\t\t\\\n\t\"pop %r11;\"\t\t\t\t\t\t\t\\\n\t\"pop %r10;\"\t\t\t\t\t\t\t\\\n\t\"pop %r9;\"\t\t\t\t\t\t\t\\\n\t\"pop %r8;\"\t\t\t\t\t\t\t\\\n\t\"pop %rdi;\"\t\t\t\t\t\t\t\\\n\t\"pop %rsi;\"\t\t\t\t\t\t\t\\\n\t\"pop %rdx;\"\t\t\t\t\t\t\t\\\n\t\"pop %rcx;\"\n\n/* We save some registers, but all of them, that's too much. We clobber all\n * caller saved registers but the argument parameter */\n#define PV_SAVE_REGS \"pushq %%rdi;\"\n#define PV_RESTORE_REGS \"popq %%rdi;\"\n#define PV_EXTRA_CLOBBERS EXTRA_CLOBBERS, \"rcx\" , \"rdx\", \"rsi\"\n#define PV_VEXTRA_CLOBBERS EXTRA_CLOBBERS, \"rdi\", \"rcx\" , \"rdx\", \"rsi\"\n#define PV_FLAGS_ARG \"D\"\n#endif\n\n/*\n * Generate a thunk around a function which saves all caller-save\n * registers except for the return value.  This allows C functions to\n * be called from assembler code where fewer than normal registers are\n * available.  It may also help code generation around calls from C\n * code if the common case doesn't use many registers.\n *\n * When a callee is wrapped in a thunk, the caller can assume that all\n * arg regs and all scratch registers are preserved across the\n * call. The return value in rax/eax will not be saved, even for void\n * functions.\n */\n#define PV_THUNK_NAME(func) \"__raw_callee_save_\" #func\n#define PV_CALLEE_SAVE_REGS_THUNK(func)\t\t\t\t\t\\\n\textern typeof(func) __raw_callee_save_##func;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tasm(\".pushsection .text;\"\t\t\t\t\t\\\n\t    \".globl \" PV_THUNK_NAME(func) \";\"\t\t\t\t\\\n\t    \".type \" PV_THUNK_NAME(func) \", @function;\"\t\t\t\\\n\t    PV_THUNK_NAME(func) \":\"\t\t\t\t\t\\\n\t    FRAME_BEGIN\t\t\t\t\t\t\t\\\n\t    PV_SAVE_ALL_CALLER_REGS\t\t\t\t\t\\\n\t    \"call \" #func \";\"\t\t\t\t\t\t\\\n\t    PV_RESTORE_ALL_CALLER_REGS\t\t\t\t\t\\\n\t    FRAME_END\t\t\t\t\t\t\t\\\n\t    \"ret;\"\t\t\t\t\t\t\t\\\n\t    \".size \" PV_THUNK_NAME(func) \", .-\" PV_THUNK_NAME(func) \";\"\t\\\n\t    \".popsection\")\n\n/* Get a reference to a callee-save function */\n#define PV_CALLEE_SAVE(func)\t\t\t\t\t\t\\\n\t((struct paravirt_callee_save) { __raw_callee_save_##func })\n\n/* Promise that \"func\" already uses the right calling convention */\n#define __PV_IS_CALLEE_SAVE(func)\t\t\t\\\n\t((struct paravirt_callee_save) { func })\n\n#ifdef CONFIG_PARAVIRT_XXL\nstatic inline notrace unsigned long arch_local_save_flags(void)\n{\n\treturn PVOP_CALLEE0(unsigned long, irq.save_fl);\n}\n\nstatic inline notrace void arch_local_irq_restore(unsigned long f)\n{\n\tPVOP_VCALLEE1(irq.restore_fl, f);\n}\n\nstatic inline notrace void arch_local_irq_disable(void)\n{\n\tPVOP_VCALLEE0(irq.irq_disable);\n}\n\nstatic inline notrace void arch_local_irq_enable(void)\n{\n\tPVOP_VCALLEE0(irq.irq_enable);\n}\n\nstatic inline notrace unsigned long arch_local_irq_save(void)\n{\n\tunsigned long f;\n\n\tf = arch_local_save_flags();\n\tarch_local_irq_disable();\n\treturn f;\n}\n#endif\n\n\n/* Make sure as little as possible of this mess escapes. */\n#undef PARAVIRT_CALL\n#undef __PVOP_CALL\n#undef __PVOP_VCALL\n#undef PVOP_VCALL0\n#undef PVOP_CALL0\n#undef PVOP_VCALL1\n#undef PVOP_CALL1\n#undef PVOP_VCALL2\n#undef PVOP_CALL2\n#undef PVOP_VCALL3\n#undef PVOP_CALL3\n#undef PVOP_VCALL4\n#undef PVOP_CALL4\n\nextern void default_banner(void);\n\n#else  /* __ASSEMBLY__ */\n\n#define _PVSITE(ptype, ops, word, algn)\t\t\\\n771:;\t\t\t\t\t\t\\\n\tops;\t\t\t\t\t\\\n772:;\t\t\t\t\t\t\\\n\t.pushsection .parainstructions,\"a\";\t\\\n\t .align\talgn;\t\t\t\t\\\n\t word 771b;\t\t\t\t\\\n\t .byte ptype;\t\t\t\t\\\n\t .byte 772b-771b;\t\t\t\\\n\t.popsection\n\n\n#define COND_PUSH(set, mask, reg)\t\t\t\\\n\t.if ((~(set)) & mask); push %reg; .endif\n#define COND_POP(set, mask, reg)\t\t\t\\\n\t.if ((~(set)) & mask); pop %reg; .endif\n\n#ifdef CONFIG_X86_64\n\n#define PV_SAVE_REGS(set)\t\t\t\\\n\tCOND_PUSH(set, CLBR_RAX, rax);\t\t\\\n\tCOND_PUSH(set, CLBR_RCX, rcx);\t\t\\\n\tCOND_PUSH(set, CLBR_RDX, rdx);\t\t\\\n\tCOND_PUSH(set, CLBR_RSI, rsi);\t\t\\\n\tCOND_PUSH(set, CLBR_RDI, rdi);\t\t\\\n\tCOND_PUSH(set, CLBR_R8, r8);\t\t\\\n\tCOND_PUSH(set, CLBR_R9, r9);\t\t\\\n\tCOND_PUSH(set, CLBR_R10, r10);\t\t\\\n\tCOND_PUSH(set, CLBR_R11, r11)\n#define PV_RESTORE_REGS(set)\t\t\t\\\n\tCOND_POP(set, CLBR_R11, r11);\t\t\\\n\tCOND_POP(set, CLBR_R10, r10);\t\t\\\n\tCOND_POP(set, CLBR_R9, r9);\t\t\\\n\tCOND_POP(set, CLBR_R8, r8);\t\t\\\n\tCOND_POP(set, CLBR_RDI, rdi);\t\t\\\n\tCOND_POP(set, CLBR_RSI, rsi);\t\t\\\n\tCOND_POP(set, CLBR_RDX, rdx);\t\t\\\n\tCOND_POP(set, CLBR_RCX, rcx);\t\t\\\n\tCOND_POP(set, CLBR_RAX, rax)\n\n#define PARA_PATCH(off)\t\t((off) / 8)\n#define PARA_SITE(ptype, ops)\t_PVSITE(ptype, ops, .quad, 8)\n#define PARA_INDIRECT(addr)\t*addr(%rip)\n#else\n#define PV_SAVE_REGS(set)\t\t\t\\\n\tCOND_PUSH(set, CLBR_EAX, eax);\t\t\\\n\tCOND_PUSH(set, CLBR_EDI, edi);\t\t\\\n\tCOND_PUSH(set, CLBR_ECX, ecx);\t\t\\\n\tCOND_PUSH(set, CLBR_EDX, edx)\n#define PV_RESTORE_REGS(set)\t\t\t\\\n\tCOND_POP(set, CLBR_EDX, edx);\t\t\\\n\tCOND_POP(set, CLBR_ECX, ecx);\t\t\\\n\tCOND_POP(set, CLBR_EDI, edi);\t\t\\\n\tCOND_POP(set, CLBR_EAX, eax)\n\n#define PARA_PATCH(off)\t\t((off) / 4)\n#define PARA_SITE(ptype, ops)\t_PVSITE(ptype, ops, .long, 4)\n#define PARA_INDIRECT(addr)\t*%cs:addr\n#endif\n\n#ifdef CONFIG_PARAVIRT_XXL\n#define INTERRUPT_RETURN\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_CPU_iret),\t\t\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  jmp PARA_INDIRECT(pv_ops+PV_CPU_iret);)\n\n#define DISABLE_INTERRUPTS(clobbers)\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_IRQ_irq_disable),\t\t\t\\\n\t\t  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  call PARA_INDIRECT(pv_ops+PV_IRQ_irq_disable);\t\\\n\t\t  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)\n\n#define ENABLE_INTERRUPTS(clobbers)\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_IRQ_irq_enable),\t\t\t\\\n\t\t  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  call PARA_INDIRECT(pv_ops+PV_IRQ_irq_enable);\t\t\\\n\t\t  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)\n#endif\n\n#ifdef CONFIG_X86_64\n#ifdef CONFIG_PARAVIRT_XXL\n/*\n * If swapgs is used while the userspace stack is still current,\n * there's no way to call a pvop.  The PV replacement *must* be\n * inlined, or the swapgs instruction must be trapped and emulated.\n */\n#define SWAPGS_UNSAFE_STACK\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_CPU_swapgs), swapgs)\n\n/*\n * Note: swapgs is very special, and in practise is either going to be\n * implemented with a single \"swapgs\" instruction or something very\n * special.  Either way, we don't need to save any registers for\n * it.\n */\n#define SWAPGS\t\t\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_CPU_swapgs),\t\t\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  call PARA_INDIRECT(pv_ops+PV_CPU_swapgs);\t\t\\\n\t\t )\n\n#define USERGS_SYSRET64\t\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_CPU_usergs_sysret64),\t\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  jmp PARA_INDIRECT(pv_ops+PV_CPU_usergs_sysret64);)\n\n#ifdef CONFIG_DEBUG_ENTRY\n#define SAVE_FLAGS(clobbers)                                        \\\n\tPARA_SITE(PARA_PATCH(PV_IRQ_save_fl),\t\t\t    \\\n\t\t  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);        \\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t    \\\n\t\t  call PARA_INDIRECT(pv_ops+PV_IRQ_save_fl);\t    \\\n\t\t  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)\n#endif\n#endif /* CONFIG_PARAVIRT_XXL */\n#endif\t/* CONFIG_X86_64 */\n\n#ifdef CONFIG_PARAVIRT_XXL\n\n#define GET_CR2_INTO_AX\t\t\t\t\t\t\t\\\n\tPARA_SITE(PARA_PATCH(PV_MMU_read_cr2),\t\t\t\t\\\n\t\t  ANNOTATE_RETPOLINE_SAFE;\t\t\t\t\\\n\t\t  call PARA_INDIRECT(pv_ops+PV_MMU_read_cr2);\t\t\\\n\t\t )\n\n#endif /* CONFIG_PARAVIRT_XXL */\n\n\n#endif /* __ASSEMBLY__ */\n#else  /* CONFIG_PARAVIRT */\n# define default_banner x86_init_noop\n#endif /* !CONFIG_PARAVIRT */\n\n#ifndef __ASSEMBLY__\n#ifndef CONFIG_PARAVIRT_XXL\nstatic inline void paravirt_arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t\t  struct mm_struct *mm)\n{\n}\n#endif\n\n#ifndef CONFIG_PARAVIRT\nstatic inline void paravirt_arch_exit_mmap(struct mm_struct *mm)\n{\n}\n#endif\n#endif /* __ASSEMBLY__ */\n#endif /* _ASM_X86_PARAVIRT_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_PARAVIRT_TYPES_H\n#define _ASM_X86_PARAVIRT_TYPES_H\n\n/* Bitmask of what can be clobbered: usually at least eax. */\n#define CLBR_NONE 0\n#define CLBR_EAX  (1 << 0)\n#define CLBR_ECX  (1 << 1)\n#define CLBR_EDX  (1 << 2)\n#define CLBR_EDI  (1 << 3)\n\n#ifdef CONFIG_X86_32\n/* CLBR_ANY should match all regs platform has. For i386, that's just it */\n#define CLBR_ANY  ((1 << 4) - 1)\n\n#define CLBR_ARG_REGS\t(CLBR_EAX | CLBR_EDX | CLBR_ECX)\n#define CLBR_RET_REG\t(CLBR_EAX | CLBR_EDX)\n#define CLBR_SCRATCH\t(0)\n#else\n#define CLBR_RAX  CLBR_EAX\n#define CLBR_RCX  CLBR_ECX\n#define CLBR_RDX  CLBR_EDX\n#define CLBR_RDI  CLBR_EDI\n#define CLBR_RSI  (1 << 4)\n#define CLBR_R8   (1 << 5)\n#define CLBR_R9   (1 << 6)\n#define CLBR_R10  (1 << 7)\n#define CLBR_R11  (1 << 8)\n\n#define CLBR_ANY  ((1 << 9) - 1)\n\n#define CLBR_ARG_REGS\t(CLBR_RDI | CLBR_RSI | CLBR_RDX | \\\n\t\t\t CLBR_RCX | CLBR_R8 | CLBR_R9)\n#define CLBR_RET_REG\t(CLBR_RAX)\n#define CLBR_SCRATCH\t(CLBR_R10 | CLBR_R11)\n\n#endif /* X86_64 */\n\n#define CLBR_CALLEE_SAVE ((CLBR_ARG_REGS | CLBR_SCRATCH) & ~CLBR_RET_REG)\n\n#ifndef __ASSEMBLY__\n\n#include <asm/desc_defs.h>\n#include <asm/kmap_types.h>\n#include <asm/pgtable_types.h>\n#include <asm/nospec-branch.h>\n\nstruct page;\nstruct thread_struct;\nstruct desc_ptr;\nstruct tss_struct;\nstruct mm_struct;\nstruct desc_struct;\nstruct task_struct;\nstruct cpumask;\nstruct flush_tlb_info;\nstruct mmu_gather;\nstruct vm_area_struct;\n\n/*\n * Wrapper type for pointers to code which uses the non-standard\n * calling convention.  See PV_CALL_SAVE_REGS_THUNK below.\n */\nstruct paravirt_callee_save {\n\tvoid *func;\n};\n\n/* general info */\nstruct pv_info {\n#ifdef CONFIG_PARAVIRT_XXL\n\tunsigned int kernel_rpl;\n\tint shared_kernel_pmd;\n\n#ifdef CONFIG_X86_64\n\tu16 extra_user_64bit_cs;  /* __USER_CS if none */\n#endif\n#endif\n\n\tconst char *name;\n};\n\nstruct pv_init_ops {\n\t/*\n\t * Patch may replace one of the defined code sequences with\n\t * arbitrary code, subject to the same register constraints.\n\t * This generally means the code is not free to clobber any\n\t * registers other than EAX.  The patch function should return\n\t * the number of bytes of code generated, as we nop pad the\n\t * rest in generic code.\n\t */\n\tunsigned (*patch)(u8 type, void *insn_buff,\n\t\t\t  unsigned long addr, unsigned len);\n} __no_randomize_layout;\n\n#ifdef CONFIG_PARAVIRT_XXL\nstruct pv_lazy_ops {\n\t/* Set deferred update mode, used for batching operations. */\n\tvoid (*enter)(void);\n\tvoid (*leave)(void);\n\tvoid (*flush)(void);\n} __no_randomize_layout;\n#endif\n\nstruct pv_time_ops {\n\tunsigned long long (*sched_clock)(void);\n\tunsigned long long (*steal_clock)(int cpu);\n} __no_randomize_layout;\n\nstruct pv_cpu_ops {\n\t/* hooks for various privileged instructions */\n\tvoid (*io_delay)(void);\n\n#ifdef CONFIG_PARAVIRT_XXL\n\tunsigned long (*get_debugreg)(int regno);\n\tvoid (*set_debugreg)(int regno, unsigned long value);\n\n\tunsigned long (*read_cr0)(void);\n\tvoid (*write_cr0)(unsigned long);\n\n\tvoid (*write_cr4)(unsigned long);\n\n\t/* Segment descriptor handling */\n\tvoid (*load_tr_desc)(void);\n\tvoid (*load_gdt)(const struct desc_ptr *);\n\tvoid (*load_idt)(const struct desc_ptr *);\n\tvoid (*set_ldt)(const void *desc, unsigned entries);\n\tunsigned long (*store_tr)(void);\n\tvoid (*load_tls)(struct thread_struct *t, unsigned int cpu);\n#ifdef CONFIG_X86_64\n\tvoid (*load_gs_index)(unsigned int idx);\n#endif\n\tvoid (*write_ldt_entry)(struct desc_struct *ldt, int entrynum,\n\t\t\t\tconst void *desc);\n\tvoid (*write_gdt_entry)(struct desc_struct *,\n\t\t\t\tint entrynum, const void *desc, int size);\n\tvoid (*write_idt_entry)(gate_desc *,\n\t\t\t\tint entrynum, const gate_desc *gate);\n\tvoid (*alloc_ldt)(struct desc_struct *ldt, unsigned entries);\n\tvoid (*free_ldt)(struct desc_struct *ldt, unsigned entries);\n\n\tvoid (*load_sp0)(unsigned long sp0);\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n\tvoid (*invalidate_io_bitmap)(void);\n\tvoid (*update_io_bitmap)(void);\n#endif\n\n\tvoid (*wbinvd)(void);\n\n\t/* cpuid emulation, mostly so that caps bits can be disabled */\n\tvoid (*cpuid)(unsigned int *eax, unsigned int *ebx,\n\t\t      unsigned int *ecx, unsigned int *edx);\n\n\t/* Unsafe MSR operations.  These will warn or panic on failure. */\n\tu64 (*read_msr)(unsigned int msr);\n\tvoid (*write_msr)(unsigned int msr, unsigned low, unsigned high);\n\n\t/*\n\t * Safe MSR operations.\n\t * read sets err to 0 or -EIO.  write returns 0 or -EIO.\n\t */\n\tu64 (*read_msr_safe)(unsigned int msr, int *err);\n\tint (*write_msr_safe)(unsigned int msr, unsigned low, unsigned high);\n\n\tu64 (*read_pmc)(int counter);\n\n\t/*\n\t * Switch to usermode gs and return to 64-bit usermode using\n\t * sysret.  Only used in 64-bit kernels to return to 64-bit\n\t * processes.  Usermode register state, including %rsp, must\n\t * already be restored.\n\t */\n\tvoid (*usergs_sysret64)(void);\n\n\t/* Normal iret.  Jump to this with the standard iret stack\n\t   frame set up. */\n\tvoid (*iret)(void);\n\n\tvoid (*swapgs)(void);\n\n\tvoid (*start_context_switch)(struct task_struct *prev);\n\tvoid (*end_context_switch)(struct task_struct *next);\n#endif\n} __no_randomize_layout;\n\nstruct pv_irq_ops {\n#ifdef CONFIG_PARAVIRT_XXL\n\t/*\n\t * Get/set interrupt state.  save_fl and restore_fl are only\n\t * expected to use X86_EFLAGS_IF; all other bits\n\t * returned from save_fl are undefined, and may be ignored by\n\t * restore_fl.\n\t *\n\t * NOTE: These functions callers expect the callee to preserve\n\t * more registers than the standard C calling convention.\n\t */\n\tstruct paravirt_callee_save save_fl;\n\tstruct paravirt_callee_save restore_fl;\n\tstruct paravirt_callee_save irq_disable;\n\tstruct paravirt_callee_save irq_enable;\n\n\tvoid (*safe_halt)(void);\n\tvoid (*halt)(void);\n#endif\n} __no_randomize_layout;\n\nstruct pv_mmu_ops {\n\t/* TLB operations */\n\tvoid (*flush_tlb_user)(void);\n\tvoid (*flush_tlb_kernel)(void);\n\tvoid (*flush_tlb_one_user)(unsigned long addr);\n\tvoid (*flush_tlb_others)(const struct cpumask *cpus,\n\t\t\t\t const struct flush_tlb_info *info);\n\n\tvoid (*tlb_remove_table)(struct mmu_gather *tlb, void *table);\n\n\t/* Hook for intercepting the destruction of an mm_struct. */\n\tvoid (*exit_mmap)(struct mm_struct *mm);\n\n#ifdef CONFIG_PARAVIRT_XXL\n\tstruct paravirt_callee_save read_cr2;\n\tvoid (*write_cr2)(unsigned long);\n\n\tunsigned long (*read_cr3)(void);\n\tvoid (*write_cr3)(unsigned long);\n\n\t/* Hooks for intercepting the creation/use of an mm_struct. */\n\tvoid (*activate_mm)(struct mm_struct *prev,\n\t\t\t    struct mm_struct *next);\n\tvoid (*dup_mmap)(struct mm_struct *oldmm,\n\t\t\t struct mm_struct *mm);\n\n\t/* Hooks for allocating and freeing a pagetable top-level */\n\tint  (*pgd_alloc)(struct mm_struct *mm);\n\tvoid (*pgd_free)(struct mm_struct *mm, pgd_t *pgd);\n\n\t/*\n\t * Hooks for allocating/releasing pagetable pages when they're\n\t * attached to a pagetable\n\t */\n\tvoid (*alloc_pte)(struct mm_struct *mm, unsigned long pfn);\n\tvoid (*alloc_pmd)(struct mm_struct *mm, unsigned long pfn);\n\tvoid (*alloc_pud)(struct mm_struct *mm, unsigned long pfn);\n\tvoid (*alloc_p4d)(struct mm_struct *mm, unsigned long pfn);\n\tvoid (*release_pte)(unsigned long pfn);\n\tvoid (*release_pmd)(unsigned long pfn);\n\tvoid (*release_pud)(unsigned long pfn);\n\tvoid (*release_p4d)(unsigned long pfn);\n\n\t/* Pagetable manipulation functions */\n\tvoid (*set_pte)(pte_t *ptep, pte_t pteval);\n\tvoid (*set_pte_at)(struct mm_struct *mm, unsigned long addr,\n\t\t\t   pte_t *ptep, pte_t pteval);\n\tvoid (*set_pmd)(pmd_t *pmdp, pmd_t pmdval);\n\n\tpte_t (*ptep_modify_prot_start)(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\tpte_t *ptep);\n\tvoid (*ptep_modify_prot_commit)(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\tpte_t *ptep, pte_t pte);\n\n\tstruct paravirt_callee_save pte_val;\n\tstruct paravirt_callee_save make_pte;\n\n\tstruct paravirt_callee_save pgd_val;\n\tstruct paravirt_callee_save make_pgd;\n\n#if CONFIG_PGTABLE_LEVELS >= 3\n#ifdef CONFIG_X86_PAE\n\tvoid (*set_pte_atomic)(pte_t *ptep, pte_t pteval);\n\tvoid (*pte_clear)(struct mm_struct *mm, unsigned long addr,\n\t\t\t  pte_t *ptep);\n\tvoid (*pmd_clear)(pmd_t *pmdp);\n\n#endif\t/* CONFIG_X86_PAE */\n\n\tvoid (*set_pud)(pud_t *pudp, pud_t pudval);\n\n\tstruct paravirt_callee_save pmd_val;\n\tstruct paravirt_callee_save make_pmd;\n\n#if CONFIG_PGTABLE_LEVELS >= 4\n\tstruct paravirt_callee_save pud_val;\n\tstruct paravirt_callee_save make_pud;\n\n\tvoid (*set_p4d)(p4d_t *p4dp, p4d_t p4dval);\n\n#if CONFIG_PGTABLE_LEVELS >= 5\n\tstruct paravirt_callee_save p4d_val;\n\tstruct paravirt_callee_save make_p4d;\n\n\tvoid (*set_pgd)(pgd_t *pgdp, pgd_t pgdval);\n#endif\t/* CONFIG_PGTABLE_LEVELS >= 5 */\n\n#endif\t/* CONFIG_PGTABLE_LEVELS >= 4 */\n\n#endif\t/* CONFIG_PGTABLE_LEVELS >= 3 */\n\n\tstruct pv_lazy_ops lazy_mode;\n\n\t/* dom0 ops */\n\n\t/* Sometimes the physical address is a pfn, and sometimes its\n\t   an mfn.  We can tell which is which from the index. */\n\tvoid (*set_fixmap)(unsigned /* enum fixed_addresses */ idx,\n\t\t\t   phys_addr_t phys, pgprot_t flags);\n#endif\n} __no_randomize_layout;\n\nstruct arch_spinlock;\n#ifdef CONFIG_SMP\n#include <asm/spinlock_types.h>\n#endif\n\nstruct qspinlock;\n\nstruct pv_lock_ops {\n\tvoid (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);\n\tstruct paravirt_callee_save queued_spin_unlock;\n\n\tvoid (*wait)(u8 *ptr, u8 val);\n\tvoid (*kick)(int cpu);\n\n\tstruct paravirt_callee_save vcpu_is_preempted;\n} __no_randomize_layout;\n\n/* This contains all the paravirt structures: we get a convenient\n * number for each function using the offset which we use to indicate\n * what to patch. */\nstruct paravirt_patch_template {\n\tstruct pv_init_ops\tinit;\n\tstruct pv_time_ops\ttime;\n\tstruct pv_cpu_ops\tcpu;\n\tstruct pv_irq_ops\tirq;\n\tstruct pv_mmu_ops\tmmu;\n\tstruct pv_lock_ops\tlock;\n} __no_randomize_layout;\n\nextern struct pv_info pv_info;\nextern struct paravirt_patch_template pv_ops;\n\n#define PARAVIRT_PATCH(x)\t\t\t\t\t\\\n\t(offsetof(struct paravirt_patch_template, x) / sizeof(void *))\n\n#define paravirt_type(op)\t\t\t\t\\\n\t[paravirt_typenum] \"i\" (PARAVIRT_PATCH(op)),\t\\\n\t[paravirt_opptr] \"i\" (&(pv_ops.op))\n#define paravirt_clobber(clobber)\t\t\\\n\t[paravirt_clobber] \"i\" (clobber)\n\n/*\n * Generate some code, and mark it as patchable by the\n * apply_paravirt() alternate instruction patcher.\n */\n#define _paravirt_alt(insn_string, type, clobber)\t\\\n\t\"771:\\n\\t\" insn_string \"\\n\" \"772:\\n\"\t\t\\\n\t\".pushsection .parainstructions,\\\"a\\\"\\n\"\t\\\n\t_ASM_ALIGN \"\\n\"\t\t\t\t\t\\\n\t_ASM_PTR \" 771b\\n\"\t\t\t\t\\\n\t\"  .byte \" type \"\\n\"\t\t\t\t\\\n\t\"  .byte 772b-771b\\n\"\t\t\t\t\\\n\t\"  .short \" clobber \"\\n\"\t\t\t\\\n\t\".popsection\\n\"\n\n/* Generate patchable code, with the default asm parameters. */\n#define paravirt_alt(insn_string)\t\t\t\t\t\\\n\t_paravirt_alt(insn_string, \"%c[paravirt_typenum]\", \"%c[paravirt_clobber]\")\n\n/* Simple instruction patching code. */\n#define NATIVE_LABEL(a,x,b) \"\\n\\t.globl \" a #x \"_\" #b \"\\n\" a #x \"_\" #b \":\\n\\t\"\n\nunsigned paravirt_patch_ident_64(void *insn_buff, unsigned len);\nunsigned paravirt_patch_default(u8 type, void *insn_buff, unsigned long addr, unsigned len);\nunsigned paravirt_patch_insns(void *insn_buff, unsigned len, const char *start, const char *end);\n\nunsigned native_patch(u8 type, void *insn_buff, unsigned long addr, unsigned len);\n\nint paravirt_disable_iospace(void);\n\n/*\n * This generates an indirect call based on the operation type number.\n * The type number, computed in PARAVIRT_PATCH, is derived from the\n * offset into the paravirt_patch_template structure, and can therefore be\n * freely converted back into a structure offset.\n */\n#define PARAVIRT_CALL\t\t\t\t\t\\\n\tANNOTATE_RETPOLINE_SAFE\t\t\t\t\\\n\t\"call *%c[paravirt_opptr];\"\n\n/*\n * These macros are intended to wrap calls through one of the paravirt\n * ops structs, so that they can be later identified and patched at\n * runtime.\n *\n * Normally, a call to a pv_op function is a simple indirect call:\n * (pv_op_struct.operations)(args...).\n *\n * Unfortunately, this is a relatively slow operation for modern CPUs,\n * because it cannot necessarily determine what the destination\n * address is.  In this case, the address is a runtime constant, so at\n * the very least we can patch the call to e a simple direct call, or\n * ideally, patch an inline implementation into the callsite.  (Direct\n * calls are essentially free, because the call and return addresses\n * are completely predictable.)\n *\n * For i386, these macros rely on the standard gcc \"regparm(3)\" calling\n * convention, in which the first three arguments are placed in %eax,\n * %edx, %ecx (in that order), and the remaining arguments are placed\n * on the stack.  All caller-save registers (eax,edx,ecx) are expected\n * to be modified (either clobbered or used for return values).\n * X86_64, on the other hand, already specifies a register-based calling\n * conventions, returning at %rax, with parameteres going on %rdi, %rsi,\n * %rdx, and %rcx. Note that for this reason, x86_64 does not need any\n * special handling for dealing with 4 arguments, unlike i386.\n * However, x86_64 also have to clobber all caller saved registers, which\n * unfortunately, are quite a bit (r8 - r11)\n *\n * The call instruction itself is marked by placing its start address\n * and size into the .parainstructions section, so that\n * apply_paravirt() in arch/i386/kernel/alternative.c can do the\n * appropriate patching under the control of the backend pv_init_ops\n * implementation.\n *\n * Unfortunately there's no way to get gcc to generate the args setup\n * for the call, and then allow the call itself to be generated by an\n * inline asm.  Because of this, we must do the complete arg setup and\n * return value handling from within these macros.  This is fairly\n * cumbersome.\n *\n * There are 5 sets of PVOP_* macros for dealing with 0-4 arguments.\n * It could be extended to more arguments, but there would be little\n * to be gained from that.  For each number of arguments, there are\n * the two VCALL and CALL variants for void and non-void functions.\n *\n * When there is a return value, the invoker of the macro must specify\n * the return type.  The macro then uses sizeof() on that type to\n * determine whether its a 32 or 64 bit value, and places the return\n * in the right register(s) (just %eax for 32-bit, and %edx:%eax for\n * 64-bit). For x86_64 machines, it just returns at %rax regardless of\n * the return value size.\n *\n * 64-bit arguments are passed as a pair of adjacent 32-bit arguments\n * i386 also passes 64-bit arguments as a pair of adjacent 32-bit arguments\n * in low,high order\n *\n * Small structures are passed and returned in registers.  The macro\n * calling convention can't directly deal with this, so the wrapper\n * functions must do this.\n *\n * These PVOP_* macros are only defined within this header.  This\n * means that all uses must be wrapped in inline functions.  This also\n * makes sure the incoming and outgoing types are always correct.\n */\n#ifdef CONFIG_X86_32\n#define PVOP_VCALL_ARGS\t\t\t\t\t\t\t\\\n\tunsigned long __eax = __eax, __edx = __edx, __ecx = __ecx;\n\n#define PVOP_CALL_ARGS\t\t\tPVOP_VCALL_ARGS\n\n#define PVOP_CALL_ARG1(x)\t\t\"a\" ((unsigned long)(x))\n#define PVOP_CALL_ARG2(x)\t\t\"d\" ((unsigned long)(x))\n#define PVOP_CALL_ARG3(x)\t\t\"c\" ((unsigned long)(x))\n\n#define PVOP_VCALL_CLOBBERS\t\t\"=a\" (__eax), \"=d\" (__edx),\t\\\n\t\t\t\t\t\"=c\" (__ecx)\n#define PVOP_CALL_CLOBBERS\t\tPVOP_VCALL_CLOBBERS\n\n#define PVOP_VCALLEE_CLOBBERS\t\t\"=a\" (__eax), \"=d\" (__edx)\n#define PVOP_CALLEE_CLOBBERS\t\tPVOP_VCALLEE_CLOBBERS\n\n#define EXTRA_CLOBBERS\n#define VEXTRA_CLOBBERS\n#else  /* CONFIG_X86_64 */\n/* [re]ax isn't an arg, but the return val */\n#define PVOP_VCALL_ARGS\t\t\t\t\t\t\\\n\tunsigned long __edi = __edi, __esi = __esi,\t\t\\\n\t\t__edx = __edx, __ecx = __ecx, __eax = __eax;\n\n#define PVOP_CALL_ARGS\t\tPVOP_VCALL_ARGS\n\n#define PVOP_CALL_ARG1(x)\t\t\"D\" ((unsigned long)(x))\n#define PVOP_CALL_ARG2(x)\t\t\"S\" ((unsigned long)(x))\n#define PVOP_CALL_ARG3(x)\t\t\"d\" ((unsigned long)(x))\n#define PVOP_CALL_ARG4(x)\t\t\"c\" ((unsigned long)(x))\n\n#define PVOP_VCALL_CLOBBERS\t\"=D\" (__edi),\t\t\t\t\\\n\t\t\t\t\"=S\" (__esi), \"=d\" (__edx),\t\t\\\n\t\t\t\t\"=c\" (__ecx)\n#define PVOP_CALL_CLOBBERS\tPVOP_VCALL_CLOBBERS, \"=a\" (__eax)\n\n/* void functions are still allowed [re]ax for scratch */\n#define PVOP_VCALLEE_CLOBBERS\t\"=a\" (__eax)\n#define PVOP_CALLEE_CLOBBERS\tPVOP_VCALLEE_CLOBBERS\n\n#define EXTRA_CLOBBERS\t , \"r8\", \"r9\", \"r10\", \"r11\"\n#define VEXTRA_CLOBBERS\t , \"rax\", \"r8\", \"r9\", \"r10\", \"r11\"\n#endif\t/* CONFIG_X86_32 */\n\n#ifdef CONFIG_PARAVIRT_DEBUG\n#define PVOP_TEST_NULL(op)\tBUG_ON(pv_ops.op == NULL)\n#else\n#define PVOP_TEST_NULL(op)\t((void)pv_ops.op)\n#endif\n\n#define PVOP_RETMASK(rettype)\t\t\t\t\t\t\\\n\t({\tunsigned long __mask = ~0UL;\t\t\t\t\\\n\t\tswitch (sizeof(rettype)) {\t\t\t\t\\\n\t\tcase 1: __mask =       0xffUL; break;\t\t\t\\\n\t\tcase 2: __mask =     0xffffUL; break;\t\t\t\\\n\t\tcase 4: __mask = 0xffffffffUL; break;\t\t\t\\\n\t\tdefault: break;\t\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t__mask;\t\t\t\t\t\t\t\\\n\t})\n\n\n#define ____PVOP_CALL(rettype, op, clbr, call_clbr, extra_clbr,\t\t\\\n\t\t      pre, post, ...)\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\trettype __ret;\t\t\t\t\t\t\\\n\t\tPVOP_CALL_ARGS;\t\t\t\t\t\t\\\n\t\tPVOP_TEST_NULL(op);\t\t\t\t\t\\\n\t\t/* This is 32-bit specific, but is okay in 64-bit */\t\\\n\t\t/* since this condition will never hold */\t\t\\\n\t\tif (sizeof(rettype) > sizeof(unsigned long)) {\t\t\\\n\t\t\tasm volatile(pre\t\t\t\t\\\n\t\t\t\t     paravirt_alt(PARAVIRT_CALL)\t\\\n\t\t\t\t     post\t\t\t\t\\\n\t\t\t\t     : call_clbr, ASM_CALL_CONSTRAINT\t\\\n\t\t\t\t     : paravirt_type(op),\t\t\\\n\t\t\t\t       paravirt_clobber(clbr),\t\t\\\n\t\t\t\t       ##__VA_ARGS__\t\t\t\\\n\t\t\t\t     : \"memory\", \"cc\" extra_clbr);\t\\\n\t\t\t__ret = (rettype)((((u64)__edx) << 32) | __eax); \\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\tasm volatile(pre\t\t\t\t\\\n\t\t\t\t     paravirt_alt(PARAVIRT_CALL)\t\\\n\t\t\t\t     post\t\t\t\t\\\n\t\t\t\t     : call_clbr, ASM_CALL_CONSTRAINT\t\\\n\t\t\t\t     : paravirt_type(op),\t\t\\\n\t\t\t\t       paravirt_clobber(clbr),\t\t\\\n\t\t\t\t       ##__VA_ARGS__\t\t\t\\\n\t\t\t\t     : \"memory\", \"cc\" extra_clbr);\t\\\n\t\t\t__ret = (rettype)(__eax & PVOP_RETMASK(rettype));\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t__ret;\t\t\t\t\t\t\t\\\n\t})\n\n#define __PVOP_CALL(rettype, op, pre, post, ...)\t\t\t\\\n\t____PVOP_CALL(rettype, op, CLBR_ANY, PVOP_CALL_CLOBBERS,\t\\\n\t\t      EXTRA_CLOBBERS, pre, post, ##__VA_ARGS__)\n\n#define __PVOP_CALLEESAVE(rettype, op, pre, post, ...)\t\t\t\\\n\t____PVOP_CALL(rettype, op.func, CLBR_RET_REG,\t\t\t\\\n\t\t      PVOP_CALLEE_CLOBBERS, ,\t\t\t\t\\\n\t\t      pre, post, ##__VA_ARGS__)\n\n\n#define ____PVOP_VCALL(op, clbr, call_clbr, extra_clbr, pre, post, ...)\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tPVOP_VCALL_ARGS;\t\t\t\t\t\\\n\t\tPVOP_TEST_NULL(op);\t\t\t\t\t\\\n\t\tasm volatile(pre\t\t\t\t\t\\\n\t\t\t     paravirt_alt(PARAVIRT_CALL)\t\t\\\n\t\t\t     post\t\t\t\t\t\\\n\t\t\t     : call_clbr, ASM_CALL_CONSTRAINT\t\t\\\n\t\t\t     : paravirt_type(op),\t\t\t\\\n\t\t\t       paravirt_clobber(clbr),\t\t\t\\\n\t\t\t       ##__VA_ARGS__\t\t\t\t\\\n\t\t\t     : \"memory\", \"cc\" extra_clbr);\t\t\\\n\t})\n\n#define __PVOP_VCALL(op, pre, post, ...)\t\t\t\t\\\n\t____PVOP_VCALL(op, CLBR_ANY, PVOP_VCALL_CLOBBERS,\t\t\\\n\t\t       VEXTRA_CLOBBERS,\t\t\t\t\t\\\n\t\t       pre, post, ##__VA_ARGS__)\n\n#define __PVOP_VCALLEESAVE(op, pre, post, ...)\t\t\t\t\\\n\t____PVOP_VCALL(op.func, CLBR_RET_REG,\t\t\t\t\\\n\t\t      PVOP_VCALLEE_CLOBBERS, ,\t\t\t\t\\\n\t\t      pre, post, ##__VA_ARGS__)\n\n\n\n#define PVOP_CALL0(rettype, op)\t\t\t\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\")\n#define PVOP_VCALL0(op)\t\t\t\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\")\n\n#define PVOP_CALLEE0(rettype, op)\t\t\t\t\t\\\n\t__PVOP_CALLEESAVE(rettype, op, \"\", \"\")\n#define PVOP_VCALLEE0(op)\t\t\t\t\t\t\\\n\t__PVOP_VCALLEESAVE(op, \"\", \"\")\n\n\n#define PVOP_CALL1(rettype, op, arg1)\t\t\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1))\n#define PVOP_VCALL1(op, arg1)\t\t\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\", PVOP_CALL_ARG1(arg1))\n\n#define PVOP_CALLEE1(rettype, op, arg1)\t\t\t\t\t\\\n\t__PVOP_CALLEESAVE(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1))\n#define PVOP_VCALLEE1(op, arg1)\t\t\t\t\t\t\\\n\t__PVOP_VCALLEESAVE(op, \"\", \"\", PVOP_CALL_ARG1(arg1))\n\n\n#define PVOP_CALL2(rettype, op, arg1, arg2)\t\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\\\n\t\t    PVOP_CALL_ARG2(arg2))\n#define PVOP_VCALL2(op, arg1, arg2)\t\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\t\\\n\t\t     PVOP_CALL_ARG2(arg2))\n\n#define PVOP_CALLEE2(rettype, op, arg1, arg2)\t\t\t\t\\\n\t__PVOP_CALLEESAVE(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\\\n\t\t\t  PVOP_CALL_ARG2(arg2))\n#define PVOP_VCALLEE2(op, arg1, arg2)\t\t\t\t\t\\\n\t__PVOP_VCALLEESAVE(op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\\\n\t\t\t   PVOP_CALL_ARG2(arg2))\n\n\n#define PVOP_CALL3(rettype, op, arg1, arg2, arg3)\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\\\n\t\t    PVOP_CALL_ARG2(arg2), PVOP_CALL_ARG3(arg3))\n#define PVOP_VCALL3(op, arg1, arg2, arg3)\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\", PVOP_CALL_ARG1(arg1),\t\t\t\\\n\t\t     PVOP_CALL_ARG2(arg2), PVOP_CALL_ARG3(arg3))\n\n/* This is the only difference in x86_64. We can make it much simpler */\n#ifdef CONFIG_X86_32\n#define PVOP_CALL4(rettype, op, arg1, arg2, arg3, arg4)\t\t\t\\\n\t__PVOP_CALL(rettype, op,\t\t\t\t\t\\\n\t\t    \"push %[_arg4];\", \"lea 4(%%esp),%%esp;\",\t\t\\\n\t\t    PVOP_CALL_ARG1(arg1), PVOP_CALL_ARG2(arg2),\t\t\\\n\t\t    PVOP_CALL_ARG3(arg3), [_arg4] \"mr\" ((u32)(arg4)))\n#define PVOP_VCALL4(op, arg1, arg2, arg3, arg4)\t\t\t\t\\\n\t__PVOP_VCALL(op,\t\t\t\t\t\t\\\n\t\t    \"push %[_arg4];\", \"lea 4(%%esp),%%esp;\",\t\t\\\n\t\t    \"0\" ((u32)(arg1)), \"1\" ((u32)(arg2)),\t\t\\\n\t\t    \"2\" ((u32)(arg3)), [_arg4] \"mr\" ((u32)(arg4)))\n#else\n#define PVOP_CALL4(rettype, op, arg1, arg2, arg3, arg4)\t\t\t\\\n\t__PVOP_CALL(rettype, op, \"\", \"\",\t\t\t\t\\\n\t\t    PVOP_CALL_ARG1(arg1), PVOP_CALL_ARG2(arg2),\t\t\\\n\t\t    PVOP_CALL_ARG3(arg3), PVOP_CALL_ARG4(arg4))\n#define PVOP_VCALL4(op, arg1, arg2, arg3, arg4)\t\t\t\t\\\n\t__PVOP_VCALL(op, \"\", \"\",\t\t\t\t\t\\\n\t\t     PVOP_CALL_ARG1(arg1), PVOP_CALL_ARG2(arg2),\t\\\n\t\t     PVOP_CALL_ARG3(arg3), PVOP_CALL_ARG4(arg4))\n#endif\n\n/* Lazy mode for batching updates / context switch */\nenum paravirt_lazy_mode {\n\tPARAVIRT_LAZY_NONE,\n\tPARAVIRT_LAZY_MMU,\n\tPARAVIRT_LAZY_CPU,\n};\n\nenum paravirt_lazy_mode paravirt_get_lazy_mode(void);\nvoid paravirt_start_context_switch(struct task_struct *prev);\nvoid paravirt_end_context_switch(struct task_struct *next);\n\nvoid paravirt_enter_lazy_mmu(void);\nvoid paravirt_leave_lazy_mmu(void);\nvoid paravirt_flush_lazy_mmu(void);\n\nvoid _paravirt_nop(void);\nu64 _paravirt_ident_64(u64);\n\n#define paravirt_nop\t((void *)_paravirt_nop)\n\n/* These all sit in the .parainstructions section to tell us what to patch. */\nstruct paravirt_patch_site {\n\tu8 *instr;\t\t/* original instructions */\n\tu8 type;\t\t/* type of this instruction */\n\tu8 len;\t\t\t/* length of original instruction */\n};\n\nextern struct paravirt_patch_site __parainstructions[],\n\t__parainstructions_end[];\n\n#endif\t/* __ASSEMBLY__ */\n\n#endif\t/* _ASM_X86_PARAVIRT_TYPES_H */\n", "// SPDX-License-Identifier: GPL-2.0-or-later\n/*  Paravirtualization interfaces\n    Copyright (C) 2006 Rusty Russell IBM Corporation\n\n\n    2007 - x86_64 support added by Glauber de Oliveira Costa, Red Hat Inc\n*/\n\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/efi.h>\n#include <linux/bcd.h>\n#include <linux/highmem.h>\n#include <linux/kprobes.h>\n#include <linux/pgtable.h>\n\n#include <asm/bug.h>\n#include <asm/paravirt.h>\n#include <asm/debugreg.h>\n#include <asm/desc.h>\n#include <asm/setup.h>\n#include <asm/time.h>\n#include <asm/pgalloc.h>\n#include <asm/irq.h>\n#include <asm/delay.h>\n#include <asm/fixmap.h>\n#include <asm/apic.h>\n#include <asm/tlbflush.h>\n#include <asm/timer.h>\n#include <asm/special_insns.h>\n#include <asm/tlb.h>\n#include <asm/io_bitmap.h>\n\n/*\n * nop stub, which must not clobber anything *including the stack* to\n * avoid confusing the entry prologues.\n */\nextern void _paravirt_nop(void);\nasm (\".pushsection .entry.text, \\\"ax\\\"\\n\"\n     \".global _paravirt_nop\\n\"\n     \"_paravirt_nop:\\n\\t\"\n     \"ret\\n\\t\"\n     \".size _paravirt_nop, . - _paravirt_nop\\n\\t\"\n     \".type _paravirt_nop, @function\\n\\t\"\n     \".popsection\");\n\nvoid __init default_banner(void)\n{\n\tprintk(KERN_INFO \"Booting paravirtualized kernel on %s\\n\",\n\t       pv_info.name);\n}\n\n/* Undefined instruction for dealing with missing ops pointers. */\nstatic const unsigned char ud2a[] = { 0x0f, 0x0b };\n\nstruct branch {\n\tunsigned char opcode;\n\tu32 delta;\n} __attribute__((packed));\n\nstatic unsigned paravirt_patch_call(void *insn_buff, const void *target,\n\t\t\t\t    unsigned long addr, unsigned len)\n{\n\tconst int call_len = 5;\n\tstruct branch *b = insn_buff;\n\tunsigned long delta = (unsigned long)target - (addr+call_len);\n\n\tif (len < call_len) {\n\t\tpr_warn(\"paravirt: Failed to patch indirect CALL at %ps\\n\", (void *)addr);\n\t\t/* Kernel might not be viable if patching fails, bail out: */\n\t\tBUG_ON(1);\n\t}\n\n\tb->opcode = 0xe8; /* call */\n\tb->delta = delta;\n\tBUILD_BUG_ON(sizeof(*b) != call_len);\n\n\treturn call_len;\n}\n\n#ifdef CONFIG_PARAVIRT_XXL\n/* identity function, which can be inlined */\nu64 notrace _paravirt_ident_64(u64 x)\n{\n\treturn x;\n}\n\nstatic unsigned paravirt_patch_jmp(void *insn_buff, const void *target,\n\t\t\t\t   unsigned long addr, unsigned len)\n{\n\tstruct branch *b = insn_buff;\n\tunsigned long delta = (unsigned long)target - (addr+5);\n\n\tif (len < 5) {\n#ifdef CONFIG_RETPOLINE\n\t\tWARN_ONCE(1, \"Failing to patch indirect JMP in %ps\\n\", (void *)addr);\n#endif\n\t\treturn len;\t/* call too long for patch site */\n\t}\n\n\tb->opcode = 0xe9;\t/* jmp */\n\tb->delta = delta;\n\n\treturn 5;\n}\n#endif\n\nDEFINE_STATIC_KEY_TRUE(virt_spin_lock_key);\n\nvoid __init native_pv_lock_init(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\tstatic_branch_disable(&virt_spin_lock_key);\n}\n\nunsigned paravirt_patch_default(u8 type, void *insn_buff,\n\t\t\t\tunsigned long addr, unsigned len)\n{\n\t/*\n\t * Neat trick to map patch type back to the call within the\n\t * corresponding structure.\n\t */\n\tvoid *opfunc = *((void **)&pv_ops + type);\n\tunsigned ret;\n\n\tif (opfunc == NULL)\n\t\t/* If there's no function, patch it with a ud2a (BUG) */\n\t\tret = paravirt_patch_insns(insn_buff, len, ud2a, ud2a+sizeof(ud2a));\n\telse if (opfunc == _paravirt_nop)\n\t\tret = 0;\n\n#ifdef CONFIG_PARAVIRT_XXL\n\t/* identity functions just return their single argument */\n\telse if (opfunc == _paravirt_ident_64)\n\t\tret = paravirt_patch_ident_64(insn_buff, len);\n\n\telse if (type == PARAVIRT_PATCH(cpu.iret) ||\n\t\t type == PARAVIRT_PATCH(cpu.usergs_sysret64))\n\t\t/* If operation requires a jmp, then jmp */\n\t\tret = paravirt_patch_jmp(insn_buff, opfunc, addr, len);\n#endif\n\telse\n\t\t/* Otherwise call the function. */\n\t\tret = paravirt_patch_call(insn_buff, opfunc, addr, len);\n\n\treturn ret;\n}\n\nunsigned paravirt_patch_insns(void *insn_buff, unsigned len,\n\t\t\t      const char *start, const char *end)\n{\n\tunsigned insn_len = end - start;\n\n\t/* Alternative instruction is too large for the patch site and we cannot continue: */\n\tBUG_ON(insn_len > len || start == NULL);\n\n\tmemcpy(insn_buff, start, insn_len);\n\n\treturn insn_len;\n}\n\nstruct static_key paravirt_steal_enabled;\nstruct static_key paravirt_steal_rq_enabled;\n\nstatic u64 native_steal_clock(int cpu)\n{\n\treturn 0;\n}\n\n/* These are in entry.S */\nextern void native_iret(void);\nextern void native_usergs_sysret64(void);\n\nstatic struct resource reserve_ioports = {\n\t.start = 0,\n\t.end = IO_SPACE_LIMIT,\n\t.name = \"paravirt-ioport\",\n\t.flags = IORESOURCE_IO | IORESOURCE_BUSY,\n};\n\n/*\n * Reserve the whole legacy IO space to prevent any legacy drivers\n * from wasting time probing for their hardware.  This is a fairly\n * brute-force approach to disabling all non-virtual drivers.\n *\n * Note that this must be called very early to have any effect.\n */\nint paravirt_disable_iospace(void)\n{\n\treturn request_resource(&ioport_resource, &reserve_ioports);\n}\n\nstatic DEFINE_PER_CPU(enum paravirt_lazy_mode, paravirt_lazy_mode) = PARAVIRT_LAZY_NONE;\n\nstatic inline void enter_lazy(enum paravirt_lazy_mode mode)\n{\n\tBUG_ON(this_cpu_read(paravirt_lazy_mode) != PARAVIRT_LAZY_NONE);\n\n\tthis_cpu_write(paravirt_lazy_mode, mode);\n}\n\nstatic void leave_lazy(enum paravirt_lazy_mode mode)\n{\n\tBUG_ON(this_cpu_read(paravirt_lazy_mode) != mode);\n\n\tthis_cpu_write(paravirt_lazy_mode, PARAVIRT_LAZY_NONE);\n}\n\nvoid paravirt_enter_lazy_mmu(void)\n{\n\tenter_lazy(PARAVIRT_LAZY_MMU);\n}\n\nvoid paravirt_leave_lazy_mmu(void)\n{\n\tleave_lazy(PARAVIRT_LAZY_MMU);\n}\n\nvoid paravirt_flush_lazy_mmu(void)\n{\n\tpreempt_disable();\n\n\tif (paravirt_get_lazy_mode() == PARAVIRT_LAZY_MMU) {\n\t\tarch_leave_lazy_mmu_mode();\n\t\tarch_enter_lazy_mmu_mode();\n\t}\n\n\tpreempt_enable();\n}\n\n#ifdef CONFIG_PARAVIRT_XXL\nvoid paravirt_start_context_switch(struct task_struct *prev)\n{\n\tBUG_ON(preemptible());\n\n\tif (this_cpu_read(paravirt_lazy_mode) == PARAVIRT_LAZY_MMU) {\n\t\tarch_leave_lazy_mmu_mode();\n\t\tset_ti_thread_flag(task_thread_info(prev), TIF_LAZY_MMU_UPDATES);\n\t}\n\tenter_lazy(PARAVIRT_LAZY_CPU);\n}\n\nvoid paravirt_end_context_switch(struct task_struct *next)\n{\n\tBUG_ON(preemptible());\n\n\tleave_lazy(PARAVIRT_LAZY_CPU);\n\n\tif (test_and_clear_ti_thread_flag(task_thread_info(next), TIF_LAZY_MMU_UPDATES))\n\t\tarch_enter_lazy_mmu_mode();\n}\n#endif\n\nenum paravirt_lazy_mode paravirt_get_lazy_mode(void)\n{\n\tif (in_interrupt())\n\t\treturn PARAVIRT_LAZY_NONE;\n\n\treturn this_cpu_read(paravirt_lazy_mode);\n}\n\nstruct pv_info pv_info = {\n\t.name = \"bare hardware\",\n#ifdef CONFIG_PARAVIRT_XXL\n\t.kernel_rpl = 0,\n\t.shared_kernel_pmd = 1,\t/* Only used when CONFIG_X86_PAE is set */\n\n#ifdef CONFIG_X86_64\n\t.extra_user_64bit_cs = __USER_CS,\n#endif\n#endif\n};\n\n/* 64-bit pagetable entries */\n#define PTE_IDENT\t__PV_IS_CALLEE_SAVE(_paravirt_ident_64)\n\nstruct paravirt_patch_template pv_ops = {\n\t/* Init ops. */\n\t.init.patch\t\t= native_patch,\n\n\t/* Time ops. */\n\t.time.sched_clock\t= native_sched_clock,\n\t.time.steal_clock\t= native_steal_clock,\n\n\t/* Cpu ops. */\n\t.cpu.io_delay\t\t= native_io_delay,\n\n#ifdef CONFIG_PARAVIRT_XXL\n\t.cpu.cpuid\t\t= native_cpuid,\n\t.cpu.get_debugreg\t= native_get_debugreg,\n\t.cpu.set_debugreg\t= native_set_debugreg,\n\t.cpu.read_cr0\t\t= native_read_cr0,\n\t.cpu.write_cr0\t\t= native_write_cr0,\n\t.cpu.write_cr4\t\t= native_write_cr4,\n\t.cpu.wbinvd\t\t= native_wbinvd,\n\t.cpu.read_msr\t\t= native_read_msr,\n\t.cpu.write_msr\t\t= native_write_msr,\n\t.cpu.read_msr_safe\t= native_read_msr_safe,\n\t.cpu.write_msr_safe\t= native_write_msr_safe,\n\t.cpu.read_pmc\t\t= native_read_pmc,\n\t.cpu.load_tr_desc\t= native_load_tr_desc,\n\t.cpu.set_ldt\t\t= native_set_ldt,\n\t.cpu.load_gdt\t\t= native_load_gdt,\n\t.cpu.load_idt\t\t= native_load_idt,\n\t.cpu.store_tr\t\t= native_store_tr,\n\t.cpu.load_tls\t\t= native_load_tls,\n#ifdef CONFIG_X86_64\n\t.cpu.load_gs_index\t= native_load_gs_index,\n#endif\n\t.cpu.write_ldt_entry\t= native_write_ldt_entry,\n\t.cpu.write_gdt_entry\t= native_write_gdt_entry,\n\t.cpu.write_idt_entry\t= native_write_idt_entry,\n\n\t.cpu.alloc_ldt\t\t= paravirt_nop,\n\t.cpu.free_ldt\t\t= paravirt_nop,\n\n\t.cpu.load_sp0\t\t= native_load_sp0,\n\n#ifdef CONFIG_X86_64\n\t.cpu.usergs_sysret64\t= native_usergs_sysret64,\n#endif\n\t.cpu.iret\t\t= native_iret,\n\t.cpu.swapgs\t\t= native_swapgs,\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n\t.cpu.invalidate_io_bitmap\t= native_tss_invalidate_io_bitmap,\n\t.cpu.update_io_bitmap\t\t= native_tss_update_io_bitmap,\n#endif\n\n\t.cpu.start_context_switch\t= paravirt_nop,\n\t.cpu.end_context_switch\t\t= paravirt_nop,\n\n\t/* Irq ops. */\n\t.irq.save_fl\t\t= __PV_IS_CALLEE_SAVE(native_save_fl),\n\t.irq.restore_fl\t\t= __PV_IS_CALLEE_SAVE(native_restore_fl),\n\t.irq.irq_disable\t= __PV_IS_CALLEE_SAVE(native_irq_disable),\n\t.irq.irq_enable\t\t= __PV_IS_CALLEE_SAVE(native_irq_enable),\n\t.irq.safe_halt\t\t= native_safe_halt,\n\t.irq.halt\t\t= native_halt,\n#endif /* CONFIG_PARAVIRT_XXL */\n\n\t/* Mmu ops. */\n\t.mmu.flush_tlb_user\t= native_flush_tlb_local,\n\t.mmu.flush_tlb_kernel\t= native_flush_tlb_global,\n\t.mmu.flush_tlb_one_user\t= native_flush_tlb_one_user,\n\t.mmu.flush_tlb_others\t= native_flush_tlb_others,\n\t.mmu.tlb_remove_table\t=\n\t\t\t(void (*)(struct mmu_gather *, void *))tlb_remove_page,\n\n\t.mmu.exit_mmap\t\t= paravirt_nop,\n\n#ifdef CONFIG_PARAVIRT_XXL\n\t.mmu.read_cr2\t\t= __PV_IS_CALLEE_SAVE(native_read_cr2),\n\t.mmu.write_cr2\t\t= native_write_cr2,\n\t.mmu.read_cr3\t\t= __native_read_cr3,\n\t.mmu.write_cr3\t\t= native_write_cr3,\n\n\t.mmu.pgd_alloc\t\t= __paravirt_pgd_alloc,\n\t.mmu.pgd_free\t\t= paravirt_nop,\n\n\t.mmu.alloc_pte\t\t= paravirt_nop,\n\t.mmu.alloc_pmd\t\t= paravirt_nop,\n\t.mmu.alloc_pud\t\t= paravirt_nop,\n\t.mmu.alloc_p4d\t\t= paravirt_nop,\n\t.mmu.release_pte\t= paravirt_nop,\n\t.mmu.release_pmd\t= paravirt_nop,\n\t.mmu.release_pud\t= paravirt_nop,\n\t.mmu.release_p4d\t= paravirt_nop,\n\n\t.mmu.set_pte\t\t= native_set_pte,\n\t.mmu.set_pte_at\t\t= native_set_pte_at,\n\t.mmu.set_pmd\t\t= native_set_pmd,\n\n\t.mmu.ptep_modify_prot_start\t= __ptep_modify_prot_start,\n\t.mmu.ptep_modify_prot_commit\t= __ptep_modify_prot_commit,\n\n#if CONFIG_PGTABLE_LEVELS >= 3\n#ifdef CONFIG_X86_PAE\n\t.mmu.set_pte_atomic\t= native_set_pte_atomic,\n\t.mmu.pte_clear\t\t= native_pte_clear,\n\t.mmu.pmd_clear\t\t= native_pmd_clear,\n#endif\n\t.mmu.set_pud\t\t= native_set_pud,\n\n\t.mmu.pmd_val\t\t= PTE_IDENT,\n\t.mmu.make_pmd\t\t= PTE_IDENT,\n\n#if CONFIG_PGTABLE_LEVELS >= 4\n\t.mmu.pud_val\t\t= PTE_IDENT,\n\t.mmu.make_pud\t\t= PTE_IDENT,\n\n\t.mmu.set_p4d\t\t= native_set_p4d,\n\n#if CONFIG_PGTABLE_LEVELS >= 5\n\t.mmu.p4d_val\t\t= PTE_IDENT,\n\t.mmu.make_p4d\t\t= PTE_IDENT,\n\n\t.mmu.set_pgd\t\t= native_set_pgd,\n#endif /* CONFIG_PGTABLE_LEVELS >= 5 */\n#endif /* CONFIG_PGTABLE_LEVELS >= 4 */\n#endif /* CONFIG_PGTABLE_LEVELS >= 3 */\n\n\t.mmu.pte_val\t\t= PTE_IDENT,\n\t.mmu.pgd_val\t\t= PTE_IDENT,\n\n\t.mmu.make_pte\t\t= PTE_IDENT,\n\t.mmu.make_pgd\t\t= PTE_IDENT,\n\n\t.mmu.dup_mmap\t\t= paravirt_nop,\n\t.mmu.activate_mm\t= paravirt_nop,\n\n\t.mmu.lazy_mode = {\n\t\t.enter\t\t= paravirt_nop,\n\t\t.leave\t\t= paravirt_nop,\n\t\t.flush\t\t= paravirt_nop,\n\t},\n\n\t.mmu.set_fixmap\t\t= native_set_fixmap,\n#endif /* CONFIG_PARAVIRT_XXL */\n\n#if defined(CONFIG_PARAVIRT_SPINLOCKS)\n\t/* Lock ops. */\n#ifdef CONFIG_SMP\n\t.lock.queued_spin_lock_slowpath\t= native_queued_spin_lock_slowpath,\n\t.lock.queued_spin_unlock\t=\n\t\t\t\tPV_CALLEE_SAVE(__native_queued_spin_unlock),\n\t.lock.wait\t\t\t= paravirt_nop,\n\t.lock.kick\t\t\t= paravirt_nop,\n\t.lock.vcpu_is_preempted\t\t=\n\t\t\t\tPV_CALLEE_SAVE(__native_vcpu_is_preempted),\n#endif /* SMP */\n#endif\n};\n\n#ifdef CONFIG_PARAVIRT_XXL\n/* At this point, native_get/set_debugreg has real function entries */\nNOKPROBE_SYMBOL(native_get_debugreg);\nNOKPROBE_SYMBOL(native_set_debugreg);\nNOKPROBE_SYMBOL(native_load_idt);\n#endif\n\nEXPORT_SYMBOL(pv_ops);\nEXPORT_SYMBOL_GPL(pv_info);\n", "// SPDX-License-Identifier: GPL-2.0\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/prctl.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/pm.h>\n#include <linux/tick.h>\n#include <linux/random.h>\n#include <linux/user-return-notifier.h>\n#include <linux/dmi.h>\n#include <linux/utsname.h>\n#include <linux/stackprotector.h>\n#include <linux/cpuidle.h>\n#include <linux/acpi.h>\n#include <linux/elf-randomize.h>\n#include <trace/events/power.h>\n#include <linux/hw_breakpoint.h>\n#include <asm/cpu.h>\n#include <asm/apic.h>\n#include <linux/uaccess.h>\n#include <asm/mwait.h>\n#include <asm/fpu/internal.h>\n#include <asm/debugreg.h>\n#include <asm/nmi.h>\n#include <asm/tlbflush.h>\n#include <asm/mce.h>\n#include <asm/vm86.h>\n#include <asm/switch_to.h>\n#include <asm/desc.h>\n#include <asm/prctl.h>\n#include <asm/spec-ctrl.h>\n#include <asm/io_bitmap.h>\n#include <asm/proto.h>\n\n#include \"process.h\"\n\n/*\n * per-CPU TSS segments. Threads are completely 'soft' on Linux,\n * no more per-task TSS's. The TSS size is kept cacheline-aligned\n * so they are allowed to end up in the .data..cacheline_aligned\n * section. Since TSS's are completely CPU-local, we want them\n * on exact cacheline boundaries, to eliminate cacheline ping-pong.\n */\n__visible DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw) = {\n\t.x86_tss = {\n\t\t/*\n\t\t * .sp0 is only used when entering ring 0 from a lower\n\t\t * privilege level.  Since the init task never runs anything\n\t\t * but ring 0 code, there is no need for a valid value here.\n\t\t * Poison it.\n\t\t */\n\t\t.sp0 = (1UL << (BITS_PER_LONG-1)) + 1,\n\n\t\t/*\n\t\t * .sp1 is cpu_current_top_of_stack.  The init task never\n\t\t * runs user code, but cpu_current_top_of_stack should still\n\t\t * be well defined before the first context switch.\n\t\t */\n\t\t.sp1 = TOP_OF_INIT_STACK,\n\n#ifdef CONFIG_X86_32\n\t\t.ss0 = __KERNEL_DS,\n\t\t.ss1 = __KERNEL_CS,\n#endif\n\t\t.io_bitmap_base\t= IO_BITMAP_OFFSET_INVALID,\n\t },\n};\nEXPORT_PER_CPU_SYMBOL(cpu_tss_rw);\n\nDEFINE_PER_CPU(bool, __tss_limit_invalid);\nEXPORT_PER_CPU_SYMBOL_GPL(__tss_limit_invalid);\n\n/*\n * this gets called so that we can store lazy state into memory and copy the\n * current task into the new thread.\n */\nint arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)\n{\n\tmemcpy(dst, src, arch_task_struct_size);\n#ifdef CONFIG_VM86\n\tdst->thread.vm86 = NULL;\n#endif\n\n\treturn fpu__copy(dst, src);\n}\n\n/*\n * Free thread data structures etc..\n */\nvoid exit_thread(struct task_struct *tsk)\n{\n\tstruct thread_struct *t = &tsk->thread;\n\tstruct fpu *fpu = &t->fpu;\n\n\tif (test_thread_flag(TIF_IO_BITMAP))\n\t\tio_bitmap_exit(tsk);\n\n\tfree_vm86(t);\n\n\tfpu__drop(fpu);\n}\n\nstatic int set_new_tls(struct task_struct *p, unsigned long tls)\n{\n\tstruct user_desc __user *utls = (struct user_desc __user *)tls;\n\n\tif (in_ia32_syscall())\n\t\treturn do_set_thread_area(p, -1, utls, 0);\n\telse\n\t\treturn do_set_thread_area_64(p, ARCH_SET_FS, tls);\n}\n\nint copy_thread_tls(unsigned long clone_flags, unsigned long sp,\n\t\t    unsigned long arg, struct task_struct *p, unsigned long tls)\n{\n\tstruct inactive_task_frame *frame;\n\tstruct fork_frame *fork_frame;\n\tstruct pt_regs *childregs;\n\tint ret = 0;\n\n\tchildregs = task_pt_regs(p);\n\tfork_frame = container_of(childregs, struct fork_frame, regs);\n\tframe = &fork_frame->frame;\n\n\tframe->bp = 0;\n\tframe->ret_addr = (unsigned long) ret_from_fork;\n\tp->thread.sp = (unsigned long) fork_frame;\n\tp->thread.io_bitmap = NULL;\n\tmemset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));\n\n#ifdef CONFIG_X86_64\n\tsavesegment(gs, p->thread.gsindex);\n\tp->thread.gsbase = p->thread.gsindex ? 0 : current->thread.gsbase;\n\tsavesegment(fs, p->thread.fsindex);\n\tp->thread.fsbase = p->thread.fsindex ? 0 : current->thread.fsbase;\n\tsavesegment(es, p->thread.es);\n\tsavesegment(ds, p->thread.ds);\n#else\n\tp->thread.sp0 = (unsigned long) (childregs + 1);\n\t/*\n\t * Clear all status flags including IF and set fixed bit. 64bit\n\t * does not have this initialization as the frame does not contain\n\t * flags. The flags consistency (especially vs. AC) is there\n\t * ensured via objtool, which lacks 32bit support.\n\t */\n\tframe->flags = X86_EFLAGS_FIXED;\n#endif\n\n\t/* Kernel thread ? */\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tkthread_frame_init(frame, sp, arg);\n\t\treturn 0;\n\t}\n\n\tframe->bx = 0;\n\t*childregs = *current_pt_regs();\n\tchildregs->ax = 0;\n\tif (sp)\n\t\tchildregs->sp = sp;\n\n#ifdef CONFIG_X86_32\n\ttask_user_gs(p) = get_user_gs(current_pt_regs());\n#endif\n\n\t/* Set a new TLS for the child thread? */\n\tif (clone_flags & CLONE_SETTLS)\n\t\tret = set_new_tls(p, tls);\n\n\tif (!ret && unlikely(test_tsk_thread_flag(current, TIF_IO_BITMAP)))\n\t\tio_bitmap_share(p);\n\n\treturn ret;\n}\n\nvoid flush_thread(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tflush_ptrace_hw_breakpoint(tsk);\n\tmemset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));\n\n\tfpu__clear_all(&tsk->thread.fpu);\n}\n\nvoid disable_TSC(void)\n{\n\tpreempt_disable();\n\tif (!test_and_set_thread_flag(TIF_NOTSC))\n\t\t/*\n\t\t * Must flip the CPU state synchronously with\n\t\t * TIF_NOTSC in the current running context.\n\t\t */\n\t\tcr4_set_bits(X86_CR4_TSD);\n\tpreempt_enable();\n}\n\nstatic void enable_TSC(void)\n{\n\tpreempt_disable();\n\tif (test_and_clear_thread_flag(TIF_NOTSC))\n\t\t/*\n\t\t * Must flip the CPU state synchronously with\n\t\t * TIF_NOTSC in the current running context.\n\t\t */\n\t\tcr4_clear_bits(X86_CR4_TSD);\n\tpreempt_enable();\n}\n\nint get_tsc_mode(unsigned long adr)\n{\n\tunsigned int val;\n\n\tif (test_thread_flag(TIF_NOTSC))\n\t\tval = PR_TSC_SIGSEGV;\n\telse\n\t\tval = PR_TSC_ENABLE;\n\n\treturn put_user(val, (unsigned int __user *)adr);\n}\n\nint set_tsc_mode(unsigned int val)\n{\n\tif (val == PR_TSC_SIGSEGV)\n\t\tdisable_TSC();\n\telse if (val == PR_TSC_ENABLE)\n\t\tenable_TSC();\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nDEFINE_PER_CPU(u64, msr_misc_features_shadow);\n\nstatic void set_cpuid_faulting(bool on)\n{\n\tu64 msrval;\n\n\tmsrval = this_cpu_read(msr_misc_features_shadow);\n\tmsrval &= ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT;\n\tmsrval |= (on << MSR_MISC_FEATURES_ENABLES_CPUID_FAULT_BIT);\n\tthis_cpu_write(msr_misc_features_shadow, msrval);\n\twrmsrl(MSR_MISC_FEATURES_ENABLES, msrval);\n}\n\nstatic void disable_cpuid(void)\n{\n\tpreempt_disable();\n\tif (!test_and_set_thread_flag(TIF_NOCPUID)) {\n\t\t/*\n\t\t * Must flip the CPU state synchronously with\n\t\t * TIF_NOCPUID in the current running context.\n\t\t */\n\t\tset_cpuid_faulting(true);\n\t}\n\tpreempt_enable();\n}\n\nstatic void enable_cpuid(void)\n{\n\tpreempt_disable();\n\tif (test_and_clear_thread_flag(TIF_NOCPUID)) {\n\t\t/*\n\t\t * Must flip the CPU state synchronously with\n\t\t * TIF_NOCPUID in the current running context.\n\t\t */\n\t\tset_cpuid_faulting(false);\n\t}\n\tpreempt_enable();\n}\n\nstatic int get_cpuid_mode(void)\n{\n\treturn !test_thread_flag(TIF_NOCPUID);\n}\n\nstatic int set_cpuid_mode(struct task_struct *task, unsigned long cpuid_enabled)\n{\n\tif (!boot_cpu_has(X86_FEATURE_CPUID_FAULT))\n\t\treturn -ENODEV;\n\n\tif (cpuid_enabled)\n\t\tenable_cpuid();\n\telse\n\t\tdisable_cpuid();\n\n\treturn 0;\n}\n\n/*\n * Called immediately after a successful exec.\n */\nvoid arch_setup_new_exec(void)\n{\n\t/* If cpuid was previously disabled for this task, re-enable it. */\n\tif (test_thread_flag(TIF_NOCPUID))\n\t\tenable_cpuid();\n\n\t/*\n\t * Don't inherit TIF_SSBD across exec boundary when\n\t * PR_SPEC_DISABLE_NOEXEC is used.\n\t */\n\tif (test_thread_flag(TIF_SSBD) &&\n\t    task_spec_ssb_noexec(current)) {\n\t\tclear_thread_flag(TIF_SSBD);\n\t\ttask_clear_spec_ssb_disable(current);\n\t\ttask_clear_spec_ssb_noexec(current);\n\t\tspeculation_ctrl_update(task_thread_info(current)->flags);\n\t}\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic inline void switch_to_bitmap(unsigned long tifp)\n{\n\t/*\n\t * Invalidate I/O bitmap if the previous task used it. This prevents\n\t * any possible leakage of an active I/O bitmap.\n\t *\n\t * If the next task has an I/O bitmap it will handle it on exit to\n\t * user mode.\n\t */\n\tif (tifp & _TIF_IO_BITMAP)\n\t\ttss_invalidate_io_bitmap();\n}\n\nstatic void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)\n{\n\t/*\n\t * Copy at least the byte range of the incoming tasks bitmap which\n\t * covers the permitted I/O ports.\n\t *\n\t * If the previous task which used an I/O bitmap had more bits\n\t * permitted, then the copy needs to cover those as well so they\n\t * get turned off.\n\t */\n\tmemcpy(tss->io_bitmap.bitmap, iobm->bitmap,\n\t       max(tss->io_bitmap.prev_max, iobm->max));\n\n\t/*\n\t * Store the new max and the sequence number of this bitmap\n\t * and a pointer to the bitmap itself.\n\t */\n\ttss->io_bitmap.prev_max = iobm->max;\n\ttss->io_bitmap.prev_sequence = iobm->sequence;\n}\n\n/**\n * tss_update_io_bitmap - Update I/O bitmap before exiting to usermode\n */\nvoid native_tss_update_io_bitmap(void)\n{\n\tstruct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);\n\tstruct thread_struct *t = &current->thread;\n\tu16 *base = &tss->x86_tss.io_bitmap_base;\n\n\tif (!test_thread_flag(TIF_IO_BITMAP)) {\n\t\tnative_tss_invalidate_io_bitmap();\n\t\treturn;\n\t}\n\n\tif (IS_ENABLED(CONFIG_X86_IOPL_IOPERM) && t->iopl_emul == 3) {\n\t\t*base = IO_BITMAP_OFFSET_VALID_ALL;\n\t} else {\n\t\tstruct io_bitmap *iobm = t->io_bitmap;\n\n\t\t/*\n\t\t * Only copy bitmap data when the sequence number differs. The\n\t\t * update time is accounted to the incoming task.\n\t\t */\n\t\tif (tss->io_bitmap.prev_sequence != iobm->sequence)\n\t\t\ttss_copy_io_bitmap(tss, iobm);\n\n\t\t/* Enable the bitmap */\n\t\t*base = IO_BITMAP_OFFSET_VALID_MAP;\n\t}\n\n\t/*\n\t * Make sure that the TSS limit is covering the IO bitmap. It might have\n\t * been cut down by a VMEXIT to 0x67 which would cause a subsequent I/O\n\t * access from user space to trigger a #GP because tbe bitmap is outside\n\t * the TSS limit.\n\t */\n\trefresh_tss_limit();\n}\n#else /* CONFIG_X86_IOPL_IOPERM */\nstatic inline void switch_to_bitmap(unsigned long tifp) { }\n#endif\n\n#ifdef CONFIG_SMP\n\nstruct ssb_state {\n\tstruct ssb_state\t*shared_state;\n\traw_spinlock_t\t\tlock;\n\tunsigned int\t\tdisable_state;\n\tunsigned long\t\tlocal_state;\n};\n\n#define LSTATE_SSB\t0\n\nstatic DEFINE_PER_CPU(struct ssb_state, ssb_state);\n\nvoid speculative_store_bypass_ht_init(void)\n{\n\tstruct ssb_state *st = this_cpu_ptr(&ssb_state);\n\tunsigned int this_cpu = smp_processor_id();\n\tunsigned int cpu;\n\n\tst->local_state = 0;\n\n\t/*\n\t * Shared state setup happens once on the first bringup\n\t * of the CPU. It's not destroyed on CPU hotunplug.\n\t */\n\tif (st->shared_state)\n\t\treturn;\n\n\traw_spin_lock_init(&st->lock);\n\n\t/*\n\t * Go over HT siblings and check whether one of them has set up the\n\t * shared state pointer already.\n\t */\n\tfor_each_cpu(cpu, topology_sibling_cpumask(this_cpu)) {\n\t\tif (cpu == this_cpu)\n\t\t\tcontinue;\n\n\t\tif (!per_cpu(ssb_state, cpu).shared_state)\n\t\t\tcontinue;\n\n\t\t/* Link it to the state of the sibling: */\n\t\tst->shared_state = per_cpu(ssb_state, cpu).shared_state;\n\t\treturn;\n\t}\n\n\t/*\n\t * First HT sibling to come up on the core.  Link shared state of\n\t * the first HT sibling to itself. The siblings on the same core\n\t * which come up later will see the shared state pointer and link\n\t * themself to the state of this CPU.\n\t */\n\tst->shared_state = st;\n}\n\n/*\n * Logic is: First HT sibling enables SSBD for both siblings in the core\n * and last sibling to disable it, disables it for the whole core. This how\n * MSR_SPEC_CTRL works in \"hardware\":\n *\n *  CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL\n */\nstatic __always_inline void amd_set_core_ssb_state(unsigned long tifn)\n{\n\tstruct ssb_state *st = this_cpu_ptr(&ssb_state);\n\tu64 msr = x86_amd_ls_cfg_base;\n\n\tif (!static_cpu_has(X86_FEATURE_ZEN)) {\n\t\tmsr |= ssbd_tif_to_amd_ls_cfg(tifn);\n\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\treturn;\n\t}\n\n\tif (tifn & _TIF_SSBD) {\n\t\t/*\n\t\t * Since this can race with prctl(), block reentry on the\n\t\t * same CPU.\n\t\t */\n\t\tif (__test_and_set_bit(LSTATE_SSB, &st->local_state))\n\t\t\treturn;\n\n\t\tmsr |= x86_amd_ls_cfg_ssbd_mask;\n\n\t\traw_spin_lock(&st->shared_state->lock);\n\t\t/* First sibling enables SSBD: */\n\t\tif (!st->shared_state->disable_state)\n\t\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\tst->shared_state->disable_state++;\n\t\traw_spin_unlock(&st->shared_state->lock);\n\t} else {\n\t\tif (!__test_and_clear_bit(LSTATE_SSB, &st->local_state))\n\t\t\treturn;\n\n\t\traw_spin_lock(&st->shared_state->lock);\n\t\tst->shared_state->disable_state--;\n\t\tif (!st->shared_state->disable_state)\n\t\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\traw_spin_unlock(&st->shared_state->lock);\n\t}\n}\n#else\nstatic __always_inline void amd_set_core_ssb_state(unsigned long tifn)\n{\n\tu64 msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);\n\n\twrmsrl(MSR_AMD64_LS_CFG, msr);\n}\n#endif\n\nstatic __always_inline void amd_set_ssb_virt_state(unsigned long tifn)\n{\n\t/*\n\t * SSBD has the same definition in SPEC_CTRL and VIRT_SPEC_CTRL,\n\t * so ssbd_tif_to_spec_ctrl() just works.\n\t */\n\twrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));\n}\n\n/*\n * Update the MSRs managing speculation control, during context switch.\n *\n * tifp: Previous task's thread flags\n * tifn: Next task's thread flags\n */\nstatic __always_inline void __speculation_ctrl_update(unsigned long tifp,\n\t\t\t\t\t\t      unsigned long tifn)\n{\n\tunsigned long tif_diff = tifp ^ tifn;\n\tu64 msr = x86_spec_ctrl_base;\n\tbool updmsr = false;\n\n\tlockdep_assert_irqs_disabled();\n\n\t/* Handle change of TIF_SSBD depending on the mitigation method. */\n\tif (static_cpu_has(X86_FEATURE_VIRT_SSBD)) {\n\t\tif (tif_diff & _TIF_SSBD)\n\t\t\tamd_set_ssb_virt_state(tifn);\n\t} else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {\n\t\tif (tif_diff & _TIF_SSBD)\n\t\t\tamd_set_core_ssb_state(tifn);\n\t} else if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||\n\t\t   static_cpu_has(X86_FEATURE_AMD_SSBD)) {\n\t\tupdmsr |= !!(tif_diff & _TIF_SSBD);\n\t\tmsr |= ssbd_tif_to_spec_ctrl(tifn);\n\t}\n\n\t/* Only evaluate TIF_SPEC_IB if conditional STIBP is enabled. */\n\tif (IS_ENABLED(CONFIG_SMP) &&\n\t    static_branch_unlikely(&switch_to_cond_stibp)) {\n\t\tupdmsr |= !!(tif_diff & _TIF_SPEC_IB);\n\t\tmsr |= stibp_tif_to_spec_ctrl(tifn);\n\t}\n\n\tif (updmsr)\n\t\twrmsrl(MSR_IA32_SPEC_CTRL, msr);\n}\n\nstatic unsigned long speculation_ctrl_update_tif(struct task_struct *tsk)\n{\n\tif (test_and_clear_tsk_thread_flag(tsk, TIF_SPEC_FORCE_UPDATE)) {\n\t\tif (task_spec_ssb_disable(tsk))\n\t\t\tset_tsk_thread_flag(tsk, TIF_SSBD);\n\t\telse\n\t\t\tclear_tsk_thread_flag(tsk, TIF_SSBD);\n\n\t\tif (task_spec_ib_disable(tsk))\n\t\t\tset_tsk_thread_flag(tsk, TIF_SPEC_IB);\n\t\telse\n\t\t\tclear_tsk_thread_flag(tsk, TIF_SPEC_IB);\n\t}\n\t/* Return the updated threadinfo flags*/\n\treturn task_thread_info(tsk)->flags;\n}\n\nvoid speculation_ctrl_update(unsigned long tif)\n{\n\tunsigned long flags;\n\n\t/* Forced update. Make sure all relevant TIF flags are different */\n\tlocal_irq_save(flags);\n\t__speculation_ctrl_update(~tif, tif);\n\tlocal_irq_restore(flags);\n}\n\n/* Called from seccomp/prctl update */\nvoid speculation_ctrl_update_current(void)\n{\n\tpreempt_disable();\n\tspeculation_ctrl_update(speculation_ctrl_update_tif(current));\n\tpreempt_enable();\n}\n\nstatic inline void cr4_toggle_bits_irqsoff(unsigned long mask)\n{\n\tunsigned long newval, cr4 = this_cpu_read(cpu_tlbstate.cr4);\n\n\tnewval = cr4 ^ mask;\n\tif (newval != cr4) {\n\t\tthis_cpu_write(cpu_tlbstate.cr4, newval);\n\t\t__write_cr4(newval);\n\t}\n}\n\nvoid __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)\n{\n\tunsigned long tifp, tifn;\n\n\ttifn = READ_ONCE(task_thread_info(next_p)->flags);\n\ttifp = READ_ONCE(task_thread_info(prev_p)->flags);\n\n\tswitch_to_bitmap(tifp);\n\n\tpropagate_user_return_notify(prev_p, next_p);\n\n\tif ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&\n\t    arch_has_block_step()) {\n\t\tunsigned long debugctl, msk;\n\n\t\trdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\t\tdebugctl &= ~DEBUGCTLMSR_BTF;\n\t\tmsk = tifn & _TIF_BLOCKSTEP;\n\t\tdebugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;\n\t\twrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\t}\n\n\tif ((tifp ^ tifn) & _TIF_NOTSC)\n\t\tcr4_toggle_bits_irqsoff(X86_CR4_TSD);\n\n\tif ((tifp ^ tifn) & _TIF_NOCPUID)\n\t\tset_cpuid_faulting(!!(tifn & _TIF_NOCPUID));\n\n\tif (likely(!((tifp | tifn) & _TIF_SPEC_FORCE_UPDATE))) {\n\t\t__speculation_ctrl_update(tifp, tifn);\n\t} else {\n\t\tspeculation_ctrl_update_tif(prev_p);\n\t\ttifn = speculation_ctrl_update_tif(next_p);\n\n\t\t/* Enforce MSR update to ensure consistent state */\n\t\t__speculation_ctrl_update(~tifn, tifn);\n\t}\n\n\tif ((tifp ^ tifn) & _TIF_SLD)\n\t\tswitch_to_sld(tifn);\n}\n\n/*\n * Idle related variables and functions\n */\nunsigned long boot_option_idle_override = IDLE_NO_OVERRIDE;\nEXPORT_SYMBOL(boot_option_idle_override);\n\nstatic void (*x86_idle)(void);\n\n#ifndef CONFIG_SMP\nstatic inline void play_dead(void)\n{\n\tBUG();\n}\n#endif\n\nvoid arch_cpu_idle_enter(void)\n{\n\ttsc_verify_tsc_adjust(false);\n\tlocal_touch_nmi();\n}\n\nvoid arch_cpu_idle_dead(void)\n{\n\tplay_dead();\n}\n\n/*\n * Called from the generic idle code.\n */\nvoid arch_cpu_idle(void)\n{\n\tx86_idle();\n}\n\n/*\n * We use this if we don't have any better idle routine..\n */\nvoid __cpuidle default_idle(void)\n{\n\ttrace_cpu_idle_rcuidle(1, smp_processor_id());\n\tsafe_halt();\n\ttrace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());\n}\n#if defined(CONFIG_APM_MODULE) || defined(CONFIG_HALTPOLL_CPUIDLE_MODULE)\nEXPORT_SYMBOL(default_idle);\n#endif\n\n#ifdef CONFIG_XEN\nbool xen_set_default_idle(void)\n{\n\tbool ret = !!x86_idle;\n\n\tx86_idle = default_idle;\n\n\treturn ret;\n}\n#endif\n\nvoid stop_this_cpu(void *dummy)\n{\n\tlocal_irq_disable();\n\t/*\n\t * Remove this CPU:\n\t */\n\tset_cpu_online(smp_processor_id(), false);\n\tdisable_local_APIC();\n\tmcheck_cpu_clear(this_cpu_ptr(&cpu_info));\n\n\t/*\n\t * Use wbinvd on processors that support SME. This provides support\n\t * for performing a successful kexec when going from SME inactive\n\t * to SME active (or vice-versa). The cache must be cleared so that\n\t * if there are entries with the same physical address, both with and\n\t * without the encryption bit, they don't race each other when flushed\n\t * and potentially end up with the wrong entry being committed to\n\t * memory.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_SME))\n\t\tnative_wbinvd();\n\tfor (;;) {\n\t\t/*\n\t\t * Use native_halt() so that memory contents don't change\n\t\t * (stack usage and variables) after possibly issuing the\n\t\t * native_wbinvd() above.\n\t\t */\n\t\tnative_halt();\n\t}\n}\n\n/*\n * AMD Erratum 400 aware idle routine. We handle it the same way as C3 power\n * states (local apic timer and TSC stop).\n */\nstatic void amd_e400_idle(void)\n{\n\t/*\n\t * We cannot use static_cpu_has_bug() here because X86_BUG_AMD_APIC_C1E\n\t * gets set after static_cpu_has() places have been converted via\n\t * alternatives.\n\t */\n\tif (!boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {\n\t\tdefault_idle();\n\t\treturn;\n\t}\n\n\ttick_broadcast_enter();\n\n\tdefault_idle();\n\n\t/*\n\t * The switch back from broadcast mode needs to be called with\n\t * interrupts disabled.\n\t */\n\tlocal_irq_disable();\n\ttick_broadcast_exit();\n\tlocal_irq_enable();\n}\n\n/*\n * Intel Core2 and older machines prefer MWAIT over HALT for C1.\n * We can't rely on cpuidle installing MWAIT, because it will not load\n * on systems that support only C1 -- so the boot default must be MWAIT.\n *\n * Some AMD machines are the opposite, they depend on using HALT.\n *\n * So for default C1, which is used during boot until cpuidle loads,\n * use MWAIT-C1 on Intel HW that has it, else use HALT.\n */\nstatic int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)\n{\n\tif (c->x86_vendor != X86_VENDOR_INTEL)\n\t\treturn 0;\n\n\tif (!cpu_has(c, X86_FEATURE_MWAIT) || boot_cpu_has_bug(X86_BUG_MONITOR))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * MONITOR/MWAIT with no hints, used for default C1 state. This invokes MWAIT\n * with interrupts enabled and no flags, which is backwards compatible with the\n * original MWAIT implementation.\n */\nstatic __cpuidle void mwait_idle(void)\n{\n\tif (!current_set_polling_and_test()) {\n\t\ttrace_cpu_idle_rcuidle(1, smp_processor_id());\n\t\tif (this_cpu_has(X86_BUG_CLFLUSH_MONITOR)) {\n\t\t\tmb(); /* quirk */\n\t\t\tclflush((void *)&current_thread_info()->flags);\n\t\t\tmb(); /* quirk */\n\t\t}\n\n\t\t__monitor((void *)&current_thread_info()->flags, 0, 0);\n\t\tif (!need_resched())\n\t\t\t__sti_mwait(0, 0);\n\t\telse\n\t\t\tlocal_irq_enable();\n\t\ttrace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());\n\t} else {\n\t\tlocal_irq_enable();\n\t}\n\t__current_clr_polling();\n}\n\nvoid select_idle_routine(const struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_SMP\n\tif (boot_option_idle_override == IDLE_POLL && smp_num_siblings > 1)\n\t\tpr_warn_once(\"WARNING: polling idle and HT enabled, performance may degrade\\n\");\n#endif\n\tif (x86_idle || boot_option_idle_override == IDLE_POLL)\n\t\treturn;\n\n\tif (boot_cpu_has_bug(X86_BUG_AMD_E400)) {\n\t\tpr_info(\"using AMD E400 aware idle routine\\n\");\n\t\tx86_idle = amd_e400_idle;\n\t} else if (prefer_mwait_c1_over_halt(c)) {\n\t\tpr_info(\"using mwait in idle threads\\n\");\n\t\tx86_idle = mwait_idle;\n\t} else\n\t\tx86_idle = default_idle;\n}\n\nvoid amd_e400_c1e_apic_setup(void)\n{\n\tif (boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {\n\t\tpr_info(\"Switch to broadcast mode on CPU%d\\n\", smp_processor_id());\n\t\tlocal_irq_disable();\n\t\ttick_broadcast_force();\n\t\tlocal_irq_enable();\n\t}\n}\n\nvoid __init arch_post_acpi_subsys_init(void)\n{\n\tu32 lo, hi;\n\n\tif (!boot_cpu_has_bug(X86_BUG_AMD_E400))\n\t\treturn;\n\n\t/*\n\t * AMD E400 detection needs to happen after ACPI has been enabled. If\n\t * the machine is affected K8_INTP_C1E_ACTIVE_MASK bits are set in\n\t * MSR_K8_INT_PENDING_MSG.\n\t */\n\trdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);\n\tif (!(lo & K8_INTP_C1E_ACTIVE_MASK))\n\t\treturn;\n\n\tboot_cpu_set_bug(X86_BUG_AMD_APIC_C1E);\n\n\tif (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))\n\t\tmark_tsc_unstable(\"TSC halt in AMD C1E\");\n\tpr_info(\"System has AMD C1E enabled\\n\");\n}\n\nstatic int __init idle_setup(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"poll\")) {\n\t\tpr_info(\"using polling idle threads\\n\");\n\t\tboot_option_idle_override = IDLE_POLL;\n\t\tcpu_idle_poll_ctrl(true);\n\t} else if (!strcmp(str, \"halt\")) {\n\t\t/*\n\t\t * When the boot option of idle=halt is added, halt is\n\t\t * forced to be used for CPU idle. In such case CPU C2/C3\n\t\t * won't be used again.\n\t\t * To continue to load the CPU idle driver, don't touch\n\t\t * the boot_option_idle_override.\n\t\t */\n\t\tx86_idle = default_idle;\n\t\tboot_option_idle_override = IDLE_HALT;\n\t} else if (!strcmp(str, \"nomwait\")) {\n\t\t/*\n\t\t * If the boot option of \"idle=nomwait\" is added,\n\t\t * it means that mwait will be disabled for CPU C2/C3\n\t\t * states. In such case it won't touch the variable\n\t\t * of boot_option_idle_override.\n\t\t */\n\t\tboot_option_idle_override = IDLE_NOMWAIT;\n\t} else\n\t\treturn -1;\n\n\treturn 0;\n}\nearly_param(\"idle\", idle_setup);\n\nunsigned long arch_align_stack(unsigned long sp)\n{\n\tif (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)\n\t\tsp -= get_random_int() % 8192;\n\treturn sp & ~0xf;\n}\n\nunsigned long arch_randomize_brk(struct mm_struct *mm)\n{\n\treturn randomize_page(mm->brk, 0x02000000);\n}\n\n/*\n * Called from fs/proc with a reference on @p to find the function\n * which called into schedule(). This needs to be done carefully\n * because the task might wake up and we might look at a stack\n * changing under us.\n */\nunsigned long get_wchan(struct task_struct *p)\n{\n\tunsigned long start, bottom, top, sp, fp, ip, ret = 0;\n\tint count = 0;\n\n\tif (p == current || p->state == TASK_RUNNING)\n\t\treturn 0;\n\n\tif (!try_get_task_stack(p))\n\t\treturn 0;\n\n\tstart = (unsigned long)task_stack_page(p);\n\tif (!start)\n\t\tgoto out;\n\n\t/*\n\t * Layout of the stack page:\n\t *\n\t * ----------- topmax = start + THREAD_SIZE - sizeof(unsigned long)\n\t * PADDING\n\t * ----------- top = topmax - TOP_OF_KERNEL_STACK_PADDING\n\t * stack\n\t * ----------- bottom = start\n\t *\n\t * The tasks stack pointer points at the location where the\n\t * framepointer is stored. The data on the stack is:\n\t * ... IP FP ... IP FP\n\t *\n\t * We need to read FP and IP, so we need to adjust the upper\n\t * bound by another unsigned long.\n\t */\n\ttop = start + THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;\n\ttop -= 2 * sizeof(unsigned long);\n\tbottom = start;\n\n\tsp = READ_ONCE(p->thread.sp);\n\tif (sp < bottom || sp > top)\n\t\tgoto out;\n\n\tfp = READ_ONCE_NOCHECK(((struct inactive_task_frame *)sp)->bp);\n\tdo {\n\t\tif (fp < bottom || fp > top)\n\t\t\tgoto out;\n\t\tip = READ_ONCE_NOCHECK(*(unsigned long *)(fp + sizeof(unsigned long)));\n\t\tif (!in_sched_functions(ip)) {\n\t\t\tret = ip;\n\t\t\tgoto out;\n\t\t}\n\t\tfp = READ_ONCE_NOCHECK(*(unsigned long *)fp);\n\t} while (count++ < 16 && p->state != TASK_RUNNING);\n\nout:\n\tput_task_stack(p);\n\treturn ret;\n}\n\nlong do_arch_prctl_common(struct task_struct *task, int option,\n\t\t\t  unsigned long cpuid_enabled)\n{\n\tswitch (option) {\n\tcase ARCH_GET_CPUID:\n\t\treturn get_cpuid_mode();\n\tcase ARCH_SET_CPUID:\n\t\treturn set_cpuid_mode(task, cpuid_enabled);\n\t}\n\n\treturn -EINVAL;\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Core of Xen paravirt_ops implementation.\n *\n * This file contains the xen_paravirt_ops structure itself, and the\n * implementations for:\n * - privileged instructions\n * - interrupt flags\n * - segment operations\n * - booting and setup\n *\n * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007\n */\n\n#include <linux/cpu.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/preempt.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/delay.h>\n#include <linux/start_kernel.h>\n#include <linux/sched.h>\n#include <linux/kprobes.h>\n#include <linux/memblock.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/page-flags.h>\n#include <linux/highmem.h>\n#include <linux/console.h>\n#include <linux/pci.h>\n#include <linux/gfp.h>\n#include <linux/edd.h>\n#include <linux/frame.h>\n\n#include <xen/xen.h>\n#include <xen/events.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/version.h>\n#include <xen/interface/physdev.h>\n#include <xen/interface/vcpu.h>\n#include <xen/interface/memory.h>\n#include <xen/interface/nmi.h>\n#include <xen/interface/xen-mca.h>\n#include <xen/features.h>\n#include <xen/page.h>\n#include <xen/hvc-console.h>\n#include <xen/acpi.h>\n\n#include <asm/paravirt.h>\n#include <asm/apic.h>\n#include <asm/page.h>\n#include <asm/xen/pci.h>\n#include <asm/xen/hypercall.h>\n#include <asm/xen/hypervisor.h>\n#include <asm/xen/cpuid.h>\n#include <asm/fixmap.h>\n#include <asm/processor.h>\n#include <asm/proto.h>\n#include <asm/msr-index.h>\n#include <asm/traps.h>\n#include <asm/setup.h>\n#include <asm/desc.h>\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include <asm/reboot.h>\n#include <asm/stackprotector.h>\n#include <asm/hypervisor.h>\n#include <asm/mach_traps.h>\n#include <asm/mwait.h>\n#include <asm/pci_x86.h>\n#include <asm/cpu.h>\n#ifdef CONFIG_X86_IOPL_IOPERM\n#include <asm/io_bitmap.h>\n#endif\n\n#ifdef CONFIG_ACPI\n#include <linux/acpi.h>\n#include <asm/acpi.h>\n#include <acpi/pdc_intel.h>\n#include <acpi/processor.h>\n#include <xen/interface/platform.h>\n#endif\n\n#include \"xen-ops.h\"\n#include \"mmu.h\"\n#include \"smp.h\"\n#include \"multicalls.h\"\n#include \"pmu.h\"\n\n#include \"../kernel/cpu/cpu.h\" /* get_cpu_cap() */\n\nvoid *xen_initial_gdt;\n\nstatic int xen_cpu_up_prepare_pv(unsigned int cpu);\nstatic int xen_cpu_dead_pv(unsigned int cpu);\n\nstruct tls_descs {\n\tstruct desc_struct desc[3];\n};\n\n/*\n * Updating the 3 TLS descriptors in the GDT on every task switch is\n * surprisingly expensive so we avoid updating them if they haven't\n * changed.  Since Xen writes different descriptors than the one\n * passed in the update_descriptor hypercall we keep shadow copies to\n * compare against.\n */\nstatic DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);\n\nstatic void __init xen_banner(void)\n{\n\tunsigned version = HYPERVISOR_xen_version(XENVER_version, NULL);\n\tstruct xen_extraversion extra;\n\tHYPERVISOR_xen_version(XENVER_extraversion, &extra);\n\n\tpr_info(\"Booting paravirtualized kernel on %s\\n\", pv_info.name);\n\tprintk(KERN_INFO \"Xen version: %d.%d%s%s\\n\",\n\t       version >> 16, version & 0xffff, extra.extraversion,\n\t       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? \" (preserve-AD)\" : \"\");\n\n#ifdef CONFIG_X86_32\n\tpr_warn(\"WARNING! WARNING! WARNING! WARNING! WARNING! WARNING! WARNING!\\n\"\n\t\t\"Support for running as 32-bit PV-guest under Xen will soon be removed\\n\"\n\t\t\"from the Linux kernel!\\n\"\n\t\t\"Please use either a 64-bit kernel or switch to HVM or PVH mode!\\n\"\n\t\t\"WARNING! WARNING! WARNING! WARNING! WARNING! WARNING! WARNING!\\n\");\n#endif\n}\n\nstatic void __init xen_pv_init_platform(void)\n{\n\tpopulate_extra_pte(fix_to_virt(FIX_PARAVIRT_BOOTMAP));\n\n\tset_fixmap(FIX_PARAVIRT_BOOTMAP, xen_start_info->shared_info);\n\tHYPERVISOR_shared_info = (void *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);\n\n\t/* xen clock uses per-cpu vcpu_info, need to init it for boot cpu */\n\txen_vcpu_info_reset(0);\n\n\t/* pvclock is in shared info area */\n\txen_init_time_ops();\n}\n\nstatic void __init xen_pv_guest_late_init(void)\n{\n#ifndef CONFIG_SMP\n\t/* Setup shared vcpu info for non-smp configurations */\n\txen_setup_vcpu_info_placement();\n#endif\n}\n\n/* Check if running on Xen version (major, minor) or later */\nbool\nxen_running_on_version_or_later(unsigned int major, unsigned int minor)\n{\n\tunsigned int version;\n\n\tif (!xen_domain())\n\t\treturn false;\n\n\tversion = HYPERVISOR_xen_version(XENVER_version, NULL);\n\tif ((((version >> 16) == major) && ((version & 0xffff) >= minor)) ||\n\t\t((version >> 16) > major))\n\t\treturn true;\n\treturn false;\n}\n\nstatic __read_mostly unsigned int cpuid_leaf5_ecx_val;\nstatic __read_mostly unsigned int cpuid_leaf5_edx_val;\n\nstatic void xen_cpuid(unsigned int *ax, unsigned int *bx,\n\t\t      unsigned int *cx, unsigned int *dx)\n{\n\tunsigned maskebx = ~0;\n\n\t/*\n\t * Mask out inconvenient features, to try and disable as many\n\t * unsupported kernel subsystems as possible.\n\t */\n\tswitch (*ax) {\n\tcase CPUID_MWAIT_LEAF:\n\t\t/* Synthesize the values.. */\n\t\t*ax = 0;\n\t\t*bx = 0;\n\t\t*cx = cpuid_leaf5_ecx_val;\n\t\t*dx = cpuid_leaf5_edx_val;\n\t\treturn;\n\n\tcase 0xb:\n\t\t/* Suppress extended topology stuff */\n\t\tmaskebx = 0;\n\t\tbreak;\n\t}\n\n\tasm(XEN_EMULATE_PREFIX \"cpuid\"\n\t\t: \"=a\" (*ax),\n\t\t  \"=b\" (*bx),\n\t\t  \"=c\" (*cx),\n\t\t  \"=d\" (*dx)\n\t\t: \"0\" (*ax), \"2\" (*cx));\n\n\t*bx &= maskebx;\n}\nSTACK_FRAME_NON_STANDARD(xen_cpuid); /* XEN_EMULATE_PREFIX */\n\nstatic bool __init xen_check_mwait(void)\n{\n#ifdef CONFIG_ACPI\n\tstruct xen_platform_op op = {\n\t\t.cmd\t\t\t= XENPF_set_processor_pminfo,\n\t\t.u.set_pminfo.id\t= -1,\n\t\t.u.set_pminfo.type\t= XEN_PM_PDC,\n\t};\n\tuint32_t buf[3];\n\tunsigned int ax, bx, cx, dx;\n\tunsigned int mwait_mask;\n\n\t/* We need to determine whether it is OK to expose the MWAIT\n\t * capability to the kernel to harvest deeper than C3 states from ACPI\n\t * _CST using the processor_harvest_xen.c module. For this to work, we\n\t * need to gather the MWAIT_LEAF values (which the cstate.c code\n\t * checks against). The hypervisor won't expose the MWAIT flag because\n\t * it would break backwards compatibility; so we will find out directly\n\t * from the hardware and hypercall.\n\t */\n\tif (!xen_initial_domain())\n\t\treturn false;\n\n\t/*\n\t * When running under platform earlier than Xen4.2, do not expose\n\t * mwait, to avoid the risk of loading native acpi pad driver\n\t */\n\tif (!xen_running_on_version_or_later(4, 2))\n\t\treturn false;\n\n\tax = 1;\n\tcx = 0;\n\n\tnative_cpuid(&ax, &bx, &cx, &dx);\n\n\tmwait_mask = (1 << (X86_FEATURE_EST % 32)) |\n\t\t     (1 << (X86_FEATURE_MWAIT % 32));\n\n\tif ((cx & mwait_mask) != mwait_mask)\n\t\treturn false;\n\n\t/* We need to emulate the MWAIT_LEAF and for that we need both\n\t * ecx and edx. The hypercall provides only partial information.\n\t */\n\n\tax = CPUID_MWAIT_LEAF;\n\tbx = 0;\n\tcx = 0;\n\tdx = 0;\n\n\tnative_cpuid(&ax, &bx, &cx, &dx);\n\n\t/* Ask the Hypervisor whether to clear ACPI_PDC_C_C2C3_FFH. If so,\n\t * don't expose MWAIT_LEAF and let ACPI pick the IOPORT version of C3.\n\t */\n\tbuf[0] = ACPI_PDC_REVISION_ID;\n\tbuf[1] = 1;\n\tbuf[2] = (ACPI_PDC_C_CAPABILITY_SMP | ACPI_PDC_EST_CAPABILITY_SWSMP);\n\n\tset_xen_guest_handle(op.u.set_pminfo.pdc, buf);\n\n\tif ((HYPERVISOR_platform_op(&op) == 0) &&\n\t    (buf[2] & (ACPI_PDC_C_C1_FFH | ACPI_PDC_C_C2C3_FFH))) {\n\t\tcpuid_leaf5_ecx_val = cx;\n\t\tcpuid_leaf5_edx_val = dx;\n\t}\n\treturn true;\n#else\n\treturn false;\n#endif\n}\n\nstatic bool __init xen_check_xsave(void)\n{\n\tunsigned int cx, xsave_mask;\n\n\tcx = cpuid_ecx(1);\n\n\txsave_mask = (1 << (X86_FEATURE_XSAVE % 32)) |\n\t\t     (1 << (X86_FEATURE_OSXSAVE % 32));\n\n\t/* Xen will set CR4.OSXSAVE if supported and not disabled by force */\n\treturn (cx & xsave_mask) == xsave_mask;\n}\n\nstatic void __init xen_init_capabilities(void)\n{\n\tsetup_force_cpu_cap(X86_FEATURE_XENPV);\n\tsetup_clear_cpu_cap(X86_FEATURE_DCA);\n\tsetup_clear_cpu_cap(X86_FEATURE_APERFMPERF);\n\tsetup_clear_cpu_cap(X86_FEATURE_MTRR);\n\tsetup_clear_cpu_cap(X86_FEATURE_ACC);\n\tsetup_clear_cpu_cap(X86_FEATURE_X2APIC);\n\tsetup_clear_cpu_cap(X86_FEATURE_SME);\n\n\t/*\n\t * Xen PV would need some work to support PCID: CR3 handling as well\n\t * as xen_flush_tlb_others() would need updating.\n\t */\n\tsetup_clear_cpu_cap(X86_FEATURE_PCID);\n\n\tif (!xen_initial_domain())\n\t\tsetup_clear_cpu_cap(X86_FEATURE_ACPI);\n\n\tif (xen_check_mwait())\n\t\tsetup_force_cpu_cap(X86_FEATURE_MWAIT);\n\telse\n\t\tsetup_clear_cpu_cap(X86_FEATURE_MWAIT);\n\n\tif (!xen_check_xsave()) {\n\t\tsetup_clear_cpu_cap(X86_FEATURE_XSAVE);\n\t\tsetup_clear_cpu_cap(X86_FEATURE_OSXSAVE);\n\t}\n}\n\nstatic void xen_set_debugreg(int reg, unsigned long val)\n{\n\tHYPERVISOR_set_debugreg(reg, val);\n}\n\nstatic unsigned long xen_get_debugreg(int reg)\n{\n\treturn HYPERVISOR_get_debugreg(reg);\n}\n\nstatic void xen_end_context_switch(struct task_struct *next)\n{\n\txen_mc_flush();\n\tparavirt_end_context_switch(next);\n}\n\nstatic unsigned long xen_store_tr(void)\n{\n\treturn 0;\n}\n\n/*\n * Set the page permissions for a particular virtual address.  If the\n * address is a vmalloc mapping (or other non-linear mapping), then\n * find the linear mapping of the page and also set its protections to\n * match.\n */\nstatic void set_aliased_prot(void *v, pgprot_t prot)\n{\n\tint level;\n\tpte_t *ptep;\n\tpte_t pte;\n\tunsigned long pfn;\n\tstruct page *page;\n\tunsigned char dummy;\n\n\tptep = lookup_address((unsigned long)v, &level);\n\tBUG_ON(ptep == NULL);\n\n\tpfn = pte_pfn(*ptep);\n\tpage = pfn_to_page(pfn);\n\n\tpte = pfn_pte(pfn, prot);\n\n\t/*\n\t * Careful: update_va_mapping() will fail if the virtual address\n\t * we're poking isn't populated in the page tables.  We don't\n\t * need to worry about the direct map (that's always in the page\n\t * tables), but we need to be careful about vmap space.  In\n\t * particular, the top level page table can lazily propagate\n\t * entries between processes, so if we've switched mms since we\n\t * vmapped the target in the first place, we might not have the\n\t * top-level page table entry populated.\n\t *\n\t * We disable preemption because we want the same mm active when\n\t * we probe the target and when we issue the hypercall.  We'll\n\t * have the same nominal mm, but if we're a kernel thread, lazy\n\t * mm dropping could change our pgd.\n\t *\n\t * Out of an abundance of caution, this uses __get_user() to fault\n\t * in the target address just in case there's some obscure case\n\t * in which the target address isn't readable.\n\t */\n\n\tpreempt_disable();\n\n\tcopy_from_kernel_nofault(&dummy, v, 1);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))\n\t\tBUG();\n\n\tif (!PageHighMem(page)) {\n\t\tvoid *av = __va(PFN_PHYS(pfn));\n\n\t\tif (av != v)\n\t\t\tif (HYPERVISOR_update_va_mapping((unsigned long)av, pte, 0))\n\t\t\t\tBUG();\n\t} else\n\t\tkmap_flush_unused();\n\n\tpreempt_enable();\n}\n\nstatic void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tconst unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;\n\tint i;\n\n\t/*\n\t * We need to mark the all aliases of the LDT pages RO.  We\n\t * don't need to call vm_flush_aliases(), though, since that's\n\t * only responsible for flushing aliases out the TLBs, not the\n\t * page tables, and Xen will flush the TLB for us if needed.\n\t *\n\t * To avoid confusing future readers: none of this is necessary\n\t * to load the LDT.  The hypervisor only checks this when the\n\t * LDT is faulted in due to subsequent descriptor access.\n\t */\n\n\tfor (i = 0; i < entries; i += entries_per_page)\n\t\tset_aliased_prot(ldt + i, PAGE_KERNEL_RO);\n}\n\nstatic void xen_free_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tconst unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;\n\tint i;\n\n\tfor (i = 0; i < entries; i += entries_per_page)\n\t\tset_aliased_prot(ldt + i, PAGE_KERNEL);\n}\n\nstatic void xen_set_ldt(const void *addr, unsigned entries)\n{\n\tstruct mmuext_op *op;\n\tstruct multicall_space mcs = xen_mc_entry(sizeof(*op));\n\n\ttrace_xen_cpu_set_ldt(addr, entries);\n\n\top = mcs.args;\n\top->cmd = MMUEXT_SET_LDT;\n\top->arg1.linear_addr = (unsigned long)addr;\n\top->arg2.nr_ents = entries;\n\n\tMULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_load_gdt(const struct desc_ptr *dtr)\n{\n\tunsigned long va = dtr->address;\n\tunsigned int size = dtr->size + 1;\n\tunsigned long pfn, mfn;\n\tint level;\n\tpte_t *ptep;\n\tvoid *virt;\n\n\t/* @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */\n\tBUG_ON(size > PAGE_SIZE);\n\tBUG_ON(va & ~PAGE_MASK);\n\n\t/*\n\t * The GDT is per-cpu and is in the percpu data area.\n\t * That can be virtually mapped, so we need to do a\n\t * page-walk to get the underlying MFN for the\n\t * hypercall.  The page can also be in the kernel's\n\t * linear range, so we need to RO that mapping too.\n\t */\n\tptep = lookup_address(va, &level);\n\tBUG_ON(ptep == NULL);\n\n\tpfn = pte_pfn(*ptep);\n\tmfn = pfn_to_mfn(pfn);\n\tvirt = __va(PFN_PHYS(pfn));\n\n\tmake_lowmem_page_readonly((void *)va);\n\tmake_lowmem_page_readonly(virt);\n\n\tif (HYPERVISOR_set_gdt(&mfn, size / sizeof(struct desc_struct)))\n\t\tBUG();\n}\n\n/*\n * load_gdt for early boot, when the gdt is only mapped once\n */\nstatic void __init xen_load_gdt_boot(const struct desc_ptr *dtr)\n{\n\tunsigned long va = dtr->address;\n\tunsigned int size = dtr->size + 1;\n\tunsigned long pfn, mfn;\n\tpte_t pte;\n\n\t/* @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */\n\tBUG_ON(size > PAGE_SIZE);\n\tBUG_ON(va & ~PAGE_MASK);\n\n\tpfn = virt_to_pfn(va);\n\tmfn = pfn_to_mfn(pfn);\n\n\tpte = pfn_pte(pfn, PAGE_KERNEL_RO);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)va, pte, 0))\n\t\tBUG();\n\n\tif (HYPERVISOR_set_gdt(&mfn, size / sizeof(struct desc_struct)))\n\t\tBUG();\n}\n\nstatic inline bool desc_equal(const struct desc_struct *d1,\n\t\t\t      const struct desc_struct *d2)\n{\n\treturn !memcmp(d1, d2, sizeof(*d1));\n}\n\nstatic void load_TLS_descriptor(struct thread_struct *t,\n\t\t\t\tunsigned int cpu, unsigned int i)\n{\n\tstruct desc_struct *shadow = &per_cpu(shadow_tls_desc, cpu).desc[i];\n\tstruct desc_struct *gdt;\n\txmaddr_t maddr;\n\tstruct multicall_space mc;\n\n\tif (desc_equal(shadow, &t->tls_array[i]))\n\t\treturn;\n\n\t*shadow = t->tls_array[i];\n\n\tgdt = get_cpu_gdt_rw(cpu);\n\tmaddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);\n\tmc = __xen_mc_entry(0);\n\n\tMULTI_update_descriptor(mc.mc, maddr.maddr, t->tls_array[i]);\n}\n\nstatic void xen_load_tls(struct thread_struct *t, unsigned int cpu)\n{\n\t/*\n\t * XXX sleazy hack: If we're being called in a lazy-cpu zone\n\t * and lazy gs handling is enabled, it means we're in a\n\t * context switch, and %gs has just been saved.  This means we\n\t * can zero it out to prevent faults on exit from the\n\t * hypervisor if the next process has no %gs.  Either way, it\n\t * has been saved, and the new value will get loaded properly.\n\t * This will go away as soon as Xen has been modified to not\n\t * save/restore %gs for normal hypercalls.\n\t *\n\t * On x86_64, this hack is not used for %gs, because gs points\n\t * to KERNEL_GS_BASE (and uses it for PDA references), so we\n\t * must not zero %gs on x86_64\n\t *\n\t * For x86_64, we need to zero %fs, otherwise we may get an\n\t * exception between the new %fs descriptor being loaded and\n\t * %fs being effectively cleared at __switch_to().\n\t */\n\tif (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU) {\n#ifdef CONFIG_X86_32\n\t\tlazy_load_gs(0);\n#else\n\t\tloadsegment(fs, 0);\n#endif\n\t}\n\n\txen_mc_batch();\n\n\tload_TLS_descriptor(t, cpu, 0);\n\tload_TLS_descriptor(t, cpu, 1);\n\tload_TLS_descriptor(t, cpu, 2);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\n#ifdef CONFIG_X86_64\nstatic void xen_load_gs_index(unsigned int idx)\n{\n\tif (HYPERVISOR_set_segment_base(SEGBASE_GS_USER_SEL, idx))\n\t\tBUG();\n}\n#endif\n\nstatic void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,\n\t\t\t\tconst void *ptr)\n{\n\txmaddr_t mach_lp = arbitrary_virt_to_machine(&dt[entrynum]);\n\tu64 entry = *(u64 *)ptr;\n\n\ttrace_xen_cpu_write_ldt_entry(dt, entrynum, entry);\n\n\tpreempt_disable();\n\n\txen_mc_flush();\n\tif (HYPERVISOR_update_descriptor(mach_lp.maddr, entry))\n\t\tBUG();\n\n\tpreempt_enable();\n}\n\n#ifdef CONFIG_X86_64\nvoid noist_exc_debug(struct pt_regs *regs);\n\nDEFINE_IDTENTRY_RAW(xenpv_exc_nmi)\n{\n\t/* On Xen PV, NMI doesn't use IST.  The C part is the sane as native. */\n\texc_nmi(regs);\n}\n\nDEFINE_IDTENTRY_RAW(xenpv_exc_debug)\n{\n\t/*\n\t * There's no IST on Xen PV, but we still need to dispatch\n\t * to the correct handler.\n\t */\n\tif (user_mode(regs))\n\t\tnoist_exc_debug(regs);\n\telse\n\t\texc_debug(regs);\n}\n\nstruct trap_array_entry {\n\tvoid (*orig)(void);\n\tvoid (*xen)(void);\n\tbool ist_okay;\n};\n\n#define TRAP_ENTRY(func, ist_ok) {\t\t\t\\\n\t.orig\t\t= asm_##func,\t\t\t\\\n\t.xen\t\t= xen_asm_##func,\t\t\\\n\t.ist_okay\t= ist_ok }\n\n#define TRAP_ENTRY_REDIR(func, ist_ok) {\t\t\\\n\t.orig\t\t= asm_##func,\t\t\t\\\n\t.xen\t\t= xen_asm_xenpv_##func,\t\t\\\n\t.ist_okay\t= ist_ok }\n\nstatic struct trap_array_entry trap_array[] = {\n\tTRAP_ENTRY_REDIR(exc_debug,\t\t\ttrue  ),\n\tTRAP_ENTRY(exc_double_fault,\t\t\ttrue  ),\n#ifdef CONFIG_X86_MCE\n\tTRAP_ENTRY(exc_machine_check,\t\t\ttrue  ),\n#endif\n\tTRAP_ENTRY_REDIR(exc_nmi,\t\t\ttrue  ),\n\tTRAP_ENTRY(exc_int3,\t\t\t\tfalse ),\n\tTRAP_ENTRY(exc_overflow,\t\t\tfalse ),\n#ifdef CONFIG_IA32_EMULATION\n\t{ entry_INT80_compat,          xen_entry_INT80_compat,          false },\n#endif\n\tTRAP_ENTRY(exc_page_fault,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_divide_error,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_bounds,\t\t\t\tfalse ),\n\tTRAP_ENTRY(exc_invalid_op,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_device_not_available,\t\tfalse ),\n\tTRAP_ENTRY(exc_coproc_segment_overrun,\t\tfalse ),\n\tTRAP_ENTRY(exc_invalid_tss,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_segment_not_present,\t\tfalse ),\n\tTRAP_ENTRY(exc_stack_segment,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_general_protection,\t\tfalse ),\n\tTRAP_ENTRY(exc_spurious_interrupt_bug,\t\tfalse ),\n\tTRAP_ENTRY(exc_coprocessor_error,\t\tfalse ),\n\tTRAP_ENTRY(exc_alignment_check,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_simd_coprocessor_error,\t\tfalse ),\n};\n\nstatic bool __ref get_trap_addr(void **addr, unsigned int ist)\n{\n\tunsigned int nr;\n\tbool ist_okay = false;\n\n\t/*\n\t * Replace trap handler addresses by Xen specific ones.\n\t * Check for known traps using IST and whitelist them.\n\t * The debugger ones are the only ones we care about.\n\t * Xen will handle faults like double_fault, so we should never see\n\t * them.  Warn if there's an unexpected IST-using fault handler.\n\t */\n\tfor (nr = 0; nr < ARRAY_SIZE(trap_array); nr++) {\n\t\tstruct trap_array_entry *entry = trap_array + nr;\n\n\t\tif (*addr == entry->orig) {\n\t\t\t*addr = entry->xen;\n\t\t\tist_okay = entry->ist_okay;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (nr == ARRAY_SIZE(trap_array) &&\n\t    *addr >= (void *)early_idt_handler_array[0] &&\n\t    *addr < (void *)early_idt_handler_array[NUM_EXCEPTION_VECTORS]) {\n\t\tnr = (*addr - (void *)early_idt_handler_array[0]) /\n\t\t     EARLY_IDT_HANDLER_SIZE;\n\t\t*addr = (void *)xen_early_idt_handler_array[nr];\n\t}\n\n\tif (WARN_ON(ist != 0 && !ist_okay))\n\t\treturn false;\n\n\treturn true;\n}\n#endif\n\nstatic int cvt_gate_to_trap(int vector, const gate_desc *val,\n\t\t\t    struct trap_info *info)\n{\n\tunsigned long addr;\n\n\tif (val->bits.type != GATE_TRAP && val->bits.type != GATE_INTERRUPT)\n\t\treturn 0;\n\n\tinfo->vector = vector;\n\n\taddr = gate_offset(val);\n#ifdef CONFIG_X86_64\n\tif (!get_trap_addr((void **)&addr, val->bits.ist))\n\t\treturn 0;\n#endif\t/* CONFIG_X86_64 */\n\tinfo->address = addr;\n\n\tinfo->cs = gate_segment(val);\n\tinfo->flags = val->bits.dpl;\n\t/* interrupt gates clear IF */\n\tif (val->bits.type == GATE_INTERRUPT)\n\t\tinfo->flags |= 1 << 2;\n\n\treturn 1;\n}\n\n/* Locations of each CPU's IDT */\nstatic DEFINE_PER_CPU(struct desc_ptr, idt_desc);\n\n/* Set an IDT entry.  If the entry is part of the current IDT, then\n   also update Xen. */\nstatic void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)\n{\n\tunsigned long p = (unsigned long)&dt[entrynum];\n\tunsigned long start, end;\n\n\ttrace_xen_cpu_write_idt_entry(dt, entrynum, g);\n\n\tpreempt_disable();\n\n\tstart = __this_cpu_read(idt_desc.address);\n\tend = start + __this_cpu_read(idt_desc.size) + 1;\n\n\txen_mc_flush();\n\n\tnative_write_idt_entry(dt, entrynum, g);\n\n\tif (p >= start && (p + 8) <= end) {\n\t\tstruct trap_info info[2];\n\n\t\tinfo[1].address = 0;\n\n\t\tif (cvt_gate_to_trap(entrynum, g, &info[0]))\n\t\t\tif (HYPERVISOR_set_trap_table(info))\n\t\t\t\tBUG();\n\t}\n\n\tpreempt_enable();\n}\n\nstatic void xen_convert_trap_info(const struct desc_ptr *desc,\n\t\t\t\t  struct trap_info *traps)\n{\n\tunsigned in, out, count;\n\n\tcount = (desc->size+1) / sizeof(gate_desc);\n\tBUG_ON(count > 256);\n\n\tfor (in = out = 0; in < count; in++) {\n\t\tgate_desc *entry = (gate_desc *)(desc->address) + in;\n\n\t\tif (cvt_gate_to_trap(in, entry, &traps[out]))\n\t\t\tout++;\n\t}\n\ttraps[out].address = 0;\n}\n\nvoid xen_copy_trap_info(struct trap_info *traps)\n{\n\tconst struct desc_ptr *desc = this_cpu_ptr(&idt_desc);\n\n\txen_convert_trap_info(desc, traps);\n}\n\n/* Load a new IDT into Xen.  In principle this can be per-CPU, so we\n   hold a spinlock to protect the static traps[] array (static because\n   it avoids allocation, and saves stack space). */\nstatic void xen_load_idt(const struct desc_ptr *desc)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\tstatic struct trap_info traps[257];\n\n\ttrace_xen_cpu_load_idt(desc);\n\n\tspin_lock(&lock);\n\n\tmemcpy(this_cpu_ptr(&idt_desc), desc, sizeof(idt_desc));\n\n\txen_convert_trap_info(desc, traps);\n\n\txen_mc_flush();\n\tif (HYPERVISOR_set_trap_table(traps))\n\t\tBUG();\n\n\tspin_unlock(&lock);\n}\n\n/* Write a GDT descriptor entry.  Ignore LDT descriptors, since\n   they're handled differently. */\nstatic void xen_write_gdt_entry(struct desc_struct *dt, int entry,\n\t\t\t\tconst void *desc, int type)\n{\n\ttrace_xen_cpu_write_gdt_entry(dt, entry, desc, type);\n\n\tpreempt_disable();\n\n\tswitch (type) {\n\tcase DESC_LDT:\n\tcase DESC_TSS:\n\t\t/* ignore */\n\t\tbreak;\n\n\tdefault: {\n\t\txmaddr_t maddr = arbitrary_virt_to_machine(&dt[entry]);\n\n\t\txen_mc_flush();\n\t\tif (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))\n\t\t\tBUG();\n\t}\n\n\t}\n\n\tpreempt_enable();\n}\n\n/*\n * Version of write_gdt_entry for use at early boot-time needed to\n * update an entry as simply as possible.\n */\nstatic void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,\n\t\t\t\t\t    const void *desc, int type)\n{\n\ttrace_xen_cpu_write_gdt_entry(dt, entry, desc, type);\n\n\tswitch (type) {\n\tcase DESC_LDT:\n\tcase DESC_TSS:\n\t\t/* ignore */\n\t\tbreak;\n\n\tdefault: {\n\t\txmaddr_t maddr = virt_to_machine(&dt[entry]);\n\n\t\tif (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))\n\t\t\tdt[entry] = *(struct desc_struct *)desc;\n\t}\n\n\t}\n}\n\nstatic void xen_load_sp0(unsigned long sp0)\n{\n\tstruct multicall_space mcs;\n\n\tmcs = xen_mc_entry(0);\n\tMULTI_stack_switch(mcs.mc, __KERNEL_DS, sp0);\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n\tthis_cpu_write(cpu_tss_rw.x86_tss.sp0, sp0);\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic void xen_invalidate_io_bitmap(void)\n{\n\tstruct physdev_set_iobitmap iobitmap = {\n\t\t.bitmap = 0,\n\t\t.nr_ports = 0,\n\t};\n\n\tnative_tss_invalidate_io_bitmap();\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &iobitmap);\n}\n\nstatic void xen_update_io_bitmap(void)\n{\n\tstruct physdev_set_iobitmap iobitmap;\n\tstruct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);\n\n\tnative_tss_update_io_bitmap();\n\n\tiobitmap.bitmap = (uint8_t *)(&tss->x86_tss) +\n\t\t\t  tss->x86_tss.io_bitmap_base;\n\tif (tss->x86_tss.io_bitmap_base == IO_BITMAP_OFFSET_INVALID)\n\t\tiobitmap.nr_ports = 0;\n\telse\n\t\tiobitmap.nr_ports = IO_BITMAP_BITS;\n\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &iobitmap);\n}\n#endif\n\nstatic void xen_io_delay(void)\n{\n}\n\nstatic DEFINE_PER_CPU(unsigned long, xen_cr0_value);\n\nstatic unsigned long xen_read_cr0(void)\n{\n\tunsigned long cr0 = this_cpu_read(xen_cr0_value);\n\n\tif (unlikely(cr0 == 0)) {\n\t\tcr0 = native_read_cr0();\n\t\tthis_cpu_write(xen_cr0_value, cr0);\n\t}\n\n\treturn cr0;\n}\n\nstatic void xen_write_cr0(unsigned long cr0)\n{\n\tstruct multicall_space mcs;\n\n\tthis_cpu_write(xen_cr0_value, cr0);\n\n\t/* Only pay attention to cr0.TS; everything else is\n\t   ignored. */\n\tmcs = xen_mc_entry(0);\n\n\tMULTI_fpu_taskswitch(mcs.mc, (cr0 & X86_CR0_TS) != 0);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_write_cr4(unsigned long cr4)\n{\n\tcr4 &= ~(X86_CR4_PGE | X86_CR4_PSE | X86_CR4_PCE);\n\n\tnative_write_cr4(cr4);\n}\n\nstatic u64 xen_read_msr_safe(unsigned int msr, int *err)\n{\n\tu64 val;\n\n\tif (pmu_msr_read(msr, &val, err))\n\t\treturn val;\n\n\tval = native_read_msr_safe(msr, err);\n\tswitch (msr) {\n\tcase MSR_IA32_APICBASE:\n\t\tval &= ~X2APIC_ENABLE;\n\t\tbreak;\n\t}\n\treturn val;\n}\n\nstatic int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)\n{\n\tint ret;\n#ifdef CONFIG_X86_64\n\tunsigned int which;\n\tu64 base;\n#endif\n\n\tret = 0;\n\n\tswitch (msr) {\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\t\twhich = SEGBASE_FS; goto set;\n\tcase MSR_KERNEL_GS_BASE:\twhich = SEGBASE_GS_USER; goto set;\n\tcase MSR_GS_BASE:\t\twhich = SEGBASE_GS_KERNEL; goto set;\n\n\tset:\n\t\tbase = ((u64)high << 32) | low;\n\t\tif (HYPERVISOR_set_segment_base(which, base) != 0)\n\t\t\tret = -EIO;\n\t\tbreak;\n#endif\n\n\tcase MSR_STAR:\n\tcase MSR_CSTAR:\n\tcase MSR_LSTAR:\n\tcase MSR_SYSCALL_MASK:\n\tcase MSR_IA32_SYSENTER_CS:\n\tcase MSR_IA32_SYSENTER_ESP:\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\t/* Fast syscall setup is all done in hypercalls, so\n\t\t   these are all ignored.  Stub them out here to stop\n\t\t   Xen console noise. */\n\t\tbreak;\n\n\tdefault:\n\t\tif (!pmu_msr_write(msr, low, high, &ret))\n\t\t\tret = native_write_msr_safe(msr, low, high);\n\t}\n\n\treturn ret;\n}\n\nstatic u64 xen_read_msr(unsigned int msr)\n{\n\t/*\n\t * This will silently swallow a #GP from RDMSR.  It may be worth\n\t * changing that.\n\t */\n\tint err;\n\n\treturn xen_read_msr_safe(msr, &err);\n}\n\nstatic void xen_write_msr(unsigned int msr, unsigned low, unsigned high)\n{\n\t/*\n\t * This will silently swallow a #GP from WRMSR.  It may be worth\n\t * changing that.\n\t */\n\txen_write_msr_safe(msr, low, high);\n}\n\n/* This is called once we have the cpu_possible_mask */\nvoid __init xen_setup_vcpu_info_placement(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\t/* Set up direct vCPU id mapping for PV guests. */\n\t\tper_cpu(xen_vcpu_id, cpu) = cpu;\n\n\t\t/*\n\t\t * xen_vcpu_setup(cpu) can fail  -- in which case it\n\t\t * falls back to the shared_info version for cpus\n\t\t * where xen_vcpu_nr(cpu) < MAX_VIRT_CPUS.\n\t\t *\n\t\t * xen_cpu_up_prepare_pv() handles the rest by failing\n\t\t * them in hotplug.\n\t\t */\n\t\t(void) xen_vcpu_setup(cpu);\n\t}\n\n\t/*\n\t * xen_vcpu_setup managed to place the vcpu_info within the\n\t * percpu area for all cpus, so make use of it.\n\t */\n\tif (xen_have_vcpu_info_placement) {\n\t\tpv_ops.irq.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);\n\t\tpv_ops.irq.restore_fl =\n\t\t\t__PV_IS_CALLEE_SAVE(xen_restore_fl_direct);\n\t\tpv_ops.irq.irq_disable =\n\t\t\t__PV_IS_CALLEE_SAVE(xen_irq_disable_direct);\n\t\tpv_ops.irq.irq_enable =\n\t\t\t__PV_IS_CALLEE_SAVE(xen_irq_enable_direct);\n\t\tpv_ops.mmu.read_cr2 =\n\t\t\t__PV_IS_CALLEE_SAVE(xen_read_cr2_direct);\n\t}\n}\n\nstatic const struct pv_info xen_info __initconst = {\n\t.shared_kernel_pmd = 0,\n\n#ifdef CONFIG_X86_64\n\t.extra_user_64bit_cs = FLAT_USER_CS64,\n#endif\n\t.name = \"Xen\",\n};\n\nstatic const struct pv_cpu_ops xen_cpu_ops __initconst = {\n\t.cpuid = xen_cpuid,\n\n\t.set_debugreg = xen_set_debugreg,\n\t.get_debugreg = xen_get_debugreg,\n\n\t.read_cr0 = xen_read_cr0,\n\t.write_cr0 = xen_write_cr0,\n\n\t.write_cr4 = xen_write_cr4,\n\n\t.wbinvd = native_wbinvd,\n\n\t.read_msr = xen_read_msr,\n\t.write_msr = xen_write_msr,\n\n\t.read_msr_safe = xen_read_msr_safe,\n\t.write_msr_safe = xen_write_msr_safe,\n\n\t.read_pmc = xen_read_pmc,\n\n\t.iret = xen_iret,\n#ifdef CONFIG_X86_64\n\t.usergs_sysret64 = xen_sysret64,\n#endif\n\n\t.load_tr_desc = paravirt_nop,\n\t.set_ldt = xen_set_ldt,\n\t.load_gdt = xen_load_gdt,\n\t.load_idt = xen_load_idt,\n\t.load_tls = xen_load_tls,\n#ifdef CONFIG_X86_64\n\t.load_gs_index = xen_load_gs_index,\n#endif\n\n\t.alloc_ldt = xen_alloc_ldt,\n\t.free_ldt = xen_free_ldt,\n\n\t.store_tr = xen_store_tr,\n\n\t.write_ldt_entry = xen_write_ldt_entry,\n\t.write_gdt_entry = xen_write_gdt_entry,\n\t.write_idt_entry = xen_write_idt_entry,\n\t.load_sp0 = xen_load_sp0,\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n\t.invalidate_io_bitmap = xen_invalidate_io_bitmap,\n\t.update_io_bitmap = xen_update_io_bitmap,\n#endif\n\t.io_delay = xen_io_delay,\n\n\t/* Xen takes care of %gs when switching to usermode for us */\n\t.swapgs = paravirt_nop,\n\n\t.start_context_switch = paravirt_start_context_switch,\n\t.end_context_switch = xen_end_context_switch,\n};\n\nstatic void xen_restart(char *msg)\n{\n\txen_reboot(SHUTDOWN_reboot);\n}\n\nstatic void xen_machine_halt(void)\n{\n\txen_reboot(SHUTDOWN_poweroff);\n}\n\nstatic void xen_machine_power_off(void)\n{\n\tif (pm_power_off)\n\t\tpm_power_off();\n\txen_reboot(SHUTDOWN_poweroff);\n}\n\nstatic void xen_crash_shutdown(struct pt_regs *regs)\n{\n\txen_reboot(SHUTDOWN_crash);\n}\n\nstatic const struct machine_ops xen_machine_ops __initconst = {\n\t.restart = xen_restart,\n\t.halt = xen_machine_halt,\n\t.power_off = xen_machine_power_off,\n\t.shutdown = xen_machine_halt,\n\t.crash_shutdown = xen_crash_shutdown,\n\t.emergency_restart = xen_emergency_restart,\n};\n\nstatic unsigned char xen_get_nmi_reason(void)\n{\n\tunsigned char reason = 0;\n\n\t/* Construct a value which looks like it came from port 0x61. */\n\tif (test_bit(_XEN_NMIREASON_io_error,\n\t\t     &HYPERVISOR_shared_info->arch.nmi_reason))\n\t\treason |= NMI_REASON_IOCHK;\n\tif (test_bit(_XEN_NMIREASON_pci_serr,\n\t\t     &HYPERVISOR_shared_info->arch.nmi_reason))\n\t\treason |= NMI_REASON_SERR;\n\n\treturn reason;\n}\n\nstatic void __init xen_boot_params_init_edd(void)\n{\n#if IS_ENABLED(CONFIG_EDD)\n\tstruct xen_platform_op op;\n\tstruct edd_info *edd_info;\n\tu32 *mbr_signature;\n\tunsigned nr;\n\tint ret;\n\n\tedd_info = boot_params.eddbuf;\n\tmbr_signature = boot_params.edd_mbr_sig_buffer;\n\n\top.cmd = XENPF_firmware_info;\n\n\top.u.firmware_info.type = XEN_FW_DISK_INFO;\n\tfor (nr = 0; nr < EDDMAXNR; nr++) {\n\t\tstruct edd_info *info = edd_info + nr;\n\n\t\top.u.firmware_info.index = nr;\n\t\tinfo->params.length = sizeof(info->params);\n\t\tset_xen_guest_handle(op.u.firmware_info.u.disk_info.edd_params,\n\t\t\t\t     &info->params);\n\t\tret = HYPERVISOR_platform_op(&op);\n\t\tif (ret)\n\t\t\tbreak;\n\n#define C(x) info->x = op.u.firmware_info.u.disk_info.x\n\t\tC(device);\n\t\tC(version);\n\t\tC(interface_support);\n\t\tC(legacy_max_cylinder);\n\t\tC(legacy_max_head);\n\t\tC(legacy_sectors_per_track);\n#undef C\n\t}\n\tboot_params.eddbuf_entries = nr;\n\n\top.u.firmware_info.type = XEN_FW_DISK_MBR_SIGNATURE;\n\tfor (nr = 0; nr < EDD_MBR_SIG_MAX; nr++) {\n\t\top.u.firmware_info.index = nr;\n\t\tret = HYPERVISOR_platform_op(&op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tmbr_signature[nr] = op.u.firmware_info.u.disk_mbr_signature.mbr_signature;\n\t}\n\tboot_params.edd_mbr_sig_buf_entries = nr;\n#endif\n}\n\n/*\n * Set up the GDT and segment registers for -fstack-protector.  Until\n * we do this, we have to be careful not to call any stack-protected\n * function, which is most of the kernel.\n */\nstatic void __init xen_setup_gdt(int cpu)\n{\n\tpv_ops.cpu.write_gdt_entry = xen_write_gdt_entry_boot;\n\tpv_ops.cpu.load_gdt = xen_load_gdt_boot;\n\n\tsetup_stack_canary_segment(cpu);\n\tswitch_to_new_gdt(cpu);\n\n\tpv_ops.cpu.write_gdt_entry = xen_write_gdt_entry;\n\tpv_ops.cpu.load_gdt = xen_load_gdt;\n}\n\nstatic void __init xen_dom0_set_legacy_features(void)\n{\n\tx86_platform.legacy.rtc = 1;\n}\n\n/* First C function to be called on Xen boot */\nasmlinkage __visible void __init xen_start_kernel(void)\n{\n\tstruct physdev_set_iopl set_iopl;\n\tunsigned long initrd_start = 0;\n\tint rc;\n\n\tif (!xen_start_info)\n\t\treturn;\n\n\txen_domain_type = XEN_PV_DOMAIN;\n\txen_start_flags = xen_start_info->flags;\n\n\txen_setup_features();\n\n\t/* Install Xen paravirt ops */\n\tpv_info = xen_info;\n\tpv_ops.init.patch = paravirt_patch_default;\n\tpv_ops.cpu = xen_cpu_ops;\n\txen_init_irq_ops();\n\n\t/*\n\t * Setup xen_vcpu early because it is needed for\n\t * local_irq_disable(), irqs_disabled(), e.g. in printk().\n\t *\n\t * Don't do the full vcpu_info placement stuff until we have\n\t * the cpu_possible_mask and a non-dummy shared_info.\n\t */\n\txen_vcpu_info_reset(0);\n\n\tx86_platform.get_nmi_reason = xen_get_nmi_reason;\n\n\tx86_init.resources.memory_setup = xen_memory_setup;\n\tx86_init.irqs.intr_mode_select\t= x86_init_noop;\n\tx86_init.irqs.intr_mode_init\t= x86_init_noop;\n\tx86_init.oem.arch_setup = xen_arch_setup;\n\tx86_init.oem.banner = xen_banner;\n\tx86_init.hyper.init_platform = xen_pv_init_platform;\n\tx86_init.hyper.guest_late_init = xen_pv_guest_late_init;\n\n\t/*\n\t * Set up some pagetable state before starting to set any ptes.\n\t */\n\n\txen_setup_machphys_mapping();\n\txen_init_mmu_ops();\n\n\t/* Prevent unwanted bits from being set in PTEs. */\n\t__supported_pte_mask &= ~_PAGE_GLOBAL;\n\t__default_kernel_pte_mask &= ~_PAGE_GLOBAL;\n\n\t/*\n\t * Prevent page tables from being allocated in highmem, even\n\t * if CONFIG_HIGHPTE is enabled.\n\t */\n\t__userpte_alloc_gfp &= ~__GFP_HIGHMEM;\n\n\t/* Get mfn list */\n\txen_build_dynamic_phys_to_machine();\n\n\t/*\n\t * Set up kernel GDT and segment registers, mainly so that\n\t * -fstack-protector code can be executed.\n\t */\n\txen_setup_gdt(0);\n\n\t/* Work out if we support NX */\n\tget_cpu_cap(&boot_cpu_data);\n\tx86_configure_nx();\n\n\t/* Determine virtual and physical address sizes */\n\tget_cpu_address_sizes(&boot_cpu_data);\n\n\t/* Let's presume PV guests always boot on vCPU with id 0. */\n\tper_cpu(xen_vcpu_id, 0) = 0;\n\n\tidt_setup_early_handler();\n\n\txen_init_capabilities();\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\t/*\n\t * set up the basic apic ops.\n\t */\n\txen_init_apic();\n#endif\n\n\tif (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {\n\t\tpv_ops.mmu.ptep_modify_prot_start =\n\t\t\txen_ptep_modify_prot_start;\n\t\tpv_ops.mmu.ptep_modify_prot_commit =\n\t\t\txen_ptep_modify_prot_commit;\n\t}\n\n\tmachine_ops = xen_machine_ops;\n\n\t/*\n\t * The only reliable way to retain the initial address of the\n\t * percpu gdt_page is to remember it here, so we can go and\n\t * mark it RW later, when the initial percpu area is freed.\n\t */\n\txen_initial_gdt = &per_cpu(gdt_page, 0);\n\n\txen_smp_init();\n\n#ifdef CONFIG_ACPI_NUMA\n\t/*\n\t * The pages we from Xen are not related to machine pages, so\n\t * any NUMA information the kernel tries to get from ACPI will\n\t * be meaningless.  Prevent it from trying.\n\t */\n\tacpi_numa = -1;\n#endif\n\tWARN_ON(xen_cpuhp_setup(xen_cpu_up_prepare_pv, xen_cpu_dead_pv));\n\n\tlocal_irq_disable();\n\tearly_boot_irqs_disabled = true;\n\n\txen_raw_console_write(\"mapping kernel into physical memory\\n\");\n\txen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base,\n\t\t\t\t   xen_start_info->nr_pages);\n\txen_reserve_special_pages();\n\n\t/* keep using Xen gdt for now; no urgent need to change it */\n\n#ifdef CONFIG_X86_32\n\tpv_info.kernel_rpl = 1;\n\tif (xen_feature(XENFEAT_supervisor_mode_kernel))\n\t\tpv_info.kernel_rpl = 0;\n#else\n\tpv_info.kernel_rpl = 0;\n#endif\n\t/* set the limit of our address space */\n\txen_reserve_top();\n\n\t/*\n\t * We used to do this in xen_arch_setup, but that is too late\n\t * on AMD were early_cpu_init (run before ->arch_setup()) calls\n\t * early_amd_init which pokes 0xcf8 port.\n\t */\n\tset_iopl.iopl = 1;\n\trc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);\n\tif (rc != 0)\n\t\txen_raw_printk(\"physdev_op failed %d\\n\", rc);\n\n#ifdef CONFIG_X86_32\n\t/* set up basic CPUID stuff */\n\tcpu_detect(&new_cpu_data);\n\tset_cpu_cap(&new_cpu_data, X86_FEATURE_FPU);\n\tnew_cpu_data.x86_capability[CPUID_1_EDX] = cpuid_edx(1);\n#endif\n\n\tif (xen_start_info->mod_start) {\n\t    if (xen_start_info->flags & SIF_MOD_START_PFN)\n\t\tinitrd_start = PFN_PHYS(xen_start_info->mod_start);\n\t    else\n\t\tinitrd_start = __pa(xen_start_info->mod_start);\n\t}\n\n\t/* Poke various useful things into boot_params */\n\tboot_params.hdr.type_of_loader = (9 << 4) | 0;\n\tboot_params.hdr.ramdisk_image = initrd_start;\n\tboot_params.hdr.ramdisk_size = xen_start_info->mod_len;\n\tboot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);\n\tboot_params.hdr.hardware_subarch = X86_SUBARCH_XEN;\n\n\tif (!xen_initial_domain()) {\n\t\tadd_preferred_console(\"xenboot\", 0, NULL);\n\t\tif (pci_xen)\n\t\t\tx86_init.pci.arch_init = pci_xen_init;\n\t} else {\n\t\tconst struct dom0_vga_console_info *info =\n\t\t\t(void *)((char *)xen_start_info +\n\t\t\t\t xen_start_info->console.dom0.info_off);\n\t\tstruct xen_platform_op op = {\n\t\t\t.cmd = XENPF_firmware_info,\n\t\t\t.interface_version = XENPF_INTERFACE_VERSION,\n\t\t\t.u.firmware_info.type = XEN_FW_KBD_SHIFT_FLAGS,\n\t\t};\n\n\t\tx86_platform.set_legacy_features =\n\t\t\t\txen_dom0_set_legacy_features;\n\t\txen_init_vga(info, xen_start_info->console.dom0.info_size);\n\t\txen_start_info->console.domU.mfn = 0;\n\t\txen_start_info->console.domU.evtchn = 0;\n\n\t\tif (HYPERVISOR_platform_op(&op) == 0)\n\t\t\tboot_params.kbd_status = op.u.firmware_info.u.kbd_shift_flags;\n\n\t\t/* Make sure ACS will be enabled */\n\t\tpci_request_acs();\n\n\t\txen_acpi_sleep_register();\n\n\t\t/* Avoid searching for BIOS MP tables */\n\t\tx86_init.mpparse.find_smp_config = x86_init_noop;\n\t\tx86_init.mpparse.get_smp_config = x86_init_uint_noop;\n\n\t\txen_boot_params_init_edd();\n\t}\n\n\tif (!boot_params.screen_info.orig_video_isVGA)\n\t\tadd_preferred_console(\"tty\", 0, NULL);\n\tadd_preferred_console(\"hvc\", 0, NULL);\n\tif (boot_params.screen_info.orig_video_isVGA)\n\t\tadd_preferred_console(\"tty\", 0, NULL);\n\n#ifdef CONFIG_PCI\n\t/* PCI BIOS service won't work from a PV guest. */\n\tpci_probe &= ~PCI_PROBE_BIOS;\n#endif\n\txen_raw_console_write(\"about to get started...\\n\");\n\n\t/* We need this for printk timestamps */\n\txen_setup_runstate_info(0);\n\n\txen_efi_init(&boot_params);\n\n\t/* Start the world */\n#ifdef CONFIG_X86_32\n\ti386_start_kernel();\n#else\n\tcr4_init_shadow(); /* 32b kernel does this in i386_start_kernel() */\n\tx86_64_start_reservations((char *)__pa_symbol(&boot_params));\n#endif\n}\n\nstatic int xen_cpu_up_prepare_pv(unsigned int cpu)\n{\n\tint rc;\n\n\tif (per_cpu(xen_vcpu, cpu) == NULL)\n\t\treturn -ENODEV;\n\n\txen_setup_timer(cpu);\n\n\trc = xen_smp_intr_init(cpu);\n\tif (rc) {\n\t\tWARN(1, \"xen_smp_intr_init() for CPU %d failed: %d\\n\",\n\t\t     cpu, rc);\n\t\treturn rc;\n\t}\n\n\trc = xen_smp_intr_init_pv(cpu);\n\tif (rc) {\n\t\tWARN(1, \"xen_smp_intr_init_pv() for CPU %d failed: %d\\n\",\n\t\t     cpu, rc);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int xen_cpu_dead_pv(unsigned int cpu)\n{\n\txen_smp_intr_free(cpu);\n\txen_smp_intr_free_pv(cpu);\n\n\txen_teardown_timer(cpu);\n\n\treturn 0;\n}\n\nstatic uint32_t __init xen_platform_pv(void)\n{\n\tif (xen_pv_domain())\n\t\treturn xen_cpuid_base();\n\n\treturn 0;\n}\n\nconst __initconst struct hypervisor_x86 x86_hyper_xen_pv = {\n\t.name                   = \"Xen PV\",\n\t.detect                 = xen_platform_pv,\n\t.type\t\t\t= X86_HYPER_XEN_PV,\n\t.runtime.pin_vcpu       = xen_pin_vcpu,\n\t.ignore_nopv\t\t= true,\n};\n"], "filenames": ["arch/x86/include/asm/io_bitmap.h", "arch/x86/include/asm/paravirt.h", "arch/x86/include/asm/paravirt_types.h", "arch/x86/kernel/paravirt.c", "arch/x86/kernel/process.c", "arch/x86/xen/enlighten_pv.c"], "buggy_code_start_loc": [21, 304, 143, 327, 325, 872], "buggy_code_end_loc": [27, 304, 143, 328, 384, 1101], "fixing_code_start_loc": [22, 305, 144, 327, 324, 873], "fixing_code_end_loc": [44, 310, 145, 329, 370, 1114], "type": "CWE-276", "message": "An issue was discovered in the Linux kernel 5.5 through 5.7.9, as used in Xen through 4.13.x for x86 PV guests. An attacker may be granted the I/O port permissions of an unrelated task. This occurs because tss_invalidate_io_bitmap mishandling causes a loss of synchronization between the I/O bitmaps of TSS and Xen, aka CID-cadfad870154.", "other": {"cve": {"id": "CVE-2020-15852", "sourceIdentifier": "cve@mitre.org", "published": "2020-07-20T19:15:11.397", "lastModified": "2022-12-03T14:32:06.300", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel 5.5 through 5.7.9, as used in Xen through 4.13.x for x86 PV guests. An attacker may be granted the I/O port permissions of an unrelated task. This occurs because tss_invalidate_io_bitmap mishandling causes a loss of synchronization between the I/O bitmaps of TSS and Xen, aka CID-cadfad870154."}, {"lang": "es", "value": "Se detect\u00f3 un problema en el kernel de Linux versiones 5.5 hasta 5.7.9, como es usado en Xen versiones hasta 4.13.x para invitados PV x86. Un atacante puede otorgar los permisos del puerto de I/O de una tarea no relacionada. Esto ocurre porque el manejo inapropiado de la funci\u00f3n tss_invalidate_io_bitmap causa una p\u00e9rdida de sincronizaci\u00f3n entre los mapas de bits de I/O de TSS y Xen, tambi\u00e9n se conoce como  CID-cadfad870154"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-276"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.5", "versionEndIncluding": "5.7.9", "matchCriteriaId": "5051079A-186E-4829-9B75-FFC19E2FF565"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:xen:xen:*:*:*:*:*:*:x86:*", "versionEndIncluding": "4.13.1", "matchCriteriaId": "8EFB7E72-537B-4289-B638-9DFE70686070"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:cloud_backup:-:*:*:*:*:*:*:*", "matchCriteriaId": "5C2089EE-5D7F-47EC-8EA5-0F69790564C4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:steelstore_cloud_integrated_storage:-:*:*:*:*:*:*:*", "matchCriteriaId": "E94F7F59-1785-493F-91A7-5F5EA5E87E4D"}, {"vulnerable": true, "criteria": "cpe:2.3:h:netapp:solidfire_baseboard_management_controller:-:*:*:*:*:*:*:*", "matchCriteriaId": "090AA6F4-4404-4E26-82AB-C3A22636F276"}]}]}], "references": [{"url": "http://www.openwall.com/lists/oss-security/2020/07/21/2", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://xenbits.xen.org/xsa/advisory-329.html", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=cadfad870154e14f745ec845708bc17d166065f2", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/cadfad870154e14f745ec845708bc17d166065f2", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20200810-0001/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/cadfad870154e14f745ec845708bc17d166065f2"}}