{"buggy_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/kernels/internal/reference/comparisons.h\"\n\n#include <stdint.h>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/string_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace comparisons {\nnamespace {\n\nconstexpr int kInputTensor1 = 0;\nconstexpr int kInputTensor2 = 1;\nconstexpr int kOutputTensor = 0;\n\nTfLiteStatus ComparisonPrepareCommon(TfLiteContext* context, TfLiteNode* node,\n                                     bool is_string_allowed) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  // Don't support string.\n  if (!is_string_allowed) {\n    TF_LITE_ENSURE(context, input1->type != kTfLiteString);\n  }\n  // Currently only support tensors have the same type.\n  TF_LITE_ENSURE_TYPES_EQ(context, input1->type, input2->type);\n  output->type = kTfLiteBool;\n\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n\n  TfLiteIntArray* output_size = nullptr;\n  if (requires_broadcast) {\n    TF_LITE_ENSURE_OK(context, CalculateShapeForBroadcast(\n                                   context, input1, input2, &output_size));\n  } else {\n    output_size = TfLiteIntArrayCopy(input1->dims);\n  }\n\n  return context->ResizeTensor(context, output, output_size);\n}\n\nTfLiteStatus ComparisonPrepare(TfLiteContext* context, TfLiteNode* node) {\n  return ComparisonPrepareCommon(context, node, false);\n}\n\nTfLiteStatus ComparisonPrepareStringAllowed(TfLiteContext* context,\n                                            TfLiteNode* node) {\n  return ComparisonPrepareCommon(context, node, true);\n}\n\ntemplate <typename input_dtype, reference_ops::ComparisonFn<int32> opname>\nvoid ComparisonQuantized(const TfLiteTensor* input1, const TfLiteTensor* input2,\n                         TfLiteTensor* output, bool requires_broadcast) {\n  if (input1->type == kTfLiteUInt8 || input1->type == kTfLiteInt8) {\n    auto input1_offset = -input1->params.zero_point;\n    auto input2_offset = -input2->params.zero_point;\n    const int left_shift = 8;\n\n    int32 input1_multiplier;\n    int input1_shift;\n    QuantizeMultiplierSmallerThanOneExp(input1->params.scale,\n                                        &input1_multiplier, &input1_shift);\n    int32 input2_multiplier;\n    int input2_shift;\n    QuantizeMultiplierSmallerThanOneExp(input2->params.scale,\n                                        &input2_multiplier, &input2_shift);\n\n    ComparisonParams op_params;\n    op_params.left_shift = left_shift;\n    op_params.input1_offset = input1_offset;\n    op_params.input1_multiplier = input1_multiplier;\n    op_params.input1_shift = input1_shift;\n    op_params.input2_offset = input2_offset;\n    op_params.input2_multiplier = input2_multiplier;\n    op_params.input2_shift = input2_shift;\n    if (requires_broadcast) {\n      reference_ops::BroadcastComparison4DSlowWithScaling<input_dtype, opname>(\n          op_params, GetTensorShape(input1), GetTensorData<input_dtype>(input1),\n          GetTensorShape(input2), GetTensorData<input_dtype>(input2),\n          GetTensorShape(output), GetTensorData<bool>(output));\n    } else {\n      reference_ops::ComparisonWithScaling<input_dtype, opname>(\n          op_params, GetTensorShape(input1), GetTensorData<input_dtype>(input1),\n          GetTensorShape(input2), GetTensorData<input_dtype>(input2),\n          GetTensorShape(output), GetTensorData<bool>(output));\n    }\n  }\n}\n\ntemplate <typename T, reference_ops::ComparisonFn<T> opname>\nvoid Comparison(const TfLiteTensor* input1, const TfLiteTensor* input2,\n                TfLiteTensor* output, bool requires_broadcast) {\n  ComparisonParams op_params;\n  requires_broadcast\n      ? reference_ops::BroadcastComparison4DSlowImpl<T, opname>(\n            op_params, GetTensorShape(input1), GetTensorData<T>(input1),\n            GetTensorShape(input2), GetTensorData<T>(input2),\n            GetTensorShape(output), GetTensorData<bool>(output))\n      : reference_ops::ComparisonImpl<T, opname>(\n            op_params, GetTensorShape(input1), GetTensorData<T>(input1),\n            GetTensorShape(input2), GetTensorData<T>(input2),\n            GetTensorShape(output), GetTensorData<bool>(output));\n}\n\nvoid ComparisonString(bool (*opname)(const StringRef&, const StringRef&),\n                      const TfLiteTensor* input1, const TfLiteTensor* input2,\n                      TfLiteTensor* output, bool requires_broadcast) {\n  bool* output_data = GetTensorData<bool>(output);\n  if (requires_broadcast) {\n    reference_ops::BroadcastComparison4DSlowStringImpl(\n        opname, GetTensorShape(input1), input1, GetTensorShape(input2), input2,\n        GetTensorShape(output), output_data);\n  } else {\n    reference_ops::ComparisonStringImpl(opname, GetTensorShape(input1), input1,\n                                        GetTensorShape(input2), input2,\n                                        GetTensorShape(output), output_data);\n  }\n}\n\nTfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteBool:\n      Comparison<bool, reference_ops::EqualFn>(input1, input2, output,\n                                               requires_broadcast);\n      break;\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::EqualFn>(input1, input2, output,\n                                                requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::EqualFn>(input1, input2, output,\n                                                  requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::EqualFn>(input1, input2, output,\n                                                  requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::EqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::EqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteString:\n      ComparisonString(reference_ops::StringRefEqualFn, input1, input2, output,\n                       requires_broadcast);\n      break;\n    default:\n      context->ReportError(\n          context,\n          \"Does not support type %d, requires bool|float|int|uint8|string\",\n          input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteBool:\n      Comparison<bool, reference_ops::NotEqualFn>(input1, input2, output,\n                                                  requires_broadcast);\n      break;\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::NotEqualFn>(input1, input2, output,\n                                                   requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::NotEqualFn>(input1, input2, output,\n                                                     requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::NotEqualFn>(input1, input2, output,\n                                                     requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::NotEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::NotEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteString:\n      ComparisonString(reference_ops::StringRefNotEqualFn, input1, input2,\n                       output, requires_broadcast);\n      break;\n    default:\n      context->ReportError(\n          context,\n          \"Does not support type %d, requires bool|float|int|uint8|string\",\n          input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::GreaterFn>(input1, input2, output,\n                                                  requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::GreaterFn>(input1, input2, output,\n                                                    requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::GreaterFn>(input1, input2, output,\n                                                    requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::GreaterFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::GreaterFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Does not support type %d, requires float|int|uint8\",\n                           input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::GreaterEqualFn>(input1, input2, output,\n                                                       requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::GreaterEqualFn>(input1, input2, output,\n                                                         requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::GreaterEqualFn>(input1, input2, output,\n                                                         requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::GreaterEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::GreaterEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Does not support type %d, requires float|int|uint8\",\n                           input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::LessFn>(input1, input2, output,\n                                               requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::LessFn>(input1, input2, output,\n                                                 requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::LessFn>(input1, input2, output,\n                                                 requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::LessFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::LessFn>(input1, input2, output,\n                                                         requires_broadcast);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Does not support type %d, requires float|int|uint8\",\n                           input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::LessEqualFn>(input1, input2, output,\n                                                    requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::LessEqualFn>(input1, input2, output,\n                                                      requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::LessEqualFn>(input1, input2, output,\n                                                      requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::LessEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::LessEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Does not support type %d, requires float|int|uint8\",\n                           input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace\n}  // namespace comparisons\n\nTfLiteRegistration* Register_EQUAL() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepareStringAllowed,\n                                 comparisons::EqualEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_NOT_EQUAL() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepareStringAllowed,\n                                 comparisons::NotEqualEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_GREATER() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepare,\n                                 comparisons::GreaterEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_GREATER_EQUAL() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepare,\n                                 comparisons::GreaterEqualEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_LESS() {\n  static TfLiteRegistration r = {\n      nullptr, nullptr, comparisons::ComparisonPrepare, comparisons::LessEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_LESS_EQUAL() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepare,\n                                 comparisons::LessEqualEval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <stdint.h>\n\n#include <initializer_list>\n#include <string>\n#include <vector>\n\n#include <gmock/gmock.h>\n#include <gtest/gtest.h>\n#include \"flatbuffers/flatbuffers.h\"  // from @flatbuffers\n#include \"tensorflow/lite/kernels/test_util.h\"\n#include \"tensorflow/lite/schema/schema_generated.h\"\n#include \"tensorflow/lite/string_type.h\"\n\nnamespace tflite {\nnamespace {\n\nusing ::testing::ElementsAre;\n\nclass ComparisonOpModel : public SingleOpModel {\n public:\n  ComparisonOpModel(std::initializer_list<int> input1_shape,\n                    std::initializer_list<int> input2_shape,\n                    TensorType input_type, BuiltinOperator op) {\n    input1_ = AddInput(input_type);\n    input2_ = AddInput(input_type);\n    output_ = AddOutput(TensorType_BOOL);\n    ConfigureBuiltinOp(op);\n    BuildInterpreter({input1_shape, input2_shape});\n  }\n\n  ComparisonOpModel(const TensorData& input1, const TensorData& input2,\n                    TensorType input_type, BuiltinOperator op) {\n    input1_ = AddInput(input1);\n    input2_ = AddInput(input2);\n    output_ = AddOutput(TensorType_BOOL);\n    ConfigureBuiltinOp(op);\n    BuildInterpreter({GetShape(input1_), GetShape(input2_)});\n  }\n\n  int input1() { return input1_; }\n  int input2() { return input2_; }\n\n  std::vector<bool> GetOutput() { return ExtractVector<bool>(output_); }\n  std::vector<int> GetOutputShape() { return GetTensorShape(output_); }\n\n private:\n  int input1_;\n  int input2_;\n  int output_;\n\n  void ConfigureBuiltinOp(BuiltinOperator op) {\n    switch (op) {\n      case BuiltinOperator_EQUAL: {\n        SetBuiltinOp(op, BuiltinOptions_EqualOptions,\n                     CreateEqualOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_NOT_EQUAL: {\n        SetBuiltinOp(op, BuiltinOptions_NotEqualOptions,\n                     CreateNotEqualOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_GREATER: {\n        SetBuiltinOp(op, BuiltinOptions_GreaterOptions,\n                     CreateGreaterOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_GREATER_EQUAL: {\n        SetBuiltinOp(op, BuiltinOptions_GreaterEqualOptions,\n                     CreateGreaterEqualOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_LESS: {\n        SetBuiltinOp(op, BuiltinOptions_LessOptions,\n                     CreateLessOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_LESS_EQUAL: {\n        SetBuiltinOp(op, BuiltinOptions_LessEqualOptions,\n                     CreateLessEqualOptions(builder_).Union());\n        break;\n      }\n      default: { FAIL() << \"We shouldn't get here.\"; }\n    }\n  }\n};\n\nTEST(ComparisonsTest, EqualBool) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_BOOL,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<bool>(model.input1(), {true, false, true, false});\n  model.PopulateTensor<bool>(model.input2(), {true, true, false, false});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, EqualFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, EqualInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, EqualString) {\n  if (SingleOpModel::GetForceUseNnapi()) {\n    return;\n  }\n  ComparisonOpModel model({1, 1, 1, 4, 1}, {1, 1, 1, 4, 1}, TensorType_STRING,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<std::string>(model.input1(), {\"A\", \"B\", \"C\", \"D\"});\n  model.PopulateTensor<std::string>(model.input2(), {\"A\", \"C\", \"B\", \"D\"});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4, 1));\n}\n\nTEST(ComparisonsTest, EqualBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, EqualBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, false, false, false,\n                                             false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(ComparisonsTest, EqualBroadcastString) {\n  if (SingleOpModel::GetForceUseNnapi()) {\n    return;\n  }\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_STRING,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<std::string>(model.input1(), {\"A\", \"B\", \"A\", \"B\"});\n  model.PopulateTensor<std::string>(model.input2(), {\"A\"});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualBool) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_BOOL,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<bool>(model.input1(), {true, false, true, false});\n  model.PopulateTensor<bool>(model.input2(), {true, true, false, false});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualString) {\n  if (SingleOpModel::GetForceUseNnapi()) {\n    return;\n  }\n  ComparisonOpModel model({1, 1, 1, 1, 4}, {1, 1, 1, 1, 4}, TensorType_STRING,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<std::string>(model.input1(), {\"A\", \"B\", \"C\", \"D\"});\n  model.PopulateTensor<std::string>(model.input2(), {\"A\", \"C\", \"B\", \"D\"});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(true, true, true, true, true, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(ComparisonsTest, NotEqualBroadcastString) {\n  if (SingleOpModel::GetForceUseNnapi()) {\n    return;\n  }\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_STRING,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<std::string>(model.input1(), {\"A\", \"B\", \"A\", \"B\"});\n  model.PopulateTensor<std::string>(model.input2(), {\"A\"});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_GREATER);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_GREATER);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_GREATER);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_GREATER);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(false, true, true, false, false, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(ComparisonsTest, GreaterEqualFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_GREATER_EQUAL);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterEqualInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_GREATER_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterEqualBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_GREATER_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterEqualBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_GREATER_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(false, true, true, false, false, true, true, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\n\nTEST(ComparisonsTest, LessFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_LESS);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_LESS);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_LESS);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_LESS);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 6, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(true, false, false, true, true, false, false, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(ComparisonsTest, LessEqualFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_LESS_EQUAL);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessEqualInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_LESS_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, true, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessEqualBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_LESS_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, true, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessEqualBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_LESS_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(true, false, false, true, true, false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(QuantizedComparisonsTest, EqualUInt8Quantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_EQUAL);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, true, false));\n}\n\nTEST(QuantizedComparisonsTest, EqualInt8Quantized) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  ComparisonOpModel model({TensorType_INT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_INT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_INT8, BuiltinOperator_EQUAL);\n  model.QuantizeAndPopulate<int8_t>(model.input1(), {1, -9, 7, 3});\n  model.QuantizeAndPopulate<int8_t>(model.input2(), {-1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, true, false));\n}\n\nTEST(QuantizedComparisonsTest, NotEqualUInt8Quantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_NOT_EQUAL);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 7, 0});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, true));\n}\n\nTEST(QuantizedComparisonsTest, NotEqualInt8Quantized) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  ComparisonOpModel model({TensorType_INT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_INT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_INT8, BuiltinOperator_NOT_EQUAL);\n  model.QuantizeAndPopulate<int8_t>(model.input1(), {1, -9, 7, 3});\n  model.QuantizeAndPopulate<int8_t>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, true));\n}\n\nTEST(ComparisonsTest, GreaterQuantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_GREATER);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n}\n\nTEST(ComparisonsTest, GreaterQuantizedSmallRange) {\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, 0.0, 1.0},\n                          {TensorType_UINT8, {1, 2, 2, 1}, 0.0, 2.0},\n                          TensorType_UINT8, BuiltinOperator_GREATER);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1.0, 0.5, 0.35, 0.1});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1.01, 0.25, 0.3, 0.4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n}\n\nTEST(ComparisonsTest, GreaterEqualQuantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_GREATER_EQUAL);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, true, true, false));\n}\n\nTEST(ComparisonsTest, LessQuantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_LESS);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, false, true));\n}\n\nTEST(ComparisonsTest, LessEqualQuantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_LESS_EQUAL);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n}\n\nTEST(ComparisonsTest, QuantizedEqualWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_EQUAL);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {2});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, false, false, false, false))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8NotEqualWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_NOT_EQUAL);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {2});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, true, true, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8NotEqualWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_NOT_EQUAL);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {-20, 2, 7, -8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {2});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, true, true, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8GreaterWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_GREATER);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, false, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8GreaterWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_GREATER);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {20, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, false, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8GreaterEqualWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_GREATER_EQUAL);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, true, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8GreaterEqualWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_GREATER_EQUAL);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {20, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, true, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8LessWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_LESS);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, true, false, false, false))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8LessWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_LESS);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {20, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, true, false, false, false))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8LessEqualWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_LESS_EQUAL);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, true, true, false, false))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8LessEqualWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_LESS_EQUAL);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {20, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, true, true, false, false))\n        << \"With shape number \" << i;\n  }\n}\n}  // namespace\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/kernels/internal/reference/comparisons.h\"\n\n#include <stdint.h>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/string_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace comparisons {\nnamespace {\n\nconstexpr int kInputTensor1 = 0;\nconstexpr int kInputTensor2 = 1;\nconstexpr int kOutputTensor = 0;\n\nTfLiteStatus ComparisonPrepareCommon(TfLiteContext* context, TfLiteNode* node,\n                                     bool is_string_allowed) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  // Don't support string.\n  if (!is_string_allowed) {\n    TF_LITE_ENSURE(context, input1->type != kTfLiteString);\n  }\n  // Currently only support tensors have the same type.\n  TF_LITE_ENSURE_TYPES_EQ(context, input1->type, input2->type);\n  output->type = kTfLiteBool;\n\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n\n  TfLiteIntArray* output_size = nullptr;\n  if (requires_broadcast) {\n    TF_LITE_ENSURE_OK(context, CalculateShapeForBroadcast(\n                                   context, input1, input2, &output_size));\n  } else {\n    output_size = TfLiteIntArrayCopy(input1->dims);\n  }\n\n  return context->ResizeTensor(context, output, output_size);\n}\n\nTfLiteStatus ComparisonPrepare(TfLiteContext* context, TfLiteNode* node) {\n  return ComparisonPrepareCommon(context, node, false);\n}\n\nTfLiteStatus ComparisonPrepareStringAllowed(TfLiteContext* context,\n                                            TfLiteNode* node) {\n  return ComparisonPrepareCommon(context, node, true);\n}\n\nvoid QuantizeMultiplier(double double_multiplier, int32_t* quantized_multiplier,\n                        int* left_shift) {\n  if (double_multiplier < 1.0) {\n    QuantizeMultiplierSmallerThanOneExp(double_multiplier, quantized_multiplier,\n                                        left_shift);\n  } else {\n    QuantizeMultiplierGreaterThanOne(double_multiplier, quantized_multiplier,\n                                     left_shift);\n  }\n}\n\ntemplate <typename input_dtype, reference_ops::ComparisonFn<int32> opname>\nvoid ComparisonQuantized(const TfLiteTensor* input1, const TfLiteTensor* input2,\n                         TfLiteTensor* output, bool requires_broadcast) {\n  if (input1->type == kTfLiteUInt8 || input1->type == kTfLiteInt8) {\n    auto input1_offset = -input1->params.zero_point;\n    auto input2_offset = -input2->params.zero_point;\n    const int left_shift = 8;\n\n    int32 input1_multiplier;\n    int32 input2_multiplier;\n    int input1_shift;\n    int input2_shift;\n    QuantizeMultiplier(input1->params.scale, &input1_multiplier, &input1_shift);\n    QuantizeMultiplier(input2->params.scale, &input2_multiplier, &input2_shift);\n\n    ComparisonParams op_params;\n    op_params.left_shift = left_shift;\n    op_params.input1_offset = input1_offset;\n    op_params.input1_multiplier = input1_multiplier;\n    op_params.input1_shift = input1_shift;\n    op_params.input2_offset = input2_offset;\n    op_params.input2_multiplier = input2_multiplier;\n    op_params.input2_shift = input2_shift;\n    if (requires_broadcast) {\n      reference_ops::BroadcastComparison4DSlowWithScaling<input_dtype, opname>(\n          op_params, GetTensorShape(input1), GetTensorData<input_dtype>(input1),\n          GetTensorShape(input2), GetTensorData<input_dtype>(input2),\n          GetTensorShape(output), GetTensorData<bool>(output));\n    } else {\n      reference_ops::ComparisonWithScaling<input_dtype, opname>(\n          op_params, GetTensorShape(input1), GetTensorData<input_dtype>(input1),\n          GetTensorShape(input2), GetTensorData<input_dtype>(input2),\n          GetTensorShape(output), GetTensorData<bool>(output));\n    }\n  }\n}\n\ntemplate <typename T, reference_ops::ComparisonFn<T> opname>\nvoid Comparison(const TfLiteTensor* input1, const TfLiteTensor* input2,\n                TfLiteTensor* output, bool requires_broadcast) {\n  ComparisonParams op_params;\n  requires_broadcast\n      ? reference_ops::BroadcastComparison4DSlowImpl<T, opname>(\n            op_params, GetTensorShape(input1), GetTensorData<T>(input1),\n            GetTensorShape(input2), GetTensorData<T>(input2),\n            GetTensorShape(output), GetTensorData<bool>(output))\n      : reference_ops::ComparisonImpl<T, opname>(\n            op_params, GetTensorShape(input1), GetTensorData<T>(input1),\n            GetTensorShape(input2), GetTensorData<T>(input2),\n            GetTensorShape(output), GetTensorData<bool>(output));\n}\n\nvoid ComparisonString(bool (*opname)(const StringRef&, const StringRef&),\n                      const TfLiteTensor* input1, const TfLiteTensor* input2,\n                      TfLiteTensor* output, bool requires_broadcast) {\n  bool* output_data = GetTensorData<bool>(output);\n  if (requires_broadcast) {\n    reference_ops::BroadcastComparison4DSlowStringImpl(\n        opname, GetTensorShape(input1), input1, GetTensorShape(input2), input2,\n        GetTensorShape(output), output_data);\n  } else {\n    reference_ops::ComparisonStringImpl(opname, GetTensorShape(input1), input1,\n                                        GetTensorShape(input2), input2,\n                                        GetTensorShape(output), output_data);\n  }\n}\n\nTfLiteStatus EqualEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteBool:\n      Comparison<bool, reference_ops::EqualFn>(input1, input2, output,\n                                               requires_broadcast);\n      break;\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::EqualFn>(input1, input2, output,\n                                                requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::EqualFn>(input1, input2, output,\n                                                  requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::EqualFn>(input1, input2, output,\n                                                  requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::EqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::EqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteString:\n      ComparisonString(reference_ops::StringRefEqualFn, input1, input2, output,\n                       requires_broadcast);\n      break;\n    default:\n      context->ReportError(\n          context,\n          \"Does not support type %d, requires bool|float|int|uint8|string\",\n          input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus NotEqualEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteBool:\n      Comparison<bool, reference_ops::NotEqualFn>(input1, input2, output,\n                                                  requires_broadcast);\n      break;\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::NotEqualFn>(input1, input2, output,\n                                                   requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::NotEqualFn>(input1, input2, output,\n                                                     requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::NotEqualFn>(input1, input2, output,\n                                                     requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::NotEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::NotEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteString:\n      ComparisonString(reference_ops::StringRefNotEqualFn, input1, input2,\n                       output, requires_broadcast);\n      break;\n    default:\n      context->ReportError(\n          context,\n          \"Does not support type %d, requires bool|float|int|uint8|string\",\n          input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus GreaterEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::GreaterFn>(input1, input2, output,\n                                                  requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::GreaterFn>(input1, input2, output,\n                                                    requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::GreaterFn>(input1, input2, output,\n                                                    requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::GreaterFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::GreaterFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Does not support type %d, requires float|int|uint8\",\n                           input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus GreaterEqualEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::GreaterEqualFn>(input1, input2, output,\n                                                       requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::GreaterEqualFn>(input1, input2, output,\n                                                         requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::GreaterEqualFn>(input1, input2, output,\n                                                         requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::GreaterEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::GreaterEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Does not support type %d, requires float|int|uint8\",\n                           input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus LessEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::LessFn>(input1, input2, output,\n                                               requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::LessFn>(input1, input2, output,\n                                                 requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::LessFn>(input1, input2, output,\n                                                 requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::LessFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::LessFn>(input1, input2, output,\n                                                         requires_broadcast);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Does not support type %d, requires float|int|uint8\",\n                           input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus LessEqualEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  bool requires_broadcast = !HaveSameShapes(input1, input2);\n  switch (input1->type) {\n    case kTfLiteFloat32:\n      Comparison<float, reference_ops::LessEqualFn>(input1, input2, output,\n                                                    requires_broadcast);\n      break;\n    case kTfLiteInt32:\n      Comparison<int32_t, reference_ops::LessEqualFn>(input1, input2, output,\n                                                      requires_broadcast);\n      break;\n    case kTfLiteInt64:\n      Comparison<int64_t, reference_ops::LessEqualFn>(input1, input2, output,\n                                                      requires_broadcast);\n      break;\n    case kTfLiteUInt8:\n      ComparisonQuantized<uint8_t, reference_ops::LessEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    case kTfLiteInt8:\n      ComparisonQuantized<int8_t, reference_ops::LessEqualFn>(\n          input1, input2, output, requires_broadcast);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Does not support type %d, requires float|int|uint8\",\n                           input1->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace\n}  // namespace comparisons\n\nTfLiteRegistration* Register_EQUAL() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepareStringAllowed,\n                                 comparisons::EqualEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_NOT_EQUAL() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepareStringAllowed,\n                                 comparisons::NotEqualEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_GREATER() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepare,\n                                 comparisons::GreaterEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_GREATER_EQUAL() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepare,\n                                 comparisons::GreaterEqualEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_LESS() {\n  static TfLiteRegistration r = {\n      nullptr, nullptr, comparisons::ComparisonPrepare, comparisons::LessEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_LESS_EQUAL() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 comparisons::ComparisonPrepare,\n                                 comparisons::LessEqualEval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n", "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <stdint.h>\n\n#include <initializer_list>\n#include <string>\n#include <vector>\n\n#include <gmock/gmock.h>\n#include <gtest/gtest.h>\n#include \"flatbuffers/flatbuffers.h\"  // from @flatbuffers\n#include \"tensorflow/lite/kernels/test_util.h\"\n#include \"tensorflow/lite/schema/schema_generated.h\"\n#include \"tensorflow/lite/string_type.h\"\n\nnamespace tflite {\nnamespace {\n\nusing ::testing::ElementsAre;\n\nclass ComparisonOpModel : public SingleOpModel {\n public:\n  ComparisonOpModel(std::initializer_list<int> input1_shape,\n                    std::initializer_list<int> input2_shape,\n                    TensorType input_type, BuiltinOperator op) {\n    input1_ = AddInput(input_type);\n    input2_ = AddInput(input_type);\n    output_ = AddOutput(TensorType_BOOL);\n    ConfigureBuiltinOp(op);\n    BuildInterpreter({input1_shape, input2_shape});\n  }\n\n  ComparisonOpModel(const TensorData& input1, const TensorData& input2,\n                    TensorType input_type, BuiltinOperator op) {\n    input1_ = AddInput(input1);\n    input2_ = AddInput(input2);\n    output_ = AddOutput(TensorType_BOOL);\n    ConfigureBuiltinOp(op);\n    BuildInterpreter({GetShape(input1_), GetShape(input2_)});\n  }\n\n  int input1() { return input1_; }\n  int input2() { return input2_; }\n\n  std::vector<bool> GetOutput() { return ExtractVector<bool>(output_); }\n  std::vector<int> GetOutputShape() { return GetTensorShape(output_); }\n\n private:\n  int input1_;\n  int input2_;\n  int output_;\n\n  void ConfigureBuiltinOp(BuiltinOperator op) {\n    switch (op) {\n      case BuiltinOperator_EQUAL: {\n        SetBuiltinOp(op, BuiltinOptions_EqualOptions,\n                     CreateEqualOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_NOT_EQUAL: {\n        SetBuiltinOp(op, BuiltinOptions_NotEqualOptions,\n                     CreateNotEqualOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_GREATER: {\n        SetBuiltinOp(op, BuiltinOptions_GreaterOptions,\n                     CreateGreaterOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_GREATER_EQUAL: {\n        SetBuiltinOp(op, BuiltinOptions_GreaterEqualOptions,\n                     CreateGreaterEqualOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_LESS: {\n        SetBuiltinOp(op, BuiltinOptions_LessOptions,\n                     CreateLessOptions(builder_).Union());\n        break;\n      }\n      case BuiltinOperator_LESS_EQUAL: {\n        SetBuiltinOp(op, BuiltinOptions_LessEqualOptions,\n                     CreateLessEqualOptions(builder_).Union());\n        break;\n      }\n      default: { FAIL() << \"We shouldn't get here.\"; }\n    }\n  }\n};\n\nTEST(ComparisonsTest, EqualBool) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_BOOL,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<bool>(model.input1(), {true, false, true, false});\n  model.PopulateTensor<bool>(model.input2(), {true, true, false, false});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, EqualFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, EqualInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, EqualString) {\n  if (SingleOpModel::GetForceUseNnapi()) {\n    return;\n  }\n  ComparisonOpModel model({1, 1, 1, 4, 1}, {1, 1, 1, 4, 1}, TensorType_STRING,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<std::string>(model.input1(), {\"A\", \"B\", \"C\", \"D\"});\n  model.PopulateTensor<std::string>(model.input2(), {\"A\", \"C\", \"B\", \"D\"});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4, 1));\n}\n\nTEST(ComparisonsTest, EqualBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, EqualBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, false, false, false,\n                                             false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(ComparisonsTest, EqualBroadcastString) {\n  if (SingleOpModel::GetForceUseNnapi()) {\n    return;\n  }\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_STRING,\n                          BuiltinOperator_EQUAL);\n  model.PopulateTensor<std::string>(model.input1(), {\"A\", \"B\", \"A\", \"B\"});\n  model.PopulateTensor<std::string>(model.input2(), {\"A\"});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualBool) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_BOOL,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<bool>(model.input1(), {true, false, true, false});\n  model.PopulateTensor<bool>(model.input2(), {true, true, false, false});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualString) {\n  if (SingleOpModel::GetForceUseNnapi()) {\n    return;\n  }\n  ComparisonOpModel model({1, 1, 1, 1, 4}, {1, 1, 1, 1, 4}, TensorType_STRING,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<std::string>(model.input1(), {\"A\", \"B\", \"C\", \"D\"});\n  model.PopulateTensor<std::string>(model.input2(), {\"A\", \"C\", \"B\", \"D\"});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, NotEqualBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(true, true, true, true, true, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(ComparisonsTest, NotEqualBroadcastString) {\n  if (SingleOpModel::GetForceUseNnapi()) {\n    return;\n  }\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_STRING,\n                          BuiltinOperator_NOT_EQUAL);\n  model.PopulateTensor<std::string>(model.input1(), {\"A\", \"B\", \"A\", \"B\"});\n  model.PopulateTensor<std::string>(model.input2(), {\"A\"});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_GREATER);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_GREATER);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_GREATER);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_GREATER);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(false, true, true, false, false, true, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(ComparisonsTest, GreaterEqualFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_GREATER_EQUAL);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterEqualInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_GREATER_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterEqualBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_GREATER_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, GreaterEqualBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_GREATER_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(false, true, true, false, false, true, true, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\n\nTEST(ComparisonsTest, LessFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_LESS);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_LESS);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_LESS);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_LESS);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 6, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(true, false, false, true, true, false, false, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(ComparisonsTest, LessEqualFloat) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_FLOAT32,\n                          BuiltinOperator_LESS_EQUAL);\n  model.PopulateTensor<float>(model.input1(), {0.1, 0.9, 0.7, 0.3});\n  model.PopulateTensor<float>(model.input2(), {0.1, 0.2, 0.6, 0.5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessEqualInt) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_LESS_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, true, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessEqualBroadcast) {\n  ComparisonOpModel model({1, 1, 1, 4}, {1, 1, 1, 1}, TensorType_INT32,\n                          BuiltinOperator_LESS_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3});\n  model.PopulateTensor<int>(model.input2(), {7});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, true, true));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 1, 4));\n}\n\nTEST(ComparisonsTest, LessEqualBroadcastTwoD) {\n  ComparisonOpModel model({1, 1, 2, 4}, {1, 1, 1, 4}, TensorType_INT32,\n                          BuiltinOperator_LESS_EQUAL);\n  model.PopulateTensor<int>(model.input1(), {-1, 9, 7, 3, 2, 4, 2, 8});\n  model.PopulateTensor<int>(model.input2(), {7, 1, 2, 4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(),\n              ElementsAre(true, false, false, true, true, false, true, false));\n  EXPECT_THAT(model.GetOutputShape(), ElementsAre(1, 1, 2, 4));\n}\n\nTEST(QuantizedComparisonsTest, EqualUInt8Quantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_EQUAL);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, true, false));\n}\n\nTEST(QuantizedComparisonsTest, EqualInt8Quantized) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  ComparisonOpModel model({TensorType_INT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_INT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_INT8, BuiltinOperator_EQUAL);\n  model.QuantizeAndPopulate<int8_t>(model.input1(), {1, -9, 7, 3});\n  model.QuantizeAndPopulate<int8_t>(model.input2(), {-1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, true, false));\n}\n\nTEST(QuantizedComparisonsTest, NotEqualUInt8Quantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_NOT_EQUAL);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 7, 0});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, true));\n}\n\nTEST(QuantizedComparisonsTest, NotEqualInt8Quantized) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  ComparisonOpModel model({TensorType_INT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_INT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_INT8, BuiltinOperator_NOT_EQUAL);\n  model.QuantizeAndPopulate<int8_t>(model.input1(), {1, -9, 7, 3});\n  model.QuantizeAndPopulate<int8_t>(model.input2(), {1, 2, 7, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, false, true));\n}\n\nTEST(ComparisonsTest, GreaterQuantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_GREATER);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n}\n\nTEST(ComparisonsTest, GreaterQuantizedSmallRange) {\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, 0.0, 1.0},\n                          {TensorType_UINT8, {1, 2, 2, 1}, 0.0, 2.0},\n                          TensorType_UINT8, BuiltinOperator_GREATER);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1.0, 0.5, 0.35, 0.1});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1.01, 0.25, 0.3, 0.4});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, true, true, false));\n}\n\nTEST(ComparisonsTest, GreaterEqualQuantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_GREATER_EQUAL);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, true, true, false));\n}\n\nTEST(ComparisonsTest, LessQuantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_LESS);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(false, false, false, true));\n}\n\nTEST(ComparisonsTest, LessEqualQuantized) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  ComparisonOpModel model({TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          {TensorType_UINT8, {1, 2, 2, 1}, kMin, kMax},\n                          TensorType_UINT8, BuiltinOperator_LESS_EQUAL);\n  model.QuantizeAndPopulate<uint8_t>(model.input1(), {1, 9, 7, 3});\n  model.QuantizeAndPopulate<uint8_t>(model.input2(), {1, 2, 6, 5});\n  model.Invoke();\n\n  EXPECT_THAT(model.GetOutput(), ElementsAre(true, false, false, true));\n}\n\nTEST(ComparisonsTest, QuantizedEqualWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_EQUAL);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {2});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, false, false, false, false))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8NotEqualWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_NOT_EQUAL);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {2});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, true, true, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8NotEqualWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_NOT_EQUAL);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {-20, 2, 7, -8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {2});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, true, true, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8GreaterWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_GREATER);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, false, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8GreaterWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_GREATER);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {20, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, false, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest,\n     QuantizedInt8GreaterWithBroadcastMultiplierGreaterThanOne) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_GREATER);\n    model.QuantizeAndPopulate<int8_t>(model.input1(),\n                                      {572, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, false, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8GreaterEqualWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_GREATER_EQUAL);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, true, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8GreaterEqualWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_GREATER_EQUAL);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {20, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(true, false, false, true, true, true))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8LessWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_LESS);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, true, false, false, false))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8LessWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_LESS);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {20, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, true, false, false, false))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedUInt8LessEqualWithBroadcast) {\n  const float kMin = -1.f;\n  const float kMax = 128.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_UINT8, test_shapes[i], kMin, kMax},\n                            {TensorType_UINT8, {}, kMin, kMax},\n                            TensorType_UINT8, BuiltinOperator_LESS_EQUAL);\n    model.QuantizeAndPopulate<uint8_t>(model.input1(), {20, 2, 7, 8, 11, 20});\n    model.QuantizeAndPopulate<uint8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, true, true, false, false))\n        << \"With shape number \" << i;\n  }\n}\n\nTEST(ComparisonsTest, QuantizedInt8LessEqualWithBroadcast) {\n  const float kMin = -127.f;\n  const float kMax = 127.f;\n  std::vector<std::vector<int>> test_shapes = {\n      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n  for (int i = 0; i < test_shapes.size(); ++i) {\n    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n                            BuiltinOperator_LESS_EQUAL);\n    model.QuantizeAndPopulate<int8_t>(model.input1(), {20, -2, -71, 8, 11, 20});\n    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n    model.Invoke();\n    EXPECT_THAT(model.GetOutput(),\n                ElementsAre(false, true, true, true, false, false))\n        << \"With shape number \" << i;\n  }\n}\n}  // namespace\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/comparisons.cc", "tensorflow/lite/kernels/comparisons_test.cc"], "buggy_code_start_loc": [83, 655], "buggy_code_end_loc": [100, 655], "fixing_code_start_loc": [84, 656], "fixing_code_end_loc": [109, 676], "type": "CWE-20", "message": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, certain TFLite models that were created using TFLite model converter would crash when loaded in the TFLite interpreter. The culprit is that during quantization the scale of values could be greater than 1 but code was always assuming sub-unit scaling. Thus, since code was calling `QuantizeMultiplierSmallerThanOneExp`, the `TFLITE_CHECK_LT` assertion would trigger and abort the process. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "other": {"cve": {"id": "CVE-2022-29212", "sourceIdentifier": "security-advisories@github.com", "published": "2022-05-21T00:15:11.720", "lastModified": "2022-06-03T15:16:54.093", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, certain TFLite models that were created using TFLite model converter would crash when loaded in the TFLite interpreter. The culprit is that during quantization the scale of values could be greater than 1 but code was always assuming sub-unit scaling. Thus, since code was calling `QuantizeMultiplierSmallerThanOneExp`, the `TFLITE_CHECK_LT` assertion would trigger and abort the process. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En versiones anteriores a 2.9.0, 2.8.1, 2.7.2 y 2.6.4, algunos modelos TFLite creados con el convertidor de modelos TFLite eran bloqueados cuando eran cargados en el int\u00e9rprete TFLite. El culpable es que durante la cuantificaci\u00f3n la escala de los valores pod\u00eda ser mayor que 1 pero el c\u00f3digo siempre asum\u00eda una escala de subunidades. As\u00ed, como el c\u00f3digo llamaba a \"QuantizeMultiplierSmallerThanOneExp\", la aserci\u00f3n \"TFLITE_CHECK_LT\" era disparada y abortaba el proceso. Las versiones 2.9.0, 2.8.1, 2.7.2 y 2.6.4 contienen un parche para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.4", "matchCriteriaId": "D9359D32-D090-44CF-AC43-2046084A28BB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:-:*:*:*:*:*:*", "matchCriteriaId": "E9EA1898-ACAA-4699-8BAE-54D62C1819FB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "130DE3C9-6842-456F-A259-BF8FF8457217"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "BBF2FCEF-989C-409D-9F4C-81418C65B972"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "9CFB1CFC-579D-4647-A472-6DE8BE1951DE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "F3F3F37E-D27F-4060-830C-0AFF16150777"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/lite/kernels/internal/quantization_util.cc#L114-L123", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/a989426ee1346693cc015792f11d715f6944f2b8", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/issues/43661", "source": "security-advisories@github.com", "tags": ["Exploit", "Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.6.4", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.7.2", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.8.1", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.9.0", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-8wwm-6264-x792", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/a989426ee1346693cc015792f11d715f6944f2b8"}}