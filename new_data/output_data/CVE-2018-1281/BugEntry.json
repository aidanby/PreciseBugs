{"buggy_code": ["/**\n *  Copyright (c) 2015 by Contributors\n */\n#ifndef PS_ZMQ_VAN_H_\n#define PS_ZMQ_VAN_H_\n#include <zmq.h>\n#include <stdlib.h>\n#include <thread>\n#include <string>\n#include \"ps/internal/van.h\"\n#if _MSC_VER\n#define rand_r(x) rand()\n#endif\n\nnamespace ps {\n/**\n * \\brief be smart on freeing recved data\n */\ninline void FreeData(void *data, void *hint) {\n  if (hint == NULL) {\n    delete [] static_cast<char*>(data);\n  } else {\n    delete static_cast<SArray<char>*>(hint);\n  }\n}\n\n/**\n * \\brief ZMQ based implementation\n */\nclass ZMQVan : public Van {\n public:\n  ZMQVan() { }\n  virtual ~ZMQVan() { }\n\n protected:\n  void Start() override {\n    // start zmq\n    context_ = zmq_ctx_new();\n    CHECK(context_ != NULL) << \"create 0mq context failed\";\n    zmq_ctx_set(context_, ZMQ_MAX_SOCKETS, 65536);\n    // zmq_ctx_set(context_, ZMQ_IO_THREADS, 4);\n    Van::Start();\n  }\n\n  void Stop() override {\n    PS_VLOG(1) << my_node_.ShortDebugString() << \" is stopping\";\n    Van::Stop();\n    // close sockets\n    int linger = 0;\n    int rc = zmq_setsockopt(receiver_, ZMQ_LINGER, &linger, sizeof(linger));\n    CHECK(rc == 0 || errno == ETERM);\n    CHECK_EQ(zmq_close(receiver_), 0);\n    for (auto& it : senders_) {\n      int rc = zmq_setsockopt(it.second, ZMQ_LINGER, &linger, sizeof(linger));\n      CHECK(rc == 0 || errno == ETERM);\n      CHECK_EQ(zmq_close(it.second), 0);\n    }\n    zmq_ctx_destroy(context_);\n  }\n\n  int Bind(const Node& node, int max_retry) override {\n    receiver_ = zmq_socket(context_, ZMQ_ROUTER);\n    CHECK(receiver_ != NULL)\n        << \"create receiver socket failed: \" << zmq_strerror(errno);\n    int local = GetEnv(\"DMLC_LOCAL\", 0);\n    std::string addr = local ? \"ipc:///tmp/\" : \"tcp://*:\";\n    int port = node.port;\n    unsigned seed = static_cast<unsigned>(time(NULL)+port);\n    for (int i = 0; i < max_retry+1; ++i) {\n      auto address = addr + std::to_string(port);\n      if (zmq_bind(receiver_, address.c_str()) == 0) break;\n      if (i == max_retry) {\n        port = -1;\n      } else {\n        port = 10000 + rand_r(&seed) % 40000;\n      }\n    }\n    return port;\n  }\n\n  void Connect(const Node& node) override {\n    CHECK_NE(node.id, node.kEmpty);\n    CHECK_NE(node.port, node.kEmpty);\n    CHECK(node.hostname.size());\n    int id = node.id;\n    auto it = senders_.find(id);\n    if (it != senders_.end()) {\n      zmq_close(it->second);\n    }\n    // worker doesn't need to connect to the other workers. same for server\n    if ((node.role == my_node_.role) &&\n        (node.id != my_node_.id)) {\n      return;\n    }\n    void *sender = zmq_socket(context_, ZMQ_DEALER);\n    CHECK(sender != NULL)\n        << zmq_strerror(errno)\n        << \". it often can be solved by \\\"sudo ulimit -n 65536\\\"\"\n        << \" or edit /etc/security/limits.conf\";\n    if (my_node_.id != Node::kEmpty) {\n      std::string my_id = \"ps\" + std::to_string(my_node_.id);\n      zmq_setsockopt(sender, ZMQ_IDENTITY, my_id.data(), my_id.size());\n    }\n    // connect\n    std::string addr = \"tcp://\" + node.hostname + \":\" + std::to_string(node.port);\n    if (GetEnv(\"DMLC_LOCAL\", 0)) {\n      addr = \"ipc:///tmp/\" + std::to_string(node.port);\n    }\n    if (zmq_connect(sender, addr.c_str()) != 0) {\n      LOG(FATAL) <<  \"connect to \" + addr + \" failed: \" + zmq_strerror(errno);\n    }\n    senders_[id] = sender;\n  }\n\n  int SendMsg(const Message& msg) override {\n    std::lock_guard<std::mutex> lk(mu_);\n    // find the socket\n    int id = msg.meta.recver;\n    CHECK_NE(id, Meta::kEmpty);\n    auto it = senders_.find(id);\n    if (it == senders_.end()) {\n      LOG(WARNING) << \"there is no socket to node \" << id;\n      return -1;\n    }\n    void *socket = it->second;\n\n    // send meta\n    int meta_size; char* meta_buf;\n    PackMeta(msg.meta, &meta_buf, &meta_size);\n    int tag = ZMQ_SNDMORE;\n    int n = msg.data.size();\n    if (n == 0) tag = 0;\n    zmq_msg_t meta_msg;\n    zmq_msg_init_data(&meta_msg, meta_buf, meta_size, FreeData, NULL);\n    while (true) {\n      if (zmq_msg_send(&meta_msg, socket, tag) == meta_size) break;\n      if (errno == EINTR) continue;\n      LOG(WARNING) << \"failed to send message to node [\" << id\n                   << \"] errno: \" << errno << \" \" << zmq_strerror(errno);\n      return -1;\n    }\n    zmq_msg_close(&meta_msg);\n    int send_bytes = meta_size;\n\n    // send data\n    for (int i = 0; i < n; ++i) {\n      zmq_msg_t data_msg;\n      SArray<char>* data = new SArray<char>(msg.data[i]);\n      int data_size = data->size();\n      zmq_msg_init_data(&data_msg, data->data(), data->size(), FreeData, data);\n      if (i == n - 1) tag = 0;\n      while (true) {\n        if (zmq_msg_send(&data_msg, socket, tag) == data_size) break;\n        if (errno == EINTR) continue;\n        LOG(WARNING) << \"failed to send message to node [\" << id\n                     << \"] errno: \" << errno << \" \" << zmq_strerror(errno)\n                     << \". \" << i << \"/\" << n;\n        return -1;\n      }\n      zmq_msg_close(&data_msg);\n      send_bytes += data_size;\n    }\n    return send_bytes;\n  }\n\n  int RecvMsg(Message* msg) override {\n    msg->data.clear();\n    size_t recv_bytes = 0;\n    for (int i = 0; ; ++i) {\n      zmq_msg_t* zmsg = new zmq_msg_t;\n      CHECK(zmq_msg_init(zmsg) == 0) << zmq_strerror(errno);\n      while (true) {\n        if (zmq_msg_recv(zmsg, receiver_, 0) != -1) break;\n        if (errno == EINTR) continue;\n        LOG(WARNING) << \"failed to receive message. errno: \"\n                     << errno << \" \" << zmq_strerror(errno);\n        return -1;\n      }\n      char* buf = CHECK_NOTNULL((char *)zmq_msg_data(zmsg));\n      size_t size = zmq_msg_size(zmsg);\n      recv_bytes += size;\n\n      if (i == 0) {\n        // identify\n        msg->meta.sender = GetNodeID(buf, size);\n        msg->meta.recver = my_node_.id;\n        CHECK(zmq_msg_more(zmsg));\n        zmq_msg_close(zmsg);\n        delete zmsg;\n      } else if (i == 1) {\n        // task\n        UnpackMeta(buf, size, &(msg->meta));\n        zmq_msg_close(zmsg);\n        bool more = zmq_msg_more(zmsg);\n        delete zmsg;\n        if (!more) break;\n      } else {\n        // zero-copy\n        SArray<char> data;\n        data.reset(buf, size, [zmsg, size](char* buf) {\n            zmq_msg_close(zmsg);\n            delete zmsg;\n          });\n        msg->data.push_back(data);\n        if (!zmq_msg_more(zmsg)) { break; }\n      }\n    }\n    return recv_bytes;\n  }\n\n private:\n  /**\n   * return the node id given the received identity\n   * \\return -1 if not find\n   */\n  int GetNodeID(const char* buf, size_t size) {\n    if (size > 2 && buf[0] == 'p' && buf[1] == 's') {\n      int id = 0;\n      size_t i = 2;\n      for (; i < size; ++i) {\n        if (buf[i] >= '0' && buf[i] <= '9') {\n          id = id * 10 + buf[i] - '0';\n        } else {\n          break;\n        }\n      }\n      if (i == size) return id;\n    }\n    return Meta::kEmpty;\n  }\n\n  void *context_ = nullptr;\n  /**\n   * \\brief node_id to the socket for sending data to this node\n   */\n  std::unordered_map<int, void*> senders_;\n  std::mutex mu_;\n  void *receiver_ = nullptr;\n};\n}  // namespace ps\n\n#endif  // PS_ZMQ_VAN_H_\n\n\n\n\n\n// monitors the liveness other nodes if this is\n// a schedule node, or monitors the liveness of the scheduler otherwise\n// aliveness monitor\n// CHECK(!zmq_socket_monitor(\n//     senders_[kScheduler], \"inproc://monitor\", ZMQ_EVENT_ALL));\n// monitor_thread_ = std::unique_ptr<std::thread>(\n//     new std::thread(&Van::Monitoring, this));\n// monitor_thread_->detach();\n\n// void Van::Monitoring() {\n//   void *s = CHECK_NOTNULL(zmq_socket(context_, ZMQ_PAIR));\n//   CHECK(!zmq_connect(s, \"inproc://monitor\"));\n//   while (true) {\n//     //  First frame in message contains event number and value\n//     zmq_msg_t msg;\n//     zmq_msg_init(&msg);\n//     if (zmq_msg_recv(&msg, s, 0) == -1) {\n//       if (errno == EINTR) continue;\n//       break;\n//     }\n//     uint8_t *data = static_cast<uint8_t*>(zmq_msg_data(&msg));\n//     int event = *reinterpret_cast<uint16_t*>(data);\n//     // int value = *(uint32_t *)(data + 2);\n\n//     // Second frame in message contains event address. it's just the router's\n//     // address. no help\n\n//     if (event == ZMQ_EVENT_DISCONNECTED) {\n//       if (!is_scheduler_) {\n//         PS_VLOG(1) << my_node_.ShortDebugString() << \": scheduler is dead. exit.\";\n//         exit(-1);\n//       }\n//     }\n//     if (event == ZMQ_EVENT_MONITOR_STOPPED) {\n//       break;\n//     }\n//   }\n//   zmq_close(s);\n// }\n"], "fixing_code": ["/**\n *  Copyright (c) 2015 by Contributors\n */\n#ifndef PS_ZMQ_VAN_H_\n#define PS_ZMQ_VAN_H_\n#include <zmq.h>\n#include <stdlib.h>\n#include <thread>\n#include <string>\n#include \"ps/internal/van.h\"\n#if _MSC_VER\n#define rand_r(x) rand()\n#endif\n\nnamespace ps {\n/**\n * \\brief be smart on freeing recved data\n */\ninline void FreeData(void *data, void *hint) {\n  if (hint == NULL) {\n    delete [] static_cast<char*>(data);\n  } else {\n    delete static_cast<SArray<char>*>(hint);\n  }\n}\n\n/**\n * \\brief ZMQ based implementation\n */\nclass ZMQVan : public Van {\n public:\n  ZMQVan() { }\n  virtual ~ZMQVan() { }\n\n protected:\n  void Start() override {\n    // start zmq\n    context_ = zmq_ctx_new();\n    CHECK(context_ != NULL) << \"create 0mq context failed\";\n    zmq_ctx_set(context_, ZMQ_MAX_SOCKETS, 65536);\n    // zmq_ctx_set(context_, ZMQ_IO_THREADS, 4);\n    Van::Start();\n  }\n\n  void Stop() override {\n    PS_VLOG(1) << my_node_.ShortDebugString() << \" is stopping\";\n    Van::Stop();\n    // close sockets\n    int linger = 0;\n    int rc = zmq_setsockopt(receiver_, ZMQ_LINGER, &linger, sizeof(linger));\n    CHECK(rc == 0 || errno == ETERM);\n    CHECK_EQ(zmq_close(receiver_), 0);\n    for (auto& it : senders_) {\n      int rc = zmq_setsockopt(it.second, ZMQ_LINGER, &linger, sizeof(linger));\n      CHECK(rc == 0 || errno == ETERM);\n      CHECK_EQ(zmq_close(it.second), 0);\n    }\n    zmq_ctx_destroy(context_);\n  }\n\n  int Bind(const Node& node, int max_retry) override {\n    receiver_ = zmq_socket(context_, ZMQ_ROUTER);\n    CHECK(receiver_ != NULL)\n        << \"create receiver socket failed: \" << zmq_strerror(errno);\n    int local = GetEnv(\"DMLC_LOCAL\", 0);\n    std::string hostname = node.hostname.empty() ? \"*\" : node.hostname;\n    std::string addr = local ? \"ipc:///tmp/\" : \"tcp://\" + hostname + \":\";\n    int port = node.port;\n    unsigned seed = static_cast<unsigned>(time(NULL)+port);\n    for (int i = 0; i < max_retry+1; ++i) {\n      auto address = addr + std::to_string(port);\n      if (zmq_bind(receiver_, address.c_str()) == 0) break;\n      if (i == max_retry) {\n        port = -1;\n      } else {\n        port = 10000 + rand_r(&seed) % 40000;\n      }\n    }\n    return port;\n  }\n\n  void Connect(const Node& node) override {\n    CHECK_NE(node.id, node.kEmpty);\n    CHECK_NE(node.port, node.kEmpty);\n    CHECK(node.hostname.size());\n    int id = node.id;\n    auto it = senders_.find(id);\n    if (it != senders_.end()) {\n      zmq_close(it->second);\n    }\n    // worker doesn't need to connect to the other workers. same for server\n    if ((node.role == my_node_.role) &&\n        (node.id != my_node_.id)) {\n      return;\n    }\n    void *sender = zmq_socket(context_, ZMQ_DEALER);\n    CHECK(sender != NULL)\n        << zmq_strerror(errno)\n        << \". it often can be solved by \\\"sudo ulimit -n 65536\\\"\"\n        << \" or edit /etc/security/limits.conf\";\n    if (my_node_.id != Node::kEmpty) {\n      std::string my_id = \"ps\" + std::to_string(my_node_.id);\n      zmq_setsockopt(sender, ZMQ_IDENTITY, my_id.data(), my_id.size());\n    }\n    // connect\n    std::string addr = \"tcp://\" + node.hostname + \":\" + std::to_string(node.port);\n    if (GetEnv(\"DMLC_LOCAL\", 0)) {\n      addr = \"ipc:///tmp/\" + std::to_string(node.port);\n    }\n    if (zmq_connect(sender, addr.c_str()) != 0) {\n      LOG(FATAL) <<  \"connect to \" + addr + \" failed: \" + zmq_strerror(errno);\n    }\n    senders_[id] = sender;\n  }\n\n  int SendMsg(const Message& msg) override {\n    std::lock_guard<std::mutex> lk(mu_);\n    // find the socket\n    int id = msg.meta.recver;\n    CHECK_NE(id, Meta::kEmpty);\n    auto it = senders_.find(id);\n    if (it == senders_.end()) {\n      LOG(WARNING) << \"there is no socket to node \" << id;\n      return -1;\n    }\n    void *socket = it->second;\n\n    // send meta\n    int meta_size; char* meta_buf;\n    PackMeta(msg.meta, &meta_buf, &meta_size);\n    int tag = ZMQ_SNDMORE;\n    int n = msg.data.size();\n    if (n == 0) tag = 0;\n    zmq_msg_t meta_msg;\n    zmq_msg_init_data(&meta_msg, meta_buf, meta_size, FreeData, NULL);\n    while (true) {\n      if (zmq_msg_send(&meta_msg, socket, tag) == meta_size) break;\n      if (errno == EINTR) continue;\n      LOG(WARNING) << \"failed to send message to node [\" << id\n                   << \"] errno: \" << errno << \" \" << zmq_strerror(errno);\n      return -1;\n    }\n    zmq_msg_close(&meta_msg);\n    int send_bytes = meta_size;\n\n    // send data\n    for (int i = 0; i < n; ++i) {\n      zmq_msg_t data_msg;\n      SArray<char>* data = new SArray<char>(msg.data[i]);\n      int data_size = data->size();\n      zmq_msg_init_data(&data_msg, data->data(), data->size(), FreeData, data);\n      if (i == n - 1) tag = 0;\n      while (true) {\n        if (zmq_msg_send(&data_msg, socket, tag) == data_size) break;\n        if (errno == EINTR) continue;\n        LOG(WARNING) << \"failed to send message to node [\" << id\n                     << \"] errno: \" << errno << \" \" << zmq_strerror(errno)\n                     << \". \" << i << \"/\" << n;\n        return -1;\n      }\n      zmq_msg_close(&data_msg);\n      send_bytes += data_size;\n    }\n    return send_bytes;\n  }\n\n  int RecvMsg(Message* msg) override {\n    msg->data.clear();\n    size_t recv_bytes = 0;\n    for (int i = 0; ; ++i) {\n      zmq_msg_t* zmsg = new zmq_msg_t;\n      CHECK(zmq_msg_init(zmsg) == 0) << zmq_strerror(errno);\n      while (true) {\n        if (zmq_msg_recv(zmsg, receiver_, 0) != -1) break;\n        if (errno == EINTR) continue;\n        LOG(WARNING) << \"failed to receive message. errno: \"\n                     << errno << \" \" << zmq_strerror(errno);\n        return -1;\n      }\n      char* buf = CHECK_NOTNULL((char *)zmq_msg_data(zmsg));\n      size_t size = zmq_msg_size(zmsg);\n      recv_bytes += size;\n\n      if (i == 0) {\n        // identify\n        msg->meta.sender = GetNodeID(buf, size);\n        msg->meta.recver = my_node_.id;\n        CHECK(zmq_msg_more(zmsg));\n        zmq_msg_close(zmsg);\n        delete zmsg;\n      } else if (i == 1) {\n        // task\n        UnpackMeta(buf, size, &(msg->meta));\n        zmq_msg_close(zmsg);\n        bool more = zmq_msg_more(zmsg);\n        delete zmsg;\n        if (!more) break;\n      } else {\n        // zero-copy\n        SArray<char> data;\n        data.reset(buf, size, [zmsg, size](char* buf) {\n            zmq_msg_close(zmsg);\n            delete zmsg;\n          });\n        msg->data.push_back(data);\n        if (!zmq_msg_more(zmsg)) { break; }\n      }\n    }\n    return recv_bytes;\n  }\n\n private:\n  /**\n   * return the node id given the received identity\n   * \\return -1 if not find\n   */\n  int GetNodeID(const char* buf, size_t size) {\n    if (size > 2 && buf[0] == 'p' && buf[1] == 's') {\n      int id = 0;\n      size_t i = 2;\n      for (; i < size; ++i) {\n        if (buf[i] >= '0' && buf[i] <= '9') {\n          id = id * 10 + buf[i] - '0';\n        } else {\n          break;\n        }\n      }\n      if (i == size) return id;\n    }\n    return Meta::kEmpty;\n  }\n\n  void *context_ = nullptr;\n  /**\n   * \\brief node_id to the socket for sending data to this node\n   */\n  std::unordered_map<int, void*> senders_;\n  std::mutex mu_;\n  void *receiver_ = nullptr;\n};\n}  // namespace ps\n\n#endif  // PS_ZMQ_VAN_H_\n\n\n\n\n\n// monitors the liveness other nodes if this is\n// a schedule node, or monitors the liveness of the scheduler otherwise\n// aliveness monitor\n// CHECK(!zmq_socket_monitor(\n//     senders_[kScheduler], \"inproc://monitor\", ZMQ_EVENT_ALL));\n// monitor_thread_ = std::unique_ptr<std::thread>(\n//     new std::thread(&Van::Monitoring, this));\n// monitor_thread_->detach();\n\n// void Van::Monitoring() {\n//   void *s = CHECK_NOTNULL(zmq_socket(context_, ZMQ_PAIR));\n//   CHECK(!zmq_connect(s, \"inproc://monitor\"));\n//   while (true) {\n//     //  First frame in message contains event number and value\n//     zmq_msg_t msg;\n//     zmq_msg_init(&msg);\n//     if (zmq_msg_recv(&msg, s, 0) == -1) {\n//       if (errno == EINTR) continue;\n//       break;\n//     }\n//     uint8_t *data = static_cast<uint8_t*>(zmq_msg_data(&msg));\n//     int event = *reinterpret_cast<uint16_t*>(data);\n//     // int value = *(uint32_t *)(data + 2);\n\n//     // Second frame in message contains event address. it's just the router's\n//     // address. no help\n\n//     if (event == ZMQ_EVENT_DISCONNECTED) {\n//       if (!is_scheduler_) {\n//         PS_VLOG(1) << my_node_.ShortDebugString() << \": scheduler is dead. exit.\";\n//         exit(-1);\n//       }\n//     }\n//     if (event == ZMQ_EVENT_MONITOR_STOPPED) {\n//       break;\n//     }\n//   }\n//   zmq_close(s);\n// }\n"], "filenames": ["src/zmq_van.h"], "buggy_code_start_loc": [66], "buggy_code_end_loc": [67], "fixing_code_start_loc": [66], "fixing_code_end_loc": [68], "type": "CWE-200", "message": "The clustered setup of Apache MXNet allows users to specify which IP address and port the scheduler will listen on via the DMLC_PS_ROOT_URI and DMLC_PS_ROOT_PORT env variables. In versions older than 1.0.0, however, the MXNet framework will listen on 0.0.0.0 rather than user specified DMLC_PS_ROOT_URI once a scheduler node is initialized. This exposes the instance running MXNet to any attackers reachable via the interface they didn't expect to be listening on. For example: If a user wants to run a clustered setup locally, they may specify to run on 127.0.0.1. But since MXNet will listen on 0.0.0.0, it makes the port accessible on all network interfaces.", "other": {"cve": {"id": "CVE-2018-1281", "sourceIdentifier": "security@apache.org", "published": "2018-06-08T19:29:00.263", "lastModified": "2018-08-03T15:53:09.547", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The clustered setup of Apache MXNet allows users to specify which IP address and port the scheduler will listen on via the DMLC_PS_ROOT_URI and DMLC_PS_ROOT_PORT env variables. In versions older than 1.0.0, however, the MXNet framework will listen on 0.0.0.0 rather than user specified DMLC_PS_ROOT_URI once a scheduler node is initialized. This exposes the instance running MXNet to any attackers reachable via the interface they didn't expect to be listening on. For example: If a user wants to run a clustered setup locally, they may specify to run on 127.0.0.1. But since MXNet will listen on 0.0.0.0, it makes the port accessible on all network interfaces."}, {"lang": "es", "value": "La instalaci\u00f3n en cl\u00fasters de Apache MXNet permite que los usuarios especifiquen en qu\u00e9 direcci\u00f3n y puerto IP va a escuchar el scheduler mediante las variables de entorno DMLC_PS_ROOT_URI y DMLC_PS_ROOT_PORT. En las versiones anteriores a la 1.0.0, sin embargo, el framework MXNet escuchar\u00e1 en 0.0.0.0 en lugar del DMLC_PS_ROOT_URI especificado por el usuario una vez se ha inicializado un nodo scheduler. Esto expone la instancia que est\u00e1 ejecutando MXNet a cualquier atacante y la vuelve alcanzable mediante la interfaz que no esperaban que estuviese escuchando. Por ejemplo: si un usuario quiere ejecutar localmente una instalaci\u00f3n en cl\u00fasters, puede especificar que se ejecute en 127.0.0.1. Pero, debido a que MXNet escuchar\u00e1 en 0.0.0.0, hace que el puerto sea accesible en todas las interfaces de red."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:P/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:apache:mxnet:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.0.0", "matchCriteriaId": "0EC14C93-9AF9-40BC-AA91-17399E9E5B6F"}]}]}], "references": [{"url": "https://github.com/dmlc/ps-lite/commit/4be817e8b03e7e92517e91f2dfcc50865e91c6ea", "source": "security@apache.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/dmlc/ps-lite/commit/4be817e8b03e7e92517e91f2dfcc50865e91c6ea"}}